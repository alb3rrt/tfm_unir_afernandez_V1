# -*- coding: utf-8 -*-
"""Feature extraction from text file

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wywalrL-VuLagTJtYI9r0o_RHkG1KJ9F
"""

import pandas as pd
import nltk
from nltk.corpus import stopwords
from  nltk.stem import SnowballStemmer

import spacy


with open('/content/drive/MyDrive/data/1.txt',encoding='ISO 8859-1') as f:
    lines = f.readlines()
    

nlp = spacy.load("en_core_web_sm")
stops = spacy.lang.en.stop_words.STOP_WORDS


def normalize(comment, lowercase, remove_stopwords):
    if lowercase:
        comment = comment.lower()
    comment = nlp(comment)
    lemmatized = list()
    for word in comment:
        lemma = word.lemma_.strip()
        if lemma:
            if not remove_stopwords or (remove_stopwords and lemma not in stops):
                lemmatized.append(lemma)
    return lemmatized

text=''.join(lines)
text_clean=normalize(text,lowercase=True, remove_stopwords=True)
# calculating tf-idf values

text_clean=[x for x in text_clean if len(x)>=3 and x!="-PRON-"]

import pandas as pd
import numpy as np
from itertools import islice
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer

cvec = CountVectorizer(stop_words='english', min_df=1, max_df=.9, ngram_range=(1,6))
cvec.fit(text_clean)

cvec_count = cvec.transform(text_clean)

transformer = TfidfTransformer()
transformed_weights = transformer.fit_transform(cvec_count)

transformed_weights

weights = np.asarray(transformed_weights.mean(axis=0)).ravel().tolist()
weight_df = pd.DataFrame({'term' : cvec.get_feature_names(), 'weight' : weights})
tf_idf = weight_df.sort_values(by='weight', ascending=False).head(20)
print(tf_idf)

occ = np.asarray(cvec_count.sum(axis=0)).ravel().tolist()
count_df = pd.DataFrame({'term': cvec.get_feature_names(), 'occurrences' : occ})
term_freq = count_df.sort_values(by='occurrences', ascending=False).head(20)
print(term_freq)

