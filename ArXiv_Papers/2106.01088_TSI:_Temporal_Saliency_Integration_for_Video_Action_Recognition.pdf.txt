TSI: Temporal Saliency Integration for Video Action Recognition
Haisheng Su1, Jinyuan Feng2 Dongliang Wang1 Weihao Gan1 Wei Wu1 Yu Qiao3,4 1SenseTime Research 2Chongqing University
3Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences 4Shanghai AI Laboratory, Shanghai, China
{suhaisheng,wangdongliang,ganweihao,wuwei}@sensetime.com, fengjinyuan@outlook.com
yu.qiao@siat.ac.cn

arXiv:2106.01088v1 [cs.CV] 2 Jun 2021

Abstract
Efficient spatiotemporal modeling is an important yet challenging problem for video action recognition. Existing state-of-the-art methods exploit motion clues to assist in short-term temporal modeling through temporal difference over consecutive frames. However, background noises will be inevitably introduced due to the camera movement. Besides, movements of different actions can vary greatly. In this paper, we propose a Temporal Saliency Integration (TSI) block, which mainly contains a Salient Motion Excitation (SME) module and a Cross-scale Temporal Integration (CTI) module. Specifically, SME aims to highlight the motion-sensitive area through local-global motion modeling, where the background suppression and pyramidal feature difference are conducted successively between neighboring frames to capture motion dynamics with less background noises. CTI is designed to perform multi-scale temporal modeling through a group of separate 1D convolutions respectively. Meanwhile, temporal interactions across different scales are integrated with attention mechanism. Through these two modules, long short-term temporal relationships can be encoded efficiently by introducing limited additional parameters. Extensive experiments are conducted on several popular benchmarks (i.e., SomethingSomething v1 & v2, Kinetics-400, UCF-101, and HMDB51), which demonstrate the effectiveness and superiority of our proposed method.
1. Introduction
With the rapid advancement of Internet and intelligent cameras, video data have increased dramatically over the past years. Hence, efficient video analytic technology is important and attracts many research interests in video understanding. Action recognition, which aims to classify the trimmed videos into specific categories, is one of the basic
 Equal contribution.  Corresponding author.

Figure 1. Illustration of motivation. First row is an example of "golf" action with static background. Second row is the "walk dog" action with dynamic background owing to the camera movement. Third row shows the action (taking something from somewhere) with large movement, while the fourth row indicates the same action with slight movement.
tasks in video analytics. And it has a wide range of applications such as intelligent surveillance, human-computer interaction, and personalized recommendation. Undoubtedly, efficient spatiotemporal modeling of videos is vital for achieving promising recognition accuracy and meanwhile ensuring reasonable inference complexity.
Recent years have witnessed significant progress achieved in action recognition [21, 14, 29, 25, 23, 24]. Deep learning based paradigm typically contains two main categories, i.e. two-stream networks [32, 21, 26, 27] and 3D networks [28, 29, 30, 6, 3]. The former type of methods capture the appearance and motion information from RGB images and stacked optical flow respectively. However, the extraction of optical flow is expensive in both time and space, which limits its real application. 3D convolutional networks exploit 3D convolutions to learn spatiotemporal features directly from raw videos. With the help of largescale video datasets, superior performance can be obtained. However, tremendous parameters of stacked 3D convolutions inevitably increase the computing cost. Currently,

many researchers explore to efficiently encode spatiotemporal features and motion encoding into a unified framework through decoupling 3D convolution into (2+1)D convolutions (i.e. 1 × 3 × 3 and 3 × 1 × 1) or performing featurelevel difference between adjacent frames to capture motion representations. Nevertheless, as shown in Fig. 1, there still exists three main drawbacks: (1) direct feature-level difference will inevitably introduce background noises because of camera movements; (2) only local motion modeling is incapable of handling motion structure of various scales; (3) stacking local 1D convolutions is inferior to model longrange temporal relationships especially in the shallow layers of deep networks.
To relieve above issues, we propose a Temporal Saliency Integration (TSI) network to capture spatiotemporal features and motion encoding for action recognition. Firstly, a Salient Motion Excitation (SME) module is proposed to capture motion dynamics through Local-Global Motion Modeling (LGMM). Taking the camera movements into consideration, we perform the background suppression operation between adjacent frames in advance to eliminate the effect of noises. Then the pyramidal feature difference is conducted to handle motion structure of various scales, and the results are summed to form the motion representation. Unlike [14], which adds motion encoding to the spatiotemporal features for action recognition, we generate a channelwise motion attention weight to enhance the discriminability of motion-sensitive area. Next, in order to model longrange temporal relationships, a Cross-scale Temporal Integration (CTI) module is designed to implement multi-scale temporal modeling through adopting a series of depth-wise 1D convolutions upon the grouped channels respectively. Meanwhile, temporal interactions across different scales are integrated with each other using attention mechanism, which is intuitive but neglected in existing methods. Finally, the proposed TSI block can be inserted into the offthe-shelf 2D CNNs to form a simple but effective network for efficient action recognition. Benefiting from the elaborate design, only limited extra computing cost is introduced (1.03× GFLOPs as many as 2D ResNet). Extensive experiments reveal that these two modules are complementary in modeling long short-term temporal relationships. To summarize, the main contributions of our work are three folds:
· The Salient Motion Excitation (SME) module is proposed to encode motion structure of various scales with less noises through background suppression and local-global motion modeling.
· The Cross-scale Temporal Integration (CTI) module is designed to perform multi-scale temporal modeling with cross-scale integration. And the two modules can be inserted into standard ResNet block to capture the long short-term temporal relationships.

· Extensive experiments are conducted on five public benchmarks, which demonstrate that the proposed TSI network can outperform other state-of-the-art methods with superior performance and achieve competitive efficiency compared to 2D CNNs.
2. Related Work
2D CNNs: Two-stream network [9] adopts the spatial stream to extract appearance features and the temporal stream to capture motion information respectively. Then TSN [32] proposes sparse sampling strategy and weighted average score fusion of these two streams. In order to further improve the temporal modeling capability, TRN [40] exploits a set of fully connected layers to combine the relationships among different frames at the end. Recently, many researchers explore to develop a feature-level inter-frame difference method to model the short-term motion information between adjacent frames. For example, STM [14] adopts feature difference of neighboring frames to obtain the complementary motion encoding of spatiotemporal features. TEINet [19] first performs feature-level spatial pooling and then adopts the adjacent feature difference to obtain channel-wise attention aiming to enhance the motion-sensitive features. TEA [17] mainly employs interframe difference for short-term temporal modeling and the Res2Net structure to model the long-term temporal relationships. TSM [18] proposes a temporal shift operation to efficiently exchange temporal information along the channel dimension of features. Furthermore, TIN [20] proposes the deformable shift module to adaptively adjust the offset of temporal misalignment. However, without background suppression, noises will be inevitably introduced through motion difference. Besides, only local motion modeling fails to handle various kind of movements. Meanwhile, temporal interactions across multiple scales are also neglected in current methods. In this paper, we propose Temporal Saliency Integration (TSI) network to handle these issues accordingly with the Salient Motion Excitation (SME) and Cross-scale Temporal Integration (CTI) modules.
3D CNNs: C3D [28] directly adopts the 3D convolutions to extract local spatiotemporal features in the frame sequence. In order to make full use of the pre-training of the existing 2D CNN model on ImageNet[4] and thus expand the video modeling capabilities of the existing 2D CNN model[41, 10, 8], I3D [2] proposes an inflate 2D convolution kernel as a 3D one to model the spatiotemporal features of video sequences. Since the spatial and temporal modeling process of 3D convolutions[6] are highly coupled, it is not conducive for intuitive analysis of temporal modeling. Hence some researchers[1, 35, 37] propose to decouple the 3D convolution into (2+1)D convolutions [30]. CSN [29] is designed to reduce the computing cost of 3D convolutions

Figure 2. Illustration of the proposed Salient Motion Excitation (SME) and Cross-scale Temporal Integration (CTI) modules. In SME, background suppression between adjacent frames is conducted before performing Local-Global Motion Modeling (LGMM). In CTI, multiscale temporal modeling is performed in cascaded and channel grouping manners with cross-scale temporal interaction integration.

through adopting a depth-wise 3D convolution without ignoring the channel interactions.
3. Method
In this section, we will introduce the proposed Temporal Saliency Integration (TSI) network. First, we give an overview of the two main containing modules (i.e. Salient Motion Excitation (SME) and Cross-scale Temporal Integration Module (CTI)). Then, we describe the technical details of these two modules respectively. Finally, we integrate them into a standard ResNet-50 backbone as TSI network.
3.1. Overview
As shown in Figure 2, the proposed SME-CTI cube is the main component of TSI block which is designed for capturing short-term motion dynamics as well as long-term temporal inter-dependencies respectively. Briefly, the proposed SME-CTI cube has two main contributions. On one hand, Salient Motion Excitation (SME) module adopts pyramidal feature difference to capture movements of various scales with local-global motion modeling. In addition, different from [17, 14], we introduce background suppression to eliminate the background noises caused by camera move-

ment. On the other hand, Cross-scale Temporal Integration (CTI) module aims to capture the long-term temporal relationships with multi-scale temporal modeling together with cross-scale interaction integration.
Specifically, in the SME-CTI cube which is consists of SME and CTI, given input feature X  RT ×C×H×W , where T, C, H, W denote the number of frames, the number of filters, height and width of input features, respectively. Xt indicates the features of t-th snippet. We adopt SME to capture the short-term motion dynamics, which is formulated as follows:

XSME = SME(X),

(1)

where SME represents the module used for extracting both local and global motion dynamics between adjacent frames, thus enhance the discriminality of motion-sensitive areas of input feature X. Details be described in Sec 3.2.
Then we further adopt another CTI module for longterm temporal relationships modeling. Concretely, multiscale temporal modeling is performed upon four separate grouped channels in parallel. Meanwhile, a cross-scale integration method is utilized to integrate temporal interaction of different scales. Elaborate design contributes to the overall high efficiency without temporal diversity loss.

Figure 3. Feature visualization of SME module. First row illustrates the input frames. The input and the output features of SME of Conv2 1 block are presented in the second row and the third row respectively. Motion-sensitive areas (yellow dotted box) can be effectively highlighted while background noises (red dotted box) can also be suppressed.

Figure 4. Visualization of background suppression. The first two columns represent the input frames, the third column is the visualization of calculated similarity matrix, and the last column shows the images after the enhancement of similar regions (best viewed in red dotted box). The first row represents the dynamic background, and the second row represents the static background.

XCT I = CTI(XSME ),

(2)

where CTI represents the Cross-scale Temporal Integration module, which performs upon the input features XSME.
Details will be described in Sec 3.3.

3.2. Salient Motion Excitation

SME is proposed to encode motion structure of various scales with less noises through background suppression and local-global motion modeling. The structure details of SME are shown in Figure 2. The core idea of SME is LocalGlobal Motion Modeling (LGMM), which aims to handle various kind of movements and then extract the motion encoding with pyramidal feature difference . Besides, considering the camera movements, which will inevitably introduce background noises through direct feature difference, hence we perform the background suppression in advance. Background Suppression. Due to the camera movements, direct feature difference operation will unexpectedly include the motion dynamics of static scene. To avoid this issue, we present a simple but effective background suppression, which adopts spatial matrix multiplication with reshape and transpose operations to calculate the similarity between adjacent frames. Then the similar regions are further adopted as spatial-wise attention weights for background suppression of current frame. Illustration is shown in the top left part of Figure 2. The background suppression process can be formulated as follow:

Xrt+(b1s) = Xrt+1 ((Xrt  Xrt+1)  Xrt+1),

(3)

where is the handamard product and  is the inner product. Xrt+(b1s) represents the output features after background suppression with self-attention between neighbor-
ing frames. Discussion. Current methods [14, 17] adopt feature-level
difference between neighboring frames to capture the short-

term motion dynamics, which make sense assuming the background is static. However, as shown in Figure 1, without considering the camera motion, the background noises will be inevitably introduced and the extracted motion difference features are thus inferior. Background suppression is designed for eliminating the dynamic background noises introduced by camera movement, which aims to calculate the regions of similar pixel between adjacent frames, hence only the moving parts inside the actors or static background are retained with structural regions of similar pixels. Then the retained regions are transformed into attention weights to highlight the similar parts between the original features of two consecutive frames while suppress the irrelevant dynamic background as much as possible. As shown in Figure 4, the first row is an input example with dynamic background. We can see from the last column that the similar areas between two frames are highlighted to a certain extent, and the different regions are simultaneously suppressed owing to the movement (moving edge of actor or camera). The second row is another example with static background. We can find that almost whole images are highlighted because both the background and the actor are nearly static. Pyramidal feature difference. After background suppression, we continue to carry out pyramidal feature difference for local-global motion modeling. Considering the complexity of various movements, instead of using a simple 3×3 2D convolution as in [14, 17], we adopt a series of 2D convolutions parallelly to model spatial information on Xrt+(b1s) and expand the spatial receptive fields in a cascaded manner. Then the motion dynamics are encoded hierarchically through neighboring feature difference:

Dk =

Conv2Dj (Xrt+(b1s)), Conv2Dj (Dk-1 + Xrt+(b1s)),

j = 0, k = 0 1  j, 1  k

(4)

Mt,t+1 = Sum(Dk - Xrt ), k = 0, 1, 2, ..., K.

(5)

Mt,t+1 is the difference features between adjacent frames. And K is set to 4 empirically. In order to reduce the computing cost, we adopt two 1 × 1 2D convolutions for channel dimension reduction and recovery before and after LGMM respectively, where the ratio r is set to 16. Conv2Dj indicates the j-th depth-wise 2D convolution used to transform the input Xrt+(b1s). Finally, SME obtains the local-global motion dynamics and further conducts spatial pooling operation to obtain channel-wise motion attention weights A, aiming to highlight the motion-sensitive areas of input features X with an identity shortcut. Visualization results of SME are illustrated in Figure 3.
At,t+1 = Sigmoid(Pooling(LGMM(Xt, Xt+1))), (6)

XSME = Xt + Xt At,t+1.

(7)

Discussion. Different from CMM module proposed in [14], our SME module transforms the feature difference information into attention weights to enhance the representation of moving edges, rather than takes the motion difference clues as another independent features. As shown in Fig 3, the red dotted box illustrates the enhanced area of moving parts. On the other hand, our SME is also different from the ME module proposed in TEA [17], where the background suppression is considered to remove the negative effect of irrelevant dynamic background. Meanwhile, local-global motion modeling is presented to model different actions with various scales in spatial domain, which is ignored in all previous methods.
3.3. Cross-scale Temporal Integration
The structure details of CTI are shown in the Figure 2. The core idea of CTI is to model long-term temporal information by multi-scale temporal modeling. Besides, effective temporal receptive fields are enlarged as well as temporal interactions are exchanged through cross-scale integration. Different from the conventional temporal modeling methods [14, 19], which captures the long-term temporal information with stacking local 1D convolutions, CTI expand the width of network to achieve this goal. Meanwhile, great efficiency is ensured in a channel grouping manner. Unlike Res2Net structure adopted in [17], which only transfers the spatiotemporal information from local to global view through a series of (2+1)D convolutions, our CTI is designed to integrate the temporal interactions of different scales with each other through attention mechanism. In this way, global temporal information can be also transferred to the local view for better understanding of details. Multi-scale Temporal Modeling. Given the input feature XSME  RT ×C×H×W , we first separate the feature channels into G groups, where XSg ME  RT ×C/G×H×W indicates the g-th group features. Then we use a series of separate 1D convolutions of kernel size 3 to extract temporal information of each group features respectively. Multi-scale

Figure 5. The overall architecutre of TSI network. The sparse sampling strategy [32] is adopted to sample T frames from the input video. We adopt 2D ResNet-50 as backbone, where the SME and CTI modules are inserted into each ResNet block to form the TSI block. The simple temporal average pooling is applied to conduct action predictions in the last score fusion stage.

receptive fields are achieved by the Cross-scale Integration (CI) method, which will be described in the following.

XSg ME ,

g=0

Tg = Conv1Dj (CI(Tg-1, XSg ME)), 1  j, 1  g (8)

Tg is the output temporal features of g-th group. G is 4. Cross-scale Integration. In order to perform selective interaction fusion of multiple temporal scales, we conduct spatial pooling on the summed features of adjacent temporal scales. And then we adopt a fully connected layer followed by Softmax activation function to obtain temporal-channel attention weights. Finally, the obtained weights are used to integrate the cross-scale temporal interactions into each scale:

 = Softmax(FC(Pooling(Tg-1 + XSg ME))), (9)

CI(Tg-1, XSg ME ) =  XSg ME + (1 - ) Tg-1. (10)
Discussion. Here we would like to focus on explaining the problem of temporal scale. Specifically, several 1D temporal convolutions with kernel size of 3 are adopted for temporal modeling on different channel groups respectively. Then the temporal receptive fields are enlarged through knowledge transfer from left to right in a cascaded structure. Meanwhile the temporal interactions between neighboring channel groups with different temporal receptive fields are also integrated through attention mechanism for knowledge transfer in a reversed way, which also haven't been explored in previous methods.

3.4. TSI Network

To encode both spatiotemporal information and motion dynamics simultaneously, we combine the proposed SME and CTI together to form a TSI block, which can be easily inserted into the off-the-shelf 2D CNNs, as shown in Fig. 5. In TSI block, the first 1×1 2D convolution is adopted for

channel reduction, then a 3×3 2D convolution is utilized to model the spatial appearance, where the output features are further passed through SME and CTI sequentially to extract long short-term relationships. Finally, in order to restore the original channel dimensions, we add another 1×1 2D convolution followed by a parameter-free identity shortcut from input to output. In order to give full play to the advantages of TSI block, from the perspective of easy optimization and flexibility, we use 2D ResNet-50 [13] as our backbone unless specified. And we replace all standard ResNet blocks with our proposed TSI block.
4. Experiments
In this section, we first introduce the datasets and the implementation details of our proposed TSI. Then we perform extensive experiments to demonstrate that the proposed TSI outperforms all the state-of-the-art methods on both temporal-related datasets and scene-related datasets. Meanwhile, we also conduct abundant ablation studies with Something-Something v1 to analyze the effectiveness of our method. Finally, we give runtime analyses to show the efficiency of TSI compare with state-of-the-art methods.
4.1. Datasets
We verify the effectiveness of our proposed TSI on two kinds of datasets, which focus on temporal and context scene respectively. First, Something-Something V1 & V2 [11] concentrate on temporal information, where V1 includes about 110K videos while V2 includes 220K video clips for 174 fine-grained classes. Second, for Kinetics-400, UCF-101 and HMDB-51, these datasets focus on context scene more. Kinetics-400 [15] has a total of 400 action classes, where the training set is about 240K and the validation set is about 20K. The UCF-101 [22] contains 101 categories with around 13K videos, while HMDB51 [16] has about 7K videos spanning over 51 categories.
4.2. Experimental Setup
Training. We train our TSI with the same strategy as mentioned in [32]. Given an input video, we first divide it into T segments. Then, we randomly sample one frame from each segment to obtain the input sequence with T frames. The size of the short side of these frames is fixed to 256. Meanwhile, corner-croping and scale-jittering are applied for data argumentation. Finally, the input size of the network is N ×T ×3×224×224, where N is the batch size and T is the number of the sampled frames per video. In our experiments, T is set to 8 or 16. For Kinetics, SomethingSomething V1 & V2, we start with a learning rate of 0.01 and reduce it by a factor of 10 at 30,40,45 epochs and stop at 50 epochs. For these large-scale datasets, we only use the ImageNet[4] pre-trained model as initialization. For UCF101 and HMDB-51, we use Kinetics pre-trained model as

initialization and start training with a learning rate of 0.001 for 25 epochs. The learning rate is decayed by a factor 10 every 15 epochs. We use mini-batch SGD as optimizer with a momentum of 0.9 and a weight decay of 5e-4. Inference. Following [17], we first re-scale the shorter spatial side to 256 and take three crops of 256 × 256 to cover the spatial dimensions and then resize them to 224 × 224. We try two kinds of testing protocols: (1) 1 clip and center crop (×1); (2) 10 clips and 3 crops (×30).
4.3. Comparison with the State-of-the-Arts
We further demonstrate the advances of our proposed TSI in comparison with state-of-the-art methods for human action recognition. For fair comparison, all methods use only RGB modality as input with 8 or 16 frames. For UCF101 and HMDB51, we use the testing protocols of 10 clips and 3 crops (×30), with 16 input frames. Results on Something-Something. We first verify the temporal modeling ability of TSI on Something-Something. For Something-V1, we can achieve superior results compared with other methods. Specifically, with 16 frames as input, TSI is 2.0% ahead of TEA[17]. Meanwhile, our method has obtained the advantage results than [14, 19] under the same settings. For Something-V2, our TSI can outperform the TEINet by about 1.4% Top-1 accuracy. Extensive experiments reveal that the temporal modeling ability of our TSI is verified effectively. Results on Kinetics-400. We also compare with other methods on Kinetics-400 to verify the model robustness in context scene, as shown in Table 2. Compared with all 2D methods, our TSI can obtain better recognition accuracy with limited additional computing cost than TSN[32]. As for 3D methods, we observe that TSI can also surpass most of the mainstream 3D methods, i.e. I3D[2], SlowFast[7], S3D-G[38] and ECO[42]. Note that our TSI achieves better performance with fewer input frames (8 vs. 32/ 64) and less GFLOPs. Although the accuracy of I3D-NL is a bit higher than ours (+1.2%), TSI only has 1/5× GFLOPs (68 vs. 359). Results on UCF-101 and HMDB-51. To verify the generalizability of our TSI network, we pretrain it on Kinetics400 and then fine-tune on UCF-101 and HMDB-51 respectively, the results are shown in the Table 3. Compared with other methods, TSI can obtain consistent better performance. Specifically, we can observe that our proposed TSI network outperforms all the existing state-of-the-art methods with only RGB modality on both datasets, which obtains 97.2% Top-1 classification accuracy on UCF-101 and 76.9% Top-1 classification accuracy on HMDB-51.
4.4. Ablation Studies
In this section, we conduct ablation experiments to evaluate the effectiveness of two proposed modules of TSI net-

Table 1. Performance of the TSI on Something-Something v1 and v2 datasets compared with the state-of-the-art methods.

Method

Backbone

Pretrain

Frames

GFLOPs

Something-Something v1

Top-1

Top-5

Something-Something v2

Top-1

Top-5

S3D-G [38]

Inception

ImageNet 64 × 1 71 × 1 48.2

78.7

-

-

ECO [42] BNIncep+3D Res-18 Kinetics 8 × 1 32 × 1 39.6

-

-

-

I3D [2]

3D ResNet-50

Kinetics 32 × 6 153 × 6 41.6

72.2

-

-

I3D+GCN [34] 3D ResNet-50

Kinetics 32 × 6 168 × 6 43.4

75.1

-

-

TSN [32]

ResNet-50

Kinetics

8×1 16 × 1

33 × 1 65 × 1

19.7 19.9

46.6 47.3

27.8 30.0

57.6 60.5

TRN [40]

BNInception

ImageNet 8 × 1 33 × 1 34.4

-

48.8

77.64

TSM [18]

ResNet-50

ImageNet

8×1 16 × 1

33 × 1 65 × 1

45.6 47.3

74.2 77.1

58.8 61.2

85.4 86.9

STM [14]

ResNet-50

8 × 30 33 × 30 ImageNet 16 × 30 66 × 30

49.2 50.7

79.3 80.4

62.3 64.2

88.8 89.8

8 × 1 33 × 1 47.4

-

61.3

-

TEINet[19]

ResNet-50

ImageNet

8 × 30 16 × 1

33 × 30 66 × 1

48.8 49.9

-

64 62.1

-

16 × 30 66 × 30 51

-

64.7

-

8 × 1 35 × 1 48.9

78.1

-

-

TEA[17]

ResNet-50

ImageNet

8 × 30 16 × 1

35 × 30 70 × 1

51.7 51.9

80.5 80.3

-

-

16 × 30 70 × 30 52.3

81.9

65.1

89.9

8 × 1 34 × 1 50.2

80.1

62.2

88.5

TSI

ResNet-50

ImageNet

8 × 30 16 × 1

34 × 30 68 × 1

53.0 53.3

82.1 82.5

64.9 63.2

89.8 89.1

16 × 30 68 × 30 54.3

83.2

66.1

90.9

Table 2. Comparison with the state-of-the-arts on Kinetics-400. N/A denotes the numbers are not available.
Method Backbone Frames GFLOPs Top-1 Top-5

I3D[2] IncepV1 64 × N/A 108 × N/A 72.1 90.3

I3D-NL[33] R101-3D 32 × 30 359 × 30 77.7 93.3

SlowFast[7] R50-3D (4+32)× 30 36.1 × 30 75.6 92.1

ECOen[42] BNIn+R18 92

267 × 1 70.7 89.4

S3D-G[38] IncepV1 64 × 30 71 × 30 74.7 93.4

TSN[32] IncepV3

R(2+1)D[30] R34-2D

TSM[18]

R50-2D R50-2D

STM[14] R50-2D

TEINet[19]

R50-2D R50-2D

TEA[17]

R50-2D R50-2D

TSI

R50-2D R50-2D

25 × 10 32 × 10 8 × 30 16 × 30 16 × 30 8 × 30 16 × 30 8 × 30 16 × 30 8 × 30 16 × 30

80 × 10 72.5 90.2 152 × 10 72 90 33 × 30 74.1 66 × 30 74.7 67 × 30 73.7 91.6 33 × 30 74.9 91.8 66 × 30 76.2 92.5 35 × 30 75 91.8 70 × 30 76.1 92.5 34 × 30 75.6 92.2 68 × 30 76.5 92.7

work on Something-Something v1. Unless specified, the reported results are calculated using the testing protocol of center crop and one clip with 8 sampled frames.
Study on SME and CTI modules. As shown in Table 4,

we first conduct a separate study on the effect of each individual module (i.e. SME or CTI). In order to evaluate the temporal modeling ability of our proposed method, we adopt the Res50+TIM as our strong baseline method, where TIM indicates the temporal interaction module proposed in [19], which adopts a depth-wise 1D temporal convolution. With the motion-sensitive areas highlighted by the proposed SME, the Top-1 accuracy can be boosted by 3.1%, which reveals the effectiveness and necessity of local-global motion modeling. Instead of using TIM, our proposed CTI can achieve superior temporal modeling performance (+2.3%) through integrating multi-scale temporal information with attention mechanism. Besides, combining CTI with SME, the recognition accuracy can be further improved, which confirm that these two modules are indispensable for long short-term temporal modeling.
Comparison with other temporal modules. We further compare our proposed TSI network with other temporal modules on long short-term temporal modeling, and the results are reported in Table 5. Compared to ME proposed in [17], our proposed SME can extract better motion encoding for short-term temporal modeling. As for the long-term temporal modeling, our proposed CTI can also surpass the existing temporal modules (i.e. TIM[19] and MTA[17] by a great margin. Therefore, we can conclude that the two proposed modules are both effective and collaborative in long

Table 3. Performance of the TSI on UCF-101 and HMDB-51 compared with the state-of-the-art methods.

Method C3D [28] STC [5] ECO [42] I3D RGB [2] I3D two-stream [2]

Backbone 3D VGG-11 ResNet101 BNInception+3D ResNet-18
3D Inception-v1

Flow

Pre-train Data Sports-1M Kinetics Kinetics
ImageNet+Kinetics

UCF-101 82.3 93.7 94.8 95.1 98.0

HMDB-51 51.6 66.8 72.4 74.3 80.7

TSN RGB [32] TSN two-Stream [32] ARTNet with TSN [31]
TSN [32] TSM [18] StNet [12] Disentangling [39] STM[14] TEINet[19] TEA[17] MVFNet[36]
TSI

BNInception
3D ResNet-18 ResNet-50 ResNet-50 ResNet50 BNInception ResNet-50 ResNet-50 ResNet-50 ResNet-50 ResNet-50

ImageNet+Kinetics

91.1 97.0

-

Kinetics

94.3

70.9

ImageNet

86.2

54.7

ImageNet+Kinetics

94.5

70.7

ImageNet+Kinetics

93.5

-

ImageNet+Kinetics

95.9

-

ImageNet+Kinetics

96.2

72.2

ImageNet+Kinetics

96.7

72.1

ImageNet+Kinetics

96.9

73.3

ImageNet+Kinetics

96.6

75.7

ImageNet+Kinetics

97.2

76.9

Table 4. Ablational experiments of SME and CTI of TSI block on Something-Something V1. TIM indicates the temporal interaction module proposed in [19].

Method Res50 + TIM Res50 + TIM+SME Res50 + CTI TSI (Res50 + CTI + SME)

Top-1 46.1 49.2 48.4 50.2

Top-5 74.7 79.1 77.7 80.1

Table 5. Comparison with other temporal modules on SomethingSomething V1 dataset.

Method Res50 + TIM [19] + ME [17]
Res50 + TIM [19] + SME Res50 + TIM [19] Res50 + MTA [17] Res50 + CTI

Top-1 48.4 49.2 46.1 47.5 48.4

Top-5 77.5 79.1 74.7 76.4 77.7

Table 6. Location and number of TSI block. Deeper location and more blocks yeild better performance on Something-Something v1 dataset. 16 frames are sampled as input.

Stage
res2 res3 res4 res5 res2-5 res2-5

TSI Blocks 1 1 1 1 4 16

Top-1 42.7 44.5 45.4 45.5 51.8 53.3

Top-5 72.7 74.4 75.3 75.0 81.2 82.5

short-term temporal modeling.

Table 7. Comparison of different fusion types of two proposed modules on Something-Something v1 dataset. Cascade fusion is better. 16 frames are sampled as input.

Fusion Summation
Cascade

Top-1 51.2 53.3

Top-5 80.3 82.5

Location and number of TSI block. Conventional ResNet-50 architecture can be divided into 6 stages, where the last four stages (refer to res2-5) consist of several residual blocks respectively. In order to ensure the effectiveness of TSI block and the impact of the block number on the performance improvement, we replace the first residual block of each stage with our TSI block respectively, as shown in Table 6. And we can find that significant performance improvement already can be obtained even with only one inserted TSI block, compared to the TSN[32] baseline, which demonstrate the effectiveness of our proposed TSI block. Besides, the inserted block in the latter stage (i.e. res5) can yield better performance than early stage (i.e. res2), owing to the semantic features are more discriminative for temporal modeling and motion excitation in the deep layers. And first TSI block inserted into each stage (4 TSI blocks in total) can be complementary and beneficial for further accuracy improvement. Finally, the best performance can be obtained through replacing all residual blocks with proposed TSI block in res2-5 (16 TSI blocks in total). Fusion of two modules. Spatiotemporal and motion encodings are validated complementary in previous [14]. And two different fusion types are evaluated (e.g. summation and concatenation) respectively, where the first type is chosen finally. However, the motion modeling and temporal model-

Table 8. Efficiency and accuracy comparison of TSI with other

state-of-the-art methods on Something-Something v1 dataset.

"vid/s" indicates number of processing videos per second. Sin-

gle crop TSI beats all competing methods with 44 videos per sec-

ond with 8 frames as input. Measured on a single NVIDIA GTX

1080TI GPU.

Method Frame GFLOPs Throughput Top-1

I3D[2]

64

360

6.1vid/s 41.6%

ECO[42] 16

64

45.6vid/s 41.4%

TSN[32]

8

33

81.5vid/s 19.7%

TSM[18]

8 16

33

77.4vid/s 43.4%

65

39.5vid/s 44.8%

STM[14]

8 16

33

62.0vid/s 47.5%

66.5 32.0vid/s 49.8%

TEINet[19]

8 16

33

46.9vid/s 47.4%

66

24.2vid/s 49.9%

Res50+CTI

8 16

33

61.6vid/s 48.4%

66

31.2vid/s 51.2%

TSI

8

34

44.0vid/s 50.2%

16

68

23.7vid/s 53.3%

ing are optimized independently in this fashion, and the importance of spatiotemporal and motion features would not always be equal in usual scenarios. For example, as shown in Fig. 1, the background information contributes a lot for determining the "golf" action, while the temporal interactions are important for "taking something from somewhere" action prediction. In TSI network, we also try another fusion type, named as cascaded fusion, where the features after Salient Motion Excitation (SME) module are fed to the following Cross-scale Temporal Integration (CTI) module for temporal relationships modeling. In this way, the long short-term temporal modeling are conducted consecutively and gradually. As shown in Table 7, the cascaded fusion type can achieve better performance as expected.
4.5. Runtime Analysis
We continue to verify the efficiency of our method and compare with several state-of-the-art methods on Something-Something v1 dataset. All evaluations are conducted on one GTX 1080TI GPU. Specifically, we adopt the number of videos processed per second as efficiency metric, termed as throughput, as shown in Table 8. For fair comparisons, we evaluate the efficiency of our method by evenly sampling 8 or 16 frames from an input video and then only adopt the center crop for testing. Concretely, compared with existing 3D methods [2], our proposed TSI network can significantly surpass them in both GFLOPs and throughput. Compared with ECO[42], our TSI network can achieve similar throughput with higher accuracy (+8%) and fewer input frames. As for 2D CNNs, though our TSI obtain half throughput than TSN[32], it can obtain 30% accuracy improvement with similar GFLOPs. It should be noted

that our CTI module can achieve remarkable performance improvement with small computing cost. To conclude, our proposed TSI network can strike a certain trade-off in efficiency and recognition accuracy.
5. Conclusion
In this paper, we propose Temporal Saliency Integration (TSI) network for video action recognition, which mainly consists of the Salient Motion Excitation (SME) and the Cross-scale Temporal Integration (CTI) modules. SME is designed to highlight motion-sensitive areas with less background noises, while CTI aims to integrate multiple temporal relationships with attention mechanism. Combining these two modules into a unified convolutional block, long short-term temporal relationships can be effectively captured. Extensive experiments are conducted on several public benchmarks, which demonstrate that our TSI network can achieve remarkable performance improvement with competitive efficiency compared to 2D CNNs.
References
[1] Liangliang Cao, Yadong Mu, Apostol Natsev, Shih-Fu Chang, Gang Hua, and John R Smith. Scene aligned pooling for complex video recognition. In European Conference on Computer Vision, pages 688­701. Springer, 2012. 2
[2] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6299­6308, 2017. 2, 6, 7, 8, 9
[3] Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng Yan, and Jiashi Feng. Multi-fiber networks for video recognition. In Proceedings of the european conference on computer vision (ECCV), pages 352­367, 2018. 1
[4] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248­255. Ieee, 2009. 2, 6
[5] Ali Diba, Mohsen Fayyaz, Vivek Sharma, M Mahdi Arzani, Rahman Yousefzadeh, Juergen Gall, and Luc Van Gool. Spatio-temporal channel correlation networks for action classification. In Proceedings of the European Conference on Computer Vision (ECCV), pages 284­299, 2018. 8
[6] Christoph Feichtenhofer. X3d: Expanding architectures for efficient video recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 203­213, 2020. 1, 2
[7] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. In Proceedings of the IEEE international conference on computer vision, pages 6202­6211, 2019. 6, 7
[8] Christoph Feichtenhofer, Axel Pinz, and Richard P Wildes. Spatiotemporal multiplier networks for video action recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4768­4777, 2017. 2

[9] Christoph Feichtenhofer, Axel Pinz, and Andrew Zisserman. Convolutional two-stream network fusion for video action recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1933­1941, 2016. 2
[10] Basura Fernando, Efstratios Gavves, Jose M Oramas, Amir Ghodrati, and Tinne Tuytelaars. Modeling video evolution for action recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5378­5387, 2015. 2
[11] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The" something something" video database for learning and evaluating visual common sense. In ICCV, volume 1, page 5, 2017. 6
[12] Dongliang He, Zhichao Zhou, Chuang Gan, Fu Li, Xiao Liu, Yandong Li, Limin Wang, and Shilei Wen. Stnet: Local and global spatial-temporal modeling for action recognition. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 8401­8408, 2019. 8
[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770­778, 2016. 6
[14] Boyuan Jiang, MengMeng Wang, Weihao Gan, Wei Wu, and Junjie Yan. Stm: Spatiotemporal and motion encoding for action recognition. In Proceedings of the IEEE International Conference on Computer Vision, pages 2000­2009, 2019. 1, 2, 3, 4, 5, 6, 7, 8, 9
[15] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. 2017. 6
[16] Hildegard Kuehne, Hueihan Jhuang, Est´ibaliz Garrote, Tomaso Poggio, and Thomas Serre. Hmdb: a large video database for human motion recognition. In 2011 International Conference on Computer Vision, pages 2556­2563. IEEE, 2011. 6
[17] Yan Li, Bin Ji, Xintian Shi, Jianguo Zhang, Bin Kang, and Limin Wang. Tea: Temporal excitation and aggregation for action recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 909­918, 2020. 2, 3, 4, 5, 6, 7, 8
[18] Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shift module for efficient video understanding. In Proceedings of the IEEE International Conference on Computer Vision, pages 7083­7093, 2019. 2, 7, 8, 9
[19] Zhaoyang Liu, Donghao Luo, Yabiao Wang, Limin Wang, Ying Tai, Chengjie Wang, Jilin Li, Feiyue Huang, and Tong Lu. Teinet: Towards an efficient architecture for video recognition. In AAAI, pages 11669­11676, 2020. 2, 5, 6, 7, 8, 9
[20] Hao Shao, Shengju Qian, and Yu Liu. Temporal interlacing network. arXiv preprint arXiv:2001.06499, 2020. 2
[21] Karen Simonyan and Andrew Zisserman. Two-stream convolutional networks for action recognition in videos. In Advances in neural information processing systems, pages 568­ 576, 2014. 1

[22] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. 6
[23] Haisheng Su, Weihao Gan, Wei Wu, Junjie Yan, and Yu Qiao. Bsn++: Complementary boundary regressor with scale-balanced relation modeling for temporal action proposal generation. arXiv preprint arXiv:2009.07641, 2020. 1
[24] Haisheng Su, Jing Su, Dongliang Wang, Weihao Gan, Wei Wu, Mengmeng Wang, Junjie Yan, and Yu Qiao. Collaborative distillation in the parameter and spectrum domains for video action recognition. arXiv preprint arXiv:2009.06902, 2020. 1
[25] Haisheng Su, Xu Zhao, and Tianwei Lin. Cascaded pyramid mining network for weakly supervised temporal action localization. In Asian Conference on Computer Vision, pages 558­574. Springer, 2018. 1
[26] Haisheng Su, Xu Zhao, Tianwei Lin, Shuming Liu, and Zhilan Hu. Transferable knowledge-based multi-granularity fusion network for weakly supervised temporal action detection. IEEE Transactions on Multimedia, 2020. 1
[27] Swathikiran Sudhakaran, Sergio Escalera, and Oswald Lanz. Gate-shift networks for video action recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1102­1111, 2020. 1
[28] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spatiotemporal features with 3d convolutional networks. In Proceedings of the IEEE international conference on computer vision, pages 4489­4497, 2015. 1, 2, 8
[29] Du Tran, Heng Wang, Lorenzo Torresani, and Matt Feiszli. Video classification with channel-separated convolutional networks. In Proceedings of the IEEE International Conference on Computer Vision, pages 5552­5561, 2019. 1, 2
[30] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A closer look at spatiotemporal convolutions for action recognition. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 6450­6459, 2018. 1, 2, 7
[31] Limin Wang, Wei Li, Wen Li, and Luc Van Gool. Appearance-and-relation networks for video classification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1430­1439, 2018. 8
[32] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment networks: Towards good practices for deep action recognition. In European conference on computer vision, pages 20­36. Springer, 2016. 1, 2, 5, 6, 7, 8, 9
[33] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7794­7803, 2018. 7
[34] Xiaolong Wang and Abhinav Gupta. Videos as space-time region graphs. In Proceedings of the European conference on computer vision (ECCV), pages 399­417, 2018. 7
[35] Chao-Yuan Wu, Manzil Zaheer, Hexiang Hu, R Manmatha, Alexander J Smola, and Philipp Kra¨henbu¨hl. Compressed

video action recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6026­6035, 2018. 2
[36] Wenhao Wu, Dongliang He, Tianwei Lin, Fu Li, Chuang Gan, and Errui Ding. Mv: Multi-view fusion network for efficient video recognition. arXiv preprint arXiv:2012.06977, 2020. 8
[37] Zuxuan Wu, Caiming Xiong, Chih-Yao Ma, Richard Socher, and Larry S Davis. Adaframe: Adaptive frame selection for fast video recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1278­1287, 2019. 2
[38] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy. Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification. In Proceedings of the European Conference on Computer Vision (ECCV), pages 305­321, 2018. 6, 7
[39] Yue Zhao, Yuanjun Xiong, and Dahua Lin. Recognize actions by disentangling components of dynamics. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6566­6575, 2018. 8
[40] Bolei Zhou, Alex Andonian, Aude Oliva, and Antonio Torralba. Temporal relational reasoning in videos. In Proceedings of the European Conference on Computer Vision (ECCV), pages 803­818, 2018. 2, 7
[41] Xizhou Zhu, Yuwen Xiong, Jifeng Dai, Lu Yuan, and Yichen Wei. Deep feature flow for video recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2349­2358, 2017. 2
[42] Mohammadreza Zolfaghari, Kamaljeet Singh, and Thomas Brox. Eco: Efficient convolutional network for online video understanding. In Proceedings of the European conference on computer vision (ECCV), pages 695­712, 2018. 6, 7, 8, 9

