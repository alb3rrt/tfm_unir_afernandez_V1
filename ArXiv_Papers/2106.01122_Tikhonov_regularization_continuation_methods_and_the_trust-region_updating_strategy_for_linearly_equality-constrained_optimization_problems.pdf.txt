arXiv:2106.01122v1 [math.NA] 2 Jun 2021

Journal of XXX manuscript No. (will be inserted by the editor)
Tikhonov regularization continuation methods and the trust-region updating strategy for linearly equality-constrained optimization problems
Xin-long Luo · Hang Xiao
Received: date / Accepted: date
Abstract This paper considers the Tikhonv regularization continuation method and the trust-region updating strategy for the linearly equality-constrained optimization problem (Trcmtr). Moreover, in order to improve its computational efficiency and robustness, the new method uses the switching preconditioned technique. That is to say, the new method uses the L-BFGS method as the preconditioned technique to improve its computational efficiency in the well-posed phase. Otherwise, it uses the inverse of the regularization two-sided projected Hessian matrix as the pre-conditioner to improve its robustness. Numerical results also show that the new method is more robust and faster than the traditional optimization method such as the sequential quadratic programming (SQP), the alternating direction method of multipliers (ADMM) and the latest continuation method (Ptctr). The computational time of the new method is about one fifth of that of SQP for the large-scale problem. Finally, the global convergence analysis of the new method is also given.
Keywords continuation method · preconditioned technique · trust-region method · Tikhonov regularization
Mathematics Subject Classification (2010) 65K05 · 65L05 · 65L20
Xin-long Luo Corresponding author. School of Artificial Intelligence, Beijing University of Posts and Telecommunications, P. O. Box 101, Xitucheng Road No. 10, Haidian District, 100876, Beijing China E-mail: luoxinlong@bupt.edu.cn Hang Xiao School of Artificial Intelligence, Beijing University of Posts and Telecommunications, P. O. Box 101, Xitucheng Road No. 10, Haidian District, 100876, Beijing China E-mail: xiaohang0210@bupt.edu.cn

2

Luo, Xiao

1 Introduction

In this article, we consider the following linearly equality-constrained optimization problem

min f (x)
xn

subject to Ax = b,

(1)

where A  m×n is a matrix and b  m is a vector. This problem has many applications in engineering fields such as the visual-inertial navigation of an unmanned aerial vehicle maintaining the horizontal flight [10, 35], and there are many practical methods to solve it such as the sequential quadratic programming (SQP) method [27, 46], the penalty function method [14].
For the constrained optimization problem (1), the continuation method [2, 11, 17, 25, 48, 56] is another method other than the traditional optimization method such as SQP or the penalty function method. The advantage of the continuation method over the SQP method is that the continuation method is capable of finding many local optimal points of the non-convex optimization problem by tracking its trajectory, and it is even possible to find the global optimal solution [5, 50, 61]. However, the computational efficiency of the classical continuation methods is lower than that of the traditional optimization method such as SQP. Recently, Luo et al [35] give a continuation method with the trusty time-stepping scheme for the problem (1), which is faster than SQP and the penalty method. In order to improve the computational efficiency of the continuation method for the large-scale optimization problem further, we consider a special limited-memory BFGS (L-BFGS) preconditioned technique for the Tikhonov regularization continuation method and use the adaptively time-stepping scheme based on the trust-region updating strategy in this article. Moreover, in order to improve the robust of the Tikhonov regularization continuation method, we replace the L-BFGS preconditioner with the inverse of the regularization two-sided projected Hessian matrix in the ill-posed phase.
The rest of the paper is organized as follows. In section 2, we give the Tikhonov regularization continuation method with the switching preconditioned technique and the trust-region updating strategy for the problem (1). In section 3, we analyze the global convergence of this new method. In section 4, we report some promising numerical results of the new method, in comparison to the traditional optimization method (SQP, the built-in subroutine fmincon.m of the MATLAB2020a [43]), the latest continuation method (Ptctr [35]) and the alternating direction method of multipliers (ADMM [8], only for convex problems) for some large-scale problems. Finally, we give some discussions and conclusions in section 5.

2 Tikhonov regularization continuation methods
In this section, we give the Tikhonov regularization continuation method with the switching preconditioned technique and the adaptive time-stepping scheme based on

Tikhonov regularization continuation methods

3

the trust-region updating strategy [12] for the linearly equality-constrained optimization problem (1). Firstly, we consider the Tikhonov regularization projected gradient flow based on the KKT conditions of linearly constrained optimization problem. Then, we give the Tikhonov regularization continuation method with the trust-region updating strategy to follow this special ordinary differential equations (ODEs). In order to improve the computational efficiency and the robustness of the new method, we also use the L-BFGS updating method as the preconditioned technique in the wellposed phase. Otherwise, we use the inverse of the regularization two-sided projected Hessian matrix as the pre-conditioner in the ill-posed phase. Furthermore, we give a preprocessing method for the infeasible initial point.

2.1 The Tikhonov regularization projected Newton flow

For the linearly constrained optimization problem (1), it is well known that its optimal solution x needs to satisfy the Karush-Kuhn-Tucker conditions (p. 328, [46]) as
follows:

xL(x,  ) =  f (x) + AT  = 0,

(2)

Ax - b = 0,

(3)

where the Lagrangian function L(x,  ) is defined by

L(x,  ) = f (x) +  T (Ax - b).

(4)

Similarly to the method of the negative gradient flow for the unconstrained optimization problem [22], from the first-order necessary conditions (2)-(3), we can construct a dynamical system of differential-algebraic equations for problem (1) [13, 32, 33, 34, 51] as follows:

dx dt = -Lx(x,  ) = -

 f (x) + AT 

,

(5)

Ax - b = 0.

(6)

By differentiating the algebraic constraint (6) with respect to t and substituting it into the differential equation (5), we obtain

dx A = -A
dt

 f (x) + AT 

= -A f (x) - AAT  = 0.

(7)

If we assume that matrix A has full row rank further, from equation (7), we obtain

 = - AAT -1 A f (x).

(8)

By substituting  of equation (8) into equation (5), we obtain the projected gradient flow [56] for the constrained optimization problem (1) as follows:

dx =-

I - AT

AAT

-1 A

 f (x) = -Pg(x),

(9)

dt

4

Luo, Xiao

where g(x) =  f (x) and the projection matrix P is defined by

P = I - AT AAT -1 A.

(10)

It is not difficult to verify P2 = P. That is to say, the projection matrix P is symmetric and its eigenvalues are zero or one. From Theorem 2.3.1 in p. 73 of [19], we know that its matrix 2-norm is

P = 1.

(11)

We denote P+ as the Moore-Penrose generalized inverse of the projection matrix P (p. 11, [55]). Since the projection matrix P is symmetric and P2 = P, it is not difficult
to verify

P+ = P.

(12)

Actually, from equation (12), we have P(P)+P = P(P)P = P2P = P2 = P. Thus, for a full rank matrix B  n×n, we obtain the generalized inverse (PB)+ of PB as follows:

(PB)+ = B+P+ = B-1P.

(13)

Furthermore, from equation (10), we have AP = 0. We denote N (A) as the null space of A. Since the rank of A is m, we know that the rank of N (A) equals n - m and there are n - m linearly independent vectors xi (i = 1, . . . , n - m) to satisfy Axi = 0 (i = 1, . . . , n - m). From equation (10), we know that these n - m linearly independent vectors xi (i = 1, . . . , n - m) satisfy Pxi = xi (i = 1, . . . , n - m). That is to say, the projection matrix P has n - m linearly independent eigenvectors of one. Consequently, the rank of P is at least n - m. By combining it with AP = 0, we know that P spans the null space of A.

Remark 1 If x(t) is the solution of the ODE (9), it is not difficult to verify that x(t)
satisfies A(dx/dt) = 0. That is to say, if the initial point x0 satisfies Ax0 = b, the solution x(t) of the generalized projected gradient flow (9) also satisfies Ax(t) = b, t  0.
This property is very useful when we construct a structure-preserving algorithm [20] to follow the trajectory of the ODE (9) and obtain its steady-state solution x.

If we assume that x(t) is the solution of the ODEs (9), by using the property P2 = P, we obtain

d f (x) dt

= ( f (x))T

dx dt

= -( f (x))T P f (x) = -g(x)T P2g(x) = -

Pg(x)

2  0.

That is to say, f (x) is monotonically decreasing along the solution curve x(t) of the dynamical system (9). Furthermore, the solution x(t) converges to x when f (x) is lower bounded and t tends to infinity [22, 50, 56], where x satisfies the first-order
Karush-Kuhn-Tucker conditions (2)-(3). Thus, we can follow the trajectory x(t) of the ODE (9) to obtain its steady-state solution x, which is also one stationary point
of the original optimization problem (1).

Tikhonov regularization continuation methods

5

However, since the right-hand-side function Pg(·) of the ODE (9) is rank-deficient, we will confront the numerical difficulties when we use the explicit ODE method to follow the projected gradient flow (9) [3, 6, 7]. In order to mitigate the stiffness of the ODE (9), we can use the generalized inverse (P2 f (x))+ of the Jacobian matrix Pg(x) as the preconditioner for the ODE (9), which is used similarly to the system of nonlinear equations [36], the unconstrained optimization problem [22, 38], the linear programming problem [37] and the underdetermined system of nonlinear equations [39].

Firstly, we integrate the ODE (9) from zero to t, then we obtain

t

t

x(t) = x(t0) - Pg(x())d = x(t0) - P g(x())d.

(14)

0

0

Thus, if we denote z(t) = -

t 0

g(x(

))d

,

from

equation

(14),

we

have

x(t) = x(t0) + Pz(t).

(15)

By substituting it into the ODE (9), we obtain

dz(t)

P dt

= -Pg(x(t0) + Pz(t)).

(16)

Then, by using the generalized inverse P2 f (x(t0) + Pz) + of the Jacobian matrix P2 f (x(t0) + Pz)P as the preconditioner for the ODE (16), we have

dz(t) P =-
dt

P2 f (x(t0) + Pz)P

+ Pg(x(t0) + Pz(t)).

(17)

We reformulate equation (17) as

P2 f (x(t0) + Pz)P

dPz(t) dt = -Pg(x(t0) + Pz(t)).

(18)

By substituting Pz(t) = x(t) - x(t0) into equation (18), we obtain the projected Newton flow for problem (1) as follows:

P2 f (x)P

dx(t) = -Pg(x).
dt

(19)

Although the projected Newton flow (19) mitigates the stiffness of the ODE such that we can adopt the quasi-explicit ODE method to integrate it on the infinite interval, there are two disadvantages. One is that the two-side projected Hessian matrix P2 f (x)P may be not positive. Consequently, it can not ensure the objective function f (x) is monotonically decreasing along the solution x(t) of the ODE (19). The other is that the solution x(t) of the ODE (19) can not ensure to preserve the linear conservation law, i.e., Adx(t)/dt = 0. In order to remedy these two problems, we use the Tikhonov regularization technique [21, 57, 58] for the projected Newton flow (19) as follows:

 (x)I + P2 f (x)P

dx(t) = -Pg(x),
dt

(20)

6

Luo, Xiao

where the scale regularization function  (x) satisfies  (x) + µmin P2 f (x)P  min > 0 and µmin P2 f (x)P represents the smallest eigenvalue of P2 f (x)P.

Remark 2 If we assume that x(t) is the solution of the ODE (20), from the property AP = 0, we have

A

 (x)I + P2 f (x)P

dx(t) = -APg(x) = 0.
dt

Consequently, we obtain A (x)dx(t)/dt = 0. By integrating it, we obtain Ax(t) = Ax(t0) = b. That is to say, the solution x(t) of the ODE (20) preserves the linear conservation law Ax = b.

Remark 3 From Remark 2, we know that the solution x(t) of the ODE (20) satisfies
Ax(t) = b. Furthermore, P is the null space of A. By combining these properties, we
obtain x(t) = x0 + Pz(t). Thus, we have dx(t)/dt = Pdz(t)/dt. By substituting the property P2 = P into it, we obtain Pdx(t)/dt = P2dz(t)/dt = Pz(t)/dt = dx(t)/dt. Consequently, from equation (20) and the assumption  (x) + min P2 f (x)P  min > 0, we obtain

d f (x(t)) = ( f (x))T dx(t) = ( f (x))T P dx(t) = (Pg(x))T dx(t)

dt

dt

dt

dt

= -(Pg(x))T  (x)I + PT 2 f (x)P -1 (Pg(x))  0.

That is to say, the objective function f (·) is monotonically decreasing along the solution x(t) of the ODE (20). Furthermore, the solution x(t) converges to x when f (x) is lower bounded and P2 f (x)P  M [22, 29, 50, 56], where M is a positive constant and x is the stationary point of the Tikhonov regularization projected Newton flow
(20). Thus, we can follow the trajectory x(t) of the ODE (20) to obtain its stationary point x, which is also one stationary point of the original optimization problem (1).

2.2 The Tikhonov regularization continuation method

The solution curve x(t) of the ODE (20) can not be efficiently followed on an infinite interval by the general ODE method such as backward differentiation formulas (BDFs, the subroutine ode15s.m of the MATLAB environment) [3, 6, 7, 24]. Thus, we need to construct the particular method for this problem (20). We apply the first-order explicit Euler method [52] to the ODE (20), then we obtain the Tikhonov regularization projected Newton method:

kI + P2 f (xk)P dk = -Pg(xk),

(21)

xk+1 = xk + kdk,

(22)

where k is the time step. If we let k = 1, the Tikhonov regularization projected Newton method is the Levenberg-Marquardt method [28, 31, 44].

Since the time step k of the Tikhonov projected Newton method (21)-(22) is restricted by the numerical stability [52]. That is to say, for the linear test equation

Tikhonov regularization continuation methods

7

dx/dt = - x, its time step size k is restricted by the stable region |1 -  k|  1. Therefore, the large time step can not be adopted in the steady-state phase. In order to avoid this disadvantage, similarly to the processing technique of the nonlinear equations [36, 39, 37] and the unconstrained optimization problem [38], we replace k with tk/(1 + tk) in equation (22) and let k = 1/tk in equation (21). Then, we obtain the Tikhonov regularization continuation method:

Bk dk

=

-Pg(xk),

sk

=

1

 tk + tk

dk,

(23)

xk+1 = xk + sk,

(24)

where tk is the time step and Bk = I/tk + P2 f (xk)P or its quasi-Newton approximation.

Remark 4 The time step tk of the Tikhonov regularization continuation method (23)-(24) is not restricted by the numerical stability. Therefore, the large time step
tk can be adopted in the steady-state phase such that the Tikhonov regularization continuation method (23)-(24) mimics the projected Newton method near the stationary point x and it has the fast local convergence rate. The most of all, the new
step size k = tk/(tk + 1) is favourable to adopt the trust-region updating strategy to adjust the time step tk such that the Tikhonov regularization continuation method (23)-(24) accurately follows the trajectory of the Tikhonov regularization flow (20)
in the transient-state phase and achieves the fast convergence rate near the stationary point x.

Remark 5 From equations (23)-(24) and the property AP = 0 of the projected matrix P, it is not difficult to verify Ask = 0. Thus, if the initial point x0 is feasible, i.e. Ax0 = b, xk also satisfies the linear constraint Axk = b. That is to say, the Tikhonov regularization continuation method (23)-(24) is a structure-preserving method.

2.3 The trust-region updating strategy

Another issue is how to adaptively adjust the time step size tk at every iteration. We borrow the adjustment technique of the trust-region radius from the trust-region method due to its robustness and its fast convergence rate [12, 62]. According to the structure-preserving property of the Tikhonov regularization continuation method (23)-(24), xk+1 will preserve the feasibility if Axk = b. That is to say, xk+1 satisfies Axk+1 = b. Therefore, we use the objective function f (x) instead of the nonsmooth penalty function f (x) +  Ax - b as the merit function. Similarly to the steppingtime scheme of the ODE method for the unconstrained optimization [23, 31, 35], we also need to construct a local approximation model of f (x) around xk. Here, we adopt the following quadratic function as its approximation model:

qk(xk

+ s)

=

f (xk) + sT gk

+

1 2

sT

Bk

s,

(25)

where gk =  f (xk) and Bk = I/tk + P2 f (xk)P or its quasi-Newton approximation.

8

Luo, Xiao

In practical computation, we may not store the matrix Bk. Thus, from the Tikhonov regularization continuation method (23)-(24), we simplify the quadratic model qk(xk + sk) - q(xk) as follows:

mk(sk)

=

gTk sk

-

0.5 tk 1 + tk

gTk sk

=

1 + 0.5tk 1 + tk

gTk sk



qk(xk

+ sk) - qk(xk).

(26)

We enlarge or reduce the time step tk at every iteration according to the following ratio:

k

=

f (xk) - f (xk + sk) . mk(0) - mk(sk)

(27)

A particular adjustment strategy is given as follows:

 1tk, if 0  |1 - k|  1, 

tk+1 = tk, else if 1 < |1 - k| < 2,

(28)

 2

tk

,

others,

where the constants are selected as 1 = 0.25, 1 = 2, 2 = 0.75, 2 = 0.5 according to our numerical experiments. We accept the trial step sk and let xk+1 = xk + sk, when k  a and the approximation model mk(0) - mk(sk) satisfies the Armijo sufficient decrease condition:

mk(0) - mk(sk)  m sk pgk ,

(29)

where a and m are the small positive constants such as a = m = 1.0 × 10-6. Otherwise, we discard it and let xk+1 = xk.

Remark 6 This new time-stepping scheme based on the trust-region updating strategy has some advantages compared to the traditional line search strategy [30]. If we use the line search strategy and the damped projected Newton method (21)-(22) to follow the trajectory x(t) of the projected Newton flow (20), in order to achieve the fast convergence rate in the steady-state phase, the time step size k of the damped projected Newton method is tried from 1 and reduced by half with many times at every iteration. Since the linear model f (xk) + gTk sk may not approximate f (xk + sk) well in the transient-state phase, the time step k will be small. Consequently, the line search strategy consumes the unnecessary trial steps in the transient-state phase. However, the selection scheme of the time step based on the trust-region strategy (27)-(28) can overcome this shortcoming.

2.4 The switching preconditioned technique

For the large-scale problem, the numerical evaluation of the projected Hessian matrix P2 f (xk)P consumes much time. In order to overcome this shortcoming, in the well-
posed phase, we use the limited memory BFGS quasi-Newton formula (see [4, 15, 18,

41, 53] or pp. 222-230, [46]) to approximate the regularized projected Hessian matrix

1  tk

I

+

P2

f

(xk

)P

of the Tikhonov regularization continuation method (23)-(24).

Tikhonov regularization continuation methods

9

Recently, Ullah, Sabi'u and Shah [59] give an efficient L-BFGS updating for-
mula for the system of monotone nonlinear equations. Furthermore, Luo, Xiao, Lv
and Zhang [38] also test its efficiency for some unconstrained optimization problems. Therefore, we adopt the L-BFGS updating formula to approximate I/tk + P2 f (xk)P in the well-posed phase via slightly revising it as

Bk+1

=

 I

-

sk sTk sTk sk

+

yk yTk yTk yk

,

if

sTk yk

>

sk

2,

(30)

I, otherwise,

where sk = xk+1 - xk, yk = P f (xk+1) - P f (xk) and  is a small positive constant such as  = 10-6.

By using the Sherman-Morrison-Woodburg formula (P. 17, [55]), from equation (30), when yTk sk =  sk 2, we obtain the inverse of Bk+1 as follows:

B-k+11

=I-

yksTk + skyTk yTk sk

+

2

yTk yk (yTk sk)2

sk

sTk

.

(31)

The initial matrix B0 can be simply selected as an identity matrix. From equation (31), it is not difficult to verify

Bk+1sk

=

yTk sk yTk yk

yk.

That is to say, Bk+1 satisfies the scaling quasi-Newton property.

The L-BFGS updating formula (30) has some nice properties such as the symmetric positive definite property and the positive lower bound of its eigenvalues.

Lemma 1 When sTk yk >  sk 2, Bk+1 is symmetric positive definite and its eigen-

values are greater than  2 sk 2 / 2 yk 2 and less than 2. Consequently, when

sTk yk

>

sk

2,

the

eigenvalues

of

B-k+11

are

greater

than

1/2

and

less

than

2 2

yk sk

2 2

.

Proof. (i) For any nonzero vector z  n, from equation (30), we have

zT Bk+1z =

z 2-

zT sk 2 sk 2

+

zT yk 2 yk 2



zT yk 2 yk 2



0.

(32)

In the first inequality of (32), we use the Cauchy-Schwartz inequality zT sk 
z sk and its equality holds if only if z = tsk. Therefore, Bk+1 is symmetric semipositive definite. When z = tsk, since sTk yk = 0, from equation (32), we have zT Bk+1z = t2 sTk yk 2/ yk 2 > 0. Consequently, Bk+1 is symmetric positive definite when sTk yk = 0.

(ii) It is not difficult to know that there exist at least n - 2 linearly independent vectors z1, z2, . . . , zn-2 to satisfy zTi sk = 0, zTi yk = 0 (i = 1 : (n - 2)). That is to say, matrix Bk+1 defined by equation (30) has at least (n - 2) linearly independent
eigenvectors associated with eigenvalues of 1. We denote the other two eigenvalues

10

Luo, Xiao

of

Bk+1

as

µik+1 (i

=

1

:

2).

We

denote

tr(C)

=

n
i=1

cii

,

C



n×n.

Then,

we

have

tr(Bk+1) = ni=1 µik+1 = µ1k+1 + µ2k+1 + (n - 2). By substituting it into equation

(30), we obtain

µ1k+1 + µ2k+1 = tr(Bk+1) - (n - 2)

= tr(I) - tr

sk sTk sTk sk

+ tr

yk yTk yTk yk

- (n - 2) = 2,

(33)

where we use the property tr ABT = tr BT A of matrices A, B  m×n. Since matrix
Bk+1 is symmetric semi-positive definite, we know that its eigenvalues are greater than or equal to 0, namely µik+1  0 (i = 1, 2). By substituting it into equation (33), we obtain

µik+1  µ1k+1 + µ2k+1 = 2, i = 1, 2.

(34)

Furthermore, the symmetric matrix Bk+1 has other (n - 2) eigenvalues of 1. Therefore, by combining it with equation (34), we know that the eigenvalues of matrix Bk+1 are less than or equal to 2.
We denote µik+1 (i = 1 : n) as the eigenvalues of Bk+1. Then, we have µik+1 = 1 (i = 3 : n). By using the property det(Bk+1) = ni µik+1 = µ1k+1µ2k+1, from equation (30), we obtain

µ1k+1µ2k+1 = det(Bk+1) = det

I

+

yk yTk yTk yk

I-

I

+

yk yTk yTk yk

-1
sk

sTk sTk sk

= det

I

+

yk yTk yTk yk

det

I-

I

+

yk yTk yTk yk

-1
sk

sTk sTk sk

=2

1-

1 sk

2 sTk

I

+

yk yTk yTk yk

-1
sk

=2

1-

1 sk

2 sTk

I

-

yk yTk 2yTk yk

sk

=

sTk yk 2 yTk yk sTk sk

.

(35)

From equation (34), we know µik  2 (i = 1, 2). By substituting it into equation (35), we obtain

µik+1



1 2

sTk yk 2 sk 2 yk

2,

i

=

1,

2.

(36)

By combining it with µik+1 = 1 (i = 3 : n), we have

µik+1  min

1, 1 2

sTk yk 2 sk 2 yk 2

=1 2

sTk yk 2 sk 2 yk

2



12 2

sk yk

2
2,

(37)

where we use the Cauchy-Schwartz inequality |sTk yk|  sk yk .

Tikhonov regularization continuation methods

11

Since the matrix Bk+1 is symmetric positive definite when sTk yk >  sk 2, the inverse of Bk+1 exists. Furthermore, the eigenvalues of B-k+11 equal 1/µi(Bk+1) (i = 1 : n), where µi(C) represents the i-th eigenvalue of matrix C  n×n. Therefore, by combining it with equations (34) and (37), we know that the eigenvalues of B-k+11 are greater than 1/2 and less than 2 yk 2 /  2 sk 2 when sTk yk >  sk 2.
According to our numerical experiments[38, 40], the L-BFGS updating formula (30) works well for most problems of unconstrained optimization and linearly equalityconstrained optimization, and the objective function decreases very fast in the wellposed phase. However, for the ill-posed problems, the L-BFGS updating formula (30) will approach the stationary solution x very slow in the ill-posed phase. Furthermore, it fails to get close to the stationary solution x sometimes.

In order to improve the robustness of the Tikhonov regularization continuation method (23)-(24), we adopt the inverse B-k+11 of the regularization two-side projected Hessian matrix as the preconditioner in the ill-posed phase, where Bk+1 is defined by

Bk+1

=

1 I
 tk+1

+ P2

f (xk+1)P.

(38)

Now, the problem is how to automatically identify the ill-posed phase and change to the inverse of the regularization two-side projected Hessian matrix from the L-BFGS updating formula (30). Here, we adopt the simple criterion. That is to say, we regard that the Tikhonov regularization continuation method (23)-(24) is in the ill-posed phase once there exists the time step tK  10-3.

In the ill-posed phase, the computational time of the projected Hessian matrix

is heavy if we update the projected Hessian matrix P2 f (xk)P at every iteration.

In order to save the computational time of the projected Hessian evaluation, we

set Bk+1 = Bk when mk(0) - mk(sk) approximates f (xk) - f (xk + sk) well, where

the approximation model mk(sk) is defined by equation (26). Otherwise, we update

Bk+1 =

1  tk

I

+

P2

f

(xk+1

)P

in the ill-posed phase. In the ill-posed phase, a prac-

tice updating strategy is give by

Bk+1 =

Bk, if |1 - k|  1,

1  tk

I

+

P2

f

(xk+1)P,

otherwise,

(39)

where k is defined by equations (26)-(27) and 1 = 0.25.
For a real-world problem, the analytical Hessian matrix 2 f (xk) may not be offered. Thus, in practice, we replace the projected Hessian matrix P2 f (xk)P with its difference approximation as follows:

P2 f (xk)P  Pg(xk + Pe1) - Pg(xk) , . . . , Pg(xk + Pen) - Pg(xk) , (40)





where ei represents the unit vector whose elements equal zeros except for the i-th element which equals 1, and the parameter  can be selected as 10-6 according to
our numerical experiments.

12

Luo, Xiao

2.5 The treatment of rank-deficient problems and infeasible initial points

For a real-world problem, matrix A may be deficient-rank. We assume that the rank of A is r and we use the QR decomposition (pp.276-278, [19]) to factor AT into a product of an orthogonal matrix Q  n×n and an upper triangular matrix R  n×m
as follows:

AT E = QR = Q1|Q2

R1 , 0

(41)

where E  m×m is a permutation, R1 = R(1 : r, 1 : m) is upper triangular and its
diagonal elements are non-zero, and Q1 = Q(1 : n, 1 : r), Q2 = Q(1 : n, (r + 1) : n) satisfy QT1 Q1 = I, QT2 Q2 = I and QT1 Q2 = 0. Then, we reduce the linear constraints Ax = b as

QT1 x = br,

(42)

where br = R1RT1 -1 R1 ET b . From equations (10) and (42), we simplify the projection matrix P as

P = I - Q1QT1 = Q2QT2 .

(43)

In practical computation, we adopt the different formulas of the projection matrix P according to r  n/2 or r > n/2. Thus, we give the computational formula of the projected gradient Pgk as follows:

Pgk =

gk - Q1

QT1 gk

,

if

r



1 2

n,

Q2 QT2 gk , otherwise.

(44)

where r is the number of columns of Q1, i.e. the rank of A.
For a real-world optimization problem (1), we probably meet the infeasible initial point x0. That is to say, the initial point can not satisfy the constraints Ax = b. We handle this problem by solving the following projection problem:

min
xn

x - x0 2 subject to QT1 x = br,

(45)

where br = R1RT1 -1 R1 ET b . By using the Lagrangian multiplier method to solve problem (45), we obtain the initial feasible point x0F of problem (1) as follows:

x0F = x0 - Q1 QT1 x0 - br .

(46)

For convenience, we set x0 = x0F in line 4, Algorithm 1.
According to the above discussions, we give the detailed implementation of the Tikhonov regularization continuation method and the trust-region updating strategy for the linearly equality-constrained optimization problem (1) in Algorithm 1.

Tikhonov regularization continuation methods

13

Algorithm 1 Tikhonov regularization continuation methods and the trust-region up-

dating strategy for linearly constrained optimization problems (the Trcmtr method)

Input: the objective function f : n  , the linear constraints Ax = b, A  m×n, b  m, the initial
point x0 (optional), the terminated parameter  (optional). Output: the optimal approximation solution x. 1: If x0 or  is not provided, then we set x0 = ones(n, 1) or  = 10-6. 2: Initialize the parameters: a = 10-6, m = 10-10, 1 = 0.25, 1 = 2, 2 = 0.75, 2 = 0.5,  = 10-6,
max itc = 300. Set t0 = 10-2, flag illposed phase = 0, flag success trialstep = 1, sk-1 = 0, yk-1 =
0, k-1 = 0, Bk-1 = I, Hk-1 = I, Rk-1 = I, itc = 0. 3: Factorize matrix AT such that AT E = Q1R1 with the QR decomposition (41). Compute br =
R1RT1 -1 R1 ET b . 4: Compute
x0  x0 - Q1 QT1 x0 - br ,

such that x0 satisfies the linear constraints Ax0 = b.

5: Set k = 0. Evaluate f0 = f (x0) and g0 =  f (x0).

6: Compute the projected gradient pg0 = Pg0 according to the formula (44). 7: while pgk >  &(itc < max itc) do
8: itc = itc + 1;

9: if tk < 10-3 then

10:

Set flag illposed phase = 1.

11: end if

12: if (flag illposed phase == 0) then

13:

if (flag success trialstep == 1) then

14:

if |sTk-1yk-1| >  sk-1 2 then

15:

dk = -

pgk

-

yk-1(sTk-1 pgk )+sk-1(yTk-1 pgk ) yTk-1 sk-1

+2

yk-1 2(sTk-1 pgk (yTk-1 sk-1 )2

)

sk-1

.

16:

else

17:

dk = -pgk .

18:

end if

19:

end if

20: else

21:

if (flag success trialstep == 1) then

22:

if (|k-1 - 1| > 0.25) then

23:

Evaluate Hk = P2 f (xk)P from equation (40).

24:

Set Bk = (1/tk)I + Hk and decompose Bk = QkRk with the QR decomposition.

25:

else

26:

Qk = Qk-1, Rk = Rk-1;

27:

end if

28:

dk = -R-k 1 QTk pgk .

29:

end if

30: end if

31:

Compute

sk

=

 tk 1+ tk

dk

and

xk+1

=

xk + sk.

32: Evaluate fk+1 = f (xk+1) and compute the ratio k from equations (26)-(27).

33: if k  a and sk satisfies the sufficient decrease condition (29) then

34:

Set flag success trialstep = 1 and evaluate gk+1 =  f (xk+1).

35:

Compute pgk+1 = Pgk+1 according to the formula (44). Set yk = pgk+1 - pgk .

36: else

37:

Set flag success trialstep = 0, xk+1 = xk, fk+1 = fk, pgk+1 = pgk , gk+1 = gk, dk+1 = dk.

38: end if

39: Adjust the time-stepping size tk+1 based on the trust-region updating strategy (28).

40: Set k  k + 1.

41: end while

14

Luo, Xiao

3 Algorithm Analysis

In this section, we analyze the global convergence of the Tikhonov regularization continuation method (23)-(24) with the trust-region updating strategy and the switching preconditioning technique for the linearly equality-constrained optimization problem (i.e. Algorithm 1). Firstly, we give a lower-bounded estimate of mk(0) - mk(sk) (k = 1, 2, . . .). This result is similar to that of the trust-region method for the unconstrained optimization problem [49]. For simplicity, we assume that the rank of matrix A is full.
We denote the feasible set S f as

S f = {x : Ax = b}.

(47)

In the following analysis of Algorithm 1, the function f is assumed to satisfy Assumption 1.

Assumption 1 Assume that the function f (·) is twice continuously differential and its Hessian function 2 f (·) is bounded. That is to say, there exists a positive constant
M such that

2 f (x)  M,

(48)

holds for all x  S f .

By combining the property P = 1 of the projection matrix P, from the assumption (48), we obtain

P2 f (x)P  P 2 f (x) P = 2 f (x)  M.

(49)

According to the property of the matrix norm, we know that the absolute eigen-

value of P2 f (x)P is less than M. If we denote µ P2 f (x)P as the eigenvalue of

P2 f (x)P, we know that the eigenvalue of

1 t

I

+

P2

f

(x)P

is

1 t

+µ

P2 f (x)P .

Consequently, from equation (49), we known that

1 t

I

+

P2

f

(x)P

1 0, x  S f , when t < M .

(50)

Lemma 2 Assume that the quadratic model qk(x) is defined by equation (26) and sk is computed by the Tikhonov regularization continuation method (23)-(24), where matrices Bk (k = 1, 2, . . .) are updated by the L-BFGS formula (30) in the well-posed phase. Then, we have

mk(0)

- mk(sk)



 tk 4(1 + tk)

pgk

2  cw pgk

sk ,

(51)

where cm is a positive constant, pgk = Pgk = P f (xk) and the projection matrix P is defined by equation (10).

Tikhonov regularization continuation methods

15

Proof. From yk-1 = Pg(xk) - Pg(xk-1) and AP = 0, we have Ayk-1 = 0. By com-
bining it with Ask-1 = 0, from the L-BFGS formula (30) and the Tikhonov regu-
larization continuation method (23)-(24), we obtain Ask = 0, namely xk+1  S f and sk = Pzk for a vector zk  n. Consequently Psk = P2zk = Pzk = sk. By induction,
we know that Psk = sk (k = 0, 1, 2, . . .). Furthermore, from the L-BFGS formula and Lemma (1), we know that the eigenvalues of B-k 1 are greater than 1/2. By combining them into equation (26) and using the symmetric Shur decomposition (p. 440, [19]) of B-k 1, we obtain

mk(0)

-

mk(sk)

=

-

1

+ 0.5tk 1 + tk

gTk

sk

=

-

1

+ 0.5tk 1 + tk

gTk

(Psk)

=

-

1

+ 0.5tk 1 + tk

pTgk

sk

=

1 + 0.5tk 1 + tk

1

 tk + tk

pTgk B-k 1 pgk



1 + 0.5tk 1 + tk

 tk 2(1 + tk)

pgk

2.

(52)

By substituting the property (1 + 0.5tk)/(1 + tk)  (0.5 + 0.5tk)/(1 + tk) = 0.5 into equation (52), we have

mk(0)

-

mk(sk)



 tk 4(1 + tk)

pgk

2.

(53)

From equation (49), we have

1

yk-1 = Pg(xk-1) - Pg(xk-2) =

P2 f (xk-2 + tsk-1)sk-1dt

0

1

1

=

P2 f (xk-2 + tsk-1)Psk-1dt 

P2 f (xk-2 + tsk-1)P

0

0

 M sk-1 .

sk-1 dt (54)

From

Lemma

1,

we

know

that

the

eigenvalues

of

Bk

are

greater

than

2 2

sk-1 yk-1

2 2

.

By

substituting equation (54) into it, we know that the eigenvalues of Bk are greater than

 2/(2M2). Furthermore, from the symmetric Shur decomposition (p. 440, [19]), we

know that there exists an orthogonal matrix Uk such that Bk = UkT diag µ1k, . . . , µnk Uk,

where µ1k  µ2k  · · ·  µnk are the eigenvalues of the symmetric matrix Bk. Thus, we

obtain

Bksk 2 = (UkBkUkT )Uksk 2 = (Uksk)T diag (µ1k)2, . . . , (µnk)2 (Uksk)



2 2M2

2
sTk UkT Uksk =

2 2M2

2
sk 2, i.e.

Bk sk



2 2M2

sk

.

(55)

By combining it with equations (23) and (53), we obtain

mk(0)

- mk(sk)



 tk 4(1 + tk)

pgk

2= 1 4

pgk

Bk sk



2 8M2

pgk

sk .

(56)

We denote cw =  2/(8M2). Then, from equation (56), we obtain the result (51).

16

Luo, Xiao

Lemma 3 Assume that the quadratic model qk(x) is defined by equation (26) and

sk is computed by the Tikhonov regularization continuation method (23)-(24), where

Bk =

1  tk

I

+

P2

f

(xk

)P

and

 tk



1 2M

in

the

ill-posed

phase.

Then,

we

have

mk(0) - mk(sk)



 tk 4(1 + tk)

pgk

2  cb pgk

sk ,

(57)

where cb is a positive constant, pgk = Pgk = P f (xk) and the projection matrix P is defined by equation (10).

Proof.

From equations (23)-(24) and Bk =

1  tk

I

+

P2

f

(xk

)P

, we have

P

1  tk

I

+

P2

f

(xk)P

sk

=

-

1

 tk +

tk

P2gk.

(58)

By substituting P2 = P into the above equation (58), we obtain Psk = sk. Conse-

quently, by combining it with the property AP = 0, we obtain Ask = 0, i.e. xk+1  S f

if xk  S f . By induction, we obtain xk  S f (k = 1, 2, . . .) when x0  S f . Therefore,

according

to

the

assumption

 tk



1 2M

,

from

equation

(50),

we

know

1  tk

I

+

P2

f

(xk

)P

0.

(59)

From equations (23), (59) and Psk = sk, by using the symmetric Shur decomposition (p. 440, [19]), we have

- sTk gk

=

-(Psk)T gk

=

-sTk (Pgk)

=

 tk 1 + tk

pTgk

1  tk

I

+

P2

f

(xk)P

-1
pgk

 tk 1 + tk 1/tk +

1 P2 f (xk)P

pgk

2  tk

1

1 + tk 1/tk + M

pgk

2.

(60)

Similarly to the estimation of equation (55), from equation (23) and the symmetric Shur decomposition (p. 440, [19]), we have

Bksk =

1  tk

I

+

P2

f

(xk

)P

sk



1 -M
 tk

sk ,

(61)

where we use the property that the absolute eigenvalues of P2 f (xk)P are less than M. From equation (23) and by substituting equation (61) into equation (60), we obtain

-sTk

gk



1/ tk 1/ tk

-M +M

pgk

2M - M

sk

 2M + M

pgk

1

sk

= 3

pgk

sk ,

(62)

where we use the assumption tk  1/(2M) and the monotonically increasing property of (t) = (t - M)/(t + M) when t > M.

Tikhonov regularization continuation methods

17

From the approximation model (26) and the estimation (62), we have

mk(0)

- mk(sk)

=

-

1

+ 0.5tk 1 + tk

gTk

sk



1 3

1

+ 0.5tk 1 + tk

pgk

sk

= 1 0.5 + 0.5(1 + tk)

3

1 + tk

pgk

1

sk

 6

pgk

sk ,

(63)

where we use the property 0.5 + 0.5(1 + tk)  0.5(1 + tk). We denote cb = 1/6. Then, from equation (63), we obtain the estimation (57).
In order to prove that pgk converges to zero when k tends to infinity, we need to estimate the lower bound of time step sizes tk (k = 1, 2, . . .).

Lemma 4 Assume that f satisfies Assumption 1 and the sequence {xk} is generated by Algorithm 1. Then, there exists a positive constant t such that

 tk  2t

(64)

holds for all k = 1, 2, . . . , where tk is adaptively adjusted by the trust-region updating strategy (26)-(28).

Proof. From the first-order Taylor expansion, we have

1

f (xk + sk) = f (xk) + sTk g(xk + tsk)dt.

(65)

0

Thus, from equations (26)-(27), (65), the Armijo sufficient decrease condition (29) and the assumption (48), we have

|k - 1| =

( f (xk) - f (xk + sk)) - (mk(0) - mk(sk)) mk(0) - mk(sk)



1 0

sTk

(g(xk

+

t

sk

)

-

g(xk))dt

+

0.5 tk

mk(0) - mk(sk)

1 + 0.5tk



1 0

sk

g(xk + tsk) - g(xk) dt +

0.5 tk

mk(0) - mk(sk)

1 + 0.5tk

 0.5M sk 2 + 0.5tk .

(66)

mk(0) - mk(sk) 1 + 0.5tk

From Lemma 2 and Lemma 3, we know that there exists a constant m such as m = min{cw, cb} such that the approximation model mk(0) - mk(sk) satisfies the Armijo sufficient decrease condition (29) when tk  1/(2M) and the function f satisfies Assumption 1. By substituting the sufficient decrease condition (29) into
equation (66), we obtain

0.5M |k - 1| 
m

sk pgk

+ 0.5tk . 1 + 0.5tk

(67)

18

Luo, Xiao

When matrix Bk is updated by the L-BFGS formula (30) in the well-posed phase,

from Lemma 1, we know that the eigenvalues of B-k 1 are less than max

1,

2 yk-1 2  2 sk-1 2

.

By combining it with equations (30) and (49), we obtain

sk

= tk 1 + tk

B-k 1 pgk

 tk max 1 + tk

1,

2 yk-1 2  2 sk-1 2

pgk

 tk max 1 + tk

1,

2

Pg(xk-1 + sk-1) - Pg(xk-1)  2 sk-1 2

2

pgk

 tk max 1 + tk

2M2 1,  2

pgk

=

1

 tk + tk

Lw

pgk

.

(68)

where we denote Lw = max

1,

2M2 2

.

When matrix Bk =

1  tk

I

+

P2

f

(xk

)P

and tk  1/(2M), from equations (23)

and (49), we have

sk

= tk 1 + tk

B-k 1 pgk

= tk 1 + tk

1  tk

I

+

P2

f

(xk)P

-1
pgk

 tk

1

1 + tk 1/tk - M

pgk

 tk 1 1 + tk M

pgk .

(69)

Thus, when Bk are updated by the formula (39) and tk  1/(2M) in the ill-posed phase, from equation (39), we have

sk

 tk 1 1 + tk M

pgk

.

(70)

We denote Lu = max{Lw, 1/M}. By substituting equations (68) and (70) into equation (67), when tk  1/(2M), we obtain

|k

-

1|



0.5MLu m

1

 tk + tk

+

1

0.5 tk + 0.5tk

 0.5MLu tk + 0.5tk  MLu + m tk .

(71)

m 1 + tk 0.5 + 0.5tk

2m 1 + tk

We denote

 t

min

21m , MLu + m

1, 2M

 t0

.

(72)

Then, from equations (71)-(72), when tk  t , it is not difficult to verify

|k - 1|  1.

(73)

We assume that K is the first index such that tK  t where t is defined by equation (72). Then, from equations (72)-(73), we know that |K - 1|  1. According to the time step adjustment formula (28), xK + sK will be accepted and the time step tK+1 will be enlarged. Consequently, tk  2t holds for all k = 1, 2, . . ..

Tikhonov regularization continuation methods

19

By using the result of Lemma 4, we prove the global convergence of Algorithm 1 for the linearly constrained optimization problem (1) as follows.
Theorem 1 Assume that f satisfies Assumption 1 and f (x) is lower bounded when x  S f , where S f is defined by equation (47). The sequence {xk} is generated by Algorithm 1. Then, we have

lim inf Pgk = 0,

(74)

k

where gk =  f (xk) and matrix P is defined by equation (10).

Proof. We prove the result (74) by contradiction. Assume that there exists a positive constant  such that

Pgk > 

(75)

holds for all k = 0, 1, 2, . . ..

According to Lemma 4 and Algorithm 1, we know that there exists an infinite subsequence {xki } such that the trial steps ski (i = 1, 2, . . .) are accepted. Otherwise, all steps are rejected after a given iteration index, then the time step size will keep decreasing, which contradicts (64). Therefore, from equations (27), (29) and (75), we have





  f

(x0)

-

lim
k

f

(xk)

=

(
k=0

f

(xk)

-

f

(xk+1))



(
i=0

f

(xki

)

-

f

(xki

+

ski

))







    a mki (0) - mki (ski )  am Pgki ski  am ski . (76)

i=0

i=0

i=0

Since f (x) is lower bounded when x  S f and the sequence { f (xk)} is monotonically decreasing, we have limk f (xk) = f . By substituting it into equation (76), we
obtain

lim
i

ski

= 0.

(77)

When Bk is updated by the L-BFGS formula (1) in the well-posed phase, from

Lemma 1, we know Bk  2. When Bk is updated by the formula (39) in the ill-

posed phase, from equations (49) and (64), we know that

Bk



1 + M . We
2  t

denote

1

LB = max 2,

+M .

(78)

2  t

By substituting equations (64) and (78) into equation (23), we obtain

Pgki

= 1 + tki  tki

Bki ski

=

1 1+
 tki

1

Bki ski



1+ 2  t

By substituting equation (79) into equation (77), we obtain

LB ski . (79)

lim
i

Pgki

= 0,

which contradicts the assumption (74). Consequently, the result (74) is true.

20

Luo, Xiao

4 Numerical Experiments

In this section, we conduct some numerical experiments to test the performance of Algorithm 1 (the Trcmtr method). The codes are executed by a HP notebook with the Intel quad-core CPU and 8Gb memory in the MATLAB R2020a environment [43]. The two-sided projected Hessian matrix P2 f (x)P of Algorithm 1 is approximated by the difference formula (40).

SQP [16, 18, 46, 60] is the traditional-representative method for the constrained optimization problems. Ptctr is the latest continuation method and significantly better than SQP for linearly equality-constrained optimization problems according to the numerical results in [35]. Therefore, we select these two typical methods as the basis for comparison. The implementation code of SQP is the built-in subroutine fmincon.m of the MATLAB2020a environment [43].

We select 57 linearly constrained-equality optimization problems from [1, 35, 45, 54] as the test problems, some of which are the unconstrained optimization problems [1, 45, 54] and we add the same linear constraint Ax = b, where b = 2  ones(n, 1) and matrix A is defined as follows:

2 1 0 · · · 0 0 0

1 1 1 · · · 1 1 1

1 2 1 · · · 0 0 0

2 2 2 · · · 2 2 2

0 1 2 · · · 0 0 0

1 1 1 · · · 1 1 1

A1

=

  

...

...

...

...

...

...

...

 , 

A2

=

  

...

...

...

...

...

...

 , 

A

=

A1 , A2

.

(80)

0 0 0 · · · 2 1 0

1 1 1 · · · 1 1 1

0 0 0 · · · 1 2 1

2 2 2 · · · 2 2 2

0 0 0 ··· 0 1 2

1 1 1 ··· 1 1 1

The alternating direction method of multipliers (ADMM [8]) is an efficient method for some convex optimization problems and studied by many researchers in recent years. Therefore, we also compare Trcmtr with ADMM for 17 linearly equalityconstrained convex optimization problems of those 57 test problems. The compared ADMM subroutine [8] is downloaded from the web site https://web.stanford. edu/~boyd/papers/admm/.
The termination conditions of the four compared methods are all set by

xL(xk, k)   1.0 × 10-6,

(81)

Axk - b   1.0 × 10-6, k = 1, 2, . . . ,

(82)

where the Lagrange function L(x,  ) is defined by equation (4) and  is defined by equation (8).

We test those 57 problems with n = 2 to n  1000. The numerical results are arranged in Tables 1-2 for the convex problems, and Tables 3-4 for the non-convex problems. The computational time and the number of iterations of Trcmtr, Ptctr and SQP are illustrated in Figure 1 and Figure 2, respectively. From Table 1 and Table 2, we find that Trcmtr can solve those convex linearly constrained-equality optimization

Tikhonov regularization continuation methods

21

problems. However, there are 3 convex problems of 17 convex problems can not be solved well by Ptctr and SQP, respectively. ADMM can not work well for those 17 test convex problems.

From Table 3 and Table 4, we find that Trcmtr can solve those 40 non-convex linearly constrained-equality optimization problems well except for a particularly difficult problem (Strectched V Function [54]). For this problem, Ptctr and SQP can not solve it, too. There are two non-convex problems and five non-convex problems of 40 non-convex problems can not be solved by Ptctr and SQP, respectively. Furthermore, from Tables 2-4 and Figure 1, we find that the computational time of Trcmtr is significantly less than those of Ptctr and SQP for most of test problems, respectively.

From those numerical results, we find that Trcmtr works significantly better than

the other three methods. One of the reasons is that Trcmtr uses the L-BFGS method

(31) as the preconditioned technique to follow their trajectories in the well-posed

phase. Consequently, Trcmtr only involves three pairs of the inner product of two vec-

tors and one matrix-vector product (pgk = Pgk) to obtain the trial step sk and involves about (n - m)n flops at every iteration in the well-posed phase. However, Ptctr needs

to solve a linear system of equations with an n × n symmetric definite coefficient

matrix

and

involves

about

1 3

n3

flops

(p.

169,

[19])

at

every

iteration.

SQP

needs

to

solve a linear system of equations with dimension (m + n) when it solves a quadratic

programming subproblem at every iteration (pp. 531-532, [46]) and involves about

2 3

(m

+

n)3

flops

(p.

116,

[19]).

102

Ptctr

Trcmtr

101

SQP

100

Consuming Time

10-1

10-2

10-3

10-4 0

10

20

30

40

50

60

Example

Fig. 1: The computational time (s) of Ptctr, Trcmtr and SQP for test problems.

22

Luo, Xiao

Table 1: Numerical results of Trcmtr and ADMM for convex problems.

Problems
Exam. 1 Kim Problem 1 [26, 35] (n = 1000, m = n/2)
Exam. 2 LLS Problem 1 [35] (n = 1200, m = n/3)
Exam. 3 Obsborne Problem 1 [35, 47] (n = 1200, m = 2/3n)
Exam. 4 Mak Problem [35, 42] (n = 1000, m = n/2)
Exam. 5 LLS Problem 2 [35] (n = 1000, m = n/2)
Exam. 6 Osborne Problem 2 [35, 47] (n = 1200, m = n/2)
Exam. 7 Carlberg Problem [9, 35] (n = 1000, m = n/2)
Exam. 8 Kim Problem 2 [26, 35] (n = 1000, m = n/2)
Exam. 9 Yamashita Problem [35, 61] (n = 1200, m = n/3)
Exam. 10 Quartic With Noise Function [1](n = 1000, m = n/2)
Exam. 11 Rotated Hyper Ellopsoid Function [54](n = 1000, m = n/2)
Exam. 12 Sphere Function [54] (n = 1000, m = n/2)
Exam. 13 Sum Squares Function [54] (n = 1000, m = n/2)
Exam. 14 Trid Function [54] (n = 1000, m = n/2)
Exam. 15 Booth Function [54] (n = 2, m = n/2)
Exam. 16 Matyas Function [54] (n = 2, m = n/2)
Exam. 17 Zakharov Function [54] (n = 10, m = n/2)

Trcmtr

steps

f (x )

(time)

(KKT)

13 (0.24)

7.27e+03 (3.44e-07)

17 (0.42)

1.44e+03 (9.37e-07)

1 (0.55)

7.15e+02 (1.27e-15)

11 (0.47)

97.96 (7.74e-07)

14 (0.66)

82.43 (7.54e-08)

14 (0.97)

5.14e+02 (8.75e-07)

15 (0.74)

1.19e+04 (1.66e-06)

21 (1.59)

4.22e+04 (1.43e-06)

25 (2.62)

0.50 (3.67e-07)

7 (0.08)

1.01e+02 (2.69e-07)

6 (2.50)

1.25e+05 (8.30e-06)

1 (0.08)

1.67e+02 (3.13e-15)

28 (4.08)

4.08e+04 (1.58e-06)

38 (2.61)

5.82e+02 (5.36e-07)

13

9.00

(1.00e-03) (1.98e-07)

17

0.18

(1.00e-04) (4.44e-07)

21

7.31

(8.00e-03) (1.65e-07)

ADMM

steps

f (x )

(time)

(KKT)

3 (0.04)

2.20e+04 (40.00) (failed)

21 (0.07)

2.73e+03 (4.00) (failed)

60 (0.18)

8.48e+02 (2.80) (failed)

4 (0.05)

1.32e+02 (1.00) (failed)

12 (0.04)

8.00e+03 (32.00) (failed)

60 (0.20)

7.86e+02 (2.80) (failed)

3 (0.04)

1.40e+04 (32.00) (failed)

3 (0.33)

3.28e+05 (1.92e+03)
(failed)

16 (0.06)

25.10 (0.50) (failed)

400 (0.40)

1.01e+02 (3.98) (failed)

400 (1.04)

1.26e+05 (2.00e+05)
(failed)

400 (0.27)

1.67e+02 (2.00) (failed)

400 (0.32)

4.16e+04 (9.98e+02)
(failed)

400 (0.36)

5.85e+02 (3.99) (failed)

18 (1.00e-03)

45.00 (30.00) (failed)

18 (2.00e-03)

2.60 (5.20) (failed)

21 (1.00e-03)

4.33e+02 (1.87e+03)
(failed)

5 Conclusions
In this paper, we give the Tikhonv regularization continuation method with the trustregion updating strategy (Trcmtr) for linearly equality-constrained optimization problems. Moreover, in order to improve its computational efficiency and robustness, the

Tikhonov regularization continuation methods

23

Table 2: Numerical results of Ptctr, Trcmtr and SQP for convex problems.

Problems
Exam. 1 Kim Problem 1 [26, 35] (n = 1000, m = n/2)
Exam. 2 LLS Problem 1 [35] (n = 1200, m = n/3) Exam. 3 Obsborne Problem 1 [35, 47] (n = 1200, m = 2/3n) Exam. 4 Mak Problem [35,42] (n = 1000, m = n/2) Exam. 5 LLS Problem 2 [35] (n = 1000, m = n/2) Exam. 6 Osborne Problem 2 [35, 47] (n = 1200, m = n/2) Exam. 7 Carlberg Problem [9,35] (n = 1000, m = n/2) Exam. 8 Kim Problem 2 [26, 35] (n = 1000, m = n/2) Exam. 9 Yamashita Problem [35, 61] (n = 1200, m = n/3) Exam. 10 Quartic With Noise Function [1] (n = 1000, m = n/2) Exam. 11 Rotated Hyper Ellopsoid Function [54] (n = 1000, m = n/2) Exam. 12 Sphere Function [54] (n = 1000, m = n/2) Exam. 13 Sum Squares Function [54] (n = 1000, m = n/2) Exam. 14 Trid Function [54] (n = 1000, m = n/2) Exam. 15 Booth Function [54] (n = 2, m = n/2) Exam. 16 Matyas Function[54] (n = 2, m = n/2) Exam. 17 Zakharov Function[54] (n = 10, m = n/2)

Ptctr

steps

f (x )

(time)

(KKT)

11 (0.56)

7.27e+03 (5.79e-08)

17 (1.01)

1.44e+03 (7.36e-07)

12 (1.01)

7.15e+02 (2.30e-07)

11 (0.59)

97.96 (3.50e-07)

14 (0.69)

82.43 (8.79e-08)

13 (1.04)

5.14e+02 (1.79e-07)

10 (0.54)

1.19e+04 (1.23e-07)

12 (0.73)

4.22e+04 (6.14e-06)

16 (0.89)

0.50 (4.39e-07)

9 (0.42)
8 (0.72)
10 (0.43)
17 (9.77)
304 (9.18)
12 (1.00e-04)

1.01e+02 (3.14e-07)
1.25e+05 (2.08e-04)
(failed)
1.67e+02 (1.11e-07)
4.08e+04 (1.85e-04)
(failed) 5.82e+02 (8.34e-04) (failed)
9.00 (1.74e-07)

11

0.18

(4.00e-03) (1.87e-08)

15

7.31

(6.00e-03) (2.93e-08)

Trcmtr

steps

f (x )

(time)

(KKT)

13 (0.24)

7.27e+03 (3.44e-07)

17 (0.42)

1.44e+03 (9.37e-07)

1 (0.55)

7.15e+02 (1.27e-15)

11 (0.47)

97.96 (7.74e-07)

14 (0.66)

82.43 (7.54e-08)

14 (0.97)

5.14e+02 (8.75e-07)

15 (0.74)

1.19e+04 (1.66e-06)

21 (1.59)

4.22e+04 (1.43e-06)

25 (2.62)

0.50 (3.67e-07)

7 (0.08)

1.01e+02 (2.69e-07)

6 (2.50)

1.25e+05 (8.30e-06)

1

1.67e+02

(7.50e-02) (3.13e-15)

28 (4.08)

4.08e+04 (1.58e-06)

38 (2.61)

5.82e+02 (5.36e-07)

13

9.00

(1.00e-03) (1.98e-07)

17

0.18

(1.00e-04) (4.44e-07)

21

7.31

(8.00e-03) (1.65e-07)

SQP

steps

f (x )

(time)

(KKT)

2 (0.36)

7.27e+03 (8.30e-13)

13 (2.59)

1.44e+03 (3.42e-07)

3 (1.48)

7.14e+02 (2.22e-15)

8 (1.18)

97.96 (1.34e-10)

11 (1.65)

82.43 (1.78e-09)

15 (5.86)

5.14e+02 (1.75e-06)

14 (1.96)

1.19e+04 (1.13e-05)

29 (3.27)

4.22e+04 (3.05e-06)

14 (2.64)

0.50 (1.01e-07)

4 (0.64)
400 (55.18)
3 (0.44)
400 (44.36)
400 (44.05)
17 (6.00e-03)

1.01e+02 (1.25e-09)
1.46e+05 (3.22e+02)
(failed)
1.67e+02 (7.67e-10)
4.10e+04 (1.01e+02)
(failed) 5.82e+02 (1.56e-04) (failed)
9.00 (3.55e-15)

3

0.18

(5.00e-03) (1.67e-16)

21

7.31

(7.00e-03) (8.50e-06)

new method replaces the inverse of the regularization two-sided projected Hessian matrix with the L-BFGS preconditioner. Numerical results show that Trcmtr is more robust and faster than the traditional optimization method such as SQP (the built-in subroutine fmincon.m of the MATLAB2020a environment [43]), the latest continuation method such as Ptctr [35] and the alternating direction method of multipliers

24

Luo, Xiao

Table 3: Numerical results of Ptctr, Trcmtr, SQP for large-scale nonconvex problems.

Problems
Exam. 1 LLS Problem 3 [35] (n = 1000, m = n/2) Exam. 2 Ackly Function [54] (n = 1000, m = n/2) Exam. 3 Rosenbrock Function [54] (n = 1000, m = n/2) Exam. 4 Dixon-Price Function [54] (n = 1000, m = n/2) Exam. 5 Griewank Function [54] (n = 1000, m = n/2) Exam. 6 Levy Function [54] (n = 1000, m = n/2) Exam. 7 Molecular Energy Function [45] (n = 1000, m = n/2) Exam. 8 Powell Function [54] (n = 1000, m = n/2) Exam. 9 Rastrigin Function [54] (n = 1000, m = n/2) Exam. 10 Schwefel Function [54] (n = 1000, m = n/2) Exam. 11 Styblinski Tang Function [54] (n = 1000, m = n/2) Exam. 12 Shubert Function [54] (n = 1000, m = n/2) Exam. 13 Strectched V Function [54] (n = 1000, m = n/2)

Ptctr

steps

f (x )

(time)

(KKT)

38 (2.45)

1.96e+02 (1.17e-05)

1 (0.11)

2.64 (1.87e-07)

9 (0.64)
400 (15.54)
20 (0.73)

9.26e+03 (9.03e-06)
8.97e+04 (2.42e-02)
(failed)
0.86 (4.81e-07)

70 (1.83)

71.06 (2.36e-08)

30 (0.94)

4.69e+02 (4.38e-07)

11 (0.67)

4.26e+03 (1.77e-06)

20 (0.63)

2.93e+03 (6.42e-07)

109 (3.17)

4.19e+05 (3.74e-07)

76 (2.05)

-9.61e+03 (1.12e-05)

6 (0.53)
1 (0.39)

2.65e+03 (1.43e-06)
3.10e-03 (1.08e+05)
(failed)

Trcmtr

steps

f (x )

(time)

(KKT)

25 (10.27)

-3.03e+03 (4.86e-07)

1

2.64

(7.10e-02) (7.50e-07)

20 (0.78)

9.26e+03 (2.15e-06)

25 (2.35)

9.00e+04 (1.74e-09)

12 (0.35)

0.86 (4.40e-08)

56 (0.12)

71.06 (8.25e-07)

55 (0.75)

4.69e+02 (8.66e-07)

17

4.26e+03

(9.50e-02) (4.52e-07)

24 (0.11)

2.93e+03 (1.56e-06)

71 (1.16)

4.19e+05 (4.16e-06)

89 (8.02)

-9.61e+03 (4.57e-06)

8 (8.40e-02)
16 (18.86)

2.65e+03 (8.92e-07)
2.30e+02 (2.41e-03)
(failed)

SQP

steps

f (x )

(time)

(KKT)

42 (7.70)

1.88e+02 (7.97e-06)

2 (0.37)
400 (44.68)
400 (46.97)
9 (1.12)

2.42 (1.94e-07)
9.26e+03 (5.00e-03)
(failed) 8.24e+06 (1.28e+05) (failed)
0.86 (1.07e-10)

31 (3.82)

71.06 (1.11e-07)

16 (2.04)
364 (41.34)
7 (1.00)

4.69e+02 (1.71e-06)
4.26e+03 (1.38e-04)
(failed)
4.44e+03 (1.13e-06)

51 (5.49)
172 (21.46)
3 (0.47)
6 (0.95)

4.02e+05 (2.31e-06)
-2.56e+04 (7.03e-04)
(failed)
2.65e+03 (1.42e-05)
1.28 (34.15) (failed)

(ADMM [8]). Therefore, Trcmtr is worth exploring further, and we will extend it to the general nonlinear optimization problem in the future.

Acknowledgments
This work was supported in part by Grant 61876199 from National Natural Science Foundation of China, Grant YBWL2011085 from Huawei Technologies Co., Ltd., and Grant YJCB2011003HI from the Innovation Research Program of Huawei Technologies Co., Ltd..

Tikhonov regularization continuation methods

25

Table 4: Numerical results of Ptctr, Trcmtr, SQP for small-scale nonconvex problems.

Problems
Exam. 1 Beale Function [54] (n = 2, m = n/2) Exam. 2 Branin Function [54] (n = 2, m = n/2) Exam. 3 Eason Function [54] (n = 2, m = n/2) Exam. 4 Hosaki Function [1] (n = 2, m = n/2) Exam. 5 Levy Function N. 13 [54] (n = 2, m = n/2) Exam. 6 McCormick Function [54] (n = 12, m = n/2) Exam. 7 Perm Function d  [54] (n = 4, m = n/2) Exam. 8 Power Sum Function [54] (n = 4, m = n/2) Exam. 9 Price Function [1] (n = 2, m = n/2) Exam. 10 Bohachevsky Function [54] (n = 2, m = n/2) Exam. 11 Colville Function [54] (n = 4, m = n/2) Exam. 12 Drop Wave Function [54] (n = 2, m = n/2) Exam. 13 Schaffer Function [54] (n = 2, m = n/2) Exam. 14 Six-Hump Camel Function [54] (n = 2, m = n/2) Exam. 15 Three-Hump Camel Function [54] (n = 2, m = n/2) Exam. 16 Trecanni Function [1] (n = 2, m = n/2) Exam. 17 Box Bettes Exponential Quadratic Function [1] (n = 3, m = 2) Exam. 18 Chichinad Function [1] (n = 2, m = n/2) Exam. 19 Eggholder Function [54] (n = 2, m = n/2) Exam. 20 Exp2 Function [1] (n = 2, m = n/2) Exam. 21 Hansen Function [1] (n = 2, m = n/2) Exam. 22 Hartmann 3-D Function [54] (n = 3, m = 2) Exam. 23 Holder Table Function [54] (n = 2, m = n/2) Exam. 24 Michalewicz Function [54] (n = 2, m = n/2) Exam. 25 Schaffer Function N. 4 [54] (n = 4, m = n/2) Exam. 26 Trefethen 4 Function [1] (n = 2, m = n/2) Exam. 27 Zettl Function [1] (n = 2, m = n/2)

Ptctr

steps

f (x )

(time)

(KKT)

11

3.35

(2.00e-03) (2.23e-08)

23

15.90

(2.00e-03) (1.51e-08)

10

-4.19e-06

(5.00e-03) (4.08e-06)

12

-0.56

(1.20e-02) (4.21e-07)

8

0.63

(4.00e-03) (9.51e-07)

12

1.31

(3.00e-03) (1.05e-07)

25

1.19e+03

(4.00e-03) (2.59e-06)

1

1.52e+04

(6.00e-03) (2.59e-06)

8

7.06

(6.00e-03) (1.50e-07)

9

2.36

(4.00e-03) (1.75e-07)

13

21.14

(2.00e-03) (1.35e-07)

10

-0.79

(2.00e-03) (2.12e-07)

14

0.61

(5.00e-03) (7.18e-08)

10

0.74

(4.00e-03) (1.73e-07)

15

0.55

(4.00e-03) (3.91e-07)

11

2.36

(4.00e-03) (2.78e-08)

20

1.42e-11

(2.60e-02) (9.96e-07)

8

8.01

(5.00e-03) (9.15e-09)

17

-69.16

(3.00e-03) (4.17e-07)

11

9.19

(2.00e-03) (1.53e-08)

9

-12.10

(6.00e-03) (3.98e-08)

13

-3.84

(2.00e-03) (1.36e-07)

13

-1.68

(2.00e-03) (1.65e-07)

15

-1.00

(4.00e-03) (7.24e-08)

8

0.30

(5.00e-03) (9.11e-07)

9

1.20

(5.00e-03) (7.61e-08)

11

0.14

(4.00e-03) (3.26e-08)

Trcmtr

steps

f (x )

(time)

(KKT)

19

3.35

(2.00e-03) (8.92e-07)

28

15.90

(1.00e-03) (3.53e-07)

25

-4.19e-06

(3.00e-03) (1.06e-07)

11

-0.56

(2.00e-03) (3.24e-08)

10

0.63

(1.00e-04) (1.61e-07)

13

1.31

(1.00e-04) (8.42e-07)

40

1.19e+03

(1.70e-02) (3.85e-06)

1

1.52e+04

(2.00e-03) (1.85e-13)

9

7.06

(1.10e-02) (1.85e-07)

11

2.36

(4.00e-03) (7.12e-07)

26

21.14

(1.70e-02) (5.81e-07)

9

-0.7858

(2.00e-03) (9.15e-07)

13

0.61

(3.00e-03) (1.04e-07)

18

0.74

(2.00e-03) (4.09e-07)

24

0.55

(1.00e-04) (4.82e-07)

13

2.36

(1.00e-04) (4.91e-07)

35

3.01e-13

(8.00e-03) (7.07e-07)

9

8.01

(5.00e-03) (4.47e-08)

22

-69.60

(2.00e-02) (9.45e-07)

15

8.45

(3.00e-03) (6.63e-07)

8

-12.10

(2.00e-03) (1.69e-07)

22

-3.84

(2.00e-03) (8.13e-07)

16

-1.68

(2.00e-03) (5.82e-07)

16

-1.00

(3.00e-03) (6.12e-07)

11

0.30

(1.00e-04) (8.35e-07)

24

-1.36

(3.00e-04) (1.51e-07)

17

0.14

(1.00e-04) (8.27e-07)

SQP

steps

f (x )

(time)

(KKT)

8

3.35

(7.00e-03) (1.50e-06)

6

34.37

(6.00e-03) (4.55e-06)

9

-4.55e-06

(6.00e-03) (5.80e-07)

6

-0.56

(5.00e-03) (9.77e-07)

9

0.98

(8.00e-03) (1.42e-05)

5

1.31

(5.00e-03) (2.64e-09)

28

1.19e+03

(8.00e-03) (7.41e-05)

2

1.52e+04

(6.00e-03) (1.14e-12)

11

7.06

(6.00e-03) (7.77e-06)

9

1.15

(6.00e-03) (1.00e-08)

11

5.19

(5.00e-03) (3.37e-05)

5

-0.79

(5.00e-03) (3.73e-09)

8

0.61

(5.00e-03) (1.62e-07)

11

0.74

(5.00e-03) (2.00e-08)

7

0.55

(4.00e-03) (1.46e-07)

9

2.36

(4.00e-03) (4.19e-08)

13

3.26e-16

(6.00e-03) (2.57e-08)

6

-20.06

(3.00e-03) (8.52e-07)

9

-69.16

(6.00e-03) (5.77e-08)

6

9.19

(5.00e-03) (6.60e-07)

6

-32.36

(5.00e-03) (5.21e-05)

2

-1.31e-30

(4.00e-03) (2.22e-16)

5

-3.51e-02

(5.00e-03) (9.62e-07)

2

-7.18e-12

(4.00e-03) (1.20e-10)

6

0.29

(6.00e-03) (7.68e-07)

8

-2.02

(6.00e-03) (7.45e-05)

11

0.14

(4.00e-03) (1.84e-09)

26

Luo, Xiao

Iterations

400 380 360 340 320 300 280 260 240 220 200 180 160 140 120 100
80 60 40 20
0 0

Ptctr Trcmtr SQP

10

20

30

40

50

60

Example

Fig. 2: The number of iterations of Ptctr, Trcmtr and SQP for test problems.

References
1. E. P. Adorio and U. P. Diliman, MVF - Multivariate test functions library in C for unconstrained global optimization, http://www.geocities.ws/eadorio/mvf.pdf, 2005.
2. E. L. Allgower and K. Georg, Introduction to Numerical Continuation Methods, SIAM, Philadelphia, PA, 2003.
3. U. M. Ascher and L. R. Petzold, Computer Methods for Ordinary Differential Equations and Differential-Algebraic Equations, SIAM, Philadelphia, PA, 1998.
4. C.G. Broyden, The convergence of a class of double-rank minimization algorithms, J Inst Math Appl 6 (1970), 76-90.
5. A. A. Brown and M. C. Bartholomew-Biggs, ODE versus SQP methods for constrained optimization, J Optim Theory Appl 62 (1989), 371-386.
6. K. E. Brenan, S. L. Campbell and L. R. Petzold, Numerical solution of initial-value problems in differential-algebraic equations, SIAM, Philadelphia, PA, 1996.
7. J.C. Butcher, Z. Jackiewicz, Construction of high order diagonally implicit multistage integration methods for ordinary differential equations, Appl Numer Math 27 (1998), 1-12.
8. S. Boyd, N. Parikh, E. Chu, J. Eckstein, et al, Distributed optimization and statistical learning via the alternating direction method of multipliers, Foundations and Trends in Machine Learning 3 (2011), 1-122.
9. K. Carlberg, Lecture notes of constrained optimization, https://www.sandia.gov/~ktcarlb/ opt_class/OPT_Lecture3.pdf, 2009.
10. F. Caballero, L. Merino, J. Ferruz and A. Ollero, Vision-based odometry and SLAM for medium and high altitude flying UAVs, J Intell Robot Syst 54 (2009), 137-161.
11. T. S. Coffey, C. T. Kelley and D. E. Keyes, Pseudotransient continuation and differential-algebraic equations, SIAM J Sci Comput 25 (2003), 553-569.
12. A. R. Conn, N. Gould and Ph. L. Toint, Trust-Region Methods, SIAM, Philadelphia, USA, 2000. 13. M. T. Chu and M. M. Lin, Dynamical system characterization of the central path and its variants- a
vevisit, SIAM J Appl Dyn Syst 10 (2011), 887-905. 14. A. V. Fiacco and G. P. McCormick, Nonlinear programming: Sequential Unconstrained Minimization
Techniques, SIAM, 1990. 15. R. Fletcher, A new approach to variable metric algorithms, Comput. J. 13 (1970), 317-322. 16. R. Fletcher and M. J. D. Powell, A rapidly convergent descent method for minimization, Comput J 6
(1963), 163-168.

Tikhonov regularization continuation methods

27

17. B. S. Goh, Approximate greatest descent methods for optimization with equality constraints, J Optim Theory Appl 148 (2011), 505-527.
18. D. Goldfarb, A family of variable metric updates derived by variational means, Math Comput 24 (1970), 23-26.
19. G. H. Golub and C. F. Van Loan, Matrix Computations, 4th ed., The Johns Hopkins University Press, Baltimore, Mayryland, 2013.
20. E. Hairer, C. Lubich and G. Wanner, Geometric Numerical Integration: Structure-Preserving Algorithms for Ordinary Differential Equations, 2nd ed., Springer, Berlin, 2006.
21. P.C. Hansen, Regularization Tools: A MATLAB package for analysis and solution of discrete ill-posed problems, Numer Algorithms 6 (1994), 1-35.
22. U. Helmke and J. B. Moore, Optimization and Dynamical Systems, 2nd ed., Springer-Verlag, London, 1996.
23. D. J. Higham, Trust region algorithms and timestep selection, SIAM J Numer Anal 37 (1999), 194210.
24. Z. Jackiewicz and S. Tracogna, A general class of two-step Runge-Kutta methods for ordinary differential equations, SIAM J Numer Anal 32 (1995), 1390-1427.
25. C. T. Kelley, L.-Z. Liao, L. Qi, M. T. Chu, J. P. Reese and C. Winton, Projected Pseudotransient Continuation, SIAM J Numer Anal 46 (2008), 3071-3083.
26. N. H. Kim, Leture notes of constrained optimization, https://mae.ufl.edu/nkim/eas6939/ ConstrainedOpt.pdf, 2010.
27. J. H. Lee, Y. M Jung, Y. X. Yuan and S. Yun, A subsapce SQP method for equality constrained optimization, Comput Optim Appl 74 (2019), 177-194.
28. K. Levenberg, A method for the solution of certain problems in least squares, Q Appl Math 2 (1944), 164-168.
29. L.-Z. Liao, H. D. Qi, and L. Q. Qi, Neurodynamical optimization, J Glob Optim 28 (2004), 175-195. 30. X.-L. Luo, Singly diagonally implicit Runge-Kutta methods combining line search techniques for
unconstrained optimization, J Comput Math 23 (2005), 153-164. 31. X.-L. Luo, L.-Z. Liao and H.-W. Tam, Convergence analysis of the Levenberg-Marquardt method,
Optim Methds Softw 22 (2007), 659-678. 32. S.-T. Liu and X.-L. Luo, A method based on Rayleigh quotient gradient flow for extreme and interior
eigenvalue problems,Linear Algebra Appl 432 (2010), 1851-1863. 33. X.-L. Luo, A dynamical method of DAEs for the smallest eigenvalue problem, J Comput Sci 3 (2012),
113-119. 34. X.-L. Luo, J.-R. Lin and W.-L. Wu, A prediction-correction dynamic method for large-scale general-
ized eigenvalue problems, Abstr Appl Anal (2013), Article ID 845459, 1-8, http://dx.doi.org/ 10.1155/2013/845459. 35. X.-L. Luo, J.-H. Lv and G. Sun, Continuation methods with the trusty time-stepping scheme for linearly constrained optimization with noisy data, Optim Eng (2021), publised online at http: //doi.org/10.1007/s11081-020-09590-z, 1-35. 36. X.-L. Luo, H. Xiao and J.-H. Lv, Continuation Newton methods with the residual trust-region timestepping scheme for nonlinear equations, Numer Algorithms (2021), published online at http:// doi.org/10.1007/s11075-021-01112-x, 1-25. 37. X.-L. Luo and Y.-Y. Yao, Primal-dual path-following methods and the trust-region updating strategy for linear programming with noisy data, J Comput Math (2021), published online at http://doi. org/10.4208/jcm.2101-m2020-0173, 1-21. 38. X.-L. Luo, H. Xiao, J.-H. Lv and S. Zhang, Explicit pseudo-transient continuation and the trust-region updating strategy for unconstrained optimization, Appl Numer Math 165 (2021), 290-302. 39. X.-L. Luo and H. Xiao,Generalized continuation Newton methods and the trust-region updating strategy for the underdetermined system, arXiv preprint available at https://arxiv.org/abs/2103. 05829, submitted, March 9, 2021. 40. X.-L. Luo, J.-H. Lv and H. Xiao, Explicit continuation methods with L-BFGS updating formulas for linearly constrained optimization problems, arXiv preprint available at http://arxiv.org/abs/ 2101.07055, January 19, 2021. 41. M.F. Mascarenhas, The BFGS method with exact line searches fails for non-convex objective functions, Math Program 99 (2004), 49-61. 42. M.-W. Mak, Lecture notes of constrained optimization and support vector machines, http://www. eie.polyu.edu.hk/~mwmak/EIE6207/ContOpt-SVM-beamer.pdf, 2019. 43. MATLAB v9.8.0 (R2020a), The MathWorks Inc., http://www.mathworks.com, 2020.

28

Luo, Xiao

44. D. Marquardt, An algorithm for least-squares estimation of nonlinear parameters, SIAM J Appl Math 11 (1963), 431-441.
45. N. Maculan and C. Lavor, A function to test methods applied to global minimization of potential energy of molecules, Numer Algorithms, 35 (2004), 287-300.
46. J. Nocedal and S. J. Wright, Numerical Optimization, Springer-Verlag, Berlin, 1999. 47. M. J. Obsborne, Mathematical methods for economic theory, https://mjo.osborne.economics.
utoronto.ca/index.php/tutorial/index/1/mem, 2016. 48. P.-Q. Pan, New ODE methods for equality constrained optimization (2): algorithms, J Comput Math
10 (1992), 129-146. 49. M. J. D. Powell, Convergence properties of a class of minimization algorithms, in: O.L. Mangasarian,
R. R. Meyer and S. M. Robinson, eds., Nonlinear Programming 2, Academic Press, New York, pp. 1-27, 1975. 50. J. Schropp, A dynamical systems approach to constrained minimization, Numer Funct Anal Optim 21 (2000), 537-551. 51. J. Schropp, One and multistep discretizations of index 2 differential algebraic systems and their use in optimization, J Comput Appl Math 150 (2003), 375-396. 52. L. F. Shampine, I. Gladwell and S. Thompson, Solving ODEs with MATLAB, Cambridge University Press, Cambridge, 2003. 53. D. F. Shanno, Conditioning of quasi-Newton methods for function minimization, Math Comput 24 (1970), 647-656. 54. S. Surjanovic and D. Bingham, Virtual library of simulation experiments: Test functions and datasets, retrieved from http://www.sfu.ca/~ssurjano, January 2020. 55. W. Y. Sun and Y. X. Yuan, Optimization Theory and Methods: Nonlinear Programming, Springer, New York, 2006. 56. K. Tanabe, A geometric method in nonlinear programming, J Optim Theory Appl 30 (1980), 181-210. 57. A. N. Tikhonov, The stability of inverse problems, Dokl Akad Nauk SSRR 39 (1943), 176-179. 58. A. N. Tikhonov and V. Y. Arsenin, Solutions of Ill-posed Problems, John Wiley & Sons, New YorkToronto-London, 1977. 59. N. Ullah, J. Sabi'u and A. Shah, A derivative-free scaled memoryless BFGS method for solving a system of monotone nonlinear equations, Numer Linear Algebra Appl. 2021;e2374. https://doi. org/10.1002/nla.2374 60. R. B. Wilson, A Simplicial Method for Convex Programming, Ph.D. thesis, Harvard University, 1963. 61. H. Yamashita, A differential equation approach to nonlinear programming, Math Program 18 (1980), 155-168. 62. Y. Yuan, Recent advances in trust region algorithms, Math Program 151 (2015), 249-281.

