Improving low-resource ASR performance with untranscribed out-of-domain data
Jayadev Billa
Information Sciences Institute, University of Southern California, Marina del Rey, CA 90292, USA
jbilla@isi.edu

arXiv:2106.01227v1 [cs.CL] 2 Jun 2021

Abstract
Semi-supervised training (SST) is a common approach to leverage untranscribed/unlabeled speech data to improve automatic speech recognition performance in low-resource languages. However, if the available unlabeled speech is mismatched to the target domain, SST is not as effective, and in many cases performs worse than the original system. In this paper, we address the issue of low-resource ASR when only untranscribed out-of-domain speech data is readily available in the target language. Specifically, we look to improve performance on conversational/telephony speech (target domain) using web resources, in particular YouTube data, which more closely resembles news/topical broadcast data. Leveraging SST, we show that while in some cases simply pooling the out-of-domain data with the training data lowers word error rate (WER), in all cases, we see improvements if we train first with the out-ofdomain data and then fine-tune the resulting model with the original training data. Using 2000 hours of speed perturbed YouTube audio in each target language, with semi-supervised transcripts, we show improvements on multiple languages/data sets, of up to 16.3% relative improvement in WER over the baseline systems and up to 7.4% relative improvement in WER over a system that simply pools the out-of-domain data with the training data. Index Terms: acoustic modeling, domain adaptation, crossdomain learning
1. Introduction
Speech recognition systems rely on the availability of large amounts of labeled, i.e. transcribed speech data, preferentially data that is similar to the target environment in which these systems will be deployed, for optimal performance. Unfortunately for domains outside of broadcast media, collection of speech data is an involved process, e.g. if the target domain is conversational/telephony speech, it would involve a targeted data collection effort with the informed consent of 100s to 1000s of participants. Moreover, labeling/transcription of speech data at scale is both costly and time consuming. Given this state of affairs, it would be useful to find an approach that can bootstrap from a limited in-domain training set and use untranscribed out-of-domain speech data, in particular broadcast media and
This research is based upon work supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via contract # FA8650-17-C9116. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.

user generated content on YouTube, to improve performance in the target domain. In this paper, we combine semi-supervised training with transfer learning to demonstrate one approach to accomplish this goal.
Semi-supervised training (SST), an approach to use untranscribed speech to train ASR models, started with early efforts to harvest training data from speech transcribed with a bootstrap ASR system, followed by the selection of a subset of transcripts based on a threshold applied to the confidence in the truth of the transcript words [1]. This harvested data is then combined with existing labeled data to create a larger training set with which to build better performing models. Later efforts applied this approach to ever larger data collections, e.g. [2] and most recently to neural network based ASR systems, e.g. [3, 4, 5, 6, 7].
Transfer learning is the broad approach of using an existing trained model, or elements thereof, for a new task or domain; see [8] for a general treatment of transfer learning, [9] for a deep learning perspective, and [10, 11, 12] for a speech recognition modeling perspective. Fine-tuning, in the context of neural models, is a transfer learning approach that involves retraining an existing neural model for a new task/domain with a lower learning rate, with data from the target task.
Semi-supervised training has been an integral tool in our work on the IARPA MATERIAL [13] program. Briefly, the IARPA MATERIAL program goal is to allow English queries and information extraction across text and audio sources in lowresource non-English languages. A key element of information extraction from audio sources is the generation of transcripts for the audio via ASR. Separate from the low-resource aspect of the ASR training data, an additional complicating factor is that test audio documents are derived from a range of domains, with the majority taken from broadcast media or similar sources. In this particular scenario, we find that the availability of large amounts of broadcast media and user generated content, sourced from YouTube, allows us to overcome the lack of broadcast media like training data with extensive use of SST on the YouTube data. However, on conversational speech, due to the domain mismatch, SST with YouTube data typically does not help and in many cases hurts performance - leading to the problem scenario we describe above.
Our approach to addressing this problem is to first build a bootstrap model with the limited labeled in-domain data and then use that initial model with SST to build an out-of-domain ASR model. The traditional approach to SST involves pooling the labeled data with the high-confidence transcripts/audio from the unlabeled set ­ high-confidence meaning that we use an appropriate metric to select the best data to train on. In our work, we simply use a median threshold on the per frame likelihood as the selection criteria. As an alternative, we can omit the pooling stage and simply train on the newly labeled out-of-domain data

exclusively. Regardless of how we build the out-of-domain model, we can then fine-tune, essentially retraining with a lower learning rate, this model with the original in-domain training data. We find that this approach, across a variety of data sets, improves upon the original bootstrap model performance and, at least in the languages and data sets evaluated, consistently outperforms a model trained with pooling the original data with out-of-domain data in an SST framework.
In Section 2 we investigate this approach with a subset of the Switchboard (SWBD) corpus [14] as our in-domain training set, a subset of the Fisher corpus [15] as a control in-domain untranscribed set, and YouTube data to illustrate potential improvements. Section 3 details experiments on several MATERIAL languages using the program provided labeled data as well as data from YouTube. Finally, in Section 4, we conclude with a summary and implications of this work.
2. Switchboard experiments
2.1. Data sets and experimental setup
In our initial experiments, we will use a 150 hour subset of speed perturbed SWBD corpus (nominally 50 hours of original data), as our low-resource in-domain training set. Likewise, we select 2000 hours of speed perturbed Fisher corpus data (nominally 667 hours of Fisher data) as an in-domain control set. With Fisher, we experiment with both the original transcripts as well as transcripts generated by decoding the data with the bootstrap SWBD ASR model. For our out-of-domain data, we use a random selection of English broadcast media data available on YouTube, again 2000 hours after speed perturbation, and generate transcripts with the bootstrap SWBD model.
For evaluation, we use the HUB5 Eval2000 data set (LDC2002S09/LDC2002T43) which consists of both SWBD and CallHome subsets. The SWBD subset is similar to the training data, telephone conversations on a prescribed set of topics between individuals unfamiliar with one another, whereas CallHome, while still being telephone conversations, is mismatched in that they are conversations between family or close friends unconstrained by topic. From this perspective, CallHome may provide a better correspondence with performance improvements in real life. In these experiments, we constrain the language model to a trigram language model (LM) trained on the complete set of SWBD transcripts.
For ASR system development, we use the Kaldi toolkit [16], using the end-to-end LF-MMI approach [17] to building acoustic models using 40-dimensional MFCC features on speed perturbed [18] data. YouTube data is downsampled to 8kHz, to match the SWBD and Fisher data, before feature generation. In terms of the actual training process, we use Kaldi's existing SWBD and Fisher recipes1 to preprocess the Switchboard and Fisher data respectively for ASR model training. The trigram language model is also generated by the process detailed in the same SWBD recipe. As our canonical acoustic model architecture, we use a 15-layer TDNN-F [19] model architecture in all experiments in this paper. Lastly, with regards to training with end-to-end training LF-MMI, we use this approach for no other reason beyond that it offers a more streamlined process for experimentation.
1available at https://github.com/kaldi-asr/kaldi in egs/swbd/s5c and egs/fisher swbd/s5

2.2. Results
Table 1 summarizes the various experiments. We see the out-ofdomain YouTube data trained model underperforms the baseline system due to the mismatched domain; adding in SWBD data and training on the pooled data improves performance, though on SWBD, performance lags its baseline, and CallHome performance is essentially on par with its baseline performance. However, if we fine-tune the under-performing YouTube data or pooled data trained model, we eliminate the performance gap on SWBD. On CallHome, we observe a 6% relative improvement in WER compared to the baseline with both fine-tuned models. Furthermore, we note that performance after fine-tuning of both the pooled and YouTube only trained models is essentially the same.
Using Fisher, an "in-domain" data set, with original transcripts, we no longer see the degradation we observed with the YouTube data alone, and pooling the Fisher data with SWBD further improves performance. Similar to what we observed with the YouTube experiments, if the Fisher model is finetuned we see a further improvement. If we switch to using transcripts generated with the bootstrap SWBD model, apart from noting that performance lags that of models trained with true transcripts, we observe the same trends.
From these results, it appears that fine-tuning models, trained on out-of-domain data, can recover from domain mismatch and improve over both the baseline and models trained on the pooled data set.

Table 1: Comparison of WER improvements on the Eval2000 test set for Switchboard (SWBD) and CallHome (CH) subsets, using models trained on different variations of SWBD/YouTube (YT) and SWBD/Fisher data. Fisher data subset using original transcripts and semi-supervised transcripts are distinguished with OT/SST respectively. Pooled implies the model is trained with combined data, fine-tuning always uses the SWBD data. %RI refers to relative improvement over the corresponding baseline.

Training
SWBD (baseline)
YouTube + fine-tuning
Pooled YouTube/SWBD + fine-tuning
Fisher w/ OT Fisher + fine-tuning
Pooled Fisher/SWBD + fine-tuning
Fisher w/ SST Fisher + fine-tuning
Pooled Fisher/SWBD + fine-tuning

WER SWBD CH
12.5 24.8 15.5 26.9 12.5 23.3 13.8 24.7 12.3 23.5
12.5 22.2 11.3 21.3 11.2 20.5 10.8 20.7
12.4 22.8 11.6 22.1 12.1 22.6 11.5 22.3

%RI

SWBD CH

-

-

-24.0 -8.5 0.0 6.0

-10.4 0.4 1.6 5.2

0.0 10.5 9.6 14.1
10.4 17.3 13.6 16.5

0.8 8.1 7.2 10.9
3.2 8.9 8.0 10.1

3. MATERIAL experiments
3.1. Data sets and experimental setup
The SWBD experiments in Section 2 allowed us to compare and contrast performance with an in-domain data set. In reality, if we have in-domain data, we would train directly on it. The MATERIAL program offers an environment to test our approach in a realistic use case: a limited in-domain labeled training set, and otherwise unconstrained in terms of usage of audio/text resources obtained on the web. In particular, we will investigate performance on three MATERIAL program languages: Farsi, Kazakh, and Lithuanian. For the ASR component of the program, a small training set, the build set, consisting of approximately 40 hours of mostly telephony speech is provided, as well as a mixed speech (telephone, broadcast news, and conversation) test set, the analysis set. In this set of experiments, since we are focused on the indomain performance only, all results presented here pertain to the telephone/conversational speech subset of the analysis set. Table 2 summarizes both acoustic modeling and language modeling data. For each language, language modeling data is largely web crawl data, but we benefit from earlier work in addition to our web crawl efforts. Concretely, for Farsi, we use a subset of MirasText [20]2, and for Kazakh we additionally use a subset of the WMT19 Kazakh data.

Table 2: Data used in MATERIAL language experiments. Training data corresponds to each language's build data set provided as part of MATERIAL program. CS is conversational/telephone speech, WB (wide-band) groups the remaining non-telephone speech in the respective build set.

Language
Farsi Kazakh Lithuanian

Training data (hr)

CS

WB

36.3

-

43.0

6.5

46.3

7.0

LM data (#words)
1B 120M 170M

As our out-of-domain data, we first download audio from YouTube in each language and then decode with available inlanguage ASR models. The ASR models used in decoding the YouTube data were originally bootstrapped with the build data but were subsequently retrained with YouTube data using SST. Once transcripts for a 2000 hour subset of speed perturbed data are generated, we combine YouTube audio and generated transcripts to create an out-of-domain training set for each language. Once again the 2000 hour YouTube data for each language nominally represents 667 hours of original YouTube audio but is not speed perturbation of a specific 667 hour selection of YouTube audio, since transcripts are generated on and selected from speed perturbed data.
The ASR training setup remains as before, we use the same acoustic model architecture with end-to-end LF-MMI training. For language modeling, given the multiple sources of data, we use language model interpolation to combine source specific trigram LMs using the SRILM Toolkit [21] using weights that minimize the perplexity on the conversational speech subset of the respective language's analysis set. For any specific language, the LM is fixed across all experiments.
2details at https://github.com/miras-tech/MirasText

3.2. Results
Table 3 details results across all three languages. As observed on the SWBD experiments, we see again that the out-of-domain YouTube data by itself always hurts performance. Similarly, training on YouTube and build data always outperforms models trained only with the YouTube data, however, the improvement is not guaranteed to improve on the build only models ­ on Farsi, we see a 4% relative increase in WER with the pooled data trained model, but both Kazakh and Lithuanian benefit from the pooled data, with 12.2% relative and 11% relative decrease in WER over their respective baselines. Across all three languages, we see a consistent improvement in WER with fine-tuning. Again we observe that regardless of whether we fine-tune the YouTube only trained model or the pooled data trained model, the resulting models perform about the same.
Another general method used in low-resource languages is to apply transfer learning on a model in another language by replacing the output layers of the acoustic model with randomly initialized layers of the appropriate size for the target language. To compare our fine-tuning approach against this method, we take an existing GALE Arabic model trained on an 800 hour subset of GALE Arabic Phase 2-4 datasets (available from LDC) consisting of broadcast news and broadcast conversation in Arabic and retrain using the build data for each language. Table 3 also details the results of these transfer training experiments. In general, we see the fine-tuning approach compares favorably with transfer models; on Farsi and Kazakh the transfer model approach is slightly worse than fine-tuning, on Lithuanian, the transfer model performs better.

Table 3: Comparison of WER improvements on CS subset of the analysis set for MATERIAL languages. Fine-tuning is always with the language's respective build set.

Language Farsi
Kazakh
Lithuanian

Training
Build (baseline) YouTube + fine-tuning
Pooled YouTube/Build + fine-tuning
Arabic Transfer
Build (baseline)
YouTube + fine-tuning
Pooled YouTube/Build + fine-tuning
Arabic Transfer
Build (baseline)
YouTube + fine-tuning
Pooled YouTube/Build + fine-tuning
Arabic Transfer

%WER
45.4
60.4 43.9
47.2 43.7
44.3
55.1
68.8 46.1
48.4 46.4
47.0
48.9
63.4 42.9
43.5 42.8
41.3

%RI
-
-33.0 3.3
-4.0 3.7
2.4
-
-24.9 16.3
12.2 15.8
14.7
-
-29.7 12.3
11.0 12.5
15.5

4. Discussion
In this paper we demonstrate, over multiple languages and data sets, the utility of combining SST and fine-tuning to improve performance in a particular domain when we have limited indomain data and access only to out-of-domain YouTube data. We find that regardless of the choice of model to fine-tune, i.e. model trained on out-of-domain data only or pooled with the in-domain data, we always improve upon the baseline model. Depending on the context, one model may be more relevant/efficacious than the other, e.g. if pooled data trained models are readily available, as in our case, since we use incremental SST [22], it is more efficient to fine-tune the existing pooled data trained model. Alternatively, if training from scratch, it might be quicker to train the model on outof-domain data only before fine-tuning, or if ensembling the output of multiple systems, it would make sense to train and fine-tune both models. Lastly, we show that transfer modeling and our fine-tuning approach provide similar performance improvements, offering another technique that can be applied to low-resource ASR system development.
5. References
[1] G. Zavaliagkos, M. Siu, T. Colthurst, and J. Billa, "Using untranscribed training data to improve performance," in The 5th International Conference on Spoken Language Processing, Sydney Convention Centre, Sydney, Australia, 30th November - 4th December 1998. ISCA, 1998. [Online]. Available: http://www.isca-speech.org/archive/icslp 1998/i98 1007.html
[2] J. Z. Ma, S. Matsoukas, O. Kimball, and R. M. Schwartz, "Unsupervised training on large amounts of broadcast news data," in ICASSP 2006, Toulouse, France, May 14-19. IEEE, 2006, pp. 1056­1059. [Online]. Available: https://doi.org/10.1109/ICASSP.2006.1660839
[3] J. Z. Ma and S. Matsoukas, "Unsupervised training on a large amount of Arabic broadcast news data," in ICASSP 2007, Honolulu, Hawaii, USA, April 15-20. IEEE, 2007, pp. 349­352.
[4] K. Vesely´, M. Hannemann, and L. Burget, "Semi-supervised training of deep neural networks," in 2013 IEEE Workshop on Automatic Speech Recognition and Understanding, Olomouc, Czech Republic, December 8-12. IEEE, 2013, pp. 267­272.
[5] S. Thomas, M. L. Seltzer, K. Church, and H. Hermansky, "Deep neural network features and semi-supervised training for low resource speech recognition," in ICASSP 2013, Vancouver, BC, Canada, May 26-31. IEEE, 2013, pp. 6704­6708.
[6] V. Manohar, H. Hadian, D. Povey, and S. Khudanpur, "Semisupervised training of acoustic models using lattice-free MMI," in ICASSP 2018, Calgary, AB, Canada, April 15-20. IEEE, 2018, pp. 4844­4848.
[7] K. Yu, M. J. F. Gales, L. Wang, and P. C. Woodland, "Unsupervised training and directed manual transcription for LVCSR," Speech Communication, vol. 52, no. 7-8, pp. 652­663, 2010.
[8] S. J. Pan and Q. Yang, "A survey on transfer learning," IEEE Transactions on knowledge and data engineering, vol. 22, no. 10, pp. 1345­1359, 2009.
[9] Y. Bengio, "Deep learning of representations for unsupervised and transfer learning," in Proceedings of ICML workshop on unsupervised and transfer learning. JMLR Workshop and Conference Proceedings, 2012, pp. 17­36.
[10] P. Swietojanski, A. Ghoshal, and S. Renals, "Unsupervised crosslingual knowledge transfer in dnn-based LVCSR," in 2012 IEEE Spoken Language Technology Workshop (SLT), Miami, FL, USA, December 2-5, 2012. IEEE, 2012, pp. 246­251.
[11] J. Huang, J. Li, D. Yu, L. Deng, and Y. Gong, "Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers," in ICASSP 2013, Vancouver, BC,

Canada, May 26-31, 2013, pp. 7304­7308. [Online]. Available: https://doi.org/10.1109/ICASSP.2013.6639081
[12] P. Ghahremani, V. Manohar, H. Hadian, D. Povey, and S. Khudanpur, "Investigation of transfer learning for asr using lf-mmi trained neural networks," in 2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2017, pp. 279­286.
[13] "Machine Translation for English Retrieval of Information in Any Language (MATERIAL) program," https://www.iarpa.gov/index.php/research- programs/material .
[14] J. J. Godfrey, E. C. Holliman, and J. McDaniel, "Switchboard: telephone speech corpus for research and development," in [Proceedings] ICASSP-92: 1992 IEEE International Conference on Acoustics, Speech, and Signal Processing, vol. 1, 1992, pp. 517­ 520 vol.1.
[15] C. Cieri, D. Miller, and K. Walker, "The fisher corpus: a resource for the next generations of speechto-text," in Proceedings of the Fourth International Conference on Language Resources and Evaluation, LREC 2004, May 26-28, 2004, Lisbon, Portugal. European Language Resources Association, 2004. [Online]. Available: http://www.lrec-conf.org/proceedings/lrec2004/summaries/767.htm
[16] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz, J. Silovsky, G. Stemmer, and K. Vesely, "The kaldi speech recognition toolkit," in IEEE 2011 Workshop on Automatic Speech Recognition and Understanding. IEEE Signal Processing Society, Dec. 2011, iEEE Catalog No.: CFP11SRW-USB.
[17] H. Hadian, H. Sameti, D. Povey, and S. Khudanpur, "End-to-end speech recognition using lattice-free MMI," in INTERSPEECH 2018, Hyderabad, India, 2-6 September 2018, B. Yegnanarayana, Ed. ISCA, 2018, pp. 12­16. [Online]. Available: https://doi.org/10.21437/Interspeech.2018-1423
[18] T. Ko, V. Peddinti, D. Povey, and S. Khudanpur, "Audio augmentation for speech recognition," in INTERSPEECH 2015, Dresden, Germany, September 6-10, 2015, pp. 3586­3589. [Online]. Available: http://www.isca-speech.org/archive/interspeech 2015/i15 3586.html
[19] D. Povey, G. Cheng, Y. Wang, K. Li, H. Xu, M. Yarmohammadi, and S. Khudanpur, "Semi-orthogonal low-rank matrix factorization for deep neural networks," in INTERSPEECH 2018, Hyderabad, India, 2-6 September 2018, B. Yegnanarayana, Ed. ISCA, 2018, pp. 3743­3747.
[20] B. Sabeti, H. A. Firouzjaee, A. J. Choobbasti, S. H. E. M. Najafabadi, and A. Vaheb, "Mirastext: An automatically generated text corpus for persian," in Proceedings of the Eleventh International Conference on Language Resources and Evaluation, LREC 2018, Miyazaki, Japan, May 7-12, 2018, N. Calzolari, K. Choukri, C. Cieri, T. Declerck, S. Goggi, K. Hasida, H. Isahara, B. Maegaard, J. Mariani, H. Mazo, A. Moreno, J. Odijk, S. Piperidis, and T. Tokunaga, Eds. European Language Resources Association (ELRA), 2018. [Online]. Available: http://www.lrec-conf.org/proceedings/lrec2018/summaries/385.html
[21] A. Stolcke, "SRILM - an extensible language modeling toolkit," in 7th International Conference on Spoken Language Processing, ICSLP2002 - INTERSPEECH 2002, Denver, Colorado, USA, September 16-20, 2002, J. H. L. Hansen and B. L. Pellom, Eds. ISCA, 2002.
[22] B. K. Khonglah, S. R. Madikeri, S. Dey, H. Bourlard, P. Motl´icek, and J. Billa, "Incremental semi-supervised learning for multigenre speech recognition," in 2020 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2020, Barcelona, Spain, May 4-8, 2020. IEEE, 2020, pp. 7419­7423.

