FedHealth 2: Weighted Federated Transfer Learning via Batch Normalization for Personalized Healthcare
Yiqiang Chen1,2 , Wang Lu1,2 , Jindong Wang3 and Xin Qin1,2 1Institute of Computing Technology, Chinese Academy of Sciences, 100190, Beijing, China
2University of Chinese Academy of Sciences, 100190, Beijing, China 3Microsoft Research Asia, Beijing, China
{yqchen,luwang,qinxin18b}@ict.ac.cn, jindong.wang@microsoft.com

arXiv:2106.01009v1 [cs.LG] 2 Jun 2021

Abstract
The success of machine learning applications often needs a large quantity of data. Recently, federated learning (FL) is attracting increasing attention due to the demand for data privacy and security, especially in the medical field. However, the performance of existing FL approaches often deteriorate when there exist domain shifts among clients, and few previous works focus on personalization in healthcare. In this article, we propose FedHealth 2, an extension of FedHealth [Chen et al., 2020] to tackle domain shifts and get personalized models for local clients. FedHealth 2 obtains the client similarities via a pretrained model, and then it averages all weighted models with preserving local batch normalization. Wearable activity recognition and COVID-19 auxiliary diagnosis experiments have evaluated that FedHealth 2 can achieve better accuracy (10%+ improvement for activity recognition) and personalized healthcare without compromising privacy and security.
1 Introduction
For the past few years, machine learning technology has been widely used in real life. Machine learning technology leverages the labeled data to learn the patterns and trends of observed objects. With the help of perception technology, welltrained machine learning models liberate people from heavy and repetitive tasks, especially in the healthcare field. A successful machine learning healthcare application often requires sufficient user data to dig into the essence of health status. However, with the increasing awareness of privacy and security of the people and organizations, China, the European Union, and some other governments or organizations enforce the protection of user data via different regulations [Inkster, 2018; Voigt and Von dem Bussche, 2017]. In this situation, federated learning [Yang et al., 2019] came into being to protect data privacy and security.
The most classic algorithm in FL is FedAvg [McMahan et al., 2017], which directly averages the parameters of models
Corresponding Author

Cloud Model

Data A

Data B

Data C

User A

User B

......

User N

Figure 1: Data islanding and personalization in healthcare.

coming from all participating clients without transporting local data. Although FedAvg is simple, it has demonstrated superior performance in many situations. However, FedAvg is not designed particularly for non-iid data, which may make it suffer from performance degradation or even diverge when it is deployed over non-iid samples [Li et al., 2019]. As shown in Figure 1, different clients (users) cannot exchange their own data straightforward, and they exchange their information via model parameters with the help of the cloud. Various distributions of the client data make it hard to achieve good performance when directly employing FedAvg. The previous work, FedHealth [Chen et al., 2020], is the first federated transfer learning framework for wearable healthcare. Although it achieves great success in federated learning for healthcare and solves the problems of data islanding and personalization, it requires a large number of data shared across different clients, which is unrealistic in many situations.
In this article, we propose FedHealth 2, a weighted federated transfer learning algorithm via batch normalization for personalized healthcare. FedHealth 2 can solve both data islanding and personalization problems without sharing common data. Specifically, FedHealth 2 gets the similarities among clients with the help of a pretrained model. The similarities are determined by the distances of the data distributions, and the distances can be calculated via the statistical values of the layers' outputs of the pretrained network. After obtaining the similarities, the cloud averages the weighted models' parameters in a personalized way and generates a unique model for each client. Each client preserves its own

batch normalization and updates the model with a momentum method. Obviously, FedHealth 2 can not only cope with feature shift non-iid [Li et al., 2021] via keeping local batch normalization but also cope with some other shifts, such as label shifts by considering clients' similarities. In addition, all parameter exchange processes are encrypted to ensure that data is not leaked. FedHealth 2 is extensible and can be deployed to many healthcare applications with different expansion methods.
Our contributions are as follows.
1. We propose FedHealth 2, a weighted federated transfer learning algorithm via batch normalization for healthcare, which can aggregate the information from different clients without compromising privacy security, and achieve personalized models for clients through weighting models and preserving local batch normalization.
2. We show the excellent performance achieved by FedHealth 2 in healthcare applications. Experiments show that FedHealth 2 dramatically improves the recognition accuracy of the local clients' models. At the same time, it reduces the number of rounds and speeds up the convergence to some extent.
3. FedHealth 2 is extensible and can be employed in many healthcare applications. Diverse extensions allow it can work in many circumstances. Even if there does not exist a pretrained model, FedHealth 2 can get similarities via FedBN [Li et al., 2021] with few rounds.
2 Related Work
2.1 Machine Learning and Healthcare
With the rapid development of the technology of perception and computing, people can make use of machine learning to help doctors diagnose, assist doctors in the operation, etc. And with the help of perception technology, machine learning even can carry out disease warnings via daily behavior supervision. For instance, certain activities in daily life reflect early signals of some cognitive disease. Through daily observation of gait changes and finger flexibility, the machine can tell people whether they are suffering from Parkinson [Chen et al., 2017].
Unfortunately, a successful healthcare application needs a large amount of labeled data of users. However, in real applications, data are often separate and few people are willing to disclose their private data. In addition, an increasing number of regulations, such as [Inkster, 2018; Voigt and Von dem Bussche, 2017], hold back the leakages of data.
2.2 Federated Learning
Federated learning is a usual way to combine each client's information while protecting data privacy and security. It was first proposed by Google [McMahan et al., 2017], where they proposed FedAvg to train machine learning models via aggregating distributed mobile phones' information without exchanging data. The key idea is to replace direct data exchanges with model parameter-related exchanges. FedAvg is able to resolve the data islanding problems.

Though FedAvg works well in many situations, it still suffers from performance degradation or even diverges when meeting non-iid issues. Some works try to tackle these problems. FedProx [Li et al., 2018] tackled the heterogeneity by allowing partial information aggregation and adding a proximal term to FedAvg. [Yeganeh et al., 2020] aggregated the models of the clients with weights computed via L1 distance among client models' parameters. These works focus on a common model shared by all clients while some other works try to obtain a unique model for each client. [Arivazhagan et al., 2019] exchanged base layers' information and preserved personalization layer to combat the ill-effects of heterogeneity. [Dinh et al., 2020] utilized Moreau envelopes as clients' regularized loss function and decoupled personalized model optimization from the global model learning in a bi-level problem stylized for personalized FL. [Yu et al., 2020] evaluated three techniques for local adaptation of federated models: fine-tuning, multi-task learning, and knowledge distillation. Two works most relevant to our method are FedHealth [Chen et al., 2020] and FedBN [Li et al., 2021]. FedHealth proposed a federated transfer learning framework which needs some sharing data in healthcare while FedBN used local batch normalization to alleviate the feature shift before averaging models. Although there are already some works to cope with non-iid issues, few works pay attention to feature shift non-iid and other shifts at the same time and getting individual model for each client in healthcare.
2.3 Batch Normalization
Batch Normalization (BN) [Ioffe and Szegedy, 2015] has been an indispensable component of deep learning since it was created. Batch Normalization improves the performance of the model and has a natural advantage in dealing with domain shifts. Nowadays, researchers have explored many effects of BN, especially in transfer learning [Segu` et al., 2020]. FedBN [Li et al., 2021] is one of few applications of BN in the field of FL field. However, FedBN does still not make full use of BN properties, and it does not consider the similarities among the clients.

3 Method

3.1 Problem Formulation

In a FL problem, there are N different clients (organiza-

tions or users), denoted as {C1, C2, · · · , CN } and each client

has its own dataset, i.e. {D1, D2, · · · , DN }. Each dataset, Di = {(xi,j, yi,j)}nj=i 1, contains two parts, i.e. a train dataset

Ditrain

=

{(xti,rjain

,

yit,rjain

)}ntirain
j=1

and

a

test

dataset

Ditest

=

{(xti,ejst

,

yit,ejst

)}ntiest
j=1

.

Obviously,

we

have

ni

=

ntirain + ntiest

and Di = Ditrain  Ditest. All of the datasets have different

distributions, i.e. P (Di) = P (Dj). Each client has its own

model denoted as {fi}Ni=1. Our goal is to combine informa-

tion of all clients to learn a good model fi for each client on

its local dataset Di without private data leakage:

1N

1 ntiest

min N {fk }N k=1

i=1 ntiest

j=1

(fi(xti,ejst), yit,ejst),

(1)





Cloud Model

Model A Data A

Model B Data B







Model C Data C

Pretrained Model

User A

User B

......

User N

Figure 2: The overall structure of the FedHealth 2 method for federated learning.

where is a loss function.

3.2 Overview of FedHealth 2

FedHealth 2 aims to achieve accurate personal healthcare

through weighted federated transfer learning via local batch

normalization without compromising data privacy and secu-

rity. Figure 2 gives an overview of the structure. Without loss

of generality, we assume there are three clients, which can be

extended to the more general case. The structure mainly con-

tains five parts. Firstly, the server distributes the pretrained

model to each client. Secondly, each client computes statis-

tics of the outputs of specific layers according to local data.

Thirdly, the server obtains clients' similarities represented by

the weight matrix W which will be used to guide aggrega-

tion. Fourthly, each client updates its own model with the

local train data and pushes their models to the cloud. Finally,

the server aggregates models and obtains N models delivered

to N clients respectively. All processes do not involve the di-

rect transmission of data, so FedHealth 2 avoids the leakage

of data privacy to a certain extent.

The keys of FedHealth 2 are obtaining W and aggregating

the models. The way of computing W will be introduced

in the following, and we first describe how to aggregate the

models after obtaining W.

We denote the parameters of each model fi as i = i  i. i corresponds to the parameters of BN layers while i corresponds to the parameters of the other layers. W is an

N × N matrix, which describes the similarities among the

clients. wij demonstrates the similarity between client i and

client j. The bigger wij is, the more similar the two clients

are. We have wii = , where  is a hyper-parameter, and

N j=1

wij

=

1.

Figure 3 demonstrates the process of aggregating the mod-

els. As shown in Figure 3, i is fixed while i is computed according to wi, where wi means the ith rows of W, and , where  = {i}Ni=1. Let ti = ti  ti represents the parameters of the model form client i in the round t, then we

pool Conv BN pool Conv

FC FC BN pool Conv BN pool Conv

FC FC BN pool Conv BN pool Conv

Data A Data B Data C

......

FC BN

FC

 Model A

... Model B

... Model C

New Model A
New Model B
New Model C

Particular Sharing

Figure 3: The concrete process of the FedHealth 2.

have

ti+1 = ti

ti+1 =

N j=1

wij

tj

.

(2)

The overall process of FedHealth 2 is described in Algorithm 1. In next sections, we will introduce how to compute the weight matrix W with or without a pretrained model.

3.3 Evaluate Weights With a Pretrained Model

In this section, we will evaluate the weights with a pretrained

model f and propose two ways to compute the weights.

We denote with a l  {1, 2, · · · , L} in superscript nota-

tions the different batch normalization layers in the model. And zi,l represents the input of lth batch normalization layer

in the ith client. The input of the classify layer in the ith client is denoted as zi which represents the domain features.

We

assume

zi,l

is

a

matrix,

zi,l
ci,l ×si,l

where

ci,l

corresponds

to the channel number while si,l is the product of the other

dimensions. Similarly, zi = zici×si . We feed Di into f , and

we

can

obtain

z . i,l
ci,l ×si,l

Obviously,

si,l

=

e

×

ni

where

e

is

an integer. Now, we try to compute statistics on the channels,

and we treat zi,l as a Gaussian distribution. For the lth layer

Algorithm 1 FedHealth 2
Input: A pretrained model f , N clients' datasets {Di}Ni=1,  Output: Client models {fi}Ni=1
1: Distribute f to each client
2: Each client computes its statistics (µi, i), where µi represents the mean values while i represents the covariance matrixs. Push (µi, i) to the cloud
3: Compute W according to the statistics 4: Update the client model with local data. Push {ti}Ni=1 to
the cloud 5: Update {ti}Ni=1 according to Eq. 2 and distribute
{ti+1}Ni=1 to the corresponding clients 6: Repeat steps 4  5 util convergence or maximum round

of the ith client, it is easy to obtain its distribution parameters, N (µi,l, i,l). And the ith client's statitics are denoted as

(µi, i) = [(µi,1, i,1), (µi,2, i,2), · · · , (µi,L, i,L)]. (3)
Now we can calculate the distance between two clients. We
usually utilize Wasserstein distance to calculate the distance between two Gaussian distributions. According to [Peyre´ et al., 2019],

W22(N (µi,l, i,l), N (µj,l, j,l))

=||µi,l - µj,l||2+

(4)

tr(i,l + j,l - 2((i,l)1/2j,l(i,l)1/2)1/2),

where tr is the trace of the matrix. Obviously, it is too dif-
ficulty to perform efficient calculations. Similar to BN, we
perform approximations and consider that each channel is independent of each other. Therefore, i,l is a diagonal matrix, i.e. i,l = Diag(ri,l). Now, we have

W22

(N

(µi,l,

i,l),

N 

(µj,l,

j,l)) 

(5)

=||µi,l - µj,l||2 + || ri,l - rj,l||22.

Therefore, we have

L
di,j = W2(N (µi,l, i,l), N (µj,l, j,l))

l=1

(6)

L



= (||µi,l - µj,l||2 + || ri,l - rj,l||22)1/2.

l=1

Big di,j means the distribution distance between the ith

client and the jth client is large. Therefore, the bigger di,j is,

the less similar two clients are, which means the smaller wi,j

is. And we set w~i,j as the inverse of di,j, i.e. w~i,j = 1/di,j.

Since, di,i = 0, we let wi,i =  and

N j=1,j=i

wi,j

=

1

- .

Normalize w~i and we have

,

j=i

wi,j =

(1 - ) ×

, j = i w~i,j

N j=1,j=i

w~i,j

(7)

We denote this weighting method as FedHealth 2. Similarly, for zi, we can obtain the corresponding W and we
denote this method as d-FedHealth 2.

class
input mean/var
class

3.4 Evaluate Weights Without a Pretrained Model
Sometimes, there does not exist a pretrained model. In this situation, we can evaluate weights with models trained from several rounds of FedBN.

1000 800

mean var

600

400

200

0 0 200 400 600 800 1000 running_mean/running_var

Figure 4: The running mean, running var of a BN layer and the inputs statistics of the corresponding layer in a client model.

As we can see from Figure 4, the running mean of the BN layer has a positive correlation with the statistical mean of the corresponding layer's inputs. And the variance has a similar relationship. From this, we can use running means and running variances of the BN layers instead of the statistics respectively. Therefore, we can perform several rounds of FedBN [Li et al., 2021] which preserves local batch normalization, and utilize parameters of BN layers replacing the statistics when there does not exist a pretrained model. We denote this extension as f-FedHealth 2.
4 Experiments
4.1 Datasets

10

9

8

7

6

5

4

3

2

1

5

10

15

20

client

4

3

2

1

5

10

15

20

client

(a) PAMAP

(b) COVID-19

Figure 5: The number of samples per class allocated to each client

(indicated by dot size).

PAMAP. We adopt a public human activity recognition dataset called PAMAP [Reiss and Stricker, 2012]. The PAMAP dataset contains data of 18 different physical activities, performed by 9 subjects wearing 3 inertial measurement units and a heart rate monitor. We use data of 3 inertial measurement units which are collected at a constant rate of 100Hz to form data containing 27 channels. We exploit the sliding window technique and filter out 10 classes of data to obtain 17639 instances in total. In order to construct the problem situation in FedHealth 2, we use the Dirichlet distribution as in [Yurochkin et al., 2019] to create disjoint non-iid. client

Client
Base FedAvg FedBN FedProx FedPer
FedHealth 2 d-FedHealth 2 f-FedHealth 2

1
92.86 60.27 60.72 60.50 48.31
77.20 64.33 64.11

2
17.68 62.36 62.59 62.36 97.51
77.55 77.55 77.78

3
100.00 50.56 50.34 50.34 61.40
77.43 78.33 69.53

4
83.52 73.98 73.53 73.98 47.29
79.64 77.38 79.86

5
18.78 74.27 74.72 73.81 58.47
81.94 79.91 77.88

6
77.66 62.90 62.44 61.76 23.98
79.86 80.77 74.43

7
95.05 64.03 62.90 63.57 49.55
86.20 85.52 84.62

8
17.58 87.78 88.24 87.78 91.86
95.02 92.53 93.67

9
92.39 74.49 74.27 74.27 51.24
85.33 86.23 74.04

10
93.37 64.71 64.48 64.71 77.60
69.23 69.23 81.00

11
29.12 65.24 65.69 66.37 89.16
91.42 87.58 79.91

12
84.78 63.35 62.90 63.12 57.92
79.41 80.09 71.95

13
98.90 68.33 68.33 68.33 42.53
74.43 74.66 74.21

14
24.18 64.79 65.24 65.69 49.44
69.75 70.43 62.98

15
98.91 63.12 62.44 62.44 58.60
81.67 83.71 78.05

16
98.90 85.26 85.94 85.49 86.62
94.10 93.88 89.57

17
41.44 66.21 65.99 66.21 77.32
82.77 80.50 79.59

18
93.62 59.64 59.64 59.41 52.38
75.74 74.60 68.71

19
85.71 67.87 68.10 67.87 73.08
77.15 78.73 71.95

20
37.02 72.46 72.69 72.46 97.52
86.91 87.13 87.81

avg
69.07 67.58 67.56 67.52 64.59
81.14 80.16 77.08

Table 1: Activity recognition results of 20 clients on PAMAP. Bold means the best result while underline means the second best result.

training data. Figure 5(a) visualizes how samples are distributed among 20 clients for PAMAP. In each client, half of the data are used to train and the remaining data are for testing.
COVID-19. We also adopt a public COVID-19 posterioranterior chest radiography images dataset [Sait et al., 2020]. This is a combined curated dataset of COVID-19 Chest Xray images obtained by collating 15 public datasets and it contains 9,208 instances of four classes (1281 COVID-19 XRays, 3270 Normal X-Rays, 1656 viral-pneumonia X-Rays, and 3001 bacterial-pneumonia X-Rays) in total. In order to construct the problem situation in FedHealth 2, we split the dataset similar to PAMAP. Figure 5(b) visualizes how samples are distributed among 20 clients for COVID-19. In each client, half of the data are used to train and the remaining data are for testing.
4.2 Implementations Details and Comparison Methods
For PAMAP, we adopt a CNN for training and predicting. The network is composed of two convolutional layers, two pooling layers, two batch normalization layers, and two fully connected layers. For COVID-19, we adopt Alexnet [Krizhevsky et al., 2012]. We use a three-layer fully connected neural network as the classifier with two BN layers after the first two fully connected layers following [Li et al., 2021]. For model training, we use the cross-entropy loss and SGD optimizer with a learning rate of 10-2. If not specified, our default setting for local update epochs is E = 1 where E means training epochs in one round. In addition, we randomly select 20% of the data to train a model of the same architecture as the pretrained model.
We did not compare with FedHealth [Chen et al., 2020] since our method focuses on more general setting where clients do not share large volume of datasets. We compare three extensions of our method with five methods including common federated learning methods and some federated learning methods designed for non-iid data particularly:
· Base: Each client uses local data to train local models.
· FedAvg [McMahan et al., 2017]: The cloud aggregate all client models without any particular operations for non-iid data.
· FedBN [Li et al., 2021]: Each client preserves the local batch normalization.
· FedProx [Li et al., 2018]: Allow partial information aggregation and add a proximal term to FedAvg.

· FedPer [Arivazhagan et al., 2019]: Each client preserves some local layers.
4.3 Classification Accuracy
The classification results for each client on PAMAP are shown in Table 1. From these results, we have the following observations: 1) No matter how the weights are calculated, our method achieves the best effects on average. It is obvious that our method significantly outperforms other methods with a remarkable improvement (over 10% on average). 2) In some clients, the base method achieves the best test accuracy. As it can be seen from Figure 5(a), the distributions on the clients are very inconsistent, which inevitably leads to generate the different difficulty levels in different clients. And some distributions in the corresponding clients are so easy that only utilizing the local data can achieve the ideal effects. 3) FedBN does not achieve the desired results. This could be caused by that FedBN is designed for the feature shifts while our experiments are mainly set in the label shifts.

Method Base FedAvg

avg

92.70 86.48

FedBN 82.26

FedProx 86.15

Method FedPer FedHealth 2 d-FedHealth 2 f-FedHealth 2

avg

88.51 94.82

94.97

93.5

Table 2: Average accuracy of 20 clients on COVID-19

The classification results for each client on COVID-19 are shown in Table 2. From these results, we have the following observations: 1) No matter how the weights are calculated, our method achieves the best effects. However, in those experiments, our methods only have slight improvements. The classification accuracy only improves by 2% on average. It may be caused by that the COVID-19 dataset has much fewer classes compared with PAMAP. Henceforth, the task is much easier. 2) The base method achieves acceptable results in this dataset. From Figure 5(b), we can see that there are only a few classes in some clients, which means it is easy to obtain the desired results with few data. 3) FedBN gets the worst results. This demonstrates that FedBN is not good at dealing with label shifts. And FedBN does not consider the similarities among different clients.
4.4 Ablation Study
Effects of Weighting. To demonstrate the effect of weighting which considers the similarities among the different clients, we compare the average accuracy on PAMAP and COVID-19 between the experiments with it and without it. Without weighting, our method degenerates to FedBN. From

Figure 6(a), we can see that our method performs much better than FedBN which does not include the weighting part. Moreover, from Figure 6(b), we can see our method performs better than FedBN on all clients. These results demonstrate that our method with weighting can cope with the label shifts while FedBN cannot deal with this situation, which means our method is more applicable and effective.

Average Accuracy (%)

100

100.0

90

Average Accuracy (%)
accuracy (%)

95

Without Weighting

97.5

90

Weighting

94.97 95.0

80

85 80

81.14

75

70 67.56

65

60 PAMPA

92.5

90.0

87.5

85.0

82.26

82.5

COVID-19 80.0

70

60

Weighting

50

Without Weighting 5 client1n0umber15 20

(a) Average Accuracy

(b) Client Acc on PAMAP

Figure 6: The effects of weighting.

Average Accuracy (%)

96.0

84

Sharing Common BN

Preserving local BN

95.5

82

81.14

94.97 95.0

80 78 78.10
76 PAMPA

94.21

94.5

94.0

93.5

COVID-19 93.0

Average Accuracy (%) accuracy (%)

95

LBN

90

SBN

85

80

75

70

65

5 10 15 20 client number

(a) Average Accuracy

(b) Client Acc on PAMAP

Figure 7: The effects of preserving local batch normalization.

Effects of Preserving Local Batch Normalization. In this part, we illustrate the function of preserving local batch normalization. Figure 7(a) shows the average accuracy between the experiments with preserving local batch normalization and the experiments with sharing common batch normalization while Figure 7(b) shows the results on each client. LBN means preserving local batch normalization while SBN means sharing common batch normalization. Obviously, the improvements are not particularly significant compared with weighting. This may be caused by there mainly exist the label shifts in our experiments while preserving local batch normalization is for the feature shifts. However, our method still has a slight improvement, which shows the superiority of our method.
4.5 Parameter Sensitivity
In this section, we evaluate the parameter sensitivity of FedHealth 2 and we use FedHealth 2. Our method is affected by three parameters: local epochs, client number, and . We change one parameter and fix the other parameters. In Figure 8(a), we can see our method is best and it is descending with local epochs increasing, which may be caused that we keep the total number of the epochs unchanged and the communication among the clients are insufficient. From Figure 8(b), we can see that our method still achieves acceptable results. When the client numbers increase, our method goes down which may due to that few data in local clients make the weight estimation inaccurate. And we may take f-FedHealth 2 instead. Figure 8(c)-8(d) demonstrates  slightly affects the

Average Accuracy (%)

80

75

70

65

60

FedBN FedAvg

55

FedHealth 2

1

3

5 10 20

Local Epochs

(a) Local Epochs

Average Accuracy (%)

80

FedBN

75

FedAvg

FedHealth 2

70

10

20

50

100

Client Number

(b) Client Number

80

78

76

FedHealth 2

74

FedBN

72

70

68

0.1 0.3 0.5 0.7 0.9 

Acc (%)

80

70

60

0.1

50

0.3

40

0.5 0.7

30

0.9

0 50 100 150 200 250 300 350 400 Rounds

(c) 

(d) 

Figure 8: Influence of different hyper-parameters.

Average Accuracy (%)

average accuracy of our method while it can change the convergence rate. The results reveal that FedHealth 2 is more effective and robust than other methods under different parameters in most cases.
4.6 Convergence
In this section, we investigate the convergence. In Figure 9, we can see our method almost convergences in the 10th round. And in the actual experiments, 20 rounds are enough for our method while FedBN needs over 400 rounds.

Average Acc (%)

80

70

60

50

40

30 20

FedBN FedAvg FedHeath 2

10 0 50 100 150 200 250 300 350 400

Rounds

Figure 9: Convergence.

5 Conclusions And Future Work
In this article, we propose FedHealth 2, a weighted federated transfer learning algorithm via batch normalization for personalized healthcare. FedHealth 2 aggregates the data from different organizations without compromising privacy security and achieves relatively personalized model learning through combing considering similarities and preserving local batch normalization. Experiments have evaluated the effectiveness of FedHealth 2. FedHealth 2 follows FedHealth and continues to move forward in the direction of federated learning for healthcare. In the future, we plan to apply FedHealth 2 to more personalized and flexible healthcare.

References
[Arivazhagan et al., 2019] Manoj Ghuhan Arivazhagan, Vinay Aggarwal, Aaditya Kumar Singh, and Sunav Choudhary. Federated learning with personalization layers. arXiv preprint arXiv:1912.00818, 2019.
[Chen et al., 2017] Yiqiang Chen, Xiaodong Yang, Biao Chen, Chunyan Miao, and Hanchao Yu. Pdassist: Objective and quantified symptom assessment of parkinson's disease via smartphone. In 2017 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), pages 939­945. IEEE, 2017.
[Chen et al., 2020] Yiqiang Chen, Xin Qin, Jindong Wang, Chaohui Yu, and Wen Gao. Fedhealth: A federated transfer learning framework for wearable healthcare. IEEE Intelligent Systems, 35(4):83­93, 2020.
[Dinh et al., 2020] Canh T Dinh, Nguyen H Tran, and Tuan Dung Nguyen. Personalized federated learning with moreau envelopes. arXiv preprint arXiv:2006.08848, 2020.
[Inkster, 2018] Nigel Inkster. China's cyber power. Routledge, 2018.
[Ioffe and Szegedy, 2015] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pages 448­456. PMLR, 2015.
[Krizhevsky et al., 2012] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25:1097­1105, 2012.
[Li et al., 2018] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. arXiv preprint arXiv:1812.06127, 2018.
[Li et al., 2019] Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of fedavg on non-iid data. arXiv preprint arXiv:1907.02189, 2019.
[Li et al., 2021] Xiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, and Qi Dou. Fedbn: Federated learning on non-iid features via local batch normalization. arXiv preprint arXiv:2102.07623, 2021.
[McMahan et al., 2017] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial Intelligence and Statistics, pages 1273­1282. PMLR, 2017.
[Peyre´ et al., 2019] Gabriel Peyre´, Marco Cuturi, et al. Computational optimal transport: With applications to data science. Foundations and Trends® in Machine Learning, 11(5-6):355­607, 2019.
[Reiss and Stricker, 2012] Attila Reiss and Didier Stricker. Introducing a new benchmarked dataset for activity monitoring. In 2012 16th International Symposium on Wearable Computers, pages 108­109. IEEE, 2012.

[Sait et al., 2020] U. Sait, Lkv Gokul, S. Prajapati, R. Bhaumik, T Kumar, S Shivakumar, and K. Bhalla. Curated dataset for covid-19 posterior-anterior chest radiography images (x-rays). 2020.
[Segu` et al., 2020] Mattia Segu`, Alessio Tonioni, and Federico Tombari. Batch normalization embeddings for deep domain generalization. arXiv preprint arXiv:2011.12672, 2020.
[Voigt and Von dem Bussche, 2017] Paul Voigt and Axel Von dem Bussche. The eu general data protection regulation (gdpr). A Practical Guide, 1st Ed., Cham: Springer International Publishing, 10:3152676, 2017.
[Yang et al., 2019] Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. Federated machine learning: Concept and applications. ACM Transactions on Intelligent Systems and Technology (TIST), 10(2):1­19, 2019.
[Yeganeh et al., 2020] Yousef Yeganeh, Azade Farshad, Nassir Navab, and Shadi Albarqouni. Inverse distance aggregation for federated learning with non-iid data. In Domain Adaptation and Representation Transfer, and Distributed and Collaborative Learning, pages 150­159. Springer, 2020.
[Yu et al., 2020] Tao Yu, Eugene Bagdasaryan, and Vitaly Shmatikov. Salvaging federated learning by local adaptation. arXiv preprint arXiv:2002.04758, 2020.
[Yurochkin et al., 2019] Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang, and Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks. In International Conference on Machine Learning, pages 7252­7261. PMLR, 2019.

