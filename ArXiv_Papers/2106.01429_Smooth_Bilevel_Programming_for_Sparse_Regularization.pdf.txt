Smooth Bilevel Programming for Sparse Regularization
Clarice Poon, Gabriel Peyré June 4, 2021

arXiv:2106.01429v1 [stat.ML] 2 Jun 2021

Abstract
Iteratively reweighted least square (IRLS) is a popular approach to solve sparsity-enforcing regression problems in machine learning. State of the art approaches are more efficient but typically rely on specific coordinate pruning schemes. In this work, we show how a surprisingly simple reparametrization of IRLS, coupled with a bilevel resolution (instead of an alternating scheme) is able to achieve top performances on a wide range of sparsity (such as Lasso, group Lasso and trace norm regularizations), regularization strength (including hard constraints), and design matrices (ranging from correlated designs to differential operators). Similarly to IRLS, our method only involves linear systems resolutions, but in sharp contrast, corresponds to the minimization of a smooth function. Despite being non-convex, we show that there is no spurious minima and that saddle points are "ridable", so that there always exists a descent direction. We thus advocate for the use of a BFGS quasi-Newton solver, which makes our approach simple, robust and efficient. We perform a numerical benchmark of the convergence speed of our algorithm against state of the art solvers for Lasso, group Lasso, trace norm and linearly constrained problems. These results highlight the versatility of our approach, removing the need to use different solvers depending on the specificity of the ML problem under study.

1 Introduction

Regularized empirical risk minimization is a workhorse of supervised learning, and for a linear model, it reads

1

min R() + L(X, y)

Rn



(P)

where X  Rm×n is the design matrix (n being the number of samples and m the number of features), L : Rm × Rm  [0, ) is the loss function, and

Department of Mathematical Sciences, University of Bath, Bath BA2 7AY, UK

cmhsp20@bath.ac.uk

CNRS and DMA, Ecole Normale Supérieure, PSL University,

45 rue d'Ulm, F-75230 PARIS cedex 05, FRANCE, gabriel.peyre@ens.fr

1

R : Rn  [0, ) the regularizer. Here  0 is the regularisation parameter which is typically tuned by cross-validation, and in the limit case  = 0, (P0) is a constraint problem min R() under the constraint L(X, y) = 0.
In this work, we focus our attention to sparsity enforcing penalties, which induce some form of structure on the solution of (P), the most celebrated examples (reviewed in Section 2) being the Lasso, group-Lasso and trace norm regularizers. All these regularizers, and much more (as detailed in Section), can be conveniently re-written as an infimum of quadratic functions. While Section 2 reviews more general formulations, this so-called "quadratic variational form" is especially simple in the case of block-separable functionals (such as Lasso and group-Lasso), where one has

1 R() = min

||g ||22

+

1 h(),

(1)

Rk+ 2 gG g

2

where G is a partition of {1, . . . , n}, k = |G| is the number of groups and h : Rk+  [0, ). An important example is the group-Lasso, where R() = g ||g|| is a group- 1 norm, in which case h() = i i. The special case of the Lasso, corresponding to the 1 norm is obtained when g = {i} for i = 1, . . . , n and k = n. This quadratic variational form (1) is at the heart of the Iterative Reweighted Least Squares (IRLS) approach, reviewed in Section 1.1. We refer to Section 2 for an in-depth exposition of these formulations.
Sparsity regularized problems (P) are notoriously difficult to solve, especially for small , because R is a non-smooth function. It is the non-smoothness of R which forces the solutions of (P) to belong to low-dimensional spaces (or more generally manifolds), the canonical example being spaces of sparse vectors when solving a Lasso problem. We refer to [3] for an overview of sparsity-enforcing regularization methods. The core idea of our algorithm is that a simple reparameterization of (1) combined with a bi-level programming (i.e. solving two nested optimization problems) can turn (P) into a smooth program which is much better conditioned, and can be tackled using standard but highly efficient optimization techniques such as quasi-Newton (L-BFGS). Indeed, by doing the change of variable (vg, ug) (g, g/g) in (1), (P) is equivalent to

min f (v) where f (v)
vRk

1 min h(v uRn 2

v)

+

1 ||u||2

+

1 L(X (v

2



G u), y).

(2)

Throughout, we define to be the standard Hadamard product and for v 

Rk and u  Rn, we define v G u  Rn to be such that (v G u)g = vgug.

Provided that v  h(v v) is differentiable and L(·, y) is a convex, proper,

lower semicontinuous function, the inner minimisation problem has a unique

solution and f is differentiable. Moreover, in the case of the quadratic loss

L(z, y)

1 2

||z

-

y||22,

the

gradient

of

f

can

be

computed

in

closed

form,

by

solving

a linear system of dimension m or n. This paper is thus devoted to study the

theoretical and algorithmic implications of this simple twist on the celebrated

IRLS approach.

2

ISTA Noncvx-Pro

Figure 1: Evolution of 20 coefficients via ISTA and gradient descent of f .
Comparison with proximal gradient To provide some intuition about the proposed approach, Figure 1 contrasts the iterations of a gradient descent on f and of the iterative soft thresholding algorithm (ISTA) on the Lasso. We consider a random Gaussian matrix X  R10×20 with  = ||X y||/10. The ISTA trajectory is non-smooth when some feature crosses 0. In particular, if a coefficient (such as the red one on the figure) is initialized with the wrong sign, it takes many iteration for ISTA to flip sign. In sharp contrast, the gradient flow of f does not exhibit such a singularity and exhibits a smooth geometric convergence. We refer to the appendix for an analysis of this phenomenon.
Contributions Our main contribution is a new versatile algorithm for sparse regularization, which applies standard smooth optimization methods to minimize the function f (v) in (2). We first propose in Section 2 a generic class of regularizers R which enjoys a quadratic variational form. This section recaps existing results under a common umbrella and shows the generality of our approach. Section 3.1 then gathers the theoretical analysis of the method, and in particular the proof that while being non-convex, the function f (v) has no local minimum and only "ridable" saddle points. As a result, one can guarantee convergence to a global minimum for many optimisation schemes, such as gradient descent with random perturbations [35, 33] or trust region type methods [44]. Furthermore, for the case of the group Lasso, we show that f is an infinitely differentiable function with uniformly bounded Hessian. Consequently, standard solvers such as Newton's method/BFGS can be applied and with a superlinear convergence guarantee. Section 4 performs a detail numerical study of the method and benchmarks it against several popular competing algorithms for Lasso, group-Lasso and trace norm regularization. Our method is consistently among the best performers, and is in particular very efficient for small values of , and can even cope with the constrained case  = 0.
1.1 Related works
State of the art solvers for sparse optimisation One of the most popular approaches to nonsmooth sparse optimisation are proximal based methods. The
3

simplest instance is the Forward-Backward algorithm [36, 18] which handles the case where L is a smooth function. There are many related inertial based acceleration schemes, such as FISTA [6] and particularly effective techniques which leads to substantial speedups are the adaptive use of stepsizes and restarting strategies [43]. Other related approaches are proximal quasi-Newton and variable metric methods [17, 7], which incorporate variable metrics into the quadratic term of the proximal operator. Another popular approach, particularly for the Lasso-like problems are coordinate descent schemes [23]. These schemes are typically combined with support pruning schemes [26, 40, 37]. In the case where both the regularisation and loss terms are nonsmooth (e.g. in the basis pursuit setting), typical solvers are the primal-dual [14], ADMM [11] and DouglasRachford algorithms [21]. Although these schemes are very popular due to their relatively low per iteration complexity, these methods have sublinear convergence rates in general, with linear convergence under strong convexity.

The quadratic variational formulation and IRLS Quadratic variational

formulations such as (1) have been exploited in many early computer vision

works, such as [24] and [25]. One of the first theoretical results can be found in

[25], see also [10] which lists many examples. Our result in Theorem 1 can be

seen as a generalisation of these early works. Norm regularizers of this form were

introduced in [38], and further studied in the monograph [3] under the name

of subquadratic norms. The main algorithmic consequence of the quadratic

variational formulation in the literature is the Iterative reweighted least squares

(IRLS) algorithm.

When L(z, y) =

1 2

||z

-

y||22

is the quadratic loss, a natural

optimisation strategy is alternating minimising. However, due to the 1/g term,

one needs to introduce some regularisation to ensure convergence. One popular

approach

is

to

add

 2

g g-1 to the formulation (1) which leads to the IRLS

algorithm [19]. and one minimise (P) over both  and . The -term can be seen

as a barrier to keep g positive which is reminiscent of interior point methods.

The idea of IRLS is to do alternating minimisation, where the minimisation

with respect to  is a least squares problem, and the minimisation with respect

to  admits a closed form solution. A nuclear norm version of IRLS has been

used in [1] where an alternating minimisation algorithm was introduced. Finally,

we remark that although nonconvex formulations for various low complexity

regularizers have appeared in the literature, see for instance [32] in the case

of the 1 norm, and [31] in the case of the nuclear norm, they are typically

associated with alternating minimisation algorithms.

Variable projection/reduced gradient approaches IRLS methods are

however quite slow because the resulting minimization problem is poorly condi-

tioned.

Adding

the

smoothing

term

 2

g g-1 only partly alleviates this, and

also breaks the sparsity enforcing property of the regularizer R. We avoid both

issues in (2) by solving a "reduced" problem which is much better conditioned

and smooth. This idea of solving a bi-variate problem by re-casting it as a

bilevel program is classical, we refer in particular to [48, Chap. 10] for some

4

general theoretical results on reduced gradients, and also Danskin's theorem

(although this is restricted to the convex setting) in [8]. Our formulation falls

directly into the framework of variable projection [49, 28], introduced initially for

solving nonlinear least squares problems. Properties and advantages of variable

projection have been studied in [49]. Nonsmooth variable projection is studied

in [54], although this work is in the classical setting of variable projection due

to our smooth reparametrization. Reduced gradients have also been associated

with the quadratic variational formulation in several works [3, 46, 47]. The idea

is

to

apply

descent

methods

over

g()

=

min R0(, ) +

1 2

||X



- y||22.

Although

the function over  and  is discontinuous, the function g over  is smooth and

one can apply first order methods, such as proximal gradient descent to minimise

g under positivity constraints. While quasi-Newton methods can be applied in

this setting with bound constraints, we show in Section 4.1 that this approach is

typically less effective than our nonconvex bilevel approach. In the setting of

the trace norm, the optimisation problem is constrained on the set of positive

semidefinite matrices, so one is restricted to using first order methods.

2 Quadratic variational formulations

We describe in this section some general results about when a regulariser has a quadratic variational form. Our first result brings together results which are scattered in the literature: it is closely related to Theorem 1 in [25], but their proof was only for strictly concave differentiable functions and did not explicitly connect to convex conjugates, while the setting for norms have been characterized in the monograph [3] under the name of subquadratic norms.

Theorem 1. Let R : Rn  R. The following are equivalent:

(i) R() = ( ) where  is proper, concave and upper semi-continuous,

with domain Rd+.

(ii)

There

exists

a

convex

function



for

which

R()

=

inf zRn+

1 2

(z).

n i=1

zii2

+

Furthermore, (z) = (-)(-z/2) is defined via the convex conjugate (-) of

-, leading to (1) using the change of variable   1/z and h() = 2(1/).

When R is a norm, the function h can be written in terms of the dual norm R as

h() = maxR(w) 1 i wi2i. Moreover, R()2 = infRn+ i i2i-1 \ h() 1 .

See Appendix A for the proof to this Theorem. Some additional properties of  are derived in Lemma 1 of Appendix A, one property is that if R is coercive, then lim||z||0 (z) = +, so the function f is coercive (see also remark 2).

2.1 Examples
Let us first give some simple examples of both convex and non-convex norms: ­ Euclidean norms: for R = || · ||2, making use of R as stated in Theorem 1,
one has h = || · ||.

5

­ Group norms: for G is a partition of {1, . . . , n}, the group norm is R() =

gG ||g||. Using the previous result for the Euclidean norm, one has h() =

gG ||(i)ig||. This expression can be further simplified to obtain (1) with

a

reduced

vector



in

|G|
R+

in

place

of

Rn

by

noticing

that

the

optimal



is

constant in each group g.

­ q (quasi) norms: For R() = ||q where q  (0, 2), one has (u) = uq/2 and

one verifies

that

q
h() = Cq 2-q

where

Cq

= (2 - q)qq/(2-q).

Note that

for

q

>

2 3

,

v



h(v2)

=

v

for



>

1

is

differentiable.

Analysis

and

numerics

for

this nonconvex setting can be found in Appendix E.

Matrix regularizer The extension of Theorem 1 to the case where  = B is a matrix can be found in the appendix. When R = (BB ) is a function on matrices, the analogous quadratic variational formulation is

1 R(B) = min min

tr(B

Z -1 B )

+

1 h(Z ).

(3)

2 ZSn+ BRn×r

g

2

where Sn+ denotes the set of symmetric positive semidefinite matrices and h(Z) = 2(-)(-Z-1/2). Letting U = Z-1/2B and V = Z1/2, we have B = V U and
the equivalence

min
B

R(B)

+

1 2

||A(B)

-

y||22

(4)

= min f (V )
V Rn×n

min
U Rn×r

1 2

||U

||2F

+

1 h(V
2

V

)

+

1 2

||A(V

U)

-

y||22.

(5)

where A : Rn×r  Rm is a linear operator. Again, provided that V  h(V V )

is differentiable, f is a differentiable function with f (V ) = V h(V V ) +

1 

A(A(V

U

)

-

y))U

and U such that U + V A(A(V U ) - y) = 0. For the

trace norm, R(B) = tr( B B), we have h(Z) = tr(Z) and h(Z) = Id. Note

that, just in the vectorial case, one could write the inner minimisation problem

over the dual variable   Rm and handle the case of  = 0.

3 Theoretical analysis

Our first result shows the equivalence between (P) and a smooth bilevel problem. The proofs are in Appendix B.

Theorem 2. Denote Ly L(·, y) and let Ly denote the convex conjugate of Ly. Assume that Ly is a convex, proper lower semicontinuous function and R takes
the form (1). The problem (P) is equivalent to

min f (v)
vRk

1 min h(v uRn 2

v)

+

1 ||u||2 2

+

1  Ly

(X (v

G u)) .

(6)

1 = max h(v
Rm 2

v)

-

1 

Ly

()

-

1 ||v
2

G (X )||22.

(7)

6

where v G u (ugvg)gG. The minimiser  to (P) and the minimiser v to (6) are related by  = v G u = -v2 G X . Provided that v  h(v v) is
differentiable, the function f is differentiable with gradient

f (v) = v h(v2) - v G ||Xg ||2 g

(8)

where





argmax~

-Ly (~ )

-

1 ||v
2

G (X ~)||22.

Note that f is uniquely defined even if  is not unique. If  > 0 and Ly is

differentiable,

then

we

have

the

additional

formula,

where

u



argminu~

1 2

||u~||22

+

1 

Ly

(X

(v

G u~))

f (v) = v

h(v2) + 1 

ug, Xg Ly(X(v

G u)

.
gG

(9)

For the Lasso and basis pursuit setting, the gradient of f can be computed in closed form:

Corollary 1. Ly(z) = z -

If y,

R has a quadratic variational

Ly() =

y, 

+

1 2

||||22

and

form and Ly(z) the gradient of

= f

1 2

||z

- y||22,

then

can be written

as in (8) and additionally (9) when  > 0. Furthermore,   Rm in (8) and

u  Rn in (9) solves

(X diag(v¯2)X + Id) = -y,

(10)

(diag(v¯)X X diag(v¯) + Id)u = v G (X y),

(11)

where v¯  Rn is defined as v¯ u = (vgug)gG for all u  Rn and Id denotes the identity matrix.

This shows that our method caters for the case  = 0 with the same algorithm in a seamless manner. This is unlike most existing approach which work well for  > 0 (and typically do not require matrix inversion) but fails when  is small, whereas solvers dedicated for  = 0 might require inverting a linear system, see Section 4.4 for an illustrative example.

3.1 Properties of the projected function f

In this section, we analyse the case of the group Lasso. The following theorem ensures that the projected function f has only strict saddle points or global minimums. We say that v is a second order stationary point if f (v) = 0 and 2f (v) 0. We say that v is a strict saddle point (often called "ridable") if it is a stationary point but not a second order stationary point. One can thus always find a direction of descent outside the set of global minimum. This can be exploited to derive convergence guarantees to second order stationary points for trust region methods [44] and gradient descent methods [35, 33].

Theorem 3. In the case h(z) =

i zi

and

L(z, y)

=

1 2

||z

-

y||22,

the

projected

function f is infinitely continuously differentiable and for v  Rk, f (v) =

7

v

1 - ||2

where

g

=

1 

Xg

(X (u

v) - y) and u solves the inner least squares

problem for v. Let J denote the support of v, by rearranging the columns and

rows, the Hessian of f can be written as the following block diagonal matrix

2f (v) =

diag(1 - ||g||22)gJ + 4U W U 0

0 diag(1 - ||g||22)gJc

(12)

where W Id -  (vgXg Xhvh)g,hJ + IdJ -1 and U is the block diagonal matrix with blocks (g)gJ , with maxgG ||g||2 C and ||f (v)|| 1 + 3C2 where
C ||y||2 maxgG ||Xg||/. Moreover, all stationary points of f are either global minimums or strict saddles. At stationary points, the eigenvalues of the Hessian of f are at most 4 and is at least

min 4(1 - /( + ^)), min(1 - ||g||2)
gJ

where ^ is the smallest eigenvalue of (vgXg Xhvh)g,hJ .

The proof can be found in Appendix B. We simply mention here that by

examining the first order condition of (P), we see that  is a minimizer if and

only

if



satisfies

-g

=

g ||g ||2

for

all

g

 Supp()

and

||g ||2

1 for all g  G.

The first condition on the support of  is always satisfied at stationary points of

the nonconvex function (2), and by examining (12), the second condition is also

satisfied unless the stationary point is strict.

Remark 1 (Example of strict saddle point for our f ). One can observe that v = 0 is a strict saddle point, as the solution to the associated linear system yields u = 0 and hence f (v) = 0. If  ||X y||, then u = v = 0 corresponds to a global minimum, otherwise, it is clear to see that there exists g such that 1 - ||g||2 < 0 and v = 0 is a strict saddle point.
Remark 2. Since f  C, it is Lipschitz smooth on any bounded domain. As mentioned, f is coercive when R is coercive, and hence, its sublevel sets are bounded. So, for any descent algorithm, we can apply results based on kf being Lipschitz smooth for all k.

Remark 3. The nondegeneracy condition that ||g||2 < 1 outside the support of v and invertibility of (Xg Xh)g,hJ is often used to derive consistency results [4, 58]. By Proposition 1, we see that this condition guarantees that the Hessian of f is positive definite at the minimum, and hence, combining with the smoothness properties of f explained in the previous remark, BFGS is guaranteed to converge superlinearly for starting points sufficiently close to the optimum [41, Theorem 6.6].

4 Numerical experiments
In this section, we use L-BFGS [13] to optimise our bilevel function f and we denote the resulting algorithm "Noncvx-Pro". One observation from our numerics

8

below is that although Noncvx-Pro is not always the best performing, unlike other solvers, it is robust to a wide range of settings: for example, our solver is mostly unaffected by the choice of  while one can observe in Figures 2 and 3 that this has a large impact on the proximal based methods and coordinate descent. Moreover, Noncvx-Pro is simple to code and rely on existing robust numerical routines (Cholesky/ conjugate gradient + BFGS) which naturally handle sparse/implicit operators, and we thus inherit their nice convergence properties. All numerics are conducted on 2.4 GHz Quad-Core Intel Core i5 processor with 16GB RAM. The code to reproduce the results of this article is available online1.

4.1 Lasso

We first consider the Lasso problem where R() =

n i=1

|i

|.

Datasets. We tested on 8 datasets from the Libsvm repository2. These datasets are mean subtracted and normalised by m.

Solvers. We compare against the following 10 methods: 1. 0-mem SR1: a proximal quasi newton method [7].

2. FISTA w/ BB: FISTA with Barzilai­Borwein stepsize [5] and restarts [43].

3. SPG/SpaRSA: spectral projected gradient [55].

4. CGIST: an active set method with conjugate gradient iterative shrinkage/thresholding [27].

5. Interior point method: from [34].

6. CELER: a coordinate descent method with support pruning [37].

7. Non-cvx-Alternating-min: alternating minimisation [32] of (2).

8. Non-cvx-LBFGS: directly solving (2) by L-BFGS.

9. L-BFGS-B: [13] applies L-BFGS-B under positivity constraints to minu,vRn+ i ui+

i

vi

+

1 2

||X

(u

-

v)

-

y||22.

10. Quad-variational: Based on our idea of Noncvx-Pro, another natural (and to

our knowledge novel) approach is to apply L-BFGS-B to the bilevel formulation

of (1) without nonconvex reparametrization. Indeed, by applying (1) and using

convex duality, the Lasso can solved by minimizing g()

maxRm

1 2

i i -

 2

||||2

-

1 2

i i| xi,  |2 +

,

y

.

The

gradient

of

g

is

g()

=

 2

-

1 

|X

|2

where | · |2 is in a pointwise sense and  maximises the inner problem, and

we apply L-BFGS-B with positivity constraints to minimise g.

1 https://github.com/gpeyre/2021-NonCvxPro 2 https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/

9

leukemia (38,7129)

mnist (60000,683)



1 2

max

1 10

max

1 50

max

Figure 2: Lasso comparisons with different regularisation parameters.

Experiments. The results are shown in Figure 2 (with further experiments in the appendix). We show comparisons at different regularisation strengths, with  being the regularisation parameter found by 10 fold cross validation on the mean squared error, and max = ||X y|| is the smallest parameter at which the Lasso solution is guaranteed to be trivial.

4.2 Group Lasso

The multi-task Lasso [29] is the problem (6) where one minimises over

  Rn×q, the observed data is y  Rm×q and R() =

n j=1

|| j ||2

with

j



Rq

denotes

the

j th

row

of

the

matrix



and

the

loss

function

is

1 2

||y

-

X



||2F

in

terms

of the the Frobenius norm.

Datasets. We benchmark our proposed scheme on a joint MEG/EEG data from [39]. X denotes the forward operator with n = 22494 source locations, and is constructed from 301 MEG sensors and 59 EEG sensors, so that m = 360. The observations y  Rm×q represent time series measurements at m sensor with q = 181 timepoints each. The  corresponds to the source locations which are assumed to remain constant across time.

Solvers. We perform a comparison against FISTA with restarts and BB step,

the spectral projected gradient method SPG/SpaRSA, and CELER. Since n is

much larger than q and m, we use for NonCvx-Pro the saddle point formulation

(7) where the maximisation is over   Rm×q. Computation of  in f (v)

involves

solving

(Idm

+

1 

X

diag(v2)X

) = y., that is, q linear systems each of

size m.

Experiments. Figure 3 displays the objective convergence plots against run-
ning time for different :  = max/r with max = maxi ||Xi y||2 for r = 10, 20, 50, 100. We observe substantial performance gains over the benchmarked

10



=

1 10

max



=

1 20

max



=

1 50

max



=

1 100

max

Figure 3: Comparisons for multitask Lasso on MEG/EEG data

methods: In MEG/EEG problems, the design matrix tends to exhibit high correlation of columns and proximal-based algorithms tend to perform poorly here. Coordinate descent with pruning is known to perform well here when the regularisation parameter large [37], but its performance deteriorates as  increases.
4.3 Trace norm
In multi-task feature learning [1], for each task t = 1, . . . , T , we aim to find ft : Rn  R given training examples (xt,i, yt,i)  Rn × R for i = 1, . . . , mt. One approach is to jointly solve these T regression problems by minimising (4) where R is the trace norm (also called "nuclear norm"), r = T , y = (yt)Tt=1, A(B) = (XtBt)Tt=1 with Xt being the matrix with ith row as xt,i and Bt being the tth column of the matrix B. Note that letting ut  Rn denote the tth column of U , the computation of U in f (V ) involves solving T linear systems (Idn + V Xt XtV )ut = V Xt yt. Here, the trace norm encourages the tasks to share a small number of linear features.
Datasets. We consider the three datasets commonly considered in previous works. The Schools dataset 3 [1] which consists of the scores of 15362 students from 139 schools. There are therefore T = 139 tasks with 15362 = t mt data points in total, and the goal is to map n = 27 student attributes to exam performance. The SARCOS dataset 4 [56, 57] has 7 tasks, each corresponding to learning the dynamics of a SARCOS anthropomorphic robot arms. There are n = 21 features and m = 48, 933 data points, which are shared across all tasks. The Parkinsons dataset [53] 5 which is made up of m = 5875 datapoints from T = 42 patients. The goal is to map n = 19 biomarkers to Parkinson's disease symptom scores for each patient.
Solvers. Figure 4 reports a comparison against FISTA with restarts and IRLS. The IRLS algorithm for (4) is introduced in [1] (see also [3]), and applies alternate minimisation after adding the regularisation term  tr(Z-1)/2 to
3 https://home.ttic.edu/~argyriou/code/ 4 http://www.gaussianprocess.org/gpml/data/ 5 http://archive.ics.uci.edu/ml/datasets/Parkinsons+Telemonitoring

11

Schools

Parkinsons

SARCOS

Figure 4: Comparisons for multi-task feature learning, IRLS-d corresponds to IRLS with  = 10-d.

(3). The update of B is a simple least squares problem while the update for

Z is Z  (BB

+

Id)

1 2

.

Our

nonconvex

approach

has

the

same

per-iteration

complexity as IRLS, but one advantage is that we directly deal with (4) without

any  regularisation.

Remark 4. Quad-variational mentioned in Section 4.1 does not extend to the trace norm case, since the function g would be minimised over Sn+, for which the application of L-BFGS-B is unclear. For this reason, bilevel formulations for the
trace norm [46] have been restricted to the use of first order methods.

Experiments. For each dataset, we compute the regularisation parameter  by 10-fold cross-validation on the RMSE averaged across 10 random splits. Then, with this regularisation parameter, we compare Non-convex-pro, FISTA and IRLS with different choices of . The convergence plots are show in Figure 4. We observe substantial computational gains for the Schools and Parkinson's dataset. For the SARCOS dataset, IRLS performed the best, and even though Nonconvex-pro is comparatively less effective here, although we remark that the number of tasks is much smaller (T = 7) and the recorded times are much shorter (less than 0.2s). Further numerical illustrations with synthetic data are shown in the appendix ­ our method is typically less sensitive to problem variations.
4.4 Constraint Group Lasso and Optimal Transport
A salient feature of our method is that it can handle arbitrary small regularization parameter  and can even cope with the constrained formulation, when  = 0, which cannot be tackled by most state of the art Lasso solvers. To illustrate this, we consider the computation of an Optimal Transport (OT) map, which has recently gained a lot of attention in ML [45]. We focus on the Monge problem, where the ground cost is the geodesic distance on either a graph or a surface (which extends original Monge's problem where the cost is the Euclidean distance). This type of OT problems has been used for instance to analyze and process brain signals in M/EEG [30], for application in computer graphics [52] and is now being applied to genomic datasets [51]. As explained for

12

instance in [50, Sec.4.2], the optimal transport between two probability measures a and b on a surface can be computed by advecting the mass along a vector field v(x)  R3 (tangent to the surface) with minimum vectorial L1 norm ||v(x)||dx (where dx is the surface area measure) subject to the conservation of mass div(v) = a - b. Once discretized on a 3-D mesh, this boils down to solving a constrained group Lasso problem (P0) where g  R3 is a discretization of v(zg) at some vertex zg of the mesh, X is a finite element discretization of the divergence using finite elements and y = a - b. The same applies on a graph, in which case the vector field is aligned with the edge of the graph and the divergence is the discrete divergence associated to the graph adjacency matrix, see [45]. This formulation is often called "Beckmann problem".
Datasets. We consider two experiments: (i) following [30] on a 3-D mesh of the brain with n = 20000 vertices with localized Gaussian-like distributions a and b (in blue and red), (ii) a 5-nearest neighbors graph in a 7-dimensional space of gene expression (corresponding to 7 different time steps) of baker's yeast, which is the dataset from [20]. The n = 614 nodes correspond to the most active genes (maximum variance across time) and this results in a graph with 1927 edges. The distributions are synthetic data, where a is a localized source whereas b is more delocalized.
Solvers. We test our method against two popular first order schemes: the Douglas-Rachford (DR) algorithm [36, 16] (DR) and the primal-dual scheme of [14]. DR is used in its dual formulation (the Alternating Direction Method of Multipliers ­ ADMM) but on a re-parameterized problem in [52]. The details of these algorithms are given in Appendix D.
Experiments. Figure 5 shows the solution of this Beckmann problem in these two settings. While DR has the same complexity per iteration as the computation of the gradient of f (resolution of a linear system), a chief advantage of PD is that it only involves the application of X and X et each iterations. Both DR and PD have stepping size parameters (denoted µ and ) which have been tuned manually (the rightmost figure shows two other sub-optimal choices of parameters). In contrast, our algorithm has no parameter to tune and is faster than both DR and PD on these two problems.
5 Conclusion
Most existing approaches to sparse regularisation involve careful smoothing of the nonsmooth term, either by proximal operators or explicit regularisation as in IRLS. We propose a different direction: a simple reparameterization leads to a smooth optimisation problem, and allows for the use of standard numerical tools, such as BFGS. Our numerical results demonstrate that this approach is versatile, effective and can handle a wide range of problems.
13

0.5

0

-0.5

-1

-1.5

-2

-2.5 NonCvx-Pro

-3

DR, =0.1

PD, =0.1

5

10

15

20

25

30

35

1

0.5

0

-0.5

-1

-1.5

NonCvx-Pro DR, =0.001

DR, =0.01

-2

DR, =0.05

PD, =0.1

-2.5

PD, =1

PD, =10

50

100

150

200

250

Figure 5: Resolution of Beckmann problem on a graph (left) and a 3-D mesh (right). The probability distributions a and b are displayed in blue and red and the optimal flow  is represented as green segments (on the edge of the graph and on the faces of the mesh). The convergence curves display the decay of log10(||t -  ||1) during the iterations of the algorithms (DR=Douglas-Rachford, PD=Primal-Dual) as a function of time t in second.

Acknowledgments
The work of G. Peyré was supported by the French government under management of Agence Nationale de la Recherche as part of the "Investissements d'avenir" program, reference ANR19-P3IA-0001 (PRAIRIE 3IA Institute) and by the European Research Council (ERC project NORIA).

A Proofs and additional results for Section 2

Proof to Theorem 1. To show that i) implies ii), recall that a convex, proper
and lower semicontinuous function - can be written in terms of its convex conjugate which has domain Rd-. By writing 2  , using the definition of R, we have

-R(2) = -(2) = sup 2, v - (-)(v) = - inf 2, u + (-)(-u).

v0

u0

which is ii) with (u) (-)(-u) as required. Conversely, if R is of the form in ii), then

R() = inf u, 2 + (u) = - sup - u, 2 - (u),

uRn+

uRn+

so R() = -(- ) and -(-·) is clearly a proper, upper semicontinuous, concave function.
For the expression of  when R is a norm,from the above, we know that

14

 = (-)(-z), and recall that for any norm, R() = maxR(w) 1 w,  . So,

(z) = max -u, z + (u)
u0

= max -2, z + (2) = max -2, z + R()





= max -z, 2 + max , w = max 1 wi2 ,



R(w) 1

R(w) 1 4
i

zi

where in the line, we swapped the two maximums and used the optimality condi-

tion over  which is 2

z

=

w.

That

is,

h()

=

2(-

1 2

)

=

maxR(w)

1

i wi2i.

To derive the identity for R()2, by the Cauchy-Schwarz inequality

R()2 = sup | , w |2
R(w) 1
for all  > 0. Therefore,

sup
R(w) 1

i2i
i

wi2 i i

= 4() i2i
i

R()2

inf
 0,()

1 4

i

i2i

1

= sup inf (() - ) +

>0  0

4

i

i2i

=

sup

 -

+

 R(/ )

=

 sup -

+

 R()

=

R()2.

>0 4

>0 4

 where we used the identity R(/ ) = i i2i + () and the fact that R is one positive homogeneous.

We derive some properties of the function h:
Lemma 1. Consider the function  and  from Theorem 1. If  : [0, )  [L, U ], where L > - and U  R  {+}, then  is an decreasing function with domain contained in [0, ), taking values in [L, U ]. If R is coercive, then lim||z||0 (z) = +.
Proof to Lemma 1. Let  : [0, )  [L, U ], where L > - and U  R  {+}. We describe the properties of the function (z) = (-)(-z) = supu 0 z, -u + (u).
(i) dom()  [0, ): since  is bounded below, it is clear that for z < 0, supu 0 z, -u + (u) = +.
(ii) (0) = supu 0 (u) = U .
(iii) Suppose M sup {v \ v  (u), u 0} < . Then, for all z M , -z + (u) < 0 for all u 0 and hence, (z) = (0) L. Therefore,  takes values in [L, U ]. So, if M is finite, then one can restrict the optimisation over z to values in [0, M ].

15

(iv)  is decreasing: By Danskin's theorem [8, Prop. B.25], for z  {v \ v  (u), u 0}, (z) = -u \ u  argminu 0 u, -z + (u)  (-, 0).

A.1 Functions on matrices

We have the following result for matrix valued functions. Let Sn+ denote the set of symmetric positive semidefinite matrices.

Theorem 4. Let R : Rn×m  R. The following are equivalent

i) R(B) = (BB ) where  is a proper concave upper semi-continuous function with domain Sn+.
ii) There exists a convex function  such that R(B) = minZSn+ tr(B ZB) +  (Z ).

Moreover, we have (Z) = (-)(-Z). If R is a norm, then  can be written

as

(Z) =

max

1 tr(W

Z-1W ).

(13)

R(W ) 1 4

Moreover,

R(B)2 = inf

tr(B ZB) \ (Z)

1 .

(14)

Z Sn+

4





Nuclear norm R(W ) = tr( W W ) where · is the matrix square root. On

the space of symmetric positive semidefinite matrices, (B) = tr( B) is concave

and

(D)

=

1 4

tr(D-1),

where

we

use

A

tr(

A) = (2

A)-1 for all symmetric

positive semidefinite matrices and A tr(AB) = B.

(Nonconvex) spectral regularisation Given a symmetric psd matrix Z = U diag(i)U and  > 0, let Z U diag(i)U . For   (0, 1), consider R(W ) = tr((W W )/2) = i i where i are the singular values of W . Then, given a symmetric psd matrix, (Z) = tr(Z/2) which is concave [9, Thm 4.2.3]
and

(Z) = min - tr(V Z) + (V ) = min - tr(diag()U ZU

V Sd+

U Od,Rd+

)+

i

i/2

= min
U Od,

-
0

i

Z^iii +

i

i/2

where



= C min
U Od

i

Z^ii -2

where Z^ = U ZU


= C tr(Z -2 )

Z^ = U ZU and U  Od

Therefore,


R(B) = inf tr(B ZB) + C tr(Z -2 ).
Z Sd+

16

Proof of Theorem 4. To derive (13),

(Z) = max - U, Z + (U ) = max - V V , Z + (V V )

U Sn+

V Rn

= max - V V , Z + R(V )
V Rn

Then, (13) follows, since by convex duality and definition of R,

max

1 tr(W

Z-1W ) =

max

max -Z, V V

+ V, W = max -Z, V V

+ R(V ).

R(W ) 1 4

R(W ) 1 V

V

Finally, by the submultiplicative property of the Frobenius norms, for all Z  Sn+ with Z 0,

R(B)2 = sup | Z-1/2W, Z1/2B |2
R(W ) 1
= 4(W ) tr(B ZB)

sup tr(W Z-1W ) tr(B ZB)
R(W ) 1

It follows that just as in the proof of Theorem 1 that

R(B)2

inf tr(B ZB) where (Z)

1 .

Z Sn+

4

B Proof of Section 3

Ptuhgreo=oqfuoagfd/Trahteigco.rveaTmrhia2et.ieoTqnuhaielvafeolqernmuciveoaftleoRn,(c7ea)nbfdeotltlwhoeewescnhba(yPngceo) noavfnevdxar(di6au)ballieistyvsigmo=npltyhedgui,neanntedor minimisation problem, that is

f

(v)

=

min
u

G1(v,

u)

1 h(v2) 2

+

1 ||u||2 2

+

1  Ly(X(v

G u))

=

min
u,z

1 h(v2) 2

+

1 ||u||2 2

+

1  Ly(z)

where

z = X(v

G u)

=

min
u,z

max


1 h(v2) 2

+

1 ||u||2 2

+

1  Ly(z)

-

, z

+

, X(v

G u)

= max min 1 h(v2) + 1 ||u||2 + , X(v

 u2

2

G u)

-

1 

Ly

().

Using the optimality condition over u, we obtain u = -v G X  and hence,

f (v) = max G2(v, )


1 h(v2)

-

1 ||v

2

2

GX

||22

-

1 

Ly

().

By [48, Theorem 10.58], if the following set is

S(v) vG2(v, ) = v - v G (||Xg ||22)gG \   argmin G2(v, )

17

singled valued, then f is differentiable with f (v) = vG2(v, ) for  
argmin G2(v, ). Observe that even if argmin G2(v, ) is not single valued, since G2 is strongly convex for v G X , S(v) is single-valued and hence, f is
a differentiable function.
In the case where Ly is differentiable, we can again apply [48, Theorem 10.58], to obtain f (v) = v h(v2) + vG1(v, y) with u = argminu G1(v, u) (noting that G1(v, ·) is strongly convex and has a unique minimiser) which is precisely
the gradient formula (9).

Proof of Proposition 3. Let

G(u, v)

1 ||u||2 2

+

1 2

||v||22

+

1 ||X (v
2

G u) - y||22.

We know from Theorem 2 that f is differentiable with

f (v) = vG(u, v) = v + -1u X (Xv G u - y)

where u = argminu G(u, v). In particular, 0 = uG(u, v) = u + -1X (X(v G u) - y).

Since uuG = -1(vgXg Xhvh)g,h + Id is invertible, by the implicit function theorem u is a smooth function of v with vu = [uuG]-1vuG. In particular,

2f (v) = vvG(u, v) + uvG(u, v)vu.

So, the Hessian of f is the Schur complement of the Hessian of G (as also

observed in [49, 54]). We write 2G =

A B

B D

where

A

vvG = -1

ug Xg Xhuh

+ Id
g,h

B uvG = -1 (ug Xg Xhvh)g,h + diag(g )

D uuG = -1(vgXg Xhvh)g,h + Id

we have 2f (t) = A - BD-1B . Note that in fact, u is infinitely differentiable by the implicit function theorem, and so, f is also infinitely differentiable.
We now derive a formula for the Hessian of f . By permuting the rows and columns of 2G, we can assume that, letting J denote the support of v,

A

-1 ug Xg Xhuh g,hJ + IdJ

0

0

IdJ c

B

-1

(ug Xg Xhvh)g,hJ + diag(g )gJ 0

D

-1(vgXg Xhvh)g,hJ + IdJ 0

0

IdJ c

0 diag(g )gJc

18

where



=

1 

X

(X (a

t) - y). Note that A and D is positive definite. So, 2G

is positive semidefinite if and only if A - BD-1B is positive semidefinite. Note

that A - BD-1B is a block diagonal matrix, with the bottom right block as

IdJc - diag(||g||22)gJc . To work out the expression for the top left block of A - BD-1B , let us

first examine the top left block of the matrix B: Note that by definition of u,

-1vgXg (X(v

G u) - y) + ug = 0

=

g  Supp(v),

g

=

- ug vg

.

Define the block diagonal matrix Uu/v = diag(ug/vg)gJ , then Uu/vUu,v = diag(||ug||2/vg2)gJ . The top left block of B is

-1(ug Xg Xhvh)g,hJ + diag(g )gJ = -1Uu/v(vgXg Xhvh)g,hJ + diag(g ) = Uu/v -1(vgXg Xhvh)g,hJ + IdJ - Uu/v + diag(g ) = Uu/v -1(vgXg Xhvh)g,hJ + IdJ -2Uu/v = Uu/vW - 2Uu/v.

H

Therefore, the top left block of D-1 is H-1. So, the top left block of BD-1B is

(Uu/vH -2Uu/v)(Uu/v-2H-1Uu/v) = Uu/vHUu/v-4Uu/vUu/v+4Uu/vH-1Uu/v.

and the top left block of A - BD-1B is

IdJ - Uu/vU + 4Uu/vUu/v - 4Uu/vH-1Uu/v = diag(1 - ||g||22)gJ + 4Uu/vUu/v - 4Uu/vH-1Uu/v.

Note that ||Id - H-1|| 1, and given w  R|G|,

f (v)w, w = (1 - ||g||22)wg2 + 4 (Id - H-1)(w G ), w G 
gG
(1 - ||g||22)wg2 + 4||g||22wg2,
gG

and it follows that ||f (v)|| 1 + 3 maxgG ||g||22. We have a global Lipschitz bound on the gradient of f if ||g|| L for some L, which is true because for
each v, u minimises

min
u

1 2

||u||22

+

1 ||X (v
2

G u) - y||22

So, maxgG ||g||2 ||y||2 maxgG ||Xg||/, and ||f (v)|| At stationary points, we also have

||y||22 2
1+3||y||2 maxgG ||Xg||2/2.

ug Xg (X(v G u) - y) + vg = 0 = g  Supp(v), ug g = -vg.

19

T =5

T = 10

T = 100

T = 500

Figure 6: Multitask feature learning (nuclear norm regularisation) with synthetic data. We have T tasks, n = 30 features and m = 10, 000 samples in total. The matrix Xt associated to each task has iid entries drawn uniformly at random from [0, 1]. See the description in Section 4.3.

Together, this means that at stationary points, ||ug||2 = vg2 and Uu/vUu/v = IdJ . Therefore, the top left block of A - BD-1B becomes
4IdJ - 4Uu/v -1(vgXg Xhvh)g,hJ + IdJ -1 Uu/v 0
since -1(vgXg Xhvh)g,hJ + IdJ (1+µ)Id, where µ = min Eig -1(vgXg Xhvh)g,hJ . Therefore, the smallest eigenvalue of A - BD-1B is at least

 min 4µ/(1 + µ), min(1 - ||g||2)
gJ

min(1 - ||g||2)
gJ

Moreover, if A - BD-1B 0, then mingJ (1 - ||g||2) 0, which implies that (u, v) defines a minimiser to the original group Lasso problem, hence, (u, v) defines a global minimum. Therefore, every stationary point is either a global minimum or a strict saddle point.

Remarks on the comparison with ISTA in the introduction To explain

the observed behavior, note that gradient descent for f with stepsize  reads

vk+1 = vk - f (vk) = vk(1 -  1 - |k|2 ) where k

1 

X

(X vk

uk - y)

(see Proposition 3). Note that if  is a minimiser, then 

1 

X

(X - y)

satisfies |||| 1 and the set {i \ |()i| = 1} is often called the extended

support and contains the support of . It is clear that we can expect coefficients

outside the extended support to (eventually) decay to 0 geometrically. Since k is

uniformly bounded (see Proposition 3), for  sufficiently small, vk never changes sign and any sign change in the iterate k vk uk is due to uk. In contrast,

the ISTA dynamics is k+1 = sign(k - k) max (|k - k| - , 0). Due to the

thresholding operation, a coefficient of k is initialised with the wrong sign will

spend some iterations as 0 before correcting its sign.

20

C Supplementary to Section 4
C.1 Remarks on numerical experiments
Initialisation points We generated random initialisation point from the normal distribution. In our experiments, methods which are not reparameterized (e.g. the proximal methods), are given the same random initial point, while reparameterized methods have their own random initialisation, since some of these require positive starting points and some need double the number of variables. We find that the comparisons are not much affected by the choice of initial points.

Inversion of linear systems As mentioned in Corollary (1), for the Lasso, when computing the gradient of f , one can either invert a n × n linear system or an m × m linear system. The same applies to Quad-variational, since the solution to the inner maximisation problem is, by the Woodbury identity,



=

(Idm

+

X X

)-1y

=

1 y


-

1  X(Idn

+ X

X )-1 (X

y)

where X = X diag(), with the correspondence that  =  X . Through-

out, we simply use backslash in MATLAB for the matrix inversion.

Implementation details All numerics are done in Matlab with the exception of CELER which is in Python:
· CELER are conducted in Python and we used the code https://mathurinm. github.io/celer/ provided by the original paper [37]
· 0-mem SR1, FISTA w/ BB and SPG/SpaRSA use the Matlab code from https://github.com/stephenbeckr/zeroSR1 of the paper [7].
· Interior point method uses the Matlab code https://web.stanford.edu/ ~boyd/l1_ls/ of [34].
· CGIST uses the Matlab code http://tag7.web.rice.edu/CGIST.html of [27].
· We had our own implementation of Non-cvx-Alternating-min and IRLS.
· Quad-variational, Non-cvx-LBFGS and Noncvx-Pro are written in Matlab using the L-BFGS-B solver from https://github.com/stephenbeckr/ L-BFGS-B-C which is a Matlab wrapper for C code converted from the well known Fortran implementation of [13].

C.2 Additional examples
Lasso In Figure 7, we show additional numerics for the Lasso, testing against datasets from the Libsvm repository. The regularisation parameter  associated to each plots is found by cross validation on the mean squared error.

21

abalone (4177,8)

housing (506,13)

cadata (20640,8)

a8a (22696,122)

w8a (49749,300)

leukemia (38,7129)

mnist (60000,683)

connect-4 (67557,126)



1 2

max

1 10

max

1 50

max

Figure 7: Comparisons of Lasso with different regularisation parameters on

datasets from Libsvm. The first column shows the optimal regularisation pa-

rameter  found by cross validation. The second, third and fourth columns

correspond to different fractions of max = ||X y|| which is the parameter for

which the Lasso solution is identically zero. The smaller this fraction, the less

sparse the solution.

22

(305, 22494,85) (50,1200,20) (300,1000,100)



=

1 10

max



=

1 20

max



=

1 50

max



=

1 100

max

Figure 8: Comparisons for multitask Lasso at different regularisation strengths. The problem sizes (m, n, q) are displayed on the left of each row. The top two rows are synthetic datasets generated by random Gaussian variables with 5 and 10 active features respectively. The last row corresponds to a MEG/EEG dataset from the MNE website

Group Lasso In Figure 8, we show additional numerics for the multitask Lasso setup described in Section 4.2. We test on two synthetic datasets of size (m, n, q) = (300, 1000, 100) with 5 relevant features and (m, n, q) = (50, 1200, 20) with 10 relevant features. The data matrix X has entries drawn from a normal distribution. We also test on a MEG/EEG dataset with (m, n, q) = (305, 22494, 85) from the MNE repository https://mne.tools/0.11/manual/datasets_index. html. We display convergence plots for different regularisation parameters.
Trace norm In Figure 6 we show additional numerics for the multifeature learning setup described in Section 4.3. The data matrices Xt has entries drawn uniformly at random from [0, 1]. We consider different number of tasks T tasks, n = 30 features and m = 10, 000 samples in total (the samples are split at random across the different tasks).
D Douglas-Rachford and Primal-Dual Algorithms
We consider the resolution of a constrained group Lasso problem
min ||||1,2 = ||g||2
X  =y g
23

which we write as the minimization of either F () + G() (for DR) or F () + G0(X) where F = || · ||1,2, G = C where the constraint set is C = { \ X = y} and G0 = {y}. Here C is the convex indicator function of a closed convex set C.
DR and PD are generic algorithm to solve minimization of function of the form F + G and F + G0  X when one is able to compute efficiently the so-called proximal operator of the involved functionals, where the proximal operator of some convex function H and some step size  0 is

ProxH ()

argmin


1 ||
2

-



||22

+

H (

).

In our special case, one has

ProxF () =

max(||g

||-,

0)

g ||g

||

,
g

ProxG() = +X (XX )-1(y-X),

and ProxG0 () = y.

DR algorithm. We denoted the reflected proximal map as rProxH () = 2 ProxH () - . For some step size µ > 0 and weight 0 <  < 2 (which is set to  = 1 in our experiments), the iterates (k)k of DR are k ProxµG(zk) where zk satisfies





zk+1 = (1 - 2 )zk + 2 rProxµF (rProxµG(zk)).

PD algorithm. Denoting G0(u) = sup , u -G0() the Legendre transform of G0, the PD iterations read
wk+1 = ProxG0 (wk + X(~k)) k+1 = ProxF (k -  K (wk+1)) ~k+1 = k+1 + (k+1 - k).
In our case, one has G0(u) = u, y so that ProxG0 (u) = u -  y. Convergence of the PD algorithm is ensure as long as  ||X||2 < 1 where ||X|| is the operator norm, and 0 <  1 (we use  = 1 in the numerical simulation). In our numerical simulation, we set  ||X||2 = 0.9 and tuned the value of the parameter .

E Non-convex optimisation with q quasi-norms
As mentioned, for q  (0, 2), R() ||||qq = j |j|q has a quadratic variational form. In the case where q > 2/3, we have the following bilevel smooth formulation:

24

Corollary 2. When q > 2/3, (P) is equivalent to

inf f (v)
vRn

inf
uRn

1 2

||u||22

+

Cq 2

n

2q

1

|vj | 2-q

+

L(X (u 

j=1

v), y)

(15)

where Cq = (2 - q)qq/(2-q). The function f is differentiable function provided that q > 2/3. Its gradient can be computed as in Theorem 2.

Remark 5 (Existing approaches). Existing approaches to q minimisation are typically iterative thresholding/proximal algorithms [12], IRLS [15, 18] or iterative reweighted 1 algorithms [22]. Iterative thresholding algorithms are applicable only for the case where the loss function is differentiable, and hence not applicable for Basis pursuit problems which we describe below. Moreover, computation of the proximal operation requires solving a nonlinear equation. For iterative reweighted algorithms, they require gradually decreasing an additional regularisation parameter  > 0. This can be problematic in practice and for finite , one does not solve the original optimisation problem.
Remark 6. Since we have a differentiable unconstrained problem, the problem (15) can be handled using descent algorithms and convergence analysis is standard. For example, since f is coercive, for any descent algorithm applied to f , we can assume that the generated sequence vk is uniformly bounded and f (vk) is also uniformly bounded. So, by applying standard results [8, Proposition 1.2.1], we can conclude that all limit points of sequences vk generated by descent methods under line search on the stepsize are stationary points. In fact, since we have an unconstrained minimisation problem with a continuously differentiable f which is also semialgebraic (for rational q) and hence satisfy the KL inequality [2], convergence of the full sequence by descent methods with line search can be guaranteed [42].

E.1 Basis pursuit
In this section, we focus on the basis pursuit problem with q  (2/3, 1),

min


||||qq

where

X = y.

The set of local minimums are all  for which X = y and there exists  such that X  i = q|i|q-1 sign(i) on the support of . When q > 2/3 and f is differentiable with

f

(v)

=

q

2 2-q

|v|-1

sign(v)

-

v

|X |2, where 

2q/(2 - q) > 1.

At a stationary point v, letting  = -v2 X , we have X = y and f (v) = 0 implies that on the support of v, q|v|2 = ||2-q and so,

X  = -v-2 = -q sign() ||q-1,

which is precisely the optimality condition of the original problem.

25

k

vk

q=0.7

q=0.8

q=1

Figure 9: Evolution of 20 coefficients for Basis pursuit with q regularisation. The same stepsize  is used for all plots. Top row show the evolution of k and the bottom row show the evolution of vk.

Illustrations for Basis pursuit In Figure 9, we show that gradient descent
dynamics for f in the case of the indicator function L(·, y) = {y} and a random Gaussian matrix X  R10×20, that is

2

3q-2

vk+1 = vk -  f (vk) = vk -  q 2-q |vk| 2-q

sign(v) - vk

|X k|2

where
X diag(vk vk)X k = -y.
Observe that as q  2/3, the evolution paths of vk becomes increasingly linear. In Figure 10, we follow the experiment setup of [15] and generate 100 problem
instances (X¯ , y¯, ¯). Each problem instance consist of a matrix X¯  Rm×n with
m = 140 rows and n = 256 columns whose entries are identical independent distributed Gaussian random variable with mean 0 and variance, a vector ¯ of
size n with K = 40 entries uniformly distributed on {1, . . . , n} and whose nonzero entries are iid Gaussian with mean 0 and variance 1 and y¯ X¯ ¯. For each
problem, we carry out the following procedure. For each m  {60, . . . , 140}  2N, we let X be the matrix from the first m rows of X¯ , and y be the first m entries
of y¯. We then compute  by minimising f for this X and y using BFGS with 10 randomly generated starting points and declare "success" if || - ¯||2 10-3 for one of these starting points.

References
[1] Andreas Argyriou, Theodoros Evgeniou, and Massimiliano Pontil. Convex multi-task feature learning. Machine learning, 73(3):243­272, 2008.
[2] Hedy Attouch, Jérôme Bolte, and Benar Fux Svaiter. Convergence of descent methods for semi-algebraic and tame problems: proximal algorithms, forward­backward splitting, and regularized gauss­seidel methods. Mathematical Programming, 137(1):91­129, 2013.

26

Figure 10: Number of successful recovery by q minimisation.
[3] Francis Bach, Rodolphe Jenatton, Julien Mairal, and Guillaume Obozinski. Optimization with sparsity-inducing penalties. arXiv preprint arXiv:1108.0775, 2011.
[4] Francis R Bach. Consistency of the group lasso and multiple kernel learning. Journal of Machine Learning Research, 9(6), 2008.
[5] Jonathan Barzilai and Jonathan M Borwein. Two-point step size gradient methods. IMA journal of numerical analysis, 8(1):141­148, 1988.
[6] Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM journal on imaging sciences, 2(1):183­202, 2009.
[7] Stephen Becker, Jalal Fadili, and Peter Ochs. On quasi-newton forwardbackward splitting: Proximal calculus and convergence. SIAM Journal on Optimization, 29(4):2445­2481, 2019.
[8] Dimitri P Bertsekas. Nonlinear programming. Journal of the Operational Research Society, 48(3):334­334, 1997.
[9] Rajendra Bhatia. Positive definite matrices. Princeton university press, 2009.
[10] Michael J Black and Anand Rangarajan. On the unification of line processes, outlier rejection, and robust statistics with applications in early vision. International journal of computer vision, 19(1):57­91, 1996.
[11] Stephen Boyd, Neal Parikh, and Eric Chu. Distributed optimization and statistical learning via the alternating direction method of multipliers. Now Publishers Inc, 2011.
[12] Kristian Bredies, Dirk A Lorenz, and Stefan Reiterer. Minimization of non-smooth, non-convex functionals by iterative thresholding. Journal of Optimization Theory and Applications, 165(1):78­112, 2015.
27

[13] Richard H Byrd, Peihuang Lu, Jorge Nocedal, and Ciyou Zhu. A limited memory algorithm for bound constrained optimization. SIAM Journal on scientific computing, 16(5):1190­1208, 1995.
[14] Antonin Chambolle and Thomas Pock. A first-order primal-dual algorithm for convex problems with applications to imaging. Journal of mathematical imaging and vision, 40(1):120­145, 2011.
[15] Rick Chartrand and Valentina Staneva. Restricted isometry properties and nonconvex compressive sensing. Inverse Problems, 24(3):035020, 2008.
[16] Patrick L Combettes and Jean-Christophe Pesquet. A douglas­rachford splitting approach to nonsmooth convex variational signal recovery. IEEE Journal of Selected Topics in Signal Processing, 1(4):564­574, 2007.
[17] Patrick L Combettes and Bng C Vu~. Variable metric forward­backward splitting with applications to monotone inclusions in duality. Optimization, 63(9):1289­1318, 2014.
[18] Ingrid Daubechies, Michel Defrise, and Christine De Mol. An iterative thresholding algorithm for linear inverse problems with a sparsity constraint. Communications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of Mathematical Sciences, 57(11):1413­1457, 2004.
[19] Ingrid Daubechies, Ronald DeVore, Massimo Fornasier, and C Sinan Güntürk. Iteratively reweighted least squares minimization for sparse recovery. Communications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of Mathematical Sciences, 63(1):1­38, 2010.
[20] Joseph L DeRisi, Vishwanath R Iyer, and Patrick O Brown. Exploring the metabolic and genetic control of gene expression on a genomic scale. Science, 278(5338):680­686, 1997.
[21] Jim Douglas and Henry H Rachford. On the numerical solution of heat conduction problems in two and three space variables. Transactions of the American mathematical Society, 82(2):421­439, 1956.
[22] Simon Foucart and Ming-Jun Lai. Sparsest solutions of underdetermined linear systems via q-minimization for 0 < q 1. Applied and Computational Harmonic Analysis, 26(3):395­407, 2009.
[23] Jerome Friedman, Trevor Hastie, and Rob Tibshirani. Regularization paths for generalized linear models via coordinate descent. Journal of statistical software, 33(1):1, 2010.
[24] Davi Geiger and Alan Yuille. A common framework for image segmentation. International Journal of Computer Vision, 6(3):227­243, 1991.
[25] Donald Geman and George Reynolds. Constrained restoration and the recovery of discontinuities. IEEE Transactions on pattern analysis and machine intelligence, 14(3):367­383, 1992.
28

[26] Laurent El Ghaoui, Vivian Viallon, and Tarek Rabbani. Safe feature elimination for the lasso and sparse supervised learning problems. arXiv preprint arXiv:1009.4219, 2010.
[27] Tom Goldstein and Simon Setzer. High-order methods for basis pursuit. UCLA CAM Report, pages 10­41, 2010.
[28] Gene Golub and Victor Pereyra. Separable nonlinear least squares: the variable projection method and its applications. Inverse problems, 19(2):R1, 2003.
[29] Alexandre Gramfort, Matthieu Kowalski, and Matti Hämäläinen. Mixednorm estimates for the m/eeg inverse problem using accelerated gradient methods. Physics in Medicine & Biology, 57(7):1937, 2012.
[30] Alexandre Gramfort, Gabriel Peyré, and Marco Cuturi. Fast optimal transport averaging of neuroimaging data. In International Conference on Information Processing in Medical Imaging, pages 261­272. Springer, 2015.
[31] Trevor Hastie, Rahul Mazumder, Jason D Lee, and Reza Zadeh. Matrix completion and low-rank svd via fast alternating least squares. The Journal of Machine Learning Research, 16(1):3367­3402, 2015.
[32] Peter D Hoff. Lasso, fractional norm and structured sparse estimation using a hadamard product parametrization. Computational Statistics & Data Analysis, 115:186­198, 2017.
[33] Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape saddle points efficiently. In International Conference on Machine Learning, pages 1724­1732. PMLR, 2017.
[34] Kwangmoo Koh, Seung-Jean Kim, and Stephen Boyd. An interior-point method for large-scale l1-regularized logistic regression. Journal of Machine learning research, 8(Jul):1519­1555, 2007.
[35] Jason D Lee, Ioannis Panageas, Georgios Piliouras, Max Simchowitz, Michael I Jordan, and Benjamin Recht. First-order methods almost always avoid saddle points. arXiv preprint arXiv:1710.07406, 2017.
[36] Pierre-Louis Lions and Bertrand Mercier. Splitting algorithms for the sum of two nonlinear operators. SIAM Journal on Numerical Analysis, 16(6):964­979, 1979.
[37] Mathurin Massias, Alexandre Gramfort, and Joseph Salmon. Celer: a fast solver for the lasso with dual extrapolation. In International Conference on Machine Learning, pages 3315­3324. PMLR, 2018.
[38] Charles A Micchelli, Jean M Morales, and Massimiliano Pontil. Regularizers for structured sparsity. Advances in Computational Mathematics, 38(3):455­ 489, 2013.
29

[39] Eugene Ndiaye, Olivier Fercoq, Alexandre Gramfort, and Joseph Salmon. Gap safe screening rules for sparse multi-task and multi-class models. arXiv preprint arXiv:1506.03736, 2015.
[40] Eugene Ndiaye, Olivier Fercoq, Alexandre Gramfort, and Joseph Salmon. Gap safe screening rules for sparsity enforcing penalties. The Journal of Machine Learning Research, 18(1):4671­4703, 2017.
[41] Jorge Nocedal and Stephen Wright. Numerical optimization. Springer Science & Business Media, 2006.
[42] Dominikus Noll and Aude Rondepierre. Convergence of linesearch and trustregion methods using the kurdyka­lojasiewicz inequality. In Computational and analytical mathematics, pages 593­611. Springer, 2013.
[43] Brendan O'donoghue and Emmanuel Candes. Adaptive restart for accelerated gradient schemes. Foundations of computational mathematics, 15(3):715­732, 2015.
[44] Razvan Pascanu, Yann N Dauphin, Surya Ganguli, and Yoshua Bengio. On the saddle point problem for non-convex optimization. arXiv preprint arXiv:1405.4604, 2014.
[45] Gabriel Peyré, Marco Cuturi, et al. Computational optimal transport: With applications to data science. Foundations and Trends® in Machine Learning, 11(5-6):355­607, 2019.
[46] Ting Kei Pong, Paul Tseng, Shuiwang Ji, and Jieping Ye. Trace norm regularization: Reformulations, algorithms, and multi-task learning. SIAM Journal on Optimization, 20(6):3465­3489, 2010.
[47] Alain Rakotomamonjy, Francis Bach, Stéphane Canu, and Yves Grandvalet. Simplemkl. Journal of Machine Learning Research, 9:2491­2521, 2008.
[48] R Tyrrell Rockafellar and Roger J-B Wets. Variational analysis, volume 317. Springer Science & Business Media, 2009.
[49] Axel Ruhe and Per Åke Wedin. Algorithms for separable nonlinear least squares problems. SIAM review, 22(3):318­337, 1980.
[50] Filippo Santambrogio. Optimal transport for applied mathematicians. Birkäuser, NY, 55(58-63):94, 2015.
[51] Geoffrey Schiebinger, Jian Shu, Marcin Tabaka, Brian Cleary, Vidya Subramanian, Aryeh Solomon, Joshua Gould, Siyan Liu, Stacie Lin, Peter Berube, et al. Optimal-transport analysis of single-cell gene expression identifies developmental trajectories in reprogramming. Cell, 176(4):928­943, 2019.
[52] Justin Solomon, Raif Rustamov, Leonidas Guibas, and Adrian Butscher. Earth mover's distances on discrete surfaces. ACM Transactions on Graphics (TOG), 33(4):1­12, 2014.
30

[53] Athanasios Tsanas, Max Little, Patrick McSharry, and Lorraine Ramig. Accurate telemonitoring of parkinson's disease progression by non-invasive speech tests. Nature Precedings, pages 1­1, 2009.
[54] Tristan van Leeuwen and Aleksandr Aravkin. Non-smooth variable projection. arXiv preprint arXiv:1601.05011, 2016.
[55] Stephen J Wright, Robert D Nowak, and Mário AT Figueiredo. Sparse reconstruction by separable approximation. IEEE Transactions on signal processing, 57(7):2479­2493, 2009.
[56] Yu Zhang and Qiang Yang. A survey on multi-task learning. IEEE Transactions on Knowledge and Data Engineering, 2021.
[57] Yu Zhang and Dit-Yan Yeung. A convex formulation for learning task relationships in multi-task learning. arXiv preprint arXiv:1203.3536, 2012.
[58] Peng Zhao and Bin Yu. On model selection consistency of lasso. The Journal of Machine Learning Research, 7:2541­2563, 2006.
31

