
# QLSD: Quantised Langevin stochastic dynamics for Bayesian federated learning

[arXiv](https://arxiv.org/abs/2106.0797), [PDF](https://arxiv.org/pdf/2106.0797.pdf)

## Authors

- Maxime Vono
- Vincent Plassier
- Alain Durmus
- Aymeric Dieuleveut
- Eric Moulines

## Abstract

Federated learning aims at conducting inference when data are decentralised and locally stored on several clients, under two main constraints: data ownership and communication overhead. In this paper, we address these issues under the Bayesian paradigm. To this end, we propose a novel Markov chain Monte Carlo algorithm coined \texttt{QLSD} built upon quantised versions of stochastic gradient Langevin dynamics. To improve performance in a big data regime, we introduce variance-reduced alternatives of our methodology referred to as \texttt{QLSD}$^\star$ and \texttt{QLSD}$^{++}$. We provide both non-asymptotic and asymptotic convergence guarantees for the proposed algorithms and illustrate their benefits on several federated learning benchmarks.

## Comments



## Source Code

Official Code



Community Code



## Bibtex

```tex
@misc{vono2021qlsd,
      title={QLSD: Quantised Langevin stochastic dynamics for Bayesian federated learning}, 
      author={Maxime Vono and Vincent Plassier and Alain Durmus and Aymeric Dieuleveut and Eric Moulines},
      year={2021},
      eprint={2106.00797},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```

## Notes

Type your reading notes here...

