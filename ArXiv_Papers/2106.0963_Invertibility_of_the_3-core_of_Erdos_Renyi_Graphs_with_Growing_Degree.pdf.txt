Invertibility of the 3-core of Erdos R´enyi Graphs with Growing Degree
Margalit Glasgow  June 3, 2021

arXiv:2106.00963v1 [math.CO] 2 Jun 2021

Abstract
Let A  Rn×n be the adjacency matrix of an Erdos R´enyi graph G(n, d/n) for d = (1) and d  3 log(n). We show that as n goes to infinity, with probability that goes to 1, the adjacency matrix of the 3-core of G(n, d/n) is invertible.

1 Introduction

Our main result is the following theorem. Big-O notation should be interpreted with respect to n going to infinity. If A is the adjacency matrix of a graph G, then we say the 3-core of A to mean the adjacency matrix of the 3-core of G.

Theorem 1. Let A  Rn×n be the adjacency matrix of a random Erdos R´enyi graph G(n, d/n). If d = (1) and d  3 log(n), the with probability 1 - o(1), the 3-core of A is full rank.

Remark 1. One can show from our proof that this theorem holds more generally for the k-core of of A for any fixed integer k  3. This requires a slight modification to the proofs of Lemma 11 and Claim 7.

This provides partial progress on the following conjecture from Van Vu [6].

Conjecture

1.

There

exists

a

constant

d0,

and

a

constant

k

such

that

the

k - core

of

G(n,

d n

)

is

full

rank

for d  d0.

We outline our strategy below. Let A(3) denote the 3-core of an adjacency matrix A. In Section 2, we state some concentration and anti-concentration lemmas. In Section 3-5, we use union bounds to prove some that with high probability, certain "sparse" dependency structures (which involve not too many rows) do not exist in A(3). Combining the results in these sections with a lower bound on the size of the 3-core shows that for some constant C,

Pr[x  Rn : A(3)x = 0, supp(x)  n/C] = o(1).

(1)

The proofs in Sections 3-5 are based off of prior work of the author with DeMichele and Moreira [3], which classifies the types of linear dependencies that occur in sparse symmetric random matrices. In Section 6, we show that with high probability, A(3) has no kernel vectors with large support:

Pr[x  Rn : A(3)x = 0, supp(x)  n/C] = o(1).

(2)

The ideas in this section are based off of ideas in the works of Ferber, Kwan, and Sauermann [4] and Costello, Tau, and Vu [1]. More closely, our proof follows the proofs of Lemmas 19-22 in [3] (also based on [4] and [1]), which show that

Pr[x  Rn : Ax = 0, supp(x)  n/C] = o(1).

(3)

Department of Computer Science, Stanford University. Email: mglasgow@stanford.edu. Research supported by NSF award DGE-1656518.

1

Our argument in is slightly more complex than that of [3] because we consider A(3) and not A.
1.1 Notation
For a matrix B  Rn×m and a sets S  [n] and T  [m], let BS denote the matrix B restricted to rows in S, and let B[T ] denote the matrix B restricted to columns in T .

2 Useful Lemmas and Definitions

Lemma 1 (Tail Bound on Binomial). If t > np, then

Pr [Bin(n, p)



t]



n t
1-

pt
pn t

.

(4)

This implies that if t  2np,

Pr [Bin(n, p)  t]  2

n

pt  2

enp

t
.

(5)

t

t

Proof.

n
Pr [Bin(n, p)  t] 

n pi

i

i=t

n


n pi

i

n-j+1

t

j

i=t

j=t+1

n


n pi

i

n

t

t

i=t

j=t+1

(6)

n
=

n pt pn i-t

t

t

i=t



n


pt

pn j

t

t

j=0

=

1

n t
-

pt
pn t

.

We will use the following two anticoncentration lemmas. The first lemma is due to Costello and Vu [2], and is proved with these constants in Lemma 3 of [3].
Lemma 2 (Sparse Littlewood Offord). For any integer m and q  (0, 1/2) such that qm  9, for all x  Rm with full support,

where bi  Ber(q).

n

1

Pr

xibi = 0  qm ,

i=1

The next lemma is a quadratic version of the Littlewood-Offord theorem, due to Costello, Tau, and Vu [1]. We state a version for sparse random vectors proved by Costello and Vu in [2].

Lemma 3 (c.f. [2] Lemma 8.4). Let M  Rn×n be a deterministic matrix with a least m non-zero entries in each of m distinct columns of M . Let z  Rn be the random vector with i.i.d. Bernoulli entries with
parameter p  1/2. Then for any fixed c,

Pr zT M z = c = O

1

.

(mp)1/4

2

Figure 1: The regions E, ESym, ESymAD, and EAsym. Figure from [3].

We prove Theorem 1 by ruling out the existence of all kernel vectors. We break this down based on the size of support of the kernel vector. We use the following definition and claim.
Definition 1. A k-minimal dependency is a set k linearly dependent rows, where all strict subsets of these rows are linearly independent.
Claim 1. Any k-minimal dependency has no columns with exactly one non-zero entry among the k rows.
Proof. Since the coefficients of the linear dependency must all be non-zero, if there any column had a single non-zero entry, it would not be orthogonal to the dependency coefficient vector.

3 Small Case

Lemma 4 (Small Case). Let A  Rn×n be the adjacency matrix of a random Erdos R´enyi graph G(n, d/n).

Then

there

exists

a

constant

d0

such

that

for

all

d0

<

d

<

3 log(n),

with

probability

1-O(

31n ),

for

all

k

<

n d7

,

A does not contain a set of k rows for k  1, which when restricted to some subset of columns, forms a

k-minimal dependency with at least 3k - 1 total non-zero entries.

Formally,

Pr x  Rn, T  [n] : xT A[T ] = 0  |Asupp(x)[T ]|1  3|supp(x)| - 1  rank(Asupp(x)[T ]) = |supp(x)| - 1 = o(1) (7)
The same result applies if A has its first column removed.

Proof. We introduce some notation to prove this. Let S  [n] be a set of size k, and let AS be the submatrix containing only the rows indexed by values in S. Let ESym be the set of entries in AS whose columns are indexed by values in S. Let ESymAD be subset of entries in ESym that are above the diagonal of A, and hence mutually independent. Let EAsym be the set of entries who columns are not in S, and finally, let E = EAsym  ESymAD be the full set of mutually independent entries that determine the rows in S (See Figure 1).
Formally:

ESym := {(i, j) : i, j  S}.

ESymAD := {(i, j) : i, j  S, i < j}.

(8)

EAsym := {(i, j) : i  S, j / S}.

E := ESymAD  EAsym.

3

For each of these four sets for T  [n], let ESym[T ], ESymAD[T ], EAsym[T ] and E[T ] denote the respective set additionally restricted to entries with j  T .
We will couple the process of putting non-zero entries in AS with a random walk that counts the number of times a non-zero entry is inserted in EAsym in a column that already contains a non-zero entry or into ES ymAD .
We condition on R, the number of nonzero entries in E. Conditioned on R = r, the process of choosing
AS is equivalent to randomly choosing r locations in E for these non-zero entries without replacement. (Note that this is true even if we are considering a matrix with the first column removed, even though ESymAD may include some entries which are not repeated below the diagonal, if 1  S). Let (Xi)i[R] be the random walk that increases by 1 if the ith random location chosen is in ESymAD or if the ith random location is in EAsym and is not the first non-zero entry in its column. Otherwise, put Xi = Xi-1.
The following claim says that XR must be large for the structure described in the lemma to occur.

Claim 2. Suppose there is some set of at columns T such that there are at least 3k - 1 non-zeros in AS[T ].

Then XR 

3k-1 2

.

Proof. Let L be the number of non-zeros entries in EAsym[T ], and let M be the number of non-zero entries in ESymAD[T ]. By assumption, 2M + L  3k - 1. Let Y be the number of non-zero entries in EAsym[T ] which are not the first non-zero in their column, so Y  XR - M . Now the number of columns in AS[T \ S] which have exactly one non-zero entry is at least (L - Y ) - Y , since there are exactly L - Y non-empty
columns in AS[T ], and at most Y of those columns have more than one non-zero entry. Now

L - 2Y  L - 2XR + 2M  3k - 1 - 2XR.

Hence if XR <

3k-1 2

, then some column in AS[T ] has exactly one non-zero entry, which rules out AS[T ]

being a k-minimal dependency.

It remains to show that XR is small enough with high enough probability to take a union bound over all

n k

sets S. We will break up this probability as follows.

Let

3k - 1

Q :=

,

2

and define Rmax := max

1 4e

n

k1-

, 2dk

,

where

=

1 12

.

Then

Pr[Structure in the lemma]  Pr [R > Rmax] + Pr [XR  Q|R < Rmax] .

(9)

The following claim bounds the second term in the equation above.

Claim

3.

For

any

k

<

n d7

,

for

d

a

large

enough

constant,

k

k+

1 3

Pr[XR  Q|R < Rmax]  2 3n

.

(10)

Proof.

We

couple

Xi

with

a

random

walk

that

(Yi)iR

which

increases

by

1

with

probability

k+R n

and

otherwise stays constant. Observe that Yi stochastically dominates Xi, because there are at most k + R

columns in E in which placing a non-zero entry will increase Xi. Then conditioned on R = r,

k+r

Pr [Xr  Q]  Pr Bin r, n  Q

(11)

We will bound the tail of this binomial using Lemma 1. We first check that the hypothesis of the lemma

is

satisfied,

namely

that

r

k+r n

<

1

for

r

<

Rmax:

If r < Rmax = max

1 4e

n

k1-

, 3dk

and

k



n d7

,

then

for

i



Q



k,

4

r(r + k)  max
nQ

1 4e

n

k1-

2+

1 4e

n

k1-

k 3d(3d + 1)k2 ,

kn

kn

1 k 1-2 1 k 1- 3d(3d + 1)k

= max 16e2 n

+

,

4e n

n

(12)

1 k 1-2 10d2k

 max

,

.

3e n

n

Since

this

value

clearly

at

most

1 2

,

the

conditions

of

Lemma

1

are

satisfied.

Hence conditioned on

R < Rmax,

eR(k + R) Q

Pr [XR  Q]  2

Qn

.

(13)

Note that the structure in the lemma cannot occur for k = 1. Plugging in Q =

3k-1 2

and k  2,

conditioned on R < Rmax, we can bound

3k - 1

Pr XR 

2

3k-1
eR(k + R) 2 2
Qn

3k-1

1 k 1-2 10d2k

2

 2 e max

,

3e n

n

 1 k 1-2
= 2 max  3n

3k-1 2
,

10ed2k n

 3k-1
2




k

k+

1 3

10ed2k

 2 max 

,

3n

n

 3k-1
2


(14)

Here the first inequality is Equation 13, the second inequality plugs in the result of Equation 12, the third

inequality uses Jenson's inequalty, and the final inequality uses the fact that (1-2

)

3k-1 2

=

5 6

3k-1 2



k+

1 3

for integers k  2.

Now

10ed2k n

3k-1 2
=
=

k

k+

1 3

3n

k

k+

1 3

3n

k

k 2

-

1 3

3n

k(30ed2)3 3n

3k-1
30ed2 2

1

k 2

-

1 3

30ed2

1

1+1k odd

(15)

Note that the function

k(30ed2)3 3n

k 2

-

1 3

30ed2 1+1k odd 1

is convex in k (when the parity of k is fixed), so its maximum is achieved at either the minimum or maximum

value of k, up to parity.

It

is

easy

to

check

that

when

2k

<

n d7

<

3n 2(30ed2 )3

(where

the

last

inequality

holds

for

d

large

enough),

we have

k(30ed2)3 3n

k 2

-

1 3

30ed2 1+1k odd < 1.
1

(16)

5

Hence for all k in this range, we have

10ed2k n

3k-1 2


k

k+

1 3

.

3n

(17)

Plugging this in to Equation 14, we have

3k - 1

k

k+

1 3

Pr XR  2

2 3n

.

(18)

This yields the claim.

Then next claim bounds the probability that R  Rmax.

Claim 4.

Pr[R  Rmax]  2

k

k
e-2 log(n)

en

(19)

Proof. Recall that R is the number of non-zero entries in the EAsym and ESymAD. Now |EAsym ESymAD|  kn, and each entry in EAsym  ESymAD is non-zero with probability d/n.
Hence

kn d

dke Rmax

Pr[R  Rmax]  Pr Bin

, 2n

 Rmax  2 Rmax

,

(20)

where the binomial bound comes from Lemma 1.

Recall that Rmax = max

1 4e

n

k1-

, 3dk

.

Let



:= logn(n/k)

such

that

Rmax

= k max

n 4e

, 3d

.

Then

Pr[R > Rmax]  2 =2

de

( k max

n 4e

,3d)

max

n 4e

, 3d

 kk

de

( max

n 4e

,3d)

k ne

en



max

n 4e

, 3d

k



kk

de

=2 en



max

n 4e

, 3d

( max

n 4e

,3d)

n1-

ne .

(21)

If n < (4de2)2, then



de



max

n 4e

, 3d

( max

n 4e

,3d)

n1-

ne 

e

3d
e(4de2)2/

3

n1-

=

e ( n1-

3d

log(

e 3

)+

4

log(4de2 )+1)



e-n1/2 ,

(22)

where

the

last

inequality

follows

from

taking

d

a

large

enough

constant

and

noticing





2 log(4e2d) log(n)



1 2

for

large enough n.

If n  (4de2)2,

6



de



max

n 4e

, 3d

( max

n 4e

,3d)

n1-

ne =

n

n1-

de
n

4e
n e

4e

 4de2
  n

n

n1-

4e

n2 

n

n1-



1 n

8e
n2

(23)

Taking a logarithm, we have

n

n1-

log

1

8e
n2

= n1-

 2 log(n) -

n

log(n)

n

8e

= n1-  log(n) 2 - n 8e
 -2n1- log(n),

(24)

where in the last line we used the bound n

> (ed)2 to show

n 8e



(4de2 )2 8e



4

for

d

a

large

enough

constant.

Now n

>

(4de2)2

implies



>

2

log(4de2 log(n)

)

.

The

function

n1- 

achieves

its

minimum

at

one

endpoint

of

the

interval





[

2

log(d) log(n)

,

1]

(this

can

be

checked

by

observing

that

the

log

of

the

function

is

concave

in

).

Thus

- 2n1- log(n)  max

-2n1-

2

log(4de2 ) log(n)

2

log(4de2)

,

-2

log(n)

 -2 log(n),

(25)

so if n  (4de2)2, we have



de



max

n 4e

, 3d

( max

n 4e

,3d)

n1-

n e

 e-2 log(n).

(26)

Combining the two cases (Equations 22 and 26) into Equation 21, we have

Pr[R > Rmax]  2

k

k
e-2 log(n)

en

(27)

as desired.

We finish the proof of the main lemma by union bounding over all k, and all set S of k rows. Plugging in the result of Claim 4, we have

n/d7

n k

n/d7
Pr[R > Rmax] 

n 4
k

k en

k
e-2 log(n)

n/d7


4 n2



1 .
n

k=1

k=1

k=1

(28)

Hence

with

probability

at

least

1 n

,

for

all

sets

of

k

<

n/d7

rows,

there

less

than

Rmax

=

max(3dk, n1/12k11/12)

non-zeros among the rows. Assuming R < Rmax, we plug in the result of Claim 3 to obtain:

7

n/d7
n

n/d7
n

k

k+

1 3

k Pr[XR  Q  R < Rmax] 

4 k 3n

k=1

k=1

n/d7
4
k=1

ne k k

k

k+

1 3

3n

n/d7
ek

k 1/3

1

4

=O  .

3 3n

3n

k=1

(29)

This union bound completes the proof of the first part of the lemma which states that with probability 1 - O( 31n ), the structure described in the lemma does not exist in A.

4 Medium Case

Lemma 5 (Medium Case). Let A  Rn×n be the adjacency matrix of a random Erdos R´enyi graph G(n, d/n). There exists a constant d0 such that for all d > d0,

Pr

x



Rn, T



[n]

:

xT A[T ]

=

0,

n d7



|supp(x)|

<

2n , |T |
d



1 1 - d7

n = e-(n).

(30)

The same result applies if A has its first column removed.

Proof. We union bound over all k, and all

n k

sets S of k rows which may be the support of x. We will lower

bound the number of length k columns in AS which have exactly one non-zero entry. We consider only the

n - k mutually independent columns. Let Xi be the event that column i has one non-zero entry. Then

d

d k-1

Pr[Xi] = k n

1- n

.

Let

c

:=

n dk

,

such

that

1 2



c

<

d6.

Now

because

1-x



e , -  x 1-x

we

have

d 1-

k-1


d 1-

k


d 1-

n cd

- 1

e

c

, 1-

d n

n

n

n

(31)

so

- 1

ec

1-

d n

1

Pr[Xi] 

c

, 2c

(32)

where

we

have

bounded

d n

<

1 2

and

used

the

fact

that

c



1/2.

If xT A[T ] = 0 for supp(x) = S, then necessarily

Xi

<

n d7

,

because

no

column

in

AS[T ]

can

have

exactly one non-zero entry.

By a Chernoff bound, for d at least a large enough constant,

Pr

n

1

Xi < d7

 Pr Bin

n - k, 2c

n < d7  Pr Bin

n1 ,
2 2c

n < d7

 e ( ) -n

1-

2c d7

2

21 4c



e-

n 32c

,

(33)

where we have used in the final inequality the fact that c  d6.

Union bounding over all

n k

choices of S, and all choices of k the probability of having the structure in

the lemma is at most

8

2n d

k=

n d7

n

n

e-

n 32c

n

max

k

c[

1 2

,d7

]

ne
n cd

e cd

-

n 32c

=n

max

n
(ecd) cd

e-

n 32c

c[

1 2

,d7

]

= max e ( ). n

log(n) n

+

log(ecd) cd

-

1 32c

c[

1 2

,d7

]

(34)

Since c grows at most polynomially in d, for d a large enough constant, the exponent becomes negative, so the probability that the structure in the lemma exists in A is e-(n).

5 Large Case

Lemma 6 (Large Case). Let A  Rn×n be the adjacency matrix of a random Erdos R´enyi graph G(n, d/n). There exist constants d0 and C such that for all d > d0,

Pr

x



Rn, T



[n]

:

xT A[T ]

=

0,

2n d



|supp(x)|

<

n , |T |
C



1 1-
d

n = e-(n).

(35)

The same result applies if A has its first column removed.

Proof. We union bound over all k, and all

n k

sets S of k rows which may be the support of x, and over all

sets T of size

1

-

1 d

n.

Fix

a

set

of

k

rows

S

and

a

subset

T

of

columns.

We

will

consider

only

the

at

least

n-k

-

n d

mutually

independent columns of AS[T ], that is, the columns of the matrix AS[T \ S].

Consider the following process, where we draw these independent columns one at a time. Let Ai  Rk

be the nullspace of the i columns drawn, and let Di be the dimension of the smallest subspace of Ai which

contains all vectors in Ai which have no zeros.

By Lemma 2, if Di > 0, then we can choose an arbitrary vector v in Ai with support k, and with

probability

at

least

1-  1 ,
kd/n

the

(i + 1)th

column drawn

is

not

orthogonal

to

v.

In

this

case

Di+1

=

Di - 1.

If at any point Di becomes 0, then this means there can be no dependency involving all the rows. It

follows that since D0 = k, using a Chernoff bound yields

n

1

Pr[D|T \S| = 0]  Pr

Bin(n - k - , 1 - d

)<k kd/n

n

1

 Pr Bin , 1 -

<k

2

kd/n

 en 2

1-  1
kd/n

2

1-

1
kd/n

-

2k n

 e-n ,

(36)

where

> 0.002. To achieve this value of

,

we

plugged

in

k

<

n 12

in

the

last

inequality.

Finally union bounding, the probability of having the structure in the lemma is at most

n
Cn

k

k=

2n d

n
n

e-

n



n
n (eC) C

n
(ed) d

e-

n



e (n

log(n) n

+

log(eC)) C

+

log(ed)) d

-

),

d

which for constants C and d large enough, is e-(n). This concludes the lemma.

(37)

9

6 3-Core is Full Rank
We will need the following claims. Claim 5. Let A be a matrix with columns Ai for i  n. Let Hi be the space spanned by the column vectors A1, A2, · · · Ai-1, Ai+1, · · · An. Let S be the set of all i such that Ai  Hi. Then there exists some y with supp(y) = S such that Ay = 0. Proof. We prove this by linearly combining the dependencies. Without loss of generality, let S = [k] for some k. For all i  [k], since Ai  Hi, there exists some x(i) such that xi(i) = 0 and Ax(i) = 0. Observe that for each i  [k], we have supp(x(i))  S - otherwise it would imply that some column Aj for j / S is spanned by Hj. Choose random coefficients ci for i  [k] from any continuous distribution and let y = i cix(i). Then with probability 1, y is nonzero on all i  S.
Let ei denote the ith standard basis vector. In the rest of this section, for a matrix B, we use Span(B) to denote the column span of B. Claim 6. With the terminology of the previous claim, if and only if ei  Span(AT ), then Ai / Hi. Proof. For the forward direction, let v be such that AT v = ei. The for any w with i  supp(w), we have wT AT v = wi = 0, so it is impossible that Aw = 0.
For the converse, suppose Ai / Hi. Then there must exist some wi such that wi, Ai = 0, but wi  Aj for all j = i. However, this implies that Awi = wi, Ai ei, so ei  Span(AT ), which is a contradiction.
We restate our main theorem. Theorem 2. Let A  Rn×n be the adjacency matrix of a random Erdos R´enyi graph G(n, d/n). If d = (1) and d  3 log(n), the with probability 1 - o(1), the 3-core of A is full rank.
We will use the following notation: For an adjacency matrix M , let Ck(M ) be the set of vertices in the k-core of the graph with adjacency matrix M . Further, let M (3) denote adjacency matrix of the 3-core of M . For a vector v and a set S, let v[S] denote the restriction of v to the indices in S. For a matrix M and set S, in the proof of Lemma 8, we abuse notation and let M [S] denote the restriction of M to the columns and rows in S. Thus M (3) = M [C3(M )].
The following lemma, which follows from [5], states that the 3-core of A is large. Lemma 7 (c.f. Theorem 2 in [5]). Let A  Rn×n be the adjacency matrix of a random Erdos R´enyi graph G(n, d/n) with d = (1).
For any constant k  3, with probability at least 1 - O(1/n),
|Ck(A)|  n 1 - e-d1/4 .
. Our main tool to prove Theorem 2 is the following lemma. It will allow us to rule out having large dependencies in A(3). Lemma 8. Let A be as in the proposition, and and let A(i) denote the submatrix of A given by the ith row and column removed. Let Ai denote the ith column of A, and let Ai be this vector with the ith entry removed. For any u, r < s, t  [n],
10

Pr [x : A(3)x = 0, supp(x) > t] 

n max Pr
t - n/d xRn:supp(x)s

xT An] = 0

+ n Pr x : A(i)(3)x = 0, r < |supp(x)| < s t - n/d

dr +
t - n/d

nn

+

Pr

t - n/d u

x = 0 : A(n)(3)x = e1, |supp(x)| < s

n

+

t

-

n/d

max
X Tnn-n/d2-r-u,s

Pr [AnXAn

=

0]

(38)

n

+o

,

t - n/d

where Tpm,q denotes the set of matrices in Rm×m with some set of p columns that each have at least q non-zero entries.
Proof. For i  [n], let Hi be the space spanned by the column vectors
A1[C3(A(i))  i], A2[C3(A(i))  i], · · · Ai-1[C3(A(i))  i], Ai+1[C3(A(i))  i], · · · , An[C3(A(i))  i].

Let T := {i : C3(A) = C3(A(i))  i}.
Then A(3)x = 0 for some x implies that for all

i  supp(x)  T,

we have

Ai[C3(A(i))  i] = Ai[C3(A)]  Hi,

since

xiAi[C3(A)] = - xjAj[C3(A)].
j

We first claim that the set T is large, such that if supp(x) is large, we must have many i for which Ai[C3(A(i))  i]  Hi. Let ci := |C3(A(i))|.

Claim 7. With probability 1 - o(1),

1

|T |  n 1 - .

(39)

d

Proof. Fix i, and consider the probability that C3(A) = C3(A(i))  i.

11

Notice that C3(A(i)) is independent of Ai, and hence

Pr[C3(A) = C3(A(i))  i]  Pr[supp(Ai)  3  Aij = 0 j / C3(A(i))]

= Eci

d n-1-ci

d

1- n

Pr Bin ci, n  3

 E e ci

-

d(n-1-ci 2n

)

d 1 - Pr Bin ci, n

d = 1 - Pr Bin ci, n

=2

= E e ci

-

d(n-1-ci 2n

)

1 - cid n

d 1-

ci-1 - c2i d2

n

n2

d ci-2 1-
n

 E e 1 - de - d e ci

-

d(n-1-ci 2n

)

-

d(ci -1) n

2

-

d(ci -2) n

 E e ci

-

d(n-1-ci 2n

)

1

-

3d2e-

dci n

1  Pr ci  n 1 - 2d3

e-

1 d2

(1

-

e-d/2)

1  Pr ci  n 1 - 2d3

1

1-

.

d3/2

(40)

where the last two inequalities hold for d large enough. By Lemma 7, with probability 1 - O(1/n), we have ci  n 1 - e-d1/4 , so we have

Pr[C3(A)

=

C3(A(i))



i]



1

-

1 d3/2

-

O

1 n

.

(41)

Now by Markov's inequality,

nd Pr |[n] \ T |  
dn

1 - Pr[C3(A) = C3(A(i))  i]

i

(42)

 d 1 - 1 - d-3/2 - O 1

= o(1).

n

Let Ei denote the event that Ai[C3(A(i))  i]  Hi. Let Xi be the indicator of this event, and let

X=

i Xi.

Then by Markov's inequality, since if |T |  n

1

-

1 d

, at most n/d vertices can be in the 3-core

but not in T , we have

1 Pr [x : A(3)x = 0, |supp(x)|  t]  Pr |T | < n 1 - + Pr [X  t - n/d]
d

 o(1) + E[X]

t - n/d

(43)

= o(1) + i Pr[Ei]

t - n/d

n = o(1) + t - n/d Pr[En].

We will break down the probability Pr[En] into several cases, depending on the size of the support of

vectors in the kernel of A(n)(3). Let S  C3(A(n)) be the set of all j such that ej  Span(A[C3(A(n))]). By

Claim 5 and Claim 6,

k := max(supp(x) : A(n)(3)v = 0) = cn - |S|.

(44)

. Case 1: A(n)(3) has a vector x with large support, that is, k  s.

12

Case 2: A(n)(3) has a vector x with medium support, that is r < k < s. Case 3: A(n)(3) does not have any vectors with large or medium support vectors in its kernel, that is, k  r. We can expand

Pr[En] = Pr[En|k  s] Pr[k  s]

+ Pr[En|r < k < s] Pr[r < k < s]

(45)

+ Pr[En|k  r] Pr[k  r].

Define a := Ai[C3(A(n))] to be the restriction of Ai to the indices in the 3-core of A(n). To evaluate the probability of the first case, we condition on A(n), and let x be any vector of support at least s in the kernel of A(n)(3). Observe that En cannot hold if xT a is non-zero. Since a is independent from
x, we have

Pr[En|k  s] Pr[k  s]  max Pr[xT a = 0]
x:supp(x)s



max

Pr xT An = 0 .

xRn :supp(x)s

(46)

Here the second inequality comes from considering x over a larger domain. Combined with Equation 43, the contribution from this case yields the first term in the right hand side of Equation 38.
For the second case, we bound

Pr[En|r < k < s] Pr[r < k < s]  Pr[r < k < s]  Pr x : A(n)(3)x = 0, r < |supp(x)| < s .

(47)

Combined with Equation 43, the contribution from this case yields the second term in the right hand
side of Equation 38.
In this third case, we will show conditions under which we can algebraically construct a vector v such that A[C3(A(n))  n]v = en. This will imply by Claim 6 that under those conditions, An[C3(A(n))  n] / Hn.
To define these conditions, recall that S  C3(A(n)) is the set of all j such that ej  Span(A[C3(A(n))]). For j  S, let wj be any vector such that A(n)(3)wj = ej. We next construct a sort of "pseudoinverse" matrix B  RC3(A(n))×C3(A(n)) as follows: For j  S, define Bjk to be the jth entry of wk. Define all other entries of B to be zero.
The following claim shows a condition for En not holding.
Claim 8. If supp(a)  S and aT Ba = 0, then en  Span(A[C3(A(n))  n]).

Proof. Let such that

w := Ba = ajwj
jS
A(n)(3)w = aj ej .
jS

Hence if supp(a)  S,

A(n)(3)w = a.

In this case, define w  RC3(A(n))n to be the vector equal to w on the coordinates in C3(A(n)) and -1 on coordinate n. Then

(A[C3(A(n))  n]w)j = 0 j  C3(A(n))

(48)

and

(A[C3(A(n))  n]w)n = aT Ba.

(49)

13

Evidently, if aT Ba = 0, then so en  Span(A[C3(A(n))  n]).

A[C3(A(n))  n]w aT Ba

=

en,

By definition, in the third case, we have |S|  n - 1 - r. Hence

Pr[En  k  r]  Pr [supp(a)  S  |S|  cn - r]

+ Pr[aT Ba = 0  |S|  cn - r].

(50)

Notice that S is a function of A(n) and so a is independent from S. It is easy to check that for any set S,

d n-|S| d(n - |S|)

Pr [supp(a)  S]  1 - 1 -



.

n

n

As by Lemma 7, with probability 1 - o(1), we have cn  n

1

-

1 d2

, we have

Pr [supp(a)



S

 |S|



cn

- r]



o(1) +

d(n - n(1 - n

1 d2

)

+

r)

=

o(1) +

1 d

+

dr n

=

o(1) +

dr .
n

(51)

Combined with Equation 43, the contribution from this equation yields the third term in the right hand side of Equation 38.
We will break up the second term in Equation 50 by conditioning on whether the support of B has many entries or not, and then by using the independence of a from B:

Pr[aT Ba = 0  |S|  cn - r]  Pr

B / Tccnn-r-u,s  |S|  cn - r

+ max Pr
X Tccnn-r-u,s

aT Xa = 0

.

(52)

For the second probability on the right hand side, notice that for any positive integers a  b, if x and y are random vectors from some product distributions P a and P b respectively, then

max Pr xT Xx = 0  max Pr yT Xy = 0 .

X Taa-r-u,s

X Tbb-r-u,s

(53)

Hence since cn  n

1

-

1 d2

with probability 1 - o(1), we have

max Pr
X Tccnn-r-u,s

aT Xa = 0

 o(1) +

max

Pr

X Tnn-n/d2-r-u,s

ATn XAn = 0

.

(54)

To bound the first probability on the right hand side of Equation 52, observe that if |S|  cn - r and B / Tccnn-r-u,s, there must exist at least u different j  S such that supp(wj)  s.
So

Pr B / Tccnn-r-u,s  |S|  cn - r  Pr |{j : x = 0 : A(n)(3)x = ej, |supp(x)| < s}|  u

n  Pr
u

x = 0 : A(n)(3)x = e1, |supp(x)| < s

,

(55)

where the last inequality follows by Markov's inequality. Plugging this and Equation 54 into Equation 52 yields

Pr[aT Ba

=

0



|S|



cn

-

r]



n u

Pr

x = 0 : A(n)(3)x = e1, |supp(x)| < s

+

max

Pr

X Tnn-n/d2-r-u,s

ATn XAn = 0

+ o(1).

(56)

14

Combining this with 51 and 50 yields

dr

Pr[En k



r]



o(1)+

n

+ max
X Tnn--n1/d2-r-u,s

Pr

ATn XAn

=0

n +
u

Pr

x = 0 : A(n)(3)x = e1, |supp(x)| < s

.

(57)

Plugging this and Equations 46 and 47 into Equation 45 and finally Equation 43 yields the lemma.

We are now ready to prove our main theorem.

Proof

of

Theorem

2.

The

proof

follows

by

instantiating

Lemma

8

with

the

following

values:

t=

n C

,

s=

n C

,

r

=

d

n log(d)

,

u

=

n 2

.

Here

C

is

the

constant

from

Lemma

6.

With the following values, it is immediate from the sparse Littlewood-Offord (Lemma 2) and quadratic

Littlewood-Offord theorems (Lemma 3) that the first and last terms in Lemma 8 respectively are o(1). The

third term is at most O(1/ log(d)) which is also o(1). The two lemmas stated immediately after this proof

(Lemmas 9 and 10) will show that the second and fourth terms are o(1).

Assuming these two lemmas, it follows that with probability 1 - o(1), there are no dependencies of at

least n/C rows A(3). To rule out any smaller dependencies in A(3), we use Lemmas 4, 5, and 6:

If there were a k-minimal dependency in A(3), then it would mean that the restriction of these k rows in

A to the set T = C3(A) would:

1. Contain  3k non-zero entries, because A(3) is a 3-core; and

2. Be a k-minimal dependency.

For

k



n d7

,

by

Lemma

4,

no

such

structure

exists

in

A

with

probability

1

-

o(1).

If

n d7

<

k

<

n/C ,

then applying Lemmas 5, and 6 with T

=

C3(A) means that,

if |C3(A)|



n(1-d7),

with

probability at least 1-e-(n), there is no dependency of these sizes in A(3). By Lemma 7, |C3(A)|  n(1-d7)

with probability 1 - o(1).

This rules out all dependencies in A(3) with probably 1 - o(1), proving the theorem.

It remains to prove lemmas 9 and 10.

Lemma 9. Let A  Rn×n be the adjacency matrix of a random Erdos R´enyi graph G(n, d/n). Let d = (1) and let C be the constant in Lemma 6.

n

n

Pr x : A(3)x = 0,

< |supp(x)| < = o(1).

(58)

d log(d)

C

Proof. This is immediate from the medium and large case Lemmas 5 and 6, since the 3-core has size at least

n

1

-

1 d7

with probability 1 - o(1). Hence if there was a dependency of k rows S in the 3-core, it would

mean there would have to be a set T = C3(A) of size at least n

1

-

1 d7

such

that

for

n d log(d)

<

|S|

<

n C

,

the

restriction of the AS[T ] forms a minimal dependency.

Lemma 10. Let A  Rn×n be the adjacency matrix of a random Erdos R´enyi graph G(n, d/n). Let d = (1) and let C be the constant in Lemma 6. Then

n

Pr x : A(3)x = e1, |supp(x)| < C = o(1)

(59)

We reduce can reduce Lemma 10 to Lemma 11, which we should be better suited to prove with our medium/small-case techniques.

Lemma 11. Let A  Rn×n be the adjacency matrix of a random Erdos R´enyi graph G(n, d/n). Let d = (1) and let C be the constant in Lemma 6.
Let A be A(3) with the first column removed. Then

Pr[x : xT A

= 0, |supp(x)| 

n ] = o(1).

(60)

C

15

Proof of Lemma 10 assuming Lemma 11. If such a vector x satisfied A(3)x = e1, then necessarily A x = 0, where x is x with the first (zero) coordinate removed, and A is A(3) with the first row removed. This occurs with probability at most o(1) by Lemma 11.

Proof of Lemma 11. By Lemmas 5 and 6, with probability 1-o(1), there are no vectors x such that xT A = 0

where

n d7



|supp(x)|

<

n/C .

Now,

suppose

there

was

some

set

S

of

size

k

<

n d7

such

that

AS

is

a

k-minimal

dependency.

This

would

mean that in A, there exists a subset of rows U such there are at least 3k non-zero entries in AS[U ] and

at most 1 column with a single non-zero entry. Indeed, letting U = C3(A), then there must be at least 3k

non-zero entries among AU [S]. Further, since AS is a k-minimal dependency, then there can be no columns

in AS[U ] with a single 1 except for possibly the first column, which does not appear in A .

If 1  U and AS[{1}] has exactly one non-zero entry, then let T := U \ 1. Otherwise, let T := U . It

follows that AS[T ]:

· has at least 3k - 1 nonzero entries and

· has no columns with a single non-zero entry.

By the proof of Lemma 4, with probability 1 - o(1), no such structure exists in A.

References
[1] Kevin P Costello, Terence Tao, and Van Vu. Random symmetric matrices are almost surely nonsingular. Duke Mathematical Journal, 135(2):395­413, 2006.
[2] Kevin P Costello and Van H Vu. The rank of random graphs. Random Structures & Algorithms, 33(3):269­285, 2008.
[3] Patrick DeMichele, Margalit Glasgow, and Alexander Moreira. Distances to the span of sparse random matrices, with applications to gradient coding. to appear, 2021.
[4] Asaf Ferber, Matthew Kwan, and Lisa Sauermann. Singularity of sparse random matrices: simple proofs, 2020.
[5] Tomasz Luczak. Size and connectivity of the k-core of a random graph. Discrete Mathematics, 91(1):61­ 68, 1991.
[6] Van Vu. Recent progress in combinatorial random matrix theory. arXiv preprint arXiv:2005.02797, 2020.

16

