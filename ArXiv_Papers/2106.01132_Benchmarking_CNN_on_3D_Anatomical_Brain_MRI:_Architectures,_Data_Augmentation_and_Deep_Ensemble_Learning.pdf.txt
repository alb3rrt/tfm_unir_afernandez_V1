NeuroImage (2021) Contents lists available at ScienceDirect
NeuroImage

arXiv:2106.01132v1 [cs.CV] 2 Jun 2021

Benchmarking CNN on 3D Anatomical Brain MRI: Architectures, Data Augmentation and Deep Ensemble Learning
Benoit Dufumiera,b,, Pietro Gorib, Ilaria Battagliab, Julie Victora, Antoine Grigisa, Edouard Duchesnaya
aNeuroSpin, CEA Saclay, Universite´ Paris-Saclay, France bLTCI, Te´le´com Paris, IPParis, France

ARTICLE INFO
Article history:
Keywords: Deep Learning, CNN Benchmark, Brain MRI, Data Augmentation, Deep Ensemble Learning

ABSTRACT
Deep Learning (DL) and specifically CNN models have become a de facto method for a wide range of vision tasks, outperforming traditional machine learning (ML) methods. Consequently, they drew a lot of attention in the neuroimaging field in particular for phenotype prediction or computer-aided diagnosis. However, most of the current studies often deal with small single-site cohorts, along with a specific pre-processing pipeline and custom CNN architectures, which make them difficult to compare to. We propose an extensive benchmark of recent state-of-the-art (SOTA) 3D CNN, evaluating also the benefits of data augmentation and deep ensemble learning, on both Voxel-Based Morphometry (VBM) pre-processing and minimally pre-processed quasi-raw images. Experiments were conducted on a large heterogeneous multi-site 3D brain anatomical MRI data-set comprising N = 10k scans on 3 challenging tasks: age prediction, sex classification, and schizophrenia diagnosis. We found that all models provide significantly better predictions with VBM images than quasi-raw data. This finding evolved as the training set approaches 10k samples where quasi-raw data almost reach the performance of VBM. Moreover, we showed that linear models perform comparably with SOTA CNN on VBM data. We also demonstrated that DenseNet and tiny-DenseNet, a lighter version that we proposed, provide a good compromise in terms of performance in all data regime. Therefore, we suggest to employ them as the architectures by default. Critically, we also showed that current CNN are still very biased towards the acquisition site, even when trained with N = 10k multi-site images. In this context, VBM pre-processing provides an efficient way to limit this site effect. Surprisingly, we did not find any clear benefit from data augmentation techniques - and more recently proposed MRI artefacts for brain MRI. Finally, we also showed that big CNN models were not well calibrated when trained with small brain MRI data-sets and we empirically proved that deep ensemble learning is well suited to re-calibrate them without sacrificing performance.
© 2021 Elsevier B. V. All rights reserved.

Corresponding author: Benoit Dufumier e-mail: benoit.dufumier@cea.fr (Benoit Dufumier)
Preprint submitted to NeuroImage

1. Introduction
Since the breakthrough in 2012 of AlexNet [42] during the ILSVRC-2012 challenge, Convolutional Neural Networks
June 3, 2021

2

Dufumier Benoit et al. / NeuroImage (2021)

(CNN) gained a lot of attention in the computer vision community. In the following years, they proved to be the SOTA for various computer vision tasks where enough data were available (typically N > 106); among them, object detection [20], semantic segmentation [8], image denoising [84], etc. Several architectures [63, 27, 82, 68, 32] have been proposed over the years to constantly improve the performance of the networks in particular for 2D image classification on natural images (e.g CIFAR [41], ImageNet [14], MNIST [46]). A key advantage of CNN-based models is that they do not require manual extraction of hand-crafted features and they are able to learn high-level abstractions of images in a hierarchical manner by using back-propagation. However, one main drawback is their need for massive amount of data to converge properly.
As a result, they drew a lot of attention in the neuroimaging field as large open-access MRI databases were becoming available (e.g UKBiobank [10] or the Human Connectome Project [74]). Deep Neural Networks (DNNs) have been used in numerous neuroimaging applications such as image registration [83], tumor detection [26], or brain disease prediction (e.g. Alzheimer's detection [80], schizophrenia [56] or autism [67, 60]).
Different studies, focusing on 3D neuroanatomical MRI data, proposed custom CNN architectures based on recent advances in computer vision to perform various regression or classification tasks (see table 1 for a detailed review). However, none of these papers compared their performance with other SOTA CNN networks. Furthermore, most of them used different pre-processing pipelines and datasets with various size (N ranging from several hundreds to several thousands, see table 1), making it difficult to compare them. Even if some benchmarks started to emerge such as [80] for Alzheimer's detection with anatomical MRI or [58, 36] for phenotype prediction, they are still difficult to reproduce because of a costly pre-processing pipeline and they still lack a fair comparison between SOTA CNN models.

Furthermore, over-fitting is quite common with currently available MRI datasets since they are relatively small compared to standard natural images ones (N < 104 vs N > 106). Data augmentation is one way of limiting this effect by adding artificial samples to the training set and it was employed in most papers (see table 1). These samples are generated by deforming the images in the training set while preserving its semantic information for the target prediction task (e.g by cropping, translating or rotating the images). Nonetheless, there is currently no consensus on the applicability of these transformations to MRI data.
Finally, modern CNN models are known to be over-confident in their prediction, especially for classification tasks [24]. The quantification of their epistemic uncertainty [16] (inherent to the model) is thus of primary importance for clinical applications. Here, we aim at comparing the epistemic uncertainties of SOTA CNNs in the small data regime(N < 103), which is the typical size when dealing with clinical cohorts. Two main scalable methods were proposed in the literature to tackle it: Deep Ensemble learning [43] and MC-Dropout [16]. Deep Ensemble learning has several advantages compared to MC-Dropout: it does not modify the CNN architecture, it is very easy to implement, it generally leads to better performance during challenges (e.g. AlexNet, VGG or GoogLeNet for ILVSRC) and it needs very little hyperparameter tuning [43]. In addition, a recent study [25] has shown that Deep Ensemble learning consistently gives better uncertainty estimates for real-world classification and regression tasks on natural images. It is thus particularly suited for this study1.
1.1. Contributions In this paper, we propose to benchmark SOTA CNN ar-
chitectures on a large-scale multi-centric brain MRI dataset comprising N = 10K scans of healthy participants, namely BHB-10K, pre-processed with two different pipelines:
1We also performed experiments with MC-Dropout. The details can be found in the Supplementary and the results are discussed in section 5.

Dufumier Benoit et al. / NeuroImage (2021)

3

minimally prepocessed quasi-raw data and Voxel-Based Morphometry (VBM) [5], see section 3.3). We also aim at giving the first benchmark of CNN models for schizophrenia's prediction using two independent clinical datasets (see table 2). We show that the pre-processing, as well as the data regime, are critical when performing DL with neuroimaging data. Specifically, we demonstrate that CNN perform equally well in the low data regime N  103), no matter the depth or architecture, and that linear models are still competitive, given an appropriate extensive pre-processing. Secondly, in the big data regime N = 104, CNN perform better than linear models, no matter the pre-processing, but given a sufficiently deep architecture. Critically, we also demonstrate that all models are currently very biased towards the site and that extensive non-linear pre-processing provides a simple way to limit this effect. We show that data augmentation brings little or no improvement in the small data regime (N=500). Finally, we demonstrate that big CNN models are mis-calibrated in this regime but deep ensemble learning provides an efficient way to re-calibrate them while even improving the performance.

ther VBM or Quasi-Raw, cf. table 1) and the classical VGG backbone architecture (repetition of blocks Convolution-Batch Normalization-ReLu with a MaxPooling layer between each block and a kernel size 3 × 3 × 3 for the convolutional layers). Notably, [4] used inception modules followed by a fire module inspired by Inception v3 [68] and SqueezeNet [33] while [36] used the classical ResNet architecture. [9] is the first paper to use a large-scale Inception-based network for transfer learning with age prediction as pre-training.
Classification between schizophrenic patients and Healthy Controls (HC) based on neuroanatomical differences, e.g cortical thinning in prefontal and temporal regions and volume reductions in thalamus, has been widely studied with traditional ML methods such as Support Vector Machine (SVM) [23, 13, 81, 77], Deep Belief Network [56, 54, 44] or Stack Auto-Encoder [55] with shallow neural network architectures. Recent studies [49, 31] are starting to tackle this task without a feature extraction step with a DL approach but they are still limited to custom 3D architectures (e.g designed for video classification in [49]).

As a step towards reproducible research, we provide an open access to the Python code and to the BHB-10K dataset, pre-processed with our 2 different pipelines (quasi-raw and VBM) here2: https://github.com/Duplums/bhb10k-dl-benchmark

Very few works proposed a benchmark between 3D CNN architectures. Among them, in [31], authors compared VGG, ResNet and Inception to discriminate between schizophrenic patients and control subjects, but they limited their tests to shallow architectures with few layers and only to data pre-processed with VBM.

As opposed to UKBioBank or HCP, this dataset is highly multi-centric and the images have been acquired with various protocols and spatial resolutions. More details about the dataset can be found in section 3.1.
2. Related Works
Very recent studies tackled brain age prediction with CNN using 3D neuroanatomical MR images [12, 52, 66, 73, 36, 4, 9]. Most of these works used a single pre-processing (ei-
2Some data-sets still may not be released because of authorization issues but the releasing process is on-going.

In [80], authors proposed to benchmark 2 pre-processing pipelines (named Minimal and Extensive) along with different input dimensions (3D subject-level, 2D slice-level, Region-OfInterest or 3D patch-level) and Transfer Learning strategies for Alzheimer's disease classification with anatomical MRI. They found no difference between 3D-ROI, 3D subject-level and 3D patch-level approach while the 2D slice-level approach performed worse. However, all tested architectures shared the VGG backbone and they did not integrate other SOTA improvements such as skip-connection (ResNet), inception module (Inception) or feature re-using (DenseNet).
Authors in [36] compared classical ML algorithms (Ridge

4

Dufumier Benoit et al. / NeuroImage (2021)

Task

Study Backbone N

[52] VGG 14K [66] VGG 724 [12] VGG 2001

Age Prediction [73] VGG 1101

[36] ResNet 1264

Inception

[4]

and

562

SqueezeNet

[76] Inception 12988

[9] Inception 11729

[48] VGG 427

[80] VGG 1455

[7]

VGG 1198

AD vs HC

[40]

VGG and ResNet

231

[29] VGG 210

[62]

VGG and ResNet

2780

[1] ResNet 828

[59]

ResNet and DenseNet

515

[64] ResNet 785

[79] DenseNet 833

ASD vs CTL

[67] [60]

VGG VGG

1064 935

VGG,

SCZ vs CTL

[31] ResNet and 450 Inception

[49] VGG 866

Sex Classification [52] VGG

6K

Preprocessing
Quasi-Raw VBM
Quasi-Raw and VBM VBM
Quasi-Raw and VBM
VBM
Raw Raw VBM Quasi-Raw and VBM Quasi-Raw Quasi-Raw Unclear Quasi-Raw VBM Unclear VBM Quasi-Raw Quasi-Raw Quasi-Raw
VBM
Quasi-Raw Quasi-Raw

Data Augmentation
Translation and Flip 
Translation and Rotation Translation, Crop and Flip
Translation and Rotation

Ensemble Learning
  



Translation and Flip







Flip and Intensity Scaling











Flip































Zooming, Affine and Flip















Translation and Flip



Table 1: Summary of studies tackling 5 clinical problems using 3D neuroanatomical MRI data with various kind of SOTA 3D CNN backbones. AD: Alzheimer's Disease; ASD: Autism Spectrum Disorder; SCZ: schizophrenia; HC: Healthy Control; N: total number of samples in the dataset; Quasi-Raw: minimal preprocessing including mostly linear registration to the MNI template; VBM: non-linear registration to the MNI template along with segmentation of the tissues and brain extraction. Note: for AD vs HC, we did not report the study with clear data leakage as defined in [80].

Regression, Gaussian Process Regression) with a ResNet-based CNN architecture on age prediction. They found that CNN model performs better than ML methods when trained on several brain tissues and Jacobian map extracted from T1-weighted images. They also observed a site effect when they tested their algorithm on an independent data-set (+70% l1 error when tested on IXI dataset without transfer learning).
Last year, the authors of [58] systematically compared DNN models (both MLP and CNN) with linear models and non-linear SVM on age and sex classification using anatomical MRI, as the number of training samples increases. They conclude that DNN models perform equally well than traditional ML algo-

rithms. Differently from our study i) they treated age and sex classification together by performing a 10-class classification, ii) they used feature selection to perform the classification, iii) their purpose was not to compare CNN models but rather to compare DNN with ML models, iv) their study used solely UKBiobank to train and test their model (only one acquisition protocols with 3 identical scanners) while we propose a new benchmark on a highly multi-centric brain MRI dataset pre-processed with 2 pipelines, and openly accessible.
More recently, [2] showed that DNN scale very well for age and sex prediction with anatomical MRI, given a large homogeneous dataset (UKBioBank with N = 104 in their case). As

Dufumier Benoit et al. / NeuroImage (2021)

5

opposed to [58], they critically demonstrated that DNN can provide a relevant representation for the task at hand (better than most ML approaches), when no feature extraction step is performed beforehand. However, their work is also limited to a big homogeneous dataset and a comprehensible study of CNN architectures is still lacking when dealing with MR images.
3. Material and Methods
Here, we present the datasets used throughout the experiments for age and sex prediction on healthy cohorts (regression and classification tasks respectively), in section 3.1, and schizophrenia's prediction (Dx), in section 3.2. Furthermore, even if CNN are known to perform well on raw data [22, 45] (at least on vision tasks, e.g. classification with ImageNet [14]), it is still not clear whether their performance on neuroimaging data can be impacted by an extensive preprocessing and to what extent it depends on the training size. To answer these important questions and similarly to [12] and [80], we studied 2 kinds of preprocessing, namely VBM and Quasi-Raw detailed in section 3.3. The architectures of SOTA CNN are described in section 3.4 and the data augmentation and deep ensemble strategies can be found in sections 3.10 and 3.11 respectively.
3.1. BHB-10K Dataset

with strict schizophrenia and also composed of T1-weighted 3D MRI scans (see table 2). SCHIZCONNECT-VIP combines 4 publicly available cohorts of controls and patients with schizophrenia. These cohorts are heterogeneous both in terms of acquisition scanners and geographical sites. As for BSNIP, the MR images were acquired on 5 different centers with 3T scanners spread across the USA. It contains cohorts of healthy controls and patients with schizophrenia and all the clinical assessments were standardized. Crucially, BSNIP is only used as a test set throughout this study while SCHIZCONNECT-VIP and BHB-10K are used for training and validation (see table 4 in the Supplementary)

Datasets SCHIZCONNECT-VIP

BSNIP [69]

BIOBD [30]

 BHB-10K

HCP IXI CoRR NPC NAR RBP OASIS 3

MALAPIoBIBCG-cILIaBDSDlePMiEiEzpeIzIrIig

Total

Diagnosis schizophrenia
control schizophrenia
control control
control

# Subjects 275 330 194 200 356 1113 559 1371 65 303 40 597 1570 622 567 559 82 316 7764

N 275 330 194 200 356 1113 559 2897 65 323 40 1262 1639 977 567 580 82 316 10420

Age 34 ± 12 32 ± 12 34 ± 12 38 ± 13 40 ± 13 29 ± 4 48 ± 16 26 ± 16 26 ± 4 22 ± 5 22 ± 5 68 ± 9 21 ± 3 30 ± 12 17 ± 8 15 ± 9 25 ± 7 37 ± 19 32 ± 19

Sex (%F) 27 47 44 58 55 45 55 50 55 58 52 62 58 45 17 30 56 40 50

# Sites 4 4 5 5 8 1 3 19 1 1 1 3 1 3 17 19 2 2 73

Table 2: Demographic information about the datasets. The number of sites indicates the number of acquisition MRI scanners used in the study. Each scanner does not necessarily use the same magnetic field intensity (e.g in the IXI study, three different scanners were used: a Philips 3T, a Philips 1.5T and a GE 1.5T).

We aggregated 13 publicly available data-sets coming from various data-sharing initiatives and including N = 10420 T1weighted 3D MRI scans for 7764 healthy individuals (with several sessions per participant in some cases). The acquisitions were performed with either 1.5T or 3T scanners with potentially different acquisition protocols across sites (see table 2 and the data-set sources for more details).
3.2. Clinical Datasets In addition to BHB-10K, we also gathered 2 other indepen-
dent multi-site data-sets, namely SCHIZCONNECT-VIP3 and Bipolar and Schizophrenia Network for Intermediate Phenotype (BSNIP) [69], including both healthy controls and patients
3http://schizconnect.org

3.3. Preprocessing 3.3.1. Voxel-Based Morphometry
The VBM pre-processing was performed with CAT12 [19] from the SPM toolbox. It essentially consisted of a correction of the bias field and the noise in MRI images, the segmentation of Gray Matter (GM), White Matter (WM), and the cerebrospinal fluid (CSF). Images were normalized into a common standard MNI space composing a linear transformation that accounts for global alignment (rotation, translation, and global brain size) with a non-linear deformation [5] that locally aligns brain structures. The normalized images are finally modulated by the Jacobian of their transformation to preserve the quantity of tissue. The images were re-sampled to an isotropic 1.5mm3 spatial resolution. The final output dimension is 121×145×121. We retained only 2,122,945 voxels of GM maps. To remove

6

Dufumier Benoit et al. / NeuroImage (2021)

Figure 1: Example of a 3D input image pre-processed with the Quasi-Raw framework (top) and the VBM procedure (bottom). At the top, the image is resampled to 1.5mm3 isotropic, linearly registered to the MNI template and the brain is extracted. At the bottom, a non-linear registration algorithm is applied with the DARTEL algorithm [5] and the image is modulated by the Jacobian of the deformation field. It is then resampled to 1.5mm3 isotropic.

the Total Intracranial Volume (TIV) co-variate effect, each GM map was normalized by the TIV estimated by CAT12 during the segmentation step. Note that this step cancels the part of the Jacobian that stems from the initial linear transformation. Finally, we also applied a visual quality check and we removed images poorly segmented or with obvious MR artefacts.
3.3.2. Quasi-raw data This pre-processing was designed to be minimal. Conse-
quently, only essential steps have been kept in order to map the images coming from different sites and scanners to the same space with the same resolution and only important image correction steps have been applied. Specifically, each scan is reoriented to the MNI space and then re-sampled to a 1.5mm3 spatial resolution through a linear spline interpolation. In the Supplementary (see table 5), we show that re-sampling at a higher resolution (e.g 1mm3) does not improve the results. The bias field is corrected using the N4ITK algorithm [72] from ANTs [6] and the brain is extracted with BET2 [34] (the skull and nonbrain tissues are removed). Each image is linearly registered (9 degrees of freedom) to the MNI template with FLIRT from FSL [35]. Finally, we also applied a visual QC to the output images.

An example of a scan pre-processed with these 2 pipelines is shown in figure 1.
3.4. CNN Models
In this work, we selected four CNN architectures which are widely used in computer vision and in most neuroimaging studies (see table 1). Specifically, we considered VGG [63], ResNet [27], ResNeXt [82] (which combined the ideas from ResNet and Inception [68]) and DenseNet [32].
VGG has a lot of different adaptations for different applications in the neuroimaging field e.g Alzheimer's detection [48, 80, 7], age prediction [12, 66], etc. The core idea is to stack multiple layers, typically following the scheme ConvolutionBatch Normalization-ReLu, with a small kernel size, usually equal to 3, for each convolution layer. Five blocks are usually used between each MaxPooling layer and the number of channels inside each block is typically set to 64, 128, 256, 512 and 512 respectively.
tiny-VGG [12] is currently the SOTA algorithm for age prediction on the BAHC [12] dataset and it has been designed based on VGG11, with 8 times less channels per block and a small Fully-Connected layer at the end. It is the smallest CNN

Dufumier Benoit et al. / NeuroImage (2021)

7

included in this study with 800K parameters.

1.8M parameters for tiny-DenseNet).

SFCN [52] is another network designed for age and sex prediction (SOTA on the UKBiobank dataset [10]) and it is also based on VGG. The main difference resides in a deeper architecture with 7 blocks, more channels per block, and a dropout layer put at the end in order to reduce over-fitting. In [52], the authors introduced also a Data Augmentation strategy and they considered the age prediction problem as a classification problem. Here, as we want to give a fair comparison between SOTA CNN architectures, we did not use this strategy.
ResNet [27] introduced skip-connections to avoid the vanishing-gradient issue, often observed with very deep CNN architectures, and to prevent over-fitting. This allows the use of more layers and parameters without losing the generalization capacity of the models. It has been shown to perform well on UKBioBank and IXI [36] for age prediction. Consequently, we compared 3 ResNet models with various depth and size (namely ResNet18, ResNet34 and ResNet50).
ResNeXt [82] integrates the advances from ResNet and Inception, making CNNs deeper and wider, while preserving the same number of parameters and FLOPs complexity of ResNet. We chose the ResNeXt50 model to have a direct comparison with ResNet50.
Finally, DenseNet [32] introduced the concept of feature reusing. It is lighter than all the previous networks (except for tiny-VGG and SFCN) while it performs better than the traditional ResNet on ImageNet. It also gave SOTA results for Alzheimer's detection [79].
We also propose a tiny version of DenseNet121, named tinyDenseNet. To build this model, we analyzed the internal latent representations of DenseNet121 trained on Dx. We first computed the similarity matrix between each layer of a trained DenseNet using the SVCCA [57] algorithm and we observed a strong correlation between the blocks 2 and 3 (see Supplementary). We thus removed the 3rd block and we decreased the growth rate from k = 32 to k = 16 to have a network about 10 times smaller than the original DenseNet121 and with a size comparable with tiny-VGG (892K parameters for tiny-VGG vs

3.5. Comparison to Regularized Linear Models For comparison purposes, we also evaluated the performance
of 2-regularized regression and logistic regression (for classification) on the 3 target tasks as it has been shown to perform comparably with kernel methods and CNN models [58, 1] (at least for phenotype prediction in the small data regime). The penalty term   {10-2, 10-1, 1} is tuned by grid-search on the validation set.
3.6. 2D slice-level CNN For completeness, we compared the 2D-slice level approach
against its 3D counterpart. Specifically, we performed the same experiments in the small data regime (N = 500) with 2D ResNet, DenseNet and VGG. Each 3D scan is decomposed into chunks of 3 consecutive axial slices. The detailed experiments and results can be found in the Supplementary and they are discussed section 5.
3.7. Metrics, loss functions and optimizer For binary classification tasks, we always reported the Area
Under Curve-ROC (AUC) as a reference metric to compare the models. It does not depend on the threshold of the classifier and it allows to compare only the discriminating power of the networks. The balanced accuracy (mean between sensitivity and specificity) has also been reported. For age prediction, we reported both the 1 (Mean Absolute Error or MAE) and 2 (Mean Squared Error) errors as well as the Pearson correlation coefficient r between the true age y and the predicted age y^ and the coefficient of determination R2 obtained with a linear regression of y vs y^. Since we wanted to be robust to outliers, we used the 1 loss for age prediction. As for sex prediction and Dx, we simply used the Binary Cross-Entropy (BCE) loss and, since the dataset was balanced for these 2 classification problems, we did not weight our loss.
3.8. Cross-Validation Strategy In order to report the scores on the independent test set
BSNIP with different training sample sizes, we performed Repeated Learning-Testing (RLT) [3], similarly to [1, 58]. RLT

8

Dufumier Benoit et al. / NeuroImage (2021)

is sometimes also referred to as Monte-Carlo Cross-Validation (MTCV), even if in MTCV one could theoretically use the same training split more than once [3]. Specifically, for a given training set size N  {100, 300, 500, 103, 1600, 104}, we randomly picked N training samples among Ntot (Ntot = 10420 for age/sex prediction and Ntot = 605 for Dx), stratified on the label to predict (in order to avoid any bias during the sampling). The left-out set containing Ntot - N samples is used for validation for Dx. The independent set BIOBD [30] is used for validation for age/sex prediction. We repeated this procedure 10× for N < 500, 5× for 500  N < 104 and 3× for N = 104. We used BSNIP as independent test set and we reported in Fig. 3 the averaged results with the corresponding standard deviation. For each repetition, we constantly sampled the same training/validation sets for the different models. Please note that we chose RTL instead of k-fold stratified CV since we wanted to fix the number of training samples at each run.

3.9. Learning Curves and Convergence Speed

For the 3 tasks at hand, we compared the performances of the

CNN architectures, described above, as the training size varied from N = 100 to N = 104 (resp. N = 500) for age and sex

prediction (resp. Dx). Importantly, we tested the models on the

independent test set BSNIP with a constant size of N = 200

(resp. N = 394). In order to have a fair comparison, we did not

perform any data augmentation in these experiments.

We used an early-stopping criteria to stop the training based

on

the

validation

loss.

Specifically,

let

(^ (kn))2

=

1 k

ni=+nk+1(Li -

L¯n,k)2 be the rolling window variance of the validation loss L

over

k

epochs

where

L¯ n,k

=

1 k

n+k n=n+1

Ln.

We

stop

the

training

at epoch n when ^ k(n) < . We fixed k = 20 and = 0.6 for age

prediction (i.e., 7.2 months), and = 0.05 for sex prediction and

Dx. Intuitively, this means that we consider that the network

has converged if the validation loss remains stable for the next

k iterations (without improvement or deterioration). In practice,

setting bigger k did not change the stopping epoch n (see figures

10 and 11 in the Supplementary).

In addition, we also studied the convergence speed of our net-

works for each task according to the number of training samples

N. To do this, we reported the number of iteration steps until convergence (defined above) for each network, task and N for both quasi-raw and VBM pre-processing (see Table 3 and figure 8 in the Supplementary).
3.10. Data Augmentation Strategies
As pointed out in table 1, we tested various data augmentation strategies. We considered 5 classical transformations widely used in computer vision: random flips, Gaussian blur, Gaussian noise, random crop and affine transformations. All of them are supposed to preserve the semantic information inside the images, while imposing stronger geometric invariance to the final trained CNN model. Furthermore, we also evaluated recent augmentation techniques [61, 85, 75] specifically designed for MRI data: ghosting artefacts [85], spike artefact [85], bias-field artefact [75] and motion artefact [61]. Finally, we also tested swapping [11] which has been originally introduced in the context of self-supervision for both classification and segmentation tasks, in particular on MR images. It consists in picking 2 patches at random location in the image and swapping them. This procedure is repeated 20 times. Originally, a decoder was added to the network so that it could restore back the initial image x based on the context surrounding each misplaced patch. Here, as we did not want to modify the architecture, we directly perform the downstreaming task (age prediction, sex prediction or Dx) by giving the transformed image x~ to the CNN. The network should implicitly learn the anatomical features of x given x~ to perform the downstreaming task.
All of these transformations, along with their hyperparameters, are detailed in table 7 in the Supplementary. They have all been applied on-the-fly during training with a probability of p = 0.5 for each input scan. The test set was never transformed and we did not apply Test-Time Augmentation [63] as the network should be already invariant to the transformations applied during training. We propose to assess the importance of each data augmentation technique separately using either VBM or quasi-raw data for the 3 tasks. To the best of our knowledge, this is the first time MRI artefacts are employed for data aug-

Dufumier Benoit et al. / NeuroImage (2021)

9

Figure 2: Illustration of the data augmentation techniques applied to quasi-raw data. The same transformations have been applied to VBM data on-the-fly during training (except for MRI artifacts)

mentation for age prediction, sex classification and Dx. Please note that we applied MRI artefacts only on quasi-raw images and not on VBM data since they were conceived for images and not for gray matter density maps. Indeed, in order to apply MRI artefacts, one needs to compute the inverse Fourier transform to map the image back to the k-space [85]. When considering VBM data, one would also need to compute the backward mapping from gray matter density to the original image and this would be computationally too demanding and prone to error.
3.11. Deep Ensemble Learning
Uncertainty quantification in DL is very important, especially for clinical applications. In [43], authors introduced deep ensemble learning as a simple method to integrate both aleatoric uncertainty (related to the noise in the data) and epistemic uncertainty [16, 37] (associated to the model's uncertainty). It consists in training independently T identical DNN with different starting points (w0t )t[1,..T] and shuffling the data during the stochastic gradient descent optimization step. At the end of the optimization, this gives T models fw^t where each model's weights w^t can be seen as a sample of an approximation of the highly multi-modal distribution p(w|X, Y) where (X, Y) repre-

sents the training set (more details in [25]). Usually, for classification, fw(x) is the output after the softmax layer, giving a probability vector. For regression, fw(x) can be modelled as a Gaussian distribution whose mean and variance (representing the aleatoric heteroscedastic uncertainty [37]) are learnt during training by optimizing the log-likelihood [43, 25]. Here, as we want to study the small data regime and we are interested in the epistemic uncertainty, we fix the variance for regression and we do not optimize it4. Thus, fw(x) outputs only one value. With the proposed framework, the final prediction for an input image x is given by:

·

for classification:

p^(y|x) =

1 T

T t=1

fw^t (x)[y]

· for regression: p^(y|x) = N(y; µ^(x), ^ 2(x)) with µ^(x) =

1 T

T t=1

fw^ t (x)

and

^ 2(x)

=

1 T

Tt=1( fw^ t (x) - µ^ (x))2

As pointed out in [25], Monte-Carlo Dropout [17, 16] could be another simple way to quantify aleatoric and epistemic uncertainties. It has been successfully applied in the medical imaging field to diabetic retinopathy diagnosis [15, 47]. How-

4We also observed a strong over-fitting effect when we tune the variance on age prediction with N = 500.

10

Dufumier Benoit et al. / NeuroImage (2021)

ever, it has been shown to under-perform compared to Deep Ensemble Learning on various real-world computer vision tasks [25]. As a result, we employed the latter method. Please note that we also evaluated Monte-Carlo Dropout by integrating Concrete Dropout [18] in our models (see Supplementary). The results are discussed in section 5.
In practice, we evaluated the calibration error of our CNN models within the Deep Ensemble learning framework to quantify their predictive uncertainty. Briefly, a well calibrated classifier should give a probability for a given class equals to its occurrence's probability. A mis-calibrated model indicates that it makes under or over-confident predictions. It is usually measured by the Expected Calibration Error (ECE) that gives the confidence error between a perfectly calibrated model and the model at hand. This metric can be extended to regression with the Area Under Calibration Error (AUCE) score as introduced in [25] (see Supplementary for more details).

First, we acknowledge that 3D CNN models performed always comparably or better than their 2D counterpart. We chose to always keep this 3D approach. Second, while all the networks performed very similarly on all tasks for the VBM pre-processing, a strong over-fitting effect has been observed on quasi-raw data, independently from the depth or size of the networks. Notably, SFCN outperforms the other CNN on age prediction with quasi-raw data thanks to its dropout layer. However, it under-performs on the other two tasks. Based on these results, we provide an in-depth study on 4 of these networks, namely tiny-VGG, tiny-DenseNet, DenseNet121 and ResNet34, as representative of the main CNN families. We did not retain ResNeXt (Inception-ResNet family) for computational reasons (it takes 3× much time than DenseNet for a feed-forward pass) and because it gave very similar results with the other networks.
4.2. Learning Curves

3.12. Optimization and Implementation Details
Input data from both pre-processing were normalized to have zero mean and unit variance. Furthermore, we use the optimizer Adam [39] with the default parameters 1 = 0.9 and 2 = 0.999 and we set the learning rate to  = 10-4 decreasing it by a constant factor  every 10 epochs. This factor is tuned between {0.2, 0.5, 0.9} through cross-validation for each task and training set size. We also set a batch size b = 8 to limit the computational cost for N < 104 and b = 32 for N = 104. All CNN architectures are implemented with Pytorch v1.6 [50] while the linear models with scikit-learn v0.23 [51]. Finally, the experiments with N = 500 samples were all performed on a single Quadro RTX 8000 GPU with 48GB. The code is available here.

We have reported the performances of the neural networks as well as of the linear model in figure 3. For age and sex prediction, DenseNet and ResNet always perform better than the linear model, no matter the pre-processing, but given enough data (N 103). Bigger models (e.g DenseNet) also perform better on quasi-raw data when N = 104 than their smaller counterpart (e.g tiny-DenseNet). Importantly, as before, we generally observed a drop in performance when using quasi-raw data as opposed to VBM. We will investigate more this difference in generalization in the next section.
For completeness, we also plotted the convergence plots of the CNN architectures in the supplementary (Fig. 8). We used the same convergence criterion for all networks and tasks, as defined in 3.9.

4. Results

4.3. Site Effect

4.1. CNN Performance on Dx, Age and Sex Prediction at N=500
Table 3 summarizes the results with the architectures presented in section 3.4 and using N = 500 training samples within a 5-fold RLT strategy described in section 3.8. We also reported the performance of 2D CNN models in the Supplementary.

In figure 4, we plotted the performances of the CNN models when they are evaluated on images coming from the same acquisition sites they are trained on (in-site) or different ones (out-site). Two main effects can be observed. First, there is a strong gap between performance on in-site and out-site images, even when CNNs are trained on a big multi-site data-set

Dufumier Benoit et al. / NeuroImage (2021)

11

Preprocessing VBM
Quasi-Raw

Architecture

Model

#Params

Linear Model 5

400K

DenseNet121 [32] 11.2M

ResNet18 [27] 33.1M

ResNet34 [27] 63.4M

ResNet50 [27] 46.1M

VGG11 [63]

50.1M

tiny-VGG [12] 892K

tiny-DenseNet

1.8M

ResNeXt [82] 25.8M

SFCN [52]

2.9M

Linear Model DenseNet121 [32]
ResNet18 [27] ResNet34 [27] ResNet50 [27] VGG11 [63] tiny-VGG [12] tiny-DenseNet ResNeXt [82]
SFCN [52]

400K 11.2M 33.1M 63.4M 46.1M 50.1M 892K 1.8M 25.8M 2.9M

MAE  7.19 ± 0.17 6.02 ± 0.24 6.91 ± 0.52 7.14 ± 0.28 6.26 ± 0.29 7.03 ± 0.44 6.94 ± 0.12 6.43 ± 0.22 6.24 ± 0.21 6.60 ± 0.35
10.73 ± 2.65 7.91 ± 0.42 8.33 ± 1.05 12.9 ± 2.53 16.42 ± 3.23 9.84 ± 0.48 14.7 ± 4.19 13.30 ± 2.2 6.68 ± 0.23

Age

RMSE 

r

8.56 ± 0.17 0.78 ± 0.014

7.25 ± 0.34 0.83 ± 0.008

8.47 ± 0.64 0.81 ± 0.02

8.87 ± 0.38 0.75 ± 0.03

7.42 ± 0.3 0.83 ± 0.02

8.63 ± 0.56 0.78 ± 0.02

8.21 ± 0.20 0.78 ± 0.02

7.74 ± 0.24 0.81 ± 0.02

7.47 ± 0.16 0.83 ± 0.007

8.07 ± 0.37 0.77 ± 0.03

13.36 ± 3.01 9.82 ± 0.55 10.31 ± 1.25 15.50 ± 2.73 18.83 ± 3.09 12.08 ± 0.66
20 ± 8.0 16 ± 2.32 8.40 ± 0.30

0.53 ± 0.05 0.69 ± 0.03 0.71 ± 0.03 0.59 ± 0.04 0.69 ± 0.02 0.66 ± 0.01 0.52 ± 0.07 0.56 ± 0.05 0.75 ± 0.02

R2  0.61 ± 0.02 0.70 ± 0.01 0.66 ± 0.03 0.57 ± 0.05 0.70 ± 0.03 0.62 ± 0.03 0.61 ± 0.04 0.66 ± 0.03 0.68 ± 0.01 0.60 ± 0.03
0.28 ± 0.05 0.48 ± 0.04 0.51 ± 0.04 0.34 ± 0.05 0.48 ± 0.03 0.43 ± 0.02 0.27 ± 0.07 0.32 ± 0.05 0.56 ± 0.05

Sex

AUC 

BAcc 

0.93 ± 0.01 0.82 ± 0.02

0.91 ± 0.01 0.83 ± 0.01

0.91 ± 0.01 0.83 ± 0.01

0.91 ± 0.01 0.80 ± 0.02

0.91 ± 0.008 0.81 ± 0.02

0.80 ± 0.02 0.72 ± 0.015

0.91 ± 0.005 0.82 ± 0.007

0.88 ± 0.01 0.81 ± 0.02

0.88 ± 0.007 0.79 ± 0.02

0.85 ± 0.02 0.76 ± 0.02

0.60 ± 0.03 0.81 ± 0.01 0.84 ± 0.01 0.84 ± 0.02 0.80 ± 0.01 0.68 ± 0.05 0.76 ± 0.03 0.76 ± 0.03 0.78 ± 0.02 0.75 ± 0.01

0.50 ± 0.001 0.68 ± 0.05 0.63 ± 0.02 0.60 ± 0.02 0.62 ± 0.04 0.54 ± 0.02 0.65 ± 0.05 0.62 ± 0.06 0.61 ± 0.02 0.53 ± 0.02

Dx

AUC 

BAcc 

0.78 ± 0.007 0.71 ± 0.01

0.78 ± 0.01 0.72 ± 0.02

0.78 ± 0.01 0.71 ± 0.01

0.75 ± 0.005 0.69 ± 0.01

0.79 ± 0.009 0.71 ± 0.01

0.72 ± 0.009 0.66 ± 0.03

0.79 ± 0.008 0.71 ± 0.01

0.79 ± 0.01 0.72 ± 0.01

0.77 ± 0.01 0.70 ± 0.009

0.77 ± 0.015 0.69 ± 0.01

0.51 ± 0.02 0.72 ± 0.01 0.66 ± 0.01 0.66 ± 0.03 0.66 ± 0.009 0.61 ± 0.02 0.69 ± 0.01 0.68 ± 0.02 0.64 ± 0.01 0.70 ± 0.008

0.50 ± 0.002 0.65 ± 0.02 0.63 ± 0.02 0.61 ± 0.02 0.61 ± 0.01 0.53 ± 0.04 0.62 ± 0.01 0.62 ± 0.01 0.60 ± 0.007 0.64 ± 0.02

Resources

Time -
11s 5s 8s 7s 23s 7s 6s 28s 6s

GPU -
10GB 4.6GB 5.4GB 9GB 26GB 4.7GB 7GB 10GB 10.6GB

-

-

11s 10GB

5s 4.6GB

8s 5.4GB

7s 9GB

23s 26GB

7s 4.7GB

6s 7GB

28s 10GB

6s 10.6GB

Table 3: Comparison between several architectures and preprocessing at N=500 samples in the training set without data augmentation. The results are reported on BSNIP, an independent test set, with a 5-fold RLT. MAE=Mean Absolute Error, RMSE=Root Mean Squared Error, r=Pearson correlation coefficient, R2=coefficient of determination, AUC=ROC-Area Under Curve, BAcc=Balanced Accuracy, Time=GPU time over 1 epoch for age prediction and GPU=GPU memory usage during
training for age prediction.

(+3.5 MAE p < 10-3 for age prediction, -3.6% AUC p < 10-3 for sex prediction at N = 104 training samples for quasi-raw images). Second, for out-site images, CNN models perform significantly better with VBM data than quasi-raw data when N  103, in line with results table 4.1 (+2.2 MAE p < 0.01, -9% AUC p < 0.003). However, it is not the case for in-site images where CNNs perform similarly.
4.4. Data Augmentation
In figure 5, we showed the results when using data augmentation (D.A) for the 3 tasks with Ntrain = 500 samples (small reallife data regime). We only tested the usefulness of each strategy alone and not their combination, since this would have been computationally too demanding. Here, we only used DenseNet since it performs well on all tasks, except for age prediction with quasi-raw data (see fig. 4.2). In that case, we trained ResNet34 because it was much more stable. Overall, D.A. seems to be counter-productive for age prediction. MRI artefacts have no benefit for diagnosis prediction and they only help for sex prediction when using quasi-raw data. Classical affine transformations, crop+resize and Gaussian blurring have a positive impact only when using VBM data for Dx.
4.5. Deep Ensemble Learning
In figure 6, we reported the performances and calibration errors of DenseNet and tiny-DenseNet on the 3 tasks as we

increase the number of independently trained models T (see section 3.11) with Ntrain = 500 training samples (small data regime). We have only used the VBM data to perform this analysis, considering the stability of CNN with this pre-processing. The baselines (dashed lines) are given with the deterministic version of DenseNet and tiny-DenseNet (i.e, T = 1).
5. Discussion
5.1. Extensive CNN Comparison with N = 500
Overall, we did not found significant differences between CNNs in the small data regime with VBM images, no matter the depth or the architecture, for the 3 tasks. This is somewhat expected since very deep architectures have been introduced originally for very large-scale datasets (e.g ImageNet with millions of 2D images). Also, even wider models such as ResNeXt (based on the idea of ResNet with Inception modules) did not help to improve the performance neither on VBM data nor on Quasi-Raw data. However, we noticed a strong over-fitting effect when using quasi-raw images for the 3 tasks and for every network, resulting in a strong drop in performance (-7% AUC on sex prediction between the 2 best models, -6% AUC for Dx and -8% correlation for age prediction). Notably, SFCN performed well on age prediction with quasi-raw data, matching the performance on VBM images (in line with [52]). This can be attribute to its last dropout layer. However, it still under-

12

Dufumier Benoit et al. / NeuroImage (2021)

Figure 3: Learning curves of CNN models with quasi-raw and VBM pre-processings. Overall, CNNs outperform linear model with quasi-raw data but they perform
similarly with their linear counter-part only with VBM images. DenseNet offers a good compromise since it performs equally well in the small data regime on Dx and in the very big data regime N = 104 on age and sex prediction, for both VBM and quasi-raw images. The results are reported on the independent data-set
BSNIP (images were not acquired with the same acquisition protocol than images in train). The results on age prediction with a linear model and quasi-raw images are outside the plot and thus not reported (MAE > 20 for all N  [100, 104]).

performed for the other 2 tasks. As we shall demonstrate, this over-fitting effect can be largely attributed to the bias towards the acquisition sites (in line with [36]), which explains why it has not been systematically reported [12, 80].
Interestingly, we noticed that 2D models perform significantly worse than their 3D counterpart for all tasks, in line with [80] for Alzheimer's detection (see table 6 in the Supplementary). It means that 3D CNN models, while being computationally more expensive during training, benefit from the underlying 3D anatomical structure of the brain that a 2D approach cannot capture, even in the small data regime.
Ultimately, the resources taken by all 3D networks, VGG11 and ResNeXt excepted, were comparable in terms of GPU time. VGG11 was very time-consuming because of its 3 Fully-Connected layers at the end (replaced by a single FullyConnected layer, lighter, in tiny-VGG). ResNeXt is very large, even if it has less parameters than its ResNet50 counterpart, and it remains computationally very costly.

5.2. Scaling up to N = 104
The learning curves reported in figure 4.2 confirmed the overfitting effect observed with N = 500 training samples and quasi-raw data. With the extensive VBM pre-processing, the networks performed better than quasi-raw no matter the data regime (from N = 100 up to N = 104, while being greatly reduced for N 103, see figure 4) or the task. Nonetheless, we also noticed that big networks (DenseNet121, ResNet34 with more than 10M parameters) gave better results when N = 104 with quasi-raw data (-1.7 MAE, p < 10-3 for age prediction, +2% AUC, p < 10-2 on sex prediction between tiny-DenseNet and DenseNet). As a result, adding more parameters (from 11M for DenseNet up to 60M for ResNet) is useful only when dealing with quasi-raw data in the very large data regime.
As reported previously [1], big CNN models outperform linear models in the large-scale data regime when N = 104 with VBM data (-0.7 MAE for age prediction p < 0.003, +0.7% AUC for sex prediction p < 10-2). It was also expected that linear model gives random results (i.e., balanced accuracy

Dufumier Benoit et al. / NeuroImage (2021)

13

Figure 4: Performance of CNN models when trained with a varying number of training data on BHB-10K and tested i) on images coming from the same acquisition sites (top); or ii) on images coming from different sites (bottom). VBM pre-processing reduces the bias induced by acquisition site, especially in the small data regime (N < 103). There is no difference in performance between quasi-raw and VBM data when testing on images coming from the same sites as the training images (in-site). The gray dashed lines in the bottom images represent the average performance on in-site images (across pre-processing and CNN) and it was
reported to ease comparison.

around 0.5) when using quasi-raw data without feature extraction. However, we should remark that it performed very well in the small data regime when N  103 on all tasks, given an appropriate pre-processing (VBM here), in line with [80]. For instance, it performed similarly to all SOTA CNN architectures for Dx up to 500 samples (79% AUC with N = 500, matching tiny-DenseNet or tiny-VGG). Overall, big networks are required when dealing with a big minimally pre-processed MRI data-set to reach SOTA results.

5.3. Site Effect
We should emphasize that we did not retrieve the very accurate results obtained on UKBioBank with N  104 in [52] for age and sex prediction (resp. MAE = 2.1 and Acc = 99% with quasi-raw data). As shown in figure 4, this is largely due to the underlying site effect that highly deteriorates the performance of CNN (+3.5 MAE p < 10-3 for age prediction, -3.6% AUC p < 10-3 for sex prediction with quasi-raw data and N = 104 between in-site and out-site images). Indeed, we retrieved the same performances as in [52] when testing the models on

14

Dufumier Benoit et al. / NeuroImage (2021)

Figure 5: Current data augmentation (D.A) techniques are highly task- and pre-processing-dependent. It does not result in large improvement and, overall, it even degrades the performance for both VBM and quasi-raw images. The error bars are obtained using a 5-split RLT strategy using each time only one data augmentation strategy. We reported the results obtained on the independent test set BSNIP (NHC = 200, NS CZ = 194). The black dashed lines are the baselines without D.A.

in-site images. However, there is a drop in performance when using the same models on a out-site independent test set. Our results also explain why authors in [80] found no difference between minimal and quasi-raw pre-processing on Alzheimer's detection (the results were reported only on an in-site validation set). Even if the extensive non-linear pre-processing indeed removes part of the site-effect, this bias still remains as pointed out figure 4 (difference between gray dashed line and blue line).
As opposed to UKBioBank, our data-sets are highly multicentric and this appears to be critical when performing ML with neuroimaging data (as confirmed by [78], with traditional ML and hand-crafted features for age prediction, or by [21] on sex prediction). As highlighted in [36], exploiting images acquired with the same protocol as the test images is one way to deal with this issue but it assumes to have access to these images before-

hand and to fine-tune the model on them (which is impractical in the real clinical setting). Other solutions are also starting to emerge towards debiased DL algorithms by directly accounting for the bias during the optimization [70]. Even if it is wellknown in computer vision that ML and DL algorithms perform poorly when images from train and test sets come from 2 different domains (i.e domain gap) [71], this issue is still rarely mentioned when performing a benchmark with neuroimaging data [58, 2]. Here, we demonstrated that even SOTA DL models, trained on a large-scale multi-site data-set, still under-perform on out-site images compared to in-site images.
From this perspective, BHB-10K is quite distinctive from UKBioBank for its diversity (images are coming from more than 70 different sites) and we believe that it can provide a new way to test and benchmark ML and DL algorithms on images from sites not seen during training.

Dufumier Benoit et al. / NeuroImage (2021)

15

Figure 6: Performance of big and small networks (resp. DenseNet and tiny-DenseNet) as we increase the number of ensemble models T when N = 500 training samples. tiny-DenseNet is always better calibrated than DenseNet while also performing better in the small data regime. AUC=Area Under ROC Curve, MAE=Mean Absolute Error, ECE=Expected Calibration Error, AUCE=Area Under Calibration Error.

5.4. Data Augmentation
Overall, we found that data augmentation brings little or no improvement for both VBM and quasi-raw images. As opposed to [52, 12, 4], affine transformation and flip did not improve the performance on age prediction. Once again, differently from the above-mentioned studies, our results are reported on an independent data-set which may explain the differences. Interestingly, horizontal and vertical flip degrade significantly the performance only for sex prediction, which is expected since the localization of left and right hemisphere matters to discriminate between male and female. For Dx, we noted some improvements with VBM data when introducing traditional D.A such as affine transformation, crop or Gaussian blur. Please note that we did not smooth the data with a Gaussian kernel

during the VBM pre-processing. As a result, this indicates that smoothing is beneficial when dealing with noisy data such as SCHIZCONNECT-VIP (in line with [1]).
From figure 5, it can be seen that data augmentation is both task- and pre-processing-dependent and it does not necessarily result in large improvement neither for regression nor classification tasks. For an easy task (sex prediction with AUC  0.9) it significantly improves the performance only with quasi-raw data (i.e, with ghosting artefact or Gaussian blur) while for a hard task such as Dx, it helped only on VBM data. This mitigates the usefulness of current data augmentation techniques on brain MRI, especially when all images have been aligned to the same template and re-sampled to the same spatial resolution. Even with the minimal pre-processing (i.e., quasi-raw), there is

16

Dufumier Benoit et al. / NeuroImage (2021)

no clear improvement with the standard D.A (affine transformation, Gaussian blur, etc.). Furthermore, we also showed that adding MRI artefacts into the data augmentation strategy brings overall no improvement and it actually worsen the results most of the time (except for ghosting artefact and spike artefact for sex and age prediction respectively).
5.5. Predictive Uncertainty in the Small Data Regime
Quantifying the model uncertainty associated to a prediction is very important when dealing with computer-aided diagnosis systems. However, this is rarely mentioned in the literature even if simple calibration metrics exist and have been extended to regression [25]. Here, we show in figure 6 that, when using few training samples (Ntrain = 500), small networks are better calibrated than their bigger counterpart (in line with [24] for vision tasks) and they also perform better. We demonstrated that Deep Ensemble learning provides a simple way to better calibrate the models, no matter their size, while also improving the results (in line with [25]). This is not the case for MC-Dropout. Our experiments (see figure 9 in the Supplementary) showed that a well calibrated model does not always perform well on the task at hand. For instance, while Bayesian tiny-DenseNet is perfectly calibrated on Dx with ECE = 0.04 (-8% compared to tiny-DenseNet), it loses 7% AUC compared to its deterministic counterpart. This applies to all tasks and models tested, except for age prediction. These results suggest that in usual clinical applications (Ntrain = 500), it is better to use small networks (e.g tiny-DenseNet) with Deep Ensemble learning since it improves both the calibration error and the accuracy.
6. Conclusion
Throughout this paper, we have empirically studied several properties of 3D CNN models on neuroimaging data. First, we have shown that all CNN models perform significantly better on VBM data than on quasi-raw images, no matter their architecture. We emphasize this gap is greatly reduced as we scale up to N = 10k samples. Importantly, we also demonstrated that simple linear models are on par with SOTA CNN

on VBM, which suggests that DL model fail to capture nonlinearities in the data, as suggested by [58]. However, this conclusion must be taken with caution since we also shown that CNN models are still very biased towards the acquisition site, even when they are supervised on a highly multi-sites data-set with N = 10k samples. Extensive non-linear pre-processing such as VBM provides a simple way to limit this bias but it still does not entirely remove it. This effect has also been reported on several other brain MRI data-sets [21, 78] and Chest X-ray data-set with COVID-19 data [70]. De-biasing methods for DL are starting to emerge mainly in computer vision [70, 38, 28] but with future potential applications to the neuroimaging field. Suprisingly, we also observed overall no benefits from using data augmentation in the small data regime. In this paper, we showed that it is task- and dataset-specific but a more in-depth study is required and left for future work. Finally, while big CNN models were poorly calibrated on all neuroimaging tasks we trained them on (as also reported on vision tasks [24]), we demonstrated that deep ensemble learning provides a simple and effective way to re-calibrate them, by even improving the performance. We highlight the importance of well-calibrated models in particular for clinical applications.
As a step towards reproducible research, we made our code publicly available here and we also provide the BHB-10K dataset used throughout this study. We give access to both quasi-raw and VBM data, directly usable within a Python environment for DL (e.g., Pytorch or TensorFlow).
Data Availability
Data used throughout this study have been collected through various public platforms (see table 2 for all the links). We have also engaged a process to release all the pre-processed datasets publicly available. The releasing status is regularly updated on our Github repository: https://github.com/Duplums/ bhb10k-dl-benchmark
Acknowledgments
This work was performed using HPC resources from GENCI-IDRIS (Grant 2020-AD011011854).

Dufumier Benoit et al. / NeuroImage (2021)

17

References
[1] Abrol, A., Bhattarai, M., Fedorov, A., Du, Y., Plis, S., Calhoun, V., Initiative, A. D. N., et al. (2020). Deep residual learning for neuroimaging: An application to predict progression to alzheimer's disease. Journal of Neuroscience Methods, page 108701.
[2] Abrol, A., Fu, Z., Salman, M., Silva, R., Du, Y., Plis, S., and Calhoun, V. (2021). Deep learning encodes robust discriminative neuroimaging representations to outperform standard machine learning. Nature communications, 12(1):1­17.
[3] Arlot, S. and Celisse, A. (2010). A survey of cross-validation procedures for model selection. Statistics Surveys, 4:40­79.
[4] Armanious, K., Abdulatif, S., Shi, W., Salian, S., Ku¨stner, T., Weiskopf, D., Hepp, T., Gatidis, S., and Yang, B. (2020). Age-net: An mri-based iterative framework for biological age estimation. arXiv preprint arXiv:2009.10765.
[5] Ashburner, J. (2007). A fast diffeomorphic image registration algorithm. Neuroimage, 38(1):95­113.
[6] Avants, B. B., Tustison, N., and Song, G. (2009). Advanced normalization tools (ants). Insight j, 2(365):1­35.
[7] Ba¨ckstro¨m, K., Nazari, M., Gu, I. Y.-H., and Jakola, A. S. (2018). An efficient 3d deep convolutional network for alzheimer's disease diagnosis using mr images. In 2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018), pages 149­153. IEEE.
[8] Badrinarayanan, V., Kendall, A., and Cipolla, R. (2017). Segnet: A deep convolutional encoder-decoder architecture for image segmentation. IEEE transactions on pattern analysis and machine intelligence, 39(12):2481­ 2495.
[9] Bashyam, V. M., Erus, G., Doshi, J., Habes, M., Nasralah, I., Truelove-Hill, M., Srinivasan, D., Mamourian, L., Pomponio, R., Fan, Y., et al. (2020). Mri signatures of brain age and disease over the lifespan based on a deep brain network and 14 468 individuals worldwide. Brain, 143(7):2312­2324.
[10] Bycroft, C., Freeman, C., Petkova, D., Band, G., Elliott, L. T., Sharp, K., Motyer, A., Vukcevic, D., Delaneau, O., O'Connell, J., et al. (2018). The uk biobank resource with deep phenotyping and genomic data. Nature, 562(7726):203­209.
[11] Chen, L., Bentley, P., Mori, K., Misawa, K., Fujiwara, M., and Rueckert, D. (2019). Self-supervised learning for medical image analysis using image context restoration. Medical image analysis, 58:101539.
[12] Cole, J. H., Poudel, R. P., Tsagkrasoulis, D., Caan, M. W., Steves, C., Spector, T. D., and Montana, G. (2017). Predicting brain age with deep learning from raw imaging data results in a reliable and heritable biomarker. NeuroImage, 163:115­124.
[13] de Pierrefeu, A., Lo¨fstedt, T., Laidi, C., Hadj-Selem, F., Bourgin, J., Hajek, T., Spaniel, F., Kolenic, M., Ciuciu, P., Hamdani, N., et al. (2018). Identifying a neuroanatomical signature of schizophrenia, reproducible across sites and stages, using machine learning with structured sparsity. Acta Psychiatrica Scandinavica, 138(6):571­580.
[14] Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248­255. Ieee.
[15] Filos, A., Farquhar, S., Gomez, A. N., Rudner, T. G., Kenton, Z., Smith, L., Alizadeh, M., de Kroon, A., and Gal, Y. (2019). A systematic comparison of bayesian deep learning robustness in diabetic retinopathy tasks. arXiv preprint arXiv:1912.10481.
[16] Gal, Y. (2016). Uncertainty in deep learning. University of Cambridge, 1:3.
[17] Gal, Y. and Ghahramani, Z. (2016). Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning, pages 1050­1059. PMLR.
[18] Gal, Y., Hron, J., and Kendall, A. (2017). Concrete dropout. In Advances in neural information processing systems, pages 3581­3590.
[19] Gaser, C. and Dahnke, R. (2016). Cat-a computational anatomy toolbox for the analysis of structural mri data. HBM, 2016:336­348.
[20] Girshick, R. (2015). Fast r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 1440­1448.
[21] Glocker, B., Robinson, R., Castro, D. C., Dou, Q., and Konukoglu, E. (2019). Machine learning with multi-site imaging data: An empirical study on the impact of scanner effects. arXiv preprint arXiv:1910.04597.
[22] Goodfellow, I., Bengio, Y., Courville, A., and Bengio, Y. (2016). Deep learning, volume 1. MIT press Cambridge.
[23] Gould, I. C., Shepherd, A. M., Laurens, K. R., Cairns, M. J., Carr, V. J., and Green, M. J. (2014). Multivariate neuroanatomical classification of

cognitive subtypes in schizophrenia: a support vector machine learning approach. NeuroImage: Clinical, 6:229­236. [24] Guo, C., Pleiss, G., Sun, Y., and Weinberger, K. Q. (2017). On calibration of modern neural networks. In International Conference on Machine Learning, pages 1321­1330. [25] Gustafsson, F. K., Danelljan, M., and Schon, T. B. (2020). Evaluating scalable bayesian deep learning methods for robust computer vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 318­319. [26] Havaei, M., Davy, A., Warde-Farley, D., Biard, A., Courville, A., Bengio, Y., Pal, C., Jodoin, P.-M., and Larochelle, H. (2017). Brain tumor segmentation with deep neural networks. Medical image analysis, 35:18­31. [27] He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770­778. [28] Hendricks, L. A., Burns, K., Saenko, K., Darrell, T., and Rohrbach, A. (2018). Women also snowboard: Overcoming bias in captioning models. In Proceedings of the European Conference on Computer Vision (ECCV), pages 771­787. [29] Hosseini-Asl, E., Ghazal, M., Mahmoud, A., Aslantas, A., Shalaby, A., Casanova, M., Barnes, G., Gimel'farb, G., Keynton, R., and El-Baz, A. (2018). Alzheimer's disease diagnostics by a 3d deeply supervised adaptable convolutional network. Frontiers in bioscience (Landmark edition), 23:584. [30] Hozer, F., Sarrazin, S., Laidi, C., Favre, P., Pauling, M., Cannon, D., McDonald, C., Emsell, L., Mangin, J.-F., Duchesnay, E., et al. (2020). Lithium prevents grey matter atrophy in patients with bipolar disorder: an international multicenter study. Psychological medicine, pages 1­10. [31] Hu, M., Sim, K., Zhou, J. H., Jiang, X., and Guan, C. (2020). Brain mribased 3d convolutional neural networks for classification of schizophrenia and controls. arXiv preprint arXiv:2003.08818. [32] Huang, G., Liu, Z., Van Der Maaten, L., and Weinberger, K. Q. (2017). Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700­4708. [33] Iandola, F. N., Han, S., Moskewicz, M. W., Ashraf, K., Dally, W. J., and Keutzer, K. (2016). Squeezenet: Alexnet-level accuracy with 50x fewer parameters and¡ 0.5 mb model size. arXiv preprint arXiv:1602.07360. [34] Jenkinson, M., Pechaud, M., Smith, S., et al. (2005). Bet2: Mr-based estimation of brain, skull and scalp surfaces. In Eleventh annual meeting of the organization for human brain mapping, volume 17, page 167. Toronto. [35] Jenkinson, M. and Smith, S. (2001). A global optimisation method for robust affine registration of brain images. Medical image analysis, 5(2):143­ 156. [36] Jonsson, B. A., Bjornsdottir, G., Thorgeirsson, T. E., Ellingsen, L. M., Walters, G. B., Gudbjartsson, D. F., Stefansson, H., Stefansson, K., and Ulfarsson, M. O. (2019). Brain age prediction using deep learning uncovers associated sequence variants. Nature Communications, 10(1). [37] Kendall, A. and Gal, Y. (2017). What uncertainties do we need in bayesian deep learning for computer vision? arXiv preprint arXiv:1703.04977. [38] Kim, B., Kim, H., Kim, K., Kim, S., and Kim, J. (2019). Learning not to learn: Training deep neural networks with biased data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9012­9020. [39] Kingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. [40] Korolev, S., Safiullin, A., Belyaev, M., and Dodonova, Y. (2017). Residual and plain convolutional neural networks for 3d brain mri classification. In 2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017), pages 835­838. IEEE. [41] Krizhevsky, A., Hinton, G., et al. (2009). Learning multiple layers of features from tiny images. [42] Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097­1105. [43] Lakshminarayanan, B., Pritzel, A., and Blundell, C. (2017). Simple and scalable predictive uncertainty estimation using deep ensembles. In Advances in neural information processing systems, pages 6402­6413. [44] Latha, M. and Kavitha, G. (2019). Detection of schizophrenia in brain mr images based on segmented ventricle region and deep belief networks. Neural Computing and Applications, 31(9):5195­5206. [45] LeCun, Y., Bengio, Y., and Hinton, G. (2015). Deep learning. nature, 521(7553):436­444. [46] LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998). Gradient-

18

Dufumier Benoit et al. / NeuroImage (2021)

based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324. [47] Leibig, C., Allken, V., Ayhan, M. S., Berens, P., and Wahl, S. (2017). Leveraging uncertainty information from deep neural networks for disease detection. Scientific reports, 7(1):1­14. [48] Li, F., Cheng, D., and Liu, M. (2017). Alzheimer's disease classification based on combination of multi-model convolutional networks. In 2017 IEEE International Conference on Imaging Systems and Techniques (IST), pages 1­5. IEEE. [49] Oh, J., Oh, B.-L., Lee, K.-U., Chae, J.-H., and Yun, K. (2020). Identifying schizophrenia using structural mri with a deep learning algorithm. Frontiers in psychiatry, 11:16. [50] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. (2019). Pytorch: An imperative style, high-performance deep learning library. arXiv preprint arXiv:1912.01703. [51] Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., et al. (2011). Scikit-learn: Machine learning in python. the Journal of machine Learning research, 12:2825­2830. [52] Peng, H., Gong, W., Beckmann, C. F., Vedaldi, A., and Smith, S. M. (2021). Accurate brain age prediction with lightweight deep neural networks. Medical Image Analysis, 68:101871. [53] Pe´rez-Garc´ia, F., Sparks, R., and Ourselin, S. (2020). Torchio: a python library for efficient loading, preprocessing, augmentation and patchbased sampling of medical images in deep learning. arXiv preprint arXiv:2003.04696. [54] Pinaya, W. H., Gadelha, A., Doyle, O. M., Noto, C., Zugman, A., Cordeiro, Q., Jackowski, A. P., Bressan, R. A., and Sato, J. R. (2016). Using deep belief network modelling to characterize differences in brain morphometry in schizophrenia. Scientific reports, 6:38897. [55] Pinaya, W. H., Mechelli, A., and Sato, J. R. (2019). Using deep autoencoders to identify abnormal brain structural patterns in neuropsychiatric disorders: A large-scale multi-sample study. Human brain mapping, 40(3):944­954. [56] Plis, S. M., Hjelm, D. R., Salakhutdinov, R., Allen, E. A., Bockholt, H. J., Long, J. D., Johnson, H. J., Paulsen, J. S., Turner, J. A., and Calhoun, V. D. (2014). Deep learning for neuroimaging: a validation study. Frontiers in neuroscience, 8:229. [57] Raghu, M., Gilmer, J., Yosinski, J., and Sohl-Dickstein, J. (2017). Svcca: Singular vector canonical correlation analysis for deep learning dynamics and interpretability. In Advances in Neural Information Processing Systems, pages 6076­6085. [58] Schulz, M.-A., Yeo, B. T., Vogelstein, J. T., Mourao-Miranada, J., Kather, J. N., Kording, K., Richards, B., and Bzdok, D. (2020). Different scaling of linear models and deep learning in ukbiobank brain images versus machinelearning datasets. Nature communications, 11(1):1­15. [59] Senanayake, U., Sowmya, A., and Dawes, L. (2018). Deep fusion pipeline for mild cognitive impairment diagnosis. In 2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018), pages 1394­1997. IEEE. [60] Shahamat, H. and Abadeh, M. S. (2020). Brain mri analysis using a deep learning based volutionary approach. Neural Networks. [61] Shaw, R., Sudre, C. H., Ourselin, S., and Cardoso, M. J. (2019). Mri k-space motion artefact augmentation: Model robustness and task-specific uncertainty. In MIDL, pages 427­436. [62] Shmulev, Y., Belyaev, M., Initiative, A. D. N., et al. (2018). Predicting conversion of mild cognitive impairments to alzheimer's disease and exploring impact of neuroimaging. In Graphs in Biomedical Image Analysis and Integrating Medical Imaging and Non-Imaging Modalities, pages 83­91. Springer. [63] Simonyan, K. and Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556. [64] Spasov, S., Passamonti, L., Duggento, A., Lio`, P., Toschi, N., Initiative, A. D. N., et al. (2019). A parameter-efficient deep learning approach to predict conversion from mild cognitive impairment to alzheimer's disease. Neuroimage, 189:276­287. [65] Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014). Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):1929­1958. [66] Sturmfels, P., Rutherford, S., Angstadt, M., Peterson, M., Sripada, C., and Wiens, J. (2018). A domain guided cnn architecture for predicting age from structural brain images. In Machine Learning for Healthcare Conference,

pages 295­311. [67] Sujit, S. J., Coronado, I., Kamali, A., Narayana, P. A., and Gabr, R. E.
(2019). Automated image quality evaluation of structural brain mri using an ensemble of deep learning networks. Journal of Magnetic Resonance Imaging, 50(4):1260­1267. [68] Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. (2016). Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2818­2826. [69] Tamminga, C. A., Pearlson, G., Keshavan, M., Sweeney, J., Clementz, B., and Thaker, G. (2014). Bipolar and schizophrenia network for intermediate phenotypes: outcomes across the psychosis continuum. Schizophrenia bulletin, 40(Suppl 2):S131­S137. [70] Tartaglione, E., Barbano, C. A., and Grangetto, M. (2021). End: Entangling and disentangling deep representations for bias correction. arXiv preprint arXiv:2103.02023. [71] Torralba, A. and Efros, A. A. (2011). Unbiased look at dataset bias. In CVPR 2011, pages 1521­1528. IEEE. [72] Tustison, N. J., Avants, B. B., Cook, P. A., Zheng, Y., Egan, A., Yushkevich, P. A., and Gee, J. C. (2010). N4itk: improved n3 bias correction. IEEE transactions on medical imaging, 29(6):1310­1320. [73] Ueda, M., Ito, K., Wu, K., Sato, K., Taki, Y., Fukuda, H., and Aoki, T. (2019). An age estimation method using 3d-cnn from brain mri images. In 2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019), pages 380­383. IEEE. [74] Van Essen, D. C., Smith, S. M., Barch, D. M., Behrens, T. E., Yacoub, E., Ugurbil, K., Consortium, W.-M. H., et al. (2013). The wu-minn human connectome project: an overview. Neuroimage, 80:62­79. [75] Van Leemput, K., Maes, F., Vandermeulen, D., and Suetens, P. (1999). Automated model-based tissue classification of mr images of the brain. IEEE transactions on medical imaging, 18(10):897­908. [76] Varatharajah, Y., Baradwaj, S., Kiraly, A., Ardila, D., Iyer, R., Shetty, S., and Kohlhoff, K. (2018). Predicting brain age using structural neuroimaging and deep learning. bioRxiv, page 497925. [77] Vieira, S., Gong, Q.-y., Pinaya, W. H., Scarpazza, C., Tognin, S., CrespoFacorro, B., Tordesillas-Gutierrez, D., Ortiz-Garc´ia, V., Setien-Suero, E., Scheepers, F. E., et al. (2020). Using machine learning and structural neuroimaging to detect first episode psychosis: reconsidering the evidence. Schizophrenia bulletin, 46(1):17­26. [78] Wachinger, C., Rieckmann, A., Po¨lsterl, S., Initiative, A. D. N., et al. (2021). Detect and correct bias in multi-site neuroimaging datasets. Medical Image Analysis, 67:101879. [79] Wang, H., Shen, Y., Wang, S., Xiao, T., Deng, L., Wang, X., and Zhao, X. (2019). Ensemble of 3d densely connected convolutional network for diagnosis of mild cognitive impairment and alzheimer's disease. Neurocomputing, 333:145­156. [80] Wen, J., Thibeau-Sutre, E., Diaz-Melo, M., Samper-Gonza´lez, J., Routier, A., Bottani, S., Dormont, D., Durrleman, S., Burgos, N., Colliot, O., et al. (2020). Convolutional neural networks for classification of alzheimer's disease: Overview and reproducible evaluation. Medical Image Analysis, page 101694. [81] Xiao, Y., Yan, Z., Zhao, Y., Tao, B., Sun, H., Li, F., Yao, L., Zhang, W., Chandan, S., Liu, J., et al. (2019). Support vector machine-based classification of first episode drug-na¨ive schizophrenia patients and healthy controls using structural mri. Schizophrenia Research, 214:11­17. [82] Xie, S., Girshick, R., Dolla´r, P., Tu, Z., and He, K. (2017). Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1492­ 1500. [83] Yang, X., Kwitt, R., Styner, M., and Niethammer, M. (2017). Quicksilver: Fast predictive image registration­a deep learning approach. NeuroImage, 158:378­396. [84] Zhang, K., Zuo, W., Chen, Y., Meng, D., and Zhang, L. (2017). Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising. IEEE transactions on image processing, 26(7):3142­3155. [85] Zhuo, J. and Gullapalli, R. P. (2006). Mr artifacts, safety, and quality control. Radiographics, 26(1):275­297.

Dufumier Benoit et al. / NeuroImage (2021)

19

Supplementary Material Training and Test Split

Task Age Sex Dx

Training Set BHB-10K BHB-10K
SCHIZCONNECT-VIP

Test Set BSNIP (only HC) BSNIP (only HC)
BSNIP

(we also computed a Singular Value Decomposition (SVD) before the computation of the CCA to remove the noisy neurons, as described in [57]). We chose to keep only 50% of the explained variance since N hwcd in our experiments (N = 394 and hwcd > 104) and we observed that a lot of neurons were noisy. Results are plotted in figure 7a.

Table 4: Training and Test Split used throughout this study for the 3 different target tasks.

Impact of Spatial Resolution with Quasi-Raw MR Images

As we wanted to make sure that down-sampling the quasiraw images from 1mm3 to 1.5mm3 isotropic had no negative impact on the final performance, we performed 3 experiments with ResNet18 and N = 500 training samples with the same experimental design as in section 4.1. Results show no drop in performance for sex and Dx predictions and even an improvement for age regression. We believe it could be partly due to the noise in the data at such a resolution.

Spatial Resolution
1.5mm3 1mm3

Age

MAE

RMSE

7.91 ± 0.42 9.82 ± 0.55

12.5 ± 1.6 11.9 ± 1.22

Sex
AUC 0.84 ± 0.01 0.84 ± 0.02

Dx
AUC 0.66 ± 0.01 0.66 ± 0.02

Table 5: ResNet18 performance as we change the spatial resolution of the input images from 1mm3 (size 182 × 218 × 182) to 1.5mm3 isotropic (size 121 × 145 × 121). It mainly affects the computational burden (higher when using the 1mm3
resolution) and it even improves the results for age prediction.

Tiny-DenseNet: we first observed that the blocks 1 and 2 (starting from 0) of DenseNet121 were highly correlated, which suggested a redundancy. In particular, it suggested that the features learnt inside the 3rd block were just copied from the second block and the specialization of the network to the prediction task did not occur in block 2. It was then natural to remove the block 2 from DenseNet121, assuming that the receptive field of a neuron before the FC layer would remain big enough for the 3 clinical tasks (its size is 32 × 32 × 32 for a an input size 128 × 128 × 128 with DenseNet121 and it is halved when we remove the 3rd block). Also, we halved the growth rate from k = 32 to k = 16 and we called the resulting network tiny-DenseNet, as it is 10× smaller than DenseNet. As before, we plotted the SVCCA between the internal layer outputs of tiny-DenseNet in figure 7b and we noticed that, differently from DenseNet121 in figure 7a, the strong correlation between blocks disappeared.

Introduction of tiny-DenseNet
Analysis of DenseNet121: as we wanted to give a tiny version of DenseNet (121 layers and 11M parameters), we analyzed its internal representation on Dx problem. In order to analyze the representation learnt inside this network, we computed the Singular Vector Canonical Correlation Analysis (SVCCA) [57] between the outputs of all pairs of layer inside every block. Formally, we define a set of neurons {zli}i[1..hwcd] for each layer l where (c, h, w, d) represent the number of channels, height, width and depth of the feature maps of layer l respectively; and zli = (zli(x1), ..., zli(xN))  RN is the response of neuron i to the entire test set (of size N). In this way, we can compute the CCA between 2 blocks of data {zli1 }i[1..h1w1c1d1] and {zli2 }i[1..h2w2c2d2] for 2 layers l1 and l2 since all vectors lie in the same space RN

Convergence speed
We observe in figure 8 that tiny-VGG constantly converges faster than any other networks for both quasi-raw and VBM data. Tiny-DenseNet also converges at the same rate for the 2 classification tasks (Dx and sex prediction). This is somewhat expected since they are the lightest networks among the ones tested (about 10× less parameters than DenseNet and 60× less than ResNet34). Surprisingly, on quasi-raw data, all the models converge i) faster than on VBM data (about 5× faster on age prediction) and ii) at the same rate (ResNet34 is the slowest globally on VBM data while it performs similarly with tiny-DenseNet on quasi-raw data).

20

Dufumier Benoit et al. / NeuroImage (2021)

(a) DenseNet121

(b) tiny-DenseNet

Figure 7: Internal representation of DenseNet and its tiny version. The SVCCA is computed between each pair of layers. Networks are trained on Dx.

Figure 8: Convergence speed of the four main CNN architectures as we vary the number of training samples with i) VBM data (top) and ii) quasi-raw (bottom). The stopping criterion is defined through the variance of a rolling window on the validation loss. The convergence is reached when this loss remains stable for the next k = 20 epochs (see section 3.9 for more details).

Dufumier Benoit et al. / NeuroImage (2021)

21

Preprocessing Architecture

VBM Quasi-Raw

ResNet18 VGG11 DenseNet121
ResNet18 VGG11 DenseNet121

Sex

BAcc 

AUC 

3D

2D

3D

2D

0.83 ± 0.01 0.78 ± 0.03 0.91 ± 0.01 0.88 ± 0.03 0.72 ± 0.015 0.75 ± 0.03 0.80 ± 0.02 0.85 ± 0.02 0.83 ± 0.01 0.68 ± 0.03 0.91 ± 0.01 0.81 ± 0.03

0.63 ± 0.02 0.54 ± 0.02 0.68 ± 0.05

0.60 ± 0, 05 0.60 ± 0.08 0.62 ± 0.08

0.84 ± 0.01 0.72 ± 0.04 0.68 ± 0.05 0.72 ± 0.05 0.81 ± 0.01 0.72 ± 0.05

Dx

BAcc 

AUC 

3D

2D

3D

2D

0.71 ± 0.01 0.56 ± 0.04 0.78 ± 0.01 0.64 ± 0.01 0.66 ± 0.03 0.63 ± 0.01 0.71 ± 0.009 0.68 ± 0.01 0.72 ± 0.02 0.58 ± 0.04 0.78 ± 0.01 0.67 ± 0.02

0.63 ± 0.02 0.54 ± 0.02 0.53 ± 0.04 0.61 ± 0.03 0.65 ± 0.02 0.56 ± 0.04

0.66 ± 0.01 0.61 ± 0.02 0.72 ± 0.01

0.59 ± 0.02 0.68 ± 0.02 0.62 ± 0.04

Table 6: Comparison between 2D and 3D models in the small data regime (N = 500 training samples). The models are tested on the independent test set BSNIP and the same 5-fold RLT CV has been used as in section 3.8. Even if 3D models have more parameters, they successfully capture the 3D brain anatomical structure by outperforming consistently their 2D counterpart. VGG11 is the only exception with lower performance in 3D, which can be due to its last 3 Fully-Connected layers that heavily increases the model size ( 50M in 3D). Best results for 2D and 3D CNN are reported in bold. BAcc=Balanced Accuracy, AUC=Area Under ROC Curve

2D slice-level approach
We also compared the performance of a 2D approach by decomposing each MRI scan into chunks of 3 consecutive axial slices. These slices are given to a 2D CNN with 3 input channels and we employed the same 5-fold RLT CV strategy as described in section 3.8. We only used N = 500 training samples and we reported the results on BSNIP, making them directly comparable with table 3. The predictions are performed at the subject-level by taking the median of the individual slice prediction and we reported the results in table 6.
Data Augmentation Strategies
All the detailed hyper-parameters used for the experiments with Data Augmentation along with their description can be found in table 7.
MC-Dropout
MC-Dropout has been introduced in [16, 17] as a simple way to capture epistemic uncertainty by putting a Bernouilli prior B(p) on the model's weight w (the resulting network is referred to as a Bayesian Neural Network, see for instance [37]). In practice, adding dropout inside a network is not new [65] but it was mainly applied as a regularization technique to limit overfitting. In the Bayesian context, dropout is applied both at training time and test time. Specifically, a single feed-forward pass at test time corresponds to a sampling w^  p(w) and to compute fw^ (x) for a given input image x. Averaging these outputs for several w^  p(w) gives an approximation of the posterior p(y|x) in the classification case. One main drawback of MC-Dropout is the tuning of the dropout

hyper-parameter p by grid-search. One way to avoid this computationally expensive grid-search is to learn this hyperparameter automatically during training, a technique referred to as Concrete Dropout [18]. We used this technique in this paper.

Calibration Metrics Calibration for classification
Let's assume that a DNN outputs a class prediction y as well as a confidence estimate p^ (usually the maximum probability after softmax) for a given x. We want to evaluate this estimation of confidence through a "calibration curve". Intuitively, if a network outputs a class y = 0 with a confidence level p^ = 0.6, then we would like that over 100 predictions of samples belonging to class 0, 60 are correctly classified. More formally, we introduce a notion of accuracy for a given confidence level p as p(y = y|p^ = p). A perfectly calibrated model should always verify:

p  [0, 1], y  [1..K], p(y = y|p^ = p) = p

in a classification problem with K classes. In practice, this ac-

curacy has to be estimated for various confidence levels p and

given a class k. To do so, we discretize uniformly the pre-

dicted confidence levels

p^

=

(p^i) into L bins Il

=

[

l-1 L

,

l L

)

and compute the accuracy of the predictions over each bin

P^ l

=

{i|

l-1 L



p^i

<

l L

}

by:

acc(P^ l )

=

1 |P^ l |

iP^ l

1yi=k

The estimation of the confidence level associated to the bin l,

22

Dufumier Benoit et al. / NeuroImage (2021)

Application Computer Vision
Neuroimaging

Transformation

Details

Hyperparameters

Flip

The images are flipped randomly along the 3 directions (axial, sagittal, coronal).



Gaussian Blur

A Gaussian filter is applied to input images with a full width at half maximum (FWHM) uniformly sampled in [, ]

FWHM  [0.35mm, 3.5mm]

Gaussian Noise

A Gaussian noise is added with a variance  uniformly sampled in [, ].

  [0.1, 1]

The images are cropped at a random location, reducing the input

Random Crop (+Resize) shape by p% in every direction, and resized linearly to match the Patch p = 70%

input size.

Affine

The images are randomly translated up to k voxels in every direction and rotated up to  degrees.

k = 10 voxels,  = 5

k-space Ghosting Artefact [85]

n lines in the k-space are randomly distorted to mimic the errors that may happen during the k-space line inversion step in an echoplanar imaging acquisition.

n = 10

The image is successively randomly linearly transformed (nsim×,

k-space Motion Artefact [61]

up to  rotation, t voxels translation) to reproduce the head motion artefact observed during an acquisition. The 3D Fourier transforms of these images are then combined to form a single

nsim = 3,  = 40, t = 10 voxels

k-space, which is transformed back to the original space.

n points with very high or low intensity are added randomly in the

k-space Spike Artefact k-space reproducing the bad data points obtained with gradients

[85]

applied at a very high duty cycle. It results in dark stripes in the

n = 10

original image.

The voxel intensities are modulated by a polynomial function (or-

der 3, coeff. magnitude m) whose coefficients are randomly sam-

Bias-Field Artefact [75] pled. It models the artefacts in the low-frequency range produced m  [-0.7, 0.7]

by the inhomogeneity of the static magnetic field inside the MRI

scanner.

n pairs of patches with shape 15 × 15 × 15 are randomly swapped.

Originally created as a self-supervision task to learn meaning-

Swap [11]

ful semantic features, the network is expected to use the context

n = 20

around each patch in order to find its original location and inter-

nally reconstruct the image.

Table 7: Description of the data augmentation strategies considered in our experiments. The input image always correspond to the pre-processed MR image. All the k-space artefacts have been implemented in the Python library TorchIO [53].

independent from class k, is then:

con f (P^l)

=

1 |P^ l |

iP^ l

p^i

In a perfectly calibrated model, we expect l 

[1..L], acc(P^l) = con f (P^l). One visual way to check the model

calibration is to plot the accuracy function of confidence, the

ideal case being acc = con f . A usual statistic derived from this

calibration curve is called Expected Calibration Error (ECE)

and it is defined as [24]:

ECE =

L l=1

|P^ l | n

acc(P^l) - con f (P^l)

where n is the total number of samples. We systematically used

this metric to measure calibration on sex prediction and Dx.

Calibration for regression

We can extend the ECE metric to the regression case, as de-

tailed in [25]. Briefly, assuming that the model outputs a mean

µ and variance 2 of a Gaussian distribution for a given x, we

can build a confidence interval CI(p) = [µ - -1

p+1 2

, µ +

-1

p+1 2

] associated to a confidence level p (where  is the

Cumulative Distribution Function, CDF, of N(0, 1)). We can

compute the proportion p^ of true target points y  R that lie in

CI(p), for all p  [0, 1]. From this, similarly to ECE, we can

deduce the Area Under the Calibration Error (AUCE) of |p^ - p|.

Dufumier Benoit et al. / NeuroImage (2021)

23

Figure 9: Performance of big and small networks (resp. DenseNet and tiny-DenseNet) as we increase the number of feed-forward passes T when N = 500 training samples. We integrated Concrete Dropout [18] inside the CNN architecture to model the epistemic uncertainty. The dashed lines represent the baselines of the deterministic models (without Concrete Dropout).
Convergence curves at N = 500
We reported the convergence curves of all tested models on age regression and Dx figures 10 and 11 with Ntrain = 500. These plots motivate the stopping criterion used throughout our experiments and described section 3.9. In particular, we see a clear plateau each time for every model without any evolution in performance given a long enough training.

24

Dufumier Benoit et al. / NeuroImage (2021)

Figure 10: Training and validation losses for age prediction using only N = 500 training samples and VBM data. The losses correspond to the 1 error between the true age and the predicted age at each epoch for all CNN.

Dufumier Benoit et al. / NeuroImage (2021)

25

Figure 11: ROC-AUC and Balanced Accuracy for Dx with N = 500 training samples and VBM data.

