arXiv:2106.01595v1 [cs.DS] 3 Jun 2021

Position Heaps for Cartesian-tree Matching on Strings and Tries
Akio Nishimoto1, Noriki Fujisato1, Yuto Nakashima1, and Shunsuke Inenaga1,2
1Department of Informatics, Kyushu University, Japan {nishimoto.akio, noriki.fujisato, yuto.nakashima,
inenaga}@inf.kyushu-u.ac.jp 2PRESTO, Japan Science and Technology Agency, Japan
Abstract The Cartesian-tree pattern matching is a recently introduced scheme of pattern matching that detects fragments in a sequential data stream which have a similar structure as a query pattern. Formally, Cartesian-tree pattern matching seeks all substrings S of the text string S such that the Cartesian tree of S and that of a query pattern P coincide. In this paper, we present a new indexing structure for this problem, called the Cartesian-tree Position Heap (CPH ). Let n be the length of the input text string S, m the length of a query pattern P , and  the alphabet size. We show that the CPH of S, denoted CPH(S), supports pattern matching queries in O(m( + log(min{h, m})) + occ) time with O(n) space, where h is the height of the CPH and occ is the number of pattern occurrences. We show how to build CPH(S) in O(n log ) time with O(n) working space. Further, we extend the problem to the case where the text is a labeled tree (i.e. a trie). Given a trie T with N nodes, we show that the CPH of T , denoted CPH(T ), supports pattern matching queries on the trie in O(m(2 +log(min{h, m}))+occ) time with O(N ) space. We also show a construction algorithm for CPH(T ) running in O(N ) time and O(N ) working space.
1 Introduction
1.1 Cartesian-tree Pattern Matching
If the Cartesian trees CT(X) and CT(Y ) of two strings X and Y are equal, then we say that X and Y Cartesian-tree match (ct-match). The Cartesian-tree pattern matching problem (ct-matching problem) [18] is, given a text string S and a pattern P , to find all substrings S of S that ct-match with P .
String equivalence with ct-matching belongs to the class of substring-consistent equivalence relation (SCER) [17], namely, the following holds: If two strings X and Y ct-match, then X[i..j] and Y [i..j] also ct-match for any 1  i  j  |X|. Among other types of SCERs ([3, 4, 5, 14, 15]), ct-matching is the most related to order-peserving matching (op-matching) [16, 7, 9]. Two strings X and Y are said to op-match if the relative order of the characters in X and the relative order of the characters in Y are the same. It is known that with ct-matching one can detect some interesting occurrences of a pattern that
1

cannot be captured with op-matching. More precisely, if two strings X and Y op-match, then X and Y also ct-match. However, the reverse is not true. With this property in hand, ct-matching is motivated for analysis of time series such as stock charts [18, 11].
This paper deals with the indexing version of the ct-matching problem. Park et al. [18] proposed the Cartesian suffix tree (CST ) for a text string S that can be built in O(n log n) worst-case time or O(n) expected time, where n is the length of the text string S. The log n factor in the worst-case complexity is due to the fact that the parent-encoding, a key concept for ct-matching introduced in [18], is a sequence of integers in range [0..n-1]. While it is not explicitly stated in Park et al.'s paper [18], our simple analysis (c.f. Lemma 10 in Section 5) reveals that the CST supports pattern matching queries in O(m log m + occ) time, where m is the pattern length and occ is the number of pattern occurrences.
1.2 Our Contribution
In this paper, we present a new indexing structure for this problem, called the Cartesiantree Position Heap (CPH ). We show that the CPH of S, which occupies O(n) space, can be built in O(n log ) time with O(n) working space and supports pattern matching queries in O(m( + log(min{h, m})) + occ) time, where h is the height of the CPH. Compared to the afore-mentioned CST, our CPH is the first index for ct-matching that can be built in worst-case linear time for constant-size alphabets, while pattern matching queries with our CPH can be slower than with the CST when  is large.
We then consider the case where the text is a labeled tree (i.e. a trie). Given a trie T with N nodes, we show that the CPH of T , which occupies O(N ) space, can be built in O(N ) time and O(N ) working space. We also show how to support pattern matching queries in O(m(2 + log(min{h, m})) + occ) time in the trie case. To our knowledge, our CPH is the first indexing structure for ct-matching on tries that uses linear space for constant-size alphabets.
Conceptually, our CPH is most related to the parameterized position heap (PPH ) for a string [12] and for a trie [13], in that our CPHs and the PPHs are both constructed in an incremental manner where the suffixes of an input string and the suffixes of an input trie are processed in increasing order of their lengths. However, some new techniques are required in the construction of our CPH due to different nature of the parent encoding [18] of strings for ct-matching, from the previous encoding [3] of strings for parameterized matching.
2 Preliminaries
2.1 Strings and (Reversed) Tries
Let  be an ordered alphabet of size . An element of  is called a character. An element of  is called a string. For a string S  , let S denote the number of distinct characters in S.
The empty string  is a string of length 0, namely, || = 0. For a string S = XY Z, X, Y and Z are called a prefix, substring, and suffix of S, respectively. The set of prefixes of a string S is denoted by Prefix(S). The i-th character of a string S is denoted by S[i] for 1  i  |S|, and the substring of a string S that begins at position i and ends at position
2

j is denoted by S[i..j] for 1  i  j  |S|. For convenience, let S[i..j] =  if j < i. Also, let S[i..] = S[i..|S|] for any 1  i  |S| + 1.
A trie is a rooted tree that represents a set of strings, where each edge is labeled with a character from  and the labels of the out-going edges of each node is mutually distinct. Tries are natural generalizations to strings in that tries can have branches while strings are sequences without branches.
Let x be any node of a given trie T , and let r denote the root of T . Let depth(x) denote the depth of x. When x = r, let parent(x) denote the parent of x. For any 0  j  depth(x), let anc(x, j) denote the j-th ancestor of x, namely, anc(x, 0) = x and anc(x, j) = parent(anc(x, j - 1)) for 1  j  depth(x). It is known that after a linear-time processing on T , anc(x, j) for any query node x and integer j can be answered in O(1) time [6].
For the sake of convenience, we consider the reversed trie where the path labels are read in the leaf-to-root direction. In the sequel, when we talk about tries, we mean reversed tries, unless otherwise stated.
For each (reversed) path (x, y) in T such that y = anc(x, j) with j = |depth(x)| - |depth(y)|, let str(x, y) denote the string obtained by concatenating the labels of the edges from x to y. For any node x of T , let str(x) = str(x, r).
Let N be the number of nodes in T . We associate a unique id to each node of T . Here we use a bottom-up level-order traversal rank as the id of each node in T , and we sometimes identify each node with its id. For each node id i (1  i  N ) let T [i..] = str(i), i.e., T [i..] is the path string from node i to the root r.

2.2 Cartesian-tree Pattern Matching
The Cartesian tree of a string S, denoted CT(S), is the rooted tree with |S| nodes which is recursively defined as follows:
· If |S| = 0, then CT(S) is the empty tree.
· If |S|  1, then CT(S) is the tree whose root r stores the left-most minimum value S[i] in S, namely, r = S[i] iff S[i]  S[j] for any i = j and S[h] > S[i] for any h < i. The left-child of r is CT(S[1..i - 1]) and the right-child of r is CT(S[i + 1..|S|]).
The parent distance encoding of a string S of length n, denoted PD(S), is a sequence of n integers over [0..n - 1] such that

PD(S)[i] = i - max1j<i{j | S[j]  S[i]} if such j exists,

0

otherwise.

Namely, PD(S)[i] represents the distance to from position i to its nearest left-neighbor position j that stores a value that is less than or equal to S[i].
It is known that PD(S)[i] is equal to the distance between the node S[i] of CT(S[1..i]) and its parent (note that in this case, S[i] is always the right child of its parent), thus its name "parent distance".
A tight connection between CT and PD is known:
Lemma 1 ([19]). For any two strings S1 and S2 of equal length, CT(S1) = CT(S2) iff PD(S1) = PD(S2).

3

CT (S1)

CT (S2 )

S1

316486759

PD(S1) 0 0 1 2 1 2 1 4 1

S2

713286945

PD(S2) 0 0 1 2 1 2 1 4 1

Figure 1: Two strings S1 = 316486759 and S2 = 713286945 ct-match since CT(S1) = CT(S2) and PD(S1) = PD(S2).

For two strings S1 and S2, we write S1  S2 iff CT(S1) = CT(S2) (or equivalently PD(S1) = PD(S2)). We also say that S1 and S2 ct-match when S1  S2. See Fig. 1 for a concrete example.
We consider the indexing problems for Cartesian-tree pattern matching on a text string and a text trie, which are respectively defined as follows:
Problem 1 (Cartesian-Tree Pattern Matching on Text String). Preprocess: A text string S of length n.
Query: A pattern string P of length m.
Report: All text positions i such that S[i..i + m - 1]  P .
Problem 2 (Cartesian-Tree Pattern Matching on Text Trie). Preprocess: A text trie T with N nodes.
Query: A pattern string P of length m.
Report: All trie nodes i such that (T [i..])[1..m]  P .

2.3 Sequence Hash Trees
Let W = w1, . . . , wk be a sequence of non-empty strings such that for any 1 < i  k, wi / Prefix(wj) for any 1  j < i. The sequence hash tree [8] of a sequence W = w1, . . . , wk of k strings, denoted SHT(W) = SHT(W)k, is a trie structure that is incrementally built as follows:
1. SHT(W)0 = SHT( ) for the empty sequence is the tree only with the root.
2. For i = 1, . . . , k, SHT(W)i is obtained by inserting the shortest prefix ui of wi that does not exist in SHT(W)i-1. This is done by finding the longest prefix pi of wi that exists in SHT(W)i-1, and adding the new edge (pi, c, ui), where c = wi[|pi| + 1] is the first character of wi that could not be traversed in SHT(W)i-1.
Since we have assumed that each wi in W is not a prefix of wj for any 1  j < i, the new edge (pi, c, ui) is always created for each 1  i  k. This means that SHT(W) contains exactly k + 1 nodes (including the root).

4

To perform pattern matching queries efficiently, each node of SHT(W) is augmented with the maximal reach pointer. For each 1  i  k, let ui be the newest node in SHT(W)i, namely, ui is the shortest prefix of wi which did not exist in SHT(W)i-1. Then, in the complete sequence hash tree SHT(W) = SHT(W)k, we set mrp(ui) = uj iff uj is the deepest node in SHT(W) such that uj is a prefix of wi. Intuitively, mrp(ui) represents the last visited node uj when we traverse wi from the root of the complete SHT(W). Note that j  i always holds. When j = i (i.e. when the maximal reach pointer is a self-loop),
then we can omit it because it is not used in the pattern matching algorithm.

3 Cartesian-tree Position Heaps for Strings

In this section, we introduce

our new indexing structure for Problem 1. For a given text string S of length n, let WS denote the sequence of the parent distance encodings of the non-empty suffixes of S which are sorted in increasing order of their lengths. Namely, WS = w1, . . . , wn = PD(S[n..]), . . . , PD(S[1..]) , where wn-i+1 = PD(S[i..]). The Cartesiantree Position Heap (CPH ) of string S, denoted CPH(S), is the sequence hash tree

0 0

0

11

0

1

0

2

01

2

0

4 2

3

1

8 2

0

5

9

0 23 10 7

1

6

1

3

11 14

13

0 12

S 26427584365741

w1

0

w2

00

w3

000

w4

0100

w5

00100

w6

012140

w7

0012140

w8

00012140

w9

010012140

w10

0010012140

w11

01214512140

w12 001214512140

w13 0001214512140

w14 01231214512140

of WS, that is, CPH(S) = SHT(WS). Note that for each 1  i  n + 1, CPH(S[i..]) = SHT(WS)n-i+1 holds.
In our algorithm we build
CPH(S[i..]) for decreasing i =

Figure 2: CPH(S) for string S = 26427584365741. For each wi = PD(S[n - i + 1..]), the underlined prefix is the string that is represented by the node ui in CPH(S). The dotted arcs are reversed suffix links (not all reversed suffix links are shown).

n, . . . , 1, which means that we process the given text string S in a right-to-left online

manner, by prepending the new character S[i] to the current suffix S[i + 1..].

For a sequence v of integers, let Zv denote the sorted list of positions z in v such that

v[z] = 0 iff z  Zv. Clearly |Zv| is equal to the number of 0's in v.

Lemma 2. For any string S, |ZPD(S)|  S.

Proof. Let ZPD(S) = z1, . . . , z . We have that u[z1] > · · · > u[z ] since otherwise PD(S)[zx] = 0 for some zx, a contradiction. Thus |ZPD(S)|  S holds.

Lemma 3. For each i = n, . . . , 1, PD(S[i..]) can be computed from PD(S[i + 1..]) in an online manner, using a total of O(n) time with O(S) working space.

5

Proof. Given a new character S[i], we check each position z in the list ZPD(S[i+1..]) in increasing order. Let z^ = z + i, i.e., z^ is the global position in S corresponding to z in S[i + 1..]. If S[i]  S[z^], then we set PD(S[i..])[z - i + 1] = z - i (> 0) and remove z from the list. Remark that these removed positions correspond to the front pointers in the next suffix S[i..]. We stop when we encounter the first z in the list such that S[i] > S[z^]. Finally we add the position i to the head of the remaining positions in the list. This gives us ZPD(S[i..]) for the next suffix S[i..].
It is clear that once a position in the PD encoding is assigned a non-zero value, then the value never changes whatever characters we prepend to the string. Therefore, we can compute PD(S[i..]) from PD(S[i + 1..]) in a total of O(n) time for every 1  i  n. The working space is O(S) due to Lemma 2.

A position i in a sequence u of non-negative integers is said to be a front pointer in u

if i - PD(u)[i] = 1. Let Fu denote the sorted list of front pointers in u. The positions of

the suffix S[i + 1..] which are removed from ZPD(S[i+1..]) correspond to the front pointers in FPD(S[i..]) for the next suffix S[i..].
Our construction algorithm updates CPH(S[i+

1..]) to CPH(S[i..]) by inserting a new node for the next suffix S[i..], processing the given string S in a right-to-left online manner. Here the task

root 00

is to efficiently locate the parent of the new node

in the current CPH at each iteration.

As in the previous work on right-to-left online construction of indexing structures for other types of pattern matching [20, 10, 12, 13], we use

v(i) a

the reversed suffix links in our construction algorithm for CPH(S). For ease of explanation, we first introduce the notion of the suffix links. Let u be any non-root node of CPH(S). We

p(i) u(i)

b

di

v^(i)

identify u with the path label from the root of CPH(S) to u, so that u is a PD encoding of

u(i+1)

some substring of S. We define the suffix link of u, denoted sl(u), such that sl(u) = v iff v is obtained by (1) removing the first 0 (= u[1]), and (2) substituting 0 for the character u[f ] at every front pointer f  Fu  [2..|u|] of u. The reversed suffix link of v with non-negative integer label a, denoted rsl(v, a), is defined such

Figure 3: We climb up the path from u(i + 1) and find the parent p(i) of the new node u(i) (in black). The label a of the reversed suffix link we traverse from v(i) is equal to the number of front pointers in p(i).

that rsl(v, a) = u iff sl(u) = v and a = |Fu|. See also Fig. 2.

Lemma 4. Let u, v be any nodes of CPH(S) such that rsl(v, a) = u with label a. Then a  S - 1.

Proof. We have |Fu| + 1  |Zv|, where the difference 1 is due to the above operation (1) of removing the first 0 = u[1] from u. Using Lemma 2, we have a = |Fu|  |Zv| - 1  v - 1  S - 1.

6

The next lemma shows that the number of out-going reversed suffix links of each node v is bounded by the alphabet size.
Lemma 5. For any node v in CPH(S), |{a | rsl(v, a) = u for some node u}|  S.
Proof. Let a be any integer such that rsl(v, a) = u exists for some node u. Since v is a node of CPH(S), v = PD(S ) for some substring S of S. Thus, by Lemma 4, we get a  S - 1. Since a  0, there can be at most S different values for a such that rsl(v, a) is defined for any node v.
Our CPH construction algorithm makes use of the following monotonicity of the labels of reversed suffix links:
Lemma 6. Suppose that there exist two reversed suffix links rsl(v, a) = u and rsl(v , a ) = u such that v = parent(v) and u = parent(u). Then, 0  a - a  1.
Proof. Immediately follows from a = |Fu|, a = |Fu |, and u = u[1..|u| - 1].
We are ready to design our right-to-left online construction algorithm for the CPH of a given string S. Since PD(S[i..]) is the (n-i+1)-th string wn-i+1 of the input sequence WS, for ease of explanation, we will use the convention that u(i) = un-i+1 and p(i) = pn-i-1, where the new node u(i) for wn-i+1 = PD(S[i..]) is inserted as a child of p(i). See Fig. 3.
Algorithm 1: Right-to-Left Online Construction of CPH(S)
i = n (base case): We begin with CPH(S[n..]) which consists of the root r = u(n+1) and the node u(n) for the first (i.e. shortest) suffix S[n..] of S. Since w1 = PD(S[n..]) = PD(S[n]) = 0, the edge from r to u(n) is labeled 0. Also, we set the reversed suffix link rsl(r, 0) = u(n).
i = n - 1, . . . , 1 (iteration): Given CPH(S[i + 1..]) which consists of the nodes u(i + 1), . . . , u(n), which respectively represent some prefixes of the already processed strings wn-i, . . . , w1 = PD(S[i + 1..]), . . . , PD(S[n..]), together with their reversed suffix links. We find the parent p(i) of the new node u(i) for PD(S[i..]), as follows: We start from the last-created node u(i + 1) for the previous PD(S[i + 1..]), and climb up the path towards the root r. Let di  [1..|u(i + 1)|]) be the smallest integer such that the di-th ancestor v(i) = anc(u(i + 1), di) of u(i + 1) has the reversed suffix link rsl(v(i), a) with the label a = |FPD(S[i..i+|v(i)|])|. We traverse the reversed suffix link from v(i) and let p(i) = rsl(v(i), a). We then insert the new node u(i) as the new child of p(i), with the edge labeled PD(S[i..])[i + |u(i)| - 1]. Finally, we create a new reversed suffix link rsl(v^(i), b) = u(i), where v^(i) = anc(u(i + 1), di - 1) and parent(v^) = v. We set b  a + 1 if the position i + di + 1 is a front pointer of PD(S[i..]), and b  a otherwise.
For computing the label a = |FPD(S[i..i+|v(i)|])| efficiently, we introduce a new encoding FP that is defined as follows: For any string S of length n, let FP(S)[i] = FS[i..n]. The FP encoding preserves the ct-matching equivalence:
Lemma 7. For any two strings S1 and S2, S1  S2 iff FP(S1) = FP(S2).
7

Proof. For a string S, consider the DAG G(S) = (V, E) such that V = {0, . . . , |S|}, E = {(j, i) | j = i - PD(S)[i]} (see also Fig. 7 in Appendix). By Lemma 1, for any strings S1 and S2, G(S1) = G(S2) iff S1  S2. Now, we will show there is a one-to-one correspondence between the DAG G and the FP encoding.
() We are given G(S) for some (unknown) string S. Since FP(S)[i] is the in-degree of the node i of G(S), FP(S) is unique for the given DAG G(S).
() Given FP(S) for some (unknown) string S, we shown an algorithm that builds DAG G(S). We first create nodes V = {0, . . . , |S|} without edges, and mark all nodes in V . For each i = n, . . . , 0 in decreasing order, if FP(S)[i] > 0, then select the leftmost FP(S)[i] unmarked nodes in the range [i-1..n], and create an edge (i, i ) from each selected node i to i. We mark all these FP(S)[i] nodes at the end of this step, and proceed to the next node i - 1. The resulting DAG G(S) is clearly unique for a given PD(S).

For computing the label a = |FPD(S[i..i+|v(i)|])| = FP(S[i..i + |v(i)|])[1] of the reversed suffix link in Algorithm 1, it is sufficient to maintain the induced graph G[i..j] of DAG G for a variable-length sliding window S[i..j] with the nodes {i, . . . , j}. This can easily be
maintained in O(n) total time.

Theorem 1. Algorithm 1 builds CPH(S[i..]) for decreasing i = n, . . . , 1 in a total of O(n log ) time and O(n) space, where  is the alphabet size.

Proof. Correctness: Consider the (n - i + 1)-the step in which we process PD(S[i..]). By

Lemma 6, the di-th ancestor v(i) = anc(u(i + 1), di) of u(i + 1) can be found by simply

walking up the path from the start node u(i + 1). Note that there always exists such

ancestor v(i) of u(i + 1) since the root r has the defined reversed suffix link rsl(r, 0) = 0.

By the definition of v(i) and its reversed suffix link, rsl(v(i), a) = p(i) is the longest prefix

of PD(S[i..]) that is represented by CPH(S[i + 1..]) (see also Fig. 3). Thus, p(i) is the

parent of the new node u(i) for PD(S[i..]). The correctness of the new reversed suffix link

rsl(v^(i), b) = u(i) follows from the definition.

Complexity: The time complexity is proportional to the total number

n i=1

di

of

nodes

that we visit for all i = n, . . . , 1. Clearly |u(i)| - |u(i + 1)| = di - 2. Thus,

n i=1

di

=

ni=1(|u(i)| - |u(i + 1)| + 2) = |u(1)| - |u(n)| + 2n  3n = O(n). Using Lemma 5 and

sliding-window FP, we can find the reversed suffix links in O(log S) time at each of the

n i=1

di

visited nodes.

Thus the total time complexity is O(n log S).

Since the number

of nodes in CPH(S) is n + 1 and the number of reversed suffix links is n, the total space

complexity is O(n).

Lemma 8. There exists a string S of length n over a binary alphabet  = {1, 2} such a node in CPH(S) has ( n) out-going edges.

Proof. Consider string S = 11211221 · · · 12k1. Then, for representing 01k-2 (see also Fig. 6 in Appendix). Since k

any 1 = ( n),

 k, there the parent

exist node

nodes 01k-2

has ( n) out-going edges.

Due to Lemma 8, if we maintain a sorted list of out-going edges for each node during our online construction of CPH(S[i..]), it would require O(n log n) time even for a constantsize alphabet. Still, after CPH(S) has been constructed, we can sort all the edges offline, as follows:

8

Theorem 2. For any string S over an integer alphabet  = [1..] of size  = nO(1), the edge-sorted CPH(S) together with the maximal reach pointers can be computed in O(n log S) time and O(n) space.
Proof. We sort the edges of CPH(S) as follows: Let i be the id of each node u(i). Then sort the pairs (i, x) of the ids and the edge labels. Since i  [0..n - 1] and x  [1..nO(1)], we can sort these pairs in O(n) time by a radix sort. The maximal reach pointers can be computed in O(n log S) time using the reversed suffix links, in a similar way to the position heaps for exact matching [10].
See Fig. 5 in Appendix for an example of CPH(S) with maximal reach pointers.

4 Cartesian-tree Position Heaps for Tries

Let T be the input text trie with N nodes. A na¨ive extension of our CPH to a trie would be

to build the CPH for the sequence PD(T [N..]), . . . , PD(T [1..]) of the parent encodings of

all the path strings of T towards the root r. However, this does not seem to work because

the parent encodings are not consistent for suffixes. For instance, consider two strings 1432

and 4432. Their longest common suffix 432 is represented by a single path in a trie T .

However, the longest common suffix of PD(1432) = 0123 and PD(4432) = 0100 is . Thus,

in the worst case, we would have to consider all the path strings T [N..], . . . , T [1..] in T

separately, but the total length of these path strings in T is (N 2).

To overcome this difficulty, we reuse the FP encoding from Section 3. Since FP(S)[i] is

determined merely by the suffix S[i..], the FP encoding is suffix-consistent. For an input

trie T , let the FP-trie T FP be the reversed trie storing FP(T [i..]) for all the original path strings T [i..] towards the root. Let N be the number of nodes in T FP. Since FP is suffix-consistent, N  N always holds. Namely, FP is a linear-size representation of the

equivalence relation of the nodes of T w.r.t. . Each node v of T FP stores the equivalence class Cv = {i | T FP[v..] = FP(T [i..])} of the nodes i in T that correspond to v. We set

min{Cv} to be the representative of Cv, as well as the id of node v. See Fig. 8 in Appendix.

Let T be the set of distinct characters (i.e. edge labels) in T and let T = |T |. The FP-trie T FP can be computed in O(N T ) time and working space by a standard traversal on T , where we store at
most T front pointers in each node of the current path in T due to Lemma 3.

x na(x, a)

 da
ya a

z' b z

Let iN , . . . , i1 be the node id's of T FP Figure 4: Illustration for the data structure sorted in decreasing order. The Cartesian- of Lemma 9, where (T [i..])[1.. ] = str(x, z). tree position heap for the input trie T is

CPH(T ) = SHT(WT ), where WT = PD(T [iN ..], . . . , PD(T [i1..]) . As in the case of string inputs in Section 3, we insert the shortest prefix of PD(T [ik..])
that does not exist in CPH(T [ik+1..]). To perform this insert operation, we use the following data structure for a random-access query on the PD encoding of any path string in T :

Lemma 9. There is a data structure of size O(N T ) that can answer the following queries in O(T ) time each.

9

Query input: The id i of a node in T and integer > 0. Query output: The th (last) symbol PD((T [i..])[1.. ])[ ] in PD(T [i..])[1.. ].
Proof. Let x be the node with id i, and z = anc(x, ). Namely, str(x, z) = (T [j..])[1.. ]. For each character a  T , let na(x, a) denote the nearest ancestor ya of x such that the edge (parent(ya), ya) is labeled a. If such an ancestor does not exist, then we set na(x, a) to the root r.
Let z = anc(x, - 1), and b be the label of the edge (z, z ). Let D be an empty set. For each character a  T , we query na(x, a) = ya. If da = |ya| - |z | > 0 and a  b, then da is a candidate for (PD(T [j..])[1.. ])[ ] and add da to set D. After testing all a  T , we have that (PD(T [j..])[1.. ])[ ] = min D. See Fig. 4 for illustration.
For all characters a  T and all nodes x in T , na(x, a) can be pre-computed in a total of O(N T ) preprocessing time and space, by standard traversals on T . Clearly each query is answered in O(T ) time.
Theorem 3. Let T be a given trie with N nodes whose edge labels are from an integer alphabet of size nO(1). The edge-sorted CPH(T ) with the maximal reach pointers, which occupies O(N T ) space, can be built in O(N T ) time.
Proof. The rest of the construction algorithm of CPH(T ) is almost the same as the case of the CPH for a string, except that the amortization argument in the proof for Theorem 1 cannot be applied to the case where the input is a trie. Instead, we use the nearest marked ancestor (NMA) data structure [21, 2] that supports queries and marking nodes in amortized O(1) time each, using space linear in the input tree. For each a  [0..T - 1], we create a copy T a of the trie T and maintain the NMA data structure on T a so that every node v that has defined reversed suffix link rsl(v, a) is marked, and any other nodes are unmarked. The NMA query for a given node v with character a is denoted by nmaa(v). We also maintain the dynamic level ancestor data structure on our CPH that allows for leaf insertions and level ancestor queries in O(1) time each [1].
We are ready to design our construction algorithm: Suppose we have already built CPH(T [ih+1..]) and we are to update it with PD(T [ih..]). We begin with the node u(ih+1) that corresponds to PD(T [ih+1..]). We initially set v  u(ih+1) and a  T - 1 (the latter is due to Lemma 4). We perform the following:
(1) Update v  nmaa(v). If |v|  a, return v and break. Otherwise, go to (2).
(2) Update v  anc(v, |v| - a). Update a  a - 1, and go to (1).
The node v returned by the above algorithm is exactly the node v(ih) from which we take the reversed suffix link. The level ancestor query of (2) skips redundant nodes in the climbing path from u(ih+1). Since a  T by Lemma 4, the number of queries in (1) and (2) is O(T ) for each node of the N nodes in the trie. This gives us a construction for CPH(T ) in O(N T ) total time and working space.
We will reuse the random access data structure of Lemma 9 for pattern matching (see Section 5.2). Thus CPH(T ) requires O(N T ) space.
10

5 Cartesian-tree Pattern Matching with Position Heaps

5.1 Pattern Matching on Text String S with CPH(S)

Given a pattern P of length m, we first compute the greedy factorization f(P ) = P0, P1, . . . , Pk

of P such that P0 = , and for 1  l  k, Pl = P [lsum(l - 1) + 1..lsum(l)] is the longest

prefix of Pl · · · Pk that is represented by CPH(S), where lsum(l) =

l j=0

|Pj

|.

We

consider

such a factorization of P since the height h of CPH(S) can be smaller than the pattern

length m.

Lemma 10. Any node v in CPH(S) has at most |v| out-going edges.
Proof. Let (v, c, u) be any out-going edge of v. When |u| - 1 is a front pointer of u, then c = u[|u|] and this is when c takes the maximum value. Since u[|u|]  |u| - 1, we have c  |u| - 1. Since the edge label of CPH(S) is non-negative, v can have at most |u| - 1 = |v| out-going edges.

The next corollary immediately follows from Lemma 10.
Corollary 1. Given a pattern P of length m, its factorization f(P ) can be computed in O(m log(min{m, h})) time, where h is the height of CPH(S).

The next lemma is analogous to the position heap for exact matching [10].
Lemma 11. Consider two nodes u and v in CPH(S) such that u = PD(P ) the id of v is i. Then, PD(S[i..])[1..|u|] = u iff one of the following conditions holds: (a) v is a descendant of u; (b) mrp(v) is a descendant of u.

See also Fig. 10 in Appendix. We perform a standard traversal on CPH(S) so that one we check whether a node is a descendant of another node in O(1) time.
When k = 1 (i.e. f(P ) = P ), PD(P ) is represented by some node u of CPH(S). Now a direct application of Lemma 11 gives us all the occ pattern occurrences in O(m log m + occ) time, where min{m, h} = m in this case. All we need here is to report the id of every descendant of u (Condition (a)) and the id of each node v that satisfies Condition (b). The number of such nodes v is less than m.
When k  2 (i.e. f(P ) = P ), there is no node that represents PD(P ) for the whole pattern P . This happens only when occ < m, since otherwise there has to be a node representing PD(P ) by the incremental construction of CPH(S), a contradiction. This implies that Condition (a) of Lemma 11 does apply when k  2. Thus, the candidates for the pattern occurrences only come from Condition (b), which are restricted to the nodes v such that mrp(v) = u1, where u1 = PD(P1). We apply Condition (b) iteratively for the following P2, . . . , Pk, while keeping track of the position i that was associated to each node v such that mrp(v) = u1. This can be done by padding i with the off-set lsum(l - 1) when we process Pl. We keep such a position i if Condition (b) is satisfied for all the following pattern blocks P2, . . . , Pk, namely, if the maximal reach pointer of the node with id i + lsum(l - 1) points to node ul = PD(Pl) for increasing l = 2, . . . , k. As soon as Condition (b) is not satisfied with some l, we discard position i.
Suppose that we have processed the all pattern blocks P1, . . . , Pk in f(P ). Now we have that PD(S[i..])[1..m] = PD(P ) (or equivalently S[i..i + m - 1]  P ) only if the position

11

i has survived. Namely, position i is only a candidate of a pattern occurrence at this

point, since the above algorithm only guarantees that PD(P1) · · · PD(Pk) = PD(S[i..])[1..m]. Note also that, by Condition (b), the number of such survived positions i is bounded by

min{|P1|, . . . , |Pk|}  m/k. For each survived position i, we verify whether PD(P ) = PD(S[i..])[1..m]. This can be

done by checking, for each increasing l = 1, . . . , k, whether or not PD(S[i..])[lsum(l - 1) +

y] = PD(P1 · · · Pl)[lsum(l-1)+y] for every position y (1  y  |Pl|) such that PD(Pl)[y] = 0.

By the definition of PD, the number of such positions y is at most Pl  P . Thus, for

each survived position i we have at most kP positions to verify. Since we have at most

m/k

survived

positions,

the

verification

takes

a

total

of

O(

m k

·

kP )

=

O(mP )

time.

Theorem 4. Let S be the text string of length n. Using CPH(S) of size O(n) augmented with the maximal reach pointers, we can find all occ occurrences for a given pattern P in S in O(m(P + log(min{m, h})) + occ) time, where m = |P | and h is the height of CPH(S).

5.2 Pattern Matching on Text Trie T with CPH(T )
In the text trie case, we can basically use the same matching algorithm as in the text string case of Section 5.1. However, recall that we cannot afford to store the PD encodings of the path strings in T as it requires (n2) space. Instead, we reuse the random-access data structure of Lemma 9 for the verification step. Since it takes O(T ) time for each random-access query, and since the data structure occupies O(N T ) space, we have the following complexity:
Theorem 5. Let T be the text trie with N nodes. Using CPH(T ) of size O(N T ) augmented with the maximal reach pointers, we can find all occ occurrences for a given pattern P in T in O(m(P T + log(min{m, h})) + occ) time, where m = |P | and h is the height of CPH(T ).

Acknowledgments
This work was supported by JSPS KAKENHI Grant Numbers JP18K18002 (YN) and JP21K17705 (YN), and by JST PRESTO Grant Number JPMJPR1922 (SI).

12

References
[1] S. Alstrup and J. Holm. Improved algorithms for finding level ancestors in dynamic trees. In U. Montanari, J. D. P. Rolim, and E. Welzl, editors, ICALP 2000, volume 1853 of Lecture Notes in Computer Science, pages 73­84. Springer, 2000.
[2] A. Amir, M. Farach, R. M. Idury, J. A. L. Poutr´e, and A. A. Sch¨affer. Improved dynamic dictionary matching. Information and Computation, 119(2):258­282, 1995.
[3] B. S. Baker. A theory of parameterized pattern matching: algorithms and applications. In STOC 1993, pages 71­80, 1993.
[4] B. S. Baker. Parameterized pattern matching by Boyer-Moore type algorithms. In Proc. 6th Annual ACM-SIAM Symposium on Discrete Algorithms, pages 541­550, 1995.
[5] B. S. Baker. Parameterized pattern matching: Algorithms and applications. J. Comput. Syst. Sci., 52(1):28­42, 1996.
[6] M. A. Bender and M. Farach-Colton. The level ancestor problem simplified. Theor. Comput. Sci., 321(1):5­12, 2004.
[7] S. Cho, J. C. Na, K. Park, and J. S. Sim. A fast algorithm for order-preserving pattern matching. Inf. Process. Lett., 115(2):397­402, 2015.
[8] E. Coffman and J. Eve. File structures using hashing functions. Communications of the ACM, 13:427­432, 1970.
[9] M. Crochemore, C. S. Iliopoulos, T. Kociumaka, M. Kubica, A. Langiu, S. P. Pissis, J. Radoszewski, W. Rytter, and T. Walen. Order-preserving indexing. Theor. Comput. Sci., 638:122­135, 2016.
[10] A. Ehrenfeucht, R. M. McConnell, N. Osheim, and S.-W. Woo. Position heaps: A simple and dynamic text indexing data structure. Journal of Discrete Algorithms, 9(1):100­121, 2011.
[11] T. Fu, K. F. Chung, R. W. P. Luk, and C. Ng. Stock time series pattern matching: Template-based vs. rule-based approaches. Eng. Appl. Artif. Intell., 20(3):347­364, 2007.
[12] N. Fujisato, Y. Nakashima, S. Inenaga, H. Bannai, and M. Takeda. Right-to-left online construction of parameterized position heaps. In PSC 2018, pages 91­102, 2018.
[13] N. Fujisato, Y. Nakashima, S. Inenaga, H. Bannai, and M. Takeda. The parameterized position heap of a trie. In CIAC 2019, pages 237­248, 2019.
[14] T. I, S. Inenaga, and M. Takeda. Palindrome pattern matching. In R. Giancarlo and G. Manzini, editors, CPM 2011, volume 6661 of Lecture Notes in Computer Science, pages 232­245. Springer, 2011.
13

[15] H. Kim and Y. Han. OMPPM: online multiple palindrome pattern matching. Bioinform., 32(8):1151­1157, 2016.
[16] J. Kim, P. Eades, R. Fleischer, S. Hong, C. S. Iliopoulos, K. Park, S. J. Puglisi, and T. Tokuyama. Order-preserving matching. Theor. Comput. Sci., 525:68­79, 2014.
[17] Y. Matsuoka, T. Aoki, S. Inenaga, H. Bannai, and M. Takeda. Generalized pattern matching and periodicity under substring consistent equivalence relations. Theor. Comput. Sci., 656:225­233, 2016.
[18] S. G. Park, M. Bataa, A. Amir, G. M. Landau, and K. Park. Finding patterns and periods in cartesian tree matching. Theor. Comput. Sci., 845:181­197, 2020.
[19] S. Song, G. Gu, C. Ryu, S. Faro, T. Lecroq, and K. Park. Fast algorithms for single and multiple pattern Cartesian tree matching. Theor. Comput. Sci., 849:47­63, 2021.
[20] P. Weiner. Linear pattern-matching algorithms. In Proc. of 14th IEEE Ann. Symp. on Switching and Automata Theory, pages 1­11, 1973.
[21] J. Westbrook. Fast incremental planarity testing. In Proc. ICALP 1992, number 623 in LNCS, pages 342­353, 1992.
14

A Figures

0 0 11

2 01

032

9

4

10

5

1

02

1

3

14

11 6

7 15

12

8 12 2

13

S 264275843647576

w1

0

w2

00

w3

012

w4

0012

w5

01212

w6

001212

w7

0121212

w8

00121212

w9

000121212

w10

0100121212

w11

00100121212

w12

012145121212

w13 0012145121212

w14 00012145121212

w15 012312145121212

Figure 5: The Cartesian position heap of string S = 264275843647576 together with the maximal reach pointers (doubly-lined arcs). For each wi = PD(S[n - i + 1..]), the singly-underlined prefix is the string that is represented by the node ui in CPH(S), and the doubly-underlined substring is the string skipped by the maximal reach pointer.

0

1 0

1

2

3

0

2

1

1

7

8

4

15

1

1

11

12

3

14

01 2 3

9 5 16 13

1

4

6 10

S

1121221222122221

w1

0

w2

00

w3

010

w4

0110

w5

01110

w6

011115

w7

0011115

w8

01011115

w9

011011115

w10

0111411115

w11

00111411115

w12

010111411115

w13

0113111411115

w14

00113111411115

w15

012113111411115

w16 0112113111411115

Figure 6: The string S of Lemma 8 with k = 4, i.e., S = 1121221222122221 and its Cartesian-tree position heap CPH(S). For each wi = PD(S[n - i + 1..]), the underlined prefix is represented by the node of CPH(S) that corresponds to wi. Node 011 has k = 4 out-going edges. This example also shows that the upper bound of Lemma 10 is tight, since node 011 has |011| + 1 = 4 out-going edges.

15

G(S)
S 26427584365741
PD(S) 0 1 2 3 1 2 1 4 5 1 2 1 4 0
FP(S) 3 0 0 4 0 1 0 0 3 0 1 0 0 0
Figure 7: DAG G(S), PD(S), and FP(S) for string S = 26427584365741.

51 44 35
25 13

11 4
10 2 93 81 74
65

14 5 13 4
12 2

15 3 16
T

5 3,4

2 0

11 9,10

1 2

13,14

0 15 0 16

2081

1

T

1

1 6,7 0

12

node T[i..] PD FP

16







15

3

0

0

14 13

53 43

00

00

12

23 01 10

11 453 010 100

10 9

253 343

012

200

8 123 011 110

7 6

423 523

001

010

5 1453 0113 2100

4 3

4253 5343

0012

0200

2 5423 0001 0010

1 3523 0101 1010

Figure 8: Left upper: An example of input trie T . Left lower: The FP-trie T FP that is obtained from the original trie T . For instance, the FP encodings of the two path strings T [3..] = 5343 and T [4..] = 4253 have the same PF encoding 0200 and thus the node id's 3 and 4 are stored in a single node in T FP. The representative (the id) of the node {3, 4} in T FP is min{3, 4} = 3.

16

16

0

0

15 1

13

0

1

2

6

2 3

12 0 12

11 8 9

1

3

15

Figure 9: CPH(T ) for the trie T of Fig. 8, where every node v store the representatives 1, 2, 3, 5, 6, 8, 9, 11, 12, 13, 15, 16 of the corresponding equivalence class Cv.

root
u iv

root
vi
u

Figure 10: Left: Illustration for Condition (a) of Lemma 11. Right: Illustration for Condition (b) of Lemma 11, where the doubly-lined arc represents the maximal reach pointer.

17

