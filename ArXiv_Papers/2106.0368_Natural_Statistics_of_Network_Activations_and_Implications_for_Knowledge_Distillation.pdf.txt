NATURAL STATISTICS OF NETWORK ACTIVATIONS AND IMPLICATIONS FOR KNOWLEDGE DISTILLATION
Michael Rotman and Lior Wolf
Tel Aviv University

arXiv:2106.00368v1 [cs.CV] 1 Jun 2021

ABSTRACT
In a matter that is analog to the study of natural image statistics, we study the natural statistics of the deep neural network activations at various layers. As we show, these statistics, similar to image statistics, follow a power law. We also show, both analytically and empirically, that with depth the exponent of this power law increases at a linear rate.
As a direct implication of our discoveries, we present a method for performing Knowledge Distillation (KD). While classical KD methods consider the logits of the teacher network, more recent methods obtain a leap in performance by considering the activation maps. This, however, uses metrics that are suitable for comparing images. We propose to employ two additional loss terms that are based on the spectral properties of the intermediate activation maps. The proposed method obtains state of the art results on multiple image recognition KD benchmarks.
Index Terms-- Knowledge Distillation, Image Statistics
1. INTRODUCTION
The hierarchical structure of Convolutional Neural Networks (CNN) has been lined to their ability to capture the visual world in a way that supports a high degree of invariance to image transformations [1]. Furthermore, their structure leads to an inductive bias that is especially suitable for reconstructing natural images [2]. It is also known that the activations that are computed in the networks are very effective in the setting of transfer learning, even without further finetuning [3, 4].
Despite the importance of CNNs and the effectiveness of their intermediate representations, there is little work on the statistical properties of the activation maps. This is in contrast to the significance of the known result in the field of natural image statistics.
One of the hallmarks of the study of natural images is the power-law behavior of natural images. In this work, we show that a similar power-law holds also for the activations obtained at deep layers of CNNs. Moreover, based on spectral
This project has received funding from the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation program (grant ERC CoG 725974). The contribution of the first author is part of a Ph.D. thesis research conducted at TAU.

analysis considerations, the scaling exponent of the powerlaw is shown to grow linearly with depth.
Our theoretical results are validated empirically. Additionally, an implication of the study is that when comparing two activation maps, the image norms, such as L1 and L2 are not optimal. We suggest instead to employ two well-known spectral norms. The first is the L1 norm in the spectrum domain. The second is the cross-power term [5].
Knowledge Distillation (KD) is an application in which comparing activation maps is essential. While earlier methods compared normalized logits [6], the more recent methods compare the activation maps after each residual block [7].
Our experiments demonstrate that the KD method that is based on the spectral norms improves performance when distilling deep ResNets [8] to shallower networks, both on CIFAR-100 and on ImageNet.

2. RELATED WORK
Natural Image Statistics Although natural images can be easily distinguished from one another, they exhibit universality. The power spectrum of an ensemble of images, P (k), when averaged over rotations, is described as a power law,

P (|k|) = |k| ,

(1)

for   -2 [9, 10, 11]. Subsequent works aimed at finding deviations from this power law when parts of the images are scaled or occluded [12].
Knowledge Distillation Hinton et al. [6] introduced a framework, termed Knowledge Distillation (KD) in which a network is trained with the assistance of a pretrained network with a higher-capacity. By entangling the hidden features of the teacher and student networks, FitNets [13] were able to compress deep architectures to thinner ones. Recent works in this field apply different criteria for matching between the hidden feature maps of the two networks, either by applying an additional convolution layer [7, 13] on these maps, or by matching between the correlation [14] or the Jacobian [15] of the feature maps.

log P (|k|) log P (|k|)

101

 = -2.203  = -3.261

100

10-1

10-2

10-3

10-4

10-5

10-6

Input Image AP Image

Activation Map 3 AP Activation Map 3

10-1.5

10-1

10-0.5

log |k|

100

10-1

10-2

10-3

10-4

10-5 10-6

Pre-trained Activation Map 1 Activation Map 2 Activation Map 3

Random Init Activation Map 1 Activation Map 2 Activation Map 3

10-1.5

10-1

10-0.5

log |k|

Fig. 1. The log-log plot of the rotational invariant power spectra of the CIFAR100 validation set (magenta) and for the last activation map (blue) as a function of the frequency |k|. The dashed lines show the slope of the power spectra,  = -2.203 for the natural images and  = -3.261 for the activation map when fitting to the high-frequency domain of the spectra. The
orange and green plots show the power spectra of the average
pooled images and activation maps, respectively.

3. THE POWER SPECTRUM OF ACTIVATIONS

The rotational invariant power spectrum of the 10000 images of the CIFAR100 [16] validation set obeys the universal power law in (1) as can be seen in Fig. 1 (magenta). When transforming back to the spatial domain, the correlation C (r) between pixels residing at distance r is of the form [11]

C (r) = C1 + C2r-(2+) ,

(2)

where C1 and C2 are constants. The functional form in Eq. (2)

reveals the scaling properties of natural images. For instance,

if an average pooling filter is applied over an image of size

N ×N and correlation

reduces lengths

it to an image of to decrease by a

size

N 2

factor

× of

N222,+we.

expect the However,

the power spectrum is invariant (up to the high frequencies, in

which data is lost due to the pooling operation) to this scaling

as apparent in Fig. 1 (orange).

To further investigate the universal behavior of the activa-

tions in a CNN, we examine the power spectrum of the activa-

tions of the feature maps residing between the residual blocks

of the WideResNet. Fig. 2 presents the power spectrum of

each activation map for both an untrained network and for a

trained network. As can be seen, the activation maps for the

untrained map are almost flat, since the parameters of the con-

volution layers that act on them are sampled from the Gaus-

sian distribution. On the other hand, the activation maps of the

trained network exhibit a decay in the high frequency region

in the power spectrum as the activations reside in a deeper

stage. This decay is due to the loss of information that oc-

curs between the blocks. A trivial and incorrect explanation

Fig. 2. The log-log plot of the rotational invariant power spectra of the three activation maps of a pre-trained and a randomly initialized (dashed) WideResNet applied on the CIFAR100 validation set as a function of the frequency |k|.

would be that the loss of information is a result of the pooling operations. In order to reject this idea, we apply an average pooling operation on the activation map and inspect its power spectrum in Fig. 1. As can be seen, the high frequency region of the power spectrum also behaves in a universal manner under scaling. Furthermore, The slope in Fig. 1 teaches us about the non-local structure of the activation maps. With an exponent of  = -3.261, the correlation length of the activation map increases almost linearly with the distance, pointing to the highly non-local structure of the activation map. Note that activation map 1 and activation map 2 in Fig. 2 have a higher slope,  = -0.687 and  = -1.572, in their respective power spectra, and therefore exhibit a more localized structure.
Analysis Consider a 3 × 3 kernel W ,

W =  w-1,1  w-1,0 w-1,-1

w0,1 w0,0 w0,-1

. w1,1 
w1,0  w1,-1

(3)

Denote by  the convolution operator. Convolving W with an image f (x) of size N × N produces output g (x) of size N × N , g (x) = W  f . Using the convolution theorem, the Fourier transform of g (x), can be expressed as,

g~ (k) = W~ (k) f~(k) ,

(4)

where W~ (k) is obtained by zero-padding W to size N × N , and applying the Fourier transform on the zero-padded kernel. The Fourier transform, W~ (k), consists of exactly nine terms,

1

1

W~ (kx, ky) = N

ei(kxx+kyy)wx,y ,

(5)

x=-1 y=-1

where kx and ky are the coordinates in Fourier space and x and y are the coordinates in the spatial domain (the

center of the image is located at x = y = 0). N is a

normalization constant. The polar coordinates is a natural

choice for representing the rotation invariant power spectra,

(kr, k) =

kx2 + ky2  |k|, tan-1

ky kx

. The rotational

averaged Fourier transform of the kernel, W~ (|k|), only de-

pends on |k| and consists of three frequency modes,

W~

(|k|)

=

w0,0

+

ei|k|W1

+


ei 2|k|W2

(6)

where W1 = w0,1 + w0,-1 + w1,0 + w-1,0 and W2 = w1,1 + w1,-1 + w-1,1 + w-1,-1. The three frequencies correspond to the the distances 0, 1, and 2 from the ori-

gin of the elements of W . Since the input image f (x) is

isotropic, and as a consequence so is its Fourier transform f~(k), the power spectrum P is also rotationally invariant,

and does not depend on k. Since the power spectrum is averaged over frequencies of the same length, one has to

also factor in the Jacobian, of the coordinates transforma-

tion which is |k|. This results in a power spectrum of the

form P (|k|)

=

|k|

W~ (|k|) 2

f~(|k|)

2
,

Thus contributions

to log P (|k|) are due to the original power spectrum of the

input, together with contributions from

W~

(|k|)

2
.

Since

universality only depends on the logarithm of the power spectrum, P (|k|), up to multiplicative constants is

P (|k|)  |k| w02,0 + W12 + W2 2 f~(|k|) 2 +

+|k|W1W2 cos

 1 - 2 |k|

f~(|k|) 2

+|k|w0,0W1 cos (|k|) f~(|k|) 2

+|k|w0,0W2 cos

1 |k| 2

f~(|k|) 2 .

(7)

The first term in Eq. (7) comes from the power spectrum of the original image. The other remaining terms are "interference" elements that appear due to the frequency content of W~ . The cosine contribution in these terms is in the range [0, 1] for the possible values of |k|, and therefore, their contribution to log P (|k|) is negligible for low frequencies, and becomes dominant for high frequencies.
The same analysis can be extended to multiple convolutional layers. In this scenario, when multiple layers are applied on an input image f , the resulting power spectrum gains a multiplicative contribution from each layer. The logarithm of the power spectrum therefore gains only an additive contribution that is proportional to the number of layers.
This result shows that the application of multiple convolutional layers only influences the high frequency region of the power spectrum. Furthermore, this analysis explains the empirical behavior of the power spectrum as a function of the activation map depth that is seen in Fig. 2.
Another issue that surfaces when examining the power spectra structure in Fig. 2, is that unlike the spectra of un-

Setup
a b c d e f

Teacher
WideResNet 28-4 WideResNet 28-4 WideResNet 28-4 WideResNet 28-4 PyramidNet-200 PyramidNet-200

Student
WideResNet 16-4 WideResNet 28-2 WideResNet 16-2
ResNet 56 WideResNet 28-4 PyramidNet-100

|S |/|T |
47.2% 25.0% 11.9% 14.7% 21.9% 14.6%

Table 1. The teacher-student setups for CIFAR100.

trained networks, as the activation map is from a deep layer, its content no longer obeys the Gaussian distribution, and therefore might not be balanced around the mean. As a consequence, the MSE metric may perform poorly since it estimates the mean of the distribution. Combining this with our analysis that each layer obeys a different power law, might indicate that an additional distance metric is required.

4. KNOWLEDGE DISTILLATION

Let {(xt, yt)}nt=1 be a set of tuples, each contains an example, xt  Rd, with its corresponding label, yt  {1, . . . , k}. Given a pre-trained teacher network, T , with parameters T , the objective is to train a student network, S, with parameters S , such that |S | < |T |.
In order to leverage the information encapsulated inside T , a feature-wise term is added to the loss function, so the features of S are entangled with the features of T . Denote by FiT and FjS the ith feature map in T and the jth feature map in S (outputs of the ith and jth convolution layer in T and S). Assume that FiT and FjS represent the same embedding of the input xt. Since these feature maps may not share the same dimensionality, learnable transformations TT and TS are applied on the feature maps to produce reduced feature maps, each with M channels, denoted next by the index m. Once the reduced feature maps, RiT,m  TT ,m FiT m and RjS,m  TS,m FjS m are embedded in the same space, a similarity metric can be utilized.
Entangling between the reduced feature maps is achieved by introducing a pixel-wise distillation loss term [7], Loverhaul.
This term drives the convergence of the positive pre-ReLU entries in the feature maps of S towards the feature maps of T .
This term, however, only acts on specific entries in the spatial
domain of the feature maps, and is unable to capture non-
local aspects of the feature maps. To remedy this, two Fourier
terms are added to the loss function since non-local properties
are naturally captured in Fourier space as in Eq. (5). The first term, L1 loss over the Fourier transform of the reduced feature maps, LL1 = R~iT,m -R~jS,m , increases the robustness of
the student feature maps in Fourier space. The second term,

LCP S

=

1 M

m

1

-

PiTj,Sm (|k|) PiTj,Tm (|k|) PiSj,Sm

(|k|)

,

(8)

Setup
a b c d e f

Teacher
21.09 21.09 21.09 21.09 15.57 15.57

Baseline
22.72 24.88 27.32 27.68 21.09 22.58

KD [6]
21.69 23.43 26.47 26.76 20.97 21.68

FitNets [13]
21.85 23.94 26.30 26.35 22.16 23.79

AT [17]
22.07 23.80 26.56 26.66 19.28 19.93

Jacobian [15]
22.18 23.70 26.71 26.60 20.59 23.49

FT [18]
21.72 23.41 25.91 26.20 19.04 19.53

AB [19]
21.36 23.19 26.02 26.04 20.46 20.89

Overhaul[7]
20.72 22.15 24.27 25.11 18.03 19.07

Ours
20.37 21.45 24.42 24.87 17.99 18.67

Table 2. Error rates on the CIFAR-100 validation set. Baseline=no distillation. Results for the literature methods are from [7].

Method
Teacher Baseline KD [6] AT [17] FT [18] AB [19] Overhaul [7] Ours

Err 1
23.84 31.13 31.42 30.44 30.12 31.11 28.75 27.49

Err 5
7.14 11.24 11.02 10.67 10.50 11.29 9.66 8.99

Table 3. Classification error rate on the ILSVRC 2012 validation set. Baseline represents no distillation. Results for other methods are from [7]. T is a ResNet 50 and S is MobileNet.

is a cross-power spectrum loss function, where · denotes expectation over equal |k| lengths, and P X Y (|k|)ij,m is,

PiXj,Ym (|k|) = R~iX,m  (|k|) R~jY,m (|k|) .

(9)

This term matches between the rotational invariant power spectra of the teacher and the student networks, enforcing the activations of these two networks to share the same non-local structure. The total loss function is

L = LCE + Loverhaul + LL1 + LCP S , (10)

where LCE = -

k c=1

c,yt

log

pc

(xt)

is

the

cross

entropy

loss function, and pc is the probability that xt belongs to class

c, assigned by network S.

5. EXPERIMENTS
We show the benefits of combining the spectral information during the process of KD for the task of image recognition. In all experiments, we applied the KD loss terms on the feature maps located after the bottlenecks of T and S.
CIFAR-100 The CIFAR-100 [16] dataset consists of 60, 000 32 × 32 color images divided into 100 classes. There are 50, 000 training examples and 10, 000 for validation. In order to show the importance of spectral matching, our method is validated over several teacher-student setups. The WideResNet [20] with 28 hidden layers and ×4 channel ratio and PyramidNet-200 [21] with 240 hidden layers were used as the teacher networks. For the student networks, smaller versions of the WideResNet, ResNet-56, and a shallower version of the PyramidNet with 84 hidden layers were selected,

see Tab. 1. We used the same setup as Heo et al. [7]. All networks were trained for 200 epochs using SGD with a learning rate of 0.1 and a momentum of 0.9, L2 regularization of 1e-4 on the network's parameters,  =  = 1e-4 and  = 0.01. The learning rate was multiplied by 0.1 after 100 epochs and again after 150 epochs. For setups (a)-(d) a batch size of 128 was used whereas for setups (e) and (f) a batch size of 64 was used for memory considerations.
Our experimental results appear in 2. As can be seen, our method outperforms in five out of the six setups. Under the experimental setup (a), both the Overhaul [7] method and our method outperform the teacher network.
ImageNet The ILSVRC 2012 [22] dataset contains 1.2M training images and 50,000 validation images. These images are cropped to the size of 224 × 224 for both training and evaluation. The teacher and student networks for this task are the ResNet 50 and MobileNet [23]. We used the same setup as Heo et al. [7], an SGD optimizer with a learning rate of 0.1 and a momentum of 0.9, L2 regularization of 0.0001 on the network's parameters,  =  = 0.00001 and  = 0.001. The student network was trained with a batch size of 256 for 100 epochs. The learning rate was reduced by a factor of 0.1 every 30 epochs.
A comparison of our approach to recent methods is shown in Tab. 3. As can be seen, our approach achieves a substantially lower error rate both in the top-1 and top-5 error rates.
6. CONCLUSIONS
In this work we have explored the power-law property of the activations of deep neural networks. We show that the correlation lengths grow linearly with depth, whereas the activations become more and more concentrated in Fourier space. This behavior indicates an increasing amount of mutual influences between distant image locations, which matches the shift that occurs with depth from local processing to higher-level semantic information.
As an immediate application of our study, we prescribe how to utilize the information in Fourier space as a distance metric for activations of deep layers. When this metric is used for learning, such as in the field of KD, it leads to an improvement over the state of the art method.

7. REFERENCES
[1] Tomaso Poggio, Tomaso A Poggio, and Fabio Anselmi, Visual cortex and deep networks: learning invariant representations, MIT Press, 2016.
[2] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky, "Deep image prior," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 9446­9454.
[3] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson, "How transferable are features in deep neural networks?," in Advances in neural information processing systems, 2014, pp. 3320­3328.
[4] Yaniv Taigman, Ming Yang, Marc'Aurelio Ranzato, and Lior Wolf, "Deepface: Closing the gap to human-level performance in face verification," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2014, pp. 1701­1708.
[5] Lawrence R Rabiner and Bernard Gold, "Theory and application of digital signal processing," tads, 1975.
[6] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean, "Distilling the knowledge in a neural network," arXiv preprint arXiv:1503.02531, 2015.
[7] Byeongho Heo, Jeesoo Kim, Sangdoo Yun, Hyojin Park, Nojun Kwak, and Jin Young Choi, "A comprehensive overhaul of feature distillation," in Proceedings of the IEEE International Conference on Computer Vision, 2019, pp. 1921­1930.
[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, "Deep residual learning for image recognition," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770­778.
[9] David J Field, "Relations between the statistics of natural images and the response properties of cortical cells," Josa a, vol. 4, no. 12, pp. 2379­2394, 1987.
[10] DJ Tolhurst, Y Tadmor, and Tang Chao, "Amplitude spectra of natural images," Ophthalmic and Physiological Optics, vol. 12, no. 2, pp. 229­232, 1992.
[11] Daniel L Ruderman, "Origins of scaling in natural images," Vision research, vol. 37, no. 23, pp. 3385­3398, 1997.
[12] RP Millane, S Alzaidi, and WH Hsiao, "Scaling and power spectra of natural images," in Proc. Image and Vision Computing New Zealand, 2003, pp. 148­153.
[13] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio, "Fitnets: Hints for thin deep nets," arXiv preprint arXiv:1412.6550, 2014.

[14] Junho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim, "A gift from knowledge distillation: Fast optimization, network minimization and transfer learning," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 4133­4141.
[15] Suraj Srinivas and Franc¸ois Fleuret, "Knowledge transfer with jacobian matching," arXiv preprint arXiv:1803.00443, 2018.
[16] Alex Krizhevsky, Geoffrey Hinton, et al., "Learning multiple layers of features from tiny images," 2009.
[17] Sergey Zagoruyko and Nikos Komodakis, "Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer," arXiv preprint arXiv:1612.03928, 2016.
[18] Jangho Kim, SeongUk Park, and Nojun Kwak, "Paraphrasing complex network: Network compression via factor transfer," in Advances in neural information processing systems, 2018, pp. 2760­2769.
[19] Byeongho Heo, Minsik Lee, Sangdoo Yun, and Jin Young Choi, "Knowledge transfer via distillation of activation boundaries formed by hidden neurons," in Proceedings of the AAAI Conference on Artificial Intelligence, 2019, vol. 33, pp. 3779­3787.
[20] Sergey Zagoruyko and Nikos Komodakis, "Wide residual networks," arXiv preprint arXiv:1605.07146, 2016.
[21] Dongyoon Han, Jiwhan Kim, and Junmo Kim, "Deep pyramidal residual networks," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 5927­5935.
[22] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al., "Imagenet large scale visual recognition challenge," International journal of computer vision, vol. 115, no. 3, pp. 211­252, 2015.
[23] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam, "Mobilenets: Efficient convolutional neural networks for mobile vision applications," arXiv preprint arXiv:1704.04861, 2017.

