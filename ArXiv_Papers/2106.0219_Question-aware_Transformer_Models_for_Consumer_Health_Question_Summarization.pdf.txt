Question-aware Transformer Models for Consumer Health Question Summarization
Shweta Yadav1,, Deepak Gupta1, Asma Ben Abacha, Dina Demner-Fushman
LHNCBC, U.S. National Library of Medicine, MD, USA

arXiv:2106.00219v1 [cs.CL] 1 Jun 2021

Abstract
Searching for health information online is becoming customary for more and more consumers every day, which makes the need for efficient and reliable question answering systems more pressing. An important contributor to the success rates of these systems is their ability to fully understand the consumers' questions. However, these questions are frequently longer than needed and mention peripheral information that is not useful in finding relevant answers. Question summarization is one of the potential solutions to simplifying long and complex consumer questions before attempting to find an answer. In this paper, we study the task of abstractive summarization for real-world consumer health questions. We develop an abstractive question summarization model that leverages the semantic interpretation of a question via recognition of medical entities, which enables generation of informative summaries. Towards this, we propose multiple Cloze tasks (i.e. the task of filing missing words in a given context) to identify the key medical entities that enforce the model to have better coverage in question-focus recognition. Additionally, we infuse the decoder inputs with question-type information to generate question-type driven summaries. When evaluated on the MEQSUM benchmark corpus, our framework outperformed the state-of-the-art method by 10.2 ROUGE-L points. We also conducted a manual evaluation to assess the correctness of the generated summaries.
Keywords: Consumer Health Question Summarization, Transformers,
Corresponding author Email addresses: shweta.shweta@nih.gov ( Shweta Yadav), deepak.gupta@nih.gov (Deepak Gupta), asma.benabacha@nih.gov (Asma Ben Abacha), ddemner@mail.nih.gov (Dina Demner-Fushman) 1Both authors contributed equally to this work.

Preprint submitted to Journal of Biomedical Informatics

June 2, 2021

Abstractive Summarization
1. Introduction
Consumers frequently query the web in search of quick and reliable answers to their healthcare information needs. A recent survey showed that on average eight million people in the United States seek health-related information on the Internet every day2. A potential solution to support consumers in their health-related information search is to build a natural language question answering (QA) system that can extract both reliable and precise answers from a variety of health-related information sources. Consumers' questions are often overly descriptive and contain peripheral information such as the patient's medical history, which makes their automatic analysis and understanding more challenging for QA systems. These peripheral details are often not required to retrieve relevant answers, and their removal can lead to substantial improvements in QA performance [1]. Hence, novel strategies should be devised for automatic question simplification prior to answer retrieval.
Recent work in abstractive summarization has made significant advancement on large corpora such as news articles [28, 30], the biomedical literature [42], and clinical records [44, 26]. Two major shortcomings, however, impede higher success in obtaining coherent and fluent summaries for consumer health questions (CHQ): (1) understanding the intent of the medical question and (2) capturing the salient medical entities to ensure a more relevant coverage. In the case of consumer health question answering, two key elements have to be identified correctly: the question focus and the question type that captures the intent. The question focus is the main entity of the question and the question type is the aspect of interest about the focus.
We can observe from Table 1 that many existing models fail to identify the question focus "Friedreich's ataxia" and the original question types: `Treatment' and `Test'. In these examples, the summarization models generate either a partial question summary (capturing only the `Treatment' question type) or an out-ofcontext summary. These observations indicate that the current models may lack semantic understanding of the input, which is crucial for generating relevant question summaries for question answering. The importance of the question focus and type was specifically highlighted in Deardorff et al. [8], where the authors showed
2https://pewrsr.ch/3l6m3mv
2

Original Question
I have been told I have Friedreich's ataxia. I am looking for a treatment to reverse it. I has to do with the chromosome number 9 defective in both of my parents and possible damage to chromosome number 10. Would you be able to tell me if number 9 is defective in me.
Reference Summary
where can i get genetic testing for friedreich's, and what are the treatments for it?
Generated Summary
Sequence to Sequence: what are the treatments for disease Pointer Generator: what are the treatments for chromosome chromosome chromosome chromosome [UNK] MINILM: what are the treatments for chickenrenon's ataxia ?
Table 1: Example of generated summaries by existing neural summarization models.
that the question focus and the question type are potent factors in retrieving relevant answers.
To this end, we present our framework for abstractive summarization with latent knowledge-induced pretrained language model (LM) in the under-explored area of consumer health question summarization. Under the proposed framework, we aim to enhance the traditional transformer-based pretrained LM with the knowledge of `question-focus' and `question-type'.
We explore two variants of knowledge-induced LM for the task of question summarization. In the first variant (`question-focus aware' model), we adapt LM by introducing various Cloze tasks for question summarization. More specifically, we define three Cloze tasks: `masked seq2seq prediction', `masked n-gram prediction', and `masked medical entities prediction' to induce good coverage of medical entities in the summarized question, which might help the model to generate more informative summaries. In the second variant, we infuse the latent question-type knowledge into the LM that guides the LM to capture the relevant and appropriate intent of the questions while generating the summary.
Our proposed models outperformed transformer based pretrained models (e.g. MiniLM) and the state-of-the-art pointer-generator model on the reference MEQSUM benchmark for consumer health question summarization [4]. More importantly, given the small size of the training corpus (5, 155 samples) when compared to the volume of training corpora in the news articles and biomedical domains, our
3

solution proved effective in harnessing the existing pretrained networks to benefit the task of question summarization. A detailed experimental analysis revealed that the proposed models are capable of generating questions with significant coverage of the question focus and the question type, leading to relevant and informative question summaries.
We summarize our contributions as follows:
· We propose a simple yet effective method for consumer health question summarization, by introducing various Cloze tasks to pretrained transformer models for better coverage of the question focus in the summarized questions.
· We also introduce an explicit and an implicit ways of infusing the knowledge of the question type in pretrained transformer models for the generation of informative and question-type driven summaries.
· Our proposed model achieves the state-of-the-art performance in the consumer health question summarization task and outperforms the recent stateof-the-art pretrained LM models (e.g. T5, PEGASUS, and BART).
· We conduct an automatic evaluation on the MEQSUM benchmark and a manual evaluation to assess the correctness of the generated question summaries.
2. Related Work
Abstractive summarization of consumer health questions is at the intersection of three research areas that we discuss in this section.
· Open-Domain Abstractive Summarization: Abstractive text summarization approaches made a leap in performance with the development of neural machine translation and text generation models [3] and the creation of large corpora such as the CNN/Daily Mail dataset [29]. The most common approaches to the task of abstractive summarization are sequence-to-sequence (seq2seq) models [27], and their pointer generator extension [36] that addresses out-of-vocabulary words using a copy mechanism. An additional coverage mechanism was also proposed to reduce word repetition. Recently, Lebanoff et al. [21] introduced a `points of correspondence' method based on text cohesion theory to fuse sentences together into a coherent
4

and more faithful summarized text. Few other recent works have exploited question answering and natural language inference (NLI) models to identify factual coherence in the generated summaries [13, 19]. Goodrich et al. [15] compared various models that can extract factual tuples from text utilizing their proposed model-based metrics to analyze the factual accuracy of the generated text. Falke et al. [13] evaluated the factual-correctness of the generated summaries using NLI systems by re-ranking the generated summaries based on the entailment predictions of the NLI models. Huang et al. [16] also exploited open information extraction to extract the salient entities from the document to generate faithful summaries. In another research track, some works explored reinforcement learning (RL) for abstractive summarization by relying on ROUGE scores [33, 6, 11, 32] or on multiple RL rewards in an unsupervised setting [20].
· Biomedical and Clinical Domains: The biomedical domain has also observed a recent surge in the development of neural summarization models. The majority of these models are focused on summarization of the biomedical literature [7, 42] and radiology reports [23, 43, 44]. For instance, MacAvaney et al. [26] improved the performance of a pointer generator model by augmenting the data with medical ontologies that enable the model to learn and decode the key entities in the generation process. In a similar approach, Sotudeh et al. [37] infused a neural summarization model with medical ontology-based terms to solve the content selection problem for clinical abstractive summarization. In a different approach, Cohan et al. [7] developed a discourse-aware neural summarization model that captures the discourse structure of the document via a hierarchical encoder. Zhang et al. [42] proposed an encoder-decoder method based on the transformer architecture and on huge text corpora with a novel self-supervised objective function. They evaluated their method on PubMed articles and obtained encouraging ROUGE scores.
· Consumer Health Questions: Most of the research work discussed above was conducted on datasets that are often well formed, well structured, and grammatically correct. In contrast, consumer health questions are profuse with misspellings, ungrammatical structures, and non-canonical forms of medical terms that resemble open-domain language more than the medical language. Moreover, the consumer questions submitted to online platforms, such as forums and institutional contact pages, are very often long
5

questions with a substantial amount of peripheral information and multiple sub-questions. This adds to the challenge of developing a summarization model that can account for concise, faithful, and succinct summaries. Ben Abacha and Demner-Fushman [4] have introduced the consumer-health question summarization task and created a corpus of 5, 155 question summaries. They experimented with seq2seq and pointer-generator models and achieved 44.16% ROUGE-1 score in consumer-health question summarization.
Our work advances the state-of-the-art for the summarization of consumers' questions and introduces new approaches that were specifically designed to preserve the intent of the original questions.
3. Question Summarization Approach
We tackle the task of consumer health question (CHQ) summarization with the goal of generating summarized questions expressing the minimum information required to find correct answers. Our proposed summarization models are tailored to generate CHQ summaries that are aware of the question focus and the question type(s). Towards this, we devise `question-focus aware' and `question-type aware' summarization models. In the `question-focus aware' model, we trained the summarization model over three Cloze tasks to ensure the maximum coverage of question focus in the generated summaries. In the `question-type aware' summarization model, we guide the model to generate question-type driven summaries. We model this by infusing the question type knowledge both explicitly and implicitly in the summarization model.
We use the pretrained language model MINILM [40] as the base model architecture to summarize the consumer health questions. We chose MINILM for two particular reasons. First, MINILM has shown near state-of-the-art results on both natural language generation and understanding. Second, it has fewer parameters (33M) compared to other large transformer-based language models, which mitigates the latency and resource challenges for fine-tuning and for use in real-time applications.
MINILM is built on deep self-attention distillation framework to compress large Transformer-based pretrained models. In this framework, a small model (student) with fewer parameters is trained by deeply mimicking the self-attention module of the large pretrained model (teacher). The self-attention module of the last Transformer layer of the teacher model is distilled to gather the knowledge to build the student model. Furthermore, MINILM utilizes the scaled dot-product between
6

values in the self-attention module as the new deep self-attention knowledge, in addition to the attention distributions (i.e., the scaled dot-product of queries and keys) that has been used in the Transformer-based pretrained model. The model is trained by minimizing the sum of (1) KL-divergence between the self-attention distributions of the teacher and student models, and (2) KL-divergence between the value relation of the teacher and student models. In the distillation setup, the uncased version of BERTBASE is used as teacher model. BERTBASE [9] has about 109M parameters with 12-layer Transformer (768 hidden size), and 12 attention heads. The number of heads of attention distributions and value relation are set to 12 for MINILM student models. Below we begin by providing details on the input representation followed by our proposed `question-focus aware' and `question-type aware' summarization models.
3.1. Input Representation Given an original question Q = {q1, q2, . . . , qm} and the reference summary
S = {s1, s2, . . . , sn}, we follow the input representation of BERT [9] and tokenize Q and S using the WordPiece [41] tokenizer. Before processing, we also add the [CLS] token to the beginning of the token sequence of the question.
We form a single input token sequence by concatenating the token sequence of the question and their summarized counterpart. We add the conventional separator token ([SEP]) between and at the end of input token sequence. We denote the final input tokens as {t1, t2, . . . , tN }, where N = |Q| + |S| + 3, and 3 is the sum of one [CLS] and two ([SEP]) tokens. For each input token, we compute the final token embedding (v) by summing the corresponding token, position, and segment embeddings. The input embeddings can be denoted by the H0 = [e1, e2, . . . , eN ].
3.2. Question-Focus Aware Model (QFA) We define multiple Cloze tasks to effectively guide the MINILM model to-
wards improved awareness of question focus words for the consumer health question summarization task. Generally, in a Cloze task, some tokens are chosen randomly and replaced with special token ([MASK]), which is predicted during training. We consider each medical entity as a candidate for the question focus.
Cloze Task 1 - Sequence-to-Sequence Masking. We adapt the seq2seq LM objective function as discussed in [10]. We aim to predict the masked token from the reference summary using the original question token sequence as input and the
7

reference summary tokens which are to the left3 of the masked token and current token. For example, given question tokens tq1, tq2, tq3 and its reference summary tokens ts4, ts5, ts6, we feed the input token sequence as "[CLS], tq1, tq2, tq3, [SEP], ts4, ts5, ts6, [SEP]" to the model. The summary token ts5 has access to the first seven tokens i.e. all question tokens (including [CLS] and [SEP]), and the previous
tokens in the reference summary, and current token. This Cloze task objective
mimics the seq2seq generation paradigm, where the encoder encodes the entire
source sentence and makes it available to the decoder that generates one word at
a time, given the encoder states and all the previously generated words. The input vector H0 is passed to L-size stack of Transformer block [39] to
obtain the contextual representations of each token. Formally, at a given level l,
we compute the contextual representation as follows:

Hl = Transformerl(Hl-1), l  [1, L].

(1)

For the lth Transformer layer, we compute the query (Q^ ), key (K^ ), and value (V^ ) vectors by projecting the output of the (l - 1)th Transformer layer as follows:

Q^ = Hl-1WlQ^, K^ = Hl-1WlK^ , V^ = Hl-1WlV^

(2)

where the previous layer's output Hl-1  RN×dh, parameter matrices WlQ^, WlK^ , WlV^  Rdh×dk , and dh and dk are the dimensions of the hidden state and scaling factor respectively.
In order to facilitate the token access behaviour, we compute the output of a self-attention head Al as follows:

Al

=

sof tmax( Q^K^  dk

+

M)V^ l

0, allow the jth token to attend ith

(3)

Mij = -, otherwise

where the mask matrix M  RN×N determines whether a pair of tokens can be attended to each other. We create the mask matrix M for each pair of Q and
S. For each token tj of S, we set the appropriate value (based on Eq. 3) in mask matrix M such that, it only attends to all the tokens which are to the left of tj including self. The final hidden state for the ith input token is computed from the

3While generating at a given time step t, the model only has the access to the < t tokens.

8

output of the last Transformer layer (HL  RN×dh) as hi  Rdh. We transform the hidden state to the vector of the vocabulary dimension as follows:

T = gelu(HLW + bT )

(4)

P = sof tmax(T U + bP )

where, W  Rdh×dh and U  Rdh×|V | are the weight matrices and bT  Rdh and bP  R|V | are the bias terms associated with the transformed hidden state T  RN×dh and prediction P  RN×|V | respectively. |V | denotes the vocabulary
size. We minimize the cross-entropy loss to train the network. Formally,

|V |

Ls2s = - yilogpi

(5)

i=1

where, yi is the actual token probability of the ith token and pi is the predicted
token probability. We followed the masked word prediction similar to [9] and choose the ith token positions from the summary sentence at random for prediction. We replaced the ith token (a) with the [MASK] token 80% of the time (b) a random token from vocabulary 10% of the time (c) the unchanged ith token 10%
of the time.

Cloze Task 2 - N-gram Masking. As many medical/clinical entities are multiword expressions, the traditional MINILM masking strategy may not be useful to correctly predict the medical entities. To address this, we introduce n-gram masking, which randomly selects an n-gram (from 1 to 3) from the summary sentence to mask and predict them using the given consumer health question similar to the Cloze Task 1 objective. We compute the cross-entropy loss (Lngm) similar to the Eq. 5.

Cloze Task 3 - Medical Entity Masking. We design another Cloze task objective to train the summarization network where we first identify the medical entities4. This important LM objective enhances the model's ability to generate medical terms correctly based on the given consumer health question. Predicting the correct medical entities leads to more relevant question summaries. We compute the cross-entropy loss (Lfwm) similar to the Eq. 5.

4We utilize the Scispacy en ner bionlp13cg md model trained on the BioNLP13CG dataset [34] to identify medical entities belonging to 16 distinct UMLS semantic types.

9

Multi-Cloze Fusion. We utilize the benefits of all the aforementioned Cloze tasks and train the question-focus aware summarization model by minimizing the aggregated (sum) loss computed from each Cloze task. Formally,

Lqfa = Ls2s + Lngm + Lfwm

(6)

where, Lqfa is the loss of the question-focus aware summarization model.

Testing Phase.. Once the model is trained on the Cloze tasks, it can be used for testing/generating summaries. The model uses all the tokens from the original question Q and previously generated sequence until the t - 1 time step to generate at a given time step t. This process is continued until the generated text has the length of the target summary. We utilize Beam Search to generate optimal summary with a beam size of five.

3.3. Question-Type Aware Model (QTA)
We incorporated the question-type knowledge in two ways: explicit and implicit knowledge infusion. We began by first identifying the question types followed by their infusion in the summarization model.

Question Type Identification. Following a previous study of consumer health questions [17], we use a heuristic to categorize the original question Q to one of the question-types (Information, Treatment, Testing, Cause, Physician, Ingredients, and Other). We searched for the specific question type with its lemma word in the lemmatized reference summary and labeled each consumer question. Since, the reference summary will not be available at the inference time, we aim to map the question type with the original question Q and infuse the question-type knowledge into the question summarization model. Specifically, in the explicit knowledge infusion, we utilize the BERTLARGE model with two layers of feedforward networks to predict the question type given Q at test stage. In the implicit question-type knowledge infusion, we used a multi-task learning (MTL) approach to simultaneously predict the labels and generate the question summary.

1. Explicit QTA Knowledge Infusion. In the explicit knowledge infusion method, we aim to integrate the question-types information directly into the question summarization network. We learn a weight matrix QE  R|C|×dh to encode the question-type information associated with a particular question-type in the summarization model. We augment the predicted jth question-type vector QE[j] with
10

the final hidden state hi  Rdh for the ith input token. Formally,

hEi = tanh(hi + QE[j])

(7)

where, hei  R2dh is question-type augmented hidden state for the ith token. Thereafter, we follow Eq. 4 to predict the masked token and use Eq. 5 to compute the
training loss for the question summarization model.

2. Implicit QTA Knowledge Infusion. In the first explicit knowledge infusion method, we utilized the pre-trained network to capture question-type semantics. However, two main challenges need to be addressed in the explicit question-type knowledge infusion: (1) its high dependency on the performance of the underlying classifier, and (2) the difficulty to learn the mapping function f : Q  C that maps a lengthy question (Q) having multiple sub-questions to a question type (C). To tackle these challenges, we attempt to model question-type knowledge infusion implicitly in the summarization model.
We propose a multi-task learning approach, where the model learns to recognize the question-type semantics via the auxiliary task of `Question-Type Prediction' along with the primary task of `Question Summarization'. We use the shared encoder from the MINILM to encode the consumer health questions. Formally, we classify each question into one of the seven aforementioned categories as:

hI = tanh(h0WI + bh)

(8)

CI = sof tmax(hIUI + bc)

The training on the `Question-Type Prediction' is performed by minimizing
the cross entropy loss (Lqtype). Similar to the explicit question-type knowledge
infusion (Eq. 7), we augment the learned question-type vector hI with the hidden state hi  Rdh for the ith input token and computed the final hidden state as hIi . We train the network by minimizing the following loss function:

Lmodel = Ls2s + Lqtype

(9)

4. Experimental Results & Analysis
4.1. Datasets We used the summarization dataset of consumer health questions MEQSUM,
consisting of 1, 000 consumer health questions and their corresponding summaries. We followed the same data augmentation method as Ben Abacha and DemnerFushman [4], and added a set of 4, 655 pairs of clinical questions asked by family

11

doctors and their short versions collected from Ely et al. [12]. We performed all the experiments5 with 5, 155 training pairs and 500 test samples pairs.
For question-type identification, we used a dataset of consumer health questions [17] that contains two types of questions: CHQA-web (MedlinePlus6 queries) and CHQA-email (questions received by the U.S. National Library of Medicine7), consisting of 23 and 31 question types, respectively.
4.2. Hyper-parameters We use a beam search algorithm with a beam size of 5 to decode the summary.
We train all summarization models on the training dataset for 20, 000 steps, with a batch size of 16. We set the maximum question and summary length to 100 and 20, respectively. To update the model parameters, we use the Adam [18] optimization algorithm with a learning rate of 7e - 5. The hidden size of MINILM is 384; we set the hidden vector size to 384 in all experiments. We use a linear learning rate decay schedule, where the learning rate decreases linearly from the initial learning set in the optimizer to 0. To avoid the gradient explosion issue, the gradient norm was clipped within 1.
4.3. Baseline Models We experimented with several competitive baseline models: Seq2Seq, Pointer
Generator, and pretrained transformers models (Transformers, Presumm, T5, PEGASUS, BART, MiniLM) as shown in Table 2. Below we explain each of the baseline models in detail.
· Seq2Seq [38]: This is the first baseline on summarizing consumer health questions. We use a one-layer LSTM with a hidden dimension of 512 for both the encoder and the decoder.
· Seq2Seq + Attention [3]: This baseline is the extension of the Seq2Seq model with the attention mechanism.
· Pointer Generator (PG) [36]: This model extends the Seq2Seq + Attention baseline model with the copy mechanism to handle the rare and unknown words.
5Wherever we refer to the MEQSUM dataset, we refer to the augmented dataset having 5, 155 training pairs and 500 test samples.
6https://medlineplus.gov/ 7https://www.nlm.nih.gov/
12

Baselines Baselines-Transformers
Proposed Method-I (QFA) Proposed Method-II (QTA)

Models
Seq2Seq [38] Seq2Seq + Attention [3] Pointer Generator (PG) [36]
SOTA [4] Transformer [39]
PreSumm [25] T5 [35]
PEGASUS [42] BART [22]
MINILM [40] (M1) Multi-Cloze Fusion (M2) Explicit QTA Knowledge-Infusion (M3) Implicit QTA Knowledge-Infusion (M4)

ROUGE-1 F1
25.28 28.11 32.41 40.00 25.84 26.24 38.92 39.06 42.30 43.13 44.58 45.20 44.44

ROUGE-2 F1
14.39 17.24 19.37 24.13 13.66 16.20 21.29 20.18 24.83 26.03 27.02 28.38 26.98

ROUGE-L F1
24.64 27.82 36.53 38.56 29.12 30.59 40.56 42.05 43.74 46.39 47.81 48.76 47.66

Table 2: Performance comparison of the proposed models and various baseline models on the MeQSum dataset. The highlighted rows represent the best results obtained by the proposed method (M3).

· PG + Coverage [4]: We also compare our proposed model with the current state-of-the-art on consumer health question summarization. This model is the extension of the PG model with the coverage mechanism [36] to avoid the generation of repeated words in the summary.
· Transformer [39]: We use the Transformer encoder-decoder architecture to train the network with the training dataset presented in Section 4.1. Both of our encoder and decoder consist of six Transformer blocks.
· PreSumm [25]: We use the PreSumm architecture proposed in [25] to train the question summarization network. It is a standard encoder-decoder framework for abstractive summarization, where the encoder is a pretrained BERTSUM [25] and the decoder is a 6-layered Transformer.
· T5 [35]: This is another pre-trained model developed by exploring the transfer learning techniques for natural language processing (NLP) by introducing a unified framework that converts all text-based language problems into a text-to-text format. The T5 model is an encoder-decoder Transformer with some architectural changes as presented in detail in [35].
· PEGASUS [42]: It is a large Transformer-based encoder-decoder model pre-trained on massive text corpora with a novel self-supervised objective, called Gap Sentences Generation, specially designed to pre-train the Transformer model for abstractive summarization. The important sentences from

13

the document are masked and are generated together as one output sequence from the remaining sentences of the document.
· BART [22]: It is a denoising autoencoder model for pre-training sequenceto-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning to reconstruct the original text. We fine-tuned the BART model on the training dataset presented in Section 4.1.
· MINILM [40]: We compare our proposed model to the Transformer-based pre-trained model for summarization, which has shown promising results on abstractive summarization and question generation tasks. For the Transformer based model, we fine-tune MINILM on the MEQSUM dataset. Thereafter, the fine-tuned MINILM model is used to summarize the consumer health questions.
4.4. Results We evaluated the performance of the summarization models using the stan-
dard ROUGE metric [24] and reported the ROUGE-1, ROUGE-2, and ROUGEL8. Table-2 provides an overview of the results, which demonstrates that both the `QFA' and the `QTA' models perform better than all the baseline models including the transformer-based pretrained networks. Our proposed `QFA' model achieves a ROUGE-L (F1) score of 47.81%, which is 1.42% better than the MINILM model. Similarly, our proposed `QTA' model achieves a ROUGE-L (F1) score of 48.76%, which is 2.37% better than the MINILM model. In comparison to the model developed by [4] on the MEQSUM dataset, we obtained a substantial improvement of 9.25% and 10.2% with the `QFA' and `QTA' models, respectively. We further provide an ablation analysis (in Table 3) of the question-focus aware summarization model by training the network only with a particular Cloze task.
4.5. Discussion The results support two important claims: (1) Infusing the knowledge of ques-
tion types leads to more informative question summaries, and (2) Awareness of salient medical focus words in the learning process enables the generation of relevant summaries. The ablation analysis reveals that the system trained with the Cloze Task 1 objective performs comparatively better than the rest of the objectives. It can be explained by the fact that the Cloze Task 1 objective is to mask
8We use the following ROUGE implementation: https://pypi.org/project/pyrouge/0.1.3/.
14

a word from the summary sentence, whereas in the second and third Cloze Tasks the phrases are masked. It is more challenging to predict the entire masked phrase compared to a word. However, the fusion of all the Cloze task objectives complements each other and improves the performance of the summarization system in terms of ROUGE-L (F1) score (47.81).

Models
Cloze Task 1 Cloze Task 2 Cloze Task 3

ROUGE-1 F1
43.81 43.13 44.58

ROUGE-2 F1 26.6 26.7
27.02

ROUGE-L F1
46.97 46.14 47.81

Table 3: Ablation study of various Cloze tasks on the MeQSum dataset.

We also extend our studies to analyze the effect of various question-type knowledge sources. Towards this, we utilized the Consumer Health Question Answering (CHQA) datasets created by Kilicoglu et al. [17]. The CHQA dataset provides the detailed annotations for the question focus and question types. It consists of two sub-datasets, CHQA-web (MedlinePlus9 queries) and CHQA-email (questions received by the U.S. National Library of Medicine10). The CHQA-web has 23 different question types, while CHQA-email has 31 distinct question types.
We performed an experiment to analyze the effect of different question types obtained from training the Question Type Identification model (BERTLARGE) on CHQA-web and CHQA-email datasets. The results shows that fine-tuning the (BERTLARGE) model with heuristically predicted question-types performs better than CHQA-web and CHQA-email dataset with implicit QTA summarization model. This may be because, both CHQA-web and CHQA-email contain very few samples (874 samples for CHQA-web and 554 samples for CHQA-email) for finetuning.
We also perform an upper-bound analysis by replacing the predicted question type with the gold-standard question type for the question set of the MEQSUM dataset. We reported the performance of the summarization model with the goldstandard question type in Table 4. The system achieves a ROUGE-L score of 54.03, which strengthens our hypothesis that knowing the question type benefits the summarization model.

9https://medlineplus.gov/ 10https://www.nlm.nih.gov/

15

Question Types Source
CHQA-web CHQA-email Heuristically-predicted Question Type Gold-standard question type (upper bound)

ROUGE-1 F1
44.06 45.02 45.20 50.45

ROUGE-2 F1
27.29 27.27 28.38 32.82

ROUGE-L F1
47.39 48.01 48.76 54.03

Table 4: Performance comparison of the `Question-type aware summarization model' on different question types from the CHQA-web and CHQA-email datasets.

4.6. Evaluation of the Proposed Summarization Model for Question Answering
In addition to our experiments on question summarization, we also performed experiments on the QA task to assess whether the summarized questions generated from the proposed summarization model can help improving the QA performance. Towards this, we utilize the LiveQA 2017 test dataset [2], that consists of 104 medical questions from the National Library of Medicine (NLM). The QA task consists of retrieving a correct answer for each medical question.
We generated the summaries of the test questions using our proposed summarization approach (Model M3) and developed a QA method based on an Information Retrieval (IR) model to retrieve relevant/similar questions and their associated answers from a collection of question-answer pairs. We utilized the search engine Terrier [31] and the MedQuad collection [5] to retrieve relevant answers for the summarized and original questions. The MedQuad collection contains 47k medical questions and their answers. We indexed the MedQuad questions and used the TF-IDF model to retrieve questions that are relevant to the original/summarized test questions.
We evaluated the answers of the retrieved questions for the LiveQA test questions using the publicly available judgments11. For the answers that were not judged earlier (25 answers), two annotators (a medical doctor and a medical informatics expert) evaluated them manually and provided their judgement scores following the LiveQA'17 reference answers [2]. For a fair comparison, we also used the same judgment scores as established by the LiveQA shared task: "Correct and Complete Answer" (4), "Correct but Incomplete" (3), "Incorrect but Related" (2) and "Incorrect" (1).

11github.com/abachaa/MedQuAD(QA-TestSet)

16

QA Results. To evaluate the QA performance using the original vs. summarized questions, we utilized the evaluation metrics proposed by the LiveQA shared task (these metrics evaluate the first retrieved answer for each test question):

· avgScore(0-3): the average score over all questions, transferring 1-4 level grades to 0-3 scores. This is the main score to rank the LiveQA systems.
· succ@k: the number of questions with score k or above (k= {2, 3, 4}) divided by the total number of questions.
· prec@k: the number of questions with score k or above (k= {2, 3, 4}) divided by the number of questions answered by the system.

Table 5 presents the results obtained by the IR-based QA system using: (i) the original questions, (ii) the automatically summarized questions by our proposed approach, and (iii) manually created reference summaries as reported in [1].

Measures
avgScore(0-3) succ@2+ succ@3+ succ@4+ prec@2+ prec@3+ prec@4+

Original Questions
0.673 0.403 0.201 0.067 0.424 0.212 0.07

Summarized Questions (using our proposed approach)
0.875 0.567 0.23 0.076 0.567 0.23 0.076

Reference Summaries (Manual) 1.125 0.663 0.317 0.144 0.663 0.317 0.144

Table 5: Evaluation of the answers retrieved using the original, automatic, and manual summaries based on the LiveQA metrics.

The results show that summarizing the consumer health questions can improve the performance of the IR/QA system in retrieving relevant answers. We also observe that the performance of the IR/QA model using the automatically summarized questions by our proposed approach is close to the performance achieved using the manually created reference summaries.

4.7. Manual Evaluation
To study the correlation between the automatic (ROUGE-1) and human evaluations in question summarization, we randomly selected 50 questions from the test set and manually evaluated the summaries generated by the best baseline model (M1) and the three proposed models (M2, M3, and M4). Three annotators experts

17

Method
M1 M2 M3 M4

Evaluation

Manual Automatic

24

42.02

46

53.64

38

49.65

42

51.20

Error Distribution E1 E2 E3 E4 8.19 8.19 31.14 42.62 11.32 9.43 28.30 26.41 16.66 7.40 29.62 22.22 20.68 8.62 31.03 17.24

Table 6: Comparison between the manual and automatic (ROUGE-1) evaluations and the error distributions of the generated summaries on 10% of the test set. All numbers are in percentage (%).

in medical informatics and medicine performed the analysis on the randomly selected 50 questions.
We used three scores: 0 (incorrect summary), 1 (acceptable summary), and 2 (perfect summary) to judge the correctness of the generated summaries. For each model, we aggregate the scores assigned to each generated summary. We present the normalized manual evaluation scores in Table 6. We also compute the interannotator agreement (IAA) using the Fleiss' kappa [14] and report the agreement of 72.67, 71.09 and 67.39 for model M2, M3 and M4 respectively.
The manual evaluation reveals a significant decline in performance of the model (M1) (18 points less, on average) over the other proposed models, while the decline in performance according to ROUGE was only 1.5 points less. Our analysis also shows that the proposed question focus-aware summarization model (M2) can generate more relevant and factually correct summaries due to its better coverage in identifying the question focus. We also observed that model (M3) with the explicit knowledge of the question types is more capable of generating near perfect summaries than the other models.
4.8. Error Analysis
We carried out an in-depth analysis of the generated summaries by the models M1, M2, M3, and M4. Table 7 presents examples of generated errors by the summarization models. The error distribution for each model is also reported in Table 6. We identified different sources of errors as follows:
· E1 - Partial Question Types. This is one of the common errors made by the models. If there are more than one question type in the original question, the models fail to generate the complete summary covering all the questions types.
18

· E2 - Semantic Inconsistency. We observe that the models sometimes fail to understand the semantic meaning of the focus words, and therefore generate wrong question types for a given focus category. For example: "What is a treatment for Cetrazine". "Cetrazine" is a drug name, the model misunderstood it as a medical problem.
· E3 - Incorrect Question Type. Our analysis on question-type aware model (M3) reveals that though our model handles the question types correctly to certain extent, there are some instances where due to the lack of model inference capability, the models generate summaries with wrong question types.
· E4 - Partial or Incorrect Focus Words. We observe that sometimes models fail to identify the exact focus words leading to incorrect summaries. For example, in Table-7 Question #3, the model only identified `poison' instead of `mercury poisoning'.

Question #1
Reference Error-Type1 Error-Type2
Question #2
Reference Error-Type3
Question #3
Reference Error-Type4

ClinicalTrials.gov - Question - specific study. my son is 17 years old he has omental panniculitis (weber cristian disease) can your institute help me to suggest what is this disear and how it care if you need i can send the full detail Where can I find information on weber christian disease, including treatment for it? where can i find information on weber christian disease ? what is the latest research on weber christian disease ?
SUBJECT: pregnancy MESSAGE: I just want to ask if have some ways or have a operation to be pregnant a woman like me had a tubal ligation before 7yrs. Ago. Where can I find information on tubal ligation reversal? what are the treatments for tubal ligation ?
MESSAGE: Hello, about 9 years ago I was poisoned and I didn't know what it was that was fed to me until a month ago. It was mercury and I'm losing my vision among other illnesses and don't know where to turn for help. What are the treatments for mercury poisoning? what are the treatments for poison in a 9 - years - old ?

Table 7: Examples of various types of errors in the summaries generated by the M1-4 models.

19

5. Conclusion
In this work, we tackle the summarization of long and complex consumer health questions. We propose question-focus and question-type aware summarization models that are able to generate relevant and succinct summaries of the original questions. Experiments show that our proposed model achieves stateof-the-art performance on the MEQSUM dataset outperforming various encoderdecoder and existing pretrained transformer-based summarization models. We also explore the use of the automatically generated summaries in an IR-based QA system and find that the automatic summaries improve the QA performance. The two proposed question summarization approaches are independent in terms of knowledge acquisition for question summarization. In the future, we aim to investigate methods and architectures allowing to combine both approaches to further improve the summarization of consumer health questions.
Acknowledgments
This work was supported by the intramural research program at the U.S. National Library of Medicine, National Institutes of Health.
References
[1] Asma Ben Abacha and Dina Demner-Fushman. On the role of question summarization and information source restriction in consumer health question answering. AMIA Summits on Translational Science Proceedings, 2019: 117, 2019.
[2] Asma Ben Abacha, Eugene Agichtein, Yuval Pinter, and Dina DemnerFushman. Overview of the medical question answering task at trec 2017 liveqa. In TREC, 2017.
[3] Dzmitry Bahdanau, Kyung Hyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In 3rd International Conference on Learning Representations, ICLR 2015, 2015.
[4] Asma Ben Abacha and Dina Demner-Fushman. On the summarization of consumer health questions. In Anna Korhonen, David R. Traum, and Llu´is Ma`rquez, editors, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages
20

2228­2234. Association for Computational Linguistics, 2019. URL https://doi.org/10.18653/v1/p19-1215.

[5] Asma Ben Abacha and Dina Demner-Fushman.

A

question-entailment approach to question answering.

BMC Bioinform., 20(1):511:1­511:23, 2019.

URL

https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859

[6] Yen-Chun Chen and Mohit Bansal. Fast abstractive summarization with reinforce-selected sentence rewriting. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 675­686, 2018.

[7] Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian. A discourse-aware attention model for abstractive summarization of long documents. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 615­621, 2018.

[8] Ariel Deardorff, Kate Masterton, Kirk Roberts, Halil Kilicoglu, and Dina Demner-Fushman. A protocol-driven approach to automatically finding authoritative answers to consumer health questions in online resources. J. Assoc. Inf. Sci. Technol., 68(7):1724­1736, 2017. URL https://doi.org/10.1002/asi.23806.

[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171­4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://www.aclweb.org/anthology/N19-1423.

[10] Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. Unified language model pre-training for natural language understanding and generation. In Advances in Neural Information Processing Systems 32, pages 13063­13075. Curran Associates, Inc., 2019.

21

[11] Yue Dong, Yikang Shen, Eric Crawford, Herke van Hoof, and Jackie Chi Kit Cheung. Banditsum: Extractive summarization as a contextual bandit. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3739­3748, 2018.
[12] John W Ely, Jerome A Osheroff, Paul N Gorman, Mark H Ebell, M Lee Chambliss, Eric A Pifer, and P Zoe Stavri. A taxonomy of generic clinical questions: classification study. Bmj, 321(7258):429­432, 2000.
[13] Tobias Falke, Leonardo FR Ribeiro, Prasetya Ajie Utama, Ido Dagan, and Iryna Gurevych. Ranking generated summaries by correctness: An interesting but challenging application for natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2214­2220, 2019.
[14] Joseph L Fleiss. Measuring nominal scale agreement among many raters. Psychological bulletin, 76(5):378, 1971.
[15] Ben Goodrich, Vinay Rao, Peter J Liu, and Mohammad Saleh. Assessing the factual accuracy of generated text. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 166­175, 2019.
[16] Luyang Huang, Lingfei Wu, and Lu Wang. Knowledge graph-augmented abstractive summarization with semantic-driven cloze reward. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5094­5107, 2020.
[17] Halil Kilicoglu, Asma Ben Abacha, Yassine Mrabet, Sonya E Shooshan, Laritza Rodriguez, Kate Masterton, and Dina Demner-Fushman. Semantic annotation of consumer health questions. BMC bioinformatics, 19(1):34, 2018.
[18] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980.
[19] Wojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. Evaluating the factual consistency of abstractive text summarization. In Pro-
22

ceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9332­9346, 2020.
[20] Philippe Laban, Andrew Hsi, John Canny, and Marti A Hearst. The summary loop: Learning to write abstractive summaries without examples. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, volume 1, 2020.
[21] Logan Lebanoff, John Muchovej, Franck Dernoncourt, Doo Soon Kim, Lidan Wang, Walter Chang, and Fei Liu. Understanding points of correspondence between sentences for abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 191­198, 2020.
[22] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871­7880, 2020.
[23] Yuan Li, Xiaodan Liang, Zhiting Hu, and Eric P Xing. Hybrid retrievalgeneration reinforced agent for medical image report generation. In Advances in neural information processing systems, pages 1530­1540, 2018.
[24] Chin-Yew Lin. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization Branches Out: Proceedings of the ACL-04 workshop, volume 8. Barcelona, Spain, 2004.
[25] Yang Liu and Mirella Lapata. Text summarization with pretrained encoders. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3730­3740, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1387. URL https://www.aclweb.org/anthology/D19-1387.
[26] Sean MacAvaney, Sajad Sotudeh, Arman Cohan, Nazli Goharian, Ish Talati, and Ross W Filice. Ontology-aware clinical abstractive summarization. In
23

Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 1013­1016, 2019.
[27] Ramesh Nallapati, Bing Xiang, and Bowen Zhou. Sequence-tosequence rnns for text summarization. In International Conference on Learning Representations, Workshop track, 2016. URL https://pdfs.semanticscholar.org/033b/c4febf590f6e011e9b0f497cadfe6
[28] Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, Caglar Gulcehre, and Bing Xiang. Abstractive text summarization using sequence-to-sequence RNNs and beyond. In Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning, pages 280­290, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/K16-1028. URL https://www.aclweb.org/anthology/K16-1028.
[29] Ramesh Nallapati, Bowen Zhou, C´icero Nogueira dos Santos, C¸ aglar Gu¨lc¸ehre, and Bing Xiang. Abstractive text summarization using sequence-to-sequence rnns and beyond. In Yoav Goldberg and Stefan Riezler, editors, Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning, CoNLL 2016, Berlin, Germany, August 11-12, 2016, pages 280­290. ACL, 2016. URL https://doi.org/10.18653/v1/k16-1028.
[30] Shashi Narayan, Shay B Cohen, and Mirella Lapata. Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1797­1807, 2018.
[31] I. Ounis, G. Amati, V. Plachouras, B. He, C. Macdonald, and C. Lioma. Terrier: A High Performance and Scalable Information Retrieval Platform. In Proceedings of ACM SIGIR'06 Workshop on Open Source Information Retrieval (OSIR 2006), 2006.
[32] Ramakanth Pasunuru and Mohit Bansal. Multi-reward reinforced summarization with saliency and entailment. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 646­653, 2018.
24

[33] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. In International Conference on Learning Representations, 2018.
[34] Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Andrew Rowley, Hong-Woo Chun, Sung-Jae Jung, Sung-Pil Choi, Jun'ichi Tsujii, and Sophia Ananiadou. Overview of the cancer genetics and pathway curation tasks of bionlp shared task 2013. BMC bioinformatics, 16(S10):S2, 2015.
[35] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21:1­67, 2020.
[36] Abigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization with pointer-generator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073­1083. Association for Computational Linguistics, 2017. doi: 10.18653/v1/P17-1099. URL http://aclweb.org/anthology/P17-1099.
[37] Sajad Sotudeh, Nazli Goharian, and Ross W Filice. Attend to medical ontologies: Content selection for clinical abstractive summarization. arXiv preprint arXiv:2005.00163, 2020.
[38] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems 27, pages 3104­3112. Curran Associates, Inc., 2014.
[39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998­ 6008, 2017.
[40] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: Deep self-attention distillation for task-agnostic compression of pretrained transformers. arXiv preprint arXiv:2002.10957, 2020.
[41] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus
25

Macherey, et al. Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016. [42] Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. Pegasus: Pretraining with extracted gap-sentences for abstractive summarization. In International Conference on Machine Learning, pages 11328­11339. PMLR, 2020. [43] Yuhao Zhang, Daisy Yi Ding, Tianpei Qian, Christopher D Manning, and Curtis P Langlotz. Learning to summarize radiology findings. In Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis, pages 204­213, 2018. [44] Yuhao Zhang, Derek Merck, Emily Tsai, Christopher D Manning, and Curtis Langlotz. Optimizing the factual correctness of a summary: A study of summarizing radiology reports. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5108­5120, 2020.
26

