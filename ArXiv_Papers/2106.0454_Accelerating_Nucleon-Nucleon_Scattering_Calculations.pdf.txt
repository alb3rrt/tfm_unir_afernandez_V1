Accelerating Nucleon-Nucleon Scattering Calculations
Sean B. S. Miller, Andreas Ekstr¨om, and Christian Forss´en Department of Physics, Chalmers University of Technology, SE-412 96 Gothenburg, Sweden
(Dated: June 2, 2021)
In this paper we analyse the efficiency, precision, and accuracy of computing elastic nucleonnucleon (NN) scattering amplitudes with the wave-packet continuum discretisation method (WPCD). This method provides approximate scattering solutions at multiple scattering energies simultaneously. We therefore utilise a graphics processing unit (GPU) to explore the benefits of this inherent parallelism. From a theoretical perspective, the WPCD method promises a speedup compared to a standard matrix-inversion method. We use the chiral NNLOopt interaction to demonstrate that WPCD enables efficient computation of NN scattering amplitudes provided one can tolerate an averaged method error of 1 - 5 mb in the total cross section. For scattering energies 40 MeV, in the laboratory frame of reference, we find a much smaller method error of 1 mb. By increasing the number of wave-packets we can further reduce the overall method error. However, the parallel leverage of the WPCD method will be offset by the increased size of the resulting discretisation mesh. In practice, the GPU implementation is only useful for matrices that fit in the fast on-chip shared memory. Nevertheless, we find that WPCD is a promising method for computationally efficient, statistical analyses of EFT nuclear interactions, where we can utilise Bayesian inference methods to incorporate relevant uncertainties.

arXiv:2106.00454v1 [nucl-th] 1 Jun 2021

I. INTRODUCTION
Low-energy nucleon-nucleon (NN) scattering crosssections constitute a bulk part of the standard dataset for inferring the most probable values for the parameters in ab initio models of the nuclear interaction, see e.g. Refs. [1­4]. The most recent, and statistically consistent, database [5] of NN scattering cross sections contains data for thousands of measured proton-proton (pp) and neutron-proton (np) cross sections at hundreds of different laboratory scattering energies, mostly below the pion-production threshold at Tlab  290 MeV. Nowadays, most interaction-potential models applied in ab initio many-nucleon calculations are constructed using ideas from chiral effective field theory (EFT) [6­ 9]. Such potentials typically contain 10-30 low-energy constants (LECs) acting as physical calibration parameters. Bayesian methods for parameter estimation, see e.g Ref. [10], typically incur a higher total computational cost compared to maximum likelihood methods. To better accommodate Bayesian inference methods, it is important to establish an efficient computational framework for generating model predictions of NN scattering observables. In this paper we explore graphics processing units (GPUs) for solving the Lippmann-Schwinger (LS) equation for two nucleons. In particular, we analyse the efficiency and accuracy of the wave-packet continuum discretisation (WPCD) method [11] for solving the LS equation in an inherently parallel fashion. This method basically corresponds to a bound-state approach that uses eigenfunctions of the full NN Hamiltonian to approximate scattering solutions at any on-shell energy.
There are essentially three approaches for improving computational efficiency and speeding up computations: i) develop improved numerical methods and algorithms tailored to the physical model at hand and its application, ii) use specific hardware, e.g. a faster CPU, increased

memory bandwidth, or parallel architectures such as in a GPU to better handle some of the main computational bottlenecks of the model, or iii) replace any computationally expensive model evaluations with a fast, as well as sufficiently accurate and precise, surrogate model, i.e. an emulator, which mimics the original model output. In this paper, we investigate approaches i) and ii) by exploring recent method developments for solving the LS equation using optimised code for CPU and GPU hardware. The third approach, efficient emulation, is an interesting and promising alternative, see e.g. [12] for recent developments.
Speeding up computations very often comes at the expense of accuracy and/or precision. The magnitude of experimental errors in the calibration data and the estimated theoretical model-discrepancy [13] provide tolerances for the level of method error that is acceptable. Indeed it is undesirable if the method error dominates the error budget such that it obscures or even hampers the inference of useful information. Here, we quantify realistic computational speedups for solving the LS equation for NN scattering while generating sufficiently precise results compared to recent estimates of the model discrepancy in EFT [14]. We also analyse and compare the numerical complexities of two different methods for solving the LS equation: i) the standard principle-value and matrixinversion method (MI), outlined in e.g. Ref. [15], and ii) the WPCD method [11].

II. NUCLEON-NUCLEON SCATTERING

The LS equation, in operator form, for the transitionmatrix operator T^ at some scattering energy E is given by

T^(E) = v^ + v^g^0(E)T^.

(1)

2

This is an inhomogeneous Fredholm integral equation where v^ is some NN-potential operator, g^0(E) = (E - h^0 + i )-1 is the free Green's function, and h^0 is the free Hamiltonian, i.e. the kinetic energy operator, and i  0 is the positive imaginary part of the complex energy. The standard MI method [15] amounts to inverting a relatively small complex-valued matrix at each scattering energy. This is a trivial operation that can be straightforwardly carried out within milliseconds on a modern CPU. However, solving at multiple scattering energies to obtain all cross-sections present in the NN database amounts to at least a few seconds of computation. In a Bayesian analysis, where one repeatedly evaluates a likelihood function across a multi-dimensional parameter domain, any speedup in the solution of the LS equation will have a significant impact on the total computation time.
The momentum-space partial-wave representation of the LS equation for the NN T -matrix, Eq. (1), is given by

TlsJl (q

,

q;

E)

=

vlsJl (q

, q)

+

2 

l


dk k2vlsJl (q , k)
0

×g0(k; E)TlsJl(k, q; E) , (2)

where q, q , and k are relative momenta, E = p2/mN is the centre-of-mass (c.m.) energy with momentum p, mN is the nucleon mass, and g0(k; E)  (E - k2/mN + i )-1. In this work we only consider the canonical NN inter-

action potential, and therefore use the shorthand notation TlsJl (q , q)  q , l , s, J|T^|q, l, s, J for partial-wave amplitudes. Furthermore we use the following normali-

sation of momentum states,

k |k

=

(k k

-k) k

.

We sup-

press isospin notation since the total isospin T is defined

uniquely by the Pauli principle given the spin and angu-

lar momentum quantum numbers s and l, respectively,

while the projected isospin Tz is defined at the outset of, and conserved throughout, a scattering observable calcu-

lation. Methods for obtaining the partial-wave-projected

potential vlsJl (q , q) can be found in e.g. Ref. [16]. Integrated and differential scattering cross sections at a spe-

cific scattering energy E can be straightforwardly eval-

uated given the partial-wave T -matrix using the expres-

sions presented in Appendix A.

For separable potentials one can obtain analytical so-

lutions to the LS equation. However, most realistic NN

potentials in nuclear ab initio calculations do not furnish

analytical expressions for the T -matrix. It is however

straightforward to numerically solve for the T -matrix

at some energy E. There exists e.g. variational meth-

ods [17­20], as well as Neumann or Born series expan-

sions for sufficiently weak potentials. One can also try

Pad´e extrapolants [21] in cases where the integral kernel

is not sufficiently perturbative to converge the resulting

Neumann series. Throughout this work we will compare

the efficiency and accuracy of the WPCD method, pre-

sented in Sec. III B, to a set of numerically exact results

obtained using the MI method presented below.

A. Matrix-inversion method for solving the Lippmann-Schwinger equation

For the resolvent g^0(E), the kernel in the LS equation
(2) has a pole singularity at k = p. This can be handled via a principal-value decomposition1, i.e.

lim
0

x

1 ±

i

=P

1 x

 i(x) ,

x  R,

(3)

such that the remaining integral can be evaluated using e.g. Gauss-Legendre quadrature on some grid {ki}Ni=Q1 of momenta ki = p with corresponding weights wi. Following [15], the complex T -matrix in Eq. (2) can be solved
via the inversion of a finite-dimensional matrix equation for the on-shell momentum q = p = mN E,

TlsJl (q,

q;

E)

=

vlsJl (q, q)

+

2 

l

NQ
wivlsJl (q, ki)
i=1
× g0(ki; E)TlsJl(ki, q) . (4)

We can introduce a basis qi  {k1, k2, . . . , kNQ , p} such that the operators can be written in matrix form with row
i and column j corresponding to qi and qj respectively, i.e. (VlslJ )ij  qi|vlsJl |qj . The on-shell T -matrix element is then given by (TlsJl )NQ+1,NQ+1. Introducing a vector D with elements defined as

Di



 2wiki2mN

 (ki2

-p2 NQ

)

2wi

p2

mN

- i=1 (ki2-p2)

+

ipmN 2

if i  NQ , if i = NQ + 1 ,
(5)

Eq. (4) can be rewritten as a linear system of equations,

FlsJl TlsJl = VlslJ ,

(6)

l

where we have introduced the wave matrix FlsJl ,

(FlsJl )ij  ij l l - (VlslJ )ij Dj .

(7)

Direct inversion of FlsJl in Eq. (6) is usually discouraged in scientific computing due to the instability of matrix inversion algorithms [22]. A more advisable practice is to use LU-decomposition. Additionally, it is numerically more stable to first introduce the K-matrix2 as the principal value part of the LS equation. Since the potential v is real, the K-matrix is also real. This leads

1 Kramers-Konig relation, dispersion relation, or the Sokhotski-
Plemelj identity. 2 Also referred to as the reactance R-matrix.

3

to a set of purely real matrix-equations [15] based on a nonsingular integral.
The on-shell momentum dependence in FlsJl demands the solution of an entire linear system of equations for every energy of interest. Formally, the T -matrix is defined by the potential operator via [23]

TlsJl (q , q)  q |v^lsJl |q+ ,

(8)

where |q+ are eigenstates (outbound scattering states) of the full Hamiltonian h^ with momentum q. With this,
we can instead express the LS equation using the full resolvent g^(E) = (E - h^ + i )-1,

TlsJl (q

,

q; E)

=

vlsJl (q

,

q)

+

2 

l


dk k2vlsJl (q , k+)
0
×g(k; E)vlsJl(k+, q) , (9)

where k2/mN

vlsJl +i

()q-,1. k+T)hi=s is

q |v^lsJl |k+ and g(k; significantly easier to

E) = (E - evaluate nu-

merically using quadrature since it only amounts to ma-

trix multiplications. However, the scattering states |q+ are not available at the outset. This is where the WPCD

method enters to effectively approximate the scatter-

ing states using square-integrable eigenstates of the NN

Hamiltonian.

III. WAVE-PACKET CONTINUUM DISCRETISATION
The WPCD method [11] effectively eliminates the requirement to explicitly solve the LS equation at several scattering energies E using matrix inversion. Instead, one diagonalises the full Hamiltonian in a finite basis, and uses the resulting discrete set of eigenstates to approximate all scattering states of interest. Equipped with these states, this approach enables straightforward evaluation of the full resolvent at any value of the on-shell scattering energy E, which makes the WPCD method intrinsically parallel with respect to obtaining scattering solutions at different energies. This is one of several known bound-state techniques to solve the multi-particle scattering problem [24]. The WPCD method is similar to the Kohn variational principle [17­19], although the variational parameters are of different origin. To provide a self-contained presentation, we devote this section to introduce the WPCD method for describing elastic NN scattering, starting with a definition of a finite wavepacket basis.

A. Scattering observables in a finite basis
Generally we can project some Hamiltonian state |(E) with positive energy E onto a complete basis

(including both bound and free basis states). In this case, the expectation value of an operator O^(h^) depending purely on the full Hamiltonian h^ can be represented
in the following form,

nb
(E)|O^|(E) = u( bi )| |ib |2

i=1

(10)



+ dE u(E )| |(E ) |2 ,

0

where {|ib }ni=b1 are bound states with energies

b i

and

|(E ) are free states with energy E , both of which are

eigenstates of h^, and we have defined u( bi ) = ib|O^|ib and u(E ) = (E )|O^|(E ) . Naturally, we have

|(E) = |(E) (for E > 0) as h^ is the Hamiltonian and

 the eigenstate, collapsing the expansion above. How-

ever, this identity is not useful in numerical approaches

since we do not know (E), leaving us to solve the inte-

gral in a finite, approximative basis.

A computational routine for evaluating integrals, such

as quadrature, uses a finite mesh of points where the

integrand is evaluated. In scattering, this approach re-

quires a finite basis of states |i with corresponding positive energies Ei. These states do not span the whole continuous momentum-space and are thus referred to as

pseudostates. Below, we will demonstrate how to con-

struct the pseudostates in a wave-packet basis. The pseu-

dostates form a basis for a finite quadrature-prescription

to express an operator

nb

n

|O^(h^)| 

u(

b i

)|

|ib

|2

+

u(Ei)| |i |2 ,

i=1

i=1

(11)

where we introduce quadrature weights wi such that

| |i |2 = wi| |(Ei) |2 .

(12)

To determine the weights wi we can use an equivalent quadrature (EQ) technique [25­29] in which it can be shown that the weights do not depend on the state | . The weights represent a type of transformation coeffi-
cient between pseudostates |i and the states |(E ) . An approximate relation based on Eq. (12) can then be introduced,

(Ei)|O^|(Ei)  i|O^w|ii .

(13)

In the WPCD method we discretise the continuum of free states. A wave packet is defined as the energy integral over some energy "bin" of width E with Hamiltonian eigenstates |(E) with positive energy E as the integrand,

E+E

|(E, E) 

dE |(E ) .

(14)

E

4

It follows that limE0 |(E, E) = |(E) . We can

define an orthogonal wave-packet basis by letting the bin

boundaries E and widths E lie on a mesh such that the bins do not overlap: {Di | Di  Dj =   i = j}Ni=W1P , where Di  [Ei, Ei + Ei] defines the integral bound-

aries. Note that Ei+1 = Ei + Ei. It is straightforward

to normalise this basis and show that the wave packets

have

eigenenergies

ei

=

Ei

+

1 2

Ei.

The expectation value of an operator O^ in some energy

bin can now be approximately represented in a wave-

packet basis

(E)|O^|(E )  (E)|O^|(Ei, Ei) ,

(15)

Ei

where E  Di. As expected, the quality of this approximation is subject to the bin widths Ei. It can be reasoned [30­33], on behalf of Eqs. (13) and (15), that
the EQ weights are approximately given by

wi  Ei .

(16)

In short, we have a method for approximating the EQ weights in a wave-packet basis, with which we can express the spectrum of a scattering operator and thereby effectively solve scattering problems.
We now proceed to approximately represent the pseudostates as Hamiltonian eigenstates in a wave-packet representation. To this end, we setup a wave-packet equivalent of a partial wave by generalising Eq. (14) and thus define a normalised free wave-packet (FWP) as

|xi

 1 Ni

k dk f (k)|k ,
Di

(17)

where f (k) is a weighting function and Ni is a normalisation constant. We obtain energy or momentum
wave-packets using weighting functions f (k) = 1 or

f (k) =

k µ

,

respectively,

where

µ

=

mN 2

is the reduced

mass. As above, these two types of wave packets have

eigenvalues

h^0|xi =

Ei

+

1 2

Ei

|xi ,

(18)

p^|xi =

ki

+

1 2

ki

|xi ,

(19)

where p^ is the momentum operator. The normalisation of the momentum and energy wave-packets is given by the bin widths, i.e. Ni = ki or Ni = Ei, respectively. Operators represented in a basis of partial-wave planewave states are related to the FWP representation via

q |O^|q



f (q)f (q ) 1 NiNj q q

xi|O^|xj

,

(20)

where q  Di, q  Dj. In this work we have implemented energy as well as momentum wave-packets, and

have found no significant difference or advantage of either

choice.

The eigenstate wave-packets |zi of the full Hamilto-

nian with positive energy, or scattering wave packets

(SWPs), are given by the eigenvalue equation h^|zi =

i|zi for energies i > 0. Similarly, the bound-state3

wave-packets |zib

have energies

b i

<

0.

We obtain

them straightforwardly via diagonalisation in a finite ba-

sis {|xi }Ni=W1P of FWPs, where NWP = n + nb. This op-

eration is the most time-consuming part in the WPCD

method to solve the LS equation. It provides us with a

matrix of transformation coefficients Cij  xi|zj such

that

H = CDCT ,

(21)

where D is a diagonal matrix of energies

b i

and

i,

and Hij = xi|h^|xj . In order to solve the LS equa-

tion on the form of Eq. (9), we must define a wave-

packet representation of the outbound scattering states.

To define SWPs, we must construct the bin boundaries

[Ei, Ei + Ei] = [Ei, Ei+1] of |zi such that the positive

eigenenergies are given by

i

=

Ei

+

1 2

Ei

.

(22)

This is no different than the wave-packet eigenvalues in Eqs. (18) and (19). This shift of the FWP energies are indicated in the left panel in Fig. 1. However, it is not possible to construct the bin-boundaries Ei exactly since Eq. (22) only provides n equations while there are n + 1 boundaries. Therefore, the following scheme [34] can be used to approximate the bin boundaries of |zi ,

E1  0 ,

Ei



1 2

(

i-1

+

i) ,

(23)

En+1 

n

+

1 2

(En

-

En-1)

,

such that they yield approximate eigenvalues ¯i,

¯i



Ei

+

1 2

Ei



i.

(24)

In the case of K coupled channels, the FWP ener-

gies are degenerate and will be split4 in the full Hamil-

tonian eigendecomposition. A FWP with energy ei =

Ei +

1 2

Ei

will

give

rise

to

K

SWPs

|zi,

with en-

ergies i,, corresponding to each coupled state  =

1, 2, . . . , K. The energies are typically ordered such

that i, < i,+1  i, [36]. Furthermore, this prevents

3 Here we change bound-state notation from Eq. (11) such that
|kb  |zkb . This seems natural since |zkb is expressed in terms of FWPs from the diagonalisation.
4 Naturally, K  2 for NN scattering.

5

e
ei+5 ei+4 ei+3 ei+2 ei+1
ei

e

i+5

ei+5

i+4

ei+4

i+3

ei+3

ei+2

i+2

ei+1

i+1

i

ei

i+5,2 i+5,1 i+4,2 i+4,1 i+3,2 i+3,1 i+2,2
i+2,1 i+1,2 i+1,1 i,2 i,1

FIG. 1.

Left:

Shift

in

FWP

energies

ei



Ei +

1 2

Ei

to

SWP energies i. Right: Shift and splitting of degenerate

FWP energies ei into energies i,1 and i,2 shown by solid and

dashed lines, respectively. Note that a solid and a dashed line

from two different energies ei and ej will not cross [11],[35].

mixing between levels with different energies [11], i.e. i, < j,+1  i < j. Therefore, we use the boundary construction scheme in Eq. (23) such that |zi, have boundaries given by the energies i,. Note it is very important to construct the Hamiltonian matrix with degenerate FWP bases representing each coupled state, i.e. |xi,1 = |xi,2 , see the right panel of Fig. 1.
The approach presented so far works fine for shortrange NN potentials. Likewise, the eigenspectrum of the long-range Coulomb Hamiltonian can be straightforwardly described using wave packets, but these must then be constructed from Coulomb wave functions instead of free states |q [35] as in Eq. (17). In WPCD, we automatically "smooth" out the typical low-momentum singularities presented by the Coulomb Hamiltonian. This means that the formalism of WPCD works well for both the short- and long-range parts of the interaction, but it is necessary to treat them separately. In this work we have not studied Coulomb wave packets as we only consider neutron-proton scattering.

B. WPCD-method for solving the Lippmann-Schwinger equation

Following Eq. (20), we can relate elements of the T matrix in a continuous partial-wave basis and a FWP basis via

TlsJl (q , q; E) =

q |T^lsJl (E)|q

 f (q)f (q ) xi|T^lsJl (E)|xj NiNj qq

.

(25)

If we use the full resolvent g^(E), defined in connection

with Eq. (9), we can write

T^(E) = v^ + v^g^(E)v^ .

(26)

such that in a partial-wave-projected wave-packet basis we obtain

xi|T^lsJl (E)|xj

= xi|v^lsJl |xj

+
l

nb xi|v^lsJl |zkb zkb |v^lsJl|xj

k

E-

b k

n

+

xi|v^lsJl |zk

l k=1

× zk|g^(E)|zk zk|v^lsJl|xj . (27)

Note that for a realistic potential we should only have nb = 1 (the deuteron). The full resolvent is given by the full Hamiltonian, of which |zk are eigenstates. In such a basis we can derive a closed form expression for g^(E), see Appendix B. In the WPCD representation of Eq. (27) all energy-dependence is straightforwardly evaluated via the resolvent. We can therefore find an on-shell T -matrix element via simple summation of the scattering wavepackets. This is an important advantage of using the WPCD method for simulating scattering processes.
Note, however, that the resolvent has a logarithmic singularity for E = Ek or E = Ek+1. We handle this by averaging with respect to the on-shell energy E,

gik



1 Ek

zi|g(E)|zi dE .
Dk

(28)

The nuclear potential, represented in a free wavepacket basis as

xi|v^lsJl |xj =

1

k k dk dk f (k )f (k)

NiNj Di Dj

× k |v^lsJl |k , (29)

usually vary mildly across a typical momentum-bin Di. It is therefore often sufficient to use a midpoint approximation to evaluate the integral. This offers a significant reduction in computational cost. In a momentum wavepacket basis, the midpoint approximation is simply given by

xi|v^lsJl |xj  k¯ik¯j kikj k¯i|v^lsJl |k¯j , (30)

where

k¯i

=

ki +ki+1 2

are

the

bin

midpoints.

Here we summarise the necessary steps to implement the WPCD method for NN-scattering calculations.

1. Distribute the bin boundaries for the free wavepackets and choose a weighting function f (k), see Eq. (17). Recommended options are e.g. uniform (equidistant), Gauss-Legendre, or Chebyshev distributions. The results in this work are based on a Chebyshev distribution. The Chebyshev distribution for NWP points, {yj}Nj=W1P , is defined by [11]

yj =  tant

2j - 1 4NWP



, j = 1, . . . , NWP , (31)

6

where t is a "sparseness degree" and  is a scal-
ing parameter. Let yk be either the momentum or energy bin boundaries, and we then set the ini-
tial boundary y0 = 0. For our simulations we have used momentum wave packets (f (k) = 1), t = 2,
and  = 100 MeV.

2. Diagonalise the Hamiltonian in a free wave-packet basis. This yields a set of energy eigenvalues i and accompanying matrix of eigenvectors C as according to Eq. (21).

3. Construct bin-boundaries Ei for the pseudostate wave-packets using the set of energy eigenvalues
and Eq. (23).

4. Express the full resolvent g(E) in the scattering wave-packet basis, see Appendix B. In this work we use the energy-averaged resolvent from Eq. (28) to avoid singularities.

5. Obtain approximate T -matrix elements in the free wave-packet basis by evaluating the LS-equation

Tij (E) = Vij + (V C)ikgkk(E)(V C)jk ,

(32)

where (V C)ik  Vij Cjk, Vij  xi|v^|xj , gkk  zk|g^(E)|zk , and C is given by Eq. (21).

The T -matrix can be transformed to a continuous plane-wave basis using Eq. (20). However, it is important to note that with energy averaging there is ambiguity in which value q , q  Di to choose, as the wave-packet representation gives the same value for all choices. While we discuss this further in Section V A, we find it more beneficial to choose q , q such that they match the wave-packet eigenvalues, given in Eqs. (18)-(19).

IV. NUMERICAL COMPLEXITIES OF THE MI AND WPCD METHODS
Here we present the minimum number of floatingpoint operations (FLOP) required for computing on-shell q|TlsJl |q matrix elements at nE different values of the scattering energy. We focus on the MI and WPCD methods, presented in Secs. II A and III B, respectively. We also assume that the potential matrix p |VlslJ |p is precomputed and available in memory at the outset. Typically, in both methods we use matrices of sizes n × n for n < 100. To avoid confusion, we let NQ denote the number of quadrature points in the case of the MI method, while NWP denotes the number of wave packets in the case of the WPCD method. We retain n to symbolise a basis size in general, regardless of method.

each scattering energy. Naively, the complexity of the matrix construction is dominated by the matrix-matrix product of the potential V with the resolvent g0. Since the resolvent is diagonal, it is more efficient to multiply each row of V with the diagonal element of g0, yielding n + 1 scalar-vector multiplications in a single F -matrix construction with numerical complexity according to

O(F ) = 2(NQ + 1)2 + 2(NQ + 1) ,

(33)

where the factor of 2 is due to g0 being complex.

The on-shell T -matrix element is obtained from the last element of the T -matrix, i.e.

NQ +1

T (q, q; E) = TNQ+1,NQ+1 =

FN-Q1+1,iVi,NQ+1 , (34)

i=0

where we have expressed it using matrix inversion. Matrix inversion typically requires O(n3) operations for an
n × n matrix. However, solving the LS linear system,

FT = V ,

(35)

is more advisable, and can be done very efficiently in two steps: Firstly, we perform a lower-upper (LU) decomposition, where a square matrix A is expressed as the product of a lower-triangular matrix L with an upper-triangular matrix U ,

A = LU ,

(36)

which requires O

2 3

n3

operations. The decomposition

allows for AX = B to be solved for each column of X

using O(2n2) operations. For complex matrices these

numbers increase by a factor 4 for both routines.

In summary, the MI-method has a total computational complexity of (note the linear scaling with the number of on-shell energies nE)

OMI(T ) = 2nE

4 3

(NQ

+

1)3

+

5(NQ

+

1)2

+

NQ

+

1

,

(37)

OMI(K) = nE

2 3

(NQ

+

1)3

+

5(NQ

+

1)2

+

NQ

+

1

,

(38)

where OMI(K) shows the cost of using a K-matrix approach instead. The difference is simply replacing the cost of doing complex calculations with real calculations. We see there is roughly a factor 4 speedup in using a K-matrix representation instead of a T -matrix representation.

A. MI complexity

B. WPCD complexity

Given a quadrature grid with NQ points, the MI

The complexity of the WPCD-method is dominated

method first requires setting up the wave matrix (7) at by three kinds of linear algebra tasks i) the Hamiltonian

7

matrix diagonalisation, ii) two matrix-matrix products, and iii) one matrix addition (see steps 1-5 in Sec. III B). We let NWP be the number of wave packets and nE the number of scattering energies as before. By "parallelism" we refer to the fact that WPCD can solve the scattering problem for several energies at once, following a single Hamiltonian diagonalisation.
For this reason we have fully implemented the WPCD method on a GPU utilising the CUDA interface with the cuBLAS [37] and cuSOLVER [38] libraries for linear operations. A more detailed outline of the GPU code is provided in Appendix C. Sequential algorithms are usually easier to handle, and not all hardware allow for optimal parallelisation due to, for example, slow computer memory transfer. Thus, we present complexity models for both the sequential and parallel energy-evaluations of the WPCD method. Note, however, that this is simply a comparison of FLOP models that do not account for memory transfer times or detailed processor architecture. We emphasise that all WPCD results presented in this paper were calculated using our parallel GPU-code, and that the sequential complexity is presented purely to provide further insight.
The MI method can also be implemented efficiently on a GPU. This approach will not exhibit the same inherent parallelism with respect to multiple on-shell calculations. However, there does exist efficient parallel routines for solving linear systems on the form of Eq. (6).

1. Sequential complexity

The sequential mode for the WPCD method means solving the LS equation for each on-shell energy in sequence rather than simultaneously. This suggests using a CPU, rather than a GPU, since CPUs typically have a faster clock frequency and can thus iterate through the on-shell energies faster than a GPU. Of course, CPUs today are multicore and with several processing units, but note that they might not have a number of cores equal to or greater than nE. The analysis below assumes a scenario where a processing unit is working with a single computational core.

1. A real-valued NN Hamiltonian is efficiently diag-

onalised using QR factorisation or a divide-and-

conquer algorithm, both of which are usually used

together with Householder transformations. The

divide-and-conquer algorithm has a complexity of

O

8 3

n3

for getting both eigenvectors and eigenval-

ues of an n × n matrix.

2. Next, we evaluate the matrix-matrix product of the
potential matrix V and the coefficient matrix C in
the rightmost term in Eq. (32). Square matrixmatrix multiplication requires 2n3 - n2 FLOP in a
straightforward, sequential approach.

3. The LS equation for Tii(E  Di) is evaluated via simple summation and multiplication. The sum,

NWP
(V C)ijgjj(E  Di)(V C)ji ,
j=1

(39)

involves two multiplications per term, and the sum will require NWP - 1 additions. There is also the addition of the first term in Eq. (32). These opera-
tions must be done for every on-shell energy, resulting in a complexity of 6NWP ×nE FLOP. Note that if we use energy averaging we only require a single
evaluation for each on-shell wave packet, giving an upper limit nE  NWP.

The WPCD sequential complexity is then given by

OWPCD,seq.(T ) = 6NW3 P - NW2 P + 6NWP × nE , (40)
To summarise, the overall complexity of the MI method scales linearly with the number of scattering energies nE, see Eq. (37). For the WPCD method we get all the on-shell energy scattering-solutions from a single Hamiltonian diagonalisation, giving a very "cheap" scaling with nE.

2. Parallel complexity

The parallel WPCD approach is based on simultaneous solutions of the LS equation for all on-shell energies. Contrary to the sequential approach, we assume a sufficiently large set of processors (like in a GPU) to handle all on-shell energies at once. This will remove the factor nE in the complexity model (40) such that the cubicallyscaling term will clearly become the dominating one. It can be combatted by making use of hardware-specific, massively parallel algorithms for matrix diagonalisation and matrix-matrix multiplications. Here we present the parallel approach to the same steps as above, and in the same ordering. We assume we have p compute processors, or threads, available.

1. The parallel cyclic-order Jacobi method is a par-

allel approach to the Jacobi eigenvalue algorithm:

a method based on finding a similarity transforma-

tion of a matrix to its diagonal form by repeated Ja-

cobi rotations (see e.g. Ref. [39]). This method has

held major appeal for parallel computing due to its

inherent parallelism compared to other eigenvalue-

finding routines. However, parallel optimisation of

the basic algorithm depends greatly on how a set

of processors is organised with regards to commu-

nication and memory. Therefore, there is no gen-

eral complexity model for the method--any algo-

rithm should be written with a specific hardware

in mind. One approach can be found in Ref. [40]

with a demonstrated complexity O

n3 p

log

n

for

convergence for a symmetric and real n × n matrix.

8

2. Parallel matrix-matrix multiplication algorithms is

an ongoing field of research (see e.g. Ref [41]).

While there exist very efficient methods, such as

Cannon's algorithm [42], they depend highly on

hardware and matrix characteristics (sparsity, sym-

metry, etc). The divide-and-conquer approach is

a very general and straightforward way to paral-

lelise matrix multiplications. In essence, we di-

vide the matrices into M =

n2 p

submatrices

of sizes m × m = p. If combined with on-chip

shared memory between the processors, this per-

mits all processors to simultaneously work on cal-

culating the product while minimising processor-

to-processor communication time. The method is

explained excellently in literature such as [39]. This

simple method has a theoretical minimum complex-

ity of O (n × M ) when performed in parallel5.

3. There is limited parallel optimisation to be gained in Eq. (39). We can multiply each term of the summation in parallel, and do the whole summation sequentially for the sake of simplicity. The result is a complexity of O(2(NWP + 2)) FLOP. Note, however, that the inherent parallelism of the WPCD method allows us to do the summation simultaneously for all E and thereby effectively removing the factor nE seen so far in the complexity models.

In conclusion, assuming p > NWP and adequately large shared memory (see point 2 above), a somewhat optimal
and parallel WPCD complexity model is given by

OWPCD,par.(T ) =

NW3 P p

log(NWP)

+ NWP

NW2 P p

(41) + 2(NWP + 2) .

We see from Fig. 2 that the efficiency of the parallel WPCD approach scales very well with the number of available processors. We also see that for a single processor (p = 1) the parallel and sequential approaches are roughly equal (while not taking into account any overhead in memory transfers), while for a realistic value p = 1024 the complexity model for the parallel approach demonstrates a clear advantage. The value p = 1024 corresponds to the current limit on threads with shared memory for typical Nvidia GPUs.

5 This is not the full picture. Often, a set of processors is divided in groups of processors with limited shared memory that cannot fit all p elements from each matrix in the matrix-matrix product, and one has to define more submatrices than given by M . This introduces yet another complication to the parallel complexity model that has to do with shared memory size, which we will not account for here.

100

Complexity [GFLOP]

10-2

10-4

10-6 0

MI (nE =16) MI (nE =32) MI (nE =64) WPCD seq. WPCD par. (p = 1) WPCD par. (p = 1024)
100 200 300 400 500 Matrix size n

FIG. 2. Complexity, as measured in FLOP, for the MI and WPCD methods as a function of n, where n represents the number of quadrature points (NQ) for MI and the number of wave packets (NWP) for WPCD. Furthermore, nE is the number of on-shell T -matrix evaluations (note nE = n = NWP for WPCD).

V. CONTINUUM-DISCRETISED NEUTRON-PROTON SCATTERING
COMPUTATIONS
In this section we present a detailed analysis of the precision and accuracy of the WPCD method for computing neutron-proton (np) scattering observables and phase shifts. For all calculations, we employed the optimised next-to-next-to-leading-order chiral potential N2LOopt [43]. The primary goal is to analyse the trade-off between minimum computational cost and maximum method accuracy in the WPCD method, and contrast this to the conventional MI method. Note that there is no problem in obtaining highly accurate results from either method. We simply focus on the performance and accuracy of the WPCD method as we reduce the number of wave-packets such that all objects fit in the fast on-chip shared memory on the GPU. In our analysis we consider the MI method with NQ = 96 Gauss-Legendre points, see Eq. (4), to yield virtually exact results. For brevity, we will refer to such numerically converged calculations as exact.

9

68
60
66

64
40
62

60

20

0

10

20

Exact
0 NWP = 32
NWP = 64
-20
-40

Phase shift [degrees]

0

1S0

3P0

150 125 100 75 50

110 105 100
95 90 85 80 75
0

20

40

25

0

3S1

0

100

1

200

300 0

100

200

Laboratory kinetic energy [MeV]

-60 3 2 1 0 -1 300

FIG. 3. Phase shifts calculated using the WPCD-method using NWP=32 (blue) and NWP=64 (orange) wave packets. Numerically exact results (black) were obtained using the MI-method.

A. Computing phase shifts
In Fig. 3 we compare np scattering phase shifts in the 1S0, 3P0, and 3S1 partial waves as well as the 1 mixing angle, as obtained from the WPCD method using NWP=32 and NWP=64 momentum wave-packets. The free wave-packet bin boundaries follow a Chebyshev distribution with scaling factor  = 100 MeV and sparseness degree t = 2, see Eq. (31), and the resolvent was energy-averaged according to Eq. (B7).
For the WPCD-result, one immediately observes a discretised, step-like version of the otherwise smoothly varying exact description of the scattering phase shifts. This is entirely due to the momentum discretisation and finite number of wave packets. Indeed, due to the energyaveraging we obtain only one T -matrix element per momentum bin. In the limit NWP , and everything else equal, we will recover the exact result obtained with the MI method. We observe this convergence trend already when going from NWP=32 to NWP=64 wave packets. For instance, the mixing angle moves closer to the exact re-

sults when increasing NWP. Overall, the WPCD method is performing as expected, but there are two features we would like to point out:
1. The values of the low-energy 1S0 phase shift are overestimated near the peak where the phase shift turns over. This is likely due to the momentumaveraging of operators. The potential matrix in the 1S0 channel ( q |V1S0 |q ) is shown in Fig. 4 for both a continuous momentum-basis and a NWP = 32 wave-packet basis with bins distributed according to Eq. (31). The potential is constant within each wave-packet bin as expected according to Eq. (29). This makes it challenging to reproduce finer details of the interaction. There is also a discrepancy between the continuous and wave-packet values when the chosen q -momentum is not near any bra-state bin midpoint. This is most distinctive in the green curves at q = 205.622 MeV, which is very close to a bin boundary. We see in Eq. (29) that we average over momenta within two bins, and when a potential varies strongly within a bin such that

10

Matrix element q |V1S0|q Phase shift [degrees]

×10-6

8

q = 21.6745 MeV

q = 118.716 MeV

q = 205.622 MeV

6

q = 265.457 MeV

4

2

0

-2

-4

-6

-8 0

100

200

300

q-momentum [MeV]

FIG. 4. The 1S0 potential of NNLOopt [43] as a function of q, at several q values. The dashed lines represent the potential calculated in a continuous basis, while the solid lines represent a wave-packet projection. The wave-packet projection was made with an NWP = 32 basis in a Chebyshev distribution (with  = 100 MeV). In the wave-packet representation, the bra-state bin contains q . Note that the units for continuous representation of the potential is in units of MeV-2 while the wave-packet projected potential is in units of MeV, as follows from the definition in Eq. (17).

68

60

66

64

40

62

60

20

0 10 20

0

0

100

200

300

Laboratory kinetic energy [MeV]

FIG. 5. Phase shifts for the 1S0 partial wave calculated using WPCD. The colour coding is the same as in Fig. 3, i.e. NWP = 32 (light blue) and NWP = 64 (light orange). The bands indicate the maximum discrepancy due to the choice of interpolation point n according to Eq. (43).

1. Linearly interpolating phase shifts

Although the resolution of the WPCD method is lim-

ited by the energy-averaging across each momentum-

bin--i.e., yields a step-like character of the predictions--

the

mid-points

q¯i

=

1 2

(qi-1

+ qi)

of

each

bin

in

Fig.

3

are

typically closest to the exact results, as expected from the

wave-packet eigenvalues, see Eqs. (18) and (19). We can

therefore linearly interpolate the phase shifts i  (q¯i) across several bins, i.e. across scattering energies, via

the matrix elements at the bin boundaries are quite different from the bin mid-point values, this averaging is too coarse to mimic the potential accurately. This effect becomes less significant with increasing NWP since the grid will grow denser.
2. The WPCD-results for 3S1 show a distinctive drop at 90 degrees. This trend is consistent for all basis sizes and is simply due to the treatment of the deuteron bound state in the WPCD framework. We extract a phase shift  via an inverse trigonometric function with values for   [-90, 90] degrees. A bound state is characterised by a transition (E + ) - (E) = 180 degrees at some energy E for an infinitesimal step > 0, as dictated by Levinson's theorem [44]. It is apparent that this transition across 90 degrees is more difficult to reproduce for the WPCD method. The same effect also gives rise to inaccuracies in the 1 mixing angle. However, this is of no concern when computing a scattering observable.

(q) =

i - i-1 q¯i - q¯i-1

q+

i-1

-

i q¯i

- -

i-1 q¯i-1

q¯i-1

. (42)

for c.m. momenta q  [qi-1, qi] where qi are the FWP bin boundaries. This simple and straightforward approach offers a rather precise prediction for any on-shell scattering energy. Of course, the phase shifts can be linearly interpolated using other points, i.e. we can let

q¯i

=

qi-1

+

n m

(qi

-

qi-1)

,

(43)

for

some

n



[0, m],

such

that

n

=

m 2

is

the

bin

mid-point

again. Figure 5 shows the resulting predictions using lin-

ear (mid-point) interpolation of the 1S0 phases presented in Fig. 3. The bands estimate the effect of varying the

interpolation point. The bands span the resulting (q)

calculated with m = 10 and n = [0.1, 1, 2, . . . 8, 9, 9.9]

in Eq. (43). As expected, mid-point interpolation yields

results that are very close to the exact calculation. In

the following we will therefore only use this interpolation

choice.

11

Total cross section [mb] Relative difference (abs.val.) [mb]

Exact

NWP = 32

NWP = 64

Interpolated WPCD

103

150

100

50

102

50 100 150 200 250 300

0

100

200

300

Laboratory kinetic energy [MeV]

101 100 10-1 10-2 10-3
0

NWP =32 NWP =64 NWP =96 NWP =128 NWP =256

100

200

300

Laboratory kinetic energy [MeV]

FIG. 6. Total cross section calculated with the WPCD method. The solid black line corresponds to the exact result as obtained using the MI method. The inset demonstrates the high accuracy of (mid-point) interpolated WPCD results. On this scale, the two separate WPCD predictions appear to overlap completely.
B. Computing cross sections
We compute scattering cross sections from scattering amplitudes according to the method outlined in Appendix A. Figure 6 shows the total cross section obtained using the WPCD-method using momentum-space bins with NWP = 32 and NWP = 64. As can be seen in the figure, the linearly-interpolated results reproduce the exact result rather well. On a larger scale it is nearly impossible to tell any difference between results obtained using NWP = 32 bins and NWP = 64 bins. To emphasise the monotonically increasing accuracy of the WPCD method as we increase the number of momentum-space bins, we calculate the absolute value of the difference between the exact and the WPCD prediction for the total cross section for a range of bin resolutions, see Fig. 7. From this it is apparent that the WPCD method converges, although slowly, for a bin partition following a Chebyshev distribution.
C. WPCD accuracy
There are primarily two approximations that impact the accuracy of the WPCD method: the number of bins NWP and the distribution thereof. Also, the averaging of continuous states into wave packets means we use momentum-averaged matrix representations of operators in the LS equation. This averaging should improve with reduced bin widths. We can easily control NWP, and in this section we analyse the performance of WPCD with respect to different choices of this method parameter.
To better quantify the convergence of the WPCD

FIG. 7. Absolute values of the relative difference between exact results and WPCD method for the total cross section, shown as a function of wave-packet basis size NWP and laboratory kinetic energy.

method we study the root-mean-square error (RMSE) with respect to the exact result for the total cross section across a range of scattering energies. We use the standard RMSE measure

RMSE =

nE i=1

(exact,i

-

method,i)2

,

nE

(44)

where exact,i denotes the exact results and method,i denotes either the WPCD- or MI-calculated total cross
sections at some scattering energy Ei for i = 1, . . . , nE.

We find that the RMSE for WPCD with scattering energies corresponding to laboratory kinetic energies 40  Tlab  350 MeV remains fairly constant at  2.0 mb when using NWP = 16. This is interesting for three reasons:
· The WPCD coupled channel Hamiltonian will be of size 2NWP × 2NWP, i.e. we diagonalise 32 × 32 Hamiltonian matrices. Most GPUs today, including the Nvidia Tesla V100 we have used here, have 64 kB shared memory (also called "on-chip" memory) which is significantly faster to access than the GPU's main memory. These matrices fit entirely on the GPU shared memory, allowing for a strong reduction in GPU memory read/write demand while performing the diagonalisation.
· A recent Bayesian analysis of chiral interactions suggests that the EFT truncation error for scattering cross sections at next-to-next-to-leadingorder is at least 2 mb, at 68% degree-of-belief (DoB), and at least 5 mb at 95% DoB [14].
· Regarding experimental uncertainties, the combined statistical and systematic uncertainties in

12

measured np total cross sections are at the 1% level [45, 46]. In absolute terms, this amounts to uncertainties of the order 1 mb at laboratory scattering energies 40 MeV. At lower energies the cross section increases of course, leading to slightly larger absolute experimental errors.
In summary, we find that the WPCD method error, when using very few wave-packets, is slightly larger than typical experimental uncertainties but smaller than the estimated model discrepancy up to next-to-next-toleading-order in EFT.
One can clearly reduce the WPCD method error by increasing NWP. However, this will also increase the computational cost and we recommend studying the actual wall time cost in relation to, e.g. the RMSE measure defined above. We perform such an analysis in Sec. VI.
Before ending this section on the method accuracy, we would like to emphasise that the maximum total angular momentum Jmax in the partial-wave expansion of the potential also impacts the accuracy of the description of the scattering amplitude. For the linearly-interpolated WPCD method with NWP = 16, where we observe a method error of  2 mb in the total cross section at TLab 40 MeV, and find it unnecessary to go beyond Jmax = 6.
VI. METHOD PERFORMANCE
With an account of the precision and accuracy of the WPCD method we now profile its time performance. Typically, when using the MI method, we calculate every on-shell phase shift of interest by explicitly solving the LS equation, while in the WPCD approach we interpolate inbetween bin mid-points, as discussed above. This will, unsurprisingly, induce a substantial time-performance penalty in using the MI method, due to the linear scaling with the number of energy solutions, as shown in Eq. (37). However, we can of course linearly interpolate the phase shifts calculated with the MI method, rather than invoking an explicit calculation at every on-shell energy. Therefore, to facilitate a balanced comparison between the two methods, we also employed linear interpolation to extract phase shifts when using the MI method. In our studies, this has turned out to be a highly efficient way to speed up the calculation with the MI method while maintaining precision and accuracy of the results.
To facilitate a comparison, when using the MI method we solve the LS equation at nE on-shell energies also following a Chebyshev distribution. We then linearly interpolate the phase shifts using these energies to calculate neutron-proton total cross sections in the Tlab energy ranges (0, 350] MeV and (40, 350] MeV for the calculation of RMSE values.
For WPCD we can only vary the number of wave packets NWP while for MI we can vary both NQ and nE, i.e. the number quadrature points and the number of interpolation points or on-shell energies (at bin mid-points),

TABLE I. Measured wall times, corresponding to the results

in Fig. 8, for solving the LS equation and predicting total

cross sections using the WPCD and MI methods, at different

RMSE levels in the Tlab  (40, 350] MeV energy region. The nE parameter indicates the number of interpolation energies used for the MI method.

RMSE [mb]

NQ

MI nE Time [ms]

WPCD NWP Time [ms]

3.0

8 16

6

­

­

2.5

10 16

7

­

­

2.0

12 16

8

16

0.5

1.5

14 16 10

24

2.5

1.0

16 24 15

32

6

0.5

16 32 20

96

35

respectively. Figure 8 shows the wall times for solving the LS equation to obtain cross sections, at different levels of method accuracy measured by the RMSE value. In Tab. I we show a few interpretations of the figure for a handful of relevant method parameters. As mentioned, the WPCD method is implemented on a GPU while the MI results were obtained using an optimised CPU implementation [2]. The time profiles were obtained using an Nvidia Tesla V100 32 GB SMX2 and an Intel Xeon Gold 6130 for the WPCD GPU and MI CPU results, respectively.
From our analysis we conclude that the WPCD method is faster than the MI method if one can tolerate  1 - 5 mb method RMSE in the prediction of total scattering cross sections. For such applications it would then be advisable to use NWP 48 bins, on the basis of Fig. 8. It is worth noting that the RMSE is dominated by contributions from scattering cross sections below laboratory kinetic energies Tlab  40 MeV. Indeed, with NWP = 48 we obtain an RMSE value of  4 mb across an interval Tlab  (0, 350] MeV. The RMSE drops to  0.8 mb when considering only cross sections with Tlab > 40 MeV. Irrespective of how the WP bins are distributed, it is therefore necessary to ensure a sufficiently high density of bins below Tlab = 40 MeV to accurately reproduce the 1S0 phase-shift peak and the 3S1 bound state. With the WPCD method we can obtain increasingly faster solutions to the LS equation as we reduce the number of wave-packets.

VII. SUMMARY AND OUTLOOK
In this study we have compared the WPCD method with the standard MI method, with an emphasis on their efficiency, precision, and accuracy when solving NN scattering problems. This study is done with an interest in leveraging Bayesian inference analyses of nuclear interaction models.
We find that the GPU implementation of the WPCD method is capable of providing scattering solutions to the NN LS equation much faster than a conventional MI

13

Time (solving LS equation) [ms]

64

48
102 32

64 48

101

16986064 48

32 16 32

MI with nE = 16 MI with nE = 32 WPCD (nE = NW P )
8 8

Tlab  (0, 350] MeV

5

10

16
15

64

48
102 32

64 48

101

16968064 48

32 16

32

8 8

Tlab  (40, 350] MeV

16

0.5 1.0 1.5 2.0 2.5 3.0 RMSE of total cross section [mb]

FIG. 8. Measured wall times for solving the LS equation for different values of total cross section RMSE for Tlab  (0, 350] MeV (top panel) and Tlab  (40, 350] MeV (bottom panel). nE is the number of on-shell energies (interpolation points) at WP bin mid-points, according to a Chebyshev distribution. The numeric label attached to each data point indicates the number of Gauss-Legendre grid points NQ for the MI method and the number of wave packets NWP for the WPCD method. The number of energies used for interpolating the MI results are indicated in the legend. See the main text for hardware specifications.

method implemented on a CPU. This is largely due to the WPCD method being capable of delivering scattering amplitudes at several on-shell energies in an inherently parallel fashion, and that linear interpolation across several WP bins offers straightforward predictions for any scattering energy. However, compared to solutions obtained using the standard MI method, the WPCD method is less accurate, in particular in regions where the scattering amplitudes vary strongly with energy. This is also expected due to the momentum-space discretisation and approximation of the scattering states. The computational gain of using GPU hardware is limited by the amount of shared memory that is available. This constraint basically corresponds to an upper limit on the number of WP bins that can be used for efficient computations. We note, however, that GPU hardware is continuously improved, and that a four-fold increase in the size of the fast shared memory on the GPU would enable a two-fold increase of both the WP-basis and method accuracy while incurring a very mild additional computational cost.
Nevertheless, in applications where a certain method

error can be tolerated--as for example in low-order EFT predictions of NN scattering cross sections--the GPUimplemented WPCD method presents a computational advantage. This finding makes it particularly promising for computational statistics analyses of EFT nuclear interactions utilising Bayesian inference methods.
In this study we have focused on the inherent parallelism of the WPCD method and the opportunities that it offers to benefit from the use of GPU hardware. Of course, one could also explore parallelisation of the WPCD method on a CPU and make parallel use of the fast cores available in moderns CPUs. The efficiency of the GPU WPCD approach is almost fully determined by the time needed to diagonalize the channel Hamiltonian and could therefore benefit from the faster CPU clock speed.
Naturally, one could also consider a GPU implementation of the MI method wherein the LS equation is solved in a parallel fashion for multiple scattering energies simultaneously. Still, this would require multiple matrix inversions (one for each interpolation energy). A simple phase-shift interpolation applied to the MI method facilitates sufficiently accurate results for NN scattering observables, at a significantly lower computational cost compared to explicitly solving the LS equation at every scattering energy of interest.

ACKNOWLEDGMENTS
This work was supported by the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation programme (Grant agreement No. 758027), and the Swedish Research Council (Grant No. 2017-04234). The computations were enabled by resources provided by the Swedish National Infrastructure for Computing (SNIC) at Chalmers Centre for Computational Science and Engineering (C3SE) and the National Supercomputer Centre (NSC) partially funded by the Swedish Research Council.

Appendix A: Scattering cross sections and the spin-scattering matrix

A scattering observable of some operator O^ can generally be written as a trace [23],

O^

=

Tr{M ^iM O^} Tr{M ^iM }

,

(A1)

where ^ is the spin-density matrix, usually represented

in a helicity basis, and M is the spin-scattering matrix.

The helicity-basis projection of the spin-scattering ma-

trix is written as total spin and ms

Mis msths,emcso,rrweshpeornedsingis

the conserved projected spin.

These matrices are usually expressed in partial-wave ex-

14

pansions,

Mms s,ms

=

2 ip

[1 - (-1)l+s+TZ ] 2l + 1 4

J,l,l

× Yml s-ms (, ) × l , s, ms - ms , ms |l , s , j, ms

× l, s, ms, 0|l, s, j, ms

× Sl1Jl (p , p) - l,l ,

(A2)

where the third and fourth rows are Clebsch-Gordan coefficients, J is the total angular momentum, l and l are the orbital angular momenta of the inbound and outbound states respectively, TZ is the azimuthal isospin projection, Yml (, ) is an azimuthal spherical harmonic, and Sl1Jl (p , p) = p , l , s, J|S|p, l, s, J is the usual scattering matrix.
The on-shell partial-wave-projected S-matrix is related to the on-shell T (p, p; E)-matrix element via

SlsJl (p, p; E) = 1 - 2iTlsJl (p, p; E) .

(A3)

Due to the conservation of probability, the S-matrix is
unitary, and can therefore be parametrised by real pa-
rameters like phase shifts. In the Stapp convention [47], the coupled-channel S-matrix Sl1Jl is then given by

S=

cos(2 J )e2i-,J

i sin(2 )e J i(-,J ++,J )

i sin(2 )e J i(-,J ++,J )

cos(2 J )e2i+,J

,

(A4)

where ±,J refers to l,J for l = J ±1 and J is the mixing angle. The uncoupled-channel S-matrix Sl0lJ is given by (for l = J)

S = e2iJ,J .

(A5)

The angle  denotes the scattering angle between the c.m. inbound and outbound relative momenta p and p respectively, while  is the rotation angle of p around the inbound momentum p, but cylindrical symmetry allows us to set  = 0. Using Eq. (A2) we can calculate scattering observables such as the differential cross section,

d d

=

1 4

Tr(M

M

)

.

(A6)

However, this approach does not utilise symmetries to re-

duce the Instead,

number of non-contributing terms of Mms s the M -matrix can be expressed in terms of

n,mosn-.

vanishing spin-momentum products after some consid-

eration of parity conservation, isospin and time-reversal

symmetries, and the Pauli principle. We use the Saclay

convention [48] for these terms,

M

=

1 2

(a + b) + (a - b)(1 · n)(2 · n)

+ (c + d)(1 · m)(2 · m)

+ (c - d)(1 · l)(2 · l)

+ e((1 + 2) · m) ,

(A7)

where a, b, c, d, and e are the Saclay amplitudes, i are the Pauli spin matrices acting on nucleon i = 1, 2, and
where we define the following unit vectors

l

p |p

+ +

p p

|

,

m

p |p

- -

p p

|

,

n

p |p

× ×

p p

|

.

(A8)

The Saclay amplitudes are given by the spin-projected matrices as,

a

=

1 2

(M111

+

M01,0

+ M11,-1) ,

b

=

1 2

(M111

+

M00,0

+ M11,-1) ,

c

=

1 2

(M111

-

M00,0

+ M11,-1) ,

d

=

-

 2

1 sin()

(M11,0

+

M01,1)

,

e

=

i 22

(M11,0

-

M01,1)

.

(A9)

In this parametrisation, the differential cross-section is given by

d d

=

1 2

|a|2 + |b|2 + |c|2 + |d|2 + |e|2

.

(A10)

For the results in this paper we used the following expression for the total cross section:

tot

=

2 p

Im(a

+

b)

.

(A11)

See Ref. [48] for a complete account of scattering observables in the Saclay parametrisation.

Appendix B: The resolvent in a wave-packet basis

The resolvent g^(E) for the full Hamiltonian h^  h^0 +v^,

g^(E)



E

-

1 h^

±

i

,

(B1)

can be calculated analytically in a pseudostate wavepacket basis {|zi }ni=1 of the full Hamiltonian. The resolvent is represented in the basis by (see Eq. (17))

zi|g^(E)|zj

=

zi|

E

-

1 h^

±

i

|zj

= µ

1

dk dk

NiNj

Di
×k

Dj k kk
E-

k(+)|k(+)

k2 2µ

±i

,

(B2)

where µ =

mN 2

.

Note that we set the weight function

f (k) =

k µ

and

normalisation

Ni

=

Ei

as

these

are

en-

ergy wave-packets (from the diagonalisation of h^). Using

k(+)|k(+)

=

(k k

-k) k

,

this

becomes

zi|g^(E)|zj

=

ij µNi

Di

dk

E

-

k
k2 2µ

±

i

,

(B3)

where we have introduced the Kronecker delta ij since

k(+)|k(+) = 0  Di = Dj. For positive energies, where

E

=

p2 2µ

and

where

p

is

the

on-shell

c.m.

momentum,

we

get

zi|g^(E)|zj

=

2ij Di

Di

dk

p2

-

k k2

±

i

,

(B4)

If E / Di, we take the limit  0 and solve the integral to find

zi|g^(E)|zj

=

2ij Ni

Di

dk

p2

k -

k2

=

ij Ni

- ln

k2 p2

-

1

ki+1 ki

=

ij Ni

ln

E + Ei+1 E + Ei

,

(B5)

where p / Di, and Ei and Ei+1 is the lower and upper boundary of Di expressed in energy, respectively. If E  Di, then we have a simple pole at p = k. The pole-integration is done using the infinitesimal complex
rotation ±i together with the residue theorem, giving

zi|g^(E)|zj

=

ij Ni

ln

E + Ei+1 E + Ei

- i((E - Ei) - (E - Ei+1)) ,
(B6)
where  is the Heaviside step-function. The derivation of the resolvent expressed in a momentum wave-packet representation follows a similar procedure. In that case, it is possible to use momentum wave-packets where f (p) = 1 and Ni = ki+1 - ki, in which case the derivation above changes a little, see [11].

Energy averaging of the resolvent is done by integrat-
ing the resolvent with respect to E, in the bin E  Dk, divided by the bin width Ek. We introduce the denotation g¯ikj to reflect this. The derivation is straightforward:

g¯ikj



1 Ek

dE zi|g^(E)|zj
Dk

=

ij Ek Ni

dE
Dk

ln

E + Ei+1 E + Ei

- iik

=

ij Ek Ei

Wki

-

i Ek

ik

,

(B7)

where we used Ni = Ei, and

k+1 i+1

Wki 

(-1)k-k +i-i [Ek - Ei ] ln |Ek - Ei | .

k =k i =i

(B8)

as presented in [11].

15

Time [ms]

V100

2000

T4

K40

1000

0

Total (V100)

200

Diagonalising Hamiltonian

Calculating phase shifts

100

Calculating observables

0

0

50 100 150 200 250

Number of wave packets NWP

FIG. 9. Top: total time used in calculating on-shell T -matrix elements and extracting phase shifts and observables, shown for three different Nvidia GPUs of the Tesla line; T4 (orange), V100 (blue), and K40 (green). Bottom: time decomposition of V100 total time, with focus on key parts of calculating NN scattering using the WPCD method. The green line includes the time spent solving the LS equation.

Appendix C: GPU code
The code for GPU-utilisation was written to make use of the CUDA interface, which is developed and maintained by the Nvidia Corporation. CUDA allows for efficient utilisation of a Nvidia GPU using high-level programming languages such as C and C++, Python, or Fortran. For numerically demanding linear algebra operations we made use of the CUDA-libraries cuBLAS[37] and cuSOLVER[38]. These libraries are very similar in use to BLAS and LAPACK--two standard libraries for linear algebra on the CPU.
Diagonalising the full Hamiltonian and solving the LS equation are the most numerically demanding parts of the steps presented in Sec. III B. These steps are therefore solved on the GPU. The ability to work on several channels simultaneously--this being the advantage of the GPU--is made possible by using CUDA's batched routines. These are pre-written routines that maximise the efficiency of the GPU to solve several linear algebra problems simultaneously for small matrix-sizes (typically less than 1000×1000 in matrix dimension). Such efficient parallelism is generally difficult to achieve "by hand"due to the massive load on GPU-memory read-write accesses.
The importance of efficient memory use is made apparent in Fig. 9 where we show the computation time spent by three different Nvidia GPUs (top panel) and the decomposition of time used for the Nvidia V100 GPU (bottom panel). We see that the majority of the total time is spent on the Hamiltonian diagonalisation. A large frac-

16

tion of the diagonalisation task is spent on memory accesses as part of the Jacobi method. The three GPUs: V100, T4, and K40, have differences in memory technology [49] which is the main reason for the observed differences in the performance.
To calculate the V C-matrix product in Eq. (32) we used cublas<t>gemmStridedBatched, which calculates a matrix-product on the form C  AB + C, where C is overwritten by the right-hand side. This setup is standard for BLAS gemm-routines. Here,  and  are scalars, while A, B, and C are sets of matrices stored

congruently in three arrays, i.e. they are "batched".

To diagonalise the Hamiltonians we used

cusolverDn<t>syevjBatched.

This routine

utilises the parallel cyclic-order Jacobi method, as was

briefly discussed in Sec. IV B, to diagonalise batches of

matrices simultaneously.

Lastly, solving Eq. (39) was done using a custom-

written function, referred to as a "kernel" in CUDA. The

advantage of using the GPU for this task comes with the

energy-dependence in the resolvent. We can calculate

all on-shell T -matrix elements simultaneously following

a single batched Hamiltonian diagonalisation.

[1] D. R. Entem and R. Machleidt, "Accurate chargedependent nucleon-nucleon potential at fourth order of chiral perturbation theory," Phys. Rev. C 68, 041001 (2003).
[2] B. D. Carlsson, A. Ekstro¨m, C. Forss´en, D. Fahlin Stro¨mberg, G. R. Jansen, O. Lilja, M. Lindby, B. A. Mattsson, and K. A. Wendt, "Uncertainty analysis and order-by-order optimization of chiral nuclear interactions," Phys. Rev. X 6, 011019 (2016).
[3] P. Reinert, H. Krebs, and E. Epelbaum, "Semilocal momentum-space regularized chiral two-nucleon potentials up to fifth order," Eur. Phys. J. A 54, 86 (2018).
[4] M. Piarulli, L. Girlanda, R. Schiavilla, R. Navarro P´erez, J. E. Amaro, and E. Ruiz Arriola, "Minimally nonlocal nucleon-nucleon potentials with chiral two-pion exchange including  resonances," Phys. Rev. C 91, 024003 (2015).
[5] R. Navarro P´erez, J. E. Amaro, and E. Ruiz Arriola, "Coarse-grained potential analysis of neutron-proton and proton-proton scattering below the pion production threshold," Phys. Rev. C 88, 064002 (2013).
[6] P. F. Bedaque and U. van Kolck, "Effective field theory for few-nucleon systems," Annu. Rev. Nucl. Part. Sci. 52, 339­396 (2002).
[7] E. Epelbaum, H.-W. Hammer, and Ulf-G. Meißner, "Modern theory of nuclear forces," Rev. Mod. Phys. 81, 1773­1825 (2009).
[8] R. Machleidt and D. R. Entem, "Chiral effective field theory and nuclear forces," Phys. Rep. 503, 1 ­ 75 (2011).
[9] H.-W. Hammer, Sebastian Ko¨nig, and U. van Kolck, "Nuclear effective field theory: Status and perspectives," Rev. Mod. Phys. 92, 025004 (2020).
[10] S. Wesolowski, N. Klco, R. J. Furnstahl, D. R. Phillips, and A. Thapaliya, "Bayesian parameter estimation for effective field theories," J. Phys. G 43, 074001 (2016).
[11] O. A. Rubtsova, V. I. Kukulin, and V. N. Pomerantsev, "Wave-packet continuum discretization for quantum scattering," Ann. Phys. (N. Y.) 360, 613­654 (2015).
[12] R. J. Furnstahl, A. J. Garcia, P. J. Millican, and X. Zhang, "Efficient emulators for scattering using eigenvector continuation," Phys. Lett. B 809, 135719 (2020), arXiv:2007.03635 [nucl-th].
[13] J. Brynjarsdo´ttir and A. O'Hagan, "Learning about physical parameters: the importance of model discrepancy," Inverse Probl. 30, 114007­25 (2014).
[14] J. A. Melendez, S. Wesolowski, and R. J. Furnstahl,

"Bayesian truncation errors in chiral effective field theory: Nucleon-nucleon observables," Phys. Rev. C 96, 024003 (2017). [15] M. I. Haftel and F. Tabakin, "Nuclear saturation and the smoothness of nucleon-nucleon potentials," Nucl. Phys. A 158, 1 ­ 42 (1970). [16] K. Erkelenz, R. Alzetta, and K. Holinde, "Momentum space calculations and helicity formalism in nuclear physics," Nucl. Phys. A 176, 413­432 (1971). [17] L. Hulth´en, Kungl. Fysiogr. Sa¨llsk. i Lund F¨orhandl. 14 (1944). [18] L. Hulth´en, Arkiv Mat. Astron. Fysik 35A (1948). [19] W. Kohn, "Variational methods in nuclear collision problems," Phys. Rev. 74, 1763­1772 (1948). [20] C. Schwartz, "Application of the schwinger variational principle for scattering," Phys. Rev. 141, 1468­1470 (1966). [21] J. L. Basdevant, "The pad´e approximation and its physical applications," Fortschr. Phys. 20, 283­331 (1972). [22] J. J. du Croz and N. J. Higham, "Stability of Methods for Matrix Inversion," IMA J. Numer. Anal. 12, 1­19 (1992). [23] W. Glo¨ckle, The quantum mechanical few-body problem (Springer Science & Business Media, 2012). [24] J. Carbonell, A. Deltuva, A. C. Fonseca, and R. Lazauskas, "Bound state techniques to solve the multiparticle scattering problem," Prog. Part. Nucl. Phys. 74, 55 ­ 80 (2014). [25] E. J. Heller, "Theory of j-matrix green's functions with applications to atomic polarizability and phase-shift error bounds," Phys. Rev. A 12, 1222­1231 (1975). [26] J. R. Winick and W. P. Reinhardt, "Moment t-matrix approach to e+-h scattering. i. angular distribution and total cross section for energies below the pickup threshold," Phys. Rev. A 18, 910­924 (1978). [27] J. R. Winick and W. P. Reinhardt, "Moment t-matrix approach to e+-h scattering. ii. elastic scattering and total cross section at intermediate energies," Phys. Rev. A 18, 925­934 (1978). [28] E. J. Heller, T. N. Rescigno, and W. P. Reinhardt, "Extraction of scattering information from fredholm determinants calculated in an L2 basis: A chebyschev discretization of the continuum," Phys. Rev. A 8, 2946­2951 (1973). [29] C. T. Corcoran and P. W. Langhoff, "Moment-theory approximations for nonnegative spectral densities," J.

17

Math. Phys. 18, 651­657 (1977). [30] O. A. Rubtsova and V. I. Kukulin, "Wave-packet dis-
cretization of a continuum: Path toward practically solving few-body scattering problems," Phys. At. Nucl. 70, 2025­2045 (2007). [31] V. I. Kukulin, V. N. Pomerantsev, and O. A. Rubtsova, "Wave-packet continuum discretization method for solving the three-body scattering problem," Theor. Math. Phys. 150, 403­424 (2007). [32] O. A. Rubtsova, V. N. Pomerantsev, and V. I. Kukulin, "Quantum scattering theory on the momentum lattice," Phys. Rev. C 79, 064602 (2009). [33] V. I. Kukulin and O. A. Rubtsova, "Discrete quantum scattering theory," Theor. Math. Phys. 134, 404­426 (2003). [34] H. Mu¨ther, O. A. Rubtsova, V. I. Kukulin, and V. N. Pomerantsev, "Discrete wave-packet representation in nuclear matter calculations," Phys. Rev. C 94, 024328 (2016). [35] V. I. Kukulin and O. A. Rubtsova, "Solving the ChargedParticle Scattering Problem by Wave Packet Continuum Discretization," Theor. Math. Phys. 145, 1711---1726 (2005). [36] O. A. Rubtsova, V. I. Kukulin, V. N. Pomerantsev, and A. Faessler, "New approach toward a direct evaluation of the multichannel multienergy s matrix without solving the scattering equations," Phys. Rev. C 81, 064003 (2010). [37] "Dense linear algebra on gpus," https://developer. nvidia.com/cublas (2021), accessed: 2021-03-22. [38] "Cuda toolkit documentation," https://docs. nvidia.com/cuda/cusolver/index.html (2021), accessed: 2021-03-22. [39] G. H. Golub and C. F. Van Loan, Matrix Computations, 3rd ed. (The Johns Hopkins University Press, Baltimore, MD, USA, 1996). [40] M. Pourzandi and B. Tourancheau, "A parallel perfor-

mance study of jacobi-like eigenvalue solution," Technical Report (1994). [41] A. Abdelfattah, S. Tomov, and J. Dongarra, "Matrix multiplication on batches of small matrices in half and half-complex precisions," J. Parallel Distrib. Comput. 145, 188 ­ 201 (2020). [42] S. E. Bae, T.-W. Shinn, and T. Takaoka, "A faster parallel algorithm for matrix multiplication on a mesh array," Procedia Comput. Sci. 29, 2230 ­ 2240 (2014), 2014 International Conference on Computational Science. [43] A. Ekstro¨m, G. Baardsen, C. Forss´en, G. Hagen, M. Hjorth-Jensen, G. R. Jansen, R. Machleidt, W. Nazarewicz, T. Papenbrock, J. Sarich, and S. M. Wild, "Optimized chiral nucleon-nucleon interaction at next-to-next-to-leading order," Phys. Rev. Lett. 110, 192502 (2013). [44] N. Levinson, "On the uniqueness of the potential in a schrodinger equation for a given asymptotic phase," Kgl. Danske Videnskab Selskab. Mat. Fys. Medd. 25 (1949). [45] P. W. Lisowski, R. E. Shamu, G. F. Auchampaugh, N. S. P. King, M. S. Moore, G. L. Morgan, and T. S. Singleton, "Search for resonance structure in the np total cross section below 800 mev," Phys. Rev. Lett. 49, 255­259 (1982). [46] W. P. Abfalterer, F. B. Bateman, F. S. Dietrich, R. W. Finlay, R. C. Haight, and G. L. Morgan, "Measurement of neutron total cross sections up to 560 mev," Phys. Rev. C 63, 044608 (2001). [47] H. P. Stapp, T. J. Ypsilantis, and N. Metropolis, "Phaseshift analysis of 310-mev proton-proton scattering experiments," Phys. Rev. 105, 302­310 (1957). [48] J. Bystricky, F. Lehar, and P. Winternitz, "Formalism of nucleon-nucleon elastic scattering experiments," J. Phys. France 39, 1­32 (1978). [49] Z. Jia, M. Maggioni, J. Smith, and D. P. Scarpazza, "Dissecting the nvidia turing t4 gpu via microbenchmarking," (2019), arXiv:1903.07486 [cs.DC].

