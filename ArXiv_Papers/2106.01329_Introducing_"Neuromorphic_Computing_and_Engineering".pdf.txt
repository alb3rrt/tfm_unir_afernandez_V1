arXiv:2106.01329v1 [cs.DC] 30 May 2021

Introducing "Neuromorphic Computing and Engineering"
Giacomo Indiveri1
1 Institute of Neuroinformatics, University of Zurich and ETH Zurich, Zurich, Switzerland E-mail: giacomo@ini.uzh.ch
Abstract. The standard nature of computing is currently being challenged by a range of problems that start to hinder technological progress. One of the strategies being proposed to address some of these problems is to develop novel brain-inspired processing methods and technologies, and apply them to a wide range of application scenarios. This is an extremely challenging endeavor that requires researchers in multiple disciplines to combine their efforts and co-design at the same time the processing methods, the supporting computing architectures, and their underlying technologies. The journal "Neuromorphic Computing and Engineering" (NCE) has been launched to support this new community in this effort and provide a forum and repository for presenting and discussing its latest advances. Through close collaboration with our colleagues on the editorial team, the scope and characteristics of NCE have been designed to ensure it serves a growing transdisciplinary and dynamic community across academia and industry.
Submitted to: Neuromorphic Computing and Engineering

Editorial

2

1. Introduction

As Editor-in-Chief I'm pleased to announce the publication of the first content [1­4] in Neuromorphic Computing and Engineering (NCE). This editorial aims at motivating the need for creating such a journal by describing our view on some of the current challenges for Information and Communication Technologies (ICT) and the role that NCE can play to address them.

While Moore's law has been fueling technological progress for decades, it is only in recent years that we could start to reap most of this progress' benefits (see Fig. 1a): ICT are becoming pervasive and are affecting every facet of our daily lives. At a global scale, ICT is affecting almost all aspects of our society, ranging from global communication to education and health, from finance to automation, transportation, and climate change. With the digitalization of our society, computing technologies and electronic devices are producing increasing amounts of electronic data every year and people are getting access to larger and larger amounts of personalized information.
To cope with the demands that are emerging with this technological revolution, and to exploit the opportunities created with the availability of this data, novel data-science methods, machine-learning techniques, and Artificial Intelligence (AI) algorithms have emerged. AI algorithms typically employ neural networks and deep learning techniques to solve pattern recognition tasks and have been shown to be extremely successful in extracting information from large amounts of data [5, 6]. However, their training methods require massive amounts of data and computing resources, which in turn requires an amount of energy that is not sustainable with respect to both the global electricity supply and the computation's Carbon footprint. For example, it has been estimated that the time required to train a recent state-of-the-art AI neural network, such as GPT-3 [7], would take more than 27 years worth of processing time on a single computer, and that these computations would generate over 78,000 pounds of CO2 emissions in total, which is more than what the average American adult produces in two years. The reason for these exorbitant costs are due to the fact that current computing technologies, based on the classical von Neumann architectures, are not well matched to the parallel processing nature of neural networks [8]. On the other hand, biological brains clearly outperform AI systems in terms of the amount of power consumption requirements, the number of training data samples, and their ability to adapt to novel and unexpected conditions. This is particularly true for those tasks that are still very difficult for computers and AI algorithms, but are done effortlessly by humans and animals, such as online learning, with small numbers of examples, interaction with the environment, or sensing and motor control.
A promising approach toward the development of novel computational paradigms and ICT systems that can interact intelligently with the environment, that could bridge the gap between natural intelligence and artificial intelligence, and that could solve
 from the Forbes article "Deep Learning's Carbon Emissions Problem".

Editorial

3

(b)
(a)
Figure 1: Technological progress. In computers, (a) Number of transistors integrated on a single CMOS chip over the years (source "The Beauty and Joy of Computing--BJC"; (b) Example of a recent neuromorphic CMOS VLSI chip comprising thousands of silicon neurons and dynamic synapses (source Institute of Neuroinformatics, University of Zurich and ETH Zurich).
many of the open challenges facing the future of computing is the one pursued by the "neuromorphic computing and engineering" field.
We are now at a very exciting time in which the convergence of the end or Moore's law, with the renewed interest in neural networks, and the needs for low power and sustainable "Green-AI" are all pointing toward the huge potential that can come from research and development in neuromorphic computing and engineering.
2. NCE thematic areas
The term neuromorphic was originally coined by Carver Mead in the late 1980s at CALTECH, to describe Very Large Scale Integration (VLSI) systems containing electronic circuits that mimic neuro-biological architectures present in the nervous system [9, 10]. However, throughout the years, the term has "morphed" its original meaning and started to be used to describe a broader set of concepts, and approaches. Next to the original "neuromorphic engineering" meaning, the term started to be used to describe "neuromorphic computing" systems that comprise also pure digital circuits or conventional processors, for simulating spiking neural networks and neural models of computation. In parallel, the same term started to be used to refer to systems comprising nano-scale "memristive" devices, developed within the field of emerging memory technologies. Today, the term is also being used to refer to algorithmic and machine learning approaches that simulate biologically plausible and hardware friendly spiking neural networks and learning mechanisms.

Editorial

4

Neural circuits and systems integrated in CMOS technology

The main goal of the original neuromorphic engineering approach was to directly emulate the physics of computation of biological neural networks, via the use of transistors operated in their "weak-inversion" or "subthreshold" regime [11­13]. This approach was aimed at building artificial neurons, synapses, networks, and sensory systems using the same organizing principles used by the nervous system in animal brains. This effort had the dual objective of both understanding neural computation by building physical emulations of real neural circuits, and developing compact and low power sensory processing and computing architectures radically different from the standard digital computers of the time. Given the high-risk and basic research aspect of this approach, there is only a small number of academic groups that are still pursuing it today. This community is mostly focusing on the development of small-scale prototype chips to explore different aspects of neural computation, ranging from sensory systems [14­18] to reconfigurable networks with biologically plausible neural dynamics [19­22], to spikebased learning and plasticity circuits [23­27].
In more recent times the term neuromorphic was adopted also to describe mixedsignal and pure digital VLSI systems which implement computing platforms that could be used to simulate models of spiking neural networks. This effort was mainly driven by the possibility to exploit progress in computing and integrated circuit technology for building large-scale dedicated brain-inspired computing systems. For example, the EU Human Brain Project funded the development of wafer-scale integrated systems designed to faithfully reproduce simulations of neuroscience modeling studies comprising large numbers of neurons at accelerated speeds [28]. Similarly, the SpiNNaker system, also developed with the support of the Human Brain Project, is a multi-core computer designed with the goal of simulating very large numbers of spiking neurons in real time [29]. At the current state of development the SpiNNaker Machine, built by stacking together 600 Printed Circuit Boards, each comprising 48 SpiNNaker processors, supports the simulation of hundreds of millions of neurons. An alternative strategy to scale up the size of simulated spiking neural networks is the one proposed by IBM, which in 2014 presented the "TrueNorth" neuromorphic systems that integrated on the same chip 4096 cores, each comprising pure digital asynchronous circuits able to simulate 256 neurons with 256×256 synaptic connections [30]. This was a significant breakthrough in the field, as it demonstrated how advanced technology nodes, such as the one used of the Samsung 28 nm bulk CMOS process, could support the integration of very large numbers of silicon neurons, while keeping the overall power consumption extremely low (e.g., with an average consumption of 70 mW total power while running a typical recurrent network at biological real-time, four orders of magnitude lower than a conventional computer running the same network). The more recent Loihi chip built by Intel [31] has some common design choices with the IBM TrueNorth chip, in that it uses pure digital asynchronous circuits to simulate neurons and synapses. Taking advantage of the progress in integration technology, this chip was fabricated using the Intel 14 nm

Editorial

5

FinFET process. Rather than focusing on large-sale, the Loihi designers chose to focus on more complex neural and synapse features, including spike-based learning mechanisms. So the Loihi chip integrates "only" 128 cores, and supports the simulation of networks containing up to 130'000 spiking neurons. Similar to the SpiNNaker and the TrueNorth chips, Loihi is being used as a research platform to support the development of spike-based processing architectures that can be applied to the solution of practical problems. Indeed, researchers and students are encouraged to develop novel spike-based computing solutions using Loihi, via the support of the Intel Neuromorphic Research Community (INRC) program, which has been very successful in promoting the growth of the community and producing a range of promising results [32].

Memristive devices and emerging memory technologies
The material science and device physics community has been carrying out research on new materials and technologies for memory and long-term storage applications for many years. Recently however, this community started using the term "neuromorphic" to refer to new devices and systems that, on one hand exhibit different types of behaviors that can be linked to those of biological synapses, and on the other represent important building blocks for the development of large-scale AI computing systems [33]. Indeed, shortly after the proposal of using these new nano-scale devices as "memristors" [34], this community quickly embraced the idea that these devices could be used to implement synapses in artificial neural networks, and to store locally their synaptic weight [35]. These devices and technologies hold a great promise of enabling "in-memory computing" in neural networks, and of supporting, through their physics, complex non-linear features that could be exploited to emulate many interesting properties of biological synapses. Within this context, a wide range of research efforts are being pursued, including the development of different types of non-volatile and volatile memristive devices, and the design of spike- or pulse-based control schemes for inducing biologically plausible learning behaviors in memristive cross-bar arrays [36, 37]. As there is not yet a single solution to the problem of finding the optimal artificial synapse, a broad range of materials, devices, and techniques are still an active area of investigation [38­44].

Algorithms and computational models of spike-based learning and inference
An important aspect of the neuromorphic computing and engineering domain is the one of hardware-software co-design. It is no surprise then that the term neuromorphic is also being used to describe research in computational models and algorithms that can be readily mapped onto memristive, CMOS, or hybrid memristive-CMOS neuromorphic architectures [45]. Most efforts in this domain focus either on exploring spike-based learning methods that approximate the backpropagation algorithm [46, 47], or on identifying local stochastic and complex non-linear plasticity mechanisms that can be reproduced by memristive devices or CMOS learning circuits [45, 48­51]. Very promising results are also being obtained by combining the latest advancements in AI

Editorial

6

Figure 2: Thinking outside the box: NCE will promote and inspire new, unconventional, and innovative ideas in neuromorphic computing and engineering by supporting cross-fertilization and promoting the convergence of the disciplines and thematic areas that characterize this emerging research field.
and machine learning algorithms with brain-inspired and computational neuroscience modeling efforts [52­55]. These investigations provide useful specifications for the design of new non-volatile and volatile memristive devices and for the design of spiking neural network chips that can use the principles of computation derived from the brain to carry out low-power and robust computation using local learning rules and unreliable and low-precision components, such as the ones present in animal brains.
3. Open challenges
The most important challenge that NCE faces is that of supporting a diverse and interdisciplinary community of researchers working on different aspects of neuromorphic computing and engineering. This journal will be instrumental in aligning the goals and objectives of this community, and in promoting its growth (see Fig. 2). The success of NCE strongly depends on how much the experts in the different thematic areas will be willing to broaden their horizon, learn the "language" of the experts of other thematic areas, and create synergies with them. This is indeed already happening to some degree: material science and device experts are collaborating with circuit designers to both extend current CMOS technologies and integrate nano-scale devices in newly developed ones; computer science and machine learning experts are collaborating with neuroscientists to develop brain-inspired processing theories that are compatible with the neuromorphic technologies being developed; electronic engineers, physicists and memory device experts are working closely with computational neuroscientists to implement devices and circuits that can emulate the biophysics of synapses and neurons onto the electronic circuits and systems being developed. By defining a common vision and by bridging the traditional boundaries between standard subjects, the NCE journal will enable the development and dissemination of breakthroughs in neuromorphic computing and engineering that will have disruptive effects, and which could revolutionize the

REFERENCES

7

nature of computation (see "thinking outside the box" Fig. 2).

Conclusions and outlook
Neuromorphic computing and engineering is becoming an extremely important and timely research area. Basic research in brain-inspired neuromorphic electronic circuits has been carried out already for a significant number of years. In parallel tremendous progress has been made in computational neuroscience, nanotechnologies, and machine learning. This is the best time to combine the know-how gained so far in these disciplines to obtain breakthroughs that can potentially solve many of the problems that ICT are starting to face. The NCE journal has been established to provide a new open access medium explicitly designed to support this convergence of efforts, and to bring together all researchers engaged in these areas in a way that unites the community and defines the field of "neuromorphic computing and engineering" for years to come.

References
[1] Mirembe Musisi-Nkambwe et al. "The Viability of Analog-based Accelerators for Neuromorphic Computing: a Survey". In: Neuromorphic Computing and Engineering (2021). doi: 10 . 1088 / 2634 - 4386 / ac0242. url: http : / / iopscience . iop.org/article/10.1088/2634-4386/ac0242.
[2] Nathan Leroux et al. "Hardware realization of the multiply and accumulate operation on radio-frequency signals with magnetic tunnel junctions". In: Neuromorphic Computing and Engineering (2021). doi: 10 . 1088 / 2634 - 4386 / abfca6. url: http://iopscience.iop.org/article/10.1088/2634-4386/abfca6.
[3] Yexin Yan et al. "Comparing Loihi with a SpiNNaker 2 Prototype on Low-Latency Keyword Spotting and Adaptive Robotic Control". In: Neuromorphic Computing and Engineering (2021). doi: 10.1088/2634-4386/abf150. url: http://iopscience. iop.org/article/10.1088/2634-4386/abf150.
[4] Herbert Jaeger. "Toward a generalized theory comprising digital, neuromorphic, and unconventional computing". In: Neuromorphic Computing and Engineering (2021). doi: 10.1088/2634-4386/abf151. url: http://iopscience.iop.org/article/ 10.1088/2634-4386/abf151.
[5] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. "Deep learning". In: Nature 521.7553 (2015), pp. 436­444.
[6] J. Schmidhuber. "Deep Learning in Neural Networks: An Overview". In: Neural Networks 61 (Jan. 2015), pp. 85­117. doi: 10.1016/j.neunet.2014.09.003.
[7] Luciano Floridi and Massimo Chiriatti. "GPT-3: Its nature, scope, limits, and consequences". In: Minds and Machines (2020), pp. 1­14.

REFERENCES

8

[8] G. Indiveri and S.-C. Liu. "Memory and information processing in neuromorphic systems". In: Proceedings of the IEEE 103.8 (2015), pp. 1379­1397. doi: 10.1109/ JPROC.2015.2444094.
[9] Carver Mead. "How we created neuromorphic engineering". In: Nature Electronics 3.7 (2020), pp. 434­435.
[10] C. Mead. "Neuromorphic Electronic Systems". In: Proceedings of the IEEE 78.10 (1990), pp. 1629­36.
[11] C.A. Mead. Analog VLSI and Neural Systems. Reading, MA: Addison-Wesley, 1989.
[12] R.J. Douglas, M.A. Mahowald, and C. Mead. "Neuromorphic analogue VLSI". In: Annual Review of Neuroscience 18 (1995), pp. 255­281.
[13] S.-C. Liu et al. Analog VLSI:Circuits and Principles. MIT Press, 2002. [14] J. Kramer. "An integrated optical transient sensor". In: IEEE Transactions on
Circuits and Systems II 49.9 (Sept. 2002), pp. 612­628. [15] P. Lichtsteiner, C. Posch, and T. Delbruck. "A 128x128 120 dB 30 mW
asynchronous vision sensor that responds to relative intensity change". In: 2006 IEEE ISSCC Digest of Technical Papers. IEEE. Feb. 2006, pp. 508­509. [16] B. Wen and K. Boahen. "A silicon cochlea with active coupling". In: Biomedical Circuits and Systems, IEEE Transactions on 3.6 (2009), pp. 444­455. [17] S.-C. Liu and T. Delbruck. "Neuromorphic sensory systems". In: Current Opinion in Neurobiology 20.3 (2010), pp. 288­295. doi: 10.1016/j.conb.2010.03.007. [18] Shih-Chii Liu et al. "Asynchronous Binaural Spatial Audition Sensor With 2x64 4 Channel Output". In: Biomedical Circuits and Systems, IEEE Transactions on 8.4 (2014), pp. 453­464. [19] Ben Varkey Benjamin et al. "Neurogrid: A Mixed-Analog-Digital Multichip System for Large-Scale Neural Simulations". In: Proceedings of the IEEE 102.5 (2014), pp. 699­716. [20] Ning Qiao et al. "A reconfigurable on-line learning spiking neuromorphic processor comprising 256 neurons and 128K synapses". In: Frontiers in neuroscience 9 (2015), p. 141. [21] S. Moradi et al. "A Scalable Multicore Architecture With Heterogeneous Memory Structures for Dynamic Neuromorphic Asynchronous Processors (DYNAPs)". In: Biomedical Circuits and Systems, IEEE Transactions on 12.1 (Feb. 2018), pp. 106­ 122. doi: 10.1109/TBCAS.2017.2759700. [22] J. Park et al. "Hierarchical Address Event Routing for Reconfigurable Large-Scale Neuromorphic Systems". In: IEEE Transactions on Neural Networks and Learning Systems (2016), pp. 1­15. doi: 10.1109/TNNLS.2016.2572164. [23] Mohammad Mahvash and Alice C. Parker. "Synaptic Variability in a Cortical Neuromorphic Circuit". In: IEEE Transactions on Neural Networks and Learning Systems 24.3 (2013), pp. 397­409. doi: 10.1109/TNNLS.2012.2231879.

REFERENCES

9

[24] Amitava Banerjee et al. "A current-mode spiking neural classifier with lumped dendritic nonlinearity". In: International Symposium on Circuits and Systems, (ISCAS), 2015. IEEE. 2015, pp. 714­717.
[25] Frank L Maldonado Huayaney, Stephen Nease, and Elisabetta Chicca. "Learning in Silicon Beyond STDP: A Neuromorphic Implementation of Multi-Factor Synaptic Plasticity With Calcium-Based Dynamics". In: IEEE Transactions on Circuits and Systems I: Regular Papers 63.12 (2016), pp. 2189­2199.
[26] Georgios Detorakis et al. "Neural and synaptic array transceiver: A brain-inspired computing framework for embedded learning". In: Frontiers in neuroscience 12 (2018), p. 583.
[27] Ning Qiao, Chiara Bartolozzi, and Giacomo Indiveri. "An Ultralow Leakage Synaptic Scaling Homeostatic Plasticity Circuit With Configurable Time Scales up to 100 ks". In: IEEE Transactions on Biomedical Circuits and Systems (2017). doi: 10.1109/TBCAS.2017.2754383.
[28] J. Schemmel et al. "A wafer-scale neuromorphic hardware system for large-scale neural modeling". In: Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on. IEEE. 2010, pp. 1947­1950.
[29] S.B. Furber et al. "The SpiNNaker Project". In: Proceedings of the IEEE 102.5 (May 2014), pp. 652­665. issn: 0018-9219. doi: 10.1109/JPROC.2014.2304638.
[30] Paul A. Merolla et al. "A million spiking-neuron integrated circuit with a scalable communication network and interface". In: Science 345.6197 (Aug. 2014), pp. 668­ 673. issn: 0036-8075, 1095-9203.
[31] Mike Davies et al. "Loihi: A neuromorphic manycore processor with on-chip learning". In: IEEE Micro 38.1 (2018), pp. 82­99.
[32] Mike Davies et al. "Advancing Neuromorphic Computing With Loihi: A Survey of Results and Outlook". In: Proceedings of the IEEE 109.5 (2021), pp. 911­934. doi: 10.1109/JPROC.2021.3067593.
[33] Massimiliano Di Ventra, Yuriy V. Pershin, and Leon O. Chua. "Circuit Elements With Memory: Memristors, Memcapacitors, and Meminductors". In: Proceedings of the IEEE 97.10 (2009), pp. 1717­1724. doi: 10.1109/JPROC.2009.2021077.
[34] Dmitri B Strukov et al. "The missing memristor found". In: nature 453.7191 (2008), pp. 80­83.
[35] S. H. Jo et al. "Nanoscale memristor device as synapse in neuromorphic systems". In: Nano letters 10.4 (2010), pp. 1297­1301.
[36] Carlos Zamarreño-Ramos et al. "On spike-timing-dependent-plasticity, memristive devices, and building a self-learning visual cortex". In: Frontiers in Neuroscience 5 (2011).
[37] S. Saighi et al. "Plasticity in memristive devices". In: Frontiers in Neuroscience 9.51 (2015).

REFERENCES

10

[38] Rainer Waser and Masakazu Aono. "Nanoionics-based resistive switching memories". In: Nature materials 6.11 (2007), pp. 833­840.
[39] J Joshua Yang et al. "Memristive switching mechanism for metal/oxide/metal nanodevices". In: Nature Nanotechnology 3.7 (2008), pp. 429­433.
[40] A. Chanthbouala et al. "A ferroelectric memristor". In: Nature materials 11.10 (2012), pp. 860­864. doi: 10.1038/nmat3415.
[41] Stefano Ambrogio et al. "Unsupervised learning by spike timing dependent plasticity in phase change memory (PCM) synapses". In: Frontiers in neuroscience 10 (2016).
[42] Martin Salinga et al. "Monatomic phase change memory". In: Nature materials 17.8 (2018), pp. 681­685.
[43] Yoeri van De Burgt et al. "Organic electronics for neuromorphic computing". In: Nature Electronics 1.7 (2018), pp. 386­397.
[44] Abu Sebastian, Manuel Le Gallo, and Evangelos Eleftheriou. "Computational phase-change memory: beyond von Neumann computing". In: Journal of Physics D: Applied Physics 52.44 (Aug. 2019), p. 443002. doi: 10.1088/1361-6463/ab37b6.
[45] Elisabetta Chicca and Giacomo Indiveri. "A recipe for creating ideal hybrid memristive-CMOS neuromorphic processing systems". In: Applied Physics Letters 116.12 (2020), p. 120501. doi: 10.1063/1.5142089.
[46] Emre O Neftci, Hesham Mostafa, and Friedemann Zenke. "Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to spiking neural networks". In: IEEE Signal Processing Magazine 36.6 (2019), pp. 51­63.
[47] Guillaume Bellec et al. "A solution to the learning dilemma for recurrent networks of spiking neurons". In: bioRxiv (2020), p. 738385.
[48] Melika Payvand et al. "Error-triggered three-factor learning dynamics for crossbar arrays". In: International Conference on Artificial Intelligence Circuits and Systems (AICAS). IEEE. 2020, pp. 218­222. doi: 10.1109/AICAS48895.2020. 9073998.
[49] Melika Payvand et al. "A neuromorphic systems approach to in-memory computing with non-ideal memristive devices: From mitigation to exploitation". In: Faraday Discussions 213 (2019), pp. 487­510.
[50] Stefano Brivio et al. "Extended memory lifetime in spiking neural networks employing memristive synapses with nonlinear conductance dynamics". In: Nanotechnology 30.1 (2019), p. 015102. url: http://stacks.iop.org/0957- 4484/ 30/i=1/a=015102.
[51] Axel Laborieux et al. "Synaptic metaplasticity in binarized neural networks". In: Nature Communications 12.1 (2021), pp. 1­12.
[52] R. Berdan et al. "Emulating short-term synaptic dynamics with memristive devices". In: Scientific Reports 6.18639 (2016), pp. 1­9. doi: 10.1038/srep18639.

REFERENCES

11

[53] Nick Diederich et al. "A memristive plasticity model of voltage-based STDP suitable for recurrent bidirectional neural networks in the hippocampus". In: Scientific Reports (Nature Publisher Group) 8 (2018), pp. 1­12.
[54] T. Serrano-Gotarredona et al. "STDP and STDP Variations with Memristors for Spiking Neuromorphic Learning Systems". In: Frontiers in Neuroscience 7.2 (2013). issn: 1662-453X. doi: 10 . 3389 / fnins . 2013 . 00002. url: http : / / www . frontiersin.org/neuroscience/10.3389/fnins.2013.00002/full.
[55] Timothy P Lillicrap and Adam Santoro. "Backpropagation through time and the brain". In: Current Opinion in Neurobiology 55 (2019). Machine Learning, Big Data, and Neuroscience, pp. 82­89. issn: 0959-4388. doi: 10.1016/j.conb. 2019 . 01 . 011. url: https : / / www . sciencedirect . com / science / article / pii / S0959438818302009.

