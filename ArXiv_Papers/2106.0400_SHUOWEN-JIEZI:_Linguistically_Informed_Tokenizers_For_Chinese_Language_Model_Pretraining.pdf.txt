SHUOWEN- JIEZI: Linguistically Informed Tokenizers For Chinese Language Model Pretraining
Chenglei Si1, Zhengyan Zhang2,3,4, Yingfa Chen2,3,4, Fanchao Qi2,3,4, Xiaozhi Wang2,3,4, Zhiyuan Liu2,3,4, Maosong Sun2,3,4 1University of Maryland, College Park, MD, USA
2Department of Computer Science and Technology, Tsinghua University, Beijing, China 3Institute for Artificial Intelligence, Tsinghua University, Beijing, China
4State Key Lab on Intelligent Technology and Systems, Tsinghua University, Beijing, China
clsi@terpmail.umd.edu, zy-z19@mails.tsinghua.edu.cn

arXiv:2106.00400v1 [cs.CL] 1 Jun 2021

Abstract
Conventional tokenization methods for Chinese pretrained language models (PLMs) treat each character as an indivisible token (Devlin et al., 2019), which ignores the characteristics of Chinese writing system. In this work, we comprehensively study the influences of three main factors on the Chinese tokenization for PLM: pronunciation, glyph (i.e., shape) and word boundary. Correspondingly, we propose three kinds of tokenizer: 1) SHUOWEN (, meaning Talk Word), the pronunciation-based tokenizers; 2) JIEZI (, meaning Solve Character), the glyph-based tokenizers; 3) Word segmented tokenizers, the tokenizers with Chinese word segmentation. To empirically compare the effectivenesses of studied tokenizers, we pretrain BERT-style language models with them and evaluate the models on various downstream NLU tasks. We find that SHUOWEN and JIEZI tokenizers can generally outperform conventional singlecharacter tokenizers, while Chinese word segmentation shows no benefit as a preprocessing step. Moreover, the proposed SHUOWEN and JIEZI tokenizers exhibit significantly better robustnesses on handling noisy texts. The code and pretrained models will be publicly released to facilitate linguistically informed Chinese NLP. 1
1 Introduction
Large-scale Transformer-based pretrained language models (PLMs) (Devlin et al., 2019; Lan et al., 2020; Clark et al., 2020; Ma et al., 2020; He et al., 2021, inter alia) have achieved great success in recent years and attracted wide research interest, in which the tokenization plays a fundamental role.
1Please refer to the Appendix A.1 for the historical meaning of SHUOWEN-JIEZI.
 Equal contribution  Corresponding author email: liuzy@tsinghua.edu.cn

Unfortunately, current tokenization methods are mostly developed primarily for English (Bostrom and Durrett, 2020). Almost all the current PLMs adopt the sub-word tokenization method originating from machine translation, such as the BytePair Encoding (Sennrich et al., 2016), WordPiece (Schuster and Nakajima, 2012; Devlin et al., 2019) and SentencePiece based on the unigram language model (Kudo and Richardson, 2018). While the idea of sub-word tokenization is intuitive and effective for morphological-rich synthetic languages, it is not the case for Chinese.
We believe that it is crucial to develop tailored techniques for the languages beyond English because there can be huge differences between different languages (Bender, 2019, #BenderRule). Towards this end, we devote this work to analysing three unique linguistic characteristics of Chinese (writing system) compared to English: 1) The Chinese writing system is morphemic (Hill, 2016), which means the Chinese characters poorly reflect the pronunciation, resulting in the conventional character-based tokenization misses much more phonological information. 2) Modern Chinese words basically do not undergo morphological alternations (Packard, 2000), thus rendering sub-word tokenization inapplicable. However, Chinese characters are mainly logograms, which means their glyphs, the composition of stokes and radicals, also contain rich semantic information (Wu et al., 2019). 3) In Chinese writing, there is no natural word boundary like the space in English writing. Although it is possible to inject word boundaries via Chinese word Segmentation (CWS), there is no study on how this works for Chinese PLMs.
Targeting the three factors, we then explore three corresponding tokenization strategies: 1) A pronunciation-based tokenizer family called SHUOWEN, which first romanizes the Chinese characters based on their pronunciations, and then

constructs the vocabulary with the romanized scripts using the unigram language model (Kudo and Richardson, 2018). 2) A glyph-based tokenizer family called JIEZI, which decomposes characters into combinations of Chinese strokes or radicals, and then constructs the vocabulary with the stroke or radical sequences using the unigram language model. 3) A word segmented tokenizer family, which first uses a Chinese word segmenter to segment Chinese texts into words, and then constructs the vocabulary with the segmented word sequences using the unigram language model.
We pretrain BERT-style PLMs using the proposed tokenizers from scratch and evaluate the resultant models on various downstream tasks. Through comprehensive evaluation on ten Chinese NLU tasks, we find that our pronunciation-based (SHUOWEN) and glyph-based (JIEZI) tokenizers outperform conventional single-character tokenizers in most tasks. Furthermore, as they have the unique advantage to learn the meanings of complex characters through the composition of simpler sub-characters, they are naturally more robust on handling noisy input. Surprisingly, we find that Chinese Word Segmentation (CWS) has no benefit for Chinese language model pretraining.
Our work suggests that linguistically informed techniques based on the characteristics of different languages need more attention. We will release the code, pretrained models, and the SHUOWENJIEZI tokenizers to serve as a better foundation for future research on Chinese PLM.
2 Related Work
Chinese PLM. Several previous works have explored techniques to improve Chinese language model pretraining. Zhu (2020) and Zhang et al. (2021) expanded BERT vocabulary with Chinese words apart from the single characters and incorporated them in the pretraining objectives. Xiao et al. (2021) and Cui et al. (2019) considered coarse-grained information through masking ngram and whole words during the masked language modeling pretraining. Diao et al. (2020) incorporated word-level information via superimposing the character and word embeddings. Lai et al. (2021) incorporated Chinese word lattice structures in the pretraining. Linguistically Informed Techniques for Chinese. CWS is a common preprocessing step for Chinese NLP tasks (Li and Sun, 2009). Meng

et al. (2019) empirically analysed whether CWS is helpful for downstream Chinese NLP tasks before the PLM era and found that in many cases the answer is negative. We examine the impact of CWS for PLM instead. Wu et al. (2019) incorporated glyph information of Chinese characters though adding extra encoders to encode the images of Chinese characters and then combine them with the character embeddings. We do not intend to fuse in additional information from sources like images, but instead, all of our proposed tokenzation methods are drop-in replacements to the existing single-character tokenizers, without adding any extra layers or parameters. Tan et al. (2018) explore to Chinese text into Wubi sequences that represent character glyph information for the task of machine translation.
3 Method
In this section, we introduce our proposed tokenization methods.
3.1 SHUOWEN: Pronunciation-based Tokenizers
The Chinese writing system is morphemic (Hill, 2016) and barely convey phonological information. However, the pronunciation of Chinese characters also reveals semantic patterns (Duanmu, 2007) and has long been widely used as input methods in China (e.g., pinyin). In order to capture such information, we propose a pronunciationbased tokenizer named SHUOWEN.
On raw Chinese input texts (e.g., ), SHUOWEN performs the following steps:
1. Romanize the text using Chinese transliteration systems. In this work, we explore two different transliteration methods: pinyin and zhuyin (i.e., bopomofo). Pinyin uses the Latin alphabet and four 2 different tones (¯, ´, , `) to romanize pronunciations of characters, e.g.,   Chi¯ Mei` Wang Liang. On the other hand, zhuyin uses a set of selfinvented characters and the same four tones to romanize the characters, e.g.,   `  . Note that in zhuyin, the first tone mark (¯) is usually omitted.
2. Insert special separation symbols (+) after each character's romanized sequence, e.g.,
2The light tone is sometimes considered as the fifth tone but we omit it for simplicity.

Chi¯+Mei`+Wang+Liang+, +`+ ++. This prevents cases where romanized sequences of different characters are mixed together, especially when there are no tone markers to split them in zhuyin.
3. Different Chinese characters often have the same pronunciation. For disambiguation, we append different indices after the romanized sequences for the homophonic characters, so that allowing a biunique mapping between each Chinese character and its romanized sequence, e.g., Chi¯33+Mei`24+Wang25+Liang13+, 10+`3+6+1+.
4. Apply a unigram language model (ULM) as in Kudo and Richardson (2018) on the romanized sequences to build the final vocabulary.
We do not set any constraint on the vocabulary other than the vocabulary size. The resultant vocabulary contains tokens corresponding to flexible combinations of characters and sub-characters.
3.2 JIEZI: Glyph-based Tokenizers
The word shapes of Chinese characters contain rich semantic information and can help NLP models (Cao et al., 2018). For example, most Chinese characters can be broken down into semantically meaningful radicals. Characters that share common radicals often share related semantic information, such as the four characters `' all share the same radical `' (meaning ghost), and their meanings are indeed related to ghosts and monsters. 3
However, the prevailing tokenization method for Chinese treats each Chinese character as a separate token and hence preventing the model to learn the shared semantics of characters with common radicals. In order to solve this problem, we propose the glyph-based tokenizer JIEZI, which performs the following steps on raw Chinese input (e.g., ):
1. Convert each character into a stroke or radical sequence. To convert into stroke sequences, we use Latin alphabet to represent the basic strokes and convert the char-
3Interestingly, the word `' is in fact a Chinese idiom, which is now often used to refer to bad people who are like monsters.

acters based on the standard stroke orders4, e.g.,   pszhshpzznnhpnzsszshn;   pszhshpzznhhspn. To convert into radical sequences, we adopt three existing glyph-based Chinese input methods: Wubi, Zhengma, Cangjie. These methods group strokes together in different ways to form radicals or stroke combinations, and then represent characters with them. We use Latin alphabet to represent these radicals or stroke combinations, e.g.,   Wubi: rqcc rqci rqcn rqcw; Zhengma: njlz njbk njld njoo; Cangjie: hiyub hijd hibtv himob.
2. Similar to the pronunciation-based tokenizers, we add the same separation symbol after each character, and also add the disambiguation indices for characters whose stroke sequences are identical (e.g.,  (people) and  (eight)).
In the converted sequences, we can see how common radicals naturally appear (the underlined parts). Please refer to the Appendix A.2 for a detailed explanation on the differences between these different input methods involved.
3.3 Word Segmented Tokenizer
Chinese Word Segmentation (CWS) is a common technique to split Chinese chunks into a sequence of Chinese words. The resultant segmented words sometimes provide better granularity for downstream tasks (e.g., Chang et al., 2008). However, the impact of CWS is unclear in the context of pretraining, especially how it interplays with statistical approaches like BPE and unigram LM. Hence, we study word segmented tokenizers that performs the following process on raw Chinese input, e.g., "(this paper is interesting)" :
1. We use a state-of-the-art segmenter THULAC (Li and Sun, 2009) to segment the sentence into a sequence of words joined by spaces, e.g., `\\\' (We use \to indicate a blank space for easier reading.)
2. We directly apply ULM on these space-joined sequences to construct the vocabulary.
In other words, CWS is used as a preprocessing step on training corpora when we build the vocabulary using above-mentioned methods. When
4https://en.wikipedia.org/wiki/Stroke_ order

Dataset
TNEWS IFLYTEK BQ THUCNEWS CLUEWSC AFQMC CSL OCNLI CHID C3

Task
DC DC SPM DC WSC SPM SPM SPM MRC MRC

MaxLen
256 256 256 256 256 256 256 256 96 512

Batch
32 32 32 32 32 32 32 32 24 24

Epoch
6 6 6 6 24 6 6 6 6 6

#Train
53.4K 12.1K 100K 669K 1.2K 34.3K 20K 45.4K 519K 12K

#Dev
10K 2.6K 10K 83.6K 0.3K 4.3K 3K 5K 57.8K 3.8K

#Test
10K 2.6K 10K 83.6K 0.3K 3.9K 3K 3K 23K 3.9K

Domain
News App Description
Bank Service News
Literature Financial Academic Papers
Mixed Mixed Mixed

Table 1: Hyper-parameters and statistics of different datasets. DC: document classification. SPM: sentence pair matching (including natural language inference). WSC: Winograd Schema Challenge. MRC: machine reading comprehension.

we perform the actual pretraining and finetuning, we also perform CWS as preprocessing before tokenization using the word segmented tokenizer.
4 Experiment
4.1 Baselines
We use two strong baseline tokenizers in this work. The first one is the conventional singlecharacter tokenizer as used in BERT-Chinese and many other follow-up Chinese PLM (e.g., Cui et al., 2019, 2020). We name this tokenizer BERTChinese as it originated from the Chinese version of BERT.
For the second baseline, we directly apply SentencePiece with unigram LM on the raw Chinese corpus to generate the vocabulary. As a result, the vocabulary contains both single characters and words (i.e., character combinations). This approach resembles the vocabulary of some recent multi-granularity Chinese PLM variants such as AMBERT (Zhang et al., 2021) and LatticeBERT (Lai et al., 2021). Unlike them, we do not add any new model designs or pretraining objectives, but instead use the original BERT architecture and masked LM objective. We name this baseline SP-ULM.
To ensure a fair comparison, we set the same vocabulary size of 22675 for all tokenizers. We use the same training corpus to train all the tokenizers. We use the SentencePiece library's unigram LM implementation to train the tokenizers.
In order to evaluate the effectiveness of the tokenizers, we pretrain a BERT model using each tokenizer and compare their performance on downstream tasks. When pretraining the BERT models using each tokenizer, we use the same pretraining

corpus and the same set of hyper-parameters for all models being compared. Notably, we also repretrain the BERT model using the BERT-Chinese tokenizer on our pretraining corpus instead of just loading from existing checkpoints to ensure that all baselines and proposed methods are directly comparable. Since our proposed tokenizers are direct drop-in replacements for the baseline tokenizers, they do not incur any extra parameters. As a result, all the models being compared have the same number of parameters, allowing for a truly apple-to-apple comparison.
4.2 Datasets
We evaluate the trained models with different tokenization methods on a total of ten different downstream datasets, including single-sentence tasks, sentence-pair tasks, as well as reading comprehension tasks. We briefly introduce each dataset below and present the dataset statistics in Table 1. TNEWS (Co., 2018) is a news title classification dataset containing 15 classes. We use the split as released in Xu et al. (2020). IFLYTEK (Co., 2019) is a long text classification dataset containing 119 classes. The task is to classify mobile applications into corresponding categories given their description. BQ (Chen et al., 2018) is a sentence-pair question matching dataset extracted from an online bank customer service log. The goal is to evaluate whether two questions are semantically identical or can be answered by the same answer. THUCNEWS (Li and Sun, 2007) is a document classification dataset with 14 classes. The task is to classify news into the corresponding categories given their title and content.

TNEWS IFLY THUC BQ WSC AFQMC CSL OCNLI CHID C3 AVG

6-layer

BERT-Chinese

64.10 57.77 96.97 81.98 62.39 68.95 82.60 68.46 72.33 53.51 70.91

SP-ULM

64.26 55.44 97.09 81.52 62.06 69.88 83.16 68.98 72.77 51.73 70.69 (-0.22)

JIEZI-CangJie

63.86 59.51 97.04 81.59 63.27 70.47 82.91 69.03 72.73 52.67 71.31 (+0.40)

JIEZI-Stroke

63.81 58.74 96.87 81.55 62.94 69.66 82.44 68.02 72.21 53.35 70.96 (+0.05)

JIEZI-Zhengma

63.96 58.74 96.99 82.27 61.95 69.86 83.46 68.56 72.12 54.91 71.28 (+0.37)

JIEZI-Wubi

64.91 59.39 97.03 81.41 62.72 69.14 82.60 69.12 72.02 53.99 71.16 (+0.25)

SHUOWEN-Pinyin 63.58 59.55 97.04 81.65 63.60 68.60 82.66 67.93 72.81 53.02 71.04 (+0.13)

SHUOWEN-Zhuyin 64.11 59.16 97.01 81.64 63.93 68.53 82.86 69.39 71.48 54.59 71.27 (+0.36)

12-layer

BERT-Chinese

65.07 58.01 97.05 82.33 73.14 71.04 83.90 70.19 76.61 55.90 73.32

SP-ULM

65.01 58.98 97.20 82.99 73.36 70.93 83.45 70.46 77.28 57.70 73.74 (+0.42)

JIEZI-CangJie

64.26 60.29 97.15 83.48 71.16 71.48 83.68 71.50 76.82 57.99 73.78 (+0.46)

JIEZI-Stroke

65.11 59.75 97.09 82.88 70.72 71.64 83.63 70.03 77.45 59.68 73.53 (+0.21)

JIEZI-Zhengma

64.51 60.78 97.14 83.15 72.15 70.76 83.68 71.22 76.72 57.49 73.76 (+0.44)

JIEZI-Wubi

64.47 60.05 97.16 82.76 72.70 72.00 83.62 70.77 76.34 58.31 73.82 (+0.50)

SHUOWEN-Pinyin 64.50 60.40 97.17 83.13 70.18 71.37 84.12 71.97 76.11 58.05 73.70 (+0.38)

SHUOWEN-Zhuyin 64.50 59.98 97.09 82.99 73.03 71.83 83.82 71.74 76.74 57.23 73.90 (+0.58)

Table 2: Results for standard evaluation. Best result on each dataset of each model size is boldfaced. The numbers in brackets in the last column indicate the average difference compared to the BERT-Chinese baseline.

SP-ULM SP-ULM + CWS JIEZI-Wubi JIEZI-Wubi + CWS SHUOWEN-Zhuyin SHUOWEN-Zhuyin + CWS

TNEWS 64.26 64.26 64.91 63.66 64.11 63.37

IFLYTEK 55.44 54.15 59.39 59.22 59.16 57.24

CLUEWSC 62.06 63.05 62.72 63.16 63.93 62.83

AFQMC 69.88 69.62 69.14 68.65 68.53 68.94

CSL 83.16 82.87 82.60 82.21 82.86 82.12

OCNLI 68.98 68.64 69.12 68.81 69.39 68.69

C3 51.73 51.77 53.99 52.76 54.59 51.48

AVG 65.07 64.91 65.98 65.50 66.08 64.95

Table 3: Results of models trained with Word Segmented Tokenization. All models are 6-layer.

CLUEWSC (Xu et al., 2020) is a coreference resolution dataset in the format of Winograd Schema Challenge. The task is to determine whether the given noun and pronoun in the sentence co-refer.
AFQMC is the Ant Financial Question Matching Corpus for the question matching task that aims to predict whether two sentences are semantically similar
CSL is the Chinese Scientific Literature dataset extracted from academic papers. Given an abstract and some keywords, the goal is to determine whether they belong to the same paper. It is formatted as a sentence-pair matching task.
OCNLI (Hu et al., 2020) is a natural language inference dataset. The task is to determine whether the relationship between the hypothesis and premise is entailment, neural, or contradiction.
CHID (Zheng et al., 2019) is a cloze-style reading comprehension dataset where . Given contexts where some idioms are masked, the task is to select the appropriate idiom from a list of candidates.

C3 (Sun et al., 2019) is a multiple choice machine reading comprehension dataset. The goal is to choose the correct answer for some questions given a context.
4.3 Experiment Setup: Standard Evaluation
For each tokenizer, we pretrain a 6-layer and 12layer BERT style model using the Baidu Baike corpus (Zhang et al., 2020) which has 2.2G of raw text before processing. The model configuration is exactly the same for all models: 6 or 12 layers, 12 attention heads, intermediate size 3072, hidden size 768. For pretraining, we follow the original BERT paper's two-stage pretraining procedure where we first train with sequence length 128 for 8k steps and then train with sequence length 512 for 4k steps. We only keep the masked language modeling objective in pretraining and discard the next sentence prediction objective as suggested in RoBERTa (Liu et al., 2019). During fine-tuning, we use the set of hyper-parameters as shown in Table 1. For all experiments in this paper, we report results of the average run of three different random

BERT-Chinese SP-ULM JIEZI-Wubi
BERT-Chinese SP-ULM JIEZI-Wubi

clean
63.99 63.99 64.05
68.07 69.10 68.43

15% TNEWS 62.18 62.88 62.80 OCNLI 62.60 64.00 67.10

30%
60.49 60.79 62.56
56.83 56.57 65.47

45%
57.64 59.20 62.71
51.37 52.43 65.40

60%
53.97 55.42 62.81
46.00 46.73 64.80

Table 4: Results for noisy evaluation with glyph noises.

BERT-Chinese SP-ULM SHUOWEN-Pinyin SHUOWEN-Zhuyin
BERT-Chinese SP-ULM SHUOWEN-Pinyin SHUOWEN-Zhuyin

clean 15% TNEWS
63.99 60.87 63.99 61.52 63.35 60.60 63.79 60.99
OCNLI 68.07 61.73 69.10 62.23 67.83 61.00 69.63 60.77

30%
57.70 58.60 57.45 57.69
54.50 54.77 54.33 54.77

45%
52.21 53.30 53.61 53.15
49.97 50.33 49.87 50.67

60%
45.51 46.81 49.29 48.98
44.80 45.70 45.10 47.70

Table 5: Results for noisy evaluation with phonology noises.

seeds.
4.4 Experiment Setup: Noisy Evaluation
Apart from evaluating on the standard benchmarks, we also evaluate in a noisy setting to illustrate the advantage of our proposed tokenization methods to handle noisy inputs. Specifically, we inject two types of synthetic noises into both the training and test data in order to test whether the models can learn from noisy training data and also perform robustly on noisy test data. We vary the ratio of noise in the data to examine the impact. The two types of noises we inject are:
· Glyph-based noise: we replace original characters with other characters that have similar glyph but have different semantic meanings (e.g.,  (wall) and  (jade)). Specifically, we obtain a substitution candidate list for each character, where the candidates are selected so that they share at least one common radical with the original character. Then, we randomly sample a certain ratio r% of the original characters, for each of them, we randomly sample a substitution character from its candidate list for substitution. This simulates common noises when people use glyph-based input methods where these sim-

ilar characters could be chosen since their input encoding are similar.
· Pronunciation-based noise: we replace original characters with other characters that have the same pronunciation but different semantic meanings (e.g.,  (real) and  (needle)). Specifically, we obtain a substitution candidate list for each character, where all the candidates have the same pronunciation as the original character. Then, similarly, we randomly sample a certain ratio r% of the original characters, for each of them, we randomly sample a substitution character from its candidate list for substitution. This simulates the common noise when users use pronunciationbased input methods where the input encoding of these characters and their substitutions are the same.
For our experiment, we vary the noise ratio r% within the range of {0, 15%, 30%, 45% 60%}. For the sampled characters to be replaced with noises, we randomly sample a substitution character from their candidate list for every appearance of the character, instead of substituting with the same candidate. This induces more noise variations. Note that some substitutions are both glyph-

based noises and pronunciation-based noises (e.g.,  and  share a same radical and also have the same pronunciation), we keep them in both types of noises.
Intuitively, our SHUOWEN tokenizer could be robust to pronunciation-based noises and JIEZI tokenizer could be robust to glyph-based noises because the substitution characters share similar pronunciation or glyph components with the original characters, which may be captured by our tokenizers.
This noisy setup is reflective of real-life use cases where user queries often contain such noises. Since most Chinese people use either glyph-based input methods (e.g., wubi) or pronunciation-based input methods (e.g., pinyin, zhuyin), such mis-typed characters can be very common. This highlights the potential impact of our work.
5 Results
5.1 Standard Evaluation
We compare the results of the baseline tokenizers (BERT-Chinese, SP-ULM) with our proposed JIEZI (including four variants: JIEZI-Cangjie, JIEZI-Stroke, JIEZI-Zhengma, JIEZI-Wubi) and SHUOWEN (including two variants: SHUOWENPinyin and SHUOWEN-Zhuyin) tokenizers in Table 2.
Despite some variations across different datasets, we observe that in terms of the average score over ten datasets, all of our proposed tokenizers outperform the BERT-Chinese baseline. Notably, for the 6-layer model size, our JIEZI-Cangjie tokenizer obtains an average of 0.40 points over the BERT-Chinese tokenizer, for the 12-layer model size, our SHUOWEN-Zhuyin tokenizer achieves an average of 0.58 points of improvement over the BERT-Chinese tokenizer.
These results indicate that on standard benchmarks, our proposed tokenizers can match or outperform the existing tokenizers for Chinese.
On the other hand, we examine the impact of CWS by comparing three tokenizers with their word segmented counterparts in Table 3. We can see that adding CWS as a preprocessing actually slightly decreased the average performance on downstream tasks.

5.2 Noisy Evaluation
We perform noisy evaluation on two datasets: TNEWS and OCNLI. For glyph-based noises, we compare baselines BERT-Chinese and SP-ULM with our JIEZI-Wubi. The results are presented in Table 4. We observe that when the noise ratio increases, the advantage of JIEZI is particularly large. For example, when 60% characters are substituted, JIEZI-Wubi still performs close to the original performance, while other baselines suffer large drops in performance. On OCNLI, the gap can be as large as 18 points in accuracy.
For pronunciation-based noises, we compare baselines BERT-Chinese and SP-ULM with our SHUOWEN-Pinyin and SHUOWEN-Zhuyin. The results are shown in Table 5. Unlike the cases on glyph-based noises, we observe that the advantage of our SHUOWEN tokenizers are not so significant compared to the baselines. One potential reason is that there many Chinese characters with the same pronunciation. Unlike how the semantic meanings of radicals can be rather consistent across different characters, phoneme combinations can have vastly different meanings across different characters (i.e., characters may pronounce the same but have totally different semantic meanings), which makes it difficult to learn these different semantic meanings in the pronunciation-based token embedding.
6 Conclusion
In this paper, we have explored three linguistically informed tokenization methods motivated by the unique linguistic characteristics of the Chinese writing system. Specifically, we find that pronunciation-based and glyph-based tokenizers can match or outperform the conventional Chinese tokenizers and Chinese word segmentation is not a useful addition for the tokenizer. Moreover, we find that our glyph-based tokenizers achieve large gains on noisy input as compared to the baselines, while our pronunciation-based tokenizers obtain limited success. This highlights the potential advantage of our proposed methods in real-life scenarios with noisy data. We believe that our work sets an important example of exploiting the unique linguistic property of a language beyond English to develop more tailored techniques, which should be an important direction for the global NLP community.

References
Emily Bender. 2019. The #BenderRule: On Naming the Languages We Study and Why It Matters. The Gradient.
Kaj Bostrom and Greg Durrett. 2020. Byte Pair Encoding is Suboptimal for Language Model Pretraining. In Findings of EMNLP.
Shaosheng Cao, Wei Lu, Jun Zhou, and Xiaolong Li. 2018. cw2vec: Learning Chinese Word Embeddings with Stroke n-gram Information. In AAAI.
Pi-Chuan Chang, Michel Galley, and Christopher D. Manning. 2008. Optimizing Chinese Word Segmentation for Machine Translation Performance. In WMT@ACL.
Jing Chen, Qingcai Chen, Xin Liu, Haijun Yang, Daohe Lu, and Buzhou Tang. 2018. The BQ Corpus: A Large-scale Domain-specific Chinese Corpus For Sentence Semantic Equivalence Identification. In EMNLP.
Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. 2020. ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators. In ICLR.
IFLYTEK Co. 2019. IFLYTEK: A multiple categories Chinese text classifier.
TouTiao Co. 2018. TNEWS Dataset.

San Duanmu. 2007. The Phonology of Standard Chinese. Oxford University Press.
Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2021. DeBERTa: Decodingenhanced BERT with Disentangled Attention. In ICLR.
Archibald A Hill. 2016. The typology of writing systems. In Papers in linguistics in honor of Leon Dostert, pages 92­99. De Gruyter Mouton.
Hai Hu, Kyle Richardson, Liang Xu, Lu Li, Sandra Kübler, and Lawrence S. Moss. 2020. OCNLI: Original Chinese Natural Language Inference. In Findings of EMNLP.
Taku Kudo and John Richardson. 2018. SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing. In EMNLP.
Yuxuan Lai, Yijia Liu, Yansong Feng, Songfang Huang, and Dongyan Zhao. 2021. LatticeBERT: Leveraging Multi-Granularity Representations in Chinese Pre-trained Language Models.
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020. ALBERT: A Lite BERT for Selfsupervised Learning of Language Representations. In ICLR.

Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, and Guoping Hu. 2020. Revisiting Pre-Trained Models for Chinese Natural Language Processing. In Findings of EMNLP.
Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang, Shijin Wang, and Guoping Hu. 2019. Pre-Training with Whole Word Masking for Chinese BERT. arXiv, abs/1906.08101.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In NAACL-HLT.

Jingyang Li and Maosong Sun. 2007. Scalable Term Selection for Text Categorization. In CoNLL.
Zhongguo Li and Maosong Sun. 2009. Punctuation as Implicit Annotations for Chinese Word Segmentation. Computational Linguistics.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv, abs/1907.11692.

Shizhe Diao, Jiaxin Bai, Yan Song, Tong Zhang, and Yonggang Wang. 2020. ZEN: Pre-training Chinese Text Encoder Enhanced by N-gram Representations. In Findings of EMNLP.

Wentao Ma, Yiming Cui, Chenglei Si, Ting Liu, Shijin Wang, and Guoping Hu. 2020. CharBERT: Character-aware Pre-trained Language Model. In COLING.

Yuxian Meng, Xiaoya Li, Xiaofei Sun, Qinghong Han, Arianna Yuan, and Jiwei Li. 2019. Is Word Segmentation Necessary for Deep Learning of Chinese Representations? In ACL.
Jerome L. Packard. 2000. The Morphology of Chinese: A Linguistic and Cognitive Approach. Cambridge University Press.
Mike Schuster and Kaisuke Nakajima. 2012. Japanese and Korean voice search. ICASSP.
Rico Sennrich, B. Haddow, and Alexandra Birch. 2016. Neural Machine Translation of Rare Words with Subword Units. arXiv, abs/1508.07909.
Kai Sun, Dian Yu, Dong Yu, and Claire Cardie. 2019. Investigating Prior Knowledge for Challenging Chinese Machine Reading Comprehension. TACL.
Mi Xue Tan, Yuhuang Hu, Nikola I. Nikolov, and Richard H. R. Hahnloser. 2018. wubi2en: Character-level Chinese-English Translation through ASCII Encoding. In WMT.
Wei Wu, Yuxian Meng, F. Wang, Qinghong Han, Muyu Li, Xiaoya Li, Jie Mei, Ping Nie, Xiaofei Sun, and Jiwei Li. 2019. Glyce: Glyphvectors for Chinese Character Representations. In NeurIPS.
Dongling Xiao, Yukun Li, Han Zhang, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. 2021. ERNIE-Gram: Pre-Training with Explicitly NGram Masked Language Modeling for Natural Language Understanding. In NAACL.
Liang Xu, Hai Hu, Xuanwei Zhang, Chenjie Cao Lu Li, Yudong Li, Yechen Xu, Kai Sun, Dian Yu, Cong Yu, Yin Tian, Qianqian Dong, Weitang Liu, Bo Shi, Yiming Cui, Junyi Li, Jun Zeng, Rongzhao Wang, Weijian Xie, Yanting Li, Yina Patterson, Zuoyu Tian, Yiwen Zhang, He Zhou, Shaoweihua Liu, Zhe Zhao, Qipeng Zhao, Cong Yue, Xinrui Zhang, Zhengliang Yang, Kyle Richardson, and Zhenzhong Lan. 2020. CLUE: A Chinese Language Understanding Evaluation Benchmark. In COLING.
Xinsong Zhang, Pengshuai Li, and Hang Li. 2021. AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization. In Findings of ACL.

Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin, Yusheng Su, Haozhe Ji, Jian Guan, Fanchao Qi, Xiaozhi Wang, Yanan Zheng, Guoyang Zeng, Huanqi Cao, Shengqi Chen, Daixuan Li, Zhenbo Sun, Zhiyuan Liu, Minlie Huang, Wentao Han, Jie Tang, Juanzi Li, Xiaoyan Zhu, and Maosong Sun. 2020. CPM: A Large-scale Generative Chinese Pre-trained Language Model. arXiv, abs/2012.00413.
Chujie Zheng, Minlie Huang, and Aixin Sun. 2019. ChID: A Large-scale Chinese IDiom Dataset for Cloze Test. In ACL.
Wei Zhu. 2020. MVP-BERT: Redesigning Vocabularies for Chinese BERT and Multi-Vocab Pretraining. arXiv, abs/2011.08539.

A Appendix
A.1 What is SHUOWEN-JIEZI?
SHUOWEN-JIEZI (`') is an ancient Chinese dictionary from the Han dynasty. It was the first dictionary to analyze the structure of Chinese characters and to give the rationale behind them, and also the first dictionary to use Chinese radicals to organise the sections.
The literal meaning of SHUOWEN and JIEZI correspond nicely to the core intuitive behind our pronunciation- and glyph-based tokenizers. We name our methods this name to pay tribute to the ancient wisdom of our ancestors.
A.2 Input Methods
Current Chinese input methods for computers can be categorized into pronunciation-based and glyph-based. Both methods encode each Chinese character into a sequence of units from a smaller set of alphabet (e.g. the Latin alphabet), but differ in what the units represent. In pronunciationbased input methods, each unit usually represents a phoneme, while in glyph-based methods, one unit or a group of units generally represent a radical or stoke composition. Chinese characters have a standardized stroke order, which can be taken into account by glyph-based methods. In almost all commonly used input methods, there exists different characters that encode into the same sequence, in which case, the solution is usually to list all matching characters and let the user select the correct one. We briefly introduce each input method used in the paper below. Cangjie (`') and Wubi (`') are two similar glyph-based input methods. They map keys on the QWERTY keyboard to fundamental radicals or combination of strokes that are combined to represent the shape of entire characters. They sometimes disregard stroke order in favor of combinations that visually more similar to the character. The main difference between them is the key mapping and the rules on how to break down characters into fundamental components. Zhengma (`'), a glyph-based method, is similar to Cangjie and Wubi. It maps each Latin letter to fundamental radicals, which are combined into entire characters. But Zhengma differs from Cangjie and Wubi in that it strictly follows stroke order. Stroke (` '), a glyph-based method, more commonly used in mobile phones or numerical

keypads, maps five keys to five basic types of strokes. The user pressed the corresponding keys according to characters' stroke order. Pinyin (` ') input method, a pronunciationbased input method, is the most widely used input method among Chinese speakers. It is based on the Hanyu Pinyin (`', meaning Chinese Sound-Spelling) romanization system for Chinese. Each Chinese monophthong is mapping to one or two Latin alphabet. Zhuyin (`') input method, a pronunciationbased input method, is the most common input method in the Taiwan province of China. It is based on the Zhuyin phonetic transcription system, which consists 37 characters that represent each Chinese phoneme. Note that while both Pinyin and Zhuyin input methods disregard tones as input methods on electonic devices, we do keep the tones during encoding in our tokenizers.

