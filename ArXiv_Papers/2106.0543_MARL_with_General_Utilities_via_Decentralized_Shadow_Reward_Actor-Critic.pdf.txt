MARL with General Utilities via Decentralized Shadow Reward Actor-Critic

arXiv:2106.00543v1 [stat.ML] 29 May 2021

Junyu Zhang Department of Electrical Engineering Center for Statistics and Machine Learning Princeton University, Princeton, NJ 08544
junyuz@princeton.edu

Amrit Singh Bedi CISD
US Army Research Laboratory Adelphi, MD 20783
amrit0714@gmail.com

Mengdi Wang Department of Electrical Engineering Center for Statistics and Machine Learning Princeton University/Deepmind, Princeton, NJ 08544
mengdiw@princeton.edu

Alec Koppel CISD
US Army Research Laboratory Adelphi, MD 20783
alec.e.koppel.civ@mail.mil

Abstract
We posit a new mechanism for cooperation in multi-agent reinforcement learning (MARL) based upon any nonlinear function of the team's long-term state-action occupancy measure, i.e., a general utility. This subsumes the cumulative return but also allows one to incorporate risk-sensitivity, exploration, and priors. We derive the Decentralized Shadow Reward Actor-Critic (DSAC) in which agents alternate between policy evaluation (critic), weighted averaging with neighbors (information mixing), and local gradient updates for their policy parameters (actor). DSAC augments the classic critic step by requiring agents to (i) estimate their local occupancy measure in order to (ii) estimate the derivative of the local utility with respect to their occupancy measure, i.e., the "shadow reward". DSAC converges to -stationarity in O(1/ 2.5) (Theorem 4.14) or faster O(1/ 2) (Corollary 4.16) steps with high probability, depending on the amount of communications. We further establish the non-existence of spurious stationary points for this problem, that is, DSAC finds the globally optimal policy (Corollary 4.15). Experiments demonstrate the merits of goals beyond the cumulative return in cooperative MARL.
1 Introduction
Reinforcement learning (RL) is a framework for directly estimating the parameters of a controller through repeated interaction with the environment [58], and has gained attention for its ability to alleviate the need for a physically exact model across a number of domains, such as robotic manipulation [26], web services [71], and logistics [17], and various games [63, 56]. In RL, an agent in a given state takes an action, and transits to another according to a Markov transition density, whereby a reward informing the merit of the action is revealed by the environment. Mathematically, this setting may be encapsulated by a Markov Decision Process (MDP) [47], in which the one seeks to select the action sequence to maximize the long-term accumulation of rewards.
In many domains, multiple agents interact in order to obtain favorable outcomes, as in finance [36], social networks [23], and games [57, 64]. In multi-agent RL (MARL) and more generally, stochastic games, a key question is the payoff structure [54, 4]. We focus on common payoffs among agents, i.e., the utility of the team is the sum of local utilities [10], which contrasts with competitive settings where one agent's gain is another's loss, or combinations thereof [39]. Whereas typically cooperative
Preprint. Under review.

Objective
Cumulative Return
Risk Exploration Priors

Approach Value-Based [25, 65, 34, 48, 15] Policy-Based [70, 12] [43] [42, 20] [32, 35]

Convergence 







 This



Work 



 

Table 1: Cumulative Returns, Risk-Sensitivity, Exploration, and the incorporation of Priors are common goals in MARL, and subsumed by the general utilities considered here. We focus on the setting when agents are cooperative and transition according to a common global dynamics model [10]. We defer a discussion of centralized training decentralized execution (CTDE), partial observability, and different transition models to appendices, with the understanding that our focus is on decentralized training under full observability. The respective technical settings of [32, 35, 42, 20, 43] are different; their inclusion here is to underscore their use of goals beyond cumulative return, which is given a conceptual underpinning for the first time in this work.

MARL defines the global utility as the average over agents' local reward accumulations, here we define a new mechanism for cooperation that permits agents to incorporate risk-sensitivity [22, 6, 46], prior experience [51, 2], or exploration [21, 61]. The usual common-payoff setting focuses on global cumulative return of rewards, which is a linear function of the the state-action occupancy measure. By contrast, the aforementioned decision-making goals define nonlinear functions of the state-action occupancy measure [24]. Such functions, we call general utilities, have recently yielded impressive performance in practice via prioritizing exploration [42, 20], risk-sensitivity [43], and prior experience [32, 35]. To date, however, there exists few formal guarantees for algorithms designed to optimize general utilities in multi-agent settings, to the best of our knowledge.
This gap motivates us to put forth the first decentralized MARL scheme for general utilities, and establish its consistency and sample complexity. Our approach hinges upon first noting that the embarking point for most RL methodologies is the Policy Gradient Theorem [66, 59] or Bellman's equation, both of which break down for general utilities. One potential path forward is a recent generalization of the PG Theorem for general utilities [68], which expresses the gradient as product of the partial derivative of the utility with respect to the occupancy measure, and the occupancy measure with respect to the policy. However, in the team setting, this later factor is a global nonlinear function of agents' policies, and hence does not permit decentralization. Thus, we define an agent's local occupancy measure as the joint occupancy measure of all agents' polices with all others' marginalized out, and its local general utility as any (not-necessarily concave) function of its marginal occupancy measure. The team objective, then, is the global aggregation of all local utilities.
From this definition, we derive a new variant of the Policy Gradient [cf. (6)] where each agent estimate its policy gradient based on local information and message passing with neighbors. Specifically, we derive a model-free algorithm, Decentralized Shadow Reward Actor-Critic (DSAC), that generalizes multi-agent actor-critic (see [29, 28]) beyond cumulative return [70]. Each agent's procedure follows four stages: (i) a marginalized occupancy measure estimation step used to evaluate the instantaneous gradient of the local utility with respect to the occupancy measure, which we dub the "shadow reward" (shadow reward computation); (ii) accumulate "shadow rewards" along a trajectory to estimate "shadow" critic parameters (critic); (iii) average critic parameters with those of its neighbors (information mixing); and (iii) a stochastic policy gradient ascent step along trajectories (actor).
Contributions. Overall, our contributions are:
· present the first MARL formulation that permits broader goals than the cumulative return and specialization among agents' roles;
· derive a variant of multi-agent actor-critic to solve this problem that employs an occupancy measure estimation step to construct the gradient of the general utility with respect to the occupancy measure, which serves as a "shadow reward" for the critic step;
· for -stationarity with high probability, we respectively establish that DSAC requires O(1/ 2.5) and O(1/ 2) steps if agents exchange information once (Theorem 4.14) or multiple times per policy update (Corollary 4.16). Under proper assumptions, we further establish
2

the convergence to the globally optimal policy under diminishing step-sizes (Corollary 4.15).

· provide experimental evaluation of this scheme for exploration maximization and safe navigation in cooperative settings [40].

2 Problem Formulation

Consider a Markov decision process (MDP) over the finite state space S and a finite action space

A. For each state s  S, a transition to state s  S occurs when selecting action a  A according

to a conditional probability distribution s  P(·|a, s), for which we define the short-hand notation

Pa(s, s ). Let  be the initial state distribution of the MDP, i.e., s0  . We let S := |S| denote the number of states and A := |A| the number of actions. Consider policy optimization for maximizing

general objectives that are nonlinear function of the cumulative discounted state-action occupancy

measure under policy , which contains the cumulative return as a special case [67, 68]:

max R() := F ()

(1)



where F is a general (not necessarily concave) functional and  is occupancy measure given by



(s, a) = t · P st = s, at = a , s0  

(2)

t=0

for a  A, s  S. For instance, often in applications one has access to demonstrations which can be used to learn a prior on the policy for ensuring baseline performance. Suppose ¯ is a prior state-action

distribution obtained from demonstrations. One may seek to maintain baseline performance with

respect to this prior via minimizing the Kullback-Liebler (KL) divergence between the normalized distribution ^ = (1 - ) and the prior ¯ stated as () = KL (1 - )||¯ . In behavioral cloning,

action information is missing, in which case one may instead consider a variant with respect to only

the state occupancy measure. Other functional forms for (1) are considered experimentally in Sec. 5.

In this work, we consider the fully decentralized version of the problem in (1), where the state space S, the action space A, the policy , and the general utility F are decentralized among N = |V| distinct agents associated with an undirected graph G = (V, E) with vertex set V and edge set E. Each agent i  V is associated with its own local incentives and actions, detailed as follows.

Space Decomposition. The global state space S is the product of N local spaces Si, i.e., S = S1 × S2 × · · · × SN , meaning that for any s  S, we may write s = (s(1), s(2), · · · , s(N)) with s(i)  Si, i  V. Each agent has access to the global state s, as customary of joint-action learners training in a decentralized manner under full observability [25, 70, 34, 65, 48, 15]. Similarly, the
global action space A is the product of N local spaces Ai: A = A1 × A2 × · · · × AN , meaning that for any a  A, we may write a = (a(1), a(2), · · · , a(N)) with a(i)  Ai, i  V. Full observability means each agent i has access to global actions a concatenating all local ones.

Policy Factorization. The global policy (a|s) that maps global action a for a given global state s is

defined as the product of local policies

N i=1

(i)(a(i)|s),

which

prescribes

statistical

independence

among agents' policies. For the parameterized policy (a|s) where   , we denote  =

(1, 2, · · · , N ) as the parameter, so we can write (a|s) = iV (ii)(a(i)|s), where the local policy of agent i is parameterized by i. Since the global state is visible to all agents, the local policy

is based on the observation of the global state. The parameters i are kept private by agent i, meaning

that agents must pass messages to become informed about others' incentives.

Local Cumulative State-Action Occupancy Measure. Similar to the global occupancy measure (s, a) [cf. (2)], define the local cumulative state-action occupancy measure:



(i)(s(i), a(i)) = t · P st(i) = s(i), at(i) = a(i) , s0  

(3)

t=0

for a(i)  Ai, s(i)  Si. This local occupancy measure is the marginalization of the global occupancy measure with respect to all others' measures than agent i, whose indices are denoted as

{-i}  V. Via marginalization, we write

(i)(s(i), a(i)) =

(s, a)

(4)

a{a(i)}×A-i s{s(i)}×S-i

3

with A-i=j=iAj and S-i=j=iSj. Note that (4) is a linear transform of  in (2). Local Utility. Let Si = |Si| denote the number of local states and Ai := |Ai| the number of local actions. For agent i, define the local utility function Fi(·) : RSiAi  R as a function of (i), depends on i when agent i follows policy i . Then, define the global utility as the sum of local ones:

R( )

=

F ( )

:=

1 N

N

Fi (i) .

(5)

i=1

Note that (5) is not node-separable, and local occupancy measures depend on the global one through (4). This means that the policy parameters i of agent i depends on global policy , and hence on global parameter  = (1, 2, · · · , N ). This is a key point of departure from standard multi-agent optimization [45]. Next we shift to deriving a variant of actor-critic that is attuned to the multi-agent
setting with general utilities (5).

3 Elements of MARL with General Utilities

This section develops an actor-critic type algorithm for MARL with general utilities (5). One challenge is that the occupancy measure, the policy parameters, and the utility are coupled. Specifically, the value function is not additive across trajectories, and hence invalidates RL approaches tailored to maximizing cumulative returns based upon either the Policy Gradient Theorem [66, 59] or Bellman's equation [47]. To address this issue, we employ a combination of the chain rule, an additional density estimation step, and the construction of a "shadow reward." We first define the shadow reward and value function as follows and then will proceed towards the proposed algorithm.

3.1 Shadow Rewards and Policy Evaluation

The general utility objective cannot be written as cumulative sum of returns. The nonlinearity invalidates the additivity, which is the origination of the definition of the conventional reward function and Q function, quantities that are central to approaches for maximizing cumulative-returns, via either dynamic programming [47] or policy search [66, 59]. To circumvent the need for additivity, we will introduce auxiliary variables, which we call shadow rewards and shadow Q functions.

Definition 3.1 (Shadow Reward and Shadow Q Function). The shadow reward r : S × A  R of

policy



w.r.t.

general

utility

F

is

r(s, a)

:=

F ( ) (s,a)

,

with

associated

shadow

Q

function

+
QF (s, a) := E t · r(st, at) s0 = s, a0 = a,  .
t=0

To understand these definitions, consider linearizing (differentiating) general utility F with respect to . The linearized problem, via the chain rule, is equivalent to a MDP with cumulative return, with
the shadow reward and Q function in place of the usual reward and Q functions:

+

F ( ) = E

t · QF (st, at) ·  log (at|st) s0  ,  .

(6)

t=0

This expression for the policy gradient illuminates the centrality of the shadow reward/value function for nonlinear functions of the occupancy measure (2), which motivates the generalized policy evaluation scheme we present next.

Policy Evaluation Criterion. We shift to how one may compute the Shadow Q-function from
trajectory information, upon the basis of which we can estimate the parameters of a critic. To do so,
we use function approximation to parameterize the high-dimensional shadow Q-function. One simple choice is linear function approximation. That is, given a set of feature vectors {(s, a)  Rd : s  S, a  A}, we want to find some weight parameter w  Rd so that

Qw(s, a) := (s, a), w (s, a)  S × A.

(7)

In our algorithm, we will update a sequence of w^ to closely approximate the sequence of implicit shadow Q functions, as policy gets updated. In practice, the parametrization (7) needs not be linear. Indeed, experimentally, we consider Q defined by a multi-layer neural network in Section 5.

4

Algorithm 1: Decentralized Shadow Reward Actor-Critic (DSAC)

1 Input: initial policy 0; actor step-sizes {k}; Batch sizes {Bk}; Episode lengths {Hk}; initial critic W 0 := [w10, w20, ..., wN0 ]  Rd with wi0 = wj0, i, j; critic step-size {wk }; mixing matrix M  RN+×N ; mixing round m  1.
2 for iteration k = 0, 1, 2, ... do

3 Perform Bk Monte Carlo rollouts to obtain trajectories  = {s0, a0, · · · , sHk , aHk } with

initial dist. , policy k collected as batch Bk. 4 for agent i = 1, 2, ..., N do

5

Compute empirical local occupancy measure

^ki

=

1 Bk

 Bk

Hk
t
t=0

·e

st(i), at(i)

.

(9)

Estimate shadow reward r^ik = i Fi(^ki ).

6 for agent i = 1, 2, ..., N do

7

With lolicalized policy gradient estimate

Gi (, wi) =

H t=0

 t Qwi

(st,

at)i

log

(ii)(at(i)|st),

compute

kwi

=

1 Bk

Gwi (, r^ik, wik)
 Bk

,

8 for iter = 1, ..., m do

wik+1 = wik - wk kwi .

9

for agent i = 1, 2, ..., N do

10

Exchange information with neighbours: wik+1 = {j:(j,i)E} M (j, i) · wik+1.

11

With Gi (, wi) =

H t=0

 t Qwi

(st,

at)i

log

(ii)(at(i)|st),

update

the

policy:

ki

:=

1 Bk

Gi (, wik+1)
 Bk

, ik+1

=

ik

+ kki .

Thus, the critic objective of policy  is defined as the mean-square-error w.r.t. shadow Q-function:

(w; ) :=E

 t 2

Qw(st, at) - QF (st, at) 2 s0  , 

t=0

1 =
2

(s, a) (s, a) w - QF (s, a) 2.

(8)

s,a

Via the definition of the occupancy measure  [cf. (2)], the expectation may be substituted by weighting factors in the summand on the second line. We assume features {(s, a)}sS,aA are bounded, as is formalized in Sec. 4. With the shadow reward and associated Q-function (Definition 3.1), the policy evaluation criterion (8), and its smoothness properties with respect to critic parameters w in place (Sec. 4), we expand on their role in the multi-agent setting.

3.2 Multi-Agent Optimization for Critic Estimation

Setting aside the issue of policy parameter updates for now, we focus on estimating the global general
utility. The shadow Q-function and shadow reward (Definition 3.1) depend on global knowledge of
all local utilities, which are unavailable as local incentives are local only. To mitigate this issue, we
introduce their localized components, which together comprise the global shadow Q-function and reward. Specifically, define the local shadow reward ri for agent i:

ri(s(i), a(i))

:=

Fi((i)) (i)(s(i), a(i))

, (s(i), a(i))  Si

× Ai.

(10)

5

Clearly,

it

holds

that

r(s, a)

=

1 N

N i=1

ri

(s(i),

a(i)).

Based

on

the

local

observation

of

the

its

own

shadow reward, agent i may access its local shadow Q-function Q : S × A  R:

+

Qi (s, a) := E

t · ri st(i), at(i) s0 = s, a0 = a,  ,

(11)

t=0

for

(s, a)



S

× A.

Therefore,

we

also

have

QF (s, a)

=

1 N

N i=1

Qi (s, a).

Then,

each

agent

i

seeks to estimate common critic parameters w that well-represent its shadow Q function in the sense

of minimizing the global mean-square error (8). By exploiting the aforementioned node-separability

and introducing a localized critic parameter vector wi associated to agent i, this may equivalently be

expressed as a consensus optimization problem [45]:

1N

min

N {wi }N i=1

i=1

i(wi; ) s.t. wi = wj, (i, j)  E ,

i(wi; ) := E

 t 2

Qwi (st, at) - QFi (st, at) 2 s0  , 

.

(12)

t=0

where the local policy evaluation criterion is defined as This formulation allows agent i to evaluate its
policy with respect to global utility (5) through the local criterion i(wi; ) as a surrogate for that which aggregates global information (8), when consensus over local parameters wi is imposed. Next, we incorporate solutions to (12) into the critic step together with a policy parameter i update along stochastic ascent directions via (6) for the actor to assemble DSAC.

3.3 Decentralized Shadow Reward Actor-Critic
Next, we put together these pieces to present Decentralized Shadow Reward Actor-Critic (DSAC) as Algorithm 1 (see Fig. 4 in the appendix for the flow diagram). This scheme allows agents to keep their local utilities Fi, and policies i with associated parameters i private. The agents share a common function approximator for the shadow Q function. Further, they retain local copies wi of the shadow critic parameters, which they communicate to neighbors according to the network structure defined by edge set E and mixing matrix M to be subsequently specified. Algorithm 1 proceeds in four stages: (i) density estimation step for to obtain the shadow reward; (ii) shadow critic updates; (iii) information mixing via weighted averaging; and (iv) actor updates. Each step is detailed in Algorithm 1, with step-by-step instructions in Appendix B.1.

4 Consistency and Sample Complexity
In this section, we study the finite sample performance of Algorithm 1. We show O~( -2.5) (Theorem 4.14) or O~( -2) (Corollary 4.16) sample complexities to obtain -stationary points of global utility, depending on the number of communications per step, akin to best known rates for non-concave expected maximization problems [53]. We also establish the nonexistance of spurious extrema for this setting, indicating the convergence to global optimality (Corollary 4.15). Before continuing, we present a few key technical conditions for the utility F , the policy , the mixing matrix M , and the critic approximation.
Assumption 4.1. For utility F [cf. (5)], we assume: (i). Fi(·) is private to agent i, for i. (ii). CF > 0 s.t. (i) Fi((i))   CF in a neighbourhood of the occupancy measure set, i. (iii). L > 0 s.t. (i) Fi((i)) - (i) Fi((i))   L (i) - (i) , for i. (iv). L > 0 s.t. F  (·) is L-smooth.
Assumption 4.2. For  and the occupancy measure  , we assume: (i). The local policy (ii) is private to each agent i. (ii).  C > 0 s.t. for any agent i, the score function is upper bounded: i log (ii)(a(i)|s)  C, for   and (s, a). (iii).   > 0 s.t.  -     -  .

6

Assumption 4.3. The mixing matrix M is a doubly stochastic matrix satisfying: (i). M  SN+×N , M(i, j) > 0 iff. (i, j)  E. (ii). M · 1N = 1N , where 1N  RN is an all-ones vector. (iii). Let the eigenvalues of M be 1 = 1(M ) > 2(M )  · · ·  N (M ).
 := max{|2(M )|, |N (M )|} < 1.

We define

Assumption 4.4.

For , define the optimal critic parameter w() := argminw

1 N

N i=1

i(w; ).

We assume that W > 0 s.t. E2 =

N i=1

i F ( ) - i

2  W , for , where

+

i := E

t · Qw()(st, at) · i log (ii)(at|st) s0  , 

t=0

is the PG estimate under w().

Assumption 4.1 requires the boundedness and Lipschitz continuity of the gradient of the utility function. Assumption 4.2 ensures that the score function is bounded, and the occupancy measure is Lipschitz w.r.t. the policy parameters. These conditions are common to RL algorithms focusing on occupancy measures in recent years [21, 68], and are automatically satisfied by common policies such as the softmax. Assumption 4.3 holds for any undirected connected loop-free static graph [13]. Assumption 4.4 states that the feature mis-specification error is uniformally upper bounded by W . Besides, we also make the following assumptions.

For the shadow Q function and the occupancy measure, we make the following assumption. This

assumption can be implied by more basic assumptions on the boundedness and Lipschitz continuity

of the score functions. We prefer directly assuming the Lipschitz continuity of the shadow Q function

in order to avoid the heavy notations.

Assumption 4.5.  Q,  > 0 s.t. for (s, a)  S ×A, Q  -  , and | (s, a) -  (s, a)|    - 

, .



,

it

holds

that

|QF (s,

a)-QF (s,

a)|



We also assume the boundedness of the feature vectors. Assumption 4.6. C > 0 s.t. (s, a)  C, (s, a).

As a consequence of the boundedness of the features, the critic objective function has Lipschitz continuous gradients.

Proposition 4.7.

Regardless of policy , critic objective

(w; )

[cf.

(8)]

is

Lw

:=

C2 1-

smooth.

This can be shown by directly computing the Hessian matrix of the critic objective function as

2w (w; ) =

s,a  (s, a)·(s, a)(s, a)

. Consequently, Lw 

2w

(w; )

F



C2 1-

.

Throughout the iterations of Algorithm 1, we also make the following assumption on the critic objective function.

Assumption 4.8. (w; k ) is µw-strongly convex for all k.

Assumption 4.8 means that the minimum eigenvalue of the feature covariance matrix s,a k (s, a)· (s, a)(s, a) is uniformly lower bounded by some constant µw > 0. Note that the shadow reward r is changing with iteration index k, and consequently we cannot assume that the fitted shadow Q-function perfectly tracks the true shadow Q-function. Motivated by the subtleties of the quality of a feature representation, we further place a condition on the shadow value function approximation error.

Next, we present the proof of the finite sample performance of the algorithm with details provided in the appendices. Based on the smoothness of of the utility function F ( ), the standard Taylor's expansion allows us to write:
Lemma 4.9. For Algorithm 1, if the step size of  satisfies k  1/4L, then

F (k+1 ) - F (k )  k 4

F (k )

2-

3k 4

N

2
i F (k ) - ki .

i=1

(13)

7

Not surprisingly, the key here is bounding the gradient estimation error term

N i=1

i F (k ) - ki

2
,

which is caused jointly by the stochastic sampling error,

the

shadow Q-function approximation error, and multi-agent concensus error, as is characterized below.

Lemma 4.10. Let k  (0, 1) be some small failure probability. Then following inequalities hold.

(i) For the ease of notation, define the error matrix of the critic estimators as

Wk := [w1 1(w1k; k ) - kw1 ; · · · ; wN N (wNk ; k ) - kwN ]. For the critic gradient estimator, we have

Prob

Wk

2 F

 Ewk

 2N k.

(14)

where Ewk = O

log(1/k )C2 (1- )2 Bk

C2

N i=1

wik

2

+

N CF2 (1-)2

+ N L2

+ 2Hk .

(ii) Denote wk+1 = w(k) (see Assumption 4.4) as the ideally fitted critic parameter. For the actor gradient estimator, there is a positive random variable k (defined in (33)) s.t.

N

ki -iF (k )

2

3C2 C2 (1-)2

N

wik+1 -wk+1 2 +k +6E2k +O(2Hk )

i=1

i=1

(15)

where Prob k  Ek

 N k, Ek = O

· C2 C2
(1-)2

N log(1/k) wk+1 2 Bk

.

See Appendix C for proof. The inequality (15) characterizes the error of the gradient estimators.

However, in this inequality, the bound on the term

N i=1

wik+1 - wk+1

2 needs further study. This

error exists because Algorithm 1 makes only one stochastic gradient update. Though minimizing the

critic objective function exactly in each iteration eliminates this term, it is often sample inefficient to

do so in practice. By splitting the error into two parts:

N i=1

wik+1 - wk+1 2  2

N i=1

wik+1 -

w¯k+1

2 + 2N

w¯k+1 - wk+1

2,

where

w¯k+1

=

1 N

i=1 wik+1, we observe that the first part is a

consensus error, which we study in Lemma 4.11. The second component is the optimality gap of the

critic fitting problem which is the focus of Lemma 4.12.

Lemma 4.11.

Let the sequence {wik} be generated by Algorithm 1.

Define w¯k

:=

1 N

N i=1

wik

.

Then as long as the step size wk  1/Lw, it holds that

N

wik -w¯k

22

max
k k

Wk

2 F

+

N CF2 C2 (1 - )4

·

k

2

wk m(k-k ) ·2m.

i=1

k =0

(16)

Lemma 4.12. There Cw > 0 s.t. wk+1 - wkk  Cw k - k-1 for all the iterates. If the step

sizes

satisfy

k-1



(1-)µwwk ,
4 3N Cw CC

then

w¯k+1 - wk+1 2



1 - wk µw 4

·

w¯k - wk

2

+

2Cw2 (k-1)2 wk µw

F (k-1 )

2+

2wk

Wk

2 F

µw · N

+

2Cw2 (k-1)2 wk µw

k-1

+ 6E2k-1

+ O(2Hk-1 ) +

6C2 C2 (1 - )2

·

N

wik - w¯k 2

i=1

.

(17)

See proof in Appendix F. Specifically, the existence of such a constant Cw is proved in Appendix J.

Next, we construct the following potential function with a carefully selected constant 

Rk := F (k ) -  w¯k - wk 2.

(18)

Taking the advantage of the contraction property of w¯k-wk 2 and this specific potential function, as well as the fact that maxk0{ wk+1 , wik }  Dw for some constant Dw > 0 (proved in Appendix J),we characterize algorithm performance in terms of optimization error, the feature mis-specification
error, the stochastic PG approximation error, and the multi-agent consensus error.

8

Lemma 4.13. For Algorithm 1, if for any k  0 we choose the step sizes to be wk+1 

wk  1/Lw, and k = min

(1-)µw wk+1 Cw CC

·

1  ,
max{4 3N ,6 10}

1 4L

, and we

set 

=

18C2 C2 (1- )2 µw

·

maxk0{k/wk } in the potential function (18), then with probability 1 - 3N

T k=0

k

,

we

have

T k=1

k

F (k )

T k=1

k

2

O

RT +1 - R1

T k=1

k

+O

T k=0

k E2k

T k=1

k

+O

T k=1

k-1

·



T k=1

k

·

+O



N

log(1/k ) Bk

+

 2Hk

T k=1

k

k k

=0

wk

k-k

T k=1

k

2



· 2m , 

(19)

where the four terms stand for the optimization error, the feature mis-specification error, the stochastic PG approximation error, and the multi-agent consensus error.

Upon the basis of Lemma 4.13, we select parameters to conclude the statements in Theorem 4.14, which is expanded upon at length in Appendix I.

Combining the above steps and suitably specify the parameters, we have the final theorem.

Theorem 4.14. Under Assumption 4.6, 4.1, 4.2, 4.8 and 4.3, with one communication round per

iteration, i.e. m = 1, Algorithm 1 satisfies, under the following parameter selections:

(i) For final iteration T = O( -1.5), trajectory lengths Hk  O

/(3N (T +1)),   (0, 1), batch sizes Bk

 = min

(1- )µw w Cw CC

·

1  ,
max{4 3N ,6 10}

1 4L

 log(1/k) -1, = O( ), then

constant

(log(1/ )/1 - step-sizes w

), k = O(

 ),

1T T

F (k ) 2  O ( + W ) . w.p. 1 - 

k=1

(ii) For unspecified final iteration T , we adaptively set:

k

=

N

2

2 (k+1)2

,





(0, 1), trajectory

lengths Hk

=

O((1 - )-1 log(k + 1)),

batchsizes Bk

=

log(1/k)(k

+

1)

2 3

,

and

step-sizes

k = min

(1-)µw wk+1 Cw CC

·

1  ,
max{4 3N ,6 10}

1 4L

,

wk

=

min{(k

+

1)-

1 3

,

L-w1},

then

T k=1

k

F (k )

T k=1

k

2
O

log T

T2 3

+W

, w.p. 1 - 

In either case, Algorithm 1 requires O~( 2.5) samples to satisfy

 O( + W ). T
k=1

k

 F (k )

2

T k=1

k

Next, we establish that for concave general utilities (1), there are no spurious stationary points.

Corollary 4.15 (Convergence to global optimality). Suppose F is concave, and the shadow Q func-

tion QF is realizable, i.e., W = 0 in Assumption 4.4. For  satisfying Assumption 1 of [68], every sta-

tionary point is a global optimizer. In Theorem 4.14(ii), if we further let ¯T be the parameter randomly

chosen from {k}Tk=1 where ¯T = k w.p. k/(

T k

=1

k

), then limT  E[

F (¯T )

2] = 0

w.p. 1 - . Thus, Algorithm 1 converges to the set of global optimizers.

Next we spotlight the role of the number of communication steps in the convergence rate.

Corollary 4.16 (Multiple-round communication). Suppose multiple-round communication is allowed,

i.e., m > 1. Under the same parameter selections as Theorem 4.14(i), while setting final iteration

index T = -1, communication rounds m = O((1 - )-1 log( -1)), and the step-sizes k 

min

(1- )µw /Lw Cw CC

·

1  ,
max{4 3N ,6 10}

1 4L

, wk  L-w1, then the total sample complexity is O( -2).

Namely, with additional communication rounds m = O((1-)-1log( -1)) per iteration, the convergence rate refines from O( -2.5) to O( -2). Next, we investigate the experimental merit of the
proposed approach for giving rise to emergent teamwork among multiple agents across various tasks.

9

5 Experimental Results
We experimentally investigate the merit of Algorithm 1 in the context of both single and multi-agent problems. The single-node case (N = 1) bears investigation as the proposed scheme is a new way to solve RL problems with general utilities relative to [68]. For this case, we consider the continuous MountainCar environment of OpenAI Gym [9]. The additional experiments for single agent settings to Appendix K.1.
5.1 Concept of Shadow Reward
To understand the concept of shadow reward, we experiment with the single-agent setup. We consider the exploration maximization problem for the MountainCar environment in which the two dimensional continuous state space is divided into [12, 11] grid size. We run the proposed algorithm for 40 epochs and then plot the count based occupancy measure estimate in the first row of Fig. 1(a). In the figure, light color denotes lower value and dark color represent the higher values as shown in the colorbar. We see that as we go from epoch 1 to epoch 39, the algorithm yields occupancy measures that better cover the state space, which is achieved by the special structure of the "shadow reward" we define as a by-product of the general utility.

(a) Shadow reward

(b) Entropy comparison

(c) State space coverage

Figure 1: (a) Occupancy measure (first row) and shadow reward (second row) for MountainCar environment. Each subplot represents a heatmap for two dimentional state space. Observe that over the course of training the measure and shadow reward's coverage of the state and action spaces grows, as a consequence of selecting actions towards maximizing the entropy of the occupancy measure. (b) Entropy comparisons for exploration maximization in a cooperative multiagent environment, (c) Agent 1 marginalized occupancy measure. For the DSCA implementation, each agents needs to estimate only 100 dimensional marginalized occupancy measure while for the centralized counterpart MaxEnt, we need to estimate 104 dimensional occupancy measure making it slow in practice.

5.2 Multi-Agent Experiments
For multi-agent problems, we experiment with N  2 agents moving in a two-dimensional continuous space associated with the problem of Cooperative navigation [40].
Exploration Maximization. We consider a variant of the cooperative navigation multi-agent environment provided in [40] for N = 2 agents. The goal of maximum entropy exploration in the multi-agent setting is one in which all agents in the network seek to cover the unknown space, whereby their local utility is the entropy in (5) is given by Fi((i)) = - s(i)(i)(s(i))·log((i)(s(i))).
We compare DSAC against its corresponding centralized implementations (Cen-AC) or a variant that uses Monte-Carlo rollouts (Cen-R, Dec-R), as well as existing MaxEnt [21] in Fig.1(b)-1(c). See Appendix K.2 for details. Observe that MaxEnt does not achieve comparable performance, and DSAC achieves comparable performance to its variants that require centralization. Fig. 1(c) visualizes the heatmap of the marginalized measure at agent 1 for DSAC (red) at different epochs as compared to MaxEnt (purple) and random baseline (green) ­ note the superior space coverage of DSAC (red).
Safe Cooperative Navigation. We consider a two agent cooperative environment from [40] where each agent needs to reach its assigned goal while traversing only through the safe region as visualized

10

(a) World model

(b) Average return

(c) Average cost

Figure 2: (a) Two agent safe navigation environment with green as safe and brown as unsafe state space. The goal is to reach to goals (G1 and G2) safely from the starting positions (A1 and A2), respectively. (b) Undiscounted average reward return comparison and (c) average constraint violation comparison for different values of penalty parameter z. Observe that imposing constraints allows agents to avoid collision and the unsafe region, while effectively reaching their goals more often in terms of cumulative return and constraint violation.

(a) Average return

(b) Average cost

(c) Consensus error

Figure 3: Safe navigation in a multi-agent cooperative environment with 4 agents and 4 landmarks. Note that the state space in this case would be 16 dimensional (location of agent and landmarks). We run this experiment for three different communication graphs among agents; fully connected (FC) (all the agents are connected to each other), ring (all the agents are connected using ring topology), and random (where agents are randomly using Erdos-Re´nyi random graph model). (a) Running average of the reward return, (b) running average of the constraint violation, and (c) running average of the consensus error for agent 1 and agent 4 for ring and random network connectivity. Note that consensus error is zero for the fully connected network or a 2 agent network.

in Fig.2(a). Agents receive a negative reward proportional to its distance from the landmark, and an additional negative reward of -1 if agents collide. Additionally, each agents receive a high cost of c = 1 if it passes through the unsafe region (middle of the state space) ­ see Fig. 2(a). We impose safety via the constraint for each agent i , c  C where i in the marginalized occupancy measure, and including the constraint as a quadratic penalty in a manner similar to (59) ­ see Appendix K.2 for further details. To solve this problem, we compare the performance of DSAC for various values of its penalty parameter z to its centralized variant, and a version of multi-agent actor-critic that only ignores the cost. Results for the average reward and constraint violation, respectively, are given in Fig. 2(b)-2(c). The decentralized DSAC achieves comparable performance to its centralized variant, and outperforms existing alternatives, yielding effective learned behaviors for navigation in team settings. Demonstrations for larger networks with different connectivities are in Figure 3 and Appendix K.
6 Conclusions
We contributed a conceptual basis for defining agents' behavior in cooperative MARL beyond the cumulative return via nonlinear functions of their occupancy measure. This motivates defining "shadow rewards" and DSAC, whose critic employs shadow value functions and weighted averaging.
11

Its consistency and sample complexity was rigorously established. Further, experiments illuminated the upsides of general utilities for teams. Future work includes improving communications and sample efficiencies, connections to meta-learning, and allowing information asymmetry.
References
[1] Sanjeevan Ahilan and Peter Dayan. Correcting experience replay for multi-agent communication. arXiv preprint arXiv:2010.01192, 2020.
[2] Brenna D Argall, Sonia Chernova, Manuela Veloso, and Brett Browning. A survey of robot learning from demonstration. Robotics and autonomous systems, 57(5):469­483, 2009.
[3] Yoram Bachrach, Richard Everett, Edward Hughes, Angeliki Lazaridou, Joel Z Leibo, Marc Lanctot, Michael Johanson, Wojciech M Czarnecki, and Thore Graepel. Negotiating team formation using deep reinforcement learning. Artificial Intelligence, 288:103356, 2020.
[4] Tamer Bas¸ar and Geert Jan Olsder. Dynamic noncooperative game theory. SIAM, 1998.
[5] Daniel S Bernstein, Robert Givan, Neil Immerman, and Shlomo Zilberstein. The complexity of decentralized control of markov decision processes. Mathematics of operations research, 27(4):819­840, 2002.
[6] Vivek S Borkar and Sean P Meyn. Risk-sensitive optimal control for Markov decision processes with monotone cost. Mathematics of Operations Research, 27(1):192­209, 2002.
[7] Stephen Boyd, Arpita Ghosh, Balaji Prabhakar, and Devavrat Shah. Randomized gossip algorithms. IEEE transactions on information theory, 52(6):2508­2530, 2006.
[8] Stephen Boyd, Neal Parikh, and Eric Chu. Distributed optimization and statistical learning via the alternating direction method of multipliers. Now Publishers Inc, 2011.
[9] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
[10] Lucian Busoniu, Robert Babuska, and Bart De Schutter. A comprehensive survey of multiagent reinforcement learning. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 38(2):156­172, 2008.
[11] Jianshu Chen and Ali H Sayed. Diffusion adaptation strategies for distributed optimization and learning over networks. IEEE Transactions on Signal Processing, 60(8):4289­4305, 2012.
[12] Tianyi Chen, Kaiqing Zhang, Georgios B Giannakis, and Tamer Bas¸ar. Communication-efficient distributed reinforcement learning. arXiv preprint arXiv:1812.03239, 2018.
[13] Fan RK Chung and Fan Chung Graham. Spectral graph theory. Number 92. American Mathematical Soc., 1997.
[14] Caroline Claus and Craig Boutilier. The dynamics of reinforcement learning in cooperative multiagent systems. 1998.
[15] Thinh Doan, Siva Maguluri, and Justin Romberg. Finite-time analysis of distributed td (0) with linear function approximation on multi-agent reinforcement learning. In International Conference on Machine Learning, pages 1626­1635, 2019.
[16] Tom Eccles, Yoram Bachrach, Guy Lever, Angeliki Lazaridou, and Thore Graepel. Biases for emergent communication in multi-agent reinforcement learning. In Advances in Neural Information Processing Systems, pages 13111­13121, 2019.
[17] Eugene A Feinberg. Optimality conditions for inventory control. In Optimization Challenges in Complex, Networked and Risky Systems, pages 14­45. INFORMS, 2016.
[18] Jakob Foerster, Ioannis Alexandros Assael, Nando De Freitas, and Shimon Whiteson. Learning to communicate with deep multi-agent reinforcement learning. Advances in neural information processing systems, 29:2137­2145, 2016.
12

[19] Jakob Foerster, Nantas Nardelli, Gregory Farquhar, Triantafyllos Afouras, Philip HS Torr, Pushmeet Kohli, and Shimon Whiteson. Stabilising experience replay for deep multi-agent reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1146­1155, 2017.
[20] Tarun Gupta, Anuj Mahajan, Bei Peng, Wendelin Bo¨hmer, and Shimon Whiteson. Uneven: Universal value exploration for multi-agent reinforcement learning. arXiv preprint arXiv:2010.02974, 2020.
[21] Elad Hazan, Sham Kakade, Karan Singh, and Abby Van Soest. Provably efficient maximum entropy exploration. In International Conference on Machine Learning, pages 2681­2691. PMLR, 2019.
[22] Ying Huang and L. C. M. Kallenberg. On finding optimal policies for Markov decision chains: A unifying framework for mean-variance-tradeoffs. Mathematics of Operations Research, 19(2):434­448, 1994.
[23] Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro Ortega, DJ Strouse, Joel Z Leibo, and Nando De Freitas. Social influence as intrinsic motivation for multi-agent deep reinforcement learning. In International Conference on Machine Learning, pages 3040­3049. PMLR, 2019.
[24] L. C. M. Kallenberg. Survey of linear programming for standard and nonstandard Markovian control problems. Part I: Theory. Zeitschrift fu¨r Operations Research, 40(1):1­42, 1994.
[25] Soummya Kar, Jose´ MF Moura, and H Vincent Poor. Qd-learning: A collaborative distributed strategy for multi-agent reinforcement learning through consensus+ innovations. IEEE Transactions on Signal Processing, 61(7):1848­1862, 2013.
[26] Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The International Journal of Robotics Research, 32(11):1238­1274, 2013.
[27] Jonas Moritz Kohler and Aurelien Lucchi. Sub-sampled cubic regularization for non-convex optimization. arXiv preprint arXiv:1705.05933, 2017.
[28] Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in Neural Information Processing Systems, pages 1008­1014, 2000.
[29] Vijaymohan R Konda and Vivek S Borkar. Actor-critic­type learning algorithms for Markov Decision Processes. SIAM Journal on Control and Optimization, 38(1):94­123, 1999.
[30] Alec Koppel, Felicia Y Jakubiec, and Alejandro Ribeiro. A saddle point algorithm for networked online convex optimization. IEEE Transactions on Signal Processing, 63(19):5149­5164, 2015.
[31] Vikram Krishnamurthy. Partially observed Markov decision processes. Cambridge University Press, 2016.
[32] Hoang M Le, Yisong Yue, Peter Carr, and Patrick Lucey. Coordinated multi-agent imitation learning. Proceedings of Machine Learning Research, 70:1995­2003, 2017.
[33] Donghwan Lee, Niao He, Parameswaran Kamalaruban, and Volkan Cevher. Optimization for reinforcement learning: From a single agent to cooperative agents. IEEE Signal Processing Magazine, 37(3):123­135, 2020.
[34] Donghwan Lee, Hyungjin Yoon, V Cichella, and N Hovakimyan. Stochastic primal-dual algorithm for distributed gradient temporal difference learning. arXiv preprint arXiv:1805.07918, 2018.
[35] Hyun-Rok Lee and Taesik Lee. Improved cooperative multi-agent reinforcement learning algorithm augmented by mixing demonstrations from centralized policy. In Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems, pages 1089­1098, 2019.
13

[36] Jae Won Lee, Byoung-Tak Zhang, et al. Stock trading system using reinforcement learning with cooperative agents. In Proceedings of the Nineteenth International Conference on Machine Learning, pages 451­458. Morgan Kaufmann Publishers Inc., 2002.
[37] JZ Leibo, VF Zambaldi, M Lanctot, J Marecki, and T Graepel. Multi-agent reinforcement learning in sequential social dilemmas. In AAMAS, volume 16, pages 464­473. ACM, 2017.
[38] Yiheng Lin, Guannan Qu, Longbo Huang, and Adam Wierman. Distributed reinforcement learning in multi-agent networked systems. arXiv preprint arXiv:2006.06555, 2020.
[39] Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In Machine learning proceedings 1994, pages 157­163. Elsevier, 1994.
[40] Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. Neural Information Processing Systems (NIPS), 2017.
[41] Aditya Mahajan and Mehnaz Mannan. Decentralized stochastic control. Annals of Operations Research, 241(1-2):109­126, 2016.
[42] Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson. Maven: Multiagent variational exploration. In Advances in Neural Information Processing Systems, pages 7613­7624, 2019.
[43] Mysterious Mystery. Rmix: Risk-sensitive multi-agent reinforcement learning. Under Review at International Conference on Learning Representations, 2021.
[44] Ashutosh Nayyar, Aditya Mahajan, and Demosthenis Teneketzis. Decentralized stochastic control with partial history sharing: A common information approach. IEEE Transactions on Automatic Control, 58(7):1644­1658, 2013.
[45] Angelia Nedic and Asuman Ozdaglar. Distributed subgradient methods for multi-agent optimization. IEEE Transactions on Automatic Control, 54(1):48­61, 2009.
[46] LA Prashanth and Mohammad Ghavamzadeh. Variance-constrained actor-critic algorithms for discounted and average reward mdps. Machine Learning, 105(3):367­417, 2016.
[47] Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons, 2014.
[48] Chao Qu, Shie Mannor, Huan Xu, Yuan Qi, Le Song, and Junwu Xiong. Value propagation for decentralized networked deep multi-agent reinforcement learning. In Advances in Neural Information Processing Systems, pages 1184­1193, 2019.
[49] Guannan Qu, Adam Wierman, and Na Li. Scalable reinforcement learning of localized policies for multi-agent networked systems. In Learning for Dynamics and Control, pages 256­266. PMLR, 2020.
[50] Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. In International Conference on Machine Learning, pages 4295­4304, 2018.
[51] Stefan Schaal. Learning from demonstration. In Advances in neural information processing systems, pages 1040­1046, 1997.
[52] Devavrat Shah. Gossip algorithms. Now Publishers Inc, 2009.
[53] Alexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczyn´ski. Lectures on stochastic programming: modeling and theory. SIAM, 2014.
[54] Lloyd S Shapley. Stochastic games. Proceedings of the national academy of sciences, 39(10):1095­1100, 1953.
14

[55] Wei Shi, Qing Ling, Gang Wu, and Wotao Yin. Extra: An exact first-order algorithm for decentralized consensus optimization. SIAM Journal on Optimization, 25(2):944­966, 2015.
[56] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484­489, 2016.
[57] Peter Stone and Richard S Sutton. Scaling reinforcement learning toward robocup soccer. In Icml, volume 1, pages 537­544. Citeseer, 2001.
[58] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
[59] Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in neural information processing systems, pages 1057­1063, 2000.
[60] Herbert G Tanner, Ali Jadbabaie, and George J Pappas. Flocking in fixed and switching networks. IEEE Transactions on Automatic control, 52(5):863­868, 2007.
[61] Jean Tarbouriech and Alessandro Lazaric. Active exploration in markov decision processes. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 974­982, 2019.
[62] Ha°kan Terelius, Ufuk Topcu, and Richard M Murray. Decentralized multi-agent optimization via dual decomposition. IFAC proceedings volumes, 44(1):11245­11251, 2011.
[63] Gerald Tesauro. Td-gammon, a self-teaching backgammon program, achieves master-level play. Neural computation, 6(2):215­219, 1994.
[64] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michae¨l Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350­354, 2019.
[65] Hoi-To Wai, Zhuoran Yang, Zhaoran Wang, and Mingyi Hong. Multi-agent reinforcement learning via double averaging primal-dual optimization. In Advances in Neural Information Processing Systems, pages 9649­9660, 2018.
[66] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229­256, 1992.
[67] Junyu Zhang, Amrit Singh Bedi, Mengdi Wang, and Alec Koppel. Cautious reinforcement learning via distributional risk in the dual domain. arXiv preprint arXiv:2002.12475, 2020.
[68] Junyu Zhang, Alec Koppel, Amrit Singh Bedi, Csaba Szepesvari, and Mengdi Wang. Variational policy gradient method for reinforcement learning with general utilities. Advances in Neural Information Processing Systems, 33, 2020.
[69] Kaiqing Zhang, Erik Miehling, and Tamer Bas¸ar. Online planning for decentralized stochastic control with partial history sharing. In 2019 American Control Conference (ACC), pages 3544­3550. IEEE, 2019.
[70] Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer Basar. Fully decentralized multi-agent reinforcement learning with networked agents. In International Conference on Machine Learning, pages 5872­5881, 2018.
[71] Xiangyu Zhao, Long Xia, Liang Zhang, Zhuoye Ding, Dawei Yin, and Jiliang Tang. Deep reinforcement learning for page-wise recommendations. In Proceedings of the 12th ACM Conference on Recommender Systems, pages 95­103, 2018.
15

Appendix
A Further Context
We expand upon different axes of comparison in order to clarify the technical setting under consideration. Notably, there has been a surge of interest in MARL in recent years, which has led to disparate possible technical settings: the definition of the MDP transition dynamics and associated correlation of agents' trajectories; the observability of the state variable itself, the availability of computational resources at a centralized location, and the protocol by which agents exchange information. We proceed to describe each of these facets in turn.
Transition Model. A key question when formulating MARL is the definition of the transition model, which determines the availability of local of global trajectory information to individuals. A standard operating hypothesis is that all agents have knowledge of the global state and action, and that the joint policy of the team factorizes into a product of individual marginal policies, which is referred to as joint action learners (JAL) [33]. When a common reward is perceived by all agents, this problem can be solved without agent coordination [14], i.e., RL agents are effectively independent.
Observability. When the joint action of the team is unavailable to an individual, or other trajectory information is occluded, then one must contend with partial observability, i.e., solve a partially observed MDP (POMDP) [31]. This is the case in both the setting of "independent learners" (ILs) where agents only know their local action [33], or more broadly when they observe an insufficient statistic of the state [41]. Policy search in POMDPs is intractable in general, and even the policy evaluation (planning) problem requires solving a Bayesian inference problem in a Hidden Markov Model [5]. To address HMMs in general requires complicated Monte Carlo schemes [44, 69] in order to propagate state information, and hence few convergence results exist for policy search in POMDPs.
Training Paradigm. To contend with the intractability of POMDPs, a number of experimentally focused works [18, 37, 19, 50] propose pooling all possibly incomplete information at a centralized location, and then feed it into a policy search routine. This paradigm, called centralized training decentralized execution (CTDE), pre-supposes the viability of an oracle agent for which centralization is safe and tractable, which in cases that agents geographically dispersed may not hold.
Communication Model. For this reason, in this work, we instead focus on the case of decentralized training of JAL, i.e., when full state-action information is available, but agents' rewards and policy parameters are held locally private. This setting has been studied extensively in recent years, giving rise to multi-agent extensions of TD [34, 15], Q-learning [25], value iteration [65, 48], and actor-critic [70]. In these works, the manner in which agents may communicate is given by a graph: each agent is represented as a node and may communicate with another when there is a link between them. This graph is typically assumed to be a design parameter, as in this work. A separate but related body of works seek to estimate the communications architecture when agents' behavior is fixed [16, 1, 3], or begin with locality-based correlation models to derive dependencies on agents' local utilities [49, 38].
Information Mixing. Under the hypothesis that the communications graph is given, the JAL setting with rewards and policy parameters held locally, then most MARL information exchange protocols may be placed within a burgeoning literature on multi-agent optimization. Predominately, in the aforementioned references, a weighted averaging step is employed in order to diffuse information between agents across time while optimizing their local utility [45, 11]. This scheme originates in flocking [60] and gossip protocols [7, 52], and may be interpreted as an approximate enforcement strategy for equality constraints of the parameters held by distinct agents [55]. Alternatives based upon Lagrangian relaxation such as primal-dual [30], alternating direction method of multipliers (ADMM) [8], and dual reformulations [62] can more sharply enforce consensus; however, we opt for a primal-only approach to enforcing consensus for simplicity and its compatibility with PerronFrobenius theory [13], which provides appropriate conditions on the mixing matrices such that consensus is assured.
16

Figure 4: The proposed DSCA algorthm execution.

B Technical Details of Problem Setting

B.1 User Guide to Algorithm 1

A flow diagram explaining the algorithm execution is provided in Fig. 4. Next we provide the detailed discussion of the proposed algorithm.

(i) Occupancy Measure Estimation. With access to trajectory  , each agent i seeks to evaluate its current policy in terms of the general utility. To do so, it must compute its shadow reward, a nonlinear function of the occupancy measure. This may be accomplished by first executing a local empirical occupancy measure estimator^ki by (9). And then one computes the shadow reward r^ik = i Fi(^ki ).

(ii) Shadow Policy Evaluation. The shadow reward, as stated in (10), is employed to formulate the

local policy evaluation error i [cf. (12)] with respect to the shadow Q function (11). Note that the

shadow Q function QFi (st, at) is substituted by an empirical estimate along the current trajectory.

Specifically, Q^ti =

H t =t

t

-t

·

ri

st(i), at(i)

is the cumulative sum of tail rewards starting from

(st, at). Then, differentiating the resulting expression with respect to local critic parameters wi yields

the critic gradient direction:

H

Gwi(,ri,wi) = t ·(Qwi(st, at)-Q^ti)·wi Qwi(st,at),

(20)

t=0

where agent i then uses to update its local shadow critic as w^ik+1 = wik - wk · kwi at step k + 1, under the initialization with w10 = · · · wN0 and step-size wk specified as in Theorem 4.14. Moreover, kwi is a mini-batched version of the stochastic gradient in (20) specified in Algorithm 1.
(iii) Information Exchange. To ensure information effectively propagates across the network G, agents perform a simple weighted averaging step using mixing matrix M , which is a symmetric doubly stochastic matrix that respects the edge connectivity of the graph, see Assumption 4.3 for details. When agents execute m-steps of averaging per step k, we compactly express it as W k+1  M m · W k+1.

(iv) Policy Update. Given the Q-function approximation parameter wi, the actor gradient is con-

structed as

H

Gi (, wi) = tQwi (st, at)i log (ii)(at(i)|st).

(21)

t=0

which is a stochastic approximation of the gradient in (6). Notice that replacing Qwi (st, at) with the exact shadow Q-function QF (st, at) reduces (21) to the REINFORCE [66] estimator equiped with the newly defined shadow Q-function. Then, each agent executes a simple mini-batch stochastic

gradient ascent step.

17

C Proof of Lemma 4.9

Proof. By the L-smoothness of F ( ) to write

N
F (k+1 )  F (k ) + k

i F (k ), ki

- L(k)2 2

N

ki 2

i=1

i=1

N

= F (k ) + k

i F (k ), i F (k ) - i F (k ) + ki

i=1

- L(k)2 N 2

i F (k ) - i F (k ) + ki 2

i=1

 F (k )+

k 2

- L (k )2

F (k ) 2 -

k 2

+ L (k )2

N

2
i F (k )-ki .

i=1

Then, by specifying the actor step-size as k  1/4L, we obtain the statement in Lemma 4.9.

D Proof of Lemma 4.10

Proof. For simplicity, we denote Fk-1 as the -algebra generated by all the trajectories kk ==k0-1Bk . Before presenting the proof, let us provide the following lemma.

Lemma D.1. For each agent i, in each iteration k, the estimator (9) of the local occupancy measure [cf. (4)] associated with policy k satisfies

Prob

^ki - (i)k

2



2 2Hk (1 - )2

+

4 + 16 log k-1 (1 - )2Bk

 k

(22)

As a result, with the shadow reward rik = (i) F (i)k as in (10), we may write

Prob

r^ik - rik

2 



2L2  2Hk (1 - )2

+

(4

+ 16 log k-1)L2 (1 - )2Bk

 k

(23)

We move the proof of this lemma at the end of Appendix D.

1. Proof of Lemma 4.10(i). First, for any   Bk, by direct computation, the estimation error of the actor update direction may be written as

Gwi (, r^ik, wik) - Gwi (, rik, wik) 2 =

H

H

2

t · t -t(r^ik(st, at) - rik(st, at))(st, at)

t=0

t =t



C2 r^ik - rik (1 - )4

2
.

where we have computed the maximum over (s, a) on the right-hand side, Cauchy-Schwartz, applied

the boundedness of the feature representation (Assumption 4.6), and applied the identity for a

geometric

sum

defined

by

the

discount

factor.

Let

us

denote

gwk i

=

1 B

Bk Gwi (, rik, wik), then

by the triangle inequality and the boundedness of the feature representation, we have we also have

2

ki - gwk i

=

2

1 Bk

 Bk

Gwi (, r^ik, wik)

-

1 Bk

 Bk

Gwi (, rik, wik)

2



1 Bk  Bk

Gwi ( k,j , rik, wik) - Gwi ( k,j , (rik), wik)

 Gwi (, r^ik, wik) - Gwi (, rik, wik) 2



C2 r^ik - rik (1 - )4

2
,

(24)

18

with probability 1. On the other hand, the magnitude of the directional error associated with the shadow critic estimate may be written as

wi i(wik; k )-E gwk i |Fk-1

+

=E

t ·

+
(st, at), wik - t -trik(st , at ) (st, at) , k

t=0

t =t

Hk

H

-E

t · (st, at), wik - t -trik(st , at ) (st, at) , k

t=0

t =t

+

+

E

t · (st, at), wik - t -trik(st , at ) (st, at) , k

t=Hk +1

t =t

Hk

+

+E

t

t -trik(st , at )(st, at) , k

(25)

t=0 t =Hk+1

 Hk 

·

C2

wik

1-

+

Hk · C rik (1 - )2



+ Hk · HkC . 1-

(26)

where the first equality follows from adding and subtracting

+ t=Hk

+1

[

t

+ t =Hk+1

t

-t rik (st

, at

)](st,

at)

inside

the

expectation,

and

the

second

ap-

plies the triangle inequality. The last follows from computing the maximum on the right-hand side,

and applying the boundedness of the feature representation (Assumption 4.6), and the identity for a

geometric sum defined by the discount factor. Consequently, with probability 1, we have

wi i(wik; k ) - E gwk i |Fk-1

2



3C2  2Hk (1 - )2

·

C2

wik

2

+

Hk2

+

(1

CF2 - )2

.

(27)

Now let us use concentration bound to control gwk i - E gwk i |Fk-1 2. Note that

Gwi (, rik, wik) 2 =

Hk

2

t · (Qwik(st, at) - Q^ti) · wi Qwik (st, at)

t=0

Hk

2

=

t · ((st, at) (wik) - Q^ti) · (st, at)

t=0

 C2

Hk
t ·

C wik

+ CF 1-

2

t=0



2C2 (1 - )2

·

C2

wik

2

+

(1

CF2 - )2

.

Consequently, by Lemma 18 of [27] and the definition of gwk i , we have

Prob

gwk i -E gwk i |Fk-1

2



(4

+ 16 log(1/k))C2 Bk(1 - )2

C2

wik

2

+

(1

CF2 - )2

 k. (28)

Now, the directional error associated with the critic with respect to kwi defined in Algorithm 1 together with basic inequality (a + b + c)2  3(a2 + b2 + c2) allows us to write:

2
kwi - wi i(wik; k )

2
= kwi - gwk i + gwk i - E gwk i |Fk-1 + E gwk i |Fk-1 - wi i(wik; k )

2
 3 kwi - gwk i + 3 gwk i - E gwk i |Fk-1

2 + 3 E gwk i |Fk-1 - wi i(wik; k ) 2 .

19

Combining (24), (27), (28), (23) and summing the inequality over all agents i yields (14).

2. Proof of Lemma 4.10(ii). The proof of this inequality is similar to that of (14). First, define

the mini-batch gradient evaluated at the optimal critic parameters in the sense of (12) as gki =

1 Bk

Bk Gi (, wk+1). Then

2

2

ki - i F (k ) = ki -gki +gki -E[gki |Fk-1]+E[gki |Fk-1]-i F (k )

(29)

3

ki -gki

3
+3

gki -E[gki |Fk-1]

2+3

E[gki |Fk-1]-i F (k ) 2 .

where we add and subtract gki and E[gki |Fk-1] and the basic inequality (a+b+c)2  3(a2 +b2 +c2). Now we bound the three terms one by one. For the first term, we substitute in the definition of ki in Algorithm 1 and apply the triangle inequality, and upper-bound this expression by its maximum, making use of Proposition 4.7 and the boundedness of the score function (Assumption 4.2) :

2

ki - gki

=

2

1 Bk

 Bk

Gi (, wik+1)

-

1 Bk

 Bk

Gi (, wk+1)



1 Bk  Bk

Gi (, wik+1) - Gi (, wk+1) 2

Hk

2

=

t · (st, at), wik+1 - wk+1 · i log (iik)(at(i)|st)

t=0



C2 C2 (1 - )2

·

wik+1 - wk+1 2.

(30)

For the second term on the right-hand side of (29), by Proposition 4.7 and the boundedness of the score function (Assumption 4.2), we may write

E Gi (, wk+1) 2 |Fk-1


H

2



= E

t · (st, at), wk+1 i log (iik)(at(i)|st) |Fk-1

t=0



C2C2 wk+1 (1 - )2

2
.

Then by Lemma 18 of [27], again, we have

Prob

gki -E[gki |Fk-1]

2



2(1

+

4 log(1/k))C2C2 (1 - )2Bk

wk+1

2

 k.

(31)

Finally, for the last term on the right-hand side of (29), we have

N
E[gki |Fk-1]-i F (k ) 2

(32)

i=1

N

Hk

2

=

E

t (st, at), wk+1 · i log (iik)(at(i)|st) , k -i F (k )

i=1

t=0

N

+

2

2 E

t (st, at), wk+1 · i log (iik)(at(i)|st) , k

i=1

t=Hk +1

N

+

2

+2

E

t (st, at), wk+1 · i log (iik)(at(i)|st) , k -i F (k )

i=1

t=0

E2k



22Hk C2C2N wk+1 (1 - )2

2

+ 2E2k .

20

where we have used the triangle inequality, Proposition 4.7, the boundedness of the score function (Assumption 4.2), and the boundedness of the feature representation (Assumption 4.6). Combining (29), (30), (31) and (32) and summing over i = 1, ..., N allows us to conclude the result of Lemma 4.10(ii), with the random variable defined as

N

k = 3

gki -E[gki |Fk-1] 2 .

i=1

(33)

D.1 Proof of Lemma D.1

Proof. First, for the ease of notation, we denote the empirical occupancy measure estimate of agent i

along trajectory  as i( ) :=

H t=0

t

·

e(st(i), at(i)).

Then

it

is

easy

to

see

that

for

any





Bk ,

one

has

E

^ki |Fk-1

- (i)k

=
1

E [i( )|Fk-1] - (i)k

1

 Hk .
1-

(34)

Second, note that

i( )

2



1 (1-)2

almost surely.

Then, by Lemma 18 of [27], we have



2

1 Prob  Bk Bk i( ) - E i( ) Fk-1

   exp

(1 - )2 -

· Bk - 2

8

By setting

=

, 2+8 log k-1
(1- )2 Bk

we

have

Prob ^ki - E i( ) Fk

2



2 + 8 log k-1 (1 - )2Bk

 k.

Consequently, we have

^ki - (i)k

2



2 2Hk (1 - )2

+

4 + 16 log k-1 (1 - )2Bk

w.p.

1 - k,

(35)

which completes the proof.

E Proof of Lemma 4.11

Proof. First, for the mixing matrix M , we introduce the following supporting lemma, whose proof is straightforward and is hence omitted.

Lemma E.1. Let the mixing matrix M satisfy the Assumption 4.3, then

where

Mk

-

1 N

· 1N 1N

 k, k  1
2

· 2 stands for the spectral norm of a matrix.

This result is standard in multi-agent optimization and spectral graph theory ­ see [45][Proposition
1 and 2]. For the ease of notation, let us work with the following matrix version of the critic objective function as well as the update of the Algorithm 1. Denote   Rd×|S||A| be the feature matrix, with the (s, a)-th column being (s, a). Denote k := Diag(k ) as a diagonal matrix in RSQQi|imS|kik|lA(a·=|r)×ly|dS,Qe|w|fiA1en|e.kadlD,sQoeinn2do(ekt1ne,1o·Q)t·.eF· QF,kQiinNaklRkly|,SR|b|Ay|SR||sa||tASsa||c|tAhktoe|i×nvbgNeec.ttthhoTeerhizvveaeentccibottooynrrsiodzfeaQfitthiinoeinktgiolootnfob,gtahweletsehhleohardca,vaowlewseQhQdaF-defuknonow=ctetQioQt-nhfeuQnkmFc·taikot1N(rnNi·x)s,.

21

where 1N is an N-dimensional all-one column vector. With those definitions, we can rewrite the critic objective and its gradient as

1 (w; k ) = 2

 w - QFk

2 k

and w (w; k ) = k( w - QFk ).

(36)

1 i(wi; k ) = 2



wi - Qik

2 k

and wi i(wi; k ) = k( wi - Qi k ).

(37)

Therefore, the update of the critic variables can be written as

W k+1 = W k - wk k  W k - Qk + wk Wk · M m.

(38)

Multiplying

both

sides

of

the

above

equation

with

1 N

· 1N

yields

w¯k+1

=

1 N

· W k+1 · 1N

=

W k - wk k

 W k - Qk

+ wk Wk

· M m · 1N N

=

W k - wk k

 W k - Qk

+ wk Wk

· 1N N

=

w¯k - wk k(

w¯k - Qk ) + wk Wk 1N . N

That is, the average of the local updates satisfies:

w¯k+1 = w¯k - wk k(

w¯k - Qk ) + wk Wk 1N . N

(39)

Consider the difference between W k+1 and w¯k+1 respectively in (38) and (39):

W k+1 - w¯k+1 · 1N

=

W k - wk k



W k -Qk

+wk Wk

·Mm-

w¯k -wk k(

w¯k -Qk )+ wk Wk 1N N

1N

= W k -wk k  W k -Qk +wk Wk ·M m -

w¯k -wk k(

w¯k -Qk )+ wk Wk 1N N

1N M m

= I -wk k

W k -w¯k · 1N ·M m +wk k Qk 1N -Qk M m -wk Wk

1N 1N N

-Mm

= I -wk k

W k-w¯k · 1N ·M m +wk k

Qk

1 · N

1N 1N

-

Qk

M m - wk Wk

1N 1N N

-Mm

= I - wk k

W k -w¯k · 1N ·M m + wk (kQk - Wk )

1 N

1N 1N

-

Mm

.

In the second equality, we exploit the fact that 1N is an eigenvector of M , in the third we group like
terms, and in the fourth we again use the fact that 1N is an eigenvector of M , and lastly we again
group like terms. Let us repeat the above recursion backwards k = k, k - 1, . . . , 0, and use the fact that W 0 - w¯01N = 0 due to our initialization. Then we have

k
W k+1 - w¯k+11N =

kt=k +1(I -wt t ) ·wk (k Qk - Wk )

1 N

1N 1N

-M m(k-k

+1)

.

k =0

(40)

For the above equality, because wt  1/Lw, we have for the first factor on the right-hand side of the preceding expression

kt=k +1(I -wt t ) 2  kt=k +1(1-wt µw)  1.

(41)

22

On the other hand, the later two factors on the right-hand side of (40) may be upper-estimated by the gradient estimation error Wk [defined in Lemma 4.10] and problem-dependent constants as

wk (k Qk - Wk )

1 N

1N 1N

-M m(k-k

+1)

F

 wk

1 N

1N 1N

-

M m(k-k

+1)

· k Qk - Wk

2

F

 wk

1 N

1N 1N

-

M m(k-k

+1)

·
2

k Qk F + Wk F

 wk m(k-k +1) k Qk F + Wk F

(42)

 = wk m(k-k +1)  Wk F +

N

2

k (s, a) · Qi k (s, a) · (s, a) 

i=1 s,a



(a)

N

 wk m(k-k +1)  Wk F +

k (s, a)

2

 CF2 C2

(1 - )2 

i=1 s,a





Wk

F+

N CF C (1 - )2

wk m(k-k +1)

where the first inequality applies Cauchy-Schwartz, the second employs the triangle inequality.

Moreover, the third inequality makes use of and the fourth expands the definition of

Lemma E.1 k Qk

and F.

the definition Additionally,

of Wk [see Lemma 4.10], (a) applies the fact that

Qi



CF 1-

,

and

the last

inequality

replaces

the

occupancy

measure

by

its

magnitude.

Combining

(40), (41) and (42) yields

W k+1 -w¯k+11N





max
k k

Wk

F+

N CF C (1 - )2

k

m

wk m(k-k ).

k =0

Squaring both sides of the above inequality yields the result of Lemma 4.11.

F Proof of Lemma 4.12

Proof. First, for the ease of notation, we will use wk+1 to denote w(k). The key observation that underlies our proof is the fact that (39) and (36) taken in tandem may be reinterpreted as the averaged critic parameter performing gradient descent of the critic objective function (w; k ), with the error Wk . The optimality condition of (w; k ) may be rearranged to write
kQk = k wk+1.

Combining the above equation with (39) yields

w¯k+1 - wk+1 = I - wk k

(w¯k

-

wk+1)

+

wk Wk 1N N

.

Computing the magnitude of both sides and applying (41) to (I - wk k ) yields:

w¯k+1 - wk+1  1 - wk µw

w¯k - wk+1

+ wk Wk N

F.

(43)

23

Consequently, squaring both sides, grouping like terms, and adding and subtracting wk inside the norm-difference term yields

w¯k+1 - wk+1 2 =  

1 - wk µw 2 1 + wk µw

w¯k - wk+1 2 +

1 1 + wk µw

1 - wk µw

w¯k - wk + wk - wk+1

2 + 2wk

Wk

2 F

µw · N

1 - wk µw

1 + wk µw 2

w¯k - wk 2

(wk )2

Wk

2 F

N

+ 1 - wk µw

1 1 + 2wk µw

wk - wk+1

2 + 2wk

Wk

2 F

µw · N

 1 - wk µw 2

w¯k - wk

2

+

1 wk µw

wk - wk+1

2 + 2wk

Wk

2 F

µw · N

(44)

where we frequently use the fact that (a + b)2  (1 + c)a2 + (1 + c-1)b2 for any c > 0. Next, we analyze the second term on the right-hand size of (44).

N

wk - wk+1 2  Cw2 k - k-1 2  Cw2 (k-1)2

ki-1 2

i=1

N

N

 2Cw2 (k-1)2

i F (k-1 ) 2 + 2Cw2 (k-1)2

i=1

i=1

i F (k-1 ) - ki-1 2

 2Cw2 (k-1)2 F (k-1) 2

+

3C2 C2 (1 - )2

N

wik -wk 2 + k-1 + 6E2k-1 + O(2Hk-1 )

i=1

 2Cw2 (k-1)2 F (k-1 ) 2 + k-1 + 6E2k-1 + O(2Hk-1 )

+

6C2 C2 (1 - )2

N

w¯k - wk 2 + wik - w¯k 2 .

(45)

i=1

In the first line, we applied the smoothness of wk+1 with respect to k in defined in as a hypothesis of Lemma 4.12, the second inequality makes use of the definition of the update for k in terms of the

update direction ki-1 and recursively expands the sum followed by the triangle inequality. The third substitutes in the definition of the update ki-1 from Algorithm 1, adds and subtracts F (k-1 ) inside the norm and applies the triangle inequality. The fourth inequality applies Lemma 4.10(ii) to the

second term on the right-hand side, and the identity

N i=1

i F (k-1 ) 2 =

F (k-1 ) 2.

Then, combining (44) and (45) yields

w¯k+1 - wk+1

2



2Cw2 (k-1)2 wk µw

F (k-1 ) 2 + k-1 + 6E2k-1 + O(2Hk-1 )

(46)

+

6C2 C2 (1 - )2

N

w¯k - wk 2 + wik - w¯k 2

+ 1 - wk µw 2

i=1

w¯k -wk

2 + 2wk

Wk

2 F

µw · N



1-

wk µw 2

+

2Cw2 (k-1)2 wk

·

6N C2C2 (1 - )2

·

w¯k -wk

2

+

2Cw2 (k-1)2 wk µw

F (k-1) 2

+

2wk

Wk

2 F

µw · N

+

2Cw2 (k-1)2 wk µw

(k-1

+

6E2k-1

+

O(2Hk-1 ))

+

2Cw2 (k-1)2 wk µw

·

6C2 C2 (1 - )2

·

N i=1

wik - w¯k 2

24

Because the step-size is selected such that k-1



(1-)µwwk , then
4 3N Cw CC

2Cw2 (k-1)2 wk µw

· 6N C2 C2
(1-)2



wk µw 4

.

Consequently,

the

right-hand

side

of

the

preceding

expression

may

be

simplified

as

w¯k+1 - wk+1 2



1 - wk µw 4

·

w¯k - wk

2

+

2Cw2 (k-1)2 wk µw

F (k-1 )

2 + 2wk

Wk

2 F

µw · N

+

2Cw2 (k-1 wk µw

)2

k-1 +6Ek2-1 +O(2Hk-1 )

+

2Cw2 (k-1)2 wk µw

·

6C2 C2 (1 - )2

N
·
i=1

wik -w¯k 2.

Note that we use wk+1 and wk to denote w(k) and w(k-1) respectively, this proves Lemma 4.12.

G Proof of Lemma 4.13

Before presenting the proof, let us first provide a supporting lemma.

Lemma G.1. For Algorithm 1, if for any k  0 we choose the step sizes to be wk+1  wk  1/Lw,

and k = min

(1-)µw wk+1 Cw CC

·

1  ,
max{4 3N ,6 10}

1 4L

, then it holds that

k 4

F (k )

2-

k-1 8

F (k-1 ) 2

(47)



Rk+1

-

Rk

+

9C2 C2 k 2(1 - )2

·

N

w¯k+1 - wik+1

2

+

3C2 C2 k-1 4(1 - )2

·

N

w¯k - wik 2

i=1

i=1

+

k-1 8

·

(wk )2

Wk

2 F

N Cw2 (k-1)2

+

3k 4

(k

+6E2k

+O(2Hk

))+

k-1 8

(k-1

+ 6E2k-1

+O(

2Hk-1

)).

The proof of Lemma G.1 is provided in Appendix H.

Proof. Begin by considering the result of Lemma G.1 and sum over iteration k = 1, 2, ..., T :

T k 8
k=1

F (k )

2-

0 8

F (0 )

2+

T 8

F (T ) 2

(48)



RT +1

- R1

+

11C2 C2 2(1 - )2

·

T

k-1

N

k=1

i=1

w¯k - wik

2

T
+
k=1

k-1 8

·

(wk )2

Wk

2 F

N Cw2 (k-1)2

T
+

7k 8

(k

+

6E2k

+

O(2Hk )).

k=0

Consequently, by rearranging the preceding expression such that F (k ) 2 is on the left-hand side, and lower-bounding the average gradient-norm-squared by its minimum over k permits us to
write:

T k=1

k

F (k ) 2

T k=1

k

(49)



8(RT +1 - R1)

T k=1

k

+

0

F (0 )

T k=1

k

2

+

44C2 C2 (1 - )2

·

T k=1

k

N i=1

w¯k - wik

2

T k=1

k

+

1

T k=1

k

·

T k=1

k-1

(wk )2 N Cw2

Wk (k

2 F
)2

+

7

T k=0

k (k

+ 6E2k

T k=1

k

+

O(2Hk )) .

25

Then Lemma 4.10 and the union bound indicates that with probability at least 1 - 3N

T k=0

k ,

it

holds for any 0  k  T that

Wk

2 F



O

log(1/k )C2 (1 - )2Bk

N
C2
i=1

wik

2

+

N CF2 (1 - )2

+ N L2

+ 2Hk

(50)

and

k  O

C2 C2 (1 - )2

· N log(1/k) Bk

wk+1

2

.

Consequently, for (49), the term involving the directional error Wk satisfies

1

T k=1

k

·

T k=1

k-1

(wk )2 Wk N Cw2 (k

2 F
)2

(i)


max{48N, 360} · C2C2 N (1 - )2µ2w

·

T k=1

k-1

Wk

2 F

T k=1

k



C2 C2 (1 - )2µ2w

·

1

T

T k=1

k

k=1

k-1O

log(1/k )C2 (1 - )2Bk

N
C2
i=1

wik

2

+

N CF2 (1 - )2

+ N L2

+ 2Hk

O

T k=1

k-1

·

N log(1/k) Bk

+

 2Hk

T k=1

k

where (i) is due to the requirement on k and wk , the second equality makes use of (50), and the last suppresses dependence on complicated constants. Returning to the consensus error term on the
right-hand side of (48):

44C2 C2 (1 - )2

·

T k=1

k

N i=1

w¯k - wik

2

T k=1

k

(51)

(i)


44C2 C2 (1 - )2

·

T k=1

2k

maxk k

Wk

+ 2

N CF2 C2

F

(1-)4

·

T k=1

k

k k

=0

wk

m(k-k

)

2
· 2m



(ii)


O

 N C4CF2 C22m  (1 - )6

·

T k=1

k

·

k k

=0

wk

m(k-k

)

2



T k=1

k





T k=1

k

·

= O



k k

=0

wk

m(k-k

)

T k=1

k

2



· 2m 

where (i) is due to Lemma 4.11 and (ii) follows via our choice of batch size Bk and horizon length

Hk horizon yielding the condition

Wk

2 F

= O(Bk-1 + 2Hk )  O

N CF2 C2 (1-)4

. Now we study the

directional error of the actor k on the right-hand side of (48):



7

T k=0

k

k

T k=1

k

O



N (1

C2 C2 - )2

·

 T

k log(1/k) wk+1

k=1 

Bk

2

T k=1

k



=O

T k=1

k

·

log(1/k)

·

Bk-1

T k=1

k

.

(52)

Combining (49) - (52) then allows us to establish the result.

26

H Proof of Lemma G.1
Proof. Begin with a rearrangement of the expression in Lemma 4.9 in terms of an upper-bound on the gradient-norm:

k 4

F (k ) 2



F (k+1 ) - F (k ) + 3k N 4

i F (k ) - ki 2

i=1

(53)

(i)


F (k+1 )

-

F (k )

+

9k C2 C2 4(1 - )2

N

wik+1 - wk+1

2 + 3k 4

k + 6E2k + O(2Hk )

i=1



F (k+1 ) - F (k ) + 3k 4

k + 6E2k + O(2Hk )

+

9C2 C2 k 2(1 - )2

N

i=1

wik+1 - w¯k+1 2 + w¯k+1 - wk+1 2

(=ii)

Rk+1

-

Rk

+

3k 4

k + 6E2k + O(2Hk )

+

9C2 C2 k 2(1 - )2

N

wik+1 - w¯k+1 2

i=1

N

-

w¯k - wk 2 +

i=1

9C2 C2 k 2(1 - )2

+

N
w¯k+1 - wk+1 2
i=1

where (i) is due to Lemma 4.10(ii), (ii) is due the definition of the potential function (18). Now apply Lemma 4.12 to the last term on the right-hand side of the preceding expression to obtain

9C2 C2 k 2(1 - )2

+



N

N
w¯k+1 - wk+1 2 - 

w¯k - wk 2

i=1

i=1



9C2 C2 k 2(1 - )2

+



·

2Cw2 (k-1)2 wk µw

F (k-1 )

2

+

2Cw2 (k-1)2 wk µw

k-1

+ 6E2k-1 + O(2Hk-1 )

+

2wk

Wk

2 F

µw · N

+

2Cw2 (k-1)2 wk µw

·

6C2 C2 (1 - )2

N
·
i=1

w¯k - wik 2

+ 1 - wk µw 4

3C2 C2 k (1 - )2

+

-

N
w¯k - wk 2

i=1

Now select the coefficient as



=

18C2 C2 (1 - )2µw

· max
k0

k wk

27

in the potential function Rk [cf. (18)] such that the coefficient before

N i=1

w¯k -wk

2 is nonpositive.

Doing so then yields

9C2 C2 k 2(1 - )2

+



N

N

w¯k+1 - wk+1 2 - 

w¯k - wk 2

(54)

i=1

i=1

(i)


9C2 C2 k 2(1 - )2µwwk

+



N

N

w¯k+1 - wk+1 2 - 

w¯k - wk 2

i=1

i=1



45C2 C2 2(1 - )2µw

· max
k 0

k wk

·

2Cw2 (k-1)2 wk µw

F (k-1 )

2

+

2Cw2 (k-1)2 wk µw

k-1

+6E2k-1 + O(2Hk-1 )

+

2wk

Wk

2 F

µw · N

+

2Cw2 (k-1)2 wk µw

·

6C2 C2 (1 - )2

·

N i=1

w¯k - wik 2



45C2C2Cw2 (k-1)2 (1 - )2wk µ2w

· max
k 0

k wk

·

F (k-1 ) 2 + k-1 + 6E2k-1 + O(2Hk-1 )

+

(wk )2

Wk

2 F

N Cw2 (k-1)2

+

6C2 C2 (1 - )2

·

N i=1

w¯k - wik

2

where (i) is because we choose the stepsize wk s.t. µwwk  µw/Lw  1. Note that the actor

update

step-size

is

chosen

to

satisfy

k



k-1



(1-)µwwk ,
6 10Cw CC

which

implies

45C2 C2 Cw2 (k-1)2 (1-)2wk µ2w

·

maxk 0

k wk



. k-1
8

Under

this

selection,

by

combining

(53)

and

(54)

we

may

write

k 4

F (k )

2-

k-1 8

F (k-1 ) 2



Rk+1

-

Rk

+

9C2 C2 k 2(1 - )2

·

N

w¯k+1 - wik+1

2

+

3C2 C2 k-1 4(1 - )2

·

N

w¯k - wik 2

i=1

i=1

+

k-1 8

·

(wk )2

Wk

2 F

N Cw2 (k-1)2

+

3k 4

(k

+6E2k

+O(2Hk

))+

k-1 8

(k-1

+ 6E2k-1

+O(

2Hk-1

)).

This proves the result.

I Proof of Theorem 4.14

Proof. Lemma 4.13 says

T k=1

k

F (k )

T k=1

k

2

O

(RT +1 - R1) + 0 F (0 ) 2

T k=1

k

+O

T k=0

k E2k

T k=1

k

+O

T k=1

k-1

·

N

log(1/k ) Bk

+

 2Hk

T k=1

k



T k=1

k

·

+O



k k

=0

wk

m(k-k

)

T k=1

k

2



· 2m . 

In the first case where we take constant stepsizes and batchsizes, we have can simplify this inequality as

1T T

F (k ) 2  O

k=1

1 +
T 

T k=0

E2k

T

+

1 B

+ 2H

+ w2 2m

.

28

where we write Bk  B, Hk  H, k  , wk  w. Consequently, setting these parameters according to the theorem and using the fact that E2k  W proves that

1T T

F (k ) 2  O ( + W ) ,

w.p. 1 - .

k=1

The total sample complexity is T × B × H = O~( -2.5).

For the second case, we should notice that for the consensus error term, for sufficiently large k, we have

k

k

wk m(k-k ) = O

(k + 1)-1/3m(k-k )

k =0

k =0

 k/2

k



 O

m(k-k ) +

( k/2

+

1)-1/3m(k-k

)


k =0

k = k/2 +1

= O m k/2 + ( k/2 + 1)-1/3

= O (k + 1)-1/3 .

As a result, according to our selection of the parameters, we have

T k=1

k

=

(T 2/3).

Notice

that

Tk=0(k+1)-1 = O(log T ), then choosing Hk = O(log((k+1)-2/3)) = O((1-)-1 log(k+1))

and Bk = log(1/k)(k + 1)2/3 gives

T k=1

k-1

·

N

log(1/k ) Bk

+

 2Hk

T k=1

k

= O(T -2/3 log T )

and

T k=1

k

·

k k

=0

wk

m(k-k

)

T k=1

k

2
· 2m = O(T -2/3 log T ).

substitute the above inequalities into Lemma 4.13 proves that

T k=1

k

F (k )

T k=1

k

2

O

log T

T2 3

+W

,

w.p. 1 - ,

where the failure probability is due to the following argument

T





2

3N k  3N k = 3N N 2(k + 1)2 = .

k=0

k=0

k=0

To make

log T
2

=

, we need T = O~( -3/2). Consequently, the total sample complexity will be

T3

T
Bk × Hk = O~
k=0

-3/2
k2/3
k=1

= O~( -5/2)

.

I.1 Proof of Corollary 4.15

Proof. Because W = 0 in this scenario, the second setting (adaptive parameter selection) of Theorem 4.14 indicates that

T k=1

k

F (k )

T k=1

k

2

O

log T T2
3

,

w.p. 1 - 

29

for any T . If we set ¯T = k w.p.

k

T k

=1

k

,1k

 T.

Then

E[ F (¯T ) 2|FT ] =

T k=1

k

F (k )

T k=1

k

2

O

log T T2
3

,

w.p. 1 - .

Therefore, as long as the Assumption 1 of [68] is satisfied, [68] indicates that problem (1) has no saddle point. In this sense, Algorithm 1 is converging to the global optima.

I.2 Proof of Corollary 4.16

Proof. First, because Corollary 4.16 is also a constant algorithmic parameter setting, the proof of Theorem 4.14 indicates that

1T T

F (k ) 2  O

k=1

1 T 

+W

+

1 B

+

2H

+ w2 2m

.

Then choosing m large enough so that 2m = O( ) (equivalently m = O((1 - )-1 log( -1))) makes w2 2m = O( ) even if we choose a constant step size for the critic update, i.e., w = 1/Lw. This also enables us to choose a constant step size  for the actor update, as is defined the corollary. Therefore, we only need to set B = O( -1) and T = O( -1), which completes the proof.

J Supporting Results

J.1 Existence of the constant Cw In this section, we prove the following statement in Lemma 4.12:
Cw > 0 s.t. wk+1 - wk  Cw k - k-1 .

Proof. First, recall the matrix characterization of the critic problem in (36), where one has

1 (w; k ) = 2

 w - QFk

2 k

and w (w; k ) = k( w - QFk ).

Denote wk+1 = w(k) for the w (wk+1; k ) = k( wk+1

ease of notation. Consequently, the KKT - QFk ) = 0. This further indicates that

condition

yields

wk+1  (k )-1 · kQFk

(55)

(i)
 µ-w1

k (s, a)(s, a)QFk (s, a)

s,a



C µw

·

s,a

k (s, a) · |QFk (s, a)|

(ii)


C · CF (1 - )2 · µw

where (i) is because k

µw

· I,

and

(ii)

is

because

|QFk (s, a)|



CF 1-

for

any

(s, a).

Next, we show the Lipschitz continuity of the critic solution. Again, by the KKT condition of the critic

problem, we have k

wk+1 = kQFk and k-1

wk

=

k-1

Qk-1
F

.

Consequently,

k (wk+1 - wk)

(56)

= kQFk - k wk

=

kQFk - k

wk + (k-1

wk

-

k-1

Qk-1
F

)

=

k QFk

-

k

Qk-1
F

+

k

Qk-1
F

-

k-1

Qk-1
F

+ k-1

wk - k

wk

T1

T2

T3

30

For the first term, T1

=

k

(s,

a)(s,

a)(QFk

(s,

a)

-

Qk-1
F

(s,

a))

s,a



C · 1-

QFk

-

Qk-1
F



 C Q k - k-1 , 1-

T2 =

(k (s, a) - k-1 (s, a))(s, a)QFk-1 (s, a)
s,a



CCF (1 - )2

·

k - k-1





CCF  (1 - )2

k - k-1

,

T3  k-1 - k F · wk



C · CF ·

(k (s, a) - k-1 (s, a))(s, a)(s, a)

(1 - )2 · µw s,a

F



C3 · CF

· k - k-1

(1 - )2 · µw

1



|S||A|C3 CF  (1 - )2 · µw

·

k - k-1

Now, combing the above terms with (55), we have

wk+1 - wk

 (k )-1 ( T1 + T2 + T3 )



(1

C Q - ) · µw

+

CCF (1 - )2


· µw

+

|S||A|C3 CF  (1 - )2 · µ2w

· k - k-1

:= Cw · k - k-1 .

J.2 Boundedness of critic parameters

In this section, we prove that as long as the algorithmic parameters are chosen according to Theorem

4.14:

Dw > 0 s.t.

max{
k0

wk+1

,

wik

}  Dw,

w.p.

1 - .

Proof. First, the proof in the last seciton already indicates that wk+1 are all bounded. Second, for the other term, by lemma 4.12, we have

w¯k+1 - wk+1 2 

1 - wk µw 4

·

w¯k - wk

2

+

2Cw2 (k-1)2 wk µw

F (k-1 )

2+

2wk

Wk

2 F

µw · N

(57)

+

2Cw2 (k-1)2 wk µw

k-1

+

6E2k-1

+

O(2Hk-1 )

+

6C2 C2 (1 - )2

·

N

wik - w¯k 2

i=1

.

It is worth noting that due to our requirement on the stepsize:

k-1 = min

(1-)µwwk ·

1



1 ,

CwCC max{4 3N , 6 10} 4L

.

31

Then

(k-1)2 wk

= min

(1-)2µ2w Cw2 C2C2

·

1 max{48N,

360}

,

1 16L(wk )2

· wk .

Because the step size wk is either a constant or diminishing, the above coefficient before wk is

bounded. Let us denote c1 := supk0 min

· , (1-)2µ2w

1

1

Cw2 C2 C2 max{48N,360} 16L (wk )2

. Therefore, (57)

indicates

w¯k+1 - wk+1 2 

1 - wk µw 4

·

w¯k - wk

2 + wk µw 4

8c1Cw2 µ2w

F (k-1 )

2

+

8 Wk

2 F

µ2w · N

(58)

+

8c1Cw2 µ2w

k-1

+

6E2k-1

+

O(2Hk-1 )

+

6C2 C2 (1 - )2

·

N

i=1

wik - w¯k 2

.

According to Assumption 4.1, F (k-1 ) 2 is bounded. According to Lemma 4.10,

maxk0{ Wk

2 F

,

k }

is

bounded

w.p.

1 - .

By

Assumption

4.4,

E2
k-1

are

all

bounded

by

W . By Lemma 4.11, maxk0

N i=1

wik - w¯k

2 is bounded. Let us denote the upper bound of the

term

in

(58)

inside

the

bracket

with

coefficient

wk µw 4

as

c2.

We

have

w¯k+1 - wk+1 2 

1 - wk µw 4

·

w¯k - wk

2

+

wk µw 4

c2

 max{ w¯k - wk 2, c2}.

Repeat the above inequality yields

w¯k - wk 2  max{ w¯1 - w1 2, c2}, k  1.

Also note that Lemma 4.11 indicates that maxk0

N i=1

wik - w¯k

2 is bounded and previous proof

shows that wk is bounded. Therefore, the wik are also bounded for all k, i.

K Additional Experiments and Details

We have performed experiments for single as well as multi agent network for both the MountainCar of OpenAI Gym [9] and Cooperative navigation environments [40]. Since the state space is continuous for both the environments, we use discretization of the state space to estimate the respective occupancy measure. Unless otherwise stated, we have used a 2 layer with 64 nodes per layer deep neural network (DNN) for the actor as well as critic in the experiments. We use a learning rate of 0.001 for all the experiments and a batch size of 10 for the count based density estimator. One epoch in the experiment consists of 1000 episodes unless otherwise stated and one episode is a one trajectory in the environment. The maximum number of steps per episode are 300 for the MountainCar environment and its 50 for the Cooperative navigation environment. We have reported running averages for all the results reported in this paper, such as, the general utility, the constraint violation, and the consensus error. Next, we provide further details for each set of experiment we presented in the paper.

K.1 Single Agent Environment
Exploration Maximization. We solve the problem of maximum entropy exploration for continuous MountainCar [9], where the goal is to maximize the entropy of the long-term occupancy measure F () = - s s(log(s)). The two dimensional continuous state space is discretized into a [10, 10] grid size for occupancy measure estimation. There are three discrete actions available [, stay, ]. We used the simple count based estimator which counts the state visitation frequency to each bin. We use a fixed batch size of 100 to estimate the occupancy measure. The actor and critic employ deep neural network (DNN) parametrization as defined by two fully connected hidden layers composed of 128 nodes each with ReLU activations. We compare DSAC with a Monte Carlo rollout (MCR) based estimation for the shadow value function in a manner reminiscent of REINFORCE and a

32

(a) Entropy comparison

(b) State space coverage

Figure 5: (a) Entropy comparisons, (b) occupancy measure heat maps for MountainCar environment. Shadow Reward actor-critic achieves superior limiting entropy for this instance.

(a) World model

(b) Average return

(c) Average cost

Figure 6: (a) World model for single-agent navigation with green as safe and brown as unsafe states. The goal is to travel from Start to Goal safely. (b) Undiscounted average reward return comparison. (c) Average constraint violation comparison for different values of penalty parameter z [cf. (59)]. Constraints yield avoidance of unsafe regions, in contrast to maximizing cumulative return.

random baseline when agent took actions randomly with uniform distribution. In Fig. 5(a), we have compared the performance with MaxEnt [21] and Variational Policy gradient (VPG) [68] for a neural parameterization of actor and critic. Fig. 5(b) shows the heatmap of occupancy measure obtained at three different epochs 1, 15, 39 for DSAC (blue), MaxEnt (green), Random (purple). Observe that our approaches based on shadow rewards (DSAC and the Monte Carlo variant (MCR)) yield superior performance in terms of entropy and hence space coverage (see Appendix K.1 for implementation details).

Safe Navigation. We experiment with a single agent version of Cooperative navigation environment
[40], which is depicted in Fig. 6(a): the task is to go from the start to goal without passing through
the unsafe (brown) region (green represents the safe region). We impose the safety via constraints , c  C on accumulated costs c(st, at) via a quadratic penalty:

F () = , r - z ( , c - C)2 ,

(59)

where z is the penalty parameter. The two-dimensional location of the agent represents the state and five discrete actions are available to agent as [, , , , stay]. We discretize the space into [200, 200] size grid for the occupancy measure estimation. The actor and the critic are two-layer fully-connected DNNs with 64 nodes at each layer and ReLU activations. The performance of the proposed algorithm is shown in Fig.6(b)-6(c) for different values of penalty z. Experimentally, we consider a fixed cost of c(st, at) = 1 for every visit to the unsafe region with a threshold of C = 0.001 (see Appendix K.1 for details). Observe that penalization yields behavior that avoids the unsafe region as compared to the unconstrained objective.

To illuminate the role of shadow rewards for the safe navigation problem (cf. Fig. 6(a)), see Fig. 7. The shadow reward arises from the second penalty term we add to the objective in (59) the appendix. In additional to the reward r, we also receive a an additional reward due to the penalty we have in the objective (see (59)). We first report the estimated occupancy measure for different epochs in the first

33

Figure 7: Occupancy measure and shadow reward for safe navigation environment shown in Fig. 2(a). The heatmaps here show the discretized version of the two dimensional state space (location of the agent) in the environment. The goal is to reach from start (bottom left) to the goal (upper right) while avoiding the mud.The first row depicts that trajectories traverse through the unsafe region since there is no penalization happening for z = 0 at the outset of training. The second row shows the occupancy measure estimate when we use z = 10 later on in training, which illuminates that the learned trajectory begins to avoid the unsafe region. The shadow reward over the course of training places a highly negative reward (red) for visiting the unsafe region and zero reward (green) for the safe region, which yields avoidance behavior. The result is depicted in the third plot of second row, which satisfies the safety constraints.

row of Fig. 7 for z = 0 (zero penalty case). We note that at epoch 0, the algorithm samples nearly uniformly across the space of trajectories from start to goal, and has not internalized any penalization associated with unsafe zones. ­ see the first row of Fig. 7 (epoch 28). Over the course of training, the shadow reward for all the states incentivizes avoidance of unsafe states as may be observed in the last row of Fig. 7 (epoch 0) with the corresponding occupancy measure shown in the second row. In the last row of Fig. 7, red means the low reward and green means the high reward states. Hence, the proposed algorithm creates a shadow reward which discourages visitation of the unsafe region and hence obtains a trajectory through the safe region as shows in second row of Fig. 7 for epoch 28.

K.2 Multi Agent Environment

Exploration Maximization. For the two agent case we are considering as shown in Fig 2(a), the

local two-dimensional state of each agent is their position in the x - y plain, and the action space is a

discrete set containing five choices [, , , , stay]. The network is complete for this experiment.

Further, to obtain an estimate of occupancy measure, we discretize the two-dimensional state space into [10, 10] grid, so that the global occupancy measure for the two-agent problem case is 104 dimensional while the marginalized occupancy measure for each agent is 102 dimensional. Safe

Cooperative Navigation. We consider a two agent cooperative environment from [40] where each

agent is equipped with the task to reach its assigned goal without traversing via the unsafe region.

Note that this behavior could be learned in a policy of a agent i via imposing a safety constraint for each agent i , c  C where i in the marginalized occupancy measure. This local constraint could be introduced into the global common objective as a quadratic penalty in a manner similar to

(59) as

F () = 1 N

N

i , ri

N
- z ( i , c - C)2 ,

(60)

i=1

i=1

where z is the penalty controlling parameter. We use the proposed DSAC algorithm to solve problem, and present the results for the average reward and constraint violation, respectively, in Fig. 2(b)-2(c). Further, a "demo.gif" file is submitted along with the paper to show the learned policy for 4 agents.

To further justify the proposed approach, we consider a set of 8 agents in a multi-agent cooperative environment similar to the one mentioned in Fig. 2(a) but with 8 agents. Similarly, the actor and critic are parameterized by a two layer DNN with 64 nodes per layer for each agent. We consider three different scenarios of network connectivity for this experiment namely; fully connected (FC)

34

(a) Average return

(b) Average cost

(c) Consensus error

Figure 8: Safe navigation in a multi-agent cooperative environment similar to the one mentioned in Fig. 2(a) with 8 agents and 8 landmarks. Note that the state space in this case would be 16 dimensional (location of agent and landmarks). We run this experiment for three communication graphs among agents; small world network (SWN) (all the agents are connected by the repeated generation of Watts-Strogatz small-world graphs with parameters k = 3 and p = 0.5) , ring (all the agents are connected using ring topology), and random (where agents are randomly using ErdosRe´nyi random graph model). (a) We plot running average of the reward return similar to Fig. 2(b). (b) We plot the running average of the constraint violation similar to Fig. 2(c). (c) We plot the running average of the consensus error for agent 1 and agent 4 for small world network, ring, and random network connectivity.

(all the agents are connected to each other), ring (all the agents are connected using ring topology), and random (where agents are randomly connected using Erdos-Re´nyi random graph model with p being uniformly selected between 0 and 1). We plot the running average of the reward returns (Fig. 8(a)), running average of the constraint violations (Fig. 8(b)), and running average of the consensus error (Fig. 8(c)) for agent 1 and 4 in Fig. 8. Since the consensus error was converging to zero quickly, we have plotted it using log scale and episodes for the x axis.

35

