arXiv:2106.01273v1 [cs.DC] 2 Jun 2021

CHUNK CONTENT IS NOT ENOUGH: CHUNK-CONTEXT AWARE RESEMBLANCE DETECTION FOR DEDUPLICATION DELTA COMPRESSION

A PREPRINT

Xuming Ye, Xiaoye Xue, Wenlong Tian Department of Computer Science University of South China Hunan, China 421001 xumingye@stu.usc.edu.cn 20189350201@stu.usc.edu.cn wenlongtian@usc.edu.cn

Zhiyong Xu Department of Computer Science
Suffork University Boston, USA 02108 zxu@suffolk.edu

Weijun Xiao Department of Electrical and Computer Engineering
Virginia Commonwealth University Richmond, USA 23229 wxiao@vcu.edu

Ruixuan Li Department of Computer Science Huazhong University of Science and Technology
Wuhan, China 430074 rxli@hust.edu.cn

June 3, 2021
ABSTRACT
With the growing popularity of cloud storage, removing duplicated data across users is getting more critical for service providers to reduce costs. Recently, Data resemblance detection is a novel technology to detect redundancy among similarity. It extracts feature from each chunk content and treat chunks with high similarity as candidates for removing redundancy. However, popular resemblance methods such as "N-transform" and "Finesse" use only the chunk data for feature extraction. A minor modification on the data chunk could seriously deteriorate its capability for resemblance detection. In this paper, we proposes a novel chunk-context aware resemblance detection algorithm, called CARD, to mitigate this issue. CARD introduces a BP-Neural network-based chunk-context aware model, and uses N-sub-chunk shingles-based initial feature extraction strategy. It effectively integrates each data chunk content's internal structure with the context information for feature extraction, the impact of small changes in data chunks is significantly reduced. To evaluate its performance, we implement a CARD prototype and conduct extensive experiments using real-world data sets. The results show that CARD can detect up to 75.03% more redundant data and accelerate the resemblance detection operations by 5.6 to 17.8 times faster compared with the state-of-the-art resemblance detection approaches.
Keywords Resemblance Detection · neural networks · Deduplication · Cloud Storage
1 Introduction
With the development of network and storage technologies, cloud storage has been widely used in daily life, such as Google Drive, Dropbox, and the Baidu Cloud Sharma et al. [2020]. Furthermore, people prefer to pay for their online data through the cloud storage service because of its reliability and flexibility. However, there are various redundancies
Coresponding Author Both authors contributed equally to this research

arXiv Template

A PREPRINT

among cross users' outsourced data, especially for the cloud storage scenario. These duplicate data seriously deteriorates the storage utility and increases the user's financial budget for cloud storage services. Thus, removing the duplicate data among these large volumes improve the cloud storage utilization and saves user money.
Therefore, deduplication techniques were proposed by Chaudhuri et al. [2007], Meyer and Bolosky [2011], Tian et al. [2018]. As the core part of deduplication techniques, the chunking algorithm Muthitacharoen et al. [2001] split the original data into small chunks. Then, the duplicate chunk is detected based on the hash comparison. To achieve a high deduplication ratio, the chunking algorithm is improved from the fixed-size to the content-defined chunking algorithm such as the BSW CDC Muthitacharoen et al. [2001], TTTD Eshghi and Tang [2005], Elastic chunking Tian et al. [2017] , and Fast CDC Xia et al. [2016]. Once a rolling hash value of a data slice divided by a predefined divisor value equals zero, the data slice's end position is a chunk boundary candidate. Only the candidate close to the maximum chunk size can be selected as a chunk boundary. If there is no candidate, the chunk size is set as the maximum chunk size. However, the chunk-level hash comparison in traditional chunking algorithms hardly detects the redundancy among resemblance chunks, which contains lots of duplicate data.
To detect the duplicate data among resemblance chunks, researchers have proposed lots of resemblance detection approaches Aronovich et al. [2009], Douglis and Iyengar [2003], Forman et al. [2005], Pucha et al. [2007], Xu et al. [2017]. Once two chunks are similar, only the diff part is stored by utilizing the delta compression Kulkarni et al. [2004]. As for a popular resemblance detection method, N-transform Shilane et al. [2012a] extracts all the Rabin fingerprints Rabin [1981] of a chunk. All the Rabin fingerprints values are linearly transformed N times into N-dimensional hash sets. The top-N values, one from each of the N dimensions, are selected as features. But the feature extraction positions in N-transform have probabilities located at the tail of the chunk or the chunk's head, which significantly deteriorates the accuracy of resemblance detection. Furthermore, the N-transform suffers from the feature calculations caused by the linear transformation process.
To overcome the above problem, Finesse Zhang et al. [2019], the state-of-the-art resemblance detection work, calculate the chunk features with a grouping strategy. Specifically, it divides the chunk into sub-chunks and extracts their corresponding Rabin fingerprints into several contiguous sets of the same size. Then, these values construct the feature by grouping together based on each set's rank. The goal of Finesse is to achieve better performance than the N-transform in resemblance detection. But, the feature extraction scheme in Finesse only depends on the content itself and is easily impacted by modifications, which is detailed in Section3. Thus, Finesse's resemblance detection accuracy is still lower than the N-transform's accuracy.
In summary, the exploration of resemblance detection is still at an elementary stage. In other words, the existing feature extraction mechanisms in the resemblance detection depends on the hash values from the content itself. Various modification patterns in the content change these hash values and deteriorate the feature extraction. Then, the resemblance chunks are dissimilar based on their features, which is detailed in Section 3. Fortunately, we discover that some chunks are co-occurred in the deduplication system. If two chunk's surrounding chunks are detected as resemblance chunks, these two chunks may also be similar with high probability. We call the contiguous sequence of n chunks from a given chunk as the chunk-context. Based on this observation, combining the co-occurred surrounding chunks information with the chunk's internal structure can significantly improve the chunk representations and the resemblance detection efficiency in deduplication.
Based on our observations, we first combine the chunk-context information with the chunk content to improve resemblance detection effectiveness and efficiency. Inspired by the traditional natural language processing, we attempt to embed the chunk-context information into the feature extraction processing and propose a chunk-context aware resemblance detection scheme, called CARD. It calculates the chunk features in two steps, initial feature extraction and chunk-context embedding. By introducing the N-sub-chunk shingles, CARD generates the initial feature, which reflects the chunk content internal structure. Then, a BP-Neural network-based chunk-context aware model further embeds the chunk-context information based on these initial features. By conducting extensive experiments, CARD achieves higher accuracy and faster performance than the state-of-the-art resemblance detection schemes. The main contributions are summarized as follows:
· First, we analyze the state-of-the-art resemblance detection work and present their limitations. The existing feature extraction methods in resemblance detection suffer from various modification patterns, which deteriorates similar chunk detection accuracy. Moreover, traditional resemblance detection schemes ignore the chunk-context, which plays a role in improves the effectiveness and efficiency of resemblance detection.
· Second, we design a chunk-context aware resemblance detection scheme, CARD, for deduplication delta compression to embed the chunk-context and the content itself into the feature. By leveraging N-sub-chunk shingles based initial feature extraction scheme and the BP-neural network based chunk-context aware model,
2

arXiv Template

A PREPRINT

CARD achieves a qualified chunk representation, which ensures a high resemblance detection accuracy under various modification patterns. · Contribution 3: Finally, we conduct extensive simulations to evaluate CARD. The experimental results show that our method outperforms the state-of-the-art resemblance detection schemes. It can remove up to 75.03% more redundancy data and accelerate the resemblance detection by 5.6×  17.8× compared with the state-of-the-art resemblance detection work, N-transform and Finesse.
The rest of the paper is organized as follows. The related work about resemblance detection in deduplication is summarized in Section 2. In Section 3, we analyze the problems and limitations of the latest resemblance detection work in deduplication. Then, we propose a solution for resemblance detection by embedding the chunk-context information in Section 4. Finally, we present experimental results in Section 5. In Section 6, we conclude the paper.
2 Related Work
With the prevalence of cloud and networks, cloud storage has been widely used. There is a lot of redundancy among different users. Thus, data deduplication becomes a critical technology in data-intensive storage scenarios for eliminating these duplicate data. Obviously, less redundancy among users efficiently saves money and improves cloud storage usage. Many researchers are dedicated to the deduplication in the cloud. To simplify the description, we elaborate these related work into two categories based on whether they support removing the redundancy among the similar chunks.
2.1 Traditional Deduplication
Traditional deduplication remove the duplicate copies by comparing hash value for each chunk Clements et al. [2009]. Many researchers focus on eliminating the duplicate data in primary and backup-level Lillibridge et al. [2013, 2009], Lu et al. [2012], Fu et al. [2014]. Other systems use different approaches to improve the effectiveness of data deduplication, such as integrating block-level deduplication with compression Upadhyay et al. [2012] and gloabal deduplication method Oh et al. [2018]. Moreover, data deduplication has also attracted considerable attention for virtual machine images Jin and Miller [2009], Srinivasan et al. [2012], Zhao et al. [2020]. However, there are lots of redundancy among non-duplicate but highly similar chunks, which can not be eliminated by traditional deduplication.
2.2 Resemblance Detection in Deduplication
Therefore, some researchers utilize the delta compression, a data reduction technique, to maximize the compression ratio Shilane et al. [2012b,a]. It takes the delta algorithm with delta format to record the difference between similar chunks. Only the differences are recorded in delta files. Nevertheless, the delta compression induces extra computation, and I/O overheads Zhu et al. [2008]. Moreover, delta compression can hardly answer which chunks should be treated as the candidates for delta compression.
Therefore, resemblance detection is proposed to detect the similar chunks. Aronovich el at. Aronovich et al. [2009] propose a novel type of similarity signatures serving in the deduplication system. It combines similarity matching schemes with byte by byte comparison or hash based identity schemes. Xu el at. Xu et al. [2017] propose a similaritybased deduplication system for database management systems by using byte-level encoding to achieve greater savings. There are also some other coarse-grained resemblance detection approaches Douglis and Iyengar [2003], Forman et al. [2005], Shilane et al. [2012a]. These methods extract features from non-overlapped chunks and may suffer from high false positives.
To overcome the above problem, the state-of-the-art work, such as N-transform Shilane et al. [2012a] and Finesse Zhang et al. [2019], attempts to improve the resemblance detection accuracy and performance using the grouping features mechanism. Both of these methods are chunk-level resemblance detection. In N-transform, it extracts the top k largest Rabin fingerprint values of a chunk. Then a super-feature of this chunk can be calculated by several such features. And Finesse extracts the largest Rabin fingerprint value in each subchunk and groups the Rabin fingerprint values as the features based on these values' ranking. However, we observe that two similar chunks also have the similar chunk-context and vice versa. The existing resemblance detection designs only consider the chunk content itself while ignoring the chunk-context's positive effect.
3 Limitations of Previous Solutions
In this section, we discuss the limitations in existing resemblance detection schemes. Most researchers extract each chunk's features to detect similar chunks, as shown in Figure 1. The common feature extraction way is to select the top
3

arXiv Template

A PREPRINT

k largest Rabin fingerprint values in a chunk as the feature. Then, the similarity distance could be measured by the resemblance detection. Only the different parts are stored according to the most similar chunks.
However, this naive resemblance detection method suffers a lot from modifications. To simplify the description, we take an example to show its disadvantages. As shown in Figure 1, we assume that ChunkA is an original version chunk. The ChunkB, ChunkC , and ChunkD are modification version based on ChunkA while the modification in ChunkB and ChunkC do not impact the top 5 largest Rabin fingerprint values, ((r1, r2, r3, r4, r5)). The modification in ChunkD is to delete the last piece of data compared with ChunkA, which contains one of the top 5 largest Rabin fingerprint value, r5. Thus, the features of ChunkD is assumed as (r1, r2, r3, r4, r6). Based on this naive feature extraction, it is hard to determine whether the ChunkB is more similar to ChunkA or ChunkC based on the features. And it also does not know whether the ChunkD is similar to ChunkA or ChunkB since the randomized character in hash value may impact the similarity distance among chunks. The situation is worse when the modification impacts the top k largest Rabin fingerprint values.
To make the resemblance detection more robust, the state-of-the-art work, such as N-transform Shilane et al. [2012a] and Finesse Zhang et al. [2019], attempts to improve the resemblance detection accuracy and performance using the grouping features mechanism. Specifically, it extracts the top k largest Rabin fingerprint values of a chunk. Then a super-feature of this chunk can be calculated by several such features. The differences between N-transform and Finesse are that the Finesse extracts the largest Rabin fingerprint value in each subchunk and groups the Rabin fingerprint values as the features based on these values' ranking.

ChunkA ChunkB ChunkC ChunkD

r1

r2

r1

r2

r1

r2

r1

r2

Features

r3

r4

r5

(r1,r2,r3,r4,r5)

r3

r4

r5

(r1,r2,r3,r4,r5)

r3

r4

r5

r3

r4 r6

(r1,r2,r3,r4,r5) (r1,r2,r3,r4,r6)

Figure 1: The Naive Chunk Feature Extraction Method
As shown in Figure 2, the Finesse divides a chunk into several sub-chunks. Then, it calculates the maximum Rabin fingerprint value in each sub-chunk. After sorting these values and dividing them into several parts, the largest Rabin fingerprint values from each part are grouped as the first feature dimension value of the chunk. The second biggest Rabin fingerprint values from each part are grouped as the second feature dimension value of the chunk, and so forth. In Finesse, any two chunks having a feature value in common are considered highly similar and the first matched chunk is selected as the base for delta encoding Kulkarni et al. [2004], which is known as "FirstFit."
However, the most serious limitation in existing resemblance detection is that it does not work well under different chunk sizes. Various chunk size will result in chunks that are supposed to be similar to be considered dissimilar in high probability. What's worse, several small modifications may also result in the similar problem like naive feature extraction. To simplify the description, we take the Finesse as an example to show the above limitations in resemblance detection. As shown in Figure 2, subckXi is short for the i-th sub-chunk of ChunkX . Each chunk feature has three dimensions. ChunkE is divided into k sub-chunks where k is a predefined fixed value. Each sub-chunk has a maximum Rabin fingerprint value. Then, these values, from r1 to rk, are divided into sets, three values in each group, and sorted according to their values. Finally, the feature of ChunkE is (D1ckE,D2ckE,D3ckE) where D1ckE equals to hash(r3, r4, · · · , rk-1), D2ckE equals to hash(r2, r5, · · · , rk-2), D3ckE equals to hash(r1, r6, · · · , rk).
Moreover, ChunkF , ChunkG are the similar to ChunkE, which modify some head information based on ChunkE. ChunkH deletes some ending part based on ChunkE. Based on Finesse, we can also get the features for each chunk, which is shown in Figure 2. Even though the modification affects the maximum Rabin fingerprint value in the first sub-chunks in ChunkF , the probability that ChunkF 's features are different from the ChunkE features is 33.3%. The reason is that the probability of r1 in ChunkE appearing randomly in any of the three positions is 33.3% based on the principle of permutation and combination Wikipedia contributors [2020]. The situation is worse once two similar chunks have different sizes, such as the ChunkH and ChunkE. The sub-chunk size in ChunkH and ChunkE is different under the same chunk feature dimensions. Most of the features in ChunkG are different from the ChunkE features, which are not treated as similar chunks.

4

arXiv Template

A PREPRINT

1.Calculate the Maximum Rabin

2.Sort values 3. Group values as Features

fingerprint value in each subchunk

Featurecki =(D1cki ,D2cki ,D3cki)

SubckE1 SubckE2 SubckE3

ChunkE

r1

r2 r3

SubckF1 SubckF2 SubckF3

......

ChunkF r1'

r2 r3

......

SubckEK
rk SubckFK
rk

r3<r2<r1 r4<r5<r6 ... ... rk-1,rk-2,rk
r1'<r3<r2 r4<r5<r6 ... ... rk-1<rk-2<rk

D1ckE: hash(r3,r4,...,rk-1) D2ckE: hash(r2,r5,...,rk-2) D3ckE: hash(r1,r6,...,rk)
D1ckF: hash(r1',r4,...,rk-1) D2ckF: hash(r3,r5,...,rk-2) D3ckF: hash(r2,r6,...,rk)

SubckG1 SubckG2 SubckG3

ChunkG r1''

r2'

r3'

......

SubckGK rk

r1''<r3'<r2' r4<r5<r6 ... ...
rk-1<rk-2<rk

D1ckG: hash(r1'',r4,...,rk-1) D2ckG: hash(r3',r5,...,rk-2) D3ckG: hash(r2',r6,...,rk)

SubckH1 SubckH2 SubckH3

SubckHK

ChunkH g1

g2 g3

...... gk

g1<g3<g2 g4<g5<g6 ... ...
gk<gk-1<gk-2

D1ckH: hash(g1,g4,...,gk) D2ckH: hash(g3,g5,...,gk-1) D3ckH: hash(g2,g6,...,gk-2)

Figure 2: The Problems in Finesse under Various Modifications

In summary, the root cause of traditional resemblance detection limitations is that chunk feature only consider the chunk content itself while ignoring chunk-context. It is easily affected by various modification patterns and does not work well under different chunk sizes. However, we observe that two similar chunks may also have similar chunk-context and vice versa. These chunks are always co-occurred in the deduplication system. The chunk-context information can help the resemblance detection accuracy and decrease the modification impacts compared with traditional methods. Thus, we focus on how to combine the chunk-context information with the chunk content to achieve high-accuracy and high-performance resemblance detection in the cloud storage for deduplication.

4 CARD Design
In this section, we first describe the overview of our design. To consider the chunk content's internal structure in the chunk representation, we propose an n-sub-chunk shingles-based initial feature extraction scheme. A BP-neural network-based chunk-context aware model is responsible for embedding the chunk-context information into the chunk initial feature to improve resemblance detection accuracy in the deduplication scenario.

1. Splitting the data into chunks
2. N-sub-chunk shingles based initial features extraction
3. Embed the chunk-context into the chunk feature

Training Process
bits stream

... cki-k ...

cki

...

cki+k

...

<0x 0x7 x76 ... fb>

...

...

...

...

Predicting Process

bits stream

ckj

...

...

<0x 0x6 x78 ... fb>

= ckj

ckx

+

The most similar

chunk

delta

1. Splitting the data into chunks
2. N-sub-chunk shingles based initial features extraction
3. Predict the chunk-context aware feature
4. Delta encoding based on contextaware features

Figure 3: The CARD Workflow
4.1 Overview Our primary goal is to embed the chunk-context information into the chunk representation and support the resemblance detection with the unequal chunk size. We introduce a Back-Propagation neural (BP-neural) network to reach this
5

arXiv Template

A PREPRINT

goal, embedding the chunk-context information into the target chunk features to achieve this goal. Compared with traditional feature extraction in resemblance detection, the chunk-context makes the chunk feature more robust under various chunk-length and modification pattern conditions. As shown in Figure 3, we introduce our design workflow in two processes: training and predicting.
In the training process, our method first splits each file into chunks based on a traditional content defined chunking (CDC) algorithmMuthitacharoen et al. [2001], Eshghi and Tang [2005], Tian et al. [2017], Xia et al. [2016]. These chunks are distinct size. Next, we divide each chunk into N sub-chunks of the same size. Then, the n-sub-chunk shingles based initial feature extraction scheme, which is detailed in Subsection4.2, generates an initial feature for the chunk from these N sub-chunks. Each feature has M dimensions. And the initial feature is only related to the chunk's content. We propose a BP-neural network-based chunk-context aware model to embed the chunk-context into the initial feature further. The model is generated based on the training samples, including the pair of the target chunk and its surrounding k chunk's initial features. Finally, the model continually adjusts each neural connection's weights based on these training samples.
For example, as shown in the left part of Figure 3, the data bits stream in deduplication is split into variance chunks by traditional CDC algorithm, such as FastCDC Xia et al. [2016]. We denote the i-th chunk as the target chunk, cki. Then, N-sub-chunk shingles based initial feature extraction scheme calculates each cki's initial features, which is denoted as the vectori. Then, the surrounding k chunks of cki can be treated as the chunk-context of cki. Similarly, we also get these surrounding chunks' initial features, from vectori-k to the vectori+k. Next, the (vectors of the ckis surrounding chunks, vectori) pairs are treated as the training data. The surrounding chunk vectors are the chunk-context aware model's input, which is based on a kind of BP-neural network, while the target chunk vectors are treated as labels. Next, we can get the model that transfers the vectori into the chunk-contextaware feature, vectori after the model training cycle iteration. The detail of BP-neural network based chunk-context aware model is described in Subsection 4.3.
The predicting process is shown in the right part of Figure 3. We also divide the coming bits stream into chunks. And each ckj's initial feature is denoted as vectorj. Then, the chunk-context aware feature vectorj is achieved from the vectorj based on the trained model. Specifically, the vectorj equals to the 2K · UD-×1 M where the matrix U is a weight matrix in the trained model. Furthermore, the cloud could find the most similar chunk based on these chunk-context aware features. Next, utilizing the delta encoding technology Kulkarni et al. [2004] removes the redundancy between these two similar chunks. Only the diff part is stored. It is noted that the model can also be distributed into the client-side to transfer each chunk feature into the chunk-context aware features. And the cloud service provider can also generate variance models for different data scenarios.
Our design's essence is to embed the chunk-context information combined with the chunk's internal structure into its feature. Precisely, if the target chunk is similar to another chunk, the surrounding chunks of the target one may also resemble chunks with a high probability. Conversely, if the surrounding chunks of a target chunk are similar to other chunks, the target chunk may also be similar. Moreover, there is a similar internal structure among resemble chunks. we leverage the chunk context for resemblance detection which is ignored by the existing schemes.
Besides, there are other benefits from embedding chunk-context into resemblance detection. First, the chunk feature is not easily affected by the modification patterns. These kinds of features are not only related to chunk content but also the chunk-context information. Second, this design supports the parallel computation for the model training, which is detailed in Subsection 4.3. Third, the design can effectively avoid the dimension disaster. D is a fixed pre-defined dimension parameter that could be adjusted based on real applications in our design.
4.2 N-Sub-chunk Shingles based Initial Feature Extraction Scheme
In this subsection, we describe the detail of the n-sub-chunk shingles based initial feature extraction scheme. This scheme aims to generate the initial feature by only considering the chunk content itself for the further embedding chunk-context feature. If two chunks are similar, there always has a common content sequence in each other chunks. Thus, our scheme divides the chunk into many sub-chunks and extracts each sub-chunk's local sensitive hash. The local sensitive hash sequence in a chunk reflects the content structure, denoted as the initial chunk feature. It can avoid the content structure information loss caused by the traditional feature extraction scheme.
The detail of n-sub-chunk shingles based initial feature extraction scheme is shown in Algorithm1. There are four input parameters, where cki denotes the i-th chunk content, K denotes the number of the sub-words in the cki, M denotes the dimension of the vector dimension of the cki, and the Hash_Sets denotes M hash functions. Firstly, our scheme divides the chunk into K sub-chunks with a fixed size. Secondly, each sub-chunk's local sensitive (LSH) hash value is recorded in a hash array. These LSH hash values are treated as the basic unit in constructing the initial vector. Thirdly,
6

arXiv Template

A PREPRINT

S is a set of unique shingles, each of which is composed of contiguous sub-sequences of sub-chunk in cki. These unique shingles embed the content structure by maintaining the sub-sequences of sub-chunk. Then, each unique shingle is converted into a sub_vector with M hash functions in Hash_Sets. Finally, the initial vector of cki, vectori, is the average of these sub_vectors.

Algorithm 1 N-sub-chunk shingles Based Initial Feature Extraction Scheme

Input: cki:chunk content; K:number of the sub-words; M :dimension of the vector; Hash_Sets: M hash functions Output: M-dimension rough features, vectori

1: Split the cki into K sub-chunks

2: for j=0 to K do

3: hash[j]LSH(j-th sub-chunk)

4: end for

5: for r=1 to N do

6: for j=0 to K do

7:

Set S  a string concatenated by the hash[j] and its surrounding r hash values in order

8: end for

9: end for

10: s_size = the element number of S

11: for g=0 to s_size do

12: eg = g-th element in S

13: sub_vector[g]  (hf0(eg),hf1(eg),· · · , hfi(eg)), where hfi is the i-th hash function in Hash_Sets

14: end for

15:

vectori=avg(

sub_vector[0] sub_vector[0]

,· · · ,

sub_vector[s_size] sub_vector[s_size]

)

16: return vectori

To represent the chunk content structure in the initial feature, we introduce the sub-chunk and obtain numeric subsequences of these sub-chunks. The cki's vector representation is associated with these sub-sequences. The essence of the N-sub-chunk shingles-based initial feature extraction scheme is that the contiguous sub-sequence set keeps the same sequence with the chunk content structure. To achieve each dimension value of an initial feature, we randomly transform each shingle into various hash values and put them together in order. It effectively embeds the chunk content information into a feature. To further improve the resemblance detection accuracy, we normalize each dimension value. These initial features are the basement for further embedding the chunk-context information in the chunk-based context-aware model, which is detailed in Subsection4.3.

4.3 BP-Neural Network-based Chunk-Context Aware Model
Although the N-sub-chunk shingles based initial feature extraction scheme keeps the chunk content structure into the feature, these features lack chunk-context information. Based on the analysis in Section3, two similar chunks may also have similar chunk-context and vice versa. The chunk-context information can improve the quality of the chunk feature. Thus, we further propose a BP-neural network based chunk-context aware model. It embeds the chunk-context information with the chunk content itself to support the resemblance detection in different sizes and make our scheme more robust under various modification patterns.
As shown in Figure 4, we introduce the BP-Neural Network technology to embed the chunk-context information into the initial feature, which is denoted as the BP-Neural Network-based chunk-context aware model. The initial features are treated as the training data for the BP-Neural network. Then, it calculates a mapping relationship from the surrounding co-occurring chunks, according to the target chunk cki of the first k and the last k chunks, based on the training dataset. Specifically, the surrounding chunk initial features of cki are the inputs of the model. The cki's initial feature is treated as the label of the model output. The hidden layer in the BP-Neural Network-based chunk-context aware model is a vector,hi, with a D dimension.

2

1

hi = WM×D · ( Kvectori) · 2K

(1)

i=1

1

vertori = 2K · UD×M · hi

(2)

The model is adjusted by the forward feedback and backward feedback mechanism in BP-Neural Network in the training process. The loss function during the training process is the hierarchical softmax function Peng et al. [2017].

7

arXiv Template

...

...

N-Sub-chunk Shingles based Initial Feature Extraction Scheme

...

...

Input Layer

A PREPRINT

Hidden Layer

Output Layer
Figure 4: BP-Neural Network-based Chunk-Context Aware Model

Since there are large volume data in cloud deduplication scenario under distinct background, cloud service providers can also generate specific training models for different background datasets. Furthermore, the optimization mapping relationship from the surrounding co-occurred chunks to the target chunk is a chunk-context aware model including two weight matrix, WM×D and UD×M . The hi is a hidden layer vector which is calculated based on the Formula1. Moreover, the hi is a vector of cki embedding the chunk-context and chunk internal structure information. And the mapping relationship between chunk's initial feature and the chunk-context feature is based on Formulation2. In the
predicting process, each new chunk's initial vector, vectorj, maps to the context-aware chunk feature, vectorj, based on the model as Formula3.

vectori = 2K · UD-×1 M · vectori

(3)

It is noted that the model can be trained and predicted in parallel mode. Specifically, most operations in our model are based on matrix multiplication and addition. Moreover, we defined the chunk-context as the surrounding co-occurring 2K chunks. It significantly avoids the calculation dependency in training, and the multiple CPUs implementation can achieve an optimal convergence speed. What's more, since the model training process can be completed offline, it does not impact the user's experience for cloud storage service. The model storage cost can be decreased by model compression and acceleration, which is not the focus of this paper.

5 Experimental Evaluations

5.1 Methodology

Implementation and Experimental Setup We implement the CARD prototype in python. All the experiments are conducted on a clustered platform. The cloud is simulated by three machines. Each machine is equipped with an Intel(R) Core(TM) i7-4790 @3.60Ghz 8 core CPU, 16GB RAM, a 500GB 5400rpm hard disk and is connected to a 100Mbps network. The OS was installed in Ubuntu 18.10 LTS 64-bit System.

DataSets To evaluate the effectiveness of our design, we choose three distinct real-world datasets. The first one is the vmdk file trace, which was collected from initial full backups and subsequent incremental backups of 6 members of a university research group and was reported by Zhou et al. [2015]. The second one is a SQL dump workload including different periods backup and was reported by Beller et al. [2017]. The third one is the Linux Kernel, reported by Tian et al. [2018]. It is noted that these three datasets containing version data for different periods. Distinct version data includes many modification patterns in daily cloud storage usages. Thus, to justify whether the chunk-context information makes the chunk feature robust than other state-of-the-art work, we conduct serials experiments based on these three datasets.

Metrics and Configuration We evaluate the CARD's resemblance detection in two metrics: Delta Compression Ratio

(DCR) and the overall time cost for resemblance detection in deduplication. The DCR metric is reported in Zhang

et

al.

[2019],

which

is

measured

by

total total

size size

bef ore af ter

delta delta

compression compression

.

The

DCR

reflects

the

total

space

saved

by

resemblance detection and delta compression. Besides, the overall time cost for resemblance detection is critical for the

cloud storage provider, closely related to the user experience for cloud storage service. We evaluate the performance of

8

arXiv Template

A PREPRINT

CARD compared with the state-of-the-art resemblance detection work, N-transform and FinesseZhang et al. [2019]. All the systems take the FastCDC Xia et al. [2016] as the chunking algorithm and the classic Xdelta as the delta compression method Kulkarni et al. [2004]. We mainly measure the performance of multiple workloads in different metrics.

5.2 Numerical Results and Discussions
To evaluate the effectiveness by embedding the chunk-context into features for resemblance detection, we first conduct the experiments on N-transform, Finesse, and CARD with various average chunk size under different workloads. It is because the average chunk size is a critical parameter in deduplication. We select the most commonly used average chunk size as the variance from 16KB to 512KB. Then, we collect the DCR and the overall time cost for each experiment under a fixed chunk-context feature dimension. It is noted that the chunk-context feature dimension is related to the maximum storage size of cloud storage. To simulate the real cloud storage, we fixed the chunk-context feature dimension in 50, which means the cloud storage could store 1PB data.
As shown in Figure 5, the Finesse has a lower DCR than the N-transform in this workload. When the average chunk size is 16KB, the DCR in N-transform is 0.36 higher than that of Finesse. As the average chunk size increasing, the DCR in Finesse and N-transform is growing slowly. For example, when the average chunk size equals 128KB, the DCR of N-transform and Finesse is 3.02% and 3.03% higher than that of 16KB, respectively. The feature extraction method easily suffers from the modification discussed in Section3, while the N-transform's feature extraction is more robust than the Finesse.

DCR(total size/compressed size)

12 10
8 6 4 2 0
16

Finesse N-transform CARD

32

64

128

256

512

Average Chunk Size(KB)

Figure 5: The DCR in SQL Dump Workload

Fortunately, our method achieves a better DCR than N-transform and Finesse in the SQL dump workload, as shown in Figure 5. Compared with the N-transform and Finesse, our method improves the DCR by 45.49% and 51.19% when the average chunk size is 16KB, respectively. As the chunk average size is 128KB, the CARD is 62.02% and 75.03% higher than the N-transform and Finesse in DCR. It does not contradict the view that the smaller the chunk size is, the more redundancy can be removed in deduplication. There are lots of redundancy in the SQL dump workload. The same content in a small average chunk size generates many delta files. In contrast, small delta files are generated in a large average chunk size. The previous one costs more storage than the latter. Thus, although it provides qualified chunk-context information in a small average chunk size, there still has a DCR improving space with the average chunk size increasing. As the average chunk size is 512KB, Both CARD and N-transform's DCR decrease because too sizeable average chunk size deteriorates the similarity detection effectiveness. Nevertheless, our method is still 16.05% higher than finesse's DCR.
Besides, we also statistic the overall time cost for Finesse, N-transform, and CARD in the SQL dump workload, as shown in Figure 6. When the average chunk size is 16KB, the CARD is 6× and 11× faster than the Finesse and N-transform, respectively. As the average chunk size increasing, the overall time of N-transform and CARD fluctuate slightly in a fixed range. N-transform's primary time consumption is the Rabin fingerprint value and the linear transformation calculations, which are independent of the average chunk size. Moreover, the chunk-context feature is generated through the N-sub-chunk shingles based initial feature extraction and the chunk-context embedding process. The previous initial feature extraction is related to the feature dimension, but not to the average chunk size. However, Finesse's feature sorting costs too much when the average chunk size is small, which dramatically raises the sorting number as the chunk number increases. Nevertheless, the CARD is still 3.8× than Finesse when the average chunk size is 512KB.

9

arXiv Template

A PREPRINT

25000 20000

Finesse N-transform CARD

Overall Time Costs(Sec)

15000

10000

5000

0 16

32

64

128

256

512

Average Chunk Size(KB)

Figure 6: The Overall Time Cost in SQL Dump Workload

Similarly, we conduct the same configuration in VMDK and Linux Kernal workloads. As shown in Figure 7, the DCR of Finesse, N-transform, and CARD is decreasing slowly with the increasing average chunk size. Specifically, the DCR of CARD, N-transform SF and Finesse, is reduced by 7.37%, 13.31%, and 8.54% from 16KB to 512KB average chunk size, respectively. The reason is that the data modification pattern in VMDK tends to be random compared with the SQL dump workload. The larger average chunk deteriorates the effectiveness of the features in all methods. Thus, it does not occur the DCR increasing tendency when the average chunk size increases. Nevertheless, the DCR of CARD is higher than the Finesse and N-transform with variance average chunk size.

DCR(total size/compressed size)

5 4.5
4 3.5
3 2.5
2 1.5
1 0.5
0 16

Finesse N-transform CARD

32

64

128

256

512

Average Chunk Size(KB)

Figure 7: The DCR in VMDK Workload

In Linux Kernel workload, we also conduct the experiments by changing the average chunk size from 16KB to 512 KB, although a large number of the file in this workload is less than 4KB except for some resource files. Our goal is to test the effectiveness of various resemblance methods under this extreme situation. As shown in Figure 7, our method has outperformed the Finesse. For example, the DCR of our method is 2.25× higher than that of Finesse when the average chunk size is 512KB. However, the CARD and N-transform have similar DCR under these kinds of extreme conditions. It is because most of the files in Linux Kernel become a single chunk without the chunk-context information under these extreme conditions. Thus, CARD's chunk-context feature will degenerate into a feature only related to the chunk content internal structures.
Besides, we also compare the overall time cost under VMDK and Linux Kernel workloads. In the VMDK dataset, when the average chunk size is 256KB, CARD is faster than Finesse and N-transform SF by 5.6× and 17.88×, respectively. Nevertheless, when the average chunk size is 16KB, Finesse is slower than N-transform by 3.2×. Moreover, as the average chunk size increases, Finesse is finally stabilizes and is 2.2× faster than N-transform. It is because each file generates more chunks when the average chunk size is less than 32KB. Therefore, the sorting process in Finesse dramatically increases. Thus, the sorting cost is much more than the time cost for extracting features. Once the average chunk size is more than 32KB, the cost of extracting feature becomes the dominant time consumption.
As shown in Figure10, the overall time cost CARD is far less than that of Finesse and N-transform when the average chunk size is changed from 16KB to 512 KB. Specifically, when the average chunk size is 16KB, the Finesse time consumption is 32 times longer than our method's overall time cost. Although the CARD has a similar DCR with

10

arXiv Template

A PREPRINT

DCR(total size/compressed size)

5 4.5
4 3.5
3 2.5
2 1.5
1 0.5
0 16

Finesse N-transform CARD

32

64

128

256

512

Average Chunk Size

Figure 8: The DCR in Linux Kernel Workload

200000 180000 160000 140000 120000 100000
80000 60000 40000 20000
0 16

Finesse N-transform CARD

32

64

128

256

512

Average Chunk Size(KB)

Overall Time Costs(Sec)

Figure 9: The Overall Time Cost in VMDK Workload

N-transform under these extreme conditions, most of the feature extraction in CARD are hash calculation without the Rabin fingerprint value's linear transform in N-transform or the Rabin fingerprint value's sorting in Finesse. Thus, the overall time cost under small average chunks size in Finesse and N-transform is much higher than the time consumption under larger average chunks size.

Overall Time Costs(Sec)

120000 100000
80000 60000 40000 20000
0 16

Finesse N-transform CARD

32

64

128

256

512

Average Chunk Size(KB)

Figure 10: The Overall Time Cost in Linux Kernel Workload

Finally, we evaluate the CARD performance on distinct workloads with various feature dimension settings in fixed average chunk size. As we discussed before, the feature dimension in 50 denotes that the cloud storage could store 1PB data. To simulate the real cloud storage, we changed the feature dimension from 40 to 80. Once the feature dimension is 80, it means the maximum volume of cloud storage is 210ZB. In Table1, the overall time cost is gradually increasing as the feature dimension expanding under distinct workloads. Moreover, the CARD DCR is slightly increasing when the dimension is changed from 40 to 80. In Linux Kernel and VMDK workloads, the DCR of our method is almost

11

arXiv Template

A PREPRINT

Table 1: Overall Cost TImes and DCR of CARD with Distinct Deature Dimensions

Dataset SQL dump Linux Kernel VMDK

Dimension
40 50 60 70 80
40 50 60 70 80
40 50 60 70 80

Time(s)
1514.86 1527.14 1693.37 1671.33 1715.07
3073.52 3128.84 3575.78 3970.23 4344.69
3681.18 3719.48 3726.99 3759.93 3824.97

DCR
7.73 7.74(+0.13%) 7.79(+0.78%) 7.77(+0.52%) 8.01(+3.62%)
3.29 3.28(-0.30%) 3.29(0.00%) 3.29(0.00%) 3.32(+0.91%)
3.69 3.69(0.00%) 3.69(0.00%) 3.69(0.00%) 3.69(0.00%)

keeps the same value when the dimension is changed from 40 to 80. It is because that some modification patterns in VMDK and Linux Kernel are tend to be random which is no conductive to the chunk-context embedding, which is a problem we will further solve in the future. Nevertheless, our method is still outperforms than the state-of-the-art resemblance work, such as N-transform and Finesse, in distinct workloads.
6 Conclusions
The paper analyzes the state-of-the-art resemblance detection technology in deduplication and presents its limitations in ignoring the chunk-context information. Based on our observation, if the chunk feature in resemblance detection technology is only related to the chunk content itself, it easily suffers from the various modifications. Moreover, if the two chunks surrounding chunks are resemblance, these two chunks might be similar with high probability and vice versa. Inspired by the phenomenon, we propose a chunk-context aware resemblance detection, called CARD, to decrease various modification impacts. In CARD, the N-sub-chunk shingles based initial feature extraction is proposed to extract a preliminary feature by considering the chunk content internal structure. Furthermore, a BP-Neural network-based chunk-context aware model is also proposed to embed the chunk-context information based on the preliminary features. Finally, based on the conducted experimental results, CARD outperforms the state-of-the-art resemblance detection solutions in deduplication, such as N-transform and Finesse. It can remove up to 75.03% more redundancy data and accelerate the resemblance detection by 5.6× 17.8× compared with the state-of-the-art resemblance detection work, N-transform and Finesse.
References
Pratima Sharma, Rajni Jindal, and Dutta Borah Malaya. Blockchain technology for cloud storage: A systematic literature review. ACM Comput. Surv., 53(4):89:1­89:32, 2020. doi:10.1145/3403954. URL https://doi.org/10. 1145/3403954.
Surajit Chaudhuri, Anish Das Sarma, Venkatesh Ganti, and Raghav Kaushik. Leveraging aggregate constraints for deduplication. In Chee Yong Chan, Beng Chin Ooi, and Aoying Zhou, editors, Proceedings of the ACM SIGMOD International Conference on Management of Data, Beijing, China, June 12-14, 2007, pages 437­448. ACM, 2007. doi:10.1145/1247480.1247530. URL https://doi.org/10.1145/1247480.1247530.
Dutch T. Meyer and William J. Bolosky. A study of practical deduplication. In Gregory R. Ganger and John Wilkes, editors, 9th USENIX Conference on File and Storage Technologies, San Jose, CA, USA, February 15-17, 2011, pages 1­13. USENIX, 2011. URL http://www.usenix.org/events/fast11/tech/techAbstracts.html#Meyer.
Wenlong Tian, Ruixuan Li, Weijun Xiao, and Zhiyong Xu. Pts-dep: A high-performance two-party secure deduplication for cloud storage. In 20th IEEE International Conference on High Performance Computing and Communications; 16th IEEE International Conference on Smart City; 4th IEEE International Conference on Data Science and Systems, HPCC/SmartCity/DSS 2018, Exeter, United Kingdom, June 28-30, 2018, pages 700­707. IEEE,
12

arXiv Template

A PREPRINT

2018. doi:10.1109/HPCC/SmartCity/DSS.2018.00122. URL https://doi.org/10.1109/HPCC/SmartCity/ DSS.2018.00122.
Athicha Muthitacharoen, Benjie Chen, and David Mazières. A low-bandwidth network file system. In Keith Marzullo and Mahadev Satyanarayanan, editors, Proceedings of the 18th ACM Symposium on Operating System Principles, SOSP 2001, Chateau Lake Louise, Banff, Alberta, Canada, October 21-24, 2001, pages 174­187. ACM, 2001. doi:10.1145/502034.502052. URL https://doi.org/10.1145/502034.502052.
Kave Eshghi and Hsiu Khuern Tang. A framework for analyzing and improving content-based chunking algorithms. Hewlett-Packard Labs Technical Report TR, 30(2005), 2005.
Wenlong Tian, Ruixuan Li, Zhiyong Xu, and Weijun Xiao. Does the content defined chunking really solve the local boundary shift problem? In 36th IEEE International Performance Computing and Communications Conference, IPCCC 2017, San Diego, CA, USA, December 10-12, 2017, pages 1­8. IEEE Computer Society, 2017. doi:10.1109/PCCC.2017.8280445. URL https://doi.org/10.1109/PCCC.2017.8280445.
Wen Xia, Yukun Zhou, Hong Jiang, Dan Feng, Yu Hua, Yuchong Hu, Qing Liu, and Yucheng Zhang. Fastcdc: a fast and efficient content-defined chunking approach for data deduplication. In Ajay Gulati and Hakim Weatherspoon, editors, 2016 USENIX Annual Technical Conference, USENIX ATC 2016, Denver, CO, USA, June 22-24, 2016, pages 101­114. USENIX Association, 2016. URL https://www.usenix.org/conference/atc16/technical-sessions/ presentation/xia.
Lior Aronovich, Ron Asher, Eitan Bachmat, Haim Bitner, Michael Hirsch, and Shmuel T. Klein. The design of a similarity based deduplication system. In Miriam Allalouf, Michael Factor, and Dror G. Feitelson, editors, Proceedings of of SYSTOR 2009: The Israeli Experimental Systems Conference 2009, Haifa, Israel, May 4-6, 2009, ACM International Conference Proceeding Series, page 6. ACM, 2009. doi:10.1145/1534530.1534539. URL https://doi.org/10.1145/1534530.1534539.
Fred Douglis and Arun Iyengar. Application-specific delta-encoding via resemblance detection. In Proceedings of the General Track: 2003 USENIX Annual Technical Conference, June 9-14, 2003, San Antonio, Texas, USA, pages 113­126. USENIX, 2003. URL http://www.usenix.org/events/usenix03/tech/douglis.html.
George Forman, Kave Eshghi, and Stephane Chiocchetti. Finding similar files in large document repositories. In Robert Grossman, Roberto J. Bayardo, and Kristin P. Bennett, editors, Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Chicago, Illinois, USA, August 21-24, 2005, pages 394­400. ACM, 2005. doi:10.1145/1081870.1081916. URL https://doi.org/10.1145/1081870.1081916.
Himabindu Pucha, David G. Andersen, and Michael Kaminsky. Exploiting similarity for multi-source downloads using file handprints. In Hari Balakrishnan and Peter Druschel, editors, 4th Symposium on Networked Systems Design and Implementation (NSDI 2007), April 11-13, 2007, Cambridge, Massachusetts, USA, Proceedings. USENIX, 2007. URL http://www.usenix.org/events/nsdi07/tech/pucha.html.
Lianghong Xu, Andrew Pavlo, Sudipta Sengupta, and Gregory R. Ganger. Online deduplication for databases. In Semih Salihoglu, Wenchao Zhou, Rada Chirkova, Jun Yang, and Dan Suciu, editors, Proceedings of the 2017 ACM International Conference on Management of Data, SIGMOD Conference 2017, Chicago, IL, USA, May 14-19, 2017, pages 1355­1368. ACM, 2017. doi:10.1145/3035918.3035938. URL https://doi.org/10.1145/3035918. 3035938.
Purushottam Kulkarni, Fred Douglis, Jason D. LaVoie, and John M. Tracey. Redundancy elimination within large collections of files. In Proceedings of the General Track: 2004 USENIX Annual Technical Conference, June 27 - July 2, 2004, Boston Marriott Copley Place, Boston, MA, USA, pages 59­72. USENIX, 2004. URL http: //www.usenix.org/publications/library/proceedings/usenix04/tech/general/kulkarni.html.
Philip Shilane, Grant Wallace, Mark Huang, and Windsor Hsu. Delta compressed and deduplicated storage using stream-informed locality. In Raju Rangaswami, editor, 4th USENIX Workshop on Hot Topics in Storage and File Systems, HotStorage'12, Boston, MA, USA, June 13-14, 2012. USENIX Association, 2012a. URL https: //www.usenix.org/conference/hotstorage12/workshop-program/presentation/shilane.
M.O. Rabin. Fingerprinting by Random Polynomials. Center for Research in Computing Technology: Center for Research in Computing Technology. Center for Research in Computing Techn., Aiken Computation Laboratory, Univ., 1981. URL https://books.google.com.sg/books?id=Emu_tgAACAAJ.
Yucheng Zhang, Wen Xia, Dan Feng, Hong Jiang, Yu Hua, and Qiang Wang. Finesse: Fine-grained feature locality based fast resemblance detection for post-deduplication delta compression. In Arif Merchant and Hakim Weatherspoon, editors, 17th USENIX Conference on File and Storage Technologies, FAST 2019, Boston, MA, February 25-28, 2019, pages 121­128. USENIX Association, 2019. URL https://www.usenix.org/conference/fast19/ presentation/zhang.
13

arXiv Template

A PREPRINT

Austin T. Clements, Irfan Ahmad, Murali Vilayannur, and Jinyuan Li. Decentralized deduplication in SAN cluster file systems. In Geoffrey M. Voelker and Alec Wolman, editors, 2009 USENIX Annual Technical Conference, San Diego, CA, USA, June 14-19, 2009. USENIX Association, 2009. URL https://www.usenix.org/conference/ usenix-09/decentralized-deduplication-san-cluster-file-systems.
Mark Lillibridge, Kave Eshghi, and Deepavali Bhagwat. Improving restore speed for backup systems that use inline chunk-based deduplication. In Keith A. Smith and Yuanyuan Zhou, editors, Proceedings of the 11th USENIX conference on File and Storage Technologies, FAST 2013, San Jose, CA, USA, February 12-15, 2013, pages 183­198. USENIX, 2013. URL https://www.usenix.org/conference/fast13/technical-sessions/ presentation/lillibridge.
Mark Lillibridge, Kave Eshghi, Deepavali Bhagwat, Vinay Deolalikar, Greg Trezis, and Peter Camble. Sparse indexing: Large scale, inline deduplication using sampling and locality. In Margo I. Seltzer and Richard Wheeler, editors, 7th USENIX Conference on File and Storage Technologies, February 24-27, 2009, San Francisco, CA, USA. Proceedings, pages 111­123. USENIX, 2009. URL http://www.usenix.org/events/fast09/tech/full_ papers/lillibridge/lillibridge.pdf.
Maohua Lu, David D. Chambliss, Joseph S. Glider, and Cornel Constantinescu. Insights for data reduction in primary storage: a practical analysis. In The 5th Annual International Systems and Storage Conference, SYSTOR '12, Haifa, Israel, June 4-6, 2012, page 17. ACM, 2012. doi:10.1145/2367589.2367606. URL https://doi.org/10.1145/ 2367589.2367606.
Min Fu, Dan Feng, Yu Hua, Xubin He, Zuoning Chen, Wen Xia, Fangting Huang, and Qing Liu. Accelerating restore and garbage collection in deduplication-based backup systems via exploiting historical information. In Garth Gibson and Nickolai Zeldovich, editors, 2014 USENIX Annual Technical Conference, USENIX ATC '14, Philadelphia, PA, USA, June 19-20, 2014, pages 181­192. USENIX Association, 2014. URL https://www.usenix.org/ conference/atc14/technical-sessions/presentation/fu_min.
Amrita Upadhyay, Pratibha R Balihalli, Shashibhushan Ivaturi, and Shrisha Rao. Deduplication and compression techniques in cloud design. In Systems Conference, 2012.
Myoungwon Oh, Sejin Park, Jungyeon Yoon, Sangjae Kim, Kang Won Lee, Sage Weil, Heon Y. Yeom, and Myoungsoo Jung. Design of global data deduplication for a scale-out distributed storage system. In 2018 IEEE 38th International Conference on Distributed Computing Systems (ICDCS), 2018.
Keren Jin and Ethan L. Miller. The effectiveness of deduplication on virtual machine disk images. In Miriam Allalouf, Michael Factor, and Dror G. Feitelson, editors, Proceedings of of SYSTOR 2009: The Israeli Experimental Systems Conference 2009, Haifa, Israel, May 4-6, 2009, ACM International Conference Proceeding Series, page 7. ACM, 2009. doi:10.1145/1534530.1534540. URL https://doi.org/10.1145/1534530.1534540.
Kiran Srinivasan, Timothy Bisson, Garth R. Goodson, and Kaladhar Voruganti. idedup: latency-aware, inline data deduplication for primary storage. In William J. Bolosky and Jason Flinn, editors, Proceedings of the 10th USENIX conference on File and Storage Technologies, FAST 2012, San Jose, CA, USA, February 14-17, 2012, page 24. USENIX Association, 2012. URL https://www.usenix.org/conference/fast12/ idedup-latency-aware-inline-data-deduplication-primary-storage.
Nannan Zhao, Hadeel Albahar, Subil Abraham, Keren Chen, Vasily Tarasov, Dimitrios Skourtis, Lukas Rupprecht, Ali Anwar, and Ali Raza Butt. Duphunter: Flexible high-performance deduplication for docker registries. In Ada Gavrilovska and Erez Zadok, editors, 2020 USENIX Annual Technical Conference, USENIX ATC 2020, July 15-17, 2020, pages 769­783. USENIX Association, 2020. URL https://www.usenix.org/conference/atc20/ presentation/zhao.
Philip Shilane, Mark Huang, Grant Wallace, and Windsor Hsu. WAN optimized replication of backup datasets using stream-informed delta compression. In William J. Bolosky and Jason Flinn, editors, Proceedings of the 10th USENIX conference on File and Storage Technologies, FAST 2012, San Jose, CA, USA, February 14-17, 2012, page 5. USENIX Association, 2012b. URL https://www.usenix.org/conference/fast12/ wan-optimized-replication-backup-datasets-using-stream-informed-delta-compression.
Benjamin Zhu, Kai Li, and R. Hugo Patterson. Avoiding the disk bottleneck in the data domain deduplication file system. In Mary Baker and Erik Riedel, editors, 6th USENIX Conference on File and Storage Technologies, FAST 2008, February 26-29, 2008, San Jose, CA, USA, pages 269­282. USENIX, 2008. URL http://www.usenix.org/ events/fast08/tech/zhu.html.
Wikipedia contributors. Twelvefold way -- Wikipedia, the free encyclopedia, 2020. URL https://en.wikipedia. org/w/index.php?title=Twelvefold_way&oldid=994636417. [Online; accessed 8-January-2021].
Hao Peng, Jianxin Li, Yangqiu Song, and Yaopeng Liu. Incrementally learning the hierarchical softmax function for neural language models. In Satinder P. Singh and Shaul Markovitch, editors, Proceedings of the Thirty-First AAAI
14

arXiv Template

A PREPRINT

Conference on Artificial Intelligence, February 4-9, 2017, San Francisco, California, USA, pages 3267­3273. AAAI Press, 2017. URL http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14732.
Y. Zhou, D. Feng, W. Xia, M. Fu, F. Huang, Y. Zhang, and C. Li. Secdep: A user-aware efficient fine-grained secure deduplication scheme with multi-level key management. In 2015 31st Symposium on Mass Storage Systems and Technologies (MSST), pages 1­14, 2015. doi:10.1109/MSST.2015.7208297.
Moritz Beller, Georgios Gousios, and Andy Zaidman. Travistorrent: synthesizing travis CI and github for fullstack research on continuous integration. In Jesús M. González-Barahona, Abram Hindle, and Lin Tan, editors, Proceedings of the 14th International Conference on Mining Software Repositories, MSR 2017, Buenos Aires, Argentina, May 20-28, 2017, pages 447­450. IEEE Computer Society, 2017. doi:10.1109/MSR.2017.24. URL https://doi.org/10.1109/MSR.2017.24.

15

