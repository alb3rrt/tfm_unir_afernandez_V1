1
3D WaveUNet: 3D Wavelet Integrated Encoder-Decoder Network for Neuron Segmentation
Qiufu Li, Linlin Shen*

arXiv:2106.00259v1 [eess.IV] 1 Jun 2021

Abstract--3D neuron segmentation is a key step for the neuron

digital reconstruction, which is essential for exploring brain

circuits and understanding brain functions. However, the fine

line-shaped nerve fibers of neuron could spread in a large region,

which brings great computational cost to the segmentation in 3D

neuronal images. Meanwhile, the strong noises and disconnected

nerve fibers in the image bring great challenges to the task. In

this paper, we propose a 3D wavelet and deep learning based

3D neuron segmentation method. The neuronal image is first

partitioned into task. Then, we

neuronal cubes to simplify design 3D WaveUNet, the

the segmentation first 3D wavelet

(a()a()a()a)

(b()b()b()b)

(c()c()c()c)

(d()d()d()d)

integrated encoder-decoder network, to segment the nerve fibers

in the cubes; the wavelets could assist the deep networks in suppressing data noise and connecting the broken fibers. We also produce a Neuronal Cube Dataset (NeuCuDa) using the biggest available annotated neuronal image dataset, BigNeuron, to train 3D WaveUNet. Finally, the nerve fibers segmented in cubes are

Fig. 1. (a) A noisy neuronal image with size of 291 × 3298 × 1881 (z-y-x), containing a complete neuron. (b) The manual reconstruction of the neuron. (c) An automatic reconstruction which incorrectly takes background noises as nerve fibers. (d) Another automatic reconstruction which overlooks the weak nerve fibers.

assembled to generate the complete neuron, which is digitally re-

constructed using an available automatic tracing algorithm. The

experimental results show that our neuron segmentation method could completely extract the target neuron in noisy neuronal images. The integrated 3D wavelets can efficiently improve the performance of 3D neuron segmentation and reconstruction. The code and pre-trained models for this work will be available at https://github.com/LiQiufu/3D-WaveUNet.

from the noisy image first and then automatically reconstruct it. Deep learning, which achieves excellent results in many fields, has also been applied into 3D neuron segmentation [5]­ [9]. As nerve fibers can spread in a very large brain region, a large neuronal image has to be processed for segmenting

Index Terms--Neuron segmentation, noise, line-shape object, the complete neuron, which leads to great computational cost

wavelet, deep network.

for the deep networks based 3D neuron segmentation. In

addition, the recent researches [10]­[12] show that the sam-

I. INTRODUCTION
N EURON reconstruction aims to establish a digital model of the neuron morphology structure by tracing nerve fibers in neuronal image, which is of great significance to explore the brain microstructure and understand brain functions. 3D microscopic optical imaging technology, such as MicroOptical Sectioning Tomography (MOST) [1], has established a foundation for the brain neuron reconstruction. Tens of automatic or semi-automatic tracing algorithms [2]­[4] have been developed to reconstruct the neurons in optical images. However, the low signal-to-noise ratio and disconnected nerve fibers are usually big challenges to these algorithms, as Fig. 1 shows.
A natural way to improve the performance of existing neuron tracing algorithms is to extract the complete neuron

pling operations used in the deep networks result in aliasing effects, which lead to noise propagation and break the basic object structures. Due to the amount of noises available in the neuronal image and the neuron's fine line-shaped structure, 3D neuron segmentation is more sensitive to these aliasing effects. Therefore, the current 3D deep networks based segmentation methods might not work well for the 3D neurons with fine line-shaped structure in the noisy neuronal images.
In this paper, to completely segment the neuron and efficiently improve its reconstruction, we present a 3D wavelet and deep learning based 3D neuron segmentation method. Firstly, to simplify the task, we partition the neuronal image into small cubes. Then, we design a 3D wavelet integrated encoder-decoder network (3D WaveUNet) to segment the nerve fibers in the cubes. As far as we know, 3D WaveUNet is the first 3D wavelet integrated deep network. In 3D Wave-

This work is supported by the National Natural Science Foundation of China (Grant No. 62006156, 91959108, and U1713214) and the Science and Technology Project of Guangdong Province (Grant No. 2018A050501014). Corresponding author: Linlin Shen.
Q. Li and L. Shen are with the Computer Vision Institute, College of Computer Science and Software Engineering, Shen zhen University, Shenzhen 518060, China, AI Research Center for Medical Image Analysis and Diagnosis, Shenzhen University, Shenzhen 518060, China, and Guangdong Key Laboratory of Intelligent Information Processing, Shenzhen University, Shenzhen 518060, China (e-mail: qiufu li 1988@163.com; llshen@szu.edu.cn).

UNet, 3D Discrete Wavelet Transform (3D DWT) and 3D Inverse DWT (3D IDWT) are applied to down-sample and upsample 3D neuronal data. While 3D DWT help the encoders in maintaining the fine structure of neurons and suppressing the noise propagation, 3D IDWT could recover the neuron details in the decoders. In addition, based on the biggest available annotated neuronal image dataset, BigNeuron [13], we produce a Neuronal Cube Dataset (NeuCuDa) to train and

2

evaluate the 3D WaveUNet. Finally, we assemble the nerve fibers segmented in cubes according to the cube locations to get the complete segmented neuron, and reconstruct it using the automatic tracing algorithm, All-Path Pruning 2.0 (APP2) [2]. In summary:
1. We propose a 3D wavelet and deep learning based 3D neuron segmentation method, to segment neurons in large size 3D neuronal images.
2. We rewrite 3D DWT/IDWT as general network layers, and design 3D WaveUNet by applying 3D DWT and IDWT as the sampling operations in 3D encoder-decoder network, which is the first 3D wavelet integrated deep network.
3. We produce a neuronal cube dataset, NeuCuDa, to train and evaluate the 3D WaveUNet. Experimental results show that 3D WaveUNet efficiently improve the performance of 3D neuron segmentation and reconstruction in the noisy images.
II. RELATED WORKS
A. Neuron reconstruction
Neuron reconstruction aims to trace the nerve fibers in the optical neuronal images and build a digital model of the neuron's morphology structure.
The recent brain imaging technologies, including MicroOptical Sectioning Tomography (MOST) [1], fluorescence MOST (fMOST) [14], CLARITY [15], [16], Magnified Analysis of the Proteome (MAP) [17], and Stabilization under Harsh conditions via Intramolecular Epoxide Linkages to prevent Degradation (SHIELD) [18], etc., establish the foundation for neuron reconstruction of animal brain.
Various 3D data visualization software, such as Vaa3D [19], NeuroBlocks [20], ManSegTool [21], TeraVR [22], etc., have been developed to show the complex arborization fibers in neuronal images and assist the experienced technicians in tracing neurons. BigNeuron project [13], initiated by Peng et al., releases hundreds of optical neuronal images and their high precision reconstructions traced by experienced experts. More than 1 × 105 digitally reconstructed neurons are released on NeuroMorpho.Org [23], while the corresponding neuronal images are not available.
It is impossible to manually reconstruct the millions of neurons in the whole brain image, so various automatic neuron tracing algorithms have been designed. The BigNeuron project [13] collects the tens of these algorithms, such as SnakeTracing [24], All-Path-Pruning (APP) [25], APP2 [2], NeuroGPSTree [4], Rivulet [26], Rivulet2 [3], etc., and integrates them into the Vaa3D software [19]. Generally, these algorithms mathematically model neuron morphology and trace nerve fibers based on graph theory. However, they would either classify background noise as nerve fibers or miss lots of nerve fibers, as Fig. 1 shows. In other words, while the tracing algorithms are sensitive to image quality, the existing neuronal imaging technologies are usually associated with low signalto-noise ratio.

B. Neuron segmentation
Segmenting the neuron before reconstruction is an effective approach to improve the performance of neuron tracing algorithms. Inspired by the superior performance in computer vision, pattern recognition, and natural language processing, etc., deep learning has also been introduced into the 3D neuron segmentation [5]­[9], [27]. The first 3D residual deep network for neuron segmentation is proposed in [5]. The authors improve their method by applying multi-scale kernels and 3D U-Net [27]. Because they take as input the original neuronal image, these deep networks require high computational cost. In [6], the authors design 3D U-Net Plus to separate the target neuron from its surrounding nerve fibers. As 3D U-Net Plus is trained and evaluated on the neuronal images with size of 32 × 128 × 128, the small nerve fibers may disappear during the image down-sampling. The above deep network based neuron segmentation methods are not suitable for neurons with long-distance fibers. A deep network based neuron segmentation method with low computational cost [7] is built on ray-shooting model and dual channel bidirectional Long Short-Term Memory (LSTM). The authors extract nerve fibers from the 2D neuronal image slice-by-slice, which does not utilize the relevance between the nerve data in the neighbouring 2D neuronal slices. To utilize the unlabeled 3D neuronal data, self-supervised and weakly supervised 3D neurons segmentation methods are studied [8], [9]. In all of these works, the BigNeuron [13] neuronal images are taken to test their methods.
C. The aliasing effects in deep networks
The recent studies [10]­[12] reveal the aliasing effects in the deep learning, which are introduced by the commonly used sampling operations in the deep networks. The aliasing effects result in noise accumulation and break the basic object structure in deep networks, which are very harmful to the 3D neuron segmentation in noisy images.
To suppress the aliasing among the low-frequency and high-frequency components, Richard [11] designs Anti-aliased CNNs by applying low-pass filters before the down-sampling in common Convolutional Neural Networks (CNNs). Zou et al [12] propose an adaptive content-aware low-pass filtering layer to adapt the varying frequencies in different locations and feature channels. These works only consider the low-frequency component of input data, which cannot extract and utilize the data details represented by the high-frequency components. In [10], the authors integrate discrete wavelet transforms into deep networks to separate the components of data in different frequency intervals. They illustrate the usefulness of wavelet transforms in assisting deep networks in extracting robust features, keeping basic object structure, and recovering data details. However, the authors only illustrate the application of 2D wavelet integrated deep networks for image classification, and do not analyze or design 3D wavelet integrated deep networks. In this paper, we will rewrite 3D DWT/IDWT as general network layers, and design 3D wavelet integrated encoder-decoder networks, to improve the 3D neuron segmentation and reconstruction in the noisy images.

3

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

A

B

C

D

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

Fig. 2. The pipeline of our 3D neuron segmAe:nptaatritoitniomne,thBo:ds.eAg:mPeanrttiatitoionn. ,B:CS:eagsmseemntabtliionng.,CD: A: rsescemonbslitnrugc. tDio:nR. econstruction.

III. THE METHOD PIPELINE
The pipeline of our 3D neuron segmentation method is illustrated in Fig. 2 and summarized as below:
1) Partition. The neurons with long-distance nerve fibers could spread in a large brain region, which result in high computational cost for the 3D neuron segmentation in the images with large size. Therefore, the neuronal image is partitioned into small cubes to simplify the segmentation task. We set the z-y-x-size (depth-heightwidth) of the cubes as 32 × 128 × 128 1, which is a compromise between the connectivity of nerve fibers in the neuronal cubes and the computational complexity of the following segmentation.
2) Segmentation. Using the well-trained 3D WaveUNet, we segment the nerve fibers in the cubes. The 3D WaveUNet is an encoder-decoder network integrated with 3D wavelet, which are trained on the neuronal cube dataset, NeuCuDa. The network architecture and NeuCuDa are described in Sec. IV and Sec. V, respectively.
3) Assembling. According to the cube locations in the neuronal image, we assemble the segmented nerve fibers to complete the 3D neuron segmentation.
4) Reconstruction. Based on segmented neuronal image, we reconstruct the neuron using APP2 [2] algorithm. Considering its low computational complexity, we choose APP2 as the benchmark algorithm in this paper.
IV. 3D WAVEUNET
The aliasing effects could significantly affect the performance of deep network based 3D neuron segmentation, due to the fine line-shaped structure of nerve fibers and interference of noises. To suppress the aliasing effects in the 3D deep networks, we design 3D wavelet integrated encoder-decoder network (3D WaveUNet), the first 3D wavelet integrated deep network, for 3D neuron segmentation.
1The typical voxel resolution is 1µm×0.35µm×0.35µm for brain imaging technologies, such as MOST.

A. 3D DWT/IDWT layers
Wavelets [28] are powerful analysis tools used in the fields of data denoising [29], [30], image compression [31]­ [33], etc., and 2D wavelets have been introduced into deep learning [10], [34]­[40]. We here introduce how to integrate 3D wavelets into deep networks.
We rewrite 3D Discrete Wavelet Transform (DWT) and Inverse DWT (IDWT) as general network layers. For a given 3D neuronal data X  Rd×m×n, and eight filters of a 3D discrete wavelet, i.e., one low-pass filter flll and seven high-pass filters fllh, flhl, flhh, fhll, fhlh, fhhl, fhhh, 3D DWT decomposes the data into one low-frequency component Xlll and seven high-frequency components Xllh, Xlhl, Xlhh, Xhll, Xhlh, Xhhl, Xhhh, where

Xc0c1c2 = ( 2)(fc0c1c2  X), c0, c1, c2  {l, h} (1)

and , ( 2) denote the 3D convolution and naive downsampling, respectively. For a tensor X = {Xi,j,k}  Rd×m×n,

( 2)X = {(( 2)X)i,j,k}  R

d 2

×

m 2

×

n 2

,

(2)

and

(( 2)X)i,j,k = X2i,2j,2k.

(3)

In theory, the size of every component is 1/2 size of X in every dimension, i.e.,

X  R , c0c1c2

d 2

×

m 2

×

n 2

c0, c1, c2  {l, h}.

(4)

Therefore, d, m, n are usually even numbers. Using the dual filters ~fc0c1c2 , c0, c1, c2  {l, h}, 3D IDWT
reconstructs the original data X based on the eight components,

X=

~fc0c1c2  ( 2)Xc0c1c2 ,

(5)

c0 ,c1 ,c2 {l,h}

where ( 2) denotes the naive up-sampling operation. For a 3D tensor X = {Xi,j,k}  Rd×m×n,

( 2)X = {(( 2)X)i,j,k}  R2d×2m×2n,

(6)

and

(( 2)X)i,j,k =

Xi
2

,

j 2

,

k 2

if

i 2

,

j 2

,

k 2

 Z,

0

else.

(7)

Original neuronal image X

Max-pooling

Max-pooling image

Max-pooling indices

3D Tensor

3x3x3 Conv + BN + ReLU

...

Down-sampling Up-sampling

Conv Block Forward process

Conv Block Reverse process

Bottom Block

4
...

DWT IDWT

Xlll

Xllh

Xlhl

Xlhh

Max-unpooling

Branch path

Fig. 4. The encoder-decoder architecture.

Xhll

Xhlh

Xhhl

Xhhh

Fig. 3. The comparison of 3D max-pooling/max-unpooling and 3D DWT/IDWT. Max-pooling would magnify the noise in the neuronal data, while noise in the low-frequency component Xlll is suppressed by 3D DWT, as the three enlarged regions show. When max-unpooling recovers the cube resolution, it normally breaks the nerve fibers. In contrast, IDWT could completely recover the neuronal data using the components of DWT decomposition.

The dual filters of orthogonal 3D wavelet are the same with the original filters,

~fc0c1c2 = fc0c1c2 , c0, c1, c2  {l, h}.

(8)

Generally, the 3D wavelet filters are tensor products of the two filters of 1D wavelet, fl, fh, i.e.,

fc0c1c2 = fc0  fc1  fc2 , c0, c1, c2  {l, h},

(9)

where  represents the tensor product. Take Haar wavelet for

example, the low-pass and high-pass filters of 1D Haar wavelet

are

fHl

=

1 (1, 1)T , 2

fHh

=

1 (1, -1)T . 2

(10)

Then, via Eq. (9), we get the filters of the corresponding 3D

Haar wavelet:

fHlll

=

1  22

11 11

;

11 11

,

(11)

fHllh

=

1  22

1 -1 1 -1

;

1 -1 1 -1

,

(12)

fHlhl

=

1  22

11 -1 -1

;

11 -1 -1

,

(13)

fHlhh

=

1  22

1 -1 -1 1

;

1 -1 -1 1

,

(14)

fHhll

=

1  22

11 11

;

-1 -1 -1 -1

,

(15)

fHhlh

=

1  22

1 -1 1 -1

;

-1 1 -1 1

,

(16)

fHhhl

=

1  22

11 -1 -1

;

-1 -1 11

,

(17)

fHhhh

=

1  22

1 -1 -1 1

;

-1 1 1 -1

.

(18)

Eqs. (1) and (5) preseBnotttomthe forward propagations of 3D DWT and IDWT. It is oneBlroockus to deduce the gradients for the backward propagations from these expressions. Fortunately, the modern deep learning framework PyTorch [41] could automatically deduce the gradients for tensor arithmetics. We have rewritten 3D DWT and IDWT as general network layers in PyTorch, which will be publicly available for other researchers. One can flexibly design end-to-end 3D wavelet integrated deep networks using these layers.
For the noisy neuronal cube X, the random noise mostly show up in its high-frequency components, while the basic nerve fiber structure is presented by the low-frequency one. Therefore, 3D DWT and IDWT could be used to denoise the neuronal cube while maintaining the structure of nerve fiber at the same time. DWT halves the size of the neuronal data, and IDWT recovers it, which are good substitutes for the commonly used sampling operations in the 3D deep networks. They could be used to reduce the aliasing effects in the 3D deep networks, and improve the performance of 3D neuron segmentation and reconstruction. Fig. 3 compares the results of 3D DWT/IDWT and 3D max-pooling/max-unpooling on a neuronal cube.
B. The architecture of 3D WaveUNets
In this section, we design 3D WaveUNet using 3D DWT and IDWT to increase neuron segmentation performance for better neuron reconstruction.
As Fig. 4 shows, the encoder-decoder architecture could be decomposed into several nested dual structures with a bottom block. The dual structure consists of forward process and reverse process connected via a branch path. While the forward process contains a serial of 3D convolutions followed by a down-sampling operation, the reverse process is an upsampling operation followed by a number of 3D convolutions. The reverse process up-samples the feature maps, and exploits the data transmitted from the forward process via the branch path. The bottom block is variant in different encoder-decoder architectures. 3D U-Net [42] takes a convolution block as the bottom block, while 3D U-Net Plus [6] employs a 3D ASPP [43]. We denote the all connected forward processes and reverse processes, together with the bottom block, as main path, and denote the data flowing through it as mainstream.
Fig. 5 shows the 3D versions of dual structures used in previous encoder-decoder networks [6], [42]­[45]. DS-

5

...

Mainstream

...

...

Mainstream

...

...

Mainstream

...

Pooling indices

3x3x3 Conv + BN + ReLU 3x3x3 Conv + BN + ReLU

Pooling indices Max-pooling
Pooling indices Max-pooling

Max-unpooling Max-unpooling

3(xa3)x3DCoSn-vP+UBN, +3RDeLvUersion of duMaaxl-psotorluincgture used in MSaexg-uNnpeootlin[g44]

...

Mainstream

...

...

Mainstream

...

...

Tensor (cMopayinasntdrecaomncatente)

...

3x3x3 Conv + BN + ReLU 3x3x3 Conv + BN + ReLU

Tensor (copy and concatente) Tensor (copyManadx-cpoonoclaintegnte)
Max-pooling

DeConv DeConv

3x3x3 Conv + BN + ReLU

Max-pooling

DeConv

(b) DS-PDc used in 3D U-Net Plus [6] and 3D U-Net [45]

... ... ...
3x3x3 Conv + BN + ReLU 3x3x3 Conv + BN + ReLU 3x3x3 Conv + BN + ReLU

Mainstream Mainstream Tensor (cMopayinasntdrecaomncatente) Tensor (copy and concatente) Tensor (copySatnrdidceodn-ccoantevnotleu)tion
Strided-convolution Strided-convolution

... ... ...
Linear interpolation Linear interpolation Linear interpolation

(c) DS-ScIn, 3D version of dual structure used in DeepLabV3+ [43]

Fig. 5. The 3D versions of the commonly used dual structures.

low-frequency component

... ... ...
3x3x3 Conv + BN + ReLU 33xx33xx33 CCoonnvv ++ BBNN ++ RReeLLUU

llooww--ffrreeqquueennccyy ccoommppoonneenntt
low-frequeMncyacionmsptroeneanmt MMaaiinnssttrreeaamm
Tensor (cMopayinasntrdeacmoncatente) TTeennssoorr ((ccooppyy aanndd ccoonnccaatteennttee)) Tensor (copy 3aDndDcWonTcatente)
33DD DDWWTT

... ... ...
DeConv DDeeCCoonnvv

3x3x3 Conv + BN (+aR)eLWU ADS-DDc 3(DDDWWTT, Deconvolute)DeConv

low-frequency component

... ... ...
3x3x3 Conv + BN + ReLU 33xx33xx33 CCoonnvv ++ BBNN ++ RReeLLUU

llooww--ffrreeqquueennccyy ccoommppoonneenntt
low-frequeMncyacionmsptroeneanmt MMaaiinnssttrreeaamm
Tensor (cMopayinasntrdeacmoncatente) TTeennssoorr ((ccooppyy aanndd ccoonnccaatteennttee)) Tensor (copy 3aDndDcWonTcatente)
33DD DDWWTT

... ... ...
Linear interpolation LLiinneeaarr iinntteerrppoollaattiioonn

3x3x3 Conv + BN + ReLU

3D DWT

Linear interpolation

(b) WADS-DIn (DWT, Interpolate)

low-frequency component

... ... ...
3x3x3 Conv + BN + ReLU 33xx33xx33 CCoonnvv ++ BBNN ++ RReeLLUU

llooww--ffrreeqquueennccyy ccoommppoonneenntt
low-frequencMy caoimnpsotnreenat m MMaaiinnssttrreeaamm
Mainstream High-frequency components HHiigghh--ffrreeqquueennccyy ccoommppoonneennttss High-freque3nDcyDcWomTponents
33DD DDWWTT

... ... ...
3D IDWT 33DD IIDDWWTT

3x3x3 Conv + BN + ReLU

3D DWT

(c) WADS-DI (DWT, low-frequency component IDWT)
llooww--ffrreeqquueennccyy ccoommppoonneenntt

...

low-frequencMy caoimnpsotnreenat m

...

MMaaiinnssttrreeaamm

...

High-freqMuaeinncsytrecaommponents

HHiigghh--ffrreeqquueennccyy ccoommppoonneennttss

3D IDWT
... ... ...

3x3x3 Conv + BN + ReLU 33xx33xx33 CCoonnvv ++ BBNN ++ RReeLLUU

High3-fDreDquWenTcy compone3nDtsIDWT

33DD DDWWTT

33DD IIDDWWTT

Denoising block DDeennooiissiinngg bblloocckk

3x3x3 Conv + BN + ReLU

3D DWT

3D IDWT

Denoising block

(d) WADS-DIDn (DWT, IDWT; Denoise)

Fig. 6. The 3D wavelet integrated dual structures (WADS).

PU, Dual Structure with max-Pooling and max-Unpooling, is a 3D version of dual structure used in SegNet [44]. As Fig. 5(a) shows, DS-PU applies max-pooling in its forward process to reduce the feature map resolution and transmits the pooling indices to the reverse process for feature map up-sampling via max-unpooling. However, the max-pooling operation would accumulate the data noise, and the maxunpooling cannot restore the lost details, as Fig. 3 shows. Fig. 5(b) shows DS-PDc, Dual Structure with max-Pooling and Deconvolution, used in 3D U-Net [45] and 3D U-Net Plus [6]. While DS-PDc applies the max-pooling for the downsampling in its forward process, it uses deconvolution for the up-sampling in the reverse process. DS-PDc copies the feature maps from its forward process to reverse process via the branch path, then concatenates it with the up-sampled feature map in reverse process to recover the low-level information in mainstream. DS-ScIn, shown in Fig. 5(c), has the same structure with DS-PDc, while it applies Strided-convolution and linear Interpolation for down-sampling and up-sampling, respectively. In DS-PDc and DS-ScIn, the data tensor injected from their forward processes to reverse processes contains redundant information including noises, which could interfere with 3D neuron segmentation.
We design four 3D WAvelet based Dual Structure (WADS) by replacing the sampling operations with 3D DWT/IDWT, as Fig. 6 shows. In comparison, WADS-DDc (Fig. 6(a)) and WADS-DIn (Fig. 6(b)) are designed by replacing the downsampling operations in DS-PDc and DS-ScIn with 3D DWT, respectively. In their forward processes, while the feature map is decomposed by 3D DWT into eight components, only the low-frequency one is kept to extract the robust highlevel features, and the seven high-frequency components are abandoned. WADS-DI (Fig. 6(c)) and WADS-DIDn (Fig. 6(d)) apply 3D DWT and 3D IDWT for down-sampling and upsampling in their forward and reverse processes, respectively. In the forward process, while feature map is decomposed by 3D DWT, the low-frequency component is kept to extract the robust high-level features, and the seven high-frequency components are transmitted, via the branch path, to reverse process for feature map up-sampling in 3D IDWT. WADSDIDn utilizes a Denoising block to filter the high-frequency components, which is implemented by hard shrinkage with threshold  = 0.25:



x, if x > ,



HardShrink(x) = x, if x < -,

(19)

0, otherwise.

In the denoising block, we filter every coefficient x in the seven high-frequency components

Xc, c  {llh, lhl, lhh, hll, hlh, hhl, hhh},

according to Eq. (19). Using the seven dual structures, we design seven 3D
encoder-decoder networks for neuron segmentation. While the first three networks are variants of 3D U-Net, the last four are 3D WaveUNets designed in this paper. Each of them contains four nested dual structures with one bottom block containing

6

TABLE I DEEP NETWORK CONFIGURATIONS.

data size
32 × 128 × 128 16 × 64 × 64 8 × 32 × 32 4 × 16 × 16 4 × 16 × 16

channel number

encoder decoder

1, 4

4, 4

4, 8

8, 4

8, 16

16, 8

16, 32 32, 16

32, 32

3D U-Netsa 3D WaveUNetsb

DS-x

WADS-y

DS-x

WADS-y

DS-x

WADS-y

DS-x

WADS-y

bottom block

a The three 3D U-Nets are named as 3D U-Net(x), x  {PU, PDc, ScIn}. b The four 3D WaveUNets designed in this paper are named as 3D WaveUNet(y),
y  {DDc, DIn, DI, DIDn}.

two convolutions. Table I illustrates their configurations. In

Table I, the first column shows the input size. Every number

in the table corresponds to a convolutional layer with batch

normalization (BN) and ReLU. While the number in column

"encoder" is the number of input channels of the convolution,

the number in column "decoder" is that of output channels. At

A

the end of networks, a convolution with kernel size of 1 × 1

converts the output of decoder into the predicted segmentation

result.

V. THE NEURONAL CUBE DATASET

B

As we partition the large neuron images into small cubes for neuron segmentation, it is crucial to collect an appropriate dataset to train and evaluate 3D WaveUNet. We here introduce the Neuronal Cube Dataset, NeuCuDa, generated from the BigNeuron images. The procedure to produce NeuCuDa is shown in Fig. 7 and summarized as below:

1) Selection. From the 166 3D neuronal images in BigNeu-

ron, we choose 61 images to generate the neuronal

cubes for the training of 3D deep networks, and 28

images to generate the testing cubes. The 28 testing

images will also be used to evaluate the performance

of reconstruction method proposed in Sec. III.

2) Labeling. For each of the chosen 89 (61 + 28) neuronal

images, we generate a 3D 0-1 label matrix according

to its standard digital reconstruction. Every voxel in the images is assigned to 0 (background) or 1 (nerve fiber). An example neuronal image with its 3D label matrix is shown in Fig. 7.

Fig. 7. A neuronal image and three cubes cut from it. A: labeling, B: cutting. The cubes are corrupted by random noises; some weak nerve fibers in the cubes are disconnected, as marked by the yellow arrows. The 3D label matrices are shown below the cubes.

3) Cutting. Neuronal cubes with size of 32 × 128 × 128 are

randomly cut from the 3D image. Meanwhile, their label cubes are cut from the corresponding 3D label matrix. Fig. 7 shows three example neuronal cubes.
NeuCuDa contains 19251 and 7132 neuronal cubes for the training and testing of 3D WaveUNet, respectively, which will be publicly available for other researchers.

descent (SGD). The training is driven by cross-entropy loss with weights 1, 5 for "background" and "nerve fiber" voxels, to address the imbalances in the cubes. We initially take a learning rate of 0.1, and reduce it following a polynomial decay for each iteration. The batch size, momentum, and weight decay are set as 32, 0.9, and 0.0001, respectively. The

VI. EXPERIMENTS

segmentation performances on the 7132 test cubes are shown

B

in Table II. We train 3D WaveUNets when various wavelets

We first train and evaluate the seven 3D deep networks on are used, and for each network architecture, the results of

NeuCuDa. Then, using the well-trained networks, we segment three wavelets are shown in Table II. In the table, "haar"

and reconstruct the neurons in the 28 test neuronal images stands for the Haar wavelet, while "dbp" and "chp.p~" stand

with the proposed method in Sec. III.

for Daubechies with approximation order p and Cohen wavelet

with orders p, p~, respectively. The filters of these wavelets

A. 3D neuronal cube segmentation

could be found in [28].

On the 19251 training cubes, we train the 3D U-Nets 3D U-Net(PU), applying max-pAooling/max-unpooling, per-

and 3D WaveUNets for 30 epochs, using stochastic gradient forms the worst on neuronal cube segmentation among the

7

TABLE II SEGMENTATION AND RECONSTRUCTION RESULTS.

Networks

Dual Structure

Pool

Encoder Str-conv DWT

Unpool

Decoder Deconv Interpolate

IDWT

wavelet

baseline (APP2)

­

­

­

­

­

­

­

­

­

DS-PU





none

3D U-Net

DS-PDc





none

DS-ScIn

none

WADS-DDc





haar ch2.2

db3

 WADS-DIn



haar

ch2.2

3D WaveUNet 
WADS-DI

db2  haar
ch4.4

db4

WADS-DIDnb



 haar ch4.4





db4

3D Residual Net [5] DS-ScDc 



none

3D U-Net Plus [6] DS-PDc

none

Segmentation (IoU) bga ner fib mean
­­­
99.04 45.55 72.30
99.29 53.65 76.43
99.21 52.96 76.09 99.32 54.80 77.06 99.32 54.58 76.95 90.27 54.76 77.02 99.20 52.59 75.90 99.27 52.89 76.08 99.20 52.67 75.94 99.30 54.31 76.81 99.28 54.46 76.87 99.22 53.06 76.14 99.30 54.76 77.03 99.26 54.78 77.02 99.24 53.22 76.23
­­­
­­­

Reconstruction ESA DSA PDS 9.3942 14.3641 0.3573 2.5444 7.3450 0.2200 2.4046 7.1799 0.1950 2.3614 6.9439 0.1968 2.3272 6.8841 0.1962 2.2495 6.5712 0.1956 2.3391 7.0144 0.1931 2.5247 7.1783 0.2083 2.0288 6.2569 0.1922 2.2197 6.6692 0.1947 2.0588 6.4043 0.1866 2.1726 6.3609 0.1938 2.0676 6.2566 0.1857 2.0336 6.0730 0.1868 2.2067 6.6038 0.1914 1.9973 6.0173 0.1897 3.2892 5.8900 0.3367 33.5020 40.3151 0.5539

Parameters ­
0.17 ×106 0.20 ×106 0.20 ×106 0.20 ×106
0.20 ×106
0.17 ×106
0.17 ×106 11.41 ×106 3.60 ×106

a "bg" and "ner fib" are short for "background" and "nerve fiber", respectively. b "Dn" stands for "Denoising block".

seven deep networks. This result suggests that the pair of sampling operations is not suitable for neuron segmentation, which matches our analysis. Comparing the segmentation results of 3D U-Net(PDc) and 3D WaveUNet(DDc), one can find that DWT performs better than max-pooling, when deconvolution is taken as up-sampling operation; 3D WaveUNet(DDc) achieves the best segmentation performance (mIoU, 77.06%).
Among the seven networks, the segmentation performances of 3D U-Net(ScIn) and 3D WaveUNet(DIn) are only better than that of 3D U-Net(PU), which indicates that the linear interpolation used in their decoder is not a good up-sampling operation for 3D neuron segmentation.
3D WaveUNet(DI) and 3D WaveUNet(DIDn), which apply 3D DWT and IDWT for neuron data down-sampling and up-sampling, perform also well for neuronal cube segmentation. Comparing their results, we conclude that denoising on high-frequency components could slightly improve the performance of neuronal cube segmentation. The segmentation performance of 3D WaveUNet(DIDn) is close to that of 3D WaveUNet(DDc), although the convolutions in decoder of 3D WaveUNet(DDc) employs more parameters. In summary, the 3D wavelets improve the performance of neuron segmentation in 3D noisy images.
B. 3D neuron reconstruction
After segmenting the nerve fibers in the partitioned cubes, we assemble 3D neurons in the 28 test neuronal images, and reconstruct them using APP2, as described in Sec. III. We evaluate the accuracy of automatic reconstructions by comparing them with the manual one, using three metrics (lower is better), i.e., "entire structure average (ESA)", "different structure average (DSA)", and "percentage of different structures (PDS)", proposed in [46] by Peng et al. Table II shows the three mean metric values of the 28 neurons reconstructed using APP2 on the segmentation results of 3D U-Nets and 3D WaveUNets.
The three mean metric values (9.3942, 14.3641, and 0.3573) of APP2 on the 28 original noisy images are taken as

baseline. From Table II, one can find that the reconstruction performance on the segmented images of the seven deep networks is clearly superior to the baseline, which indicates that our method significantly improve the 3D neuron reconstruction. Generally, with the application of 3D DWT and IDWT in the encoder and decoder of the deep networks, the reconstruction performance is getting better and better, and 3D WaveUNet(DIDn) with wavelet "db4" achieves the best reconstruction performance (1.9973, 6.0173, and 0.1897). In summary, our 3D wavelet and deep network based method efficiently improve the automatic reconstruction performance for 3D neurons in the noisy images.
Table II also lists the reconstruction performance of two previous deep network based 3D neuron segmentation methods [5], [6]. For "3D Residual Net", we compute the three mean metric values (3.2892, 5.8900, and 0.3367) of APP2 performance on 17 BigNeuron images published in [5]. For "3D U-Net Plus", we reconstruct the neurons in the 28 test neuronal images using the well-trained 3D U-Net Plus and reconstruction method released in [6], and the reconstruction performance is 33.5020, 40.3151, and 0.5539. While the DSA value (5.8900) of 3D Residual Net is slightly better than that of our method, the network employs significantly more parameters (11.41 M) than our 3D WaveUNets (0.17 M). The performance of 3D U-Net Plus is even inferior to the baseline, because the method can not completely segment the neurons in large size neuronal images.
Fig. 8 and Fig. 9 visually show the reconstructions of two noisy neuronal image. In Fig. 8, the original size of the image is 291 × 3298 × 1881 (z-y-x), while the previous deep network based 3D neuron segmentation methods [5], [6], [27] cannot process the neuronal images with such a large size. Fig. 8(a) shows the manual reconstruction of the neuron, which perfectly reflect the neuron morphology. Fig. 8(b) and Fig. 8(c) show the reconstructions of APP2 with different parameters on the original neuronal image. The following three subfigures show the reconstructions of APP2 on the neurons segmented by the three 3D U-Nets, and the last four subfigures show that

8

EESSEEAASSEEAA==SSEEAA==SSEE99AA==SS..99AA==77..9900==..7777990077..779900..7777990077770077

EESSEEAASSEEAA==SSEEAA44==SSEE00AA==SS44..55AA0044==44..005511==44..44885500114444..88005511..4488551144881188

EESSEEAASSEEAA==SSEEAA==SSEE22AA==SS..55AA22==44..225500==..440055220044..00225500..4400550044000000

EESSEEAASSEEAA==SSEEAA==SSEE11AA==..SS4411AA11==..114422==..118844112211..88114422..1188442211882288

DDSSDDAADDSSAA==SSDDAA11==DDSS66AA==SS11..77AA6611==33..667744==11..33337766441133..33667744..3333774433334433

DDSSDDAADDSSAA==SSDDAA44==DDSS99AA==SS44..66AA9944==88..996688==44..88776699884488..77996688..8877668888778877

DDSSDDAADDSSAA==SSDDAA11==DDSS00AA==SS11..33AA0011==22..003333==11..22883300331122..88003333..2288333322883388

DDSSDDAADDSSAA==SSDDAA==DDSS55AA==..SS6655AA55==..556600==..551166550055..11556600..5511660055110011

((aa))((aa(())aa))((aa(())aa)) PPDDPPSSDDPP==DDSSPPSS==DDPP00==DDSS..4400SS==33..004433==..335544003333..55004433..3355443333553355 ((bb(())bb(())bb(())bb(())bb)) PPDDPPSSDDPP==DDSSPPSS==DDPP00==DDSS..5500SS==00..005533==..004455003300..44005533..0044553300443344 ((cc))((cc(())cc))((cc(())cc)) PPDDPPSSDDPP==DDSSPPSS==DDPP00==DDSS..1100SS==44..001133==..444411003344..44001133..4444113344443344 ((dd(())dd(())dd(())dd(())dd))PPDDPPSSDDPP==DDSSPPSS==DDPP00==..DDSS1100SS00==..001166==..003311006600..33001166..0033116600336633 ((ee))((ee(())ee))((ee(())ee))

MMMMaannMMaauunnMMaauunnllMMaauunnllaauunnllaauullaall

AAPPAAPPAAPP22PPPPAA22PPAAPP22PPPP22PP22

AAPPAAPPAAPP22PPPPAA22PPAAPP22PPPP22PP22

33DD33DDUU33DD--UU33NNDDUU--33eeNNDDtt--UU((NNeePPttUU--ee((NNUUPPtt--((NNee))UUPPtt++ee((UU))PPttAA++(())UUPPPP++AAUU))PPAAPP++))22PPPP++AA22PPAAPP22PPPP22PP22 33DD33DDUU33DD--UU33NNDDUU--33eeNNDDtt--UU((NNeePPttUU--ee((NNDDPPtt--((NNeeccDDPPtt))ee((DDcc++PPtt))((ccDDPPAA++))DDccPP++AA))ccPPAAPP++))22PPPP++AA22PPAAPP22PPPP22PP22

EESSEEAASSEEAA==SSEEAA==SSEE11AA==..SS5511AA11==..115544==..116655114411..66115544..1166554411664466

EESSEEAASSEEAA==SSEEAA==SSEE11AA==..SS3311AA44==..113388==..445533118844..55113388..4455338844558855

EESSEEAASSEEAA==SSEEAA==SSEE11AA==..SS4411AA00==..114477==..003344117700..33114477..0033447700337733

EESSEEAASSEEAA==SSEEAA==SSEE11AA==..SS3311AA99==..113344==..999933114499..99113344..9999334499994499

EESSEEAASSEEAA==SSEEAA==SSEE11AA==..SS3311AA66==..113344==..660033114466..00113344..6600334466004400

DDSSDDAADDSSAA==SSDDAA==DDSS66AA==..SS0066AA77==..660088==..774400668877..44660088..7744008877448844

DDSSDDAADDSSAA==SSDDAA==DDSS55AA==..SS2255AA99==..552244==..999922554499..99552244..9999224499994499

DDSSDDAADDSSAA==SSDDAA==DDSS55AA==..SS55AA99==..5588==..9911558899..115588..9911558899118811

DDSSDDAADDSSAA==SSDDAA==DDSS55AA==..SS55AA66==..5511==..6655551166..5511..66551166551155

DDSSDDAADDSSAA==SSDDAA==DDSS55AA==..SS3355AA22==..553333==..227733553322..77553333..2277333322773377

PPDDPPSSDDPP==DDSSPPSS==DDPP00==..DDSS1100SS11==..001111==..112211001111..22001111..1122111111221122 ((ff))((ff))((ff))((ff))((ff)) PPDDPPSSDDPP==DDSSPPSS==DDPP00==..DDSS1100SS00==..001133==..004411003300..44001133..0044113300443344 ((gg))((gg(())gg))((gg(())gg))PPDDPPSSDDPP==DDSSPPSS==DDPP00==..DDSS1100SS00==..001188==..003311008800..33001188..0033118800338833 ((hh(())hh(())hh(())hh(())hh))PPDDPPSSDDPP==DDSSPPSS==DDPP00==..DDSS1100SS00==..001155==..004411005500..44001155..0044115500445544 ((ii))((ii))((ii))((ii))((ii)) PPDDPPSSDDPP==DDSSPPSS==DDPP00==..DDSS1100SS00==..001144==..006611004400..66001144..0066114400664466 ((jj))((jj))((jj))((jj))((jj))

33DD33DDUU33DD--UU33NNDDUU--33eeNNDDtt--UU((NNeeSSttUU--ee((ccNNSStt--II((NNeennccSSttII))ee((ccnnSStt++II(())nnccSSAA++II))ccnnPP++IIAA))nnPPAAPP++))22PPPP++AA22PPAAPP22PPPP22PP2233DD33DDWW33DDWW33aaDDvvWW33aaeeDDvvWWUUaaeevvWWNNUUaaeeeevvUUNNaatteevv((NNeeUUDDeettee((UUNNDDDDtt((NNeeccDDtt))ee((DDccDDtt++))((ccDDAA++))DDccPP++AA))ccPPAAPP++))22PPPP++AA22PPAAPP22PPPP22PP33DD2233DDWW33DDWW33aaDDvvWW33aaeeDDvvWWUUaaeevvWWNNUUaaeeeevvUUNNaatteevv((NNeeUUDDeettee((UUNNIIDDttnn((NNeeDDII))ttnnee((++IIDDtt))nn((AA++DDII))nnPP++IIAA))nnPPAAPP++))22PPPP++AA22PPAAPP22PPPP22PP3322DD33DDWW33DDWW33aaDDvvWW33aaeeDDvvWWUUaaeevvWWNNUUaaeeeevvUUNNaatteevv((NNeeUUDDeettee((UUNNIIDDtt))((NNeeDDII++tt))ee((IIDDttAA++))((DDIIPP++AA))IIPPAAPP++))22PPPP++AA22PPAAPP22PPPP22PP3322DD33DDWW33DDWW33aaDDvvWW33aaeeDDvvWWUUaaeevvWWNNUUaaeeeevvUUNNaatteevv((NNeeUUDDeettee((UUNNIIDDttDD((NNeeDDIIttnnDDee((II))DDttDDnn((++DDII))nnDDAA++II))DDnnPP++AA))nnPPAAPP++))22PPPP++AA22PPAAPP22PPPP22PP22

Fig. 8. The reconstructions of various methods. (a) The reconstruction manually traced by experts. (b) - (c) Two reconstructions of APP2 on the original neuronal image, with different parameters. (d) - (f) The reconstructions of APP2 on the images segmented by the three 3D U-Nets. (g) - (j) The reconstructions of APP2 on the images segmented by the four 3D WaveUNets.

of the four 3D WaveUNets.
On the original noisy image, the automatic tracing algorithm APP2 either stop tracing weak nerve fibers (Fig. 8(b)) or overreconstruct strong noises as nerve fibers (Fig. 8(c)). From Fig. 8, one can find that the reconstructions of APP2 on the segmented image are superior to that on the original image, which illustrates the efficiency of our 3D neuron reconstruction method. The 3D U-Nets either classify "background" voxels as "nerve fiber" voxels or do not extract complete nerve fibers. Therefore, their segmented neuronal images contain much background noise (Fig. 8(d) and Fig. 8(e)), or the reconstructions are not complete (Fig. 8(d) and Fig. 8(f)), as marked by the yellow arrows. The 3D WaveUNets obtain cleaner segmentation and more complete reconstruction, which justifies the effectiveness of 3D wavelets in suppressing noise and keeping nerve fiber structure. Comprehensively, the 3D WaveUNet(DIDn), which utilizes DWT, IDWT, and denoising block, achieves the best segmentation and reconstruction results.
Fig. 9 visually show another example neuron reconstructed from a noisy image, where lots of tangling nerve fibers are

available. The original size of the neuronal image is 37 × 803 × 928, and a lot of nerve fibers surround the target neuron or pass through the image, which result in gread challenges to the neuron segmentation and reconstruction algorithms. Fig. 9(a) shows the manual reconstruction of the target neuron, while Fig. 9(b) and Fig. 9(c) show the reconstructions of APP2 with different parameters on the original neuronal image. The following three subfigures in the second row show the reconstructions of APP2 on the neuron segmented by 3D UNet(PDc), 3D WaveUNet(DI), and 3D WaveUNet(DIDn).
On the original image, the automatic tracing algorithm, APP2, either overlooks the weak nerve fibers of the target neuron (Fig. 9(b)) or incorrectly include the fibers as part of the target neuron (Fig. 9(c)). On the segmented images, the reconstruction performance of APP2 is significantly improved. As marked by the yellow arrow in Fig. 9(d), a fiber is incorrectly traced by APP2 on the segmented image of 3D U-Net. The APP2 reconstructions (Fig. 9(e) and Fig. 9(f)) on the images segmented by 3D WaveUNets match better with the manual one. These results illustrate that our segmentation method and 3D wavelet could suppress the interference of

9

((((aa((aaaa))))))

((((bb((bbbb))))))

((((cc((cc))cc))))

MMMMMMaaaaaannnnnnuuuuuuaaaaaallllll ((((dd((dddd))))))

DDEEPPDDEEPPDDEEPPDDSSSSDDSSSSDDSSSSAAAASSAAAASSAAAASS==================1111001166770066770066..77......33..33..66..33..33..66333322661100221100221177004433774433774433331144331144331144

AAAAAAPPPPPPPPPPPP222222

((((ee((ee))ee))))

DDEEPPDDEEPPDDEEPPDDSSSSDDSSSSDDSSSSAAAASSAAAASSAAAASS==================1133113311003366110066110066..11......33..55..77..33..55..773355667744664477664400779955009955009944558877448877448877

AAAAAAPPPPPPPPPPPP222222

((((ff((ff))ff))))

DDEEPPDDEEPPDDEEPPDDSSSSDDSSSSDDSSSSAAAASSAAAASSAAAASS==================0022990022990022..99......22....7755..22....7755227700558888008888008888887744887744887799449977999977999977 333333DDDDDDUUUUUU----NN--NNNNeeeetteett((tt((PP((PPPPDDDDDDcccc))cc))))++++++AAAAAAPPPPPPPPPPPP222222

DDEEPPDDEEPPDDEEPPDDSSSSDDSSSSDDSSSSAAAASSAAAASSAAAASS==================0011440011440011..44......11..44..66..11..44..6611445566332255332255337722447733447733444499449944449944 333333DDDDDDWWWWWWaaaaaavvvvvveeeeUUeeUUUUNNNNNNeeeetteett((tt((DD((DDDDIIII))II))))++++++AAAAAAPPPPPPPPPPPP222222

DDEEPPDDEEPPDDEEPPDDSSSSDDSSSSDDSSSSAAAASSAAAASSAAAASS==================0011440011440011..44......11..33..22..11..33..22113366229955669955669900552277002277002233774444334444334444 333333DDDDDDWWWWWWaaaaaavvvvvveeeeUUeeUUUUNNNNNNeeeetteett((tt((DD((DDDDIIIIDDIIDDDDnnnnnn))))))++++++AAAAAAPPPPPPPPPPPP222222

Fig. 9. 3D reconstructed neuron surrounded by nerve fibers. (a) The reconstruction manually traced by experts. (b) - (c) Two reconstructions of APP2 on the

original neuronal image. (d) The reconstruction of APP2 on the images segmented by 3D U-Net(PDc). (e) - (f) The reconstructions of APP2 on the images

seg((((maa((aaaa))e))))nted by 3D WaveUNet(DI) and 3D WaveUNet(DI((D((bb((bbbbn))))))).

((((cc((cc))cc))))

surrounding nerve fibers in the image and efficiently improve the results of 3D neuron reconstruction.

REFERENCES

[1] A. Li, H. Gong, B. Zhang, Q. Wang, C. Yan, J. Wu, Q. Liu, S. Zeng,

and Q. Luo, "Micro-optical sectioning tomography to obtain a high-

VII. CONCLUSIONS

resolution atlas of the mouse brain," Science, vol. 330, no. 6009, pp. 1404­1408, 2010.

[2] H. Xiao and H. Peng, "App2: automatic tracing of 3d neuron morphology

We rewrite 3D DWT/IDWT as general network layers,

based on hierarchical pruning of a gray-weighted image distance-tree,"

and design the first 3D wavelet integrated encoder-decoder

Bioinformatics, vol. 29, no. 11, pp. 1448­1454, 2013.

network, 3D WaveUNet, for 3D neuron segmentation. The 3D [3]

wavelet transforms could suppress the noise propagation and

keep thus

inmeprvroevfie bthereMMMMsMMptaaeaaraannnnurnnuufuucuuaaoaataallrullllmreandcuerionfg

the 3D

ndeeuerponnesetwgmorekntiantfieornenacneAAdAAAA,PPPPPPPPPPPP222222[4]

S. Liu, D. Zhang, Y. Song, H. Peng, and W. Cai, "Automated 3-d DDEEPPDDEEnbpTPPDDEEPPDDSSSSeap.DDSSSSDDSSSSucAA.AASSQAAAASSkAArAASS2ou======4======tn==a====r41122na112211100t2211c,0011r11­0011..k11....a..H33..66..442..33..66i..44c3366444488n994.4488i9944889944gn445444444Z44444488,g222"448822h448822,owI2Eui0,Et1hEJ8.p.tLrreai,cniSssae. cLtbiiro,anAnsc.hoLnei,mraYes.diniLAAcgAAAAiaPP,PPlaPPPPXnPPiPP22md.2222aLcgvoi,nnfiQgDDEEPP,DDdEEPP.DDEEPPDDSSSSevDDSSSSLDDSSSSnAAAAoSSAAAASSucAAAAlSS.eo==============,3====0022c5570022H550022o..55....,..22..11..77..22..n11...77221155n77999955t9999G55996699or1133661133o6611773377.88o777788l777788n1leg1d,,

and S. Zeng, "Neurogps-tree: automatic reconstruction of large-scale

reconstruction in noisy neuronal images.

neuronal populations with dense neurites," Nature methods, vol. 13,

tio((I((ndd((ddndd))))f))ofurtiumrea,gwesecownitlal isntiundgymthuelti3pDle nneeuurroonns,inasn((t((ee((deea))ee))n))ecxetesnedgmtheen3taDwavelet integrated deep networks to more 3D medical image

[5]

no. 1, p. 51, 2016. R. Li, T. Zeng, H.

Pen((g((ff((ff)),ff))))and

S.

Ji,

"Deep

learning

segmentation

of

optical microscopy images improves 3-d neuron reconstruction," IEEE

transactions on medical imaging, vol. 36, no. 7, pp. 1533­1541, 2017.

applications.

[6] Q. Li and L. Shen, "3d neuron reconstruction in tangled neuronal image

10

with deep networks," IEEE transactions on medical imaging, vol. 39, no. 2, pp. 425­435, 2020. [7] Y. Jiang, W. Chen, M. Liu, Y. Wang, and E. Meijering, "3d neuron microscopy image segmentation via the ray-shooting model and a dcblstm network," IEEE Transactions on Medical Imaging, 2020. [8] T. Klinghoffer, P. Morales, Y.-G. Park, N. Evans, K. Chung, and L. J. Brattain, "Self-supervised feature extraction for 3d axon segmentation," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 2020, pp. 978­979. [9] Q. Huang, Y. Chen, S. Liu, C. Xu, T. Cao, Y. Xu, X. Wang, G. Rao, A. Li, S. Zeng et al., "Weakly supervised learning of 3d deep network for neuron reconstruction," Frontiers in Neuroanatomy, vol. 14, 2020. [10] Q. Li, L. Shen, S. Guo, and Z. Lai, "Wavelet integrated cnns for noiserobust image classification," in The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2020. [11] R. Zhang, "Making convolutional networks shift-invariant again," in Proceedings of the International Conference on Machine Learning, 2019, pp. 7324­7334. [12] X. Zou, F. Xiao, Z. Yu, and Y. J. Lee, "Delving deeper into anti-aliasing in convnets," in Proceedings of the British Machine Vision Conference, 2020. [13] H. Peng, M. Hawrylycz, J. Roskams, S. Hill, N. Spruston, E. Meijering, and G. A. Ascoli, "Bigneuron: large-scale 3d neuron reconstruction from optical microscopy images," Neuron, vol. 87, no. 2, pp. 252­256, 2015. [14] H. Gong, S. Zeng, C. Yan, X. Lv, Z. Yang, T. Xu, Z. Feng, W. Ding, X. Qi, A. Li et al., "Continuously tracing brain-wide long-distance axonal projections in mice at a one-micron voxel resolution," Neuroimage, vol. 74, pp. 87­98, 2013. [15] K. Chung and K. Deisseroth, "Clarity for mapping the nervous system," Nature methods, vol. 10, no. 6, p. 508, 2013. [16] K. Chung, J. Wallace, S.-Y. Kim, S. Kalyanasundaram, A. S. Andalman, T. J. Davidson, J. J. Mirzabekov, K. A. Zalocusky, J. Mattis, A. K. Denisin et al., "Structural and molecular interrogation of intact biological systems," Nature, vol. 497, no. 7449, pp. 332­337, 2013. [17] T. Ku, J. Swaney, J.-Y. Park, A. Albanese, E. Murray, J. H. Cho, Y.-G. Park, V. Mangena, J. Chen, and K. Chung, "Multiplexed and scalable super-resolution imaging of three-dimensional protein localization in size-adjustable tissues," Nature biotechnology, vol. 34, no. 9, pp. 973­ 981, 2016. [18] Y.-G. Park, C. H. Sohn, R. Chen, M. McCue, D. H. Yun, G. T. Drummond, T. Ku, N. B. Evans, H. C. Oak, W. Trieu et al., "Protection of tissue physicochemical properties using polyfunctional crosslinkers," Nature biotechnology, vol. 37, no. 1, pp. 73­83, 2019. [19] H. Peng, Z. Ruan, F. Long, J. H. Simpson, and E. W. Myers, "V3d enables real-time 3d visualization and quantitative analysis of largescale biological image data sets," Nature biotechnology, vol. 28, no. 4, p. 348, 2010. [20] A. K. Ai-Awami, J. Beyer, D. Haehn, N. Kasthuri, J. W. Lichtman, H. Pfister, and M. Hadwiger, "Neuroblocks­visual tracking of segmentation and proofreading for large connectomics projects," IEEE transactions on visualization and computer graphics, vol. 22, no. 1, pp. 738­746, 2015. [21] C. Magliaro, A. L. Callara, N. Vanello, and A. Ahluwalia, "A manual segmentation tool for three-dimensional neuron datasets," Frontiers in neuroinformatics, vol. 11, p. 36, 2017. [22] Y. Wang, Q. Li, L. Liu, Z. Zhou, Z. Ruan, L. Kong, Y. Li, Y. Wang, N. Zhong, R. Chai et al., "Teravr empowers precise reconstruction of complete 3-d neuronal morphology in the whole brain," Nature communications, vol. 10, no. 1, pp. 1­9, 2019. [23] G. A. Ascoli, D. E. Donohue, and M. Halavi, "Neuromorpho. org: a central resource for neuronal morphologies," Journal of Neuroscience, vol. 27, no. 35, pp. 9247­9251, 2007. [24] Y. Wang, A. Narayanaswamy, C.-L. Tsai, and B. Roysam, "A broadly applicable 3-d neuron tracing method based on open-curve snake," Neuroinformatics, vol. 9, no. 2-3, pp. 193­217, 2011. [25] H. Peng, F. Long, and G. Myers, "Automatic 3d neuron tracing using all-path pruning," Bioinformatics, vol. 27, no. 13, pp. i239­i247, 2011. [26] S. Liu, D. Zhang, S. Liu, D. Feng, H. Peng, and W. Cai, "Rivulet: 3d neuron morphology tracing with iterative back-tracking," Neuroinformatics, vol. 14, no. 4, pp. 387­401, 2016. [27] H. Wang, D. Zhang, Y. Song, S. Liu, H. Huang, M. Chen, H. Peng, and W. Cai, "Multiscale kernels for enhanced u-shaped network to improve 3d neuron tracing," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2019, pp. 0­0. [28] I. Daubechies, Ten lectures on wavelets. Siam, 1992, vol. 61. [29] D. L. Donoho, "De-noising by soft-thresholding," IEEE transactions on information theory, vol. 41, no. 3, pp. 613­627, 1995.

[30] D. L. Donoho and J. M. Johnstone, "Ideal spatial adaptation by wavelet shrinkage," biometrika, vol. 81, no. 3, pp. 425­455, 1994.
[31] B.-J. Kim and W. A. Pearlman, "An embedded wavelet video coder using three-dimensional set partitioning in hierarchical trees (spiht)," in Proceedings DCC'97. Data Compression Conference. IEEE, 1997, pp. 251­260.
[32] J. Wu, K. Jiang, Y. Fang, L. Jiao, and G. Shi, "Hyperspectral image compression using distributed source coding and 3d speck," in MIPPR 2009: Multispectral Image Acquisition and Processing, vol. 7494. International Society for Optics and Photonics, 2009, p. 74940Z.
[33] C. G. Bahce and U. Bayazit, "Compression of geometry videos by 3dspeck wavelet coder," VISUAL COMPUTER, 2020.
[34] H. Huang, R. He, Z. Sun, and T. Tan, "Wavelet-srnet: A wavelet-based cnn for multi-scale face super resolution," in Proceedings of the IEEE International Conference on Computer Vision, 2017, pp. 1689­1697.
[35] Y. Duan, F. Liu, L. Jiao, P. Zhao, and L. Zhang, "Sar image segmentation based on convolutional-wavelet neural network and markov random field," Pattern Recognition, vol. 64, pp. 255­267, 2017.
[36] P. Liu, H. Zhang, K. Zhang, L. Lin, and W. Zuo, "Multi-level waveletcnn for image restoration," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2018, pp. 773­ 782.
[37] T. Williams and R. Li, "Wavelet pooling for convolutional neural networks," in International Conference on Learning Representations, 2018.
[38] J. Guan, R. Lai, and A. Xiong, "Wavelet deep neural network for stripe noise removal," IEEE Access, vol. 7, pp. 44 544­44 554, 2019.
[39] J. Yoo, Y. Uh, S. Chun, B. Kang, and J.-W. Ha, "Photorealistic style transfer via wavelet transforms," in Proceedings of the IEEE International Conference on Computer Vision, 2019, pp. 9036­9045.
[40] L. Liu, J. Liu, S. Yuan, G. Slabaugh, A. Leonardis, W. Zhou, and Q. Tian, "Wavelet-based dual-branch network for image demoire´ing," arXiv preprint arXiv:2007.07173, 2020.
[41] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer, "Automatic differentiation in pytorch," 2017.
[42] O. Ronneberger, P. Fischer, and T. Brox, "U-net: Convolutional networks for biomedical image segmentation," in International Conference on Medical image computing and computer-assisted intervention. Springer, 2015, pp. 234­241.
[43] L.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, "Encoderdecoder with atrous separable convolution for semantic image segmentation," in Proceedings of the ECCV, 2018, pp. 801­818.
[44] V. Badrinarayanan, A. Kendall, and R. Cipolla, "Segnet: A deep convolutional encoder-decoder architecture for image segmentation," IEEE transactions on pattern analysis and machine intelligence, vol. 39, no. 12, pp. 2481­2495, 2017.
[45] O¨ . C¸ ic¸ek, A. Abdulkadir, S. S. Lienkamp, T. Brox, and O. Ronneberger, "3d u-net: learning dense volumetric segmentation from sparse annotation," in International conference on medical image computing and computer-assisted intervention. Springer, 2016, pp. 424­432.
[46] H. Peng, Z. Ruan, D. Atasoy, and S. Sternson, "Automatic reconstruction of 3d neuron structures using a graph-augmented deformable model," Bioinformatics, vol. 26, no. 12, pp. i38­i46, 2010.

