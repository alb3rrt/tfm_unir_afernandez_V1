arXiv:2106.01036v1 [cs.DS] 2 Jun 2021

Ultra-Sparse Near-Additive Emulators
Michael Elkin1 and Shaked Matar1 1Department of Computer Science, Ben-Gurion University of the Negev, Beer-Sheva, Israel.
Email: elkinm@cs.bgu.ac.il, matars@post.bgu.ac.il

Abstract

Near-additive (aka (1 + , )-) emulators and spanners are a fundamental graph-algorithmic

construct, with numerous applications for computing approximate shortest paths and related

problems in distributed, streaming and dynamic settings.

Known constructions of near-additive emulators enable one to trade between their sparsity

(i.e., number of edges) and the additive stretch . Specifically, for any pair of parameters > 0,

 = 1, 2, . . . , one can have a (1 +

,

)-emulator

with

O(n1+

1 

)

edges,

with



=

log 

log 
. At

their sparsest, these emulators employ c · n edges, for some constant c  2. We tighten this

bound,

and

show

that

in

fact

precisely

n1+

1 

edges

suffice.

In particular, our emulators can be ultra-sparse, i.e., we can have an emulator with n + o(n)

log log n(1+o(1))

edges and  = log log n

.

We also devise a distributed deterministic algorithm in the CONGEST model that builds

these emulators in low polynomial time (i.e., in O(n) time, for an arbitrarily small constant

parameter  > 0).

Finally, we also improve the state-of-the-art distributed deterministic CONGEST-model con-

struction of (1+ , )-spanners devised in the PODC'19 paper [EM19]. Specifically, the spanners

of

[EM19]

have

O(

· n1+

1 

)

edges,

i.e.,

at

their

sparsest

they

employ

O

log log n

log log n

·n edges.

In this paper, we devise an efficient distributed deterministic CONGEST-model algorithm that

builds

such

spanners

with

O(n1+

1 

)

edges

for



=

O

log n log(3) n

. At their sparsest, these spanners

employ only O(n · log log n) edges.

This research was supported by the ISF grant No. (2344/19).
1

1 Introduction

1.1 Background and Our Results

Given an unweighted undirected n-vertex graph G = (V, E), and a pair of parameters   1,   0, a graph G = (V , E ), with V  V is called an (, )-emulator for G, if for every pair of vertices u, v  V it holds that
dG(u, v)  dG (u, v)  dG(u, v) + .

If G is a subgraph of G, it is called an (, )-spanner.

In STOC'01, Elkin and Peleg [EP01] showed that for any > 0 and  = 1, 2, . . . , there exists a



=

(

, ) such

that for any

n-vertex

graph G there exists a

(1+

, )-emulator

of size

O(log

·n1+

1 

)

and

a

(1+

, )-spanner

of

size

O(

·n1+

1 

).

Emulators

and

spanners

with

these

parameters

are

called

near-additive. The parameter  is called the additive stretch or error of the respective emulator or

spanner. In [EP01] the additive stretch is  

log 

log -1
, and this estimate stays the state-of-

the-art. Based on [AB16], Abboud et al. [ABP18] showed a lower bound of  = 

1 log 

log -1
.

In SODA'06, Thorup and Zwick [TZ06] devised another scale-free construction of near-additive

emulators. Their size and additive stretch are similar to those in [EP01], but the same construction

applies for all > 0.

Near-additive emulators and spanners were a subject of intensive research in the last two decades

[Elk01, EZ04, TZ06, Pet07, Pet08, Pet10, EN16a, EN17a, EN20, EP01, ABP18, HP18, EM19].

They found numerous applications for computing almost shortest paths and distance oracles in

various computational settings [Elk01, EZ04, BR11, EP15, EN17a, ASZ20]. Moreover, a strong

connection between them and hopsets was discovered in [EN16a, EN17a, HP17]. Hopsets are also

extremely useful for dynamic and distributed algorithms [HKN18, Coh94, HKN16, LP15, EN16b,

EN17b, CDKL19, DP20, LN20]. See also a recent survey [EN20] for an extensive discussion about

the relationship between emulators, spanners and hopsets.

A significant research effort was put into decreasing the sparsity level of near-additive emulators

and spanners [Pet07, Pet08, Pet10, EN17a, ABP18]. Pettie [Pet08] showed that one can efficiently

construct near-additive spanners of size O(n · (log log n)), where   1.44 (with  = log n and

=

log log n

log log n
).

He

then

further

improved

the

size

bound

to

O(n · (log(4) n)),

where

log(4) n

is the four-times iterated logarithm [Pet10]. The latter construction is however less efficient. (By

efficient construction, we mean here centralized running time of O(|E| · n), for an arbitrarily small

constant  > 0, and a distributed CONGEST time of O(n). We also call the latter low polynomial

time.)

Elkin and Neiman [EN17a] devised an efficient construction of near-additive spanners with size

O(nlog log n) and of linear-size emulators. In the same paper, they also came up with an efficient

distributed construction of ultra-sparse (i.e., of size n + o(n)) multiplicative spanners1.

In the current paper we devise the first construction of ultra-sparse near-additive emulators.

Specifically, we show that for any

> 0 and  = 1, 2, . . . , there exists  = ( , ) 

log 

log -1
,

such that for any n-vertex graph G = (V, E), there exists a (1 + , )-emulator of size at most

n1+

1 

.

(Note

that

the

leading

constant

in

front

of

n1+

1 

is

1.)

By

substituting

here



=

(log n),

one obtains a near-additive emulator with  =

log log n

log log n(1+o(1))
and size n + o(n).

1A subgraph G (V, H) is said to be a multiplicative k-spanner of G = (V, E) if for every pair of vertices u, v  V , dG (u, v)  k · dG(u, v). The ultra-sparse multiplicative spanners of [EN17a] have stretch log n · f (n) for an arbitrary slow-growing function f (n) = (1).

2

We also devise efficient (in the above sense) centralized and distributed deterministic algorithms

that construct ultra-sparse emulators. Specifically, for any arbitrarily small constant  > 0 (in ad-

dition to > 0 and  = 1, 2, . . . ), there exists  = ( , , ) such that our distributed deterministic

CONGEST-model algorithm (see Section 1.5.1 for the definition of CONGEST model) computes

(1 +

,

)-emulators

with

at

most

n1+

1 

edges, for  =

log +1/ 

log +1/
in time O(n). In par-

ticular, our algorithm can construct ultra-sparse emulators with  =

log log n+1/ log log n(1+o(1)) 

in deterministic distributed low polynomial time.

A variant of our algorithm also constructs sparse near-additive spanners. Specifically, the

state-of-the-art distributed CONGEST-model deterministic algorithm for building near-additive

spanners is due to [EM19]. For any > 0,  = 1, 2, . . . ,  > 0, there exists  = ( , , ) =

log +1/ 

log +1/
, such that there for any n-vertex graph G = (V, E), the algorithm of [EM19]

constructs a (1 +

, )-spanner

with

O(

·

n1+

1 

)

edges

in

low

polynomial

time

O(n).

At their

sparsest, the spanners of [EM19] employ n ·

log log n+1/ 

log log n+O(1)
edges. Improving upon this

result, we devise a deterministic CONGEST-model algorithm with the same running time that con-

structs (1 +

,

)-spanners

with

O(log



·

n1+

1 

)

edges.

At their sparsest, these spanners employ just

O(nlog log n) edges.

1.2 Technical Overview

All known constructions of sparse near-additive emulators and spanners can be roughly divided into

those that follow the superclustering-and-interconnection (henceforth, SAI) approach of [EP01] and

those that follow its scale-free version [TZ06]. (The constructions of [Elk01, EZ04] follow a different

approach, and result in emulators of size at least (nlog n).)

In the SAI approach, one starts with a partition P0 = {{v} | v  V } of the vertex set into

singleton clusters. Let  log . There are + 1 phases, numbered 0, 1, . . . , , and in all phases

except the last one there are two steps: the superclustering and the interconnection. In the last

phase, the superclustering step is skipped. The input to each phase i  [0, ] is a partial partition2

Pi of V . The phase also accepts as input two parameters, i and degi, where the distance threshold

i determines which clusters of Pi are considered close or nearby (those whose centers are at distance

at most i in G from one another), and degi determines how many nearby clusters a cluster needs

to have to be considered popular (at least degi).

Intuitively, popular clusters C then create superclusters around them, which contain C and all

the nearby clusters. Unpopular clusters are interconnected via emulator edges of weight equal to

the distance between them. The set of superclusters is the partial partition Pi+1 for the (i + 1)st

phase.

In Thorup-Zwick's [TZ06] scale-free version of this construction, clusters of Pi are sampled

independently

at

random,

with

probability

1 degi

each,

and

each

unsampled

cluster

joins

the

closest

sampled cluster. In this way superclusters of Pi+1 are created. In addition, for every unsampled

cluster C, it is connected via an emulator edge to every other unsampled cluster C which is closer

to it than the closest sampled cluster. The weight of this edge is equal to the distance in G between

the respective cluster centers. This is an analogue of the interconnection step from [EP01].

In both these approaches, ultimately the number of edges in the emulator is analyzed as the

sum over all phases i of the number of edges added to the emulator on phase i. One notes that each

superclustering step forms a forest and thus contributes O(n) edges. In addition, in [EP01], the

2A family A of pairwise disjoint subsets of a set B is called a partial partition of B.

3

degree sequence degi is designed in such a way that the interconnection step of each phase i con-

tributes

at

most

n1+

1 

edges.

As

a

result,

the

overall

size

of

the

emulator

is

O((log

)(n

+

n1+

1 

))

=

O(log



·

n1+

1 

).

For  = log n this becomes O(n · log log n).

Subsequent improvements in the

sparsity level of near-additive emulators and spanners [Pet08, Pet10, EN17a, ABP18, HP18, Pet07]

optimized the degree sequence deg0, deg1, . . . , deg , so that the numbers of edges m0, m1, . . . , m

contributed on the interconnection steps of phases 0, 1, . . . , , respectively, decrease geometrically

and

the

total

number

of

edges

sums

up

to

O(n1+

1 

).

In

this

way

one

can

guarantee

that

the

overall

contribution

of

interconnection

steps

is

O(n1+

1 

),

while the additive stretch  grows very little if at all. Elkin and Neiman [EN17a] argued also that

the overall contribution of superclustering steps is O(n) (as opposed to the naive O(nlog )), and

as a result derived an emulator of linear size.

Our main technical contribution is in a novel analysis. We adopt the original degree sequence

of [EP01] as is, rather than using the optimized degree sequences from [Pet08, Pet10, EN17a]. We

then argue that the overall contribution of all the superclustering and interconnection steps together

is

at

most

n1+

1 

.

We

achieve

this

by

carefully

charging

edges

inserted

to

the

emulator

during

the

entire algorithm to vertices, and arguing that no vertex is overloaded. By doing so we obtain a new

structural understanding of this important construction, and derive the existence of ultra-sparse

near-additive emulators.

We also show an efficient centralized implementation of this algorithm. Specifically, given

parameters > 0,  > 0 and  = 1, 2, . . . , our algorithm constructs a (1 + , )-emulator with at

most

n1+

1 

edges

and



= (

, , ) =

log +1/

log +1/
in O(|E| · n) deterministic time. This

running time matches the state-of-the-art running time known for building denser near-additive

emulators and spanners [Pet10, EN17a, EM19].

1.2.1 Distributed Implementation
Our distributed algorithm is deterministic, and at the end of it, every vertex v  V knows about all emulator edges incident on it.
A large research effort was invested in implementing the SAI approach efficiently in the distributed CONGEST model in the context of near-additive spanners and hopsets [EP01, EN16a, EN17a, HP17, EM19]. Implementing this approach in the CONGEST model in the context of nearadditive emulators presents a new challenge, since for every new emulator edge e = (u, v), both its endpoints need to be aware of its existence and weight. Specifically, the center of every supercluster needs to learn of all clusters that have joined its supercluster. Since the number of clusters that join a single supercluster might be very large, this causes congestion. This issue does not arise in the construction of near-additive spanners, as spanner edges can be added locally, and the center of the supercluster does not need to learn of all the clusters that have joined its supercluster. In the context of hopsets, this challenge was addressed by broadcasting messages along a BFS tree that spanned the entire graph. This approach results in running time which is at least linear in the graph's diameter, which may be prohibitively large.
In the current paper, we devise a superclustering scheme that ensures that the number of messages each vertex has to send in every step is relatively small. This is done by splitting very large superclusters into many superclusters. Intuitively, such a splitting may result in an increased number of levels of the construction, and therefore, in a higher additive term  and increased running time. We show that this is not the case for our algorithm.
To our knowladge, there are no known distributed deterministic algorithms for building nearadditive emulators of linear size. The only existing algorithm with these properties is the random-

4

ized algorithm of Elkin and Neiman [EN16a]. However, the algorithm of [EN16a] does not ensure

that for every emulator edge (u, v), both endpoints know of its existence. (Our algorithm provides,

in fact, ultra-sparse emulators, while that of [EN16a] guarantees just linear size.)

The only known distributed CONGEST deterministic algorithm for building near-additive span-

ners

[EM19]

constructs

spanners

of

size

O(

n1+

1 

).

The

construction

there

can

be

adapted

to

build

emulators

of

size

O(log



·

n1+

1 

)

(i.e.,

of

size

(n

·

log

log

n)),

but,

like

the

algorithm

of

[EN16a],

it

does not guarantee that for every emulator edge e = (u, v) both u and v will be aware of it.

1.3 Related Work
The problem of efficiently constructing ultra-sparse multiplicative spanners and emulators was extensively studied in [ADD+93, HZ96, RZ04, DMP+05, DMZ06, Pet07, Pet10, EN17a].
Spanners and emulators are known to be related to spectral sparsifiers [KP12, JS20]. Ultrasparse sparsifiers, or shortly ultra-sparsifiers, play a key role in a variety of efficient algorithms. Spielman and Teng [ST04] used them for computing sequences of preconditioners. See also [CKM+14, KLOS14, KMST10, She13] for their applications to maximum flow and other fundamental problems.
We believe that the problem of devising ultra-sparse near-additive emulators is as fundamental as that of devising ultra-sparse multiplicative spanners and ultra-sparsifiers.

1.4 Outline
Section 1.5 provides basic definitions for this paper. In Section 2 we present a construction of ultra-sparse near-additive emulators in the centralized model. The properties of the construction are summarized in Corollaries 2.14 and 2.15. In Section 3 we show a distributed CONGEST implementation of our algorithm. The properties of the distributed construction are summarized in Corollaries 3.11 and 3.12. In Section 3.3 we provide an efficient centralized construction of ultrasparse near-additive emulators, which is based on our distributed construction. The properties of the construction are summarized in Theorems 3.13 and 3.14. Section 4 contains an efficient, deterministic CONGEST-model construction of sparse near-additive spanners. The properties of the construction are summarized in Corollary 4.4.

1.5 Preliminaries
Throughout this paper, we denote by rC the center of the cluster C and say that C is centered around rC. The center rC is a designated vertex from C, i.e., rC  C. Throughout the paper, when the logarithm base is unspecified, it is equal to 2. For a pair of integers a, b, where a  b, the term [a, b] stands for {a, a + 1, . . . , b}.

1.5.1 The Distributed CONGEST Model
In the distributed model [Pel00] we have processors residing in vertices of the graph. The processors communicate with their graph neighbors in synchronous rounds. In the CONGEST model, messages are limited to O(1) words, i.e., O(1) edge weights3 or ID numbers. The running time of an algorithm in the distributed model is the worst case number of communication rounds that the algorithm requires.
3In our case, the input graph is unweighted.

5

1.5.2 Ruling Sets
Given a graph G = (V, E), a set of vertices W  V and parameters ,   0, a set of vertices A  W is said to be an (, )-ruling set for W if for every pair of vertices u, v  A, the distance between them in G is at least , and for every u  W there exists a representative v  A such that the distance between u, v is at most .

2 Centralized Construction

In this section we devise an algorithm that, given a graph G = (V, E) on n vertices and parameters

< 1 and   2 constructs a (1 +

, )-emulator

for

G

with

at

most

n1+

1 

edges in polynomial

time in the centralized model, where  = O

log 

log -1
. In particular, by setting  = (log n),

we construct an emulator with n + o(n) edges with  =

log log n

log log n+O(1)
.

Section 2.1 contains a general overview of the centralized construction. The properties of the

resulting emulator and of the construction are analyzed in Section 2.2.

Our centralized construction is based on the centralized algorithm of Elkin and Peleg [EP01].

As was described in the introduction, both algorithms (of [EP01] and ours) follow the SAI approach

to constructing near-additive emulators. There are, however, some important differences in both

the algorithm and in its analysis. In the algorithm of [EP01] popular clusters C (see Section 1.5

for its definition) create superclusters C around them, that contain only clusters that are close

to the cluster C. All unpopular clusters C that are not merged into one of the superclusters

are then interconnected with other nearby unpopular clusters, but not with nearby superclusters.

To guarantee connectivity (and small stretch) between the superclusters and nearby unpopular

clusters, the algorithm of [EP01] employs a separate ground partition. The spanning forest of this

ground partition contributes at most n - 1 additional edges to the emulator, which are swallowed

by

the

overall

size

estimate

of

O(n1+

1 

).

On

the

other

hand,

in

our

current

algorithm

we

aim

at

a

size

bound

of

exactly

n1+

1 

,

and

thus we cannot afford using a separate ground partition. Instead, once our algorithm creates a

star-like supercluster C, it also inserts all unclustered clusters C that are nearby the supercluster

C into a set Ni of buffer clusters. These buffer clusters will not be allowed to create superclusters

around them. They will be allowed to join other superclusters that will be constructed in future.

However, if no future supercluster will incorporate them, the supercluster C will do so. As a result,

superclusters created in our algorithm may have larger radii in comparison to those constructed

in [EP01]. This adaptation of the SAI approach of [EP01] takes care of the connectivity (and

small stretch) between superclusters C and their nearby unclustered clusters C , without paying

an additional (additive) term of n - 1 edges in the size of the emulator.

In addition, the size analysis of [EP01] also analyzes separately contributions of different phases

of the algorithm, and then sums them up. This is also the case in all subsequent works [TZ06, Pet09,

EN17a, EM19]. As was discussed in the introduction, this naive summation (even with optimized

degree

sequences)

is

doomed

to

result

in

an

emulator

of

size

at

least

n1+

1 

+

n

-

O(1)



2n

-

O(1).

Our size analysis carefully combines contributions of all different phases altogether, and thus results

in

the

bound

of

exactly

n1+

1 

.

Finally, the adaptation that we discussed above induces some modifications of stretch analysis

as well. This is since, as discussed earlier, the radii of clusters constructed by our algorithm may

be larger than the radii of clusters constructed in [EP01]. Specifically, both in our result and in

that of [EP01],  = O

log 

log -1
, but the constant hidden by the O-notation in our bound is

6

slightly larger than that in [EP01]. Yet another variant of the construction of [EP01] was given in [EN17a]. In this variant of the
construction, cluster centers are sampled, and clusters that are close to sampled clusters join them to create superclusters. As a result, the connectivity (and small stretch) between superclusters C and nearby unclustered clusters C is ensured without employing a ground partition. On the other hand, this scheme requires randomization, while our approach is deterministic. Also, the size analysis of [EN17a], like that of [EP01], analyzes each phase separately, and as a result, it cannot be used to provide ultra-sparse emulators.
2.1 The Construction
Our algorithm initializes H =  and proceeds in phases. The input to each phase i  [0, ] is a collection of clusters Pi, a degree parameter degi and a distance threshold parameter i. The parameters , {degi, i | i  [0, ]} are specified in Section 2.1.2. The set P0 is initialized as the partition of V into singleton clusters, i.e., clusters containing one single vertex each.
Consider an index i  [0, ], and let C, C be a pair of clusters in Pi, centered around vertices rC, rC , respectively. We say that rC , rC are neighboring cluster centers if dG(rC , rC )  i. If rC, rC are neighboring cluster centers, their respective clusters C, C are said to be neighboring clusters.
Intuitively, in each phase i the algorithm sequentially considers centers of clusters from Pi and connects them with their neighboring cluster centers, i.e., it adds to the emulator H an edge between them. The weight of each new edge is set to be the length of the shortest path in G between its endpoints. Each added edge is charged to a center of a cluster in Pi. Centers of clusters that do not have many neighboring cluster centers are charged with all the edges that were added to the emulator when they were considered. However, cluster centers that have many neighboring cluster centers, i.e., popular cluster centers, require a different approach. They are still connected with their neighboring cluster centers, but they are not charged with these edges. Instead, their neighbors are required to share the burden.
Generally speaking, in each phase, we interconnect cluster centers that are not popular, and form superclusters around popular cluster centers. The set of superclusters formed in phase i is the input Pi+1 for phase i + 1. This allows us to defer work on these dense areas of the graph to later phases of the algorithm.
2.1.1 Execution Details
We now describe the execution of a phase i  [0, ] of the algorithm. At the beginning of phase i, define Si to be the set of centers of clusters C  Pi and Ui, Ni = . Ui is the set of unclustered clusters during phase i. Ni is an additional auxiliary set of cluster centers, which will be eventually superclustered. On the other hand, once a cluster C joins Ni, it will not be allowed to create a supercluster around it.
The algorithm sequentially considers vertices from Si. While the set Si is not empty, the algorithm removes a single vertex rC from Si. A Dijkstra exploration is executed from rC to depth i. Let (rC) be the set of vertices rC  Si  Ni that were discovered by the exploration (note that rC / Si  Ni, and so rC / (rC )). For each vertex rC  (rC ), the edge (rC , rC ) is added to the emulator H with weight dG(rC , rC ).
If |(rC)| < degi, then the center rC is charged with all edges added to H as a result of an exploration originated from it. See Figure 1 for an illustration. The cluster C of the vertex rC is added to the set Ui of unclustered clusters.
7

Figure 1: Interconnection edges. The considered (dark gray) cluster center does not have many neighbors. The direction of the edges indicates that they are charged to the center of C.

Figure 2: Superclustering edges. The considered cluster center (dark gray) has many neighbors. A supercluster C is formed around C and contains the neighbors C to which C added an edge. The direction of the edges indicates that the white clusters are charged to the centers of the neighbors of C.

However, if |(rC)|  degi, then rC cannot be charged with these edges. A new supercluster C is formed around rC. The new supercluster contains the cluster C  Pi of rC, and all clusters C such that their centers are in (rC). The vertex rC becomes the center of the new supercluster C. The new supercluster C joins the set Pi+1, which is the input collection for the next phase. See Figure 2 for an illustration. The cluster centers in (rC) are removed from Si and from Ni. Thus, they will not be considered by the algorithm.
The algorithm described thus far is not sufficient. Consider a case where the algorithm has
already formed some superclusters in phase i, and then it considers a cluster center rC that has at least degi neighboring cluster centers, but many of them have been superclustered in this phase. See Figure 3 for an illustration. The cluster center rC must be connected with its neighboring cluster centers. However, the center rC cannot be charged with these edges, nor can we form a supercluster around it containing all of its neighbors, as many of them already belong to other
superclusters.
To avoid such occurrences altogether, when a supercluster C is formed around a vertex rC in phase i, every cluster center rC  Si with i < dG(rC , rC )  2i is removed from Si, and is added to a set Ni.
Consider a vertex rC  Ni. If at the end of phase i it has not been superclustered, it is added to the supercluster C that was formed when rC was added to Ni. Let rC be the center of C. The edge (rC, rC ) is added to H with weight dG(rC, rC ). This edge is charged to rC . See Figure 4 for an illustration.
This completes the description of phase i. Observe that the designation of a cluster center as
popular or unpopular depends on the order in which the algorithm removes cluster centers from Si. For example, consider the star graph G = (V, E) with V = {u0, u1, . . . , un} and E = {(u0, ui)|i  [1, n]}. If in phase 0 the algorithm begins by considering the cluster center u0, then it is designated as popular. However, if the algorithm considers the cluster center u0 last, then it will not be designated as a popular cluster, since it does not have any neighbors from Si  Ni (as at this point the sets Si, Ni are empty). Hence we cannot a-priori define a set of popular clusters.
This completes the description of the algorithm. The pseudocode of the algorithm is given in
Algorithm 1.

8

Figure 3: The cluster C (depicted as white circle) can have many neighbors in Pi that have joined superclusters in phase i (the gray curved areas) before its center was considered. When we consider the center rC of C, we cannot connect it with its neighboring cluster centers, since we cannot charge rC for the added edges, nor can we require its neighbors to be charged for them. In the figure, the solid and dotted lines represent emulator and graph edges, respectively.

Figure 4: Clusters from Ni join a neighboring supercluster. In the figure, the gray curved area represents a supercluster centered around C. The dark gray circles represent neighbors of C that joined C when it was formed. The stripped circles represent clusters that had centers in Si when C was formed, and remained in Ni until the end of phase i. At the end of phase i, they join the cluster C. Their cluster centers are charged with the edges that connect them with C.

Algorithm 1 Construction of a Near-Additive Emulator

Input: Graph G = (V, E), a parameter  N, and sequences deg0, . . . , deg , 0, . . . , 

1: P0 = {{v} | v  V }

2: for i  [0, ] do

3: Si  all centers of clusters from Pi

4:

Ui, Ni, Pi+1  

5: while Si =  do

6:

remove a cluster center rC from Si

7:

for all cluster centers rC  Si  Ni s.t. dG(rC , rC )  i do

8:

add to the emulator H an edge (rC , rC ) with weight dG(rC , rC )

9:

if rC has less than degi neighboring cluster centers in Si  Ni then

10:

add the cluster C of rC to Ui

11:

else

12:

C C

13:

rC  rC

14:

for all clusters rC  Si  Ni such that dG(rC , rC )  i do

15:

remove rC from Si or Ni.

16:

let C be the cluster centered at rC

17:

C  C  {C }

18:

for all clusters rC  Si such that dG(rC , rC )  2i do

19:

Si = Si \ {rC }

20:

Ni = Ni  {rC }

21:

Pi+1  Pi+1  {C}

22: for all cluster centers rC  Ni do

23:

let C be the supercluster that was formed when rC joined Ni, and let rC be the center

of C

24:

let C be the cluster centered at rC

25:

add to the emulator H an edge (rC , rC ) with weight dG(rC, rC )

26:

C  C  {C }

9

2.1.2 Setting Parameters

In this section we specify the selection of the parameters degi, and i.

The degree parameter degi controls the number of edges added to the emulator, and also the

number of phases required until we are left with a small number of clusters. For every i  [0, ], we

2i
set degi = n  .

Set

=

log

+1 2

.

In Section 2.2.1 we show that |P |  deg .

Therefore, there are no popular

cluster centers in phase , and superclusters are not formed during this phase. It follows that

P +1 =  and U = P .

Define recursively R0 = 0, and for every i  [1, ] define Ri+1 = 2i + Ri. The distance threshold parameter is defined by i = (1/ )i + 2Ri, for every i  [0, ]. Intuitively, Ri is an upper bound on

the radii of clusters in Pi, i.e., the maximal distance in the emulator H between a center rC of a

cluster C  Pi and a vertex u  C. In Lemma 2.5 we prove that this inequality indeed holds.

2.2 Analysis of the Construction
In Section 2.2.1 we analyze the size of the emulator. In Section 2.2.3 we show that the algorithm can by executed in polynomial time. Finally, in Section 2.2.2 we analyze the stretch of the emulator.

2.2.1 Analysis of the Number of Edges

In this section, we analyze the size of the emulator H. We will charge each edge in the emulator H to a single vertex. We begin by proving that in the concluding phase there are no popular clusters. To do so we show that the size of P is at most deg .

Lemma 2.1. For every index i  [0, ], each supercluster C constructed in phase i consists of at least degi + 1 clusters from Pi.

Proof. Let i  [0, ], and let C be a supercluster that was created around a cluster C in phase i. The algorithm added edges from C to at least degi clusters from Si  Ni. These clusters, and C itself, all became superclustered into C. Thus, C contains at least degi + 1 clusters from Pi.

In the next lemma, we argue that superclusters are disjoint, and thus Lemma 2.1 can be used to bound the number of superclusters formed during each phase.

Lemma 2.2. For i  [0, - 1], all superclusters formed during phase i are pairwise disjoint.

Proof. Let C be a supercluster formed during phase i. Recall that all centers of clusters that have joined C were in Si  Ni until they joined C. Also recall that once a cluster joins a supercluster, its center is removed from Si and from Ni, and it is not added to Si or Ni in the future.
On the one hand, this implies that all clusters that have joined C did not join any other supercluster before they joined C. On the other hand, once a cluster joined C, its center is removed from Si and Ni, and therefore it will not join another supercluster in future. Hence, we conclude that all superclusters formed during phase i are pairwise disjoint.

In the next lemma we provide an upper bound in the size of Pi, for every index i  [0, ].

Lemma 2.3. For i  [0, ], we have

|Pi|



n1-

2i -1 

.

10

Proof. The proof is by induction on the index i. For i = 0, the right-hand side of the equation is equal to n. Thus the claim is trivial.
For the induction step, assume that the claim holds for some i  [0, - 1]. By Lemmas 2.1 and 2.2 and the induction hypothesis, we obtain:

|Pi+1|



n1-

2i -1 

· (degi + 1)-1



n1-

2i -1 

·

n-

2i 



n1-

2i+1 

-1

.

Hence the claim holds also for i + 1.

Recall that

=

log

+1 2

. Observe that Lemma 2.3 implies that

|P

|



n1-

2

-1 

2
n

= deg .

(1)

Therefore, in phase there are no popular clusters. It follows that P +1 is an empty set, and that U = P .
Next, we examine the edges added by each phase i of the algorithm, and charge each edge to a center of a cluster C  Pi. Recall that there are two types of edges in the emulator:
1. Interconnection edges, added when the algorithm considered an unpopular cluster center rC. These edges are charged to rC. See Figure 1.

2. Superclustering edges, added when a cluster C joined a supercluster C that was formed around a cluster C, where C = C . See Figures 2, 4 for an illustration. These edges are charged to the centers of clusters C that were superclustered into the new supercluster formed around C. For example, if for some h  1, clusters C1, C2, . . . , Ch, centered at vertices v1, v2, . . . , vh, respectively, are clustered into a supercluster rooted at a cluster C, then each of these centers v1, v2, . . . , vh is charged with a single edge. Note that the center of the cluster C is not charged with any edges in phase i.

Interconnection edges that were added in phase i are charged to centers of clusters C  Ui. Observe that a cluster C has joined Ui only if its center has added less than degi edges to the emulator H. Therefore, phase i adds at most |Ui| · degi interconnection edges to the emulator H. (Note that Ui might be empty.)
Superclustering edges that were added in phase i are charged to centers of clusters that did
not join Ui, and also that no supercluster was formed around them in phase i. Thus, phase i adds exactly |Pi| - |Ui| - |Pi+1| superclustering edges.
Hence, in phase i  [0, ], the number of edges added to the emulator H is at most:

|Ui| · degi + |Pi| - |Ui| - |Pi+1| = |Pi| + |Ui| · (degi - 1) - |Pi+1|.

(2)

In particular, this bound applies to the last phase i = . Recall that by eq. (1) we have |P | = |U |  deg , and also |P +1| = . Therefore the bound becomes just |P | · deg .
We will now use the size of Pi+1 to bound the size of Ui. Observe that by Lemma 2.1 and because superclusters of Pi+1 are disjoint, we have that for all i  [0, ],

|Ui|  |Pi| - |Pi+1| · (degi + 1).

(3)

By eqs. (2) and (3) we have that in phase i, the number of edges added to the emulator H is

at most:

|Pi| + |Ui| · (degi - 1) - |Pi+1|

 |Pi| + (|Pi| - |Pi+1| · (degi + 1)) · (degi - 1) - |Pi+1| = |Pi| · degi - |Pi+1| · (degi2 - 1) - |Pi+1|

(4)

= |Pi| · degi - |Pi+1| · degi2.

We are now ready to bound the size of the emulator H.

11

Lemma

2.4.

The

number

of

edges

in

the

emulator

H

satisfies

|H |



n1+

1 

.

Proof. By eq. (4), and since P +1 is an empty set, we obtain that the number of edges added by all phases of the algorithm is at most:

(|Pi| · degi - |Pi+1| · degi2) = |P0| · deg0 + |Pi| · (degi - degi2-1).

i=0

i=1

Recall that for all i  [0,

2i
] we have degi = n 

thus degi - degi2-1 = 0. Also, recall that |P0| = n.

Thus, the number of edges added to the emulator by all phases i  [0,

]

is

at

most

n1+

1 

.

2.2.2 Analysis of the Stretch
In this section we analyze the stretch of the emulator H. We begin by providing an upper bound on the radii of clusters in Pi.
For an index i  [0, ] and a cluster C  Pi centered around a vertex rC, the radius of C is defined to be Rad(C) = max{dH (rC, v) | v  C}. The radius of the collection of clusters Pi is defined to be Rad(Pi) = max{Rad(C) | C  Pi}. We begin by proving that Ri is an upper bound on the radii of clusters in Pi, for all i  [0, ]. Recall that i = (1/ )i + 2Ri, for every i  [0, ]. Also, recall that R0 = 0, and for every i  [1, ], we have Ri+1 = 2i + Ri.
Lemma 2.5. For every index i  [0, ], we have Rad(Pi)  Ri.
Proof. The proof is by induction on the index of the phase i. For i = 0, all clusters in Pi are singletons, and also R0 = 0, and so the claim holds.
Assume the claim holds for some index i  [0, - 1] and prove that it holds for i + 1. Consider a cluster C  Pi+1. This cluster was formed around a vertex rC during phase i. Consider a vertex u  C.
Case 1: The vertex u belonged to the cluster of rC in Pi. In this case, by the induction hypothesis we have dH (rC , u)  Ri  Ri+1.
Case 2: The vertex u belonged to a cluster C  Pi, where rC / C . Denote by rC the center of the cluster C . Since the center rC joined the supercluster of rC, we conclude that dG(rC, rC )  2i. When rC joined the supercluster C, the edge (rC, rC ) was added to the emulator H, with weight dG(rC , rC ). Thus, dH (rC , rC )  2i. By the induction hypothesis, we also have dH (rC , u)  Ri. Hence,
dH (rC , u)  dH (rC , rC ) + dH (rC , u)  2i + Ri = Ri+1.

We now provide an upper bound on Ri. Observe that for every i  [1, ], we have Ri+1 = 2i + Ri = 2 (1/ )i + 5Ri. Lemma 2.6. For every index i  [0, ], we have

i-1

Ri = 2 ·

-j · 5i-1-j .

j=0

Proof. The proof is by induction on the index i. For i = 0, both sides of the equation are equal to 0. So the base case holds.

12

Assume that the claim holds for some index i  [0, - 1], and prove that it holds for i + 1. By definition and the induction hypothesis we have:

Ri+1 = 2 (1/ )i + 5 · 2 ·

i-1 j=0

-j · 5i-1-j =

2·

i j=0

-j · 5i-j

By Lemma 2.6, we derive the following explicit bound on Ri, for all i  [0, ].

Ri = 2 · 5i-1

i-1 j=0

(5

)-j



2

· 5i-1

·

1 5

i-1 ·

1 1-5

=

2 1-5

·

1 i-1 .

Assume that  1/10. It follows that

1 i-1

Ri  4

.

(5)

Next, we show that the emulator H contains edges that connect every center of a cluster in Ui with all its neighboring cluster centers.

Lemma 2.7. Let i  [0, ] and let rC be a center of a cluster C  Ui. Then, for every neighboring cluster center rC of rC, we have

dH (rC , rC ) = dG(rC , rC ).

Proof. Let rC be a center of a cluster C  Ui and let rC be a neighboring cluster center of rC, such that rC  C and C  Pi. By definition, dG(rC, rC )  i. Since C  Ui, the algorithm has considered rC, and executed a Dijkstra exploration from it during phase i.
Case 1: The cluster center rC was in Si  Ni when the cluster center rC was considered by the algorithm. In this case, since dG(rC, rC )  i, the edge (rC, rC ) was added to H with weight dG(rC , rC ).
Case 2: The cluster center rC was not in Si  Ni when the cluster center rC was considered by the algorithm. Then, the cluster C of rC has either joined Ui or became superclustered before the cluster center rC was considered by the algorithm. Assume towards contradiction that C became superclustered before rC was considered. Therefore, a supercluster was grown around a cluster center rC with dG(rC , rC)  2i, and so rC was removed from Si, contradiction (see line 19 of Algorithm 1). Therefore, we conclude that C has joined Ui before rC was considered by the algorithm. The algorithm has executed a Dijkstra exploration from rC , and since dG(rC, rC )  i, the emulator H contains the edge (rC, rC ) with weight dG(rC, rC ).

We now show that for every vertex v  V there exists an index i  [0, ] such that v belongs to

a cluster that joins the set Ui. For notational purposes, define U-1 = , and U (i) =

i j=-1

Ui

for

all i  [-1, ]. We say that a vertex v is U (i)-clustered for some i  [0, ] if there exists a cluster

C  U (i) such that v  C.

Lemma 2.8. For every index i  [0, ], the set Pi  U (i-1) is a partition of V .

Proof. The proof is by induction on the index of the phase i. For i = 0, the claim is trivial since
P0 is a partition of V into singleton clusters. Assume the claim holds for some index i  [0, - 1]. Let v  V . By the induction hypothesis,
v belongs to a cluster C  Pi  U (i-1). If C  U (i-1), then, by definition, C  U (i). If C  Pi, then in phase i, the cluster C has either been superclustered into a supercluster of Pi+1, or it has joined Ui. In any case, C  Pi+1  U (i), and so Pi+1  U (i) is a partition of V . Thus the claim holds for i + 1.

13

Recall that by eq. (1), we have that |P |  deg and thus, P +1 = . Therefore, Lemma 2.8 implies that U ( ) is a partition of V .
In the following lemma we argue that superclusters form a laminar family.

Lemma 2.9. Let 0  j  i  be a pair of indices. Let C  Ui be a cluster, and v  C be a vertex. Then, there exists a cluster C  Pj such that v  C .

Proof. The proof is by induction on i - j. The induction base is i - j = 0. Then, C = C, and we
are done.
For the induction step, suppose that the assertion holds for some non-negative integer h. We
prove it for h + 1. By the induction hypothesis, there exists a cluster C~  Pi-h such that v  C~. Also, the
cluster C~ is a disjoint union of clusters from P(i-h)-1. Hence, in particular, there exists a cluster C  Pi-h-1 = Pi-(h+1) such that v  C .

We are now ready to bound the stretch of the emulator H. The outline of the proof is as follows.

Consider a pair of vertices u, v  V and let (u, v) be the shortest path between them in G. We

will show that (u, v) can be divided into smaller segments, and that for each such segment there is

a path in H between its endpoints u , v that is not significantly longer than the distance between

u , v in the original graph G.

Define

recursively

0

=

0,

0

=

1,

and

for

i

>

1

define

i

=

2i-1

+ 6Ri

and

i

=

i-1

+

i
1-

i

· i.

Lemma 2.10. Let u, v  V be a pair of vertices and let (u, v) be a shortest path between them. Let i be the minimal index such that all vertices on the path (u, v) are U (i)-clustered. Then,

dH (u, v)  i · dG(u, v) + i.

Proof. The proof is by induction on the index i. For i = 0, all vertices on the path (u, v) are

U0-clustered, thus they all added to the emulator H all edges that are incident to them.(Recall that 0 = 1/ 0 + 2R0 = 1. See also lines 7 to 10 of Algorithm 1.) Therefore, the path (u, v) itself

is contained in the emulator H.

Let i  [1, ]. Assume that the claim holds for i - 1, and prove that it holds for i. Let (u, v)

be a pair of vertices such that all vertices on a shortest path (u, v) are U (i)-clustered. Denote

d = |(u, v)|. For convenience, we imagine that the vertices of (u, v) appear from left to right,

where u is the leftmost vertex and v is the rightmost vertex.

We divide the path (u, v) into segments S1, S2, . . . Sq, each of length exactly (1/ )i , except

for the last segment that can be shorter than (1/ )i . Hence, q 

d (1/ )i



d (1/ )i-1

=

di 1- i

.

Consider a single segment S. Denote by x, y the left and the right endpoints of S, and denote by

S = (x, y) the subpath of (u, v) between them. (It is convenient to visualize the path (u, v) as

going from the leftmost vertex u to the rightmost vertex v.)

Case 1: The segment S does not contain a Ui-clustered vertex. Then, all the vertices of the segment are U (i-1) clustered. Hence, by the induction hypothesis

dH (x, y)  i-1 · dG(x, y) + i-1.

Case 2: Let z1, z2 be the first and the last Ui-clustered vertices on the path (x, y), respectively. Let C1, C2  Ui be the clusters such that z1  C1 and z2  C2. (Note that it is possible that C1 = C2.) Both clusters intersect S = (x, y), and the length of S is at most (1/ )i. In addition, by Lemma 2.5 we have that the radii of the clusters C1, C2 is at most Ri. Let r1, r2 denote the centers of clusters C1, C2, respectively. It follows that dG(r1, r2)  (1/ )i + 2Ri = i. Hence clusters

14

Figure 5: The analysis of the stretch of the em-
ulator. For a pair u, v  V such that a shortest path (u, v) between them is U (i)-clustered, we
divide the path (u, v) into segments of length at most (1/ )i. For a single segment, denote by
C1, C2 the first and the last Ui-clustered clusters on the path (u, v). The emulator H contains
an edge between the centers r1, r2 of the clusters C1, C2 with weight dG(r1, r2).

Figure 6: The path in H between w1, r1, where z1 is the first Ui-clustered vertex on the segment S, w1 is the predecessor of z1 on S, and r1 is the center of the cluster C1 such that z1  C1. The dotted line represents the path x, y in G. The straight solid lines represent edges of the emulator, and the curved lines represent paths in the emulator between vertices and the centers of their respective clusters in phase j.

C1, C2 are neighboring. By Lemma 2.7 we have that dH (r1, r2) = dG(r1, r2). See Figure 5 for an illustration. As a result, by triangle inequality,

dH (r1, r2) = dG(r1, r2)  Ri + dG(z1, z2) + Ri.

Next, we bound the distances dH (x, r1), dH (r2, y). Let w1, w2 be the predecessor and successor

of z1, z2 on (x, y), respectively. See Figure 6 for an illustration. Observe that both w1, w2 are U (i-1)-clustered.

We will show that there is a path of length at most 2Ri from w1 to r1. (This is also true for

r2, w2, and the proof is analogous.)

Let C  Uj be the cluster such that w1  C . Observe that j < i. Let C1  Pj such that z1  C1 (by Lemma 2.9, such a cluster exists). Denote r , r1 the centers of the clusters C , C1, respectively.

By Lemma 2.5, we have

dH (w1, r )  Rj.

(6)

In phase j, the cluster C joined Uj. Since there is an edge between the clusters C , C1, and since their radii are at most Rj, we have dG(r , r1)  2Rj + 1  j. By Lemma 2.7, we have

dH (r , r1) = dG(r , r1)  2Rj + 1.

(7)

Since r1 belongs to the cluster C1, we also have by Lemma 2.5 that

dH (r1, r1)  Ri.

(8)

By eqs. (6) to (8) we have dH (w1, r1)  3Rj + 1 + Ri. Observe that Ri = 2 1 i-1 + 5Ri-1 

2 + 5Rj. It follows that

dH (w1, r1)  2Ri.

(9)

Similarly, we have

dH (r2, w2)  2Ri.

(10)

Recall that all vertices on the subpaths of (x, y) between x, w1 and w2, y, are U (i-1)-clustered. Thus the induction hypothesis is applicable to them. It follows that:

dH (x, w1)  i-1 · dG(x, w1) + i-1 and dH (w2, y)  i-1 · dG(w2, y) + i-1 (11)

15

By eqs. (9) to (11) and section 2.2.2 we derive:

dH (x, y)  dH (x, w1) + dH (w1, r1) + dH (r1, r2) + dH (r2, w2) + dH (w2, y)  dH (x, w1) + 2Ri + 2Ri + dG(z1, z2) + 2Ri + dH (w2, y)  i-1 · dG(x, w1) + dG(z1, z2) + i-1 · dG(w2, y) + 2i-1 + 6Ri  i-1 · dG(x, y) + 2i-1 + 6Ri.

Since we divided the path (u, v) into q 

di 1- i

such segments, we obtain that for the pair

u, v, their distance in the emulator H satisfies:

dH (u, v) 

q j=1

(i-1

·

dG(xj

,

yj )

+

2i-1

+

6Ri)



i-1

·

dG(u, v)

+

(dG(u, v)

·

i
1-

i

+

1)

·

(2i-1

+

6Ri)



dG(u, v) ·

i-1

+

i
1-

i

·

(2i-1

+

6Ri)

+ 2i-1 + 6Ri.

Recall

that

i

=

2i-1 + 6Ri

and

that

i

=

i-1

+

i
1- i

· i.

It

follows

that:

dH (u, v) 

i-1

+

i
1-

i

· i

· dG(u, v) + i

=

i · dG(u, v) + i.

Recall that U ( ) is a partition of V . As a corollary to Lemma 2.10 we have:

Corollary 2.11. For every pair of vertices u, v  V , the distance between them in H satisfies:

dH (u, v)   · dG(u, v) +  .

It is left to provide an upper bound on  ,  . Recall that 0 = 0, 0 = 1, and for i > 1 we have

i

=

2i-1

+

6Ri

and

i

=

i-1

+

i
1-

i

·

i.

Lemma 2.12. For all i  [0, ], we have: i =

i j=0

2i-j

·

6Rj .

Proof. The proof is by induction on the index of the phase i. For i = 0, since 0 = 0 and R0 = 0, both sides of the equation are equal to 0.
We assume that the claim holds for some i  [0, - 1], and prove that it holds for i + 1. By the induction hypothesis we obtain:

i+1 = 2i + 6Ri+1 = 6Ri+1 + 2 ·

i j=0

2i-j

·

6Rj

=

i+1 j=0

2i+1-j

·

6Rj

We will now provide an explicit bound on i. By eq. (5) for all i  [1, ], we have that Ri  4 · 1 i-1. Recall also that R0 = 0. Since we assume  1/10, we have

i 

i j=1

2i-j

·

6Rj



i j=0

2i-j

·

24

·

1 j-1



24 1-2

·

1 i-1



30 1 i-1 .

(12)

For all i  [1,

],

we

have

i

=

i-1

+

i
1-

i

·

i.

Thus,

i



i-1

+

30 1-

i

 i-1 + 34

.

(Note that

 1/10.) Since 0 = 1, we have:

i = 1 + 34 · i.

(13)

As a corollary to Corollary 2.11 and eqs. (12) and (13), we have:

Corollary 2.13. For every pair of vertices u, v  V the distance between them in the emulator H

satisfies:

1 -1

dH (u, v)  (1 + 34 · ) · dG(u, v) + 30 ·

.

16

2.2.3 Analysis of the Running Time

The algorithm runs for + 1 phases. Each phase i  [0, ] consists of executing at most |Pi|  n

Dijkstra

explorations,

each

requires

O(|E| + nlog n)

time.

By

2.3

we

have

that

|Pi|



n1-

2i -1 

for

all i  [0, ]. Recall that

=

log

+1 2

. Hence, the running time of the entire algorithm is bounded

by

O(|E| + nlog n) · |Pi|  O(|E| + nlog n) ·

n1-

2i -1 

(14)

i=0

i=0

2.2.4 Rescaling

Define = 34 · . Observe that we have = 34 . We replace the condition < 1/10 with the much

stronger condition < 1.

Recall that

=

log

+1 2

.

Note that

log

+1 2

 log  for all   2. The additive term  now

translates to:

1 -1

-1
1

34log  log -1

  30 ·

= 30 ·

= 30 ·

34

Denote now = .

Corollary 2.14. For any parameters < 1 and   2, and any n-vertex graph G = (V, E), our

algorithm

constructs

a

(1 +

, )-emulator

with

at

most

n1+

1 

edges

in

polynomial

deterministic

time in the centralized model, where

log  log -1

=O

.

Note that be setting  = f (n) · (log n), for a function f (n) = (1), we obtain an emulator of

size at most

n1+

f

1 (n)log

n

=

1
n · 2 f(n)

=n

1+O

1

f (n)

= n + o(n).

By Corollary 2.14, we derive:

Corollary 2.15. For any parameter < 1 and any n-vertex graph G = (V, E), our algorithm

constructs a (1 + , )-emulator with n + o(n) edges in poly(n) deterministic time in the centralized

model, where

log log n (1+o(1))log log n

=

.

Using techniques discussed in Section 3 one can improve the running time in this result to O(|E| · n) , for an arbitrarily small parameter  > 0, at the expense of increasing  to
log(log n) + -1 log(log n)+-1+O(1) .


17

3 A Construction of Ultra-Sparse Near-Additive Emulators in the CONGEST Model

In this section we provide an implementation of the algorithm described in Section 2 in the dis-

tributed CONGEST model. Here we aim at a low polynomial time, i.e., O(n) for an arbitrarily

small constant parameter 1/ <  < 1/2. Recall that  is a parameter that controls the size

of the resulting emulator. We will show that for any parameters   2, and 1/   < 1/2,

and any n-vertex unweighted undirected graph G = (V, E), our algorithm constructs a (1 + , )-

emulator

with

at

most

n1+

1 

edges,

in

O (n

· )

deterministic

time

in

the

CONGEST

model,

where

=

log +-1 

log +-1+O(1)
.

In particular, by setting  = (log n), we obtain a (1 + , )-emulator of size n + o(n), with

=

log(log n)+-1 

log(log n)+-1+O(1)
, in deterministic CONGEST model in low polynomial time.

From this point until the end of the paper, we assume that all vertices have unique IDs such

that for all v, v.ID  [0, n - 1], and all vertices know their respective IDs. Moreover, we assume

that all vertices know the number of vertices n. In fact, our results apply even if vertices know an

estimate n~ for n, where n  n~  poly(n), and have distinct ID numbers in the range [1, n~].

3.1 The Construction
As in the centralized variant of the algorithm, the distributed variant also initializes H =  and proceeds in phases. The input to each phase i  [0, ] is a collection of clusters Pi, a degree parameter degi and a distance threshold parameter i. The parameters , {degi, i | i  [0, ]} are slightly different in the current variant of the algorithm, and are specified in Section 3.1.1. The set P0 is initialized as the partition of V into singleton clusters.
In the distributed model, sequentially considering clusters requires too much time. Hence, the definition of popular clusters and cluster centers is slightly different in the distributed variant of the algorithm.
For every index i  [0, ], a pair of distinct clusters C, C  Pi and their respective centers rC, rC are said to be neighboring clusters and neighboring cluster centers if dG(rC, rC )  i. A cluster C and its center rC are said to be popular, if C has at least degi neighboring clusters.
Intuitively, each phase is divided into two consecutive steps. The superclustering step of phase i begins by detecting popular clusters from Pi and clusters that have a neighboring popular cluster, and continues by grouping them into superclusters. When this step terminates, all clusters that have not been superclustered are not popular, and also, all of their neighboring clusters are not popular. Denote by Ui the set of clusters from Pi that did not join a supercluster during phase i. In the interconnection step, clusters from Ui are interconnected with their neighboring clusters. The details of the implementation of the superclustering step and the interconnection step are given in Sections 3.1.2 and 3.1.3, respectively.
As in the centralized version, we will show that in the last phase , we will have that |P |  deg , and therefore there are no popular clusters. Hence, the superclustering step of this phase is skipped, and we move directly to the interconnection step.

3.1.1 Setting Parameters

Define recursively R0 = 0, and for every i  [1,

]

define

Ri+1

=

(

4 

+

2)i

+

Ri.

The distance

threshold parameter is defined to be i = (1/ )i + 2Ri, for every i  [0, ].

18

In our distributed implementation of the algorithm, the execution of each phase i requires

(degi) time. Recall that we aim at a low polynomial time. Therefore, we ensure that degi  n

for all phases i  [0, ]. We divide the phases into two stages. In the exponential growth stage,

2i
that consists of phases {0, 1, . . . , i0 = log  }, we set degi = n  . For the fixed growth stage, that

consists of phases {i0 + 1, i0 + 2 . . . ,

=

i0

+

+1 

-

1},

we

set

degi

=

n.

3.1.2 Superclustering Step
In this section, we provide the execution details for the superclustering step of phase i  [0, - 1]. During this step, we complete three tasks. The first task is to detect the set of popular clusters. The second task is to select representatives around which superclusters will be constructed. The third and most complicated task is to construct the superclusters around the selected representatives, such that all popular clusters are superclustered.
Task 1: Detecting popular clusters. To detect popular clusters, we employ the modified Bellman-Ford exploration, devised in [EM19].
Generally speaking, we initiate a modified parallel Bellman-Ford exploration from the set of centers of clusters in Pi. The exploration consists of i + 1 strides. In stride 0, each vertex v  V initializes a list L(v) = . Each center rC of a cluster C  Pi writes the element rC, 0 to its list L(rC). In every stride j  [1, i], each vertex v  V delivers to its neighbors in G messages regarding the (up to) degi + 1 cluster centers it has learnt about during stride j - 1. If a vertex has received messages regarding more than degi + 1 centers during some stride j  [0, i - 1], it arbitrarily chooses degi + 1 of these messages to forward during stride j + 1. Observe that stride 0 requires O(1) time, and each one of the strides j  [1, i] require O(degi) communication rounds. When the exploration terminates, each center rC of a cluster C  Pi that has received messages regarding at least degi other cluster centers is defined popular. Denote by Wi the set of popular cluster centers.
For completeness, the pseudo-code of the algorithm appears below. Theorem 3.1 summarizes the properties of the algorithm. For its proof, see Theorem 2.1 in [EM19].

Algorithm 2 Detecting Popular Clusters

1: Input: graph G = (V, E), a set of clusters Pi, parameters degi, i 2: Output: a set Wi. 3: Each vertex v  V initializes a list L(v) = .

4: Each rC  Si adds rC .ID, 0 to L(rC ).

5: for j = 1 to i do

6: for degi rounds do

7:

if v received at most degi + 1 messages rC, j - 1 then

8:

For each received messages rC, j - 1 , v sends rC, j

9:

if v received more than degi + 1 messages rC, j - 1 then

10:

For arbitrary degi + 1 received messages rC, j - 1 , v sends rC, j

11: Each rC  Si that has learned about at least degi other cluster centers joins Wi.

Theorem 3.1. Given a graph G = (V, E), a collection of clusters Pi centered around cluster centers Si and parameters i, degi, Algorithm 2 returns a set Wi in O(degi · i) time such that:
1. Wi is the set of all centers of popular clusters from Pi.

19

2. Every cluster center rC  Si that did not join Wi knows the identities of all the centers rC  Si such that dG(rC , rC )  i. Furthermore, for each pair of such centers rC , rC , there is a shortest path  between them such that all vertices on  know their distance from rC .
Alternatively4, one can accomplish the task of Algorithm 2 even faster, in time O(degi + i), via the (S, d, k)-source detection algorithm of Lenzen and Peleg [LP13]. In the (S, d, k)-source detection problem, one is given a subset S of sources, and two integers d and k. The algorithm of [LP13] computes for every vertex v  V at most k sources s  S that satisfy dG(v, s)  d. The running time of their (deterministic) algorithm is O(min{d, D} + min{k, |S|}). In our case, S = Si, d = i, k = degi, and as a result the running time is O(degi + i). The algorithm of [LP13] can also produce the shortest path  between cluster centers rC and rC as above, within the same running time. Our algorithm, however, has a number of other steps that require O(degi · i) time, and thus using the (simpler) algorithm given in Algorithm 2 for detecting popular clusters is good enough for our purposes.
Task 2: Selecting representatives. To select a subset of the popular clusters, we compute a (2i+1, 2i/)-ruling set for Wi w.r.t. the graph G. See Section 1.5.2 for the definition of ruling sets. This is done using the algorithm of [SEW13, KMW18]. Theorem 3.2 summarizes the properties of the returned ruling set Si.
Theorem 3.2. [SEW13, KMW18] Given a graph G = (V, E), a set of vertices Wi  V and
1
parameters q  {1, 2, . . .}, c > 1, one can compute a (q + 1, cq)-ruling subset for Wi in O(q · c · n c ) deterministic time, in the CONGEST model.
For the sake of brevity, denote sepi = 2i +1 and ruli = (2/)·i. By Theorem 3.2, the returned subset Si is a (sepi, ruli)-ruling set for the set of popular clusters Wi.
Task 3: Constructing superclusters. First, a BFS exploration rooted at the ruling set Si is executed to depth ruli + i in G. As a result, a forest Fi is constructed, rooted at vertices of Si.
Consider a cluster center rC  Si, and let TC be its tree in the forest Fi. A cluster C is said to be spanned by TC if its center rC is spanned by TC. Intuitively, we would like to form a new supercluster C centered around rC, that will contain all the clusters C spanned by TC. This requires informing rC of all the centers of clusters that are spanned by TC, which may cause significant congestion. Therefore, we use a different approach, that may form several superclusters that will cover all clusters spanned by TC.
To form superclusters, we backtrack the BFS exploration that has created TC. The backtracking procedure operates for ruli + i strides, each consists of 2degi + 2 communication rounds. During each stride d, each vertex v  V that is spanned by TC and has dTC (rC, v) = ruli + i - d sends messages to its parent in the tree TC. For d = 0, let M = . For d > 0, let M be the set of messages that v has received during stride d - 1 of the procedure. If v is a center of a cluster from Pi, it adds the message mv = v, dG(rC, v) to M . If the number of messages in M is smaller than 2degi + 2, then v sends all the messages in M to its parent w.r.t. TC during stride d.
Consider the case where |M |  2degi + 2. In this case, we say that v is a hub-vertex. Since it cannot send |M | messages to its parent in TC, the vertex v decides to split from TC and form new superclusters. Note that the vertex v receives messages from its children in the tree TC only during stride d - 1, where d = dTC (rC , v).
If v is a center of a cluster from Pi, it forms a single new supercluster Cv, and v is set to be the center of the new supercluster Cv. For every message mrC = rC , dG(rC , rC ) in M , it adds the edge (v, rC ) to the emulator with weight dG(v, rC ) = dG(rC, rC )-dG(rC , v). The vertex v informs
4We are grateful to an anonymous reviewer of PODC'21 for pointing this to us.
20

rC of the new edge and its weight. This is done by sending the message mrrC = (v, rC ), dG(v, rC ) along the same route that the message mrC has traversed.
If v is not a center of a supercluster from Pi, we do not allow it to be a center of a cluster of Pi+1, and therefore it forms other superclusters. The vertex v partitions its children in TC into sets V1, V2, . . . , Vt, such that for every j  [1, t] the number of messages that v has received from all vertices in Vj is between 2degi + 2 and 6degi + 6. For every j  [1, t], let Zj be the set of vertices in TC that have sent messages that have arrived v via a vertex in Vj. Intuitively, a supercluster
Cj is formed for every j  [1, t]. This supercluster will contain every cluster C  Pi such that its centers rC is in Zj. See Figure 7 for an illustration.
Partitioning the children of v into t sets is done in the following way. For a set X of children
of v, denote by M (X) the set of messages that v has received from all vertices in X. The vertex v
greedily adds its children into sets V1, V2, . . . , Vt , such that each set Vj is filled until |M (Vj)| is at most 4degi + 4. Note that since the number of messages received by v from each one of its children is less than 2degi + 2, we have that |M (Vj)|  2degi + 2, for any j  [1, t - 1]. If for the last set Vt we have |M (Vt )| < 2degi + 2, we add the set Vt to the set Vt -1. Let t be the number of sets formed by this process (i.e., if |M (Vt )| < 2degi + 2 then t = t - 1. Otherwise, t = t ). Now we have that 2degi + 2  |M (Vj)|  6degi + 6 for every j  [1, t].
For every j  [1, t], the vertex v selects a single vertex r  Zj to be the center of Cj. Then,
v must inform all vertices in Zj that their attempt to join C has failed, and provide information regarding their new cluster center and superclustering edge. To this aim, we define the tree TCj to be the tree that contains all paths from TC between a vertex in Zj and v. Observe that TCj does not contain any other hub-vertices.
The vertex v broadcasts the message r in the tree TCj . This informs all centers in Zj that their attempt to join C has failed, and that the center of their new supercluster is r. In addition, for every r  Zj, the vertex v broadcasts the message r , dG(r , v) + dG(v, r) to all vertices in TCj . In particular, this step informs the vertices r, r that the edge (r, r ) was added to the emulator H with
weight dG(r , v)+dG(v, r). Observe that the vertex v knows dG(rC, v), dG(rC, r) and dG(rC, r ), and since it belongs to shortest rC - r and rC - r paths, it can infer dG(v, r) and dG(v, r ). In addition, by triangle inequality, these superclustering edges never shorten distances w.r.t. the graph G.
After the ruli + i strides terminate, for every message rC , dG(rC, rC ) that arrives to rC, the edge (rC, rC ) is added to the emulator H with weight dG(rC, rC ). All vertices that belong to the cluster C centered around rC join the supercluster C. This completes the description of the procedure for forming superclusters.
In the following lemma, we analyze the running time of Task 3, and show that all computation
terminate within the ruli + i strides.

Lemma 3.3. For every index i  [0,

-

1],

Task

3

requires

O(

i 

·

degi)

communication

rounds.

Proof. Consider an index i  [0, - 1]. On our way to constructing superclusters, we execute a BFS exploration that forms the forest Fi and a backtracking procedure. In addition, we take care of hub-vertices.
The BFS exploration that forms the forest Fi is executed to depth ruli + i from the set of vertices in Si. Thus, it requires O(ruli + i) time. The backtracking procedure of phase i is done in ruli + i strides, where each stride consists of O(degi) communication rounds. Thus, its overall running time is O((ruli + i)degi).
It is left to analyze the time required to take care of hub-vertices. Let v be a hub-vertex in some tree TC  Fi. If v is a center of a cluster of Pi, it forms a new supercluster around itself. For every vertex rC such that v has received the message mrC from rC , the vertex v sends a message

21

Figure 7: Forming superclusters that cover all clusters of Pi spanned by the BFS tree TC. In the figure, the vertex rC is the root of the tree TC. The four gray areas are four superclusters, and the big squares are their respective centers. The dashed lines from v1, v3 to rC represent the fact that v1, v3 have more than 2degi + 2 messages to deliver to rC. The vertex v1 is a center of a cluster of Pi, and so it becomes the center of a new supercluster. The vertex v3 is not a center of a supercluster, and so it divides its children into two sets, and chooses a representative from each
set to become the center of its respective supercluster.

mrrC . This message is sent along the same route that the message mrC has traversed. Note that v has received less than 2degi + 2 messages from each one of its children. Therefore, v sends less

than 2degi + 2 messages to each one of its children. The distance each such message is required

to traverse is at most ruli + i. Hence, all messages returned by v to their senders arrive within

O(ruli + i + degi) communication rounds (via pipelined broadcast).

If v is not a center of a cluster in Pi, its partitions its children into sets V1, V2, . . . , Vt. Note

that this partition is computed locally. Recall that for every j  [1, t], the number of messages

that v has received from all vertices in Vj is |Zj|  6degi + 6. Also recall that v broadcasts O(|Zj|) messages along the edges of the tree TCj . The depth of the tree is at most O(ruli + i). Hence, a broadcast that originated from v terminates within O(ruli + i + degi) communication rounds (via

pipelined broadcast).

Recall that ruli = (2/) · i. It follows that the overall time required to complete Task 3 in

phase

i

is

O((ruli

+

i)degi)

=

O(

i 

·

degi).

Next, we prove that all popular clusters in Pi and clusters in Pi that have a popular neighboring cluster are superclustered into superclusters of Pi+1.

Lemma 3.4. Consider a cluster C  Pi. If C is popular or it has a neighboring cluster that is popular, then C belongs to a supercluster of Pi+1.

Proof. Recall that Wi is the set of centers of popular clusters from Pi and that Si is a (sepi, ruli)ruling set for Wi. Consider a cluster C  Pi. If C is popular, then by definition of Si there exists a vertex v  Si such that the distance between v and the center of C is at most ruli. Hence, the cluster C became superclustered during the superclustering step of phase i.
If C is not popular but it has a neighboring cluster C that is popular, then there exists a
vertex v  Si such that the distance between v and the center of C is at most ruli. The distance between the centers of C, C is at most i. Therefore, there exists a vertex v  Si with distance at most ruli + i from the center of C. Hence, the cluster C became superclustered during the superclustering step of phase i.

3.1.3 Interconnection Step In this section, we provide the execution details for the interconnection step of phase i  [0, ]. Denote by Ui the set of clusters from Pi that have not been superclustered during the superclustering
22

step of phase i. In the interconnection step of phase i, each cluster in Ui is connected with all its neighboring clusters from Pi.
For i  [0, - 1], by Lemma 3.4 we have that every cluster C  Ui is not popular. By Theorem 3.1 the center rC of the cluster C already knows the identities and distances to all its neighboring cluster centers, since Algorithm 2 was executed during the superclustering step. Hence, the center rC knows which edges it needs to add to the emulator H, as well as their weights.
Let rC be a neighboring cluster center of rC. The center rC must inform rC that C has joined Ui, and therefore the edge (rC, rC ) is added to the emulator H. For this aim, we employ Algorithm 2, as in the superclustering step of phase i, from the centers of all clusters in Ui. Recall that by Lemma 3.4 we have that every cluster that has a neighboring cluster that is popular is superclustered. Since C  Ui, we conclude that rC is not popular. Hence, by Theorem 3.1 we have that during the current execution of Algorithm 2, the cluster rC has received a message from rC, and thus it knows the identity of rC and the distance dG(rC, rC ). Now, both endpoints of the edge (rC, rC ) know that it is added to the emulator H with weight dG(rC, rC ).
The interconnection step of phase is slightly different. Recall that the superclustering step of phase is skipped. However, as we later show (eq. (17)), all clusters in P are not popular. In the interconnection step of phase , we execute Algorithm 2 from the set P with parameters deg ,  . By Theorem 3.1, we are guaranteed that all centers of clusters in P know the identities and distance to all their neighboring cluster centers. Therefore, each center of a cluster C  P can add to the emulator H edges to all its neighboring cluster centers. This completes the description of the interconnection step of phase i.

3.2 Analysis of the Construction

In this section, we analyze the size and stretch of the emulator, and the running time that is required to construct it. We begin by showing that in the final phase , we have |P |  deg . Thus, there are no popular clusters, and the superclustering step can be safely skipped. To do so, we show that the number of clusters in Pi+1 is significantly smaller than the number of clusters in Pi.
Recall that during the superclustering step of every phase i  [0, - 1], a BFS exploration is executed from the ruling set Si, and that Fi is the ruling forest produced by this exploration.

Lemma 3.5. Let i  [0, - 1]. For every tree TC in Fi, let xC be the number of clusters of Pi that are spanned by TC. Then, the number of superclusters formed around vertices of TC is at most xC /(degi + 1).

Proof. Consider a tree TC in Fi, and let rC  Si be the root of TC. Recall that rC is the center of a popular cluster rC. Let s be the number of superclusters formed around vertices of TC.
If s = 1, then we have that all clusters that are spanned by TC belong to the supercluster formed around the center rC. Recall that the set Si is a (sepi, ruli)-ruling set, where sepi = 2i + 1. Hence, for every neighboring cluster center rC of rC, we have that dG(rC, rC )  i and thus, rC is the closest vertex to rC in Si. Therefore, rC is spanned by TC. Since C is a popular cluster, it has at least degi neighbors. Note that C itself is also spanned by TC. Hence, we have that xC  degi + 1, and the claim holds.
Consider the case where s  2. Every cluster formed around a vertex v  TC, where v = rC contains at least 2degi + 2 clusters of Pi that are spanned by TC. We conclude that

xC degi+1



(s-1)·(2degi+2) degi+1

=

2s - 2



s.

23

Observe that Lemma 3.5 implies that for every i  [0, - 1] we have that

|Pi+1|  |Pi| · degi-1.

(15)

Next, we provide an explicit bound on the number of superclusters formed in phase i.

Lemma

3.6.

For

every

i



[0, i0

+ 1],

we

have

that

|Pi|



n1-

2i -1 

.

Proof. The proof is by induction on the index of the phase i. For the base case, we have that |P0| = 0, and also n1-(20-1)/ = n.
Assume that the claim holds for some i  [0, i0], and prove that it holds for i + 1.
By the induction hypothesis and by eq. (15) we have

|Pi+1|  |Pi| · degi-1

<

n1-

2i -1 

·

n-

2i 

=

n1-

2i+1 

-1

.

Lemma 3.7.

For every i  [i0 + 1,

],

we

have

that

|Pi|



n1-

2i0

+1 -1 

-(i-i0

-1)

.

Proof. The proof is by induction on the index of the phase i. The base case (i = i0 + 1) holds since

by Lemma 3.6 we have

|Pi0+1|



n1-

2i0

+1 -1 

.

(16)

Assume the claim holds for some i  [i0 + 1, - 1], and prove that it holds for i + 1. By the induction hypothesis and by eq. (15) we have

|Pi+1|



|Pi|

·

degi-1

<

n1-

2i0 +1 

-1

-(i-i0

-1)

·

n-

=

n1-

2i0 +1 

-1

-(i-i0

)

.

Recall that i0 = log  and that

= i0 +

+1 

- 1. Note that   1/2, hence

> i0, and so

deg = n. By Lemma 3.7, the size of the set P satisfies

|P

|

 n1- 2

log  

+1-1 -(

+1 

-2)



n1-

-1 

-

+1 

+2

 n.

(17)

As a result, in the last phase we have |P |  n = deg , and there are no popular clusters in P.

3.2.1 Analysis of the Number of Edges
In this section, we analyze the number of edges added to the emulator H by the algorithm. The analysis follows the lines of the corresponding arguments in Section 2.2.1. As in the centralized construction, here we can also charge each edge that is added to the emulator H in some phase i of the algorithm to a center of a cluster in Pi.
Every superclustering edge added to the emulator during phase i can be charged to a center of a cluster C  Pi that neither joined Ui nor was it selected to grow a supercluster around it during this phase. Therefore, the number of superclustering edges added to the emulator H during phase i is exactly |Pi| - |Ui| - |Pi+1|. Every interconnection edge added to the emulator during some phase i  [0, ] is charged to a center of a cluster C  Ui. Recall that for every cluster C  Ui, its center rC is charged with less than degi edges. Hence, the number of interconnection edges added to the emulator during phase i of the algorithm is at most |Ui| · degi. (The equality is achieved

24

when Ui = .) In addition, Lemma 3.5 implies that the number of clusters from Pi that belong to superclusters of Pi+1 is at least |Pi+1| · (degi + 1). Hence, the number of clusters in Pi that did not join a supercluster during phase i satisfies |Ui|  |Pi| - |Pi+1| · (degi + 1). Thus, the number of edges added to the emulator by all phases of the algorithm is bounded by

|H| 

-1 i=0

(|Pi|

-

|Ui|

-

|Pi+1|

+

|Ui|

·

degi)

+

|P

|

·

deg

=

-1 i=0

(|Pi|

-

|Pi+1|

+

|Ui|

·

(degi

-

1))

+

|P

|

·

deg



-1 i=0

(|Pi|

-

|Pi+1|

+

(|Pi|

-

|Pi+1|

·

(degi

+

1))(degi

-

1))

+

|P

|

·

deg

(18)

=

-1 i=0

|Pi| · degi - |Pi+1| · degi2

+ |P | · deg

= |P0| · deg0 +

-1 i=0

|Pi+1| · degi+1 - |Pi+1| · degi2

.

We now show that for all i  [0, - 1], we have degi+1  degi2. Recall that in the exponential

growth

stage,

degi

=

2i
n,

and

so

degi+1

=

degi2

for

any

i



[0, i0 - 1].

Recall

that

i0

=

log  , and

so

degi20

2i0 +1
=n 

 n

= degi0+1.

In

the

fixed

growth

stage,

degi

= n,

and

so

degi+1

< degi2

for

all i  [i0 + 1, - 1]. It follows that for all i  [0, - 1], we have degi+1  degi2. Thus, by eq. (18),

the size of the emulator H is at most

|H|  |P0| · deg0 = n1+1/.

(19)

3.2.2 Analysis of the Stretch

In this section we analyze the stretch of the emulator H. We follow the lines of the analysis given

in Section 2.2.2. We begin by proving that Ri is an upper bound on the radii of clusters in Pi,

for all

i  [0,

].

Recall that

R0

= 0,

and for every i  [1,

],

we have Ri+1

=

(

4 

+

2)i

+

Ri

(see

Section 3.1.1). Also recall that for every phase i  [0, ], a BFS ruling forest Fi was constructed by

a

BFS

exploration

that

was

executed

to

depth

ruli + i

=

2i 

+ i

=

(2/ + 1) · i.

Hence

the

radius

of each tree in Fi (i.e., the maximal distance between the root of the tree and a vertex spanned

by

the

tree)

is

at

most

(

2 

+ 1)i

as

well.

For

each

tree

T



Fi,

let

Rad(T )

denote

its

radius,

and

Rad(Fi) = maxT Fi Rad(T ).

Lemma 3.8. For every index i  [0, ], we have Rad(Pi)  Ri.

Proof. The proof is by induction on the index of the phase i. For i = 0, all clusters in Pi are

singletons, and also R0 = 0. Thus the claim holds.

Assume the claim holds for i  [0, - 1] and prove that it holds for i + 1. Consider a cluster

C  Pi+1. This cluster was formed around a vertex rC during phase i. Let C  Pi be the cluster

rooted at rC. Consider a vertex u  C.

Case 1: The vertex u belongs to the cluster C. Then, by the induction hypothesis, we have

dH (rC , u)  Ri  Ri+1.

Case 2: The vertex u belonged to a cluster C  Pi, where C = C . Denote by rC the center

of the cluster C . The centers rC and rC are both spanned by the same tree T in Fi. Therefore, we

have

dT (rC , rC

)



2Rad(T )



(

4 

+

2)i.

When rC

joined the supercluster C, the edge (rC, rC )

was

added

to

the

emulator

H,

with

weight

dT (rC , rC

)



(

4 

+

2)i.

In addition, by the induction hypothesis, we have dH (u, rC )  Ri. Hence,

4

dH (rC ,

u)



( 

+

2)i

+

Ri

=

Ri+1.

25

We now provide an explicit upper bound on Ri. Recall that for every i  [1, ], we have i = (1/ )i + 2Ri, and therefore

Ri+1 =

4

+2 

i + Ri =

4 +2

(1/ )i +



8

+5 

Ri.

Lemma 3.9. For every index i  [0, ], we have

4

i-1 1 j 8

i-1-j

Ri =

+2 

·

· +5

.



j=0

Proof. The proof is by induction on the index i. For i = 0, both sides of the equation are equal to 0, and so the base case holds.
Assume that the claim holds for some i  [0, - 1], and prove that it holds for i + 1. By definition and the induction hypothesis we have:

Ri+1

=

(

4 

+

2) (1/

)i

+

(

8 

+

5)

·

(

4 

+

2)

·

=

(

4 

+

2)

·

i j=0

1 j·

i-j

8 

+

5

.

i-1 j=0

1 j·

8 

+

5

i-1-j

By Lemma 3.9, we derive the following explicit bound on Ri, for i  [0, ].

Ri

=

(

4 

+

2)

·

i-1 j=0

1 j·

8 

+

5

i-1-j

=

(

4 

+

2)

·

8 

+

5

i-1
·

i-1 j=0

1 j·

j 8+5

i-1



(

4 

+

2)

·

8 

+

5

·



i

(8+5)

 (8+5)

-1

=

(

4 

+

2)

·

8+5 

i-1
·

 (8+5)

i

·

(8+5) - (8+5)

=

4+2 - (8+5)

·

1 i-1 .

Recall that  < 1/2, and assume that   25 . It follows that for all i  [0, ]:

4 + 2

1 i-1

5

1 i-1 10 1 i-1

Ri   -

· (8 + 5)



·

 - 12.5

· 

.

(20)

As in Section 2.2.2, define recursively 0 = 0, 0 = 1, and for i > 1 define i = 2i-1 + 6Ri and

i

=

i-1

+

i
1-

i

·

i.

Recall that when a cluster is added to Ui, for some i  [0, ], the algorithm adds edges from its

center to the centers of all its neighboring clusters. The weight of each such edge is set to be the

distance in G between its two endpoints. Therefore, the assertion of Lemma 2.7 also holds for the

distributed construction. Thus, for every i  [0, ] and a center rC of a cluster C  Ui, an for every

neighboring cluster center rC of rC , we have dH (rC , rC ) = dG(rC , rC ). As a result, Lemma 2.10 also holds for the distributed construction. In other words, for every pair of vertices u, v  V such

that all vertices on a shortest u - v path are U (i) clustered, we have that

dH (u, v)  i · dG(u, v) + i.

(21)

26

Recall that U-1 = , and U (i) =

i j=-1

Ui

for

all

i



[-1,

].

As in the centralized construction,

here we also have that the set U ( ) is a partition of V . Hence, eq. (21) implies that for every pair

of vertices u, v  V we have

dH (u, v)   · dG(u, v) +  .

(22)

By Lemma 2.12, we have that the recursion 0 = 0 and i = 2i-1 + 6Ri for i > 1 solves to

i
i = 2i-j · 6Rj.
j=0

(23)

We will now provide an explicit bound on i. By eq. (20) for all i  [1, ], we have that

Ri



10 

·

1 i-1. Since we assume

< 1/10, we have

i  By eq. (24) and since

i j=0

2i-j

·

6Rj



· 60·2i


(

1 2

)i+1

1-2

2

=

75 

·

1 i-1 .

(24)



1/10,

the

recursion

0

=

0

and

i

=

i-1

+

i
1-

i

· i

for

i

>

0

solves

to

90

i = 1 +  · i.

(25)

By eqs. (22), (24) and (25) we derive the following corollary.

Corollary 3.10. For every pair of vertices u, v  V the distance between them in the emulator H

satisfies:

90 · dH (u, v)  1 + 

75 1 -1

· dG(u, v) +  ·

.

3.2.3 Analysis of the Running Time
In this section, we analyze the running time of the algorithm. We begin by analyzing the running time of a single phase i  [0, - 1].

Superclustering Step. To detect the popular clusters, we executes Algorithm 2. By Theorem

3.1, the algorithm requires O(degi · i) time. By Theorem 3.2, constructing a ruling set for the

popular

clusters

requires

O(i

·

1 

·

n)

time.

By

Lemma

3.3,

Computing

superclusters

requires

O(

i 

·

degi) time. Hence, the superclustering step of phase i can be executed in O

i·n 

deterministic

time in the CONGEST model.

Interconnection Step. The interconnection step consists of executing Algorithm 2, as in the

superclustering step. Hence, the running time of the interconnection step is dominated by the

running time of the superclustering step.

For the final phase , the superclustering step is skipped. The interconnection step of the final

phase requires executing Algorithm 2, in O(deg ·  ) time. Recall that i = (1/ )i + 2Ri, and also

that

by

eq.

(20)

we

have

that

Ri



10 

·

1

i-1, for every i  [0,

]. In addition, recall that we assume

 > 25 . Hence, for every i  [0, ] we have

i = O((1/ )i)

(26)

It follows that the running time of the entire algorithm is at most

O(deg ·  ) +

-1 i=0

O(

n·i 

)

=

O

n

+

n 

·

-1 i=0

(1/

)i

= O n .

(27)

27

3.2.4 Rescaling

Define

=

90 · 

.

Observe

that

we

have

=

 90

.

We

replace

the

condition

< 1/10 with the much

stronger condition < 1. The assumption  > 25 holds since < 1.

Recall that

=

log 

+

+1 

- 1.

Note that

= log  + -1 + O(1) for all   2. The

additive term  now translates to:

75 1 = ·


-1 75 90

=·





-1
=

log  + -1 log +-1+O(1) .


Denote

log  + -1 log +-1+O(1)

=

.



By eq. (27), the running time of the algorithm is O n = O (n) . Denote now = .

Corollary 3.11. For any parameters < 1,   2 and 1/ <  < 0.5, and any n-vertex graph

G

=

(V, E),

our

algorithm

constructs

a

(1 +

, )-emulator

with

at

most

n1+

1 

edges

in

O(n)

deterministic CONGEST time, where

log  + -1 log +-1+O(1)

=

.



Note that by setting  = f (n) · (log n), for a function f (n) = (1), we obtain an emulator of

size

at

most

n1+

f

1 (n)log

n

=

n + o(n).

By

Corollary

2.14,

we

derive:

Corollary 3.12. For any parameters < 1 and  < 0.5, and any n-vertex graph G = (V, E), our

algorithm constructs a (1 + , )-emulator with n + o(n) edges in O(n) deterministic CONGEST

time, where

log(log n) + -1 log(log n)+-1+O(1)

=

.



3.3 Fast Centralized Construction

To devise an efficient construction of ultra-sparse near-additive emulators in the centralized model of computation, one can simulate the construction provided in Section 3.1 in the centralized model. Given an unweighted, undirected graph G = (V, E) on n vertices, and parameters < 1,  = 1, 2, . . . and   [1/, 1/2], our distributed algorithm runs in O( · n) time. Note that in every communication round, at most one message of O(log n) bits is sent along each edge of the graph G. Thus, simulating this algorithm in the centralized model can be done in O(|E| ·  · n) time. In fact, such a centralized implementation is simpler than the distributed construction. This is because, in the centralized model, there is no need to inform both endpoints of every emulator edge (u, v) of the existence of the edge. Thus, constructing superclusters becomes much easier. Specifically, the execution of Task 3 is simpler, since there is no need to split trees of the forest Fi. The properties of the centralized construction are summarized in the following theorems.

Theorem 3.13. For any parameters < 1,   2 and 1/ <  < 0.5, and any n-vertex graph

G

=

(V, E),

our

algorithm

deterministically

constructs

a

(1 +

, )-emulator

with

at

most

n1+

1 

edges in O(|E| · n) time in the centralized model of computation, where

log  + -1 log +-1+O(1)

=

.



28

Note that by setting  = f (n) · (log n), for a function f (n) = (1), we obtain an emulator of

size

at

most

n1+

f

1 (n)log

n

=

n + o(n).

By

Corollary

2.14,

we

derive:

Theorem 3.14. For any parameters < 1 and  < 0.5, and any n-vertex graph G = (V, E), our algorithm deterministically constructs a (1 + , )-emulator with n + o(n) edges in O(|E| · n) time
in the centralized model of computation, where

log(log n) + -1 log(log n)+-1+O(1)

=

.



In fact, it is easy to see that the factor  can be shaved off from this running time. We omit the details in this version of the paper. (The same is true concerning both Theorems 3.13 and 3.14.)

4 Near-Additive Spanners

In this section, we show how one can modify the construction given in Section 3 to obtain sparse

near-additive spanners. Specifically, given an unweighted, undirected graph G = (V, E), and pa-

rameters > 0,  = 2, 3, . . . and   [1/k, 1/2], our current algorithm constructs a (1+ , )-spanner

with

O(n1+

1 

)

edges,

in

O(n)

deterministic

CONGEST

time,

where

=

log  + -1 log +-1+O(1+log(3) ) .


(28)

We follow the construction described in Section 3.1. We define H =  and proceed in phases.

Throughout the algorithm, instead of adding to H an emulator edges (u, v) with weight d, we

add to the spanner H a u - v path from G of length at most d. Recall that in the emulators

construction, whenever a vertex u  V adds an edge (u, v) with weight d to the emulator, it sends

a message to v along a path from G, of weight at most d. In the current version of the algorithm,

we will add to the spanner the entire path from u to v, along which u informs v of the new edge.

As a result, the construction of superclusters becomes simpler, because the message sent from u to

v contains only the details of v, and does not need to contain any information regarding u or the

edge (u, v). Therefore, there is no need to define hub-vertices as in Task 3 of the superclustering

step (see Section 3.1.2). Thus, a single supercluster C is formed from every tree T in the forest Fi.

Observe that Lemma 3.5 (and as a result, eq. (15)) and Lemma 3.8 hold under this modification.

The distance threshold sequence remains as in Section 3.1. To obtain sparse emulators, we

adopt the degree sequence used in [EN17a], and as a result, the number of phases of the algorithm

slightly increases. The analysis of the number of edges in the current construction is closely related

to the respective analysis in [EN17a].

Let  = max{2, log log }. Define i0 = min{ log  ,  }. For the exponential growth stage,

which

consists

of

phases

i



[0, i0],

we

set

degi

=

n . 2i-1 

+

1 

Define

i0 +1

as

a

transition

phase,

and set

degi0+1 = n/2. For the fixed growth stage, which consists of phases i  [i0 +1, = i0 + 1/-1/2 ],

set degi = n. We will show that |P |  n. Therefore, there are no popular clusters in the last

phase, and the superclustering step can be safely skipped.

By argument similar to those used in Section 3.2.2 and 3.2.4, one can show that the additive

term  of such a construction is

=

log  + -1 log +-1+O(1+log(3) ) .


(29)

29

In addition, by arguments similar to those used in Section 3.2.3 and 3.2.4, one can show that the running time of the algorithm can be upper-bounded by

O (n) .

(30)

4.1 Analysis of the Number of Edges

In this section, we analyze the number of edges added to the spanner H by every phase of the

algorithm. We begin by analyzing the number of superclustering edges added to the spanner H by

every phase i  [0, - 1] of the algorithm.

Observe that during each phase i, the superclustering edges added to the spanner H all belong

to a forest Fi. Hence, each phase contributes at most n superclustering edges. Since there are

+ 1 = O(log  + 1/) phases, this implies that the the number of superclustering edges in the

spanner H is at most

O(n(log  + 1/)) = O(n(log  + 1/)).

(31)

Next, we analyze the number of interconnection edges added to the spanner H by every phase

i  [0, ]. Observe that interconnection edges are added only by centers rC of clusters C  Ui. The

center rC is charged with paths to all its neighboring clusters. Since C  Ui, we know that C is

not popular. Therefore, it is charged with at most degi paths. In addition, by definition and by

eq. (26), the length of each such path is at most i = O

90 

i
. Hence, the number of edges

i

charged to each center of a cluster in Ui can be upper-bounded by O

degi ·

90 

.

1

We

restrict

ourselves

to

the

case

where

90 



n 2 2

,

which

holds

whenever





c log n log( /(

)) ,

for

a

sufficiently small constant c . Observe that Ui  Pi. The number of interconnection edges added

to the spanner H by each phase i can now be upper-bounded by

90 i

1

i

O |Pi| · degi · 

= O |Pi| · degi · n 2 /2 .

(32)

In the next three lemmas, we bound the size of Pi for the exponential growth stage, the transition phase, and the fixed growth stage, respectively.

Lemma

4.1.

For

i



[0, i0

+ 1],

we

have

|Pi|



n . 1-

2i -1-i 

-

i 

Proof. We will prove the lemma by induction on the index of the phase i.

For

i

=

0,

the

right-hand

side

is

n1-

20

-1-0 

-

0 

=

n.

Thus

the

claim

is

trivial.

Assume that the claim holds for some i  [0, - 1] and prove it also holds for i + 1. By eq. (15)

we have that |Pi+1|  |Pi| · degi-1.

Together

with

the

induction

hypothesis,

and

since

for

i,

i



[0,

i0],

we

have

degi

=

n , 2i-1 

+

1 

we

have that

|Pi+1|  |Pi| · degi-1

 n · n 1-

2i

-1-i 

-

i 

-

2i -1 

-

1 

=

n . 1-

2i+1 -1-(i+1) 

-

i+1 

30

Recall that   2. Observe that by eq. (32) and Lemma 4.1, we have that the number of edges added to the spanner H by every phase i  [0, i0] is at most

1

i

O |Pi| · degi · n 2 /2

=

O

n1-

2i

-1-i 

-

i 

· n 2i-1 

+

1 

i
· n 2

· 2-i

=

O

2-in1+

1 

.

(33)

Lemma 4.2. The size of the input collection Pi0+1 for the transition phase satisfies |Pi0+1|  n1-. Proof. By Lemma 4.1, we have

|P |  n = n = n . i0+1

1-

2i0

+1

-1-(i0 

+1)

-

i0 +1 

1-

2i0

+1 -1 

+

i0

+1- (i0 +1) 

1-

2i0

+1 

-1

-

(i0

+1)( 

-1)

Observe

that

since

i0

>0

and



 2,

we

have

(i0+1)(-1) 



1 

.

If i0 = log  , then

|P |  n  n = n . i0+1

1- 2

log  

+1

-1

-

(i0

+1)(-1) 

1-

-1 

-

1 

1-

(34)

Otherwise, if i0 =  , then

|P |  n i0+1

1-

2i0

+1 

-1

-

(-1) 

= n1-

2i0

+1 

-2

-

 

 n1-.

(35)

Recall that i0   . Observe that by eq. (32) and Lemma 4.2, we have that the number of edges added to the spanner H by phase i0 + 1 is at most

1

i0+1

O |Pi0+1| · degi0+1 · n 2 /2

=

O

2-(i0

+1)

n1-

 2

+

+1 2

=

O

2-(i0+1)

n

1 2

.

(36)

Lemma 4.3. For every j  [2, - i0] we have |Pi0+j|  n1-/2-(j-1).
Proof. The proof is by induction on the index j. For j = 2, by Lemma 4.2 we have |Pi0+1|  n1-. In addition, by eq. (15) we have that |Pi0+2|  |Pi0+1| · degi-01+1. Recall that degi0+1 = n/2. Hence we have |Pi0+1|  n1--/2 and so the claim holds.
Assume that the claim holds for some j  [2, - i0 - 1] and prove it holds for j + 1. Recall that degi0+j = n. By eq. (15) we have that |Pi0+j+1|  |Pi0+j| · degi-01+j. Together with the induction hypothesis, we have

|Pi0+j |  n1-/2-(j-1)· · n- = n1-/2-(j+1-1).

Recall that = i0 + 1/ - 1/2 . Hence, by Lemma 4.3 we have |P |  n1-/2-( 1/-1/2 -1)·  n1-/2-1+(3/2)· = n.

(37)

Thus, there are no popular clusters during phase , and eq. (32) holds also for the last phase.

31

Recall that i0 



,

and

therefore

i0 2



 2

.

It

follows

that

1
n 2 2

i0

=

2-i0

n

i0 2



2-i0

n

 2

.

Also, recall that 1/ < . Observe that by eq. (32) and Lemma 4.3, we have that the number of edges added to the spanner H by every phase i0 + j, for j  [2, - i0] is at most

1

i0+j

O |Pi0+j | · degi0+j · n 2 /2

=

O

n1-/2-(j-1)· · n · n/2 · 2-i0

1

j

n 2 /2

=

O

2-i0-j

n1-(j

-2)·

1 

+

j 2

=

O

2-i0-j

n1+

1 

.

(38)

By eqs. (31), (33), (36) and (38) we have that the overall number of edges added to the spanner H by all phases of the algorithm is

1

i0

O(n(log  + )) + O

2-in1+

1 

+O

2-(i0+1)n

1 2

-i0
+O

2-i0

-j

n1+

1 



i=0

j=2

=O

n1+

1 

. (39)

Recall

that

we

restrict

ourselves

to

the

case

where





c log n log( /(

)) ,

for

a

sufficiently

small

constant

c . Also recall that



log 

+

1/-1/2

.

Note that

c log n log( /( ))



log(1/

(log n) )+log(1/)+log(3)

,
n

where

log(3) n is the three-times iterated logarithm. The following corollary summarizes the properties of

current construction.

Corollary 4.4. For any unweighted, undirected n-vertex graph G = (V, E), and any parameters

<

1,





[2,

log(1/

clog n )+log(1/)+log(3)

n

],

for

a

constant

c

and





[1/, 1/2],

our

algorithm

computes

a

(1 +

,

)-spanner

with

O(n1+

1 

)

edges

in

O(n)

deterministic

CONGEST

time,

where

=

log  + -1 log +-1+O(1+log(3) ) .


To obtain the sparsest spanners that one can get with this construction, we set > 0 to be an

arbitrarily small constant, and  =

c log n log(3) n

.

Under this assignment of parameters, the size of the

spanner

is

just

O(nlog

log

n),

and

the

additive

error

is



=

O(

log

log n+1/ 

)log

log

n+1/.

References

[AB16]

Amir Abboud and Greg Bodwin. The 4/3 additive spanner exponent is tight. In Daniel Wichs and Yishay Mansour, editors, Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2016, Cambridge, MA, USA, June 18-21, 2016, pages 351­361. ACM, 2016.

[ABP18] Amir Abboud, Greg Bodwin, and Seth Pettie. A hierarchy of lower bounds for sublinear additive spanners. SIAM J. Comput., 47(6):2203­2236, 2018.

32

[ADD+93] Ingo Alth¨ofer, Gautam Das, David P. Dobkin, Deborah Joseph, and Jos´e Soares. On sparse spanners of weighted graphs. Discrete & Computational Geometry, 9:81­100, 1993.

[ASZ20]

Alexandr Andoni, Clifford Stein, and Peilin Zhong. Parallel approximate undirected shortest paths via low hop emulators. In Konstantin Makarychev, Yury Makarychev, Madhur Tulsiani, Gautam Kamath, and Julia Chuzhoy, editors, Proccedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing, STOC 2020, Chicago, IL, USA, June 22-26, 2020, pages 322­335. ACM, 2020.

[BR11]

Aaron Bernstein and Liam Roditty. Improved dynamic algorithms for maintaining approximate shortest paths under deletions. In Proceedings of the Twenty-Second Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2011, San Francisco, California, USA, January 23-25, 2011, pages 1355­1365, 2011.

[CDKL19] Keren Censor-Hillel, Michal Dory, Janne H. Korhonen, and Dean Leitersdorf. Fast approximate shortest paths in the congested clique. In Proceedings of the 2019 ACM Symposium on Principles of Distributed Computing, PODC 2019, Toronto, ON, Canada, July 29 - August 2, 2019, pages 74­83, 2019.

[CKM+14] Michael B. Cohen, Rasmus Kyng, Gary L. Miller, Jakub W. Pachocki, Richard Peng, Anup B. Rao, and Shen Chen Xu. Solving SDD linear systems in nearly mlog1/2n time. In David B. Shmoys, editor, Symposium on Theory of Computing, STOC 2014, New York, NY, USA, May 31 - June 03, 2014, pages 343­352. ACM, 2014.

[Coh94]

Edith Cohen. Polylog-time and near-linear work approximation scheme for undirected shortest paths. In Proceedings of the Twenty-Sixth Annual ACM Symposium on Theory of Computing, 23-25 May 1994, Montr´eal, Qu´ebec, Canada, pages 16­26, 1994.

[DMP+05] Devdatt P. Dubhashi, Alessandro Mei, Alessandro Panconesi, Jaikumar Radhakrishnan, and Aravind Srinivasan. Fast distributed algorithms for (weakly) connected dominating sets and linear-size skeletons. J. Comput. Syst. Sci., 71(4):467­479, 2005.

[DMZ06] Bilel Derbel, Mohamed Mosbah, and Akka Zemmari. Fast distributed graph partition and application. In 20th International Parallel and Distributed Processing Symposium (IPDPS 2006), Proceedings, 25-29 April 2006, Rhodes Island, Greece. IEEE, 2006.

[DP20]

Michal Dory and Merav Parter. Exponentially faster shortest paths in the congested clique. In Yuval Emek and Christian Cachin, editors, PODC '20: ACM Symposium on Principles of Distributed Computing, Virtual Event, Italy, August 3-7, 2020, pages 59­68. ACM, 2020.

[Elk01]

Michael Elkin. Computing almost shortest paths. In Proceedings of the Twentieth Annual ACM Symposium on Principles of Distributed Computing, PODC 2001, Newport, Rhode Island, USA, August 26-29, 2001, pages 53­62, 2001.

[EM19]

Michael Elkin and Shaked Matar. Near-additive spanners in low polynomial deterministic CONGEST time. In Proceedings of the 2019 ACM Symposium on Principles of Distributed Computing, PODC 2019, Toronto, ON, Canada, July 29 - August 2, 2019., pages 531­540, 2019.

33

[EN16a]

Michael Elkin and Ofer Neiman. Hopsets with constant hopbound, and applications to approximate shortest paths. In IEEE 57th Annual Symposium on Foundations of Computer Science, FOCS 2016, 9-11 October 2016, Hyatt Regency, New Brunswick, New Jersey, USA, pages 128­137, 2016.

[EN16b]

Michael Elkin and Ofer Neiman. On efficient distributed construction of near optimal routing schemes: Extended abstract. In George Giakkoupis, editor, Proceedings of the 2016 ACM Symposium on Principles of Distributed Computing, PODC 2016, Chicago, IL, USA, July 25-28, 2016, pages 235­244. ACM, 2016.

[EN17a]

Michael Elkin and Ofer Neiman. Efficient algorithms for constructing very sparse spanners and emulators. In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2017, Barcelona, Spain, Hotel Porta Fira, January 16-19, pages 652­669, 2017.

[EN17b] Michael Elkin and Ofer Neiman. Linear-size hopsets with small hopbound, and distributed routing with low memory. CoRR, abs/1704.08468, 2017.

[EN20]

Michael Elkin and Ofer Neiman. Near-additive spanners and near-exact hopsets, A unified view. Bull. EATCS, 130, 2020.

[EP01]

Michael Elkin and David Peleg. (1+epsilon, beta)-spanner constructions for general graphs. In Proceedings on 33rd Annual ACM Symposium on Theory of Computing, July 6-8, 2001, Heraklion, Crete, Greece, pages 173­182, 2001.

[EP15]

Michael Elkin and Seth Pettie. A linear-size logarithmic stretch path-reporting distance oracle for general graphs. In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2015, San Diego, CA, USA, January 4-6, 2015, pages 805­821, 2015.

[EZ04]

Michael Elkin and Jian Zhang. Efficient algorithms for constructing (1+, varepsilon;, beta)-spanners in the distributed and streaming models. In Soma Chaudhuri and Shay Kutten, editors, Proceedings of the Twenty-Third Annual ACM Symposium on Principles of Distributed Computing, PODC 2004, St. John's, Newfoundland, Canada, July 25-28, 2004, pages 160­168. ACM, 2004.

[HKN16]

Monika Henzinger, Sebastian Krinninger, and Danupon Nanongkai. A deterministic almost-tight distributed algorithm for approximating single-source shortest paths. In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2016, Cambridge, MA, USA, June 18-21, 2016, pages 489­498, 2016.

[HKN18] Monika Henzinger, Sebastian Krinninger, and Danupon Nanongkai. Decremental singlesource shortest paths on undirected graphs in near-linear total update time. J. ACM, 65(6):36:1­36:40, 2018.

[HP17]

Shang-En Huang and Seth Pettie. Thorup-zwick emulators are universally optimal hopsets. CoRR, abs/1705.00327, 2017.

[HP18]

Shang-En Huang and Seth Pettie. Lower bounds on sparse spanners, emulators, and diameter-reducing shortcuts. In David Eppstein, editor, 16th Scandinavian Symposium and Workshops on Algorithm Theory, SWAT 2018, June 18-20, 2018, Malm¨o, Sweden, volume 101 of LIPIcs, pages 26:1­26:12. Schloss Dagstuhl - Leibniz-Zentrum fu¨r Informatik, 2018.

34

[HZ96]

Shay Halperin and Uri Zwick. Optimal randomized EREW PRAM algorithms for finding spanning forests and for other basic graph connectivity problems. In E´va Tardos,
editor, Proceedings of the Seventh Annual ACM-SIAM Symposium on Discrete Algo-
rithms, 28-30 January 1996, Atlanta, Georgia, USA, pages 438­447. ACM/SIAM, 1996.

[JS20]

Arun Jambulapati and Aaron Sidford. Ultrasparse ultrasparsifiers and faster laplacian system solvers. CoRR, abs/2011.08806, 2020.

[KLOS14]

Jonathan A. Kelner, Yin Tat Lee, Lorenzo Orecchia, and Aaron Sidford. An almostlinear-time algorithm for approximate max flow in undirected graphs, and its multicommodity generalizations. In Chandra Chekuri, editor, Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2014, Portland, Oregon, USA, January 5-7, 2014, pages 217­226. SIAM, 2014.

[KMST10] Alexandra Kolla, Yury Makarychev, Amin Saberi, and Shang-Hua Teng. Subgraph sparsification and nearly optimal ultrasparsifiers. In Proceedings of the 42nd ACM Symposium on Theory of Computing, STOC 2010, Cambridge, Massachusetts, USA, 5-8 June 2010, pages 57­66, 2010.

[KMW18]

Fabian Kuhn, Yannic Maus, and Simon Weidner. Deterministic distributed ruling sets of line graphs. In Structural Information and Communication Complexity - 25th International Colloquium, SIROCCO 2018, Ma'ale HaHamisha, Israel, June 18-21, 2018, Revised Selected Papers, pages 193­208, 2018.

[KP12]

Michael Kapralov and Rina Panigrahy. Spectral sparsification via random spanners. In Shafi Goldwasser, editor, Innovations in Theoretical Computer Science 2012, Cambridge, MA, USA, January 8-10, 2012, pages 393­398. ACM, 2012.

[LN20]

Jakub Lacki and Yasamin Nazari. Near-optimal decremental approximate multi-source shortest paths. CoRR, abs/2009.08416, 2020.

[LP13]

Christoph Lenzen and David Peleg. Efficient distributed source detection with limited bandwidth. In Panagiota Fatourou and Gadi Taubenfeld, editors, ACM Symposium on Principles of Distributed Computing, PODC '13, Montreal, QC, Canada, July 22-24, 2013, pages 375­382. ACM, 2013.

[LP15]

Christoph Lenzen and Boaz Patt-Shamir. Fast partial distance estimation and applications. In Chryssis Georgiou and Paul G. Spirakis, editors, Proceedings of the 2015 ACM Symposium on Principles of Distributed Computing, PODC 2015, Donostia-San Sebasti´an, Spain, July 21 - 23, 2015, pages 153­162. ACM, 2015.

[Pel00] David Peleg. Distributed computing: A locality-sensitive approach. 01 2000.

[Pet07]

Seth Pettie. Low distortion spanners. In Lars Arge, Christian Cachin, Tomasz Jurdzinski, and Andrzej Tarlecki, editors, Automata, Languages and Programming, 34th International Colloquium, ICALP 2007, Wroclaw, Poland, July 9-13, 2007, Proceedings, volume 4596 of Lecture Notes in Computer Science, pages 78­89. Springer, 2007.

[Pet08]

Seth Pettie. Distributed algorithms for ultrasparse spanners and linear size skeletons. In Proceedings of the Twenty-Seventh Annual ACM Symposium on Principles of Distributed Computing, PODC 2008, Toronto, Canada, August 18-21, 2008, pages 253­ 262, 2008.

35

[Pet09] Seth Pettie. Low distortion spanners. ACM Trans. Algorithms, 6(1):7:1­7:22, 2009.

[Pet10]

Seth Pettie. Distributed algorithms for ultrasparse spanners and linear size skeletons. Distributed Computing, 22(3):147­166, 2010.

[RZ04]

Liam Roditty and Uri Zwick. On dynamic shortest paths problems. In Algorithms - ESA 2004, 12th Annual European Symposium, Bergen, Norway, September 14-17, 2004, Proceedings, pages 580­591, 2004.

[SEW13] Johannes Schneider, Michael Elkin, and Roger Wattenhofer. Symmetry breaking depending on the chromatic number or the neighborhood growth. Theor. Comput. Sci., 509:40­50, 2013.

[She13]

Jonah Sherman. Nearly maximum flows in nearly linear time. In 54th Annual IEEE Symposium on Foundations of Computer Science, FOCS 2013, 26-29 October, 2013, Berkeley, CA, USA, pages 263­269. IEEE Computer Society, 2013.

[ST04]

Daniel A. Spielman and Shang-Hua Teng. Nearly-linear time algorithms for graph partitioning, graph sparsification, and solving linear systems. In L´aszl´o Babai, editor, Proceedings of the 36th Annual ACM Symposium on Theory of Computing, Chicago, IL, USA, June 13-16, 2004, pages 81­90. ACM, 2004.

[TZ06]

Mikkel Thorup and Uri Zwick. Spanners and emulators with sublinear distance errors. In Proceedings of the Seventeenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2006, Miami, Florida, USA, January 22-26, 2006, pages 802­809, 2006.

36

