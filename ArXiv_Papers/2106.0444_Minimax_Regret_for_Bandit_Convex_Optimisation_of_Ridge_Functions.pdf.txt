arXiv:2106.00444v2 [cs.LG] 6 Jun 2021

Minimax Regret for Bandit Convex Optimisation of Ridge Functions
Tor Lattimore June 8, 2021

Abstract
We analyse adversarial bandit convex optimisation with an adversary that is restricted to playing functions of the form ft(x) = gt( x,  ) for convex gt : R  R and unknown   Rd that is homogeneous over time. We provide a short information-theoretic proof that the minimax regret is at most O(d n log(n diam(K))) where n is the number of interactions, d the dimension and diam(K) is the diameter of the constraint set.

1 Introduction

Let K  Rd be a convex body (non-empty interior, compact, convex). A game proceeds over n rounds. At the start of the game, an adversary secretly chooses a vector   Sd-1 = {x  Rd : x = 1} and sequence (ft)nt=1 such that for all t, ft : K  R is a function that is:
(a) convex; and

(b) bounded: ft(x)  [0, 1] for all x  K; and

(c) Lipschitz: ft(x) - ft(y)  x - y for all x, y  K; and

(d) a ridge function: ft(x) = gt( x,  ) for some gt : R  R.

The learner then sequentially chooses (xt)nt=1 with xt  K and observes ft(xt), which means that xt should only depend on the previous actions (xs)ts-=11, observed losses (fs(xs))ts-=11 and possibly a source of randomness. The minimax regret is

n

Rn = inf sup max E ft(xt) - ft(x) ,

policy adversary xK

t=1

where the infimum is over all policies, the supremum is over the choices of the adversary subject to the constraints (a)­(d) above, and the expectation integrates over the randomness in the policy. Note that the direction  of the ridge is not known to the learner, but does not change with time.

1

Fact 1. Suppose that f satisfies (a)­(d) above and f (x) = g( x,  ) with   Sd-1. Then g is convex and Lipschitz on the closed interval { x,  : x  K}.
A proof is given in Section 5. A proof of the following theorem is our main contribution.
Theorem 2. Assume that Sd-1  K. Then 
Rn  const d n log(n diam(K)) ,
where const is a universal constant and diam(K) = maxx,yK x - y .
Since the dependence on diam(K) is only logarithmic, the Lipschitz assumption can be relaxed entirely by scaling and restricting the domain of K as explained by Lattimore [2020] and Bubeck et al. [2017].

Related work Our setting is a subset of bandit convex optimisation with the

adversary restricted to ridge functions. There is a long line of work without this

restriction and correspondingly worse regret bounds. The study of bandit convex

optimisation was initiated by Kleinberg [2005] and Flaxman et al. [2005], who in-

troduced simple algorithms based on gradient descent with importance-weighted

gradient estimates. Although these approaches are simple, they suffer from prov-

ably suboptimal dependence on the regret [Hu et al., 2016]. A major open question was whether or not O~(n1/2) regret is possible, which

was closed affirmatively in dimension 1 by Bubeck et al. [2015] and in higher

dimensions by Hazan and Li [2016] and Bubeck and Eldan [2018]. The regret of

the former algorithm depends exponentially on the dimension, while in the latter

the dependence is polynomial. The regret for bandit convex optimisation

bisesO~t(dk2n.o5wnn)ubpypeLratbtoimunodreo[n20t2h0e].

minimax

Our analysis is based on the information-theoretic arguments introduced by

Russo and Van Roy [2014] and used for the analysis of convex bandits by Bubeck

et al. [2015], Bubeck and Eldan [2018] and Lattimore [2020]. All these works rely

on minimax duality and consequently do not yield efficient algorithms. This is

also true of the result presented here. The only polynomial time algorithm

with

O~(n)

regret

for

the

general

case

is by Bubeck et al. [2017]. Although a theoretical breakthrough, the dependence of this algorithm's regret on the dimension is O~(d10.5) and practically speaking

the algorithm is not implementable except when the dimension is very small. All

genuinely practical algorithms for adversarial bandit convex optimisation are gra-

dient methods with importance-weighted gradient estimates. These algorithms

have optimal dependence on the horizon for strongly convex functions [Hazan and

Levy, 2014, Ito, 2020] but otherwise not [Kleinberg, 2005, Flaxman et al., 2005,

Saha and Tewari, 2011, Hu et al., 2016].

As far as we know, the setting of the present paper has not been considered

before. Saha et al. [2021] tackle the case where ft(x) = gt(ht(x)) with ht : Rd  R
a function that is known to the learner. By choosing ft(x) = x,  + t  [0, 1] for some (adversarial) noise (t)nt=1, our setting subsumes an interesting version

2

of the stochastic linear setting, with the restriction being that the noise ishomogeneous and bounded. The standard lower bound for this setting is (d n) [Dani et al., 2008], but with the assumptions required here and by taking (t)nt=1 to be truncated Gaussian, naively this construction would yield a lower bound of (d n/ log(n)). Nevertheless, this shows that the new result is optimal up to logarithmic factors. Note that our setting does not subsume the adversarial linear setting because the direction of the ridge is fixed. An obvious open question is whether or not this can be relaxed.
Notation The unit sphere is Sd-1 = {x  Rd : x = 1}. The support function of a set A  Rd is hA() = supxA x,  . Given a convex function f : K  R, its minimum is f = minxK f (x). The expectation and variance of a random variable X are denoted by E[X] and V[X] respectively. The underlying probability measure will always be obvious from the context. All probability measures are defined over the Borel -algebra on the corresponding space. Given a positive definite matrix , let x  = x x. The convex hull of a set A  Rd is conv(A).

2 Distributions with high projected variance

Let A  Rd be compact and recall that for   Sd-1, hA() + hA(-) is the width of A in direction . The next lemma asserts the existence of a distribution on A for which the standard deviation in every direction is nearly as large as the width.
Lemma 3. For any compact set A  Rd there exists random element X supported on A such that for all   Sd-1,

hA() + hA(-)  2 dV[ , X ] .

Proof. Assume without loss of generality that the affine hull of A spans Rd. If not,

you may work in a suitable affine space. There exists a probability measure  on

A such that with µ = A x d(x) and  = A(x - µ)(x - µ) d(x), the ellipsoid

E = {x :

x-µ

2 -1

 d}

is

the

minimum

volume

enclosing

ellipsoid

of

A

[Todd,

2016, Corollary 2.11]. Therefore, when X has law ,

hA() + hA(-) = sup x - µ,  + µ - y, 
x,yA
 2 sup x - µ -1  
xA

2

d



2 

= 2 dV[ X,  ]

3 Information ratio
Let us first recall the main tool, which upper bounds the minimax regret in terms of the information ratio. The following theorem is a combination of Theorem 2
3

and Lemma 4 by Lattimore [2020], which are proven by extending the machinery developed by Russo and Van Roy [2014], Bubeck et al. [2015] and Bubeck and Eldan [2018]. We outline the key differences in Section 6.
Theorem 4. Given   Sd-1, let F be the space of all functions satisfying (a)­ (d) in the introduction and F = Sd-1 F. Suppose that ,   0 are reals such that for all f¯  conv(F ) there exist probability measures 1, . . . , m on K for which

sup min

f¯dk - f -  (f¯ - f )2 dk   .

f F 1km K

K

Then Rn  const(1 + n + dnm log(n diam(K))) with const a universal constant.

Important remark A previous version of this manuscript claimed the
above theorem even when the ridge  was time dependent, meaning that ft(x) = gt( x, t ) for some adversarial sequence (t)nt=1. Unfortunately this result does not follow from the standard machinery. The problem is that F
is not closed under convex combinations. Why this causes a problem will be
apparent when reading Section 6. New ideas are needed to understand whether
or not Theorem 4 might continue to hold for time varying ridge directions.

Combining the following proposition with Theorem 4 yields Theorem 2.

Proposition 5. The conditions of Theorem 4 hold with

  = (2 + 16 d)/n

 = 29d

m  2 + log2(n2 diam(K)) .

4 Proof of Proposition 5

The proof relies on the following simple lemma.

Lemma 6. Suppose that a  b and g : [a, b]  R is convex and continuous. Let X be a random variable supported on [a, b]. Then, for any  > g(b),

E[(g(X )

-

)2]



(g(b) - )2V[X] (b - a)2

.

Proof. If it exists, let x  [a, b] be a point such that g(x) = . Otherwise let x = a, which by continuity of g means that g(a) < . Then let  : [a, b]  R be the linear function with (b) = g(b) and (x) = g(x), which satisfies

E[(g(X) - )2]  E[((X) - )2]

 min E[((X - b) + g(b) - )2]
R

=

( - g(b))2V[X] E[(b - X)2]



( - g(b))2V[X] (b - a)2

,

4

Figure 1: The typical situation in the proof of Lemma 6. For all x  [a, b], the distance between the linear function  and the horizontal line at  is less than or equal to the distance between f and the same horizontal line.

where the first inequality is geometrically obvious from the convexity of g (Fig. 1) and the second follows by optimising over all linear functions passing through (b, g(b)). The last equality follows by optimising for , while the finally inequality holds because X is supported on [a, b].

Proof of Proposition 5. There are two steps. First, we define a collection of probability measures 0, . . . , m with m = O(log(n)). In the second step we show this collection satisfies the conditions of Theorem 4.

Step 1: Exploration distributions and perturbations Let f¯  conv(F) and choose coordinates on K so that 0 = arg minxK f¯(x). Let

f~(x) = f¯(x) +

x

,

n diam(K)

which is close to f¯, minimised at 0 and increases at least linearly with x . For > 0, define

L = x  K : f~(x) = f~ + ,

which is a level set of f~. Note that L is not convex and boundary effects mean that 0  conv(L ) is not guaranteed. Let 0 be a Dirac at 0 and for k  1 let
2k-1 k = diam(K)n2

and k be the distribution on Lk = L k given by Lemma 3. Consequentially, if hk is the support function of Lk, then for all   Sd-1,

hk() + hk(-)  2 d Vxk [ x,  ] .

(1)

5

Let

m = max{k : k  1}  1 + log2(diam(K)n2) .

(2)

Step 2 Let f  F so that f (x) = g( x,  ) for some convex g and   Sd-1. Let x  K be a minimiser of f and assume without loss of generality that the sign of  has been chosen so that x ,   0. We will prove that there exists a   {0, . . . , m} such that

f~d - f

2  + 16

d

(f~ - f )2 d .

(3)

K

n

K

Let = min{f~(x)-f~ : x  K, x-x ,  = 0}, which exists because f~ is continuous and K  {x : x - x ,  = 0} is compact. We claim that hL () = x ,  . That hL ()  x ,  is immediate. Suppose there exists an x  L with x,  > x ,  . Then by convexity of K and f~, there exists a y  K with y - x ,  = 0 and f~(y) < f~ + , which contradicts the definition of . Therefore hL ()  x ,  and hence hL () = x ,  . Next, by the definition of f~,
L  {x : f~(x)  f~ + }  {x : x  n diam(K)} .

Hence,

x ,  = hL ()  n diam(K) .

(4)

Suppose for a moment that x ,   1/n. It follows from Eq. (4) that 
1/(diam(K)n2), in which case there exists a largest k such that k  . Let x  L be such that x,  = x ,  , which satisfies f~(x) = + f~ . By continuity, there exists an   [0, 1] such that f~(x) = f~ + k. By convexity of f~ and the fact that f~ is minimised at 0,

k + f~ = f~(x)  f~(x) + (1 - )f~ =  + f~ .

Rearranging shows that   k/  1/2 and hence

x ,

x ,   hk()  x,   2

(5)

We consider four cases, the last two of which are illustrated in Fig. 2:

1a
1. f  f~ - 2/n.

2a

2b

2. f  f~ - 2/n and x ,   1/n.

3a

3b

3c

3. f  f~ - 2/n and x ,   1/n and -hk(-)  hk()/2.

4a

4b

4c

4. f  f~ - 2/n and x ,   1/n and -hk(-)  hk()/2.

6

Case 1 By (1a), Eq. (3) holds with  = 0 trivially.
Case 2 By Fact 1, g is Lipschitz on the closed interval { x,  : x  K}. Combining this with (2a) and (2b) yields

(f~ - f )2 d0 = f~ - f (0)
K

 f~ - f - x , 

(g is Lipschitz)

= 1 (f~ - f ) + 1 (f~ - f ) - x , 

2

2

 1 (f~ - f ) 2

(using 2a and 2b)

1 =
2

f~d0 - f
K

.

And again, Eq. (3) holds with  = 0.

Case 3 Let  = f~ - f . Suppose that f (0)  f~ + /4. Then

f~d0 - f =   4 (f~ - f )2 d0

K

K

and Eq. (3) holds with  = 0. Suppose for the remainder of this case that f (0)  f~ + /4. For all x  Lk,

(by Eq. (5))

(by 3c)

x ,  (by Eq. (5))

x ,   hk()  x,   -hk(-)  hk()/2 

. (6) 4

Therefore, x,  / x,   [0, 1] and

x, 

x, 

f (x) = f

x + 1-

0

x, 

x, 

x, 

x, 



f + 1-

(f~ + /4)

x ,

x ,

= f~ + 

5 x,  1-

4

x ,

 f~ -  . 16

Therefore, since k is supported on Lk,

(f~ - f )2 dk =
K

(f~
K

+

k

-

f )2

dk



1 162 (

+

k )2

=

1 162

Hence, Eq. (3) holds with  = k.

(convexity of g)
(by Eq. (6))
2
f~dk - f .
K

7

Figure 2: Examples of Case 3 (left) and Case 4 (right). Case 3 only occurs when -hk(-)  hk()/2, which means that 0 is far outside the convex hull of the level set Lk. Meanwhile, in Case 4 the width of Lk in the direction  is at least the same order of magnitude as hk(). The distance between the dotted lines in both illustrations is the width of Lk in direction , which is hk() + hk(-). Note that because f is a ridge function, all points x in K with x - x ,  = 0 minimise f .
8

Case 4 By definition of the support function x,   [-hk(-), hk()] for all
x  Lk. Furthermore, by Eq. (5), x ,   [hk(), 2hk()]. Hence, by Lemma 6 with a = -hk(-), b = x ,   2hk() and  = f~ + k,

(f~ - f )2 dk = (f~ + k - g( x,  ))2 dk(x)

K

K

(f~ 

+

k -f

)2Vxk [ x,  ]

(2hk() + hk(-))2



(f~

+ k - f )2(hk() + hk(-))2 4d(2hk() + hk(-))2

(f~ 

+

k -f

)2

64d

1 =
64d

2
f~dk - f .
K

(k supported on Lk) (By Lemma 6) (By Eq. (1)) (By 4c)
(k supported on Lk)

where the final inequality is trivial if hk(-) is positive, while if hk(-) is negative, then using (4c), hk() + hk(-)  hk()/2  (2hk() + hk(-))/4 and the result follows.

Summary We have shown that for all f  F there exists a policy   {0, . . . , m} such that Eq. (3) holds. By the definition of f~ it follows that

f¯d - f  f~d - f

K

K

(since f¯  f~)

2  + 16 d

(f~ - f )2 d

n

K

(by Eq. (3))



2 + 16 2d



+ 16 2d

(f¯ - f )2 d ,

n

K

where

we

used

the

inequalities

(a

+

b)2



2a2

+

2b2

and

 a+

b



 a

+

 b

for

reals a, b  0. The result now follows from Eq. (2) and Theorem 4 and noting that

in this proof the number of exploration policies is m + 1.

5 Proof of Fact 1
Convexity is obvious. We need to prove that g is Lipschitz on { x,  : x K}. Elementary differentiation does not suffice: for example, if d = 2 and f (x) = 2x1 and K = {(x, x) : x  [0, 1]}  R2, then f is Lipschitz on K, but g(x) = 2x is not Lipschitz. The problem is that K is not a convex body because it does not have a non-empty interior. To exploit the fact that K is a convex body, note that for any x, y in the interior of K with y - x,   0, there exists a path from x to y that moves either perpendicular to  or in the direction of . Since f is constant on hyperplanes {z : z,  = c} for any c, the Lipschitzness of f applied along the path implies that g is also Lipschitz. The extension to x, y on the boundary of K follows from the continuity of f and by passing to the limit.

9

6 Proof outline of Theorem 4

The majority of the argument follows that given by [Lattimore, 2020, Appendix A]. Let Ka  K and Sda-1  Sd-1 be finite covering sets such that

sup inf x - y 
xK yKa

and

sup inf x - y  ,
xSd-1 ySda-1

with = poly(1/n) suitably small. Let H = Sd-1 Fn, which is the set of possible sequences of functions that the adversary can choose. Let Ha = Sda-1 Fn. We assume that rather than observe ft(xt) after round t, the learner observes yt = ft(xt)+t where t is a Gaussian with zero mean and variance 2 = poly(1/n)
and truncated in [-1, 1]. This can only increase the regret, since the effect can
be modelled by restricting the class of policies to add the noise before making a decision. The value of 2 should be chosen such that the probability that a truncation occurs is O(1/n2) while should be O(poly(2/n)). Standard results show that log |Ka| and log |Sad-1| are both at most const d log(n diam(K)).

Policies and the Bayesian probability space Let P(A) be the space of

finitely supported probability measures on set A with the discrete -algebra. Given

a measure   P(H) and a policy , let P be the probability measure on (ft)nt=1 and (xt)nt=1 and (yt)nt=1 when (ft)nt=1 are sampled from  and the laws of the out-
come are determined by the interaction between  and the bandit. Expectations

with respect to P are denoted by E . The optimal action in Ka in hindsight is

x = arg minxKa

n t=1

ft(x).

Minimax duality Let a be the set of policies that plays actions in Ka almost surely. Let Rn(, (ft)nt=1) be the regret for a given policy and sequence of functions. By minimax duality,

Rn



inf
a

sup Rn(, (ft)nt=1)
(ft )nt=1 H

n



1

+

sup
 P (H)

inf
a

E

ft(xt) - ft(x )
t=1

n



2

+

sup
 P (Ha )

inf
a

E

ft(xt) - ft(x )
t=1

,

(7)

where the first inequality is trivial. The second follows from minimax duality
[Lattimore and Szepesv´ari, 2019]. The last inequality follows by choosing =
poly(1/n) suitably small and using standard information-theoretic arguments that for all (ft)nt=1  H there exists an approximation in Ha that because of the noisy losses is statistically indistinguishable from the perspective of the policy. This is where the conditions on and 2 are needed, but we omit details.

Bayesian regret For the remainder we bound Eq. (7) for any   P(Ha) and a carefully constructed policy . Abbreviate E = E . Let   Sad-1 be the random

10

element such that ft(x) = gt( x,  ) for suitably chosen (gt)nt=1 and let Z = (x , ). The next step is to bound the Bayesian regret on the right-hand side of Eq. (7). Let Pt(·) = P(·|x1, y1, . . . , xt, yt) and Et be the expectation with respect to Pt. For z  Ka × Sda-1, let
ft,z(x) = Et-1[ft(x)|Z = z] and f¯t(x) = Et-1[ft(x)] .
Note that ft,z is a ridge function. Let µt be the finitely supported measure on F with µt({ft,z}) = Pt-1(Z = z). In previous arguments Z was defined to be the optimal action, but in our setup the resulting function would no longer be a ridge function and the analysis in Section 4 would not apply. By the assumptions of the theorem and a lemma for combining exploratory distributions [Lattimore, 2020, Lemma 4], there exists a distribution t on Ka such that

f¯t(x) dt(x) - f dµt(f )  1/n +  + m

(f¯t - f )2 dµt(f ) dt(x) .

Ka

F

Ka F

The additional constant 1/n appears because t must modified to be supported on Ka. We let  = (t)nt=1. Then,

n

n

E

ft(xt) - ft(x )  E

t=1

t=1

f¯t(x) dt(x) - f dµt(f )

Ka

F

n
 1 + n + E
t=1

m

(f¯ - f )2 dµt dt .

Ka F

Next, we relate the right-hand side above to the information gain about Z. Letting It(U ; V ) be the mutual information between random elements U and V under Pt, by Pinsker's inequality,

(f¯ - f )2 dt dµt = Et-1 (Et-1[yt|xt] - Et-1[yt|Z, xt])2
FK
 Et-1 [It-1(Z; xt, yt)] .

By the chain rule for the mutual information and letting H(Z) be the entropy of random element Z,

n

E

It-1(Z; xt, yt)  H(Z)  log |Ka| + log |Sad-1|  const d log(n diam(K)) .

t=1

Therefore, by Cauchy­Schwarz, the Bayesian regret is bounded by

n

n

E ft(xt) - ft(x )  1 + n + E

t=1

t=1

mIt-1(Z; xt, yt)

 1 + n +

n

mnE

It-1(Z; xt, yt)

t=1

 1 + n + const mnd log(n diam(K)) .

11

References
S. Bubeck and R. Eldan. Exploratory distributions for convex functions. Mathematical Statistics and Learning, 1(1):73­100, 2018. 
S. Bubeck, O. Dekel, T. Koren, and Y. Peres. Bandit convex optimization: T regret in one dimension. In Proceedings of the 28th Conference on Learning Theory, pages 266­278, Paris, France, 2015. JMLR.org.
S. Bubeck, Y-T. Lee, and R. Eldan. Kernel-based methods for bandit convex optimization. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 72­85, 2017.
V. Dani, T. P. Hayes, and S. M. Kakade. Stochastic linear optimization under bandit feedback. In Proceedings of the 21st Conference on Learning Theory, pages 355­366, 2008.
A Flaxman, A Kalai, and HB McMahan. Online convex optimization in the bandit setting: Gradient descent without a gradient. In SODA'05: Proceedings of the sixteenth annual ACM-SIAM symposium on Discrete algorithms, pages 385­394, 2005.
E. Hazan and K. Levy. Bandit convex optimization: Towards tight bounds. In Advances in Neural Information Processing Systems, pages 784­792, 2014.
E. Hazan and Y. Li. An optimal algorithm for bandit convex optimization. arXiv preprint arXiv:1603.04350, 2016.
X. Hu, Prashanth L.A., A. Gy¨orgy, and Cs. Szepesv´ari. (Bandit) convex optimization with biased noisy gradient oracles. In AISTATS, pages 819­828, 2016.
S. Ito. An optimal algorithm for bandit convex optimization with strongly-convex and smooth loss. volume 108 of Proceedings of Machine Learning Research, pages 2229­2239. PMLR, 26­28 Aug 2020.
R. Kleinberg. Nearly tight bounds for the continuum-armed bandit problem. In Advances in Neural Information Processing Systems, pages 697­704. MIT Press, 2005.
T. Lattimore. Improved regret for zeroth-order adversarial bandit convex optimisation. Mathematical Statistics and Learning, 2:311­334, 2020.
T. Lattimore and Cs. Szepesv´ari. An information-theoretic approach to minimax regret in partial monitoring. In Proceedings of the 32nd Conference on Learning Theory, pages 2111­2139, Phoenix, USA, 2019. PMLR.
D. Russo and B. Van Roy. Learning to optimize via information-directed sampling. In Advances in Neural Information Processing Systems, pages 1583­1591. Curran Associates, Inc., 2014.
12

A. Saha and A. Tewari. Improved regret guarantees for online smooth convex optimization with bandit feedback. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pages 636­642, 2011.
A. Saha, N. Natarajan, P. Netrapalli, and P. Jain. Optimal regret algorithm for pseudo-1d bandit convex optimization. arXiv preprint arXiv:2102.07387, 2021.
M. J. Todd. Minimum-volume ellipsoids: Theory and algorithms. SIAM, 2016.
13

