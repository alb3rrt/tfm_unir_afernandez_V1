Controlled Update of Software Components using Concurrent Exection of Patched and Unpatched Versions
Stjepan Gros, Ivan Kovacevic´, Ivan Dujmic´, Matej Petrinovic´ Laboratory for Information Security and Privacy
Faculty of Electrical Engineering and Computing University of Zagreb Zagreb, Croatia
Contact author mail: stjepan.gros@fer.hr

arXiv:2106.01154v1 [cs.CR] 2 Jun 2021

Abstract--Software patching is a common method of removing vulnerabilities in software components to make IT systems more secure. However, there are many cases where software patching is not possible due to the critical nature of the application, especially when the vendor providing the application guarantees correct operation only in a specific configuration. In this paper, we propose a method to solve this problem. The idea is to run unpatched and patched application instances concurrently, with the unpatched one having complete control and the output of the patched one being used only for comparison, to watch for differences that are consequences of introduced bugs. To test this idea, we developed a system that allows us to run web applications in parallel and tested three web applications. The experiments have shown that the idea is promising for web applications from the technical side. Furthermore, we discuss the potential limitations of this system and the idea in general, how long two instances should run in order to be able to claim with some probability that the patched version has not introduced any new bugs, other potential use cases of the proposed system where two application instances run concurrently, and finally the potential uses of this system with different types of applications, such as SCADA systems.
Index Terms--software patching, security, reliability, updating

applied to any kind of mission-critical systems. The core idea is to run two systems in parallel, with the only difference being that one is patched and the other is not. The unpatched system is used normally, while the patched system receives the same input as the unpatched system and its output is compared to that of the unpatched system. If there is a difference between the two outputs, then the patch may have introduced an error. If there is no difference between the outputs within a certain period of time, it can be assumed with a certain probability that patching has not caused any changes in the behavior of the system and hence it can be put into production.
We present a system we developed to test this idea on web applications. It is a simplified system, but it shows the basic concepts and allows us to do some experiments and develop the idea further. To be able to send a single request to two identical components and then compare their outputs, we built a proxy that sits in front of the patched and unpatched components and controls the communication between them and the outside world while watching for possible differences in the response.

I. INTRODUCTION
Software patching is an increasingly important aspect of keeping today's computing environment secure, where the volume, complexity, and number of configurations under which software runs have increased significantly [1]. However, it is not always possible to patch all systems for a variety of reasons. A common reason is that the system in question is critical and/or is only certified for a particular configuration. Among other reasons, the software vendor may prohibit changes to the software for fear that they could cause harm. This is especially the case for mission-critical applications, such as control systems, where failures in system execution can lead to major losses and even loss of life. In particular, patching of ICS devices is usually postponed to scheduled production outages to prevent potential operational disruption of critical systems [2].
In this paper, we present an idea on how to address this challenge, i.e., how to patch systems in a safe way that can be

Besides secure patching, there are several other use cases for the system developed in this work. Due to its parallelization capabilities, it can be used in differential testing. In addition, it can be used to detect attacks, since the changes in the output do not necessarily have to be related to a bug. An attacked system that has been compromised could produce different output for the same inputs. We discuss these potential uses further in Section IV.
The paper is structured as follows. In Section II we describe the architecture and implementation of the system we used to perform the experiments. In Section III we describe experiments we did and the lessons learned. In Section IV we analyze the results and discuss what it would be necessary to do in other to support other types of systems, like components of industrial control systems. We also describe other use cases for such a system besides testing patched systems. In Section V we review related work. The paper finishes with conclusions in Section VI and list of references.

Fig. 2. Proxy architecture

Fig. 1. System Architecture

II. SYSTEM ARCHITECTURE AND IMPLEMENTATION
In this section we describe the idea on which the system is built, the architecture of the system we built, and some important details of the implementation. The results of the experiment are described in the following section.
The core idea for solving the problem of detecting behavioral changes in the system introduced by an update is to run both the unpatched and patched applications in parallel for a period of time and watch for unexpected behavioral deviations. The longer the applications run in parallel and the more different clients access them, the higher the probability that the update did not change the behavior of the software if no differences were found. Note that some differences are to be expected and should be accounted for, such as random values like tokens and cookies for web applications that use them, but otherwise there should be no differences.
The system architecture is shown in Figure 1, while Figure 3 shows the communication flow in the system. The key component is the Proxy that intercepts traffic between the Client and two software instances. One software instance is the Main application that is known to work without known bugs (except, possibly, the ones corrected by the patch) and is not patched, while the other software instance is the Patched application for which we want to confirm that it is working correctly, too. The Proxy sends requests from the Client to the Main application (Request 1) and sends adjusted copies of those requests to the Patched application (Request 2). Then, when it receives responses from both application components (Response 1 and Response 2), it passes the response from the Main application back to the Client. At the same time, it compares responses from the main application and the patched application, ignoring expected differences. If there is an unexpected difference, it signals an alarm, because a difference in behavior between the two has been found.
The proxy can be used in what is called learning mode. The goal of this mode is to find the expected differences that are ignored during testing. In this mode, two identical, i.e. unpatched, application instances that are known to work

Fig. 3. Communication sequence diagram for system shown on Figure 1
without known errors, are run simultaneously, and the proxy detects and logs differences. The detected differences can be automatically or manually converted into rules for the proxy component that specify where the differences between two applications are expected to occur.
The architecture of the proxy component is shown in Figure 2. Proxy consists of two modules: (i) Request/Response Manipulation Module, and (ii) Response Comparing Module. Mitmproxy [3] was used to implement proxy so both modules are implemented as plugins within the Mitmproxy.
The Request/Response Manipulation Module (RRMM) is a Mitmproxy script that intercepts traffic between the user and the originating application. The RRMM communicates with the response comparison module (RCM) by forwarding the intercepted responses. Any request that targets the original application is replicated and customized for the patched application container. The customizations can consist of manipulations of both the request header and the request body. The manipulations consist of mapping feature values from the original application to the values used in the patched application. The user specifies the characteristic values in the configuration file. Characteristic values are a subset of expected differences that represent random pieces of data specific to each instance of an application, such as tokens and cookies. Response scraping is built into the module to retrieve the assigned values. The order of the responses does

not have to match the order of the requests. To ensure that

the RCM receives the correct response pairs, the responses are

mapped by certain keys. The keys are inserted into the request

headers, enabling the system to associate the original request

with the replicated request. A correct response pair represents

two responses for such original and replicated requests, the

first targeting the original application and the second targeting

the patched application. Each response pair is forwarded to

the RCM.

The forwarding of the response pairs is not done in real

time. The reason for this is that the responses in a pair do1 not have the same response time, with one of them possibly having a significant delay that could block the comparison2
3
system. For this reason, the user sets the comparing rate. For4 each time interval specified in the comparing rate, the RRMM forwards response pairs that have arrived up to that time to5
6
the RCM, which compares all received response pairs.

The RCM compares the responses and logs differences. The

comparison of pairs of responses is performed at the specified

comparing rate. Differences are expected if they are part of

randomly generated data specific to each client session, such

as cookies and tokens. The RCM can be divided into two

working modes, learning mode and comparison mode.

The learning mode is based on the formatted RCM output

files. The RCM outputs the differences and the context of

the response in which the differences occur, such as the

html tag in which the token is placed. From the output

of the differences, the user can learn about the differences

and identify expected differences. Expected differences can

be detected from keywords and differences in numbers that

depend on randomization, while unexpected differences can

indicate errors and vulnerabilities. In this case, the comparing

module generates an alarm. From the alarm, one can extract1 the necessary information about the differences and check2

whether there is a bug in the code or a vulnerability.

3

Comparing mode is the inner process of the RCM that45

finds differences. Various methods are used for response6

comparison. Responses such as HTML and JavaScript can be7 compared as plain text. For plain text comparison, Diff Match8

and Patch [4] is used. Diff Match and patch is a Google library

packaged for Python. Differences are stored as tokens in diff

files.

The RCM finds differences from stored tokens. The token

represents the response part that is different in the application

instances. For each difference, there are two tokens, one for

each instance. The RCM then searches the response to find

the location of the token. If the difference token is associated

with one of the legal differences, for example in the same tag

whose attribute is included in the set of expected differences,

then the difference is expected.

JSON responses are compared as objects based on the

values of their attributes, since JSON is a text-based, human-

readable format that describes an object. With JSON, the same

object can be described in multiple ways, with differences

in, for example, the order of attributes. Moreover, an object

can contain objects that in turn contain other objects, and

so on. For this reason, the RCM iterates recursively through the object hierarchy when making a comparison. This allows the RCM to perform a deep-compare, that is, it can compare differences even at the lowest level of the object attribute tree.
In order for the RCM to know which differences are expected and which are unexpected, descriptions of the expected differences must be produced. To do this, the RCM provides a special output. From the special output, the user can extract the expected differences. An example of a special output is shown in Listing 1.
token: b0008cae11221c91b9a8f65d17c33202c819c5fa found in tag:
<script type="text/javascript"> var odoo = { csrf_token: " b0008cae11221c91b9a8f65d17c33202c819c5fao", };
</script>
Listing 1. Specialized output for extracting differences
The expected differences and the characteristic values are specified in the configuration file. The characteristic values can be extracted by examining the traffic, especially the request bodies. Web applications can send characteristic values, such as CSRF tokens, in request bodies. CSRF token is a secret, randomly generated value that the server-side applications generate for the client. When the client later makes a request, the token is validated. CSRF tokens are used to protect against CSRF attacks.
The configuration file is simple, as shown below. It consists of legal differences and characteristic values. Legal differences are indicated by a ':' character at the beginning of the line. Characteristic values are marked with a '+' sign at the beginning of the line.
:csrf_token :session_info :id :last_update :search_view :date :context +csrf token
Listing 2. Configuration file
III. EXPERIMENTS
Experiments were conducted on three applications: eShopOnWeb [5], Odoo [6] and OpenProject [7]. The goal of the experiments was to test the idea over real systems, and to detect any details that we were not aware of, such as some crucial show stoppers. We also wanted to see if the number of details that we need to consider is constant for each new application, or solved after a small number of applications tested.
The first step towards parallelization is to run two instances of the unmodified application. Docker containers [8] were chosen for this purpose. A container is a standard software unit that packages the code and all its dependencies in such a way that the application can be quickly and reliably transferred from one computing environment to another. Running

two separate Docker containers is equivalent to running two instances of an application.
A. eShopOnWeb
EShopOnWeb is a sample ASP.NET Core web shop application powered by Microsoft. The main purpose of the application is to demonstrate patterns and principles of web development [5].
The application represents a simple web shop application. Its set of functionalities consists of user login, adding items to cart, purchasing items, editing user profile, etc. HTTP traffic of the application consists of GET and POST requests and responses with HTML body form. JSON traffic and other types of body forms do not occur. Although the set of functionalities is limited, it has proven to be a huge source of information.
The first part of the response that we identified as an expected difference is the Request Verification Token. For security reasons, the application generates request verification tokens in GET and POST responses, which the client must return in the next GET or POST request. Request Verification Tokens are random values and each application instance generates a different value in each request. Therefore, patched and unpatched applications return different values for request verification tokens. This means that we need to extract the token's value from the patched application, and when the client makes the next request for the unpatched application, we need to replace the token's value in the request copied for the patched application with its characteristic token that we remembered earlier. Cookies are also specific to each application. Each instance of the application generates its own cookies. Cookies are sent via headers, specifically Set-Cookie header field.
POST requests usually have content in the request body or in the request query. When encoding the content, percent encoding [9] is used.
Although the complexity of application traffic is mostly low, there is no JSON serialized data and no multipart form data. Nevertheless, it is complex enough to recognize the patterns for application parallelization. The proxy configuration file for eShopOnWeb is shown in Listing 3. In addition to the RequestVerificationToken mentioned earlier, another legal difference is the item identifier. EShopOnWeb is a web shop application based on actions with items. Items represent assets that can be purchased in the shop. Different instances of the application use different databases. Consequently, the same items can have different identifiers in the database. When an item is added to the cart, the item identifier in the request body is forwarded and a different identifier is forwarded for the other instance.
:RequestVerificationToken :Items[ +RequestVerificationToken +Items[
Listing 3. eShopOnWeb configuration file

B. Odoo
Odoo is a business management software that includes CRM, e-commerce, billing, accounting, manufacturing, warehouse, project management, and inventory management [6].
Odoo is open source and has Docker support. It is more complex and has a greater number of functionalities than eShopOnWeb. The traffic is different from that generated in eShopOnWeb. Pure HTML responses are in the minority. JSON is most commonly used Content-Type in requests and responses.
JSON objects are easy to read and copy, so parallelization is not a problem. The proxy simply copies the content and forwards it to the application containers. The problem occurs when comparing responses. The responses can contain a lot of information, sometimes a list of JSON objects that have other JSON objects as attributes. In this case, the order of the attributes may not be the same in both responses. When the responses are compared as text using a simple diff algorithm, a difference is detected and a false positive alarm is raised. For this reason, JSON responses are compared as objects, on an individual attribute basis. This way, only real differences (e.g. date, ID, update, etc.) are detected and logged.
Another expected difference that occurs in eShopOnWeb application traffic relates to HTML JSON attributes. It is possible for two responses to have the same HTML code, but be marked as different due to a different order of attributes. We solved this problem by extracting all the characters from the tags that are different in the responses and storing them in two character lists. Then we excluded the non-alphabetic characters from the character lists and sorted them in alphabetical order. If the sorted character lists are the same, the responses are treated as equal. This approach assumes that there are no permutations of parameter names, since different attributes such as "name" and "eman" will give the same result when sorted in alphabetical order. The probability of this being the case is negligible.
The Odoo proxy configuration file is shown in listing 4. Session info contains data about the user session. Each user session has specific values are different for each user session. Write date, create date, date, and last update are time-dependent parameters. Requests sent by the client to the unpatched version and the request created by the proxy sent to the patched version do not arrive at their destination at the same time. For this reason, the time-dependent parameters are different for different instances of the application running in parallel. The Id parameter represents a randomly generated string that varies from instance to instance. The Id parameter appears in html tags as an attribute.
When all of these factors are incorporated into the proxy, parallelization is achieved. Moreover, the logs of differences show that only legal differences are displayed when we use two instances of the same version of the application. This is the expected and desirable result.
: csrf_token : session_info : write_date

: create_date : id : last_update : search_view : date + csrf_token
Listing 4. Odoo configuration file
C. OpenProject
OpenProject is a project management software. Functionalities include: project planning and scheduling, product roadmap, release planning, task management, release collaboration, Agile, Scrum, time tracking, cost reporting, budgeting, bug tracking, wikis, forums, meeting agendas and meeting minutes. OpenProject is the leading open source project management software [7].
The Docker image is publicly available, so setting up two running instances of OpenProject in parallel is not a problem. The web applications mentioned above, eShopOnWeb and Odoo, did not use multipart form as a content type, so this application comes with a new implementation challenge. Multipart form is a content type where the request body is specifically formatted as a series of "parts" separated by MIME (Multipurpose Internet Mail Extensions) boundaries. The multipart form allows one or more different sets of data to be combined into a single body, such as a text file for upload and string parameters related to the uploaded file.
To achieve parallelization of multipart requests, a multipart form encoder and decoder implementation is required. Open source implementations of this type are used for this purpose. The goal of parallelizing multipart form requests is successfully achieved.
The OpenProject proxy configuration file is shown in Listing 5. OpenProject uses the authencity token, which is the Ruby alternative for the CSRF token [10]. Its purpose is the same. CreatedAt and updatedAt are timestamp parameters that differ for application instances because of the latency introduced by the proxy. When the proxy intercepts a request, it takes time to replicate and adjust the new request. The original application and the patched application do not receive their requests at exactly the same time. Styles is a parameter that specifies the static CSS files to be used in the application. CSS file names sometimes contain a randomly generated string that is different for each application instance. Nonce is an arbitrary number that can be used only once in the communication [11]. For each application instance, nonce is different because it is an arbitrary number, so it is an expected difference.
:token :nonce :uuid :key :styles :timestamp :createdAt :updatedAt +token +nonce
Listing 5. OpenProject configuration file

IV. DISCUSSION
In this section we discuss several topics. First, in section IV-A, we discuss the issue of how long one should run the application instances in parallel to be able to assert with certain probability that the patched application instance is working correctly. In section IV-B we discusses possible problems and restrictions of the proposed patching approach. Then, in section IV-C we discusses additional use cases of the system we prototyped besides patch management. Finally, since our future work related to the presented method is related to its application in industrial control systems, and specifically for updates of SCADA systems, we give a brief overview of our plans in section IV-D.
A. Reliability
For anyone who wants to use the method proposed in this paper, the question is how long patched and unpatched application instances should run concurrently before it can be claimed with some probability that their behavior is the same. This is a very important question, since running two instances for too short a period can leave undetected and potentially disastrous bugs. On the other hand, running them for too long wastes valuable resources, so this time should be minimized. This question is left for future work, but we will provide a sketch of a possible answer.
Suppose that when we started running the patched and unpatched applications concurrently, we found N differences stemming from bugs in the patch, that we resolved. Further, suppose that the time between starting the concurrent execution and finding the i-th difference is Ti time units, with i being i = 1 . . . N . To illustrate, when we first started the comparison of the patched and unpatched application instances, it took T1 time units to find the first difference. After that, we took some time to analyze and correct the detected difference, after which we started concurrent execution again. Measured from this point in time, the next difference was detected after T2 time units, and so on. Now suppose that after all N differences have been found and resolved, we need to decide whether to stop concurrent execution or wait for the N +1-th difference to occur. To answer this question, we need to know the probability of executing two instances without finding any differences for a given time Trequired. In other words, we are interested in finding out the probability that no differences are found in the interval Trequired. For practical purposes, the person estimating the reliability of a system will set Trequired to some value, such as the time when the next patch is expected.
To calculate the probability of a difference NOT occurring in a given time we'll assume that the time between starting concurrent execution and difference occurring (Ti) follows an exponential distribution, T  E() where  > 0, with probability density function (PDF) fT (t; ) = e- t, t  0. This assumption is taken from Driel et. al [12]. Furthermore, we equate differences in applications with bugs, i.e. we assume that differences result from bugs introduced by patch.

We estimate the parameter  from the times Ti finding uniformly minimum-variance unbiased estimator (UMVUE), using method of moments estimator and point estimation theory (see [13], Chapter 10.4, page 337), so that we get the estimator with minimum error. So, the UMVUE for  is

^ =

N -1

N i=1

Ti

,

(1)

with standard error (se) defined as standard deviation of ^, i.e.

se(^) =

2

(2)

N -2

The question is what is the probability that no new differences occur in the given time Trequired. More specifically, we are looking for an estimate of the time of occurrence of a new difference (T ) after the detection has run for a given time. Mathematically, this event can be written as {T > Trequired}, and we calculate the probability for this event. The calculation is given in the following expression:

P (T > Trequired) = 1 - P (T  Trequired)

Trequired

=1-

e- tdt

(3)

0
= e- Trequired

So, to decide whether to stop with concurrent execution, the user of this system should do the following:
1) decide on the value of Trequired, 2) calculate  and standard deviation based on Ti, 3) using expression (3) calculate probability, 4) make a decision based on probability and standard
deviation of .

B. Potential Issues & Restrictions
The proposed method of patching certainly has some limitations and is definitely not applicable to every possible situation that might arise in the real world. Moreover, there are some other problems that we did not encounter in our experiments, but which are still highly likely to occur and need to be taken into account.
First, note that a patch could introduce a new behavior in the patched application that is different from the behavior of the unpatched application and fixes a bug. In this case, the proxy will detect a difference, but should not signal an error. A more interesting question is what to do in such a case. Should the proxy pass on the response of the unpatched application which has an error - or that of the patched application, for which it is not yet certain whether it functions well? The answer to this question is probably context-dependent and should be decided by the users of the system, who know the patched application well.
The next problem might arise when the patch adds a new functionality. In this case, running the patched and unpatched instances simultaneously will not test the new functionality.

In this case, of course, the new functionality needs to be thoroughly tested in the development and deployment phases, while the method presented in this paper will only help with functionality present in both application instances. In extreme cases, someone could patch the application to the point where its behavior is drastically different from the unpatched application. For example, the states that the application goes through might change, likely resulting in changes to the application's output. It seems reasonable to assume that the approach presented in this paper works best for patches that contain only bug fixes, and the more behavioral changes are introduced into the application, the less likely it is that this approach is feasible. This issue requires further and much deeper research.
The characteristics of the applications we used for the experiments must also be considered. First, they use the TCP protocol, which means that communication is reliable and messages are not reordered, duplicated or lost. Furthermore, while the applications implement complex state machines, they do so on top of a simple request-response protocol (i.e. HTTP) that runs in lockstep for both the patched and unpatched applications. If this were to change, there would be consequences to deal with:
· using unreliable protocol, like UDP, would mean that the proxy would have to know that the messages could be lost, and in that case it should wait for retransmissions. Yet, the the difference could arise because the response wasn't sent at all, rather than the message being lost. This definitely adds some complexity to the implementation of the proxy.
· Using an application protocol that is more complex than HTTP would also increase the complexity of the proxy, to the point that the proxy would have to implement a part of, or even the entirety of the protocol, to be able to monitor for differences between the applications.
· A special case of the previous point would be a protocol that doesn't specify rigidly the order of messages, i.e. the one that allows messages to be interlaced. In that case, it might happen that the patched application sends message A, while the unpatched sends message B. Later, at some point, the patched application sends message B, while the unpatched sends message A. The two are equivalent, but handling such a situation is obviously not easy.
The conclusion is that the applicability of the proposed method depends heavily on the protocols used. Further research is needed to determine all the exact limitations and problems that may arise.
Finally, as we have seen in our experiments, randomness is a frequent source of differences. We were only dealing with one aspect of it, such as cookies, i.e. well-defined data whose definition allowed us to deal with them. But randomness can occur for a variety of reasons, including operating system scheduling, application dependence on sources other than users, inaccurate timers, deferred interrupts, etc. Dealing with them would also require further research on more complex

applications than just the ones we use. Due to all the issues discussed, changes in the application
itself may be required to fully utilize this method. The goal of the changes would be to facilitate the creation of proxies that monitor updated applications. Further research is needed to understand what these changes are.
C. Other potential use cases
While we have intended this system primarily to enable patching of applications that are otherwise difficult to patch due to various restrictions, we have also envisioned two additional use cases. Both of these additional use cases expect a well-defined interface between an application and the outside world. For example, the interface could be the PSD2 OpenBanking API, or a firewall that blocks or passes packets through.
The first use case of the proposed system is related to differential testing for the purpose of certification. Differential testing compares two different implementations of the same application logic to find bugs in the code. The idea here is that running two applications side-by-side would allow easier testing of the correctness of their implementation. This could simplify the certification process. Furthermore, if one application is a reference implementation of a functionality, other applications could be compared to it through differential testing.
The second use case is concerned with protecting against attacks on an application/system and other potential misuses of the application/system in question. For example, an attacker could compromise the firewall by sending specially crafted packets, or in the case of an API, send a request that could result in the application implementing the API being compromised. The idea is that applications and systems have bugs, but if they are implemented by different teams, there is little chance that they will have bugs in the same or similar parts of the application. Therefore, running two applications concurrently and comparing their outputs could be used to detect if an application/system has been compromised. Nonetheless, the causes of differences could be bugs rather than attacks. In any case, differences in responses of two applications that should behave the same way are a reason to investigate what happened.
D. Application to other types of systems
In our experiments, we only consider use cases involving web applications. However, there are many other types of systems, such as desktop applications, mobile apps, and industrial systems. These systems differ in their architecture and require different configurations of proxies and different types of response comparison. Simple desktop applications, for example, handle input and output through graphical user interfaces. In this section, a possible configuration is proposed for a specific family of systems, namely Supervisory Control And Data Acquisition (SCADA) systems.
Compared to IT systems, Industrial Control Systems (ICSs) are characterized by simpler and more deterministic network

traffic containing smaller well-ordered packets [14]. This should, at least in theory, make both the space of potential inputs and the state space smaller than in IT systems, leading to potentially better performance of our approach on ICSs.
To apply the method presented in this paper to patching of SCADA systems, some additions are needed. As before, patched and unpatched systems - in this case SCADAs - should run concurrently. The unpatched version produces outputs that control physical processes, while equivalent output of the patched version is monitored. We assume that the unpatched and patched SCADA systems run on separate, otherwise completely identical machines. Also, each of them has a human-machine interface (HMI). In this case, two things need to be compared, first, that the HMIs produce the same output, and second, that the same control commands are sent to the same devices on the communication channel used for data acquisition and control.
To illustrate this system, we'll use Figure 4. There are two communication channels that each SCADA system uses. One communication channel exists between the SCADA systems and the industrial control system (ICS) that contains other components and PLCs. The other communication channel is between the human user and the SCADA system. Through this communication channel, the user receives information about the current state of the system and gives commands on how to change the state of the system. Since there are now two communication channels, this means that two proxies are required. The control proxy compares control requests from the unpatched and patched SCADA systems. The request from the unpatched version is propagated to the ICS, while the request from the patched version is used only to detect differences. In Figure4, the requests and data sent to the patched and unpatched systems are represented by blue lines, while the responses and control signals from these systems are represented by orange lines. Dotted lines represent responses that are only used only by the proxies and are not propagated to their final destinations.
The other communication channel is towards the users, through which they control the physical system via the humanmachine interface. The graphical user interface proxy (GUI proxy) records the user's actions, replicates them to the patched instance of the SCADA system, and compares the information returned by these systems to the user. This includes cursor movements, mouse clicks, and any actions on the front-end component. It should be noted that the information being compared will also be graphical, in which case the GUI proxy should be able to properly analyze graphical data. The user is presented with the response of the unpatched version, while the response of the patched version is compared with the unpatched version to detect potential differences.
V. RELATED WORK
When searching for related work, we identified several similar systems, but none of them was identical to ours, or had the same purpose. Some related areas include:
1) differential testing,

Fig. 4. Study of the use case with SCADA systems
2) asynchronous patching, 3) differential static analysis and symbolic execution, 4) differential fuzzing and hybrid techniques, 5) and regression testing.
The idea of testing software by comparing it to functionally equivalent codes was first proposed by McKeeman [15] in 1998. This method is a form of random testing called differential testing. Differential testing requires that two or more comparable systems are available to the tester. These systems are presented with an exhaustive set of mechanically generated test cases. If the results differ or one of the systems goes through an infinite loop or crashes, the tester has a candidate for a bug-exposing test [15].
There is a connection between our method and differential testing. The idea of testing comparable systems in differential testing can be compared to testing a patched and an unpatched application. Patching software creates two application instances that may differ in certain areas, which makes them comparable systems. The main difference between our approach and differential testing is that the comparable systems mentioned in [15] are different implementations that serve the same purpose, while in this paper we propose the comparison of different versions of the same implementation. Moreover, while the motivation behind differential testing is to find bugs in different implementations of the same idea, the motivation of this paper is to study the consequences of application updates by running different versions of the same application in parallel.
The concept of patching parallel software instances is not new. For example, the system described in [16] and [17] performs patching over a copy of a target virtual machine (VM) in a special maintenance environment, while another copy of that VM is accessible to operational clients. After the patches are successfully applied, the patched copy of the VM is made available to clients. However, unlike our method and system, their system does not analyze the differences between the patched and unpatched versions of the VM and does not automatically verify that the patches have not affected important functionality.
Approaches such as [18] and [19] use static analysis and/or symbolic execution to determine the differences between multiple software instances. These methods require access to

the source codes or binaries of the instances and often find differences that do not affect the functionality of the software. In contrast to these approaches, we do not require access to the code of the instances and are not interested in differences that do not affect the functionality of the software in question.
Other approaches use differential fuzzing [20], [21] and combinations of fuzzing with symbolic execution [22]­[26] to find test cases for which software instances that are supposed to implement the same specification produce different results. These test cases are often targeted towards simple communication sequences, such as establishing a TLS session, and aim to find edge cases that affect security. In contrast to this group of approaches, we focus on testing software instances over longer periods of time, using realistic user sessions and load rather than generated tests, to determine if a patch has introduced new bugs that affect practical functionality. Unlike [22], [23], [25], [26], we do not require access to the code of the software instances.
Many of the approaches mentioned earlier, such as [19], [25], [26], support test generation for regression testing. In regression testing, the same collection of tests is run on the old and new versions of a software to detect newly introduced bugs. We are similar to some extent to regression testing, but the main difference between regression testing and our approach is that we test with real requests rather than predefined or generated tests.
VI. CONCLUSIONS
In this paper, we have proposed a system for controlling updates of software components. We have designed a system that can monitor two web application instances, one representing the unpatched version of the application and the other representing the patched version of the application.
With this approach, the user can extract the expected differences in the behavior of two application instances. If the applications have logs of visited URLs, one can find the differences between the outputs of the two instances and evaluate the reliability of the patched version of the application. The system was tested on three web applications, with the expected differences determined experimentally.
The main challenge in developing the system was to enable meaningful parallel use of the application instances. Different web applications involve different forms of communication, such as different content types and encodings, so it is a major challenge to implement a general parallelization system.
As future work, the system can be integrated into an automated testing framework so that the reliability assessment process can be faster. Also, the system could allow the user to change the version of the application they are interacting with. In this work, we always interacted with the unpatched version of the application through the web client, but it could be useful to change the version of the application in the user session.
REFERENCES
[1] J. Dadzie, "Understanding software patching," Queue, vol. 3, no. 2, pp. 24­30, 2005.

[2] B. Chen, J. Song, P. Xu, X. Hu, and Z. M. Jiang, "An automated approach to estimating code coverage measures via execution logs," in Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering, 2018, pp. 305­316.
[3] "mitmproxy ­ an interactive HTTPS proxy," Mitmproxy Project, 2020, accessed: May 9th, 2020. [Online]. Available: https://mitmproxy.org/
[4] "diff-match-patch 20200713," Google. [Online]. Available: https: //pypi.org/project/diff-match-patch/
[5] "Microsoft eShopOnWeb ASP.NET Core Reference Application," Microsoft, 2020, accesed: May 5th, 2020. [Online]. Available: https://github.com/dotnet-architecture/eShopOnWeb
[6] "Odoo: Open Source ERP and CRM," Odoo, 2020, accessed: May 9th, 2020. [Online]. Available: https://www.odoo.com/
[7] "OpenProject," OpenProject, 2020, accesed: May 11th, 2020. [Online]. Available: https://github.com/opf/openproject
[8] "Docker," Docker, Inc. [Online]. Available: https://www.docker.com/ [9] "Percent encoding." [Online]. Available: https://en.wikipedia.org/wiki/
Percent-encoding [10] A. Chaudhuri and J. S. Foster, "Symbolic security analysis of ruby-on-
rails web applications," in Proceedings of the 17th ACM conference on Computer and communications security, 2010, pp. 585­594. [11] P. Rogaway, "Nonce-based symmetric encryption," in International workshop on fast software encryption, 2004, p. 348. [12] M. T. Willem Dirk van Driel, Jan Willem Bikker, "Software reliability for agile testing," May 14 2020. [13] M. E. Lee J. Bain, "Introduction to probability and mathematical statistics," 2000. [14] B. Galloway and G. P. Hancke, "Introduction to industrial control networks," IEEE Communications surveys & tutorials, vol. 15, no. 2, pp. 860­880, 2012. [15] W. M. McKeeman, "Differential testing for software," Digital Technical Journal, vol. 10, no. 1, pp. 100­107, 1998. [16] C. Mcneill, "Virtual machine asynchronous patch management," Feb. 11 2014, uS Patent 8,650,556. [17] ----, "Virtual machine asynchronous patch management," Mar. 8 2016, uS Patent 9,280,374. [18] V. Srivastava, M. D. Bond, K. S. McKinley, and V. Shmatikov, "A security policy oracle: Detecting security holes using multiple api implementations," ACM SIGPLAN Notices, vol. 46, no. 6, pp. 343­354, 2011.

[19] H. Palikareva, T. Kuchta, and C. Cadar, "Shadow of a doubt: testing for divergences between software versions," in Proceedings of the 38th International Conference on Software Engineering, 2016, pp. 1181­ 1192.
[20] A. Walz and A. Sikora, "Exploiting dissent: Towards fuzzing-based differential black-box testing of tls implementations," IEEE Transactions on Dependable and Secure Computing, vol. 17, no. 2, pp. 278­291, 2017.
[21] G. Argyros, I. Stais, S. Jana, A. D. Keromytis, and A. Kiayias, "Sfadiff: Automated evasion attacks and fingerprinting using black-box differential automata learning," in Proceedings of the 2016 ACM SIGSAC conference on computer and communications security, 2016, pp. 1690­ 1701.
[22] D. Brumley, J. Caballero, Z. Liang, J. Newsome, and D. Song, "Towards automatic discovery of deviations in binary implementations with applications to error detection and fingerprint generation." in USENIX Security Symposium, 2007, p. 15.
[23] C. Cadar, D. Dunbar, D. R. Engler et al., "Klee: unassisted and automatic generation of high-coverage tests for complex systems programs." in OSDI, vol. 8, 2008, pp. 209­224.
[24] T. Petsios, A. Tang, S. Stolfo, A. D. Keromytis, and S. Jana, "Nezha: Efficient domain-independent differential testing," in 2017 IEEE Symposium on security and privacy (SP). IEEE, 2017, pp. 615­632.
[25] Y. Noller, C. S. Pasareanu, M. Bo¨hme, Y. Sun, H. L. Nguyen, and L. Grunske, "Hydiff: Hybrid differential software analysis," in 2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE). IEEE, 2020, pp. 1273­1285.
[26] W. Jin, A. Orso, and T. Xie, "Automated behavioral regression testing," in 2010 Third international conference on software testing, verification and validation. IEEE, 2010, pp. 137­146.

