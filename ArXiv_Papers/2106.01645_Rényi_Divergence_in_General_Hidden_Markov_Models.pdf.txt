arXiv:2106.01645v1 [cs.IT] 3 Jun 2021

1
Re´nyi Divergence in General Hidden Markov Models
Cheng-Der Fuh Fanhai International School of Finance, Fudan University
Su-Chi Fuh Department of Computer Science and Information Engineering
National Taipei University of Technology Yuan-Chen Liu
Department of Computer Science, National Taipei University of Education Chuan-Ju Wang
Research Center for Information Technology Innovation, Academia Sinica
Abstract In this paper, we examine the existence of the Re´nyi divergence between two time invariant general hidden Markov models with arbitrary positive initial distributions. By making use of a Markov chain representation of the probability distribution for the general hidden Markov model and eigenvalue for the associated Markovian operator, we obtain, under some regularity conditions, convergence of the Re´nyi divergence. By using this device, we also characterize the Re´nyi divergence, and obtain the Kullback­ Leibler divergence as   1 of the Re´nyi divergence. Several examples, including the classical finite state hidden Markov models, Markov switching models, and recurrent neural networks, are given for illustration. Moreover, we develop a non-Monte Carlo method that computes the Re´nyi divergence of two-state Markov switching models via the underlying invariant probability measure, which is characterized by the Fredholm integral equation.
Key words and phrases. Fredholm integration equation, Kullback­Leibler divergence, Markov switching models, recurrent neural network.

2

I. INTRODUCTION

Motivated by study of the information divergence in hidden Markov models (HMM), Markov switching models, and recurrent neural networks, we here investigate Re´nyi divergences in general hidden Markov models. A general hidden Markov model is, loosely speaking, a sequence Y = {Yn, n  0} of random variables obtained in the following way. First, a realization of a finite state Markov chain X = {Xn, n  0} is created. This chain is sometimes called the regime and is not observed. Then, conditioned on X, the Yvariables are generated. Usually, the dependency of Yn on X is more or less local, as when Yn = g(Xn, n) or Yn = g(Xn, Yn-1, n) for some function g and random sequence {n}, independent of X. Yn itself is generally not Markov and may in fact have a complicated dependency structure.
HMMs have been studied extensively, and have extraordinary applications in fields as varied as speech recognition, cf. Rabiner and Juang (1993), Rabiner (1989); handwritten recognition, cf. Hu et al. (1996); Kunda et al. (1989); human activity recognition, cf. Yamato et al. (1992); target detection and tracking, cf. Blanding et al. (2009); Tugac¸ and Efe (2010); Vasuhi and Vaidehi (2014); computational molecular biology and bioinformatics, including DNA and protein modeling, cf. Churchill (1989); modeling, rapid detection, and tracking of malicious activity of terrorist groups, cf. Raghavan et al. (2013), Raghavan et al. (2014); and others, with just a small sample of references given for each application. A comprehensive survey of HMM research and applications can be found in Ephraim and Merhav (2002), Cappe et al. (2005), and Zucchini and MacDonald (2009), including an extensive bibliography.
A natural extension of the celebrated HMM is the following Markov swiching models. We start with a simple real-valued first-order autoregression around one of two constants µ1 or µ2:

Yn = µXn + Yn-1 + n,

(1.1)

where n  N (0, 2), || < 1, and {Xn, n  0} is a 2-state Markov chain. When  = 0, (1.1) reduces to the classical Gaussian HMM.

Another interesting example is the recurrent neural network (RNN) in machine learning. Note that the
RNN can take as input a variable-length sequence y = (y1, · · · , yn) by recursively processing each symbol while maintaining its internal hidden state h. At each time step n, the RNN reads the symbol Yn  Rq and updates its hidden state hn  Rp by

hn = f(Yn, hn-1),

(1.2)

3
where f is a deterministic non-linear transition function, and  is the parameter of f. The transition function f can be implemented with gated activation functions such as long short-term memory (LSTM), cf. Hochreiter and Schmidhuber (1997), or gated recurrent unit (GRU), cf. Cho et al. (2014). Although the hidden unit hn is a general state random variable, a transformation in Section IV-B shows that the RNN in (1.2) can be formulated as a general HMM.
The Re´nyi divergence rate, cf. Re´nyi et al. (1961), and the Kullback­Leibler divergence in particular, have played a significant role in certain hypothesis-testing questions, cf. Koopmans (1960), Nemetz (1974). Furthermore, the Re´nyi entropy and the Re´nyi entropy rate have revealed several operational characterizations in the problem of fixed-length source coding, cf. Csiszar (1995), Chen and Alajaji (2001); unsupervised learning, cf. Jenssen et al. (2003); variable-length source coding, cf. Blumer and McEliece (1988), Campbell (1965), Jelinek (1968), and Rached et al. (2001); error exponent calculations, cf. Erez and Zamir (2001); policy optimization in reinforcement learning, cf. Metelli et al. (2018), Papini et al. (2019), and other areas such as Arikan (1996), Bassat and Raviv (1978), and Pronzato et al. (1997).
It is known that the root of the Kullback­Leibler divergence and the Re´nyi divergence is the celebrated Shannon entropy. The question of computing the Shannon entropy (or, simply, entropy) of a HMM was studied in an early paper by Blackwell (1959), in which the analysis suggests the intrinsic complexity of expressing the HMM entropy as a function of the process parameters. The author also presents an expression of the entropy in terms of a measure Q, which solves an integral equation dependent on the parameters of the process. In general, the measure is hard to extract from the equation in any explicit way. Fuh and Mei (2015) provide a numerical method to approximate the invariant measure and the Kullback­Leibler divergence for a two-state HMM. The problem of determining the residual noise of the best filter for a HMM was studied in Khasminskii and Zeitouni (1996), Ordentlich and Weissman (2004), Ordentlich and Weissman (2006), and Jacquet et al. (2008) investigate the asymptotic estimates of the HMM entropy rate. Furthermore, Zuk et al. (2005) present formulas for higher-order coefficients of the Taylor expansion in the symmetric case. Han and Marcus (2006a) and Han and Marcus (2006b) characterize the analyticity of the HMM entropy rate, and obtain a broad generalization of the results of Zuk et al. (2005).
For an explicit computation of the Re´nyi entropy of HMMs over finite alphabets in both finite-length and asymptotic regimes, Wu et al. (2017) discuss some convergence properties with no explicit formulas. For Shannon entropy the problem has been found hard and solvable only for specific cases, being related to an

4
intractable task in random matrix products--finding top Lapunov exponents, cf. Jacquet et al. (2008). In the case of finite state Markov chains, Rached et al. (2001) apply substochastic matrices in the asymptotic regime powers, which can be approximated by spectral analysis, to yield formulas on entropy rates.
Although there are some interesting papers on computing the Re´nyi divergence in a special HMM, systematic study for a general HMM is still lacking. To fill this gap, we investigate the Re´nyi divergence for a general HMM in this paper. We make three contributions. First, we note that a major difficulty for analyzing the Re´nyi divergence in general HMMs is that the joint probability can be expressed only in summation form; see equations (2.3) and (2.4) in Section II for instance. The constribution in this paper is that we provide a device which represents the joint probability as the L1-norm of a product of random matrices and treat it as a Markov chain in an enlarged state space. This representation enables us to apply results of the strong law of large numbers, and spectral theory for the associated Markovian operator of Markov random walks, to yield an explicit characterization of the Re´nyi divergence, and hence the Kullback­Leibler divergence. Second, our formulation of the HMM in a general sense covers several interesting examples, including finite state HMMs, Markov switching models, and RNNs. Third, we develop a non-Monte Carlo method that computes the Re´nyi divergence of a two-state Markov switching model via the underlying invariant probability measure, which is characterized by the Fredholm integral equation. For this purpose, we also provide an approximated Re´nyi divergence. Our numerical study shows that this approximated Re´nyi divergence is reasonably accurate in some simple cases.
The remainder of this paper is organized as follows. In Section II, we define the HMM as a Markov chain in a Markovian random environment, and represent the probability as the L1-norm of a product of Markovian random matrices. Then, we give a brief summary of eigenvalue and eigenfunction for Markovian operators. In Section III, we study the limiting behavior of the probability and characterize the Re´nyi divergence. The Kullback­Leibler divergence is defined and can be regarded as the limit of   1 in the Re´nyi divergence. In Section IV, we consider a few examples, including finite state Markov switching models and RNNs, which are commonly used in machine learning. In Section V, we give a numerical computation of the Re´nyi divergence by applying the Fredholm integral equation for a two-state Markov switching model. Section VI concludes.

5

II. HIDDEN MARKOV MODELS
A. Hidden Markov Models
In this section, we first provide a probability framework for a general HMM under which it can be regarded as a Markov chain in an enlarged state space. That is, there are two Markov chains associated with the general HMM to be described as follows. First, a general HMM is defined as a parameterized Markov chain in a Markovian random environment with the underlying environmental Markov chain viewed as missing data. Specifically, let X = {Xn, n  0} be a Markov chain on a finite state space X = {1, · · · , d}, with transition probability pij = P {X1 = j|X0 = i} for i, j = 1, · · · , d, and stationary probability j. Suppose that a random sequence {Yn} n=0, taking values in Rq, is adjoined to the chain such that {(Xn, Yn), n  0} is a Markov chain on X × Rq satisfying P {X1  A|X0 = i, Y0 = y} = P {X1  A|X0 = i} for A  B(X ), the -algebra of X . Conditioning on the full X sequence, Yn is a Markov chain with probability

P {Yn+1  B|X0, X1, · · · ; Y0, Y1, · · · , Yn} = P {Yn+1  B|Xn+1, Yn} a.s.

(2.1)

for each n and B  B(Rq), the Borel -algebra of Rq. Note that in (2.1) the conditional probability of Yn+1 depends on Xn+1 and Yn only. Furthermore, we assume the existence of the conditional probability density f (Yk|Xk, Yk-1) of Yk given Xk and Yk-1 with respect to a -finite measure L on Rq such that

P {X1  A, Y1  B|X0 = i, Y0 = y0} =

pijf (y|j, y0)L(dy).

jA yB

(2.2)

We also assume that the Markov chain {(Xn, Yn), n  0} has a stationary probability with probability

density function jf (·|j) with respect to L. Now we give a formal definition as follows.

Definition II.1. {Yn, n  0} is called a general hidden Markov model if there is a Markov chain {Xn, n  0} such that the process {(Xn, Yn), n  0} is a Markov chain satisfying (2.1). A non-invertible function g(Yn) of Yn is also called a general hidden Markov model.

Note that this HMM setting is defined in a general sense, which includes several interesting examples

of Markov-switching Gaussian autoregression, cf. Hamilton (1989), and RNNs in machine learning, cf.

Goodfellow et al. (2016). When Yn are conditionally independent given X, denote Sn =

n t=1

Yt.

Then

the Markov chain {(Xn, Sn), n  0} is called a Markov additive process, cf. Ney and Nummelin (1987),

and {Yn, n  1} is the celebrated HMM considered in engineering literature.

6

Next we follow a similar idea in Fuh (2004b) and Fuh and Tartakovsky (2019), to have a Markov chain representation of the probability. Note that the joint probability of the general HMM {Yn, n  0} is

P {Y0  B0, Y1  B1, · · · , Yn  Bn}

(2.3)

=

···

pn(y1, · · · , yn)L(dyn) · · · L(dy1),

y1B1

yn Bn

where

d

d

n

pn(y0, y1, · · · , yn) =

···

x0 f (y0|x0) pxk-1xk f (yk|xk, yk-1),

x0=1 xn=1

k=1

(2.4)

where  = (1, · · · , d)t is an initial distribution of {Xn, n  0}, which is positive P -a.s. Here t denotes

the transpose of the underlying vector in Rd

For a given column vector a = (a1, · · · , ad)t  Rd, define the L1-norm of a as a =

d k=1

|ak |.

The

probability (2.4) can be represented as

pn(y0, y1, · · · , yn) = Mn · · · M1M0 ,

(2.5)

where y0 is given from f (·|x0) and





f (y0|x0 = 1)

0

···

0







M0

=

 





0 ...

f (y0|x0 = 2) · · ·

0

...

0 ...



 

,





(2.6)





0

···

0 f (y0|x0 = d)





p11f (yk|xk = 1, yk-1) · · · pd1f (yk|xk = 1, yk-1)



Mk

=

 

...

...

...

 , 





p1df (yk|xk = d, yk-1) · · · pddf (yk|xk = d, yk-1)

(2.7)

for k = 1, · · · , n.

Note that, for k = 1, · · · , n, the quantity pijf (Yk|Xk = j, Yk-1) in (2.7) represents Xk-1 = i and Xk = j, and Yk is a Markov chain with transition probability density f (yk|xk = j, yk-1) for given X. By definition (2.1), {(Xn, Yn), n  0} is a Markov chain, which implies that Mk is a sequence of Markovian random matrices. Therefore, by representation (2.5), pn(Y0, Y1, · · · , Yn) is the L1-norm of a product of

Markovian random matrices. Furthermore, let

Tn = Mn · · · M1M0.

(2.8)

7

Denote P (Rd) as the projection space on Rd, Gl(d, R) as the space of d × d matrices and Sd-1 as the

d-dimensional sphere. For u¯  P (Rd), M  Gl(d, R), let M · u = M u, and  = () = (1, · · · , d)t 

Sd-1, the unit sphere with respect to the L1-norm · in Rd; we have

log Tn

= log

Tn Tn-1

+ · · · + log

T0 

.

(2.9)

Let Zn = (Xn, Yn); define

W0 = (Z0, T0), W1 = (Z1, T1), · · · , Wn = (Zn, Tn).

(2.10)

Then, W0, W1, · · · , Wn is a Markov chain on the state space S := X × Rq × P (Rd) with the transition kernel

P((z, u¯), A × B) := Ez(IA×B(Z1, M1u))

(2.11)

for all z  X × Rq, u¯  P (Rd), A  X × B(Rq), and B  B(P (Rd)), the Borel -algebra of P (Rd).

Note that the initial distribution of W0 depends on Z0 only, and Z0 has the distribution if (y|i, y0) as

its initial distribution. We note that Pz := P(·, ·) in (2.11) depends only on z. Let Ez := E(z,u¯) denote the expectation under Pz. By (2.2), the Markov chain {(Xn, Yn), n  0} has transition density pijf (y|j, y0) with respect to L. Therefore, the induced transition probability P(·, ·) has a probability density p(·, ·)

with respect to L. Under condition C in Section II-B, it follows from Proposition 2 of Fuh (2004a)

or Proposition 1 of Fuh and Tartakovsky (2019) that the Markov chain Wn has an invariant probability measure  on S. Note that L is a product measure on X × Rq × P (Rd), and the first component has

probability density if (y|i, y0) with respect to L. Now, for M  Gl(d, R), let g : S × S  R be

g((z0, u¯), (z1, M u)) = log

Mu u

; then for  defined in (2.4),

log Tn = g(Wn-1, Wn) + · · · + g(W0, W1) + g(W0, W0)

(2.12)

is an additive functional of the Markov chain {Wn, n  0}, where g(W0, W0) = log

T0  

.

B. Nonnegative transition probability kernel for Markov operator
To study the Kullback­Leibler divergence and Re´yni divergence in general HMMs, we must consider a nonnegative transition probability kernel for a Markov operator of the induced Markov chain {Wn, n  0} defined in (2.10) on the state space S := X × Rq × P (Rd), with the transition kernel P in (2.11). Before that we need the following notation.

8

Note that {(Xn, Yn), n  0} defined in (2.1) and (2.2) is a Markov chain on the state space X × Rq. Below, we abuse the notation a bit to consider {Yn, n  0} as a Markov chain on a general state space Rq .

Definition II.2. A Markov chain {Yn, n  0} on a general state space Rq is said to be V -uniformly ergodic if there exists a measurable function V : Rq  [1, ), with V (y)L(dy) < , such that

lim sup
n yRq

E[h(Yn)|Y0 = y] - V (y)

h(z)L(dz) : |h|  V

= 0.

(2.13)

Definition II.3. A Markov chain {Yn, n  0} on a state space Rq is said to be Harris recurrent if there exists a recurrent set R  B(Rq), a probability measure  on R, a  > 0, and an integer n0 such that

P {Yn  R for some n  1|Y0 = y} = 1,

(2.14)

P {Yn0  A|Y0 = y}  (A),

for all y  R and A  R.

It is known that under the irreducibility and aperiodicity assumption, V -uniform ergodicity implies that

{Xn, n  0} is Harris recurrent, cf. Theorem 9.18 of Meyn and Tweedie (2009).

The following assumptions will be used throughout this paper.

Condition C: C1. The Markov chain {(Xn, Yn), n  0} defined in (2.1) and (2.2) is aperiodic and irreducible on X ×Rq. For each j  X , the conditional Markov chain {Yn|Xn, n  0} is Vj(·)-uniformly ergodic for some Vj(·) on Rq, such that there exists p  1,

sup Ey
yRq

Vj (Yp) Vj (y)

<  for all j  X .

C2. Assume 0 < supjX f (y|j, y0) < , for all y  Rq. Denote h(Y1) = maxiX supy0Rq

d j=1

pij f (Y1|j, y0).

Assume

there

exists

p



1

as

in

C1

such

that

for

all

i



X,

sup Ei
jX ,yRq

log

h(Y1)p

Vj (Yp) Vj (y)

< 0,

sup Ei
jX ,yRq

h(Y1)

Vj (Y1) Vj (y)

< .

C3. Recall that L is a -finite measure on Rq defined in (2.2). Assume

(2.15)
(2.16) (2.17)

max sup |

ipijf (y|j, y0)L(dy)| < .

iX y0Rq jX yRq

9

Remark II.4. C1 is an ergodic condition for the underlying Markov chain. The weighted mean contraction property (2.16) and the finite weighted mean average property (2.17), which appear in C2, guarantee that the induced Markov chain {Wn, n  0} is V~ -uniformly ergodic for a given function V~ , and hence to be Harris recurrent. In Section IV, we show that several interesting models satisfy these conditions. C3 is a constraint of the Re´nyi divergence (Kullback­Leibler divergence) and is a standard moment condition. The finiteness condition is quite natural and holds in most cases.

The following proposition is a generalization of Theorem 3 in Fuh (2021b). Since the proof is the same as those in Lemmas 3 and 4 of Fuh (2006), it is omitted.

Proposition II.5. Let {(Xn, Yn), n  0} be the hidden Markov model given in (2.1) and (2.2), satisfying C1­C3. Then the induced Markov chain {Wn, n  0} is an aperiodic, irreducible, and Harris recurrent Markov chain, with the invariant probability . Furthermore there exist a, C > 0, such that Ew(exp{ag(W0, W1)})  C <  for all w  W.

Under the Harris recurrent condition (2.14), it is known that, cf. Meyn and Tweedie (2009), Wn admits

a regenerative scheme with i.i.d. inter-regeneration times for an augmented Markov chain, which is

called the "split chain". Heuristic speaking, let (0) = , and let {(j), j  1} denote the times

of consecutive visits to a recurrent state   S. For a function f : S  R, let Sn =

n i=0

f

(Wi),

Sj(f ) =

 (j +1) i= (j )+1

f

(Wi).

By

the

strong

Markov

property,

the

random

variables

{Sj(f ), j



0}

are

independent and identically distributed random variables. Note that here we only consider f (Wi); the case

of f (Wi, Wi+1) is similar.

Let  =  be the first time (> 0) reaches the recurrent state  of the split chain. Let  be an initial distribution on S, and define

u(, ) = E eS - for   R.

(2.18)

Assume that

 := {(, ) : u(, ) < } is an open subset on R2.

(2.19)

Denote 1 := S1. Ney and Nummelin (1987) shows that D = { : u(, ) <  for some } is an open set and that for   D, the transition kernel

P^ (w, A) = Ew{e1 I{W1A}}

(2.20)

10

has a unique maximal simple real eigenvalue e(), where () is the unique solution of the equation u(, ()) = 1, with corresponding right eigenfunctions r(·; ) and left eigenmeasures l (·; ) defined by

r(w; ) := Ew exp{S -  ()}.

(2.21)

For a measurable subset A  B(S), any initial distribution  on S and w  S, define

 -1

l(A; ) = E

eSn-n()I{WnA} ,

n=0

(2.22)

 -1

lw(A; ) = Ew

eSn-n()I{WnA} .

n=0

(2.23)

To analyze P^  in (2.20), for completeness, we state the following proposition, which is taken from

Theorem 4.1 in Ney and Nummelin (1987). Note that by Proposition II.5, the induced Markov chain

{Wn, n  0} is an aperiodic, irreducible, and Harris recurrent Markov chain, which implies that condi-

tion M1 in Theorem 4.1 of Ney and Nummelin (1987) holds.

Proposition II.6. Let {(Xn, Yn), n  0} be the hidden Markov model given in (2.1) and (2.2), satisfying C1­C3. Let P^ (·, ·) be the operator defined in (2.20), and (·) be defined by the characteristic equation (2.18). Then
(i) D = { : u(, ) <  for some } is an open set.  is analytic, strictly convex, and essentially smooth on D.
(ii) For   D, (·) = e(·) is the largest eigenvalue of P^  with (right) eigenfunction {r(w; ) : w  S} and (left) eigenmeasure {l (A; ) : A  S} having the representation (2.21) and (2.22).
(iii) There is a set B  S with (Bc) = 0, such that for each w  B, 0 < r(w; ·) <  and is analytic on D. If B is a small set, then 0 < l(B; ) <  and 0 < lw(B; ) <  for all w  S and is analytic on D.
(iv) There exists a partition S =  i=1Si and a sequence of functions fi : R  (0, ), i = 1, 2, · · · , such that
r(w; )  fi()ISi (w), w  S,   D, i = 1, 2, · · · .
Remark II.7. We will use Proposition II.6 (i)­(iv) in Section III and the rest of this paper, the reader is referred to Ney and Nummelin (1987) Theorem 4.1 and Lemma 4.5 for details. (iv) states that there is a countable partition of the state space S =  i=1Si, independent of , such that r(w; ) is uniformly

11

positive on each Si. However in order to apply (iii), one needs to extend to 0 < l(S; ) <  and the uniform boundness of r(w; ·) over the whole space S. To this end, one needs extra condition and apply Theorem 4 of Chan and Lai (2003) under this additional assumption. For completeness, we inculde it as follows.

Note that {Wn, n  0} is V~ -uniformly ergodic as stated in Remark II.4. C4. Assume (2.19) hold. Let C be a measurable subset of S such that for any given initial distribution  on S,

L(C; ) <  and Lw(C; ) <  for all w  S.

(2.24)

Let V~ : S  [1, ) be a measurable function. Assume for some 0 <  < 1 and K > 0, we have

Ew[eW1-()V~ (W1)]  (1 - )V~ (w)  w / C,

sup Ew[eW1-()V~ (W1)] = K <  and
wC
where  is defined in (2.14).

V~ (w)(dw) < ,

(2.25) (2.26)

Remark II.8. Note that under condition C1­C4, we have 0 < l(S; ) <  and r(w; ·) is uniform boundness over the whole space S. Althought condition C4 is under the induced Markov chain {Wn, n  0}, by using the results in Fuh (2021a), this condition holds for some interesting examples, see Section IV. Moreover, if the state space is finite (compact), which is commonely used in engineering, the above results hold.

III. RE´ NYI DIVERGENCE We state our main results in this section. Section III-A presents the convergence of the Re´nyi divergence. Section III-B defines the Kullback­Leibler divergence, and shows that the Kullback­Leibler divergence is the limit of the Re´nyi divergence as   1.

A. Re´nyi divergence
Let {Yn, n  0} be the general HMM defined in (2.1). Denote Y0:n = {Y0, Y1, · · · , Yn} and y0:n = {y0, y1, · · · , yn}. With the same notation used in Section II, denote P (n)(·) and Q(n)(·) as two proba-

12

bilities on Y0:n. By (2.4), the probability density functions p(n)(·) and q(n)(·) of the random variables {Y0, Y1, · · · , Yn} under P (n) and Q(n) are given, respectively, by

p(n)(y0:n) = pn(y0, y1, · · · , yn)

d

d

n

=

···

x0 f (y0|x0) pxk-1xk f (yk; |xk, yk-1),

x0=1 xn=1

k=1

(3.1)

q(n)(y0:n) = qn(y0, y1, · · · , yn)

d

d

n

=

···

x0 g(y0|x0) pxk-1xk g(yk; |xk, yk-1),

x0=1 xn=1

k=1

where g(yk|xk, yk-1) is the probability density of Q with respect to L.

(3.2)

Recall the definition of the Re´yni Divergence for independent and identically distributed random vari-

ables (i.i.d.) {n, n  0} as follows: for given   (0, 1)  (1, ), let

D(f ||g)

=



1 -

1

log

Ef

f (1) g(1)

-1
.

(3.3)

Now for given a hidden Markov model {Yn, n  0} with transition probability density p and q, let

Dn(p(n)||q(n))

=



1 -

1

log

Ep

pn(Y0, Y1, · · · , Yn) qn(Y0, Y1, · · · , Yn)

-1
.

(3.4)

Note that here Ef (Ep) denotes the expectation under probability distribution f (p). We will use the same

type of notation without specification here and afterward.

By (2.5) and (2.9), we have

(3.4)

=



1 -

1

log Ep

Mnp · · · M1pM0pp Mnq · · · M1qM0qq

-1
,

(3.5)

where Mkp is defined in (2.6) and (2.7) under the probability from p(n), and Mkq is defined in (2.6) and

(2.7) under the probability from q(n), for k = 0, 1, · · · , n. Here E is defined as the expectation under the

probability P defined in (2.11).

13

Denote g(W0, W0) = log

T0pp / p T0q q / q

, and g(Wk-1, Wk) = log

Tkp p Tkq q

/ Tkp-1 p / Tkq-1 q

,

for

k

= 1, · · · , n.

Let

S0 = g(W0, W0) and Sn = S0 +

n k=1

g(Wk-1, Wk).

Then

by

(2.12),

we

have

Ep

Mnp · · · M1pM0pp -1 Mnq · · · M1qM0qq

= Ep exp log

Mnp · · · M1pM0pp Mnq · · · M1qM0qq

-1

=

Ep exp

( - 1)

log

Tnpp / Tnp-1p Tnqq / Tnq-1q

+ · · · + log

T1pp / T0pp T1qq / T0qq

+ log

T0pp / p T0qq / q

(3.6)

= Ep exp ( - 1) g(Wn-1, Wn) + · · · + g(W0, W1) + g(W0, W0)

= Ep exp ( - 1)Sn .

Then, using P^  defined (2.20) in Section II-B with  =  - 1, let p (q) be the initial distribution of the {Wn, n  0} under P (Q). Then we have

E e(-1)S -() = u(, ()) = 1 Ewe(-1)S -() = r(w, ).

(3.7) (3.8)

Let () be the largest eigenvalue of P^ . Denote r(w, ) as the right eigenfunctions associated with () defined in (3.8). Define

r() = inf r(w, ), r¯() = sup r(w, ),

(3.9)

w

w

Under conditions C1­C4, by Proposition II.6 (iv), the uniform positivity property, we have 0 < r() 

r¯() < .

Theorem III.1. Under conditions C1­C4, then the Re´nyi divergence rate between p(n) and q(n) is

D(p||q)

:=

lim
n

1 n

Dn

(p(n)

||q(n)

)

=



1 -

1

log (),

(3.10)

where () is the largest positive real eigenvalue of P^ , and 0 <  < 1. Furthermore, the same result

holds for  > 1 if P > 0 and Q > 0.

Proof. Let () be the largest positive real eigenvalue of P^  defined in (2.20), with associated positive right eigenfuction r(w, ) > 0 uniformly on S. Then by (3.8), we have

P^ n-1r(w; ) = n-1()r(w; ).

(3.11)

14

Let r() and r¯() be defined in (3.9). Then 0 < r()  r(w, )  r¯() < , for all w  S. Let P^ n-11 = b(). Then by (3.9), we have

n-1()r(w; ) = P^ n-1r(w; )  r¯()b().

Similary we have n-1()r(w; )  r()b().

Therefore,

r(w; ) r¯()



b() n-1()



r(w, ) r()

.

Since

w r(w; )(dw) r¯()



b() n-1()



w

r(w; )(dw) r()

,

we have

1 n

log

w

r(w; )(dw) r¯()



1 n

log

b() n-1()



1 n

log

w

r(w; )(dw) r()

.

(3.12)

Note that the constant terms in the upper- and lower-bound in (3.12) are bounded and independent of n,

which approach 0 as n  . Therefore, we have

lim
n

1 n

log

P^ n-11 n-1()

=

0.

(3.13)

Hence

lim
n

1 n

log P^ n-11

=

lim
n

1 n

log n-1()

+

lim
n

1 n

log

P^ n-11 n-1()

=

log ().

Note that (3.14) holds for both p and q. Thus

lim
n

1 n

Dn(p(n)||q

(n))

=



1 -1

log ().

(3.14) (3.15)

The proof is complete.

B. Kullback­Leibler divergence
By making use of Theorem III.1, we herein show that the Re´nyi divergence reduces to the Kullback­ Leibler divergence as   1. Let us first note the following result about the computation of the Kullback­ Leibler divergence rate between two general HMMs. The convergence rate of the Kullback­Leibler divergence has been investigated by Fuh (2004b), and Fuh and Mei (2015) for the parametric case. In

15

the following proposition, we then show that the Kullback­Leibler divergence for general HMMs can also

be written in a form similar to that in the i.i.d. case.

Recall that {Y0, Y1, · · · , } is a general HMM. Let p(n) and q(n) be two probability distributions. Let

P and Q be the probabilities associated with p(n) and q(n), respectively. Let p and q be two initial

distributions with respect to p(n) and q(n), respectively. If Q > 0, then q > 0. Denote

Dn(p(n)||q(n))

=

log

pn(Y0, Y1, · · · , Yn) qn(Y0, Y1, · · · , Yn)

.

(3.16)

Proposition III.2. Under conditions C1­C4, the Kullback­Leibler divergence rate between p(n) and q(n)

is well-defined with

K (p,

q)

=

lim
n

1 n

Dn(p(n)||q(n))

=

E,p

[log

p1(Y0,

Y1)]

-

E,q

[log

q1(Y0,

Y1)]

,

(3.17)

where E,p (E,q) is the expectation of Pp (Pq) defined in (2.11) of Section 2.1 under the invariant probability  of {Wn, n  0}. Here Pp denotes the probability under p.

Proof. Under conditions C1­C4, by Proposition II.6, the invariant probability  of the induced Markov chain {Wn, n  0} exists. Recall Tn = Mn · · · M1M0 defined in (2.8). Now let Mnp (Mnq) be Mn defined in (2.6) and (2.7) when the probability is under P (Q). We can define Tnp and Tnq similarly. First, it is

easy to see from (2.12) that

1 n

[log

pn(Y0, Y1, · · ·

, Yn)

-

log

pn(Y0,

Y1,

···

, Yn)]

=

1 n

[log

Tnp

- log

Tnq ]

=

1 n

n

gp(Wi, Wi-1) - gq(Wi, Wi-1).

i=1

(3.18)

Taking n   on both sides of (3.18), then by Proposition II.5 and the SLLN for Markov random walks

in Meyn and Tweedie (2009), we have

K(p, q) =E,p [gp(W1, W0)] - E,q [gq(W1, W0)] =E,p [log p1(Y0, Y1)] - E,q [log q1(Y0, Y1)] ,

which completes the proof.

Theorem III.3. Let   (0, 1)  (1, ). Assume conditions C1­C4 hold; then

lim
1

lim
n

1 n

Dn

(p(n)

||q(n)

)

=

lim
n

lim
1

1 n

Dn(p(n)||q(n))

=

w0

w1

w0 P(w0 ,

w1)

log

P(w0, Q(w0,

w1) w1)

dw1

dw0

=

K (p,

q),

(3.19)

16

which is the Kullback­Leibler divergence defined in (3.17).

Proof.

To

prove

(3.19),

we

first

consider

the

case

of

limn lim1

1 n

Dn(p(n)||q(n)).

By

(3.4),

(3.5),

and (3.6), we have

Dn(p(n)||q(n))

=



1 -

1

log

Ep

pn(Y0, Y1, · · · , Yn) -1 qn(Y0, Y1, · · · , Yn)

=



1 -

1

log

Ep

Mnp · · · M1pM0pp -1 Mnq · · · M1qM0qq

=



1 -

1

log

Ep

exp

( - 1)Sn

.

(3.20)

Then

lim
n

lim
1

1 n

Dn (p(n) ||q (n) )

=

lim
n

1 n

lim
1



1 -

1

log

Ep

exp

( - 1)Sn

.

(3.21)

=

lim
n

1 n Ep

Sn

=

w0

w1

w0 P(w0 ,

w1)

log

P(w0, Q(w0,

w1) w1)

dw1dw0.

Note that the second identity comes from L'Hospital's Rule and the last identity in (3.21) comes from

(3.17) in Proposition III.2.

Next,

we

consider

the

case

of

lim1

limn

1 n

Dn

(p(n)

||q(n)

).

By (3.10) in Theorem III.1, we have

lim
n

1 n

Dn(p(n)||q(n))

=



1 -

1

log

().

To

evaluate

lim1

1 -1

log

(),

note

by

Proposition

II.6

(i)

that

the

eigenvalue

()

of

P^ 

is

a

continuous

differentiable function of . Note that since Q > 0, we have

lim () = 1.
1

Let a denote an arbitrary base of the logarithm. Then, by L'Hopital's rule, we find that

lim
1

log () -1

=

1 ln a



(1)

:=

1 () ln a 

,
=1

(3.22)

which is well defined by Proposition II.6 since the algebraic multiplicity of () is 1 by (2.20). The

equation defining the largest positive eigenvalue () = 1 of P^ (w, A) := Ew{IW1A} = Pw(A). By Proposition II.6 (i), P^  is analytic for   D; therefore by (2.20), it is straightforward to check that

P^   P^ as   1.

17

Note that for   R, u(, ) = Ee(-1)S - defined in (2.18). Then the transition kernel P^ (w, A) = Ew{e(-1)1 I{W1A}} has a maximal simple real eigenvalue () = e(), where () is the unique solution of the equation u(, ()) = 1. Then using (1) = log (1) = log 1 = 0, we have

u(, ())

=

1

=

u(, ()) 

|=1

=

0

=

E{(S

-

() )e(-1)S -() }|=1

=

0

= E{(S - () )e(-1)S -() }|=1 = 0 = E{(S - (1) )} = 0

=

(1) =

() 

|=1

=

ES E

= ES1.

(3.23)

The last identity in (3.23) comes from Lemma 5.2 of Ney and Nummelin (1987).

By using (3.22) and (3.23), we obtain

lim
1



1 -

1

log

()

=

w0

w1

w0 P(w0 ,

w1)

log

P(w0, Q(w0,

w1) w1)

dw1dw0

=

K (p,

q),

which completes the proof.

(3.24)

IV. EXAMPLES We present two examples of general HMMs in this section. Section IV-A considers the Markov switching models, whereas Section IV-B studies the RNN.

A. Markov switching models

We start with a simple real valued q-order autoregression around one of d constants µ1, · · · , µd:

q

Yn - µXn = k(Yn-k - µXn-k ) + n,
k=1

(4.1)

where n  N (0, 2), |k| < 1 for k = 1, · · · , q, and {Xn, n  0} is a d-state ergodic Markov chain.

When q = 4 and d = 2, this model was studied by Hamilton (1989) in order to analyze the behavior of

the U.S. real GNP. Note that the Markov switching model (4.1) includes the classical HMM by letting

k = 0 for k = 1, · · · , q. To apply our theory in the form of (4.1), we consider a simple case of order 1 in (4.1) with d = 2. The extension to the general case is straighforward. In this case, the conditional

probability given Xn = xn and Yn-1 = yn-1, n  1, is

f (yn|xn, yn-1; )

=

1 2

exp

- [(yn - µxn ) - 1(yn-1 - µxn-1)]2/22 .

(4.2)

Denote [pij]i,j=1,2 as the transition probability of the underlying Markov chain {Xn, n  0} and let

 = (p11, p21, 1, µ1, µ2, 2) be the given parameter. Assume that |1| < 1 for the stability property, and

18

that there exists a constant c > 0 such that 2 > c. Moreover, we assume that µ1 = µ2. Since the state space of Xn is finite, we consider 0 < pij < 1 for all i, j = 1, 2, and for j = 1, 2 let Vj(y) = |y| + 1 (cf.

page 394 of Meyn and Tweedie (2009)) such that the condition C1 holds. Under the normal distribution

assumption, it is easy to see that (2.15) in conditions C1 and C3 holds.

Next we check that the mean contraction property (2.16) in C2 for a simple Markov switching model

with general innovation holds. Given p  1 as in C2, and || < 1, let Xn be a two-state Markov chain, and Yn = µXn + Yn-1 + n, where n are i.i.d. random variables with E|1| = a < . Further, we assume both 1 have a positive probability density function with respect to the Lebesgue measure. Denote h(Y1) = C < 1, b = (1 - ||p)/(1 - ||) and choose p such that Cp(ab + 1) < 1. Let d(u, v) = |u - v|.

Then we have

sup
y

Ej

log

h(Y1)pVj (Yp) Vj (y)

|Y0

=

y

<

sup
y

Ej

Cp(|py + log

p-1 k=0

k

p-k|

|y| + 1

+

1) |Y0

=

y

< log sup
y

Cp(|py| + E|

p-1 k=0

k p-k |

+

1)

|y| + 1

= log sup
y

Cp(|py| + ab + 1) |y| + 1

(4.3) < 0.

By using the same argument, it is straightforward to check that (2.17) in C2 holds. By making use the

same argement as that in Example 2 of Chan and Lai (2003), we can check C4 hold. In Section VI, we

will present a numerical computation method of the Re´nyi divergence under model (4.1) with q = 1 and

d = 2.

B. Recurrent neural network
Note that at the outset, RNN is a non-linear dynamical system commonly trained to fit sequence data via some variant of gradient descent. In this subsection, we treat RNN as a stochastic model as usual, cf. Goodfellow et al. (2016). Fuh (2021a) considers the example of RNN from a Markovian-iterated function system point of view, to study its stability. Here we apply a general HMM point of view to investigate RNN. Although some notation overlaps between these two parts, we include it here for completeness. A connection between the classical finite state HMM and RNN is in Buys et al. (2018).
An RNN can take as input a variable-length sequence y = (y1, · · · , yn) by recursively processing each symbol while maintaining its internal hidden state h. At each time step n, the RNN reads the symbol

19

Yn  Rq and updates its hidden state hn  Rp by

hn = f(Yn, hn-1),

(4.4)

where f is a deterministic non-linear transition function, and  is the parameter of f. The transition function f can be implemented with gated activation functions such as long short-term

memory (LSTM) or the gated recurrent unit (GRU). The joint probability of the RNN model sequence

can be written as a product of conditional probabilities such that

n

n

P (Y1, · · · , Yn) = P (Yk|Y1, · · · , Yk-1) = g(hn-1),

k=1

k=1

(4.5)

where g is a function that maps the RNN hidden state ht-1 to a probability distribution over possible

outputs, and  is the parameter of g.

To analyze (4.4) and (4.5), we provide a Markov chain framework as follows. Specifically, let H =

{hn, n  0} be a sequence of random variables on (Rp, B(Rp)), and suppose that a random sequence

{Yn} n=0 taking values in Rq is adjoined to H such that {Zn := (hn-1, hn, Yn), n  0} is a Markov chain on Rp × Rp × Rq satisfying

P {(hn-1, hn)  A, Yn  B|h0, h1, · · · , hn-1; Y0, Y1, · · · , Yn-1}

(4.6)

=

P {(hn-1, hn)  A|h0, h1, · · · , hn-1; Y0, Y1, · · · , Yn-1, Yn  dy}

yB

× P {Yn  dy|h0, h1, · · · , hn-1; Y0, Y1, · · · , Yn-1}Q(dy)

=

P {(hn-1, hn)  A|(hn-2, hn-1); Yn  dy}P {Yn  dy|(hn-2, hn-1); Yn-1}Q(dy)

yB

=

I{(hn-1,hn)A|(hn-2,hn-1);Yndy}P {Yn  dy|(hn-2, hn-1); Yn-1}Q(dy)

yB

for A  B(Rp × Rp), B  B(Rq) and each n = 1, 2, · · · .

By considering Xn to be degenerate and {(hn-1, hn, Yn), n  0} to be a general-state Markov chain,

we have Yn = g((hn-1, hn, Yn)) as a general HMM. Next, we consider the simple case, cf. Chung et al.

(2015), in which the generating distribution is conditioned on hn-1 such that

Yn = µy,n + y,nn,

(4.7)

where y,n > 0 P-a.s., n  N (0, 1) is a sequence of i.i.d. random variables, and n is independent of {Yn-k, k  1} for all n. Here, we assume that µy,n and y2,n are the parameters of the generating

20

distribution such that (µy,n, y2,n)  g(hn-1), with g any highly flexible function such as a neural network.
To illustrate the general HMM approach, we consider two examples: linear RNN and LSTM. To start with, we consider the linear RNN. Let Yn be the output model in (4.7); the linear RNN updates its hidden state using the following recurrence equation:

hn = f(Yn, hn-1) = 0 + 1hn-1 + 2Yn,

(4.8)

where  = (0, 1, 2) with 0 > 0, 1 > 0, and 2 > 0 constants.

For an explicit representation, we analyze the linear RNN (4.7) and (4.8) as follows: let Zn = (hn-1, hn, Yn)

be the Markov chain on X := (R × R × R). Denote n = h-n-11Yn and let n = (1 + 2n)  R. Let

An be a 3 × 3 matrix written as





010





An

=

 

0

n

0

. 

(4.9)





0 n 0

Note that although {An, n  0} are random matrices driven by the Markov chain {Zn, n  0}, since

the randomness of Yn comes only from the i.i.d. random variables n, and since Yn is independent of

Fn-1, the -algebra generated by Y1, · · · , Yn-1, {An, n  0} are i.i.d. random matrices. Let n = (0, 0, 0)t  R3. Then we have the following linear state space representation of the linear

RNN (4.7) and (4.8): Zn is a Markov chain governed by

Zn = AnZn-1 + n,

(4.10)

and Yn := g(Zn), the observed random quantity, is a non-invertible function of Zn.

It

is

easy

to

check

that

the

stability

condition

holds

if

E

Y1 h1

<

1

and

1

+

2

E

Y1 h1

<

1, where

 is the stationary distribution of the Markov chain {(Zn, An · · · A1), n  0}. The moment conditions

hold under the normality assumption in (4.7). Note that Zn defined in (4.10) is a V -uniformly ergodic

Markov chain with V (z) = z 2, cf. Theorem 16.5.1 of Meyn and Tweedie (2009). By using the results

in Bougerol and Picard (1992), it is straightforward to check that the stability condition and conditions

C1 and C3 hold. By an argument similar to that in (4.3), C2 holds.

Next, we consider the LSTM network, cf. Hochreiter and Schmidhuber (1997). By using (4.7) as the output model for Yn, we consider the hidden unit as follows. The state is a pair of vectors s = (c, h)  R2d,

21

and the model is parameterized by eight matrices, W  R2d and U  Rd×n, for   {i, f, , z}. The state-transition map LSTM for f in (4.8) is defined as

ft = (Wf ht-1 + Uf yt),

it = (Wiht-1 + Uiyt), ot = (Woht-1 + Uoyt), (4.11)

zt = tanh(Wzht-1 + Uzyt), ct = it  zt + ft  ct-1, ht = ot · tanh(ct),

where  denotes elementwise multiplication, and  is the logistic function. Let Zn = (hn-1, hn, Yn) be the Markov chain defined in (4.7) and (4.8). To provide conditions under
which the r-step iterated system rLSTM = LSTM  · · ·  LSTM is stable, we denote W  as the induced  matrix norm, which corresponds to the maximum absolute row sum maxi j |Wij|, and let E f  = supt E ft . Since  < 1 for given any weights Wf ; Uf and inputs yt, we have E f  < 1. This means the next state ct must "forget" a non-trivial portion of ct-1. We leverage this phenomenon to give sufficient conditions for LSTM to be contractive in the  norm, which in turn implies the system rLSTM is contractive in the 2 norm for r = O(log d).
By using the mean contraction under the normal distribution defined in (4.7), the following result is taken from Proposition 1 of Fuh (2021a), in which he shows that the iterated function system rLSTM is stable; see also Proposition 2 in Miller and Hardt (2018) for deterministic LSTM.

Proposition IV.1. Wf  < Bw < , Uf  < Bu < , Yt   BY , P -a.s for some random variable BY with EBY < . Moreover, assume Wi  < (1 - E f ), Wo  < (1 - E f ), Wz  < (1/4)(1 - E f ), Wf  < (1 - E f )2, and r = O(log d); then the iterated function system rLSTM is stable.
Under this assumption, the state space of the Markov chain {Zn = (hn-1, hn, Yn), n  0} defined in the LSTM model (4.11) is compact, and hence is Harris recurrent and satisfies the V -uniformly ergodic assumptions C1. Under the normality assumption in (4.7), it is easy to see that E|1|p <  for any p > 0. Therefore the moment conditions of C1 and C3 hold. The contraction property C2 holds due to the definition of the activation functions. C4 holds as the state space of the Markov chain is compact. By Theorem III.1 and Proposition III.2, we prove the existence of the Re´nyi divergence and Kullback­Leibler divergence, and provide a characterization. As numerical computations seem difficult, we will rely on Monte-Carlo simulations.

22

V. COMPUTATIONAL ISSUES IN GENERAL HMM
Since the Re´nyi divergence in general HMM involves an eigenvalue which is difficult to compute, we provide an approximated Re´nyi divergence in Section V-A. Next we present a theoretical background of the invariant measure for Jp in Section V-B, and report numerical computation of the Re´nyi divergence in Section V-C.

A. Approximated Re´nyi Divergence

Under conditions C1­C4, by Theorem III.1 the Re´nyi divergence between P and Q is

D(p||q)

:=

lim
n

1 n

Dn

(p(n)

||q(n)

)

=



1 -

1

log ().

(5.1)

Note that the computation of the Re´nyi divergence based on (5.1) involves the computation of the largest

eigenvalue (), which is not an easy task. Hence, instead of using (5.1), we will provide an alternative

approach based on the recursive formula as follows. Recall that from (3.4), D(p||q) is defined as

D(p||q)

:=

lim
n

1 n

Dn(p(n)||q(n))

=



1 -

1

lim
n

1 n

log

Ep

pn(Y0, Y1, · · · , Yn) qn(Y0, Y1, · · · , Yn)

-1
,

(5.2)

where Ep denotes the expectation according to Pp, the probability of the Markov chain {Wn,  0} when

the probability of {Y0, Y1, · · · , Yn} is under pn.

Now, we seek to show that (5.2) can be computed via iterations. In particular, we have

pn(y0, y1, . . . , yn) = Cn,1(p) + . . . + Cn,d(p),

(5.3)

where

d
Ct,j (p) = f (yt|xt-1 = j, yt-1) psjCt-1,s(p),
s=1

(5.4)

with initial values C1,j(p) = j(p)f (y1|x0 = j), for j = 1, . . . , d. The following lemma gives an iteration

method to compute the probability.

Lemma V.1. Under conditions C1­C4, the probability can be calculated by

n
(pn(y0, y1, . . . , yn))-1 = (Ct,1(p) + . . . + Ct,d(p))-1,
t=1

where

Ct,j (p)

=

f (yt|xt-1

=

j, yt-1)

d s=1

psj

d s=1

Ct-1,s

(p)

Ct-1,s

(p)

,

(5.5)

23

with initial values C1,j(p) = j(p)f (y1|x0 = j), for j = 1, . . . , d.

Proof. By (5.3) and (5.4), and using induction for n = 1, 2, · · · , it is easy to show that

n
(Ct,1(p) + . . . + Ct,d(p))-1
t=1
n
= exp ( - 1) log(Ct,1(p) + . . . + Ct,d(p))
t=1

n

d

n

d

= exp ( - 1)

log

Ct,j (p) - log

Ct-1,s(p)

t=1

j=1

t=1

s=1

d

= exp ( - 1) log

Cn,j (p)

j=1

d

=

Cn,j(p) -1 = (pn(y0, y1, . . . , yn))-1.

j=1

Remark V.2. Note that Ct,j(p) in (5.5) generally have finite means, and thus Cn,j(p) increase or decrease linearly in n. Hence, Lemma V.1 provides an algorithm that is computationally feasible when the time step is large. In addition, it is easy to see from (5.5) that the "normalization" of (5.3) reflects the idea of using the projection space P (Rd) defined in (2.10) and (2.11).

Denote

J  = E E

Ct,0(p) + Ct,1(p) Ct,0(q) + Ct,1(q)

-1
Xt-1, Yt-2, Wt-1

.

By (5.2), (5.5), and (5.6) we observe that the Re´nyi divergence can be approximated as

D(p||q)

=



1 -

1

lim
n

1 n

log

Ep

exp ( - 1) exp ( - 1)

n t=0

log

n t=0

log

d s=1

Ct,s(p)

d s=1

Ct,s

(q

)

,

=



1 -

1

1 n

log

lim
n

Ep

exp

n
log

t=0





1 -

1

1 n

log

J

n

=



1 -

1

log J.

d s=1

Ct,s(p)

-1

d s=1

Ct,s

(q

)

Remark V.3. Note that here in (5.7), we apply the following approximation

Ep

exp

1 n

n

log

t=0

d s=1

Ct,s

(p)

-1

d s=1

Ct,s(q)

 J.

(5.6) (5.7)
(5.8)

24

In other words, we approximate the largest eigenvalue () via n()  (J)n, which can be explained as follows. By (2.18), we have for given  = n,

E e(-1)Sn | = n = E e()n| = n = E n()| = n .

Let T1 be defined as (2.8) with the form in (5.2). Denote A as the number of epochs by the regeration

time  ; then use E1 · A  n to approximate

n

(E(T1))n  Ee(-1)Sn  E[e(-1)

]  E[ n
j=1

Sj

e(-1)Sj ]  E[e(-1)S1 ]n = E1·A ()  n().

j=1

In summary, we have more accurate approximation when   1 or in the `almost i.i.d.' case. In other

words, we approximate () by E(T1) via the idea of approximating the transition probability by the

invariant probability.

B. Theoretical Background of the Invariant Measure for J

Since the Re´nyi divergence in general HMM in Theorem III.1 involves the largest eigenvalue of the

operator defined in the induced Markov chain {Wn, n  0}, it is not easy to compute in general. One

standard way to calculate the Re´nyi divergence in general HMM is via Monte Carlo simulation. Specifically,

we first generate {Yt}nt=1 from the model Pp. Second, we compute

1 n

log

Ep

pn(Y0, Y1, · · · , Yn) -1 qn(Y0, Y1, · · · , Yn)

via (5.5). Then, the Re´nyi divergence can be estimated by repeating the above procedure several times and

averaging its results. Needless to say, Monte Carlo is time-consuming especially when repeated calculations

are needed. In this section, we propose a faster algorithm to compute the Re´nyi divergence in a two-state

Markov

switching

model.

By

(5.7),

the

Re´nyi

divergence

can

be

computed

as

D(p||q)

=

1 -1

log

J,

where J is defined in (5.6). For the case X = {0, 1}, J can be computed numerically. Before stating

the method, we first note that the invariant measure of J depends only on Wt = Ct,0(p)/(Ct,0(p) + Ct,1(p))  [0, 1]. The other important fact is that Wt depends to Xt and Yt-1 due to the fact that

{(Xt, Yt, Yt-1, Wt), t  0} is a Markov chain (Y-1 := 0). To find the stationary distribution of Wt, we

define mj(·, ·) as the stationary density function satisfying
x
Pr(Xt = j, Yt-1 = u, Wt  x) = mj(u, w)dw.
0
By extending the argument as in Fuh and Mei (2015), the following proposition characterizes mj(·, ·) via

Fredholm integral equations. Before that, we require the following notation.

25

For ease of presentation, we will use p1 for probability under Pp and p for a probability under P with a parameter . We will denote j() as the parameter of the underlying Markov chain Xt = j with

parameter .

Define z(w, x) as for j = 0, 1,

z(w, x)

=

1

x -

x

·

p01()w p00()w

+ +

p11()(1 p10()(1

- -

w) w)

;

Qj(u, z) = P 1

g(Yt|0(), u) g(Yt|1(), u)



z

Xt

=

j, Yt-1

=

u

.

By using an argument similar to Theorem 3 of Fuh and Mei (2015), we have

Proposition V.4. Under conditions C1­C4, for all 0 < x < 1,

m0(u, x) = p00(1)

1 0

 -

g(u|0

(1),

v)

 x

Q0(u,

z(w,

x))m0

(v,

w)dvdw

+ p10(1)

1 0

 -

g(u|1(1),

v)

 x

Q0(u,

z(w,

x))m1

(v,

w)dvdw,

m1(u, x) = p01(1)

1 0

 -

g(u|0

(1),

v)

 x

Q1(u,

z(w,

x))m0

(v,

w)dvdw

+ p11(1)

1 0

 -

g(u|1(1),

v)

 x

Q1(u,

z(w,

x))m1

(v,

w)dvdw.

(5.9)

Remark V.5. The key observation is that for a Markov switching model with a finite number of d-

hidden states, the invariant measure can essentially be defined by d-functions whose ranges are in the

(d-1)-dimensional space. This is computationally challenging for d  3, and numerically computationally

feasible for d = 2, as the corresponding two-dimensional real-valued functions can be characterized by

a two-dimensional Fredholm integral equation. Note that the Fredholm integral equation is well studied

in mathematics, and the two-dimensional case can be numerically solved by discretizing and then finding

the eigenvector of a (large) square matrix with respect to the eigenvalue.

To illustrate the usefulness of Proposition V.4, we consider the following Markov switching regression model, in which the mean depends on Xt. A more general case of the means µXt and µXt-1 is in Section V-C for the numerical computation.

26

Example V.6. Let {Xn, n  0} be a two-state ergodic (aperiodic, irreducible, and positive recurrent)





Markov

chain

with

transition

probability

matrix

P

=

p00 

p01 

.

Denote

p10 p11

Yt = µXt + Xt Yt-1 + Xt t,

(5.10)

where t  N (0, 1). Denote  = (p00, p11, µ0, µ1, 0, 1, 0, 1). To compute Qj(u, z) in (5.10), note that

g(Yt|0(), u) g(Yt|1(), u)

=

1 0

exp



Yt

+

 

2

-

2 2

+

 

,

where



=

1 212

-

1 202

,

=

µ0

+ 0u 202

-

µ1

+ 1u 212

and



=

- (µ0

+ 0u)2 202

+

(µ1

+ 1u)2 212

.

It follows that Qj(u, z) can be computed as

Qj(u, z) = Pr





1 

log

0 1

z

+

2 2

-

 

,

where   j221((µj + ju + /)2). Once we can approximate the stationary density mj(·, ·) in Proposition V.4, J can be computed as

follows and the Re´nyi divergence can be estimated.

J  = E E exp(log

Ct,0(1) + Ct,1(1) Ct,0() + Ct,1()

-1
) Xt-1, Yt-2, Wt-1

(5.11)

= E E

Ct,0(1) + Ct,1(1) Ct,0() + Ct,1()

-1
Xt-1, Yt-2, Wt-1

1 1 1

=

Pinv(Xt = j|Xt-1 = i)

j=0 0 - i=0

· E

Ct,0(1) + Ct,1(1) Ct,0() + Ct,1()

-1
Xt = j, Xt-1 = i, Yt-2 = v, Wt-1 = w mi(v, w)dvdw

1

=

p00(1)G00(v, w) + p01(1)G01(v, w) m0(v, w)dvdw

0 -

1

+

p10(1)G10(v, w) + p11(1)G11(v, w) m1(v, w)dvdw,

0 -

27

where

Gij (v, w) = E

Ct,0(1) + Ct,1(1) Ct,0() + Ct,1()

-1
Xt = j, Xt-1 = i, Yt-2 = v, Wt-1 = w

(5.12)

=

 -

 -

[p00(1)w + p10(1)(1 - w)]g(y|0(1), u) + [p01(1)w + p11(1)(1 - w)]g(y|1(1), u) -1 [p00()w + p10()(1 - w)]g(y|0(), u) + [p01()w + p11()(1 - w)]g(y|1(), u)

· g(u|i(1), v)g(y|j (1), u)dudy.

Then

D(p||q)

=



1 -

1

log J 

=



1 -

1

log J.

(5.13)

Remark V.7. We can see that this method involves two parts. The first part is solving the eigenvalue to approximate mj, and the second part is using double integration to approximate J. If we carefully design our algorithm, we need only a few seconds to calculate the Re´nyi divergence.

C. Numerical Computation of the Re´nyi Divergence To illustrate our method, in this subsection, we consider the following Markov switching model:

Yt = 1µXt + 2µXt-1 + Yt-1 + t,

(5.14)

where t  N (0, 2) and Xt  X = {0, 1} is a Markov chain with transition probability matrix P =





p00 

p01 

.

Denote



=

(p01,

p10,

µ,

,

1,

2,

).

The

derivation

of

the

Markov

switching

model

(5.14)

p10 p11

for numerical study will be given in the Appendix.

We give a summary of our numerical approximation of the invariant measure  in model (5.11) and

(5.12). Even though there are several ways to solve the two-dimensional Fredholm integral equation, the

algorithm for solving the above integral equation should be carefully designed because 0 is one of its

solutions. To resolve this problem, we vectorize mj and take grid points on (0, 1) × (-, ). Note that

the standard techniques for the general Fredholm integral equation can be used to estimate m0(·, ·), m1(·, ·)

numerically from equation (5.9) in Proposition V.4. The key step is to consider two discrete approximations:

1 0

 -

g(u|(),

v)

 x

Q(u,

z(w, x))m(v,

w)dvdw

and

 x

Q(u,

z(w, x)).

To

this

end,

we

first

approximate

 -

by

-aa, and consider the case that -a = v0 < v1 < · · · < vN-1 < vN = a, 0 = x0 < x1 < · · · <

28

xN -1

<

xN

=

1

and

0

=

w0

<

w1

<

···

<

wN -1

<

wN

=

1.

One

choice

is

to

simply

set

vi

=

2ai N

and

xi = wi

=

i N

for

i = 0, 1, . . . , N.

Assume m(a, 0) = m(a, 1) = m(b, 0) = m(b, 1) = 0.

Then

1 0

a -a

g(u|(),

v)

 x

Q(u,

z(w,

x))m(v,

w)dvdw



1 N

g(u|(),

v1

)

 x

Q(u,

z

(w1

,

x))

m(v1, w1)

+

N -1 j=2

1 2N

g(u|(),

vj

)

 x

Q(u,

z(wj

,

x))

+

g(u|(),

vj

)

 x

Q(u,

z(wj+1

,

x))

m(vj, wj )

and

 x

Q(u,

z(u,

x))



Q(u,

z(u,

x

+

)) - Q(u, 2

z(u,

x

-

))

,

for x = x1, . . . , xi, . . . , xN-1, where  = 1/(2N ).

Now we apply (u, x) = (u1, x1), . . . , (uN-1, xN-1) to equation (5.9) to obtain 2(N - 1) equations, and

use the above approximations to discretize the right-hand side of equation (5.9). Write these 2(N -1) equa-

tions in a matrix form to yield w = Mw, where w = (m0(u1, x1), · · · , m0(uN-1, xN-1), m1(u1, x1), · · · , m1(uN-1, xN-1))t and M is a 2(N -1)×2(N -1) matrix whose entries values depend on Q0(ui, z(wj , xi± )) and Q1(ui, z(wj, xi ± )), both of which can be computed for given values of (ui, xi) and wj. Since Qi(u, z(w, 1)) = 1 and Qi(u, z(w, 0)) = 0 for all w and i = 0, 1, a nice property of the matrix M is that the sum of each column equals 1, which is the main reason why we use the invariant joint density

notation. This property ensures that the matrix M has an eigenvalue (the largest) equal to 1 and thus the

corresponding eigenvector w is an efficient approximation to m0(·, ·) and m1(·, ·). With the discretized approximation m^ 0(·, ·) and m^ 1(·, ·), we can estimate J by approximating the
integration in (5.11), thereby yielding an alternative way to compute the Re´nyi divergence. Now we show

this alternative way is valuable, as the only method so far in the literature to estimate the Re´nyi divergence

in

general

HMM

is

the

Monte

Carlo

simulation

of

1 n

log Sn

for

large

value

of

n.

It

is

expected

that

our

proposed alternative non-Monte-Carlo method will allow one to check the accuracy and correctness of

both methods.

We focus on the following 8 cases listed in Table I, and Table II presents the Re´nyi divergence for

various  = 0.5, 0.8, 0.99, 0.999, 1.001, 1.01, 1.5, 2, and the Kullback­Leibler divergence (  1) for

the above 8 cases with two methods: simulation and numerical approximation. For simulation, we use

the sample size as 2000 and the replication number as 100; for numerical approximation, we use the

discretized lattice number as 16 and the lower and upper integral bounds as 15.

29

TABLE I

PARAMETERS FOR THE EIGHT EXAMPLES.

(1)  = (0.41, 0.6, (1, 0), 0, 1, 0, 2)

1 = (0.41, 0.6, (2, 1), 0, 1, 0, 1.5)

(2)  = (0.41, 0.59, (1, 0), 0, 1, 0, 2)

1 = (0.41, 0.59, (2, 1), 0, 1, 0, 1.6)

(3)  = (0.4, 0.59, (1, 0), 0, 1, 0, 1)

1 = (0.4, 0.59, [2, 1], 0, 1, 0, 0.9)

(4)  = (0.4, 0.599, (1, 0), 0, 1, 0, 1)

1 = (0.4, 0.599, (2, 1), 0, 1, 0, 0.9)

(5)  = (0.59, 0.4, (1, 0), 0, 1, 0, 1)

1 = (0.59, 0.4, (2, 1), 0, 1, 0, 0.9)

(6)  = (0.599, 0.4, (1, 0), 0.2, 1, 0, 1) 1 = (0.599, 0.4, (2, 1), 0.3, 1, 0, 1.1)

(7)  = (0.4, 0.59, (1, 0), 0.2, 1, 0.2, 1.1) 1 = (0.4, 0.59, (2, 1), 0.1, 1, 0.1, 1)

(8)  = (0.4, 0.59, (1, 1), 0, 1, 0, 1)

1 = (0.4, 0.59, (2, 2), 0, 1, 0, 0.9)

Next, we also report the value of the Re´nyi divergence and Kullback­Leibler divergence in this context. We consider two ways to estimate the Re´nyi divergence and Kullback­Leibler divergence. One is based on Monte Carlo simulations with the time step for convenience. The other is based on the invariant probability measure based on 500 discretizations over the interval [0,1]. The corresponding results are summarized in Table II, in which we report Re´nyi divergence based on numerical and Monte Carlo methods. The number in the parentheses indicates the standard deviation of the Monte Carlo estimator. We also present relative errors between these two methods and their corresponding running time in seconds.
From Table II, these two ways yield similar numerical results, and thus the two different methods validate each other. In particular, we feel confident that the sample size and the replication number for the above examples are large enough in the Monte Carlo simulation to estimate the Re´nyi divergence and Kullback­Leibler divergence. Note that the Re´nyi divergence increases as  increases, as that in the i.i.d. case. Furthermore, the Re´nyi divergence and Kullback­Leibler divergence get closer when   1. We also observe that our numerical method is stable for the Kullback­Leibler divergence, whereas it is sensitive to the underlying parameteter change for the Re´nyi divergence. This may be due to the use of the approximted Re´nyi divergence.
VI. CONCLUSION In this paper, we study the Re´nyi divergence for a general HMM, to cover the Markov switching model and RNN, including the classical HMM as a special case. The Kullback­Leibler divergence can be regarded as the limit of   1 of the Re´nyi divergence. Moreover, we express the Kullback­Leibler

30

TABLE II RESULTS FOR CASES WITH VARIOUS 
Cases

Average



1

2

3

4

5

6

7

8 time (sec)

Numerical 0.1091 0.0921 0.2196 0.2211 0.2225 0.2723 0.1227 0.2818 11.60

0.1097 0.0927 0.2211 0.2220 0.2239 0.2733 0.1293 0.2826 239.45

0.5

Simulation

(0.0145) (0.0133) (0.0214) (0.0214) (0.0217) (0.026) (0.015) (0.0247)

R.E. (%) -0.5469 -0.6472 -0.6784 -0.4054 -0.6253 -0.3659 -5.1044 -0.2831

Numerical 0.1533 0.1324 0.3372 0.3387 0.3366 0.4566 0.1939 0.4243 11.68

0.1538 0.1329 0.3382 0.3395 0.3374 0.4575 0.1979 0.4250 240.43

0.8

Simulation (0.0114) (0.0109) (0.0191) (0.0192) (0.0189) (0.0275) (0.0131) (0.0212)

R.E. (%) -0.3251 -0.3762 -0.2957 -0.2356 -0.2371 -0.1967 -2.0212 -0.1647

Numerical 0.1762 0.1541 0.4072 0.4087 0.4032 0.5850 0.2363 0.5062 12.39

0.1767 0.1546 0.4079 0.4094 0.4036 0.5857 0.2388 0.5068 238.79

0.99

Simulation (0.0101) (0.0099) (0.0184) (0.0185) (0.018) (0.0295) (0.0124) (0.0199)

R.E. (%) -0.2830 -0.3234 -0.1716 -0.1710 -0.0991 -0.1195 -1.0469 -0.1184

0.999

Numerical Simulation R.E. (%)

0.1772 0.1777 (0.0101) -0.2814

0.1550 0.1555 (0.0099) -0.3215

0.4104 0.4111 (0.0184) -0.1703

0.4120 0.4127 (0.0184) -0.1696

0.4063 0.4067 (0.018) -0.0984

0.5913 0.5921 (0.0296) -0.1351

0.2382 0.2407 (0.0123) -1.0386

0.5099 0.5105 (0.0198) -0.1175

12.31 239.49

KL (  1)

Numerical Simulation R.E. (%)

0.1773 0.1780 (0.0101) -0.3933

0.1552 0.1558 (0.0099) -0.3851

0.4108 0.4114 (0.0184) -0.1458

0.4123 0.4129 (0.0184) -0.1453

0.4066 0.4070 (0.0179) -0.0983

0.5920 0.5928 (0.0296) -0.1350

0.2386 0.2407 (0.0123) -0.8725

0.5104 0.5106 (0.0198) -0.0392

13.34 235.01

1.001

Numerical Simulation R.E. (%)

0.1774 0.1779 (0.0101) -0.2811

0.1553 0.1557 (0.0099) -0.2569

0.4112 0.4118 (0.0184) -0.1457

0.4127 0.4134 (0.0184) -0.1693

0.4069 0.4073 (0.0179) -0.0982

0.5927 0.5935 (0.0296) -0.1348

0.2387 0.2411 (0.0123) -0.9954

0.5108 0.5113 (0.0198) -0.0978

12.63 238.03

Numerical 0.1784 0.1562 0.4144 0.4159 0.4100 0.5991 0.2406 0.5145 13.39

0.1789 0.1567 0.4150 0.4166 0.4104 0.5999 0.2430 0.5151 235.90

1.01

Simulation (0.01) (0.0098) (0.0184) (0.0184) (0.0179) (0.0298) (0.0123) (0.0198)

R.E. (%) -0.2795 -0.3191 -0.1446 -0.1680 -0.0975 -0.1334 -0.9877 -0.1165

Numerical 0.2248 0.2014 0.5807 0.5823 0.5650 0.9971 0.3418 0.6995 13.23

0.2253 0.2019 0.5806 0.5828 0.5645 0.9967 0.3411 0.7000 232.73

1.5

Simulation (0.0081) (0.0082) (0.0178) (0.0178) (0.0171) (0.0437) (0.0113) (0.0181)

R.E. (%) -0.2219 -0.2476 0.0172 -0.0858 0.0886 0.0401 0.2052 -0.0714

Numerical 0.2601 0.2370 0.7330 0.7345 0.7054 1.5699 0.4364 0.8587 12.49

0.2606 0.2374 0.7321 0.7348 0.7041 1.5548 0.4335 0.8590 232.84

2

Simulation (0.0069) (0.0071) (0.0181) (0.0181) (0.0174) (0.1445) (0.0114) (0.0176)

R.E. (%) -0.1919 -0.1685 0.1229 -0.0408 0.1846 0.9712 0.6690 -0.0349

31
divergence of the general HMM as a top Lyapunov exponent of a well-defined product of Markovian random matrices. Since the Re´nyi divergence involves the largest eigenvalue of the associated Markov operator, which is notoriously difficult to compute, we turn our attention to asymptotic expansions, and derive an approximated Re´nyi divergence, which can be used for numerical approximation based on the Fredholm integral equation.
There are further studies along this line. First, it would be interesting to approximate the largest eigenvalue () to yield a more accurate numerical approximation. Second, we will study the case in which {Xn, n  0} is a general state Markov chain; or a more general HMM to cover regime switching state space models and regime switching GARCH(p, q) (stochastic volatility) models. Last, applications of the Re´nyi divergence and Kullback­Leibler divergence in general HMMs such as model selection, regularization, and variational inference are also interesting and merit further investigation.
REFERENCES Arikan, E. (1996). An inequality on guessing and its application to sequential decoding. IEEE Trans.
Inform. Theory 42, 99­105. Bassat, M. B. and J. Raviv (1978). Re´nyi's entropy and the probability of error. IEEE Trans. Inform.
Theory 24, 324­330. Blackwell, D. H. (1959). The entropy of functions of finite-state Markov chains. Matematika 3(5),
143­150. Blanding, W. R., P. K. Willett, Y. Bar-Shalom, and S. Coraluppi (2009). Multisensor track management
for targets with fluctuating SNR. IEEE Trans. Aerosp. Electron. Syst. 45, 1275­1292. Blumer, A. C. and R. J. McEliece (1988). The Re´nyi redundancy of generalized Huffman codes. IEEE
Trans. Inform. Theory 34, 1242­1249. Bougerol, P. and N. Picard (1992). Stationarity of GARCH processes and some nonnegative time series.
J. Econom. 52, 115­127. Buys, J., Y. Bisk, and Y. Choi (2018). Bridging HMMs and RNNs through architectural transformations. In
Proceedings of the 32nd Conference on Neural Information Processing Systems, pp. IRASL workshop, Montre´al, Canada. Campbell, L. L. (1965). A coding theorem and Re´nyi's entropy. Inform. Contr. 8, 423­429.

32
Cappe, O., E. Moulines, and Ryden (2005). Inference in Hidden Markov Models. Springer-Verlag, New York.
Chan, H. P. and T. L. Lai (2003). Saddlepoint approximations and nonlinear boundary crossing probabilities of Markov random walks. Ann. Appl. Probab. 13, 395­429.
Chen, P. N. and F. Alajaji (2001). Csisza´r's cutoff rates for arbitrary discrete sources. IEEE Trans. Inform. Theory 47, 330­338.
Cho, K., B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing.
Chung, J., Kastner, K., L. Dinh, K. Goel, A. C. Courville, and Y. A. Bengio (2015). Recurrent latent variable model for sequential data. Advances in Neural Information Processing Systems, 2980­2988.
Churchill, G. A. (1989). Stochastic models for heterogeneous DNA sequences. Bull. Math. Biol. 51, 79­94.
Csiszar, I. (1995). Generalized cutoff rates and Re´nyi's information measures. IEEE Trans. Inform. Theory 41, 26­34.
Ephraim, Y. and N. Merhav (2002). Hidden Markov processes. IEEE Trans. Inform. Theory 48, 1518­1569. Erez, U. and R. Zamir (2001). Error exponents of modulo-additive noise channels with side information
at the transmitter. IEEE Trans. Inform. Theory 47, 210­218. Fuh, C. D. (2004a). Asymptotic operating characteristics of an optimal change point detection in hidden
Markov models. Ann. Stat. 32, 2305­2339. Fuh, C. D. (2004b). On Bahadur efficiency of the maximum likelihood estimator in hidden Markov models.
Stat. Sin. 14, 127­144. Fuh, C. D. (2006). Efficient likelihood estimation in state space models. Ann. Statist. 34, 2026­2068.
Corrigendum in 38, 1279­1285, (2010). Fuh, C. D. (2021a). Asymptotic behavior for Markovian iterated function systems. Stoch. Process. Their
Appl. 138, 186­211. Fuh, C. D. (2021b). Asymptotically optimal change point detection for composite hypothesis in state space
models. IEEE Trans. Inform. Theory 67, 485­505. Fuh, C. D. and Y. Mei (2015). Quickest change detection and Kullback-Leibler divergence for two-state
hidden Markov models. IEEE Trans. Signal Process. 63, 4866­4878.

33
Fuh, C. D. and A. G. Tartakovsky (2019). Asymptotic Bayesian theory of quickest change detection for hidden Markov models. IEEE Trans. Inform. Theory 65, 511­529.
Goodfellow, I., Y. Bengio, and A. Courville (2016). Deep Learning. MIT Press. Hamilton, J. D. (1989). A new approach to the economic analysis of nonstationary time series and the
business cycle. Econometrica 57, 357­384. Han, G. and B. Marcus (2006a). Analyticity of entropy rate in families of hidden Markov chains (ii). In
Proceedings of the 2006 IEEE International Symposium on Information Theory, pp. 103­107. IEEE. Han, G. and B. Marcus (2006b). Analyticity of entropy rate of hidden Markov chains. IEEE Trans. Inform.
Theory 52, 5251­5266. Hochreiter, S. and J. Schmidhuber (1997). Long short-term memory. Neural Comput. 9, 1735­1780. Hu, J., M. Brown, and W. Turin (1996). HMM based on-line handwriting recognition. IEEE Trans. Pattern
Anal. Mach. Intell. 18, 1039­1045. Jacquet, P., G. Seroussi, and W. Szpankowski (2008). On the entropy of a hidden Markov process. Theor.
Comput. Sci. 395, 203­219. Jelinek, F. (1968). Buffer overflow in variable length coding of fixed rate sources. IEEE Trans. Inform.
Theory 14, 490­501. Jenssen, R., K. Hild, D. Erdogmus, J. C. Principe, and T. Eltoft (2003). Clustering using renyi's entropy. In
Proceedings of the International Joint Conference on Neural Networks, 2003., Volume 1, pp. 523­528. IEEE. Khasminskii, R. and O. Zeitouni (1996). Asymptotic filtering for finite state Markov chains. Stoch. Process. Their Appl. 63, 1­10. Koopmans, L. H. (1960). Asymptotic rate of discrimination for Markov processes. Ann. Math. Stat. 31, 982­994. Kunda, A., Y. He, and P. Bahl (1989). Recognition of handwritten word: First and second order hidden Markov model based approach. Pattern Recognit. 18, 283­297. Metelli, A. M., M. Papini, F. Faccio, and M. Restelli (2018). Policy optimization via importance sampling. Advances in Neural Information Processing Systems 31, 5442­5454. Meyn, S. P. and R. L. Tweedie (2009). Markov Chains and Stochastic Stability. Cambridge University Press. Miller, J. and M. Hardt (2018). Stable recurrent models. In Proceedings of International Conference on

34
Learning Representations. Nemetz, T. (1974). On the -divergence rate for Markov-dependent hypotheses. Probl. Contr. Inform.
Theory 3, 147­155. Ney, P. and E. Nummelin (1987). Markov additive processes I. Eigenvalue properties and limit theorems.
Ann. Probab. 15, 561­592. Ordentlich, E. and T. Weissman (2004). New bounds on the entropy rate of hidden Markov processes. In
Proceedings of Information Theory Workshop, pp. 117­122. IEEE. Ordentlich, E. and T. Weissman (2006). On the optimality of symbol by symbol filtering and denoising.
IEEE Trans. Inform. Theory 52, 19­40. Papini, M., A. M. Metelli, L. Lupo, and M. Restelli (2019). Optimistic policy optimization via multiple
importance sampling. In Proceedings of International Conference on Machine Learning, pp. 4989­4999. PMLR. Pronzato, L., H. P. Wynn, and A. A. Zhigljavsky (1997). Using Re´nyi entropies to measure uncertainty in search problems. Lectures Appl. Math. 33, 253­268. Rabiner, L. R. (1989). A tutorial on hidden Markov models and selected applications in speech recognition. Proceedings of the IEEE 77(2), 257­286. Rabiner, L. R. and B. H. Juang (1993). Fundamentals of Speech Recognition. Prentice Hall, New Jersey. Rached, Z., F. Alajaji, and L. L. Campbell (2001). Re´nyi's divergence and entropy rates for finite alphabet Markov sources. IEEE Trans. Inform. Theory 47, 1553­1561. Raghavan, V., A. Galstyan, and A. G. Tartakovsky (2013). Hidden Markov models for the activity profile of terrorist groups. Ann. Appl. Stat. 7, 2402­2430. Raghavan, V., G. Steeg, A. Galstyan, and A. G. Tartakovsky (2014). Modeling temporal activity patterns in dynamic social networks. IEEE Trans. Computat. Social Syst. 1, 89­107. Re´nyi, A. et al. (1961). On measures of entropy and information. In Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics. The Regents of the University of California. Tugac¸, S. and M. Efe (2010). Hidden Markov model based target detection. In 2010 13th international conference on information fusion, pp. 1­7. IEEE. Vasuhi, S. and V. Vaidehi (2014). Target detection and tracking for video surveillance. WSEAS Trans. Signal Process. 10, 168­177.

35
Wu, C., E. L. Xu, and G. Han (2017). Re´nyi entropy rate of hidden Markov processes. In 2017 IEEE International Symposium on Information Theory (ISIT), pp. 2970­2974. IEEE.
Yamato, J., J. Ohya, and K. Ishii (1992). Recognizing human action in time-sequential images using hidden Markov model. In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 379­385.
Zucchini, W. and I. MacDonald (2009). Hidden Markov Models for Time Series: An Introduction Using R. Chapman & Hall, New York.
Zuk, O., I. Kanter, and E. Domany (2005). Asymptotics of the entropy rate for a hidden Markov process. In Proceedings of the Data Compression Conference, pp. 173­182. IEEE.

VII. APPENDIX A. Theoretical Study of the Invariant Probability in Model (5.14)
We consider the Markov switching model in (5.14),

Yt = 1µXt + 2µXt-1 + Yt-1 + t,

where {Xt, t  0} is a Markov chain on a state space X = {0, 1}, with transition probability matrix





P

=

p00 

p01 

.

Here

we

omit



in

pij

to

simplify

the

notation.

We

will

add

1

in

pij

as

pij1

when

p10 p11

the probability is P = P1 .

We approach this problem by reformulating model (5.14) as a 1-order four-state Markov switch model.

To do so, we first create a variable Zn such that

· Zt = 0 if (Xt-1, Xt) = (0, 0), Zt = 1 if (Xt-1, Xt) = (0, 1),

· Zt = 2 if (Xt-1, Xt) = (1, 0), Zt = 3 if (Xt-1, Xt) = (1, 1).

Then {Zt, t  0} is a Markov chain on state space X = {0, 1, 2, 3}, with transition probability matrix





p00 p01 0 0





P

=

  

0

0

p10

p11

  

.

p00 p01 0

0

 





0 0 p10 p11

36

Denote the conditional density function of Yt given Yt-1 and (Xt-1, Xt) = (i, j) as fij,(Yt|Yt-1). Then, we can represent the joint probability of Y1, . . . , Yn as

p(Y1, . . . , Yn) = Mn . . . M1 ,

(6.1)

where





p00p10/(p01 + p10)







=

p01p10/(p01 

+

p10

) 

,

p10p01/(p01 + p10)





p11p01/(p01 + p10)



f00, (Y1 )





M1

=

 

0

 

0



0

0 f01, (Y1 )
0 0

0 0 f10, (Y1 ) 0

 0



0



 

,

0

 

 f11, (Y1 )

and for t  2,



p00 f00, (Yt |Yt-1 )



Mt

=

p01 f01, (Yt |Yt-1 ) 

 

0



0

0 0 p10 f10, (Yt |Yt-1 ) p11 f11, (Yt |Yt-1 )

p00 f00, (Yt |Yt-1 ) p01 f01, (Yt |Yt-1 )
0 0

 0



0



 

.

p10 f10, (Yt |Yt-1 )



p11 f11, (Yt |Yt-1 )

Let

At, = p00(At-1, + Ct-1,)f00,(Yt|Yt-1), Bt, = p01(At-1, + Ct-1,)f01,(Yt|Yt-1),

Ct, = p10(Bt-1, + Dt-1,)f10,(Yt|Yt-1), Dt, = p11(Bt-1, + Dt-1, )f11,(Yt|Yt-1),

with initial value (A1,, B1,, C1,, D1,) = M1. Then we can calculate p(Y1, . . . , Yn) recursively by

p(Y1, . . . , Yn) = An, + Bn, + Cn, + Dn,.

The log-joint probability of Y1, . . . , Yn can be computed as
n
log p(Y1, . . . , Yn) = log(At, + Bt, + Ct, + Dt,),
t=1

where

At,

=

At-1,

p00(At-1, + Ct-1,) + Bt-1, + Ct-1, +

Dt-1,

f00, (Yt |Yt-1

),

Bt,

=

At-1,

p01(At-1, + Ct-1,) + Bt-1, + Ct-1, +

Dt-1,

f01, (Yt |Yt-1

),

Ct,

=

At-1,

p10(Bt-1, + Dt-1,) + Bt-1, + Ct-1, +

Dt-1,

f10, (Yt |Yt-1

),

Dt,

=

At-1,

p11(Bt-1, + Dt-1,) + Bt-1, + Ct-1, +

Dt-1,

f11, (Yt |Yt-1

),

37

with initial value (A1,, B1,, C1,, D1,) = M1. Then the Kullback­Leibler divergence and Re´nyi divergence can be computed as follows. Here we
compute only the Kullback­Leibler divergence; the computation of the Re´nyi divergence can be done
as that in (5.12). To start with, the Kullback­Leibler divergence can be computed by J1 - J, where J = E log(At, + Bt, + Ct, + Dt,) .
Define Wt = (At, + Ct,)/(At, + Bt, + Ct, + Dt,), and the stationary density function satisfying
x
P(Xt-1 = j, Xt = k, Yt-1 = u, Wt  x) = mjk(u, w)dw.
0
Then we can express J as

J = E log(At, + Bt, + Ct, + Dt,)

= E E log(At, + Bt, + Ct, + Dt,) Xt-2, Xt-1, Yt-2, Wt-1

111 1

=

P(Xt = k|Xt-1 = j)

i=0 j=0 k=0 0 -

· E log(At, + Bt, + Ct, + Dt,) Xt-2 = i, Xt-1 = j, Xt = k, Yt-2 = v, Wt-1 = w mij(v, w)dvdw

1

=

p001 G000(v, w) + p011 G001(v, w) m00(v, w)dvdw

0 -

1

+

p101 G010(v, w) + p111G011(v, w) m01(v, w)dvdw

0 -

1

+

p001 G100(v, w) + p011G101(v, w) m10(v, w)dvdw

0 -

1

+

p101 G110(v, w) + p111G111(v, w) m11(v, w)dvdw,

0 -

where

Gijk(v, w) = E log(At, + Bt, + Ct, + Dt,) Xt-2 = i, Xt-1 = j, Xt = k, Yt-2 = v, Wt-1 = w



=

log p00wf00,(y|u) + p01wf01,(y|u)

- -

+ p10(1 - w)f10,(y|u) + p11(1 - w)f11,(y|u) fij,1(u|v)fjk,1(y|u)dudy.

38

It remains to approximate m^ ij(·). Note that

P(Xt-1 = j, Xt = k, Yt-1 = u, Wt  x)

11 1

=

P(Xt-1 = j, Xt = k, Yt-1 = u, Wt  x|Xt-2 = i, Xt-1 = l, Yt-2 = v, Wt-1 = w)

i=0 l=0 0 -

· mil(v, w)dvdw

1 1

=

P(Xt-1 = j, Xt = k, Yt-1 = u, Wt  x|Xt-2 = i, Xt-1 = j, Yt-2 = v, Wt-1 = w)

i=0 0 -

· mij(v, w)dvdw

1 1

=

P(Xt = k|Xt-1 = j)fij,1(u|v)

i=0 0 -

· P(Wt  x|Xt-2 = i, Xt-1 = j, Xt = k, Yt-1 = u, Wt-1 = w)mij (v, w)dvdw

1 1

=

P(Xt = k|Xt-1 = j)fij,1(u|v)Qjk(x; u, w)mij (v, w)dvdw,

i=0 0 -

where

Qjk(x; u, w) = P(Wt  x|Xt-2 = i, Xt-1 = j, Xt = k, Yt-1 = u, Wt-1 = w)

= P

At,

At, + Ct, + Bt, + Ct, + Dt,

x

Xt-2

= i, Xt-1

=

j, Xt

=

k, Yt-1

= u, Wt-1

=

w

= P (1 - x)p00wf00,(Yt|u) - xp01wf01,(Yt|u) + (1 - x)p10(1 - w)f10,(Yt|u)

- xp11(1 - w)f11,(Yt|u)  0 Xt-1 = j, Xt = k, Yt-1 = u .

39

It follows that

m00(u, x) = m01(u, x) = m10(u, x) = m11(u, x) =

1 0

 -

p001 f00,1

(u|v)

 x

Q00(x;

u,

w)m00

(v,

w)dvdw

+

1 0

 -

p001 f10,1

(u|v)

 x

Q00(x;

u,

w)m10

(v,

w)dvdw

1 0

 -

p011 f00,1

(u|v)

 x

Q01(x;

u,

w)m00

(v,

w)dvdw

+

1 0

 -

p001 f10,1

(u|v)

 x

Q01(x;

u,

w)m10

(v,

w)dvdw

1 0

 -

p101 f01,1

(u|v)

 x

Q10(x;

u,

w)m01

(v,

w)dvdw

+

1 0

 -

p101 f11,1

(u|v)

 x

Q10(x;

u,

w)m11

(v,

w)dvdw

1 0

 -

p111 f01,1

(u|v)

 x

Q11(x;

u,

w)m01

(v,

w)dvdw

+

1 0

 -

p111 f11,1

(u|v)

 x

Q11(x;

u,

w)m11

(v,

w)dvdw.

Note that in the model we consider in (5.14), the error term t is a normal distribution with probability

density function

fij,(Yt|u)

=

1 22

exp

-

[Yt

-

(2µi

+ 1µj 22

+

u)]2

=  1 e-Yt2/(22) exp 22

2Yt(2µi + 1µj + u) - (2µi + 1µj + u)2 22

.

It follows that

Qjk(x; u, w) = P

(1 - x)p00w exp

2Yt(2µ0 + 1µ0 + u) - (2µ0 + 1µ0 + u)2 22

- xp01w exp

2Yt(2µ0 + 1µ1 + u) - (2µ0 + 1µ1 + u)2 22

+ (1 - x)p10(1 - w) exp

2Yt(2µ1 + 1µ0 + u) - (2µ1 + 1µ0 + u)2 22

- xp11(1 - w) exp

2Yt(2µ1 + 1µ1 + u) - (2µ1 + 1µ1 + u)2 22

 0 Xt-1 = j, Xt = k, Yt-1 = u .

