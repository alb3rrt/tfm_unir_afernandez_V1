Fourier Space Losses for Efficient Perceptual Image Super-Resolution

Dario Fuoli1 Luc Van Gool1,2 Radu Timofte1
1ETH Zurich, Switzerland 2KU Leuven, Belgium
{dario.fuoli, vangool, radu.timofte}@vision.ee.ethz.ch

arXiv:2106.00783v1 [eess.IV] 1 Jun 2021 LPIPS

Abstract
Many super-resolution (SR) models are optimized for high performance only and therefore lack efficiency due to large model complexity. As large models are often not practical in real-world applications, we investigate and propose novel loss functions, to enable SR with high perceptual quality from much more efficient models. The representative power for a given low-complexity generator network can only be fully leveraged by strong guidance towards the optimal set of parameters. We show that it is possible to improve the performance of a recently introduced efficient generator architecture solely with the application of our proposed loss functions. In particular, we use a Fourier space supervision loss for improved restoration of missing high-frequency (HF) content from the ground truth image and design a discriminator architecture working directly in the Fourier domain to better match the target HF distribution. We show that our losses' direct emphasis on the frequencies in Fourier-space significantly boosts the perceptual image quality, while at the same time retaining high restoration quality in comparison to previously proposed loss functions for this task. The performance is further improved by utilizing a combination of spatial and frequency domain losses, as both representations provide complementary information during training. On top of that, the trained generator achieves comparable results with and is 2.4× and 48× faster than state-of-the-art perceptual SR methods RankSRGAN and SRFlow respectively.
1. Introduction
Super-resolution (SR) deals with the problem of reconstructing the high-frequency (HF) information from a lowresolution (LR) image x  RH×W ×C , which are inherently lost after downsampling the high-resolution (HR) image y  RrH×rW ×C due to the lower Nyquist frequency in the LR space (r denotes the scaling factor). Recent single image SR (SISR) methods [3, 16, 20, 18, 9, 13] have shown remarkable success at reconstructing the missing HF details, with emphasis on accurate restoration of the fre-

0.28 0.26

Ours (L1 only)

0.24

0.22

0.20 0.18

Ours (WaveletSRNet loss)

0.16

0.14

0.12 0.10

Ours (Full)

RankSRGAN

ESRGAN

102

103

Runtime [ms]

SRFlow

Figure 1. Runtime [ms] vs. perceptual quality (LPIPS) [35] comparison with state-of-the-art methods. The disk area is proportional to the number of parameters. We achieve the fastest runtimes with comparable perceptual quality to much larger networks.

quency content in the ground truth frames. This is typically performed with supervised training, where the ground truth images y are downsampled with a known kernel, e.g. bicubic, to obtain the LR input images x.
While it may be desirable in some applications to restore the frequencies as close to the target as possible with minimal assumptions, the ill-posed problem limits the SR networks to generate higher frequency components, as the training promotes conservative estimates imposed by the pixel-wise supervision losses. This usually results in blurry images, which appear to be of lower quality than their respective HR counterparts.
This issue has been addressed in the literature [19, 31] by employing different losses, that are designed to promote the higher frequencies for perceptually more pleasing images. These supervised objectives are often used in combination with generative adversarial networks [7] (GAN) for additional distribution learning of the HF space. Conditional GAN-based learning enables the generation of plausible high frequencies without the need for ground truth accu-

1

racy. A lot of research has been devoted to design such perceptual losses and to find suitable combinations for pleasing results.
Today, more and more deep learning based algorithms are implemented on smartphones, which requires lowcomplexity networks for fast inference and inexpensive deployment. Therefore, the design focus is slowly shifting from high-quality, high-performance methods with highcomplexity networks to more efficient enhancers, which upscale much faster and require less resources. In contrast to empowering a deep neural network's performance by simply increasing it's complexity, which is generally straight-forward, finding an efficient network with highperformance is a much harder challenge. Searching for effective low-complexity networks with high performance, that are on par with state-of-the-art methods, is the ultimate challenge in network design.
Three main ingredients are necessary in order to maximize performance and efficiency of deep neural networks. First, the best architecture design for the task has to be determined. Usually, this task is performed manually by experts. In addition to handcrafted designs, neural architecture search algorithms [6, 5] have recently been proposed to automate this task. Second, the design of the optimal loss function is imperative to fully leverage a network's performance. Third, the amount and quality of data plays a key role to maximize performance. A large portion of existing literature in SR deals with the first point. We regard the solution to the third point as straight-forward, as data can be collected efficiently for most applications. In this paper we propose a solution to the second point and try to maximize the performance of a recently proposed efficient low-complexity network [13, 34] for perceptual SR, solely by the application of our proposed loss functions.
The design of perceptual losses predominantly focuses on the spatial domain [31, 19]. However, SR is tightly coupled to the frequency domain, as only high frequencies are removed during the downsampling process. We leverage this fact and propose novel loss functions in Fourier space by calculating the frequency components with the fast Fourier transform (FFT) for direct emphasis on the frequency content. We propose a supervision loss in direct reference to the ground truth directly in Fourier domain for accurate reconstruction. Additionally, we propose a discriminator architecture to learn the HF distribution in an adversarial training setup working directly in Fourier space. To the best of our knowledge we are the first to apply a GAN loss directly on Fourier coefficients in SR. Our ablation study shows clear benefits over spatial losses for the task of perceptual SR. Also, employing a loss in Fourier space introduces global guidance as opposed to pixel-wise evaluation due to the nature of the Fourier transform. In order to leverage both global and local guidance, we also

add the corresponding spatial supervision and GAN losses. Together with an additional perceptual loss, this outperforms all other configurations in our ablation study. In addition to the advantage of our proposed losses over existing ones, we compare our trained efficient generator with highperformance state-of-the-art methods. It shows, that our losses can substantially increase the performance of a lowcomplexity generator to even compete with much larger networks.
2. Related Work
SR is a popular topic and a series of competitions are conducted by [30, 1, 2, 34, 32, 33, 21] which provide a broad overview of research and development over recent years in this area. Restoration Learning based approaches have shown to be highly effective so solve the problem of SR and are therefore predominantly used in research. SRCNN [3] is one of the first convolutional neural network (CNN) based methods to surpass non-CNN SR algorithms, VDSR [16] is an improved version which adopts a deeper network for improved performance. Further concepts and improvements are explored [19, 20, 18, 9, 13] with the aim of reconstructing the missing details in a LR image as close to the ground truth as possible. Perceptual SR Since even the best of the aforementioned methods tend to produce blurry images, another family of methods [19, 31, 36] tries to further improve the perceptual image quality by sacrificing restoration quality for increased generation of HF content [2]. For that matter, SRGAN [19] proposes the application of a generative adversarial network (GAN) [7] to better model the HF distribution in an image. The authors also propose a perceptual loss, based on features of VGG [29], which significantly boosts the perceptual quality. ESRGAN [31] extends this concept by adopting an improved GAN-loss formulation [15] and a stronger generator architecture. RankSRGAN [36] is another approach to achieve improved perceptual image quality. It uses a ranker to enable gradient based training with non-differentiable handcrafted no-reference image quality metrics. First, a dataset with pairs of images and their calculated quality score is prepared, then a ranker is trained to relatively rank two images in a differentiable manner. The learned differentiable ranker is then used in a gradient based adversarial training setup. More recently, SRFlow [22] uses normalizing flows [27] for perceptual image SR. The method explicitly models the ambiguity in HR space and is trained by maximimum likelihood with the use of a network that is invertible by design. Frequency-based SR Since SR is the problem of restoring frequency components, several works [11, 4, 8, 14] propose to model the problem closer to frequency space in various configurations. WaveletSRNet [11] uses wavelets to

2

decompose the LR image by the Haar transform and generates the missing HF wavelet coefficients instead of HR images directly. Additionally, the losses are optimized for perceptual image quality by weighing the wavelet coefficients by some heuristic, in order to balance the importance of different sub-bands. DWSR [8] uses a similar approach without a weighting scheme and uses only four sub-bands, without explicit perceptual components. The loss in [11] is composed of more sub-bands, but it does not fully decompose the image as we do by applying the Fourier transform. A more recent work [14] proposes a supervision loss in Fourier space as additional loss for generative tasks. However, this work uses a different loss formulation, i.e. it computes the differences directly between the complex components without transformation into amplitude and phase. On top of that, to the best of our knowledge, we are the first to also employ a GAN loss directly in Fourier space.
3. Proposed Method
The task of image SR, is to increase the resolution of an image x  RH×W ×C from the LR domain X to the corresponding image y  RrH×rW ×C in HR domain Y with factor r. According to Nyquist­Shannon's sampling theorem, the missing HF content above the Nyquist-frequency nc must be recovered in order to get an image y from the target HR domain Y. In contrast to the representation of an image in spatial domain, these missing frequencies can be clearly separated in Fourier domain. We therefore propose two losses in the frequency domain, to directly emphasize the training on the relevant frequencies. Additionally, the frequency components provide global guidance during training due to the nature of the Fourier transform.
3.1. Generator
Our aim is to reduce the computational complexity of the generator network for faster runtimes, while retaining the representational power for SR as high as possible. Therefore, the design of more effective losses is imperative. Improving the loss design can yield stronger gradient signals which better guide the generator during the training process. In order to test the effectiveness of our proposed losses, we use a lightweight model based on the IMDN network [13] from the same authors. This is the winner of the "AIM 2019 Challenge on Constrained SR" [34]. The network is used as an example of an efficient generator architecture to showcase the power of our loss designs against typical existing losses. The network consists of repeated information multidistillation blocks (IMDBs), that are designed to effectively integrate information from the LR-space towards the HRspace. The whole processing is conducted in LR-space for efficiency reasons. Only in the last processing step, the refined HR image is upsampled with a standard shuffling block [28]. generator G super-resolves a LR image

x  RH×W ×C into a HR image y^ = G(x)  RrH×rW ×C .
3.2. Fourier Transform and SR
The Fourier transform is widely used to analyze the frequency content in signals. It can also be applied to multidimensional signals such as images, where the spatial variations of pixel-intensities have a unique representation in the frequency domain. The discrete Fourier transform (DFT) decomposes an image x  RH×W ×C from the spatial domain into the Fourier domain. The Fourier space is spanned by complex orthonormal basis functions, where the complex frequency components X  CU×V ×C characterize the image.

1 F {x}u,v = Xu,v = 

H-1 W -1

xh,w e-i2(u

h H

+v

w W

)

H W h=0 w=0

(1)

Since images are composed of multiple color channels,

we calculate the Fourier transform for each channel sep-

arately and perform the transform per channel. The ex-

plicit notation of channels is omitted in our formulas. Each

complex component Xu,v can be represented by amplitude |F {x}u,v| and phase F {x}u,v, which provides a more in-

tuitive analysis of the frequency content.

|F {x}u,v| = |Xu,v| = R{Xu,v}2 + I{Xu,v}2 (2)
F {x}u,v = Xu,v = atan2(I{Xu,v}, R{Xu,v}) (3)
Due to symmetry in the Fourier space (Hermitian symmetry) for real valued signals x, we can omit redundant spectral components and only treat half of X, and still retain the full information in x.

F {x}u,v = F {x}-u,-v

(4)

Thus, processing can be significantly reduced by neglecting redundant components when working in the Fourier domain of real-valued signals like images. Note, despite discarding the redundant values, the total number of values in the spatial and Fourier domain remains the same since the components in Fourier space are composed of real and imaginary part (or amplitude and phase).
Since the Fourier transformation assumes an infinite signal in the transformation dimensions, finite signals like images should be preprocessed to avoid edge induced artifacts. We avoid such artifacts by applying a Hann window, which suppresses the signals' amplitude towards the edges in order to smooth out the transitions. Afterwards, the image is transformed with a more accurate representation of the frequency spectrum.
As SR is the task of reconstructing the missing HF content from a downscaled image, a reduction in the sampling

3

Spatial Domain

Fourier Domain
Figure 2. Overview of the proposed method. We employ losses in both spatial and Fourier domain to strengthen the training signal.

rate leads to a lower Nyquist-frequency nc in the LR-space, which constitutes a hard limit in the representation capability of high frequencies above said frequency. Therefore, SR deals with the problem of generating these missing frequencies, which can be seen as the extrapolation from low to high frequencies. Contrary to the representation of an image in the spatial domain, these frequencies can be clearly separated in the frequency space in order to directly emphasize the important image features for SR. Additionally, the Fourier components provide global information about the image as opposed to local information represented by pixel values in the spatial domain. We leverage these properties to design new losses for efficient perceptual SR training.
In contrast to the Fourier transform, wavelet-transforms balance local- and global frequency precision in an image by dividing the frequency content into different subbands. This property is useful for many practical applications where this information is needed. However, we are not forced to find a balance. For application in our losses, we can both leverage the global frequency content with maximal precision represented by one component for each frequency in the signal and get precise local guidance through the spatial representation of the image.
3.3. Supervision Losses
For perceptual SR, predominantly spatial domain based losses, spatial feature losses, or frequency-band separation strategies in the spatial domain, e.g. separation by wavelet decomposition or filtering, are proposed [31, 4]. Presumably, because most existing architectures are based on convolutions that expect spatial invariance in the input and also due to easy handling of variable image sizes of convolutional networks. Popular choices for supervision losses, i.e. with reference to a ground truth, are pixel-based losses L1/L2 and feature based VGG-loss [29, 19]. As proposed in [31] and for direct comparison, we investigate L1 (5) and VGG-loss (6).

1 H-1 W -1

LL1 = HW

||y^h,w - yh,w||1

(5)

h=0 w=0

1 I-1 J-1 LV GG = IJ

NV54GG(y^)i,j - NV54GG(y)i,j

1

i=0 j=0

(6)

Following the setting in [31] we calculate a VGG-loss

using the pre-trained 19-layer VGG network. In particular, the L1-norm between features NV54GG(·) (54 indicates 4th convolution before the 5th pooling layer) from generator

output y^ = G(x) and the target y constitutes the VGG-loss.

In addition to these spatial domain losses, we propose a

Fourier space loss LF for supervision from the ground truth frequency spectrum during training. First, ground truth y

and generated image y^ are pre-processed with a Hann win-

dow, as described in Section 3.2. Afterwards, both im-

ages are transformed into Fourier space by applying the fast

Fourier transform (FFT), where we calculate amplitude and

phase of all frequency components. The L1-norms of am-

plitude difference LF,|·| and phase angle difference LF, between output image and target are averaged to produce

the total frequency loss LF . Note, since 50% of all frequency components are redundant, the summation for u is

performed up to U/2 - 1 only, without affecting the loss

due to Eq. (4).

2 U/2-1 V -1 LF,|·| = U V

|Y^ |u,v - |Y |u,v

1

(7)

u=0 v=0

2 U/2-1 V -1 LF, = U V

min(Y^u,v - Yu,v)

1

(8)

u=0 v=0

1

1

LF = 2 LF,|·| + 2 LF,

(9)

Theoretical benefits of applying a supervision loss in Fourier domain are two-fold. (1) The direct emphasis, especially on the missing HF components, promotes generation in these important areas as opposed to spatial losses (L1/L2), which are known to produce blurry images. (2) Due to the nature of the Fourier transform, which computes

4

Flatten FC/1024 FC/1024 FC/1024 FC/1024
FC

1 234
5
Figure 3. Proposed Fourier GAN architecture. We process the Fourier components of y and y^ with a fully connected network to predict real s and fake s scores.
the frequency content with highest precision in trade-off for spatial precision, the loss is directly calculated on the global frequency components and therefore provides global guidance during training in contrast to local pixel-based losses in spatial domain.
In theory, it would be sufficient to calculate the loss between the HF components only, due to Nyquist­Shannon's sampling theorem (see Sec. 3). However, because of resampling and (imperfect) anti-aliasing filtering during the downscaling process, there will be mismatches between the low-frequency components as well. Applying the loss also on these frequencies, improves the restoration of lowfrequency content in y^.
In contrast to other frequency-based losses, proposed in the literature, we directly apply the losses in Fourier space, and do not tune our losses according to some heuristic, as in [11].
3.3.1 GAN Losses
In order to further boost the perceptual quality we employ a GAN training scheme with two types of GAN-architectures, applied in spatial and Fourier domain. Learning the mapping from LR to HR directly from the ground truth severely limits the generation of images with high perceptual quality. Minimizing the risk towards a single realisation represented by the ground truth is too strict because the problem is illposed. A GAN training strategy relaxes the loss formulation by allowing plausible HR reconstructions resembling images from the target distribution.
We use the discriminator from [31] for our spatial GAN loss LSGAN . Additionally, we design a discriminator working directly in Fourier domain for our proposed frequency domain GAN-loss LFGAN . After the transformation of an image into Fourier space, the spatial invariance assumption is no longer valid. Therefore, the application of a convolutional architecture will not be optimal for this task. Thus, we apply a fully connected discriminator network for adversarial guidance in Fourier space, see Fig. 3. Again, generated image y^ and ground truth y are transformed into frequency components represented by amplitude and phase in Fourier space after the application of a Hann window.
Both adversarial losses are evaluated by a relativistic

GAN formulation [15], which showed improved performance in SR over the standard GAN formulation in [31]. The discriminator's real and fake logits s = D(y), s = D(y^) = D(G(x)) are processed with the relativistic transformation by averaging the logits over the batch dimension b, Eq. (10), and subtracting them from the original logits, Eq. (11). The transformed real and fake scores ,  are then evaluated with the standard sigmoid cross-entropy GANobjective in (12).

1B

1B

s = B s(b), s = B s(b)

(10)

b

b

 =D(y) - s,  = D(y^) - s

(11)

LGGAN = - Ex,y [log( ()) + log(1 -  ())] LDGAN = - Ex,y [log( ()) + log(1 -  ())]

(12)

3.4. Training Setup

The complete training setup (13) consists of two supervision losses and two GAN-losses in both spatial and Fourier domain and an additional VGG-loss. These loss components are weighted with factors , ,  and minimized with Adam [17] optimizer in alternating steps.

min  LGG,ASN + LGG,AFN

G

2

min  LDGA,SN + LDGA,FN

D

2

+  LL1 + LF 2

+ LV GG (13)

4. Experiments and Results
All settings are trained on the DF2K dataset with a scaling factor of r = 4. DF2K is a combination of DIV2K [1] and Flickr2K [30]. Training pairs consist of paired crops of size 64 × 64 and 256 × 256 from LR and HR respectively. We evaluate all experiments on the DIV2K validation set, the standard benchmark for HR image SR. Additionally, we provide results on Urban100 [12].
We calculate restoration metrics PSNR and SSIM, perceptual metric LPIPS [35] and distributional similarity by FID [10, 26]. We deliberately refrain from using noreference metrics, since we want to learn the image quality from the target domain Y, which is different to learning for a no-reference metric, as these handcrafted metrics do not necessarily correlate with the properties of the target image distribution.

5

Configuration
1 2 3 4 5 6 7 8

Generator
IMDN [13] IMDN [13] IMDN [13] IMDN [13] IMDN [13] IMDN [13] IMDN [13] IMDN [13]

LL1 LF LSGAN LFGAN LV GG PSNR SSIM LPIPS FID

30.56 0.837 0.270 22.91

29.53 30.32 27.94 29.06 27.96 29.13 28.42

0.811 0.834 0.751 0.796 0.762 0.794 0.776

0.189 0.266 0.131 0.129 0.127 0.127 0.124

16.98 21.96 17.07 17.17 16.94 17.90 15.88

9

ESRGAN [31]

10

ESRGAN [31]

28.63 0.780 0.113 14.80 28.19 0.769 0.115 15.37

Table 1. Ablation study results. We compare different configurations of loss functions. We calculate restoration metrics PSNR and SSIM, perceptual metric LPIPS [35] and distributional similarity by FID [10]. The metrics are calculated on the DIV2K validation dataset.

4.1. Ablation
We conduct an ablation study with different loss configurations to show the effectiveness of our proposed Fourier domain losses, see Tab. 7. The generator is initialized with pretrained weights (L2) in all configurations and trained on DF2K for 500k iterations with a constant learning rate l = 10-5 and a batch size of B = 16. We do not use a learning rate scheduler for stability reasons and fairness due to the heterogeneous combinations of different loss types. The training parameters are set to  = 0.005,  = 0.01 and  = 1 as proposed in state-of-the-art method ESRGAN [31]. The averaging by factor 2 in (13) is removed whenever a single loss is employed per parameters  or , to keep the balance between supervision and GAN-losses in all configurations. Additionally, we refine the pretrained generator from ESRGAN with our additional losses in the same setting with B = 8.
A comparison between configuration 1 and 2 clearly shows the effectiveness of our proposed Fourier domain supervision loss LF for perceptual quality enhancement. Calculating the losses with our proposed formulation significantly improves the perceptual image quality in trade-off with restoration quality [2], which is reflected by the large improvement of LPIPS (-0.081) and FID (-5.93).
Configuration 4 represents the loss formulation from ESRGAN [31], these spatial losses are exchanged by our proposed Fourier domain losses LF and LFGAN in configuration 5. The perceptual quality remains comparable between the two configurations. However, the restoration quality is significantly higher compared to the ESRGAN losses by a large margin, reflected by a gain in PSNR and SSIM of +1.12dB and +0.045 respectively, which already shows the superiority of our proposed Fourier domain losses over the corresponding spatial losses employed in ESRGAN.
Configuration 8 shows the effectiveness of our proposed Fourier domain losses in combination with spatial losses. It

achieves the best LPIPS and FID scores of all configurations and clearly outperforms the losses of ESRGAN in configuration 4 in all metrics. Simultaneous application of losses in both spatial and frequency domain leverages complementary information from each image representation to significantly improve overall guidance during training. Configuration 9 shows the combination of ESRGAN generator with our proposed full combination of Fourier domain and spatial losses. We note the improvement (PSNR +0.44dB, FID -0.57) brought by our Fourier domain losses over the original ESRGAN in configuration 10.
4.2. Comparison with State-of-the-art
In addition to the effectiveness of our losses for performance in perceptual SR in our ablation study, we show that we can also compete with state-of-the-art methods in terms of performance with a more efficient generator network, due to our better losses. We tweak the loss weights towards higher perceptual quality in trade-off with restoration quality and set them to  = 0.0025,  = 0.005,  = 1 for our model in Tab. 3 and Tab. 4. Note, the proposal of our losses is to showcase the improved training performance which enables to train high-performance low-complexity generators, not necessarily to achieve state of the art performance. Despite the low complexity of G in our setting, we are able to compete with image quality of the best state-of-the-art methods, with a substantial reduction of runtime.
ESRGAN uses a combination of L1, VGG and GAN loss and propose an improved generator architecture derived from SRGAN [19]. RankSRGAN [36] proposes a method to use non-differentiable handcrafted image quality metrics (Ma [23], NIQE [25] and PI [2]) for training in a GAN-based setup. The generator network in RankSRGAN is SRGAN [19]. SRFlow [22] is a recently proposed method, which uses normalizing flows [27] for perceptual image SR. The concept of normalizing flows provides an alternative to GAN-based learning by modeling the ill-posed

6

problem explicitly as a stochastic process. We also compare our loss formulation to recently proposed losses using the wavelet transformation, see Sec. 3.2. Division into subbands with wavelet transform is used by WaveletSRNet [11] and DWSR [8], which both use the Haar transform. We compare our method to the losses in WaveletSRNet which uses a finer division and a more sophisticated loss formulation than DWSR. For this purpose, we train the efficient generator backbone G with the proposed losses in WaveletSRNet for direct comparison.
For all other methods we use the pretrained models provided by the authors, as all of them are trained on DF2K. Additionally, we provide the results for standard bicubic upsampling as a baseline. To quantify the efficiency and model complexity, we compute runtimes and number of parameters at inference on a NVIDIA TITAN RTX and an Intel i7 CPU (6 cores). We also provide visual examples in Fig. 4 which support our quantitative evaluation.
4.2.1 Discussion
The superiority of our losses compared to ESRGAN's losses is already shown in the ablation study in Tab. 7. On top of that, we can even compete with ESRGAN's highcomplexity generator, which achieves slightly better LPIPS and FID values, but lower PSNR and SSIM scores with a substantially slower inference time by a factor of over 13× on GPU.
Our losses significantly surpass all three RankSRGAN models in both restoration metrics PSNR/SSIM and even achieve the highest FID score. Only the NIQE and PI optimized models have slightly higher LPIPS scores, which however comes with a 2.4× higher runtime on GPU. Note, this is a substantial difference in complexity, e.g. this equates to reducing the number of layers in a network by a factor of 2.4. In comparison to the ranker approach, our loss formulation does not depend on the difficult design of a meaningful handcrafted quality metric, which manifests an upper bound on the achievable quality. We also do not require the expensive setup of the ranker, however we achieve stronger guidance by direct emphasis on the frequency content without an additional explicit concept of perceptual quality.
SRFlow [22] is by far the most expensive method with a large number of parameters and slow inference speeds of 1.995s and 55.33s on GPU and CPU respectively, yet does not outperform the performance of other methods, with the exception of PSNR and SSIM. Our highly efficient method is on par with SRFlow with comparable perceptual metrics. Our solution has better FID score (+0.41) but slightly lower LPIPS score (-0.001). However, there is an enormous difference in inference speed, e.g. SRFLow is 48 times slower on GPU than our method, which clearly highlights the su-

Method

PSNR SSIM LPIPS FID

ESRGAN (Our losses) [31] 25.05

ESRGAN [31]

24.36

RankSRGAN (NIQE) [36] 24.52

Ours (Full)

24.69

0.738 0.717 0.715 0.723

0.120 24.07 0.123 25.50 0.143 27.47 0.132 26.70

Table 2. Evaluation on Urban100. Red indicates best, blue second best.

periority of our proposed losses. We train G with WaveletSRNet's losses from scratch
with a learning rate of l = 10-5 for 500k iterations with a batch size of B = 16. Further, we finetune G with a lower learning rate of l = 10-6 for another 250k iterations. We clearly outperform WaveletSRNet's loss formulation with our proposed losses in regard to PSNR and perceptual metrics LPIPS and FID. Even our proposed single supervision loss in Fourier space LF , from our ablation study, substantially outperforms WaveletSRNet in three metrics with the exception of LPIPS, see configuration 2 in Tab. 7.
We evaluate our losses on Urban100 [12] in Tab. 4 to show the generalization capability of our approach. Our efficient setting achieves comparable performance also on this dataset. The application of our losses to ESRGAN again results in clear improvements in all 4 metrics by a substantial margin, especially in the restoration metrics.
5. Conclusion
We present two Fourier domain losses ­ a supervision and a GAN loss ­ to strengthen the training signal for the task of perceptual image SR. Our ablation study clearly shows the provision of complementary information during training in addition to the losses in spatial-domain. Due to the improved guidance, it is possible to train a significantly lower complexity ­ and therefore faster ­ network to achieve comparable performance of much larger networks, which we regard as an important property for many practical applications. The runtime of the generator backbone can be cut down to only 41ms, which is over 13× faster than ESRGAN and 48× faster than SRFlow on GPU. The clear separation of images into LF (retained) and HF (missing) content and therefore the direct emphasis on the missing high frequencies in Fourier space, imposed by our proposed losses, helps the SR network to generate plausible HF content. At the same time, we also apply the corresponding spatial losses to leverage the complementary local information, which results in even better perceptual quality. To the best of our knowledge, we are the first to successfully apply a GAN-based loss directly in Fourier space for SR. We are convinced that further research into architectural improvements of our Fourier-space GAN-network can even further boost the effectiveness of our approach.

7

Method
Bicubic SRFlow [22] ESRGAN [31] RankSRGAN (Ma) [36] RankSRGAN (NIQE) [36] RankSRGAN (PI) [36]
Ours (WaveletSRNet loss [11]) Ours (L1 only) Ours (Full)

PSNR
28.11 28.68 28.19 27.30 28.19 28.11
27.97 30.56 28.28

SSIM
0.782 0.773 0.769 0.742 0.765 0.765
0.786 0.837 0.770

LPIPS
0.410 0.120 0.115 0.141 0.119 0.121
0.171 0.270 0.121

FID
44.79 16.13 15.37 18.40 15.89 16.28
19.80 22.91 15.72

Par [M]
39.542 16.698 1.554
1.554 1.554
0.894 0.894 0.894

GPU [s]
1.995 0.553 0.099 0.099 0.099
0.041 0.041 0.041

CPU [s]
55.33 29.28 3.97
3.97 3.97
1.72 1.72 1.72

Table 3. Comparison with state-of-the-art methods. We compare in terms of image quality scores (PSNR, SSIM, LPIPS and FID) and efficiency measures (parameters and runtimes). Red indicates best, blue second best.

Target

SRFlow

RankSRGAN (NIQE)

ESRGAN

Ours

Figure 4. Visual examples on DIV2K validation images. 8

References
[1] Eirikur Agustsson and Radu Timofte. Ntire 2017 challenge on single image super-resolution: Dataset and study. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, July 2017. 2, 5, 11
[2] Yochai Blau, Roey Mechrez, Radu Timofte, Tomer Michaeli, and Lihi Zelnik-Manor. The 2018 pirm challenge on perceptual image super-resolution. In Proceedings of the European Conference on Computer Vision (ECCV) Workshops, September 2018. 2, 6, 11
[3] C. Dong, C. C. Loy, K. He, and X. Tang. Image super-resolution using deep convolutional networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38(2):295­307, Feb 2016. 1, 2
[4] M. Fritsche, S. Gu, and R. Timofte. Frequency separation for real-world super-resolution. In 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW), pages 3599­3608, 2019. 2, 4
[5] Yonggan Fu, Wuyang Chen, Haotao Wang, Haoran Li, Yingyan Lin, and Zhangyang Wang. AutoGAN-distiller: Searching to compress generative adversarial networks. In Hal Daume´ III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 3292­3303. PMLR, 13­18 Jul 2020. 2
[6] Xinyu Gong, Shiyu Chang, Yifan Jiang, and Zhangyang Wang. Autogan: Neural architecture search for generative adversarial networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2019. 2
[7] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems 27, pages 2672­ 2680. 2014. 1, 2
[8] Tiantong Guo, Hojjat Seyed Mousavi, Tiep Huu Vu, and Vishal Monga. Deep wavelet prediction for image superresolution. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, July 2017. 2, 3, 7
[9] Muhammad Haris, Gregory Shakhnarovich, and Norimichi Ukita. Deep back-projection networks for super-resolution. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. 1, 2
[10] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30, pages 6626­6637. Curran Associates, Inc., 2017. 5, 6, 11
[11] Huaibo Huang, Ran He, Zhenan Sun, and Tieniu Tan. Wavelet-srnet: A wavelet-based cnn for multi-scale face super resolution. In IEEE International Conference on Computer Vision, pages 1689­1697, 2017. 2, 3, 5, 7, 8, 11
[12] Jia-Bin Huang, Abhishek Singh, and Narendra Ahuja. Single image super-resolution from transformed self-exemplars.

In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5197­5206, 2015. 5, 7, 10
[13] Zheng Hui, Xinbo Gao, Yunchu Yang, and Xiumei Wang. Lightweight image super-resolution with information multidistillation network. In Proceedings of the 27th ACM International Conference on Multimedia (ACM MM), pages 2024­2032, 2019. 1, 2, 3, 6, 10
[14] Liming Jiang, Bo Dai, Wayne Wu, and Chen Change Loy. Focal frequency loss for generative models. arXiv preprint arXiv:2012.12821, 2020. 2, 3
[15] Alexia Jolicoeur-Martineau. The relativistic discriminator: a key element missing from standard gan. arXiv preprint arXiv:1807.00734, 2018. 2, 5
[16] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurate image super-resolution using very deep convolutional networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016. 1, 2
[17] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014. 5
[18] Wei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, and MingHsuan Yang. Deep laplacian pyramid networks for fast and accurate super-resolution. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017. 1, 2
[19] Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, and Wenzhe Shi. Photo-realistic single image super-resolution using a generative adversarial network. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017. 1, 2, 4, 6
[20] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung Mu Lee. Enhanced deep residual networks for single image super-resolution. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 136­144, 2017. 1, 2
[21] Andreas Lugmayr, Martin Danelljan, and Radu Timofte. Ntire 2020 challenge on real-world image super-resolution: Methods and results. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, June 2020. 2
[22] Andreas Lugmayr, Martin Danelljan, Luc Van Gool, and Radu Timofte. Srflow: Learning the super-resolution space with normalizing flow. In ECCV, 2020. 2, 6, 7, 8, 10, 11
[23] Chao Ma, Chih-Yuan Yang, Xiaokang Yang, and MingHsuan Yang. Learning a no-reference quality metric for single-image super-rolution. Computer Vision and Image Understanding, pages 1­16, 2017. 6, 11
[24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001, volume 2, pages 416­423 vol.2, 2001. 10, 11
[25] A. Mittal, R. Soundararajan, and A. C. Bovik. Making a "completely blind" image quality analyzer. IEEE Signal Processing Letters, 20(3):209­212, 2013. 6, 11

9

[26] Anton Obukhov, Maximilian Seitzer, Po-Wei Wu, Semen Zhydenko, Jonathan Kyl, and Elvis Yu-Jing Lin. Highfidelity performance metrics for generative models in pytorch, 2020. Version: 0.2.0, DOI: 10.5281/zenodo.3786540. 5
[27] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 1530­1538, Lille, France, 07­09 Jul 2015. PMLR. 2, 6
[28] Wenzhe Shi, Jose Caballero, Ferenc Husza´r, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel Rueckert, and Zehan Wang. Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. In CVPR, 2016. 3
[29] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations, 2015. 2, 4
[30] Radu Timofte, Eirikur Agustsson, Luc Van Gool, MingHsuan Yang, and Lei Zhang. Ntire 2017 challenge on single image super-resolution: Methods and results. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, July 2017. 2, 5
[31] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy. Esrgan: Enhanced super-resolution generative adversarial networks. In The European Conference on Computer Vision Workshops (ECCVW), September 2018. 1, 2, 4, 5, 6, 7, 8, 10, 11
[32] Kai Zhang, Martin Danelljan, Yawei Li, and Radu Timofte. Aim 2020 challenge on efficient super-resolution: Methods and results. In Adrien Bartoli and Andrea Fusiello, editors, Computer Vision ­ ECCV 2020 Workshops, pages 5­40, Cham, 2020. Springer International Publishing. 2
[33] Kai Zhang, Shuhang Gu, and Radu Timofte. Ntire 2020 challenge on perceptual extreme super-resolution: Methods and results. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, June 2020. 2
[34] Kai Zhang, Shuhang Gu, Radu Timofte, et al. Aim 2019 challenge on constrained super-resolution: Methods and results. In ICCV Workshops, 2019. 2, 3
[35] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In CVPR, 2018. 1, 5, 6
[36] Wenlong Zhang, Yihao Liu, Chao Dong, and Yu Qiao. Ranksrgan: Generative adversarial networks with ranker for image super-resolution. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2019. 2, 6, 7, 8, 10, 11

Supplementary Material
We provide additional evaluations and visual results for our main models, that support the conclusions in the paper. In order to show the generalization capabilities of our approach, we provide results on a third dataset (BSD100). Also, in addition to PSNR, SSIM, LPIPS and FID, we calculate popular no-reference metrics (Ma, NIQE, PI) on DIV2K and discuss their limitations for perceptual quality assessment. We also show an ablation study for our proposed Fourier space GAN architecture.
A. Quantitative Evaluations
The evaluation on 3 different datasets show the clear benefits and generalization capabilities of our proposed losses in comparison to previous approaches. The application of our losses directly in Fourier domain clearly improves not only the perceptual quality, but also the restoration quality at the same time.
A.1. Urban100
In Tab. 4 we show the results of additional methods on Urban100 [12]. As already discussed, our losses show similar performance as on DIV2K (validation), which shows the generalizability of our proposed loss functions. Again, ESRGAN (Our losses) achieves the highest perceptual scores with a substantial improvement in FID of -1.43 over the version without our losses. Our losses in conjunction with IMDN [13] achieve comparable results with SRFlow despite the enormous difference in runtime (41ms vs. 1995ms). Ours (Full), our efficient implementation, outperforms all versions of RankSRGAN in every metric, and is also faster at inference time.
A.2. BSD100
In addition to DIV2K(val) and Urban100, we evaluate the performance also on BSD100 [24], another commonly used dataset, in Tab. 5. Again, ESRGAN (Our losses) performs best in terms of perceptual quality and also achieves

Method

PSNR SSIM LPIPS FID

SRFlow [22]

25.25

ESRGAN (Our losses) [31] 25.05

ESRGAN [31]

24.36

RankSRGAN (Ma) [36] 24.12

RankSRGAN (NIQE) [36] 24.52

RankSRGAN (PI) [36] 24.47

Ours (Full)

24.69

0.735 0.738 0.717 0.704 0.715 0.716 0.723

0.127 26.22 0.120 24.07 0.123 25.50 0.143 27.72 0.143 27.47 0.139 27.84 0.132 26.70

Table 4. Evaluation on Urban100 [12]. Red indicates best, blue second best.

10

Method

PSNR SSIM LPIPS FID

SRFlow [22]

26.08

ESRGAN (Our losses) [31] 25.79

ESRGAN [31]

25.34

RankSRGAN (Ma) [36] 25.06

RankSRGAN (NIQE) [36] 25.52

RankSRGAN (PI) [36] 25.48

Ours (Full)

25.66

0.667 0.658 0.643 0.633 0.642 0.643 0.656

0.183 66.24 0.158 57.90 0.161 60.42 0.183 65.75 0.178 61.52 0.175 63.97 0.172 62.25

Table 5. Evaluation on BSD100 [24]. Red indicates best, blue second best.

high restoration quality. SRFlow has high restoration quality but can not compete in perceptual quality in comparison to all other methods. Ours (Full) outperforms all RankSRGAN models in all metrics. Only RankSRGAN (NIQE) achieves a lower FID score.
A.3. DIV2K - No-reference Metrics
No-reference metrics are handcrafted quality assessment tools, which quantify the image quality without comparison to a ground truth. However, these metrics are limited for objective image quality quantification for image superresolution, because of the lacking reference. We would like to learn the true target distribution, which involves much more than learning for beautiful images. We therefore chose FID [10] as a quality measure for distributional similarity which together with LPIPS quantifies perceptual quality.
We list the results for no-reference quality metrics Ma [23], NIQE [25] and PI [2] for DIV2K in Tab. 6. As expected the RankSRGAN [36] models perform the best, as they are explicitly trained for these metrics. Therefore, a direct comparison to all other methods is not fair. Interestingly, RankSRGAN (Ma) outperforms all other RankSRGAN models, even those that are trained for these specific metrics, which is unexpected. It is unclear to us why these inconsistencies arise, since RankSRGAN is trained on DIV2K for exactly these metrics.
Among the methods that are not explicitly trained for these metrics, our losses applied to ESRGAN and IMDN achieve the best results overall. Ours (Full) achieves the highest Ma and PI scores, ESRGAN (Our losses) achieves the best NIQE score.

B. Visual Results
We show a series of visual examples on all 3 datasets to asses the quality by visual inspection. In addition we provide PSNR and LPIPS for each method as quantitative metrics for restoration and perceptual quality respectively.
The metrics are in line with our quantitative evaluation overall. Note, we deliberately show some cases where the individual scores do not exactly match our overall quanti-

Method

Ma NIQE PI

RankSRGAN (Ma) [36]

6.8142 2.6143 2.9000

RankSRGAN (NIQE) [36]

6.6923 2.7121 3.0099

RankSRGAN (PI) [36]

6.6794 2.6851 3.0029

SRFlow [22]

6.5230 3.5421 3.5095

ESRGAN (Our losses) [31] 6.5580 3.0388 3.2404

ESRGAN [31]

6.5937 3.0918 3.2491

Ours (WaveletSRNet losses) [11] 5.9682 4.9011 4.4664

Ours (Full)

6.6792 3.0836 3.2022

Table 6. Evaluation of no-reference metrics on DIV2K. Red indicates best, blue second best.

tative evaluation. These differences on individual images arise due to variance among different strengths and weaknesses of each method.
The application of our losses in general improves the restoration- and perceptual quality, as can be seen by the examples of ESRGAN, ESRGAN (Our losses) and Ours (Full). Even our efficient setting with IMDN as generator achieves comparable performance to the larger model ESRGAN and especially the largest model SRFlow with highly improved runtimes. Ours (Full) in general also improves perceptual and restoration quality in comparison with RankSRGAN. Additionally, we observed that SRFlow tends to generate noisy output, even in areas of uniform color.
C. Fourier GAN Architecture - Ablation
We provide additional analysis of our Fourier space GAN loss by training a smaller architecture with a reduced number of layers. We compare the full size GAN architecture (LFGA,5N ) with a reduced GAN architecture where the number of layers is set to 3 (LFGA,3N ). We test this setup in configuration 5 and 8 from our ablation study in Tab. 7. The higher complexity discriminator achieves consistently better scores in PSNR, SSIM and FID in both configuration 5 and 8. LPIPS is slightly improved when using LFGA,3N . We suspect this could be in trade-off with FID due to an increased weight on the VGG-loss during training, when the discriminator is weaker.

Method

PSNR SSIM LPIPS FID

Ours (Config. 5, LFGA,3N ) 29.02 Ours (Config. 5, LFGA,5N ) 29.06 Ours (Config. 8, LFGA,3N ) 28.32 Ours (Config. 8, LFGA,5N ) 28.42

0.792 0.796
0.770 0.776

0.126 17.51 0.129 17.17
0.122 16.19 0.124 15.88

Table 7. Ablation of FFTGAN architecture on DIV2K [1]. Red indicates best.

11

Target

SRFLow

RankSRGAN (NIQE)

(PSNR / LPIPS) ESRGAN

(28.37 / 0.098) ESRGAN (Our Losses)

(28.05 / 0.076) Ours (Full)

824

(28.00 / 0.074)

(28.48 / 0.079)

Figure 5. Visual examples on DIV2K, image 824.

(28.11 / 0.080)

Target

SRFLow

RankSRGAN (NIQE)

(PSNR / LPIPS) ESRGAN

(29.27 / 0.125) ESRGAN (Our Losses)

(28.93 / 0.116) Ours (Full)

850

(29.04 / 0.107)

(29.29 / 0.110)

Figure 6. Visual examples on DIV2K, image 850.

(28.78 / 0.110)

12

Target

SRFLow

RankSRGAN (NIQE)

(PSNR / LPIPS) ESRGAN

(26.26 / 0.077) ESRGAN (Our Losses)

(25.92 / 0.093) Ours (Full)

29

(25.91 / 0.070)

(26.24 / 0.068)

(25.81 / 0.085)

Figure 7. Visual examples on Urban100, image 29.

Target

SRFLow

RankSRGAN (NIQE)

(PSNR / LPIPS) ESRGAN

(20.04 / 0.169) ESRGAN (Our Losses)

(19.95 / 0.184) Ours (Full)

54

(18.24 / 0.179)

(19.38 / 0.159)

(19.92 / 0.169)

Figure 8. Visual examples on Urban100, image 54.

13

Target

SRFLow

RankSRGAN (NIQE)

(PSNR / LPIPS) ESRGAN

(26.25 / 0.189) ESRGAN (Our Losses)

(25.57 / 0.167) Ours (Full)

12

(26.21 / 0.186)

(26.56 / 0.192)

(26.21 / 0.179)

Figure 9. Visual examples on BSD100, image 12.

Target

SRFLow

RankSRGAN (NIQE)

(PSNR / LPIPS) ESRGAN

(29.68 / 0.126) ESRGAN (Our Losses)

(29.62 / 0.122) Ours (Full)

71

(29.48 / 0.098)

(29.81 / 0.101)

(29.75 / 0.103)

Figure 10. Visual examples on BSD100, image 71.

14

