JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

1

Learning Representation over Dynamic Graph using Aggregation-Diffusion Mechanism
Mingyi Liu1, Zhiying Tu1, Xiaofei Xu1, Zhongjie Wang1 1Faculty of Computing, Harbin Institute of Technology, Harbin, China
{liumy, tzy hit, xiaofei, rainy}@hit.edu.cn
Representation learning on graphs that evolve has recently received significant attention due to its wide application scenarios, such as bioinformatics, knowledge graphs, and social networks. The propagation of information in graphs is important in learning dynamic graph representations, and most of the existing methods achieve this by aggregation. However, relying only on aggregation to propagate information in dynamic graphs can result in delays in information propagation and thus affect the performance of the method. To alleviate this problem, we propose an aggregation-diffusion (AD) mechanism that actively propagates information to its neighbor by diffusion after the node updates its embedding through the aggregation mechanism. In experiments on two real-world datasets in the dynamic link prediction task, the AD mechanism outperforms the baseline models that only use aggregation to propagate information. We further conduct extensive experiments to discuss the influence of different factors in the AD mechanism.
Index Terms--Dynamic Graph, Representation Learning, Aggregation, Diffusion.

arXiv:2106.01678v1 [cs.LG] 3 Jun 2021

I. INTRODUCTION
Representation learning on graph structured data has recently received significant attention due to its wide application scenarios in various domains such as social networks, knowledge graphs, and bioinformatics. Recently, graph neural networks (GNNs)[1], [2], [3] have been applied to learn highdimensional and non-Euclidean graph information efficiently. However, most of the existing GNNs are designed for static graphs. Graphs tend to evolve in real-world application scenarios. For example, a new friendship will be established between people in social networks, and human interaction in social networks changes during an epidemic.
Dynamic graphs can be represented in discrete or continuous form[4], and in this paper, we focus on the continuous form representation, where no temporal aggregation is applied on the graph. Temporal point process based models[5], [6], [7], [8] are emerging to address representation learning for dynamic networks with continuous representation. The temporal point process (TPP) is modeled by events ot = (u, v, t, k) where u and v are the interacting nodes, t is the time of the event, and k is the category of this event.
The TPP is parameterized by an RNN. When an event occurs, the previous embeddings of neighbors of the interacting node are aggregated and fed into this RNN to update the interacting node. This node update mechanism assumes that long-distance structural information and timely temporal information can be propagated to interacting nodes by aggregation mechanisms. However, this assumption does not always hold. Fig. 1 shows an example of problems with information propagation based on aggregation mechanisms: 1) Suppose the current time is t6, and an event occurs between node c and e. Then c can get the information of node a at time t+1 by aggregating the information of its neighbor b, since
Manuscript received XXXXXX; XXXXXXXXX. Corresponding author: Zhongjie Wang (email: rainy@hit.edu.cn).

t1

f

a

t5

t4

t2
b

c
t6

t3

e

Event

g

d

Timeline
t1 t2 t3 t4 t5 t6
Fig. 1. An example of problems with information propagation based on aggregation mechanisms.

the information of a at time t+1 is propagated to b through aggregation when the event occurs at time t2. By serving node c as a bridge, node e can also access the information of node a at time t+1 . However, before t6, the information of a is updated twice at t4 and t5, so c and e receive very lagged information of a. 2) b is hub-node for {a, f, g} and {c, d, e}, which means that no information can be exchanged between {a, f, g} and {c, d, e} unless some event happens on b, or some event happens between {a, f, g} and {c, d, e}.
To alleviate this limitation, we introduce the aggregationdiffusion (AD) mechanism into TPP-based dynamic net-
work representation learning. The core operation of the AD
mechanism is that the interacting nodes actively diffuse the changes to their neighbors after their information is updated
by aggregating information about their neighbors. The main intuition behind this mechanism is twofold: 1) aggregation: a node's information is affected by its neighbors; 2) diffusion:
changes in the node itself should be sensed by its neighbors in time, even if there is no explicit event occurring.
We use two representative TPP-based models, DyRep[5] and
LDG[6], as backbone model to demonstrate the effectiveness

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

2

and scalability of the AD mechanism. On two dynamic graph datasets, Social Evolution[9] and Github1, the model with AD mechanism produce significant performance improvement on both Mean Average Rank (MAR) and HITS@10 compared to the original DyRep(LDG) model. We further conduct extensive experiments to discuss the influence of different factors in the aggregation-diffusion mechanism.
The remainder of this paper is organized as follows: In Section II, we introduce related work. In Section III, we describe relevant details of the DyRep and LDG. In Section IV, we explain AD mechanism in detail. In Section V, we give the details of the experiment settings. In Section VI, we present the experiment results and discuss different impact factors of the AD mechanism. In the final section, we present the conclusion.
II. RELATED WORD
A. Discrete Dynamic Embedding Approaches
Discrete dynamic embedding approaches treat dynamic graph as a sequence of graph snapshots. Most of discrete dynamic embedding approaches[10], [11], [12], [13], [14] focus on the learning representations of entire dynamic graphs rather than node representations. Some approaches[15], [16], [17], [18] are starting to focus on the dynamic representation at node level, they encode each graph snapshot using static embedding approaches[19], [20], [21], [22], [23], [2] to embed each node, and then combines some time-series models (e.g. LSTM[24], RNN[25]) for per node to model the discrete dynamic.
B. Continuous Dynamic Embedding Approaches
Currently, continuous dynamic embedding approaches are divided into two main categories: RNN based approaches and temporal point processes based approaches[4].
In RNN based approaches, the embedding of interacting nodes is updated by the a RNN based architecture according to the historical information of itself. Representative works of this type of approach are JODIE[26], TGN[27] and Streaming graph neural network[28]. JODIE[26] are designed for useritem interaction networks, which uses two RNN to maintain the embedding of each node. With on RNN for users and another one for items. Instead of keeping the embedding of node directly, TGN[27] calculates the embedding of node at different time by introducing message and memory mechanisms. The architecture of Streaming graph neural network[28] is consist of two components: 1) update component; and 2) propagation component. The update component is used to update the embedding of nodes involved in an event and the propagation component propagates the event to the involved nodes neighbors. The process of our proposed mechanism is similar to [28], but there are several significant differences:
· The aggregated information is different, [28] aggregate the node's own historical information, while we focus more on aggregating neighbors information, which is more effective in TPP based models[5], [6], [7].
1 https://www.gharchive.org/

· The propagation information, propagation objects, and propagation methods are different.
· [28] focus more on the architecture of the neural network, while we focus on a mechanism that can be adapted to existing TPP based approaches.
· We discuss the influence of different factors on the aggregated-diffusion mechanism in more detail.
Know-Evolve[8] is the pioneer in bringing the temporal point processes[29] to dynamic graph representation learning, which models temporal knowledge graph as multi-relational timestamped edges by parameterizing a TPP by a deep recurrent architecture. DyRep[5] is the successor of KnowEvolve. DyRep extends Know-Evolve by using TPP to model long-term events (topological evolution) and short-term events (node communication) and introducing aggregation mechanisms. LDG[6] argues long-term events are often specific by humans, and can be suboptimal and expensive to obtain. LDG use Neural Relational Inference (NRI) model[30] to infer the type of events on the graph and replaces the self-attention originally used in DyRep by generating a temporal attention matrix to better aggregate neighbor information. GHN[7] is another TPP based approach, which uses an adapted continuoustime LSTM for Hawkes process[31]. Similar to Know-Evolve, GHN is specifically designed for knowledge graphs.
In this paper, we choose DyRep and LDG as our backbone model for the following reasons:
· DyRep and LDG are universal and not specifically designed for knowledge graphs or user-item graphs.
· DyRep and LDG can model realistic long-term events and short-term events, which can not be provided by other models.
· DyRep and LDG have an obvious aggregation mechanism when interacting nodes are updating.
It is important to note that theoretically any model that updates node embeddings in continuous dynamic graphs using the aggregation mechanism can be extended using the aggregationdiffusion mechanism.
III. BACKGROUND: DYREP & LDG
In this section, we describe relevant details of the DyRep and LDG model. We strongly recommend readers to read the original article[5], [6] for more details to better understand the details of how DyRep and LDG models work. LDG and DyRep represent the evolution of dynamic graph as two distinct processes:
· Long-term association (k = 1). This is also called "dynamic of graph", in which new nodes or edges are added resulting in a change in the topology of the graph.
· Short-term communication (k = 0). This is also called "dynamic on graph", in which interaction between nodes leads to temporary information flow between these nodes[32], [33]. And this process does not change the topology of the graph.
A. Node update
The node update mechanism is same for both DyRep and LDG. When an event o = (u, v, t, k) occurs between

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

3

z2(t¯)

event

z3(t¯)

zu(t¯)

zv (t¯)

z1(t¯)

z4(t¯)

event occurs

for each event

aggregation

z2(t¯) Su,2(t¯)

zu(t¯)

z1(t¯) Su,1(t¯)

for each event

zv (t¯)

z3(t¯) Sv,3(t¯) Sv,4(t¯)
z4(t¯)

after aggregation
z2(t¯) zu(t)
z1(t¯)

zv (t)

z3(t¯)

z4(t¯)

update interacting nodes (aggregation step)

z2(t)

zu(t)

z3(t) zv (t)

z1(t)

z4(t)

S (t)

update temporal attention

after diffusion
z2(t) zu(t)
z1(t)

z3(t) zv (t)
z4(t)

diffusion
z2(t¯) zu(t)
z1(t¯)

update influenced nodes (diffusion step)

z3(t¯) zv (t)
z4(t¯)

Fig. 2. Overview of node embedding update process with AD mechanism and without AD mechanism.

z2(t¯)

zu(t)

zv (t)

z3(t¯)

z1(t¯)

z4(t¯)

S (t)

update temporal attention

Common No AD mechanism With AD mechanism

interacting nodes u and v will cause their node embeddings zu, zv  Rd to be updated and subsequently update the temporal attention S  RN×N . The update process are show in Fig. 2.
In particular, when an event occurs, the embedding of participating node u is updated based on the three terms
of Self-propagation, Exogenous Drive and Attention-based Aggregation. Specially, for an event of node u at time t, updating zu as:

zu(t) = (Wshus (t¯)) + Wrzu(t¯u) + Wt(t - t¯u)) (1)
Aggregation Self -propagation Exogenous Drive

where Ws  Rd×d, Wr  Rd×d and Wt  Rd are learned parameters used to control the effect of above-mentioned three terms on the computation of node embedding, respectively. (·)) is a nonlinear function. zu(t¯u) is the previous representation of node u. t¯ denotes the time point just before current event time t and t¯u) represent the time point of last event involved u. hus (t¯)  Rd is the output representation obtained from the aggregation of node u's neighbors Nua:
hus (t¯) = Agg(softmax(Su(t¯))r(Whzr(t¯u)), r  Nua)) (2)
where Agg(·) is an aggregation function and Wh  Rd×d are learned parameters. The amount of information propagated from node u's neighbors is controlled by temporal attention Su(t¯)), which is updated by a hard-coded algorithm in DyRep and learned in LDG. It should be noted the update of S is affected by nodes' embedding. In DyRep, temporal attention S(t) relies on adjacency matrix A(t¯) and temporal attention S(t¯) at previous time step and conditional intensity uk,v(t) of event record o = (u, v, t, k):

S(t) = fS(A(t¯), S(t¯), uk,v(t))

(3)

where fS is the attention update function in DyRep[5]. Conditional intensity uk,v(t) models the occurrence of event o = (u, v, k, t) between u and v at time t:

uk,v(t) = k log

1 + exp

kT [zu(t¯); zv(t¯)] k

(4)

where k is trainable scalar parameter, which denotes the rate of events arising from a corresponding process, and k  R2d is designed to learn time-scale specific compatibility. [; ]
denotes concatenation.
In LDG[6], they replace the hard-coded node update algorithm fS with a learnable bilinear encoder fSenc, which is a two pass progress to ensure temporal attention S(t) depends
on node embeddings at previous time step:

S(t) = fSenc(z(t - 1))

(5)

IV. AGGREGATION-DIFFUSION MECHANISM
In this paper, we extend DyRep and LDG by using a node embedding update algorithm with aggregation-diffusion (AD) mechanism. As mentioned in Section I, the main intuition behind AD mechanism is straightforward: first, the node's information is affected by its neighbors, which is aggregation part; second, changes in the node itself should be propagated to its neighbors proactively and in a timely manner, which is diffusion part.
Algorithm 1 gives the pseudo-code for node embedding update with AD mechanism. The algorithm consists of two steps: 1) an aggregation step, which is used to update the embeddings of the nodes directly involved in a event and has been explained in Section III; 2) a diffusion step, which is used to update the embedding of other nodes that may be affected and will be discussed in this section.
The diffusion step mainly consists of diffusion message generation, diffusion node selection and update of diffused nodes.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

4

Algorithm 1: Update Node Embedding with Aggregation-Diffusion Mechanism

Input : Event record o = (u, v, t, k) ; All node embeddings of previous time z(t¯) ; Most recently updated A(t¯) and S(t¯) ; Trainable parameters Ws, Wr, Wt, Wh
and Wd;
Output: Updated node embeddings z(t) ;

1 /* Aggregation step

*/

2 for each j  {u, v} do

3 Nja  {r : Ar,j (t¯) > 0}

4 /* Aggregate information from all one-hop

neighbors. Discussed in Section VI-D */

5

hjs(t¯)

 Agg(softmax(Sj(t¯))r(Whzr(t¯j )),

r



Nja))

6 /* Update interacting node's embedding */
7 zj(t)  (Wshjs(t¯)) + Wrzj(t¯) + Wt(t - t¯j)) 8 end

9 /* Diffusion step

*/

10 for each j  {u, v} do

11 /* Generating diffusion message. Discussed

in Section VI-B

*/

12 mj(t)  Generator(z(t), z(t¯), o)

13

/* Selecting candidate diffusion nodes.

Discussed in Section VI-D and

Section VI-C

*/

14 Njd  SelectCandidate(A(t¯)) // S(t¯) for LDG

15 for each r  Njd do

16

/* Update diffused node's embedding.

Discussed in Section VI-E

*/

17

zr(t)  (zr(t¯) + qj,r(t¯)Wdmj(t))

18 end

19 end

20 return z(t)

Diffusion Message Generation. It determines what kind of message the interacting node of an event will propagate to its neighbors. We model the message in three ways. The most straightforward way is interacting node u diffuses its updated embedding zu(t) outward and the formulation is as follows:

mu(t) = zu(t)

(6)

An interacting node can also diffuse outward changes in itself rather than just current state:

delta: mu(t) = zu(t) - zu(t¯)

(7)

There is also a way to diffuse the impact of the event outward:

edge: mu(t) = (W1zu(t) + W2zv(t))

(8)

where v is another interacting node in the event, W1, W2  Rd×d are trainable parameters.
These three different approaches to diffusion message have
their own advantages and disadvantages, which will be dis-
cussed in detail in Section VI-B.

Diffusion Node Selection. This is designed to select the nodes that will receive the diffusion information.
In this paper, we choose the 1-hop neighbors of the interacting node as the diffusion nodes. We do not diffuse more hops because more hops will result in a significant decrease in training speed but not a significant performance improvement or even a decrease in performance due to the introduction of noise. We will discuss the impact of diffusion hops in detail in Section VI-C.
Specially, when diffusing the message mu(t) generated by node u, we will avoid involving another interacting node v in the event, because v has already obtained information about u through aggregation step, and repeatedly obtaining information through diffusion will lead to a negative effect, which will be discussed in Section VI-D. Therefore, the formulation for diffusion node selection is as follows:

Nud = {r : Ar,u(t¯) > 0 and r = v}

(9)

It should be noted, for LDG, we use S(t¯) to replace A(t¯), because A is not maintained in LDG.
In addition, we also tried to mask the aggregation/diffusion nodes randomly and temporally, which are also discussed in Section VI-D.
Update of Diffusion nodes. The diffusion nodes will update their embeddings based on their previous embedding and the diffusion message:

zr(t) = (zr(t¯) + qu,r(t¯)Wdmu(t)), r  Nud

(10)

where Wd  Rd×d is a trainable parameters, and qu,r(t¯) is
used to control the strength of diffusion from u to r. In this paper we discuss two methods of calculating qu,r(t¯). One is uniform, where all values are equal to 1:

uniform: qu,r(t¯) = 1, r  Nud

(11)

Another one is attention, which uses temporal attention S(t¯) to calculate qu,r(t¯):

attn: qu,r(t¯) =

r



exp(Su,r (t¯)) Nud exp(Su,r

(t¯))

,

r



Nud

(12)

This will be discussed in Section VI-E.

V. EXPERIMENT SETTINGS A. Datasets & Metrics

TABLE I DATASET STATISTICS FOR SOCIAL EVOLUTION AND GITHUB.

#Nodes #Initial Associations #Final Associations #Train Event #Test Event

SOCIAL EVOLUTION
83 575 708 43,834 10,535

GITHUB
284 149 710 11,644 9,082

We evaluate the AD mechanism on two real world dynamic graph datasets, Social Evolution[9] and Github, which are also the dataset used in DyRep[5] and LDG[6]. The statistical

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

5

TABLE II PERFORMANCE COMPARISON OF WHETHER TO USE AD MACHANISM. BLUE BOLDED RESULTS DENOTE BEST PERFORMANCE FOR DYREP-BASED
MODEL, PURPLE BOLDED RESULTS DENOTE BEST PERFORMANCE FOR LDG-BASED MODEL.

MODEL
DyRep DyRep-self DyRep-D-base DyRep-AD-base
LDG LDG-self LDG-D-base LDG-AD-base

MAR 13.88 20.61 6.74 6.28

SOCIAL EVOLUTION

HIT@10 SPEED Epoch

0.486

1x

5

0.141

0.9x

5

0.897

4x

1

0.907

4x

1

13.06 0.448

40x

5

13.75 0.479

40x

5

6.94 0.902

40x

5

6.40 0.918

40x

5

MAR 117.83 130.99 85.51 81.25
64.64 59.43 51.56 51.49

GITHUB

HIT@10 SPEED

0.165

1x

0.160

0.9x

0.287

4x

0.262

4x

0.276

40x

0.290

40x

0.462

40x

0.480

40x

Epoch 5 5 2 1
2 2 3 2

results for Social Evolution and Github are presented in Table I.
Social Evolution[9]. This dataset is released by MIT Human Dynamics Lab, which consists of over 2M events o = (u, v, t, k). Follow [5], we treat Proximity, Calls and SMS records between users as communication events (short-term events, k = 1) and all Close Friendship records between users are treated as association events (long-term events, k = 0). Follow [6], Proximity records are filted by the probability that record occurred, because the number of Proximity records is too large and contains a lot of noise. The Social Evolution data is collected from Jan 2008 to June 2009. Similar to [5] and [6], we use the association events between users from Jan 2008 to Sep 10, 2008 to initialize the graph, and events from Sep 11, 2008 to April 2009 is used as training set, and events after May 2009 is used as test set. After pre-processing, the dataset contains 83 nodes, the training set contains 43K events, and the test set contains 10K events.
Github. This dataset is released by Github Archive. The original dataset contains over 12K nodes and 600K events and compared to Social Evolution is a large graph with sparse events. Since LDG requires sufficient interactions between nodes to train temporal attention S, we extract a dense subgraph following the processing in [6]. Follow [5], we treat Follow records between users as association events and other records are treated as communication records. We use the association events in 2011-2012 to initialize the graph, and events from Jan 1, 2013 to Sep 30, 2013 is used as training set, and events from Oct 1, 2013 to Dec 31, 2013 is used as test set. After pre-processing, the dataset contains 284 nodes, the training set contains 11K events, and the test set contains 9K events.
During the test, for a given event (u, ?, t, k) or (?, v, t, k), we compute the conditional density of known node with all other nodes and rank them. Same as [5] and [6], we report Mean Average Ranking (MAR) and HIT@10.
B. Implementation Details
For LDG and DyRep we directly use the code2 provide by [6]. We only modify the node update function update node embed in the code to add the AD mechanism into LDG and DyRep.
2 https://github.com/uoguelph-mlrg/LDG

The hyper parameter setting of the experiments are also consistent with those of LDG. We use the Adam optimizer[34] with the learning rate set to 0.0002. The hidden units d per layer is set to 32. Gradient clipping is used to avoid gradient explosion, and the clipping value is set to 100. We do not use dropout and batch size is set to 200. We train for 5 epochs. We run each experiment 10 times and report the average results.
VI. RESULTS & DISCUSSION
A. Overview
In this section we give an overview performance comparison between models using the AD mechanism and not using. The AD mechanism used in this section is *-AD-base, which follows the principle of simplicity, specifically, using Eq. 6 in generating diffusion message, considering only 1-hop neighbors, using Eq. 9 in selecting the diffusion nodes, and using a uniform strength q (Eq. 11). We also conduct experiments on models that only use diffusion step (*-D-base) as well as neither diffusion nor aggregation step (*-self ). Table II shows the performance comparison.
On Social Evolution, compared with the baseline DyRep, DyRep-AD-base reduced MAR from 13.88 to 6.28 and improved HIT@10 from 0.468 to 0.907; compared with baseline LDG, LDG-AD-base reduced MAR from 13.06 to 6.40 and improved HIT@10 from 0.448 to 0.918.
On Github, compared with the baseline DyRep, DyRepAD-base reduced MAR from 117.83 to 81.25 and improved HIT@10 from 0.165 to 0.262; compared with baseline LDG, LDG-AD-base reduced MAR from 64.64 to 51.49 and improved HIT@10 from 0.276 to 0.480.
We find that the performance improvement from the diffusion step is significantly higher than that from the aggregation step, which may be due to the fact that both aggregation and diffusion are essentially for information delivery, and the number of nodes that acquire new information in the aggregation process (1 node) is less than that in the diffusion process (node's neighbors).
Another interesting phenomenon is that the training time and convergence epochs of LDG-AD-base and LDG are close to each other. DyRep-AD-base takes 4x time per epoch than DyRep, but DyRep-AD-base only needs one epoch to converge, while DyRep takes 5 epochs, which means that the AD mechanism does not increase the training time in practice. The reason why the AD mechanism can reduce the

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

6

MAR

DyRep-AD-base

DyRep-AD-edge

LDG-AD-delta

DyRep-AD-delta

LDG-AD-base

LDG-AD-edge

16

14

12

10

8

6

1

2

3

4

5

Epoch

(a) MAR on Social Evolution

DyRep-AD-base DyRep-AD-delta

DyRep-AD-edge LDG-AD-base

LDG-AD-delta LDG-AD-edge

0.9

0.8

0.7

0.6

0.5

1

2

3

4

5

Epoch

(c) HIT@10 on Social Evolution

HIT@10

MAR

DyRep-AD-base DyRep-AD-delta

DyRep-AD-edge LDG-AD-base

LDG-AD-delta LDG-AD-edge

90

80

70

60

50

1

2

3

4

5

Epoch

(b) MAR on Github

DyRep-AD-base DyRep-AD-delta

DyRep-AD-edge LDG-AD-base

LDG-AD-delta LDG-AD-edge

0.55

0.50

0.45

0.40

0.35

0.30

0.25

0.20

0.15

1

2

3

4

5

Epoch

(d) HIT@10 on Github

HIT@10

Fig. 3. Dynamic Link Prediction Performance Comparison with different diffusion messages.

number of epochs required for convergence is that one of the purpose of the repeated epochs in the training process is to propagate the delayed information through sample repetition. For example, in Fig. 1, in the i-th training epoch ot2 will lead to the information updated at t4, t5 in the previous epoch to be propagated to node b and thus to e when ot6 occurs in the i-th training epoch.
B. Impact of Diffusion Message
In this section, we discuss the impact of diffusion message. We replace the diffusion message generation method in *-ADbase with Eq. 7 (*-AD-delta) and Eq. 8 (*-AD-edge). Fig. 3 shows the dynamic link prediction performance comparison with different diffusion messages.
We find that *-AD-delta have a significant performance decline on the Social Evolution dataset and a significant improvement on Github dataset (especially LDG-AD-delta), while LDG-AD-edge performs better on the Social Evolution dataset.
In the Social Evolution dataset, the average interval for a node to appear in an event is 35.43 much lower than 107.83 in the Github dataset. The frequent interaction of nodes makes the diffusion information m generated using Eq. 7 in Social Evolution close is very small, so it cannot diffuse enough information, resulting in poor performance in the Social Evolution dataset. While in Github dataset, the longer interval allows nodes to accumulate enough difference information for diffusion.
The generation of edge diffusion message (Eq. 8) involves both interacting nodes in the event, therefore, during diffusion,

if the number of common neighbors between two nodes is high, it will cause duplicate overlap of diffusion message and thus bring negative impact to the performance. In Social Evolution dataset, the average number of common neighbors of interacting nodes in an event is 2.08, while in the Github dataset is 6.02.
C. Impact of Diffusion Hops
Fig. 4 shows the dynamic link prediction performance comparison with different diffusion hops. Diffusing more hops on the Social Evolution and Github datasets doesn't result in improved performance, but instead has a negative effect. There are two main reasons for the negative effect:
· The more hops of diffusion, the greater the risk and amount of noise introduced in the process of diffusion.
· The AD mechanism actually reduces the distance of information propagation between nodes. Compared to using only aggregation, the i-hop diffusion in the AD mechanism reduces the distance of information propagation by at least i. The average path length between nodes in the Social Evolution and Github datasets is 2.239 and 2.899, respectively. Therefore, 2-hop diffusion will make the information propagation distance less than 1 (0.239 and 0.899) which will result in the aggregated and diffused messages mixed together, thus producing a negative effect.
In addition, we find that as the number of diffusion hops increases, the training time consuming also increase dramatically, so we suggest that diffusion of 1-hop in practice can be a better balance between performance and training cost.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

7

MAR

DyRep-AD-base DyRep

DyRep-AD-2hop LDG-AD-base

LDG LDG-AD-2hop

22

20

18

16

14

12

10

8

6

1

2

3

4

5

Epoch

(a) MAR on Social Evolution

DyRep-AD-base

DyRep-AD-2hop

LDG

DyRep

LDG-AD-base

LDG-AD-2hop

1.0

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

1

2

3

4

5

Epoch

(c) HIT@10 on Social Evolution

HIT@10

MAR

140 120 100
80 60 40
1
0.50 0.45 0.40 0.35 0.30 0.25 0.20 0.15
1

HIT@10

Fig. 4. Dynamic Link Prediction Performance Comparison with different diffusion hops.

DyRep-AD-base DyRep

DyRep-AD-2hop LDG-AD-base

LDG DyRep-LDG-2hop

2

3

Epoch

(b) MAR on Github

DyRep-AD-base DyRep

DyRep-AD-2hop LDG-AD-base

4

5

LDG LDG-AD-2hop

2

3

4

5

Epoch

(d) HIT@10 on Github

D. Impact of Aggregation and Diffusion Nodes
In this section, we discuss the performance impact of different strategies for selecting aggregation and diffusion nodes. We consider the following selection strategies and the results are shown in Fig. 5:
· *-AD-v: This strategy does not remove another interacting node v when selecting diffusion nodes Nud of interacting node u: Nud = {r : Ar,u(t¯) > 0}. We find a significant performance drop in Fig. 5. This is due to v has already obtained information about u through aggregation step, and repeatedly obtaining information through diffusion will lead to negative effect.
· *-AD-: This strategy randomly mask 20% of the neighboring nodes of interacting node at the aggregation step but diffusion is still based on Eq. 9. We find that LDGAD- performs slightly better than LDG-AD-base on both Social Evolution and Github datasets, while DyRepAD-alpha performs slightly worse than DyRep-ADbase.
· *-AD-: This strategy randomly mask 20% of the neighboring nodes of interacting node at the diffusion step but still aggregate all neighbors. We find that DyRep-AD and LDG-AD- slightly improved the MAR metrics on both Social Evolution and Github datasets, while LDG-AD- significantly decreased the HIT@10 metric on Github.
· *-AD-: This strategy randomly mask 20% of the neighboring nodes of interacting node at both aggregation and diffusion step, and these two masks are independent. We observe no significant difference between *-AD-

and *-AD-base. Compared with LDG-AD-base, LDGAD- slightly improved the HIT@10 metrics on Social Evolution, while there is a significant gap on Github. · *-AD-: This strategy mask 20% of the earliest neighboring nodes of interacting node at both aggregation and diffusion step. We observe a significant drop in the performance of both MAR and HIT@10 on both Social Evolution and Github datasets.

E. Impact of Diffusion Attention

TABLE III PERFORMANCE COMPARISON OF WHETHER TO USE ATTENTION DURING DIFFUSION. BLUE BOLDED RESULTS DENOTE BEST PERFORMANCE FOR
DYREP-BASED MODEL, PURPLE BOLDED RESULTS DENOTE BEST PERFORMANCE FOR LDG-BASED MODEL.

DATASET SOCIAL EVOLUTION
GITHUB

MODEL DyRep-AD-base DyRep-AD-attn LDG-AD-base LDG-AD-attn
DyRep-AD-base DyRep-AD-attn LDG-AD-base LDG-AD-attn

MAR 6.28 6.20 6.40 6.63
81.25 84.78 51.49 44.96

HIT10 0.907 0.885 0.918 0.907
0.262 0.261 0.480 0.427

In the work of GAT[35], it has been demonstrated that attention mechanisms have an important role in the aggregation process. And in this section, we explore whether existing attention has a positive effect on the diffusion process. Table III shows the performance comparison of whether to use attention during diffusion, where *-AD-attn is the variation of

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

8

MAR

DyRep-AD-base

DyRep-AD-v

DyRep-AD- 80

1

2

70 8

60 7 6

50 5

DyRep-AD- DyRep-AD- DyRep-AD-
3

LDG-AD-base LDG-AD-v LDG-AD-
4

40

30

base, , , are

20

almost overlapped

10

0 1

2

3

4

Epoch

(a) MAR on Social Evolution

DyRep-AD-base DyRep-AD-v DyRep-AD- 1.0

DyRep-AD- DyRep-AD- DyRep-AD-

LDG-AD-base LDG-AD-v LDG-AD-

LDG-AD- LDG-AD- LDG-AD- 5
5
LDG-AD- LDG-AD- LDG-AD-

0.8 base, , , are almost overlapped
0.6

0.4

0.2

0

1

2

3

4

5

Epoch

(c) HIT@10 on Social Evolution

HIT@10

MAR

DyRep-AD-base DyRep-AD-v DyRep-AD- 90
80
70
60
50

DyRep-AD- DyRep-AD- DyRep-AD-

LDG-AD-base LDG-AD-v LDG-AD-

base, , , are almost overlapped

1

2

3

4

Epoch

(b) MAR on Github

DyRep-AD-base DyRep-AD-v DyRep-AD- 0.50

DyRep-AD- DyRep-AD- DyRep-AD-

LDG-AD-base LDG-AD-v LDG-AD-

0.45

0.40

0.35

0.30

0.25

0.20

0.15 1

2

3

4

Epoch

(d) HIT@10 on Github

HIT@10

Fig. 5. Dynamic Link Prediction Performance Comparison with different diffusion nodes.

LDG-AD- LDG-AD- LDG-AD-
5 LDG-AD- LDG-AD- LDG-AD-
5

*-AD-base with replacing Eq. 11 by Eq. 12. We observed that *-AD-base and *-AD-attn each have strengths in different datasets and different evaluation metrics. Therefore, we believe that the attention mechanism is still meaningful in the diffusion process, and we will explore a more suitable attention mechanism for the diffusion process in our future work.
F. Summary & Suggestions
Based on the above discussion, in this section, we summarize the impact factors in the AD mechanism and give some suggestions for using the AD mechanism in practice:
· In generating the diffusion message, competitive performance can be obtained based on the most concise Eq. 6. Event (edge) message (Eq. 8) is a good choice in case of few common neighbors between interacting nodes when focusing on HIT@10 metric.
· In practice, a remarkable performance can be obtained by diffusing 1-hop. When the average path length in the graph is too long, an appropriate increase in the number of hops can be considered.
· There is no need to filter the neighbors during aggregation, but there is a necessity to avoid propagate the diffusion message to another interacting node during the diffusion.
VII. CONCLUSION
We introduce a novel aggregation-diffusion mechanism into the update of node embedding to extend the existing models with TPP-based DyRep and LDG as example. By using the AD

mechanism, we get a huge improvement in all evaluation metrics on both Social Evolution and Github dataset compared to the original models. We also construct extensive experiments to explore the effects of different factors on the AD mechanism and give some suggestions for selecting suitable strategies of the aggregation and propagation process based on the graph properties.
In this paper, we have validated the effectiveness of the aggregation-diffusion mechanism mainly from experiments. In the further work, we will try to explain how the aggregationdiffusion mechanism works from the theoretical level.
ACKNOWLEDGMENT
The research in this paper is partially supported by the National Key Research and Development Program of China (No 2018YFB1402500) and the National Science Foundation of China (61772155, 61832004, 61802089, 61832014).
REFERENCES
[1] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini, "The graph neural network model," IEEE transactions on neural networks, vol. 20, no. 1, pp. 61­80, 2008.
[2] T. N. Kipf and M. Welling, "Semi-supervised classification with graph convolutional networks," arXiv preprint arXiv:1609.02907, 2016.
[3] D. Wang, P. Cui, and W. Zhu, "Structural deep network embedding," in Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, 2016, pp. 1225­1234.
[4] J. Skarding, B. Gabrys, and K. Musial, "Foundations and modelling of dynamic networks using dynamic graph neural networks: A survey," arXiv preprint arXiv:2005.07496, 2020.
[5] R. Trivedi, M. Farajtabar, P. Biswal, and H. Zha, "Dyrep: Learning representations over dynamic graphs," in International Conference on Learning Representations, 2019.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

9

[6] B. Knyazev, C. Augusta, and G. W. Taylor, "Learning temporal attention in dynamic graphs with bilinear interactions," arXiv preprint arXiv:1909.10367, 2019.
[7] Z. Han, Y. Ma, Y. Wang, S. Gu¨nnemann, and V. Tresp, "Graph hawkes neural network for forecasting on temporal knowledge graphs," in Automated Knowledge Base Construction, 2020.
[8] R. Trivedi, H. Dai, Y. Wang, and L. Song, "Know-evolve: Deep temporal reasoning for dynamic knowledge graphs," in International Conference on Machine Learning. PMLR, 2017, pp. 3462­3471.
[9] A. Madan, M. Cebrian, S. Moturu, K. Farrahi et al., "Sensing the" health state" of a community," IEEE Pervasive Computing, vol. 11, no. 4, pp. 36­45, 2011.
[10] Y. Seo, M. Defferrard, P. Vandergheynst, and X. Bresson, "Structured sequence modeling with graph convolutional recurrent networks," in International Conference on Neural Information Processing. Springer, 2018, pp. 362­373.
[11] A. Narayan and P. H. Roe, "Learning graph dynamics using deep neural networks," IFAC-PapersOnLine, vol. 51, no. 2, pp. 433­438, 2018.
[12] M. Niepert, M. Ahmed, and K. Kutzkov, "Learning convolutional neural networks for graphs," in International conference on machine learning. PMLR, 2016, pp. 2014­2023.
[13] J. Chen, J. Zhang, X. Xu, C. Fu, D. Zhang, Q. Zhang, and Q. Xuan, "E-lstm-d: A deep learning framework for dynamic network link prediction," IEEE Transactions on Systems, Man, and Cybernetics: Systems, 2019.
[14] X. Zheng, B. Zhou, M. Li, Y. G. Wang, and J. Gao, "Mathnet: Haar-like wavelet multiresolution-analysis for graph representation and learning," arXiv preprint arXiv:2007.11202, 2020.
[15] A. Sanchez-Gonzalez, N. Heess, J. T. Springenberg, J. Merel, M. Riedmiller, R. Hadsell, and P. Battaglia, "Graph networks as learnable physics engines for inference and control," in International Conference on Machine Learning. PMLR, 2018, pp. 4470­4479.
[16] M. B. Chang, T. Ullman, A. Torralba, and J. B. Tenenbaum, "A compositional object-based approach to learning physical dynamics," arXiv preprint arXiv:1612.00341, 2016.
[17] F. Manessi, A. Rozza, and M. Manzo, "Dynamic graph convolutional networks," Pattern Recognition, vol. 97, p. 107000, 2020.
[18] W. Jin, H. Jiang, M. Qu, T. Chen, C. Zhang, P. Szekely, and X. Ren, "Recurrent event network: Global structure inference over temporal knowledge graph," arXiv preprint arXiv:1904.05530, 2019.
[19] D. Liben-Nowell and J. Kleinberg, "The link-prediction problem for social networks," Journal of the American society for information science and technology, vol. 58, no. 7, pp. 1019­1031, 2007.
[20] R. Hisano, "Semi-supervised graph embedding approach to dynamic link prediction," in International Workshop on Complex Networks. Springer, 2018, pp. 109­121.
[21] W. L. Hamilton, R. Ying, and J. Leskovec, "Inductive representation learning on large graphs," arXiv preprint arXiv:1706.02216, 2017.
[22] J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, "Line: Large-scale information network embedding," in Proceedings of the 24th international conference on world wide web, 2015, pp. 1067­1077.
[23] A. Grover and J. Leskovec, "node2vec: Scalable feature learning for networks," in Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, 2016, pp. 855­ 864.
[24] S. Hochreiter and J. Schmidhuber, "Long short-term memory," Neural computation, vol. 9, no. 8, pp. 1735­1780, 1997.
[25] A. Sherstinsky, "Fundamentals of recurrent neural network (rnn) and long short-term memory (lstm) network," Physica D: Nonlinear Phenomena, vol. 404, p. 132306, 2020.
[26] S. Kumar, X. Zhang, and J. Leskovec, "Predicting dynamic embedding trajectory in temporal interaction networks," in Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2019, pp. 1269­1278.
[27] E. Rossi, B. Chamberlain, F. Frasca, D. Eynard, F. Monti, and M. Bronstein, "Temporal graph networks for deep learning on dynamic graphs," arXiv preprint arXiv:2006.10637, 2020.
[28] Y. Ma, Z. Guo, Z. Ren, J. Tang, and D. Yin, "Streaming graph neural networks," in Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, 2020, pp. 719­728.
[29] D. R. Cox and P. A. W. Lewis, "Multivariate point processes," in Contributions to Probability Theory. University of California Press, 2020, pp. 401­448.
[30] T. Kipf, E. Fetaya, K.-C. Wang, M. Welling, and R. Zemel, "Neural relational inference for interacting systems," in International Conference on Machine Learning. PMLR, 2018, pp. 2688­2697.

[31] H. Mei and J. Eisner, "The neural hawkes process: a neurally selfmodulating multivariate point process," in Proceedings of the 31st International Conference on Neural Information Processing Systems, 2017, pp. 6757­6767.
[32] D. Farine, "The dynamics of transmission and the dynamics of networks," Journal of Animal Ecology, vol. 86, no. 3, pp. 415­418, 2017.
[33] O. Artime, J. J. Ramasco, and M. San Miguel, "Dynamics on networks: competition of temporal and topological correlations," Scientific reports, vol. 7, no. 1, pp. 1­10, 2017.
[34] D. P. Kingma and J. Ba, "Adam: A method for stochastic optimization," arXiv preprint arXiv:1412.6980, 2014.
[35] P. Velickovic´, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Bengio, "Graph attention networks," arXiv preprint arXiv:1710.10903, 2017.

Active local structure c

information change

b a

d

infoorumtdaattiend

e

Inactive

node

infoorutmdaattiend

f g

