Gradient Play in Multi-Agent Markov Stochastic Games: Stationary Points and Convergence

arXiv:2106.00198v1 [cs.LG] 1 Jun 2021

Runyu Zhang School of Engineering and Applied Science
Harvard University runyuzhang@fas.harvard.edu

Zhaolin Ren School of Engineering and Applied Science
Harvard university zhaolinren@g.harvard.edu

Na Li School of Engineering and Applied Science
Harvard university nali@seas.harvard.edu

Abstract
We study the performance of the gradient play algorithm for multi-agent tabular Markov decision processes (MDPs), which are also known as stochastic games (SGs), where each agent tries to maximize its own total discounted reward by making decisions independently based on current state information which is shared between agents. Policies are directly parameterized by the probability of choosing a certain action at a given state. We show that Nash equilibria (NEs) and first order stationary policies are equivalent in this setting, and give a non-asymptotic global convergence rate analysis to an -NE for a subclass of multi-agent MDPs called Markov potential games, which includes the cooperative setting with identical rewards among agents as an important special case. Our result shows that the number of iterations to reach an -NE scales linearly, instead of exponentially, with the number of agents. Local geometry and local stability are also considered. For Markov potential games, we prove that strict NEs are local maxima of the total potential function and fully-mixed NEs are saddle points. We also give a local convergence rate around strict NEs for more general settings.
1 Introduction
The past decade has witnessed significant development in reinforcement learning (RL), which achieves successes in various tasks such as playing Go and video games. It is natural to extend RL techniques to real-life societal systems such as traffic control, autonomous driving, buildings, and energy systems. Since most such large scale infrastructures are multi-agent in nature, multi-agent reinforcement learning (MARL) has gained increasing attention in recent years [Daneshfar and Bevrani, 2010, Shalev-Shwartz et al., 2016, Vidhate and Kulkarni, 2017, Xu et al., 2020].
Among RL algorithms, policy gradient methods are particularly attractive due to their flexibility and capability to incorporate structured state and action spaces. This property makes them appealing for multi-agent learning, where agents usually need to update their policies through interactions with other agents either collaboratively or competitively. For instance, many recent works [Zhang et al., 2018, Chen et al., 2018, Wai et al., 2018, Li et al., 2019, Qu et al., 2020] have studied the convergence rate and sample complexity of gradient-based methods for collaborative multi-agent RL problems. In these problems, agents seek to maximize a global reward function collaboratively while the agents' policies and the learning procedure suffer from information constraints, i.e., each agent can only choose its own local actions based on the information it observes. However, due to a lack of understanding of the optimization landscape in these multi-agent learning problems, most such works
Preprint. Under review.

can only show convergence to a first-order stationary point. Deeper understanding of the quality of these stationary points is missing even in the simple identical-reward multi-agent RL setting.

In contrast, there has been some exciting recent theoretical progress on the analysis of the optimization landscape in centralized single-agent RL settings. Recent works have shown that the landscape for single-agent policy optimization enjoys the gradient domination property in both linear control [Fazel et al., 2018] and Markov decision processes (MDPs) [Agarwal et al., 2020], which guarantees gradient descent/ascent to find the global optimum despite the nonconvex landscape. Motivated by the theoretical progress in single-agent RL, we seek to study the landscape of multi-agent RL problems to see if similar results exist.

In this paper, we center our study on the multi-agent tabular MDP problem. Apart from the identical reward case mentioned earlier, we also make an attempt to generalize our analysis to game settings where rewards may vary among agents. The multi-agent tabular MDP problem is also known as the stochastic game (SG) in the field of game theory. The study of SGs dates back to as early as the 1950s by Shapley [1953], where the notion of SGs as well as the existence of Nash equilibria (NEs) were first established. A series of works has since been developed on designing NE-finding algorithms, especially in the RL setting (e.g. [Littman, 1994, Bowling and Veloso, 2000, Shoham et al., 2003, Bus¸oniu et al., 2010, Lanctot et al., 2017, Zhang et al., 2019a] and citations within). While well-known classical algorithms for solving SGs are mostly value-based, such as Nash-Q learning [Hu and Wellman, 2003], Hyper-Q learning [Tesauro, 2003], and WoLF-PHC [Bowling and Veloso, 2001], gradient-based algorithms have also started to gain popularity in recent years due to their advantages as mentioned earlier (e.g. [Abdallah and Lesser, 2008, Foerster et al., 2017, Zhang and Lesser, 2010]). In this work, our aim is to gain a deeper understanding of the structure and quality of first-order stationary points for these gradient-based methods. Specifically, taking a game-theoretic perspective, we strive to shed light on the following questions: 1) How do the firstorder stationary points relate to the NEs of the underlying game?, 2) Do gradient-based algorithms guarantee convergence to a NE?, 3) What is the stability of individual NEs?. For simpler finite action static (stateless) game settings, these questions have already been widely discussed [Shapley, 1964, Crawford, 1985, Jordan, 1993, Krishna and Sjöström, 1998, Shamma and Arslan, 2005, Kohlberg and Mertens, 1986, Van Damme, 1991]. For static continuous games, a recent paper [Mazumdar et al., 2020] in fact proved a negative result which suggests that gradient flow has stationary points (even local maxima) that are not necessarily NEs. Conversely, Zhang et al. [2019b] designed projected nested-gradient methods that provably converge to NEs in zero-sum linear quadratic games with continuous state-action spaces, linear dynamics, and quadratic rewards. However, much less is known in the setting of SGs with finite state-action spaces and general Markov transition probability.

Contributions. In our paper, we consider the gradient play algorithm for the infinite time-discounted reward SG where an agent's local policy is directly parameterized by the probability of choosing an action from the agent's own action space at a given state. We first show that first order stationary policies and Nash equilibria are equivalent for these directly parameterized local policies. We derive this by generalizing the gradient domination property in [Agarwal et al., 2020] to the multi-agent setting. Our result does not contradict Mazumdar et al. [2020]'s work which constructs examples of stable first order stationary points that are non-NEs, because their counterexamples consider general continuous games where the utility functions may not satisfy gradient domination.

Additionally, we provide a global convergence rate analysis for a special class of SG called Markov

potential games [González-Sánchez and Hernández-Lerma, 2013, Macua et al., 2018], which includes

identical reward multi-agent RL [Tan, 1993, Claus and Boutilier, 1998, Panait and Luke, 2005] as

an important special case. We show that gradient play (equivalent to projected gradient ascent for

Markov potential games) reaches an -Nash equilibrium within O

|S |

i |Ai|
2

steps, where |S|, |Ai|

denote the size of the state space and action space of agent i respectively. The convergence rate shows

that the number of iterations to reach an -NE scales linearly with the number of agents, instead of

exponentially with rate O

|S |

i |Ai|
2

as shown in the work of Agarwal et al. [2020]. Although

the convergence to NEs of different learning algorithms in static potential games has been very

well-studied in the literature [Monderer and Shapley, 1996a,b, Monderer and Sela, 1997, Marden

et al., 2009], to the best of our knowledge, our result provides the first non-asymptotic convergence

rate analysis of convergence to a NE in stochastic games for gradient play. We also study the local

geometry around some specific types of equilibrium points. For Markov potential games, we show

that strict NEs are local maxima of the total potential function and that fully mixed NEs are saddle

2

points. For general multi-agent MDPs, we show that strict NEs are locally stable under gradient play and provide a local convergence rate analysis.

2 Problem setting and preliminaries

An n-agent (tabular) Markov decision process (MDP) (or a stochastic game (SG) [Shapley, 1953])

M = (S, A = A1 × A2 × · · · × An, P, r = (r1, r2, . . . , rn), , )

(1)

is specified by: a finite state space S; a finite action space A = A1 × A2 × · · · × An, where Ai is the action space of agent i; a transition model P where P (s |s, a) = P (s |s, a1, a2, . . . , an) is the probability of transitioning into state s upon taking action a := (a1, . . . , an) (each agent i taking action ai respectively) in state s; i-th agent's reward function ri : S × A  [0, 1], i = 1, 2, . . . , n where ri(s, a) is the immediate reward of agent i associated with taking action a in state s; a discount factor   [0, 1); an initial state distribution  over S.

A stochastic policy  : S  (A) (where (A) is the probability simplex over A) specifies a decision-making strategy in which agents choose their actions jointly based on the current state in a stochastic fashion, i.e. Pr(at|st) = (at|st). A distributed stochastic policy is a special subclass of stochastic policies, with  = 1 × 2 × · · · × n, where i : S  (Ai). For distributed stochastic policies, each agent takes its action based on the current state s independently of other agents' choices of actions, i.e.:
n
Pr(at|st) = (at|st) = i(ai,t|st), at = (a1,t, . . . , an,t).
i=1

For notational simplicity, we define: I(aI|s) := iI i(ai|s), where I is an index set that is a subset of {1, 2, . . . , n}. Further, we use the notation -i to denote the index set {1, 2, . . . , n}\i.
Agent i's value function Vi : S  R, i = 1, 2, . . . , n is defined as the discounted sum of future rewards starting at state s via executing , i.e.



Vi(s) := E

tri(st, at) , s0 = s ,

t=0

where the expectation is with respect to the randomness of trajectory  = (st, at, ri,t) t=0 with s0 drawn from initial distribution  and at  (·|st), st+1 = P (·|st, at).

Agent i's Q-function Qi : S × A  R and the advantage function Ai : S × A  R are defined as:



Qi (s, a) := E

tri(st, at) , s0 = s, a0 = a , Ai (s, a) := Qi (s, a) - Vi(s).

t=0

Additionally, we define agent i's `averaged' Q-function Qi : S × Ai  R and `averaged' advantagefunction Ai : S × Ai  R as:

Qi (s, ai) := -i(a-i|s)Qi (s, ai, a-i), Ai (s, ai) := -i(a-i|s)Ai (s, ai, a-i). (2)

a-i

a-i

Direct distributed policy parameterizations. In this work, we consider direct distributed policy

parameterization, i.e., policies are parameterized by  = (1, . . . , n), where agent i's policy is

parameterized by i:

i,i (ai|s) = i,(s,ai), i = 1, 2, . . . , n.

(3)

For notational simplicity, we abbreviate i,i (ai|s) as i (ai|s), and i,(s,ai) as s,ai . Here i 

(Ai)|S|, i.e. i is subject to the constraints s,ai  0 and aiAi s,ai = 1 for all s  S. The

entire policy is given by

n

n

(a|s) = i (ai|s) = s,ai .

i=1

i=1

We also abbreviate Vi , Qi  , Ai  , Qi  , Ai  as Vi, Qi , Ai , Qi , Ai . We use

Xi := (Ai)|S|, X := X1 × X2 × · · · × Xn

3

to denote the feasible region of i and . Additionally we denote agent i's total reward as: Ji() = Ji(1, . . . , n) := Es0Vi(s0).

In the game setting, the concept of a Nash equilibrium is often used to characterize the performance of individuals' policies. We provide a definition below. Definition 1. (Nash equilibrium) Policy  = (1, . . . , n ) is called a Nash equilibrium (NE) if the following inequality holds:
Ji(i, - i)  Ji(i, - i), i  Xi, i = 1, 2, . . . , n.
The equilibrium is called a strict NE if the inequality holds strictly, i.e.:
Ji(i, - i) > Ji(i, - i), i  Xi, i = 1, 2, . . . , n. The equilibrium is called a pure NE if  corresponds to a deterministic policy.

An equilibrium that is not pure is called a mixed NE. Further, an equilibrium is called a fully mixed NE if every entry of  is strictly positive, i.e.: s,ai > 0,  ai  Ai,  s  S, i = 1, 2, . . . , n.

Policy gradient for direct distributed parameterization. It is useful to define the discounted state

visitation

distribution

dµ

of

a

policy



given

an

initial


state

distribution

µ

as:

dµ (s) := Es0µ(1 - ) tPr(st = s|s0),

(4)

t=0

where Pr(st = s|s0) is the state visitation probability that st = s when executing  starting at state s0. For simplicity, we use d(s) to denote d(s) that is the discounted state visitation distribution specifically for initial distribution  for the MDP (1).

The policy gradient is then given by (e.g. [Sutton et al., 1999]):

 Es0  Vi (s0 )

=

1 1 -  Esd Ea(·|s)[

log (a|s)Qi (s, a)],

i = 1, 2, . . . , n.

(5)

Applying (5), we have the following lemma (proof can be found in Appendix A):

Lemma 1. For direct distributed parameterization (3),

Ji()  s,ai

=

1

1 -

 d(s)Qi (s, ai)

(6)

3 Gradient domination for multi-agent MDP, and the equivalence of first order stationarity and Nash equilibrium

Agarwal et al. [2020] first established gradient domination for centralized tabular MDP under direct parameterization (Lemma 4.1 in [Agarwal et al., 2020]). We can show that similar property still holds for n-agent MDPs. This property will play a critical role in proving the equivalence of first order stationary policies and NEs in the later part of this section.

Lemma 2. (Gradient domination) For direct distributed parameterization (3), we have that for any  = (1, . . . , n)  X :

Ji(i, -i) - Ji(i, -i) 

d d

max (i - i) i Ji(),
 iXi

i  Xi,

i = 1, 2, . . . , n (7)

where

d d



:=

maxs

d d

(s) (s)

,

and



= (i, -i).

The proof of Lemma 2 can be found in Appendix B. For the single-agent case (n = 1), (7) is consistent

with the result in Agarwal et al. [2020], i.e.: J( ) - J() 

d d

 maxX ( - ) J ().

On the right hand side, maxX ( - ) J() can be understood as a scalar notion of first order stationarity in the constrained optimization setting. Thus if one can find  that is (approximately) a

first order stationary point, then gradient domination guarantees  will also be (near) optimal in terms

of function value. Such conditions are a standard device to establish convergence to global optima in

non-convex optimization, as they effectively rule out the existence of bad critical points. As for the

multi-agent case, we will show that this property rules out the existence of critical points that are not

NEs. We begin by giving a formal definition of first order stationary policies.

4

Definition 2. (First order stationary policy) Policy  = (1, . . . , n ) is called a first order stationary policy if the following inequality holds:
(i - i) i Ji()  0, i  Xi, i = 1, 2, . . . , n
Comparing Definition 1 (for NE) and Definition 2, we know that NEs are first order stationary policies, but not vice versa. For each agent i, first order stationarity does not rule out saddle points and does not imply that i is optimal among all possible i given other agents' policies being - i. However, interestingly, under appropriate assumptions, we can show that NEs are equivalent to first order stationary policies using the gradient domination property. Specifically, we make the following assumption on the MDPs we study. Assumption 1. The n-agent MDP (1) satisfies: d(s) > 0, s  S,   X
Assumption 1 requires that every state is visited with positive probability, which is a standard assumption to prove convergence in the RL literature (e.g. [Agarwal et al., 2020, Mei et al., 2020]). Theorem 1. Under Assumption 1, first order stationary policies and NEs are equivalent.
Due to space limit, we defer the proof to Appendix B.

4 Convergence analysis of gradient play for Markov potential games

Since the previous section suggests that first order stationary policies and NEs are equivalent, it is natural to explore the performance of first order methods, i.e., gradient based methods. The simplest such candidate might be the gradient play algorithm, which takes the following form:

i(t+1) = P rojXi (i + i Ji(i(t))),  > 0.

(8)

Gradient play can be viewed as a `better response' strategy, where each agent updates its own parameters by gradient ascent with respect to its own reward. While the gradient play dynamics in (8) looks simple and intuitive, it is in fact difficult to show that the dynamics converge to an equilibrium point, especially to a mixed NE, in general stochastic games. Even in the much simpler static game setup, gradient play-based learning algorithms might fail to converge [Shapley, 1964, Crawford, 1985, Jordan, 1993, Krishna and Sjöström, 1998]. The major difficulty lies in the fact that the vector field {i Ji()}ni=1 is not a potential gradient vector field. Accordingly, its dynamics can exhibit behavior more complicated than just convergence or divergence, e.g., chaos or convergence to a limit cycle. Thus, in this section, we restrict our analysis to a special subclass of games, namely the potential game, which is known to enjoy better convergence properties [Monderer and Shapley, 1996a, Monderer and Sela, 1997]. We now give our definition of potential games for n-agent MDPs.

Definition 3. (Markov potential game) A Markov decision process defined as (1) is called a Markov

potential game if there exists a potential function  : S × A1 × · · · × An  R such that for any agent i and any pair of policy parameters (i, -i), (i, -i) :





E

tri(st, at)  = (i, -i), s0 = s - E

tri(st, at)  = (i, -i), s0 = s

t=0

t=0





=E

t(st, at)  = (i, -i), s0 = s - E

t(st, at)  = (i, -i), s0 = s ,  s.

t=0

t=0

Similar definitions can be found in [González-Sánchez and Hernández-Lerma, 2013, Macua et al., 2018] for continuous game settings, where the Markov potential game has many applications, e.g. the great fish war [Levhari and Mirman, 1980], the stochastic lake game [Dechert and O'Donnell, 2006] etc. In MDP settings, however, `Markov potential game' is admittedly a rather strong assumption and difficult to verify for general MDPs. Nevertheless, one important special class of n-agent MDP that falls into this category is the identical reward setting, where all agents share the same reward function. Note that in this setting, agents take independent actions not because of competitive behavior, but due to physical constraints or simply to reduce the number of parameters.

For a Markov potential game, given a policy , we can define the `total potential function' :



() := Es0(·)

t(st, at)  .

t=0

5

From the definition of a Markov potential game and the total potential function, we have the following proposition that guarantees a Markov potential game to have at least one pure NE.
Proposition 1. (Proof see Appendix C) For a Markov potential game, the global maximum  of the total potential function , i.e.:  = argmaxX (), is a pure NE.

From the definition of the total potential function we obtain the following relationship

Ji(i, -i) - Ji(i, -i) = (i, -i) - (i, -i).

(9)

Thus,

i Ji() = i (),

which means that gradient play algorithm (8) is equivalent to running projected gradient ascent with

respect to the total potential function , i.e.:

(t+1) = P rojX ((t) + (i(t))),  > 0.

(10)

To measure the convergence to a NE, we define an -Nash equilibrium as follows:

Definition 4. ( -Nash equilibrium) Define the `NE-gap' of a policy  as:

NE-gapi()

:=

max
i Xi

Ji(i,

-i)

-

Ji(i,

-i);

NE-gap()

:=

max
i

NE-gapi().

A policy  is an -Nash equilibrium if: NE-gap()  .

Besides Assumption 1, we also assume that the Markov potential game satisfies the following assumption.
Assumption 2. (Bounded total potential function) For any policy   X , the total potential function () is bounded by: min  ()  max.

We are now ready to state our convergence result.

Theorem 2. (Global convergence to Nash equilibria) Suppose the n-agent MDP is a Markov potential

game with potential function (s, a), and suppose the total potential function  satisfies Assumption

2, then under gradient play algorithm (8) we have:

min NE-gap((t)) 
1tT

,

whenever

T



64M 2(max - min)|S| (1 - )3 2

n i=1

|Ai|

,

where M := max, X

d d

(by Assumption 1, we know that this quantity is well-defined).


The factor M is also known as the distribution mismatch coefficient that characterizes how the state

visitation varies with the choice of policies. Given an initial state distribution  that has positive

measure

on

every

state,

M

can

be

at

least

bounded

by

M



1 1-

max

d 





1 1-

1 mins (s)

.

The

proof of Theorem 2 is in Appendix D. Our proof structure resembles the proof of convergence for

single-agent MDPs in [Agarwal et al., 2020], where they leverage classical nonconvex optimization

results [Beck, 2017, Ghadimi and Lan, 2016] and gradient domination to get the convergence rate of

O

64

d 

2
|S ||A|


(1-)6 2

to the global optimum. In fact, our result matches this bound when there is

only one agent (the exponential factor on (1-) looks slightly different because some factors are

hidden implicitly n-agent MDP in a

in M and (max centralized way,

-min) in our bound). As a comparison, the size of the action space will be |A| =

if we parameterize the

n i=1

|Ai|,

which

blows

up exponentially with respect to the number of agents n. According to the result in [Agarwal et al.,

2020], projected gradient ascent needs O

|S |

n i=1

|Ai |

2

steps to find an -optimal policy, whereas we

only need O

|S |

n i=1

|Ai |

2

steps to find an -NE, which scales linearly with respect to n. However,

centralized parameterization can provably find a global optimum, while distributed parameterization

can only find a NE. In this sense, distributed parameterization sacrifices some optimality in favor of a

smaller parameter space.

Apart from enjoying better convergence properties, as suggested by Proposition 1, Markov potential games are guaranteed to have at least one pure NE. A natural follow-up question is whether gradient play (8) can find pure NEs (either deterministically or with high probability). Since the definition of an -NE is silent on the nature of the NE (e.g. pure or mixed), this motivates us to further examine the local geometry around pure and mixed NEs, which contains some second-order information that the definition of -NEs does not capture.

6

Theorem 3. For Markov potential games, any strict NE  is pure. Additionally, a strict NE is equivalent to a strict local maximum of the total potential function , i.e.:  , s.t.   =  that satisfies  -   ,   X , we have () < ().
Since a local maximum is locally asymptotic stable under projected gradient ascent, Theorem 3 suggests that strict NEs are locally stable. In the next section, we generalize the result to the general stochastic game setting. Note that this conclusion does not hold for settings other than stochastic games; for instance, for continuous games, one can use quadratic functions to construct simple counterexamples [Mazumdar et al., 2020]. Theorem 4. For Markov potential games, if  is a fully mixed NE, and suppose that min < max (i.e.,  is not a constant function), then  is a saddle point with regard to the total potential function , i.e.:   > 0,  , s.t.  -    and () > ().
Theorem 4 implies that by applying saddle point escaping techniques (see, e.g., [Ge et al., 2015]), first order methods can avoid convergence to fully mixed NEs. Note however that there is a gap between the theorems above and proving convergence to pure NEs, since pure but non-strict NEs as well as non-fully mixed NEs are not considered in these theorems. Nonetheless, we believe that these preliminary results can serve as a valuable platform towards a better understanding of the problem.
5 Local convergence to strict Nash equilibria for general multi-agent MDPs

In this section, we go beyond Markov potential games and consider general multi-agent MDPs. As

seen earlier, global convergence to a NE, especially to mixed NEs, remains problematic even in static

game settings. Thus, as a preliminary study, we focus our attention on the local stability and local

convergence rate around strict NEs in general multi-agent MDPs.

Lemma 3. For any n-agent MDP defined in (1), any strict NE  is also pure, meaning that for each

i and s, there exist one ai (s) such that s,ai = 1{ai = ai (s)}. Additionally,

ai (s) = argmax Ai  (s, ai)

(11)

ai

such that

Ai  (s, ai (s)) = 0;

Ai  (s, ai) < 0,  ai = ai (s).

Lemma 3 suggests that any strict NE is pure and that for every agent i and state s there is one optimal action ai (s) that maximizes the averaged advantage function of agent i (and thus also the averaged Q function). Thus we can define the following factors which will be useful in the next theorem:

i  (s)

:=

min
ai=ai (s)

Ai (s, ai)

,



:=

min min
is

1

1 -

 d (s)i  (s)

>

0.

(12)

Theorem 5. (Local finite time convergence around strict NE) Define a new metric of policy pa-

rameters as: D(|| ) := max1in maxsS i,s - i,s 1, where · 1 denote the 1- norm.

Suppose  is a strict Nash equilibrium of an n-agent MDP defined in (1). Then for any (0) such

that

D((0)||)




8n|S|(

(1-)3

) n
i=1

|Ai |

,

running gradient play (8) will guarantee:

D((t+1)||)  max D((t)||) -  , 0 , 2

which means that gradient play is going to converge within

2D((0) || ) 

steps.

Proofs of Theorem 5 and Lemma 3 can be found in Appendix F. The convergence rate seems counter-

intuitive at first glance. The convergence only requires finitely many steps and the stepsize  can

be chosen arbitrarily large so that exact convergence happens in one step. However, observe that our analysis is local, where we assume that the initial policy is sufficiently close to . Due to the

special geometry around strict NEs, one can indeed choose arbitrarily large stepsize for the algorithm

to converge in one step in this situation. However, for numerical stability considerations, one should

still pick reasonable stepsizes to run the algorithm to accommodate random initializations. Theorem

5

also

shows

that

the

radius

of

region

of

attraction

for

strict

NEs

is

at

least


8n|S|(

(1-
n i=1

)3 |Ai

|)

,

and

thus

 with a larger  will have a larger region of attraction. This intuitively implies that a strict NE

 with a larger value gap between the optimal action ai (s) and suboptimal actions will have a larger

region of attraction.

7

6 Numerical simulation

s2 = 1 s2 = 2

s1 = 1

2

0

s1 = 2

0

1

Table 1: Reward table for Game 1

a2 = 1 a2 = 2
a1 = 1 (-1,-1) (-3,0) a1 = 2 (0,-3) (-2,-2) Table 2: Reward table for Game 2

Game 1: state-based coordination game Our first numerical example studies the empirical performance of gradient play for an identical-reward Markov potential game. Consider a 2-agent identical reward coordination game problem with state space S = S1 × S2 and action space A = A1 × A2, where S1 = S2 = A1 = A2 = {1, 2}. The state transition probability is given by:
P (si,t+1 = 1|ai,t = 1) = 1 - , P (si,t+1 = 1|ai,t = 2) = , i = 1, 2.

The reward table is given by Table 1, where agents will only be rewarded if they are in the same state, and state 1 has a higher reward than state 2. Coordination games can be used to model the network effect in economics, where an agent reaps benefits from being in the same network as other agents. For this specific example, there are two networks with different utilities. Agents can observe the occupancy of each network, and take actions to join one of the networks based on the observation.

There

is

at

least

one

fully-mixed

NE

that

is

joining

network

1

with

probability

1-3 3(1-2

)

regardless

of the current occupancy of networks, and there are 13 different pure NEs that can be verified

numerically (computation of the NEs as well as detailed settings can be found in Appendix). Figure 1

shows a gradient play trajectory whose initial point lies in a close neighborhood of the mixed NE.

As the algorithm progresses, we see that the trajectory in Figure 1 diverges from the mixed-NE,

indicating that the fully-mixed NE is indeed a saddle point. This corroborates our finding in Theorem

4. Figure 2 shows the evolution of total reward J((t)) for gradient play for different random initial

points (0). We see the total reward is monotonically increasing for each initial point, which makes

sense since gradient play runs projected gradient ascent on the total reward function J. Since the only

NEs in this problem are either fully mixed or pure, the combination of Theorems 2 and 4 indicates

that gradient play will avoid the fully mixed NE and converge to one of the 13 different pure NEs. In

Figure 2, we see different initial points converging to one of 13 different NEs each with a different

total reward (some strict NEs with relatively small region of attraction are omitted in the figure).

While the total rewards are different, as shown in Figure 3, we see that the NE-gap of each trajectory

(corresponding to same initial points in Figure 2) converges to 0. This suggests that the algorithm is

indeed able to converge to a NE. Notice that NE-gaps do not decrease monotonically.

Figure 1: (Game 1:) Starting from a close neighborhood of a fully mixed NE

Figure 2: (Game 1:) Total reward for multiple runs

 ratio (mean ± std)%

0.1 433.3

(47.8± 5.1)%

0.05 979.3

(66.3± 4.3)%

0.01 2498.6 (77.4± 2.8)%

Table 3: (Game 2:) Relationship of convergence ratio and . Here we

fix 

=

0.95.

Convergence ratio is

calculated

by

#Trials that converge to  #Total number of trials

.

Here we calculate one ratio using 100 trials and the mean and standard

deviation (std) are calculated by computing the ratio 10 times using

different trials.

Figure 3: (Game 1:) NE-gap for multiple runs
Figure 4: (Game 2:) Convergence to the cooperative NE

Game 2: multi-stage prisoner's dilemma The second example -- multi-stage prisoner's dilemma model[Arslan and Yüksel, 2016] -- studies gradient play for settings other than Markov potential

8

game. It is also a 2-agent MDP, with S = A1 = A2 = {1, 2}. Assume that the reward for each agent ri(s, a1, a2), i  {1, 2} is independent of state s and is given by Table 2. Similar to the state-based coordination game, the state transition probability is determined by agents' previous actions:
P (st+1 = 1|(a1,t, a2,t) = (1, 1)) = 1 - , P (st+1 = 1|(a1,t, a2,t) = (1, 1)) =

Here action ai = 1 means that agent i choose to cooperate and ai = 2 means betray. The state s serves as a noisy indicator, with accuracy 1 - , of whether or not both agents cooperated in the previous stage. Unlike the identical-reward state-based coordination game, the two agents in this model obtain different rewards when they pick different actions.

The single-stage game corresponds to the famous Prisoner's Dilemma, and it is well-known that there is a unique NE (a1, a2) = (2, 2), where both agent decide to betray. The dilemma arises from the fact that there exists a joint non-NE strategy (1, 1) such that both players obtain a higher reward
than what they get under the NE. However, in the multi-stage case, the introduction of an additional state s allows agents to make decisions based on whether they have cooperated before. Intuitively,
given that both agents cooperated at the previous stage, it is more beneficial to keep this cooperation
rather than destroy it by betraying. It turns out that cooperation can be achieved in this manner given that the two agents are patient (i.e.,  is close to 1) and that indicator s is accurate enough (i.e. is close to 0). Apart from the fully betray strategy, where both agents will betray regardless of s, there is another strict NE  that is s=1,ai=1 = 1, s=2,ai=1 = 0, where agents will cooperate given that they have cooperated in previous stage, and betray otherwise.

We simulate gradient play for this model and mainly focus on the convergence to the cooperative

equilibrium . The initial policy is set as: s(0=)1,ai=1 = 1 - 0.4i, s(0=)2,ai=1 = 0, where i's are uniformly sampled from [0, 1]. The initialization implies that at the beginning both agents are willing

to cooperate to some extent given that they cooperated at the previous stage. Figure 4 shows a trial

converging to the NE starting from a randomly initialized policy. The size of the region of attraction

for



can

be

reflected

by

the

ratio

of

convergence

(

#Trials that converge to  #Total number of trials

)

for

multiple

trials

with

different initial points. An empirical estimate of the volume of the region is the convergence ratio

times the volume of the uniform sampling area, thus the larger the ratio, the larger the region of

attraction. Table 3 demonstrates the change of the ratio with regard to different values of indicator

error . Intuitively speaking, the more accurately the state s represents the cooperation situation of

agents, the less incentive agents will have for betraying when observing s = 1, and thus the larger

the convergence ratio will be. This intuition matches the simulation result as well as the theoretical

guarantees on the local convergence around a strict NE in Theorem 5.

7 Conclusion and Discussion
This paper studies the optimization landscape of multi-agent reinforcement learning through a game theoretic point of view. Specifically, we look into the tabular multi-agent MDP problem and prove that all first order stationary policies are NEs under this setting. Convergence rate analysis is also given for a special subclass of stochastic games called the Markov potential game, showing that the convergence rate to a NE scales linearly with the number of agents. Additionally, local geometry around strict NEs and fully-mixed NEs are also studied. We have shown that strict NEs are the local maxima of the total potential function and fully mixed NEs are the saddle points. We also give a local convergence rate around strict NEs for the more general multi-agent MDP setting.
We believe that this is a fruitful research direction with many interesting open questions. For instance, one could explore generalizing our work (which assumes access to exact gradients) to a setting where gradients are estimated using data. Extending our results beyond direct policy parameterization, to say softmax parameterization (cf. [Agarwal et al., 2020]), is another interesting topic. Other interesting questions include local stability analysis in more general games (beyond Markov potential games), faster algorithm design (via e.g. natural policy gradient, Gauss-Newton methods), and online algorithm design for stochastic learning.

References
S. Abdallah and V. Lesser. A multiagent reinforcement learning algorithm with non-linear dynamics. Journal of Artificial Intelligence Research, 33:521­549, 2008.
9

A. Agarwal, S. M. Kakade, J. D. Lee, and G. Mahajan. On the theory of policy gradient methods: Optimality, approximation, and distribution shift, 2020.
G. Arslan and S. Yüksel. Decentralized q-learning for stochastic teams and games. IEEE Transactions on Automatic Control, 62(4):1545­1558, 2016.
A. Beck. First-order methods in optimization. SIAM, 2017.
M. Bowling and M. Veloso. An analysis of stochastic game theory for multiagent reinforcement learning. Technical report, Carnegie-Mellon Univ Pittsburgh Pa School of Computer Science, 2000.
M. Bowling and M. Veloso. Rational and convergent learning in stochastic games. In International joint conference on artificial intelligence, volume 17, pages 1021­1026. Citeseer, 2001.
L. Bus¸oniu, R. Babuska, and B. De Schutter. Multi-agent reinforcement learning: An overview. Innovations in multi-agent systems and applications-1, pages 183­221, 2010.
T. Chen, K. Zhang, G. B. Giannakis, and T. Bas¸ar. Communication-efficient policy gradient methods for distributed reinforcement learning. arXiv preprint arXiv:1812.03239, 2018.
C. Claus and C. Boutilier. The dynamics of reinforcement learning in cooperative multiagent systems. AAAI/IAAI, 1998(746-752):2, 1998.
V. P. Crawford. Learning behavior and mixed-strategy nash equilibria. Journal of Economic Behavior & Organization, 6(1):69­78, 1985.
F. Daneshfar and H. Bevrani. Load­frequency control: a ga-based multi-agent reinforcement learning. IET generation, transmission & distribution, 4(1):13­26, 2010.
W. D. Dechert and S. O'Donnell. The stochastic lake game: A numerical solution. Journal of Economic Dynamics and Control, 30(9-10):1569­1587, 2006.
M. Fazel, R. Ge, S. M. Kakade, and M. Mesbahi. Global convergence of policy gradient methods for linearized control problems. CoRR, abs/1801.05039, 2018. URL http://arxiv.org/abs/ 1801.05039.
J. N. Foerster, R. Y. Chen, M. Al-Shedivat, S. Whiteson, P. Abbeel, and I. Mordatch. Learning with opponent-learning awareness. arXiv preprint arXiv:1709.04326, 2017.
R. Ge, F. Huang, C. Jin, and Y. Yuan. Escaping from saddle points--online stochastic gradient for tensor decomposition. In Conference on learning theory, pages 797­842. PMLR, 2015.
S. Ghadimi and G. Lan. Accelerated gradient methods for nonconvex nonlinear and stochastic programming. Mathematical Programming, 156(1-2):59­99, 2016.
D. González-Sánchez and O. Hernández-Lerma. Discrete­time stochastic control and dynamic potential games: the Euler­Equation approach. Springer Science & Business Media, 2013.
J. Hu and M. P. Wellman. Nash q-learning for general-sum stochastic games. Journal of machine learning research, 4(Nov):1039­1069, 2003.
J. S. Jordan. Three problems in learning mixed-strategy nash equilibria. Games and Economic Behavior, 5(3):368­386, 1993.
S. M. Kakade and J. Langford. Approximately optimal approximate reinforcement learning. In C. Sammut and A. G. Hoffmann, editors, Machine Learning, Proceedings of the Nineteenth International Conference (ICML 2002), University of New South Wales, Sydney, Australia, July 8-12, 2002, pages 267­274. Morgan Kaufmann, 2002.
E. Kohlberg and J.-F. Mertens. On the strategic stability of equilibria. Econometrica: Journal of the Econometric Society, pages 1003­1037, 1986.
V. Krishna and T. Sjöström. On the convergence of fictitious play. Mathematics of Operations Research, 23(2):479­511, 1998.
10

M. Lanctot, V. Zambaldi, A. Gruslys, A. Lazaridou, K. Tuyls, J. Pérolat, D. Silver, and T. Graepel. A unified game-theoretic approach to multiagent reinforcement learning. arXiv preprint arXiv:1711.00832, 2017.
D. Levhari and L. Mirman. The great fish war: An example using a dynamic cournot-nash solution. Bell Journal of Economics, 11(1):322­334, 1980.
Y. Li, Y. Tang, R. Zhang, and N. Li. Distributed reinforcement learning for decentralized linear quadratic control: A derivative-free policy optimization approach, 2019.
M. L. Littman. Markov games as a framework for multi-agent reinforcement learning. In Machine learning proceedings 1994, pages 157­163. Elsevier, 1994.
S. V. Macua, J. Zazo, and S. Zazo. Learning parametric closed-loop policies for markov potential games. CoRR, abs/1802.00899, 2018. URL http://arxiv.org/abs/1802.00899.
J. R. Marden, G. Arslan, and J. S. Shamma. Joint strategy fictitious play with inertia for potential games. IEEE Transactions on Automatic Control, 54(2):208­220, 2009.
E. Mazumdar, L. J. Ratliff, and S. S. Sastry. On gradient-based learning in continuous games. SIAM Journal on Mathematics of Data Science, 2(1):103­131, 2020.
J. Mei, C. Xiao, C. Szepesvari, and D. Schuurmans. On the global convergence rates of softmax policy gradient methods. In H. D. III and A. Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 6820­6829. PMLR, 13­18 Jul 2020.
D. Monderer and A. Sela. Fictitious play and no-cycling conditions. 1997.
D. Monderer and L. S. Shapley. Potential games. Games and economic behavior, 14(1):124­143, 1996a.
D. Monderer and L. S. Shapley. Fictitious play property for games with identical interests. Journal of economic theory, 68(1):258­265, 1996b.
L. Panait and S. Luke. Cooperative multi-agent learning: The state of the art. Autonomous agents and multi-agent systems, 11(3):387­434, 2005.
G. Qu, A. Wierman, and N. Li. Scalable reinforcement learning of localized policies for multi-agent networked systems. In Learning for Dynamics and Control, pages 256­266. PMLR, 2020.
S. Shalev-Shwartz, S. Shammah, and A. Shashua. Safe, multi-agent, reinforcement learning for autonomous driving. ArXiv, abs/1610.03295, 2016.
J. S. Shamma and G. Arslan. Dynamic fictitious play, dynamic gradient play, and distributed convergence to nash equilibria. IEEE Transactions on Automatic Control, 50(3):312­327, 2005. doi: 10.1109/TAC.2005.843878.
L. Shapley. Some topics in two-person games. Advances in game theory, 52:1­29, 1964.
L. S. Shapley. Stochastic games. Proceedings of the national academy of sciences, 39(10):1095­1100, 1953.
Y. Shoham, R. Powers, and T. Grenager. Multi-agent reinforcement learning: a critical survey. Technical report, Technical report, Stanford University, 2003.
R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. A Bradford Book, Cambridge, MA, USA, 2018. ISBN 0262039249.
R. S. Sutton, D. A. McAllester, S. P. Singh, Y. Mansour, et al. Policy gradient methods for reinforcement learning with function approximation. In NIPs, volume 99, pages 1057­1063. Citeseer, 1999.
M. Tan. Multi-agent reinforcement learning: Independent vs. cooperative agents. In Proceedings of the tenth international conference on machine learning, pages 330­337, 1993.
11

G. Tesauro. Extending q-learning to general adaptive multi-agent systems. Advances in neural information processing systems, 16:871­878, 2003.
E. Van Damme. Stability and perfection of Nash equilibria, volume 339. Springer, 1991. D. A. Vidhate and P. Kulkarni. Cooperative multi-agent reinforcement learning models (cmrlm)
for intelligent traffic control. In 2017 1st International Conference on Intelligent Systems and Information Management (ICISIM), pages 325­331. IEEE, 2017. H.-T. Wai, Z. Yang, Z. Wang, and M. Hong. Multi-agent reinforcement learning via double averaging primal-dual optimization. NIPS'18, page 9672­9683, Red Hook, NY, USA, 2018. Curran Associates Inc. W. Wang and M. Á. Carreira-Perpiñán. Projection onto the probability simplex: An efficient algorithm with a simple proof, and an application. CoRR, abs/1309.1541, 2013. URL http: //arxiv.org/abs/1309.1541. X. Xu, Y. Jia, Y. Xu, Z. Xu, S. Chai, and C. S. Lai. A multi-agent reinforcement learning-based data-driven method for home energy management. IEEE Transactions on Smart Grid, 11(4): 3201­3211, 2020. C. Zhang and V. Lesser. Multi-agent learning with policy prediction. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 24, 2010. K. Zhang, Z. Yang, H. Liu, T. Zhang, and T. Basar. Fully decentralized multi-agent reinforcement learning with networked agents. In International Conference on Machine Learning, pages 5872­ 5881. PMLR, 2018. K. Zhang, Z. Yang, and T. Bas¸ar. Multi-agent reinforcement learning: A selective overview of theories and algorithms. arXiv preprint arXiv:1911.10635, 2019a. K. Zhang, Z. Yang, and T. Basar. Policy optimization provably converges to nash equilibria in zerosum linear quadratic games. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019b. URL https://proceedings.neurips.cc/paper/2019/file/ 5446f217e9504bc593ad9dcf2ec88dda-Paper.pdf.
12

A Proof of Lemma 1

Proof. According to policy gradient theorem (5):

Ji() = 1

 s,ai

1-
s

a

d (s

) (a

|s

)



log (a  s,ai

|s

)

Qi (s,

a)

Since for direct parameterization:

 log (a |s )  s,ai

=

 log i (ai|s )  s,ai

= 1{ai = ai, s

1 = s}
s,ai

1

= 1{ai = ai, s

= s} i (ai|s)

Thus we have that:

Ji() = 1

 s,ai

1-
s

a

d(s )(a |s )1{ai = ai, s

=

s}

1  (ai |s)

Qi (s,

a)

=

1 1-

a-i

d

(s)i

(ai

|s)-i

(a-i

|s)



1 (ai

|s)

Qi

(s,

ai

,

a-i

)

(13)

1 = 1 -  d(s)

-i (a-i|s)Qi (s, ai, a-i)

a-i

=

1

1 -

 d(s)Qi (s, ai)

B Proofs of Lemma 2 and Theorem 1

Before giving proofs for Lemma 2 and Theorem 1, we first introduce the well-known the performance difference lemma [Kakade and Langford, 2002] in RL literature, which is helpful throughout. A proof is also provided for completeness.

Lemma 4. (Performance difference lemma) For policies ,  ,

Ji(

)

-

Ji()

=

1 1 -  Esd

Ea

Ai (s, a),

i = 1, 2, . . . , n

(14)

Proof. (of performance difference lemma) From Bellman's equation we have that:

Ji( ) - Ji() = Es0Vi (s0) - Es0Vi(s0)

= EsEa (·|s)Qi (s, a) - EsVi(s) + EsVi (s) - EsEa (·|s)Qi (s, a)

= EsEa (·|s)Ai (s, a) + Es0,sPr (s1=·|s0)Vi (s) - Vi (s) = EsEa (·|s)Ai (s, a) + Es0,sPr (s1=·|s0)Ea (·|s)Ai (s, a)
+ 2Es0,sPr (s2=·|s0)Vi (s) - Vi (s) = ···



=

Es0,sPr (st=·|s0)Ea tAi (s, a)

t=0

=

1 1 -  Esd

Ea

Ai (s, a)

13

Proof. (Lemma 2) According to performance difference lemma (14):

1 Ji(i, -i) - Ji(i, -i) = 1 - 

d (s) (a|s)Ai (s, a)

s,a

1 =
1-

d (s)i (ai|s) -i (a-i|s)Ai (s, ai, a-i)

(15)

s,ai

a-i

1 =
1-

d (s)i (ai|s)Ai (s, ai).

s,ai

According to the definition of `averaged' advantage function:

i (ai|s)Ai (s, ai) = 0, s  S
ai

which implies:

max
ai Ai

Ai (s,

ai)



0,

thus we have that:

1 Ji(i, -i) - Ji(i, -i) = 1 - 

d (s)i (ai|s)Ai (s, ai)

s,ai

1 
1-

s

d

(s) max
ai Ai

Ai (s, ai)

(16)

1 =
1-

s

d d

(s) (s)

d

(s)

max
ai Ai

Ai

(s,

ai

)

1 
1-

d d



s

d (s)

max
ai Ai

Ai

(s,

ai).

We

can

rewrite

1 1-

s d(s) maxaiAi Ai (s, ai) as:

1 1-

s

d (s)

max
ai Ai

Ai

(s,

ai)

=

1 1-

max
i Xi

s,ai

d(s)i (ai|s)Ai (s, ai)

=

max
i Xi

(i (ai|s)
s,ai

-

i (ai|s)) 1

1 -



d(s)Ai (s,

ai)

(17)

=

max
i Xi

(i (ai|s)
s,ai

-

i (ai|s)) 1

1 -



d(s)Qi (s,

ai)

= max (i - i) i Ji().
i Xi

Substituting this into (16), we may conclude that

Ji(i, -i) - Ji(i, -i) 

d d

max (i - i) i Ji()
 iXi

and this completes the proof.

Proof. (Theorem 1) The definition of a Nash equilibrium naturally implies first order stationarity, because for any i  Xi: Ji((1 - )i + i, - i) - Ji(i, - i) = (i - ) i Ji() + o( i - i )  0,   > 0 Letting   0 gives the first order stationary condition:
(i - ) i Ji()  0, i  Xi,

14

It remains to be shown that all first order stationary policies are Nash equilibria. From Assumption 1

we know that for any pair of parameters  , ,

d d

< +.


Take  = (i, - i),  = (i, - i). According to Lemma 2, we have that for any first order stationary

policy ,

Ji(i, - i) - Ji(i, - i) 

d d

max (i - i)
 iXi

i Ji()  0,

which completes the proof.

C Proof of Proposition 1

Proof. First of all, from the definition of NE, the global maximum of the potential function is a NE. We now show that this global maximum is a deterministic policy. From classical results (e.g. [Sutton and Barto, 2018]) we know that there's a optimal deterministic centralized policy

(a = (a1, . . . , an)|s) = 1{a = a(s) = (a1(s), . . . , an(s))}

that maximizes:


 = argmax E

t(st, at) , s0 = s .

:S(A) t=0

We now show that this centralized policy can also be represented by direct distributed policy parameterization. Set  as:
i (ai|s) = 1{ai = ai (s)},

then:
n

(a|s) = i (ai|s)
i=1

Since  globally maximizes the discounted summation of potential function  among centralized policies, which includes all possible direct distributedly parameterized policies,  also maximizes

the total potential function  globally among all direct distributed parameterization, which completes

the proof.

D Proof of Theorem 2

Lemma 5. Let f () be -smooth in , define gradient mapping:

G ()

:=

1 

(P rojX (

+

f ())

-

) .

The update rule for projected gradient is:

+ =  + G() = P rojX ( + f ()).

Then:

( - +) f (+)  (1 + ) G()  - +   X .

Proof. By a standard property of Euclidean projections onto a convex set, we get that ( + f () - +) ( - +)  0
= f () ( - +) + ( - +) ( - +)  0 = f () ( - +) - G() ( - +)  0 = f () ( - +)  G()  - + = f (+) ( - +)  G()  - + + (f (+) - f ()) ( - +)
 G()  - + +  + -   - + = (1 + ) G()  - +

15

Proof. (Proof of Theorem 2) Recall the definition of gradient mapping:

G ()

=

1 

(P rojX (

+

f ())

-

) .

From

Lemma

7

we

have

()

is

-smooth

with



=

2 (1-)3

i |Ai|. Then from standard result (See

Theorem

10.15

in

[Beck,

2017]

or

Theorem

E.1

in

[Agarwal

et

al.,

2020])

we

have

that

for



=

1 

:

min

G((t)) 

t=0,1,...,T -1

2(max - min) T

From gradient domination property (7) we have that:

NE-gapi((t+1))

=

max
i Xi

Ji(i.-(ti+1))

-

Ji(i(t+1),

-(ti+1))

 max
i Xi

d(i .- (ti+1) ) d(i(t+1) ,- (ti+1) )

max
i Xi 

i - i(t+1)

 M max i - i(t+1)
i Xi

i ((t+1))

 M (1 + ) max i - i(t+1) G((t))
i Xi

 M (1 + )2 |S| G((t)) ,

i Ji((t+1))

(18)

where the last step follows as i - i(t+1)  2 |S|. And then using (18) and  = 1, we have:

min NE-gap(t+1)  4M |S| 2(max - min)

t=0,1,...,T -1

T

we can get our required bound of if we set:

or equivalently

4M |S| 2(max - min)  , T

T



32M 2(max
2

-

min )|S |

=

64M 2(max - min)|S| 2(1 - )3

i |Ai| ,

which completes the proof.

E Proof of Theorem 3 and 4

Proof. (Theorem 3) The proof requires knowledge of Lemma 3 in Section 5 thus we would recom-
mend readers to first go through Lemma 3 first. The lemma immediately leads to the conclusion that a strict NE  should be deterministic. Let ai (s), i  (s), i  be the same definition as (11)(12) respectively.

For any   X , Taylor expansion suggests that:

() - () = ( - ) () + o(  -  )

= (i - i) i Ji() + o(  -  )

i

1 =
1-
i

d (s)Ai  (s, ai)(s,ai - s,ai ) + o(  -  )
s ai





1 -
1-
i

d (s)i  (s) 

(s,ai - s,ai ) + o(  -  )

s

ai=ai (s)

16

1 =-
1-

d

(s)i 

(s)

1 2

i,s - i,s

1 + o(

 - 

)

is

 -
2

i,s - i,s 1 + o(  -  )

is

 -

 - 

+ o(  -  ).

2

Thus for  -  sufficiently small,

() - () < 0 holds,

this suggests that strict NEs are strict local maxima. We now show that this also holds vice versa.

Strict local maxima satisfy first order stationarity by definition, and thus by Theorem 1 they are also NEs, we only need to show that they are strict. We prove by contradiction, suppose that there exists a local maximum  such that it is non-strict NE, i.e., there exists i  Xi, i = i such that:
Ji(i, - i) = Ji(i, - i)
According to (17) and first order stationarity of :

1 1-

s

d (s)

max
ai Ai

Ai  (s,

ai)

=

max (i
i Xi

-

i)

i Ji()  0.

Since maxaiAi Ai (s, ai)  0 for all , we may conclude:

max
ai Ai

Ai  (s,

ai)

=

0,

 s  S, ai  Ai.

We denote  := (i, -i ), according to (15)

0

=

Ji(i, - i) -

Ji(i, - i)

=

1 1-



d (s)i (ai|s)Ai  (s, ai)  0.

s,ai

Since d (s) > 0,  s, this further implies that

i (ai|s)Ai  (s, ai) = 0,  s  S,
ai

i.e., i (ai|s) is nonzero only if Ai (s, ai) = 0. Define i := i + (1 - )i, then

Thus let  := (i, - i)

i (ai|s)Ai  (s, ai) = 0,  s  S.
ai

Ji(i, - i)

-

Ji(i, - i)

=

1

1 -



d (s)i (ai|s)Ai  (s, ai) = 0.

s,ai

Since i - i  0 as   0, this contradicts the assumption that  is a strict local maximum. This suggests that all strict local maxima are strict NEs, which completes the proof.

Proof. (Theorem 4) First, we define the corresponding value function, Q-function and advantage function for potential function .



V(s) := E

t(st, at)  = , s0 = s

t=0



Q(s, a) :=

t(st, at)  = , s0 = s, a0 = a

t=0

A(s, a) := Q(s, a) - V(s).

17

For an index set I  {1, 2, . . . , n} we define the following averaged advantage potential function of

index set I as:

A,I (s, aI ) := A(s, aI , a-I ).

a-I
We choose an index set I  {1, 2, . . . , n} such that there exists s, aI such that:

A,I (s, aI ) > 0,

(19)

and that for any other index set I with smaller cardinality:

A,I (s, aI )  0,  s, aI ,  |I | < |I|.

(20)

Because  is not a constant, this guarantees the existence of such an index set I. Further, since

 I

(aI

|s)A,I

(s, aI

)

=

0,

 s,

aI

and that  is fully-mixed, we have that:

A,I (s, aI ) = 0,  s, aI ,  |I | < |I|.

(21)

We set  := (I, - I), where I is a convex combination of I , I  X :

I = (1 - )I + I ,  > 0.

According to performance difference lemma (14) we have:

(1 - ) (I , - I ) - (I , - I ) = d(s)I (aI |s)A,I (s, aI )
s,aI

=

d (s)

(1 - )i (ai|s) + i (ai|s) A,I (s, aI )

s,aI

iI

=

d(s) (1-)i0 (ai0 |s) + i0 (ai0 |s)

(1-)i (ai|s) + i (ai|s) A,I (s, aI ), ( i0  I)

s,aI

iI\{i0}

= (1 - ) d(s)

(1-)i (ai|s) + i (ai|s) A,I\{i0}(s, aI\{i0})

s,aI

iI\{i0}

+  d(s)i0 (ai0 |s)

(1 - )i (ai|s) + i (ai|s) A,I (s, aI ).

s,aI

iI \{i0 }

According to (21), we know that:

A,I\{i0}(s, aI\{i0}) = 0, thus
(1 - ) (I , - I ) - (I , - I ) =

 d(s)i0 (ai0 |s)

(1 - )i (ai|s) + i (ai|s) A,I (s, aI ).

s,aI

iI \{i0 }

Applying similar procedures recursively and using the fact that:

we get:

A,I\{i}(s, aI\{i}) = 0,  i  I,

(I , - I )

-

(I , - I )

=

|I| 1-

d(s) i (ai|s)A,I (s, aI ).

s,aI

iI

Set i (ai|s) as:

i (ai|s) =

1 ai = ai 0 otherwise

i (ai|s) = i (ai|s), s = s, where s, ai are defined in (19). Then:

(I , - I ) - (I , - I )

=

|I 1-

|


d

(s

)A,I

(s

,

aI

)

>

0,

which completes the proof.

18

F Proof of Theorem 5

Proof. (Lemma 3) For a given strict NE  randomly set:

ai (s)  argmax Ai  (s, ai),
ai

and set i be:

s,ai = 1{ai = ai (s)}.

And set  := (i, - i) From performance difference lemma (14):

Ji(i, - i) - Ji(i, - i) = d(s)i (s, ai)Ai  (s, ai)
s,ai

=

s

d (s)

max
ai

Ai 

(s,

ai)



0

Because  is a strict NE, thus the inequality above forces i = , and that maxai Ai  (s, ai) = 0. The uniqueness of  also implies uniqueness of ai (s), and thus,
Ai (s, ai) < 0,  ai = ai (s), which completes the proof of the lemma.

The proof of Theorem 5 relies on the following auxiliary lemma, whose proof we defer to Appendix H.
Lemma 6. Let X denote the probability simplex of dimension n. Suppose   X , g  Rn and that there exists i  {1, 2, . . . , n} and  > 0 such that:

i  i, i = i gi  gi + , i = i.

Let then:

 = P rojX ( + g),



i

 min{1, i

+

} 2

Proof. (Theorem 5) For a fixed agent i and state s, the gradient play (8) update rule of policy i,s is

given by:

i(,ts+1)

=

P roj(|Ai|)(i(,ts)

+

1

 -

 d(t) (s)Qi(t) (s, ·)),

(22)

where

(|Ai|)

denotes

the

probability

simplex

in

|Ai|-th

dimension

and

Q(t)
i

(s,

·))

is

a

|Ai|-th

dimensional

vector

with

ai-th

element

equals

to

Q(t)
i

(s,

ai

)).

We

will

show

that

this

update

rule

satisfies the conditions in Lemma 6, which will then allow us to prove that

D((t+1)||)  max{0, D((t)||) -  }. 2

Letting ai (s) be the same definition as (11), we have that:

1

1 -



d(t) (s)Qi (t) (s,

ai (s))

-

1

1 -



d(t) (s)Qi(t) (s, ai)

 1

1 -



d (s)Qi  (s, ai (s))

-

1

1 -



d (s)Qi  (s,

ai)

-

1

1 -

 d (s)Qi  (s, ai (s))

-

1

1 -

 d(t) (s)Qi (t) (s, ai (s))

-

1

1 -

 d (s)Qi  (s, ai)

-

1

1 -

 d(t) (s)Qi (t) (s, ai)

19

1  1 -  d (s)

Ai  (s, ai (s) - Ai  (s, ai))

- 2 i Ji((t)) - i Ji()

(23)



-

(1

4 - )3

n
|Ai|

(t) - 

(24)

i=1

 -

4

(1 - )3

n
|Ai|
i=1

n i=1 s

i(,ts) - i,s) 1



-

(1

4 - )3

n|S |

n
|Ai| D((t)||),

i=1

where (23) to (24) uses smoothness property in Lemma 7.

We use proof of induction as supposed for  t - 1, we have:

D(( +1)||)  max{D(( )||) -  , 0}, 2

thus

D((t)||)



D((0)||)



 (1 - )3

8n|S| (

n i=1

|Ai|)

.

Then we can further conclude that:

(1 - )d(t) (s)Qi (t) (s, ai (s)) - (1 - )d(t) (s)Qi(t) (s, ai)

 -

4 n|S |

(1 - )3

n
|Ai| D((t)||)

i=1

 ,
2

 ai = ai (s)

Additionally,

for

D((t)||)




8n|S|(

(1-
n i=1

)3 |Ai

|)

,

we

may

conclude:

s(t,a) i (s)  1/2  s(t,a) i ai = ai (s), then by applying Lemma 6 to (22) we have:

s(t,a+i1()s)



min{1, s(t,a) i (s)

+

 4

}

= i(,ts+1) - i,s 1 = 2 1 - s(t,a+i1()s)

 max{0,

i(,ts) - i,s



1-

}, 2

 s  S, i = 1, 2, . . . , n

= D((t+1)||)  max{0, D((t)||) -  }, 2

which completes the proof.

G Smoothness

Lemma 7. (Smoothness for Direct Distributed Parameterization) Assume that 0  ri(s, a)  1, s, a, i = 1, 2, . . . , n, then:

2

n

g( ) - g()  (1 - )3

|Ai|  -  ,

(25)

i=1

where g() = {i Ji()}.

The proof of Lemma 7 depends on the following lemma:

20

Lemma 8.

2

n

i Ji( ) - i Ji()  (1 - )3 |Ai|

|Aj | j - j

(26)

j=1

Lemma 7 is a simple corollary of Lemma 8.

Proof. (Proof of Lemma 7)

n

g( ) - g() 2 =

i Ji( ) - i Ji() 2

i=1



2

2

n

 (1 - )3

|Ai| 

i

j=1

2 |Aj| j - j 







2

2

 (1 - )3

n

n

|Ai|  |Aj| 

j - j

2


i

j=1

j=1

2

2

= (1 - )3

n

2

|Ai|  -  2,

i=1

which completes the proof.

Lemma 8 is equivalent to the following lemma: Lemma 9.

Ji(i + ui, -i) - Ji(i + ui, -i)



=0

2  (1 - )3

n
|Ai|
j=1

|Aj| j-j ,

 u =1 (27)

Proof. (Lemma 9) Define:

i,(ai|s) := i+ui (ai|s) = s,ai + uai,s i,(ai|s) := i+ui (ai|s) = s,ai + uai,s
(a|s) := i+ui (ai|s)-i (a-i|s) (a|s) := i+ui (ai|s)-i (a-i|s) Qi (s, a) := Q(i+ui,-i)(s, a)
d(s) := d(i+ui,-i)(s)
According to cost difference lemma,

Ji(i + ui, -i) - Ji(i + ui, -i)



=0

1 =

 s,a d (s)(a|s)Ai (s, a)

1-



=0

1 =

 s,a d (s) ((a|s) - (a|s)) Qi (s, a)

1-



=0





1

  

1- 

d

(s)



(a|s) -  

(a|s)

Qi (s, a)
=0

 s,a

Part A

21

+

s,a

d

(s)(

(a|s)

-



(a|s))

Qi (s, 

a)

=0

Part B


Thus:

+

s,a

d(s) 

=0((a|s) - (a|s))Qi (s, a)

    

Part C

Part A =

s,a

d

(s)

(a|s) - (a|s) 

=0Qi (s, a)

=

d(s)uai,s(-i (a-i|s) - -i (a-i|s))Qi (s, a)

(28)

s,a

1 
1-

d(s) |uai,s|

-i (a-i|s) - -i (a-i|s)

s

ai

a-i

(29)

1



max

1- s

|uai,s|

d(s)2dTV(-i (·|s)||-i (·|s))

(30)

ai

s

1



max

1- s

|uai,s|

d(s) 2dTV(j (·|s)||j (·|s))

(31)

ai

s

j=i

1

=

max

1- s

|uai,s|

d (s)

j,s - j,s 1

(32)

ai

s

j=i

1

 1-

|Ai|

d (s)

|Aj | j,s - j,s

(33)

s

j=i

1

 1-

|Ai|

|Aj |

d (s)2

j,s - j,s 2

(34)

j=i

s

s

1 =
1- 1
 1- 1
 1-

|Ai|
j=i
|Ai|
j=i n
|Ai|
j=1

|Aj |

d (s)2 j - j

s

|Aj | j - j

|Aj| j - j ,

where

(28)

to

(29)

is

derived

from

the

fact

that

|Qi (s, a)|



1 1-

.

(30)

to

(31)

relies

on

the

property

of total variation distance:

dTV(-i (·|s)||-i (·|s))  dTV(j (·|s)||j (·|s))
j=i

(32) to (33) is derived from:

max
s

|uai,s|  |Ai|, u  1

ai

j,s - j,s 1  |Aj | j,s - j,s which can be immediately verified by applying Cauchy-Schwarz inequality.

22

Before looking into Part B, we first define P () as the state-action under :

Then we have that:

P ()

= (a |s )P (s |s, a)

(s,a)(s ,a )

P ()

 =0

= uai,s -i (a-i|s )P (s |s, a)

(s,a)(s ,a )

For an arbitrary vector x:

P ()

x  =0

=

uai,s -i (a-i|s )P (s |s, a)xs ,a

(s,a) s ,a

 x  |uai,s |-i (a-i|s )P (s |s, a)
s ,a

= x  P (s |s, a) |uai,s | -i (a-i|s )

s

ai

a-i

 x  P (s |s, a) |Ai| -i (a-i|s )

s

a-i

 |Ai| x 

Thus:

P ()

x



 =0

(s,a) 

|Ai| x 

Similarly we can define P () as the state-action under , and can easily check that

Define:

P ()

x



 =0

(s,a) 

|Ai| x 

-1

-1

M () := I - P () , M () := I - P () .

Because:

-1



M () = I - P () = nP (),

n=0

which

implies

that

every

entry

of

M ()

is

nonnegative

and

M ()1

=

1 1-

1,

this

implies:

1 M ()x   1 -  x ,

and similarly

1 M () x   1 -  x .

Now we are ready to bound Part B. Because:

Qi (s, a) = e(s,a)M ()ri

=

Qi (s, a) 

=

M () e(s,a)  ri

=

P () e(s,a)M ()  M ()ri

=

Qi (s, a) 



P () M ()  M ()ri





 (1 - )2 |Ai|

23

Thus,

Part B =

s,a

d

(s)(

(a|s)

-



(a|s))

Qi (s, 

a)

=0



d(s) |(a|s) - (a|s)|
s,a

Qi (s, a) 

=0

  (1 - )2 |Ai| d(s)2dTV( (·|s)||(·|s))
s



 (1 - )2 |Ai| d(s) 2dTV(j (·|s)||j (·|s))

s

j



= (1 - )2 |Ai| d(s)

j,s - j,s 1

s

j

  (1 - )2

n

|Ai| d(s)

s

j=1

|Aj | j,s - j,s



n

 (1 - )2 |Ai|

|Aj |

j=1

d (s)2
s

j,s - j,s 2
s

  (1 - )2

n
|Ai|
j=1

|Aj | j - j

Now let's look at Part C:

d(s) = (1 - ) (s ) (a |s )e(s ,a )M () e(s,a )

s

a

a





=

d(s)

=

(1

-

 ) 

(s )





(a 

|s

)

e(s

,a

)

M

()

e(s,a )

s

a

a



v1





M ()



+ (s ) (a |s )e(s ,a ) 

e(s,a

 )

s

a

a





v2

P ()

= (1 - ) v1 M () + v2 M ()

M () 

e(s,a )
a

Note that v1, v2 are constant vectors that are independent of the choice of s. Additionally:

v1 1 = =

(s)



(a|s) 

e(s,a)

s

a

1

(s)

(a|s)



s

a

= (s) |uai,s| -i (a-i|s)

s

a

 (s) |uai,s|  |Ai|

s

a

v2 1 =

(s) (a|s)e(s,a) 1

s

a

24

Thus:

= (s) (a|s) = 1

s

a

Part C =

d(s) 

((a|s) - (a|s))Qi (s, a)
=0

s,a

P ()

= (1 - )

v1 M (0) + v2 M (0)



M (0)
=0

e(s,a )((a|s) - (a|s))Qi (s, a)

s,a a

v3

1



 (1 - ) 1 -  v1 1 v3  + (1 - )2 |Ai| v2 1 v3 



|Ai| 1-

v3



Additionally:

[v3](s0,a0) =

( (a|s0) - (a|s0))Qi (s0, a)

a

1

 1-

| (a|s0) - (a|s0)|

a

1 = 1 -  2dTV( (·|s0)||(·|s0))

1n

 1-

2dTV(j (·|s0)||j (·|s0))

j=1

1n

= 1-

j,s - j,s 1

j=1

1n 
1-
j=1

|Aj | j,s - j,s

1n 
1-
j=1

|Aj | j - j

Combining the above inequalities we get:

Part C 

|Ai| 1-

v3





(1

|Ai| - )2

n

|Aj | j - j

j=1

Sum up Part A-C we get:

Ji(i + ui, -i) - Ji(i + ui, -i)

1



(Part A + Part B + Part C)



=0 1 - 

2

n

 (1 - )3 |Ai|

|Aj| j - j ,

j=1

which completes the proof.

H Auxiliary
We recall Lemma 6.

25

Lemma 6. Let X denote the probability simplex of dimension n. Suppose   X , g  Rn and that there exists i  {1, 2, . . . , n} and  > 0 such that:

i  i, i = i gi  gi + , i = i.

Let then:

 = P rojX ( + g),



i

 min{1, i

+

} 2

Proof. Let y =  + g, without loss of generality, assume that i = 1 and that:
y1 > y2  y3  · · ·  yn.
Using KKT condition, one can derive an efficient algorithm for solving P rojX (y) [Wang and Carreira-Perpiñán, 2013], which consists of the following steps:

1.

Find  := max{1  j

 n : yj

+

1 j

1-

2. Set



:=

1 

(1

-

 i=1

yi);

3. Set i = max{yi + , 0}.

j i=1

yi

> 0};

From the algorithm, we have that:

1



= 1- 

yi

i=1

1



= 1- 

i

i=1

1

- 

gi.

i=1

1



= 1- 

(i + gi)

i=1

1

- 

gi

i=1

If   2,

1 1 = max{y1 + , 0}  y1 +   1 + g1 -  gi
i=1

1

1

-1



 1 + (1 -  )g1 - 

(g1 - ) = 1 +



  1 +

. 2

i=2

If  = 1, Thus: which completes the proof.

1 = y1 +  = y1 + (1 - y1) = 1.



1

 min{1, 1 +

}, 2

I Numerical Simulation Details

Verification of the fully mixed NE in Game 1 We now verify that joining network 1 with proba-

bility

1-3 3(1-2

) ,i.e.:

1-3

i (ai

=

1|s)

=

3(1

-

2

, )

s  S,

i = 1, 2,

26

is indeed a NE. Firstly, observe that

Pr(si,t+1 = 1) = =

1-3 3(1 - 2 )
1-3 3(1 - 2 )

1-3

P (si,t+1 = 1|ai,t = 1) +

1- 3(1 - 2 )

1-3

1

(1 - ) + 1 -

=.

3(1 - 2 )

3

P (si,t+1 = 1|ai,t = 2)

Thus,

V (s) = r(s) +



Est tr(st)

=

r(s)

+

2 3(1 -

, )

t=1

Qi (s, ai) = r(s) + 

P (s |ai, a-i)-i (a-i|s)V (s )

s ,a-i

= r(s) + 

(P (si|ai)Pr(s-i

=

1)r(si,

s-i

=

1)

+

P (si|ai)Pr(s-i

=

2)r(si,

s-i

=

2))

+

22 3(1 - )

si {1,2}

1

2

= r(s) + P (si = 1|ai) 3 r(si = 1, s-i = 1) + 3 r(si = 1, s-i = 2)

1

2

22

+ P (si = 2|ai)

3 (si = 2, s-i = 1) + 3 r(si = 2, s-i = 2)

+ 3(1 - )

2

22

2

= r(s) +  +

= r(s) +

= V (s),

3 3(1 - )

3(1 - )

which implies that: (i - i) i Ji() = 0, i  Xi, i = 1, 2,
i.e.  satisfies first order stationarity. Thus according to Theorem 1,  is a NE.

Computation strict NEs in Game 1 The computation of strict NEs is done numerically, using criterion in Lemma 3. We enumerate over all 28 possible deterministic policies and check whether conditions in Lemma 3 hold. For = 0.1,  = 0.95, and initial distribution set as:
(s1 = i, s2 = j) = 1/4, i, j  {1, 2},
there are 13 different strict NEs according to the numerical calculation.

27

