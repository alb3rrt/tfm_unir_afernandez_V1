Simultaneous Multi-View Object Recognition and Grasping in Open-Ended Domains

arXiv:2106.01866v1 [cs.RO] 3 Jun 2021

Hamidreza Kasaei, Sha Luo, Remo Sasso Department of Artificial Intelligence
University of Groningen, The Netherlands

Mohammadreza Kasaei IEETA / DETI
University of Aveiro, Portugal

Abstract
A robot working in human-centric environments needs to know which kind of objects exist in the scene, where they are, and how to grasp and manipulate various objects in different situations to help humans in everyday tasks. Therefore, object recognition and grasping are two key functionalities for such robots. Most stateof-the-art tackles object recognition and grasping as two separate problems while both use visual input. Furthermore, the knowledge of the robot is fixed after the training phase. In such cases, if the robot faces new object categories, it must retrain from scratch to incorporate new information without catastrophic interference. To address this problem, we propose a deep learning architecture with augmented memory capacities to handle open-ended object recognition and grasping simultaneously. In particular, our approach takes multi-views of an object as input and jointly estimates pixel-wise grasp configuration as well as a deep scaleand rotation-invariant representation as outputs. The obtained representation is then used for open-ended object recognition through a meta-active learning technique. We demonstrate the ability of our approach to grasp never-seen-before objects and to rapidly learn new object categories using very few examples on-site in both simulation and real-world settings.
1 Introduction
Over the past decade, deep learning approaches have successively advanced the state-of-the-art on a large variety of problems, such as object perception [1], scene segmentation [2], and manipulation [3]. A common criticism of nowadays' deep learning approaches is that they just learn to recognize classes that were present during the training process. In particular, they are trained once all data has been gathered, making them strongly dependent on the quality and quantity of training data. Therefore, the knowledge of such models is fixed after the training phase, and any changes in the environment require complicated, time-consuming, and expensive re-training by expert users. Another common criticism is that deep learning approaches lack the ability to be useful for tasks other than what they have specifically been trained for (i.e., mainly due to catastrophic forgetting [4]). Furthermore, the benefits of such approaches abate when working with small datasets.
A robot operating in real-world environments frequently faces never-seen-before objects. To deal with this situation, the robot needs to be able to learn about new object categories on-site using very few training examples, while maintaining the knowledge of the previous categories. In this paper, we aim to address these problems by making robots capable of learning in an open-ended fashion through interaction with non-expert users (i.e., the robot has the ability to ask a user to label some of the training instances on-site). This way, a robot can incrementally update its knowledge rather than retrain from scratch when a new instance is introduced or a new category is taught. We propose to study this problem at the crossroad of deep learning and meta-active learning. In particular, we
Preprint. Under review.
Corresponding author: Hamidreza Kasaei, hamidreza.kasaei@rug.nl

setup virtual cameras

a set of sample views

View 1

encoder

decoder

object view

object grasping pipeline

depth image

grasp quality

object view selection view pooling

View 2 View 3 View n

eennceocnodcdeoerdrer

point / ask category of an object

feature vector feature vector

object recognition

perceptual memory

models

recognition result

previous models

updated models

teach correct

conceptualization

oracle

object recognition pipeline

Figure 1: Overview of the proposed approach: A mixed autoencoder is designed for simultaneous multi-view object grasping and recognition tasks. First, multiple views of a given object are generated and then fed to the network to obtain (i) a pixel-wise grasp configuration, and (ii) a compact object representation. All views contributed equally to open-ended object recognition, while the view with maximum entropy is used for grasping.

develop an external-memory equipped deep learning approach capable of producing a pixel-wise grasp configuration and a compact object representation. The obtained representation is scale- and rotation-invariant, informative, and stable, and designed with the objective of supporting accurate 3D object recognition in open-ended domains. More specifically, our approach combines the best of two worlds: the ability to slowly learn an object-agnostic grasping and a compact object representation function, via gradient descent, and the ability to rapidly learn about new categories using very few examples, via meta-active learning. Figure 1 shows an overview of the proposed approach.
In summary, the key contributions of this paper are: (i) a new end-to-end approach to learn a 6DoF object-agnostic grasping function and a compact object representation simultaneously; (ii) we develop a probabilistic open-ended learning approach to handle 3D object recognition in open-ended domains; (iii) to the best of our knowledge, there is no other framework jointly tackling the 3D object category and object grasping in an open-ended manner; (iv) we perform extensive sets of experiments in both simulation and real-robot settings to evaluate the effect of parameters and training data.

2 Related work
Although an in-depth review is beyond the scope of this work, we discuss a few recent efforts in three main categories: object grasping, object recognition, and active learning.
Object grasping ­ In recent studies on object recognition and grasping, much attention has been given to Convolutional Neural Networks (CNN) [5][6][7][8][9]. In particular, CNNs have been applied successfully for empirical object grasping methods. In such approaches, the grasps are classified and ranked using a CNN, after which a robot executes the highest-ranked grasp such as in [6]. One of the biggest bottlenecks with recent deep learning-based object grasping approaches is the execution time. Most of the deep-learning-based approaches take a very long time to sample and rank grasp candidates individually [5][6]. These approaches mainly use in open-loop control scenarios and are not suitable for closed-loop scenarios. To tackle this problem, Morrison et al., [7] proposed Generative Grasping CNN (GG-CNN), a solution where grasp poses are generated for every pixel using a small CNN. Similar to our approach, GG-CNN is designed to be used for real-time closed-loop control using visual feedback. Unlike GG-CNN, our approach works in an eye-to-hand system, where the robot considers an entire scene and not just a narrow top-down view. Our approach generates a grasp map per object while GG-CNN and DexNet generate a grasp map per scene. Unlike our approach, GG-CNN and DexNet select the object to grasp based on the grasp quality map and not based on the actual object label. Recent researches on multi-view grasping [10, 11], have been done using a camera moving over a predefined trajectory, where each point of the trajectory is considered as a view. In [10], the most informative view is then chosen by means of entropy measures. In our work, we instead rendered multiple views of the object using a set of virtual cameras, and the best view of the object is then selected based on a viewpoint entropy measure. All the reviewed approaches only work in top-down camera settings and have mainly focused on solving 4DoF (x, y, z, ) grasping, where the gripper is forced to approach objects from above. A major drawback of these approaches is inevitably restricted ways to interact with objects. For instance, they are not able to grasp a horizontally placed plate. We tackle these problems by proposing a multi-view approach to handle 6DoF object grasping and recognition simultaneously.

2

Object recognition ­ Nowadays visual recognition systems are often designed based on CNN, where the number of classes is known in advance as prior information [12]. Although these approaches work well in static closed set environments, they easily fail when facing an out-of-distribution instance (e.g., fooling image) by predicting a "known" label with high confidence [13, 14]. Some researchers tried to handle this limitation by incorporating an "unknown" class [15]. Although these approaches can detect "unknown" objects to some extent, they cannot learn about new categories due to catastrophic forgetting (learning about new object categories leads to forget previously learned categories) [4, 16]. In general, deep learning approaches for 3D object recognition can be categorized into three different categories depending on their input. First, there are volume-based approaches [17, 18], where the object is represented as a 3D voxel grid and then fed to a CNN with 3D filter banks. Second, there are pointset-based approaches [8], which work directly on the 3D point clouds. The final category is view-based approaches, which are used in this research. These approaches appear to be most effective in 3D object recognition, as shown by [9], [19][20]. In such approaches, 2D images are extracted from the 3D representation by projecting the object's points onto 2D planes [21, 20]. H. Su et al., [21] developed a system that learns to recognize 3D shapes from a collection of their rendered views on 2D images, for which multiple view-wise CNN features were used. Another approach, by [9], takes multi-view images of an object as input and jointly estimates its pose and object category label using a CNN. Our research relates to these works as both use multi-view representations of 3D objects to learn deep features. However, we trained a mixed autoencoder to generate a grasp map as well as a compact deep representation for a given object. The learned deep features are used for open-ended object category learning and recognition. Unlike these approaches, the set of object categories to be learned is not completely known in advance in our approach, and the model does not know which additional objects it will have to learn, which observations will be available, and when they will be available to support the learning.
Active learning ­ In recent years, Active Learning (AL) methods have been gaining much attention to overcome the aforementioned limitations [22­25], but few AL methods target the problem of open-ended learning [26, 27]. In particular, most AL approaches, first sample a subset of training examples from a pool of unlabeled data using an acquisition function based on either 'uncertainty' measures (entropy, variance, and etc.) or density/geometric similarly measures in feature space (i.e., sampling diverse instances by considering the similarities among training data). An oracle is then asked to label the selected samples. Finally, the model is incrementally trained or re-trained from scratch to incorporate new information without catastrophic interference. These approaches are incremental by nature but not open-ended since the number of categories is pre-defined and the main objective is to update the model of known categories by finding minimally required training examples to reach a certain classification accuracy. Moreover, unlike these approaches, we formulate the AL to learn from online robot's observation and not from a set of training data. More specifically, instead of selecting a set of instances that represents the entire training dataset, we want to select a set of training samples that best represents the novel classes. We also update the model of known categories only when it is necessary. This way, we mainly use our limited labeling budget to learn about new object categories and update the model of known classes when necessary.
3 Object Representation and Grasp Learning
We formulate object representation and grasp synthesis as a learning problem. In particular, we intend to learn a function that receives a collection of rendered images of a 3D object as input, and returns (i) a compact, scale- and rotation-invariant representation, (ii) the best direction for approaching the target object, and (iii) a grasp map representing per-pixel grasp configuration for a selected view.
3.1 Generating multi views of a 3D object
Three-dimensional objects are typically stored as point clouds. Each point cloud is represented as a set of points, pi : i  {1, . . . , n}, where each point is described by their 3D coordinates [x, y, z]. For rendering 2D images from a 3D object, we set virtual cameras around the target object, where the Z axes of cameras are pointing towards the centroid of the object. Towards this goal, we first compute the geometric center of the object, which is defined as the arithmetic mean position of all the points of the object. Afterwards, we construct a local reference frame for the object by performing eigenvalue decomposition analysis on the normalized covariance matrix, , of the object, i.e., V = EV, where E = [e1, e2, e3] contains the descending sorted eigenvalues, and V = [v1, v2, v3] shows the
3

eigenvectors. In this case, the first two largest eigenvectors, v1 and v2, point to the direction of the two largest variances of the object's points. We consider these vectors as X and Y axes respectively, and define the Z axis as the cross product of v1 × v2. The object is then transformed to be placed in the reference frame.

From each camera pose, we map the point cloud of

the object into a depth image using the z-buffering and

orthogonal projection methods [28]. In particular, we

first project the object to a square plane, M , centered on

the camera's center. The projection area is then divided

into k × k square bins, where each bin is considered

as a pixel. Finally, the minimum z of all points falling Figure 2: Visualization of depth image renderinto a bin, b, is used as the pixel value. The size of the ing for both object recognition and grasp pose

projection square area, l × l, is an important factor for detection tasks: (left) a cordless drill; (right)

both object representation and object grasping tasks. In the point cloud of the object, its local reference

the case of object representation, we define the size of projection relative to the size of the object for producing a scale-invariant object representation. In particular, the size of the projection plane is defined as lp × lp dimension, where lp is the largest side of a tight-fitting axis-aligned bounding box of the object. We compute

frame, bounding box, and three projected views of the drill on XoY, XoZ, and YoZ planes. In each projection, the darker area shows the image size for the object representation task, and the lighter area represents the size of the image for the object grasping task.

the dimensions of the bounding box by computing the minimum and maximum coordinate values

along each axis. Since the grasp configurations depend on the pose and size of the target object, a

view of the object should not be scale-invariant. Therefore, we consider a fixed size projection plane

(lg × lg). The size of each side of the plane is defined as lg = k × g, where k is the number of pixels, and g represents the size of the head of the robot's finger. An illustrative example of this procedure

for a cordless drill object is provided in Fig. 2.

3.2 Viewpoint setups

The number of object views is an important parame-

ter for both object grasping and object recognition. In

the case of object grasping, we consider three ortho-

graphic views including XoY, XoZ, and YoZ projections

as shown in Fig. 2 (right). The intuition behind this selection is that most of the household objects are stably orthographic orbit elevated by 

sphere

graspable from top, side, or front views [29].

Figure 3: Illustration of three viewpoint se-

In order to define which of the three orthographic views is the best-view for approaching and grasping the target object, we use viewpoint entropy that nicely takes into account both the number of occupied pixels and their

tups used in this study. In all cases, distances between cameras and the center of the target object are constant and elevation levels are highlighted by different colors.

values. We calculate the entropy of a projection view, v, by H(v) = -

k2 k=1

pk

log2 (pk ),

where

pk is the normalized value of pixel k. In the case of object representation, viewpoint setup can

be any arbitrary choice, {vi}Vi=1. As shown in Fig.3, we consider three setups in this work: (left)

orthographic projections, i.e., {vi}3i=1, (center) an orbit elevated by  (similar to MVCNN [21]), and

(right) a sphere viewpoints setup, which is similar to the previous setup but with multiple elevation

levels. The setup of orthographic projection has been explained in the previous subsection. For

the second viewpoint setup, we place virtual cameras around the Z axis at intervals of , elevated

by

a

fixed

.

Therefore,

the

number

of

views

for

a

given

object

is

set

to

{vi

}Vi==1

360 

.

In

the

case

of sphere viewpoint setup, instead of having a fix elevation, we placed virtual cameras at multiple

elevation levels, , with the interval of [-90, 90]. Therefore, we capture V

=

360 

×

180 

views for

a given object. We have optimized ,  and  parameters to obtain a good balance between object

recognition accuracy and computation time (see section 5-A). It should be noted that we are treating

all the views as equally important for object recognition task.

3.3 Network architecture

We intend to learn a function that maps each input object to multiple outputs, including a compact deep representation, and a set of images representing pixel-wise antipodal grasp configurations,

4

f : X  Y. Towards this goal, we have designed a Generative Mixed Autoencoder that get a depth image with height H and width W as input, xi  RH×W , and returns a pixel-wise grasp configuration map, Gi, as well as a reconstructed image x^i as outputs, i.e., yi = [Gi, x^i] (multi-task learning). It should be noted that the reconstructed image is used to force the network to learn a compact deep representation in an unsupervised manner, which will be used for object recognition purposes (i.e., meta learning, as we learn about new categories using the output of other learning method). The network is parameterized by its weights . We add a constraint on number of trainable parameters of the network to be less than 150K as we ultimately plan to use the network in real-time robotics applications.

Our model is constructed using six types of layers, including convolution, deconvolution, fully connected, flatten, reshape, and dropout. The encoder part is composed of three convolutional layers (C1 to C3), a flatten layer (F) followed by a dropout (D1), and a fully connected (FC1) layer. We use a Rectified Linear Unit (ReLU) as the activation function of all layers of the encoder part to force the negative values to zero and eliminating the vanishing gradient problem which is observed in other types of activation functions. The output of the FC1 layer is considered as the deep representation of the given object (Fig. 1).

The decoder part is composed of a fully connected (FC2) layer, and a reshape layer (R), followed by three deconvolutional layers (T1 to T3). Similar to the encoder part, we use the ReLU activation function for all layers except for the FC2 layer that a sigmoid activation function is used. The underlying reason for selecting the sigmoid function is not blowing up activation, unlike the ReLU function. We use the same padding in all convolution and deconvolution layers to make the input and output be of the same size (see Section 5). Since we aim to directly approximate a compact representation and an antipodal grasp map from an input depth image, the output layer of the network, in addition to the reconstructed image (x^i), should generate a grasp map (Gi) in the form of three images. Given a grasp object dataset, D, containing n images, D = {(xi, yi)|1  i  n}, we can train our model end-to-end to learn the mapping function f(.).

In this work, an antipodal grasp point is represented as a tuple, gi = (u, v), i, wi, qi , where (u, v)

shows the center of grasp in image coordinates, i indicates the rotation of the gripper around the

depth

axis

as

a

value

in

the

range

of

[

- 2

,

 2

],

wi

represents

the

necessary

width

of

the

gripper

as

a value in the range of [0, wmax], and lastly the scalar quality qi represents the success probability

of the grasp, i.e., qi  [0, 1]. Since we predict per-pixel grasp configuration, a grasp map Gi, for

a given image, Ii, is represented by three images (, W, Q)  RH×W , in which pixel values of

each image represent measure of i, wi, qi respectively. Therefore, from f(Ii) = Gi, the best grasp

configuration, g*, is the one with maximum quality, and its coordinate shows the center of grasp, i.e.,

(u, v)  g* = argmaxQ Gi. Furthermore, we consider the feasibility of execution of the selected grasp given the kinematic chain of the robot, and also the distance that the robot needs to travel in the

configuration space.

After obtaining the grasp map of an orthographic view,

the Cartesian position of the selected grasp point, (u, v),

can be transformed from 2D view coordinates to a 3D

object's reference frame since the transformation of the

orthographic view relative to the object is known. The

depth value of the grasp point is defined based on the minimum depth value of the (u, v) point and its surrounding neighbors that are within a radius of , where

Figure 4: Examples of grasping objects in different situations.

 is defined based on the size of the robot's finger (g). In our setup, we set  to 5cm. The gripper

approaches the object in an orthogonal direction to the projection. It should be noted that in the case

of large object or pile of objects, there is a clear advantage (e.g., collision-free) to grasp from above

(Fig. 4 ­ left), while for cluttered isolated objects, it completely depends on the pose of the object

relative to the camera (Fig. 4 ­ right). As an example, when the coke can topples, the top view is

processed to specify graspable points while when the same can is standing, the front view is selected.

4 Open-ended Object Category Recognition
Most active learning methods do not perform well in open-ended domains since they need to know the number of categories in advance. In open-ended learning scenarios, the number of classes is

5

updated over time, based on the robot's observations, experiences, and interactions with human users. In other words, instead of sampling and labeling the training data in advance, we propose to iteratively and adaptively choose which training instance should be labeled next. In this study, we follow an active learning scenario by identifying the need for teaching a new category or by letting the user provides corrective feedback to learn the model as quickly as possible. In particular, we provide three basic actions for the user to either teach the robot about new categories or correct the robot on errors by providing feedback. These actions consist of the following: (i) Ask: to check the prediction accuracy of an object category model, (ii) Teach: to introduce a new object category using a set labeled samples, and (iii) Correct: to improve an object category model using a new instance. The teach and correct actions lead the robot to initialize a new class or to modify a known class incrementally using a particular instance the current classifier is the least certain about.
In particular, we are interested in learning a probabilistic model for each object category, C, using very few labelled data, Lt = {x1, . . . , xn}, where n is the number of seen instances until time t, and xi = [x1, . . . , xd] is a d-dimensional feature vector (i.e., output of the encoder). Note that Bayesian approaches are computational efficient since the parameter of the model can be updated upon a new data point is added. Moreover, they are memory efficient as new training instances are used to update category models and then forgotten immediately. Mathematically, an object category, Ck, is represented as a tuple Ck = Nk, ak, P(Ck), [P(x1|Ck), . . . , P(xd|Ck)] , where ak is a vector of accumulator for category k. In particular, aki is the accumulation of ith element of all instances of category Ck and |a| = |x|. P(Ck) is the prior probability of category Ck and P(xi|Ck) shows the probability distribution of features in category k. We consider the probability of each element of feature vector independently, regardless of any possible correlations with the other elements. This way, the P(Ck) P(x|Ck) is equivalent to the joint probability model. It should be noted that upon each teach/correct action, the prior probabilities of all categories as well as the probabilities of xi in the category k, P(xi|Ck), are updated incrementally. We also choose a probabilistic classifier to map the representation of a given object, x, to a label, ft(x) = y^i, through the maximum likelihood, argmaxk P(Ck|x).
5 Experimental Results
Given COVID-19 crisis, we had very limited access to the lab, hence, we developed a simulation environment in Gazebo to extensivly evaluate our approach. As shown in Fig. 5, the robot and camera in simulated environment are placed according to the real-robot setup to obtain consistent performance. Our setup consists of a Kinect camera, a Universal Robot (UR5e) with a two fingered gripper (Robotiq 2F-140), and a user interface. We used the same code and network (trained on the Cornell dataset) in both Figure 5: Experimental setups to evaluate the system real and simulation experiments. We integrated in: (top) simulation, and (bottom) real-robot settings. our work into the cognitive robotic system presented in [30], and all tests were performed with a PC running Ubuntu 18.04 with a 3.20 GHz Intel Xeon(R) i7, and a Quadro P5000 NVIDIA.
5.1 Network design analysis
We trained several networks with the proposed architecture but different parameters including filter size, dropout rate, number of units in fully connected layers, loss functions, optimizer, and various learning rates for 50 epochs each. We used the extended version of the Cornell dataset [31] comprising 1035 RGB-D images of 240 household objects. In this work, we considered the 5110 positive grasp configurations and discard all the negative labels. Furthermore, since the Cornell dataset is a small dataset, we augmented the data by zooming, random cropping, and rotating functions to generate 51100 images. We used the augmented dataset to train the network and reported the obtained results based on the Intersection over Union (IoU) metric. A grasp pose is considered as a valid grasp if the intersection of the predicted grasp rectangle and the grand truth rectangle is more than 25%, and the orientation difference between predicted and grand truth grasp rectangles is less than 30 degrees. The final architecture is shaped as: C(9×9×32), C(5×5×16), C(3×3×8), F1, D1(0.5), FC1(850), FC2(5000),
6

R, T(3×3×8), T(5×5×16), T(9×9×32). We used RMSprop optimizer with a learning rate of 0.001, and mean squared error as a loss function.

We compared our approach with three visual grasp detection baselines, including Lenz et al. [5], GG-CNN [7],

Table 1: Result of object grasping on the Cornell dataset [32].

and GG-CNN2 [33], in terms of IoU and inference speed.

approach

input data IoU (%) speed (ms)

Results are summarised in Table 1. By comparing all Lenz et al. [5] RGB-D 73.9 1350

results, it is clear that our approach outperformed the GG-CNN [7] depth image 73.0

19

selected approaches by a large margin. Concerning IoU

GG-CNN2 [33] depth image 75.2 Our approach depth image 83.35

21 26

metric, our approach achieved 83.35% which was 9.45,

10.35, and 8.15 percentage point (p.p) better than Lenz et al. [5], GG-CNN [7], and GG-CNN2 [33]

respectively. In addition to the IoU metric, we also computed the average grasp prediction time for

each of the mentioned approaches. The obtained results indicated that Lenz et al. [5] approach, on

average, needed a very long time to predict grasp poses for a given object. Hence, it is computation-

ally expensive not well-suited for real-time robotic applications, in which a closed-loop controller

is usually needed. In contrast, the inference times for the GG-CNN [7], GG-CNN2 [33], and our

approach was less than 30 ms, demonstrating that these approaches can be used in real-time tasks

(i.e., a closed-loop control can work steadily in  30Hz). The underlying reason of fast inference

is that, these approaches use a relatively small CNN, while Lenz et al. [5] approach is based on

very deep neural network with million of parameters. Although GG-CNN [7] and GG-CNN2 [33]

achieved slightly better execution time than our approach (i.e., 7 ms, and 5 ms faster respectively),

the grasp success rate of our approach is significantly better. The underlying reason is that we forced

the network to learn a better representation in hidden layer by reconstructing the input view as one of

the outputs. Furthermore, our approach has been designed to do simultaneous object grasping and

recognition, while the other approaches only designed for object grasping.

5.2 Evaluations of object recognition We performed two round of experiments to evaluate the performance of the proposed approach.

5.2.1 Offline evaluation

In this round of evaluation, a 10-fold cross-validation protocol (train-then-test) is used to assess the performance of the proposed approach. We used the Restaurant Object Dataset [34], which contains 306 objects' views organizing in 10 categories with significant intraclass variations. Therefore, it is suitable for performing

Table 2: Summary of offline evaluations.

Camera setup #Views Pooling
Instance accuracy Avg. class accuracy Comput. time (s)

Orthographic 3
Avg 0.9511 0.9366 0.0167

Orbit 20 Max
0.9642 0.9588 0.1102

Sphere 28 Max
0.9642 0.9406 0.1540

extensive sets of experiments.

Acc_micro: 96.74% Acc_marco: 95.88%

Our approach has several parameters that must be optimized to provide a good trade-off among recognition performance, memory usage, and computation time. The parameters are including:   {30, 45, 60},   {4, 8, . . . , 20},   {3, 5, 7}, view_pooling  {max, avg, appending}. The best results in terms of instance accuracy, class accuracy, and average computation time were found by running each possible permutation of the available parameters for Orthographic, Orbit, and Sphere setups. The instance accu-

Predicted Category

Bottle

100.0% 20

0.0% 0

0.0% 0

0.0% 0

0.0% 0

0.0% 0

0.0% 0

0.0% 0

0.0% 0

0.0% 0

Bowl

0.0% 0

100.0% 34

0.0% 0

0.0% 0

0.0% 0

0.0% 0

0.0% 0

0.0% 0

0.0% 0

0.0% 0

Flask

0.0% 0

0.0% 100.0% 0.0%

0

22

0

0.0% 0

0.0% 0

0.0% 0

0.0% 0

0.0% 0

0.0% 0

Fork

0.0% 0

0.0% 0

0.0% 81.8% 0.0%

0

9

0

0.0% 0

0.0% 18.2% 0.0%

0

2

0

0.0% 0

Knife

0.0% 0

0.0% 0

0.0% 0

0.0% 100.0% 0.0%

0

28

0

0.0% 0

0.0% 0

0.0% 0

0.0% 0

Mug

0.0% 0

2.0% 1

0.0% 0

0.0% 0

0.0% 98.0% 0.0%

0

50

0

0.0% 0

0.0% 0

0.0% 0

Plate

0.0% 0

0.0% 0

0.0% 0

0.0% 0

0.0% 0

0.0% 100.0% 0.0%

0

23

0

0.0% 0

0.0% 0

Spoon

0.0% 0

0.0% 0

0.0% 0

9.1% 3

3.0% 1

0.0% 0

0.0% 87.9% 0.0%

0

29

0

0.0% 0

Teapot

0.0% 0

0.0% 0

0.0% 0

0.0% 0

0.0% 0

7.1% 2

0.0% 0

0.0% 92.9% 0.0%

0

26

0

Vase

0.0% 0

0.0% 0

0.0% 0

0.0% 0

0.0% 0

1.8% 1

0.0% 0

0.0% 0

0.0% 98.2%

0

56

Bottle Bowl Flask Fork Knife Mug Plate Spoon Teapot Vase

100 90 80 70 60 50 40 30 20 10 0

racy (accmicro

=

#

true predictions # predictions

),

and

average

class

Target Category
Figure 6: Confusing matrix for the best orbit

accuracy

(accmacro

=

1 K

K i=1

acci)

are

used

as

evalu-

setup ( = 60).

ation metrics to compare the performance of different approaches. Note that we report average class

accuracy to address class imbalance, since instance accuracy is sensitive to class imbalance. Refer to

Table 2 for a summary of the best results. The confusion matrix for the best setup is shown in Fig. 6.

It shows most of misclassification happened between objects that are very similar together (e.g., fork

vs. spoon). This issue can be addressed through a fine-grained object categorization [35, 36]. By comparing all experiments, it is visible that Orbit ( = 60) and Sphere ( = 7,  = 4) setups

achieved slightly better instance and class accuracies than Orthographic view setup ( 2%). However,

in the case of average computation time, the Orthographic setup outperformed Orbit and Sphere

7

configurations with a large margin. This result shows that the Orthographic view setup can be used in closed-loop control ( 59Hz feedback) while Orbit ( 9Hz feedback) and Sphere ( 7Hz feedback) are computationally expensive for real-time applications.

5.2.2 Open-ended evaluation

We adopted an open-ended evaluation protocol that follows test-then-train scheme [37][38], to emulate the learning behaviour of a robot over long periods of time. In particular, it would be expected that the robot could be taught new categories that are present in its surroundings. It would be corrected on misclassifications it makes by a human user. Such experiments might take a long time with a human user. Therefore, we developed a simulated user to conduct systematic, consistent, and reproducible experiments. The simulated user can interact with the robot using teach, ask, and correct actions. We connect the simulated user to the largest publicly available 3D object dataset [39] that contains 51 object categories with 250,000 views of 300 objects.

In this round of experiments, the robot will start with no previous knowledge. The user teaches a category using three randomly selected views. After that, the user repeatedly picks unseen object views of the currently known categories and tests the robot to see if it has learned the category. This is done by asking the robot to identify new testing examples of all previously learned categories. When the agent makes a classification mistake, the user will provide feedback with the correct category label. This causes the robot to adjust its category model using the mistaken instance. The user estimates the recognition accuracy of the robot using a sliding window over the last 3n iterations, where n is the number of categories. If the classification accuracy exceeds a threshold,  = 0.75, a new category is introduced. If the robot can not reach the classification threshold after 100 iterations since the last category was taught , the user realized that the robot is not able to learn more categories and terminates the experiment (breakpoint). It is also possible that the robot learns all categories before reaching the breakpoint, and hence, the experiment is halted (reported as "lack of data" condition) [37][38].

Evaluation metrics: Since the order of introducing the categories may matter, we run ten experiments for each approach and evaluate all approaches using four metrics as introduced in [37][38]: (i) an average number of learned categories (ALC), which shows how much the system is capable of learning; (ii) the number of question/correction iterations (#QCI) needed to learn those categories, and the average number of stored instances per category (AIC), shows the amount of time and memory needed for learning; (iii) Global Classification Accuracy (GCA), representing the accuracy of agent computed based all predictions, and the Average Protocol Accuracy (APA), which represents the average accuracy of the agent over all sliding windows of the protocol.

Results: We compared our approach with four

Table 3: Result of open-ended evaluations.

state-of-the-art methods. The obtained results are summarized in Table 3. We also plot the

Approaches BoW [40]

#QCI ALC AIC GCA APA 724.30 18.40 17.24 0.74 0.78

performance of the proposed multi-view approaches in the first open-ended experiment in Fig. 7. By comparing all approaches, it is visible that sphere camera setup outperformed orthographic and orbit configurations by a large margin in all evaluation metrics. The same results achieved when comparing our approach

Open-Ended LDA [41] 572.10 12.50 12.43 0.73 0.79

Local-LDA [27]

872.10 32.30 11.58 0.77 0.81

GOOD [42]

1869.2 34.40 19.70 0.70 0.78

ours-Orthographic( ) 1789.00 44.10 14.35 0.75 0.84

ours-Orbit()

1643.40 45.60 13.62 0.77 0.85

ours-Sphere()

1358.40 51.00 7.75 0.85 0.89

Stopping condition was "lack of data": ( ) in 3 out of 10 experiments, () in 5 out of 10 experiments, () In all experiments.

with the selected state-of-the-art approaches. In particular, the agent with sphere camera setup learned

all existing categories in all experiments (ALC metric), and the stopping condition was due to the

"lack of data". This result shows the potential for learning many more categories. The robot with

orthographic and orbit camera setups achieved acceptable scalability by learning (on average) 44.10

and 45.60 categories, respectively. The other selected approaches, on average, learned less than

35 categories and their performance drops aggressively when the number of categories increases.

It is also clear that the robot with sphere setup, on average, stored fewer instances per category

while learning all categories, i.e., it required less than eight instances per category while the other

approaches, on average, need at least 11.58 instances per category (AIC). It should be noted that

#QCI, GCA, and APA metrics should be seen in light of the number of learned categories. For

instance, Open-Ended LDA achieved the best #QCI, which is expected since it learned much fewer

categories than our approaches. Hence, it can be concluded that our approach with sphere camera

setup showed a promising performance and outperformed the other approaches by a large margin

(more than 6% in GCA and 2% in APA).

8

Global Classification Accuracy

1

sphere orbit orthographic

0.9

0.8

0.7

0.6

0.5 0

10

20

30

40

50

60

Number of Learned Categories

Figure 7: System performance during the first simulated user experiment: (top) This graph shows the number of instances used to train the object categories' model in orthographic, orbit, and sphere setups; (bottom-left) Global classification accuracy as a function of number of learned category; (bottom-right) Number of learned categories as a function of ask/correct iterations.

5.3 Grasp evaluation

In this round of evaluation, we designed a pick and place scenario in the context of a clear_table task.

In these experiments, the robot needs to recognize and track the pose of the basket as the placing pose,

as well as the label and pose of another object to be cleaned from the table. Afterward, the robot infers

a graspable pose of the target object, picks it up, and puts it in the basket. We performed this scenario

to see if the object slips due to bad grasp or not and also to check the recognition performance. We

assess

the

performance

of

our

approach

by

measuring

success

rate,

i.e.,

#success #attempts

,

where

a

particular

grasp is considered a success if the object is inside the basket at the end of the experiment. Note, the

robot should detect and recognize all objects to accomplish this task successfully. At the beginning

of each experiment, we set the robot to a pre-defined setting, and randomly place objects on the table.

We used a set of 20 simulated objects, imported from the YCB dataset [43] and Gazebo repository, and 20 real daily-life objects with different shapes, sizes, and weight (see Fig. 8). Each simulated object was tested in isolation 50 times, while each real-object was tested 10 times. Note that, to speed up the real-robot experiments, we randomly placed five objects on the table and instruct the robot to clean them one by one. Therefore, the robot should first recognize all objects precisely, and then move them into the basket. In the case of simulation experiments, we achieved a grasp success rate of 88.3% (i.e., 883 success out of 1000 trials), and for real objects, the success rate was Figure 8: Objects used 86.5% (173 success out of 200 attempts). We observed that failed attempts in the experiments. were mainly due to inaccurate object's bounding box, misclassification of the target and/or basket objects, some objects in specific pose had a very low grasp quality score due to partially inaccurate depth map estimation, collisions between the gripper and the table mainly in topdown grasps, and collision between the object and the basket (mainly happened for the Pringles and JuiceBox). A video of these experiments is available online at: https://youtu.be/KvBbrgc-eAA

6 Conclusion
In this paper, we presented a multi-view deep learning approach to handle object recognition and grasping simultaneously. Furthermore, the proposed approach allows robots to incrementally learn about new object categories and adapt to new environments by accumulating and conceptualizing experiments through interaction with non-expert human-users. To validate the performance of our approach, we performed extensive sets of experiments using benchmark object datasets. Experimental results showed that the overall object recognition and grasping performance of the proposed approach is significantly better than the best results obtained with the state-of-the-art approaches. In the continuation of this work, we will investigate the possibility of learning a function to automatically select the best set of camera poses for a given object based on shape complexity and coverage criteria. As another interesting future direction, we would like to investigate the possibility of task-informed grasping (e.g., grasp the handle of a knife instead of its blade).
9

References
[1] J. Wang, R. Chakraborty, and X. Y. Stella, "Spatial transformer for 3d point clouds," IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.
[2] C. Yu, J. Wang, C. Gao, G. Yu, C. Shen, and N. Sang, "Context prior for scene segmentation," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.
[3] H.-S. Fang, C. Wang, M. Gou, and C. Lu, "Graspnet-1billion: a large-scale benchmark for general object grasping," in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 11444­11453, 2020.
[4] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, et al., "Overcoming catastrophic forgetting in neural networks," Proceedings of the national academy of sciences, vol. 114, no. 13, pp. 3521­3526, 2017.
[5] I. Lenz, H. Lee, and A. Saxena, "Deep learning for detecting robotic grasps," The International Journal of Robotics Research, vol. 34, no. 4-5, pp. 705­724, 2015.
[6] J. Mahler, J. Liang, S. Niyaz, M. Laskey, R. Doan, X. Liu, J. A. Ojea, and K. Goldberg, "Dex-net 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp metrics," arXiv preprint arXiv:1703.09312, 2017.
[7] D. Morrison, P. Corke, and J. Leitner, "Closing the Loop for Robotic Grasping: A Real-time, Generative Grasp Synthesis Approach," in Proc. of Robotics: Science and Systems (RSS), 2018.
[8] R. Klokov and V. Lempitsky, "Escape from cells: Deep kd-networks for the recognition of 3D point cloud models," in Proceedings of the IEEE International Conference on Computer Vision, pp. 863­872, 2017.
[9] A. Kanezaki, Y. Matsushita, and Y. Nishida, "RotationNet: Joint object categorization and pose estimation using multiviews from unsupervised viewpoints," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5010­5019, 2018.
[10] D. Morrison, P. Corke, and J. Leitner, "Multi-view picking: Next-best-view reaching for improved grasping in clutter," in 2019 International Conference on Robotics and Automation (ICRA), pp. 8762­8768, IEEE, 2019.
[11] A. ten Pas, M. Gualtieri, K. Saenko, and R. Platt, "Grasp pose detection in point clouds," The International Journal of Robotics Research, vol. 36, no. 13-14, pp. 1455­1473, 2017.
[12] A. Bochkovskiy, C.-Y. Wang, and H.-Y. M. Liao, "Yolov4: Optimal speed and accuracy of object detection," arXiv preprint arXiv:2004.10934, 2020.
[13] A. Bendale and T. E. Boult, "Towards open set deep networks," in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1563­1572, 2016.
[14] A. Subramanya, V. Pillai, and H. Pirsiavash, "Fooling network interpretation in image classification," in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2020­2029, 2019.
[15] Q. Da, Y. Yu, and Z.-H. Zhou, "Learning with augmented class by exploiting unlabeled data," in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 28, 2014.
[16] W. J. Scheirer, L. P. Jain, and T. E. Boult, "Probability models for open set recognition," IEEE transactions on pattern analysis and machine intelligence, vol. 36, no. 11, pp. 2317­2324, 2014.
[17] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao, "3D shapenets: A deep representation for volumetric shapes," in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1912­1920, 2015.
[18] D. Maturana and S. Scherer, "VoxNet: A 3D convolutional neural network for real-time object recognition," in 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 922­928, IEEE, 2015.
[19] C. R. Qi, H. Su, M. Nießner, A. Dai, M. Yan, and L. J. Guibas, "Volumetric and multi-view CNNs for object classification on 3D data," in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5648­5656, 2016.
[20] B. Shi, S. Bai, Z. Zhou, and X. Bai, "Deeppano: Deep panoramic representation for 3-d shape recognition," IEEE Signal Processing Letters, vol. 22, no. 12, pp. 2339­2343, 2015.
10

[21] H. Su, S. Maji, E. Kalogerakis, and E. Learned-Miller, "Multi-view convolutional neural networks for 3D shape recognition," in Proceedings of the IEEE international conference on computer vision, pp. 945­953, 2015.
[22] O. Sener and S. Savarese, "Active learning for convolutional neural networks: A core-set approach," arXiv preprint arXiv:1708.00489, 2017.
[23] U. Aggarwal, A. Popescu, and C. Hudelot, "Active learning for imbalanced datasets," in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), March 2020.
[24] Y. Siddiqui, J. Valentin, and M. Niessner, "Viewal: Active learning with viewpoint entropy for semantic segmentation," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.
[25] Y. Gal, R. Islam, and Z. Ghahramani, "Deep bayesian active learning with image data," in International Conference on Machine Learning, pp. 1183­1192, PMLR, 2017.
[26] S. H. o. Kasaei, "OrthographicNet: A deep transfer learning approach for 3D object recognition in open-ended domains," IEEE/ASME Transactions on Mechatronics, pp. 1­1, 2020.
[27] S. H. Kasaei, A. M. Tomé, and L. S. Lopes, "Hierarchical object representation for open-ended object category learning and recognition," in Advances in Neural Information Processing Systems, pp. 1948­1956, 2016.
[28] S. Liu, T. Li, W. Chen, and H. Li, "Soft rasterizer: A differentiable renderer for image-based 3D reasoning," in Proceedings of the IEEE International Conference on Computer Vision, pp. 7708­7717, 2019.
[29] S. H. Kasaei, N. Shafii, L. Seabra Lopes, and A. M. Tomé, "Object learning and grasping capabilities for robotic home assistants," in RoboCup 2016: Robot World Cup XX, pp. 279­293, Springer International Publishing, 2017.
[30] M. Oliveira, L. S. Lopes, G. H. Lim, S. H. Kasaei, A. M. Tomé, and A. Chauhan, "3D object perception and perceptual learning in the race project," Robotics and Autonomous Systems, vol. 75, pp. 614­626, 2016.
[31] Y. Jiang, S. Moseson, and A. Saxena, "Efficient grasping from RGBD images: Learning using a new rectangle representation," in IEEE International conference on robotics and automation, pp. 3304­3311, IEEE, 2011.
[32] Y. Jiang, S. Moseson, and A. Saxena, "Efficient grasping from RGBD images: Learning using a new rectangle representation," in 2011 IEEE International conference on robotics and automation, pp. 3304­ 3311, IEEE, 2011.
[33] D. Morrison, P. Corke, and J. Leitner, "Learning robust, real-time, reactive robotic grasping," The International Journal of Robotics Research, vol. 39, no. 2-3, pp. 183­201, 2020.
[34] S. H. Kasaei, M. Oliveira, G. H. Lim, L. Seabra Lopes, and A. M. Tome, "Interactive open-ended learning for 3D object recognition: An approach and experiments," Journal of Intelligent & Robotic Systems, vol. 80, no. 3-4, pp. 537­553, 2015.
[35] N. Keunecke and S. H. Kasaei, "Combining shape features with multiple color spaces in open-ended 3d object recognition," IEEE-RAS International Conference on Humanoid Robots (Humanoids), 2020.
[36] R. Ji, L. Wen, L. Zhang, D. Du, Y. Wu, C. Zhao, X. Liu, and F. Huang, "Attention convolutional binary neural tree for fine-grained visual categorization," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10468­10477, 2020.
[37] A. Chauhan and L. S. Lopes, "Using spoken words to guide open-ended category formation," Cognitive processing, vol. 12, no. 4, p. 341, 2011.
[38] S. H. Kasaei, L. S. Lopes, and A. M. Tomé, "Coping with context change in open-ended object recognition without explicit context information," in 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 1­7, IEEE, 2018.
[39] K. Lai, L. Bo, X. Ren, and D. Fox, "A large-scale hierarchical multi-view RGB-D object dataset," in Robotics and Automation (ICRA), 2011 IEEE International Conference on, pp. 1817­1824, IEEE, 2011.
[40] S. H. Kasaei, M. Oliveira, G. H. Lim, L. S. Lopes, and A. M. Tomé, "Towards lifelong assistive robotics: A tight coupling between object perception and manipulation," Neurocomputing, vol. 291, pp. 151­166, 2018.
11

[41] M. Hoffman, F. R. Bach, and D. M. Blei, "Online learning for latent dirichlet allocation," in advances in neural information processing systems, pp. 856­864, 2010.
[42] S. H. Kasaei, J. Sock, L. S. Lopes, A. M. Tomé, and T.-K. Kim, "Perceiving, learning, and recognizing 3D objects: An approach to cognitive service robots," in Thirty-Second AAAI Conference on Artificial Intelligence, 2018.
[43] B. Calli, A. Singh, J. Bruce, A. Walsman, K. Konolige, S. Srinivasa, P. Abbeel, and A. M. Dollar, "Yalecmu-berkeley dataset for robotic manipulation research," The International Journal of Robotics Research, vol. 36, no. 3, pp. 261­268, 2017.
12

