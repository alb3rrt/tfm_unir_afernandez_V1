
# Transferable Adversarial Examples for Anchor Free Object Detection

[arXiv](https://arxiv.org/abs/2106.01618), [PDF](https://arxiv.org/pdf/2106.01618.pdf)

## Authors

- Quanyu Liao
- Xin Wang
- Bin Kong
- Siwei Lyu
- Bin Zhu
- Youbing Yin
- Qi Song
- Xi Wu

## Abstract

Deep neural networks have been demonstrated to be vulnerable to adversarial attacks: subtle perturbation can completely change prediction result. The vulnerability has led to a surge of research in this direction, including adversarial attacks on object detection networks. However, previous studies are dedicated to attacking anchor-based object detectors. In this paper, we present the first adversarial attack on anchor-free object detectors. It conducts category-wise, instead of previously instance-wise, attacks on object detectors, and leverages high-level semantic information to efficiently generate transferable adversarial examples, which can also be transferred to attack other object detectors, even anchor-based detectors such as Faster R-CNN. Experimental results on two benchmark datasets demonstrate that our proposed method achieves state-of-the-art performance and transferability.

## Comments

Accepted as oral in ICME 2021

## Source Code

Official Code



Community Code

- [https://paperswithcode.com/paper/transferable-adversarial-examples-for-anchor](https://paperswithcode.com/paper/transferable-adversarial-examples-for-anchor)

## Bibtex

```tex
@misc{liao2021transferable,
      title={Transferable Adversarial Examples for Anchor Free Object Detection}, 
      author={Quanyu Liao and Xin Wang and Bin Kong and Siwei Lyu and Bin Zhu and Youbing Yin and Qi Song and Xi Wu},
      year={2021},
      eprint={2106.01618},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```

## Notes

Type your reading notes here...

