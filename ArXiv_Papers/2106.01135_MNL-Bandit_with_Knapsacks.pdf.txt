arXiv:2106.01135v1 [cs.LG] 2 Jun 2021

MNL-Bandit with Knapsacks

Abdellah Aznag
Columbia University
Vineet Goyal
Columbia University
Noemie Perivier
Columbia University

We consider a dynamic assortment selection problem where a seller has a fixed inventory of N substitutable

products and faces an unknown demand that arrives sequentially over T periods. In each period, the seller

needs to decide on the assortment of products (of cardinality at most K) to offer to the customers. The

customer's response follows an unknown multinomial logit model (MNL) with parameters v. The goal of

the seller is to maximize the total expected revenue given the fixed initial inventory of N products. We give

a policy that achieves a regret of O~

K K N T

1

+

vmax qmin

OPT

under a mild assumption on the model

parameters. In particular, our policy achieves a near-optimal O~(T ) regret in the large inventory setting.

Our policy builds upon the UCB-based approach for MNL-bandit without inventory constraints in [1]

and addresses the inventory constraints through an exponentially sized LP for which we present a tractable approximation while keeping the O~(T ) regret bound.

Key words : Exploration-exploitation, assortment optimization, multinomial logit, inventory constraints, online learning

1. Introduction
In this paper, we study a dynamic assortment optimization problem under bandit feedback, where a seller with a fixed initial inventory of N substitutable products faces a sequence of i.i.d. customer arrivals (with an unknown distribution) over a time horizon of T periods, and needs to decide in each period on an assortment of products to offer to the customer to maximize the total expected revenue. Such a problem arises in many applications including online retail and recommendations. The seller has initially no (or only limited) information about the customer's preferences and needs to learn them through repeated interaction with the i.i.d. customers. Specifically, in each period,
1

2

the seller offers an assortment to the customer; the customer makes a choice from the assortment

according to the unknown preferences or choice model, and the seller only observes the eventual

choice from the given assortment and needs to update the estimate and future actions under this

bandit feedback. Therefore, this problem exemplifies the classical trade-off between exploitation and

exploration: the seller needs to simultaneously gain information about the customer's preferences

and offer revenue-maximizing assortments, while respecting the resource constraints.

More formally, we consider in this work the MNL-bandit problem with knapsack constraints,

where a seller sells N products over a time horizon of T periods. Each product i  [N ] is associated

with a revenue ri and has an initial inventory level of qi units, which cannot be replenished once

it has been exhausted. In each period t, a customer arrives and the seller must decide on an

assortment St  [N ] of size (or cardinality) at most K to offer to the customer. The customer

makes the purchase decision ct  St  {0} according to a multinomial logit (MNL) choice model

with unknown parameters. Here, 0 denotes the no-purchase option. The MNL choice model, first

introduced in Luce (1959), Plackett (1975) and Mcfadden (1977), can be described as follows: given

the assortment S, the probability that product i is purchased is given by:





vi

pi

(S

)

=

v0
0

+

jS vj

if i  S  {0} otherwise

where {vi}i[N]{0} are model parameters unknown to the seller. We assume that the {vi}i[N] are upper bounded by some known constant vmax. The sale of product i decreases by one unit the available amount of product i. When set S is offered, the expected consumption of product i  S is equal to pi(S), while the expected revenue R(S) of assortment S can be written as follows.

R(S) = ripi(S)
iS
We would like to mention that even in the case where the model parameters are known to the seller, we are not aware of any computationally efficient algorithm to compute the optimal dynamic
policy that respects the hard inventory constraints. Note that this contrasts with the static MNL

3

assortment problem without inventory constraints, where optimal solutions are well characterized (see for instance Talluri and van Ryzin (2004), Rusmevichientong et al. (2010), D´esir et al.

(2014) and Davis et al. (2013)). Following previous works on bandits with knapsack constraints (for example, Agrawal and Devanur (2019) and Badanidiyuru et al. (2013)), we thus consider a stronger benchmark, where the hard inventory constraints are relaxed. This benchmark is based on an exponential-size LP (described more precisely in Section 2.1).

1.1. Our contributions

We present a UCB-based algorithm for the MNL-bandits with knapsacks problem. Our policy

achieves an O~

 K KNT

1 + vmax/qminOPT

regret bound. Here qmin is the minimum inven-

tory for any product. Therefore, under the large-inventory setting, our bound is near-optimal:

O~(K

 KN

T

).

There

are

two

key

challenges

that

we

address

in

our

work:

i)

the

design

of

upper

and

lower confidence bounds on the revenue and product consumption rates of each assortment S and

ii) efficient computation of the optimistic assortments. The sequence of optimistic assortments we

offer is obtained by solving in each period an exponential-size LP that incorporates these lower and

upper confidence bounds of the model parameters. We show that solving the exponential LP can

be reduced to finding the optimal solution for an assortment optimization problem which involves

a difference of MNL models. Using techniques from D´esir et al. (2014), we give an approximation

algorithm that achieves a weak approximation guarantee for the latter problem and show how

to derive from it a near-optimal solution to the exponential-size LP. Furthermore, we show that

using

this

approximate

solution,

our

optimistic

policy

still

achieves

O~(K

 K

N

T

)

regret

in

the

large-inventory setting.

Note that our work is closely related to the Bandits with Knapsacks problem (see for instance

Agrawal and Devanur (2019), Badanidiyuru et al. (2013), Xia et al. (2016) and Xia et al. (2015)), for which algorithms achieving optimal regrets in the large capacity regime have been recently proposed. However, these results are not directly applicable to our setting, as it would involve considering each possible assortment as an individual arm in the multi-arm bandits framework.

4
This would yield an exponential action space, and would therefore not lead to a tractable policy. On the contrary, by using the specific structure of our lower and upper confidence bounds, we are able to formulate an exponential LP similar to the one used in Agrawal and Devanur (2019), for which we can design an efficient approximation algorithm.
We would like to note that our algorithm estimates the unknown model parameters based on the sampling-based estimation presented in Agrawal et al. (2017a) for the MNL-bandit problem. Our work builds upon this aforementioned work to include global inventory constraints in the MNLBandit problem. Closest to our work is Cheung and Simchi-levi (2017), where the authors consider the MNL-bandits problem with inventory constraints. The authors propose two algorithms: an efficient online policy, which incurs a regret O~(T 2/3), and a UCB-based algorithm, which achieve a near optimal O~(T ) regret in the large-inventory regime. However, this last policy is not computationally efficient as it requires solving an exponential-size LP at each step. In our work, we formulate a different exponential-size LP, based on optimistic confidence bounds instead of exploration bonuses, and show that it can be efficiently solved, while still achieving the same regret guarantee as in Cheung and Simchi-levi (2017). 1.2. Related work Dynamic assortment optimization. The dynamic assortment optimization problem was first considered by Caro and Gallien (2007), under the assumption of independent demand across the products present in each assortment. More recent works incorporate a MNL model into the problem (see for instance Rusmevichientong et al. (2010), Saur´e and Zeevi (2013), Wang et al. (2018), Agrawal et al. (2017b) and Agrawal et al. (2017a)). In particular, Agrawal et al. (2017a) and Agrawal et al. (2017b) propose respectively UCB and Thompson-sampling based policies which
 achieve near-optimal O( N T ) regrets matching the information theoretic lower bounds. However, this prior stream of work does not address inventory constraints.
Online resource allocation and Online Assortments. Our problem is also closely related to the setting of online assortments and online resource allocation. Specifically, online assortments

5 with inventory constraints has been extensively studied when the seller can observe in each period the arriving customer's 'type' (which specifies the choice model governing the purchase decisions). When the customers belong to different segments with known arrival rates, this problem is the well-known choice-based network revenue management problem, for which approximation algorithm have been proposed (see for instance Gallego et al. (2015), Kunnumkal and Topaloglu (2010), Meissner and Strauss (2008) and Fata et al. (2019)). A few recent works focus on a setting with uncertainty on the future customers' types (see for instance Golrezaei et al. (2014), Cheung et al. (2018) and Bernstein et al. (2015)). In particular, the inventory balancing policy of Golrezaei et al. (2014) achieves a tight 1 - 1/e approximation guarantee for the adversarial arrival model and a 3/4-approximation guarantee for the i.i.d. arrival model in the large-inventory regime, where T / min qi = k for some positive integer k. In our setting, we do not observe the customer's 'type', but need to learn it over the T periods. Therefore, our problem combines aspects of the MNL-Bandit with online allocation or assortments.
We would like to comment on our regret bound in the light of known results in the online resource allocation framework, which considers the problem of matching an online sequence of requests to some budgeted agents. A key parameter in these works is the ratio  between the value of a single request and the agents' budgets. This ratio is usually called the 'bid to budget ratio' in reference to the Adwords problem Mehta et al. (2007). When  is assumed to be small, a 1 -  competitive ratio can be achieved in the stochastic arrival setting (Devanur and Hayes (2009), Devanur et al. (2019), Agrawal et al. (2009)). In particular, Devanur et al. (2019) recently obtained a 1 -  approximation algorithm when  = O(log(n/)/2) and showed that this upper bound on  is almost optimal. In our setting, by interpreting vmax as an upper bound on the bids and qmin as a lower bound on the
 budgets, we can see the large-inventory assumption for which we achieve a O( T ) regret as an analogue of the small-bid assumption in the resource allocation framework.
Budgeted bandits. Another closely related stream of literature is the budgeted bandits problem. This is an extension of the MAB problem where pulling an arm also generates a cost and the agent

6 is subject to a budget constraint (see for instance Tran-Thanh et al. (2012), Antos et al. (2008) and Bubeck et al. (2010)). In the case of random costs, this problem is referred to as the Bandits with Knapsacks problem (first introduced in Badanidiyuru et al. (2013)). Agrawal and Devanur (2019) recently showed that a natural extension of the UCB family of algorithms achieves a nearoptimal regret for a variant of BwK with more general resource constraints. Our work can be interpreted in this framework by considering each assortment as an independent arm. However, as mentioned above, a direct application of the techniques from Badanidiyuru et al. (2013) and Agrawal and Devanur (2019) in our setting would lead to a computational complexity and a regret bound both linear in the number of assortments, which is exponentially large.
Combinatorial bandits. Our problem is also closely related to the combinatorial bandits literature (see for instance Kveton et al. (2014), Chen et al. (2013) and Qin et al. (2014)), where in each period, the agent needs to decide on a subset of arms to pull (called a superarm), and obtains a rewards which is a function of the individual arms' rewards. Our problem can be translated to this framework, by interpreting each assortment as a superarm. However, in our setting, the individual arms' rewards depend on all the arms present in the subset offered, whereas the rewards are assumed independent in the combinatorial bandits framework. Furthermore, the revenue generated by an assortment is not even monotonic, as introducing new products in the assortment may reduce the probability of purchasing the most profitable items and may thus impair the expected revenue. Hence, we cannot directly use the existing combinatorial bandits algorithms to obtain a low regret policy.
2. Dynamic assortment policy
2.1. Preliminaries Here we discuss the model assumptions and introduce the benchmark LP. Following Agrawal et al. (2017b), we make the following assumption on the upper bound vmax on the attraction parameters {vi }i[N ] .
Assumption 1. vmax  v0 = 1.

7 The above assumption is fairly general and implies that the probability of no-purchase is at least as high as the probability of purchasing any other product. This is particularly true in the setting of online retail where probability of conversion is very small. We would like to mention that, unlike Cheung and Simchi-levi (2017), we do not require any knowledge of a lower bound on the utilities {vi}.
Benchmark. We are interested in non-anticipative policy where the set offered at time t only depends on the history up to time t: Ft = ({St, ct}t=1,...,t-1). We compare ourselves to a clairvoyant policy which knows the value of the parameters {vi} but not the choice realizations of the customers. Consider the following linear program:

(OPT-LP)

max ySR(S)
SS

s.t

yS pi(S )



qi T

i = 1, . . . , N

SS

y  0 , yS = 1
SS

Let OPT/T be the optimal value of (OPT-LP). Lemma 1 establishes that OPT is an upper

bound on the expected revenue of any feasible dynamic assortment policy.

Lemma 1. The expected revenue of any non-anticipative policy A is less than OPT.

We give the proof of Lemma 1 in Appendix B.5. Let yOPT be an optimal solution of (OPT-LP). Motivated by Lemma 1, we consider as benchmark the algorithm which plays in each period t a set St drawn according to yOPT. Note that the solution returned by this algorithm is feasible only in expectation, whereas the feasible policies in our setting are subject to hard inventory constraints. However, this advantage given to the benchmark algorithm is minimal in the largeinventory regime we are interested in. We would like to point out that this type of linear program has been often considered in the revenue management literature and is commonly referred to as the (choice-based) deterministic linear program Gallego et al. (2004). This strong benchmark was for instance used by Golrezaei et al. (2014) in the setting of dynamic assortment optimization,

8 and by Agrawal and Devanur (2019) and Badanidiyuru et al. (2013) in the more general context of bandits with knapsacks.

Objective. The objective of the seller is to minimize its cumulative expected regret, which is

expressed as: 2.2. Algorithm

T
Reg(T ) = OPT - E R(St)
t=1

We first give an outline of our dynamic assortment policy. We divide the time horizon into epochs,

where each epoch  is composed of E consecutive time steps. At the beginning of each epoch ,

we decide on the assortment S to offer to arriving customers repeatedly until we observe a no-

purchase. This policy is based on Agrawal et al. (2017a) that allows to obtain unbiased estimators

of the unknown MNL parameters. In particular, the average number of times each product has

been purchased in an epoch is an unbiased estimator of the MNL parameter corresponding to the

product. Based on these estimators, we construct at the beginning of each epoch  lower and upper

confidence bounds on the cost and revenue of each assortment S implicitly. We then compute an

optimistic distribution {yS,}S[N] and sample the assortment S for epoch  based on the optimistic

distribution. Note that the sampling of S is done only once, at the beginning of each epoch . We denote by qir the remaining inventory for each product i at time t. After observing the
purchase decision ct of the customer, we decrease qcrt by one unit. The algorithm stops as soon as any of the products i is out of stock (i.e. qir = 0).
Note that the procedure described above is not a valid algorithm as it involves computing upper

and lower bounds on the cost and revenue of all possible assortments, and computing and sampling

from a distribution with possibly exponentially sized support. One of the main contributions is to

give a tractable approach for these steps: we present a computationally efficient algorithm which

returns an approximately optimal distribution without having to explicitly use the lower and upper

bounds for each set S. Furthermore, the distribution returned by our algorithm has polynomial

size support and therefore, we can sample efficiently from it.

9 We are now ready to describe the details of the algorithm.

Confidence bounds for the revenue and cost of each assortment. At the beginning of each epoch , the algorithm uses the previous purchase decisions of the customer to design lower and upper confidence bounds on the true utility vi of each product. Let vL,CiB and vU,CiB denote the bounds obtained at the beginning of epoch  + 1, whose construction follows Agrawal et al. (2017a) and will be presented later. Based on the {vL,CiB} and {vU,CiB}, we consider the following upper confidence bound on the expected revenue of each assortment S:

RUCB(S) :=

ri 1 +

iS

vU-C1B,i vLCB
jS -1,j

(1)

On the same way, we consider the following lower confidence bound on the expected consumption

of product i when assortment S is offered:

pL,CiB(S) := 1 +

vL-CB1,i vUCB
jS -1,j

(2)

Optimistic distribution on assortments. In epoch , our algorithm computes the distribution over sets which maximizes revenue for the optimistic estimates of the consumption rates and the revenues. More precisely, the distribution {yS,} played in epoch  is the solution of the following program:

(P)

max

yS RUCB (S )

SS

s.t

yS

pL,CiB

(S

)



(1

-



)

qi T

i = 1, . . . , N

SS

y  0 , yS = 1
SS

where  > 0 is a shrinkage factor. To understand the role of , suppose we replace the lower

confidence bound {vL,CiB} by the true utilities {vi}. Then the total expected consumption of product

i while playing distribution {yS} over the T periods does not exceed qi. Adding the shrinkage factor

 allows us to convert this guarantee in expectation to a high probability guarantee, in order to

10

respect the hard inventory constraints. The exact expression of  comes naturally in the inventory

consumption analysis (in Section 3):



:=

1 qmin









vmaxT 4K K log T + KN + KT 1/4 + 2K KT 1/4 + 2CN log( N T 4 + 1) (3)

The set S played throughout epoch  is then drawn according to the distribution {yS,}.

Note that by interpreting each assortment S as an independent arm in the multi-arm bandits

framework, the program (P) is analogue to the LP used by the UCB-based algorithm presented in Agrawal and Devanur (2019) for the Bandits with Knapsacks (BwK) problem. The major difference

is that in our setting, problem (P) involves an exponential number of variables and therefore it cannot be directly solved. However, based on our special construction of the confidence bounds

for each arm (which uses the lower and upper bounds for the utilities of the individual products,

whereas Agrawal and Devanur (2019) directly constructs the confidence interval from estimates of

the utilities of each arm), we show that (P) can still be solved efficiently. In particular, we design

in section 4 an algorithm which returns in polynomial time an approximately optimal distribution

{yS,}.

Confidence intervals for the product utilities (based on Agrawal et al. (2017a)). We now present the details on the construction of the confidence bounds {vL,CiB} and {vU,CiB}. We would like to mention that we do not follow the classical approach to design confidence intervals based on maximum likelihood estimators (which has been considered for instance by Cheung and Simchi-levi (2017) in their dynamic assortment policy under inventory constraints). The main limitation of the MLE approach is that it requires the knowledge of a lower bound on the values of the true utilities, which may not be available in practical applications. In order to avoid this restrictive assumption, we choose to design our confidence intervals based on the sampling method presented in Agrawal et al. (2017a). We describe it here for the convenience of the reader.
Following Agrawal et al. (2017a), we define v^i, the number of customers who purchased i in epoch , for all i  S:
v^i, = ½(ct = i).
tE

11

We then compute v¯i, the average number of times each product i is purchased per epoch:

v¯i,

=

1 Ti()

·

v^i, ,

 Ti()

where Ti() = {   | i  S} , Ti() = |Ti()| denote respectively the set of epochs and the number

of epochs which have offered product i so far. At the end of epoch , we compute upper and lower

confidence bounds on the true utility vi of product i as:

  viU,CB = min v¯i, +





v¯i,

48

log ( N  Ti()

+

1)

+

48

log ( N  Ti()

+

1)

  , vmax ,

  viL,CB = max v¯i, -





v¯i,

48

log

(N Ti()



+

1)

+

48

log

(N Ti()



+

1)

  , 0 .

The details of our dynamic assortment policy are described in Algorithm 1. We are now ready to present our regret bound.

Theorem 1. The regret of our policy  presented in Algorithm 1 is bounded as

Reg(T )  O~

 K KNT

1

+

vmax qmin

OPT

In particular, we have a O~(T ) bound when all initial inventory levels are large:

Corollary

1.

In

the

large-inventory

regime

with

qvmminax



, OP T
P (log(T ))

for

some

polynomial

P:

Reg(T ) = O~(KKN T )

3. Regret analysis

The proof of our regret bound requires two major steps. We show that with high probability, none of the products is depleted before the end of the time horizon (i.e. the random stopping time  of our algorithm satisfies  = T with high probability.). Then we show that, conditionally on  = T , the sequence of assortments successively offered by our algorithm generates a high enough revenue.
At the heart of the proof are the design of the upper and lower confidence bounds for the revenue and product consumption rates of each assortment as described in section 2.2, and the optimistic choice of assortment made at the beginning of each epoch .

12

ALGORITHM 1: Exploration-Exploitation Policy for MNL-Bandit with Knapsacks

Initialization:



=

1 T

, viU,C0B

=

vmax, viL,C0B

=

0,

for

all

i

=

1, . . . , N ,

c0

=

0

 = 1 ; keeps track of the total number of epochs.

qr = q ; keeps track of the remaining inventory.

for t = 1, . . . , T do

if ct-1 = 0 then Sample S according to the distribution y, where y achieves a

1/(1 + )-approximation for (P). end

Offer assortment S and observe the purchase decision ct of the customer ;

if ct = 0 (i.e., the decision was a no purchase) then
Compute v^i, = tE ½(ct = i), no. of consumers who preferred i in epoch , for all
i  S ;

Update Ti() = {   | i  S} , Ti() = |Ti()|, set and no. of epochs until  that

offered product i ;

Update Update

v¯i,

=

1

Ti

()  Ti()

v^i,

,



viU,CB = min v¯i, +

sample mean of



v¯i,

48

log( Ti

N 4 ()

the estimates ;



+

1)

+

48

log( N 4 Ti()

+

1)

  Update viL,CB = max v¯i, -





v¯i,

48

log( N 4 Ti()

+

1)

+

48

log( N 4 Ti()

+

1)

  , vmax ;   , 0 ;

=+1 ;

end

else

Add time t to epoch E and decrease qcrt by 1 ;
if qcrt = 0 then Stop the algorithm.

end

end

end

13

Before giving our regret proof, we enumerate some essential properties of the epoch lengths, the estimators {v¯i,}, and the upper and lower confidence bounds {viU,CB}, {viL,CB}, which were first stated in Agrawal et al. (2017a). Note that Agrawal et al. (2017a) only considers the upper confidence bounds {viU,C B}. However, we show that similar concentration results are satisfied by the lower confidence bounds {viL,CB}. We make the following claims, whose proofs are given in Agrawal et al. (2017a).

Lemma 2 (Agrawal et al. Agrawal et al. (2017a)). The following properties hold:

(i) For every item i and epoch , the estimator v¯i, is unbiased. (ii) Conditionally on S, the length of the th epoch |E| is a geometric random variable with param-

eter 1/(1 + iS vi).

(iii)

For

every

item

i

and

epoch

,

vi  [viL,CB, viU,C B]

with

probability

at

least

1

-

6 N

.

(iv) There exist some universal constants C1 and C2 such that for all i  [N ],   [L],

max viU,CB - vi, vi - viL,CB  C1



vi

log( N  Ti()

+

1)

+

C2

 log( N  + 1)
Ti()

with probability at least 1 - 7/N .

The above lemma implies that with high probability, viU,C B and viU,C B are indeed confidence bounds for the true parameters v, and that in addition, we can bound the difference with the true parameters. Our regret analysis relies on the stronger property given in Lemma 3, which implies that starting from a certain rank, the properties (iii) and (iv) are simultaneously satisfied by all the estimators. Unlike in Agrawal et al. (2017a), this property is required to analyse the total product consumption and show that the hard inventory constraints are respected with high probability. The proof of Lemma 3 is stated in Appendix B.1.

Lemma 3. Define, for each i  [N ],   [L],   2, the following events:

ACi,B =

max viU,CB - vi, vi - viL,CB  C1





vi

log( N 4 Ti()

+

1)

+

C2

log( N 4 + 1) Ti()

 viL,CB  vi  viU,CB

NL

ACB =

ACi,B

i=1 =-1

14

where

C1, C2

are

universal

constants.

Then

for

each

i, ,

P(ACi,B )  1 -

13 N 8

.

Moreover,

P (ACB )



1

-

13(T - 8

)

Consequently, starting from epoch , the upper bounds on the revenue RUCB(S) and the lower bounds on the product consumption rates {pL,CiB(S)}iS as defined in equations (1) and (2) are indeed upper and lower bounds on the true revenue and costs of each assortment S with high

probability:

RUCB(S) =

ri 1 +

iS

vU-CB1,i vLCB
jS -1,j



iS

ri 1 +

vi
jS

vj

=

R(S)

pL,CiB(S) = 1 +

vL-CB1,i vUCB
jS -1,j



1+

vi
jS

vj

=

pi(S)

Moreover, the following lemma allows us to control the convergence of the upper confidence

bound RUCB(S) towards the true revenue.

Lemma 4. Conditionally on ACB, for all   , the difference between the estimated revenue and the true expected revenue for each assortment S is bounded as

RUCB(S) - R(S) 

ri V (S) + 1

(viU,C-B1 - vi) +

(vj - vjL,CB-1

iS

jS

On the same way, we can control the convergence of the expected consumption rates as follows.

Lemma 5. Conditionally on ACB, for all   , for all assortment S and for all i  S, the difference between the estimated consumption rate for product i and the true expected rate is bounded

as

pi(S)

-

pL,CiB (S )



V

1 (S)

+

1

(vi - viL,C-B 1) +

(vjU,C-B 1 - vj

jS

The proofs of Lemmas 4 and 5 follow naturally from the definitions of RUCB(S) and pL,CiB(S) and

are provided in Appendix B.2.

We are now ready to sketch the proof of our regret bound. We first focus on the inventory

constraints and show that with high probability, the stopping time of Algorithm 1 is T . This is

equivalent to proving that if we did not keep track of the remaining inventory, Algorithm 1 would

15

still produce a feasible policy with high probability. Then, assuming this is true, we show that our

policy achieves a O~

 K KNT

1 + OPTvmax/qmin

regret.

The proofs of these two statements involve the same ideas. The key element is the choice of the

distribution played throughout epoch  as an approximate solution of problem (P), combined with the convergence of {RUCB} and {pL,CiB} towards the true revenues and consumption rates. We then use concentration bounds to control the behavior of the real sequence of assortments S1, S2, ...SL offered by the algorithm.

Bounding the resource consumption. In this section, we prove that the algorithm stops at

 = T with high probability. This is equivalent to proving that no product is entirely consumed

before time T . To ease the presentation, in this section we assume that the algorithm does not

keep track of the remaining inventory, and we will prove that this maintains the feasibility of the

policy with high probability. We introduce the following notations:

½ ½ ½ (i, S) =

iS 1 +

vi jS vj

,

U CB(i, S) =

iS 1 +

vU-C1B,i vLCB
jS -1,j

,

LCB(i, S) =

iS 1 +

vL-CB1,i vUCB
jS -1,j

V (S) = vi
iS
We also define, for each t  [T ], t to be the (unique)   [1, L] satisfying t  E, i.e. the index of the

epoch containing t. We also assume that the last epoch is finished, as its contribution is exceedingly

small when T is large. Let  := T 1/4. Using Lemma 3, we know that ACB is a high probability event. Besides, conditionally on ACB, all the estimators simultaneously satisfy the properties (iii) and (iv) of Lemma 2 starting from epoch . Let t := min{t  [T ] | t  } be the first time step of epoch .

For each i  [N ], we decompose the total consumption of product i as follows.

T

t -1

T

½ct=i =

½ct=i +

½ct=i

(4)

t=1

t=1

t=t

We will bound each of these two terms separately. We first consider the second term and decompose

it again as follows.

T

T

½ct=i =

½ct=i - (i, St )

(5)

t=t

t=t

16

T

+

(i, St ) - LtCB(i, St )

(6)

t=t

T

+

LtCB(i, St ) - yt(S)LtCB(i, S)

(7)

t=t

S

T

+

yt (S)LtCB(i, S)

(8)

t=t S

Bounding (5) and (7). The increments ½( ct=i - (i, St))t=1,...,T are uniformly bounded by 1 and

have expectation zero conditionally on the history up to time t and the value of St. Therefore, we could directly apply Azuma-Hoeffding inequality to obtain a O~(T ) bound on sum (5). However,

we will use the stronger concentration inequality used in Agrawal and Devanur (2019) and first

given in Kleinberg et al. (2008) and Babaioff et al. (2015) in order to derive a bound with a better

dependency in the problem dependent parameter vmax. More precisely, we decompose (5) as:

½ct=i - yt (S)(i, S) +

yt(S)(i, S) - (i, St)

SS

SS

Using that ½ct=i is a Bernoulli variable with parameter uniformly bounded by vmax and combining
Optional Stopping arguments with Azuma-Hoeffding with respect to the filtration {c1, . . . , ct} (see Agrawal and Devanur (2019)), we can bound the first term in O~(vmaxT ).

To control the behavior of the second term, we can directly apply concentration inequalities

similar to Azuma-Hoeffding since the terms in expectation are uniformly bounded by vmax. The same discussion holds for bounding (7). Specifically, we have the following result, whose proof is

detailed in Appendix B.3.

Lemma 6. The following event:

Amart1 =

T

T

½ct=i - (i, St ) +

LtCB(i, St) - yt (S)LtCB(i, S)

t=t

t=t

S

 3 2vmaxT log T + 3 log T

satisfies

P(Amart1)  1 -

µ0 T

,

where

µ0

is

a

universal

constant.

17

Bounding (6). To bound (6), we use the convergence of the product consumption rates to the true

expected rates as stated in Lemma 5, and the fact that the epoch length are geometric variables

with parameters 1/ 1 + V (S) [L] . We decompose (6) as follows:

T

L

(i, St ) - LtCB(i, St )  |E| (i, S) - LCB(i, S)

t=t

=

L



|E| - (V (S) + 1) (i, S) - LCB(i, S)

=

L

+ (V (S) + 1) (i, S) - LCB(i, S)

=

(9) (10)

(10) can be bounded with high probability by using the following lemma.

Lemma 7. Conditionally on the event ACB, the following inequalities hold for all i  [N ],   , and assortment S:

L



V (S) + 1 (i, S) - LCB(i, S)  2C log( N T 4 + 1) N + KN T vmax

=1

The proof of 7 derives naturally from Lemma 5 and the bounds on {viL,CB}, {viU,C B} stated in Lemma 3. The technical details are described in Appendix B.2.
To bound (9), we first use |(i, S) - Li,CB(i, S)|  vmax  vmax  1, which is true conditionally on ACB. We then use concentration inequalities on the sum of random geometric variables. We need here a stronger concentration result than in Agrawal et al. (2017a), where the authors were

only interested in bounding the revenue in expectation and could immediately conclude from the

fact that E(|E||S) = V (S) + 1. In our setting, we have to respect the hard inventory constraints,

which requires a refined analysis on the concentration of the length of the epochs. Note that these

concentration inequalities do not hold in general. We leverage here the fact that conditionally

on the offered assortments {S}, the means of all the variables {|E|} are uniformly bounded by

(1 + K), where K is the maximal size of any feasible assortment. This concentration bound (which

we prove in Appendix B.4) can be stated as follows:

18

Lemma 8. The following event:

L
Aepochs1 =

=

satisfies

P(Aepochs1)  1 -

2 T

.

|E| - (V (S) + 1)

 (K + 1)

2(K + 1)T log T

Combining Lemmas 7 and 8 allows us to bound (6) conditionally on Aepochs1  ACB:

T
(i, St ) - LtCB(i, St ) 
t=t





vmaxT (K + 1) 2(K + 1) log T + 2C KN + 2CN log( N T 4 + 1)

Bounding (8). By construction, the distribution {y,S} played in epoch  is a feasible solution of

the program (P), hence it immediately follows that:

T

L

yt (S)LtCB(i, S) = |E| y(S)LCB(i, S)

t=t S

=

S

L



|E

|(1

-



)

qi T

=

 (1 - )qi

Putting all together, we obtain that conditionally on Amart1  ACB  Aepochs1, the following bound

holds:

T
½ct=i  3 2vmaxT log T + 3 log T
t=t
 + vmaxT (K + 1) 2(K + 1) log T + C KN

 + C(N + 1) log( N T 4 + 1)

+ (1 - )qi







 6C vmaxT 4K K log T + KN + 2CN log( N T 4 + 1) + (1 - )qi

(11)

We now consider the first term of equation (4). We bound it by using a similar concentration result on the sum of random geometric variables as in Lemma 8. More precisely, we show the following lemma.

Lemma 9. The following event:

Aepochs2 = t - K  (K + 1) 16(K + 1) log )

satisfies

P (Aepochs2)



1-

2 8

.

19

It follows from Lemma 9 that with high probability:

t -1



½ct=i  t  K + (K + 1) 16(K + 1) log  + KT 1/4 + 2K KT 1/4 log T

t=1

(12)

By combining (11) and (12) and using the definition of  as T 1/4, we obtain the following bound

on the total inventory consumption of product i, conditionally on Amart1  ACB  Aepochs1  Aepochs2:

T
½ct=i  6C
t=1









vmaxT 4K K log T + KN + KT 1/4 + 2K KT 1/4 + 2CN log( N T 4 + 1) +(1 - )qi

3 :=qmin from ( )

 qi

Moreover, by using a union bound on the events from Lemmas 9, 3, 8, 6, we get:

P(ACB  Amart1  Aepochs1  Aepochs2)  1 -

13(T - ) 8

+

2 8

+

2 T

+

µ0 T



1

-

17

+ T

µ0

=

1

-

(1) T

where we used T    T 1/4. Hence, with high probability, the total consumption of any product i

does not exceed qi. This proves that the random stopping time  of Algorithm 1 is exactly T with high probability.

Bounding the regret. In this section, we condition on the event  = T , which we have proved to

occur with high probability. The proof follows the same line as the analysis on resource consump-

tion. We only outline the main steps and defer the technical details to Appendix B. As before,

we bound separately the regret incurred before time t and after t. We start by decomposing the

total revenue generated by our policy from time t as follows.

T

T

N

rct =

rct - ri(i, St )

t=t

t=t

i=1

T
+
t=t

N
ri (i, St ) - UtCB(i, St )
i=1

T
+
t=t

N

N

riUtCB(i, St) - y(S) riUtCB(i, S)

i=1

SS

i=1

T

N

+

yt (S) riUtCB(i, S)

t=t SS

i=1

(13) (14) (15) (16)

Similarly as before, we use sharper Azuma Hoeffding concentration results to bound (13) and (15).

More precisely,

20

Lemma 10. There exists a universal constant µ1 > 0 such that the following event:

Amart2 =

T

N

N

T

r(ct) - ri(i, St ) + ri

yt (S) UtCB(i, St ) - UtCB(i, S)

t=t

i=1

i=1 t=t SS

 8Krmax vmaxT log T + 3rmax log T

satisfies

P (Amart2 )



1-

µ1 T

The proof is stated in Appendix B.3. In similar fashion to bounding (6), we use the convergence result on the expected revenue from Lemma 4 to show the following lemma, whose proof is deferred to Appendix B.2. Note that we only need to control the expected revenue, hence the high probability convergence results on the length of the epochs are not required here and the result follows more directly.

Lemma 11. Conditionally on ACB,

T
E
t=t

N
ri UtCB(i, St ) - (i, St ))
i=1

  4rmaxC log( N T 4 + 1)2 N 2 + K KN T vmax

Before bounding (16), we state the following result on the revenue achieved by the optimal

solution yOPT of (P). The proof is given in Appendix B.5.

Lemma 12. Conditionally on ACB,

N

yOPT(S) riUi,CB(i, S)  (1 - )OP T /T

SS

i=1

Using the fact that the distribution yt played at time t is a 1/(1 + )-approximate solution for

(P), this allows us to prove that, conditionally on ACB:

T

L

yt (S) riUtCB(i, S) = |E| y(S) riU CB(i, S)

t=t SS

i

=

SS

i



(1 - ) (1 + )

OPT/T

L

|E|

=

=

(1

- )(T - (1 + )T

t)

OPT

21

Therefore, by combining the previous lemmas and using the high probability bound on t from

Lemma 9, we obtain, conditionally on Ar1eg := ACB  Aepochs1  Amart1  Amart2  Aepochs2:

1 rmax E

T

OPT -

rct Ar1eg

t=t

 8K vmaxT log T + 3 log T 
+ 4C log( N T 4 + 1)2 N 2 + K

KN T vmax

+



+



+

(K+1)T 1/4 T

OPT rmax

In order to complete the proof of Theorem 1, we use that Ar2eg = (Ar1eg)c is a rare event. More

precisely, by using Lemmas 3, 6, 8, 9, 10, we obtain, by using a union bound and the choice of

 = T 1/4:

P (Ar2eg )



13(T

- ) + 8

1

+

µ0

+ T

µ1



(1) T

Moreover, we have the immediate bound OPT  T rmax, thus:

Hence, by

using

E

T
OPT - rct Ar2eg

P (Ar2eg )



T

rmax

(1) T

=

(1)

t=1

=

 6C vmaxT





4K K log T + KN

 + KT 1/4 + 2K KT 1/4 + 2CN

/qmin

and

by

setting



=

1 T

,

we

finally

obtain:

Reg(T )  E

T

OPT -

rct Ar1eg

t=t

P(Ar1eg) + E

T

OPT -

rct Ar2eg

t=t

P (Ar2eg )

= O~ K N T vmax + OPT

= O~

 K NT

1

+

vmax qmin

OPT

This concludes the proof of Theorem 1.

4. Solving the exponential-size LP

The major computational bottleneck of Algorithm 1 is to solve the exponentially-sized LP (P) for

all   [L].

(P)

max yS
SS

N
riU CB(i, S)
i=1

s.t

yS

LCB

(i,

S

)



(1

-

)

qi T

i = 1, . . . , N

SS

(17)

y  0 , yS = 1
SS

(18)

22

In this section, we provide a computationally efficient method to approximately solve problem (P). The main ingredients are the following: we first consider the dual problem (D) of (P) and show that to find an approximate solution for (D), it suffices to approximately solve an assortment problem involving the difference of two MNL objectives (MNL-diff-Assort). Using techniques from D´esir et al. (2014), we show that we can achieve a weak approximation guarantee for the assortment problem, and still be able to solve approximately the dual from it. Finally, we show that we can retrieve an approximately optimal solution for (P) from the approximate solution obtained for (D).

Reduction to a difference of MNL assortment problem (MNL-diff-Assort). Before writ-

ing the dual formulation of (P), let I = {i  [N ] | vL-CB1,i = 0}. Note that for all i  I, constraint (17) is automatically satisfied. Hence we can remove these constraints from the formulation.

First, we define reduced capacities q~,i := (1 - )qivU-C1B,i/(T vL-CB1,i) for all i  [N ] \ I, and reduced costs r~,i := rivL-CB1,i/vU-C1B,i for all i  [N ]. Let:

UCB(i, S) = 1 +

vU-CB1,i vUCB
jS -1,j

and

LCB(i, S) = 1 +

vL-CB1,i vLCB
jS -1,j

We can rewrite (P) as follows.

(P)

max yS
SS

N
r~,iLCB(i, S)
i=1

s.t ySUCB(i, S)  (1 - )q~,i i  [N ] \ I
SS

(19)

y  0 , yS = 1
SS

(20)

Note that LCB(i, S) and UCB(i, S) involve respectively only utilities of the form vL-CB1,i and vU-CB1,i, whereas U CB(i, S) and U CB(i, S) both include a mixture of the lower and upper utility confidence bounds. By rewritting (P) as (P), we are able to separate the terms vL-CB1,i and vU-C1B,i. This is the

key idea to make the difference of two MNL appear.

23 The dual of (P) has exponentially many constraints but only polynomially many variables. Hence it can be solved in polynomial time via the ellipsoid method assuming access to a polynomialtime separation oracle Bland et al. (1981). We show that in our case, a weaker oracle suffices to approximately solve the dual. The dual (D ) of (P) is given by:

(D )

min  +

iq~,i

i[N ]\I

s.t  

r~,iLCB(i, S) - iUCB(i, S) S  [N ], |S|  K

iS

i  0 i  [N ], i = 0 i  I

Given values of  and {i}i[N], finding a violating hyperplane or exhibiting a certificate of feasibility can be done by solving the following maximization problem:

max
|S|K

r~,iLCB(i, S) - iUCB(i, S)

iS

(21)

The above problem is an assortment optimization problem where the objective funtion is the

difference between the revenues obtained under two MNL models: the first one with rewards {r~,i} and utilities {vL-CB1,i}, the second one with rewards {i} and utilities {vU-C1B,i}. Note that we have added dummy variables i = 0 for i  I to highlight the writing as a difference of MNL models.

Weak approximation guarantee for the MNL-diff-Assort problem. It is not known whether the MNL-diff-Assort problem admits a FPTAS. However, we show that for all  > 0, adapting the FPTAS proposed in D´esir et al. (2014) for the capacity constrained assortment optimization problem for a mixture of MNL model allows us to compute an assortment S which satisfies the weaker property given in Proposition 1. We would like to mention that the results from D´esir et al. (2014) are not immediately applicable to our setting as this work considers a weighted sum of MNL model with positive coefficients, whereas we consider a difference of MNL models. It also explains why we could only obtain a weaker approximation guarantee as compared to the FPTAS from D´esir et al. (2014). The procedure is based on solving a polynomial number of dynamic

24

programs. It involves guessing the values of the denominators 1 +

vUCB
iS -1,i

,

1+

vLCB
iS -1,i

,

and numerators iS r~,ivU-C1B,i, iS ivL-CB1,i for the optimal assortment S within a factor (1 + ).

We give here an outline of the algorithm. The details are described in Algorithm 2 (Appendix A).

Following D´esir et al. (2014), we consider sets of guesses 2 and 2 with size polynomial in 1/.

Let U and R denote respectively an upper bound on the utilities and the rewards. For each possible

guess (h, g)  2 × 2 , a dynamic program is used to find a feasible assortment S which satisfies:

r~,ivL-CB1,i  h1(1 - 2)
iS

and

1 + vL-CB1,i  g1(1 + 2)
iS

ivU-C1B,i  h2(1 + 2)
iS

and

1 + vU-C1B,i  g2(1 - 2)
iS

Proposition 1 establishes the approximation guarantee achieved by Algorithm 2, and follows

from D´esir et al. (2014). We include the proof in Appendix A for the sake of completeness.

Proposition 1. Algorithm 2 has a running time in O(log(N RU )4)N 5/8) and returns a solu-

tion S which satisfies:

R(S)  R(S)(1 - ) - 4 iUCB(i, S)
iS

(22)

where R(S) denote the revenue obtained for assortment S:

R(S) :=

r~,iLCB(i, S) - iUCB(i, S)

iS

Note that without further restrictions on the values taken by the {i}i[N], the right-hand side of

equation (22) can be arbitrarily small, hence Proposition 1 does not provide a full approximation

guarantee on the solution returned by Algorithm 2.

Retrieve an approximate solution for the primal problem. The fact that given an approximate separation oracle for the dual, we can get an approximate solution for the primal program using the ellipsoid algorithm was also used in Carr and Vempala (2000) and Fleischer et al. (2011) for a class of packing-covering linear programs. Note that we do not have an approximate separation oracle for the dual. However, we can show a similar result by using the weaker guarantee from Proposition 1. We have the following lemma.

25
Lemma 13. For any  > 0,  > 0, we can find a 1/(1 + 8T ) - -approximate solution of (P) by running ellipsoid on (D) using Algorithm 2 (Appendix A) as a separation oracle.
W e perform a binary search with precision  to find the smallest value  for which the LP (D) with the additional constraint  + i[N] iq~,i   is feasible. At each step of the binary search, we run the ellipsoid algorithm using Algorithm 2 as a separation oracle to determine feasibility. Let (, {i}i[N]) be the solution obtained at the last step of the procedure. By definition of our separation oracle, the solution satisfies:

  R(S)(1 - ) - 4 iUCB(i, S)
iS

(23)

Let  =  + i[N]\I iq~,i. From (23), we obtain that  + 4 iS iUCB(i, S) /(1 - ), {i}

is feasible for (D). Besides, its objective value is equal to:

 + 4

iS iUCB(i, 1-

S)

+

iq~,i

=

1

 -



+

i[N ]\I

i[N]\I iq~,i(1 + 4UCB(i, S)/q~,i

1-





 (1 + 8 max(1/q~,i))  +

iq~,i

i[N ]\I

( << 1)

 (1 + 8T ) 

where the last inequality comes from q~,i = (1 - )qivU-C1B,i/(T vL-CB1,i)  1/T . Hence by strong duality, val(P) = val(D)  (1 + 8T ) . To obtain an approximately optimal
primal solution, we consider a modified version (D~ ) of the dual problem (D), where we include only the constraints generated during the successive iterations of the ellipsoid method. Let  be the precision of our binary search, as described above. Then the optimal solution of (D~ ) has value at least  - . Since the ellipsoid method makes only a polynomial number of calls to the separation oracle, (D~) has only a polynomial number of constraints, hence its primal problem (P~) has only a polynomial number of variables, whose support is given by the assortments S successively returned by the separation oracle. It thus suffices to solve the polynomial size LP (P~) to obtain a distribution {yS,} whose value in the original primal problem is at least  -   val(P)/(1 + 8T ) - . This gives the desired guarantee for  small enough.

26
5. Conclusion
In this paper, we study a dynamic assortment selection problem under an unknown MNL demand and inventory constraints. We propose an optimistic algorithm which achieves near-optimal regret O~(T ) in the large-inventory setting. Our algorithm involves repeatedly solving an exponentially sized LP, which we prove to be tractable by presenting an approximation algorithm based on a difference of MNL assortment optimization problem. Our results are thus significantly stronger than Cheung and Simchi-levi (2017), which presents an algorithm that incurs a O~(T 2/3) regret and an intractable algorithm with a O~(T ) regret.
Even though solving the LP can be done in polynomial time, this is still a computationally expensive step. Motivated by recent algorithms based on online convex optimization for the online allocation framework Lu et al. (2020), one interesting research direction would be to study whether faster algorithms can be obtained in our setting while still achieving the same regret bounds.
Acknowledgments
This material is based upon work partially supported by: the National Science Foundation (NSF) grants CMMI 1636046, and the Amazon and Columbia Center of Artificial Intelligence (CAIT) PhD Fellowship.
References
Shipra Agrawal, Vashist Avadhanula, Vineet Goyal, and Assaf Zeevi. 2017a. MNL-Bandit: A Dynamic Learning Approach to Assortment Selection. CoRR abs/1706.03880 (2017).
Shipra Agrawal, Vashist Avadhanula, Vineet Goyal, and Assaf Zeevi. 2017b. Thompson Sampling for the MNL-Bandit. CoRR abs/1706.00977 (2017). arXiv:1706.00977
Shipra Agrawal and Nikhil R. Devanur. 2019. Bandits with Global Convex Constraints and Objective. Operations Research 67, 5 (2019), 1486­1502.
Shipra Agrawal, Zizhuo Wang, and Yinyu Ye. 2009. A Dynamic Near-Optimal Algorithm for Online Linear Programming. CoRR abs/0911.2974 (2009). arXiv:0911.2974
Andra´s Antos, Varun Grover, and Csaba Szepesv´ari. 2008. Active Learning in Multi-armed Bandits. In Algorithmic Learning Theory, Yoav Freund, La´szl´o Gy¨orfi, Gy¨orgy Tur´an, and Thomas Zeugmann (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 287­302.

27 Moshe Babaioff, Shaddin Dughmi, Robert Kleinberg, and Aleksandrs Slivkins. 2015. Dynamic Pricing with
Limited Supply. ACM Trans. Econ. Comput. 3, 1, Article 4 (March 2015), 26 pages. Ashwinkumar Badanidiyuru, Robert Kleinberg, and Aleksandrs Slivkins. 2013. Bandits with Knapsacks.
CoRR abs/1305.2545 (2013). Fernando Bernstein, A. Gu¨rhan K¨ok, and Lei Xie. 2015. Dynamic Assortment Customization with Limited
Inventories. Manufacturing & Service Operations Management 17, 4 (2015), 538­553. Robert G. Bland, Donald Goldfarb, and Michael J. Todd. 1981. Feature Article--The Ellipsoid Method: A
Survey. Operations Research 29, 6 (1981), 1039­1091. https://doi.org/10.1287/opre.29.6.1039 arXiv:https://doi.org/10.1287/opre.29.6.1039 S´ebastien Bubeck, R´emi Munos, and Gilles Stoltz. 2010. Pure Exploration for Multi-Armed Bandit Problems. arXiv:0802.2655 [math.ST] Felipe Caro and J´er´emie Gallien. 2007. Dynamic Assortment with Demand Learning for Seasonal Consumer Goods. Management Science 53, 2 (2007), 276­292. Robert Carr and Santosh Vempala. 2000. Randomized Metarounding (Extended Abstract). In Proceedings of the Thirty-Second Annual ACM Symposium on Theory of Computing (Portland, Oregon, USA) (STOC '00). Association for Computing Machinery, New York, NY, USA, 58­62. https://doi.org/10.1145/335305.335312 Wei Chen, Yajun Wang, and Yang Yuan. 2013. Combinatorial Multi-Armed Bandit: General Framework and Applications. In Proceedings of the 30th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 28), Sanjoy Dasgupta and David McAllester (Eds.). PMLR, Atlanta, Georgia, USA, 151­159. Wang Chi Cheung, Will Ma, David Simchi-Levi, and Xinshang Wang. 2018. Inventory Balancing with Online Learning. CoRR abs/1810.05640 (2018). arXiv:1810.05640 Wang-Chi Cheung and David Simchi-levi. 2017. Assortment Optimization under Unknown MultiNomial Logit Choice Models. (03 2017). J. Davis, G. Gallego, and H. Topaloglu. 2013. Assortment Planning Under the Multinomial Logit Model with Totally Unimodular Constraint Structures.

28 A. D´esir, Vineet Goyal, and Jiawei Zhang. 2014. Near-Optimal Algorithms for Capacity Constrained Assort-
ment Optimization. Econometrics: Multiple Equation Models eJournal (2014). Nikhil R. Devanur and Thomas P. Hayes. 2009. The Adwords Problem: Online Keyword Matching with
Budgeted Bidders under Random Permutations. In Proceedings of the 10th ACM Conference on Electronic Commerce (Stanford, California, USA) (EC '09). Association for Computing Machinery, New York, NY, USA, 71­78. Nikhil R. Devanur, Kamal Jain, Balasubramanian Sivan, and Christopher A. Wilkens. 2019. Near Optimal Online Algorithms and Fast Approximation Algorithms for Resource Allocation Problems. CoRR abs/1903.03944 (2019). arXiv:1903.03944 Elaheh Fata, Will Ma, and David Simchi-Levi. 2019. Multi-stage and Multi-customer Assortment Optimization with Inventory Constraints. CoRR abs/1908.09808 (2019). arXiv:1908.09808 Lisa Fleischer, Michel X. Goemans, Vahab S. Mirrokni, and Maxim Sviridenko. 2011. Tight Approximation Algorithms for Maximum Separable Assignment Problems. Mathematics of Operations Research 36, 3 (2011), 416­431. G. Gallego, G. Iyengar, R. Phillips, and A. Dubey. 2004. Managing Flexible Products on a Network. Guillermo Gallego, Anran Li, Van-Anh Truong, and Xinshang Wang. 2015. Online Resource Allocation with Customer Choice. arXiv:1511.01837 [math.OC] Negin Golrezaei, Hamid Nazerzadeh, and Paat Rusmevichientong. 2014. Real-Time Optimization of Personalized Assortments. Management Science 60, 6 (2014), 1532­1551. Svante Janson. 2018. Tail bounds for sums of geometric and exponential variables. Statistics and Probability Letters 135 (2018), 1 ­ 6. https://doi.org/10.1016/j.spl.2017.11.017 Robert Kleinberg, Aleksandrs Slivkins, and Eli Upfal. 2008. Multi-Armed Bandits in Metric Spaces. CoRR abs/0809.4882 (2008). arXiv:0809.4882 Sumit Kunnumkal and Huseyin Topaloglu. 2010. A New Dynamic Programming Decomposition Method for the Network Revenue Management Problem with Customer Choice Behavior. Production and Operations Management 19, 5 (2010), 575­590. https://doi.org/10.1111/j.1937-5956.2009.01118.x arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1937-5956.2009.01118.x

29 Branislav Kveton, Zheng Wen, Azin Ashkan, and Csaba Szepesv´ari. 2014. Tight Regret Bounds for Stochastic
Combinatorial Semi-Bandits. CoRR abs/1410.0949 (2014). arXiv:1410.0949 Haihao Lu, Santiago Balseiro, and Vahab Mirrokni. 2020. Dual Mirror Descent for Online Allocation Prob-
lems. arXiv:2002.10421 [math.OC] R. Duncan Luce. 1959. Individual Choice Behavior: A Theoretical analysis. Wiley, New York, NY, USA. D. Mcfadden. 1977. Modelling the Choice of Residential Location. Transportation Research Record (1977). Aranyak Mehta, Amin Saberi, Umesh Vazirani, and Vijay Vazirani. 2007. AdWords and Generalized Online
Matching. J. ACM 54, 5 (Oct. 2007), 22­es. Joern Meissner and Arne Strauss. 2008. Network Revenue Management with Inventory-Sensitive Bid Prices
and Customer Choice. Department of Management Science, Lancaster University, Working Papers 216 (01 2008). https://doi.org/10.1016/j.ejor.2011.06.033 R. L. Plackett. 1975. The Analysis of Permutations. Journal of the Royal Statistical Society. Series C (Applied Statistics) 24, 2 (1975), 193­202. Lijing Qin, Shouyuan Chen, and Xiaoyan Zhu. 2014. Contextual Combinatorial Bandit and its Application on Diversified Online Recommendation. 461­469. Paat Rusmevichientong, Zuo-Jun Max Shen, and David B. Shmoys. 2010. Dynamic Assortment Optimization with a Multinomial Logit Choice Model and Capacity Constraint. Operations Research 58, 6 (2010), 1666­1680. Denis Saur´e and Assaf Zeevi. 2013. Optimal Dynamic Assortment Planning with Demand Learning. Manufacturing & Service Operations Management 15, 3 (2013), 387­404. Kalyan Talluri and Garrett van Ryzin. 2004. The Theory and Practice of Revenue Management. (01 2004). Long Tran-Thanh, Archie C. Chapman, Alex Rogers, and Nicholas R. Jennings. 2012. Knapsack based Optimal Policies for Budget-Limited Multi-Armed Bandits. CoRR abs/1204.1909 (2012). arXiv:1204.1909 Yining Wang, Xi Chen, and Yuan Zhou. 2018. Near-Optimal Policies for Dynamic Multinomial Logit Assortment Selection Models. In Advances in Neural Information Processing Systems, S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (Eds.), Vol. 31. Curran Associates, Inc., 3101­3110.

30 Yingce Xia, Wenkui Ding, Xu-Dong Zhang, Nenghai Yu, and Tao Qin. 2016. Budgeted Bandit Problems
with Continuous Random Costs. In Asian Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 45), Geoffrey Holmes and Tie-Yan Liu (Eds.). PMLR, Hong Kong, 317­332. Yingce Xia, Haifang Li, Tao Qin, Nenghai Yu, and Tie-Yan Liu. 2015. Thompson Sampling for Budgeted Multi-armed Bandits. CoRR abs/1505.00146 (2015).

Appendix
A. Algorithm for the MNL-diff-Assort problem

In this section, we describe the algorithm which allows us to obtain the weak approximation guarantee of Proposition 1. The procedure essentially follows D´esir et al. (2014), with some minor modifications to address the fact that our model includes negative coefficients. We detail it here for the convenience of the reader.
To ease the presentation, since we detail the algorithm for a fixed index , we drop in this section the index .
Following D´esir et al. (2014), we fix  > 0, and let u (resp U) and r (resp R) be respectively the minimum (maximum) of the utilities {viUCB}, {viLCB} and the minimum (maximum) of the rewards {r~i}, {i}. Note that some of the {viLCB} may be zero. In this case, D´esir et al. (2014) shows that the algorithm still works by replacing them with ur/(N R). We start by considering the set of guesses 2 and 2 , where:

 = {ru(1 + )k, k = 0, ..., L1}

and

 = {ru(1 + )k, k = 0, ..., L2},

and where L1 = O(log(nRU/r)/) and L2 = O(log(nRU/r)/). For each guess (h, g)  2 × 2 , a dynamic program is used to find a feasible assortment S which satisfies:

r~iviLCB  h1(1 - 2) and 1 + viLCB  g1(1 + 2)

iS

iS

iviUCB  h2(1 + 2) and 1 + viUCB  g2(1 - 2)

iS

iS

(24)

The first step is to discretize the coefficients as follows. For all i  [N ]  {0}, let

r¯i =

r~i viLCB h1/n

¯i =

r~i viUCB h2/(n + 1)

and

v¯iLCB =

r~i viLCB g1/(n + 1)

and

v¯iUCB =

r~i viUCB g2/n

(25) (26)

31

We also let v¯0LCB =

1 g1 /(n+1)

and v¯0UCB =

. 1
g2 /n

For convenience of notation, let S+ denote S  {0}, I = N/ - N and J = (N + 1)/ + N + 1. We use the following dynamic program (DP), built on the same model as the one in D´esir et al. (2014). For each (i, j, p)  [I]2 × [J]2 × [N ], let F (i, j, p) be the maximum cardinality of as subset S  [p] such that:

r¯i  i1
iS
¯i  j2
iS

and and

v¯iLCB  j1
iS+
v¯iUCB  i2
iS+

For all p  [N ], (i, j)  [I]2 × [J]2 the value of F (i, j, p) can be computed as follows.

 0 if i1  0, j2  0 and j1  v¯0LCB, i2  v¯0UCB

F (i, j, 1)

=

1+if

r¯1  i1  0, otherwise

¯1



j2,

j1



v¯0LCB

+ v¯1LCB,

i2



v¯0UCB

+ v¯1UCB

F (i, j, p + 1) = min{F (i, j, p), 1 + F (i -~ip+1, j - ~jp+1, p)} where ~ip+1 = (r¯p+1, v¯pU+C1B) and ~jp+1 = (v¯pL+CB1, ¯p+1).

(27)

For all p  [N ], we also keep track of a set associated with the value F (i, j, p). The following lemma provides guarantees on the solution returned by the DP.

Lemma 14. For any guess (h, g)  2 × 2 , if there exists a feasible S such that (24) is satisfied, then then the DP returns a subset S~ of cardinality at most K such that:

r~iviLCB  h1(1 - 2)
iS
iviUCB  h2(1 + 2)
iS

and and

1 + viLCB  g1(1 + 2)
iS
1 + viUCB  g2(1 - 2)
iS

The proof of Lemma 14 results immediately from the proof of Lemma 1 in D´esir et al. (2014).

Building upon Lemma 14, Algorithm 2 describes the procedure which allows us to approximately solve the

MNL-diff-Assort problem.

32
ALGORITHM 2: Procedure for the MNL-diff-Assort problem for (h, g)  2 × 2 do
Compute discretization coefficients using (25) ; Compute F (i, j, p) for all (i, j, p)  [I]2 × [J ]2 × [N ] using (27) ; if F (i, j, N )  K then
Let S(h,g) be a corresponding subset. end end return S that maximizes the expected revenue over S(h,g)|(h, g)  2 × 2

We are now ready to prove Proposition 1. Let S be an optimal solution to the MNL-diff-Assort problem and let k1, k2, m1, m2 such that:

ru(1 + )k1 

r~iviLCB  ru(1 + )k1+1 and u(1 + )k2  1 +

viLCB  u(1 + )k2+1

iS 

iS 

ru(1 + )m1 

iviUCB  ru(1 + )m1+1 and u(1 + )m2  1 +

viUCB  u(1 + )m2+1

iS 

iS 

Lemma 14 guarantees that for the guess h = (ru(1 + )k1, ru(1 + )m1 ), g = (u(1 + )k2 , u(1 + )m2 ), the DP

returns a set S which satisfies:

r~iviLCB  ru(1 + )k1 (1 - 2) and 1 + viLCB  u(1 + )m1(1 + 2)

iS

iS

iviUCB  ru(1 + )k2(1 + 2) and 1 + viUCB  u(1 + )m2(1 - 2)

iS

iS

Hence, the revenue of S satisfies:

(28)

R(S) 

1 - 2 1 + 2

iS r~iviLCB

1+

vLCB
iS i

-

1 + 2 1 - 2

iS iviUCB

1+

vUCB
iS i

 (1 - 4)R(S) - 16 iUCB(i, S)
iS 

This concludes the proof of Proposition 1.

B. Proofs of technical lemmas

B.1. Properties of the estimates vLCB, vUCB We start by stating these essential concentration results, which were first proven in Agrawal et al. (2017a)

(Corollary D.1):

33

Lemma 15 (From Agrawal et al. (2017a)). Consider n i.i.d geometric random variables X1, . . . , Xn

with

distribution

P(Xi = m) = (1 - p)mp, m = {0, 1, . . .}.

Let

µ = E(Xi) =

1-p p

and

X~ =

. n
i=1

Xi

n

For

µ  1,

we have:

1. P |X~ - µ| >

X~ + 48

 log( N

4

+1)

n

48

 log( N

2

+1)

n



6 N 8

2. P |X~ - µ| >

µ +  24 log( N4+1) n

48

 log( N

4

+1))

n



4 N 8

3. P

X~



3µ 2

+

48

 log( N

4

+1)

n



3 N 8

From Lemma 15, we can derive the following concentration results on our estimators:

Corollary 2. For each i, epoch , we have the following:

1. P |v¯i, - vi| >

v¯ + 48

 log( N

4

+1)

i,

Ti ()

48

 log( N

4

+1)

Ti ()



6 N 8

2. P |v¯i, - vi| >

v + 24

 log( N

4

+1)

i

Ti ()

48

 log( N

4

+1)

Ti ()

3. P

v¯i,

>

3vi 2

+

48

 log( N

4

+1)

Ti ()



3 N 8



4 N 8

Proof. The result follows from Lemma 2, and Lemma 15. To address the randomness of Ti() we use the law of total probability conditionally on Ti()nd apply the lemmas for each possible value of the sequence length in []since Ti()  .
Q.E.D.

We are now ready to prove Lemma 3.

Proof. The proof globally follows the proof of Lemma 4.1 in Agrawal et al. (2017a). By Assumption 1,

we already know that vi  [0, 1] for all i. From Corollary 2, we obtain that for each i:





|vi - v¯i,| 

v¯i,

48

log( N 4 Ti()

+

1)

+

48

log( N 4 Ti()

+

1)

w.p.



1

-

6 N 8

.

Hence,

for

each

i



[N ], 



[L],

vi



[viL,CB, viU,C B]

with

probability

at

least

1

-

6 N 8

.

Moreover,

max

|viU,C B - vi|, |viL,CB - vi|

 max |viU,C B - v¯i,|, |viL,CB - v¯i,| + |v¯i, - vi|





=

v¯i,

48

log( N 4 Ti()

+

1)

+

48

log( N 4 Ti()

+

1)

+

|v¯i,

-

vi|

(29)

We can bound each term adequately. For the first two terms of (29), we use the third inequality in Corollary

2,

and

combine

it

with

the

inequality

a

+

 b



 a

+

b:

34









P

48v¯i, log(

N 4 T()

+

1)

+

48

log( N 4 T()

+

1)

>

72vi

log( N 4 Ti()

+

1)

+

96

log( N 4 Ti()

+

1)



3 N 8

The last term of (29) can be bounded by using the second inequality in 2. Thus, by doing a union bound, 
and by choosing C1 = 72 + 24 and C2 = 144, we get:

P(ACi,B )



1

-

3

+4+ N 8

6

=

1

-

13 N 8

Finally, by using another union bound and the fact that T  L  , we obtain:

P (ACB )



1

-

N (L

-

)

13 N 80



1

-

T

-  80

which completes the proof of Lemma 3.

Q.E.D.

We also provide the following useful inequalities.

Lemma 16. Conditionally on ACB, the following inequalities hold:

max

L

L



½iS (viU,C B - vi),

½iS (vi - viL,CB)  C log( N T 4 + 1)2 1 +



=

=



L

L





max 

½iS (vi - viL,CB),

½iS (viU,C B - vi)  C log( N T 4 + 1)2

= i[N]

= i[N]

N+

Tivi KN T vmax

(30) (31)

L



½i,jS (viU,C B - vj )  C log( N T 4 + 1)2 N 2 + K

= i,j[N]

KN T vmax

(32)

where C = 2 max(C1, C2).

Proof. By using:     L  T , we get:

L
½iS
=



vi

log( N 4 Ti()

+

1)



L
½iS
=

 log( N 4
Ti()



L

vi log( N T 4 + 1) ½iS

=

+ 1)



L

 log( N T 4 + 1)

=

1 Ti()
½iS

(a)
2
1 Ti()

 vi log( N T 4 + 1)Ti

(b)


 log( N

T

4

+

1)2

where Ti = Ti(L). Inequalities (a) and (b) come respectively from:

L
½iS
=

1

Ti


1

2

Ti()

j
j=1

Ti

,

L =

½iS

1 Ti()



Ti j=1

1 j



log Ti



 log( N T 4

+

1)

35

Thus, conditioning on ACB, we can apply the bound in Lemma 3, combined with the inequality Ti  T for each i  [N ], to derive:

L



½S (vi - viL,CB)  C log( N T 4 + 1)2 + C

=

 Tivi log( N T 4 + 1)

The proof of the upper bound on

L =

½iS (viU,C B

-

vi)

is

identical.

To prove inequality (31), notice that:

N



C log( N T 4 + 1)2 + C

i=1

 Tivi log( N T 4 + 1)

  C log( N T 4 + 1)2

N + vmaxN

N
Ti

i=1

  C log( N T 4 + 1)2 N + KN T vmax

where the first inequality holds from Cauchy-Shwartz, and the second inequality holds from:

N

NL

LN

L

L

Ti =

½ = iS

½ = iS |S|  K = KL  KT

i=1

i=1 =1

=1 i=1

=1

=1

Inequality (32) follows from Lemma 3 and the next two bounds.

½ ½ L
= i,j[N]

i,jS

 log( N T 4
Ti()

+

1)



L i[N] =

j [N ]

 log( N T 4 + 1)

jS

Ti()

  N 2 log( N T 4 + 1)2

½i,jS
= i,j[N]

vj

 log( N T 4
Tj ()

+

1)



 vmax

 log( N T 4

+

L
1)
=

|S|

N j=1

½jS
Tj ()



vmax

 log( N

T

4

+

1)K

N

L

j=1 =

½jS
Tj ()

  2 log( N T 4 + 1)2K KN T vmax

Q.E.D.

B.2. Properties of LCB, U CBor   

For each   , S,  S and i  S, we have, conditionally on ACB:

1+

vUCB i,-1 vLCB jS j,-1



1+

vi jS vj



1+

vLCB i,-1 vUCB jS i,-1

which is equivalent to:

U CB(i, S)  (i, S)  LCB(i, S)

Proof of Lemma 7.

36 Proof. For all i  [N ],   , we start by bounding (i, S) - L+CB1(i, S) as follows.

(i, S) - L+CB1(i, S) = 1 +

vi S vj

- 1+

viL,CB vUCB
jS j,

= viU,C B 1 +

1
jS

vjU,C B

-

1

+

1 jS vj

+

1

vi +

- viL,CB vUCB
jS j,



V

1 (S)

+

1

(i, S)

(vjU,CB - vj + (vi - viL,CB)

jS

where the last inequality comes from 1 + jS vjU,CB  1 + V (S).

(33)

Therefore, by applying the bound obtained in Corollary 16 for each    and by using (i, S)  1, for all

S, i, we have, conditionally on ACB:

L =

V (S) + 1

L

(i, S) - LCB(i, S) 

½iS vi - (viL,CB) +

(vjU,CB - vj )

=

jS

L

NL



½iS (vi - viL,CB) +

½jS (vjU,CB - vj )

=

j=1 =

  2C log( N T 4 + 1)2 N + KN T vmax

(34)





where the last inequality follows from the inequalities in Lemma 16, and N + 1  2N, N + 1  2 N .

Q.E.D.

Proof of Lemma 11.

Proof. Similarly as in the proof of Lemma 7:

U+CB1(i, S) - (i, S) = 1 +

viU,C B
jS

vjL,CB

-

1+

vi S vj

= viU,C B 1 +

1
jS

vjL,CB

-

1+

1 jS vj

+

1

viU,C B +

- vi jS vj



V

1 (S)

+

1

U CB(i, S)

(vj - vjL,CB + (viU,C B - vi)

jS

Hence by taking the expectation conditionally on ACB,

T

N

E

ri UtCB(i, St ) - (i, St )

t=t i=1

LN

 rmaxE

V (S) + 1 UtCB(i, St) - (i, St )

= i=1

NL

NL

 rmaxE

½iS (vi - viL,CB) +

½jS (vjU,CB - vj )

i=1 =

j=1 =

37

  2rmaxC log( N T 4 + 1)2 N + KN T vmax + N 2 + K
  4rmaxC log( N T 4 + 1)2 N 2 + K KN T vmax

KN T vmax

where we used once more inequalities from Lemma 16. Q.E.D.

Proof of Lemmas 4 and 5. The proofs of these lemmas follow immediately from inequalities (33) and (34) in the proof of Lemmas 7 and 11. B.3. Azuma-Hoeffding inequalities We introduce the following well-known concentration theorems:
Theorem 2 (Azuma-Hoeffding). Consider random variables X1, . . . , Xn, defined with respect to a filtration Fn and a stopping time   n a.s, and that (Xk) is uniformly bounded by X. Then following inequality holds:

P

n
Xk - E(Xk|Xk-1, . . . , X1) > X

2n log T



1 T

k=

Note that Theorem 2 is usually mentioned with a deterministic  . However, notice that for each realization

of  , one can derive a sharper bound and relax it using 0    n. Since the probability does not depend on

such a realization, a law of total probability gives the result.

Theorem 3. Consider a sequence of random variables X1, . . . , Xn  [0, 1].

Set µ =

n k=1

E(Xk

|X1

,

.

.

.

,

Xk-1

)

and

X¯

=

n k=1

Xi.

There

exists

a

universal

constant

0

>0

for

which

for all  > 0, the following inequality holds:

P |X¯ - µ| > 3 µn + 3  exp(-/0)

The proof of Theorem 3 lies for instance in Babaioff et al. (2015), and essentially combines optional sampling arguments with sharp Azuma-Hoeffding bounds.

Proof of Lemma 6.

Proof. We apply Theorem 2 on the variables ½( ct)tt , each conditioned on the information prior to time

t, and where we use: ½ ½ E( ct | ) = ct-1,...,c1

T t=t

SS yt(S)(i, S). This gives:

38

P

T
½ct=i -

yt (S)(i, St ) > 3

vmaxT log T + 3 log T



(1) T

t=1

SS

where the upper bound comes from

T t=t

S yt (S)(i, S)  vmax  vmax  1 and T - t < T .

We combine this inequality with the following application of Theorem 2:

P

T
(i, St ) -

yt (S)(i, S) >

2vmaxT log T



(1) T

In

a

similar

fashion,

and

t=t
by using

that

SS
conditionally

on

ACB, viL,CB



vi

 vmax



vmax



1,

we

have

the

bound:

T

P

LCB i,t

(i,

St

)

-

yt (S)Li,CBt (i, S) >

t=t

SS

By the union bound property, this ends the proof.

2vmaxT log T



(1) T

Q.E.D.

Proof of Lemma 10.

Proof. Note that r/rmax  [0, 1], T - t  T , and that

T

T

N

T

yt (S)R(S) 

yt (S)rmax ½iSt (i, S) 

rmaxvmaxK  T rmaxvmaxK

t=t SS

t=t SS

i=1

t=t

.

This allows us to apply the same technique as above and to do the following decomposition:

T

N

rct - ri(i, St )

t=t

i=1

T

T

= rct -

yt (S)R(S)

t=t

t=t SS

(> 3rmax

KvmaxT log T + 3 log T

w.p



(1) T

from Th.

3)

T

+

yt(S)(R(S) - R(S))

t=t SS

(> Krmax

2vmaxT log T

w.p



(1) T

from Th.

2)

> 6Krmax vmaxT log T + 3rmax log T

w.p



(1) T

where we use in the last inequality vmax  vmax  1 and a union bound.

Finally, applying Theorem 2 with the bounds 0  UCB(i, S)  ½iSvmax, |S|  K gives:

T

N

P

riyt (S)

UCB i,t

(i,

St

)

-

UtCB

(i,

S

)

t=t SS i=1

The proof concludes by doing a union bound.

> Krmax

2vmaxT log T



(1) T

39

Q.E.D. B.4. Multiplicative Chernoff bounds for Epochs We start by introducing the following theorem, proved by Janson in Janson (2018) (Theorem 2.3), which provides tail bounds for the sum of independent geometric variables:

Theorem 4 (Janson, 2018). Let n be a positive integer. For any x0  x1, . . . , xn  1,   1, and

X1, . . . , Xn independent random variables with geometric distributions such that E(Xk) = xk for each k =

1, . . . , n, the following inequality holds:

P

n

n

Xk   xk

k=1

k=1

 -1

1- 1 x0

(-1-log )

n k=1

xk

Corollary 3. With the same notations as in 4, the following inequality holds for all T  n:

Proof.

P

n
(Xk - xk) > x0

2x0T log T



1 T

k=1



Set rT =

2x0 T n

log T

.

Then

by

applying

4

and

using

nx0



T t=1

xt



n

we

obtain:

n

P

(Xt - xt) > x0

k=1

2x0T log T

n

n

P

Xt  1 + rT

xt

k=1

k=1



1

1 + rT

1

-

1 x0

(rT -log(1+rT ))

2

1

-

1 x0

x0T log T n n2

T t=1

xt



2 T

T
xt  n
t=1
(n  T )

Q.E.D.

Proof of Lemma 8.

Proof. Consider the -field F offer := (L, {S}=1,...,L}). |E| has a geometric distribution of mean V (S) +

1 for each   [L] conditionally on S. Moreover, we also have the following uniform upper bound on the

means: maxSS V (S) + 1  K + 1, where K is maximal number of items per offered assortment. Since the

realization set of (L, S1, . . . , SL) is finite (denote it ), we can create for each realization independent copies

for each |E|, which allows us to apply 3 the following way:

L

1 - P(Aepochs1) = P

|E| - (V (S) + 1) > 2(K + 1) (K + 1)T log T

=1

40

L

= P

(|E| - (V (S) + 1)) > 2(K + 1) (K + 1)T log T (L, S1, . . . , SL) = 



=1

× P( = (L, S1, . . . , SL))



2 T

P

(

=

(L,

S1,

.

.

.

,

SL))



=

2 T

Thus

P (Aepochs1 )



1-

2 T

.

Q.E.D.

Proof of Lemma 9. Proof. The proof follows the same line as the proof of Lemma 8 and results from a direct application of
Corollary 3 to the geometric variables {|E|} (up to a minor constant modification which allows to change the exponent in the bound):

Q.E.D.

P(t0 - K  (K + 1) 16(K + 1) log )



P

|E| - (V (S) + 1)  (K + 1) 16(K + 1) log 

=1



2 8

B.5. Proofs of the auxiliary lemmas

Proof of Lemma 1.

Proof. Consider any non-anticipative algorithm A, and {St, ct}t=1...T be the assortments offered by A

and the customer's purchasing decisions. Note that these might be randomized and St might depend on

S1, c1, . . . , St-1, ct-1.

Set

yS

=

1 T

T t=1

P(St

=

S),

for

each

S



S.

Clearly,

y



0.

We

also

have:

yS

=

1 T

T

P(St = S) = 1

SS

t=1 SS

Moreover, since the inventory constraints are respected by A, the following inequalities hold for each product

i  [N ]: Thus, they hold in expectation:

T
½ct=i  qi
t=1

qi  EA

T
½ct
t=1

T

T

T

= EA EA(½ct=i|St) = EA p(i, St) =

ySp(i, S)

t=1

t=1

t=1 SS

41 Hence y is feasible in (OPT-LP). Thus, Its associated objective is less than OP T /T , the optimal value of (OPT-LP):

However,

OP T /T  ySR(S)
S
SS ySR(S) is exactly the revenue generated by A, since:

T

T

T

EA

rct = EA EA(rct |St) = EA R(St) = T

1 T

T

P(St = S)

R(S)

t=1

t=1

t=1

SS

t=1

This concludes the proof.

Q.E.D.

Proof of Lemma 12.
Set y~ = (1 - )yOPT + ½S=. It is clear that y~ is still a distribution. Moreover, for each product i:

y~(S)LCB(i, S)  (1 - ) yOPT(S)(i, S)  (1 - )qi (since Li,CB(i, S)  (i, S))

SS

SS

Thus y~ is feasible for (P). Hence by optimality of yOPT in (P):

N
yOPT(S) riUi,CB(i, S) 

N
y~(S) riUi,CB(i, S)

SS

i=1

SS

i=1

N
 (1 - ) yOPT(S) ri(i, S)

S

i=1

(since (i, S)  Ui,CB(i, S))

= (1 - )OP T /T

