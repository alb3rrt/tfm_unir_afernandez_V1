Cross-Domain First Person Audio-Visual Action Recognition through Relative Norm Alignment

Mirco Planamente*,1,2 Chiara Plizzari*,1 Emanuele Alberti*,1 Barbara Caputo1,2

1 Politecnico di Torino
name.surname@polito.it

2 Istituto Italiano di Tecnologia
name.surname@iit.it

arXiv:2106.01689v1 [cs.CV] 3 Jun 2021

Abstract
First person action recognition is an increasingly researched topic because of the growing popularity of wearable cameras. This is bringing to light cross-domain issues that are yet to be addressed in this context. Indeed, the information extracted from learned representations suffers from an intrinsic environmental bias. This strongly affects the ability to generalize to unseen scenarios, limiting the application of current methods in real settings where trimmed labeled data are not available during training. In this work, we propose to leverage over the intrinsic complementary nature of audio-visual signals to learn a representation that works well on data seen during training, while being able to generalize across different domains. To this end, we introduce an audio-visual loss that aligns the contributions from the two modalities by acting on the magnitude of their feature norm representations. This new loss, plugged into a minimal multi-modal action recognition architecture, leads to strong results in cross domain first person action recognition, as demonstrated by extensive experiments on the popular EPIC-Kitchens dataset.
1. Introduction
First Person Action Recognition is rapidly attracting the interest of the research community [51, 49, 18, 27, 21, 59], both for the significant challenges it presents and for its central role in real-world egocentric vision applications, from wearable sport cameras to human-robot interaction or human assistance. The recent release of the EPIC-Kitchen large-scale dataset [14] has given a very significant boost to the research activities in this field, offering the possibility to study people's daily actions from a unique point of
*The authors equally contributed to this work. This paper is partially supported by the ERC project RoboExNovo. Computational resources were partially provided by IIT.

Peter's home
Joy's home
Tom's home
Figure 1. Egocentric action recognition comes with a rich sound representation, due to the frequent hand-object interactions and the closeness of the sensor to the sound source. Here we show that the complementary nature of visual and audio information can be exploited to deal with the cross-domain challenge.
view. The collection of this dataset consists in the segmentation of long untrimmed videos representing people's daily activities recorded in the same kitchen. This process results in a huge number of sample clips representing a large variety of action classes, which are however captured in a limited number of environments. This intrinsic unbalance causes the so called environmental bias, meaning that the learned action representations are strictly dependent on the surroundings, and thus hardly able to generalize to videos recorded in different conditions [54]. In general, this problem is referred to in the literature as domain shift, meaning that a model trained on a source labelled dataset cannot generalize well on unseen data, called target. Recently, [41] addressed this issue by reducing the problem to an unsupervised domain adaptation (UDA) setting, where an unlabeled set of trimmed samples from the target is available during training. However, the UDA setting is not always realistic, because the target domain might not be known a priori or

1

because it might be costly (or plainly impossible) to access target data at training time.
In this paper we argue that the true challenge is to learn a representation able to generalize to any unseen domain, regardless of the possibility to access target data at training time. This means developing a method general enough to work both on UDA and Domain Generalization (DG) [40]. Inspired by the idea of exploiting the multi-modal nature of videos as done in [41], we propose a new cross-domain generalization method which leverages over the complementary nature of visual and audio information. We start by observing that first person action recognition intrinsically comes with rich sound information, due to the strong handobject interactions and the closeness of the sensors to the sound source. The use of auditory information could be a good workaround for the problems which arise from the use of wearable devices, in that it is not sensitive to the egomotion and it is not limited by the field of view of the camera. Moreover, our idea is that, since the audio and visual modalities come from different sources, the domain-shift they suffer from is not of the same nature. Motivated by these considerations, we propose a new cross-modal loss function, which we call Relative Norm Alignment loss, that operates on the relative features norm of the two modalities by acting on their magnitude. Our loss improves the cooperation between the audio and visual channels, which results in a stronger ability to overcome the domain shift. We show with extensive experiments that, when used in a very simple audio-visual architecture, our loss leads to strong results both in UDA and DG settings.
To summarize, our contributions are the following:
· we empirically bring to light a problem related to the heterogenous nature of audio and visual modalities, which causes an unbalance preventing the two modalities to correctly cooperate;
· we propose a new cross-modal audio-visual Relative Norm Alignment loss by progressively aligning the relative feature norms of the two modalities;
· we present a new benchmark for both single-source and multi-source DG settings in first person videos, which, to the best of our knowledge, no prior work has explored yet;
· we validate the effectiveness of our method on both DG and UDA scenarios, achieving competitive results compared to previous works.
2. Related Works
First Person Action Recognition. Until now, research has been focused on data provided by a specific view of the camera (often fixed), i.e., third person view [46, 57, 7]. With

the recent release of a large-scale dataset of first-person actions [14], the community has also become interested in working on videos that are recorded from an egocentric point of view. Since egocentric action recognition suffers from the motion of the camera and sudden changes of view, the main approaches proposed so far are based on multistream architectures [7, 46, 39, 34, 8, 27, 38], many of which are inherited from the third-person action recognition literature. The networks used to extract spatial-temporal information from egocentric videos can be divided into two main groups. The first exploits Long Short-Term Memory and variants [50, 51, 49, 18] to generate an embedding representation based on the temporal relations between the features frames. The second [47, 55, 59, 26] leverages 3D convolutional kernels which jointly generate spatial-temporal features by sliding along the spatial and temporal dimensions. Recent works exploit an attention mechanism at frame or clip level [51, 49, 43, 37, 38] to re-weight the spatial or temporal features, obtaining remarkable results. By observing the importance of multi-stream approaches in this context, they [48, 58, 59, 64] investigate alternative methods to fuse streams w.r.t. the standard late fusion approach, creating a more compact multi-modal representation. Although optical flow has proven to be a strong asset for the action recognition task, it is computationally expensive. As shown in [13], the use of optical flow limits the application of several methods in online scenarios, pushing the community either to investigate alternative paths [27, 8] or towards single-stream architectures [63, 13, 29, 52, 44].
Audio-Visual Learning. A wide literature exploits the natural correlation between audio and visual signals to learn cross-modal representations that can be transferred well to a series of downstream tasks, such as third person activity recognition. Most of these representation learning methods use a self-supervised learning approach, consisting in training the network to solve a synchronization task [2, 42, 28, 3, 1, 4], i.e., to predict whether the audio and visual signals are temporally aligned or not. By solving this pretext task, the network is induced to find a correspondence between audio and visual cues, making the resulting representations perfect for tasks like sound-source localization [3, 1, 62], active speaker detection [12, 1], and multispeaker source separation [42, 1]. Audio has also been used as a preview for video skimming, due to its lightweight characteristics [20]. More recently, it proved to be useful even in egocentric action recognition [27, 8]. However, the role of this information in a cross-domain context is still unexplored. In this work, we investigate the importance of audio when used together with visual information in learning a robust representation on unseen data.
Unsupervised Domain Adaptation (UDA). The goal of UDA is to bridge the domain gap between a labeled source

2

domain and an unlabeled target one. We can divide UDA approaches in discrepancy-based methods, which explicitly minimize a distance metric among source and target distributions [60, 45], e.g., the maximum mean discrepancy (MMD) in [36], and adversarial-based methods [16, 53], often leveraging a gradient reversal layer (GRL) [19]. Other works exploit batch normalization layers to normalize source and target statistics [32, 33, 9]. Still, another approach is the generative-based one, which operates by performing style-transfer directly on input data [22, 23]. The approaches described above have been designed for standard image classification tasks. Only few works analyzed UDA for video understanding [10, 41, 11, 25]. [10] focuses on aligning temporal relation features to increase robustness across domains. In [11], the network is trained to solve an auxiliary self-supervised task on source and target data. Recently [41] proposed an UDA method for first person finegrained action recognition, called MM-SADA, combining a multi-modal self-supervised pretext task with an adversarial training.
Domain Generalization (DG). The DG setting is closer to real-world conditions, in that it addresses the problem of learning a model able to generalize well using inputs from multiple distributions, when no target data is available at all. Previous approaches in DG are mostly designed for image data [6, 56, 30, 17, 31, 5] and are divided in feature-based and data-based methods. The former focus on extracting invariant information which are shared across-domains [30, 31], while the latter exploits dataaugmentation strategies to augment source data with adversarial samples and possibly get closer to the target [56]. Interestingly, using a self-supervised pretext task is an efficient solution to the extraction of a more robust data representation [6, 5]. We are not aware of previous works on first or third person DG. Among unpublished works, we found only one arXiv paper [61], in third person action recognition, designed for single modality. Under this setting, first person action recognition models, and action recognition networks in general, degenerate in performance due to the strong divergence between source and target distributions. Our work stands in this DG framework, and proposes a feature-level solution to this problem in first person action recognition by leveraging the natural audio-visual correlation.
3. Relative Norm Alignment
3.1. Problem Statement
Given one or more source domains {S1, ..., Sk}, where each S = {(xsi , yis)}Ni=s1 is composed of Ns source samples with label space Y s known, our goal is to learn a model representation able to perform well on a target do-

main T = {xti}Ni=t1 of Nt target samples whose label space Y t is unknown. Our two main assumptions are that the distributions of all the domains are different, i.e. Ds,i = Dt  Ds,i = Ds,j, with i = j, i, j = 1, ..., k, and that the label space is shared, Cs,i = Ct, i = 1, ..., k. In this work we consider two different scenarios:
Domain Generalization (DG), where at training time the model can access one or more fully labeled source datasets S1, ..., Sm, but no information is available about the target domain T .
Unsupervised Domain Adaptation (UDA), where at training time it is possible to access a set of unlabeled target samples belonging to the target domain T , jointly with one fully labeled source domain S.
For both scenarios, the ultimate goal is to learn a classifier able to generalize well on the target data.
Multi-Modal Approach. Our goal is to investigate how using multi-modal signals from source and target data affects the ability of a first-person action classification net to generalize across domains. Specifically, given a multi-modal input X = (X1, ..., XM ), where Xm = (xm 1 , ...xm Nm ) is the set of all Nm samples of the m-th modality, we use a separate feature extractor F m for Xm, and we employ all the fm = F m(xm i ) corresponding features, encoding information from multiple channels, during the learning process. We denote with h(xm i ) = ( · 2  fm)(xm i ) the L2-norm of the features fm.

3.2. Cross-Modal Audio-Visual Alignment

Let us consider a multi-modal framework characterized
by M = 2 modalities, specifically RGB clips and audio signals. We indicate with fv = F v(xvi ) and fa = F a(xai ) the features encoding the visual and audio information, re-
spectively (details about the feature extractor modules are
given in Section 4). The discrepancy between their norms, i.e., h(xvi ) and h(xai ), is measured by a mean-feature-norm distance term , defined as:

(h(xvi ), h(xai ))

=

1 N

h(xvi ) -
xvi X v

1 N

h(xai ),
xai X a

(1)

where N = |X v| = |X a| denotes the number of the sam-

ples for each modality. Figure 2 illustrates the feature norms

of the two modalities and the  between the two.

It has been shown in the literature that aligning audio and visual information by solving synchronization tasks [2, 42, 28, 3, 1] leads to representations that facilitate a number of audio-visual downstream tasks, including action recognition. Such approaches enforce feature alignment by means of Euclidean or similarity-based losses, whose objective is to embed audio and visual inputs into a shared representa-

3

Figure 2. Relative Norm Alignment. The norm h(xvi ) of the i-th visual sample (top-left) and h(xai ) of the i-th audio sample (topright) are represented, by means of segments of different lengths.
The radius of the two circumferences represents the mean feature norm of the two modalities, and  their discrepancy. By minimizing , audio and visual feature norms are induced to be the same,
as shown at the bottom.

tion space.

Our intuition is that the optimization of these loss func-

tions could, to some extent, limit action recognition net-

works when dealing with cross-modal scenarios. This is

because, as opposed to acting on the magnitude of au-

dio and visual norms, these losses mainly use the angu-

lar distance  between the two embeddings, defined as



=

arccos(

fv ·fa fv fa

).

By acting only on the normal-

ized feature vectors, they are indeed capable of aligning the

two representations (Figure 3-b) but they struggle to exploit

the modality-specific characteristics of the two streams. In

other words, when using an angular loss we impose the prior

that what is significant for the visual stream is significant

also for the audio stream, but this might not be true in prac-

tice, especially when training and test data come from dif-

ferent distributions. For instance, in a clip where the action

take does not produce any sound, the information brought

by the audio will be referred to only as background noise.

Conversely, for the same action carried out with another ob-

ject or in a different setting, the aural information might be

highly informative, possibly even more than the visual one.

We show below with a set of experiments (Section 5, Figure

7) that a large , i.e., a misalignment at feature-norm level,

negatively affects the learning process by causing an unbal-

ance between the contributions brought by each modality,

which therefore degrades the classification performance.

3.3. Relative Norm Alignment Loss
Motivated by the considerations above, we propose a new cross-modal loss function, which aims to reduce the

a)

b)

c)

Figure 3. Feature Distribution Adjustment. (a) shows the distribution of visual and audio features. We see that, without any form of alignment, audio features are predominant over visual ones, which could ultimately lead to a loss of information. By minimizing LRNA, two possible scenarios can occur, displayed in (b) and (c). In both, the range where the feature norms vary is the same, making the informative content of the two distributions comparable with each other. This lets the loss learn from data when it is more convenient to align them (b) or when it is better to preserve their peculiarities (c).

discrepaa)ncy between fbe)ature distributico) ns by aligning their contribution during training. As opposed to losses acting on the normalized feature vectors, our loss operates on their magnitude, which intuitively results in more freedom to preserve the modality-specific features (see Figures 3-b-c). We expect this to be important in cross-domain scenarios. Considering the dot product < fv, fa >, defined as

< fv, fa >= fv 2 fa 2 cos ,

(2)



our approach involves the first two terms of Equation 2, im-

posing a relative alignment between them.

Our relative norm alignment (RNA) loss function is defined

as

1 LRNA = N

xvi X v xai X a

h(xvi ) h(xai )



1,

(3)

where h(xv) = fv 2 and h(xa) = fa 2 indicate the

norm of visual and audio features respectively. This divi-

dend/divisor structure is used to encourage a relative adjust-

ment between the norm of the two modalities, enhancing an

optimal equilibrium between the two embeddings. When

minimizing LRNA , the network can either increase the divisor (fv ) or decrease the dividend (fa ), leading to

three potential main benefits:

1. Since fv 2 and fa 2 tend to the same value, features norm ranges are encouraged to be comparable, preventing one modality to drown out the other, improving the final prediction (Figures 3-b and 3-c).

2. By reducing the norm of the features while learning the feature extractor, the latter is free to choose which are the less/more discriminative ones, and lower/rise their norm accordingly, increasing the generalization ability of the model.

3. Comparing to standard similarity losses, by not constraining the angular distance  between the two

4

modality representations, feature distributions have the freedom to arrange in non-overlapping configurations (Figure 3-c).
The effects observable at feature-level of the application of our LRNA are represented in Figure 3. In Figure 3-a we represent the features distribution learned with standard cross-entropy loss. As it can be seen, the feature norms of the two modalities differ by a , meaning that the respective features lie within different ranges which make the two representations hard to compare. The solution proposed by our LRNA corresponds to 1). In Figures 3-b and 3-c we show the feature representations obtained by minimizing our loss function. The situation depicted in Figure 3-b occurs when audio and visual information "agree" by means of their modality-specific features. The scenario depicted in Figure 3-c is the most interesting, since it represents a situation which is not compatible with the aim of standard similarity losses. As stated in 3), our LRNA ensures that the modality-specific features are preserved, allowing the final classifier to exploit their complementarity.
4. Cross-Domain Audio-Visual RNA-Net
This section shows how LRNA can be effectively used in a very simple audio-visual deep network for cross-domain first person action recognition. The network, shown in Figure 4, inputs audio and visual information in two separate branches. After a separate convolution-based feature extraction step, the LRNA loss learns how to combine the two modalities, leading to a significant generalization acrossdomains, in both the domain generalization (DG) and unsupervised domain adaptation (UDA) settings. Below we describe in more details how the net works in both settings.
4.1. AV-RNA-Net for Domain Generalization
In a cross-modal context, the input comes from one or more source domains Sk = (Sv, Sa). Under the DG setting, the target is not available during training (see Section 3). As shown in Figure 4, each input modality is fed to a separate feature extractor, F v and F a respectively. The resulting features fv = F v(xvi ) and fa = F a(xai ) are then passed to separate classifiers Gv and Ga, whose outputs correspond to distinct score predictions (one for each modality). The two are combined with a late fusion approach and used to obtain a final prediction P (x) (please refer to Section 5 for more details). Our loss LRNA operates at feature-level before the final classification, acting as a bridge between the two modalities and encouraging a balance between the respective contributions.
Why should LRNA help to generalize? Our loss LRNA rises the generalization ability of the network for two main reasons. 1) By self-reweighting the two modalities contri-

"wash"
Figure 4. RNA-Net. Labeled source visual xvs,i and source audio xas,i inputs are fed to the respective feature extractors F v and F a. Unlabeled target data of any modality (xm t,i) is seen at training time only in UDA setting, and not in DG. Our LRNA operates at feature-level by balancing the relative feature norms of the two modalities. The action classifiers Gv and Ga are trained with standard cross-entropy loss Lce. At inference time, multi-modal target data is used for classification.
bution during training, the classifier has the chance to rank such contributions according to their real relevance, thus avoiding to be fooled by the natural unbalance due to their intrinsic heterogeneity. This is helpful especially in a multisource setting, as it ensures uniformity not only across modalities, but also across data from different sources. 2) By undirectly minimizing the norm of the feature activations, our loss sets a limit to the learner feature encoding, and thus it forces it to only learn the discriminative information. As a consequence, the classifier can learn to ignore information which is strictly domain-specific, distilling the most useful and transferable features.
4.2. Extension to Unsupervised Domain Adaptation
Under this setting, both labelled source data from a single source domain S = (Sv, Sa), and unlabelled target data T = (T v, T a) are available during training. Figure 4 shows the flow of source and target data, indicating with different colors source visual data (green), source audio data (blue) and target data (orange). We denote with xs,i = (xvs,i, xas,i) and xt,i = (xvt,i, xat,i) the i-th source and target samples respectively. As it can be seen from Figure 4, both xm s,i and xm t,i are fed to the feature extractor F m of the m-th specific modality, shared between source and target, obtaining respectively the features fs = (fsv, fsa) and ft = (ftv, fta). In

5

order to consider the contribution of both source and target

data during training, we redefine our LRNA under the UDA

setting as

LRNA = LsRNA + LtRNA,

(4)

LsRN A

=

1 N

xvs,i Xsv xas,i Xsa

h(xvs,i) h(xas,i)



1,

(5)

LtRN A

=

1 N

xvt,iXtv h(xvt,i) xat,iXta h(xat,i)



1,

(6)

By minimizing LsRNA we benefit from the considerations described in Section 4.1. Also, by minimizing LtRNA, and thus learning the reweighting between the modalities on the

unlabelled data, the encoded features contain useful infor-

mation which directly enable us to adapt to the target.

A problem that often occurs with UDA methods is that forcing an alignment between source and target features increases the risk of affecting the discriminative charateristics of the two, and thus destroying the inter-class separability [35]. In our UDA setting, by operating on the two domains through separate LsRNA and LtRNA, we mitigate this risk, preserving the discriminative structure of the two.

5. Experiments
5.1. Experimental Setting
Dataset. Using the EPIC-Kitchens-55 dataset [14], we adopted the same experimental protocol of [41], where the three kitchens with the largest amount of labeled samples are handpicked from the 32 available. We refer to them here as D1, D2, and D3 respectively. Since the action classification task is complicated by the large number of action labels, we consider only a subset, namely: put, take, open, close, wash, cut, mix, and pour. The challenges are not only due to the large domain shift among different kitchens, but also to the unbalance of the class distribution intra- and inter-domain, as shown in [41].
Input. Regarding the RGB input, a set of 16 frames, referred to as segment, is randomly sampled during training, while at test time 5 equidistant segments spanning across all clips are fed to the network. At training time, we apply random crops, scale jitters and horizontal flips for data augmentation, while at test time only center crops are applied. Regarding aural information, we follow [27] and convert the audio track into a 256 × 256 matrix representing the logspectrogram of the signal. The audio clip is first extracted from the video, sampled at 24kHz and then the Short-Time Fourier Transform (STFT) is calculated of a window length of 10ms, hop size of 5ms and 256 frequency bands.
Implementation Details. Our network is composed of two streams, one for each modality m, with distinct feature ex-

Supervised

57,91

55,21

59,40

Source Only

36,14

36,42

41,87

RGB

Audio RGB + Audio

RGB

Audio RGB + Audio

Figure 5. Single- vs multi-modality accuracy (%) on both super-

vised and source only settings. The drop in performances when

testing on target (right) highlight the presence of a strong domain

shift.

tractor F m and classifier Gm. The RGB stream uses the Inflated 3D ConvNet (I3D) with the same initialization pro-
1
posed by the authors [7], as done in [41]. The audio feature extractor uses the BN-Inception model [24], a 2D ConvNet pretrained on ImageNet [15], which proved to be a reliable backbone for the processing of audio spectrograms [27]. Each F m produces a 1024-dimensional representation fm which is fed to the action classifier Gm, consisting in a fully-connected layer that outputs the score logits for the 8 classes. Then, the two modalities are fused by summing the outputs and the cross entropy loss is used to train the network. To remain coherent with the setup used by [41], we follow their strategy to validate our hyper-parameters. All training models are run for 9000 iterations and finally tested with the average of the last 9 models. For further details on the optimizer, learning rate, parameters used and on the training process in general, we refer to the supplementary material.

5.2. Results

Preliminary Analysis. To verify that combining audio and visual modalities actually improves results, we assess the impact of each modality individually (Figure 5). Firstly, the two streams are trained both separately and jointly in a supervised fashion (referred to as supervised). Then, we validate the same models under a cross-domain setting, meaning that training is performed on source data only, and test on unseen target data (referred to as source-only).
Results (Figure 5, left) highlight that, by using a single domain, the visual part is more robust than the audio one (+2.7%). Conversely, when testing on target data from a different domain (Figure 5, right), audio is on-par with RGB. This suggests that when a domain-shift exists, it is mainly imputable to changes in visual appearance. In the crossdomain scenario, the accuracy drops dramatically (Figure 5, right), proving how the domain shift impacts negatively the performance. Interestingly, we see that the fusion of the two modalities brings a greater contribution when facing this problem, increasing the source-only results on single modality by 4%. This confirms that combining audio and

6

Single-Source
Source Only Align. Only Orth. Only BatchNorm SS [41] RNA-Net (Ours)

D1  D2
39.03 38.50 39.18 40.03 38.86 45.01

D1  D3
39.17 33.75 37.55 39.88 33.75 44.62

D2  D1
35.27 32.59 36.86 36.39 32.59 41.76

D2  D3
47.52 45.78 47.09 48.47 45.78 48.90

D3  D1
40.26 39.97 43.70 42.60 39.97 42.20

D3  D2
49.98 50.86 51.61 48.33 50.86 51.98

Mean
41.87 41.76 42.67 42.62 40.30 45.75

Multi-Source
Deep All Align. Only Orth. Only BatchNorm SS [41] RNA-Net (Ours)

D1, D2  D3
51.47 50.01 53.08 52.07 51.87 55.88

D1, D3  D2
43.19 42.40 41.76 42.63 39.79 45.65

D2, D3  D1
39.35 44.40 48.07 45.14 52.73 51.64

Mean
44.67 45.60 47.64 46.61 48.13 51.06

Table 1. Top-1 Accuracy (%) of our RNA-Net under the single-source DG setting (left) and the multi-source DG setting (right).

UDA
SS Only [41] Adversarial Only [19] MM-SADA [41] MMD [36] AdaBN [32] RNA-Net (Ours) RNA-Net+GRL (Ours)

D1  D2
44.83 41.02 48.90 42.40 36.64 46.89 46.65

D1  D3
42.88 43.04 46.66 43.84 42.57 48.40 49.95

D2  D1
40.61 39.36 39.51 40.87 33.97 41.58 46.06

D2  D3
54.21 49.25 50.89 48.13 46.63 51.77 51.77

D3  D1
42.58 38.77 45.42 41.46 40.51 43.19 42.20

D3  D2
53.50 50.56 55.14 50.03 51.20 54.43 53.14

Mean
46.44 43.67 47.75 44.46 41.92 47.71 48.30

Table 2. Top-1 accuracy (%) of our RNA-Net under the multi-modal UDA setting.

visual cues is useful to partially overcome the weaknesses of each individual modality across domains. A similar exploration on audio and appearance fusion was done by [27].
Baseline Methods. To empirically prove the limitations caused by strictly enforcing an alignment or orthogonality between RGB and audio representations (see Section 3.2), we compare our LRNA with an alignment-based and an orthogonality-based loss respectively, both operating on the features of the two modalities. The first, which we indicate with L , imposes an alignment constraint by minimizing the term 1 - CosineSimilarity(x, y), ideally aiming to the representation in Figure 3-b. The second, which we indicate with L, operates by minimizing the term CosineSimilarity(x, y)2, imposing an orthogonality constraint (Figure 3-c). To demonstrate that mitigating the unbalance between the modality feature norms helps the classifier to better exploit the two modalities, we add a Batch Normalization layer before the Gm classifier, that serves as a regularizer on input features. We adapt all these baseline methods to our backbone architecture, in order to fairly compare them with our RNA-Net. The baseline for singlesource DG is the standard source-only approach, while in a multi-source context we take as baseline the so-called Deep All approach, namely the backbone architecture when no other domain adaptive strategies are exploited and all and only the source domains are fed to the network. Indeed, this is the ultimate validation protocol in image-based DG methods [5]. We also provide as a competitor a selfsupervised approach, inspired by works that proved its robustness across-domains [6]. The choice fell on a multimodal synchronization task [41].
DG Results. Table 1-a shows single-source DG results. We see that L (referred to as orthogonality only) outperforms L (referred to as alignment only) by up to 1%. This confirms that preserving modality-specific features guides the network in the right direction. RNA-Net outperforms such methods by up to 3%, confirming that bounding the features

in a mutually exclusive aligned or orthogonal space representation could cause a degradation in performance. At the same time, the need of balancing between the two norm distributions is shown to be effective by the results obtained by adding a simple regularization strategy (referred to BatchNorm). Once again, our RNA-Net outperforms the competitors by up to 3%, proving the strength of LRNA. Finally, the fact that a robust method as the self-supervised (referred as SS) does not surpass the source-only baseline, highlights the complexity of the problem. Table 1-b shows the results obtained on multi-source DG. Our method achieves a consistent boost in performance (+6.4%) w.r.t. DeepAll, and outperforms all other baselines.
DA Results. We validate or method in the DA context against four existing unsupervised domain adaptation approaches: (a) AdaBN [32]: Batch Normalization layers are updated with target domain statistics; (b) MMD [36]: it minimizes separate discrepancy measures applied to single modalities; (c) Adversarial Only [19]: a domain discriminator is trained in an adversarial fashion through the gradient reverse layer (GRL) in order to make the feature representations for source and target data indistinguishable; (d) MMSADA [41]: a multi-modal domain adaptation framework which is based on the combination of existing DA methods, i.e., a self-supervised synchronization pretext task and an adversarial approach.
Results are summarized in Table 2. When target data is available at training time, our LRNA outperforms the standard DA approaches AdaBN [32] and MMD [36] by 5.8% and 3.3% respectively. Moreover, our method outperforms adversarial alignment [19] by 4%. Interestingly, when used in combination to the adversarial approach, our LRNA slightly improves performances. This complementarity is due to ability of our approach to preserve the structural discrimination of each modality and its intra-class compactness, compensating the distortion in the original distribution induced by the adversarial approach. This val-

7

Without RNA-

Ablations

Supervised Single-DG Multi-DG DA

Baseline Fusion HNA RNA-Net Fusion RNA-Net

59.76 60.18 62.41 62.11 63.13

40.93 40.33 44.58 45.48 45.75

44.67 47.61 46.70 49.56 51.06

47.19 45.73 47.71

Table 3. Top-1 Accuracy (%) of RNA-Net w.r.t. its mid-fusion implementation (RNA-Net Fusion) and HNA on all settings.

idates the considerations done at the end of Section 3.3.

Conversely, LRNA achieves a boost of more than 1% in

terms of accuracy when compared against a standard self-

supervised synchronization task, which in turn operates by

means of reducing the discrepancy, as we do. Finally, we

validate our method against the most recent approach in

video-based DA literature, i.e., MM-SADA [41], achieving

60

on-par results. Considering that MM-SADA combines both

52,5 45

a self-supervised and adversarial approach, we compete by

37,5 30

means of a lightweight architecture and by employing dif-

22,5

15

ferent modalities.

7,5

With RNA-

60

52,5

RGB

45

Audio

37,5

30

22,5

15

7,5

Ablation Study. To verify the effectiveness of our design choice, we introduce a loss variant, called Hard Norm Alignment (HNA), that induces the norms to tend to a given value arbitrarily R. The R term is chosen after observing the range of the norms of the two modalities, and picking a value half-way between the two. To further prove the strength of our method over different architectural variants, we compare the late fusion approach against the so-called mid-level fusion, proposed in [27]. It consists in feeding the prediction layer a concatenation of the two modality fea-

Feature norms without RNA-

Feature norms with RNA-

Figure 6. Qualitative DG results. Class activation maps (CAMs) obtained on a target segment using a model trained without (top) and with (bottom) our proposed LRNA loss, with its audio waveform (middle). A benefit brought by our method is a more localized focus that the network puts on relevant portions of the image after re-balancing the contribution of the two modalities (blue corresponds to higher attention, red to less). The effects on the feature norm values are visible in the histograms at the bottom.

tures. The results are shown in Table 3. Note how HNA

Accuracy

performs worse than LRNA in all contexts, confirming that 50,00 RGB

an "hard" loss function constitutes in a limit. As far as con-

Audio RGB + Audio

46,32

40,00
cern the mid-level fusion approach, it demonstrates to be

40,03

38,69

34,98

34,80

a valid alternative in both the supervised and cross-domain 30,00

Top-300 feature norm % of the total
RGB Audio
40,00

72,01%

32,50

settings, remarking the flexibility of our method to be em-

ployed in different feature fusion strategies.

20,00

19,73

1

63,35%

61,57%

87,75%

25,00

Qualitative Analysis. We give an insight of the norm unbalance problem described in Section 3.2 by showing diagrams representing the norm variations and their impact

Without RNA-

With RNA-

Without RNA-

17,50
With RNA-

Figure 7. Final score prediction unbalance between audio and vi-

sual modalities w/ and w/o our loss function (left). Discrepancy

on the performance. To the readjustment of the norms cor- between the norm ranges and their variation before and after the

responds a boost in performance (Figure 7-a). We also show in Figure 7-b the percentage of the total norm given by the

adjustment (right). When minimizing LRNA, the features which Tabella 2-1
are kept active are the most relevant for classification, iR.GeB., top-3Au0dio0.

300 most relevant features for classification. While mini-

Without RGB Without Spec

6,05 15,8

3,5 6

mizing LRNA, the top 300 features maintain (or even increase) their importance, since their norm ends up repre-

6. Conclusion Tabella 2

Without

With

RGB

19,73

34,8

With RGB With Spec

6,92

4,32

5,73

senting the majority of the total norm. This further remarks

Audio

34,98 38,6883333333333

In this work RGB + Audio we show 40,03 the46,31i5mportance of auditory infor-

that while relatively adjusting the feature norms of the two mation in cross-domain first person action recognition. We

modalities, our LRNA serves as a feature "selector" for the exploit the complementary nature of audio and visual in-

final classifier. Lastly, our method brings the side benefit of formation by defining a new cross-modal loss function that

making the network focus more on relevant portions of the operates directly on the relative feature norm of the two

image, with sharper and well defined class activation maps modalities. Extensive experiments on DG and DA settings

(CAMs) w.r.t. the baseline, as shown in Figure 6.

prove the power of our loss. Future work will further pur-

8

sue this research avenue, exploring the effectiveness of the RNA-loss in third person activity recognition settings, and combined with traditional cross-domain architectures.
References
[1] Triantafyllos Afouras, Andrew Owens, Joon Son Chung, and Andrew Zisserman. Self-supervised learning of audio-visual objects from video. arXiv preprint arXiv:2008.04237, 2020. 2, 3
[2] Relja Arandjelovic and Andrew Zisserman. Look, listen and learn. In Proceedings of the IEEE International Conference on Computer Vision, pages 609­617, 2017. 2, 3
[3] Relja Arandjelovic and Andrew Zisserman. Objects that sound. In Proceedings of the European Conference on Computer Vision (ECCV), pages 435­451, 2018. 2, 3
[4] Yusuf Aytar, Carl Vondrick, and Antonio Torralba. Soundnet: Learning sound representations from unlabeled video. In Advances in neural information processing systems, pages 892­900, 2016. 2
[5] Silvia Bucci, Antonio D'Innocente, Yujun Liao, Fabio Maria Carlucci, Barbara Caputo, and Tatiana Tommasi. Selfsupervised learning across domains. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021. 3, 7
[6] Fabio M Carlucci, Antonio D'Innocente, Silvia Bucci, Barbara Caputo, and Tatiana Tommasi. Domain generalization by solving jigsaw puzzles. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2229­2238, 2019. 3, 7
[7] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6299­6308, 2017. 2, 6
[8] Alejandro Cartas, Jordi Luque, Petia Radeva, Carlos Segura, and Mariella Dimiccoli. Seeing and hearing egocentric actions: How much can we learn? In Proceedings of the IEEE International Conference on Computer Vision Workshops, pages 0­0, 2019. 2
[9] Woong-Gi Chang, Tackgeun You, Seonguk Seo, Suha Kwak, and Bohyung Han. Domain-specific batch normalization for unsupervised domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7354­7362, 2019. 3
[10] Min-Hung Chen, Zsolt Kira, Ghassan AlRegib, Jaekwon Yoo, Ruxin Chen, and Jian Zheng. Temporal attentive alignment for large-scale video domain adaptation. In Proceedings of the IEEE International Conference on Computer Vision, pages 6321­6330, 2019. 3
[11] Jinwoo Choi, Gaurav Sharma, Manmohan Chandraker, and Jia-Bin Huang. Unsupervised and semi-supervised domain adaptation for action recognition from drones. In The IEEE Winter Conference on Applications of Computer Vision, pages 1717­1726, 2020. 3
[12] Joon Son Chung and Andrew Zisserman. Out of time: automated lip sync in the wild. In Asian conference on computer vision, pages 251­263. Springer, 2016. 2

[13] Nieves Crasto, Philippe Weinzaepfel, Karteek Alahari, and Cordelia Schmid. Mars: Motion-augmented rgb stream for action recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019. 2
[14] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et al. Scaling egocentric vision: The epic-kitchens dataset. In Proceedings of the European Conference on Computer Vision (ECCV), pages 720­736, 2018. 1, 2, 6
[15] J. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li FeiFei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248­255, 2009. 6
[16] Zhijie Deng, Yucen Luo, and Jun Zhu. Cluster alignment with a teacher for unsupervised domain adaptation. In Proceedings of the IEEE International Conference on Computer Vision, pages 9944­9953, 2019. 3
[17] Qi Dou, Daniel Coelho de Castro, Konstantinos Kamnitsas, and Ben Glocker. Domain generalization via model-agnostic learning of semantic features. Advances in Neural Information Processing Systems, 32:6450­6461, 2019. 3
[18] Antonino Furnari and Giovanni Farinella. Rolling-unrolling lstms for action anticipation from first-person video. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020. 1, 2
[19] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. volume 37 of Proceedings of Machine Learning Research, pages 1180­1189, Lille, France, 07­09 Jul 2015. PMLR. 3, 7
[20] Ruohan Gao, Tae-Hyun Oh, Kristen Grauman, and Lorenzo Torresani. Listen to look: Action recognition by previewing audio. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10457­10467, 2020. 2
[21] Deepti Ghadiyaram, Du Tran, and Dhruv Mahajan. Largescale weakly-supervised pre-training for video action recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 12046­12055, 2019. 1
[22] Rui Gong, Wen Li, Yuhua Chen, and Luc Van Gool. Dlow: Domain flow for adaptation and generalization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2477­2486, 2019. 3
[23] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei Efros, and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation. In International conference on machine learning, pages 1989­ 1998. PMLR, 2018. 3
[24] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pages 448­456. PMLR, 2015. 6
[25] Arshad Jamal, Vinay P Namboodiri, Dipti Deodhare, and KS Venkatesh. Deep domain adaptation in action space. In BMVC, volume 2, page 4, 2018. 3

9

[26] Georgios Kapidis, Ronald Poppe, Elsbeth van Dam, Lucas Noldus, and Remco Veltkamp. Multitask learning to improve egocentric action recognition. In Proceedings of the IEEE International Conference on Computer Vision Workshops, pages 0­0, 2019. 2
[27] Evangelos Kazakos, Arsha Nagrani, Andrew Zisserman, and Dima Damen. Epic-fusion: Audio-visual temporal binding for egocentric action recognition. In The IEEE International Conference on Computer Vision (ICCV), October 2019. 1, 2, 6, 7, 8
[28] Bruno Korbar, Du Tran, and Lorenzo Torresani. Cooperative learning of audio and video models from selfsupervised synchronization. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS'18, page 7774­7785. Curran Associates Inc., 2018. 2, 3
[29] Myunggi Lee, Seungeui Lee, Sungjoon Son, Gyutae Park, and Nojun Kwak. Motion feature network: Fixed motion filter for action recognition. In Proceedings of the European Conference on Computer Vision (ECCV), pages 387­ 403, 2018. 2
[30] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C Kot. Domain generalization with adversarial feature learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5400­5409, 2018. 3
[31] Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao. Deep domain generalization via conditional invariant adversarial networks. In Proceedings of the European Conference on Computer Vision (ECCV), pages 624­639, 2018. 3
[32] Yanghao Li, Naiyan Wang, Jianping Shi, Xiaodi Hou, and Jiaying Liu. Adaptive batch normalization for practical domain adaptation. Pattern Recognition, 80:109­117, 2018. 3, 7
[33] Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and Xiaodi Hou. Revisiting batch normalization for practical domain adaptation. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 2426, 2017, Workshop Track Proceedings. OpenReview.net, 2017. 3
[34] Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shift module for efficient video understanding. In Proceedings of the IEEE International Conference on Computer Vision, pages 7083­7093, 2019. 2
[35] Hong Liu, Mingsheng Long, Jianmin Wang, and Michael Jordan. Transferable adversarial training: A general approach to adapting deep classifiers. volume 97 of Proceedings of Machine Learning Research, pages 4013­4022, Long Beach, California, USA, 09­15 Jun 2019. PMLR. 6
[36] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learning transferable features with deep adaptation networks. In International conference on machine learning, pages 97­105. PMLR, 2015. 3, 7
[37] M. Lu, Z. Li, Y. Wang, and G. Pan. Deep attention network for egocentric action recognition. IEEE Transactions on Image Processing, 28(8):3703­3713, 2019. 2
[38] Minlong Lu, Danping Liao, and Ze-Nian Li. Learning spatiotemporal attention for egocentric action recognition. In

Proceedings of the IEEE International Conference on Computer Vision Workshops, pages 0­0, 2019. 2
[39] Minghuang Ma, Haoqi Fan, and Kris M Kitani. Going deeper into first-person activity recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1894­1903, 2016. 2
[40] Krikamol Muandet, David Balduzzi, and Bernhard Scho¨lkopf. Domain generalization via invariant feature representation. volume 28 of Proceedings of Machine Learning Research, pages 10­18, Atlanta, Georgia, USA, 17­19 Jun 2013. PMLR. 2
[41] Jonathan Munro and Dima Damen. Multi-modal domain adaptation for fine-grained action recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020. 1, 2, 3, 6, 7, 8
[42] Andrew Owens and Alexei A Efros. Audio-visual scene analysis with self-supervised multisensory features. In Proceedings of the European Conference on Computer Vision (ECCV), pages 631­648, 2018. 2, 3
[43] Juan-Manuel Perez-Rua, Brais Martinez, Xiatian Zhu, Antoine Toisoul, Victor Escorcia, and Tao Xiang. Knowing what, where and when to look: Efficient video action modeling with attention. arXiv preprint arXiv:2004.01278, 2020. 2
[44] Mirco Planamente, Andrea Bottino, and Barbara Caputo. Joint encoding of appearance and motion features with selfsupervision for first person action recognition. arXiv preprint arXiv:2002.03982, 2020. 2
[45] Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tatsuya Harada. Maximum classifier discrepancy for unsupervised domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3723­3732, 2018. 3
[46] Karen Simonyan and Andrew Zisserman. Two-stream convolutional networks for action recognition in videos. In Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1, NIPS'14, page 568­576, Cambridge, MA, USA, 2014. MIT Press. 2
[47] Suriya Singh, Chetan Arora, and CV Jawahar. First person action recognition using deep learned descriptors. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2620­2628, 2016. 2
[48] Swathikiran Sudhakaran, Sergio Escalera, and Oswald Lanz. Hierarchical feature aggregation networks for video action recognition. arXiv preprint arXiv:1905.12462, 2019. 2
[49] Swathikiran Sudhakaran, Sergio Escalera, and Oswald Lanz. Lsta: Long short-term attention for egocentric action recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9954­9963, 2019. 1, 2
[50] Swathikiran Sudhakaran and Oswald Lanz. Convolutional long short-term memory networks for recognizing first person interactions. In Proceedings of the IEEE International Conference on Computer Vision (ICCV) Workshops, Oct 2017. 2
[51] Swathikiran Sudhakaran and Oswald Lanz. Attention is all we need: Nailing down object-centric attention for egocen-

10

tric activity recognition. arXiv preprint arXiv:1807.11794, 2018. 1, 2 [52] Shuyang Sun, Zhanghui Kuang, Lu Sheng, Wanli Ouyang, and Wei Zhang. Optical flow guided feature: A fast and robust motion representation for video action recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1390­1399, 2018. 2 [53] Hui Tang and Kui Jia. Discriminative adversarial domain adaptation. In AAAI, pages 5940­5947, 2020. 3 [54] Antonio Torralba and Alexei A Efros. Unbiased look at dataset bias. In CVPR 2011, pages 1521­1528. IEEE, 2011. 1 [55] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spatiotemporal features with 3d convolutional networks. In Proceedings of the IEEE international conference on computer vision, pages 4489­4497, 2015. 2 [56] Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John C Duchi, Vittorio Murino, and Silvio Savarese. Generalizing to unseen domains via adversarial data augmentation. In Advances in neural information processing systems, pages 5334­5344, 2018. 3 [57] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment networks: Towards good practices for deep action recognition. In European conference on computer vision, pages 20­36. Springer, 2016. 2 [58] Xiaohan Wang, Yu Wu, Linchao Zhu, Yi Yang, and Yueting Zhuang. Symbiotic attention: Uts-baidu submission to the epic-kitchens 2020 action recognition challenge. 2 [59] Chao-Yuan Wu, Christoph Feichtenhofer, Haoqi Fan, Kaiming He, Philipp Krahenbuhl, and Ross Girshick. Long-term feature banks for detailed video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019. 1, 2 [60] Ruijia Xu, Guanbin Li, Jihan Yang, and Liang Lin. Larger norm more transferable: An adaptive feature norm approach for unsupervised domain adaptation. In Proceedings of the IEEE International Conference on Computer Vision, pages 1426­1435, 2019. 3 [61] Zhiyu Yao, Yunbo Wang, Xingqiang Du, Mingsheng Long, and Jianmin Wang. Adversarial pyramid network for video domain generalization. arXiv preprint arXiv:1912.03716, 2019. 3 [62] Hang Zhao, Chuang Gan, Andrew Rouditchenko, Carl Vondrick, Josh McDermott, and Antonio Torralba. The sound of pixels. In Proceedings of the European Conference on Computer Vision (ECCV), September 2018. 2 [63] Jiaojiao Zhao and Cees GM Snoek. Dance with flow: Twoin-one stream action detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9935­9944, 2019. 2 [64] Bolei Zhou, Alex Andonian, Aude Oliva, and Antonio Torralba. Temporal relational reasoning in videos. In Proceedings of the European Conference on Computer Vision (ECCV), pages 803­818, 2018. 2
11

