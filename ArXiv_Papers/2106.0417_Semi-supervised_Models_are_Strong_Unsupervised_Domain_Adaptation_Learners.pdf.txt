Semi-supervised Models are Strong Unsupervised Domain Adaptation Learners

arXiv:2106.00417v1 [cs.LG] 1 Jun 2021

Yabin Zhang1 Haojian Zhang2 Bin Deng2 Shuai Li1 Kui Jia2 Lei Zhang1

1 The Hong Kong Polytechnic University

2 South China University of Technology

Abstract
Unsupervised domain adaptation (UDA) and semi-supervised learning (SSL) are two typical strategies to reduce expensive manual annotations in machine learning. In order to learn effective models for a target task, UDA utilizes the available labeled source data, which may have different distributions from unlabeled samples in the target domain, while SSL employs few manually annotated target samples. Although UDA and SSL are seemingly very different strategies, we find that they are closely related in terms of task objectives and solutions, and SSL is a special case of UDA problems. Based on this finding, we further investigate whether SSL methods work on UDA tasks. By adapting eight representative SSL algorithms on UDA benchmarks, we show that SSL methods are strong UDA learners. Especially, state-of-the-art SSL methods significantly outperform existing UDA methods on the challenging UDA benchmark of DomainNet, and state-of-the-art UDA methods could be further enhanced with SSL techniques. We thus promote that SSL methods should be employed as baselines in future UDA studies and expect that the revealed relationship between UDA and SSL could shed light on future UDA development. Codes are available at https://github.com/YBZh.
1 Introduction
Although deep models have achieved great successes on various tasks [24, 16], they require a large amount of labeled training data and could not generalize to data of shifted distributions, impeding their applications in practical tasks. Given a new target task, the model learned from labeled source data of a previous task typically achieves degenerated performance on target data. It is time and labor consuming to collect a large amount of labeled training data for the new task. Fortunately, collecting unlabeled data is usually much easier; thus, there is a huge demand to learn efficient target models by utilizing labeled source data and unlabeled target data, falling in the realm of unsupervised domain adaptation (UDA) [15, 34]. Motivated by seminal UDA theories (e.g., Theorem 1) [2, 58, 56], which bound the target error with terms including domain discrepancy, several popular UDA methods [15, 29, 38, 30, 57] have been proposed to minimize the target risk by simultaneously learning domain-invariant representations and minimizing the source risk. Besides, the target risk is minimized by re-weighting labeled source samples in [39, 8]; pixel-level alignment is pursued in [5, 28] with image style transfer techniques. A comprehensive survey on UDA methods can be found in [48, 47] .
With similar motivations, semi-supervised learning (SSL) explores a different way from UDA to learn target models with minor manual efforts. Considering that manually annotating a few amount of target data is easy to achieve, SSL aims to learn target models from few labeled target data and a large amount of unlabeled target data [45, 61, 6]. SSL methods are typically motivated by basic assumptions on the data structure, such as the smoothness assumption, low-density separation assumption, and manifold assumption [45, 61, 6]. The smoothness assumption promotes that two
Preprint. Under review.

Decision boundary via labeled data only Optimal decision boundary

Labeled data

T0

S1

T0

T1 S0

T1

Labeled data

(a) SSL

(b) UDA

Figure 1: Illustrations of (a) SSL and (b) UDA on binary classification. In SSL, only a few samples per class are labeled and other training samples are unlabeled. In UDA, the source data (S0 and S1) are labeled and target data (T0 and T1) are unlabeled. Figure (a) is modified from Fig. 1 in [45].

samples close in the input space should present the same label, inspiring the consistency regularization [26, 49] and graph-based methods [60, 20]. The low-density separation assumption states that the decision boundaries should lie in areas where few samples are observed, which motivates methods of entropy minimization [18] and self-training [27]. The manifold assumption suggests that the input space could be decomposed into multiple low-dimensional manifolds and samples on the same manifold should share the same label, whose representative method is the graph-based one [60]. Some methods simultaneously adopt multiple assumptions [60, 41, 4].
We illustrate the task settings of UDA and SSL in Figure 1. Although UDA and SSL are seemingly very different, we argue that they are closely related in the following aspects:
· UDA and SSL share a similar objective of utilizing unlabeled (target) data to improve the performance of the model trained with labeled (source) data only.
· Methods of UDA and SSL similarly utilize the underlying data structure.
· SSL is a special case of UDA problems when the target support covers source support.
Based on the above relationships between UDA and SSL, we further investigate an important question: whether SSL methods work on UDA tasks? To answer this question, we make in-depth analyses by applying eight representative SSL algorithms on four UDA benchmarks. Results clearly demonstrate that SSL methods are strong UDA learners. Especially, state-of-the-art SSL methods significantly outperform existing UDA methods on the challenging UDA benchmark of DomainNet, and state-ofthe-art UDA methods could be further enhanced with SSL techniques. Therefore, we argue that SSL methods, especially the state-of-the-art ones, should be employed as baselines in future UDA studies.
2 Background of UDA and SSL
In UDA, we are given ns labeled source samples Ds = {xis, ysi }ni=s1 drawn from the source distribution Ps(X, Y ) and nt unlabeled target samples Dt = {xjt }nj=t 1 drawn from the target distribution Pt(X), where X  X and Y  Y = {0, 1} (we consider the binary classification for simplicity). The unobserved labels in the target domain follow the distribution Pt(Y |X), and the label space Y is shared across domains. The number of source samples ns and target samples nt are assumed to be large enough [11]; thus the source data Ds and target data Dt could well represent distributions of Ps(X, Y ) and Pt(X), respectively. The goal of UDA is to find a hypothesis f : X  {0, 1} to minimize the target risk Rt(f ) = E(x,y)Pt(X,Y )|f (x) - y)|. By the convenience of deep models, we define the model as f = h  g, where g : X  Z lifts any input data in X to the feature space Z and h : Z  {0, 1} is the classifier defined in Z. We denote
2

the lifted distribution P (g(X)) as P (Z). We adopt the following covariate shift assumption1 for UDA problems:

Assumption 1. (Covariate shift) Domains Ps(X, Y ) and Pt(X, Y ) satisfy the covariate shift assumption if

Ps(X) = Pt(X), Ps(Y |X) = Pt(Y |X).

(1)

Importance-weighting [39, 8] and learning domain-invariant feature representations [15, 29] are two seminal UDA methods. Specifically, importance-weighting [39] minimizes the target risk using importance-weighted source samples:

Liw

=

1 ns

ns
w(xis)
i=1

(f (xis), ysi ),

(2)

where w(x) = Pt(x)/Ps(x) and : R2  R is the loss function. The efficacy of importanceweighting in UDA problems is theoretically guaranteed when the source support supp(Ps) covers target support supp(Pt) [8], where supp(P ) = {x|P (x) > 0}.
Methods [15, 29, 38, 57] aiming at learning domain-invariant feature representations are primarily motivated from the following UDA bound:
Theorem 1. (Adaptation bound by Ben-David et al. [2]) Given the hypothesis class H, source distribution Ps(Z, Y ), and target distribution Pt(Z, Y ), the following inequality holds for all h  H,

1

Rt(h)  Rs(h) + 2 dHH(Ps, Pt) + H,

(3)

where

dHH(Ps, Pt) = 2 sup EzPs(Z)I[h(z) = h (z)] - EzPt(Z)I[h(z) = h (z)] , (4)
h,h H

H = min [Rt(h) + Rs(h)] .

(5)

hH

Here, the target risk is defined as Rt(h) = E(z,y)Pt(Z,Y )|h(z) - y| and the source risk Rs(h) is

similarly defined. I[var] =

1 0

var = T rue Others

. H is a constant depending on the capacity of the

hypothesis space H.

The bound in Theorem 1 is proposed for binary classification (i.e., h(z)  {0, 1}), which is extended
to the multi-class setting in [58, 56].
In SSL, labeled data Dl = {xil, yli}ni=l 1 and unlabeled data Du = {xju}nj=u1 are adopted as training data, where Dl and Du are sampled from distributions of Pssl(X, Y ) and Pssl(X), respectively. It is typically assumed that nl nu; in other words, the number of unlabeled samples nu is assumed to be large enough to represent the distribution Pssl(X), while the number of labeled samples nl is too small to represent the distribution Pssl(X, Y ). The goal of SSL is to find a hypothesis f : X  {0, 1} to minimize the risk on the distribution Pssl(X, Y ): Rssl(f ) = E(x,y)Pssl(X,Y )|f (x) - y|. SSL methods are majorly motivated by assumptions on the data structure, as discussed in Section 1.

3 Bridging UDA and SSL

3.1 Bridging UDA and SSL: formulation and methodology

We first introduce the general formulation of UDA and SSL, then we discuss their relationships in

terms of methodology. Specifically, UDA and SSL both adopt the model trained with labeled (source)

data as the baseline:

min Lsup(f, Dl),

(6)

f =hg

1Although it is questionable whether Ps(Y |g(X)) = Pt(Y |g(X)) holds in the feature space [17, 59], Ps(Y |X) = Pt(Y |X) in vanilla data space X generally holds in practical tasks.

3

where Lsup(f, Dl) is the supervised task loss and the labeled set Dl = Ds in UDA. In both tasks, researchers introduce regularization terms with unlabeled data Du (and labeled data Dl) as:

Lreg(f, Du, Dl),

(7)

where the unlabeled set Du = Dt in UDA. In the regularization term (7), only unlabeled (target) data are used in [21, 18, 27] whereas other methods [15, 41, 26] additionally utilize the labeled
(source) data. Note that the regularization in (7) is data-dependent, which is distinguished from the
data-independent ones, e.g., weight decay [25] and Dropout [42]. UDA and SSL both aim to utilize
unlabeled (target) data to improve the performance of the model trained with labeled (source) data only, leading to the following objective2:

min Lsup(f, Dl) + Lreg(f, Du, Dl),

(8)

f =hg

where  is the trade-off parameter.

Although methods of UDA and SSL mentioned in Section 1 are seemingly very different, they all follow the formulation (8). Further, many recent UDA methods adopt SSL techniques as the components, and some UDA methods are variants of SSL ones. For example, the entropy minimization [18], one typical SSL method, is used cooperatively with the domain discrepancy minimization in [31, 57, 40]; label propagation [60], a graph-based SSL method, is also adopted in UDA methods [13, 55] together with the domain discrepancy minimization for performance boosting. Self-training, which is initially adopted in SSL [27], and its variants are widely adopted in UDA methods [62, 63]. Self-ensembling [14] is a variant of the SSL method of mean teacher [44], which boosts the vanilla mean teacher with strategies such as confidence threshold. Tang et al. [43] tackled UDA with the framework of cluster-then-label, which is also an SSL pipeline of a long history [45, 10]. MCC [21] shares similar objectives to entropy minimization [18] and self-training [27], which promote the classification of low-density separation (i.e., low entropy predictions in [18, 27] or less confusion predictions in [21]).
In the following section, we show that SSL is a special case of UDA problems. Although SSL techniques and their variants have been practically adopted in UDA methods, we explicitly reveal their relationships in Section 3.2 and empirically illustrate the efficacy of vanilla SSL methods on UDA tasks in Section 4.1; we promote that SSL methods should be employed as baselines in future UDA studies.

3.2 SSL is a special case of UDA problems
As we discussed in Section 2, although labeled data {xil, yli}ni=l 1 are sampled from the distribution Pssl(X, Y ) in SSL, they are typically insufficient to represent the overall Pssl(X, Y ) since very few labeled data are sampled (i.e., nl is small); in other words, the few labeled data {xil, yli}ni=l 1 could only represent part of the overall distribution Pssl(X, Y ), i.e., a sub-domain of Pssl(X, Y ). Among all possible sub-domains, the one owning the smallest support set could be represented as follows:
Psmall(X) : Psmall(X = x) > 0  x  {xil}ni=l 1, Psmall(Y |X) = Pssl(Y |X) (9)
With Psmall and Pssl as distributions of labeled and unlabeled data respectively and the covariate shift assumption 1 in UDA, the SSL task turns to a UDA problem, where the unlabeled target support supp(Pssl) covers the labeled source support supp(Psmall), i.e., supp(Psmall)  supp(Pssl).
Remark 1. UDA problems could be classified into four settings according to the relationship between the source support supp(Ps) and target support supp(Pt), as illustrated in Figure 2. Specifically, the importance weighting algorithm (2) [39, 8] is proposed to tackle the specific UDA setting where supp(Pt)  supp(Ps). When supp(Ps)  supp(Pt), this UDA problem could be solved by SSL methods since SSL tasks coincidentally fall into the realm of this UDA setting. Another two settings, i.e., supp(Ps)  supp(Pt) =  and the remaining partially overlapped setting, are not specifically investigated yet to the best of our knowledge. Although the SSL task falls in a specific UDA setting of supp(Ps)  supp(Pt), the general assumptions of smoothness, low-density separation, and manifold
2The transductive SSL methods, e.g., the graph-based ones [60, 1], also share a similar form of objective comprised of labeled data supervision and unlabeled data regularization; differently, these methods output label predictions of unlabeled data directly, instead of models.

4

supp(! )

supp(! )

supp(" )

supp(" )

(a) supp(! )  supp(") (b) supp(")  supp(! ) (c) Partially overlapped (d) supp(")  supp(! )=
Figure 2: An illustration of the four UDA settings with different relationships between the source support supp(Ps) and target support supp(Pt). The settings of (a) supp(Pt)  supp(Ps) and (b) supp(Ps)  supp(Pt) are respectively investigated with the importance weighting [39, 8] and SSL; the other two settings (c) and (d) are not specifically studied yet to the best of our knowledge.

in SSL methods (cf. Section 1) commonly hold in general UDA tasks, supporting the empirical successes of SSL methods on practical UDA tasks, as will be demonstrated in Section 4.1.
Remark 2. We discuss the potential impact of our research on the UDA development. Motivated by seminal UDA theories [2, 58, 56], the most popular UDA framework minimizes the target risk by simultaneously learning domain-invariant representations and minimizing the source risk [15, 30, 38, 57]. Although some successes have been achieved, this framework has some limitations. As empirically studied in [7], the discrimination of target data may be deteriorated when learning domain-invariant feature representations via adversarial training [15]. Zhao et al. [59] theoretically indicated that learning domain-invariant representations and minimizing the source risk are not sufficient to guarantee a small target risk, since the minor joint error H in Theorem (1) may be enlarged as the change of the feature space Z. Zhao et al. [59] emphasized that the conditional shift across domains plays an important role in the target risk minimization under the UDA framework [2, 58, 56], but how to practically minimize the conditional shift is unclear due to the missing of target labels. To minimize the conditional shift, one may turn to pseudo labels of target data [50, 23], but these pseudo labels may be unreliable. Different from the seminal UDA framework, where unlabeled target data are utilized to explicitly minimize the domain divergence, recent UDA methods have been proposed to explore and utilize the data structure of unlabeled data [21, 14, 43], which is closely related to SSL assumptions on the data structure. We expect that the revealed relationship between UDA and SSL could promote the development of UDA research in terms of both algorithms and theories.
A toy example. To facilitate the understanding of our findings, we construct a simple 1-dimensional example (i.e., X = R) to intuitively illustrate that the SSL task could be formulated as a UDA problem. With U (a, b) as the uniform distribution within range [a, b], where a < b, we introduce the target distribution as:

Pssl(X) = U (0, 1), Pssl(Y = 1|X = x) =

0 1

x  0.5 x > 0.5

.

(10)

We suppose only one labeled sample per-category is given in SSL3. Specifically, the two labeled samples are Dl = (x1l = c, yl1 = 0), (x2l = d, yl2 = 1) , where 0  c  0.5 and 0.5 < d  1. Then we could introduce Psmall, the possible distribution for Dl with the smallest support set, as a variant of the Bernoulli distribution:
0.5 x = c Psmall(X) : Psmall(X = x) = 0.5 x = d , Psmall(Y |X) = Pssl(Y |X). (11)
0 Others

With Psmall and Pssl as distributions of source and target domains respectively, the SSL task is formulated as a UDA problem, as illustrated in Figure 3.

3The same conclusion holds for multiple, but few, samples per-category in multiclass classification; we analyze the binary case with one labeled sample per-category for simplicity.

5

UDA

SSL

Table 2: Transductive results on the small-scale Office31 dataset (ResNet50).

Methods

AW DW WD

AD

DA WA Avg.

Source Only

77.3±1.1 95.5±0.5 99.1±0.1 81.4±0.4 64.1±0.1 65.3±0.7 80.5

DANN [15]

83.9±2.1 97.3±0.2 100.0±.0 81.9±0.8 72.5±0.1 72.8±0.1 84.7

MCD [38]

88.1±1.5 98.3±0.2 100.0±.0 87.5±0.5 71.6±0.8 68.1±0.1 85.6

CDAN [30]

89.3±2.6 98.6±0.5 100.0±.0 94.1±1.8 75.5±2.7 71.9±0.8 88.2

AFN [51]

92.3±0.7 99.0±0.1 100.0±.0 94.9±1.6 73.6±0.2 70.7±0.8 88.3

MDD [58]

89.2±1.3 98.7±0.4 100.0±.0 94.4±2.3 76.0±1.0 73.8±0.1 88.7

Self-ensembling [14] 86.1±0.5 98.3±0.2 100.0±.0 89.2 ±0.3 68.8±0.1 67.5±0.5 85.0

MCC [21]

93.0±0.1 98.1±0.1 99.8±0.2 95.0±0.8 76.6±0.1 76.1±0.6 89.8

VAT [32]

81.8±0.7 98.5±0.2 99.9±0.1 82.5±1.0 66.2±0.6 65.3±1.0 82.4

Mean Teacher [44] 83.7±0.2 98.6±0.2 100.0±.0 84.1±0.2 68.6±0.3 66.7±0.2 83.6

-Model [26]

87.7±0.6 98.9±0.1 100.0±.0 87.1±0.7 69.6±0.5 68.0±0.1 85.2

MixMatch [4]

89.3±0.5 99.0±0.1 99.8±0.2 90.5±0.3 68.6±0.2 67.8±0.3 85.8

Self-training [27]

88.0±0.2 98.8±0.2 100.0±.0 90.6±0.2 69.5±0.1 68.6±0.2 85.9

Entropy mini. [18] 89.3±0.6 98.9±0.1 100.0±.0 88.0±0.2 72.7±0.6 68.2±0.4 86.2

FixMatch [41]

92.6±0.3 98.9±0.3 100.0±.0 93.3±0.5 73.5±1.0 71.6±0.1 88.3

Xie et al. [49]

93.6±0.8 98.6±0.4 100.0±.0 95.6±0.2 73.5±0.3 74.2±0.2 89.2

MDD + Consistency 92.2±0.4 97.9±0.1 100.0±.0 93.8±0.6 75.0±0.1 75.0±0.2 89.0

MCC + Consistency 94.0±0.7 98.1±0.1 99.6±0.2 93.6±0.4 77.3±0.2 76.7±0.1 89.9

Table 3: Transductive results on the medium-scale OfficeHome dataset (ResNet50).

Methods

ArCl ArRw ClPr PrAr PrRw RwCl Avg.

Source Only

42.6±0.3 74.1±0.4 61.7±0.4 54.5±0.1 72.8±0.1 44.2±0.1 58.3

AFN [51]

48.3±0.1 76.1±0.1 68.5±0.6 62.6±0.1 76.1±0.1 52.6±0.1 64.0

DANN [15]

50.5±0.7 75.1±0.1 65.0±0.1 61.2±0.4 78.5±0.1 56.5±0.1 64.5

MCD [38]

50.8±0.8 77.4±0.1 69.3±0.1 61.7±0.4 78.2±0.1 56.9±0.4 65.7

CDAN [30]

51.2±0.1 79.3±0.1 70.3±0.8 65.4±0.1 80.8±0.5 57.3±0.4 67.4

MDD [58]

54.2±0.4 80.0±0.2 72.3±0.1 66.1±0.1 80.0±0.3 58.5±0.5 68.5

Self-ensembling [14] 47.2±0.1 75.9±0.4 67.8±0.5 62.4±2.1 76.2±0.6 53.1±1.1 63.8

MCC [21]

55.5±0.1 83.0±0.6 75.8±0.5 69.2±0.2 81.9±0.1 59.4±0.8 70.8

Mean Teacher [44] 44.3±0.2 75.0±0.1 62.1±0.1 54.7±0.4 72.9±0.2 45.0±0.2 59.0

VAT [32]

44.0±0.1 74.8±0.2 62.3±0.1 55.0±0.3 72.8±0.1 45.1±0.2 59.0

-Model [26]

44.1±0.5 76.0±0.1 64.1±0.1 54.2±0.3 73.7±0.1 45.0±0.3 59.4

Self-training [27]

46.2±0.3 76.4±0.1 66.1±0.1 58.0±0.2 74.9±0.1 48.8±0.1 61.7

MixMatch [4]

45.3±0.2 76.6±0.1 71.1±0.1 60.1±0.1 77.0±0.2 50.8±0.2 63.5

Entropy mini. [18] 52.0±0.2 77.1±0.2 71.3±0.1 61.4±0.1 78.5±0.1 54.3±0.1 65.8

Xie et al. [49]

52.1±1.8 79.4±0.1 72.3±0.3 65.2±0.3 79.2±0.2 56.9±0.5 67.5

FixMatch [41]

52.5±0.8 79.4±0.2 73.6±0.1 66.5±0.6 80.0±0.1 57.3±0.5 68.2

MDD + Consistency 58.2±0.2 81.2±0.2 77.0±1.4 69.1±0.3 82.1±0.2 60.0±0.3 71.2

MCC + Consistency 57.6±0.2 84.1±0.3 78.3±0.1 69.2±0.2 83.3±0.3 60.8±0.1 72.2

UDA

SSL

y=0 0c

y=1 Distribution !!$ in SSL or target domain % in UDA

0.5

d 1.0

x

!"#$$ of labeled data in SSL or source domain ! in UDA

Figure 3: A toy example bridging SSL and UDA.

Datasets
Office31 [37] OfficeHome [46] VisDA-2017 [36] DomainNet [35]

Num. of Domains
3 4 2 6

Num. of Classes
31 65 12 345

Table 1: Summary of adopted datasets.

Num. of Samples
4.1K 15.5K 280.2K 586.6K

4 Experiments
The adopted datasets are briefly summarized in Table 1 and detailed in the appendices. For datasets of VisDA-2017 and DomainNet, we adopt the source and target training sets for training and report the performance on the target training set and target test set in the transductive UDA setting and inductive UDA setting, respectively. For datasets of Office31 and OfficeHome, we adopt all data in source and target domains for training and report the results on target data in the transductive UDA

6

UDA

Table 4: Transductive and inductive results on the large-scale VisDA-2017 dataset (ResNet101).

Methods

Transductive

Inductive

Acc. over Ins. Acc. over Cate. Acc. over Ins. Acc. over Cate.

Source Only

55.4±1.1

51.0±0.8

52.3±2.1

50.4±1.6

DANN [15]

74.1±0.7

79.1±0.6

72.0±1.2

71.3±1.1

MCD [38]

76.8±0.6

78.1±0.9

73.3±0.1

71.5±0.0

CDAN [30]

77.3±0.6

81.3±0.6

74.6±1.0

73.5±1.3

AFN [51]

75.5±0.1

76.0±0.3

74.4±0.6

73.9±2.7

MDD [58]

77.8±0.8

80.9±0.4

75.0±0.2

73.7±0.2

Self-ensembling [14] 80.2±0.9

80.3±1.3

74.4±0.6

75.0±0.6

MCC [21]

80.3±0.4

83.3±0.4

78.5±0.5

78.3±0.4

-Model [26]

65.4±0.3

60.4±0.2

59.6±0.1

57.5±0.4

VAT [32]

62.4±0.2

62.3±0.2

62.5±0.5

60.1±0.4

Mean Teacher [44]

63.0±0.5

59.8±0.7

63.4±0.6

60.7±0.6

Entropy mini. [18]

74.9±0.2

74.8±0.1

69.4±0.3

66.1±0.3

Self-training [27]

75.2±0.2

74.0±0.3

73.1±0.1

70.5±0.1

MixMatch [4]

78.4±0.6

78.9±0.7

74.8±0.6

73.6±0.8

Xie et al. [49]

79.0±0.7

79.0±0.9

76.7±0.7

75.7±1.3

FixMatch [41]

80.5±0.1

81.1±0.2

78.6±0.5

77.9±0.8

MDD + Consistency 78.9±1.0

76.9±3.0

75.4±3.5

76.4±3.9

MCC + Consistency 82.6±0.4

85.0±0.5

82.8±0.8

83.1±0.8

Oracle

­

­

88.2±0.1

88.2±0.2

SSL

setting following the standard setup [15]. We report the classification accuracy over instances on all datasets and additionally report the accuracy over categories on the VisDA-2017 dataset.
We adopt the public library [22], which is licensed under the MIT License, to implement the Source Only, DANN [15], Self-ensembling [14], CDAN [30], MCD [38], MDD [58], AFN [51], and MCC [21], since models in [22] are well-tuned and achieve superior results over official implementations. In Oracle settings of VisDA-2017 and DomainNet, we fine-tune the model with labeled target training data and report results on target test data, which sets the upper bound. We report results of mean±std with experiments of three random seeds. We run all experiments with eight RTX-2080Ti GPUs.

4.1 Strong UDA baselines with SSL methods

To employ SSL methods on UDA tasks, we set Dl = Ds and Du = Dt. The adopted SSL methods

[27, 18, 32, 26, 44, 4, 49, 41] are briefly introduced in the appendices. We adopt the popular network

architecture and optimization strategies [15] in all SSL methods for all UDA tasks: we adopt a

ResNet [19] pre-trained on the ImageNet dataset [12] as the feature extractor g after removing the

last fully connected (FC) layer, and add a new task classifier h of one FC layer; the overall model is

updated by stochastic gradient descent (SGD) with a momentum of 0.9; the learning rate is adjusted

by

p

=

0.01 (1+10p)0.75

,

where

p

is

the

progress

of

training

iterations

linearly

changing

from

0

to

1;

the

learning rate of the newly added task classifier is set to 10 times of the pre-trained parts. We set

most of the hyper-parameters in SSL methods following recommendations of the original papers and

empirically tune some hyper-parameters if necessary (e.g., when models do not converge). Therefore,

our results of SSL methods on UDA tasks are conservative estimates; however, such conservative

estimates already justify the efficiency of SSL methods on UDA problems.

Results on datasets of Office31, OfficeHome, VisDA-2017, and DomainNet are illustrated in Tables 2, 3, 4, and 5, respectively. All SSL methods consistently improve over the Source Only baseline except the MixMatch on the most challenging pntqdr task, justifying the effectiveness of SSL methods on UDA benchmarks. Especially, the state-of-the-art SSL method of FixMatch outperforms existing UDA methods by more than 2.0% on the largest UDA benchmark of DomainNet. Although MCC [21] and Self-ensembling [14] are proposed as UDA methods for UDA tasks, they are variants of SSL methods. Specifically, Self-ensembling is a variant of the mean-teacher [44] and MCC shares similar objectives to entropy minimization [18] and self-training [27] (cf. Section 3.1). Their superior results also imply the efficacy of SSL methods on UDA tasks.

7

Table 5: Inductive results on the large-scale DomainNet dataset (ResNet50). Transductive results are

given in the appendices.

Methods

clpinf infpnt pntqdr qdrreal realskt sktclp Avg.

Source Only

17.5±0.2 32.2±0.4 3.7±0.3 6.0±0.1 34.5±0.3 46.6±0.2 23.4

DANN [15]

20.4±0.1 27.1±0.1 6.2±0.3 15.6±0.4 40.0±0.3 48.0±0.3 26.2

CDAN [30]

20.4±0.2 28.9±0.1 3.4±0.1 16.0±0.2 42.2±0.1 49.6±0.5 26.7

UDA

MCD [38]

19.8±0.2 34.1±0.1 3.4±0.2 16.6±0.1 41.3±0.1 50.4±0.1 27.6

AFN [51]

18.8±0.4 35.5±0.2 6.2±0.4 16.5±0.1 37.9±0.1 51.2±0.3 27.7

MDD [58]

21.7±0.1 34.9±0.2 3.6±0.1 19.8±0.2 44.6±0.1 54.0±0.1 29.7

Self-ensembling [14] 18.1±0.1 32.9±0.1 5.1±0.1 15.7±0.6 41.7±0.1 50.0±0.3 27.3

MCC [21]

20.0±0.1 37.6±0.3 5.0±0.1 14.7±0.2 37.7±0.2 54.7±0.2 28.2

-Model [26]

18.0±0.1 34.2±0.1 5.3±0.1 11.5±0.1 35.0±0.1 48.6±0.2 25.4

VAT [32]

17.6±0.2 33.8±0.1 4.2±0.1 14.5±0.2 35.3±0.1 48.6±0.1 25.7

Mean Teacher [44] 17.7±0.2 33.9±0.2 5.7±0.2 13.6±0.1 35.2±0.1 48.3±0.1 25.7

SSL

Entropy mini. [18] 17.1±0.1 36.1±0.1 4.4±0.2 14.4±0.5 37.7±0.1 52.7±0.1 27.1

MixMatch [4]

18.5±0.1 37.4±0.1 1.2±0.2 16.7±0.1 37.4±0.1 54.5±0.1 27.6

Self-training [27]

18.3±0.1 37.2±0.1 4.0±0.2 17.1±0.2 40.6±0.1 53.5±0.1 28.5

Xie et al. [49]

20.9±0.2 37.3±0.5 5.8±0.1 17.5±0.4 43.5±0.1 56.4±0.2 30.2

FixMatch [41]

21.8±0.2 40.7±0.5 4.4±0.3 18.4±0.4 45.9±0.2 59.2±0.2 31.7

MCC + Consistency 19.5±0.1 39.0±0.1 4.2±0.2 16.2±0.1 38.4±0.1 55.3±0.1 28.8

MDD + Consistency 23.1±0.6 41.6±0.1 2.1±0.5 15.1±0.1 50.7±0.2 61.9±0.2 32.4

Oracle

40.5±0.1 69.5±0.1 71.1±0.4 82.5±0.1 67.6±0.1 75.4±0.2 67.8

Acc. improved over Labeled Only (%)

60

50

FixMatch Self-training

40

DANN

30

CDAN

20

MCC

10

0

(a) 250 labeled samples

Acc. improved over Labeled Only (%)

16

14

FixMatch

12

Self-training

10

DANN

8

CDAN

6

MCC

4

2

0

(b) 4000 labeled samples

Inductive Cate. Acc (%)

80
SSL loss on f=h  g 75 SSL loss on g only
70
65
60
55
50 -Model EM ST UDA FixMatch
(c) SSL implementations

Transductive Error

Source Only

0.8

DANN CDAN

MCC

0.7

Self-Training FixMatch

0.6

0.5

0.4 Training proceeds ( )
(d) Convergence

Figure 4: (a)-(b): Performance improvement over the Labeled Only on the Cifar10 dataset with (a) 250 and (b) 4000 labeled samples in the SSL task. (c): Inductive results on VisDA-2017 dataset with settings of `SSL loss on f = h  g' (8) and `SSL loss on g only' (cf. appendices for details). We abbreviate entropy minimization and self-training to EM and ST, respectively. (d): Convergence performance on the sktclp task of the DomainNet dataset.

We note that some works [33, 52] presented that SSL methods performed poorly when there was a class distribution shift across labeled and unlabeled data; in contrast, we find that SSL methods perform well when there is a marginal distribution shift (i.e., under the covariate shift assumption 1), facilitating a deeper understanding of their application fields.
4.2 Ablation study and analyses
Combining UDA and SSL methods for UDA tasks. We find that state-of-the-art UDA methods could be boosted with SSL techniques. Specifically, we combine the popular consistency regularization in SSL [49, 41] with UDA methods of MCC [21] and MDD [58], leading to boosted results and the new state of the art, as illustrated in Tables 2, 3, 4, and 5. The boosted results also justify the efficacy of SSL techniques on UDA tasks. The combining strategies are detailed in the appendices.
Employing UDA methods for SSL tasks. Given the close relationship between UDA and SSL and the superior results of SSL methods on UDA tasks, one may consider employing UDA methods on SSL tasks. As illustrated in Figures 4(a) and 4(b), UDA methods of DANN [15], CDAN [30] and MCC [21] marginally outperform the `Labeled only' baseline, presenting limited efficacy on SSL tasks. We also attempt to combine the state-of-the-art SSL method of FixMatch [41] with classical UDA techniques (e.g., adversarial distribution minimization in [15, 30]) for SSL tasks, but we observe no improvement over vanilla FixMatch.
8

A-dis A-dis
Acc (%) Acc (%)

80

2.0 1.9

2.0

Source Only

Self-training

DANN

1.5

Source Only Self-training DANN

75

Our data pre-processing Dalib's data pre-processing

32 30

Our data pre-processing Dalib's data pre-processing

1.8

CDAN FixMatch

CDAN

FixMatch

70

28

1.7

MCC

1.0

MCC

26

1.6

65

0.5

24

1.5

1.4

0.0

60 DANN

MDD

MCC

22 DANN

MDD

MCC

(a) VisDA-2017

(b) AW

(c) OfficeHome

(d) DomainNet

Figure 5: (a)-(b): A-distance between source and target features on the (a) VisDA-2017 dataset and (b) AW task of Office31 dataset, where values are rounded to the level of 0.01. (c)-(d): Results with different data pre-processing strategies on the datasets of (c) OfficeHome and (d) DomainNet.

We note that a technique termed distribution alignment is adopted in the recent SSL method [3]; however, it is different from the popular distribution alignment in UDA. Berthelot et al. [3] aim to align the marginal class distribution across labeled and unlabeled data, which is motivated by the input-output mutual information maximization objective. On the contrary, the popular distribution alignment in UDA aligns the marginal feature distribution across labeled and unlabeled data.
Applying SSL losses on the feature extractor g only for UDA tasks. Considering the domain shift, researchers typically apply the entropy minimization loss on the feature extractor g only for UDA tasks [54, 57]. We analyze whether this benefit could generalize to other SSL regularizations (7). As illustrated in Figure 4(c), better results of SSL methods on UDA benchmarks are typically achieved by applying SSL losses on the feature extractor g only (cf. the appendices for details), which is adopted as the default implementation of SSL methods for UDA tasks in this paper.
Convergence performance. We illustrate the convergence performance on the sktclp task of the DomainNet dataset in Figure 4(d). UDA and SSL methods both converge smoothly.
Domain divergence. DANN [15] and CDAN [30], which are designed for the divergence minimization, consistently and significantly minimize the distribution divergence on various tasks, as illustrated in Figures 5(a) and 5(b). On the contrary, most SSL methods and their variants marginally reduce the domain divergence; although FixMatch considerably reduces the domain divergence on the AW task, it hardly reduces the divergence on the VisDA-2017 dataset, a task with larger domain divergence, which distinguishes it from the seminal UDA methods (e.g., DANN and CDAN). In practice, SSL methods and their variants currently achieve the state of the art, as shown in Tables 2, 3, 4, and 5. Similar to [7, 59], this empirical evidence challenges the dominant position of divergence minimization in UDA studies and suggests another promising UDA solution of SSL methods.
Data pre-processing. As illustrated in Figures 5(c) and 5(d), results on UDA tasks are significantly influenced by the data pre-processing strategies; for example, different data pre-processing strategies lead to more than 2% accuracy divergence for the MCC method on the DomainNet dataset, which is even larger than its improvement over most compared methods. Thus, we promote that researchers should compare different methods on UDA tasks with the same data pre-processing, which is less concerned in previous UDA studies. We detail the data pre-processing strategies in the appendices.
5 Conclusions and discussions
In this work, we bridged UDA and SSL by revealing that SSL is a special case of UDA problems. By adapting eight representative SSL methods on UDA benchmarks, we showed that SSL methods were strong UDA learners. Specifically, the state-of-the-art SSL method of FixMatch outperformed existing UDA methods by more than 2.0% on the challenging DomainNet benchmark, and the new state of the art could be achieved by empowering UDA methods with SSL techniques. We thus promoted that SSL methods should be employed as baselines in future UDA studies. Our work bridged existing tasks of UDA and SSL; thus, it inherited their negative impacts (e.g., abused models).
However, in experiments, we conducted the model selection (e.g., choosing training checkpoints) according to results on labeled target data, which is actually a common problem in the UDA community; although the upper bound performance of SSL and UDA methods [22] were fairly compared, we expect to compare them on UDA tasks with appropriate model selection strategies in the future.
9

References
[1] Mikhail Belkin, Irina Matveeva, and Partha Niyogi. Regularization and semi-supervised learning on large graphs. In International Conference on Computational Learning Theory, pages 624­638. Springer, 2004.
[2] Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. Machine learning, 79(1-2):151­175, 2010.
[3] David Berthelot, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, and Colin Raffel. Remixmatch: Semi-supervised learning with distribution matching and augmentation anchoring. In International Conference on Learning Representations, 2019.
[4] David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A holistic approach to semi-supervised learning. In Advances in Neural Information Processing Systems, pages 5049­5059, 2019.
[5] Konstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan, and Dilip Krishnan. Unsupervised pixel-level domain adaptation with generative adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3722­3731, 2017.
[6] Olivier Chapelle, Bernhard Scholkopf, and Alexander Zien. Semi-supervised learning (chapelle, o. et al., eds.; 2006)[book reviews]. IEEE Transactions on Neural Networks, 20(3):542­542, 2009.
[7] Xinyang Chen, Sinan Wang, Mingsheng Long, and Jianmin Wang. Transferability vs. discriminability: Batch spectral penalization for adversarial domain adaptation. In International conference on machine learning, pages 1081­1090. PMLR, 2019.
[8] Corinna Cortes, Yishay Mansour, and Mehryar Mohri. Learning bounds for importance weighting. In Nips, volume 10, pages 442­450. Citeseer, 2010.
[9] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 702­703, 2020.
[10] Rozita Dara, Stefan C Kremer, and Deborah A Stacey. Clustering unlabeled data with soms improves classification of labeled real-world data. In Proceedings of the 2002 International Joint Conference on Neural Networks. IJCNN'02 (Cat. No. 02CH37290), volume 3, pages 2237­2242. IEEE, 2002.
[11] Shai Ben David, Tyler Lu, Teresa Luu, and Dávid Pál. Impossibility theorems for domain adaptation. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pages 129­136. JMLR Workshop and Conference Proceedings, 2010.
[12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248­255. Ieee, 2009.
[13] Zhengming Ding, Sheng Li, Ming Shao, and Yun Fu. Graph adaptive knowledge transfer for unsupervised domain adaptation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 37­52, 2018.
[14] Geoff French, Michal Mackiewicz, and Mark Fisher. Self-ensembling for visual domain adaptation. In International Conference on Learning Representations, 2018.
[15] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The Journal of Machine Learning Research, 17(1):2096­2030, 2016.
[16] Ross B. Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In 2014 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2014, Columbus, OH, USA, June 23-28, 2014, pages 580­587. IEEE Computer Society, 2014.
[17] Mingming Gong, Kun Zhang, Tongliang Liu, Dacheng Tao, Clark Glymour, and Bernhard Schölkopf. Domain adaptation with conditional transferable components. In International conference on machine learning, pages 2839­2848. PMLR, 2016.
[18] Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In Advances in Neural Information Processing Systems 17 [Neural Information Processing Systems, NIPS 2004, December 13-18, 2004, Vancouver, British Columbia, Canada], pages 529­536, 2004.
10

[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770­778, 2016.
[20] Martin Szummer Tommi Jaakkola and Martin Szummer. Partially labeled classification with markov random walks. Advances in neural information processing systems (NIPS), 14:945­952, 2002.
[21] Ying Jin, Ximei Wang, Mingsheng Long, and Jianmin Wang. Minimum class confusion for versatile domain adaptation. In European Conference on Computer Vision, pages 464­480. Springer, 2020.
[22] Mingsheng Long Junguang Jiang, Bo Fu. Transfer-learning-library. https://github.com/thuml/ Transfer-Learning-Library, 2020.
[23] Guoliang Kang, Lu Jiang, Yi Yang, and Alexander G Hauptmann. Contrastive adaptation network for unsupervised domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4893­4902, 2019.
[24] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097­1105, 2012.
[25] Anders Krogh and John A Hertz. A simple weight decay can improve generalization. In Advances in neural information processing systems, pages 950­957, 1992.
[26] Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.
[27] Dong-Hyun Lee. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In Workshop on challenges in representation learning, ICML, volume 3, page 2, 2013.
[28] Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-image translation networks. arXiv preprint arXiv:1703.00848, 2017.
[29] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I. Jordan. Learning transferable features with deep adaptation networks. In Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37, ICML'15, pages 97­105. JMLR.org, 2015.
[30] Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Conditional adversarial domain adaptation. In Advances in Neural Information Processing Systems, pages 1640­1650, 2018.
[31] Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Unsupervised domain adaptation with residual transfer networks. In Advances in Neural Information Processing Systems, pages 136­144, 2016.
[32] Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. IEEE transactions on pattern analysis and machine intelligence, 41(8):1979­1993, 2018.
[33] Avital Oliver, Augustus Odena, Colin A Raffel, Ekin Dogus Cubuk, and Ian Goodfellow. Realistic evaluation of deep semi-supervised learning algorithms. In Advances in neural information processing systems, pages 3235­3246, 2018.
[34] Sinno Jialin Pan, Qiang Yang, et al. A survey on transfer learning. IEEE Transactions on knowledge and data engineering, 22(10):1345­1359, 2010.
[35] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In Proceedings of the IEEE International Conference on Computer Vision, pages 1406­1415, 2019.
[36] Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and Kate Saenko. Visda: The visual domain adaptation challenge. arXiv preprint arXiv:1710.06924, 2017.
[37] Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category models to new domains. In European conference on computer vision, pages 213­226. Springer, 2010.
[38] Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tatsuya Harada. Maximum classifier discrepancy for unsupervised domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3723­3732, 2018.
[39] Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-likelihood function. Journal of statistical planning and inference, 90(2):227­244, 2000.
11

[40] Rui Shu, Hung H Bui, Hirokazu Narui, and Stefano Ermon. A dirt-t approach to unsupervised domain adaptation. arXiv preprint arXiv:1802.08735, 2018.
[41] Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Han Zhang, and Colin Raffel. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. arXiv preprint arXiv:2001.07685, 2020.
[42] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):1929­1958, 2014.
[43] Hui Tang, Ke Chen, and Kui Jia. Unsupervised domain adaptation via structurally regularized deep clustering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8725­8735, 2020.
[44] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In Advances in neural information processing systems, pages 1195­1204, 2017.
[45] Jesper E Van Engelen and Holger H Hoos. A survey on semi-supervised learning. Machine Learning, 109(2):373­440, 2020.
[46] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5018­5027, 2017.
[47] Mei Wang and Weihong Deng. Deep visual domain adaptation: A survey. Neurocomputing, 2018.
[48] Garrett Wilson and Diane J Cook. A survey of unsupervised deep domain adaptation. ACM Transactions on Intelligent Systems and Technology (TIST), 11(5):1­46, 2020.
[49] Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc Le. Unsupervised data augmentation for consistency training. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 6256­6268. Curran Associates, Inc., 2020.
[50] Shaoan Xie, Zibin Zheng, Liang Chen, and Chuan Chen. Learning semantic representations for unsupervised domain adaptation. In International conference on machine learning, pages 5423­5432. PMLR, 2018.
[51] Ruijia Xu, Guanbin Li, Jihan Yang, and Liang Lin. Larger norm more transferable: An adaptive feature norm approach for unsupervised domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1426­1435, 2019.
[52] Qing Yu, Daiki Ikami, Go Irie, and Kiyoharu Aizawa. Multi-task curriculum framework for open-set semi-supervised learning. In European Conference on Computer Vision, pages 438­454. Springer, 2020.
[53] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. ICLR, 2017.
[54] Jing Zhang, Zewei Ding, Wanqing Li, and Philip Ogunbona. Importance weighted adversarial nets for partial domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 8156­8164, 2018.
[55] Yabin Zhang, Bin Deng, Kui Jia, and Lei Zhang. Label propagation with augmented anchors: A simple semi-supervised learning baseline for unsupervised domain adaptation. In European Conference on Computer Vision, pages 781­797. Springer, 2020.
[56] Yabin Zhang, Bin Deng, Hui Tang, Lei Zhang, and Kui Jia. Unsupervised multi-class domain adaptation: Theory, algorithms, and practice. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.
[57] Yabin Zhang, Hui Tang, Kui Jia, and Mingkui Tan. Domain-symmetric networks for adversarial domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5031­5040, 2019.
[58] Yuchen Zhang, Tianle Liu, Mingsheng Long, and Michael Jordan. Bridging theory and algorithm for domain adaptation. In International Conference on Machine Learning, pages 7404­7413, 2019.
[59] Han Zhao, Remi Tachet Des Combes, Kun Zhang, and Geoffrey Gordon. On learning invariant representations for domain adaptation. In International Conference on Machine Learning, pages 7523­7532. PMLR, 2019.
12

[60] Dengyong Zhou, Olivier Bousquet, Thomas N Lal, Jason Weston, and Bernhard Schölkopf. Learning with local and global consistency. In Advances in neural information processing systems, pages 321­328, 2004.
[61] Xiaojin Zhu. Semi-supervised learning literature survey. Tech. Report University of Wisconsin Madison, 2008.
[62] Yang Zou, Zhiding Yu, BVK Kumar, and Jinsong Wang. Unsupervised domain adaptation for semantic segmentation via class-balanced self-training. In Proceedings of the European conference on computer vision (ECCV), pages 289­305, 2018.
[63] Yang Zou, Zhiding Yu, Xiaofeng Liu, BVK Kumar, and Jinsong Wang. Confidence regularized self-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5982­5991, 2019.
13

