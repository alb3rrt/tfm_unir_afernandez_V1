END-TO-END DEEP LEARNING-BASED ADAPTATION CONTROL FOR FREQUENCY-DOMAIN ADAPTIVE SYSTEM IDENTIFICATION
Thomas Haubner, Andreas Brendel, and Walter Kellermann
Multimedia Communications and Signal Processing, Friedrich-Alexander-University Erlangen-Nu¨rnberg, Cauerstr. 7, D-91058 Erlangen, Germany, thomas.haubner@fau.de

arXiv:2106.01262v1 [eess.AS] 2 Jun 2021

ABSTRACT
We present a novel end-to-end deep learning-based adaptation control algorithm for frequency-domain adaptive system identification. The proposed method exploits a deep neural network to map observed signal features to corresponding step-sizes which control the filter adaptation. The parameters of the network are optimized in an end-to-end fashion by minimizing the average system distance of the adaptive filter. This avoids the need of explicit signal power spectral density estimation as required for model-based adaptation control and further auxiliary mechanisms to deal with model inaccuracies. The proposed algorithm achieves fast convergence and robust steady-state performance for scenarios characterized by nonwhite and non-stationary noise signals, time-varying environment changes and additional model inaccuracies.
Index Terms-- System Identification, Adaptation Control, Deep Learning, Frequency-Domain Adaptive Filtering
1. INTRODUCTION
Adaptive system identification is required for many modern signal enhancement approaches, e.g., in full-duplex acoustic communication devices for the purpose of acoustic echo cancellation (AEC) [1]. Despite the recently increased focus on direct deep learning-based signal enhancement algorithms [2, 3], the benefit of additionally using model-based system identification has shown to be beneficial, especially when computational load should be minimized [4, 5].
However, the benefits of physical models are only fully exploited if the optimum model parameters can be quickly and robustly identified. For this gradient descent-based model parameter updates have proven to be a powerful approach [6, 7] when combined with carefully-designed adaptation control to deal with interfering signals and noise (jointly termed 'noise' in the sequel) [8] and model inaccuracies, e.g., undermodeling the filter length [9]. During the last decades a plethora of adaptation control methods have been
This work was supported by the German Research Foundation 282835863 - within the Research Unit FOR2457 Acoustic Sensor Networks.

proposed, ranging from simplistic binary stall-or-adapt approaches [10, 11] to sophisticated model-based step-size estimators [12, 13, 14, 15] and learning-based combination of step-size selection schemes [16]. Most adaptation control approaches assume simplifying probabilistic signal models to estimate a time-varying step-size by minimizing the mean squared error signal or a system distance between the estimated frequency response (FR) and the true FR. In particular the step-size inference by a diagonalized discrete Fourier transform (DFT)-domain Kalman filter (KF) [17] performs robustly in scenarios challenged by non-white and nonstationary noise signals. However, the performance of these model-based approaches depends on the validity of the model assumptions and robust estimation of the required statistics [18]. Especially the estimation of statistics corresponding to unobserved quantities, e.g., the noise power spectral density (PSD), poses a difficult problem [18]. Various estimators have been proposed, e.g., [19, 20, 21], whose performance often crucially depends on the choice of additional hyperparameters and the considered application. Recently, the exploitation of machine learning-based noise PSD estimators [5, 22] has shown significant improvements relative to classical, i.e., non-trainable, estimators. Yet, these approaches still rely on simplistic random walk models to describe the temporal evolution of the FR [17] and require a sophisticated cost function design for PSD estimation [23].
Thus, we introduce in this paper an end-to-end deep learning-based adaptation control algorithm for frequencydomain adaptive system identification which we term deep neural network-controlled frequency-domain adaptive filter (DNN-FDAF). We propose to learn a mapping from observable signal features to step-sizes by a DNN with the average system distance of the step-size-controlled adaptive filter as a loss function. By optimizing the DNN parameters directly w.r.t. the system distance, we avoid the estimation of auxiliary signal statistics for model-based adaptation control whose effect on the system identification performance is unclear if the model assumptions are not fulfilled perfectly. In addition, it circumvents the need for any applicationdependent hyperparameter tuning inherent to many baseline approaches. The proposed method combines the benefits of

x

DNN

D NN

w^ 

d

n d

e

- +

y

Fig. 1: Block diagram of the proposed DNN-FDAF adaptation control algorithm for online system identification.

physically-motivated system identification approaches, e.g.,
generalization to unknown environments, with the excellent
modeling capability of DNNs for adaptation control.
We use bold lowercase letters for vectors and bold upper-
case letters for matrices with underlined symbols indicating
time-domain quantities. The identity matrix and DFT matrix of dimensions D × D are denoted by ID and F D, respectively, and the all-zero matrix of dimensions D1 × D2 by 0D1×D2. Furthermore, we introduce the diag(·) operator which generates a diagonal matrix from its vector-valued argument and indicate its mth element by [·]mm. The transposition and Hermitian transposition of a matrix are represented by (·)T and (·)H , respectively. Finally, we use the Euclidean norm || · ||2 and the expectation operator E[·].

2. ADAPTIVE SYSTEM IDENTIFICATION

In the following, the online identification of an acoustic model describing the multi-path propagation from a source, e.g., a loudspeaker, to a microphone as shown in Fig. 1 is considered. We model a block of noisy microphone observations at block index 

T

y=


y


R-R+1

,

y


R-R+2,

.

.

.

,

y


R

 RR

(1)

as a linear superposition

y


=

d

+ n



RR

(2)

of a noise-free observation component d and a noise component n . The noise-free observation component d is described by a linear convolution of an observable input signal
block

x = x R-M+1, x R-M+2, . . . , x R T  RM (3)
with a finite impulse response (FIR) filter w  RL of length L = M - R modeling the multi-path propagation. The linear convolution can efficiently be implemented by overlap-save processing in the DFT domain

d = QT1F -M1X w  RR

(4)

with the FR w = F M Q2w  CM , the DFT-domain input signal matrix X = diag (x ) = diag (F M x )  CM×M and the zero-padding matrix QT2 = IM-R 0M-R×R . Note that QT1 = 0R×M-R IR ensures a linear con-
volution by constraining the inverse DFT of the product

X w . By inserting the propagation model (4) into the

signal model (2) and pre-multiplying with the transforma-

tion matrix F M Q1, we obtain the DFT-domain observation model

y = C w + n  CM ,

(5)

with

C

=

F

M

Q1QT1

F

-1 M

X



being

the

overlap-save

con-

strained input signal matrix and the DFT-domain microphone

and noise blocks y = F M Q1y and n = F M Q1n , respectively.

The unknown FR w is usually estimated by iteratively

applying the update rule [24]

e = y - d = y - C w^  -1

(6)

w^ 

=

w^  -1

+

Q3

X

H 

e

(7)

which represents a block-based frequency-domain implemen-

tation of the least mean square (LMS) algorithm [25] and is

often termed FDAF [6, 26]. Here, the preceding estimate

w^ -1 is updated by a multiplication of the stochastic gra-

dient XH e , including the prior error e , with a diagonal

step-size matrix   RM×M . Note that the gradient pro-

jection

matrix

Q3

=

F

M Q2QT2F

-1 M

ensures

that

the

FR

esti-

mate w^  corresponds to a zero-padded time-domain FIR filter

w^  = QT2F -M1w^  .

3. ADAPTATION CONTROL
The convergence rate, steady-state performance and noiserobustness of the adaptive system identification algorithm described in Sec. 2 decisively depends on an accurate choice of the step-size matrix  . In the following, we will introduce a DNN-based method which infers the step-size matrix  from the observed signals x and e as shown in Fig. 1.

3.1. Model-Based Adaptation Control
We start by discussing state-of-the-art model-based adaptation control which will serve as a motivation for the proposed method in Sec. 3.2. The vast majority of these approaches suggests a computation of the step-size matrix

M B = fMB X X, N N, WW

(8)

from the input signal PSD matrix X X = E x xH , the noise signal PSD matrix N N = E n nH and the uncertainty of the FR estimate WW = E  w  wH with  w = w^  - w . Prominent examples are the classical

FDAF update [6, 26]

FDAF mm =

µFDAF X X mm

(9)

with µFDAF being a hyperparameter, and the diagonalized KF update [17, 19, 20]

K F mm = [x xH ]mm

WW mm

 WW

+
mm

M R

. N N mm
(10)

The latter can be considered as a noise-robust state-of-the-art

approach.

Note that the factor

M R

in Eq. (10) is a normaliza-

tion factor which results from zero-padding n (cf. Eq. (2))

before applying the DFT to obtain Eq. (5) [17]. However, the

system identification performance of these model-based ap-

proaches crucially depends on a robust estimation of the re-

spective signal statistics (cf. Sec. 1). While the estimation of the input signal PSD matrix X X is straightforward due to the knowledge of x , the estimation of statistics corresponding to unobserved quantities, e.g., N N and WW, is still an un-
solved problem and has been explored extensively [19, 20].

3.2. Deep Neural Network-based Adaptation Control

We suggest to replace the model-based mapping of signal statistics to step-size matrices (cf. Eq. (8)) by a learned mapping fML of the observable input signal sequence x1, . . . , x and prior error signal sequence e1, . . . , e :

D NN = fML (x1, e1, . . . , x , e ; ) .

(11)

The parameter vector  of the function fML is optimized in
a training phase (cf. Sec. 3.3). However, a direct estimation of D NN by a DNN is complicated by the non-stationarity of the respective signals. Thus, we suggest to exploit the do-

main knowledge from optimum model-based adaptation con-

trol (cf. Sec. 3.1), by using the following DFT bin-wise step-

size

D NN mm =

µMAX

[M

µ 

]mm

^ X X

+

M R

^ PP

mm

(12)

with

^ X X = X ^ X -X1 + (1 - X) x xH

(13)

^ PP = P ^ PP-1 + (1 - P) p^ p^H

(14)

p^

=

M

e 

e

(15)

where

the

diagonal

masking

matrices

M

µ 

and

M

e 

are

in-

ferred by the DNN and where X and P are time constants

for recursive averaging. By embedding the signal power nor-

malization into the structure of the learning-based step-size

mapping (11), the DNN needs to model a significantly reduced dynamic range in contrast to directly estimating D NN.

ufeat,

fFinF

 -1 fGRU

fFµF

M

µ 

fFeF

M

e 

Fig. 2: Proposed DNN architecture which maps the feature

vector

ufeat,

to

the

diagonal

masking

matrices

M

µ 

and

M

e 

.

The step-size (12) is motivated by adding an error powerdependent normalization term ^ PP, similar to the noise PSD matrix N N in the KF update (10), to the FDAF step-size (9) and estimating the decisive hyperparameter µFDAF by a
frequency-selective DNN. To account for the different causes

of large error powers, i.e., observation noise n or system

mismatch  w , and their antipodal effect on the adaptation

rate,

a

DNN-estimated

mask

M

e 

is

applied

to

the

error

sig-

nal e before computing ^ PP (cf. Eqs. (14) and (15)). Note

that despite the structural similarity of the step-sizes (10) and

(12), ^ PP is not necessarily an estimate of the noise signal PSD matrix N N. Its actual meaning depends on the cost

function that is used to train the DNN (cf. Sec. 3.3).

We suggest the DNN architecture shown in Fig. 2 to map

the observed feature vector ufeat, (cf. Eq. (16)) to the diag-

onal

masking

matrices

M

µ 

and

M

e 

required

in

Eqs.

(12)

and (15). The architecture is motivated by the creation of

a condensed feature representation after the gated recurrent

unit (GRU) layer which includes all important effects influ-

encing the adaptation control, e.g., noise activity and filter

convergence state. For the input feature vector to the DNN

we use the normalized logarithmic power spectrum of the in-

put signal x and the prior error signal e to obtain

[ufeat, ]m

=

log max(|[usig, ]m|2, ) []m

-

[]m

(16)

which is computed from the complex signal vector

usig, =

Q4 e Q4 x

 CM+2.

(17)

The mean and element-wise standard deviation vectors of the feature vector ufeat, are denoted by  and , respectively, and can be estimated from the training data. Note that the ma-

trix Q4 =

I

M 2

+1

0M
2

+1×

M 2

-1

selects the non-redundant

part of the conjugate symmetric signals in (17) and  > 0 en-

sures a positive argument of the logarithm. The feature vec-

tor ufeat, is mapped by a feedforward layer with tanh activation fFinF to a lower dimension P . Subsequently, two stacked GRU layers fGRU extract temporal dependencies of the com-

pressed feature vectors. Finally, the GRU states are mapped

by two different feedforward networks fFµF and fFeF with sig-

moid activations to the diagonal entries of the masking ma-

trices

M

µ 

and

M

e 

.

The sigmoid activations at the output

Algorithm 1 Proposed DNN-FDAF algorithm for online sys-

tem identification using Ttest sequential testing signal blocks.

for  = 1, . . . , Ttest do

Compute prior error block e by Eq. (6)

Compute feature vector ufeat, for the DNN by Eq. (16)

Infer

masking

matrices

M

µ 

and

M

e 

(cf.

Fig.

2)

Compute step-size matrix D NN by Eqs. (12) - (15)

Update FR estimate w^  by Eq. (7)

end for



Dj,N1N

Dj,N2N

...

Dj,NTN

w^ j,0

w^ j,1

w^ j,2

...

w^ j,T

layers ensure that all elements of the masking matrices lie in the range [0, 1]. This contributes to the robustness of the approach by limiting the numerator to µMAX and the norm of p^ (cf. Eq. (15)) to ||e ||2. Furthermore, note that due to the conjugate symmetry of the DFT-domain representation it suf-
fices to compute the nonredundant part of the masking matri-
ces. An algorithmic description of the proposed DNN-FDAF
algorithm is given in Alg. 1.

3.3. Cost Function Design for Neural Network Training

The system identification performance of an adaptive filter

is often quantified by the normalized logarithmic system dis-

tance [1]



=

10 log10

||w - ||w

w^  ||22

||22

.

(18)

Note that  is usually called distance although it does not fulfil the properties of a metric on the vector space RL. How-

ever, due to the complex interaction of the DNN outputs, i.e.,

the

masking

matrices

M

µ 

and

M

e 

(cf.

Fig.

2),

via

a

sequence

of filter updates (cf. Eqs. (7) and (12) - (15)) on the system

distance  , a hand-crafted design of optimum target masking matrices is problematic. Thus, we suggest an end-to-end

approach by directly optimizing the DNN parameter vector 

w.r.t. to the average block-based system distance

J () = 1 J JT

T
j, ,

(19)

j=1  =1

with J and T being the number of training sequences and signal blocks, respectively, and j, denoting the system distance (18) at block  in training sequence j. The cost function (19) quantifies the direct effect of different masking matrices on the average system identification performance of the adaptive filter and renders the design of desired target signal statistics and hyperparameters unnecessary. The end-to-end training of the DNN requires to backpropagate the average system distance (19) through the adaptive filter updates (7) to the DNN parameter vector . This complex relation between the cost function terms j, , the FR estimates w^ j, , the step-size matrices Dj,NN and the DNN parameters  is shown in Fig. 3. Finally, note that due to the independency of the system distance (18) on the signal characteristics, the cost

j,1

j,2

...

j,T

wj,1

wj,2

...

wj,T

Fig. 3: Visualizing the relationship between the cost function terms j, and the network parameters .

function (19) is well-suited to quantify the system identification performance for non-stationary input signals as typically encountered in acoustic applications.

4. EXPERIMENTS

In this section, we evaluate the proposed algorithm for a large

variety of challenging acoustic system identification scenar-

ios which are motivated by an AEC application. The scenar-

ios are characterized by abrupt changes of acoustic impulse

responses (AIRs) and non-stationary and non-white input x
and noise signals n . The noise-free observation component d of each scenario is simulated by randomly drawing an input signal x from a subset of the LibriSpeech database [27], including 143 speakers, and convolving it with a randomly-

selected true AIR w~   RK from the databases [28, 29, 30], comprising 201 different AIRs. Note that, as the length K

of the true AIRs w~  is much larger than the modeled filter

length L in all considered scenarios, we can only estimate

the first L taps of w~  , i.e., we choose w = QT5w~  with QT5 = IL 0L×K-L in Eq. (18). Subsequently, the micro-

phone

observation

y


is

computed

by

adding

a

noise

signal

n . Each noise signal consists of a superposition of a ran-

domly selected speaker from an additional subset of [27], in-

cluding 145 speakers, and a stationary white Gaussian signal.

Both noise components are scaled to yield a random SNR,

i.e., power of the noise-free component d w.r.t. the noise component, between -10 dB and 10 dB for the speaker, and

25 dB and 35 dB for the Gaussian component. The abrupt

system change is modeled by using different AIRs, input and

noise

signals

for

creating

the

observations

y


before

and

af-

ter a specific switching time. We sampled the switching time

randomly in the range [7.2s, 8.8s] to preclude overfitting of

the DNN to a deterministic point in time.

For all considered algorithms the sampling frequency fs is 16 kHz and the modeled filter length and frame shift are

set to L = 2048 and R = 1024, respectively. As baseline

algorithms we consider the KF approach [17] and an error-

aware version of the FDAF step-size (9) which is termed

EA-FDAF,

i.e.,

setting

M

µ 

=

M

e 

=

IM in Eq. (12)

and (15). Note that for the KF update the noise PSD ma-

trix is computed by recursively averaging the prior error

e with an averaging factor of 0.5 [19, 20]. In addition, we consider two choices of the state transition parameter

A, i.e., A = 0.99 and A = 0.999, as it highly affects the

steady-state and reconvergence performance [18]. For the

proposed DNN-FDAF algorithm we consider two additional

variants

which

are

termed

DNN-FDAF

(M

e 

=

0M×M )

and

DNN-FDAF

(M

µ 

=

IM ).

Here, the respective quantity in

brackets is fixed, i.e., it is not estimated by the DNN. The

parameter settings of the considered algorithms are summa-

rized in Tab. 1. Note that the maximum numerator step-size

µMAX

is

chosen

smaller

for

the

DNN-FDAF

(M

µ 

=

IM)

al-

gorithm to compensate for the deterministic numerator of the

step-size (12). The considered DNN architecture (cf. Fig. 2)

has approximately 2.4 million parameters with the output

dimension after the first dense layer being P = 256 and

 = 10-12. It was trained on 4.4 h of training data using the ADAM optimizer [31] with a learning rate of 10-3.

As performance measures we consider the average loga-

rithmic echo return loss enhancement (ERLE) [1]

E¯

=

1 I

I i=1

10 log10

E

E ||di, ||22 ||di, - di, ||22

(20)

with the expectation operator E being approximated by recursive averaging and the average block-dependent logarithmic system distance [1]

¯ ZP,

=

1 I

I
10 log10
i=1

||w~ i, - Q5w^ i, ||w~ i, ||22

||22

.

(21)

with I = 100 (corresponding to 27 min) being the number of experiments. Note that we use a zero-padded version of the estimate w^ i, in Eq. (21) to take into account the undermodeling of the FIR filter model [1]. The test data was disjoint
from the training data, i.e., AIRs and speakers were different.

Table 1: Parameter settings for the considered algorithms.

Algorithm

X

P

µMAX

EA-FDAF

0.5 0.5 0.75

DNN-FDAF

(M

e 

=

0M×M )

0.5

0.0

1.0

DNN-FDAF

(M

µ 

=

IM)

0.5 0.0 0.5

DNN-FDAF

0.5 0.0 1.0

EA-FDAF KF (A = 0.99) KF (A = 0.999)

DNN-FDAF

(M

e 

=

0M×M )

DNN-FDAF

(M

µ 

=

IM)

DNN-FDAF

E¯ in dB

¯ZP, in dB

15
10
5
0
0
-5
-10
-15 0 2 4 6 8 10 12 14 16 Time in s
Fig. 4: Performance evaluation of the proposed DNN-FDAF algorithm, including two algorithmic variants, in comparison to various baselines. The shaded grey area represents the transition period of abrupt system changes.

We conclude from Fig. 4 that the proposed DNN-FDAF

algorithm significantly outperforms the baselines in terms of

convergence rate and reconvergence rate after abrupt AIR

changes.

Furthermore,

while

either

estimating

M

µ 

or

M

e 

,

while keeping the other one fixed, results in robust recon-

vergence at the cost of worse steady-state performance, their

joint estimation does not need any compromise. The average

inference time of the proposed DNN-FDAF algorithm for

processing one signal block of duration 64 ms on an Intel

Xeon CPU E3-1275 v6 @ 3.80GHz is tDNN = 1.5 ms which

renders the method to be real-time capable.

5. CONCLUSION
In this paper, we proposed a novel adaptation control for online frequency-domain system identification by using a DNN for step-size inference. By optimizing the DNN parameters end-to-end w.r.t. the system distance of the adaptive filter, the proposed DNN-FDAF algorithm circumvents the explicit estimation of target signal statistics for model-based step-size estimation. This renders the method robust against model inaccuracies and high-level and non-stationary noise signals and additionally avoids the need for any applicationdependent hyperparameter tuning.

6. REFERENCES
[1] G. Enzner, H. Buchner, A. Favrot, and F. Kuech, "Acoustic Echo Control," in Academic Press Library in Signal Processing, vol. 4, pp. 807­877. Elsevier, FL, USA, 2014.
[2] H. Zhang, K. Tan, and D. Wang, "Deep Learning for Joint Acoustic Echo and Noise Cancellation with Nonlinear Distortions," in Interspeech, Sept. 2019, pp. 4255­4259.
[3] N. L. Westhausen and B. T. Meyer, "Acoustic echo cancellation with the dual-signal transformation LSTM network," 2020, arXiv: 2010.14337.
[4] Mhd. M. Halimeh, T. Haubner, A. Briegleb, A. Schmidt, and W. Kellermann, "Combining adaptive filtering and complexvalued deep postfiltering for acoustic echo cancellation," in Int. Conf. Acoust., Speech, Signal Process., Toronto, CA, June 2021.
[5] T. Haubner, Mhd. M. Halimeh, A. Brendel, and W. Kellerman, "A Synergistic Kalman- and Deep Postfiltering Approach to Acoustic Echo Cancellation," in accepted for European Signal Process. Conf., Dublin, IE, Aug. 2021.
[6] S. Haykin, Adaptive Filter Theory, Prentice Hall, NJ, USA, 2002.
[7] P. S. R. Diniz, Adaptive Filtering: Algorithms and Practical Implementation, Springer, Berlin, Heidelberg, 2007.
[8] E. Ha¨nsler and G. Schmidt, Acoustic Echo and Noise Control: A practical Approach, Wiley-Interscience, NJ, USA, 2004.
[9] C. Paleologu, S. Ciochina, and J. Benesty, "Double-talk robust VSS-NLMS algorithm for under-modeling acoustic echo cancellation," in Int. Conf. Acoust., Speech, Signal Process., 2008, pp. 245­248.
[10] T. Gansler, M. Hansson, C.-J. Ivarsson, and G. Salomonsson, "A double-talk detector based on coherence," IEEE Trans. Commun., vol. 44, no. 11, pp. 1421­1427, Nov. 1996.
[11] J. Benesty, D. R. Morgan, and J. H. Cho, "A new class of doubletalk detectors based on cross-correlation," IEEE Trans. Speech Audio Process., vol. 8, no. 2, pp. 168­172, Mar. 2000.
[12] H. Huang and J. Lee, "A new variable step-size NLMS algorithm and its performance analysis," IEEE Trans. Signal Process., vol. 60, no. 4, pp. 2055­2060, 2012.
[13] B. H. Nitsch, "A frequency-selective stepfactor control for an adaptive filter algorithm working in the frequency domain," Signal Process., vol. 80, no. 9, pp. 1733­1745, 2000.
[14] J. Benesty, H. Rey, L. R. Vega, and S. Tressens, "A Nonparametric VSS NLMS Algorithm," IEEE Signal Process. Lett., vol. 13, no. 10, pp. 581­584, 2006.
[15] C. Huemmer, R. Maas, and W. Kellermann, "The NLMS algorithm with time-variant optimum stepsize derived from a Bayesian network perspective," IEEE Signal Process. Lett., vol. 22, no. 11, pp. 1874­1878, 2015.
[16] C. Breining, "Applying a Neural Network for Stepsize Control in Echo Cancellation," in Proc. Int. Workshop Acoust. Echo Noise Control, London, UK, 1997.
[17] G. Enzner and P. Vary, "Frequency-domain adaptive Kalman filter for acoustic echo control in hands-free telephones," Signal Process., vol. 86, no. 6, pp. 1140­1156, June 2006.

[18] F. Yang, G. Enzner, and J. Yang, "Frequency-Domain Adaptive Kalman Filter With Fast Recovery of Abrupt Echo-Path Changes," IEEE Signal Process. Lett., vol. 24, no. 12, pp. 1778­1782, Dec. 2017.
[19] S. Malik and G. Enzner, "Online maximum-likelihood learning of time-varying dynamical models in block-frequencydomain," in Proc. Int. Conf. Acoust., Speech, Signal Process., Dallas, USA, Mar. 2010, pp. 3822­3825.
[20] J. Franzen and T. Fingscheidt, "Improved Measurement Noise Covariance Estimation for N-channel Feedback Cancellation Based on the Frequency-Domain Adaptive Kalman Filter," in Proc. Int. Conf. Acoust., Speech, Signal Process., Brighton, UK, May 2019, pp. 965­969.
[21] T. Jiang, R. Liang, Q. Wang, C. Zou, and C. Li, "An Improved Practical State-Space FDAF With Fast Recovery of Abrupt Echo-Path Changes," IEEE Access, vol. 7, pp. 61353­61362, 2019.
[22] T. Haubner, A. Brendel, M. Elminshawi, and W. Kellerman, "Noise-Robust Adaptation Control for Supervised Acoustic System Identification Exploiting a Noise Dictionary," in Int. Conf. Acoust., Speech, Signal Process., Toronto, CA, June 2021.
[23] A. A. Nugraha, A. Liutkus, and E. Vincent, "Multichannel Audio Source Separation With Deep Neural Networks," IEEE Audio, Speech, and Language Process., vol. 24, no. 9, pp. 1652­ 1664, Sept. 2016.
[24] E. Ferrara, "Fast implementations of LMS adaptive filters," IEEE Trans. Acoust., vol. 28, no. 4, pp. 474­475, Aug. 1980.
[25] B. Widrow and M. E. Hoff, "Adaptive Switching Circuits," in Proc. WESCON Conv. Rec., Los Angeles, USA, Aug. 1960, pp. 96­104.
[26] J. J. Shynk, "Frequency-domain and multirate adaptive filtering," IEEE Signal Process. Mag., vol. 9, no. 1, pp. 14­37, 1992.
[27] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, "Librispeech: An ASR corpus based on public domain audio books," in Proc. Int. Conf. Acoust., Speech, Signal Process., 2015, pp. 5206­5210.
[28] M. Jeub, M. Scha¨fer, and P. Vary, "A binaural room impulse response database for the evaluation of dereverberation algorithms," in Int. Conf. on Digit. Signal Process., Santorini, Greece, July 2009.
[29] J. Y. C. Wen, N. D. Gaubitch, E. A. P. Habets, T. Myatt, and P. A. Naylor, "Evaluation of speech dereverberation algorithms using the MARDY database," in Proc. Int. Workshop Acoust. Echo Noise Control, 2006.
[30] "Multi-channel impulse response database (MIRD)," https://www.iks.rwth-aachen.de/en/research/tools-dow Accessed: 2020-12-04.
[31] D. Kingma and J. Ba, "ADAM: A method for stochastic optimization," arXiv preprint arXiv:1412.6980, 2014.

