
# Implicit MLE: Backpropagating Through Discrete Exponential Family Distributions

[arXiv](https://arxiv.org/abs/2106.01798), [PDF](https://arxiv.org/pdf/2106.01798.pdf)

## Authors

- Mathias Niepert
- Pasquale Minervini
- Luca Franceschi

## Abstract

Integrating discrete probability distributions and combinatorial optimization problems into neural networks has numerous applications but poses several challenges. We propose Implicit Maximum Likelihood Estimation (I-MLE), a framework for end-to-end learning of models combining discrete exponential family distributions and differentiable neural components. I-MLE is widely applicable: it only requires the ability to compute the most probable states; and does not rely on smooth relaxations. The framework encompasses several approaches, such as perturbation-based implicit differentiation and recent methods to differentiate through black-box combinatorial solvers. We introduce a novel class of noise distributions for approximating marginals via perturb-and-MAP. Moreover, we show that I-MLE simplifies to maximum likelihood estimation when used in some recently studied learning settings that involve combinatorial solvers. Experiments on several datasets suggest that I-MLE is competitive with and often outperforms existing approaches which rely on problem-specific relaxations.

## Comments



## Source Code

Official Code



Community Code

- [https://paperswithcode.com/paper/implicit-mle-backpropagating-through-discrete](https://paperswithcode.com/paper/implicit-mle-backpropagating-through-discrete)

## Bibtex

```tex
@misc{niepert2021implicit,
      title={Implicit MLE: Backpropagating Through Discrete Exponential Family Distributions}, 
      author={Mathias Niepert and Pasquale Minervini and Luca Franceschi},
      year={2021},
      eprint={2106.01798},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```

## Notes

Type your reading notes here...

