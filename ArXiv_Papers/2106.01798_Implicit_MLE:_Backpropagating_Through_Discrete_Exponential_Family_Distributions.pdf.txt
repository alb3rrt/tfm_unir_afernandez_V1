Implicit MLE: Backpropagating Through Discrete Exponential Family Distributions

arXiv:2106.01798v1 [cs.LG] 3 Jun 2021

Mathias Niepert NEC Laboratories Europe mathias.niepert@neclab.eu

Pasquale Minervini UCL
p.minervini@ucl.ac.uk

Luca Franceschi UCL & IIT
ucablfr@ucl.ac.uk

Abstract
Integrating discrete probability distributions and combinatorial optimization problems into neural networks has numerous applications but poses several challenges. We propose Implicit Maximum Likelihood Estimation (I-MLE), a framework for end-to-end learning of models combining discrete exponential family distributions and differentiable neural components. I-MLE is widely applicable: it only requires the ability to compute the most probable states; and does not rely on smooth relaxations. The framework encompasses several approaches, such as perturbation-based implicit differentiation and recent methods to differentiate through black-box combinatorial solvers. We introduce a novel class of noise distributions for approximating marginals via perturb-and-MAP. Moreover, we show that I-MLE simplifies to maximum likelihood estimation when used in some recently studied learning settings that involve combinatorial solvers. Experiments on several datasets suggest that I-MLE is competitive with and often outperforms existing approaches which rely on problem-specific relaxations.
1 Introduction
While deep neural networks excel at perceptual tasks, they tend to generalize poorly whenever the problem at hand requires some level of symbolic manipulation, reasoning, or known algorithmic structure. Logic, relations, and explanations, as well as decision processes, frequently find natural abstractions in discrete structures, ill-captured by the continuous mappings of standard neural nets. Several application domains, ranging from relational and explainable ML to (discrete) decisionmaking [Misic´ and Perakis, 2020], could benefit from general-purpose learning algorithms whose inductive biases are more amenable to integrating symbolic and neural computation. Motivated by these considerations, there is a growing interest in end-to-end learnable models incorporating discrete components that allow e.g. to sample from discrete latent distributions [Jang et al., 2017, Paulus et al., 2020] or solve combinatorial optimization problems [Pogancic´ et al., 2019, Mandi et al., 2020]. For complex discrete distributions, it is intractable to compute the exact gradients of the expected loss. For combinatorial optimization problems, there are discontinuities, and the gradients are zero almost everywhere. The standard approach revolves around problem-specific smooth relaxations, which allow one to fall back to (stochastic) backpropagation. These strategies, however, require tailor-made relaxations, presuppose access to the constraints and are, therefore, not always feasible nor tractable for large state spaces. Moreover, reverting to discrete outputs at test time may cause unexpected behavior. In other situations, discrete outputs are required at training time because one has to make one of a number of discrete choices, such as accessing discrete memory or deciding on an action in a game. With this paper, we take a step towards the vision of general-purpose algorithms for hybrid learning systems. Specifically, we consider settings where the discrete component(s), embedded in a larger
Preprint. Under review.

computational graph, are discrete random variables from the (constrained) exponential family1. Grounded in concepts from Maximum Likelihood Estimation (MLE) and perturbation-based implicit differentiation, we propose Implicit Maximum Likelihood Estimation (I-MLE). To approximate the gradients of the discrete distributions' parameters, I-MLE computes, at each update step, a target distribution q that depends on the loss incurred from the discrete output in the forward pass. In the backward pass, we approximate maximum likelihood gradients by treating q as the empirical distribution. We propose ways to derive target distributions and introduce a novel family of noise perturbations well-suited for approximating marginals via perturb-and-MAP. I-MLE is generalpurpose as it only requires the ability to compute most probable states and not faithful samples or probabilistic inference. We show that I-MLE simplifies to explicit maximum-likelihood learning when used in some recently studied learning settings involving combinatorial optimization solvers. Experimental results suggest that I-MLE is flexible and competitive compared to the straight-through and relaxation-based estimators, while being simple to implement.

2 Problem Statement and Motivation
We consider models described by the equations  = hv(x), z  p(z; ), y = fu(z), (1)
where x  X and y  Y denote feature inputs and target outputs, hv : X   and fu : Z  Y are smooth parameterized maps, and p(z; ) is a discrete probability distribution.

x

 p(z; ) ~ z

L

hv



fu

Figure 1: Illustration of the addressed learning problem. z is the discrete (latent) structure.

Given a set of examples D = {(x^j, y^j)}Nj=1, we are concerned with learning the parameters  = (v, u) of (1) by finding approximate solutions of min j L(x^j, y^j; )/N . The training error L is typically defined as:

L(x^, y^; ) = Ez^p(z;^) [ (fu(z^), y^)] with ^ = hv(x^),

(2)

where : Y × Y  R+ is a point-wise loss function. Fig. 1 illustrates the setting. For example, an interesting instance of (1) and (2) arises in learning to explain user reviews [Chen et al., 2018] where the task is to infer a target sentiment score (e.g. w.r.t. the quality of a product) from a review while also providing a concise explanation of the predicted score by selecting a subset of exactly k words (cf. Example 2). In Section 6, we present experiments precisely in this setting. As anticipated in the introduction, we restrict the discussion to instances in which p(z; ) belongs to the (constrained) discrete exponential family, which we now formally introduce.

Let Z be a vector of discrete random variables over a state space Z and let C  Z be the set of states that satisfy a given set of linear constraints.2 Let     Rm be a real-valued parameter vector. The probability mass function (PMF) of a discrete constrained exponential family r.v. is:

p(z; ) =

exp ( z,  / - A()) if z  C,

0

otherwise.

(3)

Here, ·, · is the inner product and  the temperature, which, if not mentioned otherwise, is assumed

to be z, 

1. A() is the log-partition the weight of the state z.

function defined as A() The marginals (expected

= log value,

meazn)C

exp ( of the

z,  r.v.s

/ ) . Z are

We call defined

as µ() := Ez^p(z;)[z^]. Finally, the most probable or Maximum A-Posteriori (MAP) states are

defined as captures a

bMrAoPa(dr)an:=geaorfgsmetatixnzgsCanzd,sub.suTmheesfapmroiblyaboiflitpyrodbisatbriibliutytiodnisstrsiubcuhtioasnspowseitidveefiMnearhkeorve

random fields and statistical relational formalisms [Wainwright and Jordan, 2008, Raedt et al., 2016].

We now discuss some examples which we will use in the experiments. Crucially, in Example 3 we

establish the link between the constrained exponential family and integer linear programming (ILP)

identifying the ILP cost coefficients with the distribution's parameters .

Example 1 (Categorical Variables). An m-way (one-hot) categorical variable corresponds to

p(z; ) = exp ( z,  - A()), subject to the constraint z, 1 = 1, where 1 is a vector of ones.

1This includes integer linear programs via a natural link that we outline in Example 3. 2For the sake of simplicity we assume Z  {0, 1}m. The set C is the integral polytope spanned by the problem-specific and given linear constraints.

2

AdisstCrib=utio{enic}om i=in1c,idwehweriethetiheiswtheeigih-ttsh,

vector which

of are

the canonical base, the parameters of often called logits in this context. The

the above marginals

µ coincide with the PMF and can be expressed through a closed-form smooth function of : the

softmax. This facilitates a natural relaxation that involves using µ() in place of z [Jang et al., 2017].

The properties of the categorical distribution, however, quickly disappear even for slightly more

complex distributions, as the following example shows.

Example 2 (k-subset Selection). Assume we want to sample binary m-dimensional vectors with k ones. This amounts to replacing the constraint in Example 1 by the constraint z, 1 = k.

Here, a closed-form expression for the marginals does not exist: sampling from this distribution

requires computing the time linear in m.

m k

= O(mk) weights (if k  m/2). Computing MAP states instead takes

Example 3 (Integer Linear Programs). Consider the combinatorial optimization problem given by the integer linear program arg minzC z, c , where C is an integral polytope and c  Rm is a vector of cost coefficients, and let z(c) be the set of its solutions. We can associate to the ILP the family (indexed by  > 0) of probability distributions p(z; ) from (3), with C the ILP polytope
and  = -c. Then, for every  > 0, the solutions of the ILP correspond to the MAP states: MAP() = arg maxzC z,  = z(c) and for   0 one has that Pr(Z  z(c))  1.

Many problems of practical interest can be expressed as ILPs, such as finding shortest paths, planning and scheduling problems, and inference in propositional logic.

3 The Implicit Maximum Likelihood Estimator

In this section, we develop and motivate a family of general-purpose gradient estimators for Eq. (2) that respect the structure of C . 3 Let (x^, y^)  D be a training example and z^  p(z; hv(x^)). The gradient of L w.r.t. u is given by uL(x^, y^; ) = Ez^[ufu(z^) y (y, y^)] with y = fu(z^), which may be estimated by drawing one or more samples from p. Regarding vL, one has

vL(x^, y^; ) = vhv(x^) L(x^, y^; ),

(4)

where the major challenge is to compute L. A standard approach is to employ the score function estimator (SFE) which typically suffers from high variance. Whenever a pathwise derivative estimator (PDE) is available it is usually the preferred choice [Schulman et al., 2015]. In our setting, however, the PDE is not readily applicable since z is discrete and, therefore, every (exact) reparameterization path would be discontinuous. Various authors developed (biased) adaptations of the PDE for discrete r.v.s (see Section 5). These involve either smooth approximations of p(z; ) or approximations of the derivative of the reparameterization map. Our proposal departs from these two routes and instead involves the formulation of an implicit maximum likelihood estimation problem. In a nutshell, I-MLE is a (biased) estimator that replaces L in Eq. (4) with L, where L is an implicitly defined MLE objective and  is an estimator of the gradient.

We now focus on deriving the (implicit) MLE objective L. Let us assume we can, for a given y^, construct an exponential family distribution q(z;  ), which we call the target distribution, such that:

Ez^q(z; ) [ (fu(z^), y^)]  Ez^p(z;) [ (fu(z^), y^)] . We then define L(,  ) := -Ez^q(z; )[log p(z^; )] = Ez^q(z; )[A() - z^,  ]. L can be interpreted4 as an MLE objective between the (current) model distribution p and the target distribution q. The intuition is that making p more similar to q will also reduce the loss L(x^, y^; ) = Ez^p(z;) [ (fu(z^), y^)]. Exploiting the fact that A() = µ(), the gradient of L is computed as:

L(,  ) = µ() - Ez^q(z; )[z^] = µ() - µ( ),

(5)

that is, the difference between the marginals of the current distribution p and the marginals of the target distribution q, also equivalent to the gradient of the KL divergence between p and q.

3The derivations are adaptable to other types of losses defined over the outputs of Eq. (1). 4We expand on this in Appendix A where we also review the classic MLE setup [Murphy, 2012, Ch. 9].

3

We will not use Eq. (5) directly, as computing the marginals is, in general, a #P-hard problem and scales poorly with the dimensionality m. MAP states are typically less expensive to compute (e.g. see Example 2) and are often used to approximate µ()5 or to compute perturb-and-MAP approximations, where µ()  E ( ) MAP( + ) where  ( ) is an appropriate noise distribution with domain Rm. In this work we follow ­ and explore in more detail in Section 3.2 ­ the latter approach (also referred to as the Gumbel-max trick [cf. Papandreou and Yuille, 2011]), which retains most of the computational advantages of the MAP approximation but may be far less crude. Henceforth, we only assume access to an algorithm to compute MAP states (this may be a standard ILP solver in the case of Example 3) and rephrase, with a slight abuse of notation, Eq. (1) as

 = hv(x), z = MAP( + ) with  p( ), y = fu(z).

(6)

With Eq. (6) in place, the general expression for the I-MLE estimator is vL(x, y; ) = vhv(x^) L(,  ) with  = hv(x^) where, for S  N+:

L(, 

)

=

1 S

S
[MAP( +

i) - MAP( +

i)], with

i  ( ) for i  {1, . . . , S}.

(7)

i=1

Note that, if the states of both p and q are binary vectors, L(,  )  [-1, 1]m and when S = 1 L(,  )  {-1, 0, 1}m. A pseudo-code implementation of I-MLE is available in Appendix B. In the following, we discuss the problem of constructing the target distributions q.

3.1 Target Distributions via Perturbation-based Implicit Differentiation

The efficacy of the I-MLE estimator hinges on a proper choice of q, a hyperparameter of our framework. In this section we derive and motivate a class of general-purpose target distributions, rooted in perturbation-based implicit differentiation (PID):

q(z;  ) = p(z;  - z (fu(z), y^)) with z = MAP( + ) and  ( ),

(8)

where (x^, y^)  D is a data point,  = hv(x^), and  > 0 is a hyperparameter that controls the perturbation intensity.

To motivate Eq. (8), consider the setting where the inputs to f are the marginals of p(z; ) (rather than discrete perturb-and-MAP samples as in Eq. (6)), that is, y = fu(µ()) with  = hv(x^), and redefine the training error L of Eq. (2) accordingly. A seminal result by Domke [2010] shows that, in this case, we can obtain L by perturbation-based differentiation as:

L(x^, y^; ) = lim
0

1 

[µ()

-

µ

(

-

µL(x^,

y^;

))]

,

(9)

where µL = µfu(µ) y (y, y^). The expression inside the limit may be interpreted as the gradient of an implicit MLE objective (see Eq. (5)) between the distribution p with (current) parameters  and p with parameters perturbed in the negative direction of the downstream gradient µL. Now, we can adapt (9) to our setting of Eq. (6) by resorting to the straight-through estimator (STE) assumption [Bengio et al., 2013]. Here, the STE assumption translates into reparameterizing z as a function of µ and approximating µz  I. Then, µL = µz zL  zL and we approximate Eq. (9) as

L(x^, y^; )



1 

[µ()

- µ (

- zL)]

=

1 

L

(,



-

z L),

(10)

for some  > 0. From Eq. (10) we derive (8) by taking a single sample estimator of zL (with perturb-and-MAP sampling) and by incorporating the constant 1/ into a global learning rate. I-

MLE with PID target distributions may be seen as a way to generalize the STE to more complex

distributions. Instead of using the gradients zL to backpropagate directly, I-MLE uses them to construct a target distribution q. With that, it defines an implicit maximum likelihood objective, whose

gradient (estimator) propagates the supervisory signal upstream, critically, taking the constraints

into account. When using Eq. (8) with ( ) = 0( )6, the I-MLE estimator also recovers a recently proposed gradient estimation rule to differentiate through black-box combinatorial optimization

problems [Pogancic´ et al., 2019]. I-MLE unifies existing gradient estimation rules in one framework.

We will resume the discussion about target distributions in Section 4, where we analyze more closely the setup in Example 3. Next, we focus on the perturb-and-MAP strategies and derive a class of noise distributions that are particularly apt to the settings we consider in this work.

5This is known as the perceptron learning rule in standard MLE. 60 is the Dirac delta centered around 0 ­ this is equivalent to approximating the marginals with MAP.

4

3.2 A Novel Family of Perturb-and-MAP Noise Distributions

We propose a novel and general way to design tailored perturb and MAP sampling strategies. While the proposed family of noise distributions works with I-MLE, the results of this Section are of independent interest and can also be used in other (relaxed) perturb-and-MAP based gradient estimators [Paulus et al., 2020]. The following proposition is a generalization of the classic perturb and MAP result by Papandreou and Yuille [2011] (also known as the Gumbel-max trick) in that it considers the temperature  .
Proposition 1. Let p(z; ) be a discrete exponential family distribution with integer polytope C and temperature  , and let z,  be the unnormalized weight of each z  C. Moreover, let ~ be such that, for all z  C, z, ~ = z,  + (z) with each (z) sampled i.i.d. from Gumbel(0,  ). Then we have that Pr(MAP(~) = z) = p(z; ).

All proofs can be found in Appendix C. The proposition states that if we can perturb the weights z,  of each z  C with independent Gumbel(0,  ) noise, then obtaining MAP states from the perturbed model is equivalent to sampling from p(z; ) at temperature7  . For general constrained exponential distributions, perturbing the weights z,  for each state z  C is at least as expensive as computing the marginals exactly. Hence, one usually locally perturbs each []i (the i-th component of the vector ) with Gumbel noise. Fortunately, we can prove that, for a large class of problems, it is possible to design more suitable local perturbations. First, we show that, for any   N+, a Gumbel distribution can be written as a finite sum of  i.i.d. (implicitly defined) random variables. Lemma 1. Let X  Gumbel(0,  ) and let   N+. Define the Sum-of-Gamma distribution as

 SoG(, , s) :=

s
{Gamma(1/, /i)} - log(s) ,

(11)



i=1

where s  N+ and Gamma(, ) is the Gamma distribution with shape  and scale , and let

SoG(,  ) := lims SoG(, , s). Then we have that X 

 j=1

j, with

j  SoG(,  ).

Based on Lemma 1, we can show that for exponential family distributions where every z  C has exactly k non-zero entries we can design perturbations of z,  following a Gumbel distribution.
Theorem 1. Let p(z; ) be a discrete exponential family distribution with integer polytope C and temperature  . Assume that if z  C then z, 1 = k for some constant k  N+. Let ~ be the perturbation obtained by [~]j = []j + j with j  SoG(k,  ) from Eq. (11). Then, z  C we have that z, ~ = z,  + (z), with (z)  Gumbel(0,  ).

Many problems such as k-subset selection, travel- 0.4

0.4

ing salesman, spanning tree, and graph matching strictly satisfy the assumption of Theorem 1. We

0.3

0.3

can, however, also apply the strategy in cases 0.2

0.2

where the variance of Z, 1 is small (e.g. short- 0.1

0.1

est weighted path). The Sum-of-Gamma perturbations provide a more fine-grained approach to noise perturbations. For  =  = 1, we obtain the standard Gumbel perturbations. In contrast to the standard Gumbel(0, 1) noise, the pro-

0.0 -5 0 5 10 15 20 0.0

0

5

10

Figure 2: Histograms for 10k samples where each

sample is (left) the sum of 5 j  Gumbel(0, 1) or

(right) the sum of 5 j  SoG(5, 1, 10).

posed local Sum-of-Gamma perturbations result in weights' perturbations that follow the Gum-

bel distribution. Fig. 2 shows histograms of 10k samples, where each sample is either the sum

of 5 samples from Gumbel(0, 1) (the standard approach) or the sum of k = 5 samples from

SoG(5, 1, from p(z;

10))a=s th15e

pe1ir=0tu1{rbGaatimonms aa(r1e/n5o,t5i/nid)e-pelnodge(n1t0,)w}.e

While we still cannot sample faithfully can counteract the problem of partially

dependent perturbations by increasing the temperature  and, therefore, the variance of the noise

distribution. We explore and verify the importance of tuning  empirically. In the appendix, we

also show that the infinite series from Lemma 1 can be well approximated by a finite sum using

convergence results for the Euler-Mascheroni series [Mortici, 2010].

7Note that the temperature here is different to the temperature of the Gumbel softmax trick [Jang et al., 2017] which scales both the sum of the logits and the samples from Gumbel(0, 1).

5

4 Target Distributions for Combinatorial Optimization Problems

In this section, we explore the setting where the discrete computational component arises from a combinatorial optimization (CO) problem, specifically an integer linear program (ILP). Many authors have recently considered the setup where the CO component occupies the last layer of the model defined by Eq. (1) (where fu is the identity) and the supervision is available in terms of examples of either optimal solutions [e.g. Pogancic´ et al., 2019] or optimal cost coefficients (conditioned on the inputs) [e.g. Elmachtoub and Grigas, 2020]. We have seen in Example 3 that we can naturally associate to each ILP a probability distribution (3) with  given by the negative cost coefficients c of the ILP and C the integral polytope. Letting   0 is equivalent to taking the MAP in the forward pass. Furthermore, in Section 3.1 we showed that the I-MLE framework subsumes a recently propose method by Pogancic´ et al. [2019]. Here, instead, we show that, for a certain choice of the target distribution, I-MLE estimates the gradient of an explicit maximum likelihood learning loss L where the data distribution is ascribed to either (examples of) optimal solutions or optimal cost coefficients.

Let q(z;  ) be the distribution p(z;  ), with parameters

[ ]i :=

[]i

if [zL]i = 0

-[zL]i otherwise.

(12)

In the first CO setting, we observe training data D = {(x^j, y^j)}Nj=1 where y^j  C and the loss measures a distance between a discrete z^j  p(z; j) with j = hv(x^) and a given optimal
solution of the ILP y^j. An example is the Hamming loss H [Pogancic´ et al., 2019] defined as H (z, y) = z  (1 - y) + y  (1 - z), where  denotes the Hadamard (or entry-wise) product.
Fact 1. If one uses H , then I-MLE with the target distribution of Eq. (12) and ( ) = 0( ) is equivalent to the perceptron-rule estimator of the MLE objective between p(z; hv(x^j)) and y^j.

It follows that the method by Pogancic´ et al. [2019] returns, for a large enough , the maximumlikelihood gradients (scaled by 1/) approximated by the perceptron rule. Proofs in Appendix C.

In the second CO setting, we observe training data D = {(x^j, c^j)}Nj=1, where c^j is the optimal cost conditioned on input x^j. Here, various authors [e.g. Elmachtoub and Grigas, 2020, Mandi et al., 2020, Mandi and Guns, 2020] use as point-wise loss the regret R(, c) = c (z() - z^(c)) where z() is a state sampled from p(z; ) (possibly with temperature   0, that is, a MAP state) and z^(c)  z(c) is an optimal state for c.
Fact 2. If one uses R then I-MLE with the target distribution of Eq. (12) is equivalent to the perturb-and-MAP estimator of the MLE objective between p(z; hv(x^j)) and p(z; -c^j).

This last result also implies that when using the target distribution q from (12) in conjunction with the regret, I-MLE performs maximum-likelihood learning minimizing the KL divergence between the current distribution and the distribution whose parameters are the optimal cost.

5 Related Work
Several papers address the gradient estimation problem for discrete r.v.s, many resorting to relaxations. Maddison et al. [2017], Jang et al. [2017] propose the Gumbel-softmax distribution to relax categorical r.v.s; Paulus et al. [2020] study extensions to more complex probability distributions. Tucker et al. [2017], Grathwohl et al. [2018] develop parameterized control variates based on continuous relaxations for the score-function estimator. In contrast to the above, we focus explicitly on problems where only discrete samples are used during training. Approaches that do not rely on relaxations are specific to certain distributions [Bengio et al., 2013, Franceschi et al., 2019, Liu et al., 2019] or assume knowledge of C [Kool et al., 2020]. We provide a general-purpose framework that does not require access to the linear constraints and the corresponding integer polytope C. Experiments in the next section show that while I-MLE only requires a MAP solver, it is competitive and sometimes outperforms tailor-made relaxations. SparseMAP [Niculae et al., 2018] is an approach to structured prediction and latent variables, replacing the exponential distribution (specifically, the softmax) with a sparser distribution. Similar to our work, it only presupposes the availability of a MAP oracle. LP-SparseMAP [Niculae and Martins, 2020] is an extension that uses a relaxation of the optimization problem rather than a MAP solve. Sparsity can also be exploited for efficient marginal inference in latent variable models [Correia et al., 2020].

6

A series of works about differentiating through CO problems [Wilder et al., 2019, Elmachtoub and Grigas, 2020, Ferber et al., 2020, Mandi and Guns, 2020] relax ILPs by adding L1, L2 or log-barrier regularization terms and differentiate through the KKT conditions deriving from the application of the cutting plane or the interior-point methods. These approaches are conceptually linked to techniques for differentiating through smooth programs [Amos and Kolter, 2017, Donti et al., 2017, Agrawal et al., 2019, Chen et al., 2020, Domke, 2012, Franceschi et al., 2018] that arise not only in modelling but also in hyperparameter optimization and meta-learning. Pogancic´ et al. [2019], Rolínek et al. [2020], Berthet et al. [2020] propose methods that are not tied to a specific ILP solver. As we saw above, the former two, originally derived from a continuous interpolation argument, may be interpreted as special instantiations of I-MLE. The latter addresses the theory of perturbed optimizers and discusses perturb and MAP in the context of the Fenchel-Young loss. All the COrelated works assume that either optimal costs or solutions are given as training data, while I-MLE may be also applied in the absence of such supervision by making use of implicitly generated target distributions. Other authors focus on devising differentiable relaxations for specific CO problems such as SAT [Evans and Grefenstette, 2018] or MaxSAT [Wang et al., 2019]. Machine learning intersects with CO also in other contexts, e.g. in learning heuristics to improve the performances of CO solvers or differentiable models such as GNNs to "replace" them; see Bengio et al. [2020] and references therein. Direct loss minimization (DLM) [McAllester et al., 2010, Song et al., 2016] is also related to our work, but the assumption there is that examples of optimal states z^ are given. Lorberbom et al. [2019] extend the DLM framework to discrete VAEs using coupled perturbations. Their approach is tailored to VAEs and not general-purpose. Under a methodological viewpoint, I-MLE inherits from classical MLE [Wainwright and Jordan, 2008] and perturb-and-MAP [Papandreou and Yuille, 2011]. The theory of perturb-and-MAP was used to derive general-purpose upper bounds for log-partition functions [Hazan and Jaakkola, 2012, Shpakova and Bach, 2016].
6 Experiments
The set of experiments can be divided into three parts. First, we analyze and compare the behavior of I-MLE with (i) the score function and (ii) the straight-through estimator using a toy problem. Second, we explore the latent variable setting where both hv and fu in Eq. (1) are neural networks and the optimal structure is not available during training. Finally, we address the problem of differentiating through black-box combinatorial optimization problems. Here, we use the target distribution derived in Section 4. More experimental details for available in the appendix. Synthetic Experiments. We conducted a series of experiments with a tractable 5-subset distribution (see Example 2) where z  {0, 1}10. We set the loss to L() = Ez^p(z;)[ z^ - b 2], where b is a fixed vector sampled from N (0, I). In Fig. 3 (Top), we plot optimization curves with means and standard deviations, comparing the proposed estimator with the straight-through (STE) and the score function (SFE) estimators. 8 For STE and I-MLE, we use perturb and MAP (PaM) with Gumbel and SoG(1, 5, 10) noise, respectively. The SFE uses faithful samples and exact marginals (which is feasible only when m is very small) and converges much more slowly than the other methods, while the STE converges to worse solutions than those found using I-MLE. Figure 3 (Bottom) shows the benefits of using SoG rather than Gumbel perturbations with I-MLE. While the best configurations for both are comparable, SoG noise achieves in average (over 100 runs) strictly better final values of L for more than 50% of the tested configurations (varying  from Eq. (8) and the learning rate) and exhibit smaller variance (see Fig. 6). Additional details and results in Appendix D.1. Learning to Explain. The BEERADVOCATE dataset [McAuley et al., 2012] consists of free-text reviews and ratings for 4 different aspects of beer: appearance, aroma, palate, and taste. Each sentence in the test set has annotations providing the words that best describe the various aspects. Following the experimental setup of recent work [Paulus et al., 2020], we address the problem introduced by the L2X paper [Chen et al., 2018] of learning a distribution over k-subsets of words that best explain a given aspect rating. The training set has 80k reviews for the aspect APPEARANCE and 70k reviews for all other aspects. Since the original dataset [McAuley et al., 2012] did not provide
8Hyperparameters are optimized against L for all methods independently. Statistics are over 100 runs. We found STE slightly better with Gumbel rather than SoG noise. SFE failed with all tested PaM strategies.
7

3.0 2.5

SFE (steps x 10) STE

2.0

I-MLE

Optimality gap

1.5

1.0

0.5

0.0 0 10 20 30 40 Optimization steps

I-MLE SoG - Gum. (means) 0.2

0.5

0.0

0.625

0.2

Learning rate

0.75

0.4

0.875

0.6

1.0

0.8

0.5 1.0 1.5 2.0 2.5 3.0 1.0

Lambda

Figure 3: Top: Gradient-based optimization of L with various estimators. Bottom: Mean difference of the final value of L between I-MLE with SoG or Gumbel ( ), varying  and the learning rate (blue = better SoG).

Method L2X (t = 0.1) SoftSub (t = 0.5) STE ( = 30)
I-MLE MAP I-MLE Gumbel I-MLE ( = 30)
L2X (t = 0.1) SoftSub (t = 0.5)
I-MLE ( = 5) L2X (t = 0.1)
SoftSub (t = 0.5) I-MLE ( = 30)

Test MSE

Mean Std. Dev.

k = 10

6.68

1.08

2.67

0.14

4.44

0.09

4.08

0.91

2.68

0.10

2.71

0.10

k=5

5.75

0.30

2.57

0.12

2.62

0.05

k = 15

7.71

0.64

2.52

0.07

2.91

0.18

Subset Precision Mean Std. Dev.

26.65

9.39

44.44

2.27

38.93

0.14

14.55

0.04

39.28

2.62

47.98

2.26

33.63

6.91

54.06

6.29

54.76

2.50

23.49 37.78 39.56

10.93 1.71 2.07

Table 1: Detailed results for the aspect AROMA. Test MSE and subset precision, both ×100, for k  {5, 10, 15}.

separate validation and test sets, we compute 10 different evenly sized validation/test splits of the 10k held out set and compute mean and standard deviation over 10 models, each trained on one split. Subset precision was computed using a subset of 993 annotated reviews. We use pre-trained word embeddings from Lei et al. [2016]. Prior work used non-standard neural networks for which an implementation is not available [Paulus et al., 2020]. Instead, we used the neural network from the L2X paper with 4 convolutional and one dense layer. This neural network outputs the parameters  of the distribution p(z; ) over k-hot binary latent masks with k  {5, 10, 15}. We compare to relaxation-based baselines L2X [Chen et al., 2018] and SoftSub [Xie and Ermon, 2019]. We also compare the straight-through estimator (STE) with Sum-of-Gamma (SoG) perturbations. We used the standard hyperparameter settings of Chen et al. [2018] and choose the temperature parameter t  {0.1, 0.5, 1.0, 2.0}. For I-MLE we choose   {101, 102, 103}, while for both I-MLE and STE we choose   {k, 2k, 3k} based on the validation MSE. We used the standard Adam settings. We trained separate models for each aspect using MSE as point-wise loss . Table 1 lists detailed results for the aspect AROMA. I-MLE's MSE values are competitive with those of the best baseline, and its subset precision is significantly higher than all other methods (for  = 30). Using only MAP as the approximation of the marginals leads to poor results. This shows that using the tailored perturbations with tuned temperature is crucial to achieve state of the art results. The Sum-of-Gamma perturbation introduced in this paper outperforms the standard local Gumbel perturbations. More details and results can be found in the appendix.

Discrete Variational Auto-Encoder. We evaluate various perturbation strategies for a discrete k-subset Variational Auto-Encoder (VAE) and compare them to the straight-through estimator (STE) and the Gumbel-softmax trick. The latent variables model a probability distribution over k-subsets of (or top-k assignments too) binary vectors of length 20. The special case of k = 1 is equivalent to a categorical variable with 20 categories. For k > 1, we use I-MLE using the class of PID target distributions of Eq. (8) and compare various perturb and MAP noise sampling strategies. The experimental setup is similar to those used in prior work on the Gumbel softmax tricks [Jang et al., 2017]. The loss is the sum of the reconstruction loss (binary cross-entropy loss on output pixels) and the KL divergence between the marginals of the variables and the uniform distribution. The encoding and decoding functions of the VAE consist of three dense layers (encoding: 512-256-20x20; decoding: 256-512-784). We do not use temperature annealing. Using Eq. (7) with S = 1, we use either Gumbel(0, 1) perturbations (the standard approach)9 or Sum-of-Gamma (SoG) perturbations at a temperature of  = 10. We run 100 epochs and record the loss on the test data. The difference in training time is negligible. Fig. 4 shows that using the SoG noise distribution is beneficial. The
9Increasing the temperature  of Gumbel(0,  ) samples increased the test loss.

8

BCE loss + KL divergence BCE loss + KL divergence BCE loss + KL divergence

Discrete 10-subset VAE (I-MLE)
I-MLE Gumbel (ours) 120
I-MLE SoG (ours)
100

Discrete 10-subset VAE (STE)
230 STE (Gumbel)
STE (SoG)
220

Discrete 1-subset VAE (GSMT)

Gumbel-SMT (STE)

120

Gumbel-SMT (relaxed)

100

80 0

25

50

75

Epochs

210

100

0

25

50

75

Epochs

80

100

0

25

50

75

100

Epochs

Figure 4: Plots of the sum of the binary reconstruction loss and the KL divergence as a function of the number of epochs (lower is better). (Left) Discrete 10-subset VAE trained with I-MLE with  = 10 (I-MLE). (Center) Discrete 10-subset VAE trained with the straight-through estimator (STE). (Right) Discrete 1-subset VAE using the Gumbel softmax trick (GSMT). The down-up-down artifact is due to temperature annealing. Sum-of-Gamma (SoG) perturbations have the lowest test loss for the 10-subset VAEs. For  = 10 and SoG perturbations, the test loss is similar to that of the categorical (1-subset) VAE trained with the Gumbel softmax trick. test loss using the SoG perturbations is lower despite the perturbations having higher variance and, therefore, samples of the model being more diverse. This shows that using weights' perturbations that follow a proper Gumbel distribution is indeed beneficial. I-MLE significantly outperforms the STE, which does not work in this setting and is competitive with the Gumbel-Softmax trick for the 1-subset (categorical) distribution where marginals can be computed in closed form.

Differentiating through Combi- Table 2: Results for the Warcraft shortest path task. Reported is the

natorial Solvers. In these ex- accuracy, i.e. percentage of paths with the optimal costs. Standard

periments, proposed by Pogancic´ deviations are over five runs.

et al. [2019], the training datasets

consists of 10,000 examples of K I-MLE (µ-µ) I-MLE (M-M)

BB

DPO

randomly generated images of ter- 12 97.2 ± 0.5 rain maps from the Warcraft II tile 18 95.8 ± 0.7

95.2 ± 0.3 95.2 ± 0.7 94.8 ± 0.3 94.4 ± 0.5 94.7 ± 0.4 92.3 ± 0.8

set [Guyomarch, 2017]. Each ex- 24 94.3 ± 1.0 93.2 ± 0.2 93.8 ± 0.3 91.5 ± 0.4

ample has an underlying K × K 30 93.6 ± 0.4 93.7 ± 0.6 93.6 ± 0.5 91.5 ± 0.8

grid whose cells represent terrains

with a fixed cost. The shortest (minimum cost) path between the top-left and bottom-right cell in

the grid is encoded as an indicator matrix and serves as the target output. An image of the terrain

map is presented to a CNN, which produces a K × K matrix of vertex costs. These costs are then

given to Dijkstra's algorithm (the MAP solver) to compute the shortest path. We closely follow

the evaluation protocol of Pogancic´ et al. [2019]. We considered two instantiations of I-MLE: one

derived from Fact 1 (M-M in Table 2) using H and one derived from Fact 2 (µ-µ) using R, with ( ) = SoG(k, 1, 10) where k is the empirical mean of the path lengths (different for each grid size

K). We compare with the method proposed by Pogancic´ et al. [2019] (BB10) and Berthet et al. [2020]

(DPO). The results are listed in Table 2. I-MLE obtains results comparable to (BB) with M-M and is

more accurate with µ-µ. We believe that the µ-µ advantage may be partially due to an implicit form

of data augmentation since we know from Fact 2 that, by using I-MLE, we obtain samples from the

distribution whose parameters are the optimal cost. Training dynamics, showing faster convergence

of I-MLE (µ-µ), and additional details are available in Table 4.

7 Conclusions
I-MLE is an efficient, simple-to-implement, and general-purpose framework for learning hybrid models. I-MLE is competitive with relaxation-based approaches for discrete latent-variable models and with approaches to backpropagate through CO solvers. Moreover, we showed empirically that I-MLE outperforms the straight-through estimator. A limitation of the work is its dependency on computing MAP states which is, in general, an NP-hard problem (although for many interesting cases there are efficient algorithms). Future work includes devising target distributions when zL is not available, studying the properties (including the bias) of the proposed estimator, developing adaptive strategies for  and , and integrating and testing I-MLE in several challenging application domains.
10Note that this is the same as using I-MLE with PID target distribution form Eq. (8) and ( ) = 0.

9

References
A. Agrawal, B. Amos, S. Barratt, S. Boyd, S. Diamond, and Z. Kolter. Differentiable convex optimization layers. arXiv preprint arXiv:1910.12430, 2019.
B. Amos and J. Z. Kolter. Optnet: Differentiable optimization as a layer in neural networks. In International Conference on Machine Learning, pages 136­145. PMLR, 2017.
Y. Bengio, N. Léonard, and A. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.
Y. Bengio, A. Lodi, and A. Prouvost. Machine learning for combinatorial optimization: a methodological tour d'horizon. European Journal of Operational Research, 2020.
Q. Berthet, M. Blondel, O. Teboul, M. Cuturi, J. Vert, and F. R. Bach. Learning with differentiable pertubed optimizers. In NeurIPS, 2020.
J. Chen, L. Song, M. Wainwright, and M. Jordan. Learning to explain: An information-theoretic perspective on model interpretation. In International Conference on Machine Learning, pages 883­892. PMLR, 2018.
X. Chen, Y. Zhang, C. Reisinger, and L. Song. Understanding deep architecture with reasoning layer. Advances in Neural Information Processing Systems, 33, 2020.
G. M. Correia, V. Niculae, W. Aziz, and A. F. Martins. Efficient marginalization of discrete and structured latent variables via sparsity. Advances in Neural Information Processing Systems, 2020.
J. Domke. Implicit differentiation by perturbation. In Advances in Neural Information Processing Systems 23, pages 523­531. 2010.
J. Domke. Generic methods for optimization-based modeling. In Artificial Intelligence and Statistics, pages 318­326. PMLR, 2012.
P. L. Donti, B. Amos, and J. Z. Kolter. Task-based end-to-end model learning in stochastic optimization. Advances in Neural Information Processing Systems, 2017.
A. N. Elmachtoub and P. Grigas. Smart "predict, then optimize", 2020. R. Evans and E. Grefenstette. Learning explanatory rules from noisy data. Journal of Artificial
Intelligence Research, 61:1­64, 2018. A. Ferber, B. Wilder, B. Dilkina, and M. Tambe. Mipaal: Mixed integer program as a layer. In
Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 1504­1511, 2020. L. Franceschi, P. Frasconi, S. Salzo, R. Grazzi, and M. Pontil. Bilevel programming for hyperparam-
eter optimization and meta-learning. In International Conference on Machine Learning, pages 1568­1577. PMLR, 2018. L. Franceschi, M. Niepert, M. Pontil, and X. He. Learning discrete structures for graph neural networks. In International conference on machine learning, pages 1972­1982. PMLR, 2019. W. Grathwohl, D. Choi, Y. Wu, G. Roeder, and D. Duvenaud. Backpropagation through the void: Optimizing control variates for black-box gradient estimation. ICLR, 2018. J. Guyomarch. Warcraft II Open-Source Map Editor. http://github.com/war2/war2edit, 2017. T. Hazan and T. Jaakkola. On the partition function and random maximum a-posteriori perturbations. In Proceedings of the 29th International Coference on International Conference on Machine Learning, pages 1667­1674, 2012. K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, pages 770­778. IEEE Computer Society, 2016. E. Jang, S. Gu, and B. Poole. Categorical reparameterization with gumbel-softmax. ICLR, 2017. N. L. Johnson and N. Balakrishnan. Advances in the theory and practice of statistics, 1998.
10

W. Kool, H. van Hoof, and M. Welling. Estimating gradients for discrete random variables by sampling without replacement. ICLR, 2020.
T. Lei, R. Barzilay, and T. Jaakkola. Rationalizing neural predictions. In EMNLP, 2016. R. Liu, J. Regier, N. Tripuraneni, M. Jordan, and J. Mcauliffe. Rao-blackwellized stochastic gradients
for discrete distributions. In International Conference on Machine Learning, pages 4023­4031. PMLR, 2019. G. Lorberbom, A. Gane, T. Jaakkola, and T. Hazan. Direct optimization through argmax for discrete variational auto-encoder. In Advances in Neural Information Processing Systems, pages 6203­6214, 2019. C. J. Maddison, A. Mnih, and Y. W. Teh. The concrete distribution: A continuous relaxation of discrete random variables. ICLR, 2017. J. Mandi and T. Guns. Interior point solving for lp-based prediction+optimisation. In Advances in Neural Information Processing Systems, 2020. J. Mandi, E. Demirovic´, P. J. Stuckey, and T. Guns. Smart predict-and-optimize for hard combinatorial optimization problems. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 1603­1610, 2020. D. A. McAllester, T. Hazan, and J. Keshet. Direct loss minimization for structured prediction. In Advances in Neural Information Processing Systems, volume 1, page 3, 2010. J. McAuley, J. Leskovec, and D. Jurafsky. Learning attitudes and attributes from multi-aspect reviews. 2012 IEEE 12th International Conference on Data Mining, pages 1020­1025, 2012. V. V. Misic´ and G. Perakis. Data analytics in operations management: A review. Manufacturing & Service Operations Management, 22(1):158­169, 2020. C. Mortici. Fast convergences towards Euler-Mascheroni constant. Computational & Applied Mathematics, 29, 00 2010. K. P. Murphy. Machine learning: a probabilistic perspective. MIT press, 2012. V. Niculae and A. F. T. Martins. Lp-sparsemap: Differentiable relaxed optimization for sparse structured prediction. In ICML, 2020. V. Niculae, A. F. T. Martins, M. Blondel, and C. Cardie. Sparsemap: Differentiable sparse structured inference. In ICML, 2018. G. Papandreou and A. L. Yuille. Perturb-and-map random fields: Using discrete optimization to learn and sample from energy models. In 2011 International Conference on Computer Vision, pages 193­200, 2011. M. B. Paulus, D. Choi, D. Tarlow, A. Krause, and C. J. Maddison. Gradient estimation with stochastic softmax tricks. arXiv preprint arXiv:2006.08063, 2020. M. V. Pogancic´, A. Paulus, V. Musil, G. Martius, and M. Rolinek. Differentiation of blackbox combinatorial solvers. In International Conference on Learning Representations, 2019. L. D. Raedt, K. Kersting, S. Natarajan, and D. Poole. Statistical relational artificial intelligence: Logic, probability, and computation. Synthesis Lectures on Artificial Intelligence and Machine Learning, 10(2):1­189, 2016. M. Rolínek, P. Swoboda, D. Zietlow, A. Paulus, V. Musil, and G. Martius. Deep graph matching via blackbox differentiation of combinatorial solvers. In ECCV, 2020. J. Schulman, N. Heess, T. Weber, and P. Abbeel. Gradient estimation using stochastic computation graphs. In Proceedings of the 28th International Conference on Neural Information Processing Systems-Volume 2, pages 3528­3536, 2015.
11

T. Shpakova and F. Bach. Parameter learning for log-supermodular distributions. In Advances in Neural Information Processing Systems, volume 29, pages 3234­3242, 2016.
Y. Song, A. Schwing, R. Urtasun, et al. Training deep neural networks via direct loss minimization. In International Conference on Machine Learning, pages 2169­2177, 2016.
G. Tucker, A. Mnih, C. J. Maddison, D. Lawson, and J. Sohl-Dickstein. Rebar: Low-variance, unbiased gradient estimates for discrete latent variable models. Advances in Neural Information Processing Systems, 2017.
M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational inference. Now Publishers Inc, 2008.
P.-W. Wang, P. Donti, B. Wilder, and Z. Kolter. Satnet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver. In International Conference on Machine Learning, pages 6545­6554. PMLR, 2019.
B. Wilder, B. Dilkina, and M. Tambe. Melding the data-decisions pipeline: Decision-focused learning for combinatorial optimization. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 1658­1665, 2019.
Xi'an. Which pdf of x leads to a gumbel distribution of the finite-size average of x? Cross Validated, 2016. URL https://stats.stackexchange.com/q/214875. URL:https://stats.stackexchange.com/q/214875 (version: 2016-05-27).
S. M. Xie and S. Ermon. Reparameterizable subset sampling via continuous relaxations. In IJCAI, 2019.
12

A Standard Maximum Likelihood Estimation and Links to I-MLE

In the standard MLE setting [see, e.g., Murphy, 2012, Ch. 9] we are interested in learning the parameters of a probability distribution, here assumed to be from the (constrained) exponential family (see Eq. (3)), given a set of example states. More specifically, given training data D = {z^j}Nj=1, with z^j  C  {0, 1}m, maximum-likelihood learning aims to minimize the empirical risk

L(, qD)

=

Ez^qD [- log p(z^; )]

=

1 N

N

- log p(z^j; )

=

1 N

N

(A() -

z^j,  )

(13)

j=1

j=1

with respect to , where qD(z) = j z^j (z)/N is the empirical data distribution and z^ is the Dirac delta centered in z^. In Eq. (13), the point-wise loss is the negative log likelihood -p(z^, ), and qD may be seen as a (data/empirical) target distribution. Note that in the main paper, as we assumed q to be from the exponential family with parameters  , we used the notation L(,  ) to indicate the MLE objective rather than L(, q). These two definitions are, however, essentially equivalent.

Eq. (13) is a smooth objective that can be optimized with a (stochastic) gradient descent procedure. For a data point z^, the gradient of the point-wise loss is given by  = µ() - z^, since A = µ. For the entire dataset one has

 L(,

qD )

=

µ()

-

1 N

N

z^j = µ() - Ez^qD [z^j ]

(14)

j=1

which is (cf. Eq. (5)) the difference between the marginals of p(z; ) (the mean of Z) and the empirical mean of D, µ(qD) = Ez^D[z^]. As mentioned in the main paper, the main computational challenge when evaluating Eq. (14) is to compute the marginals (of p(z; )). There are many approximate schemes, one of which is the so-called perceptron rule, which approximate Eq. (14) as

L(, qD)

=

MAP()

-

1 N

N

z^j

j=1

and it is frequently employed in a stochastic manner by sampling one or more points form D, rather than computing the full dataset mean.

We may interpret the standard MLE setting described in this section from the perspective of the problem setting we presented in Section 2. The first indeed amounts to the special case of the latter where there are no inputs (X = ), the (target) output space coincides with the state space of the distribution (Y = Z), f is the identity mapping, is the negative log-likelihood and the model's parameter coincide with the distribution parameters, that is  = .

B Algorithm

Algorithm 1 shows the pseudo-code of the algorithm implementing Eq. (7) for S = 1, using perturbation-based implicit differentiation target distributions (see Section 3.1). This demonstrates that I-MLE can be implemented and used as a layer in deep learning frameworks such as PyTorch and TensorFlow.

Algorithm 1 Forward and Backward Pass
function FORWARDPASS() // Sample from the noise distribution ( )  ( ) // Compute a MAP state of perturbed  z^ = MAP( + ) save , , and z^ for the backward pass return z^

function BACKWARDPASS(z (fu(z), y^), ) load , , and z^ from the forward pass // Compute target distribution parameters  =  - z (fu(z), y^) // Single sample I-MLE gradient estimate L(^, ^ ) = z^ - MAP( + )
return L(^, ^ )

13

C Proofs of Section 3.2 and Section 4
This section contains the proofs of the results relative to the perturb and map section (Section 3.2) and the section on optimal target distributions for typical loss functions when backpropagating through combinatorial optimization problems (section 4). We repeat the statements here, for the convenience of the reader. Proposition 1. Let p(z; ) be a discrete exponential family distribution with constraints C and temperature  , and let , z be the unnormalized weight of each z with z  C. Moreover, let ~ be such that, for all z  C,
z, ~ = z,  + (z) with each (z) i.i.d. samples from Gumbel(0,  ). Then,
Pr MAP(~) = z = p(z; ).

Proof. Let i  Gumbel(0,  ) i.i.d. and ~i = i + i. Following a derivation similar to one made in Papandreou and Yuille [2011], we have:

Pr{arg max(~1, . . . , ~m) = n} =

= Pr{~n  max{~j}}
j=n

+

=

g(t; n) G(t; j) dt

-

j=n

=

+ 1 exp

n

-

t

-

e n-t 

exp

-e j -t 

dt

- 



j=n

=

+

1 n-t e

exp

-e n-t 

exp

-e j -t 

dt

- 

j=n

1
=

zexp

j -n 

dz

with z

exp

-e n-t 

0 j=n

= 1+

1
j -n
j=n e 

e n 

=

m j=1

e

j 

,

where g and G are respectively the Gumbel probability density function and the Gumbel cumulative density function. The proposition now follows from arguments made in Papandreou and Yuille [2011] using the maximal equivalent re-parameterization of p(z; ) where we specify a parameter , z for each z with C(z) and perturb these parameters.

Lemma 1. Let X  Gumbel(0,  ) and let   N \ {0}. Then we can write

X  

s
lim {Gamma(1/, /i)} - log(s) ,

 s

j=1

i=1

where Gamma(, ) is the gamma distribution with shape  and scale .

Proof. Let   N \ {0} and let X  Gumbel(0,  ). Its moment generating function has the form

E[exp(tX)] = (1 -  t).

(15)

As mentioned in Johnson and Balakrishnan [p. 443, 1998] we know that we can write the Gamma

function as


(1 -  t) = et

1 - t

-1
- t
ei

(16)

i

i=1

14

where  is the Euler-Mascheroni constant. We have that

1 - t i

-1
=

i i - t

=

i  i- t 

=

i 

i



-

t 

=

i 

i
. -t

The last term is the moment generating function of an exponential now take the logarithm on both sides of Eq. (16) and obtain

distribution

with

scale

 i

.

We

can

s
tX =  t + lim

t Exp( /i) -  t ,

s

i

i=1

where Exp() is the exponential distribution with scale . Hence,

s
X  lim

Exp( /i) -  + 

s

i

i=1

s
= lim

Exp( /i) -  +  lim s 1 - log(s)

s

i

s

i

i=1

i=1

s
= lim

Exp( /i) -  +  -  log(s)

s

ii

i=1

s

= lim Exp( /i) -  log(s)
s i=1

Since Exp()  Gamma(1, ), and due to the scaling and summation properties of the Gamma distribution (with shape-scale parameterization), we can write for all r > 1:
r
Exp()  Gamma(1/r, r)/r.
j=1
Hence, picking r =  from the hypothesis, we have





s





X  lim

Gamma(1/,  /i)/ -  log(s)

s

 i=1 j=1








 = lim



s





Gamma(1/, /i) -

 log(s)

s
j=1

k

i=1




j=1

 = lim
s 
j=1

s
Gamma(1/, /i) - log(s)
i=1

 =

lim

s
Gamma(1/, /i) - log(s)

 s

j=1

i=1

This concludes the proof. Parts of the proof are inspired by a post on stackexchange Xi'an [2016].

Theorem 1. Let p(z; ) be a discrete exponential family distribution with constraints C and temperature  , and let k  N \ {0}. Let us assume that if C(z) then z, 1 = k. Let ~ be the perturbation obtained by ~i = i + i with

i



 k

s
lim {Gamma(1/k, k/i)} - log(s)
s

,

(17)

i=1

where Gamma(, ) is the gamma distribution with shape  and scale . Then, for every z we have that z, ~ = z,  + (z) with (z)  Gumbel(0,  ).

15

Proof. Since we perturb each i by i we have, by assumption, that , 1 = k, for every z with C(z), that

k

z, ~ = z,  + j.

(18)

j=1

Since by Lemma 1 we know that

k j=1

i



Gumbel(0,

 ),

the

statement

of

the

theorem

follows.

The following theorem shows that the infinite series from Lemma 1 can be well approximated by a finite sum using convergence results for the Euler-Mascheroni series.

Theorem 2. Let X  Gumbel(0,  ) and X~ (m) 

 j=1

 

[

m i=1{Gamma(1/, /i)} - log(m)].

Then

 2(m +

1)

<

E[X~ (m)]

-

E[X ]

<

 .
2m

Proof. We have that

E[X~ (m)] =





 = E 

m
{Gamma(1/, /i)} - log(m) 

j=1 i=1

m
= E [Gamma(1/,  /i)] -  log(m)

i=1

= m 1   -  log(m) i
i=1

=  m 1 - log(m) . i
i=1
If X  Gumbel(0,  ), we know that E [X] =  . Hence,

E[X~ (m)] - E[X] = 

m 1 - log(m) -   i

i=1

=  m 1 - log(m) -  . i
i=1
The theorem now follows from convergence results of the Euler-Mascheroni series Mortici [2010].

Fact 1. If one uses H , then I-MLE with the target distribution of Eq. (12) and ( ) = 0( ) is equivalent to the perceptron-rule estimator of the MLE objective between p(z; hv(x^j)) and y^j.

Proof. Rewriting the definition of the Hamming loss gives us

1 H (z, y) = m

m

(zi + yi - 2ziyi) .

i=1

Hence, we have that

zi

H

=

1 m

(1

-

2yi)

.

Therefore, that

zi

H

=

-

1 m

if yi

= 1 and zi H

=

1 m

if yi = 0. Since, by definition y  C, we have

MAP (-z H ) = y. Now, when using I-MLE with S = 1 and ( ) = 0( ) we approximate the gradients as

L(,  ) = MAP() - MAP( ) = MAP() - MAP (-z H ) = MAP() - y. This concludes the proof.

16

Fact 2. If one uses R then I-MLE with the target distribution of Eq. (12) is equivalent to the perturb-and-MAP estimator of the MLE objective between p(z; hv(x^j)) and p(z; -c^j).

Proof. of Eq.

(W12e)h(aavnedtwhaitthozuit

R = ci for all i. Now, loss of generality, for

when using S = 1) we

I-MLE with target distribution have that q^(z;  ) = p(z; -c),

q^(z; and

) we

approximate the gradients as

L(,  ) = MAP( + i) - MAP( + i) = MAP( + i) - MAP(-c + i), where i  ( ). Hence, I-MLE approximates the gradients of the maximum likelihood estimation problem between p(z; hv(x^j)) and p(z; -c^j) using perturb-and-MAP. This concludes the proof.

17

Learning rate

I-MLE PaM SoG

1.0

0.5

0.625

0.8

0.75

0.6

0.875

0.4

1.0

0.5 1.0 1.5 2.0 2.5 3.0 0.2 Lambda

Learning rate

I-MLE PaM Gumbel

0.5

1.25

0.625

1.00

0.75

0.75

0.875 1.0

0.50

0.5 1.0 1.5 2.0 2.5 3.0 0.25

Lambda

Learning rate

I-MLE SoG - Gum. (means) 0.2

0.5

0.0

0.625

0.2

0.75

0.4

0.875

0.6

1.0

0.8

0.5 1.0 1.5 2.0 2.5 3.0 1.0 Lambda

Figure 5: Average (over 100 runs) values of L() after 50 steps of stochastic gradient descent (with momentum) using single-sample I-MLE with SoG(1, 5, 10) noise (left) and Gumbel(0, 1) noise (center) varying the perturbation intensity  (see Eq. (8)) and learning rate. The rightmost heat-map depicts the (point-wise) difference between the two methods (blue = better SoG).

Learning rate

I-MLE PaM SoG - std 0.8
0.5

0.625

0.6

0.75

0.875

0.4

1.0 0.5 1.0 1.5 2.0 2.5 3.0 0.2 Lambda

Learning rate

I-MLE PaM Gumbel - std

0.5

1.50

0.625

1.25

0.75

1.00

0.875

0.75

1.0

0.50

0.5 1.0 1.5 2.0 2.5 3.0 0.25 Lambda

Learning rate

I-MLE SoG - Gum. (std)
0.5

0.0

0.625

0.2

0.75

0.4

0.875

0.6

1.0

0.8

0.5 1.0 1.5 2.0 2.5 3.0 1.0 Lambda

Figure 6: Same as above, but reporting standard deviations.

D Experiments: Details and Additional Results

D.1 Synthetic Experiments

In this series of experiments we analyzed the behaviour of various discrete gradient estimators,

comparing our proposed I-MLE with standard straight-trhough (STE) and score-function (SFE)

estimators. We also study of the effect of using Sum-of-Gamma perturbations rather than stan-

dard Gumbel noise. In order to be able to compute exactly (up to numerical precision) all the

quantities involved, we chose a tractable 5-subset distribution (see Example 2) of size m = 10.

We set the loss to L() = Ez^p(z;)[ z^ - b 2], where b

0.6

is a fixed vector sampled (only once) from N (0, I). This

amounts to an unconditional setup where there are no input

features (as in the standard MLE setting of Appendix A), but

0.4

Expect. Sample P&M

Seconds

where the point-wise loss (z) is the (Euclidian) distance

between the distribution output and a fixed vector b. In Fig. 7 we plot the runtime (mean and standard deviation over 10

0.2

evaluations) of the full objective L (Expect., in the plot), of a

(faithful) sample of and of a perturb-and-MAP sample with Sum-of-Gamma noise distribution (P&M) for increasing size m, with k = m/2. As it is evident from the plot, the runtime for both expectation and faithful samples, which require computing all the states in C, increases exponentially, while perturb and MAP remains almost constant.

0.0

10

15

20

Size (m)

Figure 7: Runtime (mean and standard

deviation) for computing L and samples

of it, as the dimensionality of the k-subset

distribution increases (with k = m/2).

Within this setting, the one-sample I-MLE estimator is

I-MLEL() = MAP( + ) - MAP( + ), with  SoG(1, 5, 10) where  =  - [2(z^ - b)], where z^ = MAP( + ) is a (perturb-and-MAP) sample, while the one-sample straight through estimator is

STEL() = 2(z^ - b), with z^ = MAP( + ),  Gumbel(0, 1). For the score function estimator, we have used an expansive faithful sample/full marginal implementation given by

SFEL() = z^ - b 2  log p(z^; ) = z^ - b 2 [z^ - µ()], with z^  p(z; )

18

Learning rate Learning rate
Learning rate
Learning rate Learning rate

STE PaM SoG

0.0010 0.0018

3.5

0.0032 0.0058

3.0

0.0105 0.0190

2.5

0.0342 0.0616

2.0

0.1110 0.2000

1.5

STE PaM Gumbel

0.0010

0.0018

0.0032 0.0058

3

0.0105

0.0190 0.0342

2

0.0616

0.1110 0.2000

1

STE SoG - Gum. (means)

0.0010

00..00001382

0.4

0.0058 0.0105

0.3

0.0190 0.0342

0.2

0.0616 0.1110 0.2000

0.1 0.0

SFE SM mean

0.0001

0.0002 0.0005

3

0.0010

0.0022 0.0046

2

0.0100

0.0215 0.0464

1

0.1000

SFE SM std

0.0001 0.0002

1.5

0.0005

0.0010 0.0022

1.0

0.0046

0.0100 0.0215

0.5

0.0464

0.1000

Figure 8: First three plots, from left to right: average (over 100 runs) final values of L() after 50 steps of optimization using the straight-through estimator with SoG noise, varying the learning rate; same but using Gumbel noise; difference of averages between the first and the second heat-maps. Last two plots, from left to right: average (over 20 runs) final values of L() after 500 steps of optimization using the score function estimator (with faithful samples and exact marginals), varying the learning rate; standard deviation for the same setting.

Method L2X (t = 0.1) SoftSub (t = 0.5) I-MLE ( = 30)

Appearance

Test MSE Subset precision

10.70 ± 4.82 2.48 ± 0.10 2.51 ± 0.05

30.02 ± 15.82 52.86 ± 7.08 65.47 ± 4.95

Palate

Test MSE Subset precision

6.70 ± 0.63 2.94 ± 0.08 2.96 ± 0.04

50.39 ± 13.58 39.17 ± 3.17 40.73 ± 3.15

Taste

Test MSE Subset precision

6.92 ± 1.61 2.18 ± 0.10 2.38 ± 0.04

32.23 ± 4.92 41.98 ± 1.42 41.38 ± 1.55

Table 3: Experimental results (mean ± std. dev.) for the learning to explain experiments for k = 10 and various aspects.

since, in preliminary experiments, we did not manage to obtain meaningful results with SFE using perturb-and-MAP for sampling and/or marginals approximation. These equations give the formulae for the estimators which we used for the results plotted in Fig. 3 (top) in the main paper. In Fig. 5 we plot the heat-maps for the sensitivity results comparing between I-MLE with SoG and I-MLE with Gumbel perturbations. The two leftmost heat-maps depict the average value (over 100 runs) of L() after 50 steps of stochastic gradient descent, for various choices of  and learning rates (momentum factor was fixed at 0.9 for all experiments). The rightmost plot of Fig. 5 is the same as the one in the main paper, and represents the difference between the first and the second heat-maps. Fig. 6 refers to the same setting, but this time showing standard deviations. The rightmost plot of Fig. 6 suggests that using SoG perturbations results also in reduced variance (of the final loss) for most of the tried hyperparameter combinations. Finally, in Fig. 8 we show sensitivity plots for STE (both with SoG and Gumbel perturbations) and SFE, where we vary the learning rate.
D.2 Learning to Explain Experiments were run on a server with Intel(R) Xeon(R) CPU E5-2637 v4 @ 3.50GHz, 4 GeForce GTX 1080 Ti, and 128 GB RAM. The pre-trained word embeddings and data set can be found here: http://people.csail.mit. edu/taolei/beer/. Figure 10 depicts the neural network architecture used for the experiments. As in prior work, we use a batch size of 40. The maximum review length is 350 tokens. We use the standard neural network architecture from prior work Chen et al. [2018], Paulus et al. [2020]. The dimensions of the token embeddings (of the embedding layers) are 200. All 1D convolutional layers have 250 filters with a kernel size of 3. All dense layers have a dimension of 100. The dropout layer has a dropout rate of 0.2. The layer Multiply perform the multiplication between the token mask (output of I-MLE) and the embedding matrix. The Lambda layer computes the mean of the selected embedding vectors. The last dense layer has a sigmoid activation. IMLESubsetkLayer is the layer implementing I-MLE. We train for 20 epochs using the standard Adam settings in Tensorflow 2.4.1 (learning rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-07, amsgrad=False), and no learning rate schedule. The training time (for the 20 epochs) for I-MLE, with sum-of-Gamma perturbations, is 380 seconds, for SoftSub 360 seconds, and for L2X 340 seconds. We always evaluate the model with the best validation MSE among the 20 epochs.
19

Table 4: Sample of Warcraft maps, and corresponding shortest paths from the upper left to the lower right corner of the map.
Implementations of I-MLE and all experiments will soon be made available. Table 3 lists the results for L2X, SoftSub, and I-MLE for three additional aromas and k = 10. D.3 Discrete Variational Auto-Encoder Experiments were run on a server with Intel(R) Xeon(R) CPU E5-2637 v4 @ 3.50GHz, 4 GeForce GTX 1080 Ti, and 128 GB RAM. The data set can be loaded in Tensorflow 2.x with tf.keras.datasets.mnist.load_data(). As in prior work, we use a batch size of 100 and train for 100 epochs, plotting the test loss after each epoch. We use the standard Adam settings in Tensorflow 2.4.1 (learning rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-07, amsgrad=False), and no learning rate schedule. The MNIST dataset consists in black-and-white 28 × 28 pixels images of hand-written digits. The encoder network consists of an input layer with dimension 784 (we flatten the images), a dense layer with dimension 512 and ReLu activation, a dense layer with dimension 256 and ReLu activation, and a dense layer with dimension 400 (20 × 20) which outputs the  and no non-linearity. The IMLESubsetkLayer takes  as input and outputs a discrete latent code of size 20 × 20. The decoder network, which takes this discrete latent code as input, consists of a dense layer with dimension 256 and ReLu activation, a dense layer with dimension 512 and ReLu activation, and finally a dense layer with dimension 784 returning the logits for the output pixels. Sigmoids are applied to these logits and the binary cross-entropy
20

Figure 9: Original MNIST digits from the test set and their reconstructions using the discrete 10-subset VAE trained with Sum-of-Gamma perturbations for  = 1 (center) and  = 10 (right). loss is computed. The training time (for the 100 epochs) was 21 minutes with the sum-of-Gamma perturbations and 18 minutes for the standard Gumbel perturbations. The code is available in the submission system as a Jupyter notebook. D.4 Differentiating through Combinatorial Solvers The experiments were run on a server with Intel(R) Xeon(R) Silver 4208 CPU @ 2.10GHz CPUs, 4 NVIDIA Titan RTX GPUs, and 256 GB main memory. Table 4 shows a set of 30 × 30 Warcraft maps, and the corresponding shortest paths from the upper left to the lower right corner of the map. In these experiments, we follow the same experimental protocol of Pogancic´ et al. [2019]: optimisation was carried out via the Adam optimiser, with scheduled learning rate drops dividing the learning rate by 10 at epochs 30 and 40. The initial learning rate was 5 × 10-4, and the models were trained for 50 epochs using 70 as the batch size. As in [Pogancic´ et al., 2019], the K × K weights matrix is produced by a subset of the ResNet18 [He et al., 2016] architecture whose weights are trained on the task. For training BB, in all experimental results in Section 6 and Table 4, the hyperparameter  was set to  = 20. Fig. 11 shows the training dynamics of different models, including the method proposed by Pogancic´ et al. [2019] (BB) with different choices of the  hyperparameter, the ResNet18 baseline proposed by Pogancic´ et al. [2019], and I-MLE. Code and data for the experiments will soon be made available.
21

input_3: InputLayer

input: output:

[(?, 350)] [(?, 350)]

emb_gumbel: Embedding

input: output:

(?, 350) (?, 350, 200)

conv1_gumbel: Conv1D

input: output:

(?, 350, 200) (?, 350, 100)

embedding_2: Embedding

input: output:

(?, 350) (?, 350, 200)

new_global_max_pooling1d_1: GlobalMaxPooling1D

input: output:

(?, 350, 100) (?, 100)

conv2_gumbel: Conv1D

input: output:

(?, 350, 100) (?, 350, 100)

new_dense_1: Dense

input: output:

(?, 100) (?, 100)

conv3_gumbel: Conv1D

input: output:

(?, 350, 100) (?, 350, 100)

concatenate_2: Concatenate

input: output:

[(?, 100), (?, 350, 100)] (?, 350, 200)

new_dropout_2: Dropout

input: output:

(?, 350, 200) (?, 350, 200)

conv_last_gumbel: Conv1D

input: output:

(?, 350, 200) (?, 350, 100)

conv4_gumbel: Conv1D

input: output:

(?, 350, 100) (?, 350, 1)

imle_subsetk_layer_1: IMLESubsetkLayer

input: output:

(?, 350, 1) (?, 350, 1)

multiply_2: Multiply

input: output:

[(?, 350, 200), (?, 350, 1)] (?, 350, 200)

lambda_1: Lambda

input: output:

(?, 350, 200) (?, 200)

dense_2: Dense

input: output:

(?, 200) (?, 250)

new_dense: Dense

input: output:

(?, 250) (?, 1)

Figure 10: The neural network architecture for the learning to explain experiments. (Please zoom into the vector graphic for more details.) We use the standard architecture and settings from prior work Chen et al. [2018]. The maximum review length is 350 tokens. The dimensions of the token embeddings (of the embedding layers) are 200. All 1D convolutional layers have 250 filters with a kernel size of 3. All dense layers have a dimension of 100. The dropout layer has a dropout rate of 0.2. The layer Multiply perform the multiplication between the token mask (output of I-MLE) and the embedding matrix. The Lambda layer computes the mean of the selected embedding vectors The last dense layer has a sigmoid activation. IMLESubsetkLayer is the layer implementing I-MLE. Code is available in the submission system.

22

WarCraft - Accuracy on 12 × 12 Maps
100

80

Accuracy

60

I-MLE (MAP MAP)

40

I-MLE ( )

BB ( = 20)

20

DPO ( = 0.01)

ResNet18 (Vlastelica et al. 2019)

0

0

10

20

30

40

50

100

WarCraft

-

Epoch Accuracy on

18

×

18

Maps

80

Accuracy

60

I-MLE (MAP MAP)

40

I-MLE ( )

BB ( = 20)

20

DPO ( = 0.01)

ResNet18 (Vlastelica et al. 2019)

0

0

10

20

30

40

50

100

WarCraft

-

Epoch Accuracy on

24

×

24

Maps

80

Accuracy

60

I-MLE (MAP MAP)

40

I-MLE ( )

BB ( = 20)

20

DPO ( = 0.01)

ResNet18 (Vlastelica et al. 2019)

0

0

10

20

30

40

50

WarCraft

-

Epoch Accuracy on

30

×

30

Maps

100

80

Accuracy

60

I-MLE (MAP MAP)

40

I-MLE ( )

BB ( = 20)

20

DPO ( = 0.01)

ResNet18 (Vlastelica et al. 2019)

0

0

10

20

30

40

50

Epoch

Figure 11: Training dynamics for different models on K × K shortest path tasks on Warcraft maps, with K  {12, 18, 24, 30}.

23

