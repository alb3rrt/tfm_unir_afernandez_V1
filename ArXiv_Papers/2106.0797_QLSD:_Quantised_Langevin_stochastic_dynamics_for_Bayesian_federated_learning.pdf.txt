QLSD: Quantised Langevin stochastic dynamics for Bayesian federated learning

arXiv:2106.00797v1 [cs.LG] 1 Jun 2021

Maxime Vono Lagrange Mathematics and Computing Research Center, Huawei
75007 Paris, France maxime.vono@huawei.com

Vincent Plassier
CMAP, École Polytechnique Institut Polytechnique de Paris Lagrange Mathematics and Computing Research Center, Huawei vincent.plassier@polytechnique.edu

Alain Durmus Université Paris-Saclay ENS Paris-Saclay, CNRS Centre Borelli, F-91190 Gif-sur-Yvette, France alain.durmus@ens-paris-saclay.fr

Aymeric Dieuleveut
CMAP, École Polytechnique Institut Polytechnique de Paris aymeric.dieuleveut@polytechnique.edu

Éric Moulines
CMAP, École Polytechnique Institut Polytechnique de Paris eric.moulines@polytechnique.edu

Abstract
Federated learning aims at conducting inference when data are decentralised and locally stored on several clients, under two main constraints: data ownership and communication overhead. In this paper, we address these issues under the Bayesian paradigm. To this end, we propose a novel Markov chain Monte Carlo algorithm coined QLSD built upon quantised versions of stochastic gradient Langevin dynamics. To improve performance in a big data regime, we introduce variance-reduced alternatives of our methodology referred to as QLSD and QLSD++. We provide both non-asymptotic and asymptotic convergence guarantees for the proposed algorithms and illustrate their benefits on several federated learning benchmarks.
1 Introduction
In the modern big data era, it has become commonplace to acquire and process a large amount of data at the edge nodes of a network. These nodes are typically devices (coined clients), such as IoT sensors or mobiles, coordinated by a central server [21, 26, 30, 37, 41]. This rapid progress in data acquisition and storage has contributed to bring out new paradigms regarding the access to the data and their use in machine learning. First, communication overhead is limited in many scenarios. For example, upload bandwidth is in most cases limited to 1 MB/s or less [35]. This is in contrast to traditional distributed machine learning where computational costs dominate. A second prime
Preprint. Under review.

concern in this context is data ownership [1] which presupposes a restricted access to the data stored on each client.
Federated learning (FL) is a particular branch of machine learning which aims at addressing these new challenges [30]. Similarly to traditional machine learning, the current FL paradigm considers the minimisation of an empirical risk function defined over the entire dataset gathered from each client. As such, many methods derived from stochastic gradient descent techniques have been proposed in the literature to meet the specific FL constraints [5, 25, 31, 33, 35, 40]. Whilst these approaches have successfully solved some issues associated to FL, they are unable to capture and quantify epistemic predictive uncertainty which is essential in many applications such as autonomous driving or precision medicine [20, 27]. The Bayesian paradigm [45] stands for a natural candidate to quantify uncertainty and as such has become ubiquitous in the machine learning community [6]. Many recent proposals have focused on scaling serial workhorses Bayesian methods such as variational and Markov chain Monte Carlo (MCMC) algorithms to distributed architectures [3, 11, 29, 42, 49, 53­55]; but very few have tried to address the challenges inherent in FL [19, 32].
In this paper, we address this problem by proposing a novel Bayesian FL approach based on stochastic Langevin dynamics. The posterior distribution is assumed to admit a probability density function which stands for the product of local posterior densities associated to each client. At each iteration of the algorithm, each client computes a stochastic gradient oracle of its associated negative log posterior density. However, under the considered FL framework, the upload period has a very significant practical impact [30, Section 3.5]. This communication bottleneck is addressed by making each client sending lossy compression of their stochastic gradient oracles to the central server, so that the number of communicated bits during the upload period is significantly reduced [2, 15]. This approach coined quantised Langevin stochastic dynamics and referred to as QLSD can be interestingly seen as a MCMC counterpart of the QSGD approach in FL [5], as stochastic gradient Langevin dynamics (SGLD) is for stochastic gradient descent (SGD) [56]. Because of the use of stochastic gradients, the proposed methodology has the same drawbacks as SGLD. Notably, the invariant distribution associated with QLSD might depart from the target distribution and become similar to the invariant measure of SGD when the number of observations is large [9]. We overcome this issue by deriving two variance-reduced versions of QLSD referred to as QLSD and QLSD++ which both involve control variates that are either fixed or iteratively updated.
Contributions. Our contribution is three-folded. (1) We propose a general MCMC algorithm coined QLSD especially designed for Bayesian inference under the FL paradigm along with two variancereduced alternatives. We also highlight the impact of the discrepancy between local posterior distributions and propose to mitigate its impact on convergence by either using biased stochastic gradients or introducing a memory mechanism similar to the one used in traditional risk-based FL [25]. (2) We provide a non-asymptotic convergence analysis of the proposed algorithms. This theoretical analysis is complemented by a consistency study in a big data regime. In particular, we show that variance reduction indeed allows the proposed MCMC algorithm to converge towards the target posterior distribution when the number of observations becomes large. (3) We illustrate the benefits of the proposed methodology on several FL benchmarks involving both synthetic and real datasets. Remarkably, we show that the proposed methodology allows the number of exchanged bits to be significantly reduced while keeping similar performances as its non-compressed counterpart.
Notations and conventions. The Euclidean norm on Rd is denoted by · and we set N = N\{0}. For n  N, we refer to {1, . . . , n} with the notation [n]. For N  N, we use N to denote the power set of [N ] and define N,n = {x  N : card(x) = n} for any n  [N ]. We denote by N(m, ) the Gaussian distribution with mean vector m and covariance matrix . We define the signum function, for any x  R, as sign(x) = 1{x  0} - 1{x < 0}. We define the Wasserstein distance of order 2 for any probability measures µ,  on Rd with finite 2-moment by W2(µ, ) = (infT (µ,) Rd×Rd  -  2d(,  ))1/2, where T (µ, ) is the set of transference plans of µ and .
2 Quantised Langevin stochastic dynamics
This section presents the considered Bayesian FL framework and introduces the proposed methodology called QLSD along with two specific variance-reduced instances.
2

Problem statement. We are interested in carrying out Bayesian inference about a parameter   Rd based on a dataset D. The posterior distribution of interest is assumed to admit a product-form
density with respect to the d-dimensional Lebesgue measure, i.e.

b

 ( | D) = Z- 1 e-Ui() ,

(1)

i=1

where b  N and Z = Rd

b i=1

e-Ui

()

d

is a

normalisation

constant.

This

framework

natu-

rally encompasses the considered Bayesian FL problem. In this context, {e-Ui }i[b] stand for the

unnormalised local posterior density functions associated to b clients, where each client i  [b] is

assumed to own a local dataset Di such that D = bi=1Di. For the sake of brevity, the dependency of Ui : Rd  R on the local dataset Di is notationally omitted.

A popular approach to sample from a target distribution with density  defined in (1) is based on

Langevin dynamics, i.e.

 dt = -U (t)dt + 2dBt ,

where U =

b i=1

Ui

and

(Bt)t0

is

a

d-dimensional

Brownian

motion.

Indeed, this stochastic

differential equation defines a strong Markov semigroup (Pt)t0 which is ergodic with respect to

 and converges in various metrics under mild assumptions on the potential U [8, 47]. However,

sampling Langevin dynamics is not an option in most cases and discretisation schemes have to be

used in place. The simpler and most popular choice is the Euler-Maruyama scheme which, starting from an initial point 0, defines a Markov chain (k)kN by the recursion for any k  N,

k+1 = k - U (k) + 2Zk+1 ,

where   (0, ¯], for some ¯ > 0, is a discretisation time-step and (Zk)kN is a sequence of
independent and identically distributed (i.i.d.) standard Gaussian random variables [23, 38, 47].
The corresponding scheme is referred to as the unadjusted Langevin algorithm (ULA). In machine learning applications, evaluating U can be either a computational bottleneck or not even feasible. To circumvent this issue, a common solution is to replace U by statistical estimators [39, 44]. More precisely, U is approximated by a function H : Rd × X1  Rd defined on some measurable space (X1, X1) endowed with a probability measure 1 such that U () = X1 H(, x(1))d1(x(1)). The associated recursion writes

k+1 = k - H(k, Xk(1+)1) + 2Zk+1 , k  N .

(2)

In a serial setting involving a single client which owns a dataset of size N  N, the potential

U writes U = U1 =

N j=1

U1,j

for some functions U1,j

:

Rd



R, and a popular instance of

this framework is SGLD [56]. This algorithm consists in the recursion (2) with the specific choice

H(, Xk(1+)1) = (N/n) jXk(1+)1 U1,j (), where (Xk(1))kN is a sequence of i.i.d. uniform random subsets of [N ] of cardinal n. Here, X1 = [N ] and 1 is the uniform distribution over N,n.

In the considered FL framework where the clients can perform parallel computations, we assume

that the i-th client has access to a stochastic gradient oracle Hi : Rd × X1  Rd associated to its

local negative log. posterior density Ui so that Hi only depends on Di. Therefore, H =

b i=1

Hi

is

itself a stochastic gradient oracle of U and admits a similar decomposition. Note that we do not nec-

essarily assume in what follows that {Hi}i[b] stand for unbiased estimators of {Ui}i[b] but only constrain H to be unbiased. This will allow us to consider biased local stochastic gradient oracles

with better convergence guarantees, see Section 3 for more details. A straightforward adaptation of

SGLD defined in (2) to the considered FL framework is given by the recursion

b

k+1 = k -  Hi(k, Xk(1+,i1)) + 2Zk+1 , k  N ,

(3)

i=1

where (Xk(1,1), . . . , Xk(1,b))kN is a sequence of i.i.d. random variables distributed according to 1b. When, for any i  [b], each potential function Ui also admits a finite-sum expression i.e.

Ui =

N j=1

Ui,j

where

Ui,j

:

Rd



R,

similarly

to

SGLD,

we

may

consider

for

example

the

local

stochastic gradient oracles Hi(, Xk(1+,i1)) = (N/n) jXk(1+,i1) Ui,j () where (Xk(i+,11))kN, i[b]

3

stand for i.i.d. uniform random subsets of [N ] of cardinal n. However, considering the MCMC algorithm associated with the recursion (3) is not adapted to the FL context. Indeed, this algorithm would suffer from the same issues as SGD in a risk-based minimisation context, especially a prohibitive communication overhead [22].

Proposed methodology. To address this problematic, we propose to reduce the number of bits communicated during the upload period by performing lossy compressions of {Hi}i[b] [2, 15]. This method has been extensively used in the risk-based FL literature [5, 24, 34, 48] but, up to the
authors' knowledge, has interestingly never been considered from a Bayesian perspective. To this purpose, we introduce a compression operator C : Rd × X2  Rd defined on some measurable space (X2, X2) endowed with a probability measure 2 which is unbiased, i.e. for any v  Rd, v = X2 C (v, x(2))d2(x(2)). Numerous compression operators have been proposed over the past few years ranging from quantisation operators [5, 50] to sparsification ones [4, 51]. As an example,
the seminal QSGD approach proposed in [5] is based on stochastic quantisation. More specifically, [5]
considers for C a component-wise quantisation operator parametrised by a number of quantisation levels s  1 defined, for any j  [d] and v = (v1, . . . , vd)  Rd, by

Cj(s)(v, j) = v · sign(vj) · ( s|vj|/ v + 1{j  s|vj|/ v - s|vj|/ v }) /s , (4)

where {j}j[d] is a sequence of i.i.d. uniform random variables on [0, 1]. In this particular case, we will denote the quantisation of v via (4) by C (s)(v, x(2)) = {Cj(s)(v, x(j2))}j[d] with x(2) = {x(j2)}j[d]  [0, 1]d.
The proposed general methodology, coined quantised Langevin stochastic dynamics (QLSD) stands for a compressed version of the specific instance of SGLD defined in (3). More precisely, QLSD is a MCMC algorithm associated with the Markov chain (k)kN starting from 0 and defined by

b

k+1 = k - 

C Hi(k, Xk(1+,i1)), Xk(2+,i1) + 2Zk+1 , k  N ,

(5)

i=1

where (Xk(2,1), . . . , Xk(2,b))kN is a sequence of i.i.d. random variables distributed according to 2b. The derivation of the QLSD in the considered Bayesian FL context is detailed in Algorithm 1. Note that key challenges associated to FL are addressed: each client does not broadcast its own data
and the communication bottleneck is overcome thanks to the compression step. In the particular case of the finite-sum setting, i.e. for the choice Hi(, x(1,i)) = (N/n) jx(1,i) Ui,j() for   Rd, x(1,i)  X1 = N,n and 1 being the uniform distribution on N,n, we refer to the corresponding instance of QLSD as QLSD#.

Algorithm 1 Quantised Langevin stochastic dynamics (QLSD)

Input: number of iterations K, compression operator C , stochastic gradient oracles {Hi}i[b], step-size   (0, ¯] and initial point 0. for k = 0 to K - 1 do
for i = 1 to b // In parallel on the b clients do Draw Xk(1+,i1)  1 and Xk(2+,i1)  2.
Compute and send gi,k+1 = C Hi(k, Xk(1+,i1)), Xk(2+,i1) to the central server.

end for

// On the central server

Compute gk+1 =

b i=1

gi,k+1.

Draw Zk+1  N(0d, Id)



Compute and send k+1 = k - gk+1 + 2Zk+1 to the b clients.

end for

Output: samples {k}Kk=0.

Variance-reduced alternatives. Consider the finite-sum setting i.e. for any i  [b], Ui =

N j=1

Ui,j ,

set

X1

=

[N ] and 1

the uniform distribution over N,n.

In that case,

as highlighted

4

in Section 1, SGLD-based approaches including Algorithm 1 are associated with an invariant distribution which might depart from the target posterior distribution as N goes to infinity because of
the use of stochastic gradients with large variance [7, 9]. We cope with this problem by proposing two variance-reduced alternatives of QLSD# detailed in Algorithm 2 which make use of control
variates. The most simple variance-reduced approach referred to as QLSD considers a fixed-point approach which uses a minimiser  of the potential U [7, 9]. In this scenario, the stochastic gradient oracles {Hi}i[b] write, for any i  [b],   Rd and x(1,i)  N,n, Hi(, x(1,i)) = (N/n) jx(1,i) [Ui,j () - Ui,j ( )]. Although Xb1 H(·, x(1))d1b(x(1)) = U (·), note that for any i  [b], Xb1 Hi(·, x(1,i))d1(x(1,i)) = Ui(·) so that {Hi}i[b] are not unbiased estimates of {Ui}i[b]. We show in Section 3 that introducing this bias improves the convergence properties of QLSD# with respect to the discrepancy between local posterior distributions.

Since the estimation of  in a FL context might add a computational burden to the sampling procedure, we propose another variance-reduced version of QLSD# coined QLSD++ which builds upon

stochastic variance reduced gradient (SVRG) and uses control variates (k)kN that are updated every l  N iterations [28]. In addition, we consider for {Hi}bi=1 stochastic gradient oracles which are updated through the scheme based on (k)kN. More precisely, at each iteration k  N and for i  [b], the stochastic gradient oracle Hi is defined, for any   Rd and x(1,i)  N,n, by
Hi(, x(1,i)) = (N/n) jx(1,i) [Ui,j () - Ui,j (k)] + Ui(k). In order to reduce the impact of local posterior discrepancy on convergence, we take inspiration from the risk-based FL literature

and consider a memory term (k(i))kN on each client i  [b] [16, 25]. Instead of directly compress-

ing {Hi}i[b] at each iteration k, we compress the difference Hi(, Xk(1+,i1)) - k(i), store it in gi,k+1,

and then compute the global stochastic gradient gk+1 =

b i=1

gi,k+1

+

k(i).

The memory term

(k(i))kN is then updated on each client by the recursion k(i+) 1 = k(i) + gi,k+1 where   (0, ¯],

for some ¯ > 0. The benefits of using this memory mechanism will be assessed theoretically in

Section 3 and illustrated numerically in Section 4.

3 Theoretical analysis

This section provides a detailed theoretical analysis of the proposed methodology. We will show the impact of the use of stochastic gradients and compression by deriving quantitative convergence bounds for QLSD detailed in Algorithm 1. Then, we will derive non-asymptotic convergence bounds for QLSD and QLSD++ and show explicitly that these variance-reduced algorithms indeed manage to reduce both the variance brought by stochastic gradients and the impact of local posterior discrepancy in the bounds we get for QLSD#.
We consider the following set of assumptions on the potential U and the compression operator C .
H1. For any i  [b], Ui is continuously differentiable. In addition, suppose that the following hold.
(i) U is m-strongly convex, i.e. for any 1, 2  Rd, U (1) - U (2), 1 - 2  m 1 - 2 2. (ii) U is L-Lipschitz, i.e. for any 1, 2  Rd, U (1) - U (2)  L 1 - 2 .

Note that H1-(i) implies that U admits a unique minimiser denoted by   Rd. Moreover, for any (1, 1)  Rd, H1-(i)-(ii) combined with [36, Equation 2.1.24] shows that

U (2) - U (1), 2 - 1



mL m+L

2 - 1

2

+

m

1 +

L

U (2) - U (1) 2 .

(6)

H2. There exists a probability measure 2 on a measurable space (X2, X2) and a measurable function C : Rd × X2  Rd such that the following conditions hold.
(i) For any v  Rd, X2 C (v, x(2)) 2(dx(2)) = v. (ii) There exists   R+, such that for any v  Rd, X2 C (v, x(2)) - v 2 2(dx(2))   v 2.

As an example, the assumption on the variance of the compression operator detailed inH2-(ii) is verified for the quantisation operator C (s) defined, for s  1, in (4) with  = min(d/s2, d/s) [5, Lemma 3.1].

5

Algorithm 2 Variance-reduced quantised Langevin stochastic dynamics (QLSD and QLSD++)

Input: minibatch size n, number of iterations K, compression operator C , step-size   (0, ¯]

with ¯ > 0, initial point 0 and   (0, ¯] with ¯ > 0 for QLSD++, or  for QLSD .

// Memory mechanism initialisation

Initialise {0(1), . . . , 0(b)} and 0 =

b i=1

0(i)

(for QLSD

,

set 0(i)

=

0

for any i



[b]).

for k = 0 to K - 1 do

// Update of the control variates Set k =  . // For QLSD if k  0 (mod l) then // For QLSD++
Set k = k. else
Set k = k-1 end if for i = 1 to b // In parallel on the b clients do
Draw Xk(1+,i1)  Uniform (N,n) and Xk(2+,i1)  2. Set hi,k+1 = 0d (for QLSD ) or hi,k+1 = Ui(k) (for QLSD++). Compute Hi(k, Xk(1+,i1)) = (N/n) jXk(1+,i1) [Ui,j (k) - Ui,j (k)] + hi,k+1.
Compute gi,k+1 = C Hi(k, Xk(1+,i1)) - k(i), Xk(2+,i1) .

Send gi,k+1 to the central server. Set k(i+) 1 = k(i) + gi,k+1 (for QLSD ) or k(i+) 1 = k(i) (for QLSD++). end for

// On the central server

Compute gk+1 = k +

b i=1

gi,k+1

.

Set k+1 = k + 

b i=1

gi,k+1.

Draw Zk+1  N(0d, Id). Compute and send k+1 =

k

-

gk+1

+

 2Zk+1

to

the

b

clients.

end for

Output: samples {k}Kk=0.

Non-asymptotic convergence analysis for Algorithm 1. We consider the following assumptions on the family {Hi : Rd × X1  Rd}i[b].

H3. There exists a probability measure 1 on a measurable space (X1, X1) and a family of measurable functions {Hi : Rd × X1  Rd}i[b] such that the following conditions hold.

(i) For any   Rd,

b i=1

X1 Hi(, x(1))1(dx(1)) = U ().

(ii) There exists M > 0, such that for any i  [b], 1, 2  Rd,

Hi(1, x(1)) - Hi(2, x(1)) 2 1(dx(1))  M 1 - 2, Ui (1) - Ui (2) .
X1

(iii) There exists  , B  R+ such that for any   Rd, we have

Hi( , x(1)) 2 1(dx(1))  B /b ,
X1

b

2

Hi( , x(1,i)) 1b(dx(1,1:b))  2 .
Xb1 i=1

We can notice that H3-(ii) implies that Ui is M-Lipschitz continuous since by the CauchySchwarz inequality, for any i  [b] and any 1, 2  Rd, Ui (1) - Ui (2) 2 

M 1 - 2, Ui (1) - Ui (2) . Conversely, in the finite-sum setting, H3-(ii) is satisfied by

QLSD# with M = N M¯ if for any i  [b] and j  [N ], Ui,j is convex and Ui,j is M¯-Lipschitz continuous, for M¯  0 by [36, Theorem 2.1.5]. In addition, it is worth mentioning that the first inequality

in H3-(iii) is also required for our derivation in the deterministic case where Hi = Ui due to the

compression operator. In this particular case, B stands for an upper-bound on

b i=1

Ui( ) 2

and corresponds to some discrepancy between local posterior density functions meaning that

6

Table 1: Order of the asymptotic biases {B¯, B ,¯, B,¯}, associated to the three proposed MCMC algorithms, in squared 2-Wasserstein distance for two types of asymptotic. In the finite-sum setting and asymptotic scenario N  , we set  = /N for some   (0, N ¯] and assume that lim infN m/N > 0 and lim supN A/N > 0 for A  {L, M,  }.

Algorithm, asymp. bias

Dependencies of the asymp. bias when ¯  0

d {Hi}i[b]

QLSD, B¯

d

2

QLSD#, B¯

d

N2

QLSD , B ,¯

d

N

QLSD++, B,¯ d

N

B



B



b i=1

Ui( ) 2



-



-



Dependencies of the asymp. bias as N  
O(N ) O(N ) d O (1) d O (1)

Ui = U for i  [b]. This phenomenon, referred to as data heterogeneity in risk-based literature [25, 31], is ubiquitous in the FL context [30]. Under the above assumptions and by denoting Q the Markov kernel associated to Algorithm 1, the following convergence result holds.

Theorem 1. Assume H1, H2 and H3. Then, there exists ¯ such that for ¯ < ¯, there exist A¯, B¯ > 0 (explicitly given in Appendix S1.3) satisfying for any probability measure µ  P2 Rd , any step size   (0, ¯] and k  N,

W22 µQk ,   (1 - m/2)k · W22 (µ, ) + B¯ + 2A¯(1 - m/2)k-1k ·

 -  2µ(d) .

Rd

Proof. The proof is postponed to Appendix S1.3.

Similarly to ULA [13, 17] and SGLD [14, 18], the upper bound given in Theorem 1 involves a contracting term depending on the initialisation and a bias term B¯ which does not vanish as k   because of the use of a fixed step-size . In the asymptotic scenario, i.e. ¯  0, Table 1 gives the dependencies of B¯ for QLSD and its particular instance QLSD#, with respect to key quantities associated to the setting we consider. Similarly to SGLD, we can notice that the use of stochastic gradients brings a bias term of the order 2 O(). On the other hand, compared to SGLD, the use of the compression operator leads to an additional bias of the order (mB + LMd) O() which notably
involves B corresponding to the impact of local posterior discrepancy on convergence.

Non-asymptotic convergence results for Algorithm 2. We assume here that the potential functions

{Ui}i[b] admit, for any i  [b], the finite-sum decomposition Ui =

N j=1

Ui,j ,

where

N



N,

and

consider the following set of assumptions.

H4. For any i  [b], j  [N ], Ui,j is continuously differentiable and the following holds.

(i) There exists M > 0 such that, for any 1, 2  Rd, Ui(2) - Ui(1) 2  M 2 - 1, Ui(2) - Ui(1) . (ii) There exists M¯  0 such that, for any 1, 2  Rd, Ui,j(2) - Ui,j(1) 2  M¯ Ui,j (2) - Ui,j (1), 2 - 1 .

As mentioned previously, H4 is satisfied if for any i  [b] and j  [N ], Ui,j is convex and Ui,j is M¯Lipschitz continuous. Under these additional conditions, the following non-asymptotic convergence
results hold for the two variance-reduced MCMC algorithms detailed in Algorithm 2. Denote by Q , the Markov kernel associated with QLSD with a step-size   (0, ¯].

Theorem 2. Assume H1, H2 and H4. Then, there exists ¯ , such that for ¯ < ¯ ,, there exist A ,¯, B ,¯ > 0 (explicitly given in Appendix S2.1) satisfying for any probability measure
µ  P2 Rd , any step size   (0, ¯] and k  N,

W22 µQk , ,  (1 - m/2)k · W22 (µ, ) + 2A ,¯(1 - m/2)k-1k ·

 -  2µ(d)

Rd

+ B ,¯ .

Proof. The proof is postponed to Appendix S2.1.

7

Compared to QLSD and QLSD , QLSD++ only defines an inhomogeneous Markov chain, see Appendix S3.3 for more details. For a step-size   (0, ¯] and an iteration k  N, we denote by µQ(k,) the distribution of k defined by QLSD++ starting from 0 with distribution µ.
Theorem 3. Assume H1, H2 and H4, and let l  N and   (0, 1/( + 1)]. Then, there exists ¯, such that for ¯ < ¯,, there exist A,¯, B,¯, C,¯ > 0 (explicitly given in Appendix S3.3 and independent of ) satisfying for any probability measure µ  P2 Rd , any step size   (0, ¯] and k  N,

W22(µQ(+k,) , )  (1 - m/2)k · W22 (µ, ) + 2A,¯(1 - m/2) k/l ·

 -  2µ(d)

Rd

b
+ B,¯ + C,¯[(1 - )k  (1 - m/2) k/l ]

Ui(

) - 0(i)

2
.

i=1

Proof. The proof is postponed to Appendix S3.3.
Table 1 provides the dependencies of the asymptotic bias terms B ,¯, B,¯ as ¯  0 with respect to key quantities associated to the problem we consider. For comparison, we do the same regarding the specific instance of Algorithm 1, QLSD#. Remarkably, thanks to biased local stochastic gradients for QLSD and the memory mechanism for QLSD++, we can notice that their associated asymptotic biases do not depend on local posterior discrepancy in contrast to QLSD#. This is in line with non-asymptotic convergence results in risk-based FL which also show that the impact of data heterogeneity can be alleviated using such a memory mechanism [40]. The impact of stochastic gradients is discussed in further details in the next paragraph.
Consistency analysis in the big data regime. It has been shown in [9] that ULA and SGLD define homogeneous Markov chains, each of which admits a unique stationary distribution. However, while the invariant distribution of ULA becomes closer to  as N increases, on the opposite, the invariant measure of SGLD never comes close to  and is in fact very similar to the invariant measure of SGD. In addition, the non-compressed counterpart of QLSD has been shown not to suffer from this issue and has been theoretically proven to be a a viable alternative to ULA in the big data setting.
Since QLSD is a generalisation of SGLD, the conclusions of [9] apply. On the other hand, we show that the variance-reduced alternatives to QLSD that we have introduced provide more accurate estimates for  as N increases. Based on their explicit expressions (see the Appendices), we analyse in Table 1 the asymptotic as N   of the bias terms {B¯, B ,¯, B,¯} where, similarly to [9], we set  = /N with   (0, N ¯] and assume that lim infN m/N > 0 and lim supN A/N > 0 for A  {L, M,  }. Detailed calculations are postponed to Appendix S4.

4 Numerical experiments

This section illustrates our methodology on three numerical experiments involving both synthetic
and real FL benchmarks. For all experiments, we consider the finite-sum setting and use the stochastic quantisation operator C (s) for s  1 defined in(4) to perform the compression step. In this case, recall that H2-(ii) is verified with  = min(d/s2, d/s).

Toy Gaussian example. This first experiment aims at illustrating the general behavior of Algo-

rithm 1 and Algorithm 2 with respect to the discretisation time-step  and the compression param-

eter . To this purpose, we set b = 20 and d = 50 and consider a Gaussian posterior distribution

with density defined in (1) where, for any i  [b] and   Rd, Ui() =

N j=1

 - yi,j

2/2,

{yi,j}i[b],j[N] being a set of synthetic independent but not identically distributed observations

across clients and N = 200. Note that in this specific case,  has a closed form given by

=

b i=1

N j=1

yi,j

/(bN

).

For all the algorithms, we set the step-size to 

= 4.9 × 10-4

and choose a minibatch size n = N/10 . First, we compare QLSD# and QLSD using s  {24, 216}

referred to as 4-bits and 16-bits instances of these MCMC algorithms, respectively. Then, we compare QLSD and its non-compressed counterpart referred to as LSD for s  {24, 28, 216}. Figure 1

shows the behavior of the mean squarred error (MSE) associated to the test function f :    ,

computed using 30 independent runs of each algorithm, with respect to both the number of com-

munication rounds and number of bits transmitted. We can notice that QLSD always outperforms

8

MSE for test function f :   

MSE for test function f :   

MSE for test function f :   

10-2

QLSD 4 bits

QLSD 16 bits

10-3

10-3

QLSD# 4 bits

QLSD# 16 bits
10-4 10-4

10-5

10-5

10-2
QLSD 4 bits

QLSD 8 bits

QLSD 16 bits

10-3

LSD

10-4

10-5

QLSD 4 bits QLSD 8 bits QLSD 16 bits LSD QLSD# 4 bits QLSD# 16 bits

0

10000

20000

30000

40000

50000

0

Nb. of communication rounds

100

10-3

10-1

10-2

slope = 1.09 ± 0.07

10-5

10-3

10-1

101

Compression parameter 

 10-4
10-5

Error in variance estimation

0.00021 0.00020 0.00019 0.00018 0.00017 0.00016 0.00015 0.00014 0.00013 0.00012 0

10000

20000

30000

40000

Nb. of communication rounds

50000

QLSD++ 1 bit with memory QLSD++ 1 bit

2N0b00.0of co4m000m0 unic6a00t0io0n ro8u0n00d0s 100000

Error in variance estimation

0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6
Nb. of communicated bits during upload ×109

0.00021 0.00020 0.00019 0.00018 0.00017 0.00016 0.00015 0.00014 0.00013 0.00012 0

QLSD++ 2 bits with memory QLSD++ 2 bits 2N0b00.0of co4m000m0 unic6a00t0io0n ro8u0n00d0s 100000

MSE for test function f :   

Figure 1: Toy Gaussian example (top row and very right figure on bottom row) and Bayesian logistic regression on synthetic data (the two figures on the bottom row starting from the right).

QLSD#and that decreasing the value of  does not significantly reduce the bias associated to QLSD . This illustrates the impact of the variance of the stochastic gradients and supports our theoretical analysis summarised in Table 1. On the other hand, QLSD with s = 216 achieves a similar MSE as LSD while requiring roughly 2.5 times less number of bits. As shown on the very right figure (bot-
tom row), this saving in number of transmitted bits can be further improved by decreasing the value of . This numerical finding illustrates our theory which in particular shows that the asymptotic bias associated to QLSD is of the order  O(), see Table 1.

Bayesian logistic regression. Since  is not

easily available, we implement Algorithm 2 using QLSD++. For all experiments, we adopt

Table 2: Logistic regression on real data.

a zero-mean Gaussian prior with covariance matrix 2 · 10-2Id. Synthetic data. For the QLSD++ 99% HPD error Rel. efficiency

first sub-experiment, we consider the SYN-

THETIC(, ) dataset [33] with  =  = 1, 4 bits

6.1e-3

7.6

d = 2 and b = 50. To this end, we run this al- 8 bits

4.3e-3

6.7

gorithm with and without memory terms using 16 bits

6.9e-4

3.1

l = 100,  = 1/( + 1),  = 10-5 and for

huge compression parameters, namely s  {21, 22}. In order to have access to some ground truth,

we also implement the Metropolis-adjusted Langevin algorithm (MALA) [46]. Figure 1 shows the Eu-

clidean norm of the error between the true variance under  estimated with MALA and the empirical variance computed using samples generated by QLSD++. As expected, we can notice that the memory mechanism reduces the impact of the compression on the asymptotic bias of QLSD++ when  is

large. Real data. For this second sub-experiment, we use the FEMNIST dataset [10]. We launch QLSD++ for s  {24, 28, 216} and using the same hyperparameter values as before. Under the

Bayesian paradigm, we are interested in performing uncertainty quantification by estimating highest
posterior density (HPD) regions. For any   (0, 1), we define C = {  Rd; - log (|D)  }
where   R is chosen such that C (|D)d = 1 - . We compute the relative HPD error based on the scalar summary , i.e. | - LSD|/LSD where LSD has been estimated using the non-compressed counterpart of QLSD++, referred to as LSD++. Table 2 gives this relative HPD error for  = 0.01 and provides the relative efficiency of QLSD++ corresponding to the savings in terms

of transmitted bits per iteration. One can notice that the proposed approach provides similar results

as its non-compressed counterpart while being 3 to 7 times more efficient. Figure 2 completes this

empirical study by showing that the predictive distributions of the most probable label given by QLSD++ and LSD++ almost coincide for eight test examples.

Bayesian neural networks. In our third experiment, we consider a classification problem on the
MNIST dataset [12] involving 10 classes and bN = 60, 000 observations {xi, yi}i[bN], and such that for any i  [bN ], k  [10], P(yi = k | , xi) = k where k is the k-th element of 1(W2 · 2(W1xi + a1) + a2) where 1(·) is the softmax function, 2(·) the sigmoid func-

9

Test accuracy on D(pr)ed Density Density

1.00

QLSD# 2 bits

QLSD# 1 bits

0.98

LSD#

2.0

1.5 0.96

1.0 0.94

0.5 0.92

0.0

0.2 Credib0.i4lity thre0s.6hold 0.8

1.0

0.00.0

QLSD# 1 bits QLSLDS#D# 2 bits

0.P2redictiv0e.4entropy0.o6n MNIST0.8

1.0

0.16

QLSD# 2 bits

0.14

QLSLDS#D# 1 bits

0.12

0.10

0.08

0.06

0.04

0.02

0.00 0 Pre2dictive4entrop6y on F8ashion1-0MNIST12

Figure 2: Bayesian logistic regression and neural networks on real data.

tion, xi are covariates and a1, a2, W1, W2 are matrices of size 128 × 1, 10 × 10, 128 × 784 and 10 × 128, respectively. We adopt a zero-mean Gaussian prior for  = (a1, a2, W1, W2) with covariance matrix 5 · 10-2Id, set b = 100,  = 10-4 and n = N/7.5 , and launch QLSD# for s  {21, 22}. For comparison, we also run LSD# using the same hyperparameters. Denote by Dxtest the set of covariates x belonging to Dtest. We conduct two sub-experiments for which we use the posterior mean estimate of  to predict the label ypred(x) associated to each example x  Dxtest. For our first sub-experiment, we consider, for any   [0, 1], the set D(pre)d = {x  Dxtest : Rd p(ypred(x)|x, )( | D)d   } of classified data with credibility greater than  . Figure 2 shows the evolution of the test accuracy on D(pre)d with respect to the credibility threshold  that is given by Card({x  D(pre)d : ytrue(x) = ypred(x)})/Card(D(pre)d). In our second sub-experiment, we study the behavior of our proposed algorithms in the out-of-distribution framework. To this purpose, in Figure 2 we display a kernel density estimate of the conditional predicitve density associated with {log(  p(ypred(x) | x, )( | D)d) : x  Dxtest} for Dtest  {MNIST, FASHION-MNIST}. For both sub-experiments, we can notice that QLSD# provides similar results as its non-compressed counterpart while requiring far less transmitted bits.
5 Conclusion
In this paper, a general methodology based on stochastic Langevin dynamics has been introduced for Bayesian FL. In particular, we addressed the communication bottleneck by assuming that each client sent compressed versions of their local stochastic gradient oracles to the central server. In addition, it has been established that the proposed methodology inherits favorable convergence properties which have been supported by numerical illustrations. One limitation of this work is that the proposed methodology does not target the initial posterior distribution because of the use of a fixed discretisation time-step. As such, this work paves the path for more advanced Bayesian FL approaches, e.g. based on Metropolis-Hastings schemes to remove asymptotic bias or accounting for partial device participation.
Acknowledgments
The authors acknowledge support of the Lagrange Mathematics and Computing Research Center.
References
[1] White House Report. Consumer data privacy in a networked world: A framework for protecting privacy and promoting innovation in the global digital economy. Journal of Privacy and Confidentiality, 2013.
[2] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensorflow: A system for largescale machine learning. In Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation, OSDI'16, page 265­283, USA, 2016. USENIX Association.
10

[3] Sungjin Ahn, Babak Shahbaba, and Max Welling. Distributed Stochastic Gradient MCMC. In Eric P. Xing and Tony Jebara, editors, Proceedings of the 31st International Conference on Machine Learning, volume 32 of Proceedings of Machine Learning Research, pages 1044­ 1052, 2014.
[4] Alham Fikri Aji and Kenneth Heafield. Sparse Communication for Distributed Gradient Descent. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 440­445, September 2017.
[5] Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.
[6] Christophe Andrieu, Nando de Freitas, Arnaud Doucet, and Michael I. Jordan. An introduction to MCMC for machine learning. Machine Learning, 50(1­2):5­43, 2003. doi: 10.1023/A: 1020281327116.
[7] Jack Baker, Paul Fearnhead, Emily B. Fox, and Christopher Nemeth. Control variates for stochastic gradient MCMC. Statistics and Computing, 29(3):599­615, 2019. doi: 10.1007/ s11222-018-9826-2.
[8] Dominique Bakry, Ivan Gentil, and Michel Ledoux. Analysis and Geometry of Markov Diffusion operators. Grundlehren der mathematischen Wissenschaften, Vol. 348. Springer, 2014.
[9] Nicolas Brosse, Alain Durmus, and Eric Moulines. The promises and pitfalls of stochastic gradient langevin dynamics. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. CesaBianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/ 2018/file/335cd1b90bfa4ee70b39d08a4ae0cf2d-Paper.pdf.
[10] Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Konecny, H. Brendan McMahan, Virginia Smith, and Ameet Talwalkar. LEAF: A Benchmark for Federated Settings. arXiv preprint arXiv:1812.01097, 2018.
[11] Arkabandhu Chowdhury and Christopher Jermaine. Parallel and Distributed MCMC via Shepherding Distributions. In Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics, volume 84, pages 1819­1827, 2018.
[12] Gregory Cohen, Saeed Afshar, Jonathan Tapson, and André van Schaik. Emnist: Extending mnist to handwritten letters. In 2017 International Joint Conference on Neural Networks (IJCNN), pages 2921­2926, 2017. doi: 10.1109/IJCNN.2017.7966217.
[13] Arnak S. Dalalyan. Theoretical guarantees for approximate sampling from smooth and logconcave densities. Journal of the Royal Statistical Society, Series B, 79(3):651­676, 2017. doi: 10.1111/rssb.12183.
[14] Arnak S. Dalalyan and Avetik Karagulyan. User-friendly guarantees for the langevin monte carlo with inaccurate gradient. Stochastic Processes and Their Applications, 129(12):5278­ 5311, 2019. doi: 10.1016/j.spa.2019.02.016.
[15] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc Aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, Quoc Le, and Andrew Ng. Large Scale Distributed Deep Networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 25. Curran Associates, Inc., 2012.
[16] Aymeric Dieuleveut, Alain Durmus, and Francis Bach. Bridging the gap between constant step size stochastic gradient descent and Markov chains. Annals of Statistics, 48(3):1348­1382, 06 2020. doi: 10.1214/19-AOS1850.
[17] Alain Durmus and Eric Moulines. High-dimensional Bayesian inference via the unadjusted Langevin algorithm. Bernoulli, 25(4A):2854­2882, 2019. doi: 10.3150/18-BEJ1073.
11

[18] Alain Durmus, Szymon Majewski, and Blazej Miasojedow. Analysis of Langevin Monte Carlo via convex optimization. Journal of Machine Learning Research, 20(73):1­46, 2019. Available at http://www.jmlr.org/papers/volume20/18-173/18-173.pdf.
[19] Khaoula El Mekkaoui, Diego Mesquita, Paul Blomstedt, and Samuel Kaski. Distributed stochastic gradient MCMC for federated learning. arXiv preprint arXiv:2004.11231, 2020.
[20] Gianni Franchi, Andrei Bursuc, Emanuel Aldea, Severine Dubuisson, and Isabelle Bloch. Encoding the latent posterior of Bayesian Neural Networks for uncertainty quantification. arXiv preprint arXiv:2012.02818, 2020.
[21] Daniel Garcia-Gonzalez, Daniel Rivero, Enrique Fernandez-Blanco, and Miguel R. Luaces. A Public Domain Dataset for Real-Life Human Activity Recognition Using Smartphone Sensors. Sensors, 20(8), 2020.
[22] Antonious M Girgis, Deepesh Data, Suhas Diggavi, Peter Kairouz, and Ananda Theertha Suresh. Shuffled model of federated learning: Privacy, communication and accuracy tradeoffs. arXiv preprint arXiv:2008.07180, 2020.
[23] Ulf Grenander and Michael I. Miller. Representations of knowledge in complex systems. Journal of the Royal Statistical Society. Series B (Methodological), 56(4):549­603, 1994. ISSN 00359246.
[24] Farzin Haddadpour, Mohammad Mahdi Kamani, Aryan Mokhtari, and Mehrdad Mahdavi. Federated Learning with Compression: Unified Analysis and Sharp Guarantees. arXiv preprint arXiv:2007.01154, 2020.
[25] Samuel Horváth, Dmitry Kovalev, Konstantin Mishchenko, Sebastian Stich, and Peter Richtárik. Stochastic Distributed Learning with Gradient Quantization and Variance Reduction . arXiv preprint arXiv:1904.05115, 2019.
[26] Li Huang, Yifeng Yin, Zeng Fu, Shifa Zhang, Hao Deng, and Dianbo Liu. LoAdaBoost: Loss-based AdaBoost federated machine learning with reduced computational complexity on IID and non-IID intensive care data. PLOS ONE, 15(4):1­16, 04 2020. doi: 10.1371/ journal.pone.0230706.
[27] David J. Hunter. Uncertainty in the Era of Precision Medicine. New England Journal of Medicine, 375(8):711­713, 2016.
[28] Rie Johnson and Tong Zhang. Accelerating Stochastic Gradient Descent Using Predictive Variance Reduction. In Neural Information Processing Systems, page 315­323, 2013.
[29] Michael I. Jordan, Jason D. Lee, and Yun Yang. Communication-Efficient Distributed Statistical Inference. Journal of the American Statistical Association, 114(526):668­681, 2019. doi: 10.1080/01621459.2018.1429274.
[30] Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, K. A. Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael G.L. D'Oliveira, Salim El Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adrià Gascón, Badih Ghazi, Phillip B. Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail Khodak, Jakub Konevcný, Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo, Tancrède Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard Nock, Ayfer Özgür, Rasmus Pagh, Mariana Raykova, Hang Qi, Daniel Ramage, Ramesh Raskar, Dawn Song, Weikang Song, Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tramèr, Praneeth Vepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao. Advances and Open Problems in Federated Learning. arXiv preprint arXiv:1912.04977, 2019.
[31] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. SCAFFOLD: Stochastic controlled averaging for federated learning. In Hal Daumé III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 5132­ 5143. PMLR, 13­18 Jul 2020.
12

[32] Rahif Kassab and Osvaldo Simeone. Federated Generalized Bayesian Learning via Distributed Stein Variational Gradient Descent. arXiv preprint arXiv:2009.06419, 2020.
[33] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated Optimization in Heterogeneous Networks. In I. Dhillon, D. Papailiopoulos, and V. Sze, editors, Proceedings of Machine Learning and Systems, volume 2, pages 429­450, 2020.
[34] Yujun Lin, Song Han, Huizi Mao, Yu Wang, and Bill Dally. Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training. In International Conference on Learning Representations, 2018.
[35] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-Efficient Learning of Deep Networks from Decentralized Data. In Aarti Singh and Jerry Zhu, editors, Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, volume 54 of Proceedings of Machine Learning Research, pages 1273­1282, Fort Lauderdale, FL, USA, 20­22 Apr 2017. PMLR.
[36] Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer Science & Business Media, 2003.
[37] Alexandros Pantelopoulos and Nikolaos G. Bourbakis. A Survey on Wearable Sensor-Based Systems for Health Monitoring and Prognosis. IEEE Transactions on Systems, Man, and Cybernetics, 40(1):1­12, 2010.
[38] G. Parisi. Correlation functions and computer simulations. Nuclear Physics B, 180(3):378­ 384, 1981. ISSN 0550-3213. doi: https://doi.org/10.1016/0550-3213(81)90056-0.
[39] Mariane Pelletier. On the almost sure asymptotic behaviour of stochastic algorithms. Stochastic Processes and their Applications, 78(2):217­244, 1998. ISSN 0304-4149. doi: https://doi.org/ 10.1016/S0304-4149(98)00029-5.
[40] Constantin Philippenko and Aymeric Dieuleveut. Bidirectional compression in heterogeneous settings for distributed or federated learning with partial participation: tight convergence guarantees . arXiv preprint arXiv:2006.14591, 2020.
[41] Parisa Rashidi and Diane J. Cook. Keeping the Resident in the Loop: Adapting the Smart Home to the User. IEEE Transactions on Systems, Man, and Cybernetics, 39(5):949­959, 2009.
[42] L. J. Rendell, A. M. Johansen, A. Lee, and N. Whiteley. Global consensus Monte Carlo. Journal of Computational and Graphical Statistics, 2020.
[43] Daniel Revuz and Marc Yor. Continuous martingales and Brownian motion, volume 293. Springer Science & Business Media, 2013.
[44] Herbert Robbins and Sutton Monro. A stochastic approximation method. Annals of Mathematical Statistics, 22(3):400­407, 09 1951. doi: 10.1214/aoms/1177729586.
[45] C. P. Robert. The Bayesian Choice: from decision-theoretic foundations to computational implementation. Springer, New York, 2 edition, 2001.
[46] C. P. Robert and G. Casella. Monte Carlo Statistical Methods. Springer, Berlin, 2 edition, 2004.
[47] Gareth O. Roberts and Richard L. Tweedie. Exponential convergence of Langevin distributions and their discrete approximations. Bernoulli, 2(4):341­363, 12 1996.
[48] Felix Sattler, Simon Wiedemann, Klaus-Robert Müller, and Wojciech Samek. Robust and Communication-Efficient Federated Learning From Non-i.i.d. Data. IEEE Transactions on Neural Networks and Learning Systems, 31(9):3400­3413, 2020.
13

[49] Steven L. Scott, Alexander W. Blocker, Fernando V. Bonassi, Hugh A. Chipman, Edward I. George, and Robert E. McCulloch. Bayes and Big Data: The Consensus Monte Carlo Algorithm. International Journal of Management Science and Engineering Management, 11: 78­88, 2016.
[50] Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 1-Bit Stochastic Gradient Descent and Application to Data-Parallel Distributed Training of Speech DNNs. In Interspeech 2014, September 2014.
[51] Sebastian U Stich, Jean-Baptiste Cordonnier, and Martin Jaggi. Sparsified SGD with Memory. In Advances in Neural Information Processing Systems, volume 31, 2018.
[52] Cédric Villani. Optimal Transport: Old and New. Springer Berlin Heidelberg, 2008. [53] Maxime Vono, Daniel Paulin, and Arnaud Doucet. Efficient MCMC sampling with dimension-
free convergence rate using ADMM-type splitting. arXiv preprint arXiv:1905.11937, 2019. [54] Xiangyu Wang and David B. Dunson. Parallelizing MCMC via Weierstrass sampler. arXiv
preprint arXiv:1312.4605, 2013. [55] Xiangyu Wang, Fangjian Guo, Katherine A. Heller, and David B. Dunson. Parallelizing
MCMC with random partition trees. In Advances in Neural Information Processing Systems, 2015. [56] Max Welling and Yee Whye Teh. Bayesian learning via stochastic gradient langevin dynamics. In International Conference on International Conference on Machine Learning, page 681­688, 2011.
14

S1 Proof of Theorem 1

This section aims at proving Theorem 1.

S1.1 Generalised quantised Langevin stochastic dynamics

We show that QLSD defined in Algorithm 1 can be cast into a more general framework that we refer
to as generalised quantised Langevin stochastic dynamics. Then, the guarantees for QLSD will be
a simple consequence of the ones that we will establish for generalised QLSD. For ease of reading,
we recall first the setting and the assumptions that we consider all along the paper. Recall that the dataset D is assumed to be partitioned into b shards {Di}bi=1 such that bi=1Di = D and the posterior distribution of interest is assumed to admit a density with respect to the d-dimensional Lebesgue measure which factorises across clients, i.e. for any   Rd,

() = exp {-U ()} / e-U() d ,
Rd

b
U () = Ui() .
i=1

For k  1, consider (Xk(1,1), . . . , Xk(1,b))kN and (Xk(2,1), . . . , Xk(2,b))kN two independent sequences of i.i.d. random variables distributed according to 1b and 2b, respectively. Given a step-size   (0, ¯] for some ¯ > 0 and starting from 0  Rd, QLSD recursively defines (k)kN, for any k  N, as

k+1 = k - 

b i=1

C

(Hi(k

,

Xk(1+,i1)

),

Xk(2+,i1))

+

 2

Zk+1

,

(S1)

where (Zk+1)kN is a sequence of standard Gaussian random variables. Note that (S1) can be written of the form

b

k+1 = k -  H~i(k, Xk(i+)1) + 2Zk+1 ,

k  N,

(S2)

i=1

where for any i  [b], we denote Xk(i+)1 = (Xk(1+,i1), Xk(2+,i1)) and for any   Rd, x(1)  X1, x(2)  X2,

H~i , (x(1), x(2)) = C Hi(, x(1)), x(2) .

(S3)

With this notation and setting X~ = X1 × X2, X~ = X1  X2 and ~ = 1  2, the Markov kernel associated with (S1) is given for any (, A)  Rd × B(Rd) by

Q(, A) =

exp - ~ -  + 

A×X~ b

b i=1

H~ i (,

x(i)

)

2/(4)

d~ ~b (dx(1:b) ) (4)d/2

.

(S4)

The following result establishes an essential property of {H~i}i[b] under H2 and H3. Lemma S4. Assume H2 and H3. Then, for any   Rd, we have

b i=1

X~ H~i(, x) d~b(x) = U () ,

(S5)

X~ b

b i=1

H~ i

(,

x(i)

)

-

U

()

2 ~b(dx(1:b))  2( + 1)M

-

, U ()

+ 2(B

+ 2) ,

(S6)

where for any i  [b], H~i is defined in (S3).

Proof. The first identity (S5) is straightforward using H3-(i) and H2-(i). We now show the inequality (S6). Let   Rd. Using H2-(i) or H3-(i), we get

b

2

H~i(, x(i)) - U () ~b(dx(1:b))
X~b i=1

15

=
X~ b

b

2

H~i(, x(i)) - Hi(, x(1,i)) ~b(dx(1:b))

i=1

b

2

+

Hi(, x(1,i)) - U () 1b(dx(1,1:b)) .

Xb1 i=1

(S7)

In addition, by H2-(i) and H2-(ii), we obtain

b

2

H~i(, x(i)) - Hi(, x(1,i))
X~b i=1

~b(dx(1:b))

b
=

H~i(, x(i)) - Hi(, x(1,i)) 2 1(dx(1,i))2(dx(2))

i=1 X~

b


Hi(, x(1)) 2 1(dx(1)) .

i=1 X1

(S8)

Using a 2  2 a - b 2 + 2 b 2 and H3-(ii)-(iii), for any i  [b], we obtain

Hi(, x(1)) 2 1(dx(1))  2M  -  , Ui() - Ui( )

(S9)

X1

+2

Hi( , x(1)) 2 1(dx(1))

X1

 2M  -  , Ui() - Ui( ) + 2B /b .

Therefore, combining this result and (S8) gives

b

2

H~i(, x(i)) - Hi(, x(1,i))
X~b i=1

~b(dx(1:b))  2M  -  , U () - U ( ) +2B .

(S10)

Similarly, since for any a, b  Rd, a + b 2  2 a 2 + 2 b 2, we have by H3-(i)

b

2

Hi(, x(1,i)) - U () 1b(dx(1,1) · · · dx(1,b))
Xb1 i=1

b

2

=

Hi(, x(1,i)) - Hi(, x(1))1(dx(1))

Xb1 i=1

X1

1b(dx(1,1) · · · dx(1,b))

b

2

=

Hi(, x(1)) - Hi(, x(1))1(dx(1)) 1(dx(1))

i=1 X1

X1

b

2

Hi(, x(1)) - Hi( , x(1)) -

i=1 X1

Hi(, x(1)) - Hi( , x(1))1(dx(1))
X1

b

2

+2

Hi( , x(1)) - Hi( , x(1))1(dx(1)) 1(dx(1))

i=1 X1

X1

 2M U (),  -  + 22 .

2
1(dx(1))

Finally, the last inequality combined with (S7) and (S10) completes the proof.

In view of Lemma S4, it suffices to study the recursion specified in (S2) under the following assumption on (H~i)i[b] gathered in H5. Indeed, Lemma S4 shows that Condition H5 below holds with X = X~ = X1 × X2, X = X~ = X1  X2, ~ = 1  2, {H~i}bi=1 = {Fi}bi=1, M~ = 2( + 1)M and B~ = 2(B + 2)

16

H5. There exists a probability measure  on a measurable space (X~, X~) and a family of measurable functions {Fi : Rd × X  Rd}i[b] such that the following conditions hold.

(i) For any   Rd, we have

b
Fi(, x(i))(dx(i)) = U () .
i=1 X~

(ii) There exists (M~, B~ )  R2+ such that for any   Rd, we have

b

2

Fi(, x(i)) - U () b(dx(1:b))  M~  -  , U () - U ( ) + B~ .
X~b i=1

Then under H5, consider (Xk(1), . . . , Xk(b))kN an i.i.d. sequence distributed according to b. Define the general recursion

b
~k+1 = ~k -  Fi(~k, Xk(i+)1) +
i=1

2Zk+1 ,

k  N.

and the corresponding the Markov kernel given for any   R+,   Rd, A  B(Rd) by

(S11)

Q~ (, A) = (4)-d/2

exp(-(4)-1 ¯ -  + 

A×X~ b

b i=1

Fi(,

x(i))

2) d¯db(x(1:b)) .

(S12)

We refer to this Markov kernel as the generalised QLSD kernel. In our next section, we establish

quantitative bounds between the iterates of this kernel and  in W2. We then apply this result to

QLSD and QLSD as particular cases.

S1.2 Quantitative bounds for the generalised QLSD kernel

Define ¯ = ¯1  ¯2  ¯3 , ¯1 = 2/[5(m + L)] , ¯2 = (m + L + M~)-1 , ¯3 = (10m)-1 . (S13)
Theorem S5. Assume H1 and H5. Then, for any probability measure µ  P2(Rd), any step size   (0, ¯], any k  N, we have

W22(µQ~k , )  (1 - m/2)kW22(µ, ) + B~¯ + 2A~¯(1 - m/2)k-1k

 -  2µ(d) ,

Rd

where Q~ is defined in (S12) and

B~¯ = (2d/m)(m-1 + 5¯) 1 + ¯L2/(2m) + ¯2L2/12 + 2B~ /m + LM~(2d + ¯B~ )/m2

A~¯ = LM~ .

Let   P2(R2d) be a probability measure on (R2d, B(R2d)) with marginals 1 and 2, i.e. (A ×

Rd) = 1(A) and (A × Rd) = 2(A) for any A  B(Rd). Note that under H1, the Langevin

diffusion defines a Markov semigroup (Pt)t0 satisfying Pt =  for any t  0, see e.g. [47,

Theorem 2.1]. We introduce a synchronous coupling (k, k) between 1Pk and 2Q~k for any k 

N based on a d-dimensional standard Brownian motion (Bt)t0 and a couple of random variables

(0, 0) with distribution  independent of (Bt)t0. Consider (t)t0 the strong solution of the

Langevin stochastic differential equation (SDE)



dt = -U (t)dt + 2 dBt ,

(S14)

starting from 0. Note that under H1-(i), this SDE admits a unique strong solution [43, Theorem

(2.1) in Chapter IX]. In addition, define (k)kN starting from 0 and satisfying the recursion: for k  0,

k+1 = k - 

b

Fi (k ,

xk(i+) 1)

+

 2(B(k+1)

-

B k )

,

(S15)

i=1

17

where (x(j1), . . . , x(jb))jN is an i.i.d. sequence of random variable with distribution b. Then, by definition, (k, k) is a coupling between 1Pk and 2Q~k for any k  N and therefore

W2(1Pk , 2Q~k )  E k - k 2 1/2 .

(S16)

We can now give the proof of Theorem S5.

Proof. By [52, Theorem 4.1], for any couple of probability measures on Rd, there exists an optimal transference plan  between  and  since   P2(Rd) by the strong convexity assumption H1-(i). Let (0, 0) be a corresponding coupling which therefore satisfies W2(µ, ) = E1/2[ 0 - 0 2]. Consider then (k)kN, (k)kN defined in (S14)-(S15) starting from (0, 0). Note that since Pt =  by [47, Theorem 2.1] for any t  0 and 0 has distribution , we get by [17, Proposition 1] that for any k  N, E[ k -  2]  d/m and then Lemma S7 below shows that for any
k  N,

E[ (k+1) - k+1 2]   E[ k - k 2] + 2LM~ E 0 -  2 ~k + 2D ,

where we have set

 = 1 - m(1 - 5m) ,

~ = 1 - m 2 - (m + M~) .

A straightforward induction shows that

E[ k - k 2]  k W22(µ, ) + 2LM~ E

0 - 

k-1

2

l ~k-1-l + 2D /(1 -  ) .

l=0

Using   ~  1 - m/2 since   ¯, (S16) and Pt =  for any t  0 completes the proof.

S1.2.1 Supporting Lemmata

In this subsection, we derived two lemmas. Taking (k)kN defined by the recursion (S15), Lemma S6 aims to upper bound the squared deviation between k and the minimiser of U denoted  , for any k  N.
Lemma S6. Assume H1 and H5. Let   0, 2/(m + L + M~) . Then, for any k  N, 0  Rd, we have

-
Rd

2 Q~k(0, d)  (1 - m 2 - (m + M~) )k 0 - 

2+ m

2d + B~ 2 - (m + M~)

,

where Q~ is defined in (S12).

Proof. For any 0  Rd, by definition (S12) of Q~ and using H5-(i), we obtain

-
Rd

2 Q~(0, d) = 0 -  2 - 2 0 -  , U (0)

+ 2
X~ b

b i=1

Fi(0,

x(i))

2 b(dx(1:b)) + 2d .

(S17)

Moreover, using H1, H5 and (6), it follows that

X~ b

b i=1

Fi(0,

x(i))

2 b(dx(1:b)) =

X~ b

b i=1

Fi(0,

x(i))

-

U

(0)

2 b(dx(1:b))

+ U (0) 2

 M~ 0 -  , U (0) + B~ + U (0) - U ( ) 2

 [m + L + M~] 0 -  , U (0) + B~ - Lm 0 -  2 . (S18)

Plugging (S18) in (S17) implies

18

 -  2 Q~(0, d)  (1 - 2mL) 0 -  2 - {2 - [m + L + M~]} 0 -  , U (0)
Rd
+ 2B~ + 2d .
Using H1-(i), we have 0 -  , U (0)  m 0 -  2 which, combined with the condition   1/(m + L + M~), gives
 -  2 Q~(0, d)  (1 - m[2 - (m + M~)]) 0 -  2 + (2d + B~ ) .
Rd
Using 0 <  < 2/(m + M~) and the Markov property combined with a straightforward induction completes the proof.

For any k  N, the following lemma gives an explicit upper bound on the expected squared norm between k+1 and k+1 in function of k, k. The purpose of this lemma is to derive a contraction property involving a contracting term and a bias term which is easy to control.
Lemma S7. Assume H1 and H5. Consider (t)t0 and (k)kN defined in (S14) and (S15), respectively, for some initial distribution   P2(R2d). For any k  N and   0, 2/[(5(m + L))  (m + M~ + L)] , we have

where

E (k+1) - k+1 2  {1 - m(1 - 5m)}E k - k 2 + 2D0,
+ 2LM~(1 - m 2 - (m + M~) )kE[ 0 -  2] + 3(m-1 + 5)L2E k -  2 /2 ,

D0, = d(m-1 + 5)

1 + 2L2/12

+ B~

+

LM~(2d + B~ ) m 2 - (m + M~)

.

Proof. Let k  N. By (S14) and (S15), we have

(k+1) - k+1 = k - k -  [U (k) - U (k)]



b

- [U (k+s) - U (k)] ds + 

Fi(k, Xk(i+)1) - Ui(k) .

0

i=1

Define the filtration (Fk~)k~N as F0 = (0, 0) and for k~  N,

Fk~ = (0, 0, (Xl(1), . . . , Xl(b))1lk~, (Bt)0tk~) .

Note that since (t)t0 is a strong solution of (S14), then is easy to see that (k~, k~)k~N is (Fk~)k~Nadapted. Taking the squared norm and the conditional expectation with respect to Fk, we obtain using H5-(i) that

EFk (k+1) - k+1 2 = k - k 2 - 2 k - k, U (k) - U (k)



+ 2

U (k) - U (k), EFk [U (k+s) - U (k)] ds

0



-2

k - k, EFk [U (k+s) - U (k)] ds

0

+ 2 U (k) - U (k) 2

+ EFk



2

[U (k+s) - U (k)] ds

0


b

2

+ 2EFk 

Fi(k, Xk(i+)1) - U (k)  .

i=1

(S19)

19

First, using Jensen inequality and the fact that for any a, b  Rd, | a, b |  2 a 2 + 2 b 2, we get


U (k) - U (k), EFk [U (k+s) - U (k)] ds
0 
 2 U (k) - U (k) 2 + 2 EFk U (k+s) - U (k) 2 ds ,
0

(S20)



2



EFk

[U (k+s) - U (k)] ds   EFk U (k+s) - U (k) 2 ds .

0

0

In addition, given that for any  > 0, a, b  Rd, | a, b |   a 2 + (4)-1 b 2, we get


k - k, EFk [U (k+s) - U (k)] ds   k - k 2
0 
+ (4)-1 EFk U (k+s) - U (k) 2 ds . (S21)
0
By H1, for k  N we get by (6)
U (k) - U (k) 2  (m + L) k - k, U (k) - U (k) - mL k - k 2 . (S22)

Lastly, H5-(ii) yields


b

2

EFk 

Fi(k, Xk(i+)1) - U (k)   M~ k -  , U (k) - U ( ) + B~ .

i=1

(S23)

Combining (S20), (S21), (S22) and (S23) into (S19), for k  N we get for any  > 0,

EFk (k+1) - k+1 2  (1 + 2 - 52mL) k - k 2
-  [2 - 5(m + L)] k - k, U (k) - U (k)

+ (5 + (2)-1) EFk U (k+s) - U (k) 2 ds
0
+ 2M~ k -  , U (k) - U ( ) + 2B~ .

Next, we use that under H1, k - k, U (k) - U (k)  m k - k 2 and | k -  , U (k)-U ( ) |  L k - 2, which implies taking  = m/2 and since 2-5(m+L)  0,

EFk (k+1) - k+1 2  (1 - m(1 - 5m)) k - k 2


+ (5 + m-1) EFk U (k+s) - U (k) 2
0
+ 2M~L k -  2 + 2B~ .

ds (S24)

Further, for any s  R+, using [17, Lemma 21] we have EFk U (k+s) - U (k) 2  ds 2 + s2L2/3 + 3s2L2/2 k -  2 .

Integrating the previous inequality on [0, ], for k  0 we obtain

EFk U (k+s) - U (k) 2 ds  d2 + d4L2/12 + 3L2/2 k -  2 .
0
Plugging this bound in (S24), taking expectation and using Lemma S7 conclude the proof.

20

S1.3 Proof of Theorem 1

Based on Theorem S5, the next corollary explicits an upper bound in Wasserstein distance between  and µQk, where we consider (k)kN defined in (S1) and starting from ~ following µ  P2(Rd).
Theorem S8. Assume H1, H2 and H3. Then, for any probability measure µ  P2(Rd), any step size   (0, ¯] where ¯ is defined in (S13), any k  N, we have

W22(µQk , )  (1 - m/2)kW22(µ, ) + B¯ + 2A¯(1 - m/2)k-1k

-

Rd

where Q is defined in (S4) and

B¯ = (2d/m) (1/m + 5¯) 1 + ¯L2/(2m) + ¯2L2/12 + 4(B + 2)/m

+ 4( + 1)LM d + ¯(B + 2) /m2

A¯ = 2( + 1)LM .

2µ(d) , (S25)

Proof. By Lemma S4, the assumption H5 is satified for a choice of M~ = 2( + 1)M and B~ = 2(B + 2). Therefore, applying Theorem S5 completes the proof.

S2 Proof of Theorem 2

We assume here that {Ui}i[b] are defined, for any i  [b] and   Rd, by

N
Ui() = Ui,j() ,
j=1

N  N .

In all this section, we assume that n  N, n  N is fixed. Recall that N denotes the power set of [N ] and
N,n = {x  N : card(x) = n} .
We set in this section 1 as the uniform distribution on N,n. We consider the family of measurable functions {Fi : Rd × Rd × N  Rd}i[b], defined for any i  [b],   Rd, x  N,n by

Fi

(, x)

=

N n

N

1x(j) [Ui,j() - Ui,j( )] .

j=1

(S26)

Let (Xk(1,1), . . . , Xk(1,b))kN and (Xk(2,1), . . . , Xk(2,b))kN be two independent i.i.d. sequences with distribution 1b and 2b. Let (Zk)kN be an i.i.d. sequence of d-dimensional standard Gaussian random variables independent of (Xk(1,1), . . . , Xk(1,b))kN and (Xk(2,1), . . . , Xk(2,b))kN . For ease of notation, denote for any k  N, Xk(1) = (Xk(1,1), . . . , Xk(1,b)), Xk(2) = (Xk(2,1), . . . , Xk(2,b)) and Xk = (Xk(1), Xk(2)).

Note that with this notation and under H2, QLSD can be cast into the framework of the generalised QLSD scheme defined in (S1) since the recursion associated to QLSD can be written as

b

~k+1 = ~k - 

C Fi (~k, Xk(1+,i1)), Xk(2+,i1)

2Zk+1 ,

k  N.

(S27)

i=1

Therefore, we only need to verify that H5 is satisfied with X = X~ = X1 × X2, X = X~ = X1  X2, ~ = 1  2 and {Fi}bi=1 = {Fi }bi=1. This is done in Appendix S2.2.

S2.1 Proof of Theorem 2

The Markov kernel associated with (S27) is given for any (, A)  Rd × B(Rd) by

Q , (, A) = (4)-d/2

exp - ~ -  + 

A×X~ b

b i=1

Fi

(,

x(i))

2/(4)

d~~b(dx(1:b)) .

(S28)

Then, the following non-asymptotic convergence result holds for QLSD .

21

Theorem S9. Assume H1, H2, H4. Then, for any probability measure µ  P2(Rd), any step size   (0, ¯] where ¯ is defined in (S13), any k  N, we have

W22(µQk , , )  (1 - m/2)kW22(µ, ) + B ,¯ + 2A ,¯(1 - m/2)k-1k

-

Rd

where Q , is defined in (S28) and

2µ(d) ,

B ,¯ = (2d/m) (1/m + 5¯) 1 + ¯L2/(2m) + ¯2L2/12 + 2Ld[N  + ( + 1)An,N ]M¯/m2 (S29) A ,¯ = 2( + 1)LM ,

An,N being defined in (S30).

Proof. Using Lemma S11, H5 is satisfied and applying Theorem S5 completes the proof.

S2.2 Supporting Lemmata

In this subsection, we derive two key lemmata in order to prove Theorem S9. Lemma S10. For any sequence {aj}Nj=1  (Rd)N where N  2, we have

2

X1

N

1x(1) (j)

-

n N

j=1

aj

1(dx(1))



n(N - n) N (N - 1)

N

aj 2 .

j=1

Proof. Let X(1) distributed according to 1. Since

N j=1

1X(1) (j)

=

n,

we

have

N

1X(1) (l) +

1X(1) (j)1X(1) (j ) = n2 .

l=1

j=j

Integrating this equality over X1 gives

N

×

n N

+ N (N

- 1) ×

[1x(1) (1)1x(1) (2)] 1(dx(1)) = n2 .
X1

Thus, we deduce that X1 [1x(1) (1)1x(1) (2)]1(dx(1)) = n(n - 1) [N (N - 1)]-1. In addition, using that

X1

1x(1) (j)

-

n N

1x(1) (j

)

-

n N

1(dx(1)) =

[1x(1) (1)1x(1) (2)]1(dx(1))
X1

-

n2 N2

,

we obtain

2





X1

N

1x(1) (j)

-

n N

j=1

aj

1(dx(1))

=

n(N - N2

n)



N

al 2 -

l=1

j=j

aj , aj N -1 



=

n(N - n) N 2(N - 1)

N

N

al 2 -

N

2

al  .

l=1

l=1

Denote

An,N

=

N (N - n) n(N - 1)

.

(S30)

The next lemma aims at controlling the variance of the global stochastic gradient considered in QLSD , required to apply Theorem S5.

22

Lemma S11. Assume H2 and H4. Then, for any   Rd, we have

b i=1

C

Fi (, x(1,i)), x(2,i)

- U () 2 b(dx(1:b))

Xb

 [N  + ( + 1)An,N ] M¯  -  , U () - U ( ) ,

where {Fi }i[b] and An,N are defined in (S26) and (S30), respectively. Hence H5 is satified with B~ = 0 and M~ = [N  + ( + 1)An,N ]M¯.

Proof. Let   Rd, using H2 gives

b

2

C Fi (, x(1,i)), x(2,i) - U () b(dx(1:b))
Xb i=1

=
Xb





b

N Cn

N

1x(1,i) (j) [Ui,j () - Ui,j ( )] , x(2,i)

i=1

j=1

-

b

N n

N
1x(1,i) (j) [Ui,j () - Ui,j ( )]

2
 b (dx(1:b) )

i=1 j=1

+
Xb1

b NN n

1x(1,i) (j)

-

n N

i=1 j=1

2
[Ui,j () - Ui,j ( )] 1b(dx(1,1:b))

2



N n

2b i=1 X1

N
1x(1,i) (j) [Ui,j () - Ui,j ( )]
j=1

1(dx(1,i))

2

+

N n

2b i=1 X1

N

1x(1,i) (j)

-

n N

j=1

Ui,j () - Ui,j ( )

1(dx(1,i))

b

=

Ui() - Ui( ) 2

i=1

+ ( + 1)

N n

2b i=1

Xb1

N j=1

1x(1,i) (j)

-

n N

2
[Ui,j () - Ui,j ( )] 1(dx(1,i)) . (S31)

Using Lemma S10 combined with H4 yields, for any i  [b],

N j=1

(1x(1,i)

(j

)

-

n/N

)

[Ui,j

()

-

Ui,j

(

)]

2
1(dx(1,i))

X1



n(N N (N

- n) - 1)

M¯

 -  , Ui() - Ui( )

.

(S32)

In addition, Jensen inequality implies, for any i  [b], that

N

Ui() - Ui( ) 2  N

Ui,j () - Ui,j ( ) 2 ,

j=1

and therefore, using H4, we have for any i  [b],

Ui() - Ui( ) 2  M¯N Ui() - Ui( ),  -  .

Injecting (S32) and (S33) into (S31) concludes the proof.

(S33)

23

S3 Proof of Theorem 3

S3.1 Problem formulation.

We assume here that U is still of the form (1) and that there exists N  N such that for any i  [b], there exists N functions {Ui,j :   Rd  R}j[N] such that for any   Rd,

N
Ui() = Ui,j() .
j=1
In all this section, we assume that n  N, n  N is fixed. Recall that N denotes the power set of [N ] and
N,n = {x  N : card(x) = n} .
In addition, we set in this section 1 as the uniform distribution on N,n. We consider the family of measurable functions {Gi : Rd × Rd × N  Rd}i[b], defined for any i  [b],   Rd,   Rd, x  N,n by

Gi(, ; x)

=

N n

N

1x(j) [Ui,j() - Ui,j()] + Ui() .

j=1

(S34)

For ease of reading, we formalise more precisely the recursion associated with QLSD++ under
H2. Let (Xk(1,1), . . . , Xk(1,b))kN and (Xk(2,1), . . . , Xk(2,b))kN be two independent i.i.d. sequences with distribution 1b and 2b, respectively. Let (Zk)kN be an i.i.d. sequence of d-dimensional standard Gaussian random variables independent of (Xk(1,1), . . . , Xk(1,b))kN and (Xk(2,1), . . . , Xk(2,b))kN . Denote for any k  N, Xk(1) = (Xk(1,1), . . . , Xk(1,b)), Xk(2) = (Xk(2,1), . . . , Xk(2,b)) and Xk = (Xk(1), Xk(2)). Let l  N,   (0, ¯] and   (0, ¯] for ¯, ¯ > 0. Given 0 = (0, 0, {0(i)}i[b])  Rd × Rd × Rdb, with 0 = 0, we recursively define the sequence (k)kN = (k, k, {k(i)}i[b])kN, for any k  N as

k+1 = k - G~(k; Xk+1) + 2Zk+1 ,

(S35)

where

b

G~(k; Xk+1) =

C

i=1

Gi k, k; Xk(1+,i1)

- k(i); Xk(2+,i1)

+ k(i) ,

(S36)

and for any i  [b],

k+1 =

k+1 , if k + 1  0 k , otherwise ,

(mod l) ,

(S37)

k(i+) 1 = k(i) + C Gi k, k; Xk(1+,i1) - k(i); Xk(2+,i1) .

(S38)

Since QLSD++ involves auxiliary variables gathered with (k)kN in (k)kN, we cannot follow the same proof as for QLSD by verifying H5 and then applying Theorem S5. Instead, we will adapt the proof Theorem S5 and in particular Lemma S6 and bound the variance associated to the stochastic gradient defined in (S36). Once this variance term will be tackled, the proof of Theorem 3 will follow the same lines as the proof of Theorem S5 upon using specific moment estimates for QLSD++. In the next section, we focus on these two goals: we provide uniform bounds in the number of iterations k on the variance of the sequence of stochastic gradients associated with QLSD++, (E[ G~i(k, Xk+1) - U (k) 2])kN for any i  [b], and (E[ k -  2])kN, see Proposition S18 and Corollary S17. To this end, a key ingredient is the design of an appropriate Lyapunov function defined in (S50).

24

S3.2 Uniform bounds on the stochastic gradients and moment estimates

Consider the filtration associated with (k)kN defined by G0 = (0) and for k  N, Gk = (0, (Xk~)k~k, (Zk~)k~k) .
We denote for any i  [b], ,   Rd, i(, ) = Ui() - Ui() .
Similarly, we consider, for any i  [b], j  [N ], ,   Rd, i,j (, ) = Ui,j () - Ui,j () .

(S39) (S40)

The following lemma provides a first upper bound on the variance of the stochastic gradients used in QLSD++.
Lemma S12. Assume H1, H2 and H4 and let   (0, ¯],   (0, ¯] for some ¯, ¯ > 0. Then, for any s  N, r  {0, . . . , l - 1}, we have

EGsl+r G~(sl+r; Xsl+r+1) - U (sl+r) 2  2( + 1)An,N M¯L + 2bM2 sl+r -  2

b
+ 2

Ui(

) - s(il)+r

2
+ 2( + 1)An,N M¯L

sl - 

2,

i=1

where (k~)k~N = (k~, k~, {k~(i)}i[b])k~N, G~ and An,N are defined in (S35), (S37), (S38), (S36) and (S30), respectively.

Proof. Let s  N and r  {0, . . . , l - 1}. Using H2, (S39) and (S40), we have

EGsl+r G~(sl+r; Xsl+r+1) - U (sl+r) 2



b



EGsl+r  

i=1

NN n
j=1

1Xs(1l+,ir)+1 (j)i,j (sl+r, sl+r)

2

+ Ui(sl+r) - s(il)+r

 



b

+

EGsl+r 



i=1

NN n
j=1

1Xs(1l+,ir)+1 (j)i,j (sl+r, sl+r)

2

- i(sl+r, sl+r)

 


b
 ( + 1) EGsl+r  
i=1

NN n
j=1

1Xs(1l+,ir)+1 (j)i,j (sl+r, sl+r)

2

- i(sl+r, sl+r)

 

b
+

Ui(sl+r) - s(il)+r 2

i=1



(

+

1)

N (N n(N

- -

n) 1)

M¯

sl+r

- sl+r, U (sl+r) - U (sl+r)

b
+

Ui(sl+r) - s(il)+r

2
,

i=1

where the last line follows from H4 and Lemma S10. The proof is concluded by using the CauchySchwarz inequality, H1 and sl+r = sl.

The two following lemmas aim at controlling the terms that appear in Lemma S12.

25

Lemma S13. Assume H1, H2 and H4, and let   (0, ¯],   (0, ¯] for some ¯, ¯ > 0. Then, for any s  N and r  [l], we have

EGsl+r-1 sl+r -  2  1 - 2m + 2Bn,N sl+r-1 -  2

b
+ 22

Ui( ) - s(il)+r-1 2 + 22( + 1)An,N M¯L sl -  2 + 2d ,

i=1

where

Bn,N = 2 ( + 1)An,N M¯L + bM2 + L2 ,

(S41)

(k~)k~N = (k~, k~, {k~i }i[b])k~N and An,N are defined in (S35), (S37), (S38) and (S30) respectively.

Proof. Let s  N and r  [l]. Using (S35) and H2, it follows

EGsl+r-1

sl+r -  2 = sl+r-1 -  2 + 2d - 2 U (sl+r-1), sl+r-1 -  +  E 2 Gsl+r-1 G~(sl+r; Xsl+r+1) 2 . (S42)

Using H2 and (S34)-(S36), we have

EGsl+r-1 G~(sl+r; Xsl+r+1) 2



b



E  Gsl+r-1 

i=1

NN n
j=1

1Xs(1l+,ir) (j)i,j (sl+r-1, sl+r-1)

2

+ Ui(sl+r-1) - s(il)+r-1

 



b

+

E  Gsl+r-1



i=1

NN n
j=1

1Xs(1l+,ir) (j)i,j (sl+r-1, sl+r-1)

2

- i(sl+r-1, sl+r-1)

 

+ U (sl+r-1) 2



b

 ( + 1)

E  Gsl+r-1 

i=1

NN n
j=1

1Xs(1l+,ir) (j)i,j (sl+r-1, sl+r-1)

2

- i(sl+r-1, sl+r-1)

 

b
+ U (sl+r-1) 2 + 

Ui(sl+r-1) - s(il)+r-1 2

i=1



(

+

1)

N (N n(N

- -

n) 1)

M¯

sl+r-1

-

sl+r-1, U (sl+r-1)

-

U (sl+r-1)

b
+ U (sl+r-1) 2 + 

Ui(sl+r-1) - s(il)+r-1

2
,

i=1

(S43)

where the last line follows from H4 and Lemma S10. The proof is concluded by injecting (S43) into (S42), using the Cauchy-Schwarz inequality, U ( ) = 0, H1 and sl+r-1 = sl.

Lemma S14. Assume H1, H2 and H4. Let   (0, ¯] for some ¯ > 0 and   (0, 1/( + 1)]. Then, for any s  N and r  [l], we have

b
EGsl+r-1
i=1

Ui( ) - s(il)+r 2

b
 (1 - )

Ui( ) - s(il)+r-1 2

i=1

+ Cn,N sl+r-1 -  2 + 2An,N M¯L sl -  2 ,

26

where

Cn,N = 2An,N M¯L + bM2 ,

(S44)

(k~)k~N = (k~, k~, {k~i }i[b])k~N and An,N are defined in (S35), (S37), (S38) and (S30), respectively.

Proof. Let s  N and r  [l]. Then, it follows

b
EGsl+r-1
i=1

Ui( ) - s(il)+r 2 = b Ui( ) - s(il)+r-1 2
i=1

b

+

EGsl+r-1

i=1

s(il)+r - s(il)+r-1 2

b

+2

EGsl+r-1

i=1

s(il)+r - s(il)+r-1

, s(il)+r-1 - Ui( ) . (S45)

Using (S38) and H2, we have for any i  [b],

EGsl+r-1

s(il)+r - s(il)+r-1 2

 2( + 1)EGsl+r-1 Gi sl+r-1, sl+r-1; Xs(l1+,ir) - s(il)+r-1 2 ,

(S46)

EGsl+r-1 s(il)+r - s(il)+r-1 = EGsl+r-1 Gi sl+r-1, sl+r-1; Xs(l1+,ir) - s(il)+r-1 . (S47)

Plugging (S46) and (S47) into (S45) yields

b
EGsl+r-1
i=1

Ui( ) - s(il)+r 2  b Ui( ) - s(il)+r-1 2
i=1

b

+ 2( + 1)

EGsl+r-1

i=1

Gi sl+r-1, sl+r-1; Xs(l1+,ir) - s(il)+r-1 2

b

+ 2

EGsl+r-1 Gi sl+r-1, sl+r-1; Xs(l1+,ir) - s(il)+r-1 , s(il)+r-1 - Ui( ) .

i=1

Using (1 + )  1 and the fact, for any a, b, c  Rd, that a - c 2 + 2 (a - c), (c - b) = a - b 2 - c - b 2, we have

b
EGsl+r-1
i=1

Ui( ) - s(il)+r 2

b
 (1 - )

Ui( ) - s(il)+r-1 2

i=1

b

+

EGsl+r-1

i=1

Gi

sl+r-1, sl+r; Xs(l1+,ir)

2
- Ui( )

.

Using (S34), H4 and Lemma S10, it follows

(S48)

b
EGsl+r-1

Gi

sl+r-1, sl+r-1; Xs(l1+,ir)

2
- Ui( )

i=1



N (N n(N

- -

n) 1)

M¯

sl+r-1

-

sl+r-1, U (sl+r-1)

- U (sl+r-1)

b

+

Ui(sl+r-1) - Ui( ) 2 .

i=1

(S49)

The proof is concluded by plugging (S49) into (S48), using the Cauchy-Schwarz inequality, H1 and sl+r-1 = sl.

27

Lemma S13 and Lemma S14 involve two dependent terms which prevents us from using a straightforward induction. To cope with this issue, we consider a Lyapunov function  : Rd × Rbd  R
defined, for any   Rd and  = ((1), . . . , (b))  Rbd by

b
(, ) =  -  2 + (3/)2

Ui(

) - (i)

2
.

i=1

(S50)

The following lemma provides an upper bound on this Lyapunov function. Define for  > 0,

¯,1 = m-1[{m2(Bn,N + 3Cn,N )-1}  {/3}] ,

(S51)

where Bn,N and Cn,N are defined in (S41) and (S44) respectively. Lemma S15. Assume H1, H2 and H4. Let   (0, 1/( + 1)],   (0, ¯,1]. Then, for any s  N and r  [l], we have

EGsl+r-1 [(sl+r, sl+r)]  (1 - m) (sl+r-1, sl+r-1) + 2An,N M¯L2 [4 + 1] sl -  2 + 2d ,

where  is defined in (S50) and (k~)k~N = (k~, k~, {k~i }i[b])k~N and An,N are defined in (S35), (S37), (S38) and (S30), respectively.

Proof. Let s  N and r  [l]. Using Lemma S13 and Lemma S14, we have

EGsl+r-1 [(sl+r, sl+r)]  1 - 2m + 2 [Bn,N + 3Cn,N ] sl+r-1 -  2

b
+ [3 + (1 - )] (3/)2

Ui( ) - s(il)+r-1 2

i=1

+ 2An,N M¯L2 [4 + 1] sl -  2 + 2d .

Since   ¯,1 with ¯,1 given in (S51), it follows that

1 - 2m + [Bn,N + 3Cn,N ]  1 - m 3 + (1 - )  1 - m .

Therefore, we have EGsl+r-1 [(sl+r, sl+r)]  (1 - m) (sl+r-1, sl+r-1) + 2An,N M¯L2 [4 + 1] sl - 

2 + 2d .

Lemma S16. Let p  N and fix  > 0 such that





m 4An,N M¯Lp(4

+ 1)



1 m

.

Then,

(1 - m)p + 2An,N M¯Lp[4 + 1]2  1 - m/2 ,

where An,N is defined in (S30).

Proof. The proof is straightforward using (1 - m)p  1 - m.

We have the following corollary regarding the Lyapunov function defined in (S50).

Denote for  > 0,

¯,2 = ¯,1  [m/{4An,N M¯Ll(4 + 1)}] ,

where ¯,1 is given in (S51).

28

(S52)

Corollary S17. Assume H1, H2 and H4. Let   (0, 1/( + 1)] and   (0, ¯,2]. Then, for any s  N and r  {0, . . . , l - 1} we have
EGsl ((s+1)l-r, (s+1)l-r  (1 - m/2) (sl, sl) + 2(l - r)d , where  is defined in (S50) and (k~)k~N = (k~, k~, {k~i }i[b])k~N is defined in (S35), (S37), (S38).
Proof. The proof follows from a straightforward induction of Lemma S15 combined with Lemma S16.

We are now ready to control explicitly the variance of the stochastic gradient defined in (S36).
Proposition S18. Assume H1, H2 and H4. Let   (0, 1/( + 1)] and   (0, ¯,2], where ¯,2 is defined in (S52). Then, for any k = sl + r with s  N, r  {0, . . . , l - 1}, 0  Rd and 0 = (0(1), . . . , 0(b))  Rdb, we have
E G~(sl+r; Xsl+r+1) - U (k) 2  (1 - m/2)sDn,N (0, 0) + 4ldDn,N /m

b
+ 2(1 - )k E

Ui( ) - 0(i) 2 ,

i=1

where

Dn,N = 4( + 1)An,N M¯L + 2bM2 + 4Cn,N ,

(S53)

An,N and Cn,N are defined in (S30) and (S44) respectively,  is defined in (S50), and (k~)k~N = (k~, k~, {k~i }i[b])k~N is defined in (S35), (S37), (S38).

Proof. Let k  N and write k = sl + r with s  N, r  {0, . . . , l - 1} Then, using Lemma S12, we have
E G~(sl+r; Xsl+r+1) - U (k) 2

b
 2( + 1)An,N M¯L + 2bM2 E k -  2 + 2 E
i=1
+ 2( + 1)An,N M¯LE sl -  2 .

Ui( ) - k(i) 2

(S54)

We now use our previous results to upper bound the three expectations at the right-hand side of

(S54). First, using Corollary S17 and a straightforward induction gives

s-1
E sl -  2  (1 - m/2)s(0, 0) + 2ld (1 - m/2)j

j=0

 (1 - m/2)s(0, 0) + 4ld/m .

(S55)

Similarly, we have

s
E k -  2  (1 - m/2)s+1(0, 0) + 2ld (1 - m/2)j

j=0

 (1 - m/2)s(0, 0) + 4ld/m .

(S56)

Finally, using Lemma S14 combined with (S55) and (S56), we obtain

b
E

Ui( ) - k(i) 2  (1 - ) b E

Ui( ) - k(i-) 1 2

i=1

i=1

+ 2Cn,N (1 - m/2)s(0, 0) + 8ldCn,N /m .

Then, a straightforward induction leads to

b
E

Ui( ) - k(i) 2  (1 - )k b

Ui( ) - 0(i) 2

i=1

i=1

+ 2Cn,N (1 - m/2)s(0, 0) + 8ldCn,N /m .

Combining (S55), (S56) and (S57) in (S54) concludes the proof.

(S57)

29

S3.3 Proof of Theorem 3

Note that   (0, ¯],   (0, ¯] and l  N, (k~)k~N = (k~, k~, {k~(i)}i[b])k~N defined in (S35), (S37), (S38) is a inhomogeneous Markov chain associated with the sequence of Markov kernel (Q(k,) ,l)kN defined by as follows. Define for any (, , )  Rd × Rd × Rd, and x(1)  N,n and x(2)  X2,

Fi((, , ); (x(1), x(2))) = C Gi , ; x(1) - ; x(2)
Gi((, , ); (x(1), x(2))) =  + Fi((, , ); (x(1), x(2))) .
and for ~  Rd, {(i)}bi=1  Rdb, {x(1,i)}bi=1  bN,n, {x(2,i)}bi=1  Xb2, setting x(1:b) = {(x(1,i), x(2,i))}bi=1,

 ((~, , , {(i)}bi=1); x(1:b)) = (4)-d/2 exp - ~ -  + 

b i=1

Fi

((,



,

(i)

);

x(i))

2/(4)

.

Denote X~ = N,n × X2 and ~ = 1 × 2. Set Q(0,) ,l = Id and for k  0, k = ls + r, s  N, r  {0, . . . , l - 1}, (, , )  Rd × Rd × Rdb and A  B(Rd × Rd × Rdb),

if r = 0 Q(k,+,1l)((, , ), A) =
X~b 1A(~, ~, ~) ((~, , , {(i)}bi=1); x(1:b)){
otherwise Q(k,+,1l)((, , ), A) =
X~b 1A(~, ~, ~) ((~, , , {(i)}bi=1); x(1:b)){

b i=1

Gi ((, ,);x(i) ) (d~(i) )} (d~)

d~

~b(dx(1:b))

b i=1

Gi((,,);x(i))(d~(i))}

(d~)

d~

~b(dx(1:b))

.

Consider then, the Markov kernel on Rd × B(Rd),

R(k,) ,l,0 (0, A) = Q(k,) ,l((0, 0, 0), A × Rd × Rdb) .

(S58)

Define

¯ = ¯,2  ¯4 , ¯4 = 1/(10m) ,

(S59)

where ¯,2 is defined in (S52). The following theorem provides a non-asymptotic convergence bound for the QLSD++ kernel.

Theorem S19. Assume H1, H2 and H4. and let l  N. Then, for any probability measure µ  P2(Rd), 0  Rdb,   (0, 1/(1 + )],   (0, ¯], and k = sl + r  N with s  N, r  {0, . . . , l - 1}, we have

W22(µR(k,) ,l,0 , )  (1 - m/2)kW22(µ, ) + (2/m)(1 - m/2)sDn,N

(0, 0)dµ(0)
Rd

b
+ (4/m)(1 - )k

Ui(

) - 0(i)

2
+ B,¯ ,

i=1

where R(k,) ,l,0 is defined in (S58),  is defined in (S50), Dn,N in (S53) and

B,¯ = 2d(m-1 + 5¯) 1 + ¯L2/(2m) + ¯2 L2/12 /m + 8ld(4An,N M¯L + 2bM2)(3 + 1)/m2 . (S60)

Proof. Let k  N. The proof follows from the same lines as Theorem S8. By (S14) and (S35), we have
(k+1) - k+1 = k - k -  [U (k) - U (k)]

30


- [U (k+s) - U (k)] ds +  G~(k; Xk+1) - U (k) .
0
Define the filtration (Hk~)k~N as H0 = (0, 0) and for k~  N,
Hk~ = (0, 0, (Xl(1), . . . , Xl(b))1lk~, (Bt)0tk~) .

Note that since (t)t0 is a strong solution of (S14), then is easy to see that (k~, k~)k~N is (Hk~)k~N-adapted. Taking the squared norm and the conditional expectation with respect to Hk, we obtain using H5-(i) that

EHk (k+1) - k+1 2 = k - k 2 - 2 k - k, U (k) - U (k)



+ 2

U (k) - U (k), EHk [U (k+u) - U (k)] du

0



-2

k - k, EHk [U (k+u) - U (k)] du

0

+ 2 U (k) - U (k) 2

+ EHk



2

[U (k+u) - U (k)] du

0

+ 2EHk G~(k; Xk+1) - U (k) 2 .

(S61)

Using Proposition S18, we obtain E G~(k; Xk+1) - U (k) 2  (1 - m/2) k/l Dn,N (0, 0) + 4ldDn,N /m

b
+ 2(1 - )k

Ui(

) - 0(i)

2
.

i=1

(S62)

Then, we control the remaining terms in (S61) using (S20), (S21) and (S22). Combining these bounds and (S62) into (S61), for any  > 0, yields

E (k+1) - k+1 2  (1 + 2 - 52mL)E k - k 2

-  [2 - 5(m + L)] E [ k - k, U (k) - U (k) ]

+ (5 + (2)-1) E U (k+u) - U (k) 2 du
0
+ 2(1 - m/2) k/l Dn,N E [(0, 0)] + 4ldDn,N /m

b
+ 22(1 - )k

Ui(

) - 0(i)

2
.

i=1

(S63)

Next, we use that under H1, k - k, U (k) - U (k)  m k - k 2 and | k -  , U (k)-U ( ) |  L k - 2, which implies taking  = m/2 and since 2-5(m+L)  0,

E (k+1) - k+1 2  (1 - m(1 - 5m))E k - k 2


+ (5 + m-1) E U (k+u) - U (k) 2 du
0
+ 2(1 - m/2) k/l Dn,N E [(0, 0)] + 4ldDn,N /m

b
+ 22(1 - )k

Ui(

) - 0(i)

2
.

i=1

(S64)

31

Further, for any u  R+, using [17, Lemma 21] we have
E U (k+u) - U (k) 2  du 2 + u2L2/3 + 3u2L2/2E k -  2 .
Integrating the previous inequality on [0, ], we obtain

E U (k+u) - U (k) 2 du  d2 + d4L2/12 + 3L2/2E k -  2 .
0
Plugging this bounds in (S64) and using Lemma S7 conclude the proof.

S4 Consistency analysis in the big data regime

In this section, we provide upper bounds on the asymptotic bias associated to each algorithm when the number of observations on each client N tends towards infinity.

S4.1 Asymptotic analysis for Algorithm 1

The following corollary is associated with QLSD defined in Algorithm 1.
Corollary S20. Assume H1, H2 and H3. In addition, assume that lim infN m/N > 0 and lim supN A/N > 0 for A  {L, M, B ,  }. Then, we have ¯ = ¯/N where ¯ > 0 and ¯ is defined in (S13). In addition,
B¯ = ( + 1) O(N ) , where B¯ is defined in (S25).

Proof. Since we assume that lim infN m/N > 0 and lim supN A/N > 0 for A  {L, M, B ,  }, there exist Cm, CL, CM, CB and C > 0 such that m  CmN , L  CLN , M  CMN , B  CB N and   C N . Under these assumptions, it is straightforward from (S13) to see that
there exists ¯ > 0 such that ¯ = ¯/N . In addition, it follows from (S25) that

B¯



2d CmN 2

(1/Cm

+

5¯)

1 + ¯CL2/(2Cm) + ¯2CL2/12

+ 4(CB

+ C2 N )/Cm

+ 4( + 1)CLCM d + ¯(CB + C2 N ) /Cm2 .

The proof is concluded by letting N tend towards infinity.

Regarding the specific instance QLSD# of Algorithm 1, a similar result holds. Indeed, by using Lemma S10, we can notice that H3-(iii) is verified with  = C N for some C > 0 and we can apply Corollary S20.

S4.2 Asymptotic analysis for Algorithm 2

The following corollary is associated with QLSD defined in Algorithm 2.
Corollary S21. Assume H1, H2 and H4. In addition, assume that lim infN m/N > 0 and lim supN A/N > 0 for A  {L, M}. Then, we have ¯ = ¯/N where ¯ > 0 and ¯ is defined in (S13). In addition,
B ,¯ = d( + 1) O(1) , where B ,¯ is defined in (S29).

Proof. Since we assume that lim infN m/N > 0 and lim supN A/N > 0 for A  {L, M}, there exist Cm, CL and CM > 0 such that m  CmN , L  CLN and M  CMN . Under these assumptions, it is straightforward from (S13) to see that there exists ¯ > 0 such that ¯ = ¯/N . In
addition, it follows from (S16) that

B

,¯



2d CmN 2

(1/Cm

+

5¯)

1 + ¯CL2/(2Cm) + ¯2CL2/12

+ 2CLd



+

(

+

1)

·

N -n n(N - 1)

M¯/Cm2 .

The proof is concluded by letting N tend towards infinity.

32

Lastly, we have the following asymptotic convergence result regarding QLSD++ defined in Algorithm 2.
Corollary S22. Assume H1, H2 and H4. In addition, assume that lim infN m/N > 0 and lim supN A/N > 0 for A  {L, M}. Then, we have ¯ = ¯/N where ¯ > 0 and ¯ is defined in (S59). In addition,
B,¯ = d( + 1) O(1) ,
where B,¯ is defined in (S60).

Proof. Since we assume that lim infN m/N > 0 and lim supN A/N > 0 for A  {L, M}, there exist Cm, CL and CM > 0 such that m  CmN , L  CLN and M  CMN . Under these assumptions, it is straightforward from (S59) to see that there exists ¯ > 0 such that ¯ = ¯/N . In
addition, it follows from (S60) that

B,¯



2d CmN 2

(1/Cm

+ 5¯)

1 + ¯CL2/(2Cm) + ¯2CL2/12

+ 8ld

4(N n(N

- -

n) 1)

M¯CL

+

2bCM2

(3 + 1)/Cm2 .

The proof is concluded by letting N tend towards infinity.

33

