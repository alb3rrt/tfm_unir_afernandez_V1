
# Robustifying Algorithms of Learning Latent Trees with Vector Variables

[arXiv](https://arxiv.org/abs/2106.0885), [PDF](https://arxiv.org/pdf/2106.0885.pdf)

## Authors

- Fengzhuo Zhang
- Vincent Y. F. Tan

## Abstract

We consider learning the structures of Gaussian latent tree models with vector observations when a subset of them are arbitrarily corrupted. First, we present the sample complexities of Recursive Grouping (RG) and Chow-Liu Recursive Grouping (CLRG) without the assumption that the effective depth is bounded in the number of observed nodes, significantly generalizing the results in Choi et al. (2011). We show that Chow-Liu initialization in CLRG greatly reduces the sample complexity of RG from being exponential in the diameter of the tree to only logarithmic in the diameter for the hidden Markov model (HMM). Second, we robustify RG, CLRG, Neighbor Joining (NJ) and Spectral NJ (SNJ) by using the truncated inner product. These robustified algorithms can tolerate a number of corruptions up to the square root of the number of clean samples. Finally, we derive the first known instance-dependent impossibility result for structure learning of latent trees. The optimalities of the robust version of CLRG and NJ are verified by comparing their sample complexities and the impossibility result.

## Comments



## Source Code

Official Code



Community Code



## Bibtex

```tex
@misc{zhang2021robustifying,
      title={Robustifying Algorithms of Learning Latent Trees with Vector Variables}, 
      author={Fengzhuo Zhang and Vincent Y. F. Tan},
      year={2021},
      eprint={2106.00885},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
```

## Notes

Type your reading notes here...

