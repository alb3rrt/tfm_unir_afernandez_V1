A FAST PRIMAL-DUAL-ACTIVE-JUMP METHOD FOR MINIMIZATION IN BV((0, T ); Rd)
PHILIP TRAUTMANN1 AND DANIEL WALTER2

arXiv:2106.00633v1 [math.OC] 1 Jun 2021

Abstract. We analyze a solution method for minimization problems over a space of Rd-valued functions of bounded variation on an interval I. The presented method relies on piecewise constant iterates. In each iteration the algorithm alternates between
proposing a new point at which the iterate is allowed to be discontinuous and optimizing
the magnitude of its jumps as well as the offset. A sublinear O(1/k) convergence rate
for the objective function values is obtained in general settings. Under additional
structural assumptions on the dual variable this can be improved to a locally linear rate of convergence O(k) for some  < 1. Moreover, in this case, the same rate can be expected for the iterates in L1(I; Rd).

1. Introduction

We consider minimization problems of the form

min j(u) = [F (Ku) +  u M].

(P )

uBV(I ;Rd )

where the minimizer is sought for in the space of Rd-valued functions of bounded variation on an interval I = (0, T ). Here K denotes a linear and continuous operator mapping to a Hilbert space of observations Y and F is assumed to be a convex smooth loss function. Given  > 0, the second term in the objective functional penalizes the total variation norm of the distributional derivative u . It is well known that such a penalization favours minimizers which only change their values at a finite number of time points. This structural property of (P) makes it appealing for a variety of practical applications. For example, we point out PDE constraint optimal control problems, [6, 13, 17] and the denoising of scalar signals, [26, 23]. For the precise functional analytic setting we refer to Section 3.

1.1. Contribution. The aim of this paper is to analyze a simple yet efficient iterative solution algorithm for problem (P). It relies on the identification of u  BV(I; Rd) with its distributional derivative u and the mean values of its components au  Rd.
The proposed method generates sequences of piecewise constant iterates uk and active sets Ak = {(µki , vik)}Ni=k1 which store the jumps vik of uk as well as the associated magnitudes µi. By a "jump" vik we refer to an atomic measure supported on a position tki  I together with a normalized direction vik  Rd. Each iteration then proceeds

2020 Mathematics Subject Classification. 26A45, 65J22, 65K05, 90C25, 49M05. Key words and phrases. Bounded variation functions, generalized conditional gradient,sparsity. 1 Institute for Mathematics and Scientic Computing, Karl-Franzens-Universität,Heinrichstr. 36, 8010 Graz, Austria. The author was supported by the ERC advanced grant 668998 (OCLOC) under the EU's H2020 research program. 2 Johann Radon Institute for Computational and Applied Mathematics, Altenbergerstr. 69, 4040 Linz, Austria, daniel.walter@oeaw.ac.at .
1

2

in three phases: First we allow for an additional jump vk in the iterate uk. The position and the direction of this new candidate jump are determined based on a dual variable pk  C0(I; Rd). Subsequently we determine improved magnitudes for all jumps in the active set as well as a new vector of mean values by solving the finite-dimensional
convex minimization problem



Nk+1 

Nk

min F (Ku) + 
µi0,auRd

i=1

µi

s.t. u

= µNk+1vk + µki vik.
i=1

Finally the active set is updated by removing all jumps whose associated magnitude
was set to zero. The theoretical contribution of the present manuscript is twofold.
First we prove that the generated sequence uk indeed converges, on subsequences, to minimizers of (P) and the functional values j(uk) converge sublinearly to the minimum value. Second, under appropriate structural assumptions on the optimal dual variable,
similar to [17, 13], we deduce the local linear convergence of j(uk) and of the iterates uk with respect to the strict topology on BV(I; Rd).

1.2. Related work. The efficient algorithmic solution of(P) is a delicate issue for a

variety of reasons. On the one hand this is attributed to the appearance of the BV

seminorm which makes the objective functional nonsmooth. Moreover j lacks coercivity

with respect to u which is often a vital tool in the derivation of fast convergence result for

minimization schemes. On the other hand we point out that BV(I; Rd) is non-reflexive.

Many well-studied algorithms for non-smooth optimization rely on the reflexive structure

of the underlying space and thus donot yield direct extensions to the problem at hand.

A first straightforward approach to circumventing the aforementioned difficulties

consists of discretizing the space BV(I; Rd) in (P). More in detail, instead of minimizing over all u  BV(I; Rd), one could only consider piecewise constant uh that solely jump

in the nodes 0 = t0 < t1 < · · · < tNh = T of a partition of I. This reduces (P) to a finite dimensional convex minimization problem with a nonsmooth group sparsity

regularization term, [19]. The solution of discretized 1D BV problems has been addresses

e.g. in [9, 20, 28, 21]. Nonetheless, such reasoning often leads to algorithms that

exhibit mesh-dependency meaning that their convergence behaviour critically depends

on the partition of I and might degenerate as Nh  . To mitigate these effects, a second

line of works, see e.g. [8, 16], proposes the regularization of (P) by adding (/2) u

2 L2

for 0 <  << 1 and minimizing for u in the Sobolev space H1(I; Rd). Since the total

variation norm of u remains present in objective functional, the derivative of minimizers

can still be expected to exhibit sparsity i.e. its support is small. However, due to the

Sobolev seminorm penalty, minimizers cannot be piecewise constant if  > 0. For this

reason algorithmic approaches based on regularization are usually accompanied by a

path-following strategy for   0 which requires additional analysis.

If (P) is restricted to mean-value free BV functions, it can be equivalently reformulated

as minimization problem over the space of Rd-valued vector measures. Over the past

years there has been an increasing body of work on the efficient solution of such problems

using exchange type algorithms, [25, 10, 14, 5, 4], which rely on iterates comprised of

finitely many Dirac Delta functionals. These alternate between proposing a new Dirac

Delta (i.e. a "jump" in our terminology) and approximately solving finite-dimensional

convex and/or nonconvex subproblems to achieve sufficient descent. Most recently,

linear convergence of such methods relying on convex subproblems has been addressed

in [14], for d = 1, and [25], for the general vector-valued case. Our approach is closest

3

related to the earlier work [25] but differs in the treatment of the convex subproblems.

More in detail while the present method relies solely on optimizing the magnitudes µi

in each iteration (#Ak DOF), the linear convergence result of [25] also requires the optimization of the jump directions (d#Ak DOF). Hence we obtain the same theoretical convergence guarantees while solving smaller subproblems. Let us also mention the

finite step convergence results of [14, 10]. However these require "point-moving" i.e. an

additional step in which the jump positions are optimized. This constitutes a nonconvex

problem and is therefore not considered in the present work. The idea of using exchange

type methods for 1D BV penalties has previously been proposed in [3] together with a

sublinear convergence result.

Finally we point out the denoising problem for a scalar signal yd  L2(I). In our

setting this corresponds to the case of d = 1, F = (1/2)

· -yd

2 L2

and

K

=

Id.

For

this particular instance of (P) the unique minimizer can be determined directly using

a taut-string-method, see e.g. [15, 18]. To the best of our knowledge this method does,

however, not yield extensions to the case of a general observation operator K and the

vector-valued case d > 1.

1.3. Outline of the paper. The relevant notation used throughout the paper is introduced in Section 2. In Section 3 we equivalently reformulate (P) as minimization problem over the distributional derivative and the mean value of u. Subsequently, this equivalence is used to derive first-order necessary and sufficient optimality conditions. A detailed description of the proposed solution algorithm for (P) can be found in Section 4. The convergence of the method is adressed in Section 5. Finally, Section 6 finishes the paper with numerical experiments illustrating our theoretical findings.

2. Notation & definitions

In the following set I = (0, T ) for some T > 0 and fix d  N. Denote by (·, ·)Rd the euclidean inner product on Rd and let | · |Rd denote the corresponding norm. By C0(I; Rd) together with the usual supremum norm

 C = sup |(t)|Rd   C0(I; Rd)
tI

we refer to the Banach space of Rd-valued continuous functions on I that vanish at its

boundary. Its topological dual space is readily identified with the space of regular vector

measures M(I; Rd). The corresponding duality pairing is denoted by ·, · . For example,

if q is a discrete measure, i.e. q =

N i=1

qi

ti

where

qi



Rd

and

ti

denotes

the

Dirac

Delta functional supported on ti  I, then

N
, q = ((ti), qi)Rd.
i=1

The space M(I; Rd) is equipped with the canonical dual norm

q M = sup , q .
 C =1
We call u  L1(I; Rd) a function of bounded variation if its distributional derivative u is representable by a an element of M(I; Rd) i.e.

u,  L2 = , u   Cc(I; Rd).

4

The set of Rd-valued functions of bounded variation on I is now defined as BV(I; Rd) = u  L1(I; Rd) | u M <  .

Equipping BV(I; Rd) with the norm

where

u BV = u M + u L1

u  BV(I; Rd),

u L1 = |u(s)|Rd ds,
I
makes it a Banach space which continuously embeds into Lp(I), p  [1, ], the embedding being compact for p < . Given a function u  L1(I; Rd), the vector of the mean values of its components is defined as

1

au = T

u(t) dt,
I

where integration has to be understood in the sense of Bochner. From e.g. [1, Theo-

rem 3.44] we conclude the existence of constants C1, C2 > 0 with

C1(|au|Rd + u M)  u BV  C2(|au|Rd + u M) u  BV(I; Rd).

Following e.g. [1, Remark 3.12] BV(I; Rd) can be identified as the topological dual space
of a separable Banach space. A sequence {uk}kN  BV(I; Rd) is called weak* convergent in BV(I; Rd) with limit u¯ if

uk - u¯ L1  0, uk  u¯ .

Due to the sequential Banach-Alaoglu theorem every bounded sequence in BV(I; Rd) ad-
mits a weak* convergent subsequence. Furthermore a weak* convergent sequence {uk}kN  BV(I; Rd) is called convergent with respect to the strict topology on BV(I; Rd), or shortly strictly convergent, if additionally uk M  u¯ M holds. This is indicated by " s". The strict topology on BV(I; Rd) is induced by the metric

d(u1, u2) = u1 - u2 L1 + | u1 M - u2 M|.

Last, given an open interval (t, 1) for some t  [0, 1) its characteristic function is defined by

0 on I \ (t, 1)

t = 1 else

.

There holds t  BV(I) with t = t, t > 0, and 0 = 0, respectively.

3. Optimization problem The following assumptions concerning (P) are made throughout this paper.

Assumption 1. In the following let Y be a Hilbert space with inner product (·, ·)Y and induced norm · Y . There holds:
· The operator K : L2(I; Rd)  Y is linear and continuous. · The mapping F : Y  R is strictly convex and continuously Fréchet differentiable.
The Fréchet derivative F (y)  L(Y, R) of F at y  Y is identified with its Riesz representative F (y)  Y i.e.
F (y)y = (F (y), y)Y y  Y.

5

· The functional j : BV(I; Rd)  R in (P) is radially unbounded i.e.

uk BV    j(uk)  +.

Existence of solutions to (P) can be obtained using the direct method. Since the proof is fairly standard we omit it at this point.

Proposition 1. Let Assumption 1 hold. Then there exists at least one solution u¯  BV(I; Rd) to (P).

3.1. Optimality conditions. The derivation of most subsequent results relies on an equivalent reformulation of (P) which will be introduced next. Define the linear and continuous operator

B : M(I; Rd) × Rd  L2(I; Rd),

·

1T

(q, c)  dq -

0

T0

s
dq ds + c,
0

(3.1)

where integration hast to be understood in the sense of Bochner. We arrive at the following identification.

Proposition 2. For all (q, c)  M(I; Rd) × Rd we have B(q, c)  BV(I; Rd). The linear and continuous operator B : M(I; Rd) × Rd  BV(I; Rd) from (3.1) is an isomorphism.

Proof. The bounded invertibility of B is imminent noting that its inverse is given by the operator
B-1 : BV(I; Rd)  M(I; Rd) × Rd, u  u , au .

Loosely speaking, the previous result states that any function of bounded variation on I is uniquely characterized by its distributional derivative and mean values of its components. Thus (P) is equivalent to the sparse minimization problem

min [F (K(q, c)) +  q M].
qM(I;Rd), cRd

(3.2)

where we abbreviate K = K  B. Next we characterize the adjoint operator B. Consider the system of auxiliary
ordinary differential equations

T

- =  in (0, T ),  (0) =  (T ) = 0,

(t) dt = 0,

0

(3.3)

where   L2(I; Rd) with a = 0. Clearly, this problem admits a unique solution   H2(I; Rd)  C1(I; Rd) and   C0(I; Rd).

Lemma 3. The linear and continuous operator B from (3.1) is the Banach space adjoint of

T
B : L2(I; Rd)  C0(I; Rd) × Rd,    , (s) ds .
0

(3.4)

where   C1(I; Rd) fulfills (3.3) for  =  - a.

6

Proof. Obviously, the operator B is linearly and continuous. Let   L2(I; Rd) and a pair (q, c)  M(I; Rd) × Rd be given. We readily obtain

T

T

s

Ts

T

 , q + c, (t) dt =

(s), dq ds - a,

dq ds + c, (t) dt

0

0

0

00

0

= (, B(q, 0))L2 + = (, B(q, c))L2 .

T
c, (t) dt
0

Here we used B(q, 0)  BV(I; Rd) with B(q, 0) = q as well as the integration by parts in the second equality. This establishes the result.

Combining the equivalence of (P) and (3.2) as well as the characterization of B we arrive at the following necessary and sufficient first order optimality conditions.
Theorem 4. Let u¯  BV(I; Rd) be given. Further define
t
p¯(t) = KF (Ku¯)(s) ds  C(I¯; Rd)
0
Then u¯ is an optimal solution to (P) if and only if

{} u¯ = 0 p¯ C  [0, ] else , p¯(T ) = 0

(3.5)

as well as

p¯, u¯ =  u¯ M.

(3.6)

Proof. A function u¯  BV(I; Rd) is an optimal solution to (P) if and only if the pair

1T

(q¯, c¯) = u¯ ,

u¯(s) ds

T0

is a minimizer of (3.2). Since J is convex and F is Fréchet-differentiable, optimality of (q¯, c¯) is equivalent to

(-F (K(q¯, c¯)), K(q - q¯, 0))L2(I;Rd) + G( q¯ M)  G( q M), q  M(I; Rd) (3.7) as well as

(-F (K(q¯, c¯)), K(0, c))L2(I;Rd) = 0 c  R.

(3.8)

Let ¯  C1(I¯; Rd) denote the solution of (3.3) for  = -KF (Ku¯)  L2(I). Note that K = BK. Utilizing the characterization of B, see Lemma 3, as well as the
definition of the convex subdifferential the conditions (3.7) and (3.8), respectively, can
be rewritten as

T
¯   q¯ M, p¯(T ) = -KF (Ku¯)(t) dt = 0.
0

(3.9)

It is well known, that the subdifferential inclusion is equivalent to

{} q¯ = 0 ¯ C  [0, ] else , ¯ , q¯ =  q¯ M.

7

Due to the fundamental theorem of analysis, there exists a vector c  Rd with

t

T

¯ (t) = KF (Ku¯)(s) ds - t KF (Ku¯)(s) ds + c

0

0

t
= KF (Ku¯)(s) ds + c

0

for all t  I¯. From ¯ (0) = 0 we deduce c = 0. Thus we conclude p¯ = ¯ on I¯. Combining all the previous observations now finishes the proof.

It is by now well-known that the extremality condition in (3.6) ensures the sparsity of u¯ if the dual variable p¯ only admits finitely many global extrema.

Corollary 5. Let u¯  BV(I; Rd) be a minimizer of (P) and let p¯ be defined as in Theorem 4. Assume that

{t¯i}Ni=1 = { t  I | |p¯(t)|Rd =  }
for some N  N and {t¯i}Ni=1  I. Then u¯  M(I; Rd) is of the form
N
u¯ = µ¯iv¯i where v¯i = (p¯(t¯i)/)t¯i
i=1
i.e. u¯ is piecewise constant on I.

(3.10)

Proof. This can be proven analogously to [29, Corollary 6.25].

Finally we point out that the optimal observation y¯  Y in (P) and thus also the dual variable p¯  C0(I; Rd), see Theorem 4, are unique.
Corollary 6. Let u¯1, u¯2  BV(I; Rd) denote two minimizers to (P). Moreover denote by y¯1 = Ku¯1, y¯2 = Ku¯2 and p¯1, p¯2  C0(I; Rd) the associated observations and dual variables, see Theorem 4, respectively. Then y¯1 = y¯2 and p¯1 = p¯2.
Proof. The uniqueness of the optimal observation, and thus also that of the dual variable, directly follows from the strict convexity of F .

4. Algorithmic solution

This section is devoted to the description of an efficient solution algorithm for (P). The method we propose relies on the iterative update of a finite active set Ak = {µki , vik}Ni=k1 comprised of "jumps" vik  M(I; Rd) and the associated "magnitudes" µki > 0. Each jump is of the form vik = viktki for a position tki  I and a direction vik  Rd, |vik|Rd = 1. Given an offset ck  Rd the k - th iterate is defined as

 Nk



uk = B  µki vik, ck .

i=1

(4.1)

If Ak = , i.e. uk = ck0, we adopt the convention Nk = 0. We shortly describe

the individual steps of the algorithm in the following. A summary can be found in

Algorithm 1. Given the current active set Ak and iterate uk we first compute the

current dual variable pk(·) =

· 0

KF (Kuk)

ds

as

well

one

of

its

global

extrema

t^k



I. Next, assuming that pk C > 0, see Proposition 7, we define the new candidate

8

jump vk := (pk(t^k)/ pk C)t^k and find improved jump heights µk+1/2  RN+k+1 and a new offset ck+1  Rd from solving



Nk



Nk+1 

min
(µ,c)RN+k +1 ×Rd

F

K

µNk +1 v k

+

i=1

µi vik ,

c

+



i=1

µi .

(PAk )

This represents a finite-dimensional convex minimization problem with box constraints which can be tackled by a variety of efficient solution algorithms. Now the new jump is added to the active set and the jump heights are updated setting

Ak+1/2 := (µNk+k1+/12, vk) 

(µki +1/2, vik)

Nk
.
i=1

Finally we prune the active set by removing all jumps whose associated jump magnitude

was set to zero i.e.

Ak+1 :=

µki +1, vik+1

Nk+1 =
i=1

(µ, v)  Ak+1/2 | µ > 0

and increment the iteration counter k by one.

We point out that the termination criterion of Algorithm 1 relies on the norm of pk. This is justified by the following proposition.

Proposition 7. Denote by

Ak =

(µki , vik)

Nk =
i=1

(µki , viktki )

Nk ,
i=1

 Nk



uk = B  µki vik, ck

i=1

the sequences of active sets and iterates generated by Algorithm 1. Moreover set pk(·) =

· 0

K



F

(K

uk

)

ds.

Then

there

holds

pk

 C0(I; Rd)

as

wel l

as

pk, vik = (pk(tki ), vik)Rd = .

In particular, pk, uk = 

Nk i=1

µki

and

pk C   if Ak = . Moreover, if

pk C  

then uk is a minimizer to (P). In particular this holds if (µ, vk)  Ak for some µ > 0.

Proof. By step 2. and 7., respectively, of Algorithm 1 we have µki > 0. Moreover (µk, ck) is a minimizer to

   Nk



Nk 

min
(µ,c)RN+k ×Rd

F

K


i=1

µi vik ,

c

+



i=1

µi

.

It is readily verified that the first order necessary and sufficient optimality conditions for this problem imply

pk(T ) = 0, pk, vik = (pk(tki ), vik)Rd = , i = 1, . . . , Nk Consequently we get

(4.2)

as well as

Nk

Nk

pk, uk = µki pk, vik =  µki

i=1

i=1

 = pk, vik  pk C

for every (µki , vik)  Ak. Finally assume that pk C  . If pk C <  we note that Ak = , i.e. uk = 0, and uk satisfies the first order optimality conditions for (P), see Theorem 4. Hence, in this case, uk is a minimizer to (P). The same holds true

9

Algorithm 1 Primal-dual-active-jump method (PDAJ) for (P)

Input: Active set A0 = {(µ0i , vi0)}Ni=01, iterate u0 = B Output: Minimizer u¯ to (P).

N0 i=1

µ0i vi0

,

c0

1. Find (µ1/2, c1)  RN+0 × Rd by solving

   N0



N0 

min
(µ,c)RN+0 ×Rd

F

K


i=1

µivi0,

c

+



i=1

µi

.

2. Prune active set and update iterate:

A1 =

(µ1i , vi1)

N1
=
i=1

(µ1i /2, vi0) | µi1/2 > 0





N0

, u1 = B  µ1i vi1, c1 .

i=1

for k = 1, 2, . . . do

3. Compute pk  C0(I; Rd) and t^k  I with

·

pk = KF (Kuk)(s) ds,
0

|pk(t^k)|Rd =

pk

C

=

max
tI

|pk(t)|Rd .

if pk C   then

4. Terminate with u¯ = uk a minimizer to (P).

end if

5. Find (µk+1/2, ck+1)  RN+k+1 × Rd from (PAk ) for vk = (pk(t^k)/ pk C)t^k .

6. Update active set:

Ak+1/2 := (µkN+k1+/12, vk) 

(µki +1/2, vik)

Nk .
i=1

7. Prune active set and update iterate

Ak+1 :=

µki +1, vik+1

Nk+1 =
i=1

(µ, v)  Ak+1/2 | µ > 0

,

Nk+1



uk+1 = B 

µki +1vik+1, ck+1 .

i=1

and set k = k + 1. end for

if pk C =  and uk = 0. Last let pk C =  and uk = 0 hold. Then Ak = . Let (µki , viktki )  Ak be arbitrary. Summarizing the previous observations there holds
 = (pk(tki ), vik)Rd = |pk(tki )|Rd = |pk(tki )|Rd |vik|Rd = pk C

10

and thus vik = pk(tki )/. Thus we conclude that Ak is of the form Ak = {µki , pk(tki )/}Ni=k1

with pairwise disjoint positions tki . Consequently, uk = B

Nk i=1

µki

vik

,

ck

satisfies

Nk

Nk

uk M = µki |vik|Rd = µki .

i=1

i=1

Together with pk, uk =

Nk i=1

µki

we

finish

noting

that

uk

fulfils

the

sufficient

first

order

optimality conditions from Theorem 4. Finally, if (µ, vk)  Ak for some µ > 0 then we

have

 = pk, vk = pk C

and thus uk is again a minimizer of (P) following the previous observations.

5. Convergence analysis

This section addresses the convergence of Algorithm 1. The presentation is split into two parts. In Section 5.1 we provide the subsequential strict convergence of uk towards minimizers of (P) as well as a first convergence result for the residuals

rj(uk) := j(uk) - min j(u).
uBV(I ;Rd )

In the second part, Section 5.2, we prove that under additional structural assumptions

on the optimal dual variable p¯ =

· 0

KF (y¯)(s)

ds,

(P )

admits

a

unique

minimizer

u¯

and the iterates uk generated by Algorithm 1 satisfy

rj(uk) + uk - u¯ L1 + | uk M - u¯ M|  ck

(5.1)

for some   (0, 1) and all k  N large enough.

5.1. Global sublinear convergence. In the following let

Ak =

µki , vik

Nk
,
i=1

uk

 Nk =B

 µki vik, ck ,

yk = Kuk,

pk(·) =

i=1

·
K(Kyk)(s) ds
0

denote the active set, iterate, observation and dual variable in iteration k of Algorithm 1,

respectively. Since j is radially unbounded, see Assumption 1, the norm of all elements

in the sublevel set





Nk

Euk

=

 u



BV(I; Rd)

|

j(u)



F (Kuk)

+



µki  .



i=1 

is bounded by a constant Mk > 0. By construction there holds

Nk+1

Nk

F (Kuk+1) + 

µki +1  F (Kuk) +  µki .

i=1

i=1

Hence, w.l.o.g, we can assume that Mk is monotonically decreasing. For example, if F  0 on Y , we can choose



Nk 

Mk := F (Kuk) +  µki  /.

i=1

We require additional regularity assumptions on the loss functional F .

11

Assumption 2. The following two conditions hold: A1 The gradient F is Lipschitz i.e. there is L > 0 such that

F (y1) - F (y2) Y  L y1 - y2 Y y1, y2  Y.

A2 The functional F : Y  R is strongly convex around the optimal observation i.e. there exist a neighbourhood N (y¯) of y¯ in Y and 0 > 0 with

F (y)  F (y¯) + (F (y¯), y - y¯)Y

+ 0

y - y¯

2 Y

y  N (y¯).

This is, e.g., fulfilled for the quadratic loss function F (·) = (1/2)

· -yd

2 Y

with a

target observation yd  Y . Now define the auxiliary residual

Nk

rj (uk )

:=

F

(K uk )

+



i=1

µki

-

min
uBV(I ;Rd )

j(u).

Note that rj(uk)  rj(uk) holds due to

uk M =

Nk
µki vik

Nk
 µki

i=1

M i=1

using that vik M = 1. The following version of the classical descent lemma holds.

Lemma 8. Let uk  BV(I; Rd), pk  C0(I; Rd) and vk  M(I; Rd) be generated by Algorithm 1. Then we have

rj(uk+1) - rj(uk)  min
s[0,1]

-sMk ( pk

Ls2 C - ) + 2

K(uk - Mkvk, 0)

2 Y

(5.2)

for all k  1.

Proof. For every s  (0, 1) define the auxiliary iterate uk,s = B(uk,s, ck) where

Nk
uk,s = µks,Nk+1vk + µks,ivik,
i=1

where

µks := (1 - s)µk, sM0  RNk+1.

Since uk+1 is constructed using a minimizing pair to (PAk ) we have

Nk +1

Nk 

rj(uk+1) - rj(uk)  F (Kuk,s) - F (Kuk) +  

µks,i - µki  .

i=1

i=1

By construction, the second term on the righthandside is equal to

Nk +1

Nk 



Nk 



µks,i - µki  = s Mk - µki  .

i=1

i=1

i=1

Using a Taylor's expansion of the first term F (Kuk,s)-F (Kuk) and utilizing the Lipschitz continuity of F yields

F (Kuk,s) - F (Kuk)  s pk, uk - Mkvk

Ls2 +
2

K(uk - vk, 0)

2 Y

.

Finally note that due to vk = (pk(t^k)/ pk C)t^k with |pk(t^k)|Rd = pk C and Proposition 7 we have

 Nk



s pk, uk - Mkvk = s  µki - Mk pk C .

i=1

12

Summarizing all previous observations and minimizing w.r.t s  [0, 1] we arrive at the claimed inequality.

Using Lemma 8 we prove the subsequential convergence of uk towards minimizers of (P) as well as the sublinear convergence of rj(uk).
Theorem 9. Let uk  BV(I; Rd) and pk  C0(I; Rd) be generated by Algorithm 1. Then we have

rj(uk)  rj(uk)  Mk( pk C - )

(5.3)

Moreover Algorithm 1 either terminates after finitely many steps with uk a solution to (P) or we have

rj (uk )



rj (uk )



rj (u1 ) 1 + qk

where

1 q = min
2

1, 4

rj (uk )

K

2 BV,Y

M02

(5.4)

for all k  1. In this case, uk admits at least one strict accumulation point and each
such point is a solution to (P). Moreover we have Kuk  y¯ in Y as well as pk  p¯ in C0(I; Rd). If the minimizer u¯ to (P) is unique then uk s u¯ on the whole sequence.

Proof. Let u¯ denote an arbitrary minimizer of (P). Since F is convex we estimate

 Nk



rj(uk)  (-KF (Kuk), u¯ - uk)Y +   µki - u¯ M = pk, u¯ -  u¯ M.

i=1

Finally note that u¯  Euk and thus

pk, u¯ -  u¯ M  u¯ M( pk C - )  Mk( pk C - )

yielding (5.3).
Now assume that Algorithm 1 does not converge after finitely many steps. Then pk C  , see Proposition 7, and rj(uk) > 0 for all k. Explicitly calculating the minimum in (5.2), using (5.3) and dividing by rj(u1) we obtain

rj(uk+1)  rj(uk) - rj(u1) min

rj(u1) rj(u1)

2

1

1

, rj(uk) L

K(uk - Mkvk, 0)

2 Y

rj (uk ) rj (u1 )

 rj(uk) - 1 min rj(u1) 2

1, 4

rj (uk )

K

2 BV,Y

Mk2

rj (uk ) rj (u1 )

 rj(uk) - 1 min rj(u1) 2

1, 4

rj (uk )

K

2 BV,Y

M02

rj(uk) . rj (u1 )

Invoking [11, Lemma 3.1] yields (5.4). Since j is radially unbounded, see Assumption 2, and rj(uk)  0, we conclude that uk is bounded in BV(I; Rd). Thus it admits at least one weak* convergent subsequence, denoted by the same index, with limit u¯  BV(I; Rd) i.e. uk  u¯ in L1(I; Rd) and uk  u¯ in M(I; Rd). Since BV(I; Rd) c L2(I; Rd) we also conclude yk  Ku¯ in Y as well as
·
pk  KF (Ku¯)(s) ds in C0(I; Rd).
0
Finally we note that j is weak* lower semicontinuous on BV(I; Rd). Consequently rj(u¯) =
0 and u¯ is a minimizer to (P). Finally, since F (Kuk)  F (Ku¯), we also get uk M  u¯ M yielding the strict convergence of uk towards u¯. Thus we have shown that any
weak* accumulation point of uk is indeed a strict accumulation point and a minimizer

13

of (P). Recalling that the optimal observation y¯ as well as the optimal dual variable p¯ are unique we conclude yk  y¯ in Y and pk  p¯ in C0(I; Rd) for the whole sequence. If u¯ is the unique minimizer of (P) then it is also the unique strict accumulation point of uk and thus uk s u¯ on the whole sequence.

If F is strongly convex around y¯, see Assumption 2 A2, then the convergence guarantee for the residual from Theorem 9 also carries over to the observations and dual variables.

Proposition 10. Let Assumption 2 hold. Then we have yk - y¯ Y + pk - p¯ C + | pk C - p¯ C|  crj(uk)1/2
for all k  N large enough.

Proof. Let N (y¯) denote the neighbourhood from Assumption 2 A2. Since yk  y¯ in Y ,see Theorem 9, there holds yk  N (y¯) for all k  N large enough. Consequently Assumption 2 A2 yields

rj(uk)  (F (y¯), yk - y¯)Y + ( uk

M-

u¯

M) + 0

yk - y¯

2 Y

.

Finally noting that

(F (Ku¯), yk - y¯)Y + ( uk M - u¯ M) = p¯, u¯ - uk + ( uk M - u¯ M)  0, see Theorem 4, we get
yk - y¯ Y  (1/0)1/2 rj(uk)1/2.
The remaining estimates follow from | pk C - p¯ C|  pk - p¯ C  c K(F (yk) - F (y¯)) L2  c K Y,L2 F (yk) - F (y¯) Y  cL K Y,L2 yk - y¯ Y .

5.2. Local linear convergence. Next we prove that Algorithm 1 converges linearly
provided that additional structural requirements on the optimal dual variable p¯ hold. First we assume that p¯ only admits a finite number N of global extrema {t¯i}Ni=1. Together with a linear independence assumption on {p¯(t¯i)}Ni=1 this ensures the existence of a unique, piecewise constant minimizer to (P).

Assumption 3. Recall the definition of the optimal dual variable p¯ =

· 0

K  F

(y¯)

ds.

Assume that there is N  N and {t¯i}Ni=1  I with

{t¯i}Ni=1 = { t  I | |p¯(t)|Rd = p¯ C =  } .

(5.5)

Moreover let {ei}Ni=1  Rd denote the canonical basis of Rd. The set

{Kp¯(t¯i)t¯i }Ni=1  {KeiI }di=1  Y

(5.6)

is linearly independent.

Corollary 11. Let Assumption 3 hold. Then the minimizer u¯ = B(u¯ , au¯) to (P) is unique and u¯ is given by

N

N

u¯ = µ¯iv¯i = µ¯iv¯it¯i0

i=1

i=1

for all i = 1, . . . , N .

where

µ¯i  0,

v¯i =

p¯(t¯i) 

14

Proof. Introduce the linear and continuous operator K : RN × Rd  Y by

N
K(µ, C) = K (C0) + K (p¯(t¯i)/)t¯i
i=1

µ  RN , C  Rd.

Then K is injective according to (5.6). According to Corollary 5 and (5.5) every minimizer u¯ of (P) is of the form

u¯ = B

(p¯(t¯i)/)t¯i , au¯

i=1

where µ¯  RN+ and C¯ is implicitly given by

N
= C¯0 + µ¯it¯i
i=1

C¯

=

au¯

-

1 T

N
µ¯i
i=1

(p¯(t¯i)/)) (T

-

t¯i),

see the definition of the operator B, (3.1), and its inverse B-1, respectively. Due to the optimality of u¯ for (P) we readily verify that (µ¯, C¯) is a minimizing pair for

N

min F (K(µ, C)) +  µi .

µRN+ ,CRd

i=1

(5.7)

The proof is finished noting that (5.7) admits a unique minimizer since F  K is strictly convex.

According to Assumption 3 and the continuity of |p¯|Rd there is  > 0 as well as a radius R > 0 such that the intervals (t¯i - R, t¯i + R)  I, i = 1, . . . , N , are pairwise disjoint and

N
|p¯(t)|Rd   -  t  I¯ \ (t¯i - R, t¯i + R).
i=1

(5.8)

Now we impose a final set of assumptions which requires the positivity of µ¯i as well as
the quadratic growth of |p¯|Rd around its global maximizers. From the perspective of optimization, this first condition corresponds to a strict complementarity condition and the second one is equivalent to a second-order-sufficient-condition (SSC) for t¯i.

Assumption 4. For all i = 1, . . . , N , there holds µ¯i > 0 as well as 0|t - t¯i|2   - |p¯(t)|Rd t  (t¯i - R, t¯i + R)

where R > 0 denotes the radius from (5.8). Moreover K  L(Y ; L(I; Rd)).

Remark 1. Define the scalar-valued function P¯(t) = |p¯(t)|Rd and assume that p¯  C2(I; Rd). Then it is readily verified that P¯ is also at least two times continuously differentiable on (t¯i - R, t¯ + R) if R > 0 is chosen small enough. In particular this implies P¯(t¯i) = , P¯ (t¯i) = 0 and P¯ (t¯i)  0. Thus, by potentially choosing R > 0 even smaller as well as Taylor approximation of P¯ we arrive at

P¯(t)

=



-

|P¯

(t¯)| |t
4

-

t¯i|2

for all t  (t¯i - R, t¯i + R). Hence the quadratic growth condition of Assumption 4 is fulfilled if P¯ (t¯i) = 0, i = 1, . . . , N .

15

The following quadratic growth behaviour of the linear functional induced by p¯ is a direct consequence.

Lemma 12. Let Assumption 4 hold. Then there is 1 > 0 such that

1 |t - t¯i|2 + |v - v¯i|2Rd   - p¯, vt and all i = 1, . . . , N .

t  (t¯i - R, t¯i + R), |v|Rd = 1

Proof. Fix i = 1, . . . , N and t  (t¯i - R, t¯i + R) as well as v  Rd with |v|Rd = 1. From Assumption 4 and |v|Rd = 1 we immediately get

 - p¯, vt   - |p¯(t)|Rd  0|t - t¯i|.

Second we estimate

 (1 -

p¯/, vt

) =  (1 - (p¯(t)/, v)Rd) 

 2

|p¯(t)/

-

v|2Rd .

using |p¯(t)/|Rd  1. Finally we have |p¯(t) - p¯(t¯i)|Rd  |t - t¯i| KF (Ku¯) L(I;Rn).

(5.9)

The claimed statement now follows from noting that

|t - t¯i|2 + |v - v¯i|2Rd  |t - t¯i|2 + 2 |v - p¯(t)/|2Rd + |(p¯(t) - p¯(t¯i))/|2Rd where v¯i = p¯(t¯i)/ is used in the first inequality.

Moreover we deduce the following Lipschitz property of K.

Lemma 13. There holds
K(v1t1 - v2t2 , 0) Y  c (|t1 - t2| + |v1 - v2|Rd ) for all t1, t2  I, v1, v2  Rd, |v1|Rd = |v2|Rd = 1. Proof. Using the additional regularity of K from Assumption 4 we get

K(v1t1 - v2t2 , 0) Y = sup (K(v1t1 - v2t2 , 0), y)Y
y Y =1
= sup (B(v1t1 - v2t2 , 0), Ky)L2
y Y =1
 Ky L B(v1t1 - v2t2 , 0) L1  K Y,L B(v1t1 - v2t2 , 0) L1 .
Now recall that 1
B(viti , 0) = viti - T vi(T - ti),
i = 1, 2, and thus

B(v1t1 - v2t2 , 0) L1(I)  v1t1 - v2t2 L1 + |v1(T - t1) - v2(T - t2)|Rd . The proof is finished noting that

as well as

v1t1 - v2t2 L1  T |v1 - v2|Rd + |v1|Rd t1 - t2 L1  |t1 - t2| + T |v1 - v2|Rd

(5.10) (5.11)

|v1(T - t1) - v2(T - t2)|Rd  |v1|Rd |t1 - t2| + |T - t2||v1 - v2|Rd  |t1 - t2| + T |v1 - v2|Rd .

16

Sketch of the proof. The following theorem summarizes the main results of the following sections.

Theorem 14. Let uk be generated by Algorithm 1 and let Assumption 1-4 hold. Then Algorithm 1 either terminates after finitely many steps with uk = u¯ or there is   (0, 1) such that
rj(uk) + uk - u¯ L1 + | uk M - u¯ M|  ck
for all k  N large enough.

Since the proof of this improved convergence behaviour is rather technical we give a
short outline before going into detail. Utilizing the strict convergence of uk towards u¯
as well as the isolation of the global extrema of p¯ we conclude that the iterate uk only jumps in the vicinity of {t¯i}Ni=1. More in detail, for sufficiently large k, these observations yield a partition of {1, . . . , Nk} into nonempty, pairwise disjoint sets Aik, i = 1, . . . , N , such that

(µki , vjktkj )  Ak, j  Aik  tkj  (t¯i - R, t¯i + R).

Moreover the "closedness" of the jumps vjk, j  Aik, and the optimal one v¯i, i.e. the distance between the positions tkj and t¯i as well as the misfit between the associated directions vik - v¯i, can be quantified in terms of the auxiliary residual rj(uk), see Lemma 19. Similarly, in Proposition 20, we show that the new candidate jump vk, see
step 5. in Algorithm 1, lies in the vicinity of some t¯i  {t¯i}Ni=1. Finally, as in the proof of Lemma 8, we then rely on an auxiliary iterate uk,s = B(uk,s, ck), s  (0, 1), where





uk,s

=

(1

-

s)
jAik

µkj vjk

+

s

 



jAik

µkj

 



pk (t^k ) pk C

t^k

+

N i=1, jAik i=i

µkj vjk

The descent properties of this auxiliary iterate are then exploited in Lemma 22 to prove an improved version of the descent lemma, Lemma 8, which finally yields the linear convergence of rj(uk). The linear convergence of uk w.r.t to the strict topology is then concluded as a by-product, see Lemmas 24 and 25.

Remark 2. To finish this section let us briefly compare uk,s with the auxiliary iterate uk,s = B(uk,s, ck), where

Nk
uk,s = sM0vk + (1 - s)

µki vik = (1 - s)uk + sM0vk

i=1 jAik

which is used in the proof of Lemma 8. Loosely speaking, to obtain uk,s we take "mass" from all Dirac Delta functionals in uk, i.e. the height of all jumps in the iterate is decreased, and move it to the new candidate jump vk. In contrast, the construction
of uk,s can be viewed as a local update of uk since mass is only taken away from those jumps ukj supported in (t¯i - R, t¯i + R). On the complement, I \ (t¯i - R, t¯i + R), we have uk,s = uk. This allows for a refined analysis of the descent achieved by Algorithm 1 in each iteration.

17

Linear convergence of the residual. For the sake of readability we tacitly assume that Algorithm 1 does not converge after finitely many steps. The following proposition summarizes some immediate consequences of this assumption.

Proposition 15. Assume that Algorithm 1 does not terminate after finitely many steps. Then there holds uk s u¯, pk C   and Ak =  for all k  N large enough.
Proof. Since the minimizer to (P) is unique, see Corollary 11, we get uk s u¯ from Theorem 9. In particular, this implies uk  u¯ in M(I; Rd) and thus uk = 0 for k large enough. This also yields Ak =  and pk C  , see Proposition 7.

Now we use the isolation of the global extrema of p¯, see (5.8), as well as the uniform
convergence of pk from Proposition 10 to conclude that pk is small outside of the intervals (t¯i - R, t¯i + R).

Corollary 16. Let  > 0 and R > 0 as in (5.8) be given. Moreover let pk be generated by Algorithm 1. For all k  N large enough we have

 |pk(t)|Rd   - 2

N
t  I¯ \ (t¯i - R, t¯i + R).

i=1

Proof. Choose an arbitrary but fixed t  I¯ \

N i=1

(t¯i

- R, t¯i

+ R).

We

estimate

 |pk(t)|Rd  |p¯(t)|Rd + ||pk(t)|Rd - |p¯(t)|Rd|   -  + pk - p¯ C   - 2

for all k  N large enough. Here we use (5.8) in the second inequality and the uniform convergence of pk, see Proposition 10, in the last one.

Using this estimate we prove that the iterate uk solely jumps in the vicinity of the optimal jump positions t¯i.

Proposition 17. Denote by

Ak =

(µki , vik)

Nk =
i=1

(µki , viktki )

Nk i=1

the sequence of active sets generated by Algorithm 1. For all k  N large enough there exist

pairwise disjoint index sets Aik with

N i=1

Aik

= {1, . . . , Nk}

and

tkj

 (t¯i - R, t¯i + R),

j



Aik .

Proof. Let (µkj , vjk) = (µkj , vjktkj )  Ak be arbitrary. Utilizing the first order optimality condition for the subproblem (PAk ), see Proposition 7, we have
 = (pk(tkj ), vjk)Rd  |pk(tkj )|Rd .

Thus, together with Corollary 16, we conclude tkj  (t¯i - R, t¯i + R) for exactly one i  {1, . . . , N }. The existence of the index sets Aik is now imminent.

Next we prove that the sets Aik are nonempty for large k  N. This means that
each optimal jump v¯i is approximated by at least one jump in the iterate uk. Moreover the "lumped" height jAik µkj of all jumps vjk, j  Aik, converges to the optimal jump height µ¯i. For this purpose define the restricted measures

Uk,i :=

µkj vjk.

jAik

(5.12)

18
Lemma 18. Let Uk,i be defined as in (5.12). Then there holds

Uk,i  µ¯iv¯it¯i ,

µkj  µ¯i.

jAik

In particular this implies Aik =  for all k  N and

Nk i=1

µki



u¯

M.

Proof. Let i = 1, . . . , N be arbitrary but fixed and let   C0(I) be such that (t) = 1, t  (t¯i - R, t¯i + R), as well as (t) = 0, t  (t¯j - R, t¯j + R), j = i. Moreover denote
by   C0(I; Rd) an arbitrary test function. Then we have   C0(I; Rd) and thus

, Uk,i = , uk  , u¯ = , µ¯iu¯i due to uk  u¯, see Theorem 9. Consequently Uk,i  µ¯iu¯i. Similarly we conclude



µkj = pk, uk = p¯, u¯ = µ¯i

jAik

using the first order optimality conditions for uk and u¯, see Proposition 7 and Theorem 4, respectively, as well as pk  p¯ in C0(I; Rd), see Proposition 10. Thus Aik =  for all k  N
large enough. The last statement now follows due to

Nk

N

µki =

µkj .

i=1

i=1 jAik

Up to now we have only given qualitative statements on the approximation of v¯i by jumps vjk of the iterate uk. In order to improve on the convergence result of Theorem 9 we also need a quantitative estimate for this observation. For this purpose we recall that both, v¯i and vjk, are vector-valued Dirac Delta functionals. Thus, a suitable way to compare these jumps is given in terms of the differences tkj - t¯i and vjk - v¯i of jump positions and directions, respectively. This can be quantified using the quadratic growth
behaviour of p¯ from Lemma 12.

Lemma 19. There holds

N
µkj |tkj - t¯i| + |vjk - v¯i|Rd  c rj(uk)
i=1 jAik

(5.13)

for all k  N large enough.

19

Proof. Let 1 denote the constant from Lemma 12. Applying Jensen's inequality yields



2

2

1
Nk i=1

µki

N
 
i=1

jAik

µkj

|tkj - t¯i| + |vjk - v¯i|Rd

 

 1 N 2

µkj |tkj - t¯i| + |vjk - v¯i|Rd 2

i=1 jAik

N

 1

µkj |tkj - t¯i|2 + |vjk - v¯i|2Rd

i=1 jAik

N



µkj  - p¯, vjk

i=1 jAik

Nk
=  µki - p¯, uk .
i=1

Moreover, due to the convexity of F we estimate

Nk
rj(uk) = F (Kuk) +  µki - F (Ku¯) -  u¯ M
i=1
Nk
  µki -  u¯ M + (F (Ku¯), Ku¯k - Ku¯)Y .
i=1
Now we rewrite

(F (Ku¯), Ku¯k - Ku¯)Y -  u¯ M = p¯, u¯ - uk -  u¯ M = - p¯, uk .

using the first order optimality conditions for u¯, see Theorem 4. Summarizing all previous observations we arrive at



N

 

µkj

i=1 jAik

|tkj - t¯i| + |vjk - u¯i|Rd

2 2
 

Nk i=1

µki

1

rj(uk) 

4

u¯ M 1

rj (uk ).

Taking the square root on both sides of the inequality yields the claimed statement.

A similar estimate holds for the new candidate jump vk computed in step 3. of Algorithm 1.

Proposition 20. Let vk = (pk(t^k)/ pk C)t¯k with |pk(t^k)|Rd = pk C be given. For all k  N large enough there is a k-dependent index i  {1, . . . , N } such that t^k  Aik and

|t^k - t¯i| + |pk(t^k)/ pk C - v¯i|Rd  c rj(uk)

(5.14)

Proof. According to Proposition 10 there holds pk C  . Thus we conclude t^k  (t¯i - R, t¯i + R) for some i  {1, . . . , N } from Corollary 16. Applying Lemma 12 we get

1 2

|t^k - t¯i| + |pk(t^k)/

pk

C - v¯i|Rd

2
 1

|t^k - t¯i|2 + |pk(t^k)/ pk C - v¯i|2Rd

  - p¯, vk .

Next note that  - p¯, vk = p¯, u¯i - vk  p¯ - pk, u¯i - vk = (F (y¯) - F (yk), K(u¯i - vk, 0))Y

20

since

pk C = pk, vk  pk, u¯i . Utilizing Proposition 10 and Lemma 13 we finally arrive at

1 2

|t^k - t¯i| + |pk(t^k)/

pk

C - v¯i|Rd

2


F (y¯) - F (yk) Y

K(u¯i - vk, 0) Y

 c rj(uk) |t^k - t¯i| + |pk(t^k)/ pk C - v¯i|Rd .

Now fix k  N large enough and let i  {1, . . . , N } be the index from Proposition 20. Further recall the index sets Aik, i = 1, . . . , N , from Proposition 17. For every s  [0, 1] define the locally lumped measure





uk,s

=

(1

-

s)
jAik

µkj vjk

+

s

 



jAik

µkj

 



pk (t^k ) pk C

t^k

+

N i=1, jAik i=i

µkj vjk

=

µks,Nk +1 v k

+

Nk
µks,j vjk
j=1

where µks  RNk+1 is defined as





µks,j

= µkj ,

j  Aik,

i = i,

µks,j

= (1 - s)µkj ,

j  Aik,

µks,Nk +1

=

s

 



µkj  . (5.15) 

jAik

Set uk,s = B(uk,s, ck). By construction, there holds

Nk +1

Nk 

rj(uk+1) - rj(uk)  F (Kuk,s) - F (Kuk) +  

µks,j - µkj  .

j=1

j=1

The following properties of uk,s follow directly.

Lemma 21. Let uk and pk be generated by Algorithm 1. Moreover let uk,s be defined as above. Then there holds





pk, uk - uk,s

=

-s

 



µkj

 

(

pk



jAik

C - ),

Nk +1

Nk

µks,j = µkj .

j=1

j=1

Proof. Note that





and thus

uk - uk,s = s

µkj vjk

-

s

 



µkj

 

vk



jAik

jAik













pk, uk - uk,s

=

s

 



µkj pk, vjk

-

 



µkj

 

pk, vk

 

=

-s

 







µkj

 

(

pk

C - )



jAik

jAik

jAik

using that pk, vjk = , see, and pk, vk = pk C. The statement on

Nk +1 j=1

µks,j

is

imminent.

As a final step we now use uk,s to prove a refined descent estimate for Algorithm 1.

21

Lemma 22. For all k  N large enough there holds

rj(uk+1) - rj(uk)  min s2c1 - s min µ¯i/2M0

s[0,1]

i=1,...,N

for some c1 > 0 independent of k and s.

rj (uk )

(5.16)

Proof. Let s  [0, 1] be arbitrary but fixed. We estimate

Nk +1

Nk 

rj(uk+1) - rj(uk)  F (Kuk,s) - F (Kuk) +  

µks,j - µkj  = F (Kuk,s) - F (Kuk)

j=1

j=1

where the last equality holds due to Lemma 21. As in the proof of Lemma 8 we now find

F (Kuk,s) - F (Kuk) 

pk, uk - uk,s

L +
2

K(uk - uk,s)

2 Y

(5.17)

where L > 0 denotes the Lipschitz constant of F . Summarizing the previous observa-

tions and again utilizing Lemma 21 we thus get





rj (uk+1)

-

rj

(uk)



-s

 



µkj

 

(



pk

L C - ) + 2

K(uk - uk,s)

2 Y

.

jAik

Next we use (5.3) as well as jAik µkj  µ¯i, i = 1, . . . , N , see Lemma 18, to establish the upper bound





-s

 



µkj

 

(

pk

C - )  -s



min µ¯i/2M0
i=1,...,N

rj (uk ).

jAik

Finally it remains to estimate the difference of the observations associated to uk,s and uk, respectively. For this purpose we note

K(uk - uk,s) Y  s

µkj K(vjk - vk, 0) Y

jAik

s

µkj K(vjk - u¯i, 0) Y + K(u¯i - vk, 0) Y

jAik

s

µkj |tkj - t¯i| + |vjk - v¯i|Rd + |t^k - t¯i| + |pk(t^k)/ pk C - v¯i|Rd

jAik







sc

1

+

 

µkj

 

rj (uk )





jAik

where Lemma 13 is used in the third inequality and Lemma 19 as well as Lemma 20 in the final one. Again pointing out that jAik µkj is uniformly bounded independently of i and k  N, see Lemma 18, we finally arrive at

K(uk - uk,s)

2 Y

 s2 c rj(uk)

and thus

rj(uk+1) - rj(uk)  s2c - s min µ¯i/2M0 rj(uk).
i=1,...,N

22

Minimizing both sides w.r.t s  [0, 1] yields the desired result.

Using this improved descent estimate we prove the linear convergence of the auxiliary residual rj(uk). Theorem 23. Let Assumptions 1-4 hold. Then there is   (0, 1) such that
rj(uk)  rj(uk)  ck for all k  N large enough. Proof. According to Lemma 22 there is K  N such that
rj(uk+1)  min 1 + s2c1 - sc2 rj(uk) k  K
s[0,1]
where we set

c2 := min µ¯i/2M0
i=1,...,N

for abbreviation. Explicitly calculating the minimum reveals

min
s[0,1]

1 + s2c1 - sc2

  := 1 - c2 min 2

1, c2 2c1

and thus

rj(uk)  rj(uk+1)  k-K rj(uK )

for all k  K.

Linear convergence of the iterates. In this last subsection we aim to quantify the strict convergence of uk towards u¯. More in detail we utilize Theorem 23 to prove

uk - u¯ L1 + | uk M - u¯ M|  c2k

(5.18)

for some 2  (0, 1) and all k  N large enough. For this purpose we rely on the following auxiliary estimates.

Lemma 24. For all k  N large enough there holds

N

| uk M - u¯ M|  c rj(uk) +

µkj - µ¯i

i=1 jAik

Proof. Recall the definition of the restricted measures Uk,i from (5.12). Then there holds

N
| uk M - u¯ M| 
i=1

Uk,i M -

µkj +

µkj - µ¯i

jAik

jAik

Now, fix an arbitrary i  {1, . . . , N }. Given two indices j1, j2  Aik we note that

µkj1 ukj1 + µkj2 ukj2 M = µkj1 + µkj2

if tkj1 = tkj2 and

µkj1 ukj1 + µkj2 ukj2 M = µkj1 ujk1 + µkj2 ujk2 Rd

23
if tkj1 = tkj2. Similarly we conclude the existence of a partition of Aik into pairwise disjoint, nonempty sets Ikh, h = 1, . . . , nk, with

Uk,i

M-

µkj

jAik

nk
=
h=1

µkj vjk Rd -

µkj |v¯i|Rd

jIkh

jIkh

nk

h=1

µkj vjk Rd -

µkj |v¯i|Rd

jIkh

jIkh

nk


h=1

jIkh µkj (vjk - v¯i) Rd

nk



µkj vjk - v¯i Rd

h=1 jIkh

 c rj(uk)

where we use the inverse triangle inequality in the second inequality and

nk

µkj vjk - v¯i Rd =

µkj vjk - v¯i Rd

h=1 jIkh

jAik

as well as Lemma 19 in the final inequality. Summarizing all previous observations and noting that the index i was chosen arbitrarily finishes the proof.

A similar estimate holds for the L1 distance of the iterates to the minimizer u¯.

Lemma 25. Define constants

C¯ = - 1 T T0

s
du¯
0

ds + au¯,

Ck = - 1 T

T 0

s
duk ds + auk .
0

For all k  N large enough there holds

N

uk - u¯ L1  c rj(uk) + T |Ck - C¯| +

µki - µ¯i

i=1 jAik

Proof. According to the definition of the B operator, (3.1), we have

(5.19)

and thus

N

N

u¯ = C¯ + µ¯iv¯it¯i , uk = Ck +

µkj vjktkj

i=1

i=1 jAik

N

uk - u¯ L1  T |Ck - C¯| +

µkj vjktkj - µ¯iv¯it¯i .

i=1 jAik

L1

24
Now fix an arbitrary index i  {1, . . . , N }. We estimate

µkj vjktkj - µ¯iv¯it¯i

T

µki - µ¯i +

µkj (vjktkj - v¯it¯i )

jAik

L1

jAik

jAik

L1

T

µki - µ¯i +

µkj vjktkj - v¯it¯i L1

jAik

jAik

using that v¯it¯i L1(I;Rd)  T . Moreover, from (5.10) and (5.13), we conclude

µkj vjktkj - v¯it¯i L1 

µkj |tkj - t¯i| + T |vjk - v¯i|Rd  c rj(uk).

jAik

jAik

Summarizing all previous observations yields the desired estimate.

Thus to prove (5.18) it suffices to quantify the error Ck - C¯ as well as the difference between jAik µkj and µ¯i. This is done in the following proposition.
Proposition 26. For all k  N large enough there holds

|C¯ - Ck| +

µki - µ¯i  c rj(uk).

jAik

Proof. Define u~k = Ck +

N i=1

jAik µkj v¯it¯i as well as the vector of lumped coeffi-

cients µ~k  RN , µ~ki = jAik µkj . Recall the definition of the injective operator K from

the proof of Corollary 11. Then K(µ~k - µ¯, Ck - C¯) = K(uk - u¯) and thus

|C¯ - Ck| +

µki - µ¯i  c K(u~k - u¯) Y .

jAik

Applying Proposition 10 yields

K(u~k - u¯) Y  K(uk - u¯) Y + K(u~k - uk) Y  rj(uk)/0 + K(u~k - uk) Y .

Finally we estimate

N
K(u~k - uk) Y 

µkj K(vjk - u¯i, 0) Y

i=1 jAik

N

c

µkj |tkj - t¯i| + |vjk - v¯i|Rd

i=1 jAik

 c rj(uk)

using Lemma 13 in the second inequality and Lemma 19 in the final one.

Combining the previous results we are in the position to prove linear convergence of uk with respect to the strict topology on BV(I; Rd).
Theorem 27. Let Assumptions 2, 3, 4 hold. Then we have uk - u¯ L1 + | uk M - u¯ M|  c2k.
for some 2  (0, 1) and all k  N large enough.

25
Proof. The statement directly follows from Lemma 24 and Lemma 25 taking Proposition 26 into account.

6. Numerical examples
The last section is devoted to the numerical illustration of our theoretical results. For this purpose two examples are discussed. First we address the inverse problem of identifying a piecewise constant signal from finitely many data samples. The forward operator K is modelled by convolution with a Gaussian kernel. Second we consider an optimal control problem for the linear wave equation. Here the control enters as the time-dependent signals of two spatially fixed actuators. In this case, the fidelity term is given by the L2-misfit between the solution to the wave equation and a desired state yd over the whole space-time cylinder.

6.1. Deconvolution from finitely many measurements. As a first example consider

min j(u) :=
uBV(I )

1 2

9
(k(i)
i=1



u

-

ydi )2

+



u

M

(6.1)

where I = (0, 1), yd  R9 is a given finite dimensional data vector and

1

k(i)



u

=

 2

1

u(t)

e-

(t-i ) 22

2

dt,

i = i · 0.1,

0

i = 1, . . . , 9.

The deconvolution problem (6.1) can be embedded in the general setting (P) by choosing

Y

= R9,

F (·) =

1 2

9
((·)i
i=1

-

yi)2,

(K u)i

= k(i)  u.

In this case the K = K  B operator is given by

K(q, c)i = (k(i), B(q, c))L2 = - i, q + i(1)(c + t, q ), i = 1, . . . , 9,

for i defined as

1 i(t) = 2

erf

t- i 2

+ erf

i 2

where erf(·) denotes the error function. Moreover we readily verify K : Rm  C2(I) and

·

m

pk(·) = KF (Kuk) = i(·)(k(i)  uk - yi).

0

i=1

In order to determine a global extremum of pk, see step 3., we find solutions of pk(t) = 0 using a Newton method starting at equally spaced points ti0 = i·0.1, i = 1, . . . , 9. Then t^k is chosen from the set of computed solutions by comparing the corresponding function
values. The solution of the finite dimensional subproblems relies on a semismooth
Newton method for the "normal map" reformulation of its first order sufficient optimality
conditions, see e.g. [24]. In each iteration the method is warmstarted using the current magnitudes µki and the mean value ck to construct a good starting point. Moreover we further enhance its practical performance by incorporating a heuristic globalization

26
strategy based on damped Newton steps. Finally Algorithm 1 is stopped if the upper bound
#Ak
k := Mk( pk C - ), Mk = F (Kuk) +  µki
i=1
on the residual rj(uk), see Theorem 9, is smaller than 10-13.
6.1.1. Structural assumptions on p¯. We solved (6.1) for   10-5 and observations yd = Ku+ where u  BV(I) and   R9 is a random perturbation. The ground truth u and the computed minimizer u¯ are depicted in Figure 1a. Before addressing the performance of Algorithm 1 we numerically verify Assumptions 3 and 4. For this purpose we plot the dual variable p¯ as well as its second derivative p¯ in Figures 1b and 1c. The functional values corresponding to the jumps of u¯ are marked by red crosses.

2.8 2.6 2.4 2.2
2 1.8 1.6 1.4 1.2
1 0.8
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1

10-5 1.5

10-3 2

1

1.5

0.5

1

0

0.5

-0.5

0

-1

-0.5

-1.5 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1

-1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1

(a) Ground truth u and u¯.

(b) Dual variable p¯.

(c) Second derivative p¯ .

Figure 1. Ground truth, reconstruction and dual variable.

First we point out that p¯ C =  and p¯ achieves its global maximum/minimum in three distinct points {t¯i}3i=1 which coincide with the jumps of u¯. In particular, the optimal magnitudes satisfy µ¯i > 0. Moreover the operator K from the proof of Corollary 11 has full rank which is equivalent to the linear independence of (5.6). Second, there holds p¯ (t¯i) = 0. Hence, see Remark 1, the quadratic growth condition of Assumption 4 holds.
6.1.2. Practical performance of Algorithm 1. In order to assess the performance of Algorithm 1 we plot the residuals rj(uk) alongside the sublinear convergence rate from Theorem 9 as well as a linear rate with  = 0.33 in Figure 2a. Next to it, in Figure 2b, we report on the convergence of the iterates uk in L1(I) and the norms uk M. As predicted by Theorems 23 and 27 all considered quantities converge at least linearly. Moreover we plot the evolution of the size of the active set in Figure 2c. Note that #Ak is not strictly increasing. This is testament to the efficiency of the pruning step 7. of Algorithm 1 in combination with the full resolution of the subproblem (PAk ) in step 5. Finally we compare Algorithm 1 to the Fast iterative shrinkage-thresholding algorithm (FISTA) from [2, 7]. However, in contrast to our proposed method, its practical application to (6.1) requires a discretization of the interval (0, 1). For this purpose we consider a uniform partition of [0, 1] into subintervals [ti, ti+1], i = 1, . . . , Nh - 1, where t0 = 0 and ti = ti-1 + h, else, with h = 1/(Nh - 1). Subsequently we replace BV(I) in (6.1) by

27

10-5

10-7

10-9

10-11

10-13

10-15 10-17

rj (uk ) O(1/k) O(0.33k)

1 2 3 4 5 6 7 8 9 10 k

(a) Residual error over k.

102

4

101

uk - u¯ L1(I) | uk M - u¯ M|

100

O(0.64k)

#Ak

10-1

3

10-2

10-3

10-4

2

10-5

10-6

10-7

1

10-8

10-9

11 10-10 1

2

3

4

5

6

7

8

0 9 10 11 1 2 3 4 5 6 7 8 9 10 11 12

k

k

(b) Norm/L1-error over k.

(c) Active set size over k.

Figure 2. Convergence behaviour of relevant quantities.

the finite-dimensional subspace

BVh (I )

=

 

u



BV(I )

|

u

=

B

Nh-1 

µhi thi ,

 c

,

µ  RNh-1,


 cR



i=1



and apply FISTA with constant stepsize as described in [2]. Additionally we also use

this comparison to study the behaviour of Algorithm 1 under perturbations and apply it

to the discretized problem. In this context, we restrict the search for the new candidate jump position t^k in step 3. of Algorithm 1 to the set of nodes of the partition. More in detail we choose t^k  {thi }Ni=h1-1 such that

|pk(t^k)|

=

max
i=1,...,Nh-1

|pk(thi )|.

The other steps of the method remain the same. In Figure 3a we plot the behaviour of

2 0 -2 -4 -6 -8 -10 -12 -14 -16 -18
0

5 10 15 20 25 30 35 40 45 50

2 0 -2 -4 -6 -8 -10 -12 -14 -16 -18
0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2

(a) Residual vs. iter. number.

(b) Residual vs. comp. time in seconds.

Figure 3. Comparison of Algorithm 1 and FISTA on different grids.

the residual rj(uk) = j(uk) - minuBVh(I) j(u) for FISTA and our method with different grid widths h = 10-2, 10-3, 10-1. Additionally we also include Algorithm 1 without
discretization in the plot. This is formally denoted by "h = 0". In both methods, the
same starting point u0 is used. We observe that Algorithm 1 solves the problem on each refinement level in a few iterations while the convergence of FISTA significantly slows
down after the first iterations. Moreover Algorithm 1 exhibits strong mesh-independence

28

i.e. its convergence is stable w.r.t. to h and is essentially governed by its behaviour on the continuous problem. In contrast, the convergence behaviour of FISTA degenerates as h gets smaller. Let us however point out that the per iteration cost of both algorithms is wildly different. In fact, the practical realization of FISTA only requires the computation of one proximal operator per iteration, which can be done analytically, while Algorithm 1 relies on determining a global extremum of pk as well as the full resolution of (PAk ). To respect the different cost per iteration of both methods we also give a comparison in terms of the computational time in Figure 3b. For this purpose we plot the convergence history of Algorithm 1 (up to optimality) and of FISTA (first 200 iterations) as a function of time. We observe that the more complicated subproblems in Algorithm 1 do not lead to highly increased computational times. This is, on the one hand, a consequence of the use of an efficient second order optimization scheme for (PAk ) in combination with a warmstart. On the other hand this is also attributed to the observation that the active set size #Ak, and thus the dimension of the subproblems (PAk ), is essentially independent of the underlying discretization. We omit an additional plot showcasing the convergence of #Ak on the different discretization levels since the resulting curves align themselves with the plot in Figure 2c.

6.2. Optimal control of the wave equation. In this section we apply the proposed method for the solution of a PDE-constraint optimization problem of the form

1

min

J(u) =

uBV (I;R2),yL2(I×)

2

y - yd

2 L2 (I × )

+



u

M(I ;R2 )

(6.2)

where the vector-valued control u is connected to the state variable y by a linear wave equation of the form

tty 

-

y

=

u1(t)x1 (x)

+

u2(t)x2 (x)



y = 0

  y(0) = 0, ty(0) = 0,

in I × , on (0, T ) × , on .

(6.3)

with x1 = (0.5, 0.5), x2 = (-0.5, -0.5),  = 10-5,  = (-1, 1)2 and T = 1. The desired state yd  L2(I × ) is given by yd = yd +  where yd is the unique solution of (6.3) for the reference source u = (u1, u2) given by



0.05





u1(t)

=

0.65 0.15





0.35

0 < t  0.25, 0.25 < t  0.5, 0.5 < t  0.75, 0.75 < t  1



0.775





u2(t)

=

-0.025 0.975





0.275

0 < t  0.25, 0.25 < t  0.5, 0.5 < t  0.75, 0.75 < t  1

and   L2(I × ),  L2(I×)/ yd L2(I×) = 0.05, is a noise term. Using the L2(I × ) regularity of y from [27, 22] we can eliminate the PDE-constraint
by introducing the linear continuous solution operator K  L(L2(I; R2), L2(I × )) which maps u to y. The adjoint operator K  L(L2(I × ), L2(I; R2)) of K is defined
by the mapping   (p(·, x1), p(·, x2)) where p is the solution of the corresponding
adjoint state equation

ttp - p =   
p = 0

 

p(T ) = 0,

tp(T ) = 0,

in I × , on (0, T ) × , on .

(6.4)

29
for   L2(I × ). The operator K is well-defined according to [27, 22]. In order to apply Algorithm 1 to (6.2) we need to discretize the wave equation using a finite element method. For this purpose, consider ansatz and test spaces spanned by products of piecewise linear and continuous functions on a uniform time grid in I and a spatial triangulation of . The adjoint equation is discretized consistently. Finally, as in Section 6.1.2, we also replace the control space by picewise constant functions on the time grid and then apply a discretized version of Algorithm 1 to the problem. The finite dimensional subproblems in step 5. are again solved by a semismooth Newton method. We plot the computed function u¯ alongside the reference u as well as the the norm of the optimal dual variable in Figure 4a-4b. Upon a closer inspection, in contrast to the first

0.7

1

0.6 0.8
0.5 0.6
0.4 0.4
0.3 0.2
0.2
0 0.1

0

-0.2

0

0.5

1

0

0.5

1

(a) Reference u and u¯.

10-3

10-4

10-5

10-6

10-7

10-8

rj (uk ) O(1/(k + 1)) O(0.74k)

10-9 0

5

10

15

20

25

30

35

40

k

(c) Residual error over k.

10-5 1.2

1

0.8

0.6

0.4

0.2

0

0

0.2

0.4

0.6

0.8

1

(b) Norm of dual variable p¯.
101

100

10-1

10-2

10-3

10-4

uk - u¯ L1(I)
| uk - u¯ | O(0.8k)

10-5 0

5

10

15

20

25

30

35

40

k

(d) Norm/L1-error over k.

Figure 4. Optimal control and convergence of relevant quantities.

example, we now observe local clustering of the jumps of u¯. More in detail, in the vicinity of every jump of the reference function u, u¯ admits two jumps supported on neighbouring
grid nodes. Similar discretization effects for sparse deconvolution problems have been
observed in [12]. Alongside the optimal control we also report on the convergence history of the residual rj(uk), the L1-distance of uk and u¯ as well as the error of the norms in Figure 4c and 4d. Again we observe a linear rate of convergence for all considered
quantities.

References
[1] L. Ambrosio, N. Fusco, and D. Pallara, Functions of bounded variation and free discontinuity problems, Oxford: Clarendon Press, 2000.
[2] A. Beck and M. Teboulle, A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems, SIAM Journal on Imaging Sciences, 2 (2009), pp. 183­202.

30
[3] N. Boyd, T. Hastie, S. Boyd, B. Recht, and M. I. Jordan, Saturating splines and feature selection, J. Mach. Learn. Res., 18 (2018), p. 32. Id/No 197.
[4] N. Boyd, G. Schiebinger, and B. Recht, The alternating descent conditional gradient method for sparse inverse problems, SIAM J. Optim., 27 (2017), pp. 616­639.
[5] K. Bredies and H. K. Pikkarainen, Inverse problems in spaces of measures, ESAIM, Control Optim. Calc. Var., 19 (2013), pp. 190­218.
[6] E. Casas, F. Kruse, and K. Kunisch, Optimal control of semilinear parabolic equations by BV-functions, SIAM J. Control Optim., 55 (2017), pp. 1752­1788.
[7] A. Chambolle and C. Dossal, On the convergence of the iterates of the Fast Iterative Shrinkage/Thresholding Algorithm, Journal of Optimization Theory and Applications, 166 (2015), pp. 968­ 982.
[8] C. Clason and K. Kunisch, A duality-based approach to elliptic control problems in non-reflexive Banach spaces, ESAIM, Control Optim. Calc. Var., 17 (2011), pp. 243­266.
[9] L. Condat, A Direct Algorithm for 1-D Total Variation Denoising, IEEE Signal Processing Letters, 20 (2013), pp. 1054­1057.
[10] Q. Denoyelle, V. Duval, G. Peyré, and E. Soubies, The sliding Frank-Wolfe algorithm and its application to super-resolution microscopy, Inverse Probl., 36 (2020), p. 42. Id/No 014001.
[11] J. C. Dunn, Convergence rates for conditional gradient sequences generated by implicit step length rules, SIAM J. Control Optim., 18 (1980), pp. 473­487.
[12] V. Duval and G. Peyré, Exact support recovery for sparse spikes deconvolution, Found. Comput. Math., 15 (2015), pp. 1315­1355.
[13] S. Engel, B. Vexler, and P. Trautmann, Optimal finite element error estimates for an optimal control problem governed by the wave equation with controls of bounded variation, IMA Journal of Numerical Analysis, (2020).
[14] A. Flinth, F. de Gournay, and P. Weiss, On the linear convergence rates of exchange and continuous methods for total variation minimization, Mathematical Programming, (2020).
[15] M. Grasmair, The Equivalence of the Taut String Algorithm and BV-Regularization, Journal of Mathematical Imaging and Vision, 27 (2007), pp. 59­66.
[16] D. Hafemeyer and F. Mannel, A path-following inexact Newton method for optimal control in BV. https://arxiv.org/abs/2010.11628, 2020.
[17] D. Hafemeyer, F. Mannel, I. Neitzel, and B. Vexler, Finite element error estimates for one-dimensional elliptic optimal control by BV-functions, Math. Control Relat. Fields, 10 (2020), pp. 333­363.
[18] W. Hinterberger, M. Hintermüller, K. Kunisch, M. von Oehsen, and O. Scherzer, Tube methods for BV regularization, J. Math. Imaging Vis., 19 (2003), pp. 219­235.
[19] J. Huang and T. Zhang, The benefit of group sparsity, The Annals of Statistics, 38 (2010), pp. 1978­2004.
[20] Á. Jiménez and S. Sra, Fast newton-type methods for total variation regularization, in ICML, 2011.
[21] F. I. Karahanoglu, l. Bayram, and D. Van De Ville, A Signal Processing Approach to Generalized 1-D Total Variation, IEEE Transactions on Signal Processing, 59 (2011), pp. 5265­5274.
[22] K. Kunisch, P. Trautmann, and B. Vexler, Optimal control of the undamped linear wave equation with measure valued controls, SIAM Journal on Control and Optimization, 54 (2016), pp. 1212­1244.
[23] M. A. Little and N. S. Jones, Generalized methods and solvers for noise removal from piecewise constant signals. I. Background theory, Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 467 (2011), pp. 3088­3114.
[24] A. Milzarek and M. Ulbrich, A Semismooth Newton Method with Multidimensional Filter Globalization for l1-Optimization, SIAM Journal on Optimization, 24 (2014), pp. 298­333.
[25] K. Pieper and D. Walter, Linear convergence of accelerated conditional gradient algorithms in spaces of measures. https://arxiv.org/abs/1904.09218, 2019.
[26] L. I. Rudin, S. Osher, and E. Fatemi, Nonlinear total variation based noise removal algorithms, Physica D: Nonlinear Phenomena, 60 (1992), pp. 259­268.
[27] R. Triggiani, Regularity of wave and plate equations with interior point control, Atti Accad. Naz. Lincei Cl. Sci. Fis. Mat. Natur. Rend. Lincei (9) Mat. Appl., 2 (1991), pp. 307­315.
[28] B. Wahlberg, S. Boyd, M. Annergren, and Y. Wang, An ADMM Algorithm for a Class of Total Variation Regularized Estimation Problems*, IFAC Proceedings Volumes, 45 (2012), pp. 83­88.

31
[29] D. Walter, On sparse sensor placement for parameter identification problems with partial differential equations, dissertation, Technische Universität München, 2019.

