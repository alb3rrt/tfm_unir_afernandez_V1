Solutions of the Multivariate Inverse Frobenius­Perron Problem
Colin Fox Department of Physics, University of Otago, Dunedin, New Zealand. email:colin.fox@otago.ac.nz
Li-Jen Hsiao System Manufacturing Center, National Chung-Shan Institute of Science & Technology, Taiwan. email:ljh.lijenhsiao@gmail.com
Jeong Eun (Kate) Lee Department of Statistics, The University of Auckland, Auckland, New Zealand. email:kate.lee@auckland.ac.nz
(Dated: June 2, 2021)
We address the inverse Frobenius­Perron problem: given a prescribed target distribution , find a deterministic map M such that iterations of M tend to  in distribution. We show that all solutions may be written in terms of a factorization that combines the forward and inverse Rosenblatt transformations with a uniform map, that is, a map under which the uniform distribution on the d-dimensional hypercube as invariant. Indeed, every solution is equivalent to the choice of a uniform map. We motivate this factorization via 1-dimensional examples, and then use the factorization to present solutions in 1 and 2 dimensions induced by a range of uniform maps.

arXiv:2106.00177v1 [stat.CO] 1 Jun 2021

I. INTRODUCTION

A basic question in the theory of discrete dynamical systems, and in statistical mechanics, is whether a chaotic iterated function M : X  X, that maps a space X  Rd back onto X, has an equilibrium distribution, with probability density function (PDF) (x) [1]. A necessary condition is that the distribution  is invariant under M , i.e. if x   (x is distributed as ) then so is M (x), and further that the orbit of almost all points x  X defined as O+(x) = x, M (x), M 2(x), M 3(x), . . . tends in distribution to . Then, under mild conditions, the map is ergodic for , that is, expectations with respect to  may be replaced by averages over the orbit [2, 3].
For example, it is well known [2­5] that the logistic map Mlog(x) = 4x(1 - x), for x  [0, 1], is chaotic with the equilibrium distribution having PDF

1

log(x) =

,

 x(1 - x)

implying that Mlog is ergodic for log. Our motivating interest is in using this ergodic prop-
erty to implement sample-based inference for Bayesian analysis or machine learning. In those settings, the target distribution  is defined by the application. Generating a sequence {x0, x1, x2, x3, . . .} that is ergodic for  is useful because then expectations of any quantity of interest can be computed as averages over the sequence, i.e.,

1 N-1

lim N N

g (xi) =

i=0

g(x)(x) dx.
X

(1)

Commonly in statistics, such ergodic sequences are generated using stochastic iterations that simulates a Markov chain targeting  [6]; here we explore deterministic iterations that generate an orbit that is ergodic for .

The equilibrium distribution for a given iterated function, if it exists, can be approximated by computing the orbit of the map for some starting point then performing kernel density estimation, or theoretically by seeking the stationary distribution of the Frobenius­Perron (FP) operator that is the transfer [7], aka push-forward, operator induced by a deterministic map [2, 3]; we present the FP equation in Section II.
The inverse problem that we consider here, of determining a map that gives a prescribed equilibrium distribution, is the inverse Frobenius­Perron problem (IFPP) and has been studied extensively [8­15]. Summaries of previous approaches to the IFPP are presented in [13, 16] that characterize approaches as based on conjugate functions (see [8] for details) or the matrix method (see [16] for details), and [17] that also lists the differential equation approach. Existing work almost solely considers the IFPP in one-variable, d = 1, the exception being the development of a two-dimensional solution in [13] that is also presented in [16].
The matrix method, first suggested by Ulam [18], solves the IFPP for a piecewise-constant approximation to the target density, using a transition matrix representation of the approximated FP operator. Convergence of the discrete approximation is related to Ulam's conjecture, and has been proved for the multidimensional problem; see [17] and references therein. While the matrix method allows construction of solutions, at least in the limit, existing methods only offer a limited class of very non-smooth solutions, so are not clearly useful for characterizing all solutions, as we do here. We do not further consider the matrix methods.
The development in this paper starts with the differential equation approach in which the IFPP for restricted forms of distributions and maps is written as a differential equation that may be solved. We re-derive some existing solutions to the IFPP this way in Sec. III. The main con-

2

tribution of this paper is to show that the form of these solutions may be generalized to give the general solution of the IFPP for any probability distribution in any dimension d, as presented in the factorization theorem of Sec. III. This novel factorization represents solutions of the IFPP in terms of the Rosenblatt transformation [19] and a uniform map, that is a map on [0, 1]d that leaves the uniform distribution invariant. In particular, we show that the conjugating functions in [8] are exactly the inverse Rosenblatt transformations. For a given Rosenblatt transformation, there is a one-to-one correspondence between solution of the IFPP and choice of a uniform map.
This reformulation of the IFPP in terms of two wellstudied constructs leads to practical analytic and numerical solutions by exploiting existing, well-developed methods for Rosenblatt transformations, and for deterministic iterations that target the uniform distribution. The factorization also allows us to establish equivalence of solutions of the IFPP and other methods that employ a deterministic map within the generation of ergodic sequences. This standardizes and simplifies existing solution methods by showing that they are special cases of constructing the Rosenblatt transformation (or its inverse), and selection of a uniform map.
This paper starts with definitions of the IFPP and the Lyuapunov exponent in Sec. II. Solutions of the IFPP in one-dimension, d = 1, are developed in Sec. III. These solutions for d = 1 motivate the factorization theorem in Sec. IV that presents a general solution to the IFPP for probability distributions with domains in Rd for any d. Sec. V presents further examples of univariate, d = 1, solutions to the IFPP based on the factorization theorem in Sec. IV. Two 2-dimensional numerical examples are presented in Section VI C to demonstrate that the theoretical constructs may be implemented in practice. A summary and discussion of results is presented in Section VII, including a discussion of some existing computational methods that can be viewed as implicitly implementing the factorization solution of the IFPP presented here.
II. INVERSE FROBENIUS­PERRON PROBLEM AND LYAPUNOV EXPONENT
In this section we define the forward and inverse Frobenius­Perron problems, and also the Lyuapunov exponent that measures chaotic behaviour.
A. Frobenius­Perron Operator
A deterministic map xn+1 = M (xn) defines a map on probability distributions over state, called the transfer operator [7]. Consider the case where the initial state x0  0(·) (x0 is distributed as 0) for some distribution 0 and let n denote the n-step distribution, i.e., the distribution over xn = M n(x0) at iteration n. The

transfer operator that maps n  n+1 induced by M is given by the Frobenius­Perron operator associated with M : x  y [2, 3, 20]

n+1(y) =

n (x) |J (x)|

(2)

xM -1(y)

where |J(x)| denotes the Jacobian determinant [21] of M at x, and the sum is over inverse images of y.
The equilibrium distribution  of M satisfies

(x)

(y) =

(3)

|J (x)|

xM -1(y)

and we say that  is invariant under M .

B. Inverse Frobenius­Perron Problem
The inverse problem that we address is finding an iterative map M that has a given distribution  as its equilibrium distribution. We do this by performing the inverse Frobenius­Perron problem (IFPP) of finding M that satisfies (3) to ensure that  is an invariant distribution of M . Establishing chaotic hence ergodic behaviour is a separate calculation.
We will assume throughout that  is absolutely continuous with respect to the underlying measure so that a probability density function (x) exists, and, furthermore, that (x) > 0, x  X.

C. Lyapunov Exponent

The Lyapunov exponent h of an iterative map gives the average exponential rate of divergence of trajectories. We define the (maximal) Lyapunov exponent h as [2, 3]

h = lim 1 log dxN

N N

dx0

1 N-1

= lim N N

log |J(xn)|

n=0

(4)

that features the starting value x0. For ergodic maps the dependency on x0 is lost as N   and the Lyapunov exponent may be written

h = log |J(x)|(x) dx

(5)

X

where (x) is the invariant density. A positive Lyapunov exponent h indicates that the map is chaotic.
The theoretical value for the Lyapunov exponent may be obtained using (5), while (4) provides an empirical value obtained by iterating the map M . For example, the Lyapunov exponent of the logistic map evaluated by (5) is hlog = log 2  0.693147 while (4) evaluated over an orbit with 10000 iterations gave hlog  0.693140.
For chaotic maps, any uncertainty in initial value means that an orbit cannot be precisely predicted, since

3

initial states with any separation become arbitrarily far apart, within X, as iterations increase. Hence it is useful to characterize the orbit statistically, in terms of the equilibrium distribution over states in the orbit.
It is interesting to note that theoretical chaotic and ergodic behaviour does not necessarily occur when iterations of a map are implemented on a finite-precision computer. For example, when the logistic map, above, on [0, 1] is iterated on a binary computer the multiplication by 4 corresponds to a shift left by two bits and all subsequent operations maintain lowest order bits that are zero. Repeated iterations eventually produce the number zero, no matter the starting value. While it is simple to correct this non-ideal behaviour, as was done to give the numerical Lyapunov exponent, above, it is important to note that computer implementation can have very different dynamics to the mathematical model.

III. SOLUTION OF THE IFPP IN 1-DIMENSION

In this Section we develop solutions of the IFPP in onedimension, d = 1. Without loss of generality we consider distributions on the unit interval X = [0, 1], as the domain of any univariate distribution may be transformed by a change of variable to X = [0, 1], including when the domain is the whole real line (-, ).
For distributions over a scalar random variable, the FP equation for the invariant density (3) simplifies to

 (x)

(y) =

.

(6)

|M (x)|

xM -1(y)

A. The Simplest Solution

We first note, almost trivially, that the identity map M = I, where I(x) = x, has  as an invariant distribution, so solves the IFPP for any . Somewhat less trivial is to derive this simplest solution by assuming that M is monotonic increasing and M (0) = 0, so that there is just one inverse image in (6). Writing |M (x)| = dM/dx gives the differential equation with separated variables

(M )dM = (x)dx

(7)

that has solution

F (M ) = F (x)

where F (x) =

x 0

(x

)

dx

is the cumulative distribution

function (CDF) for . If F is invertible denote the in-

verse by F -1, called the the inverse distribution function

(IDF), otherwise let F -1 denote the generalized inverse distribution function, F -1(p) = inf{x  X : F (x)  p}.

Then, M = F -1(F (x)) = x, or M (x) = x, almost every-

where. Hence the identity map is the unique monotonic

increasing map that has  as its invariant distribution.

Clearly, the identity map is not ergodic for .

FIG. 1. Translation operator T c for c = 0.7 (left) and triangle map t1 (right).

We may generalize this solution by setting M (0) = k, for some k  [0, 1), and also only requiring M to be piecewise continuous. Allowing one discontinuity in M we write the integral of the separated differential equation (7) as

F (M ) = F (x) + k mod 1

giving the solution to the IFPP

M (x) = F -1  T c  F (x)

(8)

where c = F -1(k). Here T c denotes the operator that translates by c with wrap-around on [0, 1) [22]

T c(y) = y + c - y + c ,

(9)

where denotes the floor function; see Figure 1 (left). The identity map is recovered when k = c = 0.
It is easily seen that the Lyapunov exponent of the map in (8) is h = 0, so the map is not chaotic. However, interestingly, this map can generate a sequence of states that produce a numerical integration rule with respect to , since appropriate choices of c and number of iterations N can produce a rectangle rule quadrature or a quasi Monte Carlo lattice rule; see Sec. IV C.

B. Exploiting Symmetry in (x)

When the PDF  has reflexive symmetry about 1/2, i.e. (x) = (1 - x), we can simplify the FP equation (6) by assuming that the map M has the same symmetry. Specifically, we write the triangle map (see Figure 1 (right))

t1(x) = 1 - 2 |x - 1/2|

(10)

that has reflexive symmetry about 1/2, and write

M (x) = m(t1(x))

(11)

where m(x) : [0, 1]  [0, 1] is a monotonic increasing map with m(0) = 0 (and, it will turn out, m(1) = 1). Hence the FP equation simplifies to

 (x)

(y) = 2

,

x  M -1(y),

(12)

|M (x)|

4

that we can write as the separated equations

(M ) dM = 2(x)dx, x < 1/2, (M ) dM = -2(x)dx, x > 1/2,

that has the continuous solution

F (M ) = t1(F (x))

giving the continuous solution to the IFPP

M (x) = F -1  t1  F (x).

(13)

One can solve for m(x) = F -1(2F (x/2)), though we do not further consider the function m.
The approach we have used here simplifies the approach in [10], while `doubly symmetric' maps of the form (11) were considered in [23], and again in [11, 12].

C. Symmetric Triangular Distribution

To give a concrete example of the solution in (13), we consider the symmetric triangular distribution on [0, 1] with PDF

1

tri(x) = 2 - 4 x - 2

(14)

that

has

reflexive

symmetry

about

x

=

1 2

.

The

CDF

is

Ftri(x) =

2x2

1 2

+

4x

-

2x2

0



x



1 2

1 2



x



1

giving the unimodal map, after substituting into (13),



     

Mtri(x) =

 

1

-



  

 2x 0  x  1
8

1-

1 2

-

2x2

1 8



x



1 2

1 2

- 2(1 

-

x)2

1 2



x



1-

1 8

2(1 - x) 1 - 1  x  1

8

(15)

shown in Figure 2 (left). The same map was derived

in [8]. Figure 2 (right) shows a normalized histogram

of 106 iterates of Mtri starting at x = 0.3, confirming

that the orbit of Mtri converges to the desired triangu-

lar distribution. The numerical implementation avoids

finite-precision effects, as discussed later.

The theoretical Lyapunov exponent for Mtri is htri = log 2  0.693147 while (4) evaluated over an orbit with

106 iterations gave htri  0.693148.

IV. SOLUTIONS OF THE IFPP FOR GENERAL MULTI-VARIATE TARGET DISTRIBUTIONS
The solutions to the 1-dimensional IFPP with special structure in Eqs (8) and (13) are actually examples of

FIG. 2. Iterative map Mtri in (15) (left) and a histogram of 1 × 106 iterates of the map Mtri (right).
a general solution to the IFPP for multi-variate probability distributions with no special structure. We state that connection via a theorem that establishes a factorization of all possible solutions to the IFPP, and that also provides a practical means of solving the IFPP.
We first introduce the forward and inverse Rosenblatt transformations, that is the multi-variate generalization the CDF and IDF for univariate distributions.

A. Forward and Inverse Rosenblatt Transformations

A simple transformation of an absolutely continuous d-variate distribution into the uniform distribution on the d-dimensional hypercube was introduced by Rosenblatt [19], as follows. The joint PDF can be written as a product of conditional densities,

(x1, . . . , xd) = 1(x1)2(x2|x1) · · · d(xd|x1 . . . , xd-1),
where k(xk|x1 . . . , xk-1) is a conditional density given by

k(xk|x1 . . . , xk-1)

=

pk(x1, . . . , xk) , pk-1(x1 . . . , xk-1)

(16)

in terms of the marginal densities,

pk = (x1, . . . , xd) dxk+1 · · · dxd,

(17)

where k = 1, . . . , d.
Let z = (z1, . . . , zd) = R(x1, . . . , xd) where R is the Rosenblatt transformation [19] from the state-space X  Rd of  to the d-dimensional unit cube [0, 1]d, defined in terms of the (cumulative) distribution function F by

x1

z1 = F1(x1) =

1(x1) dx1,

-

x2

z2 = F2(x2|x1) =

2(x2|x1) dx2,

-

...

xd

zd = Fd(x2|x1, . . . , xd-1) =

d(xd|x1, . . . , xd-1) dxd.

-

5

As noted in [19], there are d! transformations of this type, corresponding to the d! ways of ordering the coordinates. Further multiplicity is introduced by considering coordinate transformations, such as rotations.
Notice that in 1-dimension, the Rosenblatt transformation R(x) is simply the CDF F (x).
It follows that if x   then z = R(x)  Unif([0, 1]d), i.e., z is uniformly distributed on the d-dimensional unit cube [19]. When (x) > 0, x  X the distribution functions are strictly monotonic increasing and the inverse of the Rosenblatt transformation R-1 is well defined, otherwise let R-1 denote the generalized inverse as in Sec. III A. Then, if z  Unif([0, 1]d) it follows that x = R-1(z)  , i.e., x is distributed as the desired target distribution  [24]; this is the basis of the conditional distribution method for generating multi-variate random variables, that generalizes the inverse cumulative transformation method for univariate distributions [24­ 27]. These results may also be established by substituting R or R-1 into the the FP equation (2), noting that there is a single inverse image and that the Jacobian determinant of R equals the target PDF (x).
In the remainder of this paper, we refer to any map R satisfying x    R(x)  Unif([0, 1]d) as a Rosenblatt transformation, with (generalized) inverse as defined above.

of the uniform map U , once the Rosenblatt transformation is determined, that is, a coordinate system is chosen with an ordering of those coordinates.
Grossmann and Thomae [8] called dynamical systems M and U related by a formula of the form (18) as `related by conjugation', or just `conjugate', and the map R-1 in (18) is a `conjugating function'. Thus, in the language of [8], Theorem 1 shows that the IFPP for any distribution  has a solution (actually, it shows that there are infinitely many solutions), every solution map is conjugate to a uniform map, and the conjugating function is precisely the inverse Rosenblatt transformation.
Notice that both the translation operator T c in (9) and the triangle map in (10) are uniform maps on the unit interval [0, 1]. Thus, the solutions to the IFPP given in Eqs (8) and (13) are examples of the general solution form in (18). In particular, while the solution to the IFPP in (13) was derived assuming symmetry of the target density (·), (13) actually gives a solution of the IFPP for any density (·). Unimodal maps of this form were derived in [9].
Computed examples of solutions to the IFPP given by the factorization (18) are presented in Sec. V, in one dimension, and in Sec VI in two dimensions. High dimensional calculations are discussed in Sec. VII.

B. Factorization Theorem

The following theorem characterizes solutions to the IFPP.

Theorem 1. Given a probability distribution  in ddimensions, a map M (x) is a solution of the IFPP, that is, M (x) satisfies the FP equation (3), if and only if

M (x) = (R-1  U  R)(x),

(18)

where R is a Rosenblatt transformation and U is a `uniform map' on the unit d-dimensional hypercube, i.e. a map that has Unif([0, 1]d) as invariant distribution [28].
Proof: We will show that  is an invariant distribution of M iff M has the form (18). () Assume M has the form (18). If x   then R(x)  Unif([0, 1]d) hence U (R(x))  Unif([0, 1]d), as Unif([0, 1]d) in invariant under U , hence M (x) = R-1(U (R(x)))  , as desired. () If  is invariant under M then U = R  M  R-1 is a uniform map, since if z  Unif([0, 1]d) then R-1(z)  , M (R-1(z))  , and R(M (R-1(z)))  Unif([0, 1]d). Inserting this U into (18) gives the desired factorization.

The first part of the proof shows that any uniform map U induces a solution to the IFPP, though the particular solution depends on the particular Rosenblatt transformation. The second part of the proof shows that different solutions to the IFPP effectively differ only by the choice

C. Properties of M from U

Many properties of the map M are inherited from the uniform map U .
When R and R-1 are continuous, M is continuous iff U is continuous. In one dimension, monotonicity of the CDF and IDF imply that the number of modes of U equals the number of modes of M ; in particular, M is unimodal iff U is unimodal.
Constructing iterative maps with specific periodicity of the orbit is possible through the use of translation operators T c as uniform maps, defined in Eq. (9). Consider, first, maps in one dimension on [0, 1]. If the shift c = 0 and c / Q, the map is aperiodic. However, in the case that c = 0 and c  Q such that

N

c=

(19)

D

with N, D  N and gcd(N, D) = 1, then the map is periodic with periodicity D, and iterative maps constructed with U = T c exhibit the same periodicity. These properties may be extended to multi-dimensional settings when the translation constant c is a vector of shifts in each coordinate direction, as used in rank-one lattice rules for quasi-Monte Carlo integration [29].
The factorization in Theorem 1 also shows that performing an iteration xn+1 = M (xn) with an iterative map M on the space X is equivalent to applying the corresponding uniform map zn+1 = U (zn) on the space [0, 1]d through the transformations R and R-1, as indi-

6

cated in the following (commuting) diagram.

xn M xn+1

R-1 R
R-1 R

zn U
zn+1

Using this commuting property, it is straightforward to prove the following lemma.

Lemma 1. For given distribution , let R be a Rosenblatt transformation for . Let M = R-1  U  R be a solution of the IFFP, as guaranteed by Theorem 1, where U is a uniform map. Then

OM + (x0) = R-1 OU+(R(x0)) .

(20)

Hence, instead of iterating M on the space X to produce the sequence {x1, x2, x3, . . .}, Eq. (20) shows that one can iterate the map U on the space [0, 1]d to produce the sequence {z1, z2, z3, . . .} and then evaluate xn = R-1zn, n = 1, 2, . . ., to produce exactly the same sequence on X. Since the map M is mixing or ergodic iff the uniform map U is mixing or ergodic, respectively, in this sense mixing and ergodicity of M is inherited from U.
Using the expansion in Eq. (20), we see that M is deterministic or stochastic iff U is deterministic or stochastic, respectively. Even though we are not considering stochastic maps here, we note that, for stochastic maps, iterations of M are correlated or independent iff iterations of U are correlated or independent, respectively.
Some other properties that are, and are not, preserved by the transformation from U to M are discussed in [8].

V. EXAMPLES IN ONE DIMENSION

A. Uniform Maps on [0, 1]

We have already encountered three uniform maps on the interval [0, 1], namely the identity map I(x) = x (Figure 3 (top, left)) and the translation operator (9) (Figure 1 (left)), that have Lyapunov exponent h = 0, and the triangle map (Figure 1 (right)) with Lyapunov exponent h = log 2.
Some further elementary uniform maps on [0, 1], and associated Lyapunov exponents, are:
· periods of a sawtooth function on [0, 1] (Figure 3, top-right, for l = 3 periods) [30]

s (x) =  x -  x ,

(21)

with Lyapunov exponent h = log ,

· periods of a triangle function on [0, 1] (Figure 3 (bottom, left) for l = 3 periods) [31]

t (x) = 1 - 2 |s (x) - 1/2| .

(22)

with Lyapunov exponent h = log 2 ,

FIG. 3. Two examples of uniform maps on [0, 1]. Left: periods of a sawtooth wave, for = 3. Right: periods of a triangle wave, for = 3.

· and the asymmetric triangle, for c  (0, 1) (Figure 3 (bottom, right) for c = 0.3)

x

0xc

tc(x)

=

 

c 1

-

x

(23)

 

cx1

1-c

with Lyapunov exponent 0  h = -c log c - (1 - c) log(1 - c)  log 2.

Obviously, many more uniform maps are possible. Further examples can be formed by partitioning the domain and range of any uniform map, and then permuting the subintervals. Many existing `matrix-based' methods for constructing solutions to the IFPP, can be viewed as examples of such a partition-and-permute of an elementary uniform map [32]. Uniform maps of other forms are developed in [33] from two-segmental Lebesgue processes, producing uniform maps that are curiously non-linear.

Lemma 2. The composition of uniform maps is also a uniform map, i.e., if U1 and U2 are uniform maps then so is U = U1  U2.
An example is t , that can be constructed as the composition t = s  t1.
We mentioned the numerical artifacts that can occur with finite-precision arithmetic, particularly when implementing maps on a binary computer and when the endpoints of the interval X and constants in the maps have exact binary representations. We avoided these artifacts by composing the stated uniform map with the translation T c for c = 1/3 × 10-9 that does not have finite binary representation [34]. This small shift in indiscernible in the graphs of the maps.

7

FIG. 4. Iterative map Mramp in (25) (left) and a normalized histogram of 1 × 106 iterates that approximates the equilib-
rium PDF (right).

FIG. 5. Iterative map Mramp in (25) (left) and a normalized histogram of 1 × 106 iterates that approximates the equilib-
rium PDF (right).

B. Ramp Distribution

To give a concrete example of the solution in (13) for a distribution without reflexive symmetry, we consider the ramp distribution with PDF

ramp(x) = 2x

(24)

that has CDF

Framp(x) = x2.

We produce a unimodal, continuous map by choosing the

uniform map t1, as in (13), to give

Mramp =

 2x
 2 1 - x2

0  x  1
2
1  x  1
2

(25)

shown in Figure 4 (left). Figure 2 (right) shows a normalized histogram of 106 iterates of Mramp starting at x = 0.3, confirming that the orbit of Mramp converges to the desired ramp distribution, as guaranteed by The-
orem 1. The numerical implementation avoids finite-
precision effects, as discussed earlier. The estimated Lya-
punov exponent for this map is h  1.040035, which is
greater than the Lyapunov exponent for the inducing tri-
angular map t1 which is log 2  0.693147. Using a different uniform map gives a different solu-
tion to the IFPP. For example, choosing s3 gives the map M = F -1  s3  F shown in Figure 5 (left). A normalized histogram over an orbit of 106 iterations is shown in Fig-
ure 5 (right), confirming that this map is also ergodic for
ramp. The estimated Lyapunov exponent for this map is h  1.098612, which is the same numerical value as the
Lyapunov exponent for the sawtooth map s3 which is log 3  1.098612. Comparing with the result for the map
induced by t1, this shows that the Lyapunov exponent of a map is not generally equal to the Lyapunov exponent
of the inducing uniform map.

C. The Logistic Map and Alternatives

The logistic map, mentioned in the introduction, is

Mlog(x) = 4x(1 - x).

(26)

The equilibrium distribution of this map

1

log(x) =

,

(27)

 x(1 - x)

can be easily verified by substituting into the FP equation (6). The CDF of log(x) is

x

2



Flog(x) =

0

(x )dx = arcsin( 

x),

(28)

and the IDF is

Fl-og1(x)

=

sin2( x ) 2

=

1 (1
2

-

cos(x)).

(29)

The logistic map (26) is induced by the factorization (18) by choosing the triangle map t1 as uniform map, i.e., substituting the CDF (28) and IDF (29) into (13); see Figure 6 (left). Equivalently, one may note that the logistic map (26) is transformed into the triangle map t1 by the change of variables z = F -1(x); in the language of [8], Mlog and t1 are conjugate dynamical laws.
Other iterative maps which preserves the same equilibrium distribution (27) can be constructed by choosing another uniform map, such as periods of a triangle function (22). This gives the iterative maps

M

= Fl-og1  t

 Flog = sin2(2

 arcsin( x)),

 1,

(30)

that coincide with the nth power of the logistic map (26) for = 2n-1. Figure 6 (right) shows the map which preserves the same equilibrium distribution as the logistic map but induced by the uniform map t3. Since 3 is not of the form 2n-1, this map is not simply a power of the logistic map.
The theoretical value of the Lyapunov exponent of the map in (30) is log 2 , using (5). Table I gives the theoretical values of the Lyapunov exponent and experimentally calculated values using 10000 iterations, as in (4), for some values of .

8

1 0.8 0.6 0.4 0.2
01 0x.280.60.40.2 00 0.20.40x.610.8 1

1 0.8 0.6 0.4 0.2
01 0x.280.60.40.2 00 0.20.40x.610.8 1

FIG. 6. Logistic map, that equals Mlog = Fl-og1  t1  Flog (left), and the map M = Fl-og1  t3  Flog that has the same
equilibrium distribution (right).

he

ht

1 0.692819 0.693147

2 1.386284 1.386294

4 2.079430 2.079442 224 17.328594 17.328680 239 27.725713 27.725887

TABLE I. Experimental he and theoretical ht Lyapunov exponents for the maps in (6) for a range of , given to 6 decimal places.

VI. TWO EXAMPLES IN TWO DIMENSIONS A. Uniform Maps on [0, 1]2

Two well-known examples of uniform maps in the twodimensional unit square, X = [0, 1]2, are the baker's map

Ub(x1, x2) =

2x1

mod 1, x2 + u

x1

-

1 2

2

, (31)

where u is the unit step function, and the Arnold cat map

UA(x1, x2) = ((2x1 + x2) mod 1, (x1 + x2) mod 1) . (32)
Other uniform maps in d > 1 dimensions may be formed by 1-dimensional uniform maps acting on each coordinate, giving the coordinate-wise uniform map

U (x) = (U1(x1), U2(x2), . . . , Un(xd))

(33)

where Ui(x), i = 1, 2, . . . , d, are uniform maps in one dimension. We will use the baker's map (31) and a coordinate-wise uniform map in the 2-dimensional examples that follow.

1 0.8 0.6 0.4 0.2
01 0x.280.60.40.2 00 0.20.40x.610.8 1

1 0.8 0.6 0.4 0.2
01 0x.280.60.40.2 00 0.20.40x.610.8 1

FIG. 7. Plots of the Rosenblatt transformation R and its inverse R-1 for the checker-board distribution. Top row shows
the components of R: first component (left) and second component (right). Bottom row shows the components of R-1:
first component (left) and second component (right).

The first step in constructing a solution to the IFPP for this distribution is to construct the forward and inverse Rosenblatt transformations, which requires the marginal density functions (17) that may be evaluated analytically in this case. A plot of the two components of the functions R and R-1 is shown in Figure 7.
We construct two solutions to the IFPP, each induced by choosing a particular uniform map: the first is the baker's map (31); the second is a component-wise uniform map (33) with an asymmetric triangle map (23) acting on each component,

U (x1, x2) = (U1(x1), U2(x2))

(34)

where U1 = tc for c = 0.3 and U2 = tc with c = 0.9. Figure 8 (top row) shows the two components of the
map induced by the baker's map, the checker-board distribution (bottom-left), and a histogram of 106 iterates (bottom-right) showing that the map does indeed converge in distribution to the desired distribution.
Figure 9 (top row) shows the two components of the map constructed using the two component-wise asymmetric triangular maps, the checker-board distribution (bottom-left), and a histogram of 106 iterates (bottomright) showing that the map also converges in distribution to the desired distribution, and hence is also a solution to the IFPP.

B. Checker-Board Distribution

C. A Numerical Construction

This example demonstrates construction of a map in two-dimensions that targets the checker board distribution, shown in Figure 8 (bottom-left) and Figure 9 (bottom-left), using the factorization (18).

Numerical implementation of the factorized solution (18) is not difficult in few dimensions. In this section we present an example of numerical implementation using a normalized greyscale image of a coin [35], piecewise

9

1 0.8 0.6 0.4 0.2
01 0x.280.60.40.2 00 0.20.40x.610.8 1

1 0.8 0.6 0.4 0.2
01 0x.280.60.40.2 00 0.20.40x.610.8 1

2 1.5
1 0.5
01 0x.280.60.40.2 00 0.20.40x.610.8 1

1000 800 600 400 200
01 0x.280.60.40.2 00 0.20.40x.610.8 1

FIG. 8. Iterated function constructed with U being the baker's transformation, a histogram of iterates, and the checker board distribution. Shown is: the x1 part of the constructed map (top-left), the x2 part of the constructed map (top-right), the checker board distribution (bottom-left), and a histogram of iterates of the constructed map (bottom-right).

FIG. 10. A normalized greyscale image of a coin used as the target distribution, and a normalized histogram of iterations of the map targeting this distribution. Original image (left), and normalized histogram of 106 iterations (right).
be seen, the estimated PDF from the orbit of this map does reproduce the image of the coin. However, there are also obvious artifacts near the edge of the image showing that mixing could be better. We conjecture that a chaotic uniform map would produce better mixing and fewer numerical artifacts.
VII. SUMMARY AND DISCUSSION

1 0.8 0.6 0.4 0.2
01 0x.280.60.40.2 00 0.20.40x.610.8 1

1 0.8 0.6 0.4 0.2
01 0x.280.60.40.2 00 0.20.40x.610.8 1

2 1.5
1 0.5
01 0x.280.60.40.2 00 0.20.40x.610.8 1

1000 800 600 400 200
01 0x.280.60.40.2 00 0.20.40x.610.8 1

FIG. 9. Same as Figure 8 except M is constructed with U being component-wise asymmetric triangular maps.
constant over pixels, as the target distribution; see Figure 10 (left). The marginal distributions (17) are evaluated as linear interpolation of cumulative sums over pixel values, and hence the CDF and then forward and inverse Rosenblatt transformations follow as in section IV A. The uniform map was produced as component-wise univariate translation maps, specifically
U (x1, x2) = (U1(x1), U2(x2))
where U1 = T c for c = 0.6 and U2 = T c with c = 0.2. The resulting map is given by Eq. (18).
Figure 10 (right) shows a normalized histogram, binned to pixels, of 106 iterations of this map. As can

We have shown how the solution of the IFPP, of finding an iterative map with a given invariant distribution, can be constructed from uniform maps through the factorization established in Theorem 1,
M = R-1  U  R
where R denotes the Rosenblatt transformation that has Jacobian determinant equal to density function of the invariant distribution. In one-dimension, R is exactly the CDF of the given distribution, so the factorization generalizes existing one-dimensional solutions to the setting of arbitrary multi-variate distributions. The factorization also shows the relationship between arbitrary iterative maps and uniform maps, i.e., given a Rosenblatt transformation the solution of the IFPP is equivalent to the choice of a uniform map that has Unif([0, 1]d) as invariant distribution.
We find the factorization (18) appealing as it shows that solution of the IFPP for arbitrary distributions, and in multi dimensions, is reduced to two standard and well-studied problems, i.e., constructing the Rosenblatt transformation (or CDF in one dimension) and designing a uniform map. It is therefore surprising, to us, that the factorization (18), and more generally the Rosenblatt transformation, appears to not be widely used in the study of chaotic iterated functions and the IFPP. Grossmann and Thomae [8], in one of the earliest studies of the IFPP, essentially derived the factorization (18) by introducing conjugate maps and establishing the relation (in their notation) that (x) = dh-1(x)/dx where  is the invariant distribution and h is the conjugating function; see [8, Figure 3]. It is a small step to identify that h is the IDF, generalized in multi-dimensions

10

by the inverse Rosenblatt transformation. However, the connection was not made in [8], despite the Rosenblatt transformation having been already known, in statistics, for some decades [19].
We constructed solutions to the IFPP for distributions with special reflexive symmetry structure, and then with no special structure, by constructing the Rosenblatt transformation, and its inverse, for some examples in one and two dimensions. For simple distributions with analytic form the Rosenblatt transformation may be constructed analytically, while numerically-defined distributions required calculation of the marginal distributions (17) using numerical techniques.
Although this factorization and construction is applicable to high-dimensional problems, the main difficulty is obtaining all necessary marginal densities, which requires the high-dimensional integral over xk+1 . . . xd in (17). In general, this calculation can be extremely costly. Even a simple discretization of the PDF , or of the argument of the marginal densities (17), leads to exponential cost with dimension.
To overcome this cost, Dolgov et al [27] precomputed an approximation of (x1, . . . , xd) in a compressed tensor train representation that allows fast computation of integrals in (17), and subsequent simulation of the inverse Rosenblatt transformation R-1 from the conditionals in (17), and showed that computational cost scales linearly with dimension d. Practical examples presented in [27], in dimension d  32, demonstrate that operation by the forward and inverse Rosenblatt transformations is computationally feasible for multivariate problems with no special structure.
Finding a solution of the IFPP with desired properties is reduced to a standard problem of designing a uniform map on [0, 1]d, for which there are many existing efficient options. For example, standard computational uniform random number generators, that produce pseudorandom sequences of numbers, are one such existing uniform map, as are the quasi-Monte Carlo rules that we mentioned earlier [29]. These induce pseudo-random and quasi-Monte Carlo sequences, respectively, on the space X via the inverse Rosenblatt transformation R-1 [36]. Both these schemes were demonstrated in practical highdimensional settings in [27].
The RHS of Eq. (20) in d = 1 dimension is exactly the standard computational route for implementing inverse cumulative transformation sampling from , since computational uniform pseudo-random number generators perform a deterministic iteration on [0, 1] to implement a uniform map [36]. For d > 1, the RHS of Eq. (20) is the conditional distribution method that generalizes the inverse cumulative transformation method, as mentioned above [24­27]. Hence Lemma 1 shows that standard computational implementation of both the inverse cumulative transformation method in d = 1 and the conditional distribution method in d > 1 is equivalent to implementing a solution to the IFFP. In this sense, computational inverse cumulative transformation sampling from  can be viewed as the prototype for all

iterative maps that target the distribution , with each ergodic sequence corresponding to a particular choice of uniform map.
We mentioned that the Rosenblatt transformation associated with a given distribution  is not unique. Actually, any two Rosenblatt transformations for  are related by a uniform map, as shown in the following Lemma.

Lemma 3. If R1 is a Rosenblatt transformation for  then R2 is a Rosenblatt transformation for  if and only if

R2 = U  R1
for some uniform map U .
Proof: () Since R1 and R2 are Rosenblatt transformations for , then U = R2  R1-1 is a uniform map and R2 = U  R1. () If R2 = U  R1 then if x  , R1(x)  Unif([0, 1]d) and U  R1(x)  Unif([0, 1]d) so R2 is a Rosenblatt transformation.
Hence, any Rosenblatt transformation R may be written as R = U  R0 for some uniform map U and a fixed Rosenblatt transformation R0.
The Rosenblatt transformations that map any distribution to the uniform distribution on the hypercube may also be used to understand mappings between spaces that are designed to transform one distribution to another, such as the `transport maps' developed in [37]. Consider distributions A and B, with Rosenblatt transformations RA and RB, respectively, that may be related as in the following diagram.

U

RA

RB-1

A

Unif([0, 1]d)

B

RA-1

RB

The diagram suggests a proof of the following lemma, that generalizes the factorization Theorem 1.

Lemma 4. A map M satisfies x  A  M (x)  B iff it can be written as M = RB-1  U  RA, where RA and RB are Rosenblatt transformations for A and B, respectively, and U is a uniform map.
Hence, for given Rosenblatt transformations, the choice of a map, that maps samples from A to samples from B, is equivalent to the choice of a uniform map. Alternatively, if a fixed uniform map is selected, such as the identity map, the choice of map M is completely equivalent to the choice of Rosenblatt transformations. This factorization also shows that the equivalence class of conjugate maps, noted in [8], for each dimension d, is generated by the uniform maps, and each member of the equivalence class contains maps that target each distribution, when the associated Rosenblatt transformation satisfies the mild conditions to be a conjugating function as defined in [8].

11

[1] The density function is with respect to some underlying measure, typically Lebesgue. Throughout this paper we will use the same symbol for the distribution and associated PDF, with meaning taken from context.
[2] J.R. Dorfman. Cambridge Lecture Notes in Physics: An introduction to chaos in nonequilibrium statistical mechanics, volume 14. Cambridge University Press, 1999.
[3] Andrzej Lasota and Michael C. Mackey. Chaos, fractals, and noise. Springer-Verlag, New York, second edition, 1994.
[4] S. Ulam and J. von Neumann. On combination of stochastic and deterministic processes. Bulletin of the American Mathematical Society, 53(11):1120, 1947.
[5] R.M. May. Simple mathematical models with very complicated dynamics. Nature, 261(5560):459­467, 1976.
[6] J. S. Liu. Monte Carlo Strategies in Scientific Computing. Springer-Verlag, 2001.
[7] Stefan Klus, P´eter Koltai, and Christof Schu¨tte. On the numerical approximation of the Perron­Frobenius and Koopman operator. Journal of Computational Dynamics, 3(1):51­79, 2016.
[8] Siegfried Grossmann and Stefan Thomae. Invariant distributions and stationary correlation functions of onedimensional discrete processes. Zeitschrift fu¨r Naturforschung a, 32(12):1353­1363, 1977.
[9] S. V. Ershov and Georgii Gennad'evich Malinetskii. The solution of the inverse problem for the Perron­Frobenius equation. USSR Computational Mathematics and Mathematical Physics, 28(5):136­141, 1988.
[10] F.K. Diakonos and P. Schmelcher. On the construction of one-dimensional iterative maps from the invariant density: The dynamical route to the beta distribution. Physics Letters A, 211(4):199­203, 1996.
[11] FK Diakonos, D. Pingel, and P. Schmelcher. A stochastic approach to the construction of one-dimensional chaotic maps with prescribed statistical properties. Physics Letters A, 264(2-3):162­170, 1999.
[12] D. Pingel, P. Schmelcher, and F.K. Diakonos. Theory and examples of the inverse Frobenius­Perron problem for complete chaotic maps. Chaos, 9(2):357­366, 1999.
[13] Erik M Bollt. Controlling chaos and the inverse Frobenius­Perron problem: global stabilization of arbitrary invariant measures. International Journal of Bifurcation and Chaos, 10(05):1033­1050, 2000.
[14] Xiaokai Nie and Daniel Coca. A new approach to solving the inverse Frobenius­Perron problem. In 2013 European Control Conference (ECC), pages 2916­2920. IEEE, 2013.
[15] Xiaokai Nie and Daniel Coca. A matrix-based approach to solving the inverse Frobenius­Perron problem using sequences of density functions of stochastically perturbed dynamical systems. Communications in Nonlinear Science and Numerical Simulation, 54:248­266, 2018.
[16] A. Rogers, R. Shorten, D.M. Heffernan, and D. Naughton. Synthesis of piecewise-linear chaotic maps: Invariant densities, autocorrelations, and switching. International Journal of Bifurcation and Chaos, 18(8):2169­2189, 2008.
[17] Nijun Wei. Solutions of the inverse Frobenius­Perron

problem. Master's thesis, Concordia University, 2015. [18] Stanislaw Marcin Ulam. A Collection of Mathematical
Problems. Interscience Publishers, 1960. [19] M. Rosenblatt. Remarks on a multivariate transforma-
tion. Ann. Math. Stat., 23(3):470­472, 1952. [20] Pierre Gaspard. Chaos, scattering and statistical mechan-
ics. Cambridge University Press, 1998. [21] We have used the language of differential maps, as all
the maps that we display in this paper are differentiable almost everywhere [38]. More generally, |J(x)|-1 denotes the density of nM -1 with respect to n+1, see, e.g., [3, Remark 3.2.4.]. [22] Hence T c is the translation operator on the unit circle S1. [23] G Gyo¨rgyi and P Sz´epfalusy. Fully developed chaotic 1-d maps. Zeitschrift fu¨r Physik B Condensed Matter, 55(2):179­186, 1984. [24] M.E. Johnson. Multivariate Statistical Simulation. John Wiley & Sons, 1987. [25] L. Devroye. Non-Uniform Random Variate Generation. Springer-Verlag, 1986. [26] W. Ho¨rmann, J. Leydold, and G. Derflinger. Automatic Nonuniform Random Variate Generation. SpringerVerlag, 2004. [27] Sergey Dolgov, Karim Anaya-Izquierdo, Colin Fox, and Robert Scheichl. Approximation and sampling of multivariate probability distributions in the tensor train decomposition. Statistics and Computing, 30(3):603­625, 2020. [28] When U satisfies the stronger condition that Unif([0, 1]d) is the equilibrium distribution, U is called an exact map [3]. [29] Josef Dick, Frances Y Kuo, and Ian H Sloan. Highdimensional integration: the quasi-Monte Carlo way. Acta Numerica, 22:133, 2013. [30] The two period sawtooth map s2 is also called the Bernoulli map, and its orbit O+(x) is the dyadic transformation. [31] This is the `broken linear transformation' in [8] of order p=2 . [32] Alan Rogers, Robert Shorten, and Daniel M Heffernan. Synthesizing chaotic maps with prescribed invariant densities. Physics Letters A, 330(6):435­441, 2004. [33] W. Huang. Characterizing chaotic processes that generate uniform invariant density. Chaos, Solitons & Fractals, 25(2):449­460, 2005. [34] Computation was performed in MatLab that implements IEEE Standard 754 for double-precision binary floatingpoint format. [35] A pre-2006 New Zealand 50 cent coin. [36] James E Gentle. Random number generation and Monte Carlo methods, volume 381. Springer, 2003. [37] Matthew D Parno and Youssef M Marzouk. Transport map accelerated Markov chain Monte Carlo. SIAM/ASA Journal on Uncertainty Quantification, 6(2):645­682, 2018. [38] D. E. Varberg. Change of variables in multiple integrals. The American Mathematical Monthly, 78(1):42­45, 1971.

