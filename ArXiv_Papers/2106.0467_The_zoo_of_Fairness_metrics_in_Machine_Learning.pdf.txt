The zoo of Fairness metrics in Machine Learning
Alessandro Castelnovoa,1, Riccardo Crupia,1, Greta Grecoa,1, Daniele Regolia,1
aData Science and Artificial Intelligence, Intesa Sanpaolo S.p.A., Torino, Italy

arXiv:2106.00467v1 [cs.LG] 1 Jun 2021

Abstract
In the recent years, the problem of addressing fairness in Machine Learning (ML) and automatic decision-making has attracted a lot of attention in the scientific communities dealing with Artificial Intelligence. A plethora of different definitions of fairness in ML have been proposed, that consider different notions of what is a "fair decision" in situations impacting individuals in the population. The precise differences, implications and "orthogonality" between these notions have not yet been fully analyzed in the literature. In this work, we try to make some order out of this zoo of definitions.
Keywords: Machine learning; Fairness

Introduction
The issue of discrimination bias in Machine Learning (ML) has gained a lot of momentum in the scientific community in the last few years.
It is now widely recognized that data-driven decision making is not per se safe from producing unfair or biased decisions, either for amplifications of biases already present in the data they learn from, or for algorithmic inaccuracies (see, among others, Barocas and Selbst (2016); Angwin et al. (2016); O'neil (2016)).
The scientific literature addressing the problem of fairness in ML has focused in the recent years on two main aspects: i) how to measure and assess fairness (or, equivalently, bias), and ii) how to mitigate bias in models when necessary.
Regarding the first aspect, in the last few years an incredible number of definitions have been proposed, formalizing different perspectives from which to assess and monitor fairness in decision making processes. A popular tutorial presented at the Conference on Fairness, Accountability, and Transparency in 2018 was titled "21 fairness definitions and their politics" (Narayanan, 2018). The number has grown since then.
The proliferation of fairness definitions is not per se an issue: it reflects the evidence that fairness is not a precise concept, and concentrates on itself different meanings and nuances, in turn depending in complex ways on the specific situation considered.
Researches, in proposing definitions, have focused on different intuitive notions of "unfair decisions", often considered as the ones impacting people in different ways on
1The views and opinions expressed are those of the authors and do not necessarily reflect the views of Intesa Sanpaolo, its affiliates or its employees.
Preprint submitted to Elsevier

the basis of some personal characteristics, such as gender, ethnicity, age, sexual or political or religious orientations, considered to be protected, or sensitive. Relationships and interdependence of these sensitive variables with other features useful for making decisions is one of the crucial points, that entangles in complex ways the ultimate aim of any model, i.e. making efficient decisions, and the desired goal of not allowing unfair discrimination to impact people's lives.
Fairness notions proposed in the literature are usually classified in broad areas, such as: definitions based on parity of statistical metrics across groups identified by different values in protected attributes (e.g. male and female individuals, or people in different age groups); definitions focusing on preventing different treatment for individuals considered similar with respect to a specific task; definitions advocating the necessity of finding and employing causality among variables in order to really disentangle unfair impacts on decisions.
These three broad classes can be further seen as the result of two important distinctions:
1. observational vs. causality-based criteria; 2. group (or statistical) vs. individual (or similarity-
based) criteria.
Distinction 1. discriminates criteria that are purely based on observational distribution of the data from criteria that try to first unveil causal relationships among the variables at play (mainly through a mixture of domain knowledge and opportune inference techniques) and then assess fairness in the specific situation.
Distinction 2. discriminates criteria that focus on equality of treatment among groups of people from criteria requiring equality of treatment among couples of similar individuals.
June 2, 2021

Regarding aspect ii), i.e. that of removing bias when present, several approaches have been introduced in order to provide bias mitigation in data-driven decisions. Roughly speaking, they fall into three families, depending on the specific point along the ML pipeline in which they operate: pre-processing methods, that try to remove bias directly from data; in-processing techniques, imposing fairness either as a constraint or as an additional loss term during training optimization; post-processing methods, working directly on the outcomes of the model.
The present work focuses on the first of these aspects, and in particular deals with the aforementioned fact that the number of different metrics for fairness introduced in the literature has boomed. The researcher or practitioner first approaching this facet of ML may easily feel confused and somehow lost in this zoo of definitions. These multiple definitions capture different aspects of the concept of fairness but, at the best of our knowledge, there is still no clear understanding of the overall landscape where these metrics live. This work aims to take a step in the direction of analyzing the relationship among the metrics, and trying to put order in the fairness landscape. Table 1 provide a rough schematic list of fairness metrics discussed throughout the paper.
As general references for the definition of fairness in ML we refer to the book Barocas et al. (2019) and to the papers Verma and Rubin (2018); Mitchell et al. (2018); Oneto and Chiappa (2020) for more compact surveys. Chouldechova and Roth (2018, 2020) provide a general view of the state of the art of fairness in ML.
In this work, we shall deal with (binary) classification problems, and we shall only make a brief reference to the problem of multiple sensitive attributes: despite this huge simplifications, the landscape of fairness definitions is nevertheless extremely rich and complex.
The rest of this paper is organized as follows: section 1 proposes an account of the most important sources of bias in ML models; section 2 introduces individual fairness; section 3 is devoted to the most prolific set of definitions, namely the group (or statistical) metrics; causality-based criteria are discussed in section 4.

the model learns from biased sampling from the population, and historical or societal biases, i.e. when the model learns from data where decisions are already impacted by past prejudice.
1.1. Statistical bias
Generally speaking, statistical bias occurs whenever the data used for model training are not representative of the true population. This can be due to a form of selection bias, i.e. when the individuals appearing in the data come from a non-random selection of the full population. This happens, for example, in the context of credit lending, where the information of the repayment is known only for people that were granted the loan.
Another way in which statistical bias can enter the data is via systematic measurement errors. This happens when the record of past errors and performance is systematically distorted, especially in the case of different amount of distortion for different groups of people.
Similarly, it may happen that data are systematically missing or poorly recorded for entire strata of the population.
One aspect of paramount importance, but often omitted, is the following: in the context of fairness, even in samples perfectly representative of the population it may be that some protected group happens to be a minority group. All in all, minorities exist. In this case, especially when the minority group is highly unbalanced, a ML model can learn less accurately or may learn to discard errors on that group simply because it is less important in terms of crude counting. Again, notice that this is not a form of statistical bias tout court, since in this case the "underrepresentativeness" of the protected group is a true feature of the population. Nevertheless, this is something very common, and one of the most important source of unfair discrimination against protected groups that happen to be minorities (Hardt, 2014).
This last issue may be reasonably considered as a source of biased decisions due to poor modeling rather than to a problem in data. The boundary is subtle, and in any case this is a matter of nomenclature: the important thing is to be aware of the risk.

1. The problem of bias in data-driven decisions
Most sources of bias in data-driven decision making lie in the data itself and in the way in which they are collected. It is out of the scope of this manuscript to list and discuss all the possible shades of biases that can hide in the data, we want nevertheless to say something about the main cases that can be encountered in many real-life scenarios.
The most important qualitative distinction, that we draw from Mitchell et al. (2018) and Barocas and Selbst (2016), lies between statistical or representation biases, i.e. when

1.2. Historical or societal bias
Even when the data are free from statistical bias, i.e. they truly represent the population, take into account minorities and there is no systematic error in recording, still it may be that bias exists simply because data reflect biased decisions.
In most cases, this is due to a form of labelling bias, i.e. a systematic favour/disfavour towards groups of people at the moment of creating the target variable from which the model is going to learn. If the recorded outcomes are somehow due to human decisions (e.g. a model for granting loans may be trained on loan officers' past decisions),

2

Table 1: Fairness metrics. Qualitative schema of the most important fairness metrics discussed throughout the paper.

notion

use of Y

condition

group fairness

Demographic Parity

-

Conditional Demographic Parity

-

error parity

Equal Accuracy Equality of Odds Predictive Parity

equal acceptance rate across groups equal acceptance rate across groups
in any strata equal accuracy across groups
equal FPR and FNR across groups
equal precision across groups

individual fairness

FTU/Blindness Fairness Through Awareness

-

no explicit use of sensitive attributes

-

similar people are given similar decisions

an individual would have been given

causality-based fairness

Counterfactual Fairness
path-specific Counterfactual Fairness

-

the same decision if she had had different

values in sensitive attributes

-

same as above, but keeping fixed some specific attributes

 there are exceptions to these cases where Y is actually employed, e.g. CDP conditioning on Y becomes Equality of Odds, and there are notions of individual fairness that use a similarity metric defined on the target space (Berk et al., 2017).

then we cannot in general trust their objectiveness and "fairness".
Other forms of historical bias may be even more radical: gender bias has a rather long history, and is embedded in all sorts of characteristics and features in such a way that it is difficult evaluate its impact and disentangle its dependence on other variables. Think for example of income or profession disparities, just to name a few out of many. Thus, this is a situation in which long-lasting biases cause systematic differences in features pertaining different groups of people. Again, this is not a form of unrepresentativeness of the sample, it is a bias present in the full population.

for reference we shall take to be gender, we label with X all the other (non-sensitive) random variables that the decision-maker is going to use to provide its yes/no decisions Y^ = f (X, A)  {0, 1}; while we label Y  {0, 1} the ground truth target variable that needs to be estimated/predicted ­ typically by minimizing some loss function L(Y, Y^ ). X = (X, A) collectively represent all the features of the problem. We denote with lowercase letters the specific realizations of random variables, e.g. {(x1, y1), . . . , (xn, yn)} represent a dataset of n independent realizations of (X, Y ). We employ calligraphic symbols to refer to domain spaces, namely X denotes the space where features X live2.

Finally, this is not meant to be an exhaustive account, bias can lurk in the data in a variety of additional ways, e.g. a poor selection of features may result in a loss of important information in disproportionate ways across groups (Barocas and Selbst, 2016), and many others. We refer the interested reader to Barocas and Selbst (2016); Barocas et al. (2019); Mehrabi et al. (2019); Hardt (2014), and references therein.

2.2. Similarity-based criteria
Individual fairness is embodied in the following principle: similar individuals should be given similar decisions. This principle deals with the comparison of single individuals rather than focusing on groups of people sharing some characteristics. On the other hand, group fairness starts from the idea that there are groups of people potentially suffering biases and unfair decisions, and thus tries to reach equality of treatment for groups instead of individuals.

2. Individual fairness
2.1. Toy examples and notation To be as clear as possible, in what follows we shall make several references to 2 toy examples: credit lending decisions, and job recruiting. We call A the categorical random variable representing the protected attribute, that

The first attempt to deal with a form of individual fairness is in Dwork et al. (2012), where the concept is introduced
2Technically, since we employ uppercase letters to denote random variables, it would be more proper to say that X is the image space of X :   X , where  is the event space, and that x  X . We shall nevertheless use this slight abuse of notation for sake of simplicity.

3

as a Lipschitz condition of the map f from the feature space to the model space:

distY (y^i, y^j) < L × distX (xi, xj),

(1)

where distY and distX denote a suitable distance in target space and feature space, respectively, and L is a constant. Loosely speaking, a small distance in feature space (i.e. similar individuals) must correspond to a small distance in decision space (i.e. similar outcomes). They also propose an approach to enforce such kind of fairness by introducing a constrained optimization at training time, once a distance on the feature space is given.

The concept of individual fairness is straightforward, and it certainly resonates with our intuitive notion of "equality". Formula (1) provides also an easy way to assess whether the decision model Y^ = f (X) satisfies such concept. However, the major drawback of this type of definitions lies in the subtle concept of "similar individuals". Indeed, defining a suitable distance metric distX on feature space embodying the concept of similarity on "ethical" grounds alone is almost as tricky as defining fairness in the first place.

Indeed, while for target space the natural choice distY (Y^i, Y^j) =|Y^i - Y^j| will do, for feature space the natural choice of the euclidean distance on the space A × X implies that any smooth function f shall satisfy condition (1), but this is not a very interesting notion of fairness. The point is that one should come up with a distance that captures the essential features for determining that target, and that do not mix with sensitive attributes. But, again, this is not very different from defining what is fairness in a specific situation.

i.e. there is disparate treatment whenever two individuals sharing the same values of non-sensitive features but differing on the sensitive ones are treated differently (Barocas and Selbst, 2016; Zafar et al., 2017).
Unfortunately, despite its compelling simplicity, fairness through unawareness comes not without flaws. Firstly, if it is true that reaching FTU is straightforward, it is not that easy to assess. The problem of bias assessment in a model consists in measuring whether the model decisions are biased given a set of realizations of (A, X, Y^ ), namely {(a1, x1, y^1), . . . , (an, xn, y^n)} ­ the corresponding values of Y are needed as well for some criteria. In this setting, it is tricky to measure whether FTU holds, the main reason being that it is more a request on how the model works rather than a request on properties of the output decisions. One possible candidate metric is the following:





consistency

=

1-

1 n



n

y^i

-

1 k

y^j  , (2)

i=1

xj kN N (xi)

introduced in Zemel et al. (2013). Namely, for each observation (xi, y^i) it measures how much the decision y^i is close to the decisions given to its k nearest neighbors kN N (xi) in the X space3. Notice that it may happen that the k neighbors of, say, a male individual are all males: in this case consistency (2) would in fact be equal to 1, but this does not prevent the model from explicitly using A in making decisions. Another possibility, partly inspired by Berk (2009), would be to compute

1 n1n2

e-dist(xi,xj )|y^i
ai =1,

-

y^j |,

(3)

aj =0

Take, e.g., the job recruiting framework: what identifies the couples of individuals that should be considered similar and thus given the same chance of being recruited? Maybe the ones with same level of skills and experience irrespective of anything else?
One possibility is to define similar individuals as couples belonging to different groups with respect to sensitive features (e.g. male and female) but with the same values for all the other features. With this choice, what we are requiring is that its outcome should be unchanged if we take an observation and we only change its protected attribute A. This concept is usually referred to as Fairness Through Unawareness (FTU) or blindness (Verma and Rubin, 2018), and it is expressed as the requirement of not explicitly employing protected attributes in making decisions.

which measures the difference in decisions among men and

women weighted by their similarity in feature space: the

higher its value the higher the difference in treatment for

couples of similar males and females.4 On the other hand,

it is much easier to assess FTU if we also have access to

the model: we could create a synthetic dataset by flipping

A5: {(a1, x1), . . . , (an, xn)}, feeding it to the model to get

the the

corresponding outcomes

average

1 n

n i=1

|y^i

-

y^i|.

y^1, .

.

.,

y^n,

and

then

compute

However, the main drawback of FTU is the following: it does not take into account the possible interdependence between A and X. Other features may contain information on the sensitive attribute, thus explicitly removing the sensitive attribute is not sufficient to remove its information from the dataset. Namely, it may be that in

Notice that this idea is very likely one of the first that one may think of when asked for, e.g., a decision-making process that does not discriminate against gender: not to explicitly use gender to make decisions, i.e. Y^ = f (X). Indeed, this concept is also referred to as disparate treatment,

3Notice that also in the computation of kN N one has to choose
a distance function on X . 4The term e-dist(xi,xj ) can be substituted with any measure of
similarity of the points xi and xj . 5This holds if A is binary. But it is easy to come up with gener-
alizations to the multiclass case.

4

the actual dataset there is a very low chance that a male and a female have similar values in all the (other) features, since gender is correlated with some of them. One may or may not decide that (some of) these correlations are legitimate6. This is one of the reasons why the definition of a issue-specific distance is crucial for a more refined notion of individual fairness.
One straightforward way to deal with correlations is to develop a model that is blind not only with respect to the sensitive attribute, but also to all the other variables with sufficiently high correlation with it. This is a method known as suppression (Kamiran and Calders, 2012). Apart from the obvious issue in defining a good threshold for correlation above which a predictor should be removed, this approach has the main drawback in the potentially huge loss of legitimate information that may reside in features correlated with the sensitive attribute.
One line of research has tried to solve this issue by "cleaning" the training dataset in order to remove both the sensitive attribute and the information coming from the sensitive attribute "lurking" inside other features, while keeping the features. This can be done, e.g., by using residuals of regressions of the (correlated) features on the sensitive attribute (Berk, 2009; Johndrow et al., 2019), i.e. projecting the feature space in a subspace orthogonal to A. However, it is tricky to account for dependencies on feature interactions. Another way of keeping information while removing "unfairness" due to the sensitive attribute is to learn a fair representation of the training dataset, see Zemel et al. (2013); Louizos et al. (2015); McNamara et al. (2017); Calmon et al. (2017), i.e. to learn a new set of variables Z, such that they are able to reconstruct X with as few errors as possible, while at the same time being as independent as possible of A7. The idea is then that one can use any ML model on this "cleaned" representation of the original dataset and this will be fair by design.
Notice, however, that in most situations, when trying to remove from the data the dependence of A and X, individual fairness will be spoiled. We shall discuss this in section 3.6.
Counterfactual frameworks (Kusner et al., 2017; Chiappa, 2019), that will be more thoroughly discussed in section 4, provide a clear way in which this similarity should be thought of: a male individual is similar to himself in the counterfactual world where he is a woman. Notice that this is crucially different from fairness through unawareness approach: a male individual transported in the counterfactual world where he is a woman will have differ-
6E.g. gender may be correlated with income, but, depending on the problem, one may decide that the use of income, even if correlated with gender, is not a source of unfair discrimination. In section 4 we will dwell more on this issue, namely that there may be some information correlated to the sensitive attribute but still considered "fair".
7In the sense that it is hard to reconstruct A from Z.

ences in other features as well, and such differences are precisely due to the causal structure among the variables (very roughly speaking, this is the "causal way" to account for correlations). As an example, in the now popular 1973 Berkeley admission case (Bickel et al., 1975) where there is different admission rate between men and women, being a woman "causes" the choice of higher demanding departments, thus impacting the admission rate (in causal theory jargon, department choice is said to be a mediator from gender to admission). In this case, a male transported in the counterfactual world where he is a woman would have himself chosen higher demanding departments, thus this feature would change as well. In this case, a simple gender-blind model would not guarantee individual fairness. Indeed attribute flipping in general does not produce valid counterfactuals. More details on this will be given in section 4. We refer to Barocas et al. (2019) for a thorough discussion on the Berkeley admission case from the point of view of causal reasoning.
Turning to the general similarity-based definition (1), some work has been done to address the problem of finding a suitable similarity metrics in feature space. E.g. Dwork et al. (2012) and Jung et al. (2019); Ilvento (2019) introduce the possibility to learn a issue-specific distance from data and from the contributions of domain experts. Indeed, the simple idea of using standard similarities, e.g. related to the euclidean distance on feature space, does not take into account the trivial fact that some feature are more important than other in determining the relationship of an individual to specific target. Namely, for two applicants for a loan, the difference in income is much more important than the difference in, say, age, or even profession. Thus, judging what does it mean to be similar with respect to a specific task is not that simple, and is in some sense connected also to the ground truth target variable Y.
Indeed, Berk et al. (2017) propose a notion of individual fairness as a penalty function ­ to be added to the risk minimization fitting ­ which shifts the concept of similarity from the feature space to the target space: namely two individuals are deemed similar if they have a similar value of the target variable. Thus, this notion of individual fairness relies directly on the target attribute to define a task-specific distance. Of course, this definition is prone to biases possibly present in the target.
Broadly speaking, the Lipschitz notion (1) is sometimes referred to as Fairness Through Awareness (FTA), as opposed to Fairness Through Unawareness: in fact, even if they share the same principle of treating equally similar individuals, FTA is generally meant to use similarity metrics that are problem and target specific, i.e. that derive from an "awareness" of the possible impact, while FTU is a simple recipe that does not depend on the actual scenario.

5

3. Group fairness
Group fairness criteria are typically expressed as conditional independence statements among the relevant variables in the problem.
There are three main broad notions of observational group fairness, independence, separation, sufficiency (Barocas et al., 2019). Independence is strictly linked to what is known as Demographic Parity or Statistical Parity, separation is related to Equality of Odds and its relaxed versions, while sufficiency is connected to the concept of calibration and Predictive Parity.
There is a crucial aspect that discriminates independence criteria from the others: independence criteria rely only on the distribution of features and decisions, namely on (A, X, Y^ ), while separation and sufficiency criteria make use of the target variable Y as well. This is an important thing to have in mind when trying to find your way in the zoo of fairness criteria in a specific case study: when using separation-based or sufficiency-based criteria one must be careful to check whether the target variable Y is itself prone to sources of bias and unfairness.
In the example of credit lending, if Y represents the decision of a loan officer, than separation-based and sufficiency-based criteria must be used with particular care, since Y can be itself biased against some groups of people.
Actually, even if Y stands for the repayment (or lack of it) of the loan, a form of selection bias is very likely at work: we have that information only on applicants that received the loan in the first place, and these are almost surely not representative of the whole population of applicants.

3.1. Independence

The criterion of independence (Barocas et al., 2019) states

that the decisions should be independent of any sensitive

attribute:

Y^  A.

(4)

This can be also expressed as follows:

P (Y^ = 1 | A = a) = P (Y^ = 1 | A = b), a, b  A, (5)

i.e. the ratio of loans granted to men should be equal to the ratio of loans granted to women.
The ratio of favorite outcomes is sometimes known as positive prediction ratio (ppr), thus independence is equivalent to requiring the same positive prediction ratio across groups identified by the sensitive features. This form of independence is usually known as Demographic Parity (DP), statistical parity, or sometimes as group fairness.8

Figure 1: example of demographic parity in gender in credit lending toy model.
Figure 1 shows a very simple visualization of a model reaching demographic parity among men and women.
If some group has a significantly lower positive prediction ratio with respect to others, we say that demographic parity is not satisfied by the model decisions Y^ . In order to have a single number summarizing the amount of disparity, it is common to use either maximum possible difference or the minimum possible ratio of positive prediction ratios: a difference close to 0 or a ratio close to 1 indicate a decision system fair with respect to A in the sense of DP. Typically, some tolerance is considered by employing a threshold below (or above) which the decisions are still considered acceptable.
3.1.1. Subtleties of demographic parity The meaning of DP is intuitive only to a superficial analysis. For example, one may at first think that removing the sensitive attribute from the decision making process is enough to guarantee independence and thus demographic parity.
But, in general, this is not the case. Take the credit lending example and assume that, for whatever reason, women tend to actually pay back their loans with higher probability than men. If this is the case, it is reasonable to assume that a rating variable that we call R will be higher for women than for men. In this scenario, the sensitive attribute gender (A) is correlated with rating. If the model uses only rating (but not gender) to compute its decision, there will be a higher rate of loans granted to women than to men, resulting in a demographic disparity among these groups. In this case, if you want to reach DP, the model needs to favor men over women, granting loans to men with a lower rating threshold with respect to women.

8Demographic parity is a very common concept in the literature on fairness in ML. Kamiran and Calders (2009) provides one of the first mathematical formulation (even if without actually using the

now common nomenclature). We refer to Barocas et al. (2019) and Chouldechova (2017) for a general account.

6

We try to summarize in a non-exhaustive list, a set of scenarios in which it might be reasonable to take into account demographic parity, among other metrics:

· when you want to actively enforce some form of equality between groups, irrespective of all other information. Indeed there are some characteristics that are widely recognized to be independent in principle of sensitive attributes, e.g. intelligence and talent, and there may be the need to enforce an independence in problems where the decisions is fundamentally linked to those characteristics; more in general, there may be reasons to consider unfair any relation among A and Y , even if the data (objective data as well) is telling differently;

Figure 2: example of a subtlety of demographic parity: in order to reach demographic parity between men and women and still using rating as fundamental feature, one must use a different threshold between the groups, thus manifestly treating differently men and women.
Thus, because of interdependence of X and A, not only it is not enough to remove the sensitive attribute from the decision making process, but if you want to have DP you need, in general, to treat different groups in different ways, precisely in order to compensate for the (unwanted) effect of this dependence. This is somehow the opposite of an intuitive notion of fairness!
Figure 2 displays this example: to reach equal ppr, one must use a different rating threshold for each group.
This, in turn, reveals another subtlety: even if your dataset and your setting does not apparently contain any sensitive features, discrimination could lurk in via correlations to sensitive features that you are not even collecting.
We want to stress here what we think is a crucial aspect: another possibility to reach DP would be to use neither gender nor rating in making decisions, i.e. trying to remove all gender information from the dataset. Notice that this could be problematic for the accuracy of your decisions, since it's plausible that, by removing all variables correlated to A, information useful to estimate the target is lost as well. This is called suppression, and was discussed already in section 2, together with the concept of fair representation, by which one tries to remove all sensitive information from the dataset while keeping as much useful information as possible.
Another possible flaw is the following: if it is true that women repay their loans with higher probability, is it really fair to have demographic parity between men and women? Why the bank should agree to grant loans with the same rate to groups that actually pays back with different probabilities? Should we stick to actual repayment rates or we should ask why these probabilities are different and if this is possibly due to gender discrimination in the first place?

· (somehow similar to the previous) when you deem that in your specific problem there are hidden historical biases that impact in a complex way the entire dataset;
· when you cannot trust the objectivity of the target variable Y , then demographic parity still makes sense, while other metrics don't (e.g. separation).

3.1.2. Conditional demographic parity
Another version of the independence criteria is the one of Conditional Demographic Parity (CDP), first introduced in Kamiran et al. (2013). In the example given above, we may think that a fairer thing to do, with respect to full independence, is to require independence of the decision on gender only for men and women with the same level of rating. In other words, if a man and a woman have both a certain level of rating, we want them to have the same chance of getting the loan. This goes somewhat in the direction of being an individual form of fairness requirement, since parity is assessed into smaller groups with respect to the entire sample.

Formally, this results in requiring Y^ independent of A

given R,

Y^  A | R,

(6)

or, in other terms:

P (Y^ = 1 | A = a, R = r) = P (Y^ = 1 | A = b, R = r), a, b  A, r. (7)

This seems a very reasonable requirement in many reallife scenarios. For example, if you think of a recruitment setting where you don't want to bias women against men, but still you want to recruit the most skilled candidates, you may require your decision to be independent of gender but conditional on a score based on the curriculum and past work experiences: among people with an "equivalent" set of skills, you want to recruit men and women with the same rate. In other words, the only disparities that you are willing

7

to accept between male and female candidates are those 2019):

justified by curriculum and experience.

Y^  A | Y.

(10)

Unfortunately, also in this case one must be really careful, because the variable that you are conditioning on might itself be a source of unfair discrimination. For example, it might well be that rating is higher for women not because they actually pay back their debts more likely than men, but because the rating system is biased against men. And this may be due to self-fulfilling prophecy: if men have lower rating they may receive loans with higher interest rates, and thus have a higher probability of not paying them back, in a self-reinforcing loop.
Moreover, it may be not be so straightforward to select the variables to condition on: why condition on rating and not, e.g., on level of income, or profession, etc...?

In other terms
P (Y^ = 1 | A = a, Y = y) = P (Y^ = 1 | A = b, Y = y), a, b  A, y  {0, 1}. (11)
Equivalently, disparities in groups with different values of A (male and female) should be completely justified by the value of Y (repayment or not).
As in the conditional independence case, this seems a very reasonable fairness requirement, provided that you can completely trust the target variable. Namely, one should be extremely careful to check whether the target Y is not itself a source of bias.

Finally, in line with what outlined above for demographic parity, one may argue that women's curricula and work experiences tend to be on average different than those of male candidates for historical reasons and for a long-lasting (and die-hard) man-centered society. This may suggest that plain demographic parity could be more appropriately enforced in this case to reach a "true" equality.

For example, if Y instead of reflecting true repayment was the outcome of loan officers' decision on whether to grant the loans, it could incorporate bias, thus we it would be risky to assess fairness with direct comparisons with Y . Moreover, as we said above, even in the objective case where Y is the actual repayment, a form of selection bias would likely distort the rate of repayment.

Incidentally, notice that pushing to the extreme the notion of conditional demographic parity, i.e. conditioning on all the (non-sensitive) variables, one has

Y^  A | X,

(8)

i.e.

P (Y^ = 1 | A = a, X = x) = P (Y^ = 1 | A = b, X = x), a, b  A, x  X ; (9)

We can express separation in terms of what are known in statistics as type I and type II errors. Indeed, it is easy to see that the two conditions in equation (11) (one for y = 1 and one for y = 0) are equivalent to requiring that the model has the same false positive rate and false negative rate across groups identified via A. False positives and false negatives are precisely type I and type II errors, respectively. Namely, individuals that are granted loans but are not able to repay, and individuals that are able to repay but are not granted loans.

meaning that a male and a female with the same value for all the other features must be given the same outcome. This criterion is reachable by a gender-blind model, thus is strictly connected to the notion of FTU. We can indeed notice ­ but leave the details to section 3.6 ­ that conditioning is equivalent to restrict the groups of people among which we require parity, thus is a way to go in the direction of obtaining an individual criterion.

This is known as Equality of Odds (Hardt et al., 2016), and is thus the requirement of having the same type I and type II error rates across relevant groups, as displayed in Figure 3.
There are two relaxed version of this criterion:
· Predictive Equality: equality of false positive rate across groups,

3.2. Separation
Independence and conditional independence do not make use of the true target Y . What if, instead of conditioning over rating R we condition on the target Y ?
This is equivalent to requiring the independence of the decision Y^ and gender A separately for individuals that actually repay their debt and for individuals that don't. Namely, among people that repay their debt (or don't), we want to have the same rate of loan granting for men and women.
This concept has been called separation (Barocas et al.,

P (Y^ = 1 | A = a, Y = 0) = P (Y^ = 1 | A = b, Y = 0), a, b  A,
· Equality of Opportunity: equality of false negative rate across groups,
P (Y^ = 0 | A = a, Y = 1) = P (Y^ = 0 | A = b, Y = 1), a, b  A.
While demographic parity, and independence in general, focuses on equality in terms of acceptance rate (loan grating rate), Equality of Odds, and separation in general,

8

take into account the other side of the coin, i.e. parity given the model decision.

3.3. Sufficiency
Sufficiency (Barocas et al., 2019) takes the perspective of people that are given the same model decision, and requires parity among them irrespective of sensitive features.

While separation deals with error rates in terms of fraction of errors over the ground truth, e.g. the number of individuals whose loan request is denied among those who would have repaid, sufficiency takes into account the number of individuals who won't repay among those who are given the loan.

Mathematically speaking, this is the same distinction you have between recall (or true positive rate) and precision, i.e. P (Y^ = 1 | Y = 1) and P (Y = 1 | Y^ = 1), respectively.

Figure 3: Example of Equality of Odds between men and women: false negative and false positve rates must be equal across groups.
focuses on equality in terms of error rate: the model is fair if it is as efficient in one group as it is in the other. As remarked above, in order to assess fairness in terms of model errors, one needs to trust the ground truth Y .

A fairness criterion that focuses on this type of error rate is called Predictive Parity (Chouldechova, 2017), also referred to as outcome test (Verma and Rubin, 2018; Mitchell et al., 2018):
P (Y = 1 | A = a, Y^ = 1) = P (Y = 1 | A = b, Y^ = 1), a, b  A, (12)

The difference between Predictive Equality and Equality of Opportunity is the perspective from which equality is required: Predictive Equality takes the perspective of people that won't repay the loan, while Equality of Opportunity takes the one of people that will repay. Depending on the problem at hand, one may consider either of these two perspectives as more important. For example, Predictive Equality may be considered when we want to minimize the risk of innocent people from being erroneously arrested: in this case it may be reasonable to focus on the parity of among innocents. Both in the credit lending and job listing examples, on the other hand, it may be reasonable to focus on Equality of Opportunity, i.e. on the parity among people that are indeed deserving.
Here follows a non exhaustive set of situations in which separation criteria may be suitable:
· when your target variable Y is an objective ground truth;
· when you are willing to make discrimination as long as they are justified by actual trustable data;
· when you do not want to actively enforce an "ideal" form of equality, and you want to be as equal as possible given the data.
We can summarize by saying that that separation, being a concept of parity given the ground truth outcome, is a notion that takes the point of view of people that are subject to the model decisions, rather than that of the decision maker. In the next subsection, instead, we shall

i.e. the model should have the same precision across sensitive groups. If we require condition (12) to hold for the case Y = 0 as well, then we get the following conditional independence statement:
Y  A | Y^ ,
which is referred to as sufficiency (Barocas et al., 2019).
Predictive Parity, and its more general form of sufficiency, focuses on error parity among people who are given the same decision. In this respect, Predictive Parity takes the perspective of the decision maker, since they group people with respect to the decisions rather than the true outcomes. Taking the credit lending example, the decision maker is indeed more in control of sufficiency rather than separation, since parity given decision is something directly accessible, while parity given truth is known only in retrospect. Moreover, as we have discussed above, the group of people who are given the loan (Y^ = 1) is less prone to selection bias than the group of people who repay the loan (Y = 1): indeed we can only have the information of repayment for the Y^ = 1 group, but we know nothing about all the others (Y^ = 0).
As you may notice, going along a similar reasoning, one can define other group metrics, such as Equality of Accuracy across groups: P (Y^ = Y | A = a) = P (Y^ = Y | A = b), for all a, b  A, i.e. focusing on over unconditional errors, and others (Verma and Rubin, 2018).
Zafar et al. (2017) introduce the notion of disparate mistreatment to refer to all group fairness criteria relying on

9

disparity of errors, thus, in general, all group metrics dealing with comparisons among decisions Y^ and true outcomes Y .
3.4. Group fairness on scores
In most cases, even in classification setting, the actual output of a model is not a binary value, but rather a score S  R, estimating the probability, for each observation, to have the target equal to the favorable outcome (usually labelled 1). Then, the final decision is made by the following t-threshold rule:

when

P (Y = 1 | S = s) = s,

(16)

i.e. if the model assigns a score s to 100 people then, on average, 100 × s of them will actually be positive. Writing down the condition for sufficiency in score:

P (Y = 1 | S = s, A = a) = P (Y = 1 | S = s, A = b), a, b  A, s, (17)
it is clearly related to what can be called Calibration within Groups (Kleinberg et al., 2016):

Y^ = 1 S  t,

(13)

0 S < t.

Most of the things we have outlined for group fairness metrics regarding Y^ can be formulated for the joint distribution (A, Y, S) as well:

independence S  A,

separation S  A | Y,

(14)

sufficiency Y  A | S.

These formulations provide stronger constraints on the model with respect to the analogous with Y^ , e.g. Jiang et al. (2020); Oneto and Chiappa (2020) call the condition S  A strong demographic parity.

P (Y = 1 | A = a, S = s) = s

(18)

­ actually a consequence of it. This condition is in general not so hard to achieve, and it is often satisfied "for free" by most models. See Barocas et al. (2019) for more details.

3.5. Incompatibility statements
It is interesting to analyse the relationship among different criteria of fairness. In the next subsection 3.6 we shall discuss in detail the connections between individual and group notions, while here we focus on differences among various group criteria. We have already seen that each of them highlights one specific aspect of an overall idea of fairness, and we may wonder what happens if we require to satisfy multiple of them at once. The short answer is that, in general, it is not possible.

For instance, all three criteria in their form (5), (11), (12), i.e. with constraints on the joint distribution (A, Y, Y^ ) can effectively be satisfied by defining group dependent thresholds t on black-box model outcomes S (a technique that goes under the name of postprocessing (Barocas et al., 2019)), while this is not the case for (14).
Instead of requiring conditions on the full distribution of S as in (14), something analogous to the "binary versions" of group fairness criteria have been defined simply by requiring parity of the average score (Kleinberg et al., 2016). Balance of the Negative Class, defined as
E(S | Y = 0, A = a) = E(S | Y = 0, A = b), a, b  A, (15)
corresponds to Predictive Equality, while Balance of the Positive Class (same as (15) with Y = 1) corresponds to Equality of Opportunity. Notice that these two last definitions fall into the one given in subsection 3.2 when S = Y^ . Requiring both balances is of course equivalent to requiring EO.
AUC parity, namely the equality of the area under the ROC for different groups identified by A, can be seen as the analogous of the equality of accuracy.
Finally, notice that, the score formulation of sufficiency is connected to the concept of calibration. Calibration holds

Most incompatibility results have been provided by Chouldechova (2017); Kleinberg et al. (2016); Barocas et al. (2019). We take the following three results from the formulation of Barocas et al. (2019).
1. if Y is binary, Y  S and Y  A, then separation and independence are incompatible. In other words, to achieve both separation and independence, the only possibility is that either the model is completely useless (Y  S), or the outcome is independent of the sensitive attribute (Y  A), which implies an equal base rate for different sensitive groups. Namely, if there is an imbalance in groups identified by A, then you cannot have both EO and DP holding.
2. Analogously: if Y  A, then sufficiency and independence cannot hold simultaneously. Thus, if there is an imbalance in groups identified by A, then you cannot impose both sufficiency and independence.
3. Finally: if Y  A and the distribution (A, S, Y ) is strictly positive, then separation and sufficiency are incompatible. Meaning that separation and sufficiency can both hold either when there is an imbalance in sensitive groups, or when, given a certain value of A and given the score S, the probability of having Y = 0 or Y = 1 (for binary targets) are both non-null, i.e. when the score never exactly resolves the true target.

10

Kleinberg et al. (2016) prove that Balance of Positive Class, Balance of Negative Class and Calibration within Groups can hold together only if either there is no imbalance in groups identified by A or if each individual is given a perfect prediction (i.e. S = 0, 1 everywhere).
Chouldechova (2017) focuses on the COMPAS recidivism case, now popular in the fairness literature. Indeed, the debate on this case is a perfect example to highlight the fact that there are different and non-compatible notions of fairness, and that this may have concrete consequences on people. While we refer to Washington (2018) and Chouldechova (2017) for a thorough discussion of the COMPAS case, we here just point out that in the debate there were two parties, one stating that the model predicting recidivism was fair since it satisfied Predictive Parity by ethnicity, while the other claiming it was unfair since it had different false positive and false negative rates for black and white individuals. Chouldechova (2017) showed that, if Y  A, i.e. if the true recidivism rate is different for black and white people, then Predictive Parity and Equality of Odds cannot both hold, thus implying that a reflection on which of the two (in general of the many) notions is more important to be pursued in that specific case must be carefully considered.
Summarizing, apart from trivial or peculiar scenarios, the three families of group criteria are not mutually compatible.
3.6. Group vs individual
The most common issue with group fairness definitions is the following: since group fairness requires to satisfy conditions only on average among groups, it leaves room to bias discrimination inside the groups. As we argued in the example of section 3 referring to figure 2, one way of reaching DP is to use a different rating threshold for men and women: this means that there will be a certain range of ratings for which men will receive the loan, and women won't. More formally, conditionally on rating there is no independence among A and Y^ . In general, to reach group fairness one may "fine tune" the interdependence of A and X to reach parity on average, but effectively producing differences in subgroups of A.

(X, A), i.e. minimizing the risk EL(f (X, A), Y ), then it is unavoidable to have some form of disparate treatment among people in different groups with respect to A whenever X  A. This has been thoroughly discussed by Dwork et al. (2012), where they call fair affirmative action the process of requiring DP while trying to keep as low as possible the amount of disparate treatment between people having similar X.
To clarify the general picture, we can put the different notions of (observational) fairness in a plane with two qualitative dimensions (see figure 4): 1. to what extent a model is fair at the individual level, 2. how much information of A is retained in making decisions. The first dimension represents to what extent two individuals with similar overall features X are given similar decisions: the maximum value is reached by models blind on A (FTU). These are the models that are also using all information in X, irrespective of the interdependence with A, thus FTU-compliant models will use all information contained in X apart from the information that is contained in A only. The minimum value in this dimension is reached by models that satisfy DP. Models using suppression methods, being blind to both A and other features with high correlation with A, are individually fair in the sense of preventing disparate treatment. In so doing, they can exploit more or less information of A with respect to general DP-compliant models depending on how many correlated variables are discarded. However, the price to pay for discarding variables is in terms of errors in approximating Y , which is not highlighted in this plot. Notice that, of course, full suppression ­ i.e. removing all variables dependent on A ­ trivially satisfies the condition Y^  A, i.e. it is DP-compliant as well. In other terms, one can have a DP-compliant model that is individual as well. In figure 4, we label with DP a general model that tries to maximize performance while satisfying a DP constraint, without any further consideration.
Models satisfying CDP are somewhat in-between, of course depending on the specific variables considered for conditioning. They guarantee less disparate treatment than unconditional DP, and they use more information of A by controlling for other variables possibly dependent on A.

Notice that this is precisely what individual fairness is about. In the example above, a men and a woman that have the same rating may be treated differently, thus violating the individual notion of fairness.
As already discussed in section 3, of course this is only one possibility: one may as well reach DP by not using neither gender nor rating, and grant loan on the basis of other information, provided it is independent of A. In this case, there will be no group discrimination, but there won't be any subgroup discrimination as well.
However, we can say that, if we want to reach DP by using as much information of Y as possible contained in

Notice that approaches such as fair representation (see section 2), where you try to remove all information of A from X to get new variables Z which are as close as possible to being independent of A, produce decision systems Y^ = f (Z) that are not, in general, individually fair. This is due to the simple fact that, precisely to remove the interdependence of A and X while keeping as much information of X as possible, two individuals with same X and different A will be mapped in two distinct points on Z, thus having, in general, different outcomes. Referring again to the credit lending example, suppose that we have R = g(A) + U , with g a complicated function encoding the interdependence of rating and gender, and U some other

11

information of A used

information of A used

FTU

FTU

CDP

suppression

constant score

suppression

CDP

Y^  A

DP

group

individual

Figure 4: Landscape of observational fairness criteria with respect to the group-vs-individual dimension and the amount of information of A used (via X).

Y^  A

DP

model performance

Figure 5: Landscape of observational fairness criteria with respect to the model performance dimension and the amount of information of A used (via X).

factor independent of A representing other information in R "orthogonal" to A. In this setting, the variable Z that we are looking for is precisely U . Notice that U is indeed independent of A, thus any decision system Y^ = f (U ) satisfies DP, but given two individuals with R = r nothing prevents them from having different U . In other terms, you need to have some amount of disparate treatment to guarantee DP and employ as much information as possible to estimate Y .
Figure 5 shows a qualitative representation of observational metrics with respect to the amount of information of A (through X) that is used by the model, and the predictive performance. Notice that DP can be reached in many ways: e.g. a constant score model, namely a model accepting with the same chance all the individuals irrespective of any feature, is DP-compliant (incidentally, it is also individual), a model in which all the variables dependent on A have been removed (a full suppression), or a model where DP is reached while trying to maximize performance (e.g. through fair representations). All these ways differ in terms of the overall performance of the DPcompliant model.

Generally speaking, all the definitions and results we gave in previous sections are subject to the fact that the sensitive feature A is represented by a single categorical variable. If, for a given problem, we identify more than one characteristic that we need to take into account as sensitive or protected ­ say (A1, . . . , Al) ­ we can easily assess fairness on each of them separately. This approach, that Yang et al. (2020) call independent group fairness, unfortunately is in general not enough: even if fairness is achieved (in whatever sense) separately on each sensitive variable Ai, it may happen that some subgroups given by the intersection of two or more Ai's undergo unfair discrimination with respect to the general population. This is sometimes referred to as intersectional bias, or, more specifically, fairness gerrymandering (Kearns et al., 2018).
To prevent bias from occurring in all the possible subgroups identified by all Ai's one can simply identify a new feature A = (A1, . . . , Al), whose values are the collection of values on all the sensitive attributes, and require fairness constraints on A. Yang et al. (2020) call this intersectional group fairness.

FTU-compliant models, on the other hand, by employing all information in X will be, in general, more efficient in terms of model performance.9
Incidentally, notice that this discussion is to be taken at a qualitative level, one can come up with scenarios in which, e.g., models satisfying DP have higher performances than models FTU-compliant (think, e.g., of a situation in which Y  A and X  A).
3.7. Multiple sensitive features
Even if a detailed discussion of the problem of multiple sensitive features is out of the scope of this manuscript, we shall nevertheless give a brief overview.
9Of course it is understood that the model f in Y^ = f (X) is trained to maximize performance.

This last "trick" indeed solves the problem of intersectional bias, at least theoretically. Still, issues remain at a computational and practical level, whose two main reasons are:
· the exponential increase of the number of subgroups when adding sensitive features,
· the fact that, with finite samples, many of the subgroups will be empty or with very few observations.
These two aspects imply that even assessing (group) fairness with respect to multiple sensitive attributes may be practically unfeasible in many cases. This is a huge problem, that most of the literature on fairness in ML does not address. We refer to Yang et al. (2020); Kearns et al. (2018); Buolamwini and Gebru (2018) for a more detailed discussion.

12

4. Causality-based criteria
Another important distinction of fairness criteria is the one between observational and causality-based criteria.
As we have seen, observational criteria rely only on observed realizations of the distribution of data and predictions. In fact, they focus on enforcing equal metrics (acceptance rate, error rate, etc. . . ) for different groups of people. In this respect, they don't make further assumptions on the mechanism generating the data and suggest to assess fairness through statistical computation on observed data.

In words, if you take a random individual with X = x and force it to be, e.g., female (do(A = a)), you want to give him the same chance of acceptance as for a random individual with X = x forced to be male (do(A = b)). We refer to it as Intervention Fairness.
The same requirement can be set at the counterfactual level, and is known as Counterfactual Fairness (CFF) (Kusner et al., 2017):
P (Y^Aa = 1 | A = a, X = x) = P (Y^Ab = 1 | A = a, X = x), a, b  A, (20)

Causality-based criteria, on the other hand, try to employ domain and expert knowledge in order to come up with a casual structure of the problem, through which it becomes possible to answer questions like "what would have been the decision if that individual had a different gender?".
While counterfactual questions like this seem in general closer to what one may intuitively think of as "fairness assessment", the observational framework is on the one hand easier to assess and constrain on, and on the other more robust, since counterfactual criteria are subject to strong assumptions about the data and the underlying mechanism generating them, some of which are not even falsifiable.
As we argued above (section 2), answering to counterfactual questions is very different from taking the feature vector of, e.g., a male individual and just flip the gender label and see the consequences in the outcome. The difference lies precisely in the causal chain of "events" that this flip would trigger. If there are some features related, e.g., to the length of the hair, or the height, then it is pretty obvious that the flip of gender should come together with a change in these two variables as well. And this may be the case for other, less obvious but more important, variables.
This also suggests why counterfactual statements involve causality relationships among the variables. In general, to answer counterfactual questions, one needs to know the causal links underlying the problem. This requires a certain number of assumptions, usually driven by domain knowledge.
However, as major drawback, once given the casual structure there are many counterfactual models compatible with that structure (actually infinite), and the choice of one of them is not falsifiable in any way. Indeed, causalitybased criteria can be formulated at least in two different levels: at the level of interventions and at the level of counterfactuals.

which, in words, reads: if you take a random individual with A = a and X = x and the same individual if she had A = b, you want to give them the same chance of being accepted.
The difference between the two levels is subtle but important: roughly speaking, when talking about interventions one is considering the average value over exogenous factors U , while counterfactuals consider only the values of U that are compatible with the factual observation (namely, the distribution P(U | A = a, X = x)). In other words, counterfactuals consider only events that take into account actual observed value of A (and X as well) (see Supplementary Material of Kusner et al. (2017)).
Equations (19), (20) make use of Pearl's do-calculus, where do(A = a), called intervention, consists in modifying the causal structure of the problem by exogenously setting A = a, thus removing any causal paths impacting on A ­ the theoretical equivalent of randomized experiments. The notation Y^Ab, on the other hand, stands for the three steps of counterfactual calculus, abduction, action, prediction :
· abduction is where you account for observed values and compute the distribution of U | (A = a, X = x);
· action corresponds to implementing the intervention do(A = b);
· prediction consists in using the new causal structure and the exogenous conditional distribution P(U | A = a, X = x) to compute the posterior of Y^ .
We refer to Pearl and Mackenzie (2018) for a general review on causal inference and do-calculus, and again to Barocas et al. (2019) for a thorough discussion of causality in the context of fairness.

Fairness at the level of interventions can be formally expressed as follows (Kilbertus et al., 2017):
P (Y^ = 1 | do(A = a), X = x) = P (Y^ = 1 | do(A = b), X = x), a, b  A, x  X . (19)

As in the observational setting, causality-based criteria have a group and an individual version: equations (19), (20) correspond to the individual form, but nothing prevents to take the version unconditional on X, i.e. holding on average on all the individuals, or even to condition on only some subset of X, as in CDP.

13

4.1. Group vs. individual fairness in causality-based setting
We call Expectation Intervention Fairness the condition:
P (Y^ = 1 | do(A = a)) = P (Y^ = 1 | do(A = b)) a, b  A, (21)
i.e. requiring that, on average, an individual taken at random from the whole population and forced to be woman (do(A = a)) should have the same chance of being accepted as a random individual forced to be man. Analogously, we can define Expectation Counterfactual Fairness (ECFF) as:
P (Y^Aa = 1 | A = a) = P (Y^Ab = 1 | A = a) a, b  A, (22)
which states that, on average, the acceptance rate for a random woman (A = a) should be the same as the one given to a random woman forced to be a man. Similar definitions can be given conditioning on partial information R.
Summarizing, we can visualize CFF as the following process: the conditioning on some (A = a, R = r) represents the group of people that we take into account (a single individual for R = X), and any consideration we do on them must hold on average over that group; given that group, we force a flipping of the sensitive attribute from A = a to A = b (and here we are going in a new, nonobservable, distribution), and this will trigger a cascade of causal consequences on the other features X (namely, on the descendants of A in the causal graph). Then, we compare the model outcomes averaged on the observed group and on the counterfactual group.

notion, but it requires fictitious data to be assessed ­ a dataset with A flipped, i.e. a dataset not sampled from the real distribution of (A, X, Y^ ). This is similar to the way in which CFF can be assessed: compare the predictions over a dataset with the ones on the same datasets but with A flipped and with all the changes caused by this flipping (of course, knowledge of the underlying SCM is needed). This, in turn, reveals the subtle difference among CDP in the form Y^  A | X and FTU: CDP is an observational notion, i.e. can be measured, in principle, by having access to (realizations from) the distribution of (Y^ , A, X), while FTU does not. We say "in principle" since it's very likely that in real-world datasets there will be very few observations corresponding to X = x (typically one), thus resulting in a poor estimation of the distribution of Y^ | X.
Causality-based notions are richer than the observational ones, and permit a further possibility: to select which causal paths from A to Y^ are considered legitimate and which, instead, we want to forbid. In the job listing example, we may consider that the type of degree of the applicants is fundamental for the job position, and thus crucial to making decisions. But it could well be that the type of degree is correlated, and even "caused" by the gender of the applicants: women and men may have different attitudes towards choosing the preferred degree. In this situation, if we require CFF, we would compare a man with some degree to his "parallel self" in the counterfactual world where he is a woman. In that world, however, it's very likely that her degree is going to be different. Thus, requiring CFF means to somehow prevent the decision maker to employ the degree type for the assessment. Taking into account situations like this one is not very difficult. Supplementary Material of Kusner et al. (2017) and Chiappa (2019) introduce the following definition, known as path-specific Counterfactual Fairness (pCFF):

Intervention Fairness is slightly different: you take again all the individuals compatible with the conditioning ­ which this time is R = r without fixing the sensitive A ­ then force them first to have A = a, and then to have A = b (with all the causal consequences of these interventions), and require them to have, on average, the same acceptance rate.
This may resonate with the notion of FTU: when flipping A for an observation you want the outcome of the model not to change. Indeed Fairness Through Unawareness is a causality-based notion of fairness, at least in its formulation of not explicitly using A. The very concept of flipping A is nothing but an intervention. Notice, incidentally, that the flipping of A without any impact on other variables corresponds to assuming a causal graph where A has no descendant, or, better, corresponds to assuming that all the changes caused by the flipping of A on other variables are legitimate, i.e. considered fair. Moreover, the fact that FTU is difficult to be measured without having access to the model, is due to its nature of being a non-observational

P (Y^Aa,XF x1 = 1 | A = a, XF = x1, XFc = x2) = P (Y^Ab,XF x1 = 1 | A = a, XF = x1, XFc = x2), a, b  A, x1, x2, (23)
where XF correspond to the variables descendants of A that we consider fair mediators to make decisions (e.g. the degree type), and XFc is its complement with respect to X, namely X = (XF , XFc ). In words, take a man (A = a) with a specific degree x1 and other features x2, and force him to be a woman, letting all the causal consequences of this flipping to happen with the exception of the degree, kept fixed at x1, then compare their outcomes.
Incidentally, notice that if we allow XF to contain all the descendants of A, then we end up with a notion that we may call direct Counterfactual Fairness (dCFF), namely the only path that we are concerned of is the direct causal path from gender A to the decision Y^ . In this case we don't allow any causal consequences of the gender flipping to happen when assessing fairness. This is, again, strictly

14

A  Y^ information used

dCF F

consider only some paths from A to Y^ to be unfair

pCF F

ECF F

conditional CFF

CFF

group

individual

Figure 6: Landscape of causality-based fairness criteria.

connected to FTU.
Figure 6 is a schematic and qualitative visualization of many of the points discussed above: the dimension of group vs. individual is "controlled" by how much information we condition on, while the dimension of the causal impact of A on the decision is controlled by the fraction of paths causally connecting A to Y^ that are considered fair. Figure 6 is meant to be the causal analogue of figure 4. Of course there are many possible cases that, for simplicity, we have omitted from figure 6, namely all the Intervention notions, and the path-specific notions valid on average in broader groups.10
If we focus on CFF, it can be reached by training a model on the space (, UA, Y ) where  are the non-descendants of A, i.e. variables not caused by A, directly or indirectly; UA denotes the information inside descendants of A that is not attributable to A. A decision system Y^ = f (, U ) is counterfactually fair (20) by design. This closely reflects what we discussed in section 2 about fair representations: train a model on a new space "cleaned" from all A information. Indeed, in that case we searched for statistical independence of Y^ on A while here we look for "causal independence" of Y^ on A. In this specific sense, counterfactual fairness can be reached via a form of preprocessing of the dataset, and is very similar, in spirit, to the concept of fair representation.
We may then wonder why the fair representation approach, reaching DP, is a notion of group rather than individual fairness, while CFF is considered an individual notion: the point is simply that we employed two different concepts of

10For instance, one could think of a measure taking into account only the A flip, without causal consequences, and valid on average over X as well (i.e. a group notion), namely
P (Y^Aa,XX = 1 | A = a) = P (Y^Ab,XX = 1 | A = a), a, b  A. (24)

individual fairness in the observational and in the counterfactual setting. Namely, in the observational setting we expressed disparate treatment as the event in which two individuals with same X but different A are given different outcomes. In general, fair representation is not guaranteed to prevent such scenario. In the counterfactual context, instead, we consider disparate treatment when an individual and her "counterfactual self" with A flipped are given different outcomes. CFF is designed precisely to prevent this. Notice that this concept, translated in the observational setting, is analogous to requiring a similar treatment for two individuals similar in Z, not in X . Indeed, if you take an individual with some (A = a, X = x) and flip her gender while taking into account the interdependence of X and A, she will be transformed in the same point Z.
This is an example of the fact that the concept of individual fairness is strongly dependent on the concept of similarity that one decides to consider.
Summarizing, CFF and fair representation are very similar in the way in which they deal with disparities, namely they both try to remove all (causal) information of A from the feature space, and then use the "cleaned" space to make decisions. In this respect, CFF can be seen as both a "causal analogue" of a DP-compliant methods, in the sense that all the way in which A may impact the decision are forbidden, and an individually fair notion in the sense that it imposes condition on individual basis.
Notice that, to find the UA variables, one needs to assume a Structural Causal Model (SCM), i.e. besides the causal graph encoding the "direction" of causal relationships, one needs a set of equations X = F (U ) that express the precise relations among the observed variables X and all the unobserved (exogenous) ones U . One typical assumption is the so-called Additive Noise Model:
X = f(parents(X)) + U,  = 1, . . . d + 1,
where X represents the -th feature among the variables (X, A). To fully specify a SCM, the functions f must be specified as well. The problem with assuming a specific SCM is that it is not falsifiable: given a causal graph, there is an infinite number of SCMs compatible with that graph and with the observations.
5. Conclusions
The notion of fairness in decision making has many nuances, that have been reflected in the high number of proposed mathematical and statistical definitions. Notice, moreover, that this aspect is not limited to ML or artificial intelligence: the problem of how to define and assess bias discrimination in decision making processes is present largely independently of "who" is making the decision. The growing attention to this issue in the domain of automated data-driven decisions can be imputed to the

15

fact that these processes can potentially amplify biases at scale, and could possibly do that without human oversight.
Even if a lot of work has been done in this respect, still there is confusion on the interplay among different fairness notions and metrics to assess them. In this paper, we tried to highlight some important aspects about the relationships between fairness metrics, in particular with respect to the distinctions individual vs. group and observational vs. causality-based.
Some have expressed critics and doubts about the possibility of capturing the complexity of notions such as equity and fairness with quantitative methods (Green and Hu, 2018). Even if these doubts are reasonable, we believe that quantitative research in the domain of fairness notions can provide an important contribution to the more general issue of bias discrimination, at least in terms of understanding and comprehension ­ let alone a boost in the attention of the scientific community on it. Thus, we strongly welcome more research both on the quantitative aspects, dealing with metrics assessment and algorithmic mitigation, and on the social and legal aspects, and hopefully in the interplay between them.
References
Angwin, J., Larson, J., Mattu, S., Kirchner, L., 2016. Machine bias. ProPublica, May 23, 139­159.
Barocas, S., Hardt, M., Narayanan, A., 2019. Fairness and Machine Learning. fairmlbook.org. http://www.fairmlbook.org.
Barocas, S., Selbst, A.D., 2016. Big data's disparate impact. Calif. L. Rev. 104, 671.
Berk, R., 2009. The role of race in forecasts of violent crime. Race and social problems 1, 231.
Berk, R., Heidari, H., Jabbari, S., Joseph, M., Kearns, M., Morgenstern, J., Neel, S., Roth, A., 2017. A convex framework for fair regression. arXiv preprint arXiv:1706.02409 .
Bickel, P.J., Hammel, E.A., O'Connell, J.W., 1975. Sex bias in graduate admissions: Data from berkeley. Science 187, 398­404.
Buolamwini, J., Gebru, T., 2018. Gender shades: Intersectional accuracy disparities in commercial gender classification, in: Conference on fairness, accountability and transparency, PMLR. pp. 77­91.
Calmon, F., Wei, D., Vinzamuri, B., Ramamurthy, K.N., Varshney, K.R., 2017. Optimized pre-processing for discrimination prevention, in: Advances in Neural Information Processing Systems, pp. 3992­4001.
Chiappa, S., 2019. Path-specific counterfactual fairness, in: Proceedings of the AAAI Conference on Artificial Intelligence, pp. 7801­7808.
Chouldechova, A., 2017. Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. Big data 5, 153­163.
Chouldechova, A., Roth, A., 2018. The frontiers of fairness in machine learning. arXiv preprint arXiv:1810.08810 .
Chouldechova, A., Roth, A., 2020. A snapshot of the frontiers of fairness in machine learning. Communications of the ACM 63, 82­89.
Dwork, C., Hardt, M., Pitassi, T., Reingold, O., Zemel, R., 2012. Fairness through awareness, in: Proceedings of the 3rd innovations in theoretical computer science conference, pp. 214­226.
Green, B., Hu, L., 2018. The myth in the methodology: Towards a recontextualization of fairness in machine learning, in: Proceedings of the machine learning: the debates workshop.

Hardt, M., 2014. How big data is unfair. Medium, https://medium. com/@ mrtz/how-big-data-is-unfair-9aa544d739de .
Hardt, M., Price, E., Srebro, N., 2016. Equality of opportunity in supervised learning, in: Advances in neural information processing systems, pp. 3315­3323.
Ilvento, C., 2019. Metric learning for individual fairness. arXiv preprint arXiv:1906.00250 .
Jiang, R., Pacchiano, A., Stepleton, T., Jiang, H., Chiappa, S., 2020. Wasserstein fair classification, in: Uncertainty in Artificial Intelligence, PMLR. pp. 862­872.
Johndrow, J.E., Lum, K., et al., 2019. An algorithm for removing sensitive information: application to race-independent recidivism prediction. The Annals of Applied Statistics 13, 189­220.
Jung, C., Kearns, M., Neel, S., Roth, A., Stapleton, L., Wu, Z.S., 2019. Eliciting and enforcing subjective individual fairness. arXiv preprint arXiv:1905.10660 .
Kamiran, F., Calders, T., 2009. Classifying without discriminating, in: 2009 2nd International Conference on Computer, Control and Communication, IEEE. pp. 1­6.
Kamiran, F., Calders, T., 2012. Data preprocessing techniques for classification without discrimination. Knowledge and Information Systems 33, 1­33.
Kamiran, F., Zliobaite, I., Calders, T., 2013. Quantifying explainable discrimination and removing illegal discrimination in automated decision making. Knowledge and information systems 35, 613­644.
Kearns, M., Neel, S., Roth, A., Wu, Z.S., 2018. Preventing fairness gerrymandering: Auditing and learning for subgroup fairness, in: International Conference on Machine Learning, PMLR. pp. 2564­ 2572.
Kilbertus, N., Carulla, M.R., Parascandolo, G., Hardt, M., Janzing, D., Scho¨lkopf, B., 2017. Avoiding discrimination through causal reasoning, in: Advances in Neural Information Processing Systems, pp. 656­666.
Kleinberg, J., Mullainathan, S., Raghavan, M., 2016. Inherent trade-offs in the fair determination of risk scores. arXiv preprint arXiv:1609.05807 .
Kusner, M.J., Loftus, J., Russell, C., Silva, R., 2017. Counterfactual fairness, in: Advances in neural information processing systems, pp. 4066­4076.
Louizos, C., Swersky, K., Li, Y., Welling, M., Zemel, R., 2015. The variational fair autoencoder. arXiv preprint arXiv:1511.00830 .
McNamara, D., Ong, C.S., Williamson, R.C., 2017. Provably fair representations. arXiv preprint arXiv:1710.04394 .
Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., Galstyan, A., 2019. A survey on bias and fairness in machine learning. arXiv preprint arXiv:1908.09635 .
Mitchell, S., Potash, E., Barocas, S., D'Amour, A., Lum, K., 2018. Prediction-based decisions and fairness: A catalogue of choices, assumptions, and definitions. arXiv preprint arXiv:1811.07867 .
Narayanan, A., 2018. Translation tutorial: 21 fairness definitions and their politics, in: Proc. Conf. Fairness Accountability Transp., New York, USA, pp. 6­2.
O'neil, C., 2016. Weapons of math destruction: How big data increases inequality and threatens democracy. Crown.
Oneto, L., Chiappa, S., 2020. Fairness in machine learning, in: Recent Trends in Learning From Data. Springer, pp. 155­196.
Pearl, J., Mackenzie, D., 2018. The book of why: the new science of cause and effect. Basic Books.
Verma, S., Rubin, J., 2018. Fairness definitions explained, in: 2018 IEEE/ACM International Workshop on Software Fairness (FairWare), IEEE. pp. 1­7.
Washington, A.L., 2018. How to argue with an algorithm: Lessons from the compas-propublica debate. Colo. Tech. LJ 17, 131.
Yang, F., Cisse, M., Koyejo, O.O., 2020. Fairness with overlapping groups; a probabilistic perspective. Advances in Neural Information Processing Systems 33.
Zafar, M.B., Valera, I., Gomez Rodriguez, M., Gummadi, K.P., 2017. Fairness beyond disparate treatment & disparate impact: Learning classification without disparate mistreatment, in: Proceedings of the 26th international conference on world wide web, pp. 1171­ 1180.

16

Zemel, R., Wu, Y., Swersky, K., Pitassi, T., Dwork, C., 2013. Learning fair representations, in: International Conference on Machine Learning, pp. 325­333.
17

