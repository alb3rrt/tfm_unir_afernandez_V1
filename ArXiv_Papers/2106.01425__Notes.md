
# Gradient Assisted Learning

[arXiv](https://arxiv.org/abs/2106.01425), [PDF](https://arxiv.org/pdf/2106.01425.pdf)

## Authors

- Enmao Diao
- Jie Ding
- Vahid Tarokh

## Abstract

In distributed settings, collaborations between different entities, such as financial institutions, medical centers, and retail markets, are crucial to providing improved service and performance. However, the underlying entities may have little interest in sharing their private data, proprietary models, and objective functions. These privacy requirements have created new challenges for collaboration. In this work, we propose Gradient Assisted Learning (GAL), a new method for various entities to assist each other in supervised learning tasks without sharing data, models, and objective functions. In this framework, all participants collaboratively optimize the aggregate of local loss functions, and each participant autonomously builds its own model by iteratively fitting the gradients of the objective function. Experimental studies demonstrate that Gradient Assisted Learning can achieve performance close to centralized learning when all data, models, and objective functions are fully disclosed.

## Comments



## Source Code

Official Code



Community Code

- [https://paperswithcode.com/paper/gradient-assisted-learning](https://paperswithcode.com/paper/gradient-assisted-learning)

## Bibtex

```tex
@misc{diao2021gradient,
      title={Gradient Assisted Learning}, 
      author={Enmao Diao and Jie Ding and Vahid Tarokh},
      year={2021},
      eprint={2106.01425},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```

## Notes

Type your reading notes here...

