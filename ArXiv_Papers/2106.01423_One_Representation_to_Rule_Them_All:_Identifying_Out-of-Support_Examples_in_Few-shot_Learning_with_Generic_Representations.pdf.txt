arXiv:2106.01423v1 [cs.LG] 2 Jun 2021

One Representation to Rule Them All: Identifying Out-of-Support Examples in Few-shot Learning with
Generic Representations
Henry Kvinge, Scott Howland, Nico Courts, Lauren A. Phillips, John Buckheit, Zachary New
Elliott Skomski, Jung H. Lee, Aaron Tuor, Sandeep Tiwari, Jessica Hibler, Courtney D. Corley,
Nathan O. Hodas Pacific Northwest National Laboratory
Seattle, WA, USA first.last@pnnl.gov
Abstract
The field of few-shot learning has made remarkable strides in developing powerful models that can operate in the small data regime. Nearly all of these methods assume every unlabeled instance encountered will belong to a handful of known classes for which one has examples. This can be problematic for real-world use cases where one routinely finds `none-of-the-above' examples. In this paper we describe this challenge of identifying what we term `out-of-support' (OOS) examples. We describe how this problem is subtly different from out-of-distribution detection and describe a new method of identifying OOS examples within the Prototypical Networks framework using a fixed point which we call the generic representation. We show that our method outperforms other existing approaches in the literature as well as other approaches that we propose in this paper. Finally, we investigate how the use of such a generic point affects the geometry of a model's feature space.
1 Introduction
Over the past decade, deep learning-based methods have achieved state-of-the-art performance in a range of applications including image recognition, speech recognition, and machine translation. There are many problems however, where deep learning's utility remains limited because of its need for large amounts of labeled data [15]. The field of few-shot learning [27] aims to develop methods for building powerful machine learning models in the limited-data regime.
The common paradigm in few-shot learning is to assume that for each unlabeled instance, one has at least one labeled example belonging to the same class. At inference time then, classification of an unlabeled example x simply involves determining which of a fixed number of known classes x is most likely to belong to. In real-world problems on the other hand, it is frequently the case that one does not have labeled examples of every possible class that has support in a data distribution. This is particularly true in science and medical applications where it is time and cost prohibitive to have a subject matter expert sift through an entire dataset and identify all classes therein. Establishing methods of detecting whether or not unlabeled input belongs to any known class is thus critical to making few-shot learning an effective tool in a broad range of applications.
We define a datapoint to be out-of-support (OOS) if it does not belong to a class for which we have labeled examples, but was still drawn from the same data distribution as the labeled examples we
Preprint. Under review.

have. We call the problem of identifying such instances the out-of-support detection problem. As we explain in Section 2.3, OOS detection resembles, but is distinct from, out-of-distribution (OOD) detection where one attempts to identify examples which were drawn from a different data distribution entirely, (see Figure 1 for an illustration of the difference between these two types of problems). To our knowledge the OOS detection problem was first articulated in the literature only recently in [26], where two algorithms were proposed within the metric-based few-shot setting.
In this paper we describe a new approach to OOS detection which we call Generic Representation Out-Of-Support (GROOS) Detection. The name is inspired by the concept of generic points in algebraic geometry, which are points for which all generic properties of a geometric object are true [6]. Our method uses a so-called generic representation to represent the data distribution as a whole but no individual class in particular. Like the methods proposed in [26], our method can be adapted to work with a range of metric-based few-shot models. For simplicity, in this paper we focus on a Prototypical Networks [22] setting where the generic representation is simply a point in feature space. To predict whether an unlabeled instance q is OOS or not, one compares the distances from the encoding of q to each class representation and the generic representation. If the image of q is sufficiently close to the generic representation and sufficiently far from all class representations, it is predicted to be OOS. We state a pair of inequalities (1) relating the distances between query points, class prototypes, and the generic representation which need to be satisfied in order for GROOS detection to be able to correctly predict when q is OOS and also correctly predict the class of q when q is in-support. We analyze how these constraints effect the geometry of a model's feature space, characterizing its structure through three Propositions (Propositions 4.1, 4.2, and 4.3). We also show that for GROOS to be successful, additional `second-order' relationships between prototypes and the generic representation need to hold.
We benchmark GROOS detection against two recently proposed methods - LCBO and MinDist [26] as well as an additional method - Background OOS detection - which we describe in this paper. We find that GROOS detection not only on average outperforms previous benchmarks (Section 4.1), but an adapted version of GROOS called Centered GROOS tends to outperform other OOS detection methods in settings that require significant model generalization (Section 4.2). Despite the strong relative performance of Centered GROOS detection in this latter setting, it is clear that the community still has a considerable amount of work to do before few-shot models can satisfactorily detect OOS examples when evaluated on datasets significantly different from those that they were trained on.
In summary, our contributions in this paper include:
· We introduce the GROOS detection method, which is designed to solve the out-of-support detection problem in few-shot learning using a generic representation.
· We benchmark GROOS detection against existing metric-based methods in the literature and an additional OOS detection method, Background OOS Detection, which we describe in this paper. We show that GROOS out-performs these approaches both in a traditional few-shot trainevaluation setting, and in a more challenging setting where models are trained on ImageNet and then evaluated on a diverse range of datasets.
· We state two inequalities relating class prototypes, the generic representation, and encoded query points, which must be satisfied in order for both OOS detection and standard in-support classification to be effective. Motivated by these inequalities we prove three propositions which relate feature space geometries that arise from the standard Prototypical Networks problem and the feature space geometries that arise from GROOS.
2 Background and related work
2.1 Few-shot learning and Prototypical Networks
There are a range of approaches that have been used to address the challenges of few-shot learning. Fine-tuning methods [1, 2] use transfer learning followed by fine-tuning to train models with limited data. Data augmentation methods [5] leverage augmentation and generative approaches to produce additional training data. Gradient-based meta-learning [4, 21] is a class of methods that use sophisticated optimization techniques to learn strong initial weights which can be adapted to a new task with a small number of gradient steps. The algorithm we propose in this paper is related to a fourth class of algorithms called metric-based models. In these models an encoder function is trained to embed data into a space where a distance metric (either hard-coded or learned) captures some
2

task-appropriate notion of similarity. Well-known examples of metric-based few-shot models include Prototypical Networks [22], Matching Networks [24], and Relational Network [23].
An episode is the basic unit of few-shot inference and training. It consists of a set S of labelled examples known as the support set and an unlabeled set Q known as the query set. Within an episode, a model uses elements in S to predict labels for elements in Q. We assume that elements of S belong to classes Cin = {1, . . . , k}. For convenience, we decompose S into a disjoint union: S = cCin Sc, where Sc contains only those elements of S with label c. We will assume that the size n = |Sc| is constant for all c  Cin. The integer n is known as the shots of the episode, while the integer k is known as the ways. In this paper we will assume that Q has been drawn from a distribution p, and each set Sc has been drawn from the conditional distribution p(y = c).
By few-shot training we mean the process of calculating the loss for an entire episode and then using that loss to update the weights of the model. Few-shot inference has an analogous meaning. A few-shot split is a partition of a dataset into train and test sets by class, so that examples from each class are contained in either the train or test split, but not in both.
Prototypical networks (ProtoNets) [22] uses an encoder function f : X  Rd to map elements of both Q and S into metric space Rd (which we will always assume is equipped with the Euclidean metric). In Rd, a centroid c is formed for each set f(Sc). c is referred to as the prototype which represents class c in Rd. The model predicts the class of an unlabelled query point q based on the solution to arg mincCin ||c - f(q)||. Note that in the case where one needs probabilities associated with a prediction, one can apply a softmax function to the distance vector [-dc]cCin where dc = ||c - f(x)||.
2.2 The out-of-support detection problem
As mentioned in the Introduction, it is commonly assumed in the literature that all elements of Q have a label from Cin. It was observed in [26] that in many real-world cases, this assumption is unrealistic. In that work the authors referred to an example q  Q that does not belong to a class in Cin as being "out-of-episode". We feel it is more appropriate to describe such examples as being out-of-support (OOS), since any elements found in Q can be said to be part of the episode. Following [26] we decompose Q as Q = Qin  Qout where Qin are those elements that are in-support and Qout are those elements that are OOS. It is also convenient to use C to denote the set of all labels on elements from S  Q, with C decomposing as the disjoint union C = Cin  Cout where Cout are simply those classes for which there are unlabeled examples in Q but no labeled examples in S. Note that the user generally does not have knowledge of Cout.
The out-of-support (OOS) detection problem then involves identifying those elements of Q that do not belong to any class in Cin. All the methods for OOS detection described or introduced in this paper use a confidence score  : X  R that maps a query point q  Q to a value in R. In general,  also depends on the full support set S as well as the encoder f, but to simplify notation we assume these dependencies are implicit.
The authors of [26] proposed two methods for OOS detection. Both are presented as an additional component that can be added to Prototypical Networks and it is in this context that we will describe and evaluate them. The first uses a function called the Minimum Distance Confidence Score (MinDist), dist : X  R which is defined as dist(q, f) = - mincCin ||c - f(q)||. A query q is predicted to be OOS if dist(q) < t for some t < 0. The second method proposed in [26] is the Learnable Class BOundary (LCBO) Network which is a parametric class-conditional confidence score LC : X  R. LC uses a small, fully-connected neural network h : Rd  R to produce scores for each prototype/query pair (q, c). The confidence score LC is defined as LC (q) = maxcCin h (c, f(q)) . The model predicts that q is OOS if LC (q) < t for some predetermined threshold t. We note that the authors of [26] used an additional term in the loss function to encourage their models to correctly identify OOS examples. We did not find the addition of such loss terms necessary to achieve good performance with the models introduced in this paper.
2.3 Out-of-distribution detection
Out-of-distribution (OOD) detection aims to develop methods that can identify whether or not a data point x was drawn from some known distribution p. Methods for doing this within the context of
3

Figure 1: A diagram illustrating the difference between out-of-support detection and out-ofdistribution detection for a few-shot task where one attempts to identify images of Newfoundlands and pugs from a dataset of dog images.
deep learning models include: using a model's largest softmax output value as a confidence score [9, 14] and ODIN [17] which suggests identifying OOD examples through the use of model gradients and sofmax temperature scaling. Standard benchmarks for OOD detection focus on using OOD detection methods to identify examples drawn from very visually distinct distributions. For example, a common experiment attempts to detect Gaussian noise or MNIST [16] images injected into the CIFAR10 dataset [12].
OOS detection differs from OOD detection in that, in general, the conditional distributions corresponding to elements belonging to Cin and Cout only vary in subtle and arbitrary ways. Consider the example summarized in Figure 1 where S and Q consist of images of dogs. While it is true that the distribution of dog images belonging to classes Cin = {Newfoundland, pug} is different than those belonging to classes Cout = {Labrador, Tibertan terrier}, these differences are slight (and focus on very specific aspects of the input) relative to differences in distribution that OOD detection methods are designed to detect. Indeed, [26] showed that a few-shot analogue of [9] applied to a ProtoNet model struggled on the OOS detection problem. Additionally, OOD detection methods generally assume that even if a model has not seen examples of OOD data, it has seen many examples of in-distribution data. This is not the case for few-shot models which only have a handful of classes that they can use to characterize "in-distribution". In fact, in the generalization-focused evaluation setting described in Section 4.2, few-shot models could be described as operating exclusively out-ofdistribution in relation to their training set. Finally, while OOD examples are defined with respect to an entire dataset, OOS examples are only defined via a small support set, and this definition can vary from episode to episode. As suggested in [26], all these differences argue for identifying few-shot OOS detection as a problem which is distinct from OOD detection, requiring its own set of methods.
3 OOS detection with a generic point
In this section we describe our proposed Generic Representation Out-Of-Support (GROOS) Detection method. Let f : X  Rd be the encoder (for example, when X is an image space, then f might be a ResNet [7] with the final linear classification layer removed). Let L : Rd  Rd be an affine map, so that L(x) = W x + b for some matrix (weights) W and vector (bias) b. We construct a new encoder by composing h = L  f : X  Rd.
Next choose a point oos  Rd which will be called the generic representation and a threshold 0  t  1. There are many potential choices for oos but we find that the origin works well in practice. Inference with h is similar to inference with the standard ProtoNets (Section 2.1). For an n-shot, k-way support set S = cCin Sc, with support set labels Cin = {1, . . . , k}, and query q, h(S) and h(q) are calculated and centroid prototypes c are computed for each set h(Sc) with c  Cin. We compute the vector dq := (d1, . . . , dk, doos) where di := ||i - h(q)||. Finally, let sof tmax : Rk+1  Rk+1 be the standard softmax function. Following the notation in Section 2.2 we define gen : X  R to be gen(q) := [sof tmax(-dq)]k+1 where [sof tmax(-dq)]k+1 is the (k + 1)st output coordinate corresponding to encoded query distance from oos. If gen(q) > t then we predict that q is OOS. If gen(q) < t, then we predict that q is in-support and we use the other k softmax outputs from sof tmax(-dq) to predict its class. Informally, this process consists of
4

comparing the distance of the encoded query point from the generic representation to its distance to other support prototypes. If the query is sufficiently closer to the generic representation than it is to other prototypes, then it is predicted as OOS. This process is summarized in Algorithm 1.

Algorithm 1 Generic Representation Out-Of-Support (GROOS) Detection
Input: Encoder function h : X  Rd, generic representation oos  Rd, support set S = S1  · · ·  Sk with corresponding label set Cin = {1, . . . , k}, query q, threshold 0  t  1. for c  Cin do
Compute prototype centroid c from h(Sc) Compute dc = ||c - h(q)|| end for
Compute doos = ||oss - h(q)|| Set dq = (d1, . . . , dk, doos) and compute gen(q) = [Sof tmax(-dq)]k+1 if gen(q) > t then
q is predicted as OOS else
q is predicted as in-support, belonging to class c = arg mincCin dc. end if

One can ask what metric properties an encoded dataset h(D) must have in order for GROOS detection to be effective. For simplicity we assume that prototypes 1, . . . , k and generic representation oos are fixed (empirically we find that prototypes are fairly stable when the number of shots is high enough so this is not an unreasonable approximation). (1) To ensure in-support examples
are always predicted correctly, h must map any x  D with label c  C closer to c than to 1, . . . , c-1, c+1, . . . , k, oos. That is ||h(x) - c|| < ||h(x) - c || for all c  C  {oos} such that c = c . (2) One the other hand, when c is not represented in the support set, then h(x) must be closer to oos than to any other class prototype which is not c (which does not appear in the episode). Specifically, ||h(x) - oos|| < ||h(x) - c || for all c  C such that c = c. These inequalities can be combined for the single expression

||h(x) - c|| < ||h(x) - oos|| < ||h(x) - c || for c  C, c = c.

(1)

Inequality (1) suggests that if one is not able to actually train h on dataset D (or a similar dataset), and hence h is not able to learn how to arrange encoded data around oos, then another sensible option is to choose oos to be the centroid of h(S  Q). We call this alternative version of GROOS detection Centered GROOS Detection. We will see that it works better than the standard version of
GROOS detection when the test set differs significantly from the training set.

3.1 Background detection
We introduce a second OOS detection model to serve as an additional benchmark. We call it Background Detection since it was inspired by the "background class" described in [28]. Background detection consists of an encoder function h : X  Rd such as a ResNet, with its final classification layer replaced with a linear layer L : Rd  Rd and two predetermined constants M > 0 and 0  t  1. An episode with support set S = cCin Sk and query q proceeds with the usual calculation of class centroids c for c  Cin. Using constant M and distances ||c - h(q)|| between encoded query and prototypes the vector dq := (d1, . . . , dk, M ) is obtained. The confidence function back : X  R associated with this method is then: back(q) := sof tmax(-dq) k+1. Query q is predicted to be OOS if back(q) > t.

4 Experiments and analysis
4.1 Standard few-shot evaluation
Our first set of experiments look at how well OOS detection methods (MinDist, LCBO, Background Detection, GROOS, and Centered GROOS) can identify OOS examples in the setting where the base model is trained and evaluated on few-shot splits drawn from the same dataset. That is, we partition

5

CIFAR100

CUB-200

Omniglot

AUPR AUROC AUPR AUROC AUPR AUROC

MinDist

88.4±0.1 88.6±0.1 89.0±0.2 89.2±0.2 99.4±0.1 99.5±0.1

LCBO

83.2±0.5 84.7±0.4 85.8±0.5 87.6±0.3 99.2±0.2 99.3±0.1

Background (ours) 87.2±0.3 86.9±0.3 88.8±0.7 88.0±0.7 98.9±0.1 98.9±0.1

GROOS (ours)

90.1±0.2 90.2±0.3 90.9±0.6 90.6±0.7 99.6±0.1 99.5±0.1

Centered GROOS (ours) 88.9±0.9 88.4±0.8 89.6±0.2 89.5±0.1 99.5±0.1 99.5±0.1

Table 1: The area under the ROC curve (AUROC) and area under the precision-recall curve (AUPR) for a range of few-shot OOS detection methods. We put an  next to the top score in each column and set in bold all the rest of the scores in the column that are within 0.5 of this.

the classes of the dataset between train and test. We focus on the datasets: CIFAR100 [12] (CC-BY 4.0), CUB-200 [25] (CC0 1.0), and Omniglot [13] (MIT License).
All models were trained for four days of wall clock time on a single Tesla P100 GPU for a total of between 250,000 and 500,000 training episodes in that time. All performances stabilized around the lower end of that range. All models used a ResNet18 encoder with the final linear layer removed and were initialized with the standard ImageNet (CC-BY 4.0) pre-trained weights from Torchvision [19]. We address the question of how performance differs for different sizes of encoder in Section A.1 of the Appendix. We used the Adam optimizer for training, with a learning rate of 1 × 105, a weight decay factor of 5 × 10-5, and  values of 0.9 and 0.999. All results correspond to 5-shot, 5-way episodes, with 8 queries per support class and a total of 40 OOS images introduced per episode (that is, 50% of all images in the query were OOS). All images were resized to 224 × 224 before being fed through the model. To evaluate each model, we sampled 1000 episodes from the corresponding few-shot test set. To complete the evaluation, we computed the area under precision recall curve (AUPR) and area under the ROC curve (AUROC) for each model with respect to the evaluation queries and multiplied these by 100.
The result of these experiments can be found in Table 1. We bold all scores that are within 0.5 of the top model (in terms of both AUPR and AUROC), putting an  on the top score for each column. As can be seen, in two of the three datasets used, GROOS outperforms other methods by at least 1.0 both in terms of AUPR and AUROC. On Omniglot, MinDist, LCBO, GROOS, and Centered GROOS all do close to perfect. We include this last experiment to demonstrate that when a sufficiently strong encoder is used for a simpler dataset, then a range of OOS detection methods can do quite well.
4.2 Generalization experiments
The experiments in the previous section simulated the situation where one has access to a training dataset which is similar to the data that one wants to apply the model to during inference. However, as pointed out in the Introduction, there are many applications of few-shot learning where one does not have access to such a training set. In these cases it is important that a model can perform well, even when the dataset one wants to perform inference on is substantially different from the data used for training.
We ran experiments to evaluate how adaptable MinDist, LCBO, Background Detection, GROOS Detection, and Centered GROOS Detection were when a dataset from a previously unseen distribution was introduced at inference time. We chose to train our networks on a few-shot training split of ImageNet, as ImageNet has been shown to generally produce rich and flexible feature extractors [11, 2], and then test on: the few-shot ImageNet testset, CIFAR100, Omniglot, Aircraft [18], Describable Textures [3], and Fruits 360 [20].1 All models used the same encoder, hyperparameters, and training scheme as that described in Section 4.1.
We find that in this setting, performance is generally worse for all model types. This is not surprising since the models are essentially operating on out-of-distribution data at inference time. Aircraft is
1Aircraft is available exclusively for non-commercial research purposes, Describable Textures is available for research purposes, and Fruits 360 is covered by CC BY-SA 4.0

6

ImageNet CIFAR100 Omniglot Aircraft Textures Fruits

MinDist

95.0±0.1 80.1±0.6 85.5±0.7 59.4±0.3 72.7±0.8 95.3±0.4

LCBO

92.6±0.1 76.3±1.0 68.5±1.6 54.7±0.5 65.3±1.5 89.0±2.2

Background (ours) 93.6±0.1 77.5±0.9 58.6±1.9 58.4±0.4 71.8±0.8 89.8±1.0

GROOS (ours)

95.7±0.1 74.3±3.0 74.6±1.4 53.8±0.3 75.2±1.0 91.2±0.8

Centered GROOS (ours) 95.0±0.2 80.6±0.8 82.1±0.5 61.8±4.7 82.3±0.2 96.2±0.3

Table 2: The area under the ROC curve (AUROC) for a range of few-shot OOS detection methods which were all trained on a few-shot training split of ImageNet and then evaluated on a range of datasets. We put an  next to the top score in each column and set in bold all the rest of the scores in the column that are within 0.5 of this.

ImageNet CIFAR100 Omniglot Aircraft Textures Fruits

MinDist

95.0±0.1 79.4±0.5 86.3±0.7 59.3±0.2 73.8±0.8 95.5±0.3

LCBO

91.8±0.1 73.3±1.4 66.4±1.8 53.8±0.4 64.5±1.8 88.6±2.6

Background (ours) 93.7±0.0 77.1±0.8 76.6±0.7 58.3±0.3 70.1±0.7 92.8±0.5

GROOS (ours)

95.5±0.1 72.1±2.6 75.7±1.2 54.3±0.3 71.1±0.9 92.1±0.6

Centered GROOS (ours) 94.7±0.3 79.8±0.9 82.1±0.7 61.7±0.4 79.9±0.3 96.4±0.1

Table 3: The area under the precision recall curve (AUPR) for a range of few-shot OOS detection methods which were all trained on a few-shot training split of ImageNet and then evaluated on a range of test datasets. We put an  next to the top score in each column and set in bold all the rest of the scores in the column that are within 0.5 of this.

a particularly challenging dataset for models that have not seen the corresponding training set. A comparison of the error bars in Tables 2 and 3 on the one hand and Table 1 on the other illustrates that when operating on OOD data there is more variation between training runs. Nonetheless, Centered GROOS detection performs better than other methods on 4 out of the 5 OOD datasets, with the exception of Omniglot where MinDist does substantially better. On the in-distribution test set ImageNet test, GROOS achieves better performance than Centered GROOS, confirming our hypothesis that GROOS is better to use when inference data aligns with training data and Centered GROOS is better otherwise. Of all the datasets presented to the models in these tests, Omniglot is probably the most "unlike" ImageNet. We conjecture that for mildly OOD datasets such as CIFAR100, Aircraft, and Fruits, Centered GROOS tends to perform better, while for significantly OOD datasets such as Omniglot, the simpler MinDist model might be a better choice.
4.3 Generic points: feature space geometry and decision boundaries
In this section we analyze the geometry of the feature space induced by the use of a generic point to detect OOS examples. This is motivated by the empirical observation that the use of a generic point seems to change the way that a model clusters classes. In the low-dimensional setting of Figure 2 for example, we observe that while moving from standard ProtoNets to ProtoNets with Background Detection appears to simply tighten clusters, moving to ProtoNets with a generic point results in a distinct radial cluster structure around the generic point (at (0, 0)). Below we will try to formalize some of the intuition gained from these experiments so that we can make precise statements about the different kinds of geometry induced by these different problem formulations. Proofs for all propositions can be found in Section A.2.
Recall that an affine hyperplane in Rd is a translation of a (d-1)-dimensional subspace. Alternatively, a non-zero vector v  Rd and constant b  R define an affine hyperplane via the expression H := {w  Rd | w, v = b}. Note that any affine hyperplane H decomposes Rd into two open half-spaces: H+ := {w  Rd | w, v > b} and H- := {w  Rd | w, v < b}. For any two distinct points x1, x2  Rd, one gets a hyperplane Hx1,x2 defined by normal vector x1 - x2 and
7

Figure 2: Visualizations of the feature space of a ResNet50 encoder (left) trained without OOS examples, (center) with OOS examples using a generic representation, (right) using a background class.

constant

1 2

(||x1

||2

-

||x2||2).

In

particular,

when

1

and

2

are

centroids

for

two

classes,

then

H1 ,2

is the decision boundary of the associated 2-way ProtoNets model (or alternatively the Voronoi

partition corresponding to two points).

Let x be a point in Rd and let 1, . . . , k, oos be a list of prototypes and generic point. Let Sk+1 be the symmetric group on (or permutations of) k + 1 elements. There is a trivial bijection between Sk+1 and total orderings of 1, . . . , k, oos. In particular, for permutation   Sk+1, we associate  with the order (1) < (2) < · · · < (oos) where we write (i) = j to represent the value
j  {1, . . . , oos} that  permutes i to (we use index oos and k + 1 interchangeably).

Proposition 4.1. Let 1, . . . , k, oos  Rd be a finite list of prototypes and generic point. The set of hyperplanes corresponding to each pair of 1, . . . , k, oos induce a decomposition of Rd into open (possibly empty) subsets (cells) S, where   Sk+1 and

S := x  Rd | ||x - (1)|| < · · · < ||x - (oos)|| ,

as well as a measure zero, closed subset B which is the union of all Hi,j for i, j  {1, . . . , k, oos}.

The decomposition described in Proposition 4.1 can be used to describe those regions of Rd that can lead to the correct classification of an encoded point in different versions of the ProtoNet problem. As
we will see, these regions differ substantially between the classic ProtoNets problem and ProtoNets with generic point. We call a point x  Rd, i-viable if encoding a class i query point q such that h(q) = x results in the correct prediction that q belongs to class i, if class i is represented in the support, or that q is OOS, if class i is not represented in the support. A point is called viable if it is i-viable for some i  {1, . . . , k}. A set of points U is called i-viable if every point in U is i-viable and viable if every point in U is viable.

· Standard ProtoNets: For a point belonging to class i to be predicted correctly, it must lie in a cell of the form S with (1) = i. Note that outside of measure-zero set B, every point in Rd is i-viable for some i  {1, . . . , k} since every cell S consists of points closest to some centroid (i.e. (1) = j for some j  {1, . . . , k}) and in the setting where OOS examples do not exist, a point belonging to class i is always classified correctly if it is closer to centroid i than it is to any other centroid.
· ProtoNets with generic point: For a point belonging to class i to be predicted correctly both when its prototype is present and also when it is not, it must satisfy inequality (1). This means that it must lie in a cell of the form S with (1) = i and (2) = oos. Note that this condition means that even outside of B, there are non-viable regions of Rd. For example, if (2) = oos.
We illustrate these differences in Figure 3 for the standard ProtoNets problem (left) and ProtoNets with generic point (right). We put boxes around the label of viable regions in each diagram.
Proposition 4.2. Let {1, . . . , k} be a set of classes and let oos  Rd be a generic point.
1. In the standard ProtoNets problem, the set of i-viable points is always nonempty for each choice of distinct prototypes 1, . . . , k  Rd and for all i  {1, . . . , k}.
2. In the ProtoNets with generic point problem, there are choices k and distinct 1, . . . , k, oos  Rd for which the i-viable region of Rd is the empty set for some i  {1, . . . , k}. There are also choices of distinct 1, . . . , k, oos such that there is a nonempty i-viable region for each i.

8

Figure 3: Low dimensional illustrations of the feature space decision boundaries of the (left) standard ProtoNet problem with three prototypes, (right) the ProtoNet problem with generic point. In each region we label the ordering the closest prototypes/generic point. We box the labels of regions which are viable.
Thus we see that the introduction of a generic points puts additional constraints on how a model can arrange prototypes in feature space, with some arrangements being not only non-optimal, but actually precluding correct predictions.
Our final proposition shows that the radial pattern shown in Figure 2 actually represents general geometric structure induced by the generic point problem. For fixed 1, . . . , k, oos  Rd we call the region of Rd which consists of points that are closer to oos than to any 1, . . . , k the OOS-core (note that as a corollary to Proposition 4.2.1 this always exists). We call two sets U, V  Rd adjacent if there is a point p  Rd such that for any > 0, the open ball B (p) contains points from both U and V . Proposition 4.3. If 1, . . . , k, oos  Rd are a choice of distinct prototypes/generic point such that the set Pi of i-viable points is non-empty for i  {1, . . . , k}, then Pi is adjacent to the OOS core.
5 Limitations and broader impacts
As indicated in Section 4.2 all existing OOS detection methods struggle when applied to data that is significantly different from the model's test set. This is a major obstacle blocking the use of these models in many science domains. While Centered GROOS represents progress, we suspect that more sophisticated methods will need to be developed in the future in order to achieve satisfactory results. Additionally, our theoretical analysis only addressed limited aspects of the GROOS model. In Section 4.3 for example, we restrict ourselves to a study of fixed prototypes. Finally, in future work we would like to compare our metric-based approaches to OOS detection, to metalearning methods developed to address related problems [10].
Our work, like other work in few-shot learning, has the capability to broaden access to deep learning tools by lowering training data requirements. This can be positive because it means that groups without large data acquisition and computing budgets can apply deep learning to solve problems and negative because those uses of machine learning with negative societal impacts now require less data.
6 Conclusion
In many situations, the ability to detect OOS examples is a necessary requirement for deployment of few-shot learning models. In this paper we showed that in the metric-based setting, GROOS and its
9

variant Centered GROOS are two methods that begin to address this challenge. Despite the fact that our models, on average, outperformed existing approaches, we believe OOS detection is a challenge that deserves more attention within the few-shot community, since effective solutions will enable broader adoption of few-shot methods for real-world science and engineering applications.
Acknowlegments
This work was funded by the U.S. Government.
References
[1] Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closer look at few-shot classification. arXiv:1904.04232, 2019.
[2] Arkabandhu Chowdhury, Mingchao Jiang, and Chris Jermaine. Few-shot image classification: Just use a library of pre-trained feature extractors and a simple classifier. arXiv preprint arXiv:2101.00562, 2021.
[3] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A. Vedaldi. Describing textures in the wild. In Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2014.
[4] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1126­1135. JMLR. org, 2017.
[5] Bharath Hariharan and Ross Girshick. Low-shot visual recognition by shrinking and hallucinating features. In Proceedings of the IEEE International Conference on Computer Vision, pages 3018­3027, 2017.
[6] Robin Hartshorne. Algebraic geometry, volume 52. Springer Science & Business Media, 2013. [7] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770­778, 2016. [8] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. arXiv preprint arXiv:2006.16241, 2020. [9] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. International Conference on Learning Representations, 2017. [10] Taewon Jeong and Heeyoung Kim. OOD-MAML: Meta-learning for few-shot out-ofdistribution detection and classification. Advances in Neural Information Processing Systems, 33, 2020. [11] Simon Kornblith, Jonathon Shlens, and Quoc V Le. Do better imagenet models transfer better? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2661­2671, 2019. [12] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009. [13] Brenden M. Lake, Ruslan Salakhutdinov, and Joshua B. Tenenbaum. Human-level concept learning through probabilistic program induction. Science, 350(6266):1332­1338, 2015. [14] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pages 6405­6416, 2017. [15] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, page 436­444, 2015. [16] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998. [17] Kimin Lee, Honglak Lee, Kibok Lee, and Jinwoo Shin. Training confidence-calibrated classifiers for detecting out-of-distribution samples. In International Conference on Learning Representations, 2018.
10

[18] S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classification of aircraft. Technical report, Johns Hopkins University, 2013.
[19] Sébastien Marcel and Yann Rodriguez. Torchvision the machine-vision package of torch. In Proceedings of the 18th ACM international conference on Multimedia, pages 1485­1488, 2010.
[20] Horea Mures¸an and Mihai Oltean. Fruit recognition from images using deep learning. Acta Universitatis Sapientiae, Informatica, 10(1):26­42, 2018.
[21] Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms. arXiv:1803.02999, 2018.
[22] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In Advances in neural information processing systems, pages 4077­4087, 2017.
[23] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales. Learning to compare: Relation network for few-shot learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1199­1208, 2018.
[24] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, koray kavukcuoglu, and Daan Wierstra. Matching networks for one shot learning. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29, pages 3630­ 3638. Curran Associates, Inc., 2016.
[25] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The Caltech-UCSD Birds-200-2011 Dataset. Technical Report CNS-TR-2011-001, California Institute of Technology, 2011.
[26] Kuan-Chieh Wang, Paul Vicol, Eleni Triantafillou, Chia-Cheng Liu, and Richard Zemel. Outof-distribution detection in few-shot classification. OpenReview.net, 2019.
[27] Yaqing Wang, Quanming Yao, James Kwok, and Lionel M. Ni. Generalizing from a Few Examples: A Survey on Few-Shot Learning. In Intelligent Systems Design and Applications, pages 100­112. Springer, 2018.
[28] Xiang Zhang and Yann LeCun. Universum prescription: Regularization using unlabeled data. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 31, 2017.
11

CIFAR100

CUB-200

Omniglot

AUPR AUROC AUPR AUROC AUPR AUROC

MinDist LCBO

66.85 67.18 64.51 66.73 95.94 95.90 75.15 75.89 68.98 71.96 98.98 99.10

Background (ours) 67.71 65.97 61.26 60.38 98.38 98.29

GROOS (ours)

74.15 74.95 67.27 66.55 98.81 98.77

Centered GROOS (ours) 70.36 71.41 65.34 65.38 98.65 98.62

Table 4: Results for the same set of experiments reported in Table 1 but using a 4-Conv encoder rather than a ResNet18 encoder. We put an  next to the top score in each column and set in bold all the rest of the scores in the column that are within 0.5 of this.

A Appendix
A.1 Encoder size
Given that much of the metric-based few-shot learning literature uses small encoders, in Figure 4 we include results for the "standard" few-shot experiments using a 4-Conv encoder (as in [22, 26]) rather than the ResNet18 encoder used in Section 4.1. Interestingly, we find that with a smaller encoder the LCBO method does significantly better relative to other approaches, indicating that learning decision boundaries for OOS detection may be a more effective strategy in either a lower dimensional feature space or for less rich encoders. In future work it would be interesting to investigate whether attaching a larger MLP helps LCBO scale to larger encoders. Of course, including more fully-connected layers quickly becomes expensive which would be a potential downside of this method.
We also repeated the generalization experiments from Section 4.2 (which also used a ResNet18 encoder) with a 4-Conv encoder and a ResNet50 encoder. We summarize our results in Figures 5 and 6. The logic behind our choice to also test larger encoders in this setting stemmed from the observation that in tasks that require higher levels of generalization, large encoders can sometimes yield better results [8]. We find that larger encoders do tend to slightly improve performance in terms of AUROC and AUPR. With the exception of AUPR for the Aircraft dataset where MinDist performed slightly better than Centered GROOS when we used a ResNet50 encoder instead of a ResNet18 encoder, the top performing model on a dataset did not change based on whether one used a larger encoder. It is perhaps notable that the Aircraft dataset is also one of the few examples where model performance decreased when using a ResNet50 encoder rather than a ResNet18 encoder.
Distinct from the pattern we observed in Figure 4, in this setting using a smaller encoder did not appear to result in much better performance for LCBO. With the exception of its performance on ImageNet itself, which does not require the same level of generalization, LCBO did not out-perform other methods on any of the datasets. We suspect that this arises from the fact that learning decision boundaries is not an approach that transfers well to significantly different datasets. Similar to the results from Section 4.2 we observe that the top models in terms of generalization were Centered GROOS and MinDist suggesting that centered generic points and raw distance are better able to capture "different-ness" across datasets. We also observe that in the smaller encoder setting, MinDist is more competitive with Centered GROOS.

A.2 Proofs from Section 4.3
Proof of Proposition 4.1. For any x  Rd, either (1) there are at least two i, j for i, j  {1, . . . , k, oos} such that ||x - i|| = ||x - j|| or (2) for all i, j either ||x - i|| > ||x - j|| or ||x - i|| < ||x - j||. In the former case, x  B since x belongs to Hi,j as this hyperplane consists precisely of those x such that ||x - i|| = ||x - j||. In the latter case the set
D = ||x - i|| | i  {1, . . . , k, oos} consists of distinct real numbers. It is clear that these numbers can be ordered so that they are strictly increasing. Denote by  the permutation from Sk+1 such that
||x - (1)|| < ||x - (2)|| < · · · < ||x - (oos)||.

12

ImageNet CIFAR100 Omniglot Aircraft Textures Fruits

MinDist

71.28

4-Conv encoder 63.36

67.45

54.07

57.29 95.97

LCBO

75.51

60.36

58.68 52.49 58.16 87.35

Background (ours) GROOS (ours)
Centered GROOS (ours)

61.78 75.99 61.78

60.78 59.22 64.80

65.10 61.44 67.03

52.95 53.59 54.87

55.75 59.44 61.77

92.57 90.88 94.80

MinDist

ResNet50 encoder

97.51

82.33

84.54

58.49

76.46 92.57

LCBO

95.66

79.54

73.93 55.72 74.48 90.72

Background (ours) GROOS (ours)
Centered GROOS (ours)

95.57 97.76 96.96

79.11 79.17 84.20

60.61 78.55 81.36

53.86 53.38 59.07

78.48 80.60 84.17

92.30 94.19 96.40

Table 5: AUROC results for the same set of experiments reported on in Table 2 but using a 4-Conv encoder (top) and ResNet50 encoder (bottom) rather than a ResNet18 encoder. We put an  next to the top score in each column and set in bold all the rest of the scores in the column that are within 0.5 of this.

ImageNet CIFAR100 Omniglot Aircraft Textures Fruits

MinDist LCBO

70.44 74.50

4-Conv encoder 62.67 58.58

69.71 57.04

53.84 52.20

57.27 95.97 57.31 87.48

Background (ours)

60.67

59.07

61.99 52.39 55.75 91.66

GROOS (ours)

75.48

Centered GROOS (ours) 70.44

58.62 63.17

60.55 64.04

53.10 59.12 91.66 53.73 59.46 94.73

MinDist

ResNet50 encoder

97.63

81.41

85.62 58.72 78.77 91.66

LCBO

95.28

77.56

71.53 55.17 73.48 91.25

Background (ours) GROOS (ours)
Centered GROOS (ours)

95.63 97.68 96.75

79.26 77.16 83.27

77.26 79.53 81.19

57.87 54.63 58.24

77.09 76.87 82.02

95.08 94.51 96.48

Table 6: AUPR results for the same set of experiments reported on in Table 3 but using a 4-Conv encoder (top) and ResNet50 encoder (bottom) rather than a ResNet18 encoder. We put an  next to the top score in each column and set in bold all the rest of the scores in the column that are within 0.5 of this.

13

Then x  S. This shows that the union of B and each set in {S |   Sk+1} is equal to Rd. Using the distance parametrization of each S based on , it is also clear that B is disjoint from each S and that furthermore, S  S =  when  =  .
The fact that each S is open, and B is closed and measure zero follows from elementary topology/measure theory.

Proof of Proposition 4.2.

1. If 1, . . . , k are distinct from each other, then for any i  {1, . . . , k}, we can choose > 0 sufficiently small such that for all points x  B (i) we have that ||x - i|| < ||x - j|| for each j  {1, . . . , k} with j = i. Observe that

B (x) 

S ,

Sk+1 (1)=i

that is, B (x) belongs to the i-viable region of Rd. Hence the i-viable region is non-empty.

2. We give two examples, in the first there exists an element i  {1, . . . , k} such that the i-viable region is empty. In the second, for each i  {1, . . . , k}, the i-viable region is not empty. In both cases we leave it to the reader to verify the example.

(a) Consider the case d = 2, k = 2, oos = (1, 0), 1 = (0, 0), and 2 = (-1, 0). It can

be checked that in this case the 2-viable region consists of those points that are both

to

the

left

of

the

line

x

=

(0,

0)

and

to

the

right

of

the

line

x

=

(

1 2

,

0).

This

set

is

of

course empty.

(b) Consider the case d = 2, k = 4, oos = (0, 0), 1 = (1, 0), 2 = (0, 1), 3 = (-1, 0),

and 4 = (0, -1). Elementary calculations show that the 1-viable region is nonzero

and

defined

by

the

inequalities

y

>

-

1 2

,

y

<

1 2

,

and

x

>

1 2

.

The

2,

3,

and

4-viable

regions can be obtained from the 1-viable region via symmetry transformations.

To prove Proposition 4.3, we need to establish a couple short lemmas: Lemma A.1. Let i and j be distinct prototypes. If two points x and y satisfy the inequalities

||x - i|| < ||x - j|| and ||y - i|| < ||y - j||,

then for any point z on the line segment connecting these two points,

||z - i|| < ||z - j||.

(2)

If, instead, ||x - i|| = ||x - j|| and ||y - i|| < ||y - j||,
the strict inequality (2) holds at every point on \ {x}.

Proof. To prove the first part of the Lemma, notice that both x and y lie on the same side of the hyperplane Hi,j . Since a hyperplane splits Rd into two convex half-spaces, the entire segment lies on a single side of this hyperplane and the result follows.
The last statement is true since, if the segment does not lie entirely in the plane Hi,j , it can only intersect at a single point, x (note that could also lie entirely within Hi,j but we know that the other end point of , y, is not in Hi,j ). Since x is the endpoint of the segment, the rest lies in one of the open half spaces, in this case, that whose points satisfy (2).
Lemma A.2. Let the i, j be as in Lemma A.1, x a point in the i-viable region and oos be a distinct generic representation. Let be the line segment between x and oos and z be the point on where it intersects Hoosi . Then the line segment from x to z is such that for any point w on this segment and for all j  {1, . . . , k} with j = i,
||w - i|| < ||w - oos|| < ||w - j||.

14

Similarly, if is the line segment from z to oos, then all w on ||w - oos|| < ||w - j||
for all j  {1, . . . , k} (including j = i).

satisfy

Proof. Notice that oos and x satisfy the inequalities

0 = ||oos - oos|| < ||oos - j|| and ||x - oos|| < ||x - j||

for any j  {1, . . . , k} with j = i. Applying Lemma A.1, this implies that z, which lies on the line segment connecting x an oos, satisfies

||z - oos|| < ||z - j||.

Since it lies on Hoos,i as well,

||z - i|| = ||z - oos|| < ||z - j||.

(3)

But since x satisfies

||x - i|| < ||x - oos|| < ||x - j||,

two applications of Lemma A.1 yield that for any point w on ,

||w - i|| < ||w - oos|| < ||w - j||.

This proves the first statement.

Next, returning to (3), we see that since

0 = ||oos - oos|| < ||oos - j || for any j  {1, . . . , k} (including i = j), then by Lemma A.1, for all w on

||w - oos|| < ||w - j||,

which proves the second statement.

we must have that

Using these lemmas, we can prove Proposition 4.3:

Proof. Let x be a point in the i-viable region of Rd and z be the point on the segment between x and oos that lies on the hyperplane Hi,oos . Note that must cross this hyperplane since x lies on one side of Hi,oos , being closer to i than to oos, and oos lies on the other.
By Lemma A.2, all points w of on the same side of Hi,oos as x satisfy

||w - i|| < ||w - oos|| < ||w - j||,

for all j  {1, . . . , k} with j = i. All such points are i-viable. All points on on the same side of

Hi,oos as oos satisfy

||w - oos|| < ||w - j||

for all j  {1, . . . , k} including j = i. It follows that these points are in the OOS-core. It is clear then that for any > 0, the ball B (z) contains both points from the i-viable region of Rd and the OOS-core. This proves the Proposition.

15

