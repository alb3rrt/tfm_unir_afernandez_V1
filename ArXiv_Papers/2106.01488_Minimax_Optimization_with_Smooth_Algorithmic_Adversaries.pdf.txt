arXiv:2106.01488v1 [cs.LG] 2 Jun 2021

Minimax Optimization with Smooth Algorithmic Adversaries
Tanner Fiez Chi Jin Praneeth Netrapalli Lillian J. Ratliff
University of Washington, Princeton University, Google Research, India
Abstract This paper considers minimax optimization minx maxy f (x, y) in the challenging setting where f can be both nonconvex in x and nonconcave in y. Though such optimization problems arise in many machine learning paradigms including training generative adversarial networks (GANs) and adversarially robust models, many fundamental issues remain in theory, such as the absence of efficiently computable optimality notions, and cyclic or diverging behavior of existing algorithms. Our framework sprouts from the practical consideration that under a computational budget, the max-player can not fully maximize f (x, ·) since nonconcave maximization is NP-hard in general. So, we propose a new algorithm for the min-player to play against smooth algorithms deployed by the adversary (i.e., the max-player) instead of against full maximization. Our algorithm is guaranteed to make monotonic progress (thus having no limit cycles), and to find an appropriate "stationary point" in a polynomial number of iterations. Our framework covers practical settings where the smooth algorithms deployed by the adversary are multi-step stochastic gradient ascent, and its accelerated version. We further provide complementing experiments that confirm our theoretical findings and demonstrate the effectiveness of the proposed approach in practice.
1 Introduction
This paper considers minimax optimization minx maxy f (x, y) in the context of two-player zero-sum games, where the min-player (controlling x) tries to minimize objective f assuming a worst-case opponent (controlling y) that acts so as to maximize it. Minimax optimization naturally arises in a variety of important machine learning paradigms, with the most prominent examples being the training of generative adversarial networks (GANs) [20] and adversarially robust models [40]. These applications commonly engage deep neural networks with various techniques such as convolution, recurrent layers, and batch normalization. As a result, the objective function f is highly nonconvex in x and nonconcave in y.
Theoretically, minimax optimization has been extensively studied starting from the seminal work of von Neumann [49], with many efficient algorithms proposed for solving it [30, 48, 55]. A majority of these classical results have been focused on convex-concave functions, and heavily rely on the minimax theorem, i.e., minx maxy f (x, y) = maxy minx f (x, y), which no longer holds beyond the convex-concave setting. Recent line of works [36, 37, 50, 51, 58] address the nonconvex-concave setting where f is nonconvex in x but concave in y by proposing meaningful optimality notions and designing computationally efficient algorithms to find such points. A crucial property heavily exploited in this setting is that the inner maximization over y given a fixed x can be computed efficiently, which unfortunately does not extend to the nonconvex-nonconcave setting.
Consequently, nonconvex-nonconcave optimization remains challenging, and many fundamental issues persist: it remains open what is an appropriate notion of optimality that can be computed efficiently; it is also unsettled on how to eliminate the cyclic or diverging behavior of existing algorithms. Practitioners often use simple and popular algorithms such as gradient descent ascent (GDA) and other variants for solving these challenging optimization problems. While these algorithm seem to perform well in some cases of adversarial training, they are highly unstable in other scenarios such as training GANs. Indeed the instability of GDA and other empirically popular methods is not surprising since they are known to not converge even in very simple settings [3, 10]. This current state of affairs strongly motivates the need to understand nonconvex-nonconcave minimax optimization more thoroughly and to design better algorithms for solving them.
The author emails are fiezt@uw.edu, chij@princeton.edu, pnetrapalli@google.com, and ratliffl@uw.edu.
1

This work considers the challenging nonconvex-nonconcave setting. Our framework sprouts from the practical consideration that under a computational budget, the max-player can not fully maximize f (x, ·) since nonconcave maximization is NP-hard in general. Instead, we assume that the max-player has a toolkit of multiple (potentially randomized) algorithms A1, A2, · · · , Ak in an attempt to solve the maximization problem given fixed x, and picks the best solution among these algorithms. This motivates us to study the surrogate of the minimax optimization problem as

minx maxi[k] f (x, Ai(x)) = minx maxk

k i=1

i

f

(x,

Ai(x)),

(1)

where k denotes the k-dimensional simplex, and Ai(x) denotes the output of algorithm Ai for a given x. When both the objective function f and the algorithms {Ai}ki=1 are smooth (defined formally in Section 3), we can show that (1) becomes a smooth nonconvex-concave minimax optimization problem, where recent advances can be leveraged in solving such problems.
In particular, given the smooth algorithms deployed by the adversary (i.e. the max-player), this paper proposes two algorithms for solving problems in (1). The first algorithm is based on stochastic gradient descent (SGD), which is guaranteed to find an appropriate notion of " -approximate stationary point" in O( -4) gradient computations. The second algorithm is based on proximal algorithm, in the case of deterministic adversarial algorithms {Ai}ki=1, this algorithm has an improved gradient complexity O( -3) or O~(poly(k)/ 2) depending on the choice of subroutine within the algorithm. All our algorithms are guaranteed to make monotonic progress, thus having no limit cycles.
Our second set of results show that, many popular algorithms deployed by the adversary such as multi-step stochastic gradient ascent, and multi-step stochastic Nesterov's accelerated gradient ascent are in fact smooth. Therefore, our framework readily applies to those settings in practice.
Finally, we present complementing experimental results using our theoretical framework and algorithms for generative adversarial network problems and adversarial training. The results highlight the benefits of our approach in terms of the stable monotonic improvement during training and also underscore the importance of optimizing through the algorithm of the adversary.

2 Related Work
We now cover related work on several relevant topics. Further details are provided in Appendix A Nonconvex-Nonconcave Zero-Sum Games. The existing work on nonconvex-nonconcave zero-sum
games has generally focused on (1) defining and characterizing local equilibrium solution concepts [15, 17, 26, 53, 54, 61], (2) designing gradient-based learning algorithms with local convergence guarantees to only a desired local equilibrium concept [1, 17, 26, 43, 59, 62], and (3) characterizing the local convergence behavior of gradient descent-ascent (GDA) [11, 16, 26, 42, 45, 47], since this is known to be computationally hard in general and GDA can get stuck in limit cycles [12, 25, 33]. The closest set of works to ours in this direction propose relaxed equilibrium notions that are shown to exist and be computable in polynomial time [28, 41]. The aforementioned works are similar to this paper in the sense that the min-player faces the max-player with computational restrictions, but are different from ours in terms of the model of the max-player and the algorithms to solve the problem.
Nonconvex-Concave Zero-Sum Games. The structure presented in nonconvex-concave zero-sum games makes it possible to achieve global finite-time convergence guarantees to ­approximate stationary points of the objective function f (·, ·) and the best-response function (·) = maxy f (·, y). A significant number of papers in the past few years investigate the rates of convergence that can be obtained for this problem [26, 29, 36, 37, 38, 39, 50, 51, 52, 58, 64]. The best known existing results in the deterministic setting show that ­approximate stationary points of the functions f (·, ·) and (·) can be obtained with gradient complexities of O( -2.5) [37, 51] and O( -3) [29, 37, 58, 64], respectively. Moreover, the latter notion of an ­approximate stationarity point can be obtained using O( -6) gradient calls in the stochastic setting of nonconvex-concave zero-sum games [52]. We build on the advances in nonconvex-concave problems to obtain our results.
Gradient-Based Learning with Opponent Modeling. A number of gradient-based learning schemes have been derived in various classes of games based on modeling opponent behavior and adjusting the gradient updates based on this prediction [8, 17, 18, 34, 46, 60]. In particular, several works model the opponent as

2

doing a gradient step and derive a learning rule by plugging in the predicted endpoint into the objective, evaluating a Taylor expansion around the last strategies to form an augmented objective, and then computing the gradient of this augmented objective [18, 34, 60]. In contrast, we directly compute the derivative of the objective function of the min-players through the model of the opponent. The only work that is similar in this manner is unrolled generative adversarial networks [46]. A key conceptual distinction of our framework is its sequential nature with the opponent initializing from scratch at each interaction. Moreover, we give provable finite-time convergence guarantees which do not appear in past work in this realm.

3 Preliminaries

In this section, we present problem formulation and preliminaries. We consider function f satisfying Assumption 1. We denote w = (x, y), and assume f : Rd1 × Rd2  R is:
(a) B-bounded i.e., |f (w)|  B,

(b) G-Lipschitz i.e., |f (w1) - f (w2)|  G w1 - w2 , (c) L-gradient Lipschitz i.e., f (w1) - f (w2)  L w1 - w2 , (d) -Hessian Lipschitz i.e., 2f (w1) - 2f (w2)   w1 - w2 . where · denotes Euclidean norm for vectors and operator norm for matrices.

We aim to solve minxRd1 maxyRd2 f (x, y). Since maxyRd2 f (x, y) involves non-concave maximization and hence is NP-hard in the worst case, we intend to play against algorithm(s) that y-player uses to compute her strategy. Concretely, given x  Rd1 , we assume that the y-player chooses her (potentially random) strategy yz(x) = Ai(x)(x, zi(x)), where we use shorthand z := (z1, · · · , zk), as i(x) = argmaxi[k] f (x, Ai(x, zi)),
where A1, · · · , Ak are k deterministic algorithms that take as input x and a random seed zi  R , where zi are all independent. Note that the framework captures randomized algorithms e.g., A could be stochastic
gradient ascent on f (x, ·), with initialization, minibatching etc. determined by the random seed z. This also
incorporates running the same algorithm multiple times, with different seeds and then choosing the best
strategy. We now reformulate the minimax objective function to:

minxRd1 g(x) where g(x) := Ez [f (x, yz(x))] .

(2)

For general algorithms Ai, the functions f (x, Ai(x, zi)) need not be continuous even when f satisfies Assumption 1. However, if the algorithms Ai are smooth as defined below, the functions f (x, Ai(x, zi)) behave much more nicely.

Definition 1 (Algorithm Smoothness). A randomized algorithm A : Rd1 × R  Rd2 is:

(a) G-Lipschitz, if A(x1, z) - A(x2, z)  G x1 - x2 for any z.

(b) L-gradient Lipschitz, if DA(x1, z) - DA(x2, z)  L x1 - x2 for any z.
Here DA(x, z)  Rd1 × Rd2 is the Jacobian of the function A(·, z) for a fixed z. The following lemma tells us that f (x, A(x, z)) behaves nicely whenever A is a Lipschitz and gradient Lipschitz algorithm. For deterministic algorithms, we also use the shortened notation A(x) and DA(x).

Lemma 1. Suppose A is G -Lipschitz and L -gradient Lipschitz and f satisfies Assumption 1. Then, for a fixed z, function f (·, A(·, z)) is G(1 + G )-Lipschitz and L(1 + G )2 + GL -gradient Lipschitz.

While g(x) defined in (2) is not necessarily gradient Lipschitz, it can be shown to be weakly-convex as defined below. Note that an L-gradient Lipschitz function is L-weakly convex.

Definition 2. A function g : Rd1  R is L-weakly convex if  x, there exists a vector ux satisfying:

g(x )  g(x) +

ux, x

-x

-

L 2

x

-x

2



x.

(3)

Any vector ux satisfying this property is called the subgradient of g at x and is denoted by g(x).

3

An important property of weakly convex function is that the maximum over a finite number of weakly convex function is still a weakly convex function.
Lemma 2. Given L-weakly convex functions g1, · · · , gk : Rd  R, the maximum function g(·) := maxi[k] gi(·) is also L-weakly convex and the set of subgradients of g(·) at x is given by:

g(x) = { jS(x) j gj (x) : j  0, jS(x) j = 1}, where S(x) := argmaxi[k] gi(x).

Consequently under Assumption 1 and the assumption that Ai are all G -Lipschitz and L -gradient Lipschitz, we have that g(·) defined in (2) is L(1 + G )2 + GL -weakly convex. The standard notion of optimality for weakly-convex functions is that of approximate first order stationary point [13].
Approximate first-order stationary point for weakly convex functions: In order to define approximate stationary points, we also need the notion of Moreau envelope.

Definition 3. The Moreau envelope of a function g : Rd1  R and parameter  is:

g(x) = minx Rd1 g(x ) + (2)-1 x - x 2 .

(4)

The following lemma provides useful properties of the Moreau envelope.

Lemma 3. For an L-weakly convex function g : Rd1  R and  < 1/L, we have:

(a) The minimizer x^(x) = arg minx Rd1 g(x ) + (2)-1 x - x 2 is unique and g(x^(x))  g(x)  g(x). Furthermore, arg minx g(x) = arg minx g(x).
(b) g is -1(1 + (1 - L)-1)-smooth and thus differentiable, and
(c) minug(x^(x)) u  -1 x^(x) - x = g(x) .
First order stationary points of a non-smooth nonconvex function are well-defined, i.e., x is a first order stationary point (FOSP) of a function g(x) if, 0  f (x). However, unlike smooth functions, it is nontrivial to define an approximate FOSP. For example, if we define an -FOSP as the point x with minug(x) u  , where g(x) denotes the subgradients of g at x, there may never exist such a point for sufficiently small , unless x is exactly a FOSP. In contrast, by using above properties of the Moreau envelope of a weakly convex function, it's approximate FOSP can be defined as [13]:

Definition 4. Given an L-weakly convex function g, we say that x is an -first order stationary point (-FOSP) if, g1/2L(x)  , where g1/2L is the Moreau envelope with parameter 1/2L.
Using Lemma 3, we can show that for any -FOSP x, there exists x^ such that x^ - x  /2L and minug(x^) u  . In other words, an -FOSP is O() close to a point x^ which has a subgradient smaller than . Other notions of FOSP proposed recently such as in [50] can be shown to be a strict generalization of the above definition.

4 Main Results
In this section, we present our main results. Assuming that the adversary employs Lipschitz and gradientLipschitz algorithms (Assumption 2), Section 4.1 shows how to compute (stochastic) subgradients of g(·) (defined in (2)) efficiently. Section 4.2 further shows that stochastic subgradient descent (SGD) on g(·) can find an -FOSP in O -4 iterations while for the deterministic setting, where the adversary uses only deterministic algorithms, Section 4.3 provides a proximal algorithm that can find an -FOSP faster than SGD. For convenience, we denote gz,i(x) := f (x, Ai(x, zi)) and recall g(x) := Ez maxi[k] gz,i(x) . For deterministic Ai, we drop z and just use gi(x).

4

Algorithm 1: Stochastic subgradient descent (SGD) Input: initial point x0, step size 
1 for s = 0, 1, . . . , S do 2 Sample z1, · · · , zk and compute g(xs) according to eq. (6). 3 xs+1  xs - g(xs).
4 return x¯  xs, where s is uniformly sampled from {0, · · · , S}.

4.1 Computing stochastic subgradients of g(x)

In this section, we give a characterization of subgradients of g(x) and show how to compute stochastic subgradients efficiently under the following assumption.

Assumption 2. Algorithms Ai in (2) are G -Lipschitz and L -gradient Lipschitz as per Definition 1.
Under Assumptions 1 and 2, Lemma 1 tells us that gz,i(x) is a G(1 + G )-Lipschitz and L(1 + G )2 + GL gradient Lipschitz function for every i  [k] with

gz,i(x) = xf (x, Ai(x, zi)) + DAi(x, zi) · yf (x, Ai(x, zi)),

(5)

where we recall that DAi(x, zi)  Rd1×d2 is the Jacobian matrix of Ai(·, zi) : Rd1  Rd2 at x and xf (x, Ai(x, zi)) denotes the partial derivative of f with respect to the first variable at (x, Ai(x, zi)). While there is no known general recipe for computing DAi(x, zi) for an arbitrary algorithm Ai, most algorithms used in practice such as stochastic gradient ascent (SGA), stochastic Nesterov accelerated gradient (SNAG), ADAM, admit efficient ways of computing these derivatives e.g., higher package in PyTorch [21]. For concreteness, we obtain expression for gradients of SGA and SNAG in Section 5 but the principle behind the derivation holds much more broadly and can be extended to most algorithms used in practice [21]. In practice, the cost of computing gz,i(x) in (5) is at most twice the cost of evaluating gz,i(x)--it consists a forward pass for evaluating gz,i(x) and a backward pass for evaluating its gradient [21].
Lemma 2 shows that g(x) := Ez maxi[k] gz,i(x) is a weakly convex function and a stochastic subgradient of g(·) can be computed by generating a random sample of z1, · · · , zk as:

g(x) =

jgz,i(x) for any   k, where S(x) := argmax gz,i(x)

(6)

jS(x)

i[k]

Here k is the k-dimensional probability simplex. It can be seen that Ez[^ g(x)]  g(x). Furthermore, if all Ai are deterministic algorithms, then the above is indeed a subgradient of g.

4.2 Convergence rate of SGD
The SGD algorithm to solve (2) is given in Algorithm 1. The following theorem shows that Algorithm 1 finds an -FOSP of g(·) in S = O -4 iterations. Since each iteration of Algorithm 1 requires computing the gradient of gz,i(x) for each i  [k] at a single point xs, this leads to a total of S = O -4 gradient computations for each gz,i(x).
Theorem 1. Under Assumptions 1 and 2, if S  16BLG2 -4 and learning rate  = (2B/[LG2(S + 1)])1/2 then output of Algorithm 1 satisfies E[ g1/2L(x¯) 2]  2, where L := L(1 + G )2 + GL and G := G(1 + G ).
Theorem 1 claims that the expected norm squared of the Moreau envelope gradient of the output point satisfies the condition for being an -FOSP. This, together with Markov's inequality, implies that at least half of x0, · · · , xS are 2 -FOSPs, so that the probability of outputting a 2 -FOSP is at least 0.5. Proposition 1 in Appendix C shows how to use an efficient postprocessing mechanism to output a 2 -FOSP with high probability. Theorem 1 essentially follows from the results of [13], where the key insight is that, in expectation, the SGD procedure in Algorithm 1 almost monotonically decreases the Moreau envelope evaluated at xs i.e., E g1/2L(xs) is almost monotonically decreasing. This shows that Algorithm 1 makes (almost) monotonic progress in a precise sense and hence does not have limit cycles. In contrast, none of the other existing algorithms for nonconvex-nonconcave minimax optimization enjoy such a guarantee.

5

Algorithm 2: Proximal algorithm

Input: initial point x0, target accuracy , smoothness parameter L 1  2/(64L)
2 for t = 0, 1, . . . , S do
3 Find xs+1 such that

maxi[k] gi(xs+1) + L xs - xs+1 2  minx maxi[k] gi(x) + L xs - x 2

4 if maxi[k] gi(xs+1) + L xs - xs+1 2  maxi[k] gi(xs) - 3 /4 then

5

return xs

+ /4

4.3 Proximal algorithm with faster convergence for deterministic algorithms
While the rate achieved by SGD is the best known for weakly-convex optimization with a stochastic subgradient oracle, faster algorithms exist for functions which can be written as maximum over a finite number of smooth functions with access to exact subgradients of these component functions. These conditions are satisfied when Ai are all deterministic and satisfy Assumption 2. A pseudocode of such a fast proximal algorithm, inspired by [58, Algorithm 3], is presented in Algorithm 2. However, in contrast to the results of [58], the following theorem provides two alternate ways of implementing Step 3 of Algorithm 2, resulting in two different (and incomparable) convergence rates.

Theorem 2. Under Assumptions 1 and 2, if L := L(1 + G )2 + GL + kG(1 + G ) and S  200LB -2 then

Algorithm 2 returns xs satisfying g1/2L(xs)  . Depending on whether we use [58, Algorithm 1] or cutting plane method [32] for solving Step 3 of Algorithm 2, the total number of gradient computations of

each gi is O

L2 B
3

or O

LB
2

·

poly(k) log

L

respectively.

Ignoring the parameters L, G, L and G , the above theorem tells us that Algorithm 2 outputs an -FOSP using O k -3 or O poly(k) -2 log 1 gradient queries to each gi depending on whether [58, Algorithm 3] or cutting plane method [32] was used for implementing Step 3. While the proximal algorithm itself works
even when Ai are randomized algorithms, there are no known algorithms that can implement Step (3) with fewer than O -2 stochastic gradient queries. Hence, this does not improve upon the O -4 guarantee
for Algorithm 1 when Ai are randomized algorithms. The proof of Theorem 2 shows that the iterates xs monotonically decrease the value g(xs), guaranteeing that there are no limit cycles for Algorithm 2 as well.

5 Smoothness of Popular Algorithms

In this section, we show that two popular algorithms--namely, T -step stochastic gradient ascent (SGA)

and T -step stochastic Nesterov's accelerated gradient ascent (SNAG)--are both Lipschitz and gradient-

Lipschitz satisfying Assumption 2 and hence are captured by our results in Section 4. Consider the setting

f (x, y)

=

1 n

j[n] fj(x, y). Let z be a random seed that captures the randomness in the initial point as

well as minibatch order in SGA and SNAG. We first provide the smoothness results on T -step SGA for

different assumptions on the shape of the function f and for T -step SNAG. After giving these results, we

make remarks interpreting their significance and the implications.

T -step SGA: For a given x and random seed z, the T -step SGA update is given by:
yt+1 = yt + yf(t)(x, yt)
where  : [T ]  [N ] is a sample selection function and  is the stepsize. Observe that with the same randomness z, the initial point does not depend on x i.e., y0(x) = y0(x ), so Dy0 = 0. The following theorems provide the Lipschitz and gradient Lipschitz constants of yT (x) (as generated by T -step SGA) for the general nonconvex-nonconcave setting as well as the settings in which the function f is nonconvex-concave and nonconvex-strongly concave.

6

Theorem 3 (General Case). Suppose for all j  [n], fj satisfies Assumption 1. Then, for any fixed randomness z, T -step SGA is (1 + L)T -Lipschitz and 4(/L) · (1 + L)2T -gradient Lipschitz.
Theorem 4 (Concave Case). Suppose for all j  [n], fj satisfies Assumption 1 and fj(x, ·) is concave for any x. Then, for any fixed randomness z, T -step SGA is LT -Lipschitz and (/L) · (1 + LT )3-gradient Lipschitz.
Theorem 5 (Strongly-concave Case). Suppose for all j  [n], fj satisfies Assumption 1 and fj(x, ·) is -strongly concave for any x. Then, for any fixed randomness z, T -step SGA is -Lipschitz and 4(/L) · 3gradient Lipschitz, where  = L/ is the condition number.
T -step SNAG: For a given random seed z, the T -step SNAG update is given by:
y~t =yt + (1 - )(yt - yt-1)
yt+1 =y~t + yf(t)(x, y~t),
where  is the stepsize,   [0, 1] is the momentum parameter. The output of the algorithm is given by A(x, z) = yT (x). Furthermore, we have the following guarantee.
Theorem 6 (General Case). Suppose for all j  [n], fj satisfies Assumption 1. Then, for any fixed seed z, T -step SNAG is T (1 + L/)T -Lipschitz and 50(/L) · T 3(1 + L/)2T -gradient Lipschitz.
Remarks on the Impact of the Smoothness Results: First, the Lipschitz and gradient Lipschitz parameters for T -step SGA and T -step SNAG in the setting where f is nonconvex-nonconcave are all exponential in T , the duration of the algorithm. In general, this seems unavoidable in the worst case for the above algorithms and seems to be the case for most of the other popular algorithms such as ADAM, RMSProp etc. as well. On the other hand, in the nonconvex-concave and nonconvex-strongly concave settings, our results show that the smoothness parameters of T -step SGA are no longer exponential in T . In particular, in the nonconvex-concave the Lipschitz parameter is linear in T and the gradient Lipschitz parameter is polynomial in T while in the nonconvex-strongly concave, the analogous smoothness parameters are no longer dependent on T . We conjecture this is also the case for T -step SNAG, though the proof appears quite tedious and hence, we opted to leave that for future work. For problems of practical importance, however, we believe that the smoothness parameters are rarely exponential in T . Our experimental results confirm this intuition ­ see Figure 2. Second, while we prove the Lipschitz and gradient Lipschitz properties only for SGA and SNAG, we believe that the same techniques could be used to prove similar results for several other popular algorithms such as ADAM, RMSProp etc. However, there are other algorithms, particularly those involving projection that are not gradient Lipschitz (see Proposition 3 in Appendix E.5).

6 Empirical Results

This section presents empirical results evaluating our SGD algorithm (Algorithm 1) for generative adversarial networks [20] and adversarial training [40]. We demonstrate that our framework results in stable monotonic improvement during training and that the optimization through the algorithm of the adversary is key to robustness and fast convergence. Finally, we show that in practice the gradient norms do not grow exponentially in the number of gradient ascent steps T taken by the adversary.
Generative Adversarial Networks. A common and general formulation of generative adversarial networks is characterized by the following minimax optimization problem (see, e.g., [45, 47]):

min max f (, ) = ExpX [ (D(x))] + EzpZ [ (-D(G(z)))].

(7)

In this formulation G : Z  X is the generator network parameterized by  that maps from the latent space Z to the input space X , D : X  R is discriminator network parameterized by  that maps from the input space X to real-valued logits, and pX and pZ are the distributions over the input space and the latent space.

7

Figure 1: Dirac-GAN: Generator parameters while training using simultaneous and alternating gradient descentascent (left), and our framework (right) with & without optimizing through the discriminator. Under our framework, training is stable and converges to correct distribution. Further, differentiating through the discriminator results in faster convergence.

Figure 2:

Adversarial training:

f (, A()) as a function of number

of steps T taken by gradient ascent (GA)

algorithm A evaluated at multiple points in

the training procedure. The plot shows that,

in practice, the Lipschitz parameter of GA

does not grow exponentially in T .

(a) Real Data

(b) 10k

(c) 40k

(d) 70k

(e) 100k

(f) 130k

(g) 150k

Figure 3: Mixture of Gaussians: Generated distribution at various steps during course of training. We see that training is stable and results in monotonic progress towards the true distribution.

The loss function defines the objective where (w) = - log(1 + exp(-w)) recovers the original "saturating" generative adversarial networks formulation [20].
Dirac­GAN. The Dirac-GAN [45] is a simple and common baseline for evaluating the efficacy of generative adversarial network training methods. In this problem, the generator distribution G(z) =  is a Dirac distribution concentrated at , the discriminator network D(x) = -x is linear, and the real data distribution pX = 0 is a Dirac distribution concentrated at zero. The resulting objective after evaluating (7) with the loss function (w) = - log(1 + exp(-w)) is
min max f (, ) = () + (0) = - log(1 + e-) - log(2).
To mimic the real data distribution, the generator parameter  should converge to  = 0. Notably, simultaneous and alternating gradient descent-ascent are known to cycle and fail to converge on this problem (see Figure 1). We consider an instantiation of our framework where the discriminator samples an initialization uniformly between [-0.1, 0.1] and performs T = 10 steps of gradient ascent between each generator update. The learning rates for both the generator and the discriminator are  = 0.01. We present the results in Figure 1. Notably, the generator parameter monotonically converges to the optimal  = 0 and matches the real data distribution using our training method. We also show the performance when the generator descends using partial gradient f (x, A()) instead of the total gradient f (, A()) in Algorithm 1. This method is able to converge to the optimal generator distribution but at a slower rate. Together, this example highlights that our method fixes the usual cycling problem by reinitializing the discriminator and also that optimizing through the discriminator algorithm is key to fast convergence. Additional results are given in Appendix F.
Mixture of Gaussians. We now demonstrate that the insights we developed from the Dirac-GAN (stability and monotonic improvement) carry over to the more complex problem of learning a 2-dimensional mixture of Gaussians. This is a common example and a number of papers (see, e.g., [4, 44, 46]) show that standard training methods using simultaneous or alternating gradient descent-ascent can fail. The setup

8

Figure 4: Adversarial training: Test accuracy during course of training where the attack used during training is gradient ascent (GA) with learning rate (LR) of 4 and number of steps (Steps) of 10 but evaluated against attacks with different Steps and LR. These plots show that training with a single attack gives more robustness to even other attacks with different parameters or algorithms, compared to standard training. Further, using total gradient f (, A()) yields better robustness compared to using partial gradient f (, A()) as is done in standard adversarial training [40].
for the problem is as follows. The real data distribution consists of 2-dimensional Gaussian distributions with means given by µ = [sin(), cos()] for   {k/4}7k=0 and each with covariance 2I where 2 = 0.05. For training, the real data x  R2 is drawn at random from the set of Gaussian distributions and the latent data z  R16 is drawn from a standard normal distribution with batch sizes of 512. The network for the generator and discriminator contain two and one hidden layers respectively, each of which contain 32 neurons and ReLU activation functions. We consider the objective from (7) with (w) = - log(1 + exp(-w)) which corresponds to the "saturating" generative adversarial networks formulation [20]. This objective is known to be difficult to train since with typical training methods the generator gradients saturate early in training.
We show results using our framework in Figure 3 where the discriminator performs T = 15 steps of gradient ascent and the initialization between each generator step is obtained by the default network initialization in Pytorch. The generator and discriminator learning rates are both fixed to be  = 0.5. We see that our method has stable improvement during the course of training and recovers close to the real data distribution. We demonstrate in Appendix F that this result is robust by presenting the final output of 10 runs of the procedure. Notably, the training algorithm recovers all the modes of the distribution in each run. We also show results using Adam for the discriminator in Appendix F.
Adversarial Training. Given a data distribution D over pairs of examples x  Rd and labels y  [k], parameters  of a neural network, a set S  Rd of allowable adversarial perturbations, and a loss function (·, ·, ·) dependent on the network parameters and the data, adversarial training amounts to considering a minmax optimization problem of the form
min E(x,y)D[maxS (, x + , y)].
In practice [40], the inner maximization problem maxS (, x + , y) is solved using projected gradient ascent. However, as described in Section 5, this is not a smooth algorithm and does not fit our framework. So, we use gradient ascent, without projection, for solving the inner maximization.
We run an adversarial training experiment with the MNIST dataset, a convolutional neural network, and the cross entropy loss function. We compare Algorithm 1 with usual adversarial training [40] which descends f (, A()) instead of f (, A()), and a baseline of standard training without adversarial training. For each algorithm, we train for 100 passes over the training set using a batch size of 50. The minimization procedure has a fixed learning rate of 1 = 0.0001 and the maximization procedure runs for T = 10 steps with a fixed learning rate of 2 = 4. We evaluate the test classification accuracy during the course of training against gradient ascent or Adam optimization adversarial attacks. The results are presented in Figure 4 where the mean accuracies are reported over 5 runs and the shaded regions show one standard deviation around the means. We observe that the adversarial training procedure gives a significant boost in robustness compared to standard training. Moreover, consistent with the previous experiments, our algorithm which uses total gradient outperforms standard adversarial training which uses only partial gradient. We present
9

results against more attacks in Appendix F. As suggested in Section 5, we also find that in practice, the gradient norms f (, A()) do not grow exponentially in the number of gradient ascent steps T in the adversary algorithm A (see Figure 2). For further details and additional results see Appendix F.
7 Conclusion
In this paper, we presented a new framework for solving nonconvex-nonconcave minimax optimization problems based on the assumption that the min player has knowledge of the smooth algorithms being used by max player, proposed new efficient algorithms under this framework and verified the efficacy of these algorithms in practice on small-scale generative adversarial network and adversarial training problems. There are several interesting directions for future work such as understanding the efficacy of these algorithms on large scale problems, developing new techniques to deal with nonsmooth algorithms such as projected gradient ascent and extending this framework to more general settings such as nonzero sum games.
References
[1] L. Adolphs, H. Daneshmand, A. Lucchi, and T. Hofmann. Local saddle point optimization: A curvature exploitation approach. In International Conference on Artificial Intelligence and Statistics, pages 486­495, 2019.
[2] K. Atkinson, W. Han, and D. E. Stewart. Numerical solution of ordinary differential equations, volume 108. John Wiley & Sons, 2011.
[3] J. P. Bailey, G. Gidel, and G. Piliouras. Finite regret and cycles with fixed step-size via alternating gradient descent-ascent. In Conference on Learning Theory, pages 391­407. PMLR, 2020.
[4] D. Balduzzi, S. Racaniere, J. Martens, J. Foerster, K. Tuyls, and T. Graepel. The mechanics of n-player differentiable games. In International Conference on Machine Learning, pages 354­363, 2018.
[5] T. Baar and G. J. Olsder. Dynamic noncooperative game theory. SIAM, 1998.
[6] D. P. Bertsekas. Convex optimization theory. Athena Scientific Belmont, 2009.
[7] B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. vSrndi, P. Laskov, G. Giacinto, and F. Roli. Evasion attacks against machine learning at test time. In European conference on machine learning and knowledge discovery in databases, pages 387­402, 2013.
[8] B. Chasnov, T. Fiez, and L. J. Ratliff. Opponent anticipation via conjectural variations. In Smooth Games Optimization and Machine Learning Workshop at NeurIPS 2020: Bridging Game Theory and Deep Learning, 2020.
[9] E. A. Coddington and N. Levinson. Theory of ordinary differential equations. Tata McGraw-Hill Education, 1955.
[10] C. Daskalakis and I. Panageas. The limit points of (optimistic) gradient descent in min-max optimization. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, pages 9256­9266, 2018.
[11] C. Daskalakis and I. Panageas. The limit points of (optimistic) gradient descent in min-max optimization. In Advances in Neural Information Processing Systems, pages 9236­9246, 2018.
[12] C. Daskalakis, S. Skoulakis, and M. Zampetakis. The complexity of constrained min-max optimization. In ACM Symposium on Theory of Computing, 2021.
[13] D. Davis and D. Drusvyatskiy. Stochastic subgradient method converges at the rate o(k-1/4) on weakly convex functions. arXiv preprint arXiv:1802.02988, 2018.
10

[14] T. Deleu, T. Würfl, M. Samiei, J. P. Cohen, and Y. Bengio. Torchmeta: A meta-learning library for pytorch. arXiv preprint arXiv:1909.06576, 2019.
[15] F. Farnia and A. Ozdaglar. Do gans always have nash equilibria? In International Conference on Machine Learning, pages 3029­3039, 2020.
[16] T. Fiez and L. Ratliff. Local convergence analysis of gradient descent ascent with finite timescale separation. In International Conference on Learning Representations, 2021.
[17] T. Fiez, B. Chasnov, and L. Ratliff. Implicit learning dynamics in stackelberg games: Equilibria characterization, convergence analysis, and empirical study. In International Conference on Machine Learning, pages 3133­3144, 2020.
[18] J. Foerster, R. Y. Chen, M. Al-Shedivat, S. Whiteson, P. Abbeel, and I. Mordatch. Learning with opponent-learning awareness. In International Conference on Autonomous Agents and MultiAgent Systems, pages 122­130, 2018.
[19] Y. Freund, M. Kearns, Y. Mansour, D. Ron, R. Rubinfeld, and R. E. Schapire. Efficient algorithms for learning to play repeated games against computationally bounded adversaries. In Proceedings of IEEE 36th Annual Foundations of Computer Science, pages 332­341. IEEE, 1995.
[20] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial networks. In Advances in Neural Information Processing Systems, 2014.
[21] E. Grefenstette, B. Amos, D. Yarats, P. M. Htut, A. Molchanov, F. Meier, D. Kiela, K. Cho, and S. Chintala. Generalized inner loop meta-learning. arXiv preprint arXiv:1910.01727, 2019.
[22] J. Y. Halpern, R. Pass, and L. Seeman. Decision theory with resource-bounded agents. Topics in cognitive science, 6(2):245­257, 2014.
[23] P. Hartman. Ordinary Differential Equations. Society for Industrial and Applied Mathematics, second edition, 2002. doi: 10.1137/1.9780898719222.
[24] N. J. Harvey, C. Liaw, and S. Randhawa. Simple and optimal high-probability bounds for strongly-convex stochastic gradient descent. arXiv preprint arXiv:1909.00843, 2019.
[25] Y.-P. Hsieh, P. Mertikopoulos, and V. Cevher. The limits of min-max optimization algorithms: Convergence to spurious non-critical sets. arXiv preprint arXiv:2006.09065, 2020.
[26] C. Jin, P. Netrapalli, and M. Jordan. What is local optimality in nonconvex-nonconcave minimax optimization? In International Conference on Machine Learning, pages 4880­4889, 2020.
[27] S. Kakade, S. Shalev-Shwartz, A. Tewari, et al. On the duality of strong convexity and strong smoothness: Learning applications and matrix regularization. Unpublished Manuscript, http://ttic. uchicago. edu/shai/papers/KakadeShalevTewari09. pdf, 2(1), 2009.
[28] V. Keswani, O. Mangoubi, S. Sachdeva, and N. K. Vishnoi. Gans with first-order greedy discriminators. arXiv preprint arXiv:2006.12376, 2020.
[29] W. Kong and R. D. Monteiro. An accelerated inexact proximal point method for solving nonconvexconcave min-max problems. arXiv preprint arXiv:1905.13433, 2019.
[30] G. M. Korpelevich. The extragradient method for finding saddle points and other problems. Matecon, 12:747­756, 1976.
[31] A. Kurakin, I. J. Goodfellow, and S. Bengio. Adversarial machine learning at scale. In International Conference on Learning Representations, 2017.
11

[32] Y. T. Lee, A. Sidford, and S. C.-w. Wong. A faster cutting plane method and its implications for combinatorial and convex optimization. In 2015 IEEE 56th Annual Symposium on Foundations of Computer Science, pages 1049­1065. IEEE, 2015.
[33] A. Letcher. On the impossibility of global convergence in multi-loss optimization. In International Conference on Learning Representations, 2021.
[34] A. Letcher, J. Foerster, D. Balduzzi, T. Rocktäschel, and S. Whiteson. Stable opponent shaping in differentiable games. In International Conference on Learning Representations, 2019.
[35] H. Li, Y. Tian, J. Zhang, and A. Jadbabaie. Complexity lower bounds for nonconvex-strongly-concave min-max optimization. arXiv preprint arXiv:2104.08708, 2021.
[36] T. Lin, C. Jin, and M. Jordan. On gradient descent ascent for nonconvex-concave minimax problems. In International Conference on Machine Learning, pages 6083­6093, 2020.
[37] T. Lin, C. Jin, M. Jordan, et al. Near-optimal algorithms for minimax optimization. In Conference on Learning Theory, pages 2738­2779, 2020.
[38] S. Lu, I. Tsaknakis, M. Hong, and Y. Chen. Hybrid block successive approximation for one-sided non-convex min-max problems: algorithms and applications. IEEE Transactions on Signal Processing, 2020.
[39] L. Luo, H. Ye, Z. Huang, and T. Zhang. Stochastic recursive gradient descent ascent for stochastic nonconvex-strongly-concave minimax problems. Advances in Neural Information Processing Systems, 33, 2020.
[40] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018.
[41] O. Mangoubi and N. K. Vishnoi. Greedy adversarial equilibrium: An efficient alternative to nonconvexnonconcave min-max optimization. In ACM Symposium on Theory of Computing, 2021.
[42] E. Mazumdar, L. J. Ratliff, and S. S. Sastry. On gradient-based learning in continuous games. SIAM Journal on Mathematics of Data Science, 2(1):103­131, 2020.
[43] E. V. Mazumdar, M. I. Jordan, and S. S. Sastry. On finding local nash equilibria (and only local nash equilibria) in zero-sum games. arXiv preprint arXiv:1901.00838, 2019.
[44] L. Mescheder, S. Nowozin, and A. Geiger. The numerics of gans. In Advances in Neural Information Processing Systems, pages 1823­1833, 2017.
[45] L. Mescheder, A. Geiger, and S. Nowozin. Which training methods for gans do actually converge? In International Conference on Machine Learning, pages 3481­3490, 2018.
[46] L. Metz, B. Poole, D. Pfau, and J. Sohl-Dickstein. Unrolled generative adversarial networks. In International Conference on Learning Representations, 2017.
[47] V. Nagarajan and J. Z. Kolter. Gradient descent gan optimization is locally stable. In Advances in Neural Information Processing Systems, pages 5591­5600, 2017.
[48] A. Nemirovski. Prox-method with rate of convergence o (1/t) for variational inequalities with lipschitz continuous monotone operators and smooth convex-concave saddle point problems. SIAM Journal on Optimization, 15(1):229­251, 2004.
[49] J. v. Neumann. Zur theorie der gesellschaftsspiele. Mathematische annalen, 100(1):295­320, 1928.
[50] M. Nouiehed, M. Sanjabi, T. Huang, J. D. Lee, and M. Razaviyayn. Solving a class of non-convex min-max games using iterative first order methods. In Advances in Neural Information Processing Systems, pages 14934­14942, 2019.
12

[51] D. M. Ostrovskii, A. Lowy, and M. Razaviyayn. Efficient search of first-order nash equilibria in nonconvex-concave smooth min-max problems. arXiv preprint arXiv:2002.07919, 2020.
[52] H. Rafique, M. Liu, Q. Lin, and T. Yang. Weakly-convex­concave min­max optimization: provable algorithms and applications in machine learning. Optimization Methods and Software, pages 1­35, 2021.
[53] L. J. Ratliff, S. A. Burden, and S. S. Sastry. Characterization and computation of local Nash equilibria in continuous games. In Allerton Conference on Communication, Control, and Computing, pages 917­924, 2013.
[54] L. J. Ratliff, S. A. Burden, and S. S. Sastry. On the characterization of local Nash equilibria in continuous games. IEEE Transactions on Automatic Control, 61(8):2301­2307, 2016.
[55] J. Robinson. An iterative method of solving a game. Annals of mathematics, pages 296­301, 1951. [56] T. W. Sandhlom and V. R. Lesser. Coalitions among computationally bounded agents. Artificial
intelligence, 94(1-2):99­137, 1997. [57] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing
properties of neural networks. In International Conference on Learning Representations, 2014. [58] K. K. Thekumparampil, P. Jain, P. Netrapalli, and S. Oh. Efficient algorithms for smooth minimax
optimization. Advances in Neural Information Processing Systems, 32:12680­12691, 2019. [59] Y. Wang, G. Zhang, and J. Ba. On solving minimax optimization locally: A follow-the-ridge approach.
In International Conference on Learning Representations, 2020. [60] C. Zhang and V. Lesser. Multi-agent learning with policy prediction. In AAAI Conference on Artificial
Intelligence, 2010. [61] G. Zhang, P. Poupart, and Y. Yu. Optimality and stability in non-convex-non-concave min-max
optimization. arXiv preprint arXiv:2002.11875, 2020. [62] G. Zhang, K. Wu, P. Poupart, and Y. Yu. Newton-type methods for minimax optimization. arXiv
preprint arXiv:2006.14592, 2020. [63] S. Zhang, J. Yang, C. Guzmán, N. Kiyavash, and N. He. The complexity of nonconvex-strongly-concave
minimax optimization. arXiv preprint arXiv:2103.15888, 2021. [64] R. Zhao. A primal dual smoothing framework for max-structured nonconvex optimization. arXiv preprint
arXiv:2003.04375, 2020.
13

A Detailed Related Work
Nonconvex-Nonconcave Zero-Sum Games. The existing work on nonconvex-nonconcave zero-sum games has generally focused on (1) defining and characterizing local equilibrium solution concepts and (2) analyzing the local stability and convergence behavior of gradient-based learning algorithms around fixed points of the dynamics. The concentration on local analysis stems from the inherent challenges that arise in nonconvex-nonconcave zero-sum games from both a dynamical systems perspective and a computational perspective. In particular, it is know that broad classes of gradient-based learning dynamics can admit limit cycles and other non-trivial periodic orbits that are antithetical to any type of global convergence guarantee in this class of games [25, 33]. Moreover, on constrained domains, it has been shown that finding even a local equilibrium is computationally intractable [12].
A number of local equilibrium notions for nonconvex-nonconcave zero-sum games now exist with characterizations in terms of gradient-based conditions relevant to gradient-based learning. This includes the local Nash [53, 54] and local minmax (Stackelberg) [17, 26] equilibrium concepts, which both amount to local refinements and characterizations of historically standard game-theoretic equilibrium notions. In terms of provable guarantees, algorithms incorporating higher-order gradient information have been proposed and analyzed that guarantee local convergence to only local Nash equilibria [1, 43] or local convergence to only local minmax equilibria [17, 59, 62] in nonconvex-nonconcave zero-sum games. Beyond the local Nash and minmax equilibrium, notions including the proximal equilibrium concept [15], which is a class between the set of local Nash and local minmax equilibria, and the local robust equilibrium concept [61], which includes both local minmax and local maxmin equilibria, have been proposed and studied. It is worth noting that a shortcoming of each of the local equilibrium notions is that may fail to exist on unconstrained domains.
Significant attention has been given to the local stability and convergence of simultaneous gradient descentascent in nonconvex-nonconcave zero-sum games. This stems from the fact that it is the natural analogue of learning dynamics for zero-sum game optimization to gradient descent for function optimization. Moreover, simultaneous gradient descent-ascent is know to often perform reasonably well empirically and is ubiquitous in a number of applications such as in training generative adversarial networks and adversarial learning. However, it has been shown that while local Nash are guaranteed to be stable equilibria of simultaneous gradient descent-ascent [11, 26, 42], local minmax may not be unless there is sufficient timescale separation between the minimizing and maximizing players [16, 26]. Specific to generative adversarial networks, it has been shown that simultaneous gradient descent-ascent locally converges to local equilibria under certain assumptions on the generator network and the data distribution [45, 47]. Later in this section we discuss in further detail learning dynamics studied previously in games which bear resemblance to that which we consider in this paper depending on the model of the maximizing player.
The challenges of nonconvex-nonconcave zero-sum games we have highlighted limit the types of provable guarantees that can be obtained and consequently motivate tractable relaxations including to nonconvexconcave zero-sum games and the general framework we formulate in this work. Before moving on, we mention that from a related perspective, a line of recent work [28, 41] in nonconvex-nonconcave zero-sum games proposes relaxed equilibrium notions that are shown to be computable in polynomial time and are guaranteed to exist. At a high level, the equilibria correspond to a joint strategy at which the maximizing player is at an approximate local maximum of the cost function and the minimizing player is at an approximate local minimum of a smoothed and relaxed best-response function of the maximizing player. The aforementioned works are similar to this paper in the sense that the minimizing player faces a maximizing player with computational restrictions, but diverge in terms of the model of the maximizing player and the algorithms for solving the problem.
Nonconvex-Concave Zero-Sum Games. The past few years has witnessed a significant amount of work on gradient-based dynamics in nonconvex-concave zero-sum games. The focus of existing work on nonconvex-concave zero-sum games has key distinctions from that in nonconvex-nonconcave zero-sum games. Generally, the work on nonconvex-concave zero-sum games has analyzed dynamics on constrained domains, where typically the strategy space of the maximizing player is constrained to a closed convex set and occasionally the minimizing player also faces a constraint. In contrast, nonconvex-nonconcave zero-sum games have generally been analyzed on unconstrained domains. Moreover, instead of focusing on computing notions of game-theoretic equilibrium as is typical in nonconvex-nonconcave zero-sum games, the body of work on nonconvex-concave zero-sum games has focused on achieving stationarity of the game cost function
14

f (·, ·) or the best-response function (·) = maxy f (·, y). The structure present in nonconvex-concave zero-sum games has been shown to simplify the problem
compared to nonconvex-nonconcave zero-sum games so that global finite-time convergence guarantees are achievable. Thus, work in this direction has focused on improving the rates of convergence in terms of the gradient complexity to find ­approximate stationary points of f (·, ·) or (·), both with deterministic and stochastic gradients. Guarantees on the former notion of stationarity can be translated to guarantees on the latter notion of stationarity with extra computational cost [36].
For the the class of nonconvex-strongly-concave zero-sum games, a series of works design algorithms that are shown to obtain ­approximate stationary points of the functions f (·, ·) or (·) with a gradient complexity of O( -2) in terms of in the deterministic setting [26, 36, 37, 38, 52]. In the deterministic nonconvex-strongly concave problem, the notions of stationarity are equivalent in terms of the dependence on
up to a logarithmic dependence [36]. Lower bounds for this problem have also been established [35, 63]. In the stochastic nonconvex-strongly-concave problem, existing work has developed algorithms that are shown to obtain ­approximate stationary points of the function (·) in gradient complexities of O( -4) [26, 36, 52] and O( -3) [39] in terms of dependence.
In the deterministic nonconvex-concave problem, a number of algorithms with provable guarantees to ­ approximate stationary points of the function f (·, ·) have been shown with gradient complexities of O( -4) [38], O( -3.5) [50], and O( -2.5) [37, 51]. Similarly, in this class of problems, there exist results on algorithms that guarantee convergence to an ­approximate stationary points of the function (·) with gradient complexities O( -6) [26, 36, 52] and O( -3) [29, 37, 58, 64]. Finally, existing results in the stochastic setting for achieving an ­approximate stationary point of (·) show gradient complexities of O( -6) [52] and O( -8) [36].
In this work, we build on the developments for nonconvex-concave problems to obtain our results. Gradient-Based Learning with Opponent Modeling. A number of gradient-based learning schemes have been derived is various classes of games based on the following idea: if a player knows how the opponents in a game are optimizing their cost functions, then it is natural to account for this behavior in the players own optimization procedure. The simultaneous gradient descent learning dynamics can be viewed as the simplest instantiation of this perspective, where each player is optimizing their own cost function assuming that all other players in the game will remain fixed. In general, the more sophisticated existing learning dynamics based on opponent modeling assume the opponents are doing gradient descent on their cost function and this prediction is incorporated into the objective being optimized in place of the current strategies of opponents. A key conceptual distinction between this approach and our work is that in existing opponent modeling methods the dynamics of the players are always updated simultaneously whereas the procedure we consider is sequential in nature with the opponent initializing again at each interaction. Moreover, the types of guarantees we prove are distinct compared to existing work in this realm. In this modern literature, gradient-based learning with opponent modeling dates back to the work of Zhang and Lesser [60]. They study simple two-player, two-action, general-sum matrix games, and analyze a set of learning dynamics called iterated descent descent with policy prediction (IGA-PP) and show asymptotic convergence to a Nash equilibrium. In this set of learning dynamics, each player assumes the other player is doing gradient descent and this prediction is used in the objective. In particular, each player i has a choice variable xi and a cost function fi(xi, x-i) that after incorporating the prediction becomes fi(xit, x-t i - -if-i(xit, x-t i)). To optimize the objective, each player takes a first-order Taylor expansion of their cost function to give the augmented objective
fi(xit, x-t i - -if-i(xit, x-t i))  fi(xit, x-t i) - -ifi(xti, xt-i) -if-i(xit, x-t i).
Each player in the game simultaneously follows the gradient of their augmented objective which is given by
ifi(xit, x-t i) - -i,ifi(xti, xt-i) -if-i(xit, x-t i).
This gradient computation is derived based on the fact that the assumed update of the other player -if-i(xit, x-t i) does not depend on the optimization variable.
Similar ideas have recently been revisited in more general nonconvex multiplayer games [18, 34]. In learning with opponent learning awareness (LOLA) [18], players again assume the other players are doing gradient descent and take their objective to be fi(xit, x-t i - -if-i(xit, x-t i)). To derive the learning rule, an augmented objective is again formed by computing a first-order Taylor expansion, but now the term
15

-if-i(xit, x-t i) in the augmented objective is treated as dependent on the optimization variable so that the gradient of the augmented objective is given by
ifi(xit, x-t i) - -i,ifi(xti, xt-i) -if-i(xit, x-t i) - -i,if-i(xit, x-t i) -ifi(xti, xt-i).
Finally, to arrive at the final gradient update for each player, the middle term in the equation above is removed and each player takes steps along the gradient update
ifi(xit, x-t i) - -i,if-i(xit, x-t i) -ifi(xti, xt-i).
While no convergence results are given for LOLA, a follow-up work shows local convergence guarantees to stable fixed points for IGA-PP and learning dynamics called stable opponent shaping (SOS) that interpolate between IGA-PP and LOLA [34]. A related work derives learning dynamic based on the idea that the opponent selects a best-response to the chosen strategy [17]. The resulting learning dynamics can be viewed as LOLA with the opponent selecting a Newton learning rate. For nonconvex-nonconcave zero-sum games, local convergence guarantees to only local Stackelberg equilibrium are given in for this set of learning dynamics [17]. It is worth remarking that gradient-based learning with opponent modeling is historically rooted in the general framework of consistent conjectural variations (see, e.g., [5, Chapter 4.6]), a concept that is now being explored again and is closely related to the previously mentioned learning dynamics [8].
Perhaps the closest work on gradient-based learning with opponent modeling to this paper is that of unrolled generative adversarial networks [46]. In unrolled generative adversarial networks, the generator simulates the discriminator doing a fixed number of gradient steps from the current parameter configurations of the generator and discriminator. The resulting discriminator parameters are then used in place of the current discriminator parameters in the generator objective. The generator then updates following the gradient of this objective, optimizing through the rolled out discriminator update by computing the total derivative. Simultaneously with the generator update, the discriminator updates its parameters by performing a gradient step on its objective. In our framework, for generative adversarial networks when the discriminator is modeled as performing T -steps of gradient ascent, the procedure we propose is similar but an important difference is that when the generator simulates the discriminator unrolling procedure the discriminator parameters are initialized from scratch and there is no explicit discriminator being trained simultaneously with the generator.
Games with computationally bounded adversaries: There are also a few works in the game theory literature which consider resource/computationally bounded agents. For example [19] considers repeated games between resource bounded agents, [56] considers coalition formation between resource bounded agents in cooperative games and [22] shows that resource constraints in otherwise rational players might lead to some commonly observed human behaviors while making decisions. However, the settings, models of limited computation and the focus of results considered in all of these prior works are distinct from those of this paper.
Stability of algorithms in numerical analysis: To our knowledge such results on the smoothness of the classes of algorithms we study--i.e., gradient-based updates such as SGA and SNAG--with respect to problem parameters (e.g., in this case, x) have not been shown in the machine learning and optimization literature. This being said, in the study of dynamical systems--more specifically differential equations--the concept of continuity (and Lipschitzness) with respect to parameters and initial data has been studied using a variational approach wherein the continuity of the solution of the differential equation is shown to be continuous with respect to variations in the parameters or initial data by appealing to nonlinear variation of parameters results such as the Bellman-Grownwall inequality or Alekseev's theorem (see classical references on differential equations such as [9, Chapter 2] or [23, Chapter IV.2]).
In numerical methods, such results on the "smoothness" or continuity of the differential equation with respect to initial data or problem parameters are used to understand stability of particular numerical methods (see, e.g., [2, Chapter 1.2]). In particular, a initial value problem is only considered well-posed if there is continuous dependence on initial data. For instance, the simple scalar differential equation
y(t) = -y(t) + 1, 0  t  T, y(0) = 1
has solution y(t)  1, yet the perturbed problem,
y (t) = -y (t) + 1, 0  t  T, y (0) = 1 + ,
16

has solution y (t) = 1 + e-t so that
|y(t) - y (t)|  | |, 0  t  T.
If the maximum error y - y  is (much) larger than then the initial value problem is ill-conditioned and any typical attempt to numerically solve such a problem will lead to large errors in the computed solution. In short, the stability properties of a numerical method (i.e., discretization of the differential equation) are fundamentally connected to the continuity (smoothness) with respect to intial data.
Observe that methods such as gradient ascent can be viewed as a discretization of an differential equation:
y(t) = yf (x, y(t)) - yk+1 = yk + yf (x, yk).
As such, the techniques for showing continuity of the solution of a differential equation with respect to initial data or other problem parameters (e.g., in this case x) can be adopted to show smoothness of the T -step solution of the discretized update. Our approach to showing smoothness, on the other hand, leverages the recursive nature of the discrete time updates defining the classes of algorithms we study. This approach simplifies the analysis by directly going after the smoothness parameters using the udpate versus solving the difference (or differential) equation for yT (x) and then finding the smoothness parameters which is the method typically used in numerical analysis of differential equations. An interesting direction of future research is to more formally connect the stability analysis from numerical analysis of differential equations to robustness of adversarial learning to initial data and even variations in problem parameters.

B Proof of results in Section 3

Proof of Lemma 1. For any fixed z, we note that A(·, z) is a deterministic algorithm. Consequently, it suffices to prove the lemma for a deterministic algorithm A(·). By chain rule, the derivative of f (x, A(x)) is given by:

f (x, A(x)) = xf (x, A(x)) + DA(x) · yf (x, A(x)),

(8)

where DA(x)  Rd1×d2 is the derivative of A(·) : Rd1  Rd2 at x and xf (x, A(x)) and yf (x, A(x)) denote the partial derivatives of f with respect to the first and second variables respectively at (x, A(x)). An easy computation shows that

f (x, A(x))  xf (x, A(x)) + DA(x) · yf (x, A(x))  G + G · G = (1 + G )G.

This shows that f (x, A(x)) is (1 + G )G-Lipschitz. Similarly, we have:

f (x1, A(x1)) - f (x2, A(x2))  xf (x1, A(x1)) - xf (x2, A(x2)) + DA(x1)yf (x1, A(x1)) - DA(x2)yf (x2, A(x2)) .

For the first term, we have:

xf (x1, A(x1)) - xf (x2, A(x2))  xf (x1, A(x1)) - xf (x2, A(x1)) + xf (x2, A(x1)) - xf (x2, A(x2))  L ( x1 - x2 + A(x1) - A(x2) )  L (1 + G ) x1 - x2 .

Similarly, for the second term we have:

DA(x1)yf (x1, A(x1)) - DA(x2)yf (x2, A(x2))  DA(x1) yf (x1, A(x1)) - yf (x2, A(x2)) + yf (x2, A(x2))  (LG (1 + G ) + GL ) x1 - x2 .

DA(x2) - DA(x1)

This proves the lemma.

17

Proof of Lemma 2. Given any x and y, and any  such that j  0 and jS(x) j = 1, we have:

g(y) = max gj(y) 
j[k]

jgj(y) 

j

gj(x) +

gj(x), y - x

1 -
2L

x-y

2

jS(x)

jS(x)

= g(x) +

jgj(x), y - x

1 -
2L

x-y

2,

jS(x)

proving the lemma.

Proof

of

Lemma

3.

We

re-write

f(x)

as

minimum

value

of

a

(

1 

- L)-strong

convex

function

,x,

as

g

is

L-weakly

convex

(Definition

2)

and

1 2

x-x

2

is

differentiable

and

1 

-strongly

convex,

g(x) = min
x Rd1

1 ,x(x ) = g(x ) + 2

x-x

2

.

(9)

Then first part of (a) follows trivially by the strong convexity. For the second part notice the following,

1

min
x

g(x)

=

min
x

min
x

g(x

)

+

2

x-x

2

1 = min min g(x ) +

x-x

2

xx

2

= min g(x )
x

Thus arg minx g(x) = arg minx g(x). For (b) we can re-write the Moreau envelope g as,

1

g(x)

=

min
x

g(x

)

+

2

x-x

2

=

x 2 - 1 max(xT x - g(x ) -

x

2
)

2  x

2

x2 1

·2 

=

- g(·) +

(x)

(10)

2 

2

where (·) is the Fenchel conjugation operator. Since L < 1/, using L-weak convexity of g, it is easy to see

that g(x ) +

x 2

2

is

(1 - L)-strongly

convex,

therefore

its

Fenchel

conjugate

would

be

1 (1-L)

-smooth

[27,

Theorem

6].

This,

along

with

1 

-smoothness

of

first

quadratic

term

implies

that

g(x)

is

1 

+

1 (1-L)

-smooth,

and thus differentiable.

For (c) we again use the reformulation of g(x) as minx Rd1 ,x(x ) (9). Then by first-order necessary condition for optimality of x^(x), we have that x - x^(x)  g(x). Further, from proof of part (a) we have

that ,x(x ) (1 - L)-strongly-convex in x and it is quadratic (and thus convex) in x. Then we can use

Danskin's theorem [6, Section 6.11] to prove that, g(x) = (x - x^(x))/  g(x).

C Proofs of Results in Section 4.2

In order to prove convergence of this algorithm, we first recall the following result from [13].

Theorem 7 (Corollary 2.2 from [13]). Suppose g(·) is L-weakly convex, and Ez1,··· ,zk g(x) 2  G2. Then, the output x¯ of Algorithm 1 with stepsize  =   satisfies:
S+1

g 1 (x0) - minx g(x) + LG22

E g 1 (x¯) 2  2 · 2L



.

2L

 S+1

Proof of Theorem 1. Lemmas 1 and 2 tell us that g(x) is L-weakly convex and for any choice of z, the stochastic subgradient g(x) is bounded in norm by G. Consequently, Theorem 7 with the stated choice of S proves Theorem 1.

18

Algorithm 3: Estimating Moreau envelope's gradient for postprocessing

Input: point x, stochastic subgradient oracle for function g, error , failure probability 

1 Find x such that g(x) + L x - x 2 

minx g(x ) + L x - x

2

2
+ 4L using [24, Algorithm 1].

2 return 2L(x - x).

Proposition 1. Given a point x, an L-weakly convex function g and a G-norm bounded and a stochastic

subgradient oracle to g (i.e., given any point x , which returns a stochastic vector u such that E[u]  g(x )

and u  G), with probability at least 1 - , Algorithm 3 returns a vector u satisfying u - g(x) 

with at most O

G2

log

1 

2

queries to the stochastic subgradient oracle of g.

Proof of Proposition 1. Let  1 (x , x) := g(x ) + L x - x 2. Recall the notation of Lemma 3 x 1 (x) :=

2L

2L

x-x 1 (x)

argminx

 1 (x , x) 2L

and

g(x) = minx

 1 (x , x). 2L

The

proof

of

Lemma

3

tells

us

that

g 1 (x) = 2L

2L


and also that

x 1 (x) - x 2L



G 2L

.

Since  1 (·, x) is a L-strongly convex and G Lipschitz function in 2L

the domain

x:

x 1 (x) - x 2L



G 2L

, [24, Theorem 3.1] tells us that we can use SGD with O

G2

log

1 

L

stochastic gradient oracle queries to implement Step 1 of Algorithm 3 with success probability at least 1 - ,

where

:=

2
4L .

Simplifying

this

expression

gives

us

a

stochastic

gradient

oracle

complexity

of

O

G2

log

1 

2

.

D Proofs of Results in Section 4.3

Proof of Theorem 2. Letting h(x, ) := i[k] igi(x), we note that xxh(x, ) = i[k] i2gi(x)  L(1 + G )2 + GL , where we used Lemma 1 and the fact that i |i|  1. On the other hand, again from Lemma 1, xh(x, ) = i[k] gi(x)  kG(1 + G ). Since h(x, ) = 0, we can conclude that h is an L-gradient Lipschitz function with L := L(1 + G )2 + GL + kG(1 + G ). Consequently, g(x) = maxS h(x, ),
where S :=   Rk : i  0, i[k] i = 1 , is L-weakly convex and the Moreau envelope g 1 is well defined. 2L Denote g(x, xs) := maxi[k] gi(x) + L x - xs 2. We now divide the analysis of each of iteration of
Algorithm 2 into two cases.
Case I, g(xs+1, xs)  maxi[k] gi(xs) - 3 /4: Since maxi[k] gi(xs+1)  g(xs+1, xs)  maxi[k] gi(xs) - 3 /4, we see that in this case g(xs) decreases monotonically by at least 3 /4 in each iteration. Since by Assumption 1, g is bounded by B in magnitude, and the termination condition in Step 4 guarantees monotonic
decrease in every iteration, there can only be at most 2B/(3 /4) = 8B/ such iterations in Case I.
Case II, g(xs+1, xs)  maxi[k] gi(xs) - 3 /4: In this case, we claim that xs is an -FOSP of g = maxi[k] gi(x). To see this, we first note that

g(xs) - 3

/4



g(xs+1,

xs)



(min
x

g(x)

+

L

x - xs

2) +

/4

=

g(xs)

<

min
x

g(x;

xs)

+

.

(11)

Let xs := arg minx g(x; xk). Since g is L-gradient Lipschitz, we note that g(·; xs) is L-strongly convex. We now use this to prove that xs is close to xs:

g(xs ;

xs)

+

L 2

xs - xs

2



g(xs; xs)

=

f (xs)

(a)
<

g(xk; xs) +

=

xs - xs <

2 L

(12)

where (a) uses (11). Now consider any x, such that 4 /L  x - xs . Then,

g(x) + L

x - xs

2 = max gi(x) + L
i[k]

x - xs

2

=

g(x;

xs)

(a)
=

g( xs ;

xs)

+

L 2

x - xs

2

(b)
 f (xs) -

L

+

( 2

x - xs

-

(c)
xs - xs )2  f (xs) + ,

(13)

where (a) uses uses L-strong convexity of g(·; xs) at its minimizer xs, (b) uses (11), and (b) and (c) use

triangle inequality, (12) and 4 /L  x - xs .

19

Now consider the Moreau envelope, g 1 (x) = minx  1 ,x(x ) where  1 ,x(x ) = g(x ) + L x - x 2. Then,

2L

2L

2L

we

can

see

that



1 2L

,xs

(x

)

achieves

its

minimum

in

the

ball

{x

|

x - xs

4

3(a). Then, with Lemma 3(b,c) and = 2 , we get that,

64 L

/L} by (13) and Lemma

g 1 (xs)  (2L) xs - x^ 1 (xs) = 8 L = ,

(14)

2L

2L

i.e., xs is an -FOSP of g.

By

combining

the

above

two

cases,

we

establish

that

8B 3

"outer"

iterations

ensure

convergence

to

a

-FOSP.

We now compute the gradient call complexity of each of these "outer" iterations, where we have two options

for implementing Step 3 of Algorithm 2. Note that this step corresponds to solving minx maxS h(x, ) up to an accuracy of /4.

Option I, [58, Algorithm 2]: Since the minimax optimization problem here is L-strongly-convex­concave

and 2L-gradient Lipschitz, [58, Theorem 1] tells us that this requires at most m gradient calls for each gi where,

6(2L)2

2

L

Lm2

= 4 28L

=

O



m

(15)

Therefore the number of gradient computations required for each iteration of inner problem is O L log2

1 

.

Option II, Cutting plane method [32]: Let us consider u() := minx h(x, ) + L x - xs 2, which is a L-Lipschitz, concave function of . [32] tells us that we can use cutting plane algorithms to obtain 

satisfying u()  maxS u() - using O k log kL gradient queries to u and poly(k log L ) computation.

The gradient of u is given by u() = h(x(), ), where x() := argminx h(x, ) + L x - xs 2. Since h(x, ) + L x - xs 2 is a 3L-smooth and L-strongly convex function in x, x() can be computed up to
error using gradient descent in O log L iterations. If we choose = 2/poly(k, L/µ), then Proposition 2

tells us that x() satisfies the requirements of Step (3) of Algorithm 2 and the total number of gradient calls to each gi is at most O poly(k) log L in each outer iteration of Algorithm 2.

Proposition 2. Suppose h : Rd1 × U  R be such that h(·, ) is µ-strongly convex for every   U , h(x, ·) is

concave for and let x

every x  Rd1 and h is () := argminx h(x, ).

L-gradient Lipschitz. Then, we have max

Let  be h(x(),

such that minx h(x, )  max

)  minx max h(x, )+c

L µ

minx ·+

h(x,
LDµU

)-  ·

,

where DU = max1,2U 1 - 2 is the diameter of U .

Proof of Proposition 2. From the hypothesis, we have:

 h(x, ) - h(x(), )  h(x, ) - h(x(), )  µ x - x() 2, 2

where (x, ) is the Nash equilibrium and the second step follows from the fact that  = argmax h(x, ) and the third step follows from the fact that x() = argminx h(x, ). Consequently, x - x()  2 /µ.

20

Let ¯ := argmax h(x(), ). We now have that:

max h(x(), ) - max min h(x, ) = h(x(), ¯) - h(x, )



x

= h(x(), ¯) - h(x(), ) + h(x(), ) - h(x, )

(1 )


h(x(), ), ¯ - 

L +
2

x() - x

2

(2 )


h(x(), )

¯ - 

L +

µ

(3 )


h(x, ) + L x() - x



 LDUµ 2

L +,
µ

L DU + µ

where (1) follows from the fact that h(x(), ·) is concave and x = argminx h(x, ), (2) follows from the bound x - x()  2 /µ, (3) follows from the L-gradient Lipschitz property of h, and the last step follows from the fact that h(x, ) = 0. This proves the proposition.

E Proofs of results in Section 5

In this appendix, we present the proofs for the lemmas in Section 5. Recall the SGA update:

yt+1 = yt + yf(t)(x, yt).

(16)

Therefore, the Jacobian of the T -step SGA update is given by

Dyt+1 = I + yyf(t)(x, yt) Dyt + yxf(t)(x, yt), with A(x, z) = yT (x).

(17)

E.1 Proof of Theorem 3
Theorem 3 (General Case). Suppose for all j  [n], fj satisfies Assumption 1. Then, for any fixed randomness z, T -step SGA is (1 + L)T -Lipschitz and 4(/L) · (1 + L)2T -gradient Lipschitz.
Proof. Lipschitz of yt(x). We first show the Lipschitz claim. We have the following bound on the Jacobian of the update equation given in (17):
Dyt+1(x)  (I + 2yyf (x, yt(x)))Dyt(x) +  2yxf (x, yt(x)) (1 + L) Dyt(x) + L.
Since Dy0(x) = 0, the above recursion implies that
t-1
Dyt(x)  L (1 + L)  (1 + L)t.
 =0
Gradient-Lipschitz of yt(x). Next, we show the claimed gradient Lipschitz constant. As above, using the update equation in (17), we have the following bound on the Jacobian:
Dyt+1(x1) - Dyt+1(x2)  (I + 2yyf (x1, yt(x1)))(Dyt(x1) - Dyt(x2)) +  2yxf (x1, yt(x1)) - 2yxf (x2, yt(x2))
+  [2yyf (x1, yt(x1)) - 2yyf (x2, yt(x2))]Dyt(x2) (1 + L) Dyt(x1) - Dyt(x2) + (1 + Dyt(x2) )( x1 - x2 + yt(x1) - yt(x2) ) (1 + L) Dyt(x1) - Dyt(x2) + 4(1 + L)2t x1 - x2 .

21

The above recursion implies the claimed Lipschitz constant. Indeed,
t-1
Dyt(x1) - Dyt(x2)  4 (1 + L)t+-1 x1 - x2  4(/L) · (1 + L)2t x1 - x2 .
 =0
E.2 Proof of Theorem 4
Theorem 4 (Concave Case). Suppose for all j  [n], fj satisfies Assumption 1 and fj(x, ·) is concave for any x. Then, for any fixed randomness z, T -step SGA is LT -Lipschitz and (/L) · (1 + LT )3-gradient Lipschitz.
Proof. Lipschitz of yt(x). We first show the Lipschitz claim. Using the update equation in (17), we have the following bound on the Jacobian:
Dyt+1(x)  (I + 2yyf (x, yt(x)))Dyt(x) +  2yxf (x, yt(x))  Dyt(x) + L.
Since Dy0(x) = 0, the above recursive implies that Dyt(x)  Lt. Gradient-Lipschitz of yt(x). Next, we show the claimed gradient Lipschitz constant. Using the update
equation in (17):, we have the following bound on the Jacobian:
Dyt+1(x1) - Dyt+1(x2)  (I + 2yyf (x1, yt(x1)))(Dyt(x1) - Dyt(x2)) +  2yxf (x1, yt(x1)) - 2yxf (x2, yt(x2))
+  [2yyf (x1, yt(x1)) - 2yyf (x2, yt(x2))]Dyt(x2)  Dyt(x1) - Dyt(x2) + (1 + Dyt(x2) )( x1 - x2 + yt(x1) - yt(x2) )  Dyt(x1) - Dyt(x2) + (1 + Lt)2 x1 - x2 .
This recursion implies the following gradient Lipschitz constant:
t-1
Dyt(x1) - Dyt(x2)   (1 + L )2 x1 - x2  (/L) · (1 + Lt)3 x1 - x2 .
 =0
E.3 Proof of Theorem 5
Theorem 5 (Strongly-concave Case). Suppose for all j  [n], fj satisfies Assumption 1 and fj(x, ·) is -strongly concave for any x. Then, for any fixed randomness z, T -step SGA is -Lipschitz and 4(/L) · 3gradient Lipschitz, where  = L/ is the condition number.
Proof. Denote the condition number  = L/. Lipschitz of yt(x). We first show the claimed Lipschitz constant. Using the update equation in (17), we
have that
Dyt+1(x)  (I + 2yyf (x, yt(x)))Dyt(x) +  2yxf (x, yt(x)) (1 - ) Dyt(x) + L.
Since Dy0(x) = 0, the above recursion gives the following bound:
t-1
Dyt(x)  L (1 - )  .
 =0
22

Gradient-Lipschitz of yt(x). Next we show the claimed gradient Lipschitz constant. Again, using the update equation, we have that
Dyt+1(x1) - Dyt+1(x2)  (I + 2yyf (x1, yt(x1)))(Dyt(x1) - Dyt(x2)) +  2yxf (x1, yt(x1)) - 2yxf (x2, yt(x2))
+  [2yyf (x1, yt(x1)) - 2yyf (x2, yt(x2))]Dyt(x2) (1 - ) Dyt(x1) - Dyt(x2) + (1 + Dyt(x2) )( x1 - x2 + yt(x1) - yt(x2) ) (1 - ) Dyt(x1) - Dyt(x2) + 42 x1 - x2 .
This recursion implies that
t-1
Dyt(x1) - Dyt(x2)  42 (1 - ) x1 - x2  4(/L) · 3 x1 - x2 .
 =0

E.4 Proof of Theorem 6

Theorem 6 (General Case). Suppose for all j  [n], fj satisfies Assumption 1. Then, for any fixed seed z, T -step SNAG is T (1 + L/)T -Lipschitz and 50(/L) · T 3(1 + L/)2T -gradient Lipschitz.

Proof. Recall the SNAG update

y~t = yt + (1 - )(yt - yt-1)

(18)

yt+1 = y~t + yf(t)(x, y~t).

(19)

Observe that the update equation for T -step SNAG implies that

Dy~t = Dyt + (1 - )(Dyt - Dyt-1)

(20)

Dyt+1 = (I + yyf(t)(x, y~t))Dy~t + yxf(t)(x, y~t)

Lipschitz of yt(x), vt(x). We first show the claimed Lipschitz constant. By the update equations in (20), we have that

Dyt+1 = (I + yyf(t)(x, y~t))(Dyt + (1 - )(Dyt - Dyt-1)) + yxf(t)(x, y~t).
Denote t = Dyt - Dyt-1 , and note that Dy0 = Dy-1 = 0 so that 0 = 0. By the equation above, we have that

t+1 L Dyt + (1 + L)(1 - )t + L
t
L  + (1 + L)(1 - )t + L.
 =1

In the following, we use induction to prove that

t  (1 + L/)t.

(21)

It is easy to verify that this is true for the base case 0 = 0  1. Suppose the claim is true for all   t, then we have that
t
t+1 L (1 + L/) + (1 + L)(1 - )(1 + L/)t + L
 =1 t
L (1 + L/) + (1 - )(1 + L/)t+1
 =0
=[(1 + L/)t+1 - 1] + (1 - )(1 + L/)t+1  (1 + L/)t+1.

23

This proves the induction claim. Therefore, by (21), we have the following two bounds:

Dyt(x) Dy~t(x)

t
   t(1 + L/)t,
 =1
(2 - ) Dyt(x) + (1 - ) Dyt-1(x)

 3t(1 + L/)t.

Gradient-Lipschitz of yt(x). Next, we show the claimed gradient Lipschitz constant. For any fixed x1, x2, denote wt = Dyt(x1) - Dyt(x2), we have

wt+1 =(I + yyf(t)(x1, y~t(x1)))(wt + (1 - )(wt - wt-1)) + (yxf(t)(x1, y~t(x1)) - yxf(t)(x2, y~t(x2)))

T1
+ (yyf(t)(x1, y~t(x1)) - yyf(t)(x2, y~t(x2)))(Dyt(x2) + (1 - )(Dyt(x2) - Dyt-1(x2))) .

T2
We note that we can upper bound the last two terms above as follows:

T1 + T2 ( x1 - x2 + y~t(x1) - y~t(x2) ) + ( x1 - x2 + y~t(x1) - y~t(x2) )(2 Dyt(x2) + Dyt-1(x2) )
24t2(1 + L/)2t x1 - x2 .

Therefore, let t =

wt - wt-1 , and  = x1 - x2 , we have the following:
t+1 L wt + (1 + L)(1 - )t + 24t2(1 + L/)2t
t
L  + (1 + L)(1 - )t + 24t2(1 + L/)2t.
 =1

In the following, we use induction to prove that

t  50(/L) · t2(1 + L/)2t := (t).

(22)

It is easy to verify that this is true for the base case 0 = 0. Suppose the claim is true for all   t, then we have that

t
t+1 50  2(1 + L/)2  + (1 + L)(1 - )(t) + 24t2(1 + L/)2t

 =1

(/L)

·

[50t2

(1 + (1

L/)2(t+1) + L/)2 -

- 1

1

+

24(L/)t2(1

+

L/)2t]

+

(1

-

)(t

+

1)

(/L) · [25t2(1 + L/)2(t+1) + 24t2(1 + L/)2(t+1)] + (1 - )(t + 1)

(/L) · [50t2(1 + L/)2(t+1)] + (1 - )(t + 1)  (t + 1).

This proves the induction claim. Therefore, by (22), we have that

t
Dyt(x1) - Dyt(x2) = wt    50(/L)t3(1 + L/)2t x1 - x2 .
 =1

E.5 Projected gradient ascent is not gradient-Lipschitz
Proposition 3. Consider f (x, y) = xy for (x, y)  X × Y, where X = [0, 10] and Y = [0, 1]. 1-step projected gradient ascent given by:
y1(x) = PY (y0 + yf (x, y0)) y0 = 0, where  > 1/10 is not a gradient-Lipschitz algorithm. Proof. We see that y1(x) = min(1, x) and f (x, y1(x)) = x min(1, x). For  < 1/10, we see that f (x, y1(x)) is not gradient-Lipschitz at x = 1/  (0, 10).
24

(b)

(c)

Figure 5: Dirac-GAN: Generator parameters while training using our framework with and without optimizing through the discriminator where between each generator update the discriminator samples an initial parameter choice uniformly at random from the interval [-0.5, 1] and then performs T = 100 (Figure 5b) and T = 1000 (Figure 5c) steps of gradient ascent.

F Additional Experiments and Details
In this appendix section, we provide additional experimental results and details. Dirac-GAN. In the results presented in Section 6 for this problem, the discriminator sampled its
initialization uniformly from the interval [-0.1, 0.1] and performed T = 10 steps of gradient ascent. For the results given in Figure 5, we allow the discriminator to sample uniformly from the interval [-0.5, 1] and consider the discriminator performing T = 100 (Figure 5b) and T = 1000 (Figure 5c) gradient ascent steps. The rest of the experimental setup is equivalent to that described in Section 6.
For the result presented in Figure 5b, we see that with this distribution of initializations for the discriminator and T = 100 gradient ascent steps, the generator is not able to converge to the optimal parameter of  = 0 to recreate the underlying data distribution using our algorithm which descends f (, A()) or the algorithm that descends f (, A()). However, we see that our algorithm converges significantly closer to the optimal parameter configuration. Furthermore, we still observe stability and convergence from our training method, whereas standard training methods using simultaneous or alternating gradient descent-ascent always cycle. This example highlights that the optimization through the algorithm of the adversary is important not only for the rate of convergence, but it also influences what the training method converges to and gives improved results in this regard.
Finally, in the result presented in Figure 5b, we see that with this distribution of initializations for the discriminator and T = 1000 gradient ascent steps, the generator is able to converge to the optimal parameter of  = 0 to recreate the underlying data distribution using our algorithm which descends f (, A()) or the algorithm that descends f (, A()). Thus, while with T = 100 we did not observe convergence to the optimal generator parameter, with a stronger adversary we do see convergence to the optimal generator parameter. This behavior can be explained by the fact that when the discriminator is able to perform enough gradient ascent steps to nearly converge, the gradients f (, A()) and f (, A()) are nearly equivalent.
We remark that we repeated the experiments 5 times with different random seeds and show the mean generator parameters during the training with a window around the mean of a standard deviation. The results were very similar between runs so the window around the mean is not visible.
Mixture of Gaussians. We noted in Section 6 that we repeated our experiment training a generative adversarial network to learn a mixture of Gaussians 10 times and observed that for each run of the experiment our training algorithm recovered all modes of the distribution. We now show those results in Figure 6. In particular, in Figure 6a we show the real data distribution and in Figures 6b­6k we show the final generated distribution from 10 separate runs of the training procedure after 150k generator updates. Notably, we observe that each run of the training algorithm is able to generate a distribution that closely resembles the underlying data distribution, showing the stability and robustness of our training method.
We also performed an experiment on the mixture of Gaussian problem in which the discriminator algorithm

25

(b)

(c)

(d)

(e)

(f )

(a) Real Data

(g)

(h)

(i)

(j)

(k)

Figure 6: Mixture of Gaussians: Figure 6a shows the real data distribution and Figures 6b­6k show the final generated distributions after 150k generator updates from 10 separate runs of the training procedure described in Section 6 using gradient ascent for the discriminator. Each run recovers a generator distribution closely resembling the underlying data distribution.

(b)

(c)

(d)

(a) Real Data

(h)

(e)

(f )

(g)

Figure 7: Mixture of Gaussians: Figure 7a shows the real data distribution and Figures 7b­7h show the final generated distributions after 150k generator updates from the 7 out of 10 separate runs of the training procedure using Adam optimization for the discriminator that produced reasonable distributions.

was the Adam optimization procedure with parameters (1, 2) = (0.99, 0.999) and learning rate 2 = 0.004 and the generator learning rate was 1 = 0.05. The rest of the experimental setup remained the same. We ran this experiment 10 times and observed that for 7 out of the 10 runs of the final generated distribution was reasonably close to the real data distribution, while for 3 out of the 10 runs the generator did not learn the proper distribution. This is to say that we found the training algorithm was not as stable when the discriminator used Adam versus normal gradient ascent. The final generated distribution from the 7 of 10 runs with reasonable distributions are shown in Figure 7.
Adversarial Training. We now provide some further background on the adversarial training experiment and additional results. It is now well-documented that the effectiveness of deep learning classification models can be vulnerable to adversarial attacks that perturb the input data (see, e.g., [7, 31, 40, 57]). A common approach toward remedying this vulnerability is by training the classification model against adversarial perturbations. Recall from Section 6 that given a data distribution D over pairs of examples x  Rd and labels y  [k], parameters  of a neural network, a set S  Rd of allowable adversarial perturbations, and a

26

Figure 8: Adversarial Training: Test accuracy during the course of training against gradient ascent attacks with a fixed learning rate of 2 = 4 and the number of steps T  {5, 10, 20, 40}.
Figure 9: Adversarial Training: Test accuracy during the course of training against gradient ascent attacks with a fixed attack budget of T 2 = 40 where T is the number of attack steps and 2 is the learning rate (LR).
Figure 10: Adversarial Training: Test accuracy during the course of training against Adam optimization attacks with a fixed attack budget of T 2 = 0.04 where T is the number of attack steps and 2 is the learning rate (LR). loss function (·, ·, ·) dependent on the network parameters and the data, adversarial training amounts to considering a minmax optimization problem of the form min E(x,y)D[maxS (, x + , y)].
A typical approach to solving this problem is an alternating optimization approach [40]. In particular, each time a batch of data is drawn from the distribution, T -steps of projected gradient ascent are performed
27

(a)

(b)

Figure 11: Adversarial Training: f (, A()) as a function of the number of steps T taken by the gradient ascent algorithm A evaluated at multiple points in the training procedure. Figure 11a corresponds to using 2 = 4 in the gradient ascent procedure and Figure 11b corresponds to using 2 = 1 in the gradient ascent procedure.

Layer Type
Convolution + ReLU Max Pooling
Convolution + ReLU Max Pooling
Fully Connected + ReLU Fully Connected + ReLU
Softmax

Shape
5 × 5 × 20 2×2
5 × 5 × 20 2×2 800 500 10

Table 1: Convolutional neural network model for the adversarial training experiments.

by ascending along the sign of the gradient of the loss function with respect to the data and projecting back onto the set of allowable perturbations, then the parameters of the neural network are updated by descending along the gradient of the loss function with the perturbed examples. The experimental setup we consider is analogous but the inner maximization loop performs T -steps of regular gradient ascent (not using the sign of the gradient and without projections).
For the adversarial training experiment considered in Section 6, we also evaluated the trained models against various other attacks. Recall that the models were training using T = 10 steps of gradient ascent in the inner optimization loop with a learning rate of 2 = 4. To begin, we evaluated the trained models against gradient ascent attacks with a fixed learning rate of 2 = 4 and a number of steps T  {5, 10, 20, 40}. We also evaluated the trained models against gradient ascent attacks with a fixed budget of T 2 = 40 and various choices of T and 2. These results are presented in Figure 9. Finally, we evaluated the trained models against attacks using the Adam optimization method with a fixed budget of T 2 = 0.04 and various choices of T and 2. These results are presented in Figure 10. Notably, we see that our algorithm outperforms the baselines and similar conclusions can be drawn as from the experiments for adversarial training presented in Section 6. The additional experiments highlight that our method of adversarial training is robust against attacks that the algorithm did not use in training when of comparable computational power and also that it improves robustness against attacks of greater computational power than used during training.
In Section 6, we showed the results of evaluating the gradient norms f (, A()) as a function of the number of gradient ascent steps T in the adversary algorithm A and observed that it grows much slower than exponentially. Here we provide more details on the setup. We took a run of our algorithm trained with the setup described in Section 6 and retrieved the models that were saved after 25, 50, 75, and 100 training epochs. For each model, we then sampled 100 minibatches of data and for each minibatch performed T  {20, 30, 40, 50, 60, 70, 80, 90, 100} steps of gradient ascent with learning rate 2 = 4 (the learning rate from training) and then computed the norm of the gradient f (, A()) where A corresponds to the gradient ascent procedure with the given number of steps and learning rate. In Figure 2, which is reproduced here in

28

Figure 11a, the mean of the norm of the gradients over the sampled minibatches are shown with the shaded window indicating a standard deviation around the mean. We also repeated this procedure using 2 = 1 and show the results in Figure 11b from which similar conclusions can be drawn.
Finally, we provide details on the convolutional neural network model for the adversarial training experiments. In particular, this model is exactly the same as considered in [50] and we reproduce it in Table 1.
Experimental Details. For the experiments with neural network models we used two Nvidia GeForce GTX 1080 Ti GPU and the PyTorch higher library[14] to compute f (, A()). In total, running all the experiments in the paper takes about half of a day with this computational setup. The code for the experiments is available at https://github.com/fiezt/minmax-opt-smooth-adversary.
29

