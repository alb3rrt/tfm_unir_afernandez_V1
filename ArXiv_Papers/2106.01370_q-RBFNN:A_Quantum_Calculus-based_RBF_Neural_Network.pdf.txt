q-RBFNN: A Quantum Calculus-based RBF Neural Network
Syed Saiq Hussain, Muhammad Usman, Taha Hasan Masood Siddique, Imran Naseem, Roberto Togneri and Mohammed Bennamoun

arXiv:2106.01370v1 [cs.LG] 2 Jun 2021

Abstract--In this research a novel stochastic gradient descent based learning approach for the radial basis function neural networks (RBFNN) is proposed. The proposed method is based on the q-gradient which is also known as Jackson derivative. In contrast to the conventional gradient, which finds the tangent, the q-gradient finds the secant of the function and takes larger steps towards the optimal solution. The proposed q-RBFNN is analyzed for its convergence performance in the context of least square algorithm. In particular, a closed form expression of the Wiener solution is obtained, and stability bounds of the learning rate (step-size) is derived. The analytical results are validated through computer simulation. Additionally, we propose an adaptive technique for the time-varying q-parameter to improve convergence speed with no trade-offs in the steady state performance. The proposed time variant q-RBFNN has shown superior performance on the following tasks: (1) nonlinear system identification problem, (2) hammerstein model, (3) multiple input multiple output (MIMO) system, and (4) chaotic time series prediction. The MATLAB implementation of the proposed method is available at the author's GitHub page (https://github.com/musman88/q-RBFNN).
Index Terms--antioxidation, deep auto-encoder, composition of k-spaced amino acid pair (CKSAAP), latent space learning, neural network, classification.
I. INTRODUCTION
The advent of advanced machine learning algorithms has demonstrated tremendous performances for the challenging problems [1]. The intricate machine learning methods including deep learning, support vector machines and random forest are now greatly employed in a variety of scientific applications [2], [3]. This development, however, is achieved with large amounts of training data and at the cost of computation overhead that is required for the training of these algorithms [4]. Moreover, the architecture of these algorithms is equally complex and therefore, the cost of implementation to user is substantially increased. On the other hand, the classical neural
Syed Saiq Hussain is with the College of Engineering, Karachi Institute of Economics and Technology, Karachi, Pakistan, e-mail: saiqhussain@gmail.com, Muhammad Usman is with the Department of Computer Engineering, Chosun University, Gwangju 61452, South Korea, e-mail: usman@chosun.kr, Taha Hasan Masood Siddique is with the Department of Electronic Engineering, NEDUET, e-mail: 1tahahassan1@gmail.com, Imran Naseem is with the College of Engineering, Karachi Institute of Economics and Technology, Karachi, Pakistan and School of Electrical, Electronic and Computer Engineering, The University of Western Australia, e-mail: imrannaseem@pafkiet.edu.pk, imran.naseem@ee.uwa.edu.au, Roberto Togneri is with the School of Electrical, Electronic and Computer Engineering, The University of Western Australia, e-mail: roberto.togneri@uwa.edu.au, Mohammed Bennamoun is with the School of Computer Science and Software Engineering, The University of Western Australia, e-mail: mohammed.bennamoun@uwa.edu.au.
*This is a preprint version.

networks have simpler architecture and significantly lower implementation cost with agreeable performance. Specially, when the task is simple and available training data is scarce [5].
For instance, radial basis function neural network (RBFNN) with its virtue of simple single layered architecture and its universal approximation capabilities has been adopted in a broad range of applications [6]­[8]. In recent years, researchers have proposed several modifications in the architecture of the RBF with the intention to achieve better convergence rates. For instance, the architectures proposed in [9], the algorithm attains faster error convergence by utilizing fewer number of nodes. The new architecture of RBF proposed in [10], has been developed using fuzzy clustering techniques. In [11], an intelligent adaptive learning method for RBFNN is proposed in which the calculation of the derivative is circumvented so that the algorithm converges at a faster rate. Verification of the method is carried out using simulations for tracking control and non-linear system identification. Several researchers employed RBF for task oriented applications, for instance, in [12], critical water quality parameters have been predicted during the treatment of waste water. A modular RBFNN is designed in order to deal with the complex problems encountered by the human brain in daily life [13], this approach is inherited from neurophysiology and neuroscience. An improved version of RBF has proved its efficacy in bank credit risk management using optimal segmentation [14]. Several researches have been done for the localization of various types of vehicles, for instance, 5G assisted unmanned aerial vehicle is localized by the help of RBF neural network [15]. An adaptive kernel has been proposed in [16] for the RBF neural network. This algorithm makes use of both the Euclidean and cosine distances which are measured in an adaptive manner. This fusion consequently leads to faster error convergence. The algorithm has been evaluated on several signal processing case studies and resulted in superior performance gain compared to the conventional approaches. A fractional gradient descent based RBFNN has been proposed in [17], in which the weight update rule has been derived using fractional gradient methods. The method compared to the conventional approach achieves improved results on the signal processing problems including pattern classification, function approximation, time series prediction and non-linear system identification.
In order to achieve accelerated convergence, in the recent past a variety of techniques have been developed. One such technique is called the non-uniform update gain [18], the use

of which has been suggested for the rapid convergence rate in linear filters [19]­[22]. This is achieved by replacing the constant update gain matrix with the non-uniform update matrix containing the corresponding values for multiplication with the input coefficient. This implication has shown significant improvement in obtaining faster convergence. A similar idea has been extended for the non-linear filters [23], in which the non-uniform update gain was utilized for the second order expansion of Volterra series. The non-uniform gain for the input coefficients was obtained by exploiting the inherent nature of Jackson's derivative from the q-calculus. In [19] it is suggested that a non-uniform learning rate can accelerate the convergence, since the q-gradient descent has non-uniform q factor, which upon multiplication results in non-uniform learning rate, therefore, we can expect better convergence with q-gradient descent [24].
The concept of q-calculus is known as the calculus with no limits and this approach has been greatly utilized by a number of researchers and has gained overwhelming success in fields ranging from quantum theory, mathematics, and physics [25]. Jackson established the concepts of q-derivative [26], and qintegral [27], the notions since then have been utilized towards the development of several stochastic gradient algorithms [24], [28]. Contrary to the conventional gradient which computes the tangent, the q-derivative evaluates the secant of the function, consequently taking larger steps towards the optimal solution resulting in faster convergence.
Additionally, substantial work has been carried out to avert the constant learning rate of the gradient algorithms [29]­[35]. The dynamic updates in the learning rate has been shown to effectively contribute towards achieving faster convergence rates. A notable method is proposed in [36], which uses the notions of non-uniform update gain along with the implementation of variable step size by utilizing the concepts of signal normalization and error correlation energy.
Considering merits of variable step size and q-gradient subsequently leading to the non-uniform gains of the coefficient in the adaptive filter, in this research, we propose to extend these concepts for the RBF neural network. In particular, the following contributions have been made in this research work:
1) We propose a novel learning algorithm for the RBFNN using a modified stochastic gradient descent algorithm based on the notion of q-gradient.
2) A thorough mathematical analysis for the steady state performance of the proposed algorithm is presented which validates the simulation results of the experiments conducted for the system identification problem.
3) An optimal solution (i.e the Wiener solution) and convergence analysis is presented for the proposed method and its stability bounds are derived.
4) Analytical results are validated through computer simulations for the sensitivity, transient and steady state behavior of the proposed q-RBFNN.
5) Extensive comparative analysis of the proposed work is carried out with the contemporary approaches.

6) Additionally, an adaptive framework is designed, which aims to achieve higher convergence rate without compromising the steady-state error.
This research is intended to be a fusion of q-calculus with the RBF neural network to develop an advancement in the inherited gradient descent optimization. Sophisticated stochastic methods of learning developed for the optimization of free parameters in the RBF algorithm [37]­[39], are not the focus of this research work.
The rest of the paper is organized as follows: The proposed learning approach, along with the mathematical analysis of the proposed method, optimal solution and the convergence and sensitivity analysis of the proposed method has been discussed in Section II. The time varying q-RBF method and its robustness is discussed in Section III. The performance evaluation of the proposed method and the comparative analysis has been carried out in Section IV, and the paper is finally concluded in Section V.
II. THE PROPOSED q-GRADIENT-BASED RBF NEURAL NETWORK (q-RBFNN)
A typical RBFNN as shown in Fig.1, is a three layered architecture comprised of (1) an input layer, (2) a non-linear hidden layer and (3) an output layer.

Fig. 1. RBF neural network (RBFNN) Architecture.
To understand the mapping of of the RBF, consider an input vector x  RM , s : RM  R1, is given as

N

y = wii( x - ci ) + b,

(1)

i=1

where ci  RM depicts the center locations of the RBF network, the connection between the hidden and the output layer is weighted wi, and a bias b is added to the output layer. The number of neurons in the hidden layer are N and i is the basis of each neuron in the hidden layer. The number of output neurons are equivalent to the number of classes, here we consider a single output neuron. There are several

choices of kernels among which gaussian, multiquadratics and inverse multiquadratics are often used for RBF networks [40], however, gaussian kernel as shown in (2), due to its versatility is mostly employed [41].

i( x - ci ) = exp

- x - ci 2 2

,

(2)

Here  represents the Gaussian kernel's spread. The kernels serve the purpose of realizing the distance from the center of the network. The commonly used distance is the Euclidean distance, although other distance metrics such as cosine distance has been suggested to have complimentary properties in comparison with the Euclidean distance metric [42].

i1(x.ci) =

x

x.ci , ci + 

(3)

in exceptional cases where the denominator terms x or ci may become zero, the equation (3) will be indeterminate. To avoid such situation, a small constant term  > 0 is added.

A. Proposed q-RBF learning approach
The concepts of q-calculus has been successfully utilized in variety of fields including signal processing, mathematics and quantum theory [43]­[46].
In [47], a method of obtaining the function's derivative is described as follows:

dqi (g(k)) = g(qik) - g(k).

(4)

Differentiating the above equation leads to the result as follows:

Dqi (g(k))

=

dqi (g(k)) dqi (k)

=

g(qik) - f (k) . (qi - 1)k

(5)

In the above equation if q  1 is substituted, the qderivative function is similar to the classic derivative.
Here we introduce the proposed learning approach of RBFNN based on the q-gradient descent algorithm. To derive the weight update rule, consider the RBFNN architecture shown in Fig 1, the output of which at the nth iteration can be written as:

N

y(n) = wi(n)i(x, ci) + b(n),

(6)

i=1

The number of neurons in the hidden layer are M which formulates the final result. The values of synaptic weights wi(n) along with the bias b(n) are adapted at each iteration. The cost function E(n) is the instantaneous error e(n), found by taking difference of actual and the desired output as shown in (7)

E (n)

=

1 (d(n) -

y(n))2

=

1 e2(n),

(7)

2

2

The weight update equation is derived from the conventional

gradient descent method: q-gradient is incorporated in the

conventional gradient descent method to obtain the weight update rule as shown in (8)

wi(n + 1) = wi(n) - µqi,wi E(n),

(8)

The parameter qi controls the gradient and µ denotes the step size. Evaluating the factor -qi,wi E(n) for N = i = 1 by:

-

qi,wi E (n)

=

- dqi E(n) dqi e(n)

×

dqi e(n) dqi y(n)

×

dqi y(n) dqi wi(n)

(9)

simplifying the partial derivatives in (9) yields:

-

qi,wi E (n)

=

(qi

+ 2

1) i(x, ci)e(n),

(10)

using (10) equation (8) is reduced to be:

wi(n +

1)

=

wi(n)

+µ

(qi

+ 1) 2

i(x, ci)e(n).

(11)

Similarly, b(n) can be updated by:

b(n + 1) = b(n) + µ (q0 + 1) e(n).

(12)

2

Extending it in a similar fashion for N , the controlling

parameters qi in (11) can be contained in the diagonal matrix

G as shown in (13).

diag(G) = [ (q1 + 1) , (q2 + 1) , ..... (qN

+ 1) ]

.

(13)

2

2

2

Hence, the weight vector w in (11), can be written as:

w(n + 1) = w(n) + µ G (x, c)e(n).

(14)

B. Mathematical Analysis of q-RBFNN

The optimal solution determines the fact that a stochastic learning algorithm shall converge. To find the optimal solution we carried out theoretical analysis of the proposed method. Consider a scenario of system identification where the desired output d(n) is shown in (15):

N

d(n) = w^i(n)i(x, ci) + (n),

(15)

i=1

The system to be identified is denoted by w^, having an input

node vector x(n), and (n) is the additive white noise with

zero mean. The unknown system w^ is estimated by utilizing

the relation in (6).

1) Optimal Wiener Solution: We derive the cost function by

substituting the error relation in the simple error i.e. e(n) =

d(n) - y(n), and the output in eq. (6), in the conventional

cost function which is based on the mean square error, which

is

E (n)

=

1 2

E

[e(n)e

(n)]

=

1 2

E[|e(n)|2

].

This

yields

to

the

following relation:

N

N

E(n) = d2 - wk (n)p(-k) - wk(n)p (-k)

k=0

k=0

NN

+

wk (n)w^i(n)r(l - k), (16)

k=0 l=0

where d2 E[|d(n)|2] is the desired signal power, the cross-correlation between the desired output and the kernel output is p(-k) E[i(x, ci)d (n)], and the auto-correlation matrix of the kernel is r(l - k) E[(x(n - k), c)(x(n - l), ci) ].
The cost function q-gradient can be written as:

qk,wk E (n)

=

dqk E (n) dqk wk

=

- (qk

+

1) p(-k)

2

+

N

(qk

+ 2

1)

w^i

(n)r(l

-

k).

(17)

l=0

To achieve an optimal solution, qk,wk E(n) is set to zero:

N

p(-k) = w^i(n)r(l - k),

(18)

i=0

and

w^ = r-1p.

(19)

A least square problem's closed form of Wiener solution is mentioned in the above equation. It is of great interest that the optimal solution of the proposed algorithm gives the Wiener solution without any additional parameter(s). The minimum square error at w^ is also same as optimal Wiener power.

min = E[2(n)],

(20)

where  is the Gaussian noise. This outcome shows that the proposed method shall cease the weight update upon attaining the Weiner solution and guarantees the convergence of algorithm.
2) Convergence Analysis: The convergence analysis is performed for the mean error performance of the proposed algorithm with common assumptions [48], [49]: The noise has a Gaussian distribution with zero mean and unit variance, the input vector x is independent and identically distributed i.i.d and the such parameters of kernel function  are selected that signals remain linearly separable in the Kernel space after mapping.
The weight error vector is defined as w(n) = w(n) - w^,

e(n) = w(n)(x(n), c) + (n)

(21)

After substituting w(n) = w(n) + w^, and e(n) in (14) , we get

w(n + 1) = w(n) + G(x(n), c) w(n) ((x(n), c) + (n) (22)

After simplification it results in

E [w(n + 1)] = (I - µA) E [w(n)] .

(23)

and upon further simplification gives

E [w(n)] = (I - µA)n E [w(0)] .

(24)

where A = GE [(x(n), c)((x(n), c) ] and E[] is the expectation operator.
For convergence

1

0<µ<

(25)

max{(q1 + 1)1, . . . , (qN + 1)N }

In case when all qi's are equal to q

1

0<µ<

(26)

(q + 1)max

where max is the maximum eigenvalue of r and it implies

that

1

µmax = ||(x(n), c)||2G

(27)

C. Sensitivity Analysis of the proposed q-RBF NN

In this experiment, we analyze the sensitivity of the qRBF algorithm with respect to the parameter q. In particular, we choose a pattern classification problem and compare the Mean Square Error (MSE) learning curves of the proposed qRBF algorithm for different values of q-parameter. Simulation parameters used are as follows: for the proposed q-RBF algorithm, we investigated four different values of q which are q = 2, q = 5, q = 10, and q = 12. The task is to cluster 100 random signals obtained from two Gaussian distributions of means 0.25 and 0.75 and variance 0.1 into two different groups namely class 0 and class 1. Two Gaussian kernel neurons are used with spread of 0.1 and center values are obtained using k-means clustering. The output class signal is disturbed by adding a Gaussian noise of signal-to-noise ratio (SNR) of 20 dB. The model was trained on 200 epochs with the learning rate value chosen to be 0.10. The simulations are repeated 100 times and mean results are reported.
Fig. 2(a) clearly shows that for higher values of q, the proposed q-RBF algorithm exhibits faster convergence similar to that of classical least mean square (LMS) algorithm. The final steady-state error in all cases is close to the disturbance in system i.e., 20 dB. However, the faster convergence is achieved at a cost of steady-state error Fig. 2(b), shows the increase in residual error vs increment in value of q. This behaviour motivated us to device a mechanism for timevarying q-parameter which can provide highest convergence with lowest steady-state error. The details of the proposed time-varying q-RBF is given in section III.

D. Validation of Mathematical Analysis using simulation
The same experiment of section II-C was carried out again expect this time no disturbance signal is added and instead of MSE, mean absolute error (MAE) between optimal weights w^ and estimated weights w is calculated. Analysis results are obtained using eq. (24). The simulations are performed 100 times are mean results of MAE are reported for three different values of q, i.e., (1, 2, and 4). Fig. 3 shows the MAE curves for simulation and analysis. The analysis results are well matched with simulation results and the mean correlation coefficient of analysis and simulation MAE values is almost 1 i.e., 0.9996

Fig. 2. Sensitivity analysis of the proposed algorithm

A. Analysis of the Robustness of q-RBFNN using Small Gain Theorem

The small gain theorem [50] serves the purpose very well in order to discuss the robustness of the equation (21). The theorem can be explained by the relation ||S1||  ||S2|| < 1 where S1 and S2 are considered to be two stable systems which are connected together in a closed loop as shown in Fig. 4. S1 is said to be a feed-forward block and S2 is considered to be a feedback block. To ensure the convergence rates and reliable training scheme, the possibilities will be derived for the learning rate q. The small gain theorem provides sufficient conditions to the stability of finite-gain l2 for the efficient mapping of noisy input signal with the estimated error sequence.
A robust algorithm has consistent estimation error under the influence of perturbations regardless of their nature. This property is of great importance in situations where the prior statistical knowledge is not present. The robustness therefore, implies a positive constant as an upper bound on the estimation error to the perturbation energy as shown in (28) .

EstimationError

1

(28)

N oise

Next, while adapting the weights from the nth iteration to the (n + 1)th iteration, we have to perform a lossless mapping in between the estimation errors for all the time instances n. The transformation from x to y as y = H(x) in accordance with the relation i.e. ||H(x)||2  ||x||2 is considered to be a
lossless mapping assuring that the output energy will always
be lesser than the input energy. We define the disturbance error for the analysis i.e. ~(n) = e(n) - ea(n) To explain the lossless mapping between estimation errors ea(n) and eb(n), a feedback system is considered. Where, ea(n) is apriori error and eb(n) is considered as a posterior error which can be defined as in eq (29) and (30).

ea(n) = (n)w~ (n)

(29)

eb(n) = (n)w~ (n + 1)

(30)

Where, w~ is known as weight error vector distinguishing the optimal and its estimated weight as w~ = wo-w(n). Therefore, eq (29) and eq (30) can be rewritten as eq (31) and eq (32), acquired using eq (11), respectively.

Fig. 3. Mean convergence of the proposed algorithm on different values of q
III. DESIGN OF TIME-VARYING q-RBF From sensitivity analysis in section II-C, it is evident that selection of an appropriate q value contributes toward improvement in the convergence performance of the algorithm. The convergence rate can be further increased for q > 1, therefore, a time-varying q-RBFNN design is proposed.

ea(n) = (n)wo(n) - (n)w(n)

(31)

eb(n) = (n)[w~ (n) - µq(n)T (n)e(n)] = ea(n) - µq(n)||(n)||2e(n)

(32)

Hence, we define posterior error eb(n) in the form of µ in eq (33) and (34).

eb(n) = ea(n) - µq(n)||(n)||2e(n)

(33)

q(n)e(n) = µ{ea(n) - eb(n)}

(34)

The weight update rule mentioned in eq (14) can be expressed in error recursion form as:
w(n + 1) = w(n) - µq(n)T (n){ea(n) - eb(n)}. (35)
To calculate the energy taking l2 norm of (35):
||w(n + 1)||2 = ||w(n)||2 - 2[w(n)µq(n)T (n){ea - eb}]+ µ2[q(n)T (n){ea - eb}q(n)T (n){ea - eb}] (36)
Given that µ is a small value, therefore, we can ignore its higher power, resulting in:
||w(n + 1)||2 = ||w(n)||2 - 2[w(n)µq(n)T (n){ea - eb}] (37)

the guaranteed stability if the multiplication results of norm of feedforward and feedback mappings are bounded by 1. For the given mapping the norm of the feedforward block is equal to 1, and since the the feedback norm (M ) < 1 the overall stability requirement is guaranteed. There must be a specific range of the learning rate which can be defined as:

1

0 < q(n) < µ||(n)||2

(42)

B. Proposed design for adaptive q

We propose the following time varying rule for the q

parameter:

q(n + 1) = q(n) + e2(n)

(43)

||w(n+1)||2 +2µq(n)ea(n) = ||w(n)||2 +2µq(n)eb(n) (38)

||w(n + 1)||2 + 2µq(n)ea(n) ||w(n)||2 + 2µq(n)eb(n)

=

1

(39)

The above form (39) is valid for all the possible learning rates.

where, 0 <  < 1, and  > 0 with

q(n + 1) =

qmax f or q(n + 1) > qmax q(n + 1), otherwise

(44)

Here, qmax is calculated using (42), the parameter  is a positive value 0 <  < 1, and is dependent on its own past value, while the constant  > 0. The adaptation rule in the mappings (43) and (44) suggests that greater error correlation in the initial stage would prompt a large learning rate and shall reduce as the system approaches to the steady state. This desired behavior is similar to that proposed for the standard LMS algorithm and its variants [24], [28], [36].

IV. NONLINEAR SYSTEM IDENTIFICATION USING PROPOSED q-RBF NN

Fig. 4. Lossless Mapping of a closed loop system for q-RBFNN algorithm

By using the relations in (21), (29), (33), and (34) can be expressed as:

q(n) eb(n) = ea(n) - µ {ea(n) + (n)},

q(n)

q(n)

eb(n) = 1 - µ

ea(n) -

 (n), µ

(40)



q(n)

q(n) 

- µeb(n) = µ (n) - 1 - µ

µea(n).

Fmraopmpinthgefraobmovteheeaxcptrueaslsidoinst,uribt ainsceesvideµnt

that the (n) to the

complete resulting

estimation errors µea(n) can be represented by the closed

loop system shown in the Fig. 4. For our case, the small gain

theorem can be expressed as:

q(n)

(M ) = max 1 -

.

(41)

0nM

µ

From the above equation, (M ) is said to be an absolute maximum gain of the closed loop for time period of 0  n  M . For a system shown in 4, the small gain theorem states

Fig. 5. System identification block diagram
The linear models are employed due to their easy implementation and assurance of robustness. However, many practical and industrial applications demand highly complex and nonlinear system modeling. To this end, neural networks due to their universal approximation capabilities comes in handy. They present efficient modeling solution by utilizing the system's inputs and outputs. One of such system has been shown in Fig 5. To determine the efficacy of the proposed method, we evaluate its performance for three system identification problems. Furthermore, the effectiveness of the proposed algorithm is tested for the prediction of chaotic time series which inherit high degree of complexity [51].

A. Highly nonlinear control system identification
A non-linear system is defined by the following transfer function:

y(t) = a1r(t) + a2r(t - 1) + a3r(t - 2)
+ a4[cos(a5r(t)) + e-|r(t)|] + n(t) (45)
The output of the system is y(t), the input is r(t) with polynomials ais representing the zeros of the system. Furthermore, random noise n(t) is added to the input with the characteristics N (0, d2). For training of the model a rectangular signal with additive Gaussian noise of -10dB has been used. One period of the training signal has 500 samples with first 250 samples being set to 1 and the rest set to -1. Two periods of training signal, i.e. total signal length of 1000 was used to train the model.
Testing is carried on a rectangular pulse signal with -20 dB Gaussian noise and 2.5× higher frequency than the input and the performance of the proposed method is compared with conventional RBFNN and its fractional design f -RBF [17].
1) Architecture and Model Configurations: Three layered RBFNN structure is considered with three inputs in the input layer, 6 neurons in the hidden layer and an output layer. The spread () of the Gaussian kernel is kept 1 and experiment is performed for 100 independent runs, the mean of which has been reported.
The polynomials ais of the proposed method were chosen to be 2, -0.5, -0.1, -0.7 and 3 for i = 1, 2, 3, 4 and 5 respectively while ,  and qmax of time-varying q-RBFNN are chosen to be 0.9, 5, and 5 respectively. The tuning parameters of RBFNN, f -RBFNN, and proposed q-RBFNN are empirically opted in order to achieve the optimal results during training, and absolute value of gradient of the fractional update term is used to avoid complex values. There are several hyperparameters in RBFNN and f -RBFNN algorithm including the mixing parameter  of gradient in f -RBFNN, which is set to be 0.5, learning rates  and  were both set to 1 × 10-2, the fractional derivative power  = 0.9 is used.
Fig. 6 depicts the MSE curves of the trained algorithms. It can be observed that the proposed q-RBFNN achieves an average MSE of -14.5dB in just 220 iterations, thereby outperforming RBFNN and f -RBFNN which required 300 and 600 iterations respectively to achieve similar MSE value.
To better understand the origin of performance, we track the mean value of q at each epoch. Note that the mean value of q-parameter reduces with the increase in the fitness of the RBFNN. This is well matching with the sensitivity analysis shown in section II-C. The comparison of the estimated output for the proposed algorithm with RBFNN and f -RBFNN during test has been in shown Fig. 7.
The MSE for the test has also been measured as shown in Fig. 8. Similar test conditions were retained for other architectures. It is observed that the proposed method achieves an MSE value of 18.20 dB which better than those of RBF, FRBF, which achieved 16.85 dB, 17.10 dB respectively.

Fig. 6. MSE behavior during training for non-linear system identification problem
Fig. 7. Comparison of model and actual output for non-linear system identification problem
Fig. 8. MSE behavior in during testing for non-linear system identification problem

B. Hammerstein model non-linearity estimation
The proposed adaptive learning rate has been applied to estimate the static non-linearity present in the Hammerstein model. The simulation has been conducted using a Hammerstein model of a non-linear heat exchange coil along with linear dynamics as mentioned in (46). The proposed model is evaluated to estimate the model and the results are compared with the conventional RBFNN and f -RBFNN.
a(t) = -m1r(t) + m2r2(t) - m3r3(t) + m4r4(t), (46) c(t) = n1c(t - 1) + n2c(t - 2) + n3a(t) + h(t)
The input r(t) of the heat exchanger model is fed to the system to first get an intermediate result a(t), which is further utilized for the output evaluation c(t). The system is specified with some disturbance that is denoted by h(t). For experimental setup the parameters of m1, m2 . . . m4, n1, n2, n3 and h have to be selected for the output evaluation. The simulation using the proposed and conventional algorithms was carried out by keeping same values of all the parameters. We used the values of 31.549, 41.732, 24.201, 68.634 for m1, m2 . . . m4, 0.4, 0.35 and 0.15 for n1, n2, n3 and 0.1 for the noise signal. The input signal is comprised of values ranging from -2to2 with a step size of 0.2 constituted to generate 201 samples of training. Fig. 9 depicts the plot of the desired and estimated results using the proposed and conventional algorithms where it can be observed that the proposed algorithm maps the desired output better than conventional ones. The tuning parameters of RBFNN, f -RBFNN, and proposed q-RBFNN are empirically opted in order to achieve the optimal results during training, and absolute value of gradient of the fractional update term is used to avoid complex values. There are several hyperparameters in RBFNN and f -RBFNN algorithm including the mixing parameter  of gradient in f -RBFNN, which is set to be 0.5, learning rates  and  were both set to 1 × 10-2, the fractional derivative power  = 0.9 is used. Efficacy of the proposed method is demonstrated using MSE curves in Fig. 10 which clearly shows the superior performance of the proposed method. In particular the MSE values achieved by RBFNN, f -RBFNN and proposed q-RBFNN are 8.85dB, 4.56dB and 2.15dB respectively.
C. Estimation of Non-linear MIMO System
We consider a non-linear MIMO system with 2 inputs and 2 outputs to evaluate the efficacy of the proposed approach. The MIMO system in eq. (47) has been utilized for estimation using the proposed method and the results are compared with the performances of conventional RBFNN and f -RBFNN.
c1(t) = m1r1(t) + m2c1(t - 1) - m3r1(t - 2) + m4cos(m5r2(t)) + e-||r1(t)||,
c2(t) = n1r2(t) - n2r2(t - 1) + n3r2(t - 2)
+ n4sin(n5r1(t)) (47)
The inputs and outputs of the system are represented by r1(t), r2(t) and c1(t), c2(t) respectively.

Fig. 9. Comparison of model and actual output for Hammerstein model nonlinearity estimation
Fig. 10. MSE behavior during testing of Hammerstein model non-linearity estimation
The simulated results have been compared with the desired and estimated outputs of q-RBFNN, f -RBFNN and conventional RBFNN as shown in Fig. 11.
The values of parameters in (47) are selected to be 0.21, -0.12, 0.3, -0.6 and 0.5 for m1, m2 . . . m5 respectively, and 0.25, -0.1, -0.2, 1.2 and 0.2 for n1, n2, . . . n5 respectively while ,  and qmax of time-varying q-RBFNN are chosen to be 1,10, and 10 respectively. The input the system is same as employed in IV-A, except in case of MIMO where the inputs are r1(t) and r2(t), duplicated input streams were utilized for training of algorithm.
There are several hyper-parameters in RBFNN and f RBFNN algorithm including the mixing parameter  of gradient in f -RBFNN, which is set to be 0.5, learning rates  and v are both set to 1 × 10-3, the fractional derivative power  = 0.9 is used. Fig. 11 depicts the desired and estimated results for the proposed and conventional algorithms where the proposed method is found to outperform the competitive

approaches. The proposed method has achieved a lowest MSE of -16.9dB in 108 iterations, compared to -16.3dB in 380 iterations of RBFNN and -16.4dB in 370 iterations, which signifies the superiority of the proposed method. The same has been depicted in Fig. 10.

D. Chaotic Time Series Prediction
Herein we perform the experiment of prediction of a chaotic time series which is a commonly used quantitative model in signal processing. To determine the effectiveness of the proposed method, we consider a Mackey-Glass series which can be modeled as delayed differential equation (48).

dr(t) nr(t -  )

dt = 1 + r(t -  )10 - mr(t)

(48)

Here r(t) is a time series of interval t=1,2,3,....,3000 derived from eq. (48) by performing sampling of the curve r(t) at the intervals of one second. n and m are the coefficients having values of 0.2 and 0.1 respectively,  = 20 and r(t -  ) = 0 for (  t  0).
Furthermore, the training dataset achieved an SNR of 30dB upon introducing the white gaussian noise. In order to simulate for the proposed q-RBFNN, the samples from 100  t  2500 has been considered to train the model. While, the samples lie in the range 2500  t  3000 have been utilized for testing purpose. For comparison with the proposed method, the conventional RBFNN and f -RBFNN are also trained using the similar parameters, the results of which are shown in Fig. 15.
There are several hyper-parameters in RBFNN and f RBFNN algorithm including the mixing parameter  of gradient in f -RBFNN, which is set to be 0.5, learning rates  and v are both set to 1×10-3, the fractional derivative power  = 0.5 is used. K-means clustering has been employed to select the centers of the RBF and the algorithm is trained for 100 epochs. The mean results reported in Fig. 13 depicts the MSE curves of training where as the mean results of MSE during achieved during test are shown in Fig. 14. From the results, it can be observed that the proposed q-RBFNN outperforms the conventional RBFNN, and f -RBFNN by achieving an average MSE of -21.4dB, while the conventional RBFNN and f RBFNN achieve the average MSE of -20.8dB and -21.1dB respectively.

V. CONCLUSION
A quantum calculus based derivative method is employed to propose a gradient descent based learning algorithm for RBFNN named as q-RBFNN. The proposed method exploits complementary properties of quantum q parameter which works on secant of the function taking larger steps to reach minima resulting in faster convergence. The proposed q-RBF is examined for the optimal solution in a system identification problem and an optimal Wiener solution is thus obtained. The transient behaviour and stability bounds of the learning rate (step-size) of the q-RBF is also derived and validated through

computer simulations on a pattern classification problem. Using the analysis, results an adaptive framework for q-parameter is designed. The proposed time-varying q-RBFNN method is
proven to be more promising and has shown to outperform
the conventional RBFNN on the problem of nonlinear system
identification. The proposed method can be further improved
by incorporating the evolutionary learning and sophisticated
stochastic methods to train optimal models.
ACKNOWLEDGMENT
Syed Saiq Hussain acknowledges the support of HEC,
Pakistan under Indigenous Ph.D. Fellowship Program (PIN
417-29746-2EG4-120).
REFERENCES
[1] J. Watt, R. Borhani, and A. Katsaggelos, Machine learning refined: foundations, algorithms, and applications. Cambridge University Press, 2020.
[2] M. Usman and J. A. Lee, "Afp-cksaap: Prediction of antifreeze proteins using composition of k-spaced amino acid pairs with deep neural network," in 2019 IEEE 19th International Conference on Bioinformatics and Bioengineering (BIBE), 2019, pp. 38­43.
[3] M. Usman, S. Khan, and J.-A. Lee, "Afp-lse: Antifreeze proteins prediction using latent space encoding of composition of k-spaced amino acid pairs," Scientific Reports, vol. 10, no. 1, pp. 1­13, 2020.
[4] B. Jan, H. Farman, M. Khan, M. Imran, I. U. Islam, A. Ahmad, S. Ali, and G. Jeon, "Deep learning in big data analytics: A comparative study," Computers & Electrical Engineering, vol. 75, pp. 275­287, 2019.
[5] Y. Min and H. W. Chung, "Shallow neural network can perfectly classify an object following separable probability distribution," in 2019 IEEE International Symposium on Information Theory (ISIT). IEEE, 2019, pp. 1812­1816.
[6] A. S. Rahmati and A. Tatar, "Application of radial basis function (rbf) neural networks to estimate oil field drilling fluid density at elevated pressures and temperatures," Oil & Gas Science and Technology­Revue d'IFP Energies nouvelles, vol. 74, p. 50, 2019.
[7] F. Gagliardi, K. T. Tsiakas, and K. Giannakoglou, "A two­step mesh adaptation tool based on rbf with application to turbomachinery optimization loops," in Evolutionary and Deterministic Methods for Design Optimization and Control With Applications to Industrial and Societal Problems. Springer, 2019, pp. 127­141.
[8] Q.-X. Zhu, X.-H. Zhang, Y. Wang, Y. Xu, and Y.-L. He, "A novel intelligent model integrating plsr with rbf-kernel based extreme learning machine: Application to modelling petrochemical process," IFACPapersOnLine, vol. 52, no. 1, pp. 148­153, 2019.
[9] M. H. Barhaghtalab, H. Bayani, A. Nabaei, H. Zarrabi, and A. Amiri, "On the design of the robust neuro-adaptive controller for cable-driven parallel robots," Automatika, vol. 57, no. 3, pp. 724­735, 2016.
[10] S.-K. Oh, W.-D. Kim, and W. Pedrycz, "Design of radial basis function neural network classifier realized with the aid of data preprocessing techniques: design and analysis," International Journal of General Systems, pp. 1­21, 2015.
[11] S. S. A. Ali, M. Moinuddin, K. Raza, and S. H. Adil, "An adaptive learning rate for rbfnn using time-domain feedback analysis," The scientific world journal, vol. 2014, 2014.
[12] X. Meng, Y. Zhang, and J. Qiao, "An adaptive task-oriented rbf network for key water quality parameters prediction in wastewater treatment process," Neural Computing and Applications, pp. 1­14, 2021.
[13] J.-F. Qiao, X. Meng, W.-J. Li, and B. M. Wilamowski, "A novel modular rbf neural network based on a brain-like partition method," Neural Computing and Applications, vol. 32, no. 3, pp. 899­911, 2020.
[14] X. Li and Y. Sun, "Application of rbf neural network optimal segmentation algorithm in credit rating," Neural Computing and Applications, pp. 1­9, 2020.
[15] V. Annepu, A. Rajesh, and K. Bagadi, "Radial basis function-based node localization for unmanned aerial vehicle-assisted 5g wireless sensor networks," Neural Computing and Applications, pp. 1­14, 2021.
[16] S. Khan, I. Naseem, R. Togneri, and M. Bennamoun, "A novel adaptive kernel for the rbf neural networks," Circuits, Systems, and Signal Processing, pp. 1­15, 2016.

Fig. 11. Comparison of model and actual output for non-linear MIMO system

Fig. 12. MSE behavior during testing of non-linear MIMO system

Fig. 13. MSE behavior during training of chaotic time series prediction model

[17] S. Khan, I. Naseem, M. A. Malik, R. Togneri, and M. Bennamoun, "A fractional gradient descent-based rbf neural network," Circuits, Systems, and Signal Processing, pp. 1­22, 2018.
[18] U. Ummatov and K. Lee, "Adaptive threshold-aided k-best sphere decoding for large mimo systems," Applied Sciences, vol. 9, no. 21, p. 4624, 2019.
[19] M. Givens, "Enhanced-convergence normalized lms algorithm [dsp tips & tricks]," IEEE signal processing magazine, vol. 26, no. 3, pp. 81­95, 2009.
[20] T. I. Haweel and P. M. Clarkson, "A class of order statistic lms algorithms," IEEE Transactions on Signal Processing, vol. 40, no. 1, pp. 44­53, 1992.
[21] J. B. Evans, P. Xue, and B. Liu, "Analysis and implementation of variable step size adaptive algorithms," IEEE Transactions on Signal Processing, vol. 41, no. 8, pp. 2517­2535, 1993.
[22] R. Harris, D. Chabries, and F. Bishop, "A variable step (vs) adaptive filter algorithm," IEEE transactions on acoustics, speech, and signal

processing, vol. 34, no. 2, pp. 309­316, 1986. [23] M. Usman, M. S. Ibrahim, J. Ahmed, S. S. Hussain, and M. Moinuddin,
"Quantum calculus-based volterra lms for nonlinear channel estimation," in 2019 Second International Conference on Latest trends in Electrical Engineering and Computing Technologies (INTELLECT). IEEE, 2019, pp. 1­4. [24] U. M. Al-Saggaf, M. Moinuddin, M. Arif, and A. Zerguine, "The q-Least Mean Squares algorithm," Signal Processing, vol. 111, no. Supplement C, pp. 50 ­ 60, 2015. [25] T. Ernst, The history of q-calculus and a new method. Citeseer, 2000. [26] F. H. Jackson, "Xi.--on q-functions and a certain difference operator," Earth and Environmental Science Transactions of The Royal Society of Edinburgh, vol. 46, no. 2, pp. 253­281, 1909. [27] D. O. Jackson, T. Fukuda, O. Dunn, and E. Majors, "On q-definite integrals," in Quart. J. Pure Appl. Math. Citeseer, 1910. [28] U. M. Al-Saggaf, M. Moinuddin, and A. Zerguine, "An efficient least mean squares algorithm based on q-gradient," in 2014 48th Asilomar

Fig. 14. MSE behavior during testing of chaotic time series prediction model
Fig. 15. Comparison of model and actual output for chaotic time series prediction
Conference on Signals, Systems and Computers, Nov 2014, pp. 891­ 894. [29] R. H. Kwong and E. W. Johnston, "A variable step size LMS algorithm," IEEE Transactions on Signal Processing, vol. 40, no. 7, pp. 1633­1642, Jul 1992. [30] S. Messalti, A. Harrag, and A. Loukriz, "A new variable step size neural networks mppt controller: Review, simulation and hardware implementation," Renewable and Sustainable Energy Reviews, vol. 68, pp. 221­233, 2017. [31] S. Khan, M. Usman, I. Naseem, R. Togneri, and M. Bennamoun, "Vpflms: a novel variable power fractional lms algorithm," in 2017 Ninth International Conference on Ubiquitous and Future Networks (ICUFN). IEEE, 2017, pp. 290­295. [32] T. Aboulnasr and K. Mayyas, "A robust variable step-size LMS-type algorithm: analysis and simulations," IEEE Transactions on Signal Processing, vol. 45, no. 3, pp. 631­639, 1997. [33] S. Khan, M. Usman, I. Naseem, R. Togneri, and M. Bennamoun, "A robust variable step size fractional least mean square (rvss-flms) algorithm," in 2017 IEEE 13th International Colloquium on Signal Processing & its Applications (CSPA). IEEE, 2017, pp. 1­6. [34] S. S. Hussain, M. K. Majeed, M. D. Abbasi, M. H. S. Siddiqui, Z. A. Baloch, and M. A. Khan, "Improved varient for foc-based adaptive filter for chaotic time series prediction," in 2019 4th International Conference on Emerging Trends in Engineering, Sciences and Technology (ICEEST). IEEE, 2019, pp. 1­6.

[35] A. Sadiq, M. Usman, S. Khan, I. Naseem, M. Moinuddin, and U. M. AlSaggaf, "q-lmf: Quantum calculus-based least mean fourth algorithm," in Fourth International Congress on Information and Communication Technology. Springer, 2020, pp. 303­311.
[36] A. Sadiq, S. Khan, I. Naseem, R. Togneri, and M. Bennamoun, "Enhanced q-least mean square," Circuits, Systems, and Signal Processing, vol. 38, no. 10, pp. 4817­4839, 2019.
[37] A. Alexandridis, E. Chondrodima, and H. Sarimveis, "Cooperative learning for radial basis function networks using particle swarm optimization," Applied Soft Computing, vol. 49, pp. 485­497, 2016.
[38] D. P. F. Cruz, R. D. Maia, L. A. da Silva, and L. N. de Castro, "Beerbf: A bee-inspired data clustering approach to design rbf neural network classifiers," Neurocomputing, vol. 172, pp. 427­437, 2016.
[39] B. Jafrasteh and N. Fathianpour, "A hybrid simultaneous perturbation artificial bee colony and back-propagation algorithm for training a local linear radial basis neural network on ore grade estimation," Neurocomputing, vol. 235, pp. 217­227, 2017.
[40] S. O. Haykin, Neural Networks: A Comprehensive Foundation. Prentice Hall PTR, Upper Saddle River, NJ, USA, 1994.
[41] D. Wettschereck and T. Dietterich, "Improving the performance of radial basis function networks by learning center locations," in Advances in Neural Information Processing Systems, vol. 4. Morgan Kaufmann, San Mateo, Calif, USA, 1992, pp. 1133­1140.
[42] W. Aftab, M. Moinuddin, and M. S. Shaikh, "A Novel Kernel for RBF Based Neural Networks," Abstract and Applied Analysis, vol. 2014, 2014.
[43] G. Bangerezako, "Variational q-calculus," Journal of Mathematical Analysis and Applications, vol. 289, no. 2, pp. 650 ­ 665, 2004.
[44] J. Tariboon, S. K. Ntouyas, and P. Agarwal, "New concepts of fractional quantum calculus and applications to impulsive fractional q-difference equations," Advances in Difference Equations, vol. 2015, no. 1, p. 18, Jan 2015.
[45] J. Tariboon and S. K. Ntouyas, "Quantum calculus on finite intervals and applications to impulsive difference equations," Advances in Difference Equations, vol. 2013, no. 1, p. 282, Nov 2013.
[46] A. R. A. L. Ali, V. Gupta, R. P. Agarwal, A. Aral, and V. Gupta, Applications of q-Calculus in Operator Theory. Springer New York, 2013.
[47] V. Kac and P. Cheung, Quantum Calculus. Springer New York, 2012. [48] T. Abbas, S. Khan, M. Sajid, A. Wahab, and J. C. Ye, "Topological
sensitivity based far-field detection of elastic inclusions," Results in physics, vol. 8, pp. 442­460, 2018. [49] D. Jukic and R. Scitovski, "Existence of optimal solution for exponential model by least squares," Journal of Computational and Applied Mathematics, vol. 78, no. 2, pp. 317 ­ 328, 1997. [Online]. Available: http://www.sciencedirect.com/science/article/pii/S0377042796001604 [50] A. R. Teel, "A nonlinear small gain theorem for the analysis of control systems with saturation," IEEE transactions on Automatic Control, vol. 41, no. 9, pp. 1256­1270, 1996. [51] A. Sadiq, M. S. Ibrahim, M. Usman, M. Zubair, and S. Khan, "Chaotic time series prediction using spatio-temporal rbf neural networks," in 2018 3rd International Conference on Emerging Trends in Engineering, Sciences and Technology (ICEEST). IEEE, 2018, pp. 1­5.

