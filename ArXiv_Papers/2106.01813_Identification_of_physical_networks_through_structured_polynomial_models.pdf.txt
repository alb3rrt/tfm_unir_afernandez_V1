Identification of physical networks through structured polynomial models
E.M.M. (Lizan) Kivits and Paul M.J. Van den Hof

arXiv:2106.01813v1 [eess.SY] 3 Jun 2021

Abstract--Physical dynamic networks most commonly consist of interconnections of physical components that can be described by diffusive couplings. These diffusive couplings imply that the cause-effect relationships in the interconnections are symmetric and therefore physical dynamic networks can be represented by undirected graphs. This paper shows how (prediction error) identification methods developed for polynomial linear timeinvariant systems can be configured to consistently identify the parameters and the interconnection structure of (undirected) physical networks. Further, a multi-step least squares (convex) optimization algorithm is developed to solve the nonconvex optimization problem that results from the identification method.
I. INTRODUCTION
Physical networks can describe many physical processes from different domains, such as mechanical, magnetic, electrical, hydraulic, acoustic, thermal, and chemical processes. Physical networks are typically considered as undirected dynamic interconnections between node signals, where he interconnections represent diffusive couplings [1], [2], [3]. The model is typically described by a vector differential equation of maximum second order. The most well-known example is a mechanical mass-spring-damper system, with positions of masses as node (state) signals and the dynamics being described by a second order vector differential equation. Identification of these physical models can be done by conversion of the model into a state space form, after which matrix transformations [4], [5] or eigenvalue decompositions [6], [7] are being applied to estimate the model parameters. However, during these operations the network structure in the model is generally lost. Physical models can also be converted into transfer function models or polynomial models, of which the parameters are being identified through prediction error methods [8]. Though, in general the network structure in the model is lost again. Physical networks can also be considered to be directed dynamic networks with specific structural properties [9]. Dynamic networks are directed interconnections of transfer function modules [10], [11] for which an identification framework has been developed in [11]. However, the structural properties of physical networks cannot easily be taken into account in the identification algorithms for dynamic networks.

The overall objective of this research is to develop a compre-

hensive theory for the identification of individual interconnec-

tions in physical (undirected) networks, where the order of the

dynamics is not restricted and possibly correlated disturbances

can be present. The objective includes questions like which

nodes to measure (sense) and which nodes to excite (actuate)

in order to identify a particular (local) dynamics in the network

or to identify the full dynamics and topology of the network.

In addition, consistency and minimum variance properties of

estimates have to be specified.

This paper addresses the problem of identifying the full

dynamics and topology of physical (undirected) networks. The

physical networks that will be considered in this paper are

defined in Section II. Next, the set-up for identification of the

full network dynamics is described in Section III. In order to

be able to consistently identify the network dynamics, data

informativity and network identifiability conditions need to be

satisfied. These conditions are formulated in Section IV as well

as the results for consistent identification of physical networks.

Section V contains a multi-step algorithm for consistently

identifying the network dynamics and Section VI consists of

a simulation example that supports these results. Section VII

contains some extensions after which Section VIII concludes

the paper.

Consider the following notation throughout the paper. A

polynomial matrix A(q-1) consists of matrices A and

(j, k)-th polynomial elements ajk(q-1) such that A(q-1) =

n =0

A

q-

and ajk(q-1) =

n =0

ajk,

q-

.

Hence,

the

(j, k)-th element of the matrix A is denoted by ajk, . Physical

components are indicated in sans serif font: A or a.

II. PHYSICAL NETWORK
Physical networks can describe many physical processes from different domains, such as mechanical (translational, rotational), magnetic, electrical, hydraulic, acoustic, thermal, and chemical processes. Physical networks, such as mass-springdamper systems and resistor-inductor-capacitor (RLC) circuits, are often described by second order differential equations. They can be considered to consist of L interconnected node signals wj(t), j = 1, . . . , L, of which the behavior is described according to

Paper for submission to IEEE Transactions on Automatic Control. This project has received funding from the European Research Council (ERC), Advanced Research Grant SYSDYNET, under the European Union's Horizon 2020 research and innovation programme (grant agreement No 694504). Lizan Kivits and Paul Van den Hof are with the Department of Electrical Engineering, Eindhoven University of Technology, Eindhoven, The Netherlands {e.m.m.kivits, p.m.j.vandenhof}@tue.nl.

Mj0w¨j(t) + Dj0w j(t) +

Djk[w j(t) - w k(t)]

kNj

+ Kj0wj(t) +

Kjk[wj(t) - wk(t)] = uj(t), (1)

kNj

with real-valued coefficients Mj0  0, Djk  0, Kjk  0, Djj = 0, Kjj = 0, Nj is the set of indices of node signals

1

K13
M10 K10

M30 D13 D23
K12

u2
M20 D20

u6

X66 w6

Y67

u7 w7 X77

Y16

Y26

Y27

Y37

u1

u2

u3

u4

u5

X11 w1 Y12 w2 Y23 w3 Y34 w4 Y45 w5

X22

u8 X33

X44

X55

Y18

w8

Y48

Fig. 1. A network of masses (Mj0), dampers (Djk) and springs (Kjk).
wk(t) k = j with connections to node signals wj(t), uj(t) are the external signals and w j(t) and w¨j(t) are the first and second order derivative of the node signals wj(t), respectively. In the physical networks that will be considered in this paper, all connections are symmetric, meaning that the strength of the connection from node wi to node wk is equal to the strength of the connection (in opposite direction) from node wk to node wi. This means that the interconnections of the nodes are diffusive couplings, which emerge in (1) from the symmetric connections: Djk = Dkj and Kjk = Kkj j, k. Moreover, all systems that can be described by diffusive (symmetric) couplings are included in this paper. An example of a physical networks with diffusive couplings is the mass-spring-damper system shown in Figure 1, in which masses Mj0 are interconnected through dampers Djk and springs Kjk with k = 0 and are connected to the ground with dampers Dj0 and springs Kj0. The positions xj(t) of the masses Mj0 are the signals of interest and therefore chosen to be the node signals: wj(t) := xj(t). The couplings between the masses are diffusive, because springs and dampers are symmetric components. Further, a system as the one shown in Figure 1 would require at least a two-dimensional position vector wj(t), but for notational convenience, without loss of generality, we will restrict our attention to scalar-valued node signals wj(t). Physical processes from different fields can be combined in a single physical networks using power conversion components. The corresponding linear transformations between node signals can be included in (1), resulting in a physical networks that contains node signals and external signals of multiple physical quantities.

Fig. 2. A physical network as defined in Definition 1, with nodes wj , inputs uj , and dynamics between the nodes (yjk) and to the ground node (xjj ).

connections of nodes to a ground node. The behavior of the node signals wj(t), j = 1, . . . , L, is described by

n
xjj, wj( )(t) +

n-1
yjk, [wj( )(t) - wk( )(t)] = uj (t),

=0

kNj =0

(2)

with real-valued coefficients xjj,  0, yjk,  0, yjk, = ykj, , where wj( )(t) is the -th derivative of wj(t) and where uj(t)
is the external signal entering the j-th node.

The graphical interpretation of the coefficients is as follows:

xjj,n represent the buffers, that is the components intrinsically related to the nodes wj; xjj, with = n represent the components connecting the node wj to the ground node; and yjk,

represent the components in the diffusive couplings between

the nodes wj and wk. The ground node is characterized by wground(t) = 0 and therefore can be seen as a node with an

infinite buffer, see also [2].

A graphical representation of a physical network is shown

in Figure 2. The network dynamics are represented by the

blue boxes containing the polynomials xjj(q-1) =

n =0

xjj,

and yjk =

n-1 =0

yjk,

and the node signals are represented

by the blue circles, which sum the diffusive couplings and

the external signals. For example w5(t) = x55 w5(t) - 0 + y45 w5(t) - w4(t) + u5(t).
Furthermore, every matrix X composed of elements xjj, := xjj, is diagonal and every matrix Y composed of elements yjj, := kNj yjk, and yjk, := -yjk, for k = j is Laplacian1 representing an undirected graph of a specific

physical component (i.e. the diffusive couplings of a specific

order).

A. Higher order network
A physical network as described above is typically of second order, where all node signals are collected in w(t). Network models that explain only a selection of the node signals can be constructed by removing nodes from the network through a Gaussian elimination procedure that is referred to as Kron reduction [2], [3] or immersion [12], which will generally lead to higher order dynamics between he remaining node signals. In order to accommodate this, we will include higher order terms in our model. Definition 1 (Physical network): A physical network is a network consisting of L node signals w1(t), . . . , wL(t) interconnected through diffusive couplings and with possible

B. Discretization
Typically, identification of linear time-invariant systems takes place in discrete time and the identification tools for dynamic networks that are available in literature are developed for discrete-time models as well. Therefore, it is desirable to convert the continuous-time network into an equivalent discretetime network. There exist several discretization methods that are applicable to interconnected systems [14], which means that they possess the property that first discretizing the systems and
1A Laplacian matrix is a symmetric matrix with nonpositive off-diagonal elements and with nonnegative diagonal elements that are equal to the negative sum of all other elements in the same row (or column) [13].

2

then interconnecting them in series, parallel and/or feedback results in the same discrete-time network as first interconnecting the systems in series, parallel and/or feedback and then discretizing the network. These discretization methods include the step invariant method, (slightly adapted) impulse invariant method, matched Z-transform method, and direct mappings (forward difference method, backward difference method, and Tustin's method). All these discretization methods possess the property that asymptotic stability and instability are preserved for sufficiently high sampling frequencies [14]. In this paper, the backward difference method is chosen as it is relatively simple, results in a causal network representation and describes a (unique) bijective mapping between the continuous-time model and the discrete-time model. This discretization method assumes a linear (first order) relation between consecutive samples. Proposition 1 (Discrete-time physical network): By using the approximation

dw(t)

= w(td) - w(td-1) ,

(3)

dt t=td

Ts

with discrete time sequence td = dTs, d = 0, 1, . . . and time interval Ts, the continuous-time physical network (2) can be described in discrete time by

III. IDENTIFICATION SET-UP

As mentioned before, the objective of this paper is to identify the full dynamics and the topology of physical networks. In this section, the identification setting is described, which includes the physical network model, the network predictor, the model set, and the identification criterion. The node signals in the network might be affected by a userapplied excitation signal and subject to a disturbance signal. This needs to be included in the network description, which is achieved by splitting the external signal as

u(t) := B(q-1)r(t) + F (q)e(t),

(10)

where, the known excitation signals r(t) enter the network through dynamics described by polynomial matrix B(q-1) and where the unknown disturbance signals acting on the network are modeled as a filtered stationary white noise process F (q)e(t) with F (q) a rational matrix. For system identification purposes, we will use a slightly different, but equivalent, physical network description than the one in (7), where the partitioning (10) is included. Definition 2 (Physical network model): The physical network that will be considered during identification consists of L node signals w(t) and K excitation signals r(t) and is defined as

A(q-1)w(t) = B(q-1)r(t) + F (q)e(t),

(11)

n
¯xjj, q- wj (td) +

n-1
¯yjk, q- [wj (td) - wk(td)]

=0

kNj =0

= uj(td), (4)

with q-1 the shift operator meaning q-1wj(td) = wj(td-1) and with

¯xjj, = (-1)

n i=

i Ts-ixjj,i,

(5)

¯yjk, = (-1)

n-1 i=

i Ts-iyjk,i,

(6)

where i is a binomial coefficient. Proof: Equation (2) is discretized by a similar approach as described in [15] by using a backward shift (3). In the sequel, (t - i) is used for td-i = td - iTs. The expressions for the node signals (4) can be combined in a matrix equation describing the network as

X¯ (q-1)w(t) + Y¯ (q-1)w(t) = u(t),

(7)

with X¯ (q-1) and Y¯ (q-1) polynomial matrices in the shift operator q-1 and composed of elements

X¯jk(q-1) =

n =0

¯xjj,

q-

,

0,

if k = j otherwise

(8)



 

mNj

n-1 =0

¯yj

m,

q-

,

if k = j

Y¯jk(q-1) =

-

n-1 =0

¯yjk,

q-

,

if k  Nj (9)

0,

otherwise.

Observe that X¯ (q-1) is diagonal and Y¯ (q-1) is Laplacian, implying that the structural properties of (2) are maintained in (7)-(9).

with
· A(q-1)  A := {A  RL×L[q-1] | akj (q-1) = akj(q-1), k, j}.
· B(q-1)  B := {B  RL×K [q-1] | B proper}. · F (q)  F := {F  RL×L(q) | F monic, stable and
stably invertible}. ·  0 the covariance matrix of the noise e(t).
Further, the network is assumed to be well-posed and stable, implying that A-1(q-1) exists and is proper and stable. It is
also assumed that the network is connective, which means that there is a path between every pair of nodes2. Observe that A(q-1) = X¯ (q-1) + Y¯ (q-1). Often, B(q-1) is
chosen to be binary, diagonal and known, which represents the
assumption that each external excitation signal directly enters
the network at a distinct node (without dynamics in between). As a result, physical networks lead to polynomial models3 with the particular properties that A(q-1) is symmetric and nonmonic. Moreover, if F (q) is polynomial or even stronger if F (q) = I, the physical network (11) leads to an ARMAX-like or ARX-like4 model structure with these particular properties,
respectively.
Now the physical network representation and its properties
have been defined, the next step is to formulate the identifica-
tion setting.
2The network is connective if its Laplacian matrix (i.e. the degree matrix minus the adjacency matrix) has a positive second smallest eigenvalue [2].
3Polynomial models are linear time-invariant dynamic models of the form A(q-1)y(t) = E-1(q-1)B(q-1)u(t) + D-1(q-1)C(q-1)e(t), where A(q-1), B(q-1), C(q-1), D(q-1), and E(q-1) are polynomials in q-1 that are all monic except for B(q-1) [8], [16].
4The structure is formally only an ARMAX (autoregressive-moving average with exogenous variables) or ARX (autoregressive with exogenous variables) structure if the A(q-1) polynomial is monic [16].

3

A. Network predictor

The objective is to identify the dynamics of the complete physical network. This estimation is performed using a prediction error method, which is the most common system identification method and it is applicable to networks [11]. In order to identify the complete network dynamics, all node signals w(t) are predicted based on the measured signals that are available in the network. This leads to the following predictor. Definition 3 (Network predictor): In line with [17], the network predictor is defined as the conditional expectation

w^(t|t - 1) = E{w(t) | wt-1, rt}.

(12)

where wt-1 represents the past of w(t), that is w(t - 1), w(t - 2), . . . and rt represents r(t), r(t - 1), . . . The current node signals, w(t), are not used for prediction as the network contains direct feedthrough terms. Proposition 2 (Network predictor): For a network model (11), the one-step-ahead network predictor (12) is given by (omitting arguments q, t)

w^(t|t - 1) := I - A-0 1F -1A w + A-0 1F -1Br, (13)

where A0 := limz0 A(z). Proof: The physical network (11) can be described by

Aw = Br + F e = Br + F A0A-0 1e. Premultiplying with A-0 1F -1 gives
A-0 1F -1Aw = A-0 1F -1Br + A-0 1e.

Adding w to both sides of the equality and rewriting gives

w = I - A-0 1F -1A w + A-0 1F -1Br + A-0 1e. (14)
where the factor A-0 1 makes the filter I - A-0 1F -1A strictly proper and where A-0 1F -1B is proper. The one-step-ahead network predictor (13) follows directly by applying its definition (12) to (14). Proposition 3 (Innovation): The innovation corresponding to the network predictor (13) is

e¯(t) := w(t) - w^(t|t - 1) = A-0 1e(t),

(15)

which has covariance matrix ¯ = A-0 1A-0 1. Proof: This follows directly from subsequently substituting

w^(t|t - 1) (13) and w(t) (11) into (15).

The innovation is a scaled version of the driving noise process.

As A0 is not necessarily diagonal, the scaling possibly causes correlations among the noise channels, but the innovation

signal e¯(t) remains a white noise process.

B. Model set and prediction error
The physical network models that will be considered during identification are gathered in the network model set. Definition 4 (Physical network model set): The model set is defined as a set of parametrized functions as

M := {M (),   },

(16)

with all particular models M () := A(q-1, ), B(q-1, ), F (q, ), () (17)

satisfying the properties in Definition 2. The experimental data that are available for identification are generated by the true system. Definition 5 (Data generating system): The data generating system S is denoted by the model

M 0 := (A0, B0, F 0, 0).

(18)

The true system is in the model set, i.e. (S  M), if 0   such that M (0) = M 0, where 0 indicate the true parameters.
Using the parametrized physical network model set, the parametrized one-step-ahead network predictor is defined.
Definition 6 (Parametrized predictor): The parametrized network predictor is defined in accordance with (13) as

w^(t|t - 1; ) = [I - Ww(q, )] w(t) + Wr(q, )r(t), (19)

with filters

Ww(q, ) = A-0 1()F -1(q, )A(q-1, ),

(20)

Wr(q, ) = A-0 1()F -1(q, )B(q-1, ).

(21)

The parametrized predictor leads to the prediction error. Proposition 4 (Prediction error): The prediction error corresponding to the parametrized predictor (19) is defined as

¯(t, ) := w(t) - w^(t|t - 1; ),

(22)

which is obtained as (omitting argument q)

¯(t, ) = A-0 1()F -1() [A()w(t) - B()r(t)] , (23)

= Ww(q, )w(t) - Wr(q, )r(t).

(24)

which equals the innovation e¯(t) (15) for  = 0. Proof: The expression for the parametrized prediction error (23) directly follows from its definition (22) and the network predictor (19). Expressing the parametrized prediction error (23) in terms of r(t) and e(t) yields

¯(t, ) = W¯r(q, )r(t) + W¯e(q, )e(t) + (A00)-1e(t), (25)

with (omitting argument q)

W¯r(q, ) = A-0 1()F -1() A()(A0)-1B0 - B() , W¯e(q, ) = A-0 1()F -1()A()(A0)-1F 0 - (A00)-1.

with W¯e(q, ) strictly proper. The latter two terms in (25) are uncorrelated since e(t) is white noise. If the true system is in the model set, the prediction error for the true system is equal to the innovation (15):

¯(t, 0) = (A00)-1e(t) = e¯(t).

(26)

4

C. Identification criterion

In order to estimate the parameters, a weighted least squares identification criterion is applied:

^N = arg min VN (),

(27)



1N

VN () := N ¯ (t, )S¯(t, ),

(28)

t=1

¯ (^N

)

:=

1 N

N

¯(t, ^N )¯ (t, ^N ),

(29)

t=1

with weight S 0 that has to be chosen by the user. For analysis of the asymptotic properties of the parameter estimate, we consider the asymptotic criterion

 := arg min V¯ (),

(30)



N

V¯ () := lim E ¯ (t, )S¯(t, ) .

(31)

N 

t=1

It can been shown that under some mild conditions, see [8], the solution of the weighted least squares criterion (27) converges with probability 1 to the solution of the asymptotic criterion (30).

A. Data informativity
The data are called informative if it contains sufficient information such that Twr(q, ) and v¯() can uniquely be obtained from the spectral density of the measured data: w(), wr(), and r(). We define this for quasi-stationary data sequences in line with [17]. Definition 7 (Data informativity): A quasi-stationary data sequence {w(t), r(t)} is called informative with respect to the model set M (16) if for any two 1, 2  

E¯ ¯(t, 1) - ¯(t, 2) S ¯(t, 1) - ¯(t, 2) = 0



Ww(ei, 1) = Ww(ei, 2) Wr(ei, 1) = Wr(ei, 2)

(37)

for almost all . Applying this definition to physical networks, leads to the following conditions for data informativity. Proposition 5 (Data informativity): The quasi-stationary data sequence z(t) := {w(t), r(t)} is informative with respect to the model set M if, in the situation K  1, r() 0 for a sufficiently high number of frequencies. Proof: The premise of implication (37) is satisfied if and only if ¯ := ¯(t, 1) - ¯(t, 2) = 0, i.e. (omitting argument q)

¯ = z()z(t) = 0,

IV. CONSISTENT IDENTIFICATION
In order to be able to consistently identify the network, the experimental data need to satisfy certain conditions. These conditions are referred to as data informativity conditions. In addition, the network itself needs to satisfy certain conditions, such that it can be uniquely recovered. These conditions are referred to as network identifiability conditions. This section describes these conditions, after which the results for consistent network identification can be formulated. The physical network (11) can be represented as
w(t) = Twr(q)r(t) + v¯(t), v¯(t) = Twe¯(q)e¯(t), (32)

with

z() = Ww(1) - Ww(2) Wr(1) - Wr(2) ,

z(t) =

w(t) r(t)

.

Applying Parseval's theorem gives

1 2


z(ei, )z()z (e-i, )d = 0.
-

This implies z() = 0 only if z() 0 for a sufficiently high number of frequencies. As w(t) depends on r(t), substituting the open-loop response (11) for w(t) gives

z(t) = J(q)(t).

where e¯(t) is the innovation (15) and

Twr(q) = A-1(q-1)B(q-1),

(33)

Twe¯(q) = A-1(q-1)F (q)A0.

(34)

For estimating a network model, prediction error identification methods typically use the second order statistical properties of the measured data, which are represented by the spectral densities of w(t) and r(t). As r(t) is measured, but e¯(t) is not, the second order properties of w(t) are generated by transfer function Twr(q) and spectral density

v¯() : = F {E[v¯(t)v¯ (t -  )]},

(35)

= Twe¯(ei)¯ Twe¯(ei),

(36)

with F the discrete-time Fourier transform and (·) the com-
plex conjugate transpose. Observe that the spectral factorization in (36) is unique, as Twe¯(q)  F and ¯ 0 [18].

with

J(q) =

A-1F 0

A-1B I

,

(t) =

e(t) r(t)

.

As J(q) has always full rank, z() 0 if and only if () 0. As e(t) and r(t) are assumed to be uncorrelated and E{e(t)} = 0, we have that re = er = 0 and

 =

r er

re e

=

r 0

0 

.

Then () 0 if and only if  0 (which is assumed) and r() 0. The condition z() 0 reduces to r() 0.

The condition that r() 0 for a sufficiently high number of frequencies seems to be a general condition. However, observe that the dimensions of r() depend on the number of excitation signals r(t), denoted by K, which is specified in the model set. Thus all excitation signals r(t) that are

5

present (according to the model set), need to be persistently exciting. This is because each additional excitation signal rj(t) also introduces new polynomials bkj(q-1) that need to be identified. If no excitation signal r(t) is present (K = 0), then r() is absent and z() 0 is also satisfied. Hence, z(t) is informative with respect to the model set M. Informativity of z(t) implies that (37) is satisfied and therefore
Twr(q, 1) = Twr(q, 2), Twe¯(q, 1) = Twe¯(q, 2), (38)
for any two 1, 2  , because of the one-to-one relationship between Twr(q, ), Twe¯(q, ) and Ww(q, ), Wr(q, ) .
B. Network identifiability
Identifiability of polynomial models has been defined in [8], where also the relation between identifiability and equivalent models is discussed. Equivalent networks are different network models that describe the same data [19]. Definition 8 (Equivalent networks): Two network models M (1) and M (2) are said to be equivalent if both
Twr(q, 1) = Twr(q, 2), v¯(, 1) = v¯(, 2). (39)

As a result, we consider the following definition for network identifiability [19]. Definition 9 (Network identifiability): The network model set M (16) is globally network identifiable from measured data {w(t), r(t)} if the parametrized model M () can uniquely be recovered from Twr(q, ) and v¯(, ), that is if for all models M (1), M (2)  M

Twr(q, 1) = Twr(q, 2) v¯(, 1) = v¯(, 2)

 M (1) = M (2).

(40)

That is, a network model set is globally network identifiable if it uniquely explains the second order objects that can be obtained from data. Before formulating the conditions for global network identifiability in physical networks, a result on left matrix fraction descriptions is presented. Lemma 1 (Left matrix fraction description (LMFD)): Consider a physical network as defined in Definition 2. The LMFD A(q-1)-1B(q-1) is unique up to a scalar factor if the following conditions are satisfied:
1) The polynomials A(q-1) and B(q-1) are left coprime. 2) At least one matrix Ak or Bk is diagonal.
Proof: According to [20], the LMFD of any two polynomial and left coprime matrices is unique up to a premultiplication with a unimodular matrix. To preserve diagonality of Ak or Bk, the unimodular matrix is restricted to be diagonal. To preserve symmetry of A(q-1), this diagonal matrix is further restricted to have equal elements. In general polynomial models, like ARMAX [21], A(q-1) is monic and therefore A0 = I is diagonal. Then the LMFD A(q-1)-1B(q-1) is unique, as the conditions of Lemma 1 are satisfied and scaling with a scalar factor is not possible anymore, since the diagonal elements of A0 are equal to 1.

Hence, both condition 2 in Lemma 1 and the scaling factor freedom are a result of the fact that A(q-1) is not necessarily monic. Now the conditions for global network identifiability in physical networks can be formulated. Proposition 6 (Physical network identifiability): A physical network model set M (16) is globally network identifiable from measured data {w(t), r(t)} if the following conditions are satisfied:
1) The polynomials A(q-1) and B(q-1) are left coprime. 2) At least one matrix Ak or Bk is diagonal. 3) At least one excitation signal rj(t) is present: K  1. 4) There is at least one constraint on the parameters of
A(q-1, a) and B(q-1, a) of the form ~~ =  = 0, with ~ := a b . Proof: Condition 3 implies that Twr(q, ) is nonzero. According to Lemma 1, condition 1 and 2 imply that A(q-1, ) and B(q-1, ) are found up to a scalar factor . Twe¯(q, ) and ¯ () are uniquely recovered from v¯(, ) as Twe¯(q)  F and ¯ 0 [18]. Together with the fact that A(q-1, ) is found up to a scalar factor , Twe¯(q, ) gives a unique F (q, ), and ¯() gives () up to a scalar factor 2. Finally, condition 4 implies that the parameters cannot be scaled anymore and therefore  is fixed. The coprimeness of A(q-1) and B(q-1) ensures that A(q-1) and B(q-1) have no common factors. This condition is also necessary for global identifiability of typical polynomial model structures, see Theorem 4.1 of [8]. The parameter  is a scaling factor that is introduced by the nonmonicity of A(q-1). In physical model structures the scaling factor needs to be fixed by additional constraints induced by condition 2 and condition 4 in Proposition 6. The parameter constraint in condition 4 of Proposition 6 can for example be · One nonzero element in B(q-1, ) is known, i.e. one excitation signal enters a node through known dynamics. · One nonzero parameter is known. · The fraction of two nonzero parameters is known. · The sum of some nonzero parameters is known.
Remark 1: In general dynamic networks conditions for global network identifiability typically include algebraic conditions verifying the rank of particular transfer functions from external signals to internal node signals [19]. For the generic version of network identifiability this entails a related graph-based check on vertex disjoint paths in the network model [22], [23]. In contrast to these conditions, the current conditions in Proposition 6 are very simple and require only a single excitation signal r(t) to be present in the network. This is induced by the structural properties of the diffusive couplings between the nodes, reflected in the fact that the polynomial matrix A(q-1) is restricted to be diagonal.
C. Consistency
Now, we can formulate the consistency result as follows. Theorem 1 (Consistency): Consider a data generating system S as defined in Definition 5. Then, under mild conditions (see Section III-C), M (^N ) is a consistent estimate of M 0 if the following conditions hold:

6

1) The true system is in the model set (S  M). 2) The data are informative with respect to the model set. 3) The model set is globally network identifiable.
Proof: The proof consists of three steps. First, convergence of VN () to V¯ () for N   follows directly from applying Theorem 2B.1 of [8] and the fact that S 0 as the conditions for convergence are satisfied by the network model set. Second, by condition 1, 0 is a minimum of V¯ (), which can be seen as follows. As r(t) and e(t) are uncorrelated and W¯e(q, ) is strictly proper, the power of any cross term between the three terms in the prediction error (25) is zero, so the power of each term can be minimized individually. As a result, W¯r(q, 0) = 0 and W¯e(q, 0) = 0 and thus the cost function reaches its minimum value when the prediction error is equal to the innovation as in (26). Third, following the result of Theorem 8.3 in [8], under condition 2, this minimum of V¯ () at 0 provides unique predictor filters Ww(q, ) and Wr(q, ) and therefore also unique transfer functions Twr(q, ) and Twe(q, ). With condition 3 this implies that the resulting model M () = M (0) is unique. Therefore, M (^N ) converges to M (0) with probability 1.
Observe that any weight S 0 leads to consistent estimates, but that minimum variance is only achieved for S = ¯-1.
Now it has been proved that physical networks can be identified consistently, the next step is to formulate algorithms for obtaining these estimates.
V. A MULTI-STEP ALGORITHM
The parametrized prediction error (23) is not affine in the parameters . Only in the very special situation where F (q, ) = I and A0() = I, the structure of (23) is affine. This situation causes the optimization problem (27) to be nonconvex. Especially for networks with many nodes, this results high computational complexity and occurrences of local optima. One approach to reduce the problem is to solve multiple multiinput single-output (MISO) problems instead of one large multi-input multi-output (MIMO) problem [11], [12], [24]. However, since the dynamics are coupled (that is, A(q-1) is symmetric and therefore its elements are not independently parametrized), a decomposition into MISO problems cannot be made without loss of accuracy.
In this section, as an alternative, a multi-step algorithm is developed, where in each step a quadratic problem is solved using a linear regression scheme. With that, the developed method contains steps that are similar to sequential least squares (SLS) [25], weighted null-space fitting (WNSF) [26], and the multi-step least squares method in [27], but particularly tuned to the physical network model structure of the current paper. As only quadratic problems are solved, the optimizations are convex and have unique solutions. In this way, the formulated algorithm achieves a consistent parameter estimation with minimum variance and limited computation complexity. This makes the algorithm also applicable to networks with many nodes.

A. Physical network with an ARMAX-like model structure
Consider a data generating system S = (A0, B0, F 0, 0) with F 0(q) := C0(q-1) being a monic polynomial, representing the physical network

A0(q-1)w(t) = B0(q-1)r(t) + C0(q-1)e(t),

(41)

which would have an ARMAX structure if A0(q-1) would be monic. Multiplying both sides of (41) with [C0(q-1)A00]-1 leads to

A0(q-1)w(t) = B0(q-1)r(t) + e¯(t),

(42)

where A0(q-1) is monic, e¯(t) is the innovation (15), and

A0(q-1) = [C¯0(q-1)]-1A0(q-1),

(43)

B0(q-1) = [C¯0(q-1)]-1B0(q-1),

(44)

C¯0(q-1) = C0(q-1)A00.

(45)

Now consider the model structure A(q-1, a), B(q-1, b), and C¯(q-1, c), as models of A0(q-1), B0(q-1), and C¯0(q-1), respectively, with C¯(q-1, c) = C(q-1, c)A0(a), and with
 := a b c . The exact parametrization is given in Appendix A.

Step 1: Estimating the nonparametric ARX model

As a first step, we are going to estimate a nonparametric
ARX model for (42), by parametrizing the infinite series expansions A0(q-1) and B0(q-1) by high order polynomial (finite) expansions A(q-1, n) and B(q-1, n), according to

¯A(t, n) = A(q-1, n)w(t) - B(q-1, n)r(t), (46)

= [n(t)] n,

(47)

with n the finite order of the polynomials, which is typically chosen to be high. The parameter vector n is given in Appendix A and the matrix [n(t)] is given in Appendix B.
The nonparametric ARX model (42) is then estimated through estimating its parameters n. This step is equivalent to the first
step of SLS [25] and WNSF [26], [27]. As this step serves to
make an initial estimate of the network, the network structure
is not taken into account. Further, consistency of this step is only achieved if the order n tends to infinity as function of the data length N at an appropriate rate, according to [28]. However, the bias will be negligibly small if the order n is chosen sufficiently large. The least-squares estimate of n is
found by

^Nn =

1 N

N

n(t)[n(t)]

t=n+1

-1
1

N
n(t)w(t) .

N

t=n+1

(48)

Under conditions of consistent estimation, and so if n and N approach infinity, ¯A(t, ^Nn ) will be an accurate estimate of the
innovation e¯(t). The covariance of the innovation is estimated

as the covariance of the residual as

¯ (^Nn )

=

1 N

N

¯A(t, ^Nn )¯A(t, ^Nn ),

(49)

t=n+1

7

with residual (46) evaluated at ^Nn . The covariance of the estimation error (t, ^Nn ) := ^Nn - n0 with n0 the actual coefficients of the expansions in (42), is estimated by

P (^Nn ) =

1 N

N

n(t)¯ -1(^Nn )[n(t)]

-1
.

(50)

t=n+1

Remark 2: As each row in (47) is independently parametrized, the parameters n can be estimated for each row independently, resulting in L MISO problems instead of one MIMO
problem. This is attractive for networks with many nodes.

Step 2: Reducing to the physical network model

The high order ARX model is used to identify the physical network model through the relations (43) and (44). This step is similar to the second step of WNSF [26], [27], where the difference lies in the parametrization structure. In this step, the structural properties of A0(q-1) are incorporated and the parameter constraint is taken into account to fix the scaling parameter and obtain a unique solution. The relations (51) and (52) are equivalently written as

A0(q-1) - C¯0(q-1)A0(q-1) = 0,

(51)

B0(q-1) - C¯0(q-1)B0(q-1) = 0.

(52)

Then from (51) and (52) we can extract:

- Q(n0)0 = 0,

(53)

where A0(q-1) and B0(q-1) are incorporated in Q(n0), where 0 represents the actual underlying system described by A0(q-1), B0(q-1), and C¯0(q-1) (41), where the polynomial
terms from (51) and (52) are considered up to time lag n, and where the row dimension of Q(n0) is equal to dim(n0). The
matrix Q() is given in Appendix C.
Together with the linear parameter constraint on a b described by 0 =  = 0, with  := ~ 0 full row rank, the initial least-squares5 estimate of 0 is obtained by the
linear constraint optimization problem

^(N0)

=

min


 Q (^Nn )Q(^Nn )

(54)

subject to  = ,

(55)

which can be solved using the Lagrangian and the Karush­Kuhn­Tucker conditions [30], giving

^(N0) ^(N0)

= Q (^Nn )Q(^Nn ) 

 0

-1

0 

,

(56)

where ^(N0) are the estimated Lagrange multipliers. The covariance of the residuals is updated according to (for k = 0)

¯ (^(Nk))

=

1 N

N

¯(t, ^(Nk))¯ (t, ^(Nk)),

(57)

t=n+1

with residual

¯(t, ^(Nk)) = C¯-1(q-1, ^(Nk)) A(q-1, ^(Nk))w(t) - B(q-1, ^(Nk))r(t) . (58)
5Weighted least-squares can be used as well (see Step 3) with weighting matrix W (^Nn ) = P -1(^Nn ) [29].

Step 3: Improving the physical network model
This step aims to correct for the residuals in (53) that are not accounted for in (56), due to the fact that only a high order approximation of the nonparametric ARX model is used. This step is similar to the third step of WNSF [26], [27], where again the difference lies in the parametrization structure. Substituting A(q-1, n) and B(q-1, n) for A0(q-1) and B0(q-1), respectively, into (51) and (52) gives
A0(q-1) - C¯0(q-1)A0(q-1, n) = C¯0(q-1)[A(q-1, n) - A0(q-1)], (59)
B0(q-1) - C¯0(q-1)B0(q-1, n) = C¯0(q-1)[B(q-1, n) - B0(q-1)], (60)

which are equivalently written as (by using (53))

- Q(n)0 = T (0)(n - no)

(61)

where the matrix T () is given in Appendix C. The estimate of  with minimum variance is obtained recursively by the weighted linear constraint optimization problem

^(Nk)

=

min


 Q (^Nn )W (^(Nk-1))Q(^Nn )

(62)

subject to  = ,

(63)

where the weighting matrix W (^(Nk-1)) is iteratively updated for k = 1, 2, · · · according to

W (^(Nk-1)) = T - (^(Nk-1))P -1(^(Nk-1))T -1(^(Nk-1)), (64)

where P (^(Nk-1)) is updated according to

P -1(^(Nk-1))

=

1 N

N

n(t)¯ -1(^(Nk-1))[n(t)] . (65)

t=n+1

Similar to Step 2, this optimization problem can be solved through

^(Nk) ^(Nk)

= Q (^Nn )W (^(Nk-1))Q(^Nn ) 

 0

-1

0 

,

(66)

where ^(Nk) are the estimated Lagrange multipliers. Finally, the covariance of the residuals is updated according to (57). Remark 3: Although this step is asymptotically efficient, iterating may improve the estimate for finite data length N . The cost

1

N

VN () = N det ¯ (t, )¯(t, ).

(67)

t=1

is evaluated at each iteration to decide whether the parameter estimation has improved. However, as (67) is not affine in the parameters, an improved cost may still result in deteriorated parameter estimates. The cost (67) is used as it is independent of () and under Gaussian assumptions, (67) results in minimum variance of the estimates if () is independently parametrized from A(q-1, ), B(q-1, ), and F (q, ), because then the asymptotic (minimum) variance resulting from (31) is equal to the asymptotic variance of the maximum likelihood estimator [8].

8

Step 4: Obtaining the noise model

With A(q-1, ^N ) and B(q-1, ^N ), the dynamics of the phys-
ical network have been estimated. In this step, C(q-1, ) and () are estimated from A(q-1, ), C¯(q-1, ), and ¯(). As C¯0(q-1) = C0(q-1)A00, C(q-1, ) is estimated by

C(q-1, ^N(k)) = C¯(q-1, ^N(k))A-0 1(^Nk ).

(68)

Further, as 0 = A00¯ 0A00, () is estimated by

(^N(k)) = A0(^N(k))¯ (^(Nk))A0(^N(k)).

(69)

v1

v2

w3 v3

y23

r1 b11 w1 y21 w2

y34

y24

w4 v4

x44

Step 5: Estimating the continuous-time parameters

With A(q-1, ^N ) and B(q-1, ^N ) from Step 3, the dynamics of the discrete-time physical network have been estimated.

The dynamics of the continuous-time physical network, repre-

sented

by

A¯(

d dt

,

^Nc

)

and

B¯ (

d dt

,

^Nc

),

are

obtained

through

the

inverse mapping of (5)-(6), given by

A¯jk, = (-Ts) B¯jj, = (-Ts)

nA i=

i Ajk,i,

nB i=

i Bjj,i.

(70) (71)

The complete algorithm

The above steps describe the procedure for identifying the
parameters of a physical network with an ARMAX-like model
structure. This procedure leads to the following algorithm.
Algorithm 1 (ARMAX-like model structure): Consider a data generating system S with F 0(q) := C0(q-1) a monic
polynomial and a physical network model set M (16) with F (q, ) := C(q-1, ) a monic polynomial. Then M (^N ), a consistent estimate of M 0, is obtained through the following
steps:

1) Estimate the nonparametric ARX model (42) by least squares (48) to obtain ^Nn .
2) Reduce the nonparametric ARX model to a parametric model (11) by weighted least-squares (56) to obtain ^(N0).
3) Improve the parametric model (11) by weighted leastsquares (56) to obtain ^(Nk) for k = 1, 2, . . ..
4) Obtain the noise model by calculating (68) and (69) to obtain C(q-1, ^N(k)) and (^N(k)).
5) Estimate the continuous-time parametric model from the

discrete-time parametric model through (70) and (71) to

obtain

A¯(

d dt

,

^Nc

)

and

B¯ (

d dt

,

^Nc ).

Consistency and minimum variance of the estimates obtained

with Algorithm 1 follows from the similarity with WNSF and its proof [26]. The main difference is that A(q-1, )

is nonmonic and symmetrically parametrized, resulting in a different structure in (53). For consistency, Q(^Nn ) needs to
have full column rank, which can be shown to be satisfied

if the identifiability conditions in Proposition 6 are satisfied.

Consistency of Step 4. follows naturally.

Remark 4: In order to perform Algorithm 1, the measured data

{w(t), r(t)} is needed; the order n of the ARX model needs
to be chosen; and the true orders na, nb, and nc of A(q-1), B(q-1), and C(q-1), respectively, need to be known.

Remark 5 (Simplification to an ARX-like model structure): If the noise is not filtered, that is F (q) := C(q-1) = I, the

Fig. 3. The continuous-time physical network model with interconnection

dynamics

described

by

the

polynomials

yjk

(

d dt

),

dynamics

to

the

ground

described

by

the

polynomial

x44

(

d dt

),

and

static

excitation

filter

b11 .

physical network has an ARX-like model structure and the ARX model (42) can exactly describe the physical network, where A(q-1) and B(q-1) are of the same order as A(q-1) and B(q-1), respectively. Algorithm 1 improves in the sense that Step 1 is consistent for sufficiently large data length N and therefore, no additional estimation error is made in Step 2, which makes Step 3 superfluous. Remark 6 (Simplification to an ARX model structure): If A0 = I in addition to unfiltered noise (F (q) := C(q-1) = I), the physical network has an ARX model structure. In this case, the physical network can consistently be identified in a single step, by incorporating the symmetric structure in Step 1 of Algorithm 1 and by choosing the order of A(q-1) and B(q-1) equivalent to the order of A(q-1) and B(q-1), respectively. The resulting identification procedure has been described in [9].
VI. SIMULATION EXAMPLE
This section contains a simulation example that serves to illustrate the theory and to show that indeed the topology and the parameters of a physical network can be identified using a single excitation signal only. The identification is performed with the algorithm presented above.

A. Experimental setup

Consider the continuous-time physical network (2) consisting of four one-dimensional nodes, with external signal u(t) = B0r(t) + v(t), described by

A¯0w(t)

+

A¯1

d dt

w(t)

+

A¯2

d2 dt2

w(t)

=

B0r(t)

+

v(t),

(72)

where A¯i = Xi + Yi, r(t) is one-dimensional and known, and B0 has dimension 4×1 and has only the first element nonzero. Figure 3 shows the structure of this physical network, where

it can be seen that the excitation signal r(t) = r1(t) enters the network only at node w1. One can think of this network as a mechanical mass-spring-damper network as explained in

Section II, where X0 and Y0 contain the spring constants, X1 and Y1 contain the damper coefficients, X2 contains the masses, the node signals w(t) represent the positions of the

masses, and the excitation signal r(t) is a force. One can also

9

TABLE I THE TRUE VALUES OF THE CONTINUOUS-TIME PARAMETERS AND THE MEAN AND VARIANCE OF THEIR ESTIMATES.

Parameter True value Mean Variance
Parameter True value Mean Variance
Parameter True value Mean Variance Parameter
True value Mean Variance

1c 0.3
0.2934 3.250 × 10-4
9c 0 1.057 × 10-6 4.902 × 10-14
1c7 -0.04
-0.03861 8.432 × 10-5
2c5
-0.9
-0.8625 1.650 × 10-3

2c 0.03
0.02992 8.289 × 10-9
1c0 0 -7.648 × 10-3 3.494 × 10-4
1c8 0 -6.283 × 10-5 3.915 × 10-8 2c6
-0.06
-0.05421 3.000 × 10-5

3c 0.001 1.000 × 10-3 1.781 × 10-13
1c1 0 -3.883 × 10-4 1.867 × 10-6
1c9 -0.8
-0.7835 2.407 × 10-3
2c7
0 1.1020 × 10-4 3.141 × 10-8

4c -0.4
-0.3945 1.379 × 10-3
1c2 0 6.975 × 10-7 4.448 × 10-14
2c0 0 9.801 × 10-4 2.004 × 10-5 2c8
2.7
2.514 2.878 × 10-3

5c -0.03
-0.02930 2.910 × 10-6
1c3 1.1
1.0660 7.097 × 10-3
2c1 0 1.417 × 10-5 8.452 × 10-9 2c9
0.06
0.05880 5.920 × 10-6

6c 0 2.876 × 10-7 2.232 × 10-13
1c4 0.07
0.06934 4.499 × 10-5
2c2 0.8
0.7181 4.077 × 10-3
3c0
0.007 6.742 × 10-3 6.440 × 10-9

7c 0
-0.01184 1.418 × 10-3
1c5 0.002 1.949 × 10-3 1.246 × 10-8
2c3 0.1
0.09777 0.1347 × 10-3

8c 0 6.806 × 10-4 8.799 × 10-6
1c6 0
-0.03112 7.332 × 10-3
2c4 0.005 4.952 × 10-3 9.968 × 10-8

TABLE II THE TRUE VALUES OF THE DISCRETE-TIME PARAMETERS AND THE MEAN AND VARIANCE OF THEIR ESTIMATES.

Parameter True value Mean Variance
Parameter True value Mean Variance
Parameter True value Mean Variance
Parameter True value Mean Variance
Parameter True value Mean Variance
Parameter True value Mean Variance

1 13.3 13.29 5.199 × 10-4
9 0 0.01057 4.902 × 10-6
17 4 5.118 23.74
25 -6.9 -5.182 5.491
33 0.04 -0.03425 5.919 × 10-4
41 0.1 -3.158 × 10-3 1.362 × 10-4

2 -23 -23.00 4.3100 × 10-6
10 0 -0.03950 0.02408
18 0 -0.6283 3.915
26 6 3.218 16.61
34 0.05 0.05324 1.001 × 10-3
42 0.09 0.04633 2.070 × 10-4

3 10 10.00 1.7810 × 10-5
11 0 0.02488 0.01900
19 -0.8 -0.5437 1.983
27 0 1.102 3.141
35 0.09 0.01265 1.806 × 10-4
43 0.05 0.02551 9.917 × 10-5

4 -3.4 -3.321 0.04234
12 0 6.975 × 10-3 4.448 × 10-6
20 0 -0.3814 5.194
28 78.7 75.64 1.008
36 0.07 0.02757 2.186 × 10-4
44 0.04 0.01579 1.121 × 10-4

5 3 2.924 0.02818
13 28.1 27.49 3.458
21 0 0.1417 0.8452
29 -146 -140.4 3.069
37 0.02 -8.0280 × 10-4 5.667 × 10-40
45 0.03 -6.303 × 10-3 4.713 × 10-5

6 0 2.876 × 10-3 2.232 × 10-5
14 -47 -45.91 8.350
22 60.8 60.01 18.86
30 70 67.24 0.6440
38 0.09 0.01735 1.123 × 10-3
46 0.05 0.03574 9.373 × 10-5

7 0 0.06679 0.1116
15 20 19.49 1.246
23 -110 -108.8 55.26
31 0.01 -0.01388 8.925 × 10-4
39 0.07 0.04114 6.845 × 10-4

8 0 -0.08920 0.08815
16 -4 -4.521 8.873
24 50 49.52 9.968
32 0.07 0.01463 1.061 × 10-3
40 0.09 7.051 × 10-3 1.012 × 10-3

think of this network as an electrical circuit with nodes that

are interconnected through capacitors, resistors, and inductors

(in parallel). The matrices X0 and Y0 contain the capacitances, X1 and Y1 contain the conductance values of the resistors, X2 contain the inverses of the inductances, the node signals w(t)

represent the electric potentials of the interconnection points,

and the excitation signal r(t) is the derivative of a current flow

(in this case, the external signal would typically be of the form

u(t)

=

B1

d dt

i(t)

+

v(t)

with

i(t)

the

current

flow).

The discrete-time representation is obtained by applying

Proposition 1 with sampling frequency fs = 100 Hz. In addition, the disturbance v(t) acting on the network is modeled

in discrete time as a white noise filtered by a first order filter.

This results in the discrete-time physical network model (11)

[A0 +A1q-1 +A2q-2]w(t) = B0r(t)+[I +C1q-1]e(t). (73)

The network topology is assumed to be unknown reflected

by the situation that in the model there are parametrized

second order connections between all pairs of nodes. The

location where r(t) enters and the first nonzero parameter of

B0 are assumed to be known, which induces that B0 is fixed

and not parametrized. This guarantees that the identifiability

conditions 2 and 4 in Proposition 6 are satisfied.

The

symmetric

structure

of

A¯(

d dt

)

and

A(q-1)

is

taken

into

account in the parametrization of the continuous-time model

and discrete-time model, respectively. The continuous-time

model matrices (72) and the discrete-time model matrices (73)

are, respectively, parametrized as

 1c 4c 7c 1c0

 2c 5c 8c 1c1

A¯0

=

  

4c 7c

1c3 1c6

1c6 2c2

1c9 2c5

,

A¯1

=

  

5c 8c

1c4 1c7

1c7 2c3

2c0 2c6

,

(74)

1c0 1c9 2c5 2c8

1c1 2c0 2c6 2c9

10

 3c 6c 9c 1c2

A¯2

=

  

6c 9c

1c5 1c8

1c8 2c4

2c1 2c7

  

,

1c2 2c1 2c7 3c0

(75)

 1 4 7 10

 2 5 8 11

A0

=

  

4 7

13 16

16 22

19 25

  

,

A1

=

  

5 8

14 17

17 23

20 26

,

(76)

10 19 25 28

11 20 26 29

 3 6 9 12

31 32 33 34

A2

=

  

6 9

15 18

18 24

21 27

  

,

C1

=

35 39

36 40

37 41

38 42

  

.

(77)

12 21 27 30

43 44 45 46

The exact true parameter values, represented by c0 and 0
for the continuous-time and discrete time model, respectively,
are given in Table I and Table II, respectively. The external excitation signal r1(t) is an independent white noise process with mean 0 and variance r2 = 1. All nodes are subject to disturbances ei(t), which are independent white noise processes (uncorrelated with r1(t)) with mean 0 and variance e2 = 10-4. The order of the ARX model in Step 1 of the algorithm is n = 4. In Step 2 of the algorithm, the possibility to apply the weighting W (^Nn ) = P -1(^Nn ) is exploited. In Step 3 of the algorithm, at most 50 iterations
are allowed to improve the result of Step 2. However, only
one or none of these iterations are executed due to numerical issues that are caused by P (^N ) being ill-conditioned. The first experiment serves to show that the parameters can be
consistently identified with a single excitation signal only. In
order to do so, the identification is performed for different data lengths N , where the the data length is increased according to a logarithmic scale from N = 1, 000 up to N = 512, 000. For each N , 20 Monte-Carlo simulations are performed, where in
each run new excitation and noise signals are generated.
The second experiment serves to identify the parameters with a single excitation signal only. The identification consists of 100
Monte-Carlo simulations, where in each run new excitation
and noise signals are generated. The number of samples generated for each data set is N = 64, 000.

B. Simulation results

The simulation results for the first experiment are shown in Figure 4. It shows a Boxplot of the relative mean squared error (RMSE) of the discrete-time model parameters (left) and the continuous-time model parameters (right) for different data lengths N . The RMSE is determined as

RMSE =

^0 - ^N

^0

2 2

2
2,

(78)

where  contains the parameters of A(q-1) and C(q-1) for

the

discrete-time

model

and

the

parameters

of

A(

d dt

)

for

the

continuous-time model. From Figure 4 it can be seen that for

both the discrete-time and continuous-time model parameters

the RMSE decreases if N increases. This observation makes

it plausible that the estimated parameters values converge to

the true parameter values for N approaching infinity. The fact

that the curve seems to flatten off for high N , is caused by the

low order of the ARX model. Consistent identification is only

achieved if the order of the ARX model also tends to infinity,

as function of the data length N .

The simulation results for the second experiment are shown

in Figure 5 and in Table I and Table II. Figure 5 shows the

relative parameter estimation errors for the discrete-time model

parameters (left) and continuous-time model parameters(right).

Missing relative parameter errors indicate that the true param-

eter value is 0, which is related to an absent component or

interconnection in the network. Table I and Table II contain

the mean and variance of the estimated continuous-time and

discrete-time model parameters, respectively. From these ta-

bles it can be seen that parameters with a true value equal to

0 are estimated with mean values close to 0 and small variance.

Table I shows that the parameters of a higher order polynomial matrix have smaller variance. That is, the parameters of A¯2 have the smallest variance and the parameters of A¯0 have the
largest variance, even if the true values are in the same order

of magnitude.

The left figure in Figure 5 shows a Boxplot of the relative

estimation errors in the discrete-time model parameters. It can

be seen that the relative error is smaller for the parameters of A(q-1) (first 30 parameters) than for the parameters of C(q-1) (last 16 parameters). Further analysis shows that this

is mainly caused by the fact that the noise signal e(t) is

relative small compared to the excitation signal r(t) and by

the fact that the parameters of B0 are known. In addition, all parameters corresponding to a specific polynomial aij(q-1) of A(q-1) (given by the sets {1, 2, 3}, . . . , {28, 29, 30})
have similar results. The errors in parameter 16 to 21 and
25 to 27 are larger than the errors in the other parameters of A(q-1). This is because the mentioned parameters are related

to the three interconnections between node w2(t), w3(t), and w4(t), (represented by the polynomials a23(q-1), a24(q-1) and a34(q-1)) while the excitation enters the network at node
w1(t).

The right figure in Figure 5 shows a Boxplot of the relative

estimation errors in the continuous-time model parameters.

This figure shows similar results for the parameters describing

the

network

dynamics

(A(q-1)

and

A(

d dt

))

as

the

left

figure

of Figure 5. Again, the parameters corresponding to a specific

polynomial

aij

(

d dt

)

of

A(

d dt

)

have

similar

results,

although

this is less clear for continuous-time parameters tha for the

discrete-time parameters. Table I shows that the continuous-

time parameter estimates of a higher order polynomial matrix have smaller variance. That is, the parameters of A¯2 have the smallest variance and the parameters of A¯0 have the largest

variance, even if the true values are in the same order of

magnitude.

The fact that some parameters do not have a mean relative

estimation error close to 0 indicates that the some parameter

estimates are biased. The bias is caused by the limited order n

of the ARX model in Step 1 of the algorithm. To decrease the

bias, n can be increased, but the estimation will only improve

if the number of data samples N is increased accordingly.

Further analysis of the simulation results shows that Step 3

achieves only little performance improvement, or even none at all. This is due to numerical issues in P -1(^N ) that cause

rank loss in the weighting filter (64), which result in inaccurate

11

Relative MSE discrete-time parameters 10-1

Relative MSE continuous-time parameters 10-1

10-2

10-2

10-3

10-3

1

2

4

8 16 32 64 128 256 512

N x 1e-3

1

2

4

8 16 32 64 128 256 512

N x 1e-3

Fig. 4. Boxplot of the relative means squared error (78) of the discrete-time model parameters (left) and the continuous-time model parameters (right) for different data lengths N .

Relative discrete-time parameter errors 4 2 0 -2 -4 -6 -8 -10
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45

Relative continuous-time parameter errors 0.6 0.4 0.2
0 -0.2 -0.4 -0.6
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29

Fig. 5. Boxplot of the relative estimation errors of the discrete-time model parameters (left) and the continuous-time model parameters (right).

parameter estimates. In addition, the results after completing Step 2 of the algorithm will be worse if no weighting matrix is used in Step 2, but the improvement obtained in Step 3 will be more significant. In the end, choosing a weighting matrix in Step 2 improves the final results.
VII. DISCUSSION In this section, three extensions of the presented theory are discussed. First, the connection with dynamic networks is made. Second, networks with unmeasured nodes are considered. Third, parameter constraints are discussed.
A. Dynamic networks A commonly used description of dynamic networks is the module representation [11], in which a network is considered to be the interconnection of directed transfer functions (modules) through measured node signals as
w(t) = G(q)w(t) + R(q)r(t) + H(q)e~(t), (79)

with white noise process e~(t) and with proper rational transfer function matrices G(q), R(q), and H(q)  F, where the matrix entries Gjk(q), Rjk(q), and Hjk(q) describe the dynamics in the paths from wk(t), rk(t), and e~k(t) to wj(t), respectively. A physical network (11) can be described as a module representation with the following particular symmetrical properties: [9]
· The transfer functions Gjk(q) and Gkj(q) have the same numerator for all j, k.
· The transfer functions Gjk(q) and Rjm(q) have the same denominator for all k, m.
· The transfer functions Gjk(q) and Hjm(q) have the same denominator for all k, m if F (q) is polynomial.
Moreover, conditions for a unique mapping between a module representation and a physical network are formulated in [9]. The structure of G(q) and R(q) corresponding to a physical network with three nodes is illustrated by Figure 6. It shows that the modules Gjk(q) = -a-jj1(q-1)ajk(q-1) and Gkj(q) = -a-kk1(q-1)ajk(q-1) have the same numerator

12

-a11-1a13

w3

-a22-1a23

-a33-1a13

-a33-1a23

-a22-1a12

w1

-a11-1a12

w2

a22-1b22

r2

Fig. 6. A module representation of a physical network with three nodes.
(ajk(q-1)) and all transfer functions in the paths towards a specific node wj have the same denominator (ajj(q-1)). Since Gjk(q) and Gkj(q) have the same numerator, they will either be both present or both absent, which is in accordance with the fact that they represent a single physical interconnection. In addition, the connections to the ground node are only present in the denominators, because they are only present in ajj(q-1). This means that they do not have an effect on the topology in the module representation, although they are part of the topology in the physical network.

B. Partial measurements
Throughout this paper we assumed that all node signals are measured, although this is not always possible. As mentioned in Section II-A, unmeasured node signals can be removed from the representation by Gaussian elimination, which is equivalent to Kron reduction [2] and immersion [12]. The resulting network representation does not necessarily directly have a polynomial form with a symmetric A(q-1). However, with a unique additional step, it can always be transformed into a representation that satisfies these physical network properties, see Definition 9. Thus, the experimental setting considered in this paper is that all node signals in the physical network are measured. As a result, at least one excitation signal is necessary for identifiability, see Proposition 6. Observe that this excitation always excites a measured node. The dual problem considers the experimental setting where all nodes are excited. For this problem the identifiability conditions changes in the sense that at least one node signal needs to be measured instead of that at least one excitation signal needs to be present (condition 3 of Proposition 6). In practice, this dual situation is rare as it is uncommon that all nodes can be excited in a physical network. Therefore, the focus is on the situation where all nodes are measured. In the practical situation where only a subset of node signals can be excited and only a subset of nodes can be measured, it can be shown that identifiability can be achieved if all nodes are excited o´r measured and one node is both excited and measured. This is in accordance with the literature [4], [5].

C. Parameter constraints
Physical networks that consist of interconnected physical components, such as mass-spring-damper systems and RLC circuits, are known to have positive real-valued coefficient values in the continuous-time representation (2). This is because

the coefficients in the continuous-time network model (2) represent the values of the physical component in the network. This also leads to coefficients with known signs in the corresponding discrete-time representation. In both the theoretical consistency proof and the practical algorithm, these sign constraints are not taken into account, which means that the theory presented in this paper also holds for networks without these sign constraint. The sign constraints can be taken into account in the algorithm by adding inequality constraints of the form u < 0 to the optimization problems (54) and (62). As explained before, known parameter values can easily be taken into account by the equality constraint  =  in the optimization problems (54) and (62). Known (continuous-time) component values can be taken into account as well, by splitting  as  = u + k, where u and k represent the unknown and known part of , respectively. Then the linear form -Q(n) = 0 leads to -Q(n)u + k(n, k) = 0, where k(n, k) := -Q(n)k is known.

VIII. CONCLUSION
The undirected network description of physical networks has been extended by allowing for higher order diffusive couplings. Undirected network descriptions of physical systems with diffusive couplings can be represented as polynomial systems with particular structural properties. This allows for effective identification of the global dynamics and topology of the physical network, for which only a single excitation signal is needed. The identification is performed through a multi-step algorithm that relies on convex optimizations and the results of identifying the topology and parameters are illustrated in a Monte Carlo simulation example. It is shown how undirected physical networks form a particular subclass of directed (module) dynamic networks with structural constraints. The considered situation of all network nodes being measured can be extended to the situation of a selected set of measured and excited node signals.

APPENDIX

A. Parametrization
The model structure A(q-1, an) and B(q-1, bn) of the nonparametric ARX model (42) is parametrized in terms of the parameters n. The parameter vector n := [an] [bn] is given by

an1 

ani1 

a1,ij 

an

=

 an2

  

...

   

,

ani

=

   

ani2 ...

   

,

anij

=

a2,ij 

  

...

,  

(80)

anL

aniL

an,ij

bn1 

bn

=

   

bn2 ...

   

,

 bni1 

bni

=

   

bni2 ...

 ,  

b0,ij 

bnij

=

b1,ij 

  

...

.  

(81)

bnL

bniK

bn,ij

The model structure A(q-1, a), B(q-1, b), and C(q-1, c) of the physical network model (41) is parametrized in

13

terms of the parameters  and , where C¯(q-1, c) = C(q-1, c)A0(a). The parameter vectors a, b, c, and c
are given by

a1 

a2 

a

=

 



...

,  

aL

b1 

b2 

b

=

 



...

,  

bL

c1 

c2 

c

=

 



...

,  

cL

c1 

c2 

c

=

 



...

,  

cL

 aii 

ai(i+1) 

ai

=

 



...

,  

aiL

 bi1 

 bi2 

bi

=

 



...

,  

biK

ci1 

ci2 

ci

=

 



...

,  

ciL

ci1 

ci2 

ci

=

 



...

,  

ciL

 a0,ij 

 a1,ij 

aij

=

 



...

 , (82)  

anA,ij

 b0,ij 

 b1,ij 

bij

=

 



...

 , (83)  

bnB ,ij

 c1,ij 

 c2,ij 

cij

=

 



...

 , (84)  

cnC ,ij

 c¯1,ij 

 c¯2,ij 

cij

=

 



...

 . (85)  

c¯nC ,ij

Observe that A(q-1) is parametrized symmetrically and that c¯0,ij = a0,ij and therefore is parametrized as such. Remember that  = a b c .

B. Matrix [n(t)]

The regressor [n(t)] in (47) is given by [n(t)] = [ny (t)] [nr (t)] with

[ny (t)] [nyi (t)] [nr (t)] [nri (t)]

= [ny1 (t)] [ny2 (t)] · · · [nyL (t)] , (86)

= yi(t - 1) yi(t - 2) · · · yi(t - n) , (87)

= [nr1 (t)] [nr2 (t)] · · · [nrK (t)] , (88)

= ri(t) ri(t - 2) · · · ri(t - n) .

(89)

C. Matrices Q(n) and T ()

In order to construct Q(n) and T (), we first define some other matrices. 1) Zero and identity: Let 0i,j denote a matrix of dimension i×j with all its elements equal to 0. Let Ii,j denote an identity matrix of dimension i×j, where Ii,j = Ii,i 0i,j-i for i  j
and Ii,j = Ij,j 0j,i-j for i  j. Let Ik(i,j) denote a block diagonal matrix of k blocks of Ii,j and let I (k(i,j)) denote a block diagonal matrix of blocks of Ik(i,j). 2) : Define the matrices

 ani1

 

...



ani(i-1)

ai

:=

 

anii

ani(i+1)

  

...

aniL

0n,nA 

...

 



0n,nA

 

-In,nA

 

,

0n,nA

 

 bni1

bi

:=

   

bni2 ...

...

  

bniK

0n,nA

0n+1,nA 

0n+1,nA 

...

,  

0n+1,nA

(90)

and observe that ai has dimensions Ln × (nA + 1) and that bi has dimensions K(n + 1) × (nA + 1). For x  {a, b}, define the block matrix

 Z0x,L

Z1x,L-1

···

ZLx-1,1 

¯ xL := R(x1 , xL) R(x2 , xL) · · · R(xL, xL) ,

SL-1(x1 ) SL-2(x2 ) · · · SL-L(xL)

(91)

with Zia,j an i × j block matrix with blocks 0Ln,(nA+1) and with Zib,j an i × j block matrix with blocks 0K(n+1),(nA+1) (that is Zia,j := 0iLn,j(nA+1) and Zib,j := 0iK(n+1),j(nA+1)),

with

R(xi , xj ) := xi xi+1 · · · xj , for i  j, (92)

and with

Si(xj ) := Zix,1 Di(xj ) ,

(93)

with Di(xj ) a block diagonal matrix consisting of i blocks of

xj . and

Observe ¯ bL has

that ¯ aL has dimensions

dimensions LK(n + 1)

L2n

×

1 2

L(L+1)(nA

+1)

×

1 2

L(L

+

1)(nA

+

1).

3) Toeplitz: Let Ti,j(x) denote a Toeplitz matrix of dimension

i × j (with i  j) given by

 x0

 

...

...

Ti,j (x) := Ti,j ( x0 x1 · · · xi-1 ) = xj-1 · · ·

  

...

...

0





x0

 

.

...

  

xi-1 · · · xi-j-1 (94)

Define the following Toeplitz matrices of dimension k × :

Tk, (aij ) := Tk, ( a0,ij a1,ij · · · ak,ij ),

(95)

Tk, (bij ) := Tk, ( 0 b0,ij b1,ij · · · bk,ij ), (96)

Tk, (cij ) := Tk, ( a0,ij c¯1,ij · · · c¯k,ij ),

(97)

Note that for aij it is known that a0,ij = 1 for i = j and a0,ij = 0 for i = j; and note that for c¯ij it is known that c¯0,ij = a0,ij = a0,ji and c¯k,ij = 0 for k > nC . Define the following block matrices

Tk, (a11) · · · Tk, (aL1)

T¯k,

(A)

:=

 

...

...

, 

(98)

Tk, (a1L) · · · Tk, (aLL)

 Tk, (b11) · · · Tk, (bL1) 

T¯k,

(B )

:=

 

...

...

, 

(99)

Tk, (b1K ) · · · Tk, (bLK )

where T¯k, (A) has dimensions Lk × L and T¯k, (B) has
dimensions Kk × L . For x  {a, b}, let T¯m(k, )(X ) denote a block diagonal matrix consisting of m blocks of T¯k, (X ). Observe that T¯n,nc (A) has dimension Ln × Lnc and that T¯n+1,nc (B) has dimension K(n + 1) × Lnc.
Let Tm(k, )(c¯ij) denote a block diagonal matrix consisting of m blocks of Tk, (c¯ij). Observe that TL(n,n)(c¯ij) is an Ln×Ln block diagonal matrix consisting of L blocks of Tn,n(c¯ij) and

14

that TK(n+1,n+1)(c¯ij) is an K(n+1)×K(n+1) block diagonal matrix consisting of K blocks of Tn+1,n+1(c¯ij). Finally, define

Tm(k, )(c¯11) · · · Tm(k, )(c¯1L)

Tm(k,

)(C¯)

:=

 

...

...

 . (100) 

Tm(k, )(c¯L1) · · · Tm(k, )(c¯LL)

4) Matrix Q(n): With the matrices defined above, we can now describe the matrix Q(n) in (53) by

Q(n) =

¯ aL ¯ bL

0 -IL(K(n+1,nb))

T¯L(n,nc)(A) T¯L(n+1,nc ) (B )

,

(101)

which has row dimension Ln2 + LK(n + 1) and column

dimension

1 2

L(L

+

1)(nA

+ 1) + LK(nB

+ 1) + L2nC .

5) Matrix T (): With the matrices defined above, we can now

describe the matrix T () in (61) by

T () =

-TL(n,n) (C¯ ) 0

0 -TK (n+1,n+1) (C¯ )

,

(102)

which has dimensions [Ln2+LK(n+1)]×[Ln2+LK(n+1)].

REFERENCES
[1] X. Cheng, Y. Kawano, and J. M. A. Scherpen, "Reduction of secondorder network systems with structure preservation," IEEE Transactions on Automatic Control, vol. 62, no. 10, pp. 5026­5038, 2017.
[2] F. Do¨rfler and F. Bullo, "Kron reduction of graphs with applications to electrical networks," IEEE Transactions on Circuits and Systems I: Regular Papers, vol. 60, no. 1, pp. 150­163, 2013.
[3] F. Do¨rfler, J. W. Simpson-Porco, and F. Bullo, "Electrical networks and algebraic graph theory: Models, properties, and applications," Proceedings of the IEEE, vol. 106, no. 5, pp. 977­1005, 2018.
[4] M. Friswell, S. Garvey, and J. Penny, "Extracting second order systems from state space representations," AIAA Journal, vol. 37, no. 1, pp. 132­ 135, 1999.
[5] P. Lopes dos Santos, J. A. Ramos, T. Azevedo-Perdicou´lis, and J. L. Martins de Carvalho, "Deriving mechanical structures in physical coordinates from data-driven state-space realizations," in Proceedings of the 2015 American Control Conference (ACC), 2015, pp. 1107­1112.
[6] C.-P. Fritzen, "Identification of mass, damping, and stiffness matrices of mechanical systems," Journal of Vibration, Acoustics, Stress, and Reliability in Design, vol. 108, no. 1, pp. 9­16, 1986.
[7] H. Lus¸, M. D. Angelis, R. Betti, and R. W. Longman, "Constructing second-order models of mechanical systems from identified state space realizations. part i: theoretical discussions," Journal of Engineering Mechanics, vol. 129, no. 5, pp. 477­488, 2003.
[8] L. Ljung, System identification: theory for the user. Englewood Cliffs, NJ: Prentice-Hall, 1999.
[9] E. M. M. Kivits and P. M. J. Van den Hof, "A dynamic network approach to identification of physical systems," in Proceedings of the 58th IEEE Conference on Decision and Control (CDC), 2019, pp. 4533­4538.
[10] J. Gonc¸alves, R. Howes, and S. Warnick, "Dynamical structure functions for the reverse engineering of lti networks," in Proceedings of the 46th IEEE Conference on Decision and Control (CDC), 2007, pp. 1516­1522.

[11] P. M. J. Van den Hof, A. Dankers, P. S. C. Heuberger, and X. Bombois, "Identification of dynamic models in complex networks with prediction error methods-basic methods for consistent module estimates," Automatica, vol. 49, no. 10, pp. 2994­3006, 2013.
[12] A. G. Dankers, P. M. J. Van den Hof, X. Bombois, and P. S. C. Heuberger, "Identification of dynamic models in complex networks with prediction error methods: Predictor input selection," IEEE Transactions on Automatic Control, vol. 61, no. 4, pp. 937­952, 2016.
[13] M. Mesbahi and M. Egerstedt, Graph theoretic methods in multiagent networks. Princeton University Press, 2010.
[14] T. Mori, P. Nikiforuk, M. Gupta, and N. Hori, "A class of discrete time models for a continuous time system," in Proceedings of the 1987 American Control Conference (ACC), 1987, pp. 953­957.
[15] J. Ramos, G. Merce`re, and O. Prot, "Identifying second-order models of mechanical structures in physical coordinates: an orthogonal complement approach," in Proceedings of the 12th European Control Conference (ECC), 2013, pp. 3973­3978.
[16] E. J. Hannan and M. Deistler, The Statistical Theory of Linear Systems. SIAM, 2012.
[17] H. H. M. Weerts, P. M. J. Van den Hof, and A. G. Dankers, "Identification of dynamic networks operating in the presence of algebraic loops," in Proceedings of the 55th IEEE Conference on Decision and Control (CDC), 2016, pp. 4606­4611.
[18] D. Youla, "On the factorization of rational matrices," IRE Transactions on Information Theory, vol. 7, no. 3, pp. 172­189, 1961.
[19] H. H. M. Weerts, P. M. J. Van den Hof, and A. G. Dankers, "Identifiability of linear dynamic networks," Automatica, vol. 89, pp. 247­258, 2018.
[20] T. Kailath, Linear systems. Englewood Cliffs, NJ: Prentice-Hall, 1980. [21] M. Deistler, "The properties of the parameterization of armax systems
and their relevance for structural estimation and dynamic specification," Econometrica, vol. 51, no. 4, pp. 1187­1207, 1983. [22] J. M. Hendrickx, M. Gevers, and A. S. Bazanella, "Identifiability of dynamical networks with partial node measurements," IEEE Transactions on Automatic Control, vol. 64, no. 6, pp. 2240­2253, 2019. [23] X. Cheng, S. Shi, and P. M. J. Van den Hof, "Allocation of excitation signals for generic identifiability of linear dynamic networks," IEEE Transactions on Automatic Control, vol. 67, no. 2, 2022, to appear ArXiv:1910.04525. [24] M. Gevers, A. S. Bazanella, and G. V. da Silva, "A practical method for the consistent identification of a module in a dynamical network," IFAC-PapersOnLine, vol. 51, no. 15, pp. 862­867, 2018. [25] H. H. M. Weerts, M. Galrinho, G. Bottegal, H. Hjalmarsson, and P. M. J. Van den Hof, "A sequential least squares algorithm for armax dynamic network identification," IFAC-PapersOnLine, vol. 51, no. 15, pp. 844­ 849, 2018. [26] M. Galrinho, C. R. Rojas, and H. Hjalmarsson, "Parametric identification using weighted null-space fitting," IEEE Transactions on Automatic Control, vol. 64, no. 7, pp. 2798­2813, 2019. [27] S. J. M. Fonken, "Multi-step scalable least squares method for network identification with unknown noise topology," Master's thesis, Eindhoven University of Technology, 2020. [28] L. Ljung and B. Wahlberg, "Asymptotic properties of the least-squares method for estimating transfer functions and disturbance spectra," Advances in Applied Probabilities, vol. 24, no. 2, pp. 412­440, 1992. [29] M. Galrinho, "Least squares methods for system identification of structured models," Ph.D. dissertation, KTH School of Electrical Engineering, 2016. [30] E. K. P. Chong and S. H. Z ak, An introduction to optimization, 3rd ed. Wiley Interscience, 2008.

15

