arXiv:2106.01596v1 [cs.CV] 3 Jun 2021

Attention-Guided Supervised Contrastive Learning for Semantic Segmentation
Ho Hin Lee Yucheng Tang Qi Yang Xin Yu Shunxing Bao Bennett A. Landman
Yuankai Huo
Electrical Engineering and Computer Science, Vanderbilt University Nashville, TN 37212
ho.hin.lee@vanderbilt.edu
Abstract
Contrastive learning has shown superior performance in embedding global and spatial invariant features in computer vision (e.g., image classification). However, its overall success of embedding local and spatial variant features is still limited, especially for semantic segmentation. In a per-pixel prediction task, more than one label can exist in a single image for segmentation (e.g., an image contains both cat, dog, and grass), thereby it is difficult to define "positive" or "negative" pairs in a canonical contrastive learning setting. In this paper, we propose an attention-guided supervised contrastive learning approach to highlight a single semantic object every time as the target. With our design, the same image can be embedded to different semantic clusters with semantic attention (i.e., coerce semantic masks) as an additional input channel. To achieve such attention, a novel two-stage training strategy is presented. We evaluate the proposed method on multiorgan medical image segmentation task, as our major task, with both in-house data and BTCV 2015 datasets. Comparing with the supervised and semi-supervised training state-of-the-art in the backbone of ResNet-50, our proposed pipeline yields substantial improvement of 5.53% and 6.09% in Dice score for both medical image segmentation cohorts respectively. The performance of the proposed method on natural images is assessed via PASCAL VOC 2012 dataset, and achieves 2.75% substantial improvement.
1 Introduction
Supervised deep learning strategies provide great opportunities in semantic segmentation across different modalities, to generate automatic segmentation for images acquiring from multiple domains, such as magnetic resonance imaging (MRI), multi-contrast computed tomography (CT) and natural images [16, 56]. While large well-annotated datasets are demanded for training robust segmentation models on images from a single modality, achieving robust segmentation on multi-modality images for robust segmentation within a single model is more challenging, with the significant domain shift between the variation of imaging protocols and scanners. [16, 56]. A number of approaches have explored domain adaptation for semantic segmentation in natural image and medical perspectives. [6, 32, 41, 58, 59, 63]. However, most proposed alternatives cannot provide robust generalization for large-scale datasets and use multiple encoders to extract modal-corresponding feature representation independently [17, 33, 45].
Preprint. Under review.

Traditional contrastive learning for classification:


If image has multiple semantic targets:



Contrastive Contrastive

: Sharing Weights\Activations : Encoder Network : Linear Layer
"Dog" (Class Prediction)
"? Category" Wrong Prediction

To solve the limitation of multiple semantic targets :

, ,

Contrastive

"Dog" (Class Prediction)

Coarse Attention

, ,
Binary Mask

Contrastive

"Cat" (Class Prediction)

Figure 1: For traditional contrastive learning, object-centric images (such as Cifar-10/Cifar-100) are targeted and have significant boost in performance for classification with categorical label leveraging [10, 31]. However, when images are with multiple semantic targets, categorical definition cannot be well provided and it is limited to integrate the contrastive learning theory to generalize distinctive representation for each semantic targets. To solve this limitation, we initial generate coarse attention to provide localization of semantic targets and extract object-centric patches with attention as additional channel, preserving semantic targets' information for contrastive pre-training.

In recent years, contrastive learning, a variant of self-supervised learning (SSL) strategies, led to a major advance to extract the distinctive representation for image classification [10]. The common theory is the following: 1) pull an anchor and a 'positive' sample together, and 2) push the anchor from numerous 'negative' samples in the embedding space. Without using any class-wise or pixelwise labels, data augmentation is used to generate 'similar' samples as positive pairs and learn the pairwise representation as self-supervised setting, while the negative pairs (dissimilar samples) are compounded by the anchor and the remaining samples from the minibatch. Although previous works demonstrates significant success in classification tasks [11, 12, 60], limited studies are performed in using contrastive learning to separate semantic targets into independent embedding as multi-target exist in single image for segmentation, while some studies start to integrate the idea of contrastive learning into segmentation task [4, 37, 69]. The idea of contrastive learning provides an opportunity to independently adapt the distinctive representation of semantic targets as the specific embedding in the latent space. We believe that the contrastive learning strategies can increase the generalization power of the encoder to adapt the variation and limitation in multi-modality images.
In this work, we propose an attention-guided contrastive learning (AGCL) framework that further integrates contrastive strategies to semantic segmentation and adapt multiple limitations between multi-modality images. The attention information initially localized the semantic targets and minimize biased information from background or neighboring organs for segmentation tasks as additional channel. Meanwhile, modality and semantic target corresponding information are provided to the normalized embedding. The embedding from the same modality and target classes are pulled closer together, while the embedding is also pulled to the corresponding modality space if it belongs to the same modality but different target. Our technical novelty in this work is to 1) advance the integration of contrastive learning theory to pixel-wise semantic segmentation, and 2) achieve new contrastive learning strategy to extract distinctive representation with the explainable guidance of semantic attention. As the self-supervised contrastive loss (SSCL) only contrasts a single positive for each anchor, we propose a modal-target contrastive loss function that leverages the number of positives with the same classes of modality and target instead of considering the augmented version of the anchor only, in addition to the supervised contrastive loss [31]. It encourages that the separation of embedding can be well-defined into modality sub-classes and target sub-classes,
2

tackling the challenges of learning independent features from each modality. Figure. 1 provides a visual explanation of our proposed framework.
Our propose learning strategy, AGCL, is evaluated on two multi-contrast phase CT datasets (contrastenhanced and non-contrast phase) and one natural image public dataset. We found that a consistent improvement is achieved on ResNet-50 and ResNet-101 architectures [23]. On ResNet-50, we achieve a mean segmentation performance of 0.923, which is a 6.95% improvement over the semisupervised state-of-the-art (pre-training with SSCL [10]) and a 1.52% improvement over the fully supervised state-of-the-art (pre-training with cross-entropy loss) on the same architecture for medical image datasets. Meanwhile, substantial increases of 2.75% in segmentation performance are also demonstrated with natural image dataset in ResNet-50 backbone. The significant improvement of the segmentation performance is evaluated with the public contrast-enhanced CT dataset [34], noncontrast testing cohorts and the validation set of PASCAL VOC 2012 [19]. Our main contributions are summarized as below:
1. We propose an attention-guided framework to initially localize multiple semantic targets and extract semantic information to leverage the contrastive learning theory to pixel-wise semantic segmentation. 2. We introduce hierarchical three-stage design to integrate the attention information with images as multi-channel input and adapt explainable representation with contrastive loss. 3. We propose a modal-target contrastive loss function, in addition to the supervised contrastive loss, to increase stability and separate to the corresponding modality- and target-defined latent space for advancing segmentation performance. 4. We demonstrate that AGCL generalize the contrast intensity variation in each modality and significantly boost the segmentation performance for each modality within single network architecture.
2 Related Works
As our works focus on the perspectives of self-supervised representation learning and supervised learning for segmentation, recent works have demonstrated that deep learning strategies contributed great efforts in semantic segmentation. In the natural imaging domain, various deep learning approaches have been proposed to improve the segmentation performance gradually with large amount of pixel-level annotations [7­9, 54, 61, 62]. As generating pixel-level annotations are time-consuming and cost significant efforts, several works propose to use pre-training strategies with large-scale dataset such as ImageNet classification dataset. A soft supervision is leveraged in the forms of image-level labels [1, 27, 36, 42] and bounding boxes [27, 30, 51]. Unlabeled datasets are also used as a semi-supervised setting to adapt contextual information for leveraging robustness. On the other hand, several lowss functions are proposed to model the pixel relationships between semantic targets [29, 67]. The goal of these loss functions is to maximizes the similarity between the pixel-wise predictions and the ground truth labels.
Apart from modeling the label space, self-supervised representation learning approaches are recently proposed to learn useful representation from unlabeled data with an unsupervised loss in the training process. Some of the approaches are proposed to learn the embeddings in the lower-dimensional representation space instead of computing pixel-wise predictive loss [15, 64, 65]. Contrastive learning is one of the state-of-the-art methods for self-supervised learning to model the pixel relationships in the latent space [10, 25, 26, 49, 53, 68]. It employs a contrastive loss function to pull the representations near in the latent space for positive pairs, while the representation is pushed away for negative pairs. Similarity is defined using data augmentations to generate transformed image as depicted in Figure. 1. Maximizing mutual information between embeddings are also proposed as an alternative to extract the similar information between targets [2, 40, 43]. Adapting with memory bank and momentum contrast method are proposed to increase the batch size and generate more dissimilar pairs for comparison in minibatch [22, 38, 57]. To constraint and stabilize the embedding space, class label information is added as a supervision for contrastive learning to increase the number of positive pairs meeting with large number of negatives [31].
In the medical image domain, the initial naive approach of performing segmentation is to directly train a deep network model with post-processing techniques [14, 39, 55]. As the performance of supervised learning strategies is limited to the quality of voxel-wise labels and the resolution of volumes [20, 50], hierarchical approaches and patch-wise approaches are proposed to perform segmentation across scales and adapt the refine representation with the resolution variability [47, 71].
3

Stage 1: Generation of Organ Attention
... Multi-Modality Volumes

Stage 2: Organ-Attention Contrastive Pre-training Modal-Target Contrastive Loss

3D Seg
2D Patch Extraction

...
256-D 2048-D
Encoder

Stage 3: Multi-Modal Organs Segmentation Training

Decoder

Label Fusion Module

Atrous Spatial Pyramid Pooling
Decoder
Encoder

Image Patches
Organ Attention Patches
Multi-Modality OrganCorresponding Patches

...

, 

, 

Modalities Organs

...
: Sharing Weights\Activations

Figure 2: The complete hierarchical framework can be divided into three stages: 1) We initially use a 2D/3D segmentation pipeline (2D for natural image, 3D for medical image) to generate coarse segmentation, in order to localize the semantic information of multiple targets. 2D patches are randomly extracted corresponding to the attention information. 2) The attention map is concatenated with the image as multi-channel input for training an encoder and extract distinctive representation by adding modality and target supervision as supervised contrastive loss. 3) The encoder is weight frozen and follow by a decoder to reconstruct the distinctive representation as segmentation prediction. Each target prediction is finally fused with label fusion module as multi-organ segmentation map.

However, multiple models are needed to be trained for multiple semantic targets segmentation, which lacks of flexibility in the training process and is limited to a single modality. To adapt multi-modality images for segmentation, several works have been proposed to perform domain adaptation and use the source domain representation to adapt target domain [5, 6, 18, 48]. Multi-modal approaches are also proposed to adapt multi-modality images using multiple encoders with independent batch normalization layers and reconstruct the representation with the shared decoder [17]. However, limited studies are demonstrated to generate an encoder to adapt independent representations from multiple modalities and perform robust segmentation. On the other hand, self-supervised learning has been explored with less extent in the medical imaging perspective. Pretext tasks such as colorization and image rotation, are used as pre-training features to initialize the segmentation network [3, 28, 46]. Self-supervised contrastive learning has been also used to extract global and local representation for domain-specific MRI images [4].
3 Method
In this section, we explore the integration of semantic attention with contrastive learning to adapt segmentation purpose, and develop the theoretical derivation of modal-target supervised contrastive loss for semantic segmentation with multi-modality images within single network architecture.
3.1 Hierarchical Contrastive Segmentation Framework
Given a set of N randomly sampled patches Pi = {xi, yi, ci}i=1,...,N , where N is the total number of image patch in batch, xi is the image patch, yi and ci are the corresponding label patch and coarse attention patch respectively. Integrating the attentional information as multi-channel input ai with image patches provide robust localization of feature representation within bounded regions during interference [35]. The framework mainly consists of:
· P airwise Data Augmentation module, PAug(·), given a batch of multi-modal mixed data, a pairwise copies with multiple views are generated ai = PAug(ai) with random data augmentations
4

from ai. Both pairwise copies consist of the subset of the semantic information from the original targets. The augmentation modules composite of random cropping, rotation (random from -30 to +30), and random scaling (width: 0.3, length: 0.7).

· Encoder N etwork, E(·), ResNet-like network architecture is used the backbone of the encoder to ensure high stability of mapping ai as pairwise feature representation vector zi = E(ai), {zi}  ROE , where OE is output size of the latent space. The goal of the encoder is to learn the discrete embedding for specific targets and modalities, which map the feature representation as a point to the hypersphere with a radius of 1/T , where T is a hyperparameter as temperature scaling to control the weighting on the positive/negative pairs. The contrastive pre-trained encoder is then weight frozen and compute representation vector zi = E{ai} for decoder network to generate refine segmentation.

· P rojection N etwork, P (·), we follow the similar structure in [10] using multi-layer perceptron with output vector size of OE = 256. We compute the distances between each point in the projection space and meet our goal of separating discrete clusters with our proposed contrastive loss function for each target in corresponding modality. The projection network is discarded after the encoder section is well pre-trained. Sec.3.3 gives details on the theory of our proposed contrastive loss function for semantic segmentation.

· Decoder N etwork, D(·), the distinctive representation decode with atrous spatial pyramid pooling (ASPP) modules to generate binary segmentation predictions si = D(zi), si  Rb×2×h×w, where b is the batch size, h and w are the height and width of the image patch respectively. The segmentation
loss for medical image domain Lseg is computed with the binary organ-corresponding label yi as the following:

Lseg =

1 - 2 · si · yi si + yi

(1)

3.2 Semantic Attention Guidance
As the theory of contrastive learning is to learn the representation as independently defined embedding, superior performance is demonstrated with categorical label information leveraging by object-centric images. However, it is limited to extract categorical information when the image consists of multiple objects and independent embedding cannot be well defined in the latent space. For segmentation task, we propose to adapt coarse segmentation as semantic attention and initially localize the semantic targets in the image. Target-corresponding patches are extracted according to the attentional information to generate an object-centric setting for contrastive learning. Therefore, categorical information of each image can be classified to leverage the stability of learning independent embeddings in the latent space.
Apart from creating object-centric setting for contrastive learning, semantic attention provides an opportunity to extract explainable feature representation with contrastive learning. We convert each target-corresponding attention to binary setting and concatenate with the image as additional channel guidance. The extraction of the distinctive representation for each target can be stably guided as demonstrated in Figure. 2, with the attention bounded region and minimize the biased information obtaining from the neighboring targets.

3.3 Modal-Target Guided Contrastive Loss
We also look into the contrastive loss functions to increase the stability of learning independent categorical embedding. It begins with the self-supervised training strategies to self-predict the spatial localization of representation in the latent space. We initially compute the pairwise augmented samples for training in a total of 2N pairs, {ai, si}i=1,...,2N , where a2k and a2k-1 are the two randomly augmented versions from the original image patch with different views as k = 1...N , and li = {mi  1...M, oi  1...O} is the corresponding modal-target information, where M and O represents the number of modalities and the number of semantic targets.
5

Figure 3: Distribution of the representation of four randomly picked organs in each modality (Blue: contrastenhanced phase CT, red: non-contrast phase CT) using self-supervised contrastive loss (SSCL) and modal-target contrastive loss respectively.

3.3.1 Self-Supervised Contrastive Loss (SSCL)

In the self-supervised setting, the contrastive loss function are defined as the following for pre-training the encoder network:

2N
Lself = - log
k=1

exp(zk · zp(k)/T ) , jJ(k) exp(z2k · zj /T )

zk, zp(k) = P (E(a2k, a2k-1))

(2)

Here, pairwise feature representation vectors zk and zp(k) are generated, where the index k represents the sample of anchor and index p(k) represent the corresponding positive. The only positive pair zk and zp(k) are aimed to be classified among all pairs within a batch and encourage the corresponding representation to be similar in the latent space. In contrast, the other 2(k - 1) representation vectors zj are classified as the negative pairs, in which we define as dissimilar patches from other targets and modalities. There is a total of 2N - 1 augmentation samples as the denominator (1 positive pair and 2N - 2 negative pairs).

3.3.2 Learn Modal-Target Corresponding Embedding

With the use of the contrastive loss function above, we observed that the representation vectors of each modality can be fairly separated in the embedding space as shown in Figure. 3. However, some of the target's representation cannot be well generalize and distinguished from the embeddings. The cluster of each target in specific modality from Figure. 3, showed that some of feature representation extracted cannot be stably classified to the corresponding spatial location in latent space. SSCL is limited to provide sufficient information in the latent space for embedding separation and incapable of providing semantic information to each representation vectors. In order to solve this limitation, we adapt the modality information and the semantic information from the attention information into SSCL, generalizing Eq.2 with modal-target supervision.

2N -1

LMT = |L(k)|

log

k=1

lL(k)

exp(zk · zl/T ) jJ(k) exp(zk · zj /T )

(3)

Here, L(k)  {l  J(k) : {ml, ol} = {mi, oi}} and |L(k)| is the cardinality of the representation vector corresponding to modalities and semantic targets respectively. The modal-target information encourage the encoder to map the representation with high stability to all entries with multi-classes label leveraging. Motivated by [31], the integration of modality domain and target information preserve the summation of negatives in the denominator and increase the contrastive power with increasing number of positives towards large number of negatives. Meanwhile, as the Eq.2 may induce gradient that give rise to hard positive/negative mining implicitly, Eq.3 adding with modaltarget information avoid the need of explicit hard mining and generalize it to all / partial positive (All positive: same modality and same target; partial positive: same modality but different target).

6

Figure 4: Performance comparison between multiple pre-training strategies in box plots of 12 abdominal organs using ResNet-50 as the backbone model. (*: p<0.05, **: p<0.01)

1.0 0.9 0.8













































 

Dice Similarity Coefficient

0.7

0.6 Fully Supervised (No Pretrain)

Semi-Supervised (Pretrain with SSCL)

0.5

Fully Supervised (Pretrain with CE)

Fully Supervised (Pretrain with AGCL)

0.4 Spleen Right Left

Gall Esophagus Liver Stomach Aorta Inferior Portal Pancreas Right

Kidney Kidney Bladder

Vena Splenic

Adrenal

Cava Vein

Gland

Table 1: Ablation studies of BTCV testing dataset on the segmentation performance in various network backbones.

Encoder Pretrain Spleen R.Kid L.Kid Gall. Eso. Liver Stomach Aorta IVC PSV Pancreas R.A

ResNet50 ResNet50 ResNet50 ResNet50

× SSCL
CE AGCL

0.932 0.953 0.959 0.971

0.877 0.922 0.948 0.955

0.887 0.930 0.957 0.963

0.860 0.842 0.890 0.910

0.761 0.822 0.868 0.886

0.962 0.972 0.978 0.984

0.941 0.907 0.956 0.965

0.832 0.899 0.935 0.941

0.815 0.874 0.919 0.932

0.735 0.800 0.884 0.893

0.833 0.854 0.903 0.917

0.587 0.625 0.725 0.769

ResNet101 ResNet101 ResNet101 ResNet101

× SSCL
CE AGCL

0.909 0.950 0.960 0.965

0.849 0.928 0.933 0.948

0.864 0.935 0.945 0.954

0.859 0.805 0.887 0.901

0.694 0.792 0.822 0.875

0.953 0.969 0.975 0.981

0.909 0.900 0.952 0.962

0.770 0.905 0.920 0.930

0.767 0.877 0.901 0.917

0.713 0.800 0.834 0.876

0.814 0.846 0.877 0.902

0.526 0.602 0.670 0.748

4 Experiments
Datasets: To evaluate our proposed learning approach, one clinical research cohorts, and two publicly available datasets in medical imaging and natural imaging domain are used respectively. [I] The BTCV dataset was comprised of 100 de-identified 3D contrast-enhanced CT scans with 7968 axial slices in total. 20 scans were publicly available for the testing phase in the MICCAI 2015 BTCV challenge. For each scan, 12 organs anatomical structure are well-annotated, including spleen, right kidney, left kidney, gallbladder, esophagus, liver, stomach, aorta, inferior vena cava (IVC), portal splenic vein (PSV), pancreas and right adrenal gland. Each volume consists of 47  133 slices of 512 × 512 pixels, with resolution of ([0.54  0.98] × [0.54  0.98] × [2.5  7.0])mm3. [II] Non-contrast clinical cohort was retrieved in de-identified form from ImageVU database of Vanderbilt University Medical Center. It consists of 56 3D CT scans with 3687 axial slices, capturing in non-contrast phase with human refined annotations for 12 organs as same as the MICCAI 2015 BTCV challenge datasets. Each volume consists of 49  174 slices of 512 × 512 pixels, with resolution of ([0.64  0.98] × [0.64  0.98] × [1.5  5.0])mm3. [III] Pascal VOC 2012 dataset consists of 10582 training images, 1449 validation images and 456 testing images with pixel-level annotations of 20 classes and one class for background.
We evaluate the segmentation performance with Dice similarity coefficient and mean IoU on a number of segmentation benchmarks across both medical domain and natural image domain, including the testing phase of the BTCV dataset, testing cohort of the non-contrast clinical cohort and the validation set of PASCAL VOC 2012 dataset for multi-classes segmentation. We demonstrated the variation of performance with the changes of hyperparameters and the pre-training strategies for encoder network structure. For the encoder network, we evaluated with two common backbone architectures both in medical imaging domain and natural image domain for segmentation: Deeplabv3+ with ResNet-50 and ResNet-101 encoder. The normalized activation of the final pooling layer with DE = 2048 are used as the distinctive feature representation vector. We provide more prepossessing, training and implementation details in the Supplementary Material.
7

Table 2: Comparison of the fully-supervised, semi-sueprvised and partial-supervised state-of-the-arts on the 2015 MICCAI BTCV challenge leaderboard. Our method achieves the best Dice score, mean surface distance and Hausdorff distances. (We show 8 main organs Dice scores due to limited space.)

Method

Spleen R.Kid L.Kid Gall.

Eso.

Liver Aorta

IVC

Average Mean Surface Hausdorff

Dice

Distance

Distance

Cicek et al.[13] Roth et al.[47] Heinrich et al.[24] P awlowski et al.[44] Zhu et al.[71] Lee et al.[35] Y uyin et al.[70]

0.906 0.935 0.920 0.939 0.935 0.959 0.968

0.857 0.887 0.894 0.895 0.886 0.920 0.920

0.899 0.944 0.915 0.915 0.944 0.945 0.953

0.644 0.780 0.604 0.711 0.764 0.768 0.729

0.684 0.712 0.692 0.743 0.714 0.783 0.790

0.937 0.953 0.948 0.962 0.942 0.962 0.974

0.886 0.880 0.857 0.891 0.879 0.910 0.925

0.808 0.804 0.828 0.826 0.803 0.847 0.847

0.784 0.816 0.790 0.815 0.814 0.842 0.850

2.339 2.018 2.262 1.861 1.692 1.501 1.450

15.928 17.982 25.504 62.872 18.201 16.433 18.468

Ours (SSCL) Ours (AGCL)

0.953 0.971

0.922 0.955

0.930 0.963

0.830 0.910

0.822 0.886

0.972 0.984

0.899 0.941

0.874 0.932

0.863 0.923

1.899 0.932

17.073 13.024

Dice loss
Mean Segmentation Performance Mean Dice Similarity Coefficient

Segmentation Models Learning Loss Curve

ResNet50 (No Pretraining) Training Loss

ResNet50 (No Pretraining) Validation Loss

0.5

ResNet50 (Pretrain with SSCL) Training Loss ResNet50 (Pretrain with SSCL) Validation Loss

ResNet50 (Pretrain with CE) Training Loss

ResNet50 (Pretrain with CE) Validation Loss

ResNet50 (Pretrain with AGCL) Training Loss

0.6

ResNet50 (Pretrain with AGCL) Validation Loss

0.7

0.8

0.9

1.0

Epochs

(a)

1.00

1.0

0.95

0.90

0.9

0.85

0.80

0.8

Non-Contrast Pre-training Only

0.75

Multi-Contrast Pre-training

Contrast-Enhanced Pre-training Only

0.70

0.05

0.07 Te0.m1 perat0u.2re 0.25

0.3

0.7

NoSnu-Cbojencttrsast ContrSaustb-jEenchtsanced

(b)

(c)

Figure 5: a) Learning loss curves of specifying stability with/out different pre-training strategies, b) mean segmentation performance as a function of temperature during pre-training stage for AGCL, and c) mean Dice score per subject pre-training with contrast-enhanced dataset only and with multi-contrast phase (Contrastenhanced and non-contrast) dataset.

4.1 Segmentation Performance
We first compare the proposed AGCL with a series of state-of-the-art approaches including 1) fully supervised approaches (train on the ground-truth labeled dataset only), 2) partially-supervised approach (train on one contrast phase dataset, another one with partially labeled), and 3) semisupervised approach (pre-training with self-supervised contrastive loss). As shown from Table 2, the semi-supervised approach demonstrates significant improvement followed by the partially-supervision and fully-supervision approach. Contrastive loss provides an opportunity to initially classify the representation to a self-defined latent space instead of relying on the decoder ability to generate voxel-wise prediction. By further adding modality and anatomical information as supervision, AGCL achieves the best performance among all the supervision state-of-the-arts with a mean Dice score of 0.923. The additional gains demonstrated that the additional imaging information benefits more positive pairs preserving with large number of negative pairs.
Beyond the medical imaging dataset, we performed experiments on the natural image dataset PASCAL VOC 2012 for multi-classes segmentation. From Table 3., AGCL demonstrated substantial improvement of the segmentation performance across the current supervised state-of-the-art and . With the use of cross-entropy loss, AGCL outperforms the orginal DeepLabv3+ methods by 2.75 percent. Interestingly, the improvement is comparatively less than that with cross-entropy by using the RMI loss.
4.2 Ablation Study for AGCL
Comparing with pre-training baselines Here we perform the ablation studies among the three extra supervised and self-supervised pre-training strategies: 1) pre-training with self-supervised contrastive loss (SSCL), 2) pre-training with cross-entropy loss as classification tasks (CE), and 3) random initialization (RI) without any pre-training on both ResNet-50 and ResNet-101 encoder backbone.
8

Table 3: Average segmentation performance in Mean IoU across all class on PASCAL VOC 2012 validation set with state-of-the-arts using ResNet-50 backbone.

Figure 6: Mean Dice score per subject with AGCL using limited label quantity.
1.0

Mean Dice Similarity Coefficient

Architecture
DeepLabv3 [7] DeepLabv3+ [8]
PSP-Net [66] AGCL (Ours)

CE
0.7311 0.7204 0.7301 0.7402

Loss
AAF [29]
0.7234 0.7268 0.7256 0.7456

RMI [67]
0.7539 0.7681 0.7551 0.7722

0.9

0.8

10% Label AGCL

20% Label AGCL

50% Label AGCL

All Label AGCL

0.7

Contrast-Enhanced Subjects

As shown in Table 1, SSCL improves the segmentation performance over RI by 3.12%, which is expected as the contrastive loss helps to define the latent space and adapt normalized intensity variation for extracting distinctive representation. RI consider no constraints in the lower-dimensional space and rely on the decoder ability to reconstruct pixel correspondence from labels. The contrast variation limits the model to adapt independent representation without any pre-training strategies. With the modality and anatomical information added, the supervised CE strategies significant boost the segmentation performance by 5.93%. It demonstrates that a good definition of latent space in encoder can help address the corresponding representation for each semantic target and starts to achieve more favorable with the segmentation task. Eventually, AGCL surpass CE by 1.32% in mean Dice, which provide more generalizable guidance to define and control the separation of independent embeddings with stability comparing with CE.
Comparing with temperature variability We experimented with the variation of temperature to see and find the optimal effect towards the segmentation performance. Figure. 5(b) demonstrate the effect of temperature on the multi-organ segmentation across all subjects in the BTCV testing dataset. We found that low temperature achieves better performance than high temperature, as the radius of the hypersphere defined in the latent space is inversely proportional to the temperature scaling.
Comparing with single/multiple modal pre-training The segmentation performance is also evaluated with single modality pre-training and with multi-modality pre-training. From Figure 5(c), a better segmentation performance for contrast-enhanced dataset is achieved by pre-training with multi-modality images, than pre-training with contrast-enhanced modality only. Interestingly, we observed that the robustness of segmentation for non-contrast dataset is decreased with multi-modal pre-training.
Comparing with reduced label for AGCL In Figure 6, we performed AGCL with the variation of label quantity and compare the segmentation performance by leveraging the mount of label information. We observed that model has the best performance with fully label input. Interestingly, AGCL with 20% labeled has similar performance comparing to that with fully labeled, while AGCL with 50% labeled has a small drop of the segmentation performance. Hard cases may be provided with labels during the random sampling and cause such decrease of performance.
4.3 Limitations
Although AGCL tackles current challenges existing in multi-modality segmentation, limitations still exist in the process of AGCL. One limitation is the dependency on the quality of coarse segmentation. As 2D patches are extracted according to the attention information in each slice, patches without corresponding organ regions may also be possible to extract if the coarse segmentation is inaccurate. Biased information may bring up from the patches and cause incorrect label definition input into the pre-training stage. Another limitation is the object-centric approach for contrastive learning. It is still limited to perform contrastive learning on complete image for pixel-/voxel-wise segmentation task. We plan to consider integrating the segmentation with the contrastive learning theory as an end-to-end setting and adapt complete image in our future work.
9

5 Conclusion
Performing robust semantic segmentation with multi-modality images using deep learning strategies remains a persistent challenge. In this work, we propose a novel hierarchical contrastive framework with extensions of self-supervised contrastive loss and the integration of attention guidance from coarse segmentation model to adapt independent representations from each modality and explain the learned representation extracted with the contrastive loss. Evaluations of our proposed methods are performed and significant gain in segmentation performances are shown in two CT datasets and one natural image datasets. Further, we showed that the power of encoder can be benefits by initially defining the representation space with such contrastive pre-training strategies and increase the explainability of the model performance. Potential future work includes investigating larger scale of multi-modality dataset such as MRI or PET imaging, and include the patients demographics as the contrastive guidance.
Acknowledgments and Disclosure of Funding
This research is supported by NIH Common Fund and National Institute of Diabetes, Digestive and Kidney Diseases U54DK120058, NSF CAREER 1452485, NIH 2R01EB006136, NIH 1R01EB017230, and NIH RO1NS09529. ImageVU and RD are supported by the VICTR CTSA award (ULTR000445 from NCATS/NIH). We gratefully acknowledge the support of NVIDIA Corportaion with the donation of the Titan X Pasacl GPU usage.
References
[1] Araslanov, N., Roth, S.: Single-stage semantic segmentation from image labels. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4253­4262 (2020)
[2] Bachman, P., Hjelm, R.D., Buchwalter, W.: Learning representations by maximizing mutual information across views. arXiv preprint arXiv:1906.00910 (2019)
[3] IEEE International Symposium on Biomedical Imaging Venice, I...t..: Surrogate supervision for medical image analysis: Effective deep learning from limited quantities of labeled data. In: ISBI 2019 : 2019 IEEE 16th International Symposium on Biomedical Imaging : April 8-11, 2019, Hilton Molino Stucky, Venice, Italy /. 2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI), Institute of Electrical and Electronics Engineers,, Piscataway, New Jersey : (2019-4)
[4] Chaitanya, K., Erdil, E., Karani, N., Konukoglu, E.: Contrastive learning of global and local features for medical image segmentation with limited annotations. In: Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.F., Lin, H. (eds.) Advances in Neural Information Processing Systems. vol. 33, pp. 12546­12558. Curran Associates, Inc. (2020), https://proceedings. neurips.cc/paper/2020/file/949686ecef4ee20a62d16b4a2d7ccca3-Paper.pdf
[5] Chandrashekar, A., Handa, A., Shivakumar, N., Lapolla, P., Grau, V., Lee, R.: A deep learning approach to generate contrast-enhanced computerised tomography angiography without the use of intravenous contrast agents. arXiv preprint arXiv:2003.01223 (2020)
[6] Chen, C., Dou, Q., Chen, H., Qin, J., Heng, P.A.: Unsupervised bidirectional cross-modality adaptation via deeply synergistic image and feature alignment for medical image segmentation. IEEE transactions on medical imaging 39(7), 2494­2505 (2020)
[7] Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE transactions on pattern analysis and machine intelligence 40(4), 834­848 (2017)
[8] Chen, L.C., Papandreou, G., Schroff, F., Adam, H.: Rethinking atrous convolution for semantic image segmentation. arXiv preprint arXiv:1706.05587 (2017)
10

[9] Chen, L.C., Zhu, Y., Papandreou, G., Schroff, F., Adam, H.: Encoder-decoder with atrous separable convolution for semantic image segmentation. In: Proceedings of the European conference on computer vision (ECCV). pp. 801­818 (2018)
[10] Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for contrastive learning of visual representations. In: International conference on machine learning. pp. 1597­1607. PMLR (2020)
[11] Chen, X., Yao, L., Zhou, T., Dong, J., Zhang, Y.: Momentum contrastive learning for few-shot covid-19 diagnosis from chest ct images. Pattern Recognition 113, 107826 (2021). https://doi.org/https://doi.org/10.1016/j.patcog.2021.107826, https://www.sciencedirect. com/science/article/pii/S0031320321000133
[12] Chuang, C.Y., Robinson, J., Lin, Y.C., Torralba, A., Jegelka, S.: Debiased contrastive learning. In: Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.F., Lin, H. (eds.) Advances in Neural Information Processing Systems. vol. 33, pp. 8765­8775. Curran Associates, Inc. (2020), https://proceedings.neurips.cc/paper/2020/file/ 63c3ddcc7b23daa1e42dc41f9a44a873-Paper.pdf
[13] Çiçek, Ö., Abdulkadir, A., Lienkamp, S.S., Brox, T., Ronneberger, O.: 3d u-net: learning dense volumetric segmentation from sparse annotation. In: International conference on medical image computing and computer-assisted intervention. pp. 424­432. Springer (2016)
[14] Commandeur, F., Goeller, M., Betancur, J., Cadet, S., Doris, M., Chen, X., Berman, D.S., Slomka, P.J., Tamarappoo, B.K., Dey, D.: Deep learning for quantification of epicardial and thoracic adipose tissue from non-contrast ct. IEEE transactions on medical imaging 37(8), 1835­1846 (2018)
[15] Doersch, C., Gupta, A., Efros, A.A.: Unsupervised visual representation learning by context prediction. In: Proceedings of the IEEE international conference on computer vision. pp. 1422­1430 (2015)
[16] Donath, T., Pfeiffer, F., Bunk, O., Grünzweig, C., Hempel, E., Popescu, S., Vock, P., David, C.: Toward clinical x-ray phase-contrast ct: demonstration of enhanced soft-tissue contrast in human specimen. Investigative radiology 45(7), 445­452 (2010)
[17] Dou, Q., Liu, Q., Heng, P.A., Glocker, B.: Unpaired multi-modal segmentation via knowledge distillation. IEEE transactions on medical imaging 39(7), 2415­2425 (2020)
[18] Dou, Q., Ouyang, C., Chen, C., Chen, H., Glocker, B., Zhuang, X., Heng, P.A.: Pnp-adanet: Plug-and-play adversarial domain adaptation network with a benchmark at cross-modality cardiac segmentation. arXiv preprint arXiv:1812.07907 (2018)
[19] Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A.: The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results. http://www.pascalnetwork.org/challenges/VOC/voc2012/workshop/index.html
[20] Gamechi, Z.S., Bons, L.R., Giordano, M., Bos, D., Budde, R.P., Kofoed, K.F., Pedersen, J.H., Roos-Hesselink, J.W., de Bruijne, M.: Automated 3d segmentation and diameter measurement of the thoracic aorta on non-contrast enhanced ct. European radiology 29(9), 4613­4623 (2019)
[21] Hariharan, B., Arbeláez, P., Bourdev, L., Maji, S., Malik, J.: Semantic contours from inverse detectors. In: 2011 International Conference on Computer Vision. pp. 991­998. IEEE (2011)
[22] He, K., Fan, H., Wu, Y., Xie, S., Girshick, R.: Momentum contrast for unsupervised visual representation learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9729­9738 (2020)
[23] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 770­778 (2016)
[24] Heinrich, M.P., Maier, O., Handels, H.: Multi-modal multi-atlas segmentation using discrete optimisation and self-similarities. VISCERAL Challenge@ ISBI 1390, 27 (2015)
11

[25] Henaff, O.: Data-efficient image recognition with contrastive predictive coding. In: International Conference on Machine Learning. pp. 4182­4192. PMLR (2020)
[26] Hjelm, R.D., Fedorov, A., Lavoie-Marchildon, S., Grewal, K., Bachman, P., Trischler, A., Bengio, Y.: Learning deep representations by mutual information estimation and maximization. arXiv preprint arXiv:1808.06670 (2018)
[27] Huang, Z., Wang, X., Wang, J., Liu, W., Wang, J.: Weakly-supervised semantic segmentation network with deep seeded region growing. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 7014­7023 (2018)
[28] Jamaludin, A., Kadir, T., Zisserman, A., Cardoso, M.J., Arbel, T., Carneiro, G., SyedaMahmood, T., Tavares, J.M.R., Moradi, M., Bradley, A., Greenspan, H., Papa, J.P., Madabhushi, A., Nascimento, J.C., Cardoso, J.S., Belagiannis, V., Lu, Z.: Self-supervised Learning for Spinal MRIs, Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support, vol. 10553. Springer International Publishing :, Cham :, 1st ed. 2017. edn. (2017)
[29] Ke, T.W., Hwang, J.J., Liu, Z., Yu, S.X.: Adaptive affinity fields for semantic segmentation. In: Proceedings of the European Conference on Computer Vision (ECCV). pp. 587­602 (2018)
[30] Khoreva, A., Benenson, R., Hosang, J., Hein, M., Schiele, B.: Simple does it: Weakly supervised instance and semantic segmentation. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 876­885 (2017)
[31] Khosla, P., Teterwak, P., Wang, C., Sarna, A., Tian, Y., Isola, P., Maschinot, A., Liu, C., Krishnan, D.: Supervised contrastive learning. arXiv preprint arXiv:2004.11362 (2020)
[32] Kim, M., Byun, H.: Learning texture invariant representation for domain adaptation of semantic segmentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 12975­12984 (2020)
[33] Lai, B., Wu, Y., Bai, X., Zhou, X.Y., Wang, P., Cai, J., Huo, Y., Huang, L., Xia, Y., Xiao, J., Lu, L., Hu, H., Harrison, A.: Fully-automated liver tumor localization and characterization from multi-phase mr volumes using key-slice roi parsing: A physician-inspired approach (2021)
[34] Landman, B., Xu, Z., Igelsias, J., Styner, M., Langerak, T., Klein, A.: Miccai multi-atlas labeling beyond the cranial vault­workshop and challenge. In: Proc. MICCAI Multi-Atlas Labeling Beyond Cranial Vault--Workshop Challenge (2015)
[35] Lee, H.H., Tang, Y., Bao, S., Abramson, R.G., Huo, Y., Landman, B.A.: Rap-net: Coarse-to-fine multi-organ segmentation with single random anatomical prior. In: 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI). pp. 1491­1494. IEEE (2021)
[36] Lee, J., Kim, E., Lee, S., Lee, J., Yoon, S.: Ficklenet: Weakly and semi-supervised semantic image segmentation using stochastic inference. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5267­5276 (2019)
[37] Liu, W., Ferstl, D., Schulter, S., Zebedin, L., Fua, P., Leistner, C.: Domain adaptation for semantic segmentation via patch-wise contrastive learning (2021)
[38] Misra, I., Maaten, L.v.d.: Self-supervised learning of pretext-invariant representations. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 6707­6717 (2020)
[39] Morris, E.D., Ghanem, A.I., Dong, M., Pantelic, M.V., Walker, E.M., Glide-Hurst, C.K.: Cardiac substructure segmentation with deep learning for improved cardiac sparing. Medical physics 47(2), 576­586 (2020)
[40] Oord, A.v.d., Li, Y., Vinyals, O.: Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018)
[41] Pan, F., Shin, I., Rameau, F., Lee, S., Kweon, I.S.: Unsupervised intra-domain adaptation for semantic segmentation through self-supervision. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 3764­3773 (2020)
12

[42] Papandreou, G., Chen, L.C., Murphy, K.P., Yuille, A.L.: Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation. In: Proceedings of the IEEE international conference on computer vision. pp. 1742­1750 (2015)
[43] Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., Efros, A.A.: Context encoders: Feature learning by inpainting. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2536­2544 (2016)
[44] Pawlowski, N., Ktena, S.I., Lee, M.C.H., Kainz, B., Rueckert, D., Glocker, B., Rajchl, M.: Dltk: State of the art reference implementations for deep learning on medical images (2017)
[45] Raju, A., Cheng, C.T., Huo, Y., Cai, J., Huang, J., Xiao, J., Lu, L., Liao, C., Harrison, A.P.: Co-heterogeneous and adaptive segmentation from multi-source and multi-phase ct imaging data: A study on pathological liver and lesion segmentation (2021)
[46] Ross, T., Zimmerer, D., Vemuri, A., Isensee, F., Wiesenfarth, M., Bodenstedt, S., Both, F., Kessler, P., Wagner, M., Müller, B., Kenngott, H., Speidel, S., Kopp-Schneider, A., MaierHein, K., Maier-Hein, L.: Exploiting the potential of unlabeled endoscopic video data with self-supervised learning. International journal of computer assisted radiology and surgery. 13(6) (2018-6)
[47] Roth, H.R., Shen, C., Oda, H., Sugino, T., Oda, M., Hayashi, Y., Misawa, K., Mori, K.: A multi-scale pyramid of 3d fully convolutional networks for abdominal multi-organ segmentation. In: International conference on medical image computing and computer-assisted intervention. pp. 417­425. Springer (2018)
[48] Seo, M., Kim, D., Lee, K., Hong, S., Bae, J.S., Kim, J.H., Kwak, S.: Neural contrast enhancement of ct image. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 3973­3982 (2021)
[49] Sermanet, P., Lynch, C., Chebotar, Y., Hsu, J., Jang, E., Schaal, S., Levine, S., Brain, G.: Time-contrastive networks: Self-supervised learning from video. In: 2018 IEEE International Conference on Robotics and Automation (ICRA). pp. 1134­1141. IEEE (2018)
[50] Shahzad, R., Bos, D., Budde, R.P., Pellikaan, K., Niessen, W.J., van der Lugt, A., Van Walsum, T.: Automatic segmentation and quantification of the cardiac structures from non-contrastenhanced cardiac ct scans. Physics in Medicine & Biology 62(9), 3798 (2017)
[51] Song, C., Huang, Y., Ouyang, W., Wang, L.: Box-driven class-wise region masking and filling rate guided loss for weakly supervised semantic segmentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 3136­3145 (2019)
[52] Tang, Y., Gao, R., Han, S., Chen, Y., Gao, D., Nath, V., Bermudez, C., Savona, M.R., Bao, S., Lyu, I., et al.: Body part regression with self-supervision. IEEE Transactions on Medical Imaging 40(5), 1499­1507 (2021)
[53] Tian, Y., Krishnan, D., Isola, P.: Contrastive multiview coding. arXiv preprint arXiv:1906.05849 (2019)
[54] Vemulapalli, R., Tuzel, O., Liu, M.Y., Chellapa, R.: Gaussian conditional random field network for semantic segmentation. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 3224­3233 (2016)
[55] Wang, Y., Zhang, J., Cui, H., Zhang, Y., Xia, Y.: View adaptive learning for pancreas segmentation. Biomedical Signal Processing and Control 66, 102347 (2021)
[56] Watanabe, H., Kanematsu, M., Miyoshi, T., Goshima, S., Kondo, H., Moriyama, N., Bae, K.T.: Improvement of image quality of low radiation dose abdominal ct by increasing contrast enhancement. American Journal of Roentgenology 195(4), 986­992 (2010)
[57] Wu, Z., Xiong, Y., Yu, S.X., Lin, D.: Unsupervised feature learning via non-parametric instance discrimination. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 3733­3742 (2018)
13

[58] Yan, W., Wang, Y., Gu, S., Huang, L., Yan, F., Xia, L., Tao, Q.: The domain shift problem of medical image segmentation and vendor-adaptation by unet-gan (2019)
[59] Yi, X., Walia, E., Babyn, P.: Generative adversarial network in medical imaging: A review. Medical image analysis 58, 101552 (2019)
[60] You, Y., Chen, T., Sui, Y., Chen, T., Wang, Z., Shen, Y.: Graph contrastive learning with augmentations. In: Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.F., Lin, H. (eds.) Advances in Neural Information Processing Systems. vol. 33, pp. 5812­5823. Curran Associates, Inc. (2020), https://proceedings.neurips.cc/paper/2020/file/ 3fe230348e9a12c13120749e3f9fa4cd-Paper.pdf
[61] Yuan, Y., Chen, X., Wang, J.: Object-contextual representations for semantic segmentation. arXiv preprint arXiv:1909.11065 (2019)
[62] Zhang, H., Zhang, H., Wang, C., Xie, J.: Co-occurrent features in semantic segmentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 548­557 (2019)
[63] Zhang, L., Wang, X., Yang, D., Sanford, T., Harmon, S., Turkbey, B., Wood, B.J., Roth, H., Myronenko, A., Xu, D., Xu, Z.: Generalizing deep learning for medical image segmentation to unseen domains via deep stacked transformation. IEEE transactions on medical imaging. 39(7) (2020-7)
[64] Zhang, R., Isola, P., Efros, A.A.: Colorful image colorization. In: European conference on computer vision. pp. 649­666. Springer (2016)
[65] Zhang, R., Isola, P., Efros, A.A.: Split-brain autoencoders: Unsupervised learning by crosschannel prediction. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 1058­1067 (2017)
[66] Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J.: Pyramid scene parsing network. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2881­2890 (2017)
[67] Zhao, S., Wang, Y., Yang, Z., Cai, D.: Region mutual information loss for semantic segmentation. arXiv preprint arXiv:1910.12037 (2019)
[68] Zhao, X., Vemulapalli, R., Mansfield, P., Gong, B., Green, B., Shapira, L., Wu, Y.: Contrastive learning for label-efficient semantic segmentation. arXiv preprint arXiv:2012.06985 (2020)
[69] Zhao, X., Vemulapalli, R., Mansfield, P., Gong, B., Green, B., Shapira, L., Wu, Y.: Contrastive learning for label-efficient semantic segmentation (2021)
[70] Zhou, Y., Li, Z., Bai, S., Wang, C., Chen, X., Han, M., Fishman, E., Yuille, A.L.: Prioraware neural network for partially-supervised multi-organ segmentation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (October 2019)
[71] Zhu, Z., Xia, Y., Xie, L., Fishman, E.K., Yuille, A.L.: Multi-scale coarse-to-fine segmentation for screening pancreatic ductal adenocarcinoma. In: International conference on medical image computing and computer-assisted intervention. pp. 3­12. Springer (2019)
14

6 Network Architecture and Training Details
6.1 Network Architecture
We use a deeplabv3+ network structure as the segmentation backbone with ResNet-50 based encoder e, consisting of 16 convolutional blocks in total, following with (each convolutional layer followed with 1 batch normalization layer and 1 activation layer (ReLU)):
· A convolutional layer with kernel size of 7 × 7 with 64 channels and stride size of 2 and a maxpooling layer with stride size of 2.
· 3 convolutional blocks with following 1 × 1 with 64 channels, 3 × 3 with 64 channels and 1 × 1 with 256 channels.
· 4 convolutional blocks with following 1 × 1 with 128 channels, 3 × 3 with 128 channels and 1 × 1 with 512 channels.
· 6 convolutional blocks with following 1 × 1 with 256 channels, 3 × 3 with 256 channels and 1 × 1 with 1024 channels.
· 3 convolutional blocks with following 1 × 1 with 512 channels, 3 × 3 with 512 channels and 1 × 1 with 2048 channels.
During pre-training stage (Stage 2 in Figure 2), we apply a projection network p consisting of two dense layers with output dimension of 2048 and 256. After the encoder is well pre-pretrained with our proposed method, the small network p is discarded and the feature representation computed from the encoder is directly input into atrous spatial pyramid modules, consists of:
· 1 convolutional layers with kernel size of 1 × 1 with 512 channels
· 1 convolutional layers with kernel size of 3 × 3 with 512 channels, dilation rate of 6 and padding rate of 6
· 1 convolutional layers with kernel size of 3 × 3 with 512 channels, dilation rate of 12 and padding rate of 12
· 1 convolutional layers with kernel size of 3 × 3 with 512 channels, dilation rate of 18 and padding rate of 18
· 1 adaptive average pooling layer with size 1 and 1 convolutional layers with kernel size of 1 × 1 with 512 channels
The representation is separately input into each layer and the output from each layer is concatenated to input into a small decoder with two convolutions layers of 1 × 1 kernel size only with 512 channels and the total number of the semantic target classes respectively (No batch normalization and activation layer for the final convolutional output). The final output is bi-linearly upsampled to the same dimension of the input images.
6.2 Training Details
For training with medical imaging dataset, 5-fold cross-validation is performed for both contrastenhanced phase and non-contrast phase CT (Training: 60 volumes (contrast-enhanced) and 44 volumes (non-contrast), validation: 20 volumes (contrast-enhanced) and 6 volumes (non-contrast), and testing: 20 volumes (contrast-enhanced) and 6 volumes (non-contrast)). For the pre-training stage of AGCL, we extract 30 2D patches for each target in each axial slices to ensure that patches are fully covered the region of the corresponding organs with significant variation of anatomical morphology. More than 400k patches with dimensions 128 × 128 are used to train with stochastic gradient descent (SGD) optimizer for 5 epochs with a batch size of 4 and learning rate of 5 × 10-4. We have evaluated the variation of the temperature parameter towards the segmentation performance and T = 0.1 achieved the best performances across all other temperature values. For segmentation task, the encoder's weight is frozen and the decoder with ASPP module is trained for 10 epochs with Adam optimizer with batch size of 4 and learning rate of 10-4. We used the validation set to choose the model with the highest mean Dice score for all semantic targets segmentation and perform testing evaluation as the quantitative representation on the testing set. More implementation details can be refer to the code on github: ****
15

For training with natural imaging dataset, the augmented data with well annotations of [21] are used, resulting with 10582 images for training, 1449 images for validation and 1456 images for testing. A 2D segmentation model is initially trained with Deeplabv3+ network structure using ResNet-50 as the encoder backbone for 23 epochs. SGD optimizer with batch size of 8 and learning rate of 10-3 are used to optimize the training process for the coarse segmentation model. 2D patches are extracted corresponding to the coarse attention output as same as with the medical imaging dataset. The extracted patches are then trained with the same network architecture with that with medical imaging dataset, but with T = 0.07 and color distortions as addition to the data augmentation. For segmentation task, the encoder's weight is also frozen and the decoder with ASPP module is trained with Adam optimizer with batch size of 8 and learning rate of 10-4 for 10 epochs (credit to github: https://github.com/ZJULearning/RMI).
6.3 Details of Training Time
We train all models on NVIDIA 2080 Ti 11GB GPU with Pytorch implementation. The training time for medical imaging dataset is approximately: a) 10 hours for AGCL pre-training (per epoch) and b) 3 hours for training segmentation model (per epoch). For natural imaging dataset, it is approximately: a) 8 hours for coarse segmentation model (complete 23 epochs), b) 6 hours for AGCL pre-training (per epoch), and c) 2 hours for training segmentation model (per epoch).
6.4 Prepossessing
For medical imaging dataset, we apply the prepossessing steps as following: (i) applying soft tissue windowing within the range of -175 to 250 Hu and perform intensity normalization of each 3D volume, v with min-max normalization: (v - v1)/(v99 - v1), where vp denote as the pth intensity percentile in v, and (ii) apply volume-wise cropping in z-axis with body part regression algorithm to extract the abdominal region only for segmentation and ensure the similar field of view between scans [52]. The coarse segmentation is computed with a coarse-to-fine pipeline [35] to ensure the robustness of organ localization and reduce the chance of extracting biased organ information for contrastive pre-training.
For natural imaging dataset, we follow [8, 9] to preprocess the input images to dimensions 513 × 513 for training the coarse segmentation model (with random horizontal flipping and scaling). Color distortion is further added into the data augmentation for AGCL pre-training stage.
7 Additional experiments
7.1 Ablation Studies on non-contrast clinical cohorts

Table 4: Ablation studies of non-contrast dataset on the segmentation performance in various network backbones.

Encoder

Pretrain Spleen R.Kid L.Kid Gall. Eso. Liver Stomach Aorta IVC PSV Pancreas R.A

ResNet50 ResNet50 ResNet50 ResNet50

× SimCLR
CE AGCL

0.960 0.964 0.972 0.982

0.918 0.938 0.952 0.962

0.921 0.946 0.961 0.965

0.754 0.800 0.812 0.834

0.783 0.801 0.859 0.879

0.964 0.969 0.974 0.982

0.950 0.946 0.959 0.967

0.840 0.901 0.934 0.945

0.839 0.869 0.914 0.929

0.691 0.739 0.768 0.790

0.796 0.804 0.838 0.850

0.372 0.386 0.551 0.560

ResNet101 ResNet101 ResNet101 ResNet101

× SimCLR
CE AGCL

0.949 0.957 0.970 0.977

0.893 0.932 0.943 0.956

0.912 0.937 0.955 0.965

0.721 0.775 0.783 0.811

0.739 0.759 0.811 0.870

0.956 0.964 0.972 0.979

0.920 0.931 0.943 0.966

0.768 0.893 0.911 0.922

0.776 0.869 0.897 0.915

0.653 0.674 0.722 0.799

0.772 0.771 0.811 0.842

0.362 0.439 0.501 0.573

16

7.2 State-of-the-arts comparison for non-contrast dataset

Table 5: Comparison of the fully-supervised, semi-supervised and partial-supervised state-of-the-arts on the non-contrast dataset. Our method achieves the best Dice score, mean surface distance and Hausdorff distances. (We show 8 main organs Dice scores due to limited space.)

Method

Spleen R.Kid L.Kid Gall.

Eso.

Liver Aorta

IVC

Average Mean Surface Hausdorff

Dice

Distance

Distance

Cicek et al.[13] Roth et al.[47] Heinrich et al.[24] Zhu et al.[71] Lee et al.[35] Y uyin et al.[70]

0.937 0.940 0.910 0.948 0.954 0.960

0.856 0.890 0.865 0.880 0.874 0.900

0.912 0.923 0.889 0.920 0.928 0.943

0.690 0.701 0.624 0.710 0.701 0.739

0.631 0.724 0.656 0.734 0.753 0.810

0.920 0.948 0.930 0.950 0.958 0.965

0.880 0.878 0.860 0.879 0.897 0.920

0.769 0.770 0.759 0.803 0.794 0.810

0.762 0.771 0.748 0.790 0.798 0.833

4.235 4.668 4.976 3.850 3.481 3.213

27.102 27.183 30.241 24.384 21.733 18.866

Ours (SSCL) Ours (AGCL)

0.957 0.982

0.932 0.962

0.937 0.965

0.800 0.834

0.801 0.879

0.969 0.982

0.901 0.945

0.869 0.929

0.848 0.892

2.567 2.013

15.889 12.315

7.3 Qualitative Representation on Contrast-Enhanced Dataset
Figure 7: Qualitative Representation with different state-of-the-arts pre-training strategies on BTCV testing dataset

17

