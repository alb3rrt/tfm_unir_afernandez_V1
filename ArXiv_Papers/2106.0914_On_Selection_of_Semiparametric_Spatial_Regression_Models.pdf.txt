arXiv:2106.00914v1 [stat.ME] 2 Jun 2021

On Selection of Semiparametric Spatial Regression Models
Guannan Wanga and Jue Wangb
aCollege of William & Mary and bIowa State University
Abstract: In this paper, we focus on the variable selection techniques for a class of semiparametric spatial regression models which allow one to study the effects of explanatory variables in the presence of the spatial information. The spatial smoothing problem in the nonparametric part is tackled by means of bivariate splines over triangulation, which is able to deal efficiently with data distributed over irregularly shaped regions. In addition, we develop a unified procedure for variable selection to identify significant covariates under a double penalization framework, and we show that the penalized estimators enjoy the "oracle" property. The proposed method can simultaneously identify non-zero spatially distributed covariates and solve the problem of "leakage" across complex domains of the functional spatial component. To estimate the standard deviations of the proposed estimators for the coefficients, a sandwich formula is developed as well. In the end, Monte Carlo simulation examples and a real data example are provided to illustrate the proposed methodology. All technical proofs are given in the appendixes.
Key words and phrases: Bivariate splines, Partially linear models, Penalized regression, Semiparametric regression, Spatial data.
1. Introduction
In many economic and geographic data studies, we may have spatially-referenced covariates providing information regarding the spatial distribution which impact the response variable of interest. Meanwhile, many other explanatory variables could be introduced to the model to help explain the response variable. For example, the mortality dataset described in Section 6 consists of aggregated data from each of 3,037 counties in the United States; see Figure 1.1. The explanatory variables contain the county level social, economic and ethnic information that could affect the mortality rate.
To incorporate the spatial information into the regression, there are mainly two kinds of modeling approaches. The first approach adds spatial correlation structure (or weights) to a regression modeling to include spatial information, for example, Leung and Cooley (2014) provided a through comparison of the predictive ability of a traditional geostatistical model with that of a non-traditional Gaussian process model; (Hoshino, 2018; Lee, 2004; LeSage and Pace, 2009; Wall, 2004) studied the spatial autoregressive (SAR) model and the conditional autoregressive (CAR) model; and Nandy et al. (2017) considered the spatially weighted regression (SWR) method. A second approach is based on some
Address for correspondence: Guannan Wang, Department of Mathematics, College of William & Mary, Williamsburg, VA, USA. Email: gwang01@wm.edu

On Selection of Semiparametric Spatial Regression Models

2

< 7.5 7.5 ~ 9 9 ~ 10 10 ~ 11 11 ~ 12.5 > 12.5
Figure 1.1: Mortality rate from 3,037 counties in the U.S.

smoothing techniques, for example, kernel, wavelet or spline smoothing, which uses a deterministic smooth bivariate function to describe the variations and connections among values at different locations; see, for example, Gheriballah et al. (2010), Ramsay (2002), Wood (2003), Strand et al. (2006), Sangalli et al. (2013) and Lai and Wang (2013). In this paper, we take the second approach. We focus on the partially linear spatial model (PLSM) containing both linear components and a nonparametric component with spatial information involved for data distributed over a two-dimensional (2-D) domain.
Suppose there are n location points ranging over a bounded domain   R2 of arbitrary shape. For the ith location point, we observe (Zi, Xi, Yi), where Zi = (Zi1, . . . , Zip) is a p-dimensional vector of the predictor variables. For example, in the mortality data analysis, the vector Z contains socioeconomic and race/ethnicity information such as Gini coefficient, social affluence and proportion of African-American, etc. Variable Xi = (Xi1, Xi2) represents the location (longitude and latitude), and Yi is the response variable of interest. We consider the following semiparametric regression model

Yi = Zi  +  (Xi) + i, i = 1, . . . , n,

(1.1)

where  = (1, . . . , p) are unknown parameters and (·) is some unknown but smooth bivariate function. When (·) is a univariate function, this model is the traditional partially linear model (PLM), and its estimation and theoretical properties have been well studied; see, for example, Huang et al. (2007), He et al. (2014) and Brown et al. (2016). Following the nonparametric smoothing approach, the random noises, i's, are assumed to be i.i.d with E ( i) = 0 and Var ( i) = 2, and each i is independent of Xi and Zi.
While it may be desirable to include many predictors in the model, there are practical constraints that limit the feasibility of such an approach. For example, one problem that arises when analyzing high dimensional data is the computation efficiency. Increasing model sparsity enforces a lower dimensional model structure; thus the model can be estimated more efficiently. In addition, it makes inference more

On Selection of Semiparametric Spatial Regression Models

3

tractable, models easier to interpret, and leads to more robustness against noise. Variable selection has been well studied in the partially linear model (PLM) literature with univari-
ate functional component (·); see Bunea and Wegkamp (2004); Liang and Li (2009); Xie and Huang (2009) and among others. When X is bivariate or multivariate, existing variable selection methods have been largely limited to the additive model (AM) or partially linear additive model (PLAM) which approximates the surface by a combination of an additive collection of univariate functions; see, for example, Lian (2012); Lian et al. (2014); Liu et al. (2011); Ma et al. (2013); Ma and Yang (2011); Wang et al. (2011). However, many spatial studies showed that the additive assumption is questionable in the two-dimensional (2-D) domain.
Traditional bivariate smoothing tools require that data distributed nicely on a rectangular domain. However, the domain over which variables of interest are defined in many of the spatial data is often found to be irregular and complicated. It is challenging to achieve variable selection for irregularly spaced spatial data distributed over complex domains, and the challenges include (i) how to identify those important covariates in Z, (ii) how to estimate the bivariate function (·) ranging over some irregular 2-D domains, (iii) how to deal with unevenly distributed data with observations dense at some locations while sparse at others, (iv) how to make the estimation and selection both computationally efficient and theoretically reliable.
To approximate the bivariate (·), many of the approaches involve tensor product estimation. However, the structure of tensor products is most useful when the data are observed in a rectangular domain, and is undesirable when data are located in spatial domains with complex boundary features such as the domain of the US; see Figure 1.1. Many conventional smoothing tools (kernel smoothing, wavelet smoothing and tensor product splines) suffer from the problem of "leakage" across the complex domains, which refers to the poor estimation over difficult regions by smoothing inappropriately across boundary features; see more discussions in Ramsay (2002) and Wood et al. (2002).
To this end, we aim to address questions (i)-(iv). To deal with the irregular domain problem in (ii), Sangalli et al. (2013) proposed to use finite element analysis, a method mainly developed and used to solve partial differential equations Wood et al. (2002) proposed the soap film smoothing method. Although their method is useful in many practical applications, the theoretical properties of the smoothing were not studied in their paper. In this paper, we will approximate the nonparametric function (·) using bivariate splines, i.e., smooth piecewise polynomial functions, over triangulations (Lai and Schumaker, 2007). This method solves the problem of "leakage" across the complex domains, and it does not require constructing finite elements or locally supported basis functions. It is also computationally efficient, and provides excellent approximation properties (Lai and Wang, 2013), thus, it can handle part of the challenges mentioned in (iv).
To further meet the challenges in (i), (iii) and (iv), we incorporate a variable selection mechanism into the PLSM and propose a double penalized least squares approach based on bivariate spline approx-

On Selection of Semiparametric Spatial Regression Models

4

imation over the spatial domain. Roughness penalty based on the second-order derivatives is employed to regularize the spline fit, and shrinkage penalty on parametric components is used to achieve the variable selection. When we have regions of sparse data, penalized splines provide a more convenient tool for data fitting than the unpenalized splines. We demonstrate that the estimator possesses the "oracle" property in the sense that it is as efficient as the estimator when the true model is known prior to statistical analysis. We also propose a coordinate descent based algorithm to perform the variable selection efficiently.
The rest of this article is organized as follows. In Section 2, we first introduce the triangulations and bivariate spline spaces, then we propose the double-penalized least squares method for joint variable selection and model estimation, and define the penalized estimator (, ). In Section 3 , we further study the asymptotic properties of the estimator  including the consistency and "oracle" property, as well as the rate of convergence of . In Section 4, we discuss some implementation details such as how to select the tuning parameters. Sections 5 and 6 present simulation results and a real data application of the mortality data. Section 7 concludes the paper with a discussion of related issues. Technical details are provided in the appendixes.

2. Methodology
We approximate the function (·) by bivariate splines defined over a 2D triangulated domain. In the following, we use  to denote a triangle which is a convex hull of three points not located in one line. A collection = {1, . . . , K} of K triangles is called a triangulation of  = Kk=1k provided that if a pair of triangles in intersect, then their intersection is either a common vertex or a common edge. See Figures 5.2 and 5.4 for some examples of triangulations.
Various packages have been developed for triangulation; see for example, the "Delaunay" algorithm (MATLAB program delaunay.m or MATHEMATICA function DelaunayTriangulation), the "Triangle" (http://www.cs.cmu.edu/~quake/triangle.html) by Shewchuk (1996), and the "DistMesh" (http://persson.berkeley.edu/distmesh).
2.1. Penalized spline estimators
For a nonnegative integer r, let Cr() be the collection of all r-th continuously differentiable functions over . Given a triangulation , let Srd( ) = {s  Cr() : s|  Pd( ),   } be a spline space of degree d and smoothness r over triangulation , where s| is the polynomial piece of spline s restricted on triangle  , and Pd is the space of all polynomials of degree less than or equal to d. It has been proved in Lai and Schumaker (2007) that for a fixed smoothness r  1, the spline space Sr3r+2( ) achieves the optimal rate of convergence for noise-free datasets, thus, for notation simplicity, we let S = Sr3r+2( ). Given a  > 0 and {(Zi, Xi, Yi)}ni=1, we consider the following minimization

On Selection of Semiparametric Spatial Regression Models

5

problem:

1n

21

min sS 2 i=1

Yi - Zi  - s (Xi)

+ E(s), 2

(2.1)

where

E(s) =


2 2

2

2

2 2

x21 s

+2

s

x1x2

+ x22 s

dx1dx2.

We use Bernstein basis polynomials to represent the bivariate splines. Let {B}K be the set

of degree-d bivariate Bernstein basis polynomials for S constructed in Lai and Schumaker (2007),

where K stands for an index set of K Bernstein basis polynomials. Then we can write the function

s(x) = K B(x) = B(x) , where  = (,   K) is the spline coefficient vector. To meet the smoothness requirement of the splines, we need to impose some constraints on the spline

coefficients. Denote H the constraint matrix on the coefficients , which depends on r and the structure

of the triangulation and enforces smoothness across shared edges of triangles. A simple example of H

is given in Zhou and Pan (2014). The smoothness conditions are linear, and can be written as H = 0.

Let Y = (Y1, . . . , Yn) be the vector of n observations of the response variable. Denote by Xn×2 = {(Xi1, Xi2)}ni=1 the design matrix of the locations and Zn×p = {(Zi1, . . . , Zip)}ni=1 the collection of all covariates. Denote by B the n × K evaluation matrix of Bernstein basis polynomials

whose i-th row is given by Bi = {B(Xi),   K}. Then the minimization problem in (2.1) reduces

to

1 min

Y - Z - B 2 +  P

subject to H = 0,

(2.2)

, 2

where P is the diagonally block penalty matrix satisfying that  P = E(B).

To solve the constrained minimization problem (2.2), we first remove the constraint via a QR

decomposition of the transpose of matrix H and convert the problem to a conventional penalized re-

gression problem without any restriction. More specifically, we assume H

= QR = (Q1 Q2)

R1 R2

,

where Q is an orthogonal matrix and R is an upper triangle matrix; the submatrix Q1 is the first rH

columns of Q, where rH is the rank of matrix H, and R2 is a matrix of zeros. We reparameterize using

 = Q2 for some , and it has been proved in Wang et al. (2018) that after the reparameterization

H is guaranteed to be 0. Then the problem (2.2), is now changed to

min
,

1 2

Y - Z - BQ2

2

+

 2 (Q2)

P(Q2)

.

(2.3)

2.2. Doubly penalized spline estimators

Note that for any fixed , the minimizer of (2.3) with respect to  is

-1
(; ) = Q2 (B B + P)Q2 Q2 B (Y - Z),

(2.4)

On Selection of Semiparametric Spatial Regression Models

6

Replacing  by (; ) in (2.3), we define

1 L()  L(; ) =
2

Y - Z - BQ2(; )

2

+

 2 {Q2(;

)}

P{Q2(; )}

1

= (Y - Z) 2

{I - HB()}(Y - Z),

(2.5)

where

-1
HB() = BQ2 Q2 (B B + P)Q2 Q2 B .

(2.6)

To achieve the simultaneous estimation of the bivariate function (·) and the selection of important covariates, we propose a double-penalized least squares method via minimizing

p
R(; 1, 2) = L(; 1) + n p2(|j|),
j=1

(2.7)

where 1 and 2 are tuning parameters. The first penalty term in (2.7) penalizes the roughness of the nonparametric fit (·) and the second penalty is the shrinkage penalty which shrinks small components
of the linear estimates to zero. Various penalty functions have been used in the literature of variable selection for regression models. For example, the LASSO penalty, p2(||) = 2||, the Adaptive LASSO (ALASSO) penalty in Zou (2006) is given by p2() = 2w|| for a known data-driven weight w, and the smoothly clipped absolute deviation (SCAD) penalty in Fan and Li (2001). In this
paper, we consider the SCAD penalty defined below:

p2 () = 2

I (



2)

+

(a2 - )+ I( (a - 1)2

>

2)

,

for some a > 2 and  > 0 and a = 3.7 is used as suggested in Fan and Li (2001). The SCAD-penalized estimator of the coefficient  is then defined as follows:  = arg minRp R(; 1, 2),
and the bivariate spline estimator of (x) is

-1
(x) = B(x) Q2 Q2 (B B + 1P)Q2 Q2 B (Y - Z).

(2.8)

3. Asymptotic Results

In this section, we study the asymptotic properties of the SCAD-penalized partially linear bivariate

spline estimator (, ). We first introduce some notation. For any function f over the closure of

domain , denote f  = supx |f (x)| the supremum norm of function f over , and denote

|f |, = maxi+j=



 xi1


xj2

f

(x1

,

x2

)

the maximum norm of all the th order derivatives of f over


. Let

W ,() = {f on  : |f |k, < , 0  k  }

(3.1)

On Selection of Semiparametric Spatial Regression Models

7

be the standard Sobolev space. For any j = 1, . . . , p, let zj be the coordinate mapping that maps z to its jth component so that zj(Zi) = Zij, and let

hj = argminhL2

zj - h

2 L2

=

argminhL2 E{Zij

-

h(Xi)}2

(3.2)

be the orthogonal projection of zj onto L2.

3.1. Assumptions

Given a triangle   , let | | be its longest edge length, and  be the radius of the largest disk which can be inscribed in  . Define the shape parameter of  as the ratio  = | |/ . When  is small, the triangle is relatively uniform in the sense that all angles are relatively the same. Denote the size of by | | := max{| |,   }, i.e., the length of the longest edge of .
Before we state the results, we make the following assumptions: Assumption 1. The covariates Zij are bounded uniformly in i = 1, . . . , n, j = 1, . . . , p. Assumption 2. The eigenvalues of E{(1 Zi ) (1 Zi )|Xi} are bounded away from 0. Assumption 3. The noise satisfies that lim E 2I( > ) = 0. Assumption 4. The bivariate functions hj(·), j = 1, . . . , p, and the true function in model (1.1), (·)  W +1,(), in (3.1) for an integer  2. Assumption 5. The joint density of X = (X1, X2) is bounded away from zero and infinity. Assumption 6. The triangulation is -quasi-uniform, that is, there exists a positive constant  such that the triangulation satisfies   , for all   . Assumption 7. The number of the triangles K and the sample size n satisfy that K = Cn for some constant C > 0 and 1/( + 1)    1/3. Assumption 8. The roughness penalty parameter 1 satisfies 1 = o(n1/2K-1).
Assumptions 1­3 are typical in semiparametric smoothing literature, see for instance, Huang et al. (2007) and Wang et al. (2011). The purpose of Assumption 2 is to ensure that the covariate vector Z is not multi-collinear. Assumption 4 describes the requirement for the true bivariate function as usually used in the literature of nonparametric or semiparametric estimation; see Lai and Wang (2013). Assumptions 5­6 require that the partition is quasi-uniform, and suggest that we should not put too few or too many observations in one triangle. Assumption 7 requires that the number of triangles is above some minimum depending upon the degree of the spline, which is similar to the requirement of Li and Ruppert (2008) in the univariate case. Assumption 8 is required to reduce the bias of the bivariate spline approximation through "under smoothing" and "choosing smaller roughness penalty".

3.2. Sampling properties for the penalized estimators
We next show that with a proper choice of 1 and 2, the penalized estimator  has an "oracle" property. To avoid confusion, let 0 and 0 be the true parameter value and function in model (1.1).

On Selection of Semiparametric Spatial Regression Models

8

Let q be the number of nonzero components of 0. Let 0 = (10, · · · , p0) = (10, 20) , where 10 is assumed to consist of all q nonzero components of 0, and 20 = 0 without loss of generality. Then 1 and 2 are the corresponding estimators. In a similar fashion to , we write Z = (Z1, Z2), and Z = (Z1, Z2), where

Z1 = {h1(Xi), . . . , hq(Xi)}ni=1 , Z2 = {hq+1(Xi), . . . , hp(Xi)}ni=1

(3.3)

with hj(·) defined in (3.2). Next we denote an,2 = max1jp{|p2(|j0|)|, j0 = 0}, bn,2 = max1jp{|p2 (|j0|)|, j0 = 0}.
THEOREM 3.1. Under Assumptions 1­8, and if an,2  0 and bn,2  0 as n  , then there exists a local solution  in (2.7) such that  - 0 = OP (n-1/2 + an,2).

Next we define n,2 = {p2(|10|)sgn(10), · · · , p2(|q0|)sgn(q0)} and a diagonal matrix 2 = diag{p2(|10|), · · · , p2(|q0|)}. The theorem below shows that under regularity conditions, all the covariates with zero coefficients can be detected simultaneously with probability tending to one,

and the estimators of all the nonzero coefficients are asymptotically normally distributed.



THEOREM 3.2. Under Assumptions 1­8, if limn n2  , and

lim infn

lim infk0+

-2 1p2 (|k|)

>

0,

then

the

 n-consistent

estimator



in

Theorem

3.1

sat-

isfies P (2 = 0)  1, as n  , and

 n(s + 2 )

1 - 10 + (s + 2 )-1n,2

 N(0, 2s),

where with Z1 given in (3.3).

s = -2E[(Z1 - Z1)(Z1 - Z1) ]

(3.4)

The next result provides the global convergence of the nonparametric estimator (·).

COROLLARY 1. Suppose Assumptions 1­8 hold, then the bivariate penalized estimator (·), given in (2.8), is consistent with the true function, 0, and satisfies that

 - 0 L2 = OP

n

1 |

|3

|0|2,

+

1

+

n

1 |

|5

|

|

+1|0|

,

+

1 n|

|

.

This is a direct result from Wang et al. (2018), thus the proof is omitted.

4. Implementation

Since the SCAD penalty function is singular at the origin, and it does not have continuous second order derivatives. To solve the minimization problem in (2.7), one can locally approximate it by a

On Selection of Semiparametric Spatial Regression Models

9

quadratic function (Fan and Li, 2001; Lian, 2012), then the minimization problem of R(; 1, 2) can be solved using quadratic minimization. However, employing the local quadratic approximation can be extremely expensive since it requires the repeated factorization of large matrices repeatedly for different smoothing parameters. In addition, quadratic minimization is not able to provide naturally sparse estimates. In the implementation of our method, we consider the use of the coordinate descent algorithm (Breheny and Huang, 2015), which fits the penalized regressions more stably and efficiently.
The classical coordinate descent algorithm deals with the optimization problem with one tuning parameter, and there are several ways to address the double-penalization. A natural idea is to solve the optimization problem by searching over a 2D grid for tuning parameters, which can be computationally expensive. We propose the following algorithm based on coordinate descent:

Step 0.

Obtain  by minimizing objective function w.r.t.

:

1 2

Y - BQ2

2 + 0(Q2)

P(Q2)

with 0 selected via GCV, and obtain Y = BQ2 and Z = HB(0)Z;

Step 1.

Obtain



by

minimizing

objective

function

w.r.t.

:

1 2

Y -Y -(Z-Z)

2+n

p j=1

p2

(|j

|)

with 2 selected via BIC;

Step 2. Let Z be the selected covariates from Step 1. Based on data {(Zi , Xi, Yi)}ni=1 refit model (1.1) to obtain  and  by minimizing the following objective function w.r.t.  and : Y - Z -
BQ2 2 + 1(Q2) P(Q2).

4.1. Standard error formula
The standard errors for the estimated parameters can be obtained directly because we are estimating parameters and selecting variables at the same time. Note that for any 1 and 2 the fitted values at the n data points are Y = Z + BQ2() = S(1, 2)Y, where () is given in (2.4). Therefore, the smoothing or hat matrix can be written as

S(1, 2) = Z - Z BQ2

× {(Z - Z) (Z - Z) + n2()}-1

0

0

{Q2 (B B + 1P)Q2}-1

Z -Z ,
Q2 B

where Z = HB(1)Z and 2()  diag p2(|1|)/|1|, . . . , p2(|p|)/|p| . Finally, we derive a sandwich formula for the standard error of 

Cov() =2 ×

-1
(Z - Z) (Z - Z) + n2() (Z - Z) (Z - Z)
-1
(Z - Z) (Z - Z) + n2() ,

On Selection of Semiparametric Spatial Regression Models

10

where 2 = Y - Y 2/{n - tr(S(1, 2))}. Applying conventional techniques that arise in the bivariate splines setting, we can show that the above sandwich formula is a consistent estimator and has good accuracy in our simulation study for moderate sample sizes.

5. Simulation

In this section, we conduct Monte Carlo simulation studies to evaluate the finite-sample performance of the proposed doubly-penalized method in terms of both model estimation and variable selection. We compare our method (PLSM) with the spatial weighted regression method (SWR) proposed by Nandy et al. (2017) and linear model method (LM).

5.1. Example 1

In this example, we consider a modified horseshoe shaped domain  with the surface test function used by Wood et al. (2002). First, we generated 80×180 grid points over the domain. Then, for 100 Monte Carlo experiments, we randomly sample n grid points on  with n = 100 or 200. The response variable Yi's are generated from the following PLSM: Yi = Zi  + (Xi) + i, i = 1, . . . , n, where the true coefficients are  = (1, -1, 0, 0, 0, 0, 0, 0) and i, i = 1, . . . , n are generated independently from N (0, 2) with  = 0.2. Figure 5.1 (a) and (b) show the surface plot and the contour map of the true function (·), respectively. Note that the design of the function (·) makes it hard to have a linear approximation or nonlinear additive approximation of (·) on a rectangular domain. As a result, many traditional parametric and nonparametric methods do not work well in this case.

4

3 -3

2 -2

1 -1

5
0
-5 1
0

4

0.8
3

0.6
2

0.4
1

0.2

0

0

0

-1
-0.2

-2 -0.4

-3

-0.6

-1 -1

0

1

2

3

4

-4

-0.8

-0.5

0

0.5

1

1.5

2

2.5

3

(a)

(b)

-4

Figure 5.1: Example 1. (a) true function of (·); (b) contour map of true function (·).

In practice, some covariates may vary over space, that is, they may be correlated with spatial

locations. To study the performance of variable selection at different correlation levels, similar as in

Wang

et

al.

(2018),

we

generate

the

covariates

as

follows:

Zi1

=

-

2 3

arctan 

 Xi1
Xi2

+

(1

-

)Ui

,

On Selection of Semiparametric Spatial Regression Models

11

Zi3 = cos 

 Xi1
Xi2

+

(1

-

)Ui

, Zij  Uniform(-1, 1), j = 2, 4, . . . , 8, Ui  Uniform(-1, 1). In

particular, we consider the following three cases: (i) low correlation ( = 0.3); (ii) medium correlation

( = 0.5); and (iii) high correlation ( = 0.7).

Figure 5.2 (a) demonstrates the sampled location points of replicate 1. For the bivariate spline

approximation, we consider three different triangulations on the horseshoe domain with (i) 90 triangles

and 74 vertices; (ii) 158 triangles and 114 vertices; and (iii) 286 triangles and 186 vertices as illustrated

in Figure 5.1 (b)­(d), respectively.

Columns 4-6 in Table 5.1 report the average number of two nonzero coefficients incorrectly set

to zero (denoted as "F"), the average number of six zero coefficients correctly set to zero (denoted as

"T"), and how often a correct model is chosen among 100 replications (denoted as "C"). We compare

the sparse PLSM (S-PLSM) estimator with the "oracle" estimator (ORACLE), the estimator when the

true model is known prior to statistical analysis. In this example, the ORACLE is calculated using

triangulation 2. We also compare the S-PLSM with the sparse spatially weighted regression method (S-SWR) proposed by Nandy et al. (2017). From Table 5.1, one sees that, the proposed method per-

forms very well regardless of the level of correlation, and the "F", "T" and "C" are very close to the

ORACLE. However, the S-SWR is very sensitive to the correlation level between the covariates and

spatial locations. When some of the covariates are highly correlated with the spatial locations, the

correct selection rate of the S-SWR is low, especially when the sample size is small. The S-PLSM se-

lection results also indicate that the number of triangles has little effect on the performance of variable

selection.

Next, to see the accuracy of the estimators, we compute the root mean squared error (RMSE)

for each of the estimators based on 100 Monte Carlo samples and compare them with the ORACLE

estimator. Columns 7-9 in Table 5.1 show the RMSEs of the estimate of the parameters 1, 2 as well as the nonlinear function (·). In general, the table clearly indicates that the proposed method

estimates unknown parameters and function very well even when the correlation is high. Regardless of

the choice of triangulation, the S-PLSM with the SCAD penalty always provides accurate estimators

in the sense that they are very close to the "ORACLE". Figure 5.3 shows the estimator of (·) using

different triangulations with the SCAD penalty for a typical data with n = 200 observations generated

from different correlation levels. The proposed PLSM estimator looks globally close to the true surface

regardless of the  used.

Next we test the accuracy of the standard error estimation in (3.4) for 1 and 2. All the results

based on triangulation 2 are listed in Table 5.2. The standard deviations of the estimated parameters computed based on 100 simulations are treated as the true standard errors (column labeled "SEmc").

Then we compared the mean and median of the 100 estimated standard errors calculated using (3.4)

(columns labeled "SEmean" and "SEmedian") with SEmc. The column labeled "SEmad" is the interquar-

tile range of the 100 estimated standard errors divided by 1.349. It can be used as a robust estimate of

On Selection of Semiparametric Spatial Regression Models

1

0.8

0.6

0.4

0.2

0

-0.2

-0.4

-0.6

-0.8

-1

-1

-0.5

0

0.5

1

1.5

2

2.5

3

3.5

(a)

1

0.8

0.6

0.4

0.2

0

-0.2

-0.4

-0.6

-0.8

-1

-1

-0.5

0

0.5

1

1.5

2

2.5

3

3.5

(c)

1

0.8

0.6

0.4

0.2

0

-0.2

-0.4

-0.6

-0.8

-1

-1

-0.5

0

0.5

1

1.5

2

2.5

3

3.5

(b)

1

0.8

0.6

0.4

0.2

0

-0.2

-0.4

-0.6

-0.8

-1

-1

-0.5

0

0.5

1

1.5

2

2.5

3

3.5

(d)

12

Figure 5.2: Example 1. (a) sampled location points of replicate 1; (b) 1 over the domain; (c) 2 over the domain and (d) 3 over the domain.

4

2 3 -4

4

0 2 3
-4 0

 = 0.3  = 0.5  = 0.7

1

2

3

0.8

1

0.6

0.4

0.2
0 0

-0.2

-0.4

-2

-1

-0.6

-0.8

-3

-0.5

0

0.5

1

1.5

2

2.5

3

2

3

0.8

1

0.6

0.4

3

2

0.2
0 0

0

-0.2

-0.4

-2

-1

-0.6

-3

-0.8

-0.5

0

0.5

1

1.5

2

2.5

3

2

3

0.8

1

0.6

0.4

3

2

0.2 0
0

0

-0.2

-0.4

-2

-0.6

-3

-1

-0.8

-0.5

0

0.5

1

1.5

2

2.5

3

-4

4

4

-4

4

2

0.8

3

1

2

0.6

0.4

0.2

0

0

-0.2

0

-0.4

-3

-2

-1

-0.6

-0.8

-0.5

0

0.5

1

1.5

2

2.5

3

0.8

1

3

2

0.6

0.4

0.2

0

0

-0.2

0

-0.4

0

-3

-2

-1

-0.6

-0.8

-0.5

0

0.5

1

1.5

2

2.5

3

0.8

1

3

2

0.6

0.4

0.2 0
0

-0.2

0

-0.4

0 -1

-3

-2

-0.6

-0.8

-0.5

0

0.5

1

1.5

2

2.5

3

4

4 -4

4

-4

-4

3

2

3

0.8

1

0.6

0.4

0.2
0 0

0

-0.2

-0.4

-2

-1

-0.6

-0.8

-3

-0.5

0

0.5

12

1.5

23

2.5

3

0.8

1

0.6

0.4

3

2

0.2
0 0

0

-0.2

-0.4

-2

-1

-0.6

-3

-0.8

-0.5

0

0.5

1

1.5

2

2.5

3

2

3

0.8

1

0.6

0.4

3

2

0.2
0 0

0

-0.2

-0.4

-2

-0.6

-3

-1

-0.8

-0.5

0

0.5

1

1.5

2

2.5

3

-4

4

-4

Figure 5.3: Example 1. estimated functions using different triangulations when n = 200.

On Selection of Semiparametric Spatial Regression Models

13

Table 5.1: Example 1. model selection and estimation results.



n

Method ORACLE

Selection

F

T

C

0.00 6.00 100

1 0.103

RMSE 2
0.041

(·) 0.137

S -SWR

0.39 5.81 48 0.823 0.416

­

100 S-PLSM- 1 0.00 5.86 87 0.082 0.049 0.125

S-PLSM- 2 0.00 5.94 95 0.107 0.041 0.138

0.3

S-PLSM- 3 0.00 5.86 89 0.085 0.049 0.126

ORACLE

0.00 6.00 100 0.066 0.027 0.104

S -SWR

0.00 5.95 96 0.507 0.419

­

200 S-PLSM- 1 0.00 5.90 95 0.052 0.032 0.097

S-PLSM- 2 0.00 5.98 98 0.066 0.027 0.104

S-PLSM- 3 0.00 5.90 95 0.052 0.032 0.096

ORACLE

0.00 6.00 100 0.095 0.041 0.132

S -SWR

0.87 5.91

9

0.999 0.420

­

100 S-PLSM- 1 0.00 5.89 90 0.099 0.042 0.136

S-PLSM- 2 0.00 5.87 90 0.095 0.041 0.132

0.5

S-PLSM- 3 0.00 5.82 86 0.117 0.042 0.148

ORACLE

0.00 6.00 100 0.066 0.028 0.104

S -SWR

0.32 5.80 50 0.814 0.424

­

200 S-PLSM- 1 0.00 5.98 98 0.055 0.032 0.099

S-PLSM- 2 0.00 5.95 97 0.066 0.028 0.104

S-PLSM- 3 0.00 5.92 96 0.055 0.032 0.098

ORACLE

0.00 6.00 100 0.132 0.041 0.161

S -SWR

0.90 5.92

8

1.001 0.420

­

100 S-PLSM- 1 0.00 5.86 89 0.141 0.048 0.164

S-PLSM- 2 0.00 5.83 89 0.159 0.041 0.179

0.7

S-PLSM- 3 0.00 5.89 92 0.154 0.049 0.173

ORACLE

0.00 6.00 100 0.076 0.027 0.110

S -SWR

0.70 5.93 25 1.129 0.418

­

200 S-PLSM- 1 0.00 5.95 96 0.077 0.031 0.110 S-PLSM- 2 0.00 5.99 99 0.076 0.027 0.110 S-PLSM- 3 0.00 5.94 95 0.075 0.031 0.108

On Selection of Semiparametric Spatial Regression Models

14

Table 5.2: Example 1. standard error estimates of the coefficients using S-PLSM- 2.



1

SEmc SEmean SEmedian SEmad

SEmc

2 SEmean SEmedian

SEmad

0.3 0.0646 0.0485 0.0483 0.0036

0.0264 0.0243 0.0242 0.0013

0.5 0.0578 0.0579 0.0579 0.0065

0.0286 0.0243 0.0243 0.0015

0.7 0.0660 0.0640 0.0618 0.0106

0.0273 0.0243 0.0243 0.0015

Table 5.3: Example 2. model selection and estimation results.

Method ORACLE

Selection

F

T

C

0.00

6.00

100

RMSE

1

2

0.0600

0.0500

S -LM

0.00

5.81

89

0.1230

0.0884

S -SWR

0.00

6.00

100

0.0796

0.0635

S -PLSM

0.00

5.98

98

0.0600

0.0500

the standard deviation. Table 5.2 confirms the accuracy of the proposed standard error formula.
5.2. Example 2
In this example, we consider the case that the random noises are spatially correlated. Following Nandy et al. (2017), we consider a rectangle domain with 20×20 lattice grid points, and then, for each of the 100 Monte Carlo experiments, we randomly sample n = 100 grid points. The response variable Yi's are generated from the following model: Yi = Zi  + i, i = 1, . . . , n, where  = (1, -1, 0, 0, 0, 0, 0, 0) and  is generated from a stationary gaussian process with mean zero. All the covariates are generated independently from N (0, 1).
We compare the selection and estimation performance of the S-PLSM with the S-SWR and the sparse linear model (S-LM). For S-SWR, we calculate the weight matrix using the gaussian covariance structure. The model selection and estimation results are summarized in Table 5.3. As expected, when the true error structure follows a stationary gaussian process, the S-SWR performs perfect and the selection is 100% correct. The linear model cannot capture the error structure in this scenario and it tends to increase false positive rate. However, the proposed S-PLSM method still performs really well in this case, and the correct selection rate achieves 98%, which demonstrates that our method is pretty robust in presence of spatial dependence.
5.3. Example 3
We conduct another simulation study using the covariates and domain of the data from the mortality

On Selection of Semiparametric Spatial Regression Models

15

Table 5.4: Example 3. model selection and estimation results

Selection

RMSE

RMSPE

Method

F TC

ORACLE 0.00 7.00 100

Affluence Disadvantage ViolentCrime Urban

(·)

4

5

6

9

0.034

0.020

0.014

0.013 0.183

Y 0.766

S-LM 0.45 3.03 0

0.049

0.091

0.110

0.080 ­ 0.860

S-SWR 0.08 5.82 60

0.025

0.022

0.031

0.026 ­ 0.862

S-PLSM 0.06 6.87 86

0.034

0.020

0.021

0.015 0.184 0.796

­ indicates the measurement is not applicable.

analysis described in Section 6. Specifically, we generate the response variable Yi from the following PLSM:
Yi = Zi  + (Xi) + i, i = 1, . . . , n,

where Zij, j = 1, . . . , 11, are the same as the explanatory variables used in the mortality data, the true

j's and (·) are set to be the same as the estimates obtained by PLSM with the SCAD penalty. The random error, i, i = 1, . . . , n, are generated independently from N (0, 2) distribution, where 2 is

the variance estimate of the measurement error obtained from the mortality data.

We fit an S-PLSM and an S-SWR with the SCAD penalty for the simulated dataset, where the

triangulation used for the S-PLSM is given in Figure 5.4. To see the effect of model misspecifiation on

selection, we also consider a S-LM with the SCAD penalty without including the spatial information.

We repeat the generation and fitting procedures 100 times.

The variable selection and the parameter estimation results are summarized in Table 5.4. From this

table, we find that the number of covariates selected is much larger than the true number of nonzero

components when the misspecified LM is used. The S-SWR outperforms slightly the S-LM in terms

of the "F" and "T" values. However, the S-PLSM has comparable performance with the ORACLE, and

it performs much better than the S-LM and the S-SWR.

The last column in Table 5.4 provides the 10-fold cross-validation RMSPE for the response vari-

able, defined as n-1

10 m=1

1/2
im(Yi - Yi)2 over the 100 replications, where 1, . . . , 10 com-

prise a random partition of the dataset into 10 disjoint subsets of equal size. The cross-validation RM-

SPE shows the superior performance of the S-PLSM as it provides more accurate predictions compared

with the S-LM though it includes fewer explanatory variables than the S-LM.

6. Application to U.S. Mortality Data We apply the proposed method to the United States mortality study. Mortality is an overall assess-
ment of the population health of an area. The concentration of high mortality in specific areas in the

On Selection of Semiparametric Spatial Regression Models

16

U.S. has been an important public health concern and received considerable scholarly and policy attention in recent years (Bauer and Kramer, 2016; Chen et al., 2012; Hoyert, 2012; Yang et al., 2015). In the past few decades, the U.S. has witnessed an exceptional decrease in mortality, from almost 20 deaths per 1,000 population in 1930 to roughly 8 deaths per 1,000 population in 2010 (Hoyert, 2012). Despite the significant decrease in overall mortality through the years, disparities in mortality have persisted along various dimensions, such as, gender, age, race/ethnicity, income inequality, social affluence, concentrated disadvantage, safety and geographic space (Chen et al., 2012; Yang et al., 2015).
One of the goals of the study is to investigate the spatial pattern and identify important socioeconomic and racial/ethnic factors that affect mortality. The original mortality dataset is based on the county level, and it includes 3,037 counties from 48 states of the United States and the District of Columbia. The response variable is the average age-standardized mortality rates per 1,000 population based on county level over the period of 1998-2002, and it is publicly available from the Institute for Health Metrics and Evaluation (data IHME, 2016). We classify all the counties in the dataset into six different groups according to their mortality rates: (i) less than 7.5, (ii) 7.50­9.00, (iii) 9.00­10.00, (iv) 10.00­11.00, (v) 11.00­12.50, and (vi) more than 12.50, and these groups are plotted in Figure 1.1, which represents the observed mortality rate from each of 3037 counties in the United States.
Similar as in Chen et al. (2012); Sparks and Sparks (2010); Yang et al. (2011, 2015), the explanatory variables in the study consist of many socioeconomic and racial/ethnic factors from year 2000, such as African-American rate, Hispanic/Latino rate, Gini coefficient, social affluence, disadvantage, violent crime rate (per 1,000 population), property crime rate (per 1,000 population), residential stability, urban rate, percentage of population without health insurance coverage and local government expenditure on health per population. Specifically, the information of Gini coefficient is publicly available at U.S. Census Bureau historical income tables (https://www.census.gov/ data/tables/time-series/dec/historical-income-counties.html), and all the other explanatory variables can be obtained from U.S. Census Bureau and U.S. Federal Bureau Investigation (https://www.census.gov/support/USACdataDownloads.html). Meanwhile, the longitudes and latitudes of the geographic center of each county in the U.S. are available at https://www.census.gov/geo/maps-data/data/gazetteer.html.
According to Chen et al. (2012) and Yang et al. (2015), social affluence is measured by the percentage of households that have income over $75,000, the percentage of population obtaining at least a bachelor degree and percent of people in managerial and professional positions. As stated in Sparks and Sparks (2010) median house value is another important socioeconomic factor that influences mortality rate. Therefore, we also include median house value as an indicator of social affluence. Based on Yang et al. (2015), due to the highly positive correlation between those four variables, factor analysis is used to combine those four variables in a certain scale. Similarly, we apply factor analysis to combine public assistance rate, the percentage of female-headed families and the unemployment rate together to

On Selection of Semiparametric Spatial Regression Models

17

Table 5.5: Variables in the mortality dataset

Variable

Description

Mortality AA HL

mortality rate per 1,000 population African-American rate Hispanic/Latino rate

Gini Affluence

Gini coefficient showing the inequality between different levels of people in society social affluence factors:

percentage of households that have income over $75,000

percentage of population obtaining at least a bachelor degree

percent of people in managerial and professional positions

median house value Disadvantage disadvantage factors:

public assistance rate

percentage of female-headed families

unemployment rate ViolentCrime violent crime rate per 1000 population PropertyCrime property crime rate per 1000 population

ResidStab Urban HealthCover ExpHealth

residential stability urban rate percentage of population without health insurance coverage local government expenditures in health per population

Lat, Lon:

Latitude and longitude of the approximate geographic center of the county.

Note: The covariates with  represent that they are transformed from the original value by f (x) = log(x + ). For example, AA = log(AA + ), where  is a small number.

measure concentrated disadvantages. The factor of residential stability is measured by the percentage of population five years and over by residence in year 1995 lived in the same house in year 2000 and the ratio of housing units occupied by owners. As these two variables are highly correlated, following Yang et al. (2015), we standardize each of them and take the average to get a single indicator for residential stability factor.
As indicated in Table 5.5, we first apply the logarithm to each of the predictors except Gini coefficient and residential stability, then we standardize them before applying our method of variable selection. We fit the mortality data using the following PLSM:
Mortality = 0 + 1AA + 2HL + 3Gini + 4Affluence + 5Disadvantage + 6ViolentCrime + 7PropertyCrime + 8ResidStab + 9Urban + 10HealthCover + 11ExpHealth + (Lat, Lon).
For the bivariate spline smoothing, we use a triangulation with 262 triangles and 167 vertices; see

On Selection of Semiparametric Spatial Regression Models

18

Figure 5.4). It has been proved in Lai and Schumaker (2007), when d  3r + 2, the bivariate spline achieves full approximation power, and thus, we suggest of using d = 5 and r = 1 when we generate the Bernstein basis polynomials. Then we apply the selection approach introduced in Section 2. Figure 5.5 (d) plots the estimated surface of the (·) function in the PLSM.

Figure 5.4: A triangulation of the domain of the U.S.
The selected variables are presented in the second column in Table 5.6, from which one sees that S-PLSM selects four explanatory variables: Affluence, Disadvantage, ViolentCrime and Urban. The estimates of the coefficient (EST) and the standard errors (SE) for these selected variables with the associated p-values are shown in Columns 2­4 in Table 5.6. For comparison, we also analyze the data using the S-SWR with a gaussian spatially weighted matrix and the na¨ive S-LM without adjusting the spatial correlation. Our method of variable selection has a strict sense of selecting variables in the sense of eliminating more variables. Table 5.6 shows that our method provides a more parsimonious model and it eliminates four more variables among the variables selected by the S-SWR or S-LM. The results in Table 5.6 also show that our method provides more accurate estimation with the mean squared error (MSE) of 0.2762, compared to the MSE of 0.8628 via S-SWR and 0.6770 via S-LM.
To further validate the variable selection and prediction results, we use 80% of the observations to build the model and use the other 20% to test the prediction accuracy. All the results are summarized based on 100 partitions. In a conclusion, we have African-American rate, social affluence, concentrated disadvantage, violent crime rate and urban rate as the selected significant variables. Table 5.6 shows that the mean squared prediction error (MSPE) of the mortality rate (per 1,000 population) is 0.6923 and 0.8770 for the S-LM and S-SWR, respectively, while the corresponding MSPE for the S-PLSM is only 0.4123 with about 40%  50% reduction.
We plot the estimated mortality rates via the S-PLSM, the S-SWR and the S-LM with the SCAD penalty; see Figure 5.5 (a)­(c), respectively. Both the S-SWR and the S-LM significantly underestimate the mortality rate in the South region of the U.S. and overestimate the mortality rate in the Midwest region. In contrast, the S-PLSM fitting provides much more accurate estimates of the mortality rate.
Finally we perform model diagnostics for the S-PLSM to check whether it adequately fits the data.

On Selection of Semiparametric Spatial Regression Models

19

(a)

< 7.5 7.5 ~ 9 9 ~ 10 10 ~ 11 11 ~ 12.5 > 12.5

(b)

< 7.5 7.5 ~ 9 9 ~ 10 10 ~ 11 11 ~ 12.5

(c)

< 7.5 7.5 ~ 9 9 ~ 10 10 ~ 11 11 ~ 12.5 > 12.5

(d)

Figure 5.5: (a) estimated mortality rate via the S-PLSM with SCAD penalty; (b) estimated mortality rate via the S-SWR with SCAD penalty; (c) estimated mortality rate via the S-LM with SCAD penalty; (d) estimated spatial effect of  function via the S-PLSM with SCAD penalty.

Table 5.6: US morality rates: variable selection result.

Variable

S -PLSM

EST

SE

p-value

S -SWR

S -LM

AA

­

­

­

­

HL

­

­

­

­

Gini

­

­

­

­

­

Affluence

-0.4666

0.0160

<0.0001

Disadvantage

0.4234

0.0159

<0.0001

ViolentCrime

0.0668

0.0143

<0.0001

PropertyCrime

­

­

­

ResidStab

­

­

­

­

­

Urban

0.1095

0.0155

<0.0001

HealthCover

­

­

­

ExpHealth

­

­

­

­

MSE

0.2762

0.8628

0.6770

MSPE

0.4123

0.8770

0.6923

Note: " " indicates that variable is selected; "­" indicates that variable is not selected.

On Selection of Semiparametric Spatial Regression Models

20

Figure 5.6 (a) and (b) show a scatter plot and a histogram of the residuals of U.S. mortality rates. In addition, we conduct the Moran's I to test the spatial autoregression for the residuals. The test statistic is -0.035, and the p-value for the Moran's I test is 1, which indicates that the process of the residuals is very likely a spatially independent random process.

700

600

500

400

300

200

(a)

< -1.5

100

-1.5 ~ -0.5

-0.5 ~ 0.5

0.5 ~ 1.5

> 1.5

0

-6

-4

-2

0

2

4

6

(b)

Figure 5.6: (a) scatter plot and (b) histogram of the residuals of mortality rates via the SPLSM.
7. Concluding Remarks
In this study, we propose an efficient method for simultaneous estimation and variable selection in the PLSM for spatial data distributed on complex domains. When data are collected from irregularly shaped regions, we find in simulation studies that variable selection methods developed for regression models might usually perform poorly when the spatial information is ignored or handled inappropriately. This has motivated us for developing the proposed method in this paper. We adopt bivariate splines over triangulation to avoid the "leakage" problem in the estimation of the nonparametric spatial component. A new type of double-penalized least squares has been developed to identify and estimate the components in the PLSM simultaneously, which is sufficiently fast for the user to analyze large data sets within seconds. The "oracle" property of the proposed estimator of the parametric part has been established, and consistency of the proposed estimator of the nonparametric part is shown. The numerical results in the simulation demonstrate much better finite sample properties of the proposed estimators compared to the regression models when the spatial effect is unadjusted or adjusted inappropriately.
The selection consistency and the "oracle" property obtained in this paper are based on the assumption that the errors are independent. Although this assumption is not uncommon in the nonparametric spatial smoothing literature, it is more realistic to relax the independence assumption. For example, Gao et al. (2006) investigated the semiparametric spatial regression model for regularly spaced grid points under some stationary and mixing conditions. However, the data collected in our study are randomly distributed over complex domains with irregular boundaries. It is challenging to define the "mixing"

On Selection of Semiparametric Spatial Regression Models

21

condition appropriately in this case, which warrants further research. As illustrated in Example 2 in the simulation studies, the spatial dependence can be alleviated by choosing an appropriate triangulation; it may not fully vanish, and certainly, there is more future work ahead to investigate this issue.
The proposed method in this paper can be easily extended to the case that p is diverging or p n, and our simulation studies have shown that the variable selection method also performs well for those cases. In future research, we will investigate the properties and performance of the proposed method for the more challenging high/ultra-high situation.
Acknowledgment Guannan Wang's research was partially supported by the Faculty Summer Research Grant from
College of William & Mary. The authors are very grateful to Ming-Jun Lai for providing us with the Matlab code on triangulation and bivariate spline basis construction. The authors would like to thank Lily Wang and Lei Gao for providing expertise that greatly assisted the research. The authors would like to thank the Editor, the Associate Editor and the referees for their constructive comments and suggestions.
Data Availability Statement The datasets that support the findings of this study are openly available. The response variable is the
average age-standardized mortality rates per 1,000 population based on county level over the period of 1998-2002, and it is publicly available from the Institute for Health Metrics and Evaluation (data IHME, 2016). The explanatory variables in the study consist of many socioeconomic and racial/ethnic factors from year 2000, such as African-American rate, Hispanic/Latino rate, Gini coefficient, social affluence, disadvantage, violent crime rate (per 1,000 population), property crime rate (per 1,000 population), residential stability, urban rate, percentage of population without health insurance coverage and local government expenditure on health per population. Specifically, the information of Gini coefficient is publicly available at U.S. Census Bureau historical income tables (https://www.census.gov/ data/tables/time-series/dec/historical-income-counties.html), and all the other explanatory variables can be obtained from U.S. Census Bureau and U.S. Federal Bureau Investigation (https://www.census.gov/support/USACdataDownloads.html). Meanwhile, the longitudes and latitudes of the geographic center of each county in the U.S. are available at https://www.census.gov/geo/maps-data/data/gazetteer.html.

On Selection of Semiparametric Spatial Regression Models

22

Appendices

A. Some Preliminary Results

For any function f defined over domain , let En (f ) = n-1

n i=1

f

(Xi)

and

E

(f

)

=

E[f

(X)].

Define the empirical inner product and norm as

f1, f2 n = En (f1f2) and

f1

2 n

=

f1, f1 n for

measurable functions f1 and f2 on . The theoretical L2 inner product and the induced norm are given

by

f1, f2 L2 = E (f1f2) and

f1

2 L2

=

f1, f1 L2. Furthermore, let

· E be the norm introduced by

the inner product ·, · E , where, for g1 and g2 on ,



g1, g2 E =
 i+j=

i

() xi1xj2 g1

() xi1xj2 g2 dx1dx2.

We cite Lemma 2 in the Supplement of Lai and Wang (2013) below, which shows that the uniform difference between the empirical and theoretical inner products is negligible.

LEMMA A.1. Let f1 = K cB, f2 = K c B be any spline functions in S. Under Assumption

7, we have

sup
f1,f2S

f1, f2 n - f1, f2 L2 f1 L2 f2 L2

= OP

(N log n)1/2/n1/2

.

Following Lemma A.7 in Wang et al. (2018), it is easy to obtain the following result in Lemma A.2.

LEMMA A.2. Under Assumptions 1, 2, 7 and 8, there exist constants 0 < cZ < CZ < , such that with probability approaching 1 as n  , cZ Ip×p  n-1(Z - Z) (Z - Z)  CZ Ip×p, where Z = HB(1)Z with HB(1) in (2.6).
In the following, for any bivariate function f (·) and  > 0, define
n
s,f = argminsS {f (Xi) - s(Xi)}2 + E(s)
i=1
the penalized spline estimator of f (·). Then s0,f is the nonpenalized estimator of f (·). Let L() and 2L() be the first order and second order partial derivatives of L() in (6), then
L () = -(Z - Z) (Y - Z) and 2L () = (Z - Z) Z, where

Z = HB(1)Z,

(A.1)

and according to the proof of Lemma A.10 in Wang et al. (2018), n-12L () = n-1(Z - Z) (Z - Z) + oP (1).

On Selection of Semiparametric Spatial Regression Models

23

B. Proof of Theorem 1 Let n = n-1/2 + an,2. It suffices to show that for any given  > 0, there exists a large constant
C such that

Pr sup R(0 + nu) > R(0)  1 - .
u =C

(B.1)

Let Un,1 = L(0 + nu) - L(0) and Un,2 = n

q k=1

{p2

(|k0

+

n uk |)

-

p2 (|k0|)},

where

q is the number of components of 10. Note that p2 (0) = 0 and p2 (||)  0 for all . Thus,

R(0 + nu) - R(0)  Un,1 + Un,2.

For Un,1, we have L(0 + nu)

=

L(0) + nu

L(0)

+

1 2

n2

u

2L()u, where 

=

t(0 + nu) + (1 - t) 0, t  [0, 1], and 2L(0) = (Z - Z) Z with Z defined in (A.1). Let

0 = (0(X1), . . . , 0(Xn)) . Note that -L(0) is equal to

(Z - Z) (Y - Z0) = (Z - Z) (0 + ) = Z {I - HB(1)}0 + Z {I - HB(1)} .

Denote Zj = (Z1j, ..., Znj), and let Wj = n-1Zj {I - HB(1)}0, then, similar to the proof of Lemma A.7 in Wang et al. (2018), we can decompose Wj as follows:

Wj =

zj - hj , 0 - s1,0

n+

hj - hj , 0 - s1,0

n

+

1 n

s1,0 , hj

E = Wj,1 + Wj,2 + Wj,3,

where hj(·) is defined in (11), and hj  S satisfy

hj - hj   C | | +1 |hj | +1, .

(B.2)

By Proposition 1 in Lai and Wang (2013), one has

0 - s1,0  = OP

|

|

+1

|0|

+1,

+

n

1 | |3

|0|2, + | | -1 |0| +1,

.

Next, note that E (Wj,1) = 0, and

1 Var (Wj,1) = n2

n
E [{Zij - hj(Xi)} (0 - s1,0 )]2 

0 - s1,0

2 

n

zj - hj

2 L2

,

i=1

so one has

|Wj,1| = OP

|

| +1 n1/2

|0|

+1,

+

1 n3/2 |

|3

|0|2, + |

| -1 |0| +1,

.

(B.3)

For Wj,2, one has

|Wj,2|  hj - hj n 0 - s1,0 n = OP | | +1 |hj | +1,

× OP

|

|

+1

|0|

+1,

+

n

1 | |2

|0|2, + | | -1 |0| +1,

.

(B.4)

On Selection of Semiparametric Spatial Regression Models

24

Finally, one has

|Wj,3|



1 n

s1,0

E

hj

E



1 n

s0,0

E

hj

E



1 n

C1

|0|2, + |

| -1 |0| +1,

|hj |2, + | | -1 |hj | +1, .

Combining (B.3)-(B.5), one has

|Wj| = OP

1 
n

|

|

+1

|0|

+1,

+

1 n | |3

|0|2, + |

| -1 |0| +1,

for j = 1, . . . , p. Therefore, Assumptions 5­8 imply that Z {I - HB(1)}0 = oP (n1/2). Next,

(B.5)

Var Z {I - HB(1)} |Z, X

= Z {I - HB(1)}{I - HB(1)}Z2
n
= 2 (Zi - Zi)(Zi - Zi) ,
i=1

where Zi is the ith column of Z HB(1). Using Lemma A.2, we have Z {I - HB(1)} =

OP (n1/2). Thus, nu L(0) = OP (n1/2n) u . Next according to the proof of Lemma A.10 in

Wang et al. (2018) n-12L () = n-1(Z - Z) (Z - Z) + oP (1) = E[(Zi - Zi)(Zi - Zi) ] + oP (1),

so

one

has

1 2

n2

u

2L(0)u = OP (nn2) + oP (1). Therefore,

Un,1 = OP (n1/2n) + OP (nn2) + oP (1).

(B.6)

For Un,2, by a Taylor expansion

p2 (|k0

+

n uk |)

=

p2 (|k0|)

+

n uk p2

(|k0|)

sgn

(k0)

+

1 2

n2

u2k

p2

(|k|)

,

where k = (1 - t)k0 + t(k0 + n-1/2uk), t  [0, 1], and

p2 (|k0

+

n uk |)

=

p2 (|k0|)

+

n uk p2

(|k0|)

sgn

(k0)

+

1 2

n2

u2k

p2

(|k0|)

+

o(n-1).

Thus, by the Cauchy-Schwartz inequality,

n-1Un,2

=

n

q

uk p2

(|k0|) sgn (k0)

+

1 2

n2

q

u2kp2 (|k0|)

k=1

k=1



 rnan,2

u

+

1 2

n2bn,2

u 2 = Cn2(q + bn,2 C).

As bn,2  0, the first two terms on the right hand side of (B.6) dominate Un,2, by taking C sufficiently large. Hence (B.1) holds for sufficiently large C.

Proof of Theorem 3.2 We first show that the estimator  must possess the sparsity property 2 = 0, which is stated as
follows.

On Selection of Semiparametric Spatial Regression Models

25

LEMMA C.1. Under the conditions of Theorem 3.2, with probability tending to 1, for any given 1 satisfying that 1-10 = OP (n-1/2) and any constant C, R{(1 , 0 ) } = min 2 Cn-1/2 R{(1 , 2 )}.

Proof. To prove that the minimizer is obtained at 2 = 0, it suffices to show that with probability tending to 1, as n  , for any 1 satisfying 1 - 10 = OP (n-1/2), R()/k and k have different signs for k  (-Cn-1/2, Cn-1/2), for k = q + 1, · · · , p. Note that

R() Rk ()  k = Lk () + np2 (|k|) sgn(k),

where Lk () = Lk (0) +

p k

=1

2Lkk

{tk

+ (1 - t)0k } (k

- 0k ), t  [0, 1]. Let ek be

the zero vector except for an entry of one at position k, then

Lk (0) = -ek Z (I - HB(1))0 - ek Z (I - HB(1)) = -ek Z (I - HB(1)) + oP (n1/2).

According to Lemma A.10 in Wang et al. (2018),

n-12L (0) = n-1E (Zi - Zi)(Zi - Zi) + oP (1) ,

1 n

d1

2Lkk (k - 0k ) = ( - 0)

k =1

E (Zi - Zi)(Zi - Zi)

ek + oP (1) .

Thus, for any  satisfying -0 = OP (n-1/2) as stated in the assumption, we have n-1Lk () = OP (n-1/2). Therefore, for any nonzero k and k = q + 1, · · · , p,

Rk () = n2 2-1p2 (|k|) sgn(k) + OP (n-1/2-2 1) .

Since

lim

inf n

lim

inf k 0+

-2 1p2 (|k|)

>

0

and

 n2



,

the

sign

of

the

derivative

is

deter-

mined by that of k. Thus, the desired result is obtained.

Proof of Theorem 3.2. From Lemma C.1, it follows that 2 = 0.

R ()

=

L(0) + 2L() ( - 0) + n

p2 (|k0|) sign (k0)

q k=1

q

+

p2 (|k0|) + oP (1) (k - k0),

k=1

where  = t0 + (1 - t) , t  [0, 1]. Using an argument similar to the proof of Theorem 3.1, it can be shown that there exists a 1 in Theorem 3.1 that is a root-n consistent local minimizer of R (1 , 0 ) , satisfying n-1R (1 , 0 ) = 0.
The left hand side of the above equation can be written as

n-1Z1 (I - HB(1))

+

p2 (|k0|) sign (k0)

q k=1

+

oP

(n-1/2)

On Selection of Semiparametric Spatial Regression Models

26

+ E (Z1i - Z1i)(Z1i - Z1i) Thus, one has

q

+ oP (1) (1 - 10) +

p2 (|k0|) + oP (1) (1 - 10).

k=1

0 = n-1Z1 (I - HB(1)) + n,2 + oP (n-1/2) + E (Z1i - Z1i)(Z1i - Z1i) + 2 + oP (1) (1 - 10).

(C.1)

Next we study the conditional variance of Z1 (I - HB(1)) given Z1 and X. We write

n

Var Z1 (I - HB(1)) |Z1, X =

(Z1i - Z1i)(Zi - Z1i)

=

n zj , zj - s1,zj

n

.

1j,j q

i=1

For hj  S defined in (B.2), one has

zj , zj - s1,zj

n=

zj - hj , zj - s1,zj

n

+

1 n

s1,zj , hj

E .

(C.2)

Note that | s1,zj , hj E |  s1,zj E hj E  zj ,0 E hj E , s1,zj E  C| |-2 zj ,0 . Thus, | s1,zj , hj E |  C| |-2 zj ,0  hj E  C| |-3(|hj |2, + | | +1-|hj | +1,). We
can decompose zj - hj, zj - s1,zj n as follows:

zj - hj , zj - s1,zj n = zj - hj , zj - hj n + hj - hj , hj - hj n + zj - hj , hj - hj n

+ hj - hj , zj - hj n + zj - hj , hj - s1,zj n + hj - hj , hj - s1,zj n.

(C.3)

According to (B.2), the second term on the right side of (C.3) satisfies that

| hj - hj, hj - hj |  hj - hj  hj - hj  = oP (1).

The third term on the right side of (C.3) satisfies that

| zj - hj, hj - hj n|  { zj - hj L2(1 + oP (1))} hj - hj  = oP (1).

Similarly, we have | hj - hj, zj - hj n| = oP (1). From the triangle inequality, we have

hj - s1,zj n  hj - hj n + hj - s0,zj n + s0,zj - s1,zj n.

According to (B.2) and Lemma A.9 in Wang et al. (2018), hj - s1,zj n  hj - s0,zj n + oP (1).

Let hj,n = argminhS zj - h L2, then, based on the triangle inequality, one has hj - s0,zj n 

hj - hj,n n + hj,n - s0,zj n. It is clear that hj - hj,n L2 = oP (1). By Lemma A.1, one has

hj - hj,n n = oP (1). One also observes that

s0,zj - hj,n

2 L2

=

zj - s0,zj

2 L2

-

zj - hj,n

2 L2

and

zj - s0,zj n 

zj - hj,n n. Applying Lemma A.1 again, we have

s0,zj

- hj,n

2 L2

= oP (

zj -

On Selection of Semiparametric Spatial Regression Models

27

hj,n

2 L2

)

+

oP

(

zj - s0,zj

2 L2

).

Moreover,

there

exists

a

constant

C

such

that

zj - hj,n L2  C, and

zj -s0,zj L2  zj -hj,n L2 + hj,n -s0,zj L2  C + hj,n -s0,zj L2 . Therefore, hj,n -s0,zj L2 =

oP (1), then hj,n - s0,zj n = oP (1) by Lemma A.1. Hence,

s0,zj - hj n = oP (1).

(C.4)

Furthermore, by Lemma A.1 and (C.4), one has

| zj - hj, hj - s1,zj n|  { zj - hj L2 (1 + oP (1))} hj - s0,zj n + oP (1) = oP (1). Similarly, one has

| hj - hj , hj - s1,zj n|  hj - hj n hj - s0,zj n + oP (1) = oP (1).

(C.5)

Combining (C.2)-(C.5) yields zj, zj - s1,zj n = zj - hj, zj - hj n + oP (1). Therefore,

n-1Var Z1 (I - HB(1)) |Z1, X

n
= n-1 (Z1i - Z1i)(Z1i - Z1i) + oP (1)
i=1
= E[(Z1i - Z1i)(Z1i - Z1i) ] + oP (1),

where Z1i = {h1(Xi), . . . , hq(Xi)} . By (C.1), Slutsky's Theorem and central limit theorem, one has

 n(s + 2 )

1 - 10 + (s + 2 )-1n,2

 N(0, 2s) using similar arguments as in the

proof of Theorem 1 in Wang et al. (2018), where s = -2E[(Z1 - Z1)(Z1 - Z1) ].

Hence the result in Theorem 3.2 is proved.

Bibliography
Bauer, D. and Kramer, F. (2016), "The risk of a mortality catastrophe," Journal of Business & Economic Statistics, 34, 391­405.
Breheny, P. and Huang, J. (2015), "Group descent algorithms for nonconvex penalized linear and logistic regression models with grouped predictors," Statistics and Computing, 25, 173­187.
Brown, L. D., Levine, M., and Wang, L. (2016), "A semiparametric multivariate partially linear model: A difference approach," Journal of Statistical Planning and Inference, 178, 99­111.
Bunea, F. and Wegkamp, M. H. (2004), "Two-stage model selection procedures in partially linear regression," Canadian Journal of Statistics, 32, 105­118.
Chen, V. Y. J., Deng, W. S., Yang, T. C., and Matthews, S. A. (2012), "Geographically weighted quantile regression (GWQR): An application to US mortality data," Geographical analysis, 44, 134­150.
data IHME (2016), "United States Mortality Rates by County 1980-2014." Retrieved from http://ghdx.healthdata.org/record/united-states-mortality-rates-county-1980-2014.
Fan, J. and Li, R. (2001), "Variable selection via nonconcave penalized likelihood and its oracle properties," Journal of the American Statistical Association, 96, 1348­1360.
Gao, J., Lu, Z., Tjøstheim, D., et al. (2006), "Estimation in semiparametric spatial regression," The Annals of Statistics, 34, 1395­1435.
Gheriballah, A., Laksaci, A., and Rouane, R. (2010), "Robust nonparametric estimation for spatial regression," Journal of Statistical Planning and Inference, 140, 1656­1670.
He, H., Tang, W., and Zuo, G. (2014), "Statistical inference in the partial linear models with the double smoothing local linear regression method," Journal of Statistical Planning and Inference, 146, 102­ 112.
Hoshino, T. (2018), "Semiparametric spatial autoregressive models with endogenous regressors: with an application to crime data," Journal of Business & Economic Statistics, 36, 160­172.
28

BIBLIOGRAPHY

29

Hoyert, D. L. (2012), "75 years of mortality in the United States, 1935­2010." NCHS Data Brief, Retrieved from https://www.cdc.gov/nchs/data/databriefs/db88.pdf, 1­7.

Huang, J. Z., Zhang, L., and Zhou, L. (2007), "Efficient estimation in marginal partially linear models for longitudinal/clustered data using splines." Scandinavian Journal of Statistics, 34, 451­477.

Lai, M. J. and Schumaker, L. L. (2007), Spline functions on triangulations., Cambridge University Press.

Lai, M. J. and Wang, L. (2013), "Bivariate penalized splines for regression." Statistica Sinica, 23, 1399­1417.

Lee, L.-F. (2004), "Asymptotic Distributions of Quasi-Maximum Likelihood Estimators for Spatial Autoregressive Models," Econometrica, 72, 1899­1925.

LeSage, J. and Pace, R. K. (2009), Introduction to spatial econometrics, Chapman and Hall/CRC.

Leung, S. and Cooley, D. (2014), "A comparison of a traditional geostatistical regression approach and a general Gaussian process approach for spatial prediction," Stat, 3, 228­239.

Li, Y. and Ruppert, D. (2008), "On the asymptotics of penalized splines." Biometrika, 95, 291­297.

Lian, H. (2012), "Semiparametric estimation of additive quantile regression models by two-fold penalty," Journal of Business & Economic Statistics, 30, 337­350.

Lian, H., Liang, H., and Wang, L. (2014), "Generalized additive partial linear models for clustered data with diverging number of covariates using GEE." Statistica Sinica, 24, 173­196.

Liang, H. and Li, R. (2009), "Variable selection for partially linear models with measurement errors." Journal of the American Statistical Association, 104, 234­248.

Liu, X., Wang, L., and Liang, H. (2011), "Estimation and variable selection for semiparametric additive partial linear models." Statistica Sinica, 21, 12­25.

Ma, S., Song, Q., and Wang, L. (2013), "Simultaneous variable selection and estimation in semiparametric modeling of longitudinal/clustered data." Bernoulli, 19, 252­274.

Ma, S. and Yang, L. Y. (2011), "Spline-backfitted kernel smoothing of partially linear additive model," Journal of Statistical Planning and Inference, 141, 204­219.

Nandy, S., Lim, C. Y., and Maiti, T. (2017), "Additive model building for spatial regression." Journal of the Royal Statistical Society: Series B (Statistical Methodology), 79, 779­800.

BIBLIOGRAPHY

30

Ramsay, T. (2002), "Spline smoothing over difficult regions." Journal of the Royal Statistical Society: Series B (Statistical Methodology), 64, 307­319.

Sangalli, L., Ramsay, J., and Ramsay, T. (2013), "Spatial spline regression models." Journal of the Royal Statistical Society: Series B (Statistical Methodology), 75, 681­703.

Shewchuk, J. (1996), "Triangle: Engineering a 2D quality mesh generator and Delaunay triangulator." Applied computational geometry towards geometric engineering, 203­222.

Sparks, P. J. and Sparks, C. S. (2010), "An application of spatially autoregressive models to the study of US county mortality rates." Population, Space and Place, 16, 465­481.

Strand, E. K., Smith, A. M., Bunting, S. C., Vierling, L. A., Hann, D. B., and Gessler, P. E. (2006), "Wavelet estimation of plant spatial patterns in multitemporal aerial photography," International Journal of Remote Sensing, 27, 2049­2054.

Wall, M. M. (2004), "A close look at the spatial structure implied by the CAR and SAR models," Journal of Statistical Planning and Inference, 121, 311­324.

Wang, L., Liu, X., Liang, H., and Carroll, R. (2011), "Estimation and variable selection for generalized additive partial linear models." The Annals of Statistics, 39, 931­955.

Wang, L., Wang, G., Lai, M. J., and Gao, L. (2018), "Efficient estimation of partially linear models for data on complicated domains by bivariate penalized splines over triangulations," Statistica Sinica, accepted.

Wood, S. N. (2003), "Thin Plate Regression Splines." Journal of the Royal Statistical Society: Series B (Statistical Methodology), 65, 95­114.

Wood, S. N., Bravington, M. V., and Hedley, S. L. (2002), "Soap film smoothing." Journal of the Royal Statistical Society: Series B (Statistical Methodology), 70, 931­955.

Xie, H. and Huang, J. (2009), "Scad-penalized regression in high-dimensional partially linear models." The Annals of Statistics, 37, 673­696.

Yang, T. C., Jensen, L., and Haran, M. (2011), "Social capital and human mortality: Explaining the rural paradox with county-level mortality data." Rural sociology, 76, 347­374.

Yang, T. C., Noah, A. J., and Shoff, C. (2015), "Exploring geographic variation in US mortality rates using a spatial Durbin approach." Population, space and place, 21, 18­37.

BIBLIOGRAPHY

31

Zhou, L. and Pan, H. (2014), "Smoothing noisy data for irregular regions using penalized bivariate splines on triangulations." Computational Statistics, 29, 263­281.

Zou, H. (2006), "The adaptive lasso and its oracle properties." Journal of the American Statistical Association, 101, 1418­1429.

