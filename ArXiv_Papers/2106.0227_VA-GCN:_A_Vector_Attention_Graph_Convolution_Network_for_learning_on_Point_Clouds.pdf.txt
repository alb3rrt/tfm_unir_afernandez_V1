VA-GCN: A Vector Attention Graph Convolution Network for learning on Point Clouds

arXiv:2106.00227v1 [cs.CV] 1 Jun 2021

Haotian Hu College of Optical Science and Engineering
Zhejiang University hht1996ok@zju.edu.cn

Fanyi Wang College of Optical Science and Engineering
Zhejiang University 11730038@zju.edu.cn

Huixiao Le Peking University interesting@pku.edu.cn

Abstract
Owing to the development of research on local aggregation operators, dramatic breakthrough has been made in point cloud analysis models. However, existing local aggregation operators in the current literature fail to attach decent importance to the local information of the point cloud, which limits the power of the models. To fit this gap, we propose an efficient Vector Attention Convolution module (VAConv), which utilizes K-Nearest Neighbor (KNN) to extract the neighbor points of each input point, and then uses the elevation and azimuth relationship of the vectors between the center point and its neighbors to construct an attention weight matrix for edge features. Afterwards, the VAConv adopts a dual-channel structure to fuse weighted edge features and global features. To verify the efficiency of the VAConv, we connect the VAConvs with different receptive fields in parallel to obtain a Multi-scale graph convolutional network, VA-GCN. The proposed VA-GCN achieves state-of-the-art performance on standard benchmarks including ModelNet40, S3DIS and ShapeNet. Remarkably, on the ModelNet40 dataset for 3D classification, VA-GCN increased by 2.4% compared to the baseline. Codes and pre-trained models are available at https://github.com/hht1996ok/VA-GCN.
1 Introduction
With the booming of 3D vision in fields such as autonomous vehicles and robotics, 3D point cloud has become an emerging important research topic. In recent years, machine learning and computer vision have made a significant breakthrough in 3D point cloud processing, greatly improving the performance of point clouds in various tasks (3D shape classification [1,2,3,4], 3D semantic segmentation [1,2,3,7,8], 3D object detection [5,6], etc.). Due to the disorder, sparseness and irregularity of the point clouds, how to effectively use the point cloud information and capture the relationship between points comes to be the focus of this field.
As is shown in Figure 1, PointNet [1] is a pioneering work that uses Multi-Layer Perceptron (MLP) to independently learn point features and uses a max-pooling layer to aggregate individual point features into a global representation. However, PointNet neglects the local information of point clouds. As such, Qi et al. proposed PointNet++ [2], which utilizes the local information and hierarchically processes a set of points, but pointNet++ ignores the geometric topology information between points. DGCNN [9] attempts to use the point cloud geometric information, which utilizes KNN to select the
Corresponding Author.
Preprint. Under review.

Figure 1: The schematic diagram of VA-GCN is compared with previous works. NxC represents the input size, and NxC1 represents the output size. Different from the previous networks, VA-GCN uses a dual-channel structure, which combines high-dimensional relative structure with low-dimensional relative structure.
nearest k neighbor points of each input point, builds a directed graph to extract geometric features, and extracts the edge features between the center point and its neighbor points through EdgeConv [9]. Unfortunately, DGCNN does not consider the direction of the vector, leading to the loss of useful information. Geo-CNN [26] aggregates local information according to the angle between the relative vector and the three coordinate axes, and establishes a standard basis for six directions to simulate convolution operations. However, Geo-CNN does not pay attention to the different influences of neighbor points around the central point in the process of information aggregation, resulting in the addition of irrelevant edge features.
To solve the problems above, we propose a new dual-channel local information aggregation module named VAConv to assign greater weights to more important features in edge features. The advantage of VAConv is that it not only has explicit modelling of the local information aggregation process but also can flexibly change the size of the receptive field to make full use of the input information. What's more, by stacking the VAConv modules with different receptive fields, we constructed a Vector Attention Graph Convolution Network(VA-GCN), which hierarchically extracts the local information of the point clouds and fuses features with different scales. The proposed VA-GCN achieves state-ofthe-art performance on standard benchmarks including ModelNet40 [8] (for 3D classification), S3DIS [40] (for 3D semantic segmentation) and ShapeNet [35] (for 3D part segmentation). Remarkably. for the classification task, the overall accuracy of VA-GCN is 2.1% higher than baseline, which to the best of our knowledge, is the highest score so far. Our contributions are summarized as follows:
· We propose an efficient Vector Attention Convolution module (VAConv), which uses a geometric representation of point clouds to explicitly construct the weight matrix of the local edge features. Meanwhile, global information constrained by relative vectors are added into local information to enrich the semantics of output features.
· By stacking EdgeConv and VAConv, we construct a dual-channel multi-scale local information aggregation model VA-GCN, which adds low-dimensional and high-dimensional relative geometric relations to the global semantics.
· We conduct extensive experiments on VA-GCN, and the results indicate that VA-GCN can achieve state-of-the-art performance on three benchmark datasets, for 3D classification, 3D semantic segmentation and 3D part segmentation respectively. We creatively propose MultiSample Inference(MSI), and after performing MSI, the highest score has been achieved on the ModelNet40 benchmark.
2

2 Related works
Existing methods in literature can be roughly divided into three categories. Firstly, Multi-view methods [10,11,12,13] project point clouds into multiple two-dimensional views and use convolution to extract the features of each view, then perform feature fusion. Secondly, volumetric-based methods [14,8,16,17] voxelizes point clouds into a 3D grid, and uses 3D convolution to extract features from adjacent grids. Thirdly, point-based methods [1,2,3,4,18,19,20] is based on point clouds, which uses shared MLP to independently model each point, and then utilizes a symmetric function to aggregate the global features of point clouds.
Figure 2: VAConv schematic diagram. N represents the number of points, and K represents the number of selected neighbor points. The input (green box) is the absolute position (with size of Nx3) and the global information (with size of NxC), (a) the KNN algorithm is used to get the neighbor points of each point. (b) Establish a directed graph to represent the adjacency of the center point, from which edge features and relative vectors could be obtained. (c) Shows the calculation process of elevation and azimuth. (d) Represents the aggregation process of edge features and global features. Relative coordinates and distance (Nx4) is the output of (b).
2.1 Multi-view methods
Early works projected unstructured point clouds into multiple two-dimensional views, used 2D convolution to extract information from different views, and fused information from these sources to classify the point cloud accurately. The major challenge of this approach is how to fuse multi-view information. MVCNN [11] is a pioneering work, which uses a max-pooling to aggregate multi-view information into global information. However, the max-pooling only retains the largest element, which will inevitably cause information loss. Aimed at this problem, Wei et al. proposed View-GCN [21] which uses a directed graph and treats each view as a graph node. Max-pooling is performed on the graph nodes of all levels to obtain the global shape descriptors. Multi-view methods can directly use two-dimensional convolution after projection, which is convenient to implement, but are too slow to be suitable for general scenes.
2.2 Volumetric-based methods
The volumetric-based methods divide point clouds into a uniform spatial 3D grid, and then use a 3D convolutional neural network for 3D shape classification. Maturana et al. [14] introduced VoxNet to implement robust 3D target detection. Wu et al. [8] proposed 3D shapeNets, which uses DBN convolutional networks to learn the distribution of points with different three-dimensional shapes. Although those methods have achieved decent results, time and memory usage will explode with the increase of resolution. In order to solve this problem, recent researches have proposed sparse representation to reduce the demand for memory. OctNet [16] uses shallow octrees to represent the scene along the regular grid, and uses bit string representation to improve coding efficiency. Compared with the dense input network model, OctNet remits quite an amount of computational resources consumption but is still computational-intensive.
3

2.3 Point-based methods
The proposal of PointNet [1] paved the way for researchers. Based on the permutation invariance of point clouds, PointNet uses MLP to extract global information and aggregate features through the max-pooling layer. Although impressive results have been achieved, PointNet only processes each point individually and does not use the local information of point clouds, which will inevitably lead to the loss of key information. The successor, PointNet++ [2], introduces the concept of local information into the 3D point cloud analysis model. PointNet++ uses MLP to aggregate fine local information of neighbor points layer by layer, and obtains global descriptors through the local information obtained by max-pooling layer aggregation.
Subsequent 3D point cloud analysis works mainly focus on the research of point cloud local information aggregation. PATs [28] uses the relative positions and absolute positions of points to represent each point and then extracts the local information of the point cloud. DGCNN [9] establishes a directed graph between the center point and its neighbor points, uses EdgeConv as a feature extraction function to extract the features of each edge, and aggregates local information through the max-pooling layer. Yan et al. [22] uses the Adaptive Sampling(AS) module to adaptively adjust the FPS algorithm. In order to solve the problems of long-running time and large memory requirements for large-scale point clouds, Xu et al. proposed Grid-GCN [20]. Furthermore, Grid-GCN proposed a fast sampling method based on Voxel, which combines volumetric-based methods and point-based methods, shortening the sampling time by 5 times.
At the same time, a large number of models that simulate convolution operations to obtain global descriptors by carefully designing convolution kernels have emerged. RS-CNN [4] maps lowdimensional relations such as relative position and distance to high-level relations to simulate the convolution operation. The edge features along each direction of the base in Geo-CNN [26] are independently weighted by a learnable matrix related to one direction, then local information is aggregated based on the angle between the relative vector and the three coordinate axes. A-CNN [27] proposed an annular convolution method to learn the relationships between adjacent points, and improves the local area overlap problem that exists in the Multi-Scale Grouping(MSG) [2] module in PointNet++. These methods all regard how to aggregate local features as the main research point, but ignore the redundancy and error information existing in the local features. Thus, we propose VA-GCN which utilize the geometric relationship of point clouds to constrain the aggregation process of local information.
3 Method
To accurately analyze each point in complex point clouds, not only the information of each point but also the information of its neighbor points should be taken into consideration. Inspired by DGCNN [9] and FR3DNet [10], we propose VAConv, which exploits low-dimensional geometric relations to explicitly model the attention weight matrix of edge features to highlight the more important local proximity information, and each point can perceive the surrounding point cloud structure. The VA-GCN is constructed by stacking the VAConv module of different receptive fields, which makes full use of the local information of various scales while maintaining the geometric structure in the 3D Euclidean space. Finally, max-pooling is used to aggregate local multi-scale information. In Section 3.1, the basic structure of VAConv is introduced. In Section 3.2, the approach to stack VAConvs with different receptive fields to obtain a VA-GCN network is described.
3.1 Vector Attention Convolution
VAConv is the heart of the VA-GCN. We exploit global features Xw-1 = xw1 -1, . . . , xwn -1  RF n-1 and absolute position of point clouds P = {p1, . . . , pn}  R3 as the input of VAConv module, w - 1 is the number of layers, and F w-1 is the number of channels output of the w - 1 layer. As is shown in Figure 2, we first use the k-nearest neighbor (KNN) algorithm to select the k nearest neighbor points Qi = {qi1, . . . qik} in the range of r near the center point pi, and then establish a directed graph between the center point and its neighbor points to obtain the relative position vector eij and edge features Diwj of global features. By changing the size of r, we can flexibly adjust the receptive field. VAConv is a dual-channel structure module, the first channel exploits the relative position vector and two angles (Eij, Aij) of the three-dimensional space coordinate system to
4

Figure 3: Schematic diagram of VA-GCN structure. The upper model is for 3D classification, and the lower is for 3D segmentation. N represents the number of input point clouds, the input of the classification network is the absolute position of the point cloud (Nx3), and the input of the segmentation network is (NxC1). The output of the classification network (NxCls) and the output of the segmentation network (NxSeg). The blue module is a schematic diagram of VAConv. Its input is the global information (NxC) extracted by the previous MLP and the absolute position of the point cloud (Nx3). After calculating the elevation and azimuth by the orange module, edge features is aggregated by max-pooling.

explicitly construct the attention weight of edge features. The second channel uses the relative distance between the center point and its neighbor points to constrain the extraction process of high-dimensional information. A channel-wise max-pooling is used to aggregate local information to the center point.

In the first channel, we express the relative position vector eij between the center point pi and its neighbor points qij as:

eij = {qi1 - pi, . . . , qik - pi} , i = {1, . . . , n}

(1)

Edge features Diwj is expressed as:

Diwj = xwi1-1 - xwi -1, . . . , xwik-1 - xwi -1 , i = {1, . . . , n}

(2)

Unlike EdgeConv [9], We did not directly concatenate the edge features into the global features.

Instead, the low-dimensional geometric structure is used to constrain aggregation process of edge

features, giving each edge a different weight. We take the low-dimensional geometric structure

information into consideration because we believe that the original geometric structure have changed

in the high-dimensional space, and simply concatenate edge features and the global information will

lose the information of original geometric structure. At the same time, The distance disij between the center point itself pi and its neighbor points qij in Euclidean space can be obtained by the relative position vector eij:

eij = {xij , yij , zij }

disij = (xij )2 + (yij )2 + (zij )2

(3)

As is shown in (c) of Figure 2, We first establish an orthogonal three-dimensional space coordinate system XYZ, then project the relative position vector eij to the XY plane. The angle between the relative position vector and the projection vector of the XY plane in the Z direction is called elevation Eij, and the angle between the projection vector and the Y axis is called the azimuth Aij. They can be calculated according to the position vector eij and the distance disij between adjacent points:

 

Eij =

zij disij



Aij

=

 xij
(xij )2+(yij )2

(4)

5

VAConv uses MLP to extract high-dimensional edge features. H is the weight function used to

extract edge features, and M (p, q) is the distance of the center point to its neighbor points. Shorter

the distance is, greater weight is assigned to the corresponding edge features. The purpose of the

aggregation function G(p, q) is to aggregate edge features to the center point pi. Their expressions

are as follows:

M (p, q) =

(max (disij) - disij)2

(5)

sum (max (disij) - disij)2

G(p, q) = sum H Diwj  cos Eij cos Aij  M (p, q)

(6)

In the other channel, we use relative position vector and relative distance to constrain the extraction process of global information. The first MLP is used to extract the attention weight of the global information, and after the cross-multiplication with the global information, high-dimensional feature is extracted through the second MLP.

gw = mlp Xw-1  max (mlp (concat (eij, disij)))

(7)

After adding the outputs of the two channels, passing through the max-pooling layer which aggregate features to the center point, the final output could be achieved:

Xw = max (gw  G(p, q))

(8)

3.2 Structure of VA-GCN
VA-GCN is a dual-channel model that obtains global shape descriptors by fusing the relative positions of spatial features in different dimensions. Its input is the original representation of point clouds, denoted as X = {x1, . . . , xn}  RC , when c = 3, the input is the point cloud three-dimensional coordinates xi = {x, y, z}. As is shown in Figure 3, in the channel (a), the input of the EdgeConv module is global information. This channel cascades three EdgeConv modules. Each EdgeConv integrates edge features into the input global information, and uses MLP to extract high-dimensional global information from it.
We stack two layers of VAConv modules to establish the channel (b). In each layer, there are two VAConv modules with different receptive fields arranged in parallel. In addition, we also gradually increase the size of the receptive fields in each layer. Firstly, we extract high-dimensional global information through MLP. Secondly, after establishing the directed graph, we exploit the relative position relationship to aggregate edge features in the VAConv module. In order to better preserve the multi-scale local information, we use MLP to directly extract the high-dimensional features of the small-scale global information and add it to the output of the second layer to obtain fused multi-scale global information.
The main function of channel (c) is to fuse the global information extracted by the two channels. We concat the output of the two channels through a third-layer VAConv module with r = 0.6 and 0.8, which aims to use a larger receptive field to obtain the overall geometric structure of point clouds. Finally, the point cloud global shape descriptor is obtained, which can be input into the classifier for tasks such as 3D shape recognition and 3D segmentation.

4 Experiments
To verify the effectiveness of the VA-GCN, we performed 3D classification, 3D semantic segmentation, and 3D part segmentation experiments on the ModelNet40 [8], S3DIS [40] and ShapeNet [35] benchmark datasets separately. For training, we use Adam optimizer under the weight decay of 0.0001, the mini-batch size is 16, and the training process starts with a learning rate of 0.001. A cosine annealing algorithm is applied to reduce the learning rate to 0 in 500 epochs. We adopt PyTorch 1.1.0 framework to implement experiments on a computer with 3.4 GHz Intel Xeon-E5-2643-v3 CPU, 64G RAM, and two NVIDIA GTX 1080Ti GPUs.
4.1 3D classification experiment and discussion
Dataset: We evaluated our model on the ModelNet40 dataset for 3D classification task. ModelNet40 has a total of 12311 CAD models, among which there are 9843 training samples and 2468 test

6

Table 1: ModelNet40 shape classification results. The Overall Accuracy of VA-GCN is 2.1% higher than that of DGCNN, and Mean Class Accuracy increased by 1.2%.  indicates baseline.

Method
PointNet [1] MO-Net [29] Deep Sets [30] PointNet++ [2] PointCNN [3] PCNN [31] A-CNN [27] Point2Seq [32] KPConv [19] PointASNL [22] PointASNL [22] Geo-CNN [26] DGCNN [9]
VA-GCN VA-GCN VA-GCN+MSI

input
coordinates coordinates coordinates coordinates coordinates coordinates coordinates coordinates coordinates coordinates coordinates+normal coordinates+normal
coordinates coordinates coordinates+normal coordinates+normal

Overall Accuracy
89.2% 89.3% 90.3% 90.7% 92.2% 92.3% 92.6% 92.6% 92.7% 92.9% 93.2% 93.4%
92.2% 92.7% 0.5% 93.5% 1.3% 94.3% 2.1%

Mean Class Accuracy
86.2% 86.1%
88.1% 90.3% 91.1%
90.2% 89.3% 90.4% 0.2% 91.4% 1.2%

samples. For each training sample, we uniformly sampled 1024 points. We use two sets of data for training, one with the normal and the other without. During the training procedure, we augment the data [2] by scaling objects and perturbing the object and point locations. Overall accuracy and mean class accuracy are adopted as indicators of the performance.
Implementation Details: On the ModelNet40 dataset for 3D classification, the parameters and structure of our model are shown in the classification network of Figure 3. The last MLPs layer contains 3 fully connected layer, the sizes of them are (2048,512), (512,256), (256,40) separately.
Result: As is shown in Table 1, the VA-GCN achieved the most advanced performance on the ModelNet40 dataset for 3D classification task. Inspired by multi-scale inference [1,2], we use multisample inference(MSI) for inference. That is, we sampled 1024 points out of input points randomly for inference, and repeated this procedure for 10 times, then averaged the 10 predicted results as the final result. This method can further improve the overall accuracy by 0.8%.
4.2 3D semantic segmentation experiment and discussion
Dataset: We evaluated our model on S3DIS [36] dataset for semantic scene segmentation task. S3DIS consists 272 point cloud scanning models of 6 indoor areas. Each point in the model belongs to one of the 13 pre-classifications. We divided the room into blocks with an area of 1m2, using XYZ, RGB and normalized location to represent point clouds. We sampled 4096 points from each model for training, and utilized Area-5 as the test scene. Implementation Details: For semantic segmentation task, the parameters and structure of our model are shown in the segmentation model of
Figure 3. The input of the network is XYZ, RGB and standardized spatial coordinates (Nx9). The last MLPs layer contains 3 fully connected layer, the sizes of them are(1408,512), (512,256), (256,13).
Result: As is shown in Table 2, We use mean Inter-over-Union (mIoU) as evaluation indicator. Our VA-GCN results are 0.8% higher than the result of DGCNN with 6-fold cross validation(calculating the metrics with results from different folds merged). Figure 4 shows the visualization results on the S3DIS dataset, in visualization, our results are significantly better than PointNet++.
4.3 3D part segmentation experiment and discussion
Dataset: We conducted 3D part segmentation experiments on the ShapeNet dataset. The dataset contains 16881 models divided into 16 categories, with a total of 50 parts marked. Each point in the point clouds collection is marked as one of the preset categories. We sampled 2048 points from each
7

Figure 4: visualization results of S3DIS, from top to bottom are ground truth, PointNet++ and VA-GCN separately.

Table 2: Results of S3DIS indoor semantic segmentation on Area-5. The metrics of PointNet++ are generated by publicly available code.  indicates baseline and DGCNN use 6-fold cross validation.

Method PointNet [1] SegCloud [37] RSNet [28] PointNet++ [2] DGCNN [2] VA-GCN

mIoU

41.1%

48.9%

51.9%

55.0%

56.1%

56.9%0.8%

model for training. Afterwards, we performed voting tests with random scaling and then averaged the prediction results [1,2].
Implementation Details: For the 3D part segmentation task, the specific structure of our VA-GCN is shown in Figure 3(b). The input of the network are XYZ (Nx3) and a one-hot label (1x16). The one-hot label passes through the MLP with the size of (16,64) and then is merged with the global features. The last MLPs layer contains 3 MLP, the sizes of them are (1472,512), (512,256), (256,13) separately. We used the instance average and class average mean Inter-over-Union (mIoU) as the evaluation index.

Table 3: Metrics results on ShapeNet for 3D part segmentation task.  indicates baseline.

Method
Cls. mIoU Ins. mIoU

PointNet [1]
80.4% 83.7%

PointNet++ [2]
81.9% 85.1%

3D-GCN [39]
82.1% 85.1%

PCNN [31]
81.8% 85.2%

DGCNN [9]
82.3% 85.2%

VA-GCN
82.6%0.3% 85.5%0.3%

Result: Table 3 lists the instance average and class average mean Inter-over-Union (mIoU). The Cls.mIoU and the Ins.mIoU of VA-GCN is 0.3% better than DGCNN. Figure 5 is the visualized segmentation results, compared with PointNet++, our VA-GCN has more detailed segmentation and can better classify the part of the object.
4.4 Ablation experiments
In the ablation experiments, we verified the effectiveness of the parallel structure of VAConv modules with different scales. As shown in Table 4, we build four VA-GCN models with different numbers of parallel structure: v0 does not use parallel structure; v1 uses one parallel structure in the first layer;v2 uses parallel structure in the first two layers ; v3 uses one parallel structure in each layer of VA-GCN. We doubled the output channels of the VAConv module in a single structure to make it identical with the number of output channels of the parallel structure. The ablation experiment proved the effectiveness of the parallel VAConv structure. As is shown in Table 4, the v3 model obtained the

8

Figure 5: Visualization results on ShapeNet for part segmentation task. The first column is the ground truth, the second column is the results of PointNet++, and the third column is the results of VA-GCN. From left to right are rocket, airplane, bag, chair, earphone, knife, lamp, motorbike. Same color is uesd to label the same class.

best performance in the classification task, and Overall Accuracy improved by 0.9% compared with not using the parallel structure.

Table 4: Comparison of the Overall Accuracy of models with different parallel channel numbers.

Method

VA-GCNv0 VA-GCNv1 VA-GCNv2 VA-GCNv3

Overall Accuracy 92.6%

93.3%

92.7%

93.5%

As is shown in Table 5, we compared performance of the single-channel structure and the dual-channel structure in the ModelNet40 classification task. The Overall Accuracy of the dual-channel structure is 0.7% higher than that of the single-channel structure. This proves that the fusion of high-dimensional edge features and low-dimensional geometric information helps the point cloud analysis model to better capture the information of points.

Table 5: Comparison of the Overall Accuracy of models with different parallel channel numbers.

Method

Only EdgeConv channel [9] Only VAConv channel Dual channel

Overall Accuracy

83.1%

92.8%

93.5%

5 Conclusion
In this article, we propose a new dual-channel multi-scale aggregation model VA-GCN, the core of which is the VAConv module. Owing to the stack of VAConv with different receptive fields and EdgeConv, the global features are fully integrated. The VAConv makes use of the relative position relationship of the point cloud neighborhood to calculate the elevation and the azimuth, then utilizes these two angles to explicitly model the attention weight matrix for edge features. VA-GCN achieves state-of-the-art performance on the challenging ModelNet40, S3DIS and ShapeNet datasets. To the best of our knowledge, the proposed VA-GCN achieves the highest score on ModelNet40 dataset.
References
[1] I. Armeni, O. Sener, A. R. Zamir, H. Jiang, I. Brilakis, M. Fischer, and S. Savarese. 3d semantic parsing of large-scale indoor spaces. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition(CVPR), 2016.
9

[2] I. Armeni, O. Sener, A. R. Zamir, H. Jiang, I. Brilakis, M. Fischer, and S. Savarese. 3d semantic parsing of large-scale indoor spaces. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition(CVPR), 2016.
[3] Matan Atzmon, Haggai Maron, and Yaron Lipman. Point convolutional neural networks by extension operators. ACM Trans. Graph., 2018.
[4] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, and H. Su. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015.
[5] Q. Huang, W. Wang, and U. Neumann. "recurrent slice networks for 3d segmentation of point clouds. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition(CVPR), 2018.
[6] M. Joseph-Rivlin, A. Zvirin, and R. Kimmel. Mo-net: Flavor the moments in learning to classify shapes. In Proceedings of the IEEE International Conference on Computer Vision Workshop (ICCVW), 2018.
[7] A. Komarichev, Z. Zhong, and J. Hua. A-cnn: Annularly convolutional neural networks on point clouds. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition(CVPR), pages 7421­7430, 2019.
[8] S. Lan, R. Yu, G. Yu, and L.S. Davis. Modeling local geometric structure of 3d point clouds using geo-cnn. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition(CVPR), 2019.
[9] Truc Le and Ye Duan. A deep network for 3d shape understandings. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition(CVPR), pages 9204­9214, 2018.
[10] Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, and Baoquan Chen. Pointcnn: Convolution on x-transformed points. In Advances in Neural Information Processing Systems, pages 820­830, 2018.
[11] ZhiHao Lin, ShengYu Huang, and YuChiang Frank Wang. Convolution in the cloud: Learning deformable kernels in 3d graph convolution networks for point cloud analysis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition(CVPR), 2020.
[12] X. Liu, Z. Han, Y.-S. Liu, and M. Zwicker. Point2sequence: Learning the shape representation of 3d point clouds with an attention-based sequence to sequence network. Association for the Advancement of Artificial Intelligence(AAAI), 2019.
[13] Y. Liu, B. Fan, G. Meng, J. Lu, S. Xiang, and C. Pan. Densepoint: Learning densely contextual representation for efficient point cloud processing. In: Proceedings of the IEEE International Conference on Computer Vision, pages 5239­5248, 2019.
[14] Yongcheng Liu, Bin Fan, Shiming Xiang, and Chunhong Pan. Relation-shape convolutional neural network for point cloud analysis. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 8895­8904, 2019.
[15] D Maturana and S. Scherer. Voxnet: A 3d convolutional neural network for real-time object recognition. In Proceedings of the IEEE International Conference on Intelligent Robots and Systems, pages 922­928, 2015.
[16] C. R. Qi, H. Su, M. Nießner, A. Dai, M. Yan, and L. J. Guibas. Volumetric and multi-view cnns for object classification on 3d data. IEEE Conference of Computer Vision and Pattern Recognition (CVPR), 2016.
[17] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 652­660, 2017.
[18] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. In Advances in Neural Information Processing Systems, pages 5099­5108, 2017.
10

[19] Gernot Riegler, Ali Osman Ulusoy, and Andreas Geiger. Octnet: Learning deep 3d representations at high resolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition(CVPR), pages 3577­3586, 2017.
[20] H. Su, S. Maji, E. Kalogerakis, and E. Learned-Miller. Multiview convolutional neural networks for 3d shape recognition. IEEE Conference of Computer Vision and Pattern Recognition (CVPR), 2015.
[21] Lyne P. Tchapmi, Christopher B. Choy, Iro Armeni, JunYoung Gwak, and Silvio Savarese. Segcloud: Semantic segmentation of 3d point clouds. International Conference on 3D Vision(3DV), 2017.
[22] H. Thomas, C.R. Qi, J.E. Deschaud, B. Marcotegui, F. Goulette, and L.J. Guibas. Kpconv: Flexible and deformable convolution for point clouds. In: Proceedings of the IEEE International Conference on Computer Vision, pages 6411­6420, 2019.
[23] Dominic Zeng Wang and Ingmar Posner. Voting for voting in online point cloud object detection. In Proceedings of Robotics: Science and Systems, Rome, Italy, July 2015.
[24] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity group proposal network for 3d point cloud instance segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2569­2578, 2018.
[25] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, and Justin M.Solomon. Dynamic graph cnn for learning on point clouds. ACM Transactions on Graphics (TOG), 2019.
[26] X. Wei, R. Yu, and J. Sun. View-based graph convolutional network for 3d shape analysis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition(CVPR), 2020.
[27] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: A deep representation for volumetric shapes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1912­1920, 2015.
[28] S. Xie, S. Liu, Z. Chen, and Z. Tu. Attentional shapecontextnet for point cloud recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition(CVPR), 2018.
[29] Qiangeng Xu, Xudong Sun, Cho-Ying Wu, Panqu Wang, and Ulrich Neumann. Grid-gcn for fast and scalable point cloud learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition(CVPR), 2020.
[30] Y. Xu, T. Fan, M. Xu, L. Zeng, and Y. Qiao. Spidercnn: Deep learning on point sets with parameterized convolutional filters. In The European Conference on Computer Vision (ECCV), September 2018.
[31] X. Yan, C. Zheng, Z. Li, S. Wang, and S. Cui. Pointasnl: Robust point clouds processing using nonlocal neural networks with adaptive sampling. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition(CVPR), 2020.
[32] J. Yang, Q. Zhang, B. Ni, L. Li, J. Liu, M. Zhou, and Q. Tian. Modeling point clouds with self-attention and gumbel subset sampling. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition(CVPR), 2019.
[33] T. Yu, J. Meng, and J. Yuan. Multi-view harmonized bilinear network for 3d object recognition. IEEE Conference of Computer Vision and Pattern Recognition (CVPR), 2018.
[34] M. Zaheer, S. Kottur, S. Ravanbakhsh, B. Poczos, and A. J. Salakhutdinov, R. R.and Smola. Deep sets. Advances in Neural Information Processing Systems(NeurIPS), 2017.
[35] Y Zhang and M Rabbat. A graph-cnn for 3d point cloud classification. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, 2018.
11

[36] H. Zhao, L. Jiang, C.-W. Fu, and J. Jia. Pointweb: Enhancing local neighborhood features for point cloud processing. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition(CVPR), 2019.
[37] Yin Zhou and Oncel Tuzel. Voxelnet: End-to-end learning for point cloud based 3d object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4490­4499, 2018.
[38] S. Zulqarnain Gilani and A. Mian. Learning from millions of 3d scans for large-scale 3d face recognition. IEEE Conference of Computer Vision and Pattern Recognition (CVPR), 2019.
12

