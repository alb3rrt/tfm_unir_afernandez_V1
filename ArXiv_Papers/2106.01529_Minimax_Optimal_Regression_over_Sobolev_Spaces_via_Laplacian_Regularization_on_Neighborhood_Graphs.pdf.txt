Minimax Optimal Regression over Sobolev Spaces via Laplacian Regularization on Neighborhood Graphs
Alden Green Sivaraman Balakrishnan Ryan J. Tibshirani Department of Statistics and Data Science Carnegie Mellon University

{ajgreen,siva,ryantibs}@stat.cmu.edu

arXiv:2106.01529v1 [math.ST] 3 Jun 2021

June 4, 2021

Abstract
In this paper we study the statistical properties of Laplacian smoothing, a graph-based approach to nonparametric regression. Under standard regularity conditions, we establish upper bounds on the error of the Laplacian smoothing estimator f , and a goodness-of-fit test also based on f . These upper bounds match the minimax optimal estimation and testing rates of convergence over the first-order Sobolev class H1(X ), for X  Rd and 1  d < 4; in the estimation problem, for d = 4, they are optimal modulo a log n factor. Additionally, we prove that Laplacian smoothing is manifold-adaptive: if X  Rd is an m-dimensional manifold with m < d, then the error rate of Laplacian smoothing (in either estimation or testing) depends only on m, in the same way it would if X were a full-dimensional set in Rm.

1 Introduction

We adopt the standard nonparametric regression setup, where we observe samples (X1, Y1), . . . , (Xn, Yn) that are i.i.d. draws from the model

Yi = f0(Xi) + i, i  N (0, 1),

(1)

where i is independent of Xi. Our goal is to perform statistical inference on the unknown regression function f0, by which we mean either estimating f0 or testing whether f0 = 0, i.e., whether there is any signal present.

Laplacian smoothing [Smola and Kondor, 2003] is a penalized least squares estimator, defined over a

graph. Letting G = (V, W ) be a weighted undirected graph with vertices V = {1, . . . , n}, associated with

{X1, . . . , Xn}, and W  Rn×n is the (weighted) adjacency matrix of the graph. the Laplacian smoothing
estimator f is given by
n

f = argmin (Yi - fi)2 +  · f Lf.

(2)

f Rn i=1

Here L is the graph Laplacian matrix (defined formally in Section 3), G is typically a geometric graph (such as a k-nearest-neighbor or neighborhood graph),   0 is a tuning parameter, and the penalty

1 f Lf =
2

n

Wij (fi - fj )2

i,j=1

encourages fi  fj when Xi  Xj. Assuming (2) is a reasonable estimator of f0, the statistic

1 T=
n

f

2 2

(3)

1

is in turn a natural test statistic to test if f0 = 0.
Of course there are many methods for nonparametric regression (see, e.g., Gy¨orfi et al. [2006], Wasserman [2006], Tsybakov [2008]), but Laplacian smoothing has its own set of advantages. For instance:
· Computational ease. Laplacian smoothing is fast, easy, and stable to compute. The estimate f can be computed by solving a symmetric diagonally dominant linear system. There are by now various nearly-linear-time solvers for this problem (see e.g., the seminal papers of Spielman and Teng [2011, 2013, 2014], or the overview by Vishnoi [2012] and references therein).
· Generality. Laplacian smoothing is well-defined whenever one can associate a graph with observed responses. This generality lends itself to many different data modalities, e.g., text and image classification, as in Kondor and Lafferty [2002], Belkin and Niyogi [2003], Belkin et al. [2006].
· Weak supervision. Although we study Laplacian smoothing in the supervised problem setting (1), the method can be adapted to the semi-supervised or unsupervised settings, as in Zhu et al. [2003], Zhou et al. [2005], Nadler et al. [2009].
For these reasons, a body of work has emerged that analyzes the statistical properties of Laplacian smoothing, and graph-based methods more generally. Roughly speaking, this work can be divided into two categories, based on the perspective they adopt.
· Fixed design perspective. Here one treats the design points X1, . . . , Xn and the graph G as fixed, and carries out inference on f0(Xi), i = 1, . . . , n. In this problem setting, tight upper bounds have been derived on the error of various graph-based methods (e.g., Wang et al. [2016], Hu¨tter and Rigollet [2016], Sadhanala et al. [2016, 2017], Kirichenko and van Zanten [2017], Kirichenko et al. [2018]) and tests (e.g., Sharpnack and Singh [2010], Sharpnack et al. [2013a,b, 2015]), which certify that such procedures are optimal over "function" classes (in quotes because these classes really model the n-dimensional vector of evaluations). The upside of this work is its generality: in this setting G need not be a geometric graph, but in principle it could be any graph over V = {1, . . . , n}. The downside is that, in the context of nonparametric regression, it is arguably not as natural to think of the evaluations of f0 as exhibiting smoothness over some fixed pre-defined graph G, and more natural to speak of the smoothness of the function f0 itself.
· Random design perspective. Here one treats the design points X1, . . . , Xn as independent samples from some distribution P supported on a domain X  Rd. Inference is drawn on the regression function f0 : X  R, which is typically assumed to be smooth in some continuum sense, e.g., it possesses a first derivative bounded in L (Ho¨lder) or L2 (Sobolev) norm. To conduct graph-based inference, the user first builds a neighborhood graph over the random design points--so that Wij is large when Xi and Xj are close in (say) Euclidean distance--and then computes e.g., (2) or (3). In this context, various graph-based procedures have been shown to be consistent: as n  , they converge to a continuum limit (see Belkin and Niyogi [2007], von Luxburg et al. [2008], Garc´ia Trillos and Slepcev [2018b] among others). However, until recently such statements were not accompanied by error rates, and even so, such error rates as have been proved [Lee et al., 2016, Garc´ia Trillos and Murray, 2020] are not optimal over continuum function spaces, such as H¨older or Sobolev classes.
The random design perspective bears a more natural connection with nonparametric regression (the focus in this paper), as it allows us to formulate smoothness based on f0 itself (how it behaves as a continuum function, and not just its evaluations at the design points). In this paper, we will adopt the random design perspective, and seek to answer the following question:
When we assume the regression function f0 is smooth in a continuum sense, does Laplacian smoothing achieve optimal performance for estimation and goodness-of-fit testing?
This is no small question--arguably, it is the central question of nonparametric regression--and without an answer one cannot fully compare the statistical properties of Laplacian smoothing to alternative methods. It
2

also seems difficult to answer: as we discuss next, there is a fundamental gap between the discrete smoothness imposed by the penalty f Lf in problem (2) and the continuum smoothness assumed on f0, and in order to obtain sharp upper bounds we will need to bridge this gap in a suitable sense.

2 Summary of Results

Advantages of the Discrete Approach. In light of the potential difficulty in bridging the gap between discrete and continuum notions of smoothness, it is worth asking whether there is any statistical advantage to solving a discrete problem such as (2) (setting aside computational considerations for the moment). After all, we could have instead solved the following variational problem:

n

f = argmin

Yi - f (Xi) 2 + 

f (x)

2 2

dx,

(4)

f :X R i=1

X

where the optimization is performed over all continuous functions f that have a weak derivative f in L2(X ). Analogously, for testing, we could use:

T=

f

2 n

:=

1 n

n

f (Xi)2.

(5)

i=1

The penalty term in (4) leverages the assumption that f0 has a smooth derivative in a seemingly natural way. Indeed, the estimator f and statistic T are well-known: for d = 1, f is the familiar smoothing spline, and for d > 1, it is a type of thin-plate spline. The statistical properties of smoothing and thin-plate splines are well-understood [van de Geer, 2000, Liu et al., 2019]. As we discuss later, the Laplacian smoothing problem (2) can be viewed as a discrete and noisy approximation to (4). At first blush, this suggests that Laplacian smoothing should at best inherit the statistical properties of (4), and at worst may have meaningfully larger error.

However, as we shall see the actual story is quite different: remarkably, Laplacian smoothing enjoys optimality
properties even in settings where the thin-plate spline estimator (4) is not well-posed (to be explained
shortly); Tables 1 and 2 summarize. As we establish in Theorems 1-5, when computed over an appropriately
formed neighborhood graph, Laplacian smoothing estimators and tests are minimax optimal over first-order continuum Sobolev balls. This holds true either when X  Rd is a full-dimensional domain and d = 1, 2, or 3, or when X is a manifold embedded in Rd of intrinsic dimension m = 1, 2, or 3. Additionally, the estimator f is nearly minimax optimal (to within a (log n)1/3 factor) when d = 4 (or m = 4 in the manifold case).

By contrast, smoothing splines are optimal only when d = 1. When d > 1, the thin-plate spline estimator (4)

is not even well-posed, in the following sense: for any (X1, Y1), . . . , (Xn, Yn) and any  > 0, there exists (e.g., Green and Silverman [1993] give a construction using "bump" functions) a differentiable function f such that

f (Xi) = Yi, i = 1, . . . , n, and

f (x)

2 2

 .

X

In other words, f achieves perfect (zero) data loss and arbitrarily small penalty in the problem (4). This will

clearly not lead to a consistent estimator of f0 across the design points (as it always yields Yi at each Xi). In this light, our results when d > 1 favorably distinguish Laplacian smoothing from its natural variational

analog.

Future Directions. To be clear, there is still much left to be investigated. For one, the Laplacian smoothing estimator f is only defined at X1, . . . , Xn. In this work we study its in-sample mean squared error

f - f0

2 n

:=

1 n

n

2
fi - f0(Xi) .

(6)

i=1

3

Dimension
d=1 d = 2, 3 d=4 d5

Laplacian
smoothing (2) n-2/3 n-2/(2+d) n-1/3(log n)1/3 (log n/n)4/(3d)

Thin-plate splines (4) n-2/3 1 1 1

Table 1: Summary of estimation rates over first-order Sobolev balls. Black font marks new results from this paper, red font marks previously-known results; bold font marks minimax optimal rates. Although we suppress it for simplicity, in all cases the dependence of the error rate on the radius of the Sobolev ball is also optimal. The rates for thin-plate splines with d  2 assume the estimator f interpolates the responses, f (Xi) = Yi for i = 1, . . . , n; see the discussion in Section 2. Here, we use "1" to indicate inconsistency (error not converging to 0). Lastly, when X is an m-dimensional manifold embedded in Rd, all Laplacian smoothing results hold with d replaced by m, without any change to the method itself.

Dimension
d=1 d = 2, 3 d4

Laplacian
smoothing (3) n-4/5 n-4/(4+d) n-1/2

Thin-plate
splines (5) n-4/5 n-1/2 n-1/2

Table 2: Summary of testing rates over first-order Sobolev balls; black, red, and bold fonts are used as in
Table 1. The rates for thin-plate splines with d  2 assume the test statistic T is computed using an f that interpolates the responses, f (Xi) = Yi for i = 1, . . . , n. Rates for d  4 assume that f0  L4(X , M ). Lastly, when X is an m-dimensional manifold embedded in Rd, all rates hold with d replaced by m.

In Section 4, we discuss how to extend f to a function over all X , in such a way that the out-of-sample mean

squared error

f - f0

2 L2(X )

should remain small, but leave a formal analysis to future work.

In a different direction, problem (4) is only a special, first-order case of thin-plate splines. In general, the kth order thin-plate spline estimator is defined as

n

f = argmin

Yi - f (Xi) 2 + 

Df (x) 2 dx,

f :X Rd i=1

||=k X

where for each multi-index  = (1, . . . , d) we write Df (x) = kf /x1 1 · · · xd d . This problem is in general well-posed whenever 2k > d. In this regime, assuming that the kth order partial derivatives Df0 are all L2(X ) bounded, the degree k thin-plate spline has error on the order of n-2k/(2k+d) [van de Geer, 2000],
which is minimax rate-optimal for such functions. Of course, assuming f0 has k bounded derivatives for some 2k > d is a very strong condition, but at present we do not know if (adaptations of) Laplacian smoothing on
neighborhood graphs achieve these rates.

Notation. For an integer p  1, we use Lp(X ) for the set of functions f such that

f

p Lp(X )

:=

|f (x)|p dx < ,

X

and Cp(X ) for the set of functions that are p times continuously differentiable. For sequences an, bn, we write an bn to mean an  Cbn for a constant C > 0 and large enough n, and an bn to mean an bn and bn an. Lastly, we use a  b = min{a, b}.

4

3 Background
Before we present our main results in Section 4, we define neighborhood graph Laplacians, and review known minimax rates over first-order Sobolev spaces.

Neighborhood Graph Laplacians. In the graph-based approach to nonparametric regression, we first build a neighborhood graph Gn,r = (V, W ), for V = {1, . . . , n}, to capture the geometry of P (the design distribution) and X (the domain) in a suitable sense. The n × n weight matrix W = (Wij) encodes proximity between pairs of design points; for a kernel function K : [0, )  R and radius r > 0, we have

Wij = K

Xi - Xj 2 , r

with · 2 denoting the 2 norm on Rd. Defining D as the n × n diagonal matrix with entries Dii =

n j=1

Wij

,

the graph Laplacian can then be written as

L = D - W.

(7)

We use L =

n k=1

k vk

vk

for an eigendecomposition of L, and we always assume, by convention, ordered

eigenvalues 0 = 1  · · ·  n, and unit-norm eigenvectors.

Sobolev Spaces. We step away from graph-based methods for a moment, to briefly recall some classical results regarding minimax rates over Sobolev classes. We say that a function f  L2(X ) belongs to the first-order Sobolev space H1(X ) if, for each j = 1, . . . , d, the weak partial derivative Djf exists and belongs to L2(X ). For such functions f  H1(X ), the Sobolev seminorm |f |H1(X ) is the average size of the gradient f = (D1f, . . . , Ddf ),

|f |2H1(X ) :=

f (x)

2 2

dx,

X

with corresponding Sobolev norm

f H1(X ) := f L2(X ) + |f |H1(X ). The Sobolev ball H1(X , M ) for M > 0 is

H1(X , M ) :=

f  H1(X ) :

f

2 H1(X )



M2

.

For further details regarding Sobolev spaces see, e.g., Evans [2010], Leoni [2017].

Minimax Rates. To carry out a minimax analysis of regression in Sobolev spaces, one must impose regularity conditions on the design distribution P . We shall assume the following. (P1) P is supported on a domain X  Rd, which is an open, connected set with Lipschitz boundary.
(P2) P admits a density p such that

0 < pmin  p(x)  pmax < , for all x  X .

Additionally, p is Lipschitz on X , with Lipschitz constant Lp.

Under conditions (P1), (P2), the minimax estimation rate over a Sobolev ball of radius M  n-1/2 is (e.g., Tsybakov [2008]):

inf

sup

E

f - f0

2 L2(X )

M 2d/(2+d)n-2/(2+d).

(8)

f f0H1(X ,M )

5

(Throughout we assume M  n-1/2, as otherwise the trivial estimator f = 0 achieves smaller error than the parametric rate n-1, and the problem does not fit well within the nonparametric setup.)
As minimax rates in nonparametric hypothesis testing are (comparatively) less familiar than those in nonparametric estimation, we briefly summarize the main idea before stating the optimal error rate. In the goodness-of-fit testing problem, we ask for a test function--formally, a Borel measurable function  taking values in {0, 1}--which can distinguish between the hypotheses

H0 : f0 = f0 , versus Ha : f0  F \ {f0 }.

(9)

Typically, the null hypothesis f0 = f0  F reflects the absence of interesting structure, and F \ {f0 } is a set of smooth departures from this null. In this paper, as in Ingster and Sapatinas [2009], we focus on the problem of signal detection in Sobolev spaces, where f0 = 0 and F = H1(X , M ) is a first-order Sobolev ball. This is without loss of generality since our test statistic and its analysis are easily modified to handle the case when f0 is not 0, by simply subtracting f0 (Xi) from each observation Yi.
The Type I error of a test  is E0[], and if E0[]   for a given   (0, 1) we refer to  as a level- test. The worst-case risk of  over F is

Rn(, F , ) := sup Ef0 [1 - ] : f0  F , f0 L2(X ) > ,
and for a given constant b  1, the minimax critical radius (F) is the smallest value of such that some level- test has worst-case risk of at most 1/b. Formally,

(F ) := inf > 0 : inf Rn(, F , )  1/b ,


where in the above the infimum is over all level- tests , and Ef0 [·] is the expectation operator under the regression function f0.1

The classical approach to hypothesis testing typically focuses on designing test statistics, and studying their (limiting) distribution in order to ensure control of the Type I error. In many cases the Type II error (or risk in our terminology) is not emphasized, or the risk of the test against fixed or directional alternatives (i.e. alternatives which deviate from the null in a fixed direction) is studied. In contrast, in the minimax paradigm the (uniform or worst-case) risk against a large collection of alternatives is the central focus. See Ingster [1982, 1987], Ingster and Suslina [2012], Arias-Castro et al. [2018], Balakrishnan and Wasserman [2019, 2018] for a more extended treatment of the minimax paradigm in nonparametric testing, and for a discussion of its advantages (and disadvantages) over other approaches to studying hypothesis tests.

Testing f0 = 0 is an easier problem than estimating f0, and hence the minimax testing critical radius over H1(X , M ) is smaller than the minimax estimation rate, for 1  d < 4 (see Ingster and Sapatinas

[2009]):

2 H 1(X , M ) M 2d/(4+d)n-4/(4+d).

(10)

When d  4 the functions in H1(X ) are very irregular; formally speaking H1(X ) does not continuously embed into L4(X ) when d  4, and the minimax testing rates in this regime are unknown.

4 Minimax Optimality of Laplacian Smoothing
We now formalize the main conclusions of this paper: that Laplacian smoothing methods on neighborhood graphs are minimax rate-optimal over first-order continuum Sobolev classes. We will assume (P1), (P2) on P , and the following condition on the kernel K.
1Clearly, the minimax critical radius depends on  and b. However, we adopt the typical convention of treating   (0, 1) and b  1 as small but fixed positive constants; hence they will not affect the testing error rates, and we suppress them notationally.

6

(K1) K : [0, )  [0, ) is a nonincreasing function supported on [0, 1], its restriction to [0, 1] is Lipschitz, and K(1) > 0. Additionally, it is normalized so that

K( z 2) dz = 1.
Rd

We

assume

K

=

1 d

Rd

x

2 2

K

(

x

2) dx < .

This is a mild condition: recall the choice of kernel is under the control of the user, and moreover (K1) covers many common kernel choices.

Estimation Error of Laplacian Smoothing. Under these conditions, the Laplacian smoothing estimator f achieves an error rate that matches the minimax lower bound over H1(X , M ). This statement will hold whenever the graph Gn,r is computed with radius r in the following range.
(R1) For constants C0, c0 > 0, the neighborhood graph radius r satisfies

1

C0

log n n

d



r



c0



M

d-4 4+2d

n . -

3 4+2d

Next we state Theorem 1, our main estimation result. Its proof, as with all proofs of results in this paper, can be found in the appendix.
Theorem 1. Given i.i.d. draws (Xi, Yi), i = 1, . . . , n from (1), assume f0  H1(X , M ) where X  Rd has dimension d < 4 and M  n1/d. Assume (P1), (P2) on the design distribution P , and assume the neighborhood graph Gn,r is computed with a kernel K satisfying (K1). There are constants N, C, C1, c, c1 > 0 (not depending on f0) such that for any n  N , and any radius r as in (R1), the Laplacian smoothing estimator f in (2) with  = M -4/(2+d)(nrd+2)-1n-2/(2+d) satisfies

f - f0

2 n



C M 2d/(2+d)n-2/(2+d), 

with probability at least 1 -  - C1n exp(-c1nrd) - exp(-c(M 2n)d/(2+d)).
To summarize: for d = 1, 2, or 3, with high probability, the Laplacian smoothing estimator f has in-sample mean squared error that is within a constant factor of the minimax error. Some remarks:
· The first-order Sobolev space H1(X ) does not continuously embed into C0(X ) when d > 1 (in general, the kth order space Hk(X ) does not continuously embed into C0(X ) except if 2k > d). For this reason, one really cannot speak of pointwise evaluation of a Sobolev function f0  H1(X ) when d > 1 (as we do in Theorem 1 by defining our target of estimation to be f0(Xi), i = 1, . . . , n). We can resolve this by appealing to what are known as Lebesgue points, as explained in Appendix A.
· The assumption M  n1/d ensures that the upper bound provided in the theorem is meaningful (i.e., ensures it is of at most a constant order).

· The lower bound on r imposed in condition (R1) is compatible with practice, where by far the most common choice of radius is the connectivity threshold r (log(n)/n)1/d, which makes Gn,r as sparse as possible while still being connected, for maximum computational efficiency. The upper bound may
seem a bit more mysterious--we need it for technical reasons to ensure that f does not overfit, but we
note that as a practical matter one rarely chooses r to be so large anyway.

· It is possible to extend f to be defined on all of X and then evaluate the error of such an extension (as measured against f0) in L2(X ) norm. When f and f0 are suitably smooth, tools from empirical
process theory (see e.g., Chapter 14 of Wainwright [2019]) or approximation theory (e.g., Section 15.5 of Johnstone [2011]) guarantee that the L2(X ) error is not too much greater than its in-sample counterpart.

7

In fact, as shown in Appendix G.1, if f0 is Lipschitz smooth and we extend f to be piecewise constant over the Voronoi tessellation induced by X1, . . . , Xn, then the out-of-sample error f - f0 L2(X ) is within a negligible factor of the in-sample error f - f0 n. We leave analysis of the Sobolev case to future work.

· When f0 is Lipschitz smooth, we can also replace the factor of  in the high probability bound by a factor of 2/n, which is always smaller than  when   (0, 1).

When d = 4, our analysis results in an upper bound for the error of Laplacian smoothing that is within a (log n)1/3 factor of the minimax error rate. But when d  5, our upper bounds do not match the minimax
rates.

Theorem 2. Under the assumptions of Theorem 1, if instead X has dimension d = 4, r (log n/n)1/4 and  = M -2/3(nr6)-1(log n/n)1/3, then we obtain

f - f0

2 n



C M 4/3 

log n n

1/3
,

with the same probability guarantee as in Theorem 1. If the dimension of X is d  5, r (log n/n)1/d and  = M -2/3(nr2+d)-1n-4/(3d), then

f - f0

2 n



C M 4/3 

log n n

4/(3d)
,

again with the same probability guarantee.

This mirrors the conclusions of Sadhanala et al. [2016] who investigate estimation rates of Laplacian smoothing over the d-dimensional grid graph. These authors argue that their analysis is tight, and that it is likely the estimator, not the analysis, that is deficient when d  5. Formalizing such a claim turns out to be harder in the random design setting than in the fixed design setting, and we leave it for future work.

However, we do investigate the matter empirically. In Figure 1, we study the (in-sample) mean squared error
of the Laplacian smoothing estimator as the dimension d grows. Here X1, . . . , Xn are sampled uniformly over X = [-1, 1]d, and the regression function is taken as f0(x)  di=1 cos(axi), where a = 2 for d = 2, and a = 1 for d  3. This regression function f0 is quite smooth, and for d = 2 and d = 3 Laplacian smoothing appears to achieve or exceed the minimax rate. When d = 4, Laplacian smoothing appears modestly suboptimal; this fits with our theoretical upper bound, which includes a (log n)1/3 factor that plays a non-negligible role for
these problem sizes (n = 1000 to n = 10000). On the other hand, when d = 5, Laplacian smoothing seems to
be decidedly suboptimal.

Testing Error of Laplacian Smoothing. For a given 0 <  < 1, define a threshold t as

1n

1

1

t = n k=1 (k + 1)2 + n

2n

1

 k=1 (k + 1)4 ,

where we recall k is the kth smallest eigenvalue of L. The Laplacian smoothing test is then simply

 = 1 T > t .

We show in Appendix B that f is a level- test. In the next theorem, we upper bound the worst-case risk Rn(, H1(X , M ), ) of , whenever is at least (a constant times) the critical radius given in (10). For this to hold, we will require a tighter range of scalings for the graph radius r.

(R2) For constants C0, c0 > 0, the neighborhood graph radius r satisfies

1

log n d

(d-8) d-20

C0 n

 r  c0  M 8+2d n . 32+8d

8

d = 2. Minimax slope = -2/4.

d = 3. Minimax slope = -2/5.

d = 4. Minimax slope = -2/6.

d = 5. Minimax slope = -2/7.

Mean squared error
0.05 0.10 0.20 0.50 1.00
Mean squared error
0.05 0.10 0.20 0.50 1.00
Mean squared error
0.05 0.10 0.20 0.50 1.00
Mean squared error
0.05 0.10 0.20 0.50 1.00

LS [Slope = -0.61].

1000

2000

5000

Sample size

10000

LS [Slope = -0.41].

1000

2000

5000

Sample size

10000

LS [Slope = -0.27].

1000

2000

5000

Sample size

10000

LS [Slope = -0.15].

1000

2000

5000

Sample size

10000

Figure 1: Mean squared error of Laplacian smoothing (LS) as a function of sample size n. Each plot is on the log-log scale, and the results are averaged over 5 repetitions, with Laplacian smoothing tuned for optimal average mean squared error. The black line shows the minimax rate (in slope only; the intercept is chosen to match the observed error).

We will also require that the radius of the Sobolev class not be too large.

M  Mmax(d), where we define

n1/8

d=1

Mmax(d) := n(4-d)/(4d) d  2.

Precisely, we will require

We now give Theorem 3, our main testing result.
Theorem 3. Given i.i.d. draws (Xi, Yi), i = 1, . . . , n from (1), assume f0  H1(X , M ) where X  Rd with d < 4, and M  Mmax(d). Assume (P1), (P2) on the design distribution P , and assume Gn,r is computed with a kernel K satisfying (K1). There exist constants N, C, C1, c1 > 0 such that for any n  N , and any radius r as in (R2), the Laplacian smoothing test  based on the estimator f in (2), with  = (nrd+2)-1n-4/(4+d)M -8/(4+d), satisfies the following: for any b  1, if

2  CM 2d/(4+d)n-4/(4+d) b2 + b

1 ,



(11)

then the worst-case risk satisfies the upper bound: Rn(, H1(X , M ), )  C/b + C1n exp(-c1nrd) .

Some remarks:

· As mentioned earlier, Sobolev balls H1(X , M ) for d  4 include quite irregular functions f  L4(X ).

Proving tight lower bounds in this case is nontrivial, and as far as we understand such an analysis

remains outstanding. On the other hand, if we explicitly assume that f0  L4(X , M ), then Guerre

and Lavergne [2002] show that the testing problem is characterized by a dimension-free lower bound

2(L4(X , M )) n-1/2. Moreover, by setting  = 0 so that the resulting estimator f interpolates the

responses Y1, . . . , Yn, the subsequent test  will achieve (up to constants) this lower bound. That is, for

any f0  L4(X , M ) such that

f0

2 L2(X )



C (b2

+

1/)n-1/2, we have that E0[]   and

C(1 + M 4)

Ef0 1 -  

b2

.

(12)

· To compute the data-dependent threshold t, one must know all of the eigenvalues 1, . . . , n. Computing all these eigenvalues is far more expensive (cubic-time) than computing T in the first place (nearlylinear-time). But in practice we would not recommend using t anyway, and would instead we make the standard recommendation to calibrate via a permutation test [Hoeffding, 1952]. Recent work Kim et al. [2020], has shown that in a variety of closely related settings, calibration of a test statistic via the

9

permutation test often retains minimax-optimal power, and we expect similar results to hold for the Laplacian smoothing-based test statistic.

More Discussion of Variational Analog. With some results in hand, let us pause to offer some explanation of why Laplacian smoothing can be optimal in settings where thin-plate splines are not even consistent. First, we elaborate on why this difference in performance is so surprising. As mentioned previously, the penalties in (2), (4) can be closely tied together: Bousquet et al. [2004] show that for f  C2(X ),

1

lim n2rd+2 f

Lf =

f (x) · P f (x)p(x) dx
X

(13)

=

f (x)

2 2

p2(x)

dx.

X

In the above, the limit is as n   and r  0, P is the (weighted) Laplace-Beltrami operator

P f

:=

1 - div
p

p2f ),

and the second equality follows using integration by parts.2 To be clear, this argument does not formally
imply that the Laplacian eigenmaps estimator f and the thin-plate spline estimator f are close (for one, note that (13) holds for f  C2(X ), whereas the optimization in (4) considers a much broader set of continuous functions with weak derivatives in L2(X )). But it does seem to suggest that the two estimators should behave
somewhat similarly.

Of course, we know this is not the case: f and f look very different when d > 1. What is driving this
difference? The key point is that the discretization imposed by the graph Gn,r--which might seem problematic at first glance--turns out to be a blessing. The problem with (4) is that the class H1(X ), which fundamentally
underlies the criterion, is far "too big" for d > 1. This is meant in various related senses. By the Sobolev embedding theorem, for d > 1, the class H1(X ) does not continuously embed into any Ho¨lder space; and in fact it does not even continuously embed into C0(X ). Thus we cannot really restrict the optimization to
continuous and weakly differentiable functions, as we could when d = 1 (the smoothing spline case), without throwing out a substantial subset of functions in H1(X ). Even among continuous and differentiable functions
f , as we explained previously, we can use "bump" functions (as in Green and Silverman [1993]) to construct
f that interpolates the pairs (Xi, Yi), i = 1, . . . , n and achieves arbitrarily small penalty (and hence criterion) in (4). In this sense, any estimator resulting from solving (4) will clearly be inconsistent.

On the other hand, problem (2) is finite-dimensional. As a result f has far less capacity to overfit than does

f , for any given sample size n. Discretization is not the only way to make the problem (4) more tractable:

for instance, one can replace the penalty

X

f (x)

2 2

dx

with

a stricter

choice like ess supxX

f (x) 2, or

conduct the optimization over some finite-dimensional linear subspace of H1(X ) (i.e., use a sieve). While

these solutions do improve the statistical properties of f for d > 1 (see e.g., Birg´e and Massart [1993, 1998],

van de Geer [2000]), Laplacian smoothing is generally speaking much simpler and more computationally

friendly. In addition, the other approaches are usually specifically tailored to the domain X , in stark contrast

to f .

Overview of Analysis. The comparison with thin-plate splines highlights some surprising differences between f and f . Such differences also preclude us from analyzing f by, say, using (13) to establish a coupling between f and f --we know this cannot work, because we would like to prove meaningful error bounds on f in regimes where no such bounds exist for f .
Instead we take a different approach, and directly analyze the error of f and T using a bias-variance
2Assuming f satisfies Neumann boundary conditions.

10

decomposition (conditional on X1, . . . , Xn). A standard calculation shows that

f - f0

2 n



2 n

f0 Lf0

10 n

1

+ n k=1 (k + 1)2 ,

bias

variance

and likewise that  has small risk whenever

f0

2 n



2 n

f0 Lf0

2 +

2/ + 2b n

n

1

k=1 (k + 1)4 .

bias

variance

The bias and variance terms are each functions of the random graph Gn,r, and hence are themselves random. To upper bound them, we build on some recent works [Burago et al., 2014, Garc´ia Trillos et al., 2019, Calder and Garc´ia Trillos, 2019] regarding the consistency of neighborhood graphs to establish the following lemmas. These lemmas assume (P1), (P2) on the design distribution P , and (K1) on the kernel used to compute the neighborhood graph Gn,r.

Lemma 1. There are constants N, C2 > 0 such that for n  N , r  c0, and f  H1(X ), with probability at

least 1 - , it holds that

f

Lf



C2 

n2

rd+2

|f

|2H

1

(X

)

.

(14)

Lemma 2. There are constants N, C1, C3, c1, c3 > 0 such that for n  N and C0(log n/n)1/d  r  c0, with probability at least 1 - C1n exp(-c1nrd), it holds that

c3An,r(k)  k  C3An,r(k), for 2  k  n,

(15)

where An,r(k) = min{nrd+2k2/d, nrd}.
Lemma 1 gives a direct upper bound on the bias term. Lemma 2 leads to a sufficiently tight upper bound on the variance term whenever the radius r is sufficiently small; precisely, when r is upper bounded as in (R1) for estimation, or (R2) for testing. The parameter  is then chosen to minimize the sum of these upper bounds on bias and variance, as usual, and some straightforward calculations give Theorems 1-3.

It may be useful to give one more perspective on our approach. A common strategy in analyzing penalized least squares estimators is to assume two properties: first, that the regression function f0 lies in (or near) a ball defined by the penalty operator; second, that this ball is reasonably small, e.g., as measured by metric entropy, or Rademacher complexity, etc. In contrast, in Laplacian smoothing, the penalty induces a ball
H1(Gn,r, M ) := {f : f Lf  M 2}
that is data-dependent and random, and so we do not have access to either of the aforementioned properties a priori, and instead, must prove they hold with high probability. In this sense, our analysis is different than the typical one in nonparametric regression.

5 Manifold Adaptivity
The minimax rates n-2/(2+d) and n-4/(4+d), in estimation and testing, suffer from the curse of dimensionality. However, in practice it can be often reasonable to assume a manifold hypothesis: that the data X1, . . . , Xn lie on a manifold X of Rd that has intrinsic dimension m < d. Under such an assumption, it is known [Bickel and Li, 2007, Arias-Castro et al., 2018] that the optimal rates over H1(X ) are now n-2/(2+m) (for estimation) and n-4/(4+m) (for testing), which are much faster than the full-dimensional error rates when m d.

11

On the other hand, a theory has been developed [Belkin, 2003, Belkin and Niyogi, 2008, Niyogi et al., 2008,
Niyogi, 2013, Balakrishnan et al., 2012, 2013] establishing that the neighborhood graph Gn,r can "learn" the manifold X in various senses, so long as X is locally linear. We contribute to this line of work by showing that under the manifold hypothesis, Laplacian smoothing achieves the tighter minimax rates over H1(X ).

Error Rates Assuming the Manifold Hypothesis. The conditions and results presented here will be largely similar to the previous ones, except with the ambient dimension d replaced by the intrinsic dimension m. For the remainder, we assume the following.
(P3) P is supported on a compact, connected, smooth manifold X embedded in Rd, of dimension m  d. The manifold is without boundary and has positive reach [Federer, 1959].
(P4) P admits a density p with respect to the volume form of X such that

0 < pmin  p(x)  pmax < , for all x  X .

Additionally, p is Lipschitz on X , with Lipschitz constant Lp.
Under the assumptions (P3), (P4), and (K1), and for a suitable range of r, the error bounds on the estimator f and test  will depend on m instead of d.

(R4) For constants C0, c0 > 0, the neighborhood graph radius r satisfies

1

log n m

(m-4)

-3

C0 n

 r  c0  M (4+2m) n (4+2m) .

Theorem 4. As in Theorem 1, but where X  Rd is a manifold with intrinsic dimension m < 4, the design distribution P obeys (P3), (P4), and M  n1/m. There are constants N, C, c > 0 (not depending on f0) such
that for any n  N , and any r as in (R4), the Laplacian smoothing estimator f in (2), with L = Ln,r and  = M -4/(2+m)(nrm+2)-1n-2/(2+m), satisfies

f - f0

2 n



C M 2m/(2+m)n-2/(2+m), 

with probability at least 1 -  - Cn exp(-cnrm) - exp(-c(M 2n)m/(2+m)).
In a similar vein, we obtain results for manifold adaptive testing under the following condition on the graph radius parameter.
(R5) For constants C0, c0 > 0, the neighborhood graph radius r satisfies

1

log n m

(m-8) m-20

C0 n

 r  c0  M 8+2m n . 32+8m

Theorem 5. As in Theorem 3, but where X  Rd is a manifold with intrinsic dimension m < 4, M  Mmax(m), and the design distribution P obeys (P3), (P4). There are constants N, C, c > 0 such that for any n  N , and any r as in (R5), the Laplacian smoothing test  based on the estimator f in (2), with  = (nrm+2)-1n-4/(4+m)M -8/(4+m), satisfies the following: for any b  1, if

2  CM 2m/(4+m)n-4/(4+m) b2 + b

1 ,



(16)

then the worst-case risk satisfies the upper bound: Rn(, H1(X , M ), )  C/b + Cn exp(-cnrm).

12

The proofs of Theorems 4 and 5 proceed in a similar manner to that of Theorems 1 and 3. The key difference is that in the manifold setting, the equations (14) and (15) used to upper bound bias and variance will hold with d replaced by m.
We emphasize that little about X need be known for Theorems 4 and 5 to hold. Indeed, all that is needed is the intrinsic dimension m, to properly tune r and  (from a theoretical point of view), and otherwise f and  are computed without regard to X . In contrast, the penalty in (4) would have to be specially tailored to work in this setting, revealing another advantage of the discrete approach over the variational one.
6 Discussion
We have shown that Laplacian smoothing, computed over a neighborhood graph, can be optimal for both estimation and goodness-of-fit testing over Sobolev spaces. There are many extensions worth pursuing, and several have already been mentioned. We conclude by mentioning a couple more. In practice, it is more common to use a k-nearest-neighbor (kNN) graph than a neighborhood graph, due to the guaranteed connectivity and sparsity of the former; we suspect that by building on the work of Calder and Garc´ia Trillos [2019], one can show that our main results all hold under the kNN graph as well. In another direction, one can also generalize Laplacian smoothing by replacing the penalty f Lf with f Lsf , for an integer s > 1. The hope is that this would then achieve minimax optimal rates over the higher-order Sobolev class Hs(X ).
Acknowledgments
AG and RJT were supported by ONR grant N00014-20-1-2787. AG and SB were supported by NSF grants DMS-1713003 and CCF-1763734.
References
Ery Arias-Castro, Bruno Pelletier, and Venkatesh Saligrama. Remember the curse of dimensionality: the case of goodness-of-fit testing in arbitrary dimension. Journal of Nonparametric Statistics, 30(2):448­471, 2018.
Sivaraman Balakrishnan and Larry Wasserman. Hypothesis testing for high-dimensional multinomials: A selective review. The Annals of Applied Statistics, 12(2):727 ­ 749, 2018.
Sivaraman Balakrishnan and Larry Wasserman. Hypothesis testing for densities and high-dimensional multinomials: Sharp local minimax rates. Annals of Statistics, 47(4):1893­1927, 2019.
Sivaraman Balakrishnan, Alesandro Rinaldo, Don Sheehy, Aarti Singh, and Larry Wasserman. Minimax rates for homology inference. In International Conference on Artificial Intelligence and Statistics, volume 22, 2012.
Sivaraman Balakrishnan, Srivatsan Narayanan, Alessandro Rinaldo, Aarti Singh, and Larry Wasserman. Cluster trees on manifolds. In Advances in Neural Information Processing Systems, volume 26, 2013.
Mikhail Belkin. Problems of Learning on Manifolds. PhD thesis, University of Chicago, 2003.
Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. Neural Computation, 15(6):1373­1396, 2003.
Mikhail Belkin and Partha Niyogi. Convergence of Laplacian eigenmaps. In Advances in Neural Information Processing Systems, volume 20, 2007.
Mikhail Belkin and Partha Niyogi. Towards a theoretical foundation for Laplacian-based manifold methods. Journal of Computer and System Sciences, 74(8):1289­1308, 2008.
Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. Journal of Machine Learning Research, 7:2399­2434, 2006.
13

Mikhail Belkin, Qichao Que, Yusu Wang, and Xueyuan Zhou. Toward understanding complex spaces: Graph laplacians on manifolds with singularities and boundaries. In Shie Mannor, Nathan Srebro, and Robert C. Williamson, editors, Proceedings of the 25th Annual Conference on Learning Theory, volume 23 of Proceedings of Machine Learning Research, pages 36.1­36.26, Edinburgh, Scotland, 25­27 Jun 2012. JMLR Workshop and Conference Proceedings.
Peter J Bickel and Bo Li. Local polynomial regression on unknown manifolds. In Complex datasets and inverse problems, volume 54, pages 177­186. Institute of Mathematical Statistics, 2007.
Lucien Birg´e and Pascal Massart. Rates of convergence for minimum contrast estimators. Probability Theory and Related Fields, 97(1-2):113­150, 1993.
Lucien Birg´e and Pascal Massart. Minimum contrast estimators on sieves: exponential bounds and rates of convergence. Bernoulli, 4(3):329­375, 1998.
Olivier Bousquet, Olivier Chapelle, and Matthias Hein. Measure based regularization. In Advances in Neural Information Processing Systems, volume 16, 2004.
Dmitri Burago, Sergei Ivanov, and Yaroslav Kurylev. A graph discretization of the Laplace-Beltrami operator. Journal of Spectral Theory, 4(4):675­714, 2014.
Jeff Calder and Nicola´s Garc´ia Trillos. Improved spectral convergence rates for graph Laplacians on epsilongraphs and k-NN graphs. arXiv preprint arXiv:1910.13476, 2019.
Kamalika Chaudhuri and Sanjoy Dasgupta. Rates of convergence for the cluster tree. In Advances in Neural Information Processing Systems 23, pages 343­351. Curran Associates, Inc., 2010.
Fan RK Chung and Fan Chung Graham. Spectral graph theory. American Mathematical Soc., 1997.
Matthew M Dunlop, Dejan Slepcev, Andrew M Stuart, and Matthew Thorpe. Large data and zero noise limits of graph-based semi-supervised learning algorithms. Applied and Computational Harmonic Analysis, 49(2):655­697, 2020.
Lawrence C. Evans. Partial Differential Equations. American Mathematical Society, 2010.
Lawrence Craig Evans and Ronald F Gariepy. Measure theory and fine properties of functions. Chapman and Hall/CRC, 2015.
Herbert Federer. Curvature measures. Transactions of the American Mathematical Society, 93(3):418­491, 1959.
Nicola´s Garc´ia Trillos and Ryan W. Murray. A maximum principle argument for the uniform convergence of graph Laplacian regressors. SIAM Journal on Mathematics of Data Science, 2(3):705­739, 2020.
Nicol´as Garc´ia Trillos and Dejan Slepcev. On the rate of convergence of empirical measures in infinitytransportation distance. Canadian Journal of Mathematics, 67(6):1358­1383, 2015.
Nicol´as Garc´ia Trillos and Dejan Slepcev. A variational approach to the consistency of spectral clustering. Applied and Computational Harmonic Analysis, 45(2):239­281, 2018a.
Nicol´as Garc´ia Trillos and Dejan Slepcev. A variational approach to the consistency of spectral clustering. Applied and Computational Harmonic Analysis, 45(2):239­281, 2018b.
Nicol´as Garc´ia Trillos, Moritz Gerlach, Matthias Hein, and Dejan Slepcev. Error estimates for spectral convergence of the graph Laplacian on random geometric graphs toward the Laplace­Beltrami operator. Foundations of Computational Mathematics, 20:1­61, 2019.
Peter J. Green and Bernard W. Silverman. Nonparametric Regression and Generalized Linear Models: A Roughness Penalty Approach. Chapman & Hall/CRC Press, 1993.
14

Emmanuel Guerre and Pascal Lavergne. Optimal minimax rates for nonparametric specification testing in regression models. Econometric Theory, 18(5):1139­1171, 2002.
La´szlo´ Gyo¨rfi, Michael Kohler, Adam Krzyzak, and Harro Walk. A Distribution-Free Theory of Nonparametric Regression. Springer, 2006.
Wassily Hoeffding. The large-sample power of tests based on permutations of observations. Annals of Mathematical Statistics, 23(2):169­192, 1952.
Jan-Christian Hu¨tter and Philippe Rigollet. Optimal rates for total variation denoising. In Conference on Learning Theory, volume 29, 2016.
Yuri I. Ingster. Minimax nonparametric detection of signals in white Gaussian noise. Problems in Information Transmission, 18:130­140, 1982.
Yuri I. Ingster. Minimax testing of nonparametric hypotheses on a distribution density in the Lp metrics. Theory of Probability & Its Applications, 31(2):333­337, 1987.
Yuri I. Ingster and Theofanis Sapatinas. Minimax goodness-of-fit testing in multivariate nonparametric regression. Mathematical Methods of Statistics, 18(3):241­269, 2009.
Yuri I. Ingster and Irina A. Suslina. Nonparametric goodness-of-fit testing under Gaussian models. Springer Science & Business Media, 2012.
Iain M. Johnstone. Gaussian estimation: Sequence and wavelet models. Unpublished manuscript, 2011.
Ilmun Kim, Sivaraman Balakrishnan, and Larry Wasserman. Minimax optimality of permutation tests. arXiv preprint arXiv:2003.13208, 2020.
Alisa Kirichenko and Harry van Zanten. Estimating a smooth function on a large graph by Bayesian Laplacian regularisation. Electronic Journal of Statistics, 11(1):891­915, 2017.
Alisa Kirichenko, Harry van Zanten, et al. Minimax lower bounds for function estimation on graphs. Electronic Journal of Statistics, 12(1):651­666, 2018.
Risi Kondor and John Lafferty. Diffusion kernels on graphs and other discrete structures. In International Conference on Machine Learning, volume 19, 2002.
Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model selection. Annals of Statistics, pages 1302­1338, 2000.
Ann B. Lee, Rafael Izbicki, et al. A spectral series approach to high-dimensional nonparametric regression. Electronic Journal of Statistics, 10(1):423­463, 2016.
Giovanni Leoni. A first Course in Sobolev Spaces. American Mathematical Society, 2017.
Meimei Liu, Zuofeng Shang, and Guang Cheng. Sharp theoretical analysis for nonparametric testing under random projection. In Conference on Learning Theory, volume 32, 2019.
Boaz Nadler, Nathan Srebro, and Xueyuan Zhou. Semi-supervised learning with the graph Laplacian: The limit of infinite unlabelled data. In Neural Information Processing Systems, volume 19, 2009.
Partha Niyogi. Manifold regularization and semi-supervised learning: Some theoretical analyses. Journal of Machine Learning Research, 14(1):1229­1250, 2013.
Partha Niyogi, Stephen Smale, and Shmuel Weinberger. Finding the homology of submanifolds with high confidence from random samples. Discrete & Computational Geometry, 39(1):419­441, 2008.
Veeranjaneyulu Sadhanala, Yu-Xiang Wang, and Ryan J Tibshirani. Total variation classes beyond 1d: Minimax rates, and the limitations of linear smoothers. In Advances in Neural Information Processing Systems, volume 29, 2016.
15

Veeranjaneyulu Sadhanala, Yu-Xiang Wang, James L Sharpnack, and Ryan J Tibshirani. Higher-order total variation classes on grids: Minimax theory and trend filtering methods. In Advances in Neural Information Processing Systems, volume 30, 2017.
James Sharpnack and Aarti Singh. Identifying graph-structured activation patterns in networks. In Advances in Neural Information Processing Systems, volume 23, 2010.
James Sharpnack, Akshay Krishnamurthy, and Aarti Singh. Near-optimal anomaly detection in graphs using Lovasz extended scan statistic. In Advances in Neural Information Processing Systems, volume 26, 2013a.
James Sharpnack, Aarti Singh, and Akshay Krishnamurthy. Detecting activations over graphs using spanning tree wavelet bases. In International Conference on Artificial Intelligence and Statistics, volume 16, 2013b.
James Sharpnack, Alessandro Rinaldo, and Aarti Singh. Detecting anomalous activity on networks with the graph Fourier scan statistic. IEEE Transactions on Signal Processing, 64(2):364­379, 2015.
Alexander J. Smola and Risi Kondor. Kernels and regularization on graphs. In Learning Theory and Kernel Machines, pages 144­158. Springer, 2003.
Daniel A. Spielman and Shang-Hua Teng. Spectral sparsification of graphs. SIAM Journal on Computing, 40 (4):981­1025, 2011.
Daniel A. Spielman and Shang-Hua Teng. A local clustering algorithm for massive graphs and its application to nearly linear time graph partitioning. SIAM Journal on Computing, 42(1):1­26, 2013.
Daniel A. Spielman and Shang-Hua Teng. Nearly linear time algorithms for preconditioning and solving symmetric, diagonally dominant linear systems. SIAM Journal on Matrix Analysis and Applications, 35(3): 835­885, 2014.
Alexandre B. Tsybakov. Introduction to Nonparametric Estimation. Springer, 2008.
Sara van de Geer. Empirical Processes in M-estimation. Cambridge University Press, 2000.
Nisheeth K. Vishnoi. Laplacian solvers and their algorithmic applications. Foundations and Trends in Theoretical Computer Science, 8(1-2):1­141, 2012.
Ulrike von Luxburg, Mikhail Belkin, and Olivier Bousquet. Consistency of spectral clustering. Annals of Statistics, 36(2):555­586, 2008.
Martin J Wainwright. High-Dimensional Dtatistics: A Non-Asymptotic Biewpoint. Cambridge University Press, 2019.
Yu-Xiang Wang, James Sharpnack, Alexander J. Smola, and Ryan J. Tibshirani. Trend filtering on graphs. Journal of Machine Learning Research, 17(1):3651­3691, 2016.
Larry Wasserman. All of Nonparametric Statistics. Springer, 2006.
Dengyong Zhou, Jiayuan Huang, and Bernhard Scholkopf. Learning from labeled and unlabeled data on a directed graph. In International Conference on Machine Learning, volume 22, 2005.
Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty. Semi-supervised learning using Gaussian fields and harmonic functions. In International Conference on Machine Learning, volume 20, 2003.
A Preliminaries
In the appendix, we provide complete proofs of all results. Our main theorems (Theorems 1-5) all follow the same general proof strategy of first establishing bounds in the fixed-design setup. In Section B, we establish (estimation or testing) error bounds which hold for any graph G; these bounds are stated with respect to (functionals of) the graph G, and allow us to upper bound the error of f and  conditional on the design
16

{X1, . . . , Xn} = {x1, . . . , xn}. In Sections C, D, E, and F we develop all the necessary probabilistic estimates on these functionals, for the particular random neighborhood graph G = Gn,r. It is in these sections where we invoke our various assumptions on the distribution P and regression function f0. In Section G, we prove our main theorems and some other results. In Section H, we state a few concentration bounds that we use
repeatedly in our proofs.

Pointwise evaluation of Sobolev functions. First, however, as promised in our main text we clarify what is meant by pointwise evaluation of the regression function f0. Strictly speaking, each f  H1(X) is really an equivalence class, defined only up to sets of Lebesgue measure 0. In order to make sense of the
evaluation x  f (x), one must therefore pick a representative f  f . When d = 1, this is resolved in a standard way--since H1(X ) embeds continuously into C0(X ), there exists a continuous version of every f  H1(X ), and we take this continuous version as the representative f . On the other hand, when d  2, the Sobolev space H1(X ) does not continuously embed into C0(X ), and we must choose representatives in
a different manner. In this case we let f be the precise representative [Evans and Gariepy, 2015], defined
pointwise at points x  X as



1

 

lim

f (x) = 0 (B(x, ))

0,

f (z)dz,
B(x,)

if the limit exists, otherwise.

Note that when d = 1, the precise representative of any f  H1(X ) is continuous.

Now we explain why the particular choice of representative is not crucial, using the notion of a Lebesgue point. Recall that for a locally Lebesgue integrable function f , a given point x  X is a Lebesgue point of f if the limit of 1/((B(x, ))) B(x,) f (x)dx as   0 exists, and satisfies

1

lim

f (x)dx = f (x).

0  B(x, ) B(x,)

Let E denote the set of Lebesgue points of f . By the Lebesgue differentiation theorem [Evans and Gariepy, 2015], if f  L1(X ) then almost every x  X is a Lebesgue point, (X \ E) = 0. Since f0  H1(X )  L1(X ), we can conclude that any function g0  f0 disagrees with the precise representative f0 only on a set of Lebesgue measure 0. Moreover, since we always assume the design distribution P has a continuous density,
with probability 1 it holds that g0(Xi) = f0 (Xi) for all i = 1, . . . , n. This justifies the notation f0(Xi) used in the main text.

B Graph-dependent error bounds

In this section, we adopt the fixed design perspective; or equivalently, condition on Xi = xi for i = 1, . . . , n.

Let G = [n], W be a fixed graph on {1, . . . , n} with Laplacian matrix L = D - W . The randomness thus

all comes from the responses

Yi = f0(xi) + i

(17)

where the noise variables i are independent N (0, 1). In the rest of this section, we will mildly abuse notation and write f0 = (f0(x1), . . . , f0(xn))  Rn. We will also write Y = (Y1, . . . , Yn).

Recall (2) and (3): the Laplacian smoothing estimator of f0 on G is

n
f := argmin (Yi - fi)2 +  · f Lf = (L + I)-1Y.
f Rn i=1

and the Laplacian smoothing test statistic is

1 T :=
n

f

2 2

.

17

We note that in this section, many of the derivations involved in upper bounding the estimation error of f are similar to those of Sadhanala et al. [2016], with the difference being that we seek bounds in high probability rather than in expectation. We keep the work here self-contained for purposes of completeness.

B.1 Error bounds for linear smoothers
Let S  Rn×n be a fixed square, symmetric matrix, and let

fq := SY

be

a

linear

estimator

of

f0.

In

Lemma

3

we

upper

bound

the

error

1 n

fq- f0

2 2

as

a

function

of

the

eigenvalues

of S. Let (S) = (1(S), . . . , n(S))  Rn denote these eigenvalues, and let vk(S) denote the corresponding

unit-norm eigenvectors, so that S =

n k=1

k

(S)

·

vk

(S)vk

(S

)

.

Denote Zk = vk(S)

, and observe that

Z = (Z1, . . . , Zn)  N (0, I).

Lemma 3. Let fq = SY for a square, symmetric matrix, S  Rn×n. Then

Pf0

1 n

fq- f0

2 2



10 n

(S)

2 2

+

2 n

(S - I)f0

2 2

 1 - exp

-

(S)

2 2

Here we have written Pf0 (·) for the probability law under the regression "function" f0  Rn.

In Lemma 4, we upper bound the error of a test involving the statistic

fq

2 2

=

Y

S2Y. We will require that

S be a contraction, meaning that it has operator norm no greater than 1, Sv 2  v 2 for all v  Rn.

Lemma 4. Let Tq = Y S2Y for a square, symmetric matrix S  Rn×n. Suppose S is a contraction. Define

the threshold qt to be

qt :=

(S)

2 2

+

2 

(S)

2 4

.

(18)

It holds that:

· Type I error.

P0 Tq > qt  .

(19)

· Type II error. Under the further assumption

f0 S2f0  2

2 + 2b


·

(S)

2 4

,

(20)

then

1

16

Pf0 Tq  qt



b2

+

b

(S)

2.
4

(21)

Proof of Lemma 3. The expectation Ef0 [fq] = Sf0, and by the triangle inequality,

1 n

fq- f0

2 2



2 n

2 =
n

fq- Ef0 [fq]

2 2

+

Ef0 [fq] - f0

2 2

S

2 2

+

(S - I)f0

2 2

.

Writing

S

2 2

=

n k=1

k (S

)2Zk2,

the

claim

follows

from

the

result

of

Laurent

and

Massart

[2000]

on

concentration of 2-random variables, which for completeness we restate in Lemma 16. To be explicit, taking

t=

(S)

2 2

in Lemma 16 completes the proof of

Lemma

3.

18

Proof of Lemma 4. We compute the mean and variance of T as a function of f0, then apply Chebyshev's inequality.

Mean. We make use of the eigendecomposition S =

n k=1

k

(S

)

·

vk

(S)vk(S

)

to obtain

Tq = f0 S2f0 + 2f0 S2 +  S2

n

= f0 S2f0 + 2f0 S2 +

k(S) 2( vk(S))2

k=1

n

= f0 S2f0 + 2f0 S2 +

k(S) 2Zk2,

k=1

(22)

implying

n

Ef0 Tq = f0 S2f0 +

k(S) 2.

(23)

k=1

Variance. We start from (22). Recalling that Var(Zk2) = 2, it follows from the Cauchy-Schwarz inequality

that
n

Varf0 Tq  8f0 S4f0 + 4

k(S) 4.

(24)

k=1

Bounding Type I and Type II error. The upper bound (19) on Type I error follows immediately from (23), (24), and Chebyshev's inequality.
We now establish the upper bound (21) on Type II error. From assumption (20), we see that f0 S2f0 -qt  0. As a result,

Pf0 Tq  qt = Pf0 Tq - Ef0 Tq  qt - Ef0 Tq

 Pf0 Tq - Ef0 Tq  qt - Ef0 Tq



Varf0 Tq qt - Ef0 Tq

2,

where the last line follows from Chebyshev's inequality. Plugging in the expressions (23) and (24) for the mean and variance of Tq, as well as the definition of qt in (18), we obtain that

Pf0 Tq  qt



f0

4

(S)

4 4

S2f0 - 2/ (S)

2 4

2

+

f0

8f0 S2f0 -

S4f0 2/ (S)

2 4

2.

(25)

We now use the assumed lower bound f0 S2f0  (2

2/ + 2b)

(S)

2 4

to

separately

upper

bound

each

of

the two terms on the right hand side of (25). It follows immediately that

f0

4

(S)

4 4

S2f0 - 2/ (S)

2 4

2



1 b2 ,

(26)

giving a sufficient upper bound on the first term. Now we upper bound the second term,

8f0 S4f0 f0 S2f0 - 2/ (S)

2 4

2



32f0 S4f0 f0 S2f0 2



b

16 (S)

f0 S4f0

2 4

f0

S2f0

 b

16 (S)

2,
4

(27)

where the final inequality is satisfied because S is a contraction. Plugging (26) and (27) back into (25) then gives the desired result.

19

B.2 Analysis of Laplacian smoothing

Upper bounds on the mean squared error of f , and Type I and Type II error of T , follow from setting S = (L + I)-1 in Lemmas 3 and 4. We give these results in Lemma 5 and 6, and prove them immediately. Recall that 1, . . . , n are the n eigenvalues of L (sorted in ascending order).
Lemma 5. For any  > 0,

1 n

f - f0

2 2



2 n

f0 Lf0

10 n +
n
k=1

1 k + 1 2 ,

(28)

with probability at least 1 - exp -

n k=1

k + 1

-2

.

Recall that

1n

1

1

t := n
k=1

k + 1 2 + n

2n

1


k=1

k + 1 4 .

Lemma 6. For any  > 0 and any b  1, it holds that:

· Type I error.

P0 T > t  .

(29)

· Type II error. If

1 n

f0

2 2



2 n

f0 Lf0

2 +

2/ + 2b n

n

1

k=1 (k + 1)4

1/2
,

(30)

then

1 16 n

1

-1/2

Pf0 T (G)  t  b2 + b k=1 (k + 1)4

.

(31)

Proof of Lemma 5. Let S = (I + L)-1, the estimator f = SY , and

n

(S)

2 2

=

k=1

1 1 + k 2 .

We deduce the following upper bound on the bias term,

(S - I)f0

2 2

=

f0

L1/2L-1/2

S-I

2L-1/2L1/2f0

 f0 Lf0 · n L-1/2 S - I 2L-1/2

1

12

=

f0

Lf0

· max
k[n]

k

1- k + 1

 f0 Lf0 · .

In the above, we have written L-1/2 for the square root of the pseudoinverse of L, the maximum is over all indices k such that k > 0, and the last inequality follows from the basic algebraic identity 1 - 1/(1 + x)2  2x for any x > 0. The claim of the Lemma then follows from Lemma 3.

20

Proof of Lemma 6.

Let

S

:=

(I

+ L)-1,

so

that

T

=

1 n

Y

S2Y. Note that S is a contraction, so that

we may invoke Lemma 4. The bound on Type I error (29) follows immediately from (19). To establish the

bound on Type II error, we must lower bound f0 S2f0. We first note that by assumption (30),

f0 S2f0 =

f0

2 2

-

f0

(I

-

S2)f0

 2 f0 Lf0 - f0 I - S2 f0 + 2

2 + 2b ·


n

1

k=1 (k + 1)4

-1/2
.

Upper bounding f0 I - S2 f0 as follows:

f0 I - S2 f0 = f0 L1/2L-1/2 I - S2 L-1/2L1/2f0

 f0 Lf0 · n L-1/2 I - S2 L-1/2

1

1

=

f0

Lf0

·

max
k

k

1 - (k + 1)2

 f0 Lf0 · 2,

--where in the above the maximum is over all indices k such that k > 0--we deduce that

f0 S2f0  2

2 + 2b ·


n

1

-1/2

k=1 (k + 1)4

.

The upper bound on Type II error (31) then follows from Lemma 4.

C Neighborhood graph Sobolev semi-norm

In this section, we prove Lemma 1, which states an upper bound on f Lf that holds when f is bounded in Sobolev norm. We also establish stronger bounds in the case when f has a bounded Lipschitz constant; this latter result justifies one of our remarks after Theorem 1.

Throughout this proof, we will assume that f  H1(X ) has zero-mean, meaning X f (x) dx = 0. This is without loss of generality--assuming for the moment that (14) holds for zero-mean functions, for any
f  H1(X ), taking a = X f (x) dx and g = f - a, we have that

f

Lf = g

Lg



C2 

n2

rd+2

|g|2H

1

(X

)

=

C2 

n2

rd+2

|f

|2H

1

(X

)

.

Now, for any zero-mean function f  H1(X ) it follows by the Poincare inequality (see Section 5.8, Theorem 1

of Evans [2010]) that

f

2 H1(X )



C8|f |2H1(X ),

for

some

constant

C8

that

does

not

depend

on

f.

Therefore,

to prove Lemma 1, it suffices to show that

Ef

Lf

 Cn2rd+2

f

2 H

1

(X

)

,

since the high-probability upper bound then follows immediately by Markov's inequality. (Recall that L is positive semi-definite, and therefore f Lf is a non-negative random variable).

Since

1n f Lf =
2

f (Xi) - f (Xj) 2Wij,

i,j=1

21

it follows that

n(n - 1)

2 X -X

E f Lf = 2 E f (X ) - f (X) K

r

,

(32)

where X and X are random variables independently drawn from P .

Now, take  to be an arbitrary bounded open set such that B(x, c0)   for all x  X . For the remainder of this proof, we will assume that (i) f  H1() and additionally (ii) f H1()  C5 f H1(X ) for a constant C5
that does not depend on f . This is without loss of generality, since by Theorem 1 in Chapter 5.4 of Evans

[2010] there exists an extension operator E : H1(X )  H1() for which the extension Ef satisfies both (i)

and (ii). Additionally, we will assume f  C(). Again, this is without loss of generality, as C() is

dense in H1() and the expectation on the right hand side of (32) is continuous in H1(). The reason for

dealing with a smooth extension f  C() is so that we can make sense of the following equality for any x

and x in X :
1

f (x ) - f (x) = f x + t(x - x) (x - x) dt.

(33)

0

Obviously

E f (X ) - f (X) 2K

X -X r

 p2max

f (x ) - f (x) 2K

XX

x -x r

dx dx,

(34)

so that it remains now to bound the double integral. Replacing difference by integrated derivative as in (33), we obtain

f (x ) - f (x) 2K
XX

x -x r

1

2

x -x

dx dx =

f x + t(x - x) (x - x) dt K

dx dx

XX 0

r

(i)

1

2



f x + t(x - x) (x - x) K

XX0

x -x r

dt dx dx

(ii)
 rd+2

1

2

f x + trz z K z dt dz dx

X B(0,1) 0

(iii)
 rd+2

1

2

f x z K z dt dz dx,

(35)

 B(0,1) 0

where (i) follows by Jensen's inequality, (ii) follows by substituting z = (x - x)/r and (K1), and (iii) by exchanging integrals, substituting x = x + trz, and noting that x  X implies that x  .

Now, writing f (x) z 2 = x  X,

d i=1

zi

f

(ei

)

(x)

2,

expanding

the

square

and

integrating,

we

have

that

for

any

2

d

f x z K z dz =

f (ei)(x)f (ej)(x)

zizjK( z ) dz

B(0,1)

i,j=1

Rd

d

=

f (ei)(x) 2

zi2K z dz

i=1

B(0,1)

= K f (x) 2,

where the last equality follows from the rotational symmetry of K( z ). Plugging back into (35), we

obtain

f (x ) - f (x) 2K
XX

x -x r

dx dx  rd+2K

f

2 H 1 ()



C5rd+2K

f

2 H

1

(X

)

,

proving the claim of Lemma 1 upon taking C2 := C8C5K p2max in the statement of the lemma.

22

C.1 Stronger bounds under Lipschitz assumption
Suppose f satisfies |f (x ) - f (x)|  M x - x for all x, x  X . Then we can strengthen the high probability bound in Lemma 1 from 1 -  to 1 - 2/n, at the cost of only a constant factor in the upper bound on f Lf .
Proposition 1. Let r  C0(log n/n)1/d. For any f such that |f (x ) - f (x)|  M x - x for all x, x  X , with probability at least 1 - C2/n it holds that

f Lf 

1  + C2

n2rd+2M 2.

Proof of Proposition 1. We will prove Proposition 1 using Chebyshev's inequality, so the key step is to upper bound the variance of f Lf . Putting ij := K( Xi - Xj /r) · (f (Xi) - f (Xj))2, we can write the variance of f Lf as a sum of covariances,

1n n

Var f Lf = 4

Cov ij,  m .

i,j=1 ,m=1

Clearly Cov ij,  m depends on the cardinality of I := {i, j, k, }; we divide into cases, and upper bound the covariance in each case.
I = 4. In this case ij and  m are independent, and Cov ij,  m = 0.
I = 3. Taking i = without loss of generality, and noting that the expectation of ij and im is non-negative, we have

Cov ij , im  E ij im

=

f (z) - f (x) 2 f (x ) - f (x) 2K x - x K z - x

XXX

r

r

 p3maxM 4r4
XX
 p3maxM 4r4+2d.

K
X

x -x K
r

z-x r

dz dx dx

p(z)p(x )p(x) dz dx dx

I = 2. Taking i = and j = m without loss of generality, we have

Var ij  E 2ij



f (x ) - f (x) 4 K

XX

 p2maxM 4r4K(0)

K

XX

 p2maxM 4r4+dK(0).

x -x r
x -x r

2
p(x )p(x) dx dx
dx dx

I = 1. In this case ij =  m = 0.

Therefore

Var f Lf  n3p3maxM 4r4+2d + n2p2maxM 4r4+dK(0)  CM 4n3r4+2d,

where the latter inequality follows since nrd 1. For any  > 0, it follows from Chebyshev's inequality

that

P

f Lf - E f Lf

 M 2 n2rd+2 

2 C ,
n

23

and since we have already upper bounded E f Lf  C2M 2n2rd+2, the proposition follows.
Note that the bound on Var[ij] follows as long as we can control f L4(X ); this implies the Lipschitz assumption--which gives us control of f L(X )--can be weakened. However, the Sobolev assumption-- which gives us control only over f L2(X )--will not do the job.

D Bounds on neighborhood graph eigenvalues

In this section, we prove Lemma 2, following the lead of Burago et al. [2014], Garc´ia Trillos et al. [2019], Calder and Garc´ia Trillos [2019], who establish similar results with respect to a manifold without boundary. To prove this lemma, in Theorem 6 we give estimates on the difference between eigenvalues of the graph Laplacian L and eigenvalues of the weighted Laplace-Beltrami operator P . We recall P is defined as

1 P f (x) := - p(x) div

p2f

(x).

To avoid confusion, in this section we write k(Gn,r) for the kth smallest eigenvalue of the graph Laplacian matrix L and k(P ) for the kth smallest eigenvalue of P 3. Some other notation: throughout this section, we will write A, A0, A1, . . . and a, a0, a1, . . . for constants which may depend on X , d, K, and p, but do not depend on n; we keep track of all such constants explicitly in our proofs. We let LK denote the Lipschitz constant of the kernel K. Finally, for notational ease we set  and  to be the following (small) positive
numbers:

 := max

n-1/d, min

1 1 K(1)

1

2d+3A0 , A3 , 8LK A0 , 8 max{A1, A}c0

r

,

1

and  :=

.

8 max{A1, A}

(36)

We note that each of ,  and /r are of at most constant order. Theorem 6. For any  N such that

1

1-A r

 (P ) +  + 

 2

(37)

with probability at least 1 - A0n exp(-a0n2d), it holds that

ak(Gn,r)  nrd+2k(P )  Ak(Gn,r), for all 1  k 

(38)

Before moving forward to the proofs of Lemma 2 and Theorem 6, it is worth being clear about the differences between Theorem 6 and the results of Burago et al. [2014], Garc´ia Trillos et al. [2019], Calder and Garc´ia Trillos [2019]. First of all, the reason we cannot directly use the results of these works in the proof of Lemma 2 is that they all assume the domain X is without boundary, whereas for our results in Section 4 we instead assume X has a (Lipschitz smooth) boundary. Fortunately, in this setting the high-level strategy shared by Burago et al. [2014], Garc´ia Trillos et al. [2019], Calder and Garc´ia Trillos [2019] can still be used--indeed we follow it closely, as we summarize in Section D.1. However, many calculations need to be redone, in order to account for points x which are on or sufficiently close to the boundary of X . For completeness and ease of reading, we provide a self-contained proof of Theorem 6, but we comment where appropriate on connections between the technical results we use in this proof, and those derived in Burago et al. [2014], Garc´ia Trillos et al. [2019], Calder and Garc´ia Trillos [2019].
On the other hand, we should also point out that unlike the results of Burago et al. [2014], Garc´ia Trillos et al. [2019], Calder and Garc´ia Trillos [2019], Theorem 6 does not imply that k(Gn,r) is a consistent estimate
3Under the assumptions (P1) and (P2), the operator P has a discrete spectrum; see Garc´ia Trillos and Slepcev [2018a] for more details.

24

of k(P ), i.e. it does not imply that |(nrd+2)-1k(Gn,r) - k(P )|  0 as n  , r  0. The key difficulty in proving consistency when X has a boundary can be summarized as follows: while at points x  X satisfying B(x, r)  X , the graph Laplacian L is a reasonable approximation of the operator P , at points x near the boundary L is known to approximate a different operator altogether [Belkin et al., 2012]. This is reminiscent of the boundary effects present in the analysis of kernel smoothing. We believe a more subtle analysis might imply convergence of eigenvalues in this setting. However, the conclusion of Theorem 6--that k(Gn,r)/(nrd+2k(P )) is bounded above and below by constants that do not depend on k--suffices for our purposes.

The bulk of the remainder of this section is devoted to the proof of Theorem 6. First, however, we show that under our regularity conditions on p and X , Lemma 2 is a simple consequence of Theorem 6. The link between the two is Weyl's Law.

Proposition 2 (Weyl's Law). Suppose the density p and the domain X satisfy (P1) and (P2). Then there exist constants a2 and A2 such that

a2k2/d  k(P )  A2k2/d for all k  N, k > 1.

(39)

See Lemma 28 of Dunlop et al. [2020] for a proof that (P1) and (P2) imply Weyl's Law.

Proof of Lemma 2. Put

1/(2A) - ( + ) d

=

.

rA12/2

Let us verify that  (P ) satisfies the condition (37) of Theorem 6. Setting c0 := 1/(21/d4A12/2), the assumed upper bound on the radius r  c0 guarantees that  2. Therefore, by Proposition 2 we have that



(P )  A12/2

1/d



1 r

1 - ( + ) .
2A

Rearranging the above inequality shows that condition (37) is satisfied.
It is therefore the case that the inequalities in (38) hold with probability at least 1 - A0n exp(-a0n2d). Together, (38) and (39) imply the following bounds on the graph Laplacian eigenvalues:

a nrd+2k2/d A2



k (Gn,r )



A nrd+2k2/d a2

for all 2  k 

.

It remains to bound k(Gn,r) for those indices k which are greater than . On the one hand, since the

eigenvalues are sorted in ascending order, we can use the lower bound on  (Gn,r) that we have just

derived:

k(Gn,r)  

(Gn,r) 

a2 nrd+2 A

2/d



a2 64A3A2

nrd

.

On the other hand, for any graph G the maximum eigenvalue of the Laplacian is upper bounded by twice the

maximum degree [Chung and Graham, 1997]. Writing Dmax(Gn,r) for the maximum degree of Gn,r, it is thus a consequence of Lemma 19 that

k(Gn,r)  2Dmax(Gn,r)  4pmaxnrd,

with probability at least 1 - 2n exp -nrdpmin/(3K(0)2) . In sum, we have shown that with probability at least 1 - A0n exp(-a0n2d) - 2n exp -nrdpmin/(3K(0)2) ,

min

a2 A

nrd+2k2/d,

a2 A364A3

nrd

 k(Gn,r)  min

A2 a

nr2+d

k2/d

,

4pmax

nrd

for all 2  k  n.

25

Lemma 2 then follows upon setting

C1 := max{2A0, 4},

C3 := max

A2 a

,

4pmax

,

in the statement of that Lemma.

c1 := min

pmin 3K (0)2

,

2 r

c3 := min

a2 A

,

a2 A364A3

.

D.1 Proof of Theorem 6

In this section we prove Theorem 6, following closely the approach of Burago et al. [2014], Garc´ia Trillos

et al. [2019], Calder and Garc´ia Trillos [2019]. As in these works, we relate k(P ) and k(Gn,r) by means
of the Dirichlet energies 1
br(u) := n2rd+2 u Lu

and

D2(f ) :=

X f (x) 2p2(x) dx 

if f  H1(X ) otherwise,

Let us pause briefly to motivate the relevance of br(u) and D2(f ). In the following discussion, recall that for

a function u : {X1, . . . , Xn}  R, the empirical norm is defined as

u

2 n

:=

1 n

ni=1(u(Xi))2, and the class

L2(Pn) consists of those u  Rn for which u n < . Similarly, for a function f : X  R, the L2(P ) norm

of f is

f

2 P

:=

f (x) 2p(x) dx,

X

and the class L2(P ) consists of those f for which f P < . Now, suppose one could show the following two results:

(1) an upper bound of br(u) by D2 I(u) for an appropriate choice of interpolating map I : L2(Pn)  L2(X ),
and vice versa an upper bound of D2(f ) by br(P(f )) for an appropriate choice of discretization map P : L2(X )  L2(Pn),

(2) that I and P were near-isometries, meaning I(u) P  u n and P(f ) P  f n.

Then, by using the variational characterization of eigenvalues k(P ) and k(Gn,r)--i.e. the Courant-Fischer Theorem--one could obtain estimates on the error nrd+2k(P ) - k(Gn,r) .

We will momentarily define particular maps I and P, and establish that they satisfy both (1) and (2). In order
to define these maps, we must first introduce a particular probability measure Pn that, with high probability, is close in transportation distance to both Pn and P . This estimate on the transportation distance--which we now give--will be the workhorse that allows us to relate br to D2, and · n to · P .

Transportation distance between Pn and P . For a measure µ defined on X and map T : X  X , let T µ denote the push-forward of µ by T , i.e the measure for which
T µ (U ) := µ T -1(U )

for any Borel subset U  X . Suppose T µ = Pn; then the map T is referred to as transportation map between µ and Pn. The -transportation distance between µ and Pn is then

d(µ, Pn) := inf T - Id L(µ)
T :T µ=Pn

(40)

where Id(x) = x is the identity mapping.

26

Calder and Garc´ia Trillos [2019] take X to be a smooth submanifold of Rd without boundary, i.e. they assume X satisfies (P3). In this setting, they exhibit an absolutely continuous measure Pn with density pn that with high probability is close to Pn in transportation distance, and for which p - pn L is also small. In Proposition 3, we adapt this result to the setting of full-dimensional manifolds with boundary.
Proposition 3. Suppose X satisfies (P1) and p satisfies (P2). Then with probability at least 1-A0n exp -a0n2d , the following statement holds: there exists a probability measure Pn with density pn such that:

d(Pn, Pn)  A0

(41)

and

pn - p   A0  +  .

(42)

For the rest of this section, we let Pn be a probability measure with density pn, that satisfies the conclusions of Proposition 3. Additionally we denote by Tn an optimal transport map between Pn and Pn, meaning a transportation map which achieves the infimum in (40). Finally, we write U1, . . . , Un for the preimages of X1, . . . , Xn under Tn, meaning Ui = Tn-1(Xi).
Interpolation and discretization maps. The discretization map P : L2(X )  L2(Pn) is given by averaging over the cells U1, . . . , Un,

Pf (Xi) := n · f (x)pn(x) dx.
Ui
On the other hand, the interpolation map I : L2(Pn)  L2(X ) is defined as Iu := r-2A0(P u). Here, P = u  T is the adjoint of P, i.e.
n
P u (x) = u(xi)1{x  Ui}
j=1

and r-2A0 is a kernel smoothing operator, defined with respect to a carefully chosen kernel . To be precise, for any h > 0,

1

x -x

h(f ) := hdh(x)

h(x , x)f (x ) dx ,
X

h(x , x) := 

r

where (t) := (1/K )

 t

sK (s)

ds

and

h(x)

:=

(1/hd)

X h(x , x) dx

is a normalizing constant.

Propositions 4 and 5 establish our claims regarding P and I: first, that they approximately preserve the Dirichlet energies br and D2, and second that they are near-isometries for functions u  L2(Pn) (or f  L2(P )) of small Dirichlet energy br(u) (or D2(f )).

Proposition 4 (cf. Proposition 4.1 of Calder and Garc´ia Trillos [2019]). With probability at least 1 - A0n exp(-a0n2d), we have the following.
(1) For every u  L2(Pn),



K D2(Iu)  A8 1 + A1( + ) · 1 + A3 r br(u).

(43)

(2) For every f  L2(X ),

br(Pf ) 

1 + A1( + )

·

 1 + A9 r

·

C5p2max p2min

· K D2(f ).

(44)

27

Proposition 5 (cf. Proposition 4.2 of Calder and Garc´ia Trillos [2019]). With probability at least 1 - A0n exp(-a0n2d), we have the following.
(1) For every f  L2(X ),

f

2 P

-

Pf

2 n

 A5r f

P

D2(f ) + A1  + 

f

2 P

.

(45)

(2) For every u  L2(Pn),

Iu

2 P

-

u

2 n

 A6r

un

br(u) + A7  + 

u

2 n

.

(46)

We will devote most of the rest of this section to the proofs of Propositions 3, 4, and 5. First, however, we use these propositions to prove Theorem 6.

Proof of Theorem 6. Throughout this proof, we assume that inequalities (43)-(46) are satisfied. We take A and a to be positive constants such that

1



2 a

1 + A1( + )

1 + A9 r

C5p2max p2min

,

and

A  max

1

A1,

A5,

 a

A6,

A7,

2A8

1 + A1( + )

 1 + A3 r

.

Let k be any number in 1, . . . , . We start with the upper bound in (38), proceeding as in Proposition 4.4 of

Burago et al. [2014]. Let f1, . . . , fk denote the first k eigenfunctions of P and set W := span{f1, . . . , fk},

so that by the Courant-Fischer principle D2(f )  k(P )

f

2 P

for every f  W .

As a result, by Part (1) of

Proposition 5 we have that for any f  W ,

Pf

2 n



1 - A5r

k(P ) - A1( + )

f

2 P

1 
2

f

2 P

,

where the second inequality follows by assumption (37).

Therefore P is injective over W , and PW has dimension . This means we can invoke the Courant-Fischer Theorem, along with Proposition 4, and conclude that

k (Gn,r ) nrd+2



max
uP W

br (u)

u

2 n

u=0

=

max
f W f =0

br(Pf )

Pf

2 n

 2 1 + A1( + )

·

 1 + A9 r

·

C5p2max p2min

K k(P ),

establishing the lower bound in (38).

The upper bound follows from essentially parallel reasoning. Recalling that v1, . . . , vk denote the first k eigenvectors of L, set U := span{v1, . . . , vk}, so that nrd+2br(u)  k(Gn,r) u 2n. By Proposition 5, Part (2),
we have that for every u  U ,

Iu

2 P



u

2 n

-

A6r

u

n

br(u) - A7  + 

u

2 n



u

2 n

-

A6r

u

2 n

k (Gn,r ) nrd+2

-

A7

+

u

2 n



u

2 n

-

A6r

u

2 n

1 a k(P ) - A7  + 

u

2 n

1 
2

u

2 n

,

28

where the second to last inequality follows from the lower bound ak(Gn,r)  nrd+2k(P ) that we just derived, and the last inequality from assumption (37).
Therefore I is injective over U , IU has dimension k, and by Proposition 4 we conclude that

k (P

)



max
uU

D2 (I u)

u

2 P

 2A8 1 + A1( + )

 2A8 1 + A1( + )

 1 + A3 r
 1 + A3 r

max
uU

br (u)

u

2 n

k (Gn,r nrd+2

)

,

establishing the upper bound in (38).

Organization of this section. The rest of this section will be devoted to proving Propositions 3, 4 and 5. To prove the latter two propositions, it will help to introduce the intermediate energies

1 Er(f, , V ) := rd+2

V

f (x ) - f (x) 2
X

x -x r

pn(x )pn(x) dx dx

and

1 Er(f, , V ) := rd+2

V

f (x ) - f (x) 2
X

x -x r

p(x )p(x) dx dx.

Here  : [0, )  [0, ) is an arbitrary kernel, and V  X is a measurable set. We will abbreviate Er(f, , X ) as Er(f, ) and Er(f, K) = Er(f ) (and likewise with Er.)

The proof of Proposition 3 is given in Section D.2. In Section D.3, we establish relationships between the (non-random) functionals Er(f ) and D2(f ), as well as providing estimates on some assorted integrals. In Section D.4, we establish relationships between the stochastic functionals Er(f ) and Er(f ), between Er I(u)
and br u , and between Er f and br Pf . Finally, in Section D.5 we use these various relationships to prove Propositions 4 and 5.

D.2 Proof of Proposition 3

We start by defining the density pn, which will be piecewise constant over a particular partition Q of X . Specifically, for each Q in Q and every x  Q, we set

pn(x)

:=

Pn(Q) , vol(Q)

(47)

where vol(·) denotes the Lebesgue measure. Then Pn(U ) = U pn(x) dx. We now construct the partition Q, in progressive degrees of generality on the domain X .
· In the special case of the unit cube X = (0, 1)d, the partition will simply be a collection of cubes,

Q = Qk : k  [-1]d ,

where Qk =  [k1 - 1, k1]  · · ·  [kd - 1, kd] and we assume without loss of generality that -1  N.
· If X is an open, connected set with smooth boundary, then by Proposition 3.2 of Garc´ia Trillos and Slepcev [2015], there exist a finite number N (X )  N of disjoint polytopes which cover X . Moreover,

29

letting Uj denote the intersection of the jth of these polytopes with Xs, this proposition establishes that for each j there exists a bi-Lipschitz homeomorphism j : Uj  [0, 1]d. We take the collection

Q = -j 1(Qk) : j = 1, . . . , N (X ) and k  [-1]d

to be our partition. Denote by L the maximum of the bi-Lipschitz constants of 1, . . . , N(X ).
· Finally, in the general case where X is an open, connected set with Lipschitz boundary, then there exists a bi-Lipschitz homeomorphism  between X and a smooth, open, connected set with Lipschitz boundary. Letting j and Qj,k be as before, we take the collection

Q = Qj,k = -1  -j 1 (Qk) : j = 1, . . . , N (X ) and k  [-1]d

to be our partition. Denote by L the bi-Lipschitz constant of .

Let us record a few facts which hold for all Qj,k  Q, and which follow from the bi-Lipschitz properties of j and : first that

diam(Qj,k)  LL,

(48)

and second that

vol(Qj,k) 

1

d
d.

LL

(49)

We now use these facts to show that Pn satisfies the claims of Proposition 3. On the one hand for every Q  Q, letting N (Q) denote the number of design points {X1, . . . , Xk} which fall in Q, we have

N (Q)

Pn(Q) = pn(x) dx = Pn(Q) =
Q

. n

Moreover, ignoring those cells for which N (Q) = 0 (since Pn(Q) = 0 for such Q, and so they do not contribute to the essential supremum in (40)), appropriately dividing each remaining cell Q  Q into N (Q) subsets S1, . . . , SN(Q) of equal volume, and mapping each S to a different design point Xi  Q, we can exhibit a
transport map T from Pn to Pn for which

T

- Id

L (Pn )

 max diam(Q)  LL.
QQ

On the other hand, applying the triangle inequality we have that for x  Qj,k

|pn(x) - p(x)|  Pn(Qj,k) - P (Qj,k) +

1

|p(x ) - p(x)| dx,

vol(Qj,k )

vol(Qj,k) Qj,k

and using the Lipschitz property of p we find that

pn - p L  max Pn(Qj,k) - P (Qj,k) + LpLL.

j,k

vol(Qj,k )

(50)

From Hoeffding's inequality and a union bound, we obtain that

2n min{P (Q)}

P Pn(Q) - P (Q)  P (Q) Q  Q  1 - 2 (Q) · exp -

3

2N (X )

1-

· exp

d

-

2npmind 3 LL d

.

30

Noting that by assumption P (Q)  pmaxvol(Q) and -d  n, the claim follows upon plugging back into (50), and setting
1 a0 := 3 LL d and A0 := max 2N (X ), LpLL, LL
in the statement of the proposition.

D.3 Non-random functionals and integrals
Let us start by making the following observation, which we make use of repeatedly in this section. Let  : [0, )  [0, ) be an otherwise arbitrary function. As a consequence of (P1), there exist constants c0 and a3 which depend on X , such that for any 0 <   c0 it holds that

x -x

x -x


B(x,)X



dx  a3 ·



B(x,)



dx

(51)

As a special case: when (x) = 1, this implies vol B(x, )  X  a3dd for any 0 <   c0.
We have already upper bounded Er(f ) by (a constant times) D2(f ) in the proof of Lemma 1. In Lemma 7, we establish the reverse inequality.
Lemma 7 (cf. Lemma 9 of Garc´ia Trillos et al. [2019], Lemma 5.5 of Burago et al. [2014]). For any f  L2(X ), and any 0 < h  c0, it holds that

K D2(hf )  A8Eh(f ).

To prove Lemma 7, we require upper and lower bounds on h(x), as well as an upper bound on the gradient of h. The lower bound here--h(x)  a3--is quite a bit a looser than what can be shown when X has no boundary. The same is the case regarding the upper bound of the size of the gradient h(x) . However, the bounds as stated here will be sufficient for our purposes.
Lemma 8. For any 0 < h  c0, for all x  X it holds that

a3  h(x)  1.

and

1

h(x)

 . dK h

Finally, to prove part (2) of Proposition 5, we require Lemma 9, which gives an estimate on the error hf - f

in

·

2 P

norm.

Lemma 9 (c.f Lemma 8 of Garc´ia Trillos et al. [2019], Lemma 5.4 of Burago et al. [2014]). For

any 0 < h  c0,

hf

2 P



pmax a3pmin

f

2 P

,

(52)

and

hf - f

2 P



1 a3K pmin

h2

Eh

(f

),

(53)

for all f  L2(X ).

31

Proof of Lemma 7. For any a  R, hf satisfies the identity

1

hf (x) = a + hdh(x)

h(x , x) f (x ) - a dx ,
X

and by differentiating with respect to x, we obtain

1

1

1

hf (x) = hdh(x)

h(x , ·) (x) f (x ) - a dx + 
X

h

(x) · hd

h(x , x) f (x ) - a dx
X

Plugging in a = f (x), we get hf (x) = J1(x)/h(x) + J2(x) for

1

1

1

J1(x) := hd

h(x , ·) (x) f (x ) - f (x) dx ,
X

J2(x) := 

h

(x) · hd

h(x , x) f (x ) - f (x) dx .
X

To upper bound J1(x) 2, we first compute the gradient of h(x , ·),

1 x - x (x - x )

h(x

, ·)

(x)

=

 h

h

x -x

1

x -x

= K h2 K

h

(x - x),

and additionally note that J1(x) 2 = supw J1(x), w 2 where the supremum is over unit norm vector. Taking w to be a unit norm vector which achieves this supremum, we have that

J1(x)

2=

1 K2 h4+2d

1  K2 h4+2d

2

x -x

f (x ) - f (x) K

(x - x) w dx

X

h

x -x

K

X

h

(x - x) w 2 dx

x -x

K

X

h

By a change of variables, we obtain

f (x ) - f (x) 2 dx .

x -x

K

X

h

with the resulting upper bound

(x - x) w 2 dx  hd+2 K z
X

z w 2 dz  K hd+2,

J1(x)

2



1 K h2+d

K
X

x -x h

f (x ) - f (x) 2 dx .

To upper bound J2(x) 2, we use the Cauchy-Schwarz inequality along with the observation h(x , x) 

1 K

K

x - x /h to deduce

J2(x) 2 



1 h

21 (x) h2d

h(x , x) dx ·
X

h(x , x) f (x ) - f (x) 2 dx
X

=



1 h

(x)

2 h(x) hd

h(x , x) f (x ) - f (x) 2 dx
X





1 h

(x)

2 h(x) K hd

K
X

x -x h

f (x ) - f (x) 2 dx

1

x -x

 da23K2 h2+d

K
X

h

f (x ) - f (x) 2 dx ,

32

where the last inequality follows from the estimates on h and h provided in Lemma 8. Combining our bounds on J1(x) 2 and J2(x) 2 along with the lower bound on h(x) in Lemma 8 and integrating over X , we have

K D2(hf ) = K
X
 2K
X

2
hf )(x) p2(x) dx

J1(x) h2(x)

2

+

J2(x) 2

p2(x) dx

1

1

2

x -x



a23 + da23K

hd+2

X

K
X

h

 2 1 + Lph pmin

1

1

a23 + da23K Eh(f ),

f (x ) - f (x) 2p2(x) dx dx

and taking A8 := 2

1 + Lpc0
pmin

+ 1

1

a23

da23 K

completes the proof of Lemma 7.

Proof of Lemma 8. We first establish our estimates of h(x), and then upper bound h(x) . Using (51), we have that

1 h(x) = hd



a3 hd

x -x



dx

X B(x,h)

h

x -x



dx

B(x,h)

h

= a3

( z ) dz,

B(0,1)

and it follows from similar reasoning that h(x)  B(0,1) ( z ) dz.

We will now show that B(0,1) ( z ) dz = 1, from which we derive the estimates a3  h(x)  1. To see the identity, note that on the one hand, by converting to polar coordinates and integrating by parts we

obtain


B(0,1)

z

dz = dd

1
(t)td-1 dt = -d
0

1
 (t)td dt =

d

0

K

1
td+1K(t) dt;
0

on the other hand, again converting to polar coordinates, we have

1 K = d

Rd

x 2K( x ) dx = d

1
td+1K(t) dt,
0

and so B(0,1) ( z ) dz = 1.

Now we upper bound h(x) 2. Exchanging derivative and integral, we have

1

1

h(x) = hd

X h(x , ·) (x) dx = K hd+2

K
X

whence by the Cauchy-Schwarz inequality,

x -x h

(x - x) dx ,

h(x)

2

1 K2 h2d+4

K
X

x -x h

dx

concluding the proof of Lemma 8.

x -x

K

X

h

x - x 2 dx ,

1  dK h2 ,

We remark that while  (x) = 0 when B(x, r)  X , near the boundary the upper bound we derived by using Cauchy-Schwarz appears tight.

33

Proof of Lemma 9. By Jensen's inequality and Lemma 8,

2

1

hf (x)  hdh(x)

h(x , x) f (x ) 2 dx
X

1  a3hdpmin

h(x , x) f (x ) 2p(x ) dx .
X

Then, integrating over x, and recalling that B(0,1) ( z ) = 1 as shown in the proof of Lemma 8, we have

hf

2 P



1 a3hdpmin

X

h(x , x) f (x ) 2p(x )p(x) dx dx
X



pmax a3hdpmin

f (x ) 2p(x )
X

h(x , x) dx dx
X

 pmax

f (x ) 2p(x )

a3pmin X

= pmax a3pmin

f

2 P

.

( z ) dz dx
B(0,1)

To establish (53), noting that ha = a for any a  R, we have that

rf (x) - f (x) 2 =

1 hdh(x)

h(x , x) f (x ) - f (x) dx
X

2

1  h2dh2(x)

h(x , x) dx ·
X

h(x , x) f (x ) - f (x) 2 dx
X

1 = hdh(x)

h(x , x) f (x ) - f (x) 2 dx .
X

1  hdh(x)pmin

h(x , x) f (x ) - f (x) 2p(x ) dx .
X

From here, we can use the lower bound h(x)  a3 stated in Lemma 8, as well as the upper bound h(x , x)  (1/K )K( x - x /h), to deduce

rf (x)

- f (x)

2



1 hda3K pmin

K
X

x -x h

f (x ) - f (x) 2p(x ) dx .

Then integrating over X with respect to p yields (53).

D.4 Random functionals
We will use Lemma 10 in the proof of Proposition 5.
Lemma 10 (cf. Lemma 3.4 of Burago et al. [2014]). Let U  X be a measurable subset such that vol(U ) > 0, and diam(U )  2A0. Then, letting a = (Pn(U ))-1 · U f (x)pn(x) dx be the average of f over U , it holds that
2
f (x) - a pn(x) dx  A3r2Er(f, U ).
U

Now we relate Er(f ) and Er(f ). Some standard calculations show that for A1 := 3A0/pmin,

1 - A1( + ) Er(f )  Er(f )  1 + A1( + ) Er(f ),

(54)

34

as well as implying that the norms f P and f n satisfy

1 - A1( + )

f

2 P



f

2
Pn

1 + A1( + )

f

2 P

.

(55)

Lemma 11 relates the graph Sobolev semi-norm br(Pf ) to the non-local energy Er(f ). Lemma 11 (cf. Lemma 13 of Garc´ia Trillos et al. [2019], Lemma 4.3 of Burago et al. [2014]). For any f  L2(X ),
 br(Pf )  1 + A9 r Er+2A0(f ).
In Lemma 12, we establish the reverse of Lemma 11. Lemma 12 (cf. Lemma 14 of Garc´ia Trillos et al. [2019]). For any u  L2(Pn),
 Er-2A0 P u  1 + A3 r br(u).

Proof of Lemma 10. A symmetrization argument implies that

2

1

f (x) - a pn(x) dx =

f (x ) - f (x) 2pn(x )pn(x) dx dx

(56)

U

2Pn(U ) U U

Now, since x and x belong to U , we have that x - x  2A0. Set V = B(x, r)  B(x , r), and note that

B(x, r - 2A0)  V . Moreover, r - 2A0  r  c0 by assumption. Therefore by (51),

vol V  X

 vol B(x, r - 2A0)  X



a3d(r

-

2A0)d



a3d 2d

rd

where

the

last

inequality

follows

since





1 4A0

r.

Using

the

triangle

inequality

f (x ) - f (x) 2  2 f (x ) - f (z) 2 + f (z) - f (x) 2

we have that for any x and x in U ,

f (x ) - f (x) 2 

2

f (x ) - f (z) 2 + f (z) - f (x) 2 dz

vol(V  X ) V X

2d+1  a3drd

f (x ) - f (z) 2 + f (z) - f (x) 2 dz
V X

2d+2

 K(1)a3drdpmin F (x ) + F (x) ,

(57)

where in the last inequality we set

z-x

F (x) := K

X

r

f (z) - f (x) 2pn(x) dx,

and use the facts that pn(x)  pmin/2, that K( z - x /r)  K(1) for all z  B(x, r).

Plugging the upper bound (57) back into (56), we have that

2

2d+2

f (x) - a
U

pn(x) dx  K(1)a3drd

F (x)pn(x) dx
U

=

K

2d+2 (1)a3

d

r2Er

(f

,

U

),

and Lemma 10 follows by taking A3 := 2d+2/(K(1)a3d).

35

Proof of Lemma 11. Recalling that Pf (Xi) = n · Ui f (x)pn(x) dx, by Jensen's inequality,

2

Pf (Xi) - Pf (Xj)  n2 ·

f (x ) - f (x) 2pn(x )pn(x) dx dx.

Ui Uj

Additionally, the non-increasing and Lipschitz properties of K imply that for any x  Ui and x  Uj,

K Xi - Xj r

K

x - x - 2A0 + r

x -x K
r + 2A0

+ 2LK A0 1 r

x - x  r + 2A0 .

As a result, the graph Dirichlet energy is upper bounded as follows:

1

n

2

br(Pf ) = n2rd+2

Pf (Xi) - P f (Xj) K

i,j=1

Xi - Xj r

1n  rd+2
i,j=1

Ui

f (x ) - f (x) 2pn(x )pn(x)K
Uj

Xi - Xj r

1n



rd+2
i,j=1

Ui

f (x ) - f (x) 2pn(x )pn(x) K
Uj

x -x r + 2A0

=

 1 + 2A0 r

d+2

Er+2A0(f )

+

2LK A0 r



Er+2A0



(f

;

1[0,1])

,

dx dx + 2LK A0 1
r

x - x  r + 2

dx dx

for 1[0,1](t) = 1{0  t  1}. But by assumption Er+2A0(f ; 1[0,1])  1/(K(1))Er+2A0(f ), and so we obtain

br(Pf ) 

 1 + 2A0 r

d+2 1 + 2LK A0 rK (1)

Er+2A0(f );

the

Lemma

follows

upon

choosing

A9

:=

A0(2d+4

+

4LK K(1)

).

Proof of Lemma 12. For brevity, we write r := r - 2A0. We begin by expanding the energy Er P u as a double sum of double integrals,

1 nn

2

Er P u

= rd+2
i=1 j=1

Ui

Uj

u(Xi) - u(Xj)

K

x -x r

pn(x )pn(x) dx dx.

We next use the Lipschitz property of the kernel K--in particular that for x  Ui and x  Uj,

x -x K

 K Xi - Xj

+ 2A0LK  · 1

x -x 1 ,

r

r

r

r

--to conclude that

1

nn

2

Er P u  n2rd+2

u(Xi) - u(Xj) K

i=1 j=1

Xi - Xj r



1

+

2d+2A0

 r

br (u)

+

2A0LK r



Er

(P

u, 1[0,1]



1

+

2d+2A0

 r

br (u)

+

4A0LK  K (1)r

Er

(P

u

.

+

2A0LK  r

Er (P

u, 1[0,1]

36

In other words,

Er P u 

1 - 4A0LK  K (1)r

-1

1

+

2d+2

A0

 r

br (u)



 1+

8A0LK + 2d+3

r K(1)

br (u),

where the second inequality follows from the algebraic identities (1 - t)-1  (1 + 2t) for any 0 < t < 1/2

and (1 + s)(1 + t) < 1 + 2s + t for any 0 < t < 1 and s > 0. The Lemma follows upon choosing

A3

:=

8A0 LK K (1)

+ 2d+3.

D.5 Proof of Propositions 4 and 5
Proof of Proposition 4. Part (1) of Proposition 4 follows from

(i)
K D2(r-2A0P u)  A8Er-2A0(P u)
(ii)
 A8 1 + A1( + ) Er-2A0(P u)

(iii)



 A8 1 + A1( + ) · 1 + A3 r br(u),

where (i) follows from Lemma 7, (ii) follows from (54), and (iii) follows from Lemma 12. Part (2) of Proposition 4 follows from

(iv)



br(Pf )  1 + A9 r Er+2A0(f )

(v)



 1 + A1( + ) 1 + A9 r Er+2A0(f )

(vi)


1 + A1( + )

 · 1 + A9 r

·

C5p2max p2min

· K D2(f ),

where (iv) follows from Lemma 11, (v) follows from (54), and (vi) follows from the proof of Lemma 1.

Proof of Proposition 5. Proof of (1). We begin by upper bounding Pf n. By the Cauchy-Schwarz inequality and the bound on pn - p  in (42),

2
Pf (Xi) = n2

2
f (x)pn(x) dx

Ui

 n f (x) 2pn(x) dx
Ui

 n 1 + A1( + )

f (x) 2p(x) dx + A1( + ) f (x) 2p(x) dx ,

Ui

Ui

and summing over i = 1, . . . , n, we obtain

Pf

2 n



1 + A1( + )

f

2 P

.

(58)

37

Now, noticing that Pf n = P P f Pn , we can use the upper bound (58) to show that

Pf

2 n

-

f

2 P



Pf

2 n

-

f

2 Pn

+

f

2 Pn

-

f

2 P

(i)


Pf

2 n

-

f

2 Pn

+ A1( + ) f

2 P

(ii)
2

1 + A1( + )

Pf

n-

f

Pn

·

f

P + A1( + ) f

2 P

2

1 + A1( + ) P Pf - f

Pn ·

f

P + A1( + ) f

2 P

,

(59) (60)

where (i) follows from (55) and (ii) follows from (55) and (58).

It remains to upper bound

P

Pf - f

2 Pn

.

Noting that P

Pf is piecewise constant over the cells Ui, we

have

n

2

P Pf - f

2=
Pn i=1

Ui

f (x) - n ·

f (x )pn(x ) dx
Ui

pn(x) dx.

From Lemma 10, we have that for each i = 1, . . . , n,

2

f (x) - n · f (x )pn(x ) dx pn(x) dx  A3r2Er(f, Ui).

Ui

Ui

Summing up over i on both sides of the inequality gives

P

Pf - f

2 Pn



A3r2Er(f, X )



A3

1 + A1( + )

·

C5p2max p2min

· K r2D2(f ),

where the latter inequality follows from the proof of Proposition 4, Part (2). Then Proposition 5, Part (1) follows by plugging this inequality into (60) and taking

A5 := 2 A3 1 + A1( + )



C5pmax pmin

 · K .

Proof of (2). By the triangle inequality and (55),

Iu

2 P

-

u

2 n



Iu

2 P

-

Iu 2
Pn

+

Iu 2 -
Pn

u

2 n

 A1( + )

Iu

2 Pn

+

Iu 2 -
Pn

u

2 n

= A1( + )

Iu

2 Pn

+

Iu Pn + u n ·

Iu Pn - u n

(61)

To upper bound the second term in the above expression, we first note that u n = P u Pn , and thus

Iu Pn - u n = Iu Pn - P u Pn

(iii)
 rP u - P u Pn

(iv)

1

 r a3K pmin Er(P u)

(v)
r

1 + A1( + ) a3K pmin

 1 + A3 r

br (u),

(62)

38

where (iii) follows by the triangle inequality, (iv) follows from Lemma 9, and (v) follows from (54) and Lemma 12. On the other hand, by (55) and Lemma 9,

Iu 2 
Pn

1 + A1( + )

Iu

2 P

 pmax · a3pmin

1 + A1( + )

P

u

2 P

 pmax · a3pmin

2

1 + A1( + )

Pu2
Pn

= pmax · a3pmin

2
1 + A1( + ) u 2n.

Plugging this estimate along with (62) back into (61), we obtain part (2) of Proposition 5, upon choosing

A6 :=

3

2pmax + 1 pmin

4 ,
a3K pmin

A7

:=

4A1

pmax a3pmin

.

E Bound on the empirical norm

In Lemma 13, we lower bound

f0

2 n

by (a constant times) the L2(X ) norm of f .

Lemma 13. Fix   (0, 1) Suppose P satisfies (P2). If f  H1(X , M ) is lower bounded in L2(X ) norm,

f

L2(X )



C6M 

· max

n-1/2, n-1/d

.

(63)

Then with probability at least 1 - 5,

f

2 n





·

E

f

2 n

.

(64)

Proof of Lemma 13. In this proof, we will find it more convenient to deal with the parameterization b = 1/. To establish (64), it is sufficient to show that

E

f

4 n



1 1 + b2

·E

f

2 n

2;

then (64) follows from the Paley-Zygmund inequality (Lemma 17). Since p  pmax is uniformly bounded, we

can relate E

f

4 n

to the L4(X )-norm,

E

f

4 n

(n - 1) =
n

E

f

2 n

2 E f (X1) 4

+

n

E

f

2 n

2
+ pmax

f

4
L4(X ) . n

We will use the Sobolev inequalities as a tool to show that

f

4 L4 (X

)/n



E[

f

2 n

]

2/(b2pmax), whence the

claim of the Lemma is shown. The nature of the inequalities we use depend on the value of d. In particular,

we will use the following relationships between norms: for any f  H1(X ; M ),

supxX |f (x)|, f Lq(X ), f Lq(X ),

d=1 d = 2, for all 0 < q < 


 
 C7 · M.

d



3,

for

all

0

<

q



2d/(d

-

2)

 

(See Theorem 6 in Section 5.6.3 of Evans [2010] for a complete statement and proof of the various Sobolev inequalities.)
As a result, we divide our analysis into three cases: (i) the case where d < 2, (ii) the case where d > 2, and (iii) the borderline case d = 2.

39

Case 1: d < 2. The L4(X )-norm of f can be bounded in terms of the L2(X ) norm,

2

f

4 L4(X )



sup |f (x)|

·

[f (x)]2 dx  C72M 2 ·

f

2 L2

(X

)

.

xX

X

Since by assumption

f

2 L2(X )



C62

·

b2

·

M2

·

1 ,
n

we have

pmax

f

4

L4(X )
n

 C72M 2pmax ·

f

2 L2(X )
n



C7pmax C62b2

f

4 L2(X )



E

f

2 n

b2

2
,

where the last inequality follows by taking C6  C7 pmax/pmin.

Case 2: d > 2. Let  = 2 - d/2 and q = 2d/(d - 2). Noting that 4 = 2 + (1 - )q, Lyapunov's inequality

implies

f

4 L4(X )



f

2 L2(X )

·

f

(1-)q Lq (X )



f

4 L2(X )

·

C7 f H1(X )

d
.

f L2(X )

By assumption, f L2(X )  C6b f H1(X )n-1/d, and therefore

pmax

f

4
L4(X )  n

f

4 L2

(X

)

pmax

·

C7 f H1(X ) n1/d f L2(X )

d



C7dpmax

f

4 L2(X )

C6dbd



E

f

2 n

b2

2
.

where the last inequality follows by taking C6  C7(pmax/pmin)1/d, and keeping in mind that d > 2 and b  1.

Case 3: d = 2. Fix t  (1/2, 1), and suppose that

f

L2(X )



C6M 

· n-t/2.

(65)

Putting q = 2/(1 - t), we have that f Lq(X )  C7 · M , and it follows from derivations similar to those in

Case 2 that

f

4 L4

(X

)

/n



E[ f 2n] 2/(b2pmax) when C6  C7

pmax/pmin.

Now, suppose f  L4(X ) satisfies (65) only when t = 1. For each k = 1, 2, . . . let fk := n1/(2k)f , so that each fk satisfies (65) with respect to t = 1 - 1/k. Clearly fk - f L4(X )  0 as k  , and therefore

1 n

f

4 L4(X )

=

1 n

lim
k

fk

4 L4(X )



1 b2pmax

lim
k

E[

fk

2 n

]

2

=

1 b2pmax

E[

f

2 n

]

2.

This establishes the claim when d = 2, and completes the proof of Lemma 13.

F Graph functionals under the manifold hypothesis

In this section, we restate a few results of Garc´ia Trillos et al. [2019], Calder and Garc´ia Trillos [2019], which are analogous to Lemmas 1 and 2 but cover the case where X is an m-dimensional submanifold without boundary. As such, the results in this section will hold under the assumption (P3). We refer to Garc´ia Trillos et al. [2019], Calder and Garc´ia Trillos [2019] for the proofs of these results.

Proposition 6 follows from Lemma 5 of Garc´ia Trillos et al. [2019] and Markov's inequality. Proposition 6. For any f  H1(X ), with probability at least 1 - ,

f

Lf



C 

n2

rm+2

|f

|2H

1

(X

)

.

40

In Proposition 7, it is assumed that r,  and  satisfy the following smallness conditions.

(S1)

n-1/m

<





1 r

4

and

1 C( + )  2 pmin

and

C4

log(n)/n 1/m  r  min{c4, 1}.

Proposition 7 (c.f Theorem 2.4 of Calder and Garc´ia Trillos [2019]). With probability at least 1 - Cn exp(-cn2m), the following statement holds. For any k  N such that

1

k(P )r

+

C (

+

)



, 2

it holds that

nrm+2k(P ) 1-C r(

 k(P )+1)+ r +

 k(Gn,r)  nrm+2k(P ) 1+C r(

 k(P )+1)+ r +

.

Proposition 8 follows from Lemma 3.1 of Calder and Garc´ia Trillos [2019], along with a union bound. Proposition 8. With probability at least 1 - 2Cn exp(-cpmaxnrm), it holds that
Dmax(Gn,r)  Cnrm.

Finally, we note that a Weyl's Law holds for Riemmanian manifolds without boundary, i.e.
k(P ) k2/m.
Put Bn,r(k) := min{nrm+2k2/m, nrm}. Following parallel steps to the proof of Lemma 2, one can derive from Propositions 7 and 8, and Weyl's Law, that with probability at least 1 - Cn exp(-cnrm),

cBn,r(k)  k  CBn,r(k), for all 2  k  n.

(66)

G Proofs of main results
We are now in a position to prove Theorems 1-5, as well as a few other claims from our main text. In Section G.1 we prove all of our results regarding estimation and in Section G.2 we prove all of our results regarding testing; in Section G.3, Lemmas 14 and 15, we provide some useful estimates on a particular pair of sums that appear repeatedly in our proofs. Throughout, it will be convenient for us to deal with the normalization  := nrd+2. We note that in each of our Theorems, the prescribed choice of  will always result in   1.

G.1 Proof of estimation results

Proof of Theorem 1. We have shown that the inequalities (14) and (15) are satisfied with probability at least 1 -  - C1n exp(-c1nrd), and throughout this proof we take as granted that both of these inequalities hold.
Now, set  = M -4/(2+d)n-2/(2+d) as prescribed in Theorem 1, and note that -d/2  n is implied by the assumption M  n1/d. Therefore from (15) and Lemma 14, it follows that

n k=1

1 k + 1

2

1n

 1 + C32 k=2

1 k2/d + 1

2



1 8C32

-d/2

.

41

As a result, by Lemma 5 along with (14) and (15), with probability at least 1 -  - C1n exp(-c1nrd) - exp(--d/2/8C32) it holds that,

f - f0

2 n



C2 M 2 + 10 + 10



nn

n

k=2

1 c3 min{k2/d, r-2} + 1

2



C2 M 2 

+

10 n

+

10 nc23

n k=2

1 k2/d + 1

2 10r4 + c232 .

(67)

The first term on the right hand side of (67) is a bias term, while the second, third, and fourth terms each contribute to the variance. Of these, under our assumptions the third term dominates, as we show momentarily. First, we use Lemma 14 to get an upper bound on this variance term,

n

1 k2/d + 1

2
 4-d/2.

k=2

Then plugging this upper bound back into (67), we have that

f - f0

2 n



C2 

M 2

+

10 n

+

40-d/2 c23n

10r4 + c232

=

C2 

+

40 c23

M 2d/(2+d)n-2/(2+d)

+

10 n

+

10 c23

r4M

8/(2+d)n4/(2+d)



C2 

+

50 c23

M 2d/(2+d)n-2/(2+d),

with the last inequality following from (R1) and the assumption M  n-1/2. This completes the proof of Theorem 1.

Proof of Theorem 2. We first establish that f achieves nearly-optimal rates when d = 4, and then establish the claimed sub-optimal rates when d > 4.

Nearly-optimal rates when d = 4.

Continuing on from (67), from Lemma 14 we have that

f

- f0

2 n



C2 M 2 + 

10 n

+

10 nc232

+

10 log n nc232

+

10r4 c232 .

Setting r = (C0 log(n)/n)1/4, we obtain

f - f0

2 n



C2 

M 2

+

10 n

+

10 nc232

+

10 log n nc232

+

10C0 log nc232

n ,

and choosing  = M -2/3(log n/n)1/3 yields

f - f0

2 n



C2 

+

20 c23

+

10C0 c23

M 4/3

log n n

1/3 10 +. n

Suboptimal rates when d > 4.

Once again continuing on from (67), from Lemma 14 we have that

f

- f0

2 n



C2 M 2 + 

10 n

+

10 nc23d/2

+

10 n4/d2c23

+

10r4 2c23 .

42

Setting r = (C0 log n/n)1/d, we obtain

f - f0

2 n



C2 

M 2

+

10 n

+

10 nd/2c23

10 + n4/d2c23

+

10C04/d(log n)4/d n4/d2c23

,

and choosing  = M -2/3n-4/(3d) yields

f - f0

2 n



C2 

+

10 c23

+

10C04/d c23

M 4/3

log n n1/3

4/d

+

10 cd3/2

M

d/3n-1/3

+

10 .
n

Bounds on L2(X ) error under Lipschitz assumption. Let V1, . . . , Vn denote the Voronoi tesselation of X with respect to X1, . . . , Xn. Extend f over X by taking it piecewise constant over the Voronoi cells, i.e.
n
f (x) := fi · 1{x  Vi}.
i=1
Note that we are abusing notation slightly by also using f to refer to this extension.
In Proposition 9, we establish that the out-of-sample error f - f0 L2(X ) will not be too much larger than the in-sample error f - f0 n.
Proposition 9. Suppose f0 satisfies |f0(x ) - f0(x)|  M x - x for all x , x  X . Then for all n sufficiently large, with probability at least 1 -  it holds that

f - f0

2 L2(X )



C

log(1/)

log(n) ·

f - f0

2 n

+

M2

log n n

2/d

.

Note that n-2/d n-2/(2+d). Therefore Proposition 9 together with Theorem 1 implies that with high

probability, f achieves the nearly-optimal (up to a factor of log n) estimation rates out-of-sample error--that

is,

f - f0

2 L2(X )



C

log(n)M 2d/(2+d)n-2/(2+d)--as

long

as

M

 Cn1/d.

This justifies one of our remarks

after Theorem 1.

Proof of Proposition 9. Suppose x  Vi, so that we can upper bound the pointwise squared error |f (x) - f (x)|2 using the triangle inequality:

f (x) - f0(x) 2 = f (Xi) - f0(x) 2  2 f (Xi) - f0(Xi) 2 + 2 f0(Xi) - f0(x) 2.

Integrating both sides of the inequality, we have

n
f (x) - f0(x) 2 dx  2

2

n

f (Xi) - f0(Xi) dx + 2

2
f0(Xi) - f0(x) dx

X

i=1 Vi

i=1 Vi

n

2

n

= 2 vol(Vi) f (Xi) - f0(Xi) + 2

2
f0(Xi) - f0(x) dx,

i=1

i=1 Vi

and so by invoking the Lipschitz property of f0, we obtain

n

2

n

2

f -f

2 L2(X )



2

vol(Vi) f (Xi) - f0(Xi) + 2M 2

diam(Vi) .

i=1

i=1

(68)

Here we have written diam(V ) for the diameter of a set V .

43

Now we will use some results of Chaudhuri and Dasgupta [2010] regarding uniform concentration of empirical counts, to upper bound diam(Vi) Set

n :=

2Co log(1/)d log n

1/d
,

dpmina3n

where Co is a constant given in Lemma 16 of Chaudhuri and Dasgupta [2010]. Note that for n sufficiently large,

n



c0,

and

therefore

by

(51)

we

have

that

for

every

x



X,

P (B(x, n))



2Co

log(1/)d

log n

n

.

Consequently,

by Lemma 16 of Chaudhuri and Dasgupta [2010] it holds that with probability at least 1 - ,

for all x  X , B(x, n)  {X1, . . . , Xn} = .

(69)

But if (69) is true, it must also be true that for each i = 1, . . . , n and for every x  Vi, the distance x-Xi  n.

Thus by the triangle inequality, maxi=1,...,n diam(Vi)  2n. Plugging back in to (68), and using the upper

bound volume vol(Vi)  d diam(Vi) d, we obtain the desired upper bound on

f -f

2 L2

(X

)

.

Proof of Theorem 4. The proof of Theorem 4 follows exactly the same steps as the proof of Theorem 1, replacing the references to Lemma 1 and 2 by references to Proposition 6 and (66), and the ambient dimension d by the intrinsic dimension m.

G.2 Proofs of testing results

Proof of Theorem 3. Let  = 1/b. Recall that we have shown that the inequalities (14) and (15) are satisfied with probability at least 1 - 1/b - C1n exp(-c1nrd), and throughout this proof we take as granted that both of these inequalities hold.

Now, we would like to invoke Lemma 6, and in order to do so, we must show that the inequality (30) is
satisfied with respect to G = Gn,r. First, we upper bound the right hand side of this inequality. Setting  = M -8/(4+d)n-4/(4+d) as prescribed by Theorem 3, it follows from (14) and (15) that

2

2

n f0 Lf0 +

2/ + 2b n

n

1

k=1 (k + 1)4

1/2  C2bM 2 + 2

2/ + 2b

1n

1

n

1 + c23 

(k2/d + 1)4
k=2

1/2 r4n1/2 + c232

 C2bM 2 + 2

2/ + 2b n



1+

2 c23

-d/4

+

r4n1/2 c232

22 2  C2 + 2 + c23 + c23 ·

2 +b

· M 2d/(4+d)n-4/(4+d).



The second inequality in the above is justified by Lemma 15, keeping in mind that M  Mmax(d) implies that -d/2  n. The third inequality follows from the upper bound on r assumed in (R2) as well as the fact
that M  n-1/2.

Next we lower bound the left hand side of the inequality (30)--i.e. we lower bound the empirical norm

f0

2 n

--using

Lemma

13.

Recall that by assumption, M

 Mmax(d).

Therefore, taking C

 C6 in (11)

implies that the lower bound on f L2(X ) in (63) is satisfied. As a result, it follows from (64) that

f

2 n



E[

f b

2n]  pmin b

f

2 L2(X )



C

1 +b

M 2d/(4+d)n-4/(4+d),



 with probability at least 1 - 5/b. Taking C  C2 + 2 + (2 2)/c23 + 2/c23 in (11) thus implies (30), and we

may therefore use Lemma 6 to upper bound the type II error the Laplacian smoothing test . Observe that

by (15) and the lower bound in Lemma 15,

n k=1

1 k + 1

4

1n

 1 + C34 k=2

1 k2/d + 1

4



1 32C34

-d/2

.

44

We conclude that

Pf0 T  t

6 1 16  b + b2 + b


n

1

k=1 (k + 1)4

-1/2
+ C1n exp(-c1nrd)



7 b

+

64 b

2 C32d/4

+

C1n exp(-c1nrd),

establishing the claim of Theorem 3.

Proof of Theorem 5. The proof of Theorem 5 follows exactly the same steps as the proof of Theorem 3, replacing the references to Lemma 1 and 2 by references to Propositions 6 and (66), and the ambient dimension d by the intrinsic dimension m.

Proof of (12).

When 

=

0,

the Laplacian

smoother f

=

Y,

the test statistic T

=

1 n

Y

2 2

,

and

the

threshold t = 1 + n-1/2 2/. The expectation of T is

ET

= E f02(X)

+ 1  pmin

f0

2 L2 (X

)

+

1.

When f0  L4(X , M ), the variance can be upper bounded

Var T

1 
n

3 + pmaxM 4 + pmax

f0

2 L2(X )

.

Now, let us assume that

f0

2 L2 (X )



2

2/ + 2b n-1/2, pmin

so that E[T ] - t  E[f02(X)]/2. Hence, by Chebyshev's inequality

Pf0 T  t



4

Varf0 T E[f02 (X )]2



4 n

·

3 + pmax M 4 +

f0

2 L2(X )

p2min

f0

4 L2(X )

1  b2

3+

4bpmax pminn1/2

+ pmaxM 4

.

G.3 Two convenient estimates

The following Lemmas provides convenient upper and lower bounds on our estimation variance term (Lemma 14) and testing variance term (Lemma 15).
Lemma 14. For any t > 0 such that 1  t-d/2  n,

1 t-d/2 - 1  n 8
k=2

1 tk2/d + 1

3t-d/2,

2





 t-d/2 +

1 t2

log n,

 

1 t2

n1-4/d

,

if d < 4 if d = 4 if d > 4.

Lemma 15. Suppose d  4. Then for any t > 0 such that 1  t-d/2  n,

1 t-d/2 - 1  n

1

4
 2t-d/2.

32

tk2/d + 1

k=2

45

Proof of Lemma 14. We begin by proving the upper bounds. Treating the sum over k as a Riemann sum of a non-increasing function, we have that

n k=2

1 tk2/d + 1

2


n 1

1 tx2/d + 1

2

n

dx  t-d/2 +

t-d/2

1 tx2/d + 1

2

dx



t-d/2

+

1 t2

n
x-4/d dx.
t-d/2

The various upper bounds (for d < 4, d = 4, and d > 4) then follow upon computing the integral.

For the lower bound, we simply recognize that for each k = 2, . . . , n such that k  t-d/2 , it holds that

1/(tk2/d + 1)2  1/4, and there are at least min

t-d/2 - 1, n - 1

>

1 2

t-d/2

-

1

such

values

of

k.

Proof of Lemma 15. The upper bound follows similarly to that of Lemma 14:

n k=1

1 tk2/d + 1

4



t-d/2

+

1 t4

n

1 k8/d

 t-d/2 +

1 t4

k=t-d/2 +1

n
x-8/d dx  2t-d/2.
t-d/2

The lower bound follows from the same logic as we used to derive the lower bound in Lemma 14.

H Concentration inequalities

Lemma 16. Let 1, . . . , N be independent N (0, 1) random variables, and let U :=

for any t > 0,



P U  2 a 2 t + 2 a t  exp(-t).

N k=1

ak

(k2

- 1).

Then

In particular if ak = 1 for each k = 1, . . . , N , then
 P U  2 N t + 2t  exp(-t).

The proof of Lemma 13 relies on (a variant of) the Paley-Zygmund Inequality. Lemma 17. Let f satisfy the following moment inequality for some b  1:

E

f

4 n



1 1 + b2

·

E

f

2 n

2
.

(70)

Then,

P

f

2 n



1 bE

f

2 n

5 1- .
b

(71)

Proof. Let Z be a non-negative random variable such that E(Zq) < . The Paley-Zygmund inequality says

that for all 0    1,

q

P(Z > E(Zp)) 

(1 - p) E(Zp) (E(Z q ))p/q

q-p
.

(72)

Applying (72) with Z =

f

2n,

p

=

1,

q

=

2

and



=

1 b

,

by

assumption

(70)

we

have

P

f

2 n

>

1 b E[

f

2 n

]



1 1-
b

2
·

E[ E[

f f

2n] 2

4 n

]



1

-

2 b

1

+

1 b2

5 1- .
b

46

Let Z1, . . . , Zn be independently distributed and bounded random variables, such that E[Zi] = µi. Let Sn = Z1 + . . . + Zn and µ = µ1 + . . . + µn. The multiplicative form of Hoeffding's inequality gives sharp bounds when µ 1.
Lemma 18 (Hoeffding's Inequality, multiplicative form). Suppose Zi are independent random variables, which satisfy Zi  [0, B] for i = 1, . . . , n. For any 0 <  < 1, it holds that
2µ P Sn - µ  µ  2 exp - 3B2 .
We use Lemma 18, along with properties of the kernel K and density p, to upper bound the maximum degree in our neighborhood graph, which we denote by Dmax(Gn,r) := maxi=1,...,n Dii. Lemma 19. Under the conditions of Lemma 2,
Dmax(Gn,r)  2pmaxnrd,
with probability at least 1 - 2n exp -nrda3pmin/(3[K(0)]2) .

Proof of Lemma 19. Fix x  X , and set

n
Dn,r(x) := K

Xi - x r

;

i=1

note that Dn,r(Xi) is just the degree of Xi in Gn,r. By Hoeffding's inequality

P Dn,r(x) - E Dn,r(x)  E Dn,r(x)

 2 exp

-

2

E Dn,r(x) 3[K (0)]2

.

(73)

Now we lower bound E[Dn,r(x)] using the boundedness of the density p, and the fact that X has Lipschitz boundary:

x -x

E Dn,r(x) = n K
X

r

p(x) dx

x -x

 npmin K
X

r

dx

x -x

 npmina3 K
X

r

dx

 nrdpmin,

with the second inequality following from (51), and the final inequality from the normalization Rd K( z ) dz = 1. Similar derivations yield the upper bound

E Dn,r(x)  nrdpmax,

and plugging these bounds in to (73), we determine that

P Dn,r(x)  (1 + )nrdpmax

 2 exp

-



2nrda0pmin 3[K (0)]2

.

Applying a union bound, we get that

P max Dn,r(Xi)  (1 + )nrdpmax
i=1,...,n

 2n exp

-

2nrda0pmin 3[K (0)]2

,

and taking  = 1 gives the claimed upper bound.

47

