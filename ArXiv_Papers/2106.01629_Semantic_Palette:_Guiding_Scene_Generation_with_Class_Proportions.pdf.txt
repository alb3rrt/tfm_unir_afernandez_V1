Semantic Palette: Guiding Scene Generation with Class Proportions
Guillaume Le Moing2,* Tuan-Hung Vu1 Himalaya Jain1 Patrick Pe´rez1 Matthieu Cord1,3 1Valeo.ai, Paris, France 2Inria, Paris, France 3Sorbonne University, Paris, France

arXiv:2106.01629v1 [cs.CV] 3 Jun 2021

Abstract
Despite the recent progress of generative adversarial networks (GANs) at synthesizing photo-realistic images, producing complex urban scenes remains a challenging problem. Previous works break down scene generation into two consecutive phases: unconditional semantic layout synthesis and image synthesis conditioned on layouts. In this work, we propose to condition layout generation as well for higher semantic control: given a vector of class proportions, we generate layouts with matching composition. To this end, we introduce a conditional framework with novel architecture designs and learning objectives, which effectively accommodates class proportions to guide the scene generation process. The proposed architecture also allows partial layout editing with interesting applications. Thanks to the semantic control, we can produce layouts close to the real distribution, helping enhance the whole scene generation process. On different metrics and urban scene benchmarks, our models outperform existing baselines. Moreover, we demonstrate the merit of our approach for data augmentation: semantic segmenters trained on real layoutimage pairs along with additional ones generated by our approach outperform models only trained on real pairs.
1. Introduction
Generative Adversarial Networks (GANs) [1, 10, 15, 23] have become powerful tools to generate photo-realistic images based on a collection of examples. When trained on real photo portraits in particular, they can produce stunning results [16, 17]. However, for complex structured images like urban scenes, they still struggle to produce satisfactory results: not only do generated scenes exhibit various types of artifacts, but they are also difficult to use for downstream tasks. In automotive applications for instance, generating a wide range of synthetic driving scenes to replace or complement limited amounts of real annotated data is expected to help train better models in the future, for critical-safety
*Main part of this work was done during an internship at Valeo.ai Inria, E´ cole normale supe´rieure, CNRS, PSL Research University

35%

29%

27%

27% Layout Synthesis

5%

7%

7% 5% 5% 3% 1% 1% 1% 1%

9% 4% 3% 3% 1% 4% 3% 2% 5%

12%

conditioned on Class Proportions

Image Synthesis
conditioned on Semantic Layout

Figure 1: Scene generation guided by semantic proportions. The proposed approach, "Semantic Palette", allows a tight control of class proportions when generating semantic layouts and, conditioned on the latter, photo-realistic scenes such as urban scenes.
tasks such as object detection and segmentation. This is not yet the case, and our work is motivated by this goal.
Our target application here is image semantic segmentation, the task of predicting semantic layouts, that is, a class label (or a set of class probabilities) for each pixel of a picture. State-of-the-art models being fully supervised, their training requires scene examples with corresponding semantic layouts. Hence, generating this type of data with a GAN amounts to producing matching image-layout pairs. To this end, recent works advocate decoupling the synthesis process into two consecutive phases: first generating semantic layouts with plausible object arrangements [2, 13], then translating these layouts into realistic images [26, 35].
To improve the usability of such a pipeline, we mostly focus here on the first layout generation step. Existing works [2, 13] cast it as a standard generative process that turns a random input code into a semantic map. While simple to use, this approach offers no real control on the modes of the output distribution [24], which is a limitation for complex scenes. In contrast, we propose to control the generation of layouts with a target distribution of semantic classes in the scene. Depending on applications, this class histogram can be manually defined, automatically derived from a true one, or sampled from a suitable distribution. To this end, we introduce a conditional layout GAN that takes a class histogram (the semantic code or palette) as input beside the standard random noise. As a result, our full image-layout generation pipeline (Figure 1) offers a sim-

ple yet powerful control over the scene composition. This ability brings benefits in various applications, ranging from real image editing to data augmentation for improved model training of a downstream task.
Using a progressive GAN [15] as base architecture to generate semantic layouts, we propose novel architecture designs and learning objectives to achieve our goal. First, we inject the semantic code throughout the progressive pipeline, i.e., at multiple intermediate scales. To explicitly enforce the targeted class distribution while avoiding degenerate soft class assignments, we propose: a semanticallyassisted activation (SAA) module along with two new learning objectives, as well as a novel residual conditional fusion module to ease the progressive propagation of the semantic target through the scales. Lastly, we introduce a variant of the proposed framework that allows partial editing of subregions in existing semantic layouts.
Our main experiments are conducted on different urban scene datasets. Using suitable direct metrics, we first assess the quality of the generated layouts and of the images derived from them. We also assess thoroughly the merit of our approach in the light of semantic segmentation downstream task. To this end, we train segmentation models on synthesized (resp. real) data and measure their performance on real (resp. synthesized) data, as a way to compare our method with the baselines. We finally assess the ability of several approaches to improve model training through augmentation of a real-data training set. In all experiments, Semantic Palette outperforms baselines and produces scenes that follow better the distribution of real ones. More importantly, for real-world applications, using it to extend real datasets boosts performance in semantic segmentation.
In summary, our main contributions are:
· A novel layout generative model that allows control of the distribution of semantic classes. This benefits both the quality of the images that are subsequently generated and the practical use of these images. To further enhance the quality of the generated scene-layout pairs, our method allows end-to-end training of both layout and image generators.
· A variant of our framework for partial editing of semantic layouts. This further benefits downstream task training and opens up interesting applications like semantic editing of real images.
· Extensive evaluations on three different driving benchmarks. The proposed framework significantly outperforms several baseline approaches.
2. Related work
Scene layout generation. SB-GAN [2] and PGANCGAN [13] were the first GAN-based approaches proposed for this task. The SB-GAN pipeline combines an uncon-

ditional model based on ProGAN [15] for generating the semantic masks and GauGAN [26] for transforming these masks into photo-realistic images. PGAN-CGAN [13] is similar but uses Pix2PixHD [35] instead as the image generator. In the same spirit, [33] uses DCGAN [27] as the layout generator and Pix2Pix [14] as the image generator.
Conditional generative adversarial network. Vanilla GANs offer little control over the generation process. In contrast, conditional GANs [24] (cGANs) are designed to guide the generation with conditioning input features, that is, target attributes of the generated data. Gradually deviating from the traditional GAN framework, alternative learning setups [25] and methods to better fuse the conditioning features with the generation pipeline [8, 9, 21, 26] have been proposed. An emerging trend is to infer some generator's parameters from the conditioning input, e.g., normalization parameters [8, 9, 26], convolutional kernels [21], either uniformly [8, 9] or on class-specific regions [21, 26], to better take into account the scene structure.
Image generation conditioned on semantic layout. Pix2pix, a general-purpose image-to-image translation network [14], was among the first to produce compelling results for this conditional generation task. Later works [21, 26, 35] have produced more realistic images at higher resolution by using a multi-scale generator, adding a featurematching loss within the discriminator, or using instance boundary map information. Recently, EdgeGAN [30] proposed to generate structure and texture in parallel and blend the two together thanks to an edge transfer module.
3. Proposed approach
The central goal of this work is to learn to generate plausible semantic layouts (e.g., of urban scenes), conditioned on given class proportions. We describe our network architecture and objective functions for conditional layout generation in Sections 3.1 and 3.2. Section 3.3 discusses the image generation phase and how the complete pipeline with both layout and image generation can be trained end-to-end.
3.1. Conditional layout generative network
We wish to build a generative model that produces new layouts while controlling their semantic composition. The proposed architecture builds upon ProGAN [15], with new architecture designs and learning objectives to achieve our goal. It is thus a cascade of convolutional sub-networks handling information at multiple scales, and trained in a progressive way (Figure 2). At each scale, intermediate features are mapped into soft semantic maps (one per class) of the corresponding resolution via the TO MASK block. Only at the active resolution, the soft semantic layout is transformed through the FROM MASK block into input features used for adversarial training. The largest-scale output

2

Noise + Cond GENERATOR

4 X 8

+ up

CONV

8 X 16

+ up

CONV

16 X 32

+ up

CONV

32 X 64

LCOND
TO MASK
LCOND
TO MASK
LCOND
TO MASK
LCOND
TO MASK

active

inactive

DISCRIMINATOR LADV
Real / Fake
4 X 8 FROM MASK

FROM MASK

down 8 X 16

FROM MASK

down 16 X 32

FROM MASK

down 32 X 64

residual connection

Figure 2: Conditional synthesis of semantic layouts. Snapshot at a certain resolution of the progressive generation (16×32 here). H × W : two 3×3 convolutional layers with ReLU activations applied to feature maps of size H×W ; TO MASK : turns intermediate features into soft semantic maps; FROM MASK : turns soft semantic maps into input features for the discriminator; CONV : 1 × 1 convolutional layers with ReLU activation; " up" and "down": upand down-sampling by a factor 2.

serves as the final generated outcome. The network accepts as inputs not just a random noise
vector but also a conditioning code specifying the target class distribution, i.e., the proportion of image surface that each class should occupy, for instance 50% of "sky", 30% of "road" and 20% of "pedestrian". At intermediate scales, we explicitly enforce conditioning constraints via a new semantically-assisted activation (SAA) module operating inside TO MASK blocks. To propagate the conditioning information from previous scales onto the next, we propose to insert a residual conditional fusion between adjacent subnetworks. We explain the technical details next.

Conditioning input. We provide our model with the nec-

essary information for conditional generation by concatenating an input noise vector z in RZ (Z samples from stan-

dard Gaussian distribution) with a target normalized class

histogram t  RC+, with

C c=1

tc

=

1

and

C

the

number

of

semantic classes. We also use this target explicitly through-

out the generation scales, as shown in the following.

Semantically-assisted activation. We focus here on the design of the TO MASK module, which converts deep features into soft semantic maps while respecting the prescribed class distribution. A common choice is to use a 1×1 convolutional layer to map the number of channels of the deep features to the number of semantic classes, followed by a spatial softmax activation. We stand out from this approach by introducing SAA, which makes again explicit use of the semantic code on top of the generation process. This way, we expect to enforce the respect of the class proportions in the generated maps. SAA (Figure 3) acts in three steps upon the C-channel feature maps f  RC×H×W produced by the last convolutional layer, where H × W is the

TO MASK

Conv 1 x 1

Channel Softmax

LCOND = LSPR + LENT

Channel Cond Product
*

Spatial norm



f



t

(F x H x W) (C x H x W) (C x H x W) (C)


(C x H x W)

m
(C x H x W)

Figure 3: Semantically-assisted activation. In TO MASK modules (Figure 2), which produce soft semantic maps m from features , channel softmax combined with semantic modulation forces  to comply with the target proportions. Spread and entropy losses encourage the output mask m to retain these proportions from .

output resolution. First, a channel-wise spatial softmax is applied to f to obtain a density map  in [0, 1]C×H×W

with:

c,i,j =

exp(fc,i,j )

,

(k, ) exp(fc,k, )

(1)

for each class c  1, C and each pixel location (i, j)   = 1, H × 1, W . Its slice c,:,: is a normalized spatial map for class c.

The next step is to use the semantic code to guide the out-

put layout toward the target class distribution. To this end,

the channels of the density map are weighted by their cor-
responding target proportions to define a new map c,i,j = tc · c,i,j. This new weighted density verifies for each class c, (i,j) c,i,j = tc. Thus, each class receives a "budget" amounting to its contribution in the semantic palette,

e.g., a class with the target proportion set to zero will not be

represented in the final scene.

Finally, the semantic soft map output m in [0, 1]C×H×W

is obtained by L1 normalization of , applied indepen-

dently at every spatial location,

mc,i,j =

c,i,j

,

k 1,C k,i,j

(2)

which can be interpreted as defining at each pixel a prob-

ability distribution over all classes. As done classically,

the final semantic map is obtained at each pixel by select-

ing the label with maximum score, i.e., arg maxc mi,j,c = arg maxc i,j,c. There is no guarantee that this hard labeling complies exactly with the target distribution, but it is

tightly guided by it, as experiments will confirm. SAA can

be seen as a mechanism that transports a well-proportioned but spatially-uniform semantic map (with slice c set to tc everywhere) into a plausible spatial arrangement of the semantic content m.1

In Section 3.2, we will detail the two losses attached to

SAA at train time, which make use of this formalization to help m better follow the target semantic code.

1More formally, SAA can be interpreted as the first iteration of a Sinkhorn-type algorithm [29] in optimal transport, see Appendix A.

3

Residual conditional fusion. With the SAA design above, the output m at an intermediate scale is already conditioned by the semantic code. Such conditioned masks, though at lower resolutions, follow the target semantic code with realistic scene layout. It thus makes sense to pass those signals through the generation process of higher resolutions. This way, the TO MASK layers are still of use when moving to higher resolutions, and having access to intermediate masks produced with semantic assistance can help the network better comply with the target proportions.
To that end, we propose to include a residual conditional fusion block before each upsampling layer (in blue in Figure 2). Via a 1×1 convolutional layer, the soft mask output by the current SAA module is mapped back to features of the same size as original features  and added to them.

3.2. Learning objectives for layout generation

We train the conditional layout synthesis network with two objectives in mind: (1) a conditional objective to help generated layouts respect the target semantic proportions and (2) an adversarial objective to ensure realism. Both could be handled simultaneously by a conditional adversarial loss [24]. Here, we advocate a method that decouples the two, improving the layout quality by letting the discriminator focus on realism, as experiments will confirm.

Conditional objective. The conditional layout generator

G maps noise and target proportions pairs (z, t) to semantic

probability masks m. To make m follow as well as possible

the target distribution, a matching loss could be used, e.g.,

KL-divergence between the targeted and generated class

distributions. But such a direct objective requires the non-

differentiable counting of final max-score labels. While

spatial

aggregation

of

soft

class

maps,

1 HW

i,j mc,i,j ,

is a natural proxy for these label frequencies (coinciding

with them in case of one-hot maps), its use can lead to

undesirable solutions. For instance, a class could well be

completely absent from the final layout (never being max-

scoring at any location), while its soft map average matches exactly a non-zero target probability.2 To this end, we in-

troduce an alternative method which makes use of the pro-

posed semantically-assisted activation.

Our novel conditional training loss has two parts: one to

favor a peaky class distribution at each pixel (soft maps m

close to one-hot), and one to favor an even spatial semantic

coverage (uniform spread of activations in  over pixels).

Intuitively, let the semantic palette be paint of different col-

ors in various quantities. A sufficient condition to respect

proportions from the palette in the final painting is to not

mix colors too much in one spot (only dominant colors will

be seen) while having all the paint from the palette evenly

covering the frame (no accumulation or empty spot). The

2Details on the direct matching loss objective can be found in Appendix B.

first part is translated into a loss that penalizes pixel-wise

entropy of the soft masks m generated from (z, t) pairs:

1

LENT = HW E(z, t)

ei,j ,

(3)

(i,j)

where ei,j = - c 1,C mc,i,j ln(mc,i,j ). The second part is a spread loss on the weighted density map .

It encourages its activations to spread evenly across the

image (hence to be close to

1 HW

at each pixel since

c,i,j c,i,j = 1):

1

LSPR = H W E(z,t)

si,j ,

(4)

(i,j)

where si,j = (1 - HW c 1,C c,i,j )2 . Intuitively,  defines a joint distribution over channels and pixel locations whose marginal over channels is defined by t. The spread loss encourages the marginal over pixel locations to become uniform. This way, pixels should contribute evenly to the output semantic proportions.

The proposed conditional loss finally reads LCOND = LENT + LSPR. Taking advantage of the progressive structure of the generator, supervision via LCOND is possible at each resolution. The simultaneous action of SAA and
of the two losses encourages the generated layouts to re-
spect the target semantic code. Conditional to a palette t, if LSPR is low, in average (over z), each spatial location receives an overall  contribution close to (HW )-1, hence m  HW . We get: Ez[ i,j mc,i,j|t]  HW Ez[ i,j c,i,j |t] = HW tc since i,j c,i,j = tc by construction. Hence, the proportions of the generated soft maps are close to the target palette on average. If LENT is low as well, these generated soft maps will be, in addition,
close to one-hot distributions at each pixel, and this aver-
age compliance with the target palette extends from the soft
maps to the final layouts.

Adversarial objective. We train our generator to produce realistic layouts by trying to make it fool a discriminator which is jointly trained to distinguish real and generated layouts. We use the Improved WGAN loss [11] as the adversarial objective. Note however that real layouts are hard masks (one-hot) while generated ones are soft (even if LENT promotes, to some extent, layouts to be close to onehot). This discrepancy may harm the training as the generator will put lots of efforts trying to output discrete masks as well. The solution adopted by SB-GAN [2] is to apply a Gumbel-softmax [19] function to the generated soft masks. At each spatial location during forward pass, it samples one semantic class out of the multinomial distribution given by the semantic probabilities. During the backward pass, it behaves as a differentiable approximator of this operator.
In practice, we observed that this solution results in noisy sampled masks and using an approximator is not ideal for efficiency of training. We propose instead to tackle this is-

4

sue the other way around, i.e., by softening the real layouts. We apply a Gaussian filter to them, with a variance adapted to the image resolution. To ensure that the prevailing class at each pixel location remains the true one, we use soft semantic masks which are a weighted sum of blurred masks and original ones. We will show the merit of our soft-layout approach compared to Gumbel-softmax in the experiments.
3.3. Image generation and end-to-end training
We want to take advantage of our controllable layout generator in a complete pipeline where photo-realistic images are generated from produced layouts. To this end, we use GauGAN [26], a state-of-the-art layout-to-image translation network.3 The layout generator and the image generator are first trained individually using the default training procedure presented respectively in ProGAN [15] and GauGAN [26]. Both can then be fine-tuned in an end-to-end fashion. By doing this, the layout generator benefits from additional supervision, while the image generator grows accustomed to being fed synthetic layouts which, in turn, improves the overall image quality. We use the end-to-end setup from SB-GAN [2] where an additional discriminator is trained to tell apart real images from synthetic ones generated from synthetic layouts.
4. Semantic Palette in action
4.1. Generating semantic codes
SB-GAN [2] directly maps noise vectors to layout-image pairs, whereas our conditional model also requires target semantic codes in inputs. To use it for systematic data generation, we thus need a means to sample suitable semantic codes: To this end, we propose a palette generator in the form of a Gaussian mixture model (GMM) fitted to a set of true semantic layouts, from which the GMM can capture multiple meaningful modes. Vectors sampled from this GMM are then projected onto the probability C-simplex so as to amount to proper semantic codes. Because the exact projection is slow to compute, in practice, we simply resort to the clipping of the sampled vectors to [0, 1]C followed by L1 normalization.
4.2. Partial editing of semantic layouts
We extend our conditional layout generation method to layout editing, with the aim to plausibly modify an input "real" layout by simply manipulating its semantic palette. To this end, the generator is now conditioned on both an input layout and the target proportions. Its output is a partially edited version of the input layout guided by the target proportions. To condition the generation on this additional in-
3The choice of the base generative framework is orthogonal to our contributions and any improvement on it should increase the performance of our approach and the considered baselines.

Real Mask C x H x W Noise + Cond Input Mask
PART 4 X 8 PART 8 X 16 PART 16 X 32 PART 32 X 64

Input Mask C x H x W

Output Mask C x H x W

SPADE 16 X 32

*+
bg fg TO MASK

+ CONV up

Generated Mask (1 + C) x H x W

Figure 4: Partial layout editing. SPADE H × W replaces H × W from Figure 2. It is a SPADE [26] residual fusion block made of two convolutional layers with conditional batchnorm and ReLU activations. The generated partial layout is merged with the input one thanks to the extra background class (`bg').

put, we replace the ProGAN [15] convolutional blocks with the SPADE residual blocks from GauGAN [26].
Partial editing combined with conditional generation is a powerful tool as it facilitates data augmentation with higher fidelity to real data. It also provides controlled image editing capabilities as we will see in the Experiments section.
More specifically, we consider the task of replacing an arbitrary area of the input layout. During training, we randomly choose a rectangular patch to be replaced. The chosen patch is marked by setting all the class probabilities to 1/C at each pixel, while keeping the rest of the input layout as it is (C is the number of semantic classes). The "cropped" input mask is then passed to the generator through the SPADE [26] residual blocks, see Figure 4. The generator synthesizes an edited version of the input mask with the "cropped" part filled following the given target proportions for the crop. To this end, the generated mask has an additional background class whose proportion is also set by the semantic code so as to fill the "uncropped" part. We produce a coherent output mask by relying on this extra class to merge smoothly the generated mask to the input one. Specifically, the final output is computed as the sum of the generated mask (without background) and the input mask weighted by the background probabilities. The conditional loss LCOND is applied to the generated mask while the adversarial loss is used on the output mask.
5. Experiments
Datasets. Evaluation is done on three urban datasets: ­ Cityscapes [5] is composed of 2,975 training and 500 validation scenes taken in German suburbs. All images are annotated with 33 semantic classes. ­ Cityscapes-25k [31] extends Cityscapes with 19,998 extra training scenes annotated by a pretrained state-of-the-art model. Note that only 19 classes out of the 35 original ones are effectively annotated in these additional 20K scenes. ­ Indian Driving Dataset (IDD) [32] contains 6,993 training

5

Method

Layout KL 

Baseline 1

1.17

Baseline 2

0.32

Sem. Palette 0.07

Sem. Palette e2e 0.08

Oracle

-

Image FID  69.2 69.0 60.7 51.0 28.2

GAN-test mIoU mIoU

33.7 42.8

35.3 46.9

34.6 45.7

36.8 48.6

-

-

GAN-train mIoU mIoU
29.6 38.5 30.2 39.4 30.6 40.1 33.3 44.5 36.9 48.1

Table 1: Conditional layout synthesis on Cityscapes. "Oracle": real data for FID and for training segmentator in GAN-train metric; "e2e": end-to-end fine-tuning; "": smaller is better.

and 981 validation scenes, with 35 semantic classes.
Metrics. We use the following metrics: ­ Kullback­Leibler (KL) divergence between target class proportions and generated ones: It measures how well the generator respects the semantic codes. ­ Fre´chet Segmentation Distance (FSD) [3]: It assesses how the overall statistics of real and synthetic layouts differ; We use real layouts from the training set. ­ Fre´chet Inception Distance (FID) [12]: It is an approximate measure of generated image quality; We compute FID w.r.t. real images from the validation set. ­ GAN-test [28]: We use a segmenter pretrained on real data to yield predictions for generated images. We then report the mean Intersection-over-Union both on standard official classes (mIoU) and on all classes (mIoU). ­ GAN-train [28], the opposite of GAN-test: the segmenter is trained on generated data and tested on the real validation set. Though all metrics are interesting, GAN-train better assesses the overall utility of the generated data.
Implementation details. Generators are trained using ADAM [18]. For segmentation, we train a DeeplabV3 [4] model with Stochastic Gradient Descent, 0.01 initial lr, 0.9 momentum, 5 × 10-4 weight decay, in 300 epochs with batch-size 16. In all experiments, we generate layouts and images up to resolution 128 × 256. Please see Appendix F for details of the palette generator.
5.1. Conditional layout generation
Comparison to conditional baselines. We compare Semantic Palette with two straightforward conditional layout generation baselines, baseline 1 and baseline 2. Both accept semantic code as input. To get layout predictions respecting target class proportions, baseline 1 directly penalizes unsatisfying outputs via a matching loss, and baseline 2 leverages a conditional discriminator similar to cGAN [24]. Note that the same fixed pretrained image synthesizer is used for all.
Results are reported in Table 1 on the Cityscapes dataset using the 4 metrics previously introduced. The direct matching method, baseline 1, fails to reconstruct the semantic code: the KL value of 1.17 is nearly as bad as of random guesses made on the ground-truth semantic distribution (1.25). Though better on KL, baseline 2 produces images with FID comparable to baseline 1. Semantic Palette

(a) Interpolation between two semantic codes

0.45 0.4

0.45 0.4

0.45 0.4

0.45 0.4

target

0.35 0.3

0.35 0.3

0.35 0.3

0.35 0.3

generated

0.25

0.25

0.25

0.25

0.2

0.2

0.2

0.2

0.15

0.15

0.15

0.15

0.1

0.1

0.1

0.1

0.05

0.05

0.05

0.05

0

0

0

0

(b) Diverse samples from one semantic code

Figure 5: Conditional layout-and-scene generation. (a) For two semantic codes (left-/right-most) and interpolations between them; Class histograms in generated scenes (solid) closely follow target ones (dashed). (b) Two examples (top/bottom) of various layout-scene pairs sampled from the same semantic code (left).
improves on all metrics. Especially, we observe significant drops in KL and FID values, meaning that our conditional framework not only better respects the input semantic code, but also produces more realistic layouts.4 While we see a slight drop compared to baseline 2 on GAN-test, more importantly, the GAN-train performance improves along with KL and FID. The best performance is reached for Semantic Palettee2e after fine-tuning the layout generator and the image generator end-to-end. By doing this, the image generator grows accustomed to synthetic layouts while the layout generator benefits from additional supervision. Figure 5-(a) illustrates some qualitative results. Our layout generator clearly follows input semantic codes. We provide additional ablation studies on architecture choices in Section 5.3.
Comparison to unconditional baselines. We follow [2] and report performance on both Cityscapes and Cityscapes25k datasets. We additionally evaluate our method on the IDD dataset, to account for a very different urban landscape. On Cityscapes-25k, missing labels in the 20K extra images deteriorate performance of the 16 missing classes, resulting in lower mIoU as compared to Cityscapes-trained models.
We compare Semantic Palette to unconditional baselines on image-layout pairs generation (Table 2). We note that SB-GAN [2] does better than PGAN-CGAN [13] thanks to the improved image generator. We train these base-
4We note that, because the same fixed image synthesizer is used in all experiments, a low FID score is a proxy indicator of layout quality. Indeed, the image synthesizer is pretrained on real layout-scene pairs; the model is thus used to real layout inputs. The closer generated layouts are to the real distribution, the better the synthesized images are.

6

(a) Cityscapes

(b) Cityscapes-25k

(c) IDD

Method

Layout Image GAN-test GAN-train Layout Image GAN-test GAN-train Layout Image GAN-test GAN-train FSD  FID  mIoU mIoU mIoU mIoU FSD  FID  mIoU mIoU mIoU mIoU FSD  FID  mIoU mIoU mIoU mIoU

PCGAN [13] 63.8 85.7 30.4 39.0 28.2 35.7 161.7 62.6 20.3 34.9 16.7 31.7 104.5 53.7 30.8 39.7 25.0 32.4

SB-GAN [2] 63.8 71.0 31.8 41.2 28.8 37.2 161.7 59.9 20.7 36.8 17.6 34.1 104.5 46.7 32.1 41.5 26.0 33.7

Sem. Palette 25.3 60.7 34.6 45.7 30.6 40.1 37.8 56.3 26.8 46.3 22.4 43.7 60.0 43.5 31.1 40.2 27.0 35.0

SB-GAN e2e [2] 20.4 61.8 34.5 44.7 29.6 37.0 148.5 55.1 28.1 42.9 24.3 41.8 116.0 44.8 31.7 41.0 27.4 35.6

Sem. Palette e2e 11.8 51.0 36.8 48.6 33.3 44.5 61.3 52.8 27.1 43.9 24.7 45.1 40.2 43.2 32.3 41.7 27.7 35.9

Oracle

- 28.2 -

- 36.9 48.1 - 30.9 -

- 36.5 53.0 - 26.3 -

- 33.8 43.8

Table 2: Comparison to unconditional GANs. Same notations as in Table 1.

Data

Method

(a) Cityscapes mIoU mIoU

(b) Cityscapes-25k

(c) IDD

mIoU mIoU

mIoU mIoU

Real

Baseline

36.9 48.1

36.5 53.0

33.8 43.8

Real + Semi-Syn Real + Syn

GauGAN [26] SB-GAN [2] Sem. Palette Sem. Palette (DA) Sem. Palette (Part.) Sem. Palette (Part. + DA)

37.2 0.3 48.2 0.1 34.6 2.3 45.5 2.6 38.0 1.1 49.4 1.3 38.6 1.7 51.6 3.5 40.7 3.8 51.9 3.8 40.7 3.8 52.6 4.5

43.0 6.5 58.0 5.0 35.5 1.0 51.4 1.6 36.9 0.4 54.4 1.4 38.6 2.1 57.3 4.3 42.4 5.9 59.1 6.1 42.5 6.0 60.5 7.5

33.6 0.2 43.5 0.3 33.5 0.3 43.4 0.4 33.8- 43.8- 34.5 0.7 44.7 0.9 35.6 1.8 46.1 2.3 35.3 1.5 45.8 2.0

Table 3: Data-augmentation grouped by training data regime, tested on real data. "DA": domain adaptation; "Part.": partial editing.

lines from scratch, nonetheless, results for SB-GAN [2] on Cityscapes are in line with the ones from the original paper.
Our method outperforms the baselines in both diversity of semantic content and image quality with a clear gap in FSD, FID and GAN-train. We facilitate the layout synthesis task by guiding explicitly the generation with semantic proportions, partially lifting the burden of figuring out the scene composition. We show pairs synthesized from a single target histogram on IDD in Figure 5-(b). Though sharing the same semantic palette, diverse scenes are produced.
5.2. Data augmentation
Once trained, the image and layout generators can be used to sample new pairs, hence augmenting the real training data. Different from standard data augmentation techniques, which only modify existing data points, synthetic models create new data points, which allows not only altering the visual appearance in the image space, but also applying structural changes in the layout space. We consider two different data augmentation setups: (i) "Semi-Syn", which only relies on the pretrained image generator to synthesize images from ground-truth layouts, and (ii) "Syn", which uses both generators to synthesize new data pairs. Table 3 shows test performance of segmenters trained only on "Real", "Real + Semi-Syn", or "'Real + Syn" data.
Real + Semi-Syn. A pretrained GauGAN [26] is used as image generator. On Cityscapes and IDD datasets, we only observe marginal changes in performance compared to the baseline. However, when having more layouts to feed the image generator in Cityscapes-25k, the segmenter trained on augmented data significantly outperforms the baseline. We conjecture that there is a trade-off between the quality of synthesized images and the diversity of semantic layouts: if

(a) GT

(b) Cropped regions

(c) Generated regions

(d) Merged layout

Figure 6: Partial editing of layouts. The procedure consists in cropping ground-truth layouts and then synthesizing new objects within the cropped area, guided by the initial semantic proportions.
layouts cannot provide enough diversity to counter-balance the loss of image quality, this may harm the performance.
Real + Syn. To further highlight the benefit of layout generators, we do not use the end-to-end models. The same pretrained GauGAN is used as in the "Real + Semi-Syn" setup. On the three benchmarks, with SB-GAN [2] as layout generator, we observe drops in mIoU as compared to the baseline. The unconditional model shows its limitations in the data augmentation context where it fails to complement real data with more diverse samples, resulting in negative results. In contrast, Semantic Palette consistently improves upon baselines, except for IDD dataset where the performance is unchanged. These results demonstrate the merits of our pipeline for data augmentation.
We propose several variants of our method to further push its performance. First, to alleviate distribution gaps between synthetic and real data, we adopt AdvEnt [34], a domain adaptation technique for semantic segmentation.5 This strategy is used to ensure synthetic and real supervisions are consistent. This domain adaptation (DA) technique boosts further the performance of our approach (Table 3). Second, we test a variant using the partial layout editing method presented in Section 4.2 and illustrated in Fig-
5See Appendix F for implementation details.

7

Residual Multi Fusion Scale

Soft GT Palette Masks Gen.

Layout KL  FSD  0.32 33.9 0.13 23.4 0.11 37.1 0.03 24.1 0.07 25.3

Image FID  70.6 65.5 63.3 64.3 60.7

Table 4: Semantic Palette ablation on Cityscapes. First row: model with only SAA; In models with "Soft GT Masks" unmarked, Gumbel-softmax is used; If "Palette Gen." is unmarked, ground-truth codes are used instead of generated ones.

ure 6. The ensuing performance (Table 3) demonstrates the clear benefit of leveraging extra real information in the generation process, i.e., partial areas and semantic proportions. A straightforward combination of the two proposed strategies achieves the best performance on two benchmarks.
In Appendix G, we provide further results and discuss the additional use of standard data augmentation during training.
5.3. Ablation studies
We report the results of an ablation study in Table 4. Using residual fusion significantly decreases KL, FSD, and FID values, highlighting the benefit of leveraging lowerscale information. We then achieve further improvements in KL and FID with multi-scale training. Using a strategy based on soft ground-truth masks, as detailed in Section 3.2, instead of Gumbel-softmax improves KL score by a large margin while preserving comparable FID scores. Our final model using the palette generator presented in Section 4.1 achieves the best FID, with a slightly worse KL score compared to the model using ground-truth semantic codes.
5.4. Face editing
We showcase the new editing capabilities offered by the combination of conditional and partial layout generation on face images, using the CelebAMask-HQ dataset [15, 20, 22, 7]. For the image synthesis, we use a pretrained SEAN [37] model, an upgrade of GauGAN [26] where one can fix independently the style of individual semantic classes. We use it to maintain the content while editing the semantic structure. Our method, illustrated in Figure 7, allows one to adjust semantic attributes by a chosen amount with realistic details. This is achieved by simply modifying class proportions, avoiding the tedious task of direct manual editing of the original face layout. For this task, we do not crop the original layouts but allocate some budget for semantic additions. When target content is already present in the original layout, the generator will be inclined to replicate the original content as it fully satisfies both conditional and adversarial objectives; e.g., to increase the amount of hair, it will copy the existing hair as long as the proportion matches since it is the definition of realism for the discriminator. To counter this undesired behaviour, we introduce a

target generated
0,3
0,2
0,1
0

(a) Hair manipulation.

Ground-truth
target generated
0,3 0,2 0,1
0

Interpolation to target proportions
(b) Diverse semantic attributes

0,06

0,01

0,04 0,005
0,02

0

0

manipulation.

Ground-truth

Generated

Ground-truth

Generated

Ground-truth

Generated

Figure 7: Application of Semantic Palette to face editing at resolution 256 × 256. In (a), we illustrate the fine-controlled editing of layouts by gradually increasing the budget for the hair. Edits are convincing both in the layout and image spaces. Thanks to the novelty loss, there is little overlap between original and additional hair. In (b), we show the editing of diverse semantic attributes. Although we have a unique layout generator, we can perform very different edits. Moreover, one can play with latent codes to generate various edits for the same proportion of semantic attributes.

novelty loss that encourages edits to be different from original semantic classes (details in Appendix D).

6. Conclusion
We have proposed the Semantic Palette, a new framework for scene generation, and editing, guided by semantic proportions. Using novel architecture designs and learning objectives ­ semantically assisted activation and residual conditional fusion coupled with novel conditional losses ­, it generates plausible scene layouts with class proportions close to target ones, which then translate into realistic images. Experiments assess the superior quality of the generated layout-image pairs as well as their utility for downstream-task training: used in particular to augment an original real-data set, they deliver performance gain in semantic segmentation.

Acknowledgements. Part of this work was done using HPC resources from GENCI­IDRIS (Grant 2020-AD011012227). We would like to thank Jean Ponce for useful comments.

8

References
[1] Martin Arjovsky, Soumith Chintala, and Le´on Bottou. Wasserstein GAN. In ICML, 2017. 1
[2] Samaneh Azadi, Michael Tschannen, Eric Tzeng, Sylvain Gelly, Trevor Darrell, and Mario Lucic. Semantic bottleneck scene generation. arXiv preprint, 2019. 1, 2, 4, 5, 6, 7, 11, 12
[3] David Bau, Jun-Yan Zhu, Jonas Wulff, William Peebles, Hendrik Strobelt, Bolei Zhou, and Antonio Torralba. Seeing what a gan cannot generate. In ICCV, 2019. 6
[4] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. arXiv preprint, 2017. 6, 11
[5] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The Cityscapes dataset for semantic urban scene understanding. In CVPR, 2016. 5
[6] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In NeurIPS, 2013. 10
[7] Arnaud Dapogny, Matthieu Cord, and Patrick Perez. The missing data encoder: Cross-channel image completion with hide-and-seek adversarial network. In AAAI, 2020. 8
[8] Harm De Vries, Florian Strub, Je´re´mie Mary, Hugo Larochelle, Olivier Pietquin, and Aaron C Courville. Modulating early visual processing by language. In NeurIPS, 2017. 2
[9] Vincent Dumoulin, Jonathon Shlens, and Manjunath Kudlur. A learned representation for artistic style. In ICLR, 2017. 2
[10] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS, 2014. 1
[11] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of Wasserstein GANs. In NeurIPS, 2017. 4
[12] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In NeurIPS, 2017. 6
[13] Jonathan Howe, Kyle Pula, and Aaron A Reite. Conditional generative adversarial networks for data augmentation and adaptation in remotely sensed imagery. In Applications of Machine Learning, 2019. 1, 2, 6, 7, 12
[14] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In CVPR, 2017. 2
[15] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. In ICLR, 2018. 1, 2, 5, 8, 11
[16] Tero Karras, Samuli Laine, and Timo Aila. A stylebased generator architecture for generative adversarial networks. In CVPR, 2019. 1, 11
[17] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of styleGAN. In CVPR, 2020. 1, 11
[18] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. 6, 11

[19] Matt J Kusner and Jose´ Miguel Herna´ndez-Lobato. GANs for sequences of discrete elements with the gumbel-softmax distribution. arXiv preprint, 2016. 4
[20] Cheng-Han Lee, Ziwei Liu, Lingyun Wu, and Ping Luo. MaskGan: Towards diverse and interactive facial image manipulation. In CVPR, 2020. 8
[21] Xihui Liu, Guojun Yin, Jing Shao, Xiaogang Wang, and Hongsheng Li. Learning to predict layout-to-image conditional convolutions for semantic image synthesis. In NeurIPS, 2019. 2, 11
[22] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In ICCV, 2015. 8
[23] Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks. In ICCV, 2017. 1
[24] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint, 2014. 1, 2, 4, 6
[25] Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with auxiliary classifier gans. In ICML, 2017. 2
[26] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and JunYan Zhu. Semantic image synthesis with spatiallyadaptive normalization. In CVPR, 2019. 1, 2, 5, 7, 8, 11
[27] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. In ICLR, 2016. 2
[28] Konstantin Shmelkov, Cordelia Schmid, and Karteek Alahari. How good is my GAN? In ECCV, 2018. 6
[29] Richard Sinkhorn. Diagonal equivalence to matrices with prescribed row and column sums. The American Mathematical Monthly, 1967. 3, 10
[30] Hao Tang, Xiaojuan Qi, Dan Xu, Philip HS Torr, and Nicu Sebe. Edge guided GANs with semantic preserving for semantic image synthesis. arXiv preprint, 2020. 2
[31] Andrew Tao, Karan Sapra, and Bryan Catanzaro. Hierarchical multi-scale attention for semantic segmentation. arXiv preprint, 2020. 5
[32] Girish Varma, Anbumani Subramanian, Anoop Namboodiri, Manmohan Chandraker, and CV Jawahar. IDD: A dataset for exploring problems of autonomous navigation in unconstrained environments. In WACV, 2019. 5
[33] Anna Volokitin, Ender Konukoglu, and Luc Van Gool. Decomposing image generation into layout prediction and conditional synthesis. In CVPRw, 2020. 2
[34] Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, and Patrick Pe´rez. AdvEnt: Adversarial entropy minimization for domain adaptation in semantic segmentation. In CVPR, 2019. 7, 10, 11
[35] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. High-resolution image synthesis and semantic manipulation with conditional GANs. In CVPR, 2018. 1, 2
[36] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ADE20k dataset. In CVPR, 2017. 12
[37] Peihao Zhu, Rameen Abdal, Yipeng Qin, and Peter Wonka. SEAN: Image synthesis with semantic regionadaptive normalization. In CVPR, 2020. 8

9

A. Connection to Sinkhorn algorithm

To carry on the discussion initiated in Section 3.1, we
here elaborate on the connection between our SAA module
and the Sinkhorn algorithm [29], viewing SAA through the
lens of optimal transport [6]. Given an initial blank "canvas" having N = HW pix-
els, we define a uniform source histogram r = N -11N , standing for the equal chance of each pixel to be "drawn"
or occupied by one of the classes. The target histogram, or semantic palette, t  RC+, defines the prescribed "budget" for the C classes. One can defined the set of admissible transport plans from one distribution to the other one:

U (r, t) := {P  RC+×N |P 1N = t, P 1C = r}. (5)

A connection of the soft mask m with these transport plans is established as follows. Flattening spatial dimensions (H × W  N ), the soft mask m is now in [0, 1]C×N , and it is expected to simultaneously verify:

N -1m1N = t ,

(6)

N -1m 1C = r ,

(7)

where (6) warrants that soft pixel-to-class assignments re-

spect

the

input

class

proportions

(

1 N

n mc,n = tc) and

(7) ensures that at each pixel location there is a valid class

distribution ( c mc,n = 1). If m verifies both, then N -1m  U (r, t). Note that, in practice, only (7) is a hard

constraint.

We

can

now

formulate

the

task

of

finding

1 N

m

as

solving

an entropy-regularized optimal-transport problem [6]:

P =

argmin

P,K

1 - h(P ),

(8)

P U (r,t)



where K  RC×N is a suitable transport-cost matrix, h(P ) is the entropy of P ,  is a weight (fixed as 1 next) and , denotes the Frobenius dot-product.
In the SAA module, the cost matrix K is defined as -f . Intuitively f , the "raw" output of our network, indicates the initial class preference of each pixel i; its opposite -f can be seen as the transportation cost, i.e., the higher the chance to assign pixel i to class c, the lower the cost to "transport" from pixel i to class c is.
To find the optimal plan P , one can adopt the Sinkhorn algorithm, initializing P as exp(-K) = exp(f ) and alternating row-wise and column-wise normalization/scaling steps [6]:

P  diag t (P 1N ) P ,

(9)

P  P diag r (P 1C ) ,

(10)

where denotes the Hadamard entry-wise division. Eq. 9 amounts to successively normalizing each of the C rows and

then multiplying each by its target probability in t ­ exactly how  is derived from f ; Eq. 10 amounts to normalizing each of the N columns ­ exactly how m is derived from  (since m corresponds to N P ).
Effectively, the steps of the SAA presented in Section 3.1 correspond to a single step of this Sinkhorn algorithm. Having more steps is possible, yet we opted to a single one as to allow certain slacks in the final scene composition, i.e., not forcing an exact matching to the input semantic palette.

B. Direct matching loss in Baseline 1

The baseline 1 introduced in Section 5.1 uses a direct
matching loss to enforce conditioning constraints. We pro-
vide here the detail of this loss. The conditional layout generator G produces semantic
soft probability masks m  [0, 1]C×H×W . Let us define  : [0, 1]C×H×W  C the function that computes the class
histogram of the final semantic map derived from soft mask m, where C := {x  RC+ : x 1C = 1} is the probability simplex. For each class c  1, C , the proportion of pixels
assigned to this class in the image is given by:

1

c(m) =

HW

[ argmax mk,i,j

(i,j)

k

= c ].

(11)

This function  being non-differentiable, it cannot be easily used to define a training loss. Instead, we propose to use , a differentiable soft estimation of the semantic histogram, defined as:

1

c(m) = HW

mc,i,j ,

(12)

(i,j)

for each c  1, C . The matching loss in baseline 1 is finally defined as the KL-divergence between target and estimated histograms:

LMATCH(G) = E(z,t)

tc · log

c 1,C

tc

.

c(G(z, t))

(13)

C. Domain adaptation for data augmentation
We explain here how AdvEnt, the domain-adaptation technique in [34], is mobilized in Section 5.2 when using both real and synthetic data. In effect, we adopt the main ingredient of AdvEnt: an adversarial training procedure to perform alignment on the so-called weighted selfinformation space. While the segmenter is trained as usual, an additional discriminator, taking segmenter's prediction as input, is trained in parallel to determine from which domain (real or synthetic) the prediction originates. Playing the adversarial game, the segmenter tries to fool the discriminator, eventually resulting in closing the domain gap.

10

Such a technique has been proven effective for unsupervised domain adaptation in semantic segmentation, where images are annotated only in one domain. We revisit it in a different context, where full annotations are available in both domains. Empirical results in Table 3 demonstrate the benefit of addressing domain gap this way when using synthesized data for data augmentation. Since for DA we use the default hyper-parameters from [34], fine-tuning them might yield even higher performance.

D. Novelty loss for face editing
In the case of partial editing, the conditional layout generator G takes a semantic layout l as input, in addition to the noise z and the target palette t. We denote G(z, t, l) = m the final edited layout produced by the generator, after the generated partial layout and the input layout have been merged. For the face editing task, different from the partialediting method proposed for data augmentation in urban scenes, the input layout is not cropped. In addition, we introduce a novelty loss on top of the conditional and adversarial losses, to ensure that the edits do modify the original content. It is defined by:

1

LNOV(G) = |E| E(z,t,l)

lc,i,j mc,i,j ,

(i,j)E c 1,C

(14)

where E   is the set of pixel locations where an edit

has been made, i.e., the dominant class in the partial layout

is not the background class. This loss is, at every edited

pixel location, the scalar product between the generated

soft probability distribution and the input one-hot one, and

therefore promotes orthogonal content between the two.

E. Better base generative frameworks
In this work, we built the layout synthesizer upon ProGAN [15] as to guarantee a fair comparison to SBGAN [2] and to highlight the merits of the proposed architecture designs and learning objectives. We note that this part of the Semantic Palette's pipeline can leverage any other hierarchical GAN architecture, for example StyleGAN [16] or StyleGAN2 [17]. In fact, the choice of the base generative framework is orthogonal to our contributions and any improvement on it should increase the performance of both the Semantic Palette and the considered baselines. Similarly, for the image generation part, we adopted GauGAN [26] as done in SB-GAN [2] to ensure fair comparison while noting that the choice of this generator is orthogonal to our contributions; In particular, if using a different framework like CC-FPSE [21] was to bring improvements, they would benefit all compared pipelines.

F. Implementation details
Weights for losses. All the introduced losses are equally weighted in the experiments. However, a particular weighting may prove useful for specific applications such as to improve further the semantic control at the expense of a slight degradation of the image realism or the other way around.
Layout synthesis model. The layout generator is trained with ADAM [18], an initial learning rate of 10-3,  = (0, 0.99) and specific epochs (600 to 150) and batch sizes (1024 to 8) for every resolution (4×8 to 128×256).
Image synthesis model. The image generator is trained with ADAM [18], an initial learning rate of 2 · 10-4,  = (0.5, 0.999) for 200 epochs and a batch size of 8.
Segmenter. We train a DeeplabV3 [4] model with Stochastic Gradient Descent, an initial learning rate of 10-2, 0.9 momentum, 5·10-4 weight decay, for 300 epochs and a batch size of 16.
Palette generator. The GMM model is trained on the semantic proportions from the real training dataset, using the expectation-maximization (EM) algorithm. The number of Gaussian components control the trade-off between approximation and generalization. We select their number using the Akaike Information Criterion (AIC), which balances these two objectives.
In practice, to ensure that the vectors sampled from the GMM are true proportions, i.e., non-negative and L1normalized, one has to project them onto the probability C­simplex. As the projection is not easy to compute analytically, one can get a good approximation with constrained minimization methods such as the trust-region constrained algorithm. However, their convergence is slow, making them impractical in our case. Instead, we chose to compute a rough estimate of the projection by first clipping the sampled vectors to [0, 1] and then normalizing them.
G. Standard data augmentation.
We used random horizontal flipping in all experiments done in the main paper.
Cropping is another standard data augmentation strategy used in semantic segmentation. We provide here an ablation study where we additionally perform cropping to augment real data while training the baseline and Semantic Palette models. We report in Table 5 the performance on the three benchmarks. In terms of mIoU, we observe only on Cityscapes that cropping helps improve all methods and achieves best scores when combined with our augmentation strategy; yet, on the other datasets, having cropping degrades the performance. In terms of mIoU, the performance drops in most cases. These results reveal different behaviors in the three datasets when including cropping in the data augmentation procedure during training. We note that, as cropping is done only on real data, increase or de-

11

Data

Method

Real

Baseline

Sem. Palette (DA) Real + Syn
Sem. Palette (Part. + DA)

Crop

(a) Cityscapes mIoU mIoU 36.9 48.1 35.7 1.2 51.4 3.3 38.6 51.6 38.7 0.1 52.2 0.6 40.7 52.6 39.8 0.9 54.4 1.8

(b) Cityscapes-25k mIoU mIoU 36.5 53.0 35.5 1.0 59.6 6.6 38.6 57.3 32.9 5.7 57.0 0.3 42.5 60.5 37.0 5.5 56.4 4.1

(c) IDD mIoU mIoU 33.8 43.8 32.7 1.1 40.0 3.8 34.5 44.7 29.8 4.7 38.5 6.2 35.3 45.8 31.1 4.2 39.7 6.1

Table 5: Using cropping to augment real data. Same notations as in Table 3. In each group, models using cropping are compared against the ones without.

Method

Layout Image GAN-test GAN-train FSD  FID  mIoU mIoU

PCGAN [13] 211.7 96.6

11.1

6.8

SB-GAN [2] 211.7 93.7

12.2

7.0

Sem. Palette

76.1

88.4

20.1

10.5

SB-GAN e2e [2] 93.3

82.7

15.3

10.0

Sem. Palette e2e 19.0

76.5

21.4

11.7

Table 6: Results on ADE-Indoor.

Method

Layout Image GAN-test GAN-train FSD  FID  mIoU mIoU mIoU mIoU

SB-GAN [2] 66.0 74.8 34.9 46.0 28.0 35.7

Sem. Palette 32.4 66.7 38.0 50.0 32.1 42.3

Table 7: Results on Cityscapes at resolution 256×512.

and 12. These figures are best viewed in color.

crease in performance by using it is orthogonal to our proposed framework. Overall, the best results are obtained using Semantic Palette.
H. Additional experiments.
We provide results of a few additional experiments aimed at evaluating the Semantic Palette in different setups, namely with other types of data and at higher resolution.
Effect of Semantic Palette on non-urban scenes. To verify that the proposed method generalizes well to other types of natural images, we trained the Semantic Palette and the unconditional baselines on the ADE-Indoor dataset [36] at 128 × 256 resolution. The results, as shown in Table 6, confirm the advantage of our model over the unconditional baselines.
Ability to scale to higher resolutions. To afford an extensive evaluation of the proposed methods compared to the different baselines, all experiments were conducted at the 128×256 resolution. To evaluate the performance at higher resolution, we now compare the Semantic Palette to SBGAN at the 256×512 resolution on Cityscapes. The results, in Table 7, turn out to be consistent with the ones reported at 128×256.
I. Qualitative results
We provide additional qualitative results of scene generation in Figures 8 and 9, and of face editing in Figures 10, 11

12

0.3 0.25
0.2 0.15
0.1 0.05
0 0.3 0.25 0.2 0.15 0.1 0.05
0
0.3 0.25
0.2 0.15
0.1 0.05
0
0.3 0.25
0.2 0.15
0.1 0.05
0
Figure 8: Conditional layout-and-scene generation. Various layout-scene pairs sampled from the same semantic code (left).
13

(a) input layout

(b) cropped layout

(c) generated partial layout (d) final merged layout

Figure 9: Partial editing of layouts. The procedure consists in cropping ground-truth layouts and then synthesizing new objects within the cropped area, guided by the initial semantic proportions.
14

target generated
0,3
0,2
0,1
0

target generated
0,2
0,1
0

target generated
0,3
0,2
0,1
0

Ground-truth

Interpolation to target proportions

Figure 10: Hair manipulation 1: Grow existing hair. We select subjects with short hair and progressively increase the hair budget. The hair style corresponds to the input ground-truth image-layout pair. Please, zoom in for details.

15

target generated
0,05
0

target generated
0,2
0,1
0

target generated
0,15
0,1
0,05
0

Ground-truth

Interpolation to target proportions

Figure 11: Hair manipulation 2: Bald to not bald. We select bald subjects and progressively increase the hair budget. Since there is no hair initially, the hair style is randomly burrowed from another subject in the training set. Please, zoom in for details.

16

target

generated

0,06

0,06

0,06

0,04

0,04

0,04

0,02

0,02

0,02

0

0

0

target generated

0,3

0,15

0,2

0,2 0,1

0,1 0,1 0,05

0

0

0

target generated
0,015 0,01
0,01 0,01

0,005

0,005

0,005

0

0

0

Ground-truth Generated Ground-truth Generated Ground-truth Generated Figure 12: Manipulation of diverse semantic attributes. Glasses (1st row), hat (2nd), teeth (3rd). Please, zoom in for details.
17

