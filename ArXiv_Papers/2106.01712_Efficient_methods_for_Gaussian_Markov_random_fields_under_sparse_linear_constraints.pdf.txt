Efficient methods for Gaussian Markov random fields under sparse linear constraints

David Bolin King Abdullah University of
Science and Technology david.bolin@kaust.edu.sa

Jonas Wallin Department of Statistics,
Lund University jonas.wallin@stat.lu.se

arXiv:2106.01712v1 [stat.ME] 3 Jun 2021

Abstract
Methods for inference and simulation of linearly constrained Gaussian Markov Random Fields (GMRF) are computationally prohibitive when the number of constraints is large. In some cases, such as for intrinsic GMRFs, they may even be unfeasible. We propose a new class of methods to overcome these challenges in the common case of sparse constraints, where one has a large number of constraints and each only involves a few elements. Our methods rely on a basis transformation into blocks of constrained versus non-constrained subspaces, and we show that the methods greatly outperform existing alternatives in terms of computational cost. By combining the proposed methods with the stochastic partial differential equation approach for Gaussian random fields, we also show how to formulate Gaussian process regression with linear constraints in a GMRF setting to reduce computational cost. This is illustrated in two applications with simulated data.

1 Introduction

Linearly constrained Gaussian processes have recently gained attention, especially for Gaussian process regression where the model should obey some underlying physical principle such as conservation laws or equilibrium conditions [SU10; Sär11; Wah+13; Jid+17; LH18; Sol+18; LH21]. A well-known challenge with these models, and Gaussian processes in general, is their high computational cost for inference and prediction in the case of big data sets [AMD14; CCZ17; Jid+18]. One way to reduce computational burden is to impose conditional independence assumptions. In fact, conditional independence between random variables is often explicitly or implicitly assumed in large classes of statistical models including Markov processes, hierarchical models, and graphical models. The assumption typically increases the model's interpretability and facilitates computationally efficient methods for inference [RMC09]. Gaussian variables with conditional independence properties are known as Gaussian Markov random fields (GMRFs), and these are widely used in areas ranging from image analysis [PTBF05] to spatial statistics [Bol+09] and time series analysis [RH05]. GMRFs also arise as computationally efficient approximations of certain Gaussian processes [LRL11], which is a fundamental modeling tool in both machine learning and statistics. In particular, such approximations in combination with the integrated nested Laplace approximation (INLA) methodology [RMC09] made latent GMRFs widely used in the applied sciences [Bak+18]. GMRFs also have connections with convolutional neural networks leading to recent considerations of deep GMRFs [SL20].

The key feature of GMRFs that reduces computational cost is sparsity. Specifically, a GMRF

X = [X1, . . . , Xn]  N µ, Q-1 ,

(1)

has a sparse precision (inverse covariance) matrix Q which enables the use of sparse matrix techniques

for computationally efficient sampling and statistical inference. The sparsity is caused by conditional
independence assumptions since Qij = 0 if and only if the two variables Xi and Xj are independent conditionally on all other variables in X [see RH05, Chapter 2]. Preprint. Under review.

For a GMRF X, a set of k linear constraints can be formulated as

AX = b,

(2)

where each row in the k × n matrix A and the vector b encodes a constraint on X. For example,

a sum-to-zero constraint

n i=1

Xi

=

0

can

be written

in

this

way,

which is

commonly

used

in

hierarchical models to ensure identifiability. Observations of GMRFs can also be formulated as

linear constraints, where, e.g., a point observation of Xi is the simple constraint Xi = xi. These

deterministic restrictions are often referred to as hard constraints. If (2) is assumed to hold up to Gaussian noise, i.e., AX  N (b, 2I), then the constraints are instead referred to as soft constraints.

This scenario is common when GMRFs are incorporated in hierarchical models, where the soft

constraints represent noisy observations.

The problem with adding hard constraints to GMRFs is that it can remove the computational advantages of the non-constrained model. Specifically, current methods for GMRFs with hard constraints have a computational cost that scales cubically in the number of constraints. This will thus be prohibitive when there are many constraints, which for example is common for constrained Gaussian processes like those considered by Jidling et al. [Jid+17].

The code for reproducing all results are available at can be found at https://github.com/ JonasWallin/CB/

Summary of contributions. The main contribution of this work is the formulation of a novel class of computationally efficient methods for linearly constrained GMRFs in tasks such as parameter estimation and simulation. These methods explore the conditional independence structure to reduce computational costs compared to traditional methods for situations with large numbers of constraints. The focus is in particular on the case, referred to as sparse hard constraints, when each constraint only involves a few number of variables so that A is sparse. For this case, the main idea is to perform a change of basis so that the constraints are simpler to enforce in the transformed basis. In order to use these models for Gaussian process regression, the methods are also generalized to GMRF models with both hard and soft constraints. An important feature of the new class of methods, that previous approaches lack, is its applicability to intrinsic GMRFs, which are improper in the sense that a set of eigenvalues of Q is zero. This makes their distributions invariant to shifts in certain directions, which is a useful property for prior distributions of Bayesian models in areas such as medical image analysis [PTBF05] and geostatistics [Bol+09]. The final contribution is the derivation of GMRFs for constrained Gaussian processes, by combining the proposed methods with the stochastic partial differential equation (SPDE) approach by [LRL11] and the nested SPDE methods by [BL11]. The combined approach is highly computationally efficient compared to standard covariance-based methods, as illustrated in two simulation studies.

Outline. In Section 2, the problem is introduced in more detail and the most commonly used methods for sampling and likelihood computations for GMRFs are reviewed. Section 3 introduces the new methods for GMRFs with sparse hard constraints. These methods are extended to the case with both hard and soft constraints in Section 4, followed by the GMRF methods for constrained Gaussian processes in Section 5. The methods are illustrated nuumerically in Section 6. A discussion closes the article, which is supported by three appendices containing proofs and technical details.

2 Standard methods for GMRFs under hard constraints

Hard constraints can be divided into interacting and non-interacting constraints. In the latter, (2)

specifies a constraint on a subset of the variables in X so that AX = b can be written as Xc = bc, where c denotes a subset of the variables. Specifically, let Xu denote the remaining variables, then

X=

Xc Xu

N

µc µu

,

Qcc Quc

Qcu -1 Quu

and X|AX = b  N

bc µu|c

,

0 0

0 Q-uu1

,

where µu|c = µu - Q-uu1Quc(bc - µc). Thus, we can split the variables into two subsets and treat the unconditioned variables separately. The more difficult and interesting situation is the case of interacting hard constraints where a simple split of the variables is not possible. From now on, we will assume that we are in this scenario.

Our aim is to construct methods for sampling from the distribution of X|AX = b and for evaluating its log-likelihood function. It is straightforward to show that X|AX = b  N (µ, ), where

2

µ = µ - Q-1A (AQ-1A )-1(Aµ - b) and  = Q-1 - Q-1A (AQ-1A )-1AQ-1. Since
 has rank n - k, likelihood evaluation and sampling is in general expensive. For example, one way is to use an eigenvalue decomposition of  [see RH05, Chapter 2.3.3]. However, this procedure is not practical since it cannot take advantage of the sparsity of Q. Also, for intrinsic GMRFs,  and µ cannot be constructed through the expressions above since Q-1 is unbounded.

A commonly used method for sampling under hard linear constraints, sometimes referred to as conditioning by kriging [Rue01], is to first sample X  N µ, Q-1 and then correct for the constraints by using X = X - Q-1A (AQ-1A )-1(AX - b) as a sample from the conditional distribution. Here the cost of sampling X is CQ + SQ, where CQ denotes the computational cost of a
sparse Cholesky factorization Q = R R and SQ the cost of solving Rx = u for x given u. Adding the cost for the correction step, the total cost of the method is O(CQ + (k + 2)SQ + k3).
Let Ax(·) denote the density of Ax  N (Aµ, AQ-1A ), then the likelihood of X|AX = b can be computed through the expression

(x|Ax = b) = A x|x(b|x)(x) ,

(3)

Ax(b)

where A x|x(b|x) = I(Ax = b)|AA |-1/2 and I(Ax = b) denotes the indicator function with I(Ax = b) = 1 if Ax = b and I(Ax = b) = 0 otherwise. This result is formulated in [Rue01] and we provide further details in Appendix C by showing that (3) is a density with respect to the Lebesgue measure on the level set {x : Ax = b}. The computational cost of evaluating the likelihood using this formulation is O(CQ + (k + 1)SQ + k3).
Note that these methods only work efficiently for a small number of constraints, because of the term k3 in the computational costs, and for proper GMRFs. In the case of intrinsic GMRFs we cannot work with AQ-1A due to the rank deficiency of Q.

3 The basis transformation method

In this section, we propose the new class of methods in two steps. We first derive a change of basis in Section 3.1, and then use this to formulate the desired conditional distributions in Section 3.2. The resulting computational costs of likelihood evaluations and simulation are discussed in Section 3.3.

Before stating the results we introduce some basic notation. When working with intrinsic GMRFs the

definition in (1) is inconvenient since the covariance matrix has infinite eigenvalues. Instead one can

use the canonical parametrization X  NC (µC, Q) , which implies that the density of X is given

by (x)  exp

-

1 2

x

Qx + µC x

and thus that µC = Qµ. Also, since we will be working with

non-invertible matrices we will need the Moore­Penrose inverse and the pseudo determinant. We

denote the Moore­Penrose inverse of a matrix B by B and for a symmetric positive semi definite matrix M we define the pseudo determinant as |M| = i:i>0 i where {i} are the eigenvalues of M. Finally, for the remainder of this article, we will assume the following.

Assumption 1. X  NC (Qµ, Q) where Q is a positive semi-definite n × n matrix with rank n - s > 0 and null-space EQ. A is a k × n matrix with rank k and rank(AEQ) = k0.

3.1 Basis construction
Our main idea is to construct a basis on Rn such that the constraints are easily enforced. A key property of the basis is that A should be spanned by the first k elements of the basis. Essentially, this means that we are transforming the natural basis into one where the results for the case of non-interacting hard constraints can be used. The basis is defined by an n × n change-of-basis matrix which we denote T. In Algorithm 1 we present a simple method to produce such a matrix, using the singular value decomposition (SVD). In the algorithm, id(A) is a function that returns the indices of the non-zero columns in A and A·,D denotes the matrix obtained by extracting the columns in A with indices in the set D. The computational cost of the method is dominated by that of the SVD, which is O k3 + k2|id(A)| [GVL13, p. 493].

3

Clearly, the cubic scaling in the number of con-

straints may reduce the efficiency of any method Algorithm 1 Constraint basis construction.

that requires this basis construction as a first step.

However, suppose that the rows of A can be Require: A (a k × n matrix of rank k)

split into two sub-matrices, A1 and A2, which have no common non-zero columns. Then the

1: T  In 2: D  id(A)

SVD of the two matrices can be computed sep- 3: USV  svd(A·,D)

arately. Suppose now that A corresponds to m 4: TD,D  V

such sub-matrices, then, after reordering, A = 5: T  [T·,D T·,Dc ]

A1 , . . . , Am where {A~ i}m i=1 represent sub- 6: Return T

constraints such that id(A~ i)  id(A~ l) =  for

all i and l. By replacing the SVD of Algo-

rithm 1 by the m SVDs of the lower-dimensional matrices the computational cost is reduced to

O

m i=1

rank(A~ i)3

+

rank(A~ i)2|id(A~ i)|

.

This method is presented in Algorithm 2.

The re-

ordering step is easy to perform and is described in Appendix A, where also more details about the

algorithm are given.

3.2 Conditional distributions

Using the change of basis from the previous subsection, we can now derive alternative formulations of the distributions of AX and X|AX = b which are suitable for sampling and likelihood-evaluation. There are two main results in this section. The first provides an expression of the density of AX that allows for computationally efficient likelihood evaluations for observations AX = b. The second formulates the conditional distribution for X|AX = b in a way that allows for efficient sampling of X given observations AX = b. To formulate the results, let T = CB(A) be the output of Algorithm 1 or Algorithm 2 and X = TX which, under Assumption 1, has distribution X  NC (Qµ, Q) , where µ = Tµ and Q = TQT . Henceforth we use stars to denote quantities such as means and precisions in the transformed space. Note that we can move from the
transformed space to the original space by multiplying with T for a vector and by multiplying with
T from the left and T from the right for a matrix.

Since A is spanned by the first k elements in T, we use the index notation C = {1, . . . , k} and U = {k + 1, . . . , n}. We also use the matrix H = AT C C , which is equal to USC C from the SVD in Algorithm 1, and therefore has inverse H-1 = S-C1C U . Finally we define b = H-1b. Theorem 1. Under Assumption 1 it follows that

AX(b)

=

|QC

|U

|

 2

(2)k/2|AA

|1/2

· exp

-

1 2

(b

-

µC

)

QC |U (b - µC )

,

where QC |U

= QC C

- QC U

(QU U ) QU C

and

|QC

|U

|

 2

=

|Q|

1 2

|QU

U

|-

1 2

.

If Q is positive definite we have QC |U = QC C - QC U (QU U )-1 QU C and we can then replace

|QC

|U

|

 2

with

|QC

|U

|

1 2

in

the

expression

of

AX.

Algorithm 2 CB(A). Constraint basis construction for non-overlapping subsets of constraints.

Require: A (a k × n matrix of rank k) 1: T  In 2: Dfull  id(A) 3: h  1
4: l  k + 1

5: Reorder so that A = A1 6: for i = 1 : m do 7: D  id(A~ i)
8: USV  svd(Ai) 9: u  ncol(U)

. . . Am

10:

Th:(h+u),D  V 1:u,D

11: h  h + u + 1

12: if |D| > u then

13:

Tl:(l+|D|-u),D  V (u+1):|D|,D

14:

l  l + |D| - u + 1

15: end if

16: end for

17: T  T·,1:|Dfull| (In)·,Dfcull

18: Return T

4

Theorem 2. Under Assumption 1 it follows that

X|AX = b  NC QX|bµ, QX|b I (AX = b) ,

(4)

where QX|b = TU ,QU U TU , is positive semi-definite with rank n - s - (k - k0) and µ = T µ

with µ =

. b
µU - QUU QU C (b - µC )

Note that QX|bA = 0, which implies that the right side of (4) is a (possibly intrinsic) density with respect to Lebesgue measure on the level set {x : Ax = b}. Further, note that QU U TU E0 = 0, which implies that X is improper on the span of TU E0. If Q is positive definite we get the following corollary. Corollary 1. Under Assumption 1 with s = 0, we have

X|AX = b  N µ,  I (AX = b) ,

(5)

where  = TU , (QU U )-1 TU , is a positive semi-definite matrix of rank n - k and µ = T µ

with µ =

b µU - (QU U )-1 QU C (b - µC ) .

3.3 Sampling and likelihood evaluations

The standard method for sampling a GMRF X  N µ, Q-1 is to first compute the Cholesky factor R of Q, then sample Z  N (0, I), and finally set

X = µ + R-1Z.

(6)

To sample X|AX = b we use this method in combination with Theorem 2 as shown in Algorithm 3. The cost of using the algorithm for sampling, and for computing the expectation of X in Theorem 2, is dominated by CQU U given that T has been pre-computed. Similarly, the cost for evaluating the likelihood in Theorem 1 is dominated by the costs of the Cholesky factors CQU U + CQ + CAA .
These costs are not directly comparable to costs of the methods from Section 2 since they involve operations with the transformed precision matrix Q = TQT which may have a different, and often denser, sparsity structure than Q. In fact if T is dense the method will not be practically useful since even the construction of Q would be O n2 . Thus, to understand the computational cost we must understand the sparsity structure of the transformed matrix. To that end, first note that only the rows id(A) in Q will have a sparsity structure that is different from that in Q. In general, the variables involved for the ith constraint, id(AiX), will in the constrained distribution share all their neighbors. This implies that if i  id(A), then |Qi,j| > 0 if |Qi,j| > 0 and we might have |Qi,j| > 0 if kid(A) |Qk,j| > 0. This provides a worst-case scenario for the amount of non-zero elements in Q, where we see that the sparsity of the constraints is important.

4 GMRFs under hard and soft constraints

As previously mentioned, one can view observations of a GMRF as hard constraints. In many cases, these observations are assumed to be taken under Gaussian measurement noise, which can be seen as soft constraints on the GMRF. It is therefore common to have models with both soft and hard constraints (e.g., a model with noisy observations of a field with a sum-to-zero constraint). Here, we extend the methods of the previous section to this case. Specifically, we consider the following hierarchical model

X  NC (Qµ, Q) , subject to AX = b,

Y  N BX, Y2 I ,

(7)

where Y  Rm represent noisy observations of the linear combinations BX of X  Rn, with m  n, satisfying Assumption 1, and B is an m × n matrix with rank m. To deal with this type of models we present two results in this section. First, Theorem 3 shows how to compute the likelihood of the model. Second, the result in Theorem 4 can be used to efficiently compute the mean of X given the constraints and to sample from it.

5

Algorithm 3 Sampling X  N µ, Q-1 Algorithm 4 Sampling X  N µ, Q-1 subject to

subject to AX = b.

AX = b and Y = y where Y  N BX, Y2 I .

Require: A, b, Q, µ, T
1: C  1 : nrow(A) 2: U  (nrow(A) + 1) : ncol(A) 3: Q  TQT 4: R  chol(QU U )
5: b  solve AT C C , b 6: m  µU -
solve(R , QU C (b - TC µ)) 7: Sample Z  N (0, IU U ) 8: X  [b, solve(R, m + Z)]

Require: A, b, Q, µ, T, y, B, Y2 1: C  1 : nrow(A)

2: U  (nrow(A) + 1) : ncol(A)

3: Q  TQT

4: B  BT

5:

R  chol(QU U

+

1 Y2

(B)

B)

6: b  solve AT C C , b

7: y  y - BTC b

8: m  solve R , QU U TU µ+

9: X  T X 10: Return X

1 Y2

(B)

y - QU C (b - TC µ)

9: Sample Z  N (0, IU U )

10: X  [b, solve(R, m + Z)]

11: X  T X

12: Return X

We use the hat notation ­ like Q ­ to denote quantities for distributions conditionally on the observations Y = y. We also use the notation from Theorem 2 and additionally introduce B = BT and y = y - BTC b. We start by deriving the likelihood, Y|AX(y|b), which is needed for inference.
Theorem 3. For the model in (7) one has

Y|AX(y|b)

=

Y-m|QU

U

|

 2

(2)c0


|QU

U

|

 2

exp

1 -
2

yT y Y2

+ µU

QU U µU

- µU


QU

U

µU

,

where c0 > 0, and


QU U

= QU U

+

1 Y2

(BU )

BU ,

µU


= QU U

QU U µU

1 + Y2

(BU )

y

.

The computational cost of evaluating the likelihood is CQU U + SQU U + CQU U . The following theorem contains the distribution of X given the event {AX = b, Y = y}, which for example is

needed when the model is used for prediction.


Theorem 4. For model in (7) one has X|AX,Y(b, y)  N µ, Q where Q = TU ,QU U TU ,

and µ = T

b µU

.


Here QU U

and µU

are given in Theorem 3.

Further, let EQU U

be the null

space of QU U then rank(Q) = n - s - (k - k0) + rank(BU EQU U ).

Since the distribution in the theorem is a normal distribution, we can sample from X given the event {AX = b, Y = y} using sparse Cholesky factorization as shown in Algorithm 4.

5 Constrained Gaussian processes and the SPDE approach

Gaussian processes and random fields are typically specified in terms of their mean and covariance functions. However, a problem with any covariance-based Gaussian model is the computational cost for inference and simulation. Several authors have proposed solutions to this problem, and one particularly important solution is the GMRF approximation by [LRL11]. This method is applicable to Gaussian process and random fields with Matérn covariance functions,

r(h)

=

2 ()2

-1

(h)

K

(h),

h  0,

6

which is the most popular covariance model is spatial statistics, inverse problems and machine learning [GG06; RW06]. The method relies on the fact that a Gaussian random field X(s) on Rd
with a Matérn covariance function can be represented as a solution to the SPDE

(2

-


) 2

X

=

W ,

(8)

where the exponent  is related to  via the relation  =  + d/2,  is the Laplacian, W is Gaussian

white noise on Rd, and  is a constant that controls the variance of X. The GMRF approximation

by [LRL11] is based on restricting (8) to a bounded domain D, imposing homogeneous Neumann

boundary conditions on the operator, and approximating the solution via a finite element method

(FEM). The resulting approximation is Xh(s) =

n i=1

Xii(s),

where

{i(s)}

are

piecewise

linear

basis functions induced by a triangulation of the domain, and the vector X with all weights Xi is a

centered multivariate Gaussian distribution. This can be done for any  > d/2 [BK20], but the case

  N is of particular importance since X then is a GMRF. In particular, when  = 2 and  = 1, the precision matrix of X is Q = (2C + G)C-1(2C + G), where C is a diagonal matrix with

diagonal elements Cii = i(s)ds, and G is a sparse matrix with elements Gij = i(s)j(s)ds.

Clearly, a linear constraint on Xh(s) can be written as a constraint on X. For example, if Xh(s) is observed at a location in a given triangle, it creates a linear constraint on the three variables in X corresponding to the corners of the triangle. Thus, if we draw some observation locations s1, . . . , sk in the domain, we can write Y = (Xh(s1), . . . , Xh(sk)) = AY X where AY is a k ×n matrix with (AY )ij = j(si). Thus, a model where a Gaussian Matérn fields is observed without measurement noise can efficiently be handled by combining the SPDE approach with the methods from Section 3.
The next section contains a simulation study that compares this combined approach with a standard
covariance-based approach in terms of computational cost.

Through the nested SPDE approach in [BL11] one can also construct computationally efficient

representations of differentiated Gaussian Matérn fields like U (s) = (v )X(s), where v  is the

directional derivative in the direction given by the vector v and X(s) is a sufficiently differentiable

Matérn field. A FEM approximation of this model can be written as Uh(s) =

n i=1

Uii

(s)

where now U  N (0, AU Q-1AU ). Here AU is a sparse matrix representing the directional

derivative and Q is the precision matrix of the GMRF representation of X(s) [BL11]. If we introduce

X  N (0, Q-1), we may write U = AU X, and we can thus enforce a restriction on the directional

derivative of X as a linear restriction AU X = b. As an example, v = (1, 1) and b = 0 results in

the

restriction

  s1

X (s)

+

  s2

X

(s)

=

0,

or

in

other

words

that

the

field

is

divergence-free.

In

the

next section we use this in combination with the methods in Section 4 to construct a computationally

efficient Gaussian process regression under linear constraints.

6 Numerical illustrations
In this section we present two applications. In both cases, timings are obtained through R [R C20] implementations (see the supplementary materials) run on an iMac Pro computer with a 3.2 GHz Intel Xeon processor. The supplementary material contains the source code, including recipes for constructing the figures.
6.1 Observations as hard constraints
Suppose that we have an SPDE approximation Xh(s) of a Gaussian Matérn field X(s), as described above with D = [0, 1]2 and a triangulation for the GMRF approximation that is based on a uniform mesh with 100 × 100 nodes in D. We consider the costs of sampling Xh(s) conditionally on k point observations without measurement noise, and of log-likelihood evaluations for these observations. In both cases, the observations are simulated using the parameters 2 = 0.5,  = 1 and  = 2.
We show in the left panel of Figure 1 the computation time for evaluating the log-likelihood using the standard method from Section 2 on the GMRF of weights X for the basis expansion of Xh(s). The panel also shows the corresponding computation time for the new method from Section 3. The computation times are evaluated for different values of k, where for each k the observation locations are sampled uniformly over triangles, and uniformly within triangles, under the restriction that there can be only one observation per triangle, which guarantees that AY Q-1AY has full rank. In each iteration, the values of 2 and  that are evaluated in the likelihood are sampled from a uniform

7

Evaluation time (s) 0 5 10 15 20 25
Sampling time (s) 0 5 10 15 20 25

GMRF basis method GMRF old method Covariance-based method

GMRF old method GMRF basis method

0

1000

2000

3000

4000

0

1000

2000

3000

4000

k

k

Figure 1: Average computation times for one likelihood evaluation (left) and one sample from

X|AY X = y (right) of the Matérn model as a function of the number of observations k. As an indication of the uncertainty, the envelopes show the smallest and largest value for each k.

distribution on [1, 2]. The curves shown in the figure are computed as averages of 10 repetitions for each value of k. As a benchmark, we also show the time it takes to evaluate the log-likelihood assuming that X(s) is a Gaussian Matérn field, which means that we evaluate the log-likelihood of a k-dimensional N (0, ) distribution without using any sparsity properties.
Note that the covariance-based method is the fastest up to approximately 1000 observations, since the problem then is too small for sparsity to be beneficial. For more than 1000 observations, the new method wins and it has in fact a computation time that is decreasing in the number of observations. It should be noted that the difference between the new and old method would be even larger if we reported the computation time for more than one likelihood evaluation, since the construction of the basis needed for the new method only has to be done once.
In the right panel of Figure 1 we show the time needed to sample Xh(s) conditionally on the observations AY X = y, i.e., to simulate from X|AY X = y. Both the old method (conditioning by kriging) and the new method (using (6) with mean and precision from Theorem 2) are shown. We do not show the covariance-based method since it is much slower than both GMRF methods. Also here the displayed values are averages of 10 repetitions for each value of k, and for each repetition the simulation is performed using values of 2 and  that sampled from a uniform distribution on [1, 2]. The results are similar to those for likelihood evaluations, where the old method is fastest for a low number of observations whereas the new method is much faster for large numbers of observations.

6.2 Gaussian process regression with linear constraints

We now consider the application from [Jid+17], where we assume that we are given noisy observations Yi = f (si) + i, with i  N (0, e2I) of a bivariate function f = (f1, f2) : R2  R2 with f1(s) = e-as1s2 (as1 sin(s1s2) - s1 cos(s1s2)) and f2(s) = e-as1s2 (s2 sin(s1s2) - as2 sin(s1s2)).
The goal is to use Gaussian process regression to reconstruct f , under the assumption that we know

that

it

is

divergence-free,

i.e.,

  s1

f

+

  s2

f

=

0.

We

thus

want

to

improve

the

regression

estimate

by incorporating this information in the Gaussian process prior for f . This can be done as in [SS12;

Wah+13; Jid+17] by encoding the information directly in the covariance function, or by imposing the

restriction through a hard constraint at each spatial location through the nested SPDE approach. This

is done by setting B = AY and A = AU in (7) where the matrices are defined in Section 5.

We choose a = 0.01 and 2 = 10-4 and generate 50 observations at randomly selected locations in [0, 4] × [0, 4] and predict the function f at N 2 = 202 regularly spaced locations in the square. Independent Matérn priors with  = 4 are assumed for f1 and f2 and the covariance-based approach by [Jid+17] is taken as a baseline method. As an alternative, we consider the SPDE approximation of the Matérn priors, with n basis functions obtained from a regular triangulation of an extended domain [-2, 6] × [-2, 6] (the extension is added to reduce boundary effects). To be able to use Algorithm 2, we only enforce the divergence constraint at every third node for the SPDE model. This procedure can be seen as an approximation of the divergence operator that will converge to the true operator when the number of basis functions increases.

The parameters of the baseline model and of the SPDE model are estimated using maximum likelihood, where the likelihood for the SPDE model is computed using Theorem 3. The function f is then

8

RMSE 0.4 0.8 1.2 1.6
Prediction time (s) 0 10 20 30

SPDE method Mean Covariance-based method

SPDE method Covariance-based method

2000 3000 4000 5000 6000 7000 8000

0

500

1000

1500

2000

n

m

Figure 2: Left: Average RMSE with corresponding 95% pointwise confidence band for 50 reconstruc-

tions of f based on SPDE method with different number of basis functions n (red), the corresponding

RMSE for the Gaussian process model (black) and the RMSE for estimating f by a constant equal to

the mean of the observations (blue). Right: Computation time for the prediction of f as a function of

the number of observations (m) with envelopes as in Figure 1.

reconstructed using the posterior mean of the Gaussian process given the data, which is calculated using Theorem 4. This experiment is repeated for 50 randomly generated datasets. In the left panel of Figure 2 we show the average root mean squared error (RMSE) for the reconstruction of f for the SPDE model as a function of n, based on these 50 simulations, together with the corresponding RMSE of the baseline method. The shaded region for the SPDE model is a pointwise 95% confidence band. One can see that the SPDE model gives a comparable RMSE as long as n is large enough.
We next fix n = 3600 and consider the time it takes to compute a prediction of f given the estimated parameters. In the right panel of Figure 2 we show this computation time as a function of the number of observations, m, for the baseline method and for the SPDE-based method. Also here we see that the covariance-based method is the fastest for small numbers of observations, whereas the GMRF method (that has a computational cost that scales with the number of basis functions of the SPDE approximation rather than with the number of observations) is fastest whenever m > 600.

7 Discussion
We proposed new methods for GMRFs under hard and soft linear constraints, which can greatly reduce computational costs for models with a large number of constraints. In addition, we showed how to combine these methods with the SPDE approach to allow for computationally efficient linearly constrained Gaussian process regression.
Clearly the proposed methods will not be beneficial if the number of constraints is small. Another limitation is that the methods are only efficient if the constraints are sparse. An interesting topic for future research is to handle problems where both sparse and dense constraints are included. In that case one could combine the proposed method with a conditioning by kriging approach where the dense constraints are handled in a post-processing step as described in Section 2.
Some recent papers on constrained Gaussian processes, such as [CCZ17], consider methods that are similar in spirit to those we have developed here. However, to the best of our knowledge, the methods proposed here are the first to account for sparsity, which is the crucial property for GMRFs. We have only considered exact methods in this work, but if one is willing to relax this requirement, an interesting alternative is the Krylov subspace methods by [STP08]. Comparing, or combining, the proposed methods with iterative Krylov subspace methods is an interesting topic for future research.
In Section 6.2 we showed the advantages of using our proposed method together with SPDE approach for Gaussian process regression with linear constraints. The SPDE approach also allows for more flexible non-stationary covariance structures like the generalized Whittle­Matérn models [BK20]. Our proposed methods are directly applicable to these models in the Markov case (with integer smoothness), and an interesting research direction would be to extend our methods to the case with general smoothness by combining the constraint basis with the rational approximations in [BK20].
Finally, we can think of no potential negative societal impacts that this work may have, given that it is solely concerned with improving the performance of existing methods.
9

A Details of the constraint basis construction
In this section we provide more details about the algorithms for constructing the constraint basis.
A first natural question is why the singular value decomposition (SVD) is a natural method for building a basis with non-interacting hard constraints. To answer that, note that if USV = SV D(A), then by construction the basis V is orthonormal and the first k rows span the image of A and the last n - k rows span the null-space of A. Now, if we let x denote a vector x expressed in the basis V , then x can be transformed back to the natural basis by x = Vx hence
Ax = b  USx = b  [USC ,C 0] x = b  USC ,C xC = b.
A second question that should be addressed is how the reordering of the A matrix in Algorithm 2 is done. This is illustrated in Algorithm 5 where we show how to build the sub-matrices {A~ }m k=1.

Algorithm 5 Find all non-overlapping sub-
matrices
Require: A (a k × n matrix) 1: {A~ 1, B}  overlap(A) 2: m  1 3: while B =  do 4: m  m + 1 5: {A~ m, B}  overlap(B) 6: end while 7: Return {A~ }m k=1

Algorithm 6 overlap(A)Find first sub-matrix

Require: A (a k × n matrix) 1: U  {1} 2: d  0 3: D  id(AU,·) 4: while 1 do 5: D  id(AU,·)

6: U  id (A·,D)

7: if d = |U | then

8:

break

9: end if

10: d  |U |

11: end while

12: A = A·,U
c
13: A = A·,Uc
c
14: Return {A, A }

B Proofs

In this section we prove the four main theorems of the paper.

Proof of Theorem 1. We first transform the density of AX to the basis represented by T,

AX(b) = AT X (b) = XC H-1b |H|-1 = XC H-1b |AA |-1/2. In order to derive the density XC , note that the density of X is

XC (x) =

|Q|/2

(2)

n-s 2

exp

- 1 Q(x) 2

,

where the quadratic form Q(x) is

Q(x) =

xxUC

- -

µµCU

QQUC

U U

QQCCUC

xxUC

- -

µµUC

= (xC - µC ) QC C (xC - µC ) + (xC - µC ) QC U (xU - µU )

+ (xU - µU ) QU C (xC - µC ) + (xU - µU ) QU U (xU - µU )

=(xC - µC ) QC C (xC - µC ) - (xC - µC ) QC U QUU QU C (xC - µC ) +

+ xU - µU + QUU QU C (xC - µC ) QU U xU - µU + QUU QU C (xC - µC ) .

10

Here we in to the null

the last step wrote the expression so that we easily can space of QU U . Doing so yields the desired result,

integrate

out

XU

on the complement

(xC ) =

XC (x)dxU



|Q|/2 |QU U |/2

exp

-

1 2

Q^(xC

)

,

where

Q^(xC ) = (xC - µC ) QC C - QC U QUU QU C (xC - µC ) .

To prove Theorem 2 we need the following lemma. Lemma 1. Under Assumption 1 one has rank (QC C ) = k-k0 and rank (QU U ) = n-s-(k-k0).
Proof. We have rank(Q) = rank(Q) = n - s since T is orthonormal matrix. Further, using the eigen-decomposition of Q we can express Q as

Q =

TC TU

E0c E0

 0 E0c 0 0 E0

TC TU

,

where  is a diagonal matrix with the non-zero eigenvalues of Q. Since rank(AE0) = k0 it follows

that also rank(TC E0) = k0 and rank(TU E0) = s - k0. By Theorem 4.3.28 of [HJ13] there

earQexnqCidsuCtQisreiaCsmnUkeene-itg,=ekann0v0.de.cnBtooyro,ctheoe,nrostvfreuQcctUtoiorUnd,otaehnsa.ytHhvaeenscctaoerc, otchroernerssatprnuokcntodefidnQgbUyeitUgheenislviannleu-aer

0 if and only if QU U e = 0 span of TU E0 satisfies this k - (s - k0) and the rank of

Proof of Theorem is proportional to

2. To derive the

exp(-

1 2

Q(x

)),

distribution where

we

note

that

the

conditional

distribution

of

XU

|XC

Q(x) =

xxUC

- -

µµCU

= (xU - µU )

QQUC

U U

QQCCUC

xxUC

- -

µµUC

QU U (xU - µU ) + 2 (xU - µU )

QU C (xC - µC ) + C,

where C is a constant independent of xU . Now, since QU U (QU U ) QU C = QU C , the quadratic form Q(x) can be written as a constant plus v QU C v, where
v = xU - µU + QUU QU C (xC - µC ).

Hence

XU |XC = b  NC QU U µU - QUU QU C (b - µC ) , QU U .

(9)

Combining this with the fact that X = T X gives the desired expression for the distribution of X|AX = b. Finally, since T is orthonormal, the rank of QX|b is the same as the rank of QU U , and the result follows from Lemma 1.

We start by proving Theorem 4 as we will use result from that proof in the proof of Theorem 3.

Proof of Theorem 4. First, note that the density

Y|XU ,XC (y|xU , b)

=

1

(2)

m 2

Ym

exp

can, as a function of xU , be written as

1 - 2Y2

y - B

b xU

y - B

b xU

Y|XC ,XU (y|b, xU )  exp

- xU

BU BU xU 2Y2

y +

BU xU Y2

.

, (10)

11

Further, from (9), we have that, as a function of xU ,

XU |XC (xU |b)  exp

1 -
2

xU - µU

QU U xU - µU

,

where µU = µU - QUU QU C (b - µC ). Y|XC ,XU (y|b, xU )(xU |b), it follows that

Since

XU |Y,XC (xU |y, b)

is proportional to



XU

|Y,XC

(xU

|y,

b)



exp

-

1 2

xU

BU BU Y2

xU

+

BU y Y2

 xU  ·

exp

-

1 2

xU

QU U xU

+

QU U µU

xU

 exp

1 -
2

xU - µU


QU U

xU - µU

.

Finally, using the relation X = T X completes the proof.

Proof of Theorem 3. First note that Y|AX (y|b) = Y|XC (y|b) and

Y|XC (y|b) = XU ,Y|XC (xU , y|b) dxU

= Y|XU ,XC (y|xU , b)XU |XC (xU |b)dxU .

(11)

The goal is now to derive an explicit form of the density by evaluating the integral in (11). By the expressions in the proof of Theorem 4 we have



Y|XU

,XC

(y|xU

,

b)XU

|XC

(xU

|b)

=

exp

-

1 2

xU

BU BU Y2

xU

+

BU y Y2

 xU  ·

exp

-

1 2

xU

QU U xU

+

QU U µU

xU ·

|QU U (2)c0

|/2 Ym

exp

1 -
2

y y Y2

+ µU

QU U µU

exp =XU |Y,XC (xU |y, b)

1 2

µU


QU

U

µU


|QU U |/2

·

|QU U (2)c1

|/2 Ym

exp

1 -
2

y y Y2

+ µU

QU U µU

,

where c0 and c1 are positive constants. Inserting this expression in (11) and evaluating the integral, where one notes that XU |Y,XC (xU |y, b) integrates to one, gives the desired result.

C Conditional constrained distribution

In order to derive the conditional density (x|Ax = b) in (3) we will use what is known as the disintegration technique. The proof is built on the results in [CP97], which has the following definition. Definition 1. Let (X , A, ) and (T , B, µ) be two measure spaces with -finite measures  and µ. The measure  has a disintegration {b} with respect to the measurable map A : (X , A)  (T , B) and the measure µ, or a (A(x), µ)-disintegration if:
(i) b is a -finite measure on A such that b (A(x) = b) = 0, for µ-almost all b,
and, for each non-negative measurable function f on X :

12

(ii) b  f db is measurable.

(iii) f d = f dbdµ.

In the following theorem, we use the notation from Appendix B and let n denote the Lebesgue measure on Rn. Further, we define U as the image measure of the projection onto the image of A (which is not -finite), and C as the image measure of the projection onto the null-space of A.
Theorem 5. Let X be a multivariate random variable with distribution P on (Rn, B(Rn)), where P has density  (x) with respect to n . Then the random variable X|AX = b has density

 (x|Ax = b) = I (Ax = b) |AA

|-1/2(x) ,

AX(b)

with respect to the measure Lb(·) = U (·  {x : Ax = b}) on (Rn, B(Rn)).

The proof is based on the following lemma. Lemma 2. The measure Lb(·) is the (A, Lk)-disintegration of the Lebesgue measure n.

Proof. Thus we need to show that (i), (ii), and (iii) of Definition 1 holds. Clearly, (i) follows immediate from ·  {x : Ax = b}. To show (ii), note that

f dLb = f T x IAT x=b (dx) dxU

= ||H||

f (xC , xU )dxU = ||H|| f (H-1b, xU )dxU ,

{x:xC =H-1b}

where H = AT C C as defined in Section 3.2, ||H|| denotes the absolute value of the determinant of H, and f (x) = f (T x). Since f is a measurable function it follows by Tonelli Theorem
[Pol02] that above partial integral is measurable. Finally, to show (iii), we continue from the equation
above and get

f dLbdb = ||H|| f (H-1b, xU )dxU db

= ||H|| |H|-1

f (b, xU )dxU db = f dn.

Proof of Theorem 5. By Lemma 2 above and Theorem 3 (v) in [CP97] it follows that the random variable has density

(x)

(x)

|AA |-1/2(x)

(x|AX = b) =

=

=

,

Lb(x) AX(b) ||H||

AX(b)

a.e. with respect to Lb. Finally, it holds that (x|AX = b) = I (Ax = b) (x|AX = b) a.e. since Lb(·) = U (·  {x : Ax = b}).

References

[AMD14]
[Bak+18] [BK20]

Yoann Altmann, Stephen McLaughlin, and Nicolas Dobigeon. "Sampling from a multivariate Gaussian distribution truncated on a simplex: A review". In: 2014 IEEE Workshop on Statistical Signal Processing (SSP). 2014, pp. 113­116. DOI: 10.1109/SSP.2014. 6884588.
Haakon Bakka et al. "Spatial modeling with R-INLA: a review". In: Wiley Interdiscip. Rev. Comput. Stat. 10.6 (2018), e1443, 24. ISSN: 1939-5108. DOI: 10.1002/wics.1443.
David Bolin and Kristin Kirchner. "The rational SPDE approach for Gaussian random fields with general smoothness". In: J. Comput. Graph. Statist. 29.2 (2020), pp. 274­285. ISSN: 1061-8600. DOI: 10.1080/10618600.2019.1665537.

13

[BL11] [Bol+09] [CCZ17] [CP97] [GG06] [GVL13] [HJ13] [Jid+17] [Jid+18]
[LH18] [LH21]
[LRL11]
[Pol02] [PTBF05] [R C20] [RH05]
[RMC09]

David Bolin and Finn Lindgren. "Spatial models generated by nested stochastic partial differential equations, with an application to global ozone mapping". In: Ann. Appl. Stat. 5.1 (2011), pp. 523­550. ISSN: 1932-6157. DOI: 10.1214/10-AOAS383.
David Bolin et al. "Fast estimation of spatially dependent temporal vegetation trends using Gaussian Markov random fields". In: Comput. Statist. Data Anal. 53.8 (2009), pp. 2885­2896. ISSN: 0167-9473. DOI: 10.1016/j.csda.2008.09.017.
Yulai Cong, Bo Chen, and Mingyuan Zhou. "Fast simulation of hyperplane-truncated multivariate normal distributions". In: Bayesian Anal. 12.4 (2017), pp. 1017­1037. ISSN: 1936-0975. DOI: 10.1214/17-BA1052.
Joseph T Chang and David Pollard. "Conditioning as disintegration". In: Statist. Neerlandica 51.3 (1997), pp. 287­317. ISSN: 0039-0402. DOI: 10 . 1111 / 1467 - 9574 . 00056.
Peter Guttorp and Tilmann Gneiting. "Studies in the history of probability and statistics. XLIX. On the Matérn correlation family". In: Biometrika 93.4 (2006), pp. 989­995. ISSN: 0006-3444. DOI: 10.1093/biomet/93.4.989.
Gene H. Golub and Charles F. Van Loan. Matrix computations. Fourth. Johns Hopkins Studies in the Mathematical Sciences. Johns Hopkins University Press, Baltimore, MD, 2013, pp. xiv+756. ISBN: 978-1-4214-0794-4; 1-4214-0794-9; 978-1-4214-0859-0.
Roger A. Horn and Charles R. Johnson. Matrix analysis. Second. Cambridge University Press, Cambridge, 2013, pp. xviii+643. ISBN: 978-0-521-54823-6.
Carl Jidling et al. "Linearly constrained Gaussian processes". In: Advances in Neural Information Processing Systems 30. Ed. by I. Guyon et al. Curran Associates, Inc., 2017, pp. 1215­1224.
Carl Jidling et al. "Probabilistic modelling and reconstruction of strain". In: Nuclear Instruments and Methods in Physics Research Section B: Beam Interactions with Materials and Atoms 436 (2018), pp. 141­155. ISSN: 0168-583X. DOI: https://doi.org/ 10.1016/j.nimb.2018.08.051.
Markus Lange-Hegermann. "Algorithmic Linearly Constrained Gaussian Processes". In: Advances in Neural Information Processing Systems 31. Ed. by S. Bengio et al. Curran Associates, Inc., 2018, pp. 2137­2148.
Markus Lange-Hegermann. "Linearly Constrained Gaussian Processes with Boundary Conditions". In: Proceedings of The 24th International Conference on Artificial Intelligence and Statistics. Ed. by Arindam Banerjee and Kenji Fukumizu. Vol. 130. Proceedings of Machine Learning Research. PMLR, 2021, pp. 1090­1098. URL: http: //proceedings.mlr.press/v130/lange-hegermann21a.html.
Finn Lindgren, Håvard Rue, and Johan Lindström. "An explicit link between Gaussian fields and Gaussian Markov random fields: the stochastic partial differential equation approach". In: J. R. Stat. Soc. Ser. B Stat. Methodol. 73.4 (2011). With discussion and a reply by the authors, pp. 423­498. DOI: 10.1111/j.1467-9868.2011.00777.x.
David Pollard. A user's guide to measure theoretic probability. Vol. 8. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, Cambridge, 2002, pp. xiv+351. ISBN: 0-521-80242-3; 0-521-00289-3.
William D Penny, Nelson J Trujillo-Barreto, and Karl J Friston. "Bayesian fMRI time series analysis with spatial priors". In: NeuroImage 24.2 (2005), pp. 350­362. DOI: 10.1016/j.neuroimage.2004.08.034.
R Core Team. R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing. Vienna, Austria, 2020. URL: https://www.R- project. org/.
Håvard Rue and Leonhard Held. Gaussian Markov random fields. Vol. 104. Monographs on Statistics and Applied Probability. Theory and applications. Chapman & Hall/CRC, Boca Raton, FL, 2005, pp. xii+263. ISBN: 978-1-58488-432-3; 1-58488-432-0. DOI: 10.1201/9780203492024.
Håvard Rue, Sara Martino, and Nicolas Chopin. "Approximate Bayesian inference for latent Gaussian models by using integrated nested Laplace approximations". In: J. R. Stat. Soc. Ser. B Stat. Methodol. 71.2 (2009), pp. 319­392. ISSN: 1369-7412. DOI: 10.1111/j.1467-9868.2008.00700.x.

14

[Rue01] [RW06] [Sär11] [SL20] [Sol+18] [SS12] [STP08] [SU10] [Wah+13]

Håvard Rue. "Fast sampling of Gaussian Markov random fields". In: J. R. Stat. Soc. Ser. B Stat. Methodol. 63.2 (2001), pp. 325­338. ISSN: 1369-7412. DOI: 10.1111/14679868.00288.
Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning. Adaptive Computation and Machine Learning. MIT Press, Cambridge, MA, 2006, pp. xviii+248. ISBN: 978-0-262-18253-9.
Simo Särkkä. "Linear Operators and Stochastic Partial Differential Equations in Gaussian Process Regression". In: Artificial Neural Networks and Machine Learning ­ ICANN 2011. Ed. by Timo Honkela et al. Berlin, Heidelberg: Springer Berlin Heidelberg, 2011, pp. 151­158. ISBN: 978-3-642-21738-8.
Per Sidén and Fredrik Lindsten. "Deep Gaussian Markov Random Fields". In: Proceedings of the 37th International Conference on Machine Learning. Ed. by Hal Daumé III and Aarti Singh. Vol. 119. Proceedings of Machine Learning Research. PMLR, 2020, pp. 8916­8926.
Arno Solin et al. "Modeling and Interpolation of the Ambient Magnetic Field by Gaussian Processes". In: IEEE Transactions on Robotics 34.4 (2018), pp. 1112­1127. DOI: 10. 1109/TRO.2018.2830326.
Michael Scheuerer and Martin Schlather. "Covariance models for divergence-free and curl-free random vector fields". In: Stoch. Models 28.3 (2012), pp. 433­451. ISSN: 15326349. DOI: 10.1080/15326349.2012.699756. URL: https://doi.org/10.1080/ 15326349.2012.699756.
Daniel P. Simpson, Ian W. Turner, and Anthony N. Pettitt. "Sampling from Gaussian Markov random fields conditioned on linear constraints". In: ANZIAM J. 48.(C) (2008), C1041­C1053 (2009). ISSN: 1446-1811.
Mathieu Salzmann and Raquel Urtasun. "Implicitly Constrained Gaussian Process Regression for Monocular Non-Rigid Pose Estimation". In: Advances in Neural Information Processing Systems 23. Ed. by J. D. Lafferty et al. Curran Associates, Inc., 2010, pp. 2065­2073.
Niklas Wahlström et al. "Modeling magnetic fields using Gaussian processes". In: 2013 IEEE International Conference on Acoustics, Speech and Signal Processing. 2013, pp. 3522­3526. DOI: 10.1109/ICASSP.2013.6638313.

15

