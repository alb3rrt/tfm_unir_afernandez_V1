Hierarchical Representation Learning for Markov Decision Processes
Lorenzo Steccanella1 , Simone Totaro2 , Anders Jonsson1 1Dept. Information and Communication Technologies, Universitat Pompeu Fabra, Barcelona, Spain
2Mila ­ Quebec Artificial Intelligence Institute, Montreal, Canada {lorenzo.steccanella, anders.jonsson}@upf.edu, simone.totaro@mila.quebec

arXiv:2106.01655v1 [cs.LG] 3 Jun 2021

Abstract
In this paper we present a novel method for learning hierarchical representations of Markov decision processes. Our method works by partitioning the state space into subsets, and defines subtasks for performing transitions between the partitions. We formulate the problem of partitioning the state space as an optimization problem that can be solved using gradient descent given a set of sampled trajectories, making our method suitable for high-dimensional problems with large state spaces. We empirically validate the method, by showing that it can successfully learn a useful hierarchical representation in a navigation domain. Once learned, the hierarchical representation can be used to solve different tasks in the given domain, thus generalizing knowledge across tasks.
1 Introduction
In reinforcement learning, an agent attempts to learn useful behaviors through interaction with an unknown environment. By observing the outcome of actions, the agent has to learn from experience which action to select in each situation in order to maximize the expected cumulative reward. To learn, the agent has to balance exploration, i.e. discovering the effect of actions on the environment, and exploitation, i.e. repeating action choices that have proven successful in the past.
In hierarchical reinforcement learning [Barto and Mahadevan, 2003], the task to be solved is decomposed into subtasks, and the solutions to the subtasks are combined to form a solution to the overall task. If each subtask is easier to solve than the overall task, the decomposition can significantly speed up learning. The subtasks can also help explore the environment more efficiently, since one high-level decision typically brings the learning agent multiple steps in a promising direction, rather than exploring locally one step at a time.
In most applications of hierarchical reinforcement learning, the subtask decomposition is provided by a domain expert that exploits extensive domain knowledge to define appropriate subtasks. Though many automatic methods have
Contact Author

been proposed, learning a useful subtask decomposition from experience is still mostly an open research question. In addition, several of the proposed methods are not appropriate for high-dimensional problems since they maintain statistics about individual states.
In this paper, we propose a novel method for automatically learning a hierarchical representation for reinforcement learning. The idea is to partition the state space into subsets, and define subtasks that perform transitions between the partitions. We formulate the problem of learning a hierarchical representation as an optimization problem that can be solved using gradient descent given a set of sampled trajectories. The resulting method can be applied to high-dimensional problems and combined with state-of-the-art function approximation algorithms for reinforcement learning.
In preliminary experiments, we show that our method can learn a useful subtask decomposition in a navigation domain with high-dimensional observations in the form of images. We also show that the learned hierarchical representation can be used to transfer knowledge to new, previously unseen tasks, thus generalizing knowledge across tasks.
1.1 Related Work
Hierarchical reinforcement learning algorithms using handcrafted subgoals to guide exploration, either as part of the value function representation [Nachum et al., 2018; Schaul et al., 2015; Sutton et al., 2017], or as pseudo-rewards [Eysenbach et al., 2019; Florensa et al., 2017], can solve hard exploration tasks with sparse rewards more efficiently than a flat learner, even for high-dimensional state and action spaces.
Early work on learning hierarchical representations for reinforcement learning focused on analyzing the state transition graph [Menache et al., 2002; S¸ ims¸ek et al., 2005], clustering nearby states [Mannor et al., 2004], discovering common substructure [Pickett and Barto, 2002] or identifying landmarks [McGovern and Barto, 2001; S¸ ims¸ek and Barto, 2004; Solway et al., 2014]. However, most of these methods rely on enumerating states, which is not possible in high-dimensional domains. Lakshminarayanan et al. [2016] used a spectral clustering algorithm on the representation of the state space learned using an unsupervised model prediction network [Oh et al., 2015]. Their method scales to high-dimensional state spaces but highly relies on the representation discovered by the neural network and does not perform clustering directly

on the original state space. Skill learning [Konidaris and Barto, 2007; Da Silva et al.,
2012] identifies initiation sets of options by searching backwards from a given set of target states where options terminate. Similar to our method, skills can be reused in a range of similar tasks. The option-critic framework [Bacon et al., 2017] and similar algorithms such as MODAC [Veeriah et al., 2021] use gradient descent to learn the components of each option from trajectories. However, the resulting options are not easily interpretable, unlike our options that always transition between partitions. DDO [Fox et al., 2017] leverages an Expectation-Maximization algorithm to train a hierarchy of options end-to-end for imitation learning. Unlike our method they do not explicitly consider initiation sets of the options.
Corneil et al. [2018] learn a latent state model given a sequence of observations, akin to learning a mapping from states to abstract states, using a neural network architecture similar to a variational autoencoder. Machado et al. [2017] use learned proto-value functions to identify subtask structure, in which a proto-value function becomes a local reward function for a given option. Shang et al. [2019] use variational inference to construct a partition similar to ours. However, unlike our model-free method, the option policies are trained using dynamic programming, which requires knowledge of the environment dynamics. Eysenbach et al. [2019] build distance estimates between pairs of states, and use the distance estimate to condition reinforcement learning in order to reach specific goals, which is similar to defining temporally extended actions.
Several authors have used state space partitions to define a hierarchical structure. Ecoffet et al. [2019] provide a handcrafted partition to learn to play Montezuma's revenge, using random search to find transitions between the partitions. Wen et al. [2020] take advantage of equivalent partitions with the same local dynamics to reuse option policies in multiple partitions. When the number of termination states of options is relatively small, the resulting algorithm has much better sample complexity properties than flat learning.
2 Background
In this section we define Markov decision processes and hierarchical reinforcement learning in the form of the options framework. For any finite set X, let (X) = {p  RX :
xX p(x) = 1, p(x)  0 (x)} denote the probability simplex on X, i.e. the set of all probability distributions over X.
2.1 Markov Decision Processes
A Markov decision process (MDP) [Puterman, 2014] is defined by a tuple M = S, A, P, r , where S is a finite state space, A is a finite action space, P : S × A  (S) is a transition kernel and r : S × A  R is a Markovian reward function. At time t, the learning agent observes a state st  S, takes an action at  A, obtains a reward rt with expected value E[rt] = r(st, at), and transitions to a new state st+1  P (·|st, at). We refer to (st, at, rt, st+1) as a transition.
A stochastic policy  : S  (A) is a mapping from states to probability distributions over actions. The aim of reinforcement learning is to compute a policy  that maximizes

some notion of expected future reward. In this work, we con-
sider the discounted reward criterion, for which the expected
future reward of a policy  can be represented using a value function V , defined for each state s  S as



V (s) = E

t-1r(St, At) S1 = s .

t=1

Here, St and At are random variables that model the state and action at time t, respectively, and the expectation is taken
over the choice of action At  (·|St) and the resulting next state St+1  P (·|St, At). The discount factor   (0, 1] is used to control the relative importance of future rewards.
As an alternative to the value function V , one can instead
model expected future reward using an action-value function Q, defined for each state-action pair (s, a)  S × A as



Q(s, a) = E

t-1r(St, At) S1 = s, A1 = a .

t=1

The value function V  and action-value function Q are related through the well-known Bellman equations:

V (s) = (a|s)Q(s, a),
aA
Q(s, a) = r(s, a) +  P (s |s, a)V (s ).
s S
The aim of the learning agent is to find an optimal policy  that maximizes the value in each state, i.e.  = arg max V .

2.2 Policy Gradient Methods
Since the state space S is usually large, it is common to define a set of features , and an associated mapping  : S   from states to features. In this setting, policy gradient methods such as asynchronous advantage actor-critic (A3C) [Mnih et al., 2016] maintain both an estimate  :   (A) of the
optimal policy (the actor) and an estimate V :   R of the optimal value function (the critic), both defined on features instead of states and parameterized on vectors  and , respectively. Given a transition st, at, rt, st+1 , A3C updates the parameter vector  of  using the regularized policy gradient rule

 log (at|(st))A(st, at) + H((·|(st)),

where A(st, at) is an estimate of the advantage function

A(st, at) = rt + V((st+1)) - V((st)),
H((·|(st)) is the entropy of the policy  in state st, and the parameter  controls the amount of regularization. The stability of the algorithm increases by using n-step returns, i.e. reward accumulated during n consecutive transitions. Moreover, the vectors  and  often share parameters, e.g. in a neural network setup all non-output layers can be shared, in which case only the output layers differ for  and V.

2.3 Options

Given an MDP M = S, A, P, r , an option is a temporally extended action o = Io, o, o , where Io  S is an initiation set, o : S  (A) is a policy and o : S  [0, 1] is a

termination function [Sutton et al., 1999]. An option can be applied in any state s  Io, repeatedly selects actions using policy o, and terminates in a state s  S with probability o(s ). A primitive action a  A is a special case of an option Ia, a, a with initiation set Ia = S, a(a|s) = 1 and a(s) = 1 (s), i.e. the option can be applied in any state,

terminates with probability 1 in any state, and the associated

policy always selects action a with probability 1.

Given a set of options O, we can form a semi-Markov de-

cision process (SMDP) S = S, O, P , R , where P and

R model the transition probabilities and expected reward of

options. Such an SMDP enables a learning agent to act and

reason on multiple timescales. At the top level, the learning

agent observes a state st, selects an option ot, executes option

ot until termination, and observes a next state st+k, where k

is the time it takes option ot to terminate. The reward Rt =

t+k-1 u=t

u-tru

is

the

discounted

sum

of

rewards

obtained

during the execution of option o. Hence (st, ot, Rt, st+k) is

a high-level transition, and with minor modifications, rein-

forcement learning algorithms can be adapted to estimate an optimal SMDP policy , even when the SMDP dynamics P

and R are unknown.

To train the individual policy o of an option o, it is common to define an option-specific reward function ro, which

in turn defines an option-specific Markov decision process Mo = S, A, P, ro . The policy o is then implicitly defined as the optimal solution to Mo. Even though the original

definition of SMDPs considers options with fixed policies, in

practice one can train the option policies and the SMDP pol-

icy in parallel.

3 Hierarchical Representation Learning
In this section we present our main contribution, a method for automatically learning a hierarchical representation of a given MDP.
3.1 Compression Function
The first step of our method is to learn a compression function from MDP states to abstract states. We first define a set Z of abstract states that will represent the partitions of the state space. Without loss of generality, the elements of Z are simply integers, i.e. Z = {0, . . . , |Z| - 1}. Our goal is to learn a parameterized compression function f : S  (Z) that maps MDP states to probability distributions over abstract states. Ideally, the compression function f should be deterministic, but the learning framework we consider favors probabilistic compression functions.
Intuitively, for abstract states to represent partitions of the state space, on a given trajectory the abstract state should remain the same most of the time, and only change occasionally. We formalize this intuition as a loss term, which will later be part of the objective that the learner attempts to minimize. Let  = s1, a1, r1, . . . , sT be a trajectory sampled

from the MDP using some exploration policy. The loss associated with trajectory  is defined as

T -1

LZ ( ) = -

f(z|st) log f(z|st+1).

t=1 zZ

Here, - zZ f(z|st) log f(z|st+1) is the cross-entropy loss for consecutive states st and st+1, which is more ap-

propriate for measuring the distance between f(·|st) and

f(·|st+1) than the square norm · 2 of their difference, since

f(·|st) and f(·|st+1) are probability distributions. The

cross-entropy loss is closely related to the Kullback-Leibler

(KL) divergence between f(·|st) and f(·|st+1); in prelim-

inary experiments, replacing the above term with the KL di-

vergence achieves comparable results.

On its own, the above loss term will not result in a mean-

ingful compression function, since the easiest way to mini-

mize it is to map all states to the same abstract state. To ensure

that all abstract states appear in the compression, we define

a second loss term equivalent to the negative entropy of the

compression function across a set of sampled states. Given a

set of trajectories T = {1, . . . , m}, we uniformly sample

a subset of states ST that appear on trajectories in T . Given

an abstract

state z



Z,

let F (z|T )

=

1 |ST |

sST f(z|s)

be the average probability of being in z across states in the

sample ST . We define the loss term as

LH (T ) = -H(F (·|T )) = F (z|T ) log F (z|T ),
zZ
where H(F (·|T )) is the entropy of the function F (·|T ). This loss term is minimized when the probabilities of abstract states are uniform, i.e. each abstract state is equally likely to appear in trajectories on average.
Finally, as already stated, we would like the compression function f to be as deterministic as possible. For this reason, we define a third loss term equivalent to the entropy of the compression function for individual states. We use the same set of sampled states ST , and define this loss term as

LD(T ) = H(f(·|s)) = -

f(z|s) log f(z|s).

sST

sST zZ

This loss term is minimized when the compression function f(·|s) of each individual state s is deterministic, i.e. assigns probability 1 to a single abstract state z  Z.
Given a set of trajectories T = {1, . . . , m}, the overall loss function L(T ) is a combination of the three individual
loss terms, i.e.

m

L(T ) = LZ (i) + wH LH (T ) + wDLD(T ), (1)
i=1

where wH and wD are weights that we can tune to determine the relative importance of each loss term.

3.2 Hierarchical Representation
Once we have learned a compression function f for a given MDP M = S, A, P, r , we use it to define a set of options O and an SMDP S. First, we introduce a deterministic compression function g : S  Z, defined in each

state s as g(s) = arg maxz f(z|x). Given an abstract state z  Z, let Sz be the subset of states that map to z, i.e. Sz = {s  S : g(s) = z}.
Our algorithm then uses the compression function in an online manner, by exploring the environment and finding abstract transitions, i.e. consecutive states st and st+1 such that g(st) = g(st+1). Let Y  Z × Z be the subset of pairs of distinct abstract states (z, z ) that appear as abstract transitions while exploring, i.e. there exist two consecutive states st and st+1 such that g(st) = z and g(st+1) = z . For each pair (z, z )  Y, we introduce an option oz,z = Io, o, o whose purpose is to perform an abstract transition from z to z . Option oz,z is applicable in abstract state z, i.e. Io = Sz, and terminates as soon as we reach an abstract state different from z, i.e. o(s) = 0 if s  Sz and o(s) = 1 otherwise.
To learn the policy o of option oz,z , we define an optionspecific Markov decision process Mo = Sz, A, P, ro . Note that Mo needs only be defined for states in Sz, since option oz,z always terminates outside this set. The local reward function ro is defined for each state-action pair as ro(s, a) = -0.05, i.e. a small negative reward. We also introduce a bonus +0.9 for terminating in a state s such that g(s) = z . As a consequence, the policy o has an incentive to leave abstract state z, and prefers to transition to abstract state z whenever possible.
Let O = {oz,z : (z, z )  Y} be the set of options described above for performing abstract transitions, and let Oz = {o  O : Io = Sz} be the subset of options applicable in abstract state z. We define an SMDP S = S, O, P , R , i.e. the high-level choices of the learning agent are to select abstract transitions to perform. Once the individual option policies have been trained, exploration is typically more efficient since the single decision of which option to execute results in a state that is many steps away from the initial state. In addition, one can approximate the SMDP policy as  : Z  (O), i.e. the choice of which option to execute only depends on the current abstract state. This has the potential to significantly speed up learning if |Z| |S|.
The system is trained using a Manager-Worker architecture. The Manager performs tabular Q-learning [Watkins and Dayan, 1992] over the SMDP S while the Worker uses policy gradient to learn the policies of options oz,z .
3.3 Controllability
According to the definition of the option reward function ro, option oz,z is equally rewarded for reaching any boundary state between abstract states z and z . However, all boundary states may not be equally valuable, i.e. from some boundary states the options in Oz may have a higher chance of terminating successfully. To encourage option oz,z to reach valuable boundary states and thus make the algorithm more robust to the choice of compression function g, we add a reward bonus when the option successfully terminates in a state s belonging to abstract state z .
One possibility is that the reward bonus depends on the value of state s of options in the set Oz . However, this introduces a strong coupling between options in the set O: the value function Vz,z of option oz,z will depend on the value functions of options in Oz , which in turn depend on

the value functions of options in neighboring abstract states of z , etc. We want to avoid such a strong coupling since learning the option value functions may become as hard as learning a value function for the original state space S.
Instead, we introduce a reward bonus which is a proxy for controllability, by counting the number of successful applications of subsequent options after oz,z terminates. Let M be the number of options that are selected after oz,z , and let N  M be the number of such options that terminate successfully. We define a controllability coefficient  as

N

(z) = .

(2)

M

We then define a modified reward function r¯o which equals ro except when oz,z terminates successfully, i.e. r¯o(s, a, s ) = ro(s, a, s ) + (z) if s  z . In experiments we use a fixed
horizon M = 9 after which we consider successful option
transitions as irrelevant.

3.4 Transfer
The hierarchical representation in the form of the SMDP S
defined above can be used to transfer knowledge between tasks. Concretely, we assume that the given MDP M can be
extended to form a task by adding states and actions. Imagine for example that M models a navigation problem in a
given environment. A task can be defined by adding objects
in the environment that the learning agent can manipulate,
while navigation is still part of the task.
Formally, given an MDP M = S, A, P, r , a task T is an MDP MT = S × ST, A  AT, P  PT, r  rT . The states in ST represent information about task-specific objects, and the actions in AT are used to manipulate these objects. The transition kernel PT : (S × ST) × AT  (ST) governs the effects of the actions in AT, which may depend on the states of the original MDP (e.g. the location of the agent). Finally, the reward function rT : (S × ST) × AT  R models the reward associated with actions in AT.
To solve a task, we can replace the MDP M with the learned SMDP S = S, O, P , R , forming a task SMDP ST = S × ST, O  AT, P  PT, R  rT . Here, the options in O are used to navigate in the original state space S, while the actions in AT are used to manipulate the task-specific objects. If the policies of the options in O have been previously trained, the task SMDP ST can significantly accelerate learning compared to the task MDP MT.

4 Experimental Results
The experiments are designed to answer the following questions:
· Is the learned compression function suitable for learning a hierarchy?
· Does the learned hierarchy transfer across different tasks in the same environment?
· How does our HRL algorithm compare against stateof-the-art flat algorithms such as Self Imitation Learning [Oh et al., 2018]?

Figure 3: The discovered invariant SMDP.

Figure 1: Environment Env0, representing a nine room grid world without any task; the agent is marked by the blue square.
Figure 2: The learned representation on Env0; different colors represent different abstract states z.
4.1 Empty Environment We designed an empty navigation environment without tasks and refer to it as Env0. The states are (x, y)-positions which are mapped to images (c.f. Figure 1) and the discrete action space is A = {up, down, lef t, right}. In this environment at each episode the agent is placed at a random initial position, making exploration easier. In subsequent environments (Env1, Env2, Env3), the initial position is fixed to (1,1).
4.2 Implementation Details The first step of our procedure consists in a pre-training phase where we collected a dataset T of 1000 trajectories from Env0 using policies that always select the same primitive action, i.e. the set of policies is  = {up, down, left, right}. This simple set of policies allows us to cover all the state space.
We used the collected dataset T to train the compression function f using the AdamW optimizer [Loshchilov and Hutter, 2017] by minimizing loss (1) over 5000 steps. The learned compression function is showed in Figure 2.
Following the pre-training phase we can use the learned compression function to solve any task in the same environment. We distinguish between a manager in charge of solving the task SMDP ST and workers in charge of solving the option MDPs Mo. Manager Our algorithm iteratively grows an estimate of the SMDP S. Initially, the agent only observes a single state s  S and associated abstract state z = g(s). Hence the state space Z contains a single abstract state z, whose associated option set

Oz is initially empty. In this case the only alternative available to the agent is to explore. For each abstract state z, we add an exploration option oezxploration = z, zexploration, z to the option set O. This option has the same initation set and termination condition as the options in Oz, but the policy zexploration is an exploration policy that selects actions uniformly at random, terminating when it leaves abstract state z or exhausts a given budget.
Once the agent discovers a neighboring abstract state z of z, it adds abstract state z to the set Z and the associated option oz,z to the option set O. The agent also maintains and updates a directed graph whose nodes are abstract states and whose edges represent the neighbor relation. Hence next time the agent visits abstract state z, one of its available actions is to select option oz,z . When option oz,z is selected, it chooses actions using its policy z,z and simultaneously updates z,z based on the rewards of the option MDP Mo. Figure 3 shows an example representation discovered by the algorithm.
Algorithm 1 shows the pseudo-code of the algorithm. As explained, Z is initialized with the abstract state z of the initial state s, and O is initialized with the exploration option oezxploration. In each iteration the algorithm selects an option o which is applicable in the current abstract state z. If we transition to a new abstract state z , it is added to Z and the exploration option oezxploration and transition option oz,z is appended to O. The process then repeats from state s .
The subroutine GETOPTION that selects an option o in the current abstract state z can be implemented in different ways; in our case, we use an -greeedy policy.
Since the set of abstract states Z is small, the manager performs tabular Q-learning over the task SMDP ST. The subroutine UPDATEPOLICY updates the Q values of the Manager using the sum of rewards R accumulated until the selected option terminates. In order to recognize the new goal states in the task, while exploring we define any terminal state in the environment as a new abstract state z; hence the manager will introduce options for reaching this terminal state.
Workers
The workers are in charge of learning the policies of each option oz,z in O, allowing the manager to transition between two abstract states z, z . We use A2C (a version of A3C without asynchronous updates) combined with Self-Imitation Learning (SIL) [Oh et al., 2018] which benefits from an exploration bonus coming from the self-imitation component of the loss function. The rewards that the worker observe are defined in Section 3.3, and implemented in the routine TRAINOPTION from Algorithm 1.

Figure 4: Results on the variations of nine room gridworld environments where the goal (green square) is placed at an increasing distance from the agent (blue square). From left to right: Env1, Env2, Env3.

Moreover, since the critic update is off-policy, one can relabel failed transitions in order to speed up learning of the correct option behavior, similar to to Hindsight Experience Replay [Andrychowicz et al., 2017]. The architecture is made of two separate neural networks, one for the policy z,z , parametrized on , and one for the value function Vz,z , parametrized on .
The parameters of the two neural networks are updated in two different phases. The on-policy phase is updated using the usual online actor critic loss,

La2c = Es,a Lapo2lcicy + a2cLava2lcue ,

Lapo2lcicy = - log  (at | st) (Vtn - V (st)) - Ht ,

Lava2lcue

=

1 2

V (st) - Vtn 2 ,

while the off-policy phase is updated by sampling batches of experience from an experience replay and with the following loss:

Lsil = Es,a,RD Lspiollicy + sil Lsviallue ,

Lspiollicy = - log (a | s) max ((R - V(s)) , 0) ,

Lsviallue

=

1 2

max ((R - V(s)) , 0) 2 .

4.3 Experiments
In our experiments, the goal is positioned at an increasing distance from the initial state to make the exploration harder, c.f. Env1, Env2 and Env3 in Figure 4. Results are averaged over 4 seeds and each experiment is run for 80,000 samples. Note that even if the compression function is given, the goal location is unknown to the learning agent, which thus has to explore the environment in order to find the goal location for the first time.

Algorithm 1 MANAGER

1: Input: environment e, previously discovered SMDP S in

case of transfer learning, compression function g

2: z  g(s)

3: if z  Z then

4: Z  Z  {z}

5: end if 6: O  oezxploration 7: T  initial policy
8: o  None

9: while within budget do

10: if o is None or Terminate then

11:

o  GETOPTION(T, z, O)

12:

R=0

13: end if

14: s , r, done  e(o(s))

15:

16: TRAINOPTION(o, s, r, s , done)

17: R = R + r

18: z = g(s)

19: if z  Z then

20: Z  Z  {z }

21:

O  O  {oezxploration, ozz,z }

22: end if

23: if z = z then

24:

UPDATEPOLICY(T, z, o, R, z )

25: end if

26: if s is terminal and s not in Z then

27:

O  O  {ozz,s }

28: Z  Z  {s }

29: end if

30: (z, s)  (z , s )

31: end while

The maximum number of steps in the environment has been set to 100, making exploration hard, expecially in Env3 where the goal is at the maximum distance respect to the initial state. Results in Figure 4 show the total reward with a running average smoothing of 100 episodes. The agent receives from the environment a reward of 1 when it reaches the goal position (green square) and a reward of 0 elsewhere.
We refer to "SIL" as the flat reinforcement learning agent and to "HRL" as our algorithm in which the SMDP S has to be learned online while exploring, but the compression function g is given. "HRL-transfer" refers instead to our algorithm where the agent is exposed to the three environments Env1, Env2, Env3 in sequence. In this case the algorithm benefits from the transfer of the SMDP S while the manager policy (i.e. Q-values) are reset to 0 after training in each environment. The hyperparameters used during learning are provided in Table 1.
We can observe that the HRL algorithm learns faster than SIL in all three environments. SIL relies only on random exploration, but once it finds a positive reward, it can exploit the reward by self imitating past good experiences. SIL presents high variance since for some seeds it has not even be able to solve the task given the budget of 80,000 samples. We can also observe that transfer does slightly improve over HRL, and we would argue that this improvement could be larger if we choose harder tasks were the option policies for transitioning between abstract states become harder to learn.
5 Conclusion
In this paper we present a novel method for learning a hierarchical representation from sampled trajectories in highdimensional domains. The idea is to learn abstract states that group together neighboring states in the original space, and introduce options for performing transitions between abstract states. Experiments show that the learned representation can successfully be used to solve multiple tasks in the same environment, significantly speeding up learning compared to a flat learner.
There are many possible directions for future work. One important improvement is to relax the assumption that we can sample trajectories from arbitrary initial states, instead relying on a fixed distribution of initial states. Another possible extension is to repeatedly interleave representation learning with policy improvement, which may successively improve the quality of the sampled trajectories and lead to a better representation. An alternative way to improve the hierarchical representation is to correct the compression functions in states from which some abstract transitions are not possible, e.g. the yellow state in the middle left room of Figure 2. Finally, in the future we also plan to run experiments in more challenging domains.

Hyperparameter

Value

Worker Hyperparameters

CONV1(16, (1, 1), (1, 1))

SIL Agent Architecture

CONV2(32, (5, 5), (1, 1)) CONV3(32, (3, 3), (1, 1))

FC1(64)

Learning rate

0.0007

Number of steps per iteration

5

Entropy regularization

0.01

Batch size

512

SIL loss weight

1

SIL value loss weight

0.01

Replay buffer size

104

Replay type

Prioritized Experience

Replay

Exponent for prioritization 0.6

Bias Correction

0.1

for prioritization

Manager Hyperparameters

E-Greedy

0.005

learning rate

0.6

gamma

0.95

Compression Function Hyperparameters

CONV1(16, (1, 1), (1, 1))

BatchNorm2D(16)

CONV2(32, (5, 5), (1, 1))

Compression Function Architecture

BatchNorm2D(32) CONV3(32, (3, 3), (1, 1)) BatchNorm2D(32)

FC1(64)

BatchNorm1D(64)

FC1(1)

Learning rate

0.001

batch traj size

1

batch states

32

epochs

6000 ( early stopping)

Self Imitation Learning Hyperparameters

Same as Worker

Table 1: Hyperparameters used to train Manager, Worker and Compression Function.

References
[Andrychowicz et al., 2017] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In Advances in Neural Information Processing Systems, pages 5048­5058, 2017.
[Bacon et al., 2017] Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In Proceedings of the 31st AAAI Conference on Artificial Intelligence, 2017.
[Barto and Mahadevan, 2003] Andrew G. Barto and Sridhar Mahadevan. Recent advances in hierarchical reinforcement learning. Discrete Event Dynamic Systems, 13(1­2):41­77, January 2003.
[Corneil et al., 2018] Dane S. Corneil, Wulfram Gerstner, and Johanni Brea. Efficient modelbased deep reinforcement learning with variational state tabulation. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsma¨ssan, Stockholm, Sweden, July 10-15, 2018, pages 1057­1066, 2018.
[S¸ ims¸ek and Barto, 2004] O¨ . S¸ ims¸ek and A. Barto. Using relative novelty to identify useful temporal abstractions in reinforcement learning. Proceedings of the International Conference on Machine Learning, 21:751­758, 2004.
[S¸ ims¸ek et al., 2005] O¨ . S¸ ims¸ek, A. Wolfe, and A. Barto. Identifying useful subgoals in reinforcement learning by local graph partitioning. Proceedings of the International Conference on Machine Learning, 22, 2005.
[Da Silva et al., 2012] Bruno Castro Da Silva, George Konidaris, and Andrew G. Barto. Learning parameterized skills. In Proceedings of the 29th International Coference on International Conference on Machine Learning, ICML'12, page 1443­1450, Madison, WI, USA, 2012. Omnipress.
[Ecoffet et al., 2019] Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, and Jeff Clune. Go-explore: a new approach for hard-exploration problems. arXiv preprint arXiv:1901.10995, 2019.
[Eysenbach et al., 2019] Ben Eysenbach, Russ R Salakhutdinov, and Sergey Levine. Search on the replay buffer: Bridging planning and reinforcement learning. In Advances in Neural Information Processing Systems, pages 15220­15231, 2019.
[Florensa et al., 2017] Carlos Florensa, David Held, Xinyang Geng, and Pieter Abbeel. Automatic goal generation for reinforcement learning agents. arXiv preprint arXiv:1705.06366, 2017.
[Fox et al., 2017] Roy Fox, Sanjay Krishnan, Ion Stoica, and Ken Goldberg. Multi-level discovery of deep options. arXiv preprint arXiv:1703.08294, 2017.
[Konidaris and Barto, 2007] George Konidaris and Andrew Barto. Building portable options: Skill transfer in reinforcement learning. In Proceedings of the 20th International Joint Conference on Artifical Intelligence, IJ-

CAI'07, page 895­900, San Francisco, CA, USA, 2007. Morgan Kaufmann Publishers Inc.
[Lakshminarayanan et al., 2016] Aravind S Lakshminarayanan, Ramnandan Krishnamurthy, Peeyush Kumar, and Balaraman Ravindran. Option discovery in hierarchical reinforcement learning using spatio-temporal clustering. arXiv preprint arXiv:1605.05359, 2016.
[Loshchilov and Hutter, 2017] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.
[Machado et al., 2017] Marlos C. Machado, Marc G. Bellemare, and Michael Bowling. A laplacian framework for option discovery in reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML'17, page 2295­2304. JMLR.org, 2017.
[Mannor et al., 2004] S. Mannor, I. Menache, A. Hoze, and U. Klein. Dynamic abstraction in reinforcement learning via clustering. Proceedings of the International Conference on Machine Learning, 21:560­567, 2004.
[McGovern and Barto, 2001] A. McGovern and A. Barto. Automatic Discovery of Subgoals in Reinforcement Learning using Diverse Density. Proceedings of the International Conference on Machine Learning, 18:361­368, 2001.
[Menache et al., 2002] I. Menache, S. Mannor, and N. Shimkin. Q-Cut ­ Dynamic Discovery of SubGoals in Reinforcement Learning. Proceedings of the European Conference on Machine Learning, 13:295­306, 2002.
[Mnih et al., 2016] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pages 1928­ 1937, 2016.
[Nachum et al., 2018] Ofir Nachum, Shixiang Gu, Honglak Lee, and Sergey Levine. Near-optimal representation learning for hierarchical reinforcement learning. arXiv preprint arXiv:1810.01257, 2018.
[Oh et al., 2015] Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard Lewis, and Satinder Singh. Action-conditional video prediction using deep networks in atari games. arXiv preprint arXiv:1507.08750, 2015.
[Oh et al., 2018] Junhyuk Oh, Yijie Guo, Satinder Singh, and Honglak Lee. Self-imitation learning. In International Conference on Machine Learning, pages 3878­ 3887. PMLR, 2018.
[Pickett and Barto, 2002] M. Pickett and A. Barto. Policyblocks: An Algorithm for Creating Useful Macro-Actions in Reinforcement Learning. Proceedings of the International Conference on Machine Learning, 19:506­513, 2002.

[Puterman, 2014] Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons, 2014.
[Schaul et al., 2015] Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approximators. In International Conference on Machine Learning, pages 1312­1320, 2015.
[Shang et al., 2019] W Shang, A Trott, S Sheng, C Xiong, and R Socher. Learning World Graphs to Accelerate Hierarchical Reinforcement Learning. arXiv preprint arXiv:1907.00664, 2019.
[Solway et al., 2014] Alec Solway, Carlos Diuk, Natalia Cordova, Debbie Yee, Andrew G. Barto, Yael Niv, and Matthew M. Botvinick. Optimal behavior hierarchy. PLOS Comp. Bio., 10(8), 2014.
[Sutton et al., 1999] Richard S Sutton, Doina Precup, and Satinder Singh. Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2):181­211, 1999.
[Sutton et al., 2017] Richard S Sutton, Joseph Modayil, Michael Delp Thomas Degris, Patrick M Pilarski, and Adam White. Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction. 2017.
[Veeriah et al., 2021] Vivek Veeriah, Tom Zahavy, Matteo Hessel, Zhongwen Xu, Junhyuk Oh, Iurii Kemaev, Hado van Hasselt, David Silver, and Satinder Singh. Discovery of Options via Meta-Learned Subgoals. CoRR, abs/2102.06741, 2021.
[Watkins and Dayan, 1992] Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279­ 292, 1992.
[Wen et al., 2020] Zheng Wen, Doina Precup, Morteza Ibrahimi, Andre Barreto, Benjamin Van Roy, and Satinder Singh. On efficiency in hierarchical reinforcement learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 6708­6718. Curran Associates, Inc., 2020.

