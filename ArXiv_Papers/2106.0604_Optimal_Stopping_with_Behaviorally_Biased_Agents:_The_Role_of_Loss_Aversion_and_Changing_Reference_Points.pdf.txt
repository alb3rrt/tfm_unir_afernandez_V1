arXiv:2106.00604v1 [cs.GT] 1 Jun 2021

Optimal Stopping with Behaviorally Biased Agents: The Role of Loss Aversion and Changing Reference Points

Jon Kleinberg 

Robert Kleinberg  May, 2020

Sigal Oren 

Abstract
People are often reluctant to sell a house, or shares of stock, below the price at which they originally bought it. While this is generally not consistent with rational utility maximization, it does reflect two strong empirical regularities that are central to the behavioral science of human decision-making: a tendency to evaluate outcomes relative to a reference point determined by context (in this case the original purchase price), and the phenomenon of loss aversion in which people are particularly prone to avoid outcomes below the reference point. Here we explore the implications of reference points and loss aversion in optimal stopping problems, where people evaluate a sequence of options in one pass, either accepting the option and stopping the search or giving up on the option forever. The best option seen so far sets a reference point that shifts as the search progresses, and a biased decision-maker's utility incurs an additional penalty when they accept a later option that is below this reference point.
We formulate and study a behaviorally well-motivated version of the optimal stopping problem that incorporates these notions of reference dependence and loss aversion. We obtain tight bounds on the performance of a biased agent in this model relative to the best option obtainable in retrospect (a type of prophet inequality for biased agents), as well as tight bounds on the ratio between the performance of a biased agent and the performance of a rational one. We further establish basic monotonicity results, and show an exponential gap between the performance of a biased agent in a stopping problem with respect to a worst-case versus a random order. As part of this, we establish fundamental differences between optimal stopping problems for rational versus biased agents, and these differences inform our analysis.

Cornell University. Email: kleinber@cs.cornell.edu. Research supported in part by a Vannevar Bush Faculty
Fellowship, MURI grant W911NF-19-0217, AFOSR grant FA9550-19-1-0183, and BSF grant 2018206. Cornell University. Email: rdk@cs.cornell.edu. Research supported by NSF Grant CCF-1512964. Ben-Gurion University of the Negev. Email: sigal3@gmail.com. Work supported by BSF grant 2018206 and ISF
grant 2167/19.

1 Introduction
One of the central human biases studied in behavioral economics is reference dependence -- people's tendency to evaluate an outcome not in absolute terms but instead relative to a reference point that reflects some notion of the status quo [6]. Reference dependence interacts closely with a related behavioral bias, loss aversion, in which people weigh losses more strongly than gains of comparable absolute values. Taken together, these two effects produce a fundamental behavioral regularity in human choices: once a reference point has been established, people tend to avoid outcomes in which they experience a loss relative to the reference point.
This effect can be seen in simple examples. One well-known instance of the effect is the empirical evidence that individual investors will tend to avoid selling a stock unless it has exceeded the price at which they purchased it; the purchase price thus acquires a special status in the investor's decisionmaking [9]. In this example, the purchase price x serves as the reference point, and the investor's loss aversion relative to this reference point leads them to hold on to a stock while its value is below x. There are two useful points to note about this example, and reference dependence more generally. First, it is a deviation from rational behavior: once the investor has purchased the stock, the value of the purchase price is irrelevant to any future decisions, which should be based purely on predictions of the stock's future performance. Second, the reference point will often be set by some notion of status quo or default that is determined by the semantics of the environment. Once we realize, for example, that the purchase price serves naturally as a reference point for stocks, we might conjecture that a similar effect should hold for other items, like houses -- that home-owners should exhibit different behaviors in situations where they are contemplating selling their house at a value either above or below the price at which they originally purchased it. This effect too is borne out in empirical data [5].
In more complex examples, and most relevant to our work here, the reference may shift while an agent is making a decision. Consider for example an agent who is trying to make a large purchase or hire a job candidate, and does this by evaluating candidate options in one pass in a take-it-or-leave-it fashion -- with each candidate they must either accept it and end the search, or give up on it as an option forever. Experimental studies by Schunk and Winter [11] show that people in this type of task behave consistently with the notion that they are maintaining a time-varying reference point equal to the best option they have seen so far. This means that if they settle for a candidate A that is worse than a better candidate B that they have seen in the past, their utility from selecting A will be reduced by some notion of loss relative to the high reference point set by B. In these studies, people's decisions are best explained by a model in which they take into account this anticipated sense of loss prospectively in making their choices; they operate so as to reduce the chance that they will have to choose a future option that is dominated by one that they have earlier passed up. We can recognize this empirical regularity at an intuitive level as well: if we purchase a house that is worse than one that we passed up on earlier in our search, or take a job that is worse than one we were offered earlier and turned down, we tend to feel that we have experienced a loss relative to what we could have achieved. Our utility for the final outcome thus has a history-dependence, in that it is affected by the options that we rejected prior to our ultimate choice.
Implications for Optimal Stopping Problems. These findings have interesting implications for how we might model the ways in which human decision-makers with standard biases approach optimal stopping problems. Consider one of the canonical formulations of an optimal stopping problem, which closely follows the structure of the Schunk-Winter experiments described above: we are presented with a sequence of n candidates in order; we know that candidate t will have a value vt drawn from a distribution Ft; but we only see the materialized value of vt when we get to candidate
1

t in order. At that point, we must either accept t and end the search, or give up on t forever. The celebrated prophet inequality of Krengel and Sucheston [7] asserts that there is an on-line policy for choosing one of the candidates in such a way that the expected value of the selected candidate is within a factor of two of the highest value of any of the candidates when viewed in retrospect (i.e. the performance achieved by a prophet that can see all the values in advance). An active line of algorithmic research has developed to understand the bounds that can be achieved in models of this form; see the survey articles [3, 8] and the references therein.
Given the behavioral literature on reference dependence, it becomes interesting to ask how the model should change to incorporate this bias. In particular, consider this type of stopping problem being solved by an agent with human biases based on reference dependence. Let   0 be a parameter representing the agent's level of loss aversion;  is a multiplier on any losses the agent experiences relative to a reference point.1 Now, when the agent contemplates a candidate of value u in their one-pass search and the value of the best candidate they have seen so far is v, their utility from selecting u  v will be equal to u, however for u < v their utility will be
u - (v - u).
We will refer to an agent with this behavior as a reference-dependent agent with loss-aversion . Thus,  = 0 corresponds to the traditional optimal stopping problem with a rational agent, while larger values of  correspond to greater levels of loss aversion. The experimental results suggest that we study the setting in which agents are sophisticated about their reference dependence, in that they make choices with the awareness that their utilities will be shifted by the (v - u) term. Thus, a reference-dependent agent will choose a stopping rule that maximizes its expected "shifted" utility.
The present work: Optimal stopping with reference dependence. We thus have a basic new model of sequential decision-making that is behaviorally well-motivated, incorporating one of the central biases in human decision-making. Our goal in the present work is to explore this model; we provide both quantitative bounds for the performance of reference-dependent agents relative to optimality, including a tight prophet inequality for biased agents, as well as more qualitative analysis of the model's behavior, including questions about its monotonicity in the loss-aversion parameter  and the length of the sequence of candidates.
A brief overview of our results is as follows. First, we show that there is a strategy by which a reference-dependent agent with loss-aversion  can guarantee an expected value within a factor of  + 2 of the best candidate in retrospect. This can be viewed as a type of prophet inequality for biased agents, distinct from other results of this form, which assume the traditional rational model of decision-making. We show that the bound can be obtained by a threshold rule, analogous to Samuel-Cahn's approach to the classical prophet inequality [10] but with the threshold shifted by a parameter based on the bias. The bound is tight for all , and it recovers the factor of 2 from the classical prophet inequality as  goes to 0. A different comparison is between the performance of a reference-dependent agent with loss-aversion  and a rational agent that also must operate online; here we show that there is a strategy by which the reference-dependent agent can guarantee an expected value within a factor of  + 1 of the rational agent, and this too is tight. Essentially, the reason for these large gaps is that reference-dependent agents with loss-aversion tend to select candidates too early in the sequence since they are more worried about ending up with a candidate with lower value.
1We choose the natural, albeit simple, way of modeling loss aversion as linear in the loss to keep the focus on the role that the changing reference point plays. As part of future work it will be interesting to study the effects of different modelings of the loss function on the behavior of the agent.
2

A growing line of work has studied the effect of random arrival order on optimal stopping problems

-- when the candidates are seen according to an order selected from the uniform distribution on

permutations. For rational agents (without a notion of reference dependence) this has been shown

to improve the bound of 2 to a smaller constant strictly between 1 and 2; after a sequence of

improvements

the

best

known

bound

is

slightly

less

than

3 2

and

is

due

to

Correa

et

al.

[4].

For

reference-dependent agents, we show that random ordering has a large effect on the achievable bounds

as a function of ; under a random ordering, a reference-dependent agent can achieve an expected

value that the optimum exceeds by a factor of only (log ), an exponential improvement over the

 + 2 bound for worst-case orderings. We observe an additional improvement by order of magnitude

when the distributions have only two values in their support and the ordering of the candidates is

the one that maximizes the expected value of the candidate that the agent selects. For this case, we

improve the bound to 2. This result is analogous to the result of [1] showing that for rational agents

and 2-point distributions the bound is 5/4.

As noted above, we prove a number of monotonicity results showing that the performance of biased

agents degrades in certain consistent directions. First, suppose two reference-dependent agents have

loss-aversion parameters  and , with  < . Then we can show that the expected value of the

candidate selected by the agent with the lower parameter  is higher than the expected value obtained

by the agent with the higher parameter . To prove this we show that as the value of  increases the

agent tends to select candidates that are earlier in the sequence of candidates. Second, we establish

monotonicity in the sequence of candidates: adding a candidate to the end of a sequence cannot hurt

a biased agent's performance, but adding a candidate to the beginning of the sequence can reduce

the expected value of the candidate selected by the agent (by a tight factor of  + 1). A result like

this has no natural analogue for rational agents, where adding options cannot hurt performance.

The analysis underlying these results highlights a number of ways in which optimal stopping ques-

tions for biased agents become qualitatively different from their traditional analogues for unbiased

agents. Many of these flow from the initial observation that a biased agent's optimal decisions in these

problems are history-dependent, since they depend on the maximum among the realized values earlier

in the sequence; in contrast, a key feature of optimal stopping problems with rational agents is their

memoryless nature. The three-way comparison between the optimal value (achieved by a prophet),

the value achieved by a rational agent, and the value achieved by a reference-dependent agent also

highlights important aspects of the model, including the fact that the largest of these worst-case

gaps -- the factor of  + 2 between the optimal value and the biased agent -- is strictly less than

the product of the other two gaps, suggesting that the worst cases are obtained at fundamentally

different points. The behavior of biased agents under random orderings also highlights new aspects

of the model -- both in the exponential improvement it yields in , and also in the observation that

the gap between the biased agent's value and the optimal value can always be upper-bounded by the

sequence length. In contrast, no upper bound depending only on the sequence length is possible for

biased agents under a worst-case order.

In the next section, we present the model and accompanying definitions and notations in full

detail, and then provide proofs for the results stated here.

2 Model and Preliminaries
Consider the following selection problem. There are n time steps. At each time step t a new candidate arrives and its value vt is sampled from some known distribution Ft with non-negative support. We use Vt to denote a random variable equals to the value of candidate t and vt to denote its realized value. When candidate t arrives its value vt is realized and the agent should decide whether to select the candidate or not. Selection decisions are final. That is, if an agent decided not to select a

3

candidate it cannot change its mind later on. We assume that the agent has to select some candidate, thus, if the agent reaches the last candidate it would always select it. This is a natural assumption for a hiring scenario in which a manager has to recruit an employee to fill a vacancy. It is also applicable in many other settings, for example, a buyer that has to buy a car.
We consider an agent that has loss aversion with respect to the value of the best candidate it has considered so far. In particular, if the best candidate that was considered has value v and the candidate that was selected has a value u < v then the total utility of the loss averse agent would be u - (v - u). Here   0 is a parameter capturing the extent of loss aversion.  = 0 implies that the agent is completely rational.2 We refer to an agent that is loss averse with parameter  as a -biased agent and to the value of the best candidate so far as the reference value.
Observe that the loss averse agent's planning problem can be modeled as a finite-horizon Markov decision process (MDP) in which the state encodes the reference value and the number of candidates considered. As such, the MDP value function satisfies the Bellman Equation and can be computed by dynamic programming. From the dynamic program we can easily infer the optimal stopping rule for a -biased agent. Throughout the paper we refer to this optimal stopping rule as the optimal -biased stopping rule.
To help the reader get acquainted with the model, we briefly work out the details of the dynamic program: let m be the number of possible values for candidates (i.e., all values that are in the support of any of the distributions Ft). The table size is m О n and U[v, t] is the expected utility of playing optimally at times t, . . . , n when the reference value is v (i.e., v = max{v1, . . . , vt-1}). With this notation the agent selects candidate t if and only if:

vt - (v - vt)+  U[v  vt, t + 1].

where x  y = max{x, y} and (x)+ is x for x  0 and 0 otherwise. This gives us the following formula for the dynamic program:

U[v, t] := EvtFt[U[v  vt, t + 1]  (vt - (v - vt)+)].

vt is not selected

vt is selected

Recall that if the agent reaches the last candidate it has to select it. Thus, we have that U[v, n] := EvnFn[vn - (v - vn)+] for any value v that is in the support of some distribution. We can solve the dynamic program by continuing to fill the cells backwards. Observe that U[0, 1] holds the expected utility of the optimal -biased stopping rule as in the beginning the agent's reference value is 0.
For agents that do not exhibit a bias it is well known that the optimal stopping rule can be
described by a sequence of monotonically decreasing thresholds 1, . . . , n such that a candidate t is selected if and only if vt  t. (See, for example, Theorem 3.2 of the textbook [2].) In Section 4 we show that the optimal -biased stopping rule can also be described as a sequence of thresholds,
albeit the thresholds for biased agents are history dependent. To prove this claim we observe a monotonicity property: for reference values v > v and any t  1 we have that U[v, t]  U[v, t]. We prove this observation and observe other types of monotonicity in Section 4.

3 The Cost of Loss-Aversion
There are two interesting comparisons one can make here. First, we compare between the expected value of a candidate that a biased agent selects and the value of the best candidate in hindsight (what a prophet would have chosen). The second comparison is between the expected value of the candidate
2We only consider non-negative values of  as a negative value of  implies an agent that prefers to select a candidate worse than the best it has considered, which makes less sense.
4

that was selected by a -biased agent in comparison with the value of the candidate selected by an
unbiased agent. We focus on comparing the value of the selected candidate and not the utility of the
agent, as the value of the selected candidate is an objective measure and hence is more appropriate
for quantifying the actual loss (as oppose to the perceived loss) of a -biased agent due to its bias. We define random variables V  and V (for   0) to equal the value of the best candidate in
hindsight and the candidate selected by a -biased agent using the optimal stopping rule respectively. We show that the ratio between E[V ] and E[V] can be as high as  + 2 and the ratio between E[V0] (i.e., the expected value of a candidate selected by an unbiased agent) and E[V] can be as high as  + 1. This is done by adapting a classic example for the prophet inequality.

Example

3.1

For

 > 0,

let

V1 =

1 1+(1-)

and

V2

=

1 
0

w.p  . w.p 1 - 

Claim

3.2

For

every

>

0,

there

exists

an

instance

such

that

the

ratio

E[V ] E[V]

is

arbitrarily

close

to

 + 2.

Proof: Consider the family of instances defined in Example 3.1. A -biased agent always selects the first candidate. To see why this is the case observe that:

1 1 + (1 - )



1 - (1 - )

и

1 1 + (1 - )

=

1

+

(1 - ) - (1 - 1 + (1 - )

)

=

1

+

1 (1 -

)

The expected

value of the best candidate in hindsight is:

1- 1+(1-)

+

1

=

+2-(+1) 1+(1-)

.

Thus, we

have

that

E[V ] E[V]

=

 + 2 - ( + 1).

Claim

3.3

For

every



>

0,

there

exists

an

instance

such

that

the

ratio

E[V0 ] E[V]

is

arbitrarily

close

to

 + 1.

Proof: Consider the family of instances defined in Example 3.1. We already observed that a -biased agent always selects the first candidate. We now consider an unbiased agent. Notice that the expected value of the second candidate is higher than the expected value of the first candidate. Thus the unbiased agent always selects the second candidate, implying that:

E[V0] E[V]

=

1

+

(1 - 1

)

=



+

1

-



и



It is interesting to observe that the family of instances defined in Example 3.1 can also be used

to

show

that

the

ratio

E[V ] E[V]

for

any

0



<

can

be

as

high

as

 + 1.

The

reason

for

this

is

that

since the -biased agent is indifferent between selecting the first candidate and the second candidate,

any -biased agent would strictly prefer selecting the second candidate.

Our

main

results

in

this

section

demonstrate

that

the

lower

bounds

we

just

established

for

E[V ] E[V]

and

E[V0] E[V]

are

tight.

The

first

proof

builds

on

[10]

showing

a

2-approximation

for

prophet

inequalities

can always be achieved by a simple threshold strategy.

Theorem 3.4

E[V ] E[V]





+

2.

Proof: We consider a simple threshold strategy and show that even though this is not necessarily

the

optimal

stopping

rule,

the

expected

value

for

using

this

strategy

is

at

least

E[V ] +2

.

We define our threshold strategy as follows.

5

Definition 3.5 (-threshold strategy) The -threshold strategy is the stopping rule that selects

the

first

candidate

t

such

that

vt

>

,

where



is

defined

such

that

P r(V



>

)

=

+1 +2

. 3 If there

is no such candidate the strategy selects the last candidate.

Notice that the -threshold strategy ensures us that we select a candidate of value at least  with
probability . We denote this strategy by  and the optimal -biased stopping rule by . We show a stronger claim: the ratio between E[V ] and the utility of a -biased agent using this
threshold strategy is at most  + 2. This implies a bound on E[V ]/E[V] since

E[V ] 

E[V ]



E[V ]

.

E[V] E[V ()] -  и E[L()] E[V ()] -  и E[L()]

where V () and L() are random variables equal, respectively, to the value of the selected candidate

and the loss (i.e., difference between the value of the reference value and the chosen candidate, in

case this difference is positive) of the stopping rule . The second inequality is due to the fact that

the optimal -biased stopping rule aims to maximize the expected utility of the -biased agent.

We

now show that

E[V ] E[V ()]-иE[L()]

  + 2.

We

begin

by

bounding the

expected

value

of

the

candidate selected by the threshold strategy. Note that







E[V ()] = P r(V () > y)dy = P r(V () > y)dy + P r(V () > y)dy

0

0



We bound each of these integrals separately. First we observe that





P r(V () > y)dy   dy =  и 

0

0

This is simply because we accept a candidate that its value is higher than  with probability . Thus, for any y <  the probability that we accept a candidate with value higher than y is at least . For the second integral we bound the value of P r(V () > y) as follows:

n
P r(V () > y) = P r(Vt > y, V-t < )
t=1

where V-t <  is the event in which the values of all candidates prior to t were below . As these are two independent events we have that:

n

n

P r(Vt > y, V-t < ) = P r(Vt > y) и P r(V-t < )

t=1

t=1

Observe that P r(V-t < )  (1 - ) since with probability 1 -  we have that in each of the time

steps the value of the candidate was less than  and this is just a smaller range. Moreover by taking

a union bound we get that

n t=1

P

r(Vt

>

y)



P r(V 

>

y),

since

the

value

of

the

best

candidate

exceeds y if and only if there exists t with vt > y. Thus we conclude that:


E[V ()]   и  + (1 - ) P r(V  > y)dy


Observe that:





P r(V  > y) dy = P r(V  -  > z) dz = E[(V  - )+]



0

3Notice that if the distribution of V  contains point masses then such a  does not necessarily exist. In Appendix A we show how to extend -threshold strategies to accommodate such cases as well.

6

Thus, we have that: E[V ()]   и  + (1 - )E[(V  - )+]. Next, we observe that the expected loss of the threshold strategy is at most (1 - ) since the threshold strategy experiences zero loss except when it selects the last candidate, an event that happens with probability 1 -  and results in loss bounded above by  in the worst case that the last candidate has a value of at least 0 while the maximum preceding value is  - . Putting this together we get that:

E[V ()] - E[L()]   и  + (1 - )E[(V  - )+] - (1 - ) и  = ( - (1 - )) и  + (1 - )E[(V  - )+]

By

plugging

in



=

+1 +2

we

get

that:

E[V

()]

-

E[(L)]





1 +

2

и



+



1 +

2

и

E[(V



-

)+]





1 +

2

и

E[V

]

Theorem 3.6

E[V0] E[V]





+

1

Proof:

We

denote

by



the

optimal

-biased

stopping

rule

and

let



=

E[V ] E[V0]

and



=

E[V ] E[V]

.

We

would

like

to

bound

 

=

E[V0 ] E[V]

.

To

do

so,

we

will

get

an

upper

bound

on



as

a

function

of

.

We

obtain this upper bound by lower bounding the utility of the -biased agent by using its utility from

applying the optimal (unbiased) stopping rule (i.e., E[V ()] - E[L()]  E[V (0)] - E[L(0)]).

First observe the following bound on the expected loss of using the optimal stopping rule:

E[L(0)]  E[V  - V0] = E[V ] - E[V0]

Notice

that

using our

notation

we

have

that

E[V0] =

E[V  

]

.

Thus,

we

get

that

E[L(0)]  E[V ] -

E[V ] 

= E[V ](1 -

1 

).

Hence,

E[V

()]

-

E[L()]



E[V

(0)]

-

E[L(0)]



1 

E[V

]

-



1

-

1 

E[V ]

On

the

other

hand:

E[V ()] - E[L()]



E[V ()]

=

E[V 



]

.

This

implies

that:

1 



1 

-

1

-

1 

=



 1 - ( - 1)

By applying the bound on  we proved in Theorem 3.4 we have that    + 2. Thus,

E[V0] E[V]

=

 



min

1

-

1 (

-

1)

,



+ 

2

The minimum is maximized when:

1

-

1 ( - 1)

=

+2 

=



=

 

+ +

2 1

and

for

this

value

of



we

get

that

E[V0 ] E[V]



 + 1.

7

4 Monotonicity
In this section we prove several monotonicity properties as the parameters of the model vary. This includes showing that the agent's expected utility monotonically decreases as the reference value increases. This enables us to show that the optimal stopping rule can be described as a threshold strategy, in which the threshold for each step is history dependent. Somewhat surprisingly the expected value of the selected candidate may actually increase when the reference value increases.
We also show that both the agent's expected utility and the expected value of the selected candidate are decreasing as  increases. Finally, we consider the effect of adding more candidates on the agent's expected utility and the expected value of the selected candidate. For rational agents it is clear that additional candidates can only increase the expected value of the selected candidate. However, for biased agents we observe a significant different effect that is based on the new candidates' location in the sequence. If the candidates are added in the beginning of the sequence then the agent's expected utility and the expected value of the selected candidate can decrease by as much as a factor of  + 1. On the other hand, adding candidates to the end of the sequence can only increase the agent's expected utility and the expected value of the candidate that will be selected.

4.1 Monotonicity in the Reference Value and Threshold Strategies
In Section 2 we mention that the optimal -biased stopping rule can also be described as a sequence of thresholds such that each threshold is a function of the reference value. Before presenting the proof we prove an auxiliary observation that the expected utility of the agent monotonically decreases as the reference value increases.

Observation 4.1 For v < v and any t  1 we have that U[v, t]  U[v, t].

Proof: To see why this is the case observe that an agent with reference value v < v can use the optimal -biased stopping rule for reference value v. The expected utility of the agent for using this stopping rule is at least U[v, t] since the expected value of the candidate selected will be the same and the expected loss will only be smaller.

Interestingly, it is not necessarily the case that as the reference value increases the expected value

of the selected candidate decreases. To see that, consider the following example: V1 = 1 and V2 is 3

with

probability

1 2

and

2 will choose candidate

0 2

wsiinthcep1ro-bab(i2li-ty121).<A23n

agent that

-

1 2

и

(2

-

has at the 0) for any

 > 1 an agent with reference value 0 will choose candidate 1 since 1

beginning a reference value of

  0. On the other hand, for

>

3 2

-

1 2

(2

-

1).

As

a result

the expected value of the selected candidate will decrease from 3/2 to 1.

We are now ready to define the optimal -biased stopping rule as a threshold strategy.

Proposition 4.2 For every t  1 and v  0 the optimal -biased stopping rule selects candidate t when the reference value is v if and only if vt  (v, t) such that:

(v, t) =

U[v,t+1]+v 1+

,

for v > U[v, t + 1]

u,

for v  U[v, t + 1]

where u  v is the minimal value for which u  U[u, t + 1].
Proof: We distinguish between two cases. First we consider the case that v > U[v, t + 1]. Roughly speaking, this is the case in which the realized values of the candidates were lower than expected, and now to maximize its expected utility the agent may need to incur some loss with respect to the reference value. As can be expected, in this case the agent always selects candidate t with value

8

vt  v. This is because by applying Observation 4.1 we have that vt  v > U[v, t + 1]  U[vt, t + 1] which implies that vt  U[vt, t + 1] and hence should be selected. The agent may also select a candidate such that vt < v. This would be the case when:

vt - (v - vt)  U[v, t + 1]

=

vt 

U[v, t + 1] + v 1+

Observe

that

U[v,t+1]+v 1+

<

v

since

U[v, t + 1]

< v,

hence

we

conclude

that

in

this

case

candidate

t

will

be

selected

if

and

only

if

vt

>

U

[v,t+1]+v 1+

.

We next consider the case that v  U[v, t + 1]. This is the case for the first candidate, for

example, when the reference value is 0. It is also clear that in this case the agent would never select

a candidate with value strictly less than v. Hence, candidate t will be selected if and only if

vt  U[vt, t + 1].

Particularly, a candidate of value vt  u  v where u is the minimal value such that u  U[u, t + 1] will always be selected. Notice that, such a value necessarily exists since the left hand-side of the
inequality u  U[u, t+1] is increasing with u and by Observation 4.1 the right hand-side is decreasing with u. Also, from Observation 4.1 we get that the agent will select any candidate such that vt  u since vt  u  U[u, t + 1]  U[vt, t + 1].

4.2 Monotonicity in 
We show that the expected utility of the agent is decreasing as  increases.
Observation 4.3 For any reference value v  0, t  1 and  >   0 we have that U[v, t]  U [v, t].
Proof: Recall that U[v, t] is the expected utility of a -biased agent with reference value v that is now examining candidate t. Instead of using its optimal stopping rule the -biased agent can apply the optimal stopping rule for a -biased agent. By doing so it will achieve utility of at least U[v, t] since both agents will select the same candidate and the loss of the -biased agent will be smaller since  < .
The decrease in the agent's expected utility can be merely an artifact of the greater loss that the agent exhibits as we increase the bias parameter. However, we show that an additional reason for the decrease in the utility is that a candidate with lower expected value may be selected. To prove this we show that the optimal -biased stopping rule for  >  would select either the same candidate selected by an optimal -biased stopping rule or a candidate prior to it in the sequence. We formally define the following notion:
Definition 4.4 A stopping rule  is more patient than a stopping rule  if for every realization of the candidates the stopping rule  either selects the same candidate as  or selects a candidate that is later in the sequence. In other words, if the stopping rule  selected candidate t then  selects a candidate t  t.
Notice that if both  and  are threshold stopping rules and are defined on the same sequence of candidates, then if  is more patient than  then for any t and any realization v1, . . . , vt-1 the threshold of  for candidate t will be greater than or equal to the threshold of .
The usefulness of this notion comes from the next proposition showing that, under some conditions, a stopping rule which is more patient would select candidates of higher expected values.

9

Proposition 4.5 If an optimal -biased stopping rule  is more patient than another stopping rule  then the expected value of the candidate selected by  is higher than the expected value of the candidate selected by  (i.e., E[V ()]  E[V ()]).

Proof: Since  is more patient, then for every realization v1, . . . , vt such that  selects candidate t,  selects candidate t or a candidate that will arrive later. Let V (|v1, . . . , vt) and L(|v1, . . . , vt) be two random variables that are equal respectively to the value and loss of the optimal -biased
stopping rule for the realization v1, . . . , vt. Since  is optimal for a -biased agent, we have that:

E[V (|v1, . . . , vt)] - E[L(|v1, . . . , vt)]  vt - (max{v1, . . . , vt-1} - vt)+

(1)

We claim that for every realization v1, . . . , vt it has to be the case that E[V (|v1, . . . , vt)]  vt. This will in turn imply the proposition. Assume towards contradiction that there exists some realization for which E[V (|v1, . . . , vt)] < vt. By Equation (1), this implies that E[L(|v1, . . . , vt)] < (max{v1, . . . , vt-1} - vt)+. The expected loss is determined by the difference between the reference value and the value of the selected candidate. Since  is more patient than  the reference value at the time of the selection may only increase. Thus, if the expected value of the selected candidate decreases the expected loss would increase.
Next, we show that as the value of  decreases the optimal -biased stopping rule becomes more patient.

Claim 4.6 The optimal -biased stopping rule is more patient than the optimal -biased stopping rule for   .

Proof: Assume towards contradiction that there exists some realization v1, . . . , vt such that  selects candidate t and  selects candidate t such that t > t. We show that in this case a -biased agent can increase its expected utility by instead selecting candidate t on any such realization. Let v =
max{v1, . . . , vt-1} denote the reference value. As in the proof of Proposition 4.5, let V (|v1, . . . , vt) and L(|v1, . . . , vt) be two random variables that are equal respectively to the value and loss of the stopping rule  for the realization v1, . . . , vt. Since  is optimal for a -biased agent we have that the utility for following it is greater than using  instead:

vt - (v - vt)+  E[V ( |v1, . . . , vt)] - E[L( |v1, . . . , vt)]

(2)

We show that (v - vt)+  E[L(|v1, . . . , vt)]. By Inequality (2) we have that this has to be the case if vt  E[V (|v1, . . . , vt)]. Hence, we are left with handling the case that vt > E[V ( |v1, . . . , vt)]. Now, observe that E[L(|v1, . . . , vt)]  v - E[V ( |v1, . . . , vt)]. The reason for this is twofold: first the reference value may increase above v and second in the expres-
sion v - E[V (|v1, . . . , vt)], all the events in which the selected candidate has a higher value than v will contribute a negative number. Putting this together with the assumption that vt > E[V ( |v1, . . . , vt)] we get that E[L(|v1, . . . , vt)]  v - E[V ( |v1, . . . , vt)]  v - vt. Since E[L( |v1, . . . , vt)]  0 we get that E[L(|v1, . . . , vt)]  (v - vt)+ in this case as well.
To complete the proof, we add the inequality -( - )(v - vt)+  -( - )E[L( |v1, . . . , vt)] to Inequality (2) and get

vt - (v - vt)+  E[V ( |v1, . . . , vt)] -  и E[L(|v1, . . . , vt)].

A contradiction is reached since this implies that  should also select candidate t as well. By applying Proposition 4.5 we reach the following corollary:

Corollary 4.7 Let   . The expected value of the candidate selected by a -biased agent is higher than the expected value of a candidate selected by a -biased agent, i.e. E[V ()]  E[V ()].

10

4.3 Monotonicity and Non-Monotonicity in the Number of Candidates
Interestingly, monotonicity in the number of candidates -- which is trivial for unbiased agents (i.e., free disposal) --Г does not necessary hold for biased agents with  > 0. In particular, we observe that while adding more candidates to the end of the sequence can only increase the expected utility (the argument is similar to unbiased agents), adding more candidates to the beginning of the sequence can actually reduce the expected utility. We begin by showing that adding candidates at the end of the sequence can only increase the expected utility of the agent.
In this section, because we are contemplating varying the sequence of candidates, we modify our notation for optimal -biased stopping rules to explicitly indicate the sequence for which the stopping rule is optimized. Thus, (V1, . . . , Vn) denotes the optimal -biased stopping rule for the sequence of candidates V1, . . . , Vn, and U((V1, . . . , Vn)) denotes the expected utility of a -biased agent using it. Similar to before, V ((V1, . . . , Vn)) is a random variable that equals the expected value of the candidate selected by (V1, . . . , Vn).
Claim 4.8 For any   0 we have that U((V1, . . . , Vn+1))  U((V1, . . . , Vn)) and E[V ((V1, . . . , Vn, Vn+1))]  E[V ((V1, . . . , Vn))].
Proof: Observe that a -biased agent facing the sequence V1, . . . , Vn, Vn+1 can use a stopping rule identical to the one an agent facing V1, . . . , Vn uses. By doing so its expected utility will be identical to the expected utility of the agent facing V1, . . . , Vn. Thus, U((V1, . . . , Vn+1))  U((V1, . . . , Vn)). To see why it is also the case that E[V ((V1, . . . , Vn, Vn+1))]  E[V ((V1, . . . , Vn))] we observe that the optimal -biased stopping rule for V1, . . . , Vn+1 is more patient than the optimal -biased stopping rule for V1, . . . , Vn.4 To see why this is the case, assume towards contradiction that there exists a realization v1, . . . , vt such that (V1, . . . , Vn, Vn+1) selects candidate t and (V1, . . . , Vn) selects candidate t > t. Notice that since both stopping rules are optimal for a -biased agent, the utility in each of them for selecting candidate t is the same. Now, if the stopping rule (V1, . . . , Vn) selects candidate t > t it means that the expected utility for doing so is greater than the utility of selecting candidate t. We reach a contradiction, since (V1, . . . , Vn, Vn+1) can guarantee a utility at least as high as the utility of (V1, . . . , Vn) which means it could not have selected candidate t. By Proposition 4.5 this implies that E[V ((V1, . . . , Vn, Vn+1))]  E[V ((V1, . . . , Vn))] as required.

We now consider adding a candidate at the beginning of the sequence. Such addition can decrease

the expected utility of a -biased agent. Example 3.1 illustrates this: If the agent only considers

the candidate with the expected value of 1, then the agent's expected utility is 1. However, if the

candidate

with

the

fixed

value

of

1 1+(1-)

is

added

to

the

beginning

of

the

sequence,

then

the

expected

utility

of

the

-biased

agent

decreases

to

1 1+(1-)

.

Thus,

the

expected

utility

decreased

by

a

factor

of  + 1. We show that this ratio is tight:

Claim

4.9

For

any





0

we

have

that

U((V0, V1, . . . , Vn))



1 +1

U

(

(V1,

.

.

.

,

Vn))

and

E[V

((V0, V1, . . . , Vn))]



1 +1

E[V

((V1, . . . , Vn))].

Proof: We lower bound U((V0, V1, . . . , Vn)) by considering two alternative stopping rules that a -biased agent may use in this instance:

1. Always select candidate 0 - the expected utility is E[V0].

2. Never select candidate 0 and then continue as in the optimal stopping rule for V1, . . . , Vn - the expected utility is at least U((V1, . . . , Vn)) - E[V0].
4To be completely formal about this, notice that one can trivially extend (V1, . . . , Vn) to be defined over V1, . . . Vn+1.

11

Since, each of these stopping rules is valid we get that:

U((V0, V1, . . . , Vn))  max{E[V0], U((V1, . . . , Vn)) - E[V0]}

Notice

that

if E[V0]



U((V1, . . . , Vn)) - E[V0] we

get

that E[V0]



U

(

(V1 ,...,Vn +1

))

,

hence

U((V0, V1, . . . , Vn))



U

(

(V1 ,...,Vn +1

))

.

Else,

E[V0]

<

U (V1 ,...,Vn ) +1

and

in

this

case,we

get

that

U((V0,

V1,

.

.

.

,

Vn))



U(V1,

.

.

.

,

Vn)

-

E[V0]



U((V1,

.

.

.

,

Vn))

-



U((V1, . . +1

.

,

Vn))

=

U((V1, . . . +1

,

Vn))

as

required.

To

observe

that

E[V ((V0, V1, . . . , Vn))]



1 +1

E[V

((V1

,

.

.

.

,

Vn

))]

notice

that

adding

a candidate can never decrease the expected value of the candidate selected by an unbiased agent.

Thus, we have that E[V (0(V0, V1, . . . , Vn))]  E[V (0(V1, . . . , Vn))]. By applying Theorem 3.6 we

get

that

E[V

((V0, V1, . . . , Vn))]



1 +1

E[V

(0(V0, V1, . . . , Vn))].

Putting

these

together

we

get

that

E[V

((V0, V1, . . . , Vn))]



1 +1

E[V

((V1, . . . , Vn))]

as

required.

5 Ordering Problems
In the classic prophet inequality setting we assume that the order of the candidates is predetermined. In this section we relax the assumption. We first consider the setting in which the order of the candidates is chosen uniformly at random. We show two types of bounds here with respect to the prophet: a tight and easy bound of n and a much more challenging bound of about ln(). Both bounds provide a significant improvement over the bounds for fixed order, and both are tight (up to constant factors) as a function of their respective parameters.
In the second ordering problem we consider, the agent can decide on the ordering of the candidates. We assume that the agent will do so in order to maximize the expected value of the selected candidate. This would be, for example, the objective of an agent who is sophisticated about its bias (i.e., is aware of it) and hence when ordering the candidates it tries to maximize the expected value of the selected candidate. We mainly focus on distributions whose support only includes 2 points (henceforth, 2point distributions). Such distributions were studied -- in the ordinary setting of (unbiased) optimal stopping -- by [1], which showed a bound of 5/4 on the prophet inequality. For the optimal -biased stopping rule we show a bound of 2, irrespective of the value of , which is a significant improvement with respect to the bound of n that we have from Claim 5.1 for 3-point distributions.

5.1 Random Ordering

In this section we assume that the order of the candidates is determined uniformly at random. In such

a setting it is not hard to see that the expected value of the selected candidate is at least E[V ]/n.

To see why this is the case, consider the stopping rule that always selects the first candidate. The

expected value of the candidate selected by this rule is

n i=1

1 n

E[Vi

]



1 n

E[V

].

For this stopping

rule the expected utility of the agent equals the expected value of the selected candidate. Thus, the

expected value of the candidate selected by the optimal -biased stopping rule can only be higher.

This proves the following claim.

Claim 5.1 The ratio between the prophet and a biased agent for random order is at most n.

We demonstrate the tightness of this bound by considering a family of instances in which the values of all the candidates are drawn from the same distribution:

12

Claim 5.2 There exists a family of probability distributions parameterized by n such that as n approaches infinity and  is sufficiently large (as a function of n) E[V ]/E[V ()] approaches n.

 0,

w.p

1 n

Proof:

Consider the distribution:

V

=

1
1n,3

,

w.p

1

-

1 n2

-

1 n

w.p

1 n2

.

We bound the expected value of the best candidate using the probability that its value is 1:

E[V ]  1 и

1-

1

-

1 n2

n

(3)

As for the biased agent, we pick  large enough that the optimal -biased stopping rule always selects the first candidate that has non-zero value. Denote by OP T (, n) the expected value of this stopping rule for n candidates. To solve for the expected value of the candidate selected by the -biased agent we let OP T (, n) = E[V ((V, . . . , V ))] and define and solve the following recurrence relation:
n

OP T (, n)

=

E[V

]

+

1 n

и

OP T (,

n

-

1)

where OP T (, 0) = 0. Thus, we have that:

E[V

()]

=

OP

T

(,

n)

=

n-1

n-i

и

E[V

]

=

E[V

]

и

1 1

- -

n-n n-1

.

(4)

i=0

Combining equations (3) and (4) we get that

E[V ] E[V ()]



1-

1

-

1 n2

n

E[V

]

и

1-n-n 1-n-1



1-

1

-

1 n2

n

1 n2

+

1 n3

и

1 1-n-1

=

(n

-

1)

и

1-

(

1 n2

(1 +

-

1 n2

)n

1 n3

)

и

n

.

(5)

Observe that

1

-

1 n2

n
=

n
(-1)i

n i

i=0

1 n2

i



1

-

1 n

+

1 2n2

.

Thus, we have that

1-
1 n2

1

-

1 n2

n

+

1 n3

иn



1 n

-

1 2n2

1 n

+

1 n2

=

2n - 1 2n + 2

=

1

-

3 2n +

2

which implies that

lim
n

1

-
1 n2

1

-

1 n2

+

1 n3

и

n
 n

lim 1
n

-

3 2n +

2

=

1.

In

combination

with

(5)

this

implies

E[V ] E[V ()]

 n - o(n)

as

n

.

Since

we

established

an

upper

bound of n in Claim 5.1 we conclude that the ratio E[V ]/E[V ()] approaches n as required.

For completeness we give a bound on the value of  for which the agent will always select a

non-zero candidate. In particular, we should pick  such that the optimal -biased stopping rule

selects

a

candidate

of

value

1 n3

if

this

is

the

first

candidate

with

positive

value.

We

need

to

show

that

for every such candidate, the potential expected loss from not taking the candidate is larger than

13

the potential benefit of not taking it and getting a candidate of value 1. Observe that in any step t

the the can

probability that all the agent will exhibit a loss be crudely bounded by

n1oef-xtn1иc3an.13nT.dhiOduanst,,ewtshewegioleltthhteahrvaehtaatnhvdea, latugheeenobtfew0nieilsfil tafotflrllooemwasttnho(etn1

)n-1 and in this case taking the candidate desired stopping rule

whenever:

1 n

n-1

ии

1 n3



1-

1 n3

which clearly holds for  = nn+2.
In the previous construction we established a tight bound of n - o(n) on the ratio between the prophet and a -biased agent by taking  which is exponential in n. We now show that this dependency is required by showing that as a function of  the bound between the prophet and a -biased agent is approximately bounded by ln().

Proposition

5.3

For

a

random

order

the

ratio

E[V ] E[V ()]





where



>

1

is

the

solution

to

the

equation



-

1 +1

(

-

1)

=

ln(

+

1)

-

ln(1

-

-1).

(6)

Observe that for  = 0 we get that  = e/(e - 1). In Appendix B we prove that | - ln()|  0 as   . Proof: Choose threshold  such that

Pr(V



<

)

=

1 - -1 +1

.

As in the proof of Theorem 3.4, we denote by  the threshold strategy in which we select the first candidate that has value greater than .5 If there is no such candidate the threshold strategy selects
the last candidate.
We begin by observing that

E[V ] E[V ()]



E[V

E[V ] ()] -  и E[L()]



E[V

E[V ] ()] -  и

E[L()]

.

(7)

Let

q

=

1--1 +1

denote

the

probability

that

V

<

.

To

bound

the

numerator

on

the

right

side

of

(7),

note that
n

E[V ]   + E[(V  - )+]   + E[(Vi - )+].

(8)

i=1

To bound the denominator, for each i let ci denote the probability that all of the values observed before the arrival of vi are less than . Then

n

E[V ()]  (1 - q) + ciE[(Vi - )+]

(9)

i=1

E[L()]  q

(10)

n

E[V ()] -  и E[L()]  (1 - ( + 1)q) + ciE[(Vi - )+].

(11)

i=1

5As in the proof of Theorem 3.4, there are technicalities that arise if the distribution of V  has a point-mass at . Appendix A explains how to address the technicalities.

14

By our choice of q, we have 1 - ( + 1)q = -1. Below we will show that for every candidate i, ci  -1. Notice that i is the index of the candidate and not its location in an ordering (the ordering
is chosen uniformly at random). Assuming this inequality for the moment, it implies

n

E[V ()] -  и E[(L)]  -1 + -1 E[(vi - )+]

(12)

i=1

and, in conjunction with (8), this implies

E[V

E[V ] ()] -  и E[L()]





(13)

which will finish the proof of the proposition. To bound ci from below, it will help to analyze the following sampling process for generating
the random permutation of the items. We sample i.i.d. uniformly-distributed values 1, 2, . . . , n from the interval [0, 1] and assume the items arrive in order of increasing i. Recall that ci is the probability of the event Ei, that no items arriving before i have values above . We have

Ei = Eij ,
j=i

(14)

where Eij denotes the event that either Vj <  or j > i. Let qj = Pr(Vj < ). We have

Pr(Eij | i) = (1 - i) + iqj

(15)

and the events {Eij | j = i} are conditionally independent, given i. Hence,

Pr(Ei | i) = (1 - i + iqj).

(16)

j=i

Each factor in the product on the right side can be bounded from below using Jensen's Inequality:

1 - i + iqj = (1 - i) exp(0) + i exp(ln(qj))  exp(i ln(qj)).

(17)

Multiplying these lower bounds together, we obtain





Pr(Ei | i)  exp i ln(qj) .

(18)

j=i

Now, observe that

ln(qj) = ln Pr(max Vj < )  ln Pr(V  < ) = ln(q)

(19)

j=i

j=i

hence

Pr(Ei | i)  exp(i ln(q)).

(20)

Now integrate with respect to i to obtain the unconditional probability:

ci = Pr(Ei) 

1
exp(i
0

ln(q)) di

=

1 ln(q)

[exp(ln(q))

-

exp(0)]

=

1-q ln(1/q)

.

(21)

15

Recalling

that

q

=

1--1 +1

and

that

-

1 +1

(

-

1)

=

ln(

+

1)

-

ln(1

-

-1)

we

find that

1

-

q

=

 + -1 +1

ln(1/q)

=

ln(

+

1)

-

ln(1

-

-1)

=



-

1 +1

(

-

1)

1-q ln(1/q)

=

 + -1 ( + 1) - (

- 1)

=

 + -1  + 1

=

-1

which concludes the proof that ci  -1, as desired.

5.2 Picking the Best Ordering
In this section we consider the setting in which the agent, or someone else on his behalf, can choose the ordering to maximize the expected value of the selected candidate. It is not hard to see that by placing the candidate with highest expectation first, the value of the candidate selected by a -biased agent for any  > 0 is at least 1/n of the expected value of the candidate selected by a prophet. Claim 5.2 which considers i.i.d. distributions applies to this case as well, and implies that this is tight if we allow  to depend exponentially on n. The probability distribution used in Claim 5.2 has 3 points in its support. In this section we show that this is a necessary condition by proving that for any  > 0, the prophet inequality for the best ordering when the candidates are drawn from distributions with 2 points in their support is at most 2.
Formally, 2-point distributions are defined as follows:

Vi =

hi, li,

w.p pi w.p 1 - pi

As in previous settings we assume that the agent knows the distributions that the candidates are drawn from. The proof of the following proposition has a similar structure to the proof of Agrawal et al. [1] showing that for a rational agent the ratio between the candidate selected by the prophet and by the optimal stopping rule is at most 5/4.

Proposition 5.4 If all candidates are drawn from 2-point distributions then for every , there exists an ordering such that the ratio between the expected value of the candidate selected by the optimal -biased stopping rule and a prophet is at most 2. This is tight as  approaches infinity.

Proof: Let X# denote a candidate with a maximal low value (i.e., l#  arg max li). We show that one of the following two orderings guarantees that for any  > 0 the optimal -biased stopping rule selects a candidate of expected value at least E[V ]/2:
1. Order all the candidates in decreasing order of hi.
2. First order all candidates such that hi  E[X#] in decreasing order of high value. Then, locate X# and after it the rest of the candidates in decreasing order of high value.
In the following analysis instead of analyzing the expected value of ordering 2 we analyze the expected value of the candidate selected when ordering 2 is truncated by removing all of the candidates after X#. By Claim 4.8 we have that the expected value of the candidate selected from ordering 2 is at least as high as the expected value of the candidate selected from this truncated ordering, which we refer to as ordering 2' henceforth.

16

Observe that for any  > 0, any candidate Vi before X# such that hi  E[X#] will be selected if and only if its high value is realized. Candidates before X# are never selected when their low value is realized because for any such candidate we have that li  l#, so the expected utility for continuing is at least as great as the expected value for stopping, in the event that li is realized. On the other hand, if hi is realized, in the case of ordering 1 it is easy to see that the optimal -biased rule must stop on Vi because hi lies above the support of the value distribution of every remaining unobserved candidate. In the case of ordering 2', to show that the optimal -biased rule stops when hi  E[X#] is realized, we observe that if the stopping rule were to continue past candidate i, then conditional on stopping before X# it must select a candidate of value at most hi, and conditional on stopping at X# it selects a candidate of expected value E[X#], so in both cases the expected utility of continuing
is no greater than the utility of stopping on hi. This together with the fact that in both orderings any candidate i such that hi  h# is located before X# implies that in the scenario that V  > h# the biased agent would select the best candidate in both orderings.
From now on we focus on the scenario that V   h#. Let S denote the set of candidates whose high value is less than or equal to h# excluding X#. We denote by pS the probability that a high value was realized for at least one of the candidates in S (i.e., pS = 1 - jS(1 - pj)). We also denote by wS the expected value of the best candidate among S in this case (i.e., wS = E[maxjS Vj|j  S s.t Vj = hj] ). With this notation, the expected value of the best candidate for this scenario is:
E[V |V   h#] = p# и h# + (1 - p#)pS и wS + (1 - p#)(1 - pS) и l#
Observe that the conditional expected value of ordering 1 given V   h# is at least E[X#]. Also notice that since we are interested in a bound for any  > 0 we cannot guarantee a better lower bound. The reason for this is that since l# is the maximal low value there exists a value of  for which a -biased agent would select X# even when a low value is realized. As for ordering 2', recall that we select any candidate i prior to X# if and only if a high value is realized and hi  E[X#]. Moreover, since the candidates are ordered in decreasing order of high value the agent will select
the candidate that has the highest such value. If no such value is realized the expected value of the selected candidate will be E[X#]. Letting S = {i|E[X#]  hi  h#}, the conditional expected value of the selected candidate in this ordering is at least:

E max max hi, E[X#] = E max max hi, E[X#]

iS  :Vi =hi

iS:Vi=hi

 pS и wS + (1 - pS)E[X#]

To complete the proof we show that the sum of conditional expected values of orderings 1 and 2' is greater than or equal to E[V |V   h#]. To this end, observe that

E[X#] + pS и wS + (1 - pS)E[X#]  p# и h# + (1 - p#)pS и wS + (1 - p#)(1 - pS) и l#

ordering 1

ordering 2

= E[V |V   h#]

This implies that at least one of the orderings should provide a 2-approximation. Together with the fact that the expected value of the candidate selected in ordering 2 is at least as high the expected value of the candidate selected in ordering 2' this concludes the proof.
In Claim B.2 in the appendix we show that this bound is essentially tight. We do so by constructing a family of instances with all candidates drawn from 2-point distributions, such that as  goes to infinity the ratio between the expected value of the selected candidate in the best ordering and the value of the best candidate in hindsight approaches 2.

17

References
[1] Shipra Agrawal, Jay Sethuraman, and Xingyu Zhang. On optimal ordering in the optimal stopping problem. In Proceedings of the 21st ACM Conference on Economics and Computation, pages 187Г188, 2020.
[2] Y. S. Chow, H. Robbins, and Siegmund D. Great Expectations: The Theory of Optimal Stopping. Houghton Mifflin, 1971.
[3] Jos┤e Correa, Patricio Foncea, Ruben Hoeksma, Tim Oosterwijk, and Tjark Vredeveld. Recent developments in prophet inequalities. ACM SIGecom Exchanges, 17(1):61Г70, 2019.
[4] Jose Correa, Raimundo Saona, and Bruno Ziliotto. Prophet secretary through blind strategies. Mathematical Programming, pages 1Г39, 2020.
[5] David Genesove and Christopher Mayer. Loss aversion and seller behavior: Evidence from the housing market. The quarterly journal of economics, 116(4):1233Г1260, 2001.
[6] Daniel Kahneman and Amos Tversky. Prospect theory: An analysis of decision under risk. Econometrica, 47(2):263Г291, 1979.
[7] Ulrich Krengel and Louis Sucheston. On semiamarts, amarts, and processes with finite value. Probability on Banach spaces, 4:197Г266, 1978.
[8] Brendan Lucier. An economic view of prophet inequalities. ACM SIGecom Exchanges, 16(1):24Г 47, 2017.
[9] Terrance Odean. Are investors reluctant to realize their losses? The Journal of finance, 53(5):1775Г1798, 1998.
[10] Ester Samuel-Cahn. Comparison of threshold stop rules and maximum for independent nonnegative random variables. the Annals of Probability, 12(4):1213Г1216, 1984.
[11] Daniel Schunk and Joachim Winter. The relationship between risk attitudes and heuristics in search tasks: A laboratory experiment. Journal of Economic Behavior & Organization, 71(2):347Г360, 2009.
A Using Randomized Tie Breaking to Deal With Point Masses
In Theorem 3.4 and Proposition 5.3 we analyzed stopping rules defined by setting a threshold  so as to equate Pr(V   ) with a specified constant. When the distribution of V  contains no point masses, its cumulative distribution function is continuous, so the intermediate value theorem guarantees the existence of such a .
However, when the distribution of V  contains point masses, it is possible that there is no  that makes Pr(V   ) precisely equal to the specified constant. If so, one should interpret the definition of the -threshold strategy to include randomized tie-breaking.
Definition A.1 (randomized -threshold strategy) For   0 and 0  q  1, the randomized -threshold strategy with parameter q is the randomized stopping rule that behaves as follows: it always selects the final candidate in the sequence if no earlier candidate was selected; otherwise,
18

when observing candidate i with value vi, it never selects the candidate if vi < , it always selects the candidate if vi > , and when vi =  it selects the candidate with probability q.6

If  is a randomized -threshold strategy, the probability Pr(V ()  ) that the strategy selects an element whose value is greater than or equal to  always satisfies the following bounds.

Pr(V  > )  Pr(V ()  )  Pr(V   ).

(22)

Let a and b, respectively, denote the lower and upper bounds on the left and right sides of (22). If a = b -- i.e., if the distribution of V  does not have a point-mass at  -- then all three of the quantities
listed in (22) must be equal, irrespective of the value of the parameter q. Otherwise, as q varies from
0 to 1, the probability Pr(V ()  ) varies continuously and monotonically over the interval [a, b], starting at a when q = 0 and ending at b when q = 1. By the intermediate value theorem, for any
specified probability  in the interval [a, b], we can choose q such that Pr(V ()  ) = . In the proofs of Theorem 3.4 and Proposition 5.3, when we write that  is chosen so that Pr(V  >
) =  for some specified constant , what we really mean is that

 = inf{ | Pr(V  > ) < }.

(23)

For this value of , if we define a = Pr(V  > ) and b = Pr(V   ), then  belongs to the interval [a, b], and consequently, as argued above, there exists some q  [0, 1] such that the -threshold strategy with parameter q has probability exactly  of selecting a candidate of value greater than or equal to . This particular randomized -threshold strategy is the one analyzed in the proofs of Theorem 3.4 and Proposition 5.3. When a = b the choice of q is immaterial -- i.e., the equation Pr(V ()  ) =  is satisfied regardless -- so we adopt the convention that q = 0 to accord with earlier sections of this paper, in which -threshold strategies were assumed to select candidate i only if the strict inequality vi >  is satisfied, or if i is the last candidate.
Having thus defined  and  in terms of , the following properties are satisfied.
1. For all y < , Pr(V () > y)  .
2. For all y > ,

n

n

Pr(V () > y) = Pr(vt > y) и Pr( doesn't stop before t)  (1 - ) Pr(vt > y).

t=1

t=1

The first property holds simply because Pr(V () > y)  Pr(V ()  ) = . To justify the second property, first note that the event Vt > y is independent of the event that  doesn't stop before t, since the former depends only on the value vt whereas the latter depends only on v1, . . . , vt-1. Then, note that for any t,

Pr( doesn't stop before t)  Pr( stops at n)  Pr(V () < ) = 1 - .
The two properties listed above are the only properties of ,  required by the proofs of Theorem 3.4 and Proposition 5.3, so those proofs remain valid when we interpret the -threshold strategy as a randomized -threshold strategy with appropriately chosen parameter q.
6If the same randomized tie-breaking rule was already invoked on a previous candidate j with vj =  who was not selected, the probability of selecting candidate i in the present time step remains equal to q.

19

B Missing Proofs From Section 5

We begin this appendix with a lemma showing that the parameter  defined in Proposition 5.3, which bounds the ratio E[V ]/E[V ()] when candidates are observed in random order, is very close to ln().

Lemma B.1 For  > 0 the equation



-

1 +1

(

-

1)

=

ln(

+

1)

-

ln(1

-

-1)

(24)

has a unique solution  > 1. Considering this  as a function of , it satisfies | - ln()|  0 as   .

Proof: The left side of equation (24) is a strictly increasing continuous function of  and the right side is a strictly decreasing continuous function of  in the range 1 <  < . As   1 the left side converges to 1 while the right side converges to . As    the left side converges to  while the right side converges to ln( + 1). Since the difference between the two sides is a continuous, strictly monotonic function of , there is a unique value of  > 1 that equates the two sides.
Rewriting equation (24) as

 = ln( + 1) + ln

1 1 - -1

+



1 +

1

(

-

1),

(25)

we see that when  > 1 all three of the quantities on the right side are positive, hence  > ln( + 1) > ln(). To bound the difference  - ln() we rewrite equation (25) as



 +

1

и



=

ln(

+

1)

+

ln

1 1 - -1

-



1 +

1



=

+1 

ln( + 1) +



+ 

1

ln

1 1 - -1

-

1 

=

ln(

+

1)

+

ln( + 

1)

+



+ 

1

ln

 -1

-

1 

 - ln() = ln

+1 

+



+ 

1

ln

 -1

-

1 

.

(26)

The lower bound  > ln() implies that as  tends to infinity,  tends to infinity as well. All of the

quantities on the right side of (26) -- namely, ln

+1 

,

+1 

ln

 -1

,

and

1 

--

converge

to

zero

as

 and  both tend to infinity.

For both of the following proofs recall that V ((V1, . . . , Vn)) is a random variables equal to the value of the candidate selected by the optimal -biased stopping rule for the sequence (V1, . . . , Vn).

Claim B.2 There exists a family of instances with all candidates drawn from 2-point distributions, such that as  goes to infinity the ratio between E[V ] and E[V ()] in the best ordering approaches 2.

Proof: We consider candidates with values drawn from the following distributions, for small  > 0:

V1 =

1, 2

w.p  w.p 1 - 

,

V2 =

 + 2(1 - ), 0, w.p 2

w.p 1 - 2

20

In this family of instances the expected value achieved by a prophet is: E[V ] =  + (1 - )(1 - 2) и ( + 2(1 - )) + (1 - )4  2 - O(2)
We show that in any ordering the expected payoff of a biased agent with  > 1/4 is E[V ()] = E[V1] =  + 2(1 - )   - 2. Thus, as  goes to 0 the ratio between E[V ] and E[V ()] is approaching 2. To see that E[V ()] = E[V1] observe that:
и E[V ((V1, V2))] = E[V1] - We show that even when a low value is realized V1 is selected: 2  (1 - 2)( + (1 - )2) -  и 22
For  > 1/4 the right hand-side is negative and hence the inequality holds. и E[V ((V2, V1))] = E[V1] - Observe that the high value of candidate 2 equals E[V1]. Thus, it
suffices to show that the agents selects V2 when a high value is realized. To see why this is the case observe that for any  > 0:
E[V1]  E[V1] - (1 - )( + 2(1 - ) - 2)
21

