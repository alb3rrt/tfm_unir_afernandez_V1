arXiv:2106.01617v1 [cs.LG] 3 Jun 2021

Improving the Transferability of Adversarial Examples with New Iteration Framework and
Input Dropout
Pengfei Xie1, Linyuan Wang1, Ruoxi Qin1, Kai Qiao1, Shuhao Shi1, Guoen Hu1, and Bin Yan1
Henan Key Laboratory of Imaging and Intelligent Processing, PLA Strategy Support Force Information Engineering University, Zhengzhou, China ybspace@hotmail.com
Abstract. Deep neural networks(DNNs) is vulnerable to be attacked by adversarial examples. Black-box attack is the most threatening attack. At present, black-box attack methods mainly adopt gradient-based iterative attack methods, which usually limit the relationship between the iteration step size, the number of iterations, and the maximum perturbation. In this paper, we propose a new gradient iteration framework, which redefines the relationship between the above three. Under this framework, we easily improve the attack success rate of DI-TI-MIM. In addition, we propose a gradient iterative attack method based on input dropout, which can be well combined with our framework. We further propose a multi dropout rate version of this method. Experimental results show that our best method can achieve attack success rate of 96.2% for defense model on average, which is higher than the state-of-the-art gradient-based attacks.
Keywords: Adversarial examples, Black-box attack, Transferability
1 Introduction
In recent years, deep neural networks have begun to shine. It has achieved good results in tasks such as image classification [12,25] object detector [21], speech recognition [13], and natural language processing [23]. However, studies have shown that adversarial example, that is, a special example in which a disturbance imperceptible to the human eye and carefully designed by the human is added to the original example, which can make deep neural networks classification errors. This has attracted great attention from the academic community. In order to enable the safe deployment of applications such as autonomous driving and face recognition, it is necessary to study attack algorithms and defense methods about adversarial examples, and the two promote and develop each other. At present, adversarial attacks are mainly divided into digital domain attacks and physical domain attacks. Digital domain attacks are the basis of physical
Corresponding author

2

P.Xie, L.Wang et al.

Fig. 1. We show the spillover disturbance under clip with the step size as 1.6 and the number of iterations as 50. Our adversarial examples are generated by DI-TI-MIM.
domain attacks. Robust digital domain attack algorithms can be effectively used to physical domain attacks through 3D printing [1] and light projection [20]. The attack methods for generating adversarial examples can be divided into methods based on optimization [19,4],gradient iteration [11,16] and generative model [2,30]. Among them, gradient-based attack methods are the most popular because of their fast generation speed, low algorithm complexity and good transferability, i.e., using the gradient information of the substitute model to generate adversarial examples has a high attack success rate on other models. This type of attack poses a huge security threat on deep neural network, because the attacker does not need to know any information about the attacked model and make it wrong. The gradient-based attack method are developed from one-step gradient approaches Fast Gradient Sign Method (FGSM) , and then developed into multi-step iterative approaches Iterative Fast Gradient Sign Method (I-FGSM). In the case of white-box, the attack success rate of I-FGSM is usually better than that of FGSM, because it finds the direction that maximizes the loss function more precisely through multi-step iteration. But under the black-box setting, I-FGSM is usually weaker than FGSM, because it easily falls into overfitting in the substitute model, which makes it difficult to transfer the attack performance to other models. In order to make this kind of method have good performance in both white-box setting and black-box setting, some work begin to study how to improve transferability of adversarial examples. In previous work, in order to avoid gradient overflow, step size, iteration times and maximum disturbance often satisfy the certain relationship. In this paper, we further discuss the relationship among the three and propose a new gradient iteration framework. We improve the transferability of adversarial examples by a large constant step size and increasing the number of iterations. We use this method to easily improve the attack performance of Diverse Input, Translation Invariant, and Momentum Iterative Fast Gradient Sign Method(DI-TI-MIM). In order to further improve the transferability of adversarial examples, we pro-

Improving the Transferability with NIF and ID

3

pose an Input Dropout Iterative Fast Gradient Sign Method (IDI-FGSM). Different from previous work [17], we work at the data level rather than at the network level. We find that the classification accuracy and loss of the original image basically remain unchanged with the dropout rate transformation, so we believe that dropout is invariant, which is similar to the previous work [8,32,18]. However, dropout will lose part of the information of the adversarial examples, affecting its attack performance. Based on this, we introduce dropout into the gradient iteration attack method. Our method is easy to implement and can be effectively combined with other methods. In addition, we propose an Input Dropout Ensemble Iterative Fast Gradient Sign Method (IDEI-FGSM), i.e, a multiple dropout rate version of IDI-FGSM to further improve the transferability of adversarial examples. The experimental results show that our best method achieves an average attack success rate of 96.2% on the defense model. In summary, we make the following contributions:
· We study the relationship between the step size and the number of iterations in the gradient iteration attack method, and propose a new gradient iteration framework. Unlike previous frameworks, this framework allows gradient overflow. We use this framework to improve the attack performance of DI-TI-MIM easily. Our framework can be well combined with our proposed input dropout method. We explain why constant step size and increased iterations can improve attack performance.
· We propose an Input Dropout Iterative Fast Gradient Sign Method (IDIFGSM), which can be well combined with other methods based on gradient iteration, and achieve an average attack success rate of 95.3% on the defense model.
· We further propose a gradient iterative attack method with multiple dropout rate versions, i.e., Input Dropout Ensemble Iterative Fast Gradient Sign Method (IDEI-FGSM), which can achieve 96.2% success rate in the defense model.

2 Related work
Szegedy et at. [26] first propose adversarial examples. They use the L-BFGS box constraint algorithm to maximize the loss function between the original sample and ground-truth label, and constraint the disturbance size to obtain the adversarial examples, which can make the deep network wrong. Subsequently, a large number of digital domain-based adversarial attack algorithms have been proposed, including optimization methods, gradient iteration methods, and generative model methods. Accordingly, some defense methods are proposed accordingly [27,31]. At present, the most effective defense method is adversarial training [9]. However, some studies have shown that adversarial examples not only exist in the digital domain, but also can be applied to the real world. Advhat [15] successfully deceives the face recognition model, and Adv-Thirt [33] successfully deceives the object detection model. More and more studies have shown that adversarial examples exist not only in computer vision, but also in

4

P.Xie, L.Wang et al.

speech recognition [5], reinforcement learning [3], and text model [14]. It should be noted that our work is based on image classification tasks.

2.1 White-box attack
White-box attack refers to that an attacker can obtain all information of the model including network structure and parameters. These attacks include optimizationbased attacks, such as DeepFool [19], CW attacks [4]. However, the computation time of these methods is too long and it is easy to lead to overfitting, which makes it difficult to transfer attack performance to other models. The method based on gradient iteration has the characteristics of good attack performance, easy implementation of code and fast operation speed. This kind of method is the mainstream algorithm against attack. In addition, there are some attack methods based on generative model, such as ATN [2], AdvGAN [30], which learn the distribution of adversarial examples by training a generator.

2.2 Black-box attack
Black-box attack refers to the attacker cannot obtain the network structure, parameters and other information of the model. It can be subdivided into semiblack-box attacks and completely black-box attacks. In a semi-black box attack scenario, an attacker can calculate its gradient by querying the returned value by the model, such as the Zoo attack [6]. In a black box scenario, attackers attack substitute models to generate adversarial examples, and then use its transferability to attack other models. Dong et at. [7] propose an iterative algorithm based on momentum to enhance adversarial attacks. In addition, these works improve the transferability of adversarial examples by various transformations [8,32,18]. Wu et at. [29] smooth the gradient of local area to increase transferability. Gao et at. [10] enlarge the step size and use the overflow gradient information to increase transferability. In this paper, our goal is to improve the transferability of adversarial examples in black box backgrounds.

3 Methodology

In this section, we provide a detailed description of our algorithm. Let x denote the clean example, an ytrue denote the corresponding label. We use F to represent the DNNs, which can correctly map x to y, namely F (x) = ytrue. Then we create adversarial examples xadv that appears to be similar to the clean example, which can make the DNNs wrong, i.e., F (x) = ytrue, and x - xadv p  , where p usually as 0, 1, 2,  and  as the maximum value of the adversarial perturbation.
To generate adversarial examples, we maximize the cross entropy loss function J(xadv, ytrue), which will greatly reduce the confidence of adversarial examples
on the true label, and cause classification errors. The optimization objective
function are as follows:

arg max J (xadv, ytrue), s.t. xadv - x  .

(1)

xadv



Improving the Transferability with NIF and ID

5

Following previous work [8,32], we use L norm constrain the size of the disturbance. Since we do black-box attacks and can not know the internal information of the attacked model, the gradient information is completely derived from the substitute model, which use the transferability of the model to attack. How to improve the transfer attack ability between models is a challenging work. Below we will also introduce the classic algorithm of improving the ability of black box transfer attack.

3.1 Gradient-Based Attack Methods

In this section, we briefly introduce some excellent works based on gradient sign method. Fast Gradient Sign Method (FGSM): Goodfellow et at. [11] believe that the vulnerability of neural network stems from its linear nature, and proposed a gradient-based one-step attack method for the first time. The formula of updating adversarial examples is as follows:

xadv = xreal +  · sign(xJ (xreal, ytrue)),

(2)

Iterative Fast Gradient Sign Method (I-FGSM): Kurakin et at. [16] propose a multi-step iterative version of FGSM, which reaches the maximum loss point of the model with a smaller step. The updated formula is as follows:

X0adv = Xreal, Xta+d1v = ClipX {Xtadv +  · sign(Xtadv J (Xtadv, ytrue))},

(3)

where Clip denotes the clipping operation of adversarial examples for each iteration to limit adversarial perturbations into the L norm . Momentum Iterative Fast Gradient Sign Method (MI-FGSM): Dong et at. [7] introduce momentum into the gradient iteration process to stabilize the gradient update direction and avoid falling into extreme points. The formula is as follows:

gt+1 = µ · gt +

xJ (xat dv, ytrue) xJ (xat dv, ytrue)

,
1

Xta+d1v

=

ClipX {Xtadv +  · sign(gt+1)},

(4)

Diverse Input Iterative Fast Gradient Sign Method (DI2 -FGSM): Xie et at. [32] propose input transformation diversity to improve the mobility of adversarial examples. This method can be written as:

Xta+d1v = ClipX {Xtadv +  · sign(Xtadv J (T (Xtadv, p), ytrue))}

(5)

where T denote input transformation with p. Translation-Invariant Attack Method (TIM): Dong et at. [8] generate adversarial examples insensitive to the white-box discriminative region by translation invariance, and use predefined convolution kernels to replace translation operations to improve efficiency. This method has good attack performance on defense model. Scale-Invariant Nesterov Iterative Fast Gradient Sign Method (SI-NIFGSM): Lin et at. [18] adopt Nesterov accelerated gradient into the iterative

6

P.Xie, L.Wang et al.

attacks to effectively escape from global maximum. In addition, they also use the scale invariance of DNNs to improve the transferability of adversarial examples. Patch-wise Iterative Fast Gradient Sign Method (PI-FGSM): Gao et at. [10] adopt amplification operation on the step size and use the pre-trained convolution kernel to project the overflowed gradient information to the surrounding area to improve transferability. Resized-Diverse-Inputs Diversity-Ensemble Iterative Fast Gradient Sign Method (RDI-DEI-FGSM): Zou et at. [34] find that there are many vertical and horizontal stripes in the gradients of diverse inputs, which can be used to alleviate the loss of gradient information caused by TIM. They propose that resized-diverse-inputs method (RDIM) can combine with TIM to play a better attack performance. In addition, they propose diversity-ensemble method (DEM), that is, i.e., the multi-scale version of RDIM to further improve the transferability of adversarial examples.

Fig. 2. We show the relationship between our framework and previous iteration methods.

3.2 New iteration framework

As we know, the current attack methods based on gradient iteration usually limit the infinite norm disturbance , iteration number T and iteration step size  to the following formulas for preventing gradient overflow.



=

(6)

T

Zou et at. [34] propose the regional Region Fitting, which expand the iteration step size  to , but its iteration times are still limited. Gao et at. [10] propose to project the overflow gradient information to the regional patch to improve transferability. Our method is different from the above method, and it is very direct. Region Fitting can be seen as a case in our method. We directly select a large step size to prevent over-fitting, and then extend the number of iterations to improve attack performance. We allow gradient overflow, because in a certain iteration round, the benefit of extending the iteration round is greater than the loss of attack performance caused by clip. We propose a new gradient iteration

Improving the Transferability with NIF and ID

7

framework, which can be expressed as:



=

 

,

1

T =  · ,   1 and T  N +

(7)

where  is the step size expansion factor, which determine the step size,  is the iterative times factor. As shown in Figure 2, We show the relationship between our framework and the traditional gradient iteration method and Region Fitting.
The gradient descent leads to the increase of loss, while the clip limits the disturbance to reduce Loss, and the two are in a game state. As shown in Figure 1, we show the perturbations that overflow under different iterations. It can be seen that the disturbance does not overflow in the 10th round, and the disturbance begins to overflow obviously in the 11th round. However, the disturbance of clip is relatively small and the defense performance is weak. At this time, the attack performance still has room for improvement. As the number of iterations increases, the perturbations of clips increase gradually and begin to show a fixed shape, which may be a balanced state for the gradient descent and clip.

Fig. 3. The curves of accuracy and average loss value for Inc-v3, Inc-v4, IncRes-v2, Res-152, Inc-v3ens3, Inc-v3ens43ens4 and IncRes-v2ens given the dropout images at each dropout rate. The results are averaged over 1000 images.
3.3 Dropout In this section, we will describe our dropout-based method in detail.
3.3.1 Motivation Dropout [22] is a commonly used regularization method, which can effectively prevent network over-fitting and enhance the generalization performance of the network. Inspired by this, we introduce dropout into

8

P.Xie, L.Wang et al.

Algorithm 1: IDI-FGSM

Input : A Clean example x normalized to [-1, 1]; ground-truth label

ytrue ; L norm constraint ; step size  ; iteration times T ; a

substitute model f with loss function J; dropout rate .

Output: The adversarial example xadv.

1 Initialize xa0dv = x;

2 for t  0 to T - 1 do

3

xat dv = Drop(xat dv, (1 - )) · (1 - ));

4

Get the gradients by xat dv J (xat dv, ytrue);

5

Update adversarial example xat dv J (xat dv, ytrue) Clip adversarial

example xat+dv1 = Clip(xat+dv1, -1, 1)

6 end

7 Return xadv = xat dv;

the generation process of adversarial examples, which can prevent the generated adversarial examples from overfitting on the substitute model to enhance the transferability of adversarial examples.
From the perspective of data enhancement, dropout reduces the attack performance of adversarial examples and turns part of them into correctly classified examples, which makes the data more diverse. These examples will be used in the gradient iterative process to improve the transferability of adversarial examples. From the perspective of attack and defense, dropout can be regarded as a defense mechanism, which can make the adversarial examples lose part of the information, thereby reducing the attack effect of the adversarial examples. In the attack stage, the adversarial examples will adaptively overcome this defense mechanism during gradient descent, and become a more robust adversarial examples. Wang et at. [28] use dropout in the test can effectively reduce the success rate of adversarial examples. Li et at. [17] makes the dropout layer of the substitute model more diversified to increase the transferability of attacks. Above work are carried out around the dropout layer in the network structure, which must require the existence of dropout in the network, and our work is carried out around the data level, and there is no requirement for the network structure. Similar to translation-invariant [8] and scale-invariant [18], we verify that dropout also has invariance. As shown in Figure 3, when the dropout rate is between 0-0.5, the loss value and classification accuracy of the original image remain basically unchanged.
3.3.2 IDI-FGSM In this section, we introduce our method in detail. Our method is intuitive and easy to implement, and can be easily integrated into other gradient-based iterative methods. Specifically, before we input the adversarial examples into the neural network, we will dropout the adversarial examples, that is, randomly set 0 for some pixels of adversarial examples, where

Improving the Transferability with NIF and ID

9

Algorithm 2: IDEI-FGSM

Input : A Clean example x normalized to [-1, 1]; ground-truth label

ytrue ; L norm constraint ; step size  ; iteration times T ; a

substitute model f with loss function J; weights 1, 1 . . . , k; the

logits of K dropout rate

l(Drop(Xtadv, (1 - 1)) · (1 - 1)),l(Drop(Xtadv, (1 - 2)) · (1 - 2)),. . .,l(Drop(Xtadv, (1 - K )) · (1 - K )). Output: The adversarial example xadv.

1 Initialize xa0dv = x;

2 for t  0 to T - 1 do

3 Input xat dv and Get logits l(xat dv) following 9;

4

Get cross-entropy loss J (xat dv, ytrue) by xat dv J (xat dv, ytrue) by 10;

5

Obtain the gradients by xat dv J (xat dv, ytrue) Update adversarial example

xat dv J (xat dv, ytrue) Clip adversarial example xat+dv1 = Clip(xat+dv1, -1, 1)

6 end

7 Return xadv = xat dv;

dropout rate takes [0,1). Our method can alleviate the over-fitting phenomenon of adversarial examples in the substitute model, so as to improve the transferability of adversarial examples. Our method is iterative by the following methods:

Xta+d1v = ClipX {Xtadv +  · sign(Xtadv J ((Drop(Xtadv, (1 - )) · (1 - )), ytrue))} (8)
where  denotes dropout rate.

3.3.3 IDEI-FGSM In this section, we introduce our method in detail. Our method is intuitive and easy to implement, and can be easily integrated into other gradient-based iterative methods. Specifically, before we input the adversarial examples into the neural network, we will dropout the adversarial examples, that is, randomly set 0 for some pixels of adversarial examples, where dropout rate takes [0,1). Our method can alleviate the over-fitting phenomenon of adversarial examples in the substitute model, so as to improve the transferability of adversarial examples. Our method is iterative by the following methods:

l(x) =

K 1

k

·

l(Drop(xat dv,

(1

-

k ))

·

(1

-

k ))

(9)

where

K 1

k

=

1,

k



0,

k



[0, 1).

Following

the

former

work,

we

use

the

softmax cross-entropy loss as our loss function:

J (xat dv, ytrue) = -1ytrue · log(softmax(l(xat dv)))

(10)

10

P.Xie, L.Wang et al.

Table 1. Abbreviations used in the paper.

Abbreviation

Explanation

DI-TI-MIM

The combination of DIM,TIM,and MIM

DTPI-FGSM

The combination of DIM,TIM,and PIM

DTPIDI-FGSM The combination of DIM,TIM,PIM,and IDI-FGSM

DI-TI-ID-MIM The combination of DIM,TIM,PIM and IDI-FGSM

DI-TI-IDE-MIM The combination of DIM,TIM,MIM and IDEI-FGSM

4 Experiments
In this section, we conduct experiments on the Imagenet dataset to verify the effectiveness of our method. We specify the experimental setup parameters in Sec. 4.1. We test our iteration framework in Sec. 4.2. We show the success rate of black box attacks under different dropout rates in Sec. 4.3. We report the results of attacking a single model in Sec. 4.4 and an ensemble of models in Sec. 4.5.
4.1 Experimental Settings
Dataset. Following the previous work [7,8] we conduct experiments on the Imagenet dataset, including 1000 images that are resized to 299×299×3, which are used in the competition of NIPS2017. Networks. We selected ten models for experiments, including four normal trained models, i.e., Inception-v3(Inc-v3) [25], Inception-v4 (Inc-v4) , InceptionResnet-v2(IncRes-v2), [24] Resnet-v2-152 (Res-152) [12], and three ensemble adversarially trained models [27], i.e., ens3-adv-Inception-v3 (Inc-v3ens3), ens4adv-Inception-v3 (Inc-v3ens43ens4) and ens-adv-Inception-ResNet-v2 (IncResv2ens), and three more robust denoise models [31], ResNet152 Baseline (Res152B), ResNet152 Denoise (Res152D), ResNeXt101 DenoiseAll (ResNeXtDA). Implementation details. In our experiment, we compare the advanced attack methods based on gradient iteration, including I-FGSM, MIM, PIM, DIM, TIM, and their combinations, namely DI-TI-MIM and DMPI-FGSM. In all experiments, our maximum perturbation  is set 16. For the previous work sucn as I-FGSM, DIM, PIM, DI-TI-MIM and DTPI-MIM, we set the number of gradient iterations to 10 and the step size to 1.6. For MIM, we set the decay factor µ = 1.0. For TI-BIM, we set the kernel size k = 15. and for DIM, we set the transformation probability p = 0.7. For PIM, we set amplifification factor  = 10. For IDI-FGSM, we set the dropout rate to 0.1. For IDEI-FGSM, we set the dropout rate to 0.0, 0.1, 0.2, 0.3, 0.4, and the weight factors are equal. When attacking the ensemble of models, we set the equal weight.
4.2 Extend the number of iterations
In this section, we focus on the relationship between step size and iteration round. We attack the Inc-v3 model with DI-TI-MIM algorithm to generate adversarial

Improving the Transferability with NIF and ID

11

examples, and then test them on both defenseless model: Inc-v4, IncRes-v2, and Res-152, and defense models: Inc-v3ens3, Inc-v3ens43ens4, and IncRes-v2ens. We set the iteration round to 50, and test the attack success rate under different iteration times, where the step size is set to 1.6, 3.2, 4.8, 6.4, 8.0, 9.6, 11.2, 12.8, 14.4, 16.0 respectively. Specifically, we further study DI-TI-MIM and DI-TI-IDMIM algorithm with the case of step size as 1.6 and 16, and extend the iteration rounds to 200 times.

Fig. 4. The curves of average attack success rate (%) for defenseless model: Inc-v4, IncRes-v2, Res-152 and defense model: Inc-v3ens3, Inc-v3ens43ens4, IncRes-v2ens. The adversarial examples are crafted for Inc-v3 model by DI-TI-MIM and DI-TI-ID-MIM. Left Column: The average attack success rate of three defenseless model with 10 different step sizes. Middle Column: The average attack success rate of three defense model with 10 different step sizes. Right Column: The average attack success rate of three defenseless models and three defense models with step sizes as 1.6 and 16.
As shown in the as in Figure 4, we find that when the number of iterations is extended, the attack success rate can be significantly improved. Compared with 10 iterations, the attack success rate of 50 iterations with different step size increases by an average of 6.9% on the defenseless model. When the step size is larger, the convergence of loss is faster, and it can achieve a higher attack success rate with a smaller number of iterations.
For non-defense model, the success rate of large-step attack is better than that of small-step attack, but for defense model, the success rate of small-step

12

P.Xie, L.Wang et al.

Fig. 5. The average attack success rate(%) of Inc-v3, Inc-v4, IncRes-v2, Res-152, Incv3ens3, Inc-v3ens43ens4, IncRes-v2ens, Res152B, Res152D and ResNeXtDA. The adversarial examples are made by DI-TI-ID-MIM algorithm, and the substitute model is Inc-v3.
attack is better than that of large-step attack. We believe that this is because the similarity of the loss surface between the substitute model and the nondefense model is high, so even a large step size can find the extreme point, while the similarity of the loss surface between the substitute model and the defense model is low, and only a small step size can find the appropriate extreme point. However, it should be pointed out that for DI-TI-MIM, as the number of iterations increases, its performance on the defense model will first increase and then decrease. We believe that this is because the small step size can find a better extreme point at the beginning, but it is easy to fall into over-fitting with the increase of iterations. Our method DI-TI-ID-MIM can well overcome this problem. The results show that our DI-TI-ID-MIM improves the attack performance on the defense model by 15.5 % at 200 times compared with 10 times. Based on the above analysis, we believe that large step size has the advantage of fast convergence, and its attack performance on the non-defense model is more advantageous, but its attack performance on the defense model is weaker than that on the small step size. However, small strides must improve attack performance by extending iteration rounds, which takes a lot of time. In order to maximize the attack performance of the algorithm and prevent excessive time consumption, we set the number of iterations to 50 and the step size to 1.6. Our subsequent experiments will be based on this iterative framework.

Improving the Transferability with NIF and ID

13

Table 2. Success rate of non-target attack (%) for single model. The leftmost column is an alternative model, and the top row represents the test model ( ' * ' represents a white box attack ). The adversarial examples are generated by I-FGSM, DIM, IDI-FGSM, DI-TI-MIM, DI-TI-ID-MIM, and DI-TI-IDE-MIM, respectively.

Model Attack

Inc-v3 Inc-v4 IncRes-v2 Res-152 Inc-v3ens3 Inc-v3ens43ens4 IncRes-v2ens

Inc-v3

I-FGSM

100.0*

DIM

100.0*

IDI-FGSM(Ours)

100.0*

DI-TI-MIM

98.7*

DI-TI-ID-MIM(Ours) 99.9*

DI-TI-IDE-MIM(Ours) 100.0*

31.2 53.4 52.8 67.3 80.5 89.3

21.1 43.2 47.3 55.7 69.3 80.0

19.3 36.9 37.9 51.9 68 73.3

11.2 15 19.6 50.5 69.3 72.0

12.7 14.2 17.5 50.8 69.7 71.6

5.6 7 10.1 39.4 54.3 57.5

Inc-v4

I-FGSM DIM IDI-FGSM(Ours) DI-TI-MIM DI-TI-ID-MIM(Ours) DI-TI-IDE-MIM(Ours)

44.8 66.4 67.9 73.7 86 91.7

100.0* 100.0* 100.0* 99.4* 100.0* 100.0*

25 49.1 52 61 72.5 82.3

25.6 40.9 44.8 52.7 67.6 76.2

12.9 15 21.7 52.6 69.1 76.1

12.2 15.9 20.5 51.7 67.8 75.1

6.9 8.2 13.1 41.9 58.8 63.0

I-FGSM

DIM

IncRes-v2

IDI-FGSM(Ours) DI-TI-MIM

DI-TI-ID-MIM(Ours)

DI-TI-IDE-MIM(Ours)

46.5 73.5 74.9 77.7 88.4 91.7

38.2 68.8 65.2 73.5 86.3 81.6

99.9* 99.4* 100.0* 97* 99.6* 99.9*

26.4 51 51.1 63.5 78.9 85.1

12.6 19 22.7 58.4 82.4 84.7

12.9 19.5 21 57.6 80.5 84.3

6.5 12.4 15 49.3 79 81.9

Res-152

I-FGSM DIM IDI-FGSM(Ours) DI-TI-MIM DI-TI-ID-MIM(Ours) DI-TI-IDE-MIM(Ours)

30.3 62.1 53.8 64.0 75.7 83.6

25.8 57.5 46.4 60.0 72.4 81.7

18.7 50.5 42.7 56.4 68.9 79.5

99.5* 99.2* 99.6* 97.9* 99.7* 99.8*

13.8 22.7 22.4 63.8 77.2 85.3

12.4 20.6 20.2 64.1 76.6 85.3

8.2 12.0 13.7 62.6 69.3 80.3

4.3 Dropout rate
In this section, we study the relationship between dropout rate and attack success rate. We use the DI-TI-ID-MIM algorithm with different dropout, varied from 0.0 to 0.9, to attack Inc-v3 model, with step size as 1.6 and iteration round as 50. The generated adversarial example are then tested in three defenseless models: Inc-v4, IncRes-v2, and Res-152, three ensemble adversarial training models: Inc-v3ens3, Inc-v3ens43ens4, and IncRes-v2ens, and three more robust denoise models: Res152B, Res152D and ResNeXtDA. As shown in the Figure 5, the results show that dropout rate between 0.0 and 0.2 can improve the transferability of adversarial examples when attacking defenseless model, and the success rate of attack with dropout rate as 0.1 is relatively better. Dropout rate between 0 and 0.5 can improve the attack success rate when attacking ensemble adversarial training models, and dropout rate as 0.1 can achieve a relatively better attack success rate. When attacking denoise models, dropout rate as 0.4 or 0.5 can get better transferability of adversarial examples. Increasing dropout rate can prevent the overfitting of adversarial examples to improve the success rate of black-box attack, but too large dropout will lose too much information of adversarial examples, resulting in a reduction in the success rate of white-box attack. Therefore, there is a trade-off on dropout

14

P.Xie, L.Wang et al.

Table 3. Success rate of non-target attack (%) for ensemble model. The leftmost column are attack algorithms, and the top row represents the test model. The adversarial examples are generated by PI-FGSM, DI-TI-MIM, DTPI-FGSM, DI-TI-ID-MIM, DTPIDI-FGSM, and DI-TI-IDE-MIM respectively.

Inc-v3ens3 Inc-v3ens43ens4 IncRes-v2ens ResNeXtDA Res152B Res152D

PI-FGSM DI-TI-MIM DTPI-FGSM DI-TI-ID-MIM(Ours) DTPIDI-FGSM(Ours) DI-TI-IDE-MIM(Ours)

67.0 80.0 89.3 96.6 88.0 96.8

67.2 78.8 89.2 96.1 89.7 96.8

59.0 75.6 83.4 93.3 84.6 95.1

8.2 6.3 11.7 10.6 12.6 11.3

6.8 5.7 10.6 10.5 12.0 9.7

7.3 4.3 10.4 10.6 12.3 9.8

rate. The experimental results show that white-box and black-box have better performance when dropout rate as 0.1.
4.4 Single-Model Attacks
In this section, we show the attack performance of I-FGSM, DIM, IDI-FGSM, DITI-MIM, DI-TI-ID-MIM, and DI-TI-IDE-MIM algorithms. Specifically, we chose a single model from Inc-v3, Inc-v4, IncRes-v2, and Res-152 as our substitute model and test it on the remaining three defenseless models and three defense models: Inc-v3ens3, Inc-v3ens43ens4, and IncRes-v2ens. As in Table 2, the results show that our method increases by 15.6% on average compared with I-FGSM, and DI-TI-ID-MIM increases by 15.4% on average than DI-TI-MIM. In addition, our DI-TI-IDE-MIM achieves an average attack success rate of 79.7% on the six models, which is 21.4% higher than DI-TI-MIM.
4.5 Ensemble-based Attacks
In this section, we show the attack performance of PI-FGSM, DI-TI-MIM, DTPIFGSM, DI-TI-ID-MIM, DTPIDI-FGSM, and DI-TI-IDE-MIM algorithms. We attack four normally trained models: Inc-v3, Inc-v4, IncRes-v2, and Res-152 in an ensemble manner to obtain adversarial examples. Then we test them on six defense models: Inc-v3ens3, Inc-v3ens43ens4, IncRes-v2ens, Res152B, Res152D and ResNeXtDA. As in Table 3, our method combined with DTPI-FGSM can increase by 1.4% on average on the denoise model. Our method combined with DI-TI-MIM can increase by 17.2% on average on the ensemble adversarial training model. Our method DI-TI-IDE-MIM can achieve an average attack success rate of 96.2% on the ensemble adversarial training model, which is higher than the state-of-the-art gradient-based attacks.
5 Conclusions
In this paper, we study the relationship between step size, iteration number and maximum disturbance, and explore the potential attack performance of DI-

Improving the Transferability with NIF and ID

15

TI-MIM. We verify that dropout has input invariant, and propose a gradient iterative attack method based on dropout. On this basis, we further propose an integrated version of droput rate. Our method can be combined with other attack methods based on gradient iteration. Our best approach can achieve an average attack success rate of 96.2% on defense models.

6 Acknowledgments
This work was supported by the National Key R&D Program of China under grant 2017YFB1002502 and National Natural Science Foundation of China (No. 61701089, No.61601518 and No. 61372172).

16

P.Xie, L.Wang et al.

References

1. Athalye, A., Engstrom, L., Ilyas, A., Kwok, K.: Synthesizing robust adversarial examples. In: International conference on machine learning. pp. 284­293. PMLR (2018) 2
2. Baluja, S., Fischer, I.: Adversarial transformation networks: Learning to generate adversarial examples. arXiv preprint arXiv:1703.09387 (2017) 2, 4
3. Behzadan, V., Munir, A.: Vulnerability of deep reinforcement learning to policy induction attacks. In: International Conference on Machine Learning and Data Mining in Pattern Recognition. pp. 262­275. Springer (2017) 4
4. Carlini, N., Wagner, D.: Towards evaluating the robustness of neural networks. In: 2017 ieee symposium on security and privacy (sp). pp. 39­57. IEEE (2017) 2, 4
5. Carlini, N., Wagner, D.: Audio adversarial examples: Targeted attacks on speechto-text. In: 2018 IEEE Security and Privacy Workshops (SPW). pp. 1­7. IEEE (2018) 4
6. Chen, P.Y., Zhang, H., Sharma, Y., Yi, J., Hsieh, C.J.: Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models. In: Proceedings of the 10th ACM workshop on artificial intelligence and security. pp. 15­26 (2017) 4
7. Dong, Y., Liao, F., Pang, T., Su, H., Zhu, J., Hu, X., Li, J.: Boosting adversarial attacks with momentum. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 9185­9193 (2018) 4, 5, 10
8. Dong, Y., Pang, T., Su, H., Zhu, J.: Evading defenses to transferable adversarial examples by translation-invariant attacks. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4312­4321 (2019) 3, 4, 5, 8, 10
9. Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., Marchand, M., Lempitsky, V.: Domain-adversarial training of neural networks. The journal of machine learning research 17(1), 2096­2030 (2016) 3
10. Gao, L., Zhang, Q., Song, J., Liu, X., Shen, H.T.: Patch-wise attack for fooling deep neural network. In: European Conference on Computer Vision. pp. 307­322. Springer (2020) 4, 6
11. Goodfellow, I.J., Shlens, J., Szegedy, C.: Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572 (2014) 2, 5
12. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 770­778 (2016) 1, 10
13. Hinton, G., Deng, L., Yu, D., Dahl, G.E., Mohamed, A.r., Jaitly, N., Senior, A., Vanhoucke, V., Nguyen, P., Sainath, T.N., others: Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal processing magazine 29(6), 82­97 (2012) 1
14. Jin, D., Jin, Z., Zhou, J.T., Szolovits, P.: Is bert really robust? a strong baseline for natural language attack on text classification and entailment. In: Proceedings of the AAAI conference on artificial intelligence. vol. 34, pp. 8018­8025 (2020) 4
15. Komkov, S., Petiushko, A.: Advhat: Real-world adversarial attack on arcface face id system. In: 2020 25th International Conference on Pattern Recognition (ICPR). pp. 819­826. IEEE (2021) 3
16. Kurakin, A., Goodfellow, I., Bengio, S., others: Adversarial examples in the physical world (2016) 2, 5

Improving the Transferability with NIF and ID

17

17. Li, Y., Bai, S., Zhou, Y., Xie, C., Zhang, Z., Yuille, A.: Learning transferable adversarial examples via ghost networks. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 34, pp. 11458­11465 (2020) 3, 8
18. Lin, J., Song, C., He, K., Wang, L., Hopcroft, J.E.: Nesterov accelerated gradient and scale invariance for adversarial attacks. arXiv preprint arXiv:1908.06281 (2019) 3, 4, 5, 8
19. Moosavi-Dezfooli, S.M., Fawzi, A., Frossard, P.: Deepfool: a simple and accurate method to fool deep neural networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2574­2582 (2016) 2, 4
20. Nguyen, D.L., Arora, S.S., Wu, Y., Yang, H.: Adversarial Light Projection Attacks on Face Recognition Systems: A Feasibility Study. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. pp. 814­815 (2020) 2
21. Redmon, J., Farhadi, A.: Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767 (2018) 1
22. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.: Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research 15(1), 1929­1958 (2014) 7
23. Sutskever, I., Vinyals, O., Le, Q.V.: Sequence to sequence learning with neural networks. arXiv preprint arXiv:1409.3215 (2014) 1
24. Szegedy, C., Ioffe, S., Vanhoucke, V., Alemi, A.: Inception-v4, inception-resnet and the impact of residual connections on learning. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 31 (2017) 10
25. Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z.: Rethinking the inception architecture for computer vision. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2818­2826 (2016) 1, 10
26. Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., Fergus, R.: Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199 (2013) 3
27. Tram`er, F., Kurakin, A., Papernot, N., Goodfellow, I., Boneh, D., McDaniel, P.: Ensemble adversarial training: Attacks and defenses. arXiv preprint arXiv:1705.07204 (2017) 3, 10
28. Wang, S., Wang, X., Zhao, P., Wen, W., Kaeli, D., Chin, P., Lin, X.: Defensive dropout for hardening deep neural networks under adversarial attacks. In: Proceedings of the International Conference on Computer-Aided Design. pp. 1­8 (2018) 8
29. Wu, L., Zhu, Z., Tai, C., others: Understanding and enhancing the transferability of adversarial examples. arXiv preprint arXiv:1802.09707 (2018) 4
30. Xiao, C., Li, B., Zhu, J.Y., He, W., Liu, M., Song, D.: Generating adversarial examples with adversarial networks. arXiv preprint arXiv:1801.02610 (2018) 2, 4
31. Xie, C., Wu, Y., Maaten, L.v.d., Yuille, A.L., He, K.: Feature denoising for improving adversarial robustness. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 501­509 (2019) 3, 10
32. Xie, C., Zhang, Z., Zhou, Y., Bai, S., Wang, J., Ren, Z., Yuille, A.L.: Improving transferability of adversarial examples with input diversity. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 2730­ 2739 (2019) 3, 4, 5
33. Xu, K., Zhang, G., Liu, S., Fan, Q., Sun, M., Chen, H., Chen, P.Y., Wang, Y., Lin, X.: Adversarial t-shirt! evading person detectors in a physical world. In: European Conference on Computer Vision. pp. 665­681. Springer (2020) 3

18

P.Xie, L.Wang et al.

34. Zou, J., Pan, Z., Qiu, J., Liu, X., Rui, T., Li, W.: Improving the Transferability of Adversarial Examples with Resized-Diverse-Inputs, Diversity-Ensemble and Region Fitting. In: European Conference on Computer Vision. pp. 563­579. Springer (2020) 6

