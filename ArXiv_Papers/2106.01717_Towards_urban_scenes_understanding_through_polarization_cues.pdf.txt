Noname manuscript No. (will be inserted by the editor)
Towards urban scenes understanding through polarization cues
Marc Blanchon1 · D´esir´e Sidib´e2 · Olivier Morel1 · Ralph Seulin1 · Fabrice Meriaudeau1

arXiv:2106.01717v1 [cs.CV] 3 Jun 2021

Received: date / Accepted: date

Abstract Autonomous robotics is critically affected by the robustness of its scene understanding algorithms. We propose a two-axis pipeline based on polarization indices to analyze dynamic urban scenes. As robots evolve in unknown environments, they are prone to encountering specular obstacles. Usually, specular phenomena are rarely taken into account by algorithms which causes misinterpretations and erroneous estimates. By exploiting all the light properties, systems can greatly increase their robustness to events. In addition to the conventional photometric characteristics, we propose to include polarization sensing.
We demonstrate in this paper that the contribution of polarization measurement increases both the performances of segmentation and the quality of depth estimation. Our polarimetry-based approaches are compared here with other state-of-the-art RGB-centric methods showing interest of using polarization imaging.
Keywords Scene Understanding · Polarization · Deep Learning · Computer Vision
1 Introduction
Joint research in robotics and computer vision is striving to develop approaches achieving machine autonomy. Since there is a great diversity of phenomena in nature, the understanding of urban scenes is very challenging. To reach a form of genericity, the community
Marc Blanchon E-mail: fr.marc.blanchon@gmail.com 1EMR VIBOT CNRS 6000, ImViA, Universit´e Bourgogne Franche-Comt´e, 71200, Le Creusot, France 2IBISC, Univ Evry, Universit´e Paris-Saclay, 91025, Evry, France

tends to use massive datasets in combination with deep learning networks. Taking advantage of the abstraction and categorization capabilities of such models, one can learn intermediate representations allowing image understanding.
Therefore, semantic pixel segmentation and monocular depth estimation contribute to the goal of bringing cognition to robots. While one allows the categorization of images into semantic classes at the pixel level, the other estimates distance between sensor and objects. These two applications are crucial since they provide knowledge to the systems and finally define their behavior in front of the surrounding conditions. Traditionally, these techniques tend to allow obstacle recognition and ultimately influence other autonomy factors such as path planning (B. Chen, Gong, and Yang, 2018; Ha et al., 2017; Zhan et al., 2018).
However, the vast majority of approaches rely on feature learning by extracting characteristics from color images. This implies a strong bias since color information is not sensitive to specific phenomena like specularity or transparency. Moreover, the scenes observed in urban environments are prone to this kind of light behavior. For example, when the weather conditions are not optimal (i.e. rain, bright sun, etc.), the color sensing will repeatedly observe saturation or reflections. Thus, through many models designed based on this modality, these aspects are neglected.
Based on this observation, we propose to use the polarization information to define scenes in another space based on physics. Since this information is by definition descriptive of light interaction, we propose two approaches, successively segmentation and depth estimation, relying on these indices to take into account these phenomena. Ultimately, this will allow autonomous robots to be robust to specular behavior.

2

Marc Blanchon et al.

We therefore propose to use polarization cues and 2.1 Polarimetry

an advantageous image representation to train a convo-

lutional network to obtain accurate segmentation. Ad-

Polarization (Collett, 2005) is a non-conventional

dressing the data as the main element of the approach, image space describing the interaction of light with

we proceed to the creation of an augmentation proce- surfaces. Where colorimetry allows to characterize tex-

dure adequate to the physics of the scenes. Without tures, polarimetry acquires the information of reflected

altering the modality properties, we observe notable waves through polarizers. Indeed, the sensor, shown in

capabilities to differentiate specular areas from urban Figure 1, is composed of a multitude of 2x2 grid micro-

scenes. Our evaluation emphasizes the possibility for polarizers at different orientations.

networks to learn new information and to understand

specularity. Thus, we observe better performances when

the system observes complex scenes including particu-

Sensor

lar light phenomena.

In a second step, we propose a cost function to in0°
fer an accurate depth map from polarization images.

Differentiating the non Lambertian areas, we propose 45°
to regularize their surface normals using polarimetric

features. While this has been a major problem with
90°
RGB-centric techniques, we show that the use of this

unconventional modality encourages the reconstruction

of reflective surfaces. A quantitative evaluation then

135°

shows a better robustness to recurrent events in the Fig. 1 Illustration of a polarimetric camera sensor highlightautonomous vehicle domain such as car reflection and ing the micro-grid with polarizers at different orientations.

windows specularity. It would then be more feasible to

refer to reliable maps that take into account common

scenarios, to consider path planning.

This division of focal plane (DoFP) technology al-

In summary, we make the following contributions: lows the separation of the different components of the

(i) a supervised framework for learning segmentation of received light. Thus, as shown in Figure 2, the reflected

specular areas through a small amount of polarimetric wave is differentiated and consequently, the per-pixel

data; (ii) a novel polarization cues-based loss for depth information is influenced by the orientation of the af-

map estimation; (iii) two monocular specularity invari- fixed polarizer.

ant approaches to understand urban scenes in realistic

conditions.

2 Related Works Humans have an innate ability to understand their

Linearly Polarized Light

Unpolarized Light

surrounding scenes. Despite the presence of specular ar-

Sensor

eas, we are able to overlook their effects. Autonomous

systems aspire to the same cognitive abilities and learning-

based algorithms tend to increase their perceptive ca-

45 Linear Polarizer

pacities. However, the approaches rely on visual cues Fig. 2 Illustration of a polarizer filtering of unpolarized light.

that do not constrain some complex problems. Thus,

many methods are RGB-centric and propose to perform

segmentation or monocular depth estimation based on

The sensor technology then produces sparse images

color cues only. As a consequence, the modality-related per orientation. While previously this type of image re-

inadequacies and flaws affect the different estimation quired thoughtful interpolation (Li et al., 2019; Ratliff,

models. Consequently, we propose the alternative of LaCasse, and Tyo, 2009; J. Zhang et al., 2018) that

using physics-based vision and specifically polarization did not alter the image integrity, today's sensors have a

sensing. In the following subsections, we will start by permissive resolution resulting in neglecting this spar-

introducing polarization principles, then describe some sity effect. Eventually, from these raw images, one can

previous works about scene segmentation and depth es- extract four images reflecting the orientation of each

timation.

micro-polarizer P{0, 45, 90, 135}. Ultimately, these four

Towards urban scenes understanding through polarization cues

3

components allow to describe the polarization of light enable scene understanding tasks such as segmentation

through the following Stokes(Stokes, 1851) vector:

and monocular depth estimation.

ÜS0ê Ü P0 + P90 ê

S=

S1 S2

=

P0 - P90 P45 - P135

.

(1)

S3

0

And thanks to a projection in a spherical representation on the Poincar´e sphere, one can extract three characteristic images of the polarization:

 

=

S0







 

=

1 2

arctan

S2 S1

,

(2)

 



   =

S12 +S22 S0

where  is the grayscale intensity,  the angle of polarization identifying the orientation of the polarized light with regards to the incident plan and  the degree of polarization emphasizing the quantity or the strength of this aforementioned light. Using these different parameters, it is possible to notice the singular link between polarimetric information and surface. This has been defined through Fresnel's equations (Fresnel, 1868) defining the relationship between light and material. Therefore, several authors have taken advantage of this particular information for 3D reconstruction through the Structure from Polarization framework (Cui et al., 2017; Morel et al., 2006). Other works tried to improve the methods of perspective reconstruction by inducing polarization indices (Berger, Voorhies, and Matthies, 2017). However, these methods are often effective in constrained environments or with significant prior knowledge. Finally, despite a more differentiable scene description, the use of such a modality remains governed by the constraining Fresnel equations.
However, this relationship implies a strong ability to categorize scenes and differentiate the different surfaces in a scene. While color based techniques only use the scalar value of the incoming light, polarization imaging add the vectorial aspect of it. In addition, while an RGB-centric system will observe saturation and artifacts in front of a specular phenomenon, polarization imaging will better define it. Ultimately, in urban scenes very prone to reflective surfaces, the characterization of such occurrences may benefit the system. We therefore propose to evaluate the possibilities of polarization imaging to operate in unconstrained environments, and rely on the remarkable capabilities of deep learning to either learn polarization parameters or abstract Fresnel equations. This is the basis for the use of polarimetry to

2.2 Segmentation
Semantic segmentation is a predominant domain of computer vision and scene understanding. It consists in assigning a label to every pixel of the image. The usual approach consists in providing a massive annotated database and relying on a deep learning network to learn a model infering the semantics from the input image. Although this methodology has been intensively investigated, the vast majority of the contributions rely on color data sets which implies a textural understanding of the images. Thus, networks such as FCN (Long, Shelhamer, and Darrell, 2015), SegNet (Badrinarayanan, Kendall, and Cipolla, 2017), DilatedNet (Yu and Koltun, 2015) or DeepLab (P.-Y. Chen et al., 2019) have been built to infer a segmentation map. Since the models only learn from the data provided to them, color is the discriminating element in these models. However, the community has tended to increase the complexity of the networks to scale the models and make them more robust. It is seldom unconventional information such as polarization is exploited to constrain the problem upstream.
Fig. 3 Segmentation results from (Zhou et al., 2018) showing erroneous estimation on specular surfaces.
As can be seen in the example of Figure 3, using RGB data can lead to wrong segmentation especially in specular areas. It is therefore important to constrain the problem by using a representation space where object differentiation is facilitated. In this paper, we propose a data-driven segmentation method. Since polarization allows for the physical definition of specular objects, this modality is a potential candidate for more robust feature inference.
2.3 Depth Estimation
Depth estimation represents another essential aspect of scene understanding. Defining the distance between the sensor and the object at any point remain

4

Marc Blanchon et al.

a recurrent challenge in computer vision. Such infor-

mation can ensure the integrity of autonomous systems

during their motion by basing decision making on the

spacing of obstacles.

Initially, the depth maps were not estimated but

rather acquired by means of LiDaR which projects a laser or depth cameras relying on a pattern projection. However, the community has focused on the prob-

Fig. 4 Erroneous depth estimation along specular surfaces. Image borrowed from state-of-the-art method (Godard et al., 2019).

lem of estimation using an acquisition system with one

or more cameras. Multiple views (Favaro and Soatto, 2005; Parashar, Pizarro, and Bartoli, 2016) or stereo approaches (Gennert, 1988; Se, Lowe, and Little, 2001; Woodford, 2008) have been used for depth estimation. However, all the stated approaches have drawbacks. The laser-dependent LiDaR is not robust to meteoro-

information is sensitive to these occurrences, it would be more adequate to characterize these neglected phenomena. As follows, an inference from polarization could allow autonomous systems to be more robust to such features.

logical elements such as rain and fog or non-Lambertian

surfaces. Thus, the robustness of such an acquisition 3 Polarization-based Segmentation process can be altered by specular surfaces. Projected

patterns require favorable conditions and limited distances for optimal observation while image processing based estimation methods can be non-robust and require strong configuration requirements or assumptions such as close distance or limited brightness.

Based on the observation that a large number of surfaces present in urban scenes are specular, we proposed to use polarization to characterize these phenomena. In other words, since color imagery does not physically define these occurrences, the concept is to use an im-

Over the years, the community has therefore turned age space that facilitates the understanding of these

to learning-based approaches to alleviate the constraints. elements.

While the initial approaches required acquisition to su-

As stated in Section 2.1, starting from a raw po-

pervise the algorithms (Eigen, Puhrsch, and Fergus, larimetric image, one can extract four sparse images

2014; Liu, Shen, and Lin, 2015; Roy and Todorovic, corresponding to the intensity for each polarizer orien-

2016), recent methods generalize a model by means of tation. The densification being performed by a bilinear

a loss to estimate a disparity map based on physical interpolation, three characteristic images ,  and  are

constraints. Thus, borrowing from the perspective ge- obtainable through the Stokes vector. As a result, these

ometry formulation, many networks have demonstrated three information allow for definition of the polariza-

an ability to estimate dense and robust depth maps in tion states of the objects and therefore define specular-

an unsupervised manner.

ity phenomenon. Since deep learning approaches have

Using a generative adversarial network (Mehta, Sakurikabr,een shown to be effective in segmenting images, we

and Narayanan, 2018), recurrent online learning (Casser require a representation to enable polarization encoded

et al., 2019), 3D motion modeling (Luo et al., 2019), op- images to be used by such architectures.

tical flow estimation (Ranjan et al., 2019) or even find-

While most of the deep learning based algorithms

ing levers to neglect occlusion (Godard et al., 2019), a relies on 3-channel RGB images, this image format does

large number of methods have successively shown it is not suit the three parametric information described above.

possible to infer a depth map from a single camera sys- Indeed, while the intensity  and the degree of polar-

tem. This kind of approach is easily exportable to the ization  could accommodate such representation, the

autonomous vehicle domain since, in addition to being angle of polarization  represents an orientation and

robust, it does not complicate the acquisition system. such bounding would reduce the contained information.

On the other hand, most of the networks have been Different image representations have been proposed for

trained on KITTI (Geiger, Lenz, and Urtasun, 2012) polarimetric imaging but Wolff and Andreou (Wolff and

and only use this dataset and the attached depth as a Andreou, 1995) presented a singular modeling based on

benchmark. The favorable conditions of these images the Hue-Saturation-Luminace (HSL) format. Instead of

rarely present surfaces that could be problematic to a trying to find an optimal ratio between the compo-

sensor not sensitive to physics. Finally, as shown in Fig- nents, they chose not to compress or reduce the origi-

ure 4, the robustness to specular areas is unproven.

nal bounds of ,  and . The HSL model is composed

Consequently, we propose using the polarimetric data of three interconnected channels with different limits.

to consider both diffuse and specular areas. Since this Thus, S and L vary in the range [0, 100] while H is a

Towards urban scenes understanding through polarization cues

5

periodic value that varies in the range [0, 360]. Interestingly, this model accommodates the polarimetric characteristic images. In addition, such mapping allow for a particular representation, since the hue corresponds to the color shade, the saturation to the strength of the shade and the luminance to the texture or the contrast. Therefore, as shown in Figure 5, this mapping will propose singular behavior since the more polarized is the reflected wave from an area, the more colored is the area. Additionally, the color will allow a direct recognition of the angle thanks to the 360-periodic value.
Fig. 5 HSL representation of polarization parameters. The ground is colored according to the polarization angle and the intensity of this color is influenced by the degree of polarization.

ing into account the physics induced by the modality or by neglecting it.
3.1 Modality-based evaluation
Using the limited PolaBot1 dataset acquired for comparative learning between modalities, one can train a network for each of the modalities to compare performances on the same scenes (Blanchon, Morel, Y. Zhang, et al., 2019). Furthermore, since SegNet does not embed any attention or contextualization module while being a standard encoder-decoder architecture, this network is the perfect candidate for a fair comparison. The concept of this segmentation network is mainly based on maxpooling with index to extract the principal components at each activation map. The dimension being successively reduced by the different blocks, at the bottleneck, the initial dimension is recovered through indexed unpooling. The spatial information derived from the encoding during the downsampling is thus retranscribed through the indices during the decoding and therefore avoid a naive upsample. Integrating this concept and assuming that all classes are easily distinguishable, we present the training procedure in Figure 6.

Finally, despite a particular unnatural coloration of the images and the mapping of polarimetric information on three-channel image, HSL representation is not adapted for network usage. To benefit from color images pre-training and unnatural but representative image formation, one can transform HSL image to RGB image by converting color information into the corresponding RGB values. Consequently, transfer learning can be performed while preserving discriminating properties of the images induced by sensor.
From this image representation, one can train networks to validate the hypothesis that polarization can improve a network's robustness to specularity.
First, an evaluation will be proposed to compare the results using successively color and polarimetric imaging as input. This benchmark being designed to quantify the different cross-modalities, a SegNet network will be used, while disabling the use of advanced contextualization or learning processes. In a second step, an evaluation will be conducted on the ability of a network to to benefit from the physics of the sensor. Using a state-of-the-art DeepLab v3+ network, we assume that these are the optimal learning conditions. A database can therefore be augmented in multiple manners by tak-

Fig. 6 Two pipeline training procedure. Top pipeline receives polarization image and bottom pipeline color image. Both SegNet encoders are initially pre-trained with ImageNet (Deng et al., 2009) dataset.
Each of the networks is trained following the same image ordering, which reduces the possible differences in training. The database contains 178 annotated images in both image spaces and includes seven classes that are considered to be of primary importance for the field of autonomous robotics. The models are therefore trained to segment Sky, Water, Windows, Road, Cars, Building and None corresponding to other classes. This
1 http://vibot.cnrs.fr/polabot.html

6

Marc Blanchon et al.

two-pipeline procedure allows a quantitative evaluation of the accuracies per class and per modality shown in Table 1.
Ultimately, polarization influenced network shows increased capabilities to recognize very challenging areas like windows and cars. Considering the dataset, the image quantity being not optimal, the networks most likely have over-fitting issues. Nevertheless, we consider that these results are valid since both approaches were subject to the same constraints. However, this kind of network with limited training data cannot be considered as generic and tends to produce erroneous estimates when the images are different from the ones in training set. Overall, polarization based network highlights more robustness which validates the initial hypothesis. While some particular classes are slightly better segmented in color space, one can deduce that the images are advantageous since the color is uniform in the dataset for these specific regions (typically the sky is blue). Following experiment will tackle the task of estimating whether a network is deeply influenced with the physics induced by the modality.
3.2 Physics-based evaluation
When comparing the two modalities, polarization demonstrated increased segmentation capabilities of the areas of interest. We subsequently propose an evaluation addressing the physical properties influence rather than a comparison of intrinsic performance.
In a first step, to have several bases of estimation, one can start from the initial dataset of 178 images and produce two other additional sets using augmention procedures. While one dataset will be subjected to a standard augmentation, the other in addition to the transformation will undergo the regularization necessary to preserve the physical integrity of the images (Blanchon, Morel, Meriaudeau, et al., 2021). As a reminder, polarimetric information is not invariant to pose changes. As shown in Figure 7, a camera rotation, even though the sensor is observing the same surface, will imply a different polarization angle acquisition. This behavior is due to the nature of the polarization angle, which corresponds to the orientation of the electric field E with respect to the incident plane. Therefore, the purpose of regularized augmentation is to simulate a physical movement of the camera to create new photorealistic images.
Using three different datasets from the same images, we can evaluate the networks ability to process these data and take advantage of the physical information. We propose to use DeepLab v3+ (P.-Y. Chen et al., 2019) since it is one of the most efficient networks

Fig. 7 Influence of polarimetric camera rotation on angle of polarization  (in red).
for semantic segmentation. Moreover, this architecture has several essential interests for the task. First, the pooling mechanism conceptualized as ASPP allows an intelligent down-sampling, strongly limiting the effect of discretization of activation maps. This block is also designed to accumulate several receptive fields using atrous convolutions. For semantics, and for our application, this mechanism allows us to include contextual information and thus take advantage of the information contained in more distant neighbor regions compared to standard pooling. A second essential point is the use of residual blocks to avoid vanishing gradient problems. This effect is not negligible since it has a direct impact on polarimetric image segmentation. Indeed, despite the hypothesis made in the previous section, one assumption was the classes were distinguishable. This was valid for a simplification of the problem to compare the modalities. However, in real conditions, discrimination by polarization angle is uncertain since a set of  values do not correspond to a specific class. There is thus a more complex connection to define an object. Ultimately, the angle is periodic and depends implicitly on the camera pose. For all these reasons, we rely on a modern architecture to learn any semantic relationships between classes and polarization parameters and thus reveal whether the network is capable of learning from physical polarimetric information.
Finally, the different augmentation strategies are compared using a six-phase training procedure as shown in Figure 8. The first network is trained with the original 178 images while the other two are trained with 2136 images obtained through standard or polarizationsensitive regularization respectively. For testing, an evaluation datastet that does not replicate the type of scenes used for training is used. The evaluation results are displayed in Table 2. We decided to extract metrics by specific classes since they are of interest for scene understanding applications in urban context. Consequently, we highlight the classes cars, windows and water since they are traditionally difficult to segment using RGBcentric methods.

Towards urban scenes understanding through polarization cues

7

Table 1 Segmentation quantitative results comparing polarimetry and RGB trained models.

Model Polarimetry RGB

@sky
.753 .895

@water
.757 .786

Accuracy

@windows @road @cars

higher is better

.828

.778 .714

.445

.784 .484

@building
.876 .678

@none
.789 .834

mean
.785 .698

Table 2 Quantitative evaluation of Deeplab v3+ specular surfaces segmentations with respect to augmentation procedure and pre-training. Metrics computed excluding building class are denoted \B. Mean metrics are computed on all the seven classes while Mean \B excludes the building class.

Augmentation None
Not regularized Regularized

PreTraining No Yes No Yes No Yes

@water
.40 .54 .001 .10 .63 .70

IoU

@windows @cars Mean

higher is better

.20

.20

.30

.10

.43

.33

.03

.12

.14

.03

.19

.21

.13

.46 .43

.26

.47 .37

Mean \B
.32 .34 .13 .20 .50 .38

@water
.35 .42 .35 .35 .39 .35

Recall

@windows @cars Mean

higher is better

.15

.22 .50

.15

.57

.43

.25

.15

.31

.22

.23

.37

.21

.60 .43

.26

.48

.42

Mean \B
.50 .50 .28 .33 .50 .38

Fig. 8 The six different training strategies for physicsinfusion evaluation. From raw polarimetric image to physics influenced segmentation through different augmentation techniques.

tween color and polarimetric imagery quantified the advantage of polarization in addressing such areas. Despite the limited amount of data, it is clear that this new information can be beneficial to a deep learning network. However, it was impossible to validate if the model had been influenced with the physical properties of the image. In response, we proposed to evaluate this capability by training six models observing data whose physics had been altered or not. This implementation highlighted the ability of the model to learn polarimetric information. Thus, this experimentation emphasizes it is better not to augment than to ignore the properties of the modality during the augmentation process..
In this section, we proposed an end-to-end pipeline for semantic segmentation of urban scenes using polarization that allows the detection of specular areas which are mostly hazardous in such environments and can affect the integrity of autonomous systems. Therefore, taking into account specularity is essential to obtain robust recognition algorithms.
4 Towards Polarimetric Monodepth

3.3 Discussion
This step-by-step experimentation allowed us to evaluate the hypothesis that polarization would provide better capability to characterize urban scenes and especially specular surfaces. First, a fair comparison be-

A second axis of this urban scene understanding framework using polarization is depth estimation. However, contrary to the segmention method described in Section 3 which is supervised, we adapt here an unsupervised approach and use the physics of the modality to provide additional constraint terms enabling accurate scene reconstruction. Since polarimetry character-

8

Marc Blanchon et al.

izes light interaction and specularity effects, a model can be driven by the polarization state of the light (Blanchon, Sidib´e, et al., 2021).
Besides the deep learning approach, Berger et al. (Berger, Voorhies, and Matthies, 2017) have investigated the possibility of constraining the stereo reconstruction problem using polarization priors. Therefore, by using an adapted calibration allowing the direct mapping between polarization angle and orientation of the normal, they demonstrates it is possible to reconstruct highly specular surfaces. Although the process adds a rectification term for these areas, the acquisition process is too restrictive to be operable in real conditions. For this reason, we propose several terms to impact the depth estimation of reflective objects operable in realworld condition. We take advantage of the abstraction capabilities of deep learning model networks to neglect the consideration of Fresnel equations governing relation between light and materials.
To constitute a new penalty term representing the difference between polarization angle and orientation of the normal, one can define the relation that links them such that:

Ultimately, the electric field represents the vector perpendicular to both Rw and the surface normal n. Considering two formulations, it is possible to define E angle with regard to incident plane according to:

(Er ) =  PQ × RQ × Rw ,

(4)

or, if we neglect the impact of the reflected wave and consider each points to be aligned with the optical center:



(Ea) =  PQ × RQ

± 2

mod .

(5)

Note, Ea and Er are respectively the approximated or the real electric field. Consequently, projecting this obtained vector onto the image plane, one can deduce a bounded angular error A (represented in Figure 10) such that:

A = tan tan-1(E ) -  .

(6)

 = tan-1(E ),

(3)

with E the electric field defined as a perpendicular vector with regard to the normal for specular surfaces and () the image plane reprojection operation. As shown in Figure 9, from three 3D-projected point belonging to the same surface, one can deduce the surface normal.

Fig. 10 Representation of angular error on image plane.

Using absolute tangent allow shifting from an angular radian error into a [0, [ interval error. In addition, E can be estimated following either equation 4 or equation 5 and consequently imply A to be either exact or approximated. Because of the polarization properties, this formulation is valid iff the observed surface is specular. Since we do not observe any specularity index, we decide to use the degree of polarization to define the nature of the objects. Therefore, one can define an empirical threshold of 0.4 deemed sufficient descriptor to validate the formulation. Additionally, to quantify and scale the error, we propose to use this truncated polarization degree to complete this angle difference penalty term as follows:

Fig. 9 Electric field estimation from three neighbor points

in 2D image plane.

Lp = Ar ,

(7)

Towards urban scenes understanding through polarization cues

9

or when using electric field orientation approximation:

Lp

=



min(|A+a

 2

|,

|A-a

 2

|).

(8)

With  the degree of polarization, this regularization term allows polarization to be taken into account but it remains to comply with standard methods including reprojection and smoothness. The error based on the perspective geometry while taking into consideration the occlusion are those borrowed from (Godard et al., 2019):

Lr = min pe(It , It t ),

(9)

t

with It t the reprojected second image onto the source image plane and pe the reconstruction error:

 pe(Ia, Ib) = 2 (1 - SSIM(Ia, Ib)) + (1 - )||Ia - Ib||1. (10)
This term consider both the photometric error by considering dissimilarity measure and L1 norm while the min reduces image boundaries artifacts. Finally, smoothness term consists in densifying disparity map, smoothing surfaces and preserving salient edges. Contrarily to Godard et al., we propose using the second order smoothness which validity has been proven in (Woodford, 2008). Embedding edge preserving term, we define:

Ls = |x2dt|e-|x2It | + |y2dt|e-|y2It |,

(11)

with dt = dt /d¯t the mean-normalized inverse depth and 2 the second order derivative. Accumulating the
three term, the general loss can be defined as:

 = (µLr ) + Ls + Lp,

(12)

with µ the binary mask considering occlusion and object stationary behavior and ,  and  three empirical scaling factors managing the impact of each term onto the final reconstruction.
 being defined, we define the training framework allowing a comparison between networks trained with the different proposed losses and the state-of-the-art method. As shown in Figure 11, multiple network have been trained to compare our methods.
We choose to evaluate the initial network of Godard et al. with a concatenation of polarimetric intensity images {, , } and to train 5 networks, three of which are end-to-end with polarization parameters concatenation

Fig. 11 Training procedure for evaluation. Every trained network embed pose estimation through PoseCNN. The indices r and e refers respectively to real and approximate of the electrical field.
{, , }. Since it has been validated network are able to take advantage of transfer learning, we decided to evaluation two networks while pre-training the ResNet 50 encoder with (Godard et al., 2019). Ultimately, Table 3 display quantitative results for each methods.
Polarization image allows to infer directly on the normal to the planes. Constraining this problem by considering only the specular surfaces, an angular error can be formulated as a minimizable term of the overall reconstruction loss. We have proposed several methods to evaluate the performance based on a pre-training and/or an approximate electric field estimate. Realistically, the consideration of the reflected wave Rw is complex and involves computations in 3D space leading to the accumulation of uncertainties. However, the strategy considering that the electric field E is always perpendicular to both reflected wave and the normal for specular surfaces does not allow to observe the same robustness despite the simplicity of this formulation. We also proposed to evaluate our methods in competition with the original method of Godard et al. (ibid.) by using only an intensity concatenation to mimic the RGB. This approach allows a reliable comparison through pixel alignment since  comes from the acquisition but one can consider the network is handicapped by the absence of color. Ultimately, we proposed to train a network with the concatenation of the polarimetric infor-

10

Marc Blanchon et al.

Table 3 Quantitative evaluation of different depth estimation networks. G {, , } is network proposed in (Godard et al., 2019) and P2D G stands for end-to-end training using Godard et al. loss, hence, no polarimetric regularization. In addition p
denotes pre-trained and a and r represent respectively approximated and real electric field estimation.

Abs Rel Sq Rel RMSE RMSE log

 > 1.25  > 1.252  > 1.253

Type

Network

lower is better

higher is better

G {, , }

0.471 10.809 25.161

0.680

0.485

0.707

0.804

P2D G

0.482 9.144 22.332

0.617

0.431

0.695

0.838

Raw

P2D pa P2D a P2D pr

0.512 0.555 0.416

4.930 5.267 4.920

19.611 18.447 20.354

0.701 0.643 0.977

0.358 0.363 0.401

0.611 0.619 0.668

0.755 0.776 0.816

P2D r

0.355 3.857 17.657 0.408

0.485

0.779

0.888

G {, , }

0.533 14.050 29.312

0.780

0.449

0.658

0.771

P2D G

0.415 11.247 25.899

0.678

0.467

0.729

0.850

Cropped

P2D pa P2D a P2D pr P2D r

0.413 0.323 0.403 0.280

6.054 5.706 6.018 4.650

22.360 21.040 23.516 20.441

0.654 0.508 0.766 0.475

0.403 0.481 0.362 0.524

0.684 0.767 0.629 0.794

0.836 0.865 0.786 0.911

G {, , }

0.341 8.249 7.236

0.306

0.666

0.808

0.896

P2D G

0.208 2.248 5.491

0.233

0.639

0.877

0.952

Specular

P2D pa P2D a P2D pr P2D r

0.210 0.189 0.207 0.146

0.434 0.542 0.402 0.231

1.071 1.348 1.290 0.897

0.271 0.231 0.314 0.193

0.714 0.768 0.675 0.760

0.867 0.914 0.864 0.941

0.952 0.951 0.924 0.983

mation {, , } while neglecting the polarimetric term 5 General Discussion

of the loss to evaluate whether the other methods were

valid or Lp term did not contaminate the network.

We proposed a combination of scene understanding

To assess the methods, we propose to use three strate- approaches from a single raw polarimetric image. To

gies consisting in comparing the whole images, the cropped demonstrate the contribution of polarization indices to

images and only the specular areas. We justify cropping the definition of urban scenes, we first proposed a naive

since the maps deduced from polarization tend to pro- approach to segmentation involving polarization. This

duce short range artifacts. We assume that this is due initial method emphasized the usefulness of an uncon-

to two factors, the proximity of the pixels implies dras- ventional modality sensitive to the polarization state

tic changes in polarization angles and the polarimetric of the light, especially for segmenting specular areas.

information is not invariant to pose. Since the acqui- In a second step, we closed the segmentation line by

sition conditions of the test dataset are not identical verifying the ability of the networks to learn physical

to the training dataset, then this phenomenon can be properties. Using augmentation, this highlighted that

explained, but it is indicative of a lack of genericity of the networks could benefit from this information, but

the model.

also that the augmentation had to be in agreement with

the modality. Building on the conclusions drawn from

To conclude briefly, P2D r without pre-training and with a loss dependent on Rw gives the best results and proves that, firstly, the network can be infused with

the segmentation approaches, we proposed a depth estimation pipeline with a single view. This time, since it is difficult to acquire robust ground truth depth maps,

the physics of imaging, and secondly, the network strug- we employed a self-supervised approach. Constraining

gles to change domain when using this kind of loss.

the angle divergence of the normals to the specular sur-

Towards urban scenes understanding through polarization cues

11

faces, we demonstrated that we could use polarization to obtain an accurate reconstruction. With the aim of overcoming color-based methods weaknesses, we show depth estimation on specular surfaces is improved.
Finally, these different approaches allow us to develop a double channel method of scene understanding from a polarimetric monocular image shown in Figure 12. Ultimately, a qualitative evaluation is shown in Figure 13.

first segmentation pipeline that defines specular surfaces in a more optimal way. Of course, the robustness of such a network is not proven due to the dataset, but, as a proof of concept, this allowed us to show that deep learning based methods can benefit from polarization for a more robust urban scene characterization.
In a second step, we proposed a loss regularization term to refine the depth maps from the so-called monodepth networks. Taking advantage of the polarization indices, we have defined several functions and approaches to evaluate the validity of this data for the task. Therefore, we have shown an increased robustness for the estimation of urban scenes and specifically for specular surfaces. However, it is notable that the estimates can be erroneous and thus the method is not fully robust. As a proof of concept, we have tried to improve the process in a joint training manner. This approach greatly facilitates the training process but tends to negatively impact the estimation process using only photometric loss. This is due to the accumulation of terms that may conflict and impact the overall quality of the network training. One technique would be to use modern fusion processes to improve the depth maps without altering the prior RGB-centric network estimates.
Finally, this paper proposes an initial step towards urban scenes understanding using polarization. The different experiments have proven the interest of this physicsbased modality to characterize complex scenes, especially reflective areas.
A major advance would be the acquisition of a large annotated polarimetric dataset, which would allow for a deeper analysis and a fair and viable comparison.

Fig. 12 Two-sided scene understanding pipeline through polarization cues.

Acknowledgements This work was supported by the French National Research Agency through ANR ICUB (ANR-17CE22-0011). We gratefully acknowledge the support of NVIDIA Corporation with the donation of GPUs used for this research.

6 Conclusion
In this paper, we proposed a first step towards a robust characterization of urban scenes through polarization. Constrained by the amount of information available, we have developed both segmentation and monodepth approaches to show the interest of including polarization cues into these algorithms. It is notable that the data is critical in any domain but specifically in unconventional imaging since it is scarce. The methods using this kind of information are then at a disadvantage and tend to be neglected whereas they have a more interesting description capability. We have proposed a

Conflict of interest
The authors declare that they have no conflict of interest.
References
Badrinarayanan, Vijay, Alex Kendall, and Roberto Cipolla (2017). "Segnet: A deep convolutional encoder-decoder architecture for image segmentation". In: IEEE transactions on pattern analysis and machine intelligence 39.12, pp. 2481­2495.

12

Marc Blanchon et al.

Fig. 13 Qualitative evaluation of two-sided scene understanding pipeline using polarization.

Berger, Kai, Randolph Voorhies, and Larry H Matthies (2017). "Depth from stereo polarization in specular scenes for urban robotics". In: Robotics and Automation (ICRA), 2017 IEEE International Conference on. IEEE, pp. 1966­1973.
Blanchon, Marc, Olivier Morel, Fabrice Meriaudeau, et al. (2021). "Polarimetric image augmentation". In: 2020 25th International Conference on Pattern Recognition (ICPR). IEEE, pp. 7365­7371.
Blanchon, Marc, Olivier Morel, Yifei Zhang, et al. (2019). "Outdoor Scenes Pixel-wise Semantic Segmentation using Polarimetry and Fully Convolutional Network." In.
Blanchon, Marc, D´esir´e Sidib´e, et al. (2021). "P2D: a self-supervised method for depth estimation from polarimetry". In: 2020 25th International Confer-

ence on Pattern Recognition (ICPR). IEEE, pp. 7357­ 7364. Casser, Vincent et al. (2019). "Depth prediction without the sensors: Leveraging structure for unsupervised learning from monocular videos". In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. 01, pp. 8001­8008. Chen, Bike, Chen Gong, and Jian Yang (2018). "Importanceaware semantic segmentation for autonomous vehicles". In: IEEE Transactions on Intelligent Transportation Systems 20.1, pp. 137­148. Chen, Po-Yi et al. (2019). "Towards scene understanding: Unsupervised monocular depth estimation with semantic-aware representation". In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2624­2632.

Towards urban scenes understanding through polarization cues

13

Collett, Edward (2005). "Field guide to polarization".

pervised monocular depth estimation". In: 2018 In-

In: Spie Bellingham, WA.

ternational Conference on 3D Vision (3DV). IEEE,

Cui, Zhaopeng et al. (2017). "Polarimetric multi-view

pp. 314­323.

stereo". In: Proc. of Computer Vision and Pattern Morel, Olivier et al. (2006). "Active lighting applied to

Recognition (CVPR).

three-dimensional reconstruction of specular metal-

Deng, J. et al. (2009). "ImageNet: A Large-Scale Hier-

lic surfaces by polarization imaging". In: Applied op-

archical Image Database". In: CVPR09.

tics 45.17, pp. 4062­4068.

Eigen, David, Christian Puhrsch, and Rob Fergus (2014). Parashar, Shaifali, Daniel Pizarro, and Adrien Bartoli

"Depth map prediction from a single image using a

(2016). "Isometric non-rigid shape-from-motion in

multi-scale deep network". In: Advances in neural

linear time". In: Proceedings of the IEEE Confer-

information processing systems, pp. 2366­2374.

ence on Computer Vision and Pattern Recognition,

Favaro, Paolo and Stefano Soatto (2005). "A geometric

pp. 4679­4687.

approach to shape from defocus". In: IEEE Trans- Ranjan, Anurag et al. (2019). "Competitive collabora-

actions on Pattern Analysis and Machine Intelli-

tion: Joint unsupervised learning of depth, camera

gence 27.3, pp. 406­417.

motion, optical flow and motion segmentation". In:

Fresnel, Augustin (1868). Oeuvres compl`etes d'augustin

Proceedings of the IEEE Conference on Computer

Fresnel. Imprimerie imp´eriale.

Vision and Pattern Recognition, pp. 12240­12249.

Geiger, Andreas, Philip Lenz, and Raquel Urtasun (2012). Ratliff, Bradley M, Charles F LaCasse, and J Scott Tyo

"Are we ready for Autonomous Driving? The KITTI

(2009). "Interpolation strategies for reducing IFOV

Vision Benchmark Suite". In: Conference on Com-

artifacts in microgrid polarimeter imagery". In: Op-

puter Vision and Pattern Recognition (CVPR).

tics express 17.11, pp. 9112­9125.

Gennert, Michael A (1988). "Brightness-based stereo Roy, Anirban and Sinisa Todorovic (2016). "Monocu-

matching". In: ICCV.

lar depth estimation using neural regression forest".

Godard, Cl´ement et al. (2019). "Digging into self-supervised In: Proceedings of the IEEE conference on computer

monocular depth estimation". In: Proceedings of the

vision and pattern recognition, pp. 5506­5514.

IEEE International Conference on Computer Vi- Se, Stephen, David Lowe, and Jim Little (2001). "Vision-

sion, pp. 3828­3838.

based mobile robot localization and mapping us-

Ha, Qishen et al. (2017). "MFNet: Towards real-time

ing scale-invariant features". In: Proceedings 2001

semantic segmentation for autonomous vehicles with

ICRA. IEEE International Conference on Robotics

multi-spectral scenes". In: 2017 IEEE/RSJ Interna-

and Automation (Cat. No. 01CH37164). Vol. 2. IEEE,

tional Conference on Intelligent Robots and Systems

pp. 2051­2058.

(IROS). IEEE, pp. 5108­5115.

Stokes, George Gabriel (1851). "On the composition

Li, Ning et al. (2019). "Demosaicking DoFP images us-

and resolution of streams of polarized light from dif-

ing Newton's polynomial interpolation and polar-

ferent sources". In: Transactions of the Cambridge

ization difference model". In: Optics express 27.2,

Philosophical Society 9, p. 399.

pp. 1376­1391.

Wolff, Lawrence B and Andreas G Andreou (1995).

Liu, Fayao, Chunhua Shen, and Guosheng Lin (2015).

"Polarization camera sensors". In: Image and Vi-

"Deep convolutional neural fields for depth estima-

sion Computing 13.6, pp. 497­510.

tion from a single image". In: Proceedings of the Woodford, O J (2008). "Global Stereo Reconstruction

IEEE conference on computer vision and pattern

under Second Order Smoothness Priors". In: IEEE

recognition, pp. 5162­5170.

transactions on pattern analysis and machine intel-

Long, Jonathan, Evan Shelhamer, and Trevor Darrell

ligence 31.12, pp. 2115­2128.

(2015). "Fully convolutional networks for semantic Yu, Fisher and Vladlen Koltun (2015). "Multi-scale con-

segmentation". In: Proceedings of the IEEE confer-

text aggregation by dilated convolutions". In: arXiv

ence on computer vision and pattern recognition,

preprint arXiv:1511.07122.

pp. 3431­3440.

Zhan, Huangying et al. (2018). "Unsupervised learning

Luo, Chenxu et al. (2019). "Every pixel counts++: Joint

of monocular depth estimation and visual odometry

learning of geometry and motion with 3d holistic

with deep feature reconstruction". In: Proceedings

understanding". In: IEEE transactions on pattern

of the IEEE Conference on Computer Vision and

analysis and machine intelligence 42.10, pp. 2624­

Pattern Recognition, pp. 340­349.

2641.

Zhang, Junchao et al. (2018). "Sparse representation-

Mehta, Ishit, Parikshit Sakurikar, and PJ Narayanan

based demosaicing method for microgrid polarime-

(2018). "Structured adversarial training for unsu-

14

Marc Blanchon et al.

ter imagery". In: Optics letters 43.14, pp. 3265­ 3268. Zhou, Bolei et al. (2018). "Semantic understanding of scenes through the ade20k dataset". In: International Journal on Computer Vision.
Marc Blanchon is a thirdyear Ph.D Student in Universit´e Bourgogne Franche-Comt´e in the EMR VIBOT CNRS 6000 team. He recieved a Bachelor's degree and a Master's degree both in Computer Vision from Universit´e Bourgogne Franche-Comt´e in 2016 and 2018 respectively. His main research interests are computer vision for autonomous vehicles and physics-based deep learning.
D´esir´e Sidib´e received a Master degree from Ecole Centrale de Nantes and a Ph.D from Universit´e de Montpellier in 2004 and 2007 respectively. From 2009 to 2019, he was an assistant, then associate professor at Universit´e de Bourgogne and a member of the Le2i laboratory. Since 2019, he is a professor at Universit´e Paris Saclay and a member of the IBISC laboratory. His main research areas include computer vision for autonomous vehicles, and medical image analysis. He has authored and co-authored more than 80 papers in international journals and conferences. He is a senior member of IEEE.
Olivier Morel received his MSc degree in computer vision and image processing from the University of Burgundy in 2002. In November 2005, he received his PhD degree in computer vision from the University of Burgundy. Since September 2007, he has worked as a lecturer in the VIBOT Team EMR CNRS 6000 from University of Bourgogne FrancheComt´e. His main research interests are polarization imaging and the applications of polarimetric vision to robotics and unmanned vehicles.

Ralph Seulin is a research engineer at CNRS in EMR VIBOT CNRS 6000 team. In 2002, he received his Ph.D degree in computer vision from the University of Burgundy. His main research areas include robotics, and computer vision for autonomous vehicles. He has authored and co-authored more than 80 international publications.
Fabrice Meriaudeau received both the master degree in physics at Dijon University, France as well as an Engineering Degree (FIRST) in material sciences in 1994. He also obtained a Ph.D. in image processing at the same University in 1997. He was a postdoc for a year at The Oak Ridge National Laboratory. He is currently Professeur des Universit´es at the University of Burgundy. He was the director of the Institute Health and Analytics (2017/2018) at the Universiti Teknologi PETRONAS Malaysia and was the Director of the Le2i (UMR CNRS) France, which has more than 200 staff members, from 2011 to 2016. His research interests were focused on image processing for non-conventional imaging systems (UV, IR, polarization) and more recently on medical/biomedical imaging. He coordinated an Erasmus Mundus Master in the field of Computer Vision and Robotics from 2006 to 2010 and was the Vice President for International Affairs for the University of Burgundy from 2010 to 2012. He has authored and coauthored more than 150 international publications and holds three patents.

