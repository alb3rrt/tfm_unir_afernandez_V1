DISTRIBUTED CONTROL-ESTIMATION SYNTHESIS FOR STOCHASTIC MULTI-AGENT SYSTEMS VIA VIRTUAL INTERACTION BETWEEN NON-NEIGHBORING AGENTS

arXiv:2106.00961v1 [eess.SY] 2 Jun 2021

Hojin Lee Department of Mechanical Engineering Ulsan National Institute of Science and Technology
Ulsan, 44919 Repulic of Korea hojinlee@unist.ac.kr

Cheolhyeon Kwon Department of Mechanical Engineering Ulsan National Institute of Science and Technology
Ulsan, 44919 Repulic of Korea kwonc@unist.ac.kr

ABSTRACT
This paper considers the optimal distributed control problem for a linear stochastic multi-agent system (MAS). Due to the distributed nature of MAS network, the information available to an individual agent is limited to its vicinity. From the entire MAS aspect, this imposes the structural constraint on the control law, making the optimal control law computationally intractable. This paper attempts to relax such a structural constraint by expanding the neighboring information for each agent to the entire MAS, enabled by the distributed estimation algorithm embedded in each agent. By exploiting the estimated information, each agent is not limited to interact with its neighborhood but further establishing the `virtual interactions' with the non-neighboring agents. Then the optimal distributed MAS control problem is cast as a synthesized control-estimation problem. An iterative optimization procedure is developed to find the control-estimation law, minimizing the global objective cost of MAS.
Keywords Distributed control · Distributed estimation · Optimal control
1 Introduction
Distributed control within a cooperative multi-agent system (MAS) is the key enabling technology for different networked dynamical systems [Shi and Yan, 2020, Li et al., 2019, Zhu et al., 2020, Morita et al., 2015]. Notwithstanding diverse distributed control strategies, their optimality is one of the stumbling blocks due to individual agents' limited information. In particular, finding the optimal distributed control with network topological constraint is a well-known NP-hard problem [Gupta et al., 2005]. To ease this problem, some former studies have focused on a specific form of objective function along with certain MAS network topology conditions under which the optimal distributed control laws can be designed [Ma et al., 2015]. More particularly, different techniques have been investigated to design suboptimal distributed control laws for different MAS cooperative tasks [Gupta et al., 2005, Nguyen, 2015, Jiao et al., 2019]. In this paper, a new avenue for accomplishing the optimal distributed control of MAS is presented while not requiring the restricted form of the objective function, nor the network topology. The key idea is to expand the available information for each agent by employing the distributed estimation algorithm, and use the expanded information to relax the network topological constraint in a tractable manner. In a nutshell, the main contributions are the following.
1. A synthesized distributed control-estimation framework is proposed based on the authors' preliminarily developed distributed estimation algorithm [Kwon and Hwang, 2018]. The newly proposed framework enables the interactions between non-neighboring agents, namely virtual interactions.
© 2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. This work was supported by the National Research Foundation of Korea(NRF) grants funded by the Korea government(MSIT) (No. 2020R1A5A8018822 and No. 2020R1C1C1007323

2. With the aid of virtual interactions, a design procedure that solves for the optimal distributed control law of the stochastic MAS over a finite time horizon is developed, which was originally an intractable non-convex problem due to the network topological constraint.

2 Problem Formulation

2.1 Dynamical Model of Stochastic MAS

Consider a stochastic linear multi-agent dynamical system including N homogeneous agents whose dynamics is given

by:

xi(t + 1) = Axi(t) + Bui(t) + wi(t), i  {1, · · · , N }

(1)

where xi(t)  Rn and ui(t)  Rp are the state and the control input of the ith agent, respectively. wi(t) is a disturbance imposed on the ith agent, assumed to follow zero-mean white Gaussian distribution with covariance i(t) 0. t  Z+ = {0, 1, · · · } indicates a discrete-time index. A, B are the system matrices with appropriate dimensions, and
are assumed to satisfy the controllability condition. Accordingly, the entire MAS dynamics can be written as

x(t + 1) = A~x(t) + B~u(t) + w~(t)

(2)

A~ = (IN  A) , B~ = (IN  B)
x(t) = xT1 (t) · · · xTN (t) T , u(t) = uT1 (t) · · · uTN (t) T
w~ = w1T(t) · · · wNT (t) T
where  is the Kronecker product between matrices. The interactions between agents are rendered by inter-agent network topology, described by a graph model G consisting of a node set V = {1, 2 . . , N } indexing each agent and an edge set E  V × V indicating the network connectivity between the agents. Each edge (i, j)  E denotes that the node i can acquire the state information of the node j. An adjacency matrix A = [a]  RN×N can express the network connectivity of the graph model, where its element aij = 1 if (i, j)  E, and aij = 0 otherwise. A degree matrix is defined as D = diag(d1 · · · dN ) where di = j aij is (weighted) degree of node i. The Laplacian matrix L, given by L = D - A, is useful for analysis of the network topology. The set of agents whose state information is available to the ith agent, i.e., the neighborhood of the ith agent, is expressed as i, and its cardinality is expressed as |i|. Based on the given network topology, the noisy measurement of neighborhood states {xj(t)|j  V} from the ith agent's perspective can be represented as follows [Kwon and Hwang, 2018]:

zij(t) = cij (xj(t) + vij(t)) , j  V

(3)

where cij indicates the availability of the measurement of the jth agent's state from the ith agent such that cij = 1 when j  i, and cij = 0 otherwise. The noise of the measurement from the ith to the jth agent is specified as vij(t) which is
assumed to be independent and identically distributed (i.i.d.) Gaussian random variables with zero mean and covariance ij(t) 0. Further the measurement and the noise sets of the ith agent are denoted by Zi(t) = ziT1(t) · · · ziTN (t) T and vi(t) = viT1(t) · · · viTN (t) T, respectively. Over a finite time horizon t = 0, · · · , T , one can rewrite (2) as a static form by stacking up the variables and matrices [Furieri and Kamgarpour, 2020]:

x = P11w + P12u

(4)

where

P11 = (I - DA¯)-1, P12 = (I - DA¯)-1DB¯

A¯ = IT +1  A~,

B¯ =

IT  B~ 0N n×N pT

D=

0N n×N nT IN nT

0N n×N n 0N nT ×N n

where IT and 0T respectively denote the identity and zero matrices of size T × T , and Mi = [0p · · · Ip · · · 0p] 

Rp×Np is the block matrix having Ip in the ith block entry and filled with 0p in other block entries. And x =

[x(0)T . . . x(T )T]T  RNn(T +1), and u =

N i

(IT



MiT)ui



RN pT

are

the

stacked

agents'

states

and

their

control

inputs over the horizon, where ui = [ui(0)T . . . ui(T - 1)T]T  RpT , i  V. w = [x(0)T w~(0)T . . . w~(T - 1)T] 

RNn(T +1) is the vector containing initial agents' states and the additive noise. Over the finite time horizon T , individual

2

agents interact with their neighbors according to the control law ui embedded in each agent. Without loss of generality, ui can be designed by the following output feedback control law [Furieri and Kamgarpour, 2020]:

ui = (IT  Mi)F Zi,(0:T -1) = (IT  Mi)F C(x + vi), i  V

(5)

where vi = [vi(0)T . . . vi(T )T]T  RNn(T +1), i  V, Zi,(0:T -1) = [Zi(0)T · · · Zi(T - 1)T]  RNnT , and C = [INnT 0NnT ×Nn]. The crucial part is the design of the feedback gain, which is denoted by F  F. Here, F  RNpT ×NnT is an invariant subspace that encodes network topological constraints for distributed MAS imposed by A, as well as embeds causal feedback policies by forcing the future response entries to zeros.

2.2 Optimal MAS Distributed Control Problem

Given the equivalent static form of the stochastic linear MAS dynamics over the time horizon T (4), we seek to address the optimal distributed control problem.
Problem 1. Optimal distributed control law subject to structural constraint [Furieri and Kamgarpour, 2020]:

min E xTQx + uTRu

F F

(6)

subject to (4), (5), i  V

where Q  RNn(T +1)×Nn(T +1) 0, and R  RNpT ×NpT 0 are the associated weight matrices.

Due to the structural constraints imposed on the control input space F, Problem 1 is a highly non-convex problem, which is indeed NP-hard and a formidable computational burden [Gupta et al., 2005]. To circumvent such a difficulty, we propose a concept of virtual network topology that allows for the interactions between non-neighboring agents, i.e., virtual interaction as depicted in Figure. 1.

Figure 1: Virtual interaction based distributed control-estimation synthesis.

Since the state information of the non-neighboring agent is not available, an appropriate estimator is required for each agent to obtain the estimates of non-neighboring agents' states. Using the Bayesian approach, Kalman-like filter is adopted for estimation as we consider a linear MAS along with the Gaussian uncertainties.

Definition 1. The state estimate and its covariance of the MAS using the ith agent's measurements are denoted by ix^(t) := E x(t)|Zi,(0:t) and i(t) := E x(t) - ix^(t) x(t) - ix^(t) T |Zi,(0:t) , j  V, respectively [Kwon and
Hwang, 2018], where E[·|·] is the conditional expectation.

The nominal recursive structure of Kalman-like filter is represented by:

ix^(t) = ix^-(t) + Li(t)Hi Zi(t) - ix^-(t)

(7)

where ix^-(t) := E x(t)|Zi,(0:t-1) denotes the predicted state estimate from the ith agent's perspective. Hi  Rn|i|×nN only encodes the neighbor of the ith agent, that is, Hi = [h1 h2 · · · h|i|]T  In where hm  RN , and m = 1, 2, ..., |i| are the nonzero column vectors of the matrix diag(ci1, ci2...., ciN ). And, Li(t)  RnN×n|i| represents the estimator gain at time step t for estimating the states of the MAS from the ith agent's perspective. Once
the entire MAS state estimate information becomes available for each agent, one can replace (5) with the estimation-
based feedback control law. Accordingly, Problem 1, distributed control law subject to structural constraint, can be
reformulated into the problem that simultaneously designs both distributed control and distributed estimator.

3

Problem 2. Optimal distributed control-estimation law with virtual interactions:

min J(F , 1, · · · , N )
F F~,i,iV

subject to (1), and

(8)

ui = (IT  Mi)F C ix^, i  V

where ix^ = [ix^(0)T · · · ix^(T )T]T, J (F , 1, · · · , N ) := E xTQx + uTRu and i := {Li(t)|t = 0, · · · , T } is the set of estimator gains over the time horizon T for the ith agent. Remark 1. It is worth noting that, compared to F, F~  RNpT ×NnT is a subspace that only encodes causal feedback policies, not restricted by any network topological constraint.

Albeit Problem 2 can successfully relax the structural constraint on the control law F, it is not straightforward to solve as the control law and the state estimation errors mutually affect each other [Kwon and Hwang, 2018]. To resolve such complexity, we propose an iterative optimization procedure in a distributed fashion such that: i) divide the primal problem (Problem 2) into the set of sub-problems, each is viewed from an individual agent's perspective; ii) sequentially design the distributed estimation and control laws for each sub-problem; iii) mix the results from individual sub-problems to approximate the optimal solution to the primal problem. The overall schematic of the proposed distributed control-estimation synthesis is delineated in Figure. 2.

Figure 2: Iterative optimization procedure for the optimal distributed control-estimation synthesis.
For the lth iteration, the optimization procedure consists of the following sub-steps. Firstly, distributed estimator design optimizes a set of estimator gains (il), i  V based on the disturbance/noise model, the network topological constraint, and the suboptimal control law resulted from the previous iteration. Then, distributed control law design computes a set of optimal control laws iF (l), i  V, each from the individual agents' perspectives, based on the state estimation error information from the designed distributed estimator. Finally, distributed control-estimation synthesis mixes the set of iF (l), i  V to construct the solution candidate, F (l), for Problem 2. The constructed control law is evaluated to check the convergence and is used for the next iteration. The iteration terminates once the pre-defined stopping criteria are fulfilled, yielding the optimal control-estimation law, denoted by F  and i , i  V.
3 Algorithm Development
This section details out the proposed synthesis procedure of the optimal distributed control-estimation law that can comply with an arbitrary network topology of the stochastic MAS.
3.1 Distributed estimator design
To begin with, the distributed estimation algorithm is optimized by means of estimator gains (il), i  V. As offline design phase, individual estimators can be designed based on the entire MAS model information along with the control law computed from the previous iteration, (A, B, and F (l-1)). For brevity, we use F to designate F (l-1) in this subsection. Recalling (7), the distributed estimator embedded in the ith agent calculates ix^(t), t  {0, · · · , T }, whose performance can be measured by the estimation error.
4

Definition 2. ie(t) := x(t) - ix^(t) is the MAS state estimation error from the ith agent's perspective, and its covariance is i(t) by Definition 1. Further, e(t) = [1e(t)T · · · N e(t)T]T  RNnN stacks all the estimation errors from individual agents' estimator, and the corresponding covariance is denoted by (t) := E[e(t)e(t)T]  RNnN×NnN . Similarly, ie-(t), i-(t), e-(t), -(t) are defined in terms of the predicted state estimate ix^-(t) [Kwon and Hwang, 2018]. Assumption 1. The initial conditions ix^(0), i  V, and (0) are given to individual agents in order to initiate each of their distributed estimators.
Based on the prior knowledge, the estimation based control input of the ith agent at time step t can be written by:

t

ui(t) = Mi Fkt ix^(k)

(9)

k=0

where Fkt  RpN×nN is the block matrix which spans from (knN )th to (knN + nN - 1)th columns and from (kpN )th to (kpN + pN - 1)th rows of the control law matrix F . With (9), the entire MAS dynamics (2) can be
expressed by:

t-1

t

x(t + 1) = A~x(t) + B~Fttx(t) + B~Fktx(k) - B¯M~ F~kte(k) + w~(t)

(10)

k=0

k=0

where B¯ = 1TN  B~, M~ = blkdg(M1TM1, . . . , MNTMN )  RNpN×NpN , and F~kt = IN  Fkt. blkdg(·) denotes a block-diagonal matrix with block matrices ·, and the vector 1N  RN indicates every element equals to 1. Then the predicted state estimate of the entire MAS from the ith agent's perspective is given by:

t-1

ix^-(t + 1) = A~ ix^(t) + B~Fttix^(t) + B~Fktix^(k)

(11)

k=0

Subtracting (11) from (10), and concatenating the results for all agents in V gives:

t-1

e-(t + 1) = te(t) + kte(k) + 1N  w~(t)

k=0

(12)

where t = IN  (A~ + B~Ftt) - 1N  B¯M~ F~tt, kt = IN  B~Fkt - 1N  B¯M~ F~kt

Correspondingly, -(t + 1) is represented by:

t-1

t-1

t-1 t-1

-(t + 1) = t(t)Tt + w~(t) + tE[e(t)e(q)T]Tqt + ptE[e(p)e(t)T]Tt +

ptE[e(p)e(q)T]Tqt

q=0

p=0

p=0 q=0

(13)

where w~(t) = (1N 1TN )  blkdg(1(t), . . . , N (t)). Note that the summation terms in the RHS of (13) (e.g., E[e(p)e(q)T], q = p) imply the correlations of state estimates over time induced by the control law (9). Now, the predicted error, (7) can be rewritten by:

ix^(t + 1) =ix^-(t + 1) + Li(t + 1)Hi(ie-(t + 1) + vi(t + 1))

(14)

Like the Kalman gain, Li(t + 1) can be computed in a way minimizing the mean-squared error of the state estimate, i.e., E ie(t + 1) 2 . This is in fact equivalent to minimizing the trace of the posterior covariance matrices, i.e.,
Tr i(t + 1) , i  V. By the definition of i(t + 1), we have:

i(t + 1) := E[ie(t + 1)ie(t + 1)T|Zi,(0:t+1)]

(15)

= (InN - Li(t + 1)Hi) i-(t + 1)(InN - Li(t + 1)Hi)T + Li(t + 1)Hii(t + 1)(Li(t + 1)Hi)T

where

Li(k + 1) = i-(t + 1)HiT(Si(t + 1))-1

Si(t + 1) = Hi(i-(t + 1) + i(x + 1))HiT

(16)

i(t + 1) = blkdg(i1(t + 1), i2(t + 1), . . . iN (t + 1))

5

Correspondingly, (t + 1) can be updated by:

(t + 1) = (I - L~(t + 1))-(t + 1)(I - L~(t + 1))T + L~(t + 1)(t + 1)L~(t + 1)T

(17)

where (t + 1) = blkdg(1(t + 1), . . . , N (t + 1)), and L~(t + 1) = blkdg(L1(t + 1)H1, . . . , LN (t + 1)HN ). Note that, the covariance between the state estimation errors at current and past steps, i.e., E[e(t + 1)e(s)T] and E[e(s)e(t + 1)T], s < t need to be updated using the computed L~(t + 1). The cross-covariance between the ith and the jth agents' state estimates ij(t + 1) := E[ie(t + 1)je(t + 1)T]  RNn×Nn is at the off-diagonal block entry, while i(t + 1)  RNn×Nn is at the diagonal block entry of (t + 1)  RNnN×NnN . The detailed expansions of
(t) is as follows:

 1(t) 12(t) · · · 1N (t)

 21(t) 2(t) · · · 2N (t)

(t) =   

...

...

...

...

  

N1(t) N2(t) · · · N (t)

Based on the computed i from (16), each agent can update ix^(t), i(t) and (t), respectively using (14), (15), and (17). This completes the implementation of the distributed estimation algorithm.

Remark 2. It is noted that (t) computed by each agent is irrespective of agent's perspective since the same initial condition (0) is given to each agent by Assumption 1.

3.2 Distributed control law design

In this section, the computationally tractable formulation of the optimal distributed control law is derived from the individual agents' perspectives. The main idea starts with relaxing the structural constraints on F by applying the estimator (14) to each agent. Definition 3. Let ie := x - ix^ stacks up the time series of the estimation errors from the ith agent's perspective, over the time horizon T . Given F and i, i  V, one can construct the estimation error covariance over the time horizon T , i := E[ieieT], i  V, as well as the cross-covariance between two different agents ij := E[ie jeT], i = j  V.

In terms of the time series of the estimation errors, the state estimation based control law over the time horizon can be

expressed by:

N

N

u = MiF C ix^ = F Cx - MiF C ie

(18)

i

i

where Mi = IT  (MiTMi), i  V. Plugging (18) into (4) yields the objective cost in (8) as follows:

J (F , 1, · · · , N ) =

1
Q2

(I

-

P12

F

C

)-1

P11

1
w2

2
F

+

1
R2

(I

-

F

C

P12

)-1

F

C

P11

1
w2

2
F

+

1
Q2

P12(I

-

F

C

P12

)-1

(Mi

F

C

ij

C

T

F

T

MTj

)

1 2

2
F

i,j

+

1
R2

(I

-

F

C

P12

-1
)

(Mi

F

C

ij

C

T

F

T

MTj

)

1 2

2
F

i,j

(19)

+

1
Q2

(I

-

P12F C)-1P11µw

2 2

+

1
R 2 (I

-

F CP12)-1F CP11µw

2 2

where

·

2 2

and

·

2
F

denote Euclidean norm and the Frobenius

norm, respectively; and µw

:=

E[w]



RNn(T +1),

w := E[(w - µw)(w - µw)T]  RNn(T +1)×Nn(T +1).

Apparently, the objective cost (19) has high-dimensional, highly coupled optimization variables, i.e., F, which is our main interest, and ij, i, j  V, which are the implicit functions of both F and i, i  V. The proposed iterative optimization procedure alleviates these coupling complexities in two aspects. First, akin to the alternating direction method of multipliers (ADMM) technique [Lin et al., 2013], we set ij, i, j  V constant while optimizing F at the lth iteration, thereby treating J as the function of F only. Note that i, i  V is designed over the constant F in the distributed estimator design phase of the next iteration. Second, we interpret the global objective cost from the
individual agent's viewpoint, and translate the primal problem (Problem 2) into the agent-wise objective cost. The objective cost, locally seen by the ith agent's viewpoint at the lth iteration, is denoted by iJ(l) which can be constructed using the estimated MAS input iu(l) = iF (l)C(x - ie) instead of (18). iF (l) is constructed from the ith agent's perspective by optimizing the agent-wise objective cost, iJ(l). Then, the resulting agent-wise optimization problem is
represented as follows:

6

Problem 3. Optimal distributed control law from agent-wise viewpoint:

min iJ (l)(iF (l))
iF (l)F~

where

iJ (l)(iF (l)) =

1
Q2

(I

-

P12

i

F

(l)

C

-1
)

P11

i

1
w2

2
F

+

1
R2

(I

-

i

F

(l)

C

P12

-1
)

i

F

(l)

C

P11

i

1
w2

2
F

+

1
Q2

(I

-

P12

i

F

(l)

C

-1
)

P12

i

F

(l)

C

i(l)

1 2

2+
F

1
R2

(I

-

i

F

(l)

C

P12

-1
)

i

F

(l)

C

i(l)

1 2

2
F

+

1
Q2

(I

-

P12

i

F

(l)

C

-1
)

P11

i

µw

2 2

+

1
R2

(I

-

i

F

(l)

C

P12

-1
)

iF (l)CP11iµw

2 2

(20) (21)

where iµw = E[w|Zi,(0:T )], iw := E[(w - iµw)(w - iµw)T|Zi,(0:T )], i  V. Note that (il)  RNn(T +1)×Nn(T +1) is computed by Definition 3 at the lth iteration.

Definition 4. A subspace F~ is quadratic invariance (QI) with respect to CP12 if and only if iF (l)CP12iF (l)  F~. And it is trivial to show that F~ is QI with respect to CP12[Lessard and Lall, 2011].
It is well-known fact that QI is a sufficient and necessary condition for the exact convex reformulation [Lessard and Lall, 2011]. That is, one can apply an equivalent disturbance-feedback policy to make (21) a convex form, similar to [Furieri and Kamgarpour, 2020].

Definition 5. Let us introduce the nonlinear mapping as:

h() = (I + CP12)-1, h : RNpT ×NnT  RNpT ×NnT

(22)

and define the cost function J~ : RNpT ×NnT  R in terms of the design parameter i(l)[Furieri and Kamgarpour, 2020].

iJ~(l)(i(l)) =

1
Q2

(I

+

P12

i

(l)

C

)P11

i

1
w2

2+
F

R

1 2

i

(l)

C

P11

i

1
w2

2+
F

Q

1 2

P12

i

(l)

i(l)

1 2

2
F

+

R

1 2

i

(l)

(il)

1 2

2+
F

R

1 2

i

(l)

C

P11

i

µw

2 2

+

1
Q2

(I

+

P12 i (l) C )P11 i µw

2 2

(23)

By Theorem 1 in [Furieri and Kamgarpour, 2020], the following convex optimization problem is equivalent to Problem 3.

Problem 4. Equivalent convex problem to optimal distributed control from agent-wise viewpoint:

min iJ~(l)(i(l))
i (l) h-1 (F~)

(24)

By solving (24) using convex programming, one can find the optimal i(l), and the corresponding iF (l) from the inverse mapping h-1 of (22). The same optimization routines (Problem 3 and 4) based on the locally seen cost from the
other agents' perspectives are processed to get the optimal control laws iF (l), i  V at the lth iteration step.

3.3 Distributed control-estimation synthesis

The set of optimal control laws from individual agents' viewpoint, iF (l), i  V, is mixed to approximate the solution to Problem 2 by the agent-wise mixing strategy proposed as follows:

N

F (l) =

Mi iF (l)

i

(25)

The basic intuition of the proposed strategy is to exhibit the control law for the ith agent using the one computed from the sup-optimization problem (Problem 3) from the ith agent's perspective. Accordingly, the proposed mixing strategy
allows for individual agents to retain distributed controllers to be executed, retaining each of their sub-optimal solutions
without interfering with each other.

7

3.4 Convergence check

The last step of the iteration loop evaluates the designed distributed control law (25), together with the estimator (7).
First, let S be a set which stores the designed control law, F (l), and the estimator gains, (il), i  V, from each iteration step as follows:

S := s(l) s(l) = F (l), (1l), · · · , (Nl) , l  N

(26)

The iteration terminates if: i) the total iteration counts the threshold number Nmax; or ii) the consecutive iteration is converged with respect to the following stopping condition:

J (l, l - 1)  stop

(27)

where J (l, l - 1) := |J (F (l), (1l), · · · , (Nl)) - J (F (l-1), (1l-1), · · · , (Nl-1))| and stop is the threshold magnitude for the convergence. The objective cost of the corresponding control law J(F (l), 1(l), · · · , (Nl)) is computed by plugging the designed control law, F (l), and the set of distributed estimator gains (il), i  V into (19). The final
output of the control-estimation synthesis is given by:

F  =F (l), i = (il), i  V

where l = arg min J (F (l), (1l), · · · , (Nl))

(28)

l|S|

The overall recursive structure of the control-estimation synthesis procedure is summarized in Algorithm 1.

Algorithm 1: Virtual interaction based distributed control-estimation synthesis. Initialize the MAS dynamics information A, B, L, (0), F (0), stop, Nmax and the cost metrics Q, R. for l = 1, 2, · · · Nmax do a) Distributed estimator design
for t=0 to the termination time T 1) Update (t + 1) using F (l-1), (13), (16), and (17)
end for, Output - (il) and (il), i  V b) Distributed control law design
for i=1 to the number of total agent, N
2) Solve (24), and compute iF (l)
end for, Output - iF (l), i  V
c) Distributed control-estimation synthesis 3) Synthesize the control law F (l) using (25)
d) Convergence check
4) Store F (l), and (il), i  V in the set S 5) If (27) is satisfied or l > Nmax, then terminates. end for, Output = F , and i , i  V
It is noted that the algorithm 1 is executed in the offline design phase. Once the distributed control law F and the corresponding estimator gains i for each agent are designed, each agent is deployed into the distributed online operation using its own prior knowledge. It is worth noting that the majority of the heavy computations occur at the offline design phase. Therefore, when it comes to the online operation, it is not burdensome to the limited on-board resources of each agent. Indeed, the computational complexity of the online operation for the proposed algorithm is scaled by the number of agents, i.e., O(N ).

4 Stability Analysis

In this session, the stability analysis of the proposed distributed estimation algorithm in section 3.1 is presented. To

begin with, let us consider the control law as static memoryless feedback gain F as follows:

ui(t) = MiF ix^(t)

(29)

8

where Mi = [0p · · · Ip · · · 0p]  Rp×Np is the block matrix having Ip in the ith block entry and filled with 0p in other entries. F has structural constraints subject to the network topology of MAS specified by the Laplacian L. Note that the

estimation stability is unrelated to the design of F as will be discussed below, and thus readily applicable to memory based feedback control law as in (9). Corresponding to (29), the predicted state estimation errors of the ith agent can be

written as follows:

ie-(t + 1) = D1ie(t) + D12e(t) + w~(t)

(30)

where

D1 = A~ + B~F D12 = -(1TN  B~)blkdg(M1TM1, . . . , MNTMN )(IN  F ) e(t) = [1e(t)T · · · N e(t)T]T

It is noted from the above equation (30) that the estimation error of the individual agent ie is coupled with the augmented estimation error of the entire MAS, e. Then, the following two lemmas are required for proving the stability of the proposed distributed estimation algorithm. Lemma 1. Estimation error covariance from the ith agent, i(t) is positive definite and bounded for all t if the following system is observable [Kwon and Hwang, 2018]:

x(t + 1) = Lx(t) (31)
Zi(t) = Cix(t)

where Ci = [h1 h2 · · · h|i|]T  In  Rn|i|×nN is a observer matrix which gathers the measurements from the ith agent's perspective, i.e., those that are neighboring agents of the ith agent. hq  RN , q = 1, 2, ..., |i| are the nonzero column vectors of the matrix diag(ci1, ci2...., ciN ). It is noted that the value of cij can be decided by the graph G = (V, E) of the given network where (i, j)  E indicates the availability of measurement of the jth agent's state from the ith agent, i.e., cij = 1 and (i, j) / E means cij = 0.

Proof. The proof is referred to in the author's previous work [Kwon and Hwang, 2018].

To analyze the estimation stability of ie, we first introduce Hi(t)  RnN2×nN as a affine mapping matrix and i(t)  RnN2 as a lumped noise as follows:

Hi(t + 1) = F~(t + 1)Hi(t)(iD22(t + 1)iD2(t))-1

i(t + 1) = (F~(t + 1) - Hi(t + 1)iD22(t + 1)D12)i(t) + (t + 1) - Hi(t + 1)i(t + 1)

iD2(t) = D1 + D12Hi(t)

iD22(t + 1) = InN - Li(t + 1)Ci

i(t + 1) = iD22(t + 1)w~(t) - Li(t + 1)Civi(t + 1)

(32)

F~(t + 1) = blkdg(1D22(t + 1)1D2(t), . . . , N D22(t + 1)N D2(t))

 1D22(t + 1)D121(t) + 1(t + 1) 

(t + 1) =  

...

 

N D22(t + 1)D12N (t) + N (t + 1)

It is noted that it is trivial to compute initial affine transformation matrix Hi(0), i  V which satisfies (0) = Hii(0)HiT under the Assumptions 1. On the other hand, i follows the Gaussian distribution i(t)  NnNN (0, i(t)), where i(t) = E[i(t)iT(t)]T with initial value as i(0) = 0nNN . Then, we can show that the augmented estimation error can be mapped to the estimation error from the single agent's perspective by the following lemma.
Lemma 2. Suppose the system given in (31) is observable and satisfies the Assumption 1. Then, given the agent dynamics and the estimation based control (29) with control law as F , there exists a affine mapping between the estimation error of the ith agent, ie, and the augmented MAS estimation error, e, as follows:

e(t + 1) = Hi(t + 1)ie(t + 1) + i(t + 1), i  V, t  0

(33)

Proof. The proof can be shown by induction. Let the estimation error at time step t satisfies (33). To verify that the (33) is satisfied at the next time step t + 1 with the definitions of Hi(t + 1) and i(t + 1), the dynamics of the estimation

9

error of the ith agent is considered. By (33), (30) can be stated as follows:

ie-(t + 1) = D1ie(t) + D12(Hi(t)ie(t) + i(t)) + w~(t) = iD2(t)ie(t) + D12i(t) + w~(t)

(34)

And the updated estimation error is derived as follows:

ie(t + 1) = iD22(t + 1)iD2(t)ie(t) + iD22(t + 1)D12i(t) + i(t + 1)

(35)

By concatenating (35) for all agents i  V, the MAS augmented estimation error, e, can be formulated as follows:

e(t + 1) = F~(t + 1)e(t) + (t + 1)

= F~(t + 1)Hi(t)ie(t) + F~(t + 1)i(t) + (t + 1)

(36)

= Hi(t + 1)ie(t + 1) + i(t + 1)

Without loss of generality, the matrix iD22(t + 1)iD2(t) is invertible as it governs the estimation error dynamics (35) induced by the stochastic linear dynamical system. This completes the proof of Lemma 2.
Based on the derived affine mapping between e and ie, we are ready to present the stability of the proposed estimation algorithm. As our paper considers the stochastic MAS, the estimation error can be regarded as a super martingale of the Lyapunov functions, which satisfies the following conditions:

V (e(t), t) = 0, e(t) = 0



V (e(t), t) > 0, e(t) = 0 , t

(37)

V (e(t), t)  , e(t)  

V (t + 1, t) < 0, k

(38)

where V (t + 1, t) := V (E [e(t + 1)|e(t)] , t + 1) - V (e(t), t).

Theorem 1. Given the MAS dynamics and the control protocol as in (29), the proposed distributed estimation algorithm is globally asymptotically stable in the sense of Lyapunov if the system (31) is observable.
Proof. Let us define the Lyapunov function V : RnN × N  R of the estimation error of the ith agent as follows:
V (ie(t), t) :=ieT(t) i(t) -1 ie(t) (39)
V (t + 1, t) :=V (E[ie(t + 1)|ie(t)], t + 1) - V (ie(t), t)

As i(t) is positive definite and bounded by lemma 1, There exists i(t) -1 0. Therefore, V is a quadratic function which satisfies the condition in (37). Besides, using (34), the conditional expectation E[ie(t + 1)|ie(t)] is given by

E[ie(t + 1)|ie(t)] = iD22(t + 1)iD2(t)ie(t)

Then, by applying equation (35), V can be written as follows:

V (t + 1, t) = - ieT(t) (i(t))-1 - iD2T(t)iD2T2(t + 1) × (i(t + 1))-1iD22(t + 1)iD2(t) ie(t)

(40)

= - ieT(t)Mi(t + 1)ie(t)

To satisfies the condition in (38), Mi(t + 1) should be a positive definite matrix. By applying Lemma 2, the predicted estimation error covariance of the ith agent is derived using (34) as:

i-(t + 1) = iD2(t)i(t)iD2T(t) + D12i(t)D1T2 + w~

(41)

Correspondingly, the updated estimation error covariance can be computed as follows:

i(t + 1) =iD22(t + 1)i-(t + 1)

=i-(t + 1) - i-(t + 1)CiT Cii-(t + 1)CiT + CiiCiT -1Cii-(t + 1)

(42)

and using the definition of iD22, i(t + 1) can be defined as:

i(t + 1) = iD22(t + 1)i-1(t + 1)

(43)

10

By applying the matrix inversion lemma, (42) is rewritten as:

i(t + 1) = (i-(t + 1))-1 + CiT(CiiCiT)-1Ci -1

(44)

And using (44), multiplying i(t + 1) to the left and the right sides of i(t + 1) -1 and applying (43) yields:

i(t + 1) = iD22(t + 1) i-1(t + 1) + Wi(t + 1) iD2T2(t + 1) where Wi(t + 1) = i-(t + 1)CiT(CiiCiT)-1Cii-(t + 1). It is trivial to show that matrix Wi i-(t + 1) denoted in (41), i(t + 1) -1 can be rewritten by:

(45) 0, t. Recalling

i(t + 1) -1 = iD2T2(t + 1) -1 iD2(t)i(t)iD2T + w~ + D12i(t)D1T2 + Wi(t + 1) -1 iD22(t + 1) -1 (46)

By applying (46), Mi(t + 1) can be redefined as:

Mi(t + 1) =(i(t))-1 - iD2T(t) iD2(t)i(t)iD2T(t) + D12i(t)D1T2 + w~ + Wi(t + 1) -1iD2(t)

(47)

After going through tedious conversion using the matrix inversion lemma, (47) can be rewritten as follows:

Mi(t + 1) =(i(t))-1 (i(t))-1 + iD2T(t)(D12i(t)D1T2 + w~ + Wi(t + 1))-1iD2(t) -1(i(t))-1 (48)

As (i(t))-1 0 and iD2T(t)(D12i(t)D1T2 + w~ + Wi(t + 1) 0 in (48), one can verify Mi(t + 1) 0, t, which guarantees the Lyapunov function satisfies (37) and (38). This proves that the estimation error is globally asymptotically
stable.

5 Numerical Simulation
The effectiveness of the proposed algorithm is demonstrated with an illustrative MAS example. The MAS consists of five agents whose dynamics and objective are specified by the following parameter sets: A = 1, B = 1, i(t) = 1, i(t) = diag(1, 1, 1, 2, 1), i  V, t  {0, · · · , T }, T = 5, Q = I6  (5I5 - 15×5), and R = I25. The MAS network topology is set to be partially connected, the same as the one in [Kwon and Hwang, 2018]. To validate the performance of the proposed algorithm, we conduct a comparative analysis with two different scenarios: i) MAS with the fully connected network, which is free from network topological constraint; and ii) MAS with the same (partially connected) network topology where non-neighboring agent information is not available to each agent. For the second case, we test the suboptimal method presented in [Gupta et al., 2005]. The simulation results are shown in Figure. 3. By virtue of the virtual interactions between non-neighboring agents, our algorithm outperforms the existing method in the partially connected network, and even matches the fully connected network case despite the network topological constraints.

Figure 3: Cost value statistics histogram (Monte Carlo simulations with 100 runs).
To further demonstrate the performance of the proposed algorithm, we have performed additional simulations with respect to the different number of "real" interactions. It is worth noting that the network links between agents yield the "real" interaction while those with no explicit links create the virtual interaction. MAS with five agents has been simulated under different network topology shown in the below Figure. 4. All experiments followed the same setting except the network topology. Apparently, the number of virtual interactions decreases as the number of network links between agents increase whereby we can analyze the effect of ratio between real and virtual interactions. The performance of our proposed method under different network settings is depicted in the below Figure. 5. The result shows that the performance of our proposed method does not vary much with respect to the ratio between virtual to real interactions. At the cost of some onboard computational resources to estimate non-neighboring agents, our proposed method provides the clear advantage of having optimal performance with fewer network connections.
11

Figure 4: Different network topology with 5 agents.
Figure 5: Cost value comparison with a different number of network links between agents.
6 Conclusions
This paper has proposed a novel design procedure of the optimal distributed control for the linear stochastic MAS, generally subject to network topological constraints. The proposed method gets around the network topological constraint by employing the distributed estimator, whereby each agent can exploit the non-neighboring agent's information. Future work will include the theoretical performance guarantee of the proposed distributed controlestimation synthesis such as stability analysis, and a further extension to the infinite time horizon case for practical use.
References
Peng Shi and Bing Yan. A survey on intelligent control for multiagent systems. IEEE Transactions on Systems, Man, and Cybernetics: Systems, 2020.
Xianwei Li, Fangzhou Liu, Martin Buss, and Sandra Hirche. Fully distributed consensus control for linear multi-agent systems: A reduced-order adaptive feedback approach. IEEE Transactions on Control of Network Systems, 2019.
Yunru Zhu, Liqi Zhou, Yuanshi Zheng, Jian Liu, and Shiming Chen. Sampled-data based resilient consensus of heterogeneous multiagent systems. International Journal of Robust and Nonlinear Control, 30(17):7370­7381, 2020.
Ryosuke Morita, Takayuki Wada, Izumi Masubuchi, Toru Asai, and Yasumasa Fujisaki. Multiagent consensus with noisy communication: stopping rules based on network graphs. IEEE Transactions on Control of Network Systems, 3 (4):358­365, 2015.
Vijay Gupta, Babak Hassibi, and Richard M Murray. A sub-optimal algorithm to synthesize control laws for a network of dynamic agents. International Journal of Control, 78(16):1302­1313, 2005.
12

Jingying Ma, Yuanshi Zheng, and Long Wang. Lqr-based optimal topology of leader-following consensus. International Journal of Robust and Nonlinear Control, 25(17):3404­3421, 2015.
Dinh Hoa Nguyen. A sub-optimal consensus design for multi-agent systems based on hierarchical lqr. Automatica, 55: 88­94, 2015.
Junjie Jiao, Harry L Trentelman, and M Kanat Camlibel. A suboptimality approach to distributed linear quadratic optimal control. IEEE Transactions on Automatic Control, 65(3):1218­1225, 2019.
Cheolhyeon Kwon and Inseok Hwang. Sensing-based distributed state estimation for cooperative multiagent systems. IEEE Transactions on Automatic Control, 64(6):2368­2382, 2018.
Luca Furieri and Maryam Kamgarpour. First order methods for globally optimal distributed controllers beyond quadratic invariance. In 2020 American Control Conference (ACC), pages 4588­4593. IEEE, 2020.
Fu Lin, Makan Fardad, and Mihailo R Jovanovic´. Design of optimal sparse feedback gains via the alternating direction method of multipliers. IEEE Transactions on Automatic Control, 58(9):2426­2431, 2013.
Laurent Lessard and Sanjay Lall. Quadratic invariance is necessary and sufficient for convexity. In Proceedings of the 2011 American Control Conference, pages 5360­5362. IEEE, 2011.
13

