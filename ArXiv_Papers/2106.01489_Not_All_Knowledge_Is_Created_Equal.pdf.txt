arXiv:2106.01489v1 [cs.LG] 2 Jun 2021

Not All Knowledge Is Created Equal
Ziyun Li1, Xinshao Wang2,6,, Haojin Yang1, Di Hu3, Neil M. Robertson4,6, David A. Clifton2,5,, Christoph Meinel1
1Hasso Plattner Institute, Germany 2Institute of Biomedical Engineering, University of Oxford, UK
3Renmin University of China, China 4Queen's University Belfast, UK
5Oxford Suzhou Centre for Advanced Research, China 6Zenith Ai, UK
Email: {ziyun.li, haojin.yang, christoph.meinel}@hpi.de, {dihu}@ruc.edu.cn, {n.robertson}@qub.ac.uk, {xinshao.wang, david.clifton}@eng.ox.ac.uk
Abstract
Mutual knowledge distillation (MKD) improves a model by distilling knowledge from another model. However, not all knowledge is certain and correct, especially under adverse conditions. For example, label noise usually leads to less reliable models due to the undesired memorisation [1, 2]. Wrong knowledge misleads the learning rather than helps. This problem can be handled by two aspects: (i) improving the reliability of a model where the knowledge is from (i.e., knowledge source's reliability); (ii) selecting reliable knowledge for distillation. In the literature, making a model more reliable is widely studied while selective MKD receives little attention. Therefore, we focus on studying selective MKD and highlight its importance in this work. Concretely, a generic MKD framework, Confident knowledge selection followed by Mutual Distillation (CMD), is designed. The key component of CMD is a generic knowledge selection formulation, making the selection threshold either static (CMD-S) or progressive (CMD-P). Additionally, CMD covers two special cases: zero knowledge and all knowledge, leading to a unified MKD framework. We empirically find CMD-P performs better than CMDS. The main reason is that a model's knowledge upgrades and becomes confident as the training progresses. Extensive experiments are present to demonstrate the effectiveness of CMD and thoroughly justify the design of CMD. For example, CMD-P obtains new state-of-the-art results in robustness against label noise.
1 Introduction
"What knowledge to be selected for distillation" is an essential question of mutual knowledge distillation (MKD) but has received little attention. In this work, we study it for two reasons: (i) Existing MKD methods treat all knowledge of a deep model equally, i.e., all knowledge is distilled into another model without selection. (ii) There are two contradictory findings of label smoothing(LS). One is that in clean scenarios, when a network (a teacher) is trained with LS, distilling its knowledge into another model (a student) is much less effective [3]. Another finding is that in noisy scenarios, LS improves both teacher and student. In their contradictory studies, they only focus on the knowledge source, e.g., how a source (teacher) model is trained. There was no study on the knowledge selection,
Corresponding author: xinshao.wang@eng.ox.ac.uk, xinshao@zenithai.co.uk. Prof. David A. Clifton is supported by the National Institute for Health Research (NIHR) Oxford Biomedical Research Centre (BRC).
Preprint. Under review.

Table 1: The interactions between how each model is trained (i.e., CE, LS, CP, ProSelfLC, and our proposed variant Mconf) and what knowledge should be distilled (zero knowledge, all knowledge, and our proposed CMD-S/P). Experiments are done on CIFAR-100 using ResNet34. The symmetric label noise rate is 40%. The average final test accuracies (%) of two models are reported. The performance difference between the two models is negligible.

Distilled Knowledge

CE

LS

CP

ProSelfLC Mconf (ours)

Zero All
CMD-S CMD-P

48.74 51.42
52.52 54.28

51.53 53.63
55.10 56.73

50.06 53.18
53.86 56.47

62.75 59.26
67.26 68.29

65.04 61.11
68.45 69.09

which could be a key, as empirically indicated in Table 1. This research question can also be expressed as:
Should all knowledge or partial knowledge of a model be distilled into another model? In clean scenarios, the knowledge source is generally reliable. Thus, simply distilling all knowledge is reasonable, and it has widespread use in existing KD works. However, in label-noise scenarios3, the knowledge source is less reliable. The distilled incorrect knowledge would mislead the learning rather than help. Therefore, it is vital to note "not all knowledge is created equal" and identify "what knowledge could be distilled?". We work on this problem from two aspects: (i) making the knowledge source more reliable. (ii) selecting the certain knowledge to distill. For the first aspect, many algorithms have been proposed, e.g., Tf-KD [5] and ProSelfLC [4]. For simplicity, we exploit them and focus more on the second aspect: selective knowledge distillation.
To explore the knowledge selection problem, we design a selective MKD framework, i.e., Mutual distillation of confident knowledge, which is shown in Figure 1. We propose to only distill confident knowledge. Specifically, we design a generic knowledge selection formulation, so that we can either fix the knowledge selection threshold (CMD-Static, shortened as CMD-S) or change it progressively as the training progresses (CMD-Progressive, abbreviated as CMD-P). In CMD-P, we leverage the training time to adjust how much knowledge would be selected dynamically considering that a model's knowledge improves along with time. CMD-P performs slightly better than CMD-S, according to our empirical studies, e.g., Table 1.
We summarise our contributions as follows:
· To the best of our knowledge, we are the first one to study what knowledge to be selected for distillation in MKD. Correspondingly, we propose a generic knowledge selection formulation, which covers the variants of zero-knowledge, all knowledge, CMD-S, and CMD-P.
· Thorough studies on the models' learning curves, knowledge selection criterion's settings, and hyperparameters justify the rationale of our selective MKD design and its effectiveness.
· Our proposed CMD-P obtains new state-of-the-art results in deep robustness against label noise.
2 Related work
Notations. For a multi-class classification problem, x is a data point, and q  RC is its annotated label distribution, also seen as annotated knowledge. C is the number of training classes. In the traditional practice, q is a one-hot representation, a.k.a., hard label. Mathematically, q(j|x) = 1 only if j = y, and 0 otherwise. Here, y denotes the semantic class of x. f is a deep neural network that predicts the probabilities of x being different training classes. We denote them using a vector p  RC, which can be seen as a model's self knowledge.
3We remark that the label-noise setting is typical and challenging in real-world machine learning applications, where the given datasets have non-perfect annotations. Additionally, in some recent work, it is shown that the performance gap of different approaches is relatively small in clean scenarios [4­7]. Therefore, we study selective knowledge distillation and evaluate our design of knowledge selection mainly under the setting of robust deep learning against noisy labels.
2

(a) Conventional MKD.

(b) CMD.

Figure 1: Comparison of conventional MKD and our CMD. Dotted frames represent components from model A and solid frames represent components from model B. pA and pB are predictions from mode A and model B, respectively. In (b), q~A and q~B represent the refined labels by a self distillation method, and  is the threshold to decide whether the prediction is confident enough or not. H(p) denotes the entropy of p, and H(q, p) is the cross entropy loss between q and p.

KD is an effective method for distilling the knowledge of complex ensembles or a cumbersome model (usually named teacher models) to a small model (usually named a student) [8, 9]. Recently, many deep KD variants have been proposed, e.g., self knowledge distillation (Self KD) which trains a single learner and leverages its own knowledge [4, 5], MKD with knowledge transfer between two learners [10­12], ensemble-based KD methods [13, 14], and born-again networks with knowledge distilling from multiple student generations [15]. Since we focus on training two learners, TeacherStudent KD (T2S KD) and Mutual KD are more relevant. We briefly present some of them as follows.

2.1 T2S KD and MKD

T2S KD [9] transfers knowledge from a teacher model to a student model and can be formulated as:

LT2SKD(q, p, pt) = (1 - )H(q, p) + DKL(pt, p),

(1)

where p is the predicted distribution by a student model while pt is the output of a teacher model, H(q, p) represents the cross entropy loss between target q and prediction p.
Asynchronous Mutual KD (AsyncMKD) [10] trains two models A and B, making them learn from each other. Therefore, we have LA(q, pA, pB) = (1 - )H(q, pA) + DKL(pB, pA) and LB(q, pB, pA) = (1 - )H(q, pB) + DKL(pA, pB). In the original proposal, A and B are trained iteratively in an asynchronous fashion. The overall loss can be represented as follows:

LAsyncMKD =

LA(q, pA, pB), LB(q, pB, pA),

t%2 = 1, t%2 = 0.

(2)

t is an iteration counter.
Synchronous Mutual KD (SyncMKD) is a simple variant of AsyncMKD proposed by us. Instead of training each model iteratively, the training of A and B are synchronous and they learn from each other in real-time. Overall, the loss becomes slightly different:

LSyncMKD = LA(q, pA, pB) + LB(q, pB, pA).

(3)

In our experiments, for non-selective MKD, we implement SyncMKD because its training pipeline is the same as other methods, so that the comparison becomes more exact.

2.2 Ensemble-based and feature-map-based KD methods
Knowledge Distillation via Collaborative Learning (KDCL) [13] treats all models as students, while the teacher model is an ensemble of all students. Peer Collaborative Learning (PCL)[14] assembles multiple subnetworks as a teacher model. FFL[16] integrates feature representation of multiple models and AFD[17] transfers prediction and feature-map knowledge together.

3

Table 2: Summary of CE, LS, CP, Boot-soft, ProSelfLC, and Mconf from the angle of self KD. measures how much we trust the prediction and  [0, 1]. It can be static or progressive and adaptive as the training progresses.

Self KD

CE

LS

CP

Boot-soft ProSelfLC

Mconf

Refined label q (1 - )q + u (1 - )q - p (1 - )q + p (1 - )q + p (1 - )q + p

Self trust Zero Fixed

Fixed

Fixed

l(p) × g(t) l(p) × g(r)

2.3 The relationship between KD and label modification

As mentioned in [4, 5], the learning target modification is to replace a one-hot label representation by its convex combination with a predicted distribution p~: q~ = (1 - )q + p~. This is a generic formulation of label modification: (i) p~ can come from different sources, e.g., uniform distributions, a current model, a pretrained model of the same net architecture, a teacher model of a different net architecture, an ensemble of multiple expert models, etc; (ii) can be fixed in Boot-soft [18], Joint-soft [19], or adaptive as in ProSelfLC [4].

As DKL(pt, p) = H(pt, p) - H(pt), and H(pt) is a constant for a fixed teacher model. Hence, we can omit H(pt) and rewrite Eq. (1) as:

LT2SKD(q, p, pt) = E(1- )q+ pt (- log p)  q~T2SKD = (1 - )q + pt.

(4)

Thus, q~T2SKD is a new label that contains the distilled knowledge from another model. Similar reformulation of Eq. (2) and Eq. (3) can be done for AsyncMKD and SyncMKD, respectively.

2.4 The relationship between KD and sample selection
As demonstrated in section 2.3, when KD is exploited, only the label of a sample may be changed. Therefore, the biggest difference between KD and sample selection (e.g., Co-teaching [20, 21]) is that KD uses all samples. Symmetrically but differently, our proposed CMD, as a selective MKD method, selects knowledge to distill while sample selection methods select training data points to train.

3 Towards improving the reliability of each model independently

We explore and apply some Self Label Correction (i.e., Self KD) algorithms to train models independently so that their knowledge is more reliable, especially when noisy labels exist. We will first represent some relevant self KD methods and then propose a alternative in section 3.2: Mconf.

3.1 Revisiting independent self KD methods

Though cross entropy (CE) and label smoothing (LS) [22] do not exploit self knowledge, they are included as simple baselines and references. For the sake of clarity and conciseness, we summarise CE, LS together with some highly relevant self KD methods, including confidence penalty (CP) [23], Boot-soft [18] and ProselfLC [4] in Table 2. Accordingly, the differences between self KD and T2S KD become very obvious by looking at the Eq. (4) and the first row of Table 2. Different from Eq. (4) where the pt is from an auxiliary model, there is no knowledge token from the other models in CE, LS, and self KD approaches.
According to the Table 2, CE only uses the annotated label while LS applies a uniform distribution (u) to smooth the annotated label. CP penalizes highly confident predictions. Boot-soft and ProselfLC refine the learning target by combining a corresponding prediction. is fixed manually in LS, CP, and Boot-soft. While in ProselfLC, changes progressively during training. Here, we briefly represent its design. In ProselfLC, contains two key components, including sample confidence and model confidence according to training time:

l(p) = 1 - H(p)/H(u)  (0, 1),

= g(t) × l(p)

(5)

g(t) = h(t/ - 0.5, b1)  (0, 1),

4

where l(p) represents a model's predictive confidence w.r.t x (i.e., sample confidence). g(t) computes the global model confidence and rises along with training time. h(, b1) = 1/(1 + exp(- × b1)) is a logistic function, where b1 is a hyperparameter for controlling the smoothness of h. t and  can be the epoch (iteration) counter and the number of the total epochs (iterations), respectively.

3.2 Mconf: An alternative for self KD

In the design of ProSelfLC, the global model confidence is determined by the training time, without considering a model's overall knowledge w.r.t. all training examples. In our empirical exploration, we find another alternative which performs competitively or even better, named Mconf. Alternatively, without considering training time, Mconf defines the global model confidence according to a model's predictive confidence w.r.t. all samples and is computed as follows:

g(r) = h(r - , b1),

where r = 1 -

n i=1

H(pi)

.

n  H(u)

(6)

r represents a model's overall certainty of all examples. A higher r implies that a model is more reliable. Intuitively, if r is higher than a threshold , we should assign more trust to the model. We simply set  = 0.5 in all our experiments. Consequently, = g(r) × l(p). And the loss becomes:

LMconf = H(q~Mconf , p) = Eq~Mconf (- log p), where q~Mconf = (1 - )q + p. (7)

4 Mutual distillation of confident knowledge
CMD improves MKD by distilling confident knowledge into each other rather than all knowledge, as shown in the Figure 1. Conventional MKD transfers all knowledge between two models while our CMD selects confident knowledge to distill. We design a generic knowledge selection formulation that unifies zero knowledge, all knowledge, and partial knowledge selection in a static and progressive fashion (CMD-S and CMD-P).

4.1 Learning objectives

To distill model B's confident knowledge into model A, we optimise A's predictions towards B's

confident predictions:

LB2A =

H(q~B, pA), 0,

H(pB) < , H(pB)  .

(8)

We use the entropy H(pB) to measure the confidence of pB. Low entropy indicates high confidence, and vice versa [23, 22, 9, 24].  is a threshold to decide whether a label prediction is confident enough or not. Specifically, only when H(pB) < , the model B's knowledge w.r.t. x is confident enough. q~B is the learning target, which can be from a self KD method as it is more reliable.
Analogously, we distill model A's confident knowledge into model B:

LA2B =

H(q~A, pB), 0,

H(pA) < , H(pA)  .

(9)

The final loss functions for models A and B are:

LA = LASelfKD + LB2A =

H(q~A, pA) + H(q~B, pA) = 2E q~A+q~B (- log pA),
2
H(q~A, pA) = Eq~A (- log pA),

H(pB) < , H(pB)  .

(10)

LB = LBSelfKD + LA2B =

H(q~B, pB) + H(q~A, pB) = 2E q~A+q~B (- log pB),
2
H(q~B, pB) = Eq~B (- log pB),

H(pA) < , H(pA)  .

(11)

Relationship with label correction. In Eq. (10), for model A, given any x, its learning target

becomes

either

q~A +q~B 2

(when

model

B

is

confident

with

respect

to

x)

or

q~A.

Similarly,

for

model

B,

its

learning

target

becomes

either

q~A +q~B 2

(when

model

A

is

confident

with

respect

to

x)

or

q~B.

Therefore, our approach can also be interpreted as mutual label correction as proved in section 2.3.

5

4.2 A generic design for knowledge selection

As aforementioned, we use an entropy threshold  to decide whether a piece of knowledge is certain enough or not. We design a generic formation for  as follows:

H(u)

t

 =   2h(  - 0.5, b2),

(12)

where h(·, ·) is a logistic function. u is a uniform distribution, thus H(u) is a constant. t and  denote the current epoch and the total number of epochs, respectively. For a wider unification, we make the design of Eq. (12) generic and flexible. Therefore, we use  to control the starting point. While b2 controls how the knowledge selection changes along with t.  has two different modes:

· Static (CMD-S). The confidence threshold  is a constant when b2 = 0. Concretely,

2h(

t 

-

0.5, 0)

=

1





=

H(u) 

.

This

mode

covers

two

special

cases:

(i) One model's all knowledge is distilled into the other when   (0, 1]    H(u),

which degrades to be the conventional MKD. (ii) Zero knowledge is distilled between two models when   {+, R-}    0.

· Progressive (CMD-P). When b2 = 0,  changes as the training progresses. To make it comprehensive,  can be either increasing or decreasing at training: (i) If b2 > 0,  increases as t increases. Since the knowledge selection criteria is relaxed, more knowledge will be transferred between the two models at the later learning phase. (ii) On the contrary,  gradually decreases when setting b2 < 0. This only allows knowledge with higher confidence (lower entropy) to be distilled.

It is worth highlighting that both models' knowledge becomes more confident at the later stage, therefore it does not mean that the knowledge communication becomes rare when the knowledge selection criterion becomes stricter, as illustrated in Figure 2. In our empirical studies (e.g., Figure 4), in the noisy scenario, CMD-P with b2 < 0 performs the best. Therefore, when comparing with prior relevant methods, we use CMD-P with b2 < 0 by defaults.

5 Experiments
In this section, we first demonstrate that CMD is effective in robust learning against an adverse condition, i.e., label noise (Section 5.1). Then we empirically verify that CMD, as a selective MKD, outperforms prior MKD approaches for training two models collaboratively no matter whether they are of the same architecture or not (Section 5.2). We subsequently present a comprehensive ablation study and hyperparameters analysis (Sections 5.3 and 5.4). Different network architectures are evaluated. For all experiments, we report the final results when the training terminates. More implementation details are provided in the supplementary material. The code will be released once this work is accepted.

Knowledge communication frequency

100000

80000

60000

40000

20000 0

CCCMMMDDD---PSP(((bbb222

= = =

8) 0) -8)

20 40 Epoc6h0 80 100

Figure 2: Knowledge communication frequency is measured by the sum of the number of distilled knowledge (training labels) from A to B and that from B to A. All experiments are done on CIFAR100 with  = 2. CIFAR-100 has 50,000 training examples in total.

5.1 CMD for robust learning against noisy labels
Label noise generation. We verify the effectiveness of our proposed CMD on both synthetic and real-world label noise. For synthetic label noise, we consider symmetric noise and pair-flip noise [20]. For symmetric label noise, a sample's original label is uniformly changed to one of the other classes with a probability of noise rate r. The noise rates are set to 20%, 40%, 60%, and 80%. For pair-flip noise, the original label is flipped to its adjacent class with noise rates of 20% and 40%, respectively.

6

Table 3: Results on CIFAR-100 clean test set. All methods use ResNet34 as the network architecture. The top two results of each column are bolded.

Method

Pair-flip label noise

20%

40%

Symmetric label noise

20%

40%

Clean

CE LS CP Boot-soft ProSelfLC

63.52 65.15 64.97 64.04 74.13

45.40 50.02 49.01 48.85 69.49

63.31 67.45 65.97 63.25 71.49

47.20 51.53 51.09 48.41 64.07

75.58 76.33 75.29 75.37 75.73

CMD-S+ProselfLC CMD-P+ProselfLC
Mconf CMD-S+Mconf CMD-P+Mconf

75.68 75.76
73.12 75.39 75.89

74.22 74.55
62.29 74.32 74.72

72.11 72.58
71.04 72.20 73.22

67.26 68.29
65.04 68.45 69.09

76.25 77.32
75.20 75.92 76.42

5.1.1 The interaction between self label correction and CMD
As shown in Tables 1 and 3, CMD, as a new selective MKD method, can be easily added to existing self label correction methods as a collaborative mutual enhancer.
In Table 1, we explore to train each model using CE, and self label correction methods (LS, CP, ProselfLC [4] and Mconf). At the same time, we try four types of knowledge communication: Zero/no knowledge is distilled into the peer model and two models are trained independently; All knowledge is distilled without selection, as SyncMKD does; Our proposed methods including CMD-S and CMD-P. Vertically, from the selective knowledge distillation perspective, we clearly observe that CMD methods (CMD-S and CMD-P) are better than "Zero" and "All" consistently no matter how each model is trained. This empirically demonstrates that selecting confident knowledge for distillation is better. In addition, CMD-P is slightly better than CMD-S, mainly due to the fact that a model's knowledge upgrades and becomes confident as the training progresses. Horizontally, from improving each model's knowledge viewpoint, we observe that the methods with self confident KD (ProSelfLC and Mconf) outperforms those without self confident KD (CE, LS, CP) consistently across all distillation types. Notably, when Mconf and ProSelfLC are used, All MKD performs even worse than Zero MKD. This empirically proves the importance of improving knowledge source when noisy labels exist.
Table 3 is an extension of Table 1. Results of different noise types and rates are present. Since ProSelfLC and Mconf always performs better than the other approaches, therefore we only apply CMD over them to explore how much CMD can enhance stronger baselines.
5.1.2 Outperforming recent state-of-the-art methods for handling label noise
In this subsection, our objective is to compare with recent state-of-the-art methods for addressing label noise. For simplicity, we only train CMD-P together with ProSelfLC and Mconf, which are demonstrated to be the best in 5.1.1.
Results on the synthetically noisy CIFAR-100. Table 4 (CIFAR-100) shows results of training ResNet50 on CIFAR-100. CMD-P+ProSelfLC and CMD-P+Mconf outperform all the recent labelnoise-oriented methods under both pair-flip and symmetric noisy labels. Notably, their improvements are more significant when noise rate rises.
Results on the real-world noisy Food-101. Table 4 (Food-101) presents the results on Food-101 with real-world noisy labels. As this dataset is more challenging, thus the performance gap is smaller over all methods. Despite that, CMD-P+ProSelfLC and CMD-P+Mconf consistently outperforms all the recent algorithms.
Results on the real-world noisy Webvision. Our experiments follow the "Mini" setting in [26] and our results are reported in Table 4. The first 50 classes of the Google resized image subset is treated as training set and evaluate the trained networks on the same 50 classes on the ILSVRC12 validation set. The results of CMD-P+ProSelfLC and CMD-P+Mconf are around 5-6% higher than the latest methods including Co-teaching, APL, CDR, and ProselfLC.
7

Table 4: Recent state-of-the-art approaches for label noise are compared. All methods apply ResNet50 as the network architecture. For Food-101, we use a ResNet50 pre-trained on ImageNet. For Webvision, we follow the "Mini" setting in [25­28]. The top two results of each column are bolded.

Method

CIFAR-100

Real-world noise

Pair-flip label noise Symmetric label noise Food-101 Webvision (Mini)

20%

40%

20%

40%

20%

50%

CE GCE [29] Co-teaching [20] Co-teaching+ [21] Joint [19] Forward [30] MentorNet [26] T-revision [31] DMI [32] S2E [33] APL [28] CDR [25] ProSelfLC [4]
CMD-P+ProselfLC
Mconf CMD-P+Mconf

64.10 62.32 58.11 56.31 67.35 58.37 54.73 62.69 58.77 58.21 59.77 71.93 73.11
75.16
72.25 74.38

52.77 55.03 48.46 38.03 52.22 39.82 45.31 52.31 42.89 41.74 53.25 56.94 69.49
73.36
70.84 73.86

63.93 65.62 61.47 64.13 54.88 66.12 57.27 64.67 62.77 64.21 59.37 68.68 71.17
73.25
69.92 72.23

56.82 57.97 53.44 55.92 45.64 59.45 49.01 57.15 57.42 43.12 51.03 62.72 60.38
64.09
62.80 64.30

84.03 84.96 83.73 76.89 83.10 85.52 81.25 85.97 85.52 84.97 82.17 86.36 86.97
87.54
86.70 87.60

57.34 55.62 61.22 33.26 47.60 56.33 57.66 60.58 56.93 54.33 61.27 61.85 62.40
67.40
64.44 67.48

Table 5: MKD for training two networks of the same architecture. For self distillation, we train single model. For each algorithm, we train ResNet34 for 100 epochs. Experiments are done on CIFAR-100 under 40% symmetric noisy labels. For each case, the best result is bolded.

Baseline

Self KD

Ensemble KD

MKD

CE Tf-KDreg ProSelfLC Mconf 47.20 47.39 64.07 65.04

KDCL SyncMKD CMD-P+ProselfLC CMD-P+Mconf

51.20

51.42

68.29

69.09

5.2 Comparing with recent state-of-the-art MKD methods

MKD for two networks of the same architecture. In Table 5, we present the results of the baseline CE, self distillation methods (Tf-KDreg [5], ProselfLC and Mconf), and mutual distillation algorithms (SyncMKD, KDCL, CMD-P+ProSelfLC, and CMD-P+Mconf) under noisy scenarios. SyncMKD and KDCL distill all knowledge without selection. CMD-P+ProSelfLC and CMD-P+Mconf achieve 17%-18% absolute improvement compared to SyncMKD and KDCL.

MKD for two networks of different architectures. For self KD methods, we train each model individually (i.e., without mutual distillation) while for MKD methods, we train them together (i.e., with mutual distillation). In Table 6, we demonstrate CMD's effectiveness for training two different networks, ResNet18 and ShufflenetV2. CMD improves both ProselfLC and Mconf for around 3% for ResNet18 and 1-3% for ShuffleNetV2.

Table 6: MKD for training two models of different net architectures. All networks are trained for 200 epochs following [5]. Experiments are done on CIFAR-100 with 40% symmetric noisy labels. For each case, the best result is bolded.

Baseline Self KD Ensemble KD MKD

Method
CE
Tf-KDreg ProselfLC
Mconf
KDCL
SyncMKD CMD-P+ProselfLC
CMD-P+Mconf

ResNet18
50.63
51.05 58.51 55.94
55.45
60.38 65.77 68.10

ShuffleNetV2
44.06
44.70 58.89 61.21
46.10
47.72 59.77 64.37

8

5.3 Analysis on dynamic learning behaviours
Figure 3 shows the accuracy curves of different methods on CIFAR-100 with 40% symmetric noisy labels. Here, we only show the promoter CMD-P because CMD-P is slightly better than CMD-S. We observe that CMD-P dramatically boosts the performance of CE, ProselfLC and Mconf.
As the training goes, without CMD-P, the test accuracies of CE, ProselfLC and Mconf drop a lot after around 50 epochs, due to the undesired memorisation [1, 2, 34, 35]. However, when CMD-P is exploited, their generalisation is improved significantly.
More importantly, the test accuracies of CMD-P+ProselfLC and CMD-P+Mconf keep rising. This indicates that CMD-P is great at avoiding overfitting and learning robustly under noisy labels. In summary, CMD not only boosts self label correction methods but also promotes robust learning against adverse conditions.
5.4 Hyper-parameters analysis
5.4.1 Analysis of b2

Test Accuracy Accuracy Accuracy

0.6

0.4 0.2
0

CE+CMD-P CE PPMMrrccoooossnneeffll+ffLLCCCM+DC-MPD-P
25 Ep5o0ch 75 100

Figure 3: Learning curves on CIFAR-100 using ResNet34. The training set has 40% symmetric noisy labels.

75

70

65

60

55

50

45

r=20%

40

r=40% r=60%

35

5

b02

5

55 50 45 40

5

b02

=4 =3 =2
5

(a) The accuracy along with b2 for different noise rates. We fix  = 2.

(b) The accuracy along with b2 for different . We fix the symmetric noise rate r = 60%.

Figure 4: Experimental analysis of b2 on CIFAR-100.

Mathematically, according to section 4.2, b2 decides how the knowledge selection threshold changes along with the training epoch t.
In Figure 4a, we fix  = 2 and study the effect of b2 under different noise rates. We observe that the accuracy increases as b2 decreases for all noise rates. The trend becomes more obvious as the noise rate increases. This empirically verifies the effectiveness of confident knowledge selection again. Furthermore, progressively increasing the confidence criterion leads to better performance.
In Figure 4b, we further study b2 under different . The accuracy keeps increasing as b2 decreases for all . Additionally, the trend is more significant when  becomes smaller.

5.4.2 Analysis of 

As presented in section 4.2,  is a parameter Table 7: The results of CMD-S with different . to linearly scale the knowledge selection crite- We train on CIFAR-100 using ResNet-34. ria. To study , we first analyze the static mode.

Table 7 shows the results of CMD-S with different . We can see that a lower threshold (i.e.,

CMD-S

Symmetric label noise 20% 40% 60% 80%

larger ) has higher accuracy for all noise rates.

H(u) ( = 1) 70.37 59.26 36.18 16.17

This further demonstrates the effectiveness of 1/2 H(u) ( = 2) 72.11 65.04 46.15 18.62

distilling more confident knowledge.

1/3 H(u) ( = 3) 72.83 66.42 51.34 19.84

1/4 H(u) ( = 4) 73.25 67.26 54.34 22.45 We then analyse the dynamic mode. In Figure

4b, the green line ( = 4) has the highest ac-

9

curacy for most b2 values. Overall, the blue line ( = 3) is the second best, while the red line ( = 2) has the lowest accuracy. Therefore, we conclude that a smaller  is better in both static and progressive modes.
6 Conclusion
We are the first one to study knowledge selection in MKD and propose a unified knowledge selection framework, named CMD. CMD improves MKD by distilling only confident knowledge to guide the peer model. By extensive experiments, we empirically demonstrate the effectiveness of CMD. Furthermore, our proposed CMD outperforms related MKD methods and obtains new state-of-the-art results in handling the label noise problem.
References
[1] Zhang, C., Bengio, S., Hardt, M., Recht, B., Vinyals, O.: Understanding deep learning requires rethinking generalization. In: ICLR. (2017)
[2] Arpit, D., Jastrzebski, S., Ballas, N., Krueger, D., Bengio, E., Kanwal, M.S., Maharaj, T., Fischer, A., Courville, A., Bengio, Y., Lacoste-Julien, S.: A closer look at memorization in deep networks. In: ICML. (2017)
[3] Müller, R., Kornblith, S., Hinton, G.E.: When does label smoothing help? In: NeurIPS. (2019)
[4] Wang, X., Hua, Y., Kodirov, E., Clifton, D.A., Robertson, N.M.: ProSelfLC: Progressive self label correction for training robust deep neural networks. In: CVPR. (2021)
[5] Yuan, L., Tay, F.E., Li, G., Wang, T., Feng, J.: Revisiting knowledge distillation via label smoothing regularization. In: CVPR. (2020)
[6] Wang, Y., Ma, X., Chen, Z., Luo, Y., Yi, J., Bailey, J.: Symmetric cross entropy for robust learning with noisy labels. In: ICCV. (2019)
[7] Arazo, E., Ortego, D., Albert, P., O'Connor, N., Mcguinness, K.: Unsupervised label noise modeling and loss correction. In: ICML. (2019)
[8] Bucila, C., Caruana, R., Niculescu-Mizil, A.: Model compression. In: KDDM. (2006)
[9] Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural network. In: NeurIPS Deep Learning and Representation Learning Workshop. (2015)
[10] Zhang, Y., Xiang, T., Hospedales, T.M., Lu, H.: Deep mutual learning. In: CVPR. (2018)
[11] Romero, A., Ballas, N., Kahou, S.E., Chassang, A., Gatta, C., Bengio, Y.: Fitnets: Hints for thin deep nets. In: ICLR. (2015)
[12] Ba, J., Caruana, R.: Do deep nets really need to be deep? In: NeurIPS. (2014)
[13] Guo, Q., Wang, X., Wu, Y., Yu, Z., Liang, D., Hu, X., Luo, P.: Online knowledge distillation via collaborative learning. In: CVPR. (2020)
[14] Wu, G., Gong, S.: Peer collaborative learning for online knowledge distillation. arXiv preprint arXiv:2006.04147 (2020)
[15] Furlanello, T., Lipton, Z., Tschannen, M., Itti, L., Anandkumar, A.: Born again neural networks. In: ICML. (2018)
[16] Kim, J., Hyun, M., Chung, I., Kwak, N.: Feature fusion for online mutual knowledge distillation. In: ICPR. (2021)
[17] Chung, I., Park, S., Kim, J., Kwak, N.: Feature-map-level online adversarial knowledge distillation. In: ICML. (2020)
[18] Reed, S., Lee, H., Anguelov, D., Szegedy, C., Erhan, D., Rabinovich, A.: Training deep neural networks on noisy labels with bootstrapping. In: ICLR Workshop. (2015)
[19] Tanaka, D., Ikami, D., Yamasaki, T., Aizawa, K.: Joint optimization framework for learning with noisy labels. In: CVPR. (2018)
10

[20] Han, B., Yao, Q., Yu, X., Niu, G., Xu, M., Hu, W., Tsang, I., Sugiyama, M.: Co-teaching: Robust training of deep neural networks with extremely noisy labels. In: NeurIPS. (2018)
[21] Yu, X., Han, B., Yao, J., Niu, G., Tsang, I.W., Sugiyama, M.: How does disagreement help generalization against label corruption? In: ICML. (2019)
[22] Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z.: Rethinking the inception architecture for computer vision. In: CVPR. (2016)
[23] Pereyra, G., Tucker, G., Chorowski, J., Kaiser, L., Hinton, G.: Regularizing neural networks by penalizing confident output distributions. In: ICLR Workshop. (2017)
[24] Gal, Y.: Uncertainty in Deep Learning. PhD thesis, University of Cambridge (2016) [25] Xia, X., Liu, T., Han, B., Gong, C., Wang, N., Ge, Z., Chang, Y.: Robust early-learning: Hindering the
memorization of noisy labels. In: ICLR. (2021) [26] Jiang, L., Zhou, Z., Leung, T., Li, L.J., Fei-Fei, L.: Mentornet: Learning data-driven curriculum for very
deep neural networks on corrupted labels. In: ICML. (2018) [27] Chen, P., Liao, B.B., Chen, G., Zhang, S.: Understanding and utilizing deep neural networks trained with
noisy labels. In: ICML. (2019) [28] Ma, X., Huang, H., Wang, Y., Romano, S., Erfani, S., Bailey, J.: Normalized loss functions for deep
learning with noisy labels. In: ICML. (2020) [29] Zhang, Z., Sabuncu, M.R.: Generalized cross entropy loss for training deep neural networks with noisy
labels. In: NeurIPS. (2018) [30] Patrini, G., Rozza, A., Menon, A.K., Nock, R., Qu, L.: Making deep neural networks robust to label noise:
A loss correction approach. In: CVPR. (2017) [31] Xia, X., Liu, T., Wang, N., Han, B., Gong, C., Niu, G., Sugiyama, M.: Are anchor points really
indispensable in label-noise learning? In: NeurIPS. (2019) [32] Xu, Y., Cao, P., Kong, Y., Wang, Y.: L_dmi: A novel information-theoretic loss function for training deep
nets robust to label noise. In: NeurIPS. (2019) [33] Yao, Y., Liu, T., Han, B., Gong, M., Deng, J., Niu, G., Sugiyama, M.: Dual t: Reducing estimation error
for transition matrix in label-noise learning. In: NeurIPS. (2020) [34] Wang, X., Hua, Y., Kodirov, E., Robertson, N.M.: IMAE for noise-robust learning: Mean absolute error
does not treat examples equally and gradient magnitude's variance matters. arXiv preprint arXiv:1903.12141 (2019) [35] Wang, X., Kodirov, E., Hua, Y., Robertson, N.M.: Derivative manipulation for general example weighting. arXiv preprint arXiv:1905.11233 (2019) [36] Krizhevsky, A.: Learning multiple layers of features from tiny images. (2009) [37] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR. (2016) [38] Bossard, L., Guillaumin, M., Van Gool, L.: Food-101­mining discriminative components with random forests. In: ECCV, Springer (2014) 446­461 [39] Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical image database. In: CVPR. (2009) [40] Szegedy, C., Ioffe, S., Vanhoucke, V., Alemi, A.: Inception-v4, inception-resnet and the impact of residual connections on learning. In: AAAI. (2017)
11

Supplementary Material for CMD
A Implementation details
A.1 Datasets and data augmentation
· CIFAR100 [36] has 50,000 training images and 10,000 test images of 100 classes. The image size is 32 × 32 × 3. Simple data augmentation is applied following [37], i.e., we pad 4 pixels on every side of the image and then randomly crop it with a size of 32×32.
· Food-101 [38] has 75,750 images of 101 classes. The training set contains real-world noisy labels. In the test set, there are 25,250 images with clean labels. For data augmentation, training images are randomly cropped with a size of 224 × 224.
· Webvision [26] has 2.4 million images crawled from the websites using the 1,000 concepts in ImageNet ILSVRC12 [39]. For data augmentation, we first resize the training images to 320 × 320 and then randomly cropped with a size of 299 × 299.
A.2 Training details
· On CIFAR100, we train on 90% training data (corrupted in synthetic cases) and use 10% clean training data as a validation set to search hyperparameters, e.g., b1, b2. Finally, we retrain a model on the entire training data and report its accuracy on the test data for a fair comparison. We train CIFAR100 on three net architectures including ResNet34, ResNet50, ResNet18 and ShuffleNetV2. For ResNet34, the initial learning rate is 0.1 and then divided by 10 at the 50th and 80th epoch, respectively. The number of total epochs is 100. For ShuffleNetV2 and ResNet28, the initial learning rate is 0.1 and then divided by 5 at the 60th, 120th, and 160th epoch, respectively. We train 200 epochs in total. For all the training, we use an SGD optimizer with a momentum of 0.9, a weight decay of 5e-4, and a batch size of 128. For ResNet50, for a fair comparison, we use the same training settings as [25].
· On Food-101, we also separate the training data into two parts, 90% for training and 10% for validation. We use the validation set to search hyper-parameters. Finally, we report its accuracy on the clean test data. We train ResNet50 (initialised by a pretrained model on ImageNet) using a batch size of 32, due to GPU memory limitation. And we use the SGD as an optimizer with a momentum of 0.9, and a weight decay of 5e-4. The learning rate starts at 0.01 and then is divided by 10 at the 50th and 80th epoch, respectively in total 100 epochs.
· On Webvision, we follow the "Mini" setting in [26]. We take the first 50 classes of the Google resized image subset as the training set and the same 50 classes of the ILSVRC12 validation set as the test set and apply inception-resnet v2 [40] as training architecture with batch size of 32. We use SGD as an optimizer with a momentum of 0.9, and a weight decay of 5e-4. The learning rate starts at 0.01 and then is divided by 10 in each epoch after the 40th epoch with a total number of 80 epochs.
All models are trained on multiple 2080 Ti GPUs between 2 and 4, which is adjusted according to model size and batch size.
B Label noise methods in the literature
We compare with classical and latest label correction methods, including CE, LS, CP, Bootsoft, and ProSelfLC. We use the same training settings for all methods as mentioned in A.2. We also compare with the classical and latest label noise methods, including CE, GCE [29], Co-teaching [20] (maintaining two identical networks simultaneously and transferring small-loss instances to the peer model), Co-teaching+ [21] (transferring the small-loss samples among the disagreement predictions to the peer model), Joint [19], Forward [30] (correcting the training loss by estimating the noise-transition matrix), MentorNet [26] (providing a curriculum for StudentNet to focus on the samples whose labels are probably correct), T-revision [31] (reweighting samples based on importance and revising the noise-transition matrix by a slack variable), S2E [33] (managing noisy labels by automated machine learning), DMI [32] (introducing an information-theoretic loss function), APL
12

[28] (combining two robust loss functions that mutually boost each other), CDR [25] (reducing the side effect of noisy labels before early stopping), and ProSelfLC.
C The core implementation of CMD using PyTorch
class CMDWithLoss(nn.Module): def __init__(self): super(CMDWithLoss , self).__init__( ) def forward(self , qA , qB , pA , pB , threshold): # qA , corrected label from model A # qB , corrected label from model B # pA , knowledge from model A # pB , knowledge from model B # calculate the entropy of pA hpA = torch.sum(-pA * torch.log(pA + 1e-6), 1) # calculate the entropy of pB hpB = torch.sum(-pB * torch.log(pB + 1e-6), 1) threshold_l = threshold * torch.ones(len(hpA)).cuda( ) # select the low entropy sample from model B indexA = (hpB < threshold_l).nonzero( ) # select the low entropy sample from model A indexB = (hpA < threshold_l).nonzero( ) # distill knowledge from model B to model A lossB2A = torch.sum(qB[indexA].squeeze(1) * \ (-torch.log(pA[indexA].squeeze(1) + 1e-6)), 1) # distill knowledge from model A to model B lossA2B = torch.sum(qA[indexB].squeeze(1) * \ (-torch.log(pB[indexB].squeeze(1) + 1e-6)), 1) lossB2A = sum(lossB2A) / len(hpA) lossA2B = sum(lossA2B) / len(hpB) return lossB2A , lossA2B
13

