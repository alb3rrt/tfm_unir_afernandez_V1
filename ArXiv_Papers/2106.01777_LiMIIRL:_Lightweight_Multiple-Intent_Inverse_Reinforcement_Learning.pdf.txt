LiMIIRL: Lightweight Multiple-Intent Inverse Reinforcement Learning

arXiv:2106.01777v1 [cs.LG] 3 Jun 2021

Aaron J. Snoswell School of Information Technology and Electrical Engineering
The University of Queensland a.snoswell@uq.edu.au

Surya P. N. Singh Intuitive Surgical surya.singh@intusurg.com

Nan Ye School of Mathematics and Physics
The University of Queensland nan.ye@uq.edu.au

Abstract
Multiple-Intent Inverse Reinforcement Learning (MI-IRL) seeks to find a reward function ensemble to rationalize demonstrations of different but unlabelled intents. Within the popular expectation maximization (EM) framework for learning probabilistic MI-IRL models, we present a warm-start strategy based on up-front clustering of the demonstrations in feature space. Our theoretical analysis shows that this warm-start solution produces a near-optimal reward ensemble, provided the behavior modes satisfy mild separation conditions. We also propose a MI-IRL performance metric that generalizes the popular Expected Value Difference measure to directly assesses learned rewards against the ground-truth reward ensemble. Our metric elegantly addresses the difficulty of pairing up learned and ground truth rewards via a min-cost flow formulation, and is efficiently computable. We also develop a MI-IRL benchmark problem that allows for more comprehensive algorithmic evaluations. On this problem, we find our MI-IRL warm-start strategy helps avoid poor quality local minima reward ensembles, resulting in a significant improvement in behavior clustering. Our extensive sensitivity analysis demonstrates that the quality of the learned reward ensembles is improved under various settings, including cases where our theoretical assumptions do not necessarily hold. Finally, we demonstrate the effectiveness of our methods by discovering distinct driving styles in a large real-world dataset of driver GPS trajectories.
1 Introduction
Inverse Reinforcement Learning (IRL) methods search for a reward function to rationalize demonstrated behaviour [35]. However, in many cases, these demonstrations consist of different but unobserved behavior modes. For example, taxi drivers may choose different routes even when the pickup location and the destination are the same, upon the passenger's request to minimize the cost or the time (Fig. 1, left). Main approaches to deal with such multiple (unobserved) intent IRL (MI-IRL) problems are Expectation Maximization (EM) [3, 31] or non-parametric methods [8, 1]. We focus on the EM formulation, which requires the number of intents K as a hyper-parameter, but is computationally more practical. Unfortunately, EM algorithms easily get stuck in poor local minima, and thus good initialization is often a subject of algorithmic design and theoretical analysis [38, 4].
For MI-IRL, we similarly observed that the EM algorithm is prone to local minima that may have good likelihood, but fail to learn good reward functions for the demonstrated behaviors. To address
Preprint. Under review.

End

Latitude

1  1 2  2

Start

Longitude

Figure 1: (Left): Multiple behaviour intents in the Porto driver behaviour dataset. Taxi drivers going from the
same pick-up location to the same destination may choose very different routes. (Middle): An illustration of Assumption 1 for K = 3, || = 2, when  = 0. Demonstrations not in cluster Tk are shown to be at least some margin away from Tk when projected onto   Tk. When  > 0, some trajectories in Tk are allowed to be closer to cluster k than cluster k when projected on  . (Right): Our ElementWorld MI-IRL benchmark is a
gridworld problem that wraps around the x-axis. The agent starts in the bottom-most row of the grid world and
must navigate to the top. While en-route, the agent prefers to visit states matching it's desired element.

this, we propose Lightweight Multiple-Intent IRL (LiMIIRL), a simple warm-start strategy that constructs an initial solution by clustering the demonstrations in feature space, and then uses EM to improve the solution. Theoretically, we bound the error of the clustering-based initial solution, which shows that the initial solution can be near-optimal when the behaviors are approximately separable in the feature space. To the best of our knowledge, this is the first provable performance guarantee for an EM initialization in the MI-IRL context. Empirically, our experiments show that LiMIIRL helps avoid poor quality solutions, and allows terminating the EM loop after a very small number of iterations, significantly speeding up the convergence of the EM algorithm as compared to random initializations. These results show that the simple strategy of clustering in the feature space can effectively help grouping demonstrations into clusters with distinct reward objectives.
Another contribution of this paper is a performance metric for MI-IRL algorithms (Section 4). Our metric allows direct comparison (without reference to a dataset) between two sets of multiple-intent behaviors, each represented by an ensemble of reward functions and is efficiently computable by solving a minimum cost flow problem.
In the remainder of this paper, we present LiMIIRL with a theoretical analysis (Section 3) and introduce a new MI-IRL metric (Section 4). We demonstrate the effectiveness of LiMIIRL and analyse its sensitivity w.r.t. problem and algorithm hyper-parameters on a new MI-IRL benchmark, which allows for more comprehensive testing of MI-IRL algorithms (Section 5). LiMIIRL is also shown to effectively discover distinct driving behaviours in a real-world dataset of taxi driving trajectories (Section 6). We elaborate context in both the background and related work (Sections 2 and 7). Our code is available at [URL removed for blind review].

2 Background

We consider behaviours generated by one or more experts acting in some environment. We model the environment as a partial Markov Decision Process (MDP) M = S, A, p0, T,  with state space S and action space A, discount factor   [0, 1), starting state probabilities p0(s) and a transition function T (s, a, s ) = p(s | s, a). In MI-IRL, we are given a dataset of demonstration trajectories D = {1, . . . , N } collected when an ensemble of K experts interact with the environment to
optimize their individual reward functions Rk(s, a, s ) k (s, a, s ). Formally, we assume each trajectory  is sampled from p( | 1:K , 1:K ) = k=1 k k( ), where {i}Ki=1 are positive mixture weights, and each expert i is represented as a distribution on T , the set of all trajectories of length at most L. We work in the unsupervised setting, where the trajectories are not labeled with the
corresponding behavior mode.

Given a transition-based feature function  : S × A × S  R, we define its extension to trajectories

by ( ) =

| |-1 t=1

t-1(st, at, st+1),

where



=

(s1, a1, s2, . . . , s||).

We use D to learn an ensemble of models p( | 1:K , 1:K ) =

K k=1

k

p(

|

k), where

{1:K , 1:K } are the parameters with each i  0 and i i = 1, and p( | k) is a distribu-

tion on T with parameters k. The parameters {1:K , 1:K } are chosen to maximize the likelihood

2

p(D | 1:K , 1:K ). The EM algorithm, first applied to MI-IRL by [3], starts from some initial parameter estimates {(10:K) , 1(0:K) }, and then iteratively improves the parameter estimates until convergence.

At iteration t, EM updates the parameters as follows

u(ikt+1) =

(kt)p(i | k(t)) , k (kt)p(i | k(t))

(kt+1)

=

1 N

u(ikt+1),
i

k(t+1) = arg max u(ikt+1)p(i | ).



i

We say that a solution t = {(1t:)K , 1(t:K) } is

-optimal

if

1 |D|

i

k |u(ikt+1) - u(ikt)| < . Intuitively,

a small indicates that the EM algorithm has more or less converged to a local optimum.

3 The LiMIIRL Algorithm

Our LiMIIRL algorithm provides a simple way to generate good initial parameter estimates (0) =
((10), . . . , (K0), 1(0), . . . , K(0)) by clustering demonstrations in feature space. The initial solution is generated in three steps.

First, LiMIIRL runs a user-specified clustering algorithm on the feature vectors of the training

trajectories to obtain a partition {C1, . . . , CK } of D. Second, LiMIIRL estimates 1:K using (k0) =

N i=1

u(ik0)/N ,

where

the

membership

variable

u(ik0)

=

I(i



Ck )

denotes

whether

trajectory

i

belongs

to cluster k. Finally, LiMIIRL estimates 1(0:K) . We consider two ways to do this, mean initialization,

or Maximum Likelihood Estimate (MLE) initialization, given respectively by,

k(0,M) ean = k

1 |Ck| Ck ( ),

k(0,M) LE = arg max

ln p( | ).

(1)



 Ck

In our algorithm, p( | ) can be any single-intent IRL model, and we experimented with the MaxEnt IRL model [41], Maximum Likelihood IRL [3], and -Gradient IRL [31].

Below, we provide a theoretical justification of why clustering in feature space is an effective

choice for initializing an ensemble of behaviours based on the MaxEnt IRL model, which has

exact and computationally efficient solution methods available [36]. Specifically, this entails

choosing p( | ) = q( )e() /Z(), where q( ) = p0(s1)

| |-1 t=1

p(st+1

|

st, at), and

Z() =  T q( )e ( ) is the partition function.

Interestingly, our empirical results suggest that this warm-start technique also provides a good initial reward ensemble under alternate behaviour models, such as the Boltzmann model in ML-IRL and BIRL [3, 30] and the gradient-based model in -GIRL [31].

3.1 Theoretical Analysis
To be able to discern different behavior intents in the demonstration data, the intents must be distinguishable. When representing the demonstrations as feature vectors, this amounts to requiring the feature vectors for typical/characteristic trajectories from each expert to form clusters that are mostly distinct from each other, and the atypical/ambiguous trajectories can be different but not too different from the typical trajectories. Specifically, to formulate this intuition, we say that a trajectory  -separates two sets of trajectories Ta and Tb (in the feature space) if ( ) ((a) - (b))   for all a  Ta and   Tb. Geometrically, when  > 0, the trajectories in Ta and Tb are separated by a margin of at least / ( ) when projected on  in the feature space. That is, we can find a SVM with a margin of at least / ( ) to separate trajectories in Ta and Tb. When   0, the projections of Ta and Tb on  may overlap over an interval of length ||/ ( ) .
Assumption 1 (Approximate cluster separation). Let Tk be the set of trajectories that can be generated by expert k, and Tk = T - Tk. We assume that there exists   [0, 1/2) such that each Tk can be partitioned into two sets: a set of typical trajectories Tk+ and a set of atypical trajectories Tk- such that Q+k = Tk+ q( )  (1 - )Qk, or equivalently, Q-k = Tk- q( )  Qk, where Qk = Tk q( ). In addition, the typical trajectories and atypical trajectories satisfy the following properties
(a) There exist d,  > 0, such that for any k  [K],

3

(i) each   Tk+ d-separates Tk+ and Tk, and -separates Tk+ and Tk-. (ii) each   Tk- (-d)-separates Tk+ and Tk, and (-)-separates Tk+ and Tk-.
(b) k  [K],  rk < minTk+ d/2 ( ) s.t. ( ) - ( )  2rk for all ,   Tk+. That is, the trajectories in Tk+ are contained in a ball of radius rk in the feature space.

Intuitively, we assume that a typical trajectory in a cluster can be used to distinguish typical trajectories from atypical trajectories and trajectories in other clusters with margins  and d respectively. On the other hand, an atypical trajectory can fail to do so, but the overlaps are upper bounded by  and d respectively, so that they cannot be too different from the typical trajectories -- our analysis can be easily extended to alternative choices of the upper bounds, but we have chosen  and d for simplicity. Fig. 1 (middle) provides an illustration of Assumption 1 for the case when  = 0. In practice, a useful trick is to shift the features so that the mean feature vector is 0. This does not change the parameters of the optimal MaxEnt IRL model, but allows us to exploit the clusters in the data via Assumption 1.

With mostly-distinct clusters, we can reasonably expect a clustering algorithm to be able to discover the underlying clusters in the dataset with high accuracy.
Assumption 2 (Approximately correct clustering). There exists   [0, 1/2), such that at least 1 -  of the trajectories in Ck are in Tk+. In addition, the other trajectories (-d)-separate Tk+ and Tk , and (-)-separate Tk+ and Tk-.

Intuitively, most the trajectories in D are typical, and the clustering algorithm is able to cluster most of them correctly. However, the algorithm may fail on some trajectories, and these trajectories are assumed to be not worse than the atypical trajectories in terms of their separation ability.

To quantify the quality of the mean initialization, we first observe that k is like a typical trajectory in Tk (all proofs are in the supplementary file).
Lemma 1. For any k  [K], k d~-separates Tk+ and Tk for d~ = (1 - 2)d. In addition, k ~-separates Tk+ and Tk- for ~ = (1 - 2).

Our second observation is that p( | k) is large for typical trajectories in Tk, and small for trajectories not in Tk.

Lemma 2.

For any 

 Tk+, we have p(

| k) 

(1-)q( )ed~-2rk k (1- )Qk ed~+Qk

(1-)e~ +(1-)e~

,

where

Qk

=

 /Tk

Qk

.

In

addition,

for

any



/

Tk ,

we

have

p(

|

k )



(1-

q( ) )Qk ed~+q(

)

.

With the above two observations, we can then quantify the quality of the mean initialization.

Theorem 1. The mean initialisation LiMIIRL initial estimates {(10:K) , 1(0:K) } are -optimal for =

2

+

(1

-

)

2(K -1)  +K -1

,

where



=

mink=k

e (1- )2 e~

|Ck|((1-)Qk ed~)

d~-2rk k

+(1-)e~ |Ck |((1-)Qked~+Qk)

.

The bound in Theorem 1 implies that the quality of the warm-start solution improves when the
separability of the behaviors and the quality of the clustering algorithm are higher. Specifically, noting that d~ = (1 - 2)d and ~ = (1 - 2), we can see that is small when  is small (smaller proportion of atypical trajectories), d is large (more distinct behaviors), rk's are smaller (more concentrated clusters),  is large (larger difference between typical and atypical trajectories), and  is small (more accurate clustering algorithm). In addition, converges to  at a rate exponential in d.

For the MLE initialization, obtaining a closed-form error bound is much more involved. However,
we highlight that the above analysis also provides an insight that when the scaled margin is large, the
MLE initialization would work well too. To see this, note that cluster k is roughly contained within a hyper-cone having the origin as the vertex and an opening angle  = sin-1(rk/ k ). Any k becomes less likely to be optimal as it forms an angle of more than 2 with k, as follows: when
k's angle with k changes from 0 to more than 2, for any   Tk, e() k decreases, because the angle between ( ) and k changes from at most  to at least . On the other hand, there is an empty space with margin dk = d/ k below the cluster, thus trajectories not in Tk initially form large angles with k, and have a small contribution to the partition function, then gradually become more important as k moves away from k, eventually making the probability of trajectories in Tk small and the likelihood of k small.

4

4 MI-IRL Performance Metrics

We introduce our novel metric to evaluate an MI-IRL algorithm's performance in terms of the quality of the learned reward ensemble. We also evaluate an MI-IRL algorithm's clustering performance and describe the well-known Adjusted max-Normalized Information Distance (ANID) for completeness.
Reward Ensemble Performance. For single-intent IRL problems the Expected Value Difference (EVD) is an appropriate measure of regret of a learned reward RL w.r.t. a true reward function RGT, EVD(RGT, RL) Esp0 [vR GT (s) - vR L (s)], where v is the value function for policy , and R is the optimal policy w.r.t. reward function R [7, 21]. Previous authors applied this metric in the multiple-intent setting by averaging the EVD of the true and learned rewards for each trajectory [3, 8] ­ this process does not directly compare the reward ensembles but instead depends on a sample of demonstrations.

We propose a Generalized EVD (GEVD) metric that directly evaluates a learned reward ensemble {L1:K , R1L:K } against the ground truth reward ensemble {G1:TK , R1G:TK }. To motivate, note that a reward ensemble remains the same even if the rewards are reordered, thus for the special case where
two reward ensembles are the same, we can perform a bipartite matching to pair up the rewards.

We generalize this idea to handle the harder cases in which K = K , and the weights may not be
the same. Essentially, this requires an additional reward split step before pairing up the rewards.
Instead of just finding the optimal pairing as in the simplest case above, we now need to find the best way to split-and-pair rewards. This can be naturally formulated as follows, GEVD(RGT, RL) arg min{wij}: j wij=Gi T, i wij=Lj i,j wij eij where eij = EVD(RiGT, RjL). It is easy to show that the above problem is equivalent to a min-cost flow problem (see our note in the Supplementary file),
thus the GEVD can be efficiently computed using standard graph manipulation libraries.

The range of GEVD is

0,

maxk k 1-

, where k is the difference between the maximum and minimum

possible rewards under the k-th ground truth reward reward function. GEVD is 0 iff there is a

way to split-and-pair ground truth and learned rewards such that each pair of rewards lie in an

equivalence class (in the sense of having the same optimal policies), and depends only on the reward

ensembles.

We

can

normalize

GEVD

into

the

range

of

[0, 1]

by

dividing

it

with

maxk k 1-

,

however

a better normalization can be achieved by dividing it with a tighter upper bound i Gi Tei , where ei = Esp0 [vR GT (s) - vR-GT (s)], with R- being the policy minimizing the reward R. We can calculate R- using the same algorithm for computing R (e.g. value iteration), so the normalization

only increases the computation time by a small factor. We believe the GEVD measure provides an

elegant means for evaluating arbitrary reward ensembles in controlled IRL experiments where we

have access to the MDP model, and we believe this is the first such metric for the MI-IRL problem

that does not require reference to a held-out testing trajectory dataset. Furthermore, the GEVD

actually provides a useful tool to diagnose poorly performing MI-IRL solutions by inspecting reward

pairings with large EVDs, naturally guiding the practitioner toward parts of the model that may

require attention.

Clustering Performance. MI-IRL requires both reward ensemble learning, and demonstration

clustering ­ both tasks should be evaluated in holistic evaluation of any MI-IRL method. The

Adjusted max-Normalized Information Distance (ANID) is a popular and theoretically justified

performance metric for assessing a mixture model's (hard or soft) clustering capability [37]. Given

a learned MI-IRL mixture model, we can compute its responsibility matrix U = {uik} where

uik is the probability that demonstration i belongs to cluster k. Similarly, we can compute the responsibility matrix V = {vik } for the ground-truth MI-IRL mixture model. The ANID is given by

ANID(U , V ) 1 - (I(U , V ) - E[I(U , V )]) / (max{H(U ), H(V )} - E[I(U , V )]), where

U and V are random responsibility matrices of the same dimensions as U and V respectively

distributed according to an appropriate random clustering model,1 I(U , V ) is the mutual information

between k, k

following a joint distribution p(k, k ) =

1 N

N i=1

uik vik

,

and

H(U )

and

H (V

)

are

respectively the entropies of p(k) and p(k ), the two marginal distributions of p(k, k ). The ANID

has many attractive properties: it is symmetric w.r.t. to U and V , is 0 iff U = V modulo column

re-ordering, is computationally simple to calculate, controls for random chance agreement between

1For hard clusters, hypergeometric models are commonly used. In our soft clustering case each row of U is independently sampled from the Dirichlet distribution Dir(1/K, . . . , 1/K), and similarly for V .

5

Table 1: LiMIIRL experimental results for EW ­ ElementWorld, and DB ­ Driver Behaviour forecasting. Values are mean ± 95% confidence interval over 100 (EW), 8 (DB) repeat experiments with randomized seeds. Lower is better for all metrics.

Algorithm

Iterations Duration (s,m)

NLL

ANID

GEVD

EW

LiMIIRL-MLE LiMIIRL-Mean
Random Supervised

3.41 ± 01.40 5.08 ± 01.13 17.22 ± 04.80
N/A

33.20 ± 13.49 21.86 ± 06.65 136.58 ± 39.69
N/A

19.65 ± 00.64 19.76 ± 00.45 20.05 ± 00.71 19.65 ± 00.66

0.03 ± 00.01 0.04 ± 00.01 0.13 ± 00.07 0.04 ± 00.01

3.70 ± 01.11 4.49 ± 00.98 6.75 ± 02.09 3.65 ± 01.27

DB

LiMIIRL-MLE 1.17 ± 00.60 68.29 ± 34.01 305.55 ± 01.20

N/A

N/A

LiMIIRL-Mean 4.17 ± 01.10 243.28 ± 64.22 303.99 ± 03.17

N/A

N/A

Random 3.33 ± 01.51 198.40 ± 79.69 305.37 ± 03.17

N/A

N/A

clusters when K is large, is stochastically normalized to the range [0, 1], and in the limit N   converges to a proper metric [37].
5 ElementWorld Benchmark Experiment
Previous MI-IRL methods have used a simple two-intent puddle gridworld environment for evaluation [3, 31]. To investigate the scalability of our method on a more substantial problem, we generalized this environment to the case of many intents, creating a benchmark we call ElementWorld. ElementWorld is a randomized gridworld environment with a cylindrical topology (Fig. 1, left). An agent starts uniform randomly in the bottom row of the map, and must navigate to any of the terminal goal states at the top of the map. Between the start and goal areas, we generate `lanes' corresponding to each of E elements (e.g. water, dirt, grass), which are randomly perturbed horizontally, forcing the agent to avoid obstacles, leading to non-trivial demonstrations. The agent is randomly assigned one of E possible intents, and for the kth intent, we set the ground truth reward parameters for all j = k elements to -10, the goal to 0, and the kth element and the start zone to -1. For actions, the agent chooses from the four cardinal directions, however this fails (and a random choice is made) with a wind probability w. We use an E + 2 dimensional state feature function (s) = (Start, Goal, Element1, . . . , ElementE), which is a one-hot representation of the type of cell the agent is in.
By adjusting the number of elements E and the wind probability w we can vary the difficulty of the MI-IRL behaviour clustering sub-problem: w  0 leads to almost disjoint intent clusters in feature space, while w  1 leads to almost completely overlapping intents in feature space. Likewise, by adjusting the height h of the map, we change the length of the demonstrations, adjusting the difficulty of MI-IRL reward learning sub-problem.
5.1 ElementWorld Results
Experimental configuration. We performed an experiment to validate our LiMIIRL algorithm. With E = 3, w = 0.1, h = 6,  = 0.99 we used Q-value iteration to find an optimal stochastic policy for each behaviour intent, then sampled uniformly from the policy ensemble for a dataset of N = 100 demonstrations. Using a MaxEnt IRL reward model we compared the performance of the EM algorithm with random initialization [3] to our LiMIIRL method with k-means clustering, with either mean, or MLE reward initialisation. As a baseline, we also included a supervised reward ensemble that is provided with the labels mapping demonstrations to elements, and the EM MI-IRL algorithm with random initialisation, as used in [3]. For this initial experiment we assumed the true number of elements was known a priori (i.e. K = E). For each experiment, we recorded the number of iterations and wall time to EM convergence, the Negative Log Likelihood (NLL) and clustering performance (ANID) on a held-out testing set of 100 demonstrations, as well as the reward ensemble quality (GEVD). Every experiment was repeated 100× with different randomly generated ElementWorld instances to allow estimating the mean and 95% confidence interval for each metric. No significant difference between MLE and mean initialisation. The results are summarized in Table 1, with corresponding box-plots included in the supplementary data file. The results confirm our hypothesis that LiMIIRL is able to identify good starting points for the MI-IRL EM algorithm, at least for this problem instance which satisfies our theoretical assumptions. The results also show that MLE and mean reward initialisation are roughly equivalent in this case ­ both empirically seem to
6

identify good starting points for the MI-IRL problem, with no statistically significant difference in the performance of these methods. In general however, we have observed the MLE initialization seems more robust when there is large deviation from our theoretical assumptions, an idea we investigate further in the driver behaviour experiment below.
LiMIIRL avoid local-minima EM solutions. With hard clustering, LiMIIRL learns a better reward ensemble than the EM algorithm with random initialisation (90% relative reduction of ANID, 45% relative reduction in GEVD), despite the solutions having equivalent Negative Log Likelihood. This reveals an interesting property of the MI-EM algorithm ­ the optimization landscape of this problem has many local minima which appear equal or nearly equal in NLL, but vary greatly in terms of actual performance, highlighting the importance of a good initialization point for the ensemble.
LiMIIRL leads to faster EM convergence. Our key finding however is that LiMIIRL leads to the EM algorithm reaching convergence significantly faster. On average, LiMIIRL reduces the number of iterations and running time by around 80% and 75% respectively with k-means initialization, and 30% and 73% respectively with GMM initialization. Comparison with the corresponding Iteration and Duration box-plots reveals that the convergence metric results are heavy tailed ­ especially when random initialisation is used, further supporting the idea that a good starting point is critical for this problem.
These results provide hope that LiMIIRL will be especially helpful on larger problems where each EM iteration is itself a computationally expensive procedure, provided we have reason to believe the behaviour intents are at least approximately separated in feature space.
5.2 Sensitivity Analysis
We conducted extensive additional experiments to investigate the sensitivity of our algorithm under additional problem variations, especially cases where the theoretical assumptions are violated. We evaluated our algorithm under scenarios with varying numbers of demonstrations, degree of cluster separation in feature space, number of learned clusters, number of ground truth elements, cluster imbalance, initial clustering method (hard k-means vs. soft Gaussian Mixture Models), as well as with a different behaviour models (ML-IRL, and -GIRL [3, 31]). For each algorithm and each problem instance, we performed over 20 repeat experiments with randomized ElementWorld seeds to generate means and 95% confidence intervals. For brevity, we detail the nature of each parameter sweep and report the full results as figures in the supplementary file, however we briefly describe a few key findings here.
Hard-clustering vs. soft-clustering. We observed that in general, hard clustering with KMeans always tends to out-perform soft clustering with a GMM. Surprisingly, this is true even as the problem dynamics are made more stochastic (increasing wind factor w, which causes the intent clusters to intermingle in feature space). We hypothesise that the hard clustering leads to less mis-classification of training demonstrations, which may lead to higher-quality rewards to be learned. Encouragingly, hard clustering always performs on-par with the supervised baseline, with the exception of the case of large numbers of ground truth elements E, where there is a performance difference.
Number of learned clusters. As our algorithm is parametric, we require that the number of clusters K be specified a priori. In experiments where we varied the number of learned clusters, keeping E = 3 fixed, we observe that the mixture NLL generally increases proportional to K, however the ANID and GEVD metrics reach a minima / asymptote respectively at K = 3 ­ this confirms that our metrics are a good measure of ensemble performance, even for the case when the number of learned clusters is mis-specified. We note that in practice, the value of K can be estimated using heuristics such as the Bayesian Information Criteria, using a non-parametric clustering as a pre-processing step, or using cross-fold validation.
Cluster Imbalance. Finally, we also tested the case of non-balanced behaviour intents in the dataset. Encouragingly, LiMIIRL appears able to learn accurate ensembles in this case, correctly allocating less probability mass to the rarer behaviour intents.
Alternate IRL models. LiMIIRL appears to work with ML-IRL and -GIRL as the IRL model too, however due to the computational complexity of these methods we did not include them in our driver forecasting experiment, below.
7

6 Driver Forecasting Experiment
We demonstrate the merit of our method by application to a large driver behaviour dataset, consisting of Taxi GPS trajectories collected between 2013­14 in the city of Porto, Portugal [26, 12]. Our data pre-processing follows that used in [41, 36]: a particle filter was used to discretize the GPS trajectories to the road network, giving a deterministic MDP with 292,604 states (road segments) and 594,933 actions (turns at intersections). As state features, we used the type of road (highway, local street etc.), the number of lanes, as well as a geographical indicator for the region of the city where that road segment is located (Central, North, North east, etc.) The feature function was obtained by concatenating the individual one-hot feature vectors and multiplying by the length of the road segment in km, resulting in a 24 dimensional state-based feature vector. We trained all models on paths  300 road segments in length, and used held-out test sets for evaluation.
We qualitatively observed the presence of multiple behaviour intents in this dataset (e.g. Fig. 1, left), however we have no ground truth knowledge of the number of clusters K. Instead, as a pre-processing step we used a stick-breaking non-parametric GMM in feature space to estimate the appropriate number of clusters for this dataset as K  3. To accelerate the training process, we only perform a partial maximization at the M-step, by truncating each M-step of the EM algorithm after 50 objective function evaluations. Provided the learned solution is a monotonic improvement in objective than that of the previous iteration the convergence conditions for the EM algorithm will still be satisfied [9, 30]. Experiments were performed on a 12-core Ubuntu workstation with Intel Xeon E5-2760 v3 CPUs. We summarize the number of iterations and wall time to EM convergence, as well as the final mixture NLL in Table 1. Each value is a mean and ±95% confidence interval across 8 folds of cross validation (1000 / 8000 randomly sampled train / test demonstrations for each fold).
The results align with that of ElementWorld ­ LiMIIRL with MLE initialisation learns mixtures with similar NLL to random initialisation, however reaches convergence significantly faster - a saving of between 2.16 and 2.33 EM iterations, corresponding to a non-trivial reduction in training time of between 130 and 135 minutes. We note that the Mean initialisation method does not reduce the convergence time for this problem, which we attribute to the fact that the cluster separation assumption is substantially violated in this dataset.
We also investigate the qualitative behaviour models learned by the algorithms. The learned reward ensembles were largely similar regardless of initialisation method, with a few exceptions (see supplementary file). Interestingly, we are also able to identify qualitatively meaningful trends within the learned ensembles ­ e.g. it appears a majority (85% ­ 90%) of taxi drivers in this dataset share in a preference for higher speed limits, and most prefer navigating on routes through the northern and western suburbs ­ perhaps due to the presence of a large ring-road highway that spans this area. The remaining reward functions differed in the geographic and road type preferences - e.g. preferring eastern suburbs and/or road types other than highways. This demonstrates that LiMIIRL can be applied on a large, real world dataset to efficiently learn qualitatively meaningful behaviour models in the form of IRL reward ensembles.
7 Related Work
Alternate IRL formulations. IRL is an active research area, with fundamental methods being extended in multiple directions. Our work is complementary to many such extensions, including works considering multiple interacting agents [27], or agents with multiple sub-goals [24, 25, 11], skills [32], or options [17], or using adversarial reward learning to scale to high dimensional problems [13], or using specialised divergence to match a single intent in a multi-intent dataset [18]. Unlike these works, we consider the problem of learning multiple rewards from a dataset containing several unlabeled behaviour intents [3]. We highlight that a related but distinct problem is that of Meta-IRL, where the goal is to learn a good meta-reward prior [14, 39] or shared base reward [6] that can be used to learn new rewards efficiently. Multiple-Intent IRL methods. In the MI-IRL setting, methods can be partitioned into parametric (known number of clusters K) and non-parametric approaches (K unknown). While initialisation strategies such as ours may be helpful in Bayesian non-parametric models [7] or hierarchical nonparametric clustering schemes [1], we leave exploration of this avenue to future work, instead focusing on parametric methods modelled on the Expectation Maximization framework [9]. Specifically, our work is based on the EM-based MI-IRL approach of [3], which has also been extended to a
8

full Bayesian treatment [10], and to use model-free gradient based IRL methods [31]. While our theoretical analysis uses the MaxEnt IRL behaviour model [41, 36], our empirical experiments with the ML-IRL model from [3] and the -GIRL model of [31] suggest that our initialisation method is effective with a range of behaviour models.
Initialisation methods for Expectation Maximization. Our initialization method is designed to alleviate the problem of learning local minima solutions -- an issue that the family of EM algorithms are well-known to suffer from (as well as slow convergence rates) [9, 38, 4]. As such, the study of good initialisation methods for specific EM algorithms is an ongoing area of research [38, 29, 28], as is work on finding alternate update rules to address the same issues [38, 23]. Our work relates to these investigations, however we focus on EM algorithms that are specific to clustering problems, rather than general data-completion problems -- specifically, we consider intent clustering as encountered in MI-IRL.
IRL evaluation measures In addition to developing an initialisation method for EM-based MI-IRL, we propose a novel metric for evaluating the quality of MI-IRL reward ensembles with respect to a known ground-truth ensemble. Our method extends the popular Expected Value Difference (EVD) and Inverse Learning Error (ILE)2 metrics for comparing individual reward pairs [21, 7]. Specifically, our GEVD metric generalize these measures to the case of hard or soft reward ensembles with potentially different sizes. Previous approaches for evaluating the quality of reward ensembles have suffered from two main limitations - they depend on comparing the likelihood or return of a specific test trajectory dataset [3, 1], or if not, they require manually matching learned rewards with the corresponding ground truth reward [10, 31]. In contrast, our method does not require a held out testing dataset, and is able to automatically, elegantly and efficiently solve the ensemble matching problem through a min-cost-flow formulation. Recently, there has been renewed interest in methods for comparing reward pairs (i.e. the single-intent IRL problem) without requiring policy evaluation [15]. These reward pseudo-metrics are complementary to our work, in that a straight-forward application of the min-cost-flow formulation outlined in Section 4 can also extend these evaluation standards to the Multiple-Intent case. However, while such pseudo-metric can be theoretically and computationally attractive, enforcing symmetry for the learned reward and the ground-truth reward in the metric may be undesirable in the IRL setting, which is asymmetric in nature, as in general the effect of using a reward R on a domain with reward R is different from the effect of using a reward R on a domain with reward R. For example, if R is a constant reward function, and R is a sparse reward function, then using R in a domain with reward function R does not lead to sub-optimal behavior, while using R in a domain with reward function R is usually sub-optimal.
8 Conclusion
We presented a simple warm-start strategy for learning MI-IRL models and demonstrate its effectiveness with both theoretical and empirical analysis. In addition, we developed a novel performance metric, GEVD, for evaluating a learned reward ensemble against a ground-truth one. These contributions further MI-IRL research by developing simple strategies that can make MI-IRL methods more efficient and practical, with insight on when the methods can work well.
GEVD provides a natural and direct way to measure the quality of a learned reward ensemble, but is limited to use with benchmark problems where the ground truth reward ensemble is known, and policy iteration and evaluation can be used to compute the regret of policies. Further research could develop metrics that enjoys the benefits of GEVD but circumvents these limitations.
Our warm-start strategy requires only mild assumptions, and enjoys provable provable theoretical guarantee when using the mean initialization and the MaxEnt IRL behaviour model. Interestingly, empirical results suggest that the idea is effective for the MLE initialisation and some other IRL models. Theoretical results for these cases are non-trivial but will be an interesting area for future work. Another interesting area for future work is exploring if initialisation methods such as ours can also be beneficial in parametric latent variable adversarial models that are gaining interest in the Imitation Learning literature where policy learning, not reward learning is the goal [22, 16]. Finally, nonlinear rewards, particularly those represented by deep neural networks, have been explored in various IRL works (e.g., see [13]). We can also extend the idea in this paper to provide a warm-start solution when a pre-trained feature map is available.
2The ILE is equivalent to EVD with a specific choice of state distribution.
9

Acknowledgments and Disclosure of Funding
Aaron Snoswell is supported by through an Australian Government Research Training Program Scholarship.
References
[1] Javier Almingol and Luis Montesano. Learning multiple behaviours using hierarchical clustering of rewards. In 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 4608­4613, September 2015. doi: 10.1109/IROS.2015.7354033.
[2] Derek T. Anderson, James C. Bezdek, Mihail Popescu, and James M. Keller. Comparing Fuzzy, Probabilistic, and Possibilistic Partitions. IEEE Transactions on Fuzzy Systems, 18(5):906­918, 2010.
[3] Monica Babes¸-Vroman, Vukosi Marivate, Kaushik Subramanian, and Michael L. Littman. Apprenticeship Learning About Multiple Intentions. In Proceedings of the 28th International Conference on Machine Learning, ICML '11, pages 897­904, Bellevue, WA, USA, 2011. ACM, New York, NY, USA. ISBN 978-1-4503-0619-5.
[4] Sivaraman Balakrishnan, Martin J Wainwright, and Bin Yu. Statistical guarantees for the EM algorithm: From population to sample-based analysis. arXiv preprint arXiv:1408.2156, 2017.
[5] Kenneth Bogert, Jonathan Feng-Shun Lin, Prashant Doshi, and Dana Kulic. Expectationmaximization for inverse reinforcement learning with hidden data. In Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems, pages 1034­1042, 2016.
[6] Letian Chen, Rohan Paleja, Muyleng Ghuy, and Matthew Gombolay. Joint goal and strategy inference across heterogeneous demonstrators via reward network distillation. In Proceedings of the 2020 ACM/IEEE International Conference on Human-Robot Interaction, pages 659­668, 2020.
[7] J. D. Choi and Kee-Eung Kim. Inverse Reinforcement Learning in Partially Observable Environments. Journal of Machine Learning Research, 12:691­730, 2011.
[8] Jaedeug Choi and Kee-Eung Kim. Nonparametric Bayesian Inverse Reinforcement Learning for Multiple Reward Functions. In Advances in Neural Information Processing Systems 25, NIPS '12, pages 305­313, Lake Tahoe, United States, 2012. Curran Associates, Inc.
[9] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum Likelihood from Incomplete Data via the EM Algorithm. Journal of the Royal Statistical Society. Series B (Methodological), 39(1): 1­38, 1977.
[10] Christos Dimitrakakis and Constantin A. Rothkopf. Bayesian multitask inverse reinforcement learning. In European Workshop on Reinforcement Learning, pages 273­284. Springer, 2011.
[11] Yiming Ding, Carlos Florensa, Mariano Phielipp, and Pieter Abbeel. Goal-conditioned imitation learning. arXiv preprint arXiv:1906.05838, 2019.
[12] Dheeru Dua and Casey Graff. UCI machine learning repository. 2017.
[13] Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse reinforcement learning. 2018.
[14] Adam Gleave and Oliver Habryka. Multi-task maximum entropy inverse reinforcement learning. arXiv preprint arXiv:1805.08882, 2018.
[15] Adam Gleave, Michael Dennis, Shane Legg, Stuart Russell, and Jan Leike. Quantifying differences in reward functions. arXiv preprint arXiv:2006.13900, 2020.
[16] Nate Gruver, Jiaming Song, Mykel J. Kochenderfer, and Stefano Ermon. Multi-agent adversarial inverse reinforcement learning with latent variables. In Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems, pages 1855­1857, 2020.
10

[17] Peter Henderson, Wei-Di Chang, Pierre-Luc Bacon, David Meger, Joelle Pineau, and Doina Precup. OptionGAN: Learning joint reward-policy options using generative adversarial inverse reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.
[18] Liyiming Ke, Matt Barnes, Wen Sun, Gilwoo Lee, Sanjiban Choudhury, and Siddhartha Srinivasa. Imitation Learning as f-Divergence Minimization. arXiv preprint arXiv:1905.12888, 2019.
[19] Yang Lei, James C. Bezdek, Jeffrey Chan, Nguyen Xuan Vinh, Simone Romano, and James Bailey. Generalized information theoretic cluster validity indices for soft clusterings. In 2014 IEEE Symposium on Computational Intelligence and Data Mining (CIDM), pages 24­31, December 2014. doi: 10.1109/CIDM.2014.7008144.
[20] Yang Lei, James C. Bezdek, Jeffrey Chan, Nguyen Xuan Vinh, Simone Romano, and James Bailey. Extending Information-Theoretic Validity Indices for Fuzzy Clustering. IEEE Transactions on Fuzzy Systems, 25(4):1013­1018, August 2017. ISSN 1941-0034. doi: 10.1109/TFUZZ.2016.2584644.
[21] Sergey Levine, Zoran Popovic, and Vladlen Koltun. Nonlinear Inverse Reinforcement Learning with Gaussian Processes. In Advances in Neural Information Processing Systems 24, pages 19­27, Granada, Spain, 2011. Curran Associates, Inc.
[22] Yunzhu Li, Jiaming Song, and Stefano Ermon. InfoGAIL: Interpretable imitation learning from visual demonstrations. arXiv preprint arXiv:1703.08840, 2017.
[23] Gonzalo Mena, Amin Nejatbakhsh, Erdem Varol, and Jonathan Niles-Weed. Sinkhorn EM: An expectation-maximization algorithm based on entropic optimal transport. arXiv preprint arXiv:2006.16548, 2020.
[24] Bernard Michini and Jonathan P. How. Bayesian nonparametric inverse reinforcement learning. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 148­163. Springer, 2012.
[25] Bernard Michini, Thomas J. Walsh, Ali-Akbar Agha-Mohammadi, and Jonathan P. How. Bayesian Nonparametric Reward Learning From Demonstration. IEEE Transactions on Robotics, 31(2):369­386, April 2015. ISSN 1941-0468. doi: 10.1109/TRO.2015.2405593.
[26] Luis Moreira-Matias, Joao Gama, Michel Ferreira, Joao Mendes-Moreira, and Luis Damas. Predicting taxi-passenger demand using streaming data. IEEE Transactions on Intelligent Transportation Systems, 14(3):1393­1402, 2013.
[27] Sriraam Natarajan, Gautam Kunapuli, Kshitij Judah, Prasad Tadepalli, Kristian Kersting, and Jude Shavlik. Multi-Agent Inverse Reinforcement Learning. In 2010 Ninth International Conference on Machine Learning and Applications, pages 395­400, December 2010. doi: 10.1109/ICMLA.2010.65.
[28] Adrian O'Hagan and Arthur White. Improved model-based clustering performance using Bayesian initialization averaging. Computational Statistics, 34(1):201­231, March 2019. ISSN 1613-9658. doi: 10.1007/s00180-018-0855-2.
[29] Adrian O'Hagan, Thomas Brendan Murphy, and Isobel Claire Gormley. Computational aspects of fitting mixture models via the expectation­maximization algorithm. Computational Statistics & Data Analysis, 56(12):3843­3864, December 2012. ISSN 0167-9473. doi: 10.1016/j.csda. 2012.05.011.
[30] Deepak Ramachandran and Eyal Amir. Bayesian inverse reinforcement learning. In IJCAI, volume 7, pages 2586­2591, 2007.
[31] Giorgia Ramponi, Amarildo Likmeta, Alberto Maria Metelli, Andrea Tirinzoni, and Marcello Restelli. Truly Batch Model-Free Inverse Reinforcement Learning about Multiple Intentions. In International Conference on Artificial Intelligence and Statistics, pages 2359­2369. PMLR, 2020.
11

[32] Pravesh Ranchod, Benjamin Rosman, and George Konidaris. Nonparametric Bayesian reward segmentation for skill discovery using inverse reinforcement learning. In 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 471­477, September 2015. doi: 10.1109/IROS.2015.7353414.
[33] William M. Rand. Objective criteria for the evaluation of clustering methods. Journal of the American Statistical association, 66(336):846­850, 1971.
[34] Simone Romano, Nguyen Xuan Vinh, James Bailey, and Karin Verspoor. Adjusting for Chance Clustering Comparison Measures. Journal of Machine Learning Research, 17:1­32, 2016.
[35] Stuart Russell. Learning agents for uncertain environments. In Proceedings of the eleventh annual conference on Computational learning theory, pages 101­103, 1998.
[36] Aaron J. Snoswell, Surya P. N. Singh, and Nan Ye. Revisiting Maximum Entropy Inverse Reinforcement Learning: New Perspectives and Algorithms. In IEEE Symposium Series on Computational Intelligence, 2020.
[37] Nguyen Xuan Vinh, Julien Epps, and James Bailey. Information Theoretic Measures for Clusterings Comparison: Variants, Properties, Normalization and Correction for Chance. Journal of Machine Learning Research, 11:2837­2854, 2010.
[38] N. Vlassis and A. Likas. A greedy EM algorithm for Gaussian mixture learning. Neural Processing Letters, 15(1):77­87, 2002.
[39] Lantao Yu, Tianhe Yu, Chelsea Finn, and Stefano Ermon. Meta-inverse reinforcement learning with probabilistic context variables. arXiv preprint arXiv:1909.09314, 2019.
[40] Ying Zhao and George Karypis. Criterion functions for document clustering: Experiments and analysis. 2001.
[41] Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse reinforcement learning. In AAAI, volume 8, pages 1433­1438. Chicago, IL, USA, 2008.
12

A Theoretical Analysis of the LiMIIRL algorithm ­ additional results

Here we state an additional lemma that is useful for our analysis, and include proofs that are excluded from our main text.

The following simple result is handy useful in the below proofs.

Lemma 5. Let A = {a1, . . . , an}, A = {a1, . . . , an} B = {b1, . . . , bm} and B = {b1, . . . , bm}

be finite sets of positive numbers.

If

ai bj



ai bj



for

all

1



i



n, 1



j



m, then for I



{1, . . . , n},

J  {1, . . . , m}, we have

iI ai



iI ai

(2)

iI ai + jJ bj

iI ai + jJ bj

jJ bj



jJ bj

(3)

iI ai + jJ bj

iI ai + jJ bj

Proof. From the assumptions, we have

iI ai =

ai 

ai  =

bj

iI bj

iI bj

iI ai . bj

Inverting the fractions on both sides, we have

bj  iI ai

bj . iI ai

Summing over j  J and adding 1 to both sides, we have

iI ai + jJ bj  iI ai

iI ai + jJ bj . iI ai

Inverting the fractions on both sides, we obtain the desired inequality

iI ai



iI ai + jJ bj

iI ai

.

iI ai + jJ bj

The second inequality in the lemma thus follows easily by subtracting both sides of the inequality by 1.

Proof of Lemma 1. For any   Tk+ and any   Tk, we have

k

(( ) - (

))

=

1 |Ck |



(
Ck

)(( ) - ( ))

1 = |Ck|


(
Tk+

)(( )

-

(

))

+

1 |Ck |



(
/Tk+

)(( ) - ( ))

 (1 - )d + (-d)

= (1 - 2)d,

where the inequality follows from Assumption 1(a)(i) and Assumption 2.

For any   Tk+ and any   Tk-, we have

k

(( ) - (

))

=

1 |Ck |



(
Ck

)(( ) - ( ))

1 = |Ck|


(
Tk+

)(( )

-

(

))

+

1 |Ck |



(
/Tk+

)(( ) - ( ))

 (1 - ) + (-)

= (1 - 2).

13

Proof of Lemma 2. Let w( ) = q( ) exp(( ) k), Wk = Tk w( ), Wk = Tk w( ), Wk+ = Tk+ w( ), Wk- = Tk- w( ), and W = T w( ). We obtain a lower bound on

p(

|

k )

=

w( ) W

=

Wk W

w( ) Wk

by

lower

bounding

Wk W

and

w( ) Wk

respectively.

To

lower

bound

Wk W

,

first

note

that

Wk



Wk+,

thus

Wk W

=

Wk Wk + Wk



Wk+ Wk+ + Wk

,

(4)

and it suffices to lower bound the last fraction. From Lemma 1, k d~-separates Tk+ and Tk, thus for any   Tk+ and any   Tk, we have

w( )

q( ) exp((( ) - ( )) =

k) 

q( ) exp(d~),

w( )

q( )

q( )

where the equality is obtained by dividing both numerator and denominator by exp(( ) k). Using Lemma 5, we have

Wk+ Wk+ + Wk

=

 Tk+ w( )  Tk+ w( ) +  /Tk w( )



Tk+ q( ) exp(d~)  Tk+ q( ) exp(d~) +  Tk q( )

=

Q+k exp(d~) Q+k exp(d~) + Qk

=

(1

(1 -

- )Qk exp(d~) )Qk exp(d~) + Qk

.

(5)

Now,

to

lower

bound

w( ) Wk

,

note

that

since

k

~-separates

Tk+

and

Tk-,

we

have

for

any





Tk+

and

any 



Tk-

w( ) w( )

=

q( ) exp((( )-( q( )

))

k )



q( q(

) )

e~

.

Hence

Wk+ Wk-

=

Tk+ w( )   Tk- w( )

 Tk+

q( ) e~



(1

-

)Qk e~

=

(1 - )e~ .

 Tk- q( )

 Qk



Thus

we

have

+(1-)e~ (1-)e~

Wk+



Wk .

As

a

consequence,

w( ) Wk





(1 - )e~ + (1 - )e~

w( ) Wk+ .

(6)

On the other hand, we have

w( )

q( )

Wk+ =  Tk+ q( ) exp((( ) - ( )) k)



q( )

 Tk+ q( ) exp(2rk k )

=

q( ) exp(-2rk Q+k

k

)

 q( ) exp(-2rk k )

(7)

Qk

14

where the inequality follows from (( ) - ( )) k)  ( ) - ( ) k)  2rk k . Combining Eq. (4)-Eq. (7), for any   Tk, we have

p(

|

k )

=

w( ) W



Wk W

w( ) Wk



(1 - )Qk exp(d~)

(1 - )e~ q( ) exp(-2rk

(1 - )Qk exp(d~) + Qk  + (1 - )e~

Qk

k

)

=

(1 - )q( ) exp(d~- 2rk k (1 - )Qk exp(d~) + Qk

) (1 - )e~  + (1 - )e~

Now, for any  / Tk, we have

p( | k) =

w( )  T w( )



w( )

 Tk+ w( ) + w( )



(1

-

q( ) )Qk exp(d~)

+

, q( )

where the last inequality follows from Lemma 5.

Proof of Theorem 1. Consider an arbitrary i. Assume that i  Ck, and consider k = k, then by Lemma 2,

p(i

|

k )



(1 - )q(i)ed~-2rk k (1 - )Qked~ + Qk

(1 - )e~  + (1 - )e~ ,

p(i

|

k

)



(1

q(i) - )Qk ed~ +

. q(i)

This gives us the following bound

(k0)p(i | k) (k0)p(i | k )



(1 - )2e~  + (1 - )e~

|Ck|((1 - )Qk ed~ |Ck |((1 - )Qked~

+ +

q(i)) Qk )

ed~-2rk

k

Hence we have

u(ik1) =

(k0)p(i | k(0)) , k (k0)p(i | k(0))

=

(k0)p(i

|

(k0)p(i | k(0)) k(0)) + k =k (k0)p(i

|

k(0)) ,





 +K

-

, 1

 .

where the inequality is obtained with the help of Lemma 3 (in supplementary file). Since u(ik0) = 1, we have

|u(ik1) - u(ik0)| = 2|u(ik1) - 1|

(8)

k



2(K - 1)  + K - 1.

(9)

Since at least 1- of the trajectories are typical, and for the remaining trajectories, k |u(ik1)-u(ik0)|  2, we have

1 |D|

|u(ik1)

-

u(ik0)|



2

+

(1

-

)

2(K - 1)  +K -1

.

ik

15

B Societal Impact Discussion
Our work is mainly algorithmic and theoretical and therefore somewhat indirectly linked to potential negative societal impacts - however we briefly discuss such potential impacts here. Methodology. On the methodology side, our LiMIIRL algorithm could help enable the application of MI-IRL methods to larger or more substantive real-world problems, and our GEVD metric also provides insights on when these Multi-Intent techniques may work well. Such Multi-Intent IRL methods allow for teasing-out individual user preferences in a cohort of un-labelled data - as such, they might be considered to pose a slightly higher risk in terms of surveillance or revelation of user preferences, compared to previous work on the (single intent) IRL problem. However for the most part, our present work mostly magnifies any existing potential negative or positive impacts of IRL methods generally. The most common IRL algorithms function by observing state and action sequences, which typically requires consent of the subject (c.f. stateonly observations which can often be collected by surveillance in the absence of consent or even in adversarial contexts) - we consider this to be a strong mitigating factor regarding the potential negative impacts of IRL algorithms as a class of machine learning models. Furthermore, in our work, we are also concerned with learning simple (linear) rewards for the purpose of understanding behaviour, rather than complicated (e.g. deep neural network) rewards for replicating behaviour through apprenticeship learning. As such, our models do not directly interact with the real world in the sense that e.g. an apprenticeship learning agent might, but rather are useful for interrogating the world and building a better understanding of certain behaviours, which potentially limits or mitigates negative impact from this work. Application. The example application that we consider (driver behaviour forecasting) is incidental to our methodological contributions and was selected in part due to it's low potential for negative societal impacts, and large potential for positive societal impact.
16

C Evaluation Measures for MI-IRL Algorithms

We provide some additional comments and insight on our MI-IRL evaluation measures. Implementa-

tions of the below measures are included in our open-source project repository.

Clustering Performance ­ ANID metric. Given a set S of N items, a hard (or patitional) clus-

tering divides the set into non-overlapping sub-sets, U

{U1, . . . , UK1 } with

K1 i=1

Ui

=

N

and

Ui Uj =  j = i. Given a second hard clustering V {V1, . . . , VK2 }, the contingency table N

with elements nij |Ui Vj| and marginals ai

K2 j=1

nij

and

bj

K1 i=1

nij

allows

calculating

various information-theoretic measures about the two clusterings,

H (U)

K1
-

ai log ai ,

NN

i=1

H (V)

K2
-

bi log bi ,

NN

j=1

H(U | V)

K1
-

K2

nij log nij /N ,

N
i=1 j=1

bj /N

H(V | U)

K1
-

K2

nij log nij /N ,

N
i=1 j=1

ai/N

H(U, V) = H(V, U)

K1
-

K2

nij log nij ,

NN

i=1 j=1

I(U, V) = I(V, U)

K1 i=1

K2 j=1

nij N

log

nij aibj

/N /N

2

,

where H is a (Shannon) entropy, and I is known as the mutual information - a measure of how much information the two clustering share.

Various measures based on these terms have been proposed as ways to evaluate the similarity of hard clusterings, however recent work has highlighted the strengths (as compared to other metrics that have historically been popular, such as the clustering purity [40], Rand Index (or adjusted variant) [33], and other information theoretic measures) of the max-Normalized Information Distance (NID) due to it's beneficial theoretical and empirical properties [37]. The NID is given by,

NID

1

-

I(U, V) max{H(U), H(V)}

.

(10)

In the context of multi-modal IRL, an algorithm may in-fact produce a so-called soft (probabilistic)

assignment of demonstrations to behaviour modes. A soft clustering of N items into K1 clusters can

be represented by a responsibility matrix U  RN×K1 with elements uij  0 and

K1 j=1

uij

=

1

i.

Given a second soft clustering V  RN×K2 , work by [2] proposes an analogous contingency

table, N  U V , which allows applying any metric based on contingency tables to the case

of soft clusters, and subsequent work by Lei et al. [19, 20] shows that the NID is again a good

choice of metric for evaluating soft clusterings. In our experiments, we use the adjusted Normalized

Information Distance (ANID), which extends the NID to control for random chance - without this

adjustment, the apparent agreement between clusters will be positively correlated with the number of clusters K1 or K2 [34]. The ANID is given by,

ANID

1-

I(U , V ) - E[I(U , V )] max{H(U ), H(V )} - E[I(U , V

, )]

(11)

where U and V are from an appropriate random clustering model ­ for hard clusters hypergeometric models are commonly used, an in our soft clustering case we choose dim(U ) = dim(U ) and sample rows from the uniform Dirichlet distribution ui  GEM(1/K1), and similarly for V . The ANID has a number of useful properties - it is stochastically normalized to the range [0, 1], with values approaching zero indicating perfect agreement between the two clusters. Furthermore, in the limit N  , the ANID obeys the triangle equality and converges to a proper metric [34].
Reward Ensemble Performance ­ GEVD metric. To evaluate single-intent IRL algorithms, Choi and Kim [7] proposed the Inverse Learning Error (ILE), which was further refined by Bogert et al. [5],

ILE(RGT, RL) v(R GT ) - v(R L ) 1,

 [0, )

(12)

17

where v() indicates the vector of state values w.r.t. the ground truth reward RGT for any arbitrary policy , and R GT and R L denote the optimal policy w.r.t. the ground truth and learned reward functions respectively. The ILE is an appropriate measure of regret for the IRL context, however has the limitation that it grows with the number of states in the MDP, making it difficult to compare across different tasks, and difficult to apply to MDPs with continuous state spaces.
Instead, Levine et al. [21] proposed a generalisation3 of this measure, known as the Expected Value Difference (EVD),

EVD(RGT, RL) Esp0 [v(R GT )] - Esp0 [v(R L )]

 [0, )

(13)

where v() indicates the state-value function for the arbitrary policy  w.r.t. the ground truth reward RGT, and R GT and R L denote the optimal policy w.r.t. the ground truth and learned reward functions respectively. While the EVD has the same range as the ILE4, the upped bound does not necessarily
depend on the full set of states in the MDP, and we can more easily compute the EVD for continuous
state-space MDPs.

Unlike a single-intent IRL method, MI-IRL algorithms output a set of learned reward function-weight pairs RL = {(wi, RLi )}Ki=11, which we desire to compare with the set of ground truth reward functions and their corresponding weights RGT = {(wj, RGTj )}Kj=21. To achieve this comparison, we find the most optimistic soft matching of learned and ground truth rewards. Given eij, the matrix of EVD values for the i-th learned model w.r.t. the j-th ground truth model, we define the Generalized
Expected Value Difference (GEVD) as,

GEVD(RGT, RL)

arg min

wij eij ,

{wij }: j wij =wi, i wij =wj i,j



[0,

maxk 1-

k 

]

(14)

where k = maxs,a,s RGT k (s, a, s ) - mins,a,s RGT k (s, a, s ). Computing the GEVD is equivalent to solving a minimum cost flow problem over a dense directed graph from learned to ground
truth reward modes, where

(a) there is a single unity demand source (S) and sink (T), and
(b) the inner and outer edges have zero cost and capacity equal to the weight of that mixture component, and
(c) the inside edges have unity capacity and costs equal to the pairwise EVD values,

a construction we illustrate in Fig. 2.

3EVD generalized ILE in the sense that ILE can be viewed as EVD computed under a uniform starting state distribution.
4N.b. it is possible to upper-bound these single-intent IRL measures more tightly, using the same approach we apply to our GEVD metric (see the main text).
18

Demand = -1

Demand = +1

Capacity = Cost = 0

Capacity = 1 Cost =

Capacity = Cost = 0

Figure 2: The GEVD Score finds the most optimistic pairing between a learned mixture model {RLi }K i=11 and the ground truth mixture model {RGTj }K j=21.

19

D LiMIIRL algorithm - extended results
We provide additional results for LiMIIRL on the ElementWorld problem when using different singleintent IRL models, namely, MaxEnt IRL, ML-IRL, and -GIRL. In each case, we run a MI-IRL algorithm multiple times and represent the runs using a boxplot. We use 100× repeat experiments with randomized seeds for the MaxEnt IRL plots, and 20× repeat experiments for the ML-IRL and 40× repeat experiments for -GIRL experiments (both of which are much more computationally demanding to run). The boxes are centered at the median and extend to the first quartile, with whiskers extending to 1.5× the Inter Quartile Range. Outlying values beyond ±1.5 IQR are shown with small diamonds. For all metrics, lower values are better. The results are shown in Figs. 3 to 5. A note on -GIRL Experiments The `Multiple-Intent' Expectation Maximization -GIRL algorithm [31] solves a similar but fundamentally different problem to that solved by Multiple-Intent Maximum Likelihood IRL [3] and Multiple-Intent Maximum Entropy IRL (the primary algorithm used in our paper). Namely, -GIRL addresses a partially labelled variant of the Multiple-Intent problem, where we have N trajectories generated by M  N agents, who optimize for K  M particular behaviour intents. In -GIRL, the mapping from trajectories to agents is known as part of the problem definition. This information is used to form policy gradient Jacobian mean and covariance estimates (one for each agent), which are then provided to the EM process to cluster the agents into intents. In contrast, our present work and the original MI ML-IRL algorithm by [3] consider a fullyunsupervised version of the MI-IRL problem, where there is no such notion of `agents'. Although the fully-unsupervised setting can be seen as a special case of the partially supervised case with M = N agents (i.e. one trajectory per agent), the -GIRL algorithm cannot be directly applied in this context due to the need for more than one trajectory to estimate the policy gradient Jacobian covariance matrix. Nevertheless, we can utilise the LiMIIRL initialisation strategy for the un-labelled part of the -GIRL algorithm (clustering agents into intents), and we include experimental results for this case below, run using code provided by the original authors. Specifically, for each of the M agents, we compute the empirical feature expectation using their (known) individual sets of demonstrations, then run a hard (k-means) or soft (GMM) clustering procedure to compute the initial responsibility matrix {u(ik0)} allocating agents to intents. We then use this responsibility matrix in lieu of the first E-step of the MI -GIRL EM routine, which proceeds to iteratively allocate agent Jacobian matrices to behavioural intents. This is functionally equivalent to the LiMIIRL MLE initialisation strategy, but using the -GIRL single-intent IRL algorithm as the underlying behaviour model. It is less clear how to apply the LiMIIRL Mean initialisation method with -GIRL, however this may be possible by exploiting feature transformations to convert the MDP feature domain to the simplex reward parameter domain searched by the -GIRL algorithm ­ an idea we leave for future work. Due to the semi-supervised nature of the -GIRL algorithm, and it's prohibitive memory complexity, our -GIRL experiments used the simple continuous PuddleWorld domain from the original -GIRL paper (Figure 2 from [31]).
20

,WHUDWLRQV

0D[(QW,WHUDWLRQV 




0D[(QW'XUDWLRQV 



'XUDWLRQV

 
 


 /L0,,5/KDUG0/(/RLX0U,V,5/KDUG0HDQ/RLX0UV,,5/VRIW0/(R/XL0UV,,5/VRIW0HDQ%RDXEUVH 9URPDQHWDO6XSHUYLVHG%DVHOLQH
0HWKRG
(a) number of iterations


/L0,,5/KDUG0/(/RLX0U,V,5/KDUG0HDQ/RLX0UV,,5/VRIW0/(R/XL0UV,,5/VRIW0HDQ%RDXEUVH 9URPDQHWDO6XSHUYLVHG%DVHOLQH 0HWKRG
(b) wall time in seconds

1// *(9'

0D[(QW1// 

0D[(QW$1,' 

 

 

$1,'





 



/L0,,5/KDUG0/(/RLX0U,V,5/KDUG0HDQ/RLX0UV,,5/VRIW0/(R/XL0UV,,5/VRIW0HDQ%RDXEUVH 9URPDQHWDO6XSHUYLVHG%DVHOLQH 0HWKRG

 /L0,,5/KDUG0/(/RLX0U,V,5/KDUG0HDQ/RLX0UV,,5/VRIW0/(R/XL0UV,,5/VRIW0HDQ%RDXEUVH 9URPDQHWDO6XSHUYLVHG%DVHOLQH 0HWKRG

(c) final mixture NLL

(d) clustering performance (ANID)

0D[(QW*(9'




 /L0,,5/KDUG0/(/RLX0U,V,5/KDUG0HDQ/RLX0UV,,5/VRIW0/(R/XL0UV,,5/VRIW0HDQ%RDXEUVH 9URPDQHWDO6XSHUYLVHG%DVHOLQH
0HWKRG
(e) reward ensemble performance (GEVD) Figure 3: ElementWorld results with multi-intent MaxEnt IRL. The label `Babes¸-Vroman et al. 2011' indicates EM with random initialisation. Although all methods result in a similar final NLL distribution, we highlight that there are significant differences in the other performance metrics, with the hard-clustering strategy consistently performing best.

,WHUDWLRQV

0D[/LN,WHUDWLRQV   

'XUDWLRQV

0D[/LN'XUDWLRQV   





 /L0,,5/KDUG0/(/RLX0U,V,5/KDUG0HDQ/RLX0UV,,5/VRIW0/(R/XL0UV,,5/VRIW0HDQ%RDXEUVH 9URPDQHWDO6XSHUYLVHG%DVHOLQH
0HWKRG
(a) number of iterations

 /L0,,5/KDUG0/(/RLX0U,V,5/KDUG0HDQ/RLX0UV,,5/VRIW0/(R/XL0UV,,5/VRIW0HDQ%RDXEUVH 9URPDQHWDO6XSHUYLVHG%DVHOLQH
0HWKRG
(b) wall time in seconds

1// *(9'

0D[/LN1// 

0D[/LN$1,' 







 

$1,'












/L0,,5/KDUG0/(/RLX0U,V,5/KDUG0HDQ/RLX0UV,,5/VRIW0/(R/XL0UV,,5/VRIW0HDQ%RDXEUVH 9URPDQHWDO6XSHUYLVHG%DVHOLQH 0HWKRG

 /L0,,5/KDUG0/(/RLX0U,V,5/KDUG0HDQ/RLX0UV,,5/VRIW0/(R/XL0UV,,5/VRIW0HDQ%RDXEUVH 9URPDQHWDO6XSHUYLVHG%DVHOLQH 0HWKRG

(c) final mixture NLL

(d) clustering performance (ANID)

0D[/LN*(9'




 /L0,,5/KDUG0/(/RLX0U,V,5/KDUG0HDQ/RLX0UV,,5/VRIW0/(R/XL0UV,,5/VRIW0HDQ%RDXEUVH 9URPDQHWDO6XSHUYLVHG%DVHOLQH
0HWKRG
(e) reward ensemble performance (GEVD) Figure 4: ElementWorld results with multi-intent ML-IRL. The label `Babes¸-Vroman et al. 2011' indicates EM with random initialisation. Similarly to MaxEnt IRL (above), the hard clustering strategy consistently performs better than other initialisation strategies.

,WHUDWLRQV

*,5/,WHUDWLRQV

*,5/'XUDWLRQV











'XUDWLRQV














 /L0,,5/KDUG0/(RXUV/L0,,5/VRIW0/(RXRUV/L0,,5/KDUG0/(RXRUV/L0,,5/VRIW0/(RX%UDVEH 9URPDQHWDO 0HWKRG


  /L0,,5/KDUG0/(RXUV/L0,,5/VRIW0/(RXRUV/L0,,5/KDUG0/(RXRUV/L0,,5/VRIW0/(RX%UDVEH 9URPDQHWDO
0HWKRG

(a) number of iterations.

(b) wall time in seconds.

*,5/1// 

*,5/$1,' 









$1,'










 /L0,,5/KDUG0/(RXUV/L0,,5/VRIW0/(RXRUV/L0,,5/KDUG0/(RXRUV/L0,,5/VRIW0/(RX%UDVEH 9URPDQHWDO 0HWKRG


 /L0,,5/KDUG0/(RXUV/L0,,5/VRIW0/(RXRUV/L0,,5/KDUG0/(RXRUV/L0,,5/VRIW0/(RX%UDVEH 9URPDQHWDO 0HWKRG

(c) final mixture NLL

(d) clustering performance (ANID)

1//

Figure 5: PuddleWorld results with semi-supervised multi-intent -GIRL (see explanation in Appendix D). The label `Babes¸-Vroman et al. 2011' indicates EM with random initialisation. The -GIRL algorithm differentiates between the reward feature function R and the policy feature function  in this environment ­ we report results using both as the choice of feature vector for LiMIIRL initialisation, and find that they are roughly equivalent. For this problem, we find LiMIIRL leads to ensembles with equivalent ANID and NLL scores and provides a small but not statistically significant improvement on number of iterations and duration to convergence. We hypothesize that LiMIIRL is less helpful in these experiments (compared to the fully-unsupervised MI-IRL ElementWorld experiments) due to one of two factors: (a) the simplicity of the continuous PuddleWorld problem (i.e. it only has two behaviour intents), and/or (b) the fact that the -GIRL algorithm has already exploited the labels mapping trajectories to agents, reducing the variance present in the resultant EM clustering problem.

E LiMIIRL Sensitivity Analysis results

Below we include a series of figures plotting the results of our extensive sensitivity analysis. We briefly describe the procedure used for each experiment, and any interesting observations.
Number of demonstrations. We varied the total number of demonstrations with N  {20, 40, 100, 200, 400} to investigate the performance of our method in the small- and large-data regimes.

,WHUDWLRQV

'XUDWLRQV

1//

$1,'

*(9'











 



























 

   

1XPEHURI'HPRQVWUDWLRQVN 

   

/L0,,5/KDUG0/(RXUV /L0,,5/KDUG0HDQRXUV

/L0,,5/VRIW0/(RXUV /L0,,5/VRIW0HDQRXUV

%DEH 9URPDQHWDO 6XSHUYLVHG%DVHOLQH

Figure 6: Effect of varying number number of demonstrations N .

We observed that the number of iterations stays fairly constant, whereas the wall time duration increases, suggesting that the M-Step (IRL reward solving), not the E-Step (updating cluster allocations) is the more burdensome part of the MI algorithm. LiMIIRL with hard clustering generally outperforms the other initialisation strategies.
Cluster separation. We tested the algorithm with deterministic dynamics as well as with wind factor w  {0.05, 0.1, 0.15, 0.2}, where larger wind values correspond to the behaviour intents being less separated in feature space (progressive violation of Assumption A).
In Fig. 7, we plot the mean of the inter-cluster margin E{T ,T }[minT , T ( ) - ( ) ] (where the expectation indicates all pairwise combinations of elements) with E = 3 elements as the wind factor is increased. This confirms that increasing the stochasticity of the transition dynamics leads to progressive violation of the cluster separation assumption.

Figure 7: Assumption 1 is progressively violated as the ElementWorld wind factor is increased. The plot shows the mean inter-cluster scaled margin with the 95% confidence interval over 10 repeat experiments.
We observed that increasing the wind factor (violating Assumption 1 more) reduces the performance of all methods on all metrics, however in general, LiMIIRL initialisation with hard clustering still outperforms the other method. Number of clusters. We fixed E = 3 and varied the number of learned clusters from K = 1 to 5 to see how LiMIIRL performs when the number of learned clusters is mis-specified or unknown (i.e. under- or over-clustering).
We observe that our LiMIIRL initialisation strategy appears most helpful when the number of clusters is under, or correctly specified for the environment. We note that while the mixture NLL generally
24

,WHUDWLRQV

'XUDWLRQV

1//

$1,'

*(9'













 

 

























     &OXVWHU6HSHUDWLRQZLQGYDOXHw     

/L0,,5/KDUG0/(RXUV /L0,,5/KDUG0HDQRXUV

/L0,,5/VRIW0/(RXUV /L0,,5/VRIW0HDQRXUV

%DEH 9URPDQHWDO 6XSHUYLVHG%DVHOLQH

Figure 8: Effect of varying separation of the behaviour clusters (achieved by adjusting the `wind' factor w in the dynamics)

,WHUDWLRQV

'XUDWLRQV

1//

$1,'

*(9'



















 







































 1XP/HDUQHG&OXVWHUVKE = 3 











/L0,,5/KDUG0/(RXUV /L0,,5/KDUG0HDQRXUV

/L0,,5/VRIW0/(RXUV /L0,,5/VRIW0HDQRXUV

%DEH 9URPDQHWDO

Figure 9: Effect of varying numbers of clusters K (we fix E = 3).

increases proportional to K, the ANID and GEVD metrics reach a minima / asymptote respectively at K = 3 ­ this confirms that these measures are suitable for evaluating reward ensemble performance, even for the case when the number of learned clusters is mis-specified.
We also note that in practice, the value of K can be estimated using heuristics such as the Bayesian Information Criteria, using a non-parametric clustering as a pre-processing step, or using cross-fold validation.

25

Number of elements. We generated ElementWorld configurations with K = E  {2, 3, 4, 5, 6} to test the performance of our method as the number of behaviour intents increases.

,WHUDWLRQV

'XUDWLRQV

1//

$1,'

*(9'





















































      1XP(OHPHQWVEK= E      

/L0,,5/KDUG0/(RXUV /L0,,5/KDUG0HDQRXUV

/L0,,5/VRIW0/(RXUV /L0,,5/VRIW0HDQRXUV

%DEH 9URPDQHWDO 6XSHUYLVHG%DVHOLQH

Figure 10: Effect of varying number number of ground truth elements E (we fix K = E)

We observe that, as expected, the performance of all metrics (except the supervised baseline) gets worse as the number of behaviour intents increases. LiMIIRL with hard clustering appears to provide even more benefit over other initialisation strategies, especially for large numbers of behaviour intents.
Cluster imbalance. We tested the performance of our method when the behaviour intentions are not uniformly present in the demonstration dataset. Instead of sampling demonstration policies uniformly, we selected demonstration policies from the ground truth ensemble according to a normalized geometric distribution k  p(1 - p)k, varying the parameter p  {0.0, 0.1, 0.2, 0.3, 0.4}.

,WHUDWLRQV

'XUDWLRQV

1//

$1,'

*(9'













































    'HP R6NHZ *HRP HWULF'LVWULEXWLRQSDUDPHWHUp     

/L0,,5/KDUG0/(RXUV /L0,,5/KDUG0HDQRXUV

/L0,,5/VRIW0/(RXUV /L0,,5/VRIW0HDQRXUV

%DEH 9URPDQHWDO 6XSHUYLVHG%DVHOLQH

Figure 11: Effect of varying a varying degree of demonstration data cluster imbalance (achieved by sampling demonstrations from the ground truth ensemble under a geometric distribution with parameter p).

We observe that LiMIIRL with hard clustering appears once again to outperform the other initialisation strategies, even in the presence of heavily skewed cluster sizes. Interestingly, the soft initialisation (based on a Gaussian Mixture Model) appears particularly sensitive to cluster imbalance, suggesting this method should be avoided when the true cluster distribution is unknown.

26

F Driver Forecasting Experiment: Learned Reward Ensembles
Below we include plots that visualise the best learned reward ensemble for each of the algorithms LiMIIRL-hard, LiMIIRL-soft, and for the EM algorithm with random initialization [3]. Each figure is divided into sections corresponding to the specific feature - e.g. road type, or geographical region. Within each section, the x-axis shows the feature dimensions, and the plot indicates the learned reward parameter for that feature dimension. Each figure shows a plot for each reward in the learned ensemble, and the plot thickness is proportional to the ensemble component weight k.
27

28

(c) Best learned reward ensemble for Babes¸-Vroman et al. [3]. Figure 12: Learned reward ensembles for the Porto driver behaviour forecasting experiment.

KLJKZD\
PDMRUBVWUHHW
ORFDOBVWUHHW
RWKHU PRGHUDWH
ORZ
XQNQRZQ
KLJK
YHU\BORZ
&HQWUDO 1RUWK
1RUWK1RUWK(DVW 1RUWK(DVW
(DVW1RUWK(DVW (DVW
(DVW6RXWK(DVW 6RXWK(DVW
6RXWK6RXWK(DVW 6RXWK
6RXWK6RXWK:HVW 6RXWK:HVW
:HVW6RXWK:HVW :HVW
:HVW1RUWK:HVW 1RUWK:HVW
1RUWK1RUWK:HVW

RRR123(((8775%%%)) )

 







7\SH 

5HJLRQ

6SHHG

(b) Best learned reward ensemble for LiMIIRL-soft.

KLJKZD\
PDMRUBVWUHHW
ORFDOBVWUHHW
RWKHU PRGHUDWH
ORZ
XQNQRZQ
KLJK
YHU\BORZ
&HQWUDO 1RUWK
1RUWK1RUWK(DVW 1RUWK(DVW
(DVW1RUWK(DVW (DVW
(DVW6RXWK(DVW 6RXWK(DVW
6RXWK6RXWK(DVW 6RXWK
6RXWK6RXWK:HVW 6RXWK:HVW
:HVW6RXWK:HVW :HVW
:HVW1RUWK:HVW 1RUWK:HVW
1RUWK1RUWK:HVW

RRR123(((9613%%%)) )

 







7\SH 

5HJLRQ

6SHHG

(a) Best learned reward ensemble for LiMIIRL-hard.

KLJKZD\
PDMRUBVWUHHW
ORFDOBVWUHHW
RWKHU PRGHUDWH
ORZ
XQNQRZQ
KLJK
YHU\BORZ
&HQWUDO 1RUWK
1RUWK1RUWK(DVW 1RUWK(DVW
(DVW1RUWK(DVW (DVW
(DVW6RXWK(DVW 6RXWK(DVW
6RXWK6RXWK(DVW 6RXWK
6RXWK6RXWK:HVW 6RXWK:HVW
:HVW6RXWK:HVW :HVW
:HVW1RUWK:HVW 1RUWK:HVW
1RUWK1RUWK:HVW

5HJLRQ RRR123(((81440%%%) ))

7\SH      

6SHHG

