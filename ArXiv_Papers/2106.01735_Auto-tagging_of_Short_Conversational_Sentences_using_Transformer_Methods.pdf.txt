Kisa Konus¸ma Cu¨mlelerinin Do¨nu¨s¸tu¨ru¨cu¨ Yo¨ntemleriyle Otomatik Etiketlenmesi
Auto-tagging of Short Conversational Sentences using Transformer Methods

arXiv:2106.01735v2 [cs.CL] 4 Jun 2021

D. Emre Tas¸ar, S¸ u¨kru¨ Ozan, Umut O¨ zdil, M. Fatih Akca,Oguzhan O¨ lmez§, Semih Gu¨lu¨m¶, Sec¸ilay Kutal¶, Ceren Belhan  emretasa@garantibbva.com.tr
 sukruozan@adresgezgini.com,umutozdil@adresgezgini.com  mehmet.akca1@ogr.sakarya.edu.tr § 172803041@ogr.cbu.edu.tr
¶ semihgulum@marun.edu.tr, secilaykutal@marun.edu.tr
ceren.belhan@std.ieu.edu.tr

O¨ zetc¸e --Kisa konus¸ma cu¨ mlelerinin anlamsal o¨zelliklerine go¨re yu¨ ksek dogruluk ile kategorilere ayris¸tirilmasi problemi, dogal dil is¸leme alaninda u¨ zerinde c¸alis¸ilan bir konudur. Bu c¸alis¸mada 46 farkli kategoride siniflandirilan o¨rnekler ile olus¸turulan bir veri seti kullanilmis¸tir. O¨ rnekler, bir firmanin mu¨ s¸teri temsilcileriyle, firmanin internet sitesi ziyaretc¸ileri arasinda gerc¸ekles¸en yazili sohbet (chat) go¨ru¨ s¸melerinden alinmis¸ cu¨ mlelerden olus¸maktadir. Ana amac¸, sorulan sorulara anlamli yanitlar u¨ retebilen bir sohbet uygulamasinda kullanmaya yo¨nelik olarak internet sitesi ziyaretc¸ilerden gelen sorulari ve talepleri o¨nceden belirlenen 46 kategori ic¸in en dogru s¸ekilde otomatik olarak etiketlemektir. Bunun ic¸in Tu¨ rkc¸e dilinde o¨n egitime tabi tutulmus¸ birbirinden farkli BERT modelleri ve bir adet GPT-2 modeli tercih edilmis¸tir. Ilgili modellerin siniflandirma bas¸arimlari detayli bir s¸ekilde incelenerek raporlanmis¸tir.
Anahtar Kelimeler--otomatik etiketleme, dogal dil is¸leme, BERT, GPT-2, c¸oklu sinif.
Abstract--The problem of categorizing short speech sentences according to their semantic features with high accuracy is a subject studied in natural language processing. In this study, a data set created with samples classified in 46 different categories was used. Examples consist of sentences taken from chat conversations between a company's customer representatives and the company's website visitors. The primary purpose is to automatically tag questions and requests from visitors in the most accurate way for 46 predetermined categories for use in a chat application to generate meaningful answers to the questions asked by the website visitors. For this, different BERT models and one GPT-2 model, pre-trained in Turkish, were preferred. The classification performances of the relevant models were analyzed in detail and reported accordingly.
Keywords--auto-tagging, natural language processing, BERT, GPT-2, multi-class.
I. GIRIS¸
Bu c¸alis¸mada dogal dil is¸leme modelleri yardimi ile sorulara otomatik olarak yanitlar u¨retmesi hedeflenen bir chatbot ic¸in, gelen sorulari o¨nceden belirlenmis¸ 46 temel kategoriye

mu¨mku¨n oldugunca dogru bir s¸ekilde ayirabilmek hedeflenmektedir. Gu¨nu¨mu¨zde yogun olarak c¸alis¸ilmakta olan bu konu ile ilgili olarak literatu¨rde birc¸ok gu¨ncel c¸alis¸maya rastlamak mu¨ mku¨ ndu¨ r.
Metin siniflandirma problemi ic¸in Jain vd. tarafindan hibrit ku¨meleme ve siniflandirma modeli (HCC) [1] o¨nerilmis¸tir. Bu modelde, metinlerin ku¨melenmesi ic¸in k-ortalama metodu is¸leme alinirken, c¸ok etiketli bir siniflandirma yapilabilmesi ic¸in ise derin o¨grenme algoritmalari kullanilmis¸tir. C¸ alis¸ma sonunda siniflandirma problemi ic¸in Evris¸imli Sinir Aglari (CNN) bas¸arili bir sonuc¸ verirken, c¸ok etiketli siniflandirma ic¸in Uzun-Kisa Su¨reli Bellek (LSTM) ya da Derin CNN'in daha etkili olabilecegi belirtilmis¸tir.
BERT modeli dogal dil is¸lemede metnin siniflandirmasi ic¸in kullanilan ac¸ik kaynakli bir modeldir. Devlin vd. tarafindan gelis¸tirilen transformato¨rlerden c¸ift yo¨nlu¨ kodlayici go¨sterimlere sahip olan BERT modeli [2] ile birlikte, etiketlenmemis¸ metinlerin derin c¸ift yo¨nlu¨ temsillerinin o¨nceden egitimi sirasinda tu¨m katmanlarda sag ve sol baglam bilgilerinin dahil edilmesi saglanmaktadir. Model, sonrasinda ince ayar yapilarak go¨reve o¨zgu¨ s¸ekilde egitilebilmektedir. Bu go¨revler soru cevaplama, duygu analizi, metin siniflandirma ve adlandirilmis¸ varlik tanima gibi farkli c¸es¸itlerde olabilir [3].
Ince ayar yapilabilmesi ic¸in o¨nceden egitilmis¸ c¸es¸itli BERT modelleri mevcuttur. Bu modeller dil ac¸isindan, dile o¨zgu¨ ve c¸ok dilli olarak ikiye ayrilmaktadir. Tu¨rkc¸e dili o¨zelinde literatu¨rde yeteri kadar iyi egitilmis¸ c¸ok fazla dil modeli olmadigi ic¸in c¸alis¸mada Tu¨rkc¸e diline o¨zel olarak egitilmis¸ olanlarin yani sira c¸ok dilli modeller de kullanilmis¸tir. Ronnqvist vd. c¸alis¸malarinda [4] kullandiklari c¸ok dilli model ile 6 dil u¨zerinde c¸es¸itli go¨revlerde test gerc¸ekles¸tirmis¸ ve bu modelin bas¸arisinin diller arasindaki degis¸imini go¨zlemlemis¸lerdir. C¸ alis¸mada Tu¨rkc¸e ile ayni dil ailesine u¨ye olan Fince dilinde model %93.2 dogruluk degerine ulas¸mis¸tir.
O¨ zc¸ift vd. c¸alis¸masinda [3] morfolojik olarak zor bir dil olan Tu¨rkc¸e ile 6 farkli go¨revde geleneksel makine o¨grenmesi yo¨ntemlerinin yaninda BERT modelinin perfor-

mansini kars¸ilas¸tirmis¸tir. Spam etiketleme go¨revi dis¸inda, BERT modeli diger modellere nazaran daha yu¨ksek dogruluk orani vermis¸tir.
Tanaka vd. c¸alis¸masinda [5] BERT modelini siniflandirici olarak kullanmis¸tir. Doku¨manlarin siniflandirilmasi ic¸in ince ayar yapilan BERT modelinde, girdi olarak doku¨man verilmis¸ ve BERT'in metin siniflandirmadaki bas¸arisi ortaya konulmus¸tur.
Ozan vd. c¸alis¸masinda [6] kategorik siniflandirma probleminde yaygin olarak kullanilan Doc2Vec, LSTM ve BERT gibi modellerin performanslarini kars¸ilas¸tirmis¸tir. C¸ alis¸mada BERT modelinin %93 dogruluk seviyesiyle diger modellere go¨re yu¨ksek bas¸arim go¨sterdigi tespit edilmis¸tir.
Bahsedilen c¸alis¸malar dogrultusunda sec¸ilen BERT modelleri, Bo¨lu¨m II'de detaylari verilen veri seti ile, siniflandirma problemine yo¨nelik olarak ince ayar yapilarak egitilmis¸tir.
BERT modelleri dis¸inda c¸alis¸mada yer verilen bir diger model olan GPT-2, Radford vd. tarafindan gelis¸tirilmis¸ u¨retken yapiya sahip olan bir kod c¸o¨zu¨cu¨ transformato¨rdu¨r [7]. C¸ alis¸mada bu model, kendi ic¸inde o¨zel olarak siniflandirma yapan bir ku¨tu¨phanenin yardimi ile metin siniflandirma amaciyla kullanilmis¸tir.
Whitfield, siniflandirma problemlerinde model performansini arttirabilmek adina GPT-2 modelini kullanarak u¨rettigi sentetik verilerle, veri setini bu¨yu¨terek bas¸arim metriklerini kars¸ilas¸tirmayi hedeflemis¸tir [8]. C¸ alis¸madan ulas¸ilan sonuc¸lara go¨re GPT-2, kurulus¸larin ve is¸letmelerin, bu¨yu¨k o¨lc¸ekli veri toplamaya ilis¸kin yu¨ksek maliyetlere katlanmadan yu¨ksek performansa sahip siniflandirma modelleri olus¸turmasina izin vermektedir.

TABLO I: O¨ rnek Cu¨mleler ve Es¸les¸tirilmis¸ Etiketler

Facebook reklamlari
Instagram reklamlari
Mobil uygulama
Ofisiniz nerede
Is¸ bas¸vurusu
Web sitesine canli destek ekleme
Web sitesi yaptirmak

Facebook reklami ic¸in go¨ru¨s¸mek istiyorum Facebook reklam paketleri, u¨cretleri ve kampanyalariniz ile ilgili bilgi verir misiniz? sitemi Facebook reklam yapmak istiyoreum Instagram s¸irket reklami vermek istiyordum bu konu ile yardimci olabilir misiniz Instagram nasil reklam verebilirim Instagram reklamlariyla ilgili bilgi almak istemis¸tim mobil uygulamalarinin web entegrasyonunu yapabiliyor musunuz Merhaba, mobil uygulama ic¸in fiyat teklifi almak istiyorum detaylari hangi mail adresi ile paylas¸abilirim? mobil uygulama yaptirmak istiyorum ne yapmaliyim Adres bilgisi alabilir myim hangi ilden hizmet veriyorsunuz Ankarada yu¨zyu¨ze go¨ru¨s¸u¨p ilgi alabilecegimiz bir ekibiniz mevcut mudur firmanizda c¸alis¸ma imkanimiz var mi Bu yaz ic¸in staj imkanlarinizi o¨grenmek istiyorum. Bilgisayar Mu¨hendisligi 3.sinif o¨grencisiyim acaba is¸ bas¸vuru yapabilmesi ic¸in mail adresiniz var midir Canli destek hizmeti sunuyormusunuz mu¨s¸teri telefonlarina yetis¸emiyoruz canli destek hizmetinizden yararlanmak isteriz u¨creti ne kadar Birde bu canli destek sisteminizin u¨creti nedir ? Siz web tasarim yapiyrmusunz peki ben web sitesi yaptirmayi du¨s¸u¨nu¨yorum ya peki web adresi kurabilir miyiz

Not: Toplam kategori sayisi 46 olmasina kars¸in tabloda 7 o¨rnek kategori paylas¸ilmis¸tir.

II. KULLANILAN VERI SETI
Veri seti ic¸in firmanin internet sitesinde kullanilan sohbet arayu¨zu¨ araciligiyla web sitesi ziyaretc¸ileri tarafindan yo¨neltilen sorular ele alinmis¸tir. Elde edilen 73551 adet ve kelime sayisi 2 ile 30 arasinda degis¸en sec¸ili cu¨mle, 46 kategori ic¸in manuel olarak etiketlenmis¸tir. O¨ rnek yedi kategori ic¸in sec¸ilmis¸ bazi o¨rnek cu¨mleler Tablo I'de verilmis¸tir. Veri setinde bulunan cu¨mle-cu¨mle gruplarinin uzunlugunun genis¸ bir aralikta degis¸kenlik go¨stermesi (S¸ ekil 1), kategorilere go¨re veri dagiliminin du¨zensiz olus¸u (S¸ ekil 2) ve benzer kaliplarda verilerin olmasi hazirlanan veri setinin kullanilmasinin zorluklari olarak sayilabilir. O¨ n is¸leme as¸amasinda veri setindeki cu¨mlelerde yer alan sayilar ve noktalama is¸aretleri atilmis¸, bu¨tu¨n harfler ku¨c¸u¨k harflere c¸evrilmis¸tir. Veri seti %70'i egitim, %30'u da test ic¸in olacak s¸ekilde iki gruba ayrilmis¸tir. Egitim as¸amasinda kullanilan yo¨ntemlere ait detaylar Bo¨lu¨m III'de verilecektir.
III. YO¨ NTEMLER
A. BERT Yo¨ntemi
BERT modeli o¨n egitim ve ince ayar olarak 2 as¸amadan meydana gelmektedir. O¨ n egitim, birbirinden farkli ve etiketlenmemis¸ veriler u¨ zerinden bir konus¸ma diline o¨zel yada birden fazla konus¸ma dili ic¸in egitilir. O¨ n egitimden gec¸mis¸ bir BERT modeline etiketlenmis¸ veriler ile ince ayar yapilir. Ince ayar sayesinde BERT'in dikkat mekanizmasi neye dikkat

S¸ ekil 1: Veri Setindeki Cu¨mlelerin Uzunluklarina Go¨re Dagilim Grafigi
S¸ ekil 2: Veri Setinin Etiketlere Go¨re Dagilim Grafigi
etmesi gerektigini anlayabilir. C¸ alis¸mada kullanilan BERTbase modeli, 12 transformato¨r blogu, 12 o¨z-dikkat bas¸ligi ve 768 gizli katmana sahip bir kodlayici ic¸erir ve c¸ift yo¨nlu¨ o¨z-dikkat kullanir [9]. Kullanilan distil-BERT modeli ise BERT-base modeline go¨re daha ku¨c¸u¨k bir yapiya sahip oldugundan %60 daha hizli c¸alis¸irken BERT performansini da bu¨yu¨k o¨lc¸u¨de koruyabilmektedir [10]. BERT modelinin egitimi bu¨yu¨k harflere duyarlilik ac¸isindan, "cased" (bu¨yu¨k ku¨c¸u¨k harf kullanimina kars¸i duyarli) ve "uncased" (bu¨yu¨k ku¨c¸u¨k harf kullanimina kars¸i duyarsiz) olarak iki farkli model tipi ile gerc¸ekles¸tirilebilmektedir.

B. GPT-2 Yo¨ntemi
Mevcut sistemler, genel veri setleri ile c¸alis¸mak yerine daha dar konularda c¸alis¸maya uygundur. "U¨ retken Egitim O¨ ncesi Transformato¨r" olarak adlandirilan ve anlamli metin olus¸turmada oldukc¸a bas¸arili olan GPT-2, mevcut sistemlerin aksine c¸oklu go¨revlerde c¸alis¸abilecek ve o¨zel go¨revlere go¨re de konfigu¨re edilebilecek s¸ekilde tasarlanmis¸tir.
Bir kod c¸o¨zu¨cu¨ transformato¨ru¨ olan GPT-2'nin egitimi esnasinda girdi dizisinin son belirteci, tahminde ihtiyac¸ duyulan tu¨m bilgileri ic¸erdiginden, girdiyi takip eden bir sonraki token hakkinda tahmin yapilmasi amaciyla kullanilmaktadir. Girdi dizisinin son belirtecinin tas¸idigi bilgi, u¨retim problemi yerine bir siniflandirma probleminde tahmin yapmak amaciyla da kullanilabilir. GPT-2 ile tahmin yapilirken, BERT modelinin siniflandirma probleminde uygulandigi gibi ilk token yerles¸tirmek yerine son token yerles¸tirme uygulanmaktadir [11].
Bu kapsamda gerc¸ekles¸tirilmeye c¸alis¸ilan siniflandirma problemi ic¸in Bo¨lu¨m I'de bahsedilen yo¨ntemler ile birlikte BERT ve GPT-2 modellerinin bas¸arimi test edilmis¸tir. Daha o¨nceden firma bu¨nyesinde yapilmis¸ olan c¸alis¸ma [6] genis¸letilerek toplamda 10 adet BERT modeli ve bir adet GPT-2 modeli kullanilmis¸tir. GPT-2'nin siniflandirici olarak kullanildigi c¸alis¸mada, ilgili modeller so¨z konusu veri seti ile ince ayar yapilarak egitilmis¸tir. Model mimarilerine ait bazi temel parametrelere ait degerler Tablo II'de go¨sterilmis¸tir.
C. F1 Skoru ve Bas¸arim Kriterleri
Egitilen modellerin bas¸arisi dogruluk degeri, F1 skoru, kayip degerleri ve hata dizeyi (confusion matrix) baz alinarak degerlendirilmis¸tir. Bu bas¸ari o¨lc¸u¨tleri; True Positive (TP), False Positive (FP), True Negative (TN) ve False Negative (FN) degerlerine bakilarak hesaplanir. Modelin c¸iktisi ikili siniflandirma olarak go¨z o¨nu¨ne alinirsa; TP, modelin tahmini bir durum ic¸in olumlu sonuc¸ vermesi ve asil durumda da sonucun olumlu oldugunu go¨sterirken TN, algoritma c¸iktisinin olumsuz iken asil durumun da olumsuz oldugunu temsil eder. Iki tip hata vardir, bunlar FP ve FN kavramlaridir.FP algoritmanin tahminini olumluyken asil degerin olumsuz olmasi durumudur. FN ise algoritmanin olumsuz dedigi bir sonuc¸ kars¸isinda asil durumun olumlu olmasidir. C¸ alis¸mada kullanilan veri setinde o¨rnek bir etiket ic¸in elde edilen sonuc¸larla olus¸turulmus¸ olan Tablo III, TP, FP, TN ve FN kavramlarinin daha iyi anlas¸ilabilmesi ic¸in incelenebilir.
Dogruluk degeri en temel bas¸arim kriteri olarak ifade edilebilir. Bu metrikte modelin u¨rettigi dogru tahmin sonuc¸lari u¨retilen tu¨m tahminlere oranlanmaktadir (Denklem 1). Ancak bu yo¨ntem kategorik siniflandirma problemlerinde sinif bazli bir sonuc¸ veremediginden tercih edilmemektedir. Bu durumlarda F1 skoru adi verilen ve kesinlik-dogruluk degerlerine dayanan ayri bir metrik kullanilmaktadir.
Kesinlik (precision) degeri modelin tahmin ettigi TP sayisinin modelin u¨rettigi tu¨m olumlu sonuc¸lara orani ile bulunmaktadir (Denklem 2). Duyarlilik (recall) degeri ise modelin tahmin ettigi TP sayisinin modelin tahmin etmesi gereken tu¨m olumlu sonuc¸lara orani ile bulunmaktadir (Denklem 3). Bu iki kavram modelin u¨rettigi sonuc¸larin farkli iki durumu u¨zerinden hesaplandigindan her iki durumun da etkin bir

s¸ekilde yorumlandigi F1 skoru, Denklem 4'te ifade edildigi gibi kesinlik ve duyarlilik degerlerinin harmonik ortalamasi alinarak hesaplanmaktadir [21].

TP +TN

Dogruluk =

(1)

TP +TN +FP +FN

TP

Kesinlik = T P + F P

(2)

TP

Duyarlilik =

(3)

TP +FN

F 1skoru = 2 Kesinlik × Duyarlilik

(4)

Kesinlik + Duyarlilik

Kategorilere go¨re veri sayisinin du¨zenli dagilmamasi, c¸alis¸mada kullanilan modellerin genel bas¸ariminin tespiti ic¸in farkli bir metrigin kullanimini gerektirmektedir. Kategori bazinda hesaplanmis¸ F1 skorunun kategori bazli veri dagilimina go¨re agirlikli ortamasinin alinarak hesaplanmasiyla dengesiz veri dagiliminda ortaya c¸ikan bu problem belirli bir o¨lc¸u¨de c¸o¨zu¨lebilmektedir.

IV. UYGULAMA SONUC¸ LARI
C¸ alis¸mada siniflandirma problemi, Bo¨lu¨m III-A ve Bo¨lu¨m III-B'de anlatildigi gibi farkli BERT-base, distilBERT-base ve GPT-2 modellerinin egitimleri ile c¸o¨zu¨lmeye c¸alis¸ilmis¸tir. Bu bo¨lu¨mde ise egitilen modellerin sonuc¸lari Bo¨lu¨m III-C'de verilen kriterler go¨z o¨nu¨ne alinarak incelenecektir. Her modelin egitimi u¨c¸ do¨nem (epoch) gerc¸ekles¸tirilerek parametreler hesaplanmis¸tir. S¸ ekil 4'de modellerin dogruluk/do¨nem grafikleri c¸izdirilmis¸tir.
GPT-2 metin u¨retmede go¨stermis¸ oldugu bas¸arisini metin siniflandirma probleminde go¨sterememis¸ ve dogruluk degeri ancak %84 degerine kadar c¸ikabilmis¸tir. Ancak c¸alis¸ma ic¸in daha o¨nemli bir kars¸ilas¸tirma kriteri olan F1 skoru %73 olarak go¨ zlemlenmis¸tir.
BERT modeli ic¸in o¨nceden egitilmis¸ "cased" ve "uncased" modeller ile gerc¸ekles¸tirilen ince ayar egitimlerinin ardindan bas¸arim oranlari incelendiginde, iki farkli model tipinin sonuc¸ metrikleri arasinda, ilgili problem ve veri seti du¨s¸u¨nu¨ldu¨gu¨nde, c¸ok bu¨yu¨k bir fark go¨zlemlenmemis¸tir.
Bas¸arim oranlari incelendiginde en bas¸arili modelin %89 dogruluk degeri ve %82 F1 skoruyla loodos/bertbase-turkish-cased [17] oldugu go¨zlemlenmis¸tir. Bu egitimin c¸oklu sinif bazli bas¸arisi hata dizeyi u¨zerinden S¸ ekil 5'de go¨sterilmis¸tir. Ayni modelin egitimi do¨nem sayisinin etkisinin go¨zlemlenmesi amaciyla 10 do¨nem ile tekrarlanmis¸tir. Bulgular sonucunda 3 do¨nemden sonra modelin as¸iri o¨grenmeye gittigi go¨zlemlenmis¸tir. Modelin do¨nemlere go¨re bas¸arisi dogruluk/do¨nem grafigine go¨re S¸ ekil 3'den incelenebilir.

TABLO II: Kullanilan O¨ n Egitimli BERT Modelleri ve GPT-2 Modelinin Parametreleri

Model
bert-base-multilingual-cased [12] bert-base-multilingual-uncased [12] dbmdz/bert-base-turkish-128k-cased [13] dbmdz/bert-base-turkish-128k-uncased [14] dbmdz/bert-base-turkish-cased [15] dbmdz/bert-base-turkish-uncased [14] Geotrend/bert-base-tr-cased [16] loodos/bert-base-turkish-cased [17] loodos/bert-base-turkish-uncased [18] dbmdz/distilbert-base-turkish-cased [19] adresgezgini/turkish-gpt-2 [20]

Gizli Katman Boyutu
768
768

Maksimum Sekans
Uzunlug u
512
1024

Dikkat Ana Bas¸lig i Sayisi
12
12

Gizli Katman Sayisi
12
6 12

Mimari
BertForMaskedLM
DistilBertForMaskedLM GPT2LMHeadModel

Dil
C¸ ok dilli C¸ ok dilli Tu¨ rkc¸ e Tu¨ rkc¸ e Tu¨ rkc¸ e Tu¨ rkc¸ e Tu¨ rkc¸ e Tu¨ rkc¸ e Tu¨ rkc¸ e Tu¨ rkc¸ e Tu¨ rkc¸ e

So¨zlu¨ k Boyutu
119547 105879 128000 128000 32000 32000 19099 32000 32000 32000 50000

TABLO III: Karmas¸iklik Matrisi

Tahmin edilen sinif

Is¸ bas¸vurusu (1)
Is¸ bas¸vurusu degil (0)

Ait oldugu sinif

Is¸ bas¸vurusu (1)

Is¸ bas¸vurusu degil (0)

S¸irketinizde c¸alis¸mak ic¸in

Is¸yerim ic¸in reklam

nereyi doldurmam gerekir

vermek istiyorum

TP

FP

Is¸e alimlar devam

Sosyal medya reklamlari

ediyor mu

nasil olus¸turuluyor

FN

TN

S¸ ekil 3: loodos/bert-base-turkish-cased [17] Modeli 10 Do¨nem Egitim Sonuc¸lari
V. SONUC¸ LAR
Bu c¸alis¸mada, o¨nceden egitilmis¸ BERT modellerinin siniflandirma problemi u¨zerindeki performanslari kars¸ilas¸tirilarak en yu¨ksek bas¸ari seviyesine sahip model belirlenmeye c¸alis¸ilmis¸ ve bu modeller ayni zamanda kendi egitttigimiz GPT-2 modelinin ayni problem o¨zelindeki bas¸arisi ile kars¸ilas¸tirilmis¸tir. GPT-2 ile BERT modelinde elde edilen bas¸ari seviyelerine ulas¸ilamamis¸tir. Gerc¸ekles¸tirilen egitimler dogrultusunda BERT modelleri arasindan Loodos ekibine ait online bloglardan, e-kitaplardan, gazetelerden, Twitter'dan, makalelerden ve Wikipedia sitesi u¨zerinden toplanan 200GB boyutundaki ku¨lliyat ile o¨nceden egitilmis¸, 32000 adet kelime bulunan so¨zlu¨gu¨ ve 110 milyon parametreye sahip BERT modelinin en iyi bas¸arim sonuc¸larini verdigi go¨zlemlenmis¸tir. Ilerleyen c¸alis¸malarda, dogruluk degeri %90'in altinda kalan siniflar ic¸in GPT-2 gibi u¨retken bir o¨zellige sahip model yardimiyla veri artirma (data augmentation) yoluna gidilerek sonuc¸larin iyiles¸tirilebilecegi o¨ngo¨ru¨lmektedir.

BILGILENDIRME
Bu c¸alis¸ma TU¨ BITAK TEYDEB 1501 programi kapsaminda desteklenmis¸ olan 3190585 numarali "Makine O¨ grenmesi ile Anlamli Diyalog U¨ retebilecek Genel Amac¸li Chatbot Uygulamasi" isimli proje kapsaminda gerc¸ekles¸tirilmis¸tir.

KAYNAKC¸ A

[1] Jain, S., Jain, A. K., and Singh, S. P., "Building a machine learning model for unstructured text classification: Towards hybrid approach," in Rising Threats in Expert Applications and Solutions, pp. 447­454. Springer, 2021.

[2] Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K., "Bert: Pretraining of deep bidirectional transformers for language understanding," arXiv preprint arXiv:1810.04805, 2018.
[3] O¨ zc¸ift, A., Akarsu, K., Yumuk, F., and So¨ylemez, C., "Advancing natural language processing (nlp) applications of morphologically rich languages with bidirectional encoder representations from transformers (bert): an empirical case study for turkish," Automatika, pp. 1­13, 2021.

[4] Ro¨nnqvist, S., Kanerva, J., Salakoski, T., and Ginter, F., "Is multilingual bert fluent in language generation?," arXiv preprint arXiv:1910.03806, 2019.

[5] Tanaka, H., Shinnou, H., Cao, R., Bai, J., and Ma, W., "Document classification by word embeddings of bert," in International Conference of the Pacific Association for Computational Linguistics. Springer, 2019, pp. 145­154.

[6] S¸ u¨kru¨ Ozan ve Emre Tas¸ar,, "Kisa konus¸ma cu¨mlelerinin dogal dil Is¸leme yo¨ntemlerini kullanarak otomatik etiketlenmesi," 29. IEEE Sinyal Is¸leme ve Iletis¸im Uygulamalari Kurultayi, SIU 2021, C¸ evrimic¸i,
Basim as¸amasinda bildiri., 2021.

[7] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I., "Language models are unsupervised multitask learners," 2019.

[8] Whitfield, D., "Using gpt-2 to create synthetic data to improve the prediction performance of nlp machine learning classification models," arXiv preprint arXiv:2104.10658, 2021.

[9] Sun, C., Qiu, X., Xu, Y., and Huang, X., "How to fine-tune bert for text classification?," ArXiv, vol. abs/1905.05583, 2019.

[10] Sanh, V., Debut, L., Chaumond, J., and Wolf, T., "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter," arXiv preprint arXiv:1910.01108, 2019.

[11] Mihaila, G.,

"Gpt2 for text classifi-

cation

using

hugging

face

transformers,"

https://gmihaila.github.io/tutorial notebooks/gpt2 finetune classification/,

Mar 2021.

[12] Devlin, J., Chang, M., Lee, K., and Toutanova, K., "BERT: pre-training of deep bidirectional transformers for language understanding," CoRR, vol. abs/1810.04805, 2018.

[13] dbmdz,, "dbmdz/bert-base-turkish-128k-cased · hugging face," https://huggingface.co/dbmdz/bert-base-turkish-128k-cased, Mar 2020.

S¸ ekil 4: BERT ve GPT-2 Modelleri Egitim Sonuc¸lari

[14] dbmdz,, "dbmdz/bert-base-turkish-128k-uncased · hugging face," https://huggingface.co/dbmdz/bert-base-turkish-128k-uncased, Mar 2020.

[15] dbmdz,,

"dbmdz/bert-base-turkish-uncased · hugging face,"

https://huggingface.co/dbmdz/bert-base-turkish-uncased, Mar 2020.

[16] Abdaoui, A., Pradel, C., and Sigel, G., "Load what you need: Smaller versions of mutlilingual bert," in SustaiNLP / EMNLP, 2020.

[17] Loodos,,

"loodos/bert-base-turkish-cased · hugging face,"

https://github.com/Loodos/turkish-language-models, Dec 2020.

[18] Loodos,,

"loodos/bert-base-turkish-uncased · hugging face,"

https://github.com/Loodos/turkish-language-models, Aug 2020.

[19] dbmdz,, "dbmdz/distilbert-base-turkish-uncased · hugging face," https://huggingface.co/dbmdz/distilbert-base-turkish-cased , Mar 2020.

[20] AdresGezgini, I., "/adresgezgini/turkish-gpt-2 · hugging face," https://api- inference.huggingface.co/models/adresgezgini/turkish- gpt- 2 , Jan 2021.

[21] Powers, D. M., "Evaluation: from precision, recall and f-measure to roc, informedness, markedness and correlation," arXiv preprint arXiv:2010.16061, 2020.

S¸ ekil 5: loodos/bert-base-turkish-cased [17] Modeli 3 Do¨nem Egitim Sonuc¸lari

