PREPRINT(2021)

1

A General Framework for Learning-Based Distributionally Robust MPC of Markov Jump Systems
Mathijs Schuurmans and Panagiotis Patrinos

arXiv:2106.00561v1 [math.OC] 1 Jun 2021

Abstract-- We present a data-driven model predictive control (MPC) scheme for chance-constrained Markov jump systems with unknown switching probabilities. Using samples of the underlying Markov chain, ambiguity sets of transition probabilities are estimated which include the true conditional probability distributions with high probability. These sets are updated online and used to formulate a time-varying, risk-averse optimal control problem. We prove recursive feasibility of the resulting MPC scheme and show that the original chance constraints remain satisfied at every time step. Furthermore, we show that under sufficient decrease of the confidence levels, the resulting MPC scheme renders the closed-loop system mean-square stable with respect to the truebut-unknown distributions, while remaining less conservative than a fully robust approach. Finally, we show that the data-driven value function converges to its nominal counterpart as the sample size grows to infinity. We illustrate our approach on a numerical example.
I. INTRODUCTION
A. Background, motivation and related work
Due to the ubiquitous nature of stochastic uncertainty in processes arising in virtually all branches of science and engineering, control of dynamical systems perturbed by stochastic processes is a long standing topic of research. model predictive control (MPC) ­ stochastic MPC in particular ­ has been a popular and successful tool in this endeavour, due to its ability to naturally include probabilistic information directly into the control design via the cost, the dynamics and the constraints [2]­[4]. In classical stochastic MPC, however, it is typically assumed that the distribution of the underlying stochastic process is known, although in practice, this is mostly not the case. If the disturbance takes values on a bounded set, the absence of full distributional knowledge can be taken into account by designing the controller under the worst-case realization of the stochastic disturbance. This approach is commonly referred to as robust MPC [2], [4].
An obvious drawback of robust approaches is that the complete disregard of the probabilistic nature of the disturbance can be rather crude, resulting in a tendency for overly conservative decisions. As an alternative approach, one may simply compute an empirical estimate of the disturbance distribution and replace the true value by this estimate in the optimal control problem. Although this is a reasonable approach, given a sufficient amount of data, for more moderate sample sizes, there may be a significant misestimation of the underlying distributions -- often referred to as ambiguity. It is well known that this is likely to cause degradation of the resulting performance when evaluated on new samples from the true
M. Schuurmans and P. Patrinos are with the Department of Electrical Engineering (ESAT-STADIUS), KU Leuven, Kasteelpark Arenberg 10, 3001 Leuven, Belgium. Email: {mathijs.schuurmans, panos.patrinos}@esat.kuleuven.be
This work was supported by: FWO projects: No. G086318N; No. G086518N; Fonds de la Recherche Scientifique ­ FNRS, the Fonds Wetenschappelijk Onderzoek­Vlaanderen under EOS Project No. 30468160 (SeLMA), Research Council KU Leuven C1 project No. C14/18/068 and the Ford­KU Leuven Research Alliance project No. KUL0023.
A preliminary version of this work has been presented at the 59th IEEE Conference on Decision and Control [1].

distribution. This phenomenon is known as the optimizer's curse [5]. To account for this ambiguity, one could, instead of a point estimate, construct a set of distributions (an ambiguity set) that is in some sense consistent with the data. By accounting for the worst-case distribution within this set, the decision maker is protected against the limitations of the finite sample size.
This approach, known as distributionally robust (DR) optimization [6], addresses the drawbacks of the above approaches by utilizing available data, but only to the extent that it is statistically meaningful. As more data is gathered online and ambiguity sets get updated accordingly, it is expected that these sets will shrink, so that the optimal decisions gradually become less conservative. This, among other desirable properties, has caused an increasing popularity of DR methods in recent years, initially mostly in stochastic programming and operations research communities [5], [7]­[10] and more recently in (optimal) control [11]­[15] as well. See also [16] for comprehensive review. Much of the earlier work focuses on the study of particular classes of ambiguity sets, each modelling certain structural assumptions on the underlying distribution. Our analysis, however, does not require a particular family of ambiguity sets. We illustrate this in Section III, by reviewing some commonly used ambiguity set classes and showing how they fit into our proposed framework.
As the focus of research in data-driven and learning-based control is gradually shifting towards real-life, safety-critical applications, there has been an increasing concern for safety guarantees of datadriven methods, which are valid in a finite data regime. This has led to a variety of different approaches besides distributionally robust methodologies, each valid under different assumptions on the datagenerating process and the controlled systems. For instance, this has led to data-driven variants of tube-based MPC [17], [18], Gaussianprocess based estimation with reachability-based safe set constraints [19], or Data-enabled predictive control ("DeePC") [20] combining Willems' fundamental lemma with MPC for linear systems. We refer to [21] for a recent survey.
In this work, we allow for general (possibly nonlinear) dynamics under stochastic disturbances with unknown distribution, and subject to chance constraints. However, we restrict our attention to finitelysupported stochastic disturbances. One of the advantages of this construction is that the predicted evolution of the system can be represented on a scenario tree, which allows us to explicitly (and without approximation) optimize over closed-loop control policies, rather than open-loop sequences. This property helps combat excessive conservatism due to accumulation of uncertainty over the prediction horizon [22]­[24]. Motivated by similar considerations, [25] and [26] utilize scenario trees to approximate the realizations of continuous disturbances. [26] then considers safety separately by projecting the computed control action onto a set of control actions that keep the state within safe robust control invariant (RCI) set, similarly to [19]. This projection requires the additional solution of a mixed-integer quadratic program (MIQP), whenever the used RCI set is polyhedral. In our setting, however, we consider the switching behavior inherent to the system, allowing us to provide safety guarantees directly through the application of MPC theory on the joint controller-learner system.
We will in particular assume that the underlying disturbance

2

PREPRINT(2021)

process is a Markov chain, leading to a system class commonly referred to as Markov jump systems. Control of this class of systems has been widely studied and has been used to model systems stemming from a wide range of applications [23], [27], [28]. In the known distribution case, stability analysis of nonlinear stochastic MPC for this system class has been performed from a worst-case perspective [29], in mean-square sense [28] and in the more general risk-square sense [30], [31]. Recently, data-driven methods have been proposed to design controllers for unknown transition probabilities [32], [33], but relatively little attention has gone to providing a priori guarantees on stability and constraint satisfaction with respect to the true distributions, which is the objective of this work. By the dual interpretation of risk measures [34, Thm. 6.4], the notion of risksquare stability in [30] guarantees mean-square stability with respect to all the distributions within some set of distributions induced by the used risk measure. We show that by careful design of a data-driven ambiguity set over subsequent time steps ­ which only contain the true distributions with high probability ­ this concept can be extended to show mean-square stability with respect to the true distribution, under some additional assumptions.
We finally study the convergence of the optimal value function of our data-driven controller to the nominal counterpart. This property, known as asymptotic consistency, has recently been studied in the stochastic optimization literature for (static) distributionally robust optimization problems under Wasserstein ambiguity [5], [35]. A common assumption in this line of work is Lipschitz continuity of the cost/constraint functions with respect to the random variable. This assumption is not suitable for our purposes, since we consider discrete random variables w  W for which a suitable norm may not exist. In our setting, we will in some cases need to resort to a uniform boundedness assumption, which serves a similar purpose. In the nonconvex case, the authors of [35] base their analysis on [36], in which the ambiguity sets are not assumed to be random. An additional assumption is added that the constraint boundary has probability zero, such that almost everywhere, the constraint is continuous. This assumptions helps in dealing with the discontinuity of the stepfunction at 0 which is inherent to chance constraints. Alternatively, the chance constraints can be replaced risk constraints involving the average value-at-risk [37], which circumvents this issue. Besides the mentioned differences in set-up, some additional care is required to handle the multistage nature of the stochastic optimization problems considered here.
B. Contributions
(i) We present a general data-driven, DR-MPC framework for Markov switching systems with unknown transition probabilities. The resulting closed-loop system satisfies the (chance) constraints of the original stochastic problem and allows for online improvement of performance based on observed data. Thus, we extend the recently developed framework of risk-averse MPC [30], [31], [38] to a data-driven setting, in which the involved risk measures are selected and calibrated automatically based on their dual (DR) interpretation to obtain meaningful statistical guarantees on the resulting controllers. (ii) We provide sufficient conditions for recursive feasibility and mean-square stability of the DR-MPC law, with respect to the true-but-unknown distribution. To this end, we state the problem in terms of an augmented state vector of constant dimension, which summarizes the available information at every time. The dynamics of this so-called learner state can be easily expressed for common choices for the ambiguity set. This idea, which is closely related to that of sufficient statistics [39, Ch. 5] and information states in partially-observed Markov decision processes [40] allows us to

formulate the otherwise time-varying optimal control problem as a dynamic programming recursion, facilitating stability analysis of the original control system and the learning system jointly. (iii) We provide sufficient conditions under which the value of the DR problem converges from above to that of the nominal optimal control problem. Extending existing results in stochastic optimization to the multi-stage, dynamic setting.

C. Notation
Let IN denote the set of natural numbers and IN>0 := IN \ {0}. For two naturals a, b  IN with a  b, we denote IN[a,b] := {n  IN | a  n  b} and similarly, we introduce the shorthand w[a,b] := (wt)bt=a to denote a sequence of variables indexed from a to b. We denote the extended real line by IR := IR  {±} and the set of nonnegative (extended) real numbers by IR+ (and IR+). The cardinality of a (finite) set W is denoted by |W |. We write f : X  Y to denote that f is a set-valued mapping from X to Y . A function is lower semicontinuous (lsc) if its epigraph is closed. Given a matrix P  IRn×m, we denote its (i, j)'th element by Pij and its i'th row as Pi:  IRm. The i'th element of a vector x is denoted xi. vec(M ) denotes the vertical concatenation of the columns of a matrix M . We denote the vector in IRk with all elements one as 1k := (1)ki=1 and the probability simplex of dimension k as k := {p  IRk+ | p 1k = 1}. We define the function 1x=y = 1 if x = y and 0 otherwise. The indicator function X : IRn  IR of a set X  IRn is defined by X (x) = 0 if x  X and  otherwise. The level set of a function V : IRn  IR is denoted lev V := {x  IRn | V (x)  }. The interior of a set X is denoted int X. Finally, we denote the positive part of a quantity x as [x]+ := max{0, x}, where max is taken element-wise. We say that a function  : IR+  IR+ belongs to the class of K functions if it is continuous, strictly increasing, unbounded, and zero at zero [4]. Given a nonempty, proper cone K, the generalized inequality a K b is equivalent to b - a  K. K := {y | x, y  0, x  K} denotes the dual cone of K.

II. PROBLEM STATEMENT AND STRUCTURAL

ASSUMPTIONS

Let w := (wt)tIN denote a discrete-time, time-homogeneous Markov chain defined on some probability space (, F, P) and taking

values on W := IN[1,d]. The transition kernel governing the Markov chain is denoted by P = (Pij )i,jW , where Pij = P[wt = j | wt-1 = i]. As such, the sample space and -algebra can be identified with  = W  and F = 2, respectively, and correspondingly,

for any (wt)tIN  , P[(wt)tIN] = p0

 t=0

Pwt wt+1

,

where

p0  d is the initial distribution. We refer to wt as the mode of the

chain at time t. For simplicity, we will assume that the initial mode

is known to be i, so p0 = (1w=i)wW . As such, the Markov chain will be fully characterized by its transition kernel. Finally, we will

assume that the Markov chain is ergodic.

Assumption II.1 (Ergodicity). The Markov chain (wt)tIN is ergodic, i.e., there exists a value k  IN>0, such that P k > 0 elementwise, for some k  1.

This assumption, which states that there every mode is reachable

from any other mode in k steps, ensures that every mode of the

chain gets visited infinitely often [41, Ex. 8.7]. This will allow us to

guarantee convergence of the proposed data-driven MPC scheme to

its nominal counterpart. (See Section VI.)

We will consider discrete time dynamical systems with dynamics

of the form

xt+1 = f (xt, ut, wt+1),

(1)

SCHUURMANS et al.: A GENERAL FRAMEWORK FOR LEARNING-BASED DISTRIBUTIONALLY ROBUST MPC OF MARKOV JUMP SYSTEMS

3

where xt  IRnx , ut  IRnu are the state and control action at time t, respectively. We will assume that the state xt and mode wt are observable at time t. This is equivalent to the more common notation xt+1 = f (xt, ut, wt), assuming wt-1 is observable. However, as we will consider wt to be part of the system state at time t, the notation of (1) will be more convenient.
Since wt is drawn from a Markov chain, such systems are commonly referred to as Markov jump systems. Whenever f (· , · , w) is a linear function, (1) describes a Markov jump linear system [27]. Since the state xt and mode wt are observable at time t, the distribution of xt+1 depends solely on the conditional switching distribution Pwt:, for a given control action ut.
For a given state-mode pair (x, w)  IRnx × W , we impose ng chance constraints of the form

P[gi(x, u, w, v) > 0 | x, w]  i, i  IN[1,ng],

(2)

where v  Pw: is randomly drawn from the Markov chain w in mode w, and gi : IRnx × IRnu × W 2  IR are constraint functions with corresponding constraint violation rates i. By appropriate choices of i and gi, constraint (2) can be used to encode robust constraints (i = 0) or chance constraints (0 < i < 1) on the state, the control action, or both. Note that the formulation (2) additionally covers chance constraints on the successor state f (x, u, v) under input u, conditioned on the current values x and w. To ease notation, we will without loss of generality assume that ng = 1. In their standard form, chance constraints lead to nonconvex, nonsmooth (even discontinuous) constraints. For this reason, they are commonly approximated using risk measures [37]. Particularly, the (conditional) average value-at-risk (at level   [0, 1] and with reference distribution p  d) of the random variable  : W 2  IR is defined as

AV@Rp[(w, v) | w]



min t + 1/ IEp = tIR

[(w, v) - t]+ | w ,

=0

(3)

maxvW {(w, v)} ,

 = 0.

It can be shown that if p = Pw:, then the following implication holds tightly [34, sec. 6.2.4]

AV@Rp[(w, v) | w]  0  P[(w, v)  0 | w]  1 - . (4)
By exploiting the dual risk representation [34, Thm 6.5], the left-hand inequality in (4) can be formulated in terms of only linear constraints [38]. As such, it can be used as a tractable surrogate for the original chance constraints. Consequently, the set of feasible control actions as a function of x and w can be written as

U (x, w) := u  U : AV@RPw: g(x, u, w, v) | x, w  0 , (5)

where U  IRnu is a nonempty, closed set.

Ideally, our goal is to synthesize ­ by means of a stochastic MPC

scheme ­ a stabilizing control law N : IRnx ×W  IRnu , such that

for the closed loop system xt+1 = f (xt, N (xt, wt), wt+1), it holds

almost surely that N (xt, wt)  U (xt, wt), for all t  IN. Consider

a sequence of N control laws  of length N . Given a stage cost

= :

(IRnkx)Nk×=-0IR1,nruef×erWred

to 

as a policy IR+, and a

terminal cost Vf : IRnx × W  IR+ and corresponding terminal set

Xf : Vf (x, w) := Vf (x, w) policy , a cost

+

Xf (x, w),

we

can

assign

to

each

such

VN(x, w) := IE

N -1 k=0

(xk, uk, wk) + Vf (xN , wN ) ,

(6)

where xk+1 = f (xk, uk, wk+1), uk = k(xk, wk) and (x0, w0) = (x, w), for k  IN[0,N-1]. This defines the following stochastic optimal control problem (OCP).

Definition II.2 (Stochastic OCP). For a given state-mode pair (x, w), the optimal cost of the stochastic OCP is

VN

(x,

w)

=

min


VN (x,

w)

(7a)

subject to

x0 = x, w0 = w,  = (k)Nk=-01,

(7b)

xk+1 = f (xk, k(xk, wk), wk+1),

(7c)

k(xk, wk)  U (xk, wk), k  IN[0,N-1].

(7d)

We denote by N (x, w) the corresponding set of minimizers.
To ensure existence of a solution to (7) (and its DR counterpart, defined in Section IV), we will impose the following (standard) regularity conditions [4], [30].

Assumption II.3 (Problem regularity). The following are satisfied
for all w, v  W :
(i) Functions ( · , · , w) : IRnx × IRnu  IR+, Vf ( · , w) : IRnx  IR+, f ( · , · , w), and gi( · , · , w, v), i  IN[1,ng] are continuous;
(ii) U and Xf are closed; (iii) f (0, 0, w) = 0, (0, 0, w) = 0, 0  U (0, w), and Vf (0, w) = 0; (iv) One of the following is satisfied:

1) U is compact; or 2) (x, u, w)  c( u ) with c  K, for all (x, u)  IRnx ×U .

Let (k(x, control law is

wgi)v)eNkn=-b01y

 N (x, w), so that N (x, w) = 0 (x, w).

the stochastic MPC Sufficient conditions

on the terminal cost Vf and its effective domain dom Vf = Xf to

ensure mean-square stability of the closed-loop system, have been

studied for a similar problem set-up in [28], among others.

Both designing and computing such a stochastic MPC law requires

knowledge of the probability distribution governing the state dynam-

ics (1), or equivalently, of the transition kernel P . In the absence of

this knowledge, these probabilities are to be estimated from a finitely-

sized data set and therefore subject to some level of ambiguity. Our

goal is to devise an MPC scheme which uses the available data

in a principled manner, while explicitly taking this ambiguity into

account.

To this end, we introduce the notion of a learner state, which is

very similar in spirit to the concept of an information state, commonly

used in control of partially observed Markov decision processes [40],

where ­ in contrast to our approach ­ it is typically adopted in a

Bayesian setting. In both cases, however, it can be regarded as an

internal state of the controller that stores all the information required

to build (a set of) conditional distributions over the next state, given

the observed data. We formalize this in the following assumption.

Assumption II.4 (Learning system). Given a sequence w[0,t] sam-

pled from the Markov chain w, we can compute (i) a statistic

st : W t+1  S  IRns , with S vector of confidence parameters t

compact, accompanied = (t,i)ni=1  I := [0,

by a 1]n ,

which admit recursive update rules st+1 = L(st, t, wt, wt+1) and t+1 = C(t), t  IN; and (ii) an ambiguity set

A : S × W × [0, 1]  d : (s, w, )  A(s, w), mapping st,

wt and the component t,i to a convex subset of the d-dimensional

probability simplex d, such that for all t  IN, and for all

i  IN[1,n ],

P[Pwt:  At,i (st, wt)]  1 - t,i.

(8)

We will refer to st and t as the state of the learner and the confidence vector at time t, respectively.

Remark II.5 (confidence levels). Two points of clarification are in order. First, we consider a vector of confidence levels, rather than a

4

PREPRINT(2021)

single value. This is motivated by the fact that one would often wish to assign separate confidence levels to ambiguity sets corresponding to the cost function on the one hand; and to those corresponding to the ng chance constraints on the other hand (See Definition IV.3). Accordingly, we will assume that n = ng + 1.
Second, the confidence levels are completely exogenous to the system dynamics and can in principle be chosen to be any timevarying sequence satisfying the technical conditions discussed further (see Proposition IV.1 and Assumption II.7). The requirement that the sequence (t)tIN can be written as the trajectory of a timeinvariant dynamical system serves to facilitate theoretical analysis of the proposed scheme through dynamic programming.
We will furthermore require the following restrictions on the choice of the learning dynamics the confidence levels.

Assumption II.6. There exists a stationary learner state s = L(s , , w, v), for all (, w, v)  I × W 2, such that from any
initial state s0, limt st = s , a.s.

Assumption II.7. The confidence dynamics t+1 = C(t) is chosen

such that

 t=0

t

<

,

element-wise.

Assumption II.6 imposes that asymptotically, the learner settles down to some value which is no longer modified by additional data. It is natural to assume that in such a state, the learner unambiguously models the underlying distribution, as demonstrated, for instance, in Example III.6. However, without further assumptions, one could also consider the trivial case where S = {s } and e.g., A(s, w) = d, in which case, no learning occurs and, in fact, a robust MPC scheme is recovered. In Section VI-D, we will pose an additional constraint on the learning system, which excludes this case, but allows us to show consistency of the data-driven controller. Assumption II.7 states that the probability of obtaining an ambiguity set that contains the true conditional distribution (expressed by (8)) increases sufficiently fast. This assumption will be of crucial importance in showing stability (see Section VI-C). To fix ideas, we keep the following example in mind as a suitable choice for the confidence dynamics throughout the article.

Example II.8 (Confidence dynamics). A suitable family of sequences for the confidence levels satisfying Assumption II.7 (assuming n = 1 for simplicity) is obtained as

t = b(1 + t)-q, t  IN,

with parameters 0  b  1, q > 1. This sequence can be described by the recursion t+1 = C(t) = bt(1t/q + b1/q)-q, 0 = b. Thus, it additionally satisfies the requirements of Assumption II.4.
The learner state st will in most practical cases be composed of a sufficient statistic for the transition kernel and some parameter calibrating the size of the ambiguity set, based on statistical information. See Section III-A for some concrete examples.
Equipped with a generic learning system of this form, our aim is to find a data-driven approximation to the stochastic OCP defined by (7), which asymptotically attains the optimal cost while preserving stability and constraint satisfaction during closed-loop operation.
The remainder of this work is organized as follows. Section III presents and compares several classes of ambiguity sets found in the literature, and discusses how they fit in the framework of Assumption II.4. In Section IV, we construct a distributionally robust counterpart to the optimal control problem in terms of the ingredients introduced above. Section VI contains a theoretical analysis of the proposed scheme; and in Section VII, we illustrate the approach on a numerical example.

III. CONSTRUCTION OF AMBIGUITY SETS
To exemplify how a learning system of the form proposed in Assumption II.4 can be constructed in practice, we will now review some particular classes of ambiguity sets that have been proposed in the literature, and how they fit into the present framework. In many cases, ambiguity sets are defined as the set of distributions that lie within some radius from an empirical estimate using a particular distance metric or divergence. We will refer to such ambiguity sets as divergence-based ambiguity sets. For general, continuous distributions, popular choices for the distance metric/divergence include the Wasserstein distance [5], [13] or moment-based ambiguity sets [12], [42], where the first two moments of the distributions are confined to a ball around the empirical estimate.
For the setting involving finitely supported distributions, [43] proposes likelihood regions: ambiguity sets containing all distributions with respect to which the likelihood of observed data is larger than some threshold . [43] provides a data-driven estimate for  to satisfy a condition similar to (8) using asymptotic results. However, a modification to provide finite sample guarantees is straightforward. Closely related to this family of ambiguity sets are defined by considering distributions that are close to the empirical distribution as measured by the Kullback-Leibler (KL) divergence. Depending on the ordering of the arguments in the KL divergence one either obtains the ambiguity set proposed in [7] or the ambiguity set corresponding to the entropic value-at-risk [44].
Furthermore ambiguity sets defined as balls in the total variation (TV) metric are quite commonly used [11], [45], [46]. More generally, [47], [48] provide tractable formulations of linear programs under ambiguity, considering the broad class of -divergences, which include the KL divergence and TV distance as special cases. However, the parameters controlling the size of the ambiguity sets are calibrated on data using asymptotic results.
In what follows, we will focus on the KL and TV ambiguity sets and show how they satisfy Assumption II.4.

A. Divergence-based ambiguity sets

Our goal is to obtain for each mode w of the Markov chain, a data-driven subset of the probability simplex, containing the wth row of the transition kernel P with high probability. Given a sequence w[1,t]  W t of t  IN samples drawn from the Markov chain w,
d individual datasets Wt,i := {wk+1 | wk = i, k  IN[1,t]}, i  W can be obtained by partitioning the set of observed transitions by the mode they originated in. As such, each Wt,i contains ti i.i.d. draws from the distribution Pi:. Ambiguity sets can now be constructed for each individual row i, using concentration inequalities based on the data in Wt,i. See, for instance, [49], [50] for more details on related techniques.
With this set-up, we now consider the following broad class of ambiguity sets. In the remainder of this section, we will for ease of notation consider a scalar confidence level t  [0, 1].

Definition III.1 (Divergence-based ambiguity set). Let the learner

state be composed as st = (vec Pt, Rt)  dd × IRd, where Pt

denotes the empirical transition probability matrix at time t, that is,

Pt,ij

=

1 ti

wWt,i 1w=j . We say that an ambiguity set At (st, w)

is a divergence-based ambiguity set if it can be expressed in the form

At (st, w) := {p  d | D(Pt,w:, p)  Rt,w}, w  W
where D : d × d  IR+ is some statistical divergence.
Statistically meaningful values for the radii Rt,w under different choices of divergences can be obtained using the following standard results.

SCHUURMANS et al.: A GENERAL FRAMEWORK FOR LEARNING-BASED DISTRIBUTIONALLY ROBUST MPC OF MARKOV JUMP SYSTEMS

5

Proposition III.2 (Concentration inequalities). Let p  d

denote a distribution on the probability simplex and p =

1 m

m-1 t=0

(1wt=i

)di=1

the

empirical

distribution

based

on

m

i.i.d.

draws wt  p. Then, P

1 2

p-p

2 1

> rTV(m, )

 , with

rTV(m, )

=

d log 2 - log  .

(9)

2m

Similarly, it holds that P[DKL(p, p) > rKL(m, )]  , with

rKL(m, ) = d log m - log  ,

(10)

m

where DKL(p, q) :=

d i=1

pi

log

pi qi

denotes the KL divergence from

q to p.

The bound on the TV distance (9) is known as the BretagnolleHuber-Carol inequality [51, Thm. A.6.6].

Remark III.3. Expression (10) for the KL radius is a well-known

result from the field of information theory, obtained through the

so-called method-of-types [52], [53]. A slight improvement can be

obtained by replacing d log m by log

m+d-1 d-1

.

Moreover,

in

[54],

an even sharper result for (10) is derived. In fact, this improved

concentration bound in the KL divergence was used in the same work

to improve upon the TV Pinsker's inequality [55],

concentration which relates

tbhoeuTndV(d9i)stfaonrcemdbetw1e,enusdinisg-

tributions p, q  d to the KL divergence as

p-q

2 1

 2DKL(p, q).

These improvements remain compatible with the framework but

would complicate notations in subsequent analysis. For this reason,

we will define the following divergence-based ambiguity sets using

Proposition III.2.

Definition III.4 (TV ambiguity set). The TV ambiguity set

ADT(ptV,(qs)t,=w)21

is p

a -

divergence-based ambiguity set with divergence q 1, p, q  d and radius Rt,w = rTV(tw, t)

for every w  W .

Definition III.5 (KL ambiguity set). The KL ambiguity set

AKtL(st, w) is D = DKL and

a divergence-based ambiguity radius Rt,w = rKL(tw, t) for

set with divergence every w  W .

Since the radius rTV( · , ) is a monotone decreasing function, one can uniquely recover the corresponding sample size mode-specific sample sizes ti(st, t) as a function of the current learner state st by inverting the function. Using this fact, one can construct a timeinvariant, recursive update rule for the transition probabilities and ambiguity radii by means of straightforward manipulations.

Example III.6 (Learner dynamics for the TV ambiguity set). Maintaining n = 1 here for simplicity, recall that C : [0, 1]  [0, 1] denotes the dynamics for the confidence levels.
Consider a TV ambiguity set as defined in Definition III.4, so that the learner state is represented as s = (vec P , R). Let () := d log 2 - log . Then, using (9) to solve for the modespecific sample sizes, it is easy to verify that the dynamics for empirical transition probabilities from mode i to j can be written recursively as



Pi+j (s,

,

w,

v)

=

 ()Pij +2Ri1w=iv=j
 ( )+2Ri 1w=i

Pij

if  > 0 otherwise,

(11)

where we have continuously extended the function for  = 0. Similarly, an update rule for the radii is given by

Ri+(s, , w, v)

=

Ri(C()) , () + 2Ri1w=i

iW

(12)

If c

C 

is 0

c(hwohsiecnh,tofosar tiisnfsytalnimce, is 0thded

log case

C() = c for some constant in Example II.8), then, the

limit of (12) as  tends to 0 exists, and its domain can again be continuously extended to the full interval [0, 1]. Note that if all modes are visited infinitely often, then by construction, P converges to P and R converges to 0, which form fixed points for dynamics (11)­ (12), and hence Assumption II.4 is satisfied. Combining the update rules (11) and (12), we obtain a continuous function L representing the learner dynamics.
For the KL divergence, matters are slightly more complicated as the expression (10) is not invertible. It can be made compatible with the framework of Assumption II.4 for instance by upper-bounding rKL for very small sample sizes, such that the resulting function is invertible. In this case, a similar procedure as Example III.6 can be followed. In practice, however, the sample size can simply be stored, leading to simple to derive, but time-varying dynamics for the learner. The analysis of Section VI can be readily extended to this time-varying case, but for ease of exposition, we do not explicitly take this possibility into account here.

IV. DATA-DRIVEN MODEL PREDICTIVE CONTROL Given a learning system satisfying Assumption II.4, we define the augmented state yt = (xt, st, t)  Y := IRnx × S × I, which evolves over time according to the dynamics

yt+1 = f~(yt, wt, ut, wt+1) :=

f (xt,ut,wt+1)
L(st,t,wt,wt+1) ,
C (t )

(13)

with wt+1  Pwt:, for t  IN. Furthermore, it will be convenient to define the process zt = (yt, wt)  Z := Y × W . Consequently, the objective is now to obtain a feedback law  : Z  IRnu . To this end, we will formulate a DR counterpart to the stochastic OCP (7), in which the expectation operator in the cost and the conditional probabilities in the constraint will be replaced by operators that account for ambiguity in the involved distributions.

A. Ambiguity and risk
In order to reformulate the cost function (6), we first introduce an ambiguous conditional expectation operator, leading to a formulation akin to the Markovian risk measures utilized in [30], [56]. Consider a function  : Z × W  IR, defining a stochastic process (t)tIN = ((zt, wt+1))tIN on (, F , P), and suppose that the augmented state zt = z = (x, s, , w) is given. Let   [0, 1] denote an arbitrary component of . The ambiguous conditional expectation of (z, v), given z is then

s,w[(z, v)] := max IEp[(z, v)|z]

pA (s,w)

(14)

= max
pA (s,w)

vW pv(z, v).

Trivially, it holds that if the w'th row of the transition matrix lies in the corresponding ambiguity set, i.e., Pw:  A(s, w), then

s,w[(z, v)]  IEPw: [(z, v) | z]

(15)

= vW Pwv(z, v).

Note that the function s,w defines a coherent risk measure [34, Sec. 6.3]. We say that s,w is the risk measure induced by the ambiguity set A(s, w).
A similar construction can be carried out for the chance constraints (5). We robustify the average value-at-risk with respect to the reference distribution, defining

s,,w[(z, v)] :=pAma(xs,wA)V@Rp[(z, v) | z]  0.

(16)

6

PREPRINT(2021)

The function s,,w in turn defines a coherent risk measure. Note that we have replaced the AV@R parameter  by . The reason for this is that the ambiguity set only contains the true distribution with high probability. Considering this fact, it is natural to expect that  needs to be tightened to some extent in order to ensure that the original chance constraint remains satisfied. We make this precise in the following result.

Proposition IV.1. Let ,   [0, 1], be given values with  < .

Consider the random variable s :   S, denoting an (a priori

unknown) learner state satisfying Assumption II.4, i.e., P[Pw: 

A(s, w)]  1 - . If the parameter  is chosen to satisfy 0   

- 1-



1,

then,

for

an

arbitrary

function

g

:

Z ×W



IR,

the

following implication holds:

s,,w[g(z, v)]  0, a.s.  P[g(z, v)  0 | x, w]  1 - . (17) Proof. If s,,w[g(z, v)]  0, a.s., then (4) and (16) imply that
P[g(z, v)  0 | x, w, Pw:  A(s, w)]  1 - , a.s. Therefore,

P[g(z, v)  0 | x, w]
 P[g(z, v)  0 | x, w, Pw:  A(s, w)]P[Pw:  A(s, w)]  (1 - )(1 - ).
Requiring that (1 - )(1 - )  (1 - ) then immediately yields the sought condition.

Notice that the implication (17) in Proposition IV.1 provides an a priori guarantee, since the learner state is considered to be random. In other words, the statement is made before the data is revealed. Indeed, for a given learner state s and mode w, the ambiguity set A(s, w) is fixed and therefore, the outcome of the event E = {Pw:  A(s, w)} is determined. Whether (17) then holds for these fixed values, depends on the outcome of E. This is naturally reflected through the above condition on , which implies that   , and thus tightens the chance constraints that are imposed conditioned on a fixed s. Hence, the possibility that for this particular s, the ambiguity set may not include the conditional distribution, is accounted for. This tightening can be mitigated by decreasing , at the cost of a larger ambiguity set. A more detailed study of this trade-off is left for future work.

B. Distributionally robust model predictive control
We are now ready to describe the DR counterpart to the OCP (7), which, when solved in receding horizon fashion, yields the proposed data-driven MPC scheme.
Consider a given augmented state z = (x, s, , w)  Z. Hereafter, we will assume that  = (, ), where component  is related to the cost function and  is reserved for the constraints.
We use (16) to define the DR set of feasible inputs U(z) in correspondence to (5), as

U (z)= u  U s,,w[g(x, u, w, v)]  0 .

(18)

Remark IV.2. The parameter  remains to be chosen in relation to the confidence levels  and the original violation rates . In light of Proposition IV.1,  = - yields the least conservative choice.
1-
This choice is valid as long as it is ensured that  < .
Using (14), we express the DR cost of a policy  = (k)kN=-01 as

VN (z) := (x0, u0, w0) + s00,w0 (x1, u1, w1) + s11,w1 · · · + sNN--22,wN-2 (xN -1, uN -1, wN -1) + sNN--11,wN-1 [Vf (xN , sN , N , wN )] . . . , (19)

where z0 = z, zk+1 = f~(zk, uk, wk+1) and uk = k(zk), for all k  IN[0,N-1]. In Section VI, conditions on the terminal cost

Vf : Z domain

 IR+ : (x, are provided

s, , w)  in order to

Vgfu(axra, nwte)e+recXufr(sxiv,es,fea,swib)iliatyndanitds

stability of the MPC scheme defined by the following OCP.

Definition IV.3 (DR-OCP). Given an augmented state z  Z, the optimal cost of the distributionally robust optimal control problem (DR-OCP) is

VN

(z)

=

min


VN (z)

(20a)

subject to

(x0, s0, 0, w0) = z,  = (k)kN=-01, zk+1 = (f~(zk, k(zk), wk+1), wk+1),
k(zk)  U (zk), w[0,k]  W k,

(20b) (20c) (20d)

for all k  IN[0,N-1]. We denote by N (z) the corresponding set of minimizers.

Remark IV.4. Note that the definition of Vf implicitly imposes the terminal constraint zN  Xf , a.s.

We now define the data-driven MPC law analogously to the

stochastic case as

N (z) = 0 (z),

(21)

where MPC

sc(hekm(ze))thkNu=-s01consistsNo(fz)r.epAeat teedvleyry(i)timsoelvitn, gth(e20d) attoa-dorbitvaeinn

a control action ut = N (zt) and applying it to the system (1); (ii) observing the outcome of wt+1  W and the corresponding next

state xt+1 = f (xt, ut, wt+1); and (iii) updating the learner state

st+1 = L(st, wt, wt+1) and the confidence levels t+1 = C(t), gradually decreasing the size of the ambiguity sets.

V. TRACTABLE REFORMULATION

A. Conic risk measures
Since ambiguity sets inducing coherent risk measures are convex by construction, many classes of ambiguity sets can be represented using conic inequalities. These risk measures, referred to as conic risk measures, are of great use for reformulating DR-OCPs of the form (20).

Definition V.1 (Conic risk measure). We say that an ambiguity set A  d is conic representable if it can be written in the form

A = {p  d |  : Ep + F  K b},

(22)

with matrices E, F and vector b of suitable dimensions, and a proper cone K. The coherent risk measure induced by a conic representable ambiguity set is called a conic risk measure.
By this definition, a conic risk measure  is given as the optimal value of a standard conic program (CP). Under strong duality, which holds if the CP is strictly feasible [57, Prop. 2.1], its epigraph epi  := {(G, )  IRd+1 |   [G]} can be characterized as [38]

epi  =

(G, )  IRd+1

y : E y = G, F y = 0, y  K,   b y

(23)

Since the TV ambiguity set defined in Section III as well as the ambiguity set inducing the average value-at-risk are polyhedra, they are conic representable (taking the nonnegative orthant as the cone K). Similarly, the KL ambiguity set, and similarly the entropic value-at-risk [44] are known to be conic representable [7], [38]. Additionally, it is not difficult to show that the worst-case average

SCHUURMANS et al.: A GENERAL FRAMEWORK FOR LEARNING-BASED DISTRIBUTIONALLY ROBUST MPC OF MARKOV JUMP SYSTEMS

7

value-at-risk over a conic representable ambiguity set also defines a conic risk measure.

Proposition V.2. Let A = {p  d |  : Ep + F  K b}
be a conic-representable ambiguity set. Then, the risk measure  = maxpA AV@Rp is a conic risk measure.

Proof. For any reference distribution p  d, the ambiguity set AAV@R inducing AV@Rp can be written in the form (22) with E = [ 1d -1d I -I ] , F = 0, K = IR2+(d+1) the nonnegative orthant, and b = [ 1 -1 p 0 ] (which is of the form b = b + Bp) [38]. Writing out the definition of maxpA AV@Rp and rearranging terms yields

max AV@Rp[z]
pA

= max
µ

µ z  :

E 0

µ+

-B 0 EF



IR2+(d+1) ×K

b b

,

which is exactly of the form (22).

Thus, if for all (s, w, )  S × W × [0, 1], A(s, w) is conic representable, then s,w and s,,w are conic risk measures. This fact will allow us to leverage (23) to obtain an efficiently solvable
reformulation of (20).

B. Scenario tree reformulation

w0=(w0) w1=(w1, w2)

(y1, u1) w3

(y0, u0) w1

1

w4

0 w2 (y2, u2)

2

3 y3=f~(y1, u1, w1, w3) 4 y4=f~(y1, u1, w1, w4) 5 y5 6 y6 7 y7

t=0

t=1

t=2

Fig. 1. Scenario tree representation of the state-input sequence.

Since W is a finite set, the possible realizations of w[0,N] can be enumerated and represented on a scenario tree. A scenario tree

with horizon N is a directed acyclical graph which represents the

natural filtration of (, F , P) induced by w[0,N] [58]. An adapted stochastic process (zt) can be represented on such a scenario tree. We denote the value of zt corresponding to a node  in the tree as z, as illustrated in Figure 1. The set of nodes in the tree are partitioned

into time steps or stages. The set of nodes at a stage k is denoted

by nod (k), and similarly, for k0, k1  IN[0,N], with k1 > k0,

nod ([k0, k1]) t  IN[0,N-1],

= we

cakkll1=ak0nondoed+(k).

For a given nod (t + 1)

node   nod (t), that can be reached

from  in one step a child node, denoted +  ch (). Conversely, we

denote the (unique) parent node of a node   nod (t), t  IN[1,N]

by anc ()  nod (t - 1). The nodes   nod (N ) have no child

nodes and are called leaf nodes. The unique node at stage 0 is called

the root node.

An N -step policy  can thus be identified with a collection of

control actions u = {u |   nod ([0, N - 1])}. It therefore suffices

to optimize over a finite number of decision variables rather than

infinite-dimensional control laws.

Proposition V.3 (Tractable reformulation). Given an initial augmented state y = (x, s, ), consider an N -stage scenario tree with

given root mode w0 = w and the corresponding optimal control problem

minimize  0 + 0
,,,x,u
subj. to y0 = y, y+ = f~(y, w, u, w+ ), (x, u, w)   , Vf (xN , wN )  N +  N , ( + + + , )  epi s,w , g(x, u, w, w+ ), 0  epi s,,w , (yN , wN )  Xf ,

(24a)
(24b) (24c) (24d) (24e)
(24f) (24g)

for   nod ([0, N - 1]), +  ch (), and N  nod (N ), where y = (x, s, ). If the ambiguity sets A (s, w) are conic representable, then the optimal cost of (24) is equal to VN (z).
Proof. By Proposition V.2, it follows that both s,w and s,,w are conic risk measures. Thus, the claim is a straightforward application of the results in [38].

If (i) the costs ( · , · , w), Vf ( · , w), the constraint mappings g( · , · , w, v) and terminal set Xf are convex; and (ii) the dynamics f ( ·, ·, w) are affine for all w  W , (24) can be reduced to a convex conic optimization problem. See Section VII for a numerical illustration, as well as [59] for a case study in a slightly simplified setting. Note that the learner and confidence dynamics L and C are independent from the states xt and control actions ut, so the values of s,  over the scenario tree can be precomputed before solving the optimization problem. Therefore, they need not be affine for the problem to remain convex. For nonlinear dynamics f (· , · , v), the problem is no longer convex but can in practice still be solved effectively with standard NLP solvers.
We remark in particular that the conditional risk constraints (24f) for nodes   nod (k) at a stage k are represented here as separate constraints at each node. However, they can be represented equivalently in the framework of [38] as nested risk constraints, which are compositions of a set of conditional risk mappings. In this case, the composition consists of k - 1 max operators over values in the ancestor nodes of  and a conditional risk mapping based on (16) at stage k. This is in line with the observations of [2, Sec. 7.1].

VI. THEORETICAL ANALYSIS
A. Dynamic programming
To facilitate theoretical analysis of the proposed MPC scheme, we follow an approach similar to [30] and represent (20) as a dynamic programming recursion. We define the Bellman operator T as T(V )(z) := minuU(z) (x, u, w) + s,w[V (f~(z, u, v), v)], where z = (x, s, , w)  Z, with  = (, ) as before, are fixed quantities and v  Pw:. We denote by S(V )(z) the corresponding set of minimizers. The optimal cost VN of (20) is obtained through the iteration,

Vk = T Vk-1, V0 = Vf , k  IN[1,N].

(25)

Similarly, Zk := dom Vk is given recursively by

Zk = z u  U (z) : (f~(z, u, v), v)  Zk-1, v  W .

Now consider the stochastic closed-loop system

yt+1 = f~N (zt, wt+1) := f~(zt, N (zt), wt+1),

(26)

where N (zt)  S(VN-1)(zt) is an optimal control law obtained by solving the data-driven DR-OCP of horizon N in receding horizon.

8

PREPRINT(2021)

B. Constraint satisfaction and recursive feasibility
In order to show existence of N  S VN-1 at every time step, Proposition VI.4 will require that Xf is a robust control invariant set. We define robust control invariance for the augmented control system under consideration as follows.
Definition VI.1 (Robust control invariance). A set R  Z is an RCI set for the system (13) if for all z  R, u  U(z) such that (f~(z, u, v), v)  R, v  W . Similarly, R is a robust positive invariant (RPI) set for the closed-loop system (26) if for all z  R, (f~N (z, v), v)  R, v  W .
Since U consists of conditional risk constraints, our definition of robust invariance provides a distributionally robust counterpart to the notion of stochastic robust invariance in [60]. This notion is less conservative than the following, more classical notation of robust invariance.
Definition VI.2 (Classical robust control invariance). A set Rx  IRnx × W is RCI for system (1) in the classical sense if for all x  Rx,
u : g(x, u, w, v)  0, f (x, u, v)  Xf (v), v  W. (27)
In fact, for any set Rx as in Definition VI.2, the set Rx×S×I×W is covered by Definition VI.1, as illustrated in Example VI.3. On the other hand, our notion of robust control invariance is more strict than that of uniform control invariance considered in [30], which only requires successor states to remain in the invariant set for modes v in the cover of the given mode w, i.e., the set of modes v for which Pwv > 0. This flexibility is not available in the current setting, as the transition kernel is assumed to be unknown, so the cover of a mode cannot be determined with certainty.
Example VI.3 (Classical robust invariant set). Suppose that the terminal constraint set Xf of the nominal problem is a robust control invariant set in the classical sense and define for convenience Xf (w) := {x | (x, w)  Xf }. Then, if Xf is chosen such that Xf (w) := {y | (y, w)  Xf } = Xf (w) × S × I, Xf is RCI for the augmented system (13) according to Definition VI.1. Indeed, since AV@Rp[g(x, u, w, v)]  maxv g(x, u, w, v) for all   [0, 1] and p  d, (27) implies that for all z  Xf , there exists u  U (z), such that f~(z, u, v)  Xf (v).
Proposition VI.4 (Recursive feasibility). If Xf is an RCI set for (13), then (20) is recursively feasible. That is, feasibility of DR-OCP (20) for some z  Z, implies feasibility for z+ = (f~N (z, v), v), for all
v  W, N  IN>0.
Proof. The proof follows from a straightforward inductive argument on the prediction horizon N . We first show that if Xf is RCI, then so is ZN . This is done by induction on the horizon N of the OCP.
Base case (N = 0). Trivial, since Z0 = Xf . Induction step (N  N + 1). Suppose that for some N  IN, ZN is RCI for (13). Then, by definition of ZN+1, there exists for each z  ZN+1, a nonempty set UN (z)  U (z) such that for every u  UN (z) and for all v  W , it holds that z+  ZN , where z+ = f~(z, u, v). Furthermore, the induction hypothesis (ZN is RCI), implies that there also exists a u+  U (z+) such that f~(z+, u+, v+)  ZN (v+), v+  W . Therefore, z+ satisfies the conditions defining ZN+1. In other words, ZN+1 is RCI. The claim follows from the fact that for any N > 0 and z  ZN , u = N (z)  S(VN-1)(z)  UN-1(z), as any other choice of u would yield infinite cost in the definition of the Bellman operator.
Corollary VI.5 (Chance constraint satisfaction). If the conditions for Proposition VI.4 hold, then by Proposition IV.1, the stochastic process

(zt)tIN = (xt, st, t, wt)tIN satisfying dynamics (26) satisfies the nominal chance constraints
P[g(xt, N (zt), wt+1) > 0 | xt, wt] < ,
a.s., for all t  IN.
We conclude this section by emphasizing that although the MPC scheme guarantees closed-loop constraint satisfaction, it does so while being less conservative than a fully robust approach, which is recovered by taking A(s, w) = d for all (s, w, )  S × W × [0, 1]. It is apparent from (16) and (18), that for all other choices of the ambiguity set, the set of feasible control actions will be larger (in the sense of set inclusion).
C. Stability
In this section, we will provide sufficient conditions on the control setup under which the origin is mean-square stable (MSS) for (26), i.e., limt IE[ xt 2] = 0 for all x0 in some specified compact set containing the origin.
Our main stability result, stated in Theorem VI.7, hinges in large on the following lemma, which relates risk-square stability [30, Lem. 5] of the origin for the autonomous system (26) (with respect to a statistically determined ambiguity set) to stability in the mean-square sense (with respect to the true distribution).
Lemma VI.6 (Distributionally robust MSS condition). Suppose that Assumption II.7 holds and that there exists a nonnegative, proper function V : Z  IR+, such that (i) dom V is a compact RPI for (26) containing the origin; (ii) s,w[V (f~N (z, v), v)] - V (z)  -c x 2, for some c > 0, for all z  dom V ; (iii) V is uniformly bounded on its domain. Then, limt IE[ xt 2] = 0 for all z0  dom V , where (zt)tIN = (xt, st, t, wt)tIN is the stochastic process governed by dynamics (26).
Proof. See Appendix.
Theorem VI.7 (MPC stability). Suppose that Assumptions II.3 and II.7 are satisfied and the following statements hold. (i) T Vf  Vf ; (ii) c x 2  (x, u, w) for some c > 0, for all z = (x, s, , w)  dom VN and all u  U (z); (iii) VN is locally bounded on its domain. Then, the origin is MSS for the MPCcontrolled system (26), over all compact RPI sets Z  dom VN containing the origin.
Proof. The proof is along the lines of that of [30, thm. 6] and shows that VN satisfies the conditions of Lemma VI.6. Details are in the Appendix.
The results in this section indicate that after an appropriate choice of the learning system, the thusly defined risk measures can be used to design an MPC controller using existing techniques (e.g., those presented in [30]). Corresponding stability guarantees (assuming known transition probabilities) then translate directly into stability guarantees under an ambiguously estimated transition kernel.
D. Asymptotic consistency
Under appropriate constraint qualifications, we can show that the optimal value of the DR-OCP converges to that of the nominal problem as the sample sizes increase, see Theorem VI.11. In the particular case where the constraints do not depend on the distribution, we can relax the constraint qualification to obtain a similar result. We include this as a separate statement, as it permits a more direct and illustrative proof using dynamic programming.

SCHUURMANS et al.: A GENERAL FRAMEWORK FOR LEARNING-BASED DISTRIBUTIONALLY ROBUST MPC OF MARKOV JUMP SYSTEMS

9

Given an arbitrary state-mode pair (x, w), initial value of

the learning state s0 and confidence 0, the stochastic process defined by the optimal value of the DR-OCP (20), i.e.,

VprN(otx)i(mxa, twio)n:=ofVthNe(oxp,tsimt,al tv,awlu)e,

t  IN serves VN (x, w) of the

as a sequential aphorizon-N nominal

OCP (7). This section will establish sufficient conditions under which

VasN(ta) scyomnpvteortgices

to VN almost surely. consistency. To this

We end,

will we

refer to this property make the following

assumption on the learner state and the corresponding ambiguity set.

Assumption VI.8 (Ambiguity decrease). There exists a sequence {t}tIN with limt t = 0, such that

sup

p - q  t a.s.,

p,qAt,i (st,w)

w  W, i  IN[1,n],

Assumption VI.8 states that the ambiguity sets "shrink" to a singleton with probability one. Since the ambiguity is expected to decrease as more information is observed, this is a rather natural assumption, which is satisfied by most classes of ambiguity sets, such as those discussed in Section III above.

Example VI.9 (Divergence-based ambiguity sets). Consider again the

divergence-based ambiguity sets introduced in Section III. Propo-

sition III.2 provides an expression for the radius rt(w) of two commonly used ambiguity sets, which asymptotically behave as

rt(w)  -t-w1 log(t). Recall that tw denotes the number of visits to mode w at time t. In this case, the requirement of Assumption VI.8

results in a lower bound on the rate at which t may decrease with t, posing a trade-off with Assumption II.7, which requires summability

of the sequence (t)tIN. Given ergodicity of the Markov chain (Assumption II.1), it is straightforward to verify ­ using the Borel-

Cantelli lemma [41, Thm. 4.3] in conjunction with [61, Lem. 6] ­

that with probability 1, there exists a finite time T , such that for all

t > T and for all w  W , it holds that tw  ct, where c > 0 is a constant depending on specific properties of the Markov chain.

Hence,

so long as

(t)tIN

is chosen to satisfy limt

- log t t

=

0 element-wise, then Assumption VI.8 is satisfied. Note that the

choice in Example II.8 satisfies both this requirement and that of

Assumption II.7.

We are now ready to prove consistency of the DR-OCP in the absence of chance constraints. Below, we denote Xf (w) = {x | (x, w)  Xf } and similarly Xf (w) = {y | (y, w)  Xf }.

Theorem VI.10 (Asymptotic consistency with hard constraints). Suppose that all constraints are hard constraints, i.e.,  = 0, so
that U (z) = U (x, w) for all z = (x, s, , w). If, additionally, Xf is constructed in relation to the original problem such that for all
w  W , Xf (w) = Xf (w) × S × I, and Xf is RCI for system (1) in the sense of Definition VI.2, then for any state-mode pair (x, w)  dom VN , any initial learner state s0 = s  S and any initial confidence level 0 =   I, the optimal cost of the DR-OCP of horizon N  0 almost surely converges from above to the true
optimal cost. That is, with probability one,

VN(t)(x, w)  VN (x, w) for all sufficiently large t; (28)

lim
t

VN(t)(x,

w)

=

VN

(x,

w),

(29)

for all (x, w)  dom VN .

Proof. See Appendix.

The more general case, in which beside the cost, also the constraints are probabilistic and therefore dependent on the learner state, some additional assumptions on the problem ingredients are required.

Theorem VI.11 (Asymptotic consistency under chance constraints).

Let s  S denote a stationary learner state (cf. Assumption II.6) and suppose that for a given state-mode pair (x, w)  dom VN , the following hold:

(i) the costs ( · , · , w), Vf ( · , w), constraints g( · , · , w, v) and f~( ·, · , w, v) are continuously differentiable;

(ii) the ambiguity set A(s, w) is conic representable with convex cone K and parameters Ew(s, ), Fw(s, ) and bw(s, ) that

depend smoothly on s and ;

(iii) Xf (w) = Xf (w) × S × I, with Xf RCI in the sense of

Definition VI.2, and Xf (w) is closed and convex;

(iv) Risk levels t are chosen according to the upper bound of

Proposition IV.1, i.e., t =

-t 1-t

and

t <   1;

(v) Robinson's constraint qualification [62, Def. 2.86] holds for

(24), for initial augmented state y0 = (x, s , 0).

Then, limt VN(t)(x, w) = VN (x, w), a.s.

Proof. By Condition (iii), (x, s, , w)  Xf  (x, w)  Xf . Therefore, the nominal OCP (7) differs only from its DR counterpart

(20) in the parameters st, t that define the risk measures involved in the OCP and the choice of the risk levels t.
Assumptions II.7 and VI.8 ensure that limt t =  = 0 and consequently, by Condition (iv), limt t = . By Assump-

tion VI.8 and the requirement (8), the Borel-Cantelli lemma [41,

Tlihmmt.4.3]ptim=pliPeswt:hawtitfhorperovberaybislietqyu1en. cFeur(tphtermoAre,t

(st, w))tIN as st  s

, ,

it follows by Condition (ii) that the mapping (s, )  A(s, w) is continuous for all w  W and therefore A0(s , w) = {Pw:}. Then, for z = (x, s ,  , w), it holds that VN (z ) = VN (x, w).
Thus, it remains to show that VN is continuous with respect to the parameters s, , i.e.,

lim VN (x, s, , w) = VN (x, s ,  , w).
ss 
To do so, we will set out to show that we may write the scenario tree formulation of the DR-OCP (24), in the form

VN

(z)

=

min
z

(z)

subj. to

(z, )  K,

(30)

with  and  continuously differentiable functions and K a closed convex set, where z represents the decision variables over the scenario

tree and  = (s, ) denotes the parameters. The claim then follows

directly from [62, Thm. 2.84]. By inspection of (24a) it is clear that

 is a linear function, satisfying the requirements. We now proceed to demonstrate that furthermore, the constraints (24b)­(24g) admit

the desired representation.

I The constraints (24b)­(24d), and (24g) can be directly combined into the form 1(z, )  K1 := {0} × IRn+1 × Xf , where 1 is a concatenation of the functions ( · , · , w), Vf ( · , w), and f~( · , · , w, v) and therefore continuously differentiable, given that Condition (i)

holds. K1 is convex due to Condition (iii) II Finally, we consider the remaining constraints (24e) and (24f). Using (23), a conic risk epigraph constraint (, )  epi ~ with parameters E~(), F~() and ~b() and cone K~ can be written in the

desired form

~2(, y, )  K~2 := {0} × K~ × IRn+2

(31)

with y an auxiliary variable and

~2(, , y, ) := [ E~() F~() I -~b() ] y + [ 0 -1 ]  + [ -I 0 ] ,
which is differentiable provided that E~(), F~() and ~b() are differentiable. This is ensured exactly by Condition (ii), for the cost risk measure s,w, and thus (24e) is of the form (31).

10

PREPRINT(2021)

Invoking Proposition V.2, s,,w is conic representable with parameters

Ew(s, ) =

E 0

, F w(s, ) =

-B

0

Ew(s,) Fw(s,)

,

(32)

bw(s, ) =

b bw (s, )

, K = IR2+(d+1) × K,

with E = [ 1d -1d I -I ] , and B, b constant. Condition (iv) requires that  = - is continuously differentiable in  for all
1-
 < 1. The case  = 1 is excluded by design and furthermore inconsequential as   0. As a result, (24f), i.e., constraints
(g(x, u, w, v), 0)  epi s,,w can be written in the form (31), replacing  with g(x, u, w, v) ­ which preserves continuous differentiability, due to Condition (i) ­ and replacing the risk parameters E~(), F~() and ~b() and K~ with those in (32).
We conclude that (24g) admits the representation (30), and thus, under the constraint qualification of Condition (v), it satisfies the requirements of [62, Thm. 2.84] and the proof is complete.

Remark VI.12. The required differentiability of the learner dynamics in Condition (i) can in principle be relaxed be relaxed as these dynamics are exogenous and can be precomputed over the scenario tree. In this case, the parameter vector in the proof of Theorem VI.11 can be taken to be {s, }nod([0,N-1]), leaving the remainder of the argument mostly intact.

VII. ILLUSTRATIVE EXAMPLE

We consider a Markov jump linear system xt+1 = A(wt+1)xt + B(wt+1)ut, with

A(w) =

1+

w-1 d

0.01

0.01

1+2.5

w-1 d

, B(w) = I, w  IN[1,d]

(33)

The state xt  IR2 of this system, inspired by [63], models the deviation of temperatures from some nominal value of two adjacent

servers in a data center. The actuators ut  IR2 correspond to the amount of heating (ut  0) or cooling (ut < 0) applied to the

corresponding machines. The mode i models the load on the servers.

If i = 1, the system is idle and no heat is generated. If i = d, then

the processors are fully occupied and a maximum amount of heat is

added to the system. Note that the second server generates more heat

under increasing loads.

As in [63], we will use a mode-independent quadratic cost

(x, u, w) =

x

2 2

+

103

u

22.

We impose hard constraints -1.5  u  1.5 on the actuation and

(nominally) impose chance constraints

P[Hi:xt+1 > hi | xt, wt]   with H =

Inx 1nx

,h =

1nx 0.5

,

for all t  IN[0,N-1], and  = 0.2. Hence, in this example, we have gi(x, u, w, v) = Hi:(A(v)x + B(v)u) - hi.
We compute stabilizing terminal ingredients offline using standard

techniques from robust control. We compute a robust quadratic

Lyapunov function Vf (x) = x Qf x along with a local linear control gain K, such that Vf (A(w) + B(w)K)x  - (x, Kx), w  W by solving a linear matrix inequality (LMI) as in [64]. The RCI

terminal set Xf is computed as the level set Xf = lev Vf , where

= lies

mini{hi/ inside the

pQo-fly1h/2eHdria:l

22} is the largest set {x  IRnx

value such | H(A(w)

that lev + B(w)K)

Vf 

h, w  W }.

For the DR controllers below, we use the TV ambiguity set de-

scribed in Section III-A. We choose confidence levels t = (t, t) with t = t = 0.19t-2 <  for the cost and the constraints, respectively, ensuring that both Assumption II.7 and Assumption VI.8

are satisfied, as discussed in Example VI.9. For simplicity, we use

identical confidence levels t for all the constraints. We compare the proposed DR-MPC controller with (i) the (nom-

inal) stochastic MPC controller (see (7)), which we call omniscient

as it has access to the true transition matrix P ; and (ii) the robust

MPC controller, obtained by solving (24), taking the ambiguity set

A(s, w) regardless

= of

Athe

(s, w) mode

= or

d to learner

be the entire probability simplex, state. Both the LMIs involved in

the offline computation of the terminal ingredients as the online risk-

averse optimal control problem (24) are solved using MOSEK [65]

through the CVXPY [66] interface.

We fix the number of modes to d = 3, and take N = 5. For

these values, the average and maximum online solver times over 1500

monte-carlo runs were 56.8 ms and 87.5 ms, respectively, on an Intel

Core i7-7700K CPU at 4.20GHz.

A. Closed-loop simulation
Fixing the initial state at x = [ 0.5 0.5 ] , we perform 50 montecarlo simulations of the described MPC problems for 30 steps. As the simulation time is rather short, we initialize the DR controller with 10 and 100 offline observations of the Markov chain to obtain more interesting comparisons. Hence, the simulation below essentially compares the controller responses after a sudden disturbance after 10 and 100 time steps. All considered controllers are recursively feasible and mean-square stabilizing by construction. By the nature of the problem set-up, the optimal behavior is to just barely stabilize the system with minimal control effort. However, the larger the uncertainty on the state evolution, the controller is forced to drive the states further away from the constraint boundary, leading to larger control actions and consequently, larger costs.

ut 2 xt,2

Robust 1
0.5
0 0

DR (10 offline)

DR (100 offline)

Omniscient

0.4

0.2

0

-0.2

-0.4

5

10

0

Time step t

5

10

Time step t

Fig. 2. Control effort and second component of the state vector over 50 monte-carlo simulations. Full lines depict the means over the realizations and the shades areas are delineated by the 0.05 and 0.95 quantiles.

Closed-loop cost

·103 2

1.5

1363

1 0.5

791

612

421

Robust

DR
(10 offline)

DR
(100 offline)

Omniscient

Fig. 3. Box plot of the closed-loop cost over 50 monte-carlo simulations. The annotated lines show the mean. The whiskers depict the 0.05 and 0.95 quantiles.

This behavior can be observed in Fig. 2 and 3. Fig. 2 shows the controls and states over time and Fig. 3 presents the distribution of the closed-loop costs (sum of the stage costs over the simulation

SCHUURMANS et al.: A GENERAL FRAMEWORK FOR LEARNING-BASED DISTRIBUTIONALLY ROBUST MPC OF MARKOV JUMP SYSTEMS

11

time). In the first time step, the robust controller takes the largest step, driving the state the furthest from the constraint boundary. As illustrated in Fig. 2 (right), this is particularly pronounced for the second component of the state vector, as it is more sensitive to the mode (cf. (33)). The omniscient stochastic MPC, by contrast, has perfect knowledge of the transition probabilities, and by consequence is able to more slowly drive the state to the origin, reducing the control effort considerably. The DR controller naturally `interpolates' between these behaviors. Initially, it performs only marginally better than the robust controller (due to the very limited number of online learning steps). As it gets access to increasing sample sizes, however, it gradually approximates the behavior of the omniscient controller, while guaranteeing satisfaction of the constraints throughout.

B. Asymptotic consistency

To illustrate the consistency results from Section VI-D, we fix the

initial state-mode pair x0 = [ 0.25 0.25 ] , w0 = 1 and recompute

the solution to problem increasing sample sizes t.

(24) For

to obtain V (t) comparison, we

:c=omVNp(ut)te(x(0i),

w0) the

for true

value V := VN (x0, w0) by solving the stochastic MPC problem (7), using the true transition probabilities; and (ii) the robust value

function Vr, obtained by solving (24), taking the ambiguity set

A(s, w) = d to be the entire probability simplex, regardless of the mode or learner state.

Figure 4 shows the relative difference between the DR value V (t)

and the true value V . At very low sample sizes, the DR controller

achieves the same cost as the robust controller. However, as more

data is gathered and the ambiguity set is updated, V (t) approaches V from above, at a rate of O(1/t).

100

V (t)-V V

10-1

Robust DR

10-1 100 101 102 103 104 105 Sample size t

Fig. 4. Relative suboptimality versus sample size for the example system (33). The dashed line depicts the relative suboptimality of the robust controller: (Vr-V )/V .

VIII. CONCLUSION
We presented a distributionally robust MPC strategy for Markov jump systems with unknown transition probabilities subject to general chance constraints. Using data-driven ambiguity sets, we derived a DR counterpart to a nominal stochastic MPC scheme, and showed that the resulting controller provides a priori guarantees on closedloop constraint satisfaction and mean-square stability of the true system, without requiring explicit knowledge of the transition probabilities. Additionally, we have shown convergence of the cost to the nominal value. We illustrate the favorable properties of the obtained MPC scheme on a numerical example.
In future work, we aim to extend the methodology to the case where the discrete mode cannot be observed directly [67], and extend the numerical simulations to more extensive case studies. Furthermore, we plan to investigate tailored (parallelized) solution methods for the discussed optimal control problems, which are still hindered by an exponential growth in the prediction horizon.

REFERENCES
[1] M. Schuurmans and P. Patrinos, "Learning-Based Distributionally Robust Model Predictive Control of Markovian Switching Systems with Guaranteed Stability and Recursive Feasibility," arXiv:2009.04422, Sept. 2020.
[2] B. Kouvaritakis and M. Cannon, Model Predictive Control. Advanced Textbooks in Control and Signal Processing, Cham: Springer International Publishing, 2016.
[3] A. Mesbah, "Stochastic Model Predictive Control: An Overview and Perspectives for Future Research," IEEE Control Systems Magazine, vol. 36, pp. 30­44, Dec. 2016.
[4] J. B. Rawlings, D. Q. Mayne, and M. M. Diehl, Model Predictive Control: Theory, Computation, and Design. Madison, Wisconsin: Nob Hill Publishing, second ed., 2017.
[5] P. Mohajerin Esfahani and D. Kuhn, "Data-driven distributionally robust optimization using the Wasserstein metric: Performance guarantees and tractable reformulations," Mathematical Programming, vol. 171, pp. 115­166, Sept. 2018.
[6] J. Dupacova´, "The minimax approach to stochastic programming and an illustrative application," Stochastics, vol. 20, pp. 73­88, Jan. 1987.
[7] B. P. G. Van Parys, P. M. Esfahani, and D. Kuhn, "From Data to Decisions: Distributionally Robust Optimization Is Optimal," Management Science, Nov. 2020.
[8] R. Gao and A. J. Kleywegt, "Distributionally Robust Stochastic Optimization with Wasserstein Distance," arXiv:1604.02199 [math], Apr. 2016.
[9] W. Wiesemann, D. Kuhn, and M. Sim, "Distributionally Robust Convex Optimization," Operations Research, vol. 62, pp. 1358­1376, Dec. 2014.
[10] D. Bertsimas, V. Gupta, and N. Kallus, "Data-driven robust optimization," Mathematical Programming, vol. 167, pp. 235­292, Feb. 2018.
[11] M. Schuurmans, P. Sopasakis, and P. Patrinos, "Safe Learning-Based Control of Stochastic Jump Linear Systems: A Distributionally Robust Approach," in 58th IEEE Conference on Decision and Control (CDC), pp. 6498­6503, Dec. 2019.
[12] P. Coppens, M. Schuurmans, and P. Patrinos, "Data-driven distributionally robust LQR with multiplicative noise," in Learning for Dynamics and Control, pp. 521­530, PMLR, July 2020.
[13] I. Yang, "Wasserstein Distributionally Robust Stochastic Control: A Data-Driven Approach," arXiv:1812.09808, Dec. 2018.
[14] A. Hakobyan and I. Yang, "Wasserstein Distributionally Robust Motion Control for Collision Avoidance Using Conditional Value-at-Risk," arXiv:2001.04727 [cs, eess], Jan. 2020.
[15] J. Coulson, J. Lygeros, and F. Do¨rfler, "Regularized and Distributionally Robust Data-Enabled Predictive Control," in 2019 IEEE 58th Conference on Decision and Control (CDC), pp. 2696­2701, Dec. 2019.
[16] H. Rahimian and S. Mehrotra, "Distributionally Robust Optimization: A Review," arXiv:1908.05659, Aug. 2019.
[17] A. Aswani, H. Gonzalez, S. S. Sastry, and C. Tomlin, "Provably safe and robust learning-based model predictive control," Automatica, vol. 49, pp. 1216­1226, May 2013.
[18] L. Hewing and M. N. Zeilinger, "Scenario-Based Probabilistic Reachable Sets for Recursively Feasible Stochastic Model Predictive Control," IEEE Control Systems Letters, vol. 4, pp. 450­455, Apr. 2020.
[19] J. F. Fisac, A. K. Akametalu, M. N. Zeilinger, S. Kaynama, J. Gillula, and C. J. Tomlin, "A General Safety Framework for Learning-Based Control in Uncertain Robotic Systems," IEEE Transactions on Automatic Control, vol. 64, pp. 2737­2752, July 2019.
[20] J. Coulson, J. Lygeros, and F. Do¨rfler, "Data-Enabled Predictive Control: In the Shallows of the DeePC," arXiv:1811.05890 [math], Mar. 2019.
[21] L. Hewing, K. P. Wabersich, M. Menner, and M. N. Zeilinger, "LearningBased Model Predictive Control: Toward Safe Learning in Control," Annual Review of Control, Robotics, and Autonomous Systems, vol. 3, no. 1, 2020.
[22] D. Bernardini and A. Bemporad, "Stabilizing Model Predictive Control of Stochastic Constrained Linear Systems," IEEE Transactions on Automatic Control, vol. 57, pp. 1468­1480, June 2012.
[23] D. Bernardini and A. Bemporad, "Scenario-based model predictive control of stochastic constrained linear systems," in 48th IEEE Conference
on Decision and Control (CDC) Held Jointly with 2009 28th Chinese Control Conference, pp. 6333­6338, IEEE, Dec. 2009. [24] S. Lucia, T. Finkler, and S. Engell, "Multi-stage nonlinear model predictive control applied to a semi-batch polymerization reactor under uncertainty," Journal of Process Control, vol. 23, pp. 1306­1319, Oct. 2013.

12

PREPRINT(2021)

[25] C. Leidereiter, A. Potschka, and H. G. Bock, "Quadrature-based scenario tree generation for Nonlinear Model Predictive Control," IFAC Proceedings Volumes, vol. 47, no. 3, pp. 11087­11092, 2014.
[26] A. D. Bonzanini, J. A. Paulson, and A. Mesbah, "Safe learning-based model predictive control under state-and input-dependent uncertainty using scenario trees," in Proceedings of the IEEE Conference on Decision and Control. Jeju Island, Republic of Korea. Submitted, 2020.
[27] O. L. d. V. Costa, M. D. Fragoso, and R. P. Marques, Discrete-time Markov jump linear systems. Probability and its applications, London: Springer, 2005.
[28] P. Patrinos, P. Sopasakis, H. Sarimveis, and A. Bemporad, "Stochastic model predictive control for constrained discrete-time Markovian switching systems," Automatica, vol. 50, pp. 2504­2514, Oct. 2014.
[29] S. Lucia, S. Subramanian, D. Limon, and S. Engell, "Stability properties of multi-stage nonlinear model predictive control," Systems & Control Letters, vol. 143, p. 104743, Sept. 2020.
[30] P. Sopasakis, D. Herceg, A. Bemporad, and P. Patrinos, "Risk-averse model predictive control," Automatica, vol. 100, pp. 281­288, Feb. 2019.
[31] S. Singh, Y.-L. Chow, A. Majumdar, and M. Pavone, "A Framework for Time-Consistent, Risk-Sensitive Model Predictive Control: Theory and Algorithms," arXiv:1703.01029, Apr. 2018.
[32] R. L. Beirigo, M. G. Todorov, and A. M. S. Barreto, "Online TD() for discrete-time Markov jump linear systems," in 57th IEEE Conference on Decision and Control (CDC), pp. 2229­2234, Dec. 2018.
[33] S. He, M. Zhang, H. Fang, F. Liu, X. Luan, and Z. Ding, "Reinforcement learning and adaptive optimization of a class of Markov jump systems with completely unknown dynamic information," Neural Computing and Applications, Apr. 2019.
[34] A. Shapiro, D. Dentcheva, and A. Ruszczyn´ski, Lectures on stochastic programming: modeling and theory. SIAM, 2009.
[35] A. Cherukuri and A. R. Hota, "Consistency of Distributionally Robust Risk- and Chance-Constrained Optimization under Wasserstein Ambiguity Sets," arXiv:2012.08850 [cs, eess, math], Dec. 2020.
[36] S. Guo, H. Xu, and L. Zhang, "Convergence Analysis for Mathematical Programs with Distributionally Robust Chance Constraint," SIAM Journal on Optimization, vol. 27, pp. 784­816, Jan. 2017.
[37] A. Nemirovski, "On safe tractable approximations of chance constraints," European Journal of Operational Research, vol. 219, no. 3, pp. 707­718, 2012.
[38] P. Sopasakis, M. Schuurmans, and P. Patrinos, "Risk-averse riskconstrained optimal control," in 18th European Control Conference (ECC), pp. 375­380, June 2019.
[39] D. P. Bertsekas, Dynamic Programming and Optimal Control. Vol. 1. Athena Scientific Optimization and Computation Series, Belmont, Mass: Athena Scientific, third ed., 2005.
[40] V. Krishnamurthy, Partially Observed Markov Decision Processes: From Filtering to Controlled Sensing. Cambridge: Cambridge University Press, 2016.
[41] P. Billingsley, Probability and Measure. Wiley Series in Probability and Mathematical Statistics, New York: Wiley, third ed., 1995.
[42] E. Delage and Y. Ye, "Distributionally Robust Optimization Under Moment Uncertainty with Application to Data-Driven Problems," Operations Research, vol. 58, pp. 595­612, June 2010.
[43] Z. Wang, P. W. Glynn, and Y. Ye, "Likelihood robust optimization for data-driven problems," Computational Management Science, vol. 13, pp. 241­261, Apr. 2016.
[44] A. Ahmadi-Javid, "Entropic Value-at-Risk: A New Coherent Risk Measure," Journal of Optimization Theory and Applications, vol. 155, pp. 1105­1123, Dec. 2012.
[45] R. Jiang and Y. Guan, "Risk-Averse Two-Stage Stochastic Program with Distributional Ambiguity," Operations Research, vol. 66, pp. 1390­1405, Oct. 2018.
[46] H. Sun and H. Xu, "Convergence Analysis for Distributionally Robust Optimization and Equilibrium Problems," Mathematics of Operations Research, vol. 41, pp. 377­401, May 2016.
[47] A. Ben-Tal, D. den Hertog, A. De Waegenaere, B. Melenberg, and G. Rennen, "Robust Solutions of Optimization Problems Affected by Uncertain Probabilities," Management Science, vol. 59, pp. 341­357, Nov. 2012.
[48] G. Bayraksan and D. K. Love, "Data-Driven Stochastic Programming Using Phi-Divergences," in The Operations Research Revolution (D. Aleman, A. Thiele, J. C. Smith, and H. J. Greenberg, eds.), pp. 1­19, INFORMS, Sept. 2015.
[49] M. Wainwright, High-Dimensional Statistics: A Non-Asymptotic Viewpoint. No. 48 in Cambridge Series in Statistical and Probabilistic Mathematics, Cambridge ; New York, NY: Cambridge University Press, 2019.

[50] S. Boucheron, G. Lugosi, and P. Massart, Concentration Inequalities: A Nonasymptotic Theory of Independence. Oxford: Oxford University Press, 1st ed ed., 2013.
[51] A. W. van der Vaart and J. A. Wellner, Weak Convergence and Empirical Processes: With Applications to Statistics. New York: Springer, 2000.
[52] I. Csiszar, "The method of types," IEEE Transactions on Information Theory, vol. 44, pp. 2505­2523, Oct. 1998.
[53] T. M. Cover and J. A. Thomas, Elements of Information Theory. Hoboken, N.J: Wiley-Interscience, 2nd ed., 2006.
[54] J. Mardia, J. Jiao, E. Ta´nczos, R. D. Nowak, and T. Weissman, "Concentration inequalities for the empirical distribution of discrete distributions: Beyond the method of types," Information and Inference: A Journal of the IMA, Nov. 2019.
[55] I. Csisza´r and J. Ko¨rner, Information Theory: Coding Theorems for Discrete Memoryless Systems. Cambridge ; New York: Cambridge University Press, 2nd ed ed., 2011.
[56] A. Ruszczyn´ski, "Risk-averse dynamic programming for Markov decision processes," Mathematical Programming, vol. 125, pp. 235­261, Oct. 2010.
[57] A. Shapiro, "On Duality Theory of Conic Linear Problems," in SemiInfinite Programming (P. Pardalos, M. A´ . Goberna, and M. A. Lo´pez, eds.), vol. 57, pp. 135­165, Boston, MA: Springer US, 2001.
[58] G. C. Pflug and A. Pichler, Multistage Stochastic Optimization. Springer Series in Operations Research and Financial Engineering, Cham: Springer International Publishing, 2014.
[59] M. Schuurmans, A. Katriniok, H. E. Tseng, and P. Patrinos, "LearningBased Risk-Averse Model Predictive Control for Adaptive Cruise Control with Stochastic Driver Models," in IFAC 2020 World Congress, (Berlin), pp. 15337­15342, 2020.
[60] M. Korda, R. Gondhalekar, J. Cigler, and F. Oldewurtel, "Strongly feasible stochastic model predictive control," in 50th IEEE Conference on Decision and Control and European Control Conference, pp. 1245­ 1251, Dec. 2011.
[61] G. Wolfer and A. Kontorovich, "Minimax Learning of Ergodic Markov Chains," in Algorithmic Learning Theory, pp. 903­929, Mar. 2019.
[62] J. F. Bonnans and A. Shapiro, Perturbation Analysis of Optimization Problems. Springer Series in Operations Research, New York: Springer, 2000.
[63] B. Recht, "A Tour of Reinforcement Learning: The View from Continuous Control," Annual Review of Control, Robotics, and Autonomous Systems, vol. 2, no. 1, pp. 253­279, 2019.
[64] M. V. Kothare, V. Balakrishnan, and M. Morari, "Robust constrained model predictive control using linear matrix inequalities," Automatica, vol. 32, no. 10, pp. 1361­1379, 1996.
[65] MOSEK ApS, The MOSEK optimization toolbox for MATLAB manual. Version 8.1., 2017.
[66] S. Diamond and S. Boyd, "CVXPY: A Python-embedded modeling language for convex optimization," Journal of Machine Learning Research, vol. 17, no. 83, pp. 1­5, 2016.
[67] M. Schuurmans and P. Patrinos, "Data-driven distributionally robust control of partially observable jump linear systems," arXiv:2105.02511 [cs, eess, math], May 2021.
[68] R. T. Rockafellar and R. J. B. Wets, Variational Analysis, vol. 317 of Grundlehren Der Mathematischen Wissenschaften. Berlin, Heidelberg: Springer Berlin Heidelberg, 1998.
Mathijs Schuurmans obtained a Bachelor's degree (BSc) in Electrical and Mechanical Engineering and a Master's (MSc) in Mathematical Engineering from KU Leuven, Leuven, Belgium in 2016 and 2018, respectively. He is currently a PhD candidate at the Department of Electrical Engineering (ESAT) of KU Leuven. His research is focused on data-driven model predictive control of stochastic systems, focusing on distributionally robust approaches for safety-critical applications in autonomous driving.

SCHUURMANS et al.: A GENERAL FRAMEWORK FOR LEARNING-BASED DISTRIBUTIONALLY ROBUST MPC OF MARKOV JUMP SYSTEMS

13

Panagiotis Patrinos Panagiotis (Panos) Patrinos is associate professor at the Department of Electrical Engineering (ESAT) of KU Leuven, Belgium. In 2014 he was a visiting professor at Stanford University. He received his PhD in Control and Optimization, M.S. in Applied Mathematics and M.Eng. in Chemical Engineering from the National Technical University of Athens in 2010, 2005 and 2003, respectively. After his PhD he held postdoc positions at the University of Trento and IMT Lucca, Italy, where he became an assistant professor in 2012. His current research interests lie in the intersection of optimization, control and learning. In particular he is interested in the theory and algorithms for structured nonconvex optimization as well as learning-based, model predictive control with a wide range of applications including autonomous vehicles, machine learning and signal processing. He is the co-recipient of the 2020 best paper award in International Journal of Circuit Theory & Applications

APPENDIX

A. Technical Lemma
Lemma A.1 (Infimum convergence). Consider a sequence of proper, lsc functions V (t) : IRn  IR, t  IN and a proper, lsc, levelbounded function V : IRn  IR. Suppose that

(i) (Eventual upper bound) there exists a T  IN, such that for all t > T , and for all u, V (t)(u)  V (u);
(ii) (Pointwise convergence) V (t) p V . That is, for all u, limt V (t)(u) = V (u).
Then, limt inf u V (t)(u) = inf u V (u).

Proof. By (i) it follows that for any sequence ut  u,

lim inf V (t)(ut) = lim inf V (t)(u)  lim inf V (u)  V (u),

t

uu

uu

t

where the first inequality follows from Condition (i), and the second

inequality follows from lower semicontinuity of V . Moreover, fixing

(ut)tIN to be the constant sequence ut = u, it follows from (ii)

that lim supt V (t)(ut) Prop. 7.2], we conclude

= limt V (t)(u) that V (t) e V ,

 V (u). i.e., V (t)

Invoking [68, epi-converges

to V . Secondly, from Condition (i) and the level-boundedness of V ,

it follows that (V (t))tIN is eventually level-bounded [68, Ex. 7.32]. The claim then follows from [68, Thm. 7.33].

B. Deferred proofs
Proof of Lemma VI.6. Let (zt)tIN = (xt, st, t, wt)tIN denote the stochastic process
satisfying dynamics (26), for some initial state z0  dom V . For ease of notation, let us define Vt := V (zt), t  IN. Due to nonnegativity of V ,

IE kt=-01c xt 2  IE Vk + kt=-01c xt 2

= IE Vk - V0 +

k-1 t=0

c

xt

2

+ V0,

where the second equality follows from the fact that V0 is deterministic. By linearity of the expectation, we can in turn write

IE Vk-V0+c Therefore,

k-1 t=0

xt

2

= IE

k-1 t=0

Vt+1-Vt+c

xt

2

=

k-1 t=0

IE

Vt+1-Vt+c xt

2

.

IE c

k-1 t=0

xt

2

-V0 

k-1 t=0

IE

[Vt+1-Vt]

+c

IE

xt 2 .

(34)

Recall that t denotes the coordinate of t corresponding to the risk measures in the cost function (19). Defining the event Et := {   |

wPwe tc(an):

useAthet

(st(), law of

wt())}, and its total expectation

complement to write

¬Et

=

 \ Et,

IE [Vt+1 - Vt] = IE [Vt+1 - Vt | Et] P[Et]
+ IE [Vt+1 - Vt | ¬Et] P[¬Et].
By condition (8), P[¬Et] < t. From Conditions (i) and (iii), it follows that zt  dom V , t  IN[0,k] and that there exists a V  0 such that V (z)  V , for all z  dom V . Therefore, IE[Vt+1 - Vt | ¬ IEt]  V . Finally, by Condition (ii), IE [Vt+1 - Vt | Et]  IE[-c xt 2 | Et]. Thus,

IE [Vt+1 - Vt]  IE -c xt 2 | Et P[Et] + V t. This allows us to simplify expression (34) as

IE c

k-1 t=0

xt

2

- V0



k-1 t=0

-

c

IE

xt 2 | Et P[Et] + V t + c IE

xt 2



k-1 t=0

-

c

IE

xt 2 | Et P[Et] + V t

+ c IE xt 2 | Et P[Et] + c IE xt 2 | ¬Et P[¬Et]

=

k-1 t=0

V

t

+

c

IE

xt 2 | ¬Et P[¬Et]



k-1 t=0

t

(V

+ c IE

xt 2 | ¬Et ).

Since dom V was assumed to be compact and to contain the origin, there exists an r  0 such that x 2  r. Therefore,

IE

k-1 t=0

xt

2



V0 c

+

V c

+r

kt=-01t,

which remains finite as k  , since (t)tIN is summable. Thus, necessarily limt IE[ xt 2] = 0.

Proof of Theorem VI.7.
First, note that using the monotonicity of coherent risk measures [34, Sec. 6.3, (R2)], a straightforward inductive argument allows us to show that under Condition (i),

T VN  VN , N  IN.

(35)

Since Z  dom VN , recall that by definition (25), we have for any z = (x, s, , w)  Z that

VN (z) = (x, N (z), w) + w,s VN-1 f~N (z, v), v ,
where  denotes the component of  corresponding to the cost. Therefore, we may write

w,s VN (f~N (z, v), v) - VN (z)
= w,s VN (f~N (z, v), v) - (x, N (z), w)
- w,s VN-1 f~N (z, v), v  - (x, N (z), w)  -c x 2,
where the first inequality follows by (35) and monotonicity of coherent risk measures. The second inequality follows from Condition (ii). Combined with Condition (iii), this implies that V : z  VN (z) + foZll(ozw)s.satisfies the conditions of Lemma VI.6 and the assertion

Proof of Theorem VI.10.
By construction, the equivalence (x, w)  Xf  (x, s, , w)  Xf holds for all s,   S × I. Hence, for N = 0, we have that V0(t) = V0 = Vf and there is nothing to prove. The general case, N > 0, is proved by induction. Assume that equations (28) and (29) hold for some N  0. We will now demonstrate that this implies

14

that and

they QN

also as

hold

for

N

+

1.

Let

us

define

auxiliary

functions

Q(Nt)

Q(Nt)(x, u, w) := (x, u, w) + stt,w[VN(t-+11)(f (x, u, v), v)], QN (x, u, w) := (x, u, w) + IEPw: [VN-1(f (x, u, v), v)|x, w],

so that VN (x,

we may write VN(t)(x, w) = w) = inf uU(x,w) QN (x, u,

inf uU w).

(x,w)

Q(Nt)(x, u, w)

and

We will start with the inductive argument for (28). Under As-

sumption II.7, the Borel-Cantelli lemma [41, Thm. 4.3] guarantees

that with probability 1, there exists a finite TN  IN, such that for all t > TN , Pw:  At,i (st, w), for all w  W and i  IN[1,n], and consequently stt,w  IEPw: , uniformly. It follows that for all t > TN and for all u  U (x, w),

Q(Nt)+1(x, u, w)
 (x, u, w) + IEPw: [VN(t+1)(f (x, u, v), v) | x, w]
(a)
 (x, u, w) + IEPw: [VN (f (x, u, v), v) | x, w]
= QN+1(x, u, w),

and thus from the

ninedcuescstiaornilyhyVpN(ot+t)h1e(sxis,.wT)his

VN+1(x, w), where (a) follows establishes (28) for all N  IN.

To demonstrate the induction step N  N + 1 for (29),

we show that under the induction hypothesis, the sequence

(cQonN(dt)i+ti1o(nxs,

· , w))tIN of Lemma

and the function QN+1(x, · , A.1. Under Assumption II.3,

w), and

satisfy the using [68,

Thm. 3.31], proper, lsc,

aint dfollelovwels-bforuonmde[d38i,nPuroplo.c2a]llythautniQfoNrmalyndinQx(Nt,)+f1o,r

are all

w  W . Let us introduce the shorthand for the worst-case conditional

distribution pt (u) = (pt,v(u))vW :

pt (u) := argmax

pvVN(t+1)(f (x, u, v), v),

pAt (w,st) vW

where we have omitted the dependence on the constant x and w.

Then, by the induction hypothesis (29), there exists for every > 0,

a T  TN , such that for all t > T ,

Q(Nt)+1(x, u, w) - QN+1(x, u, w)

=

pt,v(u)VN(t+1)(f (x, u, v), v) - PwvVN (f (x, u, v), v)

vW



pt,v(u)(VN (f (x, u, v), v) + ) - PwvVN (f (x, u, v), v)

vW

= (pt,v(u) - Pwv)VN (f (x, u, v), v) + pt,v(u)

vW



tVN (f (x, u, v), v) + ,

(36)

vW

where the final inequality is due to Assumption VI.8 and the fact

that for all in (36) can

t > TN , be made

ParwbivtrarilAy stm(wal,l

st). by

As t  increasing

0, t,

the first term provided that

VN (f (x, u, v), v) < , for all w  W , hence establishing pointwise

wcohnivcehrginentcuernQh(Notl)+ds1

p QN if Xf is

+1 whenever dom RCI by Proposition

VN is RCI for (1), VI.4. The sequence

(thQeN(ct)o+n1d(ixti,o·n,swo)f)tLeINmmanadAt.h1e,

function QN+1(x, · , which establishes (29)

w) for

thus N+

satisfy 1.

PREPRINT(2021)

