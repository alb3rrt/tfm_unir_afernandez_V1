arXiv:2106.00280v1 [cs.LG] 1 Jun 2021

AAPM DL-Sparse-View CT Challenge Submission Report:
Designing an Iterative Network for Fanbeam-CT with Unknown Geometry
Martin Genzel* Jan Macdonald Maximilian Ma¨rz
Abstract. This report is dedicated to a short motivation and description of our contribution to the AAPM DL-Sparse-View CT Challenge (team name: robust-and-stable). The task is to recover breast model phantom images from limited view fanbeam measurements using data-driven reconstruction techniques. The challenge is distinctive in the sense that participants are provided with a collection of ground truth images and their noiseless, subsampled sinograms (as well as the associated limited view filtered backprojection images), but not with the actual forward model. Therefore, our approach first estimates the fanbeam geometry in a data-driven geometric calibration step. In a subsequent two-step procedure, we design an iterative end-to-end network that enables the computation of near-exact solutions.
Note: This is a technical report of a method participating in a not yet finished challenge. Therefore, it does not contain any final results. In particular, the reported reconstruction errors are only with respect to our own validation split of the provided training data. Once the official challenge report is released, these values will be updated with the results from the actual test set.
1 Introduction
In recent years, deep learning methods have been successfully applied to problems of the natural sciences. A prominent example of such scientific machine learning is the development of efficient solutions strategies for inverse problems, such as those encountered in the context of medical imaging. Despite unprecedented empirical performance in various practical scenarios, a sound theoretical understanding of data-driven reconstruction methods seems to be out of reach to date.
For this reason, more and more critical voices are heard, questioning the reliability of deeplearning-based solution strategies. For instance, Sidky et al. [SLBP21] have recently demonstrated that post-processing by the prominent UNet-architecture may not yield satisfactory recovery precision in a sparse-view computed tomography (CT) scenario. These findings have led to the AAPM Challenge, to which the present report is devoted. The goal of the challenge is "to identify the state-of-the-art in solving1 the CT inverse problem with data-driven techniques" [Sid+21].
A different and much-noticed example of such a critical perspective is the work [ARPAH20], which claims that "deep learning typically yields unstable methods for image reconstruction". Addressing this concern, we have recently examined a representative selection of end-to-end networks in the context of inverse problems [GMM20]. Surprisingly and in contrast to [ARPAH20], our study has demonstrated that deep-learning-based recovery schemes are very stable to measurement perturbations. It goes without saying that the ability to accurately solve an inverse problem also plays a central role in that regard. To cover this aspect, [GMM20] has considered scenarios that are similar to the sparse-view setup of the AAPM challenge in the following sense: Exact signal recovery by classical total variation (TV) minimization is possible for noisefree measurements. In such situations, we were also able to train neural networks (NNs) that provide visually perfect reconstructions. We took this as a motivation to participate in the AAPM challenge with the goal of designing a data-driven recovery workflow for (near-)exact image recovery.
Our Strategy in a Nutshell. Our approach is rooted in the following (debatable) observation:
High reconstruction accuracy is only possible when the forward model is explicitly incorporated into the reconstruction mapping, e.g., by an iterative promotion of data-consistent solutions.
*Mathematical Institute, Utrecht University, Netherlands Institute of Mathematics, Technische Universita¨t Berlin, Germany 1This expression is used to describe methods that provide perfect recovery in the idealistic situation of noiseless measurements. As we understand it, the greater goal of the AAPM challenge is to evaluate whether it is also possible to achieve such a (near-)exact precision by deep-learning-based schemes.
1

2 METHODOLOGY

2

Highlighting the importance of incorporating the forward operator is by no means novel. It is one of the central pillars of scientific machine learning, where NNs are frequently enriched (or constrained) by physical modeling. Indeed, the seminal works on deep-learning-based solution strategies for inverse problems are inspired by unrolling classical algorithms [GL10; YSLX16]. Furthermore, current state-of-the-art methods seem to be exclusively based on iterative end-to-end networks, e.g., see [Kno+20; Muc+20; Leu+21].
Our approach to the AAPM challenge and its contributions in a broader sense can be summarized as follows:
(i) Given that the exact forward model is unknown, we pursue a data-driven estimation of the underlying fanbeam geometry. This is achieved by fitting a generic, parameterized fanbeam operator to the provided sinogram-image pairs in a deep-learning-like fashion (i.e., by gradient descent with backpropagation/automatic differentiation). We hope that this approach can be of further use in the context of geometric calibration and forward operator correction. In particular, we currently explore an unsupervised identification strategy based on sinogram consistency conditions.
(ii) We propose a conceptually simple, yet powerful deep-learning workflow, which turns a post-processing UNet [RFB15] into an iterative reconstruction scheme. From a technical perspective, most of its design components have been previously reported in the literature. However, the overall strategy appears to be novel and differs from more common unrolled networks in several aspects, including: (a) we make use of a pre-trained UNet as the computational backbone; (b) data-consistency is inspired by an 2-gradient step, but utilizes the filtered backprojection instead of the regular adjoint. We think that the proposed strategy will be of use for other inverse problems as well, given that it outperforms other state-of-the-art data-driven approaches, such as the learned primal dual algorithm [AO¨ 18], by an order of magnitude with respect to the root-mean-square-error (RMSE).

2 Methodology
In this section, we give a short overview of our approach, together with a motivation of some design choices. A public code repository can be found under [GMM21].

Step 1 ­ Data-Driven Geometry Identification. The first step of our reconstruction pipeline learns the unknown forward operator from the provided training data. The continuous version of tomographic fanbeam measurements is based on computing line integrals:

p(s, ) =

x0(x, y) d(x, y),

L(s,)

where x0 is the unknown image and L(s, ) denotes a line in fanbeam coordinates, i.e.,  is the fan rotation angle and s encodes the sensor position; see [Fes17] for more details. In an idealized2 situation, the fanbeam model is specified by the following geometric parameters (see Fig. 1):

· dsource ­ the distance of the X-ray source to the origin,

· ddetector ­ the distance of the detector array to the origin, · ndetector ­ the number of detector elements, · sdetector ­ the spacing of the detector elements along the array, · nangle ­ the number of fan rotation angles, ·   [0, 2]nangle ­ a discrete list of rotation angles.

2We have found that this basic model was enough to accurately describe the AAPM challenge setup. If needed, it would be possible to account for other factors such as non-flat detector arrays, offsets of the axis of rotation from the origin, misalignments of the detector array, etc.

GENZEL, MACDONALD, MA¨ RZ: DL-SPARSE-VIEW CT CHALLENGE

3

nangle X-ray source

2



3

dsource

rotate

1

2

sdetector

ddetector

ndetector

detector array

Figure 1: Fanbeam geometry. Illustration of the parameters determining the geometry of the fanbeam CT model.

Here, it is assumed that integrals are only measured along a finite number of lines, determined by m := ndetector · nangle. In the AAPM DL-Sparse-View Challenge, the resulting forward operator is severely ill-posed, since only the measurements of a few fan rotation angles nangle are acquired. Furthermore, the geometric setup is not disclosed to the challenge participants -- it is only known
that fanbeam measurements are taken.
We have addressed this lack of information by a data-driven estimation strategy that fits the
above set of parameters to the given training data. To this end, we first observe that the previous parametrization is redundant, and without of loss of generality, we may assume that sdetector = 1 (by rescaling ddetector appropriately). Further, if the field-of-view angle  is known, then the relation

ddetector

=

ndetector · sdetector 2 tan 

- dsource

(2.1)

can be used to eliminate another parameter. Thus, the fanbeam geometry is effectively determined by the reduced parameter set (dsource, ndetector, nangle, ). The training data provides pairs of discrete images x0  R512·512=:N and its simulated fanbeam measurements y0  R128·1024=m, from which the dimensions nangle = 128 and ndetector = 1024 can be derived. We determine the field of view as  = arcsin(256/dsource), so that the maximum inscribed circle in the discrete image is exactly contained within each fan of lines, which is a common choice for fanbeam CT. Hence, (2.1) leads to
ddetector = 2 · sdetector · d2source - 2562 - dsource .

The main difficulty of Step 1 lies in the estimation of the remaining parameters (dsource, ). To

that end, we have implemented a discrete fanbeam transform from scratch in PyTorch (together

with its corresponding filtered backprojection). A distinctive aspect of our implementation is the

use of a vectorized numerical integration that enables the efficient computation of derivatives

with respect to the geometric parameters by means of automatic differentiation. This feature can

be exploited for a data-driven parameter identification, for instance, by a gradient descent. More

precisely, we use a ray-driven numerical integration for the forward model and a pixel-driven and

sinogram-reweighting-based filtered backprojection (with a Hamming filter) [Fes17, Sec. 3.9.2].

In addition to the parameters (dsource, ), we also introduce learnable scaling factors sfwd and sfbp for the forward and inverse transform, respectively. They account for ambiguities in chosing the

discretization units of distance compared to the actual physical units of distance.

As previously indicated, we estimate the free parameters fan = (sfwd, dsource, )  R130 of

the implemented forward operator F[fan]  Rm×N in a deep-learning-like fashion: The ability

to

compute

derivatives

dF dfan

allows

us

to

make

use

of

the

M

=

4000

sinogram-image

pairs

2 METHODOLOGY

4

{(y0i , x0i )}iM=1 by solving

M

min

1 M

fan

i=1

F[fan](x0i ) - y0i

2 2

(2.2)

with a variant of gradient descent (see Remark 2.1 for details). Finally, we determine sfbp by solving

M

min
sfbp

1 M
i=1

x0i - FBP[fan, sfbp](y0i )

2 2

,

while keeping the already identified parameters fixed. From now on, we will use the short-hand notation F and FBP for the estimated operators F[fan] and FBP[fan, sfbp], respectively.

Remark 2.1 (1) Clearly, the formulation (2.2) is non-convex and therefore it is not clear whether gradient descent enables an accurate estimation of the underlying fanbeam geometry. Indeed, standard gradient descent was found to be very sensitive to the initialization of fan and got stuck in bad local minima. To overcome this issue, we solve (2.2) by a coordinate descent instead, which alternatingly optimizes over sfwd, dsource, and  with individual learning rates. This strategy was found to effectively account for large deviations of gradient magnitudes of the different parameters. Indeed, we observed a fast convergence and a reliable identification of fan, independently of the initialization.

(2) In principle, the strategy of (2.2) requires only few training samples to be successful. However, when verifying the robustness of the outlined strategy against measurement noise, we observed that it is beneficial to employ more training data.

(3) Subsequent to the estimation of an accurate fanbeam geometry, we still noted a systematic error in our forward model. We suspect that it is caused by subtle differences in the numerical integration in comparison to the true forward model of the AAPM challenge. In compensation, we compute the (pixelwise) mean error over the training set, as an additive correction of the model bias.


Step 2 ­ Pre-Training a UNet as Computational Backbone. The centerpiece of our reconstruction scheme is formed by a standard UNet-architecture U[] : RN  RN [RFB15]. It is first
employed as a residual network to post-process sparse-view filtered backprojection images, i.e.,
we consider the reconstruction mapping

UNet[] : Rm  RN, y  [U[]  FBP] (y).

The learnable parameters  are trained from the collection of M = 4000 sinogram-image pairs {(y0i , x0i )}iM=1 that are provided by the challenge. This is achieved by standard empirical risk minimization, i.e., by (approximately) solving

M

min

1 M



i=1

x0i - UNet[](y0i )

2 2

+

µ

·



2 2

,

(2.3)

where we choose µ = 10-3. This minimization problem is tackled by 400 epochs of mini-batch stochastic gradient descent and the Adam optimizer [KB14] with initial learning rate 0.0002 and batch size 4.

Remark 2.2 The post-processing strategy of Step 2 was pioneered in [Che+17b; KMY17] and popularized by [Che+17a; JMFU17], among many others. Due to the multi-scale encoder-decoder structure with skip-connections, the UNet-architecture is very efficient in handling image-to-image problems. Therefore, solving (2.3) typically works out-of-the-box without requiring sophisticated initialization or optimization strategies (even in seemingly hopeless situations [HA20]). Making use of a more powerful or a more memory-efficient network would be beneficial, e.g., see results for the Tiramisu network below. However, we preferred to keep our workflow as simple as possible and therefore decided to stick to the standard UNet as the main computational building block. 

GENZEL, MACDONALD, MA¨ RZ: DL-SPARSE-VIEW CT CHALLENGE

5

Step 3 ­ Constructing an Iterative Scheme. In this step, we discuss our main reconstruction method. It incorporates the (approximate) forward model F from Step 1 (and the associated inversion by the FBP) via the following iterative procedure:

ItNet[] : Rm  RN, y 

4 k=1

DCk,y  U[~]

 FBP

(y),

(2.4)

for the learnable parameters  = [~, 1, 2, 3, 4] and the k-th data-consistency layer

DCk,y : RN  RN, x  x - k · FBP(Fx - y).
ItNet is trained by empirical risk minimization analogously to (2.3) with µ = 10-4. We run 500 epochs of mini-batch stochastic gradient descent and Adam with an initial learning rate of 8 · 10-5 and a batch size of 2 (restarting Adam after 250 epochs). The UNet-parameters ~ are initialized by the weights obtained in Step 2.
In the following, we will briefly discuss central aspects of the architecture in (2.4) and motivate some of the important design choices:

(i) The computational centerpiece of ItNet is formed by the UNet-architecture. This stands in contrast to earlier generations of unrolled iterative schemes, which rely on basic convolutional blocks instead, e.g., see [YSLX16; AO¨ 18]. We have found that it is advantageous to exploit the efficacy of UNet-like image-to-image networks as central image-enhancement blocks. This is in line with recent state-of-the-art architectures, which also make use of various advanced sub-networks, e.g., see [Ham+19; Kno+20; Muc+20; RCS20; Sri+20]. Somewhat surprisingly, it turned out to be beneficial that the same UNet is used in all four iterations (weight sharing), cf. [AMJ18; Ham+19].
(ii) We have observed that it is crucial to initialize the UNet-parameters ~ by the post-processing weights from Step 2. This does not only increase the speed of convergence, but it also significantly improves the final accuracy (see Fig. 2). In other words, our results show that the initialization of the UNet-block as a post-processing unit makes it possible to find better local minima. To the best of our knowledge, such an effect has not been reported in the literature yet. We emphasize that this initialization strategy is enabled by making use of a powerful enough post-processing sub-network.

(iii)

Our data-consistency layer is inspired by a gradient step on the loss x



k 2

Fx - y 22,

which would result in the update x  x - k · FT(Fx - y). We depart from this scheme by

replacing the unfiltered backprojection FT by its filtered counterpart FBP. This modification

leads to significantly improved results for two reasons: (a) it counteracts the fact that the

unfiltered backprojection is smoothing; (b) it produces images with pixel values at the right

scale. Therefore, we interpret the resulting ItNet as an industry-like iterative CT-algorithm

(e.g., see [WN19]), rather than a neurally-augmented convex optimization scheme.

In our experiments, we witnessed only minor effects by computing more than four iterations in (2.4). However, the accuracy was improved by the following post-training strategy: First, the ItNet is extended by one more iteration:

ItNet-post[] : Rm  RN, y 

5 k=1

DCk,y  U[~k]

 FBP

(y),

where ~k is initialized by the optimized weights of (2.4) for k = 1, . . . , 5. Then, ItNet-post is fine-tuned by keeping the weights ~1 = ~2 = ~3 of the first three UNets fixed and training only the last two iterations (without weight sharing). The obtained improvements indicate that there is a
trade-off between increasing the model capacity by more iterations and the difficulty of optimizing
the resulting network. The systematic study of such iterative training strategies is left to future
research.
The initialization and training of the weights k has a considerable impact on the accuracy of ItNet and ItNet-post. We have found that  = [1, 2, 3, 4] typically converges to values of the form {1 < 2 < 3 4} after sufficiently many training epochs of ItNet. For an additional

3 RESULTS

6

speed-up of the training, we use the initialization  = [1.1, 1.3, 1.4, 0.08], which was found by pre-training. Similarly, ItNet-post is initialized with the final values of ItNet for k = 1, 2, 3, together with 4 = 1.0 and 5 = 0.1. We suspect that a systematic study of these scalar weights could lead to further performance gains and to a more regular training procedure. In particular, it might be beneficial to decouple them from the training of the UNet-weights.
Fine Tuning To improve the overall performance of our networks, we have additionally applied the following "tricks", which are ordered by their importance:
(i) Due to statistical fluctuations, the networks typically exhibit slightly different reconstruction errors, despite using the same training pipeline. For the computation of our final reconstructions, we therefore ensemble 10 networks, each trained on a different split of the training set.
(ii) Due to the training with small batch sizes, we replace batch normalization of the UNetarchitecture by group normalization [WH18].
(iii) We equip the UNet-architecture with a few memory channels, i.e., one actually has that U[] : RN × (RN )cmem  RN × (RN )cmem (cf. [PW17; AO¨ 18]). While the original imageenhancement channel is not altered, the output of the additional channels is propagated through ItNet, playing the role of a hidden state (in the spirit of recurrent NNs). For our experiments, we have selected cmem = 5.
(iv) It was beneficial to restart occasionally the training of the networks, e.g., see Fig. 2.
The following modifications did not lead to a gain in performance and were omitted:
(i) Improving the FBP in Step 1 by making some of it components learnable (e.g., the filter), cf. [WGCM16]. Although this is advantageous for the reconstruction quality of the FBP itself, it leads to worse results for UNet and ItNet. This suggests that a combination of model- and data-based methods benefits most from precise and unaltered physical models.
(ii) Adding additional convolutional-blocks in the measurement domain of ItNet.
(iii) Modifying the standard 2-loss by incorporating the RMSE or the 1-norm.
(iv) Utilizing different optimizers such as RAdam, AdamW, SGD, or MADGRAD.

3 Results
We conclude this report by briefly showing some of our results. In terms of quantitative similarity measures, we restrict ourselves to reporting the RMSE, which is the main evaluation metric for the challenge.3 For comparison, we also consider a post-processing of the FBP by the more advanced Tiramisu-architecture [JDVRB17; Bub+19; GMM20]. Furthermore, we have also trained the iterative learned primal-dual (LPD) scheme [AO¨ 18] (slightly modified by replacing the unfiltered backprojection by the FBP).
In Fig. 2, we first visualize the RMSE loss curves of our training pipeline, i.e., UNet  ItNet + restart  ItNet-post + 2 × restart. Furthermore, the average performance of all considered networks is reported in Table 1. To give a visual impression as well, reconstructions of an image from the validation set can be found in Fig. 3. Finally, we analyze the aspect of data-consistency in Fig. 4. This figure suggests that the performance of ItNet-post could still be improved if the exact forward model was available. This conclusion is also underpinned by the following observation: Even a small improvement in the parameter identification of Step 1 resulted in a significantly more accurate ItNet-post.
3Note that the RMSE is only reported for a subset of 125 images from the training set, which we have used for validation. Hence, these values might differ from the actual results on the official challenge test set. In particular, the values of the ensembled ItNet-post might (slightly) overfit, since we did not hold back separate validation images for evaluating the ensembling step.

GENZEL, MACDONALD, MA¨ RZ: DL-SPARSE-VIEW CT CHALLENGE

7

Figure 2: Loss curves and network training. The first two plots demonstrate that ItNet improves the RMSE by approximately an order of magnitude in comparison to a post-processing by UNet. Furthermore, the gain of our UNet-initialization strategy can be seen in the second graph. The last two plots illustrate the advantages of restarting and of the post-training strategy, respectively. Note that we display the RMSE on the training and validation sets instead of the actual 2-losses, which behave similarly.

Figure 3: Reconstruction results. We display reconstructions of a validation image. The first row compares the FBP provided by the challenge with our own (see Step 1). The second row compares a post-processing by Tiramisu with the (ensembled) ItNet-post. The ground truth image is omitted, since it is visually indistinguishable from ItNet-post.

RMSE

Baselines FBP Our FBP 5.72e-3 3.40e-3

UNet 3.50e-4

Our Network Variants

ItNet ItNet-post ItNet-post (ens.)

1.64e-5 1.05e-5

6.42e-6

Comparison Networks

Tiramisu

LPD

2.24e-4

1.24e-4

Table 1: This table reports the average RMSE.

REFERENCES

8

Figure 4: Data-consistency. The first image analyzes the accuracy of our forward model by displaying
the error y0 - Fx0 for a sinogram-image pair (y0, x0) from the validation set. The visualization of y0 - F · ItNet-post(y0) in the middle is visually nearly indistinguishable, which shows that ItNet-post inherits the inaccuracies from Step 1. Therefore, ItNet-post would allow for even better results if a
more accurate forward model was available. It is also interesting to compare with y0 - F · Tiramisu(y0): The image on the right exhibits considerably larger errors, which reveals that the post-processing by
Tiramisu suffers from a lack of data-consistency. All images are shown in the same dynamical range.

References

[AO¨ 18] [AMJ18] [ARPAH20] [Bub+19]
[Che+17a] [Che+17b] [Fes17] [GMM20] [GMM21] [GL10]
[Ham+19]
[HA20] [JDVRB17]
[JMFU17] [KMY17]

J. Adler and O. O¨ ktem. "Learned primal-dual reconstruction". IEEE Trans. Med. Imag. 37.6 (2018), 1322­ 1332.
H. K. Aggarwal, M. P. Mani, and M. Jacob. "MoDL: Model-based deep learning architecture for inverse problems". IEEE Trans. Med. Imag. 38.2 (2018), 394­405.
V. Antun, F. Renna, C. Poon, B. Adcock, and A. C. Hansen. "On instabilities of deep learning in image reconstruction and the potential costs of AI". Proc. Natl. Acad. Sci. 117.48 (2020), 30088­30095.
T. A. Bubba, G. Kutyniok, M. Lassas, M. Ma¨rz, W. Samek, S. Siltanen, and V. Srinivasan. "Learning the invisible: A hybrid deep learning-shearlet framework for limited angle computed tomography". Inverse Probl. 35.6 (2019), 064002.
H. Chen, Y. Zhang, M. K. Kalra, F. Lin, Y. Chen, P. Liao, J. Zhou, and G. Wang. "Low-dose CT with a residual encoder-decoder convolutional neural network". IEEE Trans. Med. Imag. 36.12 (2017), 2524­2535.
H. Chen, Y. Zhang, W. Zhang, P. Liao, K. Li, J. Zhou, and G. Wang. "Low-dose CT via convolutional neural network". Biomed. Opt. Express 8.2 (2017), 679­694.
J. A. Fessler. Analytical Tomographic Image Reconstruction Methods (Chapter 3 of book draft). 2017. URL: https://web.eecs.umich.edu/~fessler/book/c-tomo.pdf.
M. Genzel, J. Macdonald, and M. Ma¨rz. "Solving Inverse Problems With Deep Neural Networks ­ Robustness Included?" Preprint arXiv:2011.04268. 2020.
M. Genzel, J. Macdonald, and M. Ma¨rz. Code repository for "AAPM DL Sparse View CT Challenge". 2021. URL: https://github.com/jmaces/aapm-ct-challenge.
K. Gregor and Y. LeCun. "Learning Fast Approximations of Sparse Coding". Proceedings of the 27th International Conference on International Conference on Machine Learning (ICML). Ed. by J. Fu¨ rnkranz and T. Joachims. 2010, 399­406.
K. Hammernik, J. Schlemper, C. Qin, J. Duan, R. M. Summers, and D. Rueckert. "-net: Systematic Evaluation of Iterative Deep Neural Networks for Fast Parallel MR Image Reconstruction". Preprint arXiv:1912.09278. 2019.
A. Hauptmann and J. Adler. "On the unreasonable effectiveness of CNNs". Preprint arXiv:2007.14745. 2020.
S. Je´gou, M. Drozdzal, D. Vazquez, A. Romero, and Y. Bengio. "The one hundred layers tiramisu: Fully convolutional densenets for semantic segmentation". Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2017, 11­19.
K. H. Jin, M. T. McCann, E. Froustey, and M. Unser. "Deep convolutional neural network for inverse problems in imaging". IEEE Trans. Image Process. 26.9 (2017), 4509­4522.
E. Kang, J. Min, and J. C. Ye. "A deep convolutional neural network using directional wavelets for low-dose X-ray CT reconstruction". Med. Phys. 44.10 (2017), e360­e375.

GENZEL, MACDONALD, MA¨ RZ: DL-SPARSE-VIEW CT CHALLENGE

9

[KB14] [Kno+20] [Leu+21] [Muc+20]
[PW17] [RCS20] [RFB15] [SLBP21] [Sid+21]
[Sri+20] [WN19] [WH18] [WGCM16] [YSLX16]

D. P. Kingma and J. Ba. "Adam: A Method for Stochastic Optimization". Preprint arXiv:1412.6980. 2014.
F. Knoll, T. Murrell, A. Sriram, N. Yakubova, J. Zbontar, M. Rabbat, A. Defazio, M. J. Muckley, D. K. Sodickson, C. L. Zitnick, and M. P. Recht. "Advancing machine learning for MR image reconstruction with an open competition: Overview of the 2019 fastMRI challenge". Magn. Reson. Med. 84.6 (2020), 3054­3070.
J. Leuschner, M. Schmidt, P. S. Ganguly, V. Andriiashen, S. B. Coban, A. Denker, D. Bauer, A. Hadjifaradji, K. J. Batenburg, P. Maass, and M. van Eijnatten. "Quantitative Comparison of Deep Learning-Based Image Reconstruction Methods for Low-Dose and Sparse-Angle CT Applications". J. Imaging 7.3 (2021).
M. J. Muckley, B. Riemenschneider, A. Radmanesh, S. Kim, G. Jeong, J. Ko, Y. Jun, H. Shin, D. Hwang, M. Mostapha, S. Arberet, D. Nickel, Z. Ramzi, P. Ciuciu, J.-L. Starck, J. Teuwen, D. Karkalousos, C. Zhang, A. Sriram, Z. Huang, N. Yakubova, Y. Lui, and F. Knoll. "State-of-the-art Machine Learning MRI Reconstruction in 2020: Results of the Second fastMRI Challenge". Preprint arXiv:2012.06318. 2020.
P. Putzky and M. Welling. "Recurrent Inference Machines for Solving Inverse Problems". Preprint arXiv:1706.04008. 2017.
Z. Ramzi, P. Ciuciu, and J.-L. Starck. "XPDNet for MRI reconstruction: an application to the fastMRI 2020 brain challenge". Preprint arXiv:2010.07290. 2020.
O. Ronneberger, P. Fischer, and T. Brox. "U-Net: Convolutional Networks for Biomedical Image Segmentation". Medical Image Computing and Computer Assisted Intervention ­ MICCAI 2015. Ed. by N. Navab, J. Hornegger, W. M. Wells, and A. F. Frangi. Springer Cham, 2015, 234­241.
E. Sidky, I. Lorente, J. G. Brankov, and X. Pan. "Do CNNs solve the CT inverse problem?" IEEE Trans. Biomed. Eng. 68.6 (2021), 1799­1810.
E. Sidky, X. Pan, J. Brankov, I. Lorente, S. Armato, K. Drukker, L. Hadjiyski, N. Petrick, K. Farahani, R. Munbodh, K. Cha, J. Kalpathy-Cramer, B. Bearce, and AAPM Working Group on Grand challenges. Deep Learning for Inverse Problems: Sparse-View Computed Tomography Image Reconstruction (DL-sparse-view CT). 2021. URL: https://www.aapm.org/GrandChallenge/DL-sparse-view-CT/.
A. Sriram, J. Zbontar, T. Murrell, A. Defazio, C. L. Zitnick, N. Yakubova, F. Knoll, and P. Johnson. "End-toend variational networks for accelerated MRI reconstruction". International Conference on Medical Image Computing and Computer-Assisted Intervention. 2020, 64­73.
M. J. Willemink and P. B. Noe¨l. "The evolution of image reconstruction for CT ­ from filtered back projection to artificial intelligence". Eur. Radiol. 29.5 (2019), 2185­2195.
Y. Wu and K. He. "Group normalization". Proceedings of the European conference on computer vision (ECCV). 2018, 3­19.
T. Wu¨ rfl, F. C. Ghesu, V. Christlein, and A. Maier. "Deep learning computed tomography". Medical Image Computing and Computer-Assisted Intervention ­ MICCAI 2016. 2016, 432­440.
Y. Yang, J. Sun, H. Li, and Z. Xu. "Deep ADMM-Net for Compressive Sensing MRI". Advances in Neural Information Processing Systems 29. Ed. by D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett. Curran Associates, Inc., 2016, 10­18.

