1
Smart Online Charging Algorithm for Electric Vehicles via Customized Actor-Critic Learning
Yongsheng Cao, Hao Wang, Member, IEEE, Demin Li, Guanglin Zhang, Member, IEEE

arXiv:2106.00854v1 [eess.SY] 1 Jun 2021

Abstract--With the advances in the Internet of Things tech-

nology, electric vehicles (EVs) have become easier to schedule

in daily life, which is reshaping the electric load curve. It is

important to design efficient charging algorithms to mitigate

the negative impact of EV charging on the power grid. This

paper investigates an EV charging scheduling problem to reduce

the charging cost while shaving the peak charging load, under

unknown future information about EVs, such as arrival time,

departure time, and charging demand. First, we formulate an

EV charging problem to minimize the electricity bill of the EV

fleet and study the EV charging problem in an online setting

without knowing future information. We develop an actor-critic

learning-based smart charging algorithm (SCA) to schedule the

EV charging against the uncertainties in EV charging behaviors.

The SCA learns an optimal EV charging strategy with continuous

charging actions instead of discrete approximation of charging.

We further develop a more computationally efficient customized

actor-critic learning charging algorithm (CALC) by reducing the

state dimension and thus improving the computational efficiency.

Finally, simulation results show that our proposed SCA can

reduce EVs' expected cost by 24.03%, 21.49%, 13.80%, compared with the Eagerly Charging Algorithm, Online Charging

Algorithm, RL-based Adaptive Energy Management Algorithm,

respectively. CALC is more computationally efficient, and its

performance

is

close

to

that

of

SCA

with

only

a

gap

of

5 56% .

in

the cost.

Index Terms--Electric vehicle, load scheduling, demand response, online learning, actor-critic method, projection.

I. INTRODUCTION
With the increasing concerns of environmental issues, electric vehicles (EVs) emerge as a promising solution as they do not directly consume fossil fuels and are more environmentally friendly. Meanwhile, the intermittent charging demands caused by electric vehicles (EVs) impact the operation of the public power grid [1]. Therefore, it is crucial to design charging control strategies to alleviate the peak load caused by EVs and cut down their electricity bills. It will be ideal if the
Y. Cao, D. Li, and G. Zhang are with College of Information Science and Technology, Donghua University; Engineering Research Center of Digitized Textile and Apparel Technology, Ministry of Education, Shanghai 201620, China (Email: yongshengcao@mail.dhu.edu.cn, deminli@dhu.edu.cn, glzhang@dhu.edu.cn). H. Wang is with the Department of Data Science and Artificial Intelligence, Faculty of Information Technology, Monash University, Melbourne VIC 3800, Australia, and also with the Stanford Sustainable Systems Lab, Stanford University, Stanford, CA 94305 USA (Email: hao.wang2@monash.edu). This work is supported by the National Natural Science Foundation of China (Grant No. 61772130, 61301118); the International Science and Technology Cooperation Program of the Shanghai Science and Technology Commission (Grant No. 15220710600); the Innovation Program of the Shanghai Municipal Education Commission (Grant No. 14YZ130); China Scholarship Council (File No. 201906630026); the Fundamental Research Funds for the Central Universities (Grant: CUSFDH-D-2018093); the FIT Academic Staff Funding of Monash University. (Corresponding author: Guanglin Zhang, Hao Wang.)

future charging demand is known in advance, such that the EV charging can be scheduled to flatten the total load [2]. However, an EV charging station faces great uncertainties in EVs' behaviors, including their travel patterns and charging demands. Online charge strategies become a promising paradigm for determining the optimal charging of EVs against uncertainties. The online EV charging problem is more practical, as it does not assume any future information but only relies on the current and past EV profiles, including the arrival time, the departure time, and the charging demand of EVs. The advances in the Internet of Thing (IoT) technology and the intelligent transportation system have paved the way for EVs [3]. The information shared between EVs can improve realtime transportation and make smart decisions for individual EVs [4]. It is easier to predict EVs' behaviors that make it possible to schedule a proper quantity of EVs to make charging decisions. In this paper, we aim to design online algorithms for EV charging without knowing future information.
In recent years, great efforts have been made to develop online EV charging algorithms under stochastic EV demands. Online charging problems do not assume future information about the profiles of EVs, which captures the realistic scenarios about uncertainties in EV charging behaviors. Yu et al. [5] proposed a distributed online algorithm by the Lyapunov optimization method and an improved alternating direction method to investigate an energy scheduling problem for distributed data centers and EVs. Qi et al. [6] proposed an online energy management framework of EVs by an evolutionary algorithm. Kang et al. [7] presented a novel centralized EV charging strategy based on spot price with the consideration of charging priority and charging location. Li et al. [8] designed a joint online learning and pricing algorithm to minimize the operational cost of utility considering time-varying demand responses and consumers' responses. A novel multi-objective evolutionary algorithm was proposed in [9] to minimize the peak-to-valley difference of the load and the operating cost. Quddus et al. [10] proposed a two-stage stochastic programming model to optimize the power flow of commercial buildings and EV charging stations with some practical constraints. However, these algorithms [5]-[10] relied on specific models or they only worked in special scenarios. Instead, we aim to develop a generic method for the EV charging problem that is less model-dependent and can work for various practical scenarios. Therefore, we refer to the model-free reinforcement learning (RL) to derive the optimal EV charging strategy in our work.
Model-free RL frameworks and policies have been utilized to handle the energy scheduling problems in the literature. For example, an improved Q-learning method has been proposed

2

to minimize the electricity bill and reduce users' discomfort for a household [11][12]. A bidirectional long short-term memory network-based parallel reinforcement learning was presented in [13] to construct an energy management strategy for a hybrid tracked vehicle. A control algorithm based on Qlearning has been designed in [14] to obtain the optimal control under physical and cyber uncertainties. A batch RL algorithm has been investigated in [15] to schedule controllable load such as washing machine. However, these studies [11]-[15] all used the RL methods, in which the charging actions have to be discrete values that restrict the model of actions. Using discrete actions for EV charging is an approximation to the real-world problem, as the EV charging amount is a continuous value. When the discrete-charging action is adopted, it is often difficult to achieve a good trade-off between computational complexity and performance. Using too few discrete levels may result in poor learning performance, but using a large discrete action space will make the training difficult and lead to high computation overhead [16].
In our work, we aim to develop optimal EV charging strategies with continuous charging amount other than discretize charging actions. The probability distribution of actions under different states is a stochastic policy, and deriving an optimal policy is at the center of RL methods. A new method was discussed in [17] to approximate the iteration of policy and improve the policy. Standard tabular Q-learning or deep Q neural network method can not derive the optimal policy over continuous action space [18][19]. Therefore, we adopt the actor-critic method [20] to solve our problem with continuous states and actions. The actor-critic method can use the deep neural network (DNN) to estimate the value functions, which can get a better approximation. There are some studies on the scheduling problem using the actor-critic method. Some recent studies in [21]-[22] developed actor-critic learning approaches for various applications. For example, a concurrent actorcritic learning framework was proposed in [21] to achieve a close-to-optimal feedback-Nash equilibrium solution to a multi-player non-zero-sum differential game in an infinite horizon. A distributed framework based on policy search was proposed in [23] to accelerate the learning processes of robot moving by reducing variance. An actor-critic approach was proposed in [22] to approximate the performance function based on adaptive dynamic programming strategy. Lu et al. [24] proposed a real-time incentive-based algorithm to help the service provider balance energy fluctuations and improve the reliability of smart grid systems using deep RL. Wei et al. [25] proposed a policy-gradient method to study the problem of user scheduling and resource allocation in heterogeneous networks with continuous states and actions. However, the actor-critic algorithm may not have a good convergence if its policy is on-policy [26]. Hence, we design an online EV charging strategy using asynchronous actor-critic learning, and we further develop a customized actor-critic algorithm, significantly improving the convergence and achieving a closeto-optimal solution of the charging schedule of EV fleet.
This paper aims to design an online EV charging scheduling algorithm leveraging the EV charging data. We first formulate an offline optimization problem, in which the future profiles of

EVs are known. The offline problem captures all the modeling components, including energy cost, arrival time, departure time, and the charging demand of EVs, thus serving as a basic formulation for EV charging. As the offline problem is unrealistic, we further formulate an online optimization problem for EV charging without assuming future profiles of EVs and develop online charging strategies. We model the EV charging decision as a Markov decision process (MDP), where the charging station determines the charging schedule according to the past and current information, including the arrival time, departure time, and charging demand of EVs. In the MDP setup, the current charging decision will affect the next state, the charging decision, and accumulative rewards in the future. We aim to develop an optimal EV charging strategy to minimize the expected total energy cost under uncertainties of EV charging behaviors. We develop an actor-critic learningbased smart charging algorithm (SCA), which determines the optimal continuous-charging amount for each EV using asynchronous actor-critic reinforcement learning. To further improve the computational efficiency of SCA, we develop a customized actor-critic learning charging algorithm (CALC) that reduces the dimension of the state during the learning phase. Finally, SCA and CALC are compared with three stateof-the-art algorithms. We summarize the contributions of our paper as follows.
· We model the online EV charging problem as a Markov decision process to capture the decision marking under uncertainty of EV charging profiles. We develop a smart charging algorithm (SCA) to solve the online EV charging problem, which leverages the advantage of the asynchronous actor-critic method with good convergence to derive the optimal charging policy.
· We further develop a more computationally efficient customized actor-critic learning charging algorithm (namely CALC), which consists of two stages. In the first stage, CALC learns the charging policy for the whole group of EVs using actor-critic learning and obtains the optimal aggregate charging amount; in the second stage, CALC allocates the aggregate charging amount to serve each EV based on the projection theory. Such a customized algorithm achieves sub-optimal performance and significantly reduces the computational overhead, achieving a good trade-off between performance and computation.
· The developed SCA and CALC algorithms learn EV charging strategies with continuous charging actions instead of discrete approximation of charging. We compare our developed charging algorithms with Q-networkbased RL algorithm, namely adaptive energy management (AEM), which makes discrete actions. Our results show that our developed charging algorithms outperform AEM with different numbers of discretized actions in achieving a better trade-off between computation and performance.
The remainder of this paper consists of five sections. Section II formulates the offline EV scheduling problem. Section III extends the offline EV charging problem to an online charging scheduling problem. Then SCA and CALC algorithms are proposed and analyzed in Section IV. Simulation results are

3

presented in Section V. Finally, Section VI concludes the paper. The summary of notations is shown in Nomenclature.
II. OFFLINE EV CHARGING PROBLEM In this section, we investigate the optimal offline EV charging problem, where the future information of EVs is known in advance. The offline EV charging optimization problem will help formulate the online charging problem in Section III.

A. System Architecture
We consider a community including an EV charging station and the inelastic base load. The charging station serves EVs in a region, and we restrict our discussion for the charging management at one charging station. We aim to minimize the total charging cost of EVs from the charging station. We study the charging scheduling of EV battery in a time horizon T and the time index t  T = {1, 2, ..., T }. We assume that N EVs arrive in the order from 1 to N and the index of EV is i  N = {1, 2, ..., N }. We set the arrival time and departure time of EV i as tai rr and tdi ep, respectively. Let Di denote the charging demand of EV i. In the traditionally offline models, the charging station knows the EV profiles Di, tai rr, and tdi ep in advance, which is unrealistic. This paper will consider an online model, in which the charging station does not know any future information about EV profiles, capturing the key uncertainty in a real-world scenario.
According to the physical constraints of EV battery, EV i should be charged at a charging amount bi(t) in time slot t and the charging amount in each time slot has a bound, that is,

bi(t)  [0, bi,max],

(1)

where bi,max is the maximum charging amount in a time slot

of EV i. We set the state of charge (SOC) of EV i in time

slot

t

as

S OCi,t ,

which

is

defined

as

S OCi,t

=

, Bi(t)
Bi,max

where

Bi,max is the battery capacity of EV i and the battery level

follows Bi(t + 1) = Bi(t) + bi(t). We denote H(t) as the set of EVs that are parked in the charging station in time slot t.

The charging station can control the charging amount bi(t). We denote the charging load of EV fleet lev(t) in time slot t,

that is,

lev(t) =

bi(t).

(2)

iH(t)

Except for the EV charging load, we also consider the inelastic
base load of the other electricity demand from the community lb(t), such as lighting or watching TV. We assume that the base load lb(t) can be predicted accurately at the beginning of each time slot t and thus is known to the operator. The total load in time slot t is L(t) and we have

L(t) = lev(t) + lb(t).

(3)

In practice, the total load includes the total EV charging load and base load and is upper-bounded by Lmax. According to [27], the unit electricity price p(t) is modeled as a linear
function of the total load,

p(t) = k0 + 2k1(lev(t) + lb(t)),

(4)

where k0 and k1 are non-negative coefficients. Essentially, reducing the electricity cost is to shift the load evenly and avoid significant peak load. We denote b(t) as a vector form of (b1(t), b2(t), ..., bi(t)) and calculate the electricity bill c(b(t)) of the charging station as follows,

L(t)

c(b(t)) =

(k0 + 2k1z)dz

lb(t)

= k0

bi(t) + k1

2
bi(t)

iH(t)

iH(t)

+ 2k1lb(t)

bi(t),

(5)

iH(t)

where the electricity bill c(b(t)) is the integral of the unit electricity price p(t) in the load interval from lb(t) to L(t).

B. Problem Formulation

We first formulate the offline optimization problem assuming that we know the EV arrival time, departure time, and charging demand. We aim to find the optimal charging solution bi(t) of each EV i to minimize the total EV charging cost in an operational horizon T , where the profiles of EVs are known ahead. Then, we can formulate the offline charging optimization problem as follows,

T

2

min

k0 bi(t) + k1

bi(t)

bi(t) t=1

iN

iN

+ 2k1lb(t) bi(t)

(6a)

iN

tdi ep

s.t. bi(t) = Di, i  N

(6b)

tai rr

0  bi(t)  bi,max, i  N , t  T

(6c)

0  L(t)  Lmax, t  T

(6d)

where Di is time of EV

the charging demand of i, tdi ep is the departure

EV i, tai rr is time of EV

the arrival i. We can

find that problem (6) is a convex optimization problem. If

the profiles tai rr, tdi ep and Di are known in advance, the

optimal charging solution bi(t) can be attained by solving

the optimization problem (6). Nevertheless, the current EV

charging departure

information, including the time tdi ep and Di(t) can

arriving only be

time tai rr, the known when it

arrives at the charging station. In the next section, we will

study an online EV charging problem derived from this offline

problem, and the future information of EVs is unknown in the

online charging problem.

III. ONLINE EV CHARGING PROBLEM
In this section, we reformulate an online charging problem based on (6). We denote Q(i, ts) as the set of time indices that EV i will charge in the charging station, and Q(i, ts) = [ts, tdi ep]. The charging amount in time slot t of EV i is defined as bi(t), where t  Q(i, ts). When an EV comes to the charging station in time slot ts, we optimize the

4

charging scheduling without knowing the demand of EVs in the future.
In the online charging problem, the profiles of EVs including the charging demand, the arrival and departure time, are not known ahead. The charging station needs to schedule the EV charging for each current time slot while facing uncertainties of EV profiles in the future. According to [28], we define the exogenous information variable (t) that comes up in time slot t, which is not known ahead when the charging decision b(t) is made. We have the exogenous information variables (t),

(t) = (t^ai rr, t^di ep, D^ i(t)),

(7)

where t^ai rr, t^di ep, D^ i(t) are the arrival time, departure time and the electricity demand of EV i in time slot t. These
exogenous variables could be field observations [28], bringing

uncertainties and challenges to the problem solving. We aim to find the optimal charging solution b = (b(1), ..., b(T )) to minimize the total EV charging cost CT in an operational horizon T ,

T

CT = c(b(t)),

(8)

t=1

where T is ending time of the period T . Therefore, we have

an online charging optimization problem as follows,

min E[CT ]

(9a)

b(t)

s.t.

bi(t) = D^ i(ts), i  H(ts)

(9b)

tQ^ (i,ts )

0  bi(t)  bi,max, i  H(ts), t  Q^(i, ts),

(9c)

0  L(t)  Lmax, t  Q^(i, ts),

(9d)

where Q^(i, ts) = [ts, t^di ep], and H(ts) is the set of EVs that park in the charging station in time slot ts and will remain in the charging station at time t, t  Q(i, ts). We define W(t) as the set of the rolling window from the current time slot t to t, where t is the maximum departure time of EVs in H(t),
when EVs are parked in the charging station, that is,

W(t) = {t|t  t & t  max{ti|i  H(t)}}.

(10)

Fig. 1 shows an example to explain the concept of H(t)

H(t),t=4 EV5

EV4

EV1

EV3

EV2

0 1 2 3 4 5 6 7 8 9 T time

Ongoing service time W(t)
Current time slot
Fig. 1. Illustration of H(t) and W(t).

and W(t). There are five EVs at the charging station in this example. In time slot 4, there are four EVs in this charging station, e.g., H(4) = {2, 3, 4, 5}. In this time slot, the
maximum service time of EVs parking in this charging station

is from t = 4 to t = 9, e.g., W(4) = {4, 5, 6, 7, 8, 9} and EV 5 is the last one leaving the charging station in time slot 9. For the rolling-based method according to [29], we replace the interval t  Q^(i, ts), in which EV i stays in the charging station as t  W(ts). Heuristic rolling-based online control is a general method widely used in various problems of smart grids, such as real-time energy scheduling [30] and ancillary services [31]. The charging station will implement the optimal solution opt[bi(t)] to Problem (9) until a new EV comes in. When a new EV arrives, or EV completes charging, or the load from the community changes, H(ts), Q^(i, ts), D^ i(ts) should be updated and Problem (9) should be solved again.
In practice, the offline problem is not practical because we do not have future information about EV profiles, and it is often challenging to obtain reliable prediction of human behaviors. In the online charging setting, it is difficult to solve this optimization problem using an intuitive mathematical programming approach, and the aforementioned rolling-based algorithm is heuristic and not optimal. We only use the rollingbased online control algorithm (OA) as a benchmark for our proposed algorithms in Section IV.
IV. RL-BASED EV CHARGING ALGORITHM
To tackle the challenges brought by the uncertainty of the EV behaviors, we seek an intelligent EV charging strategy using reinforcement learning. Considering that EV charging amount is continuous, we apply the actor-critic algorithm to solve the online EV charging problem, which combines the value-based and policy-based method.

A. RL Framework Formulation
We model EV charging decision as a Markov decision process. The charging station will make the charging schedule based on the past and current exogenous information, which includes the arrival time and departure time, and charging demand of EVs that stay in the charging station at the current time slot. The state (t) consists of electricity price and the SOC of EV i. We measure SOCi(t) as the percentage of the battery capacity Bmax of EV i. Then we have the state (t)   as follows,

(t) = (SOC1(t), ..., SOCN (t), p(t)),

(11)

where SOCi(t) is the SoC of EV i  H(t) and p(t) is the electricity price. The electricity price is modeled as a linear
function of total load in the community so that base load can
influence the price. The action consists of all the charging amount bi(t) of EV
i in time slot t, which are continuous variables,

bi(t)  [0, bi,max], i  H(ts), t  Q^(i, ts),

(12)

where bi,max is the maximum charging amount of EV i in each time slot, and bi(t) should be in the range [0, bi,max]. The charging action needs to satisfy the following constraint,

t^di ep

ts -1

bi(t) = D^ i(t) -

~bi(t),

t=ts

t=t^ai rr

(13)

5

where ts is the current time slot, D^i(t) is the electricity

demand of EV i with exogenous information in time slot t,

and arrival

ttt=sim-t^ai1errt^aiisrrthteo

actual charging amount the moment before the

of EV i current

from time

the slot

ts - 1. During the training, the environment ((t), bi(t), r(t))

is composed by the state including the base load from the

community and the exogenous information (such as the arrival time t^ai rr, departure time t^di ep, the EV charging demand D^i(t) of EV i in time slot t), the charging action bi(t), and the reward r(t). Each decision for charging action ~bi(t) will affect the state, including residual EV charging demand D^ i(t) for future

time slots in turn.

According to the relationship (4) between the electricity

price and the load, we use the EV charging cost to set the

reward function r(t) as

r(t) = -

(k0 + 2k1bi(t) + 2k1lb(t))bi(t), (14)

iH(t)

where bi(t) is the charging amount. To evaluate the expected accumulated rewards of current state with action bi(t) and use the policy  to choose the charging action according to the state (t), we denote the state-action value function Q ((t), bi(t)) which is the value of taking the charging action bi(t) in state (t) under a policy  as follows,

T
Q ((t), bi(t)) = E { [kr(t + k)]|((t), bi(t), )},
k=0
(15)

where   (0, 1) is the discount factor and the policy  is a function of the parameter .
Different from the state value function which is the optimal reward function according to current state and the stationary policy, such as Greedy policy, the state-action value function of actor-critic is the expected rewards according to current state. It utilizes a parameterized policy to select the charging action, which can be given by,

Q((t), bi(t)) = E{r(t) + Q((t + 1), bi(t + 1))}, (16)

where E{·} is the expectation function,  is the discount factor to evaluate foresighted decisions and  can be approximated by ((t), bi(t)).
We optimize the policy  with a Gaussian distribution   N (µ, 2), where the expectation µ and logarithmic standard deviation log are approximated by the multi-layer perceptron (MLP), which can be expressed as follows,

µ = µ h + µ,

(17)

log =  ,

(18)

where µ,  are the output layer's weights, µ is output layer's bias and (·) is the operation of taking the transpose. The parameter  is the network weights of MLP and µ, , µ  . The feature h is extracted from the hidden layers of MLP, which can be expressed as follows,

h = y(n vn + n),

(19)

where v+1 = y( v + ),  = 1, 2, ..., n - 1,

v1 = (t),

and  ,  are the weight and bias in the th hidden layer, y(·) is the rectified linear unit activation function, and (t) is the
state, which is the input of MLP. We build the actor process
and critic process according to MLP.

B. Actor Process

We assume that the gradient policy ((t), bi(t)) is differentiable in parameter  and the update of  is given as
follows,



=

a  J

( )

=

a

J ( 

)

 

,

(20)

where a is the learning rate for the actor and should be set small enough to avoid the oscillation of the policy [32],

because small updates of the value function will greatly

influence the update of the policy. According to the maximum

entropy principle [33], we can utilize the Gaussian probability

distribution [20] to provide a parameterized policy to select

continuous-charging action, which is represented as

((t), bi(t))

=

 1 e-(bi(t)-((t)))2/22 , 2

(21)

where ((t)) is the average action value of this charging state, and  defines the standard deviation of all the possible charging actions. Then ((t), bi(t)) is the probability of choosing action bi(t) in state (t). According to Q ((t), bi(t)), we know the expected reward of the charging action bi(t) at state (t). Then we adjust the policy  to make EV charging decisions.
The objective of the actor-critic method is to find an optimal policy  to maximize the following function,

J () = E{Q ((t), bi(t))}

= D ((t))

Q ((t), bi(t))dbi(t)d(t), (22)



bi (t)

where D () is the state distribution function of policy . We should optimize J () by improving the parameters of policy  iteratively. We utilize vector  = (1, 2, ..., n) to build the policy ((t), bi(t)) = P r(bi(t)|((t), )). In our implementation, the actor network has a fully-connected hid-
den layer with 200 neurons, where state (t) is the input and parameter  is the output. As mentioned above, ((t), bi(t)) is the probability of choosing action bi(t) in state (t). The DNN can be trained to learn the best fitting parameter vector
 by iteratively minimizing the TD error.

C. Critic Process
The policy  generates continuous actions from Gaussian distribution N (µ, 2). The expectation value µ is approximated by MLP. The temporal difference (TD) error is utilized to show the error between the approximation and the true value [20]. TD error is defined as
t = r(t + 1) + Q ((t + 1), bi(t + 1)) - Q ((t), bi(t)), (23)

6

Actor

Thread 1 Thread 2

(QYLURQPHQW
EV1 EV2 EV3

ts

t3dep t1dep t2dep

(QYLURQPHQW
EV1 EV2 EV3

ts

t3dep t1dep t2dep

7'HUURU 7'HUURU

Thread n

(QYLURQPHQW
EV1 EV2 EV3

ts

t3dep t1dep t2dep

7'HUURU

$V\QFKURQRXVXSGDWH

t

t

1234 5

N

1 2 3 4 EV i 5

EV i

N

(a)

(b)

Fig. 3. The state description of CALC.

t
lev(t) (c)

Fig. 2. The architecture of SCA.

where r(t + 1) is the reward in next time slot t + 1, and r(t + 1) + Q ((t + 1), bi(t + 1)) is actual return following
time t. Similar to the parameters' update in the actor process, the parameters v in critic process are updated as follows,

v = ctv Qv ((t), bi(t)),

(24)

where c is the learning rate for the critic and it should be chosen carefully because it will cause the oscillation if it is
too large or it will take a long time to converge if c is too small. In our implementation, the critic network has a fully-
connected hidden layer with 100 neurons, where state (t) is the input, and value function Qv ((t), bi(t)) is the output. D. Actor-critic learning-based Smart Charging Algorithm

The complete description of actor-critic learning-based
smart charging algorithm (SCA) is shown in Algorithm 1. The
architecture of SCA is shown in Fig. 2. First, we set the critic learning rate c, actor learning rate a, and the discount factor . Then we have constructed the critic process and the actor
process to develop SCA. The critic process evaluates the policy from the state-action value function Qv ((t), bi(t)) and the actor process has the following policy gradient, in which we use  and b as a logogram for state (t) and charging action bi(t),

J ()

 D () Q (, b)(b|(, ))dbd (25)



b

The actor parameter  and critic parameter v are updated

simultaneously. To be more specific, the actor parameter  is

updated in the direction decided by the critic output. When

actor-critic algorithm converges, the two sets of parameters

are optimized. This algorithm has a policy (bi(t)|((t), )) and an approximate value function of state Q((t), v) and

uses the multi-step returns [20] to update the policy and the

value function. The policy and the value function are updated

after every kmax actions where the charging demands for

EVs coming to the community are satisfied for an episode.

According to Algorithm S3 in the reference [32], the critic

parameters are updated by follows,

dv = dv + (R(t) - Q((t); v))2/v, (26)

and the actor parameters are updated by follows,
d = d +  log (bi(t)|((t), ))(R(t) - Q((t), v)), (27)

where the cumulative reward R(t + 1) = r(t) + R(t) and the immediate reward r(t) = - iH(t)(k0 + 2k1bi(t) + 2k1lb(t))bi(t). There is an agent in each thread, working in the copied environment. The gradient of one parameter is generated in each step. The gradients in many threads accumulate and parameters are shared and updated after the certain steps. After certain iterations, the reward will tend to converge, and the optimal charging solution will be achieved.
Algorithm 1 Actor-critic Learning-based Smart Charging Algorithm (SCA) Input: Critic learning rate c and actor learning rate a, discount factor , Gaussian policy ((t), bi(t)), b  N (µ, 2) Output: Action bi(t)
1: Initialization: Thread step counter t = 1, global shared counter k = 0. Starting in state (0)  d ((t)), set parameter  = 0 and I = 1.
2: for each thread do 3: Reset gradients: d = 0 and dv = 0. 4: Synchronize thread parameters  =  and v = v 5: Set tstart = t, get state (t) 6: repeat 7: for each step do 8: Select action bi(t+1)  ((t), bi(t)), move to next state
(t + 1)  P ((t), bi(t), (t + 1)), then get immediate reward r(t + 1), update k  k + 1 9: Critic: 10: Update the basis function:  ((t), bi(t)) =  ln ((t), bi(t)) 11: Update: I = I Qv ((t + 1), bi(t + 1)) = v ln v ((t + 1), bi(t + 1)) 12: critic parameters: vt+1 = vt+ctI, where t is updated by (23) 13: Actor: 14: Update the policy parameter: t+1 = t + atJ () 15: Update: (t)  (t + 1), bi(t)  bi(t + 1), z(t)  z(t + 1), Q ((t), bi(t))  Q ((t + 1), bi(t + 1)) 16: end for 17: Perform asynchronous update of  using d and of v using dv according to (26), (27). 18: until k > kmax 19: end for
SCA takes the charging amount of each individual EV as the state and may suffer from high computational overhead.

7

In the next section, we further develop a more computationally efficient customized algorithm by combining SCA with projection theorem, which takes the total charging amount as the state, and significantly reduce the state dimension.

E. Customized Actor-Critic Learning Charging Algorithm

To be more computationally efficient, we further develop a

customized actor-critic learning charging algorithm (CALC)

with two stages. In the first stage, the aggregate charging

of EVs can be solved by actor-critic learning. In the second

stage, CALC finds a close-to-optimal charging schedule for

each EV by the projection theorem. Different from SCA that

directly solves the charging amount for each EV, CALC takes

the total charging amount of arriving EV fleet as the action

and thus significantly reduces the dimension of the state space

as shown in Fig. 3. We see that the full state in Fig. 3(b)

consists ofcharging actions for all N EVs in Fig. 3(a), where the mark ` ' indicates the available time slots for charging.

The reduced state, as shown in Fig. 3(c), only considers the

aggregate charging action instead of charging actions for each

EV.

In the first stage, we optimize the aggregate charging

schedule lev(t) =

bi(t) by solving the following cost

iH(t)

minimization problem:

min

k0lev(t) + 2k1lev(t)lb(t)

lev (t) tT

s.t. 0  lev(t)  Ntbmax,

2
+ k1 lev(t)

(28a) (28b)

where Nt is the number of EVs in the charging station in time slot t, and bmax is the maximum charging amount in time slot t of EV battery. Then we can get the optimal action lev(t) by using the actor-critic learning. We have the state (t) as
follows,

(t) = (SOCev(t), lb(t)),

(29)

charging constraints are satisfied. Specifically, we solve the following projection problem,

min
bi(t),bi (t) tT

||bi(t)
iH(t)

-

bi (t)||2

tidep
s.t. bi (t ) = Di , i = 1 , 2 , ..., N
tiarr
bi(t ) = lev (t )
i H(t )

0  bi(t)  bi,max 0  bi (t)  bi,max 0  L(t)  Lmax,

(31a)
(31b) (31c) (31d) (31e) (31f)

where bi (t) is the temporary variable. We solve individual EV charging in the second stage, such that the charging allocation bi(t) is the closest to bi (t) of which the summation is the optimized aggregate charging schedule lev(t) in the first stage. Therefore, we solve close-to-optimal solution for individual

EV charging schedule that fulfills each EV's charging con-

straints. Using the reduced state, in stage-1, we optimize the total charging amount of all EVs lev(t) without considering individual EV charging constraints. Note that the derived total

charging amount may not be a feasible solution. Therefore,

we construct the stage-2 problem, in which we reallocate

the optimized total charging amount to each EV to fulfill

each EV's charging constraints and thus obtain a close-to-

optimal solution. Since CALC and SCA share the same core

algorithm, i.e., the actor-critic learning, the convergence of

CALC can be guaranteed when the learning rates a and c

satisfy

 t=0

c2

 t=0

a

=

,

< , according

 t=0

c

=



and

to [34]. Therefore,

 t=0

a2

<

we need to

, set

the learning rate properly for CALC to guarantee its conver-

gence. For the computational complexity, please refer to [35].

We also numerically show the convergence of our developed

algorithms and their computational time in simulation results

in Section V.

where SOCev(t) =

SOCi(t) is the total charging

iH(t)

amount of EVs at the charging station in time slot t. We revise

the reward function r(t) as follows.

r(t) = -(k0 + 2k1lev(t) + 2k1lb(t))lev (t), (30)

V. SIMULATION
In this section, we evaluate the performance of three stateof-the-art algorithms, SCA and CALC by using practical load profiles.

where the action lev(t) =

bi(t). We use actor-critic

iH(t)

learning to get the optimal charging solution lev(t). Then

we allocate the total charging amount bi(t) to each EV

i  H(t) by the CALC with Projection theorem according to

the problem (28). The charging station will adopt the close-

to-optimal solution to Problem (9) until a new EV comes in.

When a new EV arrives, the profiles of lev(t) will be updated. Note that the optimal aggregate charging amount lev(t) derived in the first stage does not consider individual EV charging

constraints in Problem (6b) and thus may not be the optimal or

even feasible for the offline problem in Problem (6). Therefore,

in the second stage, we aim to allocate a close-to-optimal

charging schedule for each EV and make sure individual EV

A. Parameter-Settings for the Dynamic Simulation
We adopt the base load profile in South California Edsion for two days from [36], that is, T = 48h. We set one time slot as 1h. The arrival of EVs can be obtained from the statistical data in [37]. Fig. 4 shows the distribution of the EV arrival. The initial SOC of an electric vehicle's battery affects the charging time and the load profile of the community. It is difficult to obtain the field measurement data and we can estimate the initial SOC value when EV arrives by a typical drive cycle, such as an urban dynamometer driving schedule [38]. Based on the measured statistics from [39], Fig. 5 depicts the probability density function of SOC of EVs' battery at their arrival. This period is evenly divided into 48 intervals.

8

Probability of Start Charging Time (from EPRI)

14%

station with the discrete-charging actions solved by the AEM

12%

algorithm, which is based on the Q-learning algorithm. AEM

algorithm with discrete-charging actions is a good benchmark
10%
for our proposed algorithm with continuous-charging actions.

8%

We denote the cost by AEM as RL.

6%

4%
2%
0 0

5

10

15

20

25

Time(hour)

100 EC

80

OA AEM

SCA

60

CALC

Total Load(kW)

Fig. 4. The probability of EVs' arrival over time [37].

40

Probability of SOC at start charging

25% 20% 15%

20

0

12

15

20

25

30

35

Time(hour)

Fig. 6. Comparison of total loads of 40 Type-1 EVs.

EV Charging Load(kW)

10%

5%

0

0

0.2

0.4

0.6

0.8

1

SOC

50 EC

40

OA AEM

SCA

30

CALC

Fig. 5. The probability of EVs' SOC at the arrival time [37].

20

We assume the same specifications for every EV in a scenario. We consider two types of EVs in two scenarios respectively based on two EV models [40]: the first type has a maximal charging amount per time slot as bmax = 3.2 kW, and the battery capacity is Bmax = 36 kWh. The other type has a maximal charging amount per time slot as bmax = 1.4 kW, and battery capacity is Bmax = 16 kWh. As discussed, our work models the distribution of the EV arrival, the probability density function about the SOC of EVs' battery at their arrival based on [37]. We generate the exogenous variables of EVs via Monte Carlo simulations using the distributions of arrival/departure patterns and charging demand. Note that if real-world measurements of EVs are available, they can be directly used in our algorithms as well. We simulate these algorithms by Python on Win 10 x64.
B. Performance Evaluation
We evaluate the performance of SCA by using practical data and compare it with three benchmark algorithms as follows,
1) Eagerly Charging Algorithm (EC) [41]: EV i draws the maximum amount of electricity from the charging station in each time slot. Thus, the charging amount in each time slot is bmax. We denote the cost by EC as EC .
2) Rolling online control algorithm (OA): EV i draws the optimal power bi (t) from the charging station, which is the optimal value of Problem 9. We denote the cost by OA as OA .
3) RL-based Adaptive Energy Management Algorithm (AEM) [42]: EV i draws the power bi (t) from the charging

10

0

12

15

20

25

30

35

Time(hour)

Fig. 7. Comparison of EV charging loads of 40 Type-1 EVs.

Amongst all tested algorithms, SCA achieves the lowest total load peak. We show the total loads about five algorithms of 40 Type-1 EVs in Fig. 6 and EV charging loads about five algorithms of 40 Type-1 EVs in Fig. 7. From numerical simulations, we see that the peak loads are 86 kW, 76.86 kW, 76.9 kW, 74.3 kW, 58.6 kW for of EC, OA, AEM, SCA, and CALC, respectively. CALC can reduce the peak load by 31.86%, 23.76%, 23.8%, 21.13%, compared with EC, OA, AEM, and SCA. SCA and CALC have a lower load fluctuation than EC, OA and AEM algorithms and CALC has less load fluctuation than SCA. For Type-2 EV, the total EV charging costs of EC, OA, AEM algorithms and SCA are $15.14, $13.47, $12.67, $11.51 and the EV charging costs of EC, OA, AEM algorithms are 23.97%, 15.55%, 9.16% higher than that of SCA, respectively. The EC algorithm is not an optimal algorithm where EVs are charged in a first-come-first-serve manner at the maximum rate. The OA algorithm solves the optimal EV charging over the rolling horizon but is not optimal in terms of long-term expected cost minimization. Q-learning method is used in the AEM algorithm with discrete-charging actions. For Type-1 EV, the total EV charging costs of EC, OA, AEM algorithms, SCA and CALC are $28.67, $19.45, $18.22, $16.01, and $16.90. The EV charging costs of EC, OA, and AEM algorithms are 24.03%, 21.49%, 13.80% higher than that of SCA. We see that the performance of SCA is better

9

-135

Instantaneous reward

-140

-145
-150 0

100

200 Episode

discount factor is 0.005 discount factor is 0.01 discount factor is 0.05

300

400

Fig. 8. The total moving reward versus discount factor.

-130

1400 1200 1000
800

AEM-33 AEM-3300 AEM-33000 SCA CALC

Running Time (s)

600

400

200

0

20

30

40

50

60

70

The number of EVs

Fig. 11. Comparison of the running time of AEM, SCA, and CALC during 48 time slots with different numbers of EVs.

Instantaneous reward

-140

-150
-160 0

100

200 Episode

Actor learning rate is 0.00001 Actor learning rate is 0.00005 Actor learning rate is 0.0001 Actor learning rate is 0.0005 Actor learning rate is 0.001

300

400

Fig. 9. Total community load versus actor learning rate.

0.6 0.55
0.5

AEM-33 AEM-3300 AEM-33000 SCA CALC

Cost ($)/EVs

0.45

0.4

0.35

20

30

40

50

60

70

The number of EVs

Fig. 10. Comparison of average costs per EV of AEM, SCA, and CALC with different numbers of EVs.

than those of CALC and AEM, because CALC optimizes the aggregate charging schedule for the EV fleet in the first stage and finds a close-to-optimal charging schedule for each EV in the second stage.
From Fig. 8, we can see that the discount factor can influence the convergence of SCA and CALC and it should be a very low value. The discount factor  is an important parameter to reduce TD error in critic process. We simulate SCA under a set of discount factors 0.005, 0.01, 0.05 and we can see that a low value can achieve a good performance. We can choose  = 0.01 to balance the reward and convergence, achieving both reasonably fast convergence and high reward. Furthermore, the actor learning rate a is also an important parameter to actor process. From Fig. 9, we can see that the

actor learning rate will influence the convergence of SCA and the update of policy. Large actor learning rates (e.g., 5 × 10-4 and 10-3) lead to big overshoots of the rewards and the steady-state reward depends on the configuration of the actor learning rates as well. We see that a = 10-4 achieves the best convergence performance and the highest reward among all the simulated rates.
Then we show the results of average costs per EV for all the tested algorithms in Fig. 10. There are three levels of discretecharging actions for AEM, where the charging action space is discretized into 33, 3300, and 33000 even slices, respectively. The average costs of all the tested algorithms steadily increase as the number of EVs increases. SCA achieves the lowest average cost among all simulated algorithms under all the scenarios of different numbers of EVs, validating the effectiveness of SCA. To show the time efficiency of SCA, we compare the performance of CALC with AEM algorithm and SCA and we can see that the total EV charging cost of CALC is 5.56% higher than that of SCA and 7.24% lower than that of AEM33 algorithm, where the cost of CALC algorithm is $16.90. When the charging action space is discretized into more slices, the charging cost will be lower. The running time of SCA and CALC are 476s and 90s, of which scenario is 40 EVs and 48 time slots. The running time of SCA and CALC during 48 time slots is shown in Fig. 11 and we can see that CALC has a significant higher time efficiency. Compared with SCA and CALC, AEM with more slices of discrete-charging actions has a larger computational complexity but a lower cost. When the charging action space is discretized into fewer slices, the computational complexity of AEM-33 is close to SCA. But if the charging action space of AEM is discretized into more slices, the computational complexity will increase rapidly as the number of EVs increases.
VI. CONCLUSION
In this paper, we investigate an offline EV charging scheduling problem, which minimizes the charging cost of community EVs without future information. We reformulate an online optimization problem for EV charging and develop two actorcritic learning algorithms (namely SCA and CALC) supporting continuous-charging action. Based on our proposed SCA, we

10

further develop a more computationally efficient CALC algo-
rithm by reducing the state dimension and improving the com-
putational efficiency. Simulation results show that SCA can outperform EC, OA, and AEM algorithms by 24.03%, 21.49%, 13.80% in terms of energy cost, while achieving a good convergence. The total EV charging cost of CALC is 5.56% higher than that of SCA but 7.24% lower than that of AEM. CALC has a significantly higher computational efficiency and
also achieves close-to-optimal performance compared with
SCA. For our future work, we will consider reinforcement
learning for the coordination of multiple charging stations.
We will also consider vehicle-to-grid services as an emerging
scenario for EV-grid interactions.
REFERENCES
[1] J. A. P. Lopes, F. J. Soares, and P. M. R. Almeida, "Integration of electric vehicles in the electric power system," Proceedings of the IEEE, vol. 99, no. 1, pp. 168­183, Jan 2011.
[2] Y. He, B. Venkatesh, and L. Guan, "Optimal scheduling for charging and discharging of electric vehicles," IEEE Transactions on Smart Grid, vol. 3, no. 3, pp. 1095­1105, Sept 2012.
[3] Y. Hu, C. Chen, J. He, B. Yang, and X. Guan, "Iot-based proactive energy supply control for connected electric vehicles," IEEE Internet of Things Journal, vol. 6, no. 5, pp. 7395­7405, 2019.
[4] N. Lu, N. Cheng, N. Zhang, X. Shen, and J. W. Mark, "Connected vehicles: Solutions and challenges," IEEE Internet of Things Journal, vol. 1, no. 4, pp. 289­299, 2014.
[5] L. Yu, T. Jiang, and Y. Zou, "Distributed online energy management for data centers and electric vehicles in smart grid," IEEE Internet of Things Journal, vol. 3, no. 6, pp. 1373­1384, Dec 2016.
[6] X. Qi, G. Wu, K. Boriboonsomsin, and M. J. Barth, "Development and evaluation of an evolutionary algorithm-based online energy management system for plug-in hybrid electric vehicles," IEEE Transactions on Intelligent Transportation Systems, vol. 18, no. 8, pp. 2181­2191, Aug 2017.
[7] Q. Kang, J. Wang, M. Zhou, and A. C. Ammari, "Centralized charging strategy and scheduling algorithm for electric vehicles under a battery swapping scenario," IEEE Transactions on Intelligent Transportation Systems, vol. 17, no. 3, pp. 659­669, 2016.
[8] P. Li, H. Wang, and B. Zhang, "A distributed online pricing strategy for demand response programs," IEEE Transactions on Smart Grid, vol. 10, no. 1, pp. 350­360, Jan 2019.
[9] Q. Kang, S. Feng, M. Zhou, A. C. Ammari, and K. Sedraoui, "Optimal load scheduling of plug-in hybrid electric vehicles via weightaggregation multi-objective evolutionary algorithms," IEEE Transactions on Intelligent Transportation Systems, vol. 18, no. 9, pp. 2557­2568, 2017.
[10] M. A. Quddus, O. Shahvari, M. Marufuzzaman, J. M. Usher, and R. Jaradat, "A collaborative energy sharing optimization model among electric vehicle charging stations, commercial buildings, and power grid," Applied Energy, vol. 229, pp. 841 ­ 857, 2018.
[11] Y. Liang, L. He, X. Cao, and Z. J. Shen, "Stochastic control for smart grid users with flexible demand," IEEE Transactions on Smart Grid, vol. 4, no. 4, pp. 2296­2308, Dec 2013.
[12] Z. Wen, D. O'Neill, and H. Maei, "Optimal demand response using device-based reinforcement learning," IEEE Transactions on Smart Grid, vol. 6, no. 5, pp. 2312­2324, Sept 2015.
[13] T. Liu, B. Tian, Y. Ai, and F. Wang, "Parallel reinforcement learningbased energy efficiency improvement for a cyber-physical system," IEEE/CAA Journal of Automatica Sinica, vol. 7, no. 2, pp. 617­626, 2020.
[14] J. Duan, H. Xu, and W. Liu, "Q-learning-based damping control of widearea power systems under cyber uncertainties," IEEE Transactions on Smart Grid, vol. 9, no. 6, pp. 6408­6418, Nov 2018.
[15] F. Ruelens, B. J. Claessens, S. Vandael, B. D. Schutter, R. Babuska, and R. Belmans, "Residential demand response of thermostatically controlled loads using batch reinforcement learning," IEEE Transactions on Smart Grid, vol. 8, no. 5, pp. 2149­2159, Sept 2017.
[16] K.-S. Hwang, Y.-J. Chen, W.-C. Jiang, and T.-F. Lin, "Continuous action generation of q-learning in multi-agent cooperation," Asian Journal of Control, vol. 15, no. 4, pp. 1011­1020, 2013. [Online]. Available: https://onlinelibrary.wiley.com/doi/abs/10.1002/asjc.614

[17] D. P. Bertsekas, "Feature-based aggregation and deep reinforcement

learning: a survey and some new implementations," IEEE/CAA Journal

of Automatica Sinica, vol. 6, no. 1, pp. 1­31, 2019.

[18] X. Jiang, J. Yang, X. Tan, and H. Xi, "Observation-based optimization

for pomdps with continuous state, observation, and action spaces," IEEE

Transactions on Automatic Control, vol. 64, no. 5, pp. 2045­2052, May

2019.

[19] V. Bui, A. Hussain, and H. Kim, "Double deep q -learning-based

distributed operation of battery energy storage system considering uncer-

tainties," IEEE Transactions on Smart Grid, vol. 11, no. 1, pp. 457­469,

Jan 2020.

[20] R. S. Sutton and A. G. Barto, "Reinforcement learning: An introduction,"

IEEE Transactions on Neural Networks, vol. 9, no. 5, pp. 1054­1054,

Sept 1998.

[21] R. Kamalapurkar, J. R. Klotz, and W. E. Dixon, "Concurrent learning-

based approximate feedback-nash equilibrium solution of n-player

nonzero-sum differential games," IEEE/CAA Journal of Automatica

Sinica, vol. 1, no. 3, pp. 239­247, 2014.

[22] X. Wang, D. Ding, H. Dong, and X. M. Zhang, "Neural-network-based

control for discrete-time nonlinear systems with input saturation under

stochastic communication protocol," IEEE/CAA Journal of Automatica

Sinica, vol. 8, no. 4, pp. 766­778, 2021.

[23] Z. Cao, Q. Xiao, and M. Zhou, "Distributed fusion-based policy search

for fast robot locomotion learning," IEEE Computational Intelligence

Magazine, vol. 14, no. 3, pp. 19­28, 2019.

[24] R. Lu and S. H. Hong, "Incentive-based demand response for

smart grid with reinforcement learning and deep neural network,"

Applied Energy, vol. 236, pp. 937 ­ 949, 2019. [Online]. Available:

http://www.sciencedirect.com/science/article/pii/S0306261918318798

[25] Y. Wei, F. R. Yu, M. Song, and Z. Han, "User scheduling and

resource allocation in hetnets with hybrid energy supply: An actor-

critic reinforcement learning approach," IEEE Transactions on Wireless

Communications, vol. 17, no. 1, pp. 680­692, Jan 2018.

[26] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller,

"Deterministic policy gradient algorithms," 31st International Confer-

ence on Machine Learning, ICML 2014, vol. 1, 06 2014.

[27] Z. Ma, D. S. Callaway, and I. A. Hiskens, "Decentralized charging con-

trol of large populations of plug-in electric vehicles," IEEE Transactions

on Control Systems Technology, vol. 21, no. 1, pp. 67­78, 2013.

[28] W. B. Powell, "On state variables, bandit problems and pomdps," 2020.

[29] E. Zivot and J. Wang, Modeling financial time series with S-Plus®.

Springer Science & Business Media, 2007, vol. 191.

[30] C. Gong, X. Wang, W. Xu, and A. Tajer, "Distributed real-time energy

scheduling in smart grid: Stochastic model and fast optimization," IEEE

Transactions on Smart Grid, vol. 4, no. 3, pp. 1476­1489, 2013.

[31] S. Karagiannopoulos, J. Gallmann, M. G. Vaya´, P. Aristidou, and

G. Hug, "Active distribution grids offering ancillary services in islanded

and grid-connected mode," IEEE Transactions on Smart Grid, vol. 11,

no. 1, pp. 623­633, 2020.

[32] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,

D. Silver, and K. Kavukcuoglu, "Asynchronous methods for deep rein-

forcement learning," in International conference on machine learning,

2016, pp. 1928­1937.

[33] K. Tanaka and A. A. Toda, "Discrete approximations of

continuous distributions by maximum entropy," Economics Letters,

vol. 118, no. 3, pp. 445 ­ 450, 2013. [Online]. Available:

http://www.sciencedirect.com/science/article/pii/S0165176512006568

[34] I. Grondman, L. Busoniu, G. A. D. Lopes, and R. Babuska, "A

survey of actor-critic reinforcement learning: Standard and natural policy

gradients," IEEE Transactions on Systems, Man, and Cybernetics, Part

C (Applications and Reviews), vol. 42, no. 6, pp. 1291­1307, 2012.

[35] M. Yan, G. Feng, J. Zhou, Y. Sun, and Y. Liang, "Intelligent resource

scheduling for 5g radio access network slicing," IEEE Transactions on

Vehicular Technology, vol. 68, no. 8, pp. 7691­7703, 2019.

[36] L. Gan, U. Topcu, and S. H. Low, "Optimal decentralized protocol for

electric vehicle charging," IEEE Transactions on Power Systems, vol. 28,

no. 2, pp. 940­951, May 2013.

[37] EPRI,

"Transportation

electrification:

A

tech-

nology

overview,"

2012.

[Online].

Available:

http://connection.ebscohost.com/c/articles/75649718

[38] "United states environmental protection agency," Emission Standards

Reference Guide for On-road and Nonroad Vehicles and Engines. [On-

line]. Available: http://www.epa.gov/otaq/standards/light-duty/udds.htm

[39] R. C. Leou, "Optimal charging/discharging control for electric vehicles

considering power system constraints and operation costs," IEEE Trans-

actions on Power Systems, vol. 31, no. 3, pp. 1854­1860, May 2016.

11

[40] Jianghuai iEV7L,

"Product

configuration

sheet,"

http://wap.jac.com.cn/iev7/6066.htm .

[41] W. Tang, S. Bi, and Y. J. Zhang, "Online coordinated charging deci-

sion algorithm for electric vehicles without future information," IEEE

Transactions on Smart Grid, vol. 5, no. 6, pp. 2810­2824, 2014.

[42] T. Liu, Y. Zou, D. Liu, and F. Sun, "Reinforcement learning of adaptive

energy management with transition probability for a hybrid electric

tracked vehicle," IEEE Transactions on Industrial Electronics, vol. 62,

no. 12, pp. 7837­7846, Dec 2015.

