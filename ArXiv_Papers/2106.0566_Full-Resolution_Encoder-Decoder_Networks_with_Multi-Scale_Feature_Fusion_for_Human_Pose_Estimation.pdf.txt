Full-Resolution Encoder­Decoder Networks with Multi-Scale Feature Fusion for Human Pose Estimation

Jie Ou
University of Electronic Science and Technology of China Chengdu, China oujieww6@gmail.com

Mingjian Chen
University of Electronic Science and Technology of China Chengdu, China starvingnow@163.com

Hong Wu
University of Electronic Science and Technology of China Chengdu, China hwu@uestc.edu.cn

arXiv:2106.00566v1 [cs.CV] 1 Jun 2021

ABSTRACT
To achieve more accurate 2D human pose estimation, we extend the successful encoder-decoder network, simple baseline network (SBN), in three ways. To reduce the quantization errors caused by the large output stride size, two more decoder modules are appended to the end of the simple baseline network to get full output resolution. Then, the global context blocks (GCBs) are added to the encoder and decoder modules to enhance them with global context features. Furthermore, we propose a novel spatial-attentionbased multi-scale feature collection and distribution module (SAMFCD) to fuse and distribute multi-scale features to boost the pose estimation. Experimental results on the MS COCO dataset indicate that our network can remarkably improve the accuracy of human pose estimation over SBN, our network using ResNet34 as the backbone network can even achieve the same accuracy as SBN with ResNet152, and our networks can achieve superior results with big backbone networks.
CCS CONCEPTS
· Computing methodologies  Activity recognition and understanding.
KEYWORDS
Encoder-Decoder, Full Output Resolution, Spatial Attention, MultiScale Feature Fusion, Human Pose Estimation.
ACM Reference Format: Jie Ou, Mingjian Chen, and Hong Wu. 2021. Full-Resolution Encoder­Decoder Networks with Multi-Scale Feature Fusion for Human Pose Estimation. In ACM Multimedia Asia (MMAsia '20), March 7­9, 2021, Virtual Event, Singapore. ACM, New York, NY, USA, 6 pages. https://doi.org/10.1145/3444685. 3446282
1 INTRODUCTION
2D human pose estimation, which aims to predict locations of human body joints in an image, is a very important and challenging
Corresponding authors
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. MMAsia '20, March 7­9, 2021, Virtual Event, Singapore © 2021 Association for Computing Machinery. ACM ISBN 978-1-4503-8308-0/21/03. . . $15.00 https://doi.org/10.1145/3444685.3446282

computer vision task. It has been studied for decades and the traditional methods mainly rely on hand-craft features [16]. With the application of convolutional neural networks, the accuracy of human pose estimation has been greatly improved [3, 17, 19, 31, 33].
Most multi-person pose estimation methods can be divided into two categories: top-down and bottom-up methods. The top-down methods [5, 21, 27, 33] first use a human detector to detect all person instances in an image, then perform single-person pose estimation for each of them respectively. The top-down methods is inefficient in case of crowds of persons because the run-time of the second step is proportional to the number of persons. On the contrary, the bottom-up methods [2, 12, 20, 24] detect the joints of all persons at once and then group the joints into each individual person by a grouping algorithm. The bottom-up methods suffer from high complexity of joint grouping step and the multi-scale problem in joint detection. Recently, most state-of-the-art results have been achieved by the top-down methods. In this paper, we follow the top-down strategy and develop an effective human pose estimation method.
Among the top-down methods, the simple baseline network [33] has achieved state-of-the-art results with a simple encoder-decoder architecture. Due to its simplicity and effectiveness, it can be used as a basic network to develop more advanced pose estimation methods. To achieve more accurate human pose estimation, we extend the simple baseline network in three ways. To reduce the quantization error caused by large output stride size, two more decoder modules are appended to the end of SBN to increase the output resolution to the same as input resolution. Then, the global context blocks (GCBs) are added to the encoder and decoder modules to enhance them with global context features. Furthermore, we develop a novel spatial attention based multi-scale feature collection and distribution module (SA-MFCD) to fuse the feature maps from different layers of encoders, refine them with spatial attention, and distribute them to different layers of decoders. Experimental results on the MS COCO dataset indicate that our networks can remarkably improve the accuracy of human pose estimation over SBN, our network with ResNet34 can even achieve the same accuracy as SBN with ResNet152, and our network can achieve superior results with big backbone networks.
2 RELATED WORK
2.1 Multi-Person Pose Estimation
Most multi-person pose estimation approaches can be classified into top-down and bottom-up approaches.
Top-Down Multi-Person Pose Estimation. Top-Down approaches first apply human detection to an image and then predicate

the joint locations of each human instance respectively. Convolutional Pose Machine (CPM) [31] employs a multi-stage architecture to combine the prediction of previous stages with the input to refine the pose estimation. Newell et al. [19] propose a stacked hourglass network to predict heatmap of 2D joint location. Mask-RCNN [7] extends faster-RCNN [25] to support keypoint predication and obtains very competitive results for human pose estimation. CPN [3] uses a GlobalNet to localize the "simple" keypoints and a RefineNet to address the "hard" keypoints with online hard keypoint mining. Xiao et al. [33] propose the simple baseline network for human pose estimation which has achieved state-of-the-art results. Recently, most state-of-the-art results have been achieved by top-down approaches.
Bottom-Up Multi-Person Pose Estimation. Bottom-Up approaches predict all joints first and then group them into different human poses. Some works [12, 13, 24] formulate joint grouping via a Linear Program. Cao et al. [2] use part affinity fields to encode the relation between different joints to help joint grouping. Newell et al. [18] use a detection heatmap and a tagging heatmap for supervising the joint detection and grouping, and then group joints with similar tags into an individual person. Papandreou et al. [20] develop a convolutional network to predict body joints and their relative displacements for joint grouping. Bottom-up approaches suffer from high complexity of joint grouping step and the multi-scale problem in joint detection. Although some efficient grouping methods have been proposed, bottom-up pose estimation approaches still can not win the competition against top-down approaches.
2.2 Attention Mechanism and Global Context Modeling
Attention mechanism is widely used in computer vision, which focuses on important features and suppresses unnecessary ones. It can be divided into two categories in terms of the dimension considered, channel-wise attention and spatial-wise attention. SENet [9] uses a bottleneck transform to generate channel attention weights for image classification. Zhu et al. [35] propose a channel attention mechanism in their residual attention block to refine features for image super-resolution. CBAM [32] and BAM [22] apply both channel and spatial attention modules to refine features for various visual tasks. Chu et al. [4] use attention models for human pose estimation and model the attention from different context and resolution.
Spatial attention mechanism has also been used to model global context. The non-local network [30] uses a query-specific spatial attention map for each query position to aggregate context features. GCNet [1] achieves more efficient global context modeling by using a query-independent attention map to generate a global context feature for all query positions and further re-calibrating the global context feature with channel attention.
3 METHODOLOGY
Our multi-person pose estimation approach uses faster-RCNN [25] to detect each person in an image, and then predicates pose for each person instance. Our network is an extension of the successful simple baseline network (SBN), as show in Fig. 1(a). Firstly, two more decoder modules are appended to the end of the SBN to

increase the output resolution. The encoder and decoder modules are then enhanced by the global context blocks (GCBs) [1] which compute global context features to enhance the original feature maps. Furthermore, a spatial attention based multi-scale feature collection and distribution (SA-MFCD) module is proposed to fuse feature maps from different layers of encoders, refine them with spatial attention and distribute them to different layers of decoders. More detailed information are given in the following subsections.
3.1 Extend Simple Baseline Network to Full Output Resolution
The encoder-decoder network structure is widely used in dense prediction, including semantic segmentation [6, 26], object detection [14], and human pose estimation [19, 33]. The encoder-decoder networks contain encoders that gradually reduce the resolutions of feature maps while capturing higher semantic information, and decoders that gradually recover the spatial resolutions. The output resolutions of most popular encoder-decoder networks for human pose estimation are smaller than their input resolutions. For example, the output resolutions of the Hourglass network [19] and the simple baseline network [33] are only 1/4 of their input resolution. The outputs are then resized to the input resolution by simple transformation, which introduces quantization errors. In [10], the common biased data processing for human pose estimation has been quantitatively analyzed, and it is found that the error of prediction is mainly due to the horizontal flip operation which is also related to the output stride size.
To alleviate this problem, we append two more decoder modules to the end of SBN to achieve the full output resolution. Following SBN, the encoder part of our network is a modified version of the ResNet [8], where the average pooling layer and last fully-connected layers are removed and only the convolutional layers are used. It includes a convolutional layer and a pooling layer followed by four stages of ResBlocks, both the first convolutional layer and pooling layer halves the resolution, and at the beginning of each Res-stage (except for the first one), the feature map resolution is also halved by a convolutional layer with strides=2, while the number of filters is doubled. Each decoder is composed of a Deconvolution (Transport Convolution) layer followed by batch normalization and ReLU. The decoder part of our network includes five decoder modules each of which double the resolution, and finally gets output with the same resolution as that of input. Compared to SBN, the two additional decoder modules bring more parameters and increase the model size. Our previous study indicates that reducing the channel sizes of the two additional decoder modules can make a good trade-off between the prediction accuracy and the model size. In our study, we set the channel size of the 4th decoder to 128 and that of the 5th decoder to 32 respectively. We name this basic network as the full resolution version of SBN (FR-SBN).
3.2 Enhance Encoders and Decoders by Global Context Blocks
The combination of global context information has proven useful to various visual recognition tasks. However, a convolution layer only models pixel relationship in a local neighborhood, and the long-range dependencies are mainly modeled by deeply stacking

Figure 1: (a) The full-resolution encoder-decoder network. The red arrow lines stand for the flows of spatial attention heatmaps (SAH) generated by GCBs, and black arrow lines are for feature maps. (b) The global context block (GCB).  and   are the input
and output feature maps of a GCB respectively. (c) The SA-MFCD Module.

convolution layers. Non-local operation [30] is proposed to model the long-range dependencies using self-attention mechanism [29]. The non-local network uses a query-specific attention map for each query position to aggregate context features and adds the context features to the feature of the corresponding query position. GCNet [1] is a more effective and efficient global context modeling approach, which uses a query-independent attention map to generate a global context feature and add it to all query positions.
In this paper, we apply the global context block (GCB) [1] to enhance the encoder and decoder modules. GCBs are introduced after the four Res-stages in the encoder part and the first three Decoder Models, as shown in Fig. 1(a). The structure of GCB is shown in Fig. 1(b). A 1 × 1 convolution is first used to generate a spatial attention heatmap (SAH) and followed by softmax to generate attention weights. Then attention pooling is used to obtain a global context feature. After that, a bottleneck transform is applied to it to capture its channel-wise dependencies. Finally, the global context feature is added to features of all positions. The SAH is also used in SA-MFCD which is introduced in the next subsection.

3.3 Spatial Attention Based Multi-Scale Feature Collection and Distribution
In previous encoder-decoder networks such as U-Net [26] and Hourglass [19], skip connections are used to preserve spatial information at each resolution. Skip connections are formed between the encoders and the decoders having the same output resolution to transfer the spatial information across the network for better localization. Through the skip connections, feature maps from the encoder are concatenated (or summed) with the corresponding feature maps of the decoder. For simplicity, the simple baseline network [33] does not use skip connections, which may limited its performance.
Different from the traditional skip connection, we propose a Spatial Attention based Multi-Scale Feature Collection and Distribution (SA-MFCD) module which merges multi-scale feature-maps from different layers of encoders, uses spatial attention to refine them, and finally distributes them to different layers of decoders. This scheme can bring multi-scale spatial and semantic information to the decoders for recovering high-resolution feature-maps. The details of this module is show in Fig. 1(c). This module has two different inputs: feature maps and spatial attention maps. The feature maps generated from the GCBs after Res-stage1, Res-stage2 and

Res-stage3 are sent to SA-MFCD, the feature maps with lower resolutions are up-sampled to match the highest one and concatenated together, and fused by a 3 × 3 convolution. The SAHs generated from the three GCBs are also resized and concatenated, and fused by a 3 × 3 convolution. The fused feature maps are then multiplied element-wise with the fused SAH. Finally, the refined feature maps are distributed to Decoder2, Decoder3 and Decoder4 after being resized to corresponding resolutions. The refined feature maps from SA-MFCD are concatenated with the output feature maps of the deconvolution layers in the corresponding decoders and fused by a 3 × 3 convolution to generate the outputs of the decoders. All of the convolution operations used here are followed by a batch normalization layer and ReLU function.
Via SA-MFCD, feature maps with different resolutions are fused together. The higher semantic information in lower resolution feature maps is combined with the more refined spatial information in the higher resolution feature maps. The spatial attention heatmaps (SAHs) with different resolutions can help extracting useful details and suppressing background information. And the fused feature maps are refined by the spatial attention mechanism to emphasize the positions related to pose estimation. When combining the refined feature maps from SA-MFCD model, the capacities of the corresponding decoders and thus the whole network are improved.

3.4 Ground-truth and Loss Function

Our human pose estimation network does not directly predict the coordinates, but do a pixel-level prediction by transforming joint position to heatmap. For each target-person patch cropped by human detection bounding-box, there might exist non-target person in it, we only generate ground-truth for the target one, by a 2D Gaussian function:

 (, ) = exp

-

( -  )2 + ( -  )2 2 2

,

(1)

where  is the heatmap for the -th joint(  {1, ...,  }), (,  ) is the coordinations of the -th joint, (, ) specifies a pixel location in the heatmap and the hyper-parameter  denotes a pre-fixed spatial variance, which is set to 8 and 12 for 256 × 192 and 388 × 288 input image size respectively.
As in [5, 33], we also choose Mean-Squared Error (MSE) as the loss function. The MSE loss function is given as:

Lmse =

1






 - ^ 

2
2,

(2)

 =1

where ^ refers to the predicted heatmap for the -th joint.

4 EXPERIMENTS
4.1 Experiment Setup
Dataset. MS COCO keypoint dataset [15] has more than 200k images in the wild, where more than 250k person instances are labeled with 17 human joints. We train our networks on the COCO 2017 training dataset, which has 57k images and 150k labeled person instances, no extra data involved. We evaluate our networks on the val2017 set (5k images) and test-dev2017 set (20k images) respectively.

Table 1: Comparison of networks with different additional components

Method

AP

SBN-34

69.8

FR-SBN

71.0

FR-SBN+GCB

71.6

FR-SBN+GCB+skip 71.7

FR-SBN+GCB+SA-MFCD 72.5

Performance Metrics. The OKS-based average precision and recall scores1 are used to evaluate the accuracy of keypoint localization. The object keypoint similarity (OKS) is defined as the distance between the predicted points and the ground-truth normalized by scale of the person. The mean average precision (AP) over 10 OKS thresholds is used as the main evaluation metric. In our experiments, we report AP (the mean of AP scores at OKS = 0.50, 0.55, ..., 0.90, 0.95), AP50 (AP at OKS = 0.50), AP75, AP for medium objects, AP for large objects, and AR (the mean of recalls).
Implementation Details. We implemented our networks via PyTorch [23] deep learning framework. ResNet [8] pre-trained on ImageNet is used to construct the encoder part in our network. The base learning rate is 1e-3, droped by 0.1 at step 90 and 120, with 140 epochs in total. We utilize Adam optimizer for training. Data augmentation is a very important training strategy, we adopt random horizontal flip, random rotation (-40 to +40) degrees and random scale (0.7 to 1.3).
4.2 Ablation Study
We first study the effectiveness of each component used to extend SBN, including the two more decoder modules appended to SBN (FR-SBN), the GCBs used to enhance encoders and decoders, and our SA-MFCD module. All versions of networks use ResNet34 to build their encoder part. We evaluate these versions of networks on the COCO val2017 set, all results are obtained over the input size of 256 × 192 and the same data augmentation strategies.
Table 1 shows the comparison results in detail. Our re-trained SBN-34 achieved an AP of 69.8. Comparing FR-SBN with SBN-34 (71.0 vs. 69.8), it can be seen that increase of the output resolution effectively improves AP by 1.2 points, which may due to the reduced quantization error. It can also be found that GCB can improve 0.6 AP points (71.6 vs. 71.0) over FR-SBN. SA-MFCD can further improve AP by 0.9 points (72.5 vs. 71.6), while the vanilla skip-connection (FR-SBN+GCB+skip) can only improve AP by 0.1 points (71.7 vs. 71.6). In total, our network (FR-SBN+GCB+SA-MFCD) gets 2.7 AP point improvements over the baseline (SBN-34).
4.3 Comparison with the state of the art methods
Our method is compared to the state-of-the-art methods on the validation and test set with two different input sizes respectively. Most of the compared methods are built on ResNet. PoseFix [17] uses CPN as the backbone, and UDP [10] uses SBN as the backbone. Table 2 gives the results on the validation set. It can be found
1 https://cocodataset.org/#keypoints-eval.

Table 2: Comparisons on the COCO validation set. OHKM = online hard keypoints mining

Method

Backbone Input size AP

Hourglass [19]

-

256 × 192 66.9

CPN [3]

Resnet50 256 × 192 68.7

CPN+OHKM [3] Resnet50 256 × 192 69.4

PoseFix [17]

CPN 256 × 192 72.1

SBN-50 [33] Resnet50 256 × 192 70.4

SBN-101 [33] Resnet101 256 × 192 71.4

SBN-152 [33] Resnet152 256 × 192 72.0

ours

Resnet34 256 × 192 72.5

SBN-50 [33] SBN-101 [33] SBN-152 [33]
ours

Resnet50 384 × 288 72.2 Resnet101 384 × 288 73.6 Resnet152 384 × 288 74.3 Resnet34 384 × 288 74.2

#Params
25M 27M 27M
34M 53M 68.6M 33M
34M 53M 68.6M 33M

GFLOPs
14.3G 6.2G 6.2G
8.9G 12.4G 15.7G 21.3G
20G 27.9G 35.3G 48.0G

Time
2.3ms 3.3ms 4.8ms 3.5ms
5.7ms 8.0ms 11.8ms 8.6ms

Table 3: Comparisons on the COCO test-dev set.  means use extra data

Method

Backbone Input size AP AP50 AP75 AP AP AR

Mask. [7] Res.50-FPN

-

63.1 87.3 68.7 57.8 71.4 -

Integral. [28] Res.101 256 × 256 67.8 88.2 74.8 63.9 74.0 -

CSANet [34] Res.101 256 × 192 72.3 91.2 80.2 69.3 77.6 79.1

CASNet [34] Res.152 256 × 192 72.8 91.4 80.9 69.8 78.3 79.6

UDP[10]

Res.152 256 × 192 72.9 91.6 80.9 70.0 78.5 78.4

HRNet[27] HRNet-w48 256 × 192 74.3 92.4 82.6 71.2 79.6 79.7

ours

Res.34 256 × 192 72.0 91.1 79.9 68.9 77.7 77.5

ours

Res.50 256 × 192 72.3 91.3 80.1 69.1 78.2 77.8

ours

Res.101 256 × 192 72.6 91.6 80.5 69.4 78.4 78.1

ours

Res.152 256 × 192 73.3 91.3 80.8 70.1 78.9 78.8

CPN [3]

Res.Inc. 384 × 288 72.1 91.4 80.0 68.7 77.2 78.5

CFN [11]

-

-

72.6 86.1 69.7 78.3 64.1 -

CPN (ens.) [3] Res.Inc. 384 × 288 73.0 91.7 80.9 69.5 78.1 79.0

SBN [33]

Res.152 384 × 288 73.7 91.9 81.1 70.3 80.0 79.0

PoseFix [17] Res.152 384 × 288 73.6 90.8 81.0 70.3 79.8 79.0

CSANet [34] Res.50 384 × 288 73.5 91.4 80.8 69.9 79.4 79.7

CSANet [34] Res.101 384 × 288 74.1 91.6 81.6 70.7 79.8 80.4

CSANet [34] Res.152 384 × 288 74.5 91.7 82.1 71.2 80.2 80.7

HRNet [27] HRNet-w48 384 × 288 75.5 92.5 83.3 71.9 81.5 80.5

UDP [10]

Res.50 384 × 288 72.5 91.1 79.7 68.8 79.1 77.9

UDP [10]

Res.152 384 × 288 74.7 91.8 82.1 71.5 80.8 80.0

ours

Res.34 384 × 288 73.7 91.7 81.1 70.4 79.7 78.9

ours

Res.50 384 × 288 73.9 91.7 81.3 70.4 80.0 79.0

ours

Res.101 384 × 288 74.3 92.0 82.0 71.0 80.4 79.6

ours

Res.152 384 × 288 74.9 92.2 82.4 71.6 80.9 80.2

that our method achieves the best results when the input size is 256 × 192. When the input size rises to 384 × 288, our network with ResNet34 can still achieve almost the same AP of SBN-152 (74.2 vs. 74.3) with only half of parameters and less inference time (8.6ms vs. 11.8ms), but the GFLOPs of our network is higher than that of SBN-152. This is because that our backbone is relatively small and the SA-MFCD module can run in parallel with the encoder-decoder network. The experimental results on the validation dataset indicate our network with ResNet34 can achieve the same accuracy as SBN with ResNet152, but with less inference time.

Table 3 gives the results on test-dev2017 set. When using the input of 256 × 192, our network outperforms CSANet [34] and UDP[10] with the same backbone networks. When using the input of 384 × 288, our network with ResNet34 outperforms CSANet and UDP with ResNet50. When ResNet50 is used as the backbone, our network is 0.4 points higher than CSANet and 1.4 points higher than UDP. When using ResNet101, our network reaches higher results than CSANet (74.3 vs. 74.1). When ResNet152 is used, our results (74.9) are also better than UDP (74.7) and CSANet (74.5). HRNet is the method outperforming our network at both input size

settings, which is a more advanced and elaborate network recently refreshing the state-of-the-art of many computer visual tasks.
5 CONCLUSIONS AND FUTURE WORKS
This paper has proposed an advanced encoder-decoder networks for human pose estimation, which improves the effectiveness of the simple baseline network. To the best of our knowledge, we are the first to develop network with full output resolution for human pose estimation, which can reduce the quantization error and recover more accurate spatial information. Global context blocks are also used to enhance the encoders and decoders. Furthermore, the proposed spatial attention based multi-scale feature collection and distribution module can fuse multi-scale feature information to boost the pose estimation. Our experimental results show that our networks can remarkably improve the prediction accuracy over SBN, and our network with big backbone network can achieve top ranked performance among recent works. In this paper, our network uses ResNet to build the encoder part, and it is worth trying some more advanced backbones in the future.
6 ACKNOWLEDGEMENT
This work was supported in part by the National Natural Science Foundation of China (U20B2063), the Sichuan Science and Technology Program, China (2020YFS0057), and the Fundamental Research Funds for the Central Universities (ZYGX2019Z015).
REFERENCES
[1] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han Hu. 2019. Gcnet: Nonlocal networks meet squeeze-excitation networks and beyond. In Proceedings of the IEEE International Conference on Computer Vision Workshops. 0­0.
[2] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. 2017. Realtime multiperson 2d pose estimation using part affinity fields. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 7291­7299.
[3] Yilun Chen, Zhicheng Wang, Yuxiang Peng, Zhiqiang Zhang, Gang Yu, and Jian Sun. 2018. Cascaded pyramid network for multi-person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 7103­7112.
[4] Xiao Chu, Wei Yang, Wanli Ouyang, Cheng Ma, Alan L Yuille, and Xiaogang Wang. 2017. Multi-context attention for human pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 1831­1840.
[5] Hao-Shu Fang, Shuqin Xie, Yu-Wing Tai, and Cewu Lu. 2017. Rmpe: Regional multi-person pose estimation. In Proceedings of the IEEE International Conference on Computer Vision. 2334­2343.
[6] Jun Fu, Jing Liu, Yuhang Wang, Jin Zhou, Changyong Wang, and Hanqing Lu. 2019. Stacked deconvolutional network for semantic segmentation. IEEE Transactions on Image Processing (2019).
[7] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. 2017. Mask r-cnn. In Proceedings of the IEEE International Conference on Computer Vision. 2961­2969.
[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 770­778.
[9] Jie Hu, Li Shen, and Gang Sun. 2018. Squeeze-and-excitation networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 7132­ 7141.
[10] Junjie Huang, Zheng Zhu, Feng Guo, and Guan Huang. 2019. The Devil is in the Details: Delving into Unbiased Data Processing for Human Pose Estimation. arXiv preprint arXiv:1911.07524 (2019).
[11] Shaoli Huang, Mingming Gong, and Dacheng Tao. 2017. A coarse-fine network for keypoint localization. In Proceedings of the IEEE International Conference on Computer Vision. 3028­3037.
[12] Eldar Insafutdinov, Mykhaylo Andriluka, Leonid Pishchulin, Siyu Tang, Evgeny Levinkov, Bjoern Andres, and Bernt Schiele. 2017. Arttrack: Articulated multiperson tracking in the wild. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 6457­6465.

[13] Eldar Insafutdinov, Leonid Pishchulin, Bjoern Andres, Mykhaylo Andriluka, and Bernt Schiele. 2016. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision. Springer, 34­50.
[14] Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. 2017. Feature pyramid networks for object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2117­ 2125.
[15] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In European Conference on Computer Vision. Springer, 740­755.
[16] Zhao Liu, Jianke Zhu, Jiajun Bu, and Chun Chen. 2015. A survey of human pose estimation: the body parts parsing based methods. Journal of Visual Communication and Image Representation 32 (2015), 10­19.
[17] Gyeongsik Moon, Ju Yong Chang, and Kyoung Mu Lee. 2019. Posefix: Modelagnostic general human pose refinement network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Long Beach, 7773­7781.
[18] Alejandro Newell, Zhiao Huang, and Jia Deng. 2017. Associative embedding: End-to-end learning for joint detection and grouping. In Advances in Neural Information Processing Systems. 2277­2287.
[19] Alejandro Newell, Kaiyu Yang, and Jia Deng. 2016. Stacked hourglass networks for human pose estimation. In European Conference on Computer Vision. Springer, 483­499.
[20] George Papandreou, Tyler Zhu, Liang-Chieh Chen, Spyros Gidaris, Jonathan Tompson, and Kevin Murphy. 2018. Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model. In Proceedings of the European Conference on Computer Vision. 269­286.
[21] George Papandreou, Tyler Zhu, Nori Kanazawa, Alexander Toshev, Jonathan Tompson, Chris Bregler, and Kevin Murphy. 2017. Towards accurate multi-person pose estimation in the wild. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 4903­4911.
[22] Jongchan Park, Sanghyun Woo, Joon-Young Lee, and In So Kweon. 2018. Bam: Bottleneck attention module. arXiv preprint arXiv:1807.06514 (2018).
[23] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019. PyTorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems. 8024­8035.
[24] Leonid Pishchulin, Eldar Insafutdinov, Siyu Tang, Bjoern Andres, Mykhaylo Andriluka, Peter V Gehler, and Bernt Schiele. 2016. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 4929­4937.
[25] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in Neural Information Processing Systems. 91­99.
[26] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 234­241.
[27] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. 2019. Deep high-resolution representation learning for human pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 5693­5703.
[28] Xiao Sun, Bin Xiao, Fangyin Wei, Shuang Liang, and Yichen Wei. 2018. Integral human pose regression. In Proceedings of the European Conference on Computer Vision. 529­545.
[29] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems. 5998­6008.
[30] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. 2018. Non-local neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 7794­7803.
[31] Shih-En Wei, Varun Ramakrishna, Takeo Kanade, and Yaser Sheikh. 2016. Convolutional pose machines. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 4724­4732.
[32] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. 2018. Cbam: Convolutional block attention module. In Proceedings of the European Conference on Computer Vision. 3­19.
[33] Bin Xiao, Haiping Wu, and Yichen Wei. 2018. Simple baselines for human pose estimation and tracking. In Proceedings of the European Conference on Computer Vision. 466­481.
[34] Dongdong Yu, Kai Su, Xin Geng, and Changhu Wang. 2019. A Context-and-Spatial Aware Network for Multi-Person Pose Estimation. arXiv preprint arXiv:1905.05355 (2019).
[35] Leilei Zhu, Shu Zhan, and Haiyan Zhang. 2019. Stacked U-shape networks with channel-wise attention for image super-resolution. Neurocomputing 345 (2019), 58­66.

