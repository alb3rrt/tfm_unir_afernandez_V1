Adversarial VQA: A New Benchmark for Evaluating the Robustness of VQA Models
Linjie Li1, Jie Lei2, Zhe Gan1, Jingjing Liu1 1Microsoft 2UNC Chapel Hill
{lindesy.li, zhe.gan, jingjl}@microsoft.com jielei@cs.unc.edu

arXiv:2106.00245v1 [cs.CV] 1 Jun 2021

Abstract
With large-scale pre-training, the past two years have witnessed significant performance boost on the Visual Question Answering (VQA) task. Though rapid progresses have been made, it remains unclear whether these stateof-the-art (SOTA) VQA models are robust when encountering test examples in the wild. To study this, we introduce Adversarial VQA, a new large-scale VQA benchmark, collected iteratively via an adversarial human-and-modelin-the-loop procedure. Through this new benchmark, we present several interesting findings. (i) Surprisingly, during dataset collection, we find that non-expert annotators can successfully attack SOTA VQA models with relative ease. (ii) We test a variety of SOTA VQA models on our new dataset to highlight their fragility, and find that both largescale pre-trained models and adversarial training methods can only achieve far lower performance than what they can achieve on the standard VQA v2 dataset. (iii) When considered as data augmentation, our dataset can be used to improve the performance on other robust VQA benchmarks. (iv) We present a detailed analysis of the dataset, providing valuable insights on the challenges it brings to the community. We hope Adversarial VQA can serve as a valuable benchmark that will be used by future work to test the robustness of its developed VQA models.1
1. Introduction
Visual Question Answering (VQA) [4] is a task that requires the model to provide a free-form or open-ended answer given an image and a question about it. A successful VQA system can be applied to real-life scenarios, such as a chatbot that assists visually impaired people. In these applications, it is desirable that VQA models are robust to diverse question types, ranging from recognition to reasoning,
1Our dataset is publicly available at https://adversarialvqa. github.io/.

Figure 1: Illustration of data collection examples. The workers try to attack the VQA model for at most 5 times by asking hard questions about the image, and succeeds at the last attempt. Green (red) indicates a correct (wrong) answer.
and are able to answer the questions correctly and faithfully based on the evidence in the image.
While model performance on the popular VQA dataset [13] has been advanced rapidly in recent years [4, 18, 3, 48, 8, 41, 52], with better visual representations [17, 52], more sophisticated model designs [11, 26], large-scale pre-training [29, 40] and adversarial training [10], today's VQA models are still far from being robust. Existing works have studied the robustness and sensitiveness of VQA models to visual content manipulations [1], answer distribution shift [2], linguistic variations in input questions [38] and reasoning capabilities [12, 37]. However, recent robust VQA benchmarks are mostly constructed with several limitations: (i) designed with heuristic rules [12, 2, 1]; (ii) focused on a single type of robustness [37, 38, 12]; and (iii) based on VQA v2 [13] images (or even questions), which state-of-the-art VQA models are trained on [12, 2, 1, 37, 38]. The questions [12, 16] or images [1] can also be synthesized rather than written by human.
In addition, current data collection procedures on VQA

benchmarks are often static, meaning that the examples in these datasets do not evolve. For example, model accuracy on VQA v2 has been improved from 50 [4] to 76 [52] since its creation. Similarly, on robust VQA benchmarks, a recent study [27] has also found that pre-trained models can greatly lift up current state of the art. However, it is still unclear whether such a high performance on the VQA task can be maintained when encountering examples in the wild.
To remedy these limitations, we collect a new largescale VQA benchmark, Adversarial VQA (AVQA), dynamically with Human-And-Model-in-the-Loop Enabled Training (HAMLET) [45], on images from different domains, including web images from Conceptual Captions [39], usergenerated images from Fakeddit [31] and movie images from VCR [50]. Our data collection is iterative and can be never-ending. We first collect examples from human annotators that current best models cannot answer correctly (Figure 1). These newly annotated examples expose the models' weaknesses, and are then added to the training data for training a stronger model. The re-trained model can be subjected to the same process, and the collection can last for several rounds. After each round, we train a new model and set aside a new test set. In this way, not only is the resultant dataset harder than existing benchmarks, but this process also yields a "moving post" dynamic target for VQA systems, rather than a static benchmark that will eventually saturate.
Our dataset enables quantitative evaluation of robustness of VQA models across multiple dimensions. First, we provide the first study on the vulnerability of VQA models under adversarial attacks by human annotators. Second, we benchmark several state-of-the-art VQA models to highlight the fragility of VQA models on our proposed dataset. We observe a significant drop in performance when compared to that on VQA v2 and other robust VQA benchmarks, which reinforces our belief that existing VQA models are not robust enough. Meanwhile, this also demonstrates the transferability of these adversarial examples ­ examples collected using one set of models are also challenging for other models. Third, as we do not constrain the annotators to only ask a single type of questions, or questions belong to a single type of robustness, our analyses show that state-of-the-art models suffer from various types of questions, especially on counting and reasoning.
Our main contributions are summarized as follows. (i) We introduce a new VQA benchmark to evaluate the robustness of state-of-the-art VQA models, which is dynamically collected with a Human-and-Model-in-the-Loop procedure. (ii) Despite the rapid advances in model performance on VQA v2 and robust VQA benchmarks, the evaluation of state-of-the-art models on our dataset shows that they are far from being robust. Surprisingly, they are extremely easy to be attacked by human annotators, who can succeed within

2 trials on average. (iii) We provide a detailed analysis of the collected data that sheds light on the shortcomings of current models, categorize the data by question type to examine weaknesses, and demonstrate good performance on other robust VQA benchmarks.
2. Related Work
Robust VQA Benchmarks There has been a growing interest in building new benchmarks to study the robustness of VQA models. VQA-CP [2], the first robust VQA benchmark constructed via reshuffling examples in VQA v2 [13], is proposed to evaluate question-oriented language bias in VQA models. GQA-OOD [21] improves from VQA-CP, proposes to evaluate the performance differences between in-distribution and out-of-distribution split. Besides language bias, VQA-Rephrasings [38] exposes the brittleness of VQA models to linguistic variations in questions by collecting human-written rephrasings of VQA v2 questions; Causal VQA [1] studies robustness against semantic image manipulations, and tests for prediction consistency to questions on clean images and corresponding edited images. Further studies investigate robustness against reasoning. For instance, [37] collects perception-related sub-questions per question for a new reasoning split of VQA dataset. [12] tests model's ability to logical reasoning through logical compositions of yes/no questions in VQA v2. GQA [16] provides large-scale rule-based questions from ground-truth scene graphs, that can test VQA model's ability on positional reasoning and relational reasoning.
Despite the continuous effort in evaluating robustness of VQA models, these works mostly focus on a single type of robustness, and are based on the original VQA v2 dataset via either another round of question collection given the existing VQA examples, or automatic transformation or manipulation of current examples. In comparison, we use different image sources, and collect a new challenging VQA benchmark by allowing human annotators to directly attack current state-of-the-art VQA models.
Model-in-the-Loop Data Collection Dataset collection with a model-in-the-loop setting has received increasing attention in recent years in the NLP community. In this setting, models are used in the collection process to identify wrongly predicted, thus more challenging examples. These models are used either as a post-processing filter [51, 5] or directly during annotation [47, 33, 5]. In ANLI [33], the model-in-the-loop strategy is extended to a Human-AndModel-in-the-Loop Enabled Training (HAMLET) setting, where the data collection happens in multiple rounds, and in each round, the models are updated to stronger versions by training with examples collected from previous rounds. The goal of ANLI is to create a natural language inference (NLI) dataset that can grow along with the rapid ad-

1. Write

question

4. Re-train model for next round Train

Dev

Test

Image + Question + Image + Question +

Model answer

Human answers

3. Judge

model answer

2. Predict

3.2 Failed, restart writting

3.1 A successful attack, verify question, collect more answers

Figure 2: Overview of our adversarial data collection process, for a single round. The process can be considered as a game played by two parties, a human annotator and a well-trained model. Given an image, the annotator tries to attack the model by writing a tricky question (step 1), the model then predicts an answer to the question (step 2). Next, the human annotator judges the correctness of the model answer (step 3). If the model answer is judged as "definitely wrong" , meaning the attack is successful, then we verify the question and collect more answers for it (step 3.1). Otherwise, the attack is failed, the annotator needs to write another question to attack the model (step 3.2). The dev and test splits contain only successfully attacked questions, while train split contains also the failed questions.

vance of model capabilities [9, 28, 46, 23]. In contrast to static datasets that will eventually saturate as models become stronger, datasets created with the HAMLET procedure are dynamic ­ if the test set saturates with a more powerful model, one can use this more powerful model to assist the collection of a new set of difficult examples, leading to a never-ending challenge for the community. Meanwhile, the adversarial nature of the HAMLET procedure also helps to identify the weaknesses and vulnerabilities of existing models, and the biases or annotation artifacts [14, 34, 25] in existing datasets [6, 44, 25]. Beyond its application to the NLI task, the HAMLET procedure is also proved to be useful in collecting more challenging examples for the video-andlanguage future prediction task [25].
3. Adversarial VQA Dataset
In this section, we introduce the AVQA dataset in detail. Sec. 3.1 explains the data collection pipeline. Sec. 3.2 shows data statistics and comparison to other datasets.
3.1. Data Collection Pipeline
The HAMLET data collection procedure can be considered as a game played by two parties: a human annotator and a well-trained model. The human annotator competes against the model as an adversary and tries to design adversarial examples to identify its vulnerabilities. After collecting enough examples, the model augments its training with the collected data to defend similar attacks. For VQA, we define the adversarial example as an adversarial question on a natural image that the model answers incorrectly.
As shown in Figure 2, given an image, the human annotator tries to write a tricky question that the VQA model may fail. Once the question is submitted, an online model prediction will be displayed immediately to the workers. The model answer is then judged by the same annotator as either "definitely correct", "definitely wrong", or "not sure".

If the model prediction is "definitely wrong", then the attack is successful, we further ask the annotator to provide a correct answer. Otherwise, the annotator needs to write another question until the model predicts a wrong answer, or the number of tries exceeds a threshold (5 tries). To avoid obviously invalid questions caused by annotator taking shortcuts (e.g., untruthful judgement on model predictions, questions irrelevant to the image content), we also launch an answer annotation task. Successfully attacked questions are provided to 9 other annotators to collect extra answers, as well as their confidence level ("confident", "maybe" and "not confident") to their answer. The questions that receive less than 6 "confident" answers and have no agreement in answers among 10 annotators, are removed during post-processing. In the end, each image is presented to 3 workers for question collection, and each image-question pair is shown to 10 annotators for answer collection.
This procedure can be continuously deployed for multiple rounds. At each round, we strengthen the models as we re-train them with extra data collected from previous rounds. This "dynamic" evolution of attacked models allows collection of "harder" questions in the later rounds. In our setup, we launch the data collection for 3 rounds on Amazon Mechanical Turk. However, this data collection can be a never-ending process, as we can always replace the attacked model with stronger model trained on newly collected data or better architectures that appear in the future.
Round 1 (R1) For the first round, we employ VQA models trained on examples from VQA v2 [13] and VGQA [22] as our starting point. To avoid the collected questions overfitting to the vulnerabilities of a single model or a single architecture, for each user question, we randomly sample one model from LXMERT [41], UNITER-B [8] and UNITERL [8] as the attacked model to generate the answer. We choose LXMERT and UNITER as representatives of twostream and single-stream pre-trained V+L models, due to

Dataset

Image Source #Img IsCollected

#IQ Total/Verified

Model error rate

#Tries

Time (sec.)

Total/Verified Mean/Median per verified ex.

Data Split Train/Dev/Test

Previous Robust VQA Datsets

VQA-Reph.

-

162K/-

-

VQA-Intro.

-

238K/-

-

VQA-LOL Comp.

COCO

-

×

1.25M/-

-

VQA-LOL Supp.

-

×

2.55M/-

-

VQA-CP v2

-

×

-/-

-

IV-VQA CV-VQA

COCO

357K 18.0K

× ×

376K/12.7K/-

-

-

-

-/162K/-

-

-

222K/-/93K

-

-

916M/43K/291K

-

-

1.9M/9k/669K

-

-

438K/-/220K

-

-

257K/11.6K/108K

-

-

8.5K/0.4K/3.7K

Ours

R1 R2 R3 AVQA

CC CC Various Various

13.7K 13.1K 11.1K 37.9K

93.1K/45.6K 70.4K/37.8K 60.4K/30.0K 223.9K/113.4K

48.9%/35.2% 56.1%/49.0% 49.8%/36.3% 50.7%/40.0%

1.6/1 1.5/1 1.6/1 1.6/1

71.0

53.6K/3.3K/10.0K

54.2

42.8K/2.7K/8.3K

57.3

35.3K/2.2K/6.6K

61.6

131.7K/8.2K/24.9K

Table 1: Data statistics. `Model error rate' is the percentage of examples that the model gets wrong; `Verified' is the questions with 10 answer annotations. Images for R3 are from various domains: Conceptual Captions (CC) [39], VCR [50] and Fakeddit [31]. We compare our dataset against previous robust VQA datasets, based on COCO [7] images. For number of image-question pairs (#IQ) and images (#Img), we only report the number of new examples generated/collected in each dataset.  indicates that the images are not natural, but edited. `IsCollected' indicates whether the data is collected via crowdsourcing.

their strong performance on VQA v2. We use images sampled from Conceptual Captions [39] for annotation. In total, we collected 38.7K verified2 questions and 28.2K unverfied questions over 13.7K images, and split the verified examples into 60%/10%/30% for train/dev/test splits. All unverified examples are also added into the training split.
Round 2 (R2) For the second round, we re-train our models with questions from VQA v2, VGQA and R1's train split, and select the best model checkpoints of LXMERT, UNITER-B and UNITER-L based on R1's dev set. Similarly, we randomly sample one model at a time for the workers to attack. A new set of non-overlapping Conceptual Captions images are used. In total, we collected 23.5K verified questions and 19.3K unverified question over 13.1K images, and split the data in a similar manner to R1.
Round 3 (R3) For the third round, we include more diverse images from different domains: (i) web images from Conceptual Captions [39], (ii) user-generated images from Fakeddit [31], and (iii) movie frame images from VCR [50]. The attacked model is still randomly sampled from LXMERT, UNITER-B and UNITER-L, but we add the training set from R1 and R2 to the training data.
Summary Finally, combining data collected in R1, R2 and R3 gives our proposed AVQA dataset. In the end, we collected 223.9K questions over 37.9K images, with 131.7K/8.2K/24.9K images in train/dev/test split.
3.2. Comparison to Other Datasets
Our Adversarial VQA dataset sets a new benchmark to evaluate the robustness of VQA models. It improves upon existing robust VQA benchmarks in several ways. First, the dataset by design is collected to be more difficult than
2Verified questions are all successfully attacked questions.

previous datasets. During collection, we do not constrain the worker to ask questions that only fall into a single robustness type (Sec. 4.3). As a result, our dataset is helpful in defending model robustness against several robust VQA benchmarks (Sec. 4.2). Second, most robust VQA datasets are based on VQA v2 validation set, which state-of-the-art models used for training or hyper-parameter tuning. Thus, it is difficult to analyze the robustness of the best-performing models due to this data leakage. Our dataset is built on non-overlapping images from diverse domains, which naturally resolves it. Lastly, our dataset is composed of humanwritten questions on natural images, rather than rule-based questions in [12, 16] or manipulated images in [1]. A detailed comparison on data statistics are shown in Table 1.
Our work is inspired by ANLI [33]. While ANLI focuses on the pure text task of natural language inference, our work targets at the multi-modal task of visual question answering. However, due to the open-ended nature of VQA problem, the construction of AVQA is more challenging. Instead of giving the worker a target label when collecting adversarial questions, we ask the worker to judge whether the model prediction is correct first, then provide a ground-truth answer. Our verification process is also different from ANLI. In order to evaluate model performance under the same criteria as VQA v2 [13], we collect 10 answers from workers in total. Unlike the observations shown in ANLI, where the adversarial robustness of NLI models can be improved to a large extent through data augmentation of ANLI, our analysis on AVQA in Sec. 4 will show that it is more difficult to defend against adversarial attacks for VQA models.
Data statistics The data statistics are summarized in Table 1. The number of examples we collected per image slightly decreases, starting with approximately 6.8 questions/image for R1, to around 5.4 for R2 and R3. We sus-

Model BUTD UNITER-B UNITER-L LXMERT

Training Data
VQA v2 +VGQA ALL VQA v2 +VGQA +R1 +R1+R2 ALL VQA v2 +VGQA +R1 +R1+R2 ALL VQA v2 +VGQA +R1 +R1+R2 ALL

R1
dev/test 20.80/19.28 24.96/22.11 20.60/17.91 26.03/22.94 26.60/24.76 26.85/24.93 25.04/23.72 29.31/26.63 30.13/28.15 30.80/28.45 19.76/18.15 23.89/22.65 26.76/24.86 26.35/24.55

R2
dev/test 18.77/18.85 22.62/22.78 17.86/18.55 17.30/17.36 23.21/23.86 23.38/23.92 17.82/17.49 19.34/18.66 23.11/23.54 22.95/23.11 18.98/18.79 19.01/17.91 23.28/24.11 23.84/24.02

R3
dev/test 21.17/21.31 24.03/23.70 19.45/20.20 19.41/20.23 18.60/19.09 23.76/23.02 18.86/19.34 19.53/18.30 16.09/16.47 23.75/21.88 20.75/20.26 21.47/20.85 19.16/18.93 24.00/22.90

AVQA
dev/test 20.18/19.60 23.94/22.71 19.39/18.66 21.48/20.37 23.58/23.14 24.94/24.14 21.11/20.54 23.60/21.93 24.46/23.84 26.11/25.07 19.72/18.86 21.64/20.58 23.80/23.23 24.94/23.98

VQA v2
test-dev 67.60 67.52 72.70 72.98 72.75 72.66 73.82 73.89 73.77 74.15 72.31 72.51 72.61 72.42

(v2, AVQA)
test-dev, test 48.00 44.81 54.04 52.61 49.61 48.52 53.28 51.96 49.93 49.08 53.45 51.93 49.38 48.44

Table 2: Model Performance of various models under different settings. AVQA / ALL refers to R1+R2+R3 / VQA v2+VGQA+AVQA.

pect that the annotators learn to identify model vulnerabilities more rapidly than the models learn to defend itself from the adversarial examples. We provide analyses in Sec. 4.3 and 4.1 for further investigation.
For each round, we report the model error rate, both on verified and all examples. The model error rate reported under "Total" captures the percentage of examples where the writer disagrees with the model's answer during question collection, but where we are not yet sure that the example is correct. The verified model error rate is the percentage of model errors from examples that we further collected 9 additional answers from other workers. We observe an increase in model error rate from R1 to R2. Assuming constant image domain difficulty in R1 and R2, the higher model rate suggests that the models in the later rounds are not significantly stronger, or the annotators are getting better at fooling the state-of-the-art models. In R3, where we included images from more diverse domains, the model error rate decreases from 49.0% to 36.3%. We suspect it is because the movie images from VCR are mostly humancentric, which is commonly observed in COCO.
We also report the average number of attempts ("#Tries" in Table 1) that a worker needed to complete the annotation process for each image, that is, to successfully attack the model or exceed the limits on number of tries. Surprisingly, although the VQA models used in the later rounds are trained with more data, the number of tries needed to successfully attack them does not increase. On average, it takes less than 2 tries to successfully attack a VQA model. Similarly, the average time needed per successful attack decreases by 15 seconds as data collection progresses.
4. Results and Analysis
In this section, we conduct extensive experiments to study AVQA dataset. Specifically, we evaluate differ-

ent model architectures with different modality inputs on AVQA in Sec. 4.1, examine how AVQA can help on other popular robust VQA benchmarks in Sec. 4.2, explore the question types that can fool the models in Sec. 4.3 and compare our data collection with automatic adversarial attack methods both qualitatively and quantitatively in Sec. 4.4.
4.1. Model Evaluation
Table 2 reports the main results. In addition to UNITERB, UNITER-L [8] and LXMERT [41], we also include BUTD [3] as an example of a task-specific model with different model architecture, prior to the large-scale pretraining era. We show performance on the AVQA test sets per round, the total AVQA test set and VQA v2 test-dev set. Our key observations are summarized as follows.
Adversarial examples are transferrable across model architectures. Both LXMERT and UNITER are variants of Transformer [43] architecture. We use BUTD as an example to investigate whether the adversarial examples are transferrable among the three models. The 20 performance of BUTD (trained on VQA v2+VGQA) on test set of each round indicates that workers did not find vulnerabilities specific to a single model architecture, but generally applicable ones across different model architectures.
Difficulty level of rounds does not decrease. Under the same training data, we observe that the model achieves comparable or even lower performance on later rounds. As aforementioned in data statistics, the increased model error rates and the decreased average tries annoators needed suggest that the later rounds contain more difficult examples.
Training with more rounds help defending robustness... Generally, our results indicate that training on more rounds improves model performance.
...but data augmentation alone is not effective. To investigate how much improvements are from adversarial exam-

Data

R1 R2 R3

Verified 25.63 22.84 22.36 Combined 26.85 22.82 23.70

Table 3: Comparison of verified and combined data. Results are reported on dev split from UNITER-B trained on training data of

each round, VQA v2 and VGQA.

Training

Lang. R1 R2 R3 VQA v2

Data VQA v2+VG AVQA-only All VQA v2+VG AVQA-only All

Only      

test 17.91 25.66 24.93 17.82 20.37 19.75

test 18.55 24.91 23.92 17.03 21.49 20.75

test 20.20 25.06 23.02 21.19 22.92 22.31

test-dev 72.70 59.99 72.66 45.81 38.21 46.23

(a) Language-only Model Performance.

Model

AVQA dev

VQA-CP v2 test

BUTD + [42] UNITER-B + [42]

23.94 21.84 24.94 23.95

40.62 (38.82 [42]) 43.96
47.02 (46.93 [27]) 47.12

(b) Model Performance with a VQA-CP baseline from [42].

Table 4: Analysis on language bias.

ples. We show comparison of UNITER-B results on verified and combined data in Table 3. In addition to verified data, the combined data include examples that the worker thinks model have answered correctly. Even with almost doubled data size, results on combined data are not significantly better. This implies that simply training on more examples that model correctly answers can hardly help a model be robust to adversarial attacks.
Large model does not have a clear advantage. Although outperforming UNITER-B and LXMERT on R1, UNITERL does not show a clear advantage over R2 and R3. Overall, these three models achieve similar performance across rounds and on AVQA. When trained with "All" data, the performance gain from UNITER-L over BUTD is only +2.36 on AVQA, even though UNITER-L is pre-trained with extensive amount of image-text pairs.
In the following subsections, we dive deeper into the key factors behind the low performance of state-of-the-art models on AVQA and try to answer the following questions.
Q1: Is the language in AVQA biased? Starting from VQACP [2], concerns have been raised about the propensity of models to pick up on spurious artifacts that are present just in the co-occurrence of question-answer pairs, without actually paying attention to the image content. We compare full models trained with both images and questions to models trained only on questions by zeroing out image features in Table 4a. The results show that language-only models perform poorly on AVQA, and similarly on VQA v2. Language-only model performance decreases over rounds

Model

Training Data

AVQA VQA v2 test test-dev

UNITER-B

VQA v2 +VGQA ALL

18.66 24.14

72.70 72.66

ClipBERT

VQA v2 +VGQA 20.82

ALL

24.25

69.08 69.17

VILLA-B

VQA v2 +VGQA 19.45

ALL

26.10

73.37 74.28

Table 5: Evaluation of AT-base method VILLA [10] and grid fea-

ture based method ClipBERT [24]. `AVQA' refers to R1+R2+R3.

for AVQA. However, UNITER-B is not much better than language-only on AVQA. Obviously, without manual intervention, some bias remains in how annotators phrase questions. For example, there might be more counting questions with answers other than 2, which is the majority answer in VQA v2. Therefore, models trained on AVQA only performs slightly higher for both UNITER-B and Languageonly model. However, we also observe the significant drop in VQA v2 performance is out of proportion to the slight performance improvement on AVQA.
We further investigate if the low performance is due to the difference in answer distribution between training and testing split. Due to the large number of answer candidates (more than 3000 for VQA v2), it is impossible to evenly balance the possibility of each answer. Therefore, we test out this hypothesis by adopting a simple yet effective baseline method on VQA CP [42]: adding a regularization term by replacing the image with a randomly sampled one. The intuition is that the answer to a question corresponds to a given image is very unlikely to be correct for a randomly sampled image. As reported in Table 4b, although effective on VQA-CP, adding such regularization hurts the performance on AVQA for both BUTD and UNITER-B. In addition, when applied to a stronger method on VQA-CP, i.e., UNITER-B, the regularization term is less effective.
Q2: Is AVQA transferrable to different visual features? AVQA dataset is collected with the assistance of models trained on Faster R-CNN [35] region features [3]. To investigate whether these collected adversarial examples are transferrable to different image features, we conduct experiments using another type of feature, grid-level features [17] from CNNs, which have shown to be effective for VQA tasks [17, 15, 32, 24]. Specifically, we consider the ClipBERT [24] model, unlike UNITER-B, which is built upon region-level object detection features, ClipBERT is an endto-end pre-trained model that directly takes in raw images and questions, and the images are represented by gridlevel features as in [17]. Meanwhile, ClipBERT's end-toend training strategy may also help to defend potential attacks to fixed feature representations widely used in previous work [8, 41, 3]. In Table 5, we compare ClipBERT against UNITER-B. The poor performance of ClipBERT on AVQA suggests that adversarial examples in AVQA

Model
Previous models UNITER-B [27] UNITER-B (ours)

Training Data
VQA v2 Train VQA v2 Train VQA v2 Train +AVQA

VQA-Rep.
Acc.  56.59 [38]
64.66 64.56 65.42

VQA-LOL Comp.
Acc.  49.88 [12]
54.16 54.54 55.10

VQA-LOL Supp.
Acc.  50.54 [12]
49.89 50.00 51.36

VQA-Intro.
MS 50.05 [37]
56.69 56.80 57.93

CV-VQA
#flips  7.53 [1]
8.47 8.44 8.43

IV-VQA
#flips  78.44 [1]
40.67 39.97 38.40

Table 6: Model Performance on recent Robust VQA Benchmarks.

Round Count

OCR

Position

Reasoning

Relation

Commonsense

Other

Lowlevel

Visual Concept Recognition

Action

Small Object

Occlusion

Abstract

R1

23.3% 10.7% 14.7% 8.3%

17.3% 0.7% 9.7% 4.3% 13.3% 14.7%

6.3%

R2

30.0% 22.7% 12.0% 27.7% 20.0% 4.3% 12.7% 9.3% 22.7% 10.0% 15.3%

R3

35.3% 13.0% 13.0% 28.3% 25.0% 6.3% 11.7% 4.3% 20.0% 20.0%

6.0%

Ave. 29.6% 15.4% 13.2% 21.4% 20.8% 3.8% 11.3% 6.0% 18.7% 14.9%

9.2%

Table 7: Analysis of 300 randomly sampled QA examples from successfully attacked per round and on average. Low-level visual concepts

include color, shape, texture. A question may belong to multiple different categories.

are transferrable to different image representations. However, ClipBERT performs comparably to UNITER-B on AVQA, although it significantly under-performs UNITERB on VQA v2. This observation suggests that VQA v2 is not a reliable dataset for evaluating model robustness.
Q3: How effective is adversarial training on AVQA? We examine the effectiveness of adversarial training by adopting PGD-based adversarial training method VILLA in [10]. VILLA-B is both adversarially pre-trained on large-scale image-text data and adversarially finetuned on the respective dataset. We compare its performance against UNITERB on both AVQA and VQA v2 in Table 5. Adversarial training brings slight performance improvement. However, the gap of between performance on AVQA and VQA v2 is still very significant. Note that VILLA-B crafts adversarial examples during training by adding adversarial perturbations to the embedding space. These adversarial perturbations can hardly change the intrinsic statistics of training data, such as the distribution of question types and relevant objects in the image. Our analysis of question types and visual recognition concepts in Sec. 4.3 will show that AVQA is hard because it requires the model to have the ability to reason, count and recognize different visual concepts.
4.2. Robustness Evaluation
We also test models on some recent Robust VQA benchmarks: VQA-Rephrasings [38] for linguistic variations, VQA-LOL [12] Complement/Supplement for logical reasoning, VQA-Introspect [37] for consistency of model predictions in perceptual sub-questions and main reasoning questions, CV-VQA [1] and IV-VQA [1] for model robustness to image manipulations. The results are summarized in Table 6. We observe that UNITER-B can already outperform the previous models for most of the benchmarks, which is consistent with observations in [27]. Training on

AVQA is helpful in improving model performance on robustness benchmarks. Particularly, AVQA helps to boost model reasoning capability across 3 datasets. It is likely that AVQA exposes the model training to more diverse question templates, hence improves on VQA-Rephrasings. On IVVQA, which focuses on counting questions, AVQA helps to improve performance despite of the significant performance gain UNITER-B has already achieved.
4.3. Question Type Analysis
We manually annotate 300 randomly sampled examples from each round to investigate the following questions: which types of questions do workers employ to fool the models and how they evolve as the rounds progress.
The results are summarized in Table 7. Questions are categorized into 4 meta-categories: counting, OCR, reasoning, and visual concept recognition. Although OCR and counting can be considered as visual concept and quantitative reasoning, we separate them out as they contributes a large portion per round, to almost 50% in the later rounds. There are three main reasoning questions: positional reasoning (i.e., the relative/absolute position of an object), relational reasoning (i.e., semantic relationship between two or more objects), and commonsense reasoning (i.e., visual commonsense reasoning, for example, "Is the water more likely to be a lake or an ocean", given an image showing a body of water surrounded by mountains.). Other reasoning questions including comparative reasoning (e.g., "which person is taller?") and logical reasoning (e.g., negation). For visual concept recognition, we roughly divide them into low-level visual concepts (e.g., color, shape, texture), action (e.g., "what is the person doing"), small objects, occluded objects, abstract objects (e.g., objects in painting).
We observe annotators rely heavily on counting questions to attack the models ­ nearly 30% of the sampled

(a) Visualization of examples collected per round in AVQA. Each ground truth answer (VQA score) is collected from 10 workers.

(b) Visualization of examples generated via textual adversarial attack methods. Blue indicates the changes made in adversarial questions.

Figure 3: Illustration of adversarial examples from (a) AVQA and (b) textual adversarial attack methods: Sears [36], Textfooler [19] and Sememe+PSO [49]. Green (red) indicates a correct (wrong) answer.

Method Sears [36] Textfooler [19] Sememe+PSO [49]3 AVQA

#Tries 3.0 39.5 35.9 1.6

Error Rate 11.6% 1.4% 88.6% 40.0%

Orig. Acc. 69.1 69.1 84.9 -

Adv. Acc. 63.0 67.8 12.5 -

Table 8: Comparison to adversarial attack methods. Orig. Acc. (Adv. Acc.) is the accuracy on original (adversarial) examples.

questions across all rounds fall into this category. While R1 questions are mostly on objects that of normal size and less occluded, we found that the counting questions becomes harder in R2 and R3 as the many of them are about small and occluded objects. There is also a surge in abstract and OCR questions for R2, due to the increase in the number of abstract images and the images that contain scene text. The percentage of reasoning questions, especially relation reasoning and commonsense reasoning increases drastically from R1 to R2 and R3. Visualizations in Figure 3a show that the questions in later rounds are indeed more complicated, with more detailed relational and positional descriptions when referring to an object. Overall, these findings are compatible with the idea that VQA models are not robust enough to various types of questions.
4.4. Why We Need Human-in-the-loop?
Textual adversarial attack methods [30, 19, 49] have been widely explored in NLP. The goal is to alter model predictions with minor changes to the input textual queries, so that adversarial examples can be generated and model vulnerabilities can be identified automatically. We investigate whether we can directly apply these methods to generate adversarial examples in high quality and compare the generated examples to AVQA. In total, we consider 3 different textual adversarial attack methods, including Sears [36] via bask-translation for sentence-level attacks, Textfooler [19] and Sememe+PSO [49] by replacing words with its syn-

onyms or words that shares the same sememe annotations for word-level attacks. The adversarial attacks are performed to all questions on 5000 images in the Karpathy split [20]. We visualize examples in Figure 12. Without human-in-the-loop, the generated adversarial questions share similar problems: (1) adversarial question does not share the same answer with the original question, therefore additional answer annotations may need to be collected; (2) model prediction to adversarial question is not necessarily incorrect when it is different from answers to the original questions; (3) word similarity may not hold when it needed to be grounded to the image (e.g.: window vs. skylights). In addition, we compare these methods against AVQA dataset quantitatively in Table 8. Generally, human takes much less tries and have a higher successful rate when attacking VQA models. How to design an effective adversarial attack methods to generate high-quality VQA examples can be an interesting future research direction.
5. Conclusion
In this work, we collect a new benchmark Adversarial VQA (AVQA) to evaluate the robustness of VQA models. It is collected iteratively for 3 rounds via a human-andmodel-in-the-loop enabled training paradigm, on images from different domains. AVQA questions cover diverse robustness types, enabling a more comprehensive evaluation on model robustness. Our analysis shows that state-of-theart models cannot maintain decent performance on AVQA, despite of large-scale pre-training, adversarial training, sophisticated model architecture design and stronger visual features. AVQA brings a new challenge to the community on how to design a more robust VQA models that could be ready to deploy in real-life application.
3Note that Sememe+PSO only attacks questions longer than 10 words, so 94.8% examples are not being attacked.

References
[1] Vedika Agarwal, Rakshith Shetty, and Mario Fritz. Towards causal vqa: Revealing and reducing spurious correlations by invariant and covariant semantic editing. In CVPR, 2020. 1, 2, 4, 7
[2] Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and Aniruddha Kembhavi. Don't just assume; look and answer: Overcoming priors for visual question answering. In CVPR, 2018. 1, 2, 6
[3] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In CVPR, 2018. 1, 5, 6, 12, 13
[4] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In ICCV, 2015. 1, 2, 11
[5] Max Bartolo, Alastair Roberts, Johannes Welbl, Sebastian Riedel, and Pontus Stenetorp. Beat the ai: Investigating adversarial human annotation for reading comprehension. TACL, 2020. 2
[6] Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. A large annotated corpus for learning natural language inference. In EMNLP, 2015. 3
[7] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dolla´r, and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015. 4
[8] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Universal image-text representation learning. In ECCV, 2020. 1, 3, 5, 6
[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL, 2019. 3
[10] Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu. Large-scale adversarial training for visionand-language representation learning. NeurIPs, 2020. 1, 6, 7, 12, 13
[11] Peng Gao, Zhengkai Jiang, Haoxuan You, Pan Lu, Steven CH Hoi, Xiaogang Wang, and Hongsheng Li. Dynamic fusion with intra-and inter-modality attention flow for visual question answering. In CVPR, 2019. 1
[12] Tejas Gokhale, Pratyay Banerjee, Chitta Baral, and Yezhou Yang. Vqa-lol: Visual question answering under the lens of logic. ECCV, 2020. 1, 2, 4, 7
[13] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In CVPR, 2017. 1, 2, 3, 4, 11, 12
[14] Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel R Bowman, and Noah A Smith. Annotation artifacts in natural language inference data. In NAACL, 2018. 3
[15] Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and Jianlong Fu. Pixel-bert: Aligning image pixels with text by deep multi-modal transformers. arXiv preprint arXiv:2004.00849, 2020. 6

[16] Drew A Hudson and Christopher D Manning. Gqa: a new dataset for compositional question answering over realworld images. In CVPR, 2019. 1, 2, 4
[17] Huaizu Jiang, Ishan Misra, Marcus Rohrbach, Erik LearnedMiller, and Xinlei Chen. In defense of grid features for visual question answering. In CVPR, 2020. 1, 6
[18] Yu Jiang, Vivek Natarajan, Xinlei Chen, Marcus Rohrbach, Dhruv Batra, and Devi Parikh. Pythia v0. 1: the winning entry to the vqa challenge 2018. arXiv preprint arXiv:1807.09956, 2018. 1
[19] Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. Is bert really robust? a strong baseline for natural language attack on text classification and entailment. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 8018­8025, 2020. 8, 11, 16
[20] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3128­3137, 2015. 8
[21] Corentin Kervadec, Grigory Antipov, Moez Baccouche, and Christian Wolf. Roses are red, violets are blue... but should vqa expect them to?, 2020. 2
[22] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. IJCV, 2017. 3
[23] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. In ICLR, 2020. 3
[24] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L. Berg, Mohit Bansal, and Jingjing Liu. Less is more: Clipbert for video-and-language learningvia sparse sampling. In CVPR, 2021. 6, 12, 13
[25] Jie Lei, Licheng Yu, Tamara L Berg, and Mohit Bansal. What is more likely to happen next? video-and-language future event prediction. In EMNLP, 2020. 3
[26] Linjie Li, Zhe Gan, Yu Cheng, and Jingjing Liu. Relationaware graph attention network for visual question answering. ICCV, 2019. 1
[27] Linjie Li, Zhe Gan, and Jingjing Liu. A closer look at the robustness of vision-and-language pre-trained models. arXiv preprint arXiv:2012.08673, 2020. 2, 6, 7
[28] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. 3
[29] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In NeurIPS, 2019. 1
[30] John Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, Di Jin, and Yanjun Qi. Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 119­126, 2020. 8

[31] Kai Nakamura, Sharon Levy, and William Yang Wang. r/fakeddit: A new multimodal benchmark dataset for fine-grained fake news detection. arXiv preprint arXiv:1911.03854, 2019. 2, 4, 11, 12
[32] Duy-Kien Nguyen, Vedanuj Goswami, and Xinlei Chen. Revisiting modulated convolutions for visual counting and beyond. arXiv preprint arXiv:2004.11883, 2020. 6
[33] Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial nli: A new benchmark for natural language understanding. In ACL, 2020. 2, 4
[34] Adam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, and Benjamin Van Durme. Hypothesis only baselines in natural language inference. In Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics, 2018. 3
[35] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In NeurIPS, 2015. 6
[36] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Semantically equivalent adversarial rules for debugging NLP models. In ACL, 2018. 8, 11, 16
[37] Ramprasaath R Selvaraju, Purva Tendulkar, Devi Parikh, Eric Horvitz, Marco Ribeiro, Besmira Nushi, and Ece Kamar. Squinting at vqa models: Interrogating vqa models with sub-questions. CVPR, 2020. 1, 2, 7
[38] M Shah, X Chen, M Rohrbach, and D Parikh. Cycleconsistency for robust visual question answering. In CVPR, 2019. 1, 2, 7
[39] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In ACL, 2018. 2, 4, 11, 12
[40] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. Vl-bert: Pre-training of generic visuallinguistic representations. In ICLR, 2020. 1
[41] Hao Tan and Mohit Bansal. Lxmert: Learning crossmodality encoder representations from transformers. In EMNLP, 2019. 1, 3, 5, 6
[42] Damien Teney, Kushal Kafle, Robik Shrestha, Ehsan Abbasnejad, Christopher Kanan, and Anton van den Hengel. On the value of out-of-distribution testing: An example of goodhart's law. NeurIPS, 2020. 6
[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. 2017. 5
[44] Adina Williams, Nikita Nangia, and Samuel R Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In NAACL, 2018. 3
[45] Cihang Xie, Mingxing Tan, Boqing Gong, Jiang Wang, Alan L Yuille, and Quoc V Le. Adversarial examples improve image recognition. In CVPR, 2020. 2
[46] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. In NeurIPS, 2020. 3

[47] Zhilin Yang, Saizheng Zhang, Jack Urbanek, Will Feng, Alexander H Miller, Arthur Szlam, Douwe Kiela, and Jason Weston. Mastering the dungeon: Grounded language learning by mechanical turker descent. In ICLR, 2018. 2
[48] Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian. Deep modular co-attention networks for visual question answering. In CVPR, 2019. 1
[49] Yuan Zang, Fanchao Qi, Chenghao Yang, Zhiyuan Liu, Meng Zhang, Qun Liu, and Maosong Sun. Word-level textual adversarial attacking as combinatorial optimization. In ACL, pages 6066­6080, 2020. 8, 11, 16
[50] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual commonsense reasoning. In CVPR, 2019. 2, 4, 11, 12
[51] Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. Swag: A large-scale adversarial dataset for grounded commonsense inference. In EMNLP, 2018. 2
[52] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. Vinvl: Making visual representations matter in visionlanguage models. arXiv preprint arXiv:2101.00529, 2021. 1, 2

A. Data Statistics
Type of Question. Following [4], given the structure of questions generated in English, we cluster questions into different types based on the words that start the question. Figure 4 shows the distribution of questions based on the first four words of the questions in AVQA. Interestingly, the variety of question types are quite similar to those in [4], including "What is", "How many" and "Is there". Quantitatively, we also categorize the questions into "Y/N", "Num", "OOV" and "Other". The percentage of questions for different categories is shown in Table 9. "OOV" questions refers to questions that cannot be answerable by VQA v2 [13] answer vocabularies. We also include two upper bounds, one based on VQA v2 answer vocabularies, and the other on open vocabularies. Comparing model performance reported in the main text, there is still a huge gap, approximately 50 points lower than the upper bounds.
Question Lengths. Figure 5 shows the distribution of question lengths. We see that most questions range from four to ten words.
Dataset Properties Across Rounds. Figure 6 shows a histogram of the number of tries per verified example across the three different rounds. We observe a consistent trend for all three rounds, over 80% of examples are successfully collected within 2 tries. Figure 7 shows the time taken per verified example. As the round progresses, we observe that more and more examples are collected within 100 seconds (less than 2 minutes). Figure 8 shows the proportion of different types of collected examples across three rounds. Comparing to R1 and R2, R3 contains more "not sure" judgements to model answers during question collection (type B), which indicates that the task is getting harder. There are a small amount of examples in all three rounds that there is no agreement among the answers collected (type D). Examples from B an D are excluded due to low quality. The rest are split into train/dev/test set (refer to Figure 8 captions for more information).
Answer Confidence and Inter-human Agreement. During answer collection (see interface in Figure 15), the annotators are required to provide both a correct answer to the question given the image content and a self-judgment on how confident they feel about the answer. Specifically, we ask "Do you think you were able to answer the question correctly?", and the annotator need to choose from "yes" (confident with score 1), "maybe" or "no" (not confident with score 0). Figure 9 shows the distribution of responses (black lines). A majority of the answers were labeled as confident. More than 9 annotators are confident about their answers on over 60% questions on average.
In addition, we investigate how the self-judgment confidence corresponds to the answer agreement between annotators across three rounds of data collection. Color bars

in Figure 9 show the percentage of questions in which (i) 7 or more, (ii) 3-7, or (iii) less than 3 workers agree on the answers given their average confidence score. Across all rounds, the agreement between subjects increases with confidence. We do observe that workers are more confident about their answers in R2 and R3, comparing to R1.
Answer Distribution. Figure 10 shows the distribution of answers for several question types. We can see that a number of question types, such as "Is . . . ", "Can. . . ", and "Does. . . " are typically answered using "yes" and "no" as answers. Other questions such as "What is/are. . . " and "What kind/type. . . " have a rich diversity of responses. Other question types such as "What color. . . " or "Which. . . " have more specialized responses, such as colors, or "left" and "right". These observations are similar to those in VQA v2.
B. More Visualizations
We include more visualization examples of collected data across three rounds in Figure 11. We show adversarial questions from 4 categories: Count, OCR, Reasoning and Visual Concept Recognition. Note that questions may belong to multiple categories. For example, counting question from R3 ("How many natural satellites are in the sky?") requires commonsense about "natural satellites". OCR question from R1 ("What company is on the back of the referee?") not only requires commonsense about "referee" but relational reasoning about "on the back of". Reasoning questions include positional/relational reasoning (e.g., "What is the woman closest to the camera holding in her hand?"), commonsense reasoning (e.g., "Is the egg yolk cooked?") and comparative reasoning ("Who is taller?"). There are also questions that require recognition of both low-level visual concepts (e.g., color/shape) and high-level visual concepts (e.g., action, relation).
We also visualize more examples generated via textual adversarial attack methods (Sears [36], Textfooler [19] and Sememe+PSO [49]) in Figure 12. The first two columns show invalid examples and the last column include valid examples, based on our manual examination. Without humanin-the-loop, it is difficult to verify that these auto-generated adversarial questions are actually valid.
C. More Results
Recall that questions in R3 are collected on images from various domains, including web images from Conceptual Captions [39] (CC, used in R1 and R2), user-generated images from Fakeddit [31] and movie video frames from VCR [50]. Hence, we can study how model performance can be transferable across different domains. We create a new split of R3 (R3 ) according to the image source, with

Round Y/N

Question Types Num OOV

Other

Upper Bound

dev/test

dev/test

R1

13.53% 23.36% 10.03% 50.08% 81.43/79.75 92.03/92.05

R2

8.62% 29.91% 14.37% 47.01% 76.26/77.11 93.60/93.43

R3

11.67% 34.19% 12.06% 42.07% 80.21/80.97 94.67/94.15

AVQA 11.51% 27.89% 11.90% 48.71% 79.32/79.11 93.19/ 93.02

Table 9: Question type distribution on verified examples and upper bound on dev/test set across three rounds. answer vocabularies.  is based on open vocabularies.

is based on VQA v2 [13]

Figure 4: Distribution of questions by their first four words. The arc length is proportional to the number of questions containing the word. White areas are words with contributions too small to show.

Figure 5: Percentage of questions with different word length across three rounds. Most questions range from three to ten words.

CC images for training and Fakeddit/VCR images for evaluation. Table 10 summarizes UNITER-B performance under different training settings. Despite the domain differences in images, the performance on Fakeddit and VCR split improves as we include more training data from CC images. Comparing the new split R3 with the original split R3, training on more in-domain examples on CC images does help to improve model performance on R1 and R2. We also observe that model performance on VCR is significantly higher than those on other splits across all training settings. Images from VCR are often human-centric, which may be "easier" than complex or abstract scenes depicted in CC/Fakeddit images.

In addition, we include detailed results from BUTD [3], ClipBERT [24], VILLA-B and VILLA-L [10] in Table 11. These results are consistent with observations we summarized in Section 4 of the main text.
D. Data Collection Interface
Examples of the user interface are shown in Figures 13, 14 and 15. We also include full instructions and examples shown to the annotators in Figures 16 and 17.

Training Data VQA v2+VGQA +R1 +R2 +R3 ALL

R1 20.60 26.03 26.60 27.02 26.85

R2 17.86 17.30 23.21 23.78 23.38

R3 19.45 19.41 18.60
23.76

Fakeddit [31] 23.46 24.72 24.67 27.32 -

VCR [50] 28.26 28.99 29.44 32.87 -

Table 10: Domain transfer evaluation on UNITER-B. indicates that we only use examples collected on CC [39] images for training. ALL refers to VQA v2+VGQA+R1+R2+R3.

Figure 6: Histogram of the number of tries for each good verified example across three rounds.

Figure 7: Histogram of the time spent per good verified example across three rounds.

Model BUTD

Training Data
VQA v2 +VGQA +R1 +R1+R2 ALL

R1
dev/test 20.80/19.28 20.27/20.27 24.41/21.82 24.96/22.11

R2
dev/test 18.77/18.85 19.53/20.14 22.28/21.80 22.62/22.78

R3
dev/test 21.17/21.31 21.34/21.82 21.47/20.94 24.03/23.70

AVQA
dev/test 20.18/19.60 20.26/20.58 23.00/21.61 23.94/22.71

VQA v2
test-dev 67.60 67.37 67.44 67.52

(v2, AVQA)
test-dev, test 48.00 46.79 45.83 44.81

ClipBERT

VQA v2 +VGQA +R1 +R1+R2 ALL

21.39/20.45 23.83/22.43 24.03/23.08 24.62/23.68

19.29/20.06 20.08/20.13 23.12/23.86 22.96/24.66

21.51/22.63 22.18/22.78 23.81/23.20 24.55/24.68

20.69/20.82 22.15/21.71 23.66/23.38 24.03/24.25

69.08 69.07 69.19 69.17

48.26 47.36 45.81 44.92

VILLA-B VILLA-L

VQA v2 +VGQA +R1 +R1+R2 ALL VQA v2 +VGQA +R1 +R1+R2 ALL

21.22/19.45 25.92/24.07 27.53/25.13 30.78/28.43 24.99/22.88 28.29/26.12 30.02/27.81 29.92/28.01

18.53/18.92 20.00/20.05 23.23/23.91 25.66/25.11 18.58/18.23 19.44/19.02 24.05/23.59 24.59/24.26

19.86/19.45 20.76/20.98 20.36/21.50 23.32/23.36 19.85/19.37 19.86/19.45 18.41/20.42 23.33/22.38

19.98/19.27 22.68/21.95 24.39/23.87 27.29/26.10 21.59/20.45 23.28/22.10 25.27/24.63 26.55/25.40

73.37 73.21 73.11 74.28 74.58 74.12 74.06 74.24

54.10 51.26 49.24 48.18 54.13 52.02 49.43 48.84

Table 11: Detailed results from BUTD [3], ClipBERT [24], VILLA-B and VILLA-L [10] under different settings. AVQA = R1+R2+R3, ALL = VQA v2+VGQA+AVQA.

Figure 8: Proportion across three rounds. A=Examples that model got right ("Definitely Correct") during question collection, B=Examples that model neither got right nor wrong ("Not Sure") during question collection. C, D and E are examples that model got wrong ("Definitely Wrong") during question collection and sent to 9 annotators for verification during answer collection. Specifically, C=Examples that more than 3 verifiers overruled the question author's decision of "Definitely Wrong" and agree with the model's answer. D=Examples for which there is no agreement among verifiers, E=Examples where at least two verifiers agree with each other during answer collection. We split E by images into training, dev, or test set. Examples on training images in A and C are added to training set, the rest are discarded. B and D are excluded due to low quality.
Figure 9: Number of questions per average confidence score across three rounds (black lines, 0 = not confident, 1 = confident). Percentage of questions where 7 or more answers are same, 3-7 are same, less than 3 are same across three rounds (color bars).

Figure 10: Distribution of answers per question type. Only top-100 answers to each question type are plotted. The height of each color bar is proportional to the percentage of an answer to the corresponding question type.

Figure 11: More visualization of examples collected per round in AVQA. We show examples that contains adversarial questions from 4 categories: Count, OCR, Reasoning and Visual Concept Recognition across three rounds. Each ground truth answer (VQA score) is collected from 10 workers. Green (red) indicates a correct (wrong) answer. Blue highlights the verified adversarial questions.
Figure 12: More adversarial examples from textual adversarial attack methods: Sears [36], Textfooler [19] and Sememe+PSO [49]. Green (red) indicates a correct (wrong) answer. Blue highlights the changes made in adversarial questions.

Figure 13: UI for question collection. Given an image, the annotator is required to write a tricky answer to fool our "smart VQA robot" (well-trained VQA models). After clicking the "Get Robot Ansner", the annotated question will be sent to our online model for evaluation, and a feedback will be returned immediately. See Figure 14 for an example of model feedback.

Figure 14: Example of model feedback shown to the annotators. After reviewing the model response, the annotator need to judge the correctness of the model answer ("Definitely Correct", "Not Sure" or "Definitely Wrong"). If the model answer is definitely wrong, the annotator is prompted to enter a correct answer.

Figure 15: UI for answer collection. Given an image and a question, an annotator is asked to write a concise answer to the question, and choose a confidence level for the answer ("Yes", "No", or "Maybe").

Figure 16: Full instructions for question collection.

Figure 17: Examples provided to annotators for question collection.

