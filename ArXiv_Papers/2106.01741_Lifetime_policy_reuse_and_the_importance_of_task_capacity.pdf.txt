LIFETIME POLICY REUSE AND THE IMPORTANCE OF TASK
CAPACITY

A PREPRINT

David M. Bossens University of Southampton D.M.Bossens@soton.ac.uk

Adam J. Sobey University of Southampton The Alan Turing Institute

arXiv:2106.01741v1 [cs.LG] 3 Jun 2021

4th June 2021
ABSTRACT
A long-standing challenge in artificial intelligence is lifelong learning. In lifelong learning, many tasks are presented in sequence and learners must efficiently transfer knowledge between tasks while avoiding catastrophic forgetting over long lifetimes. On these problems, policy reuse and other multi-policy reinforcement learning techniques can learn many tasks. However, they can generate many temporary or permanent policies, resulting in memory issues. Consequently, there is a need for lifetime-scalable methods that continually refine a policy library of a pre-defined size. This paper presents a first approach to lifetime-scalable policy reuse. To pre-select the number of policies, a notion of task capacity, the maximal number of tasks that a policy can accurately solve, is proposed. To evaluate lifetime policy reuse using this method, two state-of-the-art single-actor base-learners are compared: 1) a value-based reinforcement learner, Deep Q-Network (DQN) or Deep Recurrent Q-Network (DRQN); and 2) an actor-critic reinforcement learner, Proximal Policy Optimisation (PPO) with or without Long Short-Term Memory layer. By selecting the number of policies based on task capacity, D(R)QN achieves near-optimal performance with 6 policies in a 27-task MDP domain and 9 policies in an 18-task POMDP domain; with fewer policies, catastrophic forgetting and negative transfer are observed. Due to slow, monotonic improvement, PPO requires fewer policies, 1 policy for the 27-task domain and 4 policies for the 18-task domain, but it learns the tasks with lower accuracy than D(R)QN. These findings validate lifetime-scalable policy reuse and suggest using D(R)QN for larger and PPO for smaller library sizes.
Keywords lifelong learning · reinforcement learning · transfer learning · catastrophic forgetting · deep neural networks
1 The importance of efficient learning over multiple tasks
During their lifetime, animals may be subjected to a large number of unknown tasks. In some environmental conditions, nutritious food sources may be readily available, while in others they may be sparse, hidden, or even poisonous, and dangerous predators may roam in their vicinity. To address these challenging conditions, various behaviours must be selectively combined, such as avoidance, reward-seeking, or even fleeing. When direct perception provides limited or no cues about the current task, animals have to infer the task or use a strategy that works for many different tasks it may encounter. Therefore, a key challenge for learning challenging sequences of tasks is to find a limited number of strategies that work on the large domain of tasks that animals encounter over their lifetime.
In artificial intelligence, variants of the above problem have been studied with investigations focusing on two aspects: transfer learning and catastrophic forgetting. Transfer learning is a process in which learners leverage the knowledge gained from a set of previously learned tasks with similar characteristics to a new task, whilst avoiding transferring knowledge that is not relevant [Taylor and Stone, 2009, Pan and Yang, 2010, Lazaric, 2013]. Catastrophic forgetting is a process in which knowledge learned on one task completely removes knowledge learned on some previous tasks [French, 1992, Hasselmo, 2017]. Both transfer learning and catastrophic forgetting are at the heart of the challenging scenario of lifelong learning, where new tasks may appear at any time and where a learned task might be forgotten when it has not been seen regularly [Thrun and Schwartz, 1995, Silver et al., 2013, Chen and Liu, 2016].

Lifetime policy reuse and the importance of task capacity

A PREPRINT

Lifelong learning can be performed with a single representation shared (almost) completely across all tasks or with a variety of representations specialised to a subset of tasks. Although the first approach can be efficient in specialised cases, the second approach has the benefit of being applicable to larger task sets with more widely varying tasks and of allowing greater task specialisation. This paper focuses on the second approach, with a particular emphasis on scalability towards a full lifetime setting, where a task sequence of indefinite length and of unknown temporal structure is provided to the learner. The lifetime setting also comes with limited prior knowledge and low designer effort: since the learning agent is not continually supervised, the learner is not given an easy-to-learn curriculum nor is it always allowed to converge on a task before learning the next. Instead, all learning takes place in an `anti-curriculum' setting where tasks follow each other rapidly and randomly. With this setting in mind, the paper develops lifetime-scalable methods which have a fixed-size memory to allow solving task-sequences of indefinite length without memory issues. To do so, the paper provides a methodology to address three issues that arise within this context:
· Task capacity: to pre-determine the number of representations required for lifelong reinforcement learning.
· Lifetime policy reuse: to perform online multi-representation reinforcement learning which has a fixed number of representations chosen by the task capacity and which allows continued improvements across the lifetime.
· Applicability/Model-agnosticism: to provide a multi-representation method with applicability to different base-learners.
The approach is demonstrated on two different lifelong learning domains, characterised by: 1) 18 to 27 unique tasks, a number of tasks smaller than the full lifetime setting but comparable to the state-of-the-art; and 2) random task presentation, making the environment challenging as full convergence is not possible on any task before proceeding to the next.

2 Reinforcement learning preliminaries

In reinforcement learning, an agent interfaces with the environment in the following control cycle. First, it receives the state of the environment, a state s from the state space S that directly corresponds to a unique environment state, or a more limited observation, an observation o from the observation space O that is only a partial observation of the environment state. Then, based on the full state, if it is observable, or else the more limited observation, it performs an action a according to its policy  : S  A where A is the space of actions; and finally, it receives a real-valued reward r from the environment. The policy can be formulated explicitly or implicitly; for example, an often-used approach is to define an action-value function Q, which represents the utility Q(s, a) of any state-action pair s, a , and then select the optimal action for the given state probabilistically according to the action-value function. To adaptively increase the reward intake over time, the learning agent will perform periodic updates to its value-function or directly to its policy. The policy and the value-function are often fitted through a function approximator, such as a deep neural network, to be able to solve larger state spaces.

The most common frameworks in reinforcement learning are Markov Decision Processes (MDPs) and Partially Observable Markov Decision Processes (POMDPs).

An MDP is defined by a tuple S, A, T , r,  , where: S is the state space; A is the action space; T is a probabilistic

function such that st+1  T (st, at) denotes the state st+1  S that follows after selection action at  A in state st  S;

rt = r(s, a) denotes the real-valued reward at time t after performing action at in state st; and   [0, 1] denotes the

discount parameter that represents the patience in the objective of the learning agent according to the expected discounted

cumulative reward E[

H j=0

j rt+j ],

where

H

is

the

horizon.

Due

to

the

transition

function

T

being based only on the

current state-action-pair, the MDP follows the Markov property such that P(st+1|stat) = P(st+1|stat . . . s1a1).

The POMDP generalises the MDP to account for partial observability making it more widely applicable. The POMDP
S, A, T , r, , O, To assumes an underlying MDP but additionally incorporates an observation space O and an observation transition function To, such that ot+1  To(st, at) is the observation following action at  A in state st  S. In POMDPs, the observations are input to the learning agent, rather than the states, and the Markov property does not hold for the observations. That is, it is not required that P(ot+1|otat . . . o1at) = P(ot+1|otat). However, the states of the underlying MDP still follow the Markov property. The MDP can therefore be seen as a special-case
POMDP in which observations and states are always equal such that O = S and To = T .

While traditional reinforcement learning solves a single MDP or POMDP, this paper will focus on how to solve lengthy

sequences of MDPs and POMDPs. This requires leveraging the knowledge gained from one task to another, based on a

more long-term perspective than provided by the discounted cumulative reward; in particular, the work will focus on

how

to

select

reinforcement

learning

policies

to

optimise

the

lifetime

average

reward

R

=

1 T

T t=0

rt,

where

T

is

the

length of the lifetime of the reinforcement learning agent.

2

Lifetime policy reuse and the importance of task capacity

A PREPRINT

3 Lifelong reinforcement learning: a review
Lifelong reinforcement learning (LRL) is one of the branches of lifelong learning which focuses on the problem setting of reinforcement learning. This section surveys the field, identifying scalability to a large number of widely varying tasks and applicability to different base-learners as key gaps in the current state-of-the-art.
3.1 Single-policy methods
Most LRL approaches consider how to learn with a single policy and provide some task-specific aspects. The main aim of this approach is that it allows transferable features to be efficiently learned within a single representation while adding some additional task-specificity. Specificity may be represented in architectural components or within the training algorithm itself to improve performance on multiple tasks [Kirkpatrick et al., 2017, Jung et al., 2017, Li and Hoiem, 2018, Rusu et al., 2016, Isele and Cosgun, 2018, Schaul et al., 2015, Deisenroth et al., 2014]. While a single policy is compact and the neural network's hidden layers close to the input layer are able to represent transferable features, it can bring disadvantages. The number of neurons may grow with the number of tasks [Rusu et al., 2016], or expert knowledge of the task's relevant features may be needed as inputs to the learning system [Schaul et al., 2015, Deisenroth et al., 2014]. Further, there may be no common representation to all tasks; for example, one lifelong learning approach, which reused a single Deep Q-Network policy on all of the tasks, has also been outperformed by task-specific DQN policies [Mnih et al., 2015, Kirkpatrick et al., 2017]. More generally, these approaches typically have a few limiting assumptions on the relation between tasks. For example, Cheung et al. [2020] and Lecarpentier and Rachelson [2019] require some bounds on the change in reward function and transition function, implying it is more suited to continuously changing scenarios rather than a sequence of tasks from a wide task domain.
3.2 Policy reuse
To avoid some of the above-mentioned challenges, some have recognised the need for learning with multiple policies. In particular, policy reuse methods learn a limited number of policies each specialised to their own subset of tasks as a means to improve transfer to a new unseen task [Wang et al., 2018, Li et al., 2018, Li and Zhang, 2018, Rosman et al., 2016, Fernández and Veloso, 2006]. The approach represents a step forward in improving the scalability of multiple policy approaches as some empirical results have shown the viability of transfer learning. For example, 50 tasks in an office domain were learned with 14 Q-learning policies; the results show this was possible by learning a common policy for all of the tasks which have their goal locations in the same room [Fernández and Veloso, 2006, Watkins and Dayan, 1992]. To the authors' best knowledge, this number of tasks has not been replicated in other studies.
To scale the policy reuse approach to memory-consuming deep reinforcement learners or to even more policies than is currently the case, current methods have their limitations. In particular, growing the policy library without an increasing number of temporary or permanent policies remains a challenge in policy reuse. Most works have focused on how to select the new policy for a new, unlabeled task (see e.g. Rosman et al. [2016]). Other works have provided tools to build the policy library. In Fernández and Veloso [2006], new, temporary policies are formed for new tasks and these may be added to the library if after convergence on the task the resulting policy performs significantly better than the existing policies in the library. A downside of this method is that when convergence is not allowed, and many tasks are randomly presented, memory issues may become apparent. In Hernandez-Leal et al. [2016], one also forms new policies for new tasks but all policies are added to the library, which implies memory issues will be apparent even more rapidly.
More generally, policy reuse has been limited in its applicability as base-learners other than Q-learning methods have yet to be incorporated with policy reuse. This would be particularly desirable after results suggesting Q-learning may not be suitable for reuse on many tasks, since (i) policy reuse with Q-learning policies may result in negative transfer, being outperformed by task-specific Q-learning [Li et al., 2018]; and (ii) a single-policy lifelong learning approach with DQN has been outperformed by task-specific DQN policies [Mnih et al., 2015, Kirkpatrick et al., 2017].
3.3 Alternative multi-policy methods
A variety of alternative approaches have been taken to multi-policy lifelong reinforcement learning. These methods assume some similarity across all tasks and are therefore not the method of choice to learn large sets of tasks with widely varying dynamics and reward functions.
One approach is to provide an initial policy based on the prior tasks and then generate a new policy for each additional task encountered during operation [Abel et al., 2018, Finn et al., 2017, Wilson et al., 2007, Konidaris et al., 2012, Thrun and Sullivan, 1995]. However, each task requires a new policy to be stored, causing memory problems when a long
3

Lifetime policy reuse and the importance of task capacity

A PREPRINT

sequence of tasks is considered. Moreover, finding an initial policy may be difficult to locate in parameter space when tasks require widely differing policies.
Hierarchical approaches are proposed to improve transfer learning by learning sub-policies that achieve a subgoal or search for a rewarding high-level state [Thrun and Schwartz, 1995, Brunskill and Li, 2014, Konidaris et al., 2012, Tessler et al., 2016, Li et al., 2019]. Sub-policies used in this way represent solutions to sub-tasks which occur across a number of different tasks, reducing the number of required policies. While there are a few exceptions (e.g., [Levy et al., 2019]), a downside of this approach is that (i) there is a need for two separate phases, one to learn sub-policies, and one to combine them; and (ii) that tasks may come in clusters in which similar tasks are more efficiently solved by a common policy.
3.4 Meta-learning
Meta-learning, in which a meta-level algorithm optimises the underlying learning algorithm itself by means of a meta-level objective, has been investigated in domains relevant for lifelong learning. In general, such methods share key aims, such as learning over an extended lifetime and improving generalisation but do not consider the same learning setting or the same goal to learn widely varying task sequences.
Meta-learning methods are particularly prevalent in supervised learning. For example, hyper-networks, in which a top-level super-ordinate network performs gradient descent to optimise a sub-ordinate network, can solve a variety of tasks from within a similar class [Hochreiter et al., 2001, Andrychowicz et al., 2016].
In meta-reinforcement learning, the literature focuses on generic aspects of reinforcement learning such as improved exploration (e.g., Xu et al. [2018]). Other meta-reinforcement learning approaches have focused specifically on settings relevant to lifelong learning: multi-task learning, in which multiple tasks are learned in batch (e.g., Finn et al. [2017]); continual learning, in which a limited number of similar tasks of increasing difficulty are presented (e.g., Riemer et al. [2019]); and lifetime reinforcement learning methods that are guided by the lifetime average reward to improve the system in long-term environments (e.g., Bossens et al. [2019]). The lifetime policy reuse method is partly inspired by the latter approach.
4 Lifetime policy reuse
Lifetime policy reuse is a method that merges the construction of a policy library with its actual reuse. Contrasting to alternative methods, which initialise new policies for every task, lifetime policy reuse avoids excessive memory consumption by using a number of policies fixed at design-time and by continually refining those policies based on incoming tasks across the lifetime. The name lifetime policy reuse is chosen to reflect three of its principles:
1. The policies in the library are adjusted throughout the entire lifetime of the reinforcement learning agent; 2. The selection of policies in the library is performed adaptively based on the lifetime average reward on the task
because without the assumption of convergence on the task, the same task may come back at any given time. 3. Since no new policies are generated for new tasks, the method scales better to lengthy lifetimes with many
tasks, by design. The method avoids an overflow of too many policies existing in memory at the same time, which can be a problem especially with deep neural networks.
4.1 Problem setting and algorithmic overview
Assume a finite set of unique reinforcement learning tasks F = {1, 2, . . . , N }, each of which will be assumed to be either MDPs or POMDPs. The tasks share the same state- and action-space but come with their own reward functions {r1 , . . . , rN } as well as their own transition dynamics {T 1 , . . . , T N }. The discount is considered as a tuning factor for the learner and is the same for all tasks. Unlike traditional policy reuse settings, the tasks are not presented until convergence; instead they are randomly chosen, presented for a smaller number of episodes, and can possibly reoccur later in the lifetime. The purpose of the lifetime policy reuse system is then to optimise the lifetime average reward R by (a) learning when to select which policy; and (b) training the policies on the tasks for which they are selected.
Before starting the algorithm, the user selects a base-learner (e.g., DQN) and a number of policies, N, at design-time based on the task capacity (see Section 9) or other model selection criteria. Then the algorithm follows the mechanics illustrated in Figure 1. The algorithm initialises a library of policies  = {1, . . . , N } randomly in parameter space. At the start of each episode, the policy selection module decides on the policy to be used for the current task based on the current task index it is provided by the environment. The selected policy interfaces with the environment as the
4

Lifetime policy reuse and the importance of task capacity

Environment task index Policy selector

interface

select evaluate

A PREPRINT

...

Policy 1

Policy 2

Policy N

Figure 1: Lifetime policy reuse. The following cycle is repeated: first, a policy selector selects a policy to be used; second, the policy interfaces with the environment for a prolonged amount of time (e.g. one episode), during which time it not only acts but also learns in the environment. When the policy has finished, it feeds back its performance to the policy selector.

base-learner would do in a traditional single-policy approach. After the episode finishes, a new policy is chosen by the policy selection module. Contrasting to traditional policy reuse, policies selected from the library continue to be adjusted throughout their entire lifetimes, performing their periodic updates according to the base-learner that is used.

4.2 Policy selector

One possible benefit of learning with multiple policies is that if one policy is located in a low-performing region of the parameter space, selecting a different policy for the task may be more beneficial than continuing to optimise the current policy for that task.

This kind of adaptivity is included into lifetime policy reuse by using the traditional -greedy exploration-exploitation strategy to adjust the sub-tasks associated with a given policy. -greedy defines a probabilistic map M to select the best policy with a probability 1 - and a randomly chosen policy in  with probability , where is a small probability, here set as = 0.10. The "best" policy for a task index j is defined in Equation 1 as the policy with the maximal lifetime average performance,

best-policy(j) = arg max Rij(tij) ,

(1)

i

where

Rij (tij )

=

1 tij

tij t=0

rtij

is

the

average

reward

that

a

policy

i

obtained

on

task

j,

and

tij

is

a

metric

defining

the time that the policy i has been used on task j. We define the time as the number of episodes for variable-length

episodes and as the number of time steps for fixed-length episodes. By default, any policy that has not been selected

for a task yet cannot be the best policy and is chosen with probability . This conservative bias aims to first train the

best policy repeatedly before selecting another policy, which avoids being inefficient when there are many policies and

when reliable evaluation of a policy requires many policy intervals. Any ties for the best policy are broken by uniform

random choice.

The lifetime average performance is favoured over a few alternatives. Jumpstart in new tasks could provide a short-term improvement but may be followed by stagnation. Metrics considering rapid improvement in reward in the initial phase of the task-block can have a similar stagnation effect. The asymptotic performance on the task is especially important for long-term scenarios as considered in lifelong learning. However, it is unknown during the learning process and therefore cannot provide online learning. Instead, the lifetime average performance allows online, incremental learning of which policy to select and if it converges, it converges to the asymptotic performance. Finally, while a discounted cumulative reward metric is used for the policies themselves, this metric is not used in the policy selection objective because discounting defines the solution method (i.e., low-patience vs high-patience) rather than the objective itself. To give a practical example, in Pacman- or Atari-games, the reinforcement learning community evaluates the performance

5

Lifetime policy reuse and the importance of task capacity

A PREPRINT

of algorithms not on the objective of the learner but on the actual score, be it asymptotic or lifetime average performance. Discounting in continuing environments does not optimise the lifetime average but the lifetime average does (see e.g., Naik et al. [2019]).

5 Task capacity: a guide to selecting the number of policies

A key concern in lifetime policy reuse is that the number of policies must be carefully selected. If the number of policies is too low, then the learning agent may experience negative transfer and catastrophic forgetting. If the number of policies is too high, this may lead to excessive memory consumption or slower learning on new tasks that are similar to previously seen tasks.
To select the number of policies, this paper proposes the notion of task capacity, the maximal number of tasks that can be accurately solved by a single policy associated with the base-learner. The notion comes with two variants: (i) the theoretical task capacity, the number of tasks that a policy can represent to sufficient accuracy; and (ii) the empirical task capacity, the number of tasks that a policy can empirically learn to sufficient accuracy. The former does not consider any aspects of learning but does not require any preliminary runs. The latter would require a preliminary test on a representative domain of tasks, similar to a tuning procedure, or can be used as a benchmarking tool to indicate which base-learner is more scalable to a large number of tasks.

5.1 Solving multiple tasks accurately

A near-optimal lifetime policy reuse system yields a lifetime average reward R close to or equal to the optimal R for all the tasks it is presented. Defining the lifetime loss L() for a policy  as L() = R - R, a near-optimal reinforcement learner has a negligible lifetime loss. For simplicity of notation, R will also be used when referring to
the lifetime average reward over one or more tasks, which will be clear from the context.

The intuition of near-optimality is further formalised below, along with a sufficiency condition that illustrates that it is sufficient to analyse the states in which the policy performs optimally. The lemma will assume rewards and expected rewards are normalised in [0, 1]; however, this can be scaled to obtain a more general case. A further assumption is the Markov property such that P(st+1|st, at, . . . s1, a1) = P(st+1|st, at).
Lemma 1. An epsilon-optimal partition defines a subet of parameter space    such that for each    , the policy  parametrised by  satisfies L()  R. Given an arbitrary task from a given set of tasks,   F , and arbitrary  0, an -optimal partition is ensured when sS+ P[s|]r (s, a)  (1 - )R, where S+ is the set of states for which optimal actions are taken by .

For arbitrary   , its corresponding policy  will have a set of states S+  S for which its chosen action is optimal for task   F, while for the remainder of the states S- = S \ S+ its action chosen is not optimal for  . Setting the expected reward for any s  S- minimally, 0, for  allows dropping the summation over S- in the equality R = sS- P[s|]r (s, (s)) + s S+ P[s |]r (s , a). This allows the following upper bound to the loss:

L()  R -

P[s |]r (s , a) ,

(2)

s S+
where a is the optimal action, which is equal to the action taken by  for all s  S+. Therefore, an -optimal partition is ensured when

P[s |]r (s , a)  (1 - )R .

(3)

s S+
Corrolary 1. Changing transition dynamics limits scalability. With rewards normalised to be positive, a higher sS+ P[s|] will yield higher sS+ P[s|]r (s, a). When different tasks have widely varying transition dynam-
ics, then a single policy cannot ensure high-probability reachability across S+ for a subset of tasks F1  F; therefore, for tasks in F1, L()  R is not ensured on a task-by-task basis. Corrolary 2. Changing reward functions limits scalability. A higher r (s, a) will yield a higher
sS+ P[s|]r (s, a). However, when different tasks have widely varying reward functions, a single policy cannot always ensure epsilon-optimality over all tasks, as for different tasks the optimal state set, S+, may be different.

5.2 Theoretical task capacity
Epsilon-optimality introduced in Lemma 1 can further be used to define how many policies are needed or equivalently, how many tasks a single policy can represent accurately. This metric, called the theoretical task capacity, is defined below and illustrated on simple examples as well as the 27-task cartpole domain, which will be revisited in Section 6.3.

6

Lifetime policy reuse and the importance of task capacity

A PREPRINT

The theoretical task capacity is defined representationally based on (i) the hypothesis class H, the set of allowed policy representations (for example, the set of functions that can be fitted by the function approximator); (ii) the task set F; and (iii) the tolerance, , specified for epsilon-optimality.
Definition 1. The theoretical task capacity C of a hypothesis class H with regard to a task set F is based on the minimal number of policies in H needed to -optimally represent all its tasks, according to

C(H, F , ) = max N /N :  = {1, . . . , N }  H :   F :

N

(4)

   : L()  R .

Simple examples To illustrate the representational argument made for selecting the number of policies, below are two examples on how the representation of the base-learner limits its theoretical task capacity for a given task set.
Example 1. Linear approximators for changing reward functions. In this example, a set of 4 polynomial functions must be approximated, providing a simple example in which transition dynamics do not play a role but the reward function changes depending on the task. A policy here simply outputs the estimated function output, with 100% probability. States are sampled independently and identically from a uniform distribution over the state space S = [0, 1]. Consider a task set F = {1, 2, 3, 4}. Each of its four tasks are defined by the corresponding functions to approximate: f1(s) = -s2 + 2s, f2(s) = s2, f3(s) = -0.01s2 + 0.02s, and f4(s) = 0.01s2; in other words, i (s) = fi for all i  1, . . . , 4. Their reward function is the complement of the absolute error, that is, ri (s, (s)) = 1 - |(s) - i(s)| for all i  {1, . . . , 4}. Consider the hypothesis class to be linear classifiers of the form (s) = As where A  R is a constant.

The theoretical task capacity, C, the minimal number of linear functions that can be selected while maintaining expected error smaller than = 0.25, is two. An example of such a function set is {1(s) = s, 2(s) = 0.01s}. 1(s) can

approximate f1 and f2 with an expected error of

1 0

s

-

s2

ds

=

1/6.

However,

1

cannot

fit

f3

or

f4

to

-precision,

since for f3 the expected error is

1 0

s

+

0.01s2

-

0.02s

ds

=

0.493

>

and the expected error for f4 is even larger.

The function 2(s) = 0.01s can fit both f3 and f4 with an error of 0.01

1 0

s

-

s2

ds

=

1/600.

Example 2. State-action pair representations for changing transition dynamics. In this example, a set of 6 reinforce-

ment learning tasks have different transition dynamics but are otherwise the same, with state space S = {0, 1, 2, 3, 4},

action space A = {-1, +1}, the initial state s0 = 0, and the reward function r gives a reward equal to the number of

successive steps going successfully in one direction. Each task in F = {1,2, 2,3, 3,4, 3,22,1, 1,0} is identified by its unique transition model. For all i,j  F , the transition model T i,j defines T i,j (s, a) = max(0, min(s + a, 4))

except that transitioning from state i to state j is not allowed.

Each task   F has reachable states, R+( ), states that can be repeatedly visited until the end of the task, as well as

unreachable states, R-( ), and the expected reward varies across tasks. For example, in 2,3 the optimal task-specific

strategy is to take a = +1 until s = 2, take a = -1 until s = 0, etc. because state s = 3 is not reachable. The optimal

expected reward is (0 + 1)/2 = 0.5, (0 + 1 + 2)/3 = 1, or (0 + 1 + 2 + 3)/4 = 1.5, respectively, depending on the

number of reachable states. Executing a policy optimal for one task  will be sub-optimal for another task  , with

an expected reward of 0 when the agent will attempt to keep moving either forward or backward but get stuck, or an

expected reward of

N -1 i=0

i N

,

where

N

=

|R+( )|,

as

the

agent

will

go

back

and

forth

over

the

smaller

path

spanned

by R+( ). A more expressive reinforcement learner with memory of the previous state could solve multiple tasks

satisfactorily by switching direction if it observes its state is the same as before. Due to extending the path with one

additional

time

step

of

standing

still,

the

expected

reward

would

be

reduced

by

a

factor

N N +1

compared

to

optimal,

where N is the number of reachable states for the task to be solved. Therefore, a single policy would be able to solve

tasks for which N  3 with a loss of L()  R where = 0.25, but not other tasks with N < 3, which would come

with greater loss. The number of policies needed is therefore lower-bounded by N  2. However, by defining policies

which move back and forth in [0, 1], [0, 2], [0, 3], [1, 4], [2, 4],and [3, 4], the setting N = 6 provides an upper bound to

the required policies as this solves all the tasks optimally. Therefore, the corresponding task capacity is bounded by

1  C  3.

Task cluster analysis of the 27-task cartpole domain When exact arguments are not possible, as is the case for deep-reinforcement learners in relatively complex tasks, the theoretical task capacity can be roughly approximated by identifying distinct clusters of tasks. This cluster analysis is now exemplified for deep reinforcement learners on the 27-task Cartpole domain, a domain that will be revisited in the empirical experiments of this paper.
The Cartpole domain includes different MDPs characterised by different transition dynamics. In this domain, a cart must balance a pole placed on its top by moving left or right with fixed force of 1 N. Each time step is rewarded with

7

Lifetime policy reuse and the importance of task capacity

A PREPRINT

+1 but the episode terminates when (a) the pole has an angle of more than 15 from vertical; (b) when the distance of the cart to the centre is greater than 2.4 m; or (c) 200 time steps are completed, in which case the solution is considered optimal. 27 different tasks are formed in this domain by varying pole length, pole mass, and cart mass. In general, the optimal strategy is to move left when the pole turns left and to move right when the pole turns right. Many actions will come with equal reward of +1, i.e. all those that do not finish the episode.
A single deep reinforcement learning policy can represent a single task accurately since it can map arbitrary states onto arbitrary low-entropy probability distributions over actions. However, by Corrolary 2, when different MDPs have different transition dynamics, a subset of such tasks will make the policy reach states in which its action is suboptimal. In the Cartpole domain, such suboptimal states can only be the states preceding the above-mentioned terminal states of the type (a) and (b); these states are followed by a reward of 0 while all others are followed by a reward of 1. Therefore, relevant differences in policy requirements across tasks can be identified by observing for each task which states lead to cases (a) and (b) and how frequently.
A computationally cheap experiment is performed, requiring only a few minutes runtime. The experiment tests a random uniform policy on all 27 tasks for 600,000 time steps each, which amounts to 20,000 to 60,000 episodes depending on task difficulty. Since case (b) is observed rarely (at most 15 times), while case (a) happens between 17,000 and 30,000 times, depending on the length of the cart, the focus is on which states precede event (a) and how frequently. In this analysis, clear differences in the angular velocity and the episode length before event (a) are observed across tasks. The cartpole problem is symmetric, so the lowest absolute angular velocity is computed as a boundary between the safe and unsafe regions. With this methodology, 8 clusters of nearby points are observed, with means {(0.026, 17.5), (0.026, 30), (0.0125, 30), (0.013, 17.5), (0.006, 17.5), (0.006, 30), (0.004, 17.5), (0.004, 30)}. Consequently, for epsilon-optimality on each task, a rough estimate of the number of policies required is N  = 8, for a theoretical task capacity of C = 3.4.

5.3 Empirical task capacity
The theoretical task capacity focuses on representation only: it is not always possible to actually find the library of epsilon-optimal policies, even if they can be represented within the space of policies defined by the hypothesis class H chosen by the user. First, there is a trade-off between representational capacity and sample complexity, sometimes called expressive efficiency, meaning methods with higher capacity require more data [Sontag, 1998, Cohen et al., 2017]. Second, the data acquired must fairly represent the function to be approximated, which is not guaranteed in reinforcement learning, because the exploration technique inherently determines the data provided to the learner. Third, the training procedure itself exhibits an inductive bias, preferring some generalisations over others. For example, gradient-descent methods [Rumelhart et al., 1986] cannot easily escape local optima in non-convex landscapes, which makes the algorithm favour certain patterns over others. Finally, for lifetime policy reuse systems there is the general temporal structure of task-blocks, consequent presentations of the same type of task that are limited in time, do not guarantee convergence, and follow each other in random order; how exactly these are presented is a major determinant in the success of the system.
For the above reason, an empirical task capacity metric is defined that accounts for learnability. In addition to H, F, and , the metric is also dependent on the temporal structure of the environment and the training algorithm of the base-learner. Assuming epsilon-optimal policies can be represented for each task by task-specific learners, which is a safe assumption for deep reinforcement learners, any sub-optimality of the task-specific learner, which has a one-to-one mapping of policies to tasks, must be due to learnability factors. Therefore, the practical task capacity is based on the task-specific learner as a reference point. This leads to a notion of relative epsilon-optimality for a base-learner, where its one-to-one mapping of policies to tasks represents optimality for that base-learner. Further, a multi-task epsilon-optimality is defined, such that instead of requiring epsilon-optimality for all tasks, one requires epsilon-optimality, on aggregate, across tasks. This is mainly based on practical user-requirements, since the user is interested in optimising the lifetime cumulative rewards; if cumulative reward is improved by only learning tasks with high reward gains optimally, as opposed to attemping to learn all tasks, then this is overall a better lifetime strategy.

Definition Based on the above considerations, base-learners with high empirical task capacity are those for which a low number of policies results in a performance close to, or even better than, a one-to-one mapping of tasks to policies. The empirical task capacity Cemp is defined in Equation 5,

Cemp = N /N ,

(5)

where N is the total number of tasks and N is the lowest number of policies that yields

R  (1 - c)R1-to-1 ,

(6)

8

Lifetime policy reuse and the importance of task capacity

A PREPRINT

where R1-to-1 is the lifetime average performance of the one-to-one mapping with task-specific policies. Learners with high Cemp and lower c can solve on average a number of Cemp tasks with a loss of at most c percentage of R1-to-1.

The choice of c is ultimately up to the end-user, based on the trade-off between resources used and performance.

However, to provide a complete evaluation of a base-learner, as a benchmarking tool, Equation 7 defines the Integrated

Task Capacity (ITC),

1

ITC = Cemp( c)d c ,

(7)

0

which integrates the task capacity over the possible values of c  [0, 1].

Suggested usage The empirical task capacity is proposed for two use cases. First, it can be used to compare baselearners over a domain and observe which base-learner scales best over tasks. Second, it can be used, analogous to the theoretical task capacity, as a tool for pre-selecting the number of policies. In this case, the user selects a task sequence that represents the application of interest and then estimates the optimal number of policies. The experiments in this paper focus on the first case due to restrictions in computational budget, although this still includes challenging task sequences compared to state-of-the-art, involving 18 to 27 tasks that are randomly presented and including POMDPs.

6 Experiment setup
6.1 Base-learners
Two base-learners, Deep Q-Network and Proximal Policy Optimisation, are compared in the study. They represent two classes of state-of-the-art deep reinforcement learning methods, value-based learners and actor-critic learners. In both cases, a single actor is chosen instead of distributed reinforcement learning with many actors, because in a realistic, continual lifelong learning setting the learners are not able to perfectly simulate copies of their environment. Their architectures are chosen to be as similar as possible, as depicted in Figure 2.

Actor (5)

Dense RELU (80)

LSTM tanh (80)

Dense RELU (80)

LSTM tanh (80)

(a) DRQN

Q (1)

(b) PPO

Critic (1)

Figure 2: Architectures used in the experiments. A single arrow represents connections between each unit in the outgoing layer and each unit in the subsequent incoming layer. For MDP tasks, the LSTM layer is replaced by a second Dense RELU layer, also of 80 neurons.

Deep (Recurrent) Q-Network For MDP tasks, we investigate Deep Q-Network [Mnih et al., 2015], a state-of-the-art value-based reinforcement learner suitable for discrete action spaces and complex state spaces, as is the case for example in Atari game environments. Its policy depends on a state-action value, and its updates are done in an off-policy style, allowing the training to be independent of the current policy and to repeat events in the distant past. For POMDP tasks, DRQN is chosen due to its track record in partially observable video games [Hausknecht and Stone, 2015, Schulze and Schulze, 2018, Lample and Chaplot, 2017, Chaplot and Lample, 2017]. DRQN extends Deep Q-Network with a different experience replay method that samples traces of observations rather than a single observation. This allows it to learn from sequences of observations with a Long Short-Term Memory [Hochreiter and Schmidhuber, 1997] layer, making it suitable for partially observable environments. A key question in both learners is whether negative transfer results in DQN-based, and other Q-learning-based lifelong learning systems [Li et al., 2018, Kirkpatrick et al., 2017], can be replicated in other types of tasks and whether these results also hold for DRQN.
At the start of the episode, recurrent networks here perform burn-in initialisation [Kapturowski et al., 2019], where a number of initial actions are not taken by the network but are simply used to initialise the trace of the recurrent network.
9

Lifetime policy reuse and the importance of task capacity

A PREPRINT

Then periodically, updates are performed using experience replay. In DQN, updates are based on Q-learning [Watkins and Dayan, 1992] and the loss function is defined in Equation 8,

L() = E(s,a,r,s )U(B)[(r +  max Q(s , a ; ^) - Q(s, a; ))2] ,

(8)

a

where experience tuples (s, a, r, o ) of observation, action, reward and next action, are sampled uniformly from a buffer B, and where a factor  discounts future rewards. The policy's parameters are , which are updated frequently, whilst another set of policy parameters ^ are synchronised infrequently on a periodic basis to match the policy's parameters and are used as the target for the neural network. In DRQN, the updates and the loss function are the same except that the sampled states are traces of observations which are passed through an LSTM layer such that the system remembers previous time steps. In the present paper's experiments, the buffer is treated as part of the policy, and therefore each policy will have a separate experience buffer; this ensures that experiences are sampled only from the subset of tasks on which the policy specialises. The convolutional layers are replaced by a single densely connected layer due to the small observation without spatial correlations. Further details of its implementation, including the hyper-parameters, are given in Appendix A.

Proximal Policy Optimisation with/without LSTM network Proximal Policy Optimisation (PPO) [Schulman et al., 2017] is a policy optimisation algorithm which is often applied as an actor-critic reinforcement learner. PPO is chosen because of its robustness and transfer learning benefits [Burda et al., 2018, Heess et al., 2017, Nichol et al., 2018]. It applies a clipped loss function, shown in Equation 9, that takes into account that the policy updates should not be too large, to allow monotonic improvements,

L() = min(gtA^ t, clip(gt; 1 - , 1 + )A^ t) ,

(9)

where A^ t is the advantage estimated at time t by Generalised Advantage Estimation [Schulman et al., 2016], gt =

(at|ot, )/(at|ot, old) is the ratio of the probability of the chosen action at according to the new parameters 

and the old parameters old, and clip(x; y, z) clips the number x to the actor-critic style, in which a critic learns the value of a given observation

interval [y, z]. according to V

This (ot)

method is

= E[

 i=0

applied in an irt+i], with

rt the reward at time t, and in which the actor learns (at|ot, ), the probability of the chosen action in the observation

ot, based on the current policy parameters . Analogous to the DQN vs DRQN setup, the neural network architecture is

supplemented with a Long Short-Term Memory layer [Hochreiter and Schmidhuber, 1997] for POMDP tasks and not

for MDP tasks, and the start of the episode is based on burn-in initialisation [Kapturowski et al., 2019] when the LSTM

layer is used. The architecture differs from DRQN in the sense that there are two output layers, one for the actor and

one for the critic, that share their connections to the LSTM layer. Further details of its implementation, including the

hyper-parameters, are given in Appendix A.

6.2 Learning metrics

To assess forgetting and transfer, we now construct a forgetting ratio and a transfer ratio. The ratios are similar to transfer learning metrics and are directly based on the performance after transitioning from earlier tasks to the task of interest Bieger et al. [2016], Taylor and Stone [2009]. The metrics are comparable to the area ratio Taylor and Stone [2009], a metric for transfer learning that compares the performance of the policy that has seen one or more prior tasks to a policy that is randomly initialised and learns from scratch without any experience on other tasks. Like the area ratio, our metrics integrate the performance across the entire task-block to capture the complete learning behaviour, contrasting to jumpstart, time to threshold, and asymptotic performance. Due to the existence of many task-blocks of the same task, the proposed transfer and forgetting ratios are first computed for all task-block transitions across the lifetime. Forgetting ratios are then aggregated for different bins of the number of interfering task-blocks while transfer ratios are aggregated for different bins of the number of prior task-blocks.

The forgetting ratio is defined in Equation 10 as

 - 1-to-1

forgetting ratio =

,

(10)

area-uniform-random

where  = area-after-area-before is the improvement in performance area under the curve comparing the performance on the current task-block to that of the prior task-block of the same task, 1-to-1 is the analogous improvement for the
1-to-1 policy selection, and area-uniform-random is the performance area under the curve for a uniform random policy.

The transfer ratio is defined in Equation 11 as

transfer ratio = area-after - 1-to-1-area ,

(11)

area-uniform-random

10

Lifetime policy reuse and the importance of task capacity

A PREPRINT

where area-after, or area-with-transfer, is the performance area under the curve for the current task-block and 1-to-1-area is the performance area under the curve for the 1-to-1 policy. The 1-to-1 policy performance on the task-block is equal to the area-without-transfer used in the area ratio since the transfer ratio is only computed for the first presentation of the target task.
The transfer ratio differs from the area ratio in the use of the uniform random policy performance as the denominator; this compares different base-learners in the same units and avoids the sensitivity to scale mentioned in Taylor and Stone [2009]. The forgetting ratio also is formulated in the same base-learner independent scale. Another consideration for the forgetting metric is to avoid sequential effects. First, if the performance in the previous task-block was used as the denominator, then consequent task-blocks after a completely forgotten task could potentially have a performance near zero, giving a high score and dominating the aggregated forgetting ratio; the base-learner independent scale avoids this problem. Second, improvement in performance over the previous task-block may not mean much in itself due to the sequential effects, rather, the correction for the 1-to-1 policy's improvement 1-to-1 represents the expected transition improvement with no forgetting. After aggregation of task transitions for bins of the number of prior task-blocks, the forgetting ratio determines the effect of interfering task-blocks on performance. The transfer ratio has no sequential effects, as only the first task-block is being assessed for any given task. Therefore, the transfer ratio can also be interpreted as is, without aggregation.
Policy spread An additional metric, called the policy spread, estimates the variability in the policies during the lifetime of a multiple policy approach. In both PPO and DRQN, the policy is a function of the parameter space and the observations, and therefore, when different policies are spread widely in parameter space, this does not necessarily imply a wide spread in the policies. Therefore, the variability in the policies is assessed empirically by randomly sampling viable observations and determining the selected action probabilities. In PPO, this is directly based on the output of the actor-network which outputs a probability for each action a  A. In DRQN, this is based on the -greedy exploration, which first obtains the action amax = arg maxa Q(ot, at) and then assigns the probability 1 - + /|A| for amax and the probability /|A| for actions other than amax. The calculation of the spread is based on the total variation distance, a distance metric for probability distributions, applied to all pair-wise combinations of the policies' action probabilities in an experimental condition.
6.3 Lifelong learning domains
The experimental setup tests the impact of the number of policies on performance, probing in particular the effects of changing reward functions and transition dynamics as these limit the scalability (see Corrolary 1 and 2). The experiments are conducted in a 27-task Cartpole MDP domain, in which a feedforward DQN and PPO are used, and an 18-task POcman POMDP domain, in which case an LSTM layer is added for a recurrent version of PPO and DRQN.
These domains are chosen to illustrate the challenge of lifetime policy reuse, involving randomly chosen tasks presented in rapid succession. In both domains, the learner does not have time to converge its parameters, memories of earlier tasks can be lost by the time they are presented again, and policies learned on one task may transfer negatively to other tasks. The differences between these two domains help to analyse critical factors underlying its success: (i) changing reward functions; (ii) changing transition dynamics; (iii) partial observability; and (iv) task similarity. The MDP domain has a relatively large task-similarity and has full observability. The POMDP domain has lower task-similarity and has changes in reward functions, which combined with the partial observability makes it challenging to infer the task from within the policy; this contrasts to Atari and other video games where a single observation is sufficient to recognise the task. Such partial observability is often true in the real world, as illustrated in the introduction of the paper, where opposite policies are often required but this is unknown to the learner due to the effect of hidden variables. Therefore, the POMDP domain allows to assess strategies for lifetime policy reuse in low task-capacity scenarios.
27-task Cartpole MDP domain In the 27-task Cartpole MDP domain, reinforcement learners are tasked with moving a cart back and forth to balance a pole for as long as possible. The domain includes varying transition dynamics but not varying reward functions. 27 unique tasks are based on varying properties of the cart and the pole: (i) the mass of the cart varies in {0.5, 1.0, 2.0}kg; (ii) the mass of the pole varies in {0.05, 0.1, 0.2}kg; and (iii) the length of the pole varies in {0.5, 1.0, 2.0}m. Due to the tasks being relatively similar, this domain represents an optimistic evaluation of task capacity.
The lifetime of the learner starts at t = 0 and ends at t = T , where T is chosen to be 40.5 million time steps. At each time step, the learner receives an observation x, , x ,  , where x and x are the position and velocity of the cart, and  and  are the angle of the pole to vertical and its rate of change. The learner is equipped with a fixed set of 2 actions, AE = {left, right}, moving the cart left or right by applying a force of -1 N or +1 N. The observations and actions follow the Markov property such that P(ot|ot-1at-1 . . . o1at) = P(ot|ot-1at-1). Each time step is rewarded with +1
11

Lifetime policy reuse and the importance of task capacity 0 5 0 6 10 . . . 1 2 2 9 17 1 6 1 7 11 . . . 2 3 3 10 18

A PREPRINT time time

17 4 17 5 9 . . . 0 1 1 8 16
time
Figure 3: Illustration of the task sequences assuming a number of tasks N = 18. To form the zeroth task sequence, task indices are sampled randomly in {0, 1, . . . , N - 1} for each successive task-block. To form the n'th task sequence, where n > 0, the task index is chosen for each task-block as j = (i + n) mod N , where i is the task index in the zeroth task-sequence. Thus all tasks are represented exactly once when taking a vertical slice across sequences at any given time-point; apart from varying the task-order this also facilitates analysis due to a smooth aggregate development curve over time. N = 18 represents the 18-task POcman POMDP domain. For the 27-task Cartpole MDP domain, the above illustration applies after setting N = 27.
but the episode terminates when either the pole has an angle of more than 15 from vertical or when the distance of the cart to the centre is greater than 2.4 m. The episode is also terminated after 200 time steps of balancing as in this case the learner is considered to be successful.
Tasks are presented in a random sequence. Each unique task has multiple task-blocks, each of which is presented in a random order, such that each successive block of a given task likely has task-blocks of many other tasks between them. To account for the variability in the experiments due to the stochasticity in the actions selected by the reinforcement learner, as well as the random task presentation, the learners are evaluated based on 27 independent runs. Each independent run consists of 40.5 million time steps and corresponds to a unique task sequence. Each task sequence consists of 675 task-blocks. Each task-block consists of 60,000 time steps, or at least 300 episodes, of the same given task. With 27 unique tasks, this allows on average 25 blocks per task. Each episode takes at most 200 time steps, depending on the success of the learner. To minimise the effects of task order, the tasks are spread evenly between the different task sequences, as is illustrated in Figure 3.
18-task POcman POMDP domain In the 18-task POcman POMDP domain, reinforcement learners interact with an environment that contains an object of interest and has an unknown grid-world topology. The domain includes varying transition dynamics, varying reward functions, and partial observability.
The domain consists of 18 unique tasks, each of which is parametrised by properties of the object of interest and the grid-world topology. Small-scale grid-world topologies, with small state and observation spaces, are used; this limits the computational expense and allows the partially observable problems to be learned within a fairly limited number of experiences. The grid-world topologies correspond to the cheese-maze [McCallum, 1995], Sutton's maze as mentioned in [Schmidhuber, 1999] and a 9-by-9 version of the partially observable pacman (POcman) [Veness et al., 2011].
As illustrated in Figure 4, the environment's 18 distinct tasks are based on three dimensions, (reward, movement, topology), where: reward  {-1., 1.} is the reward for touching the main object in the task; movement  {0, 1, 2} is a dimension specifying the speed of the object of interest, with 0 indicating static, 1 indicating a random step (north, east, south or west) once every 20 time steps, and 2 indicating a strategy which reacts each time step to the reinforcement learning agent, by moving away defensively in case reward = 1., or by aggressively moving towards the agent in case reward = -1.;1 topology  {0, 1, 2} is the grid-world topology corresponding to the cheese-maze, the Sutton maze, and the POcman problem. When the object is static, the learner must find the location with the object, if reward = 1., or without the object, if reward = -1. When the object is dynamic, the learner should either follow the object as closely as possible, if reward = 1., or stay away at a safe distance, if reward = -1.. Objects are never removed when the agent touches them but continue to be a source of
1The defensive strategy is to move away with a 50% probability and otherwise to stand still, and the aggressive strategy is to move towards the agent with a 50% probability and otherwise to take a random action.
12

Lifetime policy reuse and the importance of task capacity

A PREPRINT

reward, allowing more frequent contact between the agent and the object of interest until the elementary task ends. To ensure a certain difficulty level and emphasise a searching strategy rather than simple goal achievement, the initial coordinates of the object of interest are not fixed over time, but are randomly chosen from a set of (x, y)-coordinates; see Appendix A for starting coordinates.

(a) positive, dynamic, cheese maze

(b) negative, dynamic, POcman maze

(c) positive, static, Sutton maze

(d) negative, static, Sutton maze

Figure 4: Illustration of various tasks based on three defining task characteristics. (a) in a positive dynamic task, the learner must touch the defensively moving ghost; (b) in a negative dynamic task, the learner must avoid the aggressively moving ghost; (c) in a positive static task, the learner must touch the food pellet; (d) in a negative static task, the learner must avoid touching the poison bottle. Note that the objects are illustrated in this way purely for interpretation, whereas the learner perceives the objects as the same observation on all tasks.

The lifetime of the learner starts at t = 0 and ends at t = T , where T is 90 million time steps. The learner is equipped with a fixed set of 5 actions: A = {north, east, south, west, stay}. Observations obtained from the environment consist of 11 bits of either 1 or -1, which are similar to those in POcman [Veness et al., 2011]: the first four indicate for each direction in a Von Neumann-neighbourhood whether the position contains an obstacle or not; the next four check, also for each direction in a Von Neumann-neighbourhood, whether the position contains the object of interest; and the final three bits indicate whether or not the object is within a Manhattan distance of 2, 3 or 4 steps from the learner. After each 1000 time steps the learner is reset to the starting location, but no terminal states exist. In the first unroll = 15 time steps after being reset, any external action is chosen randomly by the learner to allow the learner to form an initial trace of observations for its policy.
As in the MDP task sequences, independent runs account for the stochasticity in the learner and environment and task order effects are minimised by spreading tasks evenly between the different task sequences. However, in the POMDP domain, each task sequence consists of 450 task-blocks with 200 episodes of the same given task and each such episode takes 1,000 time steps.
6.4 Experimental conditions
A parametric study compares the effect of the number of policies and the inclusion of adaptivity into the policy selector.
To study the effect of adaptivity, the adaptive policy selector used in lifetime policy reuse (see Section 4.2) is compared to a fixed, unadaptive mapping of tasks to policies. The unadaptive policy selection uses a fixed deterministic mapping of each task to a particular policy. This leads to three special cases. First, for N = 1, a single policy is used throughout. Second, for N < N , where N is the number of tasks, a single policy is selected for a partition of tasks; these partions are determined at the start of the lifetime by randomly shuffling tasks, with the number of tasks being as balanced as possible across policies. Third, the special case N = N is equivalent to commonly used task-specific policies.
13

Lifetime policy reuse and the importance of task capacity

A PREPRINT

Average score Average score

200

200

175

175

AdaptivePPO2P

UnadaptivePPO2P

AdaptivePPO4P

UnadaptivePPO4P

150

150

AdaptivePPO9P AdaptivePPO14P

UnadaptivePPO9P UnadaptivePPO14P

UnadaptivePPO1P

UnadaptivePPO27P

125

125

100

100

75

75

50

50

AdaptiveDQN2P

UnadaptiveDQN2P

25

AdaptiveDQN4P AdaptiveDQN9P

UnadaptiveDQN4P UnadaptiveDQN9P

25

AdaptiveDQN14P

UnadaptiveDQN14P

00

UnadaptiveDQN1P
1 ti2me

UnadaptiveDQN27P

3

14e7

00

1

ti2me 3

14e7

(a) DRQN

(b) PPO

Figure 5: Average score (Mean ± Standard Error) in the 27-task Cartpole MDP domain depending on the number of policies, where dashed lines indicate learners with unadaptive policy selection. The average score is averaged every 25 consecutive task-blocks, or every 1,500,000 time steps.

The number of policies and the policy selection methodology are tested by varying the number of policies and the adaptivity. Unadaptive policy selection include a number of policies N  {1, 2, 4, 9, 14, 27} for the 27-task MDP domain and N  {1, 2, 4, 9, 18} for the 18-task POMDP domain. The adaptive policy selection conditions are the same, except the 1-to-1 policy selection and the 1-policy condition are removed as adaptivity is not useful in these cases.
In the results, the above-mentioned conditions will be abbreviated based on three properties: (i) adaptivity, either Adaptive or Unadaptive; (ii) the base-learner, either PPO or DQN/DRQN; and (iii) the number of policies followed by P. For instance, AdaptivePPO14P indicates a 14-policy PPO learner with adaptive policy selection.
Experiments are conducted on the IRIDIS4 supercomputer [University of Southampton, 2017] using a single Intel Xeon E5-2670 CPU (2.60GHz) with a varying upper limit to RAM proportional to the number of policies (approximately 2NGB). Each run lasts for 2 days for the 27-task MDP domain and 6-9 days for the 18-task POMDP domain. The code for the experiments is available at https://github.com/bossdm/LifelongRL.
7 Performance evaluation
7.1 27-task Cartpole MDP domain
The development of the average score in the 27-task Cartpole MDP domain is shown in Figure 5 for different settings of the number of policies. The following main trends are observed:
· For DRQN, settings of the number of policies N  9 confer a stable upward curve over time, while settings N  4 demonstrate a rapid increase at first but then have degrading performance over time. With the optimal performance being 200, settings of N  9 converge to an optimal or near-optimal performance, accounting for the need for exploratory actions.
· For PPO, settings of the number of policies N  4 confer a stable upward curve over time towards a high but suboptimal performance between 75 and 100; within this group of settings, N = 27 reaches a similar final performance but is considerably slower to converge. Settings N  2 demonstrate a degrading performance over time, although the effects are much less pronounced compared to DRQN.
· Overall DRQN demonstrates rapid and pronounced performance improvements while PPO learns more slowly but more stably.
14

Lifetime policy reuse and the importance of task capacity

A PREPRINT

· Adaptivity has a small negative effect on the performance.

The average and final performances are summarised in Table 1. For DQN, the best performances are reached by 14- and
27-policy settings, with an average lifetime performance of around 150 and a final performance of 170­180. The 1-to-1 27-policy significantly outperforms the conditions where N  2.2 However, UnadaptiveDQN4P and settings of 9 or 14 policies, except the AdaptiveDQN14P (p = 0.994), significantly outperform the 1-to-1 27-policy learner . For
PPO, all the Unadaptive conditions outperform the 1-to-1 27-policy learner; this effect is significant for all Unadaptive conditions except UnadaptivePPO1P, but not significant for Adaptive conditions (p ranging from 1.0 for N = 2 to 0.02 for N = 14). PPO's 1-policy approach performs lower than the 1-to-1 policy but not significantly so (p = 0.763).

Table 1: Performance in the 27-task Cartpole MDP domain as a function of the number of policies. The lifetime average score, and the final score, averaged across the last 10 task-blocks, or the last 600,000 time steps, are shown as the Mean ± Standard-deviation across the 27 independent runs. Scores are sampled once every 60,000 time steps.

(a) DQN

Performance

Method

lifetime

AdaptiveDQN2P AdaptiveDQN4P AdaptiveDQN9P AdaptiveDQN14P UnadaptiveDQN1P UnadaptiveDQN2P UnadaptiveDQN4P UnadaptiveDQN9P UnadaptiveDQN14P UnadaptiveDQN27P

109.2 ± 31.9 147.5 ± 16.1 153.9 ± 7.5 147.9 ± 5.6 71.8 ± 35.9 118.8 ± 31.1 156.2 ± 9.3 161.5 ± 3.3 155.8 ± 2.8 147.9 ± 2.6

final 34.7 ± 41.4 121.3 ± 35.6 163.7 ± 21.8 169.7 ± 14.6 25.1 ± 44.1 56.2 ± 47.6 123.2 ± 35.3 176.0 ± 12.7 179.5 ± 9.3 179.5 ± 8.4

(b) PPO

Performance

Method

lifetime final

AdaptivePPO2P AdaptivePPO4P AdaptivePPO9P AdaptivePPO14P UnadaptivePPO1P UnadaptivePPO2P UnadaptivePPO4P UnadaptivePPO9P UnadaptivePPO14P UnadaptivePPO27P

68.6 ± 6.6 72.5 ± 9.6 72.9 ± 7.7 72.1 ± 4.7 67.7 ± 9.1 78.1 ± 9.9 80.2 ± 6.0 80.5 ± 4.3 75.5 ± 4.2 68.5 ± 2.6

55.5 ± 28.4 71.0 ± 23.4 73.5 ± 23.2 80.1 ± 13.7 58.4 ± 38.8 63.5 ± 31.8 77.1 ± 26.5 84.1 ± 21.0 90.5 ± 19.0 84.5 ± 11.6

Cumulative reward Cumulative reward

3.0 1e7

2.5

AdaptiveDRQN2P AdaptiveDRQN4P

AdaptiveDRQN9P

UnadaptiveDRQN1P

2.0

UnadaptiveDRQN2P UnadaptiveDRQN4P

UnadaptiveDRQN9P

UnadaptiveDRQN18P

1.5

3.0 1e7

2.5

AdaptivePPO2P AdaptivePPO4P

AdaptivePPO9P

UnadaptivePPO1P

2.0

UnadaptivePPO2P UnadaptivePPO4P

UnadaptivePPO9P

UnadaptivePPO18P

1.5

1.0

1.0

0.5

0.5

0.0 0

2

t4ime 6

8 1e7 0.0 0

2

t4ime 6

8 1e7

(a) DRQN

(b) PPO

Figure 6: Cumulative reward (Mean ± Standard Error) in the 18-task POcman POMDP depending on the number of policies, where dashed lines indicate learners with unadaptive policy selection.

2All significance values reported in this paper are based on pair-wise F -tests between the conditions, and p < 0.01 is considered as statistically significant.
15

Lifetime policy reuse and the importance of task capacity

A PREPRINT

7.2 18-task POcman POMDP domain
The development of the cumulative reward in the 18-task POMDP POcman domain is shown in Figure 6 for different settings of the number of policies. Two differences with the 27-task MDP domain are the positive effect of adaptivity (see Section 8 for a discussion) and the 1-to-1 policy being much higher-performing than settings with 2 or 3 tasks per policy (see Section 9 for a discussion) and it does so at the start of the lifetime as well as the end. However, other trends are comparable to the previous domain:
· The cumulative reward intake improves as the number of policies is increased, with the 18-policy algorithm performing best in both DRQN and PPO.
· For DRQN, increasing the number of policies substantially increases the performance and the single policy's performance even decreases over time.
· For PPO, all learners are able to improve over the lifetime and the differences between conditions are less pronounced than for DRQN; PPO's single policy performance is better but its multi-policy is worse than the corresponding DRQN condition.
· DRQN's best-performing setting, here N = 18, is close to the optimal performance, here 45 million in lifetime cumulative reward, considering the need for exploratory actions.3
The average and final performances are summarised in Table 2. For DRQN, the single policy has a lifetime average performance of 0.016, and including multiple policies improves the lifetime average performance monotonically with increasing number of policies, with a factor ranging between 3, for the unadaptive 2-policy learner with its performance of 0.049, and 20, for the 18-policy learner with its performance of 0.309. All learners with multiple policies, including both adaptive and unadaptive conditions and all settings of N  {2, 4, 9, 18}, are able to improve on the lifetime average performance with significance effects. Including multiple policies is also beneficial for PPO but it has a smaller effect, although unadaptive policy selection with higher settings of the number of policies typically yield significant effects compared to lower settings. However, all conditions have a performance ranging between 0.60-0.90, which is 1-1.5 times the performance of the single policy condition. The single policy PPO performs higher than the single policy DRQN, by a factor 4 for the lifetime average performance, but the multiple policy PPO conditions have a performance 2-3 times lower than the multiple policy DRQN conditions.
The final performance yields results similar to the lifetime average, with two notable exceptions. First, the variability is much higher, resulting in higher p-values and a lower confidence in the results. Second, the single policy approach in DRQN deteriorates strongly, down to 0.002, despite the other final performances being in the range of 0.072, for the unadaptive 2-policy, to 0.323, for the 18-policy DRQN approach. This means that the improvement obtained by including multiple policies is between 35 and 133 times in performance. By contrast, the single policy approach for PPO has not deteriorated and scores similarly to its lifetime average, 0.055.
The adaptive DRQN solutions with 2 and 4 policies significantly outperform the corresponding unadaptive DRQN solution with the same number of policies; however, for the 9-policy DRQN learner there is no significant difference in performance between adaptive, 0.200, and unadaptive learners, 0.220 (p = 0.124). Adaptivity can have a beneficial effect comparable to increasing the number of policies: although comparisons of a larger to a smaller setting of N are significant, the AdaptiveDRQN2P, with its lifetime average of 0.098, does not show a significant difference to the UnadaptiveDRQN4P, with its lifetime average of 0.105 (p = 0.532). Observed data are similar for the final performance, where AdaptiveDRQN4P is not significantly different from UnadaptiveDRQN9P whereas UnadaptiveDRQN4P vs UnadaptiveDRQN9P does give a significant difference.
8 The impact of adaptivity
We hypothesise that adaptivity is beneficial if a particular policy is situated in a bad region of parameter space, in which case selecting one of the alternative policies may rapidly find a favourable location of the parameter space. Due to the positive effect of adaptivity only occurring in the DRQN conditions in the 18-task POMDP domain, this hypothesis would be demonstrated empirically by determining if DRQN has a higher policy spread (see Section 6.2 for its definition) than PPO in the 18-task POMDP domain, but not in the 27-task MDP domain. Empirical results support the hypothesis: in the 18-task POMDP domain, adaptive DRQN conditions have a policy spread within [0.6, 0.7] while PPO has a policy spread within [0.1, 0.3] (see Figure 7); in the 27-task MDP domain, adaptive DRQN conditions have a policy spread within [0.3, 0.4] while PPO has a policy spread within [0.4, 0.45].
3An upper bound for the optimal performance is achieving a reward of 1 in tasks with reward = 1 and a reward of 0 in tasks with reward = -1. With tasks being equally split in reward  {-1, 1}, this implies for a lifetime of 90 million steps that the theoretical upper bound on the lifetime cumulative reward is 45 million.
16

Lifetime policy reuse and the importance of task capacity

A PREPRINT

Table 2: Performance (Mean ± Standard-deviation) in the 18-task POcman POMDP domain as a function of the number of policies.

Two performance metrics, based on the cumulative reward function R(t) =

t  =0

r

,

are

shown

as

the

Mean

±

Standard-deviation

across the 18 independent runs: the lifetime average reward, defined as R(T )/T , where T = 9  107 is the total lifetime of the

learner; and

the

final average

reward, defined as

R(T

)-R(T t

-t)

,

where T

=

9  107

and t

=

2  106.

(a) DRQN

Performance

Method

lifetime

AdaptiveDRQN2P AdaptiveDRQN4P AdaptiveDRQN9P UnadaptiveDRQN1P UnadaptiveDRQN2P UnadaptiveDRQN4P UnadaptiveDRQN9P UnadaptiveDRQN18P

0.098 ± 0.017 0.154 ± 0.027 0.200 ± 0.025 0.016 ± 0.007 0.049 ± 0.017 0.105 ± 0.029 0.220 ± 0.028 0.309 ± 0.021

final 0.104 ± 0.092 0.160 ± 0.104 0.221 ± 0.105 0.002 ± 0.054 0.072 ± 0.104 0.098 ± 0.108 0.236 ± 0.135 0.323 ± 0.124

Method AdaptivePRPO2P AdaptivePRPO4P AdaptivePRPO9P UnadaptivePRPO1P UnadaptivePRPO2P UnadaptivePRPO4P UnadaptivePRPO9P UnadaptivePRPO18P

(b) PPO
Performance lifetime 0.062 ± 0.012 0.065 ± 0.011 0.072 ± 0.017 0.057 ± 0.011 0.063 ± 0.013 0.081 ± 0.017 0.086 ± 0.018 0.088 ± 0.015

final 0.055 ± 0.081 0.070 ± 0.098 0.070 ± 0.088 0.055 ± 0.090 0.071 ± 0.094 0.082 ± 0.093 0.084 ± 0.116 0.107 ± 0.096

The discrepancy is because for highly similar tasks there are similar requirements for the policies, in which case switching policies does nothing more than reducing the number of training iterations for any selected policy. When required policies are highly distinct, continuing to train an initially unsuccessful policy will (a) take many training iterations, and (b) come with the side-effect of reducing the performance on other tasks. In such cases, switching the policy can quickly find a policy with higher performance. The 27-task Cartpole MDP domain requires similar optimal behaviours (e.g., moving left when the angle tilts in the negative orientation, moving right when the angle tilts in the positive orientation) and therefore adaptivity has a small negative effect. By contrast, the 18-task POcman POMDP domain requires more distinct optimal behaviours and therefore adaptivity has a large positive effect.
9 The optimal number of policies?
In both domains, higher settings of N achieve a higher performance. However, in the 27-task MDP domain settings of 1 policy for every 2 to 3 tasks perform comparably to the 1-to-1 policy whereas for the 18-task POMDP domain the 1-to-1 policy is by far the best performer. This section further analyses both domains in terms of how many tasks the unadaptive policy selection methods can efficiently learn. Using performance data from Table 1 and Table 2, we now assess the task capacity, Cemp in Equation 5, with a tolerance set to c = 0.15 to allow sub-optimal but reasonable solutions. To provide a metric independent of c the analysis also includes the integrated task capacity IT C from Equation 7. The forgetting and transfer ratios (see Equation 10 and 11) are also included for further analysis. They are summarised as an aggregate over tasks and analysed separately for different numbers of previously experienced task-blocks. For the forgetting ratio, previous task-blocks are defined as the task-blocks in between two successive presentations of the current task while for the transfer ratio, previous task-blocks are defined as the task-blocks of other tasks before the first presentation of the task. .
9.1 27-task MDP domain
In the 27-task MDP domain, both learners achieve a high task capacity. A single policy PPO is sufficient to solve all tasks according to the relative c-optimality criterion, resulting in a task capacity of Cemp = 27 and an integrated task capacity of IT C = 26.7. DQN requires at least 4 policies with a task capacity of Cemp = 6.75 and an integrated task
17

Lifetime policy reuse and the importance of task capacity

1.0

AdaptiveDRQN2P

AdaptivePPO2P

AdaptiveDRQN4P

AdaptivePPO4P

AdaptiveDRQN9P

AdaptivePPO9P

0.8

A PREPRINT

policy spread

0.6

0.4

0.2

0.0 0

2

4time 6

8 1e7

Figure 7: Policy spread (Mean ± Standard Error) in the 18-task POcman POMDP as a function of time, depending on the number of policies.

capacity of IT C = 18.7. Both learners' high task capacity suggests strong positive transfer although the degrading performance of DQN over time (see Figure 5) indicates the possibility of catastrophic forgetting. In the 27-task MDP domain, positive transfer is observed for all DQN learners. The transfer ratios of DQN learners range between 2 and 7, where 7 is observed for the 1-policy for the maximal number of 30 prior task-blocks of other tasks. The 1-, 2-, and 4-policy DQN learners have a degrading forgetting ratio between -0.5 and -1 as more interfering task-blocks are presented, with -1 for the 1-policy DQN for 30 or more interfering task-blocks. For PPO, as expected from the task capacity, this domain is learnable by only one policy, as all transfer ratios are positive, with fewer policies yielding less transfer, and the forgetting ratios are zero. The lower transfer ratios between 0 and 3 are due to PPO improving at a slower rate than DQN as can be observed from PPO's lower 1-to-1 policy performance in Figure 6. In terms of statistical significance, all the observed effects differ by more than three standard errors from zero and from each other, except for (a) PPO's forgetting ratios being close to zero, and (b) transfer ratios being similar within 1- to 4-policy solutions and within 9- to 14-policy solutions of the same base-learner.
Comparing these results to the theoretical task capacity of C = 3.4, or N  = 8, for deep reinforcement learners in the cartpole domain (see final paragraphs of Section 5), a high agreement is found for DQN but not for PPO. This is not surprising: one-to-one DQN is able to solve all tasks with high accuracy, while the one-to-one PPO does not; therefore, the performance criterion for relative epsilon-optimality for PPO is lowered (see Equation 6). DQN-based systems learn rapidly and are constrained by the representational limits of the theoretical task capacity. PPO-based systems learn more conservatively which reduces their accuracy but also makes them less sensitive to the number of policies as they can take into account data from all tasks and represent them with lowered accuracy. Nevertheless, it is visible that when inspecting the final performances, which would be important when running the same system for lengthier lifetimes, the number of policies for the PPO learner with a very strict relative epsilon-optimality (e.g., c = 0.05) should be 9 as well. The results therefore (a) support a high agreement between the theoretical task capacity and the empirical task capacity; and (b) show that in practice, the number of policies needed can be reduced when the base-learner is more conservative in updating its parameters.
9.2 18-task POMDP domain
The 18-task POMDP domain presents a more challenging set of tasks. On this domain, PPO has a task capacity of Cemp = 4.5 and an integrated task capacity IT C = 13.3, while DRQN has a task-capacity of Cemp = 2 and an integrated task capacity of IT C = 4.1. DRQN's low task capacity suggests a low or even negative transfer ratio; this is supported by the transfer ratios ranging between -0.5 and -1.5. By contrast, PPO has a small but positive transfer ratio between 0 and 1. The degrading performance visible by the near-zero performance of the 1-policy DRQN in Figure 6 further suggests forgetting. For all settings of the number of policies in DRQN, there is evidence for forgetting, with
18

Lifetime policy reuse and the importance of task capacity

A PREPRINT

their forgetting ratios ranging between -0.5 and -1.5 when the number of interfering task-blocks is higher than 30; however, their forgetting ratios increase linearly as the number of blocks is decreased, such that for two consecutive or nearly consecutive presentations of the same task, the forgetting ratio is positive between 0 and 0.5. PPO demonstrates a similar forgetting trend although it has a smaller range, between 0 and 0.5 for non-interfering task-blocks and between -0.5 and 0 for more than 30 interfering task-blocks. In terms of statistical significance, all the observed forgetting effects differ by many standard errors from zero and from each other. However, manipulating the number of policies does not strongly affect the forgetting ratio, except for the following DRQN conditions with more than three standard errors difference: (a) for transitions with 1 to 9 interfering task-blocks, the 1-policy DRQN has a score of -0.25 compared to others ranging in 0.10 to 0.50; and (b) for 10 to 19 interfering task-blocks, the 9-policy DRQN has a score of 0 whereas others have -0.5.

10 The effect of experience replay

The sensitivity of DQN to the number of policies is particularly strong. The question investigated here is whether experience replay plays a role in this sensitivity.

10.1 Size of the buffer
If the size of the replay buffer is not large enough, then this may adversely affect performance. For example, the 400,000 experiences stored in the replay buffer allow training only on the most recent tasks and this may adversely impact being able to solve many tasks. The single-policy DQN is now run with a buffer size of 2,000,000 experiences, which spans 33 task-blocks such that all tasks can potentially be replayed. However, this does not improve performance (see Figure 8).

10.2 Organisation of the buffer

If the organisation of the buffer does not match the overall distribution across the lifetime then DQN may overfit on the new task rather than consolidating previously seen tasks.
200

175

DQN

150

DQN Big buffer DQN Task-matching DQN GDM

125

DQN GDM+FIFO

Average score

100

75

50

25

00

1

ti2me 3

14e7

Figure 8: Effect of experience replay methods on single-policy DQN performance (Mean ± Standard Error) in the 27-task Cartpole domain. GDM indicates global distribution matching; task-matching indicates replaying the current task from a task-specific buffer; and FIFO indicates First-In-First-Out buffer.

A first test of this hypothesis is based on the state-of-the-art selective experience replay method called global distribution matching (GDM) [Isele and Cosgun, 2018], which uses reservoir sampling. In GDM, each experience in the replay buffer is associated with a random number from a normal distribution, r  N (0, 1), and then only those experiences

19

Lifetime policy reuse and the importance of task capacity

A PREPRINT

with the highest r are maintained. Every experience then has a probability of |B|/t of being in the buffer B, where t is the time in the lifetime, regardless of when the experience was added to the buffer; consequently, the global distribution of experiences across the entire lifetime is mimicked. As observable in Figure 8, this does not help to improve the performance, which is comparable to traditional DQN. Similarly, adding a small additional First-In-First-Out (FIFO) buffer to the back of GDM's buffer has no positive effect.
A second test, called task-matching, allows separate buffers for each task and then replays only the experiences from the current task. This also does not improve the performance (see Figure 8).
In summary, all replay methods are within approximately one standard error and this suggests that rather than the replay procedure, the representational argument presented in Section 5 explains the transfer and forgetting properties of DQN.
11 Discussion
This paper presents an approach to policy reuse that scales to lifetime usage, in the sense that (a) its number of policies is fixed and does not result in memory issues; (b) its evaluation is based on the lifetime average performance; and (c) its policies are continually refined based on the lifetime of experience with newly incoming tasks. The approach is widely applicable to MDPs and POMDPs and various base-learners, as is illustrated empirically for DQN and PPO base-learners in cartpole and POcman domains. To help select the number of policies required for a domain, a theoretical task capacity is developed and illustrated for a few examples. An empirical task capacity metric is also proposed with three use-cases: (i) to pre-select, based on theoretical arguments, the number of policies required on a task sequence; (b) to select the number of policies required for a lengthy task sequence (much longer than is included in this paper) based on a limited but fairly representative task-sequence; and (c) to benchmark base-learners in terms of their scalability. The importance of selecting the number of policies is illustrated especially for D(R)QN. Using selective experience replay, or alternative variants, do not improve the empirical task capacity of D(R)QN, implying the representation is the limiting factor rather than the training; this supports the importance of the theoretical task capacity.
Lifetime policy reuse comes with certain benefits but also certain limitations compared to existing alternatives. Compared to other algorithms, lifetime policy reuse can be applied when task sequences involve (i) a rapid succession of randomly presented tasks; and (ii) distinct task clusters each of which allow significant positive transfer within clusters and negative transfer between clusters. Compared to traditional policy reuse [Zheng et al., 2018, Wang et al., 2018, Hernandez-Leal et al., 2016, Rosman et al., 2016, Li et al., 2018, Li and Zhang, 2018, Fernández and Veloso, 2006], the lifetime set-up allows online learning without growing the policy library, requiring policies to converge on the task, or modifications to the base-learner's usual learning algorithm. This allows policies to learn when tasks come in rapid succession and can be integrated with different base-learners. Lifetime policy reuse continually refines its existing policies, overcoming a downside of offline methods for policy reuse (e.g., Rosman et al. [2016]): since the simulation may not represent the real-world environment or as unexpected problems may come up during the application of interest, no suitable policy will be available in the library. However, a challenge that follows is to avoid catastrophic forgetting; the results in this paper show that this can be avoided through appropriate selection of the number of policies guided by the task capacity. As in other policy reuse methods, lifetime policy reuse avoids the use of task-specific parameters (e.g. Deisenroth et al. [2014], Li and Hoiem [2018]) in favour of more uniquely specialised sets of policies and requires instead that the task index is provided to help select the policy. To avoid this requirement, task identification methods could be used. For example, a Hidden Markov Model can predict the task from observations [Kirkpatrick et al., 2017] or an observed change in reward distribution can indicate a new task is being presented [Lomonaco et al., 2020].
The performance and adaptivity findings in the 27-task Cartpole domain and the 18-task POcman domain have significance for reinforcement learning in challenging lifelong learning domains. Performance-wise, both the 27-task MDP domain and the 18-task domain are solved near-optimally by the 1-to-1 D(R)QN learner. Learners with one policy for every two to three tasks can similarly achieve a near-optimal performance despite the challenging domains that may perhaps be paraphrased as an "anti-curriculum": tasks are presented in an unordered sequence, follow in quick succession, and vary in orthogonal dimensions. For the 18-task POcman domain, partial observability further has the detrimental effect of making the different tasks more difficult to learn but also more difficult to distinguish from each other.4 Moreover, due to the presentation of a variety of objects of interest, POcman is extremely challenging since a recent benchmarking study concluded that a variety of meta-/multi-task-RL approaches do not generalise well when the object of interest is varied [Yu et al., 2019]. Therefore, a lower number of policies would likely be needed when learners are given a well-designed curriculum or more similar tasks. Adaptivity-wise, the study shows that use of epsilon-greedy policy selection, balancing exploration and exploitation, can further provide a performance improvement comparable to a doubling of the number of policies, but only if task requirements and the policies in the library vary
4Although anti-curricula do not appear too often in structured man-made environments, they tend to appear often in the animal kingdom or in robotic missions, where environmental changes require rapid adaptation with limited if any cues from the environment.
20

Lifetime policy reuse and the importance of task capacity

A PREPRINT

widely. This finding illustrates the feasibility of adaptively selecting a limited number of policies while refining them; so far, studies incorporating adaptivity over policies have used static policy libraries which are only changed by adding newly converged policies and which are based on Markov Decision Processes, or have used a large amount of policies developed in an off-line stage of learning [Fernández and Veloso, 2006, Rosman et al., 2016, Cully et al., 2015]. In summary, adaptive policy selection with continual refinement based on the lifetime average performance may be a promising solution to solving challenging lifelong learning sequences.
The theoretical task capacity provides a representational argument for why only a limited number of tasks can be solved by a single policy. That is, while a single policy can represent the optimal behaviour required for any one particular task, it can only represent the optimal behaviours for a limited number of tasks due to the conflicting state-action pairs that are required for different tasks. The empirical results show that the theoretical task capacity is one of the key limiting factors. The difficulty of epsilon-optimality over many tasks is illustrated for D(R)QN and PPO as a low-policy PPO could learn all tasks with low accuracy while a high-policy D(R)QN could learn all tasks with high accuracy. PPO's objective function is clipped specifically to stimulate monotonic improvement over time [Schulman et al., 2017]. This slow learning allows time to learn new patterns without erasing previously learned patterns and rather than representing a few tasks optimally, the policy represents a wide set of tasks sub-optimally. By contrast, the D(R)QN objective allows more aggressive weight updates resulting in either a near-optimal performance if a sufficient number of policies allows all tasks to be represented near-optimally, or in strong catastrophic interference and negative transfer otherwise. Other supporting evidence for the importance of theoretical task capacity is that selective experience replay [Isele and Cosgun, 2018] or other modifications to experience replay do not improve single-policy D(R)QN.
12 Conclusion
Reinforcement learners may be subjected to a long sequence of unknown tasks. In this lifelong learning setting, a challenge is to avoid catastrophic forgetting, where learning of previous tasks is undone, and to stimulate selective transfer, such that the knowledge gained on learned tasks can be beneficial on new tasks. Rather than learning a single reinforcement learning policy on all tasks, it may be beneficial to specialise multiple policies each to a subset of the tasks. This paper develops a methodology for lifetime policy reuse, in which a fixed number of policies is continually refined throughout the lifetime. Lifetime policy reuse avoids temporary policies, making it applicable to computationally expensive base-learners such as deep reinforcement learners in challenging lifelong learning sequences. To optimise scalability, in terms of memory, learning speed, and asymptotic performance across the lifetime of incoming tasks, taking the lowest number of policies that achieves near-optimal lifetime performance on all tasks is desirable; therefore, the proposed approach to lifetime policy reuse pre-selects the number of policies based on the newly defined task capacity. Theoretical and empirical task capacity metrics are proposed and illustrated. Lifetime policy reuse is illustrated on 18-task sequences of POMDPs and 27-task sequences of MDPs. Empirical results show that DQN-based learners are more sensitive to the number of policies and that this sensitivity cannot be explained by the replay procedure. Instead, the purely representational argument based on theoretical task capacity explains in large part the catastrophic forgetting observed in both domains and the negative transfer in the POcman domain. With adequate selection of the number of policies based on theoretical or empirical task capacity, lifetime policy reuse of D(R)QN policies achieves near-optimal performance on all tasks and is highly beneficial compared to single-policy D(R)QN; in the POcman domain, for example, the final performance exceeds that of the single-policy DRQN by a factor of 35-133. PPO finds fewer benefits from pre-selection as its slower monotonic improvement allows it to learn on all tasks with lower number of policies but with lower accuracy. In summary, empirical findings validate the approach for lifetime policy reuse and suggest using D(R)QN for large library sizes and PPO for smaller library sizes.
Acknowledgements
This research was funded by the Engineering and Physical Sciences Research Council and by Lloyd's Register Foundation. The authors thank Nicholas Townsend for comments on an early draft of this paper and acknowledge the use of the IRIDIS High Performance Computing Facility and associated support services at the University of Southampton.
References
Matthew E Taylor and Peter Stone. Transfer Learning for Reinforcement Learning Domains : A Survey. Journal of Machine Learning Research, 10:1633­1685, 2009.
Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering, 22(10):1345­1359, 2010. ISSN 10414347. doi:10.1109/TKDE.2009.191.
21

Lifetime policy reuse and the importance of task capacity

A PREPRINT

Alessandro Lazaric. Transfer in Reinforcement Learning: a Framework and a Survey, volume 12. 2013. ISBN 978-3-642-27644-6. doi:10.1007/978-3-642-27645-3. URL http://link.springer.com/10.1007/ 978-3-642-27645-3.
Robert M. French. Semi-distributed Representations and Catastrophic Forgetting in Connectionist Networks. Connection Science, 4(3-4):365­377, 1992. ISSN 13600494. doi:10.1080/09540099208946624. URL https://doi.org/10. 1080/09540099208946624.
Michael E. Hasselmo. Avoiding Catastrophic Forgetting. Trends in Cognitive Sciences, 21(6):407­408, 2017. ISSN 1879307X. doi:10.1016/j.tics.2017.04.001. URL http://dx.doi.org/10.1016/j.tics.2017.04.001.
Sebastian Thrun and Anton Schwartz. Finding Structure in Reinforcement Learning. In Advances in Neural Information Processing Systems, pages 385­392, 1995.
Daniel L. Silver, Qiang Yang, and Lianghao Li. Lifelong Machine Learning Systems : Beyond Learning Algorithms. 2013 AAAI Spring Symposium Series, pages 49­55, 2013.
Zhiyuan Chen and Bing Liu. Lifelong Machine Learning. Morgan & Claypool Publishers, 2016.
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, and Andrei A Rusu. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences of the United States of America, 114(13):3521­3526, 2017. doi:10.1073/pnas.1611835114.
Heechul Jung, Jeongwoo Ju, Minju Jung, and Junmo Kim. Less-forgetful Learning for Domain Expansion in Deep Neural Networks. In The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18), pages 3358­3365, 2017. URL http://arxiv.org/abs/1711.05959.
Zhizhong Li and Derek Hoiem. Learning without Forgetting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(12):2935­2947, 2018. doi:10.1109/TPAMI.2017.2773081.
Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive Neural Networks. arXiv preprint, pages 1­5, 2016. URL http: //arxiv.org/abs/1606.04671.
David Isele and Akansel Cosgun. Selective Experience Replay for Lifelong Learning. In The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18), pages 3302­3309, 2018. URL http://arxiv.org/abs/1802. 10269.
Tom Schaul, Dan Horgan, Karol Gregor, and David Silver. Universal Value Function Approximators. In ICML'15 Proceedings of the 32nd International Conference on International Conference on Machine Learning, pages 1312­ 1320, Lille, France, 2015.
Marc Peter Deisenroth, Peter Englert, Jan Peters, Dieter Fox, and M L Feb. Multi-Task Policy Search. In IEEE International Conference on Robotics & Automation (ICRA), pages 3876­3881, Hong Kong, China, 2014. ISBN 9781479936854.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529­533, 2015. ISSN 14764687. doi:10.1038/nature14236.
Wang Chi Cheung, David Simchi-Levi, and Ruihao Zhu. Reinforcement learning for non-stationary Markov decision processes: The blessing of (more) optimism. arXiv, 2020. ISSN 23318422.
Erwan Lecarpentier and Emmanuel Rachelson. Non-stationary Markov Decision Processes a worst-case approach using model-based reinforcement learning. arXiv, 2019. ISSN 23318422.
Yue Wang, Qi Meng, Wei Cheng, Yuting Liug, Zhi-Ming Ma, and Tie-Yan Liu. Target Transfer Q-Learning and Its Convergence Analysis. AAAI technical report, 2018. URL http://arxiv.org/abs/1809.08923.
Siyuan Li, Fangda Gu, Guangxiang Zhu, and Chongjie Zhang. Context-Aware Policy Reuse. arXiv preprint, 2018. URL http://arxiv.org/abs/1806.03793.
Siyuan Li and Chongjie Zhang. An optimal online method of selecting source policies for reinforcement learning. 32nd AAAI Conference on Artificial Intelligence, AAAI 2018, pages 3562­3570, 2018.
Benjamin Rosman, Majd Hawasly, and Subramanian Ramamoorthy. Bayesian policy reuse. Machine Learning, 104(1): 99­127, 2016. ISSN 15730565. doi:10.1007/s10994-016-5547-y.
Fernando Fernández and Manuela Veloso. Probabilistic policy reuse in a reinforcement learning agent. In International Conference on Autonomous Agents and Multiagent Systems (AAMAS'06), page 720, 2006. ISBN 1595933034. doi:10.1145/1160633.1160762.
22

Lifetime policy reuse and the importance of task capacity

A PREPRINT

Christopher J C H Watkins and Peter Dayan. Q-learning. Machine Learning, 8(3-4):279­292, 1992. ISSN 0885-6125. doi:10.1007/BF00992698. URL http://link.springer.com/10.1007/BF00992698.
Pablo Hernandez-Leal, Benjamin Rosman, Matthew E. Taylor, L. Enrique Sucar, and Enrique Munoz De Cote. A Bayesian approach for learning and tracking switching, non-stationary opponents. In Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS, pages 1315­1316, 2016. ISBN 9781450342391.
David Abel, Yuu Jinna, Yue Guo, George Konidaris, and Michael L. Littman. Policy and Value Transfer in Lifelong Reinforcement Learning. In Proceedings of the 35th International Conference on Machine Learning, pages 1­10, Stockholm, Sweden, 2018. ISBN 9781510867963. URL http://proceedings.mlr.press/v80/abel18b. html{%}0Apapers3://publication/uuid/E87946E1-E773-4F3F-A2ED-DB27A9AA07AB{%}0Apapers3: //publication/uuid/10AF8A01-9E65-4382-AF99-B104574D95F4.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. In Proceedings of the 34th International Conference on Machine Learning, Sydney, Australia, 2017.
A Wilson, A Fern, S Ray, and P Tadepalli. Multi-task reinforcement learning: a hierarchical Bayesian approach. Proceedings of the 24th international conference on Machine learning, pages 1015­1022, 2007. doi:10.1145/1273496.1273624. URL http://dl.acm.org/citation.cfm?id=1273624.
George Konidaris, Ilya Scheidwasser, and Andrew G. Barto. Transfer in reinforcement learning via shared features. Journal of Machine Learning Research, 13(1):1333­1371, 2012. ISSN 15324435. URL http://dl.acm.org/citation.cfm?id=2503308.2343689{%}5Cnhttp://dl.acm.org/ ft{_}gateway.cfm?id=2343689{&}type=pdf.
Sebastian Thrun and Joseph O Sullivan. Clustering Learning Tasks and the Selective Cross-Task Transfer of Knowledge. Technical report, School of Computer Science, Carnegie Mellon University, Pittsburgh, USA, 1995.
Emma Brunskill and Lihong Li. PAC-inspired Option Discovery in Lifelong Reinforcement Learning. In Proceedings of the 31st International Conference on Machine Learning, volume 32, pages 316­324, Beijing, China, 2014. JMLR: W{&}CP. ISBN 9781634393973.
Chen Tessler, Shahar Givony, Tom Zahavy, Daniel J Mankowitz, and Shie Mannor. A Deep Hierarchical Approach to Lifelong Learning in Minecraft. arXiv, pages 1­6, 2016. URL http://arxiv.org/abs/1604.07255.
Siyuan Li, Rui Wang, Minxue Tang, and Chongjie Zhang. Hierarchical reinforcement learning with advantage-based auxiliary rewards. In arXiv, Vancouver, Canada, 2019.
Andrew Levy, Robert Platt, George Konidaris, and Kate Saenko. Learning multi-level hierarchies with hindsight. 7th International Conference on Learning Representations, ICLR 2019, pages 1­16, 2019.
S. Hochreiter, A.S. Younger, and P.R. Conwell. Learning to learn using gradient descent. In Lecture Notes in Computer Science 2130, Proc. Intl. Conf. on Artificial Neural Networks (ICANN-2001, pages 87­94. Springer, 2001. ISBN 3-540-42486-5. doi:10.1007/3-540-44668-0.
Marcin Andrychowicz, Misha Denil, Sergio Gómez Colmenarejo, and Matthew W Hoffman. Learning to learn by gradient descent by gradient descent. In 30th Conference on Neural Information Processing Systems (NIPS 2016), pages 1­17, 2016.
Tianbing Xu, Qiang Liu, Liang Zhao, and Jian Peng. Learning to explore via meta-policy gradient. In 35th International Conference on Machine Learning, ICML 2018, volume 12, pages 8686­8706, Stockholm, Sweden, 2018. ISBN 9781510867963.
Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and Gerald Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing interference. arXiv preprint arXiv:1810.11910, pages 1­31, 2019. ISSN 23318422.
David M. Bossens, Nick C. Townsend, and Adam J. Sobey. Learning to learn with active adaptive perception. Neural Networks, 115:30­49, 2019. ISSN 0893-6080. doi:10.1016/J.NEUNET.2019.03.006. URL https://www. sciencedirect.com/science/article/pii/S0893608019300796.
Abhishek Naik, Roshan Shariff, Niko Yasui, Hengshuai Yao, and Richard S. Sutton. Discounted reinforcement learning is not an optimization problem. In Optimization Foundations for Reinforcement Learning Workshop at NeurIPS 2019, pages 1­7, Vancouver, Canada, 2019.
Eduardo D Sontag. VC dimension of neural networks. NATO ASI Series F Computer and Systems Sciences, 168: 69­96, 1998. ISSN 1098-6596. doi:10.1017/CBO9781107415324.004. URL http://citeseerx.ist.psu.edu/ viewdoc/download?doi=10.1.1.63.9443{&}rep=rep1{&}type=pdf.
23

Lifetime policy reuse and the importance of task capacity

A PREPRINT

Nadav Cohen, Or Sharir, Ronen Tamari, and Amnon Shashua. Analysis and Design of Convolutional Networks. Technical Report May, Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI), 2017.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning representations by back-propagating errors. Nature, 323(5):534­536, 1986. ISSN 0008-4212. URL http://www.ncbi.nlm.nih.gov/pubmed/134.
Matthew Hausknecht and Peter Stone. Deep Recurrent Q-Learning for Partially Observable MDPs. In AAAI Fall Symposium Series, pages 29­37. AAAI, 2015. ISBN 9781577357520.
Christopher Schulze and Marcus Schulze. ViZDoom: DRQN with Prioritized Experience Replay, Double-Q Learning, & Snapshot Ensembling. In Proceedings of SAI Intelligent Systems Conference, pages 1­17, 2018. URL http: //arxiv.org/abs/1801.01000.
Guillaume Lample and Devendra Singh Chaplot. Playing FPS Games with Deep Reinforcement Learning. In Thirty-First AAAI Conference on Artificial Intelligence., pages 2140­2146, 2017. URL http://arxiv.org/abs/1609.05521.
Devendra Singh Chaplot and Guillaume Lample. Arnold: An Autonomous Agent to play FPS Games. Thirty-First AAAI Conference on Artificial Intelligence. Foerster,, pages 2­3, 2017. URL http://arxiv.org/abs/1507.06527.
Sepp Hochreiter and Juergen H. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1­32, 1997. ISSN 0899-7667. doi:10.1162/neco.1997.9.8.1735.
Steven Kapturowski, Georg Ostrovski, John Quan, Rémi Munos, and Will Dabney. Recurrent experience replay in distributed reinforcement learning. 7th International Conference on Learning Representations, ICLR 2019, pages 1­19, 2019.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal Policy Optimization Algorithms. arXiv preprint, pages 1­12, 2017. ISSN 03043878. doi:10.1016/j.jdeveco.2016.04.001. URL http: //arxiv.org/abs/1707.06347.
Yuri Burda, Amos Storkey, Trevor Darrell, and Alexei A Efros. Large-Scale Study of Curiosity-Driven Learning. arXiv preprint, pages 1­15, 2018.
Nicolas Heess, Dhruva TB, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg Wayne, Yuval Tassa, Tom Erez, Ziyu Wang, S. M. Ali Eslami, Martin Riedmiller, and David Silver. Emergence of Locomotion Behaviours in Rich Environments. arXiv preprint, 2017. URL http://arxiv.org/abs/1707.02286.
Alex Nichol, Vicki Pfau, Christopher Hesse, Oleg Klimov, and John Schulman. Gotta Learn Fast: A New Benchmark for Generalization in RL. arXiv preprint, pages 1­21, 2018. URL http://arxiv.org/abs/1804.03720.
John Schulman, Philipp Moritz, Sergey Levine, Michael I Jordan, and Pieter Abbeel. High-dimensional Continuous Control Using Generalised Advantage Estimation. In Proceedings of the International Conference on Learning Representations (ICLR 2016), 2016.
Jordi Bieger, Kristinn R. Thorisson, Bas R. Steunebrink, Trostur Thorarensen, and Jona S. Sigurdardottir. Evaluation of General-Purpose Artificial Intelligence : Why, What & How. In EGPAI 2016 - Evaluating General-Purpose A.I., Workshop held in conjuction with the European Conference on Artificial Intelligence, The Hague, The Netherlands, 2016.
Andrew Kachites McCallum. Reinforcement Learning with Selective Perception and Hidden State. PhD thesis, University of Rochester, 1995.
Juergen H. Schmidhuber. A general method for incremental self-improvement and multi-agent learning. In Xin Yao, editor, Evolutionary Computation: Theory and Applications., volume 1, chapter 3, pages 81­123. World Scientific, 1999. ISBN 9788578110796.
Joel Veness, Kee Siong Ng, Marcus Hutter, William Uther, and David Silver. A Monte-Carlo AIXI approximation. Journal of Artificial Intelligence Research, 40:95­142, 2011. ISSN 10769757. doi:10.1613/jair.3125.
University of Southampton. The Iridis Compute Cluster, 2017. URL https://www.southampton.ac.uk/ isolutions/staff/iridis.page.
Yan Zheng, Zhaopeng Meng, Jianye Hao, Zongzhang Zhang, Tianpei Yang, and Changjie Fan. A deep Bayesian policy reuse approach against non-stationary agents. Advances in Neural Information Processing Systems, pages 954­964, 2018. ISSN 10495258.
Vincenzo Lomonaco, Karan Desai, Eugenio Culurciello, and Davide Maltoni. Continual reinforcement learning in 3D non-stationary environments. IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, 2020-June:999­1008, 2020. ISSN 21607516. doi:10.1109/CVPRW50498.2020.00132.
Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. arXiv, (CoRL):1­18, 2019. ISSN 23318422.
24

Lifetime policy reuse and the importance of task capacity

A PREPRINT

Antoine Cully, Jeff Clune, Danesh Tarapore, and Jean Baptiste Mouret. Robots that can adapt like animals. Nature, 521 (7553):503­507, 2015. ISSN 14764687. doi:10.1038/nature14422.
Matthew D. Zeiler. ADADELTA: An Adaptive Learning Rate Method. arXiv preprint, pages 1­6, 2012. URL http://arxiv.org/abs/1212.5701.
Diederik P Kingma and Jimmy Lei Ba. Adam: A method for Stochastic Optimisation. In International Conference on Learning Representations (ICLR 2015), pages 1­15, 2015.

25

Lifetime policy reuse and the importance of task capacity

A PREPRINT

Appendix A: Implementation details

Starting coordinates of the 18-task POcman domain
Taking x increasing to the east and y increasing to the south, static main objects were initialised randomly from a set of coordinates: {(3, 3), (5, 3)} in the cheese maze; {(6, 1), (9, 1), (9, 6)} in Sutton's maze; {(1, 1), (1, 7), (7, 1), (7, 7)} in the pacman topology. This implies a general search strategy should be used, rather than the memorisation of a single path. Similar to the original tasks, the learner's initial position is (1, 2) for the cheese-maze, (1, 3) in Sutton's maze, and (4, 7) for the pacman task. The initial location of dynamic objects, similar to pacman ghosts, is based on a single home position, which is the central bottom location (3, 3) for the cheese maze, the top-right location (9, 1) for Sutton's maze, and the above-centre location (4, 3) for the pacman topology.

Base-learners

Code from https://github.com/flyyufelix/VizDoom-Keras-RL/ and https://github.com/magnusja/ ppo/ are taken as a template, and then modified to use with multiple policies, and to match the original papers [Hausknecht and Stone, 2015, Schulman et al., 2017]: for DRQN, a slowly changing target value-function; for PPO, the set-up similar to the Atari experiments in [Schulman et al., 2017], where actions are discrete, non-output layers are shared and the objective include additional terms for the value function and the entropy coefficient. Parameter settings mentioned in Table 3 and Table 5 are chosen based on the original papers and any other modifications are chosen based on a limited tuning procedure based on partial runs of the lifelong learning domain; this is illustrated for the learning rate hyperparameter of PPO and D(R)QN in Table 4 and Table 6. No annealing of parameters was done to keep the various conditions comparable. To suit the domains where observations have no spatial correlations, convolutional layers are replaced by a single densely connected layer. In partially observable environments, an LSTM tanh layer [Hochreiter and Schmidhuber, 1997] is used; otherwise this LSTM layer is replaced by a dense RELU layer with equal number of 80 neurons.

Table 3: Parameter settings for the base-learners on the 27-task Cartpole MDP domain. "All" denotes settings common to all base-learners.

Base-learner All DQN
PPO

Parameter hidden layers discount batch size update frequency replay memory size optimisation algorithm momentum learning rate exploration rate clip gradient replay start target update frequency optimisation algorithm learning rate batch size update frequency epochs clip gradient GAE parameter clipped objective coefficient value function coefficient entropy coefficient

Setting 80 (Dense RELU) - 80 (Dense RELU) 0.99 10 once every 4 time steps 400,000 experiences AdaDelta [Zeiler, 2012] 0.95 0.1 0.2 absolute value exceeding 10 50,000 time steps once every 10,000 time steps Adam [Kingma and Ba, 2015] 0.00025 34 once at end of episode 10 norm exceeding 1.0 0.95 0.10 1 0.01

26

Lifetime policy reuse and the importance of task capacity

A PREPRINT

Table 4: Final performance (Mean ± SD of the final 10 episodes) of task-specific policies as a function of the learning rate hyperparameter, after running on each of the 27 tasks in the Cartpole MDP domain for the full length of 1.5 million time steps.

Learner PPO DQN

Learning rate 0.00025 96.922 ± 42.73 11.457 ± 2.65

0.001 67.224 ± 62.21 27.089 ± 33.05

0.005 17.196 ± 24.79 78.791 ± 44.01

0.025 12.539 ± 4.07 150.898 ± 44.60

0.1 10.180 ± 2.21 176.906 ± 15.26

Table 5: Parameter settings for the base-learners on the 18-task POcman POMDP domain. "All" denotes settings common to all base-learners.

Base-learner All DRQN
PRPO

Parameter unroll & burn-in hidden layers discount batch size update frequency replay memory size optimisation algorithm momentum learning rate exploration rate clip gradient replay start target update frequency optimisation algorithm learning rate batch size update frequency epochs clip gradient GAE parameter clipped objective coefficient value function coefficient entropy coefficient

Setting 15 time steps 80 (Dense RELU) - 80 (LSTM tanh) 0.99 10 once every 4 time steps 400,000 experiences AdaDelta [Zeiler, 2012] 0.95 0.1 0.2 absolute value exceeding 10 50,000 time steps once every 10,000 time steps Adam [Kingma and Ba, 2015] 0.00025 34 once every 100 time steps 3 norm exceeding 1.0 0.95 0.10 1 0.01

Table 6: Performance (Mean ± SD) of task-specific policies as a function of the learning rate hyperparameter, after running on each of the 18 tasks in the POcman POMDP domain for the full length of 5 million time steps.

Learner PRPO DRQN

Learning rate 0.00025 0.034 ± 0.03 -0.047 ± 0.04

0.001 0.037 ± 0.03 -0.025 ± 0.03

0.005 -0.105 ± 0.10 0.010 ± 0.03

0.025 -0.128 ± 0.10 0.056 ± 0.04

0.1 -0.130 ± 0.10 0.150 ± 0.06

27

