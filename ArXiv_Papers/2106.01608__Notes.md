
# A Discussion On the Validity of Manifold Learning

[arXiv](https://arxiv.org/abs/2106.01608), [PDF](https://arxiv.org/pdf/2106.01608.pdf)

## Authors

- Dai Shi
- Andi Han
- Yi Guo
- Junbin Gao

## Abstract

Dimensionality reduction (DR) and manifold learning (ManL) have been applied extensively in many machine learning tasks, including signal processing, speech recognition, and neuroinformatics. However, the understanding of whether DR and ManL models can generate valid learning results remains unclear. In this work, we investigate the validity of learning results of some widely used DR and ManL methods through the chart mapping function of a manifold. We identify a fundamental problem of these methods: the mapping functions induced by these methods violate the basic settings of manifolds, and hence they are not learning manifold in the mathematical sense. To address this problem, we provide a provably correct algorithm called fixed points Laplacian mapping (FPLM), that has the geometric guarantee to find a valid manifold representation (up to a homeomorphism). Combining one additional condition(orientation preserving), we discuss a sufficient condition for an algorithm to be bijective for any d-simplex decomposition result on a d-manifold. However, constructing such a mapping function and its computational method satisfying these conditions is still an open problem in mathematics.

## Comments



## Source Code

Official Code



Community Code

- [https://paperswithcode.com/paper/a-discussion-on-the-validity-of-manifold](https://paperswithcode.com/paper/a-discussion-on-the-validity-of-manifold)

## Bibtex

```tex
@misc{shi2021discussion,
      title={A Discussion On the Validity of Manifold Learning}, 
      author={Dai Shi and Andi Han and Yi Guo and Junbin Gao},
      year={2021},
      eprint={2106.01608},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```

## Notes

Type your reading notes here...

