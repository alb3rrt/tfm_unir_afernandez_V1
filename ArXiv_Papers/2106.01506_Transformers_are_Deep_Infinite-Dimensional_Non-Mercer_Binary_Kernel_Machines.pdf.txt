arXiv:2106.01506v1 [cs.LG] 2 Jun 2021

Transformers are Deep Infinite-Dimensional Non-Mercer Binary
Kernel Machines
Matthew A. Wright and Joseph E. Gonzalez University of California Berkeley, CA 94705
{mwright,jegonzal}@berkeley.edu
Abstract
Despite their ubiquity in core AI fields like natural language processing, the mechanics of deep attention-based neural networks like the "Transformer" model are not fully understood. In this article, we present a new perspective towards understanding how Transformers work. In particular, we show that the "dot-product attention" that is the core of the Transformer's operation can be characterized as a kernel learning method on a pair of Banach spaces. In particular, the Transformer's kernel is characterized as having an infinite feature dimension. Along the way we consider an extension of the standard kernel learning problem to a binary setting, where data come from two input domains and a response is defined for every cross-domain pair. We prove a new representer theorem for these binary kernel machines with non-Mercer (indefinite, asymmetric) kernels (implying that the functions learned are elements of reproducing kernel Banach spaces rather than Hilbert spaces), and also prove a new universal approximation theorem showing that the Transformer calculation can learn any binary non-Mercer reproducing kernel Banach space pair. We experiment with new kernels in Transformers, and obtain results that suggest the infinite dimensionality of the standard Transformer kernel is partially responsible for its performance. This paper's results provide a new theoretical understanding of a very important but poorly understood model in modern machine learning.
1 Introduction
Since its proposal by Bahdanau et al. (2015), so-called neural attention has become the backbone of many state-of-the-art deep learning models. This is true in particular in natural language processing (NLP), where the Transformer model of Vaswani et al. (2017) has become ubiquitous. This ubiquity is such that much of the last few years' NLP breakthroughs have been due to developing new training regimes for Transformers (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019; Wang et al., 2019a; Joshi et al., 2020; Lan et al., 2020; Brown et al., 2020, etc.).
Like most modern deep neural networks, theoretical understanding of the Transformer has lagged behind the rate of Transformer-based performance improvements on AI tasks like NLP. Recently, several authors have noted Transformer operations' relationship to other, better-understood topics in deep learning theory, like the similarities between attention and convolution (Ramachandran et al., 2019; Cordonnier et al., 2020) and the design of the residual blocks in multi-layer Transformers (e.g., Lu et al. (2019); see also the reordering of the main learned (fully-connected or attentional) operation, elementwise nonlinearity, and normalization in the original Transformer authors' official reference codebase (Vaswani et al., 2018) and in some more recent studies of deeper Transformers (Wang et al., 2019b) to the "pre-norm" ordering of normalize, learned operation, nonlinearity, add residual ordering of modern ("v2") Resnets (He et al., 2016)).
In this paper, we propose a new lens to understand the central component of the Transformer, its "dot-product attention" operation. In particular, we show dot-product attention can be characterized as a particular class of kernel method (Schölkopf and Smola, 2002). More specifically, it is a so-called indefinite and asymmetric kernel method, which refer to two separate generalizations of the classic class of kernels that does not require the classic assumptions of symmetry and positive (semi-) definiteness (Ong et al., 2004; Balcan et al., 2008; Zhang et al., 2009; Wu et al., 2010; Loosli et al., 2016; Oglic and Gärtner, 2018, 2019, etc.). We in fact show in Theorem 2 below that dot-product attention can learn any asymmetric indefinite kernel.
This insight has several interesting implications. Most immediately, it provides some theoretical justification for one of the more mysterious components of the Transformer model. It also potentially opens the door for the application of
1

decades of classic kernel method theory towards understanding one of today's most important neural network models, perhaps similarly to how tools from digital signal processing are widely used to study convolutional neural networks. We make a first step on this last point in this paper, proposing a generalization of prior kernel methods we call "binary" kernel machines, that learns how to predict distinct values for pairs of elements across two input sets, similar to an attention model.
The remainder of this paper is organized as follows. Section 2 reviews the mathematical background of both Transformers and classic kernel methods. Section 3 presents the definition of kernel machines on reproducing kernel Banach spaces (RKBS's) that we use to characterize Transformers. In particular we note that the Transformer can be described as having an infinite-dimensional feature space. Section 4 begins our theoretical results, explicitly describing the Transformer in terms of reproducing kernels, including explicit formulations of the Transformer's kernel feature maps and its relation to prior kernels. Section 5 discusses Transformers as kernel learners, including a new representer theorem and a characterization of stochastic-gradient-descent-trained attention networks as approximate kernel learners. In Section 6, we present empirical evidence that the infinite-dimensional character of the Transformer kernel may be somewhat responsible for the model's effectiveness. Section 7 concludes and summarizes our work.

2 Background and Related Work

2.1 Transformer Neural Network Models

The Transformer model (Vaswani et al., 2017) has become ubiquitous in many core AI applications like natural language
processing. Here, we review its core components. Say we have two ordered sets of vectors, a set of "source" elements {s1, s2, . . . , sS}, sj  Rds and a set of "target" elements {t1, t2, . . . , tT }, ti  Rdt . In its most general form, the neural-network "attention" operation that forms the backbone of the Transformer model is to compute, for each ti, a ti-specific embedding of the source sequence {sj}Sj=1.1
The particular function used in the Transformer is the so-called "scaled dot-product" attention, which takes the form

aij

=

(W

Qti

)T(W 

K

sj

)

d

ij =

exp(aij )

S j=1

exp(aij

)

S
ti = ij W V sj
j=1

(1)

where W V , W K  Rds×d, and W Q  Rdt×d are learnable weight matrices, usually called the "value," "key," and "query" weight matrices, respectively. Usually multiple so-called "attention heads" with independent parameter
matrices implement several parallel computations of (1), with the Cartesian product (vector concatenation) of several d-dimensional head outputs forming the final output ti. Usually the unnormalized aij's are called attention scores or attention logits, and the normalized ij's are called attention weights.
In this paper, we restrict our focus to the dot-product formulation of attention shown in (1). Several other alternative forms of attention that perform roughly the same function (i.e., mapping from Rds × Rdt to R) have been proposed (Bahdanau et al., 2015; Luong et al., 2015; Velickovic´ et al., 2018; Battaglia et al., 2018, etc.) but the dot-product
formulation of the Transformer is by far the most popular.

2.2 Kernel Methods and Generalizations of Kernels
Kernel methods (Schölkopf and Smola, 2002; Steinwart and Christmann, 2008, etc.) are a classic and powerful class of machine learning methods. The key component of kernel methods are the namesake kernel functions, which allow the efficient mapping of input data from a low-dimensional data domain, where linear solutions to problems like classification or regression may not be possible, to a high- or infinite-dimensional embedding domain, where linear solutions can be found.
Given two nonempty sets X and Y, a kernel function  is a continuous function  : X × Y  R. In the next few sections, we will review the classic symmetric and positive (semi-) definite, or Mercer, kernels, then discuss more general forms.
1Often, the source and target sets are taken to be the same, si = ti i. This instance of attention is called self attention.

2

2.2.1 Symmetric and Positive Semidefinite (Mercer) kernels

If X = Y, and for all xi, xj  X = Y, a particular kernel  has the properties

(xi, xj) = (xj, xi)

(2a)

cTKc  0  c  Rn; i, j = 1, . . . , n; n  N

(2b)

where K in (2b) is the Gram matrix, defined as Kij = (xi, xj), then  is said to be a Mercer kernel. Property (2a) is a symmetry requirement, and (2b) is a condition of positive (semi-) definiteness. For Mercer kernels, it is well-known that, among other facts, (i) we can define a Hilbert space of functions on X , denoted H (called the reproducing kernel Hilbert space, or RKHS, associated with the reproducing kernel ), (ii) H has for each x a (continuous) unique element x called a point evaluation functional, with the property f (x) = x(f ) f  H, (iii)  has the so-called reproducing property, f, (x, ·) H = f (x) f  H, where ·, · H is the inner product on H, and (iv) we can define a "feature map"  : X  FH, where FH is another Hilbert space sometimes called the feature space, and (x, y) = (x), (y) FH (where ·, · FH is the inner product associated with FH). This last point gives rise to the kernel trick for RKHS's.
From a machine learning and optimization perspective, kernels that are symmetric and positive (semi-) definite
(PSD) are desirable because those properties guarantee that empirical-risk-minimization kernel learning problems like
support vector machines (SVMs), Gaussian processes, etc. are convex. Convexity gives appealing guarantees for the
tractability of a learning problem and optimality of solutions.

2.2.2 Learning with non-Mercer kernels
Learning methods with non-Mercer kernels, or kernels that relax the assumptions (2), have been studied for some time. One line of work (Lin and Lin, 2003; Ong et al., 2004; Chen and Ye, 2008; Luss and D'aspremont, 2008; Alabdulmohsin et al., 2015; Loosli et al., 2016; Oglic and Gärtner, 2018, 2019, etc.) has focused on learning with symmetric but indefinite kernels, i.e., kernels that do not satisfy (2b). Indefinite kernels have been identified as reproducing kernels for so-called reproducing kernel Krein spaces (RKKS's) since Schwartz (1964) and Alpay (1991).
Replacing a Mercer kernel in a learning problem like an SVM with an indefinite kernel makes the optimization problem nonconvex in general (as the kernel Gram matrix K is no longer always PSD). Some early work in learning with indefinite kernels tried to ameliorate this problem by modifying the spectrum of the Gram matrix such that it again becomes PSD (e.g., Graepel et al., 1998; Roth et al., 2003; Wu et al., 2005). More recently, Loosli et al. (2016); Oglic and Gärtner (2018), among others, have proposed optimization procedures to learn in the RKKS directly. They report better performance on some learning problems when using indefinite kernels than either popular Mercer kernels or spectrally-modified indefinite kernels, suggesting that sacrificing convexity can empirically give a performance boost. This conclusion is of course reminiscent of the concurrent experience of deep neural networks, which are hard to optimize due to their high degree of non-convexity, yet give superior performance to many other methods.
Another line of work has explored the application of kernel methods to learning in more general Banach spaces, i.e., reproducing kernel Banach spaces (RKBS's) (Zhang et al., 2009). Various constructions to serve as the reproducing kernel for a Banach space (replacing the inner product of an RKHS) have been proposed, including semi-inner products (Zhang et al., 2009), positive-definite bilinear forms via a Fourier transform construction (Fasshauer et al., 2015), and others (Song et al., 2013; Georgiev et al., 2014, etc.). In this work, we consider RKBS's whose kernels may be neither symmetric nor PSD. A definition of these spaces is presented next.

3 General reproducing kernel Banach spaces

Recently, Georgiev et al. (2014), Lin et al. (2019), and Xu and Ye (2019) proposed similar definitions and constructions of RKBS's and their reproducing kernels meant to encompass prior definitions. In this paper, we adopt a fusion of the definitions and attempt to keep the notation as simple as possible to be sufficient for our purposes.

Definition 1 (Reproducing kernel Banach space (Xu and Ye, 2019, Definition 2.1; Lin et al., 2019, Definitions 1.1 &
1.2; Georgiev et al., 2014)). Let X and Y be nonempty sets,  a measurable function called a kernel,  : X × Y  R,
and BX and BY Banach spaces of real measurable functions on X and Y, respectively. Let ·, · BX ×BY : BX × BY  R be a nondegenerate bilinear mapping such that

(x, ·)  BY

 x  X;

(3a)

3

f, (x, ·) BX ×BY = f (x)  x  X , f  BX ;

(3b)

(·, y)  BX

 y  Y; and

(3c)

(·, y), g BX ×BY = g(y)  y  Y, g  BY .

(3d)

Then, BX and BY are a pair of reproducing kernel Banach spaces (RKBS's) on X and Y, respectively, and  is their reproducing kernel.

Line (3a) (resp. (3c)) says that, if we take , a function of two variables x  X and y  Y, and fix x (resp. y), then we get a function of one variable. This function of one variable must be an element of BY (resp. BX ). Lines (3b) and (3d) are the reproducing properties of .
For our purposes, it will be useful to extend this definition to include a "feature map" characterization similar to the one used in some explanations of RKHS's (Schölkopf and Smola, 2002, Chapter 2).

Definition 2 (Feature maps for RKBS's (Lin et al., 2019, Theorem 2.1; Georgiev et al., 2014)). For a pair of RKBS's

as defined in Definition 1, suppose that there exist mappings X : X  FX , Y : Y  FY , where FX and FY are
Banach spaces we will call the feature spaces, and a nondegenerate bilinear mapping ·, · FX ×FY : FX × FY  R such that

(x, y) = X (x), Y (y) FX ×FY  x  X , y  Y.

(4)

In this case, the spaces BX and BY can be defined as (Xu and Ye, 2019; Lin et al., 2019)

BX = fv : X  R : fv(x) X (x), v FX ×FY ; v  FY , x  X

(5a)

BY = gu : Y  R : gu(y) u, Y (y) FX ×FY ; u  FX , y  Y .

(5b)

Remark 1. We briefly discuss how to understand the spaces given by (5). Consider (5a) for example. It is a space of real-valued functions of one variable x, where the function is also parameterized by a v. Picking a v  FY in (5a) defines a manifold of functions in BX . This manifold of functions with fixed v varies with the function X . Evaluating a function fv in this manifold at a point x is defined by taking the bilinear product of X (x) and the chosen v. This also means that we can combine (4) and (5) to say

(x, y) = X (x), Y (y) FX ×FY = fX (x), gY (y) BX ×BY for all x  X , y  Y.

(6)

for all x  X and y  Y.

Remark 2. If X (x) and Y (y) can be represented as countable sets of real-valued measurable functions, {X (x) } N and {Y (y) } N for (X ) : X  R and (Y ) : Y  R (i.e., FX , FY  N R); and u, v FX ×FY = N u v for u  FX , v  FY ; then the "feature map" construction, whose notation we borrow from Lin et al. (2019), corresponds
to the "generalized Mercer kernels" of Xu and Ye (2019).

4 Dot-Product Attention as an RKBS Kernel

We now formally state the formulation for dot-product attention as an RKBS learner. Much like with RKHS's, for a given kernel and its associated RKBS pair, the feature maps (and also the bilinear mapping) are not unique. In the following, we present a feature map based on classic characterizations of other kernels such as RBF kernels (e.g., Steinwart et al. (2006)).

Proposition 1. The (scaled) dot-product attention calculation of (1) is a reproducing kernel for an RKBS in the
sense of Definitions 1 and 2, with the input sets X and Y being the vector spaces from which the target elements {ti}Ti=1, ti  Rdt and source elements {sj}Sj=1, sj  Rds are drawn, respectively; the feature maps


X (t) =
n=0 p1+···+pd=n

Y (s) =
n=0 p1+···+pd=n

n! p1 !···pd !

d=1(q )p

d1/4

n! p1 !···pd !

d=1(k )p

d1/4

(7a) (7b)

4

where q is the th element of q = W Qt, k is the th element of k = W K s, W Q  Rd×dt W K  Rd×ds , with
d  ds, dt and rank(W Q) = rank(W K ) = d; the bilinear mapping X (t), Y (s) FX ×FY = X (t) · Y (s); and the Banach spaces



BX = fk(t) = exp (W Qt)Tk/ d ; k  FY , t  X

(8a)



BY = gq(s) = exp qT(W K s)/ d ; q  FX , s  Y

(8b)

with the "exponentiated query-key kernel,"

(W Qt)T(W K s)

(t, s) = X (t), Y (s) FX ×FY = fY (s), gX (t) BX ×BY = exp

 d

(9)

the associated reproducing kernel.

The proof of Proposition 1 is straightforward and involves verifying (9) by multiplying the two infinite series in (7), then using the multinomial theorem and the Taylor expansion of the exponential.
In the above and when referring to Transformer-type models in particular rather than RKBS's in general, we use t, s, q, and k for x, y, u, and v, respectively, to draw the connection between the elements of the RKBS's and the widely-used terms "target," "source," "query," and "key."
The rank requirements on W Q and W K mean that span({X (t), t  X }) = FX and span({Y (s), s  Y}) = FY . This in turn means that the bilinear mapping is nondegenerate.
Remark 3. Now that we have an example of a pair of RKBS's, we can make more concrete some of the discussion from Remark 1. Examining (8a), for example, we see that when we select a k  FY , we define a manifold of functions in BX where k is fixed, but W Q can vary. Similarly, selecting a q  FX defines a manifold in BY . Selecting an element from both FX and FY locks us into one element each from BX and BY , which leads to the equality in (6).
Remark 4. Examining (8)-(9), we can see that the element drawn from FY that parameterizes the element of BX , as shown in (8a), is a function of Y (and vice-versa for (8b)). This reveals the exact mechanism in which the Transformer-type attention computation is a generalization of the RKBS's considered by Fasshauer et al. (2015), Lin et al. (2019), Xu and Ye (2019), etc., for applications like SVMs, where one of these function spaces is considered fixed.
Remark 5. Since the feature maps define the Banach spaces (5), the fact that the parameters W Q and W K are learned implies that Transformers learn parametric representations of the RKBS's themselves. This is in contrast to classic kernel methods, where the kernel (and thus the reproducing space) is usually fixed. In fact, in Theorem 2 below, we show that (a variant of) the Transformer architecture can approximate any RKBS mapping.
Remark 6. The symmetric version of the exponentiated dot product kernel is known to be a reproducing kernel for the so-called Bargmann space (Bargmann, 1961) which arises in quantum mechanics.
Remark 7. Notable in Proposition 1 is that we define the kernel of dot-product attention as including the exponential of the softmax operation. The output of this operation is therefore not the attention scores aij but rather the unnormalized attention weights, ¯ij = ij j ij. Considering the exponential as a part of the kernel operation reveals that the feature spaces for the Transformer are in fact infinite-dimensional in the same sense that the RBF kernel is said to have an infinite-dimensional feature space. In Section 6, we find empirical evidence that this infinite dimensionality may be partially responsible for the Transformer's effectiveness.

5 Transformers as kernel learners

5.1 The binary RKBS learning problem and its representer theorem

Most kernel learning problems take the form of empirical risk minimization problems. For example, if we had a learning problem for a finite dataset (x1, z1), . . . , (xn, zn), xi  X , zi  R and wanted to learn a function f : X  R in an RKHS H, the learning problem might be written as

f

=

arg min
f H

1 n

n i=1

L

(xi,

zi,

f (xi))

+

R(

f

H )

(10)

5

where L : X × R × R  R is a convex loss function, R : [0, )  R is a strictly increasing regularization function, and  is a scaling constant. Recent references that consider learning in RKBS's (Georgiev et al., 2014; Fasshauer et al., 2015; Lin et al., 2019; Xu and Ye, 2019) consider similar problems to (10), but with the RKHS H replaced with an RKBS.
The kernel learning problem for attention, however, is different from (10) in that, as we discussed in the previous section, we need to predict a response zij (i.e., the attention logit) for every pair (ti, sj). This motivates a generalization of the classic class of kernel learning problems that operates on pairs of input spaces. We discuss this generalization now.
Definition 3 (Binary kernel learning problem - regularized empirical risk minimization). Let X and Y be nonempty sets, and BX and BY RKBS's on X and Y, respectively. Let ·, · BX ×BY : BX × BY  R be a bilinear mapping on the two RKBS's. Let X : X  FX and Y : Y  FY be fixed feature mappings with the property that X (xi), Y (yi) FX ×FY = fY (y), gX (x) BX ×BY . Say {x1, . . . , xnx }, xi  X , {y1, . . . , yny }, yj  Y, and {zij}i=1,...,nx; j=1,...,ny , zij  R is a finite dataset where a response zij is defined for every (i, j) pair of an xi and a yj. Let L : X × Y × R × R  R be a loss function that is convex for fixed (xi, yj, zi,j), and RX : [0, )  R and RY : [0, )  R be convex, strictly increasing regularization functions.
A binary empirical risk minimization kernel learning problem for learning on a pair of RKBS's takes the form

f , g = arg min
f BX ,gBY

1

nxny

L
i,j

xi, yj , zij ,

fY (yj ), gX (xi) BX ×BY

+ X RX ( f BX ) + Y RY ( g BY )

(11)

where X and Y are again scaling constants.

Remark 8. The idea of a binary kernel problem that operates over pairs of two sets is not wholly new: there is prior work both in the collaborative filtering (Abernethy et al., 2009) and tensor kernel method (Tao et al., 2005; Kotsia and Patras, 2011; He et al., 2017) literatures. Our problem and results are new in the generalization to Banach rather than Hilbert spaces: as prior work in the RKBS literature (Micchelli et al., 2004; Zhang and Zhang, 2012; Xu and Ye, 2019, etc.) notes, RKBS learning problems are distinct from RKHS ones in their additional nonlinearity and/or nonconvexity. An extension of binary learning problems to Banach spaces is thus motivated by the Transformer setting, where a kernel method is in a context of a nonlinear and nonconvex deep neural network, rather than as a shallow learner like an SVM or matrix completion. For more discussion, see Appendix A.
Virtually all classic kernel learning methods find solutions whose forms are specified by so-called representer theorems. Representer theorems state that the solution to a regularized empirical risk minimization problem over a reproducing kernel space can be expressed as a linear combination of evaluations of the reproducing kernel against the dataset. Classic solutions to kernel learning problems thus reduce to finding the coefficients of this linear combination. Representer theorems exist in the literature for RKHS's (Kimeldorf and Wahba, 1971; Schölkopf et al., 2001; Argyriou et al., 2009), RKKS's (Ong et al., 2004; Oglic and Gärtner, 2018), and RKBS's (Zhang et al., 2009; Zhang and Zhang, 2012; Song et al., 2013; Fasshauer et al., 2015; Xu and Ye, 2019; Lin et al., 2019).
Fasshauer et al. (2015, Theorem 3.2), Xu and Ye (2019, Theorem 2.23), and Lin et al. (2019, Theorem 4.7) provide representer theorems for RKBS learning problems. However, their theorems only deal with learning problems where datapoints come from only one of the sets on which the reproducing kernel is defined (i.e., only X but not Y), which means the solution sought is an element of only one of the Banach spaces (e.g., f : X  R, f  BX ). Here, we state and prove a theorem for the more-relevant-to-Transformers binary case presented in Definition 3.

Theorem 1. Suppose we have a kernel learning problem of the form in (11). Let  : X × Y  R be the reproducing kernel of the pair of RKBS's BX and BY satisfying Definitions 1 and 2. Then, given some conditions on BX and BY (see Appendix B), the regularized empirical risk minimization problem (11) has a unique solution pair (f , g), with
the property that

nx

ny

(f ) = i(xi, ·); (g) = j(·, yj).

(12)

i=1

j=1

6

where (f ) (resp. (g)) denotes the Gâteaux derivative of the norm of f (resp. g) with the convention that (0) where i, j  R.
Proof. See Appendix B.

0, and

5.2 A new approximate kernel learning problem and universal approximation theorem
The downside of finding solutions to kernel learning problems like (10) or (11) of the form (12) as suggested by representer theorems is that they scale poorly to large datasets. It is well-known that for an RKHS learning problem, finding the scalar coefficients by which to multiply the kernel evaluations takes time cubic in the size of the dataset, and querying the model takes linear time. The most popular class of approximation techniques are based on the so-called Nyström method, which constructs a low-rank approximation of the kernel Gram matrix and solves the problem generated by this approximation (Williams and Seeger, 2001). A recent line of work (Gisbrecht and Schleif, 2015; Schleif and Tino, 2017; Oglic and Gärtner, 2019) has extended the Nyström method to RKKS learning.
In this section, we characterize the Transformer learning problem as a new class of approximate kernel methods ­ a "distillation" approach, one might call it. We formally state this idea now.
Proposition 2 (Parametric approximate solutions of binary kernel learning problems). Consider the setup of a binary kernel learning problem from Definition 3. We want to find approximations to the solution pair (f , g). In particular, we will say we want an approximation ^ : X × Y  R such that

^(x, y) 

fY (y), g X (x)

.
BX ×BY

(13)

for all x  X and y  Y. Comparing (13) to (6) suggests a solution: to learn a function ^ that approximates . In particular, (6) suggests
learning explicit approximations of the feature maps, i.e.,

^(x, y)  X (x), Y (y) FX ×FY .
In fact, it turns out that the Transformer query-key mapping (1) does exactly this. That is, while the Transformer kernel calculation outlined in Propostion 1 is finite-dimensional, it can in fact approximate the potentially infinite-dimensional optimal solution (f , g) characterized in Theorem 1. This fact is proved next.
Theorem 2. Let X  Rdt and Y  Rds be compact; t  X , s  Y; and let q : X  R and k : Y  R for = 1, . . . , d be two-layer neural networks with m hidden units. Then, for any continuous function F : X × Y  R
and > 0, there are integers m, d > 0 such that

d

F (t, s) - q (t)k (s) <  t  X , s  Y.

(14)

=1

Proof. See Appendix C.

We now outline how Theorem 2 relates to Transformers. If we concatenate the outputs of the two-layer neural networks {q }d=1 and {k }d=1 into d-dimensional vectors q : Rdt  Rd and k : Rds  Rd, then the dot product q(t)Tk(s) denoted by the sum in (14) can approximate any real-valued continuous function on X × Y. Minus the
usual caveats in applications of universal approximation theorems (i.e., in practice the output elements share hidden units rather than having independent ones), this dot product is exactly the computation of the attention logits aij, i.e., F (t, s)  log (t, s) for the F in (14) and the  in (9) up to a scaling constant d.
Since the exponential mapping between the attention logits and the exponentiated query-key kernel used in Transformers is a one-to-one mapping, if we take F (t, s) = log fY(s), g X (t) BX ×BY , then we can use a Transformer's dot-product attention to approximate the optimal solution to any RKBS solution arbitrarily well.
The core idea of an attention-based deep neural network is then to learn parametric representations of q and k via stochastic gradient descent. Unlike traditional representer-theorem-based learned functions, training time of
attention-based kernel machines like deep Transformers (generally, but with no guarantees) scale sub-cubically with
dataset size, and evaluation time stays constant regardless of dataset size.

7

Table 1: Test BLEU scores for Transformers with various kernels on machine translation (case-sensitive sacreBLEU). Values are mean ± std. dev over 5 training runs with different random seeds.

EDP

RBF

L2 Distance

EI

Quadratic

IWSLT14 DE-EN 30.41 ± 0.03 WMT14 EN-FR 35.11 ± 0.08

30.32 ± 0.22 35.57 ± 0.20

19.45 ± 0.16 28.41 ± 0.26

30.84 ± 0.27 34.51 ± 0.17

29.56 ± 0.19 34.54 ± 0.30

EDP = Exponentiated dot product; EI = Exponentiated intersection kernel

Table 2: Test accuracies for Transformers with various kernels on sentiment classification. Values are mean ± std. dev over 5 training runs with different random seeds.

EDP

RBF

L2 Distance

EI

Quadratic

SST-2 (%) 76.70 ± 0.36 SST-5 (%) 39.44 ± 0.47

74.24 ± 0.39 39.04 ± 0.62

76.78 ± 0.67 39.44 ± 1.33

74.90 ± 1.32 37.74 ± 0.48

76.24 ± 0.65 39.34 ± 0.80

EDP = Exponentiated dot product; EI = Exponentiated intersection kernel

6 Is the exponentiated dot product essential to Transformers?

We study modifications of the Transformer with several kernels used in classic kernel machines. We train on two
standard machine translation datasets and two standard sentiment classification tasks. For machine translation, IWSLT14
DE-EN is a relatively small dataset, while WMT14 EN-FR is a considerably larger one. For sentiment classification, we consider SST-2 and SST-5. We retain the standard asymmetric query and key feature mappings, i.e., q = W Qt and k = W K s, and only modify the kernel  : Rd × Rd  R0. In the below,  > 0 and   R are per-head learned scalars.
Our kernels of interest are: 
1. the (scaled) exponentiated dot product (EDP), (q, t) = exp(qT k/ d), i.e., the standard Transformer kernel;

2. the radial basis function (RBF) kernel, (q, t) = exp( -/d(q -k) 22), where · 2 is the standard 2-norm. It is well-known that the RBF kernel is a normalized version of the exponentiated dot-product, with the normalization
making it translation-invariant;

3. the vanilla L2 distance, (q, t) = /d q - k 2;

4. an exponentiated version of the intersection kernel, (q, t) = exp(

d =1

min(q

,k

)).

The

symmetric

version

of the intersection kernel was popular in kernel machines for computer vision applications (Barla et al., 2003;

Grauman and Darrell, 2005; Maji et al., 2008, etc.), and is usually characterized as having an associated RKHS that is a subspace of the function space L2 (i.e., it is infinite-dimensional in the sense of having a feature space of

continuous functions, as opposed to the infinite-dimensional infinite series of the EDP and RBF kernels);

5. a quadratic polynomial kernel, (q, k) = (1/dqTk + )2.

Full implementation details are provided in Appendix D. Results for machine translation are presented in Table 1. Several results stand out. First, the exponentiated dot
product, RBF, and exponentiated intersection kernels, which are said to have infinite-dimensional feature spaces, indeed do perform better than kernels with lower-dimensional feature maps such as the quadratic kernel. In fact, the RBF and EDP kernels perform about the same, suggesting that a deep Transformer may not need the translation-invariance that makes the RBF kernel preferred to the EDP in classic kernel machines. Intriguingly, the (unorthodox) exponentiated intersection kernel performs about the same as the two than the EDP and RBF kernels on IWSLT14 DE-EN, but slightly worse on WMT14 EN-FR. As mentioned, the EDP and RBF kernels have feature spaces of infinite series, while the intersection kernel corresponds to a feature space of continuous functions. On both datasets, the quadratic kernel performs slightly worse than the best infinite-dimensional kernel, while the L2 distance performs significantly worse.
Results for sentiment classification appear in Table 2. Unlike the machine translation experiments, the infinitedimensional kernels do not appear strictly superior to the finite-dimensional ones on this task. In fact, the apparent

8

loser here is the exponentiated intersection kernel, while the L2 distance, which performed the worst on machine translation, is within a standard deviation of the top-performing kernel. Notably, however, the variance of test accuracies on sentiment classification means that it is impossible to select a statistically significant "best" on this task. It is possible that the small inter-kernel variation relates to the relative simplicity of this problem (and relative smallness of the dataset) vs. machine translation: perhaps an infinite-dimensional feature space is not needed to obtain Transformer-level performance on the simpler problem.
It is worth noting that the exponentiated dot product kernel (again, the standard Transformer kernel) is a consistent high performer. This may be experimental evidence for the practical usefulness of the universal approximation property they enjoy (c.f. Theorem 2).
The relatively small yet statistically significant performance differences between kernels is reminiscent of the same phenomenon with activation functions (ReLU, ELU, etc.) for neural nets. Moreover, the wide inter-kernel differences in performance for machine translation, compared against the much smaller performance differences on the SST sentiment analysis tasks, suggests differing representational needs for problems of differing complexity. As a whole, these results suggest that kernel choice may be an additional design parameter for Transformer networks.
7 Conclusion
In this paper, we drew connections between classic kernel methods and the state-of-the-art Transformer networks. Beyond the theoretical interest in developing new RKBS representer theorems and other kernel theory, we gained new insight into what may make Transformers work. Our experimental results suggest that the infinite dimensionality of the Transformer kernel makes it a good choice in application, similar to how the RBF kernel is the standard choice for e.g. SVMs. Our work also reveals new avenues for Transformer research. For example, our experimental results suggest that choice of Transformer kernel acts as a similar design choice as activation functions in neural net design. Among the new open research questions are (1) whether the exponentiated dot-product should be always preferred, or if different kernels are better for different tasks (c.f. how GELUs have recently become very popular as replacements for ReLUs in Transformers), (2) any relation between vector-valued kernels used for structured prediction (Álvarez et al., 2012) and, e.g., multiple attention heads, and (3) the extension of Transformer-type deep kernel learners to non-Euclidean data (using, e.g., graph kernels or kernels on manifolds).
References
J. Abernethy, F. Bach, T. Evgeniou, and J.-P. Vert. A New Approach to Collaborative Filtering: Operator Estimation with Spectral Regularization. Journal of Machine Learning Research, 10:803­826, Mar. 2009.
N. I. Akhiezer and M. G. Krein. Some Questons in the Theory of Moments. Number 2 in Translations of Mathematical Monographs. American Mathematical Society, Providence, RI, 1962. ISBN 0-8128-1552-0.
I. Alabdulmohsin, X. Gao, and X. Zhang. Support vector machines with indefinite kernels. In Proceedings of the Sixth Asian Conference on Machine Learning, volume 39, pages 32­47. PMLR, 2015.
D. Alpay. Some Remarks on Reproducing Kernel Krein Spaces. Rocky Mountain Journal of Mathematics, 21(4): 1189­1205, Dec. 1991. ISSN 0035-7596. doi:10.1216/rmjm/1181072903.
M. A. Álvarez, L. Rosasco, and N. D. Lawrence. Kernels for Vector-Valued Functions: A Review. Foundations and Trends® in Machine Learning, 4(3):195­266, 2012. ISSN 1935-8237, 1935-8245. doi:10.1561/2200000036.
A. Argyriou, C. A. Micchelli, and M. Pontil. When Is There a Representer Theorem? Vector Versus Matrix Regularizers. Journal of Machine Learning Research, 10:2507­2529, Nov. 2009.
R. Arora, P. Bartlett, P. Mianjy, and N. Srebro. Dropout: Explicit Forms and Capacity Control. arXiv:2003.03397 [cs, stat], Mar. 2020.
J.-G. Attali and G. Pagès. Approximations of Functions by a Multilayer Perceptron: A New Approach. Neural Networks, 10(6):1069­1081, Aug. 1997. ISSN 08936080. doi:10.1016/S0893-6080(97)00010-5.
9

D. Bahdanau, K. Cho, and Y. Bengio. Neural Machine Translation by Jointly Learning to Align and Translate. In International Conference on Learning Representations, 2015.
M.-F. Balcan, A. Blum, and N. Srebro. A theory of learning with similarity functions. Machine Learning, 72(1-2): 89­112, Aug. 2008. ISSN 0885-6125, 1573-0565. doi:10.1007/s10994-008-5059-5.
P. Baldi and P. J. Sadowski. Understanding Dropout. In Advances in Neural Information Processing Systems, volume 26, pages 2814­2822. Curran Associates, Inc., 2013.
V. Bargmann. On a Hilbert space of analytic functions and an associated integral transform part I. Communications on Pure and Applied Mathematics, 14(3):187­214, Aug. 1961. ISSN 00103640, 10970312. doi:10.1002/cpa.3160140303.
A. Barla, F. Odone, and A. Verri. Histogram intersection kernel for image classification. In Proceedings 2003 International Conference on Image Processing (Cat. No.03CH37429), volume 2, pages III­513­16, Barcelona, Spain, 2003. IEEE. ISBN 978-0-7803-7750-9. doi:10.1109/ICIP.2003.1247294.
P. W. Battaglia, J. B. Hamrick, V. Bapst, A. Sanchez-Gonzalez, V. Zambaldi, M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner, C. Gulcehre, F. Song, A. Ballard, J. Gilmer, G. Dahl, A. Vaswani, K. Allen, C. Nash, V. Langston, C. Dyer, N. Heess, D. Wierstra, P. Kohli, M. Botvinick, O. Vinyals, Y. Li, and R. Pascanu. Relational inductive biases, deep learning, and graph networks. arXiv:1806.01261 [cs, stat], June 2018.
G. Birkhoff. Orthogonality in linear metric spaces. Duke Mathematical Journal, 1(2):169­172, June 1935. ISSN 0012-7094, 1547-7398. doi:10.1215/S0012-7094-35-00115-6.
T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language Models are Few-Shot Learners. arXiv:2005.14165 [cs], June 2020.
J. Chen and J. Ye. Training SVM with indefinite kernels. In Proceedings of the 25th International Conference on Machine Learning - ICML '08, pages 136­143, Helsinki, Finland, 2008. ISBN 978-1-60558-205-4. doi:10.1145/1390156.1390174.
J. A. Clarkson. Uniformly convex spaces. Transactions of the American Mathematical Society, 40(3):396­396, Mar. 1936. ISSN 0002-9947. doi:10.1090/S0002-9947-1936-1501880-4.
J.-B. Cordonnier, A. Loukas, and M. Jaggi. On the Relationship between Self-Attention and Convolutional Layers. In International Conference on Learning Representations, 2020.
G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals and Systems, 2(4):303­314, Dec. 1989. ISSN 1435-568X. doi:10.1007/BF02551274.
M. M. Day. Reflexive Banach spaces not isomorphic to uniformly convex spaces. Bulletin of the American Mathematical Society, 47(4):313­318, Apr. 1941. ISSN 0002-9904. doi:10.1090/S0002-9904-1941-07451-3.
J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, volume 1, pages 4171­4186, Minneapolis, Minnesota, June 2019. doi:10.18653/v1/N19-1423.
I. Ekeland and R. Témam. Convex Analysis and Variational Problems, volume 28 of Classics in Applied Mathematics. Society for Industrial and Applied Mathematics, Jan. 1999. ISBN 978-0-89871-450-0 978-1-61197-108-8. doi:10.1137/1.9781611971088.
I. Ekeland and T. Turnbull. Infinite-Dimensional Optimization and Convexity. Chicago Lectures in Mathematics. University of Chicago Press, Chicago, 1983. ISBN 978-0-226-19987-0 978-0-226-19988-7.
10

G. E. Fasshauer, F. J. Hickernell, and Q. Ye. Solving support vector machines in reproducing kernel Banach spaces with positive definite functions. Applied and Computational Harmonic Analysis, 38(1):115­139, Jan. 2015. ISSN 1063-5203. doi:10.1016/j.acha.2014.03.007.
K.-I. Funahashi. On the approximate realization of continuous mappings by neural networks. Neural Networks, 2(3): 183­192, Jan. 1989. ISSN 08936080. doi:10.1016/0893-6080(89)90003-8.
P. G. Georgiev, L. Sánchez-González, and P. M. Pardalos. Construction of Pairs of Reproducing Kernel Banach Spaces. In V. F. Demyanov, P. M. Pardalos, and M. Batsyn, editors, Constructive Nonsmooth Analysis and Related Topics, pages 39­57. Springer New York, New York, NY, 2014. ISBN 978-1-4614-8615-2. doi:10.1007/978-1-4614-86152_4.
A. Gisbrecht and F.-M. Schleif. Metric and non-metric proximity transformations at linear costs. Neurocomputing, 167: 643­657, Nov. 2015. ISSN 09252312. doi:10.1016/j.neucom.2015.04.017.
T. Graepel, R. Herbrich, P. Bollmann-Sdorra, and K. Obermayer. Classification on Pairwise Proximity Data. In Advances in Neural Information Processing Systems, volume 11, pages 438­444, 1998.
K. Grauman and T. Darrell. The pyramid match kernel: Discriminative classification with sets of image features. In Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1, pages 1458­1465 Vol. 2, Beijing, China, 2005. IEEE. ISBN 978-0-7695-2334-7. doi:10.1109/ICCV.2005.239.
K. He, X. Zhang, S. Ren, and J. Sun. Identity Mappings in Deep Residual Networks. In European Conference on Computer Vision, pages 630­645, July 2016.
L. He, C.-T. Lu, G. Ma, S. Wang, L. Shen, P. S. Yu, and A. B. Ragin. Kernelized Support Tensor Machines. In Proceedings of the 34th International Conference on Machine Learning, volume 70, pages 1442­1451. PMLR, July 2017.
D. P. Helmbold and P. M. Long. Surprising properties of dropout in deep networks. The Journal of Machine Learning Research, 18(1):7284­7311, Jan. 2017. ISSN 1532-4435.
K. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward networks are universal approximators. Neural Networks, 2(5):359­366, Jan. 1989. ISSN 0893-6080. doi:10.1016/0893-6080(89)90020-8.
R. C. James. Orthogonality and Linear Functionals in Normed Linear Spaces. Transactions of the American Mathematical Society, 61(2):265­292, 1947. ISSN 0002-9947. doi:10.2307/1990220.
M. Joshi, D. Chen, Y. Liu, D. S. Weld, L. Zettlemoyer, and O. Levy. SpanBERT: Improving Pre-training by Representing and Predicting Spans. Transactions of the Association for Computational Linguistics, 8:64­77, Jan. 2020. ISSN 2307-387X. doi:10.1162/tacl_a_00300.
G. Kimeldorf and G. Wahba. Some results on Tchebycheffian spline functions. Journal of Mathematical Analysis and Applications, 33(1):82­95, Jan. 1971. ISSN 0022-247X. doi:10.1016/0022-247X(71)90184-3.
D. P. Kingma and J. Ba. Adam: A Method for Stochastic Optimization. In International Conference on Learning Representations, 2015.
I. Kotsia and I. Patras. Support Tucker Machines. In CVPR 2011, pages 633­640, Colorado Springs, CO, USA, June 2011. IEEE. ISBN 978-1-4577-0394-2. doi:10.1109/CVPR.2011.5995663.
Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut. ALBERT: A Lite BERT for Self-supervised Learning of Language Representations. In International Conference on Learning Representations, 2020.
H.-T. Lin and C.-J. Lin. A Study on Sigmoid Kernels for SVM and the Training of non-PSD Kernels by SMO-type Methods. Technical report, National Taiwan University, Taipei, Taiwan, 2003.
R. Lin, H. Zhang, and J. Zhang. On Reproducing Kernel Banach Spaces: Generic Definitions and Unified Framework of Constructions. arXiv:1901.01002 [cs, math, stat], Jan. 2019.
11

Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv:1907.11692 [cs], July 2019.
G. Loosli, S. Canu, and C. S. Ong. Learning SVM in Krein Spaces. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38(6):1204­1216, June 2016. ISSN 1939-3539. doi:10.1109/TPAMI.2015.2477830.
Y. Lu, Z. Li, D. He, Z. Sun, B. Dong, T. Qin, L. Wang, and T.-Y. Liu. Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View. arXiv:1906.02762 [cs, stat], June 2019.
M.-T. Luong, H. Pham, and C. D. Manning. Effective Approaches to Attention-based Neural Machine Translation. arXiv:1508.04025 [cs], Aug. 2015.
R. Luss and A. D'aspremont. Support Vector Machine Classification with Indefinite Kernels. In Advances in Neural Information Processing Systems, volume 20, pages 953­960, 2008.
S. Maji, A. C. Berg, and J. Malik. Classification using intersection kernel support vector machines is efficient. In 2008 IEEE Conference on Computer Vision and Pattern Recognition, pages 1­8, Anchorage, AK, USA, June 2008. IEEE. ISBN 978-1-4244-2242-5. doi:10.1109/CVPR.2008.4587630.
R. E. Megginson. An Introduction to Banach Space Theory, volume 183 of Graduate Texts in Mathematics. Springer New York, New York, NY, 1998. ISBN 978-1-4612-6835-2 978-1-4612-0603-3. doi:10.1007/978-1-4612-0603-3.
C. A. Micchelli, M. Pontil, J. Shawe-Taylor, and Y. Singer. A Function Representation for Learning in Banach Spaces. In Learning Theory, volume 3120, pages 255­269. Springer Berlin Heidelberg, Berlin, Heidelberg, 2004. ISBN 978-3-540-22282-8 978-3-540-27819-1. doi:10.1007/978-3-540-27819-1_18.
D. Oglic and T. Gärtner. Learning in Reproducing Kernel Krein Spaces. In Proceedings of the 35th International Conference on Machine Learning, volume 80, pages 3859­3867. PMLR, 2018.
D. Oglic and T. Gärtner. Scalable Learning in Reproducing Kernel Krein Spaces. In Proceedings of the 36th International Conference on Machine Learning, volume 97, pages 4912­4921. PMLR, 2019.
A. Okuno, T. Hada, and H. Shimodaira. A probabilistic framework for multi-view feature learning with many-to-many associations via neural networks. In Proceedings of the 35th International Conference on Machine Learning, volume 80, pages 3888­3897. PMLR, June 2018.
C. S. Ong, X. Mary, S. Canu, and A. J. Smola. Learning with non-positive kernels. In Proceedings of the 21st International Conference on Machine Learning, pages 639­646, Banff, Alberta, Canada, 2004. ACM Press. doi:10.1145/1015330.1015443.
M. Ott, S. Edunov, A. Baevski, A. Fan, S. Gross, N. Ng, D. Grangier, and M. Auli. Fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of NAACL-HLT 2019: Demonstrations, 2019. doi:10.18653/v1/N19-4009.
A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In Advances in Neural Information Processing Systems, volume 32, pages 8026­8037. Curran Associates, Inc., 2019.
M. Post. A Call for Clarity in Reporting BLEU Scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186­191, Belgium, Brussels, 2018. Association for Computational Linguistics. doi:10.18653/v1/W18-6319.
A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language Models are Unsupervised Multitask Learners. Technical report, OpenAI, 2018.
P. Ramachandran, N. Parmar, A. Vaswani, I. Bello, A. Levskaya, and J. Shlens. Stand-Alone Self-Attention in Vision Models. arXiv:1906.05909 [cs], June 2019.
V. Roth, J. Laub, M. Kawanabe, and J. Buhmann. Optimal cluster preserving embedding of nonmetric proximity data. IEEE Transactions on Pattern Analysis and Machine Intelligence, 25(12):1540­1551, Dec. 2003. ISSN 1939-3539. doi:10.1109/TPAMI.2003.1251147.
12

F.-M. Schleif and P. Tino. Indefinite Core Vector Machine. Pattern Recognition, 71:187­195, Nov. 2017. ISSN 00313203. doi:10.1016/j.patcog.2017.06.003.
B. Schölkopf and A. J. Smola. Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond. The MIT Press, 2002. ISBN 978-0-262-25693-3. doi:10.7551/mitpress/4175.001.0001.
B. Schölkopf, R. Herbrich, and A. J. Smola. A Generalized Representer Theorem. In D. Helmbold and B. Williamson, editors, Computational Learning Theory, Lecture Notes in Computer Science, pages 416­426, Berlin, Heidelberg, 2001. Springer. ISBN 978-3-540-44581-4. doi:10.1007/3-540-44581-1_27.
L. Schwartz. Sous-espaces hilbertiens d'espaces vectoriels topologiques et noyaux associés (Noyaux reproduisants). Journal d'Analyse Mathématique, 13(1):115­256, 1964. doi:10.1007/BF02786620.
R. Sennrich, B. Haddow, and A. Birch. Neural Machine Translation of Rare Words with Subword Units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715­1725, Berlin, Germany, Aug. 2016. Association for Computational Linguistics. doi:10.18653/v1/P16-1162.
R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Y. Ng, and C. Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631­1642, 2013.
G. Song, H. Zhang, and F. J. Hickernell. Reproducing kernel Banach spaces with the l1 norm. Applied and Computational Harmonic Analysis, 34(1):96­116, Jan. 2013. ISSN 1063-5203. doi:10.1016/j.acha.2012.03.009.
I. Steinwart and A. Christmann. Support Vector Machines. Information Science and Statistics. Springer New York, New York, NY, 2008. ISBN 978-0-387-77241-7 978-0-387-77242-4. doi:10.1007/978-0-387-77242-4.
I. Steinwart, D. Hush, and C. Scovel. An Explicit Description of the Reproducing Kernel Hilbert Spaces of Gaussian RBF Kernels. IEEE Transactions on Information Theory, 52(10):4635­4643, Oct. 2006. ISSN 0018-9448. doi:10.1109/TIT.2006.881713.
D. Tao, X. Li, W. Hu, S. Maybank, and X. Wu. Supervised Tensor Learning. In Fifth IEEE International Conference on Data Mining (ICDM'05), pages 450­457, Houston, TX, USA, 2005. IEEE. ISBN 978-0-7695-2278-4. doi:10.1109/ICDM.2005.139.
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention Is All You Need. In Advances in Neural Information Processing Systems, volume 30, pages 5998­6008, 2017.
A. Vaswani, S. Bengio, E. Brevdo, F. Chollet, A. N. Gomez, S. Gouws, L. Jones, L. Kaiser, N. Kalchbrenner, N. Parmar, R. Sepassi, N. Shazeer, and J. Uszkoreit. Tensor2Tensor for Neural Machine Translation. arXiv:1803.07416 [cs, stat], Mar. 2018.
P. Velickovic´, G. Cucurull, A. Casanova, A. Romero, P. Liò, and Y. Bengio. Graph Attention Networks. In International Conference on Learning Representations, 2018.
S. Wager, S. Wang, and P. S. Liang. Dropout Training as Adaptive Regularization. In Advances in Neural Information Processing Systems, volume 26, pages 351­359. Curran Associates, Inc., 2013.
A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. Bowman. GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding. In International Conference on Learning Representations, 2019a.
Q. Wang, B. Li, T. Xiao, J. Zhu, C. Li, D. F. Wong, and L. S. Chao. Learning Deep Transformer Models for Machine Translation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1810­1822, Florence, Italy, July 2019b. Association for Computational Linguistics. doi:10.18653/v1/P19-1176.
C. K. I. Williams and M. Seeger. Using the Nyström Method to Speed Up Kernel Machines. In Advances in Neural Information Processing Systems, volume 13, pages 682­688. MIT Press, 2001.
G. Wu, E. Y. Chang, and Z. Zhang. An Analysis of Transformation on Non-Positive Semidefinite Similarity Matrix for Kernel Machines. Technical report, University of California, Santa Barbara, June 2005.
13

W. Wu, J. Xu, H. Li, and S. Oyama. Asymmetric Kernel Learning. Technical report, Microsoft Research, June 2010. Y. Xu and Q. Ye. Generalized Mercer Kernels and Reproducing Kernel Banach Spaces, volume 258 of Memoirs
of the American Mathematical Society. American Mathematical Society, Mar. 2019. ISBN 978-1-4704-3550-9 978-1-4704-5077-9 978-1-4704-5078-6. doi:10.1090/memo/1243. Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. Salakhutdinov, and Q. V. Le. XLNet: Generalized Autoregressive Pretraining for Language Understanding. arXiv:1906.08237 [cs], June 2019. H. Zhang and J. Zhang. Regularized learning in Banach spaces as an optimization problem: Representer theorems. Journal of Global Optimization, 54(2):235­250, Oct. 2012. ISSN 1573-2916. doi:10.1007/s10898-010-9575-z. H. Zhang, Y. Xu, and J. Zhang. Reproducing kernel Banach spaces for machine learning. Journal of Machine Learning Research, 10:2741­2775, Dec. 2009.
14

A Deep neural networks lead to Banach space analysis
Examining the kernel learning problem (11), it may not immediately clear why the reproducing spaces on X and Y need be Banach spaces rather than Hilbert spaces. Suppose for example that we have two RKHS's HX and HY on X and Y, respectively. Then, we can take their tensor product HX  HY as an RKHS on X × Y, with associated reproducing kernel X ×Y (x1  y1, x2  y2) = X (x1, x2)Y (y1, y2), where X and Y are the reproducing kernels of HX and HY , respectively, and x1  y1, x2  y2  X  Y. The solutions to a regularized kernel problem like (11) would then be drawn from HX and HY . This setup is similar to those studied in, e.g., Abernethy et al. (2009); He et al. (2017).
In a shallow kernel learner like an SVM, the function in the RKHS can be characterized via its norm. Representer theorems allow for the norm of the function in the Hilbert space to be calculated from the scalar coefficients that make up the solution. On the other hand, for a Transformer layer in a multilayer neural network, regularization is usually not done via norm penalty as shown in (11). In most applications, regularization is done via dropout on the attention weights aij as well as via the implicit regularization obtained from subsampling the dataset during iterations of stochastic gradient descent. While dropout has been characterized as a form of weight decay (i.e., a variant of p-norm penalization) for linear models (Baldi and Sadowski, 2013; Wager et al., 2013, etc.), recent work has shown that dropout induces a more complex regularization effect in deep networks (Helmbold and Long, 2017; Arora et al., 2020, etc.). Thus, it is difficult to characterize the norm of the vector spaces we are traversing when solving the general problem (11) in the context of a deep network. This can lead to ambiguity as to whether the norm being regularized as we traverse the solution space is a Hilbert space norm. If f and g are infinite series or Lp functions, for example, their resident space is only a Hilbert space if the associated norm is the 2 or L2 norm. This motivates the generalization of kernel learning theorems to the general Banach space setting when in the context of deep neural networks.

B Proof of Theorem 1

B.1 Preliminiaries

To prove this theorem, we first need some results and definitions regarding various properties of Banach spaces

(Megginson, 1998). These preliminaries draw from Xu and Ye (2019) and Lin et al. (2019).

Two metric spaces (M, dM) and (N , dN ) are said to be isometrically isomorphic if there exists a bijective mapping T : M  N , called an isometric isomorphism, such that for all m  M, dN (T (m)) = dM(m) (Megginson,

1998, Definition 1.4.13).

The dual space of a vector space V over a field F, which we will denote V, is the space of all continuous linear

functionals on V, i.e.,

V = {g : V  F, g linear and continuous}.

(B.1)

A normed vector space V is reflexive if it is isometrically isomorphic to V, the dual space of its dual space (a.k.a.
its double dual).
For a normed vector space V, the dual bilinear product, which we will denote ·, · V (i.e., with only one subscript, as opposed to e.g., ·, · BX ×BY ), is defined on V and V as

f, g V g(f ) for f  V, g  V.

Given a normed vector space V and its dual space V, let U  V and W  V. The annihilator of U in V and the annihilator of W in V, denoted U  and W respectively, are (Megginson, 1998, Definition 1.10.14)

U  = {g  V : f, g V = 0 f  U } W = {f  V : f, g V = 0 g  W}.

A normed vector space V is called strictly convex if tv1 + (1 - t)v2 V < 1 whenever v1 V = v2 V = 1, 0 < t < 1, where v1, v2  V and · V denotes the norm of V (Megginson, 1998, Definition 5.1.1; citing Clarkson, 1936 and Akhiezer and Krein, 1962).
A nonempty subset A of a metric space (M, dM) is called a Chebyshev set if, for every element m  M, there is exactly one element c  A such that dM(m, c) = dM(m, A) (Megginson, 1998, Definition 5.1.17) (where recall the distance between a point m and a set A in a metric space is equal to infcA dM(m, c)). If a normed vector space V is

15

reflexive and strictly convex, then every nonempty closed convex subset of V is a Chebyshev set (Megginson, 1998, Corollary 5.1.19; citing Day, 1941).
For a normed vector space V and v, w  V, the Gâteaux derivative of the norm (Megginson, 1998, Definition 5.4.15) at v in the direction of w is defined as

lim v + tw V - v V .

t0

t

If the Gâteaux derivative of the norm at v in the direction of w exists for all w  V, then · V is said to be Gâteaux differentiable at v. A normed vector space V is called Gâteaux differentiable or smooth if its norm is Gâteaux differentiable at all v  V (Megginson, 1998, Corollary 5.4.18).
The smoothness of a normed vector space V implies that, if we define a "norm operator"  on V, (v) v V , then for each v  V \ {0}, there exists a continuous linear functional dG(v) on V such that (Xu and Ye, 2019, p. 24)

w, dG(v)

V

=

lim
t0

v + tw V - t

v V for all w  V.

Since the Gâteaux derivative of the norm is undefined at 0, following Xu and Ye (2019, Equation 2.16); Lin et al. (2019, p. 20); etc., we define a regularized Gâteaux derivative of the norm operator on V,

(v) dG(v) when v = 0

0

when v = 0

(B.2)

for v  V. Given two vector spaces V and W defined over a field F, the direct sum, denoted V  W, is the vector space with
elements (v, w)  V  W for v  V, w  W with the additional structure
(v1, w1) + (v2, w2) = (v1 + v2, w1 + w2) c(v, w) = (cv, cw)

for v1, v2, v  V, w1, w2, w  W, c  F.

If V and W are normed vector spaces with norms · V and · W , respectively, then we will say V  W has the

norm

v, w VW v V + w W for v  V, w  W.

(B.3)

Megginson (1998, Definition 1.8.1) calls (B.3) a "1-norm" direct sum norm, but notes that other norm-equivalent direct sum norms such as a 2-norm and infinity-norm are possible. Some other useful facts about direct sums are:

· if V and W are both strictly convex, then V  W is strictly convex (Megginson, 1998, Theorem 5.1.23);

· if V and W are both reflexive, then V  W is reflexive (Megginson, 1998, Corollary 1.11.20);

· and if V and W are both smooth, then V  W is smooth (Megginson, 1998, Theorem 5.4.22).

An element v of a normed vector space V is said to be orthogonal (or Birkhoff-James orthogonal) to another element w  V if (Birkhoff, 1935; James, 1947)

v + tw V  v V for all t  R.

If W  V, then we say v  V is orthogonal to W if v is orthogonal to all w  W. Now we can state a lemma regarding orthogonality in RKBS's.
Lemma 1 (Xu and Ye, 2019, Lemma 2.21). If the RKBS B is smooth, then f  B is orthogonal to g  B if and only if g, (f ) B = 0, where ·, · B means the dual bilinear product as given in (B.1) and  is the regularized Gâteaux derivative from (B.2). Also, an f  B \ {0} is orthogonal to a subspace N  B if and only if h, (f ) B = 0 for all h  N.

16

B.2 Minimum-norm interpolation (optimal recovery)

Following Fasshauer et al. (2015); Xu and Ye (2019); Lin et al. (2019), we first prove a representer theorem for a simpler problem ­ that of perfect interpolation while minimizing the norm of the solution ­ before proceeding to the representer theorem for the empirical risk minimization problem (11) in the next section.

Definition 4 (Minimum-norm interpolation in a pair of RKBS's). Let X and Y be nonempty sets, and BX and BY
RKBS's on X and Y, respectively. Let ·, · BX ×BY : BX × BY  R be a bilinear mapping on the two RKBS's, X : X  FX and Y : Y  FY . Let (xi, yi) = X (x), Y (y) F = fY (yi), gX (xi) BX ×BY be a reproducing kernel of X and Y satisfying Definitions 1 and 2. Say {x1, . . . , xnx }, xi  X , {y1, . . . , yny }, yi  Y, and {zij}i=1,...,nx; j=1,...,ny , zij  R is a finite dataset where a response zij is defined for every (i, j) pair of an xi and a yj. The minimum-norm interpolation problem is

f , g = arg min f BX + g BY
f BX ,yBY
such that (f, g)  NX ,Y,Z

(B.4)

where

NX ,Y,Z = {(f, g)  BX  BY s.t. fY (yj), gX (xi) BX ×BY = zij  i, j}.

(B.5)

To discuss the solution of (B.4), we first need to establish the condition for the existence of a solution. The following is a generalization of a result from Section 2.6 of Xu and Ye (2019).

Lemma 2. If the set {(xi, ·)}ni=x1 is linearly independent in BY , the set {(·, yj)}nj=y 1 is linearly independent in BX , and the bilinear mapping ·, · BX ×BY : BX × BY  R is nondegenerate, then NX ,Y,Z (B.5) is nonempty.

Proof. From the definition of  (3) and the bilinearity of ·, · BX ×BY , we can write that

nx

nx

nx

f, ci(xi, ·)
i=1

= ci

BX ×BY

i=1

f, (xi, ·) BX ×BY = cif (xi)
i=1

for all f  BX

for ci  R. and that

ny

ny

ny

cj(·, yj), g
j=1

= cj

BX ×BY

j=1

(·, yj ), g BX ×BY = cj g(yj )
j=1

for all g  BY

for cj  R. This means that

nx

nx

ci(xi, ·) = 0 if and only if cif (xi) = 0 for all f  BX

i=1

i=1

and

ny

ny

cj(·, yj) = 0 if and only if cjg(yj) = 0 for all g  BY .

j=1

j=1

This shows that linear independence of {(xi, ·)}ni=x1 and {(·, yj)}nj=y 1 imply linear independence of {f (xi)}ni=x1 and {g(yj)}nj=y 1, respectively. Then, considering the nondegeneracy of the bilinear mapping ·, · BX ×BY , we can say that

ny

nx

cj(·, yj), ci(xi, ·)

=0

j=1

i=1

BX ×BY


nx

ny



if and only if  cif (xi) = 0 for all f  BX or

cjg(yj) = 0 for all g  BY  .

i=1

j=1

From this, we can see that linear independence of {(xi, ·)}ni=x1 and {(·, yj)}nj=y 1, and the nondegeneracy of ensure the existence of at least one (f , g) pair in NX ,Y,Z .

·, · BX ×BY

17

Now we can prove a lemma characterizing the solution to (B.4).

Lemma 3. Consider the minimum-norm interpolation problem from Definition 4. Assume that BX and BY are smooth, strictly convex, and reflexive,2 and that {(xi, ·)}ni=x1 and {(·, yj)}nj=y 1 are linearly independent. Then, (B.4) has a unique solution pair (f , g), with the property that

nx
(f ) = i(xi, ·)
i=1

ny
(g) = j(·, yj).
j=1

where (·) is the regularized Gâteaux derivative as defined in (B.2), and where i, j  R.
Proof. The existence of a solution pair is given by the linear independence of {(xi, ·)}ni=x1 and {(·, yj)}nj=y 1 and Lemma 2. Uniqueness of the solution will be shown by showing that NX ,Y,Z is closed and convex and a subset of a strictly convex and reflexive set, which ensures it is a Chebyshev set.
Since BX and BY are strictly convex and reflexive, their direct sum BX  BY is strictly convex and reflexive, as we noted in section B.1.
Now we analyze NX ,Y,Z . We first show convexity. Pick any (f, g), (f , g )  NX ,Y,Z and t  (0, 1). Then note that for any (xi, yj, zi,j),

tfY (yj ), tgX (xi)

+
BX ×BY

(1 - t)fY (yj ), (1 - t)gX (xi) BX ×BY

=t

fY (yj ), gX (xi)

+ (1 - t)
BX ×BY

= tzi,j + (1 - t)zi,j = zi,j

fY (yj ), gX (xi)

BX ×BY

thus showing that NX ,Y,Z is convex. Closedness may be shown by the strict convexity of its superset BX  BY and

the (f 

continuity of , g)  NX ,Y

·,
,Z

· B1×B2 with

.

Thus,

the

closed

and

convex

NX ,Y,Z



BX



BY

is

a

Chebyshev

set,

implying

a

unique

f, g

BX BY

= min
(f,g)NX ,Y,Z

f BX +

g BY .

Now we characterize this solution (f , g). Similar to proofs of the classic RKHS representer theorem (Schölkopf et al., 2001) and those of earlier RKBS representer theorems (Xu and Ye, 2019; Lin et al., 2019, etc.), we approach this via orthogonal decomposition. Consider the following set of function pairs (f, g) that map all data pairs (xi, yj) to 0:

NX ,Y,0 = (f, g)  BX  BY : fY (yj), gX (xi) BX ×BY = 0; i = 1, . . . , nx; j = 1, . . . , ny

We can see that NX ,Y,0 is closed under addition and scalar multiplication, making it a subspace of BX  BY . Taking our optimal (f , g), we can see that

(f , g) + (f 0, g0) BX BY  (f , g) BX BY for any (f 0, g0)  NX ,Y,0

(B.7)

thus showing that (f , g) is orthogonal to the subspace NX ,Y,0. Consider the left and right preimages of NX ,Y,0 under ·, · BX ×BY :

·, g

-1 BX ×BY

[NX ,Y,0]

=

f, ·

-1 BX ×BY

[NX ,Y,0]

=

f  BX :

fY (yj ), gX (xi) BX ×BY = 0;

i = 1, . . . , nx; j = 1, . . . , ny ; g  BY

f  BX :

fY (yj ), gX (xi) BX ×BY = 0;

i = 1, . . . , nx; j = 1, . . . , ny ; f  BX .

2As Fasshauer et al. (2015) note, any Hilbert space is strictly convex and smooth, so it seems reasonable to assume that an RKBS is also strictly convex and smooth.

18

Since

·, g

-1 BX ×BY

[NX ,Y,0]



BX

and

f, ·

-1 BX ×BY

[NX ,Y,0]



BY ,

we

can

consider

them

as

normed

vector

spaces

with norms · BX and · BY , respectively. From (B.7) and the definition of the direct sum norm (B.3),

f  + f 0 BX  f  BX g + g0 BY  g BY

for all f 0 

·, g

-1 BX ×BY

[NX ,Y,0] ,

for

arbitrary

g



BY

for all g0 

f, ·

-1 BX ×BY

[NX ,Y,0] ,

for

arbitrary

f



BX .

(B.8a) (B.8b)

We can then use (B.8) and Lemma 1 to say

f, (f ) BX = 0

for all f 

·, g

-1 BX ×BY

[NX ,Y,0] ,

for

arbitrary

g



BY

g, (g) BY = 0

for all g 

f, ·

-1 BX ×BY

[NX ,Y,0] ,

for

arbitrary

f



BX

which means

(f )  (g) 

·, g

-1 BX ×BY

[NX ,Y,0]



for all g  BY

f, ·

-1 BX ×BY

[NX ,Y,0]



for all f  BX .

(B.9a) (B.9b)

From (B.9a) and (3a)-(3b),

f 

·, g

-1 BX ×BY

[NX ,Y,0]

=

f  BX :

fY (yj ), gX (xi) BX ×BY = 0;

gBY

i = 1, . . . , nx; j = 1, . . . , ny

= f  BX : fY (yj), h BX ×BY = 0 h  span {(xi, ·); i = 1, . . . , nx} ;

j = 1, . . . , ny

=  span {(xi, ·); i = 1, . . . , nx} .

g  BY ;

(B.10)

And from (B.9b) and (3c)-(3d),

g 

f, ·

-1 BX ×BY

[NX ,Y,0]

=

g  BY :

fY (yj ), gX (xi) BX ×BY = 0;

f BX

i = 1, . . . , nx; j = 1, . . . , ny

= g  BY : h , fY (yj) BX ×BY = 0 h  span {(·, yj); j = 1, . . . , ny} ;

i = 1, . . . , nx

=  span {(·, yj); y = 1, . . . , ny} .

f  BX ;

(B.11)

Combining (B.9a) and (B.10), we get

(f )   span{(xi, ·); i = 1, . . . , nx}  = span{(xi, ·); i = 1, . . . , nx}

(B.12)

and by combining (B.9b) and (B.11), we get

(g)   span{(·, yj); j = 1, . . . , ny}  = span{(·, yj); j = 1, . . . , ny}

(B.13)

where in both (B.12) and (B.13) we use Proposition 2.6.6 of Megginson (1998) regarding properties of annihilators.

From (B.12) and (B.13) we can write that there exist two sets of parameters 1, . . . , nx  R and 1, . . . , ny  R

such that

nx

ny

(f ) = i(xi, ·) (g) = j(·, yj)

i=1

j=1

thus proving the claim.

19

B.3 Main Proof

Before beginning the proof, we state the following lemma regarding the existence of solutions of convex optimization problems on Banach spaces:
Lemma 4 (Ekeland and Témam, 1999, Chapter II, Proposition 1.2). Let B be a reflexive Banach space and S a closed, convex, and bounded (with respect to · B) subset of B. Let f : S  R  {+} be a convex function with a closed epigraph (i.e., it satisfies the condition that c  R  {+}, the set {v  S : f (v)  c} is closed). Then, the optimization problem
inf f (v)
vS
has at least one solution.

Xu and Ye (2019) and Lin et al. (2019) also reference Ekeland and Turnbull (1983) as a source for Lemma 4. We now restate Theorem 1 with the conditions on BX and BY filled in.

Theorem 1, Revisited. Suppose we have a kernel learning problem of the form in (11). Let  : X × Y  R,

(xi, yi) = X (xi), Y (yi) FX ×FY = fY (y), gX (x) and 2. Assume that {(xi, ·)}ni=x1 is linearly independent

BX
in

×BY be BY and

a reproducing kernel that {(·, yj )}nj=y 1 is

satisfying Definitions 1 linearly independent in

BX . Assume also that BX and BY are reflexive, strictly convex, and smooth. Then, the regularized empirical risk minimization problem (11) has a unique solution pair (f , g), with the property that

nx
(f ) = i(xi, ·)
i=1

ny
(g) = j(·, yj).
j=1

where i, j  R.
Proof. As before, we begin by proving existence and uniqueness of the solution pair (f , g). We first prove uniqueness using some basic facts about convexity. Assume that there exist two distinct minimizers
(f1, g1), (f2, g2)  BX  BY . Define (f3, g3) = 1/2[(f1, g1) + (f2, g2)]. Then, since BX and BY are strictly convex, we have

f3 BX = g3 BY =

1 2

(f1

+

f2)

1

<

BX

2

f1

1 BX + 2

f2

BX

1 2

(g1

+

g2)

BY

1 <
2

g1

1 BY + 2

g2

BY

and since RX and RY are convex and strictly increasing,

RX ( f3 BX ) = RX

1 2

(f1

+

f2)

BX

< RX

1 2

f1

1 BX + 2

f2

BX



1 2 RX (

f1

1 BX + 2 RX (

f2

BX )

and

RY ( g3 BY ) = RY

1 2

(g1

+

g2)

BY

< RY

1 2

g1

1 BY + 2

g2

BY



1 2 RY (

g1

1 BY + 2 RY (

g2

BY ).

Consider the regularized empirical risk minimization cost function (11)

T (f, g) = L(f, g) + X RX ( f BX ) + Y RY ( g BY )

where we use the shorthand

1

L(f, g) = nxny

L
i,j

xi, yj , zij ,

fY (y), gX (x) BX ×BY

.

20

We have that RX ( · BX ) and RY ( · BY ) are both convex via identities about composition of convex functions. The function L(f, g) is also convex since all the functions in the summand are convex in f and g.
Then, since we have assumed that T (f1, g1) = T (f2, g2), by plugging in some of the above inequalities we can write

T (f3, g3) = T

1 2

[(f1,

g1)

+

(f2,

g2)]

=L

1 2

[(f1,

g1)

+

(f2,

g2)]

+ RX

1 2

(f1

+

f2)

BX

+ RY

1 2

(g1

+

g2)

BY

<

1 L
2

(f1,

g1)

+

1 L
2

(f2,

g2)

+

1 2 RX

(

f1

1 BX ) + 2 RX (

f2

BX )

1 + 2 RY (

f1

1 BY ) + 2 RY (

f2

BY )

=

1 T
2

(f1, g1)

+

1 T
2

(f2, g2)

= T (f1, g1)

contradicting that (f1, g1) is a minimizer, and thus showing uniqueness of the solution. We now prove existence via Lemma 4. We already know that T (·) is convex. From the bilinearity of ·, · BX ×BY
and the convexity of L, L is continuous in f and g. Since the regularization functions RX and RY are convex and
strictly increasing, is follows that the functions RX ( f BX ) and RY ( g BY ) are continuous in f and g, respectively. Thus, T (f, g) is continuous. Consider the set

E = {(f, g)  BX  BY : T (f, g)  T (0, 0)}. The set E is nonempty (it contains at least (0, 0)), and we can see that

f, g BX BY = f BX + g BY  RX-1(T (f, 0)) + RY-1(T (0, g))
showing that E is bounded. So, by Lemma 4, we are guaranteed the existence of an optimal solution (f , g). Pick any (f, g)  BX × BY and consider the set

Df,g = xi, yj , fY (yj), gX (xi) BX ×BY : i = 1, . . . , nx; j = 1, . . . , ny
i.e., the set of pairs of points (xi, yj) along with the value that the function pair (f, g) maps to via the bilinear form at the pair of points (xi, yj).
From Lemma 3, there exists an element (f , g )  BX × BY such that (f , g ) interpolates Df,g perfectly, i.e.,
fY (yj ), gX (xi) BX ×BY = fY (yj ), gX (xi) BX ×BY ; i = 1, . . . , nx; j = 1, . . . , ny
whose Gâteaux derivatives of norms satisfy

(f )  span{(xi, ·); i = 1, . . . , nx} (g )  span{(·, yj); j = 1, . . . , ny}.

Further, this element (f , g ) obtains the minimum-norm interpolation of Df,g, i.e.,

f , g BX BY  f, g BX BY .

This last fact implies

T (f , g )  T (f, g).

21

Therefore, the unique optimal solution (f , g) also satisfies

(f )  span{(xi, ·); i = 1, . . . , nx} (g)  span{(·, yj); j = 1, . . . , ny}

which implies that suitable parameters 1, . . . , nx  R and 1, . . . , ny  R exist such that

nx
(f ) = i(xi, ·)
i=1

ny
(g) = j(·, yj)
j=1

proving the claim.

C Proof of Theorem 2

First, we state the following well-known lemma.

Lemma 5. For any two compact Hausdorff spaces X and Y, continuous function  : X × Y  R, and > 0, there exists an integer d > 0 and continuous functions  : X  R,  : Y  R, = 1, . . . , d such that

d
(x, y) -  (x) (y) <
=1

 x  X,y  Y.

Proof. The product space of two compact spaces X and Y, X × Y, is of course compact by Tychonoff's theorem. Consider the algebra
d
A = f^ : f^(x, y) =  (x) (y), x  X , y  Y .
=1
It is easy to show (i) that A is an algebra, (ii) that A is a subalgebra of the real-valued continuous functions on X × Y, and (iii) that A separates points. Then, combining the aforementioned facts, by the Stone-Weierstrass theorem A is dense in the set of real-valued continuous functions on X × Y.

Remark 9. In addition to helping us prove Theorem 2 below, Lemma 5 also serves as somewhat of an analog to Mercer's theorem for the more general case of asymmetric, non-PSD kernels. It is however weaker than Mercer's theorem in that the non-PSD nature of  means that the functions in the sum cannot be considered as eigenfunctions (with  =  ) with associated nonnegative eigenvalues.

Now we proceed to the proof of Theorem 2.

Proof. To keep the equations from becoming too cluttered, below we use q(t), k(s), (t), (s)  Rd as the vector

concatenation of the scalar functions {q (t)}, {k (s)}, { (t)}, and { (s)}, = 1, . . . , d, respectively. All sup norms

are with respect to X × Y.

Our proof proceeds similarly to the proof of Theorem 5.1 of Okuno et al. (2018). We generalize their theorem and

proof to non-Mercer kernels and simplify some intermediate steps. First, by applying Lemma 5, we can write that for

any 1, there is a d such that

 - T sup < 1

(C.1)

Now we consider the approximation of  and  by q and k , respectively. By the universal approximation theorem of multilayer neural networks (Cybenko, 1989; Hornik et al., 1989; Funahashi, 1989; Attali and Pagès, 1997, etc.), we know that for any functions  : X  R,  : Y  R and scalar 2 > 0, there is an integer m > 0 such that if q : X  R and k : Y  R are two-layer neural networks with m hidden units, then

 - q sup < 2 and  - k sup < 2

(C.2)

for all . Now, beginning from (14), we can write

 - qTk sup   - T sup + T - kTq sup

(C.3)

22

by the triangle inequality. Examining the second term of the RHS of (C.3),

T - qTk sup = T( - k) + ( - q)Tk sup  T( - k) sup + ( - q)Tk sup   sup  - k sup +  - q sup k sup

(C.4)

where the first inequality uses the triangle inequality and the second uses the Cauchy-Schwarz inequality. Finally, we can combine (C.1)-(C.4) to write

 - qTk sup   - T sup +  sup  - k sup

+  - q sup k sup

< 1 + d 2(  sup +  sup + d 2).



Picking 1 and 2 appropriately, e.g. 1 = /2 and 2 

d2 ((



sup+  2d2

sup )2 +2

)
, completes the proof.

D Experiment Implementation Details
Datasets The 2014 International Workshop on Spoken Language Translation (IWSLT14) machine translation dataset is a dataset of transcriptions of TED talks (and translations of those transcriptions). We use the popular German to English subset of the dataset. We use the 2014 "dev" set as our test set, and a train/validation split suggested in demo code for fairseq (Ott et al., 2019), where every 23rd line in the IWSLT14 training data is held out as a validation set.
The 2014 ACL Workshop on Statistical Machine Translation (WMT14) dataset is a collection of European Union Parliamentary proceedings, news stories, and web text with multiple translations. We use newstest2013 as our validation set and newstest2014 as our test set.
The Stanford Sentiment Treebank (SST) (Socher et al., 2013) is a sentiment analysis dataset with sentences taken from movie reviews. We use two standard subtasks: binary classification (SST-2) and fine-grained classification (SST-5). SST-2 is a subset of SST-5 with neutral-labeled sentences removed. We use the standard training/validation/testing splits, which gives splits of 6920/872/1821 on SST-2 and 8544/1101/2210 on SST-5.

Data Preprocessing On both translation datasets, we use sentencepiece3 to tokenize and train a byte-pair encoding (Sennrich et al., 2016) on the training set. We use a shared BPE vocabulary across the target and source languages. Our resulting BPE vocabulary size is 8000 for IWSLT14 DE-EN and 32000 for WMT14 EN-FR.
For SST, we train a sentencepiece BPE for each subtask separately, obtaining BPE vocabularies of size 7465 for SST-2 and 7609 for SST-5.

Models Our models are written in Pytorch (Paszke et al., 2019). We make use of the Fairseq (Ott et al., 2019) library for training and evaluation.
In machine translation, we use 6 Transformer layers in both the encoder and decoder. Both Transformer sublayers (attention and the two fully-connected layers) have a residual connection with the "pre-norm" (Wang et al., 2019b) ordering of Layer normalization -> Attention or FC -> ReLU -> Add residual. We use an embedding dimension of 512 for the learned token embeddings. For IWSLT14, the attention sublayers use 4 heads with a per-head dimension d of 128 and the fully-connected sublayers have a hidden dimension of 1024. For WMT14, following Vaswani et al. (2017)'s "base" model, the attention layers have 8 heads with a per-head dimension d of 64 and the fully-connected sublayers have a hidden dimension of 2048.
For SST, we use a very small, encoder-only, Transformer variant, with only two Transformer layers. The token embedding dimension is 64, each Transformer self-attention sublayer has 4 heads with per-head dimension d of 16, and the fully-connected sublayers have a hidden dimension of 128. To produce a sentence classification, the output of the second Transformer layer is average-pooled over the non-padding tokens, then passed to a classification head. This classification head is a two-layer neural network with hidden dimension 64 and output dimension equal to the number of classes; this output vector becomes the class logits.
3https://github.com/google/sentencepiece

23

Training We train with the Adam optimizer (Kingma and Ba, 2015). Following Vaswani et al. (2017), for machine translation we set the Adam parameters 1 = 0.9, 2 = 0.98.
On IWSLT14 DE-EN, we schedule the learning rate to begin at 0.001 and then multiply by a factor of 0.1 when the validation BLEU does not increase for 3 epochs. FOR WMT14 EN-FR, we decay proportionally to the inverse square root of the update step using Fairseq's implementation. For both datasets, we also use a linear warmup on the learning rate from 1e-7 to 0.001 over the first 4000 update steps.
On IWSLT14 DE-EN, we end training when the BLEU score does not improve for 7 epochs on the validation set. On WMT14 EN-FR, we end training after 100k gradient updates (inclusive of the warmup stage), which gives us a final learning rate of 0.0002. We train on the cross-entropy loss and employ label smoothing of 0.1. We use minibatches with a maximum of about 10k source tokens on IWSLT14 DE-EN and 25k on WMT14 EN-FR Also on WMT14, we ignore sentences with more than 1024 tokens.
For both SST subtasks, we also use a linear warmup from 1e-7 over 4000 warmup steps, but use an initial postwarmup learning rate of 0.0001. Similar to IWSLT14, we decay the learning rate by multiplying by 0.1 when the validation accuracy does not increase for 3 epochs, and end training when the validation accuracy does not improve for 8 epochs. Evaluation for machine translation Following Vaswani et al. (2017), we use beam-search decoding, with a beam length of 4 and a length penalty of 0.6, to generate sentences for evaluation. We use sacrebleu (Post, 2018) to generate BLEU scores. We report whole-word case-sensitive BLEU. Computational resource details Each IWSLT14 DE-EN training run was run on one Nvidia Quadro RTX 6000 video card and took about 2.5 hours. Each WMT14 EN-FR training run was run on two Nvidia Tesla V100 32 GB video cards and took about 36 hours. Each SST training run was run on one Nvidia Titan Xp video card and took only a few minutes.
24

