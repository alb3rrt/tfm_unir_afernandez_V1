Collaborative Nonstationary Multivariate Gaussian Process Model

arXiv:2106.00719v1 [cs.LG] 1 Jun 2021

Rui Meng 1 Herbert Lee 2 Kristofer Bouchard 1

Abstract
Currently, multi-output Gaussian process regression models either do not model nonstationarity, or are associated with severe computational burdens and storage demands. Nonstationary multivariate Gaussian process models (NMGP) use a nonstationary covariance function with an inputdependent linear model of coregionalisation to jointly model input-dependent correlation, scale, and smoothness of outputs. Variational sparse approximation relies on inducing points to enable scalable computations. Here, we take the best of both worlds: considering an inducing variable framework on the underlying latent functions in NMGP, we propose a novel model called the collaborative nonstationary Gaussian process model (CNMGP). For CNMGP, we derive computationally tractable variational bounds amenable to doubly stochastic variational inference. Together, this allows us to model data in which outputs do not share a common input set, with a computational complexity that is independent of the size of the inputs and outputs. We illustrate the performance of our method on synthetic data and three real datasets and show that our model generally provides better predictive performance than the stateof-the-art, and also provides estimates of timevarying correlations that differ across outputs.
1. Introduction
Multi-output regression problems have arisen in various fields, including multivariate physiological time-series analysis (Du¨richen et al., 2014), chemometrics (Burnham et al., 1999), and multiple-input mupltiple-output frequency nonselective channel estimation (Sa´nchez-Ferna´ndez et al., 2004). Multivariate Gaussian process models (MGP) generalise the powerful Gaussian process predictive model to vectorvalued random fields (A´ lvarez et al., 2010; A´ lvarez &
*Equal contribution 1Lawrence Berkeley National Laboratory 2University of California, Santa Cruz. Correspondence to: Rui Meng <rmeng@lbl.gov>. Copyright 2021 by the author(s).

Lawrence, 2011). Those models demonstrate improved prediction performance compared with the univariate Gaussian process because MGPs express correlation between outputs. Since the correlation information of data is encoded in the covariance function, modeling the flexible and computationally efficient cross-corvariance function is of interest. In the literature of MGP, many approaches to building cross-covariance functions are based on combining univariate covariance functions. Specifically, those approaches can be classified into three categories: the linear model of coregionalization (LMC) (Bourgault & Marcotte, 1991; Goulard & Voltz, 1992), convolution techniques (Ver Hoef & Barry, 1998; Ver Hoef et al., 2004; Gneiting et al., 2010), and use of latent dimensions (Apanasovich & Genton, 2010). In the LMC, the covariance function is expressed as the sum of Kronecker products between coregionalization matrices and a set of underlying covariance functions. The coregionalization matrices explain the correlation across the outputs and underlying covariance functions provide explanation of the correlation between different data points. (Meng et al., 2019) have recently proposed a general framework for multi-output regression problems dealing with the input-dependent correlation, scale and smoothness of outputs, especially for multivariate time series. It leverages a input-dependent linear model of coregionalization (Gelfand et al., 2004) to model time-varying scale, correlation and smoothness of outputs, where they assume the coregionalization coefficient matrix varies depending on the value of input. Identifiability of the model parameters is guaranteed by imposing structured GP priors on the coregionalization coefficient matrices just as in (Guhaniyogi et al., 2013). Additionally, that work puts a Gibbs prior on underlying covariance functions to model the time-varying smoothness of output, with another GP prior on the lengthscale parameters in the Gibbs prior (Heinonen et al., 2016). In our work, we build upon (Meng et al., 2019) by employing the inducing variable framework on all latent processes (Titsias & Lawrence, 2010), proposing tractable variational bounds amenable to doubly stochastic variational inference. This variational inference framework allows the model to handle missing data without increasing the computational complexity. We numerically provide evidence of the benefits of simultaneously modeling time-varying correlation, scale and smoothness in both a synthetic experiment and

CNMGP

three different real problems. The main contributions of this work are threefold. (1) To the best of our knowledge, this is the first time that doubly stochastic variational inference is incorporated into nonstationary multivariate Gaussian process models, allowing mini-batching learning with rather large batch-sizes. (2) We illustrate that our proposed CNMGP model succeeds in handling the time-varying correlation with missing data under different scenarios. (3) Our approach provides an estimate of the dynamics of correlation and smoothness of outputs across the input domain. The structure of this paper is presented as follows: In section 2, we review the NMGP model in (Meng et al., 2019) and propose our novel CNMGP model. We provide an efficient variational inference approach in Section 3. Our approach is illustrated on both synthetic experiments and three real datasets in Section 4. Finally, we present conclusions of in Section 5.
1.1. Related work Most GP-based multi-output regression models build correlated outputs by mixing a set of independent latent processes. The mixing can be a linear combination with fixed coefficients (Goulard & Voltz, 1992; Seeger et al., 2005; Bonilla et al., 2008). Those models are known as the linear coregionalization model (LMC) (Wackernagel, 2013) in the geostatistics literature. Based on the LMC structure, more sophisticated models are proposed. (Titsias & La´zaro-Gredilla, 2011) place a spike and slab prior over the coefficients. (Wilson et al., 2011; Nguyen & Bonilla, 2013) model more complex dependencies using input-dependent coefficients. The Gaussian process regression network of (Wilson et al., 2011; Nguyen & Bonilla, 2013) involves having Q latent processes where Q < D and D is the dimension size of the output. Each latent process has the same covariance function. The noiseless outputs are obtained by linearly mixing the latent processes with a D × Q stochastic coefficient matrix W (x). Each element in W (x) is modulated by the same Gaussian process with covariance Kw. Our model is similar to (Wilson et al., 2011; Nguyen & Bonilla, 2013) but more interpretable and flexible. We put a structured GP based prior on the stochastic coefficients for model identifiability, consider a nonstationary covariance function for latent processes to model time-varying smoothness, and assume Q = D, so the marginal distribution at time t is a Gaussian distribution with a strictly positive definite covariance matrix. We note that if Q > D the representation of the covariance matrix is not identifiable because we can find another coefficient W~ (x)  RD×Q such that W~ (x)W~ (x)T = W (x)W (x)T .

While we use the mixing construction in NMGP (Meng et al., 2019), the key differences in this work is the role of inducing variables and our stochastic variational inference approach. Inducing variables are the key catalyst for achieving sparsity in Gaussian process models in (Titsias & Lawrence, 2010; Hensman et al., 2013). (Nguyen et al., 2014) claim that sharing "sparsity structure" is not only a reasonable assumption, but a crucial component when modeling multi-output data. This work maintains an explicit representation of the posterior over the inducing variables that is learned from all outputs and facilicates scalable learning in as similar fashion to the approach in (Hensman et al., 2013; Nguyen et al., 2014).

2. Model Specification
Here we describe the nonstationary multivariate Gaussian process model (NMGP) of (Meng et al., 2019) and explain the corresponding main inference task in Section 2.1. Then we extend this work to a sparse version, named the collaborative nonstationary multivariate Gaussian process model.

2.1. Nonstaionary multivariate Gaussian process model Assume y(x)  RD is a vector-valued function of x  RP , where D is the dimension size of the outputs and P is the dimension size of the inputs. For multivariate time series, P = 1. The NMGP model assumes noisy observations y(x) are the linear combination of latent variables g(x)  RD, corrupted by Gaussian noise (x). The coefficients L(x)  RD×D of the latent functions are assumed to be a stochastic lower triangular matrix with positive values on the diagonal for model identification. Thus, the NMGP is defined as follows:

y(x) = f (x) + (x), f (x) = L(x)g(x),

(x) iid N (0, e2rrI) .

(1)

It assumes each latent function gd is independently sampled from a Gaussian process (GP) with covariance function Kg and the stochastic coefficients are modeled via a structured GP based prior as proposed in (Guhaniyogi et al., 2013)) such that

gd iid GP(0, Kg) ,

d = 1, . . . , D ,

lij 

GP(0, Kl) , logGP(0, Kl) ,

i>j, i=j,

 logGP(0, K ) ,

(2)

where determines the length scale of the correlations in K. Kg uses the Gibbs correlation function

Kg(x, x ) =

2 (x) (x)2 +

(x) (x )2

exp

-

x-x 2 (x)2 + (x )2

,

CNMGP

and logGP denotes the log Gaussian process (Møller et al., 1998). This prior guarantees the assumed structure of L(x). Given the stochastic coefficients L, the cross covariance function of f (x) is
Kf (x, x , m, m ) = Kg(x, x )lm(x)lm T (x ) , (3) where lm(x) denotes the mth row of L(x). With a deterministic coefficient matrix L(x)  L and a stationary GP for all latent processes g, this model is equivalent to the intrinsic coregionalization model (Goulard & Voltz, 1992). Let X = {xi}Ni=1 be the set of observed inputs and Y = {yi}Ni=1 be the set of observed outputs. We denote  as the concatenation of the coefficients and length-scale parameters, i.e.,  = (l, ~) evaluated at training inputs X. Here, l is a vector including the entries below the main diagonal and the entries on the diagonal in the log scale and ~ = log . Let  = (l,  , e2rr) be the hyper-parameters, where l and  are the hyper-parameters of the covariances Kl and K . According to the model specification in (1), the prior over  evaluated at the training inputs is a N (D(D + 1)/2 + 1) dimensional Gaussian with block diagonal covariance:
p(|l,  ) = N (0, C) where the first M (M + 1)/2 blocks of C are induced by the covariance function Kl and the last one block is induced by K . Given model parameters  and hyper-parameters , by marginalizing the latent function g, the conditional likelihood is

for function log Lii when i = j and uij for function Lij when i > j, and inducing variables v for function log (x) evaluated at inducing inputs Z. We denote those collective variables as l = {lij }ij , u = {uij }ij , g = {gd}Dd=1, w = {wd}Dd=1 and v. We redefine the model parameters  = (l, u, g, w, , v), and the prior of this model can be written as
p() = p(l|w)p(w)p(g|u, , v)p(u)p( |v)p(v) . (5)
In this framework, the conditional independence assumption is the core of this model to emphasize the sufficient statistics role of u, w and v. Here by sufficient statistics we mean, for any sparse process (say Lij), the values of the function on any other set of inputs are independent of the values on training inputs (lij) given the inducing variables uij. Instead of marginalizing the latent functions g, we propose an efficient variational inference in the next section.

3. Inference

We propose a structured variational distribution of model parameters  to approximate its posterior distribution as

q() = p(l|u)p(g|w, , v)p( |v)q(u, w, v) , (6)

where q(u, w, v) is the unknown variational distribution that fully decides the characteristics of q(). The inference of q(u, w, v) is thus of interest. We assume the parameters u, w, andv are Gaussian and mutually independent

D

q(u, w, v) = N (uij|muij, Siuj) N (wd|mwd , Sdw)N (v|mv, Sv) ,

ij

d=1

(7)

p(Y |, e2rr) = N (y|0, Kf + e2rrI) , where Kf is the covariance function Kf in (3) evaluated at training inputs X. Hence, the main inference task in NMGP is to maximize the posterior

p(|Y , )  p(Y |, e2rr)p(|l,  ) ,

(4)

which is computationally intractable in general because the computational complexity of p(|Y , ) is O(N 3D3). We propose an approach to avoid this issue via variational inference.

2.2. Collaborative nonstationary multivariate Gaussian process model
To alleviate the computational burden associated with (4), we introduce a shared set of inducing inputs Z = {zm}M m=1 that lies in the same space as the inputs X and a set of shared inducing variables wd for each gd evaluated at the inducing inputs Z. Likewise, we consider inducing variables uii

Given the definition of Gaussian process priors in (2), the conditional distributions p(g|w, ~, v), p( |v) and p(l|u) have closed-form expressions as follows

p(l|u) = log N (lii|µlii, lii) N (lij |µlij , lij ) ,

i=j

i>j

(8)

D

p(g|w, , v) = N (gd|µgd, gd) ,

(9)

d=1

p( |v) = log N ( |µ ,  ) .

(10)

The derivation of the conditional mean and conditional covariance matrix is available in mentary A.2.

3.1. Variational Lower Bound The evidence lower bound (ELBO) of the log likelihood of observations under our structured variational distribution

CNMGP

q() is derived using Jensen's inequality as:

log p(Y ) = log p(Y , )d

 Eq() log

p(Y |g, l)p(u)p(w)p(v) q(u, w, v)

ND

=

Eq(gn,ln) log(p(ynd|gn, ln))

n=1 d=1

+A,

(11)

where A = KL(q(u)||p(u)) + KL(q(w)||p(w)) + KL(q(v)||p(v)) is the regularization term, gn = {gdn = (gd)n}Dd=1 and ln = {lijn = (lij )n}ij . This technique has been used by (Titsias & Lawrence, 2010) and (Hensman et al., 2013) to derive variational inference for the single output case and it has been used by (Nguyen et al., 2014) for a multivariate output case. The benefit of this structure is that the conditional distributions in (8), (9) and (10) are cancelled in the derivation of the lower bound in (11) and alleviate the computational burden. In our case, we consider the inference for the hierarchical structure with both GP and log GP priors. Also, the first term in (11) shows that the observations are all conditionally independent given g and l, so that the lower bound decomposes across both inputs and outputs and this enables the use of stochastic optimization methods. Because q(u), p(u), q(w), p(w), q(v) and p(v) are all multivariate Gaussian distributions, the KL divergence terms are analytically tractable. The challenge is to solve for the individual expectations. Before we derive the expectation, we first provide the sampling approach for l and g from the variational distribution.
3.2. Efficient Sampling for Variational Distributions To achieve efficient sampling for l and g from the variational posterior q(), we marginalize unnecessary intermediate variables to make it efficient. By marginalizing the inducing variables u and w we obtain the marginal distributions

q(l) = log N (lii|µ~lii, ~ lii) N (lij |µ~lij , ~ lij ) ,

i=j

i>j

(12)

D

q(g| , v) = N (gd|µ~gd, ~ gd) ,

(13)

d=1

with a joint distribution q( , v) = p( |v)p(v), where the conditional mean and covariance matrix are derived in mentary A.1.

3.3. Learning the parameters of the Variational Distribution

According to the marginal distributions (12) and (13), the marginal distributions for latent factors gn and coefficients ln in (11) are

q(ln) = log N (liin|µ~liin, ~ili2n) N (lijn|µ~lijn, ~ilj2n) ,

i=j

i>j

D
q(gn| , v) = N (gnd|µ~gdn, ~dgn2 ) ,
d=1

where ~ilj2n and ~dgn2 = ~ gijnn are the nth diagonal element of ~ lij and ~ gij respectively. We propose two approaches to compute the individual expectations in the first term of (11). The first method considers direct sampling such that

Eq(gn,ln) log(p(ynd|gn, ln))

D
= log N (ynd| ldjngjn, e2rr)q(gn| n, v)q( n, v)

j=1

q(ld·n)d(ld·n, gn, n, v) .

(14)

The second approach marginalizes the latent variables gn and then the individual expectation can be written as

Eq(gn,ln) log(p(ynd|gn, ln))



=

D
log N (ynd| ldjnµ~gjn, e2rr)
j=1

-

1 2e2rr

D
ld2j n ~jgn2 
j=1

q( n, v)q(ld·n)d(ld·n, n, v)) .

(15)

The details of the marginalization are available in Supplementary A.3.

However, directly evaluating the ELBO is still challenging due to the nonlinearities introduced by our structured prior. Recent progress in black box variational inference (Ranganath et al., 2014; Kingma & Welling, 2013; Rezende et al., 2014; Titsias & La´zaro-Gredilla, 2014) avoids this difficulty by computing noisy unbiased estimates of the gradient of EBLO, via approximating the expectations with unbiased Monte Carlo estimates and relying on either score function estimators (Ranganath et al., 2014) or reparameterization gradients (Kingma & Welling, 2013; Rezende et al., 2014; Titsias & La´zaro-Gredilla, 2014) to differentiate through a sampling process. In practice, reparameterization gradients exhibit significantly lower variances than score function estimators (Ghosh et al., 2018). In this work, we leverage the reparameterization gradients for both methods to separate the source of randomness from the parameters with respect to which gradients are sought. Typically, for Gaussian variational approximation, the well known non-centered param-

CNMGP (a)
(c)
(b)

Figure 1. Posterior analysis for three synthetic datasets, LF, HF and VF. (1a), (1b) display the posterior predictive processes in Dimenstion 1 and Dimension 2, respectively. Blue, black and red dots refer to the training, testing and predictive data and the red dashed lines refer to 95% credible bands. (1c) shows the estimates of log length-scale function via posterior mean.

eterization,   N (µ, 2)   N (0, 1),  = µ +  , allows us to compute the Monte-Carlo gradients. The details of the reparameterization are available in Supplementary A.4. Suppose all independent normal variables in the reparameterization are denoted by z, then the MonteCarlo gradients are

Eq(gn,ln) log(p(ynd|gn, ln))

=

1 S

 log(p(ynd|gn(s), ld(s·n) )) ,
s

(16)

where S is the number of samples. gn(s) and ld(s·n) ) depends on the randomness from z(s)  N (0, I). We note that evaluating ELBO (11) involves two sources of stochasticity. First, we approximate the expectation in (14) or (15) to compute the unbiased estimates of the gradients of ELBO (16). Second, since ELBO (11) factorizes over observations, it allows us to approximate the bound with data sub-sampling stochasticity (Hoffman et al., 2010; 2013). On the other hand, all hyper-parameters  are allowed to be optimized in the stochastic optimization.

3.4. Prediction Model prediction depends on the inferred variational distribution q(u, w, v). Given a new input x, predictive distributions on the latent processes are obtained through the following sampling procedures. We first sample the length-scale process at training inputs X and a new input x from q( , ) = p( , |v)q(v)dv. We denote the sth samples as (s) and (s) respectively. Conditional on them, we sample the latent process g(s) at input x from q(gd| (s), (s)) = p(gd| (s), (s), w)q(w)dw. We sample the coefficients l(s) at input x from q(lij) =

p(lij|uij)q(uij)duij. Finally, given g(s) and l we sample the observations y(s) at input x from q(y|g(s), l(s)) = p(y|g(s), l(s), )p( )d .
3.5. Inference for Missing Data Because in ELBO (11), observations {ynd} are mutually conditional independent on all the model parameters  and hyper-parameters , it is natural to take the sum of observations which we observed in the missing data case.
4. Experiments
This section illustrates the performance of our model with numerical results. In particular, we focus on multivariate time series where the input dimension is one. We first show that our approach can model the time-varying correlation and smoothness of outputs on 2D synthetic datasets in three scenarios with respect to different types of frequencies. We then compare the model predictive performance with other sparse Gaussian process models on three real datasets. Finally, we explore the dynamics of correlation of neuronal activities from different channels using electrocorticography data. All experiments are run on Ubuntu system with Intel(R) Core(TM) i7-7820X CPU @ 3.60GHz and 128G memory.
4.1. Synthetic Experiments We conduct experiments on three synthetic time series with low frequency (LF), high frequency (HF) and varying frequency (VF) respectively. They are generated from the

CNMGP

Table 1. Prediction measurements on three synthetic datasets and different models. LF denotes the low-frequency dataset, HF the

high-frequency dataset, and VF the time-varying dataset. Three prediction measures are provided. RMSE is root mean square error, ALCL

is average length of confidence interval, and CR is coverage rate. All three measurements are summarized by the mean and standard

deviation across 10 runs with different random initializations, i.e., 2.25(1.33e-13) denotes mean 2.25 with standard deviation 1.33e-13.

Data

Model

RMSE

ALCI

CR

IGPR (Rasmussen & Kuss, 2004) 2.25(1.33e-13) 2.18(1.88e-13)

0.835(0)

LF

ICM (Wackernagel, 2013) CMOGP (Nguyen et al., 2014)

2.26(2.54e-5) 2.18(1.22e-5)

0.835(0)

1.43(6.12-2) 1.36(1.98e-1) 0.651(3.00e-2)

CNMGP

1.00(1.43e-1) 2.21(6.56e-2) 0.892(1.63e-2)

IGPR (Rasmussen & Kuss, 2004) 1.51(6.01e-14) 3.17(1.30e-13) 0.915(2.22e-16)

HF

ICM (Wackernagel, 2013) CMOGP (Nguyen et al., 2014)

1.52(1.01e-5) 3.17(1.19e-5) 1.29(3.04e-2) 2.34(3.31e-1)

0.910(0) 0.729(3.07-2)

CNMGP

1.10(1.98e-1) 2.74(7.94e-2) 0.930(1.14e-2)

IGPR (Rasmussen & Kuss, 2004) 1.64(8.17e-14) 3.19(3.02e-13)

0.875(0)

VF

ICM (Wackernagel, 2013) CMOGP (Nguyen et al., 2014)

1.66(2.37e-3) 3.16(1.49e-3) 0.880(1.50e-3) 2.24(3.08e-1) 2.56(9.29e-1) 0.697(1.56e-1)

CNMGP

1.43(9.77e-2) 2.92(1.21e-1) 0.887(9.80e-3)

system of equations
y1(t) = 5 cos(2wts) + 1(t) , y2(t) = 5(1 - t) cos(2wts) - 5t cos(2wts) + 2(t) ,
(17)
where { i(t)}2i=1 are independent standard white noise processes. The value of w refers to the frequency and the value of s characterizes the smoothness. The LF and HF datasets use the same s = 1, implying the smoothness is invariant across time. But they employ different frequencies, w = 2 for LF and w = 5 for HF (i.e., two periods and five periods in a unit time interval respectively). The VF dataset takes s = 2 and w = 5, so that the frequency of the function is gradually increasing as time increases. For all three datasets, the system (17) shows that as time t increases from 0 to 1, the correlation between y1(t) and y2(t) gradually varies from positive to negative. Within each dataset, we randomly select 200 training data, in which 100 time stamps are sampled on the interval (0, 0.8) for the first dimension and the other 100 time stamps sampled on the interval (0.2, 1) for the second dimension. For the test inputs, we randomly select 100 time stamps on the interval (0, 1) for each dimension. In the stochastic optimization, we use the learning rates of 0.005 for all parameters with 2000 epochs. We note that the length-scale parameters are sensitive to the model performance, and for the three datasets the fixed length-scale exp(2) for covariance function Kl and fixed length-scale exp(0) for covariance function K work well. We optimize the other hyper-parameters in the optimization. We quantify the model performance in terms of root mean square error (RMSE), average length of confidence interval (ALCI), and coverage rate (CR) on the test set. Those results

are reported by the mean and standard deviation with 10 different random initializations. Quantitative comparisons relating to all three datasets are in Table 1. We compare with independent Gaussian process regression (IGPR) (Rasmussen & Kuss, 2004), the intrinsic coregionalization model (ICM) (Wackernagel, 2013) and Collaborative Multi-Output Gaussian Processes (CMOGP) (Nguyen et al., 2014) on the same synthetic datasets. In both CMOGP and CNMGP models, we use 20 inducing variables. We did not compare with the NMGP model (Meng et al., 2019) because the corresponding inference cannot handle missing data. We report the posterior predictive processes of Dimensional 1 (Figure 1a) and Dimension 2 (Figure 1b) for datasets LF, HF and VF in Figure 1. Blue, black and red dots refer to the training, testing and predictive data and the red dashed lines refer to 95 credible bands. Comparing the predictive data between the two dimensions, we find that CNMGP correctly learns the varying correlation from postive to negative. CNMGP model also displays the correct characteristics of the smoothness (Figure 1c) in three different datasets. Table 1 illustrate that our prediction results significantly outperform those from other models.
4.2. Real Data Experiments We further examine model predictive performance on three real datasets. Due to the large size of real data, standard Gaussian process models in synthetic experiments cannot handle them. Therefore, we compare our model with two sparse Gaussian process models, i.e., independent sparse Gaussian process regression (ISGPR) (Snelson & Ghahramani, 2006) and sparse linear model of corregionalization (SLMC) (Wackernagel, 2013) implemented using the GPy package from the Sheffield machine learning group. Moreover, we explore the dynamics of correlation of neuronal

CNMGP

activity visually with electorcorticography data.
4.2.1. ENVIRONMENTAL TIME SERIES DATA The first experiment is conducted on a PM2.5 dataset, coming from the UCI Machine Learning Repository (Liang et al., 2015). PM2.5 describes fine inhalable particles with diameters that are generally 2.5 micrometers and smaller and this dataset is hourly data containing the PM2.5 samples in five cities in China along with meteorological data, from Jan 1st, 2010 to Dec 31st, 2015. We consider six important attributes: PM2.5 concentration (PM), dew point (DEWP), temperature (TEMP), humidity (HUMI), pressure (PRES) and cumulated wind speed (lws). In order to be able to compare with SLMC, which cannot run on the whole dataset, we use the first 5000 standardized multivariate records. Those records have some missing values and except for missing values we have a total of 29710 observable values. For each feature, we standardize by subtracting the mean value and dividing features by its standard deviation. Finally, 20% of data of PM are taken as testing data while the remaining are treated as training data. Thus, in the 5000 records, there are 28768 output variable observations in the training set, and 942 PM values in the test set. We considered three independent experiments for CNMGP with 50, 100 and 200 equispaced inducing inputs on time range (0, 5000). The length-scale parameters were set to exp(10) for both KL and K , ran for 30 epochs with learning rate 0.01, and had batch size 1024. For the comparators, we fit SGPR and SLMC models with 100 equispaced inducing inputs. The root mean squared errors (RMSE) on the testing data are shown in Table 2, illustrating that CNMGP had better prediction performance compared with the ISGPR and SLMC. We show the training/testing time and the prediction results with different mini-batch sizes in the supplementary.
4.2.2. RESTING-STATE FUNCTIONAL MRI DATA The second experiment explores the functional connectivity of the brain, using a publicly available resting-state functional MRI (rs-fMRI) database obtained from the Human Connectome Project (HCP) S12000 data release (Smith et al., 2013) for 812 subjects. The HCP pre-processing pipeline (WU-Minn, 2017) yielded one representative time series across 4800 time points per independent component analysis (ICA) component for each subject at several different dimensionalities. We used the rs-fMRI timeseries from 15 ICA components with a random subject ID 990366 in this experiment. Specifically, we standardized the ICA components by subtracting the mean value of each feature and dividing it by its standard deviation. 20% of data in the first component are treated as the testing data while the remaining are treated as training data. Then we have 71040

training data and 960 testing data. We conducted the experiments using the same models used for the PM2.5 dataset. Because the SLMC model does not scale well, the prediction result is not available via our computing resource. Therefore, we only compared our results on CNMGP and ISGPR for the HCP dataset in Table 2. In this experiment, CNMGP set the length-scale parameters to exp(5) for both KL and K and run 10 epochs with learning rate 0.01 and batch size 1024. Table 2 shows that as the number of inducing points increases, the prediction performance improves and our model always outperforms the ISGPR. Even when we only take 50 inducing inputs for CNMGP, the prediction result is still better than that for SGPR with 100 inducing inputs. In the supplementary, we report training and testing times, and we also report the prediction results with different mini-batch sizes.
4.2.3. ELECTROCORTICOGRAPHY DATA Finally, we evaluate our model on Electrocorticography (ECoG) data collected in the Bouchard Lab(Dougherty et al., 2019). High-gamma activity from ECoG is a commonlyused signal containing the majority of task relevant information for understanding the human brain (Livezey et al., 2019), and the experiments in (Dougherty et al., 2019) simultaneously record µECoG cortical surface electrical potentials (CSEPs) and demonstrate that stimulus evoked CSEPs carry a multi-modal frequency response peaking in the H range. We selected a 5-second time interval and extracted the z-scored high gamma band (70-170Hz) from 25 channels on 5 × 5 subgrid located at the center of the 16 × 8 grid. For each channel, the records are sampled at 400Hz and thus we have 2000 time points. To illustrate the data, we plotted the functional boxplot (Sun & Genton, 2011; Meng et al., 2017) for the time series of the 25 channels in Figure 2. Next we conduct two experiments, experiment E1 for data within 2 seconds and experiment E2 for data within 5 seconds. We run 40 epochs for each experiment. To let the number of iterations within each epoch for both experiments close, we set the batch size as 512 for E1 and 2048 for E2. In experiment E1, to compare the models' performance, we randomly took 20% of data in the centered channel as testing data and took the remaining as training data. It implies we have 160 samples in the centered channel in the training set and 19840 samples in the testing set. The root square errors of testing data are reported in Table 2 and the training time and test time are reported in the supplementary. The predictive results show that our model cannot compete ISGPR and SLMC models. The reason is because that ECoG data is smooth with negligible noise in each channel and it is easy to predict the testing data using the nearby data within the channel. Learning the cross-correlation in ECoG

CNMGP

Table 2. Empirical results for PM2.5 dataset and HCP dataset. Each model's performance is summarized by root mean square error on

testing data (RMSE). The number of inducing points is given in parentheses.

Data ISGPR (100) (Snelson & Ghahramani, 2006) SLMC (100) (Wackernagel, 2013) CNMGP (50) CNMGP (100) CNMGP (200)

PM2.5

0.994

0.948

0.840

0.708

0.625

HCP

1.023

-

1.008

0.997

0.899

ECoG

0.311

0.446

0.707

0.658

0.657

data using CNMGP model does not contribute to better (a)

(b)

prediction result. Moreover, the inference in the CNMGP

model overestimate the variance of noises and causes under-

fitting result in ECoG data . However, CNMGP model

can provide estimates of the time-varying correlation for

pairwise channels while other models cannot.

(c)

(d)

(a)

(b)

Figure 2. Information of the ECoG data. (2a) refers to the z-score curves for the five channels on the diagonal of grids. (2b) provides the functional boxplot of all z-score curves on the grids.
Next, we trained the whole data in experiment E1 and experiment E2. Assume we treat the distance between two consecutive samples as one unit, i.e., ti+1 - ti = 1. We considered 50 fixed equally spaced inducing inputs on the time interval. We set the length-scale parameters to exp(10) for KL by assuming that the coefficient should smoothly change across time. For the length-scale parameters for K , we assumed that the length-scale function is flexible and less smooth and set exp(5) for E1 and exp(2) for E2. This is because the distances between the consecutive inducing inputs in E1 are E2 are 16 and 40 and the default hyper-parameters guarantee the GP can reasonably learn the dependence. We show the posterior predictive processes for one same channel from two experiments in Figure 3. For the short time scales, our model fits the time series well. Considering the same number of inducing inputs, as the length of time increases, the fitting performance becomes worse. This is because as the length of time series increases, the same number of inducing variables needs to summarize more nonstationary information. This causes the model to have more difficulty simultaneously being sensitive to the local information, making the prediction process more smooth as shown in Figure 3. We estimated the correlation process for pairwise channels using the posterior mean and took the average of those processes for which the distance between all pairs of channels

Figure 3. Posterior analysis for the ECoG for experiment E1 (top) and experiment E2 (bottom). (3a) and (3c) show the posterior predictive process for the centered channel, and (3b) and (3d) show the averaged correlation processes for distance d = 1, 2, 3 respectively.
with a constant d. We plot the averaged correlation processes for distance d = 1, 2, 3 in Figure 3. The resulting correlation processes from the two experiments show that as the distance between pair of channels increases, the correlation decreases. Moreover, the dynamical behavior in E2 in the first 2 seconds is consistent with that in E1. It implies our model estimates are robust to the length of the time series.
5. Conclusions
We proposed a novel collaborative nonstationry multivariate Gaussian process model (CNMGP). It jointly models inputdependent correlation, scale, and smoothness of outputs and it is amenable to doubly stochastic inference with a computation complexity independent of the size of inputs and outputs. Compared with the NMGP model in (Meng et al., 2019), our model provides a natural extension to missing data cases and succeeds in handling time-varying correlations under different scenarios. We also show that our model achieves better prediction performance than stateof-the-art models in synthetic experiments and real data experiments. Moreover, we provide a estimation of the correlation of outputs across input domains, as demonstrated in the ECoG experiment. In future, we will explore the CNMGP model with exogenous variables, which will better model the ECoG experiments, in which the animal was being presented with auditory stimuli, which drives the recorded neural activity.

CNMGP

References
A´ lvarez, M., Luengo, D., Titsias, M., and Lawrence, N. D. Efficient multioutput gaussian processes through variational inducing kernels. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pp. 25­32, 2010.
A´ lvarez, M. A. and Lawrence, N. D. Computationally efficient convolved multiple output gaussian processes. The Journal of Machine Learning Research, 12:1459­1500, 2011.
Apanasovich, T. V. and Genton, M. G. Cross-covariance functions for multivariate random fields based on latent dimensions. Biometrika, 97(1):15­30, 2010.
Bonilla, E. V., Chai, K. M., and Williams, C. Multi-task gaussian process prediction. In Advances in neural information processing systems, pp. 153­160, 2008.
Bourgault, G. and Marcotte, D. Multivariable variogram and its application to the linear model of coregionalization. Mathematical Geology, 23(7):899­928, 1991.
Burnham, A. J., MacGregor, J. F., and Viveros, R. Latent variable multivariate regression modeling. Chemometrics and Intelligent Laboratory Systems, 48(2):167­180, 1999.
Dougherty, M. E., Nguyen, A. P., Baratham, V. L., and Bouchard, K. E. Laminar origin of evoked ecog highgamma activity. In 2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), pp. 4391­4394. IEEE, 2019.
Du¨richen, R., Pimentel, M. A., Clifton, L., Schweikard, A., and Clifton, D. A. Multitask gaussian processes for multivariate physiological time-series analysis. IEEE Transactions on Biomedical Engineering, 62(1):314­322, 2014.
Gelfand, A. E., Schmidt, A. M., Banerjee, S., and Sirmans, C. Nonstationary multivariate process modeling through spatially varying coregionalization. Test, 13(2):263­312, 2004.
Ghosh, S., Yao, J., and Doshi-Velez, F. Structured variational learning of bayesian neural networks with horseshoe priors. arXiv preprint arXiv:1806.05975, 2018.
Gneiting, T., Kleiber, W., and Schlather, M. Mate´rn crosscovariance functions for multivariate random fields. Journal of the American Statistical Association, 105(491): 1167­1177, 2010.

Guhaniyogi, R., Finley, A. O., Banerjee, S., and Kobe, R. K. Modeling complex spatial dependencies: Low-rank spatially varying cross-covariances with application to soil nutrient data. Journal of Agricultural, Biological, and Environmental Statistics, 18(3):274­298, 2013.
Heinonen, M., Mannerstro¨m, H., Rousu, J., Kaski, S., and La¨hdesma¨ki, H. Non-stationary gaussian process regression with hamiltonian monte carlo. In Artificial Intelligence and Statistics, pp. 732­740. PMLR, 2016.
Hensman, J., Fusi, N., and Lawrence, N. D. Gaussian processes for big data. In Proceedings of the TwentyNinth Conference on Uncertainty in Artificial Intelligence, UAI'13, pp. 282­290, Arlington, Virginia, United States, 2013. AUAI Press. URL http://dl.acm. org/citation.cfm?id=3023638.3023667.
Hoffman, M., Bach, F. R., and Blei, D. M. Online learning for latent dirichlet allocation. In advances in neural information processing systems, pp. 856­864, 2010.
Hoffman, M. D., Blei, D. M., Wang, C., and Paisley, J. Stochastic variational inference. The Journal of Machine Learning Research, 14(1):1303­1347, 2013.
Kingma, D. P. and Welling, M. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
Liang, X., Zou, T., Guo, B., Li, S., Zhang, H., Zhang, S., Huang, H., and Chen, S. X. Assessing beijing's pm¡sub¿2.5¡/sub¿ pollution: severity, weather impact, apec and winter heating. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 471 (2182):20150257, 2015. doi: 10.1098/rspa.2015.0257. URL https://royalsocietypublishing. org/doi/abs/10.1098/rspa.2015.0257.
Livezey, J. A., Bouchard, K. E., and Chang, E. F. Deep learning as a tool for neural data analysis: speech classification and cross-frequency coupling in human sensorimotor cortex. PLoS computational biology, 15(9):e1007091, 2019.
Meng, R., Saade, S., Kurtek, S., Berger, B., Brien, C., Pillen, K., Tester, M., and Sun, Y. Growth curve registration for evaluating salinity tolerance in barley. Plant methods, 13 (1):18, 2017.
Meng, R., Soper, B., Lee, H., Liu, V. X., Greene, J. D., and Ray, P. Nonstationary multivariate gaussian processes for electronic health records. arXiv preprint arXiv:1910.05851, 2019.

Goulard, M. and Voltz, M. Linear coregionalization model: tools for estimation and choice of cross-variogram matrix. Mathematical Geology, 24(3):269­286, 1992.

Møller, J., Syversveen, A. R., and Waagepetersen, R. P. Log gaussian cox processes. Scandinavian journal of statistics, 25(3):451­482, 1998.

CNMGP

Nguyen, T. and Bonilla, E. Efficient variational inference for gaussian process regression networks. In Artificial Intelligence and Statistics, pp. 472­480, 2013.
Nguyen, T. V., Bonilla, E. V., et al. Collaborative multioutput gaussian processes. In UAI, pp. 643­652, 2014.
Ranganath, R., Gerrish, S., and Blei, D. Black box variational inference. In Artificial intelligence and statistics, pp. 814­822. PMLR, 2014.
Rasmussen, C. and Kuss, M. Gaussian processes in reinforcement learning. In Advances in Neural Information Processing Systems 16, pp. 751­759, Cambridge, MA, USA, June 2004. Max-Planck-Gesellschaft, MIT Press.
Rezende, D. J., Mohamed, S., and Wierstra, D. Stochastic backpropagation and approximate inference in deep generative models. In International Conference on Machine Learning, pp. 1278­1286, 2014.
Sa´nchez-Ferna´ndez, M., de Prado-Cumplido, M., ArenasGarc´ia, J., and Pe´rez-Cruz, F. Svm multiregression for nonlinear channel estimation in multiple-input multipleoutput systems. IEEE transactions on signal processing, 52(8):2298­2307, 2004.

Titsias, M. and La´zaro-Gredilla, M. Doubly stochastic variational bayes for non-conjugate inference. In International conference on machine learning, pp. 1971­1979, 2014.
Ver Hoef, J. M. and Barry, R. P. Constructing and fitting models for cokriging and multivariable spatial prediction. Journal of Statistical Planning and Inference, 69(2):275­ 294, 1998.
Ver Hoef, J. M., Cressie, N., and Barry, R. P. Flexible spatial models for kriging and cokriging using moving averages and the fast fourier transform (fft). Journal of Computational and Graphical Statistics, 13(2):265­282, 2004.
Wackernagel, H. Multivariate geostatistics: an introduction with applications. Springer Science & Business Media, 2013.
Wilson, A. G., Knowles, D. A., and Ghahramani, Z. Gaussian process regression networks. arXiv preprint arXiv:1110.4411, 2011.
WU-Minn, H. 1200 subjects data release reference manual. URL https://www. humanconnectome. org, 2017.

Seeger, M., Teh, Y.-W., and Jordan, M. Semiparametric latent factor models. In AISTATS, 2005.

Smith, S. M., Beckmann, C. F., Andersson, J., Auerbach, E. J., Bijsterbosch, J., Douaud, G., Duff, E., Feinberg, D. A., Griffanti, L., Harms, M. P., et al. Resting-state fmri in the human connectome project. Neuroimage, 80: 144­168, 2013.

Snelson, E. and Ghahramani, Z. Sparse gaussian processes using pseudo-inputs. In Weiss, Y., Scho¨lkopf, B., and Platt, J. C. (eds.), Advances in Neural Information Processing Systems 18, pp. 1257­1264. MIT Press, 2006. URL http://papers.nips.cc/paper/
2857-sparse-gaussian-processes-using-pseudo-inputs. pdf.

Sun, Y. and Genton, M. G. Functional boxplots. Journal of Computational and Graphical Statistics, 20(2):316­334, 2011.

Titsias, M. and Lawrence, N. D. Bayesian gaussian process latent variable model. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pp. 844­851, 2010.

Titsias, M. and La´zaro-Gredilla, M. Spike and slab variational inference for multi-task and multiple kernel learning. Advances in neural information processing systems, 24:2339­2347, 2011.

Supplementary Material

arXiv:2106.00719v1 [cs.LG] 1 Jun 2021

Rui Meng 1 Herbert Lee 2 Kristofer Bouchard 1

A. Derivations in Model inference
A.1. Derivations for prior distributions
Given the model specification, we have the conditional distributions in (8), (9) and (10). The conditional mean and covariance matrices are derived as follows: µlij = Kl(X, Z)Kl(Z, Z)-1uij , µgd = Kg(X, Z)Kg(Z, Z)-1wd , µ = K (X, Z)K (Z, Z)-1v , lij = Kl(X, X) - Kl(X, Z)Kl(Z, Z)-1Kl(Z, X) , gij = Kg(X, X) - Kg(X, Z)Kg(Z, Z)-1Kg(Z, X) , ij = K (X, X) - K (X, Z)K (Z, Z)-1K (Z, X) .
A.2. Derivations for variational distributions
We claim that the conditional mean and covariance matrices are shown as follows:

µ~lij = Kl(X, Z)Kl(Z, Z)-1muij , µ~gd = Kg(X, Z)Kg(Z, Z)-1mwij , ~ lij = Kl(X, X) - Kl(X, Z)Kl(Z, Z)-1Kl(Z, X)
+ Kl(X, Z)Kl(Z, Z)-1SiujKl(Z, Z)-1Kl(Z, X) , ~ gd = Kg(X, X) - Kg(X, Z)Kg(Z, Z)-1Kg(Z, X)
+ Kg(X, Z)Kg(Z, Z)-1SiwjKg(Z, Z)-1Kg(Z, X) .

This result comes from the Lemma 1.

Lemma 1 Suppose y|x K12K2-21K21) and x 

 N (K12K2-21x, K11 - N (µ, ). The marginalized

distribution of Y is

y  N (µ~, ~ ) ,

where µ~ K12K2-21

=KK2-211K2K212-.21Aµndanthde

~ = K11 - K12K2-21K21 marginal distribution of yn

+ is

yn  N (µ~n, ~n2 ) ,

*Equal contribution 1Lawrence Berkeley National Laboratory 2University of California, Santa Cruz. Correspondence to: Rui Meng <rmeng@lbl.gov>.

Copyright 2021 by the author(s).

wk~nThKere2-21µ~nK2-=21k~k~nnT.

Kk~nT22ius thaenndth~rn2ow=of

n2 K12

- k~nT K22k~nT and ~n2 is the

+ it

element of the diagonal of K11.

A.3. Derivations in the computation of ELBO We first introduce Lemma 2 as follows

Lemma 2 Suppose y|f  N (y|Xf , 2I) and f 

N (f |u, ). Then log N (y|Xf , 2I)N (f |µ, )df =

log

N

(y|X µ,

2I)

-

1 22

tr(X T

X ).

Proof 1 Let the dimension of y be n, then

log N (y|Xf , 2I)N (f |µ, )df

=

Ep(f

|mu,)(-

n 2

log(22)

-

1 22

(y

T

y

-

2yT

(Xf )

+

fT

XT Xf ))

=

-

n 2

log(22)

-

1 22

tr

yyT - 2XuyT + XT X(µµT + )

=

log

N (y|Xµ, 2I)

-

1 22

tr(XT X)

.

Then according to Lemma 2, the individual expectation can be rewritten as

Eq(gn,ln) log(p(ynd|gn, ln))

= log N (ynd|ldT·ngn, e2rr)N (gn|µ~gn, diag(~1gn2 , · · · , ~Dg2n))

q( ~,v)q(ld·n)d(ld·n, gn, ~, v)



=

D
log N (ynd| ldjnµ~gn, e2rr) -
j=1

1 2e2rr

D
ld2j n ~jgn2 
j=1

q( ~, v)q(ld·n)d(ld·n, ~, v) .

A.4. Derivations for reparameterization The reparameterization is proposed for the distribution q( n, v), q(gn| n, v) and q(ld·n).

· As for q( n, v), we have

v

=

mv

+

S

v

1 2

zv

,

n = exp K (xn, Z)K (Z, Z)-1v + K (xn, xn)

1
- K (xn, Z)K (Z, Z)-1K (Z, xn) 2 zn ,

CNMGP supplementary

where zv  N (0, I) and zn  N (0, 1). · As for q(gn| n, v), we have
gdn = µ~gdn + ~dgnzdgn where zdgn  N (0, 1). · Finally, as for q(ld·n), we have

lijn =

µ~liin + ~ilinzilin exp µ~lijn + ~iljnziljn

i=j i>j

where ziljn iid N (0, 1).

B. Real data experiments
B.1. Prediction result under different mini-batch sizes We explored how the mini-batch size B affects the prediction result in datasets, PM2.5 and HCP. Specifically, considering different number of mini-batch sizes, we plotted the RMSEs on the testing data during the training process in Figure 1. For both datasets, Figure 1 illustrates that RMSEs would converge to the same value as training time increases. In the PM2.5 data, the prediction performance monotonically improves with time increasing. However, in the HCP data, the RMSEs with different mini-batch sizes converge differently. When the batch size increases, the prediction performance becomes better. Empirical results for the PM2.5 data and HCP data suggest that the mini-batch size may affect the predictive performance in practice. The behavior depends on the characteristics of data.

Table 1. Empirical results for PM2.5 dataset and HCP dataset.

Each model's performance is summarized by training time per

epoch (Tr. Time/epoch) and testing time (Test Time). The number

of inducing points is given in parentheses.

Model

PM2.5

HCP

ECoG

Tr.Time Test Tr.Time Test Tr.Time Test

/ epoch Time / epoch Time / epoch Time

ISGPR (100) (?) 0.88s 0.02s 0.48s 0.02s 0.01s 0.00s

SLMC (100) (?) 14.22s 0.05s

-

-

25.28s 0.07s

CNMGP (50) 3.45s 0.01s 20.12s 0.05s 37.43s 0.09s

CNMGP (100) 8.34s 0.04s 45.10s 0.08s 117.49s 0.20s

CNMGP (200) 19.27s 0.15s 208.93s 0.17s 209.47s 0.41s

Figure 1. The root mean squared error on the testing data for two datasets i.e. PM2.5 and HCP. All experiments are conducted using CNMGP method with the same 100 equispace inducing inputs but considering different mini batch-sizes (BS).
B.2. Running time We provide the training time per epoch (Tr. Time/epoch) and testing time (Test Time) in Table 1.

