arXiv:2106.01737v1 [cs.DM] 3 Jun 2021

Optimal sets of questions for Twenty Questions
Yuval Filmus1 and Idan Mehalel1
1The Henry and Marilyn Taub Faculty of Computer Science, Technion, Israel. The research was funded by ISF grant 1337/16.
June 4, 2021
Abstract In the distributional Twenty Questions game, Bob chooses a number x from 1 to n according to a distribution µ, and Alice (who knows µ) attempts to identify x using Yes/No questions, which Bob answers truthfully. Her goal is to minimize the expected number of questions. The optimal strategy for the Twenty Questions game corresponds to a Huffman code for µ, yet this strategy could potentially uses all 2n possible questions. Dagan et al. constructed a set of 1.25n+o(n) questions which suffice to construct an optimal strategy for all µ, and showed that this number is optimal (up to sub-exponential factors) for infinitely many n. We determine the optimal size of such a set of questions for all n (up to sub-exponential factors), answering an open question of Dagan et al. In addition, we generalize the results of Dagan et al. to the d-ary setting, obtaining similar results with 1.25 replaced by 1 + (d - 1)/dd/(d-1).
1 Introduction
The distributional Twenty Questions game is a cooperative game between two players, Alice and Bob. Bob picks an object in Xn = {x1, . . . , xn} according to a distribution µ known to both players, and Alice determines the object by asking Yes/No questions, to which Bob answers truthfully. Alice's goal is to minimize the expected number of questions she asks.
This game is often related to information theory (see [CT06], for example) as an interpretation of Shannon's entropy [Sha48]. Moreover, it is the prototypical example of a combinatorial search game [Kat73, AW87, ACD13]. It is also a model of combinatorial group testing [Dor43], and can be interpreted as a learning task in the interactive learning model [CAL94].
In this game, Alice's strategy corresponds to a prefix code: the code of x  Xn is the list of Bob's answers to all questions asked by Alice. Alice's optimal strategy therefore corresponds to a minimum redundancy code for µ. Huffman [Huf52] (and, independently, Zimmerman [Zim59]) showed how to construct such a strategy efficiently. However, the strategy produced by Huffman's algorithm could use arbitrary questions. We ask:
What is the smallest set of questions that allows Alice to construct an optimal strategy for every distribution µ?
We call such a set of questions an optimal set of questions, and denote the minimum cardinality of an optimal set of questions for Xn by q(n). We stress that the same set of questions must be used for all µ.
1

Surprisingly, it is possible to improve on the trivial set of all 2n questions exponentially: Dagan et al. [DFGM17, DFGM19] showed that q(n)  1.25n+o(n), and furthermore, q(n)  1.25n-o(n) for infinitely many n (specifically, n of the form 1.25 · 2k). Thus 1.25 is the smallest constant C such that q(n)  Cn+o(n) for all n.
The fact that the lower bound q(n)  1.25n-o(n) holds only for some n suggests that the upper bound 1.25n+o(n) can be improved for other n. This is what our first main result shows:

Theorem 1.1. There exists a function G : [1, 2)  R such that for   [1, 2), q(n) = 2-G()n±o(n) for all n of the form n =  · 2k.

Furthermore, if  = 1.25 then

2-G() < 1.25.

This confirms a conjecture of Dagan et al. The exact formula for G() appears in Theorem 4.1.

Optimal sets of questions and fibers The proof of Theorem 1.1 relies on a result of Dagan et al.

Lemma 1.2. A set of questions Q is optimal if for every dyadic distribution µ on Xn (that is, a distribution in which the probability of each elements is 2-k for some k  N+), there is a set Q  Q of probability exactly 1/2.
Equivalently, Q is optimal if it hits Spl(µ) for all dyadic µ, where

Spl(µ) = {A : µ(A) = 1/2}.

To prove this result, Dagan et al. first show that a set of questions is optimal iff there is an optimal strategy for every dyadic distribution. Roughly speaking, given an arbitrary distribution , we construct a Huffman code C for  and convert it to a distribution µ(xi) = 2-|C(xi)|. An optimal strategy for µ turns out to be an optimal strategy for .
Dagan et al. then show that an optimal strategy for a dyadic distribution must split the probability evenly at every step. Distributions encountered in this way could have elements whose probability is zero, but by choosing an element of minimal positive probability 2-k and "splitting" it into elements of probability 2-k+1, 2-k+2, . . . , 2-k+t, 2-k+t (where t is the number of zero probability elements), we can reduce to the case of distributions of full support.
It is easy to see that Spl(µ) is an antichain: if A B then µ(A) < µ(B). It is less obvious that Spl(µ) is a maximal antichain, as observed by Dagan et al. Indeed, given any set A, if µ(A) > 1/2 and we arrange the elements of A in nonincreasing order of probability, then some prefix has probability exactly 1/2, and so some subset of A belongs to Spl(µ); and if µ(A) < 1/2, then we can apply the same argument on A to find a superset of A in Spl(µ).
These observations connect optimal sets of questions with another combinatorial object: fibers, defined by Lonc and Rival [LR87]. Given any poset, a fiber is a hitting set for the family of all maximal antichains. Any fiber of the lattice 2Xn is thus an optimal set of questions.
Duffus, Sands and Winkler [DSW90] showed that every fiber of 2Xn contains (1.25n) elements. To show this, they considered maximal antichains of the following form, for a parameter a:
S(B) = {A, A : A  B, |A| = a}, where |B| = 2a - 1.

It is easy to check that these are maximal antichains. There are

n 2a-1

such maximal antichains,

and each set in a fiber can handle at most

n-a a-1

of them, giving a lower bound of

n 2a-1 n-a a-1

 2n(h(2)-(1-)h(/(1-)),

a = .
n

2

Here h(p) = p log2(1/p) + (1 - p) log2(1/(1 - p)) is the binary entropy. This expression is maximized at  = 1/5, giving a lower bound of roughly 1.25n.
Dagan et al. used the exact same argument to prove their lower bound of 1.25n-o(n) for
optimal sets of questions. To this end, they needed to realize S(B) as a set of the form Spl(µ).
The idea is to give all elements of B a probability of 1/2a, and the remaining elements (the "tail") probabilities 1/4a, 1/8a, . . . , 1/2n-2a+1a, 1/2n-2a+1a; any set of measure 1/2 must either
contain the tail and a - 1 elements of B, or must consist of a elements of B.
For this construction to work, we need 1/2a to be a negative power of 2, that is, we need a to be a power of 2. Since a = n/5, this works as long as n is of the form 1.25 · 2k, or at least close to such a number. When n is close to  · 2k for   [1, 2) other than 1.25, the sets S(B)
are not realizable in the form Spl(µ).
In order to prove the lower bound part of Theorem 1.1, we identify, for each value of ,
a more general collection of hard-to-hit maximal antichains which are realizable as Spl(µ) for n =  · 2k. Instead of having a single set B of elements of equal probability together with a "tail", we allow several such sets B1, B2, B3, . . ., where the probability of elements in Bt is 1/2ta. This results in an expression for G() which describes a game between Asker and Builder, in
which Builder picks the proportions of the sets Bt, and Asker picks the type of questions which are best suited to handle the sets S(B1, B2, B3, . . .); we leave the details to Section 3.
Dagan et al. showed that the bound 1.25n is tight, by constructing an optimal set of questions of this size for every n. The bound is not tight for fibers of 2Xn: Luczak improved the lower bound (1.25n) to (2n/3) = (1.2599n), as described in Duffus and Sands [DS01]. Lonc and Rival conjectured that the optimal size of a fiber of 2Xn is (2n/2), realized by the collection of
all sets comparable with {x1, . . . , x n/2 }. This is also the best known explicit construction of an optimal set of questions.
Instead of designing an optimal set of questions explicitly, Dagan et al. show that if we pick roughly 1.25n random questions of each size, then with high probability we get an optimal set of
questions. A similar approach works for proving the upper bound in Theorem 1.1, though the
calculations are more intricate.
Much of the difficulty in proving Theorem 1.1 comes from the fact that G() describes an
idealized game between Asker and Builder which only makes sense in the limit k  , which
we need to connect with the corresponding game for a fixed value of k. This difficulty doesn't
come up in Dagan et al. since in their case, the optimal construction only has a single set B1.

Computing G() Unfortunately, the formula for G() involves non-convex optimization over
infinitely many variables, and for this reason we are unable to compute G() beyond the already
known value G(1.25) = - log2 1.25. Nevertheless, our techniques allow us to improve the bound on max G() given by Dagan
et al. Stated in terms of q(n), Dagan et al. showed that q(n)  1.232n-o(n) for every n, and we improve this to q(n)  1.236n-o(n) for every n.

d-ary questions Our second main result concerns d-ary questions. What happens if Alice asks Bob d-ary questions, that is, questions with d possible answers? The optimal strategy in this case corresponds to a d-ary Huffman code, a setting already considered in Huffman's original paper.
We are able to generalize the main result of Dagan et al. to this setting:

Theorem 1.3. Let q(d)(n) be the minimum cardinality of an optimal set of d-ary questions.

For all n,

q(d)(n) 

d-1 1+ d

n+o(n)
.

d d-1

3

Furthermore, this inequality is tight (up to sub-exponential factors) for infinitely many values of n.
This result holds not only for constant d, but also uniformly for all d = o(n/ log2 n). The techniques closely follow the ideas of Dagan et al., as outlined above in the case d = 2.
Theorem 1.3 shows that the magic constant 1.25 appearing in [DFGM19] generalizes to

d-1

log d

1+ d =2- d d-1

d

.

for arbitrary d. We leave a combination of Theorem 1.1 and Theorem 1.3 to future work.

Paper organization After brief preliminaries (Section 2), we prove the first half of Theorem 1.1 in Sections 3­4. We prove the "furthermore" part of Theorem 1.1 and reprove some results of [DFGM19] using our framework in Section 5, in which we also derive the improved lower bound 1.236n-o(n) (Theorem 5.3). Our results on d-ary questions appear in Section 6. Section 7 closes the paper with some open questions.

2 Preliminaries

Given a distribution  over Xn = {x1, . . . , xn}, denote i := (xi). For any set S  Xn we denote the sum iS i with (S).

2.1 Decision trees

We represent a strategy to reveal a secret element x  Xn as a decision tree. A decision tree is a

binary tree T = (V, E) such that every internal node v  V is labeled with a query Q  Xn,

every leaf l  V is labeled with an object xi  Xn, and every edge e  E is labeled with "Yes"

or "No". Moreover, if v is an internal node that is labeled with the query Q, and x is the secret

element, then v has two outgoing edges: one is labeled with "Yes" (representing the decision

"x  Q") and the other with "No" (representing the decision "x / Q").

Given a set of queries Q  2Xn (which is called the set of allowed questions), we say that

T is a decision tree using Q if for any internal node v  V , the query Q that v is labeled with

satisfies Q  Q.

Given a distribution  over Xn, we say that a decision tree T is valid for  if for any object

x  supp() there is a path in T that begins in the root and ends in a leaf that is labeled with

x. The decision trees we will consider are only those in which each object x  Xn labels at most

one leaf.

If there is a path from the root to x  Xn, we say that the number of its edges is the depth

of x, and denote this number with T (x). If T is valid for , the cost of T on  is

n i=1

iT

(xi).

2.2 Optimal sets of questions
For a distribution , let Opt() be the minimum cost of a decision tree for . A set Q  2Xn of queries is optimal if for every distribution , there is a decision tree using
Q whose cost is Opt(). We denote the minimum cardinality of an optimal set of queries over Xn by q(n). A major
goal of this paper is to estimate q(n) for all values of n. We do this using the concept of maximum relative density, borrowed from [DFGM19].

4

Definition 2.1. A distribution µ is dyadic if for all i, µi = 2-d for some d  N or µi = 0. If µ is a non-constant dyadic distribution, then a set A  Xn splits µ if µ(A) = 1/2. We
denote the collection of all sets splitting µ by Spl(µ), and the collection of all sets of size i
splitting µ by Spl(µ)i. The i'th relative density of Spl(µ) is

i(Spl(µ))

=

|

Spl(µ)i|
n

.

i

The maximum relative density of Spl(µ) is

(Spl(µ)) = max i(Spl(µ)).
i{1,...,n-1}

The following result reduces the calculation of q(n), up to polynomial factors, to the

calculation of the quantity

min(n)

=

min
µ

(Spl(µ)),

where the minimum is taken over all non-constant and full-support dyadic distributions.

Theorem 2.2 ([DFGM19, Theorem 3.3.1 and Lemma 3.2.6]). It holds that

1  q(n)  n2 log n 1 .

min(n)

min(n)

Hence, from now on, we will consider the problem of finding a formula for min(n) (up to sub-exponential factors) instead of a formula for q(n).

2.3 Tails
The tail of a dyadic distribution µ over Xn is the largest set T  Xn which satisfies, for some a  N:
· The probabilities of the elements in T are 2-a-1, 2-a-2, . . . , 2-a-(|T |-1), 2-a-(|T |-1). · Any element xi  Xn\T has probability at least 2-a.
If µ is a dyadic distribution, then [DFGM19, Lemma 3.2.5] shows that each set in Spl(µ) either contains T or is disjoint from T .

2.4 Entropy

The entropy of a distribution  is

n

1

H ()

:=

i=1

i

log

. i

For n = 2, define the binary entropy function:

1

1

h(1)

:=

1

log

1

+

(1

-

1) log

1

-

. 1

We prove some simple bounds on the binary entropy function, which will be useful in some of the proofs in this work. If y -  x  y + for some x, y, , denote x = y ± .

5

Lemma 2.3. For any 0  x, 1, 2  1 such that 2  x  1 - 1 it holds that
h(x + 1) = h(x) ± h( 1), h(x - 2) = h(x) ± h( 2).
Proof. Let 0  x, 1  1 such that x + 1  1. Since h is concave and h(0) = 0 it is known that h is sub-additive, that is h(x + 1)  h(x) + h( 1). Using that inequality together with the fact that h is symmetric, we have:
h(x) = h(1 - (x + 1) + 1)  h(1 - (x + 1)) + h( 1) = h(x + 1) + h( 1).
The second inequality is proved similarly.
Throughout this paper, we also use the fact that h(x) is increasing for x < 1/2.

3 An exact (and almost exact) formula for min(n)
Our goal in this section and the next is to find a formula for min(n) up to sub-exponential factors. We use the expression

min(n)

=

min
µ

max
d[n]

d(Spl(µ))

as our starting point. We want to present min(n) in a more "direct" or "numeric" fashion, rather than through a choice of a non-constant dyadic distribution µ. Denote n =  · 2k where
  [1, 2) and k  N. From now on and throughout this paper, when we refer to  and k that is always their meaning unless specified otherwise. Let [0, 1]N be the set of all sequences {ci} i=0 where 0  ci  1 for any i and denote a sequence {ci} i=0  [0, 1]N with c (the notation is the elements' letter in bold). In order to describe a non-constant and full support dyadic distribution
µ in this language, we can determine the following sufficient and necessary values:

· b  N, where the highest probability in µ is determined to be µ1 = 2b-k.

· An "amount sequence" c which describes how many elements µ will have of each probability. In order to obtain a valid dyadic distribution the following must hold:



ci/2i

=

1  · 2b ,

i=0



ci  1,

i=0

i : cin  N.

Those values indeed determine µ uniquely: for any i, cin elements have probability µ1/2i (assume
that c0 > 0). Actually, in order to describe µ precisely, we also have to say exactly which elements are the cin elements having probability µ1/2i. However, since we are only interested
in the identity of a distribution µ which minimizes (Spl(µ)), the identity of the cin elements having probability µ1/2i does not matter -- what matters is only their quantity. If t is the highest index such that ct > 0, then one element with probability µ1/2t is "turned" into a tail with total probability 2-a-t, such that we get n elements in total. The first constraint assures
that the probabilities in µ sum up to 1. The second constraint assures that there are no more

6

than n non-tail elements, that is, exactly n elements in total. The third constraint assures that

there is an integral number of elements of each type.

For the proof of our formula for min(n) which we will present soon, we want to distinguish between pairs (c, b)  [0, 1]N × N which satisfy all of those three constraints, and those which

do not necessarily satisfy the third "integrality" constraint. Hence, denote the set of all pairs

(c, b)  [0, 1]N × N satisfying the first two constraints, that is,

 i=0

2b-ici

=

1

and

 i=0

ci



1

with C = C(). If a pair (c, b)  C satisfies the third constraint as well, we say that (c, b) (or,

simply c, when the identity of b is clear) is k-feasible.1

Now we want to describe the choice of an integer d for the maximization part in min(n).

Due to [DFGM19], we know that each splitting set either contains all tail elements, or none of

them. For a sequence c  [0, 1]N, denote by t the last index such that ct > 0. If there is no such

index, t = . Fix (c, b)  C which is k-feasible, that describes a dyadic distribution µ (in that

case, t < ). In order to describe the set Spl(µ)d for some d  [n] (recall that those are all splitting sets of µ of size d) we can consider the following sets of sequences in [0, 1]N:

· A set Sd of all sequences   [0, 1]N describing sets in Spl(µ)d which do not contain the tail elements. Those sequences satisfy:

t

ici/2i

=



·

1 2b+1 ,

i=0

t

icin = d,

i=0

i : icin  N,

t < 1.

· A set Td of all sequences   [0, 1]N describing sets in Spl(µ)d which contain the tail elements. Those sequences satisfy:

t

ici/2i

=



·

1 2b+1 ,

i=0

t

t

icin + 1 - ci n - 1 = d,

i=0

i=0

i : icin  N,

t > 0.

The constraints of Sd, Td indeed describe splitting sets of size d: The first constraint assures that the set described by a sequence  is a splitting set. The second constraint assures that its size is d. The third constraint assures that each probability type appears in the set an integral number of times. The last constraint on Sd or Td assures that the tail elements may not be or may be a part of the splitting set, respectively. Note that a sequence   [0, 1]N satisfying those constraints does not determine which elements are exactly the elements chosen to the splitting set. We soon handle that, since here, in contrast to the choice of µ, the identity of the elements chosen having given probability matters, since any choice of different elements defines
1Even though this constraint relates to n, we choose to relate k instead of n to conform with future notations which relate to k as well. Our discussion will fix   [1, 2), and thus n will be determined uniquely by k, so this is not a problem.

7

a different splitting set. This discussion implies that we can write min(n) in the following way, which will be convenient for our purposes:

min(n) = min max
(c,b)C : d[n] : c is k-feasible SdTd= Sd

ctn-1 tctn

t-1 i=0
n d

cin icin

+
Td

ctn-1 tctn-1

t-1 cin

i=0 n

icin

.

d

For 0  i < t, each binomial coefficient

cin icin

is the number of possibilities to choose icin

elements of probability 2-k+b-i to the splitting set. For the index t, we use the expressions

ctn-1 tctn

and

ctn-1 tctn-1

because we must use or not use the tail elements, depends on whether  is

in Sd or Td.

Since our goal is to find a formula for min(n) up to sub-exponential factors, we can simplify

the expression a bit, and ignore the sequences in Td. Define

min(n) = min max
(c,b)C: d[n]: c is k-feasible Sd= Sd

t

cin

i=0 icin n

.

d

The idea is that any splitting set S described by a sequence   Td, has a matching splitting set S (the complement set of S) described by a sequence   Sn-d such that

ctn-1 tctn

t-1 cin

ctn-1

i=0 icin n

=

tctn-1

t-1 cin

i=0 n

icin

.

d

n-d

Thus by considering only sequences in Sd, we get an approximation for min(n). Here is a detailed proof for that, for the interested reader:

Lemma 3.1. It holds that

min(n)/2  min(n)  n · min(n).

Proof. Let n =  · 2k. Fix (c, b)  C which is k-feasible and d  [n]. To handle the case Sd = 

or Td = , define


 fS(d) =

( ) ( ) ctn-1 t ct n

t-1 cin i=0 icin

Sd

(nd)

Sd = ,

0

Sd = ,

and


 fT (d) =

( ) ( ) ctn-1 t ct n-1

t-1 cin i=0 icin

Td

(nd)

0

Td = , Td = .

In this language, we can write:

Define: If fS(d)  fT (d), then

min(n) = min max fS(d) + fT (d).
(c,b)C : d[n] c is k-feasible
min(n) = min max fS(d).
(c,b)C : d[n] c is k-feasible
fS(d) + fT (d)  2 · fS(d).

8

Else, assume fS(d) < fT (d). Since for any   Td we have   Sn-d such that

ctn-1 tctn

t-1 cin

ctn-1

i=0 icin n

=

tctn-1

t-1 cin i=0 icin
n

d

n-d

(i = 1 - i for any i), and the opposite holds as well in a similar fashion, we have

fS(n - d) = fT (d) > fS(d) = fT (n - d)

and thus

fS(d) + fT (d) = fS(n - d) + fT (n - d)  2 · fS(n - d). Hence, we can always choose d  [n] such that

min(n) = min fS d + fT d
(c,b)C : c is k-feasible

and fS(d )  fT (d ). Hence:

min(n)  2 · min fS d
(c,b)C :

c is k-feasible

 2 · min
(c,b)C :

max
d[n]

fS (d)

=

2

·

min(n).

c is k-feasible

Now, note that:

x

x-1

x

/x 



y

y

y

(the left inequality holds as long as x > y), and hence min(n)/2  min(n)  min(n) and moreover min(n)  n · min(n)  n · min(n), since t < 1. Hence the lemma follows.

Due to that approximation, it is enough to find a formula that estimates min(n) instead of min(n), up to sub-exponential factors.

4 Approximating min(n)

In this section we prove our first main result, which is the following theorem:

Theorem 4.1. There is a function G : [1, 2)  R such that min(n) = 2G()n±o(n), where n =  · 2k, k  N and   [1, 2). The function G is given by the following formula:

G() =

inf

bN,c[0,1]N :

 i=0

ci

/2i

=

1 ·2b

 i=0

ci1





max

h(i)ci - h

ici .

[0,1]N :

 i=0

ici/2i=

1 ·2b+1

i=0

i=0

Corollary 4.2. Putting together Theorems 2.2 and 4.1, it holds that q(n) = 2-G()n±o(n), where n =  · 2k, k  N and   [1, 2).

9

An immediate corollary is that the exponent base of min(n) (and q(n)) does not depend

on n, but only on . We assume that c0 > 0, since if c0 = 0 then we can choose another b and

construct an equivalent sequence with c0 > 0. For the rest of the section, fix   [1, 2) and denote

P (c, ) =

 i=0

h(i)ci

-

h(

 i=0

ici).

For

a

fixed

(c, b)



C

(that

is,

(c, b)



[0, 1]N × N

which

satisfies also

 i=0

ci/2i

=

1 ·2b

and

 i=0

ci



1),

we

denote

by

A(c,

b)

(or

simply

A,

from

now

on,

assuming (c, b) is fixed) the set of all sequences   [0, 1]N which satisfy

 i=0

i

ci/2i

=

1 ·2b+1

(the maximization constraint in G()). In this language, we can write

G() = inf max P (c, ).
(c,b)C A

4.1 G is well-defined

Before we prove our formula, we first show that G is indeed well-defined and finite: if we change

the inner max to sup, then it is clear that G() is well-defined and finite: it always holds that

 i=0

ci



1,

and

thus

-1



P (c, )



1

for

any

(c, b)



C,





A.

Moreover,

for

any

(c, b)



C,

A is non-empty: choosing the sequence  to be i = 1/2 for any i satisfies

 i=0

ici/2i

=

1 ·2b+1

for any (c, b)  C, thus it always belongs to A. It is known that supremum/infimum values are

defined and finite for non-empty bounded sets, thus it remains to show that the inner supremum

is attained, and hence can be written as maximum. Fix (c, b)  C. First we show:

Lemma 4.3. Let j jN be a sequence of sequences j  A such that limj P c, j = supA P (c, ). Then there is a sequence , and a subsequence of j jN which we denote by  j jN, such that  j   pointwise, that is, for any i: limj ij = i.

Proof. Consider the sequence

0j

.
jN

Since 0j  [0, 1] for any j,

0j jN must have a

convergent subsequence due to Bolzano-Weiersrtrass. Denote that subsequence by

0j

,
jN

and let 0 = limj 0j. Deonte by (0) = (0),j jN the subsequence of j jN that is

constructed from the same indices as

0j

. Now, consider the sequence
jN

1(0),j

. This
jN

sequence as well has a subsequence which converges, say to 1. Let (1) be the subsequence

of

j jN that is constructed from the same indices of the subsequence of

1(0),j

which
jN

converges to 1. Note that in addition to 1(1),j  1, we also have 0(1),j  0 since the limit

of a convergent sequence equals the limit of any of its subsequences. We can proceed in the

same fashion, constructing a sequence , and for any r a subsequence (r) of j jN such that s(r),j  s for any s  r. We take as  j jN the diagonal sequence (j),j jN which converges pointwise to  .

Let  be the sequence guaranteed by this lemma. We will show that the supremum is attained at , that is, P (c, ) = sup A P (c,  ). It remains to show:

Lemma 4.4. The sequence  found by Lemma 4.3 is in A (that is,  is feasible for (c, b)) and limj P c, j = P (c, ).

Proof. Let us begin by showing   A: pointwise convergence of j jN to  ensures that





[0, 1]N,

since

j



[0, 1]N

for

any

j.

It

remains

to

show

that

1 ·2b+1

=

 i=0

ici/2i

:

Take

an

arbitrary > 0 and find I such that i>I ci < /3. Then, find J such that iJ - i < /3 for

10

all i  I. Thus:

1  · 2b+1

=



iJ ci/2i

i=0

I
= iJ ci/2i ± /3

i=0

I
= (i ± /3)ci/2i ± /3

i=0

I



= ici/2i ± 2 /3 = ici/2i ± ,

i=0

i=0

and so indeed   A. Let us show that limj P c, j = P (c, ). For some I  N, denote

P I (c,) =

I i=0

h(i)ci

-h

I i=0

ici

. Take an arbitrary

> 0 and let I such that

i>I ci < .

So:

I



I



P (c, ) = h(i)ci +

h(i)ci - h

ici +

ici = P I (c,) ± ( + h( )) (1)

i=0

i=I +1

i=0

i=I +1

due to Lemma 2.3. Now we can use an argument similar to the one used to show feasibility of : find J such that iJ - i < for all i  I. So:

I

I

P I (c,) = h iJ ± ci - h

iJ ± ci

i=0

i=0

I

I

I

I

= h iJ ci ± h( ) ci - h

iJ ci ±

ci = P I c,J ± 2h( ). (2)

i=0

i=0

i=0

i=0

And so:

P c, J = P I c, J ± ( + h( )) = P I (c,) ± ( + 3h( )) = P (c, ) ± (2 + 4h( )),

()

(2)

(1)

where () is since i>I ci < , similarly to eq. (1). So indeed, limj P c, j = P (c, ). The following desired result is an immediate corollary:

Lemma 4.5. For any (c, b)  C there is   A such that sup A P (c,  ) = P (c, ).

4.2 Proving our formula for min(n)
The following bounds on min(n) immediately imply theorem 4.1, due to Lemma 3.1: Lemma 4.6. It holds that min(n)  2G()n-o(n). Lemma 4.7. It holds that min(n)  2G()n+o(n).
In the following subsections we prove those bounds.

11

4.2.1 Lower bounding min(n)
Recall that if a pair (c, b)  C satisfies cin  N for all i, we say that c is k-feasible. Similarly, if a sequence   A satisfies icin  N for all i, and t < 1 , we say that  is k-feasible. Note that for a fixed (c, b)  C which is k-feasible, by our definitions:

{  A :  is k-feasible} = Sd.
d[n]

We will use that connection throughout the proof, when linking between min(n), which uses the set Sd for some optimal d, and G() which uses the set A. For a set of sequences S  [0, 1]N, let
Sl = {s  S : i > l = si = 0}.

Lemma 4.6 can be inferred from the following two lemmas: Lemma 4.8. If (c, b)  C and c is k-feasible, then there is   Ak which is k-feasible.

The purpose of this lemma is to allow us to use the estimate

2h()n/O

 n



n

 2h()n

(3)

n

(the lower bound is due to [You12], for example) while proving Lemma 4.6, in a sufficiently efficient fashion.

Lemma 4.9. Fix (c, b)  C. Then:

lim max P (c, ) = max P (c, ).

k Ak :

A

 is k-feasible

For large values of k, Lemma 4.9 allows us to remove the k-feasibility and i > k = i = 0 constraints on  without changing much the value of P (c, ). Having Lemmas 4.8­4.9 in hand and using the estimate (3), Lemma 4.6 can be proved:
Proof of Lemma 4.6. Let n =  · 2k and (c, b)  C which is k-feasible. So:

max
d[n] : Sd= Sd

 cin i=0 icin
n
d



max

Lem 4.8 Ak :

 is k-feasible

 cin

i=0 icin

n

 i=0

icin

exp2(

 i=0

h(i)cin)/O

nk

 max
(3) Ak :

exp2(h(

 i=0

ici)n)

 is k-feasible

= exp2

max
Ak :

P (c, )n - o(n)

 is k-feasible

 exp2
Lem 4.9

max P (c, )n - o(n)
A

.

Let (c, b)  C which is k-feasible and

min max
(c ,b)C : d[n] : c is k-feasible Sd= Sd

 cin i=0 icin
n
d

= max
d[n] : Sd= Sd

 cin

i=0 icin n

,

d

12

and deduce:

min(n) = min max
(c ,b)C : d[n] : c is k-feasible Sd= Sd

= max
d[n] : Sd= Sd

 cin i=0 icin
n
d

 cin i=0 icin
n
d

 exp2

max P (c, )n - o(n)
A

 inf exp2
(c ,b)C

max P c ,  n - o(n)
A

= 2G()n-o(n).

Now we shall prove Lemmas 4.8­4.9:

Proof of Lemma 4.8. Let (c, b)  C and assume c is k-feasible, then we have:







ci · 2b-i ·  <

ci · 2k-1-(k+1) · 2 = 2-1 ·

ci  1/2

i=k+1

() i=k+1

i=k+1

where () is since b  k - 1 (otherwise (c, b) represents a constant dyadic distribution), i  k + 1,

and  < 2. Since

 i=0

ci

·

2b-i

·



=

1,

we

deduce

that

k i=0

ci

· 2b-i

·

>

1/2.

Thus, by

Lemma 4.1 in [DFGM19] (called there "A useful lemma"), we know that there is a splitting

set of the dyadic distribution represented by (c, b) containing only elements with probabilities

µ1, µ1/2, . . . , µ1/2k. The same lemma also implies that t < 1. That is, there is   Ak which

is k-feasible.

The proof of Lemma 4.9 will require the following:
Lemma 4.10. Fix (c, b)  C, > 0 and   A. There are K  N and ~  AK, where ~ is K-feasible and satisfies P (c, ) = P (c, ~) ± .

Having Lemma 4.10, Lemma 4.9 is almost immediate:

Proof of Lemma 4.9. Fix (c, b)  C and let > 0. Let   A such that P (c, ) = max A P (c,  ). Let K  N large enough such that Lemma 4.10 holds for (c, b),  and . Then for any k  K:

max P c,  = P (c, )  P (c, ~) +  max P c,  + .

 A

Lem 4.10

 A

 is k-feasible

Obviously,

max P c,   max P c, 

 Ak

 A

 is k-feasible

for any large enough k, thus the lemma follows. It is necessary that k is large enough: Note that for the K determined by Lemma 4.10, for example, we can be sure that there is   AK
which is K-feasible, while for small k values there might not be such , and then the left-hand

side is not well defined.

It remains to prove Lemma 4.10, which is the main part of the proof for the lower bound. We first explain the proof idea, and then give the detailed proof.

13

Proof sketch of Lemma 4.10. Let (c, b)  C and   A. Since the binary entropy function is sub-additive and symmetric, it holds that h(x ± ) = h(x) ± h( ) for 0  x ±  1 (as we have shown in Lemma 2.3). Based on that, the proof idea is that if we make very small changes in , to get some other sequence which we denote ~, we can get P (c, ) = P (c, ~) ± . The main difficulty is to make small changes to  while ensuring that for some K  N, ~  AK and ~ is K-feasible. We can solve that difficulty by defining a sequence of very small values carefully selected, and then defining ~ in the following way:

~i =

s + s i - i

i = s, i = s,

where s is some index chosen to make sure that ~  [0, 1]N. The value ~s adds s in order to

"balance", in a way, the subtraction of i in other indices, such that we have

 i=0

ici

/2i

=

 i=0

~ici

/2i

and

hence

the

constraint

 i=0

~ici/2i

=

1 ·2b+1

is

satisfied

(since

the

constraint

 i=0

ici

/2i

=

1 ·2b+1

is

satisfied).

The

purpose

of

the

addition

or

subtraction

of

the

sequence

values is to "round" the values of  to get ~ which is K-feasible and belongs to AK. The

exact choice of the sequence that guarantees that is described in the detailed proof.

Before we give a detailed proof of Lemma 4.10, we prove a lemma which will be useful in the detailed proof, and also later when upper bounding min(n). Its purpose is to ensure that when we change a sequence   A to a sequence ~, we use ~  [0, 1]N.

Lemma 4.11. Fix b  N. For any c  [0, 1]N such that

 i=0

ci/2i

=

1 ·2b

for

some

1   < 2,

and for any   [0, 1]N which satisfies

 i=0

ici/2i

=

1 2b+1 

,

there

are

indices

s1, s2



2b+4

(possibly s1 = s2) such that cs1 , cs2 > 1/22(b+5) and s1 > 1/22(b+5), s2 < 3/4.

Proof. Fix b  N and take arbitrary c,  such that

i ci/2i =

1 ·2b

.

Let us find s1:

take an

arbitrary sequence  such that

 i=0

ici/2i

=

1 2b+1



.

Denote

I

=

i : i  2b+4

and let S  I

be the set of indices in I which satisfy ci  1/22(b+5). Assume towards contradiction that for

any i  S, i 

. 1
22(b+5)

Note that since

 i=0

ici

/2i

=

1 2b+1 

>

1/2b+2

and

i>2b+4 ci/2i 

i>2b+4

1/2i

=

1 2b+4

,

it

must

hold

that

2b+4 i=0

ici/2i



3 2b+4

.

However,

we

have:

2b+4
ici/2i =

ici/2i +

ici/2i

i=0

iS

iI \S

1

 22(b+5) ·

ci +

ci

iS

iI \S

2b+5

2b+5

< 22(b+5) + 22(b+5)

1

1

= 2 · 2b+5 = 2b+4 ,

and that is a contradiction, thus there is i  S with i  1/22(b+5). That is, there is an index i  2b+4 with ci, i  1/22(b+5), and this is s1. Let us now find s2: assume towards contradiction that for any i  S, i  3/4. We show that if that assumption is true, then iS ici/2i is too large. First, we have:

1 2b =



2b+4
ci/2i = ci/2i +

2b+4
ci/2i  ci/2i + 1/2b+4,

i=0

i=0

i>2b+4

i=0

14

that is: Moreover, it holds that: Hence, we get that:

2b+4
ci/2i



1 2b

-

1/2b+4.

i=0

ci/2i



2b+5 22(b+5)

=

1/2b+5.

iI \S

ci/2i



1 2b

- 1/2b+4

- 1/2b+5

=

1 2b

- 3/2b+5.

iS

And thus, assuming i  3/4 for any i  S:

ici/2i



3 4

ci/2i



3 4

·

1 2b

- 3/2b+5

.

iS

iS

But then we get:

3 4

1 2b

-

3/2b+5

3 =
4

25 - 3 2b+5

3 25 - 23

>· 4

2b+5

3 23 22 - 1

=· 4

2b+5

3·3 = 22 · 2b+2

23

1

> 2b+4 = 2b+1 ,

and this contradicts the fact

 i=0

ici/2i

=

1 2b+1



.

Thus

there

is

i  2b+4

with

ci

 1/22(b+5)

and i < 3/4, and this is s2.

Now we can go on with the detailed proof of Lemma 4.10.

Proof of Lemma 4.10. We divide the proof into three parts: First we define the sequence ~ and show it exists. Then we show ~  AK and ~ is K -feasible for some K  N. Finally, we show P (c, ) = P (c, ~) ± .

Defining ~. Fix (c, b)  [0, 1]N satisfying

 i=0

ci/2i

=

1 ·2b

and

 i=0

ci



1

(that

is,

(c,

b)



C). Fix   [0, 1]N satisfying

 i=0

ici/2i

=

1 ·2b+1

(that

is,

  A).

Let

> 0 small enough

(the proof holds for any > 0 smaller than some constant). Let s be the lowest index satisfying

cs > 1/22(b+5) and s < 3/4 (s exists due to Lemma 4.11). Since

 i=0

ci



1,

there

is

K



N

such that i>K ci < and K  s. Define the sequence ~ as follows:

~i =

s + s i - i

i = s, i = s,

where is an arbitrary sequence of small values satisfying the following constraints:

15

1. If i > K, or ci = 0, or i = 0: i = i.

2. Otherwise, if 0  i  K and i = s:

i

=

i -

li ci··2ti

,

where

li, ti



N,

i > 0 and

h( i)  /K.

3. s =

i=s

ici

·

2s-i/cs

=

ls cs··2ts

- s,

where

ls, ts



N.

In order to continue with the proof, we first have to show that such a sequence exists. Denote

by I the set of all indices "that matter" in ~, that is, I = {i  K : ci, i > 0}. It is not hard to construct a sequence that satisfies constraints (1), (2). We should satisfy constraint (3) as well,

that is

cs

ls ·  · 2ts

- s

=

i=s

ici · 2s-i/cs



2s

=

cs


iI \{s}

i

-

ci

·

li ·

2ti


· ci/2i + ici/2i
i>K





=

2s 
cs i=s

ici/2i

-

iI \{s}



·

li 2ti

+i

.

This can be written as:





2s ici/2i -



li · 2ti+i 

=



ls · 2ts

- scs

i=s

iI \{s}



ici/2i -



li · 2ti+i

=



ls · 2ts+s

- scs/2s

i=s

iI \{s}




ici/2i -



li · 2ti+i

=



·

ls 2ts

+s

.

i=0

iI \{s}

Recall that

 i=0

ici/2i

=

1 ·2b+1

,

thus

we

have:

1  · 2b+1 -



li · 2ti+i

=



ls · 2ts+s

iI \{s}



1/2b+1 -

li/2ti+i = ls/2ts+s,

iI \{s}

and there are ls, ts  N satisfying this equation: Let ts = b + 1 + iI\{s} ti + i, then ls is

determined accordingly such that the equation holds. Clearly, ts  N. As for ls, note that by

the constraints:





0  s +

ici · 2s-i/cs · cs ·  · 2ts = ls.

i=s

Since clearly ls  Z as a sum of numbers in Z, we get ls  N, and thus there is such a sequence . We now show a few bounds on values involving the sequence , which will help us during the rest

16

of the proof. By the definition of the sequence

and

since

for

x



1/2

we

have

h(x)



x

log

1 x



x,

it holds that:

i

h( i) 

/K = .

(4)

iK,i=s

iK,i=s

iK,i=s

Moreover:

ici/2i =

ici/2i +

ici/2i 

i + ci = 2 ,

i=s

iK,i=s

i>K

iK,i=s

i>K

and thus:

s=

ici · 2s-i/cs  2s2 /cs.

(5)

i=s

~ is feasible. Now we show that ~  AK and K -feasible for some K  N. Based on the fact   A, we first show ~  AK, that is:

1. ~  [0, 1]N.

2.

 i=0

~i

ci/2i

=

1 ·2b+1

.

3. i > K = ~i = 0 (this is obvious by the definition of ~).

We show (1): For i = s, it is not hard to check that 0  ~i  1 by the definition of the sequence

. For i = s, recall that 0  s < 3/4 and thus 0  ~s = s + s  1 for small enough (due to

(5)). Let us show (2), depending on the fact

 i=0

ici/2i

=



1 ·2b+1

:


~ici/2i = (s + s)cs/2s + (i - i)ci/2i

i=0

i=s


= ici/2i + scs/2s -

ici/2i

i=0

i=s

1 =  · 2b+1 +

i=s

ici cs

·

2s-i cs/2s

-

i=s

ici/2i

=



1 · 2b+1 .

Thus, indeed ~  AK. Now we show that ~ is K -feasible where

K = max({K}  {ti : i  I}).

That is: 1. For any i  N: ~icin  N where n =  · 2K .

2. If there is t such that ct > 0 and ci = 0 for any i > t, then t < 1.

We show (1): If i > K, or ci = 0, or i = 0 then by definition of ~, ~icin = 0  N. Otherwise:

~icin

=

ci

·

li 

·

2ti

ci

·

· 2K

= li · 2K -ti  N

since K  ti. Let us show (2): If t > K or t = 0 then ~t = 0 < 1. Otherwise, if t  K and t = s then t > 0 and thus
~t = t - t  1 - t < 1.
If t = s, then since s < 3/4, we have ~t = s + s < 1 for small enough . So ~  AK  AK and is K -feasible, as required.

17

P (c, ~) approximates P (c, ). It only remains to show P (c, ) = P (c, ~) ± . Due to Lemma 2.3, We have:





P (c, ) = h(i)ci - h

ici

i=0

i=0





= h(~s - s)cs + h(~i + i)ci - h(~s - s)cs + (~i + i)ci

i=s

i=s









= h(~i)ci - h

~ici ± h

ici ± h( i)ci

i=0

i=0

i=0

i=0





= P (c, ~) ± h

ici ± h( i)ci.

i=0

i=0

We show that the expressions h(

 i=0

ici) and

 i=0

h(

i)ci

are

small.

It

holds

that:



h

ici

i=0

K



h

ici + h

ici

Lem 2.3

i=0

i=K +1

K

h

i

i=0



+h

ci

i=K +1

 h( + 2s2 /cs) + h( ),
(4),(5)

and:



K



h( i)ci = h( i)ci +

h( i)ci

i=0

i=0

i=K +1



 h( s) +

h( i) +

ci  h(2s2 /cs) + 2 .

iK,i=s

i=K+1 (4),(5)

Thus, we can choose > 0 small enough and apply the proof for , such that P (c, ) = P (c, ~) ± .

4.2.2 Upper bounding min(n) The idea here is similar to the idea of the lower bound proof. Here Cl is the set of all pairs (c, b)  C such that if i > l then ci = 0. In order to prove Lemma 4.7, we will prove two claims. The first allows us to remove or add constraints on the choice of a pair (c, b)  C without changing much the value of P (c, ):
Lemma 4.12. It holds that:
lim min max P (c, ) = G().
k (c,b)Ck : A c is k-feasible
The second claim shows, essentially, that the summation appearing in min(n) is redundant for approximation up to sub-exponential factors, if the pair (c, b) chosen by the minimization belongs to Ck:

18

Lemma 4.13. Let n =  · 2k and (c, b)  Ck such that c is k-feasible. Then:

max
d[n] Sdk

 cin

i=0 icin n
d

 exp2

max P (c, )n + o(n)
A

.

Having Lemmas 4.12 and 4.13, we can prove Lemma 4.7:

Proof of Lemma 4.7. We have:

min(n) =

min
(c,b)C :

max
d[n] :

c is k-feasible Sd= Sd

 cin i=0 icin
n
d

 min max

(c,b)Ck : d[n] : c is k-feasible Sd=

Sdk

 cin i=0 icin
n
d






Lem 4.13

exp2

min
(c,b)Ck :

max P (c, )n + o(n)
A

c is k-feasible

 exp2
Lem 4.12

inf max P (c, )n + o(n)
(c,b)C A

= 2G()n+o(n).

Now we shall prove Lemmas 4.12­4.13. We prove Lemma 4.13 first since it is simpler.

Proof of Lemma 4.13. Let n =  · 2k and (c, b)  Ck such that c is k-feasible.

d[n] Sdk such that

max
  d[n] Sdk

 cin

i=0 icin n

=

 i=0

i

ci

n

 cin

i=0 icin n

.

 i=0

i

ci

n

Combinatorial considerations imply that

Let   (6)

Sdk  (n + 1)k+1 = O nlog n+1

(7)

d[n]

since for a sequence   d[n] Sdk, if 0  i  k then in can potentially be any number between 0 and n, and else in = 0. Hence:

max
d[n]  Sdk

 cin i=0 icin
n
d


  d[n] Sdk

 cin

i=0 icin

n

 i=0

i

ci

n

 O nlog n+1 ·
(6),(7)

 cin

i=0 icin

n

 i=0

icin

 O nlog n+1
(3)

· exp2(h(exp i2=(0h(icii))cnin)/)O(n)

= exp2(P (c, )n + o(n))  exp2 max P c,  n + o(n)

()

 A

where () is since d[n] Sdk  A by definition, and hence   A. The proof of Lemma 4.12 is implied by the following:

19

Lemma 4.14. Fix (c, b)  C and let > 0. There is K  N and (c~, b)  CK, where c~ is

K-feasible, such that for any ~  [0, 1]N satisfying

 i=0

~ic~i

/2i

=

1 ·2b+1

there

is

  [0, 1]N

satisfying

 i=0

ici/2i

=

1 ·2b+1

,

and

P (c~, ~)

=

P (c, )

±

.

Having Lemma 4.14 in hand, we can prove Lemma 4.12:

Proof of Lemma 4.12. Let > 0 and = /3. Let (c, b)  C which satisfies:

max P (c, )  G() + .

(8)

A

Let K  N large enough such that Lemma 4.14 holds for (c, b) and . Let ~  [0, 1]N which

satisfies

 i=0

~ic~/2i

=

1 ·2b+1

and:

max P (c~, )  P (c~, ~) + .

(9)

A

Hence for any k  K:

min max P (c, )  max P (c~, )

(c,b)Ck A

() A

c is k-feasible

 P (c~, ~) +
(9)

 P (c, ) + 2
Lem 4.14

 max P (c, ) + 2
() A

 G() + 3
(8)

where () is since (c~, b)  Ck and c~ is k-feasible, and () is since   [0, 1]N and satisfies

 i=0

ici

/2i

=

1 ·2b+1

.

Since

3

= , and since obviously

the lemma follows.

inf max P (c, )  min max P (c, ),

(c,b)C A

(c,b)Ck A

c is k-feasible

It remains to prove Lemma 4.14. The proof idea is similar to the idea appearing in the proof of Lemma 4.10 presented in the previous subsection. Hence, we only present a detailed proof for this Lemma (without a proof sketch):

Proof of Lemma 4.14. Fix (c, b)  [0, 1]N satisfying

 i=0

ci/2i

=

1 ·2b

and

 i=0

ci



1

(that

is,

(c, b)  C). Consider two different cases. First, assume that c is the following sequence:

1 i = 0, ci = 0 i = 0.

It is possible since the equation

 i=0

ci

/2i

=1=

1 ·2b

holds

whenever



= 1, b = 0.

In

that

case,

c  C0 and c is 0-feasible, thus the lemma follows with K = 0, c~ = c. So, assume now that

c0 < 1. We divide the proof under that assumption into four parts: First we define c~. Then we

show that c~  CK and c~ is K -feasible for some K  N. After that, given ~ we define  and

show

 i=0

ici/2i

=

1 ·2b+1

.

Finally,

we

show

P (c~, ~)

=

P (c, ) ±

.

20

Defining c~. Recall that be the following sequence:

 i=0

ci



1

and

thus

there

is

K



N

such

that

c~i =

c0 + 0 ci - i

i = 0, i = 0,

where is an arbitrary sequence satisfying the following constraints:

i>K ci < . Let c~

1. If i > K: i = ci.

2. Otherwise, if 1  i  K:

i

=

ci

-

li ·2ti

,

where

li, ti



N

and

0



i

/K .

3. 0 =

 i=1

i/2i

=

l0 ·2t0

- c0,

where

l0, t0



N.

In order to continue with the proof, we first have to show that such a sequence exists. It is not hard to construct a sequence that satisfies constraints (1), (2). We should satisfy constraint (3) as well, that is:



l0 · 2t0

- c0

=



i/2i

i=1

K
=

ci

-



li · 2ti

i=1

/2i +

ci/2i =


ci/2i -

K



·

li 2ti+i

.

i>K

i=1

i=1

That can be written as:


ci/2i -

K



li · 2ti+i

=



l0 · 2t0

.

i=0

i=1

Recall that since (c, b)  C, we have

 i=0

ci/2i

=

1 ·2b

,

and

thus

we

get:

1  · 2b -

K



li · 2ti+i

=



l0 · 2t0

i=1



1 2b -

K

li 2ti+i

=

l0 2t0

.

i=1

We can find l0, t0  N satisfying this equation: t0 = b +

K i=1

ti

+

i,

and

l0

is

determined

accordingly. Obviously, t0  N. As for l0, it is obviously in Z as a sum of numbers in Z.

Moreover, by the constraints:

l0 =



c0 +

i/2i  · 2t0  0,

i=1

and thus l0  N. We have satisfied all constraints, thus we can find such a sequence . Let us now show that the values of are small, even if we sum all of them together. That fact will help us show that the change of c to c~ has only little affect.





K



K



K

i=

i/2i +

i+

i 2 i+2

i  2 /K + 2 = 4 .

i=0

i=1

i=1

i=K +1

i=1

i=K +1

i=1

(10)

21

c~ is feasible. Now we will show that (c~, b)  CK and c~ is K -feasible for some K  K. Based on the fact that (c, b)  C, we first show (c~, b)  CK, that is:

1. c~  [0, 1]N.

2.

 i=0

c~i



1.

3.

 i=0

c~i/2i

=



1 ·2b

.

4. i > K = c~i = 0 (this is obvious by the definition of c~).
We show (1): For any i = 0 it is obvious that 0  c~i  1 from the definition of c~. For i = 0, 0  c0 < 1 and hence 0  c~0 = c0 + 0  1 for small enough . Thus c~  [0, 1]N. Now we show (2):

And finally (3):





K

c~i = c0 +

i/2i + (ci - i)

i=0

i=1

i=1



K

K

=

i/2i + ci -

i

i=1

i=0

i=1

K



 ci +

i

i=0

i=K +1



= ci  1.

i=0









c~i/2i = c0 + 0 + (ci - i)/2i = ci/2i + 0 -

i/2i =



ci/2i

=



1 · 2b .

i=0

i=1

i=0

i=1

i=0

So indeed (c~, b)  CK. We show that (c~, b) is K -feasible where

K = max{K, t0, . . . , tK }.

Let i  N. If i > K then c~in = 0  N. Else:

c~in

=



li · 2ti



· 2K

= li · 2K -ti  N,

since K  ti.

Defining . Given ~  [0, 1]N such that

 i=0

~ic~i/2i

=



1 ·2b+1

,

we

construct





[0, 1]N

that

"imitates" ~ and satisfies

 i=0

i

ci/2i

=

1 ·2b+1

.

For

such

a

sequence

~,

consider

the

following

expression:



r(~) = ~i i/2i - ~0 0.

i=1

If r(~)  0, let s be the first index such that c~s, ~s > 1/22(b+5). Else, let s be the first index such that c~s > 1/22(b+5) and ~s < 3/4. In any case, s exists and is bounded by 2b+4, by Lemma
4.11. Define the sequence  as follows:

i =

~s - s ~i

i = s, i = s,

22

where

s

=

2s cs

r(~).

Note

that

|s|

is

small

since

|r(~)|

is

small:





|r(~)|  ~i i/2i + ~0 0 

i  4,

i=1

i=0 (10)

and s is bounded by a constant. We show that   [0, 1]N: If i = s then 0  ~i = i  1. For i = s, if r(~)  0 then:
s = ~s - s > 1/22(b+5) - s > 0
for small enough and obviously s = ~s - s  ~s  1. Otherwise, assume r(~) < 0, then:

s = ~s - s < 3/4 - s  1

for small enough and obviously s = ~s - s  ~s  0. Thus   [0, 1]N. We show that

 i=1

ici/2i

=

1 ·2b+1

,

that

is,

 i=1

ici/2i

=

 i=1

~ic~i/2i:





~ic~i/2i = ~0(c0 + 0) + ~i(ci - i)/2i

i=1

i=1





= ~ici/2i + ~0 0 - ~i i/2i

i=0

i=1



= ici/2i + scs/2s - r(~)

i=0

=

 i=0

ici/2i

+

2s r(~)
cs

·

cs 2s

-

r(~)

=

 i=0

ici/2i.

P (c, ) approximates P (c~, ~). It remains to show P (c~, ~) = P (c, ) ± . Due to Lemma 2.3, indeed:





P (c~, ~) = h(~i)c~i - h ~ic~i

i=0

i=0



= h(s + s)c~s + h(i)c~i - h

ic~i + sc~s

i=s

i=0





= h(i)c~i - h

ic~i ± h(|s|)c~s ± h(|s|c~s)

i=0

i=0

= P (c~, ) ± h(|s|)c~s ± h(|s|c~s).

Recall that |s| is small. Now:





P (c~, ) = h(i)c~i - h

ic~i

i=0

i=0









= h(i)ci + h(0) 0 - h(i) i - h

ici + 0 0 - i i

i=0

i=1

i=0

i=1









= h(i)ci - h

ici + h(0) 0 - h(i) i ± h(0 0) ± h

i i

i=0

i=0

i=1

i=1





= P (c, ) + h(0) 0 - h(i) i ± h(0 0) ± h

i i .

i=1

i=1

23

Recall that

 i=0

i4

due to (10). Thus, we can choose

small enough and apply the proof

for , such that

P (c~, ~) = P (c~, ) ± = P (c, ) ± 2 = P (c, ) ± .

5 Applications of Theorem 4.1

5.1 Alternative proofs for known bounds on q(n)

In the previous sections we have shown the estimate q(n) = 2-G()n±o(n). Unfortunately, we do not know how to calculate G() in general. However, we can use this estimate to give alternative proofs for known bounds on q(n), and in the next subsection, also to give a better lower bound. The following theorems are stated and proved in [DFGM19]:

Theorem 5.1 ([DFGM19]). For any n, it holds that q(n)  1.25n+o(n).

Theorem 5.2 ([DFGM19]).

For n =  · 2k it holds that q(n)  2

h

1 2b+1 

-

1 2b 

n-o(n) for any

b  N.

For any , we can find a "good" lower bound on q(n) by choosing b = 0 or b = 1 and applying Theorem 5.2. Specifically, when  = 1.25 we get q(n) = 1.25n±o(n) by choosing b = 1, and for other values of  we can always ensure that q(n)  1.232n-o(n) by choosing b = 0 or b = 1,
depending on . We give alternative, simple proofs for these bounds:

Proof of Theorem 5.1. Fix   [1, 2). Let  such that: i = 1/2 for any i. Note that   A for any fixed (c, b)  C. Thus:

Denote x =





G() = inf max h(i)ci - h ici

(c,b)C A

i=0

i=0



1

 inf h(1/2)ci - h
(c,b)C

2 ci

i=0

i=0



1

= inf
(c,b)C

ci - h 2

ci .

i=0

i=0

 i=0

ci

,

so

0



x



1,

since

(c, b)



C.

Hence

:



1

inf

ci - h

(c,b)C i=0

2 ci
i=0

 inf x - h(x/2).
0x1

Calculation shows that:

inf x - h(x/2) = 0.4 - h(0.4/2) = - log 1.25.
0x1
That is, G()  - log 1.25, and hence indeed: q(n) = 2-G()n±o(n)  2log 1.25n+o(n) = 1.25n+o(n).

Proof of Theorem 5.2. Fix   [1, 2). Let (c(b), b)  C such that b  N and c(b) is the following

sequence:

c(b)i =

1 2b
0

i = 0, i = 0.

Indeed (c(b), b)  C for any b, since all constraints are satisfied:

24

· b  N.

·

We

have

0

1 2b·2



1 2b



1 20·1

= 1 and hence 0  c(b)i  1 for any i.

·

 i=0

c(b)i

=

1 2b



1.

·

 i=0

c(b)i2b-i

·



=

1 2b

·

2b

=

1.

Moreover, a sequence   A must satisfy 0 = 1/2, and for any other i the value of i does not have any effect. Thus:





G() = inf max h(i)ci - h ici

(c,b)C A

i=0

i=0





 max
A

h(i)c(b)i - h

ic(b)i

i=0

i=0

1

11

1

1

= h(1/2) · 2b - h 2 · 2b = 2b - h 2b+1 .

Hence:

q(n) = 2-G()n±o(n)  2 h

1 2b+1 

-

1 2b 

n-o(n).

5.2 A new lower bound on q(n)

Using our estimate q(n) = 2-G()n±o(n) we can find a tighter lower bound on q(n) than the one appearing in [DFGM19], that is, 1.232n-o(n). We do that by finding a matching upper

bound on G().

We

already

know

that

G()



1 2b

-h

1 2b+1 

for any b  N, as described

in our alternative proof for the known lower bound on q(n). For   1.7 and b = 1 we have

G()



1 2·1.7

-h

1 4·1.7



-0.3083

and

for





1.95

we

have

G()



1 1.95

-

h

1 2·1.95

 -0.30846.

So, if we find M > -0.3083 such that for 1.7 <  < 1.95: G()  M , then M is an upper bound

for G(). As we will now show, it is possible for M  -0.305758. The idea is to fix b = 1 and

consider sequences c in which s = c0 + c1 is fixed and ci = 0 for all i  2. Then, due to the

constraint

 i=0

ci/2i

=

1/2

we

can

express

c0

and

c1

in

terms

of

.

Finally,

we

use

Lagrange

multipliers to find the maximizing  for c.

Theorem 5.3. For any n  N, it holds that q(n)  1.236n-o(n).

Proof. Consider the sequence c defined by c0 = 1/ - s, c1 = 2s - 1/ and ci = 0 for all i  2, for some fixed s. It is feasible:

ci/2i = 1/ - s + (2s - 1/)/2 = 1/2,
i=0

as required. We calculate maxA P (c, ) using Lagrange multipliers. Our only constraint is

 i=0

ici/2i

-

1/4

=

0,

so

we

get

the

Lagrangian

function

L(0, 1, ) = h(0)c0 + h(1)c1 - h(0c0 + 1c1) + (0c0 + 1c1/2 - 1/4).

Recall

that

h

(x)

=

log

1-x x

and

compute

the

derivatives:

dL(0, 1, ) d0

=

c0 log

1

- 0 0

-

c0

log

1

- 0c0 - 1c1 0c0 + 1c1

+

c0

=

c0

log

(1 - 0)(0c0 + 1c1) 0(1 - 0c0 - 1c1)

+

c0

=

0,

(11)

25

dL(0, 1, ) d1

=

c1

log

1

- 1 1

-

c1

log

1

- 0c0 - 1c1 0c0 + 1c1

+

c1/2

=

c1

log

(1 - 1)(0c0 + 1c1) 1(1 - 0c0 - 1c1)

+

c1/2

=

0,

(12)

dL(0, 1, ) d

=

0c0

+

1c1/2

-

1/4

=

0.

(13)

We assume c0, c1 > 0, so equations (11),(12) can be written as

log (1 - 0)(0c0 + 1c1) = -, 0(1 - 0c0 - 1c1)

log

(1 - 1)(0c0 + 1c1)

2
= -,

1(1 - 0c0 - 1c1)

so we get:

1 - 0 0

·

0c0 + 1c1 1 - 0c0 - 1c1

=

(1 - 1)2 12

·



0c0 + 1c1 2 1 - 0c0 - 1c1

1 - 0 0

=

(1 - 1)2 12

·

0c0 + 1c1 1 - 0c0 - 1c1

since 0c0 + 1c1 > 0. Together with equation (13), we can solve two equations with two

unknowns and find 0, 1. Calculation shows that there is only one real solution, which is a

critical point, and we will deduce that it is also a maximum point. First, we have to fix s. We

used a software program to try and find a good value for s. It must hold that c1 = 2s - 1/ > 0,

that

is,

s>

1 2

,

implying

s

> 5/17

(since



> 1.7).

Iteratively

checking

all

feasible

values

of

s with gaps of 1/1000, it seems that s = 829/2000 is a good choice. For that value of s, the

maximal value of P (c, ) (where  is defined by the critical point solution found for 0, 1) is

 -0.305758, attained at   1.80941. It remains to check that the critical point solution found

for 0, 1 is indeed a maximum point. Since in our critical point 0, 1 / {0, 1} for any , we

can do that by checking the values attained when 0  {0, 1} or 1  {0, 1} and verify they are

lower than the value attained at our critical point. Calculation shows that when choosing such

points we get a lower value for P (c, ) for any , compared to the value attained at the critical

point. Thus the critical point we have found is indeed a maximum point and G()  -0.305758

for any 1   < 2. That is, q(n)  20.305758n-o(n) > 1.236n-o(n).

Figure 1 demonstrates our new bound for different values of .

5.3 Improving the bound q(n)  1.25n+o(n) for n far from 1.25 · 2k

Using our formula for G(), it can be shown that when  = 1.25, the upper bound of q(n)  1.25n+o(n) can be exponentially improved. This result is formally stated as follows:

Theorem 5.4. If  = 5/4 +  such that || > 0, then there exists  > 1 such that q(n)  (1.25/)n+o(n). Furthermore,  = 2 ||2+ , where > 0 is any fixed constant of our choice.

We prove the theorem using a sequence of steps. First, we show that sequences c  C with

 i=0

ci

which

is

far

from

2/5

can

be

ruled

out

as

witnesses

for

q(n)

=

1.25n±o(n):

26

Upper bounds on G() for   1.5

Figure 1:

The

blue

and

red

curves

are

the

known

upper

bounds

1 2

-h

1 4

and

1 

-h

1 

,

respectively. Our new upper bound is the green curve, which is better in a range of  values.

Lemma 5.5. Let c  C and x =

 i=0

ci.

If

x

=

2/5 ±

,

then

maxA P (c, )



- log(5/4) +

 2 .

Proof. Define   A that assigns i = 1/2 for all i  N, which is always a feasible choice. Then:

P (c, ) = x - h(x/2).

The function x - h(x/2) attains its unique minimum at x = 2/5, at which point its value is - log(5/4). By a linear approximation around 2/5,
x - h(x/2) = - log 1.25 +  2 .

The next technical lemma is required for the argument used in the proof of the theorem.

Lemma 5.6. For every  = 0 the following holds for 0 = ||/100.

Let  = 5/4 +  and let c  C with x :=

 i=0

ci

= 2/5 ± 



1 2b

,

where

0



0.

Then

there exists I  N such that:

2I -b x -  = (||),
2I +1-b x -  = -(||).

Proof.

Let

I

N

be

the

largest

value

for

which

x-

2I -b 

 0.

There

must

be

such

I:

clearly,

there exists I

for

which

x-

2i-b 

<

0

for

any

i



I

.

Moreover,

by

assumption,

when

choosing

I

=

0

we

have

x-

2I -b 

=

x-

1 2b



0.

Having that, the correctness of the lemma boils down to whether the expression

x

-

2l 

might be very small when x is close enough to 2/5 and  is far enough from 1.25. The answer is

negative:

if

l



-2

then

2l/



1 4



1/4

and

hence

x

-

2l 

= (1) even for a constant . On

27

the other hand, if l  0 then 2l/  1/  1/2, so in this case too

x

-

2l 

= (1) even for a

constant . So, the only difficult value of l is l = -1. In this case:

x-

2l 

=

2/5 ± 

-

1 2(1.25 + )

=

(8 ± 20) ± 25 25 + 25

=

±(| |).

Now we are ready to prove the theorem.

Proof of Theorem 5.4. Let > 0 be a fixed constant. We will show that G()  - log(5/4) +

 ||2+ , thus implying the result stated in the theorem. We show that for all b  N and

c  [0, 1]N satisfying

 i=0

ci

/2i

=

1 ·2b

and

 i=0

ci



1,

we

can

find





[0, 1]N

such

that

 i=0

ici

/2i

=

1 ·2b+1

and P (c, )  - log(5/4) + 

| |2+

.

Let 0 = ||/100 and denote x =

 i=0

ci

=

2/5 ± .

By

Lemma

5.5,

if

|x - 2/5|



0,

then

the theorem follows even with = 0. Hence we can assume, from now on, that |x - 2/5| < 0.

Let S  N, let T = N \ S, and let S, T  [-1, 1] be two parameters small in magnitude.

Define

pS = ci/2i,
iS
pT = ci/2i,
iT

qS = ci,
iS
qT = ci.
iT

Consider the assignment

i =

1 2

+

S

1 2

+

T

if i  S, if i  T.

Since

 i=0

1 2

·

ci/2i

=

1 ·2b+1

,

this

assignment

is

feasible

if

SpS + T pT = 0.

We assume henceforth that  = max(|S|, |T |)  0, where 0 = O ||1+ . By construction, we have h(i) = 1 - O(2) using a linear approximation. In contrast,



x

ici = 2 + SqS + T qT .

i=0

Since |x - 2/5| < 0, this shows that



h

ici = h(x/2) + (SqS + T qT )h (x/2) ± O(2)

i=0

using a linear approximation. Overall, this shows that

P (c, ) = x - h(x/2) + (SqS + T qT )h (x/2) ± O ||2+2 .

Moreover, we have h (x/2) = (1) since x < 2/5 +  < 1/2. Since x - h(x/2)  - log(5/4), it suffices to show that there exist S, T which are bounded in magnitude by 0 such that |SqS + T qT | = (||2+ ) (if SqS + T qT < 0, we simply negate S, T ).

28

Suppose pS

 pT ,

or equivalently pS

 (pS + pT )/2 =

1 ·2b+1

.

Then the

condition SpS +

T pT

=

0

shows

that

S

=

-

pT pS

T

,

and

so

SqS + T qT =

qT

-

pT pS

qS

T .

Since

|S |



|T |,

if

|qT

-

pT pS

qS

|

=

(| |)

then

by

choosing

T

=

| |1+

we would be done.

Recall

that

pS + pT

=

1 ·2b

and

qS

+ qT

= x.

Therefore

qT

-

pT pS

qS

=

x

-

qS

-

pT pS

qS

=

x

-



1 · 2b

qS pS

.

Our goal therefore is to find a set S such that the following two conditions hold:

1 pS   · 2b+1 ,

x

-



1 · 2b

qS pS

= (||).

Let SI = {0, . . . , I} and S>I = {I + 1, . . .}. Let us first assume that the choice of S to be

S0 yields

x-

1 qS0  · 2b pS0

=x-

1  · 2b

< 0.

So, it must hold that b = 1: notice that

1 2b+1

1  2b

=



ci/2i  x = 2/5 ± ,

i=0

so if b = 0 it is a contradiction. On the other hand, if b  2 then

1

1

1

0 > x -  · 2b  2/5 ±  - 4  2/5 ±  - 4

is a contradiction. So, we get that

0>x-

1 qS0  · 2b pS0

=x-

1 2

= 2/5 ±  -

2 ,
5 + 4

implying

that



<

0,

and

hence

indeed

2/5 + 

-

2 5+4

=

-(| |).

Notice

that

qSi  qS0 = 1

pSi

pS0

for

any

i,

so

we

choose

S

to

be

SI

such

that

pSI



1 ·2b+1

and

then

both

conditions

hold.

Suppose now that the choice of S to be S0 yields

x-

1 qS0  · 2b pS0

=x-

1  · 2b

0

and let I  N be the one picked by Lemma 5.6. By construction, one of SI , S>I satisfies the first condition. As for the second condition,

x

-



1 · 2b

qSI pSI

x-



1 · 2b

2I

2I -b =x-


= (||),

x

-



1 · 2b

qS>I pS>I



x

-



1 · 2b

2I

+1

=

x

-

2I +1-b 

= -(||),

by Lemma 5.6, and so it is satisfied for both SI , S>I .

29

6 d-ary questions
In this section we generalize some results on q(n) appearing in [DFGM19] to the d-ary setting, in which each question has d possible answers (instead of only "Yes" or "No"). In this setting, a set of allowed questions Q contains a collection of partitions of Xn to d distinguished subsets (Si)i[d]. We denote the natural generalization of q(n) to the d-ary setting with q(d)(n). That is, q(d)(n) is the minimal size of a set of allowed questions Q that allows Alice to construct an optimal strategy for any distribution on Xn picked by Bob.
We present two results in this section. The first states that for any d = o n/ log2 n , it holds that q(d)(n) < 2n+o(n); this improves exponentially on the trivial upper bound q(d)(n)  dn/d!. The second result is that for any fixed d, the upper bound we have just mentioned is tight up to sub-exponential factors for infinitely many n values.
In the binary setting, our results on q(n) rely on the reduction of [DFGM19] from calculating q(n) to calculating min(n), that is, on the fact that q(n)  1/min(n) (Theorem 2.2). We take here the same approach: define (mdi)n(n) to be the natural generalization of min(n) to the d-ary setting (a formal definition appears later). We will show that q(d)(n)  1/(mdi)n(n), and then we find bounds on (mdi)n(n) to derive bounds on q(d)(n). Most of the lemmas in this section are simple generalizations of those appearing in [DFGM19].
Let us first generalize some basic notions from the standard binary setting. All logarithms have base d, unless written otherwise.
Definition 6.1. A distribution µ is d-adic if every element with non-zero probability in µ has probability d- for some positive integer .

d-ary search trees. In the d-ary setting, similarly to the standard binary setting, a strategy to reveal the secret element x is represented by a search tree. The difference is that in the d-ary setting, we use d-ary search trees (instead of binary search trees, namely, decision trees): each internal node, representing a question, has d outgoing edges, representing the possible answers.
However, if n = |Xn| is not equivalent to 1 modulo d - 1, then such a tree can not be constructed. So, if that is the case, we add a minimal set Xl of zero probability elements, such that n + l is equivalent to 1 modulo d - 1. A d-ary search tree can now be successfully constructed for Xn  Xl. For our convenience, we still relate to Xn as the set of elements (and not to Xn  Xl): note that l < d, and thus if we assume that d is an asymptotically small enough function of n, this has no affect on the results in this section (in particular, we do not care about sub-exponential factors in our estimates). Indeed, we have to limit the discussion only for d = o(n/(log n log log n)), from other reasons, even when n is equivalent to 1 modulo d - 1, so this issue has no meaning in our work.
Decision trees definitions and notation from Section 2 naturally generalize to d-ary search trees.

d-ary Huffman algorithm. Similarly to the binary case, if µ is a distribution over Xn, then

the d-ary version of Huffman's algorithm finds a d-adic distribution  that defines a search tree

T

with

T (xi) = log

1 i

for

any

non-zero

element,

such

that

the

cost

of

T

on

µ,

which

is

T (µ) =

n

1

i=1 µi log i

=

n

1

i=1 µi log µi

+

n i=1

µi

log

µi i

= H(µ) + D(µ

)

(where D(µ  ) =

n i=1

µi

log(µi/i)

is

the

Kullback­Leibler

divergence ),

is

optimal.

This

implies

the inequality T (µ)  H(µ) due to non-negativity of D(µ  ). It holds as equality when µ is

d-adic.

30

The chain rule of conditional entropy. Let S = (Sj)j[d] be a partition of Xn into d sets, and let µ be a distribution over Xn. Let M be a random variable drawn from µ, and let P be a random variable indicating the set in S that M belongs to. The probability distribution of P
is the distribution , defined by j = iDj µi for any j  [d]. The chain rule of conditional entropy states that:
H(M ) = H(P ) + H(M |P ),

where

H(M |P ) = Pr[P = p] · H[M |P = p].
p

We will use it in the following equivalent form:

d
H(µ) = H() + jH µ|Sj .
j=1

The multinomial coefficient. Let n  N and k1, . . . , kd  N such that

d i=1

kd

= n.

Let



be the induced distribution defined by i = ki/n for any 1  i  d. We will use the following

known bounds on the multinomial coefficient (see [CS04], for example):

1 O(nd)

2nH ()



n k1, k2, . . . , kd

 2nH().

(14)

In the following subsections we show the reduction q(d)(n)  1/(mdi)n(n), then we upper and lower bound (mdi)n(n), and finally prove the two main results of this section.

6.1 Reduction to d-adic hitters

First we state the following reduction.

Lemma 6.2. A set Q of questions is optimal if and only if c(Q, µ) = Opt(µ) for all d-adic distributions µ.

Proof. Assume that Q is optimal for all d-adic distributions and let  be some arbitrary distribution. Let µ be a d-adic distribution such that:

n

1

Opt()

=

i=1

i

log

. µi

Let T be an optimal decision tree for µ using Q only, and let  be the corresponding d-adic distribution, that is i = d-T (xi). Since  minimizes H(µ) + D(µ  ),  = µ must hold. Hence:

n

1

n

1

T () = i=1 i log i = i=1 i log µi = Opt().

Now we define the notion of d-adic hitters.

Definition 6.3. If µ is a non-constant d-adic distribution, we say that a partition (Si)i[d] of Xn divides µ if µ(Si) = 1/d for any i  [d]. The collection of all such partitions of Xn is denoted Div(µ). A set Div(µ), for some distribution µ, is called a d-adic set. A set of questions Q is
called a d-adic hitter if it intersects Div(µ) for all non-constant d-adic distributions µ.

31

Let us generalize the "useful lemma" appearing in [DFGM19] for our usage:

Lemma 6.4. Let d  N and let p1  · · ·  pn be a non-increasing list of numbers of the form

pi = d-ai, where ai  N, and let a  N be such that a  a1. If

n i=1

pi



d-a

then

for

some

m

we have

m i=1

pi

=

d-a.

If

furthermore

n i=1

pi

=

l

·

d-a

for

some

l



N

then

we

can

divide

[n]

to l intervals (Ij)j[l] such that for any interval Ij  [n] we have iIj pi = d-a.

Proof. Let m be the maximal index such that

m i=1

pi



d-a.

If

m

=

n

then

we

are

done,

so

suppose that m < n. Let S =

m i=1

pi.

We

would

like

to

show

that

S

=

d-a.

The

condition

p1  · · ·  pn implies that am+1  · · ·  a1, and so k = dam+1S =

m i=1

dam+1-ai

is

an

integer.

By assumption, k  dam+1-a, whereas k + 1 = dam+1

m+1 i=1

d-ai

> dam+1-a.

Since

dam+1-a

N

(since am+1  a1  a), we conclude that k = dam+1-a, and so S = d-a.

To prove the furthermore part, notice that by repeated applications of the first part of the

lemma we can partition [n] into intervals with probabilities d-a.

Among else, this lemma shows (by choosing a = 1) that Div(µ) is non-empty for any non-constant d-adic µ.

Lemma 6.5. A set Q of partitions of Xn to d subsets is an optimal set of questions if and only if it is a d-adic hitter in Xn.
Proof. Let Q be a d-adic hitter in Xn, and let µ be a d-adic distribution. We show by induction on the support size m  n that c(Q, µ) = H(µ). Recall that Opt(µ) = H(µ), and thus due to Lemma 6.2 optimality of Q will follow. The base case m = 1 is trivial. So, suppose that m > 1 and hence µ is non-constant, and therefore Q contains a partition D = (Di)i[d]  Div(µ). Since D  Div(µ), it holds that µ|Di is d-adic for all i  [d]. The induction hypothesis implies c(Q, µ|Di) = H(µ|Di) for all i  [d]. Having that, let us calculate H(µ). Let  be the distribution defined by i = µ(Di) = 1/d for any i  [d], so due to the chain rule of conditional entropy and the induction hypothesis:

d

d1

H(µ) = H() + iH(µ|Di) = 1 + d c(Q, µ|Di).

i=1

i=1

Now, consider the cost of a decision tree T asking D, and then uses the implied algorithms for µ|D1, . . . , µ|Dd, depending on the answer for D:

d

d1

T (µ) = 1 + µ(Di) · c(Q, µ|Di) = 1 + d c(Q, µ|Di) = H(µ),

i=1

i=1

and so c(Q, µ)  H(µ), thus Q is optimal. Conversely, suppose that Q is not a d-adic hitter, so let µ be a non-constant d-adic distribution
such that Div(µ) is disjoint from Q. Consider an arbitrary decision tree T for µ using Q, and let P = (Pi)i[d] be its first question. Let also  be the distribution defined by i = µ(Pi) for any i  [d]. Then

d

d

T (µ)  1 + i · c(Q, µ|Pi) > H() + i · H(µ|Pi) = H(µ),

i=1

i=1

since there is i such that i = 1/d, otherwise it contradicts Q and Div(µ) being disjoint, thus H() < 1, and moreover c(Q, µ|Pi)  H(µ|Pi). So the cost of any such arbitrary tree is more than H(µ), thus Q is not optimal.

32

6.2 Reduction to maximum relative density

Let us generalize the concept of maximum relative density defined in Section 2.

Definition 6.6. Let D be a collection of partitions D = (Di)i[d] of Xn. Let K be the set of all

vectors k = (k1, k2, . . . , kd)  {0, . . . , n}d such that

d i=1

ki

=

n.

For

k



K,

denote

by

Dk



D

the restriction of D to partitions with |Di| = ki for all i  [d]. We say that each such vector

k  K is a type of partitions, as this usage is similar to the concept of types in the theory of

types. Define k's relative density of D, denoted k(D), as

k(D) :=

Dk
n

.

k1,k2,...,kd

We define the maximum relative density of D, denoted (D), as

(D) := max k(D).
kK

Define (mdi)n(n) to be the minimal (D) over all d-adic sets D. We will show that calculating q(d)(n) up to sub-exponential factors can be reduced to calculating (mdi)n(n). First, we prove an argument used in the reduction:

Lemma 6.7. There are at most nn non-constant d-adic distributions over Xn.

Proof. Let µ be a non-constant d-adic distribution over Xn. We assume that the minimal non-zero probability in µ is d-l and show that n > l by induction on l. This argument implies that for a fixed n, the possible probabilities are only 0, d-1, d-2, . . . ., d-(n-1) and hence there are at most nn ways to construct a d-adic distribution on Xn. For the base case l = 0 it holds
that n > 0. For the induction step, assume that the claim holds for l - 1. Let us first show that the number of elements with probability d-l is a multiple of d. Denote the set of these elements with L. Since the minimal non-zero probability in Xn\L is at least d-l+1, the total weight of the elements in Xn\L can be written as x · d-l+1 where x  N, because each element with probability d-l+y for some y  1 simply contributes dy-1 to x, and dy-1 is an integer. So,
the following must hold:

n
1 = µi =

µi +

µi = |L| · d-l + x · d-l+1

i=1

µi:xiL

µi:xiXn\L



|L| · d-l = 1 - x · d-l+1



|L| = dl - x · d = d dl-1 - x .

That is, |L| is a multiple of d, since dl-1 - x is an integer. Following that, we define a new distribution µ on Xn by merging the elements in L into dl-1 - x elements with probability d-l+1. Now the minimal non-zero probability in µ is d-l+1 and since we have merged at least d > 1 elements, it holds that n  n-1. So, by the induction hypothesis we have n-1  n > l-1, that is, n > l.
Now we can prove the reduction.

33

Theorem 6.8. Fix n  N. Then:
1/(mdi)n(n)  q(d)(n)  2n2d ln n/(mdi)n(n).
Proof. Recall that q(d)(n) is actually the size of a minimal d-adic hitter for Xn, due to Lemma 6.5. Hence we bound the size of such a set, instead of q(d)(n) directly. Fix a d-adic set D over Xn with (D) = (mdi)n(n). Fix k  K and consider an arbitrary partition S = (Si)i[d] of Xn with |Si| = ki for any i  [d]. Let  be a uniformly random permutation on Xn, then:

k(D) = Pr[S  (D)].

Having that and the definition of (mdi)n(n), it follows that for any partition S on Xn:

Pr[S  (D)]  m(di)n(n).

Let Q be a collection of partitions of Xn with |Q| < 1/(mdi)n(n). Then by the union bound:

Pr[Q



(D)

=

]



Pr[Q
QQ



(D)]

<

1 m(di)n(n)

·

(mdi)n(n)

=

1.

Thus, there is a permutation  such that Q  (D) = . Since (D) is a d-adic set, it follows that

Q is not a d-adic hitter. So, indeed any d-adic hitter must contain at least 1/(mdi)n(n) questions. Now we shall upper bound q(d)(n). Construct a set Q of questions containing, for any k  K,

1 (mdi)n

(n)

2n

ln

n

uniformly

chosen

partitions

(Si)i[d]

of Xn

with |Si| = ki

for any i  [d].

Note

that

|K |



(n + 1)d

and

thus

|Q|



1 (mdi)n (n)

2n2d

ln

n.

We

will

show

that

with

positive

probability,

Q is a d-adic hitter. Fix an arbitrary d-adic set D. Let k  K such that k(D) = (D). The

probability that a random partition (Si)i[d] of Xn with |Si| = ki for all i  [d] does not belong

to D is at most

1 - k(D) = 1 - (D)  1 - (mdi)n(n)

(since (D)  (mdi)n(n)). Therefore the probability that Q is disjoint from D is at most

1 - (mdi)n(n)

 e = n . 1
(mdi)n

(n)

2n

ln

n

-(mdi)n (n)

1 (mdi)n

(n)

2n

ln

n

-2n

By Lemma 6.7, there are fewer than n2n d-adic distributions over Xn. Having that, a union bound shows that the probability that a d-adic set D (corresponding to some d-adic distribution µ) which is disjoint from Q exists is less than 1. That is, the probability that Q is a d-adic hitter is positive.

Due to this theorem, if d = o(n/(log n log log n)), we have:

q(d)(n) = 2±o(n) ·

1 .

(mdi)n(n)

Hence, from now on we discuss (mdi)n(n) instead of q(d)(n), and restrict the discussion to d = o(n/(log n log log n)).
Before we discuss some bounds on (mdi)n(n), let us define the generalized tail of a d-adic distribution:

34

Definition 6.9. Let µ be a d-adic distribution over Xn. The generalized tail of µ is the largest set T  Xn such that for some a  1:
1. µ(T ) = d-a.

2. T does not contain zero-probability elements.
3. All elements in Xn\T have probability at least d-a or zero.
If there are a few sets satisfying those requirements, the generalized tail is one of them, arbitrarily.
Lemma 6.10. Suppose that µ is a non-constant d-adic distribution. Let D = (Dj)j[d]  Div(µ) be a partition of Xn. For all j  [d], Dj either contains T or disjoint from T .
Proof. Let j  [d]. If Dj is disjoint from µ then we are done. So, Assume that Dj T = . Since all non-zero elements in Xn\T have probability at least d-a, we can denote µ(Dj  (Xn\T )) = s·d-a where s  N. Recall that µ(Dj) = 1/d, so if we denote µ(Dj  T ) = y we can write:
1/d = s · d-a + y.

Now, note that s  da-1 - 1: recall that Dj  T =  and thus y > 0. Assume towards contradiction that s > da-1 - 1, that is, s  da-1. Then:
1/d = s/da + y  da-1/da + y = 1/d + y > 1/d

which is a contradiction. Having that, we lower bound y:

y

=

1/d

-

s/da



1/d

-

da-1 - da

1

=

1/d

-

1/d

+

1/da

=

µ(T ),

and hence µ(Dj  T ) = µ(T ), that is, Dj  T = T , and so Dj contains T completely.
In the following sections we prove upper and lower bounds on (mdi)n(n). The following function fd : (0, 1)  R, defined for any d  N, will appear in both of our bounds:

1

1

fd() = d ·  · log2 d - (1 - (d - 1)) log2 1 - (d - 1) + (d - 1) log2  .

6.3 Upper bounding (mdi)n(n)
The following lemma implies different upper bounds on (mdi)n(n) for different sequences of n values.

Lemma

6.11.

Fix

d



N

and

1 d2+1







1/d.

For

any

n

of

the

form

n

=

da d·

, where a  N,

there exists a d-adic distribution µ over Xn which satisfies

(Div(µ))  2fd()n+o(n).

Proof. We first assume that

da d·

=

da d·

.

Let

n

=

da d·

where

a  N.

Note

that

n = da-1,

and

construct the following d-adic distribution µ on Xn:

1.

For i  [d · n - 1]:

µi = d-a =

1 d·da-1

=

1 d·n

.

2. All other (1 - d)n + 1 elements are a (generalized) tail of probability d-a.

35

As we have shown, the generalized tail elements must be chosen to the same set in a partition,
in order to get a partition which divides µ, thus we can think of them as a single element
when constructing a partition in Div(µ), such that we have d · n elements in total, with equal probabilities. Thus, there is only one feasible type k of partition: choosing n = da-1 elements
to each set in the partition (that is, ki = n for any i  [d], assuming that the tail is treated as a single element). The total probability of each set in the partition is thus da-1 · d-a = 1/d.
This discussion leads us to the following bound:

(Div(µ)) = k(Div(µ))

(dn)!

=

(n)!d n!

(n)!d-1((1-(d-1))n)!

2d·n·log2 d



2 (14)

n

(1-(d-1)) log2

1 1-(d-1)

+

d-1 i=1



log2

1 

/O(nd)

= O nd

·2

d··log2 d-

(1-(d-1)) log2

1 1-(d-1)

+(d-1)

log2

1 

n = 2fd()n+o(n).

Now, assume that

da d·

<

da d·

.

In that case, let 

such that

da

da

=

.

d· d·

Now, construct the aforementioned d-adic distribution µ for  instead of . From previous

arguments, we have:

(Div(µ))  2fd( )n+o(n).

Fortunately, this is enough: by the definition of  and the constraint   1/d, it holds that

1







+

d(da

-

. 1)

Recall

that





1 d2+1

.

This

implies

d(da

-

1)

=

(n),

that

is

     + (1/n).

Therefore, it holds that fd( )n  fd()n + O(1), and hence the lemma holds also for the case

da d·

<

da d·

.

For n of the form n =

da d·

, obviously (mdi)n(n)  (Div(µ)), where µ is the distribution

defined in the proof of Lemma 4.7. Hence for such n values we have

(mdi)n(n)  2fd()n+o(n).

6.4 Lower bounding (mdi)n(n)
We will use the following partition of Xn in order to lower bound (Div(µ)) for some non-constant d-adic distribution µ:
Lemma 6.12. Let µ be a non-constant d-adic distribution over Xn. There exists a partition of Xn of the form

Xn = (Di  Ei)
i=1
such that:

36

1. Di consists of elements with equal probabilities pi. 2. |Di| = dci - ri for some natural ci and 0  ri < d, and µ(Ei) = ripi.

3.  = O(log n).

Proof. We assume w.l.o.g that the elements are sorted µ1  µ2  · · ·  µn. We construct the sets Di, Ei in iterations on i from 1 to . In each iteration i, assume we have ordered probabilities

µi  µ2  · · ·  µNi of the available elements which were not chosen in previous iterations to Di, Ei (initially, 1 = 1, N1 = n). The elements chosen for Di are always an interval which begins in the leftmost index i and up to some index i. The elements in Ei (if it is not empty)

are always an interval which begins in some index Mi > i and ends at Ni. The rest of the

elements are available for the next iteration, until no elements are available and the partition

is complete. The partition must be completed since Di =  for any i. Now let us describe an

iteration i in detail. Let i be the last index with µi = µi (that is, µi+1 < µi or i = Ni). Let

Di = {xi, . . . , xi}. Denote |Di| = dci - ri where ci, ri  N and 0  ri < d. Let Mi > i be an

index such that

Ni j=Mi

µj

= ripi

if

ri

> 0,

and

Mi

=

otherwise.

Define

Ei

= {xMi , . . . , xNi }

(if d·

Mi = , then Ei µi, and Mi exists.

= ). We show inductively that For the base case 1 = 1 and N1

for any i, = n, note

Ni j=i
that

µj is a multiple

n j=1

µi

=

1

which

of is

a multiple of d · µ1 since µ is non-constant. The existence of the index M1 now follows from

Lemma 6.4: Suppose that

N1 j=1

µj

=

k

·

dp1,

so

by

Lemma

6.4

we

can

partition

{x1 , . . . , xN1 }

to k intervals, each of probability p1. So, M1 is simply the first index of the interval composed

from the concatenation of the last r1 intervals, if r1 > 0. For the induction step, assume that

for iteration i - 1,

µ Ni-1
j=i-1 j

is

a

multiple

of

d · µi-1

and

that

Mi-1

exists.

By

assumption,

i-1 j=i-1

µj

+

µ Ni-1
j=Mi-1 j

= d · µi-1

· ci-1

for

some

integer

ci-1.

When

continuing

to

iteration

i, we are removing Di-1  Ei-1 from the available elements, and recall that

Ni-1 j=i-1

µj

is

also

a

multiple of d · µi-1 by assumption, and thus we still have a multiple of d · µi-1 in the available

elements of iteration i (that is, after removing Di-1  Ei-1). Since µi-1 is a multiple of µi,

we also have a multiple of d · µi. The existence of the index Mi now follows from Lemma 6.4

similarly as in the base case.

It remains to show that  = O(log n). Let us consider the first iteration. If the case is that

|D1| is a multiple of d · µ1, we change the partition a bit, and leave the last element of D1 out,

and therefore use a non-empty E1. Now, it must hold that µ(E1)  µ1. Since the probabilities are ordered µ1  · · ·  µn we have

n · µM1-1  n · µM1  µ(E1)  µ1,

that is, µM1-1  µ1/n. Since the probabilities are d-adic, there are at most log n + 1 different probabilities in µ2, . . . , µM1-1:
µ1/d0, µ1/d1, µ1/d2, . . . , µ1/dlog n

and therefore  = O(log n).

Now we can prove the main lemma:

Lemma 6.13. If d = o n/ log2 n , then for every non-constant d-adic distribution µ there is

0 <  < 1 such that

(Div(µ))  2fd()n-o(n).

37

Proof. We will use a partition of Xn of the form

Xn = (Di  Ei)
i=1

as constructed in Lemma 6.12. It is implied from Lemma 6.12 that µ(Di  Ei) = d · ci · pi

for some ci  N. If Ei = , we consider a partition of Ei into ri subsets, each with total

probability pi. Indeed, such a partition exists by Lemma 6.4. Denote by Ei the set of those

subsets, where each subset is contracted into a single element. So, Ei is a set of ri elements with

probability pi each, and Xn = i=1(Di  Ei).

thus

in

Di  Ei

we

have

d · ci

elements,

each

with

probability

pi.

Denote

Let us define a form of partition of Xn into d subsets with equal total probabilities: for any

i  [], let the sets Si(1), Si(2), . . . , Si(d) be d distinct subsets of Di  Ei of size ci each. For any

j  [d], define Sj by


Sj := Si(j).

i=1

Indeed S = (Sj)j[d] exists in Div(µ) after "unpacking" all elements in i[] Ei back to their original state: for any j  [d] we have





1

µ(Sj) = µ(Si(j)) = cipi = d µ(Di  Ei) = 1/d.

i=1

i=1

i=1

So, any partition S = (Sj)j[d] defined in this fashion exists in Div(µ). Having that, consider the

following type k of partitions which includes at least some of those partitions: let kj =

 i=1

ci

for any 1  j  d - 1 and kd = n - (d - 1)k1 (kd can be thought as the size of the set that

"contains the tail", which S1, . . . , Sd-1

as discussed contain only

ienletmheenutps pfreormbouin=d1)D. Di. iSv(oµ, )fokrcaonnytaiins

at []

least the partitions in we choose ci elements

from Di to Sj, for 1  j  d - 1. Moreover, we put all the elements of

 i=1

Ei

in

Sd.

Thus:

Hence:

Div(µ)k



 i=1

(d · ci - (ci!)d-1(ci

ri)! . - ri)!

Div(µ)k



 i=1

(d

· ci - d)! (ci!)d



 i=1

1 (dci)d

(d · ci)! (ci!)d



1 nd

 i=1

(d · ci)! (ci!)d


(14)

1 O(n2d )


2ci·d log d
i=1

1 


2ci·d log d  2d log2 d

22 log2 n·o

n log2 n

·O(log n) i=1

 i=1

ci-o(n).

38

Now,

denote



=

1 n

 i=1

ci

and

note

that

n

2

(d-1)

log2

1 

+(1-(d-1))

log2

1 (1-(d-1))

n

k1, k2, . . . , kd

(similarly to the discussion in the upper bound section). Therefore overall

k(Div(µ))  2

d log2 d-

(d-1)

log2

1 

+(1-(d-1))

log2

1 (1-(d-1))

n-o(n) = 2fd()n-o(n),

and since obviously (Div(µ))  k(Div(µ)), we get the desired result.

6.5 Estimating q(d)(n)
Now we can deduce some explicit bounds on q(d)(n). Those bounds allow us to calculate q(d)(n) up to sub-exponential factors, for infinitely many n values. The upper bound on q(d)(n) will imply that even though the trivial upper bound on the cardinality of Q which allows constructing optimal strategies for all distributions is dn/d!, the true minimal cardinality is much smaller, and in particular it is less than 2n+o(n).
Theorem 6.14. For any n and any d = o n/ log2 n :

q(d)(n) 

d-1 1+ d
d d-1

n+o(n)
=

2-

log d d

n+o(n)
.

Moreover, for any fixed d, the following holds for infinitely many n values:

q(d)(n) =

d-1 1+ d
d d-1

n±o(n)
=

2-

log d d

n±o(n)
.

Proof. Since Lemma 6.13 holds for any d-adic distribution µ where d = o n/ log2 n , we can deduce the lower bound

(mdi)n(n)  exp2

min fd() n - o(n) .
0<<1

Calculation shows that 
fd() = (d - 1) log2 (1 - (d - 1)) + d log2 d

and the minimum is attained at

1

= d

.

d d-1 - 1 + d

We are interested in what happens when  minimizes fd. So, we want to estimate the following function of d:

1

f (d) = fd d

.

d d-1 - 1 + d

After some algebraic simplifications, we get:

d

d d-1

f (d) = log2 d

.

d d-1 + d - 1

39

Since d = o(n/ log n log log n), the reduction in Theorem 6.8 allows us to calculate exp2(-f (d)) in order to get an estimate of q(d)(n): we have

d

d d-1 + d - 1

d-1

exp2(-f (d)) =

d

=1+ d ,

d d-1

d d-1

which implies

q(d)(n) 

d-1 1+ d

n+o(n)
.

d d-1

Moreover, it holds that:

d-1 d =

d1-

d d-1

d d-1

=

d-

1 d-1

.

Calculating

the

Puiseux

expansion

of

d-

1 d-1

shows

that

d-

1 d-1

=1-

log d d

and hence:

q(d)(n)  exp2(-f (d)n + o(n)) =

2-

log d d

n+o(n)
.

For the second part of the theorem, assume that d is fixed, let  =

1
d

and suppose that

d d-1 -1+d

n=

da d·

where a  N.

Note that

1 d2+1



1
d
d d-1 -1+d

 1/d, so we can use Lemma 6.11 and

deduce

(mdi)n(n)  exp2(fd()n + o(n)) = exp2(f (d)n + o(n)) =

d
d d-1
d
d d-1 + d - 1

n+o(n)
,

and hence

q(d)(n) 

d-1 1+ d
d d-1

n-o(n)
=

2-

log d d

n-o(n)
.

7 Open questions

Our work suggests a few open questions which we think are interesting enough for future research. Open Question 1. Is G continuous?

Notes It seems that techniques similar to those used in Section 4 can show continuity-related results, but additional work seems necessary in order to determine whether G is continuous. First, it seems not hard to show that G is upper semi-continuous. Moreover, denote by Gb the function G restricted to some fixed b, such that G() = infbN Gb(). It also seems not hard to show that Gb is continuous. We should use a fixed b since otherwise Lemma 4.11 is not helpful. It is not clear, however, whether G is continuous as well. If we could show that G is lower semi-continuous, or that b can be chosen over some compact subset of N instead of the entirety of N, then continuity of G would follow.

Open Question 2. Is the outer infimum in G attained?

Notes We have shown that the inner supremum in the definition of G is attained and thus can be written as maximum. It is not clear, however, whether the outer infimum is attained as well. Unfortunately, even if we assume that b is fixed, we still can not apply a similar

40

argument to the one used in the supremum case: Say we have a fixed b  N and a sequence of sequences cj jN  C converging to the infimum, and converging pointwise to a sequence c. It does not guarantee (not immediately, at least) that cj jN converges to c in 1-norm, and that property is crucial for c being a minimizing sequence for maxA P (c, ) across all sequences in C.
Open Question 3. Can we calculate G()?
Notes While our formula for G implies G()  -0.305758 for any 1   < 2, it would be interesting to calculate G() in terms of , similarly to the calculation suggested in [DFGM19] for  = 1.25, that is G(1.25) = - log 1.25. This will allow us to calculate q(n) for n of the form n = 2k, up to sub-exponential factors.
Open Question 4. Can we generalize the function G : [1, 2)  R to a function G(d) : [1, d)  R such that (mdi)n(n) = 2G(d)()n±o(n)?

References

[ACD13] Harout Aydinian, Ferdinando Cicalese, and Christian Deppe, editors. Information Theory, Combinatorics, and Search Theory. Springer-Verlag Berlin Heidelberg, 2013.

[AW87] Rudolf Ahlswede and Ingo Wegener. Search problems. John Wiley & Sons, Inc., New York, 1987.

[CAL94] David Cohn, Les Atlas, and Richard Ladner. Improving generalization with active learning. Machine learning, 15(2):201­221, 1994.

[CS04]

Imre Csisza´r and Paul C Shields. Information theory and statistics: A tutorial. Now Publishers Inc, 2004.

[CT06]

Thomas M. Cover and Joy A. Thomas. Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing). Wiley-Interscience, USA, 2006.

[DFGM17] Yuval Dagan, Yuval Filmus, Ariel Gabizon, and Shay Moran. Twenty (simple) questions. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 9­21, 2017.

[DFGM19] Yuval Dagan, Yuval Filmus, Ariel Gabizon, and Shay Moran. Twenty (short) questions. Combinatorica, 39(3):597­626, 2019.

[Dor43] Robert Dorfman. The detection of defective members of large populations. The Annals of Mathematical Statistics, 14(4):436­440, 1943.

[DS01]

Dwight Duffus and Bill Sands. Minimum sized fibres in distributive lattices. J. Aust. Math. Soc., 70(3):337­350, 2001.

[DSW90] D. Duffus, B. Sands, and P. Winkler. Maximal chains and antichains in Boolean lattices. SIAM J. Discrete Math., 3(2):197­205, 1990.
[Huf52] David A. Huffman. A method for the construction of minimum-redundancy codes. Proceedings of the IRE, 40(9):1098­1101, 1952.

41

[Kat73]
[LR87] [Sha48] [You12] [Zim59]

Gyula O. H. Katona. Combinatorial search problems. In J. N. Srivastava et al., editor, A Survery of Combinatorial Theory. North-Holland Publishing Company, 1973.
Zbigniew Lonc and Ivan Rival. Chains, antichains, and fibres. J. Combin. Theory Ser. A, 44(2):207­228, 1987.
Claude E Shannon. A mathematical theory of communication. The Bell system technical journal, 27(3):379­423, 1948.
Neal Young. Reverse Chernoff bound. Theoretical Computer Science Stack Exchange, 2012. URL:https://cstheory.stackexchange.com/q/14476 (version: 2012-11-26).
Seth Zimmerman. An optimal search procedure. Amer. Math. Monthly, 66:690­693, 1959.

42

