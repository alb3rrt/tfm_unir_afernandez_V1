1
Consistent Two-Flow Network for Tele-Registration of Point Clouds
Zihao Yan, Zimu Yi, Ruizhen Hu, Niloy J. Mitra, Daniel Cohen-Or, and Hui Huang
Abstract--Rigid registration of partial observations is a fundamental problem in various applied fields. In computer graphics, special attention has been given to the registration between two partial point clouds generated by scanning devices. State-of-the-art registration techniques still struggle when the overlap region between the two point clouds is small, and completely fail if there is no overlap between the scan pairs. In this paper, we present a learning-based technique that alleviates this problem, and allows registration between point clouds, presented in arbitrary poses, and having little or even no overlap, a setting that has been referred to as tele-registration. Our technique is based on a novel neural network design that learns a prior of a class of shapes and can complete a partial shape. The key idea is combining the registration and completion tasks in a way that reinforces each other. In particular, we simultaneously train the registration network and completion network using two coupled flows, one that register-and-complete, and one that complete-and-register, and encourage the two flows to produce a consistent result. We show that, compared with each separate flow, this two-flow training leads to robust and reliable tele-registration, and hence to a better point cloud prediction that completes the registered scans. It is also worth mentioning that each of the components in our neural network outperforms state-of-the-art methods in both completion and registration. We further analyze our network with several ablation studies and demonstrate its performance on a large number of partial point clouds, both synthetic and real-world, that have only small or no overlap.
Index Terms--Point cloud registration, tele-registration, shape completion, shape prediction, deep points learning
!

arXiv:2106.00329v1 [cs.CV] 1 Jun 2021

1 INTRODUCTION
Shape registration is a long-standing problem with a large variety of methods proposed over the last decades. The registration of partial shapes is significantly more challenging than complete shapes, particularly when the overlap between the parts is small. Popular methods, like RANSAC that match between three or four points, perform well when the overlapping base is large [1], [2], [3], [4], [5], but completely fail when there is no overlap between the two partial shapes. In the case of partial matching, the information theoretic explanation is that the lack of sufficient information in the scans leads to a family of plausible completions, which in turn results in failure of traditional rigid registration.
In recent years, neural networks for geometry processing have rapidly emerged and changed the landscape of 3D processing. One notable competence of neural networks is their ability to learn priors of a family of shapes, thus effectively capturing a distribution over possible shapes. Two examples are the reconstruction of a complete shape from a partial input, and the registration of two non-overlapping partial shapes.
We present a rigid registration technique for two partial scans presented in arbitrary initial poses and having little
· Zihao Yan, Zimu Yi, Ruizhen Hu, and Hui Huang are with Visual Computing Research Center, College of Computer Science and Software Engineering, Shenzhen University. E-mail: {mr.salingo, yizimu, ruizhen.hu, hhzhiyan}@gmail.com
· Niloy J. Mitra is with University College London and Adobe Research. E-mail: n.mitra@cs.ucl.ac.uk
· Daniel Cohen-Or is with Shenzhen University and Tel Aviv University. E-mail: cohenor@gmail.com
· Ruizhen Hu is the corresponding author of this paper · Our code is available at https://github.com/Salingo/CTF-Net

or no overlap at all. This non-overlapping setting has been referred to as tele-registration, and had been attempted in 2D [6] and 3D [7], based on a prescribed feature-conforming prior. In our work, we approach the tele-registration problem using learning tools, in particular, deep learning to encode shape priors. The idea is to jointly train two separate networks on the two tasks of shape completion and shape registration of non-overlapping partial shapes. In training, the networks learn proper priors that allow performing well on these two difficult tasks, rather than treating the tasks independently.
Our key observation is that registered shapes are easier to be completed than each one alone, and complete shapes are easier to be registered, since their overlap clearly increases. Hence, we combine the registration and completion tasks in a way that reinforces each other. In particular, we train the registration and completion networks simultaneously using two coupled flows. One network performs register-and-complete and the other complete-and-register, such that both registration and completion consistencies are maximized. Fig. 1 illustrates our consistent two-flow network (CTF-Net). Note that our completion network only generates information for the missing part, and hence the completions along the two branches of the network can be different, and require a dedicated consistency term to produce canonical completion.
Given two partial point cloud inputs with little or no overlap, our method transforms the partial shapes to canonical positions by learning the prior geometry of its class of shapes, and thus improves the state-of-the-art in terms of completion results on 3D model from the learned class. By comparing our two-flow method to each singleflow and other baselines, we validate that composing two

2
Registration

Two parital scans

CTF-NET

Completion

Fig. 1: CTF-Net registers pairs of partial scans with little or no overlap. The network is designed to encourage the registration and completion network branches to mutually cooperate to be consistent, and thereby regularizes both the (global) registration and completion problems. Here, we show the result of the combined registration and completion of two partial scans, with little overlap, of a real scan of a toy airplane.

flows together effectively strengthens each component. We evaluate our method on synthetic and real-world examples (e.g., RedWood and Pix3D), and demonstrate the superiority of the approach compared to state-of-the-art methods in both global registration (4PCS, DCP) and completion networks (TopNet, PF-Net). In summary, we present a method that addresses the problem of non-overlapping point cloud registration, based on a symmetric neural network that is designed to jointly perform registration and completion, in a way that reinforces each other to establish a new state-ofthe-art.
2 RELATED WORK
There has been extensive research on general registration problems and in three dimensions in particular. ICP [8] is a widely used algorithm for 3D registration. Some following ICP variants [3], [5], [9] aim at improving from different aspects. Recently, DCP [10] is proposed that revisits ICP from a deep learning perspective.
There is an abundance of SLAM works that deals with registrations and pose estimation at the scene level. For a broad survey, see [11]. An apparently similar idea to our is presented by Yang et al. [12]. Unlike our one-step technique, they refine the registration and completion modules iteratively. Yang et al. [13] also propose hybrid representations for relative pose estimation. These methods, however, match 3D RGB-D scenes rather than point clouds, thus require more information such as color, 360-image. Chen et al. [14] introduce a plane-based descriptor for the point cloud registration with a small overlap. However, for many shapes on object-level, like cars or lamps, it is hard to find a plane surface for matching. Brachmann et al. [15] propose a learning-based method for pose estimation, however, this method is mainly designed for camera localization and it's hard to directly adapt it for shape registration.
In the following, we discuss previous works that are most related to the specific tasks of paired shape registration and partial shape completion on the object level, focusing on deep learning techniques.
2.1 Paired shape registration
Recently, there have been research efforts to apply deep neural networks for the task of rigid [16] and non-rigid [17],

[18] registration, to offer faster and more robust than classic techniques. Elbaz et al. [19] propose a method that focuses on localizing the close-proximity scanned point cloud in a large-scale point cloud scene. They use super-points to match the corresponding region, and a deep neural network to calculate the transformation between the local and global point cloud. Yew et al. [20] propose a weakly supervised deep learning framework to holistically learn a 3D feature detector and descriptor from GPS/INS tagged 3D point clouds. They use a Siamese architecture that learns to recognize if the given point clouds are from the same location. The correspondences between point clouds are obtained by a learned descriptor vector. Choy et al. [21] propose a framework for pairwise registration of real-world 3D scans. This method contains a 6-dimensional convolutional network for correspondence confidence prediction, and then the pose is estimated and further refined recursively.
Aoki et al. [22] propose PointNetLK, a modification to the classical LK(Lucas & Kanade) algorithm which circumvents the need for convolution on the PointNet representation. This framework for rigid registration is more robust to initialization and missing parts than classic ICP. Wang et al. [10] propose Deep Closest Point(DCP), that revisits ICP from a deep learning perspective. The ICP-style method consists of three parts, that learns the common features of the input point clouds to register them together. Similarly, Yew and Lee [23] propose RPM-Net, which is less sensitive to initialization alignment comparing to the original ICP method. However, RPM-Net assumes that the normal information is given in the point cloud data. Wong and Solomon [24] propose PRNet, a sequential decision-making framework to achieve point cloud registration iteratively. Unlike the methods mentioned above, this method is able to handle partial-to-partial registration, the key is to use a detector to find the points in common between partial views, and keypoint-to-keypoint correspondences. These deep-learning based methods are aiming at detecting key points in the input paired shape, then match and pair them to compute the alignment transformation. These methods assume that the two parts have a significant overlap region that contains a few key points. Huang et al. [7] present a field-guided algorithm that is able to automatically compose the 3D shape given several pieces of it, where the input pieces

R-C flow
Completion

inverse
Input

R-C flow

3
Registration Consistency

Registration

inverse

C-R flow

C-R flow

Completion Consistency

Fig. 2: The architecture of CTF-Net. Given a pair of partial scans, CTF-Net simultaneously predicts the transformation parameters for registration and coordinates of points for completion. The prediction follows a mirrored manner, which performs register-and-complete(R-C) in one flow and complete-and-register(C-R) in another. The R-C flow and C-R flow are denoted by green and purple lines respectively. These two flows mutually reinforce each other by enhancing consistency on their outputs, which is represented by gray lines.

have no overlapping. In our work, we utilize a data-driven approach to enable the registration, without the prescribed feature prior. Given two partial point clouds with little or no overlapping, we do registration and completion at the same time, to allow the network to better align the input shapes.
2.2 Partial shape completion
There are an increasing number of works focusing on the partial to complete shape generation, many of which are applied on point cloud representation, since it is strongly related to realistic scenarios, where the point clouds are the raw data coming from 3D acquisition devices. PointNet [25] proposed a deep learning method for point cloud shape, which promotes several learning-based applications on the point cloud, including completion. Yuan et al. [26] proposed a method that generates point clouds in two stages, where the first stage is to use a fully-connected decoder to obtain a coarse resolution point cloud and the second stage generates the final output by a folding-based decoder. Tchapmi et al. [27] introduced a novel decoder for point cloud completion which generates arbitrarily structured point clouds without explicitly enforcing a specific structure. The proposed decoder generates point clouds according to a tree structure where each node of the tree represents a subset of the point cloud. Wang et al. [28] proposed a completion method that contains an up-sampling module that predicts denser results than other completion methods. Huang et al. [29] introduced a method that only completes the missing regions of the input shape and can preserve its details, and successfully addressed the blurring issues caused by the auto-encoder structure. However, they still require the input partial data to cover a significant portion of the surface region of the shape, while in many real cases only a small region of the shape is captured by a single scan.
3 OVERVIEW
Our tele-registration method consists of a two-flow network, as illustrated in Fig. 2. The idea is to simultaneously learn

two networks, one for registration (colored in green in the figure) and one for completion (colored in purple in the figure). Taking a pair of partial shapes as input, the C-R flow branch first completes each partial shape, separately, and then registers the parts, which now have higher overlap, to produce an aligned shape. In the R-C flow branch, the input pair is first registered by the registration network, and then completed by the completion network.
The key of our method is to connect and couple the R-C and C-R flows with two losses: one is a registration consistency loss, which encourages the registration networks in the two flows to predict the same transformation parameters; the other is the completion consistency loss, which encourages the two flows to output similar reconstruction results. As we shall show, the two flows strengthen each other. It should be noted that the complexity of the completion and registration in two flows are different. For the completion network, the one trained in R-C flow is easier, since the input shape is already registered, and contains more overlapping geometric information than the one in C-R flow. Similarly, the task for the registration network in the C-R flow is easier because the input pair shapes are more complete than the one in R-C flow. Thus, our final output is the shape completion results from the R-C flow, and the registration parameters from the C-R flow.
In the following method section, we first describe the input and output of our method, then introduce the registration and completion networks separately, which form the proposed CTF-Net. Finally, we describe the loss functions that enable the two flows to reinforce each other.
4 METHOD
4.1 Input and output
The input of our method is a pair of partial point clouds, denoted as P1 and P2, with little overlap extracted from the same shape S. The point cloud pairs are centered at the origin point and randomly rotated in 3D. We achieve the goal

PointNet Encoder

Multiply

4 Concat
Multiply

T-Net T-Net

Orientation

Inverse

Generation

Fig. 3: The architecture of the completion network. For each partial shape P, the orientation module first predicts the parameter Ro that rotates P into the canonical view and obtains Po. The following generation module predicts the missing part Go relative to Po by an encoder-decoder pair, and the completed shape Po  Go is then rotated back to the original pose, by multiplying the inverse of Ro, to obtain the final output S = P  G.

of tele-registering the point cloud pairs and complete the missing part by making use of our key idea to combine the registration and completion tasks in a way that reinforces each other. Our outputs include the completion of those two parts in their original states denoted respectively as S1 and S2, and the relative transformation between them with M21 denoting the transformation that registers P2  P1 and M12 denoting the transformation that registers P1  P2.
CTF-Net consists of two flows: C-R flow and R-C flow. Since each flow provides a set of outputs, we use superscripts to distinguish the two sets of outputs.
In the C-R flow, P1 and P2 are first passed through the completion network to get the completion results S1C-R = C(P1) and S2C-R = C(P2). Subsequently, the two completions are passed to the registration network to output their relative transformation MC12-R and MC21-R . In the RC flow, P1 and P2, which can come in arbitrary poses, are first passed through the registration network to get their relative transformations MR12-C and MR21-C. The aligned parts are first combined, before passing to the completion network to get the final results S1R-C = C(P1  MR21-CP2) and S2R-C = C(P2  MR12-CP1).
Our goal is to make sure that the completion and registration outputs of both flows are close to the ground-truth, denoted as S1, S2, M1 and M2, and more importantly, making the two flows mutually consistent.
4.2 Network Architecture
4.2.1 Completion Network
The completion network, as shown in Fig. 3, takes a partial shape P as input and outputs the completed shape S = C(P). Note that similar to [29], for the given partial shape P, we only generate the missing part G and thus the final completed shape is a union of those two S = P  G. Moreover, different from most of the previous works on completion which assume that the input shapes are all wellaligned, our input partial shape P are given in an arbitrary orientation. Therefore, our completion network is composed of two modules: one orientation module and one generation module, where the orientation module rotates P into a canonical pose to facilitate the following generation module and then the generation module generates the missing part to complete the shape.
Specifically, the orientation module takes the partial shape P as input, and outputs the rotation transformation Ro, which is then applied to P to obtain the oriented

shape Po. The generation module is adapted from the PFNet proposed in [29]. The original PF-Net assumes that the input partial shape covers a large portion of the whole shape, thus the number of the generated points is far less than the input. However, in our work, since the input is usually a much smaller part of the shape, we set the output point number to equal to the input point number. Note that directly modify the output points of the original PFNet will highly increase the number of network parameters and lead to huge memory cost and slow training speed, so we modify the parameters of each layer to enlarge the output size gradually. In more detail, we first pass Po to a encoder that extracts the feature of dimension nc = 1920, which is then passed to a decoder that generates the missing part in three levels: primary Gp, secondary Gs and the final detailed output Go, with the points number of 128, 512, 2048, respectively. Go is then concatenated with Po to form the complete shape. Finally, we multiply Go  Po with the inverse of Ro to obtain the final complete shape S = C(P) = G  P.
4.2.2 Registration Network
The registration network, as shown in Fig. 4, takes two shapes, P1 and P2, either complete or partial, and outputs the relative transformation M12 from P1  P2 by taking P2 as the anchor. We decompose the transformation M12 into rotation R12 and translation T12 to reduce the complexity of 3D transformation, i.e., the network first rotates P1 to make it have the same pose of P2, and then translates it to align with P2.
Specifically, we first pass the input pair through a PointNet [25] encoder, to obtain the feature vector of dimension nr = 512 for P1 and P2, respectively. The feature vectors are concatenated and passed to a decoder composed of several linear layers, which provides the quaternion parameters that represent the 3D rotation. The quaternion is then converted to rotation matrix R12 and multiplied with P1 to obtain the rotated part P1R. Note that all the quaternions are normalized in each process. Afterwards, P1R is passed to the PointNet encoder again to obtain the feature vector of dimension Rnr (nr = 512). The feature is then concatenated again with that extracted from P2 and passed to another set of linear layers to output the set of translation parameters (dx, dy, dz), which are then converted to the translation matrix T12 and multiplied with P1R to obtain the final

T-Net T-Net

PointNet Encoder

Concat

Multiply

PointNet Encoder

5
Multiply
Concat

T-Net T-Net

T-Net T-Net

Concat

Fig. 4: The architecture of the registration network. Taking two paired shapes S1 and S2 and considering S2 as the anchor, the registration network first outputs parameters of a relative rotation R12 from S1 to S2, then, a relative translation T12 from R12S1 to S2 is predicted. The complete transformation is then denoted as M12 = T12R12.

registered part. The final transformation matrix is calculated by M12 = T12  R12.
In both flows, our registration network also takes those two shapes with the other order and output the relative transformation M21 from P2 to P1. Furthermore, in the C-R flow, the registration network takes two completed shapes S1 and S2 as input. We further apply the transformation on the moving shape, say S1, and combine it with the anchor shape S2 to get the registered full shapes. Note that since the points size of the registered point cloud is twice that of each input partial point cloud, we down-sample it to half of its original points size in order to keep all the point clouds in the flows to have the same size.
4.3 Loss Functions
To train this two-flow network, we design suitable loss functions to account for each type of output and also the consistency between two flows. We define the loss function of our CTF-network as:

(EMD) [30] computed for all three generated levels(Gp, Gs, Go), and Dr is the distance measure between two rotations. In more detail, the rotation matrix is converted to a quaternion Q = q(R). Since a quaternion Q is equivalent to its minus -Q when representing a rotational transformation, we measure the distance as follows :
Dr(R1, R2) = Dq(q(R1), q(R2)), Dq(Q1, Q2) = min(norm(Q1 - Q2), norm(Q1 + Q2)).
(3)
Note that for the same part, for example P1, the ground truth missing part is different in two flows. In the R-C flow, P1 will first be registered and combined with P2 before completion, so the missing part would be smaller than the one in C-R flow. All the ground truth missing parts are extracted and subsampled from the corresponding ground truth complete shape S.

L = Lc + Lr + Ls.

(1)

where Lc and Lr are the completion and registration loss against the ground-truth, and Ls is the loss to ensure the consistency between two flows.

4.3.1 Completion loss.

Note that our completion network works for input parts with arbitrary orientation, and we rotate the parts into the canonical view first and then generate the missing part to complete the shape. Therefore, the completion loss is computed for outputs from the R-C and C-R flows against the respective ground truth missing part geometry and rotation transformation as Lc = cc-rLCc -R + oc-rLCo -R + cr-cLRc -C + or-cLRo -C with

LCc -R = Demd(G1C-R, G1C-R) + Demd(G2C-R, G2C-R) /2,

LRc -C = Demd(G1R-C, G1R-C) + Demd(G2R-C, G2R-C) /2,

LCo -R = Dr(RC1o-R, R1o) + Dr(RC2o-R, R2o) /2,

LRo-C = Dr(RR1o-C, R1o) + Dr(RR2o-C, R2o) /2,

(2)

where the weights cc-r, oc-r, cr-c, or-c are set as 1, 3, 0.5, and 1.5, respectively. Demd is the distance measure between the generated part and the corresponding ground truth, which is defined as the mean of the earth mover's distance

4.3.2 Registration loss.
The registration loss is also computed for outputs from the C-R and R-C flows against the respective ground truth transformations as Lr = rc-rLCr -R + rr-cLRr -C with
LCr -R = Dm(MC12-R, M12) + Dm(MC21-R, M21) /2, LRr -C = Dm(MR12-C, M12) + Dm(MR21-C, M21) /2. (4)
where Dm is the distance measure between two transformations, which is defined as the Dq between their quaternions plus the mean square error between the translations in the x, z, y axis. The weights rc-r and rr-c are set as 3 and 9, respectively.

4.3.3 Consistency loss. The consistency loss consists of four components:

Ls = soLOs + scLCs + srLRs + stLTs .

(5)

where LOs , LCs and LRs are the consistency loss defined on the orientation correction, completion and registration in those two flows, while LTs is the consistency loss defined on the

6 Plane Car Watercraft Couch Cabinet Table Chair Lamp

Fig. 5: The prediction results of CTF-Net. The first two columns show the input pairs (colored in blue and orange). The third and fourth columns show the registration results from C-R flow, where the corresponding original mesh is shown in transparent gray to better display the relative position of the registered parts. The fifth and sixth columns show the completion results from R-C flow, and the last two columns are the ground truth point clouds.

relative transformations between two shapes with either one as the anchor. In more detail:
LOs = Dr(RC1o-R, RR1o-C) + Dr(RC2o-R, RR2o-C) /2,
LCs = Demd(S1C-R, S1R-C) + Demd(S2C-R, S2R-C) /2,
LRs = Dr(MC12-R, MR12-C) + Dr(MC21-R, MR21-C) /2,
LTs = Dr(MC12-RMC21-R, I) + Dr(MR12-CMR21-C, I) /2. (6)
where I is the 4 × 4 identity matrix. The weights so, sc, sr and st are set as to 3, 1, 3 and 3, respectively by default. For the details about how we choose this set of weights, please refer to the supplementary material.
5 RESULTS AND EVALUATION
We first introduce the details of our training data generation in Section 5.1, then, we show results of pair registration and partial completion obtained with our CTF-Net and present a set of qualitative results to demonstrate the capabilities of our method in Section 5.2. A quantitative evaluation and ablation studies are described in Sections 5.3 and 5.4. We present results on both synthetic and real data.

5.1 Data preparation
Our dataset contains 31, 742 shapes in eight categories from ShapeNet v2 [31]. The training, validating and testing split is similar to the dataset of Tchapmi et al. [27]. Each shape in our dataset is represented as a point cloud which contains 16, 384 points, and all the shapes are normalized into a unit cube centered at the origin.
For each shape S in the dataset, we crop two parts from it and apply random transformations to get the training pairs (P1, P2). Specifically, we first randomly generate two spheres centered at the surface of the bounding sphere of each shape, with the radius sampled from [0.3, 1.3], and then use these two spheres to crop two parts out of the shape. We use this method to simulate the real scan cases when a camera is held and scan around the shape. Note that we set a minimum distance (0.3 in our setting) between each pair of cropped parts to avoid too much overlap between those two parts, where the distance is calculated by the Euclidean distance between the centers of those two parts. We also ensure that each cropped part and the remaining part both have more than N = 4096 points, so that they can be down-sampled to N = 2048 points. Each part is first

7
Plane Car
Watercraft Table Couch

Cabinet

Chair

Lamp
Fig. 6: The prediction results on real scans. The first column shows the photo of each real object. The next two columns show the paired partial inputs. The fourth and fifth columns are the registered results, which take each part as an anchor. The fifth and sixth columns show the completion results. The complete fused shape from a much denser RGB-D sequences are shown in the last column for comparison.

translated to the origin and then rotated about its center. The rotation and translation matrix for pairs of parts are
denoted as R1, T1, R2 and T2 respectively, and the complete transformations are then denoted as M1 = R1T1 and M2 = R2T2. Therefore, the corresponding ground truth for completed shapes are S1 = M1S and S2 = M2S, and the corresponding ground truth for relative transformations between those two parts are M21 = M1M-2 1 and M12 = M2M-1 1. Note that in the following experiments, CTF-Net is trained individually for each of those eight categories. For a fair comparison, all the compared methods are trained category-by-category.
5.2 Qualitative results
We show results for synthetic shapes on eight categories. To verify the generality of our method, we also test our method on real scans.
5.2.1 Results on synthetic data
Fig. 5 shows visual examples of the registration and completion results. The input pair of parts are centered at the

origin point, as shown in the first two columns. By taking either part as the anchor, our method is able to transfer the other part to make it align with the anchor well, as shown in the third and fourth columns respectively. We can see that even when there is little overlap between the input parts, like most of the examples in the results, CTF-Net can still register them correctly. Also, the union shapes after registration with different anchor parts are quite similar thanks to the effect of LTs . The fifth and sixth columns show the completed shape for each part, and the last two columns are the corresponding ground truth complete shape. We can see that the completion module in our CTF-Net can work well for parts given in two different orientations, and by solving registration and completion together, the parts do not need to be pre-aligned before completion. Note that we take registration results from the C-R flow and completion results from the R-C flow, mainly because each will improve the performance for the other, and the end output of each flow is more reliable even with the consistency loss.

8

Fig. 7: The prediction results on RedWood (upper 4 rows) and Pix3D (lower 4 rows) datasets. The first column shows the photo of each real model, and the next two columns show the paired partial inputs. The fourth and fifth columns are the registered results with either part as the anchor, and the completion results are shown in the following two columns. The last column shows the corresponding reconstructed meshes provided by these two datasets.

5.2.2 Results on real data
To demonstrate the generality of our method, we use 3D scanners to manually scan several objects from all those eight categories for testing. Specifically, for small objects with detailed textures, in particular, Plane, Car, Watercraft, and Lamp, we place each object on a turnplate and use an Artec Spider scanner for scanning which directly produces the reconstructed model; for other larger objects, we use a Microsoft Kinect v2 scanner to do the scanning, then utilize bundle fusion [32] to obtain the reconstructed model. The reconstruction results are shown in the last column in Fig. 6, denoted as F .
To generate the input pairs, we first normalize the reconstructed full model into a unit box and put it in a

virtual environment, and then randomly place two cameras pointing at the model to capture RGB-D images. Two partial point clouds are then obtained by back-projecting the depth images to the 3D reconstructed model, and those two singleview point clouds are taken as input to test our CTF-Net. Note that here we do not use the single-view point cloud directly from the scanner since there exist some affections such as reflect light which could make the single-view point cloud extremely sparse.
Fig. 6 shows the results on our real scanned data. Note that the input partial parts can still be correctly aligned even the point clouds are noisy and non-uniformly distributed. For example, although the two parts of the couch shown on the fifth row are quite noisy and covers different region of

9

TABLE 1: Quantitative results on all eight categories. The registration error is quantified using E and Et while the completion error is quantified using Ecg and Ecf . More details about the measures are specified in section 5.3
.

E
Et Eegmd Eefmd

Plane 9.582 3.927 3.573 1.603

Cabinet 18.405 5.605 5.792 2.695

Car 8.172 1.606 3.041 1.457

Chair 15.114 4.144 6.274 2.729

Lamp 26.126 13.925 11.573 3.765

Couch 11.625 4.045 5.142 2.322

Table 15.486 7.399 5.725 2.491

Watercraft 16.434 7.105 5.580 2.126

Average 15.118 5.970 5.838 2.399

the couch, our method is still able to successfully align them and complete the missing region in the front.
To further justify the generality of our method, we also tested our method on RedWood [33] and Pix3D [34] datasets, both of which provided the full reconstructed model. Similar to our real scanned data, the input pairs of these two datasets are generated by single-view scanning. Some example results are shown in Fig. 7. For each example, the full reconstructed model is shown in the last column, denote as F , for comparison. We see that although the single-view input data is quite different from the synthetic training data, the predicted registration is still quite accurate and the completion results are also quite reasonable. For example, in the fourth row, our method predicts the missing leg of the table, and in the fifth row, we can notice that the entire chair back is successfully reconstructed. Note that for each real scan example, we slightly rotated the view during visualization to show the single-view inputs more clearly.

5.3 Quantitative evaluation

We perform quantitative evaluation of both registration and completion networks by measuring the errors of predicted transformation parameters and reconstructed point clouds.
For registration, we calculate the error for the final output of the C-R flow. Rotational error E is calculated as the absolute error between the predicted and ground truth angle in degrees, and Translation error Et is calculated by the L2 distance between the predicted and ground truth translation (in normalized units):

E = |(RC12-R, RC12-R)| + |(RC21-R, RC21-R)| /2,

(7)

Et = |t(T1C2-R, T1C2-R)| + |t(T2C1-R, T2C1-R)| /2 × 103. (8)

where  denotes the angle derived from the relative quaternion from the predicted quaternion RC-R to the ground truth quaternion RC-R, and t denotes the distance between the
translation decomposed from the predicted transformation T C-R and the ground truth transformation T C-R.
For completion, we also calculate the error for the com-
pletion obtained from the C-R flow. Since our completion
network generates the missing region without modifying the given part, we use two measures Eegmd and Eefmd to compute the errors in the generated region and full shape, respectively, where Eegmd is calculated as the EMD between the reconstructed and corresponding ground truth point cloud of the missing region, and Eefmd is calculated as the EMD between the reconstructed shape concatenate with the
input part and ground truth full shape:

Eegmd = Demd(G1C-R, G1C-R) + Demd(G2C-R, G2C-R) /2 × 103, (9)

TABLE 2: Quantitative comparisons of different registration methods.
4PCS PRNet DCP BL-Regi Regi Ours E 138.910 25.796 17.767 21.336 17.912 15.118 Et 62.456 28.188 22.317 17.305 6.927 5.970
Eefmd = Demd(S1C-R, S1C-R) + Demd(S2C-R, S2C-R) /2 × 103. (10)
Table 1 shows the errors of all eight categories. Note that Et, Eegmd and Eefmd values have been scaled by 103 to amplify the error. In most categories, our CTF-Net is able to predict accurate registration together with completion. We observe that the registration error of Lamp is significant higher than other categories due to the ambiguity arising from its strong symmetry. For example, the stands of most lamps are cylindrical, which could lead to the rotational ambiguity. Furthermore, the variety and complexity of Lamp category are very large, e.g., a lamp with multi-fold stand, or a pendant lamp with cluttered accessories. The rotational error of other categories are all lower than 20 degrees; meanwhile, the full shape completion errors of these categories are lower than 3, meaning that the good reconstruction quality is achieved.
Overall, we achieve an average of 15.12 degrees rotational and 5.97 translation error in registration, and 2.40 completion error for the whole shape.
5.3.1 Comparison on registration
We compare the registration results of our method to four other options:
1) A classic registration method 4PCS [4], which can globally register complete or partial point sets;
2) A deep-learning based registration method PRNet [24], which focuses on partial-to-partial registration, with self-supervised learning;
3) A deep-learning based registration method DCP [10], can be seen as state-of-the-art;
4) Baseline registration network which predicts the rotation and translation parameters at once from a single decoder, denoted as BL-Regi. The detailed network structure can be found in the supplementary material;
5) Our registration network alone, denoted as Regi.
The geometry-based method 4PCS is directly tested on our dataset, while the remaining three learning-based methods are trained/tested on the same dataset as ours. The comparison of prediction error is reported in Table 2. We can see that the prediction error of 4PCS is the highest since it assumes that there are certain-level overlap between

10

Fig. 8: Visual comparisons of our CTF-Net to other registration methods on different overlap data. The input pair of partial parts is shown in the first row. To better illustrate the overlap region, we show the input pair in the second row after rotating each part into a canonical view, and mark the overlap region with a lighter color. Next four rows show the registration results provided by 4PCS, DCP, PRNet and ours respectively. Here we only show the registration results that takes the blue part as the anchor.

Fig. 9: Quantitative comparisons of the rotation and translation errors of our CTF-Net on different overlapping data. In the left chart, we crop the error value larger than 50 degrees to show the difference between 4PCS, PRNet, DCP and our method more clearly.
the input pair, while our data are mainly non-overlap. The errors obtained using PRNet are lower than 4PCS but higher than others, meaning that PRNet can not works well on our data with little overlap between the input pairs. DCP performs better comparing to our BL-Regi method in rotational error, however, the translation error of BL-Regi is 23% lower. Comparing DCP to our single registration module Regi, we can see that the rotational error is quite close, but the translation error of Regi is 68% lower. The key idea of DCP is to find the correspondences between two point sets, which also fails on our dataset. The baseline registration method, which predicts the rotation and translation parameters simultaneously from the same decoder, has slightly higher rotation error to our registration network, however, the translation error is more than twice as much as ours. This proves that splitting the prediction of rotation and translation could achieve better performance. Last but

not least, our method with two complete flows gets the best results comparing to all five other options, which shows the benefit of combining the registration and completion tasks.
In order to assess the advantage of our method on nonoverlap data, we further compare our method with 4PCS, PRNet and DCP on the data with different levels of overlap. To generate testing pairs with a certain overlap , we modify our data preparation procedure slightly. In more detail, for each shape in our testing set, we randomly generate two spheres with fixed centers (0, 0.75, 0) and (0, -0.75, 0), respectively, and the radius randomly sampled from [0.3, 1.3] to crop two parts from a complete shape. We then calculate the IoU of the cropped parts and keep the ones with IoU in [0.9, 1.1]. Finally, we downsample each part to N = 2048 points.
We take the Plane category for testing, and quantitative comparisons of the rotation and translation errors are shown in Fig. 9. We can see that both the rotation and translation errors of almost all the methods keep increasing as the overlap region decreases. Regards to rotation error, 4PCS performs best when the overlap region is 80%, however, as the overlap decreases, the performance of 4PCS dropped significantly and the error is lager than 50 degrees from 40% overlap to 0%.
PRNet and DCP performs more stable than 4PCS, and the rotational error of DCP is slightly lower than ours at 80% and 60% overlap, but CTF-Net keeps the lowest error from 40% to 0%. For translation error, the four methods are quite similar at 80% overlap, however, the error of 4PCS increases rapidly when overlap region decreases. The error of DCP is slightly lower than PRNet but significantly higher than ours at 20% and 0%. PRNet is supposed to be able to deal with

11

Fig. 10: Visual comparisons of results obtained by different completion methods. Note that all the results are rotated to canonical view to facilitate the visual comparison.

TABLE 3: Quantitative comparisons of different completion methods.

Eegmd Eefmd Ecgd Ecfd

TopNet -
10.003 -
2.735

MSN -
3.536 -
2.274

PF-Net 10.560 4.919 7.245 2.187

BL-Comp 8.330 3.811 10.290 3.460

Comp 6.937 3.172 8.183 2.959

Ours 5.838 2.399 7.339 2.466

a partial-to-partial registration, however, it shows worse performance than DCP in our setting. The main reason is that PRNet assumes a large overlap between the input pair data and requires a good initial alignment for the ensuing iteration, while for our input data with quite little overlap, they fail to find a sufficiently good initial alignment.
We observe that the translation error of our method decreases a bit even the overlap region drop from 80% to 20%, implying that our network has higher capability in learning data with little overlap.
Fig. 8 shows the visual comparison of our method to 4PCS, PRNet, and DCP. To highlight the differences in the overlap regions, we rotate all the input pairs and align their first part (shown in blue), and use a lighter color to show the overlap region between the input pair, shown in the second row. From the results, we can see that 4PCS fails at 20% and 0% overlap, since the algorithm assumes that two parts should have a large overlapped region. For the deeplearning based methods PRNet and DCP, we observe that the rotational error of PRNet is large at and 60%, 20% and 0% overlap. DCP produces more accurate rotation angle, however, the translation error is still high. Our method performs stable on different overlap data. We observe that even there is no overlap between the head and tail of the plane, our method is still able to align two parts in the correct position, with a certain margin in the middle. This experiment also shows that our method are robust when the area of the input data variants, which is helpful for the use of real scanned data.
5.3.2 Comparison on completion
We compare the completion results of our method to four other options:
1) TopNet [27], which direct reconstruct the whole shape using a structural decoder;
2) MSN [30], which predicts the whole shape in a coarse to fine manner;

3) The original PF-Net [29], which can be seen as state-ofthe-art;
4) Our completion network alone without orientation module, denoted as BL-Comp;
5) Our completion network alone, denoted as Comp;
All the four methods are trained/tested on the same dataset as ours, TopNet and PF-Net are trained using chamfer distance (CD) as proposed in their paper, and the remaining methods are trained using EMD. For fair comparison, we add another two completion quality measures based on CD instead of EMD, denoted as Ecgd and Ecfd, by substituting Demd by Dcd in Equation 9 and Equation 10. Specifically,
Ecgd = Dcd(G1C-R, G1C-R) + Dcd(G2C-R, G2C-R) /2 × 104,
(11)
Ecfd = Dcd(S1C-R, S1C-R) + Dcd(S2C-R, S2C-R) /2 × 104.
(12)
Note that Ecgd and Ecfd values have been scaled by 104 to amplify the error. The reconstruction error is reported in Table 3. TopNet and MSN predict the whole shape directly, thus we only compute Eefmd and Ecfd for them. For all other methods, only points on the missing regions are generated, so Eegmd and Ecgd are also computed. We can see that TopNet gets the highest error Eefmd since it doesn't keep the points from the input part and it is trained using CD. For comparison, MSN also predicts the whole shape, but it is trained using EMD, so both Eefmd and Ecfd are lower than TopNet. PF-Net achieves the lowest error in Ecgd and Ecfd, since it keeps the input region and is trained using CD. The main difference between BL-Comp and PF-Net is that BLComp is trained using EMD measure, thus both Eegmd and Eefmd are lower than PF-Net. To further improve the results on input part pairs with randomly 3D rotation, we added an orientation module to BL-Comp, denoted as Comp, and all the errors get lower comparing to BL-Comp. Our method obtains the best result in EMD measure, with 76%, 32%, and 51% drop on Eefmd comparing to TopNet, MSN and PF-Net, thanks to the consistent two flow network.
Fig. 10 shows two examples of different completion methods. We see that the prediction results of TopNet are quite blurred and lack fine details. Similar results can be seen in the fourth column, i.e., PF-Net keeps the original input but the predicted parts are sparse, leading to the inconsistent results. These two methods are trained using

12

30

E

25

Et Eegmd

Eef md

20

15

10

5

0 0.005

0.03

0.06Noise level0.09

0.12

0.15

Fig. 11: Quantitative evalution of the stress test with the increase of noise level added to the input parts.

Fig. 12: Visual examples of the stress test. Given a pair of input parts with different noise level (first two columns), the registration (third column) and completion (last column) results are presented.
CD measure, which is not able to effectively constrain the sparseness of the point cloud. The results of MSN are obviously more compact, however, we can see that there are some outlier points distributed in the blank region. For example, there are some unnecessary points located between the back and the seat region in the second row. Our baseline completion network performs better than PFNet, but incurs some distortions in the output. The completion method, which includes the orientation module to the baseline completion, predicts points more accurately. Our method achieves the best results comparing to others, and the generated points are evenly distributed in the missing regions. Note that here we take the completion results from C-R flow to compare with the other methods.

Fig. 13: Visual example of the input point cloud with outliers. We show original scan, scan with 100 outliers, scan with 100 outliers processed by a filter, respectively.
5.3.3 Stress test
To better simulate the scenario of real scanning, we generate the data with different noise level to test the robustness of our CTF-Net. Specifically, to generate the data with noise level , we randomly sample N × 3 values range in [0, ] as the noise, then add the sampled noise to the original input partial pair P1 and P2. We set five different  to test our method, and the results are shown in Fig. 11. Note that our original data generated by virtual scanning have the noise level at 0.005. We observe that all the errors increase quite slightly as the noise level increases from 0.005 to 0.09, i.e., E, Et, Eegmd, and Eefmd keep lower than 20, 10, 8 and 5 respectively. At noise level of 0.12 and 0.15, which rarely appear in real scanning, the errors increasing slightly faster, but the rotational error keeps lower than 27 degree. The results show that our method is able to maintain low prediction error given noisy data.
Fig. 12 shows how the registration and completion errors change with the increase of noise level added to the input parts. The registration of our method is correct even the input partial data are quite noisy, as shown in the examples in the last row. Besides, the missing parts generated by our method are recognizable at all noise levels.
Other than points with random and small noise, outliers can also exist in the captured point cloud data, thus we perform another experiment to show how outliers affect the performance of our CTF-Net. Specifically, as shown in Fig. 13, for each testing point cloud data P, we randomly select K points and each of them is then translated by a random displacement to simulate the outliers. The direction of each displacement is uniformly sampled in SE(3) space, and the length of each displacement is uniformly sampled between [0.1, 0.5]. In this experiment, we set K to be 50 and 100, respectively, to test our method. We denote the processed data as Pol.
Furthermore, since automatic outlier removal algorithms are widely used in many works related to point cloud processing, we also test our CTF-Net on the outlier data after being processed by an outlier filter. Specifically, we utilize the radius outlier removal method from Zhou et al. [35], which removes points that have few neighbors in a given sphere around them, and we use the default parameters setting proposed in [35]. The processed data are denoted as Pol with filter.
The results are reported in Fig. 14. We observe that the errors are relatively high when the outliers are added to the input scans without filtering, especially for Et. Such amount of random outliers will confuse the network when distinguishing the part poses, thus lead to high errors. However, when we apply a filter to the data contaminated

40

E

35

Et

30

Eegmd Eef md

25

20

15

10

5

00

50 with filter

Outl1i0e0rwitnhufiltmer ber

50

100

Fig. 14: Quantitative evaluation of the stress test with an increasing number of outliers added to the input scans with and without filtering.

with outliers, the errors remain very close compared to those of the results on clean data, meaning that our method is able to perform well on the data cleaned up by a simple and automatic outlier removal filter.
5.4 Ablation studies
To justify the network structure and loss functions designed in our method, we perform several ablation studies for our CTF-Net. For network structure, we take single R-C flow and single C-R flow to perform the comparison, the results are shown in Table 4. Since both R-C flow and C-R flow can produce the transformation parameters and completed shapes, we use the same measure as described in Section 5.3. Since our CTF-Net contains twice parameters comparing to each single flow, for fair comparison, we slightly modify R-C flow and C-R flow to make them have comparable parameters to ours, denoted as R-C1 and C-R1. Specifically, we double the output dimension of each layer (except for the last layer) of all the decoders, and then obtain the network with almost twice parameters as the original single flow network. In addition, the registration network in our CTF-Net is trained with both partial point clouds and (predicted) complete point clouds and the completion network in our CTF-Net is trained with both partial input point clouds and (predicted) registered point clouds, so we compare our method to each single flow which trained with augmented data. Specifically, for single R-C flow, we train the completion network with both input partial point cloud and combined point cloud after registration, denoted as R-C2, while for single C-R flow, we train the registration network with both input partial point cloud and predicted complete point cloud, denoted as C-R2.
From the results, we can see that for single R-C flow, the errors of both rotational and translation is significantly higher than our method, which shows that the direct registration of two non-overlap partial shapes is quite challenging. For the version with double parameters R-C1, the errors are quite similar to R-C flow. For R-C2, we observe that training the completion network with additional input can improve the results of completion comparing to single R-C flow, but the errors are still higher than our method. The errors of single C-R flow are also higher than ours, the registration error of C-R1 is lower than C-R, but the completion error increases. For C-R2, the error of registration is

13
TABLE 4: Ablation studies in which we compare our CTFNet to versions where we remove either flow of the CTFNet.

E
Et Eegmd Eefmd

R-C 19.138 7.919 7.495 3.640

R-C1 18.777 8.187 7.230 3.594

R-C2 18.844 8.124 7.041 3.478

C-R 18.383 7.296 7.421 3.652

C-R1 17.565 6.349 7.633 3.952

C-R2 16.974 6.033 7.587 3.869

Ours 15.118 5.970 5.838 2.399

TABLE 5: Ablation studies in which we compare our CTFNet to versions where we remove selected terms of the loss function.

E
Et Eegmd Eefmd

w/o LOs 17.038 5.995 6.965 3.180

w/o LCs 18.767 6.864 7.928 3.876

w/o LRs 19.014 6.053 6.824 3.044

w/o LTs 16.345 6.290 6.393 2.561

Ours 15.118 5.970 5.838 2.399

lower than C-R and C-R1, however, the errors are higher than CTF-Net. The experiments show that the performance of either single flow is not comparable to our CTF-Net, even with the doubled parameters, and only the combination of two flows with consistency loss can actually improve the results.
We design four consistency losses after combining the R-C flow and C-R flow together: LOs , LCs , LRs , LTs . To show the effectiveness of our consistency losses, we perform comparisons of our method with and without each loss term, as shown in Table 5. By comparing the errors of the last five columns, we validate that the consistency losses are able to make the two flows strengthen each other, which leads to the lowest error. We can see that both registration and completion errors are higher without the reconstruction consistency loss. Removing the parameter consistency loss results in the registration between two parts being less consistent, and thus leads to larger errors in prediction.

6 DISCUSSION AND FUTURE WORK
We presented CTF-Net for tele-registration of two partial point clouds. Our method excels where the surfaces represented by point clouds have little or no overlap. The success of our method is attributed to the competence of neural network to learn a prior of a class of shapes, which allows predicting complete shapes from partial observations and registering non-overlapping parts. The key architectural design comes in the form of consistency between the registerand-complete and the complete-and-register networks.
The consistency of the two network flows encourages the two networks to predict reliably and performs surprisingly well. Given that parts P1 and P2 are disjoint, then their completions S1 and S2 should agree without conditioning. We expect S1 to trivially agree with P1 in the overlap regions, but the rest of S1 is ambiguous. Here, we also expect S1 to agree with P2 in the overlapping region, which is a hard task. Thus, the completion task by itself is ambiguous, while the registration with no overlap is ill-posed. Solving both tasks jointly and consistently, however, makes the problems more well-posed and less ambiguous.

14
ACKNOWLEDGMENT
This work was supported by NSFC (U2001206, 61872250), GD Talent Program (2019JC05X328), GD Natural Science Foundation (2021B1515020085, 2020A0505100064), DEGP Key Project (2018KZDXM058), Shenzhen Science and Technology Program (RCJC20200714114435012), Royal Society (NAF-R1-180099), ISF (2472/17, 2492/20), and GD Laboratory of Artificial Intelligence and Digital Economy (SZ).

Fig. 15: Failure cases of registration prediction with CTFNet. The upper row contains a table where two parts with some freedom on the degree of overlap, and the lower one contains a chair where the symmetry of one part can cause misplacement of the other part.
Limitations of data-driven methods are naturally carried over to our method. The range by which our method works well is directly derived by the training data and the capacity of the network, which includes the variety of the expected input geometries and their mutual pre-disposition. As shown in Fig. 15, our CTF-Net may fail in registration prediction if the input pairs are sufficiently ambiguous. The upper row shows a table example, where the matching parts have very similar geometry and the ground truth degree of overlap between two input parts are ambiguous, which can cause different registration results. If we assume that the overlap region is larger, the two parts will be placed closer to each other as the result shown in the figure. For the chair shown in the second row, since the complete version of P1 that captures the leg and seat information can be symmetric, so the back part (complete version) of P2 can be put on any side to make it a valid chair, which introduce the ambiguity of registration between P1 and P2. As shown in the figure, the back in placed on the right side instead of back side of the seat. Another limitation is that our dataset is generated by sphere cropping, since the ground truth for each network is obtained in this way. However, a more natural way to create training data might be by back-projecting depth images to 3D. Also, currently, our method is trained category-by-category, which may limit the generality. We are planning to enable cross-category training by improving the network structures and loss functions in the future. Further, our current implementation considers the coordinates of the point cloud, and ignores the additional attributes that can be associated with scanned data, like normals and colors. We leave this for future research.
Another direction is to leverage the tele-registration of our method to align the parts from different objects. For example, given a chair without legs and a partial cabinet, one may register them to form a new object that have both the functionality of chair and cabinet. This part-based modeling requires well designed dataset and network.
In the context of continuous scanning (e.g., using Kinect Fusion), data fusion works better in short sequences than on longer ones, where tracking and registration errors accumulate. In our tests, we found CTF-Net to complement scanning performance by successfully stitching shorter bursts of fused results. This emphasizes the importance of teleregistration for disjoint scans in the context of autonomous (or semi-autonomous) robots and drones.

REFERENCES
[1] M. A. Fischler and R. C. Bolles, "Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography," Communications of the ACM, vol. 24, no. 6, pp. 381­395, 1981.
[2] C.-S. Chen, Y.-P. Hung, and J.-B. Cheng, "RANSAC-based DARCES: A new approach to fast automatic registration of partially overlapping range images," IEEE Trans. Pattern Analysis & Machine Intelligence, vol. 21, no. 11, pp. 1229­1234, 1999.
[3] S. Rusinkiewicz and M. Levoy, "Efficient variants of the ICP algorithm," in Proc. Int. Conf. on 3D Digital Imaging and Modeling, 2001, pp. 145­152.
[4] D. Aiger, N. J. Mitra, and D. Cohen-Or, "4-points congruent sets for robust pairwise surface registration," ACM Trans. on Graphics (Proc. of SIGGRAPH), vol. 27, no. 3, pp. 85:1­85:10, 2008.
[5] A. Segal, D. Haehnel, and S. Thrun, "Generalized-ICP," in Proc. of Robotics: science and systems, 2009.
[6] H. Huang, K. Yin, M. Gong, D. Lischinski, D. Cohen-Or, U. Ascher, and B. Chen, "Mind the gap: Tele-registration for structure-driven image completion," ACM Trans. on Graphics (Proc. of SIGGRAPH Asia), vol. 32, pp. 174:1­174:10, 2013.
[7] H. Huang, M. Gong, D. Cohen-Or, Y. Ouyang, F. Tan, and H. Zhang, "Field-guided registration for feature-conforming shape composition," ACM Trans. on Graphics (Proc. of SIGGRAPH Asia), vol. 31, pp. 171:1­171:11, 2012.
[8] P. Besl and N. D. McKay, "A method for registration of 3D shapes," IEEE Trans. Pattern Analysis & Machine Intelligence, vol. 14, no. 2, pp. 239­256, 1992.
[9] S. Bouaziz, A. Tagliasacchi, and M. Pauly, "Sparse iterative closest point," in Computer Graphics Forum (Proc. Eurographics Symp. on Geometry Processing), 2013, pp. 113­123.
[10] Y. Wang and J. M. Solomon, "Deep closest point: Learning representations for point cloud registration," in Proc. Int. Conf. on Computer Vision, 2019, pp. 3523­3532.
[11] G. K. Tam, Z.-Q. Cheng, Y.-K. Lai, F. C. Langbein, Y. Liu, D. Marshall, R. R. Martin, X.-F. Sun, and P. L. Rosin, "Registration of 3D point clouds and meshes: A survey from rigid to nonrigid," IEEE Trans. Visualization & Computer Graphics, vol. 19, no. 7, pp. 1199­ 1217, 2012.
[12] Z. Yang, J. Z. Pan, L. Luo, X. Zhou, K. Grauman, and Q. Huang, "Extreme relative pose estimation for rgb-d scans via scene completion," in Proc. IEEE Conf. on Computer Vision & Pattern Recognition, 2019, pp. 4531­4540.
[13] Z. Yang, S. Yan, and Q. Huang, "Extreme relative pose network under hybrid representations," in Proc. IEEE Conf. on Computer Vision & Pattern Recognition, 2020, pp. 2455­2464.
[14] S. Chen, L. Nan, R. Xia, J. Zhao, and P. Wonka, "Plade: A planebased descriptor for point cloud registration with small overlap," IEEE Transactions on Geoscience and Remote Sensing, 2019.
[15] E. Brachmann, A. Krull, S. Nowozin, J. Shotton, F. Michel, S. Gumhold, and C. Rother, "Dsac-differentiable ransac for camera localization," in Proc. IEEE Conf. on Computer Vision & Pattern Recognition, 2017, pp. 6684­6692.
[16] H. Su, C. R. Qi, Y. Li, and L. J. Guibas, "Render for CNN: Viewpoint estimation in images using CNNs trained with rendered 3D model views," in Proc. Int. Conf. on Computer Vision, 2015, pp. 2686­ 2694.
[17] R. Hanocka, N. Fish, Z. Wang, R. Giryes, S. Fleishman, and D. Cohen-Or, "ALIGNet: Partial-shape agnostic alignment via unsupervised learning," ACM Trans. on Graphics, vol. 38, no. 1, pp. 1:1­1:14, 2018.
[18] T. Groueix, M. Fisher, V. G. Kim, B. C. Russell, and M. Aubry, "Unsupervised cycle-consistent deformation for shape matching," Computer Graphics Forum (Proc. Eurographics Symp. on Geometry Processing), vol. 38, no. 5, pp. 123­133, 2019.

15

[19] G. Elbaz, T. Avraham, and A. Fischer, "3D point cloud registration for localization using a deep neural network auto-encoder," in Proc. IEEE Conf. on Computer Vision & Pattern Recognition, 2017, pp. 4631­4640.
[20] Z. J. Yew and G. H. Lee, "3DFeat-Net: Weakly supervised local 3D features for point cloud registration," in Proc. Euro. Conf. on Computer Vision, 2018, pp. 630­646.
[21] C. Choy, W. Dong, and V. Koltun, "Deep global registration," in Proc. IEEE Conf. on Computer Vision & Pattern Recognition, 2020, pp. 2514­2523.
[22] Y. Aoki, H. Goforth, R. Arun Srivatsan, and S. Lucey, "PointNetLK: Robust & efficient point cloud registration using pointnet," in Proc. IEEE Conf. on Computer Vision & Pattern Recognition, 2019, pp. 7163­7172.
[23] Z. J. Yew and G. H. Lee, "Rpm-net: Robust point matching using learned features," in Proc. IEEE Conf. on Computer Vision & Pattern Recognition, 2020, pp. 11 824­11 833.
[24] Y. Wang and J. M. Solomon, "PRNet: Self-supervised learning for partial-to-partial registration," in Proc. of Advances in Neural Information Processing Systems, 2019, pp. 8812­8824.
[25] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, "PointNet: Deep learning on point sets for 3D classification and segmentation," in Proc. IEEE Conf. on Computer Vision & Pattern Recognition, 2017, pp. 652­660.
[26] W. Yuan, T. Khot, D. Held, C. Mertz, and M. Hebert, "PCN: Point completion network," in Proc. Int. Conf. on 3D Vision, 2018, pp. 728­737.
[27] L. P. Tchapmi, V. Kosaraju, H. Rezatofighi, I. Reid, and S. Savarese, "TopNet: Structural point cloud decoder," in Proc. IEEE Conf. on Computer Vision & Pattern Recognition, 2019, pp. 383­392.
[28] X. Wang, M. H. Ang Jr, and G. H. Lee, "Cascaded refinement network for point cloud completion," 2020, pp. 787­796.
[29] Z. Huang, Y. Yu, J. Xu, F. Ni, and X. Le, "Pf-net: Point fractal network for 3D point cloud completion," in Proc. IEEE Conf. on Computer Vision & Pattern Recognition, 2020.
[30] M. Liu, L. Sheng, S. Yang, J. Shao, and S.-M. Hu, "Morphing and sampling network for dense point cloud completion," in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 07, 2020, pp. 11 596­11 603.
[31] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su et al., "Shapenet: An information-rich 3d model repository," arXiv preprint, 2015.
[32] A. Dai, M. Nießner, M. Zollo¨ fer, S. Izadi, and C. Theobalt, "Bundlefusion: Real-time globally consistent 3d reconstruction using onthe-fly surface re-integration," Proc. of SIGGRAPH, 2017.
[33] S. Choi, Q.-Y. Zhou, S. Miller, and V. Koltun, "A large dataset of object scans," arXiv preprint, 2016.
[34] X. Sun, J. Wu, X. Zhang, Z. Zhang, C. Zhang, T. Xue, J. B. Tenenbaum, and W. T. Freeman, "Pix3d: Dataset and methods for single-image 3D shape modeling," in Proc. IEEE Conf. on Computer Vision & Pattern Recognition, 2018, pp. 2974­2983.
[35] Q.-Y. Zhou, J. Park, and V. Koltun, "Open3D: A modern library for 3D data processing," arXiv:1801.09847, 2018.

Zimu Yi is a M.Sc. candidate in the Visual Computing Research Center at Shenzhen University. He received his Bachelor's degree in Computer Science from Beihang University in 2019. His research interest lies in computer graphics.
Ruizhen Hu is an Associate Professor at Shenzhen University, China. She received her Ph.D. from the Department of Mathematics, Zhejiang University. Before that, she spent two years visiting Simon Fraser University, Canada. Her research interests are in computer graphics, with a recent focus on applying machine learning to advance the understanding and generative modeling of visual data including 3D shapes and indoor scenes. She is an editorial board member of The Visual Computer and IEEE CG&A.
Niloy J. Mitra leads the Smart Geometry Processing group in the Department of Computer Science at University College London. He received his PhD from Stanford University under the guidance of Leonidas Guibas. His research interests include shape analysis, creativeAI, and computational design and fabrication. Niloy received the Eurographics Outstanding Technical Contributions Award in 2019, the BCS Roger Needham award in 2015, and the ACM Siggraph Significant New Researcher Award in 2013.
Daniel Cohen-Or is a Professor in Computer Science. He received his Ph.D. from the State University of New York at Stony Brook in 1991. He was the recipient of Eurographics Outstanding Technical Contributions Award in 2005, ACM SIGGRAPH Computer Graphics Achievement Award in 2018. In 2013 he received The People's Republic of China Friendship Award. In 2015 he was named a Thomson Reuters Highly Cited Researcher. In 2019 he won Kadar Family Award for Outstanding Research. In 2020 he received Eurographics Distinguished Career Award. His research interests are in Computer Graphics, in particular, synthesis, processing and modeling.

Zihao Yan is a Ph.D. candidate in the Visual Computing Research Center at Shenzhen University. He received his Bachelor's degree in Electrical Engineering from the University of Electronic Science and Technology of China in 2016. His research interests include shape/scene understanding, point cloud analysis, 3D reconstruction.

Hui Huang is a Distinguished TFA Professor of Shenzhen University, where she directs the Visual Computing Research Center. She received her PhD in Applied Math from The University of British Columbia in 2008. Her research interests span on Computer Graphics, Vision and Visualization. She is currently a Senior Member of IEEE/ACM/CSIG, a Distinguished Member of CCF, and is on the editorial board of ACM Trans. on Graphics, IEEE Trans. on Visualization and Computer Graphics, Computers & Graphics.

