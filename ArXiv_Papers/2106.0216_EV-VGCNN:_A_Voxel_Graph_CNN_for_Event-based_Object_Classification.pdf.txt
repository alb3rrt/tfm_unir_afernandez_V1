UNDER REVIEW

1

EV-VGCNN: A Voxel Graph CNN for Event-based Object Classification
Yongjian Deng, Graduate Student Member, IEEE, Hao Chen, Huiying Chen, and Youfu Li, Senior Member, IEEE,

arXiv:2106.00216v1 [cs.CV] 1 Jun 2021

Abstract--Event cameras report sparse intensity changes and hold noticeable advantages of low power consumption, high dynamic range, and high response speed for visual perception and understanding on portable devices. Event-based learning methods have recently achieved massive success on object recognition by integrating events into dense frame-based representations to apply traditional 2D learning algorithms. However, these approaches introduce much redundant information during the sparse-to-dense conversion and necessitate models with heavyweight and large capacities, limiting the potential of event cameras on real-life applications. To address the core problem of balancing accuracy and model complexity for event-based classification models, we (1) construct graph representations for event data to utilize their sparsity nature better and design a lightweight end-to-end graph neural network (EV-VGCNN) for classification; (2) use voxel-wise vertices rather than traditional point-wise methods to incorporate the information from more points; (3) introduce a multi-scale feature relational layer (MFRL) to extract semantic and motion cues from each vertex adaptively concerning its distances to neighbors. Comprehensive experiments show that our approach advances state-of-the-art classification accuracy while achieving nearly 20 times parameter reduction (merely 0.84M parameters).
I. INTRODUCTION
E VENT cameras, which emulate biological retinas to capture visual changes [1], [2], have drawn mounting attention in robotic and computer vision. As illustrated in the left part of Fig. 1, each pixel output from this sensor (called "event") is independent and is triggered only when absolute lightness changes in a pixel's location exceed the contrast threshold. As visualized in the right part of Fig. 1, this novel working mechanism enables their output to be sparse and non-redundant. Consequently, event cameras hold considerable superiority on low power consumption, high response speed, and high dynamic range compared to their
Yongjian Deng and Youfu Li are with Department of Mechanical Engineering, City University of Hong Kong, Kowloon, Hong Kong SAR. {yongjdeng2-c@my., meyfli@}cityu.edu.hk.
Hao Chen is with School of Computer Science and Engineering, Southeast University, and Key Lab of Computer Network and Information Integration (Southeast University), Ministry of Education, Nanjing 211189, China. haochen593@gmail.com.
Huiying Chen is with the Department of Industrial and Systems Engineering, The Hong Kong Polytechnic University, Hong Kong SAR. velvet.chen@polyu.edu.hk.
© 20XX IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.
The first two authors contribute equally. *: Corresponding author

ln L Threshold
Y (pixels)

events (p=1) events (p=-1)

TT

X (pixels)

Fig. 1. Left: A sketch of the working principle of event cameras (the detailed working principle is introduced in Sec. III-A). Events are produced asynchronously according to the lightness (ln L) changes. Red and blue arrows represent positive and negative events, respectively. Right: The RGB image captured from the traditional RGB camera (top) and event signals in the original format produced from an event camera (bottom). Best viewed in color.

frame-based counterparts (e.g., traditional cameras). These advantages allow mobile and wearable devices to perceive and understand real-life scenarios in real-time [3].
As a fundamental vision task, event-based object classification has been a hot research topic recently. The key problem for event-based object classification is how to tailor a model for the particular format of event data. Motivated by the significant success of deep learning on frame-based vision tasks, developing learning-based algorithms for event data becomes the leading choice. Spiking Neural Networks (SNNs) [4] have initially been adopted to extract semantic features from event data. Although these approaches fit the asynchronous nature of events, the lack of specific hardware supports hinders their potential in real-world applications. Recent works [5]­[10] resort to traditional convolutional neural networks (CNNs) by converting sparse and non-redundant events to dense frames as demonstrated in Fig. 2 (a). Although these representations acquire high performance by taking advantage of the latest traditional CNNs, the conversion procedure will introduce noticeable redundant information, e.g., a large number of pixels of dense frames are empty (Fig. 2 (a)). Moreover, these approaches usually need heavy-weight models with large capacities to extract semantic cues from the complex, dense inputs. As a result, they sacrifice the sparsity of event data and limit the potential of event cameras on mobile applications.
Recently, Bi et al. [11] provide another path to deal with such sparse space-time data, in which they propose a graphbased learning architecture to utilize the sparsity of events better. To save computational cost and decrease model com-

UNDER REVIEW

2

plexity, they construct graphs by sampling a subset of event points as vertices as shown in Fig. 2 (b) and train their model without considering the time dimension of events. Although their model holds a leading position in model complexity compared to previous event-based methods, building such an extremely compact graph representation leads to a severe loss of temporal messages in each vertex, thereby largely sacrificing their classification accuracy. This dilemma leaves open the question of whether it is possible to highlight the classification accuracy and model complexity simultaneously?
In this work, we answer this question by tailoring a new graph-based learning architecture (EV-VGCNN) with a distinctive voxel-wise graph construction strategy and a carefully customized edge embedding method for event data. The proposed model elegantly emphasizes both without compromising any side, i.e., it achieves state-of-the-art (SOTA) accuracy while holding surprisingly low model complexity. Defining the graph vertex is the first core challenge when designing a graph network for event data. To fully encode the input with limited computational cost, we build our graph by voxelizing event points, selecting representative voxels as vertices as depict in Fig. 2 (c), and connecting them according to their spatiotemporal relationships. Specifically, we obtain local semantic features of each vertex by linearly integrating internal events along the time axis, thereby scaling up semantic and motion representation of the graph to orders of magnitude more points. Instead of the frame-based methods that contain much redundant information, our constructed voxel-wise graph can take advantage of the sparsity of event data. Meanwhile, it encodes more informative spatio-temporal cues than the pointwise graph in [11].
Another critical problem for designing an event-based graph model is how to learn the embeddings for the edges and vertices' features. Intuitively, for a vertex in the event-based graph, its adjacent neighbors usually convey local spatial messages while distant neighbors are more likely to carry motion cues or global changes. Inspired by this variation, we design a multi-scale feature relational layer (MFRL) to extract semantic and motion cues from each vertex adaptively. Specifically, two learnable blocks in MFRL are applied to adjacent and distant neighbors respectively to extract different features, and their results are aggregated as the joint representation of a vertex. Finally, we cascade multiple MFRL modules with graph pooling operations and a classifier as the EV-VGCNN to perform end-to-end object classification. The proposed model is capable of extracting discriminative features with low model complexity.
The main contributions of this paper are summarized as follows:
· We introduce a novel method to construct event-based graph representation, which can effectively utilize informative features from voxelized event streams while maintaining the sparsity advantage of event data.
· We propose the MFRL module to adaptively learn spatial semantic and motion cues from the event-based graph according to spatio-temporal relations between vertices and their neighbors.

Y

Y

T

T

Y Event Signals X Y

(a)

X

T

T

(b)

X

(c)

X

Fig. 2. Visual comparison of three types of event-based representation: (a) the frame-based representation by integrating events into dense frames; (b) the point-based representation generated by sampling a subset of event signals; (c) representative event voxels selected as vertices in the proposed graph. Visually, our graph can maintain the sparsity advantage of event signals considerably while learning dense semantic messages inside each voxel (different colors represent varying event point densities among voxels).

· Extensive experiments show that our model enjoys noticeable accuracy gains while achieving nearly 20 times parameters reduction.
II. RELATED WORK
Regarding the special format of event data, considerable works customize different handcrafted descriptors from event data for their tasks, such as corner detection [12], [13], edge/line extraction [5], [14], optical flow prediction [15], [16] and object classification [17], [18]. These works can be categorized into point-based models, which generally take a single event or a set of regional events as input units for feature extraction and aggregation. Specifically, their model can make task-specific decisions based on single or a few incoming events, thereby taking full advantage of the asynchronous nature of event signals and the temporal information contained therein. Nevertheless, this potential comes with a cost, i.e., the quality of their extracted features is usually sensitive to the noise and scene variation, limiting their generalizability to complex scenarios.
Motivated by the breakthrough in traditional vision tasks benefited from CNNs [19]­[22], data-driven learning methods gradually become the leading choice for event-based approaches, such as motion estimation [23], motion segmentation [24] and object classification [11], [25]­[28]. Initially, SNNs are adopted for event data on classification as their asynchronous nature shows good fitness to process event streams. However, it is challenging to apply these methods [25], [26] to real-life applications due to the lack of specialized efficient back-propagation algorithms. To mitigate this problem, several works transfer the pretrained weights from CNNs to SNN models [27], [28] or use a predefined Gabor filter [18] to initialize SNNs. However, the performance of these models

UNDER REVIEW

3

Event signals

Voxelized event data

Representative voxels selected for graph construction

Voxelization

Vertex selection

Feature calculation

Events integration
Voxels Flatten

(a) Graph Construction

Vertices' Vertices' coordinates features

non-Linear (128, 512)

Label

Event signals (N x 4) MFRL MFRL MFRL MFRL
512 + 512 512 256

Graph Construction

Graph pooling

Graph pooling

Graph pooling

Avg and Max pooing

FC (1024, 512)

FC (512, 256)

FC (256, c) Softmax

(b) EV-VGCNN

Classifier

Fig. 3. (a) Graph construction: We first voxelize event data and then select Np representative voxels as the graph's input vertices according to the event points number inside each voxel. Finally, we attain features of vertices by integrating their internal events along the time axis. (b) EV-VGCNN: It consists of
multiple multi-scale feature relational layers (MFRL) followed by graph pooling for learning global features from the input graph, and several linear layers
for object classification. The MFRL module is to aggregate features for each vertex from k-nearest neighbors. In the figure, non-Linear(x, y) is a block with the input channel of x and output channel of y. It contains a linear layer, a Batch Normalization, and a ReLU layer. Nrn represents the output number of vertices of each graph pooling operation. Best viewed in color.

is still unsatisfactory as SNNs trained in both cases are suboptimal.
Instead of designing event point-based methods, another branch of solutions integrates event signals into frame-wise 2D representations to directly adopt traditional CNN architectures for event data. Particularly, Maqueda et al. [5] embed events within a time window to two channels based on the polarity. Zhu et al. [7] integrate events into a four-channel grid by concerning both the polarity and spike time. Besides, Zhu et al. [6] discretize 3D event streams into several temporal bins to reduce the loss of spatio-temporal information. To adaptively embed events to frames, [8] and [9] introduce learnable neural blocks to weigh the importance of each event in the frame. Moreover, a speed invariant kernel is introduced by [10] to avoid the interference from deformation of spatial semantics resulted from motion. Approaches proposed in [29]­[32] tend to improve models' performance by utilizing additional data sources. A two-branch fusion network is recently proposed in [33] to capture spatio-temporal messages from multiple views. While these frame-based methods achieve encouraging performance on event-based classification, the conversion from sparse points to dense 2D maps introduces much redundant information [24]. As a result, they typically require heavyweight models to extract discriminative features, limiting the potential of event cameras on mobile or wearable applications.
The closest work to our method is the RG-CNNs introduced in [11], which is the first approach to build graph CNNs for event data. This approach also acquires superior results

over other point-based methods on various datasets. The RGCNNs builds graphs by sampling a subset of event points as vertices and discarding temporal coordinates of events when the network propagates. Although this construction process reduces the complexity of the input largely, it leads to a severe loss of motion information and the performance is far behind the frame-based solutions. In this work, instead of using the original events as vertices and discarding temporal coordinates, we obtain the features of vertices according to the selected event voxels and link these vertices with edges relying on their spatio-temporal relationships. Comparing to the method in [11], our graph representation retains more semantic and motion cues while maintaining the sparsity advantage. Moreover, the MFRL module allows us to flexibly aggregate features from different neighbors of each vertex in the graph. As a result, our model outperforms RG-CNNs [11] with a large margin in terms of accuracy and model complexity.
III. THE PROPOSED METHOD
We propose a novel graph construction method for event data and design a lightweight network EV-VGCNN for object classification. The pipeline of our graph-based solution is shown in Fig. 3, which can be depicted as follows. (i) The event stream is organized into voxels, where each voxel may contain several events. (ii) We regard each voxel as a vertex in our graph. In specific, the coordinate of a vertex is determined by voxel positions. The feature (temporal and regional

UNDER REVIEW

4

semantics) at each vertex is obtained by accumulating event point features inside its corresponding voxel. (iii) We utilize the EV-VGCNN for sequentially aggregating each vertex's features to generate global representations. In the following sections, we first briefly introduce the working principle of event cameras in Sec. III-A. Then, we detail two crucial components in our learning architecture sequentially, including the graph construction (Sec. III-B) and the EV-VGCNN (Sec. III-C).

A. Event signals
As visualized in Fig. 1, event cameras produce events asynchronously when they detect changes in the log brightness ( ln L(x, y, t)) that exceed the contrast threshold C [34] as Eq. (1) described.
| ln L| = |ln L (x, y, t) - ln L (x, y, t - t)| > C, (1)
where t is the time between the new event and the last event generated at the same location. Each event ei = (xi, yi, ti, pi) is triggered at the pixel location of (xi, yi) at time ti with polarity pi p  -1, 1 . The polarity of an event shows the sign of brightness changes. Precisely, positive events (p = +1) represent the lightness increasing ( ln L > C) and negative events (p = -1) represent the lightness decreasing ( ln L < -C ).

B. Graph construction

In this part, we build a directed graph G = V, E ,

where V = 1, ..., Np and E represent vertices and edges

respectively. Each vertex Vi has two attributes which are the spatio-temporal coordinate Ui  R1×3 and the feature vector Fi  R1×D. As the network EV-VGCNN is capable of finding

neighbors and calculate edges' weights for vertices, we only

need to determine the coordinate and features of each vertex

in the graph construction stage.

1) Voxelization: Event streams can be expressed as a 4D

tuple:

ei N = xi, yi, ti, pi N ,

(2)

where the first two dimensions xi and yi are constrained with the range of the spatial resolution ( H, W ) of event cameras and ti is the timestamp when the event is triggered. Hence, (xi, yi, ti) represents the spatio-temporal coordinate of an event, and the last dimension pi can be seen as the attribute. Given an event stream, we can subdivide the 3D x-y-t space
into voxels, as shown in Fig. 3 (a). Considering the value discrepancy between (xi, yi) and (ti), we first normalize the time dimension with a compensation coefficient A using:

ti N =

ti - t0 N × A . tN-1 - t0

(3)

After normalization, event signals encompass 3D space with range H, W, A along the x, y, t axes respectively. Then, we voxelize this 3D space with the size of each voxel as vh, vw and va. The resulting voxels in spatio-temporal space is of size Hvoxel = H/vh, Wvoxel = W/vw and Avoxel = A/va, where each voxel may contain several events. In this work, we

refer voxel locations to define the coordinate of each vertex in the graph. Accordingly, the coordinate of i-th vertex Ui = (xvi , yiv, tvi ) is with the range of Hvoxel, Wvoxel, Avoxel . For simplicity, we assume that H, W, A are divisible by vh, vw and va.
2) Vertex selection: In practice, even though we only consider non-empty voxels as vertices, there are still tens of thousands of vertices that will be contained in a graph. Constructing a graph with all these vertices imposes a substantial computational burden. In addition, due to noise points (also known as hot pixels) produced by event cameras typically occurs isolated without any connections with other events, plenty of vertices may only be composed of single or a few noise points only. Accordingly, taking these uninformative vertices as input would inevitably introduce interference to the whole learning system. Therefore, we propose to keep only representative vertices for graph construction. To this end, we adopt a simple yet effective selection strategy, which is to find Np vertices with the largest number of event points inside and feed them into the learning system. On the one hand, this selection strategy can work as a noise filter for input data purification. On the other hand, this procedure can help us save computational costs.
3) Feature calculation for vertices: The performance of graph-based neural networks heavily relies on the quality of each vertex's input features. Since each vertex in our graph contains several event points inside, an appropriate method to encode the features for these points is required. Event streams can be seen as a 2D video recorded asynchronously. As an event voxel is generally with a short time span, we argue that integrating its internal event points into a 2D representation along the time axis can well represent 2D appearance semantics for this event voxel. Particularly, given a graph with vertices Vi Np and coordinates (xvi , yiv, tvi ) Np , we can attain 2D features Fi2d  R1×vh×vw of its i-th vertex Vi as formulated in Eq. (4).

Nv

Fi2d(x, y) = piin(x - xiin, y - yiin)tiin,

(4)

i

where Nv denotes the number of events inside the vertex Vi. For each internal event, its coordinate (xiin, yiin, tiin) is constrained with the size of voxels ( vh, vw, va ). By linearly
integrating the events according to their temporal coordinates
and polarities, we can encode the 2D semantics of each vertex
while capturing the temporal (motion) cues. Eventually, we flatten the resulting 2D features Fi2d Np to obtain feature vectors Fi Np  RNp×D of all vertices in the graph, where D = vhvw.

C. EV-VGCNN
The proposed learning architecture comprises three main components: the multi-scale feature relational layer, the graph pooling operation, and the classifier. In this section, we show how to design them and assemble these modules into the EVVGCNN.

UNDER REVIEW

5

(a) MFRL

(b) SFRL

SFRL for adjacent neighbors

SFRL for distant neighbors

non-Linear

Vertices' coordinates Vertices' features (F) KNN
Neighbor index Geometric relationship (g)
Scroing matrix (M) Summation
Aggregated features (F')

Fig. 4. Structure of the MFRL and its base component SFRL. (a) MFRL: The MFRL is composed of two SFRL modules and a shortcut connection, in which the two SFRL modules are to encode features from adjacent and distant neighbor vertices respectively. This design allows us to explore motion and spatial messages behind event signals flexibly. : element-wise addition. (b) SFRL: This module realizes the Eq. (5) and Eq. (6) using neural networks. Particularly, the SFRL module takes vertices with their coordinates and features as input. For each vertex, the SFRL builds edges between the vertex and its Nneigh neighbors, computes the scoring matrix for its neighbors then aggregates the features from neighbors using the matrix to obtain the representation of this vertex. : matrix multiplication.

Searching neighbors via KNN

Multi-scale feature relational learning

Fig. 5. An intuitive illustration of how the MFRL aggregate features for a vertex from its adjacent and distant neighbors. For a vertex (the red point) in our graph, we firstly determine its Nnaedjigh (blue points) and Nndeisigh (yellow points) neighbors w.r.t distances. Then, we aggregate features from these neighbors utilizing two independent network branches, where yellow arrows and blue arrows represent learned weights from two SFRLs.
1) Multi-scale feature relational layer (MFRL): Unlike the traditional 3D point clouds whose coordinates only express geometric messages, event data contains two different types of cues, i.e., 2D spatial messages, and motion information. For a vertex in the event-based graph, its adjacent neighbors usually carry local spatial cues. In contrast, its distant neighbors are more likely to comprise much motion information. Notably, though adjacent and distant neighbors carry motion and spatial cues simultaneously in most cases, the motion and spatial variances between a vertex and its adjacent and distant neighbors are different, i.e., adjacent neighbors hold small and local variance while distant neighbors carry more global changes. Given this disparity, it is difficult to use a shared CNN branch to learn all neighbors. To this end, we introduce the MFRL module to extract motion and semantic messages from vertices adaptively depending on the distance between vertices and their neighbors. As shown in Fig. 4 (a), the MFRL consists of one shortcut connection and two single-scale feature relational layers (SFRL) to extract correlations from adjacent and distant neighbors respectively. More specifically, for a vertex, we define Nnaedjigh and Nndeisigh as the numbers of its adjacent and distant neighbors, respectively. The results obtained from these three branches are then aggregated as the

output. We intuitively illustrate this learning process in Fig. 5.
We then detail how to design the SFRL module. Through the construction procedure depicted in Sec. III-B, we have obtained features (F) and coordinates (U) of vertices (V) in the graph. The SFRL takes these features and coordinates as inputs and is entailed to accomplish three functions: (i) building connections among vertices with edges, (ii) computing the scoring matrix for neighbors of the vertex, and (iii) integrating features from the neighborhood for each vertex. As shown in Fig. 4 (b), to achieve these goals, we first utilize the K-Nearest Neighbor algorithm (K-NN) to determine Nneigh neighbors for each vertex and link them with edges. The graph includes a self-loop, meaning that each vertex also links itself as a neighbor [35]. Then, for the i-th vertex Vi with edges Ei (Ei  R ) 1×Nneigh and coordinates Ui, the scoring matrix can be calculated using:

Mi = Q( SG (gi,j ); Wm),

(5)

j:(i,j)Ei

where gi,j = [Ui, Ui - Uj]  R6 represents the geometric relation between a vertex and its neighbors. [·, ·] denotes the concatenation of two vectors. SG(·) is a function that stacks all geometric relations of the vertex's neighbors ( gi,j : (i, j)  Ei ) and its output is in R . Nneigh×6 Q is parameterized by Wm and comprises a linear mapping function, a Batch Normalization, and a Tanh function to explore geometric messages from neighbor vertices. The output Mi  RNneigh×Nneigh is the scoring matrix for Vi, which aims to reweight features from its neighbors based on spatio-temporal
relationships when aggregating the neighbors' features for the
central vertex. Finally, we formulate the function of aggregat-
ing features from neighbors into the vertex as:

Fi = Mi(H( SF (Fj); Wf )),

(6)

jEi

where SF (·) is a function that stacks all features (Fj  R1×Din ) from neighbor vertices and its output is in

UNDER REVIEW

6

R . Nneigh×Din H is a non-linear transform function with parameters Wf and consists of a linear mapping function, a Batch Normalization and a ReLU. The function H takes the stacked features as input and produces transformed features in R . Nneigh×Dout After that, the scoring map Mi is utilized to re-weight neighbors' features. We then apply a summation operation on the feature space over all neighbors to generate the aggregated features Fi  R1×Dout for the i-th vertex.
2) Graph pooling operation: The pooling operation is to reduce the vertex number in the network progressively. The pooling layer in 3D vision models commonly aggregates local messages of each point and then selects a subset of points with the dominant response for the following processing. In our case, the feature aggregation step has been fulfilled by the MFRL module. Hence, in our pooling layers, we only need to randomly select vertices from the graph to enlarge the receptive field for the feature aggregation of each vertex. We denote the output number of vertices of the graph pooling operation as Nr.
3) Classifier: Following the operation used in [36], [37], we apply symmetry functions on the high-level features to achieve a global representation for the input. Specifically, we use max and average pooling operations to process these highlevel features respectively, and then concatenate them to form a one-dimensional feature vector. Finally, we feed the global feature vector to three fully connected layers for classification.
4) Network architecture: The structure of EV-VGCNN is the same for all datasets. As shown in Fig. 3 (b). We embed four relational learning modules (MFRL) into our model to obtain the global representation of an event stream. Besides, we apply the pooling operation after each of the former three MFRLs. We further apply a non-linear block consisting of a linear layer, a Batch Normalization, and a ReLU after the fourth MFRL and then feed the output feature of this nonlinear block to the classifier.
IV. EXPERIMENTAL EVALUATION
In this section, we use several benchmark datasets to evaluate the proposed method on the classification accuracy, the model complexity (measured in the number of trainable parameters), and the number of floating-point operations (FLOPs).
A. Datasets
We validate our method on five representative event-based classification datasets: N-MNIST (N-M) [38], N-Caltech101 (N-Cal) [38], [39], CIFAR10-DVS (CIF10) [40], N-CARS (N-C) [18] and ASL-DVS (ASL) [41]. In general, there are two alternatives to generate these datasets. Particularly, NMNIST, N-Caltech101, and CIFAR10-DVS are obtained by recording traditional images displayed on monitors with fixed motion trajectories. This recording method may suffer from artificial noise introduced by the shooting and emulation environments. On the contrary, N-CARS and ASL-DVS are recorded in the real-world environment using event cameras, which means that the evaluation results on these two datasets should better reflect the performance of event-based models in

TABLE I COMPARISON OF EVENT-BASED CLASSIFICATION DATASETS.

Dataset N-MNIST [38] N-Caltech101 [38] CIFAR10-DVS [40] N-CARS [18] ASL-DVS [41]

From images Yes Yes Yes No No

Samples 60000 8246 10000 24029 100800

Classes 10 101 10 2 24

practice. The statistical comparison of these datasets is shown in Table I. We train models separately for each dataset and evaluate the performance of our approach on their testing sets. For those datasets (N-Caltech101, CIFAR10-DVS, and ASL-DVS) without official splitting, we follow the experiment setup adopted in [11], [18], in which 20% data are randomly selected for testing, and the remaining is used for training and validation. We average over five repetitions with different random seeds as our reported results.
B. Implementation details
1) Graph construction: We fix the compensation coefficient A for all datasets as 8 to normalize event data. We set the input vertex number Np as 512 for N-MNIST and ASLDVS as the objects in them are small size, set Np as 1024 for N-CARS, and Np = 2048 for more complex datasets N-Caltech101 and CIFAR10-DVS. According to the spatiotemporal discrepancy across different datasets, we set the voxel size as (vh, vw, va) = (2, 2, 1) for N-MNIST and (5, 5, 3) for other datasets.
2) Network: The values of Nnaedjigh and Nndeisigh for all MFRL modules are fixed as 10 and 15 respectively. We set the output vertex number of three graph pooling operations as 896, 768, and 640 for N-Caltech101, N-CARS, and CIFAR10DVS datasets. For the other two datasets, which only take 512 vertices as input, we remove the pooling operation between MFRL modules. We add dropout layers with a probability of 0.5 after the first two fully-connected layers in the classifier to avoid overfitting. Each fully-connected layer is followed by a LeakyReLU and a Batch Normalization except for the prediction layer.
3) Training: We train our model from scratch for 250 epochs by optimizing the cross-entropy loss using the SGD [42] optimizer (except for the ASL-DVS) with a learning rate of 0.1 and reducing the learning rate until 1e-6 using cosine annealing. As for the ASL-DVS, we experimentally find that using the Adam [43] optimizer with a learning rate of 0.001 and decaying the learning rate by a factor of 0.5 every 20 epochs contributes to better performance. The batch size for training is set to 32 for all datasets.
C. Comparison to other methods
In this section, we report the comparison to two mainstream event-based object classification solutions, namely point-based and frame-based methods, to show the advantages of our model comprehensively.

UNDER REVIEW

7

TABLE II COMPARISON OF THE CLASSIFICATION ACCURACY BETWEEN OURS AND
OTHER POINT-BASED METHODS.

Method H-First [25] HOTS [17] Gabor-SNN [18] HATS [18] RG-CNNs [11] Ours

N-M 0.712 0.808 0.837 0.991 0.990 0.994

N-Cal 0.054 0.21 0.196 0.642 0.657 0.746

N-C 0.561 0.624 0.789 0.902 0.914 0.957

CIF10 0.077 0.271 0.245 0.524 0.540 0.651

ASL -
0.901 0.988

TABLE III
COMPARISON OF DIFFERENT EVENT-BASED CLASSIFICATION MODELS ON
THE MODEL COMPLEXITY (#PARAMETERS) AND THE NUMBER OF FLOPS.  GFLOPS = 109 FLOPS

Method Ev-Count [11] EST [8] AMAE [10] M-LSTM [9] MVF-Net [33] RG-CNNs [11] Ours

Type Frame-based Frame-based Frame-based Frame-based Frame-based Point-based Point-based

#Parameters 25.61M 21.38M 33.52M 21.43M 33.62M 19.46M 0.84M

GFLOPs 3.87 4.28 5.46 4.82 5.62 0.79 0.70

1) Comparison with point-based methods: As our proposed work also falls under the category of point-based methods, we firstly compare it to SOTA point-based models, including H-First [25], HOTS [17], Gabor-SNN [18], HATS [18] and RG-CNNs [11]. As shown in Table II, the proposed method outperforms SOTA point-based models consistently. Especially, our approach improves the model's performance by a large margin on four challenging datasets such as NCal, N-C, CIF10, and ASL. Furthermore, Table III illustrates that our method shows significant advantages on the model complexity (approximately 20 times parameter reduction) with fewer FLOPs compared to RG-CNNs. We attribute the twosided improvement to two designs in our model. (i) Our graph is constructed by setting an event voxel instead of a single event point as the vertex. This modification retains more powerful regional semantics and temporal cues than other strategies. The resulting compact and informative inputs largely ease the following network to learn discriminative features in various scenarios. (ii) The MFRL module in our network can adaptively exploit the spatio-temporal correlations across different vertices in the graph, allowing us to construct a shallow and lightweight network while achieving SOTA accuracy.
2) Comparison with frame-based methods: To have a comprehensive analysis, we also compare our method with several representative frame-based approaches, including Ev-Count [11], EST [8], Matrix-LSTM (M-LSTM) [9], AMAE [10] and MVF-Net [33]. In particular, MVF-Net has achieved SOTA performance on different classification datasets, as shown in Table IV. From the table, we can see that EST, M-LSTM, AMAE, and MVF-Net have been consistently improved after using pretrained networks, especially on two datasets (N-Cal, CIF10) converted from traditional images. This is because that the frame-based classification models can take advantage of the weights pretrained on large-scale traditional image datasets (e.g., ImageNet) [44]. However, without utilizing the prior knowledge from conventional images, our approach can still achieve comparable accuracy to these frame-based methods on N-M, N-C, and ASL datasets. More importantly, the proposed method obtains better results on all evaluated datasets than most frame-based methods trained from scratch. These comparisons demonstrate that our architecture and designs are greatly suitable for extracting discriminative representations

TABLE IV COMPARISON OF THE CLASSIFICATION ACCURACY BETWEEN OURS AND
FRAME-BASED METHODS.  RESULTS ARE ACQUIRED BY USING THE CLASSIFIER WITH RESNET-34 [20] AS THE BACKBONE.  WE TRAIN
THESE APPROACHES FROM SCRATCH USING THE OPTIMIZING SETTINGS
DESCRIBED IN [10] AND ADOPT THE SAME TRAINING AND TESTING SETS USED IN THIS PAPER.

Method

N-M N-Cal N-C CIF10

Pretrained on ImageNet [19]

EST [8]

0.991 0.837 0.925 0.749

AMAE [10] M-LSTM [9] 

0.993 0.851 0.955 0.751 0.989 0.857 0.957 0.730

MVF-Net [33] 0.993 0.871 0.968 0.762

Without pretraining

Ev-Count [11] EST [8]  AMAE [10]  M-LSTM [9]  MVF-Net [33] 

0.984 0.990 0.983 0.986 0.981

0.637 0.753 0.694 0.738 0.687

0.903 0.919 0.936 0.927 0.927

0.558 0.634 0.620 0.631 0.599

Ours

0.994 0.746 0.957 0.651

ASL
0.991 0.994 0.992 0.996
0.886 0.985 0.989 0.984 0.976 0.988

from event data.
3) Complexity analysis: We follow the calculation method described in [11], [45]­[47] to compute FLOPs for these methods. Since models' architecture may vary when evaluated on different datasets, we obtain results from these models on the same dataset N-Caltech101. The model complexity and the number of FLOPs of these frame-based methods are listed in Table III, in which our approach is capable of performing classification with lower computational cost and at least 20 times fewer parameters. Compared to frame-based solutions which introduce much redundant information, our graph network learns discriminative features directly from the sparse inputs, thus effectively relieving the learning pressure of the neural network. For instance, the 18-channel framebased representation of samples from the N-Cal dataset with a spatial resolution of 180 × 240 used in [8] has 777600 input elements, while our proposed graph only contains 2048 ones.

UNDER REVIEW

8

TABLE V IMPACT OF DIFFERENT VALUE OF Nnaedjigh AND Nndeisigh ON THE
PERFORMANCE EVALUATED ON THE N-CAL DATASET.

Value

Variants Nnaedjigh Nndeisigh Accuracy

A 10 15 0.746

B 10 10 0.740

C 10 20 0.749

D 5 15 0.735

E 20 15 0.738

F 5 20 0.741

G 20 5 0.729

TABLE VII COMPARISON BETWEEN VOXEL-WISE AND POINT-WISE GRAPH CONSTRUCTION STRATEGIES WITH THE SAME NETWORK ARCHITECTURE.

Vertex type Original events Original events Original events Event voxels (Ours)

#Vertex 2048 4096 8192 2048

Accuracy 0.545 0.580 0.597 0.746

GFLOPs 0.63 0.66 0.72 0.70

TABLE VI ABLATION STUDY ON THE EFFECTIVENESS OF THE MFRL.

Model EV-VGCNN w/ SFRL EV-VGCNN w/ MFRL

N-CAL 0.734 0.746

N-C 0.946 0.957

CIF10 0.638 0.651

ASL 0.965 0.988

D. Ablation study
In this part, we conduct ablation studies to verify the
efficacy of our MFRL module, advantages of our voxel-wise
graph representations, and the robustness of the proposed
graph model to the input vertex density. Also, discussing the impact of hyper-parameters NNadejigh and NNdiesigh to the system.
1) The value setting for NNadejigh and NNdiesigh: In this part, we set up a series of experiments on the N-Cal dataset to discuss how the variation of NNadejigh and NNdiesigh affects the final performance of our model. Results of the controlled
experiment are listed in Table V. Comparing the settings A, B and C, we can find that when NNadejigh is fixed, a larger value of NNdiesigh results in better performance. Intuitively, when we involve more distant neighbors to aggregate the features of a
vertex, a denser neighborhood, carrying more global messages
and cross-vertex spatio-temporal relationships, is encoded to
aggregate the features for the central vertex. Differently, when we fix the NNdiesigh and change the value of NNadejigh (e.g., settings A, D, and E) from 10 to 20, the final performance
drops considerably. We argue that this is due to only a small
number of adjacent neighbors being informative to character-
ize the local semantic information of a vertex. If a considerable
part of adjacent neighbors is actually with large distance,
then these "adjacent" neighbors are difficult to characterize
this vertex's local semantics and tend to be interference. For
the value chosen of these two hyper-parameters in this work,
we firstly fix the summation of neighbors as 25 considering computational budget, then experimentally set NNadejigh and NNdiesigh as 10 and 15 respectively according to the comparison among settings A, F and G.
2) Efficacy of the MFRL: To evaluate the effectiveness of
the proposed MFRL module, we introduce a baseline model
which replaces the MFRL with the SFRL module in the EV-
VGCNN. The MFRL learns local representations adaptively
depending on the distance between a vertex and its neighbors.
In contrast, all neighbors of a vertex are equally treated in
the SFRL to learn their correlations with shared parameters.
For a fair comparison, we set Nneigh for each SFRL as the

summation of Nnaedjigh and Nndeisigh in MFRL. We perform this comparison on the four most challenging datasets (NCaltech101, N-CARS, CIFAR10-DVS, and ASL-DVS) and show results in Table VI. We can see that the MFRL consistently improves the performance on four datasets, suggesting that the proposed MFRL module can effectively enhance the discriminativeness of features by additionally considering the spatial-temporal variation across vertices and their neighbors.
3) Advantages of the proposed graph construction strategy: Our voxel-wise graph construction strategy carries more local cues over the point-wise graph construction method. To validate its advantages, we perform comparisons to a point-based graph construction strategy by selecting a random subset of event points as vertices and assigning the polarity of events to vertices as their features. Our comparison focuses on two concerns: (i) when the number of vertices is the same in two types of graph, whether the proposed voxel-wise graph can provide more discriminative features for the graph network? (ii) If we increase the number of vertices in the point-wise graph to make its model has the same FLOPs as our voxel-wise graph, will our graph still show significant advantages on the classification accuracy? To answer these questions, we feed the graph inputs generated by these two graph construction methods to the same EV-VGCNN architecture and perform comparisons on the N-Caltech101 dataset.
The results in Table VII show that with the same number of vertices (2048) inside, our graph construction strategy contributes to a significant accuracy gain, indicating that it encodes more informative features from the event data than the point-wise graph. We then increase the vertex number of the point-wise graph to 8192, which is much larger than ours. Even so, our method still has a considerable accuracy leading. We credit these superiorities to that our voxel-wise graph construction strategy enables the vertex to encode local correlations among points, thus carrying a more powerful representation for a point region. In contrast, although the compared point-wise graph reduces the complexity of the input, it leads to a severe loss of local 2D appearance and motion information in each vertex.
4) Model robustness to the vertex density: Practical applications in real life call for our graph-based classification network to be flexible to different input sizes. It means that one can input our EV-VGCNN with an arbitrary number of vertices. This characteristic is significantly helpful to event-based models since real-world scenarios are naturally with different spatial scales or motion trajectories, which entail graphs with

UNDER REVIEW

9

Fig. 6. Results of the proposed model tested with different number of input vertices in the graph.
distinct numbers of vertices to embed their semantic features correspondingly. Therefore, we validate the robustness of our model (trained on graphs with 1024 vertices) to the input vertex density on the N-Caltech101 dataset. Fig. 6 shows that although the classification accuracy drops dramatically when the number of vertices is fewer than 1536, our approach is capable of keeping a stable performance (performance drops within 1%) in a large range ([1536, 2816]) of input vertices, suggesting its robustness to the input vertex density and potential on real-world applications.
V. CONCLUSION
In this work, we introduce a novel graph-based learning framework for event data. The proposed voxel-wise graph construction method retains more local information than previous point-wise methods while maintaining the sparsity of event data. Moreover, we tailor a MFRL module to explore spatialtemporal relationships between vertices and their neighbors adaptively. Extensive experiments show the advantages of our designs, as well as the elegant improvement on both accuracy and model complexity achieved by the proposed end-to-end lightweight EV-VGCNN.
REFERENCES
[1] P. Lichtsteiner, C. Posch, and T. Delbruck, "A 128× 128 120 db 15 µs latency asynchronous temporal contrast vision sensor," IEEE J. SolidState Circuit, vol. 43, no. 2, pp. 566­576, Feb 2008.
[2] C. Posch, D. Matolin, and R. Wohlgenannt, "A qvga 143 db dynamic range frame-free pwm image sensor with lossless pixel-level video compression and time-domain cds," IEEE J. Solid-State Circuit, vol. 46, no. 1, pp. 259­275, 2010.
[3] G. Gallego, T. Delbruck, G. M. Orchard, C. Bartolozzi, B. Taba, A. Censi, S. Leutenegger, A. Davison, J. Conradt, K. Daniilidis, and D. Scaramuzza, "Event-based vision: A survey," IEEE Trans. Pattern Anal. Mach. Intell., pp. 1­1, 2020.
[4] W. Maass, "Networks of spiking neurons: the third generation of neural network models," Neural networks, vol. 10, no. 9, pp. 1659­1671, 1997.
[5] A. I. Maqueda, A. Loquercio, G. Gallego, N. Garc´ia, and D. Scaramuzza, "Event-based vision meets deep learning on steering prediction for selfdriving cars," in IEEE Conf. Comput. Vis. Pattern Recog., 2018, pp. 5419­5427.
[6] A. Zihao Zhu, L. Yuan, K. Chaney, and K. Daniilidis, "Unsupervised event-based optical flow using motion compensation," in Eur. Conf. Comput. Vis., 2018, pp. 0­0.

[7] A. Zhu, L. Yuan, K. Chaney, and K. Daniilidis, "Ev-flownet: Selfsupervised optical flow estimation for event-based cameras," in Proceedings of Robotics: Science and Systems, Pittsburgh, Pennsylvania, June 2018.
[8] D. Gehrig, A. Loquercio, K. G. Derpanis, and D. Scaramuzza, "End-toend learning of representations for asynchronous event-based data," in IEEE/CVF Int. Conf. Comput. Vis., 2019, pp. 5633­5643.
[9] M. Cannici, M. Ciccone, A. Romanoni, and M. Matteucci, "A differentiable recurrent surface for asynchronous event-based data," in Eur. Conf. Comput. Vis., August 2020.
[10] Y. Deng, Y. Li, and H. Chen, "Amae: Adaptive motion-agnostic encoder for event-based object classification," IEEE Robot. Autom. Lett., vol. 5, no. 3, pp. 4596­4603, 2020.
[11] Y. Bi, A. Chadha, A. Abbas, E. Bourtsoulatze, and Y. Andreopoulos, "Graph-based spatio-temporal feature learning for neuromorphic vision sensing," IEEE Trans. Image Process., pp. 1­1, 2020.
[12] X. Clady, S.-H. Ieng, and R. Benosman, "Asynchronous event-based corner detection and matching," Neural Netw., vol. 66, pp. 91­106, 2015.
[13] E. Mueggler, C. Bartolozzi, and D. Scaramuzza, "Fast event-based corner detection." in British Mach. Vis. Conf, 2017.
[14] S. Seifozzakerini, W.-Y. Yau, B. Zhao, and K. Mao, "Event-based hough transform in a spiking neural network for multiple line detection and tracking using a dynamic vision sensor." in British Mach. Vis. Conf, 2016.
[15] X. Clady, J.-M. Maro, S. Barre´, and R. B. Benosman, "A motion-based feature for event-based pattern recognition," Front. Neurosci., vol. 10, p. 594, 2017.
[16] T. Brosch, S. Tschechne, and H. Neumann, "On event-based optical flow detection," Front. Neurosci., vol. 9, p. 137, 2015.
[17] X. Lagorce, G. Orchard, F. Galluppi, B. E. Shi, and R. B. Benosman, "Hots: a hierarchy of event-based time-surfaces for pattern recognition," IEEE Trans. Pattern Anal. Mach. Intell., vol. 39, no. 7, pp. 1346­1359, 2016.
[18] A. Sironi, M. Brambilla, N. Bourdis, X. Lagorce, and R. Benosman, "Hats: Histograms of averaged time surfaces for robust event-based object classification," in IEEE Conf. Comput. Vis. Pattern Recog., 2018, pp. 1731­1740.
[19] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, "Imagenet: A large-scale hierarchical image database," in IEEE Conf. Comput. Vis. Pattern Recog., 2009, pp. 248­255.
[20] K. He, X. Zhang, S. Ren, and J. Sun, "Deep residual learning for image recognition," in IEEE Conf. Comput. Vis. Pattern Recog., 2016, pp. 770­ 778.
[21] H. Chen and Y. Li, "Three-stream attention-aware network for rgb-d salient object detection," IEEE Trans. Image Process., vol. 28, no. 6, pp. 2825­2835, 2019.
[22] H. Chen, Y. Deng, Y. Li, T. Hung, and G. Lin, "Rgbd salient object detection via disentangled cross-modal fusion," IEEE Trans. Image Process., vol. 29, pp. 8407­8416, 2020.
[23] Y. Sekikawa, K. Hara, and H. Saito, "Eventnet: Asynchronous recursive event processing," in IEEE Conf. Comput. Vis. Pattern Recog., June 2019.
[24] A. Mitrokhin, Z. Hua, C. Fermuller, and Y. Aloimonos, "Learning visual motion segmentation using event surfaces," in IEEE Conf. Comput. Vis. Pattern Recog., 2020, pp. 14 414­14 423.
[25] G. Orchard, C. Meyer, R. Etienne-Cummings, C. Posch, N. Thakor, and R. Benosman, "Hfirst: A temporal approach to object recognition," IEEE Trans. Pattern Anal. Mach. Intell., vol. 37, no. 10, pp. 2028­2040, Oct 2015.
[26] A. Amir, B. Taba, D. Berg, T. Melano, J. McKinstry, C. Di Nolfo, T. Nayak, A. Andreopoulos, G. Garreau, M. Mendoza et al., "A low power, fully event-based gesture recognition system," in IEEE Conf. Comput. Vis. Pattern Recog., 2017, pp. 7243­7252.
[27] P. U. Diehl, D. Neil, J. Binas, M. Cook, S.-C. Liu, and M. Pfeiffer, "Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing," in 2015 International Joint Conference on Neural Networks. IEEE, 2015, pp. 1­8.
[28] J. A. Pe´rez-Carrasco, B. Zhao, C. Serrano, B. Acha, T. SerranoGotarredona, S. Chen, and B. Linares-Barranco, "Mapping from framedriven to frame-free event-driven vision systems by low-rate rate coding and coincidence processing­application to feedforward convnets," IEEE Trans. Pattern Anal. Mach. Intell., vol. 35, no. 11, pp. 2706­2719, 2013.
[29] H. Rebecq, R. Ranftl, V. Koltun, and D. Scaramuzza, "High speed and high dynamic range video with an event camera," IEEE Trans. Pattern Anal. Mach. Intell., pp. 1­1, 2019.

UNDER REVIEW

10

[30] D. Gehrig, M. Gehrig, J. Hidalgo-Carrio´, and D. Scaramuzza, "Video to events: Recycling video datasets for event cameras," in IEEE Conf. Comput. Vis. Pattern Recog., June 2020.
[31] Y. Hu, T. Delbruck, and S.-C. Liu, "Learning to exploit multiple vision modalities by using grafted networks," in Eur. Conf. Comput. Vis., 2020, pp. 85­101.
[32] Y. Deng, H. Chen, H. Chena, and Y. Li, "Learning from images: A distillation learning framework for event cameras," IEEE Trans. Image Process., pp. 1­1, 2021.
[33] Y. Deng, H. Chen, and Y. Li, "Mvf-net: A multi-view fusion network for event-based object classification," IEEE Trans. Circuits Syst. Video Technol., pp. 1­1, 2021.
[34] G. Gallego, J. E. A. Lund, E. Mueggler, H. Rebecq, T. Delbruck, and D. Scaramuzza, "Event-based, 6-dof camera tracking from photometric depth maps," IEEE Trans. Pattern Anal. Mach. Intell., vol. 40, no. 10, pp. 2402­2412, Oct 2018.
[35] T. N. Kipf and M. Welling, "Semi-supervised classification with graph convolutional networks," in Int. Conf. Learn. Represent., 2016, pp. 1­35.
[36] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, "Pointnet: Deep learning on point sets for 3d classification and segmentation," in IEEE Conf. Comput. Vis. Pattern Recog., 2017, pp. 652­660.
[37] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, "Pointnet++: Deep hierarchical feature learning on point sets in a metric space," in Conf. Neural Inf. Process. Syst., 2017, pp. 5099­5108.
[38] G. Orchard, A. Jayawant, G. K. Cohen, and N. Thakor, "Converting static image datasets to spiking neuromorphic datasets using saccades," Front. Neurosci., vol. 9, p. 437, 2015.
[39] L. Fei-Fei, R. Fergus, and P. Perona, "One-shot learning of object categories," IEEE Trans. Pattern Anal. Mach. Intell., vol. 28, no. 4, pp. 594­611, 2006.
[40] H. Li, H. Liu, X. Ji, G. Li, and L. Shi, "Cifar10-dvs: An event-stream dataset for object classification," Front. Neurosci., vol. 11, p. 309, 2017.
[41] Y. Bi, A. Chadha, A. Abbas, E. Bourtsoulatze, and Y. Andreopoulos, "Graph-based object classification for neuromorphic vision sensing," in IEEE/CVF Int. Conf. Comput. Vis., 2019, pp. 491­501.
[42] I. Sutskever, J. Martens, G. Dahl, and G. Hinton, "On the importance of initialization and momentum in deep learning," in Int. Conf. Mach. Learn, 2013, pp. 1139­1147.
[43] D. P. Kingma and J. Ba, "Adam: A method for stochastic optimization," arXiv preprint arXiv:1412.6980, 2014.
[44] H. Rebecq, R. Ranftl, V. Koltun, and D. Scaramuzza, "High speed and high dynamic range video with an event camera," IEEE Trans. Pattern Anal. Mach. Intell., pp. 1­1, 2019.
[45] K. He and J. Sun, "Convolutional neural networks at constrained time cost," in IEEE Conf. Comput. Vis. Pattern Recog., 2015, pp. 5353­5360.
[46] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam, "Mobilenets: Efficient convolutional neural networks for mobile vision applications," arXiv preprint arXiv:1704.04861, 2017.
[47] P. Molchanov, S. Tyree, T. Karras, T. Aila, and J. Kautz, "Pruning convolutional neural networks for resource efficient inference," in Int. Conf. Learn. Represent., 2019.

