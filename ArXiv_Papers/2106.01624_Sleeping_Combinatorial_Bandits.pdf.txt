Sleeping Combinatorial Bandits

Kumar Abhishek, IIIT Hyderabad, India.

kumar.abhishek@research.iiit.ac.in

Ganesh Ghalme, Technion, Israel. Sujit Gujar, IIIT Hyderabad, India. Yadati Narahari, IISc Banglore, India.

ganeshg@campus.technion.ac.il sujit.gujar@iiit.ac.in narahari@iisc.ac.in

arXiv:2106.01624v1 [cs.LG] 3 Jun 2021

Abstract
In this paper, we study an interesting combination of sleeping and combinatorial stochastic bandits. In the mixed model studied here, at each discrete time instant, an arbitrary availability set is generated from a fixed set of base arms. An algorithm can select a subset of arms from the availability set (sleeping bandits) and receive the corresponding reward along with semi-bandit feedback (combinatorial bandits). We adapt the well-known CUCB algorithm in the sleeping combinatorial bandits setting and refer to it as CS-UCB. We prove -- under mild smoothness conditions -- that the CS-UCB algorithm achieves an O(log(T )) instance-dependent regret guarantee. We further prove that (i) when the range of the rewards is bounded, the regret guarantee of CS-UCB algorithm is O( T log(T )) and (ii) the instance-independent regret is O( 3 T 2 log(T )) in a general setting. Our results are quite general and hold under general environments -- such as non-additive reward functions, volatile arm availability, a variable number of base-arms to be pulled -- arising in practical applications. We validate the proven theoretical guarantees through experiments.
1 Introduction
The stochastic multi-armed bandit (MAB) problem is one of the fundamental online learning problems that captures the classic exploration vs. exploitation dilemma. A MAB algorithm, operating in an uncertain environment, is expected to optimally trade-off acquisition of new information with optimal use of information-at-hand to choose an action that maximizes the expected reward or, equivalently, minimizes the expected regret. In a classical stochastic MAB setup, an algorithm has to pull (aka select) a single arm (aka choice) at each time instant and receive a reward corresponding to a pulled arm. The reward from each arm is an independent sample from a fixed but unknown stochastic distribution. The goal is to minimize the expected regret; the difference between the expected cumulative reward of the best offline algorithm with known distributions and the expected cumulative reward of the algorithm.
In this paper, we study a combination of two well studied extensions of classical stochastic MABs, namely sleeping bandits [KNMS10] and combinatorial bandits [GKJ12]. In the sleeping bandits setting,
1

only a subset of base arms is available at each time instant. This variant, sometimes also known as volatile bandits [BPSF13] or mortal bandits [CKRU09], models many real-world scenarios such as crowdsourcing [LLJ19], online advertising [CKRU09], and network routing [KNMS10, BPSF13] where an algorithm is restricted to select from only the available set of choices.
Another well studied generalization of the classical MAB setting is the combinatorial MAB (CMAB) problem [AB09, GKJ10, CBL12, CTMSPl15, WC18]. Similar to sleeping bandits, this variant too provides an abstraction to many real-world decision problems. For instance, in an online advertising setup, the platform selects multiple ads to display at any point in time [GKJ10]; in crowdsourcing, the requester chooses multiple crowd workers at the same time [uHC16] and in network routing algorithm has to choose a path instead of a single edge [TZC+17, KWAS15a]. Studying the two settings together presents interesting and non-trivial technical challenges.
We consider the semi-bandit feedback model and a general reward function (under mild smoothness constraints). In the semi-bandit feedback model, an algorithm observes the reward realizations corresponding to each of the selected arms along with the overall reward for pulling the subset of arms. The smoothness properties on the reward functions studied in this paper are similar to those in [CWY13]. It is worth mentioning here that in the sleeping MAB setting, the conventional definition of regret is not appropriate as the best arm (or the best subset of arms in the combinatorial sleeping MAB case) may not be available at all time instants. Hence, we evaluate the performance of an algorithm in terms of its sleeping regret [KNMS10], defined as the difference between the expected reward obtained from best available arm and the arm pulled by the algorithm.
The paper is organized as follows: In Section 2, we formally introduce the sleeping combinatorial bandits problem and define Lipschitz smoothness and Bounded smoothness assumptions. The required notational setup is introduced and CS-UCB algorithm is given in Section 3. In Section 4, we provide regret analysis of CS-UCB under Lipschitz smoothness and Bounded smoothness assumptions. In Section 5, we provide an in-depth verify of the theoretical results on simulated data with few reward functions. The related literature is discussed in Section 6 and in Section 7, we conclude our paper with a brief discussion on the results and future directions.

2 Model and Assumptions

In a classical stochastic multi-armed bandits (MAB) problem, at each discrete time step t, an algo-

rithm pulls a single arm it  [k] and observes a random reward Xit,t. The random variables (Xi,t)t are identical and independently distributed according to a distribution Di(µi). Here, µi is the mean

of distribution Di. Note that the reward corresponding to arms j = it is not observed. The reward

distributions (Di)i[k] are unknown to the algorithm. Throughout this paper we consider that the re-

ward distributions have a bounded support. The algorithm's objective is to minimize expected regret

defined as, RALG(T ) = E[

T t=1

(Xi

,t

-

Xit,t)].

Here,

i

= arg maxi µi denotes the best arm.

In this paper we consider a sleeping combinatorial bandits problem with [k] := {1, 2, · · · , k} denoting the set of base arms and µ  [0, 1]k, the vector of unknown mean qualities of the base

arms. Similar to the classical stochastic MAB problem, each base arm i corresponds to an unknown

distribution Di with mean µi  [0, 1] over its quality. At each time instant t, a subset At  [k] of the base arms become available. Throughout the paper, we consider that At is an arbitrary non-empty

2

subset. A decision maker (i.e. an algorithm) can pull any non-empty subset St  At of arms and receive a reward Rt := R(St, µ). The reward depends upon the selected subset St and the mean qualities of the arms, µ. We define, RS := R(S, µ) whenever the quality vector is clear from the context. Furthermore, the reward depends only on the qualities of pulled arms St 1. We remark here that the classical stochastic bandits setting is a special case of our setting with At = [k], |St| = 1 and Rt = XSt,t for all t.

For a given reward function R the problem reduces to finding a reward maximizing subset of

arms. This problem, even when the qualities of the base arms are known, is known to be NP-hard

in general [WN99]. However, many important settings, such as submodular reward functions, ad-

mit a polynomial time approximation schemes that provides a decent approximation guarantee. To

demarcate the computational problem of finding an optimal set of arms from effectively learning the

quality distributions (and hence the learning an optimal set of arms to be pulled) we assume the ex-

istence of an (, )-approximation oracle (denoted by (, )-ORACLE ), which, given an availability

set

A

and

a

quality

vector

µ,

outputs

a

set

S

such

that

RS (µ)





·

R
S

(µ),

for

all

S

 2A with the

probability of at least , with ,   (0, 1]. The computation oracle separates the learning task from

the offline computation task and is extensively used in the literature [GKJ12, CWY13, CHL+16].

For the semi-bandit feedback to work effectively, we assume some smoothness properties on the reward function. These smoothness properties ensure that when the learning parameters are estimated with a certain precision, one can approximate the true reward with high accuracy. Formally, the reward function RS(µ), as a function of stochastic parameter µ, satisfies the following properties.
Property 1. Monotonicity: Let µ, µ  [0, 1]k be two vectors such that µi  µi for all i  [k] then, for any S  [k], RS(µ )  RS(µ).

The monotonicity property implies that the reward from any subset increase if the mean qualities of an base arms increase.
Property 2. Lipschitz Continuity: There exists real valued constant C  1 such that for all S  [k], we have |RS(µ) - RS(µ )|  C maxiS |µi - µi|.
Property 3. Bounded Smoothness: There exists a strictly increasing function f such that for any S  [k], |RS(µ) - RS(µ )|  f () whenever maxiS |µi - µi|  .
In our first setting we study the reward function R(·) satisfying monotonicity (Property 1) and the Lipschitz continuity property (Property 2) whereas in the setting setting we consider Property 1 and Property 3. With a slight abuse of terminology, we call the first setup as Lipschitz smoothness and the second setup (i.e. monotonicity and bounded smoothness) as the Bounded smoothness.
The reward assumptions and regret notion considered in the paper encompass many specialized settings studied in literature as a special case. For instance, additive rewards with a fixed number of arms to pull [KWAS15b], submodular rewards with volatile bandits [CXL18], average reward, and so on. However, we remark here that the technical treatment of this problem requires newer proof techniques as the existing proof techniques from combinatorial bandits setup do not generalize trivially to the sleeping combinatorial bandits setting.
1That is for any set S  [k], RS(µ) = RS(µ ) if µi = µi for all i  S.

3

Main Results of the Paper
· In the Lipschitz smoothness setting, we show that CS-UCB achieves O(log(T )/min) instancedependent regret guarantee (See Theorem 1). Here, min is the difference between the reward from an optimal super-arm and a sub-optimal super-arm with maximum reward.
· Note that for smaller values of min, the regret guarantee of Theorem 1 regret guarantee is vacuous. In Theorem 2, we show that CS-UCB attains an instance-dependent regret guarantee of O( kT log(T )). Here,  = max/min. Note that, in contrast with Theorem 1 this result depends only on the range of rewards of super-arms. In particular, if the best and worst super-arms do not have a large reward ratio, the result in Theorem 2 is tight. We refer to this setting as weak instance-dependent.
· Next, in Theorem 3, we obtain a O( 3 kT 2 log(T )) instance-independent regret guarantee without any dependence on  in a Lipschitz smoothness setting.
· Finally, in a Bounded smoothness setting, in Theorem 4, we show that CS-UCB attains O(log(T )) regret guarantee. Though a similar result exists for the non-sleeping case [CWY13]; the regret analysis does not trivially generalize to the combinatorial sleeping MAB setting.

3 The Setting

In this paper, we consider that only a subset At  [k] of arms is available at time t. Note that,
At is revealed only at time t. Further, let St  At be the set of arms pulled by the algorithm at
time t. The set St is also called as a super-arm. To evaluate the performance of an algorithm with
limited availability of arms, we extend the notion of regret considered for classical CMAB problem
appropriately and call it a sleeping regret given by RALG(T ) := max(At)Tt=1 EALG Tt=1(RSt - RSt ) .
Here, St  arg maxSAt RS. Note that when At = [k] for all t, we recover the setting of [CWY13]. Next, we define the regret in the presence of (, )-oracle. Let, Bt be the event that an oracle returns
an -approximate solution at time t i.e. Bt = {RSt   · RSt }. Note that P(Bt)  . The expected
sleeping regret of ALG with oracle access is given by,

T

E RA L G (T

)

=

max
(At )Tt=1

ALG

( ·  · RSt - RSt ) .
t=1

(1)

Notational Setup

We begin with the additional notation required to prove the results. For each base arm i  [k], let Ni,t denotes the number of times arm i is pulled till time t and µ^i,t be the average reward obtained from arm i till (and excluding) time t. Let

µi,t := µ^i,t + 3 log(t)/2Ni,t.

(2)

Following a standard terminology, we call µi,t as the UCB estimate of arm i at time t. Furthermore, let S :=  · OPTA - RS be the regret incurred by pulling super-arm S. Here, OPTA := RS = maxSA RS denotes the optimal reward when the set of available arms is A. A super-arm S  A is bad (sub-
optimal), if S > 0. For a given A  [k], we define the set of bad super-arms as SB(A) = {S  A|S > 0}. Further, for a given A  [k], define

min(A) =  · OPTA - max RS and,
SSB (A)

4

max(A) =  · OPTA - min RS.
SSB (A)

Note that, for any availability set A, we have max(A)  min(A) > 0. The strict inequality follows from the definition of SB(A). Next, define max = maxA[k] max(A) and min = minA[k] min(A). Arm i is called saturated if it is pulled for sufficiently many number of time steps, i.e., Ni,t  t, where t works as threshold exploration. Note that a saturated arm at any time instant may become unsaturated in future. Further, we call a set At explored if all the arms in At are saturated, i.e., Ni,t  t for all i  At. First observe that if At is either empty or a singleton set then CS-UCB incurs a zero regret. Hence, without loss of generality we assume that |At|  2 for all t  T . We first make following useful observation.

Observation 1. Let t  R+ be a positive number, S  [k] be any non-empty set of arms and such that

Ni,t  t for all i  S and t :=

3

log(t) 2t

,

then

P{maxiS

|µi,t

-

µi|

<

2t}



1

-

2|S|/t3.

The proof of Observation 1 follows from Hoeffding's inequality and is presented in the supplementary material for completeness.

CS-UCB
Note that the proposed CS-UCB algorithm is the same as CUCB [CWY13] except that at each time, only a subset of the arms is available, and the regret notion considered is sleeping regret instead of conventional regret. Similar to CUCB, we assume that the algorithm has access to a (, )approximation oracle.
At each time t, CS-UCB receives the set of available arms At. If there is a base arm in At which is not pulled previously, an algorithm pulls all the available arms. For each time instances where all available arms are pulled atleast once, CS-UCB obtains St = ORACLE(µt, At). Here, µt represent the vector of UCB estimates given by Equation 2. The algorithm then pulls a super-arm St and obtain rewards RSt(µ) and an individual base arm rewards (semi-bandit feedback) Xi,t for each i  St. Finally, CS-UCB update parameters

· Ni,t+1 = Ni,t + 1(i  St)

·

µi,t+1 =

Ni,t·µ^i,t+1(iSt)·Xi,t Ni,t+1(iSt)

+

3 log(t)
Ni,t+1(iSt

)

.

Note that the regret (Equation 1) depends on the rewards from the base arm Xi,t only through R(.). Further, observe that when At = [k] for all t, the sleeping regret is same as conventional regret guarantee and CS-UCB is same as CUCB; hence the regret guarantees of [CWY13] will apply.

4 Regret Analysis of CS-UCB
In our first result, we show that under the Lipschitz smoothness setting, CS-UCB incurs a logarithmic instance-dependent regret. However, note that the regret depends inversely on the min value. That is, for arbitrarily smaller values of min, the regret bound is vacuous. In Theorem 2, we prove that the weak instance-dependent regret of the proposed algorithm is O( kT log(T )). Here,

5

 = max/min; i.e., this result depends only on the ratio of the maximum and minimum achievable rewards. Finally, in Theorem 3, we show that the instance-independent regret of the proposed algorithm is O( 3 kT 2 log(T )) in general. We begin with the following observation.
Observation 2. For all time instances t such that Ni,t > 0 and the reward function satisfies monotonicity and Lipschitz continuity (Properties 1 and 2), St  C 1 + 3 log(T )/2 .

We are ready to present out first result.
Theorem 1. The expected sleeping regret incurred by CS-UCB when the reward function satisfies Lipschitz condition (Properties 1 and 2) is given by

3 log(T ) C log(T )

RCS-UCB(T )  2kC (3)(1 +

)+3

2

min

Here,  is the Reimann zeta function and  = max/min.

Proof Outline: Set t := 6C2 log(t)/2min and t := 3 log(t)/2 t and divide the time instants into sets Te and Tu as described as follows. Let Te be the set of time instances t such that At is explored, i.e., Te = {t  T : Ni,t  t, i  At} and Tu = [T ] \ Te. Further let, for t  Tu, Ae,t be the set of saturated arms that are available at time t, i.e., Ae,t := {i|Ni,t  t} and Au,t := At \ Ae,t. We have

Tu = {t : j  Au,t}

= {t : j  Au,t  St}  {t : j  Au,t, j / St} .

D

E

We bound the sleeping regret incurred in disjoint sets Te, D and E separately. Recall that Bt is an event that the oracle returns -approximate solution i.e. RSt(µt)   · RS {µt}. We begin with following supporting lemmas.
Lemma 1. For all t  Te we have P{St  SB(At)|Bt}  2|At|t-3.

Lemma 2. |D|  k T .
Lemma 3. For all t  E we have, P{St  SB(At)|Bt}  2|St|/t3.

Lemma 1 establishes that when all the base arms in the availability set are sufficiently explored then the set St returned by the oracle is an optimal set with high probability. This result follows from the fact that, as all the available arms are sufficiently pulled in the past, µ is sufficiently close to µ. Lemma 2 follows directly from the fact that each base arm remains unsaturated till atmost T pulls.
Finally, in Lemma 3 we handle the case that the availability set contains both saturated and unsaturated base arms. Note that the previous two lemmas also hold for CMAB settings. However, in contrast with CMAB, in our setting, the availability sequence may be such that at each time instant only a few explored arms are available and this may lead to high regret. Lemma 3 dismisses this hypothesis. First, note that only those base arms that are available but not-pulled are responsible for the regret. Furthermore, if an arm is available and it is not pulled for many time instances, its UCB

6

estimate increases and hence increasing its chances of getting pulled in the future due to the monotonicity assumption. This means that an optimal subset of the availability set will be pulled after some time with high probability. The detailed proof of Lemmas 1, 2 and 3 are given in supplementary material.
Putting everything together: For a given arbitrary availability sequence (At)Tt=1, the regret of CS-UCB is given as

RCS-UCB(T ) = E

 ·  · RSt - RSt

t[T ]

E

 · RSt - RSt | Bt · 

t[T ]



P{St  SB(At)|Bt} · St

tTeE

+ P{St  SB(At)|Bt} · max · 

tD



tTeE

2

|At| t3

St

+k

T max

·

 2kC 1 +

3 log(T ) 2

 t=1

1/t3

+

6kC2 log(T ) 2min

·

max

·

6C2k log(T )

 2kC(3) 1 + 3 log(T )/2 +

.

min

(From Lemmas 1, 2 and 3) (from Observation 2)

Since the last equation holds for any arbitrary sequence (At)Tt=1, it also holds for an adversarially chosen availability sequence. This completes the proof of the theorem.

Notice that the regret guarantee in Theorem 1 depends on the value of min. If this value is sufficiently low the regret guarantee is vacuous. In the next result, we show a weak instance-dependent regret guarantee where the regret is given in terms of the ratio max/min.
Theorem 2. The weak instance-dependent sleeping regret of CS-UCB when the reward function satisfies Lipschitz condition (properties 1 and 2) is given by
RCS-UCB(T )  4C 6kT log(T ) + 2kC(3).
Here, (.) is a Reimann zeta function and  = max/min.

It is easy to see that for large values of min one can use the result of Theorem 1 to obtain the desired bound of Theorem 2. However, when min is small i.e. min < C 6k log(T )/T , the upper bound on regret is obtained by parametrized analysis with selecting parameter   (min, max] appropriately to minimize the regret. The detailed proof of Theorem 2 is given in supplementary material. Observe that the regret dependence of Theorem 2 on time horizon increase from O(log(T )) to O( T log(T )) when we consider the weak instance-dependent regret guarantee. In the next result, we further relax the dependence on instance parameters () to obtain a strong instance-independent regret guarantee of O( 3 T 2 log(T )).

7

Theorem 3. The instance-independent sleeping regret of CS-UCB when the reward function satisfies Lipschitz condition (Properties 1 and 2) is given by
RCS-UCB(T )  C(1 + ) · 3 6kT 2 log(T ) + 2kC(3) where  = (1 + 3 log(T )/2).

First, using Theorem 1 we establish that the said instance-independent upper bound holds in

this

setting

if

min



(

T log(T

)

)-1/3.

Then,

similar

to

Theorem

2,

we

split

the

regret

at

any

time

t

into

two

parts,

where

the

per

time

regret

is

at

most



and

larger

than



with





(

T log(T

)

)-1/3.

As

stated

previously, this result provides the instance-independent regret guarantee without any additional

restrictions on minimum and maximum rewards.

Theorem 4. The expected sleeping regret incurred by CS-UCB when the reward function satisfies bounded smoothness condition (Properties 1 and 3), is upper bounded by

6 log(T ) RCS-UCB(T )  (f -1(min))2 + 2(3) k · max.

A detailed proof is provided in supplementary material. Note that the proof technique closely follow Theorem 1. We remark here that we recover the regret bound of [CWY13] for non-sleeping ttchiovemeO,b~ii(.en.aTRto)Srtriae=lgbreatniudSpittpsXecria,tbseoonui.ene.dc,awhnohaledcnhsiAuevnt ed=eO~r[k(b]ofTour)nadrelelgdtr.esFtmuboorotuhtnhedrn,e[osKbsWsaesArsvSue1mt5hpba]tt;iohwnoh.weFeninvraeelrwlyita, irtshdens oiantrsectlaaednarcdeii--f independent regret (Theorem 2 and Theorem 3) guarantee under bounded smoothness condition follows trivially by choosing C = supx[0,1] f (x).

5 Simulation Results
In this section we validate the theoretical results of the paper using different reward functions on simulated data. In particular, we perform experiments on two different combinatorial bandits settings studied in the literature [JGB+18, KWAS15a]. In the first setting, the average quality of base arm i takes the form ai · µi - bi; here µi is a mean of the Bernoulli random variable and ai and bi are unknown constants. In this setting, the quality is also referred to as the utility from arm i; where ai · Xi being random reward with mean µi and bi being the fixed cost corresponding to arm i. We call this reward setting 2 as UtilReward. The goal is to select all the available base arms with positive quality. In the second setting (which we call TopKReward), we consider the problem of pulling top K (in terms of quality) available arms and the reward function is additive 3. In this setting, note that, if at some time t, |At|  K, all the available arms are pulled and the regret at time instant t is zero. Further observe that, both the settings admit polynomial time exact oracles; i.e. (1, 1)-ORACLE.
Simulation Setup and Observations
We run two experiments for each of the reward settings mentioned above. In the first experiment which we call ExpOne, the quality parameter µi of each of the base arms i is chosen independently
2See [JGB+18] for detailed motivation and applications of this setting. 3More details and the regret analysis in non-sleeping case is given in [KWAS15a].
8

Figure 1 Regret Vs Time Plots For UtilReward: From L to R, (a) ExpOne: Instance-dependent regret with randomly generated qualities (Theorem 1) (b) ExpOne: Instance-dependent guarantee for min = 0.001 (c) ExpTwo: Weak instance-dependent guarantee (Theorem
2) (d) ExpTwo: Instance-independent regret guarantee (Theorem 3).

from uniform distribution over interval [0.3, 0.8]. A quality feedback from the base arm i  St is an independent sample from a Bernoulli distribution with mean µi. The availability parameter corresponding to arm i is uniformly sampled from [0.4, 0.9]. Similar to the quality feedback, availability of i is decided by a random draw from a Bernoulli distribution with a given availability parameter.

The second experiment, ExpTwo, is designed to validate the results of Theorem 2 and 3. The
availability of base arms is generated using same approach as in the first experiment. However, the
qualities of base arms is fixed to be close to each other. We validate the result of Theorem 2, by fixing
the value of  := max/min and varying the values of min, and Theorem 3 by varying the values of min. Each of the experiments is executed over time horizon T = 106 and the average rewards from 50 independent runs.

We present the plots associated to UtilReward reward function in Fig. 1. The first two plots in Fig.

1 show that as min value decreases, The next two plots show that the the

the expected regret guarantee of Theorem 1 becomes vacuous.

 regret dependence on time horizon increases from T

to

3

T2

for similar values of min when we fix  and change min to arbitrary values of min and max.

Similar results were observed for different values of , k, min and reward function TopKReward (Fig.

2).

6 Related Work
The stochastic bandits problem has been extensively studied in the literature [LR85, ACBF02, Tho33, AG12]. We refer the reader to [LS18, Sli19] for a book exposition on multi-armed bandits and their applications. Most previous work in literature-- with few exceptions such as [CGJ+17, KNMS10, LLJ19] -- assume that all the arms are available at all time instants. It is shown that the classical
9

Figure 2 Regret Vs Time Plots For TopKReward: From L to R, (a) ExpOne: Instance-dependent regret with randomly generated qualities (Theorem 1) (b) ExpOne: Instance-dependent guarantee for min = 0.001 (c) ExpTwo: Weak instance-dependent guarantee (Theorem
2) (d) ExpTwo: Instance-independent regret guarantee (Theorem 3).
algorithms, adapted appropriately, are also optimal in a sleeping bandits setting [CGJ+17, KNMS10].
Combinatorial multi-armed bandits (CMAB) is another well studied variant of stochastic MAB problem which considers multi-pull setup [CBL12, CLK+14, CWY13, CTMSPl15, GKJ10, GKJ12, KWAS15b, LLJ19, Ont13, WC18, WKA15]. [CWY13] consider a general reward function with some smoothness condition and proposed CUCB, a UCB-style algorithm. In contrast, we consider arbitrary arm availability and general rewards and show that CUCB when extended to sleeping bandits setting achieves optimal regret guarantee. We also remark here that their analysis does not generalize to combinatorial sleeping bandits, and hence we need novel proof techniques to bound the sleeping regret in a CMAB setting. To the best of our knowledge, we are the first to address combinatorial sleeping MAB with a general reward structure and provide instance-dependent as well as instance-independent regret upper bound.
The closest work to this work is [CXL18]. Similar to their work we consider semi-bandit feedback and combinatorial sleeping bandits framework. However, [CXL18] considers contextual bandits setting, whereas we study a sleeping combinatorial MAB setting. The proposed algorithms (CC-MAB and CS-MAB, respectively) differ crucially in how they carry out exploration. CC-MAB explores the
10

subset of available arms if it contains at least a single unsaturated base arm ([CXL18], Algorithm 2,

Line 7). Hence, the exploitation is carried only if all the available base arms are saturated. In contrast,

CS-UCB does not demarcate the exploration and exploitation in this manner. So, even if some "obvi-

ously" bad super-arms are not explored, CS-UCB does not pull them. Also, there are following two

important differences in the setting considered. Firstly, [CXL18] consider that the reward function

is submodular, whereas we consider general reward functions. Indeed, if the reward function satis-

fies

submodularity,

our

results

can

be

extended

easily

by

considering

(1

-

1 e

)-

approximation

oracle.

Secondly, they consider that the time horizon is a-priori known to the algorithm, which may be an

unrealistic assumption in many practical cases. Note that we provide an any-time regret guarantee,

i.e., T is not given as an input to the algorithm. Also, they proved that CC-MAB achieves the re-

gret of O( 3 T 2 log(T )) for the specific case of the submodular reward function, whereas we provide

similar regret bound with more general reward functions. The recent work of [NET20] also studies

contextual combinatorial bandits set up with sleeping arms and semi-bandit feedback. The authors

consider a setting where the arms are differentiated based on the context.

7 Conclusion and Future Work
In this paper, we considered combinatorial sleeping multi-armed bandits setting where a subset of arms is available at a given time instant. We analyzed the CS-UCB algorithm and analyzed its regret guarantee under two setups; Lipschitz smoothness and Bounded smoothness. We showed that under Lipschitz smoothness setting, CS-UCB achieves O(log(T )/min) instance-dependent sleeping regret guarantee. Additionally, we prove that CS-UCB achieves O( T log(T )) weak instancedependent regret under the assumption that the ratio of maximum and minimum achievable rewards is bounded. Also, we provide O( 3 T 2 log(T )) instance-independent regret in the most general case. We also show that CS-UCB in Bounded smoothness setting matches the conventional regret guarantee for the combinatorial MAB setting under the same set of assumptions (i.e., O(log(T ))). Finally, we validate the proven theoretical guarantees through experiments.
The instance-independent regret guarantee under Bounded smoothness setting remains an interesting open problem. Also, a finely tuned analysis with availability specific regret guarantees is an interesting future direction. This setup could be used together with other MAB settings, for instance, rotting bandits [LCM17], where the arm pulling strategy may lead to the dropping of the arms.

References

[AB09]

Jean-Yves Audibert and Se´bastien Bubeck. Minimax policies for adversarial and stochastic bandits. In COLT, January 2009. (Cited on page 2)

[ACBF02] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. Machine learning, 2002. (Cited on page 9)

[AG12]

Shipra Agrawal and Navin Goyal. Analysis of thompson sampling for the multi-armed bandit problem. In COLT, 2012. (Cited on page 9)

11

[BPSF13] Zahy Bnaya, Rami Puzis, Roni Stern, and Ariel Felner. Volatile multi-armed bandits for guaranteed targeted social crawling. AAAI, 2(2.3):16­21, 2013. (Cited on page 2)

[CBL12] [CGJ+17] [CHL+16]

Nicolo Cesa-Bianchi and Ga´bor Lugosi. Combinatorial bandits. Journal of Computer and System Sciences, 2012. (Cited on pages 2 and 10)
Aritra Chatterjee, Ganesh Ghalme, Shweta Jain, Rohit Vaish, and Y Narahari. Analysis of thompson sampling for stochastic sleeping bandits. In UAI, 2017. (Cited on pages 9 and 10)
Wei Chen, Wei Hu, Fu Li, Jian Li, Yu Liu, and Pinyan Lu. Combinatorial multi-armed bandit with general reward functions. In NIPS, 2016. (Cited on page 3)

[CKRU09] [CLK+14]

Deepayan Chakrabarti, Ravi Kumar, Filip Radlinski, and Eli Upfal. Mortal multi-armed bandits. In NIPS. 2009. (Cited on page 2)
Shouyuan Chen, Tian Lin, Irwin King, Michael R Lyu, and Wei Chen. Combinatorial pure exploration of multi-armed bandits. In NIPS. 2014. (Cited on page 10)

[CTMSPl15] Richard Combes, Mohammad Sadegh Talebi Mazraeh Shahi, Alexandre Proutiere, and marc lelarge. Combinatorial bandits revisited. In NIPS. 2015. (Cited on pages 2 and 10)

[CWY13] Wei Chen, Yajun Wang, and Yang Yuan. Combinatorial multi-armed bandit: General framework and applications. In ICML, 2013. (Cited on pages 2, 3, 4, 5, 8, and 10)

[CXL18]

Lixing Chen, Jie Xu, and Zhuo Lu. Contextual combinatorial multi-armed bandits with volatile arms and submodular reward. In NIPS, 2018. (Cited on pages 3, 10, and 11)

[GKJ10]

Yi Gai, Bhaskar Krishnamachari, and Rahul Jain. Learning multiuser channel allocations in cognitive radio networks: A combinatorial multi-armed bandit formulation. In IEEE DySPAN, 2010. (Cited on pages 2 and 10)

[GKJ12]

Yi Gai, Bhaskar Krishnamachari, and Rahul Jain. Combinatorial network optimization with unknown variables: Multi-armed bandits with linear rewards and individual observations. IEEE/ACM Transactions on Networking, 2012. (Cited on pages 1, 3, and 10)

[Hoe63] [JGB+18]

Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the American Statistical Association, 58(301):13­30, 1963. (Cited on page 15)
Shweta Jain, Sujit Gujar, Satyanath Bhat, Onno Zoeter, and Y Narahari. A quality assuring, cost optimal multi-armed bandit mechanism for expertsourcing. Artificial Intelligence, 254:44­63, 2018. (Cited on page 8)

[KNMS10] Robert Kleinberg, Alexandru Niculescu-Mizil, and Yogeshwer Sharma. Regret bounds for sleeping experts and bandits. Machine learning, 80(2-3), 2010. (Cited on pages 1, 2, 9, and 10)

[KWAS15a] Branislav Kveton, Zheng Wen, Azin Ashkan, and Csaba Szepesvari. Combinatorial cascading bandits. In NIPS, pages 1450­1458, 2015. (Cited on pages 2 and 8)

12

[KWAS15b] Branislav Kveton, Zheng Wen, Azin Ashkan, and Csaba Szepesvari. Tight regret bounds for stochastic combinatorial semi-bandits. In Artificial Intelligence and Statistics, 2015. (Cited on pages 3, 8, and 10)

[LCM17]

Nir Levine, Koby Crammer, and Shie Mannor. Rotting bandits. In NIPS, 2017. (Cited on page 11)

[LLJ19]

Fengjiao Li, Jia Liu, and Bo Ji. Combinatorial sleeping bandits with fairness constraints. IEEE Transactions on Network Science and Engineering, 2019. (Cited on pages 2, 9, and 10)

[LR85]

Tze Leung Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances in applied mathematics, 6(1), 1985. (Cited on page 9)

[LS18]

Tor Lattimore and Csaba Szepesva´ri. Bandit algorithms. preprint, 2018. (Cited on page 9)

[NET20]

Andi Nika, Sepehr Elahi, and Cem Tekin. Contextual combinatorial volatile multiarmed bandit with adaptive discretization. In AISTATS, 2020. (Cited on page 11)

[Ont13]

Santiago Ontano´ n. The combinatorial multi-armed bandit problem and its application to real-time strategy games. In AIIDE, 2013. (Cited on page 10)

[Sli19]

Aleksandrs Slivkins. Introduction to multi-armed bandits. Foundations and Trends® in Machine Learning, 2019. (Cited on page 9)

[Tho33] [TZC+17]

William R Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3/4), 1933. (Cited on page 9)
Mohammad Sadegh Talebi, Zhenhua Zou, Richard Combes, Alexandre Proutiere, and Mikael Johansson. Stochastic online shortest path routing: The value of feedback. IEEE TACON, 2017. (Cited on page 2)

[uHC16]

Umair ul Hassan and Edward Curry. Efficient task assignment for spatial crowdsourcing: A combinatorial fractional optimization approach with semi-bandit learning. Expert Systems with Applications, 58:36­56, 2016. (Cited on page 2)

[WC18]

Siwei Wang and Wei Chen. Thompson sampling for combinatorial semi-bandits. In ICML, 2018. (Cited on pages 2 and 10)

[WKA15] Zheng Wen, Branislav Kveton, and Azin Ashkan. Efficient learning in large-scale combinatorial semi-bandits. In ICML, 2015. (Cited on page 10)

[WN99]

Laurence A Wolsey and George L Nemhauser. Integer and combinatorial optimization. 1999. (Cited on page 3)

13

A Preliminaries
Definition 1. Let X1, X2, ....., Xn be n independent random variables, and Sn = X1 + X2 + .... + Xn, where i, Xi  [ai, bi], then according to Hoeffding's inequality,
-2t2
P{Sn - E[Sn]  t}  e i(bi-ai)2

A.1 Notation

[k]
T
At µi µt µt St RSt St SB (A) Xi,t Ni,t µ^i,t
i,t
t Te Tu Ae,t Au,t St

Set of arms. Time horizon (or the number time steps). Set of arms available at time t. Bernoulli parameter (or mean) for arm i. The vector of mean rewards of each arm in set At  [k] at time t. The vector of UCB estimates of means (unknown) for each arm in set At  [k] at time t The subset of arms (super-arm) pulled at time t. Reward obtained when super-arm St is pulled at time t. arg maxSAt RS The set of all bad super-arms i.e. SB(A) = {S  A|S > 0}. A random reward obtained at time t from arm i. Number of times arm i is pulled till t time steps. Xi,1:t/Ni,t; Empirical estimate of arm i till time step t.
3 log(t)/2Ni,t; UCB confidence interval of arm i till step t.
3 log(t)/2 t. The set of time instances t such that At is explored, i.e., all arms in At are saturated. [T ] \ Te. The set of saturated arms that are available at time t. At \ Ae,t  · RSt (µ) - RSt(µ); Quantitative measure for sub-optimality of super-arm St.
Table 1 Notation Table

A.2 Algorithm: CS-UCB

14

Algorithm 1 CS-UCB

1: Initialization:

2: for i  [k] do

3: Ni,0 = 0, µi,0 = 1, Xi,0 = 0 4: end for

5: for t = 1, 2, 3, . . . do

6: Observe set of available arms as At

7: if j  At, such that, Nj,t = 0 then

8:

Select St = At

9: else

10:

St = ORACLE(At, µt)

11: end if

12: Observe: Semi-bandit feedback as Xj,t  {0, 1}, j  St and RSt(µ); 13: Update:

· Ni,t = Ni,t-1

if i / St

Ni,t-1 + 1 if i  St

·

Xi,1:t =

Xi,1:t-1

if i / St

Xi,1:t-1 + Xi,t if i  St

·

µi,t

=

Xi,1:t Ni,t

+

3 log(t) 2Ni,t

14: end for

B Omitted Proofs

We begin with introducing additional notation used in rest of the paper. Let RS(µ; µ) denote the reward obtained according to the quality vector µ from the set S which also satisfies the condition that RS(µ)   · RS (µ). Here, RS = maxSA RS(µ).

Observation 1. Let t  R+ be a positive number, S  [k] be any non-empty set of arms and such that

Ni,t  t for all i  S and t :=

3

log(t) 2t

,

then

P{maxiS

|µi,t

-

µi|

<

2t}



1

-

2|S|/t3.

Proof. Using Hoeffding's lemma [Hoe63] we have,

P{|µ^i,t - µi|  t}  2e-2Ni,t2t = 2e-3Ni,t log(t)/ t  2/t3.

(3)

Here, µ^i,t is the empirical mean reward of arm i till time t. The last inequality follows from the fact

that Ni,t 

t.

Further,

from

the

definition

of

µ,

with

probability

at

least

1

-

2 t3

,

t > |µ^i,t - µi| = |µi,t - µi - t|  |µt - µi| - t.

(4)

(i)

(ii)

(iii)

Here, (i) follows from Equation 3, (ii) is immediate from the definition of µ and finally (iii) follows from the triangle inequality. Thus, we have |µi,t - µi| < 2t with probability atleast 1 - 2|S|/t3.

15

Observation 2. For all time instances t such that Ni,t > 0 and the reward function satisfies monotonicity and Lipschitz continuity (Properties 1 and 2), St  C 1 + 3 log(T )/2 .

Proof. The monotonicity property and Lipschitz smoothness implies that,

|RSt (µt)

-

RSt (µ)|

=

RSt (µ)

-

RSt (µ)



C

max
iSt

|µi,t

-

µi|

(Monotonicity property and Lipschitz property)

However,

RSt (µt) - RSt (µ)   · RSt (µt) - RSt (µt)   · RSt (µt) - RSt (µ) = St .

(5)

Further, from the definition of µi,t = µ^i,t + i,t, where i,t = have
|µi,t - µi|  |µ^i,t - µi| + i,t  1 +

3 log(T )/2Ni,t for arm i till time t, we

3 log(T )/2.

(6)

From Eq. 3 and Eq. 4, observe that St  C maxiSt |µi,t - µi|  C(1 + 3 log(T )/2).
Lemma 1. For all t  Te we have P{St  SB(At)|Bt}  2|At|t-3.

Proof. Let St = arg maxSAt RS and the event Bt has occurred. We prove the lemma using the following supporting claim.

Claim 1.

Let t



Te

and St

=

ORACLE(At, µ) and St

=

ORACLE(At, µ).

Then P{RSt(µ)

=

R (µ)}
St



1 - 2|At|/t3.

To see the proof of the lemma observe that

RSt (µ)

=

R (µ)
St





·

RSt

(µ)

=



·

OPTµ(At).

The first equality in the above equation is true with probability atleast 1 - 2|At|/t3 from Claim 1.
The first inequality holds from the fact that the event Bt has occurred. Hence, we have P(St /
SB(At)|Bt)  1 - 2|At|/t3. This completes the proof of the lemma.

Proof of Claim 1. First note that, it is enough to show that St = St. However, these sets might not be
unique and hence we assume St = St. Let Qt  arg maxSAt RS(µ) and St  arg maxSAt RS(µ). From the monotonicity property of R and the definition of µ it holds that

RSt

(µt)



R
St

(µt)



R
St

(µ)

(7)

Here, the first inequality follows from the optimality of St with respect to µt and the second inequality

follows

from

the

monotonicity

property.

For

contradiction,

let

us

assume

that

St

=

St

and

R (µ)
St

>

RSt(µ). Using this inequality with Equation 7 we get RSt(µt) > RSt(µ). From Lipschitz property we

have,

RSt (µt)

-

RSt (µ)

=

|RSt (µt)

-

RSt (µ)|



C

max
iSt

|µi,t

-

µi|.

(8)

16

Let t = 6C2 log(t)/2min. As t  Te, we have Ni,t  t for all i  At. Hence, from Observation

1,

with

probability

atleast

1

-

2|At t3

|

,

we

have,

maxiSt

|µi,t

-

µi|



maxiAt |µi,t - µi|

<

2t.

This

gives, RSt(µt) - RSt(µ) < 2C · t = min. To see the last inequality recall from Observation 1 that

t =

3

log(t) 2t

.

Hence,

we

have

min

>



·

RSt

(µ)

-

RSt (µ)





·

RSt

(µ)

-

RSt (µ).

This

contradicts

the

definition

of

min.

Thus,

with

probability

atleast

1

-

2|At| t3

we

have

that

RSt (µt)

=

RSt (µt).

This

completes the proof of the claim.

Lemma 2. |D|  k T .

Proof. Recall that by definition, we have |D| := |{t : j such that j  Au,t, j  St}|. Hence we have,
k
|D| = |{t : j such that Nj,T < T }|  |{t : Nj,T  T }|  k T .
j=1

Lemma 3. For all t  E we have, P{St  SB(At)|Bt}  2|St|/t3.

Proof. Consider t  E and recall from Lemma 1 that Qt = arg maxSAt RS(µ). We have,

RSt(µt)   · RS(µt ), S  At.

For all j  St we have Nj,t 

t.

Hence

from

Observation

1,

with

probability

atleast

1

-

2|St| t3

we

get,

max
jSt

|µj,t

-

µj

|

<

2t.

(9)

This implies,

|RSt (µt) - RSt (µ)| < min = RSt (µt) - RSt (µ) < min  min(At)
 · RSt (µt) - RSt (µ) < min(At).

(Lipschitz property(Property 2)) (As, RSt (µt)   · RSt (µt))

From the definition of min(At) and monotonicity property (Property 1) we have, a contradiction.

Hence, St

/ SB(At), which implies that P{St

 SB(At)} 

2|St| t3

for

all

t

 E.

Hence, P{St



SB(At)|Bt}  2|St|/t3.

Theorem 2. The weak instance-dependent sleeping regret of CS-UCB when the reward function satisfies Lipschitz condition (properties 1 and 2) is given by

RCS-UCB(T )  4C 6kT log(T ) + 2kC(3). Here, (.) is a Reimann zeta function and  = max/min.

Proof. First, consider the case min  C

6k

log(T T

)

.

From

Theorem

1

we

have,

6C2k log(T )

RCS-UCB(T ) 

min

+ 2kC(3) 1 + 3 log(T )/2

17

 C 6kT log(T ) + 2k(3)C(1 + 3 log(T )/2)  3C 6kT log(T ) + 2kC(3).

(as, min  C 6k log(T )/T )

The last inequality follows for all T  k, from the fact that 6 log(T )kC(3)  2 6 log(T )kC  2C 6kT log(T ).

Next, let min < C 6k log(T )/T . Further, let   C 6k log(T )/T be a constant. We decompose the regret St at any time t into two parts; i.e. St   and St < , respectively. Thus, instance-independent sleeping regret of CS-UCB,

T

T

RCS-UCB(T ) = E

1(St  SB(At))St = E

[1(St  SB(At), St < ) + 1(St  SB(At), St  )]St .

t=1

t=1

The first term is upper bounded by T . To bound the second term, consider a CSMAB instance such
that SB(A) = SB(A)  {S  A|St  }. In this instance we have max = max and min = . Hence,

T

RCS-UCB(T )  T +

P{St  SB(At)}St

t=1

6C2k log T

 T +

+ 2kC(3)(1 +

min

6C2k log T

 T +

+ 2kC(3)(1 +



3 log(T )/2) 3 log(T )/2).

(from Theorem 1) (As St  )

Choose  = C

6k log T T

1/2
to get the desired upper bound.

Theorem 3. The instance-independent sleeping regret of CS-UCB when the reward function satisfies Lipschitz condition (Properties 1 and 2) is given by

RCS-UCB(T )  C(1 + ) · 3 6kT 2 log(T ) + 2kC(3) where  = (1 + 3 log(T )/2).

Proof. Let the regret of selecting super-arm St at round t be, St := ·OPTAt -RSt(µ), where OPTAt :=

RSt = maxSAt RS. From Theorem 1 it is easy to see that the said instance-independent upper bound

holds

if

min



(

T log(T

)

)-1/3

.

Hence,

without

loss

of

generality

let

min

<

(

T log(T

)

)-1/3.

Further,

let





(

T log(T

)

)-1/3

be

a

constant.

We

decompose

the

regret

at

any

time

t

into

two

parts,

where

the

per

round

regret is at most  and larger than . From Observation 2, observe that St  C 1 + 3 log(T )/2 .

Let  = 1 + 3 log(T )/2 . Thus, instance-independent sleeping regret of CS-UCB,

T

RCS-UCB(T ) = E

( ·  · RSt - RSt ))

t=1

T

T

=

P{St  SB(At)|Bt}St1{St < } + P{St  SB(At)|Bt}St1{St  } · 

t=1

t=1

18

T
 T + P{St  SB(At)  St  |Bt}St
t=1

(  1)

= T +

P{St  SB(At)  St  |Bt}St + P{St  SB(At)  St  |Bt}St

tTeE

tD

 T + 2(3)kC 1 + 3 log(T )/2 + C 1 + 3 log(T )/2

P{St  }

tD

(Observation 2)

 T + 2kC(3) + C P{St  }
tD
6C2 log(T )  T + 2kC(3) + kC 2 .

Choose  = C

6k log T T

1/3
to get the following sleeping regret:

RCS-UCB(T )  C(1 + ) · 3 6kT 2 log(T ) + 2kC(3).

Theorem 4. The expected sleeping regret incurred by CS-UCB when the reward function satisfies bounded smoothness condition (Properties 1 and 3), is upper bounded by
6 log(T ) RCS-UCB(T )  (f -1(min))2 + 2(3) k · max.

Proof. Following the similar 3 step proof of Theorem 1. We choose with

t

=

6 log(t) (f -1(min))2

and t

=

3 log(t) 2t

and

divide

the time instants into

sets

Te

and

Tu

as

described

in

Section

4.

Step

1 and

2

is

proved as Lemma 4 and Lemma 5. Observe that Step 3 follows trivially as for in Theorem 1.

Lemma 4. Let t  Te, when the reward function satisfies monotonicity and Lipschitz smoothness property
then P{St  SB(At)|Bt}  2|At|t-3.

Proof of the lemma. Let

t

:=

6 log(t) (f -1(min))2

and t

:=

3

log(t) 2t

.

We

have

P{St  SB(At)} = P{i  At : |µ^i,t - µi|  t, St  SB(At)|Bt} + P{i  At : |µ^i,t - µi| < t, St  SB(At)|Bt}.
(10)

We first prove an upper bound on the first term on the right side of the above expression. We have, for all the arms i in At,

P{|µ^i,t - µi|  t}  2e-2Ni,t2t

= 2e-Ni,t

3

log(t) t

(from Hoeffding's inequality)

 2t-3.

(as Ni,t  t)

Using union bound we get the following upper bound on the first term

P i  At : |µ^i,t - µi|  t, St  SB(At)  P{i  At : |µ^i,t - µi|  t}  2|At|t-3.

(11)

19

Next, we bound the second term. From Equation 4 and the bounded smoothness property (Property

3),

for

any

St



At,

we

have

|R
St

(µt)

-

R (µ)|
St

<

f (2t).

In

particular,

for

the

selected

super-arm

St

we have,

|RSt(µt) - RSt(µ)| < f (2t).

(12)

This implies,

RSt (µ) + min = RSt (µ) + f (2t) > RSt (µt)   · RSt (µt)   · RSt (µ) =  · OPTAt.

(As f (2t) = min) (from Eq.12)
(As St is optimal super-arm for µ) (from the monotonicity property)

Hence, we have min >  · OPTAt - RSt(µ). This contradicts the definition of min and hence we
have that P i  At : |µ^i,t - µi| < t, St  SB(At)|Bt = P i  At : |µ^i,t - µi| < t = 0. This, Eq.
10 and Eq. 11 completes the proof of the lemma.

Lemma 5. For given t, if i  St, Ni,t  t is true and reward function satisfies monotonicity and bounded

smoothness property then

P{St



SB (At )|Bt }



2|St| t3

.

Proof.

Let

t

:=

6 log(t) (f -1(min))2

and t

:=

3

log(t) 2t

.

Consider

t



E,

where

E

=

{t



Tu|j



St, Nj,t



t},

i.e., at each t  E the each arm in super-arm are saturated. We have,

RSt (µt)  RS(µt), S  2At .

(13)

Let St = arg maxSAt RS(µ) be an optimal super-arm for given available arms At at time t. For all

j  St we have Nj,t >

t.

Hence

from

Observation

1,

with

probability

atleast

1

-

2|St| t3

we

have,

max
jSt

|µj,t

-

µj

|



2t.

(14)

This implies,

|RSt (µt) - RSt (µ)| < min

(Property 3, t and Ni,t  t)

= Rµt (St) - RSt (µ) < min  min(At)

RSt (µt) - RSt (µ) < min(At)

(As, RSt (µt)  RSt (µt))

=

RSt (µ)

>

max
SSB (At)

RS

(µ).

(by definition of min(At) and monotonicity property)

Hence,

we

have

P{St

/

SB (At )|Bt }



1-

2|St t3

|

,

which

implies

that

P{St



SB (At )|Bt }



2|St| t3

for

all

t  E.

Putting everything together:

With this, the upper bound on sleeping regret of CS-UCB under Bounded smoothness setting is

RCS-UCB(T )  · max

P{St  SB(At)|Bt} + P{St  SB(At)|Bt} + P{St  SB(At)|Bt}

tTe

tE

tD

20

 ·

T t=1

2

max t3

|St|

+

max|D|

 2(3)kmax + k T max ·  =

(From Lemma 2, Lemma 4, and Lemma 5) 6C2 log(T ) (f -1(min))2 + 2(3)  · k · max.

21

