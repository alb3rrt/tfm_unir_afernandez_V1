
# Framing RNN as a kernel method: A neural ODE approach

[arXiv](https://arxiv.org/abs/2106.01202), [PDF](https://arxiv.org/pdf/2106.01202.pdf)

## Authors

- Adeline Fermanian
- Pierre Marion
- Jean-Philippe Vert
- Gérard Biau

## Abstract

Building on the interpretation of a recurrent neural network (RNN) as a continuous-time neural differential equation, we show, under appropriate conditions, that the solution of a RNN can be viewed as a linear function of a specific feature set of the input sequence, known as the signature. This connection allows us to frame a RNN as a kernel method in a suitable reproducing kernel Hilbert space. As a consequence, we obtain theoretical guarantees on generalization and stability for a large class of recurrent networks. Our results are illustrated on simulated datasets.

## Comments

32 pages, 7 figures

## Source Code

Official Code

- [https://github.com/afermanian/rnn-kernel](https://github.com/afermanian/rnn-kernel)

Community Code

- [https://paperswithcode.com/paper/framing-rnn-as-a-kernel-method-a-neural-ode](https://paperswithcode.com/paper/framing-rnn-as-a-kernel-method-a-neural-ode)

## Bibtex

```tex
@misc{fermanian2021framing,
      title={Framing RNN as a kernel method: A neural ODE approach}, 
      author={Adeline Fermanian and Pierre Marion and Jean-Philippe Vert and Gérard Biau},
      year={2021},
      eprint={2106.01202},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
```

## Notes

Type your reading notes here...

