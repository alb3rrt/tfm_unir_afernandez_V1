arXiv:2106.00554v1 [math.NA] 1 Jun 2021

Recovering wavelet coefficients from binary samples using fast transforms
Vegard Antun
June 2, 2021
Abstract Recovering a signal (function) from finitely many binary or Fourier samples is one of the core problems in modern medical imaging, and by now there exist a plethora of methods for recovering a signal from such samples. Examples of methods, which can utilise wavelet reconstruction, include generalised sampling, infinite-dimensional compressive sensing, the parameterisedbackground data-weak (PBDW) method etc. However, for any of these methods to be applied in practice, accurate and fast modelling of an N ^ M section of the infinite-dimensional change-ofbasis matrix between the sampling basis (Fourier or Walsh-Hadamard samples) and the wavelet reconstruction basis is paramount. In this work, we derive an algorithm, which bypasses the N M storage requirement and the OpN M q computational cost of matrix-vector multiplication with this matrix when using Walsh-Hadamard samples and wavelet reconstruction. The proposed algorithm computes the matrix-vector multiplication in OpN log N q operations and has a storage requirement of Op2qq, where N " 2dqM , (usually q P t1, 2u) and d " 1, 2 is the dimension. As matrix-vector multiplications is the computational bottleneck for iterative algorithms used by the mentioned reconstruction methods, the proposed algorithm speeds up the reconstruction of wavelet coefficients from Walsh-Hadamard samples considerably.
Keywords: Fast transforms, Sampling theory, Wavelets, Walsh functions, Walsh-Hadamard samples.
Mathematics Subject Classification (2010): 94A20, 94A11, 42C10, 42C40, 46C05.
1 Introduction
Approximating a function from finitely many samples is one of the fundamental problems in approximation theory, and, by now, there exist myriads of conditions and algorithms for obtaining good function approximation. The problem is often motivated by the many applications in natural sciences where one is given a finite set of samples of an underlying unknown signal (function) that one wants to recover (approximate).
In this work, we consider the recovery of signals, where physical constraints dictate the type of samples one can acquire. This is a well-studied problem with numerous applications in medical imaging. Examples include Magnetic Resonance Imaging (MRI) [41, 42], surface scattering [37, 38], X-ray Computed Tomography (CT) [27] and electron microscopy [40], all of which employ Fourier
Department of Mathematics, University of Oslo, Norway. (vegarant@math.uio.no)
1

sampling. Other examples, employing binary samples, include fluorescence microscopy [52, 49], lensless imaging [15] and compressive holography [19].
Given the long list of applications, there are many efficient methods for reconstructing a function from a fixed sampling modality. Examples of such methods include generalised sampling [2, 5, 6, 9, 36, 43], studied by Adcock, Hansen, Hrycak, Gr¨ochenig, Kutyniok, Ma, Poon, Shadrin and others, its predecessor; consistent sampling [24, 25, 26, 35, 57, 58], developed by Aldroubi, Eldar, Unser and others. More recently Adcock, Antun, Hansen, Kutyniok, Lim, Poon, Thesing and many others have developed reconstruction methods based on infinite-dimensional compressive sensing [1, 3, 7, 39, 50, 55]. Other approaches can be found within data assimilation. A first approach here was introduced by Maday & Mula in [45], called generalised empirical interpolation method, this was later followed by the Parametrized Background Data-Weak (PBDW) method, developed by Maday, Patera, Penn & Yano in [44, 46], and later analysed by Binev, Cohen, Dahmen, DeVore, Petrova, and Wojtaszczyk in [14, 22].
We model the problem as follows. Let H be an infinite-dimensional separable Hilbert space with inner product x¨, ¨y and norm } ¨ }. Let tsk : k P Nu and trk : k P Nu be two orthonormal bases for H, called the sampling and reconstruction basis, respectively. Furthermore define the sampling space, as the linear span SN " spants1, . . . , sN u and the reconstruction space as RM " spantr1, . . . , rM u.
Suppose that we can only observe the function f P H, using finitely many linear measurements xf, sky, k " 1, . . . , N . Since tsk : k P Nu is an orthonormal basis, this immediately gives the truncated series approximation

fN " y1s1 ` ¨ ¨ ¨ ` yN sN P SN

(1)

where yk " xf, sky. In all the applications mentioned above, we have limited freedom in designing the sampling basis tsk : k P Nu and the approximation fN may, therefore, suffer from unpleasant reconstruction artefacts due to the characteristics of the sampling basis, slow convergence rates or
the Gibbs phenomenon. An example of such artefacts can be seen in Figure 1. Here we have chosen H " L2pr0, 1sq
and consider the Fourier sampling basis tp2q´1{2e2in : n P Zu and the Walsh sampling basis twn : n P Z` :" t0, 1, . . . , uu, where the wn's are Walsh functions (see §3.1 for more on these functions, and their relation to Hadamard matrices). In the figure, we can see how the Walsh
sampling basis gives a blocky approximation to the continuous hat functions and how the Fourier
sampling basis, (no matter how large we choose N ), always produce the very characteristic Op1q Gibbs oscillations around the discontinuity. This is because fN only converges to f in the 2-norm, rather than the stronger uniform norm.
To resolve this issue, the idea of the aforementioned reconstruction techniques is to utilise prior
knowledge on f , to compute a better approximation in the reconstruction space RM , using the samples ty1, . . . , yN u. In this work RM is spanned by orthonormal wavelets and we consider H " L2pr0, 1sdq, for d " 1, 2. This reconstruction space has several advantages.

(i) Orthonormal wavelets can be computed with any desired degree of smoothness, ranging from the discontinuous Haar wavelet to higher-order Daubechies wavelets or symlets. This means that we can tailor-make the smoothness of the reconstruction space.

(ii) In one dimension, orthonormal wavelets allows for optimal non-linear approximation of functions with bounded variations [4, Ch. 10] (see also [23]) and while wavelets are not provably optimal in two dimensions, their use and applicability in imaging is ubiquitous [31, 42, 51].

(iii) For Walsh sampling (considered in this work) and orthonormal wavelet reconstruction, the so-called stable sampling rate (see Def. 2.2) is linear [33]. That is, to recover M wavelet

2

f ptq

gptq

1

f(t)

1

g(t)

0.8

0.8

0.6

0.6

0.4 0.4

0.2
0.2 0

0

0

0.5

1

0

0.5

1

Fourier: fN , N " 16

Fourier: fN , N " 256

Walsh: gN , N " 64

1

Fourier

1

Fourier

1

Walsh

0.8

0.8

0.8

0.6

0.6

0.6

0.4

0.4

0.4

0.2

0.2

0.2

0

0

0

0

0.5

10

0.5

10

0.5

1

Figure 1: (Undesirable artefacts). The two functions f and g (top row) are sampled using a Fourier and Walsh
sampling basis, respectively. Given the acquired samples, we use the native truncated Fourier series fN and truncated Walsh series gN , known from (1), to approximate the functions. On the bottom row we show reconstructions fN and gN , for different values of N . Notice how the truncated Fourier series causes Op1q Gibbs oscillations around the discontinuity for every choice of N , and how the truncated Walsh series produce a reconstruction with blocky
artefacts.

coefficients using, e.g., generalised sampling, we require N  CM Walsh samples, where C  1 is a constant. We note that this rate is not necessarily linear for all reconstruction bases. For Fourier sampling and polynomial reconstruction, the requirement is quadratic in M , i.e., N  CM 2 samples are required [36]. For Walsh sampling and polynomial reconstruction, the stable sampling rate is not known.
1.1 Notation
Let 2pN q denote the usual set of square summable sequences, and let Bp 2pNqq, denote the set of bounded linear operators between such sequences. For   t1, . . . , N u, we let P : 2pNq Ñ 2pNq be the projection onto the coordinates indexed by . That is, for z P 2pNq, pPzqi " zi if i P , and 0 otherwise. Let m " ||. We sometimes abuse notation slightly and say that P : 2pNq Ñ Cm, by simply ignoring all the zero entries. Furthermore, if  " t1, . . . , N u we simply write PN . Often we do not specify the domain and range of PN , and let this be given by the context. Thus for an operator U P Bp 2pNqq, we write PN U PM both to mean a finite dimensional N ^ M matrix and an operator in Bp 2pNqq, depending on the context. When PM : 2pNq Ñ CM , we have that PM° : CM Ñ 2pNq, however, to unify the notation we still write PN U PM , rather than PN U PM° .
Finally, for some closed subspace V  H we let PV : H Ñ H denote the projection onto V.
1.2 Computing approximations in RM
For f P H, let xk " xf, rky and yk " xf, sky be the coefficients of f in the reconstruction basis and sampling basis, respectively. Let x " txkukPN and y " tykukPN and notice that x, y P 2pNq. The
3

GS rec. fr pM " 8, N " 16q from Fourier samples

1

GS

GS

rec.

g r

pM

"

32, N

"

64q

from Walsh samples

1

GS

0.8

0.8

0.6 0.6

0.4 0.4

0.2 0.2

0

0

0

0.5

1

0

0.5

1

Figure 2: (Improved reconstruction in RM ). Given N " 16 Fourier samples from the function f in Figure 1

and

N

"

64

Walsh

samples

from

the

function

g

in

the

same

figure,

we

compute

approximations

fr and

g, r

respectively,

using generalised sampling (GS). Here we use a Haar wavelet basis with M " 8 functions for fr, and a Daubechies

2 (DB2) wavelet basis with M " 32 basis functions for the function gr. Note how increasing N in Figure 1 can not remedy the Op1q Gibbs oscillation for the discontinuous Haar scaling function f seen in the figure, whereas choosing

a basis which spans this function enables us to capture f , using only N " 16 samples.

change-of-basis matrix U P Bp 2pNq between trk : k P Nu and tsk : k P Nu, is given by
Ui,j " xrj, siy , and y " U x,
where U is unitary, since both bases are orthonormal. Given a finite set of (noiseless) samples, the previously mentioned reconstruction techniques
compute an approximation to f , by utilising the reconstruction space RM . We review three of the most modern approaches.

(i) (Generalised sampling). In generalised sampling [2, 6] one has access to the N samples PN y and using these we solve the least squares problem

min }PN U PM z ´ PN y}22 , where N  M.

(2)

zPCM

Let

x r

"

txrk uM k"1

be

the

minimiser

of

(2).

In generalised sampling we approximate f with

fr " xr1r1 ` ¨ ¨ ¨ ` xrM rM P RM . Moreover, the error committed by fr, is upper bounded by [6,

Thm. 4.5]

}f ´ fr}  C1}f ´ PRM f },

(3)

where C1  0 is a constant depending on the subspace angle between SN and RM (see §2 for details).

(ii) (PBDW-method). The PBDW-method [14, 46] is a data consistent method, which approx-

imates f using the same N samples PN y as in generalised sampling. The approximation is

computed as fp " PSN f ` PSNK fr where fr is the generalised sampling approximation. As fp P H, does not lie in a finite dimensional subspace, it can not be represented on a computer. We

may, however, approximate fp, by choosing some large K  N , and use the truncated sum

fp «

N
k"1

yk sk

`

K
k"N

`1pPK

U

PM

xrqk

sk

where

xr

is

the

minimizer

form

(2).

It

was

shown

in

[46], that the error committed by fp is upper bounded by

}f ´ fp}  C1}f ´ PRM `pSN XRKM qf },

(4)

where C1 is the same constant as in the generalised sampling error bound above.

4

(iii) (Infinite-dimensional compressive sensing). While the two methods above are linear reconstruction methods, compressive sensing (and more generally sparse regularization), is an example of a non-linear reconstruction method. In compressive sensing one computes an approximation in RM using m  N samples. Let   t1, . . . , N u have cardinality m " || and consider the measurements Py. A standard way of computing a compressive sensing reconstruction is by solving the quadratically constrained basis pursuit optimisation problem

min }z} 1 subject to }PU PM z ´ Py}22  .

(5)

zPCM

Here  is chosen so that   }PU PMK x}22 , to ensure that PM x is a feasible point. Given a minimizer x7 of (5), one approximates f with f 7 " x71r1 ` . . . x7M rM P RM . Error bounds for compressive sensing reconstructions are probabilistic in nature and depend on the number
of measurements m, and the bases tskukPN and trkukPN used. For concrete error bounds for Walsh sampling and wavelet reconstruction, we refer to [55] for non-uniform and [1] uniform
recovery guarantees in infinite-dimensions. For a more general treatment of the subject, we
refer to [4, 28].

1.3 Contributions
In this work, we let H " L2pr0, 1sdq, d " 1, 2 and consider the recovery of orthonormal wavelet coefficients from Walsh samples (also called Walsh-Hadamard, or just Hadamard samples). As outlined above, this setup has numerous applications in binary imaging [15, 19, 49, 52]. However, for any of the reconstruction methods mentioned above to work in practice, it is crucial to solve one of the optimisation problems (2) or (5). To do this, we need to form the matrix PN U PM (potentially also PU PM ), for different values of N and M . This can be computationally challenging since the entries of PN U PM are given as the solution of M N integrals. Furthermore, ­ ignoring the computational burden of computing these integral ­ using a densely stored matrix PN U PM has several disadvantages.
(i) (Storage). In imaging applications it is not uncommon to have large dimensions, say N " 5122 and M " 2562. However, naively storing a dense matrix PN U PM P CN^M with these dimensions requires approximately 137 GB of memory. This is substantially more than most workstations can handle.
(ii) (Computational complexity). When solving (2) or (5), iterative algorithms are often applied. For (2), the conjugate gradient method [34] is a popular choice, and for (5) SPGL1 [59] or Chambolle and Pock's primal-dual [17] algorithm are well-known choices. However, all of these algorithms rely on fast matrix-vector multiplications with PN U PM or PU PM , and their adjoins. However, standard matrix-vector multiplication with a N ^ M matrix require OpM N q operations, and for large dimensions this cost can be substantial.
While some of these issues can be reduced in higher dimensions (d  1), by considering tensor decompositions of the linear map PN U PM , none of these approaches can obtain a computational complexity of OpN log N q and avoid storing the matrix PN U PM altogether. In this work, we do exactly this. We present an algorithm, which can compute matrix-vector multiplications with the matrix PN U PM in OpN log N q1 operations for Walsh sampling and orthonormal wavelet reconstruction in one and two dimensions without storing the matrix PN U PM . Applying the reconstruction
1Note that our bound here, is independent of M , but due to the stable sampling rate (see §2), we can take N " 2dqM for small values of q, usually q P t1, 2, 3, 4u (see Rem. 4.1).

5

methods outlined above allows for fast reconstruction of wavelet coefficients from Walsh samples with minimal memory usage and computational complexity.
Our work extends the work of Gataric & Poon [29], which derives a similar algorithm for Fourier sampling and wavelet reconstruction. However, our work differs from [29] in that we utilise special properties of the Walsh functions and derive an algorithm that can be used for both vanishing moments preserving wavelets on the interval [10, 20] and periodic wavelets on the interval [47, Sec. 7.5.1]. The paper is also accompanied by a software implementation in MatLab, demonstrating how this can be implemented in practice. It is an well known issue that MatLab's implementation of the fast Walsh-Hadamard transform (FWHT), is extremely slow2. To mitigate this issue, the implementation also includes a MatLab interface to the C++ library FXT (https://www.jjj.de/ fxt/) [11], for speeding up this part of the code. Other time-critical parts of the code have also been written in C++ and interfaced with MatLab. All accompanying code and data are accessible from
https://github.com/vegarant/cww
and
https://github.com/vegarant/fastwht.

Remark 1.1 (Avoiding inverse crimes). Note that the proposed model avoids certain inverse crimes

stemming from too early discretisation of the considered inverse problem. Indeed, by considering an

infinite-dimensional model, we model measurements yk that come from continuous integral trans-

forms yk

"

1
0

f

pxqsk

pxq

dx,

rather

than

discrete

inner-products.

This model is motivated by the

observation that most sensors do not compute pointwise samples of f , but rather integrate f over

a short time or area [32, 38]. Discretising the problem at a too early stage using discrete inner

products can result in measurement mismatch [18].

Remark 1.2 (Measurement noise). Above, we have focused on noiseless measurements to make the mathematical model clear. However, any realistic measurement model should also incorporate noisy measurements. Our overall goal in this manuscript is to develop an algorithm that can compute matrix-vector multiplications with the matrix PN U PM in OpN log N q operations. We will, therefore, not discuss noisy measurements in any detail. We refer to the literature on each of the specific reconstruction methods for further discussions on how the methods handle noisy measurements.

1.4 Outline of the paper
In §2 we define the subspace angle and the stable sampling rate, and we explain how these quantities dictate how we must choose N in relation to M to achieve stable and accurate reconstruction. This is followed by the definitions of the Walsh and Wavelet sampling bases in §3, along with a key lemma used extensively in the derivation of the algorithm. We then describe the algorithm in one and two dimensions in §4 and §5, respectively, followed numerical examples in §6.

2 The subspace angle and the stable sampling rate

It is important to realize that stable and accurate recovery in RM , from samples yk " xf, sky, k " 1, . . . N , is not possible for arbitrary choices of bases tsk : k P Nu and trk : k P Nu. What is crucial for accurate and stable recovery in RM , is that the subspace angle between SN and RM is sufficiently small.

2See

https://ch.mathworks.com/matlabcentral/answers/395334-why-does-the-fwht-function-calculate-slower-

than-the-fft-function-even-though-the-documentation-say

6

Definition 2.1 (Subspace angle). Let RM " spantr1, . . . rM u and SN " spants1, . . . , sN u. The subspace angle  P r0, {2s between RM and SN is

cosppRM

,

SN

qq

:"

inf
hPRM ,}h}"1

}PSN

h}

We

set

the

reciprocal

value

as

µpRM , SN q

:"

1 cosppRM

,SN

qq

,

and

if

cosppRM , SN qq

"

0,

we

set

µpRM , SN q " 8.

We note that a necessary condition for µpRM , SN q  8 is that N  M (see e.g. [53, Thm.
2.1]). Furthermore, we have that µpRM , SN q, is related to the condition number of the matrix PM U °PN U PM , used for solving the normal equations in generalised sampling or the PBDW-method. Indeed, let 1pAq  ¨ ¨ ¨  M pAq denote the ordered singular values of a matrix A P CN^M , with
N  M . Then, using Parseval's identity, we have that

cosppRM ,

SN qq

:"

inf
hPRM ,}h}"1

}PSN

h}

"

inf
zPCM ,}z}"1

}PN U PM z}

"

M pPN U PM q.

We also have that 1pPN U PM q " supzPCM ,}z}"1 }PN U PM z}  1, since U is unitary, and hence the

condition number

condpPM U °PN U PM q

"

12pPN U PM q M 2 pPN U PM q



µ2pRM , SN q.

This directly relates to the numerical stability of the normal equations, used to solve the least-squares

problem (2), and compute the generalised sampling and the PBDW-method's solution.

Furthermore, the accuracy of these two methods is also related to the subspace angle. Indeed,

the constant C1 fund in the error bounds (3) and (4) equals C1 " µpRM , SN q. See [6, Thm. 4.5] and [14, Eq. (1.7)] (and [46] for earlier work). Thus both the numerical stability and accuracy of these

two methods hinges on choosing RM in relation to the samples one can acquire. The situation is the same in infinite-dimensional compressive sensing, but the quantity µpRM , SN q,
is camouflaged via the so-called balancing property, introduced in [3]. In infinite-dimensional com-

pressive sensing, the balancing property typically governs the required number of samples needed

to satisfy the restricted isometry property (RIP) [28], and its generalisations [1, 12, 56], for certain

constants. These constants will again affect the constants found in the error bound for the minimiser x7 of (5), see, e.g., [28] for details. To see the relation between the subspace angle and the balancing

property, we refer to the proof of Proposition 4.4 in [1].

From the above discussion, it is evident that the subspace angle between RM and SN , affects both the accuracy and the stability of all the reconstruction methods. Thus, an important question is,

therefore, how we should choose N in relation to M , to ensure that µpRM , SN q   stays bounded. This relates to the so-called stable sampling rate [3, 6].

Definition 2.2 (Stable sampling rate). Let RM " spantr1, . . . rM u and SN " spants1, . . . , sN u. The stable sampling rate for M P N and   1 is

pM, q " mintN P N : µpRM , SN q  u.
For Walsh sampling and orthonormal wavelet reconstruction in H " L2pr0, 1sdq, d  1, it was shown by Hansen & Thesing [33] that the stable sampling rate scales linearly in M . That is, for a fixed   1, there exist a constant q  0 such that whenever N " 2dpr`qq  2dr " M for r P N, we have µpRM , SN q  . Hence for a fixed q  0, we get a fixed upper bound on µpRM , SN q, for all M and N on the form above.

7

This is important, since it tells us that for a fixed number of reconstruction coefficients M , we need no more than N " CM samples, where C " 2dq is a constant, to ensure that µpRM , SN q  . In Table 1, we have computed 1{M pPN U PM q " µpRM , SN q, for N " 2dqM , for d " 1, 2 and q " 1, 2, 3, 4, for Walsh sampling and different wavelet reconstruction bases. From the table, we see
that in all cases the choice q " 1 or q " 2 is sufficient to ensure that 1    2, indicating that the
constant C is not necessarily very large for these bases.

3 The sampling and reconstruction spaces
This section introduces the necessary notation and background on the Walsh sampling basis and the orthonormal wavelet reconstruction bases. We also present a few useful results, needed to derive the final algorithm in later sections.

3.1 Walsh functions

Walsh functions (see [13] or [30] for an introduction) are closely related to dyadic representations of numbers. For an integer n P Z` " t0, 1, 2, . . .u its dyadic series is n " np1q20 ` np2q21 ` np3q22 ` ¨ ¨ ¨ , where the npjq's are binary numbers. Similarly for x P r0, 1q we can express its dyadic series as x " xp1q2´1 ` xp2q2´2 ` xp3q2´3 ` ¨ ¨ ¨ , for xpjq P t0, 1u. For rational numbers x, this expansion is not unique and in such cases we consider the expansion not ending with infinitely many repeating 1's.
There exist different orderings of Walsh functions, all of which leads to slightly different defi-
nitions. In this manuscript, we use the sequency ordered Walsh functions. This ordering has the
advantage that the n'th Walsh function wn has n sign changes.

Definition 3.1. Let n P Z` and x P r0, 1q. The Walsh function wn : r0, 1q Ñ t`1, ´1u is given by

wnpxq

:"

p´1q8 j"1

pnpj

q

`npj

`1q

qxpjq

We note that twn : n P Z`u is an orthonormal basis for L2pr0, 1sq, and we let

1
Wf pnq " f pxqwnpxq dx
0

denote the Walsh transform of a function f P L2pr0, 1sq. When working with Walsh functions, the XOR operation applied to binary sequences has many
uses. We denote it by ` and define it as follows.

Definition 3.2. Let x " txpjqu8j"1 P t0, 1uN and y " typjqu8j"1 P t0, 1uN be binary sequences. The operation ` applied to these sequences is given by x ` y :" t|xpjq ´ ypjq|u8j"1. For x, y P Z` or x, y P r0, 1q, the operation x ` y is understood in the sense of x and y's representation as binary
sequences.

Lemma 3.3. For x, y P r0, 1q, n, j, l P Z`, the following three equalities holds

wnpx ` yq " wnpxqwnpyq,

(6)

wnp2´jlq " wlp2´jnq if n, l  2j,

(7)

wnp2´j xq " wtn{2jupxq.

(8)

Proof. The two first equalities can be found in any book on Walsh functions, see e.g. [30]. The last equality follows from direct computations, see e.g. [1, Prop. 6.4].

8

Seq. ord. Walsh func. w0 w1 w2 w3 w4 w5 w6 w7

Seq. ord. Hadamard mat.

Figure 3: (Relation between Walsh functions and Hadamard matrices). Left: The eight first sequency ordered Walsh functions. Right: A 8 ^ 8 sequency ordered Hadamard matrix, where black corresponds to 1 and white to ´1. We can see that the Walsh functions' sign changes correspond to the sign changes in the matrix.

We also note that Walsh functions and Hadamard matrices are closely related and the pn, kq'th entry of a sequency ordered Hadamard matrix H P R2j^2j is given by wn´1p2´jpk ´ 1qq. See Figure 3 for an illustration of this relationship. Furthermore, for N " 2j we note that a matrix-
vector product with H can be computed in OpN log N q operations using the fast Walsh-Hadamard transform (FWHT) [13]. That is, for x " txkuNk"1, the N sums

#N

+N ´1

ÿ wnppk ´ 1q{N qxk

k"1

n"0

can utilize the FWHT algorithm to compute the result with OpN log N q operations, and without storing the matrix H in memory.

3.2 Wavelets
Let  : R Ñ R and  : R Ñ R be a compactly supported orthonormal scaling function and wavelet [21], respectively, corresponding to an multiresolution analysis (MRA). We say that the wavelet  has  vanishing moments if it is orthogonal to all polynomials of degree  ´1. That is, if xxk, yL2pRq " 0 for k " 0, . . . ,  ´1. For simplicity, we work with wavelets with minimal support. Thus, for  " 1 the above wavelet is the Haar wavelet, but for   2 there are different choices, ranging from the classical Daubechies wavelet (which has minimum-phase) to symlets which are close to being symmetric, but with a larger phase [47, p. 294].
If  generates a system of orthonormal wavelets with  vanishing moments and minimal support, then the support of  and  is an interval of size 2 ´ 1. For convenience, we use the convention that supppq " supppq " r´ ` 1, s.
Let j,mpxq :" 2j{2p2jx ´ mq and j,mpxq :" 2j{2p2jx ´ mq denote the dilated and translated versions of  and . To work on the interval r0, 1s, we need to construct bases on this interval consisting of functions j,m and j,m, with j  J0 for some J0, chosen so that supppj,mq " supppj,mq  r0, 1s for at least one choice of m. It is readily seen that if J0  rlog2p2qs for   2 and J0  0 for  " 1, then this holds for at least one m.
Constructing an orthonormal wavelet basis on the interval requires special care at the boundaries, and it is common to replace all wavelets and scaling functions intersecting the boundary with certain

9

"replacement" functions. Hence for j  J0 we define the set of functions

B,j " B,j "

rje,mp (m´"10  tj,mu2mj"´´1 

rje,mp

(2j ´1
m"2j

´

,

jr,emp

(´1
m"0



tj,mu2mj"´´1



jr,emp

(2j ´1
m"2j

´

where jr,emp and rje,mp are replacement wavelets and scaling functions supported on r0, 1s. There are several ways to construct these replacement functions so that they retain the orthonormality condition, and we consider both a periodic boundary extension and the vanishing moments preserving (VMP) boundary wavelets introduced by Cohen, Daubechies & Vial in [20].
The advantage of the former is that it is both easy to define and implement. Indeed, to compute a discrete wavelet transform (DWT) using a periodic boundary extension, one simply use a periodic convolutions between between the filters and the signal. The disadvantage of the periodic wavelets basis is that we lose the vanishing moments property at the boundaries. This may result in a few high amplitude coefficients at each scale. Another issue with these wavelets is that any 2-approximation of a non-periodic function on r0, 1s will have certain artefacts at the boundaries due to the underlying assumption of periodicity.
This can be seen in Figure 4, where we consider a generalised sampling reconstructions of the periodic function f ptq " cosp2tq and non-periodic function gptq " cosp2tq ` t, on r0, 1s. With a periodic wavelet basis, we achieve high accuracy for the periodic function f ptq, whereas we get artefacts at the boundaries when we reconstruct gptq, due to the underlying periodic assumption.
The vanishing moments preserving boundary extension introduced in [20] circumvents this issue by designing special wavelets at the boundaries, which retain both orthonormality, vanishing moments and avoids any assumptions about periodicity. However, as pointed out by Antun & Ryan in [10], most wavelet libraries do not support these wavelets. In [29] Gataric & Poon extended the WaveLab library [16] with a special set of Daubechies wavelets. In this work, we use the implementation from [10], to also include orthonormal wavelets such as symlets.
For the periodic wavelet basis, we extend the wavelets and scaling functions at the boundaries periodically. That is, we let

pj,emr " j,m|r0,1s`j,2j `m|r0,1s pj,emr " j,m|r0,1s`j,m´2j |r0,1s

for m " 0, . . . ,  ´ 1, for m " 2j ´ , . . . , 2j ´ 1,

and similar for jp,emr . Here |ra,bs means the restriction to the interval ra, bs. Strictly speaking, we could have omitted the definition of pj,er, jp,er and pj,e2rj´ , jp,e2rj´ , as these function are pure interior functions, but we define these functions to unify the notation with the vanishing moments preserving
boundary wavelets, In [20] one constructs special boundary wavelets and scaling functions lmeft, mleft, rmight, and
mrigth, for m " 0, . . . , ´1. These functions are created using finite linear combinations of the interior functions, and their supports are staggered. That is suppplmeftq " r0,  ` ms and suppprmightq " r´m ´ , 0s and similar for mleft and mright. The corresponding boundary functions (similar for the wavelets) are defined as

bj,dmpxq " 2j{2lmeftp2j xq bj,dmpxq " 2j{2r2ijg´ht1´mp2j px ´ 1qq

for m " 0, . . . ,  ´ 1, for m " 2j ´ , . . . , 2j ´ 1.

With these functions well defined, we let "rep", mean either "per" or "bd". Let Vj " spantB,ju and Uj " spantB,ju, and note that by construction these satisfy Vj ` Uj "
Vj`1. Now, let C,j " B,J0 Y B,J0 Y ¨ ¨ ¨ Y B,j´1. It should be clear from the previous discussion

10

Periodic function f

1

f(t)

Non-periodic function g

2

g(t)

0.5

1.5

1 0
0.5
-0.5 0

-1

0

0.5

GS rec. of f using a periodic

wavelet basis

1

GS - per.

-0.5

1

0

GS rec. of g using a periodic

wavelet basis

2

GS - per.

0.5

1

GS rec. of g using a VMP wavelet

basis

2

GS - bd.

0.5

1.5

1.5

0

1

1

0.5

0.5

-0.5

0

0

-1

-0.5

-0.5

0

0.5

1

0

0.5

1

0

0.5

1

Figure 4: (Periodic boundary extension is best suited for periodic functions). We consider the two functions

f ptq " cosp2tq and gptq " cosp2tq ` t (top row). Given N " 32 Walsh samples from these functions, we use generalized sampling (GS) with M " 16 DB4 wavelet-basis functions with different boundary extensions for reconstruction. For the reconstruction basis, we use a periodic boundary extension (bottom row, left and middle) and vanishing

moments preserving (VMP) boundary extension (bottom row, right). As we can see from the middle figure at the

bottom row, the periodic boundary wavelets create artefacts at the boundaries when approximating a non-periodic

function. The VMP boundary wavelets circumvent this issue.

that B,j and C,j span the same space. We can perform a change-of-basis between the two bases using a DWT matrix W P R2j^2j .
Finally, note that there no closed-form formula exists for the compactly supported orthonormal
wavelets considered (except for the Haar wavelet). We can, however, compute approximations to p2jkq and p2jkq, at dyadic grid points using the cascade algorithm [21].

3.3 A useful lemma

Before we proceed, we prove a lemma that lays the foundation for the fast computations derived in the following sections. We note that the lemma is a generalisation of what is used in the proof of Lemma 6.6 in [1].

Lemma 3.4. Let h P L2pRq with suppphq  ra, bs for integers a  0  b. Denote by hj,mpxq " 2j{2hp2jx ´ mq a translated and dilated version of h. Suppose that j, m P Z are chosen so that
suppphj,mq  r0, 1s. Then

xhj,m,

wny

"

2´j{2

b´1
ÿ

wn

^l

`m 2j



Wh0,´l|r0,1q`X2´j n\

.

l"a

Proof. First notice that by assumption we have that suppphj,mq  r2´jpa ` mq, 2´jpb ` mqs  r0, 1s. This implies that b ´ a  2j, and that m P t´a, ´a ` 1, . . . , 2j ´ bu  t0, . . . , 2j ´ 1u, where have

11

used the assumption a  0  b, in the final inclusion. Next notice that

x 2j

`

m 2j

"

8
ÿ xpi´j`1q2´i´1 `

j
ÿ mpiq2´j´1`i

i"j

i"1

(9)

"

8
ÿ xpi´j`1q2´i´1 `

j
ÿ mpiq2´j´1`i

"

x 2j

`

m 2j .

i"j

i"1

Utilising (9) and Lemma 3.3, now give

1 xhj,m, wny " 2j{2hp2jx ´ mqwnpxq dx
0

b´1  2´j pl`1`mq

ÿ "

2j{2hp2jx ´ mqwnpxq dx

l"a 2´j pl`mq

"

b´1
ÿ
l"a

 l`1`m
l`m

2´j{2h

px

´

mq

wn

´

x 2j

¯

dx

"

b´1
ÿ
l"a

1
0

2´j{2h

px

`

lq

wn

^

x

`l` 2j

m



dx

"

b´1
ÿ
l"a

1
0

2´j{2h

px

`

lq

wn

^

x 2j

`

l ` m 2j

dx

"

2´j{2

b´1
ÿ

wn

^l

`m 2j



Wh0,´l|r0,1q`X2´j n\

.

l"a

4 The one dimensional algorithm

Next, we describe an algorithm for computing a matrix-vector multiplication with the matrix

» xrje,0p, w0y ¨ ¨ ¨ xrje,Mp ´1, w0y fi

PN U PM

"

-- ­

...

...

...

ffi fl

xrje,0p, wN´1y ¨ ¨ ¨ xrje,Mp ´1, wN´1y

(10)

and its adjoint, using OpN log N q operations and without explicitly storing the matrix (10) in memory. Throughout, we let M " 2j and N " 2j`q where j  J0 and q  0 are integers. Other values of M and N can be considered by ultilizing appropriate zero padding. Below, we describe the algorithm stepwise by defining different operators, which we combine to achieve the desired matrix-vector multiplication. The complete algorithm is summarised in Algorithm 1.
Remark 4.1 (On the scaling between N and M ). In Table 1 we have computed the ratio between the largest and smallest singular value of the matrix PN U PM , for different wavelets and choices for q, both in one and two dimensions. We observe that in all cases, the matrix is well-conditioned for the simplest choice of q " 1. This corresponds to N " 2M in one dimension and N " 4M in two dimensions. Moreover, since we know that the stable sampling rate for Walsh sampling and wavelet reconstruction is linear, we have that N " OpM q, with a reasonable constant.

12

The value of µpRM , SN q " 1{ cosppRM , SN qq " 1{M pPN U PM q

One dimension

Two dimensions

M " 27, N " 27`q, L2pr0, 1sq

M " 22¨5, N " 22p5`qq, L2pr0, 1s2q

Wavelet q " 1 q " 2 q " 3 q " 4

Wavelet q " 1 q " 2 q " 3 q " 4

DB2 DB3 DB4 DB5 DB6 sym2 sym3 sym4 sym5 sym6

1.200 2.610 1.251 1.392 6.499 1.200 2.610 1.188 1.179 1.300

1.050 1.135 1.068 1.109 1.137 1.050 1.135 1.037 1.042 1.059

1.014 1.028 1.023 1.025 1.033 1.014 1.028 1.008 1.013 1.015

1.004 1.006 1.007 1.011 1.016 1.004 1.006 1.003 1.005 1.005

DB2 DB3 DB4 DB5 DB6 sym2 sym3 sym4 sym5 sym6

1.439 6.814 1.565 1.937 42.233 1.439 6.814 1.412 1.389 1.690

1.102 1.289 1.141 1.230 1.292 1.102 1.289 1.075 1.085 1.121

1.028 1.057 1.047 1.050 1.068 1.028 1.057 1.016 1.026 1.029

1.008 1.013 1.013 1.022 1.032 1.008 1.013 1.006 1.009 1.009

Table 1: We compute the fraction 1{M pPN U PM q " µpRM , SN q, for the matrix PN U PM , where U is the change-ofbasis matrix between a Walsh sampling basis and an orthonormal wavelet basis with vanishing moments preserving boundary wavelets. We consider both one and two-dimensional bases. We see that in all the considered cases, the smallest singular value is reasonably close to 1, indicating good conditioning of the matrix. Here DBX and symX, refer to a Daubechies or symlet wavelet, respectively, with X vanishing moments.
Remark 4.2 (Applications to compressive sensing). Note that a sparse representation of f is needed for compressive sensing to achieve successful recovery. For this method it is, therefore, better to represent an approximation to f in the basis C,j, than the B,j basis used above. Changing the basis can easily be achieved by using the matrix PN U PM W ´1, where PN U PM is as above, and W ´1 P CM^M is the inverse discrete wavelet transform (IDWT). As W ´1 is a change of basis matrix from C,j to B,j, this matrix will simulate the desired matrix if C,j is the reconstruction basis. Furthermore, the cost of applying W ´1 is OpM q, using the cascade algorithm. This means that the overall cost of the matrix-vector multiplication does not grow by applying this change-of-basis.
Remark 4.3 (Haar wavelet reconstruciton). For N " M " 2j, the Haar wavelet basis and Walsh sampling basis, span the same space. For the Haar reconstruction basis, there is, therefore, no benefit of applying generalised sampling or the PBDW-method for reconstruction. Compressive sensing, on the other hand, can be applied since it allows for reconstruction of M wavelet coefficients from m  M samples, under the assumption of sparsity. Since many natural images are sparse in the Haar wavelet basis, this approach is widely studied, see e.g. [8, 48, 54]. For Walsh sampling and Haar wavelet reconstruction using the basis C,j, the truncated change-of-basis matrix PM U PM " HW ´1, where W ´1 is the Haar IDWT matrix, and H is the Hadamard matrix. This matrix can be computed using fast transforms with the FWHT and DWT algorithms. Below, we do, therefore, not consider Haar wavelet reconstruction.

4.1 The forward operation

The wavelet basis B,j with   1 vanishing moments, consists of three types of wavelets, the

left boundary corrected wavelets, interior wavelets and the right boundary corrected wavelets. The

matrix-vector multiplication PN U PM  for  P CM is, therefore, naturally divided into the three

sums

´1

M ´´1

M ´1

ÿ xrje,mp , wnym `

ÿ xj,m, wnym `

ÿ xrje,mp , wnym

(11)

m"0

m"

m"M ´

for each 0  n  N . In this subsection we foucs on how to speed up the computations of the middle summand, as a naive implementation would require OpM N q operations. Throughout we take 

13

to be some small fixed number, usually in the range {2, . . . , 8}, and we omitt the dependence on , whenever we summarize the computatinal cost of the algorithm. The first and thrid summand require OpN q " OpN q operations each, and their dependence is therefore independent of M . We consider the edge scaling functions in §4.3.
We start by applying Lemma 3.4 to the middle summand in (11). This gives

M ´´1

M ´´1 ´1

ÿ xj,m, wnym " 2´j{2 ÿ

ÿ

W

0,´l|r0,1qpX2´j

n\qwn

`

l`m 2j



m

m"

m" l"´`1

´1

M ´´1

" 2´j{2

ÿ

W0,´l|r0,1qpX2´j n\q

ÿ

wn

`

l`m 2j



m

,

l"´`1

m"

Recall that M " 2j and N " 2j`q, and define the linear operator Hl : RM Ñ RN by

Hlpq

"

«M ´´1 ÿ

wn

^

2qpl ` N

mq



ffN ´1 m

,

m"

n"0

and the linear operator Dl : RN Ñ RN by

 P RM ,

"

iN ´1

Dlpq " 2´j{2W0,´l|r0,1qpX2´j n\qn

,  P RN .

n"0

Combining these opertors, we can write the middle sum in (11) as

«M ´´1

ffN ´1

´1

ÿ

ÿ

xj,m, wnym

"

DlpHlpqq.

(12)

m"

n"0 l"´`1

Note that Hl can be implemented by embedding  P RM in a zero-padded vector of length N , and
apply an N ^ N fast Walsh-Hadamard transform. Thus, evaluatning Hl can be done in OpN log N q operations. Also notice that the coefficients W0,´l|r0,1qpX2´jn\q are independent of the input, and can be computed a priori. This reduced the cost of evaluating Dl to at most OpN q operations. The
cost of computing (12) is, therefore, OpN log N q. Also note that n  N " 2j`q, and implying that X2´jn\  2q ´ 1. This means that for each l we
only compute W0,´lpsq|r0,1q for s " 0, . . . , 2q ´ 1. Furthermore, from §2 we know that for a fixed   1 the stable sampling rate scales linearly. Hence for fixed q we may vary j without affecting

the stable sampling rate. This implies that we only need to precompute these coefficients for some q  q1 where 1  1 is the smallest stable sampling rate of interest. Moreover, from Table 1 we see that even the simplest choice of q " 1, results in 1    2, in many cases.

4.2 The adjoint operation

Next we consider the matrix-vector multiplication PM U °PN  for  P CN . Since the computational burder is on the M ´ 2 middle columns, we once more foucs on these and prostpone the edge wavelet functions, until §4.3. That is, for m " , . . . , M ´  ´ 1 we can write the matrix-vector product as

N ´1
ÿ xwn, j,my n
n"0

"

1 ?
2j

N ´1
ÿ

´1
ÿ

wn

^

l

`m 2j



W

´Y 0,´l |r0,1q

n 2j

]¯

n

n"0 l"´`1

1

´1 N ´1
ÿÿ

´n¯

´Y n ]¯

"

? 2j

l"´`1

n"0

w2q pl`mq

N

W 0,´l|r0,1q

2j

n

(13)

14

by utilizing Lemma 3.4 and (7). Next define the operator Bl : RN Ñ RM as

Blpsq

"

#N ´1
n"0
0

w2qpl`mqpn{N qsn

for m " , . . . , M ´  ´ 1 for m P t0, . . . ,  ´ 1u Y tM ´ , . . . , M ´ 1u

for s P RN , and observe that Bl " Hl°. Thus, from Equation (13) we now have

« N1

ffM ´´1

´1

ÿ

ÿ

xwn, j,my n

"

BlpDlpqq.

n"0

m"

l"´`1

Finally, observe that Bl can be computed in OpN log N q operations by applying a fast Walsh Hadamard transform of dimension N and selecting the appropriate output from this transform. Since the cost of applying Dl is OpN q, the total cost of computing the output from the middle rows are of order OpN log N q.

4.3 The edge operations

We now turn to the edge functions and consider the two boundary extensions given by the peri-

odic and vanishing moments preserving boundary wavelets. This gives us four different edge inner

products, one for each edge and boundary extension.

Consider the forward operation. According to (11) we can write the sum of the  first and  last

columns as

« ´1

ffN ´1

ÿ xrje,mp , wnym

m"0

n"0

and

« M´1

ffN ´1

ÿ xrje,mp , wnym

,

m"M ´

n"0

(14)

respectively, for  P RM . Likewise for the adjoint operation we can write the  first and  last rows

as

«N ´1

ff´1

«N ´1

ffM ´1

ÿ xwn, rje,mp yn

and

ÿ xwn, rje,mp yn

,

(15)

n"0

m"0

n"0

m"M ´

respectively, for  P RN . At the edges, we could, ­ potentially ­ compute the inner products xrje,mp , wny a priori and store
the result as dense matrices. A challenge with this approach, is that we need to compute and store

the inner products for every possible combination of j, m and n. This is infeasible in general, and

would only allows us to do computations for certain dimensions. However, by applying Lemma

3.4 once more, we can disentangle j, m, n from the integral computation, so that we only need to compute Wr0e,´p l|r0,1spsq for s P t0, . . . , 2q ´ 1u. In the next proposition we do just this. Note that we use the convention that if b  a and we write bl"ap¨ ¨ ¨ q, then this should be interpreted as zero.

Proposition 4.4. Let  be a scaling function, whose wavelet has   1 vanishing moments. Let M " 2j and N " 2j`q for positive integers j  rlog2p2qs and q  0. Let n P t0, . . . , N ´ 1u. Then for m " 0, . . . ,  ´ 1,

@pj,emr , wnD "

´m´1
ÿ

2´j{2wn

^ 2j

`m 2j

`

l

´Y n W 0,´l|r0,1q 2j

]¯

l"´`1

`

´1
ÿ

2´j{2wn

^

m` 2j

l



W

0,´l

|r0,1q

´Y

n 2j

]¯

l"´m

15

and

@bj,dm,

wnD

"

´1`m
ÿ

2´j{2wn

^l 2j



W lmeft p¨

`

lq|r0,1q

´Y n 2j

]¯

.

l"0

Furthermore, for m " 2j ´ , . . . , 2j ´ 1,

@pj,emr ,

wnD

"

2j ´m´1
ÿ

2´j{2wn

^

l

`m 2j



´Y W 0,´l|r0,1q

n 2j

]¯

l"´`1

`

´1
ÿ

2´j{2wn

^

l

`

m´ 2j

2j



W

´Y 0,´l |r0,1q

n 2j

]¯

,

l"2j ´m

and

@bj,dm, wnD "

´1
ÿ

2j{2wn

^

l

` 2j 2j



W

r2ijg´h1t ´mp¨

`

lq|r0,1q

´Y

n 2j

]¯

.

l"m´2j ´`1

Proof. For an interval I  R, let I denote the characteristic funciton on I. The result follows by using Lemma 3.4 on all the considered inner products. For all functions intersecting the left edge this is trivial, the result follows by recalling that supppq " r´ ` 1, s and suppplmeftq " r0,  ` ms. The same can be said, about the functions pj,er´1 " j,´1, and pj,e2rj´ " j,2j´ , since these are interior functions. Applying Lemma 3.4 to the right edges require slighly more care, since it is assumed that
the function under consideration is supported on an interval ra, bs, with a, b P N, and b  0. On the right edges this can be achived by using the change of variable y " 2jx ´ p2j ´ 1q. We do not
write out the details for all the considered functions, but demonstrate the idea on j,2j`m|r0,1s, for m " 0, . . . ,  ´ 2 (used in pj,emr ). We start by noticing that r0,1spxq " r´p2j´1q,1sp2jx ´ p2j ´ 1qq. This means that

j,2j`m|r0,1spxq " 2j{2p2j x ´ p2j ´ 1q ´ pm ` 1qqr´p2j´1q,1sp2j x ´ p2j ´ 1qq,

where the function p¨ ´ pm ` 1qqr´p2j´1q,1s has support r´ ` 1 ` pm ` 1q, 1s. Applying Lemma 3.4, and using that W0,´lp¨ ´ pm ` 1qq|r0,1qpsq " W0,´l`m`1|r0,1qpsq gives the result.
Given the inner products @rje,mp , wnD and @wn, rje,mp D, the computational cost of (14) and (15), is OpN q. Furthermore, to compute these inner products we may use Proposition 4.4 for each n P t0, . . . , N ´ 1u.
However, this can can be challenging since, evaluating the above sums, require the computation of wnp2´jpm`lqq for many different choices of m, l and n, and ­ to the best of the author's knowledge ­ there are no software packages implementing the pointwise evaluation of Walsh functions. Moreover, a naive implementation in C++ using Defenition 3.1 is rather slow. To speed up this part of the code we use the relation between Walsh functions and Hadamard matrices, and use the FWHT algorithm to evaluate wnp2´jpm ` lqq for all the relevant values of m, l and n, simultaniusly. However, this raises the computational cost of the edge computations to OpN log N q.

5 Extension to two dimensions
We restrict our attention to d " 2 dimensions, since it applies to any kind of imaging application. It is certainly possible to extend the algorithm to any d-dimensional tensor product space, though, it

16

Algorithm 1 The one-dimensional forward and adjoint operation.

1: procedure The forward operation

2: Input: j, q P N. M " 2j, N " 2j`q and  P RM .

3: Output:  " PN U PM  where PN U PM is given by (10). 4: Compute vectors left P RN and right P RN from (14), using Prop. 4.4.

5:

Compute

the

vector

mid

"

´1
l"´`1

DlHlpq.

6: return  " left ` mid ` right.

1: procedure The adjoint operation

2: Input: j, q P N. M " 2j, N " 2j`q and  P RN .

3: Output:  " PM U °PN  where PN U PM is given by (10).

4: Compute 0, . . . , ´1 and M´ , . . . M´1 from (15), using Prop. 4.4.

5:

Compute

1 , . . . , M 1 ´´1

extracting

elements

from

1

"

´1
l"´`1

BlpDlpqq.

6: return  " r0, . . . , ´1, 1 , . . . , M 1 ´´1, M´ , . . . , M´1s

practical relevance seems limited. We, therefore, let H " L2pr0, 1s2q and consider samples from the
tensor product basis twn1 b wn2 : pn1, n2q P N2u. As for the one dimensional algorithm, we consider the case where the sampling and reconstruction spaces are dyadid cubes. That is, for N " 2j`q and
M " 2j we let the sampling space SN2 " twn1 b wn2 : 0  n1, n2  N u and the reconstion space
RM2 " tj,m1 b j,m2 : 0  m1, m2  M u. For a tensor  P RM^M , we can split the change-of-basis computation as

M ´1 M ´1

ÿÿ

n1,n2 "

m1,m2 xj,m1 b j,m2 , wn1 b wn2 y

m1"0 m2"0

M ´1

M ´1

ÿ

ÿ

"

xj,m1 , wn1 y

m1,m2 xj,m2 , wn2 y

m1 "0

m2 "0

(16)

for each n1, n2 P t0, . . . , N ´ 1u, so that it is a double sum of one-dimensional inner products. Thus, letting G P RN^M denote the forward operator we derived for the one dimensional case, and letting  P CM^N have components

M ´1

m1,n2 "

ÿ

m1 ,m2

xj,m2 ,

wn2 y

"

G

´ rm1

,m2

sM m2´"10

¯

m2 "0

we see that the above computation simplifies to

n1 ,n2

"

´ G

´ rm1

,n2

sM m1´"10

¯¯
n1

,

for n1, n2 P t0, . . . , N ´ 1u,

or simply  " GG°, for  P RM^M . Now, since G can be evaluated in OpN log N q operations, we can compute  " GG° in
Op2M N log N q operations. Furthermore, since N " 22qM , and q P t1, 2u is a resonable choice, this is reduces to OpN 2 log N 2q, where N 2 is the dimension of the sampling space.
By considering (16), it should be clear that we can do the same type of splitting also for the

adjoint operation. We do not do the full derivation, but notice that as an intermediate step one

would need to compute

N ´1

n1,m2 "

ÿ

n1 ,n2

xwn2 ,

j,m2 y

"

G°

´ rn1

,n2

sNn2´"10

¯

n2 "0

17

for 0  m2  M . Applying the same transform in the row direction, leads to a transform which can be computed in OpN 2 log N 2q operations. The complete algorithm is summarized in Algorithm 2.
Algorithm 2 The two dimensional forward and adjoint operation.
1: Let G P RN^M be the one dimensional truncated change-of-basis matrix (10). 2: procedure The forward operation 3: Input: j, q P N. M " 2j, N " 2j`q and  P RM^M . 4: Compute  " GG°. 5: return .
1: procedure The adjoint operation 2: Input: j, q P N. M " 2j, N " 2j`q and  P RN^N . 3: Compute  " G°G. 4: return .

6 Numerical examples

We conclude by demonstrating how the proposed fast transform be used by the three reconstruction
methods presented in the introduction. The code for producing these figures can be found on the Github page. Throughout the section we let I denote a step function on the set I  Rd, d " 1, 2.

Example 1 We compare the four reconstruction methods (1) truncated Walsh series, (2) gener-

alised sampling, (3) the PBDW-method and (4) compressive sensing by acquiring N " 32 Walsh

samples

from

the

function

f

ptq

"

cosp2tqr0,1{2s ptq `

1 2

t

sinp6tqp1{2,1sptq.

The

resulting

reconstruc-

tions can be seen in Figure 5. The first three methods are linear reconstruction methods, and we

acquire Walsh samples using the N first Walsh functions. Compressive sensing (CS), on the other

hand, is an example of a non-linear reconstruction method. For compressive sensing we, therefore,

subsample 32 samples from the first 256 Walsh samples, using a variable density sampling scheme.

For the GS and PBDW reconstructions, we use the first M " 16 basis functions in the DB4 wavelet

basis for reconstruct, whereas for the compressive sensing reconstruction use the 128 first functions

in this basis. In all cases, we use vanishing moments preserving boundary wavelets to minimise the

artefacts at the boundaries. It is clear from the figure that the compressive sensing reconstruction

causes the least artefacts and best reconstruction, despite some wiggles around the discontinuity

at t " 1{2. The truncated Walsh series cause the very characteristic blocky artefacts, whereas the

generalised sampling method produces a smooth approximation to f . However, since we only use 16

wavelet functions, we obtain a very poor approximation around the discontinuity with generalised

sampling. The PBDW method approximates f better in the smooth areas on the right, using a large

number of Walsh functions. Still, it produces severe artefacts around the discontinuity and at the

top of the sine curve.

Example 2 We explore how choosing the reconstruction space in relation to the function one

would like to recover can improve the reconstruction quality. The reconstructions can be seen in

Figure 6.

In

this

example

f pt1, t2q

"

cos

`

3 2

t1



sin

p3t2q

and

we

acquire

f 's

32

^

32

first

Walsh

samples. Using these samples, we compute a truncated Walsh series approximation to f , along with

generalised sampling reconstructions with different wavelet smoothness. Note that the smoothness

of the wavelet basis increases with . Since f is smooth we expect that the reconstruction improves

18

f ptq

1.5

f(t)

1

Compressive sensing (CS)

1.5 f(t)

1

CS - bd.

Sampling pattern linear methods

0.5 0
-0.5

0

0.5

Truncated Walsh (TW)
1.5 f(t)

TW 1

0.5 0
-0.5

1

0

0.5

Generalised sampling (GS)

1.5

f(t)

GS - bd. 1

50

100

150

200

250

Sampling pattern compressive

sensing

50

100

150

200

250

1

PBDW-method

1.5

f(t)

PBDW 1

0.5

0.5

0.5

0

0

0

-0.5

-0.5

-0.5

0

0.5

1

0

0.5

10

0.5

1

Figure 5: (Comparison of reconstruction methods). We reconstruct the function f " cosp2tqr0,1{2sptq `

1 2

t

sinp6tqp1{2,1sptq

from

32

Walsh

samples

using

the

methods:

truncated

Walsh

series,

generalised

sampling,

PBDW-

method and compressive sensing. The first three of these methods are linear reconstruction methods, and the samples

we acquire are shown in the upper right corner. Note that for the CS reconstruction, we use subsampled measurements (see upper right corner).

with increasing values of . In Figure 6, we see this effect, as the reconstruction error decreases with increasing values of .
Example 3 Finally, we consider an experiment using compressive sensing in two dimensions. As in Example 2, we consider a smooth function, but this time we also introduce a few discontinuities by adding different boxes in the image. The smooth part of the image is then well approximated by wavelets at coarse scales, whereas the discontinuity around the boxes will produce a few non-zero spikes among the wavelet coefficients at finer scales. The considered function is shown in Figure 7, along with a truncated Walsh series approximation and a compressive sensing reconstruction, both from 128 ^ 128 Walsh samples. The compressive sensing approximation is based on solving (5) with  " 0.001, using a DB4 wavelet reconstruction basis. As we can see from the figure, we can use compressive sensing to obtain a high-resolution image from relatively few measurements. In contrast, the naive Walsh approximation results in a low-resolution image where one can clearly see the pixels when zooming in.
Acknowledgments
The author would like to thank Anders C. Hansen for his comments.

19

Truncated Walsh

GS with DB2

GS with DB4

GS with DB6

Error map, relative error 0.0950

Error map, relative error 0.0518

Error map, relative error 0.0290

Error map, relative error 0.0215

Figure 6: (Reconstruction error decreases with increasing ). We consider

the

function

f pt1, t2q

"

cos

`

3 2

t1



sin

p3t2

q

and

approximate

f

from

its

32 ^ 32

first Walsh samples using a truncated Walsh series (left column), and generalised

sampling with different wavelets (column 2-4, row 1-2). In the top row we show the

reconstructed functions and in the second row the show the absolute difference |f ´ f~| between f and the computed approximations f~. The relative error is computed as }f ´ f~} 2 {}f } 2 , by evaluating f and f~ in a large number of points.

To the right we show the function f . Notice how the reconstruction error decreases

with increasing values of .

Original function

References
[1] B. Adcock, V. Antun, and A. C. Hansen. Uniform recovery in infinite-dimensional compressed sensing and applications to structured binary sampling. Appl. Comput. Harmon. Anal., 55:1­40, 2021.
[2] B. Adcock and A. C. Hansen. A generalized sampling theorem for stable reconstructions in arbitrary bases. J. Fourier Anal. Appl., 18(4):685­716, 2012.
[3] B. Adcock and A. C. Hansen. Generalized sampling and infinite-dimensional compressed sensing. Found. Comput. Math., 16(5):1263­1323, 2016.
[4] B. Adcock and A. C. Hansen. Compressive imaging: Structure, Sampling, Learning. Cambridge University Press (in press), 2021.
[5] B. Adcock, A. C. Hansen, G. Kutyniok, and J. Ma. Linear stable sampling rate: Optimality of 2D wavelet reconstructions from Fourier measurements. SIAM J. Math. Anal., 47(2):1196­1233, 2015.
[6] B. Adcock, A. C. Hansen, and C. Poon. Beyond consistent reconstructions: optimality and sharp bounds for generalized sampling, and application to the uniform resampling problem. SIAM J. Math. Anal., 45(5):3132­ 3167, 2013.
[7] B. Adcock, A. C. Hansen, C. Poon, and B. Roman. Breaking the coherence barrier: A new theory for compressed sensing. In Forum Math., Sigma, volume 5. Cambridge University Press, 2017.
[8] B. Adcock, A. C. Hansen, and B. Roman. A note on compressed sensing of structured sparse wavelet coefficients from subsampled Fourier measurements. IEEE Signal Process. Lett., 23(5):732­736, 2016.
20

f pt1, t2q

CS sampling pattern

TW sampling pattern

f pt1, t2q (cropped)

CS reconstruction (cropped)

TW reconstruction (cropped)

Figure 7: (Compressive sensing allows for resolution enhancing). We consider the function f : r0, 1s2 Ñ R seen in the upper left corner, and acquire 128 ^ 128 Walsh samples from f using the two strategies seen in the upper right corner. The red squares indicate the cropped area. In the bottom row, we show from left to right the function f , the approximation using compressive sensing, and a truncated Walsh series approximation. From these crops, we can see that the compressive sensing reconstruction provides higher fidelity than the truncated Walsh series.
[9] B. Adcock, A. C. Hansen, and A. Shadrin. A stability barrier for reconstructions from Fourier samples. SIAM J. Numer. Anal., 52(1):125­139, 2014.
[10] V. Antun and Ø. Ryan. On the unification of schemes and software for wavelets on the interval. Acta Appl. Math., 173(7), 2021.
[11] J. Arndt. Matters Computational: ideas, algorithms, source code. Springer Science & Business Media, 2010. [12] A. Bastounis and A. C. Hansen. On the absence of uniform recovery in many real-world applications of compressed
sensing and the restricted isometry property and nullspace property in levels. SIAM J. Imaging Sci., 10(1):335­ 371, 2017. [13] K. G. Beauchamp. Walsh functions and their applications. Academic press, 1975. [14] P. Binev, A. Cohen, W. Dahmen, R. DeVore, G. Petrova, and P. Wojtaszczyk. Data assimilation in reduced modeling. SIAM/ASA J. Uncertain. Quantif., 5(1):1­29, 2017. [15] V. Boominathan, J. K. Adams, M. S. Asif, B. W. Avants, J. T. Robinson, R. G. Baraniuk, A. C. Sankaranarayanan, and A. Veeraraghavan. Lensless imaging: A computational renaissance. IEEE Signal ProcM ag., 33(5):23­35, 2016. [16] J. Buckheit, S. Chen, D. L. Donoho, I. Johnstone, and J. Scargle. About WaveLab, 1995. [17] A. Chambolle and T. Pock. A first-order primal-dual algorithm for convex problems with applications to imaging. J. Math. Imaging Vision, 40(1):120­145, 2011. [18] Y. Chi, L. L. Scharf, A. Pezeshki, and A. R. Calderbank. Sensitivity to basis mismatch in compressed sensing. IEEE Trans. Signal Proces., 59(5):2182­2195, 2011.
21

[19] P. Clemente, V. Dur´an, E. Tajahuerce, P. Andr´es, V. Climent, and J. Lancis. Compressive holography with a single-pixel detector. Opt. Lett., 38(14):2524­2527, Jul 2013.
[20] A. Cohen, I. Daubechies, and P. Vial. Wavelets on the interval and fast wavelet transforms. Appl. Comput. Harmon. Anal, 1(1):54­81, 1993.
[21] I. Daubechies. Ten lectures on wavelets. SIAM, 1992.
[22] R. DeVore, G. Petrova, and P. Wojtaszczyk. Data assimilation and sampling in Banach spaces. Calcolo, 54(3):963­ 1007, 2017.
[23] R. A. DeVore. Nonlinear approximation. Acta Numer., 7:51­150, 1998.
[24] Y. C. Eldar. Sampling with arbitrary sampling and reconstruction spaces and oblique dual frame vectors. J. Fourier Anal. Appl., 9(1):77­96, 2003.
[25] Y. C. Eldar. Sampling without input constraints: Consistent reconstruction in arbitrary spaces. In Sampling, wavelets, and tomography, pages 33­60. Springer, 2004.
[26] Y. C. Eldar and T. Werther. General framework for consistent sampling in Hilbert spaces. Int. J. Wavelets. Multi., 3(04):497­509, 2005.
[27] C. L. Epstein. Introduction to the mathematics of medical imaging. SIAM, 2007.
[28] S. Foucart and H. Rauhut. A Mathematical Introduction to Compressive Sensing. Springer - Birka¨user, 1th edition, 2013.
[29] M. Gataric and C. Poon. A practical guide to the recovery of wavelet coefficients from Fourier measurements. SIAM J. Sci. Comput., 38(2):A1075­A1099, 2016.
[30] B. Golubov, A. Efimov, and V. Skvortsov. Walsh series and transforms: theory and applications, volume 64. Springer Science & Business Media, 1991.
[31] M. Guerquin-Kern, M. Haberlin, K. P. Pruessmann, and M. Unser. A fast wavelet-based reconstruction method for magnetic resonance imaging. IEEE Trans. Med. Imaging, 30(9):1649­1660, 2011.
[32] M. Guerquin-Kern, L. Lejeune, K. P. Pruessmann, and M. Unser. Realistic analytical phantoms for parallel magnetic resonance imaging. IEEE Trans. Med. Imaging, 31(3):626­636, 2012.
[33] A. C. Hansen and L. Thesing. On the stable sampling rate for binary measurements and wavelet reconstruction. Appl. Comput. Harmon. Anal., 48(2):630­654, 2020.
[34] M. R. Hestenes and E. Stiefel. Methods of conjugate gradients for solving linear systems. J. Res. Natl. Bur. Stand., 49(6), 1952.
[35] A. Hirabayashi and M. Unser. Consistent sampling and signal recovery. IEEE Trans. Signal Proces., 55(8):4104­ 4115, 2007.
[36] T. Hrycak and K. Gr¨ochenig. Pseudospectral Fourier reconstruction with the modified inverse polynomial reconstruction method. J. Comput. Phys., 229(3):933­946, 2010.
[37] A. Jardine, H. Hedgeland, G. Alexandrowicz, W. Allison, and J. Ellis. Helium-3 spin-echo: Principles and application to dynamics at surfaces. Prog. Surf. Sci., 84(11-12):323­379, 2009.
[38] A. Jones, A. Tamto¨gl, I. Calvo-Almaz´an, and A. Hansen. Continuous compressed sensing for surface dynamical processes with helium atom scattering. Sci. rep., 6(1):1­11, 2016.
[39] G. Kutyniok and W.-Q. Lim. Optimal compressive imaging of Fourier data. SIAM J. Imaging Sci., 11(1):507­ 546, 2018.
[40] R. Leary, Z. Saghi, P. A. Midgley, and D. J. Holland. Compressed sensing electron tomography. Ultramicroscopy, 131:70­91, 2013.
[41] Z.-P. Liang and P. C. Lauterbur. Principles of magnetic resonance imaging: a signal processing perspective. SPIE Optical Eng. Press, 2000.
[42] M. Lustig, D. L. Donoho, J. M. Santos, and J. M. Pauly. Compressed sensing MRI. IEEE Signal Proc. Mag., 25(2):72­82, 2008.
[43] J. Ma. Generalized sampling reconstruction from Fourier measurements using compactly supported shearlets. Appl. Comput. Harmon. Anal., 42(2):294­318, 2017.
[44] Y. Maday, T. Anthony, J. D. Penn, and M. Yano. PBDW state estimation: Noisy observations; configurationadaptive background spaces; physical interpretations. ESAIM: Proceedings and Surveys, 50:144­168, 2015.
[45] Y. Maday and O. Mula. A generalized empirical interpolation method: application of reduced basis techniques to data assimilation. In Analysis and numerics of partial differential equations, pages 221­235. Springer, 2013.
22

[46] Y. Maday, A. T. Patera, J. D. Penn, and M. Yano. A parameterized-background data-weak approach to variational data assimilation: formulation, analysis, and application to acoustics. Int. J. Numer. Meth. Eng., 102(5):933­965, 2015.
[47] S. Mallat. A wavelet tour of signal processing: The sparse way. Academic Press, 3rd edition, 2008. [48] A. Moshtaghpour, J. M. Bioucas-Dias, and L. Jacques. Close encounters of the binary kind: Signal reconstruction
guarantees for compressive Hadamard sampling with Haar wavelet basis. IEEE Trans. Inf. Theory, 66(11):7253­ 7273, 2020. [49] M. Muller. Introduction to confocal fluorescence microscopy, volume 69. SPIE press, 2006. [50] C. Poon. A consistent and stable approach to generalized sampling. J. Fourier Anal. Appl., 20(5):985­1019, 2014. [51] S. Ravishankar, J. C. Ye, and J. A. Fessler. Image reconstruction: From sparsity to data-adaptive methods and machine learning. Proc. of the IEEE, 108(1):86­109, 2019. [52] V. Studer, J. Bobin, M. Chahid, H. S. Mousavi, E. Candes, and M. Dahan. Compressive fluorescence microscopy for biological and hyperspectral imaging. Proc. Natl. Acad. Sci. USA, 109(26):E1679­E1687, 2012. [53] W.-S. Tang. Oblique projections, biorthogonal Riesz bases and multiwavelets in Hilbert spaces. P. Amer. Math. Soc., 128(2):463­473, 2000. [54] L. Thesing and A. C. Hansen. Linear reconstructions and the analysis of the stable sampling rate. Sampling Theory in Signal and Image Processing, 2018. [55] L. Thesing and A. C. Hansen. Non-uniform recovery guarantees for binary measurements and infinite-dimensional compressed sensing. J. Fourier Anal. Appl., 27(2):1­44, 2021. [56] Y. Traonmilin and R. Gribonval. Stable recovery of low-dimensional cones in Hilbert spaces: One RIP to rule them all. Appl. Comput. Harmon. Anal., 45(1):170­205, 2018. [57] M. Unser and A. Aldroubi. A general sampling theory for nonideal acquisition devices. IEEE Trans. Signal Proces., 42(11):2915­2925, 1994. [58] M. Unser and J. Zerubia. A generalized sampling theory without band-limiting constraints. IEEE tran. circuitsII, 45(8):959­969, 1998. [59] E. van den Berg and M. P. Friedlander. Probing the Pareto frontier for basis pursuit solutions. SIAM J. Sci. Comput., 31(2):890­912, 2008.
23

