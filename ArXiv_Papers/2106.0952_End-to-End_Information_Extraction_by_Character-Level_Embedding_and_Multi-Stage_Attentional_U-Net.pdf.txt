arXiv:2106.00952v1 [cs.CV] 2 Jun 2021

TUAN ANH & DAT: MULTI-STAGE ATTENTIONAL U-NET

1

End-to-End Information Extraction by Character-Level Embedding and Multi-Stage Attentional U-Net

Tuan Anh Nguyen Dang tadashi@cinnamon.is Dat Nguyen Thanh marc@cinnamon.is

Cinnamon AI Labs Hanoi, Vietnam
Cinnamon AI Labs Hanoi, Vietnam

Abstract
Information extraction from document images has received a lot of attention recently, due to the need for digitizing a large volume of unstructured documents such as invoices, receipts, bank transfers, etc. In this paper, we propose a novel deep learning architecture for end-to-end information extraction on the 2D character-grid embedding of the document, namely the Multi-Stage Attentional U-Net. To effectively capture the textual and spatial relations between 2D elements, our model leverages a specialized multi-stage encoder-decoders design, in conjunction with efficient uses of the self-attention mechanism and the box convolution. Experimental results on different datasets show that our model outperforms the baseline U-Net architecture by a large margin while using 40% fewer parameters. Moreover, it also significantly improved the baseline in erroneous OCR and limited training data scenario, thus becomes practical for real-world applications.
1 Introduction
Information Extraction (IE) is the process of extracting structured information from unstructured documents. Traditional document processing systems prior to deep learning era often rely on template matching methods to extract information from fixed-format documents [9, 10]. However, these methods have scaling issues to operate on a large number of templates, and they are also sensitive to slight input distortions. A more general approach to handle varied-format document would be a "document understanding" system, which utilizes a layout analysis step to detect the structure of text-lines, tables and paragraphs in the input image [8, 15]. After that, an information extraction step [24, 25] is required to determine their functional roles and semantics. Designing such a method is considered as a difficult task, especially when the input documents have a high degree of variance and complex layouts [30]. Moreover, the errors from preceding processes such as text recognition (OCR) and layout analysis further hinder the performance of various IE methods. Hence, there is still much room for improvement to make automatic document extraction become feasible.
To address the aforementioned problems, we present an end-to-end deep neural network architecture, so-called Multi-Stage Attentional U-Net, which can be trained directly with
© 2019. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.

2

TUAN ANH & DAT: MULTI-STAGE ATTENTIONAL U-NET

supervision from annotated document data. Using the 2D-character grid (or char-grid for short) representation of the document as described in [11], our network performs pixel-level semantic segmentation task to label and extract the relevant information. The char-grid embedding has some substantial differences from natural images, featuring both strong local texture cues and global spatial relationships, thus is challenging for conventional semantic segmentation architecture. In order to correctly exploit both textual and spatial features of the char-grid, our network leverages a specialized multi-stage encoder-decoders design, where the former encoder-decoder blocks focus on the textual components to identify the important elements (e.g. headers/keywords), and the later ones attempt to learn the spatial relations between elements and propagate that information from the intermediate context to the target values. Long-range dependencies and correlations between spatial positions on the document are captured by an additional self-attention mechanism [31] and the box convolution layer [4], which further strengthen the ability of the proposed network to model diverse document layouts. All modules are differentiable and can be jointly trained in an end-to-end fashion. Notably, the training process utilizes a multi-task training scheme with an auxiliary loss and a data augmentation process to enhance the generalization of our model. We validate our method on various real-world information extraction datasets and achieve significant improvements over the baseline U-Net model in all tasks while using fewer parameters. The network also shows good performance in limited training data scenarios and erroneous results from the OCR step, which matches the standards of industrial needs for automatic document extraction pipeline. In summary, our contribution is several-fold:

· We proposed a specialized deep neural network architecture to perform the information extraction task on the 2D character-grid, which largely surpasses the strong baseline U-Net and has 40% fewer parameters.

· We systematically evaluate the use of self-attention mechanism, box-convolution and multi-stage encoder-decoders upon modeling complex spatial relations in the document layout. Extensive ablation studies are conducted to verify the effectiveness of each component.

· Rigorous experiments are performed on our in-house customer-provided datasets to validate the robustness of our method under harsh conditions (limited training data, heavy OCR errors).

2 Related works
Information extraction techniques for document images can be roughly classified into three groups: the template matching-based methods, the heuristic-based methods and the deeplearning-based methods. Those state-of-the-art approaches are reviewed as follows:
Template Matching Based Methods: This research direction formulates the problem of information extraction as document registration [9, 10] or form classification [1], in which image would first be matched with a type of form in a database and the information will be extracted from predefined areas. Further readings on document image information extraction that uses template matching methods can be referred in [9, 10]. However, these methods, in general, have certain drawbacks while dealing with undefined and unseen structured form, which hinder wide-adaptation in automatic document processing pipelines.

TUAN ANH & DAT: MULTI-STAGE ATTENTIONAL U-NET

3

Heuristic Methods: The conventional information extracting pipeline [3, 17] often tries to build up the components using carefully designed feature engineering. Kise et al. in their seminal work [12] introduced a selective method based upon the edges of primary and residual Voronoi graphs that can reduce the cost of character-wise segmentation, which mainly relies on connected component labeling of binary inputs. As conventional computer vision-based segmentation methods evolved, their adaptations to process image document were exploited: while [5], [19] and [7] used text lines to detect tables or segment page in to regions, [29] defines table reading orders and heuristically performs table understanding on segmented text lines and separators. One major limitation of the heuristic approaches is that they assume a certain condition on the dataset, which limits their generality to be used with different types of forms.
Deep learning based methods Instead of using hand-crafted feature design, recent development in deep learning on image segmentation like Fully Convolutional Networks [14] and U-Net [23] have allowed early attempts in training end to end text line-level segmentation and labeling like that of [18] which applied fully convolutional network to directly extract regions of interest from historical documents. Our proposed method is more aligned with recent end-to-end systems which try to incorporate both semantic structure and spatial image features [11, 20, 30]. While [30] use sentence-level embedding which assumes perfect text line grouping to segment the document semantically and targets to conventional documents, we aim for information extraction from visually rich documents, of which characterlevel segmentation is essential.
To the best of our knowledge, the proposed neural network-based model is essentially different from the two aforementioned approaches: since we are considered the problems of progressive information extraction (i.e. first the `key' is segmented before the whole `key-value' is segmented), the potential of multiple stacked U-Nets has to be considered, for which we adapted the structure of the recent coupled U-Net [27], while [11, 20, 30] leverage variants of a single-output fully convolutional neural networks. Further, the model is adapted with global information propagation capability by the use of self-attention mechanism on each convolutional block, it is noted that box convolution [4] is also leveraged to support distanced region information incorporation.
Regarding the embedding, the char-grid [20] is chosen and tweaked over the hierarchical levels of sub-word [2], word [16, 22] or sentence level embedding which have been used in [30], because, as observed, we note that there are cases like numbers, formatted date, etc. that cannot appropriately be represented using a finite number of vectors nor be modeled accurately by N-grams.

3 Method
3.1 Chargrid Conversion
The document image, after being processed by a standalone OCR module to form the text lines, can help to construct the chargrid as follows:
· Character separation: We first convert the text line boxes to character boxes by dividing them horizontally to the number of character reside in the text with the assumption that in each text line, the variance of width and height of each character are minimal. After this step, a list of character boxes Lc = {(c1, b1), (c2, b2), . . . , (cN, bN)} is obtained where ci is the character index and bi its respective bounding box.

4

TUAN ANH & DAT: MULTI-STAGE ATTENTIONAL U-NET

· Character encoding: Each character is indexed, a constant number of most frequent
characters Nchar is chosen. The character boxes are then used to create a mask CM  {0, 1, . . . , Nchar}H×W . For each character box in the list with the character index ichar 
{1, . . . , Nchar}, the region of the mask covered by the box will be filled with the value ichar. CM is then converted to one-hot format CMone_hot  {0, 1}H×W ×(Nchar+1)

At this point, the problem of information extraction is now equivalent to the pixel-wise
segmentation on the chargrid: Taking CMone_hot as input, we wish to train the neural network that can output the mask M  {0, . . . , K - 1}H×W masking the regions of K target fields, with the supervision from labeled mask LM  {0, . . . , K - 1}H×W for each input document.

3.2 Network architecture

Figure 1: Architecture of the Multi-Stage Attentional U-Net. The input chargrid goes through several of encoder-decoder blocks to generate the output mask. Both encoder-decoder blocks are tightly connected by the additional coupling-connections. An auxiliary loss is used during training to increase the context-awareness of the model.
The architecture of our model is depicted in Figure 1. We use the backbone as Coupled U-Net [27], with the inspiration behind this utilization is the benefit of multi-stage outputs. For example, in order to understand what kind of information a value in a table represents, a person have to look at the top header or the side description (see Figure 2(a)). This order of behavior can be satisfied by forcing the middle U-Net block to segment key and/or description while the final end outputs the full segmentation of all key/value collections. In addition, other advantages of the Coupled U-Net are also leveraged such as the avoidance of gradient vanishing while promoting feature reuse by the dense skip-connections between corresponding encoder and decoder blocks of each U-Net to another; and the use of auxiliary loss applied to the output of middle U-Net block.
The detail is further explained as follows: Firstly, for the sake of flexibility, each encoder/decoder block is constructed by chaining multiple ResBlocks, each of which is a ResNet-like bottleneck, with (3 × 3) convolutions, batch normalization and residual connection. The downsampling blocks are equipped with dilated convolution while the upsampling blocks use transposed convolution blocks for upscaling their resolutions. Secondly, from an intuitive observation that different information flows from table headers and descriptors are

TUAN ANH & DAT: MULTI-STAGE ATTENTIONAL U-NET

5

required to extract the right target value (illustrated in Figure 2(a)), we incorporate the BoxConvolution layer in [4] (Figure 2(b)) to a variant of ResBlock, which helps modeling long range vertical and horizontal interaction. The box convolutions [4] are sliding mean window with learnable offsets instead of weights over windows (see Figure 2(b)). Learning the offset patterns aids the model in aggregating information from distant regions in a straight forward way, making it a suitable alternative to conventional convolution blocks. Thirdly, to strengthen the Coupled U-Net design, expansion paths (also known as skip-connection) from the same level of downsampling blocks to upsampling blocks are added, with self-attention mechanism (non-local network) [31] (see Figure 3) to promote global information propagation between each encoder-decoder pair of an U-Net block.
In short, with the described building blocks, we have built an architecture that is lightweight and feasible to exploit both textual and spatial relations from the chargrid embedding.

(a) The need for spatial information flow in modeling (b) Box convolution operation allowing storing the mean

the document semantic layout

value of distanced region. The output of the stored pixel

value is mean of the values of pixels covered by the box

Figure 2: Illustration of spatial information propagation

Figure 3: The self-attention mechanism applied on feature maps. Each pixel's feature is projected to two separate latent spaces by two linear transformation f , g respectively. The per-pixel correlation is then calculated, of which the result is used to aggregate information from each high-correlating pixels to the destination in the output map of the third linear transformation h. The propagation of information is cancelled when either the correlation matrix is a zero matrix or the third linear transformation is a mapping to zero.

6

TUAN ANH & DAT: MULTI-STAGE ATTENTIONAL U-NET

3.3 Training specification

We specify the additional detail about the training stage, including model hyper-parameters, the loss function and the data augmentation process. The hyper-parameters of the standard msau model is chosen as follows: the number of most-frequent characters Nchar = 256, number of U-Net blocks nblocks = 2, the depth (number of convolution operations) of the ResBlock res_depth = 2, the number of channels of the first input feature map C = 16, the number of downsampling blocks in an encoder ndownsampling = 4. msau_big has the same configuration except ndownsampling = 5. The same setups are also used for our baseline variants of U-Net, which contain a single encoder-decoder block. Our code, trained model and test samples will be tentatively released via this link1 to foster the reproducibility of this work.
Loss function. The multi-task loss used in the training process is a weighted sum of two smaller losses: the ending loss of the final output and the auxiliary loss of the intermediate output. We adopt the Focal Loss [13] to resolve the class imbalance problem during training. The auxiliary loss is computed from the ground-truth keys mask, after removing all the values. The corresponded weight are  = 0.4 for the auxiliary loss, and 1 -  is applied to the ending loss.
Data augmentation. In order to increase the robustness of the model against varied layouts, we perform various types of data augmentation. The text-boxes in the input document are re-scaled so that the median text height is 3 pixels in the char-grid. We use the following steps for augmentation: random character replacement to mimic OCR errors, random text box shifting, random affine transformation, random background padding.

4 Experiments
We evaluate and analyze our approach on two customer-provided documents: self-collected Japanese invoices and the medical receipts datasets.
4.1 Datasets
Our in-house datasets comprise of challenging document images with a variety of layouts. The images are also corrupted by several distortions such as noises, blurs, missing strokes due to the scanning process or poor paper quality. These conditions often appear in a practical document processing pipeline, thus make our datasets a good benchmark for information extraction methods. Ground-truth for the data was annotated manually by human operators, which includes the location and content of text-boxes as well as the key information boxes that need to be extracted.
Japanese invoices dataset consists of 261 invoices in Japanese from several vendors. Each invoice contains 16 key-value pairs to extract. Example keys are date_issue, sender_name, receiver_name, total_amount, tax, item_name, item_amount, etc. A few sample images are shown in the supplement (Figure 5). This dataset features some notable challenges such as: mixed handwriting and printed text, random element placements, low resolution scanning.
Insurance medical receipts dataset consists of 200 medical receipts in Japanese with varied formats. Each document has 12 keys to be extracted: date_issue, billing_period,
1https://github.com/datvo06/MSAU

TUAN ANH & DAT: MULTI-STAGE ATTENTIONAL U-NET

7

insurance_amount, patient_co-payment_ratio, surgery_fee, hospitalization_fee, etc. Examples of this dataset can be accessed via the supplement as well (see Figure 6). The challenges of this data include long-range key-value correlation, multiple keys relation, skewed images.
Both datasets are divided into 70% for training and 30% for testing.

4.2 Implementation details

The proposed network was implemented using Tensorflow library. We perform experiments on a server equipped by an Nvidia Quadro M4000 (8GB memory GPU). All models were trained for 200 epochs with a mini-batch size of 4. The RMSProp method [28] is used for optimizing the model. The learning rate is set to 0.001 at the beginning and decreases every 10 epochs with a polynomial decay of 0.9.
Baselines. In our experiment, we used two U-Net variants with dilated convolution [6, 21] and residual bottleneck in the downsampling path, which is similar to the encoder backbone of our architecture. The first variant has ndownsampling = 5, res_depth = 2 and C = 16. The second variant has ndownsampling = 6, res_depth = 3. We abbreviate them as unet_small and unet_big, respectively. These two networks are very strong baselines due to the proven effectiveness of their sub-components. Additionally, the same architecture was also presented in [11], which allows us to make a direct comparison to one of the current state-of-the-art methods. Table 1 summarized the number of parameters of our network and the baselines. As will be shown shortly, although our msau_small uses 40% less parameters, it is still able to outperform unet_big by a large margin in term of accuracy.
Metrics. To compare the performance across models, we use three metrics: mean Intersection-over-Union (mIOU), mean pixel accuracy (pix acc) and the box F1-score (F1score). We note that we leveraged a modified metric, so-called box F1-score, to evaluate the correctness of the predicted bounding box for each key field. The bounding box of each field is constructed from the connected component analysis on the output segmentation mask. A predicted box is marked as a correct detection if its IoU ratio with one of the ground-truth boxes is larger than a certain threshold 0.8. This metric is suitable for testing the model on practical OCR-ed document where the ground-truth mask may slightly misalign with the generated char-grid from the OCR text.

Table 1: Number of Table 2: Performance comparison

trainable parameters

between ground-truth and OCR text

Model

Number of Dataset Model

F1-score F1-score

unet_small unet_big msau (ours) msau_big (ours)

parameters
6.7 × 105 10.6 × 105 6.6 × 105 10.5 × 105

Invoice Insurance

unet_big msau_big (ours) unet_big msau_big (ours)

(GT text)
87.3 96.0 89.9 96.1

(OCR)
79.7 (-8%) 91.2 (-5%) 83.4 (-7%) 92.4 (-4%)

4.3 Results
Overall results. We first compare the performance of our msau models to the baseline architectures. Table 3 shows the results of all models that have been deployed on the Japanese invoices testset. As we can see, the MSAU-nets surpassed the baseline U-Net models in all metrics by a significant margin. The base msau model improved the mIOU score by 7% compare to the unet_big, with only 60% the number of parameters. The msau_big further

8

TUAN ANH & DAT: MULTI-STAGE ATTENTIONAL U-NET

increased this gap to 9%, leading to an remarkable 40% error reduction. This demonstrates the effectiveness and efficiency of our architectures. F1-score is also largely improved, with a 9% increase from unet_big to msau_big and 7% increase from unet_big to msau.

Table 3: Comparison on Japanese invoices (left) and medical receipts (right) datasets

Model

mIOU pix acc F1-score Model

mIOU pix acc F1-score

unet_small

74.4 79.2

83.2 unet_small

78.9 83.3

86.2

unet_big

78.0 85.7

87.3 unet_big

81.7 86.5

89.9

msau (ours)

85.5 91.1

94.6 msau (ours)

86.4 91.8

95.0

msau_big (ours) 87.2 92.5

96.0 msau_big (ours) 89.1 93.3

96.1

Similar achievements are expected while testing on the Insurance medical receipts datasets (Table 3). The MSAU models continue to outperform the baseline U-Nets consistently. The msau_big model leads to an obvious 8% improvement in mIOU from the unet_big. The base msau also demonstrated good parameter efficiency by delivering better performance compared to unet_big (5%) in both mIOU and F1-score.
We further evaluate our models on the two datasets with output text from an OCR engine (Tesseract [26]). As can be seen in Table 2, the msau_big model shows its robustness to OCR errors with 5% and 4% drops in F1-score, while the baseline unet_big has 8% and 7% drops respectively. Furthermore, the gap between unet_big and msau_big is enlarged to 910% in the OCR-ed documents. This shows that our method is able to detect the errors produced by OCR step by a successful rate.
For a more comprehensive comparison, we show our model results across different levels of synthetic OCR errors (randomly replacing/removing character with probability e) in Figure 4(a). The impact of OCR errors is less severe on our MSAU-net as the proposed model still achieve more than 70% F1-score in an extreme condition where character error rate is as high as 25%.

(a) Performance with different OCR error rates e on (b) Performance with different train/val ratios on the

the Japanese Invoice dataset

Japanese Invoice dataset

Figure 4: Performance comparison in two different scenarios.

Ablation study of sub-components. In this part, we perform a step-wise decomposition on our model to study the effect of each sub-component. Experiment is conducted using the unet_big and msau_big baseline on the Japanese Invoice dataset. The performance results are shown in Table 4.
As we can see, the base CU-Net model (4) improves the mIOU score by 2% compared to the base U-Net (1). It also surpasses the UNet+NL (2) configuration. The Non-local (selfattention) block helps the base model gained 1% and 2% in mIOU, respectively, in cases of the U-Net (2) and CU-Net (6) baseline. The CU-Net configuration gained much greater im-

TUAN ANH & DAT: MULTI-STAGE ATTENTIONAL U-NET

9

Table 4: Performance comparison with different model configurations. Abbreviations: NL -

Non-local self-attention, MS - Multi-Stage learning, BC - Box Convolution, CUNet - Coupled U-Net

# Model

Params (×105) mIOU F1-score

1 UNet (unet_big)

10.6

2 UNet+NL

10.5

3 UNet+NL+BC

10.8

4 CUNet

10.3

5 CUNet+MS

10.3

6 CUNet+NL

10.5

7 CUNet+MS+BC

10.3

8 CUNet+NL+BC

10.5

9 CUNet+NL+BC+MS (msau_big)

10.5

78.0

87.3

79.2

88.1

80.7

91.2

79.5

89.6

82.3

92.5

81.5

91.3

86.2

95.1

84.6

94.6

87.2

96.0

provement in comparison with the base U-Net when adding the box convolution (3% and 1% mIOU in (3) and (8)), respectively. This demonstrates the effectiveness of the component, especially when combined with the CU-Net for modeling spatial relation in the char-grid. Notably, with the multi-stage training (5) plus the box convolution, configuration (7) improves the overall mIOU by 4%. This is a significant achievement because in order to reach the same number, unet_small has to transform to unet_big with more 40% parameters. The effect of box convolution can be further analyzed in the Figure 5, in which we can see that the improved result come from the larger receptive field by expanding the box convolution filter dimension.

Figure 5: Distribution of filter sizes in the box convolution layer. The horizontal axis shows the max dimension of learned box filters)
Ablation study of multi-stage training. Table 4 shows that the multi-stage training (9) increased the mIOU by a significant 3%, compared to the CUNet+NL+BC (8) configuration. This observation also holds in the base CU-Net model with 3% improvement from (4) to (5). We can see that the multi-stage training greatly helps to gain the performance while introducing no additional complexity to the model. One explanation is that the separation of textual and spatial feature learning in the auxiliary loss can encourage each sub-component to focus on certain task, hence better generalization. Note that the multi-stage paradigm can not be applied to the U-Net baseline due to the single encoder-decoder design. The best CU-Net configuration with multi-stage greatly outperformed the best U-Net model by more than 6.5% mIOU. Lastly, we found that the multi-stage training also helps increase the robustness of the model in limited training data scenarios. Figure 4(b) shows the impact of different train/val ratios on the model performance. It can be seen that the auxiliary loss significantly boost the performance in all cases. More importantly, the improvement is more noticeable in the case of less training data. Qualitative result. We provide a qualitative visualization of our algorithm in Figure 6. More

10

TUAN ANH & DAT: MULTI-STAGE ATTENTIONAL U-NET

results are provided in the supplementary, and we encourage the reader to view them at full size on a screen.

Figure 6: Sample outputs on the Japanese invoice dataset. Segmented key mask is presented in blue, value mask is presented in red
5 Conclusion
We have presented a novel deep neural network, Multi-Stage Attentional U-Net (MSAU), for end-to-end information extraction on the 2D char-grid representation of the document. We incorporate attention mechanism, box convolution with the multi-stage encoder-decoder architecture to handle complex textual and spatial relation in the char-grid. Moreover, we propose a multi-task training scheme to improve the model performance on practically challenging scenarios. Experiments on two benchmark datasets have demonstrated the effectiveness of our approach. In the future work, we wish to open-source our datasets and provide the proposed methods as new baselines to promote the current active research in document analysis area. Acknowledgement. The authors wish to offer a sincere thanks to Cinnamon AI Labs for providing the required resources to complete this work. We also express our deep gratitude to

TUAN ANH & DAT: MULTI-STAGE ATTENTIONAL U-NET

11

Tran Minh Quan (lotus@cinnamon.is) and Nghiem Nguyen Viet Dung (henry@cinnamon.is) for their valuable aids in the final preparation of the paper.

References
[1] David Aldavert, Marcal Rusinol, and Ricardo Toledo. Automatic Static/Variable Content Separation in Administrative Document Images. Proceedings of the International Conference on Document Analysis and Recognition, ICDAR, 1:87­92, 2017. ISSN 15205363.
[2] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching Word Vectors with Subword Information. Analytical Methods, 5(3):729­734, jul 2016. ISSN 17599679.
[3] Syed Saqib Bukhari, Faisal Shafait, and Thomas M Breuel. High Performance Layout Analysis of Arabic and Urdu Document Images. Icdar, pages 1275­1279, 2011.
[4] Egor Burkov. Deep Neural Networks using Box Convolutions. (NeurIPS):1­11, 2018.
[5] Deepayan Chakrabarti, Ravi Kumar, and Kunal Punera. A graph-theoretic approach to webpage segmentation. Proceeding of the 17th international conference on World Wide Web - WWW '08, page 377, 2008.
[6] Liang-chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs. pages 1­14, jun 2016.
[7] Bertrand Co. Recognition of Tables and Forms. 2014. ISBN 9780857298591.
[8] Deepika Ghai and Neelu Jain. Text Extraction from Document Images- A Review. International Journal of Computer Applications, 84(3):40­48, 2013.
[9] Hanchuan Peng, Fuhui Long, Wan-Chi Siu, Zheru Chi, and David Dagan Feng. Document image matching based on component blocks. In Proceedings 2000 International Conference on Image Processing (Cat. No.00CH37101), pages 601­604 vol.2. IEEE, 2000.
[10] Hanchuan Peng, Fuhui Long, and Zheru Chi. Document image recognition based on template matching of component block projections. IEEE Transactions on Pattern Analysis and Machine Intelligence, 25(9):1188­1192, sep 2003. ISSN 0162-8828.
[11] Anoop Raveendra Katti, Christian Reisswig, Cordula Guder, Sebastian Brarda, Steffen Bickel, Johannes Höhne, and Jean Baptiste Faddoul. Chargrid: Towards Understanding 2D Documents. 2018.
[12] Koichi Kise, Motoi Iwata, and Keinosuke Matsumoto. On the application of Voronoi diagrams to page segmentation. . . . on Document Layout Interpretation . . . , pages 6­9, 1999. ISSN 10773142.
[13] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal Loss for Dense Object Detection. In 2017 IEEE International Conference on Computer Vision (ICCV), pages 2999­3007. IEEE, oct 2017. ISBN 978-1-5386-1032-9.

12

TUAN ANH & DAT: MULTI-STAGE ATTENTIONAL U-NET

[14] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), volume 31, pages 3431­3440. IEEE, jun 2015. ISBN 978-14673-6964-0.

[15] Song Mao, Azriel Rosenfeld, and Tapas Kanungo. Document structure analysis algorithms: a literature survey. pages 197­207, jan 2003.

[16] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed Representations of Words and Phrases and their Compositionality. Literatura de viajes y traducción, pages 319­333, oct 2013. ISSN 10495258.

[17] Anoop M. Namboodiri and Anil K. Jain. Document Structure and Layout Analysis. pages 29­48, 2007. ISSN 03029743.

[18] Sofia Ares Oliveira, Benoit Seguin, and Frederic Kaplan. dhSegment: A generic deeplearning approach for document segmentation. 2018.

[19] Ermelinda Oro and Massimo Ruffolo. PDF-TREX: An approach for recognizing and extracting tables from PDF documents. Proceedings of the International Conference on Document Analysis and Recognition, ICDAR, pages 906­910, 2009. ISSN 15205363.

[20] Rasmus Berg Palm, Florian Laws, and Ole Winther. Attend, Copy, Parse - End-to-end information extraction from documents. dec 2018.

[21] George Papandreou, Iasonas Kokkinos, and Pierre-Andre Savalle. Modeling local and global deformations in Deep Learning: Epitomic convolution, Multiple Instance Learning, and sliding window detection. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 390­399. IEEE, jun 2015. ISBN 978-1-4673-69640.

[22] Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global Vectors for Word Representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), volume 37, pages 1532­1543, Stroudsburg, PA, USA, jul 2014. Association for Computational Linguistics. ISBN 9781937284961.

[23] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. CoRR, 2015.

[24] Marcal Rusinol, Tayeb Benkhelfallah, and Vincent Poulain Dandecy. Field extraction from administrative documents by incremental structural templates. Proceedings of the International Conference on Document Analysis and Recognition, ICDAR, pages 1100­1104, 2013. ISSN 15205363.

[25] Luisa Simoes, Goncalo; Galhardas, Helena; Coheur. Information Extraction tasks : a survey. page 13, 2005.

[26] R. Smith. An Overview of the Tesseract OCR Engine. In Ninth International Conference on Document Analysis and Recognition (ICDAR 2007) Vol 2, pages 629­633. IEEE, sep 2007. ISBN 0-7695-2822-8.

[27] Zhiqiang Tang, Xi Peng, Shijie Geng, Yizhe Zhu, and Dimitris N. Metaxas. CU-Net: Coupled U-Nets. aug 2018. ISSN 1053-0487.

TUAN ANH & DAT: MULTI-STAGE ATTENTIONAL U-NET

13

[28] T. Tieleman and G. Hinton. Lecture 6.5--RmsProp: Divide the gradient by a running average of its recent magnitude, 2012.

[29] Yalin Wang, Ihsin T. Phillips, and Robert M. Haralick. Table structure understanding and its performance evaluation. Pattern Recognition, 37(7):1479­1497, 2004. ISSN 00313203.

[30] Xiao Yang, Ersin Yumer, Paul Asente, Mike Kraley, Daniel Kifer, and C. Lee Giles. Learning to extract semantic structure from documents using multimodal fully convolutional neural networks. In Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, 2017. ISBN 9781538604571.

[31] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. Self-Attention Generative Adversarial Networks. may 2018.

