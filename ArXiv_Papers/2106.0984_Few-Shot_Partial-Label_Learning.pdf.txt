Few-Shot Partial-Label Learning
Yunfeng Zhao1,2 , Guoxian Yu1,2 , Lei Liu1,2 , Zhongmin Yan1,2 , Lizhen Cui1,2 , Carlotta Domeniconi3
1School of Software Engineering, Shandong University, Jinan, Shandong, China 2Joint SDU-NTU Centre for Artificial Intelligence Research, Shandong University, Jinan, China.
3Department of Computer Science, George Mason University, VA, USA yunfengzhao@mail.sdu.edu.cn, {gxyu, l.liu, yzm, clz}@sdu.edu.cn, carlotta@cs.gmu.edu

arXiv:2106.00984v1 [cs.CL] 2 Jun 2021

Abstract
Partial-label learning (PLL) generally focuses on inducing a noise-tolerant multi-class classifier by training on overly-annotated samples, each of which is annotated with a set of labels, but only one is the valid label. A basic promise of existing PLL solutions is that there are sufficient partiallabel (PL) samples for training. However, it is more common than not to have just few PL samples at hand when dealing with new tasks. Furthermore, existing few-shot learning algorithms assume precise labels of the support set; as such, irrelevant labels may seriously mislead the meta-learner and thus lead to a compromised performance. How to enable PLL under a few-shot learning setting is an important problem, but not yet well studied. In this paper, we introduce an approach called FsPLL (Few-shot PLL). FsPLL first performs adaptive distance metric learning by an embedding network and rectifying prototypes on the tasks previously encountered. Next, it calculates the prototype of each class of a new task in the embedding network. An unseen example can then be classified via its distance to each prototype. Experimental results on widely-used few-shot datasets (Omniglot and miniImageNet) demonstrate that our FsPLL can achieve a superior performance than the state-of-the-art methods across different settings, and it needs fewer samples for quickly adapting to new tasks.
1 Introduction
In partial label learning (PLL) [Cour et al., 2011], each `partial-label' (PL) training sample is annotated with a set of candidate labels, among which only one is the groundtruth label. The aim of PLL is to induce a noise-tolerant multi-class classifier from such PL samples. PLL is currently one of the most prevalent weakly-supervised learning paradigms, which include inaccurate supervision, where the given labels do not always correspond to the ground-truth; incomplete supervision, where only a subset of the training
corresponding author

data is labeled; and inexact supervision, where the training data have only coarse-grained labels [Zhou, 2018]. This paper focuses on the first paradigm, where the given labels of the training data do not always represent the ground-truth. This learning problem arises in diverse domains, where a large number of inaccurately annotated samples can be easily collected, and it is very difficult (or impossible) to identify the true labels from the given ones [Zheng et al., 2017; Tu et al., 2020].
Let X  Rd denote the d-dimensional instance feature space and Y = {0, 1}l denote the label space with l distinct labels. The aim of PLL is to learn a noise-robust multiclass classification model f : X  Y with the PL dataset D = {(xi, yi)|1  i  n}, where xi  X is the feature vector of the i-th instance, yi is the one-hot label vector of candidate labels (Yi  Y) of the i-th instance, and zi  Yi is the unknown ground-truth label of this instance. The key challenge to address the PLL problem is to recover the ground-truth label concealed within the candidate label set for every training instance. Existing PLL methods can be roughly categorized into averaging-based disambiguation and identification-based disambiguation. The former class of methods typically equally treats each candidate label during the process of model induction, and performs label prediction by averaging the modeling outputs [Cour et al., 2011; Gong et al., 2017]. The second category of methods models the ground-truth label of the training instance as a latent variable, and estimates it via an iterative refining procedure [Yu and Zhang, 2017; Chai et al., 2020; Yu et al., 2018].
These PLL approaches rely on the assumption that sufficient labeled/unlabeled training data which are relevant to the task are available. They don't perform well in a few-shot scenario, where each class has only few training samples, annotated with inaccurate labels. Although Few-Shot Learning (FSL) has been extensively applied in diverse domains [Snell et al., 2017; Finn et al., 2017; Vanschoren, 2018], the existing FSL methods typically assume that the labels of the few-shot support samples are noise free. Unfortunately, the violation of this assumption seriously compromise the performance of the few-shot classifier, as shown in our experiments. To the best of our knowledge, how to make FSL effective with few-shot PL samples, is an open and under-studied problem. To bridge this gap, we propose a Few-shot PLL approach (FsPLL), which is based on the prototypical network [Snell

Feature space

bird goose goose

Qt
dog cat

...

Q

duck goose

1

bird

goose

duck

Embedding space

Prototype rectification

Figure 1: The overall schematic framework of FsPLL. FsPLL learns

an embedding network (f) and rectifies label confidence matrix {Qt}Tt=1 to perform adaptive distance metric learning based on PL samples previous encountered (Dttrain). Next, it updates label confidence matrix Q and rectifies prototypes w.r.t. the new task (Dtest) in the embedding space. An unseen example can then be classified

via its distance to prototypes. The `circle with 1' in the embedding

space is the contaminated prototype (no rectification) of `goose'.

et al., 2017] and on the local manifold [Belkin et al., 2006] in feature space, which states that instances that have similar feature vectors are more likely to share a same ground-truth label. More specifically, FsPLL first aims at iterative rectifying the ground-truth class prototypes of support PL samples and learning an embedding network, where, based on the previous tasks, every sample is closer to its ground-truth prototype, and further apart from its non-ground-truth prototypes. Next, it calculates the prototype of each class of the new task by the embedding network and prototype rectification. Then, an unseen example can be classified via its distance to each class prototype. The whole framework of FsPLL is illustrated in Fig. 1.
The main contributions of our work are as follows: (i) We focus on a practical and general PLL setting, where the training samples of the target task are few-shot. We also tackle the problem of noisy labels of few-shot support samples, which can seriously mislead the meta-learner when adapting to the target task. Both issues are not addressed by existing PLL solutions and few-shot/meta learning methods. (ii) We introduce a prototype rectification strategy with prototypical embedding network to learn the underlying groundtruth prototypes of support and query PL samples, which is less impacted by irrelevant labels and can more credibly adapt to new tasks. (iii) Extensive experiments on benchmark few-shot datasets show that our FsPLL outperforms the state-of-the-art PLL approaches [Zhang et al., 2017; Zhang et al., 2016; Wu and Zhang, 2018; Wang et al., 2019] and baseline FSL methods [Snell et al., 2017; Finn et al., 2017]. The overlook of irrelevant labels of few-shot PL samples indeed seriously compromises the performance of FSL methods, and our FsPLL can greatly remedy this problem.
2 Related work
2.1 Partial Label Learning PLL is different from learning from noisy labels [Natarajan et al., 2013], where training samples are incorrectly annotated with the wrong label; it is also different from semi-supervised

learning [Belkin et al., 2006], where some training samples

are completely unlabeled but can be leveraged for training;

and also different from weak-label learning [Sun et al., 2010;

Dong et al., 2018], where the labels of training samples are

incomplete. The current efforts for PLL can be roughly

grouped into two categories: the averaging-based and the

identification-based disambiguation.

The averaging-based disambiguation technique gener-

ally induces the classifier model by treating all can-

didate labels equally. Following this protocol, some

instances-based methods [Hu¨llermeier and Beringer, 2006;

Gong et al., 2017] classify the ground-truth y of an un-

seen instance x by averaging the candidate labels of its

neighbors, i.e., y where Si denotes

= the

arg maxyY candidate label

sxeitNof(xt)hIe(yi-th

Si), in-

stance and N (x) denotes the set of neighbors of instance

x, while other parametric methods [Cour et al., 2011;

Zhang et al., 2016] aim at inducing a parametric model

 by maximizing the gap between the average model-

ing output of the candidate labels and that of the non-

candidate ones, i.e., max(

m i=1

(

1 |Si

|

ySi F (xi, y; ) -

c|Sa1in|didayteSliaFbe(lxsi.

,

y; ))) where Si denotes the As to the identification-based

set of nondisambigua-

tion technique, the ground-truth labels of the training in-

stances are seen as latent variables and to be optimized by an

iterative refining procedure. Following this paradigm, some

methods train the model based on the maximum likelihood

criterion [Jin and Ghahramani, 2002] or the maximum mar-

gin criterion [Nguyen and Caruana, 2008]. Recently, some

teams mine the topological information [Zhang et al., 2016;

Feng and An, 2018] in the instance feature space to help the

optimization of label confidence.

Nevertheless, although these methods can disambiguate la-

bels and induce a noise-tolerance classifier by different tech-

niques, they can hardly work in a more universal scenario,

in which the PL samples we collected are few-shot, which

break the premise of many-shot training samples per label

for inducing a PLL classifier. In fact, existing PLL methods

still work in a close label set fashion. But in practice, we

may often come into new scenarios, where we can only col-

lect few-shot PL samples and each target label is annotated

to several samples. To enable PLL in this general setting, we

propose FsPLL to learn noise-robust class prototypes by an

embedding network and by rectifying prototypes therein.

2.2 Few-shot Learning Few-Shot Learning (FSL) [Li et al., 2006] is an example of meta-learning [Huisman et al., 2020], where a learner is trained on several related tasks during the meta-training phase, so that it can generalize well to unseen (but related) tasks using just few samples with supervision during the meta-testing phase. Existing FSL solutions mainly focus on supervised learning problems, and usually one may term as N -way K-shot classification, where N stands for the number of classes and K means the number of training samples per class, so each task contains KN samples. Given limited support samples for training, unreliable empirical risk minimization is the core issue of FSL, and existing solutions for

FSL can be grouped from the perspective of data, model and algorithm [Wang et al., 2020]. Data augmentation-based FSL methods aim to acquire more supervised training samples by generating more samples from original few-shot samples, weakly-labeled/unlabeled data or similar datasets [Douze et al., 2018], and thus to reduce the uncertainty of empirical risk minimization. Model-based FSL methods typically manage to shrink the ambient hypothesis space into a smaller one by extracting prior knowledge in the meta-training phase [Snell et al., 2017; Sung et al., 2018], so the empirical risk minimization becomes more reliable and the over-fitting issue is reduced. Algorithm-based FSL approaches use prior knowledge to guide the seek of optimal model parameters by providing a good initialized parameter or directly learning an optimizer for new tasks [Finn et al., 2017].
Unfortunately, most FSL methods ideally assume the support samples in meta-testing set is with accurate supervision, namely, these samples are precisely annotated with labels. But these support samples are PL ones with irrelevant labels, which mislead the adaption of FSL methods toward the target task (as shown in Fig. 1) and cause a compromised performance. To address this problem, our FsPLL performs the optimization of embedding network and prototype rectification therein in an iterative manner. In this way, the learnt embedding network and prototypes are less impacted by irrelevant labels of PL samples, and can credibly adapt to new tasks.

3 The Proposed Methodology
Suppose we are given a small support/training set of n PL samples D = {(xi, yi)|1  i  n} and its corresponding label space and feature space are Y = {0, 1}l and X  Rd, respectively. The goal of FsPLL is to induce a multi-class classifier f : X  Y, which can precisely predict the groundtruth label of an unseen instance x under this few-shot classification scenario. Different from existing PLL methods, FsPLL should and can utilize the knowledge previously acquired from meta-training phase to quickly adapt to the new classification task D in the meta-testing phase. In the metatraining phase, FsPLL learns an embedding network (metaknowledge) to project PL samples more nearby with their ground-truth prototypes and apart from their non ground-truth prototypes by iteratively rectifying these prototypes in this embedding space. In the meta-test phase, it rectifies the prototypes of support PL samples using the embedding work and then classifies new PL samples by their distance to rectified prototypes in the embedding space. In this paper, we take Prototypical Network (PN) [Snell et al., 2017] as the base of our embedding network. The framework overview of FsPLL is given in Fig. 1. The following subsections elaborates on the two phases.

3.1 Meta-training phase

The meta-training phase mainly aims to extract prior knowl-

edge from multiple relevant tasks for the target task. Sup-

pose we are given T

1 few-shot datasets (tasks) de-

noted as Dttrain (1  t  T ). For each dataset Dttrain = {Xts, X~ tq, Yt}, where Xts = (xt1, xt2, . . . , xtns )  Rd×ns denotes the data matrix of support samples, X~ tq =

(x~t1, x~t2, · · · , x~tnq )  Rd×nq denotes data matrix of query

samples, sponding

lYabtel=ma(tyri1tx,

oyf2t s, u· p· ·po, rytntssa)mplesR, la×nnds

is ns

the corre+ nq < n.

Ycti = 1 means the c-th label is a candidate label of the i-th

suanmdeprlley;inYgctliab=el

0 otherwise. Let Qt confidence matrix of

 Rl×ns denotes the support samples and it

is initialized c-th label as

tahseYgrto, uwnhde-rteruQthtcilaibnedlicoaftethsethie-thcosnafimdpelnec.e

of

the

From these datasets, we aim at learning an embedding net-

work, i.e., f : Rd  Rm, by which we can obtain the representation of every label in the embedding space and

can be more robust to irrelevant labels of support samples

therein. Suppose Pt totype/representation

=ma(tprit1x,

poft2,l

.c.l.a,sspltla)belsRol×f mtheist-tthhe

protask,

wdihnegrseppatcced.ePnNote[sSntheellpertoatol.t,y2p0e1o7f]tchoemc-ptuhtleasbtehleinprtohteoetympbeebdy-

ptc =

ns i=1

Ynscti

annotated wiit=h1

×cYftctoi (xinti )d,uwcehtihche

equally treats all PL samples prototype, neglects that some

PL samples actually not annotated with this label. Therefore,

PN gives contaminated prototypes. For example, prototype of

goose (`circle with 1') in Fig. 1 is misled by irrelevant labels.

These prototypes consequently compromise the classification

performance, especially when support PL samples with ex-

cessive irrelevant labels. To address this issue, FsPLL per-

forms prototype rectification and label confidence update in

an iterative way to seek noise-robust embedding network and

prototypes in the embedding space, as shown in Fig. 1. Fs-

PLL defines each prototype based on the confidence weighted

mean of the corresponding support samples in the embedding

space as follows:

pc =

ns i=1

Qci × f

ns i=1

Qci

(xi)

,

(1)

Unlike the prototypes optimized by PN, FsPLL rectifies the

prototypes using iterative updated label confident matrix Qt,

and thus explicitly accounts for the irrelevant labels of train-

ing samples.

It is expected for a sample to be closer to its ground-truth

prototype in the embedding space; this would enable a con-

fident label prediction in this space. Given this, we use a

softmax to update the label confidence matrix Qt as follows:

Qci =

exp(-d(f (xi),pc))

l c=1

exp(-d(f

(xi

),pc

))×Yci

0

if Yci = 1 otherwise

,

(2)

wtTwhheeeerlneabsdea(lmsfpo(lfexatix)Pti,Lpatcsn)admqppurloaetnocttiayfinpeebs epthdtceisianEmuthbceilgideuemaatbneedddbdiysitnargenfcseeprraibcneeg-. to labels of its neighborhood samples [Zhang et al., 2016; Wang et al., 2019]. We observe that PN and Eq. (1) disregard the neighborhood support samples when computing the prototype. Unlike these PLL methods that disambiguate in the original feature space or linearly projected subspace, FsPLL further updates the label confidence matrix in the embedding space as follows:

Qci

=

Qci

+

 |Nk (xi )|

xj Nk(xi)

Qcj ,

if Yci = 1

(3)

nwehigerheboNrhk(oxotid)isindcelutedremsitnheed

kb-yneEaurcelsitdesaamn pdliestsanocfexitin,

and the the em-

bedding space.  trade-offs the confidence from the sample

itself and those from neighborhood samples. In this way, Fs-

PLL utilizes local manifold of samples to rectify prototypes.

Based on the rectified prototypes and embedding network

f, we can predict the label of a query sample with a softmax over its distances to all prototypes in the embedding space as:

p(zjt = c | x~tj ) =

exp(-d(f(x~tj), ptc))

l i=1

exp(-d(f

(x~tj

),

pti ))

,

(4)

wsahmeprelez. jtTios

the unknown ground-truth make the representation of

label of the j-th query every query sample in

the embedding space closer to its ground-truth prototype and

apart from its non ground-truth prototypes, FsPLL minimizes

the negative log-probability of the most likely label of a query

example as follows:

J(,

x~ ti )

=

-

log( max
c=1,··· ,l

p (zjt

=

c

|

x~ ti ))

(5)

By minimizing the above equation, FsPLL can obtain the rec-

tified prototypes Pt and the corresponding embedding net-

wthoartkthpearca-mtheltaebreizlefdorbydifffefroenr ttatsakskDs ttirsanino.t

We want to always the

remark same.

The meta-training phase involves a lot of different tasks,

each of which is composed of support/query samples. To en-

able a good generalization ability, it attempts to gain the op-

timal mode parameter  by minimizing the average negative

log-probability of the most likely labels of all query samples

over T tasks as follows:

 = arg min

T1 t=1 nq

nq i=1

J(

,

x~ti

)

(6)

To this end, FsPLL obtains an embedding network f that is robust to irrelevant labels of PL samples across T tasks. Via

this network, a PL sample in the embedding space is made

closer to its ground-truth prototype than to other prototypes,

and the generalization and fast adaption ability are pursued

among T different tasks.

3.2 Meta-test phase In the meta-test phase, we are only given a small set of PL samples, which compose the target task with support and query samples. These support samples are overly-annotated with irrelevant labels, while query samples are without label information. We want to highlight that the labels of these PL samples are disjoint with the labels used in the meta-training phase. In other words, the PL samples are few-shot ones. Here, FsPLL aims to use the knowledge (embedding network f ) acquired in the meta-training phase to precisely annotate the query samples based on the inaccurately supervised few-shot support examples.
Formally, FsPLL aims to quickly generalize to a new task Dtest = {Xs, X~ q, Y}, where Xs  Rd×ns , X~ q  Rd×nq and Y  Rl×ns denote the data matrices of support examples, of query examples, and of labels of query examples, respectively. Alike the meta-training phase, FsPLL first computes the prototypes P  Rm×l of this new task in the embedding space using the confidence-weighted mean of support samples Xs and label confidence matrix Q as in Eq. (1).

Algorithm 1 FsPLL: Few-shot Partial Label Learning

Ienpopcuhte: s{Dfottrratrinai}nTti=n1g,

Dtest, maxEpoch = embedding network),

200 (number  = 0.5, k

of =

K2 - 1. Output: Embedding model f , predicted labels of query

sa1m: pfolerse{pzo1c,hz2=, ·1· ·, zmnqa}xEpoch do

2: for t = 1  T do

3:

Update Pt  Rm×l, Qt  Rl×ns via Eq. (1) and

Eq. (3) in an iterative way.

4: end for

5: Calculate loss via Eq. (6) and updata  via SGD.

6: end for

7: Update P  Rm×l, Q  Rl×ns via Eq. (1) and Eq. (3)

using f , Xs  Dtest and Y in an iterative way. 8: for i = 1  nq do

9: Get zi via Eq. (7) 10: end for

Then the label confidence matrix Q of the support samples is updated based on a softmax over their distances to prototypes as in Eq. (3). FsPLL repeats the above two steps to rectify the prototypes and update label confidence matrix for adapting to the target task. Note, the embedding network f is fixed during the above repetitive optimization.
Given a query sample xi, FsPLL classifies it using its distance to rectified prototypes P  Rm×l as follows:

zi

=

arg

max
q

p

(zi

= q | xi)

(q

= 1, · · ·

, l)

(7)

Algorithm 1 lists the procedure of meat-training phase (step 1-6) and meta-test phase (step 7-10) of FsPLL.

4 Experiments
4.1 Experimental Setup Datasets: We conduct experiments on two benchmark FSL datasets (Omniglot [Lake et al., 2011] and miniImageNet [Vinyals et al., 2016]). Following the canonical protocol adopted by previous PLL methods [Wang et al., 2019; Zhang et al., 2016], we generate the semi-synthetic PL datasets on Omniglot and miniImageNet by two controlling parameters p and r. p controls the proportion of PL samples, and r controls the number of irrelevant labels of a PL sample. EsaamchplDedttrfarionmco4n8s0is0t/e8d0ocflaNss1es=of30Ocmlansisgelsotw/meirneiIrmanadgoemNleyt without replacement. As to the meta-test set, we randomly selected another N2 classes from 1692/20 test classes without replacement. For each selected class, K1 = 5 (K2) samples were randomly chosen from 20/600 samples without replacement for the meta-training (meta-test) support samples, and the remaining/15 samples per class were randomly chosen as the query samples. More details on data split are given in the Supplementary file. Compared Methods: We compare FsPLL against four recent PLL methods (PL-ECOC [Zhang et al., 2017] , PLLEAF [Zhang et al., 2016], PALOC [Wu and Zhang, 2018], PL-AGGD [Wang et al., 2019]), two representative FSL methods (MAML [Finn et al., 2017], PN [Snell et al., 2017]),

FsPLL FsPLL-nM
PN MAML PL-AGGD PALOC PL-ECOC PL-LEAF FsPLL+
PN+ MAML+
FsPLL FsPLL-nM
PN MAML PL-AGGD PALOC PL-ECOC PL-LEAF FsPLL+
PN+ MAML+

N2 = 5 K2 = 5 K2 = 10

.892±.083 .852±.092 .579±.104 .673±.079 .664±.118 .616±.116 .473±.065 .629±.117 .995±.029 .965±.046 .858±.016

.895±.051 .886±.072 .636±.105 .647±.097 .777±.103 .726±.111 .694±.109 .768±.107 .997±.009 .985±.030 .902±.036

.673±.098 .616±.171 .476±.121 .476±.121 .496±.131 .473±.117 .425±.105 .484±.134 .975±.076 .825±.117 .675±.076

.742±.073 .706±.136 .559±.114 .553±.098 .668±.129 .611±.125 .481±.114 .650±.125 .997±.009 .926±.076 .798±.056

N2 = 10

N2 = 20

K2 = 5 K2 = 10 K2 = 5 K2 = 10

r=1

.789±.045 .823±.067 .712±.034 .757±.056

.776±.070 .816±.062 .695±.053 .745±.047

.435±.070 .485±.071 .317±.043 .360±.044

.592±.067 .642±.053 .514±.061 .544±.032

.576±.086 .714±.076 .450±.053 .649±.063

.528±.075 .651±.078 .447±.058 .568±.058

.513±.076 .619±.073 .279±.040 .418±.054

.568±.087 .712±.070 .495±.060 .630±.060

.990±.020 .993±.012 .986±.009 .990±.010

.956±.037 .981±.021 .939±.027 .968±.020

.849±.026 .877±.014 .774±.019 .831±.023

.712±.068 .566±.100 .378±.073 .458±.078

r=2 .756±.063 .654±.049 .665±.087 .494±.065 .442±.074 .270±.044 .549±.093 .427±.037

.689±.063 .584±.056 .321±.047 .472±.075

.490±.086 .456±.083 .361±.081 .489±.086

.664±.082 .591±.086 .531±.086 .645±.083

.451±.055 .385±.056 .175±.044 .436±.059

.578±.061 .525±.057 .343±.047 .586±.060

.991±.010 .994±.010 .986±.009 .989±.012

.871±.064 .945±.041 .850±.047 .926±.032

.783±.024 .760±.076 .668±.023 .727±.025

N2 = 30 K2 = 5 K2 = 10

.665±.015 .643±.008 .255±.032 .421±.018 .459±.043 .392±.047 .363±.042 .452±.043 .981±.009 .924±.025 .638±.035

.701±.046 .693±.042 .291±.034 .475±.065 .601±.057 .513±.049 .465±.046 .592±.054 .986±.008 .958±.018 .795±.189

.598±.063 .442±.053 .213±.032 .397±.043 .416±.053 .402±.049 .292±.046 .398±.045 .980±.009 .826±.040 .619±.026

.602±.038 .527±.048 .258±.033 .437±.036 .545±.054 .524±.061 .408±.052 .525±.058 .985±.007 .908±.030 .679±.450

Table 1: Classification accuracy (mean±std) of comparison methods on Omniglot. {FsPLL, PN, MAML}+ separately use the precise labels of meta-training support samples. N2: the number of support classes; K2: the number of training samples per class. The best performance in each setting is boldface.

0.8

0.6

0.6

0.5

Accuracy Accuracy Accuracy Accuracy

0.6

0.5

0.4

0.4

0.4

0.3

0.4

FsPLL+

PALOC

0.3

FsPLL+

PALOC

0.2

FsPLL+

PALOC

0.2

FsPLL+

PALOC

PN+

PL-ECOC

0.2

PN+

PL-ECOC

PN+

PL-ECOC

PN+

PL-ECOC

0.2

MAML+

PL-LEAF

PL-AGGD

0.1

MAML+ PL-AGGD

PL-LEAF

MAML+

PL-LEAF

0.1

PL-AGGD

MAML+ PL-AGGD

PL-LEAF

0

10

20

30

40

50

K2 (number of sample per class)

10

20

30

40

50

K2 (number of sample per class)

10

20

30

40

50

K2 (number of sample per class)

10

20

30

40

50

K2 (number of sample per class)

(a) N2 = 5

(b) N2 = 10

(c) N2 = 15

(d) N2 = 20

Figure 2: Accuracy of each compared method vs. K2 (number of support samples per class in the meta-test set) on miniImageNet (r = 1).

and FsPLL-nM (a variant of FsPLL) which disregards the local manifold of training samples but updates the label confidence matrix via Eq. (2) for prototype rectification. Each compared method is configured with the suggested parameters according to the corresponding literature, and the configuration details are given in the Supplementary file. As to our FsPLL, the trade-off parameter  is fixed as 0.5 (0 for FsPLL-nM), the number of nearest neighbors k = K2 - 1, the number of iterations for prototype rectification in each epoch is fixed to 10, the learning rate is fixed as 0.001 and cut into half per 20 epochs. FsPLL uses the embedding network proposed by [Vinyals et al., 2016]. For Omniglot, the size of prototypes is m = 64; while for miniImageNet, m = 1600. For non-FSL PLL methods, they also used the image features extracted by [Vinyals et al., 2016]. They only use the samples in meta-test set for training and validation. We randomly generate Dtrain (T = 100) as the meta-training tasks in each

round, and report average results on Dtest in 100 rounds for reducing the randomness.
4.2 Result Analysis Results on Omniglot: Table 1 reports the accuracy of each compared method on Omniglot as p is fixed to 1, r is fixed to 1, 2 or 3, N2 is fixed to 5, 10, 20 or 30, K2 is fixed to 5 or 10. Due to the page limit, we only report the results of each compared method when r = 1, 2, while the results with r = 3 are reported in the Supplementary file. From this Table, we have the following observations: (i) FsPLL significantly outperforms other compared methods across all the settings, which proves the effectiveness of FsPLL on few-shot PL samples. The performance margin between FsPLL and non-FSL methods are more prominent w.r.t. a small K2, since these non-FSL methods build on the promise of many-shot PL samples for training. Although PN

and MAML additionally use many tasks with support PL samples for the few-shot setting, they often lose to many-shot PLL methods. That is because they are both heavily misled by irrelevant labels of support samples. In contrast, our FsPLL is much less impacted by irrelevant labels of support samples, it reduces the negative impact of irrelevant labels by iteratively rectifying the prototypes and embedding network. By virtue of precise labels of meta-training samples, PN+ and MAML+ outperform many-shot PLL methods, but they still lose to FsPLL+ by a large margin. These observations confirm that the noisy labels of PL samples heavily mislead the adaption of meta-learner toward the target task. (ii) Prototype rectification can greatly reduce the negative impact of irrelevant labels of PL samples. This is supported by the performance margin between FsPLL (FsPLL+) and PN (PN+). They both perform distance metric learning in the embedding space to learn prototypes and classify samples therein, but FsPLL additionally rectifies the prototypes in the embedding space by explicitly modeling irrelevant labels and mining local manifold. (iii) Local manifold helps prototype rectification, this is verified by the clear margin between FsPLL and FsPLL-nM, especially when the number of irrelevant labels is large. (iv) As N2 steps from 5 to 30 under a fixed K2, the performance of each compared method gradually decreases. This is due to the increased class labels and task complexity. The random guess accuracy decrease from 1/5 to 1/30. Even though, FsPLL (FsPLL+) always maintains a better performance than PN (PN+) and MAML (MAML+). On the other hand, as the increase of K2 under a fixed N2, each compared method has an improved performance, since more support samples can be used for training. We see non-FSL PLL methods frequently outperform FSL methods (PN and MAML) when K2 = 10. This fact again proves the vulnerability of FSL methods on few-shot PL samples. (v) As the increase of r, all methods have a reduced performance, since the meta-training PL samples have more irrelevant labels, which seriously compromise the performance of many-shot PLL and FSL methods. This fact signifies the importance to account for PL samples. All compared methods have a relatively large standard deviation, that is due to the noisy labels of PL samples were randomly injected, and more noisy labels cause an even larger fluctuation. We applied signed-rank test to check the statistical significance between FsPLL/FsPLL+ and other compared methods, all p-values are small than 0.001. Results on miniImageNet: We also conduct experiments on miniImageNet with the following control setting: r  {1, 2, 3} with p = 1, N2  {5, 10, 15, 20} and K2  [5, 50]. We enlarge the range of K2 to check how FsPLL works in the many-shot setting. Due to the page limit, we only report the results of each compared method when r = 1. Similar trends can be observed with other settings, which are reported in the Supplementary file. As shown in Fig. 2, FsPLL again outperforms state-of-the-art FSL and many-shot PLL methods under different K2 shots, and the conclusions are similar as those on Omniglot. With the increase of K2, all methods show an increased performance, and FsPLL still has a higher accuracy than other methods when K2 > 20, which proves

87.1% 99.2%

PN

PN+

1.0

PN++

FsPLL

0.8

FsPLL+

FsPLL++

0.6

Accuracy

37.8%

0.4

0.2

Figure 3: The performance of PN and FsPLL in three different settings on Omniglot. FsPLL++ and PN++ use the precise labels of meta-training and meta-test samples, give the upper bound accuracy.

the effectiveness of FsPLL in many-shot settings.
4.3 Further Analysis Impact of PL samples on FSL methods: We conduct additional experiments to further investigate the impact of noisy support set of meta-training and meta-test. For this investigation, we introduce another variant FsPLL++, which uses precise labels of support samples in the meta-training and metatest stages. For comparison, we introduce PN++ for PN. So FsPLL++/PN++ gives the upper bound performance of FsPLL/PN. Fig. 3 shows the performance of FsPLL and PN and their variants under the setting of N2 = 10, K2 = 5 and r = 2 on Omniglot. In the figure, FsPLL/PN uses PL samples both in the meta-training and meta-test stages; while FsPLL+/PN+ uses precise labels of support samples in the meta-training stage, and PL samples in the meta-test stage. FsPLL significantly outperforms PN whenever there are support PL samples with irrelevant labels. They can have a comparable performance with precise labels of all support samples. FsPLL improves the accuracy of PN by 88%, and FsPLL+ improves this of PN+ by 13%. More importantly, FsPLL+ has a similar accuracy with FsPLL++. These results not only confirm the negative impact of noisy PL samples on FSL methods, but also prove the effectiveness of FsPLL on handling noisy labels of PL samples. Parameter analysis: We also study the parameter sensitivity of FsPLL w.r.t.  and k (see Eq. (3)), which uses the local manifold to update the label confidence matrix, and consequently rectify the prototype and embedding network f. The results is reported and analyzed in the Supplementary file. We find the local manifold indeed helps rectifying prototypes,  = 0.5 and k  K2 - 1 give a better performance.
5 Conclusion
This paper studies the problem of few-shot learning with noisy support samples and proves noisy labels of support samples can greatly compromise the performance. We introduce a Few-shot Partial Label Learning approach (FsPLL) to address this problem. FsPLL learns an embedding network and rectifies prototypes to reduce the impact of noisy labels. Extensive experimental results prove the effectiveness of FsPLL in both few-shot and many-shot settings.

71.2% 99.1% 99.2%

References
[Belkin et al., 2006] Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. JMLR, 7(11):2399­2434, 2006.
[Chai et al., 2020] Jing Chai, Ivor W Tsang, and Weijie Chen. Large margin partial label machine. TNNLS, 31(7):2594­2608, 2020.
[Cour et al., 2011] Timothee Cour, Ben Sapp, and Ben Taskar. Learning from partial labels. JMLR, 12:1501­ 1536, 2011.
[Dong et al., 2018] Hao-Chen Dong, Yu-Feng Li, and ZhiHua Zhou. Learning from semi-supervised weak-label data. In AAAI, pages 2926­2933, 2018.
[Douze et al., 2018] Matthijs Douze, Arthur Szlam, Bharath Hariharan, and Herve´ Je´gou. Low-shot learning with large-scale diffusion. In CVPR, pages 3349­3358, 2018.
[Feng and An, 2018] Lei Feng and Bo An. Leveraging latent label distributions for partial label learning. In IJCAI, pages 2107­2113, 2018.
[Finn et al., 2017] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In ICML, pages 1126­1135, 2017.
[Gong et al., 2017] Chen Gong, Tongliang Liu, Yuanyan Tang, Jian Yang, Jie Yang, and Dacheng Tao. A regularization approach for instance-based superset label learning. TCYB, 48(3):967­978, 2017.
[Huisman et al., 2020] Mike Huisman, Jan N van Rijn, and Aske Plaat. A survey of deep meta-learning. arXiv preprint arXiv:2010.03522, 2020.
[Hu¨llermeier and Beringer, 2006] Eyke Hu¨llermeier and Ju¨rgen Beringer. Learning from ambiguously labeled examples. Intelligent Data Analysis, 10(5):419­439, 2006.
[Jin and Ghahramani, 2002] Rong Jin and Zoubin Ghahramani. Learning with multiple labels. NeurIPS, 15:921­ 928, 2002.
[Lake et al., 2011] Brenden Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua Tenenbaum. One shot learning of simple visual concepts. In Annual Cog. Sci., pages 2568­ 2573, 2011.
[Li et al., 2006] Fei-Fei Li, Fergus Rob, and Perona Pietro. One-shot learning of object categories. TPAMI, 28(4):594­611, 2006.
[Natarajan et al., 2013] Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with noisy labels. In NeurIPS, pages 1196­1204, 2013.
[Nguyen and Caruana, 2008] Nam Nguyen and Rich Caruana. Classification with partial labels. In KDD, pages 551­559, 2008.
[Snell et al., 2017] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In NeurIPS, pages 4077­4087, 2017.

[Sun et al., 2010] Yu-Yin Sun, Yin Zhang, and Zhi-Hua Zhou. Multi-label learning with weak label. In AAAI, pages 593­598, 2010.
[Sung et al., 2018] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales. Learning to compare: Relation network for few-shot learning. In CVPR, pages 1199­1208, 2018.
[Tu et al., 2020] Jinzheng Tu, Guoxian Yu, Carlotta Domeniconi, Jun Wang, Guoqiang Xiao, and Maozu Guo. Multi-label crowd consensus via joint matrix factorization. KAIS, 62(4):1341­1369, 2020.
[Vanschoren, 2018] Joaquin Vanschoren. Meta-learning: A survey. arXiv preprint arXiv:1810.03548, 2018.
[Vinyals et al., 2016] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. NeurIPS, 29:3630­3638, 2016.
[Wang et al., 2019] Deng-Bao Wang, Li Li, and Min-Ling Zhang. Adaptive graph guided disambiguation for partial label learning. In KDD, pages 83­91, 2019.
[Wang et al., 2020] Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni. Generalizing from a few examples: A survey on few-shot learning. ACM Computing Surveys, 53(3):1­34, 2020.
[Wu and Zhang, 2018] Xuan Wu and Min-Ling Zhang. Towards enabling binary decomposition for partial label learning. In IJCAI, pages 2868­2874, 2018.
[Yu and Zhang, 2017] Fei Yu and Min-Ling Zhang. Maximum margin partial label learning. Machine Learning, 106(4):573­593, 2017.
[Yu et al., 2018] Guoxian Yu, Xia Chen, Carlotta Domeniconi, Jun Wang, Zhao Li, Zili Zhang, and Xindong Wu. Feature-induced partial multi-label learning. In ICDM, pages 1398­1403, 2018.
[Zhang et al., 2016] Min-Ling Zhang, Bin-Bin Zhou, and Xu-Ying Liu. Partial label learning via feature-aware disambiguation. In KDD, pages 1335­1344, 2016.
[Zhang et al., 2017] Min-Ling Zhang, Fei Yu, and Cai-Zhi Tang. Disambiguation-free partial label learning. TKDE, 29(10):2155­2167, 2017.
[Zheng et al., 2017] Yudian Zheng, Guoliang Li, Yuanbing Li, Caihua Shan, and Reynold Cheng. Truth inference in crowdsourcing: Is the problem solved? VLDB Endowment, 10(5):541­552, 2017.
[Zhou, 2018] Zhi-Hua Zhou. A brief introduction to weakly supervised learning. National Science Review, 5(1):44­53, 2018.

Supplementary File of FsPLL

arXiv:2106.00984v1 [cs.CL] 2 Jun 2021

1 Experimental Setup

1.1 Datasets

In this section, we conduct experiments on two benchmark

FSL datasets (Omniglot and miniImageNet). Following the canonical protocol adopted by previous PLL methods [Wang

et al., 2019; Zhang et al., 2016], we generate the semi-

synthetic PL datasets on Omniglot and miniImageNet by two

controlling parameters p and r. Here, p controls the propor-

tion of training examples that are partially labeled, and r con-

trols the number of irrelevant labels within candidate label set

of each instance.

Omniglot [Lake et al., 2011] is consisted of 1623 hand-

written characters collected from 50 alphabets, each charac-

ter includes 20 instances drawn by different human subjects.

Following [Vinyals et al., 2016], we resized the grayscale im-

ages to 28 × 28 and augmented the character classes with

rotation in 90, 180 and 270 degrees. We used original 1200

characters plus rotations for meta-training (4800 classes in to-

tal) and the remaining classes, including rotations, for meta-

test (1692 classes in total). To avoid too repeated computa-

tions, of N1

ea=ch3t0rairnainndgodmaltyasseat mDpttlreadinc=las{seXs tsf,rXo~mtq ,

Yt} consists 4800 classes

without replacement. As to the meta-test set, we randomly

selected N2 classes from 1692 classes without replacement. For each selected class, we randomly sampled K1 = 5 (K2) samples without replacement as training (test) support sam-

ples, and took the remaining samples as query samples.

miniImageNet [Vinyals et al., 2016] is collected from the larger ILSVRC-12 dataset [Russakovsky et al., 2015] and

consisted of 100 classes with 600 examples per class (60000

color images of size 84 × 84 in total). We use the splits (64

classes for training, 16 classes for validation and 20 classes

for test) proposed by [Ravi and Larochelle, 2016] for exper-

iments. We use 64 training classes plus 16 validation classes

for meta-training, and remaining 20 test classes for meta-test.

Similar as what consisted of N1

we had set up on = 30 classes were

rOanmdnoimglloyt,saemacphledDtftrroaimn

80 classes without replacement. As to the meta-test set, we

randomly selected another N2 classes from 20 test classes without replacement. For each selected class, K1 = 5 (K2) samples were randomly chosen from 600 samples without

replacement for respective support samples, and another 15

samples per class were randomly chosen as the query sam-

ples.
1.2 Compared Methods To comparatively study the performance of FsPLL, we compare it against four state-of-the-art PLL methods and two representative FSL methods, each configured with the suggested parameters according to the corresponding literature:
· PL-ECOC [Zhang et al., 2017] transforms PLL into a disambiguation-free problem via error-correcting output codes. Suggested configuration: codeword length L = 10 · log2 l .
· PL-LEAF [Zhang et al., 2016] learns from PL examples based on feature-aware disambiguation. Suggested configuration: K = 10, C1 = 10 and C2 = 1.
· PALOC [Wu and Zhang, 2018] adapts one-vs-one decomposition strategy to enable binary decomposition for learning from PLL examples. Suggested configuration: µ = 10.
· PL-AGGD [Wang et al., 2019] proposes a unified framework to jointly optimize the ground-truth label confidences, instance similarity graph, and model parameters to achieve generalization performance. Suggested configuration: k = 10, T = 20,  = 1, µ = 1 and  = 0.05.
· MAML [Finn et al., 2017] is a representative optimization based meta learning algorithm that can adopt to a new task with a small number of gradient steps and a small amount of training data. Suggested configuration: using the same neural network architecture as the embedding function used by [Vinyals et al., 2016]
· PN [Snell et al., 2017] serves as the baseline of FsPLL, it directly uses PL samples to seek the prototypes.
· FsPLL-nM disregards the local manifold of training samples to update the label confidence matrix and to rectify the prototypes.
As to our FsPLL, the trade-off parameter  is fixed as 0.5 (0 for FsPLL-nM), the number of nearest neighbors k = K2 -1, the number of iterations for prototype rectification in each epoch is fixed to 10, the learning rate is fixed as 0.001 and cut into half per 20 epochs. The used embedding network is proposed by [Vinyals et al., 2016] and is consisted of four

convolutional blocks, each of which is a 64-filter 3 × 3 convolution followed by a batch normalization layer, a ReLU non-linearity and a 2 × 2 max-pooling layer. For Omniglot, the size of prototypes is m = 64, while for miniImageNet, m = 1600.
For non-FSL PLL methods, they also used the image features extracted by [Vinyals et al., 2016]. They only use the N2  K2 meta-test support samples for training, and use the same meta-test query samples for validation. To reduce random impact, we randomly generated Dtrain (T = 100) as the meta-training tasks, and Dtest as the meta-testing task in each round, report average results in 100 rounds.
2 Additional results on Omniglot and miniImageNet
Table 1 gives the results of compared methods on Omniglot datasets under the setting of r = 3. Figure 2 and 3 reveal the results of compared methods on miniImageNet with r = 2 and r = 3. These results are consistent with the observations stated in the main text, and also prove the effectiveness of our FsPLL.

Accuracy Accuracy

0.6

r=1 r=2

0.55

r=3

0.5

0.45

0.4

0.35

2 4 6 8 10 12 14
k (number of nearest neighbors)
(a) Accuracy vs. k

0.6

r=1

r=2

0.5

r=3

0.4

0.3

0.2 0.001 0.01 0.05 0.1 0.5 1 5 10
(trade-off parameter)
(b) Accuarcy vs. 

Figure 1: Accuracy of FsPLL on miniImageNet under different input values of k and of , here N2 = 10 and K2 = 10. (a) Accuracy varies with k ( = 0.5); (b) Accuracy varies with  (k = K2 - 1).

3 Parameter sensitivity analysis
Figure 1 gives the main text mentioned results of FsPLL under different input values of k and . As shown in Fig. 1(a) and Fig. 1(b), FsPLL first manifests a gradually increased accuracy until k  K2 - 1 (  0.5). This trend shows the benefit of local manifold for updating the label confidence matrix and rectifying the prototypes. However, the accuracy starts to decrease as k and  further increase. That is due to the over-weight (large ) of local manifold and the inclusion of unreliable neighbors (large k) from other classes.

References
[Finn et al., 2017] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In ICML, pages 1126­1135, 2017.
[Lake et al., 2011] Brenden Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua Tenenbaum. One shot learning of simple visual concepts. In Annual Cog. Sci., pages 2568­ 2573, 2011.
[Ravi and Larochelle, 2016] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In ICLR, pages 1­11, 2016.
[Russakovsky et al., 2015] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. IJCV, 115(3):211­252, 2015.
[Snell et al., 2017] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In NeurIPS, pages 4077­4087, 2017.
[Vinyals et al., 2016] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. NeurIPS, 29:3630­3638, 2016.
[Wang et al., 2019] Deng-Bao Wang, Li Li, and Min-Ling Zhang. Adaptive graph guided disambiguation for partial label learning. In KDD, pages 83­91, 2019.
[Wu and Zhang, 2018] Xuan Wu and Min-Ling Zhang. Towards enabling binary decomposition for partial label learning. In IJCAI, pages 2868­2874, 2018.
[Zhang et al., 2016] Min-Ling Zhang, Bin-Bin Zhou, and Xu-Ying Liu. Partial label learning via feature-aware disambiguation. In KDD, pages 1335­1344, 2016.
[Zhang et al., 2017] Min-Ling Zhang, Fei Yu, and Cai-Zhi Tang. Disambiguation-free partial label learning. TKDE, 29(10):2155­2167, 2017.

FsPLL FsPLL-nM
PN MAML PL-AGGD PALOC PL-ECOC PL-LEAF FsPLL+
PN+ MAML+

N2 = 5 K2 = 5 K2 = 10

.591± .118 .369±.158 .337±.115 .379±.124 .311±.114 .308±.117 .373±.125 .246±.103 .972±.072 .555±.154 .452±.135

.708±.131 .482±.180 .398±.114 .469±.112 .439±.134 .435±.126 .401±.098 .452±.109 .996±.011 .722±.143 .693±.098

N2 = 10

N2 = 20

K2 = 5 K2 = 10 K2 = 5 K2 = 10

r=3

.571±.105 .631±.081 .473±.061 .557±.057

.404±.109 .519±.105 .351±.066 .461±.064

.277±.066 .328±.063 .184±.037 .222±.037

.342±.076 .403±.068 .367±.071 .381±.053

.419±.089 .537±.094 .376±.068 .468±.066

.380±.089 .523±.092 .349±.056 .483±.062

.293±.076 .435±.081 .079±.024 .264±.048

.415±.091 .520±.089 .388±.056 .445±.058

.990±.014 .994±.009 .985±.009 .989±.012

.744±.092 .885±.064 .748±.062 .874±.046

.608±.076 .665±.037 .473±.027 .598±.035

N2 = 30 K2 = 5 K2 = 10

.432±.036 .313±.052 .143±.026 .345±.023 .375±.060 .310±.051 .225±.048 .319±.043 .980±.010 .724±.047 .598±.027

.495±.037 .415±.050 .171±.026 .407±.061 .453±.054 .407±.056 .326±.053 .414±.050 .985±.008 .852±.037 .596±.035

Table 1: Classification accuracy (mean±std) of comparison methods on Omniglot. {FsPLL, PN, MAML}+ separately use the precise labels of meta-training support samples. N2: the number of support classes; K2: the number of training samples per class. The best performance in each setting is boldface.

Accuracy Accuracy Accuracy Accuracy

0.8

0.6

0.4

FsPLL+

PALOC

0.2

PN+

PL-ECOC

MAML+

PL-LEAF

PL-AGGD

10

20

30

40

50

K2 (number of sample per class)

(a) N2 = 5

0.6

0.4

0.2

FsPLL+

PALOC

PN+

PL-ECOC

MAML+

PL-LEAF

0

PL-AGGD

10 20 30 40 50 K2 (number of sample per class)
(b) N2 = 10

0.6

0.4

0.2

FsPLL+

PALOC

PN+

PL-ECOC

MAML+

PL-LEAF

PL-AGGD
0

10

20

30

40

50

K2 (number of sample per class)

(c) N2 = 15

0.5

0.4

0.3

0.2

FsPLL+

PALOC

0.1

PN+

PL-ECOC

0

MAML+

PL-LEAF

PL-AGGD

10

20

30

40

50

K2 (number of sample per class)

(d) N2 = 20

Figure 2: Accuracy of each compared method vs. K2 (number of support samples per class) on miniImageNet (r = 2).

Accuracy Accuracy Accuracy Accuracy

0.8

0.6

0.4

0.2

FsPLL+ PN+

PALOC PL-ECOC

MAML+

PL-LEAF

0

PL-AGGD

10

20

30

40

50

K2 (number of sample per class)

(a) N2 = 5

0.6

0.4

0.2

FsPLL+

PALOC

PN+

PL-ECOC

0

MAML+

PL-LEAF

PL-AGGD

10 20 30 40 50 K2 (number of sample per class)
(b) N2 = 10

0.6

0.4

0.2

FsPLL+

PALOC

PN+

PL-ECOC

MAML+

PL-LEAF

PL-AGGD
0

10

20

30

40

50

K2 (number of sample per class)

(c) N2 = 15

0.5

0.4

0.3

0.2

FsPLL+

PALOC

0.1

PN+

PL-ECOC

0

MAML+

PL-LEAF

PL-AGGD

10

20

30

40

50

K2 (number of sample per class)

(d) N2 = 20

Figure 3: Accuracy of each compared method vs. K2 (number of support samples per class) on miniImageNet (r = 3).

