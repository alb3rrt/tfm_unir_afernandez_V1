JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

1

Dense Nested Attention Network for Infrared Small Target Detection
Boyang Li, Chao Xiao, Longguang Wang, Yingqian Wang, Zaiping Lin, Miao Li, Wei An, Yulan Guo

arXiv:2106.00487v1 [cs.CV] 1 Jun 2021

Abstract--Single-frame infrared small target (SIRST) detection aims at separating small targets from clutter backgrounds. With the advances of deep learning, CNN-based methods have yielded promising results in generic object detection due to their powerful modeling capability. However, existing CNNbased methods cannot be directly applied for infrared small targets since pooling layers in their networks could lead to the loss of targets in deep layers. To handle this problem, we propose a dense nested attention network (DNANet) in this paper. Specifically, we design a dense nested interactive module (DNIM) to achieve progressive interaction among highlevel and low-level features. With the repeated interaction in DNIM, infrared small targets in deep layers can be maintained. Based on DNIM, we further propose a cascaded channel and spatial attention module (CSAM) to adaptively enhance multilevel features. With our DNANet, contextual information of small targets can be well incorporated and fully exploited by repeated fusion and enhancement. Moreover, we develop an infrared small target dataset (namely, NUDT-SIRST) and propose a set of evaluation metrics to conduct comprehensive performance evaluation. Experiments on both public and our self-developed datasets demonstrate the effectiveness of our method. Compared to other state-of-the-art methods, our method achieves better performance in terms of probability of detection (Pd), false-alarm rate (Fa), and intersection of union (IoU ).
Index Terms--Infrared small target detection, deep learning, dense nested interactive module, channel and spatial attention, dataset.
I. INTRODUCTION
S INGLE-frame infrared small target (SIRST) detection is widely used in many applications such as maritime surveillance [1], early warning systems [2], and precise guidance [3]. Compared to generic object detection, infrared small target detection has several unique characteristics: 1) Small: Due to the long imaging distance, infrared targets are generally small, ranging from one pixel to tens of pixels in the images. 2) Dim: Infrared target usually have low signal-to-clutter ratio (SCR) and are easily immersed in heavy noise and clutter background. 3) Shapeless: Infrared small targets have limited shape characteristics. 4) Changeable: The sizes and shapes of infrared targets vary a lot among different scenarios.
To detect infrared small targets, numerous traditional methods have been proposed, including filtering-based methods
This work was partially supported in part by the National Natural Science Foundation of China (Nos. 61972435, 61401474, 61921001, 62001478)
Boyang Li, Chao Xiao, Longguang Wang, Yingqian Wang, Zaiping Lin, Miao Li, Wei An, Yulan Guo are with the College of Electronic Science and Technology, National University of Defense Technology (NUDT), P. R. China. Yulan Guo is also with the School of Electronics and Communication Engineering, Sun Yat-sen University, P. R. China. Emails: {liboyang20, xiaochao12, wanglongguang15, wangyingqian16, linzaiping, lm8866, anwei, yulan.guo}@nudt.edu.cn. (Corresponding author: Zaiping Lin, Miao Li)

Point Target

Tophat

IPI

RIPT

DNA-Net

GT

Spot Target
Extended Target

Fig. 1: Visual results achieved by Tophat [4], IPI [5], RIPT [6], and our network for different infrared small targets. The correctly detected target, false alarm, and miss detection areas are highlighted by red, yellow, and green dotted circle, respectively.
[4], [7], local-contrast-based methods [8]­[11], and low-rankbased methods [5], [6], [12], [13]. However, these traditional methods heavily rely on handcrafted features. When the characteristics of real scenes (e.g., target size, target shape, SCR, and clutter background) change dramatically, it is difficult to use handcrafted features and fixed hyper-parameters to handle such variations.
Different from traditional methods, CNN-based methods can learn features of infrared small targets in a data-driven manner. Liu et al. [14] proposed the first CNN-based SIRST detection method. They designed a multi-layer perception (MLP) network with 5 layers for infrared small target detection. Then, McIntosh et al. [15] fine-tuned several existing generic object detection networks (e.g., Faster-RCNN [16] and Yolo-v3 [17]) for infrared small target detection. Specifically, Dai et al. [18] proposed the first segmentation-based SIRST detection method. They designed an asymmetric contextual module (ACM) to replace the plain skip connection of Unet [19]. Although recent CNN-based methods have achieved the state-of-the-art performance, most of them only fine-tuned these networks designed for generic objects. Since the size of infrared small targets is much smaller than generic objects, directly applying these methods for SIRST detection can easily lead to the loss of small targets in deep layers.
Inspired by the success of nested structure in medical image segmentation [20]­[23], we propose a dense nested attention network (namely, DNANet) to maintain small targets in deep

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

2

Unprecise Precise

shape shape

&

&

weak

high

response response

A

A

A

A

A

A

A

II. RELATED WORK
In this section, we briefly review the major works in infrared small target detection and SIRST dataset.

A

A

A

Weak Enhanced Response Response

A

A

(a) U-shape

Weak Enhanced Response Response

A

A

(b) Nested U-shape (DNA-Net)

Fig. 2: The representation of small targets in deep CNN layers of (a) U-shape network (b) our Nested U-shape (DNANet) network.

layers. Specifically, we design a tri-directional dense nested interactive module (DNIM) with a cascaded channel and spatial attention module (CSAM) to achieve progressive feature interaction and adaptive feature enhancement. Within our DNIM, multiple nodes are imposed on the pathway between the encoder and decoder sub-networks. As shown in Fig. 2(b), all nodes are connected with each other to form a nestedshape network. Using DNIM, those middle nodes can receive features from its own and the adjacent two layers, leading to repeated multi-layer feature fusion at deep layers. Through repeated feature fusion and enhancement, our network can maintain the targets in deep layers. Meanwhile, contextual information of maintained small targets can be well incorporated and fully exploited. In addition, we develop a novel infrared small target dataset (namely, the NUDT-SIRST dataset) to evaluate the performance of SIRST detection methods under different clutter backgrounds, target shapes, and target sizes. In summary, the contributions of this paper can be summarized as follows.
· We propose a DNANet to maintain small targets in deep layers. The contextual information of small targets can be well incorporated and fully exploited by repeated feature fusion and enhancement.
· A dense nested interactive module and a channel-spatial attention module are proposed to achieve progressive feature fusion and adaptive feature enhancement.
· We develop an infrared small target dataset (namely, NUDT-SIRST). To the best of our knowledge, our dataset is the largest dataset with numerous categories of target shapes, rich target sizes, diverse clutter backgrounds, and ground truth annotations.
· Experiments on both public and our NUDT datasets demonstrate the superior performance of our method. Compared to existing methods, our method is more robust to the variations of clutter background, target size, and target shape (as shown in Fig. 1).
The remainder of this paper is organized as follows: In Section II, we briefly review the related work. In Section III, we introduce the architecture of our DNANet and our self-developed dataset in details. Section IV represents the experimental results. Section V gives the conclusion.

A. Single-frame Infrared Small Target Detection
SIRST detection has been extensively investigated for decades. The traditional paradigm achieves SIRST detection by measuring the discontinuity between targets and backgrounds. Typical methods include filtering-based methods [24], local contrast measure based methods [25], [26], and low rank based methods [27], [28]. However, these traditional methods heavily rely on handcrafted features. When real scenes change dramatically, such as in SCR, clutter background, target shape, and target size, it is difficult to use handcrafted features and fixed hyper-parameters to handle such variations. To address this problem, recent CNN-based methods learn trainable features in a data-driven manner. Thanks to the large quantity of data and the powerful model fitting capability of CNNs, these methods achieve better performance than traditional ones.
Existing CNN-based methods can be divided into detection based methods and segmentation based methods. Liu et al. [14] first introduced a generic target detection framework for infrared small target detection. They designed a multi-layer perception (MLP) network with 5 layers for infrared small target detection. Then, McIntosh et al. [15] fine-tuned several generic target detection network (e.g., Faster-RCNN [16] and Yolo-v3 [17]) and used the optimized eigen-vectors as input to achieve improved performance.
Recently, segmentation-based methods have attracted increasing attention. That is because, these methods can produce both pixel-level classification and localization outputs. Dai et al. [18] proposed the first segmentation-based network (i.e., ACM). They designed an asymmetric contextual module to aggregate features from shallow layers and deep layers. Then, Dai et al. [29] further improved their ACM by introducing a dilated local contrast measure. Specifically, a feature cyclic shift scheme was designed to achieve a trainable local contrast measure. Moreover, Wang et al. [30] decomposed the infrared target detection problem into two opposed sub-problems (i.e., miss detection and false alarm) and used a conditional generative adversarial network (CGAN) to achieve the trade-off between miss detection and false alarm for infrared small target detection.
Although the performance is continuously improved by recent networks, the loss of small targets in deep layers still remains. This problem ultimately results in the poor robustness to dramatic scene changes (e.g., clutter background, targets with different SCR, shape, and size).
B. Datasets for SIRST Detection
Existing open-source dataset in infrared small target detection is scarce, most traditional methods are evaluated on their in-house datasets. Only a few infrared small target datasets are released by CNN-based methods [18], [30]. Wang et al. [30] built the first big and open SIRST dataset. This dataset

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

3

PartFeature Extraction Module

A

A

A

A

A

A

A

A

A

Part Feature Pyramid Fusion Module

Keep Dimension
UP Sampling

Feature Fusion
Feature Fusion

Part Eight-connected Neighborhood Clustering Algorithm

A

A

A

UP Sampling

Feature Fusion

A

A

UP Sampling

Feature Fusion

A
A Channel and Spatial Attention Module

Down/Up-Sampling

UP Sampling
Dense Plain Skip Connection

Feature Fusion

1x1 Conv

Bi-direction Interactive Skip Connection

Multi-scale Feature Concatnation

Eight-connected Neighborhood

Fig. 3: An illustration of the proposed dense nested attention network (DNANet). (a) Feature extraction module. Input images are first fed into the dense nested interactive module (DNIM) to achieve progressive feature fusion. Then, features from different semantic levels are adaptively enhanced by a channel and spatial attention module (CSAM). (b) Feature pyramid fusion module (FPFM). The enhanced features are upsampled and concatenated to achieve multi-layer output fusion. (c) Eightconnected neighborhood clustering algorithm. The segmentation map is clustered and the centroid of each target region is finally determined.

includes 10000 training images and 100 test images. Since many targets in this dataset do not meet the definition of society of photo-optical instrumentation engineers (SPIE) [31] and have obvious synthesized traces with illogical annotations. These problems may lead to the inapplicability toward SIRST detection. Dai et al. [18] built the first real SIRST dataset with high-quality images and labels. However, the number of images in NUAA-SIRST is 427 (256 for training), which cannot well cover dramatic scene changes in infrared small target detection. Moreover, these real infrared data are all manually labelled with many inaccurately labeled pixels.
Although these open-sourced datasets greatly prompt the prosperity of SIRST detection, their limited data capacity, data variety, and poor annotation hinder the further development of this field. Synthesized data can be easily generated to achieve higher variety and annotation quality at very low cost (i.e., time and money). Hence, we developed a new NUDT-SIRST dataset with numerous categories of target, rich target sizes, diverse clutter backgrounds, and accurate annotations. The effectiveness of our dataset is evaluated in Section IV.
III. METHODOLOGY
In this work, we introduce our DNANet in details. The overall architecture of the proposed method is shown in Fig. 3.
A. Overall Architecture
As illustrated in Fig. 3, our DNANet takes a SIRST image as its input and sequentially performs feature extraction (Section III-B), feature pyramid fusion (Section III-C), and eightconnected neighborhood clustering (Section III-D).
Section III-B introduces our feature extraction module, including the dense nested interactive module (DNIM) and the channel-spatial attention module (CSAM). Input images

are first preprocessed and fed into the backbone of DNIM to extract multi-layer features. Then, multi-layer features are repeatedly fused at the middle convolution nodes of skip connection and then are gradually passed into the decoder subnetworks. Due to the semantic gap at multi-layer feature fusion stage of DNIM, we used CSAM to adaptively enhance these multi-level features for achieving better feature fusion. Section III-C presents the feature pyramid fusion module. Enhanced multi-layer features at each scale are upscaled to the same size. Next, the shallow-layer features with rich spatial information and deep-layer features with high-level information are concatenated to generate robust feature maps. Section III-D elaborates the eight-connected neighborhood clustering module. Feature maps G are fed into this module to calculate the spatial location of target centroid, which is then used for comparison in Section IV. In Section III-E, we introduce our NUDT-SIRST dataset.
B. The Feature Extraction Module
1) Motivation: As shown in Fig. 4(a), traditional U-shape structure [19] consists of an encoder, a decoder, and plain skip connections. The encoder is used to enlarge the receptive field and extract high-level information. Decoder helps to recover to the same size of input images. The plain skip connection acts as a bridge to pass these low-level and high-level features from encoder to decoder subnetworks.
To achieve powerful contextual information modeling capability, a straightforward idea is to continuously increase the layers of the network. In this way, high-level information can be obtained and larger receptive field can be achieved. However, infrared small targets are different significantly in their sizes, ranging from one pixel (i.e., point targets) to tens of pixels (i.e., extended targets). With the increase of network layers, high-level information of extended targets is obtained,

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

4

Plain Skip Connection

Depth=2 : Depth=3 : Depth=4 : Depth=5 :

Input Feature
L
(i, j)

MC(L)
Channel Attention Module

MS(L)
Spatial Attention Module

(a) Channel and Spatial Attention Module

Enhanced Feature
`L '
(i, j)

(a) U-shape Paradigm

(b) U-shape Sub-networks Stacking

L(0,0)

L(0,1)

L(0,2)

L(0,3)

L(0,4)

L(1,0)

L(1,1)

L(1,2)

L(1,3)

L(2,0)

L(2,1)

L(2,2)

L(3,0)

L(3,1)

(c) Tri-direction Interactive Skip Connection

L(4,0)
(d) Dense Nested Skip Connection

Fig. 4: An illustration of the U-shape structure and our dense nested structure. The insight comes from the multiple U-shape subnetwork stacking. The representation of small targets in the deep layers is maintained and the high-level information is extracted.

`L '

(i-1, j)
`L '
(i, j-1)
`L '

Feature Concatenate

Channel

L

and spatial

attention

(i+1, j-1) (b) Attention-based Multi-layer Features Fully Fusion

`L '
(i, j)

Fig. 5: Channel and spatial attention module. CSAM is used to reduce the semantic gap at multi-layer feature fusion stage of DNIM.

and nested bi-direction interactive skip connection, the stack of feature maps represented by Li,j is generated as:

while the point targets are easily lost after multiple pooling

operation. Therefore, we should design a special module to

extract high-level features and maintain the representation of

small targets in the deep layers.

2) The Dense Nested Interactive Module: As shown in

Fig. 4(b), we stack multiple U-shape sub-networks together

to build a dense nested structure. Since the optimal receptive

field for different sizes of target varies a lot, these U-shape

sub-networks with different depths are naturally suitable for

targets with different sizes. Based on this idea, we impose

multiple nodes in the pathway between encoder and decoder

sub-networks. All of these middle nodes are densely connected

with each other to form a nested-shape network. As shown

in Fig. 4(c) and (d), each node can receive features from its

own and the adjacent layers, leading to repeated multi-layer

feature fusion. As a result, the representation of small targets

is maintained in the deep layers and thus better results can be

achieved.

In this paper, we stack I layers of DNIM to form our feature

extraction module. Without loss of generality, we take the ith(i = 0, 1, 2, ..., I) DNIM layer as an example to introduce this structure, as shown in Fig. 4(c) and (d). Assume Li,j denote the output of node L^i,j, where i is the ith downsampling layer along the encoder and j is the jth convolution

layer of dense block along the plain skip pathway. When

j = 0 , each node only receives features from dense plain skip connection. The stack of feature maps represented by Li,j are

computed as

Li,j = Pmax(F (Li-1,j )),

(1)

where F(·) denotes multiple cascaded convolution layers of the same convolution block. Pmax(·) denotes max-pooling with a stride of 2. When j > 0 , each node receives outputs from three directions including dense plain skip connection

Li,j =

F

Li,k

j-1 k=0

,

Pmax

(F

(Li+1,j-1

)),

U

(F

(Li-1,j))

,

(2)

where U(·) denotes the up-sampling layer, and [ ·, ·] denotes
the concatenation layer. 3) Channel and Spatial Attention Module: As shown in
Fig. 5(b), CSAM is used for adaptive feature enhancement at
the multi-layer feature fusion stage of DNIM.
The CSAM consists of two cascaded attention units. The feature maps Li,j from node L^i,j (i  {0, 1, 2, ...I}, j 
{0, 1, 2, ...J}) are sequentially processed by a 1D channel attention map Mc  RCi×1×1 and a 2D spatial attention map Ms  R1×Hi×Wi . As shown in Fig. 5(a). The channel attention process can be summarized as follows:

Mc(L) =  M LP (Pmax(L))), (M LP (Pavg(L)) , (3)

L = Mc(L)  L,

(4)

where  denotes the element-wise multiplication,  denotes sigmoid function, Ci, Hi, Wi denote the number of channels, height, and width of Li,j. Pavg(·) and Pmax(·) denote average pooling and max pooling with a stride of 2, respectively.
The shared network is composed of a multi-layer perceptron
(MLP) with one hidden layer. Before multiplication, the attention maps Mc(L) are stretched to the size of Mc(L)  RCi×Hi×Wi .
Similar to channel attention process, the spatial attention
process can be summarized as follows:

Ms(L ) =  f 7×7(Pmax(L ))), (Pavg(L ) ,

(5)

L = Ms(L )  L ,

(6)

where f 7×7 represents a convolutional operation with a filter
size of 7×7. The attention maps Ms(L) are also stretched to the size of Mc(L)  RCi×Hi×Wi before multiplication.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

5

TABLE I: Main characteristics of several popular SIRST datasets. Note that, our NUDT-SIRST dataset contains common background scenes, various target types, and most ground truth annotations.

Datasets NUAA-SIRST(ACM) [18] NUST-SIRST [30] CQU-SIRST(IPI) [5] NUDT-SIRST(ours)

Image Type real
synthetic synthetic synthetic

Background Scene Cloud/City/Sea Cloud/City/River/Road Cloud/City/Sea Cloud/City/Sea/Field/Highlight

#Image 427
10000 1676 1327

Label Type Manual Coarse Label Manual Coarse Label
Ground Truth Ground Truth

Target Type Point/Spot/Extended Point/Spot/Extended
Point/Spot Point/Spot/Extended

Public 
×

Various Real Backgrounds

City

Field

Highlight

Sea

Cloud

Various Target(Size/Type)

Point

Spot

UAV

Plane

Ship

Rich Pose

Fig. 6: Samples of our NUDT-SIRST dataset. Our dataset covers multiple real infrared backgrounds, various target types, rich poses, and ground truth labels. , , , and  represents different moving directions of targets.

C. The Feature Pyramid Fusion Module
After the feature extraction module, we develop a feature pyramid fusion module to aggregate the resultant multi-layer features. As shown in Fig. 3, we first upscale multi-layer features to the same size of Lie,nJ up  RCi×H0×W0 i  {0, 1, ..., I}. Then, the shallow-layer feature Lsehnaullpow with rich spatial and profile information and deep-layer feature Ldeneepup with rich semantic information are concatenated to generate global robust feature maps:

G = {L0en,J up, L1en,J up, ..., LIen,J up}.

(7)

D. The Eight-connected Neighborhood Clustering Module
After the feature pyramid fusion module, we introduce an eight-connected neighborhood clustering module [32] to clutter all pixels and calculate the centroid of each target. If any two pixels g(m0,n0), g(m1,n1) in feature maps G have intersection areas in their eight neighborhoods (e.g., Eq. 8) and have the same value (0 or 1) (e.g., Eq. 9), these two pixels are considered to be in a connected area. Pixels in a connected area belong to the same targets. Once all targets in the image are determined, centroid is calculated as their coordinate.

N8(m0, n0)  N8(m1, n1) = ,

(8)

g(m0,n0) = g(m1,n1), g(m0,n0), g(m1,n1)  G.

(9)

E. The NUDT-SIRST dataset
Quality, quantity, and scene diversity of data significantly affect the performance of CNN-based methods. Existing CNNbased methods mainly use real infrared data with manual annotations. However, it is costly to collect a large-scale dataset with accurate pixel-level annotations. These issues hinder the further development of CNN-based methods. Inspired by the solutions in other data-scarcity fields (e.g., ship detection [33]), we develop a large-scale infrared small target dataset (namely, the NUDT-SIRST dataset). Our NUDTSIRST dataset enables performance evaluation of CNN-based methods under numerous categories of target type, rich target size, and diverse clutter backgrounds.
Our NUDT-SIRST is compared with existing SIRST datasets in Table I. It has 5 main background scenes and covers various targets. All scenes in our dataset are rendered by synthesizing real infrared backgrounds with various virtual infrared targets. For each scene, all images have a resolution of 256 × 256. To render realistic infrared small targets, we adopt an adaptive target size function and apply a 5×5 Gaussian blur. We illustrate 5 main background scenes (including city, field, highlight, sea, and cloud) in Fig. 6.

IV. EXPERIMENT
In this section, we first introduce our evaluation metrics and training protocol. Then, we compare our DNANet to several state-of-the-art SIRST detection methods. Finally, we present ablation studies to investigate our network.

A. Evaluation Metrics

Pioneering CNN-based works [18], [29], [30] mainly use pixel-level evaluation metrics like IoU , precision, and recall values. These metrics mainly focus on the target shape evaluation. However, infrared small targets are generally lack of shapes and textures. For a 3 × 3 small target, one falsely predicted pixel will cause 11.1% decrease in Pd. Consequently, these pixel-level evaluation metrics are unsuitable for small targets. Actually, the overall target localization is the most important criteria for SIRST detection. Therefore, we adopt Pd and Fa to evaluate both localization ability and use IoU to evaluate shape description ability.
1) Intersection over Union: Intersection over Union (IoU ) is a pixel-level evaluation metric. It evaluates profile description ability of the algorithm. IoU is calculated by the ratio of intersection and union area between the predictions and labels.

IoU = Ainter ,

(10)

AAll

where Ainter and AAll represent the interaction areas and all

areas, respectively.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

6

TABLE II: IoU , Pd, and Fa values achieved by different SOTA methods on the NUDT-SIRST and NUAA-SIRST datasets, For IoU and Pd. larger values indicate higher performance. For Fa, smaller values indicate higher performance. The best results are in red and the second best results are in blue.

Method Description
Filtering Based: Top-Hat [4] Filtering Based: Max-Median [7] Local Contrast Based: WSLCM [11] Local Contrast Based: TLLCM [10] Local Rank Based: IPI [5] Local Rank Based: NRAM [12] Local Rank Based: RIPT [6] Local Rank Based: PSTNN [13] Local Rank Based: MSLSTIPT [3] CNN Based: MDvsFA-cGAN [30] CNN Based: ACM [18] CNN Based: ALCNet [29] DNANet-VGG10 (ours) DNANet-ResNet10 (ours) DNANet-ResNet18 (ours) DNANet-ResNet34 (ours)

NUDT-SIRST (Tr=50%) IoU (×10-2) Pd(×10-2) Fa(×10-6)

20.72

78.41

166.7

4.197

58.41

36.89

2.283

56.82

1309

2.176

62.01

1608

17.76

74.49

41.23

6.927

56.40

19.27

29.44

91.85

344.3

14.85

66.13

44.17

8.342

47.40

888.1

75.14

90.47

25.34

67.08

95.97

10.18

81.40

96.51

9.261

85.23

96.95

6.782

86.36

97.39

6.897

87.09

98.73

4.223

86.87

97.98

3.710

NUAA-SIRST (Tr=50%) IoU (×10-2) Pd(×10-2) Fa(×10-6)

7.143

79.84

1012

4.172

69.20

55.33

1.158

77.95

5446

1.029

79.09

5899

25.67

85.55

11.47

12.16

74.52

13.85

11.05

79.08

22.61

22.40

77.95

29.11

10.30

82.13

1131

60.30

89.35

56.35

70.33

93.91

3.728

73.33

96.57

30.47

74.96

97.34

26.73

76.24

97.71

12.80

77.47

98.48

2.353

77.54

98.10

2.510

2) Probability of Detection: Probability of Detection (Pd) is a target-level evaluation metric. It measures the ratio of correctly predicted target number over all target number. Pd is defined as follows:

Pd

=

Tcorrect , TAll

(11)

where Tcorrect and TAll represent the numbers of correctly predicted targets and all targets, respectively. If the centroid derivation of the target is less than a maximum allowed derivation, we consider those targets as correctly predicted ones. We set the maximum centroid derivation as 3 in this paper.
3) False-Alarm Rate: False-Alarm Rate (Fa) is another target-level evaluation metric. It is used to measure the ratio of falsely predicted pixels over all image pixels. Fa is defined as follows:

Fa

=

Pfalse , PAll

(12)

where Pfalse and PAll represent the numbers of falsely predicted pixels and all image pixels, respectively. If the centroid derivation of the target is larger than a maximum allowed derivation, we consider those pixels as falsely predicted ones. We set the maximum centroid derivation as 3 in this paper.
4) Receiver Operation Characteristics: Receiver Operation Characteristics (ROC) is used to describe the changing trends of the detection probability (Pd) under varying false alarm rate (Fa).

B. Implementation Details
As discussed in Section IV-E, we used the published NUAA-SIRST dataset [29] and our NUDT-SIRST dataset for both training and test. All input images with different initial

sizes were first resized to a resolution of 256 × 256. To accelerate the network convergence, all images were normalized to ensure that their values centered at zero.
In this paper, we adopted a segmentation network as our baseline to generate a pixel-level segmentation map and then used a clustering algorithm to achieve target localization. The U-net paradigm with ResNets [34] was chosen as our segmentation backbone. The number of down-sampling layer i was chosen as 4. Our network was trained using the Soft-IoU loss function and optimized by the Adagrad method [35]. We initialized the weights and bias of our model using the Xavier method [36]. We set the batch size to 16 and the learning rate to 0.05. All models were implemented in PyTorch [37] on a computer with an Intel i7 7700H @ 2.80 GHz CPU and an Nvidia GeForce 1080Ti GPU.
C. Comparison to the State-of-the-art Methods
To demonstrate the superiority of our method, we compare our DNANet to several state-of-the-art (SOTA) methods, including traditional methods (Top-Hat [4], Max-Median [7], WSLCM [11], TLLCM [10], IPI [5], NRAM [12], RIPT [6], PSTNN [13], MSLSTIPT [3]) and CNN-based methods (MDvsFA-cGAN [30], ACM [18], ALCNet [29]) on the NUAA-SIRST and NUDT-SIRST datasets. For fair comparison, we retrained all the CNN-based methods on the same training datasets as our DNANet. It is worth noting that we use our implementations for these methods for fair comparison. Most of these open-source CNN-based codes are rewritten by pytorch and released in our homepage: https://github.com/ YeRen123455/Infrared-Small-Target-Detection.
1) Quantitative Results: For all the compared algorithms, we first obtain their predicts and then performed noise suppression by setting a threshold to remove low-response areas. Specifically, the adaptive threshold was calculated by Eq. 13 for traditional methods. For CNN-based methods, we followed

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

Input

Tophat

IPI

RIPT

ALCNet

MDvsFA DNA-Net

7
GT

(1)

(2)

(3)

(4)

(5)

(6)

(a)

(b)

(c)

(d)

(e)

(f)

(g)

(h)

Fig. 7: Qualitative results achieved by different SIRST detection methods. For better visualization, the target area is enlarged in the right-top corner. The correctly detected target, false alarm, and miss detection areas are highlighted by red, yellow, and green dotted circles, respectively. Our DNANet can achieve output with precise target localization, shape segmentation with a lower false alarm rate.

their original papers and adopted their fixed thresholds (i.e., 0, 0, 0.5 for ACM [18], ALCNet [29], and MDvsFA-cGAN [30], respectively). We kept all remaining parameters the same as their original papers.
Tadaptive = M ax[M ax(G) × 0.7, 0.5 × (G) + avg(G)], (13)
where M ax(G) represents the largest value of output. (G) and avg(G) mean the standard derivation and average value of output, respectively.
Quantitative results are shown in Table II. The improvements achieved by our DNANet over traditional methods are significant. That is because, both NUDT-SIRST and NUAASIRST contain challenging images with different SCR, clutter background, target shape, and target size. Our DNANet can

learn discriminative features robust to scene variations. In contrast, the traditional methods are usually designed for specific scenes (e.g., specific target size and clutter background). The manually-selected parameters (e.g., structure size in Tophat and patch size in IPI) limit the generalization performance of these methods. Moreover, we also observe that the IoU improvements are obviously higher than the improvement of Pd and Fa. That is because, the traditional methods mainly focus on the overall localization of the target instead of precise shape matching. It also validates our claim that using pixel-level evaluation metric (such as IoU ) introduces unfair comparison and results in inaccurate conclusion.
As shown in Table II, the improvements achieved by DNANet over other CNN-based methods (i.e., MDvsFAcGAN, ACM, and ALCNet) are obvious. That is because, we

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

8

Fig. 8: 3D visualization results of different methods on 6 test images.

redesign a new backbone network that is tailored for SIRST detection. The U-shape basic backbone with our dense nested interactive skip connection module can achieve progressive feature fusion and selectively enhance the informative features in deep CNN layers. Consequently, intrinsic features of infrared small targets can be maintained and fully learned in the network. It is also worth noting that the IoU improvements of our method on NUDT-SIRST is significantly higher than those on the NUAA-SIRST dataset. That is because, our dataset contains more challenging scenes with various target sizes, types and poses. Our channel and spatial attention module and feature pyramid fusion module help to learn discriminative features to achieve better performance.
2) Qualitative Results: Qualitative results on two datasets (i.e., NUDT-SIRST, NUAA-SIRST) are shown in Fig. 7 and Fig. 8. Compared with traditional methods, our method can produce output with precise target localization and shape segmentation under very low false alarm rate. Nonetheless, the traditional methods only perform well on point targets, (e.g., image-3), and easily generate lots of false alarm areas

in local highlight areas (e.g., image-4 and image-6). Moreover, as shown in Fig. 9, we divided our NUDT-SIRST dataset into point targets subset, spot targets subset, and extended targets subset. With the increase of spot and extended targets ratio, traditional methods suffers dramatic performance decrease while our DNANet maintains high accuracy. That is because, the performance of traditional methods rely heavily on handcrafeted features and cannot adapt to the variations of target sizes. The CNN-based methods (i.e., MDvsFA-cGAN, ACM, and ALCNet) perform much better than traditional methods. However, due to the complicated scenes in our NUDT-SIRST, MDvsFA-cGAN produces many false alarm and miss detection areas (Fig. 8). Our DNANet is more robust to these scene changes. Moreover, our DNANet can generate better shape segmentation than ALCNet. That is because, our designed new backbone can well adapt to various clutter background, target shape, and target size challenges and thus achieves better performance.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

9

Probablity of detection Probablity of detection Probablity of detection

1

1

1

Top-hat

IPI

Top-hat

0.8

RIPT DNANet

0.8

IPI

RIPT

0.8

MDvsFA-cGAN

DNANet

MDvsFA-cGAN

0.6

0.6

0.4

0.4

0.6

Top-hat

IPI

RIPT

DNANet

0.4

MDvsFA-cGAN

0.2

0.2

0.2

0

0

0

0

0.2

0.4

0.6

0.8

1

0

0.2

0.4

0.6

0.8

1

0

0.2

0.4

0.6

0.8

1

False-alarm ratio

#10-4

False-alarm ratio

#10-4

False-alarm ratio

#10-4

(a)

(b)

(c)

Fig. 9: ROC performance on (a) point targets subset, (b) point targets subset + spot targets subset, (c) all kinds of targets of NUDT-SIRST. With the increase of spot and extended targets ratio, the performance of traditional methods suffers dramatic decrease. In constrast, the performance our DNANet is stable.

DNANet

L(4,0)

L(3,1)

L(2,2)

L(1,3)

L(0,4)

DNANet

L(4,0)

L(3,1)

L(2,2)

L(1,3)

L(0,4)

Image 1
w/o DNIM

L(4,0)

L(3,1)

L(2,2)

L(1,3)

L(0,4)

Image 2
w/o DNIM

L(4,0)

L(3,1)

L(2,2)

L(1,3)

L(0,4)

Image 1

Image 2

Fig. 10: Visualization map of DNANet and DNANet w/o DNIM. The output of DNANet is marked by a solid red frame. The feature maps from the deep layer of DNANet w/o DNIM loses representation of small targets. It finally results in low values and miss detection in the output layer.

D. Ablation Study

In this subsection, we compare our DNANet with several variants to investigate the potential benefits introduced by our network modules and design choice.
1) The Dense Nested Interactive Module (DNIM): The dense nested interactive skip-connection module is used to interact with features at different scale levels to enlarge receptive fields while maintain fine-grained features at the finest scale level. To demonstrate the effectiveness of our DNIM, we introduced three network variants and made their model sizes comparable for fair comparison.
· DNANet w/o DNIM: We replaced the dense nested interactive skip connection module with a regular plain skip connection module.
· DNANet-left-to-right: As shown in Fig. 11(c), multiple U-shape subnetworks with different depths are stacked from left to right. Each node in the middle part of the network can receive features from its own and the lower layer.
· DNANet-top-to-bottom: We stacked the U-shape subnetworks from top to bottom to generate DNANet-top-tobottom, as shown in Fig. 11(b). Different from DNANetleft-to-right, this variant stacks U-shape subnetworks with three kinds of depth and only its core part uses tri-

0,0

0,4 0,0

0,1

0,2

0,3

0,4

1,0

1,3

1,0

1,1

1,2

1,3

2,0

2,2

2,0

2,1

2,2

3,0

3,1

3,0

3,1

4,0 (a) Plain SC

4,0 (b) Top-to-Bottom

0,0

0,1

0,2

0,3

0,4 0,0

0,1

0,2

0,3

0,4

1,0

1,1

1,2

1,3

1,0

1,1

1,2

1,3

2,0

2,1

2,2

2,0

2,1

2,2

3,0

3,1

3,0

3,1

4,0 (c) Left-to-Right

4,0 (d) Bi-direction

Fig. 11: Three variants of DNIM. (a) DNANet w/o DNIM. (b) DNANet-top-to-bottom. (c) DNANet-left-to-right. (d) DNANet, each color represents different U-shape subnetworks.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

DNANet

L(4,0)

L(3,1)

L(2,2)

L(1,3)

L(0,4)

10

DNANet

L(4,0)

L(3,1)

L(2,2)

L(1,3)

L(0,4)

Image 1
w/o CSAM

L(4,0)

L(3,1)

L(2,2)

L(1,3)

L(0,4)

Image 2
w/o CSAM

L(4,0)

L(3,1)

L(2,2)

L(1,3)

L(0,4)

Image 1

Image 2

Fig. 12: Visualization map of DNANet and DNANet w/o CSAM. The output of DNANet is circled by a solid red frame. The feature maps from the deep layer of DNANet have high values representation to informative cues and finally results in precise profile segmentation in output layer.

TABLE III: IoU , Pd, and Fa values achieved by main variants of DNANet and DNIM on the NUDT-SIRST and NUAASIRST datasets. Top-to-bottom and Left-to-right mean stack U-shape sub-network from different directions

TABLE IV: IoU , Pd, and Fa values achieved by main variants of DNANet and CSAM on the NUDT-SIRST and NUAASIRST datasets.  means element-wise summing as feature fusion method.

Model
DNANet w/o DNIM DNANet-top-to-bottom DNANet-left-to-right
DNANet-ResNet18

#Params(M)
4.71 4.72 4.71 4.70

Datasets

NUDT-SIRST

NUAA-SIRST

85.01/96.50/8.521 75.12/97.34/12.05

85.75/96.96/7.682 75.94/97.71/11.84

85.89/97.29/4.649 76.59/98.10/11.05

87.09/98.73/4.223 77.47/98.48/2.353

Model
DNANet w/o CSAM DNANet w/o CSAM
DNANet w/o CA DNANet w/o SA DNANet-ResNet18

#Params(M)
4.70 4.71 4.73 4.73 4.70

Datasets

NUDT-SIRST

NUAA-SIRST

85.90/96.62/5.738 75.81/96.19/22.12

85.25/96.62/6.710 75.35/95.82/34.97

86.27/96.96/4.881 76.20/96.96/12.69

86.14/96.73/4.128 76.69/97.34/10.96

87.09/98.73/4.223 77.47/98.48/2.353

direction skip connection.
Table III shows the comparative results achieved by DNANet and its variants. It can be observed that the IoU , Pd, and Fa values of DNANet w/o DNIM suffer decreases of 2.08%, 2.23%, and an increase of 4.298×10-6 on the NUDT-SIRST dataset, respectively. Similar results are also observed on the NUAA-SIRST dataset. That is because, DNIM progressively aggregates features at multiple scales to maintain the target information at the finest scale for better performance. Visualization maps shown in Fig. 10 also demonstrates the effectiveness of our DNIM. Small targets are lost in the feature maps of the deep layer in DNANet w/o DNIM (i.e., L(4,0), L(3,1)).
As shown in Table III, DNANet-left-to-right suffers decreases of 1.20%, 1.44%, and an increase of 0.426 ×10-6 in terms of IoU , Pd, and Fa values over DNANet on the NUDT-SIRST dataset. That is because, each node in DNANetleft-to-right only interacts with the deep layer instead of full interaction among shallow, their-own, and deep layers. Shallow layer has rich localization and profile information, but the information is not fully incorporated at the skip connection stage. Consequently, this variant has limited performance.
As compared to our DNANet, the variant DNANet-top-tobottom suffers decreases of 1.34%, 1.77%, and an increase of 3.459 ×10-6 in terms of IoU , Pd, and Fa values on NUDTSIRST dataset. That is because, only the core part of this variant adopts tri-direction skip connection, the remaining part still uses the plain skip connection. Moreover, its tri-direction interactive area is relatively shallow, high-level information can not be fully exploited at shallow layers.
2) The Channel and Spatial Attention Module (CSAM): The channel and spatial attention module is used for adaptive feature enhancement to achieve better feature fusion. To inves-

tigate the benefits introduced by this module, we compare our DNANet with four variants. To achieve fair comparison (i.e., comparable model size), we increased the number of filters of all convolution layers of four variants to made their model sizes slightly larger than DNANet.
· DNANet w/o CSAM: We removed the channel and spatial attention module in this variant and directly concatenate multi-layer features for subsequent process.
· DNANet w/o CSAM (Element-wise summation): We replaced CSAM with common element-wise summation in this variant to explore the effectiveness of CSAM. Specifically, we used 1×1 convolution operation and upsampling/down-sampling to make features from different layer identical. Then, an element-wise summation is used to achieve multi-layer feature fusion.
· DNANet w/o channel attention: We removed the channel attention operation in this variant to evaluate its contribution.
· DNANet w/o spatial attention: We canceled the spatial attention operation in this variant to investigate the benefit introduced by spatial attention.
If CSAM is removed, the performance suffers decreases of 1.19%/1.84%, 2.11%/2.11%, and 1.515/2.487 ×10-6 in terms of IoU , Pd, and Fa for DNANet w/o CSAM and DNANet w/o CSAM  on the NUDT-SIRST dataset, respectively. Similar results are achieved on the NUAA-SIRST dataset. This clearly demonstrates the importance of the channel and spatial attention module. As shown in Fig. 12, with the help of CSAM, the feature maps from the deep layer of DNANet have high response to informative cues and finally results in precise shape segmentation.
As shown in Table IV, DNANet w/o channel attention

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

Input Image

Ground Truth (Public)

Predict Ours

Input Image

Ground Truth (Public)

Predict Ours

11

TABLE V: IoU , Pd, and Fa values achieved by DNANet on real datasets. The DNANet is trained on mixed dataset with different real images ratios

# Real images 0% (0/427) 10% (42/427) 20% (85/427)
30% (128/427)

# Synthesized images 50% (663/1327) 50% (663/1327) 50% (663/1327) 50% (663/1327)

Performance on real SIRST 61.69/88.97/30.26 68.26/94.67/14.11 72.38/95.63/8.932 74.53/97.71/4.341

V. CONCLUSION

Fig. 13: Samples of the input images, public ground truth masks [18] (manually labeled), and output of our DNANet trained on mixed dataset. Our method can even produce more precise segmentation result than manually labeled ground truth masks.
suffers decreases of 0.82%, 1.77%, and an increase of 0.658 ×10-6 in terms of IoU , Pd, and Fa values over DNANet on NUDT-SIRST dataset. That is because, channel attention unit in our DNANet can better exploit informative channels to enhance the representation capability of features.
If the spatial attention unit is removed, the performance suffers decreases of 0.95%, 2.00%, and an increase of 0.095 ×10-6 in terms of IoU , Pd, and Fa values over the DNANet on NUDT-SIRST dataset. That is because, infrared small targets are easily immersed in heavy cloud and noise, it is hard to distinguish these small and dim targets from the background. Spatial attention facilitates the network to pay attention to local informative areas and thus produces better results.
E. Potential of The Synthesized Dataset
In this section, we evaluate the potential of our synthesized dataset for real IRST tasks. Specifically, we mixed real SIRST images from NUAA-SIRST and synthesized SIRST images from NUDT-SIRST with different ratios to train our DNANet. With more real SIRST images being included for training, the performance of our network is gradually improved. Even on the extreme situation with only 42 real images, our DNANet still achieves better performance than ACM with 100% real SIRST images. That is because, our synthesized dataset can well cover the main challenges for infrared small target detection (i.e., different SCR, clutter background, target shape, and target size). Consequently, the huge cost for collecting real SIRST images can be reduced.
Moreover, we compared the output of our network trained on the mixed dataset with the manually labeled masks of NUAA-SIRST in Fig. 13. It can be observed that the output of our network has more reasonable shape segmentation. That is because, the synthesized SIRST images have absolutely precise labels. The network can learn the essence of infrared small targets with sufficiently well labeled data and finally contribute to the improvement of real SIRST images. As a result, our network can generate better visual performance than ground truth label.

In this paper, we propose a DNANet to achieve SIRST detection. Different from existing CNN-based SIRST detection methods, we explicitly handle the problem of small targets being lost in deep layers by designing a new tri-direction dense nested interactive module with a cascaded channel and spatial attention model. The intrinsic information of small targets can be incorporated and fully exploited by repeated fusion and enhancement. Moreover, we develop an open SIRST dataset to evaluate the performance of infrared small target detection with respect to challenging scenes. We also reorganized a set of evaluation metrics. Experiments on both our dataset and the public dataset have shown the superiority of our method over the state-of-the-art methods.
REFERENCES
[1] M. Teutsch and W. Kru¨ger, "Classification of small boats in infrared images for maritime surveillance," in 2010 International WaterSide Security Conference. IEEE, 2010, pp. 1­7. 1
[2] H. Deng, X. Sun, M. Liu, C. Ye, and X. Zhou, "Small infrared target detection based on weighted local difference measure," IEEE Transactions on Geoscience and Remote Sensing, vol. 54, no. 7, pp. 4204­4214, 2016. 1
[3] Y. Sun, J. Yang, and W. An, "Infrared dim and small target detection via multiple subspace learning and spatial-temporal patch-tensor model," IEEE Transactions on Geoscience and Remote Sensing, 2020. 1, 6
[4] J.-F. Rivest and R. Fortin, "Detection of dim targets in digital infrared imagery by morphological image processing," Optical Engineering, vol. 35, no. 7, pp. 1886­1893, 1996. 1, 6
[5] C. Gao, D. Meng, Y. Yang, Y. Wang, X. Zhou, and A. G. Hauptmann, "Infrared patch-image model for small target detection in a single image," IEEE Transactions on Image Processing, vol. 22, no. 12, pp. 4996­5009, 2013. 1, 5, 6
[6] Y. Dai and Y. Wu, "Reweighted infrared patch-tensor model with both nonlocal and local priors for single-frame small target detection," IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, vol. 10, no. 8, pp. 3752­3767, 2017. 1, 6
[7] S. D. Deshpande, M. H. Er, R. Venkateswarlu, and P. Chan, "Maxmean and max-median filters for detection of small targets," in Signal and Data Processing of Small Targets 1999, vol. 3809. International Society for Optics and Photonics, 1999, pp. 74­83. 1, 6
[8] C. P. Chen, H. Li, Y. Wei, T. Xia, and Y. Y. Tang, "A local contrast method for small infrared target detection," IEEE Transactions on Geoscience and Remote Sensing, vol. 52, no. 1, pp. 574­581, 2013. 1
[9] J. Han, Y. Ma, B. Zhou, F. Fan, K. Liang, and Y. Fang, "A robust infrared small target detection algorithm based on human visual system," IEEE Geoscience and Remote Sensing Letters, vol. 11, no. 12, pp. 2168­2172, 2014. 1
[10] J. Han, S. Moradi, I. Faramarzi, C. Liu, H. Zhang, and Q. Zhao, "A local contrast method for infrared small-target detection utilizing a tri-layer window," IEEE Geoscience and Remote Sensing Letters, vol. 17, no. 10, pp. 1822­1826, 2019. 1, 6
[11] J. Han, S. Moradi, I. Faramarzi, H. Zhang, Q. Zhao, X. Zhang, and N. Li, "Infrared small target detection based on the weighted strengthened local contrast measure," IEEE Geoscience and Remote Sensing Letters, 2020. 1, 6

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

12

[12] L. Zhang, L. Peng, T. Zhang, S. Cao, and Z. Peng, "Infrared small target detection via non-convex rank approximation minimization joint l2, 1 norm," Remote Sensing, vol. 10, no. 11, p. 1821, 2018. 1, 6
[13] L. Zhang and Z. Peng, "Infrared small target detection based on partial sum of the tensor nuclear norm," Remote Sensing, vol. 11, no. 4, p. 382, 2019. 1, 6
[14] M. Liu, H.-y. Du, Y.-j. Zhao, L.-q. Dong, and M. Hui, "Image small target detection based on deep learning with snr controlled sample generation," in Current Trends in Computer Science and Mechanical Automation Vol. 1. De Gruyter Open Poland, 2018, pp. 211­220. 1, 2
[15] B. McIntosh, S. Venkataramanan, and A. Mahalanobis, "Infrared target detection in cluttered environments by maximization of a target to clutter ratio (tcr) metric using a convolutional neural network," IEEE Transactions on Aerospace and Electronic Systems, 2020. 1, 2
[16] S. Ren, K. He, R. Girshick, and J. Sun, "Faster r-cnn: Towards realtime object detection with region proposal networks," arXiv preprint arXiv:1506.01497, 2015. 1, 2
[17] J. Redmon and A. Farhadi, "Yolov3: An incremental improvement," arXiv preprint arXiv:1804.02767, 2018. 1, 2
[18] Y. Dai, Y. Wu, F. Zhou, and K. Barnard, "Asymmetric contextual modulation for infrared small target detection," in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2021, pp. 950­959. 1, 2, 3, 5, 6, 7, 11
[19] O. Ronneberger, P. Fischer, and T. Brox, "U-net: Convolutional networks for biomedical image segmentation," in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2015, pp. 234­241. 1, 3
[20] J. Zhang, Y. Jin, J. Xu, X. Xu, and Y. Zhang, "Mdu-net: Multi-scale densely connected u-net for biomedical image segmentation," arXiv preprint arXiv:1812.00352, 2018. 1
[21] J. Dolz, I. B. Ayed, and C. Desrosiers, "Dense multi-path u-net for ischemic stroke lesion segmentation in multiple image modalities," in International MICCAI Brainlesion Workshop. Springer, 2018, pp. 271­ 282. 1
[22] H. Huang, L. Lin, R. Tong, H. Hu, Q. Zhang, Y. Iwamoto, X. Han, Y.-W. Chen, and J. Wu, "Unet 3+: A full-scale connected unet for medical image segmentation," in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 1055­1059. 1
[23] Z. Zhou, M. M. R. Siddiquee, N. Tajbakhsh, and J. Liang, "Unet++: Redesigning skip connections to exploit multiscale features in image segmentation," IEEE Transactions on Medical Imaging, vol. 39, no. 6, pp. 1856­1867, 2019. 1
[24] M. M. Hadhoud and D. W. Thomas, "The two-dimensional adaptive lms (tdlms) algorithm," IEEE Transactions on Circuits and Systems, vol. 35, no. 5, pp. 485­494, 1988. 2
[25] S. Kim and J. Lee, "Scale invariant small target detection by optimizing signal-to-clutter ratio in heterogeneous background for infrared search and track," Pattern Recognition, vol. 45, no. 1, pp. 393­406, 2012. 2
[26] X. Wang, G. Lv, and L. Xu, "Infrared dim target detection based on visual attention," Infrared Physics & Technology, vol. 55, no. 6, pp. 513­521, 2012. 2
[27] H. Zhu, S. Liu, L. Deng, Y. Li, and F. Xiao, "Infrared small target detection via low-rank tensor completion with top-hat regularization," IEEE Transactions on Geoscience and Remote Sensing, vol. 58, no. 2, pp. 1004­1016, 2019. 2
[28] Y. Dai, Y. Wu, Y. Song, and J. Guo, "Non-negative infrared patch-image model: Robust target-background separation via partial sum minimization of singular values," Infrared Physics & Technology, vol. 81, pp. 182­194, 2017. 2
[29] Y. Dai, Y. Wu, F. Zhou, and K. Barnard, "Attentional local contrast networks for infrared small target detection," IEEE Transactions on Geoscience and Remote Sensing, 2021. 2, 5, 6, 7
[30] H. Wang, L. Zhou, and L. Wang, "Miss detection vs. false alarm: Adversarial learning for small object segmentation in infrared images," in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 8509­8518. 2, 5, 6, 7
[31] W. Zhang, M. Cong, and L. Wang, "Algorithms for optical weak small targets detection and tracking," in International Conference on Neural Networks and Signal Processing, 2003. Proceedings of the 2003, vol. 1. IEEE, 2003, pp. 643­647. 3
[32] K. Wu, E. Otoo, and A. Shoshani, "Optimizing connected component labeling algorithms," in Medical Imaging 2005: Image Processing, vol. 5747. International Society for Optics and Photonics, 2005, pp. 1965­ 1976. 5
[33] J. Shermeyer, T. Hossler, A. Van Etten, D. Hogan, R. Lewis, and D. Kim, "Rareplanes: Synthetic data takes flight," in Proceedings of

the IEEE/CVF Winter Conference on Applications of Computer Vision, 2021, pp. 207­217. 5 [34] K. He, X. Zhang, S. Ren, and J. Sun, "Deep residual learning for image recognition," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 770­778. 6 [35] J. Duchi, E. Hazan, and Y. Singer, "Adaptive subgradient methods for online learning and stochastic optimization." Journal of Machine Learning Research, vol. 12, no. 7, 2011. 6 [36] X. Glorot and Y. Bengio, "Understanding the difficulty of training deep feedforward neural networks," in Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, 2010, pp. 249­256. 6 [37] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., "Pytorch: An imperative style, high-performance deep learning library," in Advances in Neural Information Processing Systems (NeurIPS), 2019, pp. 8026­ 8037. 6

