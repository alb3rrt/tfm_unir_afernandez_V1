
# Preview, Attend and Review: Schema-Aware Curriculum Learning for Multi-Domain Dialog State Tracking

[arXiv](https://arxiv.org/abs/2106.0291), [PDF](https://arxiv.org/pdf/2106.0291.pdf)

## Authors

- Yinpei Dai
- Hangyu Li
- Yongbin Li
- Jian Sun
- Fei Huang
- Luo Si
- Xiaodan Zhu

## Abstract

Existing dialog state tracking (DST) models are trained with dialog data in a random order, neglecting rich structural information in a dataset. In this paper, we propose to use curriculum learning (CL) to better leverage both the curriculum structure and schema structure for task-oriented dialogs. Specifically, we propose a model-agnostic framework called Schema-aware Curriculum Learning for Dialog State Tracking (SaCLog), which consists of a preview module that pre-trains a DST model with schema information, a curriculum module that optimizes the model with CL, and a review module that augments mispredicted data to reinforce the CL training. We show that our proposed approach improves DST performance over both a transformer-based and RNN-based DST model (TripPy and TRADE) and achieves new state-of-the-art results on WOZ2.0 and MultiWOZ2.1.

## Comments

7 pages, 2 figures, accepted to ACL21

## Source Code

Official Code



Community Code



## Bibtex

```tex
@misc{dai2021preview,
      title={Preview, Attend and Review: Schema-Aware Curriculum Learning for Multi-Domain Dialog State Tracking}, 
      author={Yinpei Dai and Hangyu Li and Yongbin Li and Jian Sun and Fei Huang and Luo Si and Xiaodan Zhu},
      year={2021},
      eprint={2106.00291},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```

## Notes

Type your reading notes here...

