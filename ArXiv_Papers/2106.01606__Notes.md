
# Exploring Memorization in Adversarial Training

[arXiv](https://arxiv.org/abs/2106.01606), [PDF](https://arxiv.org/pdf/2106.01606.pdf)

## Authors

- Yinpeng Dong
- Ke Xu
- Xiao Yang
- Tianyu Pang
- Zhijie Deng
- Hang Su
- Jun Zhu

## Abstract

It is well known that deep learning models have a propensity for fitting the entire training set even with random labels, which requires memorization of every training sample. In this paper, we investigate the memorization effect in adversarial training (AT) for promoting a deeper understanding of capacity, convergence, generalization, and especially robust overfitting of adversarially trained classifiers. We first demonstrate that deep networks have sufficient capacity to memorize adversarial examples of training data with completely random labels, but not all AT algorithms can converge under the extreme circumstance. Our study of AT with random labels motivates further analyses on the convergence and generalization of AT. We find that some AT methods suffer from a gradient instability issue, and the recently suggested complexity measures cannot explain robust generalization by considering models trained on random labels. Furthermore, we identify a significant drawback of memorization in AT that it could result in robust overfitting. We then propose a new mitigation algorithm motivated by detailed memorization analyses. Extensive experiments on various datasets validate the effectiveness of the proposed method.

## Comments



## Source Code

Official Code



Community Code

- [https://paperswithcode.com/paper/exploring-memorization-in-adversarial](https://paperswithcode.com/paper/exploring-memorization-in-adversarial)

## Bibtex

```tex
@misc{dong2021exploring,
      title={Exploring Memorization in Adversarial Training}, 
      author={Yinpeng Dong and Ke Xu and Xiao Yang and Tianyu Pang and Zhijie Deng and Hang Su and Jun Zhu},
      year={2021},
      eprint={2106.01606},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```

## Notes

Type your reading notes here...

