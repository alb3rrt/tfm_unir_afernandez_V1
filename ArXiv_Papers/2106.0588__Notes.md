
# TransVOS: Video Object Segmentation with Transformers

[arXiv](https://arxiv.org/abs/2106.0588), [PDF](https://arxiv.org/pdf/2106.0588.pdf)

## Authors

- Jianbiao Mei
- Mengmeng Wang
- Yeneng Lin
- Yong Liu

## Abstract

Recently, Space-Time Memory Network (STM) based methods have achieved state-of-the-art performance in semi-supervised video object segmentation (VOS). A critical problem in this task is how to model the dependency both among different frames and inside every frame. However, most of these methods neglect the spatial relationships (inside each frame) and do not make full use of the temporal relationships (among different frames). In this paper, we propose a new transformer-based framework, termed TransVOS, introducing a vision transformer to fully exploit and model both the temporal and spatial relationships. Moreover, most STM-based approaches employ two disparate encoders to extract features of two significant inputs, i.e., reference sets (history frames with predicted masks) and query frame, respectively, increasing the models' parameters and complexity. To slim the popular two-encoder pipeline while keeping the effectiveness, we design a single two-path feature extractor to encode the above two inputs in a unified way. Extensive experiments demonstrate the superiority of our TransVOS over state-of-the-art methods on both DAVIS and YouTube-VOS datasets. Codes will be released when it is published.

## Comments

9 pages, 2 figures

## Source Code

Official Code



Community Code



## Bibtex

```tex
@misc{mei2021transvos,
      title={TransVOS: Video Object Segmentation with Transformers}, 
      author={Jianbiao Mei and Mengmeng Wang and Yeneng Lin and Yong Liu},
      year={2021},
      eprint={2106.00588},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```

## Notes

Type your reading notes here...

