Connections between the Nystro¨m and Sparse Variational Gaussian Processes

arXiv:2106.01121v1 [stat.ML] 2 Jun 2021

Connections and Equivalences between the Nystro¨m Method and Sparse Variational Gaussian Processes

Veit Wild Department of Statistics, University of Oxford, UK
Motonobu Kanagawa Data Science Department, EURECOM, France
Dino Sejdinovic Department of Statistics, University of Oxford, UK

veit.wild@keble.ox.ac.uk motonobu.kanagawa@eurecom.fr
dino.sejdinovic@stats.ox.ac.uk

Abstract
We investigate the connections between sparse approximation methods for making kernel methods and Gaussian processes (GPs) scalable to massive data, focusing on the Nystr¨om method and the Sparse Variational Gaussian Processes (SVGP). While sparse approximation methods for GPs and kernel methods share some algebraic similarities, the literature lacks a deep understanding of how and why they are related. This is a possible obstacle for the communications between the GP and kernel communities, making it difficult to transfer results from one side to the other. Our motivation is to remove this possible obstacle, by clarifying the connections between the sparse approximations for GPs and kernel methods. In this work, we study the two popular approaches, the Nystr¨om and SVGP approximations, in the context of a regression problem, and establish various connections and equivalences between them. In particular, we provide an RKHS interpretation of the SVGP approximation, and show that the Evidence Lower Bound of the SVGP contains the objective function of the Nystr¨om approximation, revealing the origin of the algebraic equivalence between the two approaches. We also study recently established convergence results for the SVGP and how they are related to the approximation quality of the Nystr¨om method. Keywords: Gaussian Processes, Kernel Methods, Sparse Approximation, Nystr¨om Method, Sparse Variational Gaussian Processes
1. Introduction
Gaussian processes (GPs) and kernel methods are the two principled learning approaches that make use of positive definite kernels, and have been studied extensively in statistics and machine learning. On one hand, GP-based approaches (Rasmussen and Williams, 2006) employ a kernel to induce the corresponding GP, in oder to define a prior distribution of the ground-truth latent function of interest. Given data, Bayes' rule is then applied to obtain the posterior distribution of the latent function. On the other hand, kernel methods (Scho¨lkopf and Smola, 2002) make use of a kernel to induce the corresponding Reproducing Kernel Hilbert Space (RKHS) as a "hypothesis space." Given data, empirical risk minimization is then performed in the RKHS to estimate the ground-truth function of interest. Although the GP and kernel approaches have different modeling philosophies, there are indeed deep connections and equivalences between them, which extend beyond a superficial
1

Wild, Kanagawa and Sejdinovic
similarity (Parzen, 1961; Kimeldorf and Wahba, 1970; Berlinet and Thomas-Agnan, 2004; Kanagawa et al., 2018).
The elegance of the GP and kernel approaches are that the infinite dimensional learning problems can be reduced to the corresponding finite dimensional problems. However, this comes with a cost: the computational complexity of either approach is usually cubic or at least quadratic with respect to the data size. This unfavorable scaling property has motivated the developments of several approximation methods to make the GP and kernel approaches scalable. Sparse approximation methods, which approximate the solution of interest using a set of input points smaller than training data, are among the most popular and successful approximation approaches. These approaches have been studied since the earliest developments of the GP and kernel approaches (e.g., Williams and Seeger, 2001; Csato´ and Opper, 2002; Smola and Scho¨lkopf, 2000; Seeger et al., 2003).
As the GP and kernel communities grow, sparse approximation methods for either approach tend to be developed independently to those for the other approach. For instance, consider the Sparse Variational Gaussian Process (SVGP) approach of Titsias (2009a,b), which is one of the most successful and widely used sparse approximation methods for GPs. The SVGP is derived in the framework of variational Bayesian inference, so that the sparse approximation is to be chosen to minimize the KL divergence to the exact GP posterior. As such, the developments in SVGP (e.g., Hensman et al., 2013, 2015a; Matthews et al., 2016; Burt et al., 2019; Rossi et al., 2021) have proceeded almost independently of the corresponding literature on sparse approximations for kernel methods. Similarly, the recent advances in using and understanding the Nystr¨om method (Williams and Seeger, 2001), which is one of the most popular sparse approximations in kernel methods, have been made independently to those of sparse GP approximations. The majority of these advances focus on an efficient approximation of the kernel matrix (e.g., Drineas and Mahoney, 2005; Belabbas and Wolfe, 2009; Gittens and Mahoney, 2016; Derezinski et al., 2020) or empirical risk minimization in the RKHS with a reduced basis (e.g, Bach, 2013; El Alaoui and Mahoney, 2015; Rudi et al., 2015, 2017; Meanti et al., 2020). This separation of two lines of research are arguably due to the difference in the notations and modeling philosophies of GPs and kernel methods. The separation makes it difficult to transfer useful and interesting results from one side to the other, and the communities might have missed an important advance that may be obtained otherwise. The motivation of the current work is to overcome this potential difficulty by bridging the two lines of research.
In this work, we investigate the connections between the sparse approximation methods for GPs and kernel methods. Specifically, we focus on the regression setting, and study the relationships between the SVGP and Nystr¨om approximations. We summarize below our contributions and main findings:
· In Section 3, we establish an equivalence between the SVGP posterior mean function and kernel ridge regression (KRR) using the Nystr¨om method. To understand this equivalence, we analyze the Evidence Lower Bound (ELBO) that is optimized by the SVGP approximation. We show that the ELBO contains the objective function that the Nystr¨om KRR essentially minimizes. In this sense, the equivalence is not a "coincidence."
2

Connections between the Nystro¨m and Sparse Variational Gaussian Processes
· We make an RKHS interpretation for the ELBO of the SVGP approximation. We reformulate the ELBO in terms of orthogonal projections onto the subspace spanned by the inducing inputs (or landmark points). This formulation enables one to understand the ELBO geometrically. Specifically, it shows that the SVGP posterior covariance function is given via the solution to a certain optimization problem in the RKHS. Moreover, it enables a geometric understanding of the ELBO as an objective function for choosing inducing inputs. These results may be useful for inspiring more advanced SVGP approaches, in a similar manner as Shi et al. (2020) who used a certain geometric argument for justifying their approach.
· We present a worst case error interpretation of the SVGP posterior variance function. We show that it is the sum of two worst case errors in RKHSs: one is that of kernelbased interpolation at inducing inputs, and the other is that of the Nystr¨om KRR based on noisy observations. This interpretation may be useful for understanding the impacts of the choice of a kernel and inducing points on the SVGP uncertainty estimates, as the RKHS formulation enables a discussion of the "capacity" of the model. Moreover, these RKHS interpretations enable kernel researchers to understand the SVGP approximation in their own terminology and provide a tool for uncertainty quantification.
· In Section 4, we discuss convergence results for the SVGP recently established by Burt et al. (2019, 2020), and investigate how they are related to the approximation quality of the Nystr¨om KRR. To this end, we first show that the "data fit term" in the marginal likelihood of Gaussian process regression (GPR) is essentially identical to the objective function value of the corresponding KRR estimator. This result enables us to rewrite relevant terms in the KL divergence for the SVGP approximation as the "excess risk" of the Nystr¨om KRR over the exact KRR estimator. Since the theoretical arguments of Burt et al. (2020) are essentially based on bounding this "excess risk," many of their results can be directly translated to the corresponding results on the Nystr¨om KRR. Moreover, the existence of the "excess risk" of the Nystr¨om KRR suggests that a more refined analysis may be done for the KL divergence for the SVGP approximation by employing sharper theoretical results on the Nystr¨om KRR (e.g., Bach, 2013; El Alaoui and Mahoney, 2015; Rudi et al., 2015).
· We also establish a novel approximation error bound for the Nystr¨om KRR in terms of its RKHS distance to the exact KRR solution. This bound may be interesting in its own right, since it shares a certain structural similarity to a fundamental bound in Burt et al. (2020) on the KL divergence for the SVGP approximation. Moreover, because of the equivalence between the SVGP posterior mean function and the Nystr¨om KRR estimator, and that between the exact GP posterior mean function and the exact KRR estimator, the novel bound also holds as a bound on the RKHS distance between the SVGP and exact posterior mean functions. This result is useful in that it leads to an approximation bound for the derivatives of the SVGP posterior mean function, when the kernel is continuously differentiable. This demonstrates the usefulness of the RKHS interpretations.
3

Wild, Kanagawa and Sejdinovic
· Finally, we study the lower bound of the averaged KL divergence for the SVGP approximation, where the average is taken with respect to the distribution of training outputs under the prior model (Burt et al., 2020, Lemma 4). By the identity of the "data fit terms" and the "excess risk" of the Nystr¨om KRR, this lower bound can be directly transformed to a lower bound for the excess risk of the Nystr¨om KRR. This new lower bound is given in terms of the ratio of the "complexities" of the exact and approximation kernel models, and provides a novel theoretical understanding of the Nystr¨om KRR. This is another manifestation of benefits of studying the connections between the GP and kernel approaches.
This paper is organized as follows. Section 2 provides relevant background on Gaussian processes and kernel methods. Section 3 investigates the connections and equivalences between the SVGP and the Nystr¨om approximations. Section 4 studies the approximation properties of the SVGP and Nystr¨om approximations and their connections. Section 5 concludes.
1.1 Notation
We use the following notation in this paper. Let N be the set of natural numbers, R be the real line, and Rd for d  N be the d-dimensional Euclidean space. For any v  Rd, v denotes the Euclidean norm.
Let X be a nonempty set. For a function f : X  R and X := (x1, . . . , xn)  X n with n  N, denote by fX the n-vector consisting of function values evaluated at points in X: fX := (f (x1), . . . , f (xn))  Rn. Similarly, for a function with two arguments k : X × X  R, and X := (x1, ..., xn)  X n and Z := (z1, ..., zm)  X m with n, m  N, define kXZ  Rn×m by (kXZ )i,j = k(xi, zj) for i = 1, . . . , n, j = 1, . . . , m. For X := (x1, ..., xn)  X n, let kX (x) := (k(x1, x), . . . , k(xn, x))  Rn for any x  X and denote by kX (·) the vector-valued function x  X  kX (x)  Rn.
For a symmetric matrix , denote by   0 and  0 that  is positive definite and positive semi-definite, respectively. For µ  Rn and   Rn×n with  0, denote by N (µ, ) the Gaussian distribution on Rn with mean vector µ and covariance matrix . Let N (· | µ, ) be its probability density function. For a matrix A  Rn×n, tr(A) and det(A) denote its trace and determinant, respectively.
Let Y be a measurable space, Y  Y be a random variable, and P be a probability measure on Y. We write Y  P to mean that Y follows P. For a measurable function g : Y  R, denote by g(y)dP(y) its integral with respect to P and by E[g(Y )] the expectation of g(Y ). When P has a density function p : Y  R with respect to a reference measure  on Y (e.g., the Lebesgue measure when Y = Rn), the integral is denoted by
g(y)p(y)d(y).
2. Background
This section briefly reviews reproducing kernel Hilbert spaces (RKHS) and Gaussian processes (GP). In particular, we focus on the respective approaches to regression, namely kernel ridge regression (KRR) and Gaussian process regression (GPR).
4

Connections between the Nystro¨m and Sparse Variational Gaussian Processes

We first describe the regression problem. Let X be a non-empty set. Suppose we are
given n  Æ paired observations

(x1, y1), . . . , (xn, yn)  X × Ê.

We assume that there exists a function f0 : X  R such that

yi = f0(xi) + i, i = 1, . . . , n.

(1)

where 1, . . . , n  R are independent, zero-mean, noise variables. f0 is called regression function. The task of regression is to estimate (or learn) f0 from the training data (xi, yi)ni=1. We will often write X := (x1, . . . , xn)  X n and y := (y1, . . . , yn)  Rn.

2.1 Kernel Ridge Regression (KRR)
2.1.1 Kernels and RKHSs
We review here basics of kernels and RKHSs. For details, we refer to Scho¨lkopf and Smola (2002); Hofmann et al. (2008); Steinwart and Christmann (2008).
Let X be an arbitrary non-empty set. A symmetric function k : X × X  R is called a positive definite kernel, if for every n  N and every X = (x1, ..., xn)  X n, the induced kernel matrix kXX = (k(xi, xj))ni,j=1  Rn×n is positive semi-definite. We may simply call such k kernel. By the Moore-Aronszajn theorem (Aronszajn, 1950), for any such kernel k there exists a uniquely associated Hilbert space H, ·, · H of real-valued functions f : X  R called reproducing kernel Hilbert space (RKHS) such that

1. k(·, x)  H for every x  X and

2. f (x) = f, k(·, x) H for every f  H and x  X ,

where k(·, x) denotes the function of the first argument with x being fixed: x  X 

k(x, x). The kernel k is called reproducing kernel of H.

Examples of kernels on X  Rd include the following. For  > 0 the Gaussian kernel

or

square-exponential

kernel

is

defined

as

k (x, x)

:=

exp(-

x-x 2

2
)

for

x, x



X.

For

constants  > 0 and h > 0 the Mat´ern kernel is defined for x, x  X as k,h(x, x) :=

1 2-1 ()

(

2

x-x h

)K( 2

x-x h

),

where



is

the

gamma

function

and

K

is

the

modified

Bessel function of the second kind of order .

The RKHS Hk may be "explicitly" constructed from k as follows (Hofmann et al., 2008,

Section 2.2.1). Define a function space

For f =

n
H0 := f = ik(·, xi) | n  N, 1, ..., n  R, x1, ..., xn  X .
i=1

n i=1

ik(·,

xi

)



H0

and

g

=

m i=1

ik(·, yi)



H0,

define

an

inner

product

nm

f, g H0 :=

ij k(xi, yj).

i=1 j=1

5

Wild, Kanagawa and Sejdinovic

Then we have

Hk = H0,

i.e., the RKHS Hk is the closure of H0 with respect to the norm induced by the inner product ·, · H0 .

2.1.2 Regression Approach
Kernel ridge regression (KRR) is an approach to regression using a kernel k and its RKHS Hk. The KRR estimator f^ of the regression function f0 in (1) is defined as the solution of the following regularized empirical risk minimization (ERM) problem

f^ =

argmin
f Hk

1 n

n
(yi
i=1

- f (xi))2

+ ||f ||2Hk ,

(2)

where  > 0 is a regularization constant. To gain an intuitive understanding for the necessity
of the regularization, suppose that the function space Hk is potentially very large and therefore the unregularised ERM tends to interpolate the data points. The regularization term in (2) imposes a certain degree of smoothness onto the solution f^ and hence prevents
it from adapting too strongly to the training data (x1, y1), . . . , (xn, yn). Let X := (x1, ..., xn)  X n and y := (y1, ..., yn)  Rn. By the representer theo-
rem (Scho¨lkopf et al., 2001), the solution f^ is given as a linear combination of k(·, x1), . . . , k(·, xn). Hence, the optimization problem (2) reduces to that of the coefficients of the linear combination. As a result, the estimator is given by

n

f^ = ik(·, xi),

(3)

i=1

where  := (1, ..., n)  Rn is given by

 = (kXX + nIn)-1y,

where In  Rn×n is the identity matrix. The prediction of KRR at any x  X is compactly

written as

f^(x) = kX (x) = kX (x)(kXX + nIn)-1y,

(4)

where kX (x) = (k(x1, x), . . . , k(xn, x))  Rn. The elegance of KRR is that we arrive at a simple, closed form expression for an infinite-
dimensional optimisation problem. As we shall see soon, the same expression will arise in the context of non-parametric Bayesian learning.

2.2 Gaussian Process Regression (GPR)
2.2.1 Gaussian Processes
Gaussian processes (GPs) are one of the main workhorses of Bayesian nonparametric statistics and machine learning, as they can be used to place a prior distribution over functions. See Rasmussen and Williams (2006) for more details.

6

Connections between the Nystro¨m and Sparse Variational Gaussian Processes

Let X be a non-empty set, m : X  R a function and k : X × X  R a positive definite kernel. A random function F : X  R is called Gaussian process (GP) with mean function m and covariance kernel k, if for all n  N and all X = (x1, ..., xn)  X n, the random vector FX := (F (x1), . . . , F (xn))  Rn satisfies
FX  N (mX , kXX ),
i.e., FX follows the Gaussian distribution with mean vector mX = (m(x1), . . . , m(xn))  Rn and covariance matrix kXX = (k(xi, xj))ni,j=1  Rn×n. In this case, we write
F  GP (m, k).
By definition, we have
m(x) = E [F (x)] , x  X , k(x, x) = E (F (x) - m(x))(F (x) - m(x)) , x, x  X
For any function m : X  R and kernel k : X × X  R, there exists1 a GP whose mean function is m and covariance function is k. Therefore, by choosing m and k, one can implicitly define the corresponding GP, F  GP (m, k). This is how a GP is used to define a prior distribution in Bayesian nonparametrics.

2.2.2 Regression Approach
Gaussian process regression (GPR) is a Bayesian nonparametric approach to the regression problem. In GPR, the regression function f0 in Eq. (1) is the quantity of interest and modeled as a random function F . The prior distribution is given by a GP

F  GP (m, k).

(5)

where the modeler choses the mean function m and covariance function k to encode his/her prior knowledge/assumption about the regression function f0.
The likelihood model of F for the observations y = (y1, . . . , yn) is given by

yi = F (xi) + i, i = 1, . . . , n,

(6)

where2 i  N (0, 2) is an independent Gaussian noise with variance 2 > 0.
By Bayes' rule, the posterior distribution of F given y, under the prior (5), is given by again a GP3

F | y  GP (m¯ , k¯),

where k¯ : X × X  R and m¯ : X  R are defined as

m¯ (x) := m(x) + kX (x)(kXX + 2In)-1(y - mX ),

(7)

k¯(x, x) := k(x, x) - kX (x)(kXX + 2In)-1kX (x),

(8)

1. This is a consequence of the Kolmogorov consistency theorem (see e.g. Tao, 2011, Chapter 2.4). 2. In GPR, the noise assumption can be weaker, e.g., 1, . . . , n can be dependent and/or their variances
can be different. In this paper, we consider this simplest noise model to investigate connections to the KRR. 3. See e.g. Rasmussen and Williams (2006) for derivation.

7

Wild, Kanagawa and Sejdinovic

where kX (x) = (k(x1, x), . . . , k(xn, x))  Rn. We call GP (m¯ , k¯) the posterior GP, m¯ the posterior mean function and k¯ the posterior covariance function. We use the following notation for the probability measure of the posterior GP:

PF |y := GP (m¯ , k¯).

(9)

The posterior mean function m¯ serves as an estimator of the regression function f0. On the other hand, the posterior covariance function k¯ provides a way of uncertainty quantification: k¯(x, x) is the posterior variance of F (x) given y. Its square root k(x, x) is the posterior standard deviation and may be used to construct a Bayesian credible interval for f0(x).

2.3 Connections between KRR and GPR
There is a well-known equivalence between the KRR estimator and the posterior mean function of GPR (Kimeldorf and Wahba, 1970; Kanagawa et al., 2018). This is summarized in the following theorem.

Theorem 1 Let k be a kernel, and suppose that data (xi, yi)ni=1  X × R are given.
· Let m¯ be the posterior mean function (7) of GPR with the prior being the zero-mean Gaussian process F  GP (0, k) (i.e., m(x) = 0, x  X , in (5)).
· Let f^ be the KRR estimator (4) performed on the RKHS Hk. Then if 2 = n, we have f^ = m¯ .

This result provides a Bayesian interpretation for the KRR estimator, and a leastsquares interpretation for the GPR posterior mean function. In particular, the condition 2 = n shows that specifying the noise variance 2 in GPR is equivalent to specifying a regularization constant  in the KRR. For GPR, this equivalence implies that assuming observation noises works as regularization or smoothing. For KRR, it implies that the regularization constant may be learned from the data via interpreting it as a (scaled) noise variance. The equivalence between KRR and GPR enables an alternative interpretation of either approach and opens up a possibility of devising novel learning algorithms.

RKHS Interpretation of Posterior Variances. There is also an RKHS interpretation of the posterior variance k¯(x, x) of GPR (Kanagawa et al., 2018, Section 3.4). To describe this, define w : X  Rn by

w(x) := (kXX + 2In)-1kX (x), x  X ,

Then, provided 2 = n, the KRR estimator in (4) can be written as

n

f^(x) = wi(x)yi = yw(x), x  X .

(10)

i=1

which is linear in the training outputs y = (y1, . . . , yn). Define also an augmented kernel

k(x, x) = k(x, x) + 2½{x = x}, x, x  X ,

8

Connections between the Nystro¨m and Sparse Variational Gaussian Processes

where ½{x = x} = 1 if x = x and ½{x = x} = 0 otherwise. Intuitively, its RKHS Hk
may be understood as an "corrupted" version of Hk, in the sense that each function in Hk is the sum of a function from Hk and an independent noise function; see Kanagawa et al. (2018, Section 3.4) for a discussion.
The following theorem relates that the posterior variance k¯(x, x) of GPR to a worst case
error of predictions by KRR in Hk .

Theorem 2 (Kanagawa et al., 2018, Proposition 3.8) Let k¯ be the posterior covariance function (8) of GPR based on a prior F  GP (0, k) and observation noise variance 2  0 (Suppose the kernel matrix kXX is invertible if 2 = 0). If 2 = n and x = xi for all
i = 1, ..., n, then

n

k¯(x, x) + 2 =

sup

g(x) - wi(x)g(xi) .

gHk : g Hk 1

i=1

In the right hand side,

n i=1

wi (x)g(xi )

can

be

interpreted

as

the

prediction

of

g(x)

by

KRR trained with data (xi, yi)ni=1 where yi = g(xi) (see Eq. (10)). Thus, the right hand

side is the worst case error of KRR predictions at input x, for functions g from the unit ball

in Hk. This result suggests that uncertainty quantification can be also done in the RKHS framework.

In this paper, we investigate whether these parallels between KRR and GPR extend to

their sparse approximations, which have been developed (largely independently within the

two research communities) to deal with the unfavourable computational properties of KRR

and GPR. The following sections are devoted to this question.

3. Sparse Approximations
The elegance of KRR and GPR is that closed form expressions are respectively available for the solution of optimization or Bayesian inference in potentially infinite dimensional function spaces. Unfortunately, this comes with high computational costs, since both methods involve the inversion of the regularized kernel matrix, which leads to the computational complexity of O(n3), where n is the size of training data (xi, yi)ni=1. Both kernel and GP communities have been developing a variety of approximation methods for making their respective approaches scalable. One of the most successful approaches is sparse approximation, which approximates the solution of interest using a smaller set of input points z1, . . . , zm  X , where m  N may be much smaller than the original data size n.
We investigate here connections between the sparse approximation methods for KRR and GPR. Specifically, we focus on the connections between the Nystro¨m approximation and the Sparse Variational Gaussian Process (SVGP) approximation, which are respectively popular sparse approximation methods for the kernel and GP-based approaches. Sections 3.1 and 3.2 review the Nystr¨om and the SVGP methods, respectively. Section 3.3 describes an equivalence between the SVGP posterior mean function and the Nystr¨om KRR, and investigates the original of the equivalence by studying the ELBO for the SVGP. Section 3.4 summarizes an equivalence between the Nystr¨om and the Deterministic Training Conditional (DTC) approximation, a classic sparse GP approximation approach. Section 3.5 provides a geometric interpretation of the SVGP posterior covariance function, based

9

Wild, Kanagawa and Sejdinovic

on which Section 3.6 investigates further the ELBO. Section 3.7 provides an RKHS interpretation of the SVGP posterior variance function as consisting of worst case errors of kernel-based interpolation and the Nystr¨om KRR.

3.1 Nystro¨m Approximation

The Nystr¨om method was first proposed by Williams and Seeger (2001) for scaling up

kernel-based learning algorithms. It has been successfully used in a variety of applications

including manifold leaning (Talwalkar et al., 2008, 2013), computer vision (Fowlkes et al.,

2004; Belabbas and Wolfe, 2009), and approximate sampling (Affandi et al., 2013), to name

a few. Recent studies make use of the Nystr¨om method to enable KRR to handle millions

to billions of data points (e.g. Rudi et al., 2017; Meanti et al., 2020).

We describe here the use of the Nystr¨om approximation in KRR. In particular, we con-

sider a popular version classically known as the subset of regressors (Wahba, 1990, Chapter

7), which has been widely used both in practice and theory (e.g., Smola and Scho¨lkopf,

2000; Rudi et al., 2015, 2017; Meanti et al., 2020). As before, let (xi, yi)ni=1  X × R be training data, and let X := (x1, ..., xn)  X n and y := (y1, ..., yn)  Rn.
For m  N, let z1, . . . , zm  X be a set of input points based on which we approximate
the KRR solution. These points z1, . . . , zm are usually a subset of training input points x1, . . . , xn in the kernel literature, but we allow for z1, . . . , zm to be generic points in X for a later comparison with the GP counterpart. Write Z = (z1, . . . , zm)  X m. Suppose that the kernel matrix kZZ = (k(zi, zj ))mi,j=1  Rm×m is invertible.
Let M  Hk be the finite dimensional subspace spanned by k(·, z1), . . . , k(·, zm):





m



M : = span(k(·, z1), . . . , k(·, zm)) :=

jk(·, zj ) | 1, ..., m  R .

(11)

j=1



We replace the hypothesis space Hk in the KRR objective function (2) by this subspace M , and define its solution f¯ as the Nystr¨om approximation of the KRR solution f^:

f¯ :=

arg min
f M

1 n

n i=1

yi - f (xi) 2 +  f

2 Hk

,

(12)

In other words, we approximately solve the minimization problem of KRR by searching for

the solution of the form

m
f = ik(·, zi) = kZ (·)

i=1

for some coefficients  := (1, . . . , m)  Rm, where kZ (·) := (k(·, z1), . . . , k(·, zm)). Inserting this expression in (12), the optimization problem now becomes

min
Rm

1 n

y - kXZ 

2 + kZZ ,

where kXZ  Rn×m with (kXZ )i,j = k(xi, zj ) and kZZ  Rm×m with (kZZ )i,j = k(zi, zj ). Taking the first order derivative with respect to  leads to the condition

-

2 n

kZ

X

y

+

2 n

kZ X

kX

Z



+

2kZZ 

=

0,

10

Connections between the Nystro¨m and Sparse Variational Gaussian Processes

which is satisfied for

 = kZX kXZ + nkZZ -1kZX y.

This leads to the following expression of the Nystr¨om approximation:

f¯(x) = kZ (x)(nkZZ + kZX kXZ )-1kZX y.

(13)

This approximation can be computed with the complexity of O(nm2 +m3) instead of O(n3), since the inversion of a n × n matrix is replaced by that of a m × m matrix. This grants significant computational gains, if m is much smaller than n and hence allows KRR to be applied to large data sets. Of course, how to choose m and the input points z1, . . . , zm depends not only on the computational budget but also on how accurately f¯ approximates the KRR solution f^. We discuss this issue in Section 4.

3.1.1 Characterization with an Approximate Kernel

We study here another characterization of the Nystr¨om approximation based on a certain approximate kernel. This characterization provides a natural connection of the Nystr¨om method to one of sparse approximation methods for GPR, as we will see later.
For any f  Hk, denote by PM f  M the orthogonal projection of f onto the subspace M:
PM f := argmin f - g Hk ,
gM

which is the best approximation of f by an element in M . The projection is given as

Pm(f ) =

m j=1

j k(·, zj ),

where

the



=

(1, . .

.

,

m)



Rm

is

the

solution

of

m

min
Rm

f - j k(·, zj ) Hk .
j=1

Given that the kernel matrix kZZ is invertible, the solution can be shown to be  = kZ-Z1 fZ with fZ = f (z1), ..., f (zm)   Rm. Thus the projection of f is given by

PM f = kZ (·)kZ-Z1 fZ .

(14)

We can use the orthogonal projection PM to define an approximate kernel. Note that by definition,
k(x, x) = k(·, x), k(·, x) Hk , x, x  X .
We then define a new kernel q : X × X  R as the inner product between the projections of k(·, x) and k(·, x) onto the subspace M :

q(x, x) := PM k(·, x) , PM k(·, x) Hk

= kZ (x)kZ-Z1 kZ (·), kZ (x)kZ-Z1 kZ (·) Hk

= kZ (x)kZ-Z1 kZ (x), x, x  X .

(15)

Since q is a positive definite kernel, it induces its own RKHS Hq. As the following lemma shows, this RKHS Hq is nothing but the subspace M , with the inner product of Hq being identical to that of the original RKHS Hk. The proof can be found in Appendix A.

11

Wild, Kanagawa and Sejdinovic

Lemma 3 Let Z = (z1, . . . , zm)  X m be such that the kernel matrix kZZ is invertible. Then we have M = Hq as a set, and
f, g Hq = f, g Hk , f, g  M = Hq.
In particular, Lemma 3 implies that

f Hq = f Hk , f  Hq = M
By using this identity and Hq = M in the Nystr¨om KRR objective function (12), we immediately have the following characterization of the Nystr¨om approximation in (13).

Theorem 4 Let X := (x1, ..., xn)  X n and y := (y1, ..., yn)  Rn be given. Let Z = (z1, . . . , zm)  X m be such that the kernel matrix kZZ is invertible, and f¯ be the Nystro¨m approximation f¯ of KRR in (13). Then we have

f¯ = arg min 1 n

f Hq

n
i=1

yi - f (xi) 2 +  f

2 Hq

where q is the approximate kernel defined in (15).

Theorem 4 shows that the Nystr¨om approximation (13) is the solution of the KRR with the approximate kernel q in (15). Note that, by Lemma 3, any f  Hq can be written as
f (x) = f, q(·, x) Hq = f, q(·, x) Hk = f, kZ (·)kZ-Z1 kZ (x) Hk = fZkZ-Z1 kZ (x).
The last expression coincides with the expression (14) of the orthogonal projection onto M , and also with the kernel-based interporator4 obtained from noise-free observations (zj, f (zj))mj=1. Therefore, each function Hq is the best approximation of functions in Hk that pass (zj, f (zj))mj=1. In this sense, Hq consists of functions that approximate the functions in Hk on the landmark points z1, . . . , zm. Hence, Theorem 4 shows that the Nystr¨om approximation is the solution of the KRR where the hypothesis space Hq consists of such approximate functions.

3.2 Sparse Approximations for GPR
We review here the Sparse Variational Gaussian Process (SVGP) approach by Titsias (2009a) based on a measure-theoretic formulation suggested by Matthews et al. (2016). There have been many works on sparse approximations for scaling up GP-based methods. In a nutshell, there are two common approaches: either the generative model is approximated and inference is performed exactly (Seeger et al., 2003; Snelson and Ghahramani, 2006, 2007) or the generative model is left unaltered and inference is done approximately
4. This corresponds to the KRR estimator (4) with  := 0, X := Z and y := fZ. Setting  = 0 leads to the minimum-norm interpolation in the RKHS; see e.g. Kanagawa et al. (2018, Section 3.2) and references therein.

12

Connections between the Nystro¨m and Sparse Variational Gaussian Processes

(Csato´ and Opper, 2002; Titsias, 2009a). In this work, we mainly focus on the SVGP approximation by Titsias (2009a), which is the latter approach, but we also discuss the Deterministic Training Conditional (DTC) approximation (Seeger et al., 2003), one of the former approaches, as this will provide us a more insight about the SVGP approximation. We refer to Bauer et al. (2016) for a systematic comparison of the two approaches.
Since we focus on the basic framework of Titsias (2009a) and its comparison to the kernel counterpart in a regression setting, we do not discuss sparse variational GP approaches to the classification problem (Hensman et al., 2015a) and other (more recent) developments (e.g., Hensman et al., 2015b, 2018; Dutordoir et al., 2020; Adam et al., 2020; Shi et al., 2020; Rossi et al., 2021; Tran et al., 2021). See e.g., Leibfried et al. (2020) for an overview over variational GP approaches.
We first recall the setting of GPR using a measure-theoretic notation. As before, let (xi, yi)ni=1  X × R be training data and let X = (x1, . . . , xn)  X n and y = (y1, . . . , yn)  Rn. For simplicity, we assume the zero prior mean function, m(x) = 0. We denote by P the probability measure of a Gaussian process F  GP (0, k). For any finite set of points D := (d1, . . . , d)  X  with   N, let PD be the corresponding distribution of FD := (F (d1), . . . , F (d)) on R, which is PD = N (0, kDD) by definition.

3.2.1 Variational Family
We first introduce a variational family of probability measures of functions on X , from which we search for a computationally tractable approximation of the GP posterior PF |y = GP (m¯ , k¯) in (9). Let m  N be fixed, and  be a set of variational parameters defined by

 := { := (Z, µ, ) | Z := (z1, . . . , zm)  X m, kZZ is invertible, µ  Rm,   Rm0×m}
where Rm0×m stands for symmetric and positive definite matrices in Rm×m. The points Z = (z1, . . . , zm) are the so-called inducing inputs, based on which we approximate the posterior GP. On the other hand, µ and  are parameters for the distribution of function values at z1, . . . , zm.
We then define a variational family

Q := {Q |   }

as a set of Gaussian processes parametrized by the tuple  = (Z, µ, ) defined as follows:

Q := GP (m , k ),

(16)

m (x) := kZ (x)kZ-Z1 µ,

(17)

k (x, x) := k(x, x) - kZ (x)kZ-Z1 kZ (x)

+ kZ (x)kZ-Z1 kZ-Z1 kZ (x).

(18)

Each variational distribution (16) is defined so as to have the following properties, where F   GP (m , k ) denotes the corresponding GP sample function:

13

Wild, Kanagawa and Sejdinovic

1. The function values5 FZ := (F  (z1), . . . , F  (zm))  Rm at the inducing inputs z1, . . . , zm follow the Gaussian distribution with mean vector µ  Rm and covariance matrix   Rm0×m, i.e., FZ  N (µ, ). We denote by QZ by the distribution of FZ i.e., QZ = N (µ, ).
2. The conditional distribution of the process F  given (zi, F (zi))mi=1 is identical to the conditional distribution of F  GP (m, k) given (zi, F (zi))mi=1:

F  | (zi, F  (zi))mi=1 =d F | (zi, F (zi))mi=1.

(19)

In fact, starting from the expressions (16) (17) (18), one can check that F   GP (m , k ) satisfies the above two requirements as follows:

1. mZ = µ and kZ Z = , and thus FZ  N (µ, ); 2. By using (7) and (8) with m := m, k := k, X := Z, Y := FZ and 2 := 0,6 the
conditional distribution of F  given (zi, F  (zi))mi=1 is given by the Gaussian process F  | (zi, F  (zi))mi=1  GP (m~  , k~ ),
with mean function m~  : X  R and covariance function k~ : X × X  R given by

m~  (x) := m (x) + kZ (x)(kZ Z )-1(FZ - mZ )

= kZ (x)kZ-Z1 FZ ,

(20)

k~ (x, x) := k (x, x) - kZ (x)(kZ Z )-1kZ (x)

= k(x, x) - kZ (x)kZ-Z1 kZ (x)

(21)

Since the expressions (20) and (21) are respectively the mean function and covariance

function of GP (m~ , k~),

the we

conditional distribution of F given (zi have the distributional identity (19).

,

F

(zi))mi=1

i.e.,

F

|

(zi, F (zi))mi=1



3.2.2 Evidence Lower Bound and Optimal Vartional Parameters

The aim of variational inference is to obtain a distribution Q from the variational family

Q that best approximates the posterior measure PF |y in terms of the Kullback-Leibler

(KL) divergence, without explicitly computing the posterior. That is, we want to compute

   such that

  arg min KL(Q PF |y).

(22)



where KL(Q PF |y) is the KL divergence between Q and PF |y defined by

KL(Q||PF |y) :=

log

dQ dPF |y

(f

)

dQ(f ).

5. Note that FZ is usually called inducing variables and is denoted with symbol u in the literature. 6. The case 2 = 0 is well defined as long as the kernel matrix kZ is invertible. In this case, the regression
problem becomes that of interpolation, i.e., function approximation from noise-free observations. See
e.g. Kanagawa et al. (2018) and references therein.

14

Connections between the Nystro¨m and Sparse Variational Gaussian Processes

with

dQ dPF |y

being

the construction

the Radon-Nikodym of Q (Matthews et

derivative of Q with respect al., 2016, Section 3.3).

to

PF

|y,

which

exists

by

Matthews et al. (2016, Eq. 15) show that this KL divergence can be written as

KL Q PF |y = log p(y) - L(),

(23)

where p(y) is the marginal likelihood, or the Evidence, of observing y = (y1, . . . , yn) under the prior F  GP (m, k) and the likelihood model yi  N (F (xi), 2), while L() is the Evidence Lower Bound (ELBO) defined as

L() := -KL QZ PZ + EF Q log p(y|FX ) ,

(24)

where

· KL QZ PZ is the standard KL divergence between QZ = N (µ, ), which is the marginal distribution of FZ  Rm of the parameterized Gaussian process F   Q = GP (m , k ) in (16), and PZ = N (0, kZZ ), which is the marginal distribution of FZ  Rm of the prior Gaussian process F  P = GP (0, k):

KL QZ

PZ = log
Rm

dQZ dPZ

(fZ

)

dQZ (fZ )

=

1 2

tr(kZ-Z1 ) + µkZ-Z1 µ - m + log

detkZZ det

,

(25)

where the last identity is the well-known expression of the KL divergence between multivariate Gaussian densities (see, e.g., Appendix A.5 of Rasmussen and Williams 2006).

· EF Q [log p(y|FX )] is the marginal log likelihood of observing y = (y1, . . . , yn) under the likelihood model yi = F (xi) + i with independent i  N (0, 2) and the parametrized process F   Q:

 EF Q [log p(y|FX )] = -n log( 22) - EF Q

n

(yi - F  (xi))2 22

(26)

i=1

where p(y|FX ) := N (y; FX , 2In) is the Gausian density of the likelihood function (6).

Since the marginal likelihood p(y) under the original GP prior does not depend on the
variational parameters , the minimization of the KL divergence (23) is equivalent to the maximization of the ELBO L() in (24). Titsias (2009a, Eq.(10)) show that,7 for fixed inducing points Z, the optimal parameters µ and  that maximize the ELBO are given
analytically as

µ := kZZ (2kZZ + kZX kXZ )-1kZX y  := kZZ (kZZ + -2kZX kXZ )-1kZZ

(27) (28)

7. See Appendix A of Titsias (2009b) or Hensman et al. (2013, Section 2) for the derivation

15

Wild, Kanagawa and Sejdinovic

and the resulting ELBO, denoted by L, is

L

=

-

1 2

log

det(qXX

+

2In)

-

1 2

y(qXX

+

2 I )-1 y

-

n 2

log

2

-

1 22

tr(kXX

-

qXX ),

(29)

where q is the approximate kernel in (15). Inserting these expressions in the definition of
the variational distribution (16), the optimal variational approximation (for fixed inducing points Z) is given by GP (m, k) with:

m(x) := kZ (x)(2kZZ + kZX kXZ )-1kZX y

(30)

k(x, x) := k(x, x) - kZ (x)kZ-Z1 kZ (x)

+ kZ (x)(kZZ + -2kZX kXZ )-1kZ (x)

(31)

The computational complexity of obtaining the mean function m and the covariance function k is O(nm2 + m3), which can be much smaller than the complexity O(n3) of
the exact posterior as long as the number of inducing points m is much smaller than the
training data size n. The ELBO (29) with optimal µ and  is a key quantity, as it can be used i) as a
criterion for optimizing the inducing inputs z1, . . . , zm and ii) for theoretically analyzing the quality of variational approximation.

3.3 Equivalence between the Nystro¨m and SVGP Approximations
We now focus on the relations between the Nystr¨om method for KRR and the variational approximation for GPR. Like Theorem 1, our result below summarizes the equivalence between the predictors in the two approaches. It directly follows from the corresponding expressions (13) and (30).
Theorem 5 Let k be a kernel, and suppose that data (xi, yi)ni=1  X × R are given. Let Z = (z1, . . . , zm)  X m be fixed inducing inputs such that the kernel matrix kZZ = (k(zi, zj ))mi,j=1  Rm×m is invertible.
· Let m be the mean function (30) of the variational posterior for GPR with the prior being the zero-mean Gaussian process F  GP (0, k).
· Let f¯ be the Nystro¨m approximation (13) of KRR performed on the RKHS Hk.
Then if 2 = n, we have m = f¯.
Theorem 5 shows that the two approximate regressors are identical, although their derivations are (seemingly) quite different. The condition n = 2 is the same as that required for the equivalence between KRR and GPR in Theorem 1.
The question now is why there is this equivalence in the two ways of approximation. To investigation this, we first inspect closely the ELBO (24) of the variational approximation, to reveal the source of the equivalence. In the expression (24), the first term -KL(QZ PZ ) can be understood as a regularizer, constraining the variational approximation Q not to

16

Connections between the Nystro¨m and Sparse Variational Gaussian Processes

deviate too much from the GP prior P = GP (0, k) on the inducing points Z = (z1, . . . , zn). The second term EF Q [log p(y|FX )] represents the quality of predictions evaluated at the training data (xi, yi)ni=1. Therefore, the ELBO may be understood as a regularized empirical risk for the (distributional) regression problem.
The following result reveals the source of the equivalence between the two approaches.
The proof can be found in Appendix A.2.

Theorem 6 Let  = (Z, µ, )  X m × Rm × Rm>0×m be such that the kernel matrix kZZ  Rm×m is invertible, and let L() be the ELBO in (24). Then we have

n
-22L() =

yi - kZ (xi)kZ-Z1 µ 2 + 2µkZ-Z1 µ

(32)

i=1

n

+ kZ (xi)kZ-Z1 kZ-Z1 kZ (xi)

(33)

i=1

+ 2 tr(kZ-Z1 ) + log(detkZZ /det) - m

(34)

n

+

k(xi, xi) - kZ (xi)kZ-Z1 kZ (xi) .

(35)

i=1

An important consequence of Theorem 6 is that, given Z being fixed, the optimization of
µ and  can be decoupled, since there is no term that depends on both of µ and . Consider
the term (32), which does not depend on  but on µ and Z. Since kZZ is invertible, kZZ as an operator defines a one-to-one mapping from Rm to Rm. Thus we can consider the reparametrization  := kZ-Z1 µ. With this, the µ-dependent term (32) can be written as

n
(yi - kZ (xi))2 + 2kZZ 

i=1

n

=

(yi - fm(xi))2 + 2

fm

2 Hk

,

i=1

(36)

where we defined

m
fm := jk(·, zj )  M  Hk,
j=1

with M = span(k(·, z1), . . . , k(·, zm)) being the subspace in (11). Notice that (36) is the objective function of KRR if 2 = n, evaluated for the function fm from the subspace
M . In other words, the maximization of the ELBO with respect to µ is equivalent to the Nystr¨om KRR (12) with 2 = n. This is summarized in the following corollarly.

Corollary 7 Let  = (Z, µ, )  X m × Rm × Rm0×m be such that the kernel matrix kZZ  Rm×m is invertible, and let L() be the ELBO in (24). For any fixed Z and , let

µ = arg max L()
µRm

17

Wild, Kanagawa and Sejdinovic

and define  = kZ-Z1 µ. Then f  =

m j=1

j k(·, zj )

is

the

solution

of

the

Nystro¨m

KRR

n

f  = arg min (yi - f (xi))2 + 2
f M i=1

f

2 Hk

,

where M = span(k(·, z1), . . . , k(·, zm))  Hk.

Thus, Theorem 6 shows the origin of the equivalence between the SVGP approach of Titsias

(2009a) for GPR the Nystr¨om approximation for KRR.

Note that µ = (µ1, . . . , µm) can be interpreted as noise-free observations at the inducing

points z1, . . . , zm. In fact, fm =

m j=1

jk(·, zj )

with



=

(1,

.

.

. , m)

=

kZ-Z1 µ,

which

is

the mean function (17) of the variational distribution, is the kernel interpolator8 obtained

from training data (zj , µj)mj=1. Thus, the specification of µ1, . . . , µm can be understood as the specification of pseudo observations at z1, . . . , zm. This interpretation is consistent with the fact that µ is the mean vector of "inducing variables" FZ = (F (z1), . . . , F  (zm)).

3.4 Relation to the Deterministic Training Conditional
Before proceeding further, we mention here the equivalence between the Nystr¨om approximation and the Deterministic Training Conditional (DTC), a classic sparse approximation approach to GPR by Seeger et al. (2003); see also Quin~onero-Candela and Rasmussen (2005, Section 5). This discussion will be useful in our investigation of the connections between the Nystr¨om and SVGP.
With DTC, one performs GPR using the approximate kernel q in (15) for the prior, F  GP (0, q), instead of the original kernel k. Given observations y, the resulting GP posterior is F |y  GP (m¯ , k¯) with the posterior mean function m¯ and posterior covariance function k¯ given by

m¯ (x) = qX (x)(qXX + 2In)-1y

= kZ (x)(2kZZ + kZX kXZ )-1kZX y,

q¯(x, x) = q(x, x) - qX (x)(qXX + 2In)-1qX (x)

= kZ (x)(kZZ + -2kZX kXZ )-1kZ (x).

(37)

Notice that the posterior mean function m¯ here and the Nystr¨om approximation (13) are the same if 2 = n. In fact, this identity immediately follows from Theorem 1 on the
equivalence between the KRR and GPR and Theorem 4 on the formulation of the Nystr¨om
as the KRR with the approximate kernel q.

3.5 Geometric Interpretation of the Variational Covariance Function
To further investigate the connections between the sparse approximation methods, we make a geometric interpretation for the covariance function (18) of a variational distribution
8. This corresponds to the KRR estimator (4) with  = 0, X = Z, and y = µ, which is well-defined as long as the kernel matrix kZZ is invertible. In this case, the solution (4) is that of minimum-norm interpolation: minfHk f Hk subject to f (zj ) = µj, j = 1, . . . , m. See e.g. Kanagawa et al. (2018) and references therein.

18

Connections between the Nystro¨m and Sparse Variational Gaussian Processes

Q = GP (m , k ):

k (x, x) = k(x, x) - kZ (x)kZ-Z1 kZ (x)

(38)

+ kZ (x)kZ-Z1 kZ-Z1 kZ (x).

(39)

Part (38) is the posterior covariance function of F  GP (0, k) given noise-free observations (zj, F (zj ))mj=1 and can be written as

k(x, x) - kZ (x)kZ-Z1 kZ (x)

(40)

= k(x, x) - q(x, x)

= k(x, x) - PM (k(·, x)), PM (k(·, x)) Hk = k(·, x) - PM (k(·, x)), k(·, x) - PM (k(·, x)) Hk ,

where q is the approximate kernel (15) and PM : Hk  M is the orthogonal projection (14) onto the subspace M = span(k(·, z1), . . . , k(·, zm)).
Note that k(·, x) - PM (k(·, x)) is the residual of the orthogonal projection PM (k(·, x)). Therefore geometrically, part (38) is the inner product between the two residuals. Intuitively, (38) represents the part of the kernel k that is not captured by the approximate kernel q or, equivalently, by the subspace M .
We next consider part (39). To this end, let Hk be the RKHS of k, M  be the subspace spanned by k (·, z1), . . . , k (·, zm):

M  := span(k (·, z1), . . . , k (·, zm))  Hk ,

(41)

and PM : Hk  M  be the orthogonal projection onto M :

PM (f ) := arg min
gM 

f -g

Hk = kZ (·)(kZ Z )-1fZ ,

for any f  Hk . (See (14)) It can be easily verified that

kZ Z = , kZ (x) = kZ-Z1 kZ (x), x  X .

We can now rewrite part (39) as

kZ (x)kZ-Z1 kZ-Z1 kZ (x) = kZ (x)(kZ Z )-1kZ (x) = PM (k (·, x)), PM (k (·, x)) Hk =: q (x, x)

(42)

Thus, part (39) is the inner product in Hk between the projections PM (k (·, x)) and PM (k (·, x)) on M . Notice that this structure is the same as the definition of the approximate kernel q in (15), but is given with the kernel k parametrized by  and Z. Therefore, part (39) can be understood as the approximation, denoted by q, of k using
the inducing inputs z1, . . . , zm.

19

Wild, Kanagawa and Sejdinovic

3.6 Further Investigation of the ELBO

We now investigate further the expression of ELBO in Theorem 6, focusing on the terms (33) (34) that depend on the covariance matrix , and the term (35) that depends only on Z.
Using (42), the term (33) can be written as

n

n

n

kZ (xi)kZ-Z1 kZ-Z1 kZ (xi) =

PM (k (·, xi))

2 Hk

=

q(xi, xi).

i=1

i=1

i=1

Thus, this term represents the variances (or uncertainties) of a GP with the approximate kernel q at training inputs x1, . . . , xn. On the other hand, the term (34) can be written as

2 tr(kZ-Z1 ) + log(detkZZ /det) - m = 2KL(N (0, ) N (0, kZZ )).

This is the KL divergence between the two zero-mean multivariate Gaussians with covariance matrix  and kZZ. Thus, the sum of (33) and (34) in ELBO is

n

q(xi, xi) + 2KL(N (0, ) N (0, kZZ ))

(43)

i=1

The first term represent uncertainties at training input x1, . . . , xn, and the second term
can be interpreted as a regularizer that encourages  not to deviate from prior covariance matrix kZZ too much. Thus intuitively, the optimal , which minimizes the sum, is
such that the uncertainties at x1, . . . , xn are small while  is not very different from kZZ . This interpretation is consistent with the fact that  is the one that produces a best approximation to the GP posterior given observations (xi, yi)ni=1.
The last term (35) depends only on Z, and therefore it only works when optimizing Z.
This term represents the posterior variances of F  GP (0, k) at x1, . . . , xn given noise-free observations (zj, F (zj ))mj=1. By (40) it can be written as

n

k(xi, xi) - kZ (xi)kZ-Z1 kZ (xi)

i=1

n

n

= (k(xi, xi) - q(xi, xi)) =

k(·, xi) - PM (k(·, xi))

2 Hk

i=1

i=1

Geometrically, this is the sum of the squared length of residuals k(·, xi) - PM (k(·, xi)), and becomes small when the subspace M = span(k(·, z1), . . . , k(·, zm)) approximates well the feature representations k(·, x1), . . . , k(·, xn) of training inputs x1, . . . , xn.

3.7 RKHS Interpretation of the Variational Posterior Covariance Function

We present here an RKHS interpretation of the posterior covariance function (31) of the variational distribution with optimal  in (28):

k(x, x) = k(x, x) - kZ (x)kZ-Z1 kZ (x)

(44)

+ kZ (x)(kZZ + -2kZX kXZ )-1kZ (x).

(45)

20

Connections between the Nystro¨m and Sparse Variational Gaussian Processes

As mentioned, the first term (44) is the posterior covariance function of F  GP (0, k) given noise-free observations (zj, F (zj))mj=1. It is the same as the corresponding term (38) of the covariance function of a variational GP with generic , as this term does not depend on . The second term (45) is given by the approximate kernel in (42) with the optimal  in (28), which minimizes the KL-regularized objective function (43).
Interestingly, the term (45) coincides with the posterior covariance function (37) of the DTC approximation. We summarize these observations in the following proposition:
Proposition 8 Let  be the minimizer of the KL-regularized objective function (43), and let q be the resulting approximate kernel in (42). Then we have q = q¯, where q¯ is the posterior covariance function (37) of the DTC approximation.

Thus, the covariance function (31) of the variational posterior is the sum of two posterior covariance functions:9: (i) one is that of F  GP (0, k) given noise-free observations (zj, F (zj ))mj=1, and (ii) the other is that of F  GP (0, q) given noisy training data (xi, yi)ni=1, where q is the approximate kernel (15) based on inducing inputs z1, . . . , zm.

k(x, x) = k(x, x) - kZ (x)kZ-Z1 kZ (x) + q¯(x, x) .

(i)

(ii)

By using Theorem 2, we can now provide the following RKHS interpretation of the variational posterior variance function (i.e., (31) with x = x). For part (i) with x = x, we
have by Theorem 2, for x  {z1, . . . , zm},

k(x, x) - kZ (x)kZ-Z1 kZ (x) = ( sup {f (x) - kZ (x)kZ-Z1 fZ})2.
f Hk 1
The is the worst case error of kernel interpolation from noise-free observations (zj , f (zj))mj=1, where f is from the unit ball in Hk.
For part (ii) with x = x, define an augmented kernel

q(x, x) = q(x, x) + 2½{x = x}

(46)

and let Hq be its RKHS. Then, again by Theorem 2, for x  {x1, . . . , xn} we have
q¯(x, x) + 2 = ( sup {h(x) + qX (x)(qXX + 2In)-1hX })2.
h Hq 1
Note that the expression qX (x)(qXX + 2In)-1hX is the solution to the KRR for 2 = n with kernel q and training data (xi, h(xi))ni=1. By Theorem 4, this is equal to the Nystr¨om approximation (13) and thus
qX (x)(qXX + 2In)-1hX = kZ (x)(2kZZ + kZX kXZ )-1kZX hX
9. Therefore, k(x, x) - q¯(x, x) = k(x, x) - kZ (x)kZ-Z1 kZ (x). From this expression, it follows that k - q¯ is a positive definite kernel. In particular, this implies that k(x, x)  q¯(x, x) for all x  X . In this sense, the uncertainty estimate of the SVGP approach is more conservative than the DTC, as the posterior variance k(x, x) is always larger or equal to that of the DTC q¯(x, x).

21

Wild, Kanagawa and Sejdinovic

Hence, part (ii) with x = x is the worst case error of the Nystr¨om KRR predicting h(x) using data (xi, h(xi))ni=1, where h is from the unit ball in the RKHS Hq and may be interpreted as a noisy version of a certain function from the RKHS Hq of the approximate kernel q.
To summarize, we have the following RKHS interpretation the variational posterior
variance:

Theorem 9 Let k be a kernel with RKHS Hk, and q be the augmented approximate kernel defined in (46) with RKHS Hq . Suppose that data (xi, yi)ni=1  X × R are given, and that Z = (z1, . . . , zm)  X m are fixed inducing inputs such that the kernel matrix kZZ = (k(zi, zj))mi,j=1  Rm×m is invertible. Let k be the covariance function (31) of the variational posterior. Then, for x  {x1, . . . , xn, z1, . . . , zm}, we have

k(x, x) + 2 = ( sup {f (x) - kZ (x)kZ-Z1 fZ })2

f Hk 1

Kernel Interpolation

+( sup {h(x) - kZ (x)(2kZZ + kZX kXZ )-1kZX hX })2

h Hq 1

Nystro¨m KRR

The posterior variance function of the SVGP provides a means for uncertainty quantification, and thus it is important to understand its behaviors. Theorem 9 shows that this posterior variance (plus the noise variance) is equal to the sum of two worst case errors: (a) the worst case error of kernel interpolation in the original RKHS Hk given noise-free observations at inducing inputs z1, . . . , zm, and (b) the worst case error of the Nystr¨om KRR in the RKHS Hq of the augmented approximate kernel q given noisy observations at training inputs x1, . . . , xn. The first part (a) becomes large when the test input point x is far from inducing inputs z1, . . . , zm, and the second part (b) becomes large when the test input x is far from training inputs x1, . . . , xn. Note that the part (b) depends on the capacity of the RKHS Hq , which depends on the inducing inputs z1, . . . zm since q is defined from the approximate kernel q. Therefore, in general, as the number m of inducing inputs increases, the capacity of Hq increases and approaches that of the RKHS Hk in Theorem 2, and thus the part (b) would also increase.
We have discussed the equivalences and connections between the Nystr¨om and SVGP approximations. In the next section, we focus on their theoretical properties, focusing on the quality of approximation.

4. Connections in Theoretical Properties of Sparse Approximations
We investigate here connections between the theoretical properties of the Nystr¨om and SVGP approximations. The Nystr¨om method provides an approximation to the exact KRR solution, and the SVGP approximates the exact GP posterior. The quality of approximation of either approach depends on the choice of inducing inputs Z = (z1, . . . , zm). We focus here on theoretical error bounds for the approximation quality of either approach, and investigate how they are related.
For the Nystr¨om approximation, researchers have studied various approaches for subsampling inducing inputs z1, . . . , zm from training inputs x1, . . . , xn and their theoretical
22

Connections between the Nystro¨m and Sparse Variational Gaussian Processes
properties. These range from uniform subsampling to subsampling methods based on leverage scores (Rudi et al., 2015; Musco and Musco, 2017; Chen and Yang, 2021), determinantal point processes (DPPs) (Li et al., 2016), and to ensemble methods (Kumar et al., 2009, 2012). Theoretical works either quantify a (relative) deviation of the approximate kernel matrix from the exact one and its impact on downstream tasks (e.g., Cortes et al., 2010; Musco and Musco, 2017), or more directly bound the expected loss of the resulting approximate KRR estimator (e.g. Bach, 2013; El Alaoui and Mahoney, 2015; Rudi et al., 2015). On the other hand, for the SVGP approach, Burt et al. (2019, 2020) recently provided a theoretical analysis of its quality of approximation for the first time.
In Section 4.1, we first discuss a fundamental result of Burt et al. (2020, Lemma 3) on bounding the KL divergence between the approximate and true GP posteriors. In Section 4.2, we then study how their theoretical results are related to the approximation properties of the Nystr¨om KRR. To this end, we first show that the "data fit" term in the marginal likelihood of GPR is identical to the regularized empirical risk of the corresponding KRR estimator. Using this, we show that the analysis of Burt et al. (2020) is essentially based on bounding the difference between the regularized empirical risks of the approximate and exact KRR solutions. Thus, many of the theoretical arguments of Burt et al. (2020), such as the analysis of impacts of using certain inducing inputs (e.g., DPPs and leverage scores), can be directly translated to the analysis of the Nystr¨om KRR. On the other hand, our finding suggests that more sophisticated theoretical arguments for the Nystr¨om KRR (e.g. Bach, 2013; El Alaoui and Mahoney, 2015; Rudi et al., 2015; Chen and Yang, 2021) may be used for obtaining sharper bounds for the SVGP approach. This investigation is left for future research.
Moreover, in Section 4.3, we establish a novel error bound for the Nystr¨om KRR in terms of the RKHS distance to the exact KRR. Note that the RKHS distance is stronger than standard error metrics such as the L2 or L1 errors. This new error bound is parallel to a fundamental bound of Burt et al. (2020) for the SVGP, and thus the theoretical arguments of Burt et al. (2020) can also be applied to bounding the RKHS error for the Nystr¨om KRR.
Lastly, in Section 4.4, we study a lower bound of the KL divergence for the SVGP approximation by Burt et al. (2020, Lemma 4) by considering an average case performance. We show that, under the same setting, this lower bound leads to a lower bound of the approximation error for the Nystr¨om KRR, by the established equivalence result. This new lower bound may be useful in a further analysis of the performance limit of the Nystr¨om approximation.
4.1 A Fundamental Result of Burt et al. (2020)
We first consider the approximation quality of the SVGP approach. In particular, we study a fundamental result of Burt (Lemma 3), from which many other results in Burt et al. (2020) are derived. As before, let µ and  be the optimal variational parameters in (27) and (28), respectively, and let Z = (z1, . . . , zm)  X m be m inducing inputs such that the kernel matrix kZZ is invertible. Let  := (Z, µ, ) and Q = GP (m, k) be the resulting variational GP posterior with mean function m and covariance function k in (17) and (18), respectively.
23

Wild, Kanagawa and Sejdinovic

A natural metric of quantifying the approximation quality of Q is the KL divergence to the exact GP posterior PF |y, which is given by (23) with  = (Z, µ, )

KL Q PF |y = log p(y) - L,

where p(y) is the marginal likelihood and L is the ELBO in (29). We know that (e.g., Rasmussen and Williams, 2006, Eq. (5.8))

log

p(y)

=

-

1 2

log

det(kXX

+

2In)

-

1 2

y(kX

X

+

2In)-1y

-

n 2

log

2

Therefore,

2KL Q PF |y = 2 log p(y) - 2L

= - log det(kXX + 2In) + log det(qXX + 2In)

(47)

-

y(kXX

+

2In)-1y

+

y(qXX

+

2In)-1y

+

1 2

tr(kXX

-

qXX )



-y(kXX

+

2In)-1y

+ y(qXX

+

2In)-1y

+

1 2

tr(kX

X

-

qXX ),

(48)

where the inequality follows from kXX - qXX being positive semi-definite. Burt et al. (2020, Proof of Lemma 3) proceed to bound the first two terms in (48) as

- y(kXX + 2In)-1y + y(qXX + 2In)-1y



y 2 kXX - qXX op 2( kXX - qXX op + 2)



2

y 2tr(kXX - qXX (tr(kXX - qXX ) +

) 2

)

,

(49)

where · op is the operator norm. Thus, we arrive at the following bound (Burt et al., 2020, Lemma 3):

2KL Q

PF |y



tr(kXX - qXX ) 2

tr(kXX

y2 - qXX )

+ 2

+

1

.



tr(kXX - qXX ) 2

y2 2

+1

.

(50)

This result shows that the KL divergence becomes small if tr(kXX - qXX ) is small.

The latter quantity becomes small if, intuitively, the approximate kernel matrix qXX =

kXZ kZ-Z1 kZX is close to the exact one kXX . geometric interpretations of tr(kXX - qXX ) =

See also Section 3.6 for probabilistic and

n i=1

k(xi,

xi)

-

kZ

(xi)kZ-Z1 kZ (xi).

Burt et al. (2020) then establish various results on the KL divergence for the SVGP

approximation by i) relating tr(kXX - qXX ) to the eigenvalues of kXX by considering a specific sampling scheme for Z, such as DPPs and leverage score sampling, ii) relating the

eigenvalues of kXX to those of the corresponding kernel integral operator, and iii) bounding

the eigenvalue decays of the integral operator by considering specific choices of the kernel

k and the probability distribution of training input points x1, . . . , xn.

24

Connections between the Nystro¨m and Sparse Variational Gaussian Processes

4.2 Connection to the Nystro¨m KRR
We now investigate how the bounds on the KL divergence for the SVGP approach are related to the Nystr¨om KRR. The key is the following lemma, which provides an RKHS interpretation of y(kXX + 2I)-1y (and y(qXX + 2I)-1y) appearing in the bound (48). The proof can be found in Appendix B.1.

Lemma 10 Let k be a kernel with RKHS Hk. Let X = (x1, . . . , xn)  X n and y = (y1, . . . , yn)  Rn be given. Then for any 2 > 0, we have

y(kXX

+ 2In)-1y

=

min
f Hk

1 2

n
(yi
i=1

- f (xi))2

+

f

2 Hk

.

Lemma 10 shows that y(kXX + 2In)-1y is a scaled version of the KRR objective function in (2) with 2 = n. Since the solution to the KRR is given by f^ = kX (·)(kXX + 2In)-1y, Lemma 10 implies that

y(kXX

+

2In)-1y

=

1 2

n
(yi - f^(xi))2 +

f^

2 Hk

,

i=1

Similarly, Lemma 10 implies that y(qXX + 2In)y can be written as

y(qXX

+ 2In)-1y

=

min
f Hq

1 2

n
(yi
i=1

-

f¯(xi))2

+

f

2 Hq

=

1 2

n
(yi - f¯(xi))2 +

f¯

2 Hk

,

i=1

where f¯ = kZ (·)(2kZZ + kZX kXZ )-1kZX y is the Nystr¨om approximation by Theorem 4, and we used f¯ Hk = f¯ Hq from Lemma 3. Therefore, y(qXX + 2I)-1y is a scaled version of the regularized empirical risk of the Nystr¨om KRR.
Therefore, the first two terms in the upper bound (48) of the KL divergence can be
written as

y(qXX + 2In)-1y - y(kXX + 2In)-1y

=

1 2

n
(yi - f¯(xi))2 +

f¯

2 Hk

-

1 2

n
(yi - f^(xi))2 +

f^

2 Hk

i=1

i=1

(51)

which is essentially the difference between the KRR objectives for the Nystr¨om and exact estimators. Note that the difference (51) is non-negative since the Nystr¨om approximation is obtained from the subspace M of the RKHS Hk;

(51) = min A(f ) - min A(f )  0

f M

f Hk

where A(f ) :=

1 2

n i=1

(yi

-

f (xi))2

+

f

2 Hk

.

We can interpret the difference (51) as

quantifying the accuracy of the Nystr¨om approximation to the exact KRR solution. That is,

25

Wild, Kanagawa and Sejdinovic

if (51) is small, the Nystr¨om approximation is accurate in the sense that the approximation makes the objective function as small as the exact solution.
As we saw, Burt et al. (2020, Proof of Lemma 3) provides upper-bounds on this difference (51) as in (49). Thus we immediately obtain the following corollary by multiplying (49) by 2/n and setting 2 = n.

Corollary 11 Let k be a kernel with RKHS Hk. Let X = (x1, . . . , xn)  X n and y = (y1, . . . , yn)  Rn be given, and let Z = (z1, . . . , zm)  X m be such that the kernel matrix kZZ  Rm×m is invertible. Let f¯ and f^ be the Nystro¨m and exact KRR estimators in (13) and (4), respectively, with a regularization constant  > 0. Then we have

Rn(f¯; y) - Rn(f^; y)



n(

y 2 kXX - qXX op kXX - qXX op + n)



y 2tr(kX n(tr(kXX -

X- qXX

qXX ) ) + n)

,

where

Rn(f ; y)

:=

1 n

ni=1(yi - f (xi))2 + 

f

2 Hk

.

Therefore, all the results in Burt et al. (2020) on the KL divergence for the SVGP based on Burt et al. (2020, Lemma 3) can be directly translated to the corresponding results for the Nystr¨om KRR using Corollary 11. This is one useful consequence of the RKHS interpretation.
On the other hand, since we now know that there exist KRR objective functions in the expression (47) of the KL divergence, it may be possible to use more sophisticated theoretical arguments for the Nystr¨om KRR (e.g. Bach, 2013; El Alaoui and Mahoney, 2015; Rudi et al., 2015; Chen and Yang, 2021) to obtain sharper bounds on the KL divergence for the SVGP approximation. This investigation is reserved for future research.

4.3 An RKHS Error Bound and its Implications
We present here an upper-bound on the RKHS distance between the Nystr¨om and exact KRR estimators, which is novel to the best of our knowledge. This bound shares a structural similarity to the bound of Burt et al. (2020, Lemma 3) on the KL divergence between the SVGP and exact GP posteriors (50). The proof can be found in Appendix B.2.

Theorem 12 Let k be a kernel with RKHS Hk. Let X = (x1, . . . , xn)  X n and y = (y1, . . . , yn)  Rn be given, and let Z = (z1, . . . , zm)  X m be such that the kernel matrix kZZ  Rm×m is invertible. Let f¯ and f^ be the Nystro¨m and exact KRR estimators in (13) and (4), respectively, with a regularization constant  > 0. Then we have

f^ - f¯

2 Hk



2

tr(kXX - qXX ) (n)2

y

2
.

The upper-bound takes a similar form as the bound (50) on KL divergence for the SVGP approximation in terms of the dependence on tr(kXX - qXX ), y 2 and 2 = n. Similar to our discussion on Corollary 11, one can thus translate the results from Burt et al. (2020) to the correspond bounds on the RKHS distance between the Nystr¨om and exact KRR estimators.
By the equivalence results in Theorems 1 and 5, we immediately obtain the following corollary for the SVGP and exact GP posterior mean functions:

26

Connections between the Nystro¨m and Sparse Variational Gaussian Processes

Corollary 13 Let k be a kernel with RKHS Hk. Let X = (x1, . . . , xn)  X n and y = (y1, . . . , yn)  Rn be given, and let Z = (z1, . . . , zm)  X m be such that the kernel matrix kZZ  Rm×m is invertible. Let m and m¯ be the SVGP and exact GP posterior mean functions in (30) and (7), respectively, with prior F  GP (0, k) and likelihood model (6) with noise variance 2 > 0. Then we have

m¯ - m

2 Hk



2 tr(kXX - qXX ) 4

y

2
.

Note that the RKHS distance is stronger than the supremum norm between two functions. In fact, by the reproducing property, it can be shown that

(f¯(x) - f^(x))2 

f¯ -

f^

2 Hk

k(x,

x),

x  X .

Moreover, if the kernel k is smooth, then the RKHS distance upper-bounds the derivatives of the RKHS functions. To describe this, let X  Rd be an open set. Suppose that the kernel k is continuously differntiable10 on X in the sense that, for any j = 1, . . . , d, the partial derivative jj k(x, x) exists and is continuous on X , where j and j denote the partial derivatives with respect to the j-th coordinate of the first and second arguments of k(x, x), respectively. Then Steinwart and Christmann (2008, Corollary 4.36) implies that,
for all j = 1, . . . , d and all x  X ,

(jf¯(x) - jf^(x))2 

f¯ -

f^

2 Hk

j

j

k(x,

x),

Thus, the bound in Theorem 12 implies that, if tr(kXX - qXX ) is small, then the partial derivatives (and thus the gradients) of the Nystr¨om KRR approximate well those of the exact KRR. By the same argument and Corollary 13, we immediately obtain the following corollary on the equivalent result for the SVGP approximation.

Corollary 14 Suppose the same notation and assumptions in Corollary 13. Let X  Rd be an open set and assume that k is continuously differentiable on X . Then we have for all j = 1, . . . , d and all x  X ,

(jm(x) - jm¯ (x))2



2

tr(kXX

- qXX ) 4

y

2j j k(x, x) .

This shows that the SVGP can approximate not only the exact posterior mean function but also its derivatives, if tr(kXX - qXX ) is small. In applications where the derivative estimates are used (e.g., see Wu et al. 2017), this result provides a support for using the SVGP approximation in place of the exact GP posterior means of derivatives.

4.4 Lower Bounds for Approximation Errors

We discuss here lower bounds for the average case errors of sparse approximations, by assuming a probabilistic model for training outputs y = (y1, . . . , yn). As before, we fix training inputs X = (x1, . . . , xn) and inducing inputs Z = (z1, . . . , zm). Following Burt et al. (2020), we consider the following model for y:

y|X  N (0, kXX + 2In)

(52)

10. Many commonly used kernels, such as the Gaussian kernel, satisfy this requirement.

27

Wild, Kanagawa and Sejdinovic

which is given by the likelihood model (6) and by marginalizing the latent prior GP, F  GP (0, k). Burt et al. (2020, Lemma 4) shows the following lower and upper bounds for the averaged KL divergence between the SGVP and exact GP posteriors:

tr(kXX - 22

qXX )



Ey

KL Q

PF |y



tr(kXX - 2

qXX ) ,

(53)

where Ey denotes the expectation with respect to y generated according to (52). These lower and upper bounds are a priori bounds in the sense that they hold for the
average with respect to the model and thus are informative before observing the actual
training outputs y1, . . . , yn. While this performance measure (the averaged KL divergence) is less informative for the approximation accuracy after one has observed actual training
outputs y1, . . . , yn (the a posteori setting), the lower bound still provides a useful insight. Specifically, the lower bound (53) is proportional to tr(kXX - qXX ). Thus if tr(kXX - qXX ) is large, then the SGVP posterior Q cannot accurately approximate the exact posterior PF |y on average. This is intuitively the case where the inducing inputs Z = (z1, . . . , zm) do not effectively "cover" the training inputs X = (x1, . . . , xn). Since tr(kXX - qXX ) appears both in the upper and lower bounds, the above result shows that tr(kXX - qXX ) can serve as an average performance metric for the SVGP approximation.
By combining (47), (51) and (53), we can obtain the corresponding lower bound on the
difference of the KRR objectives for the Nystr¨om and exact KRR estimators:

Corollary 15 Let k be a kernel with RKHS Hk. Let X = (x1, . . . , xn)  X n and let Z = (z1, . . . , zm)  X m be such that the kernel matrix kZZ  Rm×m is invertible. Suppose y = (y1, . . . , yn)  Rn are generated as (52). Let f¯ and f^ be the Nystro¨m and exact KRR estimators in (13) and (4), respectively, with a regularization constant  = 2/n. Then we have

 log

det(kXX det(qXX

+ 2In) + 2In)



Ey

Rn(f¯; y) - Rn(f^; y)

where

Rn(f ; y)

:=

1 n

ni=1(yi - f (xi))2 + 

f

2 Hk

.

In the left hand side of Corollary 15, log det(kXX + 2I) and log det(qXX + 2I) can intuitively be interpreted as the complexities of the models associated with the kernels k and q, respectively. Thus, Corollary shows that, if the complexity for q is much smaller than that for k, then the difference of the KRR objectives cannot be small on average. This suggests that the left hand side of Corollary 15 may be useful as a quality metric for the Nystr¨om approximation in the a priori setting.

5. Conclusions
We have established various connections and equivalences between sparse approximation methods for GPR and KRR, namely the SVGP and Nystr¨om approximations. We believe that these connections and equivalences will be useful for researchers working on either approach to understand the other. As we demonstrated, framing the approach of interest in the language of the other deepens understanding of that approach and provides new insights. For instance, the RKHS formulation is useful in providing geometric interpretation

28

Connections between the Nystro¨m and Sparse Variational Gaussian Processes

for various concepts in the GP approach and its sparse approximations. We hope that our investigation will inspire further advances in sparse approximation methods as well as new theoretical studies on the topic.

Appendix A. Proofs for Section 3

A.1 Proof of Lemma 3

Proof We first show Hq = M as a set of functions. First note that

q(·, x) = kZ (·)kZ-Z1 kZ (x) = PM k(·, x) , x  X .

Define H0,q as the vector space

n
H0,q := f = iq(·, di) | n  N,  = (1, ..., n)  Rn, D = (d1, ..., dn)  X n .
i=1

Lffoer=mjt=af1ny:ni=j=fP1M=(inikq=((1·mj·,,=dz1ijiq))()j·,k=d(Mi·,)zjbmj)y=H1t0hM,jeqqlb(wi·ne,ieztaahjr)rbistoiytmrHaoerfqy,M0.1. ,.S.Ti.Tnh.ceh,ereermqfeo(f·ro,erdReMi),Hw=0e,qPHhMaqv,M0ek. (f.·T,=Odhiun)s

 M , we have

the other hand,

m j=1

j

k(·,

zj

)

=

we have shown

H0,q = M as a set.

Note that the RKHS Hq is the closure of H0,q with respect to the norm

n

2

iq(·, di)

= qDD = kDZ kZ-Z1 kZD = kDZ kZ-Z1 kZZ kZ-Z1 kZD

i=1

Hq

=

kZ (·)kZ-Z1 kZD

2
=
Hk

qD(·) 2 =
Hk

n

2

iq(·, di) ,

i=1

Hk

which coincides with the norm of Hk. As Hq = M is a finite-dimensional subspace of Hk, it is closed and therefore
Hq = H0,q = M = M.
where the closure is with respect to the norm · Hk = · Hq . Next we show that the scalar products on M and Hq also coincide. Take arbitrary f
and g from Hq. As Hq = H0,q, we find a representation of the form

f = qD(·) = kZ (·)kZ-Z1 kZD = kZ (·)~ g = qE(·) = kZ (·)kZ-Z1 kZE  = kZ (·)~,

where D = (d1, ..., dn)  X n, E = (e1, ..., e)  X ,   Rn,   R, ~ := kZ-Z1 kZD and ~ := kZ-Z1 kZE. This leads to
f, g Hq = qDE = kDZ kZ-Z1 kZE

and f, g Hk = ~kZZ ~ = kDZ kZ-Z1 kZE ,

29

Wild, Kanagawa and Sejdinovic which shows that the scalar products are the same.

A.2 Proof of Theorem 6 Proof We first analyze the term (26) in the ELBO. Let fZ  Rm be an arbitrary vector in the support of QZ. Define a notation for the conditional expectation
Fm(x) := E[F (x) | FZ = fZ] = E[F  (x) | FZ = fZ ].

where the identity follows from the definition of F   Q. Then, from the standard biasvariance decomposition argument, we have

n

n

E (yi - F (xi))2 | FZ = fZ = E (yi - F (xi))2 | FZ = fZ

i=1

i=1

n

n

= E (yi - Fm(xi))2 | FZ = fZ + E (Fm(xi) - F (xi))2 | FZ = fZ

i=1

i=1

n

n

= (yi - Fm(xi))2 + E (Fm(xi) - F (xi))2 | FZ = fZ .

(54)

i=1

i=1

Note that, because Fm(x) is the conditional expectation of F (x) given FZ = fZ, it is equivalent to the kernel interpolator with training data (zi, F (zi))ni=1 and can be written as
Fm(x) = kZ (x)kZ-Z1 fZ .

Therefore

(yi - Fm(xi))2dQZ (fZ ) = (yi - kZ (xi)kZ-Z1 fZ )2dQZ (fZ )
= (yi - kZ (xi)kZ-Z1 µ)2dQZ (fZ ) + (kZ (xi)kZ-Z1 µ - kZ (xi)kZ-Z1 fZ )2dQZ (fZ ) = (yi - kZ (xi)kZ-Z1 µ)2 + kZ (xi)kZ-Z1 kZ-Z1 kZ (xi),
where the last identity follows from QZ = N (µ, ) by definition. On the other hand, the second term in (54) is the conditional variance of F (xi) given
FZ = fZ , and thus given by
E (Fm(xi) - F (xi))2 | FZ = fZ = k(xi, xi) - kZ (xi)kZ-Z1 kZ (xi),
which is independent to the "observations" fZ. Therefore

E (Fm(xi) - F (xi))2 | FZ = fZ dQZ (fZ) = k(xi, xi) - kZ (xi)kZ-Z1 kZ (xi).

30

Connections between the Nystro¨m and Sparse Variational Gaussian Processes

Using these identities, we have

n

E (yi - F  (xi))2 | FZ = fZ dQZ (fZ )

i=1

n

n

= (yi - kZ (xi)kZ-Z1 µ)2 + kZ (xi)kZ-Z1 kZ-Z1 kZ (xi)

i=1

i=1

n

+

k(xi, xi) - kZ (xi)kZ-Z1 kZ (xi)

i=1

The proof completes by inserting this last expression of (26) and the expression of the KL divergence (25) in the ELBO (24) and rearranging.

Appendix B. Proofs for Section 4

B.1 Proof of Lemma 10 Proof Recall that f^ := kX (·)(kXX + 2In)-1y is the solution of KRR. We have
f^X = kXX (kXX + 2In)-1y = (In - 2(kXX + 2In)-1)y,

where we used the formula A(A + 2I)-1 = In - 2(A + 2In)-1 that holds for any positive semidefinte matrix A. Now we have

n

min
f Hk

(yi
i=1

-

f (xi))2

+

2

f

2 Hk

=

y - f^X

2 + 2

f^

2 Hk

The first term can be expanded as

y - f^X 2 = y 2 - 2yf^X + f^X 2 = y 2 - 2y(In - 2(kXX + 2In)-1)y + y(In - 2(kXX + 2In)-1)2y = y 2 - 2y(In - 2(kXX + 2In)-1)y + y(In - 22(kXX + 2In)-1 + 4(kXX + 2In)-2)y = 4y(kXX + 2In)-2y.

The second term is

2

f^

2 Hk

= 2y(kXX + 2In)-1kXX (kXX + 2In)-1y

= 2y(kXX + 2In)-1(In - 2(kXX + 2In)-1)y

= 2y(kXX + 2In)-1y - 4y(kXX + 2In)-2y.

Therefore,

y - f^X

2 + 2

f^

2 Hk

=

2y(kXX

+

2In)-1y.

31

Wild, Kanagawa and Sejdinovic

B.2 Proof of Theorem 12
Proof We first make preliminaries for proving the theorem. For a symmetric matrix B  Rn×n with n  N, denote by 1(B)  · · ·  n(B) its eigenvalues with multiplicities in the decreasing order. For any symmetric and positive semi-definite (SPSD) matrix A  Rn×n and any B  Rn×n, we have (see Saniuk and Rhodes, 1987)

tr(AB)  tr(A) B op,

(55)

where B op := supvR: v 1 Bv denotes the operator norm (or spectral norm). If B is symmetric, we have B op = max(|1(B)|, |n(B)|).
For any SPSD matrix A  Rn×n and any symmetric and negative semi-definite (SNSD)
matrix B  Rn×n, we have

n

tr(AB) = tr(BA) = tr A1/2BA1/2 = i(A1/2BA1/2)  0,

(56)

i=1

where the inequality follows from the fact that A1/2BA1/2 is SNSD and hence all its eigenvalues are non-positive.
We also use the following short hand notation

K := kXX , Q := qXX , K~ := K + nIn, Q~ := Q + nIn,  = K~ -1y,  = Q~-1y, ~ = nkZZ + kZX kXZ -1kZX y.

Note that the matrices K, Q, K~ , Q~ are SPSD. It holds that

K~ -1

op



1 n

,

Q~ -1

op 

1 n

,

KK~ -1 op  1,

QQ~-1 op  1.

Using the above notation, the KRR estimator f^ and the Nystr¨om approximation f¯ can be written for any x  X as

f^(x) = kX (x)K~ -1y, f¯(x) = qX (x)Q~-1y = qX (x)
= kZ (x) nkZZ + kZX kXZ -1kZX y = kZ (x)~.

We will use the following identity: kXZ ~ = f¯X = qXX Q~-1y = QQ~-1y.

With these preparations, we now prove the assertion. First we have

f^ -

f¯

2 Hk

=

f^

2 Hk

-

2

f^, f¯

Hk

+

f¯

2 Hk

=

f^

2 Hk

-

2

f^, f¯

Hk

+

f¯

2 Hq

,

32

Connections between the Nystro¨m and Sparse Variational Gaussian Processes

where we used

f¯

2 Hq

=

f¯

2 Hk

,

which

holds

from

f¯ 

M

and

Lemma

3.

The

expression

is

equal to

= K - 2kXZ ~ + Q

= tr(K) - 2tr(kXZ ~) + tr(Q)

= tr(KK~ -1yyK~ -1) - tr(QQ~-1yyK~ -1) + tr(QQ~-1yyQ~-1) - tr(QQ~-1yyK~ -1)

= tr (KK~ -1 - QQ~-1)yyK~ -1 + tr QQ~-1yy(Q~-1 - K~ -1)

 tr(KK~ -1 - QQ~-1) yy op K~ -1 op + tr(Q~-1 - K~ -1) QQ~-1 op yy op



1 n

tr(K

K~

-1

-

QQ~ -1 )

y

2 + tr(Q~-1 - K~ -1)

y

2

=

1 n

tr (K - Q)K~ -1

+ tr Q(K~ -1 - Q~-1)

y 2 + tr(Q~-1(K - Q)K~ -1) y 2

Since Q is SPSD and K~ -1 - Q~-1 is SNSD, we have

tr Q(K~ -1 - Q~-1)  0

due to (56). Using this and (55), we obtain



1 n

tr(K

-

Q)

K~ -1

op

y

2 + tr(K - Q)

K~ -1

op

Q~ -1

op

y

2



2 (n)2

tr(K

-

Q)

y

2,

which concludes the proof.

References
Vincent Adam, Stefanos Eleftheriadis, Artem Artemev, Nicolas Durrande, and James Hensman. Doubly sparse variational gaussian processes. In International Conference on Artificial Intelligence and Statistics, pages 2874­2884. PMLR, 2020.
Raja Hafiz Affandi, Alex Kulesza, Emily Fox, and Ben Taskar. Nystr¨om approximation for large-scale determinantal processes. In Artificial Intelligence and Statistics, pages 85­98. PMLR, 2013.
Nachman Aronszajn. Theory of reproducing kernels. Transactions of the American Mathematical Society, 68(3):337­404, 1950.
Francis Bach. Sharp analysis of low-rank kernel matrix approximations. In Conference on Learning Theory, pages 185­209. PMLR, 2013.
Matthias Bauer, Mark van der Wilk, and Carl Edward Rasmussen. Understanding probabilistic sparse Gaussian process approximations. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016.
33

Wild, Kanagawa and Sejdinovic
Mohamed-Ali Belabbas and Patrick J Wolfe. Spectral methods in machine learning and new strategies for very large datasets. Proceedings of the National Academy of Sciences, 106(2):369­374, 2009.
A. Berlinet and C. Thomas-Agnan. Reproducing Kernel Hilbert Spaces in Probability and Statistics. Kluwer, 2004.
David Burt, Carl Edward Rasmussen, and Mark Van Der Wilk. Rates of convergence for sparse variational Gaussian process regression. In Proceedings of the 36th International Conference on Machine Learning, pages 862­871, 2019.
David R. Burt, Carl Edward Rasmussen, and Mark van der Wilk. Convergence of sparse variational inference in Gaussian processes regression. Journal of Machine Learning Research, 21(131):1­63, 2020.
Yifan Chen and Yun Yang. Fast statistical leverage score approximation in kernel ridge regression. In International Conference on Artificial Intelligence and Statistics, pages 2935­2943. PMLR, 2021.
Corinna Cortes, Mehryar Mohri, and Ameet Talwalkar. On the impact of kernel approximation on learning accuracy. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pages 113­120. JMLR Workshop and Conference Proceedings, 2010.
Lehel Csato´ and Manfred Opper. Sparse on-line Gaussian processes. Neural Computation, 14(3):641­668, 2002.
Michal Derezinski, Rajiv Khanna, and Michael W Mahoney. Improved guarantees and a multiple-descent curve for Column Subset Selection and the Nystr¨om method. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 4953­4964. Curran Associates, Inc., 2020.
Petros Drineas and Michael W. Mahoney. On the Nystr¨om method for approximating a Gram matrix for improved kernel-based learning. Journal of Machine Learning Research, 6(72):2153­2175, 2005.
Vincent Dutordoir, Nicolas Durrande, and James Hensman. Sparse Gaussian processes with spherical harmonic features. In Hal Daum´e III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 2793­2802. PMLR, 2020.
Ahmed El Alaoui and Michael W Mahoney. Fast randomized kernel ridge regression with statistical guarantees. In Advances in Neural Information Processing Systems, pages 775­783, 2015.
Charless Fowlkes, Serge Belongie, Fan Chung, and Jitendra Malik. Spectral grouping using the Nystr¨om method. IEEE Transactions on Pattern Analysis and Machine Intelligence, 26(2):214­225, 2004.
34

Connections between the Nystro¨m and Sparse Variational Gaussian Processes
Alex Gittens and Michael W Mahoney. Revisiting the nystr¨om method for improved largescale machine learning. The Journal of Machine Learning Research, 17(1):3977­4041, 2016.
James Hensman, Nicol`o Fusi, and Neil D Lawrence. Gaussian processes for big data. In Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence, pages 282­290, 2013.
James Hensman, Alexander Matthews, and Zoubin Ghahramani. Scalable variational Gaussian process classification. In Artificial Intelligence and Statistics, pages 351­360. PMLR, 2015a.
James Hensman, Alexander G Matthews, Maurizio Filippone, and Zoubin Ghahramani. MCMC for variationally sparse Gaussian processes. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015b.
James Hensman, Nicolas Durrande, and Arno Solin. Variational Fourier features for Gaussian processes. Journal of Machine Learning Research, 18(151):1­52, 2018.
Thomas Hofmann, Bernhard Scho¨lkopf, and Alexander J Smola. Kernel methods in machine learning. Annals of Statistics, 36(3):1171­1220, 2008.
Motonobu Kanagawa, Philipp Hennig, Dino Sejdinovic, and Bharath K Sriperumbudur. Gaussian processes and kernel methods: A review on connections and equivalences. arXiv preprint arXiv:1807.02582, 2018.
G. S. Kimeldorf and G. Wahba. A correspondence between Bayesian estimation on stochastic processes and smoothing by splines. The Annals of Mathematical Statistics, 41(2): 495­502, 1970.
Sanjiv Kumar, Mehryar Mohri, and Ameet Talwalkar. Ensemble Nystr¨om method. In Y. Bengio, D. Schuurmans, J. Lafferty, C. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems, volume 22. Curran Associates, Inc., 2009.
Sanjiv Kumar, Mehryar Mohri, and Ameet Talwalkar. Sampling methods for the nystr¨om method. Journal of Machine Learning Research, 13(34):981­1006, 2012.
Felix Leibfried, Vincent Dutordoir, ST John, and Nicolas Durrande. A tutorial on sparse Gaussian processes and variational inference. arXiv preprint arXiv:2012.13962, 2020.
Chengtao Li, Stefanie Jegelka, and Suvrit Sra. Fast DPP sampling for Nystr¨om with application to kernel methods. In Maria Florina Balcan and Kilian Q. Weinberger, editors, Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pages 2061­2070, New York, New York, USA, 20­22 Jun 2016. PMLR.
Alexander G de G Matthews, James Hensman, Richard Turner, and Zoubin Ghahramani. On sparse variational methods and the Kullback-Leibler divergence between stochastic processes. In Artificial Intelligence and Statistics, pages 231­239, 2016.
35

Wild, Kanagawa and Sejdinovic
Giacomo Meanti, Luigi Carratino, Lorenzo Rosasco, and Alessandro Rudi. Kernel methods through the roof: Handling billions of points efficiently. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 14410­14422. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/a59afb1b7d82ec353921a55c579ee26d-Paper.pdf.
Cameron Musco and Christopher Musco. Recursive sampling for the Nystr¨om method. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.
E. Parzen. An approach to time series analysis. The Annals of Mathematical Statistics, 32 (4):951­989, 1961.
Joaquin Quin~onero-Candela and Carl Edward Rasmussen. A unifying view of sparse approximate Gaussian process regression. Journal of Machine Learning Research, 6:1939­1959, 2005.
C.E. Rasmussen and C.K.I. Williams. Gaussian Processes for Machine Learning. MIT Press, 2006.
Simone Rossi, Markus Heinonen, Edwin Bonilla, Zheyang Shen, and Maurizio Filippone. Sparse Gaussian processes revisited: Bayesian approaches to inducing-variable approximations. In Arindam Banerjee and Kenji Fukumizu, editors, Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, volume 130 of Proceedings of Machine Learning Research, pages 1837­1845. PMLR, 13­15 Apr 2021.
Alessandro Rudi, Raffaello Camoriano, and Lorenzo Rosasco. Less is more: Nystr¨om computational regularization. In Advances in Neural Information Processing Systems, pages 1657­1665, 2015.
Alessandro Rudi, Luigi Carratino, and Lorenzo Rosasco. FALKON: An optimal large scale kernel method. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.
J Saniuk and I Rhodes. A matrix inequality associated with bounds on solutions of algebraic Riccati and Lyapunov equations. IEEE Transactions on Automatic Control, 32(8):739­ 740, 1987.
Bernhard Scho¨lkopf and Alexander J Smola. Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond. MIT press, 2002.
Bernhard Scho¨lkopf, Ralf Herbrich, and Alex J Smola. A generalized representer theorem. In International Conference on Computational Learning Theory, pages 416­426. Springer, 2001.
Matthias Seeger, Christopher Williams, and Neil Lawrence. Fast forward selection to speed up sparse Gaussian process regression. In Artificial Intelligence and Statistics, 2003.
36

Connections between the Nystro¨m and Sparse Variational Gaussian Processes
Jiaxin Shi, Michalis Titsias, and Andriy Mnih. Sparse orthogonal variational inference for Gaussian processes. In Silvia Chiappa and Roberto Calandra, editors, Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, volume 108 of Proceedings of Machine Learning Research, pages 1932­1942. PMLR, 26­28 Aug 2020.
Alex J Smola and Bernhard Scho¨lkopf. Sparse greedy matrix approximation for machine learning. In Proceedings of the Seventeenth International Conference on Machine Learning, pages 911­918, 2000.
Edward Snelson and Zoubin Ghahramani. Sparse Gaussian processes using pseudo-inputs. In Advances in Neural Information Processing Systems, pages 1257­1264, 2006.
Edward Snelson and Zoubin Ghahramani. Local and global sparse Gaussian process approximations. In Artificial Intelligence and Statistics, pages 524­531, 2007.
I. Steinwart and A. Christmann. Support Vector Machines. Springer, 2008.
Ameet Talwalkar, Sanjiv Kumar, and Henry Rowley. Large-scale manifold learning. In 2008 IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 2008.
Ameet Talwalkar, Sanjiv Kumar, Mehryar Mohri, and Henry Rowley. Large-scale SVD and manifold learning. Journal of Machine Learning Research, 14(60):3129­3152, 2013.
Terence Tao. An Introduction to Measure Theory, volume 126. American Mathematical Society Providence, 2011.
Michalis Titsias. Variational learning of inducing variables in sparse Gaussian processes. In Artificial Intelligence and Statistics, pages 567­574, 2009a.
Michalis K Titsias. Variational model selection for sparse Gaussian process regression. Technical Report, University of Manchester, UK, 2009b.
Gia-Lac Tran, Dimitrios Milios, Pietro Michiardi, and Maurizio Filippone. Sparse within sparse Gaussian processes using neighbor information. In Proceedings of the Thirty-eighth International Conference on Machine Learning, 2021.
Grace Wahba. Spline Models for Observational Data. SIAM, 1990.
Christopher KI Williams and Matthias Seeger. Using the Nystr¨om method to speed up kernel machines. In Advances in Neural Information Processing Systems, pages 682­688, 2001.
Jian Wu, Matthias Poloczek, Andrew G Wilson, and Peter Frazier. Bayesian optimization with gradients. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.
37

