{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy: Advanced NLP in Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/sanikamal/spacy-advanced-nlp-in-python\n",
    "# !pip install spacy\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import glob\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Higher-order Derivatives of Weighted Finite-state Machines\\nRan Zmigrod Tim Vieira Ryan Cotterell , University of Cambridge Johns Hopkins University ETH Zu\\xa8rich\\nrz279@cam.ac.uk tim.f.vieira@gmail.com ryan.cotterell@inf.ethz.ch\\n\\narXiv:2106.00749v1 [cs.CL] 1 Jun 2021\\n\\nAbstract\\nWeighted finite-state machines are a fundamental building block of NLP systems. They have withstood the test of time--from their early use in noisy channel models in the 1990s up to modern-day neurally parameterized conditional random fields. This work examines the computation of higher-order derivatives with respect to the normalization constant for weighted finite-state machines. We provide a general algorithm for evaluating derivatives of all orders, which has not been previously described in the literature. In the case of second-order derivatives, our scheme runs in the optimal O(A2N 4) time where A is the alphabet size and N is the number of states. Our algorithm is significantly faster than prior algorithms. Additionally, our approach leads to a significantly faster algorithm for computing second-order expectations, such as covariance matrices and gradients of first-order expectations.\\n1 Introduction\\nWeighted finite-state machines (WFSMs) have a storied role in NLP. They are a useful formalism for speech recognition (Mohri et al., 2002), machine transliteration (Knight and Graehl, 1998), morphology (Geyken and Hanneforth, 2005; Linde\\xb4n et al., 2009) and phonology (Cotterell et al., 2015) inter alia. Indeed, WFSMs have been \"neuralized\" (Rastogi et al., 2016; Hannun et al., 2020; Schwartz et al., 2018) and are still of practical use to the NLP modeler. Moreover, many popular sequence models, e.g., conditional random fields for part-ofspeech tagging (Lafferty et al., 2001), are naturally viewed as special cases of WFSMs. For this reason, we consider the study of algorithms for the WFSMs of interest in se for the NLP community.\\nThis paper considers inference algorithms for WSFMs. When WFSMs are acyclic, there exist\\n\\nsimple linear-time dynamic programs, e.g., the forward algorithm (Rabiner, 1989), for inference. However, in general, WFSMs may contain cycles and such approaches are not applicable. Our work considers this general case and provides a method for efficient computation of mth-order derivatives over a cyclic WFSM. To the best of our knowledge, no algorithm for higher-order derivatives has been presented in the literature beyond a generalpurpose method from automatic differentiation. In contrast to many presentations of WFSMs (Mohri, 1997), our work provides a purely linear-algebraic take on them. And, indeed, it is this connection that allows us to develop our general algorithm.\\nWe provide a thorough analysis of the soundness, runtime, and space complexity of our algorithm. In the special case of second-order derivatives, our algorithm runs optimally in O(A2N 4) time and space where A is the size of the alphabet, and N is the number of states. In contrast, the second-order expectation semiring of Li and Eisner (2009) provides an O(A2N 7) solution and automatic differentiation (Griewank, 1989) yields a slightly faster O(AN 5+A2N 4) solution. Additionally, we provide a speed-up for the general family of second-order expectations. Indeed, we believe our algorithm is the fastest known for computing common quantities, e.g., a covariance matrix.1\\n2 Weighted Finite-State Machines\\nIn this section we briefly provide important notation for WFSMs and a classic result that efficiently finds the normalization constant for the probability distribution of a WFSM.\\nDefinition 1. A weighted finite-state machine M is a tuple , {W(a)}aA,  where A is an al-\\n1Due to space constraints, we keep the discussion of our paper theoretical, though applications of expectations that we can compute are discussed in Li and Eisner (2009), Sa\\xb4nchez and Romero (2020), and Zmigrod et al. (2021).\\n\\n\\x0cphabet of size A, A d=ef A  {}, each a  A has a symbol-specific transition matrix W(a)  R0N\\xd7N where N is the number of states, and ,   R0N are column vectors of start and end weights, respectively. We define the matrix W d=ef aA W(a).\\nDefinition 2. A trajectory i is an ordered sequence of transitions from state i to state . Visually,\\nwe can represent a trajectory by\\n\\ni d=ef i -a j \\xb7 \\xb7 \\xb7 k -a\\n\\nThe weight of a trajectory is\\n\\n\\n\\n\\n\\nw(i\\n\\n)\\n\\nd=ef\\n\\ni\\n\\n \\n\\nWj(ak) \\n\\n(1)\\n\\n(j-a k)i\\n\\nWe denote the (possibly infinite) set of trajectories\\nfrom i to by Ti and the set of all trajectories by T d=ef i, [N] Ti .2 Consequently, when we say i  T , we make i and implicit arguments to which Ti we are accessing.\\nWe define the probability of a trajectory i  T ,\\n\\np(i\\n\\n) d=ef w(i Z\\n\\n)\\n\\n(2)\\n\\nwhere\\n\\n\\n\\nZ d=ef \\n\\nWk \\n\\n(3)\\n\\nk=0\\n\\nOf course, p is only well-defined when 0 < Z < .3 Intuitively,  Wk  is the total weight of all trajectories of length k. Thus, Z is the total weight of all possible trajectories as it sums over the total weight for each possible trajectory length.\\n\\nTheorem 1 (Corollary 4.2, Lehmann (1977)).\\n\\n\\n\\nW d=ef Wk = (I - W)-1\\n\\n(4)\\n\\nk=0\\n\\nThus, we can solve the infinite summation that defines W by matrix inversion in O(N 3) time.4\\nCorollary 1.\\n\\nZ= W \\n\\n(5)\\n\\n2|T | is infinite if and only if M is cyclic. 2Another formulation for Z is i T w(i ). 3This requirement is equivalent to W having a spectral radius < 1. 4This solution technique may be extended to closed semir-\\nings (Kleene, 1956; Lehmann, 1977).\\n\\nProof. Follows from (4) in Theorem 1. By Corollary 1, we can find Z in O(N 3 + AN 2).5\\n\\nStrings versus Trajectories. Importantly, WFSMs can be regarded as weighted finite-state acceptors (WFSAs) which accept strings as their input. Each trajectory i  T has a yield (i ) which is the concatenation of the alphabet symbols of the trajectory. The yield of a trajectory ignores any  symbols, a discussion regarding the semantics of  is given in Hopcroft et al. (2001). As we focus on distributions over trajectories, we do not need special considerations for  transitions. We do not consider distributions over yields in this work as such a distribution requires constructing a latent-variable model\\n\\n1\\n\\np() = Z\\n\\nw(i )\\n\\n(6)\\n\\ni T ,\\n\\n(i )=\\n\\nwhere   A and (i ) is the yield of the trajectory. While marginal likelihood can be found efficiently,6 many quantities, such as the entropy of the distribution over yields, are intractable to compute (Cortes et al., 2008).\\n\\n3 Computing the Hessian (and Beyond)\\n\\nIn this section, we explore algorithms for efficiently computing the Hessian matrix 2Z. We briefly describe two inefficient algorithms, which are derived by forward-mode and reverse-mode automatic differentiation. Next, we propose an efficient algorithm which is based on a key differential identity.\\n3.1 An O(A2N 7) Algorithm with Forward-Mode Automatic Differentiation\\nOne proposal for computing the Hessian comes from Li and Eisner (2009) who introduce a method based on semirings for computing a general family of quantities known as second-order expectations (defined formally in \\xa74). When applied to the computation of the Hessian their method reduces precisely to forward-mode automatic differentiation (AD; Griewank and Walther, 2008, Chap 3.1). This\\n5Throughout this paper, we assume a dense weight matrix and that matrix inversion is O(N 3) time. We note, however, that when the weight matrix is sparse and structured, faster matrix-inversion algorithms exist that exploit the strongly connected components decomposition of the graph (Mohri et al., 2000). We are agnostic to the specific inversion algorithm, but for simplicity we assume the aforementioned running time.\\n6This is done by intersecting the WFSA with another WFSA that only accepts .\\n\\n\\x0capproach requires that we \"lift\" the computation of Z to operate over a richer numeric representation\\nknown as dual numbers (Clifford, 1871; Pearlmut-\\nter and Siskind, 2007). Unfortunately, the second-\\norder dual numbers that we require to compute the Hessian introduce an overhead of O(A2N 4) per numeric operation of the O(N 3) algorithm that computes Z, which results in O(A2N 7) time.\\n\\n3.2 An O(AN 5 +A2N 4) Algorithm with\\nReverse-Mode Automatic Differentiation\\nAnother method for materializing the Hessian 2Z is through reverse-mode automatic differ-\\nentiation (AD). Recall that we can compute Z in O(N 3 + AN 2), and can consequently find Z in O(N 3 + AN 2) using one pass of reverse-\\nmode AD (Griewank and Walther, 2008, Chap-\\nter 3.3). We can repeat differentiation to materialize 2Z. Specifically, we run reverse-mode AD\\nonce for each element i of Z. Taking the gra-\\ndient of (Z)i gives a row of the Hessian matrix, [(Z)i] = [2Z](i,:). Since each of these passes takes time O(N 3+AN 2) (i.e., the same as the cost of Z), and Z has size AN 2, the overall time is O(AN 5 +A2N 4).\\n\\n3.3 Our Optimal O(A2N 4) Algorithm\\nIn this section, we will provide an O(A2N 4)-time and space algorithm for computing the Hessian. Since the Hessian has size O(A2N 4), no algorithm can run faster than this bound; thus, our algorithm\\'s time and space complexities are optimal. Our algorithm hinges on the following lemma, which shows that the each of partial derivatives of W can be cheaply computed given W .\\nLemma 1. For i, j, k,  [N ] and a  A\\n\\nWi Wj(ak)\\n\\n=\\n\\nWij W. j(ak)Wk\\n\\n(7)\\n\\nwhere W. j(ak) is shorthand for Wj(ak).\\nProof.\\n\\nWi Wj(ak)\\n\\n=\\n\\n Wj(ak)\\n\\n(I - W)-i 1\\n\\n = -Wij Wj(ak) [(I - W)] Wk\\n= Wij W. j(ak)Wk\\n\\nThe second step uses Equation 40 of the Matrix Cookbook (Petersen and Pedersen, 2008).\\n\\nWe now extend Lemma 1 to express higher-\\n\\noinrdLeermdmeraiv1a,tiwveeswinillteursme sW.oi(fja)Was\\n\\n. a\\n\\nNote that as shorthand for\\n\\nthe partial derivative Wi(ja).\\n\\nTheorem 2. For m  1 and m-tuple of transitions  = i1 -a1 j1, . . . , im -a-m  jm\\n\\nmZ\\n\\n= Wi(1aj11) \\xb7 \\xb7 \\xb7 Wi(mamjm)\\n\\n(8)\\n\\ni1-a1 j1,\\xb7\\xb7\\xb7 ,im-a-m jm S\\n\\nsi1 W. i(1aj11)Wj1i2 W. i(2aj22) \\xb7 \\xb7 \\xb7 Wjm-1im W. i(mamjm) ejm\\n\\nwhere s =  W , e = W  and S is the multiset of permutations of  .7 Proof. See App. A.1\\nCorollary 2. For i, j, k, l  [N ] and a, b  A\\n\\n2Z\\n\\nWi(ja)Wk(bl) =\\n\\n(9)\\n\\nsiW. i(ja)WjkW. k(bl)el + skW. k(bl)WliW. i(ja)ej\\n\\nProof. Application of Theorem 2 for the m=2 case.\\n\\nTheorem 2 shows that, if we have already computed W , each element of the mth derivative can be found in O(m m!) time: We must sum over O(m!) permutations, where each summand is the product of O(m) items. Importantly, for the Hessian (m = 2), we can find each element in O(1) using Corollary 2. Algorithm Dm in Fig. 1 provides pseudocode for materializing the tensor containing the mth derivatives of Z.\\nTheorem 3. For m  1, algorithm Dm computes mZ in O(N 3 + m m! AmN 2m) time and O(AmN 2m) space.\\nProof. Correctness of algorithm Dm follows from Theorem 2. The runtime and space bounds follow by needing to compute and store each combination of transitions. Each line of the algorithm is annotated with its running time.\\nCorollary 3. The Hessian 2Z can be materialized in O(A2N 4) time and O(A2N 4) space. Note that these bounds are optimal.\\nProof. Application of Theorem 3 for the m=2 case.\\n\\n7As  may have duplicates, S can also have duplicates and so must be a multi-set.\\n\\n\\x0c1: def Dm(W, , ) :\\n\\n2:\\n\\nCompute the tensor of mth-order derivative of a\\n\\nWFSM; requires O(N 3 + m m! AmN 2m) time,\\n\\nO(AmN 2m) space.\\n\\n3: W  (I - W)-1\\n\\nO(N 3)\\n\\n4: s   W ; e  W \\n\\nO(N 2)\\n\\n5: D  0\\n\\n6: for   ([N ]\\xd7[N ]\\xd7A)m : O(mm!AmN2m)\\n\\n7: 8:\\n\\nforDi1+-=a1\\xb7s\\xb7ij\\xb711W W,..ij(.1am.j1-1,)i1Wmimj-W 1a-.im 2i(Wma.mjji(mm2)aj2e2)jWm jS2i3 :\\n\\n9: return D\\n\\n10: def E2(W, , , r, t) :\\n\\n11:\\n\\nCompute the second-order expectation of a\\n\\nWFSM; requires O(N 3 + N 2(R T + AR T ))\\n\\ntime, O(N 2 + RT + N (R + T )) space where\\n\\nR d=ef min(N R , R) and T d=ef min(N T , T ).\\n12: Compute W , s, and e as in Dm O(N3)\\n\\n13: Z   W \\n\\n14: rs  0; re  0; ts  0; te  0\\n\\n15: 16: 17: 18: 19:\\n\\nforrrttsieiiisei,++++j====sWsW.j.j[WWNi(i(..jaja)a()]j(ei,ejai)jaj)WWWW. j(i(j(aj(ijaaAiai))))ttrW(j(i:aji(jaai))i()ja) ri(ja)\\n\\nO(AN 2) O(R ) O(R ) O(T ) O(T )\\n\\n20:\\n\\nreturn\\n\\n1 Z\\n\\nN i,j=0\\n\\nris\\n\\nWij\\n\\ntej\\n\\n+ tsi Wij rje\\n\\n+ aA siW. i(ja)ej Wi(ja)ri(ja)t(ija)\\n\\nO(N 2(R T +AR T ))\\n\\nFigure 1: Algorithms\\n4 Second-Order Expectations\\n\\nIn this section, we leverage the algorithms of the previous section to efficiently compute a family expectations, known as a second-order expectations. To begin, we define an additively decomposable function r: T  RR as any function expressed as\\n\\nr(i ) =\\n\\nrj(ak)\\n\\n(10)\\n\\n(j-a k)i\\n\\nwhere each rj(ak) is an R-dimensional vector. Since many r of interest are sparse, we analyze our al-\\n\\ngorithms in terms of R and its maximum den-\\n\\nsity R\\n\\nd=ef\\n\\nmax\\nj\\n\\n-a k\\n\\nrj(ak) 0. Previous work has\\n\\nconsidered expectations of such functions (Eisner,\\n\\n2001) and the product of two such functions (Li\\n\\nand Eisner, 2009), better known as second-order ex-\\n\\npectations. Formally, given two additively decom-\\n\\nposable functions r: T  RR and t: T  RT , a second-order expectation is\\n\\nEi r(i )t(i ) d=ef\\n\\n(11)\\n\\np(i )r(i )t(i )\\ni T\\n\\nExamples of second-order expectations include the Fisher information matrix and the gradients of firstorder expectations (e.g., expected cost, entropy, and the Kullback\\xadLeibler divergence).\\nOur algorithm is based on two fundamental concepts. Firstly, expectations for probability distributions as described in (1), can be decomposed as expectations over transitions (Zmigrod et al., 2021). Secondly, the marginal probabilities of transitions are connected to derivatives of Z.8\\nLemma 2. For m  1 and m-tuple of transitions  = i1 -a1 j1, . . . , im -a-m  jm\\n\\np( )\\n\\n=\\n\\n1 Z\\n\\nm n=1\\n\\n\\n\\n Wi(1aj11) .\\n\\nnZ ..\\n\\nWi(nanjn)\\n\\nn\\nWi(kajkk)\\nk=1\\n\\n(12)\\n\\nProof. See App. A.2.\\n\\nWe formalize our algorithm as E2 in Fig. 1. Note that we achieve an additional speed-up by exploiting associativity (see App. A.3).\\nTheorem 4. Algorithm E2 computes the secondorder expectation of additively decomposable functions r: T  RR and t: T  RT in:\\n\\nO(N 3 +N 2(R T +AR T )) time O(N 2 +RT +N (R + T )) space\\n\\nwhere R= min(N R , R) and T = min(N T , T ).\\nProof. Correctness of algorithm E2 is given in App. A.3. The runtime bounds are annotated on each line of the algorithm. We note that each r and t is R and T sparse. O(N 2) space is required to store W , O(RT ) is required to store the expectation, and O(N (R + T )) space is required to store the various r and t quantities.\\nPrevious approaches for computing secondorder expectations are significantly slower than E2. Specifically, using Li and Eisner (2009)\\'s secondorder expectation semiring requires augmenting the arc weights to be R \\xd7 T matrices and so runs in O(N 3RT + AN 2RT ). Alternatively, we can\\n8This is commonly used in the case of single transition marginals, which can be found by  log Z\\n\\n\\x0cuse AD, as in \\xa73.2, to materialize the Hessian and compute the pairwise transition marginals.\\nThis would result in a total runtime of O(AN 5+ A2N 4R T ).\\n5 Conclusion\\nWe have presented efficient methods that exploit properties of the derivative of a matrix inverse to find m-order derivatives for WFSMs. Additionally, we provided an explicit, novel, algorithm for materializing the Hessian in its optimal complexity, O(A2N 4). We also showed how this could be utilized to efficiently compute second-order expectations of distributions under WFSMs, such as covariance matrices and the gradient of entropy. We hope that our paper encourages future research to use the Hessian and second-order expectations of WFSM systems, which have previously been disadvantaged by inefficient algorithms.\\nAcknowledgments\\nWe would like to thank the reviewers for engagine with our work and providing valuable feedback. The first author is supported by the University of Cambridge School of Technology ViceChancellor\\'s Scholarship as well as by the University of Cambridge Department of Computer Science and Technology\\'s EPSRC.\\nEthical Concerns\\nWe do not foresee how the more efficient algorithms presented this work exacerbate any existing ethical concerns with NLP systems.\\nReferences\\nW. K. Clifford. 1871. Preliminary sketch of biquaternions. Proceedings of the London Mathematical Society, 1.\\nCorinna Cortes, Mehryar Mohri, Ashish Rastogi, and Michael Riley. 2008. On the computation of the relative entropy of probabilistic automata. International Journal of Foundations of Computer Science, 19.\\nRyan Cotterell, Nanyun Peng, and Jason Eisner. 2015. Modeling word forms using latent underlying morphs and phonology. Transactions of the Association for Computational Linguistics, 3.\\nJason Eisner. 2001. Expectation semirings: Flexible EM for learning finite-state transducers. In Proceedings of the European Summer School in Logic, Language and Information Workshop on Finite-state Methods in Natural Language Processing.\\n\\nAlexander Geyken and Thomas Hanneforth. 2005. TAGH: A complete morphology for German based on weighted finite state automata. In Finite-State Methods and Natural Language Processing, 5th International Workshop.\\nAndreas Griewank. 1989. On automatic differentiation. Mathematical Programming: Recent Developments and Applications, 6.\\nAndreas Griewank and Andrea Walther. 2008. Evaluating Derivatives\\xadPrinciples and Techniques of Algorithmic Differentiation, 2nd edition. Society for Industrial and Applied Mathematics.\\nAwni Hannun, Vineel Pratap, Jacob Kahn, and WeiNing Hsu. 2020. Differentiable weighted finite-state transducers. CoRR, abs/2010.01003.\\nJohn E. Hopcroft, Rajeev Motwani, and Jeffrey D. Ullman. 2001. Introduction to automata theory, languages, and computation, 2nd Edition. AddisonWesley series in computer science. Addison-WesleyLongman.\\nStephen C. Kleene. 1956. Representation of events in nerve nets and finite automata. Automata Studies.\\nKevin Knight and Jonathan Graehl. 1998. Machine transliteration. Computational Linguistics, 24.\\nJohn D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning.\\nDaniel J. Lehmann. 1977. Algebraic structures for transitive closure. Theoretical Computer Science, 4.\\nZhifei Li and Jason Eisner. 2009. First- and secondorder expectation semirings with applications to minimum-risk training on translation forests. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.\\nKrister Linde\\xb4n, Miikka Silfverberg, and Tommi A. Pirinen. 2009. HFST tools for morphology - an efficient open-source package for construction of morphological analyzers. In Proceedings of the State of the Art in Computational Morphology - Workshop on Systems and Frameworks for Computational Morphology.\\nMehryar Mohri. 1997. Finite-state transducers in language and speech processing. Computational Linguistics, 23.\\nMehryar Mohri, Fernando Pereira, and Michael Riley. 2002. Weighted finite-state transducers in speech recognition. Computer Speech and Language, 16.\\nMehryar Mohri, Fernando C. N. Pereira, and Michael Riley. 2000. The design principles of a weighted finite-state transducer library. Theoretical Computer Science, 231.\\n\\n\\x0cBarak A. Pearlmutter and Jeffrey Mark Siskind. 2007. Lazy multivariate higher-order forward-mode AD. In Proceedings of the 34th Association for Computer Machinery Special Interest Group on Programming Languages and Special Interest Group on Algorithms and Computation Theory Symposium on Principles of Programming Languages.\\nK. B. Petersen and M. S. Pedersen. 2008. The matrix cookbook. Version 20081110.\\nLawrence R. Rabiner. 1989. A tutorial on hidden Markov models and selected applications in speech recognition. Proceedings of the Institute of Electrical and Electronics Engineers, 77.\\nPushpendre Rastogi, Ryan Cotterell, and Jason Eisner. 2016. Weighting finite-state transductions with neural context. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics.\\nJoan-Andreu Sa\\xb4nchez and Vero\\xb4nica Romero. 2020. Computation of moments for probabilistic finitestate automata. Information Sciences, 516.\\nRoy Schwartz, Sam Thomson, and Noah A. Smith. 2018. Bridging CNNs, RNNs, and weighted finitestate machines. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, volume 1.\\nRan Zmigrod, Tim Vieira, and Ryan Cotterell. 2021. Efficient computation of expectations under spanning tree distributions. Transactions of the Association for Computational Linguistics.\\n\\n\\x0cA Proofs\\n\\nA.1 Theorem 2. For m  1 and m-tuple of transitions  = i1 -a1 j1, . . . , im -a-m  jm\\n\\nmZ =\\nWi(1aj11) . . . Wi(mamjm)\\n\\nsi1 .Wi(1aj11)Wj1i2 W. i(2aj22) \\xb7 \\xb7 \\xb7 Wjm-1im W. i(mamjm) ejm\\n\\ni1-a1 j1,...,im-a-m jm S\\n\\nwhere s =  W , e = W  and S is the multi-set of permutations of  . Proof. We prove this by induction on m. Base Case: m = 1 and  = i -a j :\\n\\n\\n\\n\\n\\nZ Wi(ja)\\n\\n=\\n\\n Wi(ja)\\n\\n\\n\\nN\\nkWkll =\\n\\nN kWkiW. i(ja)Wjll\\n\\nk,l=0\\n\\nk,l=0\\n\\n=\\n\\nsiW. i(ja)ej\\n\\nInductive Step: Assume that the expression holds for m. Let  = i1 -a1 j1, . . . , im -a-m  jm and consider the tuple  , the concatenation of (i -a j) and  .\\n\\nm+1Z Wi(ja)Wi(1aj11) . . . Wi(mamjm)\\n\\n =\\nWi(ja)\\n\\nsi1 W. i(1aj11)Wj1i2 \\xb7 \\xb7 \\xb7 W. i(mamjm) ejm\\n\\ni1-a1 j1,...,im-a-m jm S\\n\\nConsider the derivative of each summand with respect to Wi(ja). By the product rule, we have\\n\\n Wi(ja)\\n\\nsi1 W. i(1aj11)Wj1i2 \\xb7 \\xb7 \\xb7 W. i(mamjm) ejm\\n\\n=\\n\\nsi \\xb7\\xb7 \\xb7\\xb7\\n\\nW \\xb7\\xb7.++i(jass)iiW11 W\\xb7.j\\xb7ii(\\xb711aWWj1.1)Wji(k1aji1j1W)1.Wi2i(ja\\xb7j)1\\xb7Wi\\xb72W\\xb7j.i\\xb7ki(\\xb7m +aWm1j.m)\\xb7i(\\xb7maW\\xb7mjemj)jmm eij+Wm. +i(ja)\\n\\nej\\n\\nThe above expression is equal to inserting i -a j in every spot of the induction hypothesis\\'s permutation, thereby creating a permutation over  . Reassembling with the expression for the derivative,\\n\\n\\n\\nWi(ja)\\n\\n\\n\\nm+1 Wi(1aj11) .\\n\\nZ =\\n. . Wi(mamjm) i1-a1 j1,...,im+1\\n\\nsi1 W. i(1aj11)Wj1\\n-a-m-+1 jm+1 )S\\n\\ni2\\n\\nW. i(2aj22)\\n\\n\\xb7\\n\\n\\xb7\\n\\n\\xb7\\n\\n.W(am+1) im+1 jm+1\\n\\nejm+1\\n\\nA.2 Lemma 2. For m  1 and m-tuple of transitions  = i1 -a1 j1, . . . , im -a-m  jm\\n\\np( )\\n\\n=\\n\\n1m Z\\nn=1\\n\\nnZ Wi(1aj11) . . . Wi(nanjn)\\n\\nn\\nWi(kajkk)\\nk=1\\n\\n(10)\\n\\n\\x0cProof. Let T be the set of trajectories such that i 1\\np( ) = Z\\ni\\n\\n T    i\\nw(i )\\nT\\n\\n. Then,\\n\\nWe prove the lemma by induction on m. Base Case: Then, m = 1 and  = i1 -a1 j1 . We have that\\n\\n\\n\\n1 Z Z Wi(1aj11)\\n\\nWi(1aj11)\\n\\n=\\n\\n1 Z Wi(1aj11)\\n\\n\\ni\\n\\nw(i\\nT\\n\\n\\n\\n\\n\\n) Wi(1aj11)\\n\\n(=a)\\n\\n1 Z\\n\\n\\n\\ni\\n\\nw(i\\nT\\n\\n ) = p(i1 -a1 j1)\\n\\nStep (a) holds because taking the derivative of Z with respect to Wi(1aj11) yields the sum of the weights all trajectories which include i1 -a1 j1 where we exclude Wi(1aj11) from the computation of the weight.\\nThen, we can push the outer Wi(1aj11) into the equation to obtain the sum of the weights of all trajectories containing i1 -a1 j1. Inductive Step: Suppose that (10) holds for any m-tuple. Let  = i1 -a1 j1, . . . , im+1 -a-m-+1 jm+1 . Without loss of generality, fix i1 -a1 j1 and let  be  without i1 -a1 j1.\\n\\n1 m+1\\n\\nnZ\\n\\nZ n=1 Wi(1aj11) . . . Wi(nanjn)\\n\\nn\\nWi(kajkk)\\nk=1\\n\\n(=b)\\n\\nWi(1aj11)\\n\\n  Wi(1aj11)\\n\\n1 m+1\\n\\n(n-1)Z\\n\\nZ n=2 Wi(2aj22) . . . Wi(nanjn)\\n\\nn\\nWi(kajkk)\\nk=2\\n\\nInductive hypothesis\\n\\n\\n\\n\\n\\n(c)\\n=\\n\\nWi(1aj11)\\n\\n\\n\\n Wi(1aj11)\\n\\n1 Z\\n\\ni\\n\\nw(i\\nT\\n\\n)\\n\\n\\n\\n\\n\\n(d) 1 \\n\\n=\\n\\nZ\\n\\n Wi(1aj11)\\n\\n\\ni\\n\\nw(i\\nT\\n\\n) Wi(1aj11)\\n\\n(e)\\n= p( )\\n\\nStep\\n\\n(b)\\n\\npushes\\n\\n1 Z\\n\\nand\\n\\nn k=2\\n\\nWi(kajkk)\\n\\nas\\n\\nconstants\\n\\ninto\\n\\nthe\\n\\nderivative\\n\\nand\\n\\nstep\\n\\n(c)\\n\\nuses\\n\\nour\\n\\ninduction\\n\\nhypothesis\\n\\non\\n\\n\\n\\n.\\n\\nThen,\\n\\nstep\\n\\n(d)\\n\\ntakes\\n\\n1 Z\\n\\nout\\n\\nof\\n\\nthe\\n\\nderivative\\n\\nas\\n\\nwe\\n\\npushed\\n\\nit\\n\\nin\\n\\nas\\n\\na\\n\\nconstant.\\n\\nFinally,\\n\\nstep (e) follows by the same reasoning as step (a) in the base case above.\\n\\nA.3\\nTheorem 4. Algorithm E2 computes the second-order expectation of additively decomposable functions r: T  RR and t: T  RT in:\\nO(N 3 +N 2(R T +AR T )) time O(N 2 +RT +N (R + T )) space\\n\\nwhere R= min(N R , R) and T = min(N T , T ). Proof. We provide a proof of correctness (the time and space bounds are discussed in the main paper). Zmigrod et al. (2021) show that we can find second-order expectations over by finding the expectations over pairs of transitions. That is,\\n\\nN\\nEi r(i )t(i ) =\\n\\np i -a j, k -b l ri(ja)t(kbl)\\n\\ni,j,k,l=0 a,bA\\n\\n\\x0cWe can use Lemma 2 for the m = 2 case, to find that the expectation is given by\\n\\nEi r(i )t(i )\\n\\n1 =\\nZ\\n\\nN i,j=0\\n\\naA\\n\\nZ Wi(ja)\\n\\nWi(ja)ri(ja)t(ija)\\n\\nN\\n+\\ni,j,k,l=0\\n\\na,bA\\n\\n2Z Wi(ja)  Wk(bl)\\n\\nWi(ja)Wk(bl)\\n\\nri(ja)t(kbl)\\n\\nThe first summand can be rewritten as\\n\\nN i,j=0\\n\\naA\\n\\n\\n\\nZ Wi(ja)\\n\\nWi(ja)ri(ja)t(ija)\\n\\nN\\n=\\n\\nsiW. i(ja)ej Wi(ja)ri(ja)t(ija)\\n\\ni,j=0 aA\\n\\nThe second summand can be rewritten as\\n\\nN i,j,k,l=0\\n\\n2Z a,bA  Wi(ja)  Wk(bl)\\n\\nWi(ja)Wk(bl)ri(ja)t(kbl)\\n\\nN\\n=\\n\\nsiW. i(ja)WjkW. k(bl)elWi(ja)Wk(bl)ri(ja)t(kbl)\\n\\ni,j,k,l=0 a,bA\\n\\n+ skW. k(bl)WliW. i(ja)ej Wi(ja)Wk(bl)ri(ja)t(kbl)\\n\\nConsider the first summand of the above expression\\n\\nN\\n\\nsiW. i(ja)WjkW. k(bl)elWi(ja)Wk(bl)ri(ja)t(kbl)\\n\\ni,j,k,l=0 a,bA\\n\\n\\nNN\\n=\\n\\n\\nsiW. i(ja)Wi(ja)ri(ja)Wjk N\\n\\n\\nW. k(bl)elWk(bl)t(kbl)\\n\\nj,k=0 i=0aA\\n\\nl=0 bA\\n\\nN\\n= rjsWjktek\\nj,k=0\\n\\nd=ef rjs\\n\\nd=ef tek\\n\\nSimilarly, the second summand can be written as\\n\\nN\\nrke Wj k tsj\\nj,k=0\\n\\nFinally, recomposing all the pieces together,\\n\\nEi\\n\\nr(i )t(i )\\n\\n1 =\\nZ\\n\\nN\\nrisWij tej + rjeWij tsi +\\n\\nsiW. i(ja)ej Wi(ja)ri(ja)t(ija)\\n\\ni,j=0\\n\\naA\\n\\n\\x0c'\n"
     ]
    }
   ],
   "source": [
    "txt_list = glob.glob(\"*.txt\")\n",
    "#print(txt_list)\n",
    "file = txt_list[0]\n",
    "with open(file, 'rb') as f:\n",
    "  contents = f.read()\n",
    "print(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Higher-order Derivatives of Weighted Finite-state Machines\\nRan Zmigrod Tim Vieira Ryan Cotterell , University of Cambridge Johns Hopkins University ETH Zu\\xa8rich\\nrz279@cam.ac.uk tim.f.vieira@gmail.com ryan.cotterell@inf.ethz.ch\\n\\narXiv:2106.00749v1 [cs.CL] 1 Jun 2021\\n\\nAbstract\\nWeighted finite-state machines are a fundamental building block of NLP systems. They have withstood the test of time--from their early use in noisy channel models in the 1990s up to modern-day neurally parameterized conditional random fields. This work examines the computation of higher-order derivatives with respect to the normalization constant for weighted finite-state machines. We provide a general algorithm for evaluating derivatives of all orders, which has not been previously described in the literature. In the case of second-order derivatives, our scheme runs in the optimal O(A2N 4) time where A is the alphabet size and N is the number of states. Our algorithm is significantly faster than prior algorithms. Additionally, our approach leads to a significantly faster algorithm for computing second-order expectations, such as covariance matrices and gradients of first-order expectations.\\n1 Introduction\\nWeighted finite-state machines (WFSMs) have a storied role in NLP. They are a useful formalism for speech recognition (Mohri et al., 2002), machine transliteration (Knight and Graehl, 1998), morphology (Geyken and Hanneforth, 2005; Linde\\xb4n et al., 2009) and phonology (Cotterell et al., 2015) inter alia. Indeed, WFSMs have been \"neuralized\" (Rastogi et al., 2016; Hannun et al., 2020; Schwartz et al., 2018) and are still of practical use to the NLP modeler. Moreover, many popular sequence models, e.g., conditional random fields for part-ofspeech tagging (Lafferty et al., 2001), are naturally viewed as special cases of WFSMs. For this reason, we consider the study of algorithms for the WFSMs of interest in se for the NLP community.\\nThis paper considers inference algorithms for WSFMs. When WFSMs are acyclic, there exist\\n\\nsimple linear-time dynamic programs, e.g., the forward algorithm (Rabiner, 1989), for inference. However, in general, WFSMs may contain cycles and such approaches are not applicable. Our work considers this general case and provides a method for efficient computation of mth-order derivatives over a cyclic WFSM. To the best of our knowledge, no algorithm for higher-order derivatives has been presented in the literature beyond a generalpurpose method from automatic differentiation. In contrast to many presentations of WFSMs (Mohri, 1997), our work provides a purely linear-algebraic take on them. And, indeed, it is this connection that allows us to develop our general algorithm.\\nWe provide a thorough analysis of the soundness, runtime, and space complexity of our algorithm. In the special case of second-order derivatives, our algorithm runs optimally in O(A2N 4) time and space where A is the size of the alphabet, and N is the number of states. In contrast, the second-order expectation semiring of Li and Eisner (2009) provides an O(A2N 7) solution and automatic differentiation (Griewank, 1989) yields a slightly faster O(AN 5+A2N 4) solution. Additionally, we provide a speed-up for the general family of second-order expectations. Indeed, we believe our algorithm is the fastest known for computing common quantities, e.g., a covariance matrix.1\\n2 Weighted Finite-State Machines\\nIn this section we briefly provide important notation for WFSMs and a classic result that efficiently finds the normalization constant for the probability distribution of a WFSM.\\nDefinition 1. A weighted finite-state machine M is a tuple , {W(a)}aA,  where A is an al-\\n1Due to space constraints, we keep the discussion of our paper theoretical, though applications of expectations that we can compute are discussed in Li and Eisner (2009), Sa\\xb4nchez and Romero (2020), and Zmigrod et al. (2021).\\n\\n\\x0cphabet of size A, A d=ef A  {}, each a  A has a symbol-specific transition matrix W(a)  R0N\\xd7N where N is the number of states, and ,   R0N are column vectors of start and end weights, respectively. We define the matrix W d=ef aA W(a).\\nDefinition 2. A trajectory i is an ordered sequence of transitions from state i to state . Visually,\\nwe can represent a trajectory by\\n\\ni d=ef i -a j \\xb7 \\xb7 \\xb7 k -a\\n\\nThe weight of a trajectory is\\n\\n\\n\\n\\n\\nw(i\\n\\n)\\n\\nd=ef\\n\\ni\\n\\n \\n\\nWj(ak) \\n\\n(1)\\n\\n(j-a k)i\\n\\nWe denote the (possibly infinite) set of trajectories\\nfrom i to by Ti and the set of all trajectories by T d=ef i, [N] Ti .2 Consequently, when we say i  T , we make i and implicit arguments to which Ti we are accessing.\\nWe define the probability of a trajectory i  T ,\\n\\np(i\\n\\n) d=ef w(i Z\\n\\n)\\n\\n(2)\\n\\nwhere\\n\\n\\n\\nZ d=ef \\n\\nWk \\n\\n(3)\\n\\nk=0\\n\\nOf course, p is only well-defined when 0 < Z < .3 Intuitively,  Wk  is the total weight of all trajectories of length k. Thus, Z is the total weight of all possible trajectories as it sums over the total weight for each possible trajectory length.\\n\\nTheorem 1 (Corollary 4.2, Lehmann (1977)).\\n\\n\\n\\nW d=ef Wk = (I - W)-1\\n\\n(4)\\n\\nk=0\\n\\nThus, we can solve the infinite summation that defines W by matrix inversion in O(N 3) time.4\\nCorollary 1.\\n\\nZ= W \\n\\n(5)\\n\\n2|T | is infinite if and only if M is cyclic. 2Another formulation for Z is i T w(i ). 3This requirement is equivalent to W having a spectral radius < 1. 4This solution technique may be extended to closed semir-\\nings (Kleene, 1956; Lehmann, 1977).\\n\\nProof. Follows from (4) in Theorem 1. By Corollary 1, we can find Z in O(N 3 + AN 2).5\\n\\nStrings versus Trajectories. Importantly, WFSMs can be regarded as weighted finite-state acceptors (WFSAs) which accept strings as their input. Each trajectory i  T has a yield (i ) which is the concatenation of the alphabet symbols of the trajectory. The yield of a trajectory ignores any  symbols, a discussion regarding the semantics of  is given in Hopcroft et al. (2001). As we focus on distributions over trajectories, we do not need special considerations for  transitions. We do not consider distributions over yields in this work as such a distribution requires constructing a latent-variable model\\n\\n1\\n\\np() = Z\\n\\nw(i )\\n\\n(6)\\n\\ni T ,\\n\\n(i )=\\n\\nwhere   A and (i ) is the yield of the trajectory. While marginal likelihood can be found efficiently,6 many quantities, such as the entropy of the distribution over yields, are intractable to compute (Cortes et al., 2008).\\n\\n3 Computing the Hessian (and Beyond)\\n\\nIn this section, we explore algorithms for efficiently computing the Hessian matrix 2Z. We briefly describe two inefficient algorithms, which are derived by forward-mode and reverse-mode automatic differentiation. Next, we propose an efficient algorithm which is based on a key differential identity.\\n3.1 An O(A2N 7) Algorithm with Forward-Mode Automatic Differentiation\\nOne proposal for computing the Hessian comes from Li and Eisner (2009) who introduce a method based on semirings for computing a general family of quantities known as second-order expectations (defined formally in \\xa74). When applied to the computation of the Hessian their method reduces precisely to forward-mode automatic differentiation (AD; Griewank and Walther, 2008, Chap 3.1). This\\n5Throughout this paper, we assume a dense weight matrix and that matrix inversion is O(N 3) time. We note, however, that when the weight matrix is sparse and structured, faster matrix-inversion algorithms exist that exploit the strongly connected components decomposition of the graph (Mohri et al., 2000). We are agnostic to the specific inversion algorithm, but for simplicity we assume the aforementioned running time.\\n6This is done by intersecting the WFSA with another WFSA that only accepts .\\n\\n\\x0capproach requires that we \"lift\" the computation of Z to operate over a richer numeric representation\\nknown as dual numbers (Clifford, 1871; Pearlmut-\\nter and Siskind, 2007). Unfortunately, the second-\\norder dual numbers that we require to compute the Hessian introduce an overhead of O(A2N 4) per numeric operation of the O(N 3) algorithm that computes Z, which results in O(A2N 7) time.\\n\\n3.2 An O(AN 5 +A2N 4) Algorithm with\\nReverse-Mode Automatic Differentiation\\nAnother method for materializing the Hessian 2Z is through reverse-mode automatic differ-\\nentiation (AD). Recall that we can compute Z in O(N 3 + AN 2), and can consequently find Z in O(N 3 + AN 2) using one pass of reverse-\\nmode AD (Griewank and Walther, 2008, Chap-\\nter 3.3). We can repeat differentiation to materialize 2Z. Specifically, we run reverse-mode AD\\nonce for each element i of Z. Taking the gra-\\ndient of (Z)i gives a row of the Hessian matrix, [(Z)i] = [2Z](i,:). Since each of these passes takes time O(N 3+AN 2) (i.e., the same as the cost of Z), and Z has size AN 2, the overall time is O(AN 5 +A2N 4).\\n\\n3.3 Our Optimal O(A2N 4) Algorithm\\nIn this section, we will provide an O(A2N 4)-time and space algorithm for computing the Hessian. Since the Hessian has size O(A2N 4), no algorithm can run faster than this bound; thus, our algorithm\\'s time and space complexities are optimal. Our algorithm hinges on the following lemma, which shows that the each of partial derivatives of W can be cheaply computed given W .\\nLemma 1. For i, j, k,  [N ] and a  A\\n\\nWi Wj(ak)\\n\\n=\\n\\nWij W. j(ak)Wk\\n\\n(7)\\n\\nwhere W. j(ak) is shorthand for Wj(ak).\\nProof.\\n\\nWi Wj(ak)\\n\\n=\\n\\n Wj(ak)\\n\\n(I - W)-i 1\\n\\n = -Wij Wj(ak) [(I - W)] Wk\\n= Wij W. j(ak)Wk\\n\\nThe second step uses Equation 40 of the Matrix Cookbook (Petersen and Pedersen, 2008).\\n\\nWe now extend Lemma 1 to express higher-\\n\\noinrdLeermdmeraiv1a,tiwveeswinillteursme sW.oi(fja)Was\\n\\n. a\\n\\nNote that as shorthand for\\n\\nthe partial derivative Wi(ja).\\n\\nTheorem 2. For m  1 and m-tuple of transitions  = i1 -a1 j1, . . . , im -a-m  jm\\n\\nmZ\\n\\n= Wi(1aj11) \\xb7 \\xb7 \\xb7 Wi(mamjm)\\n\\n(8)\\n\\ni1-a1 j1,\\xb7\\xb7\\xb7 ,im-a-m jm S\\n\\nsi1 W. i(1aj11)Wj1i2 W. i(2aj22) \\xb7 \\xb7 \\xb7 Wjm-1im W. i(mamjm) ejm\\n\\nwhere s =  W , e = W  and S is the multiset of permutations of  .7 Proof. See App. A.1\\nCorollary 2. For i, j, k, l  [N ] and a, b  A\\n\\n2Z\\n\\nWi(ja)Wk(bl) =\\n\\n(9)\\n\\nsiW. i(ja)WjkW. k(bl)el + skW. k(bl)WliW. i(ja)ej\\n\\nProof. Application of Theorem 2 for the m=2 case.\\n\\nTheorem 2 shows that, if we have already computed W , each element of the mth derivative can be found in O(m m!) time: We must sum over O(m!) permutations, where each summand is the product of O(m) items. Importantly, for the Hessian (m = 2), we can find each element in O(1) using Corollary 2. Algorithm Dm in Fig. 1 provides pseudocode for materializing the tensor containing the mth derivatives of Z.\\nTheorem 3. For m  1, algorithm Dm computes mZ in O(N 3 + m m! AmN 2m) time and O(AmN 2m) space.\\nProof. Correctness of algorithm Dm follows from Theorem 2. The runtime and space bounds follow by needing to compute and store each combination of transitions. Each line of the algorithm is annotated with its running time.\\nCorollary 3. The Hessian 2Z can be materialized in O(A2N 4) time and O(A2N 4) space. Note that these bounds are optimal.\\nProof. Application of Theorem 3 for the m=2 case.\\n\\n7As  may have duplicates, S can also have duplicates and so must be a multi-set.\\n\\n\\x0c1: def Dm(W, , ) :\\n\\n2:\\n\\nCompute the tensor of mth-order derivative of a\\n\\nWFSM; requires O(N 3 + m m! AmN 2m) time,\\n\\nO(AmN 2m) space.\\n\\n3: W  (I - W)-1\\n\\nO(N 3)\\n\\n4: s   W ; e  W \\n\\nO(N 2)\\n\\n5: D  0\\n\\n6: for   ([N ]\\xd7[N ]\\xd7A)m : O(mm!AmN2m)\\n\\n7: 8:\\n\\nforDi1+-=a1\\xb7s\\xb7ij\\xb711W W,..ij(.1am.j1-1,)i1Wmimj-W 1a-.im 2i(Wma.mjji(mm2)aj2e2)jWm jS2i3 :\\n\\n9: return D\\n\\n10: def E2(W, , , r, t) :\\n\\n11:\\n\\nCompute the second-order expectation of a\\n\\nWFSM; requires O(N 3 + N 2(R T + AR T ))\\n\\ntime, O(N 2 + RT + N (R + T )) space where\\n\\nR d=ef min(N R , R) and T d=ef min(N T , T ).\\n12: Compute W , s, and e as in Dm O(N3)\\n\\n13: Z   W \\n\\n14: rs  0; re  0; ts  0; te  0\\n\\n15: 16: 17: 18: 19:\\n\\nforrrttsieiiisei,++++j====sWsW.j.j[WWNi(i(..jaja)a()]j(ei,ejai)jaj)WWWW. j(i(j(aj(ijaaAiai))))ttrW(j(i:aji(jaai))i()ja) ri(ja)\\n\\nO(AN 2) O(R ) O(R ) O(T ) O(T )\\n\\n20:\\n\\nreturn\\n\\n1 Z\\n\\nN i,j=0\\n\\nris\\n\\nWij\\n\\ntej\\n\\n+ tsi Wij rje\\n\\n+ aA siW. i(ja)ej Wi(ja)ri(ja)t(ija)\\n\\nO(N 2(R T +AR T ))\\n\\nFigure 1: Algorithms\\n4 Second-Order Expectations\\n\\nIn this section, we leverage the algorithms of the previous section to efficiently compute a family expectations, known as a second-order expectations. To begin, we define an additively decomposable function r: T  RR as any function expressed as\\n\\nr(i ) =\\n\\nrj(ak)\\n\\n(10)\\n\\n(j-a k)i\\n\\nwhere each rj(ak) is an R-dimensional vector. Since many r of interest are sparse, we analyze our al-\\n\\ngorithms in terms of R and its maximum den-\\n\\nsity R\\n\\nd=ef\\n\\nmax\\nj\\n\\n-a k\\n\\nrj(ak) 0. Previous work has\\n\\nconsidered expectations of such functions (Eisner,\\n\\n2001) and the product of two such functions (Li\\n\\nand Eisner, 2009), better known as second-order ex-\\n\\npectations. Formally, given two additively decom-\\n\\nposable functions r: T  RR and t: T  RT , a second-order expectation is\\n\\nEi r(i )t(i ) d=ef\\n\\n(11)\\n\\np(i )r(i )t(i )\\ni T\\n\\nExamples of second-order expectations include the Fisher information matrix and the gradients of firstorder expectations (e.g., expected cost, entropy, and the Kullback\\xadLeibler divergence).\\nOur algorithm is based on two fundamental concepts. Firstly, expectations for probability distributions as described in (1), can be decomposed as expectations over transitions (Zmigrod et al., 2021). Secondly, the marginal probabilities of transitions are connected to derivatives of Z.8\\nLemma 2. For m  1 and m-tuple of transitions  = i1 -a1 j1, . . . , im -a-m  jm\\n\\np( )\\n\\n=\\n\\n1 Z\\n\\nm n=1\\n\\n\\n\\n Wi(1aj11) .\\n\\nnZ ..\\n\\nWi(nanjn)\\n\\nn\\nWi(kajkk)\\nk=1\\n\\n(12)\\n\\nProof. See App. A.2.\\n\\nWe formalize our algorithm as E2 in Fig. 1. Note that we achieve an additional speed-up by exploiting associativity (see App. A.3).\\nTheorem 4. Algorithm E2 computes the secondorder expectation of additively decomposable functions r: T  RR and t: T  RT in:\\n\\nO(N 3 +N 2(R T +AR T )) time O(N 2 +RT +N (R + T )) space\\n\\nwhere R= min(N R , R) and T = min(N T , T ).\\nProof. Correctness of algorithm E2 is given in App. A.3. The runtime bounds are annotated on each line of the algorithm. We note that each r and t is R and T sparse. O(N 2) space is required to store W , O(RT ) is required to store the expectation, and O(N (R + T )) space is required to store the various r and t quantities.\\nPrevious approaches for computing secondorder expectations are significantly slower than E2. Specifically, using Li and Eisner (2009)\\'s secondorder expectation semiring requires augmenting the arc weights to be R \\xd7 T matrices and so runs in O(N 3RT + AN 2RT ). Alternatively, we can\\n8This is commonly used in the case of single transition marginals, which can be found by  log Z\\n\\n\\x0cuse AD, as in \\xa73.2, to materialize the Hessian and compute the pairwise transition marginals.\\nThis would result in a total runtime of O(AN 5+ A2N 4R T ).\\n5 Conclusion\\nWe have presented efficient methods that exploit properties of the derivative of a matrix inverse to find m-order derivatives for WFSMs. Additionally, we provided an explicit, novel, algorithm for materializing the Hessian in its optimal complexity, O(A2N 4). We also showed how this could be utilized to efficiently compute second-order expectations of distributions under WFSMs, such as covariance matrices and the gradient of entropy. We hope that our paper encourages future research to use the Hessian and second-order expectations of WFSM systems, which have previously been disadvantaged by inefficient algorithms.\\nAcknowledgments\\nWe would like to thank the reviewers for engagine with our work and providing valuable feedback. The first author is supported by the University of Cambridge School of Technology ViceChancellor\\'s Scholarship as well as by the University of Cambridge Department of Computer Science and Technology\\'s EPSRC.\\nEthical Concerns\\nWe do not foresee how the more efficient algorithms presented this work exacerbate any existing ethical concerns with NLP systems.\\nReferences\\nW. K. Clifford. 1871. Preliminary sketch of biquaternions. Proceedings of the London Mathematical Society, 1.\\nCorinna Cortes, Mehryar Mohri, Ashish Rastogi, and Michael Riley. 2008. On the computation of the relative entropy of probabilistic automata. International Journal of Foundations of Computer Science, 19.\\nRyan Cotterell, Nanyun Peng, and Jason Eisner. 2015. Modeling word forms using latent underlying morphs and phonology. Transactions of the Association for Computational Linguistics, 3.\\nJason Eisner. 2001. Expectation semirings: Flexible EM for learning finite-state transducers. In Proceedings of the European Summer School in Logic, Language and Information Workshop on Finite-state Methods in Natural Language Processing.\\n\\nAlexander Geyken and Thomas Hanneforth. 2005. TAGH: A complete morphology for German based on weighted finite state automata. In Finite-State Methods and Natural Language Processing, 5th International Workshop.\\nAndreas Griewank. 1989. On automatic differentiation. Mathematical Programming: Recent Developments and Applications, 6.\\nAndreas Griewank and Andrea Walther. 2008. Evaluating Derivatives\\xadPrinciples and Techniques of Algorithmic Differentiation, 2nd edition. Society for Industrial and Applied Mathematics.\\nAwni Hannun, Vineel Pratap, Jacob Kahn, and WeiNing Hsu. 2020. Differentiable weighted finite-state transducers. CoRR, abs/2010.01003.\\nJohn E. Hopcroft, Rajeev Motwani, and Jeffrey D. Ullman. 2001. Introduction to automata theory, languages, and computation, 2nd Edition. AddisonWesley series in computer science. Addison-WesleyLongman.\\nStephen C. Kleene. 1956. Representation of events in nerve nets and finite automata. Automata Studies.\\nKevin Knight and Jonathan Graehl. 1998. Machine transliteration. Computational Linguistics, 24.\\nJohn D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning.\\nDaniel J. Lehmann. 1977. Algebraic structures for transitive closure. Theoretical Computer Science, 4.\\nZhifei Li and Jason Eisner. 2009. First- and secondorder expectation semirings with applications to minimum-risk training on translation forests. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.\\nKrister Linde\\xb4n, Miikka Silfverberg, and Tommi A. Pirinen. 2009. HFST tools for morphology - an efficient open-source package for construction of morphological analyzers. In Proceedings of the State of the Art in Computational Morphology - Workshop on Systems and Frameworks for Computational Morphology.\\nMehryar Mohri. 1997. Finite-state transducers in language and speech processing. Computational Linguistics, 23.\\nMehryar Mohri, Fernando Pereira, and Michael Riley. 2002. Weighted finite-state transducers in speech recognition. Computer Speech and Language, 16.\\nMehryar Mohri, Fernando C. N. Pereira, and Michael Riley. 2000. The design principles of a weighted finite-state transducer library. Theoretical Computer Science, 231.\\n\\n\\x0cBarak A. Pearlmutter and Jeffrey Mark Siskind. 2007. Lazy multivariate higher-order forward-mode AD. In Proceedings of the 34th Association for Computer Machinery Special Interest Group on Programming Languages and Special Interest Group on Algorithms and Computation Theory Symposium on Principles of Programming Languages.\\nK. B. Petersen and M. S. Pedersen. 2008. The matrix cookbook. Version 20081110.\\nLawrence R. Rabiner. 1989. A tutorial on hidden Markov models and selected applications in speech recognition. Proceedings of the Institute of Electrical and Electronics Engineers, 77.\\nPushpendre Rastogi, Ryan Cotterell, and Jason Eisner. 2016. Weighting finite-state transductions with neural context. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics.\\nJoan-Andreu Sa\\xb4nchez and Vero\\xb4nica Romero. 2020. Computation of moments for probabilistic finitestate automata. Information Sciences, 516.\\nRoy Schwartz, Sam Thomson, and Noah A. Smith. 2018. Bridging CNNs, RNNs, and weighted finitestate machines. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, volume 1.\\nRan Zmigrod, Tim Vieira, and Ryan Cotterell. 2021. Efficient computation of expectations under spanning tree distributions. Transactions of the Association for Computational Linguistics.\\n\\n\\x0cA Proofs\\n\\nA.1 Theorem 2. For m  1 and m-tuple of transitions  = i1 -a1 j1, . . . , im -a-m  jm\\n\\nmZ =\\nWi(1aj11) . . . Wi(mamjm)\\n\\nsi1 .Wi(1aj11)Wj1i2 W. i(2aj22) \\xb7 \\xb7 \\xb7 Wjm-1im W. i(mamjm) ejm\\n\\ni1-a1 j1,...,im-a-m jm S\\n\\nwhere s =  W , e = W  and S is the multi-set of permutations of  . Proof. We prove this by induction on m. Base Case: m = 1 and  = i -a j :\\n\\n\\n\\n\\n\\nZ Wi(ja)\\n\\n=\\n\\n Wi(ja)\\n\\n\\n\\nN\\nkWkll =\\n\\nN kWkiW. i(ja)Wjll\\n\\nk,l=0\\n\\nk,l=0\\n\\n=\\n\\nsiW. i(ja)ej\\n\\nInductive Step: Assume that the expression holds for m. Let  = i1 -a1 j1, . . . , im -a-m  jm and consider the tuple  , the concatenation of (i -a j) and  .\\n\\nm+1Z Wi(ja)Wi(1aj11) . . . Wi(mamjm)\\n\\n =\\nWi(ja)\\n\\nsi1 W. i(1aj11)Wj1i2 \\xb7 \\xb7 \\xb7 W. i(mamjm) ejm\\n\\ni1-a1 j1,...,im-a-m jm S\\n\\nConsider the derivative of each summand with respect to Wi(ja). By the product rule, we have\\n\\n Wi(ja)\\n\\nsi1 W. i(1aj11)Wj1i2 \\xb7 \\xb7 \\xb7 W. i(mamjm) ejm\\n\\n=\\n\\nsi \\xb7\\xb7 \\xb7\\xb7\\n\\nW \\xb7\\xb7.++i(jass)iiW11 W\\xb7.j\\xb7ii(\\xb711aWWj1.1)Wji(k1aji1j1W)1.Wi2i(ja\\xb7j)1\\xb7Wi\\xb72W\\xb7j.i\\xb7ki(\\xb7m +aWm1j.m)\\xb7i(\\xb7maW\\xb7mjemj)jmm eij+Wm. +i(ja)\\n\\nej\\n\\nThe above expression is equal to inserting i -a j in every spot of the induction hypothesis\\'s permutation, thereby creating a permutation over  . Reassembling with the expression for the derivative,\\n\\n\\n\\nWi(ja)\\n\\n\\n\\nm+1 Wi(1aj11) .\\n\\nZ =\\n. . Wi(mamjm) i1-a1 j1,...,im+1\\n\\nsi1 W. i(1aj11)Wj1\\n-a-m-+1 jm+1 )S\\n\\ni2\\n\\nW. i(2aj22)\\n\\n\\xb7\\n\\n\\xb7\\n\\n\\xb7\\n\\n.W(am+1) im+1 jm+1\\n\\nejm+1\\n\\nA.2 Lemma 2. For m  1 and m-tuple of transitions  = i1 -a1 j1, . . . , im -a-m  jm\\n\\np( )\\n\\n=\\n\\n1m Z\\nn=1\\n\\nnZ Wi(1aj11) . . . Wi(nanjn)\\n\\nn\\nWi(kajkk)\\nk=1\\n\\n(10)\\n\\n\\x0cProof. Let T be the set of trajectories such that i 1\\np( ) = Z\\ni\\n\\n T    i\\nw(i )\\nT\\n\\n. Then,\\n\\nWe prove the lemma by induction on m. Base Case: Then, m = 1 and  = i1 -a1 j1 . We have that\\n\\n\\n\\n1 Z Z Wi(1aj11)\\n\\nWi(1aj11)\\n\\n=\\n\\n1 Z Wi(1aj11)\\n\\n\\ni\\n\\nw(i\\nT\\n\\n\\n\\n\\n\\n) Wi(1aj11)\\n\\n(=a)\\n\\n1 Z\\n\\n\\n\\ni\\n\\nw(i\\nT\\n\\n ) = p(i1 -a1 j1)\\n\\nStep (a) holds because taking the derivative of Z with respect to Wi(1aj11) yields the sum of the weights all trajectories which include i1 -a1 j1 where we exclude Wi(1aj11) from the computation of the weight.\\nThen, we can push the outer Wi(1aj11) into the equation to obtain the sum of the weights of all trajectories containing i1 -a1 j1. Inductive Step: Suppose that (10) holds for any m-tuple. Let  = i1 -a1 j1, . . . , im+1 -a-m-+1 jm+1 . Without loss of generality, fix i1 -a1 j1 and let  be  without i1 -a1 j1.\\n\\n1 m+1\\n\\nnZ\\n\\nZ n=1 Wi(1aj11) . . . Wi(nanjn)\\n\\nn\\nWi(kajkk)\\nk=1\\n\\n(=b)\\n\\nWi(1aj11)\\n\\n  Wi(1aj11)\\n\\n1 m+1\\n\\n(n-1)Z\\n\\nZ n=2 Wi(2aj22) . . . Wi(nanjn)\\n\\nn\\nWi(kajkk)\\nk=2\\n\\nInductive hypothesis\\n\\n\\n\\n\\n\\n(c)\\n=\\n\\nWi(1aj11)\\n\\n\\n\\n Wi(1aj11)\\n\\n1 Z\\n\\ni\\n\\nw(i\\nT\\n\\n)\\n\\n\\n\\n\\n\\n(d) 1 \\n\\n=\\n\\nZ\\n\\n Wi(1aj11)\\n\\n\\ni\\n\\nw(i\\nT\\n\\n) Wi(1aj11)\\n\\n(e)\\n= p( )\\n\\nStep\\n\\n(b)\\n\\npushes\\n\\n1 Z\\n\\nand\\n\\nn k=2\\n\\nWi(kajkk)\\n\\nas\\n\\nconstants\\n\\ninto\\n\\nthe\\n\\nderivative\\n\\nand\\n\\nstep\\n\\n(c)\\n\\nuses\\n\\nour\\n\\ninduction\\n\\nhypothesis\\n\\non\\n\\n\\n\\n.\\n\\nThen,\\n\\nstep\\n\\n(d)\\n\\ntakes\\n\\n1 Z\\n\\nout\\n\\nof\\n\\nthe\\n\\nderivative\\n\\nas\\n\\nwe\\n\\npushed\\n\\nit\\n\\nin\\n\\nas\\n\\na\\n\\nconstant.\\n\\nFinally,\\n\\nstep (e) follows by the same reasoning as step (a) in the base case above.\\n\\nA.3\\nTheorem 4. Algorithm E2 computes the second-order expectation of additively decomposable functions r: T  RR and t: T  RT in:\\nO(N 3 +N 2(R T +AR T )) time O(N 2 +RT +N (R + T )) space\\n\\nwhere R= min(N R , R) and T = min(N T , T ). Proof. We provide a proof of correctness (the time and space bounds are discussed in the main paper). Zmigrod et al. (2021) show that we can find second-order expectations over by finding the expectations over pairs of transitions. That is,\\n\\nN\\nEi r(i )t(i ) =\\n\\np i -a j, k -b l ri(ja)t(kbl)\\n\\ni,j,k,l=0 a,bA\\n\\n\\x0cWe can use Lemma 2 for the m = 2 case, to find that the expectation is given by\\n\\nEi r(i )t(i )\\n\\n1 =\\nZ\\n\\nN i,j=0\\n\\naA\\n\\nZ Wi(ja)\\n\\nWi(ja)ri(ja)t(ija)\\n\\nN\\n+\\ni,j,k,l=0\\n\\na,bA\\n\\n2Z Wi(ja)  Wk(bl)\\n\\nWi(ja)Wk(bl)\\n\\nri(ja)t(kbl)\\n\\nThe first summand can be rewritten as\\n\\nN i,j=0\\n\\naA\\n\\n\\n\\nZ Wi(ja)\\n\\nWi(ja)ri(ja)t(ija)\\n\\nN\\n=\\n\\nsiW. i(ja)ej Wi(ja)ri(ja)t(ija)\\n\\ni,j=0 aA\\n\\nThe second summand can be rewritten as\\n\\nN i,j,k,l=0\\n\\n2Z a,bA  Wi(ja)  Wk(bl)\\n\\nWi(ja)Wk(bl)ri(ja)t(kbl)\\n\\nN\\n=\\n\\nsiW. i(ja)WjkW. k(bl)elWi(ja)Wk(bl)ri(ja)t(kbl)\\n\\ni,j,k,l=0 a,bA\\n\\n+ skW. k(bl)WliW. i(ja)ej Wi(ja)Wk(bl)ri(ja)t(kbl)\\n\\nConsider the first summand of the above expression\\n\\nN\\n\\nsiW. i(ja)WjkW. k(bl)elWi(ja)Wk(bl)ri(ja)t(kbl)\\n\\ni,j,k,l=0 a,bA\\n\\n\\nNN\\n=\\n\\n\\nsiW. i(ja)Wi(ja)ri(ja)Wjk N\\n\\n\\nW. k(bl)elWk(bl)t(kbl)\\n\\nj,k=0 i=0aA\\n\\nl=0 bA\\n\\nN\\n= rjsWjktek\\nj,k=0\\n\\nd=ef rjs\\n\\nd=ef tek\\n\\nSimilarly, the second summand can be written as\\n\\nN\\nrke Wj k tsj\\nj,k=0\\n\\nFinally, recomposing all the pieces together,\\n\\nEi\\n\\nr(i )t(i )\\n\\n1 =\\nZ\\n\\nN\\nrisWij tej + rjeWij tsi +\\n\\nsiW. i(ja)ej Wi(ja)ri(ja)t(ija)\\n\\ni,j=0\\n\\naA\\n\\n\\x0c'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot use a string pattern on a bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-7a3c946f4913>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimpiar_tokenizar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexto\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-7a3c946f4913>\u001b[0m in \u001b[0;36mlimpiar_tokenizar\u001b[0;34m(texto)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mnuevo_texto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtexto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Eliminacin de pginas web (palabras que empiezan por \"http\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mnuevo_texto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'http\\S+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnuevo_texto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Eliminacin de signos de puntuacin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mregex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'[\\\\!\\\\\"\\\\#\\\\$\\\\%\\\\&\\\\\\'\\\\(\\\\)\\\\*\\\\+\\\\,\\\\-\\\\.\\\\/\\\\:\\\\;\\\\<\\\\=\\\\>\\\\?\\\\@\\\\[\\\\\\\\\\\\]\\\\^_\\\\`\\\\{\\\\|\\\\}\\\\~]'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/re.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot use a string pattern on a bytes-like object"
     ]
    }
   ],
   "source": [
    "def limpiar_tokenizar(texto):\n",
    "    '''\n",
    "    Esta funcin limpia y tokeniza el texto en palabras individuales.\n",
    "    El orden en el que se va limpiando el texto no es arbitrario.\n",
    "    El listado de signos de puntuacin se ha obtenido de: print(string.punctuation)\n",
    "    y re.escape(string.punctuation)\n",
    "    '''\n",
    "    \n",
    "    # Se convierte todo el texto a minsculas\n",
    "    nuevo_texto = texto.lower()\n",
    "    # Eliminacin de pginas web (palabras que empiezan por \"http\")\n",
    "    nuevo_texto = re.sub('http\\S+', ' ', nuevo_texto)\n",
    "    # Eliminacin de signos de puntuacin\n",
    "    regex = '[\\\\!\\\\\"\\\\#\\\\$\\\\%\\\\&\\\\\\'\\\\(\\\\)\\\\*\\\\+\\\\,\\\\-\\\\.\\\\/\\\\:\\\\;\\\\<\\\\=\\\\>\\\\?\\\\@\\\\[\\\\\\\\\\\\]\\\\^_\\\\`\\\\{\\\\|\\\\}\\\\~]'\n",
    "    nuevo_texto = re.sub(regex , ' ', nuevo_texto)\n",
    "    # Eliminacin de nmeros\n",
    "    nuevo_texto = re.sub(\"\\d+\", ' ', nuevo_texto)\n",
    "    # Eliminacin de espacios en blanco mltiples\n",
    "    nuevo_texto = re.sub(\"\\\\s+\", ' ', nuevo_texto)\n",
    "    # Tokenizacin por palabras individuales\n",
    "    nuevo_texto = nuevo_texto.split(sep = ' ')\n",
    "    # Eliminacin de tokens con una longitud < 2\n",
    "    nuevo_texto = [token for token in nuevo_texto if len(token) > 1]\n",
    "    \n",
    "    return(nuevo_texto)\n",
    "\n",
    "test = contents\n",
    "print(test)\n",
    "print(limpiar_tokenizar(texto=test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xad in position 1967: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-dd6d8d1fe9c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# archivo-entrada.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'2106.0138_Implication_of_the_swampland_distance_conjecture_on_the_Cohen-Kaplan-Nelson_bound_in_de_Sitter_space.pdf.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmensaje\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmensaje\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xad in position 1967: invalid start byte"
     ]
    }
   ],
   "source": [
    "# archivo-entrada.py\n",
    "f = open ('2106.0138_Implication_of_the_swampland_distance_conjecture_on_the_Cohen-Kaplan-Nelson_bound_in_de_Sitter_space.pdf.txt','r')\n",
    "mensaje = f.read()\n",
    "print(mensaje)\n",
    "f.close()\n",
    "doc = nlp(\"Success in management requires learning as fast as the world is changing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2175f920b6aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Print the document text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'doc' is not defined"
     ]
    }
   ],
   "source": [
    "# Print the document text\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Success',\n",
       " 'in',\n",
       " 'management',\n",
       " 'requires',\n",
       " 'learning',\n",
       " 'as',\n",
       " 'fast',\n",
       " 'as',\n",
       " 'the',\n",
       " 'world',\n",
       " 'is',\n",
       " 'changing']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Token texts\n",
    "[token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOUN',\n",
       " 'ADP',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'VERB',\n",
       " 'ADV',\n",
       " 'ADV',\n",
       " 'ADP',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'AUX',\n",
       " 'VERB']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Coarse-grained part-of-speech tags\n",
    "[token.pos_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['requires',\n",
       " 'Success',\n",
       " 'in',\n",
       " 'requires',\n",
       " 'requires',\n",
       " 'fast',\n",
       " 'learning',\n",
       " 'fast',\n",
       " 'world',\n",
       " 'changing',\n",
       " 'changing',\n",
       " 'requires']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Syntactic head token (governor)\n",
    "[token.head.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Artificial Inteligence', 'PERSON')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"Machine learning, Artificial Inteligence\")\n",
    "# Text and label of named entity span\n",
    "[(ent.text, ent.label_) for ent in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
