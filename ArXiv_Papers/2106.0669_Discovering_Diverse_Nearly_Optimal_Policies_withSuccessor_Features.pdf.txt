arXiv:2106.00669v1 [cs.AI] 1 Jun 2021

Discovering Diverse Nearly Optimal Policies with Successor Features
Tom Zahavy, Brendan O'Donoghue, Andre Barreto, Volodymyr Mnih, Sebastian Flennerhag and Satinder Singh
{tomzahavy,bodonoghue,andrebarreto,vmnih,flennerhag,baveja}@deepmind.com DeepMind, London
Abstract
Finding different solutions to the same problem is a key aspect of intelligence associated with creativity and adaptation to novel situations. In reinforcement learning, a set of diverse policies can be useful for exploration, transfer, hierarchy, and robustness. We propose Diverse Successive Policies, a method for discovering policies that are diverse in the space of Successor Features, while assuring that they are near optimal. We formalize the problem as a Constrained Markov Decision Process (CMDP) where the goal is to find policies that maximize diversity, characterized by an intrinsic diversity reward, while remaining near-optimal with respect to the extrinsic reward of the MDP. We also analyze how recently proposed robustness and discrimination rewards perform and find that they are sensitive to the initialization of the procedure and may converge to sub-optimal solutions. To alleviate this, we propose new explicit diversity rewards that aim to minimize the correlation between the Successor Features of the policies in the set. We compare the different diversity mechanisms in the DeepMind Control Suite and find that the type of explicit diversity we are proposing is important to discover distinct behavior, like for example different locomotion patterns.
1 Introduction
Creative problem solving is the mental process of searching for an original and previously unknown solution to a problem [29]. The relationship between creativity and intelligence is widely recognized across many fields; for example, in the field of Mathematics, finding different proofs to the same theorem is considered elegant and often leads to new insights.
Closer to Artificial Intelligence (AI), consider the field of game playing and specifically the game of Chess in which a move is considered creative when it goes beyond known patterns [14]. In some cases, such moves can only be detected by human players while remaining invisible to currently state-of-the-art Chess engines. A famous example thereof is the winning move in game eight of the Classical World Chess Championship 2004 between Leko and Kramnik [6]. Humans and indeed many animals employ similarly creative behavior on a daily basis; faced with a challenging problem we often consider qualitatively different alternative solutions.
Yet, the majority of AI research is focused on finding a single best solution to a given problem. For example, in the field of Reinforcement Learning (RL), most algorithms are designed to find a single reward-maximizing policy. However, for many problems of interest there may be many qualitatively different optimal or near-optimal policies; finding such diverse set of policies may help an RL agent become more robust to changes in the task and/or environment and to generalize better to future tasks.
In the field of Quality-Diversity (QD), evolutionary algorithms are used to find useful diverse policies (e.g., [33, 28, 21, 25, 30, 18, 31, 48]). In a related line of work, intrinsic rewards are used to find diverse skills for fast adaptation [19, 16] to be robust to model miss-specification [24, 45] and for exploration [2]. It was also suggested that policies that maximize diversity are more correlated with human behaviour than those that maximize only the extrinsic reward [26].
Preprint. Under review.

This work makes the following contributions. First, we propose an incremental method for discovering a diverse set of near-optimal policies. Each policy in the set is trained to solve a Constrained Markov Decision Process (CMDP). The main objective in the CMDP is to maximize the diversity of the growing set, measured in the space of Successor Features [SFs; 4], and the constraint is that the policies are near-optimal. Second, we analyze how previously proposed robustness and discrimination mechanisms for the "no-reward" setting perform in terms of diversity in our setup. We find that they are sensitive to the initialization of the procedure and may converge to sub-optimal solutions. To alleviate this, we propose two explicit diversity rewards that aim to minimize the correlation between the SFs of the policies in the set. Third, we demonstrate our method in the DeepMind Control Suite [40]. Given an extrinsic reward (e.g. for standing or walking) our method discovers qualitatively diverse locomotion behaviours for approximately maximizing this reward.

2 Preliminaries and Notation

An MDP [34] is a tuple M (S, A, P, r, , ), where S is the set of states, A is the set of actions, P = {P a | a  A} is the set of transition kernels,   [0, 1) is the discount factor and  is the initial state distribution. The function r : S × A  R defines the rewards. A policy in M , denoted by , is a mapping  : S  P(A), where P(A) is the probability distributions over A.
Usually in RL, the agent's objective is to maximize the expected cumulative extrinsic reward. In this work, we will also be interested in discovering and maximizing intrinsic reward functions [35]. These rewards can be a function of the policy (e.g., its entropy) or a function of observed features. Let (s, a)  [0, 1]d be an observable vector of bounded features. Then there is a set of rewards induced by all possible linear combinations of the features . Specifically, for any w  Rd, we can define a reward function rw(s, a) = w · (s, a). Given w, the intrinsic reward rw is well defined and we will use w and rw interchangeably to refer to it. Any policy induces a state transition matrix P , where P (x, y) = P (x)(x, y) is the probability of transitioning from state x to state y when the action is selected according to (x). Thus, any policy yields a Markov chain (S, P ). By looking at the Markov chain induced by a policy we can study its long-term behavior, such as its stationary distribution. This in turn allows us to define a notion of diversity based on the limiting behavior of policies, in contrast with most previous work on diversity that focus on short-term behavior [19, 16].
Concretely, in defining diversity we use measures defined in the average-case setting. The stationary distribution d of a Markov chain with transition matrix P  is defined to be d(s) = limt Pr(st = s|s0  , ), which we assume exists and is independent of s0 for all policies. In ergodic MDPs this limit is unique and is known to be the probability distribution satisfying d = d P  [34]. The asymptotic average reward value, hereafter simply value, of a policy  under reward function r, denoted vr, can be defined as an expectation over d as: vr = Esd r(s, (s)) = d · r, where r is a vector with Ea(s)r(s, a) in its coordinates. A natural time scale in this long-term average-case context is the mixing time of the policy ­ the time until the Markov chain is "close" to its stationary state distribution. Formally, the -mixing time Tmix of an ergodic Markov chain with a stationary distribution d is the smallest time t such that x0, TV[Prt(·|x0), d]  , where Prt(·|x0) is the distribution over states after t steps, starting from x0 and TV[·, ·] is the total variation distance. In other words, if we follow a policy in an MDP for Tmix steps, we will observe states that are approximately distributed according to d.
Similarly, we can define the expected features, also known as successor features, under d as  = Exd (x, (x)). Note that the SFs are conditioned on  and  and that they are vectors in Rd; similar definitions were suggested in [27, 44]. For linear rewards there is a simple way to express the average reward value of the policy (Section 2) using the SFs: vw =  · w. To keep the notation simple, we will refer to the SFs of policy i as i; and, since we are dealing with different intrinsic rewards, we will use the notation vdi to refer to the value of policy i for reward rd.

3 Discovering diverse near-optimal policies

We are diverse

interested according

in to

discovering a set of n some diversity metric.

nLeeatr-onptbime tahlepsoelticoifesSFsnc=orr{espi}onin=d1intghattoatrheempoalxicimieaslilny

n, then we are interested in solving the following constrained optimization problem:

max Diversity(n) s.t
n

ve  ve,   n,

(1)

where Diversity : {Rd}n  R measures the diversity of a set of SFs (n) that we shall define shortly, and the constraint requires that all the policies in n achieve value better than a parameter   [0, 1]

2

times the value of the optimal policy (here ve is the value of policy  for extrinsic reward re and ve is the value of the optimal policy with respect to re). Note that  controls how big a space of policies we search over for our diverse set of policies. In general, the smaller the  parameter the larger the
set of -optimal policies and thus the greater the diversity of the policies found in n1.

Common to many approaches is to define a diversity objective using in-

Algorithm 1 Diverse Successive Policies

trinsic rewards [19, 16, 24, 47], i.e., 1: Input: mechanism to compute rewards re and rd.

rewards not from the environment but 2: Initialize: 0  arg max re · d,

defined by the agent itself. Our ap-
proach also uses intrinsic rewards to
induce diversity, as we describe in
Algorithm 1. The algorithm receives as input two reward functions re and rd, which together define a CMDP. The reward rd corresponds to a diver-

3: ve = v0 , 0 = {0} 4: for i = 1, . . . , T do 5: Compute diversity reward rdi = D(i-1) 6: i = arg max d · rdi s.t. d · re  ve 7: Estimate the SFs i of the policy i 8: i = i-1  {i}, i = i-1  {i}

sity intrinsic reward. We will discuss 9: end for five different candidate rd's. The con- 10: return T

straint reward re will typically be the

extrinsic reward, but we will also consider two alternative choices for re. In the initialization step of

Algorithm 1 (line 2) there are no policies in the set, and so the goal of the first policy 0 is to solve

the MDP with reward re. Algorithm 1 then adds 0 and its SFs to the set, and the variable ve is set to be v0. ve defines the near-optimality constraint ve for the other policies (say with  = 0.9).

After this first step, the algorithm proceeds in iterations. In iteration i, an intrinsic reward rdi is computed given the previous policies in the set i-1. The next policy to be added to the set, i, is the solution to the following Constrained MDP (CMDP) (line 6 in Algorithm 1):

arg max


d · rdi

s.t.

d · re  ve.

(2)

In words, the new policy optimizes the average intrinsic reward value subject to the constraint that it
be near-optimal with respect to its average extrinsic reward value. In Section 5 we discuss the details of how to solve Eq. (2). Clearly, the behavior of Algorithm 1 strongly depends on the choice of rd, the intrinsic reward used to induce diversity. We now discuss five alternatives to define this reward.

4 Measuring Policy Diversity
A key aspect of our method is the measure of diversity. Our focus is on diverse policies, as measured by their stationary distribution after they have mixed. This suggests we should measure diversity in the space of SFs, as they are defined under the policy's stationary distribution (see Section 2). In contrast, prior work have focused on learning diverse skills, which is often measured before the skill policy mixes. A common approach to measuring skill diversity is to measure skill discrimination in terms of trajectory-specific quantities such as terminal states [19], a mixture of the initial and terminal states [5], or trajectories [16]. An alternative approach that implicitly induces diversity is to learn policies that maximize the robustness of the set n to the worst-possible reward [24, 47].
In Subsections 4.1 and 4.2, we analyze the diversity of these two approaches in the space of SFs and find that they both depend on the initialization of the algorithm and cannot guarantee diversity. Motivated by these findings, we develop two new explicit diversity rewards that aim to minimize the correlation between the SFs of the policies in the set. We discuss these new methods in Section 4.3.
4.1 Diversity via Discrimination
Discriminative approaches rely on the intuition that skills should be distinguishable from one another simply by observing the states that they visit. Learning diverse skills is then a matter of learning skills that can be easily discriminated. For instance, DIAYN [16] maximizes the mutual information between skills and states as follows. Given a probability space (, F, P), we denote by I(S; Z) the mutual information between the random variable state S :   S and latent random variable (skill) Z :   Z [13]. We also use H[A|S] to refer to the conditional entropy of the action random variable A :   A conditioned on state S. Finally, the conditional mutual information between A
1When the extrinsic reward is positive (re(s, a)  0, s, a), the extrinsic value is positive ve  0, , and setting  = 0 in Eq. (1) is equivalent to the no-reward setting where the goal is to maximize diversity.

3

and Z given S is denoted by I(A; Z|S). Then, the DIAYN objective to be maximized, given a prior over the latents, p, is:

I(S; Z) + H[A|S] - I(A; Z|S) = H[A|S, Z] + Ezp(z)[log p(z|s) - log p(z)].

(3)

sdz

This is an entropy-regularized objective that seeks to maximize the information that states contain about the skill used to reach it. In particular, the term of interest is Ezp(z),sdz [log p(z|s) - log p(z)], which corresponds to the value of a skill in an MDP with reward r(s|z) = log p(z|s) - log p(z). A skill policy (a|s, z) controls the first component of this reward, p(z|s), which measures the probability of identifying the skill in state s. Hence, the policy is rewarded for visiting states that differentiates it from other skills, thereby implicitly encouraging diversity.

The exact form of p(z|s) depends on how skills are encoded [19]. The most common version is to encode z as a one-hot d-dimensional variable [e.g.; 19, 1, 16]. Similarly, we represent z as z  {1, . . . , n} to index n separate policies z. In addition, the concept of finding a small set of meaningful policies is appealing from the interpretability perspective.

p(z|s) is typically intractable to compute due to the large state space and is instead approximated via a learned discriminator q(z|s). In our case, we measure p(z|s) under the stationary distribution of
the policy; that is, p(s|z) = dz (s). Therefore, for the purpose of analysis, we can find an analytic form for the objective of DIAYN before we apply the variational approximation. Given this, applying Bayes rule to p(z|s) yields

p(z|s) = dz (s)p(z) .

(4)

k dk (s)p(k)

And in the kernel case, we define a Gibbs distribution

p(z) exp ((s) · z)

p(z|s) = p(k) exp((s) · k) .

(5)

Plugging p(z|s) from Eq. (4) in the objective of DIAYN, the relevant term in Eq. (3) becomes

Ezp(z),sd(z)[log p(z|s)] = p(z) dz (s) log

z

s

dz (s)p(z) . k dk (s)p(k)

(6)

Finding a policy with maximal value for this reward can be seen as solving an optimization program
in dz under the constraint that the solution is a valid stationary state distribution (Section 2). The term s p(s|z) log p(s|z) corresponds to the negative entropy of dz , meaning that the objective to be maximized is convex in dz .

Lemma 1. The function s dz (s) log

dz (s)p(z) k dk (s)p(k)

is a convex function of dz .

The proof can be found in Appendix B; briefly, Lemma 1 holds because the function can be written as KL(dz || k p(k)dk ) + s dz (s) log p(z) and the KL-divergence is jointly convex on both arguments [9, Example 3.19]. The convexity of the objective results from the fact that the intrinsic reward log p(z|s) is a function of the policy. In the standard RL setup, the reward is not a function of the policy and the objective is linear in it, thus, maximizing and minimizing the reward are both convex minimization problems. However, when the reward is a function of the policy, maximization and minimization of the reward are not equivalent optimization problems. In DIAYN, the maximization of log p(z|s) leads to convex maximization while the minimization of the same reward leads to convex minimization. We note that the convexity of the objective has nothing to do with the variational approximation typically used to compute p(z|s); it is encountered with or without it.
The observation that discriminatory objectives lead to a set of n convex maximization problems in our setting is problematic, since the optimality of the solutions--in particular, their diversity--cannot be guaranteed. From the perspective of the policy set, the algorithm may converge to a set which is a local maxima rather than the global maxima, and therefore result in suboptimal diversity. In practice, different initializations and stochastic updates might mitigate the issue to some degree. In addition, it is possible that all the local maxima are close to optimal. For example, similar observations were made regarding the loss surface of deep neural networks, but the local optima points were shown to be very good in practice [15, 11, 36], mitigating the issues mentioned above. Thus, we recommend taking Lemma 1 as an observation regarding the optimization landscape of DIAYN which we hope to further explore in future work.

4

4.2 Diversity via Robustness

An alternative approach that implicitly induces diversity is to seek robustness among a set of policies by maximizing the performance w.r.t the worst case reward [24, 47]; for fixed n, the goal is:

max min max i · w.

(7)

n wB2 in

Here B2 is the 2 unit ball,  is the set of all possible policies, n = {1, . . . , n} is the set of n policies for which we are optimizing. Let us parse this objective term by term. First, the inner product i · w yields the expected value under the steady-state distribution (see Section 2) of the policy i. The inner min-max is a two-player zero-sum game, where the minimizing player is finding the worst-case reward function (since weights and reward functions are in a one-to-one correspondence)
that minimizes the expected value, and the maximizing player is finding the best policy from the set n (since policies and SFs are in a one-to-one correspondence) to maximize the value. The outer maximization is to find the best set of n policies that the maximizing player can use.

Intuitively speaking, the solution n to this problem might be a diverse set of policies since a non-diverse set is likely to yield a low value of the game, that is, it would easily be exploited by the minimizing player. In this way diversity and robustness are dual to each other, in the same way as a diverse financial portfolio is more robust to risk than a heavily concentrated one. By forcing our policy set to be robust to an adversarially chosen reward it will be diverse.

In [24], the authors proposed a solution to Eq. (7) using a CMDP with rd as discrimination (via DIAYN) and re is the extrinsic reward; we discuss it in more detail in Section 5. In [47], the authors proposed an iterative solution to Eq. (7) that incrementally adds policies to a solution set n (Algorithm 2 in the appendix). The authors define a Set Max Policy (SMP) as a policy that takes a set of policies and a reward as inputs and returns the best policy in the set for this reward. In each iteration, the algorithm computes the worst case reward w.r.t to the SMP, finds the policy that maximizes it, and adds it to the set. In iteration n The value of the SMP on the set n is defined as vn = minwB2 maxin i · w, and it is guaranteed that this value strictly increases vn+1 > vn in each iteration until the optimal solution is found. The following Lemma suggests that this procedure is equivalent to a fully corrective FW [17] algorithm on the function f = || · ||2. As a consequence, it is guaranteed to convergence to the optimal solution in a linear rate [22].
Lemma 2. The iterative procedure in [47] is equivalent to a fully corrective FW algorithm to minimize the function f = ||||2. As a consequence, to achieve an -optimal solution, the algorithm requires at most O(log(1/ )) iterations.

The proof in Appendix C suggests that the SMP policy is equivalent to the fully corrective search (maintaining a dictionary of solutions from previous iterations and choosing the best convex combination). The only difference between the two algorithms is that one of them solves a max-min problem where the other solves the equivalent min-max problem, and therefore they are guaranteed to have the same iterations from strong duality. Unfortunately this approach, like the discriminative approaches, has a weakness that can limit the ultimate diversity in the set. To see this note that

max min max i · w  min max i · w = max min i · w = - min i =def v,

n wB2 in

wB2 i

i wB2

 i 

where the inequality comes from the fact that n  , and the first equality uses von Neumann's
minimax theorem [42]. If we let  = arg mini i , then if n = {} we have an optimal policy set for the game, since we have found a policy set that achieves the known upper bound on the value of the game, v. In other words a single policy is a sufficient solution for Eq. (7), which is

problematic since the goal was to build up a set of many diverse policies. Similar to the discriminative approaches, in practice we obtain more policies by initializing the set away from , or alternatively restricting n to deterministic policies. However, this issue likely explains the empirical observations

in [47] that there are only a few active policies in the optimal sets.

Note that the results above hold only in the case that  is the set of all the stochastic policies in the MDP; if only deterministic policies are used, we cannot apply the von Neumann's minimax theorem. This is not an issue since we are interested in stochastic policies for multiple reasons: optimal solutions to CMDPs are stochastic policies [3] and stochastic policies are the most common approach in continuous control tasks, which is the focus of our experiments.

4.3 Explicit diversity methods
The two diversity mechanisms we have discussed so far were designed to maximize robustness or discrimination. Each one has its own merits in terms of diversity, but since they do not explicitly

5

maximize a diversity measure they cannot guarantee that the resulting set of policies will be diverse. We now propose two reward signals designed to induce a diverse set of policies. The way they do so is to leverage the information about the policies' long-term behavior available in their SFs. Both rewards are based on the intuition that the correlation between SFs should be minimized.

To motivate this approach, we note that SFs can be seen as a compact representation of a policy's stationary distribution. This becomes clear when we consider the case of a finite MDP with |S|dimensional "one-hot" feature vectors  whose elements encode the states: i(s) = I{s = i}, where I{ ·} is the indicator function. In this special case the SFs of a policy  coincide with its stationary distribution, that is,  = d. Under this interpretation, minimizing the correlation between SFs intuitively corresponds to encouraging the associated policies to visit different regions of the state
space--which in turn leads to diverse behavior. As long as we assume the tasks of interest are linear combinations of the features   Rd, which we do, similar reasoning applies when d < |S|.

But how do we compute policies in order to minimize the correlation between their SFs? To answer this question, we first consider the extreme scenario where there is a single policy k in the set . In this case the objective is: maxz z · w, where w = -k. Solving this problem is an RL problem whose reward is linear in the features weighted by w. A similar objective was investigated in [20], but there w was sampled i.i.d from a fixed prior. The question we are trying to address is: how to define w taking into account multiple policies in the set n?

We propose two answers to this question. The first one is to have w be the negative average of the SFs

of

the

policies

currently

in

the

set,

that

is,

w

=

-

1 k

k. This formulation is useful as it measures

the sum of negative correlations within the set. However, when two policies in the set happen to have

the same SFs with opposite signs, they cancel each other, and do not impact the diversity measure.

This diversity objective shares some similarities with the novelty search algorithm in [12], where the

mean pairwise distance between the current policy and an archive of other policies is used.

The second diversity-inducing reward we propose addresses this issue. It is defined as the minimum over the SFs in each state: r(s) = mink (s) · -k . This objective encourages the policy to
have the largest "margin" from the set, as it maximizes the negative correlation from the element that is "closest" to it. This objectives shares some similarities with a recent work [30] that uses the determinant of the kernel matrix and penalizes it to the closest agents in the population, building on ideas from Determinantal point processes [23]. Finally, we note that we also apply a non linear transformation to bound both of these rewards; the details are in the supplementary (Appendix D).

5 Solving the constrained MDP

At the core of our approach is the solution of a CMDP. The literature on CMDPs is quite vast and we refer the reader to [3] and [39] for treatments of the subject at different levels of abstraction. In this work we will focus on a reduction of CMDPs to MDPs via gradient updates. The idea is to look at the Lagrangian of Eq. (2):

L(, ) = -d · (rd + re) - ve.

(8)

Then, solving the CMDP in Eq. (2) is equivalent to solving min max0 L(, ).

Solving CMDPs via Lagrangian methods dates back to [8, 7]; more recently the problem has been tackled using Deep RL techniques [41, 10]. These algorithms perform primal-dual gradient updates on the min-max game. When the value function of the policy satisfies the constraint, the Lagrange multiplier will decrease, putting more emphasis on the extrinsic reward; when the constraint is not satisfied, the Lagrange multiplier will increase to satisfy the constraint.

Non linear Lagrange multiplier. We would like our agent to optimize a bounded reward signal, and we discuss how to bound each reward rd in the supplementary (Appendix D). To guarantee that a combination of two bounded rewards remains bounded, it is sufficient to combine them via a convex combination. To achieve that, we use a Sigmoid activation on the Lagrange multiplier so the reward is a convex combination of the diversity and the extrinsic rewards:

r(s) = ()re(s) + (1 - ())rd(s).

We further introduce an entropy regularization on  to prevent () from getting to extreme values (1 or 0), where the Sigmoid activation is saturated and has low gradients. This can happen, for example, at the beginning of learning where the agent's policy is sub-optimal and does not satisfy the constraint for many iterations. The objective for  is thus:

f () = ()(v^ - ve) - ahH(()),

(9)

6

where H is the entropy function, ah is the weight of the entropy regularization and v^ is an estimate of the total cumulative extrinsic return that the agent obtained in recent trajectories. The Lagrangian  is updated by performing gradient descent on Eq. (9) every N agent steps.

Estimation of average rewards. Another important step of Algorithm 1 which is not directly related

to solving the CMDP is the estimation of the average rewards. For that, we used a simple Monte Carlo

estimates:

v~j

=

1 T

T t=1

rt,

i.e,

the

empirical

average

reward

obtained

by

the

agent

in

trajectory

j

(where T = 1000). We used the same estimator to estimate the average SFs (replace rt with t).

The value v~j is a good estimate of the average reward, but it is not perfect. The issue is that the trajectory is of finite length, and therefore the samples in the beginning of the trajectory, before the policy is mixed, are biased. Our experiments are in the DM control suite [40] where the mixing time is small; the policies we discover roughly mix after  50 steps (as can be seen in the videos in the supplementary). Since the mixing time is much shorter than T , the effect of the biased samples is small ( 5%). It is also possible to wait until the policy is mixed or to collect a perfect unbiased estimate of the average reward via Coupling From the Past procedure [32] as was done in [44]. Note that this is a known issue with any practical policy gradient method but was not found to make a big difference empirically.

We further average the estimate using a running average with decay factor of ad: v^j = adv^j-1 + (1 - ad)v~j; this is the estimate we use in Eq. (9). The running average variables are set to 0 between iterations of Algorithm 1. Finally, we note here that we also experimented with the discounted criteria (discounted SFs). In that case, we observed that there is too much emphasis on the features that are observed at the beginning of the trajectory, resulting in less diversity across the entire trajectory.

Discussion. A different feasible approach to combine rd and re is to model the problem as a multiobjective MDP. That is, the diversity objective is added to the main one via a fixed, stationary weighting of the two rewards, e.g., r = a1rd + a2re. We note that the solution of such a multiobjective MDP cannot be be a solution to a CMDP. I.e., it is not possible to find the optimal dual variables , plug them in Eq. (8) and simply solve the resulting (unconstrained) MDP. Such an
approach ignores the fact the dual variables must be a `best-response' to the policy and is referred to
as the "scalarization fallacy" in [39, Section 4].

While Multi objective MDPs have been used in prior QD-RL papers [21, 25, 30, 18, 31, 48], we now outline a few potential advantages for using CMDPs. First, the CMDP formulation guarantees that the policies that we find are near optimal (satisfy the constraint). Secondly, the weighting coefficient in multi-objective MDPs has to be tuned, while in our case it is being adapted over time. This is particularly important in the context of maximizing diversity while satisficing reward. In many cases, as we observed in our experiments, the diversity reward might have no other option other than being the negative of the extrinsic reward. In these cases our algorithm will return good policies that are not diverse, while a solution to multi-objective MDP might fluctuate between the two objectives and not be useful at all.

CMDPs in related QD papers. Kumar et al. [24] proposed that solving a CMDP with rd as discrimination reward and re as the extrinsic reward will lead to a solution to the robustness objective (Eq. (7)). Sun et al. [38] also investigated CMDPs, but focused on the setup where the diversity reward has to satisfy a constraint, so the diversity reward is re and the extrinsic reward is rd. But most importantly, we use a different method to solve CMDPs, which is based on Lagrange multipliers and
SFs and is justified from CMDP theory [3, 8, 7], while these other two papers use techniques that are
not guaranteed to solve CMDPs.

6 Experiments
We conducted our experiments on domains from the DM Control Suite [40], standard continuous control locomotion tasks where diverse near-optimal policies should naturally correspond to different gaits. We focused on the setup where the agent is learning from feature observations corresponding to the positions and velocities of the body joints being controlled by the agent. Due to space considerations, we focus on domains where the diversity is interesting from a visual point of view, and in particular on Walker and Dog. In simpler domains like Cartpole and Reacher, we observed simple symmetric diversity ­ one policy moves a certain way clockwise and then the second policy moves in the same way anti-clockwise (see Fig. 4 in the supplementary). Later policies in the set are less distinguishable visually but can learn, for example, to balance the pole while moving. Note that without a diversity mechanism, the agent tends to only move in a single direction (e.g. clockwise).
7

In most of our experiments, the extrinsic reward re, which defines the optimality constraint in Algorithm 1, is set to be the environment reward provided by the DM Control Suite . The first policy in the set is trained to only maximize the extrinsic reward, and the other policies has to satisfy the constraint of being  = 0.9 optimal w.r.t it. In these experiments, we report the reward that each policy collects in white color on top of each figure. Additionally we report the reward of each policy in a small table in the main text.

In the QD community, there is no consensus regarding a single metric for measuring diversity, and some argue that there shouldn't be such (see, for example, the book "Why Greatness Cannot Be Planned" [37]). Inspired by this literature, we focus on measuring diversity only qualitatively by visualizing the learned policies. We strongly recommend the reader to check our visualization website where we show videos of the trajectories that each policy takes at https://anon98723.github.io/. In addition, we present "motion figures" by discretizing the videos (details in the Appendix) that give a fair impression of the policy behaviours. We would like to note that we did not tune our method to maximize diversity based on any metric other than constraint satisfaction (maintaining near-optimality). The main purpose of our experiments are the feasibility of the CMDP framework as proposed in Algorithm 1, i.e., to demonstrate that we discover diverse near-optimal policies.

Choice of rd: Given that our Diverse Successive Policies algorithm (1) can be used with different

measures of diversity, we compared four different choices. The previously proposed robustness and

discrimination measures and the new min and average explicit measures of diversity we proposed

in Section 4.3, corresponding to: (1) Robustness: the worst case linear reward with respect to

the previous policies in the set: rd(s) = w · (s), where w = minwB2 maxz[1,..,n-1] z · w

is the internal minimization in Eq. (7). (2) Discrimination rd(s) = log(

exp ((s)·n)

n i=j

exp((s)·j

)

),

where

n is the running average estimator of the SFs of the current policy. This reward corresponds to

Eq. (5) with a uniform prior. (3) Min: rd(s) = minz[1,..,n-1] -z · (s). (4) Average: rd(s) =

-

1 n-1

n-1 j=1

j

·

(s).

(5)

None:

rd(s)

=

0

or

no

diversity.

Fig. 1a presents eight polices that were discovered by Algorithm 1 where rd is the minimum explicit diversity criteria for Walker.stand. As we can see, the policies exhibit different types of standing: standing on both legs, standing on either leg, lifting the other leg forward and backward, spreading the legs and stamping. Not only are the policies different from each other, they also achieve high extrinsic reward in standing (see values on top of each policy visualization). Similar figures for the other diversity mechanisms can be found in the supplementary material (Appendix E.2). We observed that in this domain the Average diversity criterion can also discover policies that behave differently, but they are not as diverse as the ones found using the Minimum criterion (see Appendix E.2 in the supplementary material)

# re %
1 920 100 2 809 88 3 820 89 4 878 95 5 818 89 6 818 89 7 490 53 8 926 101

# re %
1 951 100 2 866 91 3 813 85 4 872 92 5 971 102 6 837 88 7 876 92 8 870 91

(a) Walker Stand, re as reward; rd as min.

(b) Walker Walk, re as reward; rd as average.

Figure 1: Diverse near optimal policies in Walker

The robustness mechanism can also provide diverse policies, but it tends to converge after a few iterations so no further diversity is achieved by the algorithm after 3 iterations. We also include a figure of different policies with no diversity mechanism in the supplementary (Fig. 9); in this case there is a small amount of diversity from training, but it is much less significant than the diversity we get with a diversity objective. Similarly, the discrimination method exhibits diversity but not as good as the explicit methods. We believe that this is due to the fact that the policies that maximize the extrinsic reward are already discriminative, and the algorithm fails to escape these local minima.
Fig. 1b presents similar results in the Walker.walk environment where rd is the average explicit diversity criteria. In this case the walker discovered how to walk in different ways, such as lifting one of the legs while up walking, walking with high knees, or walking with the heels to the bottom. In this domain we observed much better diversity with the explicit diversity mechanisms than with robustness or discrimination, see Appendix E.3. We also note that in both of the Walker environments, all (but one) of the discovered policies that we found are indeed near optimal, and satisfy the constraint (which was set to 90%).

8

# re %
1 921 100 2 870 94 3 879 95 4 909 98 5 944 102 6 975 106 7 938 102 8 930 101

# re %
1 812 -- 2 936 -- 3 765 -- 4 892 -- 5 926 -- 6 891 -- 7 921 -- 8 948 --

(a) Dog Stand, re as reward; rd as min.

(b) Dog Stand, re as reward; rd as none.

Figure 2: Diverse near optimal policies in Dog

Fig. 2 presents results in the Dog.stand environment where in Fig. 2a rd is the minimum explicit diversity criteria and in Fig. 2b there is no diversity mechanism. Inspecting Fig. 2b we can see that the dog learns how to stand (different policies are independent of each other so we leave the % blank), but in all cases, it stands with four legs on the ground. On the other hand, in Fig. 2a the dog learns different variations of "three leg standing" (lifting one of his legs), and still achieves high reward.
Next, we present results in the no-reward setting, where the agent has no access to the reward from the environment. Our results with None diversity confirm that the implementation of these diversity mechanisms yields complex locomotion in the no-reward setting as was reported in the original papers. However, in more complex domains like Walker, without adding the explicit diversity we get static behaviours that resemble "Yoga" exercises, as was also reported, for example, in [47].
Fig. 3a presents results for Walker where re is robustness and rd is average. Inspecting the results, we can see that the agent discovered complex locomotion skills such as kneeling backwards, crawling and flick-flack jumping. We also report the extrinsic reward for standing as another measure of zero-shot transfer (it was not used during training at all). In Appendix E.4 we can see that other diversity mechanisms discovered other surprising skills such as "head walking".
Finally, Fig. 3b presents results for Cheetah where re is discrimination and rd is robustness. The cheetah learns to run forward, backwards, and then to do various jumps. While previous methods were able to discover similar behaviours, they are typically not that diverse with such a small set.

(a) Walker, re as robustness and rd as average. (b) Cheetah, re as discrimination and rd as robustness.
Figure 3: Diversity without reward in Walker and Cheetah.
7 Conclusion
In this work we proposed a framework for discovering near optimal diverse behaviours. We framed the problem as solving a CMDP where a diversity intrinsic reward and the extrinsic reward are adaptively combined. There are interesting connections to whitebox metagradients [43, 46] ­ the updates of the Lagrangian can be viewed as the outer update in metagradients where satisfying the constraint is the outer loss. Using metagradients to learn other diversity hyperparameters or even to discover the diversity reward itself [49] are exciting directions for future work. Key to our approach was the idea of measuring diversity in the space of SFs. This design choice allowed us to provide insights on how existing diversity mechanisms behave from the perspective of convex optimization. There are many exciting applications for our framework. For example, consider the process of using RL to train a robot to walk. The designer does not know a priori which reward will result in the desired walking pattern. Thus, robotic engineers often train a policy to maximize an initial reward, tweak the reward, and iterate until they reach the desired behaviour. Using our approach, the engineer would have multiple forms of walking to choose from in each attempt, which are also interpretable (linear in the weights).
9

References
[1] J. Achiam, H. Edwards, D. Amodei, and P. Abbeel. Variational option discovery algorithms. arXiv preprint arXiv:1807.10299, 2018.
[2] A. Agarwal, M. Henaff, S. Kakade, and W. Sun. Pc-pg: Policy cover directed exploration for provable policy gradient learning. arXiv preprint arXiv:2007.08459, 2020.
[3] E. Altman. Constrained Markov decision processes, volume 7. CRC Press, 1999.
[4] A. Barreto, W. Dabney, R. Munos, J. J. Hunt, T. Schaul, H. P. van Hasselt, and D. Silver. Successor features for transfer in reinforcement learning. In Advances in neural information processing systems, pages 4055­4065, 2017.
[5] K. Baumli, D. Warde-Farley, S. Hansen, and V. Mnih. Relative variational intrinsic control. arXiv preprint arXiv:2012.07827, 2020.
[6] R. Behovits. Game 8: Leko wins to take the lead, 2004. URL https://en.chessbase.com/post/ game-8-leko-wins-to-take-the-lead.
[7] S. Bhatnagar and K. Lakshmanan. An online actor­critic algorithm with function approximation for constrained markov decision processes. Journal of Optimization Theory and Applications, 153(3):688­708, 2012.
[8] V. S. Borkar. An actor-critic algorithm for constrained markov decision processes. Systems & control letters, 54(3):207­213, 2005.
[9] S. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press, 2004.
[10] D. A. Calian, D. J. Mankowitz, T. Zahavy, Z. Xu, J. Oh, N. Levine, and T. Mann. Balancing constraints and rewards with meta-gradient d4pg. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=TQt98Ya7UMP.
[11] A. Choromanska, M. Henaff, M. Mathieu, G. B. Arous, and Y. LeCun. The loss surfaces of multilayer networks. In Artificial intelligence and statistics, pages 192­204. PMLR, 2015.
[12] E. Conti, V. Madhavan, F. P. Such, J. Lehman, K. O. Stanley, and J. Clune. Improving exploration in evolution strategies for deep reinforcement learning via a population of novelty-seeking agents. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, pages 5032­5043, 2018.
[13] T. M. Cover. Elements of information theory. John Wiley & Sons, 1999.
[14] C. da Fonseca-Wollheim. Swapping songs with chess grandmaster garry kasparov, 2020. URL https://www.nytimes.com/2020/12/18/arts/music/garry-kasparov-classical-music.html.
[15] Y. N. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli, and Y. Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc., 2014. URL https: //proceedings.neurips.cc/paper/2014/file/17e23e50bedc63b4095e3d8204ce063b-Paper.pdf.
[16] B. Eysenbach, A. Gupta, J. Ibarz, and S. Levine. Diversity is all you need: Learning skills without a reward function. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=SJx63jRqFm.
[17] M. Frank and P. Wolfe. An algorithm for quadratic programming. Naval research logistics quarterly, 3(1-2):95­110, 1956.
[18] T. Gangwani, J. Peng, and Y. Zhou. Harnessing distribution ratio estimators for learning agents with quality and diversity. arXiv preprint arXiv:2011.02614, 2020.
[19] K. Gregor, D. J. Rezende, and D. Wierstra. Variational intrinsic control. International Conference on Learning Representations, Workshop Track, 2017. URL https://openreview.net/forum? id=Skc-Fo4Yg.
[20] S. Hansen, W. Dabney, A. Barreto, D. Warde-Farley, T. V. de Wiele, and V. Mnih. Fast task inference with variational intrinsic successor features. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=BJeAHkrYDS.
10

[21] Z.-W. Hong, T.-Y. Shann, S.-Y. Su, Y.-H. Chang, T.-J. Fu, and C.-Y. Lee. Diversity-driven exploration strategy for deep reinforcement learning. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, pages 10510­10521, 2018.
[22] M. Jaggi and S. Lacoste-Julien. On the global linear convergence of frank-wolfe optimization variants. Advances in Neural Information Processing Systems, 28, 2015.
[23] A. Kulesza, B. Taskar, et al. Determinantal point processes for machine learning. Foundations and Trends® in Machine Learning, 5(2­3):123­286, 2012.
[24] S. Kumar, A. Kumar, S. Levine, and C. Finn. One solution is not all you need: Few-shot extrapolation via structured maxent rl. Advances in Neural Information Processing Systems, 33, 2020.
[25] M. A. Masood and F. Doshi-Velez. Diversity-inducing policy gradient: Using maximum mean discrepancy to find a set of diverse policies. arXiv preprint arXiv:1906.00088, 2019.
[26] B. Matusch, J. Ba, and D. Hafner. Evaluating agents without rewards. arXiv preprint arXiv:2012.11538, 2020.
[27] N. Mehta, S. Natarajan, P. Tadepalli, and A. Fern. Transfer in variable-reward hierarchical reinforcement learning. Machine Learning, 73(3):289, 2008.
[28] J.-B. Mouret and J. Clune. Illuminating search spaces by mapping elites. arXiv preprint arXiv:1504.04909, 2015.
[29] A. F. Osborn. Applied imagination. 1953.
[30] J. Parker-Holder, A. Pacchiano, K. M. Choromanski, and S. J. Roberts. Effective diversity in population based reinforcement learning. Advances in Neural Information Processing Systems, 33, 2020.
[31] Z. Peng, H. Sun, and B. Zhou. Non-local policy optimization via diversity-regularized collaborative exploration. arXiv preprint arXiv:2006.07781, 2020.
[32] J. G. Propp and D. B. Wilson. Exact sampling with coupled markov chains and applications to statistical mechanics. Random Structures and Algorithms, 1996.
[33] J. K. Pugh, L. B. Soros, and K. O. Stanley. Quality diversity: A new frontier for evolutionary computation. Frontiers in Robotics and AI, 3:40, 2016.
[34] M. L. Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons, 1984.
[35] S. Singh, R. L. Lewis, A. G. Barto, and J. Sorg. Intrinsically motivated reinforcement learning: An evolutionary perspective. IEEE Transactions on Autonomous Mental Development, 2(2): 70­82, 2010.
[36] D. Soudry and Y. Carmon. No bad local minima: Data independent training error guarantees for multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.
[37] K. O. Stanley and J. Lehman. Why greatness cannot be planned: The myth of the objective. Springer, 2015.
[38] H. Sun, Z. Peng, B. Dai, J. Guo, D. Lin, and B. Zhou. Novel policy seeking with constrained optimization. arXiv preprint arXiv:2005.10696, 2020.
[39] C. Szepesvári. Constrained mdps and the reward hypothesis, 2020. URL https://readingsml. blogspot.com/2020/03/constrained-mdps-and-reward-hypothesis.html.
[40] Y. Tassa, Y. Doron, A. Muldal, T. Erez, Y. Li, D. d. L. Casas, D. Budden, A. Abdolmaleki, J. Merel, A. Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018.
[41] C. Tessler, D. J. Mankowitz, and S. Mannor. Reward constrained policy optimization. In International Conference on Learning Representations, 2019. URL https://openreview.net/ forum?id=SkfrvsA9FX.
[42] J. von Neumann. Zur theorie der gesellschaftsspiele. Mathematische annalen, 100(1):295­320, 1928.
11

[43] Z. Xu, H. van Hasselt, and D. Silver. Meta-gradient reinforcement learning. arXiv preprint arXiv:1805.09801, 2018.
[44] T. Zahavy, A. Cohen, H. Kaplan, and Y. Mansour. Average reward reinforcement learning with unknown mixing times. The Conference on Uncertainty in Artificial Intelligence (UAI), 2020.
[45] T. Zahavy, A. Hasidim, H. Kaplan, and Y. Mansour. Planning in hierarchical reinforcement learning: Guarantees for using local policies. In Algorithmic Learning Theory, pages 906­934, 2020.
[46] T. Zahavy, Z. Xu, V. Veeriah, M. Hessel, J. Oh, H. P. van Hasselt, D. Silver, and S. Singh. A self-tuning actor-critic algorithm. Advances in Neural Information Processing Systems, 33, 2020.
[47] T. Zahavy, A. Barreto, D. J. Mankowitz, S. Hou, B. O'Donoghue, I. Kemaev, and S. Singh. Discovering a set of policies for the worst case reward. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=PUkhWz65dy5.
[48] Y. Zhang, W. Yu, and G. Turk. Learning novel policies for tasks. In K. Chaudhuri and R. Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 7483­7492. PMLR, 09­15 Jun 2019. URL http://proceedings.mlr.press/v97/zhang19q.html.
[49] Z. Zheng, J. Oh, and S. Singh. On learning intrinsic rewards for policy gradient methods. arXiv preprint arXiv:1804.06459, 2018.
12

A Checklist
1. For all authors (a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] We discussed the limitations of diversity seeking methods, from being convex maximization problems (diayn) and from using auxiliary objectives (robustness). (c) Did you discuss any potential negative societal impacts of your work? [No] . Our paper studies how RL algorithms can find diverse solutions, we believe that promoting algorithmic diversity in AL should not have any negative societal impacts. (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [Yes] (b) Did you include complete proofs of all theoretical results? [Yes]
3. If you ran experiments. (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [No] (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [No] Instead of repeating the same experiment over multiple seeds, each of our experiments was performed to discover eight policies sequentially. Our only numerical claim in this paper is about constraint satisfaction, and as our results suggest, it is being satisfied, in any of these eight, consecutive but independent trials (the parameters were initialized after each iteration. Another axis in which we tested our algorithm was the domain. So instead of repeating Walker.Walk a few times, we performed the second experiment on Walker.Stand (so, a diff in the extrinsic reward) and the same in Dog. (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [No]
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] (b) Did you mention the license of the assets? [No] (c) Did you include any new assets either in the supplemental material or as a URL? [No] (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [No] (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [No]
5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [No] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [No] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [No]
13

B Proof for Lemma 1

Proof. We will focus on the case that there are no zero elements in d which is a standard assumption in ergodic MDPs. Under this assumption f is a twice differentiable function so it is convex if its Hessian is positive semidefinite.
Recall that the prior p(z) is constant, and that the policies k = 1, ..., k = z are also constant from the perspective of dz . We can therefore introduce a simplified notation and write the objective as

dz (s) log
s

dz (s)p dz (s)p + cs

The variable dz is a vector in the |S|-simplex. We can represent it using |S| - 1 degrees of freedom

x1, ..xs-1  [0, 1] where the last element is xs = 1 -

|S|-1 i=1

xi.

Notice

that

xs

is

a

function

of

xi

so it has a derivative with respect to. xi which equals -1. So we have

f (x) = xi log
i

xip xip + ci

+ xs log

xsp xsp + cs

The first derivative of this function with respect to xi, i  [1, .., |S| - 1 is

f xi

=

log(xi)

+1+

log(p) -

xip xip +

ci

- log(xip

+ ci)

-

log(xs)

-

1

-

log(p)

+

xsp xsp + cs

+

log(xsp

+

cs)

=

log(xi)

-

log(xip

+

ci)

-

xip xip +

ci

(10)

-

log(xs)

-

log(xsp

+

ci)

-

xsp xsp +

cs

(11)

We can see that the terms in Eq. (10) depend only on xi and the terms in Eq. (11) depend only on xs. In addition, we will soon see that the derivatives of xs will be equal for any j  1, ..., s - 1. These two observations imply that the Hessian will have the form of

H = D + m1,

where D is a diagonal matrix with derivatives of Eq. (10) with respect to xi as it elements, 1 is a matrix of all ones, and m is the derivative of Eq. (11) with respect to xj which we will show to be equal for all j. Notice that x, we have that xT (D + m1)x = Dix2i + m( xi)2. This implies that in order for the Hessian to be positive definite, we only need to show that the elements of d and
the scalar m are positive. The derivative of Eq. (10) with respect to xi is

1 xi

-

p pxi +

ci

-

p(pxi + (pxi

ci) - p2xi + ci)2

=

pxi + ci - pxi xi(pxi + ci)

-

pci (pxi + ci)2

=

ci(pxi + ci) - pxici xi(pxi + ci)2

=

xi

c2i (pxi +

ci

)2

,

(12)

which is positive because xi  0. Similarly, The derivative of Eq. (11) with respect to xj is

1 xs

-

p pxs +

cs

-

p(pxs + cs) - p2xs (pxs + cs)2

=

c2s xs(pxs +

cs)2 ,

(13)

which is also positive because xs  0 and concludes our proof.

14

C Proof for Lemma 2

Algorithm 2 The iterative procedure in [47]

Initialize: Sample w  N (¯0, ¯1), 0  { }, 1  arg max w · , t  1 v¯SM1 P  -||1|| repeat
t  t-1  {t}

t = t-1  {t}

w¯ SMt P  arg minwB2 maxi w · 

t+1  arg max () · w¯ SMt P tt+1

until

vt
w¯ SMnP



v¯SMt-P1

return t-1

Algorithm 3 Fully corrective FW for h() = 0.5||||22
Initialize: Let 1 be a random policy and let 1 be its SFs. Also, let 0 = {} and 0 = {} and t  1. repeat
t = t-1  {t} t = t-1  {t} ^ = arg minCo(i) 0.5||||22. t+1 = arg max () · -h(^) = arg max () · -^ tt+1 until h(^)  return t-1

In this section we show that the iterates of the fully corrective FW algorithm (Algorithm 3) correspond to the iterates of the Worst Case Policy Iteration algorithm (Algorithm 2). Examining the two algorithms, it is easy to see that all that is needed is to show that

arg

max


()

·

-^

=

arg

max


()

·

w¯ SMnP.

To show this, first observe that w¯ SMnP can be also written as

w¯ SMnP

=

arg

min
wB2

max
xi

w

·



=

arg

min
wB2

max w
 Co(i )

·

,

(14)

that is, maximizing  over Co(i) instead of i (SMP). This is correct because for any reward w there is always a maximizer in the convex hull that is one of the vertices (a property of the linear inner product). And therefore, the same maximum value is attained when maximizing over these two sets.

Next, we have that

arg

min
Co(i

)

||

||22

=

arg

max
 Co(i )

-||||22

(15)

= arg max -||||2 = arg max min  · w.

(16)

 Co(i )

Co(i) wB2

Now, if we denote the optimal solutions to Eq. (16) as w^, ^ then, they are also an optimal solution to Eq. (14) via Von Neuman's min-max theorem. This means that w¯ SMnP = w^ = -^/||^||.

Thus

arg

max


()

·

w¯ SMnP

=

arg

max


()

·

-^/||^||

=

arg

max


()

·

-^,

where the second inequality follows from the fact that dividing the reward by the same constant

across all states does not change the optimal policy (the arg max).

Finally, note that the function h = 0.5||x||22 has 1-Lipschitz gradient and is strongly convex. Thus, since the algorithms are equivalent, Algorithm 2 achieves a linear convergence according to the following theorem.

15

Theorem 1 (Linear Convergence [22]). Suppose that h has L-Lipschitz gradient and is µ-strongly convex. Let D = {d,   } be the set of all the state occupancy's of deterministic policies in the MDP and let K = Co(D) be its Convex Hull. Such that K a polytope with vertices D, and let M = diam(K). Also, denote the Pyramidal Width of D,  = P W idth(D) as in [22, Equation 9 1].
Then the suboptimality ht of the iterates of all the fully corrective FW algorithm decreases geometrically at each step, that is
µ2 h(xt+1)  (1 - )h(xt) , where  = 4LM 2

D Additional implementation details and hyper parameters

When we add a new policy, t, to the set t-1, we reset the maximum value ve = max{ve, vt}. This step is useful because the policies and their value functions are computed approximately in practice and in some of the domains the optimal performance is not achieved in the first iteration of Algorithm 1.

To

bound

the

intrinsic

rewards

we

first

use

the

following

transformation

r~w (s)

=

w·(s) + w2

w

2

and

then apply the following non-linear transformation:

r(s) = (1 - exp (- r~w(s))) /(1 - exp( )),

(17)

This transformation is useful when we want the reward to be more sensitive to small variations of the inner product, i.e., when many policies are relatively similar to each other.

Finally, Table 1 summarizes the hyperparameters that we use in Algorithm 1

Table 1: Hyperparameters table
Parameter Optimality level  (Eq. (8))
Environment steps per policy Number of policies Lagrange entropy regularization weight ah (Eq. (9)) Lagrange learning rate Lagrange update frequency (N) Estimation decay factor ad Normalization temperature  (Eq. (17))

Value
0.9 106 8 0.01 0.1 30 0.9 3

E Additional results
Our "motion figures" were created in the following manner. Given a trajectory of frames that composes a video f1, . . . , fT , we first trim and sub sample the trajectory into a point of interest in time: fn, . . . , fn+m. We always use the same trimming across the same set of policies (the sub figures in a figure). We then sub sample frames from the trimmed sequence at frequency 1/p: fn, fn+p, fn+2p . . . ,. After that, we take the maximum over the sequence and present this "max" image. In Python, this simply corresponds to, for example, to
n=400, m=30, p=3 indices = range(n,n+m,p) im = np.max(f[indices])
This creates the effect of motion in single figure since the object has higher values then the background.
E.1 Clockwise Diversity in Cartpole and Reacher

16

(a) Cartpole

(b) Reacher

Figure 4: Clockwise Diversity in Cartpole and Reacher.

17

E.2 Walker Stand Figure 5: Min
Figure 6: Average 18

Figure 7: Robustness
Figure 8: Discrimination
Figure 9: None 19

E.3 Walker Walk Figure 10: Min
Figure 11: Average 20

Figure 12: Robustness
Figure 13: Discrimination
Figure 14: None 21

E.4 Robustness in Walker Figure 15: Min
Figure 16: Average 22

Figure 17: Discrimination
Figure 18: None 23

E.5 Robustness in Cheetah Figure 19: Min
Figure 20: Average 24

Figure 21: Discrimination
Figure 22: None 25

