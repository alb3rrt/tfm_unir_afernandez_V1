On Finding the K-best Non-projective Dependency Trees
Ran Zmigrod Tim Vieira Ryan Cotterell , University of Cambridge Johns Hopkins University ETH Zu¨rich
rz279@cam.ac.uk tim.f.vieira@gmail.com ryan.cotterell@inf.ethz.ch

arXiv:2106.00780v1 [cs.CL] 1 Jun 2021 Root constraint violation rate (%)

Abstract
The connection between the maximum spanning tree in a directed graph and the best dependency tree of a sentence has been exploited by the NLP community. However, for many dependency parsing schemes, an important detail of this approach is that the spanning tree must have exactly one edge emanating from the root. While work has been done to efficiently solve this problem for finding the one-best dependency tree, no research has attempted to extend this solution to finding the K-best dependency trees. This is arguably a more important extension as a larger proportion of decoded trees will not be subject to the root constraint of dependency trees. Indeed, we show that the rate of root constraint violations increases by an average of 13 times when decoding with K = 50 as opposed to K = 1. In this paper, we provide a simplification of the K-best spanning tree algorithm of Camerini et al. (1980). Our simplification allows us to obtain a constant time speed-up over the original algorithm. Furthermore, we present a novel extension of the algorithm for decoding the K-best dependency trees of a graph which are subject to a root constraint.1
1 Introduction
Non-projective, graph-based dependency parsers are widespread in the NLP literature. (McDonald et al., 2005; Dozat and Manning, 2017; Qi et al., 2020). However, despite the prevalence of K-best dependency parsing for other parsing formalisms-- often in the context of re-ranking (Collins and Koo, 2005; Sangati et al., 2009; Zhu et al., 2015; Do and Rehbein, 2020) and other areas of NLP (Shen et al., 2004; Huang and Chiang, 2005; Pauls and Klein, 2009; Zhang et al., 2009), we have only found three works that consider K-best non-projective
1Our implementation is available at https://github. com/rycolab/spanningtrees.

35

K = 50

K=1

30

25

20

15

10

5

0

102

103

104

Training set size (log-scale)

Figure 1: Violation rate of the root constraint when using regular K-best decoding (Camerini et al., 1980) on pre-trained models of Qi et al. (2020) for languages with varying training set sizes.

dependency parsing (Hall, 2007; Hall et al., 2007; Agic´, 2012). All three papers utilize the K-best spanning tree algorithm of Camerini et al. (1980). Despite the general utility of K-best methods in NLP, we suspect that the relative lack of interest in K-best non-projective dependency parsing is due to the implementation complexity and nuances of Camerini et al. (1980)'s algorithm.2
We make a few changes to Camerini et al. (1980)'s algorithm, which result in both a simpler algorithm and simpler proof of correctness.3 Firstly, both algorithms follow the key property that we can find the second-best tree of a graph by removing a single edge from the graph (Theorem 1); this property is used iteratively to enumerate the K-best trees in order. Our approach to finding the second-best tree (see §3) is faster because of it performs half as many of the expensive cycle-contraction operations (see §2). Overall, this change is responsible for our 1.39x speed-up
2In fact, an anonymous reviewer called it "one of the most `feared' algorithms in dependency parsing."
3While our algorithm is by no means simple, an anonymous reviewer called it "a big step in that direction."

(see §4). Secondly, their proof of correctness is based on reasoning about a complicated ordering on the edges in the Kth tree (Camerini et al., 1980, Section 4); our proof side-steps the complicated ordering by directly reasoning over the ancestry relations of the Kth tree. Consequently, our proofs of correctness are considerably simpler and shorter. Throughout the paper, we provide the statements of all lemmas and theorems in the main text, but defer all proofs to the appendix.
In addition to simplifying Camerini et al. (1980)'s algorithm, we offer a novel extension. For many dependency parsing schemes such as the Universal Dependency (UD) scheme (Nivre et al., 2018), there is a restriction on dependency trees to only have one edge emanate from the root.4 Finding the maximally weighted spanning tree that obeys this constraint was considered by Gabow and Tarjan (1984) who extended the O(N 2) maximum spanning tree algorithm of Tarjan (1977); Camerini et al. (1979). However, no algorithm exists for Kbest decoding of dependency trees subject to a root constraint. As such, we provide the first K-best algorithm that returns dependency trees that obey the root constraint.
To motivate the practical necessity of our extension, consider Fig. 1. Fig. 1 shows the percentage of trees that violate the root constraint when doing one-best and 50-best decoding for 63 languages from the UD treebank (Nivre et al., 2018) using the pre-trained model of Qi et al. (2020).5,6 We find that decoding without the root constraint has a much more extreme effect when decoding the 50-best than the one-best. Specifically, we observe that on average, the number of violations of the root constraint increased by 13 times, with the worst increase being 44 times. The results thus suggest that finding K-best trees that obey the root constraint from a non-projective dependency parser requires a specialist algorithm. We provide a more detailed results table in App. A, including root constraint violation rates for K = 5, K = 10, and K = 20. Furthermore, we note that the K-best algorithm may also be used for marginalization of latent variables (Correia et al., 2020) and for constructing parsers with global scoring functions (Lee et al., 2016).
4There are certain exceptions to this such as the Prague Treebank (Bejcek et al., 2013).
5Zmigrod et al. (2020) conduct a similar experiment for only the one-best tree.
6We note that Qi et al. (2020) do apply the root constraint for one-best decoding, albeit with a sub-optimal algorithm.

 40 2

90

60

20

4

30 50

1

70 3

10

Figure 2: Example graph G (taken from Zmigrod et al. (2020)). Edges that are part of both the best tree G(1) and the best dependency tree G[1] are marked as thick solid edges. Edges only in G(1) are dashed and edges only in G[1] are dotted.

2 Finding the Best Tree
We consider the study of rooted directed weighted graphs, which we will abbreviate to simply graphs.7 A graph is given by G = (, N , E) where N is a set of N + 1 nodes with a designated root node   N and E is a set of directed weighted edges. Each edge e = (i A j)  E has a weight w(e)  R+. We assume that self-loops are not allowed in the graph (i.e., (i A i)  E). Additionally, we assume our graph is not a multi-graph, therefore, there can exist at most one edge from node i to node j.8 When it is clear from context, we abuse notation and use j  G and e  G for j  N and e  E respectively. When discussing runtimes, we will assume a fully connected graph (|E| = N 2).9 An arborescence (henceforth called a tree) of G is a subgraph d = (, N , E ) such that E  E and the following is true:

1. For all j  N {}, |{( A j)  E }| = 1. 2. d does not contain any cycles.

Other definitions of trees can also include that there is at least one edge emanating from the root. However, this condition is immediately satisfied by the above two conditions. A dependency tree
7As we use the algorithm in Zmigrod et al. (2020) as our base algorithm, we borrow their notation wherever convenient.
8We make this assumption for simplicity, the algorithms presented here will also work with multi-graphs. This might be desirable for decoding labeled dependency trees. However, we note that in most graph-based parsers such as Qi et al. (2020) and Ma and Hovy (2017), dependency labels are extracted after the unlabeled tree has been decoded.
9We make this assumption as in the context of dependency parsing, we generate scores for each possible edge. Furthermore, (Tarjan, 1977) prove that the runtime of finding the best tree for dense graphs is O(N 2). This is O(|E| log N ) in the non-dense case.

d = (, N , E ) is a tree with the extra constraint

3. |{( A )  N }| = 1

The set of all trees and dependency trees in a graph are given by A(G) and D(G) respectively. The weight of a tree is given by the sum of its edge weights10

w(d) = w(e)

(1)

ed

This paper concerns finding the K highest-

weighted (henceforce called K-best) tree or de-

pendency tree, these are denoted by G(K) and G[K]

respectively. Tarjan (1977); Camerini et al. (1979)

provided the details for an O(N 2) algorithm for

decoding the one-best tree. This algorithm was ex-

tended by Gabow and Tarjan (1984) to find the best

dependency tree in O(N 2) time. We borrow the

algorithm (and notation) of Zmigrod et al. (2020),

who provide an exposition and proofs of these algo-

rithms in the context of non-projective dependency

parsing. The pseudocode for finding G(1) and G[1]

is given in Fig. 3. We briefly describe the key com-

ponents of the algorithm.11

The

greedy

graph

of

G

is

denoted

by

-A G

=

(, N , E ) where E contains the highest weighted

incoming edge to each non-root node. Therefore, if

-A G

has

no

cycles,

then

-A G

=

G(1).

A

cycle

C

in

-A G

is called a critical cycle. If we encounter a critical

cycle in the algorithm, we contract the graph by

the critical cycle. A graph contraction, G/C, by a cycle C replaces the nodes in C by a mega-node

c such that the nodes of G/C are N C  {c}. Furthermore, for each edge e = (i A j)  G:

1. If i  C and j  C, then e = (i A c) 

G/C

such that w(e )

=

w(e) + w

-A Cj

where Cj is the subgraph of C rooted at j.

2. If i  C and j  C, then e = (c A j)  G/C such that w(e ) = w(e).

3. If i  C and j  C, then e  G/C.

4. If i  C and j  C, then there is no edge related to (i A j) in G/C.

There also exists a bookkeeping function  such

10For inference, the weight of a trees often decomposes multiplicatively rather than additively over the edges. One can take the exponent (or logarithm) of the original edge weights to make the weights distribute additively (or multiplicative).
11For a more complete and detailed description as well as a proof of correctness, please refer to the original manuscripts.

1: def opt(G) :

2:

if

-A G

has

a

cycle

C

:

Recursive case

3:

return opt G/C C

4: else

Base case

5:

if we require a dependency tree :

6:

return constrain(G)

7: 8:

else

-A

return G

9: def constrain(G) :

10:





set

of

's

outgoing

edges

in

-A G

11:

if

||

=

1

:

return

-A G

Constraint satisfied

---A

12: e  argmax w G\\e

e 

13:

if

--A G\\e

has

cycle

C

:

14:

return constrain G/C C

15: else

16:

return constrain(G\\e)

Figure 3: Algorithms for finding G(1) and G[1]. These are from Zmigrod et al. (2020).

that for all e  G/C, (e )  G. This bookkeeping function returns the edge in the original graph that

led to the creation of the edge in the contracted

graph using one of the constructions above.

Finding G(1) is then the task of finding a con-

tracted graph G

such

that

-A G

=

G

(1).

Once this is

done, we can stitch back the cycles we contracted.

If G = G/C , for any d  A(G/C ), d C 

A(G) is the tree made with edges (d) ( applied

to

each

edge

d)

and

-A Cj

where

Cj

is

the

subgraph

of

the nodes in C rooted at node j and (e) = (i A j)

for e = (i A c)  d. The contraction weight-

ing scheme means that w(d) = w(d C) (Geor-

giadis, 2003). Therefore, G(1) = (G (1) C)(1).

The strategy for finding G[1] is to find the con-

tracted graph for G(1) and attempt to remove edges

emanating from the root. This was first proposed

by Gabow and Tarjan (1984). When we consider

removing an edge emanating from the root, we are

doing this in a possibly contracted graph, and so an

edge ( A j) may exist multiple times in the graph.

We denote G\\e to be the graph G with all edges

with the same end-points as e removed. Fig. 2 gives

an example of a graph G, its best tree G(1), and its

best dependency tree G[1].

The runtime complexity of finding G(1) or G[1]

is O(N 2) for dense graphs by using efficient pri-

ority queues and sorting algorithms (Tarjan, 1977;

Gabow and Tarjan, 1984). We assume this runtime


4 1

 b(G, e, G(1))

 b(G, e, G(1))

2

2

2

e

4

4

e

1 3

3

1

3

r(G, e, G(1))

r(G, e, G(1))

(a)
 b(G, e, G(1))
2

4 e

1

f 3

r(G, e, G(1))

(b)
 b(G, e, G(1))
2

4

f

e

1

f 3

r(G, e, G(1))

(c)



2

e

4

f

e

1

f 3

(d)

(e)

(f)

Figure 4: Worked example of Lemma 1. Consider a fully connected graph, G, of the example given in Fig. 2 as given in (a). Suppose that the solid edges in (a) represent -AG. Therefore, G(1) = -AG. Next, suppose that we know that e = (2 A 4)  G(1) is not in G(2). Then one of the dashed edges in (b) must be in G(2) as 4 must have an incoming edge. The edges emanating from  and 1 make up the set of blue edges, b(G, e, G(1)) while the edge emanating from 3 makes the set of red edges, r(G, e, G(1)). If e  b(G, e, G(1)) is in G(2) as in (c), then the solid lines in (c) make a tree and G(2) differs from G(1) by exactly one blue edge of e. Otherwise, we know that e  r(G, e, G(1)) is in G(2) as in (d). However, the solid edges in (d) contain a cycle between 3 and 4 with edges e and f . We could break the cycle at 3 and include edge f in our tree as in (e). However, while the solid edges in (e) make a valid tree, as w(e) > w(e ) and w(f ) > w(f ), the tree given by the solid lines of (f) will have a higher weight. This would mean that e  G(2) which leads to a contradiction. Therefore, we must break the cycle at 4 , which leads us to a tree as in (c). Consequently, G(2) will differ from G(1) by exactly one blue edge of e.

for the remainder of the paper.
3 Finding the Second Best Tree
In the following two sections, we provide a simplified reformulation of Camerini et al. (1980) to find the K-best trees. The simplifications additionally provide a constant time speed-up over Camerini et al. (1980)'s algorithm. We discuss the differences throughout our exposition.
The underlying concept behind finding the Kbest tree, is that G(K) is the second best tree G (2) of some subgraph G  G. In order to explore the space of subgraphs, we introduce the concept of edge inclusion and exclusion graphs.
Definition 1 (Edge inclusion and exclusion). For any graph G and edge e  G, the edge-inclusion graph G + e  G is the graph such that for any d  A(G + e), e  d. Similarly, the edgeexclusion graph G - e  G is the graph such that for any d  A(G - e), e  d.

When we discuss finding the K-best dependency trees in §5, we implicitly change the above definition to use D(G + e) and D(G - e) instead of A(G + e) and A(G - e) respectively.
In this section, we will specifically focus on finding G(2), we extend this to finding the G(k) in §4. Finding G(2) relies on the following fundamental theorem.

Theorem 1. For any graph G and e  G(1)

G(2) = (G - e)(1)

(6)

where

e = argmax w (G - e )(1)

(7)

e G(1)

Theorem 1 states that we can find G(2) by identifying an edge e  G(1) such that G(2) = (G - e)(1). We next show an efficient method for identifying this edge, as well as the weight of G(2) without actually having to find G(2).

Definition 2 (Blue and red edges). For any graph

1: def next(G) :

2:

if

-A G

has

a

cycle

C

:

Recursive case

3:

d, w, e  next G/C

4:

d d C

5:

e  argmin wG,d (e )

e Cd

6:

w  w(d ) - wG,d (e)

7:

return d , max( w, (e) , w , e )

8: else

Base case

9:

e  argmin wG(e )

-A

e G

10:

ww

-A G

- wG(e)

11:

-A return G, w, e

Figure 5: Algorithm for finding G(1), the best edge e to delete to find G(2), and w G(2) .
G, tree d  A(G), and edge e = (i A j)  d, the set of blue edges b(G, e, d) and red edges r(G, e, d) are defined by12

b(G, e, d) d=ef {e =(i A j) | w e  w(e),
d {e}  {e }  A(G)} (2)

r(G, e, d) d=ef {e =(i A j) | e  b(G, e, d)} (3)
An example of blue and red edges are given in Fig. 4.
Lemma 1. For any graph G, if G(1) = -AG , then for some e  G(1) and e  b(G, e, G(1))

G(2) = G(1) {e}  {e }

(8)

Lemma 1 can be understood more clearly by following the worked example in Fig. 4. The moral of Lemma 1 is that in the base case where there are no critical cycles, we only need to examine the blue edges of the greedy graph to find the second best tree. Furthermore, our second best tree will only differ from our best tree by exactly one blue edge. Camerini et al. (1980) make use of the concepts of the blue and red edge sets, but rather than consider a base case as Lemma 1, they propose an ordering in which to visit the edges of the graph. This results in several properties about the possible orderings,

12We can also define b(G, e, d) as (i A j)  b(G, e, d)  i is an ancestor of j in d and r(G, e, d) as (i A j)  r(G, e, d)  i is a descendant of j in d. This equivalence exists as we can only swap an incoming edge to j in d without introducing a cycle if the new edge emanates from an ancestor of j. The exposition using ancestors and descendants is more similar to the exposition originally presented by Camerini et al. (1980).

requiring much more complicated proofs.

Definition 3 (Swap cost). For any graph G, tree d  A(G), and edge e  d, the swap cost denotes the minimum change to a tree weight to replace e by a single edge in d. It is given by

wG,d(e) = min w(e) - w e

(4)

e b(G,e,d)

We will shorthand wG(e) to mean wG,G(1)(e).
Corollary 1. For any graph G, if G(1) = -AG , then G(2) = (G - e)(1) where e is given by

e = argmin wG e

(5)

e G(1)

Furthermore, w G(2) = w G(1) - wG(e).
Corollary 1 provides us a procedure for finding the best edge to remove to find G(2) as well as its weight in the base case of G having no critical cycles. We next illustrate what must be done in the recursive case when a critical cycle exists.

Lemma 2. For any G with a critical cycle C, either G(2) = (G/C )(2) C (with w G(2) = w (G/C )(2) ) or G(2) = (G - e)(1) (with w G(2) = w G(1) - wG(e)) for some
e  C  G(1).

Combining Corollary 1 and Lemma 2, we can directly modify opt to find the weight of G(2) and the edge we must remove to obtain it. We detail this algorithm as next in Fig. 5.

Theorem 2. For any graph G, executing next(G) returns G(1) and w, e such that G(2) = (G - e)(1) and w G(2) = w.

Runtime analysis. We know that without lines 5, 6, 9 and 10, next is identical to opt and so will run in O(N 2). We call w at most N + 2 times during a full call of next: N times from lines 5 and 9 combined, once from Line 6, and once from Line 10. To find w, we first need to find the set of blue edges, which can be done in O(N ) by computing the reachability graph. Then, we need another O(N ) to find the minimising value. Therefore, next does O(N 2) extra work than opt and so retains the runtime of O(N 2). Camerini et al. (1980) require G(1) to be known ahead of time. This results in having to run the original algorithm in O(N 2) time and then having to do the same amount of work as next because they must still contract the graph. Therefore, next has a constant-time speed-up over its counterpart in Camerini et al. (1979).

G(1), w : 260
 2
4
1 3

G(5), w : 190 +e
 2

G(6), w : 150 +e
 2

4

4

G(2), w : 220 +e 1

3 -e

+e 1

3

 2

e : ( A 1)

G(4), w : 200

e : ( A 1) -e

4



1 3
e : (4 A 3)

G(3), w : 210 +e



-e

2

4

1 3

2
4
1 3
e : (2 A 3)

G(7), w : 130 +e

 2

-e

4

1 3

e : ( A 2) -e

e : ( A 1) -e

Figure 6: Example of running through kbest using the graph of Fig. 2. We start with G(1) that has a weight of 260 and consider the best edge to remove to find G(2). Using next we find that G(2) = (G - e)(1) for e = (4 A 3). We then know that either e  G(3) or e  G(3). We can push these two possibilities to the queue using two calls to next. We find that G(3) comes from the graph without e, and also removes the edge e = ( A 2). We attempt to push two new elements to the queue, but we see that only by including e in the graph can we find another tree. We repeat this process until we have found G(K) or the queue is empty.

1: def kbest(G, K) : 2: G(1), w, e  next(G) 3: yield G(1)

4: Q  priority queue([ w, e, G ])

5: for k = 2, . . . , K :

6:

if Q.empty() : return

7:

w, e, G  Q.pop()

8:

G(k), w , e  next(G - e)

9:

yield G(k)

10:

Q.push( w , e , G - e )

11:

· , w , e  next(G + e)

12:

Q.push( w , e , G + e )

Figure 7: K-best tree enumeration algorithm.
4 Finding the Kth Best Tree
In the previous section, we found an efficient method for finding G(2). We now utilize this method to efficiently find the K-best trees.
Lemma 3. For any graph G and K > 1, there exists a subgraph G  G and 1  l < K such that G(l) = G (1) and G(K) = G (2).
Lemma 3 suggests that we can find the K-best trees by only examining the second best trees of subgraphs of G. This idea is formalized as algorithm kbest in Fig. 7. A walk-through of the exploration space using kbest for our example graph in Fig. 2 is shown in Fig. 6.
Theorem 3. For any graph G and K > 0, at any iteration 1  k  K, kbest(G, K) returns G(k).
Runtime analysis. We call next once at the

Camerini et al. kbest Speed-up

K = 10
6.95 4.89
1.42×

K = 20
14.04 10.10
1.39×

K = 50
35.11 25.63
1.37×

Table 1: Runtime experiment for parsing the K-best spanning trees in the English UD test set (Nivre et al., 2018). Times are given in 10-2 seconds for the average parse of the K-best spanning trees.
start of the algorithm, then every subsequent iteration we make two calls to next. As we have K -1 iterations , the runtime of kbest is O(KN 2). The first call to next in each iteration finds the Kth best tree as well as an edge to remove. Camerini et al. (1980) make one call to of opt and two calls to next which only finds the weight-edge pair of our algorithm. Therefore, kbest has a constant time speed-up on the original algorithm.13 A short experiment. We empirically measure the constant time speed-up between kbest and the original algorithm of Camerini et al. (1980). We take the English UD test set (as used for Fig. 1) and find the 10, 20, and 50 best spanning trees using both algorithms.14 We give the results of the experiment in Tab. 1.15 We note that on average kbest leads to a 1.39 times speed-up. This is
13In practice, we maintain a set of edges to include and exclude to save space.
14Implementations for both versions can be found in our code release (see footnote 1)
15The experiment was conducted using an Intel(R) Core(TM) i7-7500U processor with 16GB RAM.

G[1], w : 210

+e



2

G[3], w : 150

+e



2

4
1 3
e : ( A 1)

G[2], w : 190 +e



-e

2 4

4
1 3
e : (4 A 3)

G[4], w : 130

+e



-e

2 4

1 3
e : ( A 2) -e

1 3
e : (2 A 3) -e

Figure 8: Example of running through kbest dep using the graph of Fig. 2. We start with G[1] that has a weight of 210 and consider the best edge to remove to find G(2). We consider removing the best dependency tree with the same edge emanating from the root e = ( A 1) using next. However, no such dependency tree exists, and so we only need to push the graph G - e. When we next pop from the queue, we see that we have removed root edge e, and so must consider removing the new root edge e = ( A e). In this case, no dependency tree exists without e and e , and so we only push to the queue the results of running next. We repeat this process until we have found G[K] or the queue is empty.

lower than we anticipated as we have to make half as many calls to next than the original algorithm. However, in the original next of Camerini et al. (1980), we do not require to stitch together the tree, which may explain the slightly smaller speed-up.
5 Finding the Kth Best Dependency Tree
In this section, we present a novel extension to the algorithm presented thus far, that allows us to efficiently find the K-best dependency trees. Recall that we consider dependency trees to be spanning trees with a root constraint such that only one edge may emanate from . Na¨ively, we can use kbest where we initialize the queue with (G + e)(1) for each e = ( A j)  G. However, this adds a O(N 3) component to our runtime as we have to call opt N times. Instead, our algorithm maintains the O(KN 2) runtime as the regular K-best algorithm. We begin by noting that we can find second best dependency tree, by finding either the best dependency tree with a different root edge or the second best tree with the same root edge.
Lemma 4. For any graph G and edge e = ( A j)  G[1], G[2] = (G - e)[1] or G[2] = (G + e)[2].
Lemma 5. For any graph G and K > 1, if e = ( A j)  G[K], then either e is not in any of the K-1-best trees or there exists a subgraph G  G and 1  l < K such that G[l] = G [1], e  G [1] and G[K] = G [2].
Lemma 5 suggests that we can find the K-best dependency trees, by examining the second best dependency trees of subgraphs of G or finding the best dependency tree with a unique root edge. This

1: def kbest dep(G, K) :

2: G[1]  opt(G)

3: yield G[1]

4: e  outgoing edge from  in G[1]

5: ·, w, e  next(G + e)

6: d  opt(G - e)

7: Q  priority queue([ w(d), e, G ])

8: Q.push( w, e, G + e ) 9: for k = 2, . . . , K :

10:

if Q.empty() : return

11:

w, e, G  Q.pop()

12:

if e does not emanate from  :

13:

G[k], w , e  next(G - e)

14:

Q.push( w , e , G - e )

15:

· , w , e  next(G + e)

16:

Q.push( w , e , G + e )

17:

else

18:

G[k]  opt(G )

19:

e  outgoing edge from  in G[k]

20:

d  opt(G - e)

21:

Q.push( w(d), e, G - e )

22:

·, w , e  next(G + e)

23:

Q.push( w , e , G + e )

24:

yield G(k)

Figure 9: K-best dependency tree enumeration algorithm.
idea is formalized as algorithm kbest dep in Fig. 9. A walk-through of the exploration space using kbest dep for our example graph in Fig. 2 is shown in Fig. 8.
Theorem 4. For any graph G and K  1, at iteration 1  k  K, kbest dep(G, K) returns G[k].

Runtime analysis. At the start of the algorithm, we call opt twice and next once. Then, at each iteration we either make two calls two next, or two calls to opt and one call to next. As both algorithms have a runtime of O(N 2), each iteration has a runtime of O(N 2). Therefore, running K iterations gives a runtime of O(KN 2).
6 Conclusion
In this paper, we provided a simplification to Camerini et al. (1980)'s O(KN 2) K-best spanning trees algorithm. Furthermore, we provided a novel extension to the algorithm that decodes the K-best dependency trees in O(KN 2). We motivated the need for this new algorithm as using regular K-best decoding yields up to 36% trees which violation the root constraint. This is a substantial (up to 44 times) increase in the violation rate from decoding the one-best tree, and thus such an algorithm is even more important than in the one-best case. We hope that this paper encourages future research in K-best dependency parsing.
Acknowledgments
We would like to thank the reviewers for their valuable feedback and suggestions to improve this work. The first author is supported by the University of Cambridge School of Technology ViceChancellor's Scholarship as well as by the University of Cambridge Department of Computer Science and Technology's EPSRC.
Ethical Concerns
We do not foresee how the more efficient algorithms presented this work exacerbate any existing ethical concerns with NLP systems.
References
Z eljko Agic´. 2012. K-best spanning tree dependency parsing with verb valency lexicon reranking. In Proceedings of COLING.
Eduard Bejcek, Eva Hajicova´, Jan Hajic, Pavl´ina J´inova´, Va´clava Kettnerova´, Veronika Kola´rova´, Marie Mikulova´, Jir´i M´irovsky´, Anna Nedoluzhko, Jarmila Panevova´, Lucie Pola´kova´, Magda S evc´ikova´, Jan S tepa´nek, and S a´rka Zika´nova´. 2013. Prague dependency treebank 3.0.
Paolo M. Camerini, Luigi Fratta, and Francesco Maffioli. 1979. A note on finding optimum branchings. Networks, 9.

Paolo M. Camerini, Luigi Fratta, and Francesco Maffioli. 1980. The k best spanning arborescences of a network. Networks, 10.
Michael Collins and Terry Koo. 2005. Discriminative reranking for natural language parsing. Computational Linguistics, 31.
Gonc¸alo M. Correia, Vlad Niculae, Wilker Aziz, and Andre´ F. T. Martins. 2020. Efficient marginalization of discrete and structured latent variables via sparsity. In Advances in Neural Information Processing Systems: Annual Conference on Neural Information Processing Systems.
Bich-Ngoc Do and Ines Rehbein. 2020. Neural reranking for dependency parsing: An evaluation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.
Timothy Dozat and Christopher D. Manning. 2017. Deep biaffine attention for neural dependency parsing. In Proceedings of the International Conference on Learning Representations.
Harold N. Gabow and Robert Endre Tarjan. 1984. Efficient algorithms for a family of matroid intersection problems. Journal of Algorithms, 5.
Leonidas Georgiadis. 2003. Arborescence optimization problems solvable by Edmonds' algorithm. Theoretical Computer Science, 301.
Keith Hall. 2007. K-best spanning tree parsing. In Proceedings of the Annual Meeting of the Association of Computational Linguistics.
Keith Hall, Jir´i Havelka, and David A. Smith. 2007. Log-linear models of non-projective trees, k-best MST parsing and tree-ranking. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.
Liang Huang and David Chiang. 2005. Better k-best parsing. In Proceedings of the International Workshop on Parsing Technology.
Kenton Lee, Mike Lewis, and Luke Zettlemoyer. 2016. Global neural CCG parsing with optimality guarantees. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.
Xuezhe Ma and Eduard Hovy. 2017. Neural probabilistic model for non-projective MST parsing. In Proceedings of the International Joint Conference on Natural Language Processing.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajic. 2005. Non-projective dependency parsing using spanning tree algorithms. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing.

Joakim Nivre, Mitchell Abrams, Z eljko Agic´, Lars Ahrenberg, Lene Antonsen, Katya Aplonova, Maria Jesus Aranzabe, Gashaw Arutie, Masayuki Asahara, Luma Ateyah, Mohammed Attia, Aitziber Atutxa, Liesbeth Augustinus, Elena Badmaeva, Miguel Ballesteros, Esha Banerjee, Sebastian Bank, Verginica Barbu Mititelu, Victoria Basmov, John Bauer, Sandra Bellato, Kepa Bengoetxea, Yevgeni Berzak, Irshad Ahmad Bhat, Riyaz Ahmad Bhat, Erica Biagetti, Eckhard Bick, Rogier Blokland, Victoria Bobicev, Carl Bo¨rstell, Cristina Bosco, Gosse Bouma, Sam Bowman, Adriane Boyd, Aljoscha Burchardt, Marie Candito, Bernard Caron, Gauthier Caron, Gu¨ls¸en Cebiroglu Eryigit, Flavio Massimiliano Cecchini, Giuseppe G. A. Celano, Slavom´ir C e´plo¨, Savas Cetin, Fabricio Chalub, Jinho Choi, Yongseok Cho, Jayeol Chun, Silvie Cinkova´, Aure´lie Collomb, C¸ agri C¸ o¨ltekin, Miriam Connor, Marine Courtin, Elizabeth Davidson, Marie-Catherine de Marneffe, Valeria de Paiva, Arantza Diaz de Ilarraza, Carly Dickerson, Peter Dirix, Kaja Dobrovoljc, Timothy Dozat, Kira Droganova, Puneet Dwivedi, Marhaba Eli, Ali Elkahky, Binyam Ephrem, Tomaz Erjavec, Aline Etienne, Richa´rd Farkas, Hector Fernandez Alcalde, Jennifer Foster, Cla´udia Freitas, Katar´ina Gajdosova´, Daniel Galbraith, Marcos Garcia, Moa Ga¨rdenfors, Sebastian Garza, Kim Gerdes, Filip Ginter, Iakes Goenaga, Koldo Gojenola, Memduh Go¨kirmak, Yoav Goldberg, Xavier Go´mez Guinovart, Berta Gonza´les Saavedra, Matias Grioni, Normunds Gru¯z¯itis, Bruno Guillaume, Ce´line GuillotBarbance, Nizar Habash, Jan Hajic, Jan Hajic jr., Linh Ha` My~, Na-Rae Han, Kim Harris, Dag Haug, Barbora Hladka´, Jaroslava Hlava´cova´, Florinel Hociung, Petter Hohle, Jena Hwang, Radu Ion, EdelersnaJoIhraimnniase, nO,. Fla´rje´iddre´ikIsJhøorglae,nsTeonm, Ha´su¨nJeerl´iKneaks¸,ikAarna-, Sylvain Kahane, Hiroshi Kanayama, Jenna Kanerva, Boris Katz, Tolga Kayadelen, Jessica Kenney, Va´clava Kettnerova´, Jesse Kirchner, Kamil Kopacewicz, Natalia Kotsyba, Simon Krek, Sookyoung Kwak, Veronika Laippala, Lorenzo Lambertino, Lucia Lam, Tatiana Lando, Septina Dian Larasati, Alexei Lavrentiev, John Lee, Phuong Le^ H`o^ng, Alessandro Lenci, Saran Lertpradit, Herman Leung, Cheuk Ying Li, Josie Li, Keying Li, KyungTae Lim, Nikola Ljubesic´, Olga Loginova, Olga Lyashevskaya, Teresa Lynn, Vivien Macketanz, Aibek Makazhanov, Michael Mandl, Christopher Manning, Ruli Manurung, Catalina Maranduc, David Marecek, Katrin Marheinecke, He´ctor Mart´inez Alonso, Andre´ Martins, Jan Masek, Yuji Matsumoto, Ryan McDonald, Gustavo Mendonc¸a, Niko Miekka, Margarita Misirpashayeva, Anna Missila¨, Catalin Mititelu, Yusuke Miyao, Simonetta Montemagni, Amir More, Laura Moreno Romero, Keiko Sophie Mori, Shinsuke Mori, Bjartur Mortensen, Bohdan Moskalevskyi, Kadri Muischnek, Yugo Murawaki, Kaili Mu¨u¨risep, Pinkey Nainwani, Juan Ignacio Navarro Horn~iacek, Anna Nedoluzhko, Gunta Nespore-Be¯rzkalne, Luong Nguy~e^n Thi., Huy`e^n Nguy~e^n Thi. Minh, Vitaly

Nikolaev, Rattima Nitisaroj, Hanna Nurmi, Stina Oovjaa,laR, oAbdee´rdt aOy¨ os.tlOinlgu´,o`kLuinlj,aMØavireOlimd,urNa,ikPoetPyaartOanseenn-, Elena Pascual, Marco Passarotti, Agnieszka Patejuk, Guilherme Paulino-Passos, Siyao Peng, CenelAugusto Perez, Guy Perrier, Slav Petrov, Jussi Piitulainen, Emily Pitler, Barbara Plank, Thierry Poibeau, Martin Popel, Lauma Pretkalnin¸a, Sophie Pre´vost, Prokopis Prokopidis, Adam Przepio´rkowski, Tiina Puolakainen, Sampo Pyysalo, Andriela Ra¨a¨bis, Alexandre Rademaker, Loganathan Ramasamy, Taraka Rama, Carlos Ramisch, Vinit Ravishankar, Livy Real, Siva Reddy, Georg Rehm, Michael Rießler, Larissa Rinaldi, Laura Rituma, Luisa Rocha, Mykhailo Romanenko, Rudolf Rosa, Davide Rovati, Valentin Ros, ca, Olga Rudina, Jack Rueter, Shoval Sadde, Beno^it Sagot, Shadi Saleh, Tanja Samardzic´, Stephanie Samson, Manuela Sanguinetti, Baiba Saul¯ite, Yanin Sawanakunanon, Nathan Schneider, Sebastian Schuster, Djame´ Seddah, Wolfgang Seeker, Mojgan Seraji, Mo Shen, Atsuko Shimada, Muh Shohibussirri, Dmitry Sichinava, NatSailmiakSo´i,lvMeia´rrai,aMSaimriakoSvima´,i,KRiraidluSSimimoivo,nAesacruo,nKSamtailtihn, Isabela Soares-Bastos, Carolyn Spadine, Antonio Stella, Milan Straka, Jana Strnadova´, Alane Suhr, Umut Sulubacak, Zsolt Sza´nto´, Dima Taji, Yuta Takahashi, Takaaki Tanaka, Isabelle Tellier, Trond Trosterud, Anna Trukhina, Reut Tsarfaty, Francis Tyers, Sumire Uematsu, Zdenka Uresova´, Larraitz Uria, Hans Uszkoreit, Sowmya Vajjala, Daniel van Niekerk, Gertjan van Noord, Viktor Varga, Eric Villemonte de la Clergerie, Veronika Vincze, Lars Wallin, Jing Xian Wang, Jonathan North Washington, Seyi Williams, Mats Wire´n, Tsegay Woldemariam, Tak-sum Wong, Chunxiao Yan, Marat M. Yavrumyan, Zhuoran Yu, Zdenek Z abokrtsky´, Amir Zeldes, Daniel Zeman, Manying Zhang, and Hanzhi Zhu. 2018. Universal dependencies 2.3. LINDAT/CLARIN digital library at the Institute of Formal and Applied Linguistics (U´ FAL), Faculty of Mathematics and Physics, Charles University. Adam Pauls and Dan Klein. 2009. K-best A* parsing. In Proceedings of the Joint Conference of the Annual Meeting of the ACL and the International Joint Conference on Natural Language Processing of the AFNLP. Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and Christopher D. Manning. 2020. Stanza: A Python natural language processing toolkit for many human languages. In Proceedings of the Association for Computational Linguistics: System Demonstrations. Federico Sangati, Willem Zuidema, and Rens Bod. 2009. A generative re-ranking model for dependency parsing. In Proceedings of the International Conference on Parsing Technologies. Libin Shen, Anoop Sarkar, and Franz Josef Och. 2004. Discriminative reranking for machine translation. In Proceedings of the Human Language Technology

Conference of the North American Chapter of the Association for Computational Linguistics. Robert Endre Tarjan. 1977. Finding optimum branchings. Networks, 7. Hui Zhang, Min Zhang, Chew Lim Tan, and Haizhou Li. 2009. K-best combination of syntactic parsers. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Chenxi Zhu, Xipeng Qiu, Xinchi Chen, and Xuanjing Huang. 2015. A re-ranking model for dependency parser with recursive convolutional neural network. In Proceedings of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing, volume 1. Ran Zmigrod, Tim Vieira, and Ryan Cotterell. 2020. Please mind the root: Decoding arborescences for dependency parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.

A Supplementary Materials for Section 1 (Introduction)

Results Table for Fig. 1

Language
Czech Russian Estonian Korean Latin Norwegian Ancient Greek French Spanish Old French German Polish Hindi Catalan Italian English Dutch Finnish Classical Chinese Latvian Bulgarian Slovak Portuguese Romanian Japanese Croatian Slovenian Arabic Ukrainian Basque Hebrew Persian Indonesian Danish Swedish Old Church Slavonic Urdu Chinese Turkish Gothic Serbian Galician North Sami Armenian Greek Uyghur Vietnamese Afrikaans Wolof Maltese Telugu Scottish Gaelic Hungarian Irish Tamil Marathi Belarusian Lithuanian Kazakh Upper Sorbian Kurmanji Buryat Livvi

|Train|
68495 48814 24633 23010 16809 15696 15014 14450 14305 13909 13814 13774 13304 13123 13121 12543 12264 12217 11004 10156 8907 8483 8328 8043 7125 6914 6478 6075 5496 5396 5241 4798 4477 4383 4303 4124 4043 3997 3664 3387 3328 2272 2257 1975 1662 1656 1400 1315 1188 1123 1051 1015
910 858 400 373 319 153 31 23 20 19 19

|Test|
10148 6491 3214 2287 2101 1939 1047 416 1721 1927 977 1727 1684 1846 482 2077 596 1555 2073 1823 1116 1061 477 729 550 1136 788 680 892 1799 491 600 557 565 1219 1141 535 500 983 1029 520 861 865 278 456 900 800 425 470 518 146 536 449 454 120 47 253 55 1047 623 734 908 106

Root Constraint Violation Rate (%) K = 1 K = 5 K = 10 K = 20 K = 50

0.45 0.49 0.93 0.96 0.52 0.52 0.57 1.68 0.17 0.52 1.54 0.00 0.18 0.54 0.21 0.48 0.67 0.39 0.96 0.88 0.27 0.38 0.42 0.41 0.00 0.88 0.38 0.29 0.90 0.67 1.02 0.67 1.26 0.53 1.23 1.05 1.12 1.80 2.54 0.78 0.19 1.16 1.27 0.00 0.44 0.56 3.38 6.35 1.49 0.58 0.00 0.75 4.23 2.42 0.00 2.13 0.79 7.27 2.58 6.42 23.57 6.61 12.26

5.07 5.07 5.59 6.68 5.17 4.26 4.74 3.85 2.25 6.81 5.12 4.76 1.34 2.32 4.02 9.12 3.39 4.72 22.52 7.05 4.66 4.81 3.31 1.26 5.13 2.90 2.66 3.79 7.49 3.64 2.81 2.43 4.06 4.35 4.63 14.32 2.47 4.80 12.47 8.65 2.04 2.07 7.49 7.34 3.20 7.18 6.78 13.65 6.89 5.17 27.81 7.16 7.44 7.14 1.17 20.85 5.61 9.82 7.97 9.34 27.06 10.37 14.15

6.18 6.58 7.02 9.51 5.57 5.20 7.00 4.95 3.25 9.41 6.37 7.86 2.19 2.97 5.66 10.73 4.18 6.12 25.95 8.77 6.73 5.34 4.15 1.66 6.24 3.71 3.53 4.15 9.15 5.06 4.01 3.47 5.48 5.59 6.08 17.64 3.08 5.90 15.53 11.18 2.60 2.36 10.15 8.42 4.19 9.64 8.25 14.73 8.32 6.70 32.81 8.97 8.66 8.68 1.50 21.70 9.05 10.36 10.68 10.72 29.22 13.00 15.00

6.76 7.66 8.24 11.91 6.25 6.22 8.38 5.81 3.96 11.38 7.63 10.11 2.98 3.68 7.25 11.12 4.82 7.39 28.09 9.95 8.16 5.29 4.76 2.16 7.20 4.44 4.59 4.72 10.13 6.67 5.04 4.28 6.65 6.35 7.09 19.88 3.60 7.68 17.09 13.10 3.16 2.88 12.43 9.64 4.80 12.24 9.56 16.12 9.91 8.12 36.16 10.20 9.82 10.23 1.83 27.34 8.99 10.82 13.45 11.78 30.87 15.48 15.99

7.67 8.99 9.42 14.74 7.62 7.38 10.69 6.98 4.89 13.01 9.06 13.00 4.04 4.51 9.19 11.34 5.59 9.15 29.91 11.31 10.29 5.29 5.75 2.81 8.79 5.62 5.79 5.27 11.72 8.71 5.90 5.25 8.25 7.45 8.73 22.05 4.39 9.31 18.73 14.86 4.23 3.46 15.54 10.81 5.82 15.57 11.39 18.26 12.17 9.73 36.99 11.75 10.75 11.73 3.05 33.36 7.27 12.47 17.41 13.45 33.33 19.13 17.68

B Supplementary Materials for Section 3 (Finding the Second Best Tree)

Theorem 1. For any graph G and e  G(1)

G(2) = (G - e)(1)

(6)

where

e = argmax w (G - e )(1)

(7)

e G(1)

Proof. There must be at least one edge e  G(1) such that e  G(2). Therefore, there exists an e  G(1) such that G(2) = (G - e)(1). Now suppose by way of contradiction that e is not as given in (7). If we choose an e that satisfies (7), then by definition w (G - e )(1) > w (G - e)(1) . As
(G - e )(1) = G(1), we arrive at a contradiction.

Lemma 1. For any graph G, if G(1) = -AG , then for some e  G(1) and e  b(G, e, G(1))

G(2) = G(1) {e}  {e }

(8)

Proof. By Theorem 1, we have G(2) = (G - e)(1) where e = (i A j) is chosen according to (7). Consider

the

graph

G

-

e;

we

have

that

---A G-e

=

G(1)

{e}  {e } where e is the second best incoming edge to j

in G by the definition of the greedy graph.

1.

Case e



b(G,

e,

G(1)):

Then

---A G-e

is

a

tree

and

(G

-

e)(1)

=

-G---Ae.

2.

Case e



r(G,

e,

G(1)):

Then,

---A G-e

has

a

cycle

C

by

construction.

Since this is a greedy graph,

cycle C is critical. In the expansion phase of the 1-best algorithm, we will break the cycle C.

(a) Case break C at j: Then, e  (G - e)(1) and we must choose an edge e = (i A j) to be in (G - e)(1). We require that e  b(G, e, G(1)) as we would otherwise re-introduce a cycle in the expansion phase, which is not possible. Therefore, G(2) = G(1) {e}  {e }.
(b) Case break C at j = j: Then, there exists an edge f = (i A j )  C (and in G(1)) which is not in G(2). Instead, we choose f = (i A j ) to be in G(2). Therefore, G(2) = G(1) {e, f }  {e , f }. However, it is not possible for f and e to form a cycle and so d = G(1) {f }  {f }  A(G) and w(d) > w G(2) . This is a contradiction as only
w G(1) > w G(2) .

Lemma 2. For any G with a critical cycle C, either G(2) = (G/C )(2) C (with w G(2) = w (G/C )(2) ) or G(2) = (G - e)(1) (with w G(2) = w G(1) - wG(e)) for some e  C  G(1). Proof. It must be that G(2) = (G/C )(2) C or G(2) = (G/C )(2) C.
1. Case G(2) = (G/C)(2) C: Since the weight of a tree is preserved during expansion, we are done. 2. Case G(2) = (G/C )(2) C: Then, for all e  (G/C )(1), (e )  G(2). Therefore, if j is the entrance
site of C in (G/C )(1), G(2) = ((G/C )(1))  Cj(2). As Cj(1) = -CAj, by Corollary 1, Cj(2) = (Cj - e)(1) for e  Cj(1) and w Cj(2) = w Cj(1) - wCj (e). Thus, G(2) = (G - e)(1) where e  C  d and w G(2) = w G(1) - wG(e).
Theorem 2. For any graph G, executing next(G) returns G(1) and w, e such that G(2) = (G - e)(1) and w G(2) = w.

Proof. next(G) returns G(1) by the correctness of opt. We prove that w, e satisfy the above conditions.
1. Case G(1) = -AG : Then, by Corollary 1 we can find the best edge to remove and the weight of G(2). 2. Case G(1) = -AG: Then, G has a critical cycle C. By Lemma 2, we can either recursively call
next G/C or examine the edges in C  G(1) to find the best edge to remove and the weight of G(2).
C Supplementary Materials for Section 4 (Finding the Kth Best Tree)
Lemma 3. For any graph G and K > 1, there exists a subgraph G  G and 1  l < K such that G(l) = G (1) and G(K) = G (2).
Proof. There must exist some subgraph G  G such that G(K) = G (2). Suppose by way of contradiction that there does not exist an l < K such that G(l) = G (1). However, since w G (1) > w G(K+1) , G (1) must be in the K-highest weighted trees. Therefore, there must exist an l such that G(l) = G (1)
Theorem 3. For any graph G and K > 0, at any iteration 1  k  K, kbest(G, K) returns G(k).
Proof. We prove this by induction on k. Base Case: Then, k = 1 and G(1) is returned by Theorem 2. Inductive Step: Assume that for all l  k, at iteration l, G(l) is returned. Now consider iteration k + 1, by Lemma 3, we know that G(k+1) = G (2) where G (1) = G(l) for some l  k. By the induction hypothesis, G(l) is returned at the lth iteration, and by Theorem 2, we have pushed G (2) onto the queue. Therefore, we will return G(k+1).
D Supplementary Materials for Section 5 (Finding the Kth Best Dependency Tree)
Lemma 4. For any graph G and edge e = ( A j)  G[1], G[2] = (G - e)[1] or G[2] = (G + e)[2]. Proof. If e  G[2], then clearly G[2] = (G - e)[1]. Otherwise, e  G[2]. As e  G[1], G[2] = (G + e)[2].
Lemma 5. For any graph G and K > 1, if e = ( A j)  G[K], then either e is not in any of the K -1-best trees or there exists a subgraph G  G and 1  l < K such that G[l] = G [1], e  G [1] and G[K] = G [2].
Proof. It must be that either there exists an 1  l < K such that e  G[l] (Case 1) or no such l exists (Case 2).
1. Consider the graph G + e. Under our definition of edge-inclusion graphs for dependency trees, A(G + e) = D(G + e). Then, by Lemma 3, there exists a l and G such that (G[l ] = G [1] and G[K] = G [2].
2. Then, e is not in any of the (K -1)-best trees.
Theorem 4. For any graph G and K  1, at iteration 1  k  K, kbest dep(G, K) returns G[k].
Proof. We prove this by induction on k. Base Case: Then, k = 1 and G(1) is returned by the correctness of opt. Inductive Step: Assume that for all l  k, at iteration l, G[l] was returned. Now consider iteration k + 1, by Lemma 5, we know that either G[k+1] has a unique root edge to the k-best trees (Case 1) or e = ( A j) and there exists a G and l  k such that G (1) = G(l), e  G(l), and G[k+1] = G [2] (Case 2).

1. There always exists a tree in the queue that has a unique root edge to all trees that came before it. Furthermore, it is the highest such tree by the correctness of opt.
2. By our induction hypothesis, G[l] is returned at the lth iteration, and by Theorem 2, we have pushed G + e[2] onto the queue. Therefore, we will return G[k+1].

