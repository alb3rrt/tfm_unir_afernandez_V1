ClustRank: a Visual Quality Measure Trained on Perceptual Data for Sorting Scatterplots by Cluster Patterns
Mostafa Abbas, Ehsan Ullah, Abdelkader Baggag, Halima Bensmail, Michael Sedlmair, Michae¨ l Aupetit
Abstract--Visual quality measures (VQMs) are designed to support analysts by automatically detecting and quantifying patterns in visualizations. We propose a new data-driven technique called ClustRank that allows to rank scatterplots according to visible grouping patterns. Our model first encodes scatterplots in the parametric space of a Gaussian Mixture Model, and then uses a classifier trained on human judgment data to estimate the perceptual complexity of grouping patterns. The numbers of initial mixture components and final combined groups determine the rank of the scatterplot. ClustRank improves on existing VQM techniques by mimicking human judgments on two-Gaussian cluster patterns, and gives more accuracy when ranking general cluster patterns in scatterplots. We demonstrate its benefit by analyzing kinship data for genome-wide association studies, a domain in which experts rely on the visual analysis of large sets of scatterplots. We make the three benchmark datasets and the ClustRank VQM available for practical use and further improvements.
Index Terms--Visual Quality Measure, Cluster Pattern, Data-Driven, Gaussian Mixture Model, Perceptual Data.

arXiv:2106.00599v1 [cs.HC] 1 Jun 2021

1 INTRODUCTION
Cluster discovery is a typical task in visual data analysis [14, 35]. Clusters can have various shapes, densities, and other characteristics [40], and may exist in different sub-spaces of the same data.
Several pipelines for cluster analysis exist in Visual Analytics [50]. One way to discover clusters in high-dimensional (HD) spaces is to use multidimensional projection techniques [34], RadViz [24], or star coordinate plots [37], and examine the resulting scatterplots looking for grouping patterns that could support the existence of their highdimensional counterpart.
However, as any of these projections generates artifacts [34, 37], it is unlikely for one view to be enough to visualize and discover all the actual high-dimensional clusters. Moreover, clusters may exist only in some sub-spaces of the original HD data space. Hence, visual cluster analysis requires generating projections from possibly many (weighted) combinations of the initial features, and different tuning of the parameters of projection techniques. Usually, this task is done using specific interactive visual exploration techniques [12, 14, 25, 47], automated Projection Pursuits [21], Grand Tours [13], or through a bulk approach [16, 43] letting the user skim through all of the visualizations rendered as small images. All these techniques allow the user to spot the most interesting patterns for further investigation.
With the increasing dimensionality of the data this process becomes tedious and cumbersome though. Visual Quality Measures [10] can support the user in such situations by automatically detecting and quantifying visual patterns [32, 51]. Ranking visualizations by order of interest with respect to a specific type of pattern lets the user focus her limited time budget on the most promising views. Recently, ClustMe [1] has been proposed to rank scatterplots with cluster patterns as they would be perceived by human subjects. ClustMe proved to be better than standard clustering techniques used as VQM, like x-means [36], CLIQUE [2], DBSCAN [18], and the Clumpiness Scagnostic measure [51] .
Yet, also ClustMe only scored 0.671 Vanbelle Kappa [46] in substantial agreement with human rankings for cluster patterns in scatterplots, leaving ample room for improvement. In this work, we set out to en-

hance ClustMe by developing ClustRank, an end-to-end, data-driven approach.
Both the original ClustMe and the new ClustRank, are based on the same processing pipeline (see Figure 1). The first density modeling stage uses a two-dimensional Gaussian Mixture Model (GMM) of the points in the scatterplot. In the following components merging stage the intermediate Gaussian components get agglomerate to represent clusters with more complex shapes (see Figure 3). Finally, the cluster counting stage determines the numbers of components before and after merging, and in doing so can account for the complexity of detected cluster patterns. These numbers are paired to form the ClustMe and ClustRank scores of the scatterplot. ClustRank differs from ClustMe by the merging stage.
For the original ClustMe [1] merging stage, a human subject experiment has been conducted, and the results were used to decide about the best among a small set of 7 state-of-the-art heuristic merging techniques.
The main goal of our new novel approach is to improve exactly this step. To this end, the ClustRank merging stage (Figure 3), uses a datadriven approach and learns the parameters of a merging prediction model directly from human-subject data. The first challenge is to determine the data space into which the merging model could be trained and to align the available data coming from the ClustMe [1] user experiment not initially designed for that purpose. The second challenge is to augment the available data [42] to better cover the data space and get an optimal model (Figure 5). The third challenge is to train and select the best merging model on available data and compare it to existing techniques.
In summary, our contributions are the following:
1. Data alignment: We designed a specific alignment process of human judgments data available from the former ClustMe experiment [1], with the GMM encoding of the input scatterplots.

· M. Abbas, E. Ullah, A. Baggag, H. Bensmail and M. Aupetit are with Qatar Computing Research Institute, Hamad Bin Khalifa University, Qatar. Emails: {mohamza, eullah, abaggag, hbensmail, maupetit}@hbku.edu.qa
· M. Sedlmair is with VISUS, University of Stuttgart, Germany. E-mail: Michael.Sedlmair@visus.uni-stuttgart.de
Manuscript received xx xxx. 201x; accepted xx xxx. 201x. Date of Publication xx xxx. 201x; date of current version xx xxx. 201x. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org. Digital Object Identifier: xx.xxxx/TVCG.201x.xxxxxxx

2. Data augmentation: We designed a specific data augmentation process of human judgments data to ensure a better generalization of the predictive model.
3. Merging model training and selection: We developed a predictive merging model (classifier) based on the 34 000 merging judgments from the ClustMe experiment [1]. We aligned and augmented this data to automatically predict the merging decision that a human subject would take on unseen scatterplots. We trained 320 different predictive models and chose the best one to form the merging stage of ClustRank.

The best merging model turned out to be a tree-bag with specific pre-processing. It achieves 0.97 Matthew's correlation with human judgments on test data, the highest score among all 320 models. It also gets a 0.962 Vanbelle Kappa index (Almost perfect agreement), improving by 22% the 0.786 index (Substantial agreement) of ClustMe's best merging heuristic (Demp). We use the resutling ClustRank VQM to rank 435 pairs of scatterplots of real scatterplots from a second benchmark dataset [1] and got a 0.727 Vanbelle Kappa index. This result improves the current top score of 0.671 provided by CLustMe. We also demonstrate the use of ClustRank in a real use case application of genome-wide association data analysis.
2 RELATED WORK
We review related work on VQMs designed to detect and quantify cluster patterns, VQMs built from data rather than heuristics, and merging decision techniques used in Gaussian Mixture Models specific to the ClustRank approach.
2.1 Visual Quality Measure for Clustering
Visual cluster patterns have been taxonomized [41] and empiric ally studied [35]. These works show the rich variety of characteristics, demonstrating how challenging it is to develop VQMs for such loosely defined types of pattern. Several approaches have been proposed to design Visual Quality Measures for grouping patterns each focusing on some specific definition. The Clumpiness measure [44] detects clumps in a scatterplot. It is part of the Scagnostics scatterplot descriptors [51]. Other VQM approaches are based on CLIQUE clustering [26]. Existing VQMs are mostly heuristics loosely related to human perceptual data. For instance, Pandey et al. [35] showed that Scagnostics are not wellrelated to their participants judgements (and were never specifically designed for that). In contrast, ClustRank is a data-driven VQM directly optimized to mimic human judgments.
2.2 Data-driven VQMs
In contrast to heuristics-base approaches, data-driven approaches like ScatterNet [31] or perception-based VQMs [3] get trained on human judgment data. Recent work on data-driven approaches showed that fine-tuning a VQM on a specific pattern to mimic perceptual judgments outperforms heuristic techniques not based on such data. This is, for instance, the case for class separation measures for class color-coded scatterplots [4, 39]. These data-driven VQMs led to new applications in supervised dimensionality reduction of labeled data [49] and color optimization for scatterplots [48].
Regarding cluster patterns, X-means [36], DBSCAN [18], and CLIQUE [2] clustering techniques, and Clumpiness [51] VQM, have been compared to the ClustMe data-driven VQM [1] on two human judgment benchmark datasets. ClustMe outperforms all others in terms of Vanbelle kappa agreement index.
Among all these approaches, only ScatterNet [31] relies on a parametric model (auto-encoder) of human-subject judgments. Parameters of the model are optimized to predict pairwise similarity judgments between monochrome scatterplots. But no such model exists for quantifying grouping patterns.
ClustMe [1] is focused on grouping patterns but its parameters are not estimated directly from the human-subject data. Instead, the data has been used only for the choice of the best merging technique called Demp, among a finite set of seven other heuristics [23]. As a result, the best merging heuristics is in substantial agreement with human judgments. We propose ClustRank as a new parametric technique to quantify cluster patterns in monochrome scatterplots. ClustRank fully exploits the human-subject data to optimize the parameters of the merging part of its process. As a result the merging model in ClustRank reaches an almost perfect agreement with the same human judgment data.
2.3 Merging Decision in Gaussian Mixture Models
The former ClustMe [1] and the proposed ClustRank both use a twodimensional Gaussian Mixture Model [9, 20] to model the distribution of the points in the scatterplot. The number of components (clusters)

is found with the Bayesian Information Criterion (BIC) [38]. Subsequently, each pair of Gaussian components of this mixture is considered for merging by the merging part of the process. Several merging techniques exist in the literature [23]. All of them are heuristics which do not involve perceptual judgment. In ClustMe, human judgment data are used as a ground-truth to compare the different heuristics. In ClustRank, we consider for the first time a predictive approach to learn the parameters of the merging model from human judgment data.
3 CLUSTRANK: PRINCIPLE AN DESIGN
We first define the ClustRank process and how it is related to ClustMe. Then we present the pre-processing and training prototcol it requires.
ClustRank follows the same clustering pipeline as ClustMe [1], which is made of three stages, as illustrated in Figure 1:
1. Density Modeling (Figure 1a) the parameters of a Gaussian Mixture Model (GMM) [9, 20] are estimated using ExpectationMaximization inference. The optimal model maximizes the likelihood of having generated the points of the scatterplot. The Bayesian Information Criterion [38] is used to select the optimal number K of Gaussian components.
2. Components Merging (Figure 1b) pairs of "overlapping" GMM components are "merged" using a merging decision function G , operating in the parametric space of these pairs. Several existing heuristics for G are analyzed by Hennig [23], and are used in the previous ClustMe [1]. In contrast, in ClustRank, G is learned directly from human judgment data.
3. Cluster Counting (Figure 1c) the number of components before (K) and after (M) merging are counted. The (M, K) pair defines the ClustRank VQM for grouping patterns used to rank the scatterplots by M first, and then by K for equal M, as:
Simple patterns  (1, 1)  (1, 2)  · · ·  (1; K)  (2, 2)  . . . · · ·  (M, K)  · · ·  (K, K)  Complex patterns

In ClustRank, the main novelty comes from the specific "Components Merging" stage (Figure 2) which relies on training and exploiting a model of human judgment data rather than on using a state-of-the-art merging heuristic. We first detail the Gaussian Mixture Model and its parameters, a necessary step before explaining the ClustRank merging model itself.

3.1 Gaussian Mixture Models for Density Modeling
As input to the ClustRank process, we consider a set of N points X = {x1, . . . , xN }  (R2)N in a 2 dimension real space, represented graphically as dots in the scatterplot image SP(X). We ignore the image representation (pixel space) of the scatterplot on purpose, working directly with the N underlying points X to avoid image artefacts due to occlusion, mark shapes, or pixel binning.
A standard Gaussian Mixture Model [20] M is a probabilistic modelbased clustering technique which assumes points are sampled from a convex combination of K components with D-variate Gaussian distributions (D = 2 in the sequel). The probability density at point x  R2 is estimated given some model parameters  by:

K

K

p(x|K) =  kg(x, µk, k) =  kgk(x)

(1)

k=1

k=1

with K = (1, . . . , K , µ1, . . . , µK , 1, . . . , K ) where Kk=1 k = 1. We have also R2 p(x| )dx = 1. The parameter k  [0, 1] represents the prior probability that component gk generates some data, and µk  R2 and k  R2×2 are mean vector and variance matrix of the bivariate Gaussian distribution gk(x) = g(x, µk, k) with:

g(x,

µ

,

)

=

det(2

)-

1 2

e-

1 2

(x-µ

)

-1 (x-µ )

(2)

For clustering purpose, it is assumed that each component of the model represents a "natural" cluster in the data space. Typically, a point

Benchmark data N

Sampling of 2-component GMM

X

X

Scatterplots 

SP(X)

Human Judgment =1/>1 cluster

H(SP(X))

Labeled Scatterplots , X , H

VQM pipeline ClustMe/ClustRank

Sampling from X

unknown

Scatterplot

distribution

SP(X)

X

Density Modeling GMM EM+BIC *

K*
Components Merging
Pairwise (u,v) merging decision

Cluster

Counting

K*

M

VQM (M,K*)

Fig. 1.

Process to generate benchmark data and

ClustMe/ClustRank VQM pipelines: 1000 benchmark data (la-

beled scatterplots) from [1] were generated from varying the parameters

of a mixture of 2 Gaussians components evaluated by 34 subjects for

seeing "one" or "more-than-one" clusters (left). The VQM pipeline of

both ClustMe and ClustRank (right) is the same except for their merging

process (purple) (See figure 2).

X for a given set of parameters:

N

K = arg max L (X, K ) = arg max  log p(xi|K )

(3)

K

K i=1

The best model complexity K is then selected among all likelihoodoptimal models, maximizing the Bayesian Information Criterion (BIC):

K

=

arg

max(BICK )
K

=

arg

max(2L
K

(X ,

K

)

-

K

log(N))

(4)

The final model M (X) determines the density function p(x|K ) which "best" approximates the density of the points X in the scatterplot SP(X). We use K as the first part of the ClustRank VQM.

3.2 Merging components of the mixture
Each pair of components (u, v)  {1, . . . , K}2, u = v of M  is considered independently. When two components overlap "too much", points generated by one or the other component as per the MAP
rule should be eventually assigned to the same cluster. The deci-
sion to merge (1) or not (0) the two components is taken by a binary function G : G (X, K , u, v)  {0, 1}. We compile all the pairwise merging decisions into a binary matrix G  {0, 1}K×K with Gu,v = Gv,u = G (X, K , u, v). The final clusters are formed by the connected components of G viewed as the adjacency matrix of a graph. The number M of connected components of G measures the complex-
ity of the grouping structure in the scatterplot after merging. We use M as the second part of the ClustRank VQM.
All state-of-the-art merging techniques [23] to implement G are based on heuristics or grounded in statistics and Information Theory. ClustMe [1] relies on Demp, one of the merging techniques studied by
Hennig [23] which best matches with human judgments (Figure 2 left). ClustRank instead, uses a classifier trained on these human judgment data, to take the merging decision and finally count the merged clusters
(Figure 2 right).

Fig. 2. ClustRank uses the best among 320 classifiers trained on human labeled scatterplots, instead of 7 merging heuristics for ClustMe: "Labeled Scatterplots" show black points sampled from a 2-component GMM, and come with 34 human judgments ("one" or "morethan-one" clusters) translated into a merging decision ("merge" or "not merge"). ClustMe ran 7 merging heuristics on a set of GMM parameters uv and their sampled points X, and compared the merging decision with human judgments. In ClustRank, we optimize the parameters of 320 merging models G (supervised classifiers) on labeled data made of pairs of GMM parameters (features uv) and human judgments (labels H). The objective is to predict what would be the human merging decision H^ = G (^ uv) for any new input pair of components in a GMM ^ uv.

3.3 The ClustRank merger

We can view a GMM as a projection technique, embedding the underly-

ing points X of a scatterplot scatterplot is encoded as a rameters (equations (3) and

s(i4pn)et)ocditfiehcpee"GnodMpintMigmopanla"rKasme,tettheKersopopafticm5eKalKn-u: m1eabpceahr-

of components for that scatterplot. The dimension of that embed-

ding space varies with each scatterplot, making it difficult to define a

common space to represent all of them.

However, if we consider any pair (u, v) of Gaussian compo-

nents independently, the subspace spanned by related parameters uv has a fixed dimension (|uv| = 14) independently of K: uv =

(u, v, µu, µv, u, v). uv can be further reduced to a set of 8 indepen-

dent real parameters (Figure 4) due to cross-dependencies:

uv = (, µ, ux, uy, vx, vy, u, v)  [0, 1] × (R+)5 × [0, /2]2 (5)

where  = u/(u + v), and µ = ||µv - µu||. .. and . come from

the Singular Value Decomposition of the covariance i (i  {u, v})

into the diagonal "scaling" matrix of eigenvalues Si and the "rotation"

matrix of eigenvectors Ri: with independent scales

i ix

=anRdiSi2iRy Tia.loSnigisxa

diagonal scaling matrix and y orthogonal axes

respectively. This gives an elliptic shape to the cluster with width and

length driven by x and y, whenever ix = iy. Ri is a rotation matrix of

angle i which orients the elliptic shape with respect to the x-axis:

x is assigned to a cluster cx using the Maximum A Posteriori (MAP) rule: cx = arg maxk{1,...,K} kgk(x).
In our case, we do not compute cluster assignment but only count the number K of clusters, i.e., the number of components of the generative model. Expectation-Maximization (EM) is typically used to estimate the optimal parameters K for a given K, which maximizes the loglikelihood, i.e., a goodness-of-fit measure of the model p to the sample

Si =

ix 0

0
y
i

Ri =

cos i - sin i sin i cos i

(6)

This parameter subspace S represents all possible mixtures of 2 bivariate Gaussian distributions. A point in that space determines a unique mixture distribution from which one can randomly sample N points to form a 2D scatterplot with a unique cluster pattern up to the sampling variation and some symmetry we will explain below. As

Fig. 3. ClustRank merging decision is taken by a classifier trained on human labeled scatterplots: A bivariate Gaussian Mixture Model (1) estimates the distribution of the points in a scatterplot. Each possible pair of its K Gaussian components is evaluated for merging (2). For
that purpose, a binary classifier G has been trained in the 8 dimensional parameter space uv of such a pair of Gaussian components. In that space, a pair of components is represented as a single point (red and blue dots on the right). A set of 1000 such points have been labeled in a
previous experiment [1] by 34 human subjects. Their task was to label scatterplots (Solid red and blue frames) generated by such pairs of Gaussian
components regarding if they can see "one" (Red) or "more-than-one" (Blue) clusters in them. After training, the classifier G can predict automatically the merging decision (Purple solid line separating blue and red areas) that humans would take for any new scatterplot (Dashed purple frame). This merging decision (3) taken for each pair of components of the initial GMM generates M connected components. The ClustRank score of the initial scatterplot is given by the pair (K, M), the higher the score, the more complex the grouping pattern in the scatterplot.

Fig.

4.

Illustration

of

the

parameters

uv

=

( ,

µ

,

ux ,

uy ,

vx ,

y
v

,

u

,

v

)

of

a

pair of Gaussian components (u, v) of M , spanning the feature space

S (Figure 3) input of the classifier GClustRank taking decision of merging u

and v.

4 ESTIMATING THE PARAMETERS OF CLUSTRANK FROM HU-
MAN JUDGMENT DATA
In this section, we consider the human subject data collected in the first experiment of the ClustMe project [1]
These data uv  S m are m distinct parameter vectors uv of a 2-dimensional GMM with 2 components u and v, having generated m scatterplots SP(X) with X  p(x|uv)), or SP(uv) for short, and sampling the 8-dimension space S that we propose to use as input for the ClustRank merger. Moreover, a set of 34 human subjects' binary judgments has been assigned to each of the scatterplots SP(uv) indicating if the subjects see one or more-than-one clusters therein.
In order to get the merging function GClustRank from uv and human judgments, we follow 4 steps:
1. Summarize the human judgments of each data into a binary class Hi representing the merging decision;

2. Align training and new data in the space S to allow exploiting this classifier with the uncharted parameters K of GMM modeling new scatterplots;

illustrated in Figure 3, in some part of this space S , and as per human perceptual judgment, the generated scatterplots will show 2 clearly separated Gaussian clusters. In some other part of S , the scatterplots will show a single blob of two strongly overlapping Gaussian distributions. Moreover, we can assign label 0 or 1 to the scatterplots for which most humans judge they are showing "more-than-one" or "one" clusters respectively, hence coding for the decision to not merge or merge respectively. Then it becomes clear that we can train a classifier in the space S to model this human judgment and to take merging decision automatically for any possibly yet-unseen pair (u, v) of GMM components projected in S . This automatic classifier GClustRank : S  {0, 1} forms the core of the ClustRank VQM. Figure 3 illustrates the full ClustRank merging process.

3. Augment training data to better cover the space S , ensuring better generalization;
4. Train the classifier with standard cross-validation and testing approaches.
4.1 Summarizing Human Judgments
We summarize the 34 human judgments of one scatterplot SP(i) generated by a sample data i into a single binary class Hi  {0, 1} by applying a majority vote. Label Hi = 0 (do not merge) is assigned to a data instance i  uv if most of the judgments on SP(i) are more-than-one cluster. Label Hi = 1 (merge) is assigned otherwise.
A ClustRank training data i is then a pair (i, Hi)  Xuv of the labeled data set Xuv.

4.2 Data Alignment with the ClustRank Feature Space

Parameters  and  used to generate points in scatterplot training

data [1] are slightly different from the ones automatically inferred from

the points of new scatterplots. We need to align them.

The original scatterplots [1] were generated with a GMM by specifying parameters of rotation (i  [0, /2]) and scaling (ix,y) to get the covariance matrix i = RiSi2RTi = f (i, ix, iy) (see Equation 6, Table 1, Figure 4). However, in inference mode, points' distribution of a new scatterplot SP(X) is modeled by a GMM M (X). The estimated

covariance matrix ^ i of each GMM component i is decomposed using

SVD back into S^i and R^i (6) from which we get angle ^i and scaling

parameters ^ix,y. Unfortunately, the estimated angle ^i lies in the range

[-/2, /2]. In order to align angles i of training data with estimated

angles ^i obtained from new data, we passed each triplet (i, ix, iy)

of (

all training data

i,

xi , 

y i

)

=

SV

uv D( f

into the composition-decomposition

(i,

ix,

y
i

)).

process:

Moreover, given the points X of a new scatterplot, the optimal param-

eters uv = {, components of

tµh,eouxp,timuy,al mvx,odvye,lMu, u(}X,)o, bnteaeidnetdo

from a pair (u, v) of be rescaled. Indeed,

it is likely that the scale of the points X is orders of magnitude bigger

or smaller than the one of the points in the scatterplots generated in the

training data uv (see Table 1. This scaling factor impacts parameters

µ and  . We also need to correct the angles u and v defined rela-

tively to the axis orthogonal to (µu - µv), while the rotation matrix R

of the SVD decomposition is relative to the vector space of the points

X. Therefore, first compute

for the

any new data uv correcting angle

inferred from  = (-µ-uµv,

a new scatterplot, we -y ) between the two

components' centers and the y-axis. We add  to all  angles. Then we

rescale

the

parameters

ux,

y
u

maximum of these values s =

,mavxx,({vyµa, ndux,µuyb,yvdx,ivivdy}in)g.

them

by

the

Finally,

we

obtain

the

input

data

align
uv

to

the

merging

function

GCl ust Rank :

uvalign = align(uv) = (, µ/s, ux/s, uy/s, vx/s, vy/s, u + , v + ) (7)
Regarding training data already in uv, we first apply the composition-decomposition process above to get i , then we rescale µ and  . However, the correcting rotation by  is useless as the y-axis is
by definition directed by the components' centers ( = 0) (See Figure
4):

Xuavlign =

(, µ/s, 

xu/s, 

y u

/s,



xv/s, 

y v

/s,



u,

v, Hi)

| (, µ, ux, uy, vx, vy, u, v, Hi)  Xuv,

(

j,

x j

,



yj )

=

SV D(

f

(

j

,



xj ,



y j

)),

j



{u, v},

(8)

s=

max({µ, 

xu, 

yu, 

x v

,



yv})

one, unseen so far, can lead to very different parameters inferred from
the GMM model for reasons explained below. Then, they can end up being located far away from each other in S making difficult to train a classifier to predict the decision. Therefore, we propose to augment the data by replicating the initial data i  uv in different locations i of S to better cover it. Labels are replicated too, so we get the new labeled data i = (i , Hi) from the initial one i = (i, Hi).
For any initial data sample i = (i, Hi)  Xuavlign:

i = (, µ, ux, uy, vx, vy, u, v, Hi)

(9)

we generate the following replica, to account for y-axis symmetry:

(9)  i- = (, µ, ux, uy, vx, vy, -u, -v, Hi)

(10)

And to account for the non-identifiability of the Gaussian components, we swap components u and v for the cases (10) and (9) above:

(9)



swa p
i

=

(1

-

,

µ,

vx,

vy,

ux,

uy, v,

u,

Hi)

(10)



-swa p
i

=

(1

- , µ, vx, vy, ux, uy, - v, - u, Hi)

(11)

We also generate replica to account for the cases of isotropic covari-

ance,

where

u

=

ux

=

y
u

or

v

=

vx

=

y
v

.

So

for

any

data

i = (, µ, u, u, vx, vy, u, v, Hi)

(12)

or i = (, µ, ux, uy, v, v, u, v, Hi)

(13)

we generate replica

(12)  iu = {(, µ, u, u, vx, vy, u, v, Hi)

|

u



{-

 2

,

-

3 8

,

-

 4

,

-

 8

,

0,

 8

,

 4

,

3 8

,

 2

}

(14)

(13)  iv = {(, µ, ux, uy, v, v, u, v, Hi)

|v



{-

 2

,

-

3 8

,

-

 4

,

-

 8

,

0,

 8

,

 4

,

3 8

,

 2

}}

The initial data and all its replica form the extended dataset Xuavll:

Xuavll

=

{i

,

i-

,

swa
i

p

,

-swa
i

p

,

iu

,

iv

|i



Xuavlign}

(15)

Then we filter out any duplicate data from that set to avoid oversampling of some data, and get the final set used to train the classifier:

The data set Xuavlign forms the aligned data to be augmented before training the classifier GClustRank.

Xuuvni = U nique(Xuavll )

(16)

4.3 Data Augmentation
We need to consider the symmetries arising in the parametric representation of a pair of Gaussian components (u, v) in S (see Figure 5). Indeed, the parameters used to generate the data sample uv in the original experiment [1] were intended to fall into a restricted part of S to avoid that the exact same pattern is displayed to the human subject while different random parameters are generated. For instance the scatterplot SP(uv) generated by uv = (, . . . , u, v) is identical up to the sampling variation, to the one SP(uv) generated by uv = (, . . . , u, v + ) while uv = uv.
Conversely, in the present work, we need to cover extensively the parameter space S with labeled data to get the best possible generalization from the classifier, i.e., predicting accurately the human judgments for yet unseen scatterplots. Indeed, two scatterplots with perceptually very similar point distributions are likely to get the same human judgment. But one can be among the available data uv while the other

4.4 Training Merging Models

Finally, training on Xuuvni, we can obtain the ClustRank merger

GClustRank optimal at predicting the labels Hi from the data i  uunvi,

and

use

it

to

predict

the

label

H^

of

the

current

data

align
uv

(7):

H^ = GClustRank(uvalign)  {0, 1}

(17)

H^ estimates the unobserved aggregated judgments humans would make for the scatterplot SP(uv).
The training uses standard approach in data-driven estimation of parameters of supervised classifiers. We split the set Xuuvni into a training (80%) and a test set (20%). The training set is used to estimate
the parameters of the classifier, and the test set to evaluate and compare
optimal models accuracy. The training set can be further split for
cross-validation purpose depending on the model used.

Fig. 5. Data augmentation process: (a) We expect that each set of parameters of a pair of GMM components corresponds to a unique scatterplot up to the natural sampling variability, and vice-versa. But there are symmetries for some settings of these parameters or some scatterplots. (b) Parameters of a pair of components can be different while they represent the exact same cluster pattern in the scatterplot, due to symmetry or rotation of the scatterplot itself. (c) The exact same scatterplot can be modeled by GMMs with different parameters leading to different location in the feature space. In all cases, we need to "better" cover the feature space with labeled examples to support the training of the classifier, otherwise the classifier will generalize poorly in these areas (Left side). The human judgment dataset does not contain such symmetries because it has been designed to avoid showing twice the same scatterplot to the humans subjects. Therefore, we need to augment these data in the feature space by duplicating labeled scatterplots considering these symmetries (Right side).

5 EXPERIMENTS
We first revisit the set of parameters available in the original data from [1] and describe the Benchmark dataset 1 (Section 5.1.1). We then define the set of automatic classifiers and training techniques we used to learn the optimal merging function G from the human judgments in this data (Section 5.1.2). Then, we analyze the results of the 320 merging models (Section 5.1.3), pick the best one, train it again on all available data, and use it to implement the ClustRank VQM. Finally, we evaluate and compare ClustRank to the state-of-the-art ClustMe VQM for cluster patterns, using the Benchmark dataset 2 of a second experiment described in [1] (Section 5.2).
5.1 Experiment 1:
In order to get the ClustRank merging model, we first align and augment the data and human judgments from available dataset (Section 5.1.1), then we present the classification techniques and protocols (Section 5.1.2), and finally compare the trained mergers to pick the best (Section 5.1.3).
5.1.1 Human judgments data Xuuvni
The initial human judgment data from the ClustMe study [1] is summarized in table 1. We ignore the  parameter which gave an additional random rotation to the whole scatterplot for each trial, and we ignore the number N  {100, 1000} of points generated in the scatterplot, as none of these parameters appears in the GMM modeling the scatterplot underlying density. The dataset uv is a sample of 1000 of these scatterplots. We discovered four of them are duplicate, which means four scatterplots were generated twice with the same set of parameters but differing by the number of sampled points (N = 100 and N = 1000), or turned out having the same parameters after data alignment. These four duplicates were removed. We ended up with 996 unique scatterplots with 34 human judgments that we summarized by majority vote (Section 4.1). The final alignment and augmentation processes (equation

Table 1. Initial data [1] uv are 1000 unique parameter sets uv picked randomly among the following values . In this work, we ignore  and N, and finally 996 unique sets uv remain to form Xuavlign (See section 5.1.1).

Param. 
µ
x,y
u,v u, v  N

Description Prior probability of component u. Euclidean distance between components u and v
Scaling factors Rotation angles of components u and v Rotation angle of the scatterplot image. Number of points in the scatterplot

Values {0.1, 0.2, 0.3, 0.4, 0.5} {0, 1, 2, 3, 5, 8, 13, 21}
{0.5, 1, 1.5, 2, 2.5, 3} {0, /8, /4, 3/8, /2 } {0, /2, 5/4} {100, 1000}

(16)) led to 16181 scatterplots in the set Xuuvni forming the Benchmark dataset 1.
5.1.2 Classifiers and training protocol
The 996 initial scatterplots, although chosen to cover the space of parameters uniformly, were assigned unequally to the two classes by the 34 subjects in the original study [1]. Therefore, 81.5% of the data with ended up with a merging decision of Hi = 1. This class imbalance requires a specific process for training classifiers to avoid bias in favor of the majority class. Another issue is the relative scale of the parameters, for instance, the parameter µ scales up to two orders of magnitude bigger than . Correlated features must also be dealt with. This requires pre-processing steps.
The 16181 scatterplots Xuuvni were stratified by class, each subset being randomly split into 80% training and 20% testing to finally get 12945 training and 3236 test points preserving the (imbalanced) class distribution. Notice that the 3236 test data points correspond to 709

of the 996 unique scatterplots while the 12945 training data points correspond to 991 of them. Still, none of them is duplicated in the parameter space S after augmentation, forming valid independent training and test sets for learning the automatic classifiers in that space.
We used the CARET R-package [29] for training 12 classification techniques, trying 4 methods to deal with class imbalance, and 4 preprocessing methods for scaling and remove correlated features, all summarized in Table 2. This process resulted in 320 different classification models. We used 10-fold cross-validation on the training set, with 10 repetitions of the training with random initialization,

Table 2. CARET R-package [29] [28] methods used (Section 5.1.2). Note xgbTree and gbm only used None and C pre-processing.

Balancing Pre-processing

Method None Center+Scale (C) C+BoxCox (CB) C+PCA (CP) C+B+P (CBP) CBP+spatialSign (CBPS) None upSample downSample ROSE [33] Smote [15] nb knn rf treebag blackBoost gbm xgbTree earth svmRadial mlpWeightDecay glm glmnet

Description No pre-processing Zero mean and unit variance Box-Cox transformation Principal Component Analysis
Dividing by norm (unit sphere)
No balancing Random replica of minority class Random sampling of major. class Random Over-Sampling Examples Synth. minor. class near. neighbors Naive Bayes k-Nearest Neighbors Random Forest Bagged Classif. Adapt Regres. Tree Boosted Regression Tree Generalized Boosted Regres. Model Extreme Gradient Boosting Tree Multivar. Adaptive Regres. Spline Radial Kernel Support Vector Mach. Multi-Layer Perceptron Generalized Linear Model GLM via penalized max. likelihood

Classification technique

In order to evaluate and select the best classifier on the test data, we computed the Matthews Correlation Coefficient (MCC) which is regarded as immune to large class imbalance [8, 11].
5.1.3 ClustRank merger is better than ClustMe-Demp
Table 3 lists the best setting for each classification technique. The overall best combination to realize the ClustRank merging function GClustRank is a bagged Classification and Regression Tree (CART) model (treebag) with up-sampling of the minority class (upSample) and running all pre-processing methods (Center+Scale+BoxCox+PCA+spatialSign).

Vanbelle Kappa on Experiment 1 Augmented Test Data
Moderate Substantial Almost perfect agreement agreement agreement
Vanbelle Kappa on Experiment 1 Test data
Moderate Substantial Almost perfect agreement agreement agreement Vanbelle Kappa on Experiment 2 Moderate Substantial Almost perfect agreement agreement agreement

judgments. Vanbelle's kappa v takes into account both the agreement between the group of human raters and the VQM, and the within group inter-rater agreements. The v values are interpreted using the scale proposed by Landis and Koch [30]: < 0 poor, ]0, 0.2] slight, ]0.2, 0.4 fair, ]0.4, 0.6] moderate, ]0.6, 0.8] substantial, and ]0.8, 1] almost perfect agreements. We run 10000 bootstrap simulations [17] on the test data to estimate the average score the two mergers would have varying the distributions of the scatterplots, and better evaluate their difference.
There are two ways to compare the ClustRank and ClustMe-Demp mergers. In case 1, we compute v on the 3236 augmented test data, which are unique for ClustRank but duplicates of some of the 709 ClustMe-Demp merging decisions and corresponding human judgments, biasing the comparison towards the duplicated cases (Figure 6 left). In case 2, we compute v on the 709 scatterplots from the test set, which is fair for ClustMe, but forces us to summarize the ClustRank predictions by a majority vote over the duplicated data. (Figure 6 center).
In case 1 favoring ClustRank, the ClustRank merger gets v = 0.986, 16% greater than ClustMe-Demp v = 0.848, both being in Almost perfect agreement with human judgments. In case 2, favoring ClustMeDemp, ClustRank merger gets v = 0.962 (Almost perfect agreement), a 22% improvement over ClustMe-Demp's v = 0.786 (Substantial agreement; similar to the state-of-the-art score v = 0.788 computed on the full 1000 dataset [1]). The ClustRank merger is noticeably better than the ClustMe-Demp merger in both cases.

1.0

1.0

1.0

0.8

0.8

0.8

0.6

0.6

0.6

0.4 ClustRank merging

ClustMe merging

0.4 ClustRank merging

ClustMe merging

0.4 ClustRank merging

ClustMe merging

Fig. 6. Left: ClustRank merger is noticeably better than ClustMe-Demp merger based on Vanbelle Kappa score on the 3236 augmented data of the test set in Experiment 1 (Left) and on the 709 test scatterplots with class computed by majority vote of ClustRank predictions on augmented data (Center) (Section 5.1.3). ClustRank VQM is noticeably better than ClustMe VQM at ranking the 435 pairs of scatterplots in Experiment 2 (Right) (Section 5.2) although as expected, scores are lower than in Experiment 1 as these scatterplots display more complex patterns and involve the full VQM pipeline.

Table 3. Treebag with up sampling and all pre-processing options is the best on 3236 test data of Experiment 1. The best of each classification technique is displayed together with its specific class balancing and preprocessing compounds (See Table 2). Matthew's correlation coefficient (MCC) accuracy score (the higher the better) is best suited to class imbalance [8, 11]. The target class used for training and testing, is the majority vote among the 34 human judgments.

Classification tech. treebag rf gbm mlp knn earth
blackBoost xgbTree svmRadial glmnet glm nb

Balancing upSample
None upSample
None None None Smote None None None None None

Pre-Processing CBPS None None CBPS CB CBPS C None CBPS CBPS None CB

MCC 0.970 0.959 0.953 0.893 0.888 0.876 0.875 0.868 0.866 0.832 0.831 0.828

Following previous work [1], we use the Vanbelle's Kappa v agreement index [46] to compare both merging techniques with the 34 human

5.2 Experiment 2: ClustRank is better at ranking scatterplots
In this experiment, we compare the ClustRank and ClustMe VQM with their full pipelines illustrated in Figure 1. To this end, we use them to rank pairs of scatterplot projections of real and synthetic highdimensional data from the Experiment 2 benchmark dataset used in the ClustMe paper [1]. None of these scatterplots has been used at any time in the training process of ClustRank, nor in determining parameters of ClustMe. The Benchmark dataset 2 [1] is made of all 435 possible pairs of 30 monochrome scatterplots selected among the Benchmark dataset 3 composed of 257 scatterplots from another earlier study [41]. Each pair has been judged by 31 subjects to rank the scatterplots by the perceived group structure complexity of the displayed point patterns on a 3-category scale: "<", "=", ">".
We use the mclust R-package with BIC model selection to train the GMM. We run the ClustRank and ClustMe merging functions on each pair of components identified by the GMM, and finally get the respective VQM for each of the 30 scatterplots. Finally, following the same protocol defined for ClustMe evaluation [1], we use this VQM score to rank the scatterplots and we compare the ranking with humans judgments on all 435 pairs .

28

229

102 36

221

30

56

240

130 72

114

244 238 169

216 108

30

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq qqqqqq

qq q qq

qqq q

q

qqq q q

q qq q q

240

qq q

q

q

q q

q

q

q q
q

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

qq q

q

q

244

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

q qqqqqqqqqqqq qq

qqqqqqqqqqqqqqqqqqqqq

qqq

qqqqqqqqqqqqq

q

q q qq q

q

238 qq

q

q

q qq

q q

q
qq q

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

q q

qq q

q

qqq

102 qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

130qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

169

q

q

q

qq

q

q q

q

q q

q

q

qq

qq

q qqq

q

q

qq

q qq

qq q qq

q
q qq q

q q

q

q q q qq q q

q

q

q

qq

q

q

q

q

qqq

q

q

q

q q qq

q

q q q qq

q qq

qq

q

q

q qq qqq
q

q q

q q

q

q

q

q

q

q q
q q

221

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq q

qqqqqqqqqqqqq q q

q qqqqq

qqqqq q q

q q qq q q q

q

q qq

q

qq

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

q qqqqqqq qq

q

q
qq qq

q q

q qq q

q q qqqqqqq q qq q

q

q q q qqq

q

q

q

q

q

28

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

qq q
qq

q q
qq q

qq q q

q q
q
q q

q

72 qqqq

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

qq q q qqqq q qqqqqqqqqqqqqqqqqqqqqq qqqqqqqqqq qq

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

q qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq q
qq qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

108qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

qq

q

q

q q
q

q q

qq q

qq q

q

q q

q

q

q

qq

q qq q qqqq qq qq q

q q
q q
q q

q

q

229

q

q

q

q

q

q

q

q

q

q

q

q

q

q

q

q

q

q

q

q

q

q

q

q

q

qq

qq

qq

qq

qq

q

q

q

q

q

qq

qq

qq

qq

qq

qq

qq

qq

qq

qq

q

q

q

q

q

qq

qq

qq

qq

qq

qq

qq

qq

qq

qq

q

q

q

q

q

qq

qq

qq

qq

qq

q

q

q

q

q

q

q

q

q

q

q

q

q

q

q

q

q

q

q

q

q

q

q

q

q

36
qqqqqqqq

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq qqqqqqqq qqqqqqqq qq qq

q q

q

q q

q

qqqqqqq qq qqqq

qqq q

q qq q q qq q

qq
q q
q q

q
q q q

q qq

114qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

216

q

q

q

q

qqq q

q q q

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

q

q

56
qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq q

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

Fig. 7. Top: Comparison of ClustRank and ClustMe scores of 257 scatterplots from benchmark dataset 3. Bottom: 16 selected scatterplots from the top view, with corresponding colors and numbers. See section 5.2 for details.

5.3 Interpretation of Vanbelle Kappa with a worst-case analysis
To better understand the meaning of these ranking scores, we compute the Vanbelle index when altering randomly 10000 times, k of the 435 ClustRank decisions, for each k  {1, . . . , 435}. By alteration we mean changing any of <,= or > order relations to a different order relation picked randomly among the same set. The resulting distribution of the Vanbelle Kappa for each value of k is displayed in Figure 8. ClustRank score requires between rmin = 9 (2%) and rmax = 49 (11%) alterations, with r = 20 (4.6%) on average, to get down to ClustMe score.
Altering a decision occurs whenever the order of two of the scatterplots is changed. For instance, on average, the difference between ClustRank and ClustMe is equivalent to move a single scatterplot down or up by 20 positions in the total ordering, or changing rank of more scatterplots by a total of 20 rank alterations.
Let us consider a realistic usage scenario where the user has a time budget so she can afford to explore deeper only the top-K scatterplots in search of new insights. Moving n elements out of the top-K set (K  n) requires at least r = n2 rank permutations if we pick up the bottom n of that set. For instance, if abcde f |ghi jkl...z is an ordered set of 26 scatterplots and the user as a time budget to explore only the top K = 6 (a to f delimited by |), then moving n = 2 scatterplots out of the top-6, say e and f to get abcdgh|e f i j...z, requires altering at least r = 4 pairwise rankings (g  e, g  f , h  e, h  f ). Any other set of permutations to push any set of n items out of the top-K requires at least n2 rank permutations. Hence in the worst case, given a ClustRank ordering of the 30 scatterplots, ClustMe in comparison, may down-rank between nmin = rmin = 3 and nmax = 7 scatterplots off the top-K most potentially insightful ones. To be fair, it is also possible that most or all the alterations created by ClustMe occur outside of the top-K, so would have no impact on the time-budgeted insight gathering, ClustMe and ClustRank having then identical top-K sets.
We can extrapolate this observation to any dataset size. For a dataset with N scatterplots, a simple calculus shows that if p is the percentage of ranking alterations over the N(N - 1)/2 pairs, with p  50, then the number of ranking alterations is r = N(N - 1)p/200. Moreover,in the worst case, the percentage of down-graded scatterplots q is 100 r/N. Hence, q = 10 (N - 1)p/2N  50p for large N.
ClustMe alters in the worst case about 11% of all pairs of N scatterplots (p = 100 × rmax/N = 100 × 49/435  11), therefore it could down-grade up to 23.5% of the scatterplots ordered by ClustRank in a worst-case scenario.

Vanbelle Kappa score 290 49

ClustRank VQM gets v = 0.727, improving over ClustMe's v = 0.671 top score to date. Figure 6 shows the distribution of 10000 bootstrap samples of the 435 pairs of scatterplots for the two VQMs. ClustRank is still noticeably better than ClustMe on this data. However, the score difference is lower than in the previous experiment with only 8% improvement, and both scores are within the Substantial agreement range.

0.8 0.727 ClustRank 0.671 ClustMe
0.6 0.4 0.2 0.0

The ClustMe and ClustRank VQMs are used to rank the whole set of 257 scatterplots available in benchmark dataset 3. Their scores are compared in Figure 7. In this figure, some scatterplots (SPs) at the bottom show details of the dots in the top view. Numbers indicate numerical identifiers of the SPs in the dataset. SPs 28, 36, 102 and 72, 114, 130 are scored far higher by ClustRank ( [5.9, 9.9]) than by ClustMe ( [3.9, 4.9]). ClustMe scored them the same as SPs 108 and 216, while they show far clearer group structures as correctly measured by ClustRank. SPs 30 and 240 are scored higher by ClustRank than 244 while showing similar low structure pattern better measured by ClustMe. 238 is correctly scored low by both VQM, while 169 is slightly undervalued by ClustRank. Both VQM also correctly measure complex groupings in SPs 56, 221, 229. Overall, ClustRank shows a slight advantage over ClustMe in ranking these scatterplots.

-0.2

-0.4 0

100

200

300

400

Number of ClustRank altered judgments

Fig. 8. Distribution of Vanbelle kappa when altering 10000 time k values randomly chosen among the ClustRank decisions (k  {1, . . . , 435}) over the 435 pairs of 30 scatterplots in Experiment 2 (Section 5.2). Dark grey area shows one standard deviation above and below the average value (black line). Light grey extends between min and max values of the 10000 samples. This serves to evaluate how much ClustMe would worsen ClustRank ordering.

% of Variance Explained

6 USAGE SCENARIO ON GENOMIC DATA
In many domains of micro-biology, analysts rely on data visualization to spot interesting patterns that deserve further detailed analysis. For instance, Kiselev et al. [27] underline the challenge of automatic clustering of single-cell data. Feng et al. [19] compare different dimension reduction techniques that instead let the user visualize scatterplots and decide about clusters of cells and their features. Another work on single-cell transcriptome study [22], proposes to use a Scatterplot Matrix (SPLOM) of all pairs of eigengenes coding groups of coexpressed genes, to visually identify interesting groups of cells. In the field of Genome-Wide Association Studies, analysts project the genetic data into Principal Components space for visual inspection [6,45]. However, the numerous projection methods and their parameters lead to possibly hundreds of scatterplots representing different facets of the same multi-dimensional data, similar in essence to the type of data we used in section 5.2.
In this usage scenario, we consider the data from the 1000 Genome Project phase 3 dataset [7] composed of genetic data of 26 populations of about 100 individuals each. We measure kinship between individuals of each population separately computing identity-by-descent [45]. We project these data using Multidimensional scaling into 30 dimensions and compute ClustRank on each possible pair of components for each population separately. The top view in Figure 9 shows the 11310 SPs in the space of ClustRank score and proportion of variance explained. The bottom show several SPs found exploring highest ClustRank scores in search of complex patterns that could relate to subgroups of individuals in each population.
Usually, analysts only rely on exploring the orange, red, magenta or blue dots coding for scatterplot having first, second, third or fourth principal components as one of their axes. Thanks to ClustRank, we show that we can find SPs with higher order components and relatively high variance explained. Some of these SPs display possibly insightful patterns (See Figure 9 Bottom).
30
20
10

7 DISCUSSION AND FUTURE WORK
We proposed a new data-driven VQM for cluster patterns. ClustRank is based on the previously proposed ClustMe pipeline [1] but differs by its merging component fully trained on perceptual judgment data. This is the first time such an approach is attempted in an unsupervised setting.
Initially, this work has been an attempt to introduce human perception at the core of a clustering technique (Assigning the data points to clusters), but the resulting color-coded clusters were not convincing through visual inspection, while it happened to work good enough as an aggregated measure (counting clusters) to form the ClustRank VQM. We only scratched the surface of this interesting topic in our recent work [5] where we compare existing clustering techniques to human perceptual judgments on the scatterplots used in Experiment 1.
When we imagined using previous user study data [1] for training the ClustRank merging model, we did not foresee the difficulty of the task. The experimental protocol to collect perceptual data was optimized for that sole purpose. Indeed, generating data for user experiments requires parsimony and careful design to optimally utilize participants' short focus time, and exploring as much as possible the combinations of factors at play with a small set of visual stimuli and trials. Training a classifier requires both plenty of data and an extensive coverage of the feature space. The necessity of rescaling and augmenting the data to account for various scales and symmetries, or the existence of duplicated data due to dropped factors (number of points N and rotation angle ) had not been foreseen before engaging in this work. It highlights the importance of considering from the design stage of the data collection protocol, all the practical implications of developing a perceptual data-driven approach. The devil is in the details.
At the core of the GMM clustering technique, human judgments are only used to merge pairs of components. But these pairwise decisions are aggregated through connected components to form the final clusters ignoring the fact that some points in the scatterplot are encoded by several components in a smooth manner (non-binary), and also ignoring what is perceived by human subjects in more complex grouping structures. Although it works well for a VQM, it would be interesting to find other ways to integrate human perception into clustering techniques and to better understand how different types of cluster patterns in scatterplots [41] are perceived by human subjects.
Advanced computer vision techniques like deep neural networks (DNN) could tackle this problem in the image space directly. The ScatterNet technique [31] has been proposed to model similarity between scatterpots, but not to rank them. Finding a way to train a DNN for that task looks like an interesting challenge too.

PCb

ASW - PC1-PC2 10 ClustRank 2.4
Total var. 12.4%
5

0

-5

-10

-10

-5

0

5

10

PCa

GWD - PC1-PC2 ClustRank 4.5
10
Total var. 18.6%

5

0

-5

-10

-10

-5

0

5

10

PCa

2.5

5.0

ClustRank

ASW - PC5-PC6 10 ClustRank 6.7
Total var. 5.6%
5

BEB - PC1-PC2 ClustRank 2.3 Total var. 13.5%
5

PCb

PCb

0

0

-5

-10

-10

-5

0

5

10

PCa

GWD - PC2-PC21 ClustRank 8.8
10
Total var. 7.1%

5

0

-5

-10

-10

-5

0

5

10

PCa

PCb

-5

-5

0

5

PCa

PUR - PC1-PC2 ClustRank 2.2 Total var. 13.5%
4

0

-4

-4

0

4

PCa

PCb

7.5

PCb

BEB - PC12-PC22 ClustRank 4.4 Total var. 5.1%
5

0

-5

-5

0

5

PCa

PUR - PC2-PC7 ClustRank 3.4 Total var. 8.9%
4

0

-4

-4

0

4

PCa

PCb

PCb

Fig. 9. Top: Distribution of 11310 scatterplots from all pairs of top 30 principal components (PCs) of 1000 Genome Project kinship data in the space of ClustRank score and percentage of variance explained. Dots are color-coded by the axis with more variance in the scatterplots, showing the ones directed mainly by the first, second, third, or fourth PC (black otherwise). Solid orange dots with black edge are SPs directed by the first and second PCs. SPs at the top are typically visualized in priority. Bottom: SPs selected in bottom right area of top view show interesting cluster patterns which would have been missed by a superficial overview of only the main components.

REFERENCES
[1] M. Abbas, M. Aupetit, M. Sedlmair, and H. Bensmail. Clustme: A visual quality measure for ranking monochrome scatterplots based on cluster patterns. Computer Graphics Forum (Proc. EuroVis 2019), 2019.
[2] R. Agrawal, J. Gehrke, D. Gunopulos, and P. Raghavan. Automatic subspace clustering of high dimensional data for data mining applications. In Proc. ACM Int. Conf. on Management of Data (SIGMOD), pp. 94­105. ACM Press, 1998.
[3] G. Albuquerque, M. Eisemann, and M. Magnor. Perception-based visual quality measures. In Proc. IEEE Symp. on Visual Analytics Science & Technology, pp. 13­20, 2011. doi: 10.1109/VAST.2011.6102437
[4] M. Aupetit and M. Sedlmair. Sepme: 2002 new visual separation measures. Proc. IEEE Pacific Visualization Symp. (PacificVis), pp. 1­8, 2016. doi: 10 .1109/PACIFICVIS.2016.7465244
[5] M. Aupetit, M. Sedlmair, M. M. Abbas, A. Baggag, and H. Bensmail. Toward perception-based evaluation of clustering techniques for visual analytics. In 2019 IEEE Visualization Conference, VIS 2019, Vancouver, BC, Canada, October 20-25, 2019, pp. 141­145. IEEE, 2019. doi: 10. 1109/VISUAL.2019.8933620
[6] M. Aupetit, E. Ullah, R. Rawi, and H. Bensmail. A design study to identify inconsistencies in kinship information: The case of the 1000 genomes project. In C. Hansen, I. Viola, and X. Yuan, eds., 2016 IEEE Pacific Visualization Symposium, PacificVis 2016, Taipei, Taiwan, April 19-22, 2016, pp. 254­258. IEEE Computer Society, 2016. doi: 10.1109/ PACIFICVIS.2016.7465281

[7] A. Auton et al. A global reference for human genetic variation. Nature, 526(7571):68­74, 2015.
[8] M. Bekkar, H. K. Djemaa, and T. A. Alitouche. Evaluation measures for models assessment over imbalanced datasets. Journal of Information Engineering and Applications, 3(10), 2013.
[9] H. Bensmail, G. Celeux, A. Raftery, and C. Robert. Inference in modelbased cluster analysis. Statistics and Computing, 7:1­10, 1997.
[10] E. Bertini and G. Santucci. Visual quality metrics. In Proc. Workshop on Beyond Time and Errors on Novel Evaluation Methods for Visualization (BELIV), pp. 1­5, 2006. doi: 10.1145/1168149.1168159
[11] S. Boughorbel, F. Jarray, and M. El-Anbari. Optimal classifier for imbalanced data using Matthews Correlation Coefficient metric. PLoS ONE, 12(6):e0177678, June 2017. doi: 10.1371/journal.pone.0177678
[12] P. Bruneau, P. Pinheiro, B. Broeksema, and B. Otjacques. Cluster sculptor, an interactive visual clustering system. Neurocomputing, 150(Part B):627 ­ 644, 2015. Special Issue on Information Processing and Machine Learning for Applications of Engineering Solving Complex Machine Learning Problems with Ensemble Methods Visual Analytics using Multidimensional Projections. doi: 10.1016/j.neucom.2014.09.062
[13] A. Buja, D. H. Cook, D. Asimov, and C. Hurley. Dynamic projections in high-dimensional visualization: Theory and computational methods. Technical report, University of Pennsylvania, 1997.
[14] M. Cavallo and C. Demiralp. Clustrophile 2: Guided visual clustering analysis. IEEE Transactions on Visualization and Computer Graphics, 25(1):267­276, Jan 2019. doi: 10.1109/TVCG.2018.2864477
[15] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer. Smote: Synthetic minority over-sampling technique. Journal of Artificial Intelligence Research, 16:321­357, 2002.
[16] T. N. Dang and L. Wilkinson. Scagexplorer: Exploring scatterplots by their scagnostics. In Proc. IEEE Pacific Visualization Symp. (PacificVis), pp. 73­80, 2014. doi: 10.1109/PacificVis.2014.42
[17] B. Efron and R. J. Tibshirani. An Introduction to the Bootstrap. Chapman et Hall, 1993.
[18] M. Ester, H.-P. Kriegel, J. Sander, and X. Xu. A density-based algorithm for discovering clusters a density-based algorithm for discovering clusters in large spatial databases with noise. In Proc. Int. Conf. on Knowledge Discovery and Data Mining (KDD), pp. 226­231. AAAI Press, 1996.
[19] C. Feng, S. Liu, H. Zhang, R. Guan, D. Li, F. Zhou, Y. Liang, and X. Feng. Dimension reduction and clustering models for single-cell rna sequencing data: A comparative study. International Journal of Molecular Sciences, 21(6):2181, Mar 2020. doi: 10.3390/ijms21062181
[20] C. Fraley and A. E. Raftery. Model-based clustering, discriminant analysis and density estimation. Journal of the American Statistical Association, 97:611­631, 2002.
[21] J. H. Friedman and J. W. Tukey. A projection pursuit algorithm for exploratory data analysis. IEEE Trans. on Computers, C-23(9):881­890, 1974. doi: 10.1109/T-C.1974.224051
[22] Z. Han, T. Johnson, J. Zhang, X. Zhang, and K. Huang. Functional virtual flow cytometry: A visual analytic approach for characterizing single-cell gene expression patterns. BioMed research international, 2017:3035481­ 3035481, 2017. doi: 10.1155/2017/3035481
[23] C. Hennig. Methods for merging gaussian mixture components. Advances in Data Analysis and Classification, 4(1):3­34, 2010. doi: 10.1007/s11634 -010-0058-3
[24] P. Hoffman, G. Grinstein, and D. Pinkney. Dimensional anchors: A graphic primitive for multidimensional multivariate information visualizations. In In Proc of the NPIV 99, pp. 9­16. ACM Press, 1999.
[25] D. H. Jeong, C. Ziemkiewicz, B. Fisher, W. Ribarsky, and R. Chang. Ipca: An interactive system for pca-based visual analytics. In Proceedings of the 11th Eurographics / IEEE - VGTC Conference on Visualization, EuroVis'09, p. 767­774. The Eurographs Association & John Wiley & Sons, Ltd., Chichester, GBR, 2009. doi: 10.1111/j.1467-8659.2009.01475 .x
[26] S. Johansson and J. Johansson. Interactive dimensionality reduction through user-defined combinations of quality metrics. IEEE Trans. on Visualization & Computer Graphics, 15(6):993­1000, 2009. doi: 10.1109/ TVCG.2009.153
[27] V. Y. Kiselev, T. S. Andrews, and M. Hemberg. Challenges in unsupervised clustering of single-cell rna-seq data. Nature Reviews Genetics, 20(5):273­ 282, May 2019. doi: 10.1038/s41576-018-0088-9
[28] M. Kuhn. Building predictive models in r using the caret package. Journal of Statistical Software, 28(5):1­26, 2008. doi: 10.18637/jss.v028.i05
[29] M. Kuhn. caret: Classification and regression training. R package version

6.0-82, 2019. [30] J. R. Landis and G. G. Koch. The measurement of observer agreement for
categorical data. Biometrics, pp. 159­174, 1977. [31] Y. Ma, A. K. H. Tung, W. Wang, X. Gao, Z. Pan, and W. Chen. Scatternet:
A deep subjective similarity model for visual analysis of scatterplots. IEEE Transactions on Visualization and Computer Graphics, pp. 1­1, 2018. doi: 10.1109/TVCG.2018.2875702 [32] J. Matute, A. C. Telea, and L. Linsen. Skeleton-based scagnostics. IEEE Trans. on Visualization & Computer Graphics, 24(1):542­552, 2018. doi: 10.1109/TVCG.2017.2744339 [33] G. Menardi and N. Torelli. Training and assessing classification rules with imbalanced data. Data Mining and Knowledge Discovery, 28:92­122, 2014. [34] L. G. Nonato and M. Aupetit. Multidimensional projection for visual analytics: Linking techniques with distortions, tasks, and layout enrichment. IEEE Trans. on Visualization & Computer Graphics, 2018. doi: 10. 1109/TVCG.2018.2846735 [35] A. V. Pandey, J. Krause, C. Felix, J. Boy, and E. Bertini. Towards understanding human similarity perception in the analysis of large sets of scatter plots. In Proc. ACM Conf. on Human Factors in Computing Systems (CHI), pp. 3659­3669, 2016. doi: 10.1145/2858036.2858155 [36] D. Pelleg and A. Moore. X-means: Extending k-means with efficient estimation of the number of clusters. In Proc. Int. Conf. on Machine Learning (ICML), pp. 727­734. Morgan Kaufmann, 2000. [37] M. Rubio-Sa´nchez, L. Raya, F. D´iaz, and A. Sanchez. A comparative study between radviz and star coordinates. IEEE Transactions on Visualization and Computer Graphics, 22(1):619­628, Jan 2016. doi: 10.1109/TVCG. 2015.2467324 [38] G. Schwarz. Estimating the dimension of a model. The annals of statistics, 6:461­464, 1978. [39] M. Sedlmair and M. Aupetit. Data-driven evaluation of visual quality measures. Computer Graphics Forum, 34(3):201­210, 2015. doi: 10. 1111/cgf.12632 [40] M. Sedlmair, T. Munzner, and M. Tory. Empirical guidance on scatterplot and dimension reduction technique choices. IEEE Trans. on Visualization & Computer Graphics, 19(12):2634­2643, 2013. doi: 10.1109/TVCG. 2013.153 [41] M. Sedlmair, A. Tatu, T. Munzner, and M. Tory. A taxonomy of visual cluster separation factors. Computer Graphics Forum, 31(3):1335­1344, 2012. doi: 10.1111/j.1467-8659.2012.03125.x [42] C. Shorten and T. M. Khoshgoftaar. A survey on image data augmentation for deep learning. J. Big Data, 6:60, 2019. [43] A. Tatu, F. Maass, I. Fa¨rber, E. Bertini, T. Schreck, T. Seidl, and D. A. Keim. Subspace search and visualization to make sense of alternative clusterings in high-dimensional data. In Proc. IEEE Symp. on Visual Analytics Science & Technology, pp. 63­72, 2012. doi: 10.1109/VAST. 2012.6400488 [44] J. W. Tukey and P. A. Tukey. Computer Graphics and Exploratory Data Analysis: An Introduction. In Proc. the Sixth Annual Conference and Exposition: Computer Graphics, Vol. III, Technical Sessions, pp. 773­785. Nat. Computer Graphics Association, 1985. [45] E. Ullah, M. Aupetit, A. Das, A. Patil, N. A. Muftah, R. Rawi, M. Saad, and H. Bensmail. Kinvis: a visualization tool to detect cryptic relatedness in genetic datasets. Bioinform., 35(15):2683­2685, 2019. doi: 10.1093/ bioinformatics/bty1028 [46] S. Vanbelle and A. Albert. Agreement between an isolated rater and a group of raters. Statistica Neerlandica, 63(1):82­100, 2009. doi: 10. 1111/j.1467-9574.2008.00412.x [47] B. Wang and K. Mueller. The subspace voyager: Exploring highdimensional data along a continuum of salient 3d subspaces. IEEE Transactions on Visualization and Computer Graphics, 24(2):1204­1222, Feb 2018. doi: 10.1109/TVCG.2017.2672987 [48] Y. Wang, X. Chen, T. Ge, C. Bao, M. Sedlmair, C. Fu, O. Deussen, and B. Chen. Optimizing color assignment for perception of class separability in multiclass scatterplots. IEEE Transactions on Visualization and Computer Graphics, 25(1):820­829, 2019. [49] Y. Wang, K. Feng, X. Chu, J. Zhang, C. Fu, M. Sedlmair, X. Yu, and B. Chen. A perception-driven approach to supervised dimensionality reduction for visualization. IEEE Transactions on Visualization and Computer Graphics, 24(5):1828­1840, 2018. [50] J. Wenskovitch, I. Crandell, N. Ramakrishnan, L. House, S. Leman, and C. North. Towards a systematic combination of dimension reduction and clustering in visual analytics. IEEE Transactions on Visualization and

Computer Graphics, 24(1):131­141, Jan 2018. doi: 10.1109/TVCG.2017. 2745258 [51] L. Wilkinson, A. Anand, and R. L. Grossman. Graph-theoretic scagnostics. In J. T. Stasko and M. O. Ward, eds., Proc. IEEE Information Visualization Symp. (INFOVIS), p. 21. IEEE Computer Society, 2005. doi: 10.1109/ INFOVIS.2005.14

