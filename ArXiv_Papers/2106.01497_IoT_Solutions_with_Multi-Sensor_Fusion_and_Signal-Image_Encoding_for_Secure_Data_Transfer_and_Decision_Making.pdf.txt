IoT Solutions with Multi-Sensor Fusion and Signal-Image Encoding for Secure Data Transfer
and Decision Making*
Piyush K. Sharma1, Mark Dennison2, and Adrienne Raglin2
Army Research Laboratory1,2 Adelphi, MD 20783, USA
Abstract. Deployment of Internet of Things (IoT) devices and Data Fusion techniques have gained popularity in public and government domains. This usually requires capturing and consolidating data from multiple sources. As datasets do not necessarily originate from identical sensors, fused data typically results in a complex data problem. Because military is investigating how heterogeneous IoT devices can aid processes and tasks, we investigate a multi-sensor approach. Moreover, we propose a signal to image encoding approach to transform information (signal)to integrate (fuse) data from IoT wearable devices to an image which is invertible and easier to visualize supporting decision making. Further-more, we investigate the challenge of enabling an intelligent identificationand detection operation and demonstrate the feasibility of the proposed Deep Learning and Anomaly Detection models that can support future application that utilizes hand gesture data from wearable devices.
Keywords: IoT · Data Fusion · Smart City · Transfer Learning · Anomaly Detection · Command and Control · C2.
1 Introduction
Advances in technology have contributed to the growing use of laptops, smartphones, and tablets in government and industry globally. Nascent 5G network enables mobile computing more effectively [13]. Increasing amount of information ow are impacting daily life of individuals, business conducted across industry, and government operations [3]. Expansion of digital information introduces several challenges related to complexity of data management, storage, privacy, processing and transfer. These challenges are also seen in utilizing information for decision making. In order to leverage this abundant, raw, digital information, military must keep abreast with advanced datadriven technology to meetmission requirements.
* This work was supported by Army Research Laboratory (ARL) Grant# W911NF18-2-0043.

2

P. K. Sharma et al.

Modern military's Multi Domain operations (MDO) can potentially utilize Internet of Things (IoT) technology connecting land, water, air, space and cyberspace in a cohesive network [10,2,1]. This increases situational awareness and risk assessment, reduces response time, and can help prepare military for the future battle elds. IoT devices capture and consolidate data from multiple sources allowing the full realization of C2 (Command and Control) system to provide situational awareness [36]. This combined information gathered by a range of heterogeneous IoT devices can give an edge for potential strategic advantages.
IoT allows collection and dissemination of digital information by deploying several devices which output massive amount of data [38], [39]. Data related technologies provide IoT solutions to help industry and government to make better decisions by exploring underlying data regularities and revealing patterns, and provide an early warning to act upon possible threats (malware, terrorism, fraud, etc.) [4]. This allows military to take agile, perceptive, resilient, and re- liable decisions in timely manner to meet mission requirements. We seek out to address questions: How to tailor information for transmission over a com- promised network and retain most of it in the process with the goal of enabling decisions? How to model complex systems, detect and understand issues, and improve transmission?
Moreover, growing number of cyber-attacks put sensitive information at risk [32]. Information is a strategic asset for government organizations, and they need to protect it for national security [35]. Recently Mirai botnet malware launched a distributed denial of service (DDoS) attack causing much of the internet inaccessible on the U.S. east coast [11]. With the Army's growing dependency on IoT devices and the possible failure of current technology to keep up with the cyber security, C2 operations are susceptible to cyber-attack from possible threats capable of compromising IoT devices to exterminate multi-domain operations by injecting false or compromised information [18]. Hackers are developing new variants of IoT-focused malware with alarming regularity. Due to potentially sensitive nature of IoT datasets, Blockchain technology is used to facilitate se- cure sharing of IoT datasets, which allows digital information to be distributed, but not copied [26]. However, blockchain has several limitations related to complexity, scalability, and excessive energy consumption [27]. In order to leverage communication in C2 systems and to address the raised questions, we propose a sensor signal to image encoding approach to modify information (signal) by transforming it to an image which is invertible and easier to visualize [37].
Future military combat suits, helmets, weapons, and other equipment will be embedded with sensing and computing devices. These devices not only provide real-time interaction between soldiers and commanding officers in combat zones,but also help understand the psychophysical condition of soldiers. Modern devices are capable of collecting various aspects of biometric forensics, such as, data with facial, fingerprint, heartbeat, body temperature, etc. attributes. Consolidating data from disparate sensory devices can provide more consistent, accurate and relevant information helpful in understanding the state of mind and cognitive load during tasks.

Title Suppressed Due to Excessive Length

3

One of the main goals of this paper is to investigate multi-sensor fusion approach producing combined data for improved accuracy and reduced uncertainty in channeled information [22]. A controlled exploitation of this approach can reduce cognitive load of soldiers, improve decision making, and establish a better soldier-machine (IoT devices) interaction; essential and ideal conditions for mission success. The purpose of proposed multi-sensor fusion and signal-image encoding approaches is to transform senseless and unstructured data into structured data from which meaningful information can be extracted to be used as actionable insights that enable intelligent military decision-making [36].
In Section 2, we provide background and related work. In Section 3,
we de ne CJSD kernels and related research literature. In Section 4, we
describe experimental data. In Section 5, we explain our signal-image encoding
approach. In Section 6, we describe why we choose performed experiments. In
Section 7, we provide a comparative analysis of results obtained with our experiments. Finally, we conclude our contribution with a summary of results in Section 8.

2 Background and Motivation
Potential of massive amount of data generated from a multi-sensor data fusion approach has limited value if it cannot be correlated with some context [25]. This plethora of information can be used in a context-based paradigm to improve personnel authentication accuracy using aforementioned biometric forensics. There can be added challenges to con rm identities of individuals in the fast pace environment that future soldiers might face. Because timing is crucial for mission success, suspicious and potential targets can be identified and neutralized instantly with Edge Computing [12].
The goal of this study is to measure the in influence different aspects of human state have on how a person makes gestures. These data were collected with the sole purpose of training machine learning algorithms to automatically detect the onset and o set of separate gestures in a series.
Research literature presents examples where multi-sensor data fusion techniques were implemented in different application domains. One paper proposed a smart home system with wearable intelligent technology, artificial
intelligence, and multisensor data fusion technology [14]. A 3D gesture
recognition algorithm was developed to recognize hand gestures. Another work presented a human activity recognition example using wearable and environmental sensor data fusion approach [29]. Furthermore, examples of Human-Computer-Interaction (HCI) for sign language and gesture recognition have been proposed within a multi- sensor fusion framework [19], [20].
Our work extends beyond the multi-sensor data fusion by introducing signalimage encoding scheme which transforms each fused data point into an image
as described in Section 5. Research literature presents many examples of
signal to image transformation techniques [45,15]. Our contribution is an information encoding approach which has multiple advantages; transformed data preserves allinformation, generated image les are small in size thus can easily be transferred

4

P. K. Sharma et al.

over internet, transformed data is invertible and easier to visualize. Therefore, we can retrieve original signal from encoded image without requiring a key.
For image data obtained from signal transformation, we explore Transfer Learning approach which uses the pre-trained weights of a large image dataset [30]. This emerging approach has gained popularity in recent years and it intends to improve the performance of a neural network trained on a small dataset. For fusion sensor (signal) data, we explore recently introduced information theoretic kernels, Chisini Jensen Shannon Divergence (CJSD) and its metric version, Metric-Chisini Jensen Shannon Divergence (M-CJSD), known for their utility to tease apart data classes with discernible differences in Support Vector Machine (SVM) classification [43,41,40,42,38,39]. For benchmarking, we implement a multi-layered Deep Neural Network (DNN) model and tune it on a large hyperparameter grid. Furthermore, we explore a number of novelty detection techniques for their ability to identify anomalous observations which deviate significantly from the majority of data in contrast to classification models which assign predicted classes into groups. We perform a comparative analysis of employed models and measure their performances in terms of classification accuracy, sensitivity and specificity. Moreover, we compute confusion matrices for comparative analysis of our models. We evaluate all implemented methods on hand gesture datasets described in
Section 4.

3 Family of CJSD and M-CJSD Kernels
Let, , P ={pi}Ni=1 and Q={qi}Ni=1 be two probability distributions, where pi and qi are the respective probabilities associated to the ith state (possible values). The family of CJSDs is defined in [43] with the following expression:

(||)

=

1 2

[=1

 

 

+

=1



 ]


(1)

where, M is the mid-point of distributions pi and qi. If M is the arithmetic mean, equation (1) becomes Jensen-Shannon divergence (JSD). This modified CJSD is useful when distributions are similar and it is hard to distinguish them. A further modification was proposed in [39] which extends CJSDs to exploit the metric properties of JSD. Metric-Chisini-Jensen-Shannon divergences (MCJSD) is defined as:

 - (||) = (||)

(2)

Some M-CJSD based kernels are:

2

 =  - (||)-|-2|

(3)



 = --(||)|-|

(4)



- =  - (||)--(||)|-|

(5)

Title Suppressed Due to Excessive Length

5

Likewise, replacing M-CJSD with CJSD in (3), (4) and (5), we get CJSD based kernels. In this paper, we study these kernels for Arithmetic, Geometric, and Harmonic (AM, GM, HM) means. Therefore, we employ AM, GM, HM for Amplified, Scaled, and Amplified-Scaled kernel versions for both M-CJSD and
CJSD. This results in a total of 18 kernels.

4 Gesture Myo Data

Fig. 1: Experimental Design.
As we set out to investigate how soldiers' can increase situational awareness by using gestures in military operations, our data came from physiological experimentation, however, this paper is focused on use by soldiers versus medicaldoctors [8]. Gesture datasets were collected by a Myo armband, made by Thalamic Labs (Figure 2). The device contains an inertial measurement unit (IMU) and electromyography (EMG) sensors to measure changes along the updown, left-right, and front-back axes and changes in muscle activity, respectively. These measurements are based on electrical stimulation created by flexing forearm muscles coupled with a 9-axis IMU, which in fact is comprised of 3axis gyroscope, a 3-axis accelerometer, and a 3-axis magnetometer to track arm movement.

6

P. K. Sharma et al.

Table 1: Data from all sensors in the experiment

Sensor Name

Channel

Electromyogram (EMG)

1 - 8

Accelerometer

9 - 11

Gyroscope

12 - 14

Myo Pose Classification

15

Class Label (gesturing or not gesturing) 16

The goal was to develop an algorithm

that can automatically provide the

likely start and stop indices of each

gesture segment enabling the

parsing of (and labeling of) gesture

data from the actual experiment for

further analysis and modeling.

Subjects wore the Myo armband on

their left arm and performed a series of NATO gestures one after the next according to a display on a screen in front of them. The display was run in

Fig. 2: The Myo armband used in actual experimentation with associated IMU axes and EMG sensor positions.

the Unity game engine. Subjects

relaxed their arm in between

gestures. Timing

information of the visual display and data from the Myo armband were recorded

and synchronized through custom code using LabStreamingLayer (LSL) [17].

This synchronization produced a data frame (Table 1), that enabled analyses

of all data within the same time-space (Figure 1). Detailed description of the

experimental design is given in [21].

5 Signal-Image Encoding
We consolidated 3-axis accelerometer, 3-axis gyroscope from IMU, and 8-axis EMG to produce a fused signal of length 14. After preprocessing, our data consisted of a total 38507 instances, with 24845 instances of no gesture, and 13662 instances of gestures. Each instance represented a signal, derived from the 3 different sensors (Accelerometer, Gyroscope, EMG). This resulted in a channel of length 14. In our experiment, we used the channel at each axis as a feature. Therefore, our data instances were vector valued in 14 dimensions.
Because we are dealing with short signals of length 14 each, in order to produce an image of size 4×4, we make it of length 16 by by adding 2 zeros at the end. This is called Zero Padding which is useful in many applications allowing us to increase the frequency resolution arbitrarily. This is like treating

Title Suppressed Due to Excessive Length

7

a signal as if the short burst is followed by silence and does not impact our data significantly. However, for a long zero padding a reasonable approximation of

the actual note is required. Next, we scale each signal between 0-255, and

then use Python Imaging Library (PIL, aka Pillow) library to save encoded signal as an image.

In order to

compute texture

features,we use GIST

descriptor which

uses a wavelet

decomposition of an

image [28]. After

preprocessing, our

data consisted of a total 36140 samples with 23043 no gesture, and 13097

gestures. An image

has

1

GIST

descriptor of 512

dimensions. Therefore, our data

consisted of a total

Fig. 3: Illustration of data originated from the Myo device and Unity gesture experiment program. Square wave wraps cleanly around the rest of the data, shown as noisier channels within the square wave.

36140 instances,

each representing a single GIST descriptor of 512 dimensions. Data thus obtained poses challenges with high dimensionality (Figure 4).

Fig. 4: (Left) Original signal data (Right) Image data after transforming signal.
6 Experiments
In order to detect if a gesture was made, we explore classical machine learning, deep learning, transfer learning, and anomaly (novelty) detection approaches mentioned in Section 1. We validate applied methods on fused sensor data (which we call signal data), on images obtained with our signal-image encoding scheme, and on GIST features (Section 5). For all of our experiments, we build a bi- nary classification problem by doing 80:20 percent data split into train and test/validation datasets, and label data instances as Gesture and No-Gesture.

8

P. K. Sharma et al.

Table 2: Model summary of trainable parameters

Layer (type) resnet50 (Model)

Output Shape Param # (None, 8, 8, 2048)

23587712

atten_1 (Flatten)

(None, 131072) 0

dense_1 (Dense)

(None, 1024)134218752

dropout_1 (Dropout)

(None, 1024)

0

dense_2 (Dense)

(None, 2)

2050

Total params:

157, 808, 514

Trainable params: 135, 275, 522

Non-trainable params:22, 532, 992

For the initial set of

experiments, we

employ

transfer

learning approach.

This

requires

selection of a suitable

a pre-trained model.

For this purpose, we

tried different pre-

trained models from

the list of available

models for

image classification with weights trained on ImageNet (Xception, VGG16, VGG19, ResNet, ResNetV2, InceptionV3, etc.) [9]. Our experiments show that all models give similar classification results, therefore, we report results only for ResNet50. A summary of the trainable parameters of the model is provided in Table 2. We ne-tune the model by tweaking its parameters for our data and
freeze all of the layers except the last 4 convolutional layers which we use
for training. Fine-tuning avoids limitations of model by not training from scratch on small data and saving training time (because less parameters will be updatedin training). We evaluate model performance by computing validation accuracyand validation loss.
For the next set of experiments, we explore the performance of CJSD and M-CJSD kernels in SVM classification (mentioned in Section 2), and that of DNN on both, the signal data and the GIST features. We estimated probability densities of CJSDs and M-CJSDs using multivariate kernel density estimation (KDE), a nonparametric approach, on each dataset [34,31,44]. This distribution is defined by a smoothing function and a bandwidth value that controls the smoothness of the resulting density curve. Results were validated through 10fold nested cross-validation on the randomized datasets constructed from binary classification problem. It is important to note that datasets used in experimentsfor each type of CJSD and M-CJSD kernel (A.M., G.M., H.M.) were randomized using unique random number seeds. For fair comparison, we tested RBF kernels with the same randomized datasets. This resulted in a total of 21 kernel versions(3 for RBF and 18 explained in (Section 3).
For DNN hyperparameter tuning, we searched over a large grid with Batch Size, Epochs, Optimizer, Learn Rate, Momentum, Initial Mode, Activation, Dropout Rate, Weight Constraint, and Neurons. DNN and radial basis function (RBF) kernel are used as the performance benchmark for comparison with CJSD and M-CJSD kernels. We report the classification results with sample
mean and standard error in error bar plots (see Section 7).
For GIST data, we employed PCA and used Pareto charts to select the top k principal components that explained over 99% variance [16]. This resulted
in a dimensionality of 20. For visualization, we employed t-SNE [24] (Figure 5).
Our goal is to increase confidence in decision-making in military operations using our model's ability to detect gestures correctly for soldiers being able to

Title Suppressed Due to Excessive Length

9

Fig. 5: (Left) PCA scatter and pareto plots. Notice the thick blue line corresponding to explained variance of 76% for 10 principal components. (Right) t-SNE scatter subplot. Best seen in color.
decide whether a new observation belongs to the same distribution as existing observations (inlier), or should be considered as different (outlier). Therefore, for the final set of experiments, we prepare our training data which is not polluted by outliers and detect whether a new observation is an outlier. In this context an outlier is also called a novelty. We perform anomaly detection by training the model only on the positive (known) class dataset and predicting negative (unseen) classes. Out of many available algorithms for anomaly detection, we choose One-class SVMs [6], Isolation Forests [23] and Gaussian Mixture Model (GMM) [33]. We use isotonic regression to convert the output of the GMM to a probability score. We train our models on No-Gesture as a positive class and use it to predict Gesture as a negative class. We evaluate models' performances by comparing their respective sensitivity (true positive rate) and specificity (true negative rate) along with confusion matrices.
We used LIBSVM library in MATLAB to employ aforementioned kernels [5]. For all other experiments and data preprocessing, we used the latest versions of Python and Keras framework with TensorFlow as a backend [7]. Our computing system consisted of 128 GB RAM for CPU, and NVIDIA Quadro P3200 6144MB - Memory Type: GDDR5 (Samsung). Performance in terms of time taken to run by each model is reported in Table 3.
7 Results
7.1 Classification
Transfer Learning achieved 64.52% average accuracy with 4.84 loss on validation set (Figure 6). For signal data, kernels and DNN achieved classification accuracy ranging between 70%-88%. For GIST data, it ranges between 63.76%-67.60%. Among all tested methods, each except scaled_AM gave higher classification ac-curacy on signal datain comparison to GIST data (67%) with statistical significance at the 95% confidence level (Figure 7). We use the overlap in confidence in-

10

P. K. Sharma et al.

Fig. 6: Fine tuning with weights trained on ResNet50 model by freezing all the layers except the last 4 layers we obtain (Left) Training and Validation Accuracy (Right) Training and Validation Loss.

tervals to check the statistical significance. As the intervals do not overlap,

there is at least 95% confidence (with p-value at the p < 0.05 level of

significance).

On the other hand,

Table 3: Comparison of models' performance

run-time comparison

shows that CJSD and

M-CJSD

based

kernels outperform

DNN significantly.

Because we tested 21

kernels in total,

approximate time

Model

Data

Transfer Learning DNN DNN
CJSD, M-CJSD, RBF Kernels in SVM
CJSD, M-CJSD, RBF Kernels in SVM
Anomaly Detection (One-class SVM,
Isolation Forest, GMM)

Image GIST Signal GIST
Signal
Signal and GIST

Platform
GPU GPU GPU

Time taken to run the model (Hours-Minutes-Seconds) 1 : 24 : 37 3 : 45 : 07 3 : 53 : 06

CPU

9 : 57 : 00

CPU

7 : 18 : 00

CPU

< 25 seconds

taken by each

kernel is between 20-30 minutes for both GIST and signal datasets. Time can

be computed by dividing total time with 21 from Table 3.

7.2 Novelty Detection
Table 4 summarize results from anomaly detection approaches described in Section 6. Models were trained on Gesture instances. Among all tested methods, for signal data, Isolation Forest achieves the highest accuracy of 90.51% with 100% sensitivity (blue). Moreover, for both signal and GIST datasets, GMM with Isotonic Regression achieves very similar results with respective accuracy of 90.09% and 89.79% and 100% sensitivity (green). Thus, with our proposed signal-image encoding approach; we were able to perform detection task while preserving most of the information in data transformation. It is also to significant that GMM was able to achieve higher accuracy over kernels and DNN. Figure 8 provides corresponding confusion matrices. For signal data, notice that Isolation Forest was able to detect all 24844 Gesture instance correctly with only 1 misclassification. On the other hand, for both signal and GIST datasets, GMM with Isotonic Regression was able to detect all 24845 and 23043 Gesture instances correctly with 0 misclassifications, however, with comparatively low

Title Suppressed Due to Excessive Length

11

Fig. ×7: Error bars show accuracy achieved by ×CJSD, M-CJSD, RBF kernels in SVM classification and DNN for sensor (green), GIST with image resized to 256 x 256 (blue), and GIST with original 4 x 4 image (orange). Results are
reported for 3 randomized data versions (AM, GM, HM) for CJSDs and MCJSDs. For signal data (green), notice that DNN and all metric CJSD kernel version except its scaled version outperform other kernels achieving around 88% average accuracy. RBF performs the same for each randomized data version achieving around 82% average accuracy. For GIST features (orange and blue),
the highest accuracy achieved by some kernels is around 67% and for DNN it
is around 66%. Finally, notice the high standard error for DNN and some kernels.

false positives. Also, notice that anomaly detection methods took comparatively much less run time (of the order of a few seconds) than did other classical and deep learning methods (Table 3).

Table 4: Anomaly detection results after training on Gesture instances as

explained in figure 8. Best model is colored in blue.

Accuracy

Sensitivity

Specificity

One Class SVM Isolation Forest

On sensor fusion (signal) data model

5.96%

1.02%

50.86%

90.51%

100%

4.32%

GMM with Isotonic Regression 90.09%

100%

0%

One Class SVM Isolation Forest

44.64% 86.91%

Using GIST feature descriptors 44.11% 96.51%

49.27% 2.44%

GMM with Isotonic Regression 89.79%

100%

0%

12

P. K. Sharma et al.

Fig. 8: (Left to Right) Confusion matrices for One Class SVM, Isolation Forest and GMM with Isotonic Regression trained on 80% Gesture instances, and tested on 20% Gesture + 100% No-Gesture instances with features extracted from (Above) fusion sensor (signal) data (Below) GIST Feature descriptor.
8 Conclusion
In this paper we made several contributions concerning the challenges to analyze signal data originating from multiple sources. In particular, we adopted a multisensor data fusion approach using IMU and EMG sensors to leverage abundant,raw, digital information for strategic advantage. Impetus behind our research arise from growing consideration of potential use of Internet of Things (IoT) devices in military's Multi Domain operations (MDO).
Massive data generated from IoT devices allows the full realization of system to provide situational awareness. In order to leverage communication in C2 (Command and Control) systems, and limitations of Blockchain technology which is commonly used to facilitate secure sharing of IoT datasets, we introduced a signal-image encoding approach to modify information (signal) by transforming it to an image which is invertible and easier to visualize. We proposed zero padding to address the challenges of very short signals in conversion. Significance of multi-sensor fusion and signalimage encoding approaches is to transform senseless and unstructured data into structured data from which meaningful information can be extracted to be used as actionable insights thatenable intelligent military decision-making. In general, our approach is applicable to military environments where IoT devices maybe used.

Title Suppressed Due to Excessive Length

13

Moreover, we investigated the challenge of enabling an intelligent identification and detection operation and demonstrated the feasibility of the proposed Machine Learning, Deep Learning and Anomaly Detection models to support a detection and identification of hand gestures. We evaluated all methods on hand gesture datasets before and after signal-image encoding, and extracted GIST descriptors from images. Results from anomaly detection with GMM + IsotonicRegression confirmed that it achieved statistically significantly similar accuracy for both, signal and GIST datasets. Thus, our proposed encoding approach wasable to perform detection task while preserving most of the information in datatransformation. It is also showed that GMM was able to achieve higher accuracy over classical and deep learning methods. Another virtue of tested anomaly detection methods is associated extremely low computational complexity in terms of time and memory over other classification approaches (Table 3).
Future work will extend our analysis to a more complex heterogeneous IoT environment. We will investigate the impact of governing parameters (IoT topology, connectivity, security, etc.) on model performance for data fusion. In this work we addressed the challenges of signal-image encoding arising from very short signals. We will explore methods to encode short signals for secure information transformation via wireless network to support decision making.

References
1. Summary of the 2018 national defense strategy of the united states of america, https://dod.defense.gov/Portals/1/Documents/pubs/2018-National- DefenseStrategy-Summary.pdf
2. The u.s. army in multi-domain operations 2028, https://www.tradoc.army.mil/ Portals/14/Documents/MDO/TP525-3-1_30Nov2018.pdf
3. Abdelzaher, T., Ayanian, N., Basar, T., Diggavi, S., Diesner, J., Ganesan, D., Govindan, R., Jha, S., Lepoint, T., Marlin, B., et al.: Toward an internet of battle eld things: A resilience perspective. Computer 51(11), 24 36 (2018)
4. Azmoodeh, A., Dehghantanha, A., Choo, K.K.R.: Big data and internet of things security and forensics: Challenges and opportunities. In: Handbook of Big Data and IoT Security, pp. 1 4. Springer (2019)
5. Chang, C.C., Lin, C.J.: LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology 2, 27:1 27:27 (2011), software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm
6. Chen, Y., Zhou, X.S., Huang, T.S.: One-class svm for learning in image retrieval. In: Image Processing, 2001. Proceedings. 2001 International Conference on. vol. 1, pp. 34 37. IEEE (2001)
7. Chollet, F., et al.: Keras. https://github.com/fchollet/keras (2015) 8. Ciufudean, C., Buzduga, C.: Modelling the diagnosis of industry internet of things.
In: Smart Cities, Green Technologies, and Intelligent Transport Systems, pp. 125 134. Springer (2016) 9. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: ImageNet: A LargeScale Hierarchical Image Database. In: CVPR09 (2009) 10. Drew II, M.J.V.: The army's gap in operational-level intelligence for space as part of multi-domain operations. MILITARY REVIEW (2020)

14

P. K. Sharma et al.

11. Fruhlinger, J.: The mirai botnet explained: How teen scammers and cctv cameras almost brought down the internet, https://www.csoonline.com/ article/3258748/the-mirai-botnet-explained-how-teen-scammers-and- cctv-cameras-
almost-brought-down-the-internet.html
12. Hamilton, E.: What is edge computing: The network edge explained, https:// www.cloudwards.net/what-is-edge-computing/
13. Holma, H., Toskala, A., Nakamura, T.: 5G Technology: 3GPP New Radio. John Wiley & Sons (2020)
14. Hsu, Y.L., Chou, P.H., Chang, H.C., Lin, S.L., Yang, S.C., Su, H.Y., Chang, C.C., Cheng, Y.S., Kuo, Y.C.: Design and implementation of a smart home system using multisensor data fusion technology. Sensors 17(7), 1631 (2017)
15. Hur, T., Bang, J., Lee, J., Kim, J.I., Lee, S., et al.: Iss2image: A novel signalencoding technique for cnn-based human activity recognition. Sensors 18(11), 3910 (2018)
16. Jackson, J.E.: A user's guide to principal components, vol. 587. John Wiley & Sons (2005)
17. Kothe, C., Medine, D., Boulay, C., Grivich, M., Stenner, T.: Iot data is growing fast, and security remains the biggest hurdle (2018), https://internetofthingsagenda.techtarget.com/blog/IoT-Agenda/IoT- data-is-
growing-fast-and-security-remains-the-biggest-hurdle
18. Kott, A., Alberts, D.S.: How do you command an army of intelligent things? Computer 50(12), 96 100 (2017)
19. Kumar, P., Gauba, H., Roy, P.P., Dogra, D.P.: Coupled hmm-based multi-sensor data fusion for sign language recognition. Pattern Recognition Letters 86, 1 8 (2017)
20. Latta, S.G., Mount, B.J., Poulos, A.G., Kohler, J.A., Tomlin, A.C., Steed, J.T.: Multiple sensor gesture recognition (Jun 26 2018), uS Patent 10,007,349
21. Lee, M., Harrison, A., Winkler, R.: Squad-level command and control using the myo for tactical hand signal recognition. In: 21st International Command and Control Research and Technology Symposium (ICCRTS) (2016)
22. Liggins II, M., Hall, D., Llinas, J.: Handbook of multisensor data fusion: theory and practice. CRC press (2017)
23. Liu, F.T., Ting, K.M., Zhou, Z.H.: Isolation forest. In: 2008 Eighth IEEE International Conference on Data Mining. pp. 413 422. IEEE (2008)
24. Maaten, L.v.d., Hinton, G.: Visualizing data using t-sne. Journal of machine learning research 9(Nov), 2579 2605 (2008)
25. Manaligod, H.J.T., Diño, M.J.S., Ghose, S., Han, J.: Context computing for internet of things. Journal of Ambient Intelligence and Humanized Computing (2019)
26. Miraz, M.H.: Blockchain of things (bcot): The fusion of blockchain and iot technologies. In: Advanced Applications of Blockchain Technology, pp. 141 159. Springer (2020)
27. Nakamoto, S., Bitcoin, A.: A peer-to-peer electronic cash system. Bitcoin. URL: https://bitcoin. org/bitcoin. pdf (2008)
28. Oliva, A., Torralba, A.: Modeling the shape of the scene: A holistic representation of the spatial envelope. International journal of computer vision 42(3), 145 175 (2001)
29. Palumbo, F., Gallicchio, C., Pucci, R., Micheli, A.: Human activity recognition using multisensor data fusion based on reservoir computing. Journal of Ambient Intelligence and Smart Environments 8(2), 87 107 (2016)
30. Pan, S.J., Yang, Q.: A survey on transfer learning. IEEE Transactions on knowledge and data engineering 22(10), 1345 1359 (2009)

Title Suppressed Due to Excessive Length

15

31. Parzen, E.: On estimation of a probability density function and mode. The annals of mathematical statistics 33(3), 1065 1076 (1962)
32. Pollmann, M.: Iot data is growing fast, and security remains the biggest hurdle, https://internetofthingsagenda.techtarget.com/blog/IoT-Agenda/IoTdata-is-growing-fast-and-security-remains-the-biggest-hurdle
33. Prabakaran, I., Wu, Z., Lee, C., Tong, B., Steeman, S., Koo, G., Zhang, P.J., Guvakova, M.A.: Gaussian mixture models for probabilistic classi cation of breast cancer. Cancer research pp. canres 0573 (2019)
34. Rosenblatt, M.: Remarks on some nonparametric estimates of a density function. The Annals of Mathematical Statistics 27(3), 832 837 (1956)
35. Rosner, G., Kenneally, E.: Privacy and the internet of things. emerging frameworks for policy and design. usa: Center for long-term cybersecurity; 2018
36. Russell, S., Abdelzaher, T.: The internet of battle eld things: The next generation of command, control, communications and intelligence (c3i) decision-making. In: MILCOM 2018-2018 IEEE Military Communications Conference (MILCOM). pp. 737 742. IEEE (2018)
37. Sharma, P.K., Raglin, A.: Image-audio encoding for information camou age and improving malware pattern analysis. In: 2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA). pp. 1059 1064. IEEE (2018)
38. Sharma, P.K., Raglin, A.: Iot in smart cities: Exploring information theoretic and deep learning models to improve parking solutions. In: The 5th IEEE International Conference on Internet of People (IoP) (2019)
39. Sharma, P.K., Raglin, A.: Iot: Smart city parking solutions with metric-chisinijensen-shannon divergence based kernels. In: IEEE International Conference on Military Communications Conference (MILCOM) (2019)
40. Sharma, P.K.: Finding a suitable model for novel data using range transformation. Ph.D. thesis, Delaware State University, Dover, DE, USA (2016)
41. Sharma, P.K., Holness, G.: Dilation of chisini-jensen-shannon divergence. In: 2016 IEEE International Conference on Data Science and Advanced Analytics (DSAA). pp. 174 183. IEEE (2016)
42. Sharma, P.K., Holness, G.: L2-norm transformation for improving k-means clustering. International Journal of Data Science and Analytics 3(4), 247 266 (2017)
43. Sharma, P.K., Holness, G., Markushin, Y., Melikechi, N.: A family of chisini mean based jensen-shannon divergence kernels. In: 2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA). pp. 109 115. IEEE (2015)
44. Simono , J.S.: Smoothing methods in statistics. Springer Science & Business Media (2012)
45. Singh, M.S., Pondenkandath, V., Zhou, B., Lukowicz, P., Liwickit, M.: Transforming sensor data to the image domain for deep learning an application to footstep detection. In: 2017 International Joint Conference on Neural Networks (IJCNN). pp. 2665 2672. IEEE (2017)

