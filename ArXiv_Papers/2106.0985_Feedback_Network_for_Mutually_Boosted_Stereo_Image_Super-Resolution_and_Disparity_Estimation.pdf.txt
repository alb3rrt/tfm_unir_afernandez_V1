Feedback Network for Mutually Boosted Stereo Image Super-Resolution and Disparity Estimation

arXiv:2106.00985v1 [cs.CV] 2 Jun 2021

Qinyan Dai1 , Juncheng Li1, Qiaosi Yi1, Faming Fang1 *, Guixu Zhang1
1School of Computer Science and Technology, East China Normal University, Shanghai, China 51194506008@stu.ecnu.edu.cn *fmfang@cs.ecnu.edu.cn

ABSTRACT
Under stereo settings, the problem of image super-resolution (SR) and disparity estimation are interrelated that the result of each problem could help to solve the other. The effective exploitation of correspondence between different views facilitates the SR performance, while the high-resolution (HR) features with richer details benefit the correspondence estimation. According to this motivation, we propose a Stereo Super-Resolution and Disparity Estimation Feedback Network (SSRDE-FNet), which simultaneously handles the stereo image super-resolution and disparity estimation in a unified framework and interact them with each other to further improve their performance. Specifically, the SSRDE-FNet is composed of two dual recursive sub-networks for left and right views. Besides the cross-view information exploitation in the low-resolution (LR) space, HR representations produced by the SR process are utilized to perform HR disparity estimation with higher accuracy, through which the HR features can be aggregated to generate a finer SR result. Afterward, the proposed HR Disparity Information Feedback (HRDIF) mechanism delivers information carried by HR disparity back to previous layers to further refine the SR image reconstruction. Extensive experiments demonstrate the effectiveness and advancement of SSRDE-FNet.
CCS CONCEPTS
· Computing methodologies  Reconstruction; Matching.
KEYWORDS
Stereo image super-resolution, disparity estimation, mutually boosted.
ACM Reference Format: Qinyan Dai1 , Juncheng Li1, Qiaosi Yi1, Faming Fang1 *, Guixu Zhang1, 1School of Computer Science and Technology, East China Normal University, Shanghai, China, 51194506008@stu.ecnu.edu.cn *fmfang@cs.ecnu.edu.cn. 2018. Feedback Network for Mutually Boosted Stereo Image Super-Resolution and Disparity Estimation. In Woodstock '18: ACM Symposium on Neural Gaze Detection, June 03­05, 2018, Woodstock, NY . ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/1122445.1122456
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Woodstock '18, June 03­05, 2018, Woodstock, NY © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00 https://doi.org/10.1145/1122445.1122456

1 INTRODUCTION
With the development of dual cameras, stereo images have shown greater impact in many applications, such as smartphones, drones, and autonomous vehicles. However, the stereo images often suffer from resolution degradation in practice. Therefore, a technology that can restore the high-resolution (HR) left and right views in a 3D scene is essential. In the binocular system, parallax effects between the low resolution (LR) images cause a sub-pixel shift between them. Therefore, making full use of cross-view information can help reconstruct high-quality SR images since one view may have additional information relative to the other.
Recently, several deep learning based methods have been proposed to capture cross-view information by modeling the disparity. For example, [27, 29, 30, 32, 34, 37] leverage the parallax attention module (PAM) proposed by Wang et al. [29, 30] to search for correspondences along the horizontal epipolar line without parallax limit; In [35], a pre-trained disparity network [9] was used to deploy the disparity prior into image reconstruction. Although continuous improvements have been achieved in stereo image SR, the utilization of cross-view information is still insufficient and less effective.
In fact, under stereo settings, disparity estimation and image SR are interrelated that the result of each problem could help to solve the other one, and each task benefits from the gradual improvement over the other task. However, previous methods have not explored this mutually boosted property. Moreover, all these methods exploit correspondent information only in the LR space, which usually does not provide enough accuracy in high-frequency regions due to the loss of fine-grained details in LR features. Thus, the positive additional information brought by these correspondences is still limited, hindering sufficient feature aggregation and further SR performance improvements. Thus, it is highly desirable to model disparity in a more powerful way and have a guidance mechanism that can fully interact between super-resolution and disparity estimation.
To address the aforementioned problem, we propose a novel method that can handle stereo image super-resolution and HR disparity estimation in an end-to-end framework (Figure 1), interacting in a mutually boosted manner. We perform disparity estimation in the HR space to overcome the accuracy limitation of LR correspondence and better guide the stereo SR. To achieve this efficiently, we leverage the features from LR space and the reconstructed HR space to estimate disparity in a coarse-to-fine manner. In the framework, the guidance and interaction of super-resolution and disparity estimation are three-folds: (i). the coarse correspondence estimation in LR space benefits the cross-view information exploration for SR, initial SR results and HR features for both views are produced;

Woodstock '18, June 03­05, 2018, Woodstock, NY

Qinyan Dai1 , Juncheng Li1, Qiaosi Yi1, Faming Fang1 *, Guixu Zhang1 1School of Computer Science and Technology, East China Normal University, Shanghai, China
51194506008@stu.ecnu.edu.cn *fmfang@cs.ecnu.edu.cn

Figure 1: The architecture of SSRDE-FNet, which introduces the HR disparity information feedback mechanism.

(ii). the HR representations from (i) with richer details serve as finer features for HR disparity estimation, which reduces the search range of HR disparity for better accuracy and efficiency; (iii). The HR disparity can further benefit SR reconstruction. Specifically, we align the HR features of the two views using HR disparity maps and perform attention-driven feature aggregation to produce the enhanced HR features, upon which a finer SR result is generated. To achieve a more essential facilitation of HR disparity to stereo SR, we propose the HR Disparity Information Feedback (HRDIF) mechanism that feeds the enhanced HR features and the HR disparity back to previous layers for the refinement of low-level features in the SR process. In summary, the main contributions of this paper are as follows:
· We propose a Stereo Super-Resolution and Disparity Estimation Feedback Network (SSRDE-FNet) that can simultaneously solve the stereo image super-resolution and disparity estimation in a unified framework. To the best of our knowledge, this is the first end-to-end network that can achieve the mutual boost of these two tasks.
· We propose a novel HR Disparity Information Feedback (HRDIF) mechanism for HR disparity and promote the quality of the SR image in an iterative manner.
· Extensive experiments illustrate that the proposed model can restore high-quality SR images, and the model achieves state-of-the-art results in the field of stereo image superresolution.
2 RELATED WORKS
2.1 Image Super-Resolution
Image Super-Resolution aims to reconstruct a super-resolution (SR) image from its degraded low-resolution (LR) one, which is an extremely hot topic in the computer vision field. Since the pioneer work of Super-Resolution Convolutional Neural Network (SRCNN [4]), learning-based methods have dominated the research

of single image super-resolution (SISR). Methods like VDSR [10], SRDenseNet [28], EDSR [18], MSRN [15], and RDN [39] achieved excellent performance and greatly promoted the development of SISR. However, due to the lack of reference features, the development of SISR has encountered a bottleneck, and its performance is difficult to further improve. Therefore, stereo image super-resolution has received great attention in recent years since it has the available left and right view information. The critical challenge for enhancing spatial resolution from stereo images is how to register corresponding pixels with sub-pixel accuracy. Bhavsar et al. [1] argued that the twin problems of image SR and HR disparity estimation are intertwined under stereo settings. They formulate the two problems into one energy function, and minimize it by iteratively updating the HR image and disparity map. The following conventional methods [12, 23] usually follow this pipeline, however, these methods usually take a large amount of computation time. Recently, several deep learning-based stereo SR methods have emerged by using the parallax. For example, StereoSR [7] stacks stereo images with horizontal shift intervals to feed into the network to learn stereo correspondences. However, the maximum parallax that can be processed is fixed as 64. To explore correspondences without disparity limit, Wang et al. [29, 30] proposed PASSRnet, with a parallax-attention module (PAM) that has a global receptive field along the epipolar line for global correspondence capturing. Ying et al. [37] and Song et al. [27] also made use of the PAM, while [37] incorporated several PAMs to different stages of the pre-trained SISR networks to enhance the cross-view interaction. In iPASSR [32], a symmetric bi-directional PAM (biPAM) and an inline occlusion handling scheme are proposed to further improve SR performance. Besides the PAM based methods, Yan et al. [35] uses a pre-trained disparity flow network to predict the disparity map based on the input stereo pair, and incorporates the disparity prior to better utilize the cross-view nature. Lei et al. [13] builds up an interaction

Feedback Network for Mutually Boosted Stereo Image Super-Resolution and Disparity Estimation

Woodstock '18, June 03­05, 2018, Woodstock, NY

module-based stereo SR network (IMSSRnet), in which the interaction module is composed of a series of interaction units with a residual structure.
Above methods all explore the correspondence information between stereo images only in the LR space, limiting the positive effects provided by cross-view. Our work hunts for the mutual contributions between the stereo image SR and HR disparity estimation, leading to higher image quality and more accurate disparity, which is new in literature w.r.t learning-based method.

2.2 Disparity Estimation
Disparity estimation has been investigated to obtain correspondence between a stereo image pair [19, 25], which can be utilized to capture long-range dependency for stereo SR. Existing end-to-end disparity estimation networks usually include cost volume computation, cost aggregation, and disparity prediction. 2D CNN based methods [17, 20, 33] generally adopt a correlation layer for 3D cost volume construction, while 3D CNN based methods [2, 3, 8, 22, 38] mostly use direct feature concatenation to construct 4D cost volume and use 3D convolution for cost aggregation. However, learning matching costs from 4D cost volumes suffers from a high computational and memory burden. Apart from supervised methods, several unsupervised learning methods [14, 24, 29, 36, 40] have been developed to avoid the use of costly ground truth depth annotations. Most relevantly, Wang et al. [29] uses cascaded PAM to regress matching costs in a coarse-to-fine manner, getting rid of the limitation of fixed maximum disparity in cost volume techniques. However, as Gu at al. [6] pointed out, due to computational limitation, methods usually calculate matching cost at a lower resolution by the downsampled feature maps and rely on interpolation operations to generate HR disparity. Differently, they decompose the single cost volume into a cascade formulation of multiple stages for efficient HR stereo matching. Inspired by this, we achieve the HR disparity estimation in a coarse-to-fine manner.

3 METHOD

As shown in Figure 1, we develop a Stereo Super-Resolution and

Disparity Estimation Feedback Network (SSRDE-FNet) in this paper.

The goal of our method is to obtain SR images  ,  of both

view

and

relevant

HR

disparity

maps

,


 ,

from

LR

stereo

images input  ,  , and interact the two tasks in a mutually

boosted way. In this section, we first introduce the overall insights

and network architecture in Sec. 3.1. Then, we detail the novel

proposed HR Disparity Information Feedback (HRDIF) mechanism

in Sec. 3.2. Finally, the loss functions are presented in Sec. 3.3.

3.1 SSRDE-FNet
A key to improve stereo SR is utilizing disparity for sub-pixel information registration, and a key to disparity estimation accuracy is the resolution of input features. To let these two tasks make mutually effective contribution to each other, the modeling power of both tasks are important. Thus, we propose a Stereo Super-Resolution and Disparity Estimation Feedback Network (SSRDE-FNet). As shown in Figure. 1, SSRDE-FNet is essentially a recurrent network with the proposed HR Disparity Information Feedback (HRDIF) mechanism. In each iteration, two SR reconstruction steps are involved. The HR

Figure 2: The architecture of the proposed SR backbone.

disparity is achieved in a coarse-to-fine way, coarse disparity is first estimated from LR features and the finer one is estimated from the reconstructed HR features. The advantages of this method are: (1) Stereo image SR can utilize cross-view information in multi-scales since both LR and HR correspondences can be obtained, leading to more sufficient feature aggregation; (2) The coarse-to-fine manner leads to a more compact and efficient network.
Stereo Image SR Backbone We develop a lightweight stereo SR network as shown in Figure 2(a), which leverages both intraview and cross-view LR information for image reconstruction. Since hierarchical features have been demonstrated to be effective in both SISR [15, 39] and disparity estimation [3, 8], we are also committed to maximizing the use of hierarchical features in the model. Specifically, after a convolution layer that extracts shallow features, four RDBs [39] are stacked to extract hierarchical features. Finally, we make full use of the features from all the RDBs by concatenating them and fusing them with a 1 × 1 convolution. Meanwhile, in order to alleviate the training conflict that may suffered by directly sharing features across different tasks [26] and explore more adaptive features for LR disparity estimation, a transition block is performed on  and  , expressed as:





=

 

(

),




=

  (

).

(1)

Among

them,



and



denote

the

extracted

features,




and



denote the transformed features, and   denotes the transition

block (TB). As shown in Figure 2(b)), we apply a Spatial Pyramid

Pooling (SPP) module in the TB for multi-scale feature extraction,

which can further improve model performance.

Under LR space, we explore cross-view information by sam-

pling disparity across the entire horizontal-range of a scene. To

achieve this, bi-directional parallax attention module (biPAM [32])

is adopted. In this work, it serves as both self-attention LR feature

registration and coarse disparity estimation for HR disparity initial-

ization, thus its reliability is important. However, even with deep

features, matching from unaries is far from reliable. To this end, we

cascade  biPAMs for matching cost aggregation. Therefore, the

operation of the  biPAM can be defined as:

Woodstock '18, June 03­05, 2018, Woodstock, NY

Qinyan Dai1 , Juncheng Li1, Qiaosi Yi1, Faming Fang1 *, Guixu Zhang1 1School of Computer Science and Technology, East China Normal University, Shanghai, China
51194506008@stu.ecnu.edu.cn *fmfang@cs.ecnu.edu.cn





=   

(  , -1 ) ,





=

 (,-1),

C




=

C -1

+





(


)






(

)

,



C




=

C-1

+


 ( )






(


)

,

(2)

,



=

, -1



+





,

,


=

, -1



+ ,


where  denotes two 3×3 convolutions.  and  are both 1 × 1 convolution.  is geometry-aware matrix multiplication, T is

transposition operation that exchanges the last two dimensions of a

matrix. Finally, the softmax is applied on C and C to generate parallax attention map M and M . Therefore, the warped feature maps  ,  for sub-pixel registration are generated
by the corresponding parallax attention map and inline occlusion

inline occlusion handling [32]. For each view, its own feature and

the warped feature from the other view are then sent to the fea-

ture fusion module (FFM) for cross-view information aggregation.

Instead of directly concatenate the two features, we build a resid-

ual based aggregation module (Fig. 2(c)). To allow the network to

concentrate on more informative features that are complementary

from cross-view, we first compute the residual between the two

features, and then apply a RDB [39] on the residual features, the

output features are then added back to the view's own feature. Take

the left view as example, the operation can be defined as:

 =   -  ,

(3)





=  (  ( ) +  ),

where






denotes the fused features for left view and



denotes the channel attention layer. Such inter-residual projection

allows the network to focus only on the distinct information be-

tween feature sources while bypassing the common knowledge,

enabling a more discriminative feature aggregation compared with

trivial

adding

or

concatenating.

Finally,

the

fused

features





,




go

through the reconstruction module that has the same architecture

with the feature extraction module, and a sub-pixel convolutional

layer is applied to produce the HR feature  ,  . Meanwhile, the

SR

images



0,


0

are

reconstructed

at

this

step

by

adding

the

corresponding bicubic upsampled LR images:

0


=

 

( )

+



( ),

0


=

 

( )

+



( ).

(4)

The main role of the two super-resolved images is to guarantee

the effectiveness of the HR features  ,  , which serve as important inputs to the subsequent HR disparity estimation module.

HR Disparity Estimation Module The downside to rely only

on coarse matching is that the resulting correspondences lack fine

details. Although LR correspondences have been demonstrated to

benefit the stereo SR [27, 30], the low-level LR features limit the

accuracy in correspondence matching, especially in high-frequency

regions like object boundaries, which is the most important goal

of SR. Thus, we suggest to also estimate the HR disparity map

for more fine-gained correspondence information. To ensure the

effectiveness of high-level HR features  ,  , we connect the image

reconstruction

loss

on

the

first

step

HR

results

0,


0,

thus

the

HR features  ,  can be seen as containing the information of

Figure 3: Illustration of HR disparity estimation module.

HR images, and serve as reliable representations for HR disparity

estimation. However, directly estimating from scratch costs massive

computation cost, a more efficient strategy should be adopted. We

found

that

the

disparity

maps

Disp


and

Disp

regressed

from

the parallax attention maps M and M have relative high

accuracy in most regions (see the 1 column of Tab. 4), which can

be obtained as:

 -1



Disp =


 × M  (:, :, ),

 =0

(5)

 -1



Disp =  × M (:, :, ),

 =0

where  is the width of the input LR image. Thus, we only con-

struct partial cost volumes C , C based on coarse estimation and disparity residual hypotheses to achieve disparity maps with higher

resolution and accuracy. As shown in Fig.6, the upsampled disparity

maps

(

(Disp


),



(Disp

))

are

used

as

initialization

of

the

HR

disparity estimation for the left and right view, respectively. The

disparity searching range can then be narrowed, we task the net-

work of only finding a residual to add or subtract from the coarse

prediction, blending in high-frequency details.

Specially, we denote the disparity searching residual for each

pixel in high resolution as . Take the left view as an example,

when performing × SR, for the  pixel in HR space, the disparity

range for building the left cost volume is [ ( (Disp) () -

/2, 0), ( (Disp) () +/2,  )]. By uniformly sampling

 disparity hypotheses in this range (in this work, we set  =  =

24), 3D cost volume with size  ×  ×  can be obtained through

feature correlation operation [20]. To learn more context informa-

tion, we aggregate the cost volume using hourglass architecture.

Then through soft-argmax operation, we can regress the HR dispar-

ity

Disp ,


Disp

for

both

view,

with

higher

accuracy.

For

occlusion

handling, we use the estimated disparity maps to check the geo-

metric consistency and estimate the valid masks to be used in the

loss functions:

V

=

1 -  (0.2

Disp


-







(Disp

,

Disp


)

),

(6)

V

= 1 -  (0.2

Disp

-







(Disp


,

Disp

)

),

where 

 (Disp ,

Disp


)

represents

using

Disp


to

warp

Disp

.

The HR disparity is in turn used to explore additional informa-

tion from different views in the HR space, thus the registered HR

features can be obtained by: 

=



 

(

,

Disp


),

 

=

Feedback Network for Mutually Boosted Stereo Image Super-Resolution and Disparity Estimation

Woodstock '18, June 03­05, 2018, Woodstock, NY

Figure 4: Illustration of our HR disparity information feedback (HRDIF) mechanism. (Please zoom in for details)

  ( , Disp ). For HR cross-view information aggregation, the residual-based module is adopted (similar to FFM), the only difference is that an additional attention map for each view is introduced to improve the aggregation reliability. Take the left view as example, the attention map measure the similarity of  and  :  =  (51 ( ) · 2 (  )), where 1 and 2 are both 3 × 3 convolutional layers, · is the element-wise multiplication. Therefore, the aggregated HR left features  are:

 = (  -  ) ·  , (7)
 =  (  ( ) +  ) .
where  adaptively weights down the regions with too large difference with the original view and emphasis the regions that are favorable for providing complementary information. Similarly, we can get the aggregated right HR feature  . Afterwards, better SR images can be reconstructed through  ,  :

1


=

 

( )

+



( ),

(8)

1 =   ( ) +  ( ) .

This section introduces a whole feed-forward pipeline for per-

forming the two tasks. Three stages of task interactions have been

shown: Firstly, LR disparity (correspondence) promotes image SR

by adding extra details. Secondly, image SR promotes HR disparity

estimation accuracy by providing fine-gained HR representations.

Thirdly, the more accurate disparity promotes the quality of the SR

images by aggregating features in the HR space. The interactions

mentioned above all act in a straightforward way, however, we

intend to further explore a more essential and intrinsic connection

of the two tasks.

3.2 HRDIF Mechanism
The flow of information from the LR image to the final SR image is purely feed-forward in all previous stereo SR network architectures [27, 29, 30, 37], which cannot fully exploit effective high-resolution features in representing the LR to HR relation. The purely feed-forward network also makes it impossible for the HR disparity map to send useful information to the preceding low-level features, thus cannot refine these features in the SR process. To

this end, we intend to project the useful information carried by the HR disparity back to preceding layers. Since the essential influence of the disparity to SR task is acting on the feature level, i.e., by registering the sup-pixel feature of two views and aggregating to obtain the enriched representations, we propose two strategies to feedback the HR disparity and act upon the feature space (Figure5, this illustration is based on the left view, the similar operation can be done on the right branch).
Firstly, the HR disparity information is embedded in the aggre-
gated HR features  ,  , thus we recommend to feed them back to refine the low-level features. Different from original feedback operation in [16] that simply send the high-level features of the view back to low-level layer, our feedback HR features contain information both from intra-view and cross-view. To handle the spatial resolution gap, we back-project the HR features to LR space, and leverage a simple attention strategy to highlight the high-frequency regions in the downsampled features to compensate for the resolution loss. As shown in the downside branch of Fig.5, for the  iteration, we first apply strided convolution to -1 to obtain the

back-projected feature  .






=

 

(-1) .


(9)

Secondly, in order to get the high-frequency regions, we apply

average pooling to  , then a deconvolution layer is applied to


project the feature back to original resolution, obtaining  . In

addition, the attention map   is calculated by computing the


residual between  and  .








=

 ( ),








=



(


),

(10)




= 

(

-




)

.







Then,

the

highlighted

regions

activated

by  

is

added

to




:








=




+ (

·  ),

(11)







where  is a hyper-parameter used to control the importance of the attention weights. We name this feedback operation as AHFF (Aggregated HR Feature Feedback).

Woodstock '18, June 03­05, 2018, Woodstock, NY

Qinyan Dai1 , Juncheng Li1, Qiaosi Yi1, Faming Fang1 *, Guixu Zhang1 1School of Computer Science and Technology, East China Normal University, Shanghai, China
51194506008@stu.ecnu.edu.cn *fmfang@cs.ecnu.edu.cn

It is worth noting that one of the requirements that contains in

a feedback system is providing an LR input at each iteration, i.e.,

to ensure the availability of low-level information which is needed to be refined. Thus, for the  iteration, the LR feature -1 from


the ( - 1) iteration is meant to be refined by  . Instead of

directly leveraging the coarse original feature -1, we propose the

second HR disparity information feedback strategy to enrich the

low-level representations. As shown in the upside of Figure.5, we

first

apply

spatial-to-depth

operation

upon

the

, -1




R ×

,



obtaining LR disparity cube of size R× ×2 . We leverage each

disparity slice in the cube to warp -1, obtaining 2 warped feature

maps of the right view. Each warped feature map is concatenated

with the same left feature -1, and each concatenated feature map

is going through a residual block and a 1 × 1 convolution for fusion.

Finally, we sum up the 2 fused LR feature maps to get -1. The


operation can be defined as:

2




-1



=



  (  (

(


-1,





-1, 

)

)

)

.

(12)

 =0

We name this strategy as LRE (Low-level Representations Enrich-

ment).

Finally, -1 and  are concatenated and fused to reduce the





channel back to the same with -1, and the new LR feature  for





the new iteration is generated according to:





=





(

(


-1,





)

)

.

(13)

In

this

way,

the

low-level

features




carry

information

from



the HR disparity, and this feature enhancement dose favor to the

whole pipeline right from the beginning. Finally, we adopt the last

SR output as the final result.

3.3 Loss Functions

Since our work aims to achieve stereo SR and disparity estimation

simultaneously, we set loss constraints for both tasks. Note that

we learn the disparity in an unsupersived manner and do not use

groundtruth (GT) disparities during the training phase. We intro-

duce SR loss L, biPAM loss L , and disparity loss L to train our network. The overall loss function of our network is

defined as:

L = L + 1 L + 2 L ,

(14)

where both 1 and 2 are set to 0.1 in this work. SR Loss. The SR loss is essentially an 1 loss function that is
used to measure the difference between the SR images and GT
images, i.e., for T iterations,



L

=





SR ,0


- HR

1 +  SR,0 - HR

1

 =0

(15)

+



SR ,1


-

HR

1 +

 SR,1 - HR

1,

where SR and SR represent the restored left and right images, and

HR and HR represent their corresponding HR images. BiPAM Loss. We formulate the BiPAM loss as a combination

of photometric, smoothness, cycle and consistency terms, connect-

ing

to

bi-directional

parallax-attention

maps

M




,

M

,

t=1,...,T.

That is, L = L + L + L + L . The loss

is employed in a residual manner [32] to overcome illuminance

variation. Please refer to [32] for details.

Disparity Loss. Besides tying loss on the parallax-attention

maps, we also enforce direct constraints on all the estimated dis-

parity

maps,

namely

, ,


,


,




,

,



 ,


for 

=

1, ..., .

We

first

penalize the reconstruction loss on HR images using each disparity

map (LR disparity upsamples to the same size of HR images), for

the left view,

L  = 1 

 =


1

-

S (HR

(),

HR




())



2

V , =1

(16)



+ (1 - )

HR

()

-

HR




()

1,

= 1, ..., ,

where

HR




=







(HR

,

Disp


,

).

S

is

a

structural

similarity

index (SSIM) function,  represents a valid pixel in the valid mask,

 is the number of valid pixels, and  is empirically set to 0.85. The

loss for the right view is also calculated as the similar method.

Moreover, we constrain edge-aware smoothness loss on HR dis-

parity, which is defined as:

L


=

1





pV



D


,

(p)

-  HR (p) 1
1

(17)

+



D


,

(

p)

-  HR (p) 1 ,  = 1, ...,  ,
1

where  and  are gradients in the  and  directions respectively.

Finally, residual based cycle and consistency losses [32] are also

used to constrain HR disparity maps. The total disparity loss can

be

written

as:

L

=

L

+

L
  

+

L

+

0.1



L .

4 EXPERIMENTS
4.1 Experimental Settings
Following iPASSR[32], we adopt 60 Middlebury images and 800 images from Flickr1024 [31] as the training dataset during training. For images from the Middlebury dataset, we followed [7, 29, 30, 32, 37] to perform bicubic downsampling by a factor of 2 to generate HR ground truth images to match the spatial resolution of Flickr1024 dataset. To produce LR images, we downscale the HR images on particular scaling factors by using the bicubic operation and then cropped 30 × 90 patches with a stride of 20 as input samples. Our network was implemented using PyTorch and trained on NVIDIA V100 GPU. All models were optimized by the Adam [11] with 1 = 0.9 and 2 = 0.999. The batch size is set to 16, the initial learning rate is set to 2 × 10-4 and reduced to half after every 30 epochs.
To evaluate SR results, 20 images from KITTI 2012[5], 20 images from KITTI 2015[21], 5 images from Middlebury, and 112 images from Flickr1024 are utilized as the test dataset. For fair comparison with [7, 30, 37], we followed these methods to calculate peak signalto-noise ratio (PSNR) and structural similarity (SSIM) scores on the left views with their left boundaries (64 pixels) being cropped, and these metrics were calculated on RGB color space. Moreover, to comprehensively evaluate the quality of the reconstructed stereo SR image, we also report the average PSNR and SSIM scores on stereo image pairs (i.e., (Left + Right) /2) without any boundary cropping. Meanwhile, in order to evaluate disparity estimation results, we apply the end-point-error (EPE) in both non-occluded region (NOC) and all (ALL) pixels.

Feedback Network for Mutually Boosted Stereo Image Super-Resolution and Disparity Estimation

Woodstock '18, June 03­05, 2018, Woodstock, NY

Table 1: Quantitative results achieved by different methods on the KITTI 2012, KITTI 2015, Middlebury, and Flickr1024 datasets. # represents the number of parameters of the networks. Here, PSNR/SSIM values achieved on both the left images (i.e., Left) and a pair of stereo images (i.e., (Left + Right) /2) are reported. The best results are in bold faces and the second best results are underlined.

Method
VDSR EDSR RDN RCAN StereoSR PASSRnet IMSSRnet iPASSR SSRDE-FNet (ours) VDSR EDSR RDN RCAN StereoSR PASSRnet SRRes+SAM IMSSRnet iPASSR SSRDE-FNet (ours)

Scale
×2 ×2 ×2 ×2 ×2 ×2 ×2 ×2 ×2 ×4 ×4 ×4 ×4 ×4 ×4 ×4 ×4 ×4 ×4

#
0.66M 38.6M 22.0M 15.3M 1.08M 1.37M 6.84M 1.37M 2.10M 0.66M 38.9M 22.0M 15.4M 1.42M 1.42M 1.73M 6.89M 1.42M 2.24M

KITTI 2012 30.17/0.9062 30.83/0.9199 30.81/0.9197 30.88/0.9202 29.42/0.9040 30.68/0.9159
30.90/30.97/0.9210 31.08/0.9224 25.54/0.7662 26.26/0.7954 26.23/0.7952 26.36/0.7968 24.49/0.7502 26.26/0.7919 26.35/0.7957
26.44/26.47/0.7993 26.61/0.8028

Left
KITTI 2015 28.99/0.9038 29.94/0.9231 29.91/0.9224 29.97/0.9231 28.53/0.9038 29.81/0.9191
29.97/30.01/0.9234 30.10/0.9245 24.68/0.7456 25.38/0.7811 25.37/0.7813 25.53/0.7836 23.67/0.7273 25.41/0.7772 25.55/0.7825
25.59/25.61/0.7850 25.74/0.7884

Middlebury 32.66/0.9101 34.84/0.9489 34.85/0.9488 34.80/0.9482 33.15/0.9343 34.13/0.9421
34.66/34.41/0.9454 35.02/0.9508 27.60/0.7933 29.15/0.8383 29.15/0.8387 29.20/0.8381 27.70/0.8036 28.61/0.8232 28.76/0.8287
29.02/29.07/0.8363 29.29/0.8407

KITTI 2012 30.30/0.9089 30.96/0.9228 30.94/0.9227 31.02/0.9232 29.51/0.9073 30.81/0.9190
30.92/31.11/0.9240 31.23/0.9254 25.60/0.7722 26.35/0.8015 26.32/0.8014 26.44/0.8029 24.53/0.7555 26.34/0.7981 26.44/0.8018
26.43/26.56/0.8053 26.70/0.8082

(Left + Right) /2

KITTI 2015 29.78/0.9150 30.73/0.9335 30.70/0.9330 30.77/0.9336 29.33/0.9168 30.60/0.9300
30.66/30.81/0.9340 30.90/0.9352 25.32/0.7703 26.04/0.8039 26.04/0.8043 26.22/0.8068 24.21/0.7511 26.08/0.8002 26.22/0.8054
26.20/26.32/0.8084 26.43/0.8118

Middlebury 32.77/0.9102 34.95/0.9492 34.94/0.9491 34.90/0.9486 33.23/0.9348 34.23/0.9422
34.67/34.51/0.9454 35.09/0.9511 27.69/0.7941 29.23/0.8397 29.27/0.8404 29.30/0.8397 27.64/0.8022 28.72/0.8236 28.83/0.8290
29.02/29.16/0.8367 29.38/0.8411

Flickr1024 25.60/0.8534 28.66/0.9087 28.64/0.9084 28.63/0.9082 25.96/0.8599 28.38/0.9038
-/28.60/0.9097 28.85/0.9132 22.46/0.6718 23.46/0.7285 23.47/0.7295 23.48/0.7286 21.70/0.6460 23.31/0.7195 23.27/0.7233
-/23.44/0.7287 23.59/0.7352

Left

PSNR(dB): 32.71

32.71

32.68

30.21

32.47

32.76

33.35

Right

PSNR(dB): 32.95

32.96

32.87

30.23

32.51

32.77

HR

EDSR

RDN

RCAN StereoSR PASSRnet iPASSR

Figure 5: Qualitative results (×2) on image "motorcycle" from Middlebury dataset.

33.45 Ours

Left

PSNR(dB): 29.44

29.45

29.48

28.75

28.89

29.45

29.73

Right

PSNR(dB): 29.47

29.50

29.44

27.48

28.96

29.55

HR

EDSR

RDN

RCAN SRResNet+SAM PASSRnet iPASSR

Figure 6: Qualitative results (×4) on image "testing 2" from Flickr1024 dataset.

30.02 Ours

4.2 Comparisons with SOTA Methods
We compare SSRDE-FNet with several state-of-the-art methods, including four SISR methods(VDSR, EDSR, RDN, and RCAN) and five stereo image SR methods (i.e., StereoSR, PASSRnet, SRResNet+SAM, IMSSRnet, and iPASSR). Moreover, to achieve fair comparison with SISR methods, we retrained these methods on the same training datasets as our method.
Quantitative Evaluations: In Table 1, we show the quantitative comparisons with these SR methods. Among both SISR and stereo image SR methods, our FSSRHD-net achieves the best results on all

datasets and upsampling factors (×2, ×4). This fully demonstrates the effectiveness and advancement of the proposed SSRDE-FNet.
Visual Comparison: In Figures 5 and 6, we show the visual comparisons on ×2 and ×4, respectively. According to the figure, we can clearly observe that most compared SR methods cannot recover clear and right image edges. In contrast, our SSRDE-FNet can reconstruct high-quality SR images with rich details and clear edges. This further validates the effectiveness of our SSRDE-FNet.

Woodstock '18, June 03­05, 2018, Woodstock, NY

Qinyan Dai1 , Juncheng Li1, Qiaosi Yi1, Faming Fang1 *, Guixu Zhang1 1School of Computer Science and Technology, East China Normal University, Shanghai, China
51194506008@stu.ecnu.edu.cn *fmfang@cs.ecnu.edu.cn

Table 2: Ablation study on different settings of SSRDE-FNet on Middlebury. The average PSNR and SSIM score of the SR left and right images are shown.

Method

Disparity method HRDIF HFF PSNR/SSIM Up disp HR disp AHFF LRE

baseline

baseline + Up disp



baseline + HR disp

SSR-FNet

SSRDE-FNet w/o LRE

SSRDE-FNet (Ours)

29.16/0.8361

29.20/0.8370



29.27/0.8383

 29.27/0.8385









29.35/0.8407 29.38/0.8411

4.3 Ablation Study

In order to verify the effectiveness of the proposed mutually boost

strategies, we designed a series of ablation experiments. In addition,

all ablation studies are conducted on the ×4 stereo image SR task. It

is worth noting that the baseline model does not use the HR dispar-

ity estimation mechanism and the feedback strategy. This means

that the baseline model has only one step of SR reconstruction, as

shown in Figure2.

Effectiveness of HR disparity estimation boost SR

1)Effectiveness of the HR disparity estimation method. In order

to verify that the feature aggregation by the HR disparity in HR

space benefits the SR performance, we designed three models, in-

cluding "baseline", "baseline+ Up disp", and "baseline + HR disp".

Among them, "baseline+ Up disp" means that the high-resolution

disparity directly achieved by the interpolation operation and "base-

line+ HR disp" represents our proposed method. Meanwhile, all of

these three model are in purely feed-forward manner. The PSNR

and SSIM results are presented in Table 2. According to these re-

sults, we can draw the following conclusions: (1). high-resolution

disparity can effectively improve the quality of the reconstructed

SR images; (2). the more precise disparity can bring higher perfor-

mance improvement; (3) the high-resolution disparity provided by

our method enables the model to achieve the best results.

2) Effectiveness of the HR disparity information feedback mech-

anism (HRDIF): To verify that the HR disparity truly contribute

to stereo SR in the HRDIF mechanism, but not just the original

feedback operation that plays a major role, we compare two models

that both have the feedback operation. The variant removes the

HR disparity estimation model, directly use the  and  as the high-level features to feedback. We name this variant as SSR-FNet

(Stereo SR Feedback Network), which also means adding HR Fea-

ture Feedback (HFF) to the baseline. The feedback manner in the

variant is just concatenating the down-projected HR feature and

the low-level features of the last iteration. Although noticeable

improvement can be observed, the PSNR drops 0.11 dB as compared

to our SSRDE-FNet. The experiment indicates that our method does

benefit from the HR disparity information feedback mechanism,

instead of only rely on the power of the original feedback structure.

Moreover, to verify the effectiveness of strategy of the low-level

representations enhancement (LRE) in HRDIF, we remove this op-

eration and directly concatenate -1 and  for the  iteration,





a slight PSNR drop can be observed.

Table 3: The PSNR changing of intermediate SR outputs on Middlebury.

Iteration 1

Iteration 2

Step 1 Step 2 Step 1 Step 2

Middlebury

29.16

29.25

29.32

29.38

Table 4: Average disparity EPE errors (lower is better) on

KITTI 2012 and KITTI 2015 for 4× SR. Best results are shown

in boldface.

Baseline Estimated HR PASSRnet iPASSR

disparity disparity

[30] [32]

KITTI 2012

Noc All

6.72 7.81

3.90 5.12

11.33 7.88 12.29 8.96

KITTI 2015

Noc All

5.71 6.38

3.52 4.28

9.36

6.57

9.91

7.20

Table 5: Disparity accuracy improvements across inference time on KITTI 2012 and KITTI 2015 dataset.

Iteration 1

Iteration 2

Step 1 Step 2 Step 1 Step 2

KITTI 2012

Noc ALL

7.13 8.14

6.50 7.53

4.59 5.79

3.90 5.12

KITTI 2015

Noc ALL

6.98 7.60

6.47 7.11

4.06 4.81

3.52 4.28

Figure 7: Visualization result of the disparity map on KITTI 2015.
3) SR performance improvements in a single inference: As mentioned, each iteration of SSRDE-FNet contains two SR reconstruction steps. In our experiments, we iterate the network twice (T=2) to balance the efficiency and performance. We then compare the PSNR values of all intermediate SR images. The results are shown in Table 3. Each intermediate result outperforms the former one, and the final result achieves a PSNR gain of 0.22dB over the first result. This demonstrates that the HR disparity surely benefits the information flow across time.
Effectiveness of SR boost disparity estimation 1) Comparison of disparity accuracy: We compare the estimated HR disparity and upsampled disparity of the baseline to the ground truth on the KITTI2012 and KITTI2015 datasets, shown in Table.4. We also include the disparity regressed from two stereo SR methods for comparison, including PASSRnet and iPASSR. The disparity maps estimated from LR stereo images are upsampled for fair evaluation. Even using our baseline, our disparity EPE error is obviously lower than that of other state-of-the-art stereo SR methods. By interacting stereo SR task and disparity estimation task in our network, the final HR disparity become much more accurate as compared to the straightforward baseline, with about 2  3 pixel EPE error drop. A visualization disparity result is shown in Figure.7. 2) The disparity accuracy improvements within a single inference of SSRDE-FNet: To show the changing process of the disparity estimation accuracy, we calculate the EPE error on each intermediate disparity estimation in a single inference process of SSRDE-FNet.

Feedback Network for Mutually Boosted Stereo Image Super-Resolution and Disparity Estimation

Woodstock '18, June 03­05, 2018, Woodstock, NY

The mean EPE error change in KITTI 2012 and KITTI 2015 are shown in Tab. 5. It can be observed that in each iteration, the estimated HR disparity (step2) has 0.5  0.6 pixel EPE error drop compared to the coarse estimation (step1). More obvious disparity accuracy improvements can be achieved after the HRDIF, since the low-level features are refined and lead to better disparity accuracy right from the LR space. The results above demonstrate that both stereo SR and disparity estimation are improved along time.
5 CONCLUSION
In this work, we propose to explore the mutually boosted property of stereo image super-resolution and high-resolution disparity estimation, and build a novel end-to-end deep learning framework, namely SSRDE-FNet. Our model is essentially a feedback network with a proposed HR Disparity Information Feedback (HRDIF) mechanism. By fully interacting the two tasks and making guidance to each other, we achieve to improve both tasks during a single inference. Experiments have demonstrated our state-of-the-art stereo SR performance and the disparity estimation improvements.
REFERENCES
[1] A. Bhavsar and A. Rajagopalan. 2010. Resolution Enhancement in Multi-Image Stereo. IEEE Transactions on Pattern Analysis and Machine Intelligence 32 (2010), 1721­1728.
[2] Rohan Chabra, J. Straub, C. Sweeney, Richard A. Newcombe, and H. Fuchs. 2019. StereoDRNet: Dilated Residual StereoNet. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2019), 11778­11787.
[3] Jia-Ren Chang and Y. Chen. 2018. Pyramid Stereo Matching Network. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (2018), 5410­ 5418.
[4] Chao Dong, Chen Change Loy, Kaiming He, and X. Tang. 2014. Learning a Deep Convolutional Network for Image Super-Resolution. In ECCV.
[5] Andreas Geiger, Philip Lenz, and R. Urtasun. 2012. Are we ready for autonomous driving? The KITTI vision benchmark suite. 2012 IEEE Conference on Computer Vision and Pattern Recognition (2012), 3354­3361.
[6] X. Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, Feitong Tan, and Ping Tan. 2020. Cascade Cost Volume for High-Resolution Multi-View Stereo and Stereo Matching. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2020), 2492­2501.
[7] D. S. Jeon, Seung-Hwan Baek, Inchang Choi, and M. Kim. 2018. Enhancing the Spatial Resolution of Stereo Images Using a Parallax Prior. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (2018), 1721­1730.
[8] Alex Kendall, H. Martirosyan, S. Dasgupta, and Peter Henry. 2017. End-toEnd Learning of Geometry and Context for Deep Stereo Regression. 2017 IEEE International Conference on Computer Vision (ICCV) (2017), 66­75.
[9] S. Khamis, S. Fanello, Christoph Rhemann, Adarsh Kowdle, Julien P. C. Valentin, and S. Izadi. 2018. StereoNet: Guided Hierarchical Refinement for Real-Time Edge-Aware Depth Prediction. In ECCV.
[10] Jiwon Kim, J. Lee, and Kyoung Mu Lee. 2016. Accurate Image Super-Resolution Using Very Deep Convolutional Networks. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016), 1646­1654.
[11] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. CoRR abs/1412.6980 (2015).
[12] H. S. Lee and Kyoung Mu Lee. 2013. Simultaneous Super-Resolution of Depth and Images Using a Single Camera. 2013 IEEE Conference on Computer Vision and Pattern Recognition (2013), 281­288.
[13] Jianjun Lei, Zhe Zhang, Xiaoting Fan, Yang Bolan, Li Xin-xin, Y. Chen, and Qingming Huang. 2020. Deep Stereoscopic Image Super-Resolution via Interaction Module. IEEE Transactions on Circuits and Systems for Video Technology (2020), 1­1.
[14] Ang Li and Zejian Yuan. 2018. Occlusion Aware Stereo Matching via Cooperative Unsupervised Learning. In ACCV.
[15] Juncheng Li, F. Fang, Kangfu Mei, and Guixu Zhang. 2018. Multi-scale Residual Network for Image Super-Resolution. In ECCV.
[16] Z. Li, J. Yang, Z. Liu, X. Yang, G. Jeon, and Wei Wu. 2019. Feedback Network for Image Super-Resolution. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2019), 3862­3871.
[17] Zhengfa Liang, Yiliu Feng, Yulan Guo, H. Liu, Wei Chen, Linbo Qiao, Li Zhou, and J. Zhang. 2018. Learning for Disparity Estimation Through Feature Constancy.

2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (2018), 2811­2820. [18] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung Mu Lee. 2017. Enhanced Deep Residual Networks for Single Image Super-Resolution. 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) (2017), 1132­1140. [19] W. Luo, Alexander G. Schwing, and R. Urtasun. 2016. Efficient Deep Learning for Stereo Matching. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016), 5695­5703. [20] N. Mayer, Eddy Ilg, Philip Häusser, P. Fischer, D. Cremers, A. Dosovitskiy, and T. Brox. 2016. A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016), 4040­4048. [21] Moritz Menze and Andreas Geiger. 2015. Object scene flow for autonomous vehicles. 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2015), 3061­3070. [22] Guang-Yu Nie, Ming-Ming Cheng, Yun Liu, Zhengfa Liang, Deng-Ping Fan, Y. Liu, and Yongtian Wang. 2019. Multi-Level Context Ultra-Aggregation for Stereo Matching. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2019), 3278­3286. [23] Haesol Park, Kyoung Mu Lee, and S. Lee. 2012. Combining multi-view stereo and super resolution in a unified framework. Proceedings of The 2012 Asia Pacific Signal and Information Processing Association Annual Summit and Conference (2012), 1­4. [24] Andrea Pilzer, Stéphane Lathuilière, D. Xu, Mihai Marian Puscas, E. Ricci, and N. Sebe. 2020. Progressive Fusion for Unsupervised Binocular Depth Estimation Using Cycled Networks. IEEE Transactions on Pattern Analysis and Machine Intelligence 42 (2020), 2380­2395. [25] D. Scharstein and R. Szeliski. 2004. A Taxonomy and Evaluation of Dense TwoFrame Stereo Correspondence Algorithms. International Journal of Computer Vision 47 (2004), 7­42. [26] O. Sener and V. Koltun. 2018. Multi-Task Learning as Multi-Objective Optimization. In NeurIPS. [27] Wonil Song, S. Choi, Somi Jeong, and K. Sohn. 2020. Stereoscopic Image SuperResolution with Stereo Consistent Feature. In AAAI. [28] T. Tong, Gen Li, Xiejie Liu, and Qinquan Gao. 2017. Image Super-Resolution Using Dense Skip Connections. 2017 IEEE International Conference on Computer Vision (ICCV) (2017), 4809­4817. [29] Longguang Wang, Yulan Guo, Yingqian Wang, Zhengfa Liang, Zaiping Lin, Jungang Yang, and Wei An. 2020. Parallax Attention for Unsupervised Stereo Correspondence Learning. IEEE transactions on pattern analysis and machine intelligence PP (2020). [30] Longguang Wang, Yingqian Wang, Zhengfa Liang, Zaiping Lin, J. Yang, Wei An, and Yulan Guo. 2019. Learning Parallax Attention for Stereo Image SuperResolution. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2019), 12242­12251. [31] Yingqian Wang, Longguang Wang, Jungang Yang, Wei An, and Yulan Guo. 2019. Flickr1024: A Large-Scale Dataset for Stereo Image Super-Resolution. 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW) (2019), 3852­3857. [32] Yingqian Wang, Xinyi Ying, Longguang Wang, Jungang Yang, Wei An, and Yulan Guo. 2020. Symmetric Parallax Attention for Stereo Image Super-Resolution. ArXiv abs/2011.03802 (2020). [33] H. Xu and J. Zhang. 2020. AANet: Adaptive Aggregation Network for Efficient Stereo Matching. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2020), 1956­1965. [34] Qingyu Xu, Longguang Wang, Yingqian Wang, Weidong Sheng, and Xinpu Deng. 2021. Deep Bilateral Learning for Stereo Image Super-Resolution. IEEE Signal Processing Letters 28 (2021), 613­617. [35] Bo Yan, Chenxi Ma, Bahetiyaer Bare, Weimin Tan, and S. Hoi. 2020. DisparityAware Domain Adaptation in Stereo Image Restoration. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2020), 13176­13184. [36] Guorun Yang, Hengshuang Zhao, J. Shi, Z. Deng, and J. Jia. 2018. SegStereo: Exploiting Semantic Information for Disparity Estimation. In ECCV. [37] Xinyi Ying, Yingqian Wang, Longguang Wang, Weidong Sheng, Wei An, and Yulan Guo. 2020. A Stereo Attention Module for Stereo Image Super-Resolution. IEEE Signal Processing Letters 27 (2020), 496­500. [38] F. Zhang, V. Prisacariu, Ruigang Yang, and P. Torr. 2019. GA-Net: Guided Aggregation Net for End-To-End Stereo Matching. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2019), 185­194. [39] Yulun Zhang, Yapeng Tian, Yu Kong, B. Zhong, and Yun Fu. 2018. Residual Dense Network for Image Super-Resolution. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (2018), 2472­2481. [40] Chao Zhou, H. Zhang, Xiaoyong Shen, and J. Jia. 2017. Unsupervised Learning of Stereo Matching. 2017 IEEE International Conference on Computer Vision (ICCV) (2017), 1576­1584.

