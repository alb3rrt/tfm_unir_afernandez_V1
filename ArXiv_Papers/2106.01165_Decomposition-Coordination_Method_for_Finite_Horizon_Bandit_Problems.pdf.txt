arXiv:2106.01165v1 [math.OC] 2 Jun 2021

Decomposition-Coordination Method for Finite Horizon Bandit Problems
Michel De Lara, Benjamin Heymann, Jean-Philippe Chancelier
June 3, 2021
Abstract Optimally solving a multi-armed bandit problem suffers the curse of dimensionality. Indeed, resorting to dynamic programming leads to an exponential growth of computing time, as the number of arms and the horizon increase. We introduce a decompositioncoordination heuristic, DeCo, that turns the initial problem into parallelly coordinated one-armed bandit problems. As a consequence, we obtain a computing time which is essentially linear in the number of arms. In addition, the decomposition provides a theoretical lower bound on the regret. For the two-armed bandit case, dynamic programming provides the exact solution, which is almost matched by the DeCo heuristic. Moreover, in numerical simulations with up to 100 rounds and 20 arms, DeCo outperforms classic algorithms (Thompson sampling and Kullback-Leibler upper-confidence bound) and almost matches the theoretical lower bound on the regret for 20 arms.
Keywords. multi-armed bandit problem, dynamic programming, decomposition-coordination, DeCo heuristic
1 Introduction
A multi-armed bandit (MAB) is a mathematical model for sequential decision making under partial feedback. At each round, a decision maker selects an arm, and then the arm yields a random reward that depends on the intrinsic characteristics of the arm, which are unknown to the decision maker. The selection of an arm hence serves two purposes: amassing reward and acquiring information on the arm, to be used in the future rounds. For this reason, MABs embody the well-known exploration-exploitation tradeoff and have concentrated the attention of several research communities (reinforcement learning, statistics, operations research, economics. . . ).
CERMICS, Ecole des Ponts, Marne-la-Vall´ee, France Criteo, Paris, France
1

The first occurrence of MABs in the literature was motivated by clinical trials [20], but the rise of the digital economy has unlocked many new applications [8, 17, 21]. As observed in [6], two schools emerged from the early works on MABs. The first school follows [1] and aims at maximizing the expected total reward over a discounted horizon, and envisions the multi-armed bandit as a Markov Decision Process (MDP). The pioneering breakthrough is the Gittins Index Theorem [12] which provides a way to decompose the problem into tractable sub-problems, one per arm. The second school follows Robbins formulation [18] and minimizes an expected regret over a finite horizon. The seminal work [15] identifies an asymptotically efficient policy. Other problem formulations and approaches were proposed following this milestone [16, 3].
In this article, we take the MDP perspective, and aim at minimizing the Bayesian regret of a binary bandit over a finite horizon. Theoretically, such problem could be addressed with dynamic programming, but this is not feasible because of the curse of dimensionality: the problem size grows exponentially in the number of arms. We leverage the ideas from [4] to show how time decomposition (dynamic programming) can be made compatible with arm decomposition. Indeed, it is illustrated how a structured, large scale intertemporal maximization problem can be transformed into a collection of parameterized, simpler, intertemporal subproblems by relaxing coupling constraints. Thus doing, one obtains a collection of local value functions, one per subproblems, all functions of a common coordinating parameter process. After optimizing this latter, one sums the local value functions and uses the resulting surrogate global value function in the online phase of the Bellman equation. This gives both a theoretical upper bound for the original maximization problem, and a heuristic online policy. Our contribution is twofold: first, we introduce a novel way to establish a lower bound for the optimal regret; second, we derive, from this principled approach, a policy that achieves state of the art performances on the experiments we ran.
The paper is organized as follows. In Sect. 2, we describe the stochastic multi-armed bandit problem, show how it can be framed in the multistage stochastic optimal control formalism, and then adapt the method in [4] to finally show how this control problem can be treated by decomposition-coordination. In Sect. 3, we benchmark DeCo against Thomson Sampling (Ts) [20, 8], Kullback-Leibler upper-confidence bound (Kl-Ucb) [10] and, in the case of two arms, the exact dynamic programming resolution.
2 Decomposition-coordination method for the bandit problem
In §2.1, we describe the stochastic multi-armed bandit problem, and show how it can be framed in the multistage stochastic optimal control formalism. In §2.2, we adapt the method in [4], and we show how this control problem can be treated by decomposition-coordination.
2

2.1 Multistage stochastic optimal control formulation
For any integers a  b, a, b denotes the subset {a, a + 1, . . . , b - 1, b}. We consider a decision-maker (DM) who selects an arm a in a finite set A, at each discrete time step t in the set 0, T -1 , where T  1 is an integer. Thus doing, the arm a delivers, at the end of the time interval [t, t+1[, a random variable1 Wta+1 that takes two values2 in the set {B, G} ("bad" B, "good" G) and with unknown parameter p¯aG, the probability of the event Wta+1 = G. The parameter p¯aG  [0, 1] is unknown to the DM, which we formalize below.

Probabilistic model Let  = p = (pB, pG)  R2 pB  0 , pG  0 , pB + pG = 1 be the

one-dimensional simplex3. For any p = (pB, pG)  , we denote by B(pB, pG) =

T t=1

pBB + pGG

the probability on {B, G}T which corresponds to a sequence of independent (Bernoulli) ran-

dom variables with values in {B, G}. For any {pa}aA = {(pBa, pGa)}aA  aA , we consider the probability aA B(pBa, pGa) on the product space aA{B, G}T , which corresponds to
independence between arms in A. We denote by E{pa}aA the corresponding mathematical expectation. We suppose that the DM holds a prior 0a over the unknown pa = (pBa, pGa)  , for every arm a  A. In practice, we will consider a beta distribution (nB, nG) on , with

positive integers nB > 0 and nG > 0 as parameters.

We consider the probability space (, F , P) where  = aA  × {B, G}T , F = 2, P = aA 0a d(pBa, pGa)  B(pBa, pGa). Then, Wa = {Wta}t 1,T denotes the coordinate mappings for every arm a  A, with Wta a random variable having values in the set {B, G}. For a given family {(p¯aB, p¯aG)}aA  aA  and for 0a =  , (p¯aB ,p¯aG ) for every arm a  A, the family {Wta}aA,t 1,T consists of independent random variables, where Wta has (Bernouilli)
probability distribution with parameter p¯aG  [0, 1], that is, P Wta = B = 1 - p¯aG and P Wta = G = p¯aG. With this probabilistic model, we represent the sequential independent
outcomes of |A| independent arms.

Decision model We consider a sequence U = {Ut}t 0,T -1 of random variables (on the probability space (, F , P)), where Ut = {Uat }aA, Uat  {0, 1}, a  A, t  0, T - 1 . Their possible values in {0, 1}, represent that either arm a has been selected at the beginning
of the time interval [t, t+1[ (Uat = 1) or not (Uat = 0). Since, at each given time, one and only one arm has to be selected, we add the constraint

Uat = 1 , t  0, T - 1 .

(1)

aA

This way of modeling the selection of a unique arm is not the most common in the bandit

literature, but we can find it for example in [6].

1The shifted index t + 1 is here to indicate that the random variable Wta+1 materializes during the time interval [t, t + 1[.
2We call these two values "bad" (for B), and "good" (for G), and not {0, 1} to avoid confusion with the
possible values for the controls ("do not select arm", "select arm"). In fact, we take two values for the sake
of simplicity, but we could have taken a finite or even infinite number of values. 3For the sake of symmetry between outcomes B and G, we do not identify the simplex  with the unit
segment [0, 1] by the mapping  (pB, pG)  pB  [0, 1].

3

Information and admissible controls When the arm a has been selected at stage t
(that is, when Uat = 1), the DM observes the outcome, in the set {B, G}, of the random variable Wta+1. When the arm a has not been selected at stage t (that is, when Uat = 0), the DM observes nothing. Thus, the DM observes the random variable Yt+1 = Uat Wta+1 aA, t  0, T - 1 , and the admissible controls U = {Ut}t 0,T -1 are those that satisfy

(Ut)  (Y0, U0, Y1, . . . , Ut-1, Yt) , t  0, T - 1 ,

(2)

where (Z)  F is the -field generated by the random variable Z on the probability space (, F, P).

Random rewards We suppose given a family {Lat }aA,t 0,T -1 of functions Lat : {B, G} 

R, that represent instantaneous rewards as follows. When the arm a has been selected at

stage t (that is, when Uat = 1), the random variable Wta+1 materializes and the DM receives the payoff 1 × Lat (Wta+1) = Uat Lat (Wta+1). When the arm a has not been selected at stage t (that is, when Uat = 0), the DM receives the payoff 0 = Uat Lat (Wta+1). Thus, the total random

reward associated with the control U = {Ut}t 0,T -1 is given by

T -1 t=0

aA Uat Lat (Wta+1).

Optimality criteria in the Bayesian framework Let () denote the set of probability

distributions on the simplex . We denote by 0 = {0a}aA  aA () the family of initial priors, one for each arm, and we formulate the following maximization problem -- where the

supremum is taken over U = {Uat }aA,t 0,T -1  {0, 1}A× 0,T -1 , subject to constraints (1) and (2),

T -1

V0(0) = sup

0a( dpa)E{pa}aA

Uat Lat (Wta+1) .

(3)

()A aA

t=0 aA

2.2 Dynamic programming and arm decomposition

Now, adapting the method in [4], we show how the stochastic optimal control problem (3) can be treated by decomposition.

Proposition 1. For any vector µ = {µt}t 0,T -1  RT of multipliers, we define the family {Vta[µ]}aA,t 0,T of functions Vta[µ] : N × N  R by the following backward induction: for all arm a  A,

VTa[µ](nBa, nGa) = 0 , (nBa, nGa)  N × N ,

Vta[µ](nBa, nGa) = max Vta+1[µ](nBa, nGa), -µt

(4)

nBa +
nBa + nGa

Lat (B) + Vta+1[µ](nBa + 1, nGa)

nGa + nBa + nGa

Lat (G) + Vta+1[µ](nBa, nGa + 1)

,

(nBa, nGa)  N × N , t  0, T - 1 .

4

Then, identifying (by an abuse of notation) V0 (nB0a)aA, (nG0a)aA with the value V0(0) of problem (3) when 0 = {(nB0a, nG0a)}aA, we have the upper bound

T -1

V0 (nB0a)aA, (nG0a)aA

 inf µRT

V0a[µ](nB0a, nG0a) + µt

aA

t=0

.

(5)

Proof. The proof is in two steps. First, we transform the stochastic optimal control problem (3) under imperfect information into one under perfect state information. Second, we show how this latter can be decomposed, arm by arm, and we provide an upper bound.
It is well-known that a stochastic optimal control problem under imperfect information like (3) can be turned into a perfect state information (see [2, Chapter 10] for instance), but with the state being a probability distribution. For this purpose, we need to introduce some notation. For a = (nB, nG), we set a B = nB/(nB + nG) and a G = nG/(nB + nG). We also define the two shift mappings B(nB, nG) = (nB + 1, nG) and G(nB, nG) = (nB, nG + 1).
By [2, Propositions 10.5 and 10.6], it can be shown that the imperfect state information problem (3) can be reduced to a perfect state one, with information state t = {ta}aA = {(nBt a, nGt a)}aA, information state transition kernels

kt( dt+1 | t, ut) = kta( dta+1 | ta, uat ) ,
aA

(6a)

where kta( dta+1 | ta, uat ) = with the one-stage payoff

ta ta B Bta + ta G Gta

if uat = 0 , if uat = 1 ,

(6b)

L~t(t, ut) = uat ta BLat (B) + ta GLat (G) ,

(7)

aA

so that the original stochastic optimal control problem (3) under imperfect information is

T -1
V0(0) = sup E

Uat ta BLat (B) + ta GLat (G)

t=0 aA

ta+1  kta( dta+1 | ta, Uat ) , a  A ,

Ut = {Uat }aA  {0, 1}A , (Ut)  ({ta}aA) , t  0, T - 1 ,

Uat = 1 , t  0, T - 1 .

aA

(8a)
(8b) (8c) (8d) (8e)

Now, adapting the method in [4], we show how the stochastic optimal control problem (8) can be treated by decomposition. For this purpose, we are going to dualize4 the equality
4We have chosen the dualization term -µt aA Uat - 1 and not µt aA Uat - 1 because it is likely that, at the optimum, µt  0. Indeed, had we chosen the constraint aA Uat - 1  0 (corresponding to selecting at most one arm, hence either no arm or a single arm), we would have considered multipliers -µt  0, hence µt  0.

5

constraints (8e). For any vector µ = {µt}t 0,T -1  RT of multipliers, we readily get that (see (8a)) aA Uat ta BLat (B) + ta GLat (G) - µt aA Uat - 1 =
aA Uat ta BLat (B) + ta GLat (G) - µt + µt. Hence, from (8), we obtain

T -1

V0(0)  sup

E Uat ta BLat (B) + ta GLat (G) - µt

aA t=0

ta+1  kta( dta+1 | ta, Uat ) , a  A ,

Ut = {Uat }aA  {0, 1}A ,

(Ut)  ({ta}aA) , t  0, T - 1 ,

T -1
+ µt
t=0

(9a)
(9b) (9c) (9d)

and, because of the separability with respect to arms a  A,

T -1

= sup E

Uat ta BLat (B) + ta GLat (G) - µt

aA

t=0

ta+1  kta( dta+1 | ta, Uat ) ,

Uat  {0, 1}A ,

(Uat )  (ta) , t  0, T - 1 ,

T -1
+ µt
t=0

(10a)
(10b) (10c) (10d)

where the feedback constraint (9d) is reduced to (10d) because there is no loss of optimality as each inner maximization problem in (10) only depends on ta, and not on the ta for a = a.
The corresponding dynamic programming equations, for the Bellman value functions Vta[µ] of each inner maximization problem in (10), are given by

a  A , a  () , VTa[µ](a) = 0 , t  0, T - 1 , Vta[µ](a) =

max
ua{0,1}

ua

a BLat (B) + a GLat (G) - µt + kta(d a | a, ua)Vta+1[µ]( a)


.

As a consequence, by (6b) we obtain that VTa[µ](a) = 0 , Vta[µ](a) = max Vta+1[µ](a), a B Lat (B) + Vta+1[µ](Ba) + a G Lat (G) + Vta+1[µ](Ga) - µt . Thus, by (9) and (10),

the optimal value V0(0) in (3) is such that V0(0) 

aA V0a[µ](0a) +

T -1 t=0

µt

, µ 

RT . Then, we readily get (5).

3 The DeCo algorithm
To go beyond the limitations of the curse of dimensionality, we rely on the use of the decentralized control policy obtained by arm decomposition as described in §2.2. We call this algorithm DeCo (decomposition-coordination algorithm). By contrast to the (brute
6

force) dynamic programming solution (Bf), in the decomposed formulation we have to solve Bellman equations for each arm, and thus we use Dynamic programming with a state of dimension 2, no matter the number of arms. The DeCo algorithm is made of an offline computation and of an online computation phases as follows. The offline phase is summarized in Figure 1: it consists in the minimization of a dual function , where each evaluation of  relies on solving |A| independent Bellman equations. The minimization step can be performed by gradient descent.

3.1 Offline phase of the DeCo algorithm

The offline phase of the DeCo algorithm is the minimization of the dual function (µ) =

aA V0a[µ](0a) +

T -1 t=0

µt

, for all µ  RT , for a family {0a}aA = {(nB0a, nG0a)}aA of

beta priors. The algorithm is summarized in Figure 1 and its four steps are as follows.

Multiplier (µ(tk))t 0,T -1

µ(tk+1) = µ(tk) - t(tk)

Arm 1, compute Vt1 [µk ]

···

Arm A compute VtA [µk ]

Monte Carlo estimation

A

(tk) = E

Uat ,(k) · - 1 , t  0, T - 1

a=1

Figure 1: The decomposition coordination algorithm (DeCo)

(S1) Choose an initial vector µ(0)  RT of multipliers.
(S2) At step k, given a vector µ(k)  RT of multipliers, compute the collection Vta[µ(k)] t 0,T ,aA of Bellman value functions given by (4) and the collection of associated optimal controls. The computation is performed in parallel, arm per arm. Note that Vta[µ(k)], the Bellman value function at time t  0, T , is to be evaluated only on the finite grid {(nB0a + nBa, nG0a + nGa) | nBa + nGa  t}. Note also that, when all the arms share the same prior and the same instantaneous reward, a unique sequence of Bellman value functions is to be computed, that is, all the arms share the same sequence of Bellman value functions.
(S3) Once gotten the subfamily V0a[µ(k)] aA of initial Bellman value functions at step k, update the vector of multipliers by a gradient step to obtain µ(k+1). The gradient of the dual function  with respect to the multipliers is obtained by computing the expectation of the dualized constraint as formulated in Problem (9) (see [5] for more details). Numerically, the expectation is obtained by Monte Carlo simulations. The gradient step can be replaced by a more sophisticated algorithm such as the conjugate gradient or the quasi-Newton method. In our numerical experiments, we use a solver (limited memory Bfgs) of the Modulopt library from Inria [11].
7

(S4) Stop the iterations (stopping criterion) or go back to Item S2 with multiplier µ(k+1).

3.2 Online phase of the DeCo algorithm

The stochastic optimal control problem (8) is, theoretically, solvable by dynamic program-
ming. Denoting by {Vt}t 0,T the corresponding Bellman value functions, an optimal policy would be given by the feedback (where t = {ta}aA = {(nBt a, nGt a)}aA)

Ut(t)  arg max

L~t(t, ut) +

Vt+1(t+1)kt( dt+1 | t, ut) .

ut={uat }aA{0,1}A aA uat =1

()

(11)

The DeCo algorithm consists in replacing the Bellman value function Vt+1 by aA Vta+1[µ],

using the collection

Vta+1[µ]

,
aA

of

Bellman

value functions

given

by

(4),

and a

suitable

vector µ  RT .

Using the expressions (6a)­(6b) for the kernels kt( dt+1 | t, ut) and (7) for the new one-stage payoff L~t(t, ut), an easy computation (using a A Vta+1[µ] = a A Vta+1[µ] -
a =a
Vta+1[µ]) gives the following policy. When the state of the multi-armed system is given by (nBt a, nGt a)aA  aA N×N at time t, the DeCo algorithm selects an arm A[µ] {(nBt a, ntGa)}aA in5

arg max
aA

-

Vta+1[µ](nBt a, nGt a)

+

nBt a nBt a + nGt a

Lat (B) + Vta+1[µ](nBa+1, nGa)

+

nGt a nBt a + nGt a

Lat (G) + Vta+1[µ](nBa, nGa+1)

.

(12)

The structure of policy (12) is that of a nonstationary6 index policy. Indeed, the right hand
side in (12) is a quantity that depends only on t and on the state (nBt a, nGt a) of arm a at time t. The DeCo policy used in numerical experiments is the policy A[µ] in (12), where µ is given by the offline phase of the DeCo algorithm.

4 Numerical experiments

In this section, we present numerical results. The policies U = {Uat }aA,t 0,T -1 are compared using the expected Bayesian regret given by

T -1

R(U) =

0a( dpa)E{pa}aA

UBt a,a - Uat Wta+1 ,

()A aA

t=0 aA

(13)

where we have set the instantaneous costs Lat equal to 1 on G and 0 on B for and where the Ba (best arm) policy is, for all a  A, given by UBt a,a = 1  a  arg maxa A paG , and

5In case of non uniqueness, take any arm in the arg max. 6If we had considered an infinite horizon, we would have obtained a (stationary) index policy.

8

where the prior is supposed to be the uniform law for all arms. As an example the DeCo policy is defined, for all a  A, by UDt eCo,a = 1  a  A[µ] {(nBt a, nGt a)}aA , where A[µ] is defined in (12). See Footnote 5 in case of non uniqueness.

4.1 Algorithms tested
The DeCo algorithm The expected Bayesian regret (13) is evaluated using the DeCo policy detailed above. Numerically, the expected Bayesian regret is obtained by Monte Carlo simulations, where the expectation with respect to the prior is obtained with a sample of size 1000 and expectation with respect to the arms parameters is obtained with a sample of size 1000 in Figure 2 and of size 100 in Figure 3.
Moreover, the Bellman upper bound given by the right hand side of Inequality (5), associated with the multiplier µ, yields the inequality

R(U)  RLb =

|A| (T - 1) -

|A| + 1

T -1
V0a[µ](0a) + µt .

(14)

aA

t=0

The lower bound RLb, for the expected Bayesian regret (13), will then be compared to the expected Bayesian regret obtained by the following policies (DeCo, Ts, Kl-Ucb and, possibly, to the exact value given by Bf).

The Ts and the Kl-Ucb algorithms The Ts and Kl-Ucb policies are index policies. The associated expected Bayesian regret values (13) are obtained, as for the DeCo policy, by Monte Carlo simulations. It should be noted that the Monte Carlo samples are the same for all the evaluated policies.

Brute force Dynamic programming (Bf ) Here, we do not describe a policy U = {Uat }aA,t 0,T -1 , but an algorithm Bf to compute V0(0) in (3). Solving the maximization problem (3), that is, computing V0(0) for a given prior (like, for instance, the uniform law given by the beta distribution (1, 1) for all arms) can be done using Dynamic programming on the equivalent formulation (8). This is however only possible for relatively small instances of problem (3), that is, for a limited number |A| of arms and a limited time horizon T . We recall here that solving the problem for |A| arms requires solving a Bellman equation with a state of dimension 2|A| (a state described by two integers per arm), which implies an exponential increase in computational cost with respect to |A|. This is an instance of what Richard Bellman referred to as the curse of dimensionality.

4.2 Results
For the sake of reproducibility, we have performed two separate implementations with two different languages, one in Python 3, the other in Nsp [7].

9

Comparison with the optimal solution on simple instances In Table 1, we compare the performance of the DeCo policy against the brute force approach Bf. As already explained, such comparison is limited by the computational cost of the brute force method. The results, expressed in term of total expected reward, are derived from the Bellman value functions -- the solution of the recursion-- for the Bf simulation and are computed by sample average (Monte Carlo simulations) for the DeCo policy (see §3 for details). We observe that the performance of the decentralized policy DeCo is close to the optimal solution while keeping the computational cost reasonable (at most 1.3 second) when the number of arms increase. As the computation of the expected Bayesian regret (13) by Monte Carlo
Table 1: Comparison of Bf and DeCo in term of total expected reward (higher is better). As, for DeCo, this quantity is estimated with Monte-Carlo simulation (see §4.1), it might happen that the empirical average makes better than Bf, but this is due to the simulation noise. For those examples for which a resolution with Bf is feasible, we observe that DeCo is very close to optimality.

|A| T Bf DeCo
2 5 2.888 2.892 2 20 12.431 12.436 2 50 31.996 31.872

|A| T Bf DeCo
3 10 6.409 6.411 3 20 13.465 13.458 5 10 6.659 6.645

simulations is computationaly expensive, scaling the computation, as the horizon increases, is left for further work.
Comparison of the regrets We also compare the DeCo policy against the Thompson Sampling policy Ts and Kullback-Leibler upper-confidence bound Kl-Ucb [10]. Note that all policies use a uniform prior as initial state.
The results are provided in Figure 2. On all cases, DeCo beats both Ts and Kl-Ucb with a comfortable margin. For the two arms case in Figure 2(a), DeCo is very close to the optimal solution, computed by dynamic programming (we used the Julia BinaryBandit library [14, 13]). Also, we observe that the lower bound (14), obtained on the regret by adapting Proposition 1, becomes very close to the regret obtained with DeCo for the experiment with 20 arms in Figure 2(d). This means that, on this instance, we are close to the optimal solution one would get with dynamic programming (which is not feasible for 20 arms).
Figure 3 shows the regret lower bound, and the DeCo, Ts and Kl-Ucb regrets as a function of the number of arms for T = 100 and T = 500 (beware: the x axis is the number of arms!). The lower bound is of no use (lower than 0) for small numbers 2 and 5 of arms. Nevertheless, when the number of arms increases, the regret of DeCo and the lower bound become quite close, which indicates that, for those examples, DeCo is close to being optimal.
10

Expected Bayesian Regret

(a) 2 arms

4

3

2

1

0

-1

-2

Ts

-3

Kl-Ucb DeCo

-4

Dp

-5

Lb

-6 0

20 40 60 80 100

horizon T (c) 10 arms

15

Ts

Kl-Ucb

DeCo

10

Lb

Expected Bayesian Regret

(b) 5 arms 10
Ts Kl-Ucb DeCo
Lb
5
0 0 20 40 60 80 100 horizon T (d) 20 arms
Ts Kl-Ucb 20 DeCo
Lb 15

Expected Bayesian Regret

Expected Bayesian Regret

10 5
5

0 0 20 40 60 80 100 horizon T

0 0 20 40 60 80 100 horizon T

Figure 2: Expected Bayesian regret (13) for DeCo, Ts and Kl-Ucb policies (the lower the better) for 2, 5, 15 and 20 arms with uniform prior. The (DeCo) lower bound Lb in (14) is also plotted.

11

Expected Bayesian Regret Expected Bayesian Regret

30

Ts 25 Kl-Ucb

DeCo

20

Lb

15

T=100

10

5

0

-5 0

10

20

Arms

80

Ts

Kl-Ucb

60 DeCo Lb

40

20

0

T=500

30

-30 0 10 20 30 40 50 60 70 80 90

Arms

Figure 3: Expected Bayesian regret (13) for DeCo, Ts and Kl-Ucb with uniform prior, as functions of the number of arms. The (DeCo) lower bound Lb in (14) is also plotted and demonstrates that DeCo is close to the optimal solution when the number of arms is large enough. Here, the expectation with respect to the prior is obtained with samples of size 1000 and expectation with respect to the arms parameters is obtained with samples of size 100 (compared to 1000 in Figure 2).

12

5 Conclusion
The numerical results demonstrate the value of the decomposition-coordination approach: DeCo performances are close to the optimal Bayesian solution for several configurations of arms and horizons, while keeping the computing time reasonable. Further works include extensions to the discounted infinite horizon case, to the frequentist setting, as well as the theoretical analysis of the DeCo policy. Also, while we have not reproduced their results, the DeCo policy seems to have performance close to the state of the art [19, 9].
References
[1] R. Bellman. A problem in the sequential design of experiments. Sankhy¯a: The Indian Journal of Statistics (1933-1960), 16(3/4):221­229, 1956.
[2] D. P. Bertsekas and S. E. Shreve. Stochastic Optimal Control: The Discrete-Time Case. Athena Scientific, Belmont, Massachusetts, 1996.
[3] S. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multiarmed bandit problems. arXiv preprint arXiv:1204.5721, 2012.
[4] P. Carpentier, J.-P. Chancelier, M. De Lara, and F. Pacaud. Mixed spatial and temporal decompositions for large-scale multistage stochastic optimization problems. Journal of Optimization Theory and Applications, 186(3):985­1005, 2020.
[5] P. Carpentier, J.-P. Chancelier, V. Lecl`ere, and F. Pacaud. Stochastic decomposition applied to large-scale hydro valleys management. European Journal of Operational Research, 270(3):1086­1098, 2018.
[6] J. Chakravorty and A. Mahajan. Multi-armed bandits, Gittins index, and its calculation. Methods and applications of statistics in clinical trials: Planning, analysis, and inferential methods, 2(416-435):455, 2014.
[7] J.-P. Chancelier. Website: http://cermics.enpc.fr/~jpc/nsp-tiddly. Nsp, a numerical computing environment, 2021.
[8] O. Chapelle and L. Li. An empirical evaluation of thompson sampling. In J. ShaweTaylor, R. Zemel, P. Bartlett, F. Pereira, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 24. Curran Associates, Inc., 2011.
[9] V. F. Farias and E. Gutin. Optimistic Gittins indices. In Proceedings of the 30th International Conference on Neural Information Processing Systems,(3161-3169), 2016.
[10] A. Garivier and O. Capp´e. The KL-UCB algorithm for bounded stochastic bandits and beyond. In Proceedings of the 24th annual conference on learning theory, pages 359­376. JMLR Workshop and Conference Proceedings, 2011.
13

[11] J. C. Gilbert and X. Jonsson. LIBOPT ­ An environment for testing solvers on heterogeneous collections of problems, 2007.
[12] J. Gittins, K. Glazebrook, and R. Weber. Multi-armed bandit allocation indices. John Wiley & Sons, 2011.
[13] P. Jacko. Binarybandit: An efficient julia package for optimization and evaluation of the finite-horizon bandit problem with binary responses, 2019.
[14] P. Jacko. The finite-horizon two-armed bandit problem with binary responses: A multidisciplinary survey of the history, state of the art, and myths, 2019.
[15] T. L. Lai and H. Robbins. Asymptotically efficient adaptive allocation rules. Advances in applied mathematics, 6(1):4­22, 1985.
[16] T. Lattimore and C. Szepesv´ari. Bandit algorithms. Cambridge University Press, 2020. [17] L. Li, W. Chu, J. Langford, and R. E. Schapire. A contextual-bandit approach to
personalized news article recommendation. In Proceedings of the 19th international conference on World wide web, pages 661­670, 2010. [18] H. Robbins. Some aspects of the sequential design of experiments. Bulletin of the American Mathematical Society, 58(5):527­535, 1952. [19] D. Russo and B. Van Roy. Learning to optimize via information-directed sampling. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc., 2014. [20] W. R. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3/4):285­294, 1933. [21] J. Weed, V. Perchet, and P. Rigollet. Online learning in repeated auctions. In Conference on Learning Theory, pages 1562­1583. PMLR, 2016.
14

