A Topological Solution of Entanglement for Complex-shaped Parts in Robotic Bin-picking
Xinyi Zhang1, Keisuke Koyama1, Yukiyasu Domae2, Weiwei Wan1,2, and Kensuke Harada1,2

arXiv:2106.00943v1 [cs.RO] 2 Jun 2021

Abstract-- This paper addresses the problem of picking up only one object at a time avoiding any entanglement in binpicking. To cope with a difficult case where the complex-shaped objects are heavily entangled together, we propose a topologybased method that can generate non-tangle grasp positions on a single depth image. The core technique is entanglement map, which is a feature map to measure the entanglement possibilities obtained from the input image. We use entanglement map to select probable regions containing graspable objects. The optimum grasping pose is detected from the selected regions considering the collision between robot hand and objects. Experimental results show that our analytic method provides a more comprehensive and intuitive observation of entanglement and exceeds previous learning-based work [1] in success rates. Especially, our topology-based method does not rely on any object models or time-consuming training process, so that it can be easily adapted to more complex bin-picking scenes.

Fig. 1. Failure case and success case for a parallel jaw of picking up a single object from a random clutter using proposed entanglement map. Grasp marked as green shows the failure example of picking up multiple objects. Grasp marked as blue is successfully generated avoiding the region containing tangled objects.

I. INTRODUCTION
Bin-picking is commonly utilized in industrial robot assembly line to provide and arrange necessary parts for assembly. It refers to picking up objects randomly placed in a bin. Bin-picking has been studied over the decades covering vision, planning, system integration, and solutions to specific tasks [2], [3], [4], [5], [6], [7], [8]. However, there still remain challenges due to the complex physical phenomenon or the uncertainty of the environment. For instance, complex-shaped objects randomly placed in the bin are extremely easily entangled with each other. When the shape of target object is complex such as S-shaped or C-shaped, the entanglement situation becomes much more complex to analyze. In this case, the robot has difficulty in picking up only one object without any entanglement.
A few studies address this issue in robotic bin-picking. Matsumura et.al [1] first tackle the problem of picking only one object from a stacked pile without causing entanglement. They propose a learning-based method that can predict potentially entangled objects and generate grasp positions avoiding target object tangled with others. In [1], a Convolutional Neural Network (CNN) is proposed to predict whether if the robot can pick up a single object among several pre-computed grasp candidates. They also use a physics simulator to collect training data by simulating binpicking process. However, we found some limitations in this research as follows. On one hand, data-driven method requires previous training and data collection so that it can only be verified empirically, while analytic approaches
1Graduate School of Engineering Science, Osaka University, Japan 2Artificial Intelligence Research Center, National Institute of Advanced Industrial Science and Technology (AIST), Japan

can provide the a series of criteria for entanglement in bin-picking. On the other hand, since CNN only makes predictions on cropped regions from input image, it cannot observe all the entanglement from the whole input depth map. Especially when the objects are heavily entangled, CNN may predict that all pre-computed grasping candidates share high possibilities of potential entanglement.
Motivated by the previous work, we present an analytic approach to solve the entanglement issue in bin-picking using topological knowledge. In particular, we propose topology coordinates to obtain a series of metrics which can describe entanglement situation from a single depth image. Besides, we scan the input image in a sliding window manner to generate a feature map called entanglement map, which indicates the possibilities of containing entangled objects for each region. As Fig.1 shows, entanglement map is able to discriminate which regions contain tangled objects and which do not from a depth map. The regions marked as blue has high possibilities of containing graspable objects. Once the entanglement map is obtained, we select non-tangle regions and detect collision-free grasp candidates using graspability measure [2] respectively on selected regions. The output is ranked grasp configurations of avoiding all entanglement and collisions.
Our main contributions are as follows. 1) We proposed a topology-based approach that can detect optimal grasps avoiding entanglement, which is a challenging problem in robotic bin-picking. 2) We fix the problems existing in previous work. Besides, our method only requires simple parameter tuning instead of time-consuming training and data collection. 3) We provide a complete observation and intuitive un-

derstanding of entanglement so that the performance is improved dealing with complex-shaped parts.
4) We develop a vision-based bin-picking system and demonstrate bin-picking experiments on a real robot.
II. RELATED WORK Researches in grasp detection for bin-picking can be categorized into two approaches according to how the grasp is determined. Some researches develop the grasp detection algorithm using shape or geometric information directly on a depth map or a point cloud [2], [3], [9]. Some researches use CAD model for better recognizing the scene [10], [11], [12]. Recently learning-based method has been widely used to achieve better robustness and generalization [1], [13], [14]. However, robotic bin-picking faces some challenging tasks related to picking up complex-shaped objects. As described in Section I, Matsumura et.al. tackle the challenge of manipulating potentially-tangled objects in bin-picking for the first time [1]. They propose a learning-based approach to predict non-tangle grasping pose among several pre-computed grasp candidates. The major limitation of this work is timeconsuming training process and data collection. Besides, the network is only applied on the cropped regions of input image, which may increase bias in the prediction. Other works such as [15] and [16] also solve the problem of picking up only one single tube in bin-picking. They develop a geometric modeling method to fit multiple cylinders to an input point cloud and plan an escaping trajectory to let the robot drag out only one object. Nevertheless, modeling and trajectory planning would be unstable when objects are in dense clutter. In this paper, we present an analytic approach without any object model or data collection, which make it relatively easier to execute on a real robot.
Fig. 2. GLI identifies whether two strands are tangled or not.
Topological representation has been studied for decades and it is widely used for solving complex robotic motion synthesis. Ho and Komura firstly present the definition of topology coordinates to analyze whole-body behaviors between two humanoid robot [17], [18], [19]. They calculate the topological relationship between two robot characters applying for different scenes. The most significant representation of entanglement would be Gaussian Link Integral (GLI) developed from knot theory [20]. It describes mathematical relationship between two tangled strands as Fig.2 shows. GLI can be applied for entanglement discrimination both in 3dimensional space or a projection of 2-dimensional plane. Moreover, topological representation plays an important role for robotic manipulation of deformable objects, such as tubes or ropes [21], [22], [23], [24], [25]. This research is the first one to use topological knowledge in robotic bin-picking.

The topological solution provides a more comprehensive observation for dense clutter than previous learning-based method.

III. TOPOLOGY COORDINATES FOR BIN-PICKING
In this section, we introduce our revised topology coordinates based on the original theory proposed by Ho and Komura [18], [19]. The details of calculation is also presented.

A. Topology Coordinates
Topology coordinate is constructed between two tangled objects by three attributes [18], [19]. First attribute is writhe, which explains how much the two curves are twisting around each other. For instance, entangled objects get higher score than separated objects. Writhe between two objects is calculated by Gaussian Link Integral (GLI) as follows. If we have two curves 1 and 2 which are point sets in Cartesian Coordinate, writhe can be calculated by GLI as follows.

1 GLI(1, 2) = 4

1

d1 × d2 · (1 - 2)

2

||1 - 2||3

(1)

Second attribute is density, which describes how much the

twisted area is concentrated on two curves. Third attribute is

center, which is a location explains the center location of the

twisted area. Density and center provide extra information

for entanglement respectively on distribution and position.

B. Topology Coordinates in Depth Map
Given a single depth image of cluttered scene, topology coordinates can be constructed to measure the entanglement (Fig.3). For a single depth map, writhe is a scalar attribute that indicates how much objects are tangled together. Depth map containing tangled objects has higher writhe than the one with objects just overlapped together. Density is also a scalar attribute which indicates the distribution of entanglement is evenly or intensively on the depth map. Center indicates the center position of entanglement on the depth map.
Original topology coordinates [18] can only be applied for two characters and assume the exact position of characters

Fig. 3. Proposed topology coordinate illustrated by several rigid industrial parts.

are known. Different from the definition and calculation, we construct topology coordinates only using a single depth image containing multiple objects so that the position of each object remains unknown. Instead of computing relationship between two objects, we extract the line segments of edge from the depth map and calculate the topology coordinates using the relationship between each pair of line segments. Edge contains all information we need to describe the shape and position of objects. Even though the line segments of edges may not indicate the complete contours of all parts, the topological relationship calculated by line segments can still reflect how and where the entanglement occurs.

C. Calculation

We use a depth map I to compute topology coordinates G = (w, c, d). In order to calculate these three attributes, we need to generate a matrix called writhe matrix T firstly. Taken I as input, we detect the edges and transfer them to a collection of 3-dimensional vectors L = (l1, l2, ..., ln). Writhe matrix T is a n × n matrix that stores GLI of each segment pair in L. Particularly, instead of using Eq.(1), GLI between two 3-dimensional line segments is computed using the algorithm proposed by Klenin and Langowski [26]. For instance, Ti, j in the writhe matrix between i-th segment li and j-th segment l j can be calculated by

Ti, j = GLI(li, l j)

(2)

It can be seen that writhe matrix T is a upper-triangle-like matrix where half of elements in T are zero. We can compute the writhe w, density d and center c using writhe matrix T . First, writhe w is the sum of all values in T divided by the number of line segments as follows.

1n n

  w

=

n i=1

Ti, j
j=1

(3)

Then, density d is calculated by the ratio of the pairs that has higher value in writhe matrix T . We extract the nonzero elements from T and compute d using the number of elements higher than some threshold divided by the total number of non-zero elements. Here, we define the threshold as the mean of extracted non-zero elements. Finally, center c is simply obtained by the center of mass for matrix T , which is a segment pair that contributes the most to the entanglement.

D. Explanations
Let us explain how the topology coordinates serve for understanding entanglement using a depth map.
Firstly, we present the visualization of the writhe matrix in order to analyze more intuitively. This visualized matrix derives from T in Section.III-C since it only remains the larger elements and is resized to a certain size. In other words, it it the visualized version of matrix that is used to calculate density. In the visualized matrix, we can observe the values of entanglement for each segment pair, and the distributions of these segment pairs. We can directly see what density and center refer to in this visualized matrix.

Fig. 5. Scenes with different writhe and similar density. Overlapped objects in (b) has lower writhe. Writhe value w and density d is also written.
Fig. 6. Scenes with different density and similar writhe. Visualized writhe matrix show that for the sparse clutter such as (b), density would be lower. Writhe value w and density d is also written.
Fig. 7. Illustration of generating center from a depth map. Center is a pair of segments marked as green. Center mask is a binary matrix with a area consisting both center segments.
Writhe. Fig.5(a) shows how the writhe explains entanglement. (a) is the situation where two objects are twisting together while (b) refers to two simply overlapped objects. They have similar density d but differ from writhe w. Writhe of twisted objects is larger than overlapped ones. If the robot wants to pick objects from theses scene, (a.2) with a lower writhe has higher possibility of a successful picking.
Density. Fig.6 shows the visualization of density and how it describes entanglement. We can tell by human observation that (b) would be a better choice for robot simply by taking a look. Visualized writhe matrix and density value can also

(a) Entanglement map generation.

(b) Grasp detection.

Fig. 4. Proposed grasp detection method to generate optimal grasps avoiding any entanglement and collision.

explain the scene numerically. Visualized matrix in (b) has a even distribution of brighter pixels since every line segment tends to tangle with more segments. Because as the number of segment pairs with larger GLI increases, the number of bright pixels in the visualized matrix also increases. Thus, these brighter pixels distribute more evenly, in other words, the density becomes smaller. On the contrary, every object in (a) is concentrated with each other heavily so that the entanglement is distributed intensely on the depth map. For the visualized matrix, the pixels with larger value is just located around the axis. Therefore, when writhe values are similar, density can also contribute to entanglement analysis.
Center. Additionally, we present how center effects the entanglement by presenting a mask which has the same size as input depth image (Fig.7). The center is computed by the center of mass of the writhe matrix, which is a pair of line segments that contributes the most to the entanglement. Center mask is generated by a region containing both two line segments. It somehow indicates the position information of entanglement but not as much as writhe and density do.
To summarize, by focusing on the metrics of entanglement regardless of the number of object, situation with lower writhe and lower density is preferred. Therefore, the topology coordinate proposed in this section can be used to determine where a non-tangle grasp should located.
IV. GRASP DETECTION OF AVOIDING ENTANGLEMENT
This section elaborates grasp detection method for picking up only one object avoiding all entanglement in the clutter. We scan the depth image following a sliding window fashion to obtain analytic metrics of entanglement. The metrics can be computed by the writhe and density values in each region, the center mask of the whole input image, and the weights evaluated by topology coordinate from the whole image. The suitable grasps can be detect away from the entangled regions.
A. Overview
We select a parallel jaw gripper and use a single depth map as input to compute grasp hypotheses. The overview of proposed grasp detection method is illustrated in Fig.8.

Fig. 8. Proposed bin-picking pipeline.
First, a depth map of piled objects is captured, and it is used to construct a topology coordinate. Then, we use writhe value in topology coordinates to determine if the objects in the bin are tangled. If not, grasp is detected only considering the collision. If entanglement exists, we calculate the entanglement map which provides the position information for the potential tangled parts in the box. We crop several region candidates with high possibilities of containing graspable objects from entanglement map. Finally, combined with graspability measure [2] which is can avoid collision between robot gripper and the objects, the optimal grasps can be obtained.
B. Entanglement Representation: Entanglement map
We explain how to compute entanglement map by a given depth map I. First, we compute the line segments for edges on input depth map I, and generate topology coordinate G = (w, c, d) along with center mask cmask for I . Then, we use a pre-defined sliding window function for I to obtain expanding information. For each window, we calculate its own writhe and density. This sliding window function returns two matrices Ws, Ds, which respectively store writhe and density of each region. The combination of two matrices refers to the rough entanglement information on each regions from I. However, we would like to precisely evaluate the entanglement situation upon the whole image. We use calculated topology coordinate G to evaluate the weights for these matrices and center mask. The initial weights are manually defined as W = 0.8, D = 0.15, D = 0.05 respetively for Ws, Ds and Cmask since writhe affect more on predicting potential tangled regions. If d, average of Ds, is larger than

d of the coordinate G, it means that density may affect the result of entanglement map generation. Therefore, the weights are modified as,

D = (d/d)D; W = 1 - D

(4)

The center mask is independent from sliding window algorithm so that the weight C remains the same. Finally, entanglement map E is obtained by addition of weighted metrics as Eq.(5) shows following a bi-linear interpolation.

E = W Ws + D Ds + cCmask

(5)

Some examples are presented in Fig.9. In our perspective, entanglement map is the visualization that indicates possibilities of entanglement in every region for the whole depth map. We can observe that areas where objects are heavily tangled with each other are marked as yellow, while blue areas refers to non-tangled regions. We prefer to generate grasps on blue areas to avoid entanglement.

score. Finally, the best grasp position is selected as the top of ranked grasp candidates.
V. EXPERIMENTS AND RESULTS
A. Experiment Setup
We perform several real-world robot experiments to evaluate our method in bin-picking. We use NEXTAGE from Kawada Robotics and set a fixed 3D camera YCAM3D-II one meter straight above from the bin. We use Choreonoid and graspPlugin to simulate and execute the movement of robot. The execution time was recorded on a PC running Ubuntu 16.04 with a 2.7 GHz Intel Core i5-6400 CPU. Our experiment system is set as Fig.10 shows.

Fig. 10. Experiment setting. We fix a 3D camera at the height of 1 meter above the parts bin. We select the parallel jaw gripper to execute picking.

Fig. 9. Input depth map and corresponding entanglement map. Yellows stands for entangled region. Blue area is the region where has low possibility of entanglement.

C. Grasp Detection
We present a grasp detection method considering both entanglement and collisions. Another sliding window function is executed to seek the regions with smaller sum in entanglement map, where the best grasping poses have higher possibilities locating. We select several region candidates with smaller values in entanglement map. In each region, a grasping pose can be computed using graspability measure [2]. Graspability is an index for detecting a grasping position by convoluting a template of contact areas and collision areas for a robot hand. To put it more precisely, it is based on the idea that the object should be in the trajectory of hand closing, and there should be no object in the position to lower the robot hand. We use a parallel jaw gripper and rotate the gripper template along for orientations. The output is a pixel location on the depth map which refers to the best grasp pose and a rotation angle indicating the orientation of parallel jaw gripper. For the detected grasp candidates in each region, we rank them simply by pixel values in entanglement map of the corresponding positions combined with graspability

Fig. 11. Types of objects and pile patterns used in the experiment.
As Fig.11 shows, two types of industrial parts with complex shapes are selected. We prepare three patterns of clutter state by only C-shaped objects, only S-shaped objects, and mixed objects. In particular, three picking trials are performed for each clutter. Each trial contains 20 times of picking to record the success rate of only picking one object.
The purpose of experiment is to compare our method with two baselines.
1) Graspability. We select graspability measure[2] which is a general grasp detection algorithm only using a hand template. It outputs several grasp candidates ranked by graspability measure.
2) CNN. We also select previous work from [1] which is the first approach to predict potential tangled objects in binpicking. It takes the same grasp candidates as Graspabilty, but rank them with a prediction network.

Fig. 12. Experiment results using the same depth maps. Best grasps are emphasized using green dot. (a) Results of Graspability. Grasps marked as red denote to the best grasp. Yellow refers to grasps with the ranked order of 2nd, 3rd, 4th, and 5th. (b) Results of CNN. The grasp candidates are the same as (a) while CNN predicts the best grasps marked as green. (c) Result of proposed method. Both depth maps and entanglement maps are presented. Red denotes to the best grasp. Yellow refers to grasps with the ranked order of 2nd and 3rd.

TABLE I SUCCESS RATES OF PICKING ONE SINGLE OBJECT

Success rate Time cost (s)

C-shaped object S-shaped object Mixed objects
Total

Graspability[2]
11/20 6/20 8/20
25/60
2.1

CNN[1]
14/20 8/20 10/20
32/60
2.7

Ours
15/20 10/20 14/20
39/60
7.8

B. Bin-picking Performance
First, we evaluate the success rates and time costs for bin-picking experiments (Table I). The number after slash denotes to the total picking times of one trial, and the one before slash is the number of times when the robot

picks up only one object. As a baseline, grasability struggles in success rate since if can not discriminate whether the target is entangled with others or not. Our method and CNN both reach relatively higher success rates for picking single object. Particularly, our method improves the performance of picking from S-shaped objects by a success rate of 50%. The reason why CNN struggles with S-shaped object is that it uses quarters of depth map to make predictions. Even if the cropped image contains complete shape of target objects, it still lack of information of entangling with others. Our method directly evaluates entanglement for a complete depth map to solve the problem bothering CNN. For the mixed objects, our method reaches a high success rate of 70% since our model-free method only focus on the information of edges in the depth map. The superior performance of our method indicates that our hand-engineered approach can analyze the relationship between these objects directly and

efficiently. CNN may require more evaluation for generalization while our method can be utilized without any training process.
In addition, the average time costs of Graspability, CNN, and our method are 2.1s, 2.7s, 7.8s. The time cost of our method depends on how many line segments detected from the depth map. Fig.13 shows the computational time for only generating entanglement map. The number of line segments and time cost follow an approximated linear function. In the experimental condition, we estimate the edge of depth image using a line detector and cut the shorter line segments. We limit the number of line segments per image to 129 so that the average time cost is 7-8s per grasp.

Common failures that result from our method is caused by the situation where the selected region does not containing any graspable positions. Even if the entanglement map can be generated correctly, grasps can not be detected due to the collisions in the selected region. In this future, it may be possible to add more collision information during generating the entanglement map to improve the performance.

Fig. 14. (a) Our method can not apply to this type of object. (b-c) Piled clutter provides too much information. It makes the topology coordinates can not handle extra edge segments, which leads to fail prediction.

Fig. 13. Time cost(s) for entanglement map corresponding to the number of line segments from depth map.
C. Qualitative Analysis
From the examples presented in Fig.12, we validate how our method select graspable objects qualitatively. For the same depth map as input, we use baselines and our method to detect optimal grasp positions and the top-ranked grasp is marked using green dots. It is observed that our method can directly find the objects that are not tangled with others in the bin, while Graspability and CNN always focus on objects at the top of the clutter. Especially in the fifth column when all five grasp candidates are classified as tangled, both existing approaches predict poorly while our method successfully find the graspable objects without any entanglement in the bin. Graspable objects selected by proposed method is similar as the observation of humans. The reason is that our method uses edge and topological knowledge to explain entanglement relationship more intuitively, which guarantees a complete observation of all potential entanglement in the bin.
D. Discussion
Let us consider other kinds of industrial parts like Fig.14 shows. This type of object provides too much edge information for topology coordinates, which may cause some misunderstandings for topology coordinates. In this case, learning-based approach would be necessary. However, since our method prefers the objects with the shape of pure edges such as rigid linear objects with a smooth edge, it is possible to develop our method on manipulation of deformable linear objects such as tubes or ropes in the future work.

VI. CONCLUSIONS
In this paper, we present a topology-based solution for robot to only pick only one object in robotic bin-picking. We develop the entanglement map which is a topological feature map describing the entanglement situation for a stacked pile. A grasp synthesis method is proposed to search for the suitable grasp avoiding picking up entangled objects from the whole input image. We reach fine success rates on real world experiment. Our method is dependable upon its generalization capability even if for mixed objects in bin-picking. Particularly, we do not need large training data to make predictions since proposed method can obtain the topological relationship of entangled objects even for complex-shaped objects.
REFERENCES
[1] R. Matsumura, Y. Domae, W. Wan, and K. Harada, "Learning based robotic bin-picking for potentially tangled objects," in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 7990­7997, IEEE, 2019.
[2] Y. Domae, H. Okuda, Y. Taguchi, K. Sumi, and T. Hirai, "Fast graspability evaluation on single depth maps for bin picking with general grippers," in 2014 IEEE International Conference on Robotics and Automation (ICRA), pp. 1997­2004, IEEE, 2014.
[3] O. Ghita and P. F. Whelan, "A bin picking system based on depth from defocus," Machine Vision and Applications, vol. 13, no. 4, pp. 234­ 244, 2003.
[4] A. Zuo, J. Z. Zhang, K. Stanley, and Q. J. Wu, "A hybrid stereo feature matching algorithm for stereo vision-based bin picking," International Journal of Pattern Recognition and Artificial Intelligence, vol. 18, no. 08, pp. 1407­1422, 2004.
[5] J. Kirkegaard and T. B. Moeslund, "Bin-picking based on harmonic shape contexts and graph-based matching," in 18th International Conference on Pattern Recognition (ICPR'06), vol. 2, pp. 581­584, IEEE, 2006.
[6] J.-K. Oh, S. Lee, and C.-H. Lee, "Stereo vision based automation for a bin-picking solution," International Journal of Control, Automation and Systems, vol. 10, no. 2, pp. 362­373, 2012.
[7] K. Harada, W. Wan, T. Tsuji, K. Kikuchi, K. Nagata, and H. Onda, "Initial experiments on learning-based randomized bin-picking allowing finger contact with neighboring objects," in 2016 IEEE International Conference on Automation Science and Engineering (CASE), pp. 1196­1202, IEEE, 2016.
[8] K. Harada, W. Wan, T. Tsuji, K. Kikuchi, K. Nagata, and H. Onda, "Experiments on learning based industrial bin-picking with iterative visual recognition," arXiv preprint arXiv:1805.08449, 2018.

[9] D. C. Dupuis, S. Le´onard, M. A. Baumann, E. A. Croft, and J. J. Little, "Two-fingered grasp planning for randomized bin-picking," in Proc. of the Robotics: Science and Systems 2008 Manipulation WorkshopIntelligence in Human Environments, 2008.
[10] B. Drost, M. Ulrich, N. Navab, and S. Ilic, "Model globally, match locally: Efficient and robust 3d object recognition," in 2010 IEEE computer society conference on computer vision and pattern recognition, pp. 998­1005, Ieee, 2010.
[11] C. Choi, Y. Taguchi, O. Tuzel, M.-Y. Liu, and S. Ramalingam, "Votingbased pose estimation for robotic assembly using a 3d sensor," in 2012 IEEE International Conference on Robotics and Automation, pp. 1724­1731, IEEE, 2012.
[12] D. Chetverikov, D. Svirko, D. Stepanov, and P. Krsek, "The trimmed iterative closest point algorithm," in Object recognition supported by user interaction for service robots, vol. 3, pp. 545­548, IEEE, 2002.
[13] J. Mahler, J. Liang, S. Niyaz, M. Laskey, R. Doan, X. Liu, J. A. Ojea, and K. Goldberg, "Dex-net 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp metrics," arXiv preprint arXiv:1703.09312, 2017.
[14] R. Matsumura, K. Harada, Y. Domae, and W. Wan, "Learning based industrial bin-picking trained with approximate physics simulator," in International Conference on Intelligent Autonomous Systems, pp. 786­ 798, Springer, 2018.
[15] G. Lea~o, C. M. Costa, A. Sousa, and G. Veiga, "Perception of entangled tubes for automated bin picking," in Iberian Robotics conference, pp. 619­631, Springer, 2019.
[16] G. Lea~o, C. M. Costa, A. Sousa, and G. Veiga, "Detecting and solving tube entanglement in bin picking operations," Applied Sciences, vol. 10, no. 7, p. 2264, 2020.
[17] E. S. Ho and T. Komura, "Wrestle alone: Creating tangled motions of multiple avatars from individually captured motions," in 15th Pacific Conference on Computer Graphics and Applications (PG'07), pp. 427­430, IEEE, 2007.
[18] E. S. Ho and T. Komura, "Character motion synthesis by topology coordinates," in Computer Graphics Forum, vol. 28, pp. 299­308, Wiley Online Library, 2009.
[19] E. S. Ho, T. Komura, S. Ramamoorthy, and S. Vijayakumar, "Controlling humanoid robots in topology coordinates," in 2010 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 178­ 182, IEEE, 2010.
[20] G. N. Reeke Jr, "Protein folding: Computational approaches to an exponential-time problem," Annual review of computer science, vol. 3, no. 1, pp. 59­84, 1988.
[21] H. Wakamatsu, A. Tsumaya, E. Arai, and S. Hirai, "Planning of one-handed knotting/raveling manipulation of linear objects," in IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA'04. 2004, vol. 2, pp. 1719­1725, IEEE, 2004.
[22] M. Saha and P. Isto, "Motion planning for robotic manipulation of deformable linear objects," in Proceedings 2006 IEEE International Conference on Robotics and Automation, 2006. ICRA 2006., pp. 2478­ 2484, IEEE, 2006.
[23] M. Saha and P. Isto, "Manipulation planning for deformable linear objects," IEEE Transactions on Robotics, vol. 23, no. 6, pp. 1141­ 1150, 2007.
[24] V. Ivan, D. Zarubin, M. Toussaint, T. Komura, and S. Vijayakumar, "Topology-based representations for motion planning and generalization in dynamic environments with interactions," The International Journal of Robotics Research, vol. 32, no. 9-10, pp. 1151­1163, 2013.
[25] M. P. Bell, W. Wang, J. Kunzika, and D. Balkcom, "Knot-tying with four-piece fixtures," The International Journal of Robotics Research, vol. 33, no. 11, pp. 1481­1489, 2014.
[26] K. Klenin and J. Langowski, "Computation of writhe in modeling of supercoiled dna," Biopolymers: Original Research on Biomolecules, vol. 54, no. 5, pp. 307­317, 2000.

