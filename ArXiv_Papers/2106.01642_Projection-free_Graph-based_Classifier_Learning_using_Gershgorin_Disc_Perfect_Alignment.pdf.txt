Projection-free Graph-based Classifier Learning using Gershgorin Disc Perfect Alignment

arXiv:2106.01642v1 [cs.LG] 3 Jun 2021

Cheng Yang Shanghai Jiao Tong University
Shanghai 200240, China bobcy@sjtu.edu.cn
Wai-tian Tan Cisco Systems San José, CA 95134 dtan2@cisco.com

Gene Cheung York University Toronto M3J 1P3, ON, Canada genec@yorku.ca
Guangtao Zhai Shanghai Jiao Tong University
Shanghai 200240, China zhaiguangtao@sjtu.edu.cn

Abstract
In semi-supervised graph-based binary classifier learning, a subset of known labels x^i are used to infer unknown labels, assuming that the label signal x is smooth with respect to a similarity graph specified by a Laplacian matrix. When restricting labels xi to binary values, the problem is NP-hard. While a conventional semidefinite programming (SDP) relaxation can be solved in polynomial time using, for example, the alternating direction method of multipliers (ADMM), the complexity of iteratively projecting a candidate matrix M onto the positive semi-definite (PSD) cone (M 0) remains high. In this paper, leveraging a recent linear algebraic theory called Gershgorin disc perfect alignment (GDPA), we propose a fast projection-free method by solving a sequence of linear programs (LP) instead. Specifically, we first recast the SDP relaxation to its SDP dual, where a feasible solution H 0 can be interpreted as a Laplacian matrix corresponding to a balanced signed graph sans the last node. To achieve graph balance, we split the last node into two that respectively contain the original positive and negative edges, resulting in a new Laplacian H¯ . We repose the SDP dual for solution H¯ , then replace the PSD cone constraint H¯ 0 with linear constraints derived from GDPA-- sufficient conditions to ensure H¯ is PSD--so that the optimization becomes an LP per iteration. Finally, we extract predicted labels from our converged LP solution H¯ . Experiments show that our algorithm enjoyed a 40× speedup on average over the next fastest scheme while retaining comparable label prediction performance.
1 Introduction
Binary classification--assignment of labels to an N -sample set x  {-1, 1}N to separate two distinct classes--is a basic machine learning problem [1]. One common setting is semi-supervised graph classifier learning, where M known labels, x^i, 1  i  M , are used to infer N - M unknown labels xi, M + 1  i  N , in signal x, assuming that x is smooth with respect to (w.r.t.) a similarity graph G specified by a graph Laplacian matrix L [2, 3, 4]. This graph-based binary classification problem is NP-hard in general [5]. A conventional semi-definite programming (SDP) relaxation [6] replaces the binary label constraint with a more relaxed positive semi-definite (PSD) cone constraint (i.e., matrix variable M related to xx satisfying M 0), and the relaxed problem can be solved in polynomial time using, for example, the alternating direction method of multipliers (ADMM) [7]. However, ADMM still requires projection to the PSD cone S = {M | M 0} per iteration, which is
Preprint. Under review.

 2 -2 M =  -2 5
-1 -2
2 SMS-1 =  -3.0746
-1.6915

-1  -2  4
-1.301 5
-2.2007

1
2 -0.5912  -1.8176  3
4 -1

min(M)
j|ji|Mi,j| Mi,i 1 35 7

1 min(MM )

GDPA 2

3

9

-1 1

j|ji|MM i,j|
Mi,i 35 7 9

Figure 1: Example of a PD matrix M and its similarity transform M~ = SMS-1, and their respective Gershgorin discs i. Note that Gershgorin disc left-ends of M~ are aligned at min(M) = 0.1078.

expensive (O(N 3)) due to full matrix eigen-decomposition. An alternative approach eliminates the
binary constraint and minimizes directly a quadratic graph smoothness term called graph Laplacian regularization (GLR) x Lx [8] for x  RN , and then rounds xi's to {-1, 1}. However, in general spectral methods such as GLR do not have tight performance bounds common in SDP relaxation [9].

To ensure matrix variable M is PSD without eigen-decomposition, one naïve approach is to enforce

linear constraints derived directly from the Gershgorin circle theorem (GCT) [10]. By GCT, every

real eigenvalue  of a real symmetric matrix M resides inside at least one Gershgorin disc i--

corresponding to row i of M--with center ci(M) Mi,i and radius ri(M)

j=i |Mi,j |, i.e.,

ci(M) - ri(M)    ci(M) + ri(M), i.

(1)

The corollary is that the smallest eigenvalue, min(M), of M is lower-bounded by the smallest Gershgorin disc left-end, denoted by -min(M), i.e.,

-min(M)

min ci(M) - ri(M)  min(M).
i

(2)

Thus, to ensure M 0, one can impose the sufficient condition -min(M)  0. While replacing the PSD cone constraint with a set of N linear constraints, ci(M) - ri(M)  0, i, is attractive computationally, GCT lower bound -min(M) tends to be loose. As an example, consider the positive definite (PD) matrix M in Fig. 1(a) with min(M) = 0.1078 [11]. The first Gershgorin disc left-end is c1(M) - r1(M) = 2 - 3 = -1, and -min(M) < 0. Thus, imposing -min(M)  0 directly would unnecessarily restrict the search space and result in a sub-optimal solution to the posed problem.
A recent linear algebraic theory called Gershgorin disc perfect alignment (GDPA) [11] provides a theoretical foundation to tighten the GCT lower bound. Specifically, GDPA states that given a graph Laplacian matrix L corresponding to a balanced signed graph G [12], one can perform a similarity transform1, L~ = SLS-1, where S = diag(v1-1, . . . , vN-1) and v is the first eigenvector of L, such that the Gershgorin disc left-ends of L~ are exactly aligned at min(L) = min(L~). This means that transformed L~ satisfies -min(L~) = min(L~); i.e., the GCT lower bound is the tightest possible after an appropriate similarity transform. Continuing our example, similarity transform M~ = SMS-1 of M has all its disc left-ends exactly aligned at min(M) = min(M~ ) = 0.1078.
Leveraging GDPA, we develop a fast projection-free algorithm for semi-supervised graph classifier learning. We first observe that the optimal solution M of the SDP relaxation is an adjacency matrix to a balanced signed graph. However, GDPA requires a Laplacian matrix, which has opposite signs in the off-diagonal terms to the corresponding adjacency matrix of the same graph. Thus, we convert the problem to its SDP dual [13] and interpret the dual variable H instead as a Laplacian to a balanced graph sans the last graph node. To achieve graph balance, we split the last node into two and divide the original positive and negative edges among them, resulting in a revised Laplacian H¯ . We repose the SDP dual problem for solution H¯ , then replace the PSD cone constraint H¯ 0 with linear constraints derived from GDPA. This changes the optimization to a linear program (LP) per iteration that is solved efficiently using fast LP solvers [14]. Finally, we extract prediction labels from our converged LP solution H¯ . Experiments show that our algorithm enjoyed a 40× speedup on average over the next fastest scheme while retaining comparable label prediction performance.

1A similarity transform B = SAS-1 and the original matrix A share the same set of eigenvalues [10].

2

2 Related Work
Graph-based classification was first studied almost two decades ago [2, 3, 4]. With the advent of graph signal processing (GSP) [15, 16]--spectral analysis of discrete signals residing on combinatorial graphs--interest in the problem was revived [17, 18, 19]. The problem of learning a similarity graph from data has been extensively studied [20]. We focus instead on the orthogonal problem of predicting binary labels given a similarity graph and a subset of M labels.
The graph-based binary classification problem is NP-hard in general [5]. SDP--useful in approximating various NP-hard problems [13]--provides an intuitive relaxation [6]. An interior point method tailored for the slightly more general binary quadratic problem2 (BQP) has complexity O(N 3.5 log(1/ )), where is the tolerable error [21]. The complexity was improved to O(N 3) by SDCut [22, 23] via spectrahedron-based relaxation. Replacing PSD cone constraint M 0 with a factorization M = XX was proposed in [24], but resulted in a non-convex optimization for X that was solved locally via alternating minimization, where in each iteration a matrix inverse of worst-case complexity O(N 3) was required. More recent first-order methods for SDP such as [7] used ADMM [25, 26, 27], but the iterative projection onto PSD cone requires full matrix eigen-decomposition and thus expensive. In contrast, leveraging GDPA theory [11], our algorithm is entirely projection-free.
It is known in graph spectral theory [28] that balanced signed graphs have unique spectral properties [29]; for example, the signed graph Laplacian matrix [30] has eigenvalue 0 iff the corresponding signed graph is balanced. In contrast, extending the original GCT [10], GDPA [11] states that the Gershgorin disc left-ends of a similarity transform SMS-1 of graph Laplacian M to a balanced graph can be perfectly aligned at min(M). GDPA theory was developed for metric learning [31] to optimize a PD matrix M given a convex and differentiable objective Q(M) so that the optimal Mahalanobis distance (fi - fj) M(fi - fj) for feature vectors fi and fj can be defined. This paper leverages GDPA [11] in an entirely different direction for graph-based binary classifier learning. Specifically, observing that solution matrix H to the SDP dual is a Laplacian to a balanced graph G sans the last graph node, we augment the last node to obtain an overall balanced graph G¯, and solve a modified SDP dual for Laplacian H¯ to G¯ via GDPA linearization.

3 Preliminaries

3.1 Graph Definitions

A graph is defined as G(V, E), with node set V = {1 . . . , N }, and edge set E = {(i, j)}, where (i, j) means nodes i and j are connected with weight wi,j  R. A node i may have self-loop of weights ui  R. Denote by W the adjacency matrix, where Wi,j = wi,j and Wi,i = ui. We assume that edges are undirected, and W is symmetric. Define next the diagonal degree matrix D, where Di,i = j Wi,j. The combinatorial graph Laplacian matrix [15] is then defined as L = D - W. To account for self-loops, the generalized graph Laplacian matrix is defined as L = D - W + diag(W). Note that any real symmetric matrix can be interpreted as a generalized graph Laplacian matrix.
The graph Laplacian regularizer (GLR) [8] that quantifies smoothness of signal x  RN w.r.t. graph specified by L is

x Lx =

wi,j (xi - xj )2 + uix2i .

(3)

(i,j)E

iV

GLR is also the objective of our graph-based classification problem.

3.2 Iterative GDPA Linearization
Denote by L a generalized graph Laplacian matrix to a balanced and connected signed graph G (with or without self-loops). A balanced graph is a graph with no cycle of odd number of negative edges. By Cartwright-Harary Theorem (CHT) [12], a graph is balanced iff nodes can be colored into blue and red, such that each positive (negative) edge connects nodes of the same (different) colors. GDPA [11] states that a similarity transform L~ = SLS-1, where S = diag(v1-1, . . . , vN-1) and v is the first
2BQP objective takes a quadratic form x Qx, but Q is not required to be a Laplacian to a similarity graph.

3

Figure 2: (a) 3-node line graph example. (b) Ideal solution M to SDP primal (8) as adjacency matrix. (c) Solution H to SDP dual (12) as Laplacian matrix. (d) Solution H¯ to modified SDP dual (20) as Laplacian matrix. Positive / negative edges are colored in blue / red. Self-loop weight u4 in (c) for node 4 is u4 = y4 + z1 + z2.

eigenvector of L, has its Gershgorin disc left-ends aligned exactly at min(L), i.e.,

L~i,i - |L~i,j | = Li,i - |siLi,j /sj | = min(L), i  {1, . . . , N }.

(4)

j=i

j=i

To solve an optimization of the form minL 0 Q(L), one can leverage GDPA and optimize iteratively as follows. At iteration t with solution Lt, compute first eigenvector vt to Lt corresponding to min(Lt); extreme eigenvector vt can be efficiently computed in complexity O(ab) using Locally
Optimal Block Preconditioned Conjugate Gradient (LOBPCG) [32], where a is the number of nonzero entries in Lt and b is the iteration number till convergence3. Define scalars sti = 1/vit, i. Then for iteration t + 1, solve the following optimization:

min Q(L),
L

s.t. Li,i -

|stiLi,j/stj|  0, i  {1, . . . , N }.

(5)

j=i

Linear constraints in (5) ensure that the similarity transform L~ = SLS-1 is PSD by GCT, and hence solution L is PSD. Since scalars {sti} are computed from first eigenvector vt of Lt 0, by GDPA SLtS-1 has all its disc left-ends aligned exactly at min(Lt)  0, and hence Lt remains feasible at iteration t + 1. Thus, objective Q(Lt) is monotonically non-increasing with t, and the algorithm
converges to a local minimum. We invoke this iteration to solve our posed SDP dual as well.

4 Formulation of Graph-based Classifier Learning

We first formulate the graph-based classifier learning problem and relax it to an SDP problem in Section 4.1. We then present its SDP dual with dual variable matrix H in Section 4.2. Finally, we interpret H as a graph Laplacian, and augment its corresponding graph G to a balanced graph G¯ for GDPA linearization in Section 4.3.

4.1 SDP Primal

Given a PSD graph Laplacian matrix L  RN×N of a positive similarity graph Go (i.e., all edge weights wi,j  0), one can formulate a graph-based binary classification problem as follows:

min x Lx, s.t.
x

x2i = 1, i  {1, . . . , N } xi = x^i, i  {1, . . . , M }

.

(6)

where {x^i}M i=1 are the M known labels. The objective in (6) dictates that signal x is smooth w.r.t. graph Go specified by L. Because L is PSD [16], the objective is lower-bounded by 0, i.e., x Lx  0, x  RN . The first binary constraint ensures xi  {-1, 1}. The second constraint ensures that entries xi in signal x agrees with known labels {x^i}M i=1.

As an example, consider a 3-node line graph shown in Fig. 2(a), where edges (1, 2) and (2, 3) have weights w1,2 and w2,3, respectively. The adjacency matrix W and graph Laplacian matrix L are:

0 w1,2 0

d1 -w1,2 0

W = w1,2 0 w2,3 , L = -w1,2 d2 -w2,3

(7)

0 w2,3 0

0 -w2,3 d3

3Warm start [11] can be employed to reduce b in subsequent iterations given vt is computed repeatedly for gradually changing Lt's. See Section 5 for details.

4

where di = j|(i,j)E wi,j is the degree of node i. Suppose known labels are x^1 = 1 and x^2 = -1.

Due to the binary constraint on xi's, (6) is NP-hard [5]. One can define an SDP relaxation [5] as follows. Define first X = xx and M = [X x; x 1]. M is PSD because: i) block [1] is PSD,
and ii) the Schur complement of block [1] of M is X - xx = 0, which is also PSD. Thus, the two
constraints M 0 and rank(X) = 1 is equivalent to X = xx , which together with Xii = 1, i implies x2i = 1, i. To convexify the problem, we drop the non-convex rank constraint and write the SDP relaxation for optimization variable M as

 Xii = 1, i  {1, . . . , N }





min Tr(LX) s.t. M

x,X



Xx x1

0

(8)

 xi = x^i, i  {1, . . . , M }

where Tr(x Lx) = Tr(Lxx ) = Tr(LX). Because (8) has linear objective and constraints with an additional PSD cone constraint, M 0, it is an SDP problem. We call (8) the SDP primal.

Continuing our example, consider ground-truth labels x = [1 - 1 1] for the 3-node graph in Fig. 2(a). The corresponding solution matrix M = [xx x; x 1] is

 1 -1 1 1 

M= 

-1 1

1 -1

-1 1

-1 1

. 

(9)

1 -1 1 1

Observe that M can be interpreted as an adjacency matrix to a balanced signed graph; nodes 1, 3 and 4 can be colored blue, and node 2 can be colored red, so that positive (negative) edges connect only nodes of the same (different) colors. See Fig. 2(b) for an illustration of the corresponding signed graph when interpreting M as an adjacency matrix (self-loops are not shown). However, while the solution space for the SDP primal (8) exhibits desirable graph balance, GDPA requires instead a graph Laplacian matrix to a balanced graph, which has opposite signs in the off-diagonal terms as the adjacency matrix. This motivates us to investigate the corresponding SDP dual problem instead.

4.2 SDP Dual

We derive the dual problem based on SDP duality theory [13]. We first define

Ai = diag(eN+1(i)),

Bi =

0N ×N eN (i)

eN (i) 0

.

(10)

where eN (i)  {0, 1}N is a length-N binary canonical vector with a single non-zero entry equals to 1 at the i-th entry, 0N×N is a N -by-N matrix of zeros, and diag(v) is a diagonal matrix with diagonal entries equal to v. Note that Ai and Bi are symmetric. Next, we collect M known labels {x^i}M i=1 into a vector b  RM of length M , i.e.,

bi = 2x^i, i  {1, . . . , M }.

(11)

We now define the SDP dual of (8) as

min
y,z

1N+1y + b

z,

s.t. H

N +1

M

yiAi + ziBi +

i=1

i=1

L 0N 0N 0

0 (12)

where 1N is a length-N vector of ones, and dual variables are y  RN+1 and z  RM . Because the objective is a minimization, when bi < 0 (i.e., x^i < 0), the corresponding zi  0. Similarly, for bi > 0, zi  0. Thus, the signs of variables zi's are known a priori. Without loss of generality, we assume zi  0, i  {1, . . . , M1} and zi  0, i  {M1 + 1, . . . , M } in the sequel.

4.3 Reformulating the SDP Dual

We interpret H  R(N+1)×(N+1) in (12) as a graph Laplacian corresponding to a graph G. However, G is not a balanced signed graph, because of the last row / column in H. To see this, we write

H=

Ly g

g yN +1

(13)

5

where g = [z1 . . . zM 0N-M ] . Matrix Ly  RN×N , which equals to Ly = diag(y1, . . . , yN ) + L, is a generalized Laplacian to a N -node positive graph G+. However, node N + 1 has both positive and negative edges to G+ stemming from negative zi's and positive zi's, respectively. As a result, H
is not a Laplacian corresponding to a balanced signed graph.

Continuing our 3-node line graph example with Laplacian L, the corresponding Ly and H are

y1 + d1 -w1,2

0

Ly = -w1,2 y2 + d2 -w2,3 ,

0

-w2,3 y3 + d3

 y1 + d1

H= 

-w1,2 0

z1

-w1,2 y2 + d2 -w2,3
z2

0
-w2,3 y3 + d3
0

z1 

z2 0

. 

(14)

y4

Interpreting H as a graph Laplacian, node 4 has degree d4 = -z1 - z2. Thus, y4 = u4 + d4, and self-loop weight for node 4 iss u4 = y4 + z1 + z2. See Fig. 2(c) for an illustration of this graph G.

In graph terminology, node (N +1) has positive and negative edges, with respective weights {-zi}M i=11

and {-zi}M i=M1+1, to G+, and a self-loop with weight uN+1 = yN+1 +

M i=1

zi.

We

construct

an

augmented graph G¯ with N + 2 nodes from G by splitting node N + 1 in G into two in G¯, dividing

positive and negative edges between them. The specific graph construction for G¯ procedure is

1. Construct first N nodes with the same inter-connections as sub-graph G+.
2. Construct node N + 1 with positive edges {-zi}M i=11 and node N + 2 with negative edges {-zi}M i=M1+1 to the first N nodes in sub-graph G+.
3. Add self-loops for node N +1 and N +2 with respective weights uN+1/2- and uN+1/2+ , where  R is a parameter to be discussed.

Denote by H¯  R(N+2)×(N+2) the graph Laplacian matrix corresponding to G¯. Continuing our 3-node graph example, Fig. 2(d) shows the augmented graph G¯, and the corresponding H¯ is

 y1 + d1 -w1,2

0

z1

0



 -w1,2 y2 + d2 -w2,3

0

z2



H¯ =  0

-w2,3 y3 + d3

0

0

 . (15)

  z1

0

0

1 2

(y4

-

z1

+

z2)

-



0



0

z2

0

0

1 2

(y4

+

z1

-

z2)

+

Spectrally, H¯ and H are related; we prove that min(H¯ ) is a lower bound for min(H).

Lemma 1. The smallest eigenvalue min(H¯ ) of graph Laplacian H¯ to augmented graph G¯ is a lower

bound for min(H) of Laplacian H to G, i.e.,

min(H¯ )  min(H).

(16)

Proof. Denote by G the graph represented by generalized graph Laplacian H, with inter-node edge weights {wij} and self-loop weights {ui}. Denote by v  RN+1 the first eigenvector of H
corresponding to the smallest eigenvalue min(H). From (3), GLR of H computed using v is

M

N

v Hv =

wi,j (vi - vj )2 - zi(vN+1 - vi)2 + yivi2 + uN+1vN2 +1. (17)

(i,j)E | 1i,jN

i=1

i=1

Now construct   RN+2, where  = [v1 . . . vN vN+1 vN+1] . GLR of H¯ computed using  is

 H¯  =

M1

M

wi,j (vi - vj )2 - zi(vN+1 - vi)2 -

zi(vN+1 - vi)2

(i,j)E | 1i,jN

i=1

i=M1 +1

N
+ yivi2 +

uN+1 - 2

i=1

vN2 +1 +

uN+1 + 2

vN2 +1.

(18)

Thus, v Hv =  H¯ . Since first eigenvector v minimizes the Rayleigh quotient of H,

min(H)

=

v v

Hv v

(a)


 

H¯  

(b)


min(H¯ ).

(19)

(a) holds since v

v



by

construction,

and

(b)

holds

since

min(H¯ )

=

minx

x x

H¯ x x

.

6

From the proof above, the usefulness of parameter becomes clear: the bound min(H¯ )  min(H) becomes tight when the last two entries in the first eigenvector of H¯ are similar. To promote this, we set to an appropriate large value, so that the first eigenvector minimizing the Rayleigh quotient of H¯ would choose similar small values for the last two entries.

Given Lemma 1, we now reformulate the SDP dual (12) by keeping the same objective but imposing PSD cone constraint on H¯ instead of H. Define Ai, Bi and Bi similarly to (10) but for a larger (N + 2)-by-(N + 2) matrix; i.e., Ai = diag(eN+2(i)), Bi = [Bi 0N+1; 0N+1 0], and Bi = [0(N+1)×(N+1) eN+1(i); eN+1(i) 0]. The reformulated SDP dual is

min
y,z

1N+1y + b

z,

s.t. H¯

N

M1

M

yiAi + N+1AN+1 + N+2AN+2 + ziBi +

ziBi +

i=1

i=1

i=M1 +1

L 0N×2 02×N 02×2

(20) 0

where N+1

=

uN +1 2

-

M1 i=1

zi

-

and N+2

=

uN +1 2

-

M i=M1 +1

zi

+

. Given H¯ is now a

Laplacian to a balanced graph, we discuss the application of GDPA linearization to solve (20) next.

5 Algorithm Implementation

5.1 GDPA Linearization

We replace the PSD cone constraint on H¯ in (20) with N + 2 linear constraints via GDPA [11]. Specifically, at iteration t, we compute first eigenvector vt of solution H¯ t using LOBPCG [32]. We define scalars si = 1/vit, i  {1, . . . , N + 2}. Finally, we write N + 2 constraints corresponding to -min(SH¯ S-1)  0, where S = diag(s1, . . . , sN+2), i.e.,

yi + di - j=i |siwi,j /sj | - |sizi/sN+1|  0, i  {1, . . . , M1}

yi + di - j=i |siwi,j /sj | - |sizi/sN+2|  0, i  {M1 + 1, . . . , M }

yi + di - j=i |siwi,j/sj|  0, i  {M + 1, . . . , N }

(21)

uN+1/2 - -

M1 j=1

|sN

+1zj

/sj

|

0

uN+1/2 + -

M j =M1 +1

|sN

+2zj

/sj

|

0

where the indices for summation j=i are {1, . . . , N } \ i. Note that the absolute value operation can be appropriately removed for each term siwi,j/sj and sizi/sj, since the signs for si, wi,j and zi are known. Together with linear objective in (20), this constitutes an LP for variables y and z, solvable using any available fast LP solvers [14]. Compared to SDP primal (8) with a large matrix variable M  R(N+1)×(N+1), our LP variables, y  RN+1 and z  RM , are much smaller.
A sequence of LPs are solved, each time with scalars si's updated from computed solution H¯ t, until convergence. The bulk of the complexity resides in the computation of the first eigenvector vt for each LP solution H¯ t. LOBPCG is an iterative algorithm that can benefit from warm start [11]: with a good initial guess for vt, the algorithm converges faster. Since H¯ t changes gradually through our iterations, we use previously computed eigenvector vt-1 of H¯ t-1 as initial guess for vt of H¯ t. Experiments show that warm start reduces the iteration number till convergence significantly.

5.2 Initialization & Prediction Label Extraction
Our LP in Section 5.1 requires an initial H¯ 0 to compute first eigenvector v0, so that scalars {si}Ni=+12 can be defined for N + 2 linear constraints in (21). To initialize H¯ 0, we set y0 = [1M 0N-M M ] and z0 = [-x^1 . . . - x^M ]. Parameter is set to t = 1N+1yt-1 + 1M zt-1 at iteration t. H¯ 0 can then be computed using definition of H¯ in (20).
As similarly done in [5], we extract prediction labels x = [x1 . . . xN ] from converged LP solution y and z as follows. We first construct H using y and z using definition of H in (12). We then compute x = sign(x^1v1v), where v1 is the first entry of the first eigenvector v of H. See [5] for details of recovering SDP primal variables from dual variables in BQP.

7

6 Experiments
6.1 Experimental Setup
We implemented our GDPA-graph-based classifier learning scheme in Matlab4, and evaluated it in terms of average classification error rate and running time. We compared our algorithm against the following schemes that solve the SDP primal problem (8) directly: i) two primal-dual interior-point solvers for SDP, SeDuMi and MOSEK, both of which are available in CVX with a CVX Professional license [33], ii) an ADMM first-order operator-splitting solver CDCS [26, 27] with an LGPL-3.0 License [34], iii) a spectrahedron-based relaxation solver SDCut [22, 23, 35] that involves L-BFGS-B [36], and iv) a biconvex relaxation solver BCR [24, 37], all of which are implemented in Matlab. In addition, we employed CDCS again to solve our modified SDP dual problem (20).
We set the convergence threshold of the first eigenvector solver LOBPCG to be 10-4, with maximum number of iterations 200. We set the convergence threshold of our LP solver to be 10-4 also, with maximum number of iterations 100, since first-order methods, i.e., CDCS and SDCut, aim at computing a solution of moderate accuracy [26]. Accordingly, we set the convergence threshold of SeDuMi and MOSEK to be `low', which is approximately equal to 10-4 and the lowest precision setting in CVX. We set the convergence thresholds of CDCS and SDCut to be 10-3, the maximum number of ADMM iterations in CDCS to be 1000, the maximum number of iterations for L-BFGS-B in SDCut and the main loop in BCR to be 100, and the Frobenius norm weight in SDCut to be 100. We chose these settings since smaller convergence thresholds and larger number of iterations would cause CDCS, SDCut and BCR to be significantly slower to converge. We used default settings for all remaining solvers. All computations were carried out on a Windows 10 64bit PC with AMD RyzenThreadripper 3960X 24-core processor 3.80 GHz and 128GB of RAM.
We adopted 17 binary datasets that are freely available in UCI [38] and LibSVM [39]. For experimental efficiency, we first performed a K-fold (K  5) split for each dataset with random seed 0, and then created 10 instances of 50% training-50% test split for each fold, with random seeds 1-10 [40]. The above setup resulted in problem sizes from 29 to 400. We applied the following two data normalization schemes for the training/test data: i) a standardization scheme in [41] that first subtracts the mean and divides by the feature-wise standard deviation, and then normalizes to unit length sample-wise, and ii) a min-max scheme [40] that rescales each feature to within 0 and 1. We added 10-12 noise to the dataset to avoid NaN's due to data normalization on small samples.
6.2 Experimental Results
Fig. 3 and the first two plots of Fig. 4 show classification error rates and runtime (in log scale) using min-max and standardization data re-scaling strategies for 17 different datasets, respectively. The x-axis of each plot denotes the datasets in ascending order of problem sizes. Each point in the plots denotes the average of 10K runs. Fig. 4 (right) shows runtime versus problem size (4 to 24428) using the same dataset cod-rna (freely avaiable in LibSVM [39]). We did not execute SeDuMi, MOSEK, CDCS (8), BCR, SDcut, or CDCS (20) when the problem size was larger than 976.
In terms of classification error rate for min-max re-scaling, MOSEK, CDCS (8) and SeDuMi had slightly larger error rates: 32.52%, 32.38% and 29.92%, respectively. GDPA had 29.11%, which was very close to CDCS (20) at 29.24% and SDCut at 28.76%. This shows that our proposed GDPA linearization (21) closely approximated the modified SDP dual (20) in performance. BCR at 26.82% was roughly 2% smaller. In the standardization re-scaling case, CDCS (8), MOSEK, and SeDuMi had the largest error rates: 32.75%, 32.5% and 31.0%, respectively. GDPA had 26.88%, close to CDCS (20) at 26.9% and SDCut with 26.82%. BCR at 24.8% was again roughly 2% smaller. By factorizing a PSD matrix M = XX , BCR avoided any SDP relaxation, which may explain its slightly better performance here. However, BCR solved a non-convex optimization problem converging to a local minimum, and thus occasionally the performance was quite poor (e.g., see sonar in Fig. 3(left)). Overall, all solvers performed similarly given constructed similarity graphs in the two cases.
In terms of runtime, BCR was competitive with GDPA when the problem size was small, but GDPA significantly outperformed all competing solvers when the problem size was large. Specifically, the speed gain increased as problem size increased; for madelon with problem size 400, the speedup of
4results reproducible via code in https://anonymous.4open.science/r/GDPA_matlab-EF4D/
8

GDPA over the next fastest scheme BCR was 346×. Fig. 4 (right) also shows that the computation time for GDPA increased gracefully as the problem size increased to very large sizes. The reason for our dramatic speed gain is the fast computation of first eigenvectors using LOBPCG, which benefited from warm start during the LP iterations. In general, GDPA performed fewer than 10 LP's until convergence. In contrast, both CDCS and SDCut required eigen-decomposition of a matrix of size N × N per iteration. Because L described a dense graph in our experiments, the speedup of replacing the full eigen-decomposition with simpler first eigenvector computation per iteration was significant. For BCR, each iteration required either N -dimensional matrix inversion for a least-squares problem or iterative gradient descent, which was computationally expensive as the problem size increased. On average, GDPA enjoyed a 40.9× speedup over the next fastest solver BCR.

error rate (%)

50

30

SeDuMi

MOSEK

60

40

29.5

CDCS (8)

BCR

50

error rate (%)

30

29

SDcut

CDCS (20)

40

28.5

GDPA

30

20

20

28

10 10
27.5

27

SeDuMi

MOSEK

CDCS (8)

26.5

BCR

SDcut

CDCS (20)

26

GDPA

25.5

25

liver-disorders planning sonar heart haberman
colon-cancer voting monk1 WDBC ILPD
breast-cancer australian diabetes pima fourclass german madelon avg.
avg.

liver-disorders planning sonar haberman
colon-cancer voting monk1 WDBC ILPD heart
breast-cancer australian diabetes pima fourclass german madelon avg.
avg.

Figure 3: Error rates (%) for min-max (left) and standardization (right) data re-scaling.

runtime (ms) runtime (ms) runtime (ms)

104

104

105

SeDuMi

MOSEK

CDCS (8)

104

BCR

SDcut

CDCS (20)

102

102

103

GDPA

102

100

100

101

liver-disorders planning sonar haberman
colon-cancer voting monk1 WDBC ILPD heart
breast-cancer australian diabetes pima fourclass german madelon avg.
liver-disorders planning sonar heart haberman
colon-cancer voting monk1 WDBC ILPD
breast-cancer australian diabetes pima fourclass german madelon avg.

102

104

problem size

Figure 4: Runtime (ms) for min-max (left) and standardization (center) data re-scaling on different datasets, and runtime (ms) for variable problem sizes on the same dataset cod-rna (right).

7 Conclusion
We propose a fast projection-free algorithm for the graph-based classifier learning problem. The key idea is to replace the difficult-to-compute positive semi-definite (PSD) cone constraint with linear constraints derived from the recent Gershgorin disc perfect alignment (GDPA) theory, so that the optimization can be solved as a sequence of linear programs (LP). Experiments show that our algorithm enjoyed a considerable speedup while retaining comparable label prediction performance.
A graph classifier scalable to very large sizes encourages ubiquitous deployment for wide-ranging applications. Negative social impact can result if the tool is misused by enabling classification for discriminatory purposes. As an optimization problem, graph-based binary classification is rather narrowly defined (though multi-class classification can be implemented as a tree of binary classifiers). Furthermore, good performance depends heavily on the construction of a good similarity graph, which is outside the scope of this paper. However, we conjecture that the general methodology of GDPA linearization can be similarly tailored to other SDP problems with PSD cone constraints. We anticipate that speedups in other SDP problems will also be significant.
9

References
[1] C. M. Bishop, Pattern Recognition and Machine Learning (Information Science and Statistics), Springer-Verlag, Berlin, Heidelberg, 2006.
[2] D. Zhou, O. Bousquet, T. N. Lal, J. Weston, and B. Scholkopf, "Learning with local and global consistency," in 16th International Conference on Neural Information Processing (NIPS), Whistler, Canada, December 2003.
[3] M. Belkin, I. Matveeva, and P. Niyogi, "Regularization and semisupervised learning on large graphs," in Shawe-Taylor J., Singer Y. (eds) Learning Theory, COLT 2004, Lecture Notes in Computer Science, 2004, vol. 3120, pp. 624­638.
[4] A. Guillory and J. Bilmes, "Label selection on graphs," in Twenty-Third Annual Conference on Neural Information Processing Systems, Vancouver, Canada, December 2009.
[5] Z. Luo, W. Ma, A. M. So, Y. Ye, and S. Zhang, "Semidefinite relaxation of quadratic optimization problems," IEEE Signal Processing Magazine, vol. 27, no. 3, pp. 20­34, 2010.
[6] Z. Li, J. Liu, and X. Tang, "Pairwise constraint propagation by semidefinite programming for semi-supervised classification," in ACM International Conferene on Machine Learning, Helsinki, Finland, July 2008.
[7] B O'Donoghue, E. Chu, N. Parikh nad, and S. Boyd, "Conic optimization via operator splitting and homogeneous self-dual embedding," in Journal of Optimization Theory and Applications, 2016, vol. 169, no.3, pp. 1042­1068.
[8] J. Pang and G. Cheung, "Graph Laplacian regularization for inverse imaging: Analysis in the continuous domain," in IEEE Transactions on Image Processing, April 2017, vol. 26, no.4, pp. 1770­1785.
[9] M. Goemans and D. Williamson, "Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming," J. ACM, vol. 42, no. 6, pp. 1115­1145, Nov. 1995.
[10] R. S. Varga, Gershgorin and his circles, Springer, 2004.
[11] C. Yang, G. Cheung, and H. Wei, "Signed graph metric learning via Gershgorin disc perfect alignment," arXiv, 2021.
[12] D. Cartwright and F. Harary, "Structural balance: a generalization of Heider's theory," in Psychological Review, 1956, vol. 63, no.5, pp. 277­293.
[13] B. Gartner and J. Matousek, Approximation Algorithms and Semidefinite Programming, Springer, 2012.
[14] R. Vanderbei, Linear Programming: Foundations and Extensions (5th Edition), Springer Nature, 2021.
[15] A. Ortega, P. Frossard, J. Kovacevic, J. M. F. Moura, and P. Vandergheynst, "Graph signal processing: Overview, challenges, and applications," in Proceedings of the IEEE, May 2018, vol. 106, no.5, pp. 808­828.
[16] G. Cheung, E. Magli, Y. Tanaka, and M. Ng, "Graph spectral image processing," in Proceedings of the IEEE, May 2018, vol. 106, no.5, pp. 907­930.
[17] M. Gavish, B. Nadler, and R. Coifman, "Multiscale wavelets on trees, graphs and high dimensional data: Theory and applications to semi-supervised learning," in 27th International Conference on Machine Learning, Haifa, Israel, June 2010.
[18] D. Shuman, M. Faraji, and P. Vandergheynst, "Semi-supervised learning with spectral graph wavelets," in International Conference on Sampling Theory and Applications (SampTA), Singapore, May 2011.
[19] G. Cheung, W.-T. Su, Y. Mao, and C.-W. Lin, "Robust semisupervised graph classifier learning with negative edge weights," in IEEE Transactions on Signal and Information Processing over Networks, December 2018, vol. 4, no.4, pp. 712­726.
[20] X. Dong, D. Thanou, M. Rabbat, and P. Frossard, "Learning graphs from data: A signal representation perspective," IEEE Signal Processing Magazine, vol. 36, no. 3, pp. 44­63, 2019.
[21] C. Helmberg, F. Rendl, R. Vanderbei, and H. Wolkowicz, "An interior-point method for semidefinite programming," in SAIM J. Optim., 1996, vol. 6, no.2, pp. 342­361.
10

[22] P. Wang, C. Shen, and A. van den Hengel, "A fast semidefinite approach to solving binary quadratic problems," in IEEE International Conference on Computer Vision and Pattern Recognition, Portland, OR, June 2013.
[23] P. Wang, C. Shen, A. Hengel, and P. Torr, "Large-scale binary quadratic optimization using semidefinite relaxation and applications," IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 39, no. 3, pp. 470­485, 2017.
[24] S. Shah et al., "Biconvex relaxation for semidefinite programming in computer vision," in European Conference on Computer Vision, Amsterdam, the Netherlands, October 2016.
[25] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, "Distributed optimization and statistical learning via the alternating direction method of multipliers," in Foundations and Trends in Optimization, 2011, vol. 3, no.1, pp. 1­122.
[26] Y. Zheng, G. Fantuzzi, and A. Papachristodoulou, "Fast ADMM for sum-of-squares programs using partial orthogonality," IEEE Transactions on Automatic Control, vol. 64, no. 9, pp. 3869­3876, 2019.
[27] Y. Zheng, G. Fantuzzi, A. Papachristodoulou, P. Goulart, and A. Wynn, "Chordal decomposition in operator-splitting methods for sparse semidefinite programs," Mathematical Programming, vol. 180, pp. 489---532, 2020.
[28] F. Chung, Spectral Graph Theory, American Mathematical Society, 1996. [29] T. Dittrich and G. Matz, "Signal processing on signed graphs: Fundamentals and potentials," in
IEEE Signal Processing Magazine, November 2020, vol. 37, no.6, pp. 86­98. [30] J. Kunegis, S. Schmidt, A. Lommatzsch, J. Lerner, E. D. Luca, and S. Albayrak, "Spectral
analysis of signed graphs for clustering, prediction and visualization," in SIAM International Conference on Data Mining, Columbus, OH, May 2010. [31] Panagiotis Moutafis, Mengjun Leng, and Ioannis A. Kakadiaris, "An overview and empirical comparison of distance metric learning methods," IEEE Transactions on Cybernetics, vol. 47, no. 3, pp. 612­625, 2017. [32] A. V. Knyazev, "Toward the optimal preconditioned eigensolver: Locally optimal block preconditioned conjugate gradient method," SIAM Journal on Scientific Computing, vol. 23, no. 2, pp. 517­541, 2001. [33] "CVX Research," http://cvxr.com/cvx/, Accessed: 2021-5-28. [34] "CDCS implementation," https://github.com/oxfordcontrol/CDCS, Accessed: 2021-528. [35] "SDcut implementation," https://github.com/chhshen/SDCut, Accessed: 2021-5-28. [36] C. Zhu, R. Byrd, P. Lu, and J. Nocedal, "Algorithm 778: L-BFGS-B: Fortran subroutines for large-scale bound-constrained optimization," ACM Trans. Math. Softw., vol. 23, no. 4, pp. 550­560, Dec. 1997. [37] "BCR implementation," https://github.com/shahsohil/biconvex-relaxation, Accessed: 2021-5-28. [38] "UCI machine learning repository," https://archive.ics.uci.edu/ml/datasets.php, Accessed: 2021-5-28. [39] "LibSVM Data: Classification (Binary Class)," https://www.csie.ntu.edu.tw/~cjlin/ libsvmtools/datasets/binary.html, Accessed: 2021-5-28. [40] S. Russell and P. Norvig, Artificial Intelligence: A Modern Approach, Prentice Hall Press, USA, 3rd edition, 2009. [41] M. Dong, Y. Wang, X. Yang, and J. Xue, "Learning local metrics and influential regions for classification," IEEE TPAMI, vol. 42, no. 6, pp. 1522­1529, June 2020.
11

