PIGLeT:
Language Grounding Through Neuro-Symbolic Interaction in a 3D World
Rowan Zellers Ari Holtzman Matthew Peters Roozbeh Mottaghi Aniruddha Kembhavi Ali Farhadi Yejin Choi Paul G. Allen School of Computer Science & Engineering, University of Washington
Allen Institute for Artificial Intelligence
https://rowanzellers.com/piglet

arXiv:2106.00188v1 [cs.CL] 1 Jun 2021

Abstract
We propose PIGLeT: a model that learns physical commonsense knowledge through interaction, and then uses this knowledge to ground language. We factorize PIGLeT into a physical dynamics model, and a separate language model. Our dynamics model learns not just what objects are but also what they do: glass cups break when thrown, plastic ones don't. We then use it as the interface to our language model, giving us a unified model of linguistic form and grounded meaning. PIGLeT can read a sentence, simulate neurally what might happen next, and then communicate that result through a literal symbolic representation, or natural language.
Experimental results show that our model effectively learns world dynamics, along with how to communicate them. It is able to correctly forecast "what happens next" given an English sentence over 80% of the time, outperforming a 100x larger, text-to-text approach by over 10%. Likewise, its natural language summaries of physical interactions are also judged by humans as more accurate than LM alternatives. We present comprehensive analysis showing room for future work.
1 Introduction
As humans, our use of language is linked to the physical world. To process a sentence like "the robot turns on the stove, with a pan on it" (Figure 1) we might imagine a physical Pan object. This meaning representation in our heads can be seen as a part of our commonsense world knowledge, about what a Pan is and does. We might reasonably predict that the Pan will become Hot ­ and if there's an Egg on it, it would become cooked .
As humans, we learn such a commonsense world model through interaction. Young children learn to reason physically about basic objects by manipulating them: observing the properties they have,

t Name: Egg Temperature: RoomTemp isCooked: False isBroken: True
...
<heatUp, Pan>

t+1
Name: Egg Temperature: Hot isCooked: True isBroken: True
...

Physical Dynamics Model

PIGLeT

Language Model

The robot turns on the stove, with a pan on it.

The pan is now hot and the egg becomes cooked.

Figure 1: PIGLeT. Through physical interaction in a 3D world, we learn a model for what actions do to objects. We use our physical model as an interface for a language model, jointly modeling elements of language form and meaning. Given an action expressed symbolically or in English, PIGLeT can simulate what might happen next, expressing it symbolically or in English.

and how they change if an action is applied on them (Smith and Gasser, 2005). This process is hypothesized to be crucial to how children learn language: the names of these elementary objects become their first "real words" upon which other language is scaffolded (Yu and Smith, 2012).
In contrast, the dominant paradigm today is to train large language or vision models on static data, such as language and photos from the web. Yet such a setting is fundamentally limiting, as suggested empirically by psychologists' failed attempts to get kittens to learn passively (Held and Hein, 1963). More recently, though large Transformers have made initial progress on benchmarks, they also have frequently revealed biases in those same datasets, suggesting they might not be solving underlying tasks (Zellers et al., 2019b). This has been argued philosophically by a flurry of re-

1

Throw object X at Y:

Name: Vase

Name: Laptop

Size: medium

Size: medium

isBroken: False isBroken: False

isPickedUp: True isPickedUp: False
...

isTurnedOn: False isTurnedOn: True

The robot is holding a vase, and there is a laptop on the coffee table that is on.

<throwHeldObjectAt, laptop>
The robot throws the vase onto the coffee
table.

Name: Vase

Name: Laptop

Size: medium

Size: medium

isBroken: True isBroken: False

isPickedUp: False isPickedUp: False
...
isTurnedOn: False isTurnedOn: False

The laptop and the vase both break, with the vase shattering into smaller pieces, and the laptop powers off.

Figure 2: PIGPeN, a setting for few-shot language-world grounding. We collect data for 280k physical interactions in THOR, a 3D simulator with 20 actions and 125 object types, each with 42 attributes (e.g. isBroken). We annotate 2k interactions with English sentences describing the initial world state, the action, and the action result.

cent work arguing that no amount of language form could ever specify language meaning (McClelland et al., 2019; Bender and Koller, 2020; Bisk et al., 2020); connecting back to the Symbol Grounding Problem of Harnad (1990).
In this paper, we investigate an alternate strategy for learning physical commonsense through interaction, and then transferring that into language. We introduce a model named PIGLeT, short for Physical Interaction as Grounding for Language Transformers. We factorize an embodied agent into an explicit model of world dynamics, and a model of language form. We learn the dynamics model through interaction. Given an action heatUp applied to the Pan in Figure 1, the model learns that the Egg on the pan becomes Hot and Cooked , and that other attributes do not change.
We integrate our dynamics model with a pretrained language model, giving us a joint model of linguistic form and meaning. The combined PIGLeT can then reason about the physical dynamics implied by English sentences describing actions, predicting literally what might happen next. It can then communicate that result either symbolically or through natural language, generating a sentence like `The egg becomes hot and cooked." Our separation between physical dynamics and language allows the model to learn about physical commonsense from the physical world itself, while also avoiding recurring problems of artifacts and biases that arise when we try to model physical world understanding solely through language.
We study this through a new environment and evaluation setup called PIGPeN, short for Physical Interaction Grounding Paired with Natural Language. In PIGPeN, a model is given unlimited access to an environment for pretraining, but only 500 examples with paired English annotations. Models in our setup must additionally generalize to novel `unseen' objects for which we intentionally do not provide paired language-environment supervision. We build this on top of the THOR environment

(Kolve et al., 2017), a physics engine that enables agents to perform contextual interactions (Fig 2) on everyday objects.
Experiments confirm that PIGLeT performs well at grounding language with meaning. Given a sentence describing an action, our model predicts the resulting object states correctly over 80% of the time, outperforming even a 100x larger model (T511B) by over 10%. Likewise, its generated natural language is rated by humans as being more correct than equivalently-sized language models. Last, it can generalize in a `zero-shot' way to objects that it has never read about before in language.
In summary, we make three key contributions. First, we introduce PIGLeT, a model decoupling physical and linguistic reasoning. Second, we introduce PIGPeN, to learn and evaluate the transfer of physical knowledge to the world of language. Third, we perform experiments and analysis suggesting promising avenues for future work.

2 PIGPeN: A Resource for Neuro-Symbolic Language Grounding

We introduce PIGPeN as a setting for learning and evaluating physically grounded language understanding. An overview is shown in Figure 2. The idea is that an agent gets access to an interactive 3D environment, where it can learn about the world through interaction ­ for example, that objects such as a Vase can become Broken if thrown. The goal for a model is to learn natural language meaning grounded in these interactions.
Task definition. Through interaction, an agent observes the interplay between objects o  O (represented by their attributes) and actions a  A through the following transition:

{o1, . . . , oN } ×a  {o1, . . . , oN } . (1)

o, state pre-action

o , state post-action

Actions change the state of a subset of objects: turning on a Faucet affects a nearby Sink , but it will not change a Mirror on the wall.

2

To encourage learning from interaction, and not just language, an agent is given a small number of natural language annotations of transitions. We denote these sentences as so, describing the state pre-action, sa the action, and so the state postaction respectively. During evaluation, an agent will sometimes encounter new objects o that were not part of the paired training data.
We evaluate the model's transfer in two ways: a. PIGPeN-NLU. A model is given object states
o, and an English sentence sa describing an action. It must predict the grounded object states o that result after the action is taken. b. PIGPeN-NLG. A model is given object states o and a literal action a. It must generate a sentence so describing the state post-action. We next describe our environment, feature representation, and language annotation process.
2.1 Environment: THOR
We use AI2-THOR as an environment for this task (Kolve et al., 2017). In THOR, a robotic agent can navigate around and perform rich contextual interactions with objects in a house. For instance, it can grab an Apple , slice it, put it in a Fridge , drop it, and so on. The state of the Apple , such as whether it is sliced or cold, changes accordingly; this is not possible in many other environments.
In this work, we use the underlying THOR simulator as a proxy for grounded meaning. Within THOR, it can be seen as a `complete' meaning representation (Artzi et al., 2013), as it fully specifies the kind of grounding a model can expect in its perception within THOR.
Objects. The underlying THOR representation of each object o is in terms of 42 attributes; we provide a list in Appendix B. We treat these attributes as words specific to an attribute-level dictionary; for example, the temperature Hot is one of three possible values for an object's temperature; the others being Cold and RoomTemp .
Actions. An action a in THOR is a function that takes up to two objects as arguments. Actions are highly contextual, affecting not only the arguments but potentially other objects in the scene (Figure 2). We also treat action names as words in a dictionary.
Filtering out background objects. Most actions change the state of only a few objects, yet there can be many objects in a scene. We keep annotation and computation tractable by having models predict (and humans annotate) possible changes

of at most two key objects in the scene. As knowing when an object doesn't change is also important, we include non-changing objects if fewer than two change.
Exploration. Any way of exploring the environment is valid for our task, however, we found that exploring intentionally was needed to yield good coverage of interesting states. Similar to prior work for instruction following (Shridhar et al., 2020), we designed an oracle to collect diverse and interesting trajectories {o, a, o }. Our oracle randomly selects one of ten high level tasks, see Appendix B for the list. These in turn require randomly choosing objects in the scene; e.g. a Vase and a Laptop in Figure 2. We randomize the manner in which the oracle performs the task to discover diverse situations.
In total, we sampled 20k trajectories. From these we extracted 280k transitions (Eqn 1's) where at least one object changes state, for training.
2.2 Annotating Interactions with Language
2.2.1 Data Selection for Annotation
We select 2k action state-changes from trajectories held out from the training set. We select them while also balancing the distribution of action types to ensure broad coverage in the final dataset. We are also interested in a model's ability to generalize to new object categories ­ beyond what it has read about, or observed in a training set. We thus select 30 objects to be "unseen," and exclude these from paired environment-language training data. We sample 500 state transitions, containing only "seen" objects to be the training set; we use 500 for validation and 1000 for testing.
2.2.2 Natural Language Annotation
Workers on Mechanical Turk were shown an environment in THOR before and after a given action a. Each view contains the THOR attributes of the two key objects. Workers then wrote three English sentences, corresponding to so, sa, and so respectively. Workers were instructed to write at a particular level of detail: enough so that a reader could infer "what happens next" from so and sa, yet without mentioning redundant attributes.We provide more details in Appendix C.
3 Modeling PIGLeT
In this section, we describe our PIGLeT model. First, we learn a neural physical dynamics model

3

o~ <latexit sha1_base64="Tk3HXnFVIrKVryije/QEdnLVRCo=">AAADMHicfVJLb9NAEN66BVrzSoEbF4sICXGI7IIExwo4cEEUibSV4igabybOKvuwZtelqeX/0isc+TVwQr3yK9gkPuCEMtJqvv3mubOTFVJYF8c/t4LtnRs3b+3uhbfv3L13v7P/4Niakjj2uZGGTjOwKIXGvhNO4mlBCCqTeJLN3i7sJ2dIVhj92c0LHCrItZgIDs5To86j9Ax5lWZGju1ceVWZuh51unEvXkq0CZIGdFkjR6P9YCcdG14q1I5LsHaQxIUbVkBOcIl1mJYWC+AzyHHgoQaFdlgt26+jp54ZRxND/mgXLdm/IypQdtGc91TgpnbdtiD/ZRuUbvJ6WAldlA41XxWalDJyJlrMIhoLQu7k3APgJHyvEZ8CAXd+Yq0qqpROkPnSeknFQfI2kxMUU8HP2yyhtOKiPYZrUpJx/md03mYz1b6XJNeSGcLNEpkxMweZvbbwO/S/RfjBT+5jgQTO0PMqBcoVnNdVo//nJvTKzeswDP3eJOtbsgmOD3rJi1786WX38E2zQbvsMXvCnrGEvWKH7D07Yn3G2QW7ZF/Zt+B78CP4FVytXIOtJuYha0nw+w/LmQ/3</latexit>
Name: Vase Size: medium isBroken: False isPickedUp: True
isTurnedOn: False

Object Encoder
Tenc <latexit sha1_base64="FAlDcCJyxBBe5Frg+r9it6VUkmA=">AAADNXicfVJLb9NAEN6aAsW8UjghLhYREuIQOQUJjhVw4IIoUtNWiqNovRk7q+zDmh1DgmXxa7jCkd/CgRviyl9gnfqAE8pIq/n2m5mdnUdaKOkojr/vBJd2L1+5unctvH7j5q3bvf07J86WKGAkrLJ4lnIHShoYkSQFZwUC16mC03TxsrGfvgd00ppjWhUw0Tw3MpOCk6emvXuJ5jRPs+q4nlYJwZJQV2BEXU97/XgQryXaBsMW9FkrR9P9YDeZWVFqMCQUd248jAuaVBxJCgV1mJQOCi4WPIexh4ZrcJNqXUMdPfTMLMos+mMoWrN/R1RcO7fSqfdsfuw2bQ35L9u4pOz5pJKmKKmpa50oK1VENmoaEs0kgiC18oALlP6vkZhz5IJ82zpZdKlIov3QqaQSXIkukyMv5lIsuyyCcvJjtw0XPImW/HhM3mVT3b2XqDYeswjbKVJrF8RTd2HiV+CnhfDGd+5tAcjJ4uMq4ZhrvqyrVv/PTZpzN6/DMPR7M9zckm1wcjAYPhnE7572D1+0G7TH7rMH7BEbsmfskL1mR2zEBPvEPrMv7GvwLfgR/Ax+nbsGO23MXdaR4PcfWwoSPA==</latexit>

ho1 <latexit sha1_base64="CqR6VsVzGuZlPuXKLqpMZY9btz4=">AAADOXicfVJLj9MwEPaGBZbw6sIRIUVUSIhDlQASHFfAgcuKRaK7KzVVNHEnqVU/ItuBLVZO/BqucOSX7JEb4sofwGlzIC3LSNZ8/mbseeYVZ8bG8flOcGn38pWre9fC6zdu3ro92L9zbFStKY6p4kqf5mCQM4ljyyzH00ojiJzjSb541dpPPqA2TMn3dlnhVEApWcEoWE9lg/upADvPCzdvMpfmis/MUnjlVJMlTTYYxqN4JdE2SDowJJ0cZfvBbjpTtBYoLeVgzCSJKzt1oC2jHJswrQ1WQBdQ4sRDCQLN1K3qaKKHnplFhdL+SBut2L9fOBCmTc97tlmbTVtL/ss2qW3xYuqYrGqLkq4DFTWPrIrapkQzppFavvQAqGY+14jOQQO1vnW9KKLmlmn1sVeJo8Bpnyk1VHNGz/qsRm7Yp34bLvhSK+tHJMs+m4v+vdZ84zOlcTtErtTCQm4uDPwa/bQ0HvrOva1Qg1X6sUtBlwLOGtfp/7kxuXbzOgxDvzfJ5pZsg+Mno+TpKH73bHjwstugPXKPPCCPSEKekwPyhhyRMaHkM/lCvpJvwffgR/Az+LV2DXa6N3dJT4LffwAIhRPU</latexit> ...
ho2 <latexit sha1_base64="QDwIL9dC1n1EUQ5JgckmT4EFRp4=">AAADOXicfVJLj9MwEPaGBZbw6sIRIUVUSIhDlewiwXEFHLggFonurtRU0cSdpFb9iGwHtlg58Wu4wpFfwpEb4sofwGlzIC3LSNZ8/mbseeYVZ8bG8fed4NLu5StX966F12/cvHV7sH/nxKhaUxxTxZU+y8EgZxLHllmOZ5VGEDnH03zxorWfvkdtmJLv7LLCqYBSsoJRsJ7KBvdTAXaeF27eZC7NFZ+ZpfDKqSY7aLLBMB7FK4m2QdKBIenkONsPdtOZorVAaSkHYyZJXNmpA20Z5diEaW2wArqAEiceShBopm5VRxM99MwsKpT2R9poxf79woEwbXres83abNpa8l+2SW2LZ1PHZFVblHQdqKh5ZFXUNiWaMY3U8qUHQDXzuUZ0Dhqo9a3rRRE1t0yrD71KHAVO+0ypoZozet5nNXLDPvbbcMGXWlk/Iln22Vz077XmG58pjdshcqUWFnJzYeCX6Kel8bXv3JsKNVilH7sUdCngvHGd/p8bk2s3r8Mw9HuTbG7JNjg5GCWHo/jtk+HR826D9sg98oA8Igl5So7IK3JMxoSST+Qz+UK+Bt+CH8HP4NfaNdjp3twlPQl+/wELPRPV</latexit>

Action Application
MLPapply <latexit sha1_base64="zPFXFrp2Awq1em5tGLQOTqO0h4o=">AAADOXicfVJNj9MwEPWGhV3CVxeOCCmiQkIcqmRBYo8r4MCBFUWiuyu1VTVxp6lVO7bsCbREOfFruMKRX8KRG+LKH8Btg0RalpEsP795M2OPJzVSOIrjbzvBpd3LV/b2r4bXrt+4eat1cPvU6cJy7HEttT1PwaEUOfZIkMRzYxFUKvEsnT1f+s/eoXVC529pYXCoIMvFRHAgT41a9waEc7KqPHnVrUblnxMYIxdVNWq14068smgbJDVos9q6o4NgdzDWvFCYE5fgXD+JDQ1LsCS4xCocFA4N8Blk2PcwB4VuWK7eUUUPPDOOJtr6lVO0Yv+OKEE5t1CpVyqgqdv0Lcl/+foFTY6GpchNQZjzdaFJISPS0bIp0VhY5CQXHgC3wt814lOwwMm3rlFFFZKE1e8bLyk5SN5kMgtmKvi8yVqUTnxotuGClFaT/6I8a7Kpap4LKzeSaYvbJVKtZwSpu7DwC/S/ZfHEd+61QQuk7aNyADZTMK/Kev+fTORrmd/DMPRzk2xOyTY4Pewkjzvxmyft42f1BO2zu+w+e8gS9pQds5esy3qMs4/sE/vMvgRfg+/Bj+DnWhrs1DF3WMOCX78ByRUUGw==</latexit>

h^ o01 <latexit sha1_base64="0abHHpkgF29PUVVxn/YJE6aKEYY=">AAADQXicfVLNjtMwEPaGBZbw14Ujl4hqBeJQJQsSHFfAgQtikejuSk0VTdxJatU/ke3AFitPwNNwhSNPwSNwQ1y54LQ5kJZlJGs+fzP2/OYVZ8bG8fed4NLu5StX966F12/cvHV7sH/nxKhaUxxTxZU+y8EgZxLHllmOZ5VGEDnH03zxorWfvkdtmJLv7LLCqYBSsoJRsJ7KBgdROgfrUgF2nhdu3jSZS3PFZ2YpvHIqS5oHTTYYxqN4JdE2SDowJJ0cZ/vBbjpTtBYoLeVgzCSJKzt1oC2jHJswrQ1WQBdQ4sRDCQLN1K3qaaIDz8yiQml/pI1W7N8vHAjT5uc927zNpq0l/2Wb1LZ4NnVMVrVFSdeBippHVkVtc6IZ00gtX3oAVDOfa0TnoIFa38JeFFFzy7T60KvEUeC0z5Qaqjmj531WIzfsY78NF3yplfWjkmWfzUX/Xmu+8ZnSuB0iV2phITcXBn6JfloaX/vOvalQg1X6kUtBlwLOG9fp/7kxuXbzOgxDvzfJ5pZsg5PDUfJ4FL99Mjx63m3QHrlH7pOHJCFPyRF5RY7JmFDyiXwmX8jX4FvwI/gZ/Fq7Bjvdm7ukJ8HvP1cnFvw=</latexit> ... h^ o02 <latexit sha1_base64="lCctrn5jUI+pPItihMG7PAcLL3k=">AAADQXicfVLNjtMwEPaGBZbw14Ujl4hqBeJQpQsSHFfAgQtikejuSk0VTdxJYtU/ke3AFitPwNNwhSNPwSNwQ1y54LQ5kJZlJGs+fzP2/GYVZ8bG8fed4NLu5StX966F12/cvHV7sH/nxKhaU5xQxZU+y8AgZxInllmOZ5VGEBnH02zxorWfvkdtmJLv7LLCmYBCspxRsJ5KBwdRUoJ1iQBbZrkrmyZ1Sab43CyFV06lh82DJh0M41G8kmgbjDswJJ0cp/vBbjJXtBYoLeVgzHQcV3bmQFtGOTZhUhusgC6gwKmHEgSamVvV00QHnplHudL+SBut2L9fOBCmzc97tnmbTVtL/ss2rW3+bOaYrGqLkq4D5TWPrIra5kRzppFavvQAqGY+14iWoIFa38JeFFFzy7T60KvEUeC0zxQaqpLR8z6rkRv2sd+GC77UyvpRyaLPZqJ/rzXf+Exp3A6RKbWwkJkLA79EPy2Nr33n3lSowSr9yCWgCwHnjev0/9yYXLt5HYah35vx5pZsg5PD0fjxKH77ZHj0vNugPXKP3CcPyZg8JUfkFTkmE0LJJ/KZfCFfg2/Bj+Bn8GvtGux0b+6SngS//wBZ4Rb9</latexit>

Object Decoder
Tdec <latexit sha1_base64="2gP2+xYkdi+qahQkMatfm5CHGxA=">AAADNXicfVLNjtMwEPaGhV3CXxdOiEtEhYQ4VOmCxB5XwIELYpG2uys1VTVxJ6lVO47sCbRYEU/DFY48CwduiCuvgNvmQFqWkSx//uZ/PGkphaU4/r4TXNm9em1v/3p44+at23c6B3fPrK4MxwHXUpuLFCxKUeCABEm8KA2CSiWep7OXS/35ezRW6OKUFiWOFOSFyAQH8tS4cz9RQNM0c6f12CWEczLKTZDX9bjTjXvxSqJt0G9AlzVyMj4IdpOJ5pXCgrgEa4f9uKSRA0OCS6zDpLJYAp9BjkMPC1BoR27VQx098swkyrTxp6Boxf7t4UBZu1Cpt1xWbDd1S/JfumFF2dHIiaKsCAu+TpRVMiIdLQcSTYRBTnLhAXAjfK0Rn4IBTn5srSyqkiSM/tDqxHGQvM3kBsqp4PM2a1Ba8bE9hktCGk3+e4q8zaaq/a6M3AimDW6nSLWeEaT20sSv0P+WwTd+cm9LNEDaPHEJmFzBvHbN/T8zUazN/B2God+b/uaWbIOzw17/aS9+96x7/KLZoH32gD1kj1mfPWfH7DU7YQPG2Sf2mX1hX4NvwY/gZ/BrbRrsND73WEuC338AP8USMg==</latexit>

o~0 <latexit sha1_base64="dSJfxBPnRwowtpN8zynXN7N+BKY=">AAADMXicfVJLb9NAEN6aAsW8UhAnLhYRAnGI7IIExwo4cEEUibSV4igabybJKvuwZtelYeUfwxWO/JreEFf+BJvEB5xQRlrNt988d3aKUgrr0vRiJ7qye/Xa9b0b8c1bt+/c7ezfO7amIo59bqSh0wIsSqGx74STeFoSgioknhTzN0v7yRmSFUZ/cosShwqmWkwEBxeoUedBfobc54WRY7tQQXlT109GnW7aS1eSbIOsAV3WyNFoP9rNx4ZXCrXjEqwdZGnphh7ICS6xjvPKYgl8DlMcBKhBoR36Vf918jgw42RiKBztkhX7d4QHZZfdBU8FbmY3bUvyX7ZB5Savhl7osnKo+brQpJKJM8lyGMlYEHInFwEAJxF6TfgMCLgLI2tVUZV0gszn1ks8B8nbzJSgnAl+3mYJpRVf2mO4JCUZF75GT9tsodr3iuRGMkO4XaIwZu6gsJcWfovhtwjfh8l9KJHAGXrmc6CpgvPaN/p/bkKv3YKO4zjsTba5Jdvg+KCXPe+lH190D183G7THHrJH7CnL2Et2yN6xI9ZnnHn2lX1j36Mf0UX0M/q1do12mpj7rCXR7z9kcRAo</latexit>
Name: Vase Size: medium isBroken: True isPickedUp: False
isTurnedOn: False

a <latexit sha1_base64="elI5FeejPpeM9uUnRf3IQjSevxU=">AAADKHicfVJLb9NAEN6aQot5tXDkYhEhIQ6RTZHgWAEHLogikbZSEqrxZuKssg9rdlwarPwPrnDk13BDvfJL2CQ+4IQy0mq+/ea5s5OXWnlO08ut6Nr29Rs7uzfjW7fv3L23t3//2LuKJPak045Oc/ColcUeK9Z4WhKCyTWe5NPXC/vJOZJXzn7kWYlDA4VVYyWBA/VpkDs98jMTVA3zs71O2k2XkmyCrAEd0cjR2X60PRg5WRm0LDV438/Skoc1ECupcR4PKo8lyCkU2A/QgkE/rJdtz5PHgRklY0fhWE6W7N8RNRi/6C14GuCJX7ctyH/Z+hWPXw5rZcuK0cpVoXGlE3bJYgbJSBFK1rMAQJIKvSZyAgSSw6RaVUylWZH73HpJLUHLNlMQlBMlL9osofbqS3sMV6Qkx+FHbNFmc9O+V6TXkjnCzRK5c1OG3F9Z+A2G3yJ8Fyb3vkQCdvS0HgAVBi7mdaP/56bsyi3oOI7D3mTrW7IJjp91s4Nu+uF55/BVs0G74qF4JJ6ITLwQh+KtOBI9IQWJr+Kb+B79iH5Gv6LLlWu01cQ8EC2Jfv8B/q8M6g==</latexit>

Action: ThrowHeldObjectAt
Target: Floor

Action Encoder

The robot is holding a glass vase.

sa <latexit sha1_base64="LeR1JAfP96tE2MTQrXBVv0E7xEo=">AAADO3icfVI7bxNBEN4cAcLxcqCk4ISFhCisu4AEZQQUNIgg4SSSbVlz6/F55X2cducgZnUlv4YWSn4INR2ipWdtX5GzCSOt5ttvZnYeO3kphaM0/bETXdq9fOXq3rX4+o2bt2539u8cO1NZjn1upLGnOTiUQmOfBEk8LS2CyiWe5POXS/vJB7ROGP2eFiWOFBRaTAUHCtS4c3+YGzlxCxWUd/XYn79DXY873bSXriTZBlkDuqyRo/F+tDucGF4p1MQlODfI0pJGHiwJLrGOh5XDEvgcChwEqEGhG/lVJ3XyMDCTZGpsOJqSFXs+woNyy+KCpwKauU3bkvyXbVDR9PnIC11WhJqvE00rmZBJlmNJJsIiJ7kIALgVodaEz8ACpzC8VhZVSRLWfGx14jlI3mYKC+VM8LM2a1E68ak9hguetIbCJ+mizeaqfa+s3HjMWNxOkRszJ8jdhYlfYfgti2/C5N6WaIGMfeyHYAsFZ7Vv9P/chF67BR3HcdibbHNLtsHxQS970kvfPe0evmg2aI/dYw/YI5axZ+yQvWZHrM84+8y+sK/sW/Q9+hn9in6vXaOdJuYua0n05y+veRUa</latexit>

The robot throws the vase.

Language Model
TLM <latexit sha1_base64="bWXv2O8NNh4H6P1ViAQ01k2Mz6E=">AAADK3icfVJLb9NAEN6aAsU8mpYjF4sICXGIbECCYwU9cKCilZq2UhJF483YWWUf1u4YGiz/Eq5w5NdwAnHlf3ST+IATykir+fab585OWkjhKI5/bgU3tm/eur1zJ7x77/6D3c7e/pkzpeXY50Yae5GCQyk09kmQxIvCIqhU4nk6e7uwn39E64TRpzQvcKQg1yITHMhT487uUAFN06w6rcfV+6N63OnGvXgp0SZIGtBljRyP94Lt4cTwUqEmLsG5QRIXNKrAkuAS63BYOiyAzyDHgYcaFLpRtey8jp54ZhJlxvqjKVqyf0dUoJybq9R7Lvp067YF+S/boKTs9agSuigJNV8VykoZkYkWY4gmwiInOfcAuBW+14hPwQInP6xWFVVKEtZ8ar2k4iB5m8ktFFPBL9usRenE5/YYrklpDflP0XmbTVX7Xlq5lsxY3CyRGjMjSN21hQ/R/5bFIz+5DwVaIGOfVUOwuYLLumr0/9yEXrl5HYah35tkfUs2wdnzXvKiF5+87B68aTZohz1ij9lTlrBX7IC9Y8eszzgr2Rf2lX0Lvgc/gl/B75VrsNXEPGQtCf5cARg6DUM=</latexit>

ha <latexit sha1_base64="IpTzzwZ9IjK0OgGcISMVC68hBQ0=">AAADN3icfVJLb9NAEN66BYp5peXYi0WEhDhENiDBsaI9cEEUibSV4igab8bOKvuwdtdtw8oHfg1XOPJTOHFDXPkHrBMfcEIZaTXffjO788xKzoyN4+9bwfbOjZu3dm+Hd+7eu/+gt7d/alSlKQ6p4kqfZ2CQM4lDyyzH81IjiIzjWTY/auxnF6gNU/KDXZQ4FlBIljMK1lOT3kEqwM6y3M3qiUszxadmIbxyUNeTXj8exEuJNkHSgj5p5WSyF+ykU0UrgdJSDsaMkri0YwfaMsqxDtPKYAl0DgWOPJQg0Izdsoo6euyZaZQr7Y+00ZL9+4UDYZrkvGeTs1m3NeS/bKPK5q/GjsmysijpKlBe8ciqqGlJNGUaqeULD4Bq5nON6Aw0UOsb14kiKm6ZVpedShwFTrtMoaGcMXrVZTVywz5223DNl1pZPyBZdNlMdO+V5mufKY2bITKl5hYyc23gY/TT0vjWd+5diRqs0k9dCroQcFW7Vv/PjcmVm9dhGPq9Sda3ZBOcPhskzwfx+xf9w9ftBu2SA/KIPCEJeUkOyRtyQoaEkk/kM/lCvgbfgh/Bz+DXyjXYat88JB0Jfv8B94ETIg==</latexit>

Action Summarizer
MLP <latexit sha1_base64="J5yL7sR91yRU8/LfFSoplTfAVlE=">AAADMXicfVJLb9NAEN6aAsW8UhAnLhYREuIQ2YAExwp64EBFkEhbKY6i8WbirLIPa3cMCZZ/DFc48mt6Q1z5E2wSH3BCGWk1337z3NnJCikcxfHFXnBl/+q16wc3wpu3bt+52zm8d+pMaTkOuJHGnmfgUAqNAxIk8bywCCqTeJbN36zsZ5/QOmH0R1oWOFKQazEVHMhT486DlHBBVlUn7/r1uEqPURLU40437sVriXZB0oAua6Q/Pgz204nhpUJNXIJzwyQuaFSBJcEl1mFaOiyAzyHHoYcaFLpRte6/jh57ZhJNjfVHU7Rm/46oQDm3VJn3VEAzt21bkf+yDUuavhpVQhcloeabQtNSRmSi1TCiibDISS49AG6F7zXiM7DAyY+sVUWVkoQ1n1svqThI3mZyC8VM8EWbtSid+NIewyUprSH/NTpvs5lq30srt5IZi7slMmPmBJm7tPAx+t+yeOIn975AC2Ts0yoFmytY1FWj/+cm9MbN6zAM/d4k21uyC06f9ZLnvfjDi+7R62aDDthD9og9YQl7yY7YW9ZnA8ZZxb6yb+x78CO4CH4GvzauwV4Tc5+1JPj9B9KOD/M=</latexit>

Language Model
TLM <latexit sha1_base64="bWXv2O8NNh4H6P1ViAQ01k2Mz6E=">AAADK3icfVJLb9NAEN6aAsU8mpYjF4sICXGIbECCYwU9cKCilZq2UhJF483YWWUf1u4YGiz/Eq5w5NdwAnHlf3ST+IATykir+fab585OWkjhKI5/bgU3tm/eur1zJ7x77/6D3c7e/pkzpeXY50Yae5GCQyk09kmQxIvCIqhU4nk6e7uwn39E64TRpzQvcKQg1yITHMhT487uUAFN06w6rcfV+6N63OnGvXgp0SZIGtBljRyP94Lt4cTwUqEmLsG5QRIXNKrAkuAS63BYOiyAzyDHgYcaFLpRtey8jp54ZhJlxvqjKVqyf0dUoJybq9R7Lvp067YF+S/boKTs9agSuigJNV8VykoZkYkWY4gmwiInOfcAuBW+14hPwQInP6xWFVVKEtZ8ar2k4iB5m8ktFFPBL9usRenE5/YYrklpDflP0XmbTVX7Xlq5lsxY3CyRGjMjSN21hQ/R/5bFIz+5DwVaIGOfVUOwuYLLumr0/9yEXrl5HYah35tkfUs2wdnzXvKiF5+87B68aTZohz1ij9lTlrBX7IC9Y8eszzgr2Rf2lX0Lvgc/gl/B75VrsNXEPGQtCf5cARg6DUM=</latexit>

so~0 <latexit sha1_base64="99UQbAsrvAY02F4vviNX5hJslv0=">AAADQnicfVLNbhMxEHaXAmX5S+HIZUXEjzhEuwWpPVbAgQuiSKStlI2iWWeyseK1V/ZsabD2DXgarnDkJXgFbogrB5xkD92EMpI1n7/59XiyUgpLcfxjK7iyffXa9Z0b4c1bt+/c7ezeO7a6Mhz7XEttTjOwKIXCPgmSeFoahCKTeJLNXi3sJ2dorNDqA81LHBaQKzERHMhTo87jNNNybOeFV87WI5eeIXcXSV3XT+pRpxv34qVEmyBpQJc1cjTaDbbTseZVgYq4BGsHSVzS0IEhwSXWYVpZLIHPIMeBhwoKtEO3fFAdPfLMOJpo44+iaMlejHBQ2EV73rMAmtp124L8l21Q0eRg6IQqK0LFV4UmlYxIR4vpRGNhkJOcewDcCN9rxKdggJOfYatKUUkSRn9svcRxkLzN5AbKqeDnbdagtOJTewyXpDSa/F+pvM1mRfteGbmWTBvcLJFpPSPI7KWFX6P/LYNv/eTelWiAtHnmUjB5Aee1a/T/3IRauXkdhqHfm2R9SzbB8V4ved6L37/oHr5sNmiHPWAP2VOWsH12yN6wI9ZnnH1mX9hX9i34HvwMfgW/V67BVhNzn7Uk+PMXrAYYJw==</latexit>
The vase breaks and is no longer being held.

Figure 3: PIGLeT architecture. We pretrain a model of physical world dynamics by learning to transform objects o and actions a into new updated objects o . Our underlying world dynamics model ­ the encoder, the decoder, and
the action application module, can augment a language model with grounded commonsense knowledge.

from interactions, and second, integrate with a pretrained model of language form.

3.1 Modeling Physical Dynamics
We take a neural, auto-encoder style approach to model world dynamics. An object o gets encoded as a vector ho  Rdo. The model likewise encodes an action a as a vector ha  Rda, using it to manipulate the hidden states of all objects. The model can then decode any object hidden representation back into a symbolic form.

3.1.1 Object Encoder and Decoder

We use a Transformer (Vaswani et al., 2017) to encode objects into vectors o  Rdo, and then another to decode from this representation.
Encoder. Objects o are provided to the encoder as a set of attributes, with categories c1,..., cn. Each attribute c has its own vocabulary and embedding Ec. For each object o, we first embed all the attributes separately and feed the result into a Transformer encoder Tenc. This gives us (with position embeddings omitted for clarity):

ho = Tenc E1(o1), . . . , Ecn (ocn )

(2)

Decoder. We can then convert back into the original symbolic representation through a left-to-right Transformer decoder, which predicts attributes oneby-one from c1 to cn. This captures the inherent correlation between attributes, while making no independence assumptions, we discuss our ordering in Appendix A.2. The probability of predicting the next attribute oci+1 is then given by:

p(oci+1|ho, o:ci)=Tdec ho,E1(o1),..., Eci(oci) (3)

3.1.2 Modeling actions as functions
We treat actions a as functions that transform the state of all objects in the scene. Actions in our environment take at most two arguments, so we embed the action a and the names of its arguments, concatenate them, and pass the result through a multilayer perceptron; yielding a vector representation ha.
Applying Actions. We use the encoded action ha to transform all objects in the scene, obtaining updated representations h^o for each one. We take a global approach, jointly transforming all objects. This takes into account that interactions are contextual: turning on a Faucet might fill up a Cup if and only if there is one beneath it.
Letting the observed objects in the interaction be o1 and o2, with encodings ho1 and ho2 respectively, we model the transformation via the following multilayer perceptron:
[h^o1 , h^o2 ] = MLPapply ha, ho1 , ho2 . (4)
The result can be decoded into symbolic form using the object decoder (Equation 3).
3.1.3 Loss function and training
We train our dynamics model on (o,a,o ) transitions. The model primarily learns by running o,a through the model, predicting the updated output state h^o , and minimizing the cross-entropy of generating attributes of the real changed object o . We also regularize the model by encoding objects o, o and having the model learn to reconstruct them. We weight all these cross-entropy losses equally. We discuss our architecture in Appendix A.1; it uses 3-layer Transformers, totalling 17M parameters.

4

3.2 Language Grounding
After pretraining our physical dynamics model, we integrate it with a Transformer Language Model (LM). In our framework, the role of the LM will be to both encode natural language sentences of actions into a hidden state approximating ha, as well as summarizing the result of an interaction (o,a,o ) in natural language.
Choice of LM. Our framework is compatible with any language model. However, to explore the impact of pretraining data on grounding later in this paper, we pretrain our own with an identical architecture to the smallest GPT2 (Radford et al. (2019); 117M). To handle both classification and generation well, we mask only part of the attention weights out, allowing the model to encode a "prefix" bidirectionally; it generates subsequent tokens leftto-right (Dong et al., 2019). We pretrain the model on Wikipedia and books; details in Appendix D.
We next discuss architectural details of performing the language transfer, along with optimization.

ones. Importantly we only provide the surfaceform names, not underlying information about these objects or their usage as with few-shot scenarios in the recent GPT-3 experiments (Brown et al., 2020) ­ necessitating that PIGLeT learns what these names mean through interaction.
3.2.2 Loss functions and training.
Modeling text generation allows us to incorporate a new loss function, that of minimizing the loglikelihood of generating each so given previous words and the result of Equation 5:
p(spi+os1t |so ,1:i) = TLM(ho1 , ho2 , so ,1:i). (6)
We do the same for the object states so pre-action, using hoi as the corresponding hidden states.
For PIGPeN-NLU, where no generation is needed, optimizing Equation 5 is not strictly necessary. However, as we will show later, it helps provide additional signal to the model, improving overall accuracy by several percentage points.

3.2.1 Transfer Architecture
English actions to vector form. Given a natural language description sa of an action a, like "The robot throws the vase," for PIGPeN-NLU, our model will learn to parse this sentence into a neural representation ha, so the dynamics model can simulate the result. We do this by encoding sa through our language model, TLM , with a learned linear transformation over the resulting (bidirectional) encoding. The resulting vector hsa can then be used by Equation 4.
Summarizing the result of an action. For PIGPeN-NLG, our model simulates the result of an action a neurally, resulting in a predicted hidden state h^o for each object in the scene o. To write an English summary describing "what changed," we first learn a lightweight fused representation of the transition, aggregating the initial and final states, along with the action, through a multilayer perceptron. For each object oi we have:

hoi = MLP([hoi , h^oi , ha]).

(5)

We then use the sequence [ho1, ho2] as bidirectional context for our our LM to decode from.

Additionally, since our test set includes novel ob-

jects not seen in training, we provide the names of

the objects as additional context for the LM genera-

tor (e.g. `Vase, Laptop'); this allows a LM to copy

those names over rather than hallucinate wrong

4 Experiments
We test our model's ability to encode language into a grounded form (PIGPeN-NLU), and decode that grounded form into language (PIGPeN-NLG).
4.1 PIGPeN-NLU Results.
We first evaluate models by their performance on PIGPeN-NLU: given objects o, and a sentence sa describing an action, a model must predict the resulting state of objects o . We primarily evaluate models by accuracy; scoring how many objects for which they got all attributes correct. We compare with the following strong baselines: a. No Change: this baseline copies the initial state
of all objects o as the final state o . b. GPT3-175B (Brown et al., 2020), a very large
language model for `few-shot' learning using a prompt. For GPT3, and other text-to-text models, we encode and decode the symbolic object states in a JSON-style dictionary format, discussed in Appendix A.4.
c. T5 (Raffel et al., 2019). With this model, we use the same `text-to-text' format, however here we train it on the paired data from PIGPeN. We consider varying sizes of T5, from T5-Small ­ the closest in size to PIGLeT, up until T5-11B, roughly 100x the size.
d. (Alberti et al., 2019)-style. This paper originally proposed a model for VCR (Zellers et al.,

5

BERT style text-to-text

Model
No Change
GPT3-175B (Brown et al., 2020) T5-11B (Raffel et al., 2019) T5-3B T5-Large T5-Base T5-Small
Alberti et al.2019, Pretrained Dynamics Alberti et al.2019 G&D2019, Pretrained Dynamics G&D2019
PIGLeT

Accuracy (%)

Val

Test

Overall Seen

27.4 25.5 29.9

23.8 22.4 22.4 68.5 64.2 79.5 66.6 63.3 77.1 56.5 54.1 69.2 56.0 53.9 69.2 39.9 36.2 57.0

61.3 53.9 71.4 9.7 6.8 16.2 43.8 35.3 60.9 15.1 11.3 23.1

81.8 81.1 83.8

Unseen
24.0
21.4 59.1 58.7 49.1 48.8 38.0
48.1 3.7
26.9 7.3
80.2

Attribute-level accuracy (Test-Overall,%)

size distance mass Temperature isBroken

8-way 8-way 8-way 3-way

boolean

83.2 84.1 96.3

86.0

94.8

73.7 77.0 89.5

84.2

94.7

83.9 88.9 94.3

95.4

98.1

81.6 90.0 94.0

95.6

98.4

81.8 84.6 94.3

96.3

95.8

81.1 87.5 93.6

96.1

96.5

82.2 84.9 93.8

89.6

93.5

87.7 87.6 97.5

93.4

97.5

53.4 43.6 84.0

88.1

95.1

83.0 86.9 94.0

93.7

97.4

68.6 47.3 82.2

88.3

95.8

92.3 91.9 99.2

99.8

99.0

Table 1: Overall results. Left: we show the model accuracies at predicting all attributes of an object correctly. We compare PIGLeT with `text-to-text' approaches that represent the object states as a string, along with BERT-style approaches with additional machinery to encode inputs or decode outputs. PIGLeT outperforms a T5 model 100x its size (11B params) and shows gains over the BERT-style models that also model action dynamics through a language transformer. Right: we show several attribute-level accuracies, along with the number of categories per attribute; PIGLeT outperforms baselines by over 4 points for some attributes such as size and distance.

2019a), where grounded visual information is fed into a BERT model as tokens; the transformer performs the grounded reasoning. We adapt it for our task by using our base LM and feeding in object representations from our pretrained object encoder, also as tokens. Our object decoder predicts the object, given the LM's pooled hidden state. This is "pretrained dynamics," we also consider a version without a randomly initialized dynamics model.
e. (Gupta and Durrett, 2019)-style. Thiso paper proposes using Transformers to model physical state, for tasks like entity tracking in recipes. Here, the authors propose decoding a physical state attribute (like isCooked ) by feeding the model a label-specific [CLS] token, and then mapping the result through a hidden layer. We do this and use a similar object encoder as our (Alberti et al., 2019)-style baseline.
We discuss hyperparameters in Appendix A.3. Results. From the results (Table 1), we can draw
several patterns. Our model, PIGLeT performs best at getting all attributes correct; doing so over 80% on both validation and test sets, even for novel objects not seen during training. The next closest model is T5-11B, which scores 68% on validation. Though when evaluated on objects `seen' during training it gets 77%, that number drops by over 18% for unseen objects. On the other hand, PIGLeT has a modest gap of 3%. This suggests that our approach is particularly effective at connecting unpaired language and world representations. At

Model

Accuracy (val;%)

PIGLeT, No Pretraining

10.4

PIGLeT, Non-global MLPapply

72.0

PIGLeT, Global MLPapply

78.5

PIGLeT, Global MLPapply, Gen. loss (6)

81.8

PIGLeT, Symbols Only (Upper Bound)

89.3

Table 2: Ablation study on PIGPeN-NLU's validation set. Our model improves 6% by modeling global dynamics of all objects in the scene, versus applying actions to single objects in isolation. We improve another 3% by adding an auxiliary generation loss.

the other extreme, GPT3 does poorly in its `fewshot' setting, suggesting that size is no replacement for grounded supervision.
PIGLeT also outperforms `BERT style' approaches that control for the same language model architecture, but perform the physical reasoning inside the language transformer rather than as a separate model. Performance drops when the physical decoder must be learned from few paired examples (as in Gupta and Durrett (2019)); it drops even further when neither model is given access to our pretrained dynamics model, with both baselines then underperforming `No Change.' This suggests that our approach of having a physical reasoning model outside of an LM is a good inductive bias.
4.1.1 Ablation study
In Table 2 we present an ablation study of PIGLeT's components. Of note, by using a global representation of objects in the world (Equation 4), we get

6

over 6% improvement over a local representation where objects are manipulated independently. We get another 3% boost by adding a generation loss, suggesting that learning to generate summaries helps the model better connect the world to language. Last, we benchmark how much headroom there is on PIGPeN-NLU by evaluating model performance on a `symbols only' version of the task, where the symbolic action a is given explicitly to our dynamics model. This upper bound is roughly 7% higher than PIGLeT, suggesting space for future work.

Model

BLEU BERTScore Human (test; [ 1, 1]) Val Test Val Test Fluency Faithfulness

T5

46.6 43.4 82.2 81.0 0.82

LM Baseline 44.6 39.7 81.6 78.8 0.91

PIGLeT

49.0 43.9 83.6 81.3 0.92

0.15 -0.13 0.22

Human

44.5 45.6 82.6 83.3 0.94

0.71

Table 3: Text generation results on PIGPeN-NLG, showing models of roughly equivalent size (up to 117M parameters). Our PIGLeT outperforms the LM baseline (using the same architecture but omitting the
physical reasoning component) by 4 BLEU points, 2 BERTScore F1 points, and 0.35 points in a human evaluation of language faithfulness to the actual scene.

4.2 PIGPeN-NLG Results
Next, we turn to PIGPeN-NLG: given objects o, and the literal next action a, a model must generate a sentence so describing what will change in the scene. We compare with the following baselines:
a. T5. We use a T5 model that is given a JSONstyle dictionary representation of both o and a, it is finetuned to generate summaries so .
b. LM Baseline. We feed our LM hidden states ho from our pretrained encoder, along with its representation of a. The key difference between it and PIGLeT is that we do not allow it to simulate neurally what might happen next ­ MLPapply is never used here.
Size matters. Arguably the most important factor controlling the fluency of a language generator is its size (Kaplan et al., 2020). Since our LM could also be scaled up to arbitrary size, we control for size in our experiments and only consider models the size of GPT2-base (117M) or smaller; we thus compare against T5-small as T5-Base has 220M parameters. We discuss optimization and sampling hyperparameters in Appendix A.3.
Evaluation metrics. We evaluate models over the validation and test sets. We consider three main evaluation metrics: BLEU (Papineni et al., 2002) with two references, the recently proposed BERTScore (Zhang et al., 2020), and conduct a human evaluation. Humans rate both the fluency of post-action text, as well as its faithfulness to true action result, on a scale from -1 to 1.
Results. We show our results in Table 3. Of note, PIGLeT is competitive with T5 and significantly outperforms the pure LM baseline, which uses a pretrained encoder for object states, yet has the physical simulation piece MLPapply removed. This suggests that simulating world dynamics not only allows the model to predict what might happen

next, it leads to more faithful generation as well.
5 Analysis
5.1 Qualitative examples.
We show two qualitative examples in Figure 4, covering both PIGPeN-NLU as well as PIGPeN-NLG. In the first row, the robot empties a held Mug that is filled with water. PIGLeT gets the state, and generates a faithful sentence summarizing that the mug becomes empty. T5 struggles somewhat, emptying the water from both the Mug and the (irrelevant) Sink . It also generates text saying that the Sink becomes empty, instead of the Mug.
In the second row, PIGLeT correctly predicts the next object states, but its generated text is incomplete ­ it should also write that the mug becomes filled wtih Coffee. T5 makes the same mistake in generation, and it also underpredicts the state changes, omitting all changes to the Mug .
We suspect that T5 struggles here in part because Mug is an unseen object. T5 only experiences it through language-only pretraining, but this might not be enough for a fully grounded representation.
5.2 Representing novel words
The language models that perform best today are trained on massive datasets of text. However, this has unintended consequences (Bender et al., 2021) and it is unlike how children learn language, with children learning novel words from experience (Carey and Bartlett, 1978). The large scale of our pretraining datasets might allow models to learn to perform physical-commonsense like tasks for wrong reasons, overfitting to surface patterns rather than learning meaningful grounding.
We investigate the extent of this by training a `zero-shot' version of our backbone LM on Wikipedia and books ­ the only difference is that

7

State pre-action

t

Name: Sink

isFilledWithLiquid:True Name: Mug

The robot empties the mug.

isFilledWithLiquid:True

isPickedUp: True

<emptyLiquid, Mug>

Predicted post-action states

PIGLeT

T5

Name: Sink isFilledWithLiquid:True

Name: Sink isFilledWithLiquid:False

Name: Mug isFilledWithLiquid:False isPickedUp: True

Name: Mug isFilledWithLiquid:False isPickedUp: True

The mug is now empty.

The sink is now empty.

Ground truth postaction states
Name: Sink isFilledWithLiquid:True
Name: Mug isFilledWithLiquid:False isPickedUp: True
The mug is no longer filled with water.

t

Name: Mug

Name: Mug

Name: Mug

isFilledWithLiquid:False Temperature: RoomTemp
Name: CoffeeMachine

The robot turns on the coffee maker.

isFilledWithLiquid:True Temperature: Hot
Name: CoffeeMachine

isFilledWithLiquid:False Temperature: RoomTemp
Name: CoffeeMachine

isTurnedOn: False

isTurnedOn: True

isTurnedOn: True

containsObject: Mug

<toggleObject, CoffeeMaker>

containsObject: Mug

containsObject: Mug

The coffee machine is turned on.

The coffee machine becomes on.

Name: Mug isFilledWithLiquid:True Temperature: Hot
Name: CoffeeMachine isTurnedOn: True containsObject: Mug
The coffee maker is now on and the mug is hot and filled with coffee.

Figure 4: Qualitative examples. Our model PIGLeT reliably predicts what might happen next (like the Mug becoming empty in Row 1), in a structured and explicit way. However, it often struggles at generating sentences for unseen objects like Mug that are excluded from the training set. T5 struggles to predict these changes, for example, it seems to suggest that emptying the Mug causes all containers in the scene to become empty.

Test Accuracy (%)

100
80 83.8 81.9 80.2
73.4
60

Microwave 72.2
61.9

Mug 84.9
80.1

Sink 90.2
74.5

Toaster 95.1
75.4

Fridge 91.2 88.2

Egg 80.0
74.1

40

CellPhone

20

30.8 26.9

0 Seen ObjectsUnseen Objects

model PIGLeT PIGLeT ZeroShotLang
Accuracy at predicting selected unseen objects

Figure 5: PIGPeN-NLU performance of a zero-shot

PIGLeT, that was pretrained on Books and Wikipedia

without reading any words of our `unseen' objects like

`mug.' It outperforms a much bigger T5-11B overall,

though is in turn beaten by PIGLeT on unseen objects

like `Sink' and `Microwave.'

we explicitly exclude all mentioned sentences containing one of our "unseen" object categories. In this setting, not only must PIGLeT learn to ground words like `mug,' it must do so without having seen the word `mug' during pretraining. This is significant because we count over 20k instances of `Mug' words (including morphology) in our dataset.
We show results in Figure 5. A version of PIGLeT with the zero-shot LM does surprisingly well ­ achieving 80% accuracy at predicting the state changes for "Mug" ­ despite never having been pretrained on one before. This even outperforms T5 at the overall task. Nevertheless, PIGLeT outperforms it by roughly 7% at unseen objects, with notable gains of over 10% on highly dynamic objects like Toaster s and Sink s.
6 Related Work
Grounded commonsense reasoning. In this work, we study language grounding and common-

sense reasoning at the representation and concept level. The aim is to train models that learn to acquire concepts more like humans, rather than performing well on a downstream task that (for humans) requires commonsense reasoning. Thus, this work is somewhat different versus other 3D embodied tasks like QA (Gordon et al., 2018; Das et al., 2018), along with past work for measuring such grounded commonsense reasoning, like SWAG, HellaSWAG, and VCR (Zellers et al., 2018, 2019b,a). The knowledge covered is different, as it is self-contained within THOR. While VCR, for instance, includes lots of visual situations about what people are doing, this paper focuses on learning the physical properties of objects.
Zero-shot generalization. There has been a lot of past work involved with learning `zero-shot': often learning about the grounded world in language, and transferring that knowledge to vision. Techniques for this include looking at word embeddings (Frome et al., 2013) and dictionary definitions (Zellers and Choi, 2017). In this work, we propose the inverse. This approach was used to learn better word embeddings (Gupta et al., 2019) or semantic tuples (Yatskar et al., 2016), but we consider learning a component to be plugged into a deep Transformer language model.
Past work evaluating these types of zero-shot generalization have also looked into how well models can compose concepts in language together (Lake and Baroni, 2018; Ruis et al., 2020). Our work considers elements of compositionality through grounded transfer. For example, in

8

PIGPeN-NLG, models must generate sentences about the equivalent of dropping a `dax', despite never having seen one before. However, our work is also contextual, in that the outcome of `dropping a dax' might depend on external attributes (like how high we're dropping it from).
Structured Models for Attributes and Objects. The idea of modeling actions as functions that transform objects has been explored in the computer vision space (Wang et al., 2016). Past work has also built formal structured models for connecting vision and language (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013), we take a neural approach and connect today's best models of language form to similarly neural models of a simulated environment.
7 Conclusion

Emily M. Bender and Alexander Koller. 2020. Climbing towards NLU: On meaning, form, and understanding in the age of data. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5185­5198, Online. Association for Computational Linguistics.
Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella Lapata, Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich, et al. 2020. Experience grounds language. arXiv preprint arXiv:2004.10151.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165.
S. Carey and E. Bartlett. 1978. Acquiring a single new word.

In this paper, we presented an approach PIGLeT for jointly modeling language form and meaning. We presented a testbed PIGPeN for evaluating our model, which performs well at grounding language to the (simulated) world.
Acknowledgments
We thank the reviewers for their helpful feedback, and the Mechanical Turk workers for doing a great job in annotating our data. Thanks also to Zak Stone and the Google Cloud TPU team for help with the computing infrastructure. This work was supported by the DARPA CwC program through ARO (W911NF-15-1-0543), the DARPA MCS program through NIWC Pacific (N66001-19-2-4031), and the Allen Institute for AI.

Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, and Dhruv Batra. 2018. Embodied question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1­10.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171­4186.
Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2019. Unified language model pre-training for natural language understanding and generation. arXiv preprint arXiv:1905.03197.

References
Chris Alberti, Jeffrey Ling, Michael Collins, and David Reitter. 2019. Fusion of detected objects in text for visual question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2131­2140.
Yoav Artzi, Nicholas FitzGerald, and Luke S Zettlemoyer. 2013. Semantic parsing with combinatory categorial grammars. ACL (Tutorial Abstracts), 3.
Emily M Bender, Timnit Gebru, Angelina McMillanMajor, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big. Proceedings of FAccT.

Andrea Frome, Greg Corrado, Jonathon Shlens, Samy Bengio, Jeffrey Dean, Marc'Aurelio Ranzato, and Tomas Mikolov. 2013. Devise: A deep visualsemantic embedding model.
Daniel Gordon, Aniruddha Kembhavi, Mohammad Rastegari, Joseph Redmon, Dieter Fox, and Ali Farhadi. 2018. Iqa: Visual question answering in interactive environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
Aditya Gupta and Greg Durrett. 2019. Effective use of transformer networks for entity tracking. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 759­769.

9

Tanmay Gupta, Alexander Schwing, and Derek Hoiem. 2019. Vico: Word embeddings from visual cooccurrences. In Proceedings of the IEEE International Conference on Computer Vision, pages 7425­ 7434.
Stevan Harnad. 1990. The symbol grounding problem. Physica D: Nonlinear Phenomena, 42(13):335­346.
Richard Held and Alan Hein. 1963. Movementproduced stimulation in the development of visually guided behavior. Journal of comparative and physiological psychology, 56(5):872.
Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy. 2020. Spanbert: Improving pre-training by representing and predicting spans. Transactions of the Association for Computational Linguistics, 8:64­77.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361.
Diederik P. Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. CoRR, abs/1412.6980.
Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi. 2017. Ai2-thor: An interactive 3d environment for visual ai. arXiv preprint arXiv:1712.05474.
Jayant Krishnamurthy and Thomas Kollar. 2013. Jointly learning to parse and perceive: Connecting natural language to the physical world. Transactions of the Association for Computational Linguistics, 1:193­206.
Brenden Lake and Marco Baroni. 2018. Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. In International Conference on Machine Learning, pages 2873­2882. PMLR.
Cynthia Matuszek, Nicholas FitzGerald, Luke Zettlemoyer, Liefeng Bo, and Dieter Fox. 2012. A joint model of language and perception for grounded attribute learning. In Proceedings of the 29th International Coference on International Conference on Machine Learning, pages 1435­1442.
James L McClelland, Felix Hill, Maja Rudolph, Jason Baldridge, and Hinrich Schu¨tze. 2019. Extending machine language models toward humanlevel language understanding. arXiv preprint arXiv:1912.05877.
Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311­318.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. Technical report, OpenAI.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv e-prints.
Laura Ruis, Jacob Andreas, Marco Baroni, Diane Bouchacourt, and Brenden M Lake. 2020. A benchmark for systematic generalization in grounded language understanding. Advances in Neural Information Processing Systems, 33.
Noam Shazeer and Mitchell Stern. 2018. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pages 4603­4611.
Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. 2020. Alfred: A benchmark for interpreting grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10740­10749.
Linda Smith and Michael Gasser. 2005. The development of embodied cognition: Six lessons from babies. Artificial life, 11(1-2):13­29.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pages 6000­6010. Curran Associates Inc.
Xiaolong Wang, Ali Farhadi, and Abhinav Gupta. 2016. Actions ~ transformations. In CVPR.
Mark Yatskar, Vicente Ordonez, and Ali Farhadi. 2016. Stating the obvious: Extracting visual common sense knowledge. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 193­198.
Chen Yu and Linda B Smith. 2012. Embodied attention and word learning by toddlers. Cognition, 125(2):244­262.
Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019a. From recognition to cognition: Visual commonsense reasoning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. SWAG: A large-scale adversarial dataset for grounded commonsense inference. In Proceedings of the 2018 Conference on Empirical

10

Methods in Natural Language Processing, pages 93­ 104, Brussels, Belgium. Association for Computational Linguistics. Rowan Zellers and Yejin Choi. 2017. Zero-shot activity recognition with verb attribute induction. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 946­ 958. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019b. HellaSwag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4791­ 4800, Florence, Italy. Association for Computational Linguistics. Tianyi Zhang, V. Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert. ArXiv, abs/1904.09675.
11

A Model implementation details and hyperparameters.
We discuss the architectures and learning hyperparameters of our various models in the subsections below.
A.1 Physical Dynamics Model
We implemented our dynamics model with three Transformer layers for both the encoder and the decoder, and a hidden dimension of 256 for objects and actions. The resulting model has 17 million parameters. We pretrained the model for 20 epochs over 280k state transitions, with a batch size of 1024. We use an Adam optimizer (Kingma and Ba, 2014) with a learning rate of 1e-3.
A.2 Ordering attributes in decoding.
Recall that we use a left-to-right transformer to decode into an attribute representation, predicting attributes one-by-one from c1 to cn. Our model is agnostic to the actual order, as no matter what the order is, it still is modeling a decomposition of the joint probability of generating that object. However, we implemented this by using the name as the first attribute c1 that is predicted, and ordered the rest in a descending way by vocabulary size so as to predict harder attributes first.
A.3 Optimization Hyperparameters chosen
We finetuned PIGLeT for both tasks with an Adam optimizer (Kingma and Ba, 2014). We did a small grid search for hyperparameter values, choosing the best learning rate {2e-5, 1e-5, 1e-6} by accuracy on the development set, and likewise the best batch size 16 or 32. We considered freezing the physical dynamics backbone as another hyperparameter. We found it slightly boosted performance on PIGPeN-NLG when we froze the physical dynamics backbone, but not so for PIGPeN-NLU. We trained our model for 80 epochs on paired data.
We trained the baseline models with the same backbone in the same way, using similar hyperparameters. However, we found that after 80 epochs, the baseline models without pretrained dynamics failed to converge, so we finetuned them for 200 epochs total. For T5, we used similar identical hyperparameter ranges as the other models. However, because T5 uses a different optimizer (AdaFactor; Shazeer and Stern (2018)), which operates on a slightly different scale, we used a different

set of learning rates. We chose the best one over {1e-4, 2e-4, 4e-4}.
Search. Both of our tasks involve left-to-right decoding. We used argmax (greedy) search for PIGPeN-NLU, finding that it worked well as a `closed-ended generation' style task. On the other hand, we used Nucleus Sampling for PIGPeNNLG as there are often several ways to communicate a state transition; here we set p = 0.8.
A.4 Encoding the input for text-to-text models
Text-to-text models, needless to say, can only handle text. We encode the world states into a representation suitable for these models by formatting the object states as a JSON-style dictionary of keys and values. We had to make several modifications to the encoding however from a default JSON, because we handle a lot of attributes in this task, and JSON has quote characters `'` that take up a lot of space in a BPE encoding. We thus strip the quote characters and lowercase everything (with this also helping BPE-efficiency). We put parentheses around each object and give names to all `binned' attributes.
An example encoding might be:
Predict next object states: (objectname: bowl, parentreceptacles: cabinet, containedobjects: none, distance: 6 to 8 ft, mass: .5 to 1lb, size: medium, temp: roomtemp, breakable: true, cookable: false, dirtyable: true, broken: false, cooked: false, dirty: false, filledwithliquid: false, open: false, pickedup: false, sliced: false, toggled: false, usedup: false, moveable: false, openable: false, pickupable: true, receptacle: true, sliceable: false, toggleable: false, materials: glass) (objectname: egg, parentreceptacles: none, containedobjects: none, distance: 2 to 3ft, mass: .1 to .2lb, size: tiny, temp: cold, breakable: true, cookable: true, dirtyable: false, broken: false, cooked: false, dirty: false, filledwithliquid: false, open: false, pickedup: true, sliced: false, toggled: false, usedup: false, moveable: false, openable: false, pickupable: true, receptacle: false, sliceable: true, toggleable: false, materials: food) (action: throwobject10)
We have models decode directly into this kind of format when predicting state changes. Though the T5 models usually get the format right, we often have to sanitize the text in order for it to be a valid object state in our framework. This is espe-

12

cially an issue with GPT3, since it is given limited supervision (we squeeze 3 examples into the 2048BPE token context window) and often makes up new names and attributes. Thus, for each word not in an attribute's vocabulary, we use a Levenstein distance heuristic to match the an invalid choice with its closest (valid) option. If the model fails to generate anything for a certain attribute key ­ for example if it does not include something like openable somewhere, we copy the representation of the input object for that attribute, thereby making the default assumption that attributes do not change.

B All THOR attributes

We list a table with all of the attributes we used for this work in Table 4.

C Turk Annotation Details

We followed crowdsourcing best practices, such as using a qualification exam, giving feedback to workers, and paying workers well (above $15 per hour). Each of our HITs required writing three sentences, and we paid Mechanical Turk workers 57 cents per HIT. We used three workers per example, allowing us to have multiple language references for evaluation. A screenshot of our user interface is shown in Figure 6.

Figure 6: Our user interface for Mechanical Turk annotation.

D Our Pretrained Language Model
We use our own pretrained language model primarily because it allows us to investigate the impact of data on model performance. We trained a prefixmasked language model (Dong et al., 2019) on Wikipedia and Book data, mimicing the data used by the original BERT paper (Devlin et al., 2019). We trained the model for 60000 iterations, at a batch size of 8192 sequences each of length 512. This corresponds to 50 epochs over the dataset. We masked inputs in the bidirectional prefix with SpanBERT masking (Joshi et al., 2020). Since BERTstyle `masked' out inputs are easier to predict than tokens generated left-to-right, we reduced the loss component of left-to-right generation by a factor of 20; roughly balancing the two loss components.

Counts

Bed CellPhone Window

Painting Plate

Egg Mirror CounterTop Bread Potato Sink Drawer Fridge Pan Mug Vase Microwave GarbageCan Kettle Blinds TeddyBear Lettuce LightSwitch Toaster WineBottle

500k
Counts (in Wikipedia / Toronto Books) of our zero-shot words
400k
300k
200k
100k
0
Figure 7: Counts of zero-shot words that appear in BERT's training data (Wikipedia and Toronto Books). For example, in the 4 billion words BERT is trained on, it sees the word `Bed' almost 500k times. This might allow it to perform superficially well at answering questions about beds ­ while not necessarily possessing deep physical knowledge about them.

13

Attribute Name

Vocab size Values

objectName

126

parentReceptacles

126

receptacleObjectIds

126

mass

8

size

8

distance

8

ObjectTemperature

3

breakable

2

canBeUsedUp

2

canFillWithLiquid

2

cookable

2

dirtyable

2

isBroken

2

isCooked

2

isDirty

2

isFilledWithLiquid

2

isOpen

2

isPickedUp

2

isSliced

2

isToggled

2

isUsedUp

2

moveable

2

openable

2

pickupable

2

receptacle

2

sliceable

1

toggleable

2

salientMaterials Ceramic 2

salientMaterials Fabric 2

salientMaterials Food

2

salientMaterials Glass

2

salientMaterials Leather 2

salientMaterials Metal

2

salientMaterials None

2

salientMaterials Organic 2

salientMaterials Paper

2

salientMaterials Plastic 2

salientMaterials Rubber 2

salientMaterials Soap

2

salientMaterials Sponge 2

salientMaterials Stone

2

salientMaterials Wax

2

salientMaterials Wood 2

One per object type, along with None One per object type, along with None One per object type, along with None 8 bins 8 bins 8 bins Hot , Cold , RoomTemp

Generator

Description

put_X_in_Y throw_X_at_Y
toggle_X slice_X dirty_X clean_X
toast_bread brew_coffee fry_X
microwave_X
fill_X

Samples an object X from the scene, and a receptacle Y . Tries to put it in Y .
Samples two objects X and Y from the scene. Picks up X , moves to face Y , and throws it forward with variable intensity.
Samples an object X , and turns it on or off.
Samples an object X and a surface Y . Picks up X , places it on Y , and cuts it.
Samples an object X , and makes it dirty.
Samples a dirty object X . Finds a Sink nearby a Faucet , and places X inside. Turns on/off the Faucet , cleaning X .
Finds some Bread , slicing it if necessary, places it in a Toaster , then turns it on.
Picks up a Mug , places it under a CoffeeMachine , and turns the machine on.
Picks up a food X , slices it if necessary, and puts it in a Pot or Pan . Brings it to a StoveBurner and turns the burner on.
Picks up an object X and slices it if necessary. Places it in a Microwave , closes it, and then turns it on.
Picks up an object X places it under a Faucet . Turns on/off the Faucet , then pours out the liquid.

Table 5: Trajectory generation functions that we used to sample `interesting' physical interactions, such as the effects that actions will have on objects, and which actions will succeed or not.

Table 4: All attributes that we consider for this work in THOR. We list the attribute's name, the size of the attribute vocabulary, and the range of values the attribute can take on. For attributes like `mass', `size', and `distance', we note that the underlying simulator stores them as floats; we bin them to 8 values for this work. All the values for attributes with a vocabulary size of 2 are boolean.

14

