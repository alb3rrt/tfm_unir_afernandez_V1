
# Improving Long-Term Metrics in Recommendation Systems using Short-Horizon Offline RL

[arXiv](https://arxiv.org/abs/2106.0589), [PDF](https://arxiv.org/pdf/2106.0589.pdf)

## Authors

- Bogdan Mazoure
- Paul Mineiro
- Pavithra Srinath
- Reza Sharifi Sedeh
- Doina Precup
- Adith Swaminathan

## Abstract

We study session-based recommendation scenarios where we want to recommend items to users during sequential interactions to improve their long-term utility. Optimizing a long-term metric is challenging because the learning signal (whether the recommendations achieved their desired goals) is delayed and confounded by other user interactions with the system. Immediately measurable proxies such as clicks can lead to suboptimal recommendations due to misalignment with the long-term metric. Many works have applied episodic reinforcement learning (RL) techniques for session-based recommendation but these methods do not account for policy-induced drift in user intent across sessions. We develop a new batch RL algorithm called Short Horizon Policy Improvement (SHPI) that approximates policy-induced distribution shifts across sessions. By varying the horizon hyper-parameter in SHPI, we recover well-known policy improvement schemes in the RL literature. Empirical results on four recommendation tasks show that SHPI can outperform matrix factorization, offline bandits, and offline RL baselines. We also provide a stable and computationally efficient implementation using weighted regression oracles.

## Comments



## Source Code

Official Code



Community Code



## Bibtex

```tex
@misc{mazoure2021improving,
      title={Improving Long-Term Metrics in Recommendation Systems using Short-Horizon Offline RL}, 
      author={Bogdan Mazoure and Paul Mineiro and Pavithra Srinath and Reza Sharifi Sedeh and Doina Precup and Adith Swaminathan},
      year={2021},
      eprint={2106.00589},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```

## Notes

Type your reading notes here...

