An Entropy Regularization Free Mechanism for Policy-based Reinforcement Learning

arXiv:2106.00707v1 [cs.LG] 1 Jun 2021

Changnan Xiao ByteDance
xiaochangnan@bytedance.com
Jiajun Fan Nankai University jiajunfanthu@gmail.com

Haosen Shi Nankai University shihaosen98@gmail.com
Shihong Deng ByteDance
dengshihong@bytedance.com

Abstract
Policy-based reinforcement learning methods suffer from the policy collapse problem. We find valued-based reinforcement learning methods with -greedy mechanism are capable of enjoying three characteristics, Closed-form Diversity, Objective-invariant Exploration and Adaptive Trade-off, which help value-based methods avoid the policy collapse problem. However, there does not exist a parallel mechanism for policy-based methods that achieves all three characteristics. In this paper, we propose an entropy regularization free mechanism that is designed for policy-based methods, which achieves Closed-form Diversity, Objective-invariant Exploration and Adaptive Trade-off. Our experiments show that our mechanism is super sample-efficient for policy-based methods and boosts a policy-based baseline to a new State-Of-The-Art on Arcade Learning Environment.
1 INTRODUCTION
Reinforcement Learning (RL) algorithms can be divided into two categories, value-based methods and policy-based methods (Nachum et al., 2017). Policy-based reinforcement learning methods learn a parameterized policy directly without consulting a value function (Sutton, Barto, 2018). These methods suffer from the policy collapse problem, where the entropy of the policy drops to zero but the learned policy is far from achieving the optimal policy (Mnih et al., 2016a; Dadashi et al., 2019; Mnih et al., 2016b). It's unsatisfactory to mitigate this problem by increasing the amount of training data due to the fact that, when the learned policy is trapped at a sub-optimal solution, it cannot be improved anymore. This weakness prevents policy-based methods from enjoying the benefit of a large training scale.
Value-based reinforcment learning methods generate the behavior policy through a learned stateaction value function (Sutton, Barto, 2018). The behavior policy is generally controlled by -greedy. This mechanism enjoys several characteristics. Firstly, -greedy is closed-form, and it enjoys one dimension of flexibility for adjusting the exploration rate of the behavior policy. Specifically, the exploration rate can be simply controlled by adjusting the scale of . So it has a closed-form family of behavior policies, which is diverse in one dimension of freedom, . We call this characteristic that the behavior policy can be sampled from a family of policies that are defined in a closed-form function of the target policy as Closed-form Diversity. Secondly, the objective function of the value-based methods is free from . The choice of is arbitrary, which would not interfere with the convergence of the target policy to the optimal policy. We call this characteristic that no matter how the behavior policy explores, the objective of the target policy is always the original objective of the MDP as Objective-invariant Exploration. Thirdly, due to the fact that -greedy is closed-form and would not
Preprint. Under review.

influence the convergence property of the target policy, it's not difficult to implement some adaptive mechanism for a better trade-off between exploration and exploitation, such as (Badia et al., 2020a; Tokic, 2010; Santos Mignon dos, Rocha da, 2017). We call this characteristic that there exists some mechanisms that the behavior policy can adaptively balance the trade-off between exploration and exploitation as Adaptive Trade-off.
From the perspective of exploration and exploitation, these three characteristics of -greedy are critical to balancing exploration and exploitation. Closed-form Diversity guarantees sufficient exploration, where the trajectories are generated from not only the target policy but a family of behavior policies with different exploration rates. Objective-invariant Exploration guarantees sufficient exploitation, where the optimal policy and fundamental elements of the Markov Decision Process (MDP) are consistent, regardless from which behavior policy the trajectories are generated. Adaptive Trade-off guarantees sample efficiency, where the behavior policy with elite trajectories should be paid more attention by the target policy.
As mentioned above, policy-based methods suffer from the policy collapse problem. Utilizing entropy regularization in the objective function for encouraging exploring is a crucial component for recent policy-based RL methods (Schulman et al., 2017b; Mnih et al., 2016b; Espeholt et al., 2018; Haarnoja et al., 2018a, 2017). The entropy regularization coefficient is usually declining according to some annealing mechanism or fixed at a picked value. But the exploration rate induced by the entropy regularization coefficient is implicit, which fails to define the exploration rate of the behavior policy as a closed-form function of the target policy. Meanwhile, since it encourages exploration of the behavior policy by introducing an entropy regularization term into the objective function of the target policy, the objective of the target policy is inconsistent and not Objective-invariant during the training process. The gap between original MDP and entropy augmented MDP leads that target policy converges to a sub-optimal policy (Song et al., 2019). Moreover, in practice, it is hard to control the effect of such mechanism adaptive to different environments and targets. It generally takes many parallel experiments to find a proper regularization coefficient, which results in a lower sample efficiency behind the reported results. Although some policy-based methods (Haarnoja et al., 2018b; Song et al., 2020) achieve Adaptive Trade-off, they also fail to achieve Closed-Form Diversity and Objective-invariant Exploration.

DQN R2D2 Agent57 A2C PPO IMPALA SAC

Closed-form Diversity

××

×

×

Objective-invariant Exploration

×

××

×

×

Adaptive Trade-off

×

×

××

×

Table 1: Survey of Algorithms. represents the algorithm has the characteristic. × represents not.

Value-based methods (Mnih et al., 2015; Kapturowski et al., 2018; Badia et al., 2020a) is capable to enjoying three characteristics. DQN and R2D2 lack a meta-controller. Agent57 involves intrinsic rewards, so it's not Objective-invariant.
But for policy-based methods, to the best of our knowledge, there is no mechanism for policy-based methods that is capable to enjoying Closed-form Diversity, Objective-invariant Exploration and Adaptive Trade-off.
We propose a new mechanism DiCE1, Directly Control Entropy, which is designed specifically for policy-based methods. DiCE for policy-based methods is analogy to -greedy for valued-based methods and it also enjoys Closed-form Diversity, Objective-invariant Exploration and Adaptive Trade-off. Same as -greedy, DiCE holds one dimension of freedom to adjust the exploration rate, which is the temperature  . The behavior policies are in a Closed-form family. The target policy is always Objective-invariant, no matter what the behavior policy's temperature is. Additionally, we exploit these properties of DiCE and provide a simple implementation for the Adaptive Trade-off between exploration and exploitation. Our experiments show that DiCE boosts policy-based baseline by a large margin and achieves the new State-Of-The-Art (SOTA) under 200M training scale.
Our main contributions are as follows:
1DiCE means that the behavior policy can be sampled from a family of behavior policies as easy as tossing a dice.

2

· We propose three critical characteristics of value-based methods, Closed-Form Diversity, Objective-invariant Exploration and Adaptive Trade-off.
· We propose a new mechanism for policy-based methods that enjoys Closed-Form Diversity, Objective-invariant Exploration and Adaptive Trade-off.
· We provide one kind of bandit-based implementation for Adaptive Trade-off. · Our experiments show that the proposed mechanism boosts policy-based methods to a new
state-of-the-art.

DiCE


7000.00

6456.63

4666.67

2333.33
IMPALA
 957.34

LASER
 1741.36
RAINBOW
 873.97

AGENT57
 4763.69

R2D2


NGU


3374.31 3169.90

1E+08

1E+09

1E+10

Training Scale

(a) Mean

1E+11

AGENT57
 1933.49
2000.00

R2D2
 1342.27

NGU
 1208.11

1000.00 DiCE
 477.17

LASER
 454.91

RAINBOW
 IMPALA


230.99

191.82

1E+08

1E+09

1E+10

Training Scale

(b) Median

1E+11

Figure 1: Human Normalized Scores (%) of 57 games.

MEAN HNS(%) MEDIAN HNS(%)

2 BACKGROUND

Consider an MDP, defined by a 5-tuple (S, A, p, r, ), where S is thee state space, A is the action space, p is the state transition probability function which associates distributions over state to stateaction pair, r : S × A  R is the reward function, and   (0, 1) is the discounted factor. Let  a distribution over action to each state be the policy.

The objective of reinforcement learning is to find



arg max J = E(s0,a0,s1,...,st,at,...)

krk .

(1)



k=0

, where E(s0,a0,s1,...,st,at,...) is a brief form of Ea1(a1|s0),s1p(s1|a1,s0),....

Value-based methods maximize J by generalized policy iteration (GPI) (Sutton, Barto, 2018). The policy evaluation is conducted by minimizing E[(T Q - Q)2], where T Q is
T Q(st, at) = Ep[rt + V (st)], V (st) = E[Q(st, at)]

where Ep is a improvement

shorthand is usually

of Est+1p(st+1|st,at) achieved by -greedy.

and A

E is a refined

ssthruocrttuhraenddeosfigEnatofQ(at|sits)

. The policy provided by

dueling-Q (Wang et al., 2016). It estimates Q by the summation of the advantage function and the

state value function, Q = A + V .

When the large scale training is involved, the off-policy problem is inevitable. Denote µ as the

behavior

policy,



as

the

target

policy.

ct

d=ef

min{

t µt

,

c¯}

is

the

clipped

importance

sampling.

For

brevity, c[t:t+k] d=ef

k i=0

ct+i.

ReTrace

(Munos

et

al.,

2016)

estimates

Q(st,

at)

by

clipped

per-step

importance sampling

Q~ (st, at) = Eµ[Q(st, at) + kc[t+1:t+k]tQ+kQ],

(2)

k0

where tQQ d=ef rt + Q(st+1, at+1) - Q(st, at). The ReTrace operator is a contraction mapping and has a fixed point Q~ReTrace corresponding to some ~ReT race. Q is learned by minimizing Eµ[(Q~ - Q)2], which gives the gradient ascent direction

Eµ[(Q~ - Q)Q].

(3)

3

HNS(%)

SABER(%)

Num. Frames Mean Median Mean Median

DiCE Rainbow IMPALA LASER

200M 200M 200M 200M

6456.63 873.97 957.34 1741.36

477.17 230.99 191.82 454.91

50.11 28.39 29.45 36.77

13.90 4.92 4.31 8.08

R2D2 NGU Agent57

10B 35B 100B

3374.31 1342.27 60.43 3169.90 1208.11 50.47 4763.69 1933.49 76.26

33.62 21.19 43.62

Table 2: Atari Scores. Rainbow's scores are from (Hessel et al., 2017). IMPALA's scores are from (Espeholt et al., 2018). LASER's scores are from (Schmitt et al., 2020), no sweep at 200M. R2D2's scores are from (Kapturowski et al., 2018). NGU's scores are from (Badia et al., 2020b). Agent57's scores are from (Badia et al., 2020a).

Policy-based methods maximize J by policy gradient. It's shown (Sutton, Barto, 2018) that

J = becomes

E[G log ], where G an actor-critic algorithm.

= The

 i=t

i

ri.

When

policy gradient is

involved given by

with J

an estimated baseline, it = E[(G - V ) log ],

where V  is optimized by minimizing E[(G - V )2].

Since (Williams, Peng, 1991), many on-policy policy-based methods (Mnih et al., 2016b; Schulman et al., 2017b) add an entropy regularization to loss function like H(), where H() = - a (a |s) log (a |s) is the entropy of .

As one large scale policy-based method, IMPALA (Espeholt et al., 2018) also introduces entropy.

Besides, IMPALA introduces V-Trace off-policy actor-critic algorithm to correct the discrepancy

between

the

target

policy

and

the

behavior

policy.

Let

t

d=ef

min{

t µt

,

¯},

ct

d=ef

min{

t µt

,

c¯}

to

be

the clipped importance sampling. For brevity, let c[t:t+k] d=ef

k i=0

ct+i.

V-Trace

estimates

V

(st)

by

V ~ (st) = Eµ[V (st) + kc[t:t+k-1]t+ktV+kV ],

(4)

k0

where tV V d=ef rt + V (st+1) - V (st). V  is learned by minimizing Eµ[(V ~ - V )2], giving the

gradient ascent direction

Eµ[(V ~ - V )V ].

(5)

If c¯  ¯, the V-Trace operator is a contraction mapping, and V converges to V ~ that corresponds to

min {¯µ(a|s), (a|s)}

~(a|s) =

.

bA min {¯µ(b|s), (b|s)}

Combining V-Trace into the Actor-Critic paradigm, the direction of the policy gradient is given by

Eµ t(rt + V ~ (st+1) - V (st)) log  .

(6)

Another related work is maximum entropy RL (Haarnoja et al., 2017, 2018a), which maximizes state value as well as the entropy of the policy. Here we call them MaxEnt. The objective of MaxEnt is to



maximize Jsoft = E

k(rk +  H[]) .

k=0

Qsoft and Vsoft are defined by the soft Bellman equation

T Qsoft(st, at) = Ep[rt + Vsoft(st+1)], where Vsoft(st) = E[Qsoft(st, at) -  log (at|st)].

(7)

soft is defined as a Boltzmann policy soft(at|st)  exp((Qsoft(st, at) - Vsoft(st))/ ).

4

Although (Haarnoja et al., 2018a) achieves Adaptive Trade-off by adjusting  adaptively, it still suffers some drawbacks. i) Since  in Jsoft encourages exploration by maximizing entropy reward when optimizing the target policy, it couples the exploration rate of the behavior policy with the target policy, which makes it neither Closed-form nor Diversity. ii) Jsoft is regularized by H[] compared to J, which means the target policy is not Objective-invariant during training.

3 Method

In this section, we introduce DiCE and also check three characteristics. At the beginning of 3.1, we use V to estimates V , A to estimates A, Q = V + A to estimate Q and  to approximate , where  represents all parameters to be optimized. But the target functions of V, A and Q will be changed later, which is detailed in 3.1. In 3.2 and 3.3, V, A and Q estimate the new target
functions.

3.1 Closed-form Diversity

Assume v  R|A| is a parameterized vector. For brevity, we write v as v. Inspired by MaxEnt, we define a Boltzmann policy as

 (v)  exp(v/ ).

It's evident that,

H[ ]  0,

  argmax, as   0+;

H[ ]  log |A|,   unif orm, as   +.

Since H[ ] is a continuous function of  , for  0 < e < log |A|,  0 s.t. H[0 ] = e. So we can simply control H[ ] by choosing a proper  .

If we define a family of behavior policies as

Bv = { (v)|   (0, +)},

it's obvious that Bv is a family of closed-form functions of v and Bv has one dimension of freedom  . So Bv achieves Closed-form Diversity for v.

Since the behavior policy is sampled from a family Bv, it's nature to ask what the definitions of V , Q and A are, i.e. what functions V, Q and A are estimating. As (V  , Q ) satisfies the following Bellman equation,
T Qt  = Ep[rt + Vt+1], where Vt = Eat [Qt  (·, at)], we observe that  only influences (V  , Q ) when taking the expectation Vt = E [Qt  ].

Assume  is sampled from a distribution , the influence is totally eliminated as
T Qt  = Ep[rt + Vt+1], where Vt = E[Eat [Qt  (·, at)]].

Now Qt  only depends on the transition probability p and the average policy  d=ef   , which is free from  . Hence, we rewrite the above Bellman equation as

T Qt = Ep[rt + Vt+1], where Vt = Eat [Qt (·, at)],

where Qt d=ef  Qt  , Vt d=ef  Vt . Similarly, we have At d=ef  At  . Now we use V to estimates V , A to estimates A, Q = V + A to estimate Q.

3.2 Objective-invariant Exploration

To check if Bv is Objective-invariant, by a typical derivative of cross entropy, we have  log  =

(1

-



)

v 

.

Plugging

into

(6),

we

have

the

following

policy

gradient

J = Eµ

t(rt

+

V

~

(st+1)

-

V



(st))(1

-



)

v 

,

5

where µ is the behavior policy corresponding to  .

It's obvious that maximize J is equivalent to maximize  J . Taking an additional  , we free the scale of policy gradient from  ,

 J = Eµ t(rt + V ~ (st+1) - V  (st))(1 -  )v .

Taking expectation w.r.t.  , we have

E[ J ] = E Eµ t(rt + V ~ (st+1) - V  (st))(1 -  )v .

(8)

When the importance sampling clip ¯ and c¯ is +, we know V-Trace is an unbiased estimation

(Espeholt et al., 2018). Regardless the function approximation error of V, we have

E[ J ] = E [E [A (1 -  )v]] .

Therefore, if  is a fixed distribution of  , Bv is Objective-invariant as E[ J ] is free from  . If  can be optimized, let 0 = arg max { J }, then E[ J ] = 0J0 is equivalent to the objective defined in (1). So it's also  -free.

In practice, we choose v = A, this is because (Schulman et al., 2017a) proves the equivalence between the evaluation of the state-action value function and the policy gradient with the evaluation of the state value function. But it's an open question if there exists a better choice of v.

We use the Learner-Actor architecture. The Actor part of DiCE is shown in Algorithm 1, and the full version can be found in appendix B.

Algorithm 1 DiCE Algorithm (Actor).
Initialize Parameter Server (PS) and Data Collector (DC). // ACTOR Initialize dpull, M , . Initialize , i.e. initialize A, V,Q = V + A and . Initialize {Bm}m=1,...,M and sample   . Initialize count = 0, G = 0. Initialize environment and s = s0. while T rue do
Calculate  (·|s)  exp(A/ ). Sample a   (·|s). s, r, done  p(·|s, a). G  G + r. if done then
Update  by (, G). Send data to DC. Sample   . G  0. Reset the environment and s = s0. end if if count mod dpull = 0 then Pull  from PS and update . end if count  count + 1. end while
3.3 Adaptive Trade-off
In Algorithm 1, both "Update  by (, G)" and "Sample   " are achieved by Adaptive Trade-off.
In practice, we use a Bandits Vote Algorithm (BVA) to adaptively update  to maximize the expected cumulative return. Different from other meta-controllers, BVA uses an ensemble of bandits. During the training process, each bandit is updated individually. During the evaluation process, each bandit provides one evaluation for each  and the final evaluation of each  is the average evaluation of all bandits. During the sample process, each bandit provides several candidates and one candidate is sampled uniformly from all candidates. The details of BVA is shown in Appendix A.
6

4 Experiments
We firstly introduce our basic setup. Then we report our results on ALE, namely, 57 atari games. In our ablation study, we report the results of the policy-based baseline of DiCE, which shows that DiCE boosts our policy-based baseline by a large margin. We also make additional ablation study to check the effect of the proposed characteristics.

4.1 Basic Setup
All the experiment is accomplished using 92 CPU cores and a single Tesla-V100-SXM2-32GB GPU. To deal with partially observable MDP (POMDP), we use a recurrent encoder by LSTM (Schmidhuber, 1997) with 256 units. We use burn-in (Kapturowski et al., 2018) to deal with representational drift. We store the recurrent state during inference and make it the start point of the burn-in phase. We train each sample twice. We do not use any intrinsic reward in any experiment. To be general, we will not end the episode if life is lost.
We use BVA to sample  s, evaluate  s and find the best region of  s. We use  s sampled from the best region to evaluate the expected return. Hyperparameters are listed in Appendix C.

4.2 Analysis and Summary of Results

Table 2 summarizes the mean and the median Human Normalized Scores (HNS) of 57 games, as well as Standardized Atari BEnchmark for RL (SABER)(Toromanoff et al., 2019).2 Scores of 57 games and learning curves are listed in Appendix E. The videos will be released in the future.
In general, DiCE achieves SOTA on mean HNS compared to other algorithms, including 10B+ algorithms. DiCE also achieves the highest median HNS among 200M algorithms.
The results can roughly be classified into three kinds: (i) results of some games achieve historical highest score, such as Atlantis, Gopher, Jamesbond. For fairness, we also report mean and median SABER, which is normalized by Human World Records and capped by 200%. The mean SABER is very competitive compared to other 10B+ algorithms. The mean SABER and median SABER is much higher than other 200M algorithms, which shows the overall performance is much better than other 200M algorithms. (ii) results of some games are increasing and the learning processes have not converged, such as Alien, BeamRider, ChopperCommand; (iii) results of some games encounter the bottleneck due to the hard exploration problem, such as IceHockey, PrivateEye, Surround.
One crucial thing for solving (iii) is how to acquire better samples. Even DiCE achieves controlling the entropy of the behavior policy in a Closed-form, DiCE only maintains one dimension of freedom. DiCE tries to collect diverse samples by enlarging the behavior policy to a family of policies { }[K,+]. The family can achieve a large range of entropy. However, it's obvious that  is an order-preserving mapping of A, which means that the total order relations of actions' probability distributions are identical for    { }[K,+]. Value-based methods such as R2D2 collect samples by a family of policies { } [0,1], where  represents -greedy. Except for the max Q-value,  does not preserve the order. { } [0,1] is more likely to explore the state where { }[K,+] less prefers. The data distribution of samples is influenced by the inductive bias induced by the family of behavior policies. It's critical and open to define a proper family of closed-form functions for more elite samples. Instead, NGU and Agent57 achieve active exploration of the environment by the intrinsic reward. Such exploration enlarges MDP, so UVFA (Schaul et al., 2015) is needed.
DiCE focuses on Objective-invariant Exploration. There are some hints to improve DiCE. The first is to enlarge the function space of the behavior policy by a linear combination. Since  is identical to
· + + (1 - ) · 0+, it's reasonable to consider a combination representation for a larger family of behavior policies, such as { i i · i | i i = 1}. The second is to adopt the idea of LASER by shared experience replay, which gives diverse samples from different families of policies {{n, }[K,+]}n=1,2,.... The third is to make a better policy evaluation. Now the value function V is estimating the average policy , which is V . A better way is to use a UVFA or a closed-form expression, if exists, to estimate {V  }[K,+].

2HNS

=

. G-Grandom
GH uman -Grandom

SABER

=

min{200%,

}. G-Grandom
GHumanW orldRecord-Grandom

7

4.3 Ablation Study
We firstly provide the scores of the baseline on 57 atari games. All hyperparameters of the baseline are the same as DiCE. Except for that we remove "Update  by (, G)" and change "Sample   " to " = 1" in Algorithm 1, the baseline is trained and evaluated identically to DiCE. From another perspective, we apply DiCE directly on the baseline without any change.

DiCE baseline

HNS(%)

SABER(%)

Num. Frames Mean Median Mean Median

200M 200M

6456.63 477.17 50.11 1929.95 195.54 35.91

Table 3: Comparison to baseline.

13.90 8.73

The summary is shown in Table 3. Full comparison on 57 games are listed in Appendix D. The progress is impressive. DiCE promotes mean HNS by 235%, median HNS by 145%, mean SABER by 40% and median SABER by 59%. Recalling Table 2, although our baseline meets SOTA performance, there is no evident performance difference between our baseline and other 200M algorithms. But DiCE achieves the best performance on all criteria among 200M algorithm and is approaching 10B+ algorithms on some criteria. This phenomenon proves that DiCE is very effective and boosts our policy-based baseline to a new State-Of-The-Art.

Name DiCE baseline random entropy

Characteristics Closed-form Diversity, Objective-invariant Exploration, Adaptive Trade-off
N/A Closed-form Diversity, Objective-invariant Exploration
Closed-form Diversity, Adaptive Trade-off Table 4: Ablation Settings.

We make additional ablation study to find how much each characteristic contributes to DiCE. To check Objective-invariant, we add an entropy regularization with coefficient = 1 to the loss function, named as entropy. To check Adaptive Trade-off, we remove "Update  by (, G)" in Algorithm 1, which means the distribution of  is always the fixed initial distribution, named as random. It's impossible to check Closed-form Diversity solely. If Closed-form Diversity does not hold, there is no need to consider Objective-invariant Exploration and Adaptive Trade-off.
We choose Breakout and ChopperCommand to do ablation study, because the performances of DiCE and the baseline on these two games are comparable.

1.1e+3 1e+3 900 800 700 600 500 400 300 200 100

1e+6 9e+5 8e+5 7e+5 6e+5 5e+5 4e+5 3e+5 2e+5 1e+5

0

0

-100

-1e+5

-20k 0 20k 40k 60k 80k 100k 120k 140k 16

-20k 0 20k 40k 60k 80k 100k120k140k160k18

(a) breakout

(b) chopper_command

DiCE baseline random entropy

Figure 2: Evaluation Return Curves.
The evaluation curve are shown in Figure 2. It's obvious that random performs the worst, which evidently proves that Adaptive Trade-off is critical in DiCE. On the early stage of Breakout, DiCE

8

and entropy show a higher sample efficiency that baseline and random. Notice that DiCE and entropy have Adaptive Trade-off, while baseline and random do not. We see Adaptive Trade-Off helps boosting the sample efficiency. There is no significant difference between DiCE and entropy, as DiCE performs better on ChopperCommand but worse on Breakout. So we cannot make a conclusion whether Objective-invariant is better or not. As for Closed-form Diversity, the full comparison between DiCE and baseline has already shown that DiCE is effective. Since Closed-form Diversity is the necessary condition for DiCE, there is no doubt that Closed-form Diversity is important. In general, we conclude that Closed-form Diversity and Adaptive Trade-off are critical. As for Objective-invariant Exploration, it's uncertain whether Objective-invariant is better, but Exploration is one of the most important thing in RL.
Although DiCE enlarges the behavior policy to a family of behavior policies, it's still a question to find what a good exploration is. DisCor (Kumar et al., 2020) has claimed that the choice of the sampling distribution is of crucial importance for the stability and efficiency of ADP algorithms. With a given replay buffer, DisCor re-weights each sample to mitigate this phenomenon. But if working on sample acquiring rather than sample re-weighting, it's an interesting question to find a proper family of behavior policies such that the target policy learned with collected samples can achieve higher sample efficiency. Meanwhile, the superiority of this proper family of the behavior policies should be guaranteed. We leave this question for future study.
5 Conclusion
This paper proposes three characteristics, Closed-form Diversity, Objective-invariant Exploration and Adaptive Trade-off. We propose a mechanism for policy-based methods that enjoys three characteristics. The mechanism is sample efficient and boosts policy-based baseline to State-Of-TheArt with 200M training scale. The overall performance is higher than other 200M algorithms and is competitive compared to 10B+ algorithms. We analyse the ablation cases and discuss the potential improvement in future work.
References
Badia Adrià Puigdomènech, Piot Bilal, Kapturowski Steven, Sprechmann Pablo, Vitvitskyi Alex, Guo Daniel, Blundell Charles. Agent57: Outperforming the atari human benchmark // arXiv preprint arXiv:2003.13350. 2020a.
Badia Adrià Puigdomènech, Sprechmann Pablo, Vitvitskyi Alex, Guo Daniel, Piot Bilal, Kapturowski Steven, Tieleman Olivier, Arjovsky Martín, Pritzel Alexander, Bolt Andew, others . Never Give Up: Learning Directed Exploration Strategies // arXiv preprint arXiv:2002.06038. 2020b.
Dadashi Robert, Taiga Adrien Ali, Le Roux Nicolas, Schuurmans Dale, Bellemare Marc G. The value function polytope in reinforcement learning // International Conference on Machine Learning. 2019. 1486­1495.
Espeholt Lasse, Soyer Hubert, Munos Remi, Simonyan Karen, Mnih Volodymir, Ward Tom, Doron Yotam, Firoiu Vlad, Harley Tim, Dunning Iain, others . Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures // arXiv preprint arXiv:1802.01561. 2018.
Haarnoja Tuomas, Tang Haoran, Abbeel Pieter, Levine Sergey. Reinforcement learning with deep energy-based policies // arXiv preprint arXiv:1702.08165. 2017.
Haarnoja Tuomas, Zhou Aurick, Abbeel Pieter, Levine Sergey. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor // arXiv preprint arXiv:1801.01290. 2018a.
Haarnoja Tuomas, Zhou Aurick, Hartikainen Kristian, Tucker George, Ha Sehoon, Tan Jie, Kumar Vikash, Zhu Henry, Gupta Abhishek, Abbeel Pieter, others . Soft actor-critic algorithms and applications // arXiv preprint arXiv:1812.05905. 2018b.
Hessel Matteo, Modayil Joseph, Van Hasselt Hado, Schaul Tom, Ostrovski Georg, Dabney Will, Horgan Dan, Piot Bilal, Azar Mohammad, Silver David. Rainbow: Combining improvements in deep reinforcement learning // arXiv preprint arXiv:1710.02298. 2017.
9

Kapturowski Steven, Ostrovski Georg, Quan John, Munos Remi, Dabney Will. Recurrent experience replay in distributed reinforcement learning // International conference on learning representations. 2018.
Kumar Aviral, Gupta Abhishek, Levine Sergey. Discor: Corrective feedback in reinforcement learning via distribution correction // arXiv preprint arXiv:2003.07305. 2020.
Mnih Volodymyr, Badia Adria Puigdomenech, Mirza Mehdi, Graves Alex, Lillicrap Timothy, Harley Tim, Silver David, Kavukcuoglu Koray. Asynchronous methods for deep reinforcement learning // International conference on machine learning. 2016a. 1928­1937.
Mnih Volodymyr, Badia Adrià Puigdomènech, Mirza Mehdi, Graves Alex, Lillicrap Timothy P., Harley Tim, Silver David, Kavukcuoglu Koray. Asynchronous Methods for Deep Reinforcement Learning. 2016b.
Mnih Volodymyr, Kavukcuoglu Koray, Silver David, Rusu Andrei A, Veness Joel, Bellemare Marc G, Graves Alex, Riedmiller Martin, Fidjeland Andreas K, Ostrovski Georg, others . Human-level control through deep reinforcement learning // nature. 2015. 518, 7540. 529­533.
Munos Remi, Stepleton Tom, Harutyunyan Anna, Bellemare Marc. Safe and Efficient Off-Policy Reinforcement Learning // Advances in Neural Information Processing Systems 29. 2016. 1054­ 1062.
Nachum Ofir, Norouzi Mohammad, Xu Kelvin, Schuurmans Dale. Bridging the gap between value and policy based reinforcement learning // Advances in Neural Information Processing Systems. 2017. 2775­2785.
Santos Mignon Alexandre dos, Rocha Ricardo Luis de Azevedo da. An Adaptive Implementation of -Greedy in Reinforcement Learning // Procedia Computer Science. 2017. 109. 1146­1151.
Schaul Tom, Horgan Daniel, Gregor Karol, Silver David. Universal Value Function Approximators // ICML. 2015. 1312­1320.
Schmidhuber Sepp Hochreiter; Jürgen. Long short-term memory // Neural Computation. 1997.
Schmitt Simon, Hessel Matteo, Simonyan Karen. Off-policy actor-critic with shared experience replay // International Conference on Machine Learning. 2020. 8545­8554.
Schulman John, Chen Xi, Abbeel Pieter. Equivalence between policy gradients and soft q-learning // arXiv preprint arXiv:1704.06440. 2017a.
Schulman John, Wolski Filip, Dhariwal Prafulla, Radford Alec, Klimov Oleg. Proximal policy optimization algorithms // arXiv preprint arXiv:1707.06347. 2017b.
Song H. Francis, Abdolmaleki Abbas, Springenberg Jost Tobias, Clark Aidan, Soyer Hubert, Rae Jack W., Noury Seb, Ahuja Arun, Liu Siqi, Tirumala Dhruva, Heess Nicolas, Belov Dan, Riedmiller Martin, Botvinick Matthew M. V-MPO: On-Policy Maximum a Posteriori Policy Optimization for Discrete and Continuous Control // International Conference on Learning Representations. 2020.
Song Zhao, Parr Ron, Carin Lawrence. Revisiting the softmax bellman operator: New benefits and new perspective // International Conference on Machine Learning. 2019. 5916­5925.
Sutton Richard S, Barto Andrew G. Reinforcement learning: An introduction. 2018.
Tokic Michel. Adaptive -greedy exploration in reinforcement learning based on value differences // Annual Conference on Artificial Intelligence. 2010. 203­210.
Toromanoff Marin, Wirbel Emilie, Moutarde Fabien. Is deep reinforcement learning really superhuman on atari? leveling the playing field // arXiv preprint arXiv:1908.04683. 2019.
Wang Ziyu, Schaul Tom, Hessel Matteo, Hasselt Hado, Lanctot Marc, Freitas Nando. Dueling network architectures for deep reinforcement learning // International conference on machine learning. 2016. 1995­2003.
Williams Ronald J, Peng Jing. Function optimization using connectionist reinforcement learning algorithms // Connection Science. 1991. 3, 3. 241­268.
10

A Bandit Vote Algorithm
The algorithm is shown in Algorithm 2.

Algorithm 2 Bandits Vote Algorithm.
for m = 1, ..., M do Sample mode  {argmax, random}, sample lr, sample width. Initialize Bm = Bandit(mode, l, r, acc, width, lr, w, N, d).
end for while T rue do
for m = 1, ..., M do Evaluate Bm by (9). Sample candidates cm,1, ..., cm,d by mode and (11) from Bm.
end for Sample x from {cm,i}m=1,...,M;i=1,...,d. Execute x and receive g. for m = 1, ..., M do
Update Bm with (x, g) by (10). end for end while

Let's firstly define a bandit as B = Bandit(mode, l, r, acc, width, lr, w, N, d).

· mode is the mode of sampling, with two choices, argmax and random. · l is the left boundary of B, and each x is clipped to x = max{x, l}. · r is the right boundary of B, and each x is clipped to x = min{x, r}. · acc is the accuracy, where each x is located in the (min{max{x, l}, r} - l)/acc th tile. · width is the tile coding width, where the value of the ith tile is estimated by the average of
{wi-width, ..., wi+width}. · lr is the learning rate. · w is a vector in R (r-l)/acc , which represents the weight of each tile. · N is a vector in R (r-l)/acc , which counts the number of sampling of each tile. · d is an integer, which represents how many candidates is provided by B when sampling.

During the evaluation process, we evaluate the value of the ith tile by

1 Vi = 2  width + 1

i+width

wk .3

(9)

k=i-width

During the training process, for each sample (x, g), where g is the target value. Since x locates in the ith tile, we update B by

i = (min{max{x, l}, r} - l)/acc ,



wj  wj + lr  (g - Vi) , j = i - width, ..., i + width,

(10)

Ni  Ni + 1.

During the sampling process, we firstly evaluate B by (9) and get (V1, ..., V (r-l)/acc ). We calculate the score of ith tile by

scorei

=

Vi - µ({Vj }j=1,..., (r-l)/acc ({Vj }j=1,..., (r-l)/acc )

)

+c·

log(1 + j Nj) . 1 + Ni

(11)

For different modes, we sample the candidates by the following mechanism,

3For the boundary case, we change the summation indexes and the denominator accordingly.

11

· if mode = argmax, find tiles with top-d scores, then sample d candidates from these tiles, one uniformly from a tile;
· if mode = random, sample d tiles with scores as the logits without replacement, then sample d candidates from these tiles, one uniformly from a tile;

In practice, we define a set of bandits {Bm}m=1,...,M . At each step, we sample d candidates

{cm,i}i=1,...,d from each Bm, so we have a set of m × d candidates {cm,i}m=1,...,M;i=1,...,d. Then

we sample uniformly from these m × d candidates to get x. At last, we transform the selected x to



by



=

1 exp(x)-1

.

When

we

receive

(, g),

we transform



to

x

by

x

=

log(1 + 1/ ).

Then

we

update each Bm by (10).

12

B Full Algorithm
Algorithm 3 DiCE Algorithm full version. Initialize Parameter Server (PS) and Data Collector (DC).
// LEARNER Initialize dpush. Initialize , i.e. initialize A, V, Q = V + A. Initialize   exp(A/ ). Initialize count = 0. while T rue do
Load data from DC. Estimate Q and V  by proper off-policy algorithms.
(For instance, ReTrace (2) for Q and V-Trace (4) for V .) Update  by policy gradient (8) and policy evaluation (3), (5). if count mod dpush = 0 then
Push  to PS. end if count  count + 1. end while
// ACTOR Initialize dpull, M , . Initialize , i.e. initialize A, V,Q = V + A and . Initialize {Bm}m=1,...,M and sample   . Initialize count = 0, G = 0. Initialize environment and s = s0. while T rue do
Calculate  (·|s)  exp(A/ ). Sample a   (·|s). s, r, done  p(·|s, a). G  G + r. if done then
Update  by (, G). Send data to DC. Sample   . G  0. Reset the environment and s = s0. end if if count mod dpull = 0 then Pull  from PS and update . end if count  count + 1. end while
In practice, to save memory usage, we pre-send data back as long as the trajectory length surpasses recurrent sequence length.
13

C Hyperparameters

Parameter

Value

Image Size Grayscale Num. Action Repeats Num. Frame Stacks Action Space End of Episode When Life Lost Num. States Sample Reuse Num. Environments Reward Shape Reward Clip Intrinsic Reward Random No-ops Burn-in Seq-length Burn-in Stored Recurrent State Bootstrap Batch size Discount () V -loss Scaling () Q-loss Scaling () -loss Scaling () Entropy Regularization Importance Sampling Clip c¯ Importance Sampling Clip ¯ Backbone LSTM Units Optimizer Weight Decay Rate Weight Decay Schedule Learning Rate Warmup Steps Learning Rate Schedule AdamW 1 AdamW 2 AdamW AdamW Clip Norm Learner Push Model Every n Steps Actor Pull Model Every n Steps Num. Bandits Bandit Learning Rate Bandit Tiling Width Num. Bandit Candidates Bandit Value Normalization Bandit UCB Scaling Bandit Search Range for 1/

(84, 84) Yes 4 4 Full No 200M 2 160 log(abs(r) + 1.0) · (2 · 1{r0} - 1{r<0}) No No 30 40 80 Yes Yes 64 0.997 1.0 10.0 10.0 No 1.05 1.05 IMPALA,deep 256 Adam Weight Decay 0.01 Anneal linearly to 0 5e-4 4000 Anneal linearly to 0 0.9 0.98 1e-6 50.0 25 64 7 Uniform([0.05, 0.1, 0.2]) Uniform([1, 2, 3]) 7 Yes 1.0 [0.0, 50.0]

Table 5: Hyperparameters for Atari Experiments.

14

D Comparison to Baseline

Games
Scale
alien amidar assault asterix asteroids atlantis bank heist battle zone beam rider berzerk bowling boxing breakout centipede chopper command crazy climber defender demon attack double dunk enduro fishing derby freeway frostbite gopher gravitar
hero ice hockey jamesbond kangaroo
krull kung fu master montezuma revenge
ms pacman name this game
phoenix pitfall pong private eye qbert riverraid road runner robotank seaquest skiing solaris space invaders star gunner surround tennis time pilot tutankham up n down venture video pinball wizard of wor yars revenge zaxxon MEAN HNS(%) MEDIAN HNS(%)

RND
227.8 5.8 222.4 210 719 12850 14.2 236 363.9 123.7 23.1 0.1 1.7 2090.9 811 10780.5 2874.5 152.1 -18.6 0 -91.7 0 65.2 257.6 173 1027 -11.2 29 52 1598 258.5 0 307.3 2292.3 761.5 -229.4 -20.7 24.9 163.9 1338.5 11.5 2.2 68.4 -17098 1236.3 148 664 -10 -23.8 3568 11.4 533.4 0 0 563.5 3092.9 32.5 0.00 0.00

HUMAN
7127.8 1719.5
742 8503.3 47388.7 29028.1 753.1 37187.5 16926.5 2630.4 160.7
12.1 30.5 12017 7387.8 36829.4 18688.9 1971 -16.4 860.5 -38.8 29.6 4334.7 2412.5 3351.4 30826.4 0.9 302.8 3035 2665.5 22736.3 4753.3 6951.6 8049 7242.6 6463.7 14.6 69571.3 13455.0 17118.0 7845 11.9 42054.7 -4336.9 12326.7 1668.7 10250 6.5 -8.3 5229.2 167.6 11693.2 1187.5 17667.9 4756.5 54576.9 9173.3 100.00 100.00

BASELINE
200M
13720 560
16228 213580 18621 3211600 895.3 70137 34920
1648 162.4 98.3 624.3 102600 616690 161250 421600 291590 20.25 10019 53.24 3.46 1583 188680 4311 24236 1.56 12468 5399 64347 124630.1 2488.4 7579 32098 498590 -17.8 20.39 134.1 21043 11182 251360 10.44 11862 -12730 2319 3031 337150 -10 -21.05 84341 381 416020
0 297920 26008 76903.5 46070.8

HNS(%)
195.54 32.34 3080.37 2572.80 38.36 19772.10 119.24 189.17 208.64 60.81 101.24 818.33 2161.81 1012.57 9364.42 600.70 2647.75 16022.76 1765.91 1164.32 273.99 11.69 35.55 8743.90 130.19 77.88 105.45 4543.10 179.25 5878.13 553.31 52.35 109.44 517.76 7681.23 3.16 116.40 0.16 157.09 62.38 3208.64 84.95 28.09 34.23 9.76 189.58 3510.18 0.00 17.74 4862.62 236.62 3723.06 0.00 1686.22 606.83 143.37 503.66 1929.95 195.54

DiCE
200M
10641 653.9 36251 851210 759170 3670700 1381 130410 104030 1222 176.4 99.9 696 38938 41495 157250 837750 549450
23 14317 48.8 33.7 8102 454150 6150 17655 -8.1 567020 14286 11104 1270800 2528 4296 30037 597580 -21.8
21 15095 19091 17081 57102 69.7 2728 -9327 3653 105810 358650 -9.8 23.7 150930 380.3 907170 1969 673840 21325 84684 62133

HNS(%)
150.92 37.82 6933.91 10261.30 1625.15 22609.89 184.98 352.28 625.90 43.81 111.41 831.67 2410.76 371.21 618.60 584.73 5279.21 30199.46 1890.91 1663.80 265.60 113.85 188.24 21063.27 188.05 55.80 25.62 207082.18 477.17 890.49 5652.43 53.18 60.03 481.95 9208.60 3.10 118.13 21.67 142.40 99.77 728.80 695.88 6.33 60.90 21.79 6948.25 3734.47 1.21 306.45 8871.35 236.17 8124.13 165.81 3813.92 495.15 158.48 679.38 6456.63 477.17

15

Games
Scale
alien amidar assault asterix asteroids atlantis bank heist battle zone beam rider berzerk bowling boxing breakout centipede chopper command crazy climber defender demon attack double dunk enduro fishing derby freeway frostbite gopher gravitar hero ice hockey jamesbond kangaroo krull kung fu master montezuma revenge ms pacman name this game phoenix pitfall pong private eye qbert riverraid road runner robotank seaquest skiing solaris space invaders star gunner surround tennis time pilot tutankham up n down venture video pinball wizard of wor yars revenge zaxxon MEAN SABER(%) MEDIAN SABER(%)

RND
227.8 5.8 222.4 210 719 12850 14.2 236 363.9 123.7 23.1 0.1 1.7 2090.9 811 10780.5 2874.5 152.1 -18.6 0 -91.7 0 65.2 257.6 173 1027 -11.2 29 52 1598 258.5 0 307.3 2292.3 761.5 -229.4 -20.7 24.9 163.9 1338.5 11.5 2.2 68.4 -17098 1236.3 148 664 -10 -23.8 3568 11.4 533.4 0 0 563.5 3092.9 32.5 0.00 0.00

HWR
251916 104159 8647 1000000 10506650 10604840 82058 801000 999999 1057940
300 100 864 1301709 999999 219900 6010500 1556345 21 9500 71 38 454830 355040 162850 1000000 36 45550 1424600 104100 1000000 1219200 290090 25220 4014440 114000 21 101800 2400000 1000000 2038100 76 999999 -3272 111420 621535 77400 9.6 21 65300 5384 82840 38900 89218328 395300 15000105 83700 100.00 100.00

BASELINE
200M
13729 560
16228 213580 18621 3211600 895.3 70137 34920
1648 162.4 98.3 624.3 102600 616690 161250 421600 291590 20.25 10019 53.24 3.46 1583 188680 4311 24236 1.56 12468 5399 64347 124630.1 2488.4 7579 32098 498590 -17.8 20.39 134.1 21043 11182 251360 10.44 11862 -12730 2319 3031 337150 -10 -21.05 84341 381 416020
0 297920 26008 76903.5 46070.8

SABER(%)
5.36 0.53 189.99 21.34 0.17 30.20 1.07 8.73 3.46 0.14 50.31 98.30 72.20 7.73 61.64 71.95 6.97 18.73 98.11 105.46 89.08 9.11 0.33 53.11 2.54 2.32 27.03 27.33 0.38 61.22 12.44 0.20 2.51 130.00 12.40 0.19 98.54 0.11 0.87 0.99 12.33 11.17 1.18 31.59 0.98 0.46 200.00 0.00 6.14 130.84 6.88 200.00 0.00 0.33 6.45 0.49 55.03 35.91 8.73

DiCE
200M
10641 653.9 36251 851210 759170 3670700 1381 130410 104030 1222 176.4 99.9 696 38938 41495 157250 837750 549450
23 14317 48.8 33.7 8102 454150 6150 17655 -8.1 567020 14286 11104 1270800 2528 4296 30037 597580 -21.8
21 15095 19091 17081 57102 69.7 2728 -9327 3653 105810 358650 -9.8 23.7 150930 380.3 907170 1969 673840 21325 84684 62133

SABER(%)
4.14 0.62 200.00 85.12 7.22 34.53 1.67 16.26 10.37 0.10 55.36 99.90 80.52 2.84 4.07 70.04 13.90 35.30 105.05 150.71 86.36 88.68 1.77 127.94 3.67 1.66 6.57 200.00 1.00 9.27 127.09 0.21 1.38 121.01 14.87 0.18 100.00 14.81 0.79 1.58 2.80 91.46 0.27 56.21 2.19 17.00 200.00 1.02 106.03 200.00 6.87 200.00 5.06 0.76 5.26 0.54 74.22 50.11 13.90

16

E Atari Results
E.1 Atari Games Table of Scores Based on Human Average Records
Random scores and average human's scores are from (Badia et al., 2020a). Rainbow's scores are from (Hessel et al., 2017). IMPALA's scores are from (Espeholt et al., 2018). LASER's scores are from (Schmitt et al., 2020), no sweep at 200M. As there are many versions of R2D2 and NGU, we use original papers'. R2D2's scores are from (Kapturowski et al., 2018). NGU's scores are from (Badia et al., 2020b). Agent57's scores are from (Badia et al., 2020a).

Games

RND HUMAN RAINBOW HNS(%) IMPALA HNS(%) LASER HNS(%) DiCE HNS(%)

Scale

200M

200M

200M

200M

alien amidar assault asterix asteroids atlantis bank heist battle zone beam rider berzerk bowling boxing breakout centipede chopper command crazy climber defender demon attack double dunk enduro fishing derby freeway frostbite gopher gravitar
hero ice hockey jamesbond kangaroo
krull kung fu master montezuma revenge
ms pacman name this game
phoenix pitfall pong private eye qbert riverraid road runner robotank seaquest skiing solaris space invaders star gunner surround tennis time pilot tutankham up n down venture video pinball wizard of wor yars revenge zaxxon MEAN HNS(%) MEDIAN HNS(%)

227.8 5.8
222.4 210 719 12850 14.2 236 363.9 123.7 23.1 0.1 1.7 2090.9 811 10780.5 2874.5 152.1 -18.6
0 -91.7
0 65.2 257.6 173 1027 -11.2 29 52 1598 258.5
0 307.3 2292.3 761.5 -229.4 -20.7 24.9 163.9 1338.5 11.5 2.2 68.4 -17098 1236.3 148 664 -10 -23.8 3568 11.4 533.4
0 0 563.5 3092.9 32.5 0.00 0.00

7127.8 1719.5
742 8503.3 47388.7 29028.1 753.1 37187.5 16926.5 2630.4 160.7 12.1
30.5 12017 7387.8 36829.4 18688.9 1971 -16.4 860.5 -38.8 29.6 4334.7 2412.5 3351.4 30826.4
0.9 302.8 3035 2665.5 22736.3 4753.3 6951.6 8049 7242.6 6463.7 14.6 69571.3 13455.0 17118.0 7845 11.9 42054.7 -4336.9 12326.7 1668.7 10250 6.5 -8.3 5229.2 167.6 11693.2 1187.5 17667.9 4756.5 54576.9 9173.3 100.00 100.00

9491.7 5131.2 14198.5 428200 2712.8 826660 1358 62010 16850.2 2545.6
30 99.6 417.5 8167.3 16654 168788.5 55105 111185 -0.3 2125.9 31.3 34 9590.5 70354.6 1419.3 55887.4 1.1 19809 14637.5 8741.5 52181 384 5380.4 13136 108529
0 20.9 4234 33817.5 22920.8 62041 61.4 15898.9 -12957.8 3560.3 18789 127029 9.7
0 12926 241 125755
5.5 533936.5 17862.5 102557 22209.5

134.26 15962.1 228.03 35565.9 512.15 10641 150.92

299.08 1554.79 90.39 1829.2 106.4 653.9 37.82

2689.78 19148.47 3642.43 21560.4 4106.62 36251 6933.91

5160.67 300732 3623.67 240090 2892.46 851210 10261.30

4.27 108590.05 231.14 213025 454.91 759170 1625.15

5030.32 849967.5 5174.39 841200 5120.19 3670700 22609.89

181.86 1223.15 163.61 569.4 75.14 1381 184.98

167.18 20885 55.88 64953.3 175.14 130410 352.28

99.54 32463.47 193.81 90881.6 546.52 104030 625.90

96.62 1852.7 68.98 25579.5 1015.51 1222 43.81

5.01 59.92 26.76 48.3 18.31 176.4 111.41

829.17 99.96 832.17 100 832.5 99.9 831.67

1443.75 787.34 2727.92 747.9 2590.97 696 2410.76

61.22 11049.75 90.26 292792 2928.65 38938 371.21

240.89 28255 417.29 761699 11569.27 41495 618.60

630.80 136950 503.69 167820 626.93 157250 584.73

330.27 185203 1152.93 336953 2112.50 837750 5279.21

6104.40 132826.98 7294.24 133530 7332.89 549450 30199.46

831.82 -0.33 830.45 14 1481.82 23 1890.91

247.05

0

0.00

0

0.00 14317 1663.80

232.51 44.85 258.13 45.2 258.79 48.8 265.60

114.86

0

0.00

0

0.00 33.7 113.85

223.10 317.75 5.92 5083.5 117.54 8102 188.24

3252.91 66782.3 3087.14 114820.7 5316.40 454150 21063.27

39.21 359.5 5.87 1106.2 29.36 6150 188.05

184.10 33730.55 109.75 31628.7 102.69 17655 55.80

101.65 3.48 121.32 17.4 236.36 -8.1

25.62

72.24 601.5 209.09 37999.8 13868.08 567020 207082.18

488.05 1632 52.97 14308 477.91 14286 477.17

669.18 8147.4 613.53 9387.5 729.70 11104 890.49

230.99 43375.5 191.82 607443 2701.26 1270800 5652.43

8.08

0

0.00

0.3

0.01 2528 53.18

76.35 7342.32 105.88 6565.5 94.19 4296 60.03

188.37 21537.2 334.30 26219.5 415.64 30037 481.95

1662.80 210996.45 3243.82 519304 8000.84 597580 9208.60

3.43

-1.66

3.40

-0.6

3.42 -21.8

3.10

117.85 20.98 118.07 21

118.13 21

118.13

6.05

98.5

0.11 96.3

0.10 15095 21.67

253.20 351200.12 2641.14 21449.6 160.15 19091 142.40

136.77 29608.05 179.15 40362.7 247.31 17081 99.77

791.85 57121 729.04 45289 578.00 57102 728.80

610.31 12.96 110.93 62.1 617.53 69.7 695.88

37.70 1753.2 4.01 2890.3 6.72 2728

6.33

32.44 -10180.38 54.21 -29968.4 -100.86 -9327 60.90

20.96 2365 10.18 2273.5 9.35 3653 21.79

1225.82 43595.78 2857.09 51037.4 3346.45 105810 6948.25

1318.22 200625 2085.97 321528 3347.21 358650 3734.47

119.39 7.56 106.42 8.4 111.52 -9.8

1.21

153.55 0.55 157.10 12.2 232.26 23.7 306.45

563.36 48481.5 2703.84 105316 6125.34 150930 8871.35

146.99 292.11 179.71 278.9 171.25 380.3 236.17

1122.08 332546.75 2975.08 345727 3093.19 907170 8124.13

0.46

0

0.00

0

0.00 1969 165.81

3022.07 572898.27 3242.59 511835 2896.98 673840 3813.92

412.57 9157.5 204.96 29059.3 679.60 21325 495.15

193.19 84231.14 157.60 166292.3 316.99 84684 158.48

242.62 32935.5 359.96 41118 449.47 62133 679.38

873.97

957.34

1741.36

6456.63

230.99

191.82

454.91

477.17

17

Games

R2D2 HNS(%) NGU HNS(%) AGENT57 HNS(%) DiCE HNS(%)

Scale

10B

35B

100B

200M

alien

109038.4 1576.97 248100 3592.35 297638.17 4310.30 10641 150.92

amidar

27751.24 1619.04 17800 1038.35 29660.08 1730.42 653.9 37.82

assault

90526.44 17379.53 34800 6654.66 67212.67 12892.66 36251 6933.91

asterix

999080 12044.30 950700 11460.94 991384.42 11951.51 851210 10261.30

asteroids

265861.2 568.12 230500 492.36 150854.61 321.70 759170 1625.15

atlantis

1576068 9662.56 1653600 10141.80 1528841.76 9370.64 3670700 22609.89

bank heist

46285.6 6262.20 17400 2352.93 23071.5 3120.49 1381 184.98

battle zone

513360 1388.64 691700 1871.27 934134.88 2527.36 130410 352.28

beam rider 128236.08 772.05 63600 381.80 300509.8 1812.19 104030 625.90

berzerk

34134.8 1356.81 36200 1439.19 61507.83 2448.80 1222 43.81

bowling

196.36 125.92 211.9 137.21 251.18 165.76 176.4 111.41

boxing

99.16 825.50 99.7 830.00

100

832.50 99.9 831.67

breakout

795.36 2755.76 559.2 1935.76 790.4 2738.54 696 2410.76

centipede

532921.84 5347.83 577800 5799.95 412847.86 4138.15 38938 371.21

chopper command 960648 14594.29 999900 15191.11 999900 15191.11 41495 618.60

crazy climber 312768 1205.59 313400 1208.11 565909.85 2216.18 157250 584.73

defender

562106 3536.22 664100 4181.16 677642.78 4266.80 837750 5279.21

demon attack 143664.6 7890.07 143500 7881.02 143161.44 7862.41 549450 30199.46

double dunk

23.12 1896.36 -14.1 204.55 23.93 1933.18 23 1890.91

enduro

2376.68 276.20 2000 232.42 2367.71 275.16 14317 1663.80

fishing derby

81.96 328.28 32 233.84 86.97 337.75 48.8 265.60

freeway

34

114.86 28.5 96.28

32.59 110.10 33.7 113.85

frostbite

11238.4 261.70 206400 4832.76 541280.88 12676.32 8102 188.24

gopher

122196 5658.66 113400 5250.47 117777.08 5453.59 454150 21063.27

gravitar

6750 206.93 14200 441/32 19213.96 599.07 6150 188.05

hero

37030.4 120.82 69400 229.44 114736.26 381.58 17655 55.80

ice hockey

71.56 683.97 -4.1 58.68

63.64 618.51 -8.1

25.62

jamesbond

23266 8486.85 26600 9704.53 135784.96 49582.16 567020 207082.18

kangaroo

14112 471.34 35100 1174.92 24034.16 803.96 14286 477.17

krull

145284.8 13460.12 127400 11784.73 251997.31 23456.61 11104 890.49

kung fu master 200176 889.40 212100 942.45 206845.82 919.07 1270800 5652.43

montezuma revenge 2504 52.68 10400 218.80 9352.01 196.75 2528 53.18

ms pacman

29928.2 445.81 40800 609.44 63994.44 958.52 4296 60.03

name this game 45214.8 745.61 23900 375.35 54386.77 904.94 30037 481.95

phoenix

811621.6 125.11 959100 14786.66 908264.15 14002.29 597580 9208.60

pitfall

0

3.43 7800 119.97 18756.01 283.66 -21.8

3.10

pong

21

118.13 19.6 114.16 20.67 117.20 21

118.13

private eye

300

0.40 100000 143.75 79716.46 114.59 15095 21.67

qbert

161000 1210.10 451900 3398.79 580328.14 4365.06 19091 142.40

riverraid

34076.4 207.47 36700 224.10 63318.67 392.79 17081 99.77

road runner

498660 6365.59 128600 1641.52 243025.8 3102.24 57102 728.80

robotank

132.4 1342.27 9.1

71.13 127.32 1289.90 69.7 695.88

seaquest

999991.84 2381.55 1000000 2381.57 999997.63 2381.56 2728

6.33

skiing

-29970.32 -100.87 -22977.9 -46.08 -4202.6 101.05 -9327 60.90

solaris

4198.4 26.71 4700 31.23 44199.93 387.39 3653 21.79

space invaders

55889 3665.48 43400 2844.22 48680.86 3191.48 105810 6948.25

star gunner

521728 5435.68 414600 4318.13 839573.53 8751.40 358650 3734.47

surround

9.96 120.97 -9.6

2.42

9.5

118.18 -9.8

1.21

tennis

24

308.39 10.2 219.35 23.84 307.35 23.7 306.45

time pilot

348932 20791.28 344700 20536.51 405425.31 24192.24 150930 8871.35

tutankham

393.64 244.71 191.1 115.04 2354.91 1500.33 380.3 236.17

up n down

542918.8 4860.17 620100 5551.77 623805.73 5584.98 907170 8124.13

venture

1992 167.75 1700 143.16 2623.71 220.94 1969 165.81

video pinball 483569.72 2737.00 965300 5463.58 992340.74 5616.63 673840 3813.92

wizard of wor 133264 3164.81 106200 2519.35 157306.41 3738.20 21325 495.15

yars revenge 918854.32 1778.73 986000 1909.15 998532.37 1933.49 84684 158.48

zaxxon

181372 1983.85 111100 1215.07 249808.9 2732.54 62133 679.38

MEAN HNS(%)

3374.31

3169.90

4763.69

6456.63

MEDIAN HNS(%)

1342.27

1208.11

1933.49

477.17

18

E.2 Atari Games Table of Scores Based on SABER Human World Records (HWR) are from (Toromanoff et al., 2019).

Games

RND HWR RAINBOW SABER(%) IMPALA SABER(%) LASER SABER(%) DiCE SABER(%)

Scale

200M

200M

200M

200M

alien

227.8 251916 9491.7

amidar

5.8 104159 5131.2

assault

222.4 8647 14198.5

asterix

210 1000000 428200

asteroids

719 10506650 2712.8

atlantis

12850 10604840 826660

bank heist

14.2 82058 1358

battle zone

236 801000 62010

beam rider

363.9 999999 16850.2

berzerk

123.7 1057940 2545.6

bowling

23.1 300

30

boxing

0.1 100

99.6

breakout

1.7 864

417.5

centipede

2090.9 1301709 8167.3

chopper command 811 999999 16654

crazy climber 10780.5 219900 168788.5

defender

2874.5 6010500 55105

demon attack

152.1 1556345 111185

double dunk

-18.6 21

-0.3

enduro

0

9500 2125.9

fishing derby

-91.7 71

31.3

freeway

0

38

34

frostbite

65.2 454830 9590.5

gopher

257.6 355040 70354.6

gravitar

173 162850 1419.3

hero

1027 1000000 55887.4

ice hockey

-11.2 36

1.1

jamesbond

29 45550 19809

kangaroo

52 1424600 14637.5

krull

1598 104100 8741.5

kung fu master 258.5 1000000 52181

montezuma revenge 0 1219200 384

ms pacman

307.3 290090 5380.4

name this game 2292.3 25220 13136

phoenix

761.5 4014440 108529

pitfall

-229.4 114000

0

pong

-20.7 21

20.9

private eye

24.9 101800 4234

qbert

163.9 2400000 33817.5

riverraid

1338.5 1000000 22920.8

road runner

11.5 2038100 62041

robotank

2.2

76

61.4

seaquest

68.4 999999 15898.9

skiing

-17098 -3272 -12957.8

solaris

1236.3 111420 3560.3

space invaders

148 621535 18789

star gunner

664 77400 127029

surround

-10

9.6

9.7

tennis

-23.8 21

0

time pilot

3568 65300 12926

tutankham

11.4 5384

241

up n down

533.4 82840 125755

venture

0 38900

5.5

video pinball

0 89218328 533936.5

wizard of wor 563.5 395300 17862.5

yars revenge 3092.9 15000105 102557

zaxxon

32.5 83700 22209.5

MEAN SABER(%) 0.00 100.00

MEDIAN SABER(%) 0.00 100.00

3.68 4.92 165.90 42.81 0.02 7.68 1.64 7.71 1.65 0.23 2.49 99.60 48.22 0.47 1.59 75.56 0.87 7.13 46.21 22.38 75.60 89.47 2.09 19.76 0.77 5.49 26.06 43.45 1.02 6.97 5.19 0.03 1.75 47.30 2.69 0.20 99.76 4.14 1.40 2.16 3.04 80.22 1.58 29.95 2.11 3.00 164.67 100.51 53.13 15.16 4.27 152.14 0.01 0.60 4.38 0.66 26.51 28.39 4.92

15962.1 1554.79 19148.47 300732 108590.05 849967.5 1223.15 20885 32463.47 1852.7 59.92
99.96 787.34 11049.75 28255 136950 185203 132826.98 -0.33
0 44.85
0 317.75 66782.3 359.5 33730.55 3.48 601.5 1632 8147.4 43375.5
0 7342.32 21537.2 210996.45
-1.66 20.98 98.5 351200.12 29608.05 57121 12.96 1753.2 -10180.38 2365 43595.78 200625 7.56 0.55 48481.5 292.11 332546.75
0 572898.27
9157.5 84231.14 32935.5

6.25 1.49 200.00 30.06 1.03 7.90 1.47 2.58 3.21 0.16 13.30 99.96 91.11 0.69 2.75 60.33 3.03 8.53 46.14 0.00 83.93 0.00 0.06 18.75 0.11 3.27 31.10 1.26 0.11 6.39 4.31 0.00 2.43 83.94 5.24 0.20 99.95 0.07 14.63 2.83 2.80 14.58 0.17 50.03 1.02 6.99 200.00 89.59 54.35 72.76 5.22 200.00 0.00 0.64 2.18 0.54 39.33 29.45 4.31

976.51 1829.2 21560.4 240090 213025 841200 569.4 64953.3 90881.6 25579.5 48.3
100 747.9 292792 761699 167820 336953 133530
14 0 45.2 0 5083.5 114820.7 1106.2 31628.7 17.4 37999.8 14308 9387.5 607443 0.3 6565.5 26219.5 519304 -0.6 21 96.3 21449.6 40362.7 45289 62.1 2890.3 -29968.4 2273.5 51037.4 321528 8.4 12.2 105316 278.9 345727 0 511835 29059.3 166292.3 41118

14.04 1.75 200.00 23.99 2.02 7.82 0.68 8.08 9.06 2.41 9.10 100.00 86.54 22.37 76.15 75.10 5.56 8.57 82.32 0.00 84.14 0.00 1.10 32.29 0.57 3.06 60.59 83.41 1.00 7.60 60.73 0.00 2.16 104.36 12.92 0.20 100.00 0.07 0.89 3.91 2.22 81.17 0.28 -93.09 0.94 8.19 200.00 93.88 80.36 164.82 4.98 200.00 0.00 0.57 7.22 1.09 49.11 36.78 8.08

10641 653.9 36251 851210 759170 3670700 1381 130410 104030 1222 176.4 99.9 696 38938 41495 157250 837750 549450
23 14317 48.8 33.7 8102 454150 6150 17655 -8.1 567020 14286 11104 1270800 2528 4296 30037 597580 -21.8
21 15095 19091 17081 57102 69.7 2728 -9327 3653 105810 358650 -9.8 23.7 150930 380.3 907170 1969 673840 21325 84684 62133

4.14 0.62 200.00 85.12 7.22 34.53 1.67 16.26 10.37 0.10 55.36 99.90 80.52 2.84 4.07 70.04 13.90 35.30 105.05 150.71 86.36 88.68 1.77 127.94 3.67 1.66 6.57 200.00 1.00 9.27 127.09 0.21 1.38 121.01 14.87 0.18 100.00 14.81 0.79 1.58 2.80 91.46 0.27 56.21 2.19 17.00 200.00 1.02 106.03 200.00 6.87 200.00 5.06 0.76 5.26 0.54 74.22 50.11 13.90

19

Games

R2D2 SABER(%) NGU SABER(%) AGENT57 SABER(%) DiCE SABER(%)

Scale

10B

35B

100B

200M

alien

109038.4

amidar

27751.24

assault

90526.44

asterix

999080

asteroids

265861.2

atlantis

1576068

bank heist

46285.6

battle zone

513360

beam rider

128236.08

berzerk

34134.8

bowling

196.36

boxing

99.16

breakout

795.36

centipede

532921.84

chopper command 960648

crazy climber

312768

defender

562106

demon attack 143664.6

double dunk

23.12

enduro

2376.68

fishing derby

81.96

freeway

34

frostbite

11238.4

gopher

122196

gravitar

6750

hero

37030.4

ice hockey

71.56

jamesbond

23266

kangaroo

14112

krull

145284.8

kung fu master 200176

montezuma revenge 2504

ms pacman

29928.2

name this game 45214.8

phoenix

811621.6

pitfall

0

pong

21

private eye

300

qbert

161000

riverraid

34076.4

road runner

498660

robotank

132.4

seaquest

999991.84

skiing

-29970.32

solaris

4198.4

space invaders

55889

star gunner

521728

surround

9.96

tennis

24

time pilot

348932

tutankham

393.64

up n down

542918.8

venture

1992

video pinball 483569.72

wizard of wor

133264

yars revenge 918854.32

zaxxon

181372

MEAN SABER(%)

MEDIAN SABER(%)

43.23 26.64 200.00 99.91 2.52 14.76 56.40 64.08 12.79 3.22 62.57 99.16 92.04 40.85 96.06 144.41 9.31 9.22 105.35 25.02 106.74 89.47 2.46 34.37 4.04 3.60 175.34 51.05 0.99 140.18 20.00 0.21 10.22 187.21 20.20 0.20 100.00 0.27 6.70 3.28 24.47 176.42 100.00 -93.10 2.69 8.97 200.00 101.84 106.70 200.00 7.11 200.00 5.12 0.54 33.62 6.11 200.00 60.43 33.62

248100 17800 34800 950700 230500 1653600 17400 691700 63600 36200 211.9 99.7 559.2 577800 999900 313400 664100 143500 -14.1 2000
32 28.5 206400 113400 14200 69400 -4.1 26600 35100 127400 212100 10400 40800 23900 959100 7800 19.6 100000 451900 36700 128600 9.1 1000000 -22977.9 4700 43400 414600 -9.6 10.2 344700 191.1 620100 1700 965300 106200 986000 111100

98.48 17.08 200.00 95.07 2.19 15.49 21.19 86.35 6.33 3.41 68.18 99.70 64.65 44.30 99.99 144.71 11.01 9.21 11.36 21.05 76.03 75.00 45.37 31.89 8.62 6.84 15.04 58.37 2.46 122.73 21.19 0.85 13.97 94.24 23.88 7.03 96.64 98.23 18.82 3.54 6.31 9.35 100.00 -42.53 3.14 6.96 200.00 2.04 75.89 200.00 3.34 200.00 4.37 1.08 26.76 6.55 132.75 50.47 21.19

297638.17 29660.08 67212.67 991384.42 150854.61 1528841.76 23071.5 934134.88 300509.8 61507.83
251.18 100 790.4
412847.86 999900
565909.85 677642.78 143161.44
23.93 2367.71
86.97 32.59 541280.88 117777.08 19213.96 114736.26 63.64 135784.96 24034.16 251997.31 206845.82 9352.01 63994.44 54386.77 908264.15 18756.01 20.67 79716.46 580328.14 63318.67 243025.8 127.32 999997.63 -4202.6 44199.93 48680.86 839573.53
9.5 23.84 405425.31 2354.91 623805.73 2623.71 992340.74 157306.41 998532.37 249808.9

118.17 28.47 200.00 99.14 1.43 14.31 28.10 116.63 30.03 5.80 82.37 100.00 91.46 31.61 99.99 200.00 11.23 9.19 107.40 24.92 109.82 85.76 119.01 33.12 11.70 11.38 158.56 200.00 1.68 200.00 20.66 0.77 21.98 200.00 22.61 16.62 99.21 78.30 24.18 6.21 11.92 169.54 100.00 93.27 38.99 7.81 200.00 99.49 106.34 200.00 43.62 200.00 6.74 1.11 39.71 6.64 200.00 76.26 43.62

10641 653.9 36251 851210 759170 3670700 1381 130410 104030 1222 176.4 99.9 696 38938 41495 157250 837750 549450
23 14317 48.8 33.7 8102 454150 6150 17655 -8.1 567020 14286 11104 1270800 2528 4296 30037 597580 -21.8
21 15095 19091 17081 57102 69.7 2728 -9327 3653 105810 358650 -9.8 23.7 150930 380.3 907170 1969 673840 21325 84684 62133

4.14 0.62 200.00 85.12 7.22 34.53 1.67 16.26 10.37 0.10 55.36 99.90 80.52 2.84 4.07 70.04 13.90 35.30 105.05 150.71 86.36 88.68 1.77 127.94 3.67 1.66 6.57 200.00 1.00 9.27 127.09 0.21 1.38 121.01 14.87 0.18 100.00 14.81 0.79 1.58 2.80 91.46 0.27 56.21 2.19 17.00 200.00 1.02 106.03 200.00 6.87 200.00 5.06 0.76 5.26 0.54 74.22 50.11 13.90

20

1.8e+4 1.6e+4 1.4e+4 1.2e+4
1e+4 8e+3 6e+3 4e+3 2e+3
0 -2e+3
-20k 0 20k 40k 60k 80k 100k 120k 140k 16
1. alien

800 700 600 500 400 300 200 100
0 -100
-20k 0 20k 40k 60k 80k 100k 120k 140k 16
2. amidar

2e+4 1.8e+4 1.6e+4 1.4e+4 1.2e+4
1e+4 8e+3 6e+3 4e+3 2e+3
0 -2e+3
-20k 0 20k 40k 60k 80k 100k 120k 140k 16
3. assault

8e+5 7e+5 6e+5 5e+5 4e+5 3e+5 2e+5 1e+5
0 -1e+5
-20k 0

20k 40k 60k 80k 100k 120k 140k 16

4. asterix

2.4e+4 2.2e+4
2e+4 1.8e+4 1.6e+4 1.4e+4 1.2e+4
1e+4 8e+3 6e+3 4e+3 2e+3
0 -2e+3 -4e+3
-20k 0

20k 40k 60k 80k 100k 120k 140k 16

5. asteroids

4e+6 3.5e+6
3e+6 2.5e+6
2e+6 1.5e+6
1e+6 5e+5
0 -5e+5
-10k 0 10k 20k 30k 40k 50k 60k 70k 80k 90k100k11
6. atlantis

1.8e+3 1.6e+3 1.4e+3 1.2e+3
1e+3 800 600 400 200 0 -200 -20k 0 20k 40k 60k 80k 100k 120k 140k 16
7. bank_heist

2e+5 1.8e+5 1.6e+5 1.4e+5 1.2e+5
1e+5 8e+4 6e+4 4e+4 2e+4
0 -2e+4
-20k 0 20k 40k 60k 80k 100k 120k 140k 16
8. battle_zone

2e+5 1.8e+5 1.6e+5 1.4e+5 1.2e+5
1e+5 8e+4 6e+4 4e+4 2e+4
0 -2e+4
-20k 0 20k 40k 60k 80k 100k 120k 140k 16
9. beam_rider

2e+3 1.8e+3 1.6e+3 1.4e+3 1.2e+3
1e+3 800 600 400 200 0 -200 -20k 0 20k 40k 60k 80k 100k 120k 140k 16
10. berzerk

200 180 160 140 120 100
80 60 40 20
0 -20
-20k 0 20k 40k 60k 80k 100k 120k 140k 16
11. bowling

120 110 100
90 80 70 60 50 40 30 20 10
0 -10
-20k 0 20k 40k 60k 80k 100k 120k 140k 160k 18
12. boxing

800 700 600 500 400 300 200 100
0 -100
-20k 0

20k 40k 60k 80k 100k 120k 140k 16

13. breakout

1.4e+5 1.2e+5
1e+5 8e+4 6e+4 4e+4 2e+4
0 -2e+4
-20k 0 20k 40k 60k 80k 100k 120k 140k 16
14. centipede

6e+4 5.5e+4
5e+4 4.5e+4
4e+4 3.5e+4
3e+4 2.5e+4
2e+4 1.5e+4
1e+4 5e+3
0 -5e+3
-20k 0 20k 40k 60k 80k 100k 120k 140k 16
15. chopper_command

2.2e+5 2e+5
1.8e+5 1.6e+5 1.4e+5 1.2e+5
1e+5 8e+4 6e+4 4e+4 2e+4
0 -2e+4
-20k 0 20k 40k 60k 80k 100k 120k 140k 16
16. crazy_climber

5.5e+5 5e+5
4.5e+5 4e+5
3.5e+5 3e+5
2.5e+5 2e+5
1.5e+5 1e+5 5e+4 0 -5e+4 -20k 0 20k 40k 60k 80k 100k 120k 140k 16
17. defender

7e+5 6e+5 5e+5 4e+5 3e+5 2e+5 1e+5
0 -1e+5
-20k 0 20k 40k 60k 80k 100k 120k 140k 16
18. demon_attack

35 30 25 20 15 10
5 0 -5 -10 -15 -20 -25 -30 -20k 0

20k 40k 60k 80k 100k 120k 140k 16

19. double_dunk

1.4e+4 1.2e+4
1e+4 8e+3 6e+3 4e+3 2e+3
0 -2e+3
-20k 0 20k 40k 60k 80k 100k 120k 140k 16
20. enduro

21

80

5.5

60 40 20
0 -20

5 4.5
4 3.5
3 2.5

-40

2

-60

1.5

-80

1

-100

0.5

-120

0

-140

-0.5

-20k 0 20k 40k 60k 80k 100k 120k 140k 160

0

21. fishing_derby

20k 40k 60k 80k 100k 120k 140k 16
22. freeway

2e+3 1.8e+3 1.6e+3 1.4e+3 1.2e+3
1e+3 800 600 400 200 0 -200 -20k 0 20k 40k 60k 80k 100k 120k 140k 16
23. frostbite

2.4e+5 2.2e+5
2e+5 1.8e+5 1.6e+5 1.4e+5 1.2e+5
1e+5 8e+4 6e+4 4e+4 2e+4
0 -2e+4
-20k 0

20k 40k 60k 80k 100k 120k 140k 16

24. gopher

5.5e+3 5e+3
4.5e+3 4e+3
3.5e+3 3e+3
2.5e+3 2e+3
1.5e+3 1e+3 500 0 -500 -20k 0 20k 40k 60k 80k 100k120k140k160k18
25. gravitar

3e+4 2.5e+4
2e+4 1.5e+4
1e+4 5e+3
0 -5e+3
-20k 0 20k 40k 60k 80k 100k 120k 140k 16
26. hero

2 1 0 -1 -2 -3 -4 -5 -6 -7 -8 -9 -10 -11 -12 -20k 0

20k 40k 60k 80k 100k 120k 140k 16

27. ice_hockey

1.6e+4 1.4e+4 1.2e+4
1e+4 8e+3 6e+3 4e+3 2e+3
0 -2e+3
-20k 0 20k 40k 60k 80k 100k 120k 140k 16
28. jamesbond

7e+3 6e+3 5e+3 4e+3 3e+3 2e+3 1e+3
0 -1e+3
-20k 0 20k 40k 60k 80k 100k 120k 140k 16
29. kangaroo

9e+4 8e+4 7e+4 6e+4 5e+4 4e+4 3e+4 2e+4 1e+4
0 -1e+4
-20k 0 20k 40k 60k 80k 100k 120k 140k 16
30. krull

2e+6 1.8e+6 1.6e+6 1.4e+6 1.2e+6
1e+6 8e+5 6e+5 4e+5 2e+5
0 -2e+5
-20k 0 20k 40k 60k 80k 100k 120k 140k 16
31. kung_fu_master

3.5e+3 3e+3
2.5e+3 2e+3
1.5e+3 1e+3 500 0 -500 -20k 0

20k 40k 60k 80k 100k 120k 14

32. montezuma_revenge

5.5e+3 5e+3
4.5e+3 4e+3
3.5e+3 3e+3
2.5e+3 2e+3
1.5e+3 1e+3 500 0 -500 -20k 0 20k 40k 60k 80k 100k 120k 140k 16
33. ms_pacman

4e+4 3.5e+4
3e+4 2.5e+4
2e+4 1.5e+4
1e+4 5e+3
0 -5e+3
-20k 0 20k 40k 60k 80k 100k 120k 140k 16
34. name_this_game

6e+5 5.5e+5
5e+5 4.5e+5
4e+5 3.5e+5
3e+5 2.5e+5
2e+5 1.5e+5
1e+5 5e+4
0 -5e+4
-20k 0

20k 40k 60k 80k 100k 120k 140k 16

35. phoenix

40 20
0 -20 -40 -60 -80 -100 -120 -140 -160 -180 -200 -220
-20k 0 20k 40k 60k 80k 100k 120k 140k 16
36. pitfall

30 25 20 15 10
5 0 -5 -10 -15 -20 -25 -30
-20k 0

20k 40k 60k 80k 100k 120k 140k 16

37. pong

2e+4 1.8e+4 1.6e+4 1.4e+4 1.2e+4
1e+4 8e+3 6e+3 4e+3 2e+3
0 -2e+3 -4e+3 -6e+3
-10k 0 10k20k30k40k50k60k70k80k90k1001k101k201k3
38. private_eye

2.6e+4 2.4e+4 2.2e+4
2e+4 1.8e+4 1.6e+4 1.4e+4 1.2e+4
1e+4 8e+3 6e+3 4e+3 2e+3
0 -2e+3 -4e+3
-20k 0 20k 40k 60k 80k 100k 120k 140k 16
39. qbert

1.4e+4 1.2e+4
1e+4 8e+3 6e+3 4e+3 2e+3
0 -2e+3
-20k 0 20k 40k 60k 80k 100k120k140k160k18
40. riverraid

22

8e+4 7e+4 6e+4 5e+4 4e+4 3e+4 2e+4 1e+4
0 -1e+4
-20k 0 20k 40k 60k 80k 100k 120k 140k 16
41. road_runner

14 12 10
8 6 4 2 0 -2 -20k 0 20k 40k 60k 80k 100k 120k 140k 16
42. robotank

1.6e+4 1.4e+4 1.2e+4
1e+4 8e+3 6e+3 4e+3 2e+3
0 -2e+3
-20k 0 20k 40k 60k 80k 100k 120k 140k 16
43. seaquest

-6e+3 -8e+3 -1e+4 -1.2e+4 -1.4e+4 -1.6e+4 -1.8e+4 -2e+4 -2.2e+4 -2.4e+4 -2.6e+4 -2.8e+4 -3e+4 -3.2e+4 -3.4e+4 -3.6e+4
-20k 0 20k 40k 60k 80k 100k 120k 140k 16
44. skiing

2.9e+3 2.8e+3 2.7e+3 2.6e+3 2.5e+3 2.4e+3 2.3e+3 2.2e+3 2.1e+3
2e+3 1.9e+3
-20k 0

20k 40k 60k 80k 100k 120k 140k 16

45. solaris

1.6e+5 1.4e+5 1.2e+5
1e+5 8e+4 6e+4 4e+4 2e+4
0 -2e+4
-20k 0 20k 40k 60k 80k 100k 120k 140k 16
46. space_invader

4.5e+5 4e+5
3.5e+5 3e+5
2.5e+5 2e+5
1.5e+5 1e+5 5e+4 0 -5e+4 -20k 0 20k 40k 60k 80k 100k 120k 140k 16
47. star_gunner

-9.98 -9.98 -9.99 -9.99 -9.99 -9.99 -9.99
-10 -10 -10 -10 -10
-20k 0

20k 40k 60k 80k 100k120k140k160k18

48. surround

-20 -20.5
-21 -21.5
-22 -22.5
-23 -23.5
-24 -24.5
-25 -5k 0 5k 10k 15k 20k 25k 30k 35k 40k 45k 50
49. tennis

2.4e+5 2.2e+5
2e+5 1.8e+5 1.6e+5 1.4e+5 1.2e+5
1e+5 8e+4 6e+4 4e+4 2e+4
0 -2e+4
-20k 0

20k 40k 60k 80k 100k 120k 140k 16

50. time_pilot

500 450 400 350 300 250 200 150 100
50 0
-50 -20k 0 20k 40k 60k 80k 100k 120k 140k 16
51. tutankham

1.1e+6 1e+6 9e+5 8e+5 7e+5 6e+5 5e+5 4e+5 3e+5 2e+5 1e+5 0 -1e+5 -20k 0 20k 40k 60k 80k 100k 120k 140k 16
52. up_n_down

1.2 1
0.8 0.6 0.4 0.2
0 -0.2 -0.4 -0.6 -0.8
-1 -1.2
-20k 0 20k 40k 60k 80k 100k 120k 140k 16
53. venture

9e+5 8e+5 7e+5 6e+5 5e+5 4e+5 3e+5 2e+5 1e+5
0 -1e+5
-20k 0 20k 40k 60k 80k 100k 120k 140k 16
54. video_pinball

3.5e+4 3e+4
2.5e+4 2e+4
1.5e+4 1e+4 5e+3 0 -5e+3 -20k 0 20k 40k 60k 80k 100k 120k 140k 16
55. wizard_of_wor

1.2e+5 1.1e+5
1e+5 9e+4 8e+4 7e+4 6e+4 5e+4 4e+4 3e+4 2e+4 1e+4
0 -1e+4
-20k 0 20k 40k 60k 80k 100k 120k 140k 16
56. yars_revenge

6e+4 5.5e+4
5e+4 4.5e+4
4e+4 3.5e+4
3e+4 2.5e+4
2e+4 1.5e+4
1e+4 5e+3
0 -5e+3
-20k 0 20k 40k 60k 80k 100k 120k 140k 16
57. zaxxon

23

