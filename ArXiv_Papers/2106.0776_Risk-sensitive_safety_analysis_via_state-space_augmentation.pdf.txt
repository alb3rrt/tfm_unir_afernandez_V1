Risk-sensitive safety analysis via state-space augmentation*
Margaret P. Chapman, Michael Fauﬂ, H. Vincent Poor, Kevin M. Smith

arXiv:2106.00776v1 [eess.SY] 1 Jun 2021

Abstract-- Risk-sensitive safety analysis is a safety analysis method for stochastic systems on Borel spaces that uses a risk functional from finance called Conditional Value-at-Risk (CVaR). CVaR provides a particularly expressive way to quantify the safety of a control system, as it represents the average cost in a fraction of worst cases. In prior work, the notion of a risk-sensitive safe set was defined in terms of a non-standard optimal control problem, in which a maximum cost is assessed via CVaR. Here, we provide a method to compute risk-sensitive safe sets exactly in principle by utilizing a state-space augmentation technique. In addition, we prove the existence of an optimal pre-commitment policy under a measurable selection condition. The proposed framework assumes continuous system dynamics and cost functions, but is otherwise flexible. In particular, it can accommodate probabilistic control policies, fairly general disturbance distributions, and controldependent, non-monotonic, and non-convex stage costs. We demonstrate how risk-sensitive safety analysis is useful for a stormwater infrastructure application. Our numerical examples are inspired by current challenges that cities face in managing precipitation uncertainty.
Index Terms-- Conditional Value-at-Risk, Stochastic optimal control, Safety analysis, Markov decision processes.
I. INTRODUCTION
T HE typical notion of safety for a control system is defined in terms of a minimax optimal control problem [12]≠[16]. This problem represents the setting in which a non-stochastic bounded disturbance is modeled as an adversary that attempts to prevent safe and efficient operation. This notion of safety is particularly useful when it is feasible to estimate disturbance
This paper was submitted for initial review in June 2021. This work was supported in part by the Computational Hydraulics International University Grant Program for complementary use of PCSWMM Professional software. K. M. Smith was supported by an NSF Integrative Graduate Education and Research Training award (NSF 0966093).
M. P. Chapman is with the Edward S. Rogers Sr. Department of Electrical and Computer Engineering, University of Toronto, Toronto, Ontario M5S 3G4 Canada (email: mchapman@ece.utoronto.ca).
M. Fauﬂ and H. V. Poor are with the Department of Electrical and Computer Engineering, Princeton University, Princeton, New Jersey 08544 USA (email: {mfauss,poor}@princeton.edu).
K. M. Smith is with the Department of Civil and Environmental Engineering, Tufts University, Medford, MA 02155 USA and OptiRTC, Inc., Boston, MA 02116 USA (email: kevin.smith@tufts.edu).
The work of M. Fauﬂ was supported by the German Research Foundation (DFG) under grant number 424522268.
*This work provides a method to compute risk-sensitive safe sets and optimal pre-commitment policies exactly in principle. Our prior work [1] provided a method to under-approximate risk-sensitive safe sets.

bounds accurately and the range of the disturbance is not too large. However, if the disturbance is better modeled as a random variable, one may prefer to use a different notion of safety. One may define safety in terms of a stochastic optimal control problem, in which the optimal value function quantifies a probability of safe and efficient operation. This framework (called stochastic safety analysis) assumes that the disturbance is stochastic and need not be adversarial [17], [18], but an adversarial disturbance can also be accommodated [19]≠[21].
The notion of safety is closely linked to the notion of risk. In engineering, risk is defined usually in terms of the probability of a dangerous outcome. However, in finance, risk is defined usually in terms of the probability in addition to the severity of a dangerous outcome. The latter, more general notion of risk may be useful for control systems applications, in which
1) major departures from desired operating regions may cause considerably more harm compared to minor departures, or
2) assuming adversarial bounded disturbances may be too conservative or otherwise inappropriate, but providing protection against rare harmful outcomes is still important.
For example, applications related to the management of natural resources, such as water and renewable energy, fall into the above categories.
Consequently, new safety analysis methods that use risk functionals from finance to characterize heavy-tailed distributions are emerging [1], [22], [27]. A risk functional is a map from a set of random variables to the extended real line.1 Examples include Exponential Utility, Value-at-Risk (VaR), Conditional Value-at-Risk (CVaR), and Mean-Deviation [26]. We illustrate VaR and CVaR in Fig. 1 and provide technical details about these functionals in a subsequent section. While using risk functionals to quantify safety for control systems is recent, this research topic is linked to risk-sensitive Markov decision processes (MDPs), which have been studied since at least the 1970s. We provide a brief history and refer the reader to our prior work [1] and the references therein for a more detailed history.
Howard and Matheson studied risk-sensitive MDPs, where risk was defined in terms of Exponential Utility, in 1972 [2]. D. H. Jacobson investigated Exponential-Utility-optimal
1The term risk measure is also used in the literature because such a map "measures" risk. We use quotes because the meaning in this context is different from the formal definition of a measure in measure theory.

Fig. 1. A risk-sensitive safe set Sr is a set of initial conditions from which the Conditional Value-at-Risk (CVaR) of Y at level   (0, 1]
can be reduced to a threshold r  R. Y is the random maximum
signed distance between the state trajectory and a desired operating
region K. The CVaR of Y at level  represents the average value of Y in the  ∑ 100% worst cases. Sr quantifies the safety of a control system in terms of the probability mass or density above a given
quantile. This notion of safety may be quite useful for applications in
which leaving a desired operating region is unavoidable, but the extent
of such a departure should be limited when possible. In this work, we provide a method to compute Sr exactly in principle via a state-space augmentation technique. We compare this method to our prior work [1], which provides under-approximations Ur , that depend on a soft-max parameter  that requires tuning.

control of linear systems in 1973 [3]. P. Whittle and colleagues

significantly advanced this theory in the 1980s and 1990s (e.g.,

see [5], [25]). di Masi and Stettner studied an infinite-time

Exponential-Utility-optimal control problem for systems on

Borel spaces [4]. Ba®uerle and Rieder studied the problem of

optimizing an expected utility for systems on Borel spaces and

analyzed Exponential Utility as a special case [9].

The (risk-averse) Exponential Utility functional, which takes

the

form

-2 

log

E

(exp(

- 2

G)),

uses

the

exponential

function

and a parameter  < 0 to exaggerate larger values of a random

cost G. The intention is to penalize these values more strongly

in the process of optimizing a controller. Under appropriate

conditions, Exponential Utility approximates a weighted sum

of mean and variance, where the weight on the variance

depends on  (e.g., see [5]). It is challenging to guarantee

a priori that the mean-variance approximation is valid for a

particular problem instance (unless, of course, G is known

to be a Gaussian random variable). Also, it is challenging

to specify a precise quantitative interpretation of Exponential

Utility and its parameter . These challenges have motivated the study of risk-sensitive MDPs that express the notion of risk differently.
Frameworks for optimizing MDPs with respect to expected utility functionals [7], [9], [24], compositions of risk functionals [10], [34], and the CVaR functional [8], [24], [28] have been proposed over the years. A related line of work considers the problem of minimizing an expected cumulative cost subject to a risk constraint on a stage cost [22]≠[24], [31]. Our method is most closely related to [8] because we consider a finite, discrete time horizon and a CVaR objective. However, we focus on the setting of a maximum cost for safety analysis of control systems, which is distinct from the setting in [8].
A challenge with optimizing MDPs with respect to some objectives, such as CVaR or expected (non-exponential) utility, is that a dynamic programming (DP) recursion on the state space does not exist. For example, a CVaR-MDP problem aims to minimize an expected cumulative cost in the ∑100% worst cases by choosing a suitable policy that starts at time t = 0. To achieve this goal, however, one cannot simply minimize the CVaR of a random cost-to-go at level  at each time t. The fraction of outcomes considered by the CVaR objective varies over time to satisfy the original  level at time t = 0 [29], [30]. This property is called time inconsistency, and it precludes applying dynamic programming directly to the state space [6].
To overcome this challenge, one approach is to define an augmented state space and dynamics for the extra state, which encodes the fraction of outcomes to be considered, so that a DP recursion holds for the augmented dynamical system. This recursion can be used to optimize a control policy that depends on the augmented dynamics. Such a policy may be called an optimal pre-commitment policy to emphasize its extra dependencies (e.g., an actor pre-commits to using the augmented dynamics to inform its decisions.) The above techniques have been used by [8], [9], [11], [24], for example,2 and we make use of such techniques as well.
Here, we extend our method to analyze the safety of a control system, where we quantify safety in terms of the probability mass or density above a given quantile of a trajectory-wise cost [1], [27]. We call this method risksensitive safety analysis (Fig. 1). Its foundation is a nonstandard optimal control problem that penalizes the random maximum signed distance between the state trajectory and a desired operating region. This random cost is assessed via CVaR, which represents the average in a fraction  of worst cases. Our formulation evaluates a random maximum cost in terms of its probability mass or density above a given quantile, whereas classical risk-sensitive control evaluates a random cumulative cost in terms of a scaled exponential function. In prior work, we defined risk-sensitive safe sets as sub-level sets of the optimal value function of the non-standard problem just described [27], and we provided a method to compute underapproximations of such sets [1].
2We note that Miller and Yang studied a CVaR optimal control problem in continuous time [28] and used a bilevel optimization procedure that does not require state-space augmentation.

In the current work, our primary theoretical contribution is an exact characterization of risk-sensitive safe sets in terms of the solutions to a family of stochastic dynamic programs. We derive this characterization by expressing the minimum CVaR (for a given initial state x and risk-sensitivity level ) as a nested optimization problem with respect to a control policy and a dual parameter s. We solve a stochastic dynamic program with a non-standard, but tractable objective to compute an optimal value function and pre-commitment policy, which depend on s. Subsequently, we perform an outer minimization over s to obtain the minimum CVaR as a function of x and . Risk-sensitive safe sets are characterized by the sub-level sets of this function.
In some applications, it is appropriate to assume that the cost of a system operating outside of a desired region accumulates over time. That is, the cost incurred increases for every time interval during which the system violates its constraints. However, there also exist applications in which the extent of the violation over a brief time interval is more critical to assess than its accumulation. For example, in stormwater management applications, the maximum water level can be a useful surrogate for both the maximum flood extent (in more extreme cases) and the maximum discharge rate (in general). These are instantaneous rather than cumulative properties. For example, for gravity-drained stormwater systems, the instantaneous discharge rate through an uncontrolled outlet into open atmosphere is a function of the water level behind the outlet. Therefore, from water levels, we can estimate instantaneous demands on downstream conveyance infrastructure (i.e., infrastructure to transport water rather than to store it). Designing for the worst maximum discharge rate may be costprohibitive. However, assessing the average maximum water level in the worst  ∑ 100% of cases would allow designers to estimate downstream conveyance capacity demands along a spectrum of worst cases.
Our method provides, in principle, the exact optimal CVaR of a maximum cost of the state trajectory for the purpose of safety analysis. We illustrate the usefulness of this method in numerical examples inspired by the challenges of managing uncertain stormwater inflows. The theoretical optimality guarantee comes at the expense of significantly more computational complexity compared to the under-approximation method in our prior work [1]. We discuss how the current method and our prior method may be useful in different ways for evaluating the relative merits of stormwater system designs. We envision future extensions to applications in the management of natural resources more broadly, including renewable energy and water-energy systems.
Organization. We present the control system model in Sec. II and the safety analysis problem in Sec. III. The core methodology and a summary of the approach are in Sec. IV. The main algorithm is in Sec. V. We provide numerical examples in Sec. VI and concluding remarks in Sec. VII. Theoretical details are in the Appendix and the supplementary material.
Notation. If X is a metric space, then BX is the Borel sigma algebra on X . C(X ) is the Banach space of bounded, realvalued, and continuous functions on X . P(X ) is the set of

probability measures on (X , BX ). If x  X , then x : BX  {0, 1} is the Dirac measure on (X , BX ) concentrated at x. We define R+ := R  [0, +), R := R  {+, -}, and N := {1, 2, . . . }. Rn+ is the non-negative orthant in Rn. Let N  N, and we define T := {0, 1, . . . , N - 1} and TN := {0, 1, . . . , N }. We distinguish between random objects and their values (i.e., realizations) via capital letters and lowercase letters, respectively. E.g., xt is a value of the random state Xt. We abbreviate lower semi-continuous as lsc.
II. CONTROL SYSTEM MODEL
We consider a stochastic control system operating on a finite time horizon TN , where N  N is given. The state space S and the control space A are non-empty Borel spaces. The random state Xt, the random control Ut, and the random disturbance Wt take values in S, A, and Rd, respectively. The disturbance process (W0, W1, . . . , WN-1) is a sequence of random vectors such that (Wt  W | Xt, Ut) for all  = t. That is, given (Xt, Ut), Wt is conditionally independent of W for any  = t. The initial state X0 is fixed at an arbitrary x  S. The stage cost c : S ◊ A  R and the terminal cost cN : S  R are Borel-measurable functions. The notation  denotes a class of history-dependent control policies, which we define formally in Sec. IV.
Let p(dw|x, u) be a Borel-measurable stochastic kernel on Rd given S ◊ A, and let f : S ◊ A ◊ Rd  S be a Borelmeasurable function. If (xt, ut) is the value of (Xt, Ut), then the probability that Wt is in B  BRd is given by p(B|xt, ut), and the probability that Xt+1 is in D  BS is given by
Q(D|xt, ut) := p {wt  Rd : f (xt, ut, wt)  D} xt, ut . (1)
Note that Q(dxt+1|xt, ut) is a Borel-measurable stochastic kernel on S given S ◊ A.
Assumption 1: We assume the following conditions:
1) c and cN are bounded below by 0 and bounded above by c, where c  R+. We define Z := [0, c].
2) The control space A is compact. 3) The dynamics function f , the stage cost c, and the
terminal cost cN are continuous functions. 4) p(dw|x, u) is a continuous stochastic kernel.3 Remark 1 (Justification of Assumption 1): We assume bounded costs for simplicity and computational tractability.4 Our method accommodates real-valued bounded costs as well, which we explain in Sec. III. The last three conditions guarantee the existence of a Borel-measurable selector so that an optimal pre-commitment controller exists. Such measurable selection conditions are common, e.g., see [8, p. 368], [32, pp. 27-28], [9, pp. 107-108], [35, Def. 8.7,
3The function  : S ◊ A  P(Rd), such that (x, u) := p(dw|x, u), is continuous [35, Def. 7.12, p. 134].
4In principle, our results can be generalized to the setting in which the stage and terminal costs are not bounded from above but have the property of being bounded in expectation. For example, a terminal cost c~N that is not bounded from above can be approximated by a clipped version c~N (x) := min{L, c~N (x)}, L  R. Since the probability of the event {c~N (XN ) > L} goes to zero as L  +, the solution of the exact problem can be approximated arbitrarily closely by choosing a sufficiently large L. We leave a formal discussion to a future paper.

pp. 208-209]. In particular, Refs. [8] and [9] involve the optimization of pre-commitment controllers. We assume continuous costs, rather than lower semi-continuous costs, as a consequence of studying a non-standard risk-sensitive safety analysis problem. In our setting, the operation to update the cost is no longer an addition, but a composition of two functions. Hence, lower semi-continuity must be replaced by a property that is preserved under compositions. Continuity is a natural choice that also simplifies some proofs. A possible alternative to retain lower semi-continuity is to introduce additional monotonicity assumptions; compare, for example, [10], [11].
III. A RISK-SENSITIVE SAFETY ANALYSIS PROBLEM
In this work, we advance safety specifications for control systems, where we quantify the notion of safety in terms of the probability mass or density above a given quantile of a trajectory-wise cost. Suppose that there is a desired operating region K  S, where the state trajectory should remain inside if possible. However, it may not be possible for the state trajectory to remain inside K always due to disturbances that arise in the environment. We define a random variable representing the random maximum signed distance between the state trajectory and K. Then, we define a safety specification in terms of this random variable and CVaR.
1) Stage and terminal costs, c = cN = gK - g: Let gK : S  R be a bounded continuous function that quantifies a distance between a state x and the boundary of K. For example, if K := [0, k1] ◊ [0, k2] represents the preferred highest water levels in two storage tanks, then max{x1 - k1, x2 - k2, 0} (Fig. 2, left) or max{x1 - k1, x2 - k2} (Fig. 2, right) may be suitable choices for gK(x). More generally, if x is outside K and far from its boundary, then gK(x) has a large positive value. Otherwise, if x is inside K, then gK(x) may equal
1) zero, or 2) a more negative value when x is more deeply inside K.
The former represents a setting in which there is no preference for certain trajectories inside K. In contrast, the latter represents a setting in which there is a preference for trajectories that are inside K and further from its boundary. These options may be used to incorporate additional (soft) criteria, such as minimizing the energy consumption of the system, by explicitly favoring trajectories that serve these criteria.
Since we aim to keep the state trajectory within a desired operating region when this task is feasible, we define the stage cost c and the terminal cost cN in terms of gK . Denote the upper and lower bounds of gK as g and g, respectively, and let c := g - g. Then, we define
c(x, u) := gK (x) - g (x, u)  S ◊ A, (2)
cN (x) := gK (x) - g x  S.
By the above definitions, c and cN are equivalent, and their values are elements of Z = [0, c]. Although our focus is on cost functions of the form above, our main theoretical results hold when the stage cost c at time t is allowed to depend on the control at time t (Theorem 1, Sec. V; Theorem 2, Appendix).

Fig. 2. Examples of the function gK when K := [0, k1] ◊ [0, k2], k1,2 > 0, and the state space S is a compact subset of R2+. The left plot shows max{x1 - k1, x2 - k2, 0} versus x, and the right plot shows max{x1 - k1, x2 - k2} versus x. Each plot shows K as a shaded green region in the (x1, x2)-plane, where k1 = 3 and k2 = 4.
2) The random variable Y : We define a random cost Y that represents the random maximum signed distance between the state trajectory and the desired operating region K. If (x0, x1, . . . , xN )  SN+1 is the value of the state trajectory (X0, X1, . . . , XN ), then y = max gK (xt) : t  TN is the value of the random cost Y .
Soon, we will define a safety specification in terms of Y and CVaR, which is called a risk-sensitive safe set. This set is characterized by the quantity CVaR,x(Y ), which depends on a risk-sensitivity level , an initial condition x  S, and a control policy   . Thus, before defining the safety specification, we define CVaR,x(Y ). Please also refer to Fig. 1, which illustrates the concepts that we discuss below if Y is a continuous random variable.
3) The quantity CVaR,x(Y ): If Y is a continuous random variable, then the CVaR,x(Y ) is the expectation of Y in the  ∑ 100% worst cases when the system is initialized at x and uses the policy . More specifically, if Y is continuous and   (0, 1), then CVaR,x(Y ) equals the expectation of Y conditioned on the event that Y exceeds VaR,x(Y ) [26, Thm. 6.2, p. 259]. This quantity is the Value-at-Risk of Y at level , which is the left-side (1 - )-quantile of the distribution of Y . A synonym is the generalized inverse of the cumulative distribution function of Y . VaR,x(Y ) can be viewed as a threshold above which a cost is considered significant and is defined by
VaR,x(Y ) := inf y  R : Px({Y  y })  1 -  . (3)
We introduce the probability measure Px subsequently. In our setting, Y is a bounded random variable on a
probability space (, B, Px). The system's trajectory of states, controls, and maximum stage costs is a random object, and the sample space  is a Cartesian product containing the values of this random object. The probability measure Px, which is parametrized by an initial condition x and a policy , provides the probability that the trajectory belongs to a Borel-measurable rectangle in . We provide more details regarding Px in Section 2 of the supplementary material. For any   (0, 1], the quantity CVaR,x(Y ) is defined in terms

of Px as follows:

CVaR,x(Y

) := inf
sR

s

+

1 

Ex

(max{Y

- s, 0})

,

(4)

where s  R is called a dual parameter. The quantity Ex(max{Y -s, 0}) is the expectation of the random variable max{Y - s, 0} with respect to Px. It can be shown (see [26, p. 258]) that if   (0, 1), then

CVaR,x(Y ) = VaR,x(Y )

+

1 

Ex (max{Y

- VaR,x(Y ), 0}).

(5)

The above equation means that CVaR,x(Y ) quantifies the amount of probability mass or density above the left-side (1 - )-quantile of Y . Overall, CVaR is an attractive choice for
defining safety specifications for control systems because

1) The CVaR of a continuous random cost is the average in the  ∑ 100% worst cases;
2) The parameter  has a precise quantitative interpretation as a fraction of worst cases; and
3) CVaR quantifies the part of a distribution above a particular quantile, and therefore is designed to assess more rare, harmful outcomes.

4) Risk-sensitive safe sets: Having defined the quantity CVaR,x(Y ), we are now ready to present our safety specification, which is illustrated in Fig. 1.
Definition 1 (Risk-Sensitive Safe Set [27]): Let   (0, 1]
and r  R. The (, r)-risk-sensitive safe set is defined as
follows:

Sr :=

xS

:

inf


CVaR,x(Y

)r

,

W (x)

where Y := max{gK (Xt) : t  TN }. Sr is the set of initial conditions from which the risk-
sensitive optimal value W(x) is at most r. W(x) is the minimum value of CVaR,x(Y ) when optimized over control policies in . (We define  formally in the next section.)
5) Evaluating W (x) in terms of V (x): To evaluate W(x), we formulate an equivalent problem in terms of a non-negative
random variable Y := Y -g. We use the fact that the CVaR at
level   (0, 1] of a random variable with finite expectation is translation-equivariant.5 Since Y is bounded for any x  S
and   , g  R, and   (0, 1], we have

CVaR,x(Y ) = CVaR,x(Y ) - g,

(6)

and consequently,

inf


CVaR,x(Y

)

=

inf


CVaR,x(Y

) -g.

(7)

V (x)

W (x)

Next, we present a method to compute V(x). Then, we use V(x) to compute W(x) via (7). Lastly, we use W(x) to compute Sr (Def. 1).

5If G is a random variable with finite expectation, a  R, and   (0, 1], then CVaR(G + a) = CVaR(G) + a.

IV. METHOD TO COMPUTE RISK-SENSITIVE SAFE SETS VIA STATE-SPACE AUGMENTATION
Unlike the expectation of a cumulative cost, V(x) cannot be computed via a dynamic programming recursion on the state space S. Indeed, such a recursion holds in special cases due to the structure inherent in certain problems, but it does not hold universally. Instead, we use a state-space augmentation approach, as we initially presented in the Introduction.
1) Augmented state (Xt, Zt): We define an augmented state (Xt, Zt). Xt is the original S-valued random state, and Zt is a Z-valued random object that keeps track of the maximum stage cost from time 0 to time t. For any t  T, if xt  S, ut  A, and zt  Z are the values of Xt, Ut, and Zt, respectively, then the value of Zt+1 is given by zt+1 = max{zt, c(xt, ut)}.
2) Sample space : Formally, we define Xt, Zt, and Ut as functions on a common sample space ,

 := (S ◊ Z ◊ A)N ◊ S ◊ Z.

(8)

Any    is a value of the augmented system's random trajectory, which takes the form

 = (x0, z0, u0, . . . , xN-1, zN-1, uN-1, xN , zN ), (9)

with xt  S, zt  Z, and ut  A, respectively. For any  of the form (9), we define Xt() := xt, Zt() := zt, and Ut() := ut, and therefore, Xt, Zt, and Ut are Borel-

measurable. While these definitions are general enough to

capture arbitrary dependencies between elements of , we

restrict ourselves to particular casual dependencies, which we

have discussed and will continue to present.

3) Class of control policies: We consider a class of control

policies , which are history-dependent through (Xt, Zt). Definition 2 (Control policies in ): A control policy  

 takes the form,  := (0, 1, . . . , N-1), where

t(dut|xt, zt) is a Borel-measurable stochastic kernel on A

given S ◊ Z for each t. In particular, t(dut|xt, zt) is the

distribution of Ut, if the value of (Xt, Zt) is (xt, zt).

Remark 2 ( is history-dependent): The

distribution

t(dut|xt, zt) of Ut is a function of the current value (xt, zt) of the random augmented state (Xt, Zt). This value implicitly

depends on the previous states and controls, implying that the

policy is history-dependent.

4) Evolution of the system: We assume that the system

evolves in the following way. First, let   , and suppose

that (xt, zt)  S ◊ Z is the value of (Xt, Zt). Second, the distribution of Ut is given by t(dut|xt, zt), and a control ut  A is sampled from this distribution. Third, the distribution of Wt is given by p(dwt|xt, ut), and a disturbance wt  Rd is sampled from this distribution. Then, the value of Xt+1 is given by xt+1 = f (xt, ut, wt), and the value of Zt+1 is given by zt+1 = max{zt, c(xt, ut)}. The above process repeats from time t = 0 to time t = N - 1. The initial augmented state

(X0, Z0) is fixed at (x, 0)  S ◊ Z, where x  S is arbitrary. 5) Re-expressing V (x): We re-express V(x) (7) by using
the definition of CVaR,x(Y ). Note that Y is a bounded random variable on (, B, Px), and the quantity CVaR,x(Y )
is defined by

CVaR,x(Y

)

:=

inf
sR

s+

1 

Ex

(max{Y

- s, 0})

,

(10)

where Ex(max{Y -s, 0}) is the expectation of max{Y -s, 0} with respect to Px. In particular, it holds that Y ()  Z for all   . By using this fact and (10), it follows that

V(x)

=

inf


inf
sR

s

+

1 

Ex

max{Y - s, 0}

(11)

= inf inf
sR 

s

+

1 

Ex

max{Y - s, 0}

(12)

= inf
sR

s

+

1 

inf


Ex

max{Y - s, 0}

(13)

= min
sZ

s

+

1 

inf


Ex

max{Y - s, 0}

.

(14)

V s(x)

Eq. (11) holds by (7) and (10). Eq. (12) holds by exchanging
the order of the infima over R and . Eq. (13) holds because s and  are fixed from the point of view of minimizing over .
Eq. (14) holds as a consequence of Y ()  Z for all   ,
and we write min in (14) to indicate that a minimizer exists
(Lemma 1, Appendix).
Next, we explain how the reformulation above facilitates the computation of Sr by outlining the procedure. In particular, the procedure uses a dynamic programming algorithm to compute V s(x) and a policy s  , which are parametrized
by s  Z. In this algorithm, the value function at time 0 is denoted by J0s(x, 0). We present the algorithm formally as Theorem 1 in Sec. V, but first note the high-level procedure.
6) Procedure to compute Sr : We proceed through the following steps to compute Sr .
1) For all s  Z, use Theorem 1 to compute V s and a policy s  , which satisfy

J0s(x, 0)

= V s(x)

:=

inf


Ex

max{Y - s, 0}

= Exs max{Y - s, 0}

x  S. (15)

The policy s is a deterministic Markov policy on the

augmented state space, whose existence is shown in the

proof of Theorem 1 (Appendix). 2) Use the family of functions {V s : s  Z} to compute
V(x) and sx,  Z, which satisfy

V(x) (=14) min

s

+

1 

V

s(x)

:

s



Z

,

(16)

sx,  arg min

s

+

1 

V

s(x)

:

s



Z

(17)

for all x  S and   (0, 1]. A minimizer exists by Lemma 1 (Appendix). 3) Use the function V to compute the function W via (7), W(x) = g + V(x) for all x  S and   (0, 1]. 4) Use the function W to compute the (, r)-risk-sensitive safe set Sr := {x  S : W(x)  r} (Def. 1).
By repeating steps 2-4 for all   (0, 1] and r  [g, gØ] of interest, we obtain a family of risk-sensitive safe sets.

V. VALUE ITERATION AND POLICY SYNTHESIS

In this section, we present and prove an algorithm to compute the value function

VØ

s(x)

:=

inf


Ex

max{YØ - s, 0}

x  S,

(18)

where the random variable YØ is defined by

YØ () := max cN (xN ), max c(xt, ut) ,

(19)

tT

for any  = (x0, z0, u0, . . . , xN-1, zN-1, uN-1, xN , zN )  . In the setting of safety analysis, we choose cN = c = gK - g, and therefore, we obtain YØ = Y and VØ s = V s.
Theorem 1 (Computing VØ s and Øs): Let s  R, and as-
sume the conditions of Assumption 1. Define the function JNs : S ◊ Z  R as follows:

JNs (xN , zN ) := max max{cN (xN ), zN } - s, 0 . (20a)

For t = N - 1, . . . , 1, 0, define the function Jts : S ◊ Z  R as follows:

Jts(xt,

zt)

:=

inf
ut A

vts(xt,

zt,

ut),

where vts : S ◊ Z ◊ A  R is defined by

(20b)

vts(xt, zt, ut) = Rd Jts+1 f (xt, ut, wt), max{c(xt, ut), zt} p(dwt|xt, ut).
(20c)
Then, for all t  TN , Jts is lower semi-continuous and bounded below by 0. In addition, for all t  T, there exists a Borel-measurable function st : S ◊ Z  A such that
Jts(xt, zt) = vts xt, zt, st (xt, zt) (xt, zt)  S ◊ Z. (21)

Moreover, it holds that
J0s(x, 0) = VØ s(x) = ExØs max{YØ - s, 0} x  S, (22)
where VØ s is defined in (18) and Øs := (s0 , s1 , . . . , sN-1 )   is the policy that is obtained by choosing the control st (xt, zt)  A when the value of (Xt, Zt) is (xt, zt).
Remark 3: The (non-unique) optimal policy Øs in Theo-
rem 1 is deterministic and is of the form specified by Defini-
tion 2. Note that st is a Borel-measurable stochastic kernel on A given S ◊ Z because the function t : S ◊ Z  P(A), such that t(xt, zt) := st (xt,zt), is Borel-measurable. t is the composition of two Borel-measurable functions. The function  : A  P(A), such that (u) := u is the Dirac measure on (A, BA) concentrated at u, is continuous by [35, Corollary 7.21.1, p. 130]. The function st : S ◊ Z  A is Borelmeasurable by Theorem 1.
A proof for Theorem 1 and proofs for supporting results
are in the Appendix. In the next remark, we explain how Theorem 1 enables the computation of V s and s.
Remark 4 (Computing V s and s): Theorem 1 pertains to
continuous bounded stage and terminal cost functions, c and
cN , respectively. If we let cN = c = gK - g, then we obtain an algorithm to compute V s and a policy s   such that V s(x) = Exs max{Y - s, 0} for all x  S by Theorem 1.
Subsequently, we explain how to obtain and deploy an
optimal pre-commitment policy by using the above results.
Remark 5 (Procedure for policy deployment): Let   (0, 1] and x  S be given. Let sx,  Z satisfy (17). Let sx,   satisfy (15) with s = sx,, and we know that such a policy exists by Theorem 1. sx, is an optimal
pre-commitment policy, which depends on the risk-sensitivity

level  and the augmented system dynamics initialized at

(x0, z0) = (x, 0). law for time t that

For any t  T, is associated with

let the

pstoxl,icybesxth,e.

control Let the

value of (X0, Z0) be (x0, z0) = (x, 0), and let t = 0.

1)

Choose

the

value

ut

of

Ut

to

be

ut

=

sx,
t

(xt

,

zt).

2) Nature provides a value wt of Wt according to the

distribution p(dwt|xt, ut).

3) Then, the value xt+1 of Xt+1 is given by xt+1 =

f (xt, ut, wt), and the value zt+1 of Zt+1 is given by

zt+1 = max{zt, c(xt, ut)}.

4) The time index t updates by 1.

The above steps repeat for all t  T := {0, 1, . . . , N - 1}.

VI. NUMERICAL EXAMPLES
The computations required for risk-sensitive safety analysis are completed off-line. While these computations suffer from the curse of dimensionality (as in any computation that requires dynamic programming), the task of designing engineering systems often requires large-scale off-line simulations or computations. We see risk-sensitive safety analysis as a tool to help practitioners evaluate which design changes to a control system may be more appropriate despite uncertainties.
1) Description of application: We consider the problem of modifying the design of an urban stormwater system (i.e., a network of pipes, storage tanks, natural streams, etc., near an urban area). Apart from being actively controlled, the stormwater system that we consider is otherwise typical.6 The system consists of two tanks connected by a valve, and water flows by gravity between the tanks according to the relative water level (Fig. 3). Water enters the system via a random process of surface runoff. Water exits the system through a storm sewer drain that is connected to tank 2 or through outlets that lead to a combined sewer. The storm sewer directs stormwater to a nearby water body. In our model, this is the desired outcome and occurs without penalty. Unfortunately, the storm sewer capacity is limited, and when water levels become too high, excess flows are directed to a combined sewer. In drier periods, a combined sewer carries a mixture of untreated wastewater and stormwater to a wastewater treatment plant. However, when storm events cause the flow in a combined sewer to exceed its design capacity, a flow regulator (downstream from our system) will divert some of the untreated mixture of stormwater and sewage into a nearby water body. This event is known as a combined sewer overflow. Combined sewers are present in older cities, such as Toronto and San Francisco, and overflows from these sewers can harm local ecosystems. We aim to use risk-sensitive safety analysis to examine how design modifications to the system above may reduce the risk of combined sewer overflows by managing the maximum water levels in the system.
2) System model for baseline design: The state Xt = [Xt1, Xt2]T is a vector of the random water levels in tank 1 and tank 2 at time t. The co-domain of Xt is S := [0, k1] ◊ [0, k2] ft2, where ki := ki + 2 and ki is the
6Actively controlled stormwater systems are becoming more common but are relatively novel technologies (e.g., see [36]).

maximum water level that tank i can hold without releasing water into the combined sewer. The control Ut is the valve setting at time t, and the co-domain of Ut is A := [0, 1] (closed to open, unitless). The sequence of random variables (W0, W1, . . . , WN-1) is a random process of surface runoff that arises due to precipitation uncertainty (units: cubic feet per second, cfs). Wt and W are independent for all t =  and do not depend on the current state or control in this example. The notation  denotes the duration between time t and time t + 1, which is constant for all t  T.
Let t  T be given. If xt  S, ut  A, and wt  R are the values of Xt, Ut, and Wt, respectively, then the value of Xt+1 is given by

xt+1 = xt + f0(xt, ut, wt) ∑ ,

(23a)

such that if xt+1,i  ki, then we redefine xt+1,i := ki. The function f0 : R2+ ◊ A ◊ R  R2+ is chosen according to
simplified Newtonian physics:

f0(x, u, w) := [f01(x, u, w), f02(x, u, w)]T ,

f01(x, u, w)

:=

w

-

qcso,1(x) - a1

qvalve(x, u) ,

f02(x, u, w)

:=

w

-

qcso,2(x)

+

qvalve(x, u) a2

-

qstorm,2(x) ,

qvalve(x, u) := u ∑ ~rv2 ∑ sign h(x) ∑ 2g~|h(x)|,

h(x) := max(x1 - z1, 0) - max(x2 - z1,in, 0). (23b)

Table I lists model parameters. The outlets to the combined

sewer and to the storm sewer are equipped with outflow

regulation devices that produce a linear outflow rate. For

example, qstorm,2(x) with x  S is given by

qstorm,2(x)

:=

qmax,2

-

qmax,2 k2 - z2

min{k2 - x2, k2 - z2},

(23c)

1
where qmax,2 := cd~rs2 2g~(k2 - z2) 2 is tank 2's maximum outflow rate to the storm sewer, cd is a discharge coefficient, ~  3.14, g~ is gravitational acceleration, rs is the storm sewer outlet radius, and z2 is the storm sewer outlet elevation. The outflow rates to the combined sewer, qcso,1 and qcso,2, are defined similarly to (23c). The constraint set K := [0, k1]◊[0, k2]

specifies the invert elevations of the combined sewer outlets

(i.e., the maximum water levels that the tanks can hold without

releasing water into the combined sewer). The function gK

quantifies the maximum water elevation above the combined

sewer invert elevations,

gK (x) := max{x1 - k1, x2 - k2, 0} x  S. (24)

We show this function in the left panel of Fig. 2. We use a finite distribution for the disturbance process,
which does not depend on the values of the current state or control. In prior work [37], we simulated a design storm in PCSWMM software (Computational Hydraulics International), which is an extension of the US Environmental Protection Agency's Stormwater Management Model [38]. A design storm is a synthetic precipitation time series based on historical data that a local government uses to specify regulations for new or retrofitted stormwater systems. The empirical distribution from our simulations had positive skew, and the mean was

Fig. 3. A schematic of the stormwater system and a probability mass function for the surface runoff disturbance (cubic feet per second, cfs). The baseline design and three alternative designs are shown.

TABLE I STORMWATER SYSTEM PARAMETERS (BASELINE)

Symbol Description

Value

a1

Surface area of tank 1

a2

Surface area of tank 2

30000 ft2 10000 ft2

cd

Discharge coefficient

g~

Acceleration due to gravity

g

Minimum of gK

0.61 (no units)

32.2

ft s2

0 ft

g

Maximum of gK

2 ft

k1

Combined sewer (cs) outlet elevation, 3 ft

tank 1

k2

cs outlet elevation, tank 2

4 ft

k1

Maximum value of x1

5 ft

k2

Maximum value of x2

6 ft

N

Length of discrete time horizon

20 (= 1 h)

~

Circle circumference-to-diameter ratio 3.14

rs

Storm sewer outlet radius

1/3 ft

rv

Valve radius

1/3 ft

 Duration of [t, t + 1)

3 min

z1

Pipe elevation w.r.t. tank 1 base

1 ft

z1,in Pipe elevation w.r.t. tank 2 base

2 ft

z2

Storm sewer outlet elevation

1 ft

N/A Number of cs outlets, tank 1

3

N/A Number of cs outlets, tank 2

1

N/A cs outlet radius, tank 1

1/4 ft

N/A cs outlet radius, tank 2

3/8 ft

cs = combined sewer, ft = feet, s = seconds, min = minutes, h = hours.

approximately 12.2 cfs. We used these characteristics to inform the choice of the disturbance distribution shown in Fig. 3: mean (12.2 cfs), variance (9.9 cfs2), and skew (0.74).
3) Verification of Assumption 1: It holds that gK (x)  [g, g] = [0, 2] for all x  S, where gK is defined by (24). We choose c(x, u) = cN (x) = gK (x) - g for all x  S and u  A. Thus, all values of c and cN are elements of Z := [0, cØ] = [0, 2], and c and cN are continuous functions. The control space A = [0, 1] is compact. In our example, the stochastic kernel for the disturbance process p(dw|x, u) does not depend on (x, u), which implies that it is constant and therefore continuous in (x, u). The dynamics function (23) is

continuous since it is a composition of continuous functions. Recall that the function (x) := sign(x) |x| is continuous since limx0 (x) = limx0 (x) = 0.
4) Designs: We investigate the effect of different designs on the system's safety, as quantified in terms of risk-sensitive safe sets. The designs are listed below:
a) Baseline; b) Replace the valve with a controllable bidirectional pump,
whose maximum pumping rate is qpump; c) Retrofit tank 1 with an outlet that drains to a storm sewer
without penalty; or d) Increase the surface area of tank 2 by 20%.
We modify the baseline system model to obtain a model representing design b, c, or d. For design d, we set a2 = 12000 ft2. For design c, the equation for the flow through tank 1 changes to the following:

f01(x, u, w)

:=

w

- qcso,1(x)

- qvalve(x, u) - qstorm,1(x) , a1

(25)

where qstorm,1(x) takes the same form as qstorm,2(x) in (23c).

For design b, the control space becomes A = [-1, 1], and

the term qvalve(x, u) is replaced by qpump(x, u), which models

the flow rate generated by a pump. Prior to presenting the

form of qpump(x, u), we introduce its dependencies, Ii(x, u) and (xi, u). Ii(x, u) is a Boolean variable that determines

whether the water level is too low to permit pumping, and the

function (xi, u) represents a start-up phase. I1(x, u) is true

if and only if the pump attempts to push water from tank 1

to tank 2 (u < 0), but the water level in tank 1 is too low.

I2(x, u) has an analogous interpretation. Formally, we define

I1(x, u) and I2(x, u) as follows:

I1(x, u) := x1 < zp - and u < 0 I2(x, u) := x2 < zp - and u  0,

(26a)

where zp is a threshold elevation and is a small positive number. We define the function (xi, u) as follows:

(xi, u)

:=

qpump 2

∑u (xi

+

- zp).

(26b)

We define qpump(x, u) as follows:

One can run the under-approximation method on a standard

laptop (2-4 CPU cores) in approximately 10 minutes for a

0 

if I1(x, u) or I2(x, u)

fixed  and a fixed design. However, this approach is not



qpump(x, u)

:=

- -

(x1, u) (x2, u)





-u ∑ qpump

if x1  [zp - , zp + ] and u < 0 suitable for the state-augmentation method. In particular, we

if x2  [zp - , zp + ] and u  0 used a high-performance computing cluster. The complete job

otherwise.

(four designs) required about 54 hours and 30 CPU cores.7

(26c) These run-time and CPU values should be considered a rough

One can show that qpump is a composition of continuous comparison of the resources that naive implementations of the

functions by replacing the case statements in (26c) with two methods require; no attempt has been made to optimize

minimum and maximum operators:

computational efficiency beyond parallelizing the operations

in a given DP recursion. Table III summarizes the main trade-

qpump(x, u)

=

-qpump 2

min{0, u}(y1) + max{0, u}(y2)

offs between the state-augmentation and under-approximation methods.

(y) := max{0, min{y, 2 }}

yi := xi + - zp. Table II lists model parameters for the pump design (b).

TABLE II STORMWATER SYSTEM PARAMETERS (PUMP DESIGN)

Symbol Description

qpump Maximum pumping rate

Slack variable

zp

Threshold pumping elevation

ft = feet, cfs = cubic feet per second.

Value

10 cfs

1 12

ft

1 ft

TABLE III TRADE-OFFS BETWEEN METHODS

State-Augmentation Method Provides Sr exactly in principle
Does not require parameter tuning

Under-Approximation Method [1]
Provides an under-approximation for Sr in principle The soft-maximum parameter  requires tuning.

Requires significantly more compu- Requires significantly less computa-

tational resources

tional resources

Useful for in-depth analysis of a Useful as a screening tool to idensmall number of promising designs tify more promising designs from a
collection of candidate designs

5) State-augmentation vs. under-approximation methods: Computations of Sr for the four designs are shown in Fig. 4, which we completed via the procedure outlined in Sec. IV-.6. For comparison, we provide computations of the underapproximation set Ur, ( = 20), which we completed via the method from our prior work [1]. The under-approximation method uses a -dependent soft-maximum and an -dependent upper bound for CVaR to derive a (, )-dependent upper bound for W (Def. 1). For a fixed , solving one MDP problem is required to compute Ur, for all  and r of interest. The value functions for this MDP problem are defined on the original state space S, and the objective is an expected cumulative -dependent cost. We explored values of  between 10 and 120 in increments of roughly 10. We chose  = 20 because this value provides relatively large estimates of Ur, for more risk-averse values of . The selection of an appropriate  depends on one's preferences, and additional guidance is provided in [1].
While no parameter tuning is required for the stateaugmentation method, which provides Sr exactly in principle, greater computational resources are required. First, due to time inconsistency and a non-additive cost function, the dynamic program that determines the minimum CVaR is solved on an augmented state space, which is the Cartesian product of the original state space S and the interval Z. Moreover, due to the representation of CVaR that our method uses, a second outer optimization over the dual parameter must be solved. Consequently, the inner dynamic program is solved repeatedly with different values of the dual parameter. While this increases the computational complexity significantly, problems with different dual parameters can be solved in parallel to reduce computation time.

6) Discussion of numerical results: We use the notation S^r (U^r,) to indicate a computation of Sr (Ur,). This notation emphasizes the distinction between an exact mathematical
quantity and a computation of this quantity returned by a com-
puter program. The under-approximation method preserves
interesting and potentially useful qualitative features that are
provided by the state-augmentation method. For example, the S^r -contours for the pump design (b) are more rectangular in comparison to those for the baseline design (a) (Fig. 5). These features are apparent from the U^r,-contours as well (Fig. 4, first two rows, pink dotted lines). The S^r -contours for the outlet design (c) are stretched along the x1-axis in comparison to the baseline design (a) (Fig. 6, top, black vs. pink). This effect is also seen by observing the associated contours of U^r, (Fig. 6, bottom, black vs. pink). Increasing the surface area of tank 2 (design d) stretches the contours of S^r and U^r, along the x2-axis in comparison to the baseline design (Fig. 6, top and bottom, orange dotted vs. pink). As the risk-aversion level  becomes smaller (more pessimistic), the contours of S^r and U^r, contract, as we expect, while the qualitative features are preserved (Fig. 4).
While the under-approximation method recovers qualitative
features and requires reduced computational resources, it tends
to over-estimate the effect of making a design change (Table
IV). Consequently, we see the under-approximation method
as a preliminary screening tool to identify more promising
designs from a collection of candidate designs. On the other
hand, we see the state-augmentation method as a tool for in-
depth analysis of a small number of promising designs that
7We used the Tufts Linux Research Cluster (Medford, MA) running MATLAB (The Mathworks, Inc.), and our code is available at https://github.com/risk-sensitive-reachability/RSSAVSA-2021.

have been selected a priori by preliminary screening. The risk-aversion level  allows one to specify a degree
of pessimism in terms of a fraction of worst cases, which has benefits for designing systems in practice. Stormwater systems are often required to satisfy precise regulatory criteria. For example, an outflow rate must be no more than a given threshold when simulating the outcome from a (non-stochastic) design storm via hydrology and hydraulic modeling software (e.g., [38]). Our framework could be used in parallel with standard practices to quantify the effect of stochastic surface runoff on low-dimensional models of proposed designs, as the degree of risk aversion  varies. The value of  provides a systematic and interpretable way to assess a design with respect to varying degrees of pessimism about the future. While the typical minimax approach to control systems leads to robust designs by adopting a worst-case perspective, designing for the worst case may not be financially feasible, especially given the limited budgets afforded to "ordinary" rather than "safetycritical" infrastructure. Therefore, the flexibility afforded by  may be useful for assessing trade-offs between system performance and financial considerations in practice.

TABLE IV QUANTITATIVE COMPARISON OF DESIGNS (r = 1)

 = 0.99

b vs. a

c vs. a

d vs. a

State-Aug

0.93

0.079

0.34

Under-Approx. 2.6

0.069

0.93

 = 0.05

b vs. a

c vs. a

d vs. a

State-Aug

2.1

0.068

0.71

Under-Approx. 3.6

0.059

1.3

 = 0.005 b vs. a

c vs. a

d vs. a

State-Aug

3.1

0.059

1.1

Under-Approx. 5.1

0.072

1.9

 = 0.0005 b vs. a

c vs. a

d vs. a

State-Aug

4.9

0.055

1.8

Under-Approx. 7.6

0.03

2.8

 = 0.00005 b vs. a

c vs. a

d vs. a

State-Aug

9.0

0.054

3.3

Under-Approx. 14

0.031

5.3

We list the increase in the size of a risk-sensitive safe set for design
b, c, or d compared to the baseline (design a). This is a quantitative
depiction of the some of the results in Fig. 6. Let Ny, denote the number of states in the computation of Sr for design y  {a, b, c, d} and r = 1. Let N^y, denote the number of states in the computation of Ur, for design y, r = 1, and  = 20. Each quantity in a row labeled State-Aug takes the form: (Ny, - Na,)/Na,. Each quantity in a row labeled Under-Approx. takes the form: (N^y, - N^a,)/N^a,.

VII. CONCLUSIONS
In this work, we have provided a method for obtaining risk-sensitive safe sets exactly in principle via a stateaugmentation technique. We have investigated the practical utility of applying risk-sensitive safety analysis to stormwater infrastructure design. We believe our approach may augment existing stormwater design practices by explicitly considering both stochastic surface runoff and risk aversion. Modeled as a whole, stormwater networks may have many nodes. However, for reasons of practicality, standard "source control" design practices often only require designers to model the new nodes without considering the dynamics of the existing network. This

effectively leads to low-dimensional design models, like those considered in this paper. However, there is growing concern that this site-level approach may fail to produce satisfactory results when viewed at the network scale [41], [40]. Therefore, extending the application of our method to higher dimensional systems is an important research direction. One approach is to incorporate the use of scalable approximate methods, such as rollout [39], to improve scalability. In addition, for highdimensional systems, it is more challenging to visualize risksensitive safe sets. This suggests the use of trade-off curves of estimates of the quantities in (5) to analyze different designs, as the risk-sensitivity level  varies.

APPENDIX

Lemma 1 (Existence of a minimizer): Let x  S and   (0, 1]. Recall that Z := [0, cØ], where cØ  R+. Let G :   R be Borel-measurable, and G()  Z for all   . Define vs(x) := inf Ex(max{G - s, 0}). Then, we have

inf
sR

s

+

1 

vs(x)

= min
sZ

s

+

1 

vs(x)

,

(27)

where we write min to indicate that a minimizer sx,  Z exists.
Proof: Define Lx : R  R as follows:

Lx (s)

:=

s

+

1 

vs(x).

(28)

First, let s  c, which implies that G()-s  0 for all   . Hence, it holds that

vs(x)

sc

=

inf


Ex

(0)

=

0,

(29)

and consequently,

Lx (s)

sc

=

s

+

1 

0

=

s



c

=

Lx (c),

(30)

which implies that8

inf Lx (s) = Lx (cØ).

(31)

s[cØ,+)

Analogously, s  0 implies that G() - s  0 for all   , so we have

vs(x)

s0

=

inf


Ex (G

-

s)

=

inf


Ex

(G)

-

s,

(32)

and consequently,

Lx (s)

s0

=

1 

inf


Ex

(G)

-

(1

-

)s

(33)



1 

inf


Ex

(G)

(34)

= Lx (0).

(35)

Since Lx (s)  Lx (0) for all s  0, we have

inf
s(-,0]

Lx (s)

=

Lx (0).

(36)

8We have infs[cØ,+) Lx (s)  Lx (cØ)  infs[cØ,+) Lx (s) by (77) and by the definition of the infimum.

Fig. 4. Contours of computations of risk-sensitive safe sets for   {0.99, 0.05, 0.005, 0.0005, 5 ∑ 10-5} and r  {0.2, 1, 1.8}. Solid blue lines show the numerical results of Sr via the state-augmentation method (recall the procedure in Sec. IV-.6). Pink dotted lines show the numerical results of Ur , ( = 20) via the under-approximation method [1]. Each row pertains to a particular design.

From (78) and (83), it follows that

inf
sR

Lx (s)

=

inf
s(-,0](0,cØ)[cØ,+)

Lx (s)

=

inf
s{0}(0,cØ){cØ}

Lx (s)

= inf Lx (s).
s[0,cØ]

(37) (38) (39)

One can show that Lx (s) is continuous in s. Since [0, c] is compact, continuity of Lx (s) in s implies that the infimum in (84) is attained.
To prove Theorem 1, we require several intermediary results
and definitions, which are stated below. Definition 3 (Random Cost-to-Go YØts): Let s  R. For any

 = (x0, z0, u0, . . . , xN-1, zN-1, uN-1, xN , zN )  , the random cost-to-go for time N is defined by:

YØNs () := max max{cN (xN ), zN } - s, 0 , (40)

and the random cost-to-go for time t  T is defined by:

YØts() := max

max

cN (xN ), max c(xi, ui), zt
i{t,...,N -1}

-s, 0

.

(41)

Remark 6: YØ0s() = max{YØ () - s, 0} for all   .

The following lemma provides properties of a conditional

expectation of YØts and a dynamic programming recursion.

Lemma 2 (A dynamic programming recursion): Let   

and s  R. For any t  TN , consider a conditional expectation

Fig. 5. Contours of computations of risk-sensitive safe sets for   {0.99, 0.05, 0.005, 0.0005, 5 ∑ 10-5} and r  {0.1, 0.2, . . . , 1.9} when using the state-augmentation method for two designs: a) baseline and b) replace valve with pump. This figure shows the contour shapes for these two designs in more detail by presenting more values of r compared to Fig. 4.

Fig. 6. This figure presents numerical results from Fig. 4 from a different perspective to depict how a different design may produce a differently shaped or sized contour, r  {0.2, 1, 1.8}. We placed the results for all designs in a single sub-plot pertaining to a particular . We used the state-augmentation method to obtain the results in the top row, and the under-approximation method ( = 20) to obtain the results in the bottom row. Note that the contours for design a (baseline, magenta solid lines) and design c (add outlet, black solid lines) overlap in regions where x2 has larger values.

of YØts, t ,s : S ◊ Z  R, t ,s(xt, zt) := E(YØts|Xt = xt, Zt = zt).
Then, t ,s is Borel-measurable, and it holds that 0 ,s(x, 0) = Ex max{YØ - s, 0} x  S

and (42)
N,s(xN , zN ) = max max{cN (xN ), zN } - s, 0 (44)
(43) for all (xN , zN )  S ◊ Z. Moreover, the following recursion

holds: for any t  T and (xt, zt)  S ◊ Z,

t ,s(xt, zt) = A Rd t+,s1(xt+1, zt+1) p(dwt|xt, ut) t(dut|xt, zt),
(45)

where xt+1 is shorthand for f (xt, ut, wt) and zt+1 is shorthand for max{c(xt, ut), zt} inside the integral in (45).
We prove Lemma 2 in the supplementary material due to

space limits. We will work up to proving that certain properties

are preserved under the recursion in Lemma 2, which will

allow us to guarantee the existence of an optimal control law.

For this, we require some intermediary results, whose proofs

are in the supplementary material.

Lemma 3 (Sequence of inc. bounded functions): Let X be a metric space. Suppose that J : X  R is lower semi-

continuous and bounded below by 0. Then, there exists a sequence {Jm : m  N} in C(X ) such that 0  Jm  J.9
Lemma 3 is similar to [35, Lemma 7.14 (a), p. 147], [33,

Theorem A6.6, pp. 390-391], and [32, Proposition A.2, p.

170]. However, Lemma 3 explicitly states that J and Jm for all m  N have a common lower bound, and the lemma applies

to settings in which J is not finite-valued. The existence

of a common lower bound allows us to use the Monotone

Convergence Theorem to prove a subsequent result that is

required (Lemma 4). To prove Lemma 3, a first step is to show

the desired statement under the restriction that J is finite. To

extend the result, a key tool is the function h : [0, +] 

[0,

 2

]

defined

by

h(x)

:=

arctan(x),

which

is

continuous

and

increasing, and whose inverse has these properties. The proof

of Lemma 3 is in the supplementary material.

We use Lemma 3 to prove Lemma 4.

Lemma 4 (Preserving lsc, boundedness below): Recall

that f and c are continuous functions, and p(dw|x, u) is a continuous stochastic kernel. If v : S ◊ Z  R is lower

semi-continuous (lsc) and bounded below by 0, then the function gv : S ◊ Z ◊ A  R defined by

gv(x, z, u) := Rd v f (x, u, w), max{z, c(x, u)} p(dw|x, u) (46)
is lsc and bounded below by 0. Proof: The function gv is bounded below by 0 because it
is defined by the integral of a non-negative Borel-measurable
function. To prove that gv is lsc, it suffices to show that if {(xn, zn, un) : n  N} is a sequence in S ◊ Z ◊ A converging to (x, z, u)  S ◊ Z ◊ A, then

lim inf
n

gv (xn ,

zn,

un)



gv (x,

z,

u).

(47)

By Lemma 3, there is a sequence {vm : m  N} in C(S ◊ Z)
such that 0  vm  v because S ◊ Z is a metric space and v : S ◊ Z  R is lsc and bounded below by 0.
Let m  N and n  N. Note that (Rd, BRd , p(∑|xn, un)) is a probability space, v  vm  0, and the functions

v f (xn, un, ∑), max{zn, c(xn, un)} : Rd  R vm f (xn, un, ∑), max{zn, c(xn, un)} : Rd  R

(48)

9The notation 0  Jm  J means that the following statements hold:
1) 0  Jm(x)  Jm+1(x)  J(x) for all x  X and m  N, and 2) limm Jm(x) = J(x) for all x  X .

are Borel-measurable. It follows that
gv(xn, zn, un)
:= Rd v f (xn, un, w), max{zn, c(xn, un)} p(dw|xn, un)  Rd vm f (xn, un, w), max{zn, c(xn, un)} p(dw|xn, un) =: gvm (xn, zn, un),
(49) where all the integrals exist. Since the inequality (182) was derived for arbitrary n  N and m  N, we have

gv(xn, zn, un)  gvm (xn, zn, un) m  N, n  N. (50)

For any m  N, it holds that vm  C(S ◊ Z), which implies that gvm : S ◊ Z ◊ A  R is continuous by using [35, Proposition 7.30, p. 145] with the continuity of f , c, max,
and p. By using (50) and the continuity of gvm , we have

lim inf
n

gv (xn ,

zn,

un)



lim inf
n

gvm (xn,

zn,

un)

(51)

= gvm (x, z, u) m  N. (52)

Next, we use a result that holds by an application of the Monotone Convergence Theorem [33, Theorem 1.6.7, p. 47]. Since v and vm for all m  N are Borel-measurable and 0  vm  v, after several steps, it follows that

lim
m

Rd vm

f (x, u, w), max{z, c(x, u)} p(dw|x, u)

gvm (x,u,w)

(53)

= Rd v f (x, u, w), max{z, c(x, u)} p(dw|x, u) .

gv (x,u,w)

By using (51)≠(53), we derive the desired result,

lim inf
n

gv (xn ,

zn,

un)



lim
m

gvm

(x,

z,

u)

=

gv (x,

u,

w).

(54)

There are subtle, measure-theoretic details that allow the above arguments to hold, and more information is in the supplementary material. Subsequently, we use the conclusions of Lemma 4 to show that key properties hold under a dynamic programming recursion.
Theorem 2 (Lower semi-continuity, measurable selection):
Recall the conditions of Assumption 1. Suppose that v : S ◊ Z  R is lower semi-continuous (lsc) and bounded below by 0. Define the function v : S ◊ Z ◊ A  R

v(x, z, u) := Rd v f (x, u, w), max{c(x, u), z} p(dw|x, u), (55)
and the function v : S ◊ Z  R

v(x,

z)

:=

inf
uA

v (x,

z,

u).

(56)

Then, v is bounded below by 0 and lsc. Moreover, there is a Borel-measurable function  : S ◊ Z  A such that

v(x, z) = v(x, z, (x, z)) (x, z)  S ◊ Z. (57) Proof: By Lemma 4, v is lsc and bounded below by 0. v is bounded below by 0 as a consequence of its definition (56) and v being bounded below by 0. Since S ◊ Z and
A are metric spaces, where A is compact, and since v : S ◊ Z ◊ A  R is lsc, the two remaining conclusions hold
by [35, Proposition 7.33, p. 153].

Now, we use the above results to prove Theorem 1. Recall
that we use the notation T := {0, 1, . . . , N - 1} and TN := {0, 1, . . . , N }.
Proof: [Theorem 1] To prove the first part, we proceed by induction. JNs (212a) is continuous because it is a composition of continuous functions. Thus, JNs is lsc. JNs is bounded below by 0 because max(a, 0)  0 for all a  R. Now, assume (the induction hypothesis) that for some t  T, Jts+1 : S ◊Z  R is lsc and bounded below by 0. Recall that Jts : S ◊ Z  R is defined by (212b):

Jts(xt,

zt)

:=

inf
ut A

vts(xt,

zt,

ut),

where vts : S ◊ Z ◊ A  R is defined by (212c):

vts(xt, zt, ut)
:= Rd Jts+1 f (xt, ut, wt), max{c(xt, ut), zt} p(dwt|xt, ut).
By Theorem 2, Jts is lsc and bounded below by 0. By induction, we conclude that Jts is lsc and bounded below by 0 for all t  TN . Also, for any t  T, there is a Borel-measurable function st : S ◊ Z  A such that

Jts(xt, zt) = vts(xt, zt, st (xt, zt)) (xt, zt)  S ◊ Z (58)

because vts is lsc and A is compact (see the proof of Theorem 2). Define Øs := (s0 , s1 , . . . , sN-1 )  , and recall Remark 3 for why st is a Borel-measurable stochastic kernel on A given S ◊Z. Now, to show (214), which we repeat below
for convenience,

J0s(x, 0) = VØ s(x) = ExØs max{YØ - s, 0} x  S,

we again proceed by induction. Let    and t  TN , and recall the definition (42)

t ,s(xt, zt) := E(YØts|Xt = xt, Zt = zt) (xt, zt)  S ◊ Z,

and the equality (43)

0 ,s(x, 0) = Ex max{YØ - s, 0} x  S

from Lemma 2. By using (43) and the definition of the infimum, we see that

0 ,s(x,

0) inf


0 ,s

(x,

0)

=

inf


Ex

max{YØ -s, 0}

=:VØ s(x)

(59)

for all x  S and   . To show (214), it suffices to show

that

t ,s  Jts   , t  TN ,

(60)

and

tØs,s = Jts t  TN .

(61)

We proceed by induction. The base case (t = N ) for (222) and (223) holds because we have

N,s(xN , zN ) = max max{cN (xN ), zN } - s, 0 = JNs (xN , zN )

(62)

for all (xN , zN )  S ◊ Z and    by the definition of JNs (212a) and (44) in Lemma 2. Now, assume (the induction hypothesis for (222)) that for some t  T, we have

t+,s1  Jts+1   .

(63)

We will show that t ,s  Jts for all    to prove (222) by induction. Let    and (xt, zt)  S ◊ Z be given. By (45)
in Lemma 2, it holds that

t ,s(xt, zt) = A Rd t+,s1(xt+1, zt+1) p(dwt|xt, ut) t(dut|xt, zt),
(64)

where xt+1 is shorthand for f (xt, ut, wt) and zt+1 is shorthand for max{c(xt, ut), zt}. Recall that t+,s1 and Jts+1 are Borel-measurable functions.10 In addition, since t+,s1  Jts+1  0, it follows that

t ,s(xt, zt)  A vts(xt, zt, ut) t(dut|xt, zt),

(65)

where vts is defined by (212c). Since Jts+1 is lsc and bounded below by 0, vts is lsc and bounded below by 0 (Lemma 4). Recall the definition (212b)

Jts(xt, zt)

:=

inf
ut A

vts(xt, zt, ut).

Consequently, the following properties hold:
1) vts(xt, zt, ut)  Jts(xt, zt)  0 for all ut  A (recall that we have fixed xt  S and zt  Z arbitrarily),
2) vts(xt, zt, ∑) : A  R is lsc, and 3) Jts(xt, zt) does not depend on ut  A.
These properties, together with (235), imply that

t ,s(xt, zt)  A Jts(xt, zt) t(dut|xt, zt) = Jts(xt, zt). (66)
Since    and (xt, zt)  S ◊ Z are arbitrary, we have shown that

t ,s(xt, zt)  Jts(xt, zt) (xt, zt)  S ◊ Z,   , (67)

which proves (222) by induction.

Next, we show (223) by induction, and recall that the

base case for (223) holds by (229). Assume (the induction

hypothesis for (223)) that for some t  T, it holds that

ttØØ+ss ,,1ss

= =

Jts+1. To complete the proof, we need to Jts. Let (xt, zt)  S ◊ Z be given. By

show that using the

recursion (45) with  equal to Øs := (s0 , . . . , sN-1 ), we

have

tØs,s(xt, zt) = A Rd tØ+s,1s(xt+1, zt+1) p(dwt|xt, ut) st (xt,zt)(dut),
(68)

where xt+1 is shorthand for f (xt, ut, wt) and zt+1 is shorthand for max{c(xt, ut), zt}. By the induction hypothesis for (223), it follows that

tØs,s(xt, zt)
= A Rd Jts+1(xt+1, zt+1) p(dwt|xt, ut) st (xt,zt)(dut). (69)
By substituting the definition of vts (212c), we have

tØs,s(xt, zt) = A vts(xt, zt, ut) st (xt,zt)(dut).

(70)

Since st (xt

,

st (xt,zt) concentrates zt), we find that

all

values

of

Ut

at

the

point

tØs,s(xt, zt) = vts xt, zt, st (xt, zt) = Jts(xt, zt), (71)

t+1,0s1Jtsi+s 1Biosrelol-wmeerasseumraib-cleonbtyinuLoeumsm, wah2ic. h implies that it is Borel-measurable.

where we use (58) to write the second equality. This proves
(223) by induction. Since (222) and (223) imply the desired
statement (214), the proof is complete.
REFERENCES
[1] M. P. Chapman, R. Bonalli, K. Smith, I. Yang, M. Pavone, and Claire J. Tomlin, "Risk-sensitive safety analysis using Conditional Value-at-Risk," arXiv:2101.12086, under revision for IEEE Transactions on Automatic Control, 2021.
[2] R. A. Howard and J. E. Matheson, "Risk-sensitive Markov decision processes," Management Science, vol. 18, no. 7, pp. 356≠369, 1972.
[3] D. H. Jacobson, "Optimal stochastic linear systems with exponential performance criteria and their relation to deterministic differential games," IEEE Transactions on Automatic Control, vol. 18, no. 2, pp. 124≠131, 1973.
[4] G. B. di Masi and L. Stettner, "Risk-sensitive control of discrete-time Markov processes with infinite horizon," SIAM Journal on Control and Optimization, vol. 38, no. 1, pp. 61≠78, 1999.
[5] P. Whittle, "Risk-Sensitive Linear/Quadratic/Gaussian Control," Advances in Applied Probability, vol. 13, no. 4, pp. 764≠777, 1981.
[6] A. Shapiro, "On a time consistency concept in risk averse multistage stochastic programming," Operations Research Letters, vol. 37, no. 3, pp. 143≠147, 2009.
[7] D. Kreps, "Decision problems with expected utility criteria. I: Upper and lower convergent utility," Mathematics of Operations Research, vol. 2, no. 1, pp. 45≠53, 1977.
[8] N. Ba®uerle and J. Ott, "Markov decision processes with Average-Valueat-Risk criteria," Mathematical Methods of Operations Research, vol. 74, no. 3, pp. 361≠379, 2011.
[9] N. Ba®uerle and U. Rieder, "More risk-sensitive Markov decision processes," Mathematics of Operations Research, vol. 39, no. 1, pp. 105≠ 120, 2014.
[10] N. Ba®uerle and A. Glauner, "Markov decision processes with recursive risk measures," European Journal of Operational Research, in press, 2021.
[11] N. Ba®uerle and Alexander Glauner, "Minimizing spectral risk measures applied to Markov decision processes," arXiv 2012.04521, 2020.
[12] D. P. Bertsekas and I. B. Rhodes, "On the minimax reachability of target sets and target tubes," Automatica, vol. 7, no. 2, pp. 233≠247, 1971.
[13] I. M. Mitchell, A. M. Bayen, and C. J. Tomlin, "A time-dependent Hamilton-Jacobi formulation of reachable sets for continuous dynamic games," IEEE Transactions on Automatic Control, vol. 50, no. 7, pp. 947≠957, 2005.
[14] K. Margellos and J. Lygeros, "Hamilton-Jacobi formulation for reachavoid differential games," IEEE Transactions on Automatic Control, vol. 56, no. 8, pp. 1849≠1861, 2011.
[15] M. Chen, et al., "Decomposition of reachable sets and tubes for a class of nonlinear systems," IEEE Transactions on Automatic Control, vol. 63, no. 11, pp. 3675≠3688, 2018.
[16] M. Chen and C. J. Tomlin, "Hamilton-Jacobi reachability: Some recent theoretical advances and applications in unmanned airspace management," Annual Review of Control, Robotics, and Autonomous Systems, vol. 1, no. 1, pp. 333≠358, 2018.
[17] A. Abate, M. Prandini, J. Lygeros, and S. Sastry, "Probabilistic reachability and safety for controlled discrete time stochastic hybrid systems," Automatica, vol. 44, no. 11, pp. 2724≠2734, 2008.
[18] S. Summers and J. Lygeros, "Verification of discrete time stochastic hybrid systems: A stochastic reach-avoid decision problem," Automatica, vol. 46, no. 12, pp. 1951≠1961, 2010.
[19] M. Kamgarpour, J. Ding, S. Summers, A. Abate, J. Lygeros, and C. Tomlin, "Discrete time stochastic hybrid dynamical games: Verification and controller synthesis," in Proceedings of the IEEE Conference on Decision and Control, 2011, pp. 6122≠6127.
[20] J. Ding, M. Kamgarpour, S. Summers, A. Abate, J. Lygeros, and C. Tomlin, "A stochastic games framework for verification and control of discrete time stochastic hybrid systems," Automatica, vol. 49, pp. 2665≠ 2674, 2013.
[21] I. Yang, "A dynamic game approach to distributionally robust safety specifications for stochastic systems," Automatica, vol. 94, pp. 94≠101, 2018.
[22] S. Samuelson and I. Yang, "Safety-aware optimal control of stochastic systems using Conditional Value-at-Risk," in Proceedings of the American Control Conference, 2018, pp. 6285≠6290.

[23] V. Borkar and R. Jain, "Risk-constrained Markov decision processes," IEEE Transactions on Automatic Control, vol. 59, no. 9, pp. 2574≠2579, 2014.
[24] W. B. Haskell and R. Jain, "A convex analytic approach to risk-aware Markov decision processes," SIAM Journal on Control and Optimization, vol. 53, no. 3, pp. 1569≠1598, 2015.
[25] P. Whittle, Risk-sensitive Optimal Control, Chichester: Wiley, 1990. [26] A. Shapiro, D. Dentcheva, and A. Ruszczyn¥ski, Lectures on Stochastic
Programming: Modeling and Theory. Society for Industrial and Applied Mathematics, Mathematical Programming Society, 2009. [27] M. P. Chapman, J. Lacotte, A. Tamar, D. Lee, K. M. Smith, V. Cheng, J. F. Fisac, S. Jha, M. Pavone, and C. J. Tomlin, "A risk-sensitive finitetime reachability approach for safety of stochastic dynamic systems," in Proceedings of the American Control Conference, 2019. [28] C. W. Miller and I. Yang, "Optimal control of Conditional Value-at-Risk in continuous time," SIAM Journal on Control and Optimization, vol. 55, no. 2, pp. 856≠884, 2017. [29] G. C. Pflug and A. Pichler, "Time-inconsistent multistage stochastic programs: Martingale bounds," European Journal of Operational Research, vol. 249, no. 1, pp. 155≠163, 2016. [30] G. C. Pflug and A. Pichler, "Time-consistent decisions and temporal decomposition of coherent risk functionals," Mathematics of Operations Research, vol. 41, no. 2, pp. 682≠699, 2016. [31] B. P. G. Van Parys, D. Kuhn, P. J. Goulart, and M. Morari, "Distributionally robust control of constrained stochastic systems," IEEE Transactions on Automatic Control, vol. 61, no. 2, pp. 430≠442, 2015. [32] O. Herna¥ndez-Lerma and J. B. Lasserre, Discrete-Time Markov Control Processes: Basic Optimality Criteria, New York: Springer, 1996. [33] R. B. Ash, Real Analysis and Probability, New York: Academic Press, 1972. [34] A. Ruszczyn¥ski, "Risk-averse dynamic programming for Markov decision processes," Mathematical Programming, vol. 125, no. 2, pp. 235≠ 261, 2010. [35] D. P. Bertsekas and S. Shreve, Stochastic Optimal Control: The DiscreteTime Case, Belmont: Athena Scientific, 2004. [36] A. Mullapudi, B. P. Wong, and B. Kerkez, "Emerging investigators series: Building a theory for smart stormwater systems," Environmental Science: Water Research & Technology, vol. 3, no. 1, pp. 66≠77, 2017. [37] M. P. Chapman, K. M. Smith, V. Cheng, D. L. Freyberg, and C. J. Tomlin, "Reachability analysis as a design tool for stormwater systems," in Proceedings of the IEEE Conference on Technologies for Sustainability, pp. 1≠8, 2018. [38] L. A. Rossman, Storm Water Management Model User's Manual, Version 5.0. National Risk Management Research Laboratory, Office of Research and Development, US EPA, Cincinnati, 2010. [39] D. Bertsekas, "Multiagent reinforcement learning: Rollout and policy iteration," IEEE/CAA Journal of Automatica Sinica, vol. 8, no. 2, pp. 249≠272, 2021. [40] G. Petrucci, E. Rioust, J.-F. Deroubaix, and B. Tassin, "Do stormwater source control policies deliver the right hydrologic outcomes?," Journal of Hydrology, vol. 485, pp. 188≠200, 2013. [41] C. H. Emerson, C. Welty, and R. G. Traver, "Watershed-Scale Evaluation of a System of Storm Water Detention Basins," Journal of Hydrologic Engineering, vol. 10, no. 3, pp. 237≠242, 2005.

Margaret Chapman is an Assistant Professor in the Department of Electrical and Computer Engineering at the University of Toronto, where she joined in July 2020. Her research focuses on risk-sensitive and stochastic control, with emphasis on safety analysis and applications in healthcare and sustainable cities. She earned her BS degree with Distinction and MS degree in Mechanical Engineering from Stanford University in 2012 and 2014, respectively. Margaret earned her PhD degree in Electrical Engineering and Computer Sciences from the University of California Berkeley (UC Berkeley) in August 2020. In 2021, Margaret received a Leon O. Chua Award for outstanding achievement in nonlinear science from her doctoral alma mater. In addition, she is a recipient of a US National Science Foundation Graduate Research Fellowship, Berkeley Fellowship for Graduate Study, a Fulbright Scholarship (granted by the US Department of State), and a Stanford University Terman Engineering Scholastic Award.

Kevin Smith is a Ph.D. candidate in Environmental and Water Resources Engineering at Tufts University and a recipient of the NSF Integrative Graduate Education and Research Traineeship (IGERT) on Water and Diplomacy. Kevin also works as a product developer at OptiRTC, Inc., where he is responsible for developing flexible real-time systems for the continuous monitoring and adaptive control of stormwater infrastructure. Kevin earned his B.A. in Environmental Studies from Oberlin College and his B.S. in Earth and Environmental Engineering from Columbia University. Kevin's research seeks to understand the opportunities and risks associated with semi-autonomous civil infrastructure when considered as a technology for mediating environmental conflicts. Kevin recently co-edited a research manuscript on interdisciplinary approaches to addressing complex water issues, Interdisciplinary Collaboration for Water Diplomacy: A Principled and Pragmatic Approach, with his advisor Dr. Shafiqul Islam.

Michael Fauss (Member, IEEE) received the Dipl.-Ing. degree in electrical engineering from Technische Universita® t Mu® nchen, Munich, Germany, in 2010 and the Dr.-Ing. degree in electrical engineering from Technische Universita® t Darmstadt, Darmstadt, Germany, in 2016. In November 2011, he joined the Signal Processing Group, Technische Universita® t Darmstadt. In September 2019, he joined Prof. H. Vincent Poor's Group, Princeton University, Princeton, NJ, USA, as a Postdoc on a research grant by the German Research Foundation Bonn, Germany. His current research interests include statistical robustness, sequential detection and estimation, and the role of similarity measures in statistical inference. In 2017, he was the recipient of the Dissertation Award of the German Information Technology Society for his Ph.D. thesis on robust sequential detection.

H. Vincent Poor (Fellow, IEEE) received the Ph.D. degree in EECS from Princeton University, Princeton, NJ, USA, in 1977. From 1977 until 1990, he was on the faculty of the University of Illinois at Urbana-Champaign, Urbana, IL, USA. Since 1990, he has been on the faculty at Princeton, where he is the Michael Henry Strater University Professor. From 2006 until 2016, he was the Dean of Princeton's School of Engineering and Applied Science. He has also held visiting appointments with several other institutions, including most recently at Berkeley and Cambridge. His research interests in these areas is the forthcoming book Machine Learning and Wireless Communications, Cambridge University Press, in 2021. His research interests include the areas of information theory, machine learning and network science, and their applications in wireless networks, energy systems and related fields. Dr. Poor is a Member of the National Academy of Engineering and the National Academy of Sciences, and is a Foreign Member of the Chinese Academy of Sciences, the Royal Society and other national and international academies. Recent recognition of his work includes the 2017 IEEE Alexander Graham Bell Medal, the 2019 ASEE Benjamin Garver Lamme Award, a D.Sc. honoris causa from Syracuse University, awarded in 2017, and a D.Eng. honoris causa from the University of Waterloo, awarded in 2019.

SUPPLEMENTARY MATERIAL This document provides technical details to accompany the main paper above.

VIII. LEMMA 1 (EXISTENCE OF A MINIMIZER IN Z)

A. Statement of Lemma 1

Let x  S and   (0, 1]. Recall that Z := [0, cØ], where cØ  R+. Let G :   R be measurable relative to B and BR, such that G()  Z for all   . Then, we have

inf
sR

s

+

1 

inf


Ex

max{G - s, 0}

=

min
sZ

s

+

1 

inf


Ex

max{G - s, 0} ,

(72)

where we write min to indicate that the minimum is attained by some sx,  Z.

B. Proof of Lemma 1 Let x  S and   (0, 1]. We would like to show that

inf
sR

s+

1 

vs(x)

=

min
s[0,c]

s+

1 

vs(x),

(73)

where

vs(x)

:=

inf


Ex (max{G

-

s,

0}).

(74)

To show (73), define Lx : R  R as follows:

Lx (s)

:=

s

+

1 

vs

(x).

(75)

First, let s  c, which implies that G() - s  0 for all   . Hence, it holds that

vs(x)

sc

=

inf


Ex

(0)

=

0,

(76)

and consequently,

Lx (s)

sc

=

s

+

1 

0

=

s



c

=

Lx (c),

(77)

which implies that11

inf Lx (s) = Lx (cØ).

(78)

s[cØ,+)

Analogously, s  0 implies that G() - s  0 for all   , so we have

vs(x)

s0

=

inf


Ex (G

-

s)

=

inf


Ex (G)

-

s,

(79)

and consequently,

Lx (s)

s0

=

s

+

1 

inf


Ex

(G)

-

s

=

1 

inf


Ex

(G)

-

(1

-

)s

(80)



1 

inf


Ex

(G)

(81)

= Lx (0),

(82)

where the inequality (81) holds since 0 <   1 and s  0. Since Lx (s)  Lx (0) for all s  0, we have

inf
s(-,0]

Lx (s)

=

Lx (0).

(83)

From (78) and (83), it follows that

inf
sR

Lx (s)

=

inf
s(-,0](0,cØ)[cØ,+)

Lx (s)

=

inf
s{0}(0,cØ){cØ}

Lx (s)

=

inf
s[0,cØ]

Lx (s).

(84) (85) (86)

11We have infs[cØ,+) Lx (s)  Lx (cØ)  infs[cØ,+) Lx (s) by (77) and by the definition of the infimum.

Next, we show that Lx (s) is continuous in s. Note that vs(x) is continuous in s since it is Lipschitz continuous. Please see the argument below for more details: for any s  R and   0, we have

|vs+(x) - vs(x)| =

inf


Ex

(max{G

-

s

-

,

0})

-

inf


Ex

(max{G

-

s,

0})

(87)

=

inf


Ex

(max{G

-

s,

0})

-

inf


Ex

(max{G

-

s

-

,

0})

(88)



inf


Ex

(max{G

-

s,

0})

-

inf


Ex

(max{G

-

s,

0})

-



(89)

= ,

(90)

where the second equality holds since vs(x) is decreasing in s. Thus, vs(x) is Lipschitz continuous in s. Moreover, we have

that

|Lx (s

+

)

-

Lx (s)|



(

1+ 

).

(91)

The steps are below:

Lx (s

+

)

-

Lx (s)

=

s

+



+

1 

vs+

(x)

-

s

+

1 

vs(x)

=



+

1 

vs+(x) - vs(x)

.

(92)

Thus,

|Lx (s

+

)

-

Lx (s)|

=

|

+

1 

vs+(x) - vs(x)

|





+

1 

|vs+

(x)

-

vs(x)|





+

1 



=

(1

+

1 

)

(93)

=

(

 

+

1 

)

=

(

+1 

).

Since [0, c] is compact, continuity of Lx (s) in s implies that the infimum in (84) is attained.

Some of our results rely on the derivation of the expectation with respect to the probability measure Px, which is provided in the following section.

IX. DERIVATION OF EXPECTATION WITH RESPECT TO Px

It is a well-established result, e.g., see [35, Proposition 7.28, pp. 140-141], that the system model considered in this paper allows the construction of a unique probability measure, which will be used to evaluate expectations of costs that are incurred as the system evolves over time.

∑ Let x  S and   . x : BS  {0, 1} is the Dirac measure on (S, BS) concentrated at x. ∑ S, Z, and A are Borel spaces. S and A are Borel spaces by assumption. Z := [0, cØ] is a closed subset of R, which implies

that Z  BR. Since R with the Euclidean metric is a complete and separable metric space, and Z is a Borel-measurable subset of R, Z is a Borel space.

∑ Q(dxt+1|xt, ut) is a Borel-measurable stochastic kernel on S given S ◊ A (for details, see [35], the bottom of p. 189 to

the top of p. 190).

∑ max{zt,c(xt,ut)}(dzt+1) is a Borel-measurable stochastic kernel on Z given S ◊ Z ◊ A. Let  : S ◊ Z ◊ A  P(Z) be defined as follows:

(xt, zt, ut) := max{zt,c(xt,ut)}(dzt+1).

(94)

max{zt,c(xt,ut)} is Borel-measurable if and only if  is Borel-measurable. The function (xt, zt, ut)  max{zt, c(xt, ut)} is continuous, and the mapping  : Z  P(Z)

(z) := z,

(95)

where z is the Dirac measure on (Z, BZ ) concentrated at z, is continuous by [35, Corollary 7.21.1, p. 130].12 Since  is a composition of continuous functions,  is Borel-measurable. ∑ t(dut|xt, zt) is a Borel-measurable stochastic kernel on A given S ◊ Z by the definition of the class of policies .
The text in blue denotes the symbols used in [35, Proposition 7.28, p. 140], and the text in black denotes our symbols:
∑ S, Z, A, S, Z, A, . . . is a sequence of Borel spaces, ∑ Y1 := X1 := S
x1 = x0

12A homeomorphism is continuous. Z is a metric space, which implies that Z is a metrizable space.

∑ Y2 := X1 ◊ X2 := S ◊ Z (x1, x2) = (x0, z0)
∑ Y3 := X1 ◊ X2 ◊ X3 := S ◊ Z ◊ A (x1, x2, x3) = (x0, z0, u0)
∑ Y4 := X1 ◊ X2 ◊ X3 ◊ X4 := S ◊ Z ◊ A ◊ S (x1, x2, x3, x4) = (x0, z0, u0, x1)
∑ Y5 := X1 ◊ X2 ◊ X3 ◊ X4 ◊ X5 := S ◊ Z ◊ A ◊ S ◊ Z (x1, x2, x3, x4, x5) = (x0, z0, u0, x1, z1)
∑ Y6 := X1 ◊ X2 ◊ X3 ◊ X4 ◊ X5 ◊ X6 := S ◊ Z ◊ A ◊ S ◊ Z ◊ A (x1, x2, x3, x4, x5, x6) = (x0, z0, u0, x1, z1, u1)
∑ ... ∑ YM :=  := (S ◊ Z ◊ A)N ◊ S ◊ Z.
(x1, x2, x3, . . . , xM-4, xM-3, xM-2, xM-1, xM ) = (x0, z0, u0, . . . , xN-1, zN-1, uN-1, xN , zN ) ∑ p(dx1) := x(dx0). ∑ q1(dx2|y1) := q1(d x2|x1) := 0(dz0).

z0 |x0
∑ q2(dx3|y2) := q2(d x3|x1, x2) := 0(du0|x0, z0).

u0 |x0 ,z0
∑ q3(dx4|y3) := q3(d x4|x1, x2, x3) := Q(dx1|x0, u0).

x1 |x0 ,z0 ,u0
∑ q4(dx5|y4) := q4(d x5|x1, x2, x3, x4) := max{c(x0,u0),z0}(dz1).
z1 |x0 ,z0 ,u0 ,x1
∑ q5(dx6|y5) := q5(d x6|x1, x2, x3, x4, x5) := 1(du1|x1, z1).

∑ ...

u1 |x0 ,z0 ,u0 ,x1 ,z1

∑ qM-5(dxM-4|yM-5) := qM-5(d xM-4|x1, x2, x3, . . . , xM-7, xM-6, xM-5) := Q(dxN-1|xN-2, uN-2).

xN -1 |x0 ,z0 ,u0 ,...,xN -2 ,zN -2 ,uN -2
∑ qM-4(dxM-3|yM-4) := qM-4(d xM-3|x1, x2, x3, . . . , xM-7, xM-6, xM-5, xM-4) :=

zN -1 |x0 ,z0 ,u0 ,...,xN -2 ,zN -2 ,uN -2 ,xN -1
max{c(xN-2,uN-2),zN-2}(dzN -1). ∑ qM-3(dxM-2|yM-3) := qM-3(d xM-2|x1, x2, x3, . . . , xM-4, xM-3) := N-1(duN-1|xN-1, zN-1).

uN -1 |x0 ,z0 ,u0 ,...,xN -1 ,zN -1
∑ qM-2(dxM-1|yM-2) := qM-2(d xM-1|x1, x2, x3, . . . , xM-4, xM-3, xM-2) := Q(dxN |xN-1, uN-1).

xN |x0,z0,u0,...,xN-1,zN-1,uN-1
∑ qM-1(dxM |yM-1) := qM-1(d xM |x1, x2, x3, . . . , xM-4, xM-3, xM-2, xM-1) := max{c(xN-1,uN-1),zN-1}(dzN ).
zN |x0,z0,u0,...,xN-1,zN-1,uN-1,xN
As explained above, the above stochastic kernels are Borel-measurable. Thus, by [35, Proposition 7.28, p. 140], there exists a unique probability measure Px  P() such that

Px(BX0 ◊ BZ0 ◊ BU0 ◊ ∑ ∑ ∑ ◊ BXN-1 ◊ BZN-1 ◊ BUN-1 ◊ BXN ◊ BZN )

=

∑∑∑

BX0 BZ0 BU0 BX1 BZ1 BU1

BXN-1 BZN-1 BUN-1 BXN BZN

max{c(xN-1,uN-1),zN-1}(dzN ) Q(dxN |xN-1, uN-1)

N-1(duN-1|xN-1, zN-1) max{c(xN-2,uN-2),zN-2}(dzN-1) Q(dxN-1|xN-2, uN-2)

∑∑∑

1(du1|x1, z1) max{c(x0,u0),z0}(dz1) Q(dx1|x0, u0) 0(du0|x0, z0) 0(dz0) x(dx0)

(96a)

for all BXt  BS, BZt  BZ , BUt  BA, t  T := {0, 1, . . . , N - 1}, BXN  BS, and BZN  BZ . A Cartesian product of Borel-measurable sets, e.g.,

B := BX0 ◊ BZ0 ◊ BU0 ◊ ∑ ∑ ∑ ◊ BXN-1 ◊ BZN-1 ◊ BUN-1 ◊ BXN ◊ BZN , is called a measurable rectangle. Note that B  B. For brevity, we use the notation
QØ(d(xi+1, zi+1)|xi, zi, ui) := max{c(xi,ui),zi}(dzi+1) Q(dxi+1|xi, ui) i  T

(96b)

to write Px(B) more concisely as follows:
Px(B) = QØ(d(xN , zN )|xN-1, zN-1, uN-1)
B
N-1(duN-1|xN-1, zN-1) QØ(d(xN-1, zN-1)|xN-2, zN-2, uN-2) ∑∑∑ 1(du1|x1, z1) QØ(d(x1, z1)|x0, z0, u0) 0(du0|x0, z0) 0(dz0) x(dx0).
Moreover, if G :   R is Borel-measurable and non-negative,13 then

(96c)

G() dPx()


=

∑∑∑

G(x0, z0, u0, x1, z1, u1, . . . , xN-1, zN-1, uN-1, xN , zN )

S Z AS Z A

S Z AS Z

QØ(d(xN , zN )|xN-1, zN-1, uN-1)

(97)

N-1(duN-1|xN-1, zN-1) QØ(d(xN-1, zN-1)|xN-2, zN-2, uN-2)

∑∑∑

1(du1|x1, z1) QØ(d(x1, z1)|x0, z0, u0) 0(du0|x0, z0) 0(dz0) x(dx0).

We define the expectation of G with respect to Px as Ex(G) :=  G() dPx(). We will use the definition of the expectation with respect to Px to establish a dynamic programming recursion for our problem setting.

X. LEMMA 2 (A DYNAMIC PROGRAMMING RECURSION)

A. Statement of Lemma 2

Let    and s  R. For any t  TN := {0, 1, . . . , N }, consider a conditional expectation of YØts, t ,s : S ◊ Z  R,

t ,s(xt, zt) := E(YØts|Xt = xt, Zt = zt).

(98)

Then, t ,s is Borel-measurable, and the following statements hold:

0 ,s(x, 0) = Ex max{YØ - s, 0} x  S

(99)

and

N,s(xN , zN ) = max max{cN (xN ), zN } - s, 0 (xN , zN )  S ◊ Z.

(100)

Moreover, the following recursion holds: for any t  T := {0, 1, . . . , N - 1},

t ,s(xt, zt) =

t+,s1 f (xt, ut, wt), max{c(xt, ut), zt} p(dwt|xt, ut) t(dut|xt, zt).

A Rd

for all (xt, zt)  S ◊ Z.

(101)

B. Proof of Lemma 2

Let   , x  S, and s  R. Recall that for any  = (x0, z0, u0, . . . , xN-1, zN-1, uN-1, xN , zN )   and t  T, we

define

YØ () := max cN (xN ), max c(xi, ui) ,
iT

YØts() := max

max

cN (xN ), max c(xi, ui), zt
i{t,...,N -1}

- s, 0 ,

YØNs () := max max{cN (xN ), zN } - s, 0 .

YØ0s = max{YØ - s, 0} is a random variable on a probability space (, B, Px) because YØ0s :   R is measurable relative to B and BR, and Px is a probability measure on the measurable space (, B). The form of Px on measurable rectangles in B is given by (96).

We denote the probability measure induced by (Xt, Zt) by the notation PXt,Zt . PXt,Zt is an element of P(S ◊ Z) and takes the following form on measurable rectangles in BS◊Z :

PXt,Zt (BX ◊ BZ ) := Px {(Xt, Zt)  BX ◊ BZ } BX  BS , BZ  BZ .

(102)

13If G :   R is non-negative and Borel-measurable, then G-() := max{-G(), 0} = 0 for all   , which implies that  G-()dPx() < +.

The event that Xt is in BX  BS and Zt is in BZ  BZ is defined by:

{(Xt, Zt)  BX ◊ BZ } :=    : Xt()  BX , Zt()  BZ ,

(103)

which is an element of B since it is a measurable rectangle in B. We note the following facts:
1) For any t  TN , YØts is a random variable on (, B, Px) since it is a function from  to R that is measurable relative to B and BR.
2) The function (Xt, Zt) :   S ◊ Z is measurable relative to B and BS◊Z , which means that it is a random object. 3) The integral Ex(YØts) :=  YØts()dPx() exists (i.e., is not of the form + - ) because YØts()  0 for all   . Thus, by [33, Thm. 6.3.3, p. 245], there is a function ht ,s : S ◊ Z  R, which is measurable relative to BS◊Z and BR , such that

YØts() dPx() =

ht ,s(xt, zt) dPXt,Zt (xt, zt) BX  BS , BZ  BZ .

{(Xt,Zt)BX ◊BZ }

BX ◊BZ

(104a)

(If BX  BS and BZ  BZ , then BX ◊ BZ  BS◊Z . BS◊Z contains sets that need not be of the form BX ◊ BZ .) We define

t ,s(xt, zt) := ht ,s(xt, zt) := E(YØts|Xt = xt, Zt = zt),

(104b)

which is unique for almost every (xt, zt)  S ◊ Z with respect to PXt,Zt [33, Thm. 6.3.3, p. 245].

For t = N , for any (xN , zN )  S ◊ Z, we have

N,s(xN , zN ) := E(YØNs |XN = xN , ZN = zN ) := E max max{cN (XN ), ZN } - s, 0 |XN = xN , ZN = zN
= max max{cN (xN ), zN } - s, 0 .

(105)

We will make the above argument more rigorous by using (104). Intuitively, the induced probability measure PXN ,ZN encodes the process starting at time 0 and ending where (XN , ZN ) may be realized. Formally, for any BX  BS and BZ  BZ , we
have PXN ,ZN (BX ◊ BZ ) := Px    : XN ()  BX , ZN ()  BZ = Px (x0, z0, u0, . . . , xN-1, zN-1, uN-1, xN , zN )   : xN  BX , zN  BZ
= Px (S ◊ Z ◊ A)N ◊ BX ◊ BZ

=

QØ(d(xN , zN )|xN-1, zN-1, uN-1)

(S◊Z◊A)N ◊BX ◊BZ

N-1(duN-1|xN-1, zN-1) QØ(d(xN-1, zN-1)|xN-2, zN-2, uN-2)

∑∑∑

1(du1|x1, z1) QØ(d(x1, z1)|x0, z0, u0) 0(du0|x0, z0) 0(dz0) x(dx0)

(106)

=

dPXN ,ZN (xN , zN ).

BX ◊BZ

We evaluate the left-hand-side of (104a) and use (106) to deduce the form of N,s(xN , zN ):

YØNs () dPx()
{(XN ,ZN )BX ◊BZ }

=

max max{cN (xN ), zN } - s, 0 QØ(d(xN , zN )|xN-1, zN-1, uN-1)

(S◊Z◊A)N ◊BX ◊BZ

N-1(duN-1|xN-1, zN-1) QØ(d(xN-1, zN-1)|xN-2, zN-2, uN-2)

∑∑∑ 1(du1|x1, z1) QØ(d(x1, z1)|x0, z0, u0) 0(du0|x0, z0) 0(dz0) x(dx0)

(107)

=

max max{cN (xN ), zN } - s, 0 dPXN ,ZN (xN , zN ).

BX ◊BZ

We see that definition of

hNN,,ss(x(1N0,4zbN).)

=

max

max{cN (xN ), zN } - s, 0

by observing the right-hand-side of (104a) and by using the

We will use a similar procedure to deduce the form of t ,s for any t  {0, 1, . . . , N - 1}. Intuitively, t ,s(xt, zt) is

the expectation of YØts when the process starts at time t with the initialization (xt, zt) and proceeds to time N . Formally, we will derive that for any (xt, zt)  S ◊ Z,

t ,s(xt, zt) =

∑∑∑

max

AS Z A

S Z AS Z

QØ d(xN , zN )|xN-1, zN-1, uN-1

max

cN (xN ), max c(xi, ui), zt
i{t,...,N -1}

N-1(duN-1|xN-1, zN-1) QØ d(xN-1, zN-1)|xN-2, zN-2, uN-2

∑∑∑

- s, 0

t+1(dut+1|xt+1, zt+1) max{c(xt,ut),zt}(dzt+1) Q(dxt+1|xt, ut) t(dut|xt, zt).

Note that (108) with t = 0 implies that

(108)

0 ,s(x, 0) =

0 ,s(x0, z0) 0(dz0) x(dx0) = Ex max{YØ - s, 0} .

SZ

(109)

Now, we will derive (108). Let t  {0, 1, . . . , N - 1}. For any BX  BS and BZ  BZ , we have

PXt,Zt (BX ◊ BZ ) := Px    : Xt()  BX , Zt()  BZ = Px (x0, z0, u0, . . . , xN-1, zN-1, uN-1, xN , zN )   : xt  BX , zt  BZ = Px (S ◊ Z ◊ A)t ◊ BX ◊ BZ ◊ A ◊ (S ◊ Z ◊ A)N-t-1 ◊ S ◊ Z

=

QØ(d(xN , zN )|xN-1, zN-1, uN-1)

(S◊Z◊A)t◊BX ◊BZ ◊A◊(S◊Z◊A)N-t-1◊S◊Z

N-1(duN-1|xN-1, zN-1) QØ(d(xN-1, zN-1)|xN-2, zN-2, uN-2)

∑∑∑

1(du1|x1, z1) QØ(d(x1, z1)|x0, z0, u0) 0(du0|x0, z0) 0(dz0) x(dx0).

(110)

Several of the inner integrals, starting with S◊Z QØ(d(xN , zN )|xN-1, zN-1, uN-1) = 1, evaluate to 1, and the expression for PXt,Zt (BX ◊ BZ ) simplifies as follows:

PXt,Zt (BX ◊ BZ ) =

QØ(d(xt, zt)|xt-1, zt-1, ut-1)

(S◊Z◊A)t◊BX ◊BZ

∑∑∑

1(du1|x1, z1) QØ(d(x1, z1)|x0, z0, u0) 0(du0|x0, z0) 0(dz0) x(dx0)

(111)

=

dPXt,Zt (xt, zt).

BX ◊BZ

The induced We evaluate

probability measure PXt,Zt the left-hand-side of (104a)

encodes and use

the process starting (111) to deduce the

at time 0 and ending where Xt form of t ,s(xt, zt) (104b):

and

Zt

may

be

realized.

YØts() dPx()
{(Xt,Zt)BX ◊BZ }

=

max max cN (xN ), max c(xi, ui), zt - s, 0

(S◊Z◊A)t◊BX ◊BZ ◊A◊(S◊Z◊A)N-t-1◊S◊Z

i{t,...,N -1}

QØ(d(xN , zN )|xN-1, zN-1, uN-1) N-1(duN-1|xN-1, zN-1) QØ(d(xN-1, zN-1)|xN-2, zN-2, uN-2)

∑∑∑

t(dut|xt, zt) QØ(d(xt, zt)|xt-1, zt-1, ut-1)

∑∑∑

1(du1|x1, z1) QØ(d(x1, z1)|x0, z0, u0) 0(du0|x0, z0) 0(dz0) x(dx0)

(112)

=

max max cN (xN ), max c(xi, ui), zt - s, 0

BX ◊BZ A◊(S◊Z◊A)N-t-1◊S◊Z

i{t,...,N -1}

QØ(d(xN , zN )|xN-1, zN-1, uN-1) N-1(duN-1|xN-1, zN-1) QØ(d(xN-1, zN-1)|xN-2, zN-2, uN-2)

∑∑∑

t(dut|xt, zt) dPXt,Zt (xt, zt).

t ,s(xt, zt) is given by the expression (indicated in blue text) inside the integral over the set BX ◊ BZ with respect to the measure PXt,Zt .

Having derived (108), the last part of the proof is to derive the recursion. Let t  {0, 1, . . . , N - 2}, and note that

YØts() := max

max

cN (xN ), max c(xi, ui), zt
i{t,...,N -1}

- s, 0

= max max cN (xN ), max c(xi, ui), max{c(xt, ut), zt} - s, 0
i{t+1,...,N -1}

for all  = (x0, z0, u0, . . . , xN-1, zN-1, uN-1, xN , zN )  . We substitute (113) into (108) to derive:

(113)

t ,s(xt, zt) =

∑∑∑

AS Z A

S Z AS Z

max max cN (xN ), max c(xi, ui), max{c(xt, ut), zt} - s, 0
i{t+1,...,N -1}
QØ d(xN , zN )|xN-1, zN-1, uN-1 N-1(duN-1|xN-1, zN-1) QØ d(xN-1, zN-1)|xN-2, zN-2, uN-2
∑∑∑

(114)

t+1(dut+1|xt+1, zt+1) max{c(xt,ut),zt}(dzt+1) Q(dxt+1|xt, ut) t(dut|xt, zt).

The Dirac measure max{c(xt,ut),zt} concentrates all possible realizations zt+1  Z at the point max{c(xt, ut), zt}  Z. Thus, we substitute max{c(xt, ut), zt} for zt+1 and remove the integral associated with max{c(xt,ut),zt}. We obtain an expression in which the term zt+1 does not appear:

t ,s(xt, zt) =

∑∑∑

ASA

S Z AS Z

max max cN (xN ), max c(xi, ui), max{c(xt, ut), zt} - s, 0
i{t+1,...,N -1}
QØ d(xN , zN )|xN-1, zN-1, uN-1 N-1(duN-1|xN-1, zN-1) QØ d(xN-1, zN-1)|xN-2, zN-2, uN-2
∑∑∑

t+1(dut+1|xt+1, max{c(xt, ut), zt}) Q(dxt+1|xt, ut) t(dut|xt, zt).

Next, let xt+1  S and ut  A. We write (108) for t + 1 evaluated at the point (xt+1, max{c(xt, ut), zt}):

(115)

t+,s1(xt+1, max{c(xt, ut), zt}) = ∑ ∑ ∑

A

S Z AS Z

max max cN (xN ), max c(xi, ui), max{c(xt, ut), zt} - s, 0
i{t+1,...,N -1}
QØ d(xN , zN )|xN-1, zN-1, uN-1 N-1(duN-1|xN-1, zN-1) QØ d(xN-1, zN-1)|xN-2, zN-2, uN-2

∑∑∑

t+1(dut+1|xt+1, max{c(xt, ut), zt}).

We see that t+,s1(xt+1, max{c(xt, ut), zt}) appears inside the expression for t ,s(xt, zt) (115):

(116)

t ,s(xt, zt) =

t+,s1(xt+1, max{c(xt, ut), zt}) Q(dxt+1|xt, ut) t(dut|xt, zt).

AS

Finally, by using the definition of Q and a change of variable, we have

(117)

t ,s(xt, zt) =

t+,s1(f (xt, ut, wt), max{c(xt, ut), zt}) p(dwt|xt, ut) t(dut|xt, zt).

A Rd

(118)

We have derived the recursion for any t  {0, 1, . . . , N - 2}. A similar procedure shows that the recursion is also valid when t = N - 1. By using (108) with t = N - 1 and the definition of QØ, we have that for any (xN-1, zN-1)  S ◊ Z,

N,-s 1(xN-1, zN-1) =

max max cN (xN ), c(xN-1, uN-1), zN-1 - s, 0

ASZ

max{c(xN-1,uN-1),zN-1}(dzN ) Q(dxN |xN-1, uN-1) N-1(duN-1|xN-1, zN-1)

=

max max cN (xN ), c(xN-1, uN-1), zN-1 - s, 0

AS

Q(dxN |xN-1, uN-1) N-1(duN-1|xN-1, zN-1).

(119)

The last line holds because the inner-most function does not depend on zN . Recall that N,s(xN , zN ) = max max{cN (xN ), zN } - s, 0 for all (xN , zN )  S ◊ Z, which we will use to simplify (119). Observe that for all
xN  S, xN-1  S, zN-1  Z, and uN-1  A, we have

N,s(xN , max{c(xN-1, uN-1), zN-1}) = max max cN (xN ), max{c(xN-1, uN-1), zN-1} - s, 0 = max max cN (xN ), c(xN-1, uN-1), zN-1 - s, 0 .

(120)

We substitute (120) into (119) to find that

N,-s 1(xN-1, zN-1)

=

N,s(xN , max{c(xN-1, uN-1), zN-1}) Q(dxN |xN-1, uN-1) N-1(duN-1|xN-1, zN-1)

AS

=

N,s f (xN-1, uN-1, wN-1), max{c(xN-1, uN-1), zN-1} p(dwN-1|xN-1, uN-1) N-1(duN-1|xN-1, zN-1),

A Rd

(121)

where we use the definition of Q and a change of variable to write the last line. This shows that the recursion also holds

when t = N - 1 and completes our proof.

The next result indicates that certain properties are preserved under our dynamic programming recursion.

XI. LEMMA 3 (EXISTENCE OF A SEQUENCE OF INCREASING, CONTINUOUS FUNCTIONS WITH A COMMON LOWER BOUND)
A. Statement of Lemma 3
Let (X , ) be a metric space. Suppose that J : X  R is lower semi-continuous and bounded from below by 0. Then, there exists a sequence {Jm : m  N} in C(X ) such that 0  Jm  J, i.e.,
1) 0  Jm(x)  Jm+1(x)  J(x) for all x  X and m  N, and 2) limm Jm(x) = J (x) for all x  X .

B. Proof of Lemma 3 This proof uses some techniques, which are also used by [35, Lemma 7.14 (a), p. 147] and [33, Theorem A6.6, pp. 390-391].

The function J : X  R is lower semi-continuous and bounded below by 0 if and only if the following properties hold:
1) For any sequence {xn : n  N} in X converging to x  X (i.e., (xn, x)  0 as n  ), we have

lim inf
n

J

(xn)



J (x),

(122)

and

2) J(x)  0 for all x  X .

There are two cases to consider. The first case is that J(x) = + for all x  X .14 In this case, let Jm : X  R be defined by

Jm(x) = m x  X , m  N,

(123)

which implies that

0  Jm(x)  Jm+1(x)  J (x) x  X , m  N,

(124)

because 0  m  m + 1  + for all m  N. For any m  N, Jm is constant and finite, and therefore Jm is continuous and bounded; i.e., Jm  C(X ). Finally,

lim Jm(x) = lim m = + = J(x) x  X ,

m

m

which completes the proof in the first case.

(125)

Now, in the second case, there exists an x0  X such that J(x0) < +. Define gm(x) := inf J(y) + m(x, y) x  X , m  N.
yX 14We know that J(x) > - for all x  X because J is bounded below.

(126)

Since m(x, y)  0 for all (x, y)  X ◊ X and m  N, and since J(y)  0 for all y  X , we have 0  J(y)  J(y) + m(x, y) y  X , x  X , m  N.
Thus, 0 is a lower bound for the set {J(y) + m(x, y) : y  X } for all x  X and m  N, which implies that 0  inf{J(y) + m(x, y) : y  X } x  X , m  N.
gm (x)
Since x0  X , J(x0) < +, and metrics are real-valued, we have
(128)
0  inf{J(y) + m(x, y) : y  X }  J(x0) + m(x, x0) < + x  X , m  N.
gm (x)
Thus, gm(x)  R for all x  X and m  N.

(127) (128) (129)

To show that gm  gm+1 for all m  N, note that since (x, y)  0 for all (x, y)  X ◊ X and 0  m  m + 1 for all m  N, we have

J(y) + m(x, y)  J(y) + (m + 1)(x, y) y  X , x  X , m  N.

(130)

By taking infima over y  X , we obtain

inf J(y) + m(x, y)  inf J(y) + (m + 1)(x, y) x  X , m  N.

yX

yX

(131)

gm (x)
To show that gm  J, note that

gm+1 (x)

inf J(y) + m(x, y)  J(x) + m(x, x) = J(x) x  X , m  N,
yX

(132)

gm (x)
which we obtained by setting y = x in the objective of gm(x).

In summary, by (128), (131), and (132), we have 0  gm(x)  gm+1(x)  J (x) x  X , m  N,
where gm is finite for all m  N by (129).

(133)

Consequently, for any x  X , {gm(x)} m=1 is an increasing sequence in R that is bounded above by J(x)  R. Thus, the limit of the sequence {gm(x)} m=1 exists in R (it may be +), and the limit is less than or equal to J(x). This reasoning holds for any x  X . Therefore,

lim
m

gm(x)



J

(x)

x  X .

For any m  N, to show that gm is (uniformly) continuous (with respect to ), we will show that

(134)

 > 0  > 0 s.t. (x, z)  X ◊ X , (x, z)   = |gm(x) - gm(z)|  .

Using the procedure on p. 126 of [35] (a symmetry argument using the definition of gm), we have that

|gm(x) - gm(z)|  m(x, z) (x, z)  X ◊ X .

Let > 0 be given, and set  := m . Suppose that (x, z)  X ◊ X satisfies (x, z)  . Then, we have

(135)

|gm(x) - gm(z)|



m(x, z)  m = m = m

.

Thus, for each m  N, gm is (uniformly) continuous (with respect to ).

(135) (136)

We will show that equality holds in (134) by considering two cases. In the first case, assume that J is finite-valued. Let x  X be given. For all m  N, gm(x)  R, which implies (by using the definition of the infimum) that

 > 0 ym  X s.t. J(ym) + m(x, ym)  gm(x) + .

(137)

Note that ym depends on and x, which we do not write explicitly for brevity. We will construct a sequence by using (137).

Let > 0 be given. Since g1(x)  R, we have

y1  X s.t. J (y1) + 1(x, y1)  g1(x) + .

(138)

Since g2(x)  R, we have

y2  X s.t. J (y2) + 2(x, y2)  g2(x) + .

(139)

By repeating this process, we obtain a sequence {ym : m  N} in X such that

J(ym) + m(x, ym)  gm(x) + m  N.

(140)

Moreover, since 0  J(ym) and gm(x)  J(x) for all m  N, we have

m(x, ym)  J(ym) + m(x, ym)  gm(x) +  J(x) + m  N.

(141)

Therefore,

0  m(x, ym)  J(x) + m  N,

(142)

where we also use the fact that m(x, ym)  0 for all m  N. Since m  N is positive and finite,

J(x) +

0  (x, ym)  m

m  N.

Since J(x) is finite, it follows that

J(x) +

lim

= 0.

m m

The statements (143) and (144) imply that the limit of {(x, ym)} m=1 exists and equals zero. This is because

J(x) +

0  lim inf (x, ym)  lim inf

m

m

m

=0

and

J(x) +

0  lim sup (x, ym)  lim sup

m

m

m

= 0,

and therefore,

lim inf (x, ym) = lim sup (x, ym) = 0,

m

m

which allows us to conclude that

lim (x, ym) = 0.
m

Moreover, since J is lower semi-continuous and by (148), we have

(143) (144)
(145) (146) (147) (148)

J

(x)



lim inf
m

J

(ym).

By using (140), 0  m(x, ym), and gm(x)  J(x), we have

(149)

J(ym)  J(ym) + m(x, ym)  gm(x) +  J(x) + m  N,

(150)

which implies that

J(ym)  gm(x) +  J(x) + m  N.

By (149), (151), and the existence of the limit of {gm(x)} m=1, we have

J(x)  lim inf J(ym)  lim gm(x) +  J(x) + .

m

m

Since J(x)  R, it follows that Since the above analysis holds for any

|

-

J

(x)

+

lim
m

gm(x)|



.

> 0, we conclude that

(151) (152) (153)

lim
m

gm(x)

=

J

(x).

(154)

Since the above analysis holds for any x  X , we have that limm gm(x) = J(x) for all x  X , under the assumption that J is finite-valued.

Fig. part

7. of

Graphs of the tangent and arctangent functions, where the domain these functions in the non-negative quadrant, i.e., h : [0, +] 

of the

[0,

 2

tangent

function

is

restricted

to

[-

 2

,

] such that h(x) = arctan(x), and

h2-].1In:o[u0r,p2ro]o f, we[0u,s+e the]

such that h-1(y) = tan(y).

Now, suppose that J is not necessarily finite-valued. We will modify the argument from [33, Theorem A6.6, pp. 390-

391].

Refer

to

Fig.

7.

The

function

h:

[0, +]



[0,

 2

],

such

that

h(x)

=

arctan(x),

is

increasing

and

continuous.

The

inverse h-1(y)

of h exists and = tan(y). Since

is increasing and continuous. The inverse

the

range

of

h

is

[0,

 2

],

the

composition

h

of 

h J

is the :X 

function

[0,

 2

]

is

h-1

:

[0,

 2

]



finite-valued and

[0, +] bounded

such that below by

0. As a consequence of h being increasing, continuous, and finite-valued, and J : X  R being lower semi-continuous, the

composition h  J is lower semi-continuous. To show this explicitly, let {xn} n=1 be a sequence in X converging to x  X ;

i.e., limn (xn, x) = 0. We will show that

lim inf
n

h(J

(xn))



h(J (x)).

Since {xn} n=1 converges to x and J is lower semi-continuous, it holds that

(155)

lim inf
n

J

(xn)



J

(x).

Since h is increasing, we have

h

lim inf
n

J

(xn)

 h(J(x)).

Now,

lim inf
n

J (xn)

:=

sup
nN

inf
kn

J (xk)

=

lim
n

inf
kn

J (xk).

The second inequality holds because

(156) (157) (158)

inf J(xk)  inf J(xk)  inf J(xk)  ∑ ∑ ∑

k1

k2

k3

since

{J(xk) : k  {1, 2, 3, 4, . . . }}  {J(xk) : k  {2, 3, 4, . . . }}  {J(xk) : k  {3, 4, . . . }}  ∑ ∑ ∑ .

By (158) and since h is continuous,

(159) (160)

Let n  N. Note that

h

lim inf
n

J

(xn)

=h

lim
n

inf
kn

J

(xk

)

= lim h
n

inf J(xk) .
kn

J(xk)  inf J(xk) k  n,
kn

(161) (162)

and since h is increasing,

h J(xk)  h inf J(xk) k  n.
kn

(163)

Now, h infkn J(xk)  R is a lower bound for the set h(J(xk)) : k  n , and so it is less than the greatest lower bound, i.e.,

inf h J(xk)  h inf J(xk) .

kn

kn

(164)

Since we derived (164) for any n  N, it holds for all n  N,

inf h J(xk)  h inf J(xk) n  N.

kn

kn

The limit of the left-hand-side is the limit inferior, and the limit of the right-hand-side exists by (161), and thus,

(165)

lim inf h
n

J (xn)

=

lim inf h
n kn

J (xk)

 lim h
n

inf J(xk)
kn

.

By (161) and (166), we have

(166)

lim inf h
n

J (xn)

 lim h
n

inf J(xk)
kn

=h

lim inf
n

J

(xn)

.

Finally, by (157) and (167), we find that

(167)

lim inf h J(xn)  h lim inf J(xn)  h(J(x)),

n

n

which shows that h  J is lower semi-continuous.

(168)

Since h  J is finite-valued, lower semi-continuous, and bounded below by 0, there is a sequence of continuous functions

fm : X  R such that

1)

0  fm(x)  fm+1(x)  h(J (x)) 

 2

for

all

mN

and

x  X,

and

2) limm fm(x) = h(J (x)) for all x  X .

(Recall

that

h

is

bounded

above

by

 2

.)

Recall

that

the

function

h-1

:

[0,

 2

]



[0, +],

such

that

h-1(y)

=

tan(y),

is

continuous and increasing. It follows that

1) 0 = h-1(0)  h-1(fm(x))  h-1(fm+1(x))  J (x) for all m  N and x  X , and 2) limm h-1(fm(x)) = h-1 limm fm(x) = J (x) for all x  X .

In summary, h-1  fm : X  R is continuous, and 0  (h-1  fm)  J .

For any m  N, define Jm : X  R by

Jm(x) := min m, h-1(fm(x)) ,

which is a composition of continuous functions, and therefore is continuous. Each Jm is bounded, and in particular,

0  Jm(x)  m x  X , m  N. Since h-1  fm  h-1  fm+1  J and m  m + 1 for all m  N, we have
Jm(x)  min m, h-1(fm+1(x))  Jm+1(x)  J (x) x  X , m  N.

Finally, since min is continuous, we have

lim min m, h-1(fm(x)) = min lim m, lim h-1(fm(x))

m

m m

and therefore,

lim
m

Jm(x)

=

J

(x)

x  X .

In summary, each Jm : X  R is continuous and bounded, and 0  Jm  J.

= min{+, J(x)}

x  X ,

(169) (170) (171) (172) (173)

XII. LEMMA 4 (PRESERVING LSC, BOUNDEDNESS BELOW)

A. Statement of Lemma 4
Recall that f and c are continuous functions, and p(dw|x, u) is a continuous stochastic kernel on Rd given S ◊ A. If v : S ◊ Z  R is lower semi-continuous and bounded below by 0, then the function gv : S ◊ Z ◊ A  R

gv(x, z, u) := Rd v f (x, u, w), max{z, c(x, u)} p(dw|x, u) is lower semi-continuous and bounded below by 0.

(174)

B. Supporting Results

To prove Lemma 4, we require Lemma 3 in addition to two other results, which are stated below. Lemma 5: Recall that f and c are continuous functions, and p(dw|x, u) is a continuous stochastic kernel on Rd given S ◊A.
If v  C(S ◊ Z), then the function gv : S ◊ Z ◊ A  R

gv(x, z, u) := Rd v f (x, u, w), max{z, c(x, u)} p(dw|x, u)

(175)

is continuous. Lemma 6: Recall that f and c are continuous functions, and p(dw|x, u) is a continuous stochastic kernel on Rd given S ◊A.
Suppose that vm : S ◊Z  R is Borel-measurable for all m  N, v : S ◊Z  R is Borel-measurable, and b  R. Moreover,
suppose that b  vm  vm+1  v for all m  N, and limm vm(y, q) = v(y, q) for all (y, q)  S ◊ Z. Then,

lim
m

Rd vm f (x, u, w), max{z, c(x, u)} p(dw|x, u) =

Rd v f (x, u, w), max{z, c(x, u)} p(dw|x, u)

(176)

for all (x, z, u)  S ◊ Z ◊ A. In short, Lemma 6 holds by an application of the Monotone Convergence Theorem [33, Theorem 1.6.7, p. 47].

First, we will prove Lemma 4, and then we will prove the supporting results.

C. Proof of Lemma 4 Since v(x , s )  0 for all (x , s )  S ◊ Z, we have

v f (x, u, w), max{z, c(x, u)}  0 (x, z, u, w)  S ◊ Z ◊ A ◊ Rd.

(177)

For any (x, z, u)  S ◊ Z ◊ A, it holds that
∑ the function v f (x, u, ∑), max{z, c(x, u)} : Rd  R is Borel-measurable since it is a composition of Borel-measurable functions;
∑ the function v f (x, u, ∑), max{z, c(x, u)} : Rd  R is non-negative; and ∑ (Rd, BRd , p(∑|x, u)) is a probability space.
By the above three items, the integral

gv(x, z, u) := Rd v f (x, u, w), max{z, c(x, u)} p(dw|x, u) exists and is non-negative for all (x, z, u)  S ◊ Z ◊ A. Therefore, gv is bounded below by 0.

(178)

To prove that gv is lower semi-continuous, it suffices to show that if {(xn, zn, un) : n  N} is a sequence in S ◊ Z ◊ A converging to (x, z, u)  S ◊ Z ◊ A, then

lim inf
n

gv (xn ,

zn,

un)



gv (x,

z,

u).

(179)

Recall that S ◊ Z is a metric space, and v : S ◊ Z  R is lower semi-continuous and bounded below by 0. By Lemma 3, there is a sequence {vm : m  N} in C(S ◊ Z) such that 0  vm  v, i.e.,
1) 0  vm(y, q)  vm+1(y, q)  v(y, q) for all (y, q)  S ◊ Z and m  N, and 2) limm vm(y, q) = v(y, q) for all (y, q)  S ◊ Z. Let m  N and n  N. Note that (Rd, BRd , p(∑|xn, un)) is a probability space. Since v  vm  0, we have

v f (xn, un, w), max{zn, c(xn, un)}  vm f (xn, un, w), max{zn, c(xn, un)}  0 w  Rd.

(180)

Since v, f , max, and c are Borel-measurable functions, the functions

v f (xn, un, ∑), max{zn, c(xn, un)} : Rd  R vm f (xn, un, ∑), max{zn, c(xn, un)} : Rd  R

(181)

are Borel-measurable. It follows that

gv(xn, zn, un) := Rd v f (xn, un, w), max{zn, c(xn, un)} p(dw|xn, un)  Rd vm f (xn, un, w), max{zn, c(xn, un)} p(dw|xn, un),

(182)

gvm (xn,zn,un)

where all the integrals exist. Since the inequality (182) was derived for arbitrary n  N and m  N, it holds for all n  N and

m  N, i.e.,

gv(xn, zn, un)  gvm (xn, zn, un) m  N, n  N.

(183)

For any m  N, we have vm  C(S ◊ Z), which implies that gvm : S ◊ Z ◊ A  R is continuous (Lemma 5). Therefore, for

any m  N, we have

lim inf
n

gv (xn ,

zn,

un)



lim inf
n

gvm

(xn,

zn,

un)

=

lim
n

gvm (xn,

zn,

un)

(184)

= gvm (x, z, u),

where we recall that {(xn, zn, un)} n=1 is a sequence in S ◊ Z ◊ A that converges to (x, z, u)  S ◊ Z ◊ A. Since vm : S ◊ Z  R is Borel-measurable for all m  N, v : S ◊ Z  R is Borel-measurable, and 0  vm  v, we have

lim
m

Rd vm f (x, u, w), max{z, c(x, u)} p(dw|x, u) =

Rd v f (x, u, w), max{z, c(x, u)} p(dw|x, u)

gvm (x,z,u)

gv (x,z,u)

by Lemma 6. Finally, by (184) and (185), it holds that

(185)

lim inf
n

gv (xn ,

zn,

un)



lim
m

gvm (x,

z,

u)

=

gv (x,

z,

u),

which shows that gv is lower semi-continuous.

(186)

D. Proofs of Supporting Results
1) Proof of Lemma 5: Recall that f and c are continuous functions, and p is a continuous stochastic kernel. We will show that if v  C(S ◊ Z), then the function gv : S ◊ Z ◊ A  R

gv(x, z, u) := Rd v f (x, u, w), max{z, c(x, u)} p(dw|x, u)

(187)

is continuous. For this, we use [35, Proposition 7.30, p. 145]. S ◊ Z ◊ A, S ◊ A, and Rd are separable metrizable spaces.15 p(dw|x, u) is a continuous stochastic kernel on Rd given S ◊ A by Assumption 1.16 Thus, the stochastic kernel pØ(dw|x, z, u) on Rd given S ◊ Z ◊ A defined by

pØ(dw|x, z, u) := p(dw|x, u) (x, z, u)  S ◊ Z ◊ A

(188)

is also continuous. If h  C(S ◊ Z ◊ A ◊ Rd), then the function  : S ◊ Z ◊ A  R defined by

(x, z, u) := Rd h(x, z, u, w)pØ(dw|x, z, u) = Rd h(x, z, u, w)p(dw|x, u) is continuous by [35, Proposition 7.30, p. 145]. Hence, it suffices to show that

(189)

h(x, z, u, w) := v f (x, u, w), max{z, c(x, u)}

(190)

satisfies h  C(S ◊ Z ◊ A ◊ Rd). Consider the function h1 : S ◊ Z ◊ A ◊ Rd  S ◊ Z (x, z, u, w)  f (x, u, w), max{z, c(x, u)} .

(191) (192)

15See Fig. 8 for an illustration of terminology. A Borel-measurable subset of a complete and separable metric space is a Borel space [35, Definition 7.7, p. 118]. For example, Rd is a Borel space. S and A are Borel spaces by the system model that we have assumed. A countable Cartesian product of Borel
spaces is a Borel space [35, Proposition 7.13, p. 119]. A Borel space is metrizable and separable [35, p. 118, just below Definition 7.7]. 16The function  : S ◊ A  P(Rd), defined by (x, u) := p(dw|x, u), is continuous [35, Definition 7.12, p. 134].

Fig. 8. An illustration of terminology related to Borel spaces. A formal definition of a Borel space is provided by [35, Definition 7.7, p. 118], for example. A common example of a Borel space is Rd. A Borel-measurable subset of a complete and separable metric space is a Borel space. A
Borel space is separable and metrizable.

Since f , c, and max are continuous, it holds that

lim h1(xn, zn, un, wn) = lim f (xn, un, wn), max{zn, c(xn, un)}

n

n

= lim f (xn, un, wn), lim max{zn, c(xn, un)}

n

n

= f (x, u, w), max{z, c(x, u)}

(193) (194) (195)

for any sequence {(xn, zn, un, wn)} n=1 in S ◊ Z ◊ A ◊ Rd that converges to a point (x, z, u, w)  S ◊ Z ◊ A ◊ Rd. Therefore, h1 is continuous. The function h can be written as v  h1 : S ◊ Z ◊ A ◊ Rd  S ◊ Z  R, where  denotes composition. Since the composition of continuous functions is again a continuous function [?, Proposition 3.1.8], h is continuous. Moreover,

it holds that

sup h = sup v  h1  sup v < +,

S ◊Z ◊A◊Rd

S ◊Z ◊A◊Rd

S◊Z

(196)

where the last inequality holds because v is bounded. Hence, h is bounded. Since h : S ◊ Z ◊ A ◊ Rd  R is continuous and bounded, we have that h  C(S ◊ Z ◊ A ◊ Rd), which concludes the proof.
2) Proof of Lemma 6: Recall that f and c are continuous functions, and p is a continuous stochastic kernel. Suppose that vm : S ◊ Z  R is Borel-measurable for all m  N, v : S ◊ Z  R is Borel-measurable, b  R, b  vm  vm+1  v for all m  N, and ml imvm(y, q) = v(y, q) for all (y, q)  S ◊ Z. We will show that

lim
m

Rd vm f (x, u, w), max{z, c(x, u)} p(dw|x, u) =

Rd v f (x, u, w), max{z, c(x, u)} p(dw|x, u)

(197)

for all (x, z, u)  S ◊ Z ◊ A.

For this, we use the Extended Monotone Convergence Theorem [33, Theorem 1.6.7, p. 47]: Let (Ø , F, µ) be a measure space. Let g1, g2, . . . , g, h be functions from Ø to R, which are measurable relative to F and BR . If gn()  h() for all   Ø and n  N, Ø h()µ(d) > -, and gn  g,17 then Ø gn()µ(d)  Ø g()µ(d).

Let (x, z, u)  S ◊ Z ◊ A be given. We use the measure space (Rd, BRd , p(∑|x, u)). Define the functions gmx,u,z : Rd  R for all m  N, gx,u,z : Rd  R, and h : Rd  R as follows:

gmx,u,z(w) := vm f (x, u, w), max{z, c(x, u)} gx,u,z(w) := v f (x, u, w), max{z, c(x, u)}

(198)

h(w) := Øb.

The above functions are measurable relative to BRd and BR . Specifically, f , max, c, and h are continuous, which implies that they are Borel-measurable. vm and v are Borel-measurable, and the composition of Borel-measurable functions is
Borel-measurable.

Recall that b  vm(y, q)  vm+1(y, q)  v(y, q) for all (y, q)  S ◊ Z and m  N. It follows that, for all w  Rd and m  N,

Øb  vm f (x, u, w), max{z, c(x, u)}  vm+1 f (x, u, w), max{z, c(x, u)}  v f (x, u, w), max{z, c(x, u)} .

h(w)

gm x,u,z (w)

gm x,+u,1z (w)

gx,u,z (w)

(199)

Moreover, since limm vm(y, q) = v(y, q) for all (y, q)  S ◊ Z, we have

In addition,

lim
m

vm

f (x, u, w), max{z, c(x, u)}

= v f (x, u, w), max{z, c(x, u)}

gm x,u,z (w)

gx,u,z (w)

Rd h(w) p(dw|x, u) = Rd Øb p(dw|x, u) = Øb > -.

w  Rd.

(200)

To summarize, we are working on the measure space (Rd, BRd , p(∑|x, u)), and the following properties hold: ∑ g1x,u,z, g2x,u,z, . . . , gx,u,z, h are Borel-measurable functions from Rd to R. ∑ gmx,u,z  h for all m  N, Rd h(w) p(dw|x, u) > -, and gmx,u,z  gx,u,z.

Thus, we have

lim
m

Rd gmx,u,z(w) p(dw|x, u) =

Rd gx,u,z(w) p(dw|x, u).

(201)

Since we derived the equality (201) for an arbitrary (x, z, u)  S ◊ Z ◊ A, it holds for all (x, z, u)  S ◊ Z ◊ A, which completes the proof.

17gn  g means gn()  gn+1()  g() for all   Ø and n  N, and limn gn() = g() for all   Ø .

XIII. THEOREM 2 (LOWER SEMI-CONTINUITY, MEASURABLE SELECTION)
In the main paper, we use the result [35, Proposition 7.33, p. 153] to prove Theorem 2. To understand how this result applies to our setting, we state and explain a less general version of the result here.

A. Statement of Lemma 7

Lemma 7 (Less General Version of Proposition 7.33, p. 153, Bertsekas and Shevre): Let X and Y be metric spaces, where Y is compact. Assume that f : X ◊ Y  R is lower semi-continuous. Let f  : X  R be defined as follows:

f (x) := inf f (x, y).
yY
Then, f  is lower semi-continuous, and there exists a Borel-measurable function  : X  Y such that

(202)

f (x, (x)) = f (x) x  X.

(203)

B. Proof of Lemma 7

The proof combines several results, which we restate in a less general form below. In particular, we use metric spaces rather than metrizable spaces. If a space is a metric space, then it is also a metrizable space because an appropriate metric exists. See [35, p. 104] for the definition of a metrizable space.
1) Proposition 7.32 [35, p. 148]: Let X and Y be metric spaces, where Y is compact. Assume that f : X ◊ Y  R is lower semi-continuous. Let f  : X  R be defined as follows:

f (x) := inf f (x, y).
yY

(204)

Then, f  is lower semi-continuous, and for every x  X, there is a yx  Y such that

f (x) = f (x, yx).

(205)

2) Lemma 7.20 [35, p. 152]: Let X and Y be metric spaces, where Y is compact. Assume that f : X ◊ Y  R is lower semi-continuous. Let f  : X  R be defined by f (x) := infyY f (x, y). Define F  : X  {A  Y } as follows:

F (x) := {y  Y : f (x, y)  f (x)}.

(206)

Then, F  is Borel-measurable. 3) Lemma 7.18 [35, p. 151]: Let Y be a compact metric space. Then, there is a Borel-measurable function  : {A  Y :
A is non-empty}  Y such that if A is a non-empty subset of Y , then (A)  A.
Remark 7: Since the co-domain of  is Y ,  returns points in Y . I.e., if A is in the domain of , then (A) is a point in Y. By Proposition 7.32 [35, p. 148], it holds that the function f  : X  R, defined by f (x) := infyY f (x, y), is lower semi-continuous, and the infimum is attained for any x  X. Hence, the following subset of Y

F (x) := {y  Y : f (x, y)  f (x)} = {y  Y : f (x, y) = f (x)}

(207)

is non-empty for all x  X. The second equality holds in (207) because f (x) is the infimum of f (x, y) over y  Y , which
implies that {y  Y : f (x, y) < f (x)} =  x  X,

where  denotes the empty set.

By Lemma 7.20 [35, p. 152], the set-valued function F  : X  {A  Y } (207) is Borel-measurable.

Lemma 7.18 [35, p. 151] provides a Borel-measurable function  : {A  Y : A is non-empty}  Y such that if A is a non-empty subset of Y , then (A)  A.

Since a composition of Borel-measurable functions is Borel-measurable, and F (x) is in the domain of  for all x  X (i.e., F (x) is a non-empty subset of Y for all x  X), the function  : X  Y

(x) := (F (x))

(208)

is Borel-measurable. Since if A is a non-empty subset of Y , then (A)  A, and since F (x) is a non-empty subset of Y for

all x  X, we have

(x) := (F (x))  F (x) x  X,

(209)

which implies that

(x)  {y  Y : f (x, y) = f (x)} x  X,

(210)

which implies that

f (x, (x)) = f (x) x  X.

(211)

Since we have shown the existence of a Borel-measurable function  : X  Y that satisfies (211), the proof of Lemma 7 is complete.

XIV. THEOREM 1 (COMPUTING VØ s AND Øs) We use several of the previous results to prove the main result, which is stated below.

A. Statement of Theorem 1

Let s  R, and assume the conditions of Assumption 1. Define the function JNs : S ◊ Z  R as follows:

JNs (xN , zN ) := max max{cN (xN ), zN } - s, 0 .

For t = N - 1, . . . , 1, 0, define the function Jts : S ◊ Z  R as follows:

where vts : S ◊ Z ◊ A  R is defined by

Jts(xt,

zt)

:=

inf
ut A

vts(xt,

zt,

ut),

(212a) (212b)

vts(xt, zt, ut) := Jts+1 f (xt, ut, wt), max{c(xt, ut), zt} p(dwt|xt, ut).
Rd

(212c)

Then, for all t  TN := {0, 1, . . . , N }, Jts is lower semi-continuous and bounded below by 0. In addition, for all t  T := {0, 1, . . . , N - 1}, there exists a Borel-measurable function st : S ◊ Z  A such that

Jts(xt, zt) = vts xt, zt, st (xt, zt) (xt, zt)  S ◊ Z.

(213)

Recall that we have defined

VØ

s(x)

:=

inf


Ex

max{YØ - s, 0}

x  S.

Let st (xt,zt) denote the Dirac measure on (A, BA) concentrated at the point st (xt, zt)  A, where (xt, zt)  S ◊ Z is arbitrary.18 Define Øs := (s0 , s1 , . . . , sN-1 )  . Then, we have

J0s(x, 0) = VØ s(x) = ExØs max{YØ - s, 0} x  S.

(214)

B. Proof of Theorem 1 1) Part 1 (Properties of value functions): To prove the first part of Theorem 1, we proceed by induction. JNs is continuous
because cN is continuous, max is continuous, and a composition of continuous functions is continuous. Since JNs is continuous, it is also lower semi-continuous. JNs is bounded below by 0 because max(a, 0)  0 for all a  R. Now, assume (the induction hypothesis) that for some t  {N - 1, . . . , 1, 0}, Jts+1 : S ◊ Z  R is lower semi-continuous and bounded below by 0. Then, by Lemma 4, the function vts : S ◊ Z ◊ A  R

vts(xt, zt, ut) := Jts+1 f (xt, ut, wt), max{c(xt, ut), zt} p(dwt|xt, ut)
Rd

(215)

is lower semi-continuous and bounded below by 0. By Theorem 2, the function Jts : S ◊ Z  R

Jts(xt,

zt)

:=

inf
ut A

vts(xt, zt, ut)

(216)

is lower semi-continuous and bounded below by 0. Since we have shown the induction step, we conclude that Jts is lower semi-continuous and bounded below by 0 for all t  {N, . . . , 1, 0}.

Theorem 2 also tells us that for all t  {0, 1, . . . , N - 1}, there is a Borel-measurable function st : S ◊ Z  A

such that

Jts(xt, zt) = vts(xt, zt, st (xt, zt)) (xt, zt)  S ◊ Z.

(217)

18st is a Borel-measurable stochastic kernel on A given S ◊ Z because the function t : S ◊ Z  P(A), such that t(xt, zt) = st (xt,zt), is Borel-measurable. t is the composition of two Borel-measurable functions. The function  : A  P(A), such that (u) = u is the Dirac measure on (A, BA) concentrated at u  A, is continuous by [35, Corollary 7.21.1, p. 130]. The function st : S ◊ Z  A is Borel-measurable.

More specifically, this conclusion holds by [35, Proposition 7.33, p. 153], which applies to our setting because vts : S ◊ Z ◊ A  R is lsc, S ◊ Z and A are metric spaces, and A is compact.

We define Øs := (s0 , s1 , . . . , sN-1 ). For any (xt, zt)  S ◊ Z, st (xt,zt)(dut) is the Dirac measure on (A, BA) concentrated at the point st (xt, zt)  A. We know that st (xt,zt)(dut) is a Borel-measurable stochastic kernel on A given S ◊ Z by the rationale provided in Footnote 18. Since Øs is a tuple of N Borel-measurable stochastic kernels on A given S ◊ Z, it holds that Øs is an element of .
2) Part 2 (Optimality of value functions and measurable selectors): Our goal is to prove the following statement:

J0s(x, 0) = VØ s(x) = ExØs max{YØ - s, 0} x  S.

(218)

Recall from Lemma 2 that

t ,s(xt, zt) := E(YØts|Xt = xt, Zt = zt) (xt, zt)  S ◊ Z,   , t  {0, 1, . . . , N },

0 ,s(x, 0) = Ex max{YØ - s, 0}

x  S,   .

(219) (220)

By the second line (220) along with the definition of the infimum, we have

0 ,s(x,

0)



inf


0 ,s(x,

0)

=

inf


Ex

max{YØ - s, 0}

=: VØ s(x)

x  S,   .

(221)

Recall that TN := {0, 1, . . . , N }. We will explain why it suffices to show that t ,s(xt, zt)  Jts(xt, zt) (xt, zt)  S ◊ Z,   , t  TN ,

(222)

and

tØs,s(xt, zt) = Jts(xt, zt)

(xt, zt)  S ◊ Z, t  TN .

(223)

If the above statements hold, then (set t = 0, x0 = x, and z0 = 0)

0 ,s(x,

0)

(222)


J0s(x,

0)

(2=23)

0Øs,s(x,

0)

(2=20)

ExØs

max{YØ - s, 0}

 VØ s(x)

x  S,   .

The last inequality holds because Øs is an element of  and VØ s(x) is the infimum over ,

(224)

VØ

s(x)

:=

inf


Ex

max{YØ - s, 0} .

(225)

The statement (224) implies that

inf


0 ,s(x,

0)



J0s(x,

0)

=

ExØs

max{YØ - s, 0}

 VØ s(x)

x  S.

(226)

Since

VØ

s(x)

(2=21)

inf


0 ,s(x,

0)

x  S,

(227)

the statement (226) is equivalent to

VØ s(x)  J0s(x, 0) = ExØs max{YØ - s, 0}  VØ s(x) x  S,

(228)

which shows (218). In summary, if (222) and (223) hold, then the desired statement (218) holds, and the proof is complete.

To show (222) and (223), we proceed by induction. For the base case (t = N ), we use Lemma 2 and the definition

of JNs :

N,s(xN , zN ) = max max{cN (xN ), zN } - s, 0 = JNs (xN , zN ) (xN , zN )  S ◊ Z,   ,

(229)

which implies the base case for (222)

N,s(xN , zN )  JNs (xN , zN ) (xN , zN )  S ◊ Z,   . We show the base case for (223) by choosing  = Øs in (229),

(230)

NØs,s(xN , zN ) = JNs (xN , zN ) (xN , zN )  S ◊ Z.

(231)

Now, assume (the induction hypothesis for (222)) that for some t  {N - 1, . . . , 1, 0}, we have

t+,s1(xt+1, zt+1)  Jts+1(xt+1, zt+1) (xt+1, zt+1)  S ◊ Z,   .

(232)

We will show that

t ,s(xt, zt)  Jts(xt, zt) (xt, zt)  S ◊ Z,   

(233)

to prove (222) by induction. Let  := (0, 1, . . . , N-1)   and (xt, zt)  S ◊ Z be given. By Lemma 2, we have

t ,s(xt, zt) =

t+,s1 f (xt, ut, wt), max{c(xt, ut), zt} p(dwt|xt, ut) t(dut|xt, zt).

A Rd

(234)

Since t+,s1(xt+1, zt+1)  Jts+1(xt+1, zt+1)  0 for all (xt+1, zt+1)  S ◊ Z, and since t+,s1 and Jts+1 are Borel-measurable functions,19 the following inequality holds:

t ,s(xt, zt) 

Jts+1 f (xt, ut, wt), max{c(xt, ut), zt} p(dwt|xt, ut) t(dut|xt, zt),

A Rd

(235)

vts (xt ,zt ,ut )
where we use [33, Theorem 1.5.9 (b), p. 41]. Since Jts+1 : S ◊ Z  R is lower semi-continuous and bounded below by 0 (Sec. XIV-B.1), the function vts : S ◊ Z ◊ A  R is lower semi-continuous and bounded below by 0 (Lemma 4). Moreover, the function Jts : S ◊ Z  R defined by

Jts(xt,

zt)

:=

inf
ut A

vts(xt, zt, ut)

(xt, zt)  S ◊ Z

(236)

is bounded below by 0. In particular, note the following properties: 1) vts(xt, zt, ut)  Jts(xt, zt)  0 for all ut  A (recall that we have fixed xt  S and zt  Z arbitrarily), 2) vts(xt, zt, ∑) : A  R is lsc, and 3) Jts(xt, zt) does not depend on ut  A.
These properties imply, together with (235), that the following statement holds:

t ,s(xt, zt)  vts(xt, zt, ut) t(dut|xt, zt)  Jts(xt, zt) t(dut|xt, zt) = Jts(xt, zt).

A

A

Since    and (xt, zt)  S ◊ Z are arbitrary, we have shown that

t ,s(xt, zt)  Jts(xt, zt) (xt, zt)  S ◊ Z,   ,

which completes the proof of (222) by induction.

(237) (238)

Next, we will show (223) by induction. Recall that we have shown the base case for (223) in (231). Now, assume (the induction hypothesis for (223)) that for some t  {N - 1, . . . , 1, 0}, we have

tØ+s,1s(xt+1, zt+1) = Jts+1(xt+1, zt+1) (xt+1, zt+1)  S ◊ Z.

(239)

To prove (223) by induction, we need to show that

tØs,s(xt, zt) = Jts(xt, zt) (xt, zt)  S ◊ Z.

(240)

For this, let (xt, zt)  S ◊ Z be given. By the recursion provided by Lemma 2 with  = Øs := (s0 , . . . , sN-1 )  , it holds

that

tØs,s(xt, zt) =

tØ+s,1s f (xt, ut, wt), max{c(xt, ut), zt} p(dwt|xt, ut) st (xt,zt)(dut).

A Rd

(241)

Recall that tØ+s,1s(xt+1, zt+1) = Jts+1(xt+1, zt+1)  0 for all (xt+1, zt+1)  S ◊ Z by the induction hypothesis for (223) and since Jts+1 is bounded below by 0. In addition, for any (xt, zt, ut)  S ◊ Z ◊ A,

Jts+1(f (xt, ut, ∑), max{c(xt, ut), zt} : Rd  R

(242)

is a Borel-measurable function because it is a composition of Borel-measurable functions. Consequently, it follows that

tØs,s(xt, zt) =

Jts+1 f (xt, ut, wt), max{c(xt, ut), zt} p(dwt|xt, ut) st (xt,zt)(dut)

A Rd

(243)

= vts(xt, zt, ut) st (xt,zt)(dut),
A

(244)

where we used possible values

the definition ut  A at the

of vts point

(212c) to write st (xt, zt)  A,

the we

second have

line

(244).

Since

the

Dirac

measure

st (xt,zt)

concentrates

all

tØs,s(xt, zt) = vts(xt, zt, st (xt, zt)).

(245)

t+1,9s1Jts:+S1

:S◊Z  ◊ Z  R

R is lower semi-continuous (Sec. is Borel-measurable by Lemma 2.

XIV-B.1),

which

implies

that

it

is

Borel-measurable

(i.e.,

measurable

relative

to

BS◊Z

and

BR ).

By using (217), we conclude that

tØs,s(xt, zt) = Jts(xt, zt).

Since we have derived (246) for an arbitrary (xt, zt)  S ◊ Z, we have shown that tØs,s(xt, zt) = Jts(xt, zt) (xt, zt)  S ◊ Z,

which proves (223) by induction.

In summary, we have shown (222) and (223) by induction. Since (222) and (223) imply the desired statement: J0s(x, 0) = VØ s(x) = ExØs max{YØ - s, 0} x  S,
we have completed the proof of Theorem 1.

(246) (247)
(248)

