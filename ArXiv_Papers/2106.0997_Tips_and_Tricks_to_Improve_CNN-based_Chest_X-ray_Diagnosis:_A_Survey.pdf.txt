[Invited Review Article] Tips and Tricks to Improve CNN-based Chest X-ray Diagnosis: A Survey

Changhee HAN, Takayuki OKAMOTO, Koichi TAKEUCHI, Dimitris KATSIOS, Andrey GRUSHNIKOV, Masaaki KOBAYASHI,
Antoine CHOPPIN, Yutaka KURASHINA, Yuki SHIMAHARA

LPIXEL Inc. 6F, Otemachi Building, 1-6-1, Otemachi, Chiyoda-ku, Tokyo, 100-0004, Japan

Abstract: Convolutional Neural Networks (CNNs) intrinsically requires large-scale data whereas Chest X-Ray (CXR) images tend to be data/annotation-scarce, leading to over-fitting. Therefore, based on our development experience and related work, this paper thoroughly introduces tricks to improve generalization in the CXR diagnosis: how to (i) leverage additional data, (ii) augment/distillate data, (iii) regularize training, and (iv) conduct efficient segmentation. As a development example based on such optimization techniques, we also feature LPIXEL's CNN-based CXR solution, EIRL Chest Nodule, which improved radiologists/non-radiologists' nodule detection sensitivity by 0.100/0.131, respectively, while maintaining specificity. Keywords: Chest X-ray, Chest Radiograph, Convolutional Neural Networks, Transfer Learning, Survey

1. Introduction Since many findings on Chest X-Ray (CXR), the world's
most performed medical imaging test [1], are subtle or doubtful, CXR reading suffers from high inter-observer variability among even expert radiologists [2]. In this context, Convolutional Neural Networks (CNNs) have revolutionized CXR diagnosis (i.e., classification, regression, object detection, segmentation) [3-5]. However, the CNNs intrinsically requires ample data whereas the CXR images tend to be data/annotation-scarce, leading to over-fitting [6].
Therefore, as Fig.1 shows, this paper thoroughly introduces tricks to improve generalization in the CXR diagnosis: how to (i) leverage additional data, (ii) augment/distillate data, (iii) regularize training, and (iv) conduct efficient segmentation. A discussion on which

specific CNN architectures to choose for Medical Imaging is out of scope in this paper, so refer to other surveys [7-9].
As a development example, we also feature LPIXEL's CNN-based CXR solution, EIRL Chest Nodule, which partially applies the introduced optimization techniques to empower doctors for more rapid and reliable nodule diagnosis. With the EIRL Chest Nodule assistance, radiologists/non-radiologists improved sensitivity by 0.100/0.131, respectively, while maintaining specificity.
2. Tricks to Improve CXR Diagnosis 2.1 Leveraging Public CXR Datasets
Since CNNs are data-hungry and obtaining large-scale CXR images is often unfeasible, it is essential to exploit publicly-available CXR images for pre-training, supervised

Fig.1 Categorization of tricks to improve generalization in CXR diagnosis

learning, or semi-supervised learning. Currently, 4 large labeled open datasets exist: ChestXray-NIHCC [10] (~112,000 images); CheXpert [11] (~224,000 images); MIMIC-CXR [12] (~372,000 images); PadChest [13] (~160,000 images). For more detailed information on 20 public CXR datasets, refer to this review paper [14]. It should be noted that those datasets do not always provide definitive diagnosis information by Computed Tomography (CT) scans, which is associated with better prediction performance.
2.2 Pre-training Most dominant pre-training methods are (i) transfer
learning, which uses labeled natural/medical images to obtain good initial weights, and (ii) self-supervised learning,

which uses unlabeled medical images for initialization by solving auxiliary tasks based on input samples [6]. Generally, transfer learning on medical images (e.g., public CXR datasets) is ideal since the learned representation is strongly associated with the target medical task. However, due to the difficulty of data collection/annotation and unavailability of pre-trained models, transfer learning on natural images (e.g., ImageNet [15]/COCO [16]-based transfer learning) and selfsupervised learning on medical images (e.g., Models Genesis [17], MoCo-CXR [18]) have been prevailing. Truncating final blocks of pre-trained models may significantly decrease parameters while statistically maintaining prediction performance on CXR images [19].

2.3 Semi-supervised Learning Semi-supervised learning refers to training a model on
both limited labeled/large-scale unlabeled (or pseudolabeled) medical images to cut (especially segmentation) annotation cost [20]. Usually, effective semi-supervised learning requires at least thousands of labeled CXR images.
2.4 Unsupervised Learning Unsupervised anomaly detection can discover various
unseen abnormalities (e.g., rare disease, bleeding) without specifying disease types, relying on large-scale unannotated healthy medical images. Towards this, Generative Adversarial Networks (GANs) and (Variational) AutoEncoders reconstruct medical images to detect outliers either in the learned feature space or from high reconstruction loss [21].
2.5 Multi-modal Learning Along with CXR images, concatenating patient data (e.g.,
age, gender, X-ray view position) to the (flattened) layer could improve prediction [22].
2.6 Data Augmentation Data Augmentation (DA) plays a key role in improving
generalization. Traditional DA adopts various intensity/geometric transformations (e.g., rotation, flipping, shearing, random resized cropping, changing contrast/sharpness); automated augmentation strategies (e.g., Randaugment [23]) can efficiently automate such DA by reducing a search space. In addition to the traditional DA, combining pairs of training images/labels (e.g., Mixup [24]) has shown promising performance, especially in medical image segmentation [25]. Conditional GAN-based DA also plays a big role in Medical Imaging, offering both interpolation/extrapolation effect [26, 27].

2.7 Data/Model Distillation In Medical Imaging, Test-time DA and model ensembling
assures robust model prediction [28], similar to dropout [29], which provides robustness in training network parameters. Training a model on both gray-scale/color images, respectively, and combining their results might also improve prediction [22].
2.8 Regularized Image Representation Reducing a parameter space to a suitable subspace via regularized image representation helps avoid over-fitting on CXR images: multi-scale patch-based prediction [30] and resizing to a smaller image size simply reduces a search space; Conditional GAN-based denoising removes prediction-irrelevant noises while preserving image structure and details [31]; similarly, Conditional GAN-based bone suppression also increases the visibility of soft tissues by suppressing bones [32]; lung field detection isolates a lung region (i.e., region of interest) [4].
2.9 Multi-task Learning As training with regularization, multi-task learning
performs multiple tasks (e.g., classification, object detection, segmentation) using a single learned representation. In Medical Imaging, it typically refers to training a segmentation model with auxiliary heads, each for an individual classification task; urging the model to represent a classification-relevant (i.e., diagnosis-relevant) feature space tends to improve segmentation [33]. Since the prevalence of positive cases significantly differs across diseases, we need to address the data imbalance; wide-spread solutions are (i) under-sampling normal class, (ii) oversampling rare class, (iii) Synthetic Minority Oversampling Technique [34], and (iv) weighted loss. In addition, combining multi-scale receptive fields helps capture diverse diseases varying in size [35].

Table1 Physicians' reading performance with/without AI assistance. AUC stands for Area Under the receiver operating characteristic Curve

9 Radiologists w/o AI Assistance 9 Non-radiology Physicians w/o AI Assistance 9 Radiologists w/ AI Assistance 9 Non-radiology Physicians w/ AI Assistance

AUC 0.717 ± 0.034 0.700 ± 0.059 0.768 ± 0.021 0.769 ± 0.031

Sensitivity 0.471 ± 0.611 0.438 ± 0.112 0.571 ± 0.041 0.569 ± 0.063

Specificity 0.964 ± 0.020 0.963 ± 0.025 0.966 ± 0.026 0.970 ± 0.022

2.10 Automatic Hyper-parameter Tuning Since CNNs usually involve many hyper-parameters for
tuning, manual searching is inefficient to handle the black box. Grid/random search also suffer from the curse of dimensionality and time inefficiency. Meanwhile, Bayesian optimization can effectively approximate the hyperparameters from known samples (i.e., prior knowledge) on CXR images [3]. On top of the hyper-parameter tuning, reparametrization techniques allow for efficient large-batch training (e.g., batch normalization [36], Adaptive Gradient Clipping [37]).
2.11 Effective Segmentation Annotation CXR reading suffers from high inter-observer variability.
Therefore, computational ground truth prediction by estimating the annotator confusion leads to robust annotation [38]. Moreover, cost-effective annotation requires (i) active learning [39], which provides the annotators with samples to annotate that may improve generalization and (ii) interactive segmentation [40], which supports the annotators by propagating their modifications through the whole segmentation mask.
2.12 Post-segmentation Refinement Post-segmentation refinement removes false positives and
produces a smoother CXR segmentation: Kuzin et al. heuristically averaged cross-fold predictions using an optimized binarization threshold and a dilation technique [5];

Larrazabal et al. used a denoising autoencoder to obtain an anatomically plausible segmentation from the initial prediction [41].
3. EIRL: Doctor's Anytime Assistant Japanese medical AI startup LPIXEL provides a variety of
intelligent AI diagnostic solutions called EIRL Series to empower doctors for more rapid and reliable diagnosis. Specifically, EIRL Chest Nodule reliably detects nodules (between 5-30 mm) on CXR images by partially applying the numerous optimization techniques as mentioned in Section 2. Its version 1.8 adopts various DA methods (e.g., intensity/geometric augmentation) and post-processing methods (e.g., thresholding-based segmentation, isolated small area exclusion, lung field-based false positive reduction).
To validate the EIRL Chest Nodule's clinical efficacy, 18 physicians (9 radiologists/9 non-radiologists) took a reading test on 67 cases with nodules (76 nodules in total) and 253 cases without findings (Table1). With AI assistance, radiologists/non-radiologists improved sensitivity by 0.100/0.131, respectively, while maintaining specificity. Fig.2 shows example nodules found by a radiologist only with AI assistance, which implies the EIRL Chest Nodule's capacity to diagnose lung cancer at an early stage.

Fig.2 Nodules found by a radiologist only with AI assistance

As a doctor's anytime assistant, the EIRL Chest Nodule has 4 features.
· High-Quality Training Data: It uses multi-institutional malignant nodule (i.e., primary lung cancer) data as training data, based on radiologists' agreements for CT findings.
· Outstanding AI Model: It adopts unique technology based on numerous state-of-the-art optimization techniques, including DA, transfer learning, and model ensembling.
· Continuous Update: It continues to add functions, cover more findings, and improve detection performance, whenever pharmaceutical affairs approval is given, by accumulating feedback/data from clinical environment and introducing cutting edge technology.
· Excellent Compatibility: It allows seamless and reliable integration/analysis with all major CXR scanners and Picture Archiving and Communication Systems (PACS).
4. Conclusion Based on our development experience and related work, we thoroughly introduced various optimization tricks to

improve CNN-based CXR diagnosismany of them are generally applicable in Medical Imaging. Our CNN-based CXR solution, EIRL Chest Nodule, partially applies such techniques to empower doctors for more rapid and reliable nodule diagnosis. We plan to cover various findings on CXR while improving detection performance.
References [ 1 ] United Nations: Sources and effects of ionizing radiation, United Nations Scientific Committee on the Effects of Atomic Radiation (UNSCEAR) report, 2008. [ 2 ] Olatunji T, Yao L, Covington B, Upton A: Caveats in generating medical imaging labels from radiology reports with natural language processing, In: Proc. International Conference on Medical Imaging with Deep Learning (MIDL), 1-4, 2019. [ 3 ] Ucar F, Korkmaz D, COVIDiagnosis-Net: Deep Bayes-SqueezeNet based diagnosis of the coronavirus disease 2019 (COVID-19) from X-ray images, Med Hypotheses, 140, 109761, 2020.

[ 4 ] Souza JC, Diniz JOB, Ferreira JL et al.: An automatic method for lung segmentation and reconstruction in chest X-ray using deep neural networks, Comput Methods Programs Biomed, 177, 285-296, 2019. [ 5 ] Groza V, Kuzin A: Pneumothorax segmentation with effective conditioned post-processing in chest X-ray. In Proc. IEEE International Symposium on Biomedical Imaging (ISBI) Workshops, 1-4, 2020. [ 6 ] Tajbakhsh N, Jeyaseelan L, Li Q et al.: Embracing imperfect datasets: A review of deep learning solutions for medical image segmentation, Med Image Anal, 63, 101693, 2020. [ 7 ] Mohapatra S, Swarnkar T, Das, J: Deep convolutional neural network in medical image processing, In: Handbook of deep learning in biomedical engineering, 25-60, 2021. [ 8 ] Wang W, Liang D, Chen Q et al.: Medical image classification using deep learning, In: Deep learning in healthcare, 33-51, 2020. [ 9 ] Du G, Cao X, Liang J et al.: Medical image segmentation based on U-Net: A review, J Imaging Technol, 64(2), 20508-1, 2020. [10] Wang X, Peng Y, Lu L et al.: Chestx-ray8: Hospitalscale chest x-ray database and benchmarks on weaklysupervised classification and localization of common thorax diseases, In: Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2097-2106, 2017. [11] Irvin J, Rajpurkar P, Ko M et al.: Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison, In: Proc. AAAI Conference on Artificial Intelligence (AAAI), 590-597, 2019. [12] Johnson AEW, Pollard TJ, Berkowitz SJ et al.: MIMIC-CXR, a deidentified publicly available database of chest radiographs with free-text reports. Sci Data, 6(1), 1-8, 2019. [13] Bustos A, Pertusa A, Salinas JM et al.: Padchest: A large chest X-ray image dataset with multi-label annotated

reports, Med Image Anal, 66, 101797, 2020. [14] Sogancioglu E, Çalli E, Ginneken BV et al.: Deep learning for chest X-ray analysis: A survey, arXiv preprint arXiv:2103.08700, 2021. [15] Krizhevsky A, Sutskever I, Hinton GE: ImageNet classification with deep convolutional neural networks, In: Proc. Advances in Neural Information Processing Systems (NIPS), 25, 1097-1105, 2012. [16] Lin TY, Maire M, Belongie S et al.: Microsoft COCO: Common objects in context, In European Conference on Computer Vision (ECCV), 740-755, 2014. [17] Zhou Z, Sodha V, Pang J et al.: Models Genesis, Med Image Anal, 67, 101840, 2021. [18] Sowrirajan H, Yang J, Ng AY et al.: MoCo-CXR: MoCo pretraining improves representation and transferability of chest X-ray models, In: Proc. International Conference on Medical Imaging with Deep Learning (MIDL), 2021. [19] Ke A, Ellsworth W, Banerjee O et al.: CheXtransfer: performance and parameter efficiency of ImageNet models for chest X-ray interpretation, arXiv preprint arXiv: 2101.06871, 2021. [20] Terzopoulos D: Semi-supervised multi-task learning with chest X-ray images, In: Proc. International Workshop on Machine Learning in Medical Imaging, 151-159, 2019. [21] Han C, Rundo L, Murao K et al.: MADGAN: unsupervised Medical Anomaly Detection GAN using multiple adjacent brain MRI slice reconstruction, BMC Bioinformatics, 2021. [22] Bharati S, Podder P, Mondal MRH: Hybrid deep learning for detecting lung diseases from X-ray images, Inform Med Unlocked 20, 100391, 2020. [23] Cubuk ED, Zoph B, Shlens J, Le QV: Randaugment: Practical automated data augmentation with a reduced search space, In: Proc. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition

(CVPR) Workshops, 702-703, 2020. [24] Zhang H, Cisse M, Dauphin YN, Lopez-Paz D: Mixup: Beyond empirical risk minimization, In: Proc. International Conference on Learning Representations (ICLR), 2018. [25] Jung W, Park S, Jung KH, Hwang SI: Prostate cancer segmentation using manifold mixup U-Net, In: Proc. International Conference on Medical Imaging with Deep Learning (MIDL), 2019. [26] Han C, Kitamura Y, Kudo A et al.: Synthesizing diverse lung nodules wherever massively: 3D multiconditional GAN-based CT image augmentation for object detection, In: Proc. International Conference on 3D Vision (3DV), 729-737, 2019. [27] Han C, Murao K, Noguchi T et al.: Learning more with less: Conditional PGGAN-based data augmentation for brain metastases detection using highly-rough annotation on MR images, In: Proc. ACM International Conference on Information and Knowledge Management (CIKM), 119127, 2019. [28] Min S, Chen X, Zha Z et al.: A two-stream mutual attention network for semi-supervised biomedical segmentation with noisy labels, In: Proc. AAAI Conference on Artificial Intelligence (AAAI), 33, 4578-4585, 2019. [29] Srivastava N, Hinton G, Krizhevsky A et al.: Dropout: A simple way to prevent neural networks from overfitting, J Mach Learn Res, 15(1), 1929-1958, 2014. [30] Arbabshirani MR, Dallal AH, Agarwal C et al.: Accurate segmentation of lung fields on chest radiographs using deep convolutional networks, In: Proc: Medical Imaging 2017: Image Processing, 1013305, 2017. [31] Kim HJ, Lee D, Image denoising with Conditional Generative Adversarial Networks (CGAN) in low dose chest images, Nucl Instrum Meth Phys Res A, 954, 161914, 2020. [32] Eslami M, Tabarestani S, Albarqouni S et al.: Imageto-images translation for multi-task organ segmentation and

bone suppression in chest X-ray radiography, IEEE Trans Med Imaging, 39(7), 2553-2565, 2020. [33] Mehta S, Mercan E, Bartlett J et al.: Y-Net: joint segmentation and classification for diagnosis of breast biopsy images. In: Proc. International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI), 893-901, 2018. [34] Chawla NV, Bowyer KW, Hall LO, Kegelmeyer WP: SMOTE: Synthetic minority over-sampling technique, J Artif Intell Res, 16, 321-357, 2002. [35] Fan DP, Ji GP, Sun G et al.: Camouflaged object detection, In Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2777-2787, 2020. [36] Ioffe S, Szegedy C: Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proc. International Conference on Machine Learning, 448-456, 2015. [37] Brock A, De S, Smith SL, Simonyan K, Highperformance large-scale image recognition without normalization. arXiv preprint arXiv:2102.06171. [38] Zhang L, Tanno R, Xu MC et al.: Disentangling human error from the ground truth in segmentation of medical images, In: Proc. Advances in Neural Information Processing Systems (NIPS), 33, 2020. [39] Nguyen C, Huynh MT, Tran MQ et al.: GOAL: Gistset Online Active Learning for efficient chest X-ray image annotation, In: Proc. International Conference on Medical Imaging with Deep Learning (MIDL), 2021. [40] Sakinis T, Milletari F, Roth H et al.: Interactive segmentation of medical images through fully convolutional neural networks, arXiv preprint arXiv:1903.08205, 2019. [41] Larrazabal AJ, Martinez C, Glocker B, Ferrante E: Post-DAE: Anatomically plausible segmentation via postprocessing with denoising autoencoders, IEEE Trans Med Imaging, 39(12), 3813-3820, 2020.

