arXiv:2106.01819v1 [math-ph] 3 Jun 2021

Can one hear a matrix? Recovering a real symmetric matrix from its spectral data
Tomasz Maciazek
School of Mathematics, University of Bristol, Fry Building, Woodland Road, Bristol BS8 1UG, UK
Uzy Smilansky
Department of Physics of Complex Systems, Weizmann Institute of Science, Rehovot 7610001, Israel
Abstract. The spectrum of a real and symmetric N × N matrix determines the matrix up to unitary equivalence. More spectral data is needed together with some sign indicators to remove the unitary ambiguities. This work specifies the spectral and sign information required for a unique reconstruction of general matrices. More specifically, the spectral information consists of the spectra of the N nested main minors of the original matrix of the sizes 1, 2, . . . , N . However, due to the complicated nature of the required sign data, improvements are needed in order to make the reconstruction procedure feasible. With this in mind, we restrict our attention to banded matrices where the amount of spectral data exceeds the number of the unknown matrix entries. It is shown that one can take advantage of this redundancy to guarantee unique reconstruction of generic matrices. The space of non-generic matrices is defined either via a set of polynomial equations or via a set of equations involving eigenvectors. Thus, the space of non-generic matrices has a positive co-dimension as a subspace of the space of all matrices. It is shown that one can optimize the ratio between redundancy and genericity by using the freedom of choice of the spectral information input. We demonstrate our constructions in detail for pentadiagonal matrices.
1. Introduction
The answer to the question posed in the title is definitely: No! The spectrum determines the matrix up to unitary equivalence. The question is then, what additional spectral information is required for this purpose. This question accompanies Mathematical Physics since a long time. It appears e.g., in studies of mechanical systems near equilibrium, where one tries to construct a quadratic Hamiltonian model [11], or in atomic, molecular and nuclear physics where spectra are measured, with the hope that they will provide information on the underlying interactions. Since the early beginnings the subject was studied extensively and it is usually referred to as the spectral inversion or spectral reconstruction problems. This subject is still an active area of research. A recent paper [16] addresses the subject from the experimental point of view. Another paper [17] studies neutrino oscillations by reconstructing the so-called

Can one hear a matrix?

2

mixing matrix in matter from the spectra of its main (N - 1) × (N - 1)-minors. There, the relevant identity is the so-called eigenvector-eigenvalue identity [18].
Much of the work in the field concentrated on the Sturm-Liouville equation and its discrete analogue, expressed in terms of finite and symmetric tridiagonal matrices often referred to as Jacobi matrices [15]. For an N dimensional Jacobi matrix, the number of unknown entries is 2N - 1 and the spectral information necessary for the reconstruction is the spectrum with N eigenvalues, to which one adds the spectrum of the N - 1 dimensional minor obtained by removing the last column and row from the original matrix (also called a main minor of order N - 1). This data provides the diagonal elements (with their signs), but only the absolute value of the off diagonal entries can be recovered [12­14]. Thus, N - 1 sign indicators must also be supplied in order to recover the original matrix in full detail.
The problem becomes more acute when less sparse matrices are considered. Then, more spectral information has to be supplied, and there are several options to do so. The signindicators needed in the general case are not necessarily the signs of the individual entries. However, they are absolutely needed to resolve the ambiguity due to spectral invariance of the main minors under similarity with diagonal matrices with entries arbitrarily chosen from {-1, +1}. Various methods were developed for recovering banded or full matrices [1­4, 6­8, 18]. Most of them provide only an exemplary matrix satisfying the spectral data and stop short of resolving the sign-related or other ambiguities. One of the main approaches in reconstructing banded symmetric matrices is to fix the spectra of a few largest main minors (specifically d + 1 main minors of orders N , N - 1, . . ., N - d for a (2d + 1)-banded matrix) and using generalized Householder transformations, bring the reconstructed matrix to a (2d + 1)-banded form [1­4]. Importantly, the Householder transformations change the spectra of all remaining main minors and thus this method differs from the approach presented in this work. Similar problems have been considered for matrices with complex entries. A reconstruction algorithm using spectra of all 2N - 1 main minors has been developed [5]. The algorithm produces an example of a complex matrix whose main minors have the given spectra and, under some regularity assumptions, the result is unique up to similarity transformations by nonsingular diagonal matrices and transpose [6]. Even more general settings concerning matrices with entries from an algebraically closed field have been considered. In particular, it has been shown that there exists a finite number of square N × N matrices with a prescribed spectrum and prescribed off-diagonal entries [7]. Recently, there has been some revived interest in the so-called eigenvector-eigenvalue identity [18] which allows one to find the amplitudes of eigenvector's entries of a hermitian matrix using the spectra of its (N - 1) × (N - 1)-minors and the spectrum of the full matrix itself. As the authors of [18] point out, the eigenvector-eigenvalue identity appears in different guises in many places in the literature (the authors of [18] also provide a thorough review of the relevant literature). Here also, an identity which is used in the proof of Lemma 4 and proved in Appendix B was stated and proved previously. We provide the proof so that the exposition is complete.
The present paper consists of three main parts. The first part deals with the inverse problem of full, real and symmetric matrices of dimension N , where the number of unknown

Can one hear a matrix?

3

entries

is

1 2

N

(N

+

1).

The

spectral

data

to

be

used

is

the

union

of

the

spectra

of

the

first

N

nested

main

minors

of

the

matrix

of

the

sizes

1, 2, . . . , N ,

and

1 2

N

(N

+

1)

sign

indicators

needed for the complete reconstruction. The precise definition of the sign indicators will be

given below. The actual construction is inductive: given a matrix A and a minor A(n) of

dimension n, its next neighbor A(n+1) is obtained by computing the (n + 1)'th column from

the given spectra and sign indicators. The uniqueness of the resulting solution is proved.

Thus, the matrix unfolds like a telescope, hence its name - the telescopic construction. The

fly in the ointment is that the computation of the sign indicators is rather cumbersome.

The second part uses the telescopic method restricted to banded matrices with band width

D = 2d + 1 much smaller than N . The spectral input exceeds the number of unknowns, but

this redundancy enables circumventing the need to compute the sign-indicators, and the only

sign information required consists of the signs of the off-diagonal matrix elements in the first

row. The proposed method is proved to provide a unique solution only for generic D-diagonal

matrices,

where

"generic"

means

here

that

out

of

the

space

R(N

-

1 2

)(d+1)

of

real

parameters

that

characterize all N × N real, symmetric and D-diagonal matrices, only a subspace of positive

co-dimension N is excluded. An explicit criterion which distinguishes non-generic cases is

proposed.

Finally, in the last part it is shown that the large redundancy which exists in the telescopic

approach can be reduced appreciably by studying a different construction: One considers only

the d + 1 dimensional principal minors M (n) which are distinguished by the position of their

upper corner of M (n) along the diagonal. Their spectra and appropriate sign indicators are

used to compute successive minors in a recursive process. This method will be referred to as

the sliding-minor construction. Also in the sliding-minor construction we are able to introduce

certain redundancy by considering (d + 2)-dimensional sliding minors. Then, we show that

the required sign data typically reduces to the signs of the off-diagonal matrix elements in

the first row. The application of the last two methods for banded matrices, requires a special

treatment of the upper d × d main minor, as will be explained in detail for each case. To allow

a smooth flow of the main ideas, some of the more technical proofs of lemmas and theorems

which are stated in the text are deferred to the appendices.

We believe that turning the focus to generic rather than to unconditionally applicable

methods increases the domain of practical application and the scope of the study of inverse

problems.

1.1. Notations and preliminaries
Before turning to the main body of the paper, the present subsection introduces notations and conventions that will be used throughout.
Let A be a symmetric real N × N matrix. Denote by A(n) its n × n upper main diagonal minor, so that A(1) = A1,1 and A(N) = A. The upper suffix (n) will be used throughout for the dimension of the subspace under discussion.
The Dirac notation for vectors is used. That is, given a list x(n) = {x(jn)}nj=1, then x(n) denotes the column vector in dimension n with entries x(jn). Its transpose (row) will be denoted

Can one hear a matrix?

4

by x(n) so that the scalar product is x(n) y(n) . It is convenient also to introduce the unit

vectors e(n)(j) , 1  j  n whose entries all vanish but for the j'th which is unity. Thus,
clearly e(n)(j) x(n) = x(jn), and A(i,nj) := e(n)(i) A(n) e(n)(j) . For every minor A(n), 1  n  (N - 1) define the upper off-diagonal part of the next column

 A(1n,n++11) 

n

a(n) :=  

...

= 

A(jn,n++11) e(n+1)(j) .

A(nn,n++11)

j=1

The spectra of A(n) for 1  n  N will be denoted by (n) = {j(n)}nj=1, with (kn)  (jn)  k  j, and the corresponding eigenvectors (determined up to an overall sign) are
v(n)(j) nj=1. In the following, we often need to remove the overall sign ambiguity by fixing the choice of eigenvectors of A(n) which is done, for instance by demanding the last nonzero

entry of each v(n)(j) to be positive. Then, the projections of a(n) on the eigenvectors of A(n) will be denoted by s(jn)j(n) where

s(jn) := Sign a(n) v(n)(j)

n

+1 for x  0

, with Sign(x) =

.

j=1

-1 for x < 0

(1)

The s(jn) will play the role of the sign-indicators in what follows. However, the arbitrary choice of the overall phase of the vectors v(n)(j) will not have any effect on the results, as long as
the chosen phases are not altered throughout the proof.

2. Inversion by the telescopic construction

Given the spectra (n) for 1  n  N , one can use very simple arguments to derive the following information on the matrix A, without relaying on any sign information. They are known in different guises, see e.g. references [1­4]. We bring them here for completeness and as a background necessary for the ensuing developments.

Theorem 1. The spectra (n) for 1  n  N suffice to determine the diagonal elements of A and the norms of the vectors |a(n) .

Proof. The minor A(n+1) is different from A(n) by the added diagonal entry A(nn++11,n)+1 to be denoted by h, and the off-diagonal column a(n) . Clearly

tr A(n+1) - tr A(n) = A(nn++11,n)+1 = h ,

(2)

Hence h is deduced directly from the spectral data. Also,

n

A(kn,n++11)

2
=

a(n) 2 = 1 2

tr

(A(n+1))2

- tr

(A(n))2

- h2

=: R(n).

(3)

k=1

Thus h, and the value of a(n) 2 can be computed for all n.

Can one hear a matrix?

5

Remark 1. This result can be considered as the inverse of the Gerschgorin theorem [19],

which for symmetric matrices states that given a symmetric matrix A, its spectrum lies in the

union of real intervals centered at the points An,n and are of lengths

N k=1

|An,k|.

Theorem

1

states that given the spectra of the successive minors, the diagonal elements of the matrix are

determined precisely, and the vectors of off-diagonal elements a(a) are restricted to a sphere

of a radius determined by Equation (3).

Remark 2. If A is a Jacobi (symmetric, tridiagonal) matrix, only the nth component of a(n) is different from zero, and therefore the off-diagonal entries are determined up to a sign. In other words, one has to provide their signs so that the Jacobi matrix could be heard.

The following identities are similar in spirit to the former ones, and can be proven by similar ways. They will be used in the sequel.

n

n

a(n) A(n) a(n) =

(kn)| a(n)|v(n)(k) |2 =

(kn)|k(n)|2 =

(4)

k=1

k=1

1 =

tr[(A(n+1))3] - tr[(A(n))3] - 3h|a(n)|2 - h3

.

3

and, a(n) A(n) -1 a(n)

n1 = k=1 (kn)

a(n)

v(n)(k)

2

=

n k=1

1 (kn)

|k(n)|2

=

h-

det A(n+1) det A(n)

, (5)

which is valid if 0 is not in the spectrum of A(n). The identities (3,4,5) are quadratic in the n components of the vector |a(n) . When A is assumed to be a pentadiagonal (D = 5), only the last two components of |a(n) do not vanish, and the quadratic forms describe three concentric curves - a circle and two conic sections in R2. Their mutual intersections are candidates for
the solution for the inversion problem. This is discussed in detail in section (3.2.2).

2.1. A general algorithm
Unlike the case of tridiagonal matrices where the signature indicators required are just the signs of the off diagonal entries (see Remark 2), for the general case one requires the sign indicators s(jn) n defined in (1) .
j=1
In the following, it will often be convenient to assume certain regularity properties of the matrix A.
Definition 2 (Regular matrix). A real N × N symmetric matrix is called regular if and only if the following conditions are satisfied.
(i) The main minors {A(n)}Nn=1 are all of full rank. (ii) The multiplicity of all eigenvalues of each A(n) is one. (iii) The successive spectra do not share a common value, (n)  (n+1) =  for n =
1, . . . , N - 1.
Having the spectral and sign information we can prove the following result.

Can one hear a matrix?

6

Theorem 3. Given the spectra

(n)

N n=1

and

the

sign

indicators

s(n)

N n=1

defined

above.

Assume that A is regular. Then, these data suffice to construct the original matrix A uniquely.

( Irregular matrices will be discussed in a subsequent subsection ).

Consider two successive minors A(n) and A(n+1). Cauchy's spectral interlacing theorem guarantees that

(1n+1)  (1n)  (2n+1)  · · ·  (nn+1)  (nn)  n(n++11) .

(6)

Lemma 4. The vector a(n) and the diagonal element h := An(n++11,n)+1 , as well as the (normalized) eigenvectors of the minor A(n+1), v(n+1)(j) n+1, are uniquely determined
j=1
by the following data.

(i) The full spectral data of A(n), i.e. its spectrum (n) and eigenvectors v(n)(j) n . j=1
(ii) The spectrum (n+1) of A(n+1), (iii) The sign indicators s(jn), j = 1, . . . , n.

Proof. The value of h is determined by (2). The next steps are based on the relation between
the two spectra (n) and (n+1) which is used in the proof of (6). Define an auxiliary matrix A~(n+1) with A(n) being its main n × n minor, A~(nn++11,n)+1 = h, and whose off diagonal entries of the (n + 1)'th column and row all vanish. Then, the two matrices of dimension (n + 1), A(n+1) and A~(n+1) differ by a matrix of rank 2.

A(n+1) = A~(n+1) + e(n+1)(n + 1) a(n), 0 + a(n), 0 e(n+1)(n + 1)

(7)

The spectrum of A~(n+1) consists of (n)  {h} . Correspondingly, the eigenvectors of A~(n+1) are obtained by increasing the dimension of each of the eigenvectors of A(n) by adding a 0 in the (n + 1)'th position and adding the eigenvector e(n+1)(n + 1) . These eigenvectors will be denoted by v~(n+1)(j) nj=+11, where

v~(n+1)(j) = v(n)(j), 0 for j = 1, . . . , n and v~(n+1)(n + 1) = e(n+1)(n + 1) .

To express the spectrum and eigenvalues of A(n+1), expand any of its eigenvectors in the basis v~(n+1)(j) nj=+11, so that

n+1

v(n+1)(k) =

bk,r v~(n+1)(r) and A(n+1) v(n+1)(k) = (kn+1) v(n+1)(k) .

(8)

r=1

Replacing A(n+1) by its form (7) one easily finds that the expansion coefficients {bk,r}nr=+11 of the kth eigenvector satisfy

(kn+1)bk,r

= (1 - r,n+1)bk,r(rn) + r,n+1h bk,n+1

(9)

n

+ r,n+1 bk,j a(n), 0 v~(n+1)(j) + (1 - r,n+1) v~(n+1)(r) a(n), 0 bk,n+1 .

j=1

From these relations and the definitions above we conclude that:

Can one hear a matrix?

7

(i) for r = n + 1

n

((kn+1) - h)bk,n+1 =

bk,j a(n) v(n)(j) ,

j=1

(10)

(ii) for r  n

((kn+1) - (rn))bk,r = bk,n+1 v(n)(r) a(n) .

(11)

Since by the conditions of the theorem, (kn+1) = (rn) for all the pairs 1  r  n, 1  k  n + 1, one can write

v(n)(r) a(n) bk,r = (kn+1) - (rn) bk,n+1

(12)

n
(kn+1) - h =
r=1

a(n) v(n)(r) 2 k(n+1) - (rn) ,  1  k  n + 1 .

(13)

Moreover, all minors are assumed to be of full rank, and both (n+1) and (n) are given. Therefore, one can consider (13) as a set of linear equations from which the unknown n parameters
(r(n))2 := a(n) v(n)(r) 2 , r = 1, . . . , n

could be computed and none vanishes as will be shown in (15) below. However, the number of equations exceeds by 1 the number of unknowns, and therefore there exists a solution only if the equations are consistent. To prove consistency, we start by obtaining an explicit solution of the first n equations in (13). Introducing the Cauchy matrix

1

Ck,r = (kn+1) - (rn) , for, 1  k, r  n ,

(14)

the unknown parameters {(r(n))2}nr=1 then read (see Appendix B):

n

(r(n))2 =

C-1 r,k (k(n+1) - h) = -

k=1

n+1 k=1

(rn) - (kn+1)

.

n k=1,k=r

r(n) - (kn)

(15)

Formula (15) can also be found in earlier works [1­3] and the recursive relation for the
eigenvectors (12) has been also used in [2]. Similarly, one can combine Equation (12) with Equation (15) to obtain (after requiring the eigenvectors of A(n+1) to be normalized to one) the following expression for the coefficients |bk,n+1|2

|bk,n+1|2 =

n r=1

k(n+1) - (rn)

.

n r=1,r=k

k(n+1) - (rn+1)

(16)

Equation (16) is an instance of the eigenvector-eigenvalue identity from [18] which allows one to compute the amplitudes e(rn+1) v~(n+1)(k) directly from the spectra of the n main
minors of A(n+1) of the size n × n. Here, we are using only one n × n minor (namely, A(n)), thus we recover only the last squared entry of each eigenvector. By combining Equation (16)

Can one hear a matrix?

8

with Equation (12) we can express the coefficients bk,r in terms of the spectra of A(n) and A(n+1), but the resulting formulae will be different then the eigenvector-eigenvalue identity

from [18] as we use different spectral data and the entries refer to the basis of the eigenvectors of A(n) only.

The consistency of the linear n+1 equations (10) and (11) could be proved by substituting the (r(n))2 computed above in the last equation in (13), and showing that the resulting equation

((nn++11)

- h)

=

n r=1

n k=1

1 (nn++11) - r(n)

C-1 r,k ((kn+1) - h),

(17)

is satisfied identically. An explicit expression for (C-1)r,k is given in [21] :

C -1 r,k

=

p((rn))q((kn+1))

,

((rn+1) - (kn))p ((kn+1))q ((rn))

n

n

where : p(x) = (x - (in+1)) ; q(x) = (x - (in)) .

i=1

i=1

(18)

The identities which are proved in Appendix A show that equation (17) is satisfied identically

provided that h =

n+1 k=1

(kn+1)

-

n r=1

(rn)

as

is

the

case

in

the

present

context.

To recapitulate, the solution of equations (13) exists and it provides the (r(n))2 in terms

of the spectra in a unique way as shown in Equation (15). This information together with

the sign indicators determine a(n)|v(n)(r) = s(rn)r(n), and the vector a(n) is obtained by

an orthogonal change of basis. Moreover, the possible 2n combinations of the sign indicators

encapsulate all possible choices of the overall signs of the eigenvectors of A(n). Thus, an

a priori choice of the overall phases of the eigenvectors does not result with any loss of

generality or the loss of solutions.

The knowledge of the a v(n)(r) with their signs enables the use of (12) to obtain the coefficients of the eigenvectors of A(n+1) up to the constant factor bk,n+1 which can be determined by requiring a proper normalization.

Proof. (of Theorem 3) The proof proceeds by a recursive construction: Start with a known A(1) and with the known spectrum of A(2). Clearly, (11) = A1,1, and v(1) = 1. Furthermore,

h = (12) + (22) - (11) = A2,2, a = A1,2,

a v(1) = A1,2,

s(11)

=

A1,2 . |A1,2|

Hence, Equation (15) gives

(1(1))2 = (A1,2)2 = (11) - (12) (22) - (11) = A1,1 - (12) (22) - A1,1 .

Therefore, we get two possible solutions for A(2) corresponding to choosing s(11) = +1 or s(11) = -1, which written explicitly read

 A1,1

s(11)

A1,1 - (12) (22) - A1,1 

A(2) = 

 . (19)





s(11)

A1,1 - (12) (22) - A1,1

(12) + (22) - A1,1

Can one hear a matrix?

9

The normalized eigenvectors can be computed from Equation (12). In the standard basis, their forms read

v(2)(1) =  1 1 + 2

 1

, v(2)(2) =  1 1 + 2

-1 

,  := -s(11)

(22) - A1,1 . A1,1 - (12)

(20)

Thus the theorem is valid for n = 1, 2. Note also that one can show that (13) is satisfied which is not clear at first sight but can be easily checked. The lemma provides the final stage of the inductive proof.

The above inductive procedure is optimal for generic matrices whose all entries are typically nonzero. However, the requirement of providing signs of projections of the unknown column to the eigenvectors of A(n) seems awkward for practical use. However, when one wants to apply this result to banded matrices, it turns out that there is large redundancy which can be used to our advantage. In particular, as we argue in the following sections, the inductive reconstruction procedure for banded matrices can be improved so that the number of required sign data can be tremendously reduced.

2.1.1. Explicit reconstruction of A(3). It is instructive to work out the above inductive
procedure explicitly for n = 3. The results will prove useful in the following sections. Consider the problem of reconstructing a regular 3 × 3 real symmetric matrix, A(3), given its spectrum (13) < (23) < (33) and its 2 × 2 top-left minor A(2). It has a spectral decomposition
A(2) = (12) v(2)(1) v(2)(1) + (22) v(2)(2) v(2)(2) , (12) < (22),

where

(12,2)

=

1 2

A(12,1) + A(22,2) ±

(A(12,1) - A(22,2))2 + 4 A(12,2) 2 .

and

v(2)(1) =  1

 ,

1 + 2 1

v(2)(2) =  1 1 + 2

-1 

,

 := -s

(22) - A(12,1) A(12,1) - (12)

(21)

with s = Sign A(12,2) . The unknown entries of matrix A(3) read

A(i,33) = s(12)|1(2)| v(2)(1) i + s(22)|2(2)| v(2)(2) i , s(i2)  {+1, -1}, i  {1, 2}, (22)

where {r(2)}2r=1 are given by Equation (15). Thus, there are four possible solutions for the last column of A(3) corresponding to different choices of signs si. Next, let us show how to

determine si directly from the sign of a certain expression involving only matrix elements of

A(3). Using Equation (15) and Equation (21), we get that

 1

+

2A(13,3)

=

s1|1(2)|

-

s(22)|2(2)|,

 1

+

2A(23,3)

=

s(12)|1(2)|

+

s2|2(2)|.

This allows us to express s(12) and s(22) as

s(12)

=

A(13,3) 1+

+ 2

A(23,3) |1(2)|

,

s(22)

=

A(23,3) 1+

- A(13,3) 2|2(2)|

.

Can one hear a matrix?

10

Because denominators of the above expressions are positive numbers, we finally get
s(12) = Sign -sA(13,3)|| + A(23,3) , s(22) = -Sign -sA(23,3)|| + A(13,3) . (23)
The main advantage of these expressions is that they provide expressions for the signatures s(12), s(22) in terms of data directly available from the matrix.

2.2. Non-regular matrices

In this subsection we show that non-regular matrices that have degeneracy in (n) or where the overlaps (n)  (n+1) are nonempty can be effectively treated using the methods developed
above for regular matrices. We consider the situation where there is a single block of degeneracy in (n), i.e. for some 1  l  n and m  0 we have (ln) = l(+n)1 = . . . = (l+n)m :=  and other eigenvalues of A(n) are non-degenerate. Let us denote the respective degeneracy indices by
D(n)(l, m) := {l, l + 1, . . . , l + m}.

The interlacing property (6) dictates the following possibilities for the respective degeneracy indices D(n+1)(l, m) (see Fig. 1):

DI(n+1)(l, m) := {l, l + 1, . . . , l + m + 1}, DI(nI+1)(l, m) := {l, l + 1, . . . , l + m}, DI(nII+1)(l, m) := {l + 1, l + 2, . . . , l + m + 1}, DI(nV+1)(l, m) := {l + 1, l + 2, . . . , l + m} for

m  1,

DI(nV+1)(l, 0) := .

Figure 1. The four possibilities for the position of the degeneracy indices D(n+1)(l, m) relative to D(n)(l, m).

Can one hear a matrix?

11

In what follows we assume that the eigenvalues (kn+1) with k / D(n+1)(l, m) are nondegenerate. Furthermore, the degenerate eigenspaces of A(n) and A(n+1) will be denoted by V (n)() and V (n+1)() respectively, and their respective orthogonal complements in Rn and Rn+1 by V (n)() and V (n+1)(). We will also embed Rn into Rn+1 by appending a zero at the end of every vector. With a slight abuse of notation we will also denote by V (n)() and V (n)() the images of these spaces under the above embedding.
Theorem 5 below shows how to reduce each of the above four degenerate cases to a
regular problem, where Theorem 3 can be applied. Interestingly, each case requires a different
procedure.

Theorem 5. Assume that the spectra of A(n) and A(n+1) as well as the eigenvectors

v(n)(r)

n r=1

of

A(n)

are

given

and

fixed.

Assume

that

the

spectra

have

single

degeneracy

blocks with the eigenvalue  given by degeneracy indices D(n)(l, m) and D(n+1)(l, m) =

Di(n+1)(l, m) for some i  {I, II, III, IV } respectively. The matrix A(n+1) is reconstructed

from the above spectral data as follows.

I. The vector a(n) as well as the non-degenerate eigenvectors of A(n+1) belong to the

space V (n)(). They are constructed by applying Theorem 3 to the truncated regular

matrices A~(n-m) := A(n+1)

and A~(n-m-1) := A(n)

. The degenerate

V (n)()

V (n)()

eigenvectors of A(n+1)

n
v~(n+1)(k) = bn+1,k e(n+1) + bk,r v(n)(r), 0 ,
r=1

k  DI(n+1)(l, m)

have coefficients

a(n) v(n)(r)

bk,r =

 - (rn) bk,n+1,

r / D(n)(l, m),

k  DI(n+1)(l, m).

The remaining coefficients bk,n+1 and br,k with k  DI(n+1)(l, m) and r / D(n)(l, m) can be arbitrary so that the resulting eigenvectors form an orthonormal set.

II. The vector a(n) as well as the non-degenerate eigenvectors of A(n+1) belong to the space V (n)(). They are constructed by applying Theorem 3 to the truncated regular

matrices A~(n-m) := A(n+1)

and A~(n-m-1) := A(n)

. The degenerate

V (n)()

V (n)()

eigenvectors of A(n+1) form an orthonormal basis of V (n)().

III. The same as case II above.

IV. The space V (n+1)() is a subspace of V (n)() of codimension one. The degenerate eigenvectors of A(n+1), v~(n+1)(k) , k  D(n+1)(l, m) can be freely chosen as any orthonormal subset of V (n)(). They determine the embedding  : V (n+1)()  V (n)(). The vector a(n), 0 as well as the remaining eigenvectors of A(n+1) belong to the orthogonal complement of the image of the embedding  in Rn+1, (V (n+1)()). They are constructed using Theorem 3 applied to the regular matrices A~(n-m+1) :=

A(n+1)

and A~(n-m) := A(n)

.

V (n+1)()

(V (n+1)())

The proof of Theorem 5 is deferred to Appendix C.

Can one hear a matrix?

12

3. Applications to banded matrices

In the present chapter we shall discuss the application of the above formalism for banded matrices. The fact that the spectral data exceeds the number of unknowns will be shown to provide stringent constraints on the sign distribution of the off diagonal elements, so that generically their signs are determined up to an overall sign per column. If not stated otherwise, we will assume that the considered matrices are regular.

3.1. Tridiagonal matrices
In Remark 1 above we have shown that the present construction applies for the tridiagonal case. However it is certainly not efficient since the two spectra and the signs of the offdiagonal matrix elements suffice for the purpose - as is well known [10].

3.2. Pentadiagonal matrices
The next simple case which illustrate however the main ingredients of the general case are the D = 5 - banded matrices a.k.a. pentadioagonal matrices. Here,
a(1n) = a(2n) = · · · = a(nn-)2 = 0 ,
hence the inductive step requires solving a number of equations in the two real variables a(nn) and a(nn-)1. We shall address the subject using two different approaches. In the first we shall apply the method based on Theorem 3. In the second we shall use the quadratic forms (4,5), which give an alternative point of view that applies exclusively for pentadiagonal matrices.
Note that for both approaches, one should compute the upper 3 × 3 minor using the procedure outlined in subsection (2.1.1). Alternatively, it suffices to provide as input the spectrum and the eigenvectors of the upper 3 × 3 minor, and use this data as the starting data for the ensuing inductive steps.

3.2.1. Applying Theorem 3 to the pentadiagonal case In this subsection, we will focus on answering the following two questions
(i) Given that the spectra (n), n = 1, . . . , N correspond to a pentadiagonal matrix, what additional information is needed to uniquely reconstruct the matrix?
(ii) What are the necessary conditions for a matrix A(n) and the spectrum (n+1) of A(n+1) so that A(n+1) is a pentadiagonal matrix?
Starting with the first question, the general answer is given by Theorem 3. However, we will make use of the redundant information to show that typically, one can do away with the computation of the sign indicators as prescribed in the theorem. It will be shown that the vectors |a(n) are determined up to an overall sign, which is uniquely provided by the easily available signs of the entries in the upper diagonal.

Can one hear a matrix?

13

Consider the inductive step, where A(n) and the spectrum (n+1) are known. All vectors |a(n) have the same norm determined by Equation (3). Hence,

a(nn-)1

2
+

a(nn) 2 =

R(n) 2 := 1 2

tr[(A(n+1))2] - tr[(A(n))2]

- h2).

Furthermore, the solutions of Equation (15) imply that

either a(nn-)1v(n)(r)n-1 + a(nn)v(n)(r)n = r(n) or a(nn-)1v(n)(r)n-1 + a(nn)v(n)(r)n = -r(n) (24)

for every r = 1, . . . , n. Thus, every |a(n) belongs to the intersection of one of the above lines (for every r) with the circle of radius R(n) (recall also that r(n) were defined to be positive).
By the assumption of Theorem 3, (n) is non-degenerate and none of the elements of (n) belongs to (n+1). By Equation (15), this implies that r(n) > 0 for all r = 1, . . . , n.
Then, Equation (24) defines two distinct parallel lines for each r. Moreover, for a fixed r,

the intersection of the two lines with the circle occurs at two distinct pairs of points, where a

single pair consists of a point and its antipode (see Fig. 2).

Figure 2. Intersecting lines from Equation (24) with the circle. The blue line corresponds to the line with r(n) as the constant while the green line corresponds to the choice of -r(n).
Dashed lines connect the antipodal solutions.

Since the spectral data comes from a pentadiagonal matrix, each of the lines from

Equation (24) must have an intersection point |a~(n) or - |a~(n) where |a~(n) is the nth column

(with only two non zero entries) of the original matrix (see Fig. (3).

However, there exist special situations when such intersections with the circle give more

than just one solution up to a sign. These are highly degenerate cases where all lines are

determined by a fixed pair of parallel lines (l+, l-). More precisely, for every r, the lines

from Equation (24) are either equal to (l+, l-) or to another fixed pair of lines (l+, l-) where

l+/-  l+/- (see Fig. 4). The precise conditions when that happens are as follows. Let us

denote

the

slope

of

l+/-

by

.

Then,

the

slope

of

l+/-

is

-

1 

.

Thus,

for

every

r

the

slope

of

Can one hear a matrix?

14

Figure 3. A typical situation showing the result of intersecting lines from Equation (24) with
the circle for r = 1, 2, 3. Blue lines correspond to the line with r(n) as the constant while green lines correspond to the choice of -r(n).

Figure 4. A typical situation where intersecting all lines from Equation (24) with the circle leads to four solutions for |a(n) .

the

lines

from

Equation

(24)

is

either



or

-

1 

,

i.e.

either

v(n)(r)n-1 =  v(n)(r)n

or

v(n)(r)n-1

=

1 -

v(n)(r)n



for all r = 1, . . . , n. (25)

Define the subsets I, J  {1, . . . , n}, I  J =  by

I :=

r : v(n)(r)n-1 =  ,

J :=

r:

v(n)(r)n-1

=

1 -

.

v(n)(r)n

v(n)(r)n



Note that, we can never have I =  or J = , because this implies that the eigenvectors of A(n) are linearly dependent, which is a contradiction. Furthermore, because the eigenvectors

Can one hear a matrix?

15

of A(n) are orthonormal, we necessarily have

n
v(n)(r)n-1v(n)(r)n = 0,
r=1

n
v(n)(r)n-1 2 = 1,
r=1

n
v(n)(r)n 2 = 1.
r=1

Using , we can express vr(,nn)-1 by vr(,nn) and obtain that

1 SI -  SJ = 0,

2SI

+

1 2 SJ

=

1,

where

SI + SJ = 1,

SI :=

v(n)(i)n 2 , SJ :=

v(n)(j)n 2 .

iI

jJ

It is straightforward to see that

2 = SJ = 1 - 1.

(26)

SI SI

Finally, note that conditions (25) readily imply that the intersecting lines (24) with the circle

results with only four points and it is not necessary to look at the free coefficients of the lines.

This is because we assume that the spectral data comes from an existing pentadiagonal matrix

and hence all lines must intersect in at least one common point.

To sum up, we have the following theorem.

Theorem 6. For a regular pentadiagonal matrix, the inductive step leads to at most four

solutions for |a(n) . Typically, by intersecting the lines from Equation (24) there exists only one

solution for |a(n) , up to an overall sign. However, there are two possible solutions for |a(n) ,

up to an overall sign, if and only if the spectral data satisfies the following condition (which

we call the -condition): there exists a nonempty subset I  {1, . . . , n}, I = {1, . . . , n},

such that for all r

=

1, . . . , n we have

v (n) (r)n-1 v (n) (r)n

=

 if r



I

and

v (n) (r)n-1 v (n) (r)n

=

-

1 

if r

/

I

for

 defined by

=±

1 - 1,
SI

SI :=

v(n)(i)n 2 .

iI

Remark 3. The set of matrices satisfying the -condition from Theorem 6 is a subset of

all pentadiagonal symmetric matrices of codimension at least one. This explains the results

of simulations where sets of a few millions of randomly chosen pentadiagonal matrices of

dimensions up to n = 10 where tested, and none satisfied the -condition .

Another outcome of the above analysis provides the answer to the second question stated

at the beginning of the section: It provides a set of necessary conditions that ensures that the

spectral data indeed belong to a pentadiagonal matrix. The conditions are that the distance of each line from Equation (24) from the origin must be no greater than R(n), i.e.

r(n)

 R(n) for all r = 1, . . . , n.

(27)

(v(n)(r)n-1)2 + (v(n)(r)n)2

Can one hear a matrix?

16

3.2.2. Using the conic sections for recovering pentadiagonal matrices Before proceeding

to the general case, we shall show that using the three quadratic forms from Equation (3),

Equation (4) and Equation (5) typically allow one to reconstruct a pentadiagonal matrix from

its spectral data up to an overall sign of each of its columns. For a pentadiagonal matrix, we have that a(n) has only two non-zero entries and the quadratic forms from Equation (4)

and Equation (5) describe either ellipses or hyperbolae (a.k.a. conic sections). Their explicit

forms read

a(nn-)1

2
+

a(nn) 2 =

R(n) 2 ,

(28)

(n) a(nn-)1 2 + 2(n)a(nn-)1a(nn) + (n) a(nn) 2 = (n),

(29)

~(n) a(nn-)1 2 + 2~(n)a(nn-)1a(nn) + ~(n) a(nn) 2 = ~(n),

(30)

where

R(n) 2 = 1 tr (A(n+1))2 - tr (A(n))2 - h2 , 2

(n) = 1 tr[(A(n+1))3] - tr[(A(n))3] - h3 , 3

~(n)

=

h

-

det A(n+1) det A(n)

,

(n) = A(nn-)1,n-1 + h, (n) = A(nn-)1,n, (n) = A(nn,n) + h,

~(n) = A(n) -1

, ~(n) = A(n) -1 , ~(n) = A(n) -1 .

n-1,n-1

n-1,n

n,n

The three conic sections are centered at (0, 0), thus each of the curves from Equation (29) and

Equation (30) typically intersects the circle given by Equation (28) at exactly four points (or

none). There may also be only two intersection points in certain degenerate cases. If any of the

intersections is empty or both intersections have zero overlap, then there is no pentadiagonal matrix A(n+1) whose top-left main minor is A(n). If such a pentadiagonal matrix A(n+1) exists, then the three conic sections intersect at at-least two points which yield a(n) up to an overall

sign. However, in certain non-generic cases it may happen that the three conic curves intersect

at four points (see Fig. 5). Then, one cannot decide which of the intersection points give a

correct solution. This happens if and only if the principal axes of the two conic sections

are identical. This can be written as a condition for the eigenvectors of the quadratic forms

defining the conic sections. Namely, both sets of eigenvectors must be the same (up to a scalar

factor) [9]. This in turn happens if and only if the matrices of both quadratic forms commute.

This boils down to the following algebraic condition for the coefficients defining the conic

intersections (29)-(30).

(n)~(n) - ~(n)(n) + ~(n)(n) - (n)~(n) = 0

(31)

In the special case when n = 2, Equation (31) is automatically satisfied and all four points of intersection give correct solutions for a(n). For n > 2, the set of pentadiagonal n × n matrices satisfying condition (31) is a strict subset of the set of all n × n pentadiagonal matrices of codimension one. Thus, a typical (generic) pentadiagonal matrix can be completely reconstructed using the above conic curves' method. However, considering non-generic cases brings in a few subtleties. In particular, satisfying the -condition from Theorem 6 implies satisfying Equation (31), but the converse statement is not true. The set of n×n pentadiagonal

Can one hear a matrix?

17

Figure 5. Intersecting the conic sections (29)-(30) with the circle (28). a) A typical situation where the two conics intersect at two antipodal points which determine a(n) up to an overall sign. b) A degenerate situation when conditions (31) are satisfied and the conic sections intersect at four points.
matrices satisfying the -condition is of codimension at least one, but it is a strict subset of the set of matrices satisfying Equation (31). Thus, in non-generic cases only Theorem 6 can give a definite answer about the possible solutions (see Fig. 6).
Figure 6. Conic curves (29)-(30) for a random 5 × 5 pentadiagonal matrix satisfying degeneracy condition (31). Green lines are lines from Equation (24) which determine the correct solutions for the column a(5). Despite Equation (31) being satisfied, only one pair of antipodal points gives solutions for a(5).
3.3. A finite algorithm for reconstructing a D-diagonal matrix. Given that the spectra (n), n = 1, . . . , N correspond to a D-diagonal real symmetric matrix, we provide a finite algorithm which produces the set of possible D-diagonal matrices whose

Can one hear a matrix?

18

main minors have the given spectra {(n)}Nn=1. By definition, the nth column of a D-diagonal

matrix

satisfies

a(1n)

=

...

=

a(nn-)d

=

0,

where

d

:=

1 2

(D

-

1).

The algorithm starts by computing |r| = a(n) v(n)(r) from Equation (15). By

choosing the sign indicators s := (s1, . . . , sn), we obtain 2n potential solutions for |a(n)

which read
n

a(n)(s) := sr|r| v(n)(r) .

(32)

r=1

For the inductive steps involving the main minors of size n = 1, . . . , d these are all valid

solutions. However, if n > d, we have additional constraints for |a(n) of the form

e(n)(k) a(n)(s) = 0 for k = 1, . . . , n - d.

The above equations are satisfied only by some choices of s which determine the valid solutions in the nth inductive step. Alternatively, one can verify the solutions by demanding that vector a(n)(s) belongs to the (d - 1)-dimensional sphere, i.e.

n-1

e(n)(k) a(n)(s) 2 = R(n),

(33)

k=n-d

where R(n) depends only on the spectral input data via Equation (3). Thus, we have the following algorithm for realizing the inductive step of constructing A(n+1) from A(n)

(Algorithm 1). One can see that the main contribution to the numerical complexity of the

Algorithm 1 Algorithm for constructing A(n+1) from A(n) for a D-banded matrix.

1: Input: A(n) and its spectral decomposition, (n+1), - numerical accuracy.

2: Output: The main minor A(n+1).

3:

d=

1 2

(D

-

1)



size

of

the

new

column

4: {|r(n)|}nr=1  list of overlaps from Equation (15)

5: solutions  list of solutions for the vector a(n)

6: for s  {0, 1}n do

7: a(n)(s)  vector from Equation (32)

8:

if

n k=n-d+1

e(n)(k) a(n)(s) 2 - R(n) 

then

9:

solutions.append a(n)(s)

10: return solutions

above inductive step relies on the loop going through all 2n choices of the possible signs. Thus, the complexity scales like polyn,d2n, where the polynomial factor polyn,d comes from the necessity of evaluating expressions from Equation (32) and Equation (33) in each step of the loop. What is more, heuristics shows that the required numerical accuracy grows with d and n, as the potential solutions tend to bunch close to the surface of the sphere.
Let us next argue that for generic input data, the result of the above Algorithm 1 for sufficiently small are just two antipodal solutions for a(n). To this end, consider the hyperplanes
Hr(n),± : an-d+1v(n)(r)n-d+1 + . . . + anv(n)(r)n = ±|r|.

Can one hear a matrix?

19

The procedure of intersecting lines with the circle from Subsection 3.2 for d > 2 generalizes here to a procedure involving the study of intersections of hyperplanes Hr(n),± with the (hyper)sphere of radius R(n). As before, typically, as a result of such a procedure one obtains only a single pair of antipodal solutions for a(n). Non-typical situations arise when some
hyperplanes are perpendicular to each other. This requires the existence of certain relations between the eigenvectors of A(n) analogously to the relations stated in Theorem 6. However,
the precise form of these conditions seems to be much more complicated and we leave this
problem open for future research. For an illustration of the intersection procedure for a typical heptadiagonal matrix (d = 3), see Fig. 7.

Figure 7. The result of intersecting eight hyperplanes Hr(n),±, r = 1, 2, 3, 4 for a typical random heptadiagonal matrix. Intersecting one of the hyperplanes with the sphere of radius R(n) results with a single circle. The common points where n circles meet (here n = 4) determine the solutions for the vector a(n) (the red dots). As one can see, typically there are just two such points which lie on the opposite sides of the sphere. The orange and blue colors were used to mark the relevant groups of intersecting circles.

Similarly to the pentadiagonal case, one can find necessary conditions for the spectral data to correspond to a D-diagonal matrix. Every hyperplane Hr(n),± must have a nontrivial intersection with the sphere of radius R(n), hence its distance from the origin must be no greater than R(n).

|r|

 R(n) for all r = 1, . . . , n.

d k=1

(v

(n)(r)n-d+k

)2

Remark 4. Using Theorem 5 we can apply the above methodology mutatis mutandis to nonregular banded matrices. In particular, whenever the dimension of the smaller truncated matrix from Theorem 5 is greater than d, we can use the same redundancy effects to our advantage when dealing with typical matrices.

Can one hear a matrix?

20

4. Improved strategies for D-diagonal matrices ­ the sliding minor construction

The number of independent non vanishing entries of a D-diagonal real symmetric matrix is

1

ND

=

(2N 2

-

d)(d

+

1),

where

D = 2d + 1.

(34)

The spectral data for all the main minors of a D-diagonal N × N matrix is NN-1, which exceeds ND by far. In this section, we provide two alternative methods for reconstructing Ddiagonal matrices and which use much less spectral data. The first utilizes the minimum

number of spectral parameters needed for the purpose, together with the necessary sign

indicators. The second method makes use of N - d - 1 more spectral data. This introduces

enough redundancy to render the method much more straight forward to use at the cost of

being applicable only for generic matrices.

4.1. Inverse method with minimal spectral input

The number of non vanishing data needed to write a D-diagonal matrix, ND, can also be

written

as

ND

=

1 2

d(d

+

1)

+

(d

+

1)(N

-

d).

It

can

be

interpreted

as

the

sum

of

two

terms:

(i)

The spectra

of the minors A(1)

=

A1,1,

A(2),

. . .,

A(d)

which

give

a

total

of

1 2

d(d

+

1)

numbers needed to reconstruct the first d × d minor.

(ii) The spectra of (N - d) minors of size (d + 1) × (d + 1) with upper diagonal entry at

successive positions along the diagonal of A which are denoted by M1(d+1) = A(d+1),

M2(d+1), . . ., MN(d-+d1) (see (35) for an example with N = 6 and d = 2). The "sliding

minors" in the present construction are the

Mk(d+1)

N -d
minors.
k=2





A1,1

000



 

A2,2

 0 0








A3,3



0 

(35)

 

0

A4,4

 





0 0 

A5,5

 

 000

 A6,6

The reconstruction algorithm consists of the following steps.

(1) Reconstruct A(d+1) from the spectral data of A(1) through A(d) and the sign indicators

(

1 2

d(d

+

1)

signs

in

total)

as

described

in

the

inductive

procedure

from

the

proof

of

Theorem 3.

(2) The matrix elements of the d × d minor M2(d) starting with the upper diagonal A2,2 are obtained from the known spectral decomposition of A(d+1) = M1(d+1). The eigenvalues and eigenvectors of M2(d) are then computed.
(3) Use Theorem 3 to reconstruct M2(d+1) from the spectral decomposition of M2(d) and appropriate sign indicator data.

Can one hear a matrix?

21

(4) Repeat the previous two steps recursively to procedure the successive minors M3(d+1), · · · , MN(d-+d1).
The main drawback of this method is the need to provide the sign indicators at each step. To overcome this, one can also introduce minimal redundancy to the above sliding-minor method so that for a typical matrix the required sign data reduces only to the overall signs of columns of A(N). This applies only for generic matrices, as explained in the previous section.

4.2. Inverse method with optimal spectral input

Here, the sliding minors are the

Mk(d+2)

N -d-1
minors (see (36)).
k=2

The increase of their

dimension amounts also to the vanishing of the (1, d + 2) and (d + 2, 1) entries. This enables

turning the redundant information to impose constraints, which for generic matrices remove

the need for computing the sign -indicators required in the previous construction.





A1,1

000



 

A2,2

 0 0


 

A3,3



0

 



(36)

 

0

A4,4

 





0 0 

A5,5

 

 000

 A6,6

The inversion algorithm consists of the following steps.

(1) Reconstruct A(d+1) from the spectral data of A(1) through A(d) and the sign indicators

(

1 2

d(d

+

1)

signs

in

total)

as

described

in

the

inductive

procedure

from

the

proof

of

Theorem 3.

(2) The matrix elements of the (d + 2) × (d + 2) minor M1(d+2) starting at the upper diagonal A1,1 are computed using the algorithm proposed in section (3.3). The only sign indicator needed is the sign of the entry A1,d+1.
(3) The (d+1), (d+1) minor M2(d+1) is extracted from the computed M1(d+2). Its eigenvalues and eigenvectors are then computed. (M2(d+1) is the d + 1 × d + 1 minor with A2,2 as its upper diagonal entry)

(4) Use Theorem 3 to reconstruct M3(d+2) from the spectral decomposition of M2(d+1) and the sign of the entry A1,d+1.

(5) Repeat the previous two steps recursively to produce the successive minors M3(d+1), · · · , MN(d-+d2-) 1.

In order to minimize further the set of exceptional matrices where this strategy fails, one could choose a larger dimension for the sliding minor. In choosing an "optimum" strategy, one has to weigh computational effort against minimizing the exceptional set. This is an individual decision left for the discretion of the practitioner.

Can one hear a matrix?

22

4.3. Pentadiagonal matrices

The first inversion strategy for regular pentadiagonal matrices (d = 2) is a combination of

two steps which we have already analyzed in detail in the previous sections. The first step

of reconstructing A(2) has been described in the proof of Theorem 3 (see Equation (19) and

Equation (20)). The second as well as all other steps rely on reconstructing 3 × 3 matrices

Mk(3)

N -2
from their top-left 2 × 2 minors.
k=2

This has been done explicitly in subsection

2.1.1. In total, the 2 × 2  3 × 3 steps require the following sign data.

(i) The sign of the matrix element A1,2.

(ii) A pair of signs from Equation (23) applied to each minor

Mk(3)

N -2
.
k=2

The second inversion strategy which uses the larger 4 × 4 sliding minor gives extra

redundancy and, as explained above, it typically reduces the required sign data to the overall

signs of columns of A(N). Here, the notion of typicality is particularly straightforward to

state. Namely, all the sliding minors of a typical matrix do not satisfy the -condition from

Theorem 6 . The computation of the spectral decomposition of the (3 × 3) minors can be

carried out analytically.

5. Summary
We revisited here the question of how much spectral data is needed to determine a real symmetric matrix. In principle, a matrix is determined by its spectrum only up to unitary equivalence. Hence, more spectral information is needed in order to reconstruct a nondiagonal matrix. Heuristically, the number of spectral data should match the number of unknowns. Hence, for a generic real symmetric N × N matrix, we need N (N + 1)/2 numbers coming from the spectral data. In our work, these numbers come from the spectra of the main minors. Given such spectra, we provide a finite algorithm that reconstructs the matrix up to a finite number of possibilities. The construction can be made unique by supplementing additional sign data as stated in Theorem 3 which comprises of N (N - 1)/2 signs in total. However, the signs are not easily deduced from the matrix itself, therefore the requirement of supplementing the signs is a notable limitation to our procedure. Thus, in further parts of this paper, we focus on ways of reducing the required sign data in the case of banded matrices. For (2d + 1)-banded matrices, there are less unknowns, hence one can find a smaller optimal set of (d + 1) × (d + 1) minors whose spectra allow one to reconstruct the matrix up to a finite number of possibilities. As before, some additional sign information is required to make the construction unique. By analysing the case of pentadiagonal matrices, we compare both methods of reconstruction. We find that the redundant information coming from the spectra of all main minors in a typical case (where the notion of typicality has been made precise in Subsection 3.2) allows one to reduce the number of required signs to N - 1 that are just the overall signs of columns of the upper-triangular part of A(N), while using the optimal amount of spectral information requires 2N - 1 signs. In Section 4 we argue that this is a general fact, i.e. the redundancy coming from all main minors typically allows one to uniquely reproduce

Can one hear a matrix?

23

a general banded matrix from the spectral data using only the overall signs of columns of the upper-triangular part of A(N).

Acknowledgment
We would like to thank Professors Raphael Loewy and Percy Deift for suggestions and critical comments during the first stages of the present work. The inspiration for the work came by listening to a seminar by Professor Christiane Tretter within the webinar series "Spectral Geometry in the Clouds". Thanks Christiane. The seminar is organized by Doctors Jean Lagacé and Alexandre Girouard whom we thank for initiating and running such successful seminar during the miserable Corona days.

Appendix A. Identities for Cauchy Matrices

For two sequences of real numbers, x := {xi}ni=1 and y := {yi}ni=1, define the Cauchy matrix

C(x, y)

as

(C(x, y))i,j

:=

. 1
xi-yj

The

matrix

C(x, y)

has

the

followng

symmetry

(C(x, y))T = -C(y, x).

(A.1)

C will be used to denote C(x, y) by default. An explicit expression for C-1 reads [21]

(C-1)i,j =

(yi

-

xi)

n k=i

yi yi

- -

xk yk

1 yi - xj

(xj

-

yj )

n k=j

xj xj

- -

yk xk

The sum of the matrix elements of C-1 is known to be [23]

n

n

(C-1)i,j = (xk - yk)

i,j=1

k=1

We aim to prove the following identities

C -1

1 =1-

i,j

i,j xj

n
i=1 n
i=1

yi xi

,

1 C-1 =

i,j yi

i,j

n i=1

xi

n i=1

yi

- 1,

(A.2) (A.3) (A.4)

i,j

yi

C -1

i,j

1 xj

=

n i=1

yi

n i=1

xi

n j=1

(xj

- yj) ,

i,j

1 yi

C-1 i,j xj =

n i=1

xi

n i=1

yi

n
(xj
j=1

- yj) .(A.5)

Note first, that the LHS identities imply the RHS identities by symmetry A.1. Hence, it is

enough to prove the LHS identities. To this end, we write them down as the following matrix-

vector products

C -1

1 = 1(n) C-1 | 1 (n) ,

i,j

i,j xj

x

yi
i,j

C -1

1 =

i,j xj

y(n) C-1 | 1 (n) x

,

(A.6)

where

vectors

|1(n)

,

|

1 x

(n)

and |y(n)

are defined as

1

 1/x1 

1

|1(n)

:=

  

...

,  

1 (n) | x

:=

   

1/x2 ...

   

,

1

1/xn

 y1 

|y(n)

 y2

:=

  

...

 .  

yn

Can one hear a matrix?

24

We will use the following lemma.

Lemma 7 (Vandermonde Matrix Identity for Cauchy Matrix). C(x, y) has the following decomposition

C(x, y) = -P Vx-1VyQ-1, where P and Q are diagonal matrices defined as

P = diag (p1(x1), . . . , pn(xn)) , Q = diag (p(y1), . . . , p(yn))

with

n

n

p(t) := (t - xi), pk(t) :=

(t - xi), 1  k  n,

i=1

i=1,i=k

and Vx, Vy are the Vandermonde matrices

 1 1 ··· 1 



Vx

=

 



x1 ...

x2 ...

··· ...

xn ...

 ,  

xn1-1 xn2-1 · · · xnn-1

By Lemma 7, we have

 1 1 ··· 1 



Vy

=

 



y1 ...

y2 ...

··· ...

yn ...

 .  

y1n-1 y2n-1 · · · ynn-1

C-1 = -QVY-1VxP -1.

Next, we define the vectors |1 , |y and |x as

1| := - 1(n) QVY-1,

y| := - y(n) QVY-1,

|x

:=

VxP

-1

|

1 x

(n)

.

Calculation of A.6 boils down to computing overlaps

i,j

C -1

1 =
i,j xj

1 | x

,

i,j

yi

C -1

1 =

i,j xj

y | x

.

(A.7)

Let us start with finding |x . A straightforward calculation using Equation (A.7) shows

n

1

k=1 xkpk(xk)

n


1



|x

=

  

...

k=1 pk(xk)

 

.







n

xnk -2

k=1 pk(xk)

(A.8)

In order to compute the sums in Equation (A.8), we use the following Lemma [22].

Lemma 8 (Summation of Powers over Product of Differences). Let (t1, t2, . . . , tn) be a sequence of real numbers. Then,

0

:0m<n-1





n k=1

tmk i=1,i=k(tk - ti)

  

1

=n

   

tj

:m=n-1 :m=n



j=1

Can one hear a matrix?

25

By Lemma 8, we have e(jn) x = for j  2. The only nonzero entry is

e(1n) x

n

1

(-1)n

=

=-

,

k=1 xkpk(xk)

x1· . . . · xn

which is evaluated in the following Proposition.

(A.9)

Proposition 9. Let (t1, t2, . . . , tn) be a sequence of real numbers. Then,

n
k=1 tk

1

(-1)n

=-

.

i=k(tk - ti)

t1· . . . · tn

Proof. Consider the following complex integral over a circle of radius R > max{|ti| : 1  i  n}

1

dz

IR(t1, . . . , tn) := 2i

. |z|=R z(z - t1)· . . . · (z - tn)

Because the integrand is a regular function at z = , we have IR(t1, . . . , tn) = 0. On the

other hand, the integrand has simple poles at z = 0, z = t1, . . ., z = tn. Hence, the residue

theorem asserts that

IR(t1, . . . , tn)

=

(-1)n 1 t1· . . . · tn

+

n k=1

tk

1 ,
i=k(tk - ti)

which finishes the proof.

Next, let us compute the vectors |1 and |y . Note first that because the first entry of |x is its only nonzero entry, we only need to find the first entries of |1 and |y . By Equation (A.7), the desired vectors are solutions to the following linear equations

VYT |1 = -QT |1(n) , VYT |y = -QT |y(n) .

 p(y1) 

QT |1(n)

 p(y2) 

=

  

...

,  

p(y3)

 y1p(y1) 

QT |y(n)

 y2p(y2)

=

  

...

 .  

ynp(yn)

We use now Crammer's rule.

 p(y1) y1 y12 · · · y1n-1 

e(1n) 1

=

1 - det VyT

   

p(y2) ...

y2 ...

y22 ...

··· ...

y2n-1 ...

   

p(yn) yn yn2 · · · ynn-1

 y1p(y1) y1 y12 · · · y1n-1 

e(1n) y

1 = - det VyT

   

y2p(y2) ...

y2 ...

y22 ...

··· ...

y2n-1 ...

   

.

ynp(yn) yn yn2 · · · ynn-1

Can one hear a matrix?

26

Applying the Laplace expansion of both determinants with respect to their first columns.

e(1n) 1 e(1n) y

1 =-
det VyT

n
(-1)jp(yj) det (V
j=1

(y1, . . . , y^j, . . . , yn))T

,

1 = - det VyT

n
(-1)jyjp(yj) det (V
j=1

(y1, . . . , y^j, . . . , yn))T

,

where

 y1

 

...

y12 · · · y1n-1 

...

...

...

 

(V

(y1, . . . , y^j, . . . , yn))T

:=

   

yj-1 yj+1

  

...

yj2-1 yj2+1
...

···
··· ...

yjn--11

 

yjn+-11 ...

    

yn yn2 · · · ynn-1

Using the expression for the determinant of a Vandermonde matrix, we have

det (V (y1, . . . , y^j, . . . , yn))T =

yk ·

(yl - yk)

k=j

k<l,k=j,l=j

It follows that

det (V

(y1, . . . , y^j, . . . , yn))T det VyT

= (-1)n-j

k=j yk

= (-1)n-j k=j yk .

k=j(yk - yj)

pj (yj )

After application of the above result, the desired expressions read

e(1n) 1

=

(-1)ny1· . . . · yn

n j=1

p(yj) , yj pj (yj )

e(1n) y

=

(-1)ny1· . . . · yn

n j=1

p(yj) .(A.10) pj (yj )

As the final step, we use the following expansion of p(t).
n
p(t) = tn + altl-1, al = (-1)n+1-len+1-l ({x1, . . . , xn}) ,
l=1
where en+1-l ({x1, . . . , xn}) is the (n+1-l)th Viete sum in variables {x1, . . . , xn}. Plugging the above expansion into Equation (A.10), we get

e(1n) 1 = (-1)ny1· . . . · yn

n j=1

yjn-1 pj (yj )

+

n l=1

al

n j=1

yjl-2 pj (yj )

.

e(1n) y = (-1)ny1· . . . · yn

n j=1

yjn pj (yj )

+

n l=1

al

n j=1

yjl-1 pj (yj )

.

By Lemma 8, we get that

e(1n) 1

= (-1)ny1· . . . · yn

n

1

1 + a1 j=1 yjpj(yj)

,

n

e(1n) y = (-1)ny1· . . . · yn

yj + an .

j=1

Can one hear a matrix?

27

Finally, after making use of Proposition 9 and expanding Viete sums a1 = (-1)nx1· . . . · xn

and an = -(x1 + . . . + xn)

e(1n) 1 = (-1)n(y1· . . . · yn - x1· . . . · xn),

n
e(1n) y = (-1)ny1· . . . · yn (yj - xj).

j=1

The result follows now directly by multiplying the above expressions by e(1n) x from Equation (A.9).

The identity which is used in the discussion of (17) follows by combining Equation (A.4)

and Equation (A.5) to give

s=

i,j

1 yi

(C -1 )i,j (xj

-

s)

,

where

s

=

n
(xk
k=1

-

yk)

.

(A.11)

Appendix B. Derivation of the formula for (r(n))2.

The aim is to prove the identity used in Equation (15) which reads

n
C-1 r,k (xk - h) = -
k=1

n+1 k=1

(yr

-

xk)

n k=1,k=r

(yr

-

yk

)

,

(B.1)

where C = C(x, y) is a Cauchy matrix and h :=

n+1 k=1

xk

-

n k=1

yn.

Note

first

that

n

n

C-1 r,k xk = e(rn) C-1 |x(n) ,
k=1

C-1 = r,k

e(rn) C-1 |1(n) .

k=1

(B.2)

Next, let us use Lemma 7 to find that C-1 = -QVY-1VxP -1. By a reasoning relying on the

use of Lemma 8 (which is analogous to the one used in Equation (A.9)), we obtain that





VxP -1 |x(n)

=

 





0 ... 

0

 

,

 1

n k=1

xk

0

VxP -1 |1(n)

=

 

...

 

0

1

(B.3)

The next step is the multiplication by Vy-1. To this end, we apply the following result

concerning the inverse of a Vandermonde matrix [22].

Lemma 10. Let Vt(n) be the Vandermonde matrix of size n in variables t1, . . . , tn, i.e. Vi,j = tji-1, 1  i, j  n. Then

Vt(n)

-1

i,j

=

(-1)n-j

en-j({t1, . kk==1n,k=i(ti

. . , tn}/{ti}) - tk)

,

where em denotes the mth Viete sum.

We only need the last two columns of (Vy)-1, because using Equation (B.3), we get

n
e(rn) C-1 |x(n) = -p(yr) ((Vy)-1)r,n-1 + ((Vy)-1)r,n xk ,
k=1
e(rn) C-1 |1(n) = -p(yr)((Vy)-1)r,n.

(B.4) (B.5)

Can one hear a matrix?

28

By Lemma 10, the necessary matrix elements of Vy-1 forms read

((Vy)-1)r,n-1 = -

n k=1,k=r
kk==n1,k=r (yr

yk - yk)

,

((Vy)-1)r,n =

1

k=n k=1,k=r

(yr

-

. yk)

Plugging the above results to the LHS of Equation (B.1) yields

n
C-1 r,k (xk - h) =
k=1

p(yr)

k=n k=1,k=r

(yr

-

yk)

n
h - yr - (xk - yk)
k=1

,

which after substituting expressions for h and p(yr) yields the desired result.

Appendix C. Non-regular matrices: proof of Theorem 5

Let us start with the simplest situation where for some n we have (n)  (n+1) = 1 and

both spectra are non-degenerate. The other cases can be analyzed by essentially repeating and

slightly adjusting the proof of the simple case. By the interlacing property (6), the scenario

described above happens if and only if there exists a single l  {1, . . . , n} such that either (ln) = (ln+1) or (ln) = (l+n+1 1). Equation (11) applied to k = r = l, gives bl,n+1l(n) = 0, i.e. i) bl,n+1 = 0 or ii) l(n) = 0. The two cases are analyzed separately.
(i) bl,n+1 = 0. Equation (11) applied to k = l and r = l gives ((ln+1) - (rn))bl,r = 0, which in turn implies that bl,r for r = l (recall that (ln+1) = r(n) by the assumption that (n)  (n+1) = 1). Thus, the eigenvector v~(n+1)(l) = v(n)(l), 0 . Equation (10)
applied for k = l implies v(n)(l) a(n) = 0, thus bl,n+1 = 0 leads to l(n) = 0.
(ii) l(n) = 0 and bl,n+1 = 0. Here, we will show that this is in contradiction with the assumption that (n)  (n+1) = 1. To this end, we combine Equations (11) for r = l

with Equations (10) to obtain the following reduced set of equations involving a Cauchy

matrix

(kn+1)

-

h~

=

n r=1,r=l

r(n) (kn+1) -

2
(rn) ,

 1  k  n + 1,

(C.1)

where h~ =

n+1 k=1,k=l

(kn+1)

-

n k=1,k=l

k(n).

Note, that Equations (C.1) for k

=l

can be viewed as a set equations for the regular spectra ~(n-1) := (n)/{(ln)} and

~(n) := (n+1)/{(ln+1)}. Thus, solutions for

r(n)

2
, r = l read

(r(n))2 = -

n+1 k=1,k=l

(rn) - (kn+1)

.

n k=1,k/{r,l}

r(n) - (kn)

(C.2)

However, the above formula for

r(n)

2
has to be consistent with Equation (C.1) for

k = l. By denoting (ln) = (ln+1) := , this boils down to the following condition

n

 - h~ =

-

1

r=1,r=l  - (rn)

n+1 k=1,k=l

(rn) - (kn+1)

,

n k=1,k/{r,l}

(rn) - (kn)

(C.3)

Can one hear a matrix?

29

We view Equation (C.3) as a polynomial equation for . It turns out that Equation (C.3)

can be simplified to the equation

n+1 k=1,k=l

 - (kn+1)

= 0. This is in contradiction

with the regularity assumption, because it implies that  = (l+n+1 1).

Summing up the above analysis of the simplest case where (ln) = (ln+1) for a single l and

both spectra are non-degenerate, we have obtained that we necessarily have l(n) = 0 and

v~(n+1)(l)

= v(n)(l), 0 . The remaining solutions for

r(n)

2
, r = l are given by Equation

(C.2). Moreover, Equation (11) applied for k = l yields bk,r = 0 whenever r = l and
k = l. Thus, the expressions for the eigenvectors v~(n+1)(r) , r = l, of the matrix A(n+1) are formally identical with the expressions for the eigenvectors of a regular matrix A~(n) obtained by removing the eigenspaces corresponding to the eigenvalue (ln) in A(n) and A(n+1).
Let us next consider the situation where there is a single degeneracy block in (n) and

(n+1). Note first that whenever k  D(n+1)(l, m) and r  D(n)(l, m), then Equation (11) implies that bk,n+1r(n) = 0. Thus, we have two possibilities for each k  D(n+1)(l, m):

(i) bk,n+1 = 0 or (ii) bk,n+1 = 0 and r(n) = 0 for all r  D(n)(l, m).
As the following Lemma states, case (ii) is only relevant when D(n+1)(l, m) = DI(n+1)(l, m).
Lemma 11. Assume that the spectrum (n) has degeneracy indices D(n)(l, m) and denote the degenerate eigenvalue by . If the degeneracy indices in the spectrum (n+1) are of the form DI(nI+1)(l, m), DI(nII+1)(l, m) or DI(nV+1)(l, m), then for every k  D(n+1)(l, m) we have v~(n+1)(k) e(n+1)(n + 1) = 0, i.e. the coefficient bk,n+1 = 0.

Proof. (Ad absurdum.) Assume that bk,n+1 = 0 for some k  D(n+1)(l, m) and D(n+1)(l, m) is of the form Di(n+1)(l, m) with i  {II, III, IV }. Denote K := {k  D(n+1)(l, m) : bk,n+1 = 0}. Then, Equation (11) applied for any k  K and r  D(n)(l, m) implies that r(n) = 0 for all r  D(n)(l, m). On the other hand, for any k  K 
{1, . . . , n + 1}/D(n+1)(l, m) , Equation (11) gives

bk,r

=

r(n) (kn+1) -

(rn) bk,n+1.

This in turn combined with Equation (10) for the same k yields

(kn+1) - h =

(n)
rD(n)(l,m) r
(kn+1) - 

2

r(n) 2

+ r/D(n)(l,m) (kn+1) - (rn) .

(C.4)

In particular, we can use the Cauchy matrix method to solve the above equations for

rD(n)(l,m) r(n) 2 or

r(n)

2
with r

/

D(n)(l, m).

However, each case with Di(n),

i  {II, III, IV } has to be treated separately.

(i) D(n+1)(l, m) = DI(nV+1)(l, m). Consider the set of Equations (C.4) for k  {1, . . . , n + 1}/DI(nV+1)(l, m). They are Cauchy equations for the regular spectra of a reduced dimension ~(n-m+1) = {(kn+1) : k  {1, . . . , n + 1}/DI(nV+1)(l, m)} and ~(n-m) =

Can one hear a matrix?

30

{}  {(kn) : k  {1, . . . , n}/D4(n)(l, m)} with

~1(n-m)

2
:=

analogy to Equation (15), we solve Equation (C.4) to obtain

rD(n)(l,m)

r(n)

2
. In

~1(n-m)

2
=-

n+1 k=1,k/DI(nV+1)(l,m)

 - (kn+1)

.

n k=1,k/D(n)(l,m)

 - (kn)

(C.5)

As we mentioned earlier, K =  implies ~1(n-m) 2 = 0, thus the above expression
implies that (kn+1) =  for some k / DI(nV+1)(l, m), which is in contradiction with the Lemma's assumptions.
(ii) D(n+1)(l, m) = DI(nI+1)(l, m) or D(n+1)(l, m) = DI(nII+1)(l, m) Consider the set of Equations (C.4) for k  {1, . . . , n + 1}/D(n+1)(l, m) with r(n) = 0 whenever r  D(n)(l, m). They are Cauchy equations for the regular spectra of a reduced dimension ~(n-m) = {(kn+1) : k  {1, . . . , n + 1}/D(n+1)(l, m)} and ~(n-m-1) = {k(n) : k  {1, . . . , n}/D(n)(l, m)}. In analogy to Equation (15), we solve Equation (C.4) to obtain

(r(n))2 = -

n+1 k=1,k/D(n+1)(l,m)

r(n) - (kn+1)

,

n k=1,k/D(n)(l,m),k=r

(rn) - (kn)

r  {1, . . . , n}/D(n)(l, m).

(C.6)

The above expressions for (r(n))2 have to be compatible with Equation (C.4) applied to k  K . This is in full analogy to Equation (C.3) which we treated as a polynomial equation for . This polynomial equation is equivalent to

n+1 k=1,k/D(n+1)(l,m)

 - k(n+1)

= 0.

This in turn implies that (kn+1) =  for some k / D(n+1)(l, m), which is in contradiction with this Lemma's assumptions.

Thus, whenever D(n+1)(l, m) = DI(n+1)(l, m), we have bk,n+1 = 0 for all k  D(n+1)(l, m). Let us now focus on this particular case. By Equation (11) we get that for
every k  D(n+1)(l, m) and r / D(n)(l, m) bk,r = 0. Thus, vector v~(n+1)(k) effectively belongs to the degenerate subspace of A(n), i.e.

v~(n+1)(k) =

bk,r v(n)(r), 0 , k  D(n+1)(l, m).

rD(n)(l,m)

(C.7)

Depending on the precise form of the set D(n+1)(l, m), the dimension of the degenerate subspace of A(n+1) can be:

(i) equal to the dimension of the degenerate subspace of A(n); this happens when D(n+1)(l, m) = Di(n+1)(l, m) with i = II, III
(ii) one dimension smaller than the degenerate subspace of A(n); this happens when D(n+1)(l, m) = DI(nV+1)(l, m).

Can one hear a matrix?

31

If D(n+1)(l, m) = Di(n+1)(l, m) with i = II, III, then Equation (C.7) effectively describes just a change of basis. Thus, coefficients br,k with r  D(n)(l, m) and k  D(n+1)(l, m) form an orthogonal matrix. If D(n+1)(l, m) = DI(nV+1)(l, m), then Equation (C.7) geometrically means that the degenerate eigenspace of A(n+1) can be any subspace of the
degenerate eigenspace of A(n) of codimension one. Equation (10) yields the condition that
v~(n+1)(k) a(n) = 0 for all k  D(n+1)(l, m), i.e.

bk,rr(n) = 0, k  D(n+1)(l, m).
rD(n)(l,m)

(C.8)

If D(n+1)(l, m) = Di(n+1)(l, m) with i = II, III, the dimensionality of the degenerate eigenspaces together with Equation (C.8) forces r(n) = 0 for all r  D(n)(l, m). This together with Equation (11) imply that any eigenvector of A(n+1) corresponding to a nondegenerate eigenvalue is orthogonal to the degenerate eigenspace of A(n). Thus, we have
effectively reduced the dimension of the problem by m + 1 and we can apply Theorem 3 for the regular reduced matrices A~(n-m-1) and A~(n-m) with the degenerate eigenspaces removed.
In particular,

bk,r

=

r(n) (kn+1) -

(rn) bk,n+1,

r / D(n)(l, m),

k / D(n+1)(l, m),

a(n) =

r(n) v(n)(r), 0 ,

r/D(n)(l,m)

r(n)

2
=-

n+1 k=1,k/D(n+1)(l,m)

(rn) - (kn+1)

n k=1,k/D(n)(l,m),k=r

r(n) - k(n)

and bk,r = 0 for k / D(n+1)(l, m) otherwise. Let us next move to the case when D(n+1)(l, m) = DI(nV+1)(l, m). The geometric
interpretation of Equation (C.8) is that a(n) is orthogonal to the degenerate eigenspace
of A(n+1) embedded in the degenerate eigenspace of A(n). Consider next Equation (C.4) from the proof of Lemma 11 applied for k / DI(nV+1)(l, m). This is a set of n - m + 1 equations involving regular spectra of a reduced dimension ~(n-m+1) = {(kn+1) : k  {1, . . . , n + 1}/DI(nV+1)(l, m)} and ~(n-m) = {}  {(kn) : k  {1, . . . , n}/D(n)(l, m)}. The reduced regular matrix A~(n-m+1) is obtained from A(n+1) by removing the entire degenerate eigenspace with eigenvalue . The reduced regular matrix A~(n-m) is determined by the
embedding of the degenerate eigenspace of A(n+1) into the degenerate eigenspace of A(n)
­ the image of this embedding is removed. Next, we use the familiar method of Theorem 3
applied the above regular matrices. However, in contrast to the previous case the coefficients r(n) and bk,r for r  D(n)(l, m) and k / D(n+1)(l, m) may be nonzero and are determined by
the choice of the embedding of eigenspaces. Finally, let us consider D(n+1)(l, m) = DI(n+1)(l, m). Firstly, note that |DI(n+1)(l, m)| =
|D(n)(l, m)| + 1 = m + 2, thus the degenerate eigenspace of A(n+1) cannot be spanned only
by the vectors v(n)(r), 0 with r  D(n)(l, m). Hence, we necessarily have bk,n+1 = 0 for some k  DI(n+1)(l, m). By Equation (11) this implies that r(n) = 0 for all r  D(n)(l, m), i.e. a(n) is orthogonal to the degenerate eigenspace of A(n). This in turn implies that for any k / DI(n+1)(l, m) we have bk,r = 0 whenever r  D(n)(l, m), i.e. the non-degenerate

Can one hear a matrix?

32

eigenvectors of A(n+1) are orthogonal to the degenerate eigenspace of A(n). Thus, we consider the regular matrices A~(n-m) and A~(n-m-1) which are the original matrices truncated to the
space defined as the orthogonal complement of the degenerate eigenspace of A(n). They have the regular spectra ~(n-m) = {}  {k(n+1) : k  {1, . . . , n + 1}/DI(n+1)(l, m)} and ~(n-m-1) = {(kn) : k  {1, . . . , n}/D(n)(l, m)}. Thus, we use Theorem 3 to find the vector a(n) as well as the non-degenerate eigenvectors of A(n+1). What is left to find are the
remaining m + 2 degenerate eigenvectors of A(n+1) corresponding to the eigenvalue . Their
coefficients bk,r with r / D(n)(l, m) are proportional to bk,n+1 as stated in Equation (11), i.e.

bk,r

=



r(n) - (rn)

bk,n+1,

Note, that the equations

r / D(n)(l, m),

k  DI(n+1)(l, m).

n

( - h)bk,n+1 =

bk,r r(n) ,

r=1,r/D(n)(l,m)

k  DI(n+1)(l, m).

are now automatically satisfied by the above expressions for bk,r. The unknown coefficients bk,n+1 and br,k with k  DI(n+1)(l, m) and r / D(n)(l, m) are completely free to choose so that the resulting vectors v~(n+1)(k) kDI(n+1)(l,m) form an orthonormal basis of the degenerate eigenspace.

References
[1] Boley D., Golub G.H. Inverse eigenvalue problems for band matrices, In: Watson G.A. (eds) Numerical Analysis. Lecture Notes in Mathematics, vol 630. Springer, Berlin, Heidelberg, 1978
[2] F. W. Biegler-König, Construction of band matrices from spectral data, Linear Algebra and its Applications Vol. 40, Pages 79-87, 1981
[3] D. Boley and G. H. Golub, A survey of matrix inverse eigenvalue problems, Inverse Problems 3 595, 1987 [4] M. Chu and G. Golub, Inverse Eigenvalue Problems: Theory, Algorithms, and Applications, Oxford
University Press, 2005 [5] Kent Griffin, Michael J. Tsatsomeros, Principal minors, Part II: The principal minor assignment problem,
Linear Algebra and its Applications 419 125­171, 2006 [6] R. Loewy, Principal minors and diagonal similarity of matrices, Linear Algebra and its Applications 78
23­64, 1986 [7] Shmuel Friedland, Matrices with prescribed off-diagonal elements, lsrael J. Math. 11:184-189 (1972). [8] R. S. MartinJ. H. Wilkinson, Symmetric Decomposition of Positive Definite Band Matrices, Numer. Math.
7, 355 - 361 (1965) [9] Ayoub, A. B., The central conic sections revisited, Mathematics Magazine, 66 (5): 322­325, 1993 [10] L. Anderson, On the effective determination of the wave operator from given spectral data in the case of
a difference equation corresponding to a Sturm-Liouville differential equation Math. Anal. A 29 (1970), 467497. [11] F. R. Gantmacher and M. G. Krein, Oszillationsmatrizen, Oszillations-keme und kleine Schwingungen mechanischer Systeme, Akademie-Verlag, Berlin, 1960. [12] H. Hochstadt, On some inverse problems in matrix theory, Arch. Math. 18 (1967), 201-207. [13] H. Hochstadt, On the construction of a Jacobi matrix from spectral data , Linear Algebra Appl. 8 (1974), 435446. [14] Ole H. Hald, Inverse Eigenvalue Problems for Jacobi Matrices, LINEAR ALGEBRA AND ITS APPLICATIONS 14, 63-85 (1976)

Can one hear a matrix?

33

[15] Fritz Gesztesy, Inverse spectral theory as influenced by Barry Simon In Spectral Theory and Mathematical Physics: A Festschrift in Honor of Barry Simon's 60th Birthday Fritz Gesztesy, Percy Deift, Cherie Galvez, Peter Perry, and Wilhelm Schlag, Editors ISBN-10: 0-8218-3783-4 (2010)
[16] Steven J. Cox, Mark Embree, Jeffrey M. Hokanson, One Can Hear the Composition of a String: Experiment with an Inverse Eigenvalue Problem SIAM REVIEW 54, 157­178, (2012 )
[17] P. B. Denton, S. J. Parke, and X. Zhang, Neutrino oscillations in matter via eigenvalues, Phys. Rev. D 101, 093001 (2020)
[18] P. B. Denton, S. J. Parke, T. Tao, Xining Zhang, Eigenvectors from eigenvalues: A survey of a basic identity in linear algebra, BULLETIN (New Series) OF THE AMERICAN MATHEMATICAL SOCIETY, 2021 Article electronically published on February 18, 2021
[19] Gerschgorin S Über die Abgrenzung der Eigenwerte einer Matrix Izv. Akad. Nauk. USSR Otd. Fiz.-Mat. Nauk 6 749­54 (1931)
[20] Q. I. Rahman and G. Schmeisser, Analytic theory of polynomials, London Mathematical Society Monographs. New Series, 26, The Clarendon Press Oxford University Press, Oxford, 2002, ISBN 019-853493-0. MR 2004b:30015
[21] R. Gow Cauchy's matrix, the Vandermonde matrix and polynomial interpolation Irish Math. Soc. Bull 28, 45-52 (1992)
[22] D. E. Knuth, Art of Computer Programming, The: Volume 1: Fundamental Algorithms, Addison-Wesley; 3rd edition (1997)
[23] See https://proofwiki.org/wiki/VandermondeMatrixIdentityforCauchyMatrix

