Markpainting: Adversarial Machine Learning meets Inpainting

arXiv:2106.00660v1 [cs.LG] 1 Jun 2021

David Khachaturov * 1 Ilia Shumailov * 1 2 Yiren Zhao 1 Nicolas Papernot 2 Ross Anderson 1

Abstract
Inpainting is a learned interpolation technique that is based on generative modeling and used to populate masked or missing pieces in an image; it has wide applications in picture editing and retouching. Recently, inpainting started being used for watermark removal, raising concerns. In this paper we study how to manipulate it using our markpainting technique. First, we show how an image owner with access to an inpainting model can augment their image in such a way that any attempt to edit it using that model will add arbitrary visible information. We find that we can target multiple different models simultaneously with our technique. This can be designed to reconstitute a watermark if the editor had been trying to remove it. Second, we show that our markpainting technique is transferable to models that have different architectures or were trained on different datasets, so watermarks created using it are difficult for adversaries to remove. Markpainting is novel and can be used as a manipulation alarm that becomes visible in the event of inpainting. Source code is available at: https://github. com/iliaishacked/markpainting.
1. Introduction
Improvements to machine learning (ML) have enabled automatic content creation (Ramesh et al., 2021) and manipulation (Yu et al., 2018): a user just needs to provide an image and describe the changes they want as the input to a generative model (Goodfellow, 2016; Korshunova et al., 2017; Antic, 2018). Computer graphics tools brought us digital inpainting: programs such as Photoshop enable manipulation of digital images with powerful software and, more recently, ML support (Vincent, 2020). Modern inpainting software lets the user select a patch to be filled in; it then fills this
*Equal contribution 1Computer Laboratory, University of Cambridge 2University of Toronto and Vector Institute. Correspondence to: Ilia Shumailov <ilia.shumailov@cl.cam.ac.uk>.
Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s).

area in with artificially generated content.
One increasingly popular application of inpainting is the removal of objects from photographs. This can be done for malicious purposes. For example, many images are distributed with a watermark that asserts copyright or carries a marketing message; people wishing to reuse the image without permission may want to remove the mark and restore a plausible background in its place. This naturally leads to the question of how we can make watermarks more robust, i.e. difficult to remove. There is substantial literature on using classic signal-processing techniques for mark removal, e.g. from Cox et al. (2007), but such tricks predate recent advances in ML and inpainting more specifically.
In this paper we investigate whether ML inpainters can be manipulated using techniques adapted from the field of adversarial machine learning. Our technique, which we dub markpainting, allows for arbitrary manipulation of how inpainters fill in the patch defined by a given 2-bit image mask. We do this by setting an arbitrary target image which we wish to appear in the filled-in area. We then generate perturbations ­ small pixel-wise augmentations ­ which are applied to the original image to manipulate the inpainting algorithms into producing something resembling our target. For example, in Figure 1a, the original image is a blackand-white cartoon; we set the target image to be the same cartoon but with La Gioconda pasted onto the otherwise blank canvas. After the application of our technique, the perturbations to the original image ensure that the resulting infilled patch does indeed resemble our target.
We find that the introduction of minor perturbations to input images can force many inpainters to generate arbitrary patches -- even patterns not present in the training datasets. Consequently, setting the target to be the original image and applying our markpainting technique makes the image robust against watermark removal as shown in Figure 2. The original (left-most) image has an unsightly watermark that was removed successfully in the middle image by an inpainter. However, after treating the original image with our markpainting technique ­ setting the target to be the original image itself, to preserve the watermark ­ the attempt to paint out the watermark fails.
Figure 1b demonstrates the effect of markpainting on six different inpainters. The resulting markpainted sample (bottom

Markpainting: Adversarial Machine Learning meets Inpainting







+



mask

input image

Infill Model

result

perturbation
Lmark Loss
target

(a)

(b)

Figure 1. Demonstration of the proposed markpainting technique. The target image is set to be Leonardo da Vinci's La Gioconda pasted onto the otherwise-blank cartoon canvas. Figure 1a shows a visual abstract of the proposed markpainting technique, using the CRFILL model. Figure 1b shows the application of markpainting to multiple different inpainting models simultaneously -- our technique can target multiple models at once and is not limited to just a single model. The Adversarial pane shows the combination of the original input image and the resulting perturbations. The top six images show the result of various inpainters filling-in the rectangular patch on the canvas as defined by the mask. Note that all six inpainters use the same input, namely Adversarial. Original cartoon from freesvg.

evaluate the extent to which markpainting transfers from one inpainter to another and within the same inpainter trained on different datasets; the impact of perturbation size; and the viability of mask-agnostic markpainting.
Overall, we make the following contributions:

Figure 2. Example of countering watermark removal using markpainting on Vincent van Gogh's Boats at Sea. The left-most image depicts the original image with the watermark. The middle image is the result of inpainting the mark without any perturbations, resulting is the successful removal of the watermark. The right-most image contains generated perturbations and has been treated with an inpainter for watermark removal; the output simply restores the mark. Performed on the CRFILL inpainting model with = 0.3.
right in Figure 1b) is a combination of the original image (top left in Figure 1a) and the accumulated perturbations. We can see that La Gioconda (the target) appears on the canvases (the patch to fill in as dictated by the mask) of the final inpainted images (top two rows in Figure 1b). These final images are obtained by running the markpainted sample through each of the inpainting models.
We find that markpainting can work even if the colors and structures of the target image are not present in the input image itself or the dataset the model was trained on. We

· We show that inpainting can be manipulated to produce arbitrary content, a technique we name markpainting.
· We present a mask-agnostic markpainting method that works regardless of the mask used.
· We evaluate the performance of markpainting thoroughly and find that markpainting a specific target is significantly more effective against more advanced inpainters (a 38% reduction in loss to target in the case of a weak Generative model, compared to a 78% reduction in EdgeConnect's case).
· In a robustness test, we show that markpainted samples sometimes transfer within the same inpainter trained on different datasets, and across different inpainters for markpainting with a target.
2. Broader Impact and Motivation
Malicious actors now manipulate public discourse with artificially generated or manipulated images, such as deep-

Markpainting: Adversarial Machine Learning meets Inpainting

fakes (Goodfellow et al., 2014; Zhang et al., 2020). For example, as shown in Figure 3, it takes no special knowledge to remove a participant from a photo of the 6 January 2021 raid on United States Congress; this is not noticeable without inspecting the image closely. This motivating example led us to study the capacity of inpainting tools to remove or replace objects in images.
Figure 3. Photo taken from the 6 January 2021 raid on United States Congress. Original on the left; the right photo has been modified using an inpainter to remove a participant. It is near impossible to tell which of the two images is the original one, without closer inspection.
Markpainting can provide protection against evidence tampering by preserving the integrity of published images. Consider an image of a crowd and an attacker who wants to forge evidence by removing a person from the crowd. The defender ­ e.g. the distributor of the image ­ does not know which person will be removed, but wants to stop the attacker. If they use our mask-agnostic markpainting technique with a solid color target image (such as pure red), then any attempt to remove a person from the image via inpainting will result in a red patch, clearly marking the image. In practice one would use more subtle techniques, which we discuss later.
3. Related Work
Humans have been restoring paintings for centuries. As ultraviolet light degrades both pigment and bindings in paint, exterior paintwork needs regular reworking; and although artworks kept indoors deteriorate more slowly, they still require upkeep from time to time. Images are touched up for other reasons; after Trotsky fell from favor in Russia, he was airbrushed out of numerous paintings. Digital inpainting is newer, going back to the 1990s when computer-graphics tools started to become both capable and widespread. Early approaches included patch search methods (Bertalmio et al., 2000; Osher et al., 2005) and texture synthesis (Efros & Leung, 1999; Barnes et al., 2009). Those approaches can only work with small missing regions because of the lack of semantic understanding; they are usually computationally expensive because of the time taken to find close matches to missing objects in a large corpus of data (Hays & Efros, 2007).

Recent advances in machine learning have enabled more semantically-aware inpainting. In 2016, Pathak et al. (2016) presented Context Encoders ­ CNNs trained to predict the contents of an arbitrary image based on its surroundings. They used L2 and an adversarial loss as in generative adversarial networks (Goodfellow et al., 2014). In 2017, Iizuka et al. (2017) built on this work by splitting the discriminator into three: a completion network, a local discriminator and a global discriminator. This architecture allowed inpainting of images of arbitrary size and helped maintain local consistency. Poisson blending allowed further refinement and sharpening of the image. In 2018, Wang et al. (2018) proposed a Generative Multi-column Convolutional Neural Network (GMCNN) with three sub-networks: a generator to inpaint the image, global and local discriminators and a pretrained VGG network for Implicit Diversified Markov Random Fields (ID-MRF) loss calculation. They use filters of different sizes to capture information at different granularity levels, which allowed more fine-grained inpainting. In 2019, Nazeri et al. (2019) used image structure knowledge and developed a two-stage model composed of an edge generator and an image generator.
Recently, there has been significant work in this field. Li et al. (2020b) proposed Recurrent Feature Reasoning (RFR), an inpainting method based on Knowledge Consistent Attention modules. RFR recurrently infers the hole boundaries, then uses them to solve more complex parts of the image. It is split into three parts: an area identification model, a feature reasoning module and a feature-merging operator designed to combine intermediate feature maps. The networks are trained to optimize VGG perceptual and style losses. Li et al. (2020a) proposed a deep generative inpainting network named DeepGin, using a customized ResNet block (He et al., 2016) to allow different receptive fields so that information from both local and distant regions can be gathered efficiently. Jie Yang (2020) built on Nazeri's work with a shared generator to generate both the completed image and its corresponding structures, placing the inpainting problem into a multi-task learning setup. Yu et al. (2020) investigated the feature normalization problem in the context of image inpainting, and proposed a spatially region-wise normalization for image inpainting. Zeng et al. (2020) proposed using a contextually-aware reconstruction loss to replace the contextual attention layers so a network could explicitly borrow from a known region as a reference to inpaint images.
On the adversarial machine learning side of things, in 2013 two separate teams led by Szegedy and Biggio discovered adversarial examples which, during inference, cause a model to output a surprisingly incorrect result (Szegedy et al., 2013; Biggio et al., 2013). In a white-box environment ­ where the adversary has direct access to the model ­ such examples

Markpainting: Adversarial Machine Learning meets Inpainting

Figure 4. Inpainting of the squirrel eye requires both local and global knowledge. With just local knowledge only the fur patterns could be produced. Image on the right features exaggerated normalized gradients of EdgeConnect (Nazeri et al., 2019) during the first algorithm iteration.

Algorithm 1 General markpainting algorithm
Input: image I, mask M, target T, perturbation step size , iterations t, targeted models 
for j = 0 to t do 0 for    do    + sign(ILmark(, I, T)) end for I  I - ( (1 - M))
end for Iadv  I Output: markpainted sample Iadv (combination of original input image I and the accumulated perturbations)

can be found using various gradient-based methods that typically aim to maximize the loss function under a series of constraints (Biggio et al., 2013; Szegedy et al., 2013; Goodfellow et al., 2015; Madry et al., 2019). In a blackbox setting, the adversary can transfer adversarial examples from another model (Papernot et al., 2017) or approximate gradients by observing output labels and confidence (Chen et al., 2017). In their various forms, adversarial examples can affect the confidentiality, integrity and availability of machine learning systems (Biggio & Roli, 2018; Papernot et al., 2016; Shumailov et al., 2021).
4. Methodology
4.1. Inpainting
Inpainting fills in information that is missing in an input image. During training, a part of the image is masked out and the inpainter aims to learn how to restore this area.
We define an input RGB image I  RH×W ×3 and a binary mask M  RH×W . The binary mask M has 0s for the areas to be inpainted and 1s otherwise. We then assume an inpainter f , that populates the region covered by 1 - M taking as input masked input ^I = I (1 - M), where
represents the Hadamard product. The function f was trained to minimize dissimilarity Ltrain between ^I and I. Training here may involve images of different sizes and irregular masks depending on the system.
4.2. Markpainting
We present two different flavors of markpainting: targeted and mask-agnostic. Targeted markpainting forces the reconstruction to resemble the target image, whilst mask-agnostic markpainting aims to generalize the technique to work with an arbitrary mask. These are presented in Algorithm 1 and Algorithm 2 respectively. Algorithm 1 is visualized in Figure 1a. The formal setup is similar to adversarial ex-

ample generation (Szegedy et al., 2013; Madry et al., 2019), where the perturbation  is accumulated iteratively from scaled gradients ( sign(ILmark(, I, T))).
We define Lmark(, x, x ) = Lnetwork(, x) + l2(x - x ), where Lnetwork is the VGG perceptual loss (Johnson et al., 2016) and l2 is MSE loss. We use the VGG perceptual loss to measure human visual similarity of markpainting, which is usually missed by pure L2 loss. L2 penalizes large deviations from the target, whilst VGG promotes humanunderstandable granularity. We set  = 4, based on experimentation. The effect of different  values on the markpainted result can be found in Section D of our Appendix.
Notice that the perturbation propagated to the natural input is ( (1 - M)), because the regions to be infilled are masked out and do not receive gradients.
The technique aims to find a perturbation  with a given perturbation budget such that the used dissimilarity function Lmark parameterized by  is minimized.

minimize


Lmark(, f ((I + )

(1 - M)), ^I)

subject to ||||p <

||||p is the lp norm of  and in this paper we use p = .
We represent the original input image using I, the original image with our carefully crafted perturbation as Ipert, the naturally inpainted image using Ibenign, and the inpainted results of Ipert as Imark . We denote the target image using T, and the mask is represented using M.
We find that we can apply our technique to a collection of models  simultaneously using a single input image I as detailed in Algorithm 1. An example result of application of markpainting to multiple models simultaneously is presented in Figure 1b and in Section A of our Appendix, where the same markpainted sample produces a visually-recognizable face, similar to the target, after being

Markpainting: Adversarial Machine Learning meets Inpainting

Algorithm 2 EoT markpainting algorithm
Input: image I, target T, number of masks n, mask size range [mmin, mmax], perturbation step size , iterations t, targeted models  Initialize set M^ to contain n random rectangular masks of size s  [mmin, mmax] Initialize M   for j = 0 to t do
M  M^ i for a random 0  i < n 0 for    do
   + U(0, 1)sign(ILmark(, I, T)) end for I  I - ( M) end for Iadv  I Output: markpainted sample Iadv (combination of original input image I and the accumulated perturbations)
run through six different inpainters.
4.3. Mask-agnostic Markpainting
Although Algorithm 1 works well against a known mask M, there are other cases where we do not know which parts of an image might be tampered with. We adapt our technique to generate an image that will cause a system to markpaint regardless of the mask used. This problem is related to the construction of adversarial examples that work in physical environments under different conditions of lighting and viewing angles. We therefore extend an approach first introduced by Athalye et al. (2018) called Expectation over Transformation (EoT).
This extension is presented in Algorithm 2. For this technique, a set of random masks is produced with a given size range [mmin, mmax]. We iteratively sample a single mask from the set and apply an algorithm similar to Algorithm 1. We find that further adding stochasticity helps to transfer to unseen masks: we weight the gradient step with a random uniformly-distributed vector U(0, 1).

fur. Global information (shown in orange) on the other hand, should tell the inpainter that the picture features a squirrel in a particular pose and that an eye should be located there. As illustrated in the gradient visualization in Figure 4, gradients focus on both the area around the eye and the rest of the image. This dependency on global information makes inpainting both complex and prone to manipulation. The markpainter does not need to concentrate their perturbation around the patch area but can scatter it all over the image.
While at first glance markpainting seems similar to older techniques, such as ones proposed by Levin et al. (2004), there are fundamental differences in the two approaches. Inpainting requires a semantic understanding of the scenery and heavily depends on global information, as shown in Figure 4. Furthermore, markpainting can produce artifacts that are semantically meaningless for the model and not present in its training distribution.
5. Evaluation
In this section, we evaluate the performance of targeted markpainting in Section 5.2, and the effect of different masks and target images in Section 5.3. Section 5.4 focuses on the transferability of the generated samples, while Section 5.5 discusses mask-agnostic markpainting.
5.1. Datasets and Models
In Table 1 we list the inpainter systems used in the evaluation. Our evaluation covers systems that provide different levels of granularity of the inpainted regions and different levels of representation. We intentionally chose a variety of inpainters with differing levels of performance and from different years. We use pretrained models provided by the authors of the respective systems. Table 1 also indicates the datasets with which these inpainters are pretrained. A maximum perturbation budget of was used with a step size of = 50 unless specified otherwise. We justify the parameter choices in Section D of the Appendix. We clip the markpainted image at each iteration to make sure that the total perturbation budget does not exceed .

4.4. Why does it work?
Inpainting is a complex task, with neural networks trained to manipulate images of arbitrary size and with arbitrary patches. Furthermore, modern inpainters can fill irregular holes. As they are trying to be semantically aware and display both local and global consistency, they need to understand the global scenery well. That in turn makes them dependent not only on the area around the patch, but on the whole image. Imagine trying to fill in a hole around the squirrel eye depicted in Figure 4. Here, local information (shown in pink) would suggest that it has to be filled with

Table 1. Inpainters used in the evaluation

System

Dataset

Generative

(Yu et al., 2018) ImageNet (Deng et al., 2009)

GMCNN

(Wang et al., 2018) CelebA-HQ (Liu et al., 2015)

EdgeConnect (Nazeri et al., 2019) Paris StreetView (Doersch et al., 2012), CelebA (Liu et al., 2015), Places2 (Zhou et al., 2017)

RFR

(Li et al., 2020b) Paris StreetView, CelebA

RN

(Yu et al., 2020) Places2

CRFILL

(Zeng et al., 2020) Places2, Salient Object Segmentation (Xiong et al., 2019)

Markpainting: Adversarial Machine Learning meets Inpainting

Figure 5. Inpainting with an increasing perturbation budget. Top row is the perturbed images generated using markpainting, and second row is the inpainted results of these perturbed images. We target the RN inpainter with 500 iterations and a step size of /100. Note that this example is really hard, because we are filling a black and white image with color. Details are discussed in Section 5.2.

The systems are evaluated on places subset16, a series of 16 randomly-selected images from the Places2 dataset (Zhou et al., 2017)1 ­ using fixed random masks of three different sizes respectively covering 5%, 10% and 20% of the image. We use three solid-color targets for evaluation: pure red, green, and blue. Further details are provided in the Appendix.

5.2. Targeted Markpainting
Figure 5 illustrates the visual effect of applying markpainting to the inpainter based on Region Normalization (RN) (Yu et al., 2020) with an increasing perturbation budget. The top row shows the markpainted images we produced and the second row shows the final inpainted results. The inpainting task here is complex, as it requires constructing a colored patch from a black-and-white image. Even with a small budget = 0.05 that is barely perceptible, RN markpaints the region with a lot of detail from the target image: we can see the structure and edges of La Gioconda. At larger values, facial details start to appear.
Table 2 presents an evaluation on the places subset16 dataset, averaging results from all possible input-masktarget combinations. It is seen that larger perturbation budgets cause the dissimilarity metric Lmark and l2 norm distance to reduce, and PSNR and SSIM to increase, between the markpainted image Imarkand the target T. This is expected since the increased budget results in better reconstruction of the target, with the effect that the samples lose resemblance to the benign reconstruction Ibenign. Larger budgets help fine-grained artifacts from the target to appear in Imark, whilst sacrificing imperceptibility. We empirically see that = 0.05 is usually invisible, while allowing for a
1We abstain from evaluating on CelebA dataset due to ethical concerns over the dataset labels.

Figure 6. The impact that perturbation sizes have on Lmark between the markpainted patch and the target (lower is better). We run markpainting over 100 iterations. The x-axis shows different perturbation budgets and the vertical axis is the loss between Imark and T. Results are averaged across the places subset16 images, with ± error bars. Details are discussed in Section 5.3.
good level of reconstruction detail.
The effectiveness of our technique when targeting multiple models simultaneously is discussed in Section A of our Appendix.
5.3. Impact of Mask-target Choice
To illustrate the effectiveness of our markpainting technique, we show how it performs under varying mask sizes and target images in Figure 6. We find that although mask size has an influence on technique performance, the color of the target image has a greater impact. We find that green color areas are harder to markpaint for both models, whereas blues are easiest. We suspect this is due to the source images having little green, and the training datasets perhaps also lacking this color.

Markpainting: Adversarial Machine Learning meets Inpainting

Distance to:
GENERATIVE RFR 0.0 RN CRFILL GMCNN EDGE CONNECT
GENERATIVE RFR 0.05 RN CRFILL GMCNN EDGE CONNECT
GENERATIVE RFR 0.3 RN CRFILL GMCNN EDGE CONNECT

Loss
0.467 0.279 0.300 0.470 0.485 0.290
0.441 0.332 0.386 0.491 0.605 0.351
0.489 0.412 0.456 0.698 0.681 0.428

Original l2 PSNR

0.111 0.027 0.025 0.109 0.122 0.025

12.491 19.156 17.891 13.342 10.891 18.198

0.108 0.040 0.081 0.244 0.818 0.057

12.121 15.533 11.697 7.291
3.047 13.032

0.184 0.095 0.154 0.896 1.112 0.112

8.676 10.965 8.620 0.759
0.968 10.336

SSIM
0.250 0.387 0.420 0.319 0.210 0.390
0.225 0.325 0.266 0.128 -0.018 0.317
0.132 0.288 0.216 0.035 -0.067 0.285

Adversarial Target

Loss

l2 PSNR SSIM

0.755 0.433 0.473 0.796 0.721 0.422

1.186 0.292 0.292 1.205 1.136 0.299

-0.622 5.464 5.438 -0.683 -0.432 5.383

0.036 0.090 0.104 0.044 0.047 0.104

0.623 0.310 0.203 0.509 0.579 0.216

1.093 0.232 0.153 0.782 1.349 0.206

-0.281 6.547 8.763 1.481 -0.453 7.120

0.050 0.139 0.239 0.135 0.040 0.214

0.466 0.143 0.067 0.159 0.533 0.089

0.768 0.110 0.049 0.060 1.457 0.091

1.284 10.191 14.125 14.076 0.175 11.166

0.140 0.247 0.322 0.695 0.081 0.268

Loss
0.179 0.001 0.001 0.180 0.181 0.001
0.289 0.161 0.271 0.406 0.432 0.190
0.379 0.278 0.365 0.646 0.511 0.289

Benign l2 PSNR

0.000 0.000 0.000 0.000 0.000 0.000

134.254 144.880
inf inf inf 135.133

0.028 0.019 0.062 0.186 0.698 0.042

17.483 18.374 13.094
8.676 4.491 14.655

0.135 0.079 0.140 0.884 0.993 0.103

9.746 11.604
8.970 0.789 1.851 10.865

SSIM
1.000 1.000 1.000 1.000 1.000 1.000
0.476 0.630 0.435 0.232 0.038 0.546
0.167 0.381 0.315 0.047 -0.066 0.398

Table 2. Impact of markpainting on different inpainter models. This table reports the loss (Lmark from Section 4.2), L2 norms, peak signal to noise ratio (PSNR) and structural index similarity (SSIM) for assessing the inpainted image quality. Markpainting is applied to each individual inpainter and evaluated on the same inpainter with different perturbations budgets; this table is a compact version of Table 3 in our Appendix, where more epsilon values are available. In this table we highlight cases where the loss to the target image is better than to the original reconstruction. For these three meta-columns, we report how the markpainted patch (Imark M) compares to different images in different metrics. `Original' refers to the original image patch (I M), `Adversarial target' is the target image used (T M), and `Benign' is the image that the model would have produced without any adversarial perturbation (Ibenign M). Increasing the perturbation budget increases the similarity between the markpainted patch and the target but decreases similarity to the original image and benign inpainted patch. Details are discussed in Section 5.2.

Figure 7. Showing technique transferability between different models using Lmark between the markpainted patch and the target. The perturbations come from targeting the model listed in the title but the errors are shown for the 6 color-coded models. Results are averaged across all possible input-mask-target combinations, with ± error bars to highlight the standard deviation in obtained results. Details are described in Section 5.4.

(a) EdgeConnect, trained on Places2.

(b) RFR, trained on CelebA.

Figure 8. Showing technique transferability between the same model, trained on different datasets, using Lmark between the markpainted patch and the target. Results are averaged across all possible input-mask-target combinations. Notice the large ± error bars, indicating high variability in transferability depending on the input combination.

5.4. Transferability of Targeted Markpainted Examples
Here we investigate the transferability of markpainted images in a blind black-box scenario, using previouslyconstructed markpainted samples to fool other models with-

out knowledge of their internals. We investigate transferability across both model architectures and datasets. We report mean model performance with an increasing perturbation budget. Although an increased budget helps with transferability, the improvement is marginal in most cases. Note that variances of the measurements here are large; they

Markpainting: Adversarial Machine Learning meets Inpainting

(a) EoT technique on RN.

(b) EoT technique on EdgeConnect.

Figure 9. Effect of on the effectiveness of the proposed maskagnostic markpainting technique. Results are averaged across the places subset16 images, with ± error bars. It is evident that the technique's effectiveness is very much architecture dependent, with a high sensitivity to the input.

reflect differences in input images and how different color targets transfer across models.
In Figure 7, we turn to the question of whether inpainter models show greater transferability if pretrained with the same dataset. RN and CRFILL, both trained on Places2, demonstrate a correlated decreasing pattern on the right plot of Figure 7, showing that inpainters trained on the same dataset might suffer from transferred markpainting samples.
In Figure 8 we demonstrate the transferability of markpainted examples within the same model architecture trained on different datasets. For this experiment we use EdgeConnect ­ trained on CelebA, Paris StreetView, and Places2 ­ and RFR ­ trained on CelebA and Paris StreetView. Effectiveness of markpainted examples degrades to varying degrees when used by a model trained on a different dataset. However, the graphs demonstrate that markpainted examples are transferable within the same model architecture.
In general, we found that transferability exists across different inpainter models and datasets. This is broadly equivalent to the robustness of a watermark protected by our technique. It shows greater transferability when inpainters are trained on the same dataset or share the model architecture.
5.5. Mask-agnostic Markpainting
Figure 9 shows the effectiveness of our EoT method for mask-agnostic markpainting. The number of iterations was taken to be 1500 with a step size of 30 , with mmin = 0.01 and mmax = 0.1. The evaluation masks are taken to be fixed random masks covering 2.5%, 5% and 10% of the image. The effectiveness was found to be architecture dependent. Moreover, certain images were more susceptible to markpainting than others. Investigating the exact causes of this architecture and image dependence is left to future work.

6. Discussion
6.1. Countering Markpainting
We have shown that modern inpainters can be manipulated to inpaint arbitrary target images. This naturally leads to the question of how one can counter markpainting. As markpainting aims to be explicitly imperceptible, it usually does not disrupt lower parts of the frequency spectra responsible for sharp edges, instead concentrating on the higher-frequency components. We propose a mechanism which accounts for this.
We find that transformation-based manipulations work relatively well in countering markpainting. Figure 10 shows dissimilarity Lmark between the markpainted patch and the target/benign images. We test five different transformations: JPEG compression, low-pass filtering, Gaussian blurring, contrast adjustments, and brightness adjustments. Each manipulation significantly reduced markpainting performance, but had different impact on the inpainting performance in the benign cases. Simple low-pass filtering reduces similarity of the reconstruction to the target image, but also causes significant deviation from the benign reconstruction. This highlights the trade-off between countering markpainting and preserving the benign inpainted patch. Although some transformations decrease the performance of markpainting, they change the original image significantly as well. Thus, manual human involvement appears to be required, which is highly likely to limit the scalability of abuse based on inpainting.
6.2. Interpretability of Markpainting
Unlike adversarial examples for classification tasks, markpainting can be interpreted. Indeed, we find that we could often visually tell what an increased perturbation budget was changing in our perception of the inpainter model. Evaluation suggests that although markpainting can be made transferable, it usually is not. We find that it is, perhaps intuitively, harder to markpaint colors or shapes that are not present in the original image. Complex shapes, and contours that do not naturally extend from the mask's boundaries, also prove to be a challenge. In contrast, models that have been trained on the CelebA dataset are easier to fool into markpainting faces, as demonstrated strikingly in a visualization provided in Section A of our Appendix.
7. Conclusion
We introduce the idea of markpainting: fooling an inpainting system into generating a patch similar to an arbitrary target. Moreover, we demonstrate through mask-agnostic markpainting that the technique does not need to be restricted to a particular mask to be effective. We also show the existence of some degree of transferability of these ad-

Markpainting: Adversarial Machine Learning meets Inpainting

(a) JPEG compression, low pass filtering and Gaussian blur.

(b) JPEG compression, brightness and contrast.

Figure 10. Effect of different transformation-based defenses. Results are from performing these transformations on the watermark example presented in Figure 2.

versarial examples both within a single model and between different model architectures.
Markpainting has wide implications. Image owners can now protect their digital assets with less removable watermarks, or treat them so that any later manipulation such as object removal becomes easier to detect.
Acknowledgments
We thank the reviewers for their insightful feedback. We want to explicitly thank Mohammad Yaghini, Stephan Rabanser, Gabriel Deza, Natalie Dullerud, Ali Shahin Shamsabadi and Nicholas Boucher for their help and comments. This work was supported by CIFAR (through a Canada CIFAR AI Chair), by EPSRC, by Apple, by Bosch Forschungsstiftung im Stifterverband, by NSERC, and by a gift from Microsoft. We also thank the Vector Institute's sponsors.
References
Antic, J. Deoldify. https://github.com/jantic/ DeOldify/, 2018.
Athalye, A., Engstrom, L., Ilyas, A., and Kwok, K. Synthesizing robust adversarial examples, 2018.
Barnes, C., Shechtman, E., Finkelstein, A., and Goldman, D. B. Patchmatch: A randomized correspondence algorithm for structural image editing. ACM Trans. Graph., 28(3):24, 2009.
Bertalmio, M., Sapiro, G., Caselles, V., and Ballester, C. Image inpainting. In Proceedings of the 27th Annual

Conference on Computer Graphics and Interactive Techniques, SIGGRAPH '00, pp. 417­424, USA, 2000. ACM Press/Addison-Wesley Publishing Co. ISBN 1581132085. doi: 10.1145/344779.344972. URL https://doi. org/10.1145/344779.344972.
Biggio, B. and Roli, F. Wild patterns: Ten years after the rise of adversarial machine learning. Pattern Recognition, 84:317­331, 2018.
Biggio, B., Corona, I., Maiorca, D., Nelson, B., S rndic´, N., Laskov, P., Giacinto, G., and Roli, F. Evasion attacks against machine learning at test time. In Joint European conference on machine learning and knowledge discovery in databases, pp. 387­402. Springer, 2013.
Chen, P.-Y., Zhang, H., Sharma, Y., Yi, J., and Hsieh, C.J. Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pp. 15­26, 2017.
Cox, I., Miller, M., Bloom, J., Fridrich, J., and Kalker, T. Digital Watermarking and Steganography. Morgan Kaufmann, 2007.
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248­255. Ieee, 2009.
Doersch, C., Singh, S., Gupta, A., Sivic, J., and Efros, A. A. What makes paris look like paris? ACM Transactions on Graphics (SIGGRAPH), 31(4):101:1­101:9, 2012.
Efros, A. A. and Leung, T. K. Texture synthesis by nonparametric sampling. In Proceedings of the seventh IEEE

Markpainting: Adversarial Machine Learning meets Inpainting

international conference on computer vision, volume 2, pp. 1033­1038. IEEE, 1999.
Goodfellow, I. Nips 2016 tutorial: Generative adversarial networks. arXiv preprint arXiv:1701.00160, 2016.
Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative adversarial networks, 2014.
Goodfellow, I. J., Shlens, J., and Szegedy, C. Explaining and harnessing adversarial examples, 2015.

Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. Towards deep learning models resistant to adversarial attacks, 2019.
Nazeri, K., Ng, E., Joseph, T., Qureshi, F. Z., and Ebrahimi, M. Edgeconnect: Generative image inpainting with adversarial edge learning, 2019.
Osher, S., Burger, M., Goldfarb, D., Xu, J., and Yin, W. An iterative regularization method for total variation-based image restoration. Multiscale Modeling & Simulation, 4 (2):460­489, 2005.

Hays, J. and Efros, A. A. Scene completion using millions of photographs. ACM Trans. Graph., 26(3): 4­es, July 2007. ISSN 0730-0301. doi: 10.1145/ 1276377.1276382. URL https://doi.org/10. 1145/1276377.1276382.
He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016.
Iizuka, S., Simo-Serra, E., and Ishikawa, H. Globally and locally consistent image completion. ACM Trans. Graph., 36(4), July 2017. ISSN 0730-0301. doi: 10. 1145/3072959.3073659. URL https://doi.org/ 10.1145/3072959.3073659.
Jie Yang, Zhiquan Qi, Y. S. Learning to incorporate structure knowledge for image inpainting. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 12605­12612, 2020.
Johnson, J., Alahi, A., and Fei-Fei, L. Perceptual losses for real-time style transfer and super-resolution, 2016.
Korshunova, I., Shi, W., Dambre, J., and Theis, L. Fast face-swap using convolutional neural networks, 2017.
Levin, A., Lischinski, D., and Weiss, Y. Colorization using optimization. ACM Transactions on Graphics, 23, 06 2004. doi: 10.1145/1015706.1015780.
Li, C.-T., Siu, W.-C., Liu, Z.-S., Wang, L.-W., and Lun, D. P.-K. Deepgin: Deep generative inpainting network for extreme image inpainting, 2020a.
Li, J., Wang, N., Zhang, L., Du, B., and Tao, D. Recurrent feature reasoning for image inpainting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020b.

Papernot, N., McDaniel, P., Sinha, A., and Wellman, M. Towards the science of security and privacy in machine learning. arXiv preprint arXiv:1611.03814, 2016.
Papernot, N., McDaniel, P., Goodfellow, I., Jha, S., Celik, Z. B., and Swami, A. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia conference on computer and communications security, pp. 506­519, 2017.
Pathak, D., Kra¨henbu¨hl, P., Donahue, J., Darrell, T., and Efros, A. Context encoders: Feature learning by inpainting. In Computer Vision and Pattern Recognition (CVPR), 2016.
Ramesh, A., Pavlov, M., Goh, G., and Gray, S. Dall·e: Creating images from text, Jan 2021. URL https:// openai.com/blog/dall-e/.
Shumailov, I., Zhao, Y., Bates, D., Papernot, N., Mullins, R., and Anderson, R. Sponge examples: Energy-latency attacks on neural networks. In 6th IEEE European Symposium on Security and Privacy (EuroS&P), 2021.
Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., and Fergus, R. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Vincent, J. Photoshop's ai neural filters can tweak age and expression with a few clicks, 2020. URL www.theverge.com/2020/10/20/21517616/ adobe-photoshop-ai-neural-filters-beta\ -launch-machine-learning.
Wang, Y., Tao, X., Qi, X., Shen, X., and Jia, J. Image inpainting via generative multi-column convolutional neural networks. In Advances in Neural Information Processing Systems, pp. 331­340, 2018.
Xiong, W., Yu, J., Lin, Z., Yang, J., Lu, X., Barnes, C., and Luo, J. Foreground-aware image inpainting, 2019.

Liu, Z., Luo, P., Wang, X., and Tang, X. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), December 2015.

Yu, J., Lin, Z., Yang, J., Shen, X., Lu, X., and Huang, T. S. Generative image inpainting with contextual attention. arXiv preprint arXiv:1801.07892, 2018.

Markpainting: Adversarial Machine Learning meets Inpainting
Yu, T., Guo, Z., Jin, X., Wu, S., Chen, Z., Li, W., Zhang, Z., and Liu, S. Region normalization for image inpainting. In AAAI, pp. 12733­12740, 2020.
Zeng, Y., Lin, Z., Lu, H., and Patel, V. M. Image inpainting with contextual reconstruction loss, 2020.
Zhang, B., Zhou, J. P., Shumailov, I., and Papernot, N. Not my deepfake: Towards plausible deniability for machinegenerated media, 2020.
Zhou, B., Lapedriza, A., Khosla, A., Oliva, A., and Torralba, A. Places: A 10 million image database for scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017.

A. Applying Markpainting to a Collection of Models
As illustrated in the markpainting algorithm, we find that we are able to markpaint a collection of models  simultaneously using a single input image I as detailed in Algorithm 1 in the paper. This means a single adversarial image serves as an input to all considered models. An example application of this in a white-box setting is shown in Figure 11 , where the same sample is perturbed to be producing a different face after inpainting by a 6 different inpainters. Figure 12 shows the benign samples from the inpainters, these are the infilling effect if there are no markpainting generated noises. The inpainter will naively fill the masked region without facial details, but the generated markpainted example can influence this filling to provide facial details of Obama.
Table 3 shows the details of how this technique works with different inpainters. In Table 3, the adversarial samples are generated using all models and evaluated on each model individually. The table reports the loss, L2 norms, peak signal to noise ratio (PSNR) and structural similarity index measure (SSIM) for accessing the inpainted image quality.

Figure 11. Markpainting Vladimir Putin's face with former President Obama's. The mix of colors in the image makes it significantly easier to attack. Attack performed over 500 iterations, with
= 0.1. The original photo with Putin and Trump is taken from The Guardian. Photograph of Obama taken from Acclaim Images.

B. Targeted Application of Inpainting
Table 4 shows the results of the markpainting technique on different inpainter models and it is an extended version of Table 2 in the main paper. The markpainting technique is launched at each individual inpainter and evaluated on that inpainter with different perturbation budgets.
The table reports the loss, L2 norms, peak signal to noise ratio (PSNR) and structural index similarity (SSIM) for accessing the inpainted image quality.
C. Transferability of Markpainting
In Table 5, we assess the transferability of markpainting.

Figure 12. Benign infilling of the example image in Figure 11.
D. Parameter Choices
In the paper, we provided a visualization of having an increasing budgets in Figure 4. This term controls a loss trade-off between the network loss and the L2 loss. As we can see, in Figure 13, when the  value increases, the markpainted image gets closer to the target. We also show the original benign inpainting results in Figure 12 as a baseline for comparisons. It is worth to mention that the baseline simply fills the face with surrounding colors.

Markpainting: Adversarial Machine Learning meets Inpainting

Figure 13. Effect of  on markpainting.   {0, 1, 2, 4, 8}.

We also further study the effect of of the markpainting technique. In the evaluation, the effect of different epsilons are shown for the RN inpainter, we further illustrate the effect of epsilons on other inpainters (RFR and CRFILL), and they are Figure 17 and Figure 18 respectively.
E. Evaluation Details
The places subset16 dataset that we used to evaluate our proposed method on ­ a series of 16 randomly-selected images from the Places2 dataset (Zhou et al., 2017) ­ is visualized in Figure 19.
We understand that it is of interest to readers to be able to visualize the numeric loss values we quote in our results. In Figure 14, we present visual examples of how numeric loss relates to the markpainted results for complex targets; and in Figure 15 we do the same for a solid-color target.

(a) Applied to the GMCNN inpainter model.

(b) Applied to the RN inpainter model.
Figure 14. Correspondence between numeric loss to target image and the obtained markpainted results for a complex target.

Markpainting: Adversarial Machine Learning meets Inpainting

(a) Applied to the RFR inpainter model.

(b) Applied to the EDGE CONNECT inpainter model.

Figure 15. Correspondence between numeric loss to target image and the obtained markpainted results for a solid-color target.
F. Watermark Removal
We show more results on the watermark removal with = 0.15. The objective is to build an image that is resistant to watermark removals using markpainting. Figure 16 shows that markpainted images (top two rows) are in general more robust to different inpainters trying to fill the watermark.

Figure 16. Example usage of the methods proposed in this paper: watermark encoding into an image using perceptually indistinguishable noise ( = 0.15). In this example, GENERATIVE is markpainted, but the intentionally disruptive result is transferred to all models. Image from (Yu et al., 2018).

Markpainting: Adversarial Machine Learning meets Inpainting
Figure 17. Inpainting with increasing perturbation epsilon budget. Top row is the adversarial images generated using markpainting, and second row is the inpainted results of these adversarial images. We target the CRFILL inpainter with 500 iterations and a step size of /100. Note that this example is really hard, because we are filling a black and white image with color.
Figure 18. Inpainting with increasing perturbation epsilon budget. Top row is the adversarial images generated using markpainting, and second row is the inpainted results of these adversarial images. We target the RFR inpainter with 500 iterations and a step size of /100. Note that this example is really hard, because we are filling a black and white image with color.

Markpainting: Adversarial Machine Learning meets Inpainting Figure 19. The places subset16 dataset used for evaluation. A subset of Places2 (Zhou et al., 2017).

Markpainting: Adversarial Machine Learning meets Inpainting

Distance to:
GENERATIVE RFR 0.0 RN CRFILL GMCNN EDGE CONNECT
GENERATIVE RFR 0.03 RN CRFILL GMCNN EDGE CONNECT
GENERATIVE RFR 0.05 RN CRFILL GMCNN EDGE CONNECT
GENERATIVE RFR 0.1 RN CRFILL GMCNN EDGE CONNECT
GENERATIVE RFR 0.2 RN CRFILL GMCNN EDGE CONNECT
GENERATIVE RFR 0.3 RN CRFILL GMCNN EDGE CONNECT

Loss
0.467 (±0.282) 0.279 (±0.220) 0.300 (±0.227) 0.470 (±0.295) 0.485 (±0.267) 0.290 (±0.226)
0.435 (±0.249) 0.298 (±0.209) 0.328 (±0.207) 0.446 (±0.251) 0.549 (±0.286) 0.305 (±0.209)
0.436 (±0.248) 0.323 (±0.218) 0.357 (±0.218) 0.483 (±0.247) 0.579 (±0.315) 0.320 (±0.215)
0.445 (±0.249) 0.365 (±0.231) 0.385 (±0.231) 0.546 (±0.268) 0.606 (±0.344) 0.350 (±0.228)
0.468 (±0.259) 0.411 (±0.243) 0.407 (±0.244) 0.580 (±0.295) 0.615 (±0.373) 0.388 (±0.243)
0.494 (±0.269) 0.439 (±0.249) 0.418 (±0.247) 0.602 (±0.309) 0.641 (±0.367) 0.411 (±0.249)

Original l2

PSNR

0.111 (±0.113) 0.027 (±0.034) 0.025 (±0.023) 0.109 (±0.120) 0.122 (±0.108) 0.025 (±0.022)

12.491 (±6.257) 19.156 (±7.224) 17.891 (±4.161) 13.342 (±7.767) 10.891 (±4.560) 18.198 (±5.129)

0.104 (±0.110) 0.031 (±0.038) 0.042 (±0.027) 0.124 (±0.149) 0.554 (±0.704) 0.036 (±0.020)

12.627 (±5.859) 17.069 (±4.168) 14.641 (±2.940) 11.034 (±4.111) 5.127 (±4.498) 15.090 (±2.432)

0.103 (±0.109) 0.037 (±0.038) 0.052 (±0.033) 0.195 (±0.186) 0.687 (±0.764) 0.039 (±0.022)

12.499 (±5.582) 15.711 (±3.484) 13.757 (±2.969) 8.130 (±2.850) 4.007 (±4.579) 14.666 (±2.352)

0.107 (±0.110) 0.057 (±0.052) 0.067 (±0.041) 0.384 (±0.222) 0.750 (±0.799) 0.049 (±0.027)

11.825 (±4.632) 13.515 (±2.895) 12.476 (±2.645) 4.727 (±2.177) 3.553 (±4.553) 13.838 (±2.581)

0.134 (±0.123) 0.087 (±0.053) 0.092 (±0.054) 0.495 (±0.225) 0.768 (±0.803) 0.071 (±0.039)

10.145 (±3.532) 11.230 (±2.331) 11.054 (±2.421) 3.458 (±1.875) 3.436 (±4.543) 12.092 (±2.348)

0.174 (±0.131) 0.114 (±0.057) 0.106 (±0.058) 0.566 (±0.236) 0.811 (±0.793) 0.094 (±0.052)

8.528 (±2.806) 9.917 (±2.066) 10.338 (±2.263) 2.825 (±1.764) 2.883 (±4.171) 10.882 (±2.293)

SSIM
0.250 (±0.222) 0.387 (±0.261) 0.420 (±0.220) 0.319 (±0.257) 0.210 (±0.208) 0.390 (±0.245)
0.247 (±0.212) 0.346 (±0.242) 0.343 (±0.192) 0.232 (±0.193) 0.038 (±0.236) 0.343 (±0.212)
0.237 (±0.209) 0.323 (±0.228) 0.323 (±0.195) 0.139 (±0.137) 0.020 (±0.225) 0.335 (±0.209)
0.210 (±0.198) 0.299 (±0.200) 0.311 (±0.191) 0.064 (±0.111) 0.009 (±0.231) 0.320 (±0.205)
0.158 (±0.171) 0.267 (±0.167) 0.291 (±0.187) 0.045 (±0.105) 0.013 (±0.227) 0.291 (±0.184)
0.112 (±0.143) 0.243 (±0.155) 0.280 (±0.183) 0.038 (±0.106) -0.023 (±0.218) 0.259 (±0.165)

Loss
0.755 (±0.334) 0.433 (±0.191) 0.473 (±0.204) 0.796 (±0.355) 0.721 (±0.289) 0.422 (±0.189)
0.663 (±0.268) 0.357 (±0.147) 0.299 (±0.142) 0.642 (±0.279) 0.611 (±0.275) 0.305 (±0.135)
0.639 (±0.254) 0.323 (±0.133) 0.223 (±0.110) 0.526 (±0.258) 0.601 (±0.281) 0.269 (±0.119)
0.595 (±0.235) 0.259 (±0.112) 0.145 (±0.069) 0.319 (±0.172) 0.584 (±0.303) 0.212 (±0.093)
0.513 (±0.209) 0.187 (±0.084) 0.101 (±0.042) 0.237 (±0.132) 0.547 (±0.319) 0.163 (±0.068)
0.445 (±0.197) 0.154 (±0.071) 0.087 (±0.039) 0.211 (±0.119) 0.539 (±0.334) 0.145 (±0.060)

Adversarial Target

l2

PSNR

1.186 (±0.298) 0.292 (±0.072) 0.292 (±0.060) 1.205 (±0.309) 1.136 (±0.283) 0.299 (±0.085)

-0.622 (±0.989) 5.464 (±0.992) 5.438 (±0.851) -0.683 (±1.021) -0.432 (±1.014) 5.383 (±1.088)

1.147 (±0.282) 0.259 (±0.069) 0.221 (±0.058) 1.065 (±0.289) 1.200 (±0.656) 0.252 (±0.080)

-0.481 (±0.966) 6.003 (±1.086) 6.724 (±1.233) -0.134 (±1.081) -0.301 (±1.938) 6.171 (±1.201)

1.119 (±0.273) 0.230 (±0.067) 0.177 (±0.056) 0.826 (±0.298) 1.281 (±0.769) 0.223 (±0.073)

-0.375 (±0.961) 6.542 (±1.202) 7.734 (±1.431) 1.118 (±1.639) -0.418 (±2.293) 6.694 (±1.218)

1.036 (±0.256) 0.176 (±0.065) 0.128 (±0.044) 0.403 (±0.189) 1.292 (±0.841) 0.181 (±0.065)

-0.037 (±0.976) 7.822 (±1.560) 9.169 (±1.496) 4.404 (±2.039) -0.272 (±2.640) 7.659 (±1.361)

0.860 (±0.223) 0.117 (±0.052) 0.091 (±0.031) 0.241 (±0.104) 1.239 (±0.912) 0.136 (±0.056)

0.783 (±1.036) 9.788 (±2.117) 10.679 (±1.520) 6.624 (±2.065) 0.178 (±3.046) 8.964 (±1.622)

0.709 (±0.205) 0.089 (±0.045) 0.076 (±0.028) 0.183 (±0.091) 1.226 (±0.958) 0.112 (±0.048)

1.654 (±1.164) 11.242 (±2.689) 11.483 (±1.638)
7.918 (±2.238) 0.464 (±3.428) 9.928 (±1.945)

SSIM
0.036 (±0.100) 0.090 (±0.055) 0.104 (±0.051) 0.044 (±0.115) 0.047 (±0.110) 0.104 (±0.056)
0.040 (±0.098) 0.105 (±0.058) 0.171 (±0.066) 0.062 (±0.105) 0.046 (±0.188) 0.157 (±0.058)
0.044 (±0.097) 0.125 (±0.060) 0.203 (±0.060) 0.121 (±0.132) 0.040 (±0.224) 0.175 (±0.057)
0.063 (±0.098) 0.178 (±0.065) 0.240 (±0.048) 0.333 (±0.183) 0.060 (±0.257) 0.201 (±0.051)
0.104 (±0.104) 0.229 (±0.060) 0.264 (±0.045) 0.566 (±0.162) 0.093 (±0.296) 0.212 (±0.047)
0.146 (±0.121) 0.257 (±0.064) 0.269 (±0.027) 0.655 (±0.118) 0.116 (±0.327) 0.212 (±0.056)

Loss
0.179 (±0.062) 0.001 (±0.001) 0.001 (±0.001) 0.180 (±0.059) 0.181 (±0.060) 0.001 (±0.001)
0.254 (±0.109) 0.102 (±0.076) 0.183 (±0.089) 0.315 (±0.126) 0.358 (±0.187) 0.105 (±0.067)
0.277 (±0.125) 0.147 (±0.098) 0.239 (±0.118) 0.392 (±0.158) 0.399 (±0.205) 0.142 (±0.088)
0.309 (±0.144) 0.212 (±0.119) 0.286 (±0.144) 0.486 (±0.218) 0.435 (±0.253) 0.193 (±0.115)
0.352 (±0.170) 0.272 (±0.139) 0.314 (±0.159) 0.526 (±0.252) 0.446 (±0.275) 0.241 (±0.137)
0.387 (±0.190) 0.304 (±0.147) 0.328 (±0.164) 0.549 (±0.266) 0.475 (±0.273) 0.267 (±0.147)

Benign l2

PSNR

0.000 (±0.000) 0.000 (±0.000) 0.000 (±0.000) 0.000 (±0.000) 0.000 (±0.000) 0.000 (±0.000)

134.177 (±3.400) 144.860 (±4.024)
inf (±nan) inf (±nan) inf (±nan) 135.065 (±3.821)

0.013 (±0.011) 0.008 (±0.006) 0.023 (±0.017) 0.044 (±0.044) 0.439 (±0.681) 0.017 (±0.012)

21.106 (±5.096) 22.280 (±3.564) 17.508 (±3.191) 14.935 (±3.315)
7.602 (±5.764) 18.685 (±3.159)

0.022 (±0.020) 0.017 (±0.013) 0.036 (±0.022) 0.136 (±0.108) 0.567 (±0.729) 0.024 (±0.014)

18.740 (±4.933) 18.790 (±3.099) 15.286 (±2.971)
9.709 (±2.998) 5.714 (±5.406) 17.011 (±2.606)

0.040 (±0.031) 0.039 (±0.026) 0.055 (±0.028) 0.358 (±0.180) 0.641 (±0.773) 0.036 (±0.021)

15.478 (±3.927) 14.942 (±2.691) 13.197 (±2.453)
4.965 (±2.107) 4.890 (±5.185) 15.168 (±2.732)

0.081 (±0.059) 0.073 (±0.037) 0.079 (±0.037) 0.483 (±0.199) 0.659 (±0.765) 0.061 (±0.033)

11.906 (±2.875) 11.902 (±2.202) 11.511 (±2.128)
3.528 (±1.795) 4.698 (±5.162) 12.766 (±2.416)

0.127 (±0.075) 0.102 (±0.046) 0.094 (±0.042) 0.556 (±0.213) 0.716 (±0.763) 0.085 (±0.049)

9.608 (±2.286) 10.379 (±1.997) 10.698 (±1.956)
2.872 (±1.697) 3.915 (±4.798) 11.350 (±2.386)

SSIM
1.000 (±0.000) 1.000 (±0.000) 1.000 (±0.000) 1.000 (±0.000) 1.000 (±0.000) 1.000 (±0.000)
0.638 (±0.117) 0.775 (±0.122) 0.604 (±0.127) 0.497 (±0.139) 0.182 (±0.323) 0.715 (±0.126)
0.512 (±0.150) 0.666 (±0.137) 0.514 (±0.155) 0.260 (±0.138) 0.111 (±0.306) 0.627 (±0.138)
0.349 (±0.169) 0.504 (±0.163) 0.448 (±0.157) 0.089 (±0.113) 0.065 (±0.289) 0.529 (±0.150)
0.208 (±0.157) 0.382 (±0.151) 0.404 (±0.153) 0.061 (±0.108) 0.044 (±0.265) 0.433 (±0.144)
0.140 (±0.132) 0.329 (±0.141) 0.380 (±0.150) 0.053 (±0.108) 0.005 (±0.245) 0.374 (±0.137)

Table 3. Demonstration of a multi-model attack. Note that this produces just a single adversarial image, which all models subsequently inpaint. The results are after 500 iterations with a step size of 50 are presented in the form µ (±).

Distance to:
GENERATIVE RFR 0.0 RN CRFILL GMCNN EDGE CONNECT
GENERATIVE RFR 0.03 RN CRFILL GMCNN EDGE CONNECT
GENERATIVE RFR 0.05 RN CRFILL GMCNN EDGE CONNECT
GENERATIVE RFR 0.1 RN CRFILL GMCNN EDGE CONNECT
GENERATIVE RFR 0.2 RN CRFILL GMCNN EDGE CONNECT
GENERATIVE RFR 0.3 RN CRFILL GMCNN EDGE CONNECT

Loss
0.467 (±0.282) 0.279 (±0.220) 0.300 (±0.227) 0.470 (±0.295) 0.485 (±0.267) 0.290 (±0.226)
0.438 (±0.248) 0.312 (±0.212) 0.361 (±0.216) 0.453 (±0.244) 0.621 (±0.347) 0.334 (±0.213)
0.441 (±0.249) 0.332 (±0.219) 0.386 (±0.225) 0.491 (±0.241) 0.605 (±0.329) 0.351 (±0.219)
0.452 (±0.255) 0.365 (±0.229) 0.420 (±0.238) 0.581 (±0.264) 0.650 (±0.372) 0.382 (±0.231)
0.473 (±0.265) 0.395 (±0.238) 0.445 (±0.246) 0.671 (±0.335) 0.663 (±0.361) 0.414 (±0.249)
0.489 (±0.271) 0.412 (±0.242) 0.456 (±0.251) 0.698 (±0.352) 0.681 (±0.383) 0.428 (±0.256)

Original l2

PSNR

0.111 (±0.113) 0.027 (±0.034) 0.025 (±0.023) 0.109 (±0.120) 0.122 (±0.108) 0.025 (±0.022)

12.491 (±6.257) 19.156 (±7.224) 17.891 (±4.161) 13.342 (±7.767) 10.891 (±4.560) 18.198 (±5.129)

0.106 (±0.113) 0.034 (±0.040) 0.065 (±0.037) 0.146 (±0.161) 0.853 (±0.910) 0.053 (±0.030)

12.386 (±5.484) 16.647 (±4.209) 12.642 (±2.712) 9.908 (±3.587) 3.225 (±4.776) 13.325 (±2.230)

0.108 (±0.111) 0.040 (±0.041) 0.081 (±0.047) 0.244 (±0.229) 0.818 (±0.824) 0.057 (±0.033)

12.121 (±5.201) 15.533 (±3.600) 11.697 (±2.776) 7.291 (±3.089) 3.047 (±4.426) 13.032 (±2.216)

0.119 (±0.123) 0.055 (±0.045) 0.110 (±0.065) 0.508 (±0.299) 0.953 (±0.896) 0.072 (±0.051)

11.390 (±4.767) 13.639 (±2.979) 10.333 (±2.615) 3.579 (±2.371) 2.254 (±4.355) 12.158 (±2.498)

0.150 (±0.145) 0.081 (±0.064) 0.140 (±0.075) 0.801 (±0.338) 1.035 (±0.892) 0.096 (±0.065)

9.902 (±3.962) 11.838 (±2.770) 9.130 (±2.277) 1.328 (±1.803) 1.610 (±4.047) 11.004 (±2.691)

0.184 (±0.165) 0.095 (±0.064) 0.154 (±0.076) 0.896 (±0.340) 1.112 (±0.849) 0.112 (±0.075)

8.676 (±3.404) 10.965 (±2.496) 8.620 (±2.073) 0.759 (±1.550) 0.968 (±3.710) 10.336 (±2.672)

SSIM
0.250 (±0.222) 0.387 (±0.261) 0.420 (±0.220) 0.319 (±0.257) 0.210 (±0.208) 0.390 (±0.245)
0.236 (±0.212) 0.337 (±0.240) 0.294 (±0.176) 0.205 (±0.174) -0.037 (±0.261) 0.320 (±0.201)
0.225 (±0.211) 0.325 (±0.228) 0.266 (±0.163) 0.128 (±0.130) -0.018 (±0.246) 0.317 (±0.200)
0.202 (±0.209) 0.314 (±0.211) 0.234 (±0.144) 0.054 (±0.094) -0.049 (±0.243) 0.307 (±0.193)
0.163 (±0.196) 0.301 (±0.199) 0.222 (±0.142) 0.040 (±0.098) -0.067 (±0.246) 0.294 (±0.190)
0.132 (±0.178) 0.288 (±0.188) 0.216 (±0.137) 0.035 (±0.100) -0.067 (±0.239) 0.285 (±0.186)

Loss
0.755 (±0.334) 0.433 (±0.191) 0.473 (±0.204) 0.796 (±0.355) 0.721 (±0.289) 0.422 (±0.189)
0.643 (±0.253) 0.342 (±0.141) 0.266 (±0.143) 0.603 (±0.260) 0.607 (±0.294) 0.253 (±0.114)
0.623 (±0.244) 0.310 (±0.132) 0.203 (±0.115) 0.509 (±0.254) 0.579 (±0.294) 0.216 (±0.100)
0.585 (±0.228) 0.244 (±0.115) 0.133 (±0.084) 0.316 (±0.213) 0.570 (±0.329) 0.158 (±0.075)
0.519 (±0.205) 0.172 (±0.093) 0.085 (±0.057) 0.185 (±0.116) 0.542 (±0.341) 0.107 (±0.051)
0.466 (±0.189) 0.143 (±0.078) 0.067 (±0.045) 0.159 (±0.094) 0.533 (±0.366) 0.089 (±0.042)

Adversarial Target

l2

PSNR

1.186 (±0.298) 0.292 (±0.072) 0.292 (±0.060) 1.205 (±0.309) 1.136 (±0.283) 0.299 (±0.085)

-0.622 (±0.989) 5.464 (±0.992) 5.438 (±0.851) -0.683 (±1.021) -0.432 (±1.014) 5.383 (±1.088)

1.125 (±0.269) 0.257 (±0.072) 0.194 (±0.072) 1.001 (±0.284) 1.392 (±0.838) 0.233 (±0.079)

-0.406 (±0.938) 6.067 (±1.163) 7.520 (±2.116) 0.165 (±1.236) -0.699 (±2.484) 6.547 (±1.323)

1.093 (±0.258) 0.232 (±0.073) 0.153 (±0.071) 0.782 (±0.313) 1.349 (±0.869) 0.206 (±0.078)

-0.281 (±0.926) 6.547 (±1.356) 8.763 (±2.617) 1.481 (±2.070) -0.453 (±2.674) 7.120 (±1.481)

1.015 (±0.236) 0.184 (±0.075) 0.102 (±0.061) 0.366 (±0.306) 1.379 (±0.954) 0.158 (±0.074)

0.035 (±0.917) 7.698 (±1.801) 10.819 (±3.101) 5.892 (±3.956) -0.283 (±3.164) 8.411 (±1.845)

0.879 (±0.214) 0.135 (±0.066) 0.063 (±0.044) 0.104 (±0.107) 1.396 (±1.043) 0.111 (±0.064)

0.674 (±0.967) 9.268 (±2.317) 13.022 (±3.056) 11.718 (±4.190) 0.053 (±3.807) 10.150 (±2.228)

0.768 (±0.208) 0.110 (±0.055) 0.049 (±0.038) 0.060 (±0.058) 1.457 (±1.103) 0.091 (±0.057)

1.284 (±1.082) 10.191 (±2.460) 14.125 (±3.057) 14.076 (±4.019)
0.175 (±4.346) 11.166 (±2.523)

SSIM
0.036 (±0.100) 0.090 (±0.055) 0.104 (±0.051) 0.044 (±0.115) 0.047 (±0.110) 0.104 (±0.056)
0.044 (±0.098) 0.117 (±0.060) 0.214 (±0.092) 0.076 (±0.105) 0.010 (±0.238) 0.193 (±0.058)
0.050 (±0.099) 0.139 (±0.063) 0.239 (±0.098) 0.135 (±0.147) 0.040 (±0.259) 0.214 (±0.056)
0.068 (±0.102) 0.190 (±0.066) 0.278 (±0.098) 0.349 (±0.246) 0.055 (±0.303) 0.238 (±0.050)
0.104 (±0.107) 0.232 (±0.052) 0.308 (±0.083) 0.598 (±0.231) 0.078 (±0.356) 0.259 (±0.045)
0.140 (±0.114) 0.247 (±0.043) 0.322 (±0.086) 0.695 (±0.184) 0.081 (±0.395) 0.268 (±0.047)

Loss
0.179 (±0.062) 0.001 (±0.001) 0.001 (±0.001) 0.180 (±0.059) 0.181 (±0.060) 0.001 (±0.001)
0.273 (±0.121) 0.129 (±0.086) 0.231 (±0.116) 0.344 (±0.137) 0.440 (±0.250) 0.159 (±0.087)
0.289 (±0.132) 0.161 (±0.100) 0.271 (±0.133) 0.406 (±0.156) 0.432 (±0.234) 0.190 (±0.102)
0.317 (±0.150) 0.213 (±0.118) 0.319 (±0.153) 0.518 (±0.208) 0.476 (±0.273) 0.233 (±0.126)
0.355 (±0.173) 0.258 (±0.134) 0.351 (±0.164) 0.618 (±0.294) 0.492 (±0.273) 0.273 (±0.149)
0.379 (±0.187) 0.278 (±0.140) 0.365 (±0.169) 0.646 (±0.311) 0.511 (±0.284) 0.289 (±0.158)

Benign l2

PSNR

0.000 (±0.000) 0.000 (±0.000) 0.000 (±0.000) 0.000 (±0.000) 0.000 (±0.000) 0.000 (±0.000)

134.254 (±3.384) 144.880 (±3.990)
inf (±nan) inf (±nan) inf (±nan) 135.133 (±3.776)

0.018 (±0.015) 0.011 (±0.009) 0.043 (±0.029) 0.072 (±0.061) 0.729 (±0.886) 0.036 (±0.027)

19.479 (±4.839) 20.816 (±3.711) 14.735 (±3.338) 12.690 (±3.306)
4.847 (±5.691) 15.380 (±2.813)

0.028 (±0.025) 0.019 (±0.013) 0.062 (±0.040) 0.186 (±0.167) 0.698 (±0.801) 0.042 (±0.031)

17.483 (±4.559) 18.374 (±3.171) 13.094 (±3.125)
8.676 (±3.444) 4.491 (±5.206) 14.655 (±2.658)

0.050 (±0.044) 0.037 (±0.022) 0.092 (±0.054) 0.478 (±0.273) 0.825 (±0.860) 0.060 (±0.049)

14.609 (±3.986) 15.165 (±2.771) 11.133 (±2.737)
3.861 (±2.430) 3.409 (±4.919) 13.205 (±2.890)

0.093 (±0.078) 0.064 (±0.037) 0.124 (±0.061) 0.788 (±0.308) 0.916 (±0.882) 0.086 (±0.063)

11.588 (±3.386) 12.670 (±2.637)
9.574 (±2.225) 1.368 (±1.730) 2.670 (±4.723) 11.646 (±3.006)

0.135 (±0.103) 0.079 (±0.041) 0.140 (±0.062) 0.884 (±0.309) 0.993 (±0.843) 0.103 (±0.073)

9.746 (±2.973) 11.604 (±2.382)
8.970 (±1.995) 0.789 (±1.480) 1.851 (±4.251) 10.865 (±3.012)

SSIM
1.000 (±0.000) 1.000 (±0.000) 1.000 (±0.000) 1.000 (±0.000) 1.000 (±0.000) 1.000 (±0.000)
0.569 (±0.123) 0.705 (±0.143) 0.497 (±0.151) 0.405 (±0.151) 0.029 (±0.332) 0.603 (±0.146)
0.476 (±0.146) 0.630 (±0.149) 0.435 (±0.150) 0.232 (±0.141) 0.038 (±0.301) 0.546 (±0.151)
0.346 (±0.169) 0.504 (±0.172) 0.368 (±0.140) 0.079 (±0.100) -0.024 (±0.285) 0.476 (±0.153)
0.226 (±0.173) 0.414 (±0.181) 0.327 (±0.130) 0.053 (±0.101) -0.055 (±0.268) 0.421 (±0.166)
0.167 (±0.163) 0.381 (±0.173) 0.315 (±0.129) 0.047 (±0.100) -0.066 (±0.258) 0.398 (±0.168)

Table 4. Impact of markpainting attack on each model individually: the model in each row is attacked and the results presented are from evaluation on that same model. This table uses the same input/target/mask combinations as Table 5. The results are after 100 iterations with a step size of 50 and are presented in the form µ (±).

Markpainting: Adversarial Machine Learning meets Inpainting

Distance to:
GENERATIVE RFR 0.0 RN CRFILL GMCNN EDGE CONNECT
GENERATIVE RFR 0.03 RN CRFILL GMCNN EDGE CONNECT
GENERATIVE RFR 0.05 RN CRFILL GMCNN EDGE CONNECT
GENERATIVE RFR 0.1 RN CRFILL GMCNN EDGE CONNECT
GENERATIVE RFR 0.2 RN CRFILL GMCNN EDGE CONNECT
GENERATIVE RFR 0.3 RN CRFILL GMCNN EDGE CONNECT

Loss
0.467 (±0.282) 0.279 (±0.220) 0.300 (±0.227) 0.470 (±0.295) 0.485 (±0.267) 0.290 (±0.226)
0.466 (±0.280) 0.287 (±0.219) 0.361 (±0.216) 0.472 (±0.296) 0.484 (±0.266) 0.291 (±0.223)
0.466 (±0.279) 0.296 (±0.220) 0.386 (±0.225) 0.478 (±0.300) 0.485 (±0.265) 0.293 (±0.221)
0.469 (±0.278) 0.318 (±0.224) 0.420 (±0.238) 0.499 (±0.315) 0.485 (±0.265) 0.304 (±0.222)
0.478 (±0.280) 0.352 (±0.234) 0.445 (±0.246) 0.526 (±0.321) 0.489 (±0.266) 0.322 (±0.225)
0.487 (±0.277) 0.373 (±0.237) 0.456 (±0.251) 0.541 (±0.320) 0.493 (±0.266) 0.334 (±0.225)

Original l2

PSNR

0.111 (±0.113) 0.027 (±0.034) 0.025 (±0.023) 0.109 (±0.120) 0.122 (±0.108) 0.025 (±0.022)

12.491 (±6.257) 19.156 (±7.224) 17.891 (±4.161) 13.342 (±7.767) 10.891 (±4.560) 18.198 (±5.129)

0.110 (±0.113) 0.028 (±0.035) 0.065 (±0.037) 0.107 (±0.115) 0.121 (±0.103) 0.025 (±0.021)

12.551 (±6.370) 18.593 (±6.047) 12.642 (±2.712) 13.200 (±7.266) 10.816 (±4.359) 18.041 (±4.864)

0.110 (±0.110) 0.028 (±0.035) 0.081 (±0.047) 0.108 (±0.116) 0.121 (±0.101) 0.025 (±0.021)

12.552 (±6.336) 18.146 (±5.525) 11.697 (±2.776) 12.947 (±6.918) 10.789 (±4.306) 17.837 (±4.701)

0.111 (±0.114) 0.032 (±0.038) 0.110 (±0.065) 0.117 (±0.124) 0.121 (±0.097) 0.028 (±0.021)

12.406 (±6.074) 17.145 (±4.658) 10.333 (±2.615) 12.039 (±5.854) 10.729 (±4.167) 17.147 (±4.125)

0.113 (±0.110) 0.039 (±0.043) 0.140 (±0.075) 0.139 (±0.136) 0.122 (±0.091) 0.032 (±0.023)

12.037 (±5.618) 15.691 (±3.755) 9.130 (±2.277) 10.363 (±4.237) 10.537 (±3.947) 16.066 (±3.440)

0.115 (±0.109) 0.043 (±0.038) 0.154 (±0.076) 0.157 (±0.126) 0.127 (±0.101) 0.036 (±0.024)

11.625 (±5.054) 14.869 (±3.380) 8.620 (±2.073) 9.376 (±3.685) 10.359 (±3.862) 15.416 (±3.074)

SSIM
0.250 (±0.222) 0.387 (±0.261) 0.420 (±0.220) 0.319 (±0.257) 0.210 (±0.208) 0.390 (±0.245)
0.245 (±0.219) 0.362 (±0.252) 0.294 (±0.176) 0.299 (±0.243) 0.206 (±0.201) 0.378 (±0.239)
0.240 (±0.215) 0.340 (±0.241) 0.266 (±0.163) 0.281 (±0.235) 0.204 (±0.198) 0.368 (±0.235)
0.228 (±0.202) 0.299 (±0.216) 0.234 (±0.144) 0.240 (±0.216) 0.200 (±0.191) 0.344 (±0.222)
0.204 (±0.183) 0.245 (±0.172) 0.222 (±0.142) 0.181 (±0.177) 0.189 (±0.176) 0.307 (±0.194)
0.169 (±0.144) 0.215 (±0.147) 0.216 (±0.137) 0.133 (±0.127) 0.173 (±0.160) 0.282 (±0.180)

Loss
0.755 (±0.334) 0.433 (±0.191) 0.473 (±0.204) 0.796 (±0.355) 0.721 (±0.289) 0.422 (±0.189)
0.748 (±0.330) 0.416 (±0.184) 0.266 (±0.143) 0.786 (±0.353) 0.717 (±0.289) 0.407 (±0.180)
0.745 (±0.329) 0.408 (±0.180) 0.203 (±0.115) 0.782 (±0.356) 0.717 (±0.289) 0.400 (±0.175)
0.739 (±0.323) 0.394 (±0.169) 0.133 (±0.084) 0.770 (±0.357) 0.714 (±0.287) 0.392 (±0.169)
0.734 (±0.322) 0.382 (±0.160) 0.085 (±0.057) 0.736 (±0.333) 0.713 (±0.288) 0.380 (±0.161)
0.734 (±0.317) 0.374 (±0.153) 0.067 (±0.045) 0.710 (±0.322) 0.712 (±0.286) 0.373 (±0.157)

Adversarial Target

l2

PSNR

1.186 (±0.298) 0.292 (±0.072) 0.292 (±0.060) 1.205 (±0.309) 1.136 (±0.283) 0.299 (±0.085)

-0.622 (±0.989) 5.464 (±0.992) 5.438 (±0.851) -0.683 (±1.021) -0.432 (±1.014) 5.383 (±1.088)

1.175 (±0.298) 0.282 (±0.071) 0.194 (±0.072) 1.180 (±0.308) 1.129 (±0.282) 0.293 (±0.088)

-0.579 (±0.997) 5.617 (±1.011) 7.520 (±2.116) -0.589 (±1.036) -0.405 (±1.019) 5.489 (±1.139)

1.167 (±0.299) 0.273 (±0.070) 0.153 (±0.071) 1.157 (±0.305) 1.126 (±0.282) 0.289 (±0.090)

-0.548 (±1.006) 5.761 (±1.029) 8.763 (±2.617) -0.503 (±1.040) -0.392 (±1.022) 5.563 (±1.173)

1.149 (±0.300) 0.254 (±0.070) 0.102 (±0.061) 1.091 (±0.292) 1.118 (±0.281) 0.280 (±0.092)

-0.475 (±1.020) 6.103 (±1.099) 10.819 (±3.101) -0.244 (±1.048) -0.359 (±1.029) 5.714 (±1.234)

1.119 (±0.302) 0.227 (±0.071) 0.063 (±0.044) 0.973 (±0.273) 1.104 (±0.280) 0.268 (±0.093)

-0.355 (±1.048) 6.632 (±1.255) 13.022 (±3.056) 0.265 (±1.077) -0.301 (±1.041) 5.937 (±1.300)

1.099 (±0.303) 0.211 (±0.072) 0.049 (±0.038) 0.895 (±0.267) 1.095 (±0.290) 0.259 (±0.093)

-0.270 (±1.068) 6.988 (±1.393) 14.125 (±3.057) 0.642 (±1.136) -0.257 (±1.078) 6.092 (±1.351)

SSIM
0.036 (±0.100) 0.090 (±0.055) 0.104 (±0.051) 0.044 (±0.115) 0.047 (±0.110) 0.104 (±0.056)
0.036 (±0.098) 0.088 (±0.053) 0.214 (±0.092) 0.044 (±0.109) 0.048 (±0.108) 0.107 (±0.056)
0.035 (±0.098) 0.086 (±0.052) 0.239 (±0.098) 0.045 (±0.105) 0.049 (±0.108) 0.107 (±0.056)
0.034 (±0.092) 0.082 (±0.048) 0.278 (±0.098) 0.052 (±0.095) 0.049 (±0.106) 0.106 (±0.054)
0.032 (±0.084) 0.078 (±0.043) 0.308 (±0.083) 0.061 (±0.083) 0.049 (±0.101) 0.107 (±0.052)
0.025 (±0.068) 0.076 (±0.039) 0.322 (±0.086) 0.063 (±0.076) 0.046 (±0.095) 0.106 (±0.051)

Loss
0.179 (±0.062) 0.001 (±0.001) 0.001 (±0.001) 0.180 (±0.059) 0.181 (±0.060) 0.001 (±0.001)
0.214 (±0.085) 0.032 (±0.027) 0.231 (±0.116) 0.230 (±0.090) 0.195 (±0.074) 0.036 (±0.030)
0.228 (±0.096) 0.059 (±0.051) 0.271 (±0.133) 0.266 (±0.124) 0.198 (±0.071) 0.054 (±0.044)
0.254 (±0.118) 0.107 (±0.078) 0.319 (±0.153) 0.334 (±0.181) 0.207 (±0.078) 0.090 (±0.067)
0.285 (±0.135) 0.161 (±0.108) 0.351 (±0.164) 0.405 (±0.227) 0.225 (±0.088) 0.131 (±0.088)
0.312 (±0.149) 0.191 (±0.116) 0.365 (±0.169) 0.439 (±0.239) 0.236 (±0.090) 0.153 (±0.098)

Benign l2

PSNR

0.000 (±0.000) 0.000 (±0.000) 0.000 (±0.000) 0.000 (±0.000) 0.000 (±0.000) 0.000 (±0.000)

134.217 (±3.413) 144.923 (±3.966)
inf (±nan) inf (±nan) inf (±nan) 135.041 (±3.839)

0.005 (±0.006) 0.001 (±0.001) 0.043 (±0.029) 0.008 (±0.008) 0.005 (±0.019) 0.001 (±0.001)

26.037 (±6.145) 30.462 (±3.392) 14.735 (±3.338) 23.472 (±5.608) 31.205 (±5.169) 29.890 (±3.880)

0.009 (±0.011) 0.003 (±0.003) 0.062 (±0.040) 0.016 (±0.016) 0.005 (±0.018) 0.003 (±0.002)

23.663 (±6.018) 26.769 (±3.464) 13.094 (±3.125) 20.294 (±5.380) 28.754 (±4.657) 27.208 (±3.807)

0.015 (±0.015) 0.008 (±0.007) 0.092 (±0.054) 0.038 (±0.037) 0.007 (±0.023) 0.006 (±0.005)

20.624 (±5.403) 22.348 (±3.283) 11.133 (±2.737) 16.156 (±4.685) 25.782 (±4.223) 23.328 (±3.647)

0.027 (±0.023) 0.016 (±0.014) 0.124 (±0.061) 0.078 (±0.065) 0.013 (±0.035) 0.013 (±0.010)

17.797 (±4.840) 18.952 (±3.007)
9.574 (±2.225) 12.485 (±3.724) 22.891 (±4.258) 20.012 (±3.443)

0.036 (±0.028) 0.022 (±0.015) 0.140 (±0.062) 0.106 (±0.071) 0.014 (±0.030) 0.018 (±0.014)

16.181 (±4.334) 17.416 (±2.858)
8.970 (±1.995) 10.830 (±3.355) 21.002 (±3.783) 18.638 (±3.443)

SSIM
1.000 (±0.000) 1.000 (±0.000) 1.000 (±0.000) 1.000 (±0.000) 1.000 (±0.000) 1.000 (±0.000)
0.829 (±0.100) 0.948 (±0.038) 0.497 (±0.151) 0.809 (±0.092) 0.922 (±0.071) 0.911 (±0.049)
0.760 (±0.127) 0.899 (±0.070) 0.435 (±0.150) 0.703 (±0.123) 0.888 (±0.079) 0.860 (±0.068)
0.642 (±0.151) 0.801 (±0.102) 0.368 (±0.140) 0.529 (±0.164) 0.824 (±0.099) 0.759 (±0.099)
0.502 (±0.166) 0.692 (±0.120) 0.327 (±0.130) 0.347 (±0.167) 0.738 (±0.122) 0.645 (±0.117)
0.400 (±0.152) 0.633 (±0.123) 0.315 (±0.129) 0.247 (±0.132) 0.664 (±0.120) 0.586 (±0.124)

Table 5. Key attribution based on the adversarial sample produced. RN was attacked. The results are after 100 iterations with a step size of 50 . The results are presented in the form µ (±)

