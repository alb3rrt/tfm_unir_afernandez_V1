1
PDPGD: Primal-Dual Proximal Gradient Descent Adversarial Attack
Alexander Matyasko, Member, IEEE and Lap-Pui Chau, Fellow, IEEE,

arXiv:2106.01538v1 [cs.LG] 3 Jun 2021

Abstract--State-of-the-art deep neural networks are sensitive to small input perturbations. Since the discovery of this intriguing vulnerability, many defence methods have been proposed that attempt to improve robustness to adversarial noise. Fast and accurate attacks are required to compare various defence methods. However, evaluating adversarial robustness has proven to be extremely challenging. Existing norm minimisation adversarial attacks require thousands of iterations (e.g. Carlini & Wagner attack), are limited to the specific norms (e.g. Fast Adaptive Boundary), or produce sub-optimal results (e.g. Brendel & Bethge attack). On the other hand, PGD attack, which is fast, general and accurate, ignores the norm minimisation penalty and solves a simpler perturbation-constrained problem. In this work, we introduce a fast, general and accurate adversarial attack that optimises the original non-convex constrained minimisation problem. We interpret optimising the Lagrangian of the adversarial attack optimisation problem as a two-player game: the first player minimises the Lagrangian wrt the adversarial noise; the second player maximises the Lagrangian wrt the regularisation penalty. Our attack algorithm simultaneously optimises primal and dual variables to find the minimal adversarial perturbation. In addition, for non-smooth lp-norm minimisation, such as l-, l1-, and l0-norms, we introduce primal-dual proximal gradient descent attack. We show in the experiments that our attack outperforms current state-of-the-art l-, l2-, l1-, and l0-attacks on MNIST, CIFAR-10 and Restricted ImageNet datasets against unregularised and adversarially trained models.
Index Terms--Adversarial examples, adversarial attacks, adversarial machine learning, deep learning (DL).
I. INTRODUCTION
D EEP neural networks (DNNs) have been remarkably successful for a wide range of perceptual problems: image classification [1], object detection [2], speech recognition [3], machine translation [4]. Despite their excellent performance in an extensive range of practical applications, DNNs are sensitive to small, imperceptible perturbations in the input data. This intriguing vulnerability was discovered by Szegedy et al. [5]. It was found that it is possible to perturb any given image in such a way that deep neural network misclassifies it with high confidence, but the image remains visually indistinguishable from the original image to a human observer. The lack of robustness is not specific to convolutional neural networks (CNNs) for image recognition problems. Subsequently, it was found that recurrent neural networks (RNNs) are susceptible to perturbations in text for language understanding task [6] and audio for speech recognition task [7].
The authors are with the Department of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, 639798 SG (e-mail: aliaksan001@e.ntu.edu.sg; elpchau@ntu.edu.sg).

The lack of robustness to small, imperceptible perturbations is counter-intuitive. Unlike state-of-the-art deep neural networks models, human vision is remarkably robust to variations in the input, e.g. changes in the lighting condition or changes in the object shape or pose. For example, if someone wants to conceal his or her identity from the police, the person will have to wear a mask or undergo cosmetic surgery. In comparison, an adversary needs to change only a few pixels in the image to fool a state-of-the-art facial recognition system [8]. Additionally, the existence of adversarial examples poses a serious concern for the application of deep neural networks in safety and security-critical applications [9]. For example, recent studies showed that the attacks could be applied in the physical world [10, 11].
The problem of adversarial examples has spurred significant interest in the research of deep neural networks. The field of research in the area of robust deep learning can be broadly divided into the research on attacks [12, 13, 14] and the research on the defences [15, 16, 17]. Like in an arms race, these two sides compete with each other: new defences introduced to protect against existing attacks; new attacks introduced to counteract existing defences. Since this vulnerability was discovered, dozens of techniques have been proposed to improve robustness to adversarial noise. To no avail, the problem of training a robust deep neural network remains unsolved. It's now even speculated that adversarial examples could fool time-limited human observers [18].
The goal of the adversarial attack is to find a minimal lpnorm perturbation that changes the model's prediction. Solving this non-convex constrained minimisation problem has proven to be challenging [19]. Existing attacks, such as C&W [13] and EAD [20], in place of the original constrained problem, solve a sequence of unconstrained problems for multiple values of the regularisation weight  selected using line search or binary search. The attack's optimisation search needs to be restarted multiple times for each value , which increase the computational cost of the attack. Other methods, such as B&B [21] and FAB [14] attacks, are either limited to the specific norms or produce sub-optimal results. Fast, general and accurate attack, such as PGD [10, 16], reformulates the original norm minimisation problem with non-convex error constraint as a surrogate loss minimisation with convex lp-norm perturbation constraint. For this "simpler" problem, projected gradient descent attack (PGD) is an optimal first-order adversary [16]. However, PGD attack does not explicitly minimise the perturbation lp-norm. Instead, PGD attack minimises the model's accuracy at the threshold . PGD attack needs to be restarted multiple times to evaluate the robust accuracy at multiple thresholds.

2

In this work, we introduce a fast and efficient adversarial attack. Our attack directly optimises the original non-convex constrained norm minimisation problem without intensive optimisation restarts. We interpret optimising the Lagrangian of the attack as playing a two-player game. The first player minimises the Lagrangian wrt the adversarial noise, while the second player maximises the Lagrangian wrt the regularisation penalty, which penalises the first player for the violation of the misclassification constraint. Then, we apply primaldual gradient descent to simultaneously update primal and dual variables to find the minimal adversarial perturbation. In addition, we introduce primal-dual proximal gradient descent attack for non-smooth lp-norm minimisation, such as l-, l1- and l0-norms. For RGB images, we propose a group l0Gnorm proximal operator, which we use to minimise the number of perturbed pixels. We demonstrate in the experiments that our attack outperforms current state-of-the-art l-, l2-, l1and l0-attacks on MNIST, CIFAR-10 and Restricted ImageNet datasets against unregularised and adversarially trained models. The source code to reproduce all our experiments is available at https://github.com/aam-at/cpgd. We hope that our attack will be considered as a benchmark for a future comparison between different defence methods.
II. RELATED WORK
A plethora of adversarial attacks and defences against adversarial attacks have been proposed since the discovery of the vulnerability of DNNs to adversarial examples. In this section, we attempt to review the landscape of the research on adversarial attacks. Due to the space constraints, only selected relevant works are discussed. For a detailed overview of this diverse and active field, interested readers may refer to [22].
Adversarial attacks can be broadly categorized based on the attacker's knowledge about the model (white-box versus blackbox attacks), the attack's specificity (targeted versus nontargeted attacks), and the perturbation measurement (l-, l2-, l1-, and l0-norm attacks). White-box attacks have full knowledge of the neural network model, including training data, model architecture, model weights, and model hyperparameters. Adversarial examples are generated using the model's gradients. Black-box attacks have access to the model's outputs but do not know the training data and the model architecture. This assumption is valid for attacking online ML services. Targeted attacks aim to produce a targeted misclassification, whereas the adversarial label for an untargeted attack can be arbitrary except the original one.
A. White-box Adversarial Attacks
[5] first discovered the phenomenon of adversarial examples and introduced a targeted gradient-based adversarial attack against DNNs known as LBFGS-B method. LBFGS-B method is the basis of many attack algorithms. Starting with an input x with a label y, the authors minimised the norm of the perturbation r subject to the constraint that the neural network misclassifies the adversarial example x + r as some target yt = y. A surrogate loss function is introduced in place of the original non-differentiable error constraint. Then, an

unconstrained optimisation problem is solved using LBFGSB optimiser for multiple values of the constraint regularisation weight , which is selected using a line search. However, this attack is impractical against large models because 1) it uses computationally intensive LBFGS-B method; 2) it requires full optimisation for each value of the regularisation weight .
1) l-norm attacks: [15] reformulated the attack's problem as minimising the misclassification surrogate loss subject to the lp-norm perturbation constraint. Then, they noticed that after applying the first-order approximation to the surrogate loss, the normalised loss gradient wrt inputs is the adversarial direction. In particular, the adversarial perturbation for the l-norm constraint is the scaled sign of the gradient. This method, known as fast gradient method (FGM), is inaccurate but extremely fast. [10, 16] proposed an iterative version of FGM, which is known as basic iterative method (BIM) [10] or projected gradient descent (PGD) [16]. PGD iteratively takes a step in the direction of FGM attack and constrains the perturbation after each update. [16] argued that PGD is an optimal first-order adversary. PGD attack is a recommended starting attack for l-norm distortions [19]. [23] integrated the momentum into PGD iterative process. Distributionally adversarial attack (DAA) [24] finds an adversarial data distribution that maximises the generalisation risk. The optimal data distribution is described by PDE, which they solve using particle optimisation. [25] introduced a proximal log-barrier attack (ProxLogBarrier), which, similar to our work, uses proximal optimisation for non-smooth norms. In comparison with our work, ProxLogBarrier attack: 1) requires an adversarial starting point, while any starting point can be used for our attack; 2) optimises the log-barrier loss, while our primal-dual attack solves for the original error constraint.
2) l2-norm attacks: In seminal work, Carlini and Wagner [13] formally defined the problem of finding adversarial perturbation. They introduced C&W attack that solves a sequence of unconstrained optimisation problems, similar to [5]. They investigated how the choice of the optimiser, the surrogate loss function, and handling of the box constraints affect the attack's success. C&W is the recommended attack for the assessment of DNNs robustness to l2-norm perturbations [19]. C&W, like LBFGS-B attack, requires full optimisation for each value of the regularisation weight , which increases the attack's computational cost. DeepFool [12] finds the closest class boundary and takes a step in that direction. DeepFool attack does not enforce box constraints and does not explicitly minimise the norm of the perturbation. The optimisation process stops as soon as adversarial perturbation is found. Fast Adaptive Boundary attack (FBA) [14] solves the boxconstrained lp-norm projection on the decision hyperplane exactly and introduces a biased backward step to minimise the perturbation norm. Decoupling direction and norm l2attack (DDN) [26] proposes to adjust the radius of l2-norm ball used for the l2-norm projection. If the perturbation is adversarial/not adversarial, the radius of l2-norm projection ball can be decreased/increased, respectively.
3) l1-norm attacks: [20] introduced l1-norm attack known as Elastic-net (EAD). EAD attack, similar to C&W, solves a sequence of unconstrained optimisation problems. To

3

minimise non-smooth, subdifferentiable l1-norm, they proposed to use fast iterative shrinkage-thresholding algorithm (FISTA) [27]. EAD attack is recommended l1-norm attack [19]. SparseFool (SF) [28] is a geometry-inspired l1norm attack that uses DeepFool attack [12] as subprocedure to estimate the local curvature of the decision boundary. They developed an efficient algorithm to compute l1-projection of the perturbation on the decision boundary subject to the box constraints. Sparse l1 descent attack (SLIDE) [29] is a variant of PGD attack for l1-norm. SLIDE iteratively takes a small step in the direction of the q-th percentile of the loss gradient and applies l1-norm projection, which can be computed efficiently in O (n log n) time [30].
4) l0-norm attacks: l and l2 adversarial attacks often produce perturbations that change a large number of pixels in the image. Several attack methods have been proposed to minimise non-convex l0-quasinorm. [9] proposed a targeted adversarial attack, known as Jacobian saliency map attack (JSMA), which minimises l0-norm perturbation constraint. JSMA uses the Jacobian matrix to select and modify a pair of the most salient pixels in the image. This process is repeated until the adversarial perturbation is found. [8] found that it is possible to change the prediction of the model by modifying a single pixel. To generate adversarial examples, they applied differential evolution (DE) algorithm on the population of vectors that change a single pixel in the image. [31] introduced l0-norm variant of PGD attack and black-box, score-based l0-norm attack known as CornerSearch (CS) attack. PGD-l0 attack iteratively takes a small step in the direction of the loss gradient and applies l0-norm projection. CS attack creates a probability distribution of the most salient pixels from which the adversarial perturbation is sampled.
B. Black-box adversarial attacks
A black-box adversary has limited knowledge about the model, e.g. model prediction scores or outputs. Black-box attacks are more difficult to perform than white-box attacks because we do not know the model's gradient. [32] introduced a practical black-box adversarial attack based on the property that adversarial examples can transfer between models. They trained a substitute model on the model's task. Then, adversarial examples generated for the substitute model are used to attack the target model. Brendel et al. [33] introduced a decision-based attack that estimates the decision boundary using rejection sampling. Starting at some adversarial input, they randomly draw a random perturbation from the candidate distribution and minimise the distance to the original input. [34] sampled a random perturbation from an orthonormal basis of discrete cosine transform (DCT), which improves queryefficiency of the decision-based attack. Gradient-based attacks should be almost always more precise than gradient-free attacks. However, gradient masking [32] can fool gradient-based attacks and give a false sense of security [35]. If the defence obfuscates the gradients, gradient-free attacks often perform better than white-box attacks. [19] suggested that defences should be tested on both white-box and black-box adversaries. If the model is more robust to white-box adversaries, then the model obfuscates the gradients.

III. ADVERSARIAL ATTACKS ON DNNS

In this section, we introduce a general formulation of the
attack against deep neural networks in white-box settings,
where the attacker has full knowledge about the model. Let f ( · ) be the mapping from the space of input pixels to the
unnormalised predictive distribution on discrete label output space f : RN  Rk where k is the number of classes. The network prediction is the label with the highest score k^(x) = arg max f (x). For a given input image x with the label y, an adversary aims to find a minimal adversarial perturbation r wrt some norm · , such that after adding the perturbation to the original image x it changes the network prediction k^(x + r) = y. We can formulate the attack as the
following optimisation problem:

min r
r

s.t. k^(x + r) = y

(1)

x+rC

where r and x + r is the adversarial noise and the adversarial example respectively; C is the input domain, e.g. [0, 1]N box constraints for the normalised image. The problem above is an example of an untargeted adversarial attack. A targeted adversarial attack searches for the perturbation which changes the network prediction to the specific target: k^(x + r) = yt.
The optimisation problem in Equation (1) is a non-convex constrained norm minimisation problem. Solving it is a challenging and non-trivial task because 1) the misclassification constraint is non-convex and non-differentiable; 2) the norm of the perturbation is non-differentiable, e.g. l0- and l1-, and l-norms. A plethora of attack methods have been proposed to solve the adversarial attack problem. Szegedy et al. [5] first introduced a blueprint for a targeted white-box adversarial attack, LBFGS-B method, against DNNs in 2014. The authors optimised the following unconstrained minimisation problem:

min r + L (x + r; y)

r

(2)

s.t. x + r  C

where  is the regularisation penalty which penalises the violation of the misclassification constraint; L is a surrogate loss function, which is minimised when k^(x + r) = y. The solution of the problem in Equation (2) for a fixed  can be found using an off-shelf optimiser, e.g. LBFGS-B in [5] or ADAM in [13]. Finally, a line search or a binary search is performed to find the optimal regularisation weight , which minimises r . Optimisation of Equation (2) needs to be restarted for each , which significantly increases the attack cost. Besides that, the optimisation search above requires gradient, so this procedure is not applicable for non-smooth or non-differentiable function minimisation, e.g. l- or l0-norms.
Goodfellow et al. [15] reformulated the original non-convex constrained minimisation problem as the problem of minimising the differentiable surrogate loss function subject to the convex lp-norm perturbation constraint:

min L (x + r; y)
r

s.t. r 

(3)

x+rC

4

They observed that after applying the first-order approximation to the surrogate loss L, the normalised gradient of the loss is the solution of the lp-norm constrained minimisation problem. In particular, the adversarial direction for the lnorm constraint is the sign of the loss gradient wrt inputs: r = sign xL(x; y). This method, known as fast gradient sign method (FGSM), is inaccurate but has dramatically increased the speed of generating adversarial noise. [10, 16] proposed an iterative version of FGSM attack, known as projected gradient descent (PGD). PGD iteratively updates the perturbation r by taking a small step in the direction of the adversarial target and constraints the total perturbation to after each iteration. PGD is a simple, fast and accurate attack. This attack has also been extended to other norms, including non-differentiable l1- and l0-norms [29, 31]. However, PGD does not explicitly minimise the lp-norm of the perturbation r. PGD attack needs to be restarted N times to evaluate robustness at N distinct thresholds , which increases the attack's computational cost.

IV. PRIMAL-DUAL GRADIENT DESCENT ATTACK

Let us revisit the Lagrangian of the original non-convex constrained lp-norm minimisation problem in Equation (1):

L(r, ) = r + I k^ (x + r) = y

(4)

where  is a dual variable, which controls the weight of the misclassification constraint; I is an indicator function. For brevity, we omit the domain constraints x + r  C as we can enforce it easily for typical inputs, e.g. [0, 1] box-projection C for natural images. As   , the solution of the problem in Equation (4) converges to the feasible solution of the original non-convex constrained norm minimisation problem.
Optimising the Lagrangian in Equation (4) can be interpreted as playing a two-player game: the r-player seeks to minimise the Lagrangian wrt primal variable r; the -player wishes to maximise the Lagrangian wrt dual variable . The dual variable  penalises the r-player for the violation of the misclassification constraint. C&W attack [13] uses a binary search to find an optimal value of the dual variable . For a fixed , each iteration of binary search requires full optimisation of the Lagrangian to find an optimal value of the primal variable r, which increases the attack's running time.
We propose a primal-dual gradient algorithm to simultaneously optimise primal and dual variables. Unfortunately, we cannot calculate the constraint gradients to optimise the Lagrangian L(r, ) using the first-order algorithm because the misclassification constraint is non-differentiable. In line with the previous research, we express the error constraint in terms of the prediction margin. We define the prediction margin my(x) for the input x with label y as follows:

my(x) = f (x)y - max f (x)i

(5)

i=y

The input x with the label y is misclassified if and only
if my(x) < 0. Some popular smooth relaxations of 0/1indicator function are squared loss, hinge loss, and logistic loss. We adopt logistic loss log 1 + emy(x) instead of hinge
loss max (0, 1 + my (x)) as in [13] because logistic loss is differentiable everywhere unlike hinge loss.

Using our surrogate loss L for the misclassification constraint, we introduce two proxy-Lagrangians for the r-player and the -player as follows:

Lr(r, ) = 1 r + 2L (x + r, y)

L(r, ) = 2I k^ (x + r) = y

(6)

where (1, 2)   and   R2+ is a 2-dimensional simplex. In the formulation above, we represent   R+ as point in the 2-dimensional simplex  = 2/1. Note that only the r-player

uses surrogate loss in place of the misclassification constraint,

while the -player uses original non-differentiable misclassifi-

cation constraint. The -player chooses how much the r-player

should penalize surrogate misclassification constraint, but does

so in a way as to satisfy original non-differentiable constraint.

The proxy-Lagrangians formulation in Equation (6) avoids

the issue of the non-differentiable error constraint. The r-

player wishes to find perturbation r which minimises Lr(r, ), while the -player wants to maximise L(r, ). Unfortunately,

the proxy-Lagrangian formulation corresponds to a non-zero-

sum game because two players optimise two different func-

tions. Fortunately, this proxy-Lagrangian formulation admits a

weaker type of equilibrium, -correlated equilibrium (see [36,

Theorem 2] for the details).

Next, we describe our primal-dual gradient descent at-

tack (PDGD) in Algorithm 1. The r-player minimises the ex-

ternal regret, while the -player minimises the swap regret. We

perform gradient descent on primal variables using Adam [37].

Adam optimiser produced the smallest perturbation in our

experiments. In line 5, after each iteration, we project the

perturbation r on the domain constraints C using operator xC,

which

we

define

as

follows:

x (r)
C

=

C(x+r)-x.

For

the

-

player, we perform gradient ascent in the log domain. Gradient

updates in the log domain are equivalent to multiplicative

updates, which guarantee that dual variables remain positive.

In line 6, after each update, we project  onto a 2-dimensional

simplex. Intuitively, if at an iteration k, the misclassification

constraint is not satisfied, we can increase the penalty 2 for the r-player. If the constraint is satisfied, we can reduce

the penalty weight 2. Finally, we record and store the best perturbation found during the optimisation in lines 7 to 9.

Algorithm 1 Primal-Dual Gradient Descent Attack

Input: Image x, label y, initial perturbation r(0), the total

number of iterations T , learning rate r and . Output: Adversarial perturbation r.

1: r  0

2: for k  1 to T do 3: Let (rk) be a gradient of Lr(r(k), (k)) 4: Let (k) be a gradient of L(r(k), (k)) 5: Update r(k+1) = xC r(k) - r(rk)

6: Update (k+1) =  (k) + (k)

7: if k^(x + r(k+1)) = y and r(k+1)  r

8:

r  r(k+1)

then

9: end if

10: end for

5

Our primal-dual gradient descent attack has two shortcom-

ings. First, we use gradient descent to minimise the external

regret of the r-player. Gradient descent for smooth convex

functions has a convergence rate of O (1/T ) [38], where T

is the number of gradient iterations. For the non-smooth

functions, descent is

eO.g(.1/l1-Tn)o,rmw,hitchhe

convergence rate of subgradient is considerably slower than sub-

linear convergence of gradient descent for smooth functions.

Secondly, our optimisation algorithm requires gradient, so it

cannot be used to minimise non-differentiable functions, such

as l0-quasinorm. In the next section, we introduce a proximal

formulation of our attack suitable for minimising any lp-norm

or function with a closed-form proximity operator, including

non-differentiable functions.

V. PRIMAL-DUAL PROXIMAL GRADIENT DESCENT

In this section, we introduce a proximal formulation of PDGD attack introduced in the previous section. Our primaldual proximal gradient attack (PDPGD) can be used to directly minimise any norm or function for which the proximal operator can be computed easily, including but not limited l-, l2-, and l1-norms, and l0-quasinorm.
First, we review some basics of proximal algorithms before introducing our attack. A detailed overview of proximal algorithms can be found in [39]. We define the proximal operator of the scaled function f where  > 0 as follows:

proxf (x) = arg min
u

1 f (u) +
2

u-x

2 2

(7)

The following useful relation holds true for any proximal operator of the proper closed function f :

proxf (x) + prox-1f (x/) = x

(8)

where f  is the convex conjugate of f . The equation above is known as Moreau decomposition. Moreau decomposition is useful for deriving the proximal operators of lp-norm functions. In particular, it implies that for any norm · :

prox · (x) + B(x/) = x

(9)

where B is a projection operator on the unit lp ball B. Let us revisit the r-player proxy-Lagrangian:

Lr(r, ) = 1 r + 2L (x + r, y)

(10)

where L is the surrogate loss for the non-differentiable

misclassification constraint. The goal of the r-player is to

minimise the proxy-Lagrangian function. However, if the

norm · is non-smooth, first-order subgradient descent needs

O(1/ 2) iterations to find -error local minimum. Moreover,

we cannot use first-order algorithms for l0-norm minimisation because the gradient of l0-norm is  almost everywhere. We

can address the above limitations in the framework of the

proximal optimisation.

First, we rewrite the proxy-Lagrangian for the r-player as

follows:

Lr(r, ) =  r + L(x + r, y)

(11)

where   R+ and is equal to 1/2; L is the surrogate loss, e.g. logistic or hinge loss.

Consider a quadratic approximation of the r-player proxyLagrangian at iteration k and point u:

L^(rk)(u,) =  u + L(x^(k), y)

+ L(x^(k), y)T

u - r(k)

1 +

u - r(k) 2

(12)

2t

2

where r(k) and x^(k) = x+r(k) are the adversarial perturbation and the adversarial example at iteration k, respectively. Note that we ignore not necessarily differentiable lp-norm penalty when applying a quadratic approximation.
We can find the perturbation at iteration k+1 by minimising the quadratic approximation above:

r(k+1) = arg min L^(rk)(u, )
u

1

=arg min  u + u -

u

2t

r(k) - tL(x^(k), y)

2 (13)
2

which is by the definition of the proximal operator in Equation (7) equivalent to:

r(k+1) = prox ,t r(k) - tL(x^(k))

(14)

where t is a step size or a learning rate. The algorithm above is known as proximal gradient. Proximal gradient descent has a convergence rate of O (1/T ) for non-smooth functions minimisation [39], which is faster than subgradient descent with a convergence rate of O (1/T ).
We list our primal-dual proximal gradient (PDPGD) attack in Algorithm 2. Compare to PDGD attack in Algorithm 1, PDPGD differs: 1) in line 3, it ignores non-smooth and not necessarily differentiable norm when computing gradient wrt primal variable r; 2) in line 5, it uses proximal gradient update instead of gradient update when updating primal variable r. PDPGD attack can be used to minimise any function with a closed-form proximal operator. In this paper, we limit our discussion to the minimisation of lp-norm perturbations. Next, we derive and list l-, l2-, l1-, and l0-norm proximal operators used for our proximal attack.

Algorithm 2 Primal-Dual Proximal Gradient Descent Attack

Input: Image x, label y, initial perturbation r(0), the total

number of iterations T , learning rate r and . Output: Adversarial perturbation r.

1: r  0

2: for k  1 to T do 3: Let (rk) be a gradient of L(x + r(k), y) where L is the

surrogate loss, e.g. logistic loss 4: Let (k) be a gradient of L(r(k), (k)) 5: Update r(k+1) = xC prox · ,r r(k) - r(rk)

6: Update (k+1) =  (k) + (k)

7: if k^(x + r(k+1)) = y and r(k+1)  r then

8:

r  r(k+1)

9: end if

10: end for

6

A. l-attack
l-norm of the vector x returns the largest absolute element of the vector x: l(x) = max |x|. Using Moreau decomposition in Equation (9), we can show that:

prox ·  (x) = x - proj{ · 11}(x/)

(15)

where proj is a projection operator. l1-norm projection can be computed efficiently in O (n log n) time [30].

B. l2-attack
Using Moreau decomposition in Equation (9), we can derive the proximal operator of the l2-norm as follows:

prox

·

(x) = (1 - / x
2

2)+x

(16)

This operator is known as block soft thresholding operator.

C. l1-attack
l1-norm proximal operator is well-known in signal processing [27]. It is defined as follows:

prox

·

(x) = T(x)
1

(17)

where T(x) = sign(x)(x - )+ is soft-thresholding.

D. l0-attack
l0-norm is non-convex quasinorm. It measures the cardinality of the vector x. The proximal operator of l0-norm minimises the total number of non-zero elements in the vector x, and it is defined as follows:

prox · 0 (x) = H2(x)

(18)

where H(x) = I [x - ] x is a hard-thresholding operator. The goal of the attack for the multichannel images is to
minimise the number of non-zero pixels. We define group l0,Gnorm of the vector x for the groups G = (g1, g2, . . . , gG) as the number of groups for which at least one element of the
group is non-zero:

G

x

G 0

=

I max |x|gi > 0

(19)

i=1

For RGB images, the group partition G naturally corresponds

to the pixels in the image. Then, we can derive the proximal

operator of l0,G-quasinorm as follows:

prox

·

(x)
0

=

HG2(x)

(20)

where HG(x) = I [max |xg| - ] x is a group hardthresholding operator which sets all elements in the group to
0 if the maximal element in the group is less than . Minimising l0-norm is NP-hard problem. We also exam-
ine lp-norm relaxation of the original l0-norm minimisation problem where 0 < p  1. We consider l1/2-, l2/3-, and l1norm relaxation of l0-norm because 1) it promotes sparsity of the solution; 2) its proximal operators can be computed in
a closed-form (see eq. (17) and [40]). For RGB images, we
apply lp-norm proximal operator to the pixel with the maximal value and set other channels to 0 if the maximal pixel is 0 after
applying the proximal operator.

VI. EXPERIMENTS
Models: We compare our attack to state-of-the-art attacks on MNIST, CIFAR-10 and Restricted ImageNet (R-ImageNet) datasets [41]. For each dataset, we consider a naturally trained model (plain) and l (l-AT) and l2 (l2-AT) adversarially trained models as in [16]. The models for MNIST and CIFAR10 dataset are available at https://github.com/fra31/fab-attack, while on R-ImageNet dataset we use models from [41], which can be downloaded from https://github.com/MadryLab/ robust-features-code.
The models on MNIST achieve the following clean accuracy on the test dataset (first 1000 test images): plain 99.17% (98.7%), l-AT 98.53% (98.5%), and l2-AT 98.95% (98.7%). The models on CIFAR-10 achieve the following clean test accuracy (first 1000 test images): plain 88.38% (89.4%), lAT 79.9% (80.4%), and l2-AT 80.44% (80.7%). The models on R-ImageNet achieve the following clean validation accuracy (first 1000 images of the validation set): plain 94.5% (94.9%), l-AT 91.62% (91.5%), and l2-AT 91.68% (91.9%).
Attacks: We test the robustness of each model wrt to l-, l2-, l1-, and l0-norm adversaries. We compare the performance of our attacks to attacks representing state-of-the-art for each norm: Brendel & Bethge attack (B&B, l-, l2-, l1-, l0norms) [21]; Carlini-Wagner l2-attack (C&W, l2-norm) [13]; CornerSearch l0-attack (CS, l0-norm) [31]; Decoupled Direction and Norm l2-attack (DDN, l2-norm) [26]; DeepFool (DF, l-, and l2-norms) [12]; Distributionally Adversarial Attack (DAA, l-norm) [24]; Elastic-net attack (EAD, l1-norm) [20]; Fast Adaptive Boundary Attack (FAB, l-, l2-, l1-norms) [14]; Jacobian-based Saliency Map attack (JSMA, l0-norm) [9]; One Pixel attack (Pixel, l0-norm) [8]; Projected Gradient Descent (PGD, l-, l2-, l1-, l0-norms) [10, 16, 29, 31]; Sparsefool (SF, l1- and l0-norms) [28]. We use B&B, C&W, DDN, and EAD attacks from Foolbox [42]; Pixel and JSMA attacks from ART [43]; PGD l-, l2-, and l1norm attacks as in Cleverhans [44]; CS, DF, FAB, and SF attacks with the code from the original papers, while we reimplemented DAA and PGD l0-norm attacks.
We conduct all our experiments using Tensorflow [45]. The code for the experiments to reproduce all our results is available at https://github.com/aam-at/cpgd. For a fair comparison, we perform a hyperparameter search for each attack, model and dataset. We report the results for the best configuration of parameters. For all attacks with multiple restarts, we find optimal parameters using 1 random restart. The parameters optimal for the attack with 1 random restart are used for the attack's experiments with multiple random restarts. Next, we present details about the parameters of all attacks.
Attack parameters:
· B&B [21] with 1000 iterations on MNIST and CIFAR10 and 1001 on R-ImageNet; initial learning rate selected from {1.0, 0.1, 0.01}; learning rate decay selected from every {20, 100} steps.
· C&W [13] with 9 binary search steps; 10000 iterations on MNIST and CIFAR-10, and 10001 on R-ImageNet;
1We reduce the number of iterations on R-ImageNet due to the attack's high computational cost.

7

learning rate 0.01; initial const 0.01, and no early stopping. · CS [31] with 1000 iterations; top-100 candidates for sampling; 784 and 1024 maximum sparsity on MNIST and CIFAR-10. We are unable to run CS on R-ImageNet due to the attack's high memory usage. · DDN [26] with 10000 iterations on MNIST, and 1000 on CIFAR-10 and R-ImageNet; initial epsilon selected from {1.0, 0.1}; gamma selected from {0.1, 0.05, 0.01}. · DF [12] with 100 iterations and 0.02 overshoot. · DAA [24] with Lagrangian Blob method; 500 iterations; epsilon step selected from { , /2, /5, /10, /25, /50, /100} for every epsilon ; surrogate loss selected from crossentropy and hinge losses. · EAD [20] with 9 binary search steps; 10000 iterations on MNIST and CIFAR-10, and 10001 on R-ImageNet; learning rate 0.01; initial const 0.01; beta 0.05; l1 decision rule, and no early stopping. · FAB [14] with 100 iterations. The remaining parameters are set to the values recommended in [14]. · JSMA [9] with gamma set to 1.0 and theta selected from {±0.1, ±1.0}. JSMA requires selecting the target. We attack all targets on MNIST and CIFAR-10. On RImageNet, we attack only the second-highest class due to the attack's high computational cost. · Pixel [8] with differential evolution strategy; 400 population size, and 100 iterations. We are unable to provide the results for Pixel attack on R-ImageNet due to its high computational cost. · PGD [10, 16, 31] with 500 iterations; epsilon step selected from { , /2, /5, /10, /25, /50, /100} for every epsilon ; surrogate loss selected from cross-entropy and hinge losses; optimal sparsity levels selected from {10%, 5%, 1%} (PGD-l1 only). · SF [28] with 20 iterations; epsilon 0.02; lambda incremented from 1 to 5, so the attack always succeeds.
Parameters for our attack: we set the number of iterations to 500, so the computational cost of our attack is similar to PGD attack with 500 iterations. Yet, the overall complexity of our attack is lower than PGD since PGD attack needs to be restarted for each threshold. For the primal variable r, we use Adam [37] and Proximal Adam [46] in PDGD and PDPGD attacks, respectively. For the dual variable , we perform gradient ascent in the log domain to guarantee that it remains positive. We apply an exponential moving average to smooth the value of the dual variable during optimisation. We select the learning rate for primal variables from {1.0, 0.1, 0.01} using 1 random restart. The optimal learning rate for the attack with 1 random restart is used in the experiments with 10 and 100 random restarts. Learning rate and initial value of the dual variable is set to 0.1 in all experiments. We exponentially and linearly decay the learning rate for the primal and dual variables to 0.01 and 0.1 of its initial value. We sample the initial perturbation from a uniform distribution U = [- , ] with set to 0.5 on MNIST, 0.25 on CIFAR-10, and 0.1 on R-ImageNet datasets. We finetune the perturbation found after N -restarts for an additional 500 iterations.

Evaluation metrics: We define the robust accuracy of the model at a threshold as the classification accuracy of the model if the adversary is allowed to perturb the input with the perturbation of lp-norm smaller than the threshold in order to change the model prediction. Given a perturbation budget , an adversarial attack aims to maximise the reduction of the model's accuracy. We fix five thresholds per model and per dataset and calculate the robust accuracy of each attack at five selected thresholds. We compare the attacks using the following statistics for each dataset: i) avg. rob. accuracy: the mean of the robust accuracies achieved by the attack over all models and thresholds (lower is better); ii) # best: how many times the attack achieves the lowest robust accuracy (it is the most effective); iii) avg. difference to best: for each model/threshold we compute the difference between the robust accuracy of the attack and the best one across all the attacks, then we average over all models/thresholds; iv) max difference to best: as "avg. difference to best", but with the maximum difference instead of the average one. In addition, we compare the average norm of the perturbations if the adversary is allowed to perturb the input without any perturbation bound (perturb only correctly classified inputs). Unbounded adversarial attack aims to minimise the perturbation budget while also achieving a high attack success rate. Please note that the comparison using the average norm of the adversarial perturbation is only available for the attacks that minimise the perturbation norm and excludes PGD and DAA attacks.
We compare the attack methods based on their computational complexity in Section VI-A. We test the effectiveness of the attacks on MNIST, CIFAR-10, and Restricted ImageNet datasets in Section VI-B. We summarise our main results in Tables II and III. We provide the complete results for all our experiments in supplementary materials, including detailed comparison of the proposed attack with PGD and FAB.
A. Runtime Comparison
It is difficult to compare the speed of various attack methods due to the differences in the per iteration runtime complexity, the number of iterations required for the attack to converge, and the attacks' implementation details. We perform a twofold comparison of various attack methods based on the theoretical per iteration runtime complexity and the attack's actual running time on the equivalent hardware.
1) Runtime Complexity: We measure the per iteration runtime complexity as the number of forward and backward passes per attack's iteration. Yet, counting only the number of the forward and backward passes is insufficient as attacks at each iteration may perform additional non-trivial operations. For example, B&B solves a second-order cone program at each iteration, significantly increasing the attack's running time. Our attack requires computing lp-norm proximity operator at an additional cost of O(d log d) for l-norm, O(d) for l1- and l0-norms proximity operators, where d is the input's dimensionality. We summarise the results of the comparison in Table I. Because the number of the model's parameters significantly larger than the input's dimensionality, the cost of computing proximity operator is negligible. So, the overall complexity of our attack is similar to PGD attack.

8

TABLE I RUNTIME COMPLEXITY COMPARISON OF ADVERSARIAL ATTACKS.

Attack B&B CS C&W DAA DDN DF EAD FAB JSMA Pixel PGD
SF
Our

# FW 1 1 1 1 1 1 1 2 1 1 1
1
1

# BP 1 - 1 1 1 k 1 k k - 1
-
1

Extra cost
Solve SOCP
-
- Compute pairwise distance matrix O(d2) l2-ball projection O(d) lp-ball projection O(d) l1-norm proximity operator O(d) lp-ball box projection O(d log d) -
- lp-ball projection: O(d) for l- and l2-norms; O(d log d) for l1- and l0-norms Compute DF and l1-norm projection onto hyperplane
lp-norm proximity operator: O(d log d) for l-norm, O(d) for l1- and l0-norms

2) Running Time: We report the running time in seconds on Nvidia Titan V averaged across all models for 1000 points on MNIST, CIFAR-10 and Restricted ImageNet. Unless otherwise stated, the running time includes all the restarts. For PGD, DAA and Pixel attacks, this is the time for evaluating robust accuracy at five thresholds. Note that when measuring the running time for these attacks, we exploit the fact that the inputs non-robust at the threshold are also non-robust at thresholds larger than . For the other attacks, a single attack is sufficient to compute the robust accuracy at all thresholds. MNIST: B&B 328s; CS 2611s; C&W 3758s; DAA-100 4680s; DDN 254s; DF 12s; EAD 4812s; FAB-100 2974s; JSMA 275s; Pixel 41126s; PGD-100 2135s for l/l2 and 2583 for l1/l0; SF 1301s; Our-10/Our-100 217s/1805s for l, 122s/965s for l2 and 176s/1484s for l1/l0. CIFAR-10: B&B 1327s; CS 23603s; C&W 15230s; DAA-100 29030s; DDN 165s; DF 13s; EAD 16222s; FAB-100 20590s; JSMA 1333s; Pixel 22928s; PGD-100 12267s for l/l2 and 12361s for l1/l0; SF 790s; Our-10/Our-100 1358s/12060s for l, 1157s/10054s for l2 and 1316s/11620s for l1/l0. Restricted ImageNet: B&B 10942s; C&W 36451s; DAA-10 87194s; DDN 3703s; DF 173s; EAD 46762s; FAB-10 33757s; JSMA 19551s; PGD-10 32671s for l/l2 and 53858s for l1/l0; SF 16367s; Our-1/Our-10 6818s/38719s for l, 5127s/29335s for l2 and 5644s/30586s for l1/l0.
The running time depends upon the number of attack's iterations. DF is the fastest attack requiring few iterations to succeed, but it is also the least accurate attack as its goal is to find adversarial perturbation as fast as possible without minimising its norm. The running time of our attack is comparable to existing attacks. PGD attack is fast and accurate with the complexity similar to our attack, but it needs to be restarted at each threshold, so the total running time of our attack is lower than PGD. FAB converges faster and requires relatively small number of iterations (100 in our experiments), but the complexity of each iteration is higher than our attack as it requires to compute k gradients at each step. We also report the running time of our lower complexity attack with a reduced number of restarts, which is the fastest when excluding less

accurate DF and SF attacks. As we show in the section, our lower complexity attack often outperforms state-of-the-art attacks.
B. Main Results We compare the attacks on the first 1000 images of MNIST
and CIFAR-10 test sets and 1000 images of Restricted ImageNet validation set. For each dataset, we evaluate robust accuracy at five thresholds for plain, l (l-AT) and l2 (l2AT) adversarially trained models. We use the same five thresholds that were selected in [14]. We show adversarial examples generated by our attack for 10 randomly selected MNIST test images in Figure 1. We report aggregated results for all datasets, models, norms and thresholds in Table II, while we provide complete results at each threshold in supplementary materials. Note that we are unable to run CornerSearch and Pixel attacks on R-ImageNet due to high memory usage and high computational cost of the attacks.
Our attack is the strongest as it most substantially reduces the robust accuracy compared with other attacks (see table II). In particular, it outperforms other attacks in 11 out of 12 cases in terms of the average robust accuracy. The second best attack varies depending upon the dataset and norm, which shows that our attack is accurate and general. PGD l-norm attack on Restricted ImageNet is the only attack to outperform our method. The improvement over the state-of-the-art attacks is
Fig. 1. l-, l2-, l1-, and l0-norm adversarial examples for a naturally trained, l-, and l2- adversarially trained models on MNIST.
(a) Original images
l
l2
l1
l0
(b) Adversarial examples for naturally trained model
l
l2
l1
l0
(c) Adversarial examples for l-AT model
l
l2
l1
l0
(d) Adversarial examples for l2-AT model

9

TABLE II PERFORMANCE SUMMARY (AGGREGATED) OF ALL ATTACKS ON MNIST, CIFAR-10 AND RESTRICTED IMAGENET IN TABLE IIIA, TABLE IIIB AND
TABLE IIIC, RESPECTIVELY. *NOTE THAT FOR OUR-10 THE "# BEST" IS COMPUTED EXCLUDING THE RESULTS OF OUR-100.

l-norm avg. rob. acc. # best avg. diff. to best max diff. to best

DeepFool 78.46 0 23.33 81.6

B&B 57.51
2 2.38 7.6

DAA-100 55.93 5 0.80 4.0

PGD-100 56.24 3 1.10 4.2

FAB-100 59.47 1 4.34 24.6

Our-10 56.00
8 0.87 3.9

Our-100 55.15 14 0.02 0.3

l2-norm avg. rob. acc. # best avg. diff. to best max diff. to best

DeepFool 67.4 0 34.35 91.9

C&W 48.13
1 15.09 63.9

DDN 42.81
2 9.76 53.3

B&B 39.57
2 6.52 30.9

PGD-100 45.65 1 12.6 66.3

FAB-100 34.66 4 1.61 5.0

Our-10 33.91 12 0.87 4.1

Our-100 33.05 14 0.01 0.1

l1-norm avg. rob. acc. # best avg. diff. to best max diff. to best

SparseFool 74.33 0 42.18 91.9

EAD 45.36
1 13.21 50.8

B&B 48.85
0 16.69 52.6

PGD-100 58.59 0 26.44 80.3

FAB-100 37.75 0 5.6 19.0

Our-10 34.19 12 2.03 8.2

Our-100 32.15 15 0.0 0.0

l0-norm avg. rob. acc. # best avg. diff. to best max diff. to best
l-norm avg. rob. acc. # best avg. diff. to best max diff. to best

SparseFool 80.43 0 38.83 92.5

JSMA 84.98
0 44.55 96.1

DeepFool 40.63 0 9.81 17.4

Pixel

B&B

77.03

58.37

0

0

36.61

17.94

94.3

57.0

(a) MNIST results

B&B 32.75
0 1.93 3.0

DAA-100 31.72 0 0.89 1.7

PGD-100 56.09 0 15.66 65.9

CornerSearch 49.99 2 9.57 36.0

Our-10 43.15
6 2.73 11.9

PGD-100 31.65 2 0.83 1.7

FAB-100 31.61 0 0.78 1.4

Our-10 31.24 13 0.41 1.1

Our-100 40.57 14 0.15 2.2
Our-100 30.83 14 0.01 0.1

l2-norm avg. rob. acc. # best avg. diff. to best max diff. to best

DeepFool 44.81 0 8.92 14.8

C&W 37.49
0 1.60 2.8

DDN 37.77
1 1.89 4.7

B&B 38.49
0 2.60 4.5

PGD-100 36.90 2 1.01 2.2

FAB-100 36.63 1 0.75 1.4

Our-10 36.09 14 0.20 0.6

Our-100 35.89 14 0.01 0.1

l1-norm avg. rob. acc. # best avg. diff. to best max diff. to best

SparseFool 40.53 0 22.17 35.9

EAD 20.76
0 2.40 4.9

B&B 22.79
0 4.43 9.1

PGD-100 26.46 0 8.10 13.8

FAB-100 21.49 0 3.13 6.5

Our-10 19.59 12 1.23 2.5

Our-100 18.36 15 0.00 0.0

l0-norm avg. rob. acc. # best avg. diff. to best max diff. to best
l-norm avg. rob. acc. # best avg. diff. to best max diff. to best

SparseFool 48.45 0 25.91 45.7

JSMA 69.15
0 46.61 68.6

DeepFool 36.99 0 9.72 18.5

Pixel

B&B

51.66

47.86

0

0

29.12

25.32

53.3

39.7

(b) CIFAR-10 results

B&B 29.26
0 1.99 4.5

DAA-10 27.99 2 0.72 1.8

PGD-100 35.23 0 12.69 16.4
PGD-10 27.39 7 0.11 0.5

CornerSearch 30.51 4 7.97 20.6

Our-10 27.33 10 4.79 13.0

FAB-10 28.50
0 1.23 2.6

Our-1 28.05
5 0.78 2.9

Our-100 25.00 11 2.46 11.5
Our-10 27.54
9 0.27 2.1

l2-norm avg. rob. acc. # best avg. diff. to best max diff. to best

DeepFool 45.80 0 14.67 28.4

C&W 42.25
0 11.12 43.9

DDN 32.87
0 1.74 5.1

B&B 36.17
0 5.03 10.0

PGD-10 32.70 1 1.57 5.0

FAB-10 34.57
0 3.44 8.0

Our-1 32.09
10 0.95 2.9

Our-10 31.21
14 0.08 1.2

l1-norm avg. rob. acc. # best avg. diff. to best max diff. to best

SparseFool 67.51 0 32.83 61.6

EAD 36.05
6 1.37 6.4

B&B 41.46
0 6.78 10.3

PGD-10 58.89 0 24.21 54.5

FAB-10 41.79
0 7.11 16.6

Our-1 38.19
2 3.51 8.2

Our-10 35.21
10 0.53 3.1

l0-norm avg. rob. acc. # best avg. diff. to best max diff. to best

SparseFool 49.18 0 20.43 40.1

JSMA 79.82
0 51.07 74.2

Pixel

B&B

PGD-10

-

46.28

46.56

-

0

0

-

17.53

17.81

-

25.9

36.0

(c) Restricted ImageNet results

CornerSearch -

Our-1 33.40
14 4.65 9.0

Our-10 28.75
15 0.00 0.0

10

TABLE III AVERAGE NORM OF THE PERTURBATION FOUND BY THE ATTACKS (WHEN
SUCCESSFUL, EXCLUDING THE ALREADY MISCLASSIFIED POINTS) FOR EVERY MODEL ON MNIST, CIFAR-10 AND RESTRICTED IMAGENET.

l × 10-1
plain l-at l2-at

DF B&B FAB-100 Our-10 Our-100

0.82 0.65

0.66 0.63 0.63

5.28 3.2

3.27 3.18 3.16

2.59 1.72

1.7 1.67 1.66

l2
plain l-at l2-at

DF C&W 1.13 1.01 5.03 2.08 3.08 2.34

DDN 1.0
1.71 2.29

B&B 1.01
1.4 2.35

FAB-100 0.99 1.12 2.25

Our-10 0.98 1.11 2.20

Our-100 0.98 1.06 2.18

l1
plain l-at l2-at

Sparsefool 8.71
207.70 16.52

EAD 6.21 6.73 11.96

B&B 6.59 6.32 13.97

FAB-100 6.04 3.48 12.15

Our-10 6.03 2.59 11.40

Our-100 5.87 2.32 10.93

l0
plain l-at l2-at

Sparsefool JSMA B&B

CS Our-10

12.45 13.76 7.66

8.74 7.01

249.06 59.30 11.29

3.85 4.27

21.35 26.61 15.14 17.28 12.28

(a) Average norm of the perturbation on MNIST

Our-100 6.76 3.82 11.72

l × 10-2
plain l-at l2-at

DF B&B FAB-100 Our-10 Our-100

0.77 0.57

0.56 0.55 0.54

3.13 2.5

2.37 2.34 2.32

2.59 1.98

1.94 1.90 1.88

l2 × 10-1 DF C&W
plain 2.72 2.13 l-at 9.42 7.28 l2-at 9.06 7.12

DDN 2.11 7.51 7.38

B&B 2.15 7.62 7.22

FAB-100 2.06 7.15 6.98

Our-10 2.04 6.99 6.85

Our-100 2.02 6.95 6.82

l1
plain l-at l2-at

Sparsefool 6.99 10.74 13.77

EAD 2.90 5.63 7.79

B&B 2.83 6.53 8.78

FAB-100 2.86 6.07 8.02

Our-10 2.67 5.38 7.50

Our-100 2.58 5.09 7.16

l0
plain l-at l2-at

Sparsefool 17.73 9.09 27.46

JSMA 21.29 25.03 26.13

B&B 6.80 9.60 9.78

CS Our-10 Our-100 3.98 3.16 2.86 5.76 4.38 3.99 6.50 5.07 4.60

(b) Average norm of the perturbation on CIFAR-10

l × 10-2
plain l-at l2-at

DF B&B FAB-10 Our-1 Our-10 0.25 0.19 0.19 0.19 0.18 2.16 1.75 1.71 1.68 1.67 1.87 1.58 1.54 1.53 1.51

l2
plain l-at l2-at

DF C&W 0.54 0.75 3.43 2.34 4.53 3.65

DDN 0.40 2.28 3.56

B&B 0.42 2.41 3.78

FAB-10 0.41 2.35 3.67

Our-1 0.39 2.17 3.56

Our-10 0.38 2.10 3.54

l1
plain l-at l2-at

Sparsefool 87.56 138.35 374.02

EAD 15.61 40.42 166.27

B&B 16.67 52.48 188.17

FAB-10 21.25 52.78 179.35

Our-1 15.65 53.14 169.74

Our-10 13.48 47.87 165.71

l0
plain l-at l2-at

Sparsefool JSMA B&B

CS Our-1 Our-10

52.06 211.77 33.60

- 22.96 19.21

80.33 372.33 62.14

- 53.66 41.84

131.27 629.18 191.86

- 123.90 117.80

(c) Average norm of the perturbation on Restricted ImageNet

most significant for our l1- and l0-norm attacks. For example, our l1- and l0-norm attack reduces the average robust accuracy by 17.4% and 23.2% on MNIST against FAB-100 and CornerSearch attacks, respectively. Our fast lower complexity attack with 10 random restarts on MNIST and CIFAR-10 and 1 random restart on Restricted ImageNet outperforms other attacks in 9 out of 12 cases.

Robust accuracy measures the model's robustness at a specific threshold . Robust norm gives a full picture of the model's robustness as a continuous function of the perturbation's size . In Table III, we report the average lp-norm of the adversarial perturbations found by the attacks (when successful) for every dataset, model and norm. We exclude the points that the models already misclassify. All attacks, except DF attack on MNIST dataset against l-AT model, have a 100% success rate, so we do not include the attack's success rate in the table results.
Our attack finds the smallest norm adversarial perturbation in 35 out of 36 cases (see table III). The improvements for l1- and l0-norm minimisation are particularly significant. For example, our l1-norm attack reduces the average robust norm of l-AT model on MNIST by 50% compared to the secondbest attack FAB-100 (a reduction from 3.48 to 2.32). Our l0norm attack reduces the average robust norm of l2-AT model on MNIST by 29.2% compared to the second-best attack B&B (reduction from 15.14 to 11.72). EAD l1-attack against l-AT model on Restricted ImageNet is the only attack to outperform our method when comparing the average robust l1-norm (an increase from 40.42 to 47.87). We also report the results of our attack with the reduced number of random restarts. Our lower complexity attack with 10 random restarts outperforms all other attacks in 23 out of 24 cases on MNIST and CIFAR10 datasets. Our-1 outperforms all other attacks in 9 out of 12 cases on R-ImageNet dataset.
To summarize, our main results are:
· Our l-norm attack reduces the average robust norm / robust accuracy by 1.4%/1.5% on MNIST, 2.2%/2.5% on CIFAR-10 and 2.2%/-0.5% on R-ImageNet compared to the second-best attack. PGD-10 on R-ImageNet is the only attack to outperform our attack in terms of the average robust accuracy.
· Our l2-norm attack reduces the average robust norm / robust accuracy by 3.8%/4.9% on MNIST, 2.5%/2.0% on CIFAR-10 and 4.2%/4.8% on R-ImageNet compared to the second-best attack.
· Our l1-norm attack reduces the average robust norm / robust accuracy by 21.7%/17.4% on MNIST, 9.7%/13% on CIFAR-10 and 0.2%/2.4% on R-ImageNet compared to the second-best attack. EAD on R-ImageNet against l-AT is the only attack to outperform our attack in terms of the average robust norm.
· Our l0-norm attack reduces the average robust norm / robust accuracy by 14.5%/23.2% on MNIST, 41.6%/22.0% on CIFAR-10 and 45.0%/61.0% on R-ImageNet compared to the second-best attack.
Overall, our attack is the best attack to reduce the robust accuracy and the robust norm compared to state-of-the-art attacks with a similar computational budget. Our attack is fast, accurate and general as it works for all lp-norms in p  {0, 1, 2, }. It outperforms all algorithms, including even one that is specialised in individual norms. Our lower complexity attack with the reduced number of restarts is the second-best attack and requires a fraction of the running time / computational cost.

11

VII. CONCLUSION
Fast and accurate estimation of the robust norm and robust accuracy of deep neural networks is crucial for comparing models. However, evaluating the DNNs robustness has proven to be challenging. The original non-convex constrained norm minimisation problem is difficult to solve. In this work, we introduce an adversarial attack that efficiently solves the original attack's problem. We interpret optimising the Lagrangian of the adversarial attack as playing a two-player game. The first player minimises the Lagrangian wrt the adversarial noise; the second player maximises the Lagrangian wrt the regularisation penalty, which penalises the first player for violating the misclassification constraint. We apply a primal-dual gradient descent algorithm to simultaneously update primal and dual variables to find the minimal optimal adversarial perturbation. For non-smooth lp-norm minimisation, such as l-, l1-, and l0-norms, we introduce primal-dual proximal gradient descent attack. We also derive group l0,G-norm proximal operator, which we use to minimise the number of perturbed pixels. Our method is fast, accurate and general. In the experiments on MNIST, CIFAR-10 and Restricted ImageNet, we show that our attack outperforms state-of-the-art l-, l2-, l1- and l0-norm attacks in terms of robust norm and robust accuracy in 35 out 36 and 11 out of 12 cases, respectively. In future work, we plan to extend the proposed attack to multiple norm perturbations and combine our attack with adversarial training defence.
REFERENCES
[1] K. He, X. Zhang, S. Ren, and J. Sun, "Deep residual learning for image recognition," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun 2016.
[2] S. Ren, K. He, R. Girshick, and J. Sun, "Faster r-CNN: Towards real-time object detection with region proposal networks," in Proc. Adv. Neural Inf. Process. Syst. (NIPS), 2015.
[3] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, and B. Kingsbury, "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups," IEEE Signal Proc. Mag., vol. 29, no. 6, pp. 82­97, Nov 2012.
[4] I. Sutskever, O. Vinyals, and Q. V. Le, "Sequence to sequence learning with neural networks," in Proc. Adv. Neural Inf. Process. Syst. (NIPS), 2014.
[5] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus, "Intriguing properties of neural networks," in Proc. Int. Conf. Learn. Represent. (ICLR), 2014.
[6] B. Liang, H. Li, M. Su, P. Bian, X. Li, and W. Shi, "Deep text classification can be fooled," in Proc. Int. Joint Conf. Artif. Intell. (IJCAI), 2018.
[7] N. Carlini and D. Wagner, "Audio adversarial examples: Targeted attacks on speech-to-text," in Proc. IEEE Secur. Privacy Workshops (SPW), 2018.
[8] J. Su, D. V. Vargas, and K. Sakurai, "One pixel attack for fooling deep neural networks," IEEE Trans. Evol. Comput., vol. 23, no. 5, pp. 828­841, 2019.

[9] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swami, "The limitations of deep learning in adversarial settings," in Proc. IEEE Eur. Symp. Secur. Privacy (EuroS&P), Mar 2016.
[10] A. Kurakin, I. J. Goodfellow, and S. Bengio, "Adversarial examples in the physical world," in Proc. Int. Conf. Learn. Represent. Workshop (ICLRW), 2017.
[11] K. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rahmati, C. Xiao, A. Prakash, T. Kohno, and D. Song, "Robust physical-world attacks on deep learning visual classification," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun 2018.
[12] S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard, "DeepFool: A simple and accurate method to fool deep neural networks," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun 2016.
[13] N. Carlini and D. Wagner, "Towards evaluating the robustness of neural networks," in Proc. IEEE Symp. Secur. Privacy (SP), May 2017.
[14] F. Croce and M. Hein, "Minimally distorted adversarial examples with a fast adaptive boundary attack," in Proc. Int. Conf. Mach. Learn. (ICML), Jul 2020.
[15] I. J. Goodfellow, J. Shlens, and C. Szegedy, "Explaining and harnessing adversarial examples," in Proc. Int. Conf. Learn. Represent. (ICLR), 2015.
[16] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, "Towards deep learning models resistant to adversarial attacks," in Proc. Int. Conf. Learn. Represent. (ICLR), 2018.
[17] A. Matyasko and L. P. Chau, "Margin maximization for robust classification using deep learning," in Proc. Int. Joint Conf. Neural Netw. (IJCNN), May 2017.
[18] G. Elsayed, S. Shankar, B. Cheung, N. Papernot, A. Kurakin, I. Goodfellow, and J. Sohl-Dickstein, "Adversarial examples that fool both computer vision and time-limited humans," in Proc. Adv. Neural Inf. Process. Syst. (NeurIPS), 2018.
[19] N. Carlini, A. Athalye, N. Papernot, W. Brendel, J. Rauber, D. Tsipras, I. Goodfellow, A. Madry, and A. Kurakin, "On Evaluating Adversarial Robustness," Feb 2019. See arXiv:1902.06705.
[20] P. Chen, Y. Sharma, H. Zhang, J. Yi, and C. Hsieh, "EAD: elastic-net attacks to deep neural networks via adversarial examples," in Proc. Conf. Artif. Intell. (AAAI), Feb 2018.
[21] W. Brendel, J. Rauber, M. Ku¨mmerer, I. Ustyuzhaninov, and M. Bethge, "Accurate, reliable and fast robustness evaluation," in Proc. Adv. Neural Inf. Process. Syst. (NeurIPS), 2019.
[22] X. Yuan, P. He, Q. Zhu, and X. Li, "Adversarial examples: Attacks and defenses for deep learning," IEEE Trans. Neural Netw. Learn. Syst., vol. 30, no. 9, pp. 2805­2824, Sep 2019.
[23] Y. Dong, F. Liao, T. Pang, H. Su, J. Zhu, X. Hu, and J. Li, "Boosting adversarial attacks with momentum," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun 2018.
[24] T. Zheng, C. Chen, and K. Ren, "Distributionally

12

adversarial attack," in Proc. Conf. Artif. Intell. (AAAI), Feb 2019. [25] A.-A. Pooladian, C. Finlay, T. Hoheisel, and A. Oberman, "A principled approach for generating adversarial images under non-smooth dissimilarity metrics," in Proc. Int. Conf. Artif. Intell. Stats., Aug 2020. [26] J. Rony, L. G. Hafemann, L. S. Oliveira, I. Ben Ayed, R. Sabourin, and E. Granger, "Decoupling direction and norm for efficient gradient-based l2 adversarial attacks and defenses," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun 2019. [27] A. Beck and M. Teboulle, "A fast iterative shrinkagethresholding algorithm for linear inverse problems," SIAM J. Imag. Sci., vol. 2, no. 1, pp. 183­202, Jan. 2009. [28] A. Modas, S.-M. Moosavi-Dezfooli, and P. Frossard, "Sparsefool: A few pixels make a big difference," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun 2019. [29] F. Tramer and D. Boneh, "Adversarial training and robustness for multiple perturbations," in Proc. Adv. Neural Inf. Process. Syst. (NeurIPS), 2019. [30] J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra, "Efficient projections onto the l1-ball for learning in high dimensions," in Proc. Int. Conf. Mach. Learn. (ICML), 2008. [31] F. Croce and M. Hein, "Sparse and imperceivable adversarial attacks," in Proc. Int. Conf. Comput. Vis. (ICCV), 2019. [32] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, and A. Swami, "Practical black-box attacks against machine learning," in Proc. ACM Asia Conf. Comput. Commun. Secur. (ASIA CCS), 2017. [33] W. Brendel, J. Rauber, and M. Bethge, "Decision-based adversarial attacks: Reliable attacks against black-box machine learning models," in Proc. Int. Conf. Learn. Represent. (ICLR), 2018. [34] C. Guo, J. Gardner, Y. You, A. G. Wilson, and K. Weinberger, "Simple black-box adversarial attacks," in Proc. Int. Conf. Mach. Learn. (ICML), Jun 2019. [35] A. Athalye, N. Carlini, and D. Wagner, "Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples," in Proc. Int. Conf. Mach. Learn. (ICML), Jun 2018. [36] A. Cotter, H. Jiang, and K. Sridharan, "Two-player games for efficient non-convex constrained optimization," in Proc. Int. Conf. Alg. Learn. Theory (ALT), 2019. [37] D. P. Kingma and J. Ba, "Adam: A method for stochastic optimization," in Proc. Int. Conf. Learn. Represent. (ICLR), 2015. [38] S. Boyd and L. Vandenberghe, "Convex Optimization," Cambridge Univ. Press, 2004. [39] N. Parikh and S. Boyd, "Proximal algorithms," Found. Trends Optim., vol. 1, no. 3, p. 127­239, Jan. 2014. [40] F. Chen, L. Shen, and B. W. Suter, "Computing the proximity operator of the lp norm with 0 < p < 1," IET Signal Process., vol. 10, no. 5, pp. 557­565, 2016. [41] D. Tsipras, S. Santurkar, L. Engstrom, A. Turner, and

A. Madry, "Robustness may be at odds with accuracy," in Proc. Int. Conf. Learn. Represent. (ICLR), 2019. [42] J. Rauber, W. Brendel, and M. Bethge, "Foolbox: A python toolbox to benchmark the robustness of machine learning models," Jul 2017. See arXiv:1707.04131. [43] M.-I. Nicolae, M. Sinn, M. N. Tran, B. Buesser, A. Rawat, M. Wistuba, V. Zantedeschi, N. Baracaldo, B. Chen, H. Ludwig, I. Molloy, and B. Edwards, "Adversarial robustness toolbox v1.2.0," Jul 2018. See arXiv:1807.01069. [44] N. Papernot, F. Faghri, N. Carlini, I. Goodfellow, R. Feinman, A. Kurakin, C. Xie, Y. Sharma, T. Brown, A. Roy, A. Matyasko, V. Behzadan, K. Hambardzumyan, Z. Zhang, Y.-L. Juang, Z. Li, R. Sheatsley, A. Garg, J. Uesato, W. Gierke, Y. Dong, D. Berthelot, P. Hendricks, J. Rauber, and R. Long, "Technical report on the cleverhans v2.1.0 adversarial examples library," Oct 2016. See arXiv:1610.00768. [45] The Tensorflow Development Team, "TensorFlow: Large-scale Machine Learning on Heterogeneous Distributed Systems," Mar 2016. See arXiv:1603.04467. [46] P. Melchior, R. Joseph, and F. Moolekamp, "Proximal Adam: Robust Adaptive Update Scheme for Constrained Optimization," Oct 2019, See arXiv:1910.10094.

13

A. DETAILED EXPERIMENTAL RESULTS
Table IV. COMPARISON OF l , l2 , l1 , AND l0 -ATTACKS ON A NATURALLY TRAINED, l -, AND l2 - ADVERSARIALLY TRAINED MODELS ON MNIST.

l model

DF B&B DAA-1 DAA-10 DAA-100 PGD-1 PGD-10 PGD-100 FAB-1 FAB-10 FAB-100 Our-1 Our-10 Our-100

0.03 93.6 91.8 91.9

91.9

91.9 92.0 91.9

91.9 92.0 92.0

92.0 91.9 91.9

91.8

0.05 84.4 74.6 75.6

74.7

74.4 75.6 74.8

74.4 77.1 76.9

76.1 73.9 73.6

73.2

plain 0.07 65.0 40.3 42.8

41.1

40.1 42.5 40.3

39.7 44.8 43.3

42.7 39.4 38.1

37.9

0.09 38.5 13.1 15.0

13.3

12.4 15.1 13.1

12.2 16.0 15.0

14.6 12.5 11.1

10.5

0.11 16.7 2.0

3.0

2.4

2.2 2.7

2.2

2.0 3.4

3.1

2.4 2.1

1.4

1.3

0.2 95.2 94.2 94.1

94.0

93.7 94.7

93.9

93.7 94.6 94.3

93.9 94.2 93.7

93.7

0.25 94.7 91.8 92.2

91.2

91.1 93.1

91.9

91.2 93.4 91.9

91.7 92.0 91.5

91.1

l-at 0.3 93.9 89.1 88.7

87.4

0.325 92.1 63.3 63.6

58.9

87.0 91.4

88.5

57.4 73.4 62.9

87.6 91.3 89.5 59.0 86.5 83.4

88.8 89.1 88.4 87.3

81.3 65.3 60.0

56.7

0.35 89.5 14.6 13.4

9.4

7.9 26.8

14.0

10.8 50.2 30.9

24.7 17.0 11.8

7.9

0.05 96.7 96.3 96.4

96.3

96.3 96.3 96.3

96.3 96.4 96.3

96.3 96.4 96.3

96.3

0.1 93.8 90.3 90.7

90.1

90.0 90.5 90.2

90.0 90.8 90.5

90.4 90.2 90.0

89.7

l2-at 0.15 86.9 72.6 73.0

71.4

0.2 76.0 26.9 29.8

25.2

70.7 73.0 71.6 23.3 29.0 25.9

70.8 74.0 72.4 23.5 33.6 27.7

71.7 71.8 70.9

70.2

24.6 24.8 20.8

19.3

0.25 59.9 1.8

0.9

0.9

0.6 1.5

0.6

0.4 1.7

1.0

0.9 1.0 0.5

0.4

(a) l-attack

l2 model
plain l-at l2-at

DF
0.5 92.1 1 60.1 1.5 18.6 2 1.9 2.5 0.1 1 94.4 1.5 93.3 2 92.1 2.5 90.0 3 87.2 1 93.5 1.5 87.2 2 79.2 2.5 67.5 3 53.8

C&W
91.1 48.4
9.5 0.9 0.0 87.6 72.9 54.5 32.4 14.3 92.3 84.5 70.4 45.9 17.3

DDN
90.8 46.8
8.0 0.8 0.0 87.0 62.3 31.6 10.4 1.8 92.4 83.6 68.4 42.7 15.5

B&B
91.1 49.0
8.4 0.6 0.0 78.3 39.9 11.3 2.3 0.0 92.9 85.4 71.1 46.6 16.6

PGD-1
91.1 49.8 10.0 0.9
0.0 90.9 85.4 74.6 59.0 40.8 92.3 84.0 68.7 44.2 15.6

PGD-10 PGD-100

91.1

91.1

47.8

47.6

8.2

7.8

0.7

0.6

0.0

0.0

90.1

88.3

80.2

75.3

63.2

51.1

36.9

24.3

16.9

6.8

92.3

92.3

83.6

83.6

68.0

67.6

39.9

38.0

11.3

10.3

(b) l2-attack

FAB-1
91.1 48.4
8.4 0.7 0.0 83.7 45.9 15.3 4.0 2.0 92.3 84.2 70.2 46.5 19.0

FAB-10
91.1 47.8
8.2 0.6 0.0 71.7 19.6 2.4 0.1 0.0 92.3 83.8 69.1 41.7 14.2

FAB-100
91.1 47.2
7.8 0.6 0.0 64.7 12.3 0.7 0.0 0.0 92.2 83.7 68.2 39.6 11.8

Our-1
90.8 47.7
8.9 0.8 0.0 72.6 27.7 3.3 0.2 0.0 92.7 84.1 68.6 40.7 11.5

Our-10
90.8 45.3 6.4 0.3
0.0 63.8 13.0
0.8 0.0 0.0 92.3 83.6 67.2 36.6 8.6

Our-100
90.8 45.2 6.0 0.3
0.0 59.7 9.0 0.2 0.0
0.0 92.3 83.3 66.0 34.6
8.4

l1 model
plain l-at l2-at

2 4 6 8 10 2.5 5 7.5 10 12.5 5 8.75 12.5 16.25 20

SparseFool
95.0 86.8 70.6 50.4 32.7 94.5 92.5 92.0 91.4 90.8 91.7 80.2 65.3 49.3 31.8

EAD
93.3 75.5 45.9 25.0
9.7 88.0 54.4 31.2 17.0 10.3 89.8 71.3 42.4 19.1
7.5

B&B
94.2 79.7 51.8 28.6 12.6 84.2 56.2 29.7 14.1
6.7 92.7 78.9 58.4 30.6 14.3

PGD-1
94.3 81.0 58.4 36.1 16.0 93.4 87.6 81.6 77.3 71.3 90.7 75.5 60.5 46.4 23.7

PGD-10 PGD-100

94.1

94.0

78.0

76.9

50.6

48.1

30.2

28.0

13.2

12.3

93.1

92.8

85.8

83.9

75.6

71.6

67.4

59.8

56.9

46.8

90.3

90.2

73.8

72.8

56.8

54.4

39.0

32.3

18.3

14.9

(c) l1-attack

FAB-1
94.2 79.6 53.4 31.6 15.3 85.2 57.6 47.0 39.3 33.7 90.5 75.6 55.3 32.7 14.9

FAB-10
93.8 76.1 47.4 24.5 10.6 69.9 32.4 15.6
8.0 5.6 90.3 73.6 48.8 25.0 9.5

FAB-100
93.6 75.0 43.5 22.4
8.3 56.8 19.4
7.3 3.0 0.9 89.9 72.4 45.8 20.3 7.7

Our-1
94.2 79.7 55.2 32.7 16.6 65.3 18.2 4.9 0.9 0.1 90.5 74.1 50.9 24.8 10.0

Our-10
93.5 75.4 43.9 22.3 8.3 46.0 6.2 0.5 0.0 0.0 88.9 69.2 38.4 15.3 4.9

Our-100
93.3 73.7 41.3 20.5
6.8 37.8 3.6 0.1 0.0 0.0 88.5 66.7 33.7 12.7 3.6

l model

SparseFool JSMA Pixel B&B PGD-1 PGD-10 PGD-100 CornerSearch Our-1 Our-10 Our-100

1

98.4 98.7 97.7 97.7 97.8

97.6

97.3

97.1 97.5

97.1

97.0

3

95.3 96.6 91.6 90.9 92.4

90.1

89.4

89.0 90.3

88.7

88.2

plain 5

89.0 92.9 88.9 72.0 77.6

68.2

65.0

72.0 72.3 65.2

62.7

8

73.9 73.4 87.2 36.5 47.9

31.9

27.9

44.5 38.1 28.1

25.4

12

45.3 48.2 85.2 7.2 17.1

7.9

5.2

19.9 10.9

4.9

4.0

1

97.7 98.5 92.5 95.2 98.2

97.3

95.0

90.9 93.8

91.0

90.9

3

96.5 97.7 61.8 85.5 97.6

93.8

89.1

46.5 75.2

58.4

48.7

l-at 5 8

95.5 97.1 29.8 72.8 96.1

89.8

81.7

93.8 96.9 21.4 49.1 94.2

83.6

65.5

16.4 47.7 24.3

15.8

3.6 16.9

4.7

2.9

12

92.7 96.2 17.9 31.7 89.9

68.0

42.1

0.3

3.7

0.2

0.1

5

94.8 96.3 97.4 93.6 92.3

91.1

89.9

92.1 91.2 90.0

89.2

10

83.4 87.9 96.6 75.5 75.1

66.8

61.8

77.1 69.9

61.6

58.5

l2-at 15 20

68.7 80.4 96.4 45.0 47.1

29.2

23.3

50.5 64.1 96.0 16.8 21.2

9.9

6.8

56.1 35.7 24.5

20.1

29.7 13.7

6.8

4.3

25

31.0 49.8 95.1 6.0

8.5

3.1

1.3

14.7

4.0

1.8

0.8

(d) l0-attack

14

Table V. COMPARISON OF l -, l2 -, l1 -, AND l0 -ATTACKS ON A NATURALLY TRAINED, l -, AND l2 - ADVERSARIALLY TRAINED MODELS ON CIFAR-10.

l model

DF B&B DAA-1 DAA-10 DAA-100 PGD-1 PGD-10 PGD-100 FAB-1 FAB-10 FAB-100 Our-1 Our-10 Our-100

/1 255 63.0 57.1 56.9

56.0

56.0 56.1 55.6

55.5 56.5 55.9

55.8 56.4 56.1 55.6

/1.5 255 49.7 37.9

39.0

37.7

37.5 39.0 37.8

37.4 38.7 37.7

37.2 37.7 37.5

36.6

plain /2 255 37.8 23.0 23.3

22.5

22.1 23.3 22.4

22.0 22.8 22.4

21.4 22.0 21.5

20.4

/2.5 255 26.9 12.6

13.3

12.8

12.3 13.5 12.7

12.3 13.0 12.3

11.8 12.0 11.3

10.6

/3 255 19.2 6.0

7.2

6.5

5.8 7.1

6.2

6.1 6.7

6.0

5.5 5.9

5.1

4.6

/2 255 66.8 66.5 65.6

65.6

65.5 65.5 65.5

65.4 65.8 65.7

65.7 65.5 65.4

65.4

/4 255 53.0 50.3 49.5

49.1

48.9 49.4 49.2

48.9 49.2 49.1

48.9 49.4 48.9

48.5

l-at /6 255 42.7 36.9 35.4

34.8

/8 255 32.6 25.3 24.1

24.0

34.6 35.2 34.8 23.7 24.2 23.8

34.6 35.3 34.8 23.6 23.8 23.5

34.6 34.6 34.0

33.9

23.4 23.7 23.0

22.6

/10 255 24.3 16.1 14.7

14.4

13.9 14.7 14.4

14.0 15.2 14.6

14.5 14.3 13.7

13.2

/2 255 64.3 63.0 62.7

62.5

62.4 62.6 62.6

62.4 62.7 62.6

62.6 62.5 62.4

62.3

/4 255 49.1 45.1 44.6

44.4

44.4 44.6 44.4

44.3 44.4 44.2

44.2 44.1 43.7

43.7

l2-at /6 255 36.8 28.1 27.9

27.6

/8 255 25.8 15.0 14.7

14.3

27.2 27.9 27.5 14.2 14.6 14.2

27.2 27.1 26.8 13.9 14.9 14.1

26.8 27.2 26.2

25.9

13.8 13.9 13.2

12.7

/10 255 17.5 8.4

8.5

7.6

7.3 8.7

7.5

7.2 8.6

8.1

7.9 7.4

6.6

6.5

(a) l-attack

l2 model
plain l-at l2-at

DF
0.1 71.7 0.15 61.8 0.2 51.8 0.3 34.1 0.4 20.1 0.25 67.2 0.5 53.3 0.75 42.3
1 32.0 1.25 23.2 0.25 66.5 0.5 54.3 0.75 41.9
1 29.9 1.25 22.0

C&W
68.9 56.4 44.0 22.1
9.4 65.5 49.2 33.8 22.2 11.6 65.8 49.2 33.1 20.1 11.0

DDN
67.5 55.0 43.5 21.5
9.7 64.8 49.0 34.7 23.3 13.9 65.3 49.3 33.2 21.7 14.2

B&B
68.5 56.3 44.0 23.1
9.8 65.8 51.1 35.4 24.1 14.0 66.1 50.4 34.0 22.1 12.6

PGD-1
67.9 55.8 43.8 22.5 10.2 64.7 49.2 34.0 22.2 12.3 65.4 49.4 33.4 20.6 11.5

PGD-10 PGD-100

67.6

67.5

55.1

54.8

43.2

43.0

21.9

21.5

9.5

9.0

64.7

64.7

49.1

48.9

33.8

33.5

21.6

21.2

11.6

11.5

65.4

65.3

49.1

49.1

33.1

32.7

20.3

20.1

11.0

10.7

(b) l2-attack

FAB-1
68.1 55.7 43.4 21.2
9.7 64.9 49.3 34.0 21.8 12.3 65.4 49.6 33.1 20.7 11.4

FAB-10
68.1 55.2 42.8 20.1
8.5 64.8 49.0 33.4 21.2 11.5 65.4 49.2 32.8 20.0 11.1

FAB-100
67.9 55.0 42.4 20.0
8.3 64.7 49.0 33.3 20.8 11.2 65.4 48.9 32.4 19.7 10.5

Our-1
67.5 54.6 42.6 20.5
8.6 64.7 48.8 33.1 20.2 10.6 65.5 48.8 32.3 19.1 10.3

Our-10
67.5 53.7 41.9 19.8
7.9 64.7 48.6 32.8 19.9 10.1 65.4 48.5 31.9 18.8 9.8

Our-100
67.4 53.6 41.3 19.3
7.4 64.7 48.5 32.7 19.6 9.9 65.4 48.5 31.8 18.8 9.5

l1 model
plain l-at l2-at

2 4 6 8 10 5 8.75 12.5 16.25 20 3 6 9 12 15

SparseFool
70.0 54.4 40.9 30.0 20.9 53.3 38.1 27.2 19.2 12.5 67.3 58.3 47.6 37.5 30.7

EAD
53.2 23.1
8.0 2.1 0.6 36.1 18.3 7.0 2.3 0.5 62.5 44.5 27.2 16.7 9.3

B&B
51.7 22.5
7.2 1.9 0.4 41.0 22.9 10.9 4.6 1.6 63.7 47.0 32.1 20.9 13.4

PGD-1
54.8 25.8 12.1
6.7 3.6 46.6 30.3 21.3 15.1 12.4 65.1 50.4 37.3 30.7 24.3

PGD-10 PGD-100

53.3

52.8

24.6

23.8

10.1

9.5

5.5

4.5

2.7

2.4

45.5

43.8

28.6

27.6

18.5

17.2

12.9

11.8

10.7

10.0

64.9

64.8

49.3

48.7

35.4

34.4

27.9

26.5

21.5

19.1

(c) l1-attack

FAB-1
56.1 31.3 17.0
7.7 4.6 43.4 26.0 14.8 7.1 3.9 63.4 49.0 33.5 23.7 16.5

FAB-10
52.0 25.0 10.1
3.3 1.4 39.7 22.2 11.1 4.7 2.0 63.1 47.2 30.7 20.2 12.6

FAB-100
50.4 22.7
8.0 2.5 1.1 38.3 20.3 8.7 3.5 1.2 63.0 45.8 28.8 17.6 10.5

Our-1
51.8 23.2
8.1 2.1 0.9 39.1 21.4 9.8 4.9 1.9 63.5 47.3 31.5 20.2 13.2

Our-10
50.0 20.5 6.0 1.3 0.7 34.5 15.6 6.2 2.3 0.8 62.2 42.3 26.5 15.5 9.4

Our-100
48.3 19.0 5.0 1.1 0.2 32.0 13.8 5.4 1.9 0.3 61.7 41.0 24.3 14.2 7.2

l model

SparseFool JSMA Pixel B&B PGD-1 PGD-10 PGD-100 CornerSearch Our-1 Our-10 Our-100

1

81.3 88.6 71.3 83.1 82.2

75.6

72.2

56.5 72.8

69.3

67.2

3

67.3 82.6 43.1 64.7 70.6

54.8

43.1

33.0 39.6

31.6

26.7

plain 5

52.8 75.7 35.8 46.8 57.5

36.1

23.5

27.7 17.5

10.6

7.1

8

33.3 65.2 36.2 25.5 37.0

17.8

9.9

15.1

4.1

2.2

1.1

12

16.7 53.3 33.2 8.1 19.8

8.3

4.5

4.9

0.5

0.2

0.0

1

70.3 80.3 68.2 76.3 74.5

72.1

69.4

54.6 70.2

67.6

66.0

3

57.4 75.3 49.5 64.0 62.8

51.0

43.8

36.0 45.5 39.9

35.9

l-at 5 8

46.4 68.7 46.2 49.5 51.7

38.3

32.7

34.1 60.1 47.7 35.1 39.0

27.1

21.3

32.9 30.0

21.7

18.6

20.4 14.3

8.1

6.1

12

22.7 51.6 47.4 19.8 30.4

20.7

14.9

10.9

6.0

3.0

1.7

1

72.8 79.6 71.2 77.0 75.0

73.2

71.2

58.0 71.9

70.3

69.5

3

64.1 75.4 58.5 66.8 66.1

57.2

52.4

41.5 54.0

48.5

44.8

l2-at

5

10

55.2 71.4 56.0 56.4 55.0

42.9

37.3

33.7 59.7 56.6 30.4 38.7

26.3

19.4

38.8 35.0

29.5

25.4

17.6 10.9

5.9

4.2

15

18.7 49.7 54.0 14.4 27.8

18.0

12.8

9.7

2.8

1.5

0.7

(d) l0-attack

15

Table VI. COMPARISON OF l -, l2 -, l1 -, AND l0 -ATTACKS ON A NATURALLY TRAINED, l -, AND l2 - ADVERSARIALLY TRAINED MODELS ON RESTRICTED IMAGENET.

l model
plain
l-at
l2-at

/ 0.25 255 /0.5 255 / 0.75 255 /1 255 / 1.25 255 /2 255 /4 255 /6 255 /8 255 /10 255 /2 255 /4 255 /6 255 /8 255 /10 255

DF
81.0 55.1 30.8 14.0 6.6 77.0 54.9 34.4 19.5 11.1 75.4 49.1 27.5 12.6 5.9

B&B
76.6 41.0 14.1
3.9 0.9 76.5 50.3 23.5 7.8 2.1 74.6 43.6 16.9 5.5 1.6

DAA-1
76.5 40.3 14.1
4.1 0.9 75.4 47.3 21.1 7.1 1.7 73.3 39.2 14.9 4.4 0.8

DAA-10 PGD-1

76.5 76.6

40.1 39.1

13.9 12.8

3.9

3.6

0.9

0.7

75.4 75.4

47.3 47.3

20.9 19.4

7.1

6.2

1.6

1.2

73.3 73.3

39.2 39.1

14.8 13.0

4.3

3.6

0.7

0.7

(a) l-attack

PGD-10
76.5 38.8 12.3
3.5 0.7 75.4 47.3 19.4 6.1 1.1 73.3 39.1 13.0 3.6 0.7

FAB-1
77.1 41.4 15.2
4.0 1.3 76.3 49.0 22.0 7.3 1.8 74.4 40.8 14.3 4.0 1.3

FAB-10
77.1 41.1 14.9
3.9 1.0 76.2 48.9 21.7 7.2 1.6 74.3 40.5 13.9 3.9 1.3

Our-1
76.5 39.9 13.6
3.9 0.7 75.4 48.3 20.8 6.2 1.3 73.2 42.0 14.4 3.9 0.7

Our-10
76.4 38.5 12.4 3.0
0.6 75.4 47.4 20.1 5.8 1.3 73.1 41.2 13.8 3.5 0.6

l2 model
plain l-at l2-at

DF
0.2 81.9 0.4 57.9 0.6 34.3 0.8 18.0 1 9.3 1 80.4 2 65.3 3 47.0 4 31.8 5 19.6 2 74.7 3 60.9 4 46.4 5 34.8 6 24.7

C&W
83.8 70.4 55.9 35.3 26.4 79.1 50.2 24.0
9.8 3.5 72.3 54.1 36.1 21.8 11.1

DDN
78.5 40.9 14.4
4.5 0.9 78.4 48.5 23.4 9.9 3.8 72.2 53.4 33.3 20.4 10.6

B&B PGD-1 PGD-10

79.1 78.6

78.4

44.9 42.5

42.1

18.1 15.6

15.1

6.8

4.6

4.6

2.8

1.1

1.1

79.5 77.6

77.6

52.9 49.3

48.4

28.6 23.7

22.6

12.5

9.8

8.9

5.6

3.5

3.2

74.4 72.0

72.0

58.5 53.6

53.3

39.3 33.0

33.0

25.0 20.2

20.0

14.5 10.4

10.2

(b) l2-attack

FAB-1
79.3 44.4 18.2
6.2 1.3 78.4 50.8 25.4 11.0 3.9 73.0 54.6 36.5 22.6 11.9

FAB-10
79.3 44.3 18.2
5.9 1.2 78.5 51.4 26.1 11.7 4.4 73.0 54.6 36.2 22.0 11.8

Our-1
77.9 40.0 14.6 4.0 0.9 77.1 46.3 19.6 7.6 2.0 72.2 54.1 34.9 19.8 10.3

Our-10
77.4 39.3 12.0 3.3 0.8 76.8 43.4 18.6 6.2 1.9 71.7 53.1 34.2 19.8
9.7

l1 model

SparseFool EAD B&B PGD-1 PGD-10 FAB-1 FAB-10 Our-1 Our-10

5

89.7 78.2 80.9 90.4

90.4 83.0

80.0 80.6

76.2

16

80.6 36.7 39.8 65.0

64.5 53.8

45.5 37.9

30.3

plain 27

70.6 13.2 17.9 30.6

29.7 34.4

25.6 14.2

9.0

38

61.6 5.2 5.9 12.2

10.9 22.2

13.7

4.5

2.2

49

52.9 2.2 2.1

4.8

3.7 15.4

7.7

1.5

0.9

15

79.8 66.6 71.4 90.9

88.7 72.8

69.6 67.2

63.6

25

74.1 50.4 59.8 86.7

86.6 61.0

56.4 54.3 50.5

l-at 40 60

64.5 34.0 44.1 82.6 52.6 19.8 30.1 74.5

82.6 48.6 74.3 36.0

42.8 39.8 35.8 29.4 28.0 22.9

100

39.2 7.4 14.2 46.5

45.3 19.5

13.6 13.2 10.1

50

86.2 79.0 81.3 87.4

87.4 80.8

80.5 79.1

78.6

100

78.1 59.4 65.4 79.2

79.1 64.6

62.2 60.1

59.2

l2-at 150 200

69.8 41.5 48.3 65.1 60.7 28.2 35.7 47.7

64.4 48.6 46.6 34.8

45.7 42.7 41.7

31.3 29.1

28.1

250

52.2 19.0 25.0 30.4

29.1 26.6

22.8 20.6

19.0

(c) l1-attack

l model

SparseFool JSMA B&B PGD-1 PGD-10 Our-1 Our-10

10

83.0 91.4 79.7 88.0

83.4 69.7

62.9

20

71.3 88.3 58.5 78.2

69.7 43.4

34.4

plain 30

57.9 85.0 40.8 66.6

53.8 25.2

17.8

40

46.1 82.1 27.3 56.5

41.6 12.9

8.5

50

37.9 78.8 16.3 40.4

25.4

6.9

4.6

10

78.9 88.0 79.5 85.0

81.0 70.2

65.4

30

64.2 83.0 57.0 68.9

59.4 45.4

37.3

l-at 50 80

51.4 78.8 40.7 51.2 34.5 71.6 23.4 34.1

40.6 28.8

21.6

23.8 18.7

12.7

100

26.9 68.1 17.7 25.8

16.2 14.3

10.3

50

75.9 87.8 80.7 78.9

75.2 69.4

66.2

100

52.6 83.5 65.6 62.0

52.6 42.4

39.7

l2-at 150 200

30.9 76.8 46.9 42.8 16.5 69.6 35.1 32.3

35.4 26.7

24.8

24.4 16.5

15.8

250

9.7 64.5 25.0 23.6

15.9 10.5

9.3

(d) l0-attack

16

B. ANALYSIS OF THE ATTACKS
In this section, we perform additional analysis of the proposed attack compared to PGD and FAB attacks. We illustrate how the attack's average robust norm and the attack's average robust accuracy changes as we increase the number of random restarts and the number of gradient queries, respectively.
First, we compare our attack with FAB attack, which has computational complexity similar to our attack (see Section VI.A of the main paper for details). We show the evolution of the average robust norm for our and FAB attacks in Figures 2 and 3. As we can see in figs. 2 and 3, our attack with 1 random restart outperforms FAB attack with 1 random restart in 14 out 18 cases. FAB attack requires the computation of k gradients in order to find the optimal target. The adversarial target for our attack depends on the initial random initialisation. Nonetheless, our l-norm attack with 1 random restart against naturally trained and l-AT models outperforms FAB attack with 100 random restarts on MNIST dataset. Our attack with 10 random restarts beats FAB attack with 100 random restarts for all models and norms on MNIST and CIFAR-10 datasets. Our attack on average reduces the robust l-norm by 3.5%/2.5%, l2-norm by 3.2%/2.5% and l1norm by 21.4%/14.1% on MNIST / CIFAR-10 datasets. The improvement over FAB is the most significant for l1-norm.

In the next experiment, we compare PGD and our attack without restarts as we increase the number of model's gradient queries in Figures 4 and 5 on MNIST and CIFAR-10 datasets, respectively. PGD attack allows to quickly estimate the robust accuracy at the specific threshold. To estimate the robust accuracy at 5 thresholds, we sequentially run 5 PGD attacks with an equal computational budget as we increase , so the total number of gradient queries on each clean image is the same for both attacks. We disable random initialisation for both attacks to reduce the variation due to the random starting point. For PGD, we also exploit the fact that the inputs non-robust at a threshold are non-robust for thresholds larger than . As we can see in figs. 4 and 5, our attack outperforms PGD attack for all norms and models on MNIST and CIFAR-10 datasets. Our attack has a "slow start` because it optimises primal and dual variables simultaneously. We can improve the convergence speed of our attack by changing the initial value C of the dual variable. Overall, our attack always outperforms PGD attack on MNIST and CIFAR-10 datasets given a sufficient computational budget.

Fig. 2. Evolution of the average robust norm for FAB and our attack on MNIST as we increase the number of restarts.

plain

l -AT

l2 -AT

0.067

Our-l FAB-l

0.340

0.335

Our-l 0.176 FAB-l 0.174

Our-l FAB-l

0.066

0.330

0.172

0.065

0.325

0.170

0.064

0.320

0.168

1

10

20

30

40

50

60

70

80

90 100

0.315 1

10

20

30

40

50

60

70

80

90 100

0.1661

10

20

30

40

50

60

70

80

90 100

1.03 1.02

OFAuBr--l2l2

1.4

1.01

1.3

1.00

0.99

1.2

OFAuBr--l2l2

2.350 2.325

2.300

2.275

2.250

2.225

OFAuBr--l2l2

0.98

1.1

2.200

1 10 20 30 40 50 60 70 80 90 100 1 10 20 30 40 50 60 70 80 90 100

1 10 20 30 40 50 60 70 80 90 100

7.0 6.8

OFAuBr--l1l1 20

OFAuBr--l1l1 13.5

OFAuBr--l1l1

6.6

15

13.0

12.5

6.4

10

12.0

6.2

6.0

5

11.5

11.0

1 10 20 30 40 50 60 70 80 90 100 1 10 20 30 40 50 60 70 80 90 100 1 10 20 30 40 50 60 70 80 90 100

17

Fig. 3. Evolution of the average robust norm for FAB and our attack on CIFAR-10 as we increase the number of restarts.

0.00570 0.00565 0.00560 0.00555

plain
Our-l 0.0242 FAB-l 0.0240
0.0238 0.0236

l -AT

Our-l 0.0198 FAB-l 0.0196
0.0194 0.0192

l2 -AT
Our-l FAB-l

0.00550

0.0234

0.0190

0.00545 1

10

20

30

40

50

60

70

80

90 100

0.0232 1

10

20

30

40

50

60

70

80

90 100

0.01881

10

20

30

40

50

60

70

80

90 100

0.212

OFAuBr--l2l2 0.73

0.210 0.72
0.208

0.206

0.71

0.204

0.70

OFAuBr--l2l2 0.71
0.70 0.69

OFAuBr--l2l2

1 10 20 30 40 50 60 70 80 90 100 1 10 20 30 40 50 60 70 80 90 100 1 10 20 30 40 50 60 70 80 90 100

3.6 3.4

OFAuBr--l1l1 7.0

OFAuBr--l1l1 9.0

OFAuBr--l1l1

3.2

6.5

8.5

3.0

6.0

8.0

2.8

5.5

7.5

2.6 1

10

20

30

40

50

60

70

80

90 100

5.01

10

20

30

40

50

60

70

80 90 100

1 10 20 30 40 50 60 70 80 90 100

18

Fig. 4. Evolution of the average robust accuracy for PGD and our attack with C = 0.1, C = 1, C = 10 on MNIST as we increase the number of gradient queries for each example (best viewed on-screen). We disable random initialisation for both attacks. We run 5 PGD attacks sequentially as we increase . For PGD, we exploit the fact that the inputs non-robust at a threshold are non-robust for thresholds larger than .

100 90 80 70
60
50 45
1 50 100

plain

PGD-l

100

Our-l with C=0.1

Our-l with C=1

Our-l with C=10

90

45.5

45.0

44.5 480

490

500

80

75

200

300

400

500 1 50 100

l -AT 100 90

PGD-l

80

Our-l with C=0.1

Our-l with C=1

Our-l with C=10

70

60

200

300

400

500 551 50 100

l2 -AT

PGD-l Our-l with C=0.1 Our-l with C=1 Our-l with C=10

200

300

400

500

100

PGD-l2

100

90 80

Our-l2 with C=0.1 Our-l2 with C=1

90 80

70

Our-l2 with C=10

70

60

60

50

30.0

50

29.5

40

40

480

490

500

100

90

PGD-l2

80

Our-l2 with C=0.1

Our-l2 with C=1

Our-l2 with C=10

70

PGD-l2 Our-l2 with C=0.1 Our-l2 with C=1 Our-l2 with C=10

30

30

60

25

251 50 100

200

300

400

500 1 50 100

200

300

400

500 551 50 100

200

300

400

500

100

PGD-l1

100

100

PGD-l1

90

Our-l1 with C=0.1 Our-l1 with C=1 Our-l1 with C=10

90 80
70

90

Our-l1 with C=0.1

Our-l1 with C=1

80

Our-l1 with C=10

80

60

PGD-l1

50

Our-l1 with C=0.1

70

70

Our-l1 with C=1

40

Our-l1 with C=10

60

60

30

55

25

50

1 50 100

200

300

400

500 1 50 100

200

300

400

500 451 50 100

200

300

400

500

100

PGD-l0

100

90

Our-l0 with C=0.1 Our-l0 with C=1

90

Our-l0 with C=10 80

80

62.5

70 62.0

70

480

490

500

100 90

80

PGD-l0

Our-l0 with C=0.1

70

Our-l0 with C=1

Our-l0 with C=10

60

PGD-l0 Our-l0 with C=0.1 Our-l0 with C=1 Our-l0 with C=10

47.75 47.50

47.25

480

490

500

65

60

50

55

45

1 50 100

200

300

400

500 1 50 100

200

300

400

500 1 50 100

200

300

400

500

19

Fig. 5. Evolution of the average robust accuracy for PGD and our attack with C = 0.1, C = 1, C = 10 on CIFAR-10 as we increase the number of gradient queries for each example (best viewed on-screen). We disable random initialisation for both attacks. We run 5 PGD attacks sequentially as we increase . For PGD, we exploit the fact that the inputs non-robust at a threshold are non-robust for thresholds larger than .

plain

l -AT

l2 -AT

100

100

100

90 80

PGD-l Our-l with C=0.1 90

PGD-l

90

Our-l with C=0.1 80

PGD-l Our-l with C=0.1

70

Our-l with C=1

80

Our-l with C=1

70

Our-l with C=1

60

Our-l with C=10

70

Our-l with C=10 60

Our-l with C=10

50 40

27.5

60

27.0480

490

500

50

37.75

50

37.50480

490

500

40

31.75 31.50 31.25

480

490

500

30

40

30

251 50 100

200

300

400

500 351 50 100

200

300

400

500 251 50 100

200

300

400

500

100

100

90

PGD-l2 Our-l2 with C=0.1

90

80

Our-l2 with C=1

80

70

Our-l2 with C=10

70

60

40.0

60

39.5

50

480

490

500

50

100

PGD-l2 Our-l2 with C=0.1

90

Our-l2 with C=1

80

Our-l2 with C=10

70

60

50

PGD-l2 Our-l2 with C=0.1 Our-l2 with C=1 Our-l2 with C=10

36.0

35.5

480

490

500

40

40

40

351 50 100

200

300

400

35 500 1 50 100

200

300

400

35 500 1 50 100

200

300

400

500

100

100

100

90 80

PGD-l1

90

Our-l1 with C=0.1 80

PGD-l1 Our-l1 with C=0.1

90

PGD-l1 Our-l1 with C=0.1

70

Our-l1 with C=1

70

Our-l1 with C=1

80

Our-l1 with C=1

60 50

Our-l1 with C=10

60

50

Our-l1 with C=10

70

Our-l1 with C=10

40

40

60

30

30

50

20

20

40

151 50 100

200

300

400

500 151 50 100

200

300

400

35 500 1 50 100

200

300

400

500

100

100

100

90 80 70

PGD-l0 Our-l0 with C=0.1

90

Our-l0 with C=1

80

PGD-l0 Our-l0 with C=0.1 90

Our-l0 with C=1

80

PGD-l0 Our-l0 with C=0.1 Our-l0 with C=1

60

Our-l0 with C=10

70

Our-l0 with C=10

70

Our-l0 with C=10

50

60

60

50

40

50

40

30

40

35

251 50 100

200

300

400

500 1 50 100

200

300

400

500 351 50 100

200

300

400

500

