arXiv:2106.00694v1 [cs.LG] 1 Jun 2021

Symmetry-via-Duality: Invariant Neural Network Densities from Parameter-Space Correlators
Anindita Maiti, Keegan Stoner, and James Halverson 
The NSF AI Institute for Artificial Intelligence and Fundamental Interactions
Department of Physics Northeastern University
Boston, MA 02115
{maiti.a, stoner.ke, j.halverson}@northeastern.edu
Abstract
Parameter-space and function-space provide two different duality frames in which to study neural networks. We demonstrate that symmetries of network densities may be determined via dual computations of network correlation functions, even when the density is unknown and the network is not equivariant. Symmetry-viaduality relies on invariance properties of the correlation functions, which stem from the choice of network parameter distributions. Input and output symmetries of neural network densities are determined, which recover known Gaussian process results in the infinite width limit. The mechanism may also be utilized to determine symmetries during training, when parameters are correlated, as well as symmetries of the Neural Tangent Kernel. We demonstrate that the amount of symmetry in the initialization density affects the accuracy of networks trained on Fashion-MNIST, and that symmetry breaking helps only when it is in the direction of ground truth.
1 Introduction
Many systems in Nature, mathematics, and deep learning are described by densities over functions. In physics, it is central in quantum field theory (QFT) via the Feynman path integral, whereas in deep learning it explicitly arises via a correspondence between infinite networks and Gaussian processes. More broadly, the density associated to a network architecture is itself of foundational importance. Though only a small collection of networks is trained in practice, due to computational limitations, a priori there is no reason to prefer one randomly initialized network over another (of the same architecture). In that case, ideally one would control the flow of the initialization density to the trained density, compute the trained mean µ(x), and use it to make predictions. Remarkably, µ(x) may be analytically computed for infinite networks trained via gradient flow or Bayesian inference [1­3]. In systems governed by densities over functions, observables are strongly constrained by symmetry, which is usually determined via experiments. Examples include the Standard Model of Particle Physics, which has gauge symmetry SU (3) × SU (2) × U (1) (possibly with a discrete quotient), as well as certain multi-dimensional Gaussian Processes. In the absence of good experimental data or an explicit form for the density, it seems difficult to deduce much about its symmetries.
Equal contributions by Maiti and Stoner.
Preprint. Under review.

We introduce a mechanism for determining the symmetries of a neural network density via duality, even for an unknown density. A physical system is said to exhibit a duality when it admits two different, but equally fundamental, descriptions, called duality frames. Hallmarks of duality include the utility of one frame in understanding a feature of the system that is difficult to understand in the other, as well as limits of the system where one description is more tractable than the other. In neural networks, one sharp duality is Parameter-Space / Function-Space duality: networks may be thought of as instantiations of a network architecture with fixed parameter densities, or alternatively as draws from a function space density. In GP limits where a discrete hyperparameter N   (e.g. the width), the number of parameters is infinite and the parameter description unwieldy, but the function space density is Gaussian and therefore tractable. Conversely, when N = 1, the function space density is generally non-perturbative due to large non-Gaussianities, yet the network has few parameters.
We demonstrate that symmetries of network densities may be determined via the invariance of correlation functions computed in parameter space. We call this mechanism symmetry-via-duality, and it is utilized to demonstrate numerous cases in which transformations of neural networks (or layers) at input or output leave the correlation functions invariant, implying the invariance of the functional density. It also has implications for learning, which we test experimentally. For a summary of our contributions and results, see Section (5).
Symmetries, Equivariance, and Invariant Generative Models Densities. Symmetries of neural networks are a major topic of study in recent years. Generalizing beyond mere invariance of networks, equivariant networks [4­20] have aided learning in a variety of contexts, including gauge-equivariant networks [21, 22] and their their utilization in generative models [23­26], for instance in applications to Lattice QCD [27, 28]. See also [29, 30] for symmetries and duality in ML and physics.
Of closest relation to our work is the appearance of symmetries in generative models, where invariance of a generative model density is often desired. It may be achieved via draws from a simple symmetric input density  on V and an equivariant network f : V  V , which ensures that the induced output density f is invariant. In Lattice QCD applications, this is used to ensure that gauge fields are sampled from the correct G-invariant density f , due to the G-equivariance of a trained network f.
In contrast, in our work it is the network f itself that is sampled from an invariant density over functions. That is, if one were to cast our work into a lattice field theory context, it is the networks themselves that are the fields, and symmetry arises from symmetries in the density over networks. Notably, nowhere in our paper do we utilize equivariance.
Modeling Densities for Non-Gaussian Processes. One motivation for understanding symmetries of network densities is that it constrains the modeling of neural network non-Gaussian process densities using techniques from QFT [31] (see also [32]), as well as exact non-Gaussian network priors [33] on individual inputs. Such finite-N densities arise for network architectures admitting a GP limit [2, 34­41] as N  , and they admit a perturbative description when N is large-but-finite. These functional symmetry considerations should also place constraints on NTK scaling laws [42] and preactivation distribution flows [43] studied in parameter space for large-but-finite N networks.

2 Symmetry Invariant Densities via Duality

Consider a neural network f with continuous learnable parameters . The architecture of f and parameter density P induce a density over functions Pf . Let Z and Zf be the associated partition functions. Expectation values may be computed using either P or Pf , denoted as E and Ef , respectively, or simply just E when we wish to be agnostic as to the computation technique.

The n-point correlation functions (or correlators) of neural network outputs are then

G(n)(x1, . . . , xn) = E[f (x1) . . . f (xn)],

(1)

and (if the corresponding densities are known) they may be computed in either parameter- or function-
space. These functions are the moments of the density over functions. When the output dimension D > 1, we may write output indices explicitly, e.g. fi(x), in which case the correlators are written G(i1n,)...,in (x1, . . . , xn).

Neural network symmetries are a focus of this work. Consider a continuous transformation

f (x) = (f (x )),

(2)

2

i.e. the transformed network f at x is a function  of the old network f at x . We say there is a
classical symmetry if Pf is invariant under the transformation, which in physics is usually phrased in terms of the action Sf = -log Pf . If the functional measure Df is also invariant, it is said that there is a quantum symmetry and the correlation functions are constrained by

G(n)(x1, . . . , xn) = E[f (x1) . . . f (xn)]

= E[(f (x1)) . . . (f (xn))] =: G (n)(x1, . . . , xn).

(3)

See appendix for the elementary proof. In physics, if x = x but  is non-trivial the symmetry is called internal, and if  is trivial but x = x it is called a spacetime symmetry. Instead, we will call them output and input symmetries to identify the part of the neural network that is transformed; examples include rotations of outputs and translations of inputs. Of course, if f is composed with other functions to form a larger neural network, then input and output refer to those of the layer f .

Our goal in this paper is to determine symmetries of network densities via the constraint 3. For a discussion of functional densities, see Appendix E.

A Glimpse of Symmetry from Gaussian Processes. Further study in this direction is motivated by first deriving a result for one of the simplest function-space densities: a Gaussian process.

Consider a neural network Gaussian Process (NNGP): a real-valued neural network f,N where N is a discrete hyperparameter such that in the asymptotic limit N  , f, is drawn from a Gaussian process. The simplest example is [2] a fully-connected single-layer network of width N . Suppressing , N subscripts and assuming the N   limit, a NNGP f can be stacked to obtain a vector-valued neural network fi : Rd  RD. The associated two-point function G(i12i)2 (x1, x2) = i1i2 K(x1, x2), K(x1, x2) is the NNGP kernel (2-pt function) associated to f ; i.e. stacking adds tensor structure to the kernel in the form of a Kronecker delta.
If the NNGP has zero mean, then G(2n+1)(x1, . . . , xn) = 0 for all n and the even-point functions may be computed in terms of the kernel via Wick's theorem,

G(i12,n..).,i2n (x1, . . . , x2n) =

ia1 ib1 . . . ian ibn K(xa1 , xb1 ) . . . K(xan , xbn )

(4)

P Wick(2n)

where the Wick contractions are defined by the set

Wick(n) = {P  Partitions(1, . . . , n) | |p| = 2 p  P }.

(5)

We write P  Wick(2n) as P = {(a1, b1), . . . , (an, bn)}. A network transformation fi  Rijfj induces an action on each index of each Kronecker delta in (4). For instance, as ik  RijRkljl = (R RT )ik = ik where the last equality holds for R  SO(D). By this phenomenon, the evenpoint correlation functions (4) are SO(D) invariant. Conversely, if the NNGP has a mean µ(x) = G1i1 (x1) = 0, it transforms with a single R and is not invariant. From the NNGP correlation functions, the GP density has SO(D) symmetry iff it has zero mean. This is not surprising, and could be shown
directly by inverting the kernel to get the GP density and then checking its symmetry.

However, we see correlation functions contain symmetry information, which becomes particularly powerful when the correlation functions are known, but the network density is not.

Parameter-Space / Function-Space Duality. To determine the symmetries of an unknown network density via correlation functions, we need a way to compute them. For this, we utilize duality.
A physical system is said to exhibit a duality if there are two different descriptions of the system, often with different degrees of freedom, that exhibit the same predictions either exactly (an exact duality) or in a limit, e.g., at long distances (an infrared duality); see [44] for a review. Duality is useful precisely when one perspective, a.k.a. a duality frame, allows you to determine something about the system that would be difficult from the other perspective. Examples in physics include electric-magnetic duality, which in some cases allows a strongly interacting theory of electrons to be reinterpreted in terms of a weakly coupled theory of monopoles [45], and gauge-gravity duality [46], which relates gravitational and non-gravitational quantum theories via the holographic principle.
In the context of neural networks, the relevant duality frames are provided by parameter-space and function-space, yielding a Parameter-Space / Function-Space duality. In the parameter frame, a neural

3

network is considered to be compositions of functions which themselves have parameters drawn
from P, whereas in the function frame, the neural network is considered as an entire function drawn from a function-space density Pf . Of course, the choice of network architecture and densities P determine Pf , but they do not appear explicitly in it, giving two different descriptions of the system.

Symmetry-via-Duality. Our central point is that symmetries of function-space densities may be determined from correlation functions computed in the parameter-space description, even if the function space density is not known. That is, it is possible to check (3) via correlators computed in parameter space; if so, then the product Df Pf is invariant. Barring an appearance of the GreenSchwarz mechanism [47] in neural networks, by which Df Pf is invariant but Df and Pf are not, this implies that Pf is invariant. This leads to our main result.
Theorem 1. Consider a neural network or layer

f : Rd  RD

(6)

with associated function space measure Df and density Pf , as well as a transformation f (x) = (f (x )) satisfying

E[f (x1) . . . f (xn)] = E[(f (x1)) . . . (f (xn))].

(7)

Then Df Pf is invariant, and Pf is itself invariant if a Green-Schwarz mechanism is not effective.

The proof of the theorem follows from the proof of (3) in the Appendix and the fact that correlators may also be computed in parameter space. Additionally, there may be multiple such transformations that generate a group G of invariant transformations, in which case Pf is G-invariant.
The schematic for each calculation is to transform the correlators by transforming some part of the network, such as the input or output, absorb the transformation into a transformation of parameters T   (which could be all ), and then show invariance of the correlation functions via invariance of PT . Thus,
Corollary 1.1. Symmetries of Pf derived via duality rely on symmetry properties of PT .

In what follows we will show that (7) holds in numerous well-studied neural networks for a variety of transformations, without requiring equivariance of the neural network. Throughout, we use Z to denote the parameter space partition function of all parameters  of the network.

Example: SO(D) Output Symmetry. We now demonstrate in detail that a linear output layer
leads to SO(D) invariant network densities provided that its weight and bias distributions are
invariant. The network is defined by fi(x) = Wijgj(x) + bi where i = 1, . . . , D and gj is an N dimension postactivation with parameters g. Consider an invertible matrix transformation R acting as as fi  Rijfj; we use Einstein summation convention here and throughout. The transformed correlation functions are

Gi(1n..).in (x1, . . . , xn) = E[Ri1j1 fj1 (x1) . . . Rinjn fjn (xn)]

1 =
Z

DW Db Dg Ri1j1 (Wj1k1 gk1 (x1) + bj1 ) . . . Rinjn (Wjnkn gkn (xn) + bjn )PW PbPg

1 =
Z

|R-1|2DW~ D~b Dg (W~ i1k1 gk1 (x1) + ~bi1 ) . . . (W~ inkn gkn (xn) + ~bin )PR-1W~ PR-1~bPg

= E[fi1 (x1) . . . fin (xn)] = G(n)(x1, . . . , xn),

(8)

where, e.g., DW denotes the standard measure for all W -parameters in the last layer, and the crucial

second-to-last equality stipulations hold in the

holds when |R| = well-studied case

1, of

WPW=NP(R0-, 1W~W 2

= ),

PW~ , W

and Pb = N (0, b2)

PR-1~b when

= R

P~b. S

These O(D),

due to the invariance of the bjbj in Pb = exp(-bjbj/2b2), and similarly for Gaussian PW .

The result holds more generally, for any invariant PW and Pb, which as we will discuss in Section 3 includes the case of correlated parameters, as is relevant for learning.

Example: SO(d) Input Symmetry. We now demonstrate an example of neural networks with density invariant under SO(d) input rotations, provided that the input layer parameters are drawn from an invariant distribution.

4

We will take a linear input layer and turn off the bias for simplicity, since it may be trivially included
as in the SO(D) output symmetry above. The network function is fi(x) = gij(Wjkxk), W  PW , and the input rotation R  SO(d) acts as xi  xi = Rijxj. The output of the input layer is the preactivation for the rest of the network g, which has parameters g. The transformed correlators are

Gi(1n..).in (x1, . . . , xn) = E[fi1 (Rk1l1 x1l1 ) . . . fin (Rknln xnln )]

1 =
Z

DW Dg gi1j1 (Wj1k1 Rk1l1 x1l1 ) . . . ginjn (Wjnkn Rknln xnln )PW Pg

1 =
Z

|R-1|DW~ Dg gi1j1 (W~ j1l1 x1l1 ) . . . ginjn (W~ jnln xnln )PR-1W~ Pg

= E[fi1 (x1) . . . fin (xn)] = G(n)(x1, . . . , xn),

(9)

where we have changed the x subscript label to a superscript to make room for indices, and again the
important second-to-last equality holds when PW is invariant under R  SO(D). This again holds for Wij  N (0, W 2 ), but also for any distribution PW constructed from SO(D) invariants. See [32] for SO(d) input symmetry of the NNGP kernel.

SU (D) Output Symmetry We also demonstrate that a linear complex-valued output layer, given
in details in Appendix (A.2), leads to SU (D) invariant networks densities provided that last linear
layer weight and bias distributions are invariant. For clarity we leave off the bias term; it may be
added trivially similar to Eqn. (8). This network is defined by fi = Wijgj(x, g), and transforms as fi  Sijfj, fk  flSlk under an invertible matrix transformation by SU (D) group element S. A necessary condition for symmetry is that the only non-zero correlation functions have an equal number of f and f 's, as in (32), which transform as

Gi(12··n·i)2n (x1, · · · , x2n) = E Si1j1 fj1 (xp1 ) · · · Sinjn fjn (xpn )fjn (xpn+1 )Sjn+1in+1 · · · fj2n (xp2n )Sj2ni2n

1 =
4Z

DWDWDgSi1j1 Wj1k1 gk1 (xp1 , g) · · · Sinjn Wjnkn gkn (xpn , g)

g
kn+1

(xpn+1

,

g

)Wkn+1

jn+1

S
jn+1 in+1

·

·

·

gk2n (xp2n , g )Wk2nj2n

Sj2ni2n PW,W Pg

1 =
4Z

|(S)-1||S-1|DW~ DW~ Dg W~ i1k1 gk1 (xp1 , g) · · · W~ inkn gkn (xpn , g)

g
kn+1

(xpn+1

,

g

)W~ kn+1

in+1

···

gk2n (xp2n , g )W~ k2ni2n P P S-1W~ ,W~ S-1 g

= E fi1 (xp1 ) · · · fin (xpn )fin+1 (xpn+1 ) · · · fi2n (xp2n ) = G(i12·n··)i2n (x1, · · · , x2n),

(10)

where {p1, · · · , p2n} is any permutation of {1, · · · , 2n} as in (A.2), and the crucial second-to-last equality holds when |S|-1 = 1, and PW,W = PW~ ,W~  . This stipulation holds true, for example, when S  SU (D), and Re(W), Im(W)  N (0, W 2 ), due to the SU (D) invariance of Tr(WW) in PW,W = exp(-Tr(WW)/2W 2 ). For more details, please see appendix (A.2).

Example: Translation Input Symmetry and T-layers. We now study architectures with translation (T ) invariant network densities, which arise when the correlation functions are invariant under input translations x  x + c, c  Rd, the usual notion of translations in physics. Our translations are more general than the pixel translations often studied in computer vision; for instance, in one dimension a pixel shift to the right is induced by the choice ci = -xi + x(i+1)%d.
To arrive at T -invariant network densities via correlation functions, we first define the T -layer, a standard linear layer with deterministic weight matrix W and uniform bias on the circle, bi  U(S1), where we also map the W x term to the circle by taking it mod 1 (written as % 1) before adding the already S1-valued bias. Since such a layer is defined by the hyperparameter weight matrix W , we label it TW , with parameters b. Suppressing obvious indices, we write the T -layer as TW (x) = (W x % 1) + b. Under translations of the input to the T -layer, we have TW (x + c) = (W x) % 1 + (W c) % 1 + b =: (W x) % 1 + b , where b and b are equally probable bias draws and the periodic boundary condition of the S1 solves problems that arise in the presence of boundaries of a uniform distribution on a subset of R.
The T -layer may be prepended to any neural network to arrive at a new one with T -invariant density. Consider any neural network g : RN  RD, to which we prepend TW : Rd  RN to form a

5

new network f(x) = g(TW (x)), where  = {, b}. The transformed correlation functions are translation invariant,

G(i1n,)...,in (x1 + c, . . . , xn + c) = G(i1n,)...,in (x1, . . . , xn)

n,

(11)

which follows by absorbing the shift via b = (W c % 1) + b, using db = db, and renaming variables. See appendix (A.4) for details. Thus, the density Pf is translation invariant.

The T -layer is compatible with training since it is differentiable everywhere except when the layer input is  0 mod 1, i.e. an integer. When doing gradient descent, the mod operation is treated as the identity, which gives the correct gradient at all non-integer inputs to the layer, and thus training
performs well on typical real-world datasets.

3 Symmetry-via-Duality and Deep Learning

We now various aspects of relating symmetry, deduced via duality, and learning.

Preserving Symmetry During Training, via Duality. It may sometimes be useful to preserve a
symmetry (deduced via duality) during training that is present in the network density at initialization.
Corollary 1.1 allows symmetry-via-duality to be utilized at any time, relying on the invariance properties of PT (and therefore P). The initialization symmetry is preserved if the invariance properties of P that ensured symmetry at t = 0 persist at all times.

While this interesting matter deserves a systematic study of its own, here we study it in the simple case of continuous time gradient descent. The parameter update di/dt = -L/i, where L is the loss function, induces a flow in P governed by

P(t) = t

 L i i

P (t)

+

 P (t) i

L ,
i

(12)

the update equation for P(t). If P(t) is invariant at initialization (t = 0), then the update is invariant provided that 2L/(i)2 is invariant and the second term is invariant.

When these conditions are satisfied, the symmetry of the network initialization density is preserved
throughout training. However, they must be checked on a case-by-case basis. As a simple example, consider again the SO(D) output symmetry from Section 2. Absorbing the action on output into parameters as before, the (/i)(/i) is itself invariant, and therefore the first term in (12) is invariant when L is invariant. If additionally

 P (t) i

=

IP

i

L i = IL i

(13)

for -dependent invariants IP and IL, then the second term is invariant as well, yielding an invariant symmetry-preserving update. See Appendix (A.6) for a detailed example realizing these conditions.

Supervised Learning, Symmetry Breaking, and the One-point Function. A priori, it is nat-
ural to expect that symmetry breaking helps training: a non-zero one-point function or mean G(i1)(x) = E[fi(x)] of a trained network density is crucial to making non-zero predictions in supervised learning, and if a network density at initialization exhibits symmetry, developing a one-point
function during supervised learning usually breaks it. Contrary to this intuition, we will see in
experiments that symmetry in the network density at initialization can help training.

To develop these ideas and prepare for experiments, we frame the discussion in terms of an architecture

with symmetry properties that are easily determined via duality: a network with a linear no-bias

output layer from an N -dimensional hidden layer to D-dimensional network output. The network

function is fi(x) = Wiljgj(x), i = 1, . . . , D, where gj(x) is the post-activation of the last hidden

layer,

with

parameters

g. Output

weights

in

the

final

(lth)

layer

are

initialized 

as

Wilj  N (µW l , 1/ N ), i < k + 1

Wilj  N (0, 1/ N ), i  k + 1, (14)

where k is a hyperparameter that will be varied in the experiments. By a simple extension of our SO(D) output symmetry result, this network density has SO(D - k) symmetry. The symmetry
breaking is measured by the one-point function

G(i1)(x) = E[fi(x)] = N µW l Eg [gi(x)] = 0, i < k,

(15)

6

and zero otherwise. Here, = 0 means as a function; for some x, G(i1)(x) may evaluate to 0. This non-zero mean breaks symmetry in the first k network components, and since SO(D) symmetry is restored in the µW l  0 limit, µW l and k together control the amount of symmetry breaking.

Symmetry and Correlated Parameters. Learning-induced flows in neural network densities lead
to correlations between parameters, breaking any independence that might exist in the parameter priors.
Since symmetry-via-duality relies only on an invariant P, it can apply in the case of correlations, when P does not factorize. In fact, this is the generic case: a density P which is symmetric due to being constructed from group invariants is not, in general, factorizable. For instance, the multivariate Gaussian P = exp(-xixi/22) with i = 1, . . . , m is constructed from the SO(m) invariant xixi and leads to independent parameters due to factorization. However, provided a density normalization condition is satisfied, additional SO(l)-invariant terms cn (xixi)n (or any other Casimir invariant) may be added to the exponent which preserve symmetry, but break independence for n > 1.

As an example, consider again the case with SO(D) output symmetry with network function f (x) = L(g(x, g)) where L : RN  RD is a linear layer, but now draw its weights and biases from
symmetric parameter distributions with quartic non-Gaussianities,

W  P = e ij

W

-

Tr(W T W 2W 2

)

-

(Tr

(W

T

W

))2

bi



Pb

=

e . -

b·b 2b2

-

(b·b)2

(16)

By construction these distributions are invariant under SO(D), and therefore the function-space density is as well. However, independence is broken for  = 0. Training could also mix in parameters from other layers, yielding a non-trivial joint distribution which is nevertheless invariant provided that the final layer parameter-dependence arises only through SO(D) invariants.

Such independence-breaking networks provide another perspective on neural networks and GPs. Since
the NNGP correspondence relies crucially on the central limit theorem, and therefore independence of an infinite number of parameters as N  , we may break the GP to a non-Gaussian process not only by taking finite-N , but also by breaking independence, as with  = 0 above. In this example, symmetry-via-duality requires neither the asymptotic N   limit nor the independence limit.

Independence breaking introduces potentially interesting non-Gaussianities into function-space densities, which likely admit an effective field theory (EFT) akin to the finite-N EFT treatment developed in [31]. We leave this treatment for future work.

Symmetry and the Neural Tangent Kernel. Gradient descent training of a neural network f is governed by the Neural Tangent Kernel (NTK) [1], ^ (x, x ) = i f(x) i f(x ). Since ^ (x, x ) depends on concrete parameters associated to a fixed neural network draw, it is not invariant. However,
the NTK converges in appropriate large-N limits to a kernel  that is deterministic, due to the
appearance of ensemble averages, allowing for the study of symmetries of (x, x ) via duality.

As an fi(x)

example, consider a = Wiljgj(x)/ N , W

neural lN

network (0, W 2 ),

with a linear output layer (mapping from where gj(x) is the post-activation of the last

RN  hidden

RD ), layer

and we have turned off bias for clarity; it may be added trivially. The corresponding NTK is

^ i1i2 (x, x

)

=

1 N

gj (x,

g) gj(x

,

g )

i1 i2

+

Wil1j1 Wil2j2

gj1 (x, gk

g )

gj2 (x , gk

g )

.

(17)

This depends on the concrete draw f and is not invariant. However, as N  , ^ at initialization becomes the deterministic NTK

i1i2 (x, x ) = i1i2 E gj (x, g)gj (x , g)

+ E[Wil1j1 Wil2j2 ]E

gj1 (x, g) gj2 (x , g)

gk

gk

.

The transformation fi  Rijfj for R  SO(D) acts only on the first factor in each term, which themselves may be shown to be invariant, rendering i1i2 (x, x ) invariant under SO(D). Alternatively, a related calculation shows that E[i1i2 (x, x )] is also SO(D) invariant.
Such results are more general, arising similarly in other architectures according to Corollary 1.1. Though the deterministic NTK at t = 0 is crucial in the linearized regime, invariance of  may also hold during training, as may be studied in examples via invariance of P. See [32] for SO(D) input symmetry of the deterministic NTK .

7

0.2

0.1

µW 1

Accuracy vs. Symmetry Breaking
80
70
60
0 2 4 6 8 10 k

Max Accuracy(%)

One-cold Accuracy vs. Init. Mean 86.0

85.5

85.0

84.5 0.00

0.05

0.10

µW 1

0.0

Figure 1: Test accuracy %age on Fashion-MNIST. (Left): Dependence on symmetry breaking parameters µW and k for one-hot encoded labels. The error is presented in Appendix (D). (Right): Dependence on µW for one-cold encoded labels, showing the 95% confidence interval.

4 Experiments

We carry out two classes of experiments2 testing symmetry-via-duality. In the first, we demonstrate that the amount of symmetry in the network density at initialization affects training accuracy. In the second, presented in Appendix C, we demonstrate how symmetry may be tested via numerically computed correlators.

Does Symmetry Affect Training? We now wish to test the ideas from Section 3 on how the amount
of symmetry at initialization affects test accuracy after training, as controlled by the hyperparameters µW 1 and k; see [48] for another analysis of symmetry breaking and learning. We further specify the networks discussed there by choosing a single-layer network (l = 1, d = 1) with ReLU nonlinearities (i.e.,gj is the post-activation of a linear layer), N = 50, weights of the first linear layer W 0  N (0, 1/ d), and weights of the output initialized as in (14). All networks were trained on the Fashion-MNIST dataset [49] for 20 epochs with MSE loss, with one-hot (or one-cold) encoded class labels, leading to network outputs with dimension D = 10, and therefore symmetry SO(10 - k) at initialization.

In the first experiment, we study how the amount of rotational symmetry breaking at initialization
affects test accuracy on Fashion-MNIST with one-hot encoded class labels. We vary the amount of symmetry breaking by taking k  {0, 2, 4, 6, 10} and µW 1  {0.0, . . . , 0.2} with .01 increment. Each experiment is repeated 20 times with learning rate  = .001. For each (k, µW 1 ) pair, the mean of the maximum test accuracy across all 20 experiments is plotted in Figure 1 (LHS). We see
that performance is highest for networks initialized with µW 1 = 0 or k = 0, i.e. with an SO(D) symmetric initialization density, and decreases significantly with increasing amounts of symmetry
breaking (increasing µW 1 and k), contrary to the intuition discussed in Section 3. See Appendix (D) for more details about the experiments.

If supervised learning breaks symmetry via developing a non-trivial one-point function (mean), but we see that symmetry at initialization helps training in this experiment, then what concept is missing?

It is that symmetry breaking at initialization could be in the wrong direction, i.e. the initialization mean is quite different from the desired trained mean, which (if well-trained) approximate ground truth labels. In our experiment, the initialization mean is

G(i1)(x) = E[fi(x)] = 50µW 1 EW 0 [ ReLU(Wi0j xj ) ] = 0,

(18)

which will in general be non-zero along all output components. It is "in the wrong direction" since class labels are one-hot encoded and therefore have precisely one non-zero entry. Furthermore, even for means in the right direction, the magnitude could be significantly off.

2We provide an implementation of our code at https://github.com/keeganstoner/nn-symmetry.

8

In the second experiment, we test this idea by imposing a better match between the initialization mean and the class labels. To do so, we keep the network fixed and instead one-cold encode the class labels, i.e. instead of class i being encoded by the unit vector ei  RD, it is encoded by 1 - ei, where 1 is the vector of ones. At initialization we find that EW 0 [ ReLU(Wi0jxj) ]  0.5 - 1.0 for Fashion-MNIST, and therefore G(i1)(x)  25µW 1 - 50µW 1 is of the right order of magnitude to match the ones in the one-cold vectors for µW 1  .02 - .04. Relative to the first experiment, this experiment differs only in the one-cold encoded class labels and the fixed value k = 10, which ensures complete symmetry breaking in the prior for µW 1 = 0. We see from Figure 1 (right) that performance improves until µW 1  .02-.04, but then monotonically decreases for larger µW 1 . By construction, the symmetry breaking is much closer to the correct direction than in the first experiment, but the magnitude of the initialization means affects performance: the closer they are to the D - 1 ones in the one-cold encoding, the better the performance. The latter occurs for µW 1  .02 - .04 according to our calculation, which matches the experimental result.
5 Conclusion
We introduce symmetry-via-duality, a mechanism that allows for the determination of symmetries of neural network functional densities Pf , even when the density is unknown. The mechanism relies crucially two facts: i) that symmetries of a statistical system may also be determined via their correlation functions; and ii) that the correlators may be computed in parameter space. The utility of parameter space in determining symmetries of the network density is a hallmark of duality in physical systems, in this case, Parameter-Space / Function-Space duality. We demonstrated that invariance of correlation functions ensures the invariance of Df Pf , which yields the invariance of the density Pf itself in the absence of a Green-Schwarz mechanism. Symmetries were categorized into input and output symmetries, the analogs of spatial and internal symmetries in physics, and a number of examples of symmetries were presented, including SO(D) and SU (D) symmetries at both input and output. In all calculations, the symmetry transformation induces a transformation on the input or output that may be absorbed into a transformation of network parameters T , and invariance of the correlation functions follows from invariance of DT PT . The invariance of DP also follows, since T are by definition the only parameters that transform. The mechanism may also be applied at any point during training, since it relies on the invariance of P. If duality is used to ensure the symmetry of the network density at initialization, then the persistence of this symmetry during training requires that P remains symmetric at all times. Under continuous time gradient descent, the flow equation for P yields conditions preserving the symmetry of P. We also demonstrated that symmetry could be partially broken in the initialization density, that symmetry-via-duality may also apply in the case of non-independent parameters, and that the Neural Tangent Kernel may be invariant under symmetry transformations. Our analysis allows for different amounts of symmetry in the network density at initialization, leading to increasing constraints on the density with increasing symmetry. Accordingly, it is natural to ask whether this affects training. To this end, we performed Fashion-MNIST experiments with different amounts of network density symmetry at initialization. The experiments demonstrate that symmetry breaking helps training when the associated mean is in the direction of the class labels, and entries are of the same order of magnitude. However, if symmetry is broken in the wrong direction or with too large a magnitude, performance is worse than for networks with symmetric initialization density.
9

Acknowledgements
We thank Sergei Gukov, Joonho Kim, Neil Lawrence, Magnus Rattray, and Matt Schwartz for discussions. We are especially indebted to Sébastian Racanière, Danilo Rezende, and Fabian Ruehle for comments on the manuscript. J.H. is supported by NSF CAREER grant PHY-1848089. This work is supported by the National Science Foundation under Cooperative Agreement PHY-2019786 (The NSF AI Institute for Artificial Intelligence and Fundamental Interactions).
References
[1] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In NeurIPS, 2018.
[2] Radford M Neal. BAYESIAN LEARNING FOR NEURAL NETWORKS. PhD thesis, University of Toronto, 1995.
[3] Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent. ArXiv, abs/1902.06720, 2019.
[4] Taco S. Cohen, Mario Geiger, and Maurice Weiler. Intertwiners between induced representations (with applications to the theory of equivariant neural networks), 2018.
[5] Mirgahney Mohamed, Gabriele Cesa, Taco S. Cohen, and Max Welling. A data and compute efficient design for limited-resources deep learning, 2020.
[6] Luca Falorsi, Pim de Haan, Tim R. Davidson, Nicola De Cao, Maurice Weiler, Patrick Forré, and Taco S. Cohen. Explorations in homeomorphic variational auto-encoding, 2018.
[7] Taco S. Cohen and Max Welling. Group equivariant convolutional networks, 2016.
[8] Maurice Weiler, Mario Geiger, Max Welling, Wouter Boomsma, and Taco Cohen. 3d steerable cnns: Learning rotationally equivariant features in volumetric data, 2018.
[9] Fabian B. Fuchs, Daniel E. Worrall, Volker Fischer, and Max Welling. Se(3)-transformers: 3d roto-translation equivariant attention networks, 2020.
[10] Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph networks, 2019.
[11] Haggai Maron, Or Litany, Gal Chechik, and Ethan Fetaya. On learning sets of symmetric elements, 2020.
[12] Haggai Maron, Ethan Fetaya, Nimrod Segol, and Yaron Lipman. On the universality of invariant networks, 2019.
[13] Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick Riley. Tensor field networks: Rotation- and translation-equivariant neural networks for 3d point clouds, 2018.
[14] Siamak Ravanbakhsh, Jeff Schneider, and Barnabas Poczos. Equivariance through parametersharing, 2017.
[15] Risi Kondor, Zhen Lin, and Shubhendu Trivedi. Clebsch-gordan nets: a fully fourier space spherical convolutional neural network, 2018.
[16] Irina Higgins, David Amos, David Pfau, Sebastien Racaniere, Loic Matthey, Danilo Rezende, and Alexander Lerchner. Towards a definition of disentangled representations, 2018.
[17] Risi Kondor and Shubhendu Trivedi. On the generalization of equivariance and convolution in neural networks to the action of compact groups, 2018.
[18] Brandon Anderson, Truong-Son Hy, and Risi Kondor. Cormorant: Covariant molecular neural networks, 2019.
10

[19] Alexander Bogatskiy, Brandon Anderson, Jan T. Offermann, Marwah Roussi, David W. Miller, and Risi Kondor. Lorentz group equivariant neural network for particle physics, 2020.
[20] Daniel E. Worrall, Stephan J. Garbin, Daniyar Turmukhambetov, and Gabriel J. Brostow. Harmonic networks: Deep translation and rotation equivariance, 2017.
[21] Taco S. Cohen, Maurice Weiler, Berkay Kicanaoglu, and Max Welling. Gauge equivariant convolutional networks and the icosahedral CNN. CoRR, abs/1902.04615, 2019.
[22] Erik J. Bekkers. B-spline cnns on lie groups. CoRR, abs/1909.12057, 2019.
[23] Christopher M. Bender, Juan Jose Garcia, Kevin O'Connor, and Junier Oliva. Permutation invariant likelihoods and equivariant transformations. CoRR, abs/1902.01967, 2019.
[24] Kashif Rasul, Ingmar Schuster, Roland Vollgraf, and Urs Bergmann. Set flow: A permutation invariant normalizing flow. CoRR, abs/1909.02775, 2019.
[25] Danilo Jimenez Rezende, Sébastien Racanière, Irina Higgins, and Peter Toth. Equivariant hamiltonian flows, 2019.
[26] Jonas Köhler, Leon Klein, and Frank Noé. Equivariant Flows: Exact Likelihood Generative Learning for Symmetric Densities. arXiv e-prints, page arXiv:2006.02425, June 2020.
[27] Gurtej Kanwar, Michael S. Albergo, Denis Boyda, Kyle Cranmer, Daniel C. Hackett, Sébastien Racanière, Danilo Jimenez Rezende, and Phiala E. Shanahan. Equivariant flow-based sampling for lattice gauge theory. Physical Review Letters, 125(12), Sep 2020.
[28] Denis Boyda, Gurtej Kanwar, Sébastien Racanière, Danilo Jimenez Rezende, Michael S. Albergo, Kyle Cranmer, Daniel C. Hackett, and Phiala E. Shanahan. Sampling using SU (N ) gauge equivariant flows. Phys. Rev. D, 103(7):074504, 2021.
[29] Philip Betzler and Sven Krippendorf. Connecting Dualities and Machine Learning. Fortsch. Phys., 68(5):2000022, 2020.
[30] Sven Krippendorf and Marc Syvaeri. Detecting Symmetries with Neural Networks. 3 2020.
[31] James Halverson, Anindita Maiti, and Keegan Stoner. Neural networks and quantum field theory. Machine Learning: Science and Technology, Mar 2021.
[32] Omry Cohen, Or Malka, and Zohar Ringel. Learning curves for overparametrized deep neural networks: A field theory perspective. Physical Review Research, 3(2), Apr 2021.
[33] Jacob A. Zavatone-Veth and Cengiz Pehlevan. Exact priors of finite neural networks, 2021.
[34] Christopher KI Williams. Computing with infinite networks. In Advances in neural information processing systems, pages 295­301, 1997.
[35] Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S. Schoenholz, Jeffrey Pennington, and Jascha Sohl-Dickstein. Deep neural networks as gaussian processes, 2017.
[36] Omry Cohen, Or Malka, and Zohar Ringel. Learning curves for deep neural networks: A gaussian field theory perspective, 2020.
[37] Alexander G. de G. Matthews, Mark Rowland, Jiri Hron, Richard E. Turner, and Zoubin Ghahramani. Gaussian process behaviour in wide deep neural networks. ArXiv, abs/1804.11271, 2018.
[38] Greg Yang. Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes. arXiv e-prints, page arXiv:1910.12478, October 2019.
[39] Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior, gradient independence, and neural tangent kernel derivation. ArXiv, abs/1902.04760, 2019.
11

[40] Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Daniel A. Abolafia, Jeffrey Pennington, and Jascha Sohl-Dickstein. Bayesian convolutional neural networks with many channels are gaussian processes. ArXiv, abs/1810.05148, 2018.
[41] Adrià Garriga-Alonso, Laurence Aitchison, and Carl E. Rasmussen. Deep convolutional networks as shallow gaussian processes. ArXiv, abs/1808.05587, 2019.
[42] Ethan Dyer and Guy Gur-Ari. Asymptotics of wide networks from feynman diagrams. ArXiv, abs/1909.11304, 2020.
[43] Sho Yaida. Non-gaussian processes and neural networks at finite widths. ArXiv, abs/1910.00019, 2019.
[44] Joseph Polchinski. Dualities of Fields and Strings. Stud. Hist. Phil. Sci. B, 59:6­20, 2017. [45] N. Seiberg and Edward Witten. Electric - magnetic duality, monopole condensation, and
confinement in N=2 supersymmetric Yang-Mills theory. Nucl. Phys. B, 426:19­52, 1994. [Erratum: Nucl.Phys.B 430, 485­486 (1994)]. [46] Juan Martin Maldacena. The Large N limit of superconformal field theories and supergravity. Adv. Theor. Math. Phys., 2:231­252, 1998. [47] Michael B. Green and John H. Schwarz. Anomaly Cancellation in Supersymmetric D=10 Gauge Theory and Superstring Theory. Phys. Lett. B, 149:117­122, 1984. [48] Daniel Kunin, Javier Sagastuy-Breña, Surya Ganguli, Daniel L. K. Yamins, and Hidenori Tanaka. Neural mechanics: Symmetry and broken conservation laws in deep learning dynamics. CoRR, abs/2012.04728, 2020. [49] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. CoRR, abs/1708.07747, 2017. [50] P. Di Francesco, P. Mathieu, and D. Senechal. Conformal Field Theory. Graduate Texts in Contemporary Physics. Springer-Verlag, New York, 1997. [51] Kevin 1977 Costello. Renormalization and effective field theory. Mathematical surveys and monographs ; v. 170. American Mathematical Society, Providence, R.I., 2011.
12

Supplementary Material

A Proofs and derivations

A.1 Symmetry from Correlation Functions

We begin by demonstrating the invariance of network correlation functions under transformations that leave the functional measure and density invariant. Consider a transformation

f (x) = (f (x ))

(19)

that leaves the functional density invariant, i.e.

D[f ] e-S[f] = Df e-S[f].

(20)

Then we have

1 E[f (x1) . . . f (xn)] = Zf

Df e-S[f] f (x1) . . . f (xn)

(21)

1 =
Zf

Df e-S[f ] f (x1) . . . f (xn)

(22)

1 =
Zf

D[f ] e-S[f] (f (x1)) . . . (f (xn))

(23)

1 =
Zf

Df e-S[f] (f (x1)) . . . (f (xn)),

(24)

= E[(f (x1)) . . . (f (xn))]

(25)

where the second to last equality holds due to the invariance of the functional density. This completes the proof of (3); See, e.g., [50] for a QFT analogy.

For completeness we wish to derive the same result for infinitesimal output transformations, where the parameters of the transformation depend on the neural network input; in physics language, these are called infinitesimal gauge transformations.

The NN output, transformed by an infinitesimal parameter a(x), is f (x) = (f (x )) = f (x ) + f (x ), where f (x ) = -ia(x )Taf (x ); Ta is the generator of the transformation group. Corresponding output space log-likelihood transforms as S  S - dx µ jaµ(x ) a(x ), for a current jaµ(x ) that may be computed. The transformed n-pt function at O() is given by,

E[f

(x1) · · · f

(xn)]

=

1 Z

Df  f (x1) · · · f (xn) e-S

1 =
Z

Df f (x1) · · · f (xn) +  f (x1) · · · f (xn) e-S-

dx µjaµ(x )a(x )

= E[f (x1) · · · f (xn)] + E [f (x1) · · · f (xn)] - dx µE[ jaµ(x ) f (x1) · · · f (xn)] a(x ),

where we obtain second and last equalities under the assumption that functional density is invariant, following (20), and invariance of function-space measure, Df = Df , respectively.

Following the -independence of L.H.S. of (26), O() terms on R.H.S. must cancel each other, i.e.

n

dx µ E jaµ f (x1) · · · f (xn) + E

f (x1) · · · Taf (xi) · · · f (xn) (x - xi) (x ) = 0,

i=1
(26)

for any infinitesimal function (x ). Thus, the coefficient of (x ) in above integrand vanishes at all

x , and we have the following by divergence theorem

n

-i dx E

f (x1) · · · Taf (xi) · · · f (xn) (x - xi) = dsµE jaµ f (x1) · · · f (xn) . (27)

i=1



13

Taking hypersurface  to infinity does not affect the integral in (27), therefore in limR, if E jaµf (x1) · · · f (xn) dies sufficiently fast, we obtain

n

i E f (x1) · · · Taf (xi) · · · f (xn) = 0 =  E f (x1) · · · f (xn) ,

(28)

i=1

a statement of invariance of correlation functions under infinitesimal input-dependent transformations.

Thus, we obtain the following invariance under finite / infinitesimal, input-dependent/independent transformations, whenever Df = Df ,

E f (x1) · · · f (xn) = E f (x1) · · · f (xn) .

(29)

(29) is same as (3), completing the proof.

A.2 SU (D) Output Symmetry

We show the detailed construction of SU (D) invariant network densities, for networks with a complex linear output layer, when weight and bias distributions are SU (D) invariant.

The network is defined by f (x) = L(g(x, g)), for a final affine transformation L on last postactivation g(x, ); x and g are the inputs and parameters until the final linear layer, respectively. As SU (D) is the rotation group over complex numbers, SU (D) invariant NN densities require complex-valued
outputs, and this requires complex weights and biases in layer L. Denoting the real and imaginary parts of complex weight W and bias b in layer L as W 1, W 2, b1, b2 respectively, we obtain W, b
distributions as PW,W = PW 1 PW 2 and Pb,b = Pb1 Pb2 . The simplest SU (D) invariant structure is

Tr[WW] = W W = W1W1 + W2W2 = Tr(W 1T W 1) + Tr(W 2T W 2), (30)

and similarly for bias. To obtain an SU -invariant structure in PW,W as a sum of SO-invariant structures from products of PW 1 , PW 2 , all three PDFs need to be exponential functions, with equal coefficients in PW 1 , PW 2 . Therefore, starting with SO(D)-invariant real and imaginary parts W 1, W 2  N (0, W 2 ) and b1, b2  N (0, b2), one can obtain the simplest SU (D) invariant complex weight and bias distributions, given by PW,W = exp(-Tr[WW]/2W 2 ), Pb,b = exp(-Tr[bb]/2b2) respectively.

We want to express the network density and its correlation functions entirely in terms of

complex-valued outputs, weights and biases, therefore, we need to transform the measures of W 1, W 2, b1, b2 into measures over W, b. As DW 1DW 2 = |J|DWDW, for Jacobian of

W1 W2

=

11

2i 2

-22i

W W

,

we

obtain

DW 1DW 2Db1Db2

=

|J |2DWDWDbDb, and

|J|2 = 1/4. With this, the n-pt function for any number of f 's and f 's becomes the following,

G(i1n,)··· ,in (x1, ·

·

·

,

xn)

=

E[fi1 (x1) ·

·

·

fir (xr)

f
ir+1

(xr+1

)

·

·

· fin (xn)]

1 =
4Z

DWDWDbDbDg Wi1j1 gj1 (x1, g) + bi1 · · · Wirjr gjr (xr, g) + bir

g
jr+1

(xr+1

,

g

)Wjr+1

ir+1

+

b
ir+1

···

g (x ,  )W + b e P , 
jn n g

 jn in

 in

-

Tr(W W) 2W 2

-

Tr(b b) 2b2

g

(31)

where Z is the normalization factor. We emphasize that the transformation of fi and fj only transforms two indices inside the trace in Tr(WW); it is invariant in this case, and also when all
four indices transform.

From the structure of PW,W and Pb,b , only those terms in the integrand of (31), that are functions
of Wis Wis and bit bit alone, and not in product with any number of Wiu , Wiu , biu , biu individually, result in a non-zero integral. Thus, we have the only non-vanishing correlation functions from an equal number of f 's and f 's. We hereby redefine the correlation functions of this complex-valued

network as

G(i12,n··)· ,i2n (x1, · · ·

, x2n)

:=

E[fi1

(xp1

)

·

·

·

fin

(xpn

)

f
in+1

(xpn+1

)

·

·

·

fi2n

(xp2n

)],

(32)

where {p1, · · · , p2n} can be any permutation of set {1, · · · , 2n}.

14

A.3 SU (d) Input Symmetry

We now show an example of neural networks densities invariant under SU (d) input transformations, provided that input layer parameters are drawn from an SU (d) invariant distribution.

We will take a linear input layer and turn off bias for simplicity, as it may be trivially included as in

SU (D) output symmetry. SU (d) group acts on complex numbers, therefore network inputs and input

layer parameters need to be complex, such a network function is fi = gij(Wjkxk). The distribution of W is obtained from products of distributions of its real and imaginary parts W 1, W 2. Following

SU (D) output symmetry demonstration, the simplest SU (d) invariant PW,W is obtained when W 1, W 2  N (0, W 2 ) are both SO(d) invariant, we get PW,W = exp(-Tr[WW]/2W 2 ). The measure of W is obtained from the measures over W 1, W 2 as DW 1DW 2 = |J|DWDW, with |J| = 1/2. Following a similar analysis as (A.2), the only non-trivial correlation functions are

G(i12,n··)· ,i2n (x1, · · ·

, x2n)

:=

E[fi1

(xp1

)

·

·

·

fin

(xpn

)

f
in+1

(xpn+1

)

·

·

·

fi2n

(xp2n

)]

1 =
2Z

DWDWDg

gi1j1 (Wj1k1 xpk11 )

·

·

·

ginjn (Wjnkn xpknn )

(x W )g pn+1
kn+1





kn+1jn+1 jn+1in+1

·

·

·

(x

p2n k2n

Wk2n

j2n

)gj2n

i2n

-
e

Tr(W W) 2W 2

Pg

,

(33)

where {p1, · · · , p2n} is any permutation over {1, · · · , 2n}, and we have changed the x subscript

label to a superscript to make room for the indices.

Under input rotations xi  Sijxj, xk  xl Slk by S  SU (d), the correlation functions transform into

Gi(12,·n··),i2n (x1, · · ·

, x2n)

=

E[fi1

(Sk1

l1

xpl11

)

·

·

·

fin

(Skn

ln

xplnn

)

f
in+1

(x

pn+1 ln+1

S
ln+1

kn+1

)

·

·

·

fi2n

(x

p2n l2n

Sl2n

k2n

)]

1 =
2Z

DWDWDg gi1j1 (Wj1k1 Sk1l1 xpl11 ) · · · ginjn (Wjnkn Sknln xplnn )

(x S W )g · · · (x S W )g P P pn+1 
ln+1 ln+1kn+1





kn+1jn+1 jn+1in+1

p2n  l2n l2nk2n





k2nj2n j2ni2n W,W g

1 =
2Z

|S-1||S-1|DW~ DW~ Dg gi1j1 (W~ j1l1 xpl11 ) · · · ginjn (W~ jnln xplnn )

(x W~ )g pn+1
ln+1





ln+1jn+1 jn+1in+1

·

·

·

(x

p2n l2n

W~ l2n

j2n

)gj2n

i2n

PS-1

W~ ,W~ 

S



-1

Pg

=

E[fi1 (xp1 ) · ·

·

fin (xpn )

f
in+1

(xpn+1

)

···

fi2n (xp2n )]

=

G(i12,n··)· ,i2n (x1, ·

·

·

,

x2n).

(34)

The crucial second-to-last equality holds for |(S)-1||S-1| = 1, as is the case here; further we need PW,W = PW~ ,W~  , this stipulation holds true when Re(W), Im(W)  N (0, W 2 ), due to the SU -invariance of Tr(WW).

A.4 Translation Input Symmetry

We demonstrate an example of network densities that remain invariant under continuous translations on input space, when the input layer weight is deterministic and input layer bias is sampled from a uniform distribution on the circle, b  U(S1). We will map the weight term to the circle by taking it mod 1, (i.e. % 1).

The network output fi(x) = gij((Wjkxk) % 1+bj) transforms into f (x ) = gij((Wjkxk) % 1+bj) under translations of inputs xk  xk + ck, where bj = (Wjkck) % 1 + bj. With a deterministic W , the network parameters are given by  = {, b}, and Db = Db. The transformed n-pt function is

Gi(1n,·)·· ,in (x1, · · · , xn) = E[fi1 (xn) · · · fin (xn)]

1 =
Z

DbD gi1j1 ((Wj1k1 xk1 + Wj1k1 ck1 ) % 1 + bj1 ) · · ·

· · · ginjn ((Wjnkn xkn + Wjnkn ckn ) % 1 + bjn )PbP

1 =
Z

Db D gi1j1 ((Wj1k1 xk1 ) % 1 + bj1 ) · · · ginjn ((Wjnkn xkn ) % 1 + bjn )Pb P

= E[fi1 (xn) · · · fin (xn)] = G(i1n,)··· ,in (x1, · · · , xn),

(35)

15

where the crucial third-to-last equality holds when Pb = Pb . This stipulation is true as E[bk] = E[b k] for any k when b  U(S1) and W is deterministic; since the layer is valued on the circle with circumference 1, we know that any bias value is equally probable. Thus b and b have identical moment generating functions and Pb = Pb .

A.5 Sp(D) Output Symmetry

We also demonstrate an example of network densities that remain invariant under the compact symplectic group Sp(D) transformations on output space.

The compact symplectic Sp(D) is the rotation group of quaternions, just as SU is the rotation group

of complex numbers. Thus, a network with linear output layer would remain invariant under compact

symplectic group, if last linear layer weights and biases are quaternionic numbers, drawn from Sp(D)

invariant distributions. We define the network output fi(x) = Wijgj(x, g) + bi as before, with

parameters Wab = Wab,0 + iWab,1 + jWab,2 + kWab,3 and ba = ba,0 + iba,1 + jba,2 + kba,3, such

that Hermitian norms Tr(W W ) = WabWab =

3 i=0

Wa2b,i

and Tr(bb)

=

baba

=

3 i=0

b2a,i

are compact symplectic Sp(D) invariant by definition, where the conjugate of a quarternion

q = a + ib + jc + kd is q = a - ib - jc - kd. The distributions of W, b are obtained as

products of distributions of the components W0, W1, W2, W3 and b0, b1, b2, b3 respectively. Fol-

lowing the SU (D) symmetry construction, we can obtain the simplest Sp(D) invariant PW,W 

and Pb,b when these are functions of the Hermitian norm, and PDF of each component PWi

is an larly

exponential function of with bias. Starting with

SO(D) W0, W1,

Win2v,aWria3ntteNrm(0T, r(W 2W)iT,

Wi) and

with equal b0, b1, b2, b3

coefficient, simi N (0, b2), we

get Sp(D) invariant quaternionic parameter distributions PW,W  = exp(-Tr(W W )/2W 2 ) and

Pb,b = exp(-Tr(bb)/2b2). We also obtain the measures over W, b from measures over Wi and bi,

e.g. DW DW  = |J|DW0DW1DW2DW3. Following an analysis similar to (A.2), it can be shown

that the only non-trivial correlation functions of this quaternionic-valued network are

G(i1n,)··· ,i2n (x1, · · ·

, x2n)

|J |2 =
Z

DW

DW



DbDb

fi1

(xp1

)

·

·

·

fin

(xpn

)

f
in+1

(xpn+1

)

·

·

·

f (x )e P , 
i2n

p2n

-

Tr(W W 2W 2

)

-

Tr(b b) 2b2

g

(36)

for {p1, · · · , p2n} any permutation over {1, · · · , 2n}. Under Sp(D) transformation of outputs
fi  Sijfj, fk  flSlk, by S  Sp(D) in quaternionic basis, the correlation functions transform as

Gi(12··n·i)2n (x1, · · · , x2n) = E

Si1j1 fj1 (xp1 )

·

·

·

Sinjn fjn (xpn )

f (x )S 
jn+1

pn+1

 jn+1 in+1

·

·

·

fj2n (xp2n )Sj2ni2n

|J |2 =
Z

DW DW DbDbDgSi1j1 Wj1k1 gk1 (xp1 , g) + bj1 · · · Sinjn Wjnkn gkn (xpn , g)

+ bjn

g
kn+1

(xpn+1

,

g

)Wkn+1

jn+1

+

b
jn+1

S
jn+1 in+1

·

·

·

gk2n (xp2n , g )Wk2nj2n + bj2n

Sj2ni2n PW,W  Pb,b Pg

|J |2 =
Z

|S-1||S-1|DW~ DW~ D~bD~bDg W~ i1k1 gk1 (xp1 , g) + ~bi1 · · · W~ inkn gkn (xpn , g)

+ ~bin

g
kn+1

(xpn+1

,

g

)W~ kn+1

in+1

+

~bin+1

···

gk2n (xp2n , g )W~ k2ni2n + ~bi2n

P P P S-1W~ ,W~ S-1 S-1~b,~bS-1 g

=E

fi1

(xp1

)

·

·

·

fin

(xpn

)

f
in+1

(xpn+1

)

·

·

·

fi2n

(xp2n

)

= G(i12·n··)i2n (x1, · · · , x2n).

(37)

The crucial second-to-last equality holds when |S-1| = 1, PW,W  = PW~ ,W~  , and Pb,b = P~b,~b . These stipulations hold true, for example, when S  Sp(D), W0, W1, W2, W3  N (0, W 2 ), and b0, b1, b2, b3  N (0, b2), due to the invariance of Tr(W W ) in PW,W  = exp(-Tr(W W )/2W 2 ), and similarly for Pb,b .

16

A.6 Preserving Symmetry During Training: Examples

We study further the example of an SO(D) output symmetry from Section 2. Turning off the bias for

simplicity, the network function is

fi(x) = Wijgj(x),

(38)

with parameters  = {W, g}; transformations of fi may be absorbed into W , i.e. W = T .

The network density remains symmetric during training when the updates to P preserve symmetry; for this example, we showed in Section 3 that it occurs when L is invariant and

P i

= IP

i

L i = IL i.

(39)

Let the initial density be P(0) = exp[-

k j=1

aj(Tr(T ))j]

for

aj



R.

This

clearly

satisfies

the

first condition in (39). An example of SO-invariant loss function is

L=

fi(x)fi(x) - yjyj

x,y

=

Wilgl(x)Wikgk(x) - yj yj ,

(40)

x,y

where L/g is invariant because L is and g does not transform. Furthermore,

L

Wmn = 2 Wmk

gk (x)gn (x)
x,y

(41)

which satisfies the second condition in (39), since the first index is the one that transforms when W absorbs the transformation of fi.

B More General SO Invariant Network Distributions

We will now give an example of an SO(D) invariant non-Gaussian network distribution at infinite width, as parameters of the initialized SO(D) invariant GP become correlated through training. Such a network distribution can be obtained up to perturbative corrections to the initialized network distribution, if the extent of parameter correlation is small.

Training may correlate last layer weights ij of a linear network output fi = ijgj(x, g) initialized

with

ij



N (0, 2),

such

that

at

a

particular

training

step,

we

get

P

=

e-

1 22

2  -

ab ab cd cd

independent from Pg , with small . Correlation functions of this network distribution can be ob-

tained by perturbative corrections to correlation functions of the network distribution at initialization.

For example, the 2-pt function of the correlated network distribution is given by

G(i12i)2,NGP(x1, x2) =

DDg i1j1 i2j2

1 -  a2bc2d

gj1

(x1

,

g

)gj2

(x2

,

g

-
)e

1 22

2 

P

(g

)

DDg

1 -  a2bc2d

-
e

1 22

2 

P

(g

)

=

E[i21j1 ] - 

E[i21j1 ]E[a2b]E[c2d] +

E[i21j1 ]E[a4b] + 2 E[i41j1 ]·

i1 j1

i1 j1 =ab=cd

i1 j1 =ab

E[a2b] + E[i61j1 ] 1 - 

E[a4b] +

E[a2b]E[c2d]

-1
E gj1 (x1, g) gj1 (x2, g)

+ O(2)

ab

ab=cd

= G(i12i)2,GP(x1, x2) - 

E[i61j1 ] - E[i21j1 ]E[i41j1 ] - 2 (E[i21j1 ])2E[a2b] + 2 E[i41j1 ]E[a2b] ·

i1 j1 =ab

E gj1 (x1, g)gj1 (x2, g) + O(2).

(42)

E[inj] is evaluated using Gaussian P,GP

= e-

2  22

of initialized network distribution. O() terms in

(42)

scale

as

both

1/N

and

1/N 2,

as

can

be

seen

after

properly

normalization

of



by

2



2 N

;

this

`mixed' 1/N scaling results from parameter correlations. (We set bias to 0 everywhere for simplicity,

analysis for nontrivial bias follows similar method.)

17

C SO(D) Invariance in Experiments

The correlator constraint (3) gives testable necessary conditions for a symmetric density. Consider a single-layer fully-connected network, called Gauss-net due to having a Gaussian GP kernel defined by f (x) = W1((W0x + b0)) + b1, where W0  N (0, W 2 / d), W1  N (0, W 2 / N ), and b0, b1  N (0, b2), with activation (x) = exp(W0x + b0)/ exp(2(b2 + W 2 /d)).
To test for SO(D) invariance via (3), we measure the average elementwise change in n-pt functions before and after an SO(D) transformation. To do this we generate 2-pt and 4-pt correlators at various D for a number of experiments and act on them with 1000 random group elements of a given SO(D) group. Each group element is generated by exponentiating a random linear combination of generators of the corresponding algebra, namely

p

Rb = exp

i · T i ,

(43)

i=1

for b = 1, · · · , 1000, p = dim(SO(D)) = D(D - 1)/2, i  U (0, 1) and T i are generators of so(D) Lie algebra; i.e. D × D skew-symmetric matrices written in a simple basis3. For example, for D = 3 we take the standard basis for so(3),

00 0

0 0 -1

0 -1 0

T 1 = 0 0 -1 , T 2 = 0 0 0 , T 3 = 1 0 0

(44)

01 0

10 0

000

to generate group elements of SO(3).

We define the elementwise deviation Mn = abs(G (n)-G(n)) to capture the change in correlators due to SO(D) transformations. Here Gi(1n,·)·· ,in (x1, . . . , xn) := Ri1p1 · · · Rinpn G(pn1,)··· ,pn (x1, . . . , xn) is the SO-transformed n-pt function; both Mn and G(n) have the same rank.

Error bounds for deviation Mn are determined as Mn = (G (n))2 + (G(n))2 using the standard error propagation formulae, G(n) equals the average (elementwise) standard deviation of n-pt functions across 10 experiments, and G (n) is calculated as the following

G

(n)

=

1 Dn

1 1000

1000

b =1

(in ,pn )

1/2

Rib1p1 · · · Rtb · · · Ribnpn G(pn1,)··· ,pn 2 + Rib1p1 · · · Ribnpn G(n) 2

.

t=(i1 ,p1 )

(45)

We have changed the R subscript label b to superscript to make room for the indices. Rij denotes the average error in generated group elements R, it is captured by computing RT R for our experimentally
generated matrices R and measuring the magnitude of the off-diagonal elements, which are expected to be zero. We measure an average magnitude of O(10-18) =: R in these off-diagonal elements.

We take the deviation tensors Mn over 10 experiments for SO(3) and SO(5) transformations of
2-pt and 4-pt functions at D = 3, 5 respectively, both correlators of each experiment are calculated using 4 · 106 and 106 network outputs respectively. An element-wise average and standard deviation across 10 deviation tensors Mn are taken and then averaged over, to produce the mean of the SO-transformation deviation µMn , and its error Mn , respectively. We plot µMn ± Mn (the blue shaded area) in Figure (2), this signal lies well within predicted error bounds of ±Mn (in orange), although µMn deviates significantly from 0 at low widths, in contradiction to width independence

3The

generators

are

obtained

by

choosing

each

of

the

D(D-1) 2

independent

planes

of

rotations

to

have

a

canonical ordering with index i, determined by a (p, q)-plane. Each ith plane of rotation has a generator matrix

[T i]D×D with Tpiq = -1, Tqip = 1, for p < q, rest 0. For instance, at D = 3, there are 3 independent planes

of rotation formed by direction pairs {2, 3}, {1, 3} and {1, 2}. For each ith plane defined by directions {p, q},

general SO(3) elements [Ri]3×3 have Rpi q = - sin , Rqi p = sin , Rqi q = cos , Rqi q = cos , Rri r = 1 for

p < q , r = p, q and variable . Expanding each Ri in Taylor series; the coefficients of O() terms are taken to

define the generators of so(3) as in (44).

18

variation measures

SO(3) invariance of Gauss-net G(2)

0.01

µM2 µM2 ± M2 µM2 ± M2

0.00

1

2

3

log10 N

SO(5) invariance of Gauss-net G(2)

0.05

µM2 µM2 ± M2 µM2 ± M2

0.00

-0.05

1

2

3

log10 N

variation measures

variation measures

SO(3) invariance of Gauss-net G(4)

101

µM4

µM4 ± M4

100

µM4 ± M4

0

-100

-101

1

2

3

log10 N

SO(5) invariance of Gauss-net G(4)

101

µM4

µM4 ± M4

100

µM4 ± M4

0

-100

-101

1

2

3

log10 N

variation measures

Figure 2: Variation measures of 2-pt and 4-pt functions and their predicted error bounds, for SO(3) and SO(5) transformations of D = 3 and D = 5 networks, respectively.
of (3). This is due to the smaller sample size of parameters in low-width networks, and therefore small fluctuations in the weight and bias draws lead to more significant deviations from the "true" distribution of these parameters. A nonzero mean of the parameters caused by fluctuations leads to a nonzero mean of the function distribution f = 0, thus breaking SO(D) symmetry. We believe this is a computational artefact and does not contradict SO-invariance in (3).
D Experiment Details
The experiments in Section (4) were done using Fashion-MNIST under the MIT License4, using 60000 data points for each epoch split with a batch size of 64, and test batch size of 1000. Each experiment was run on a 240GB computing node through the Discovery Cluster at Northeastern University and took between 1 and 1.5 hrs to train 20 epochs. The experiments were repeated 20 times for each configuration, and were run with 21 × 6 configurations for the left plot in Fig. (1), and 11 for the right plot. The error in the left plot of Fig. (1) is shown in Fig. (3). The experiments in Appendix (C) were done on the same cluster, with the same memory nodes. These took around 24 hours on 10 compute nodes to generate models for each of the D values. The n-pt functions then took another 6 hours on a single node each.
E Comments on Functional Densities
Functional integrals are often treated loosely by physicists: they use them to great effect and experimental agreement in practice, but they are not rigorously defined in general; see, e.g., [51].
4The MIT License (MIT) Copyright © 2017 Zalando SE, https://tech.zalando.com
19

0.2

Accuracy vs. Symmetry Breaking : Error
15 10 5

0.1

µW 1

0.0

0 2 4 6 8 10 k
Figure 3: Error in the data from Fig. (1). Color represents the %age of variation across 20 experiments of the same configuration, computed as the standard deviation normalized by the mean.

We follow in this tradition in this work, but would like to make some further comments regarding cases that are well-defined, casting the discussion first into the language of Euclidean QFT, and then bringing it back to machine learning.
First, the standard Feynman functional path integral for a scalar field (x) is

Z = D e-S[],

(46)

but in many cases the action S[] is split into free and interacting pieces

S[] = SF [] + Sint[],

(47)

where the free action SF [] is Gaussian and the interacting action Sint[] is non-Gaussian. The free theory, which has Sint[] = 0, is a Gaussian process and is therefore well-defined. When interactions are turned on, i.e. the non-Gaussianities in Sint[] are small relative to some scale, physicists compute correlation functions (moments of the functional density) in perturbation theory,
truncating the expansion at some order and writing approximate moments of the interacting theory
density in terms of a sum of well-defined Gaussian moments, including higher moments.

Second, it is also common to put the theory on a lattice. In such a case the function  : Rd  R is restricted to a concrete collection of points {xi}, i = 1, . . . , m, with each xi  Rd. Instead of considering the random function (x) drawn from the difficult-to-define functional density, one
instead considers the random variable (xi) =: i, and the joint distribution on the set of i defines the lattice theory. One sees this regularly in the Gaussian process literature: evaluated on any discrete
set of inputs, the Gaussian process reduces to a standard multivariate Gaussian.

In this work we study functional densities associated to neural networks, and have both the perturbative and especially the lattice understanding in mind when we consider them. In particular, for readers uncomfortable with the lack of precision in defining a functional density, we emphasize that our results can also be understood on a lattice, though input symmetries may be discrete subgroups of those existing in the continuum limit. Furthermore, any concrete ML application involves a finite set of inputs, and for any fixed application in physics or ML one can simply choose the spacing between lattice points to be smaller than the experimental resolution.

20

