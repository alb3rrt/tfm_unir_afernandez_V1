
# Optimization Variance: Exploring Generalization Properties of DNNs

[arXiv](https://arxiv.org/abs/2106.01714), [PDF](https://arxiv.org/pdf/2106.01714.pdf)

## Authors

- Xiao Zhang
- Dongrui Wu
- Haoyi Xiong
- Bo Dai

## Abstract

Unlike the conventional wisdom in statistical learning theory, the test error of a deep neural network (DNN) often demonstrates double descent: as the model complexity increases, it first follows a classical U-shaped curve and then shows a second descent. Through bias-variance decomposition, recent studies revealed that the bell-shaped variance is the major cause of model-wise double descent (when the DNN is widened gradually). This paper investigates epoch-wise double descent, i.e., the test error of a DNN also shows double descent as the number of training epoches increases. By extending the bias-variance analysis to epoch-wise double descent of the zero-one loss, we surprisingly find that the variance itself, without the bias, varies consistently with the test error. Inspired by this result, we propose a novel metric, optimization variance (OV), to measure the diversity of model updates caused by the stochastic gradients of random training batches drawn in the same iteration. OV can be estimated using samples from the training set only but correlates well with the (unknown) \emph{test} error, and hence early stopping may be achieved without using a validation set.

## Comments

Work in progress

## Source Code

Official Code

- [https://github.com/ZhangXiao96/OptimizationVariance](https://github.com/ZhangXiao96/OptimizationVariance)

Community Code

- [https://paperswithcode.com/paper/optimization-variance-exploring-1](https://paperswithcode.com/paper/optimization-variance-exploring-1)

## Bibtex

```tex
@misc{zhang2021optimization,
      title={Optimization Variance: Exploring Generalization Properties of DNNs}, 
      author={Xiao Zhang and Dongrui Wu and Haoyi Xiong and Bo Dai},
      year={2021},
      eprint={2106.01714},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```

## Notes

Type your reading notes here...

