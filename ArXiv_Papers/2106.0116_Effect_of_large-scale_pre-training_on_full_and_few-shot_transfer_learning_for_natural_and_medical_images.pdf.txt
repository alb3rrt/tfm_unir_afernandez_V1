Effect of large-scale pre-training on full and few-shot transfer learning for natural and medical images

arXiv:2106.00116v1 [cs.LG] 31 May 2021

Mehdi Cherti Helmholtz AI Juelich Supercomputing Center Juelich, Germany m.cherti@fz-juelich.de

Jenia Jitsev Helmholtz AI Juelich Supercomputing Center Juelich, Germany j.jitsev@fz-juelich.de

Abstract
Transfer learning aims to exploit pre-trained models for more efficient follow-up training on wide range of downstream tasks and datasets, enabling successful training also on small data. Recent line of work posits strong benefits for model generalization and transfer when model size, data size, and compute budget are increased for the pre-training. It remains however still largely unclear whether the observed transfer improvement due to increase in scale also holds when source and target data distributions are far apart from each other. In this work we conduct largescale pre-training on large source datasets of either natural (ImageNet-21k/1k) or medical chest X-Ray images and compare full and few-shot transfer using different target datasets from both natural and medical imaging domains1. Our observations provide evidence that while pre-training and transfer on closely related datasets do show clear benefit of increasing model and data size during pre-training, such benefits are not clearly visible when source and target datasets are further apart. These observations hold across both full and few-shot transfer and indicate that scaling laws hinting improvement of generalization and transfer with increasing model and data size are incomplete and should also take into account the degree of how distinct the source and target data distributions are, to correctly predict effect of model size and data size variation during pre-training on transfer.
1 Introduction
Re-using models obtained by pre-training on available source datasets to improve learning performance on further target datasets is core idea behind transfer learning approaches. Transfer learning has a long history in machine learning [1, 2] and was also employed already at the very early rise of deep neural networks in the vision domain [3, 4]. Architectures like AlexNet [5], OverFeat [6], and VGG [7] were pre-trained on supervised tasks using ImageNet-1k, a publicly available natural image dataset that contains about 1.4 Million images and 1000 classes [8, 9]. After pre-training, the resulting models were taken as off-the-shelf generic features reservoirs and re-used by re-training, or fine-tuning, on multitude of different downstream target datasets and tasks, including classification, object detection, and segmentation [3, 4]. Importantly, the transfer approach allowed to improve performance on target datasets when compared to training from scratch with randomly initialized weights [10, 11]. Further, it enabled to train models of good quality also on comparatively small amounts of data, in contrast to large amounts usually required when training a deep neural network from scratch.
Pre-training sets the stage for the quality of the model, as representations formed during pre-training determine model's ability to generalize across conditions not seen during pre-training. Models with
1Repository for reproducing the experiments will be made available via link in the supplementary material.
Preprint. Under review.

stronger generalization capability can be then naturally expected to be better in transfer learning. Recent line of work from language modeling using very large Transformer based networks trained in self-supervised manner on very large text datasets found scaling laws for generalization, which improves when increasing model, dataset size, and compute budget for the training [12]. When pre-trained on very large scales, such models show consequently much stronger transfer performance on a broad range of novel tasks, compared to pre-trained models of smaller scale [13]. In the same line, experimental studies on large-scale pre-training and transfer in image domain found evidence that increasing network model and data size during pre-training results in transfer performance benefits [14, 15].
The majority of the large-scale studies looking at transfer improvement when increasing model and data size during pre-training use similar types of datasets for source and transfer, for instance those containing natural images of different kind. However, it remains unclear whether observed transfer improvements due to larger model and data sizes will still uphold when source and target datasets are not closely related. To address this question, we conduct a series of large-scale pre-training and transfer experiments where we vary not only model (using ResNet backbone architecture [16, 14]) and dataset size during pre-training, but also the degree of relatedness between the source and the target datasets, by taking either large natural image datasets (ImageNet-1k, ImageNet-21k [8]) or medical chest X-Ray imaging datasets (CheXpert [17], MIMIC-CXR [18], PadChest [19], NIH Chest X-ray14 [20]) as source and transferring to either natural image or medical imaging datasets as target. For transfer, we also vary the operation in either full or few-shot regime, where only few examples per class are shown to the pre-trained models during fine-tuning on a target dataset. As large-scale pre-training requires heavy computational resources, we make use of a state-of-the art supercomputer (JUWELS Booster [21]) tailored for distributed training to conduct our experiments.
The obtained results showing dependence of transfer performance on model and data size in the pretraining and on alignment of source and target data types suggest a differentiated picture. For closely related natural-natural or medical-medical image source and target datasets, we observe improvement in transfer when increasing model and data size during pre-training in agreement with previous reports. For natural-medical source and target datasets that are further apart, we do not observe a clear improvement in transfer performance across conditions when increasing pre-training model and data size. In the few-shot transfer regime, increasing model or data size in the pre-training do not produce measurable transfer improvement. In the full-shot transfer regime, improvement on larger PadChest target data becomes evident with larger pre-training data and model size, the improvement being though small. There, we also observe the largest model pre-trained on ImageNet-21k being on par with best models pre-trained on medical imaging source data. We discuss the implications of our findings which indicate that the scaling laws, previously observed mostly for training and testing on closely related datasets, suggesting benefits for transfer from larger model and data scale, are incomplete and have to be expanded by including dependency on data type and degree of alignment between the source and target data.
2 Background and related work
Scaling laws for generalization and transfer. Strong evidence that increasing model and data size for the training may result in steady improvement of generalization comes from language modeling studies systematically looking on the dependency of test error on model, data size, and compute budget used for training [13, 12, 22]. The experiments conducted there set up scaling laws with a power law shape and show consistent further decrease of test error when further increasing model, data size, and compute budget over many orders in magnitude hand in hand. For images, a similar line of work by Henighan et al. shows a decrease of test classification top-1 error when fine-tuning on ImageNet pre-trained generative image models of increasing size [23]. Those works use selfsupervised training of autoregressive models, in language modeling performed on text and in image domain on image patches, employing transformer networks as a backbone. Additional backup for this line of work comes from studies that revise the dependency of generalization performance on model, data size, and epoch number during training and report double or multi descent curves for the test error [24, 25, 26]. There, keeping on increasing model, data size or training time substantially also shows continuous drop in the test error pointing to generalization improvement, for instance when crossing the interpolation threshold and transiting into the over-parameterized regime by scaling up the model size.
2

Improving transfer by scaling up pre-training. Also improvement in transfer on downstream datasets and tasks is strongly evident from large-scale language modeling experiments. In the study by Brown et al. [13], large transformer networks in the order of hundred billions of parameters (GPT-3) pre-trained on large text datasets in the order of billions of sentences were shown to have much stronger transfer performance than smaller GPT network models, measured by the test error on different downstream tasks. The difference in transfer performance between different sized models was especially pronounced in the very low data regime when doing zero- or few-shot transfer with only few examples available during fine-tuning. Further systematic study on transfer improvements induced by increasing scale was done by Hernandez et al. [22], who examined scaling laws for transfer on language modeling tasks in the low-data regime, being defined as transfer using less than 10% of the available target data. The authors have shown that increasing model size in the pre-training decreases test error on the target data, emphasizing that in the low-data regime the same kind of test error improvement cannot be observed without pre-training and transfer when increasing model size and training instead directly from scratch on the target. It was also pointed out that the degree of proximity between the source and the target dataset plays a role when predicting effect of the scale on the performance and the efficiency of transfer. This additional dependency appeared in the measure of efficient transferred data and in a revised version of the scaling law predicting test loss after fine-tuning on target data as introduced in this work.
In the image domain, the performed studies on transfer improvement due to scale have still far less systematic character. Models and datasets used for training on images are 3-4 orders of magnitude behind those studied in language modeling [13]. Recently, number of works were starting to employ datasets like ImageNet-21k [8], YFCC-100M [27] and JFT-300M [28] that are larger than standard ImageNet-1k to pre-train large network models on large data and observe the effect of scaling up on transfer. The work on Big Transfer by Kolesnikov et al. [14] performed supervised classification based pre-training on ImageNet-1k, ImageNet-21k, and JFT-300M using different sized deep residual networks (ResNets [16]) to study the performance of pre-trained models on transfer across different target datasets. They found consistent improvement in transfer performance when using larger models and larger data during pre-training. In the same direction, work by [15] pre-trained different sized network models on ImageNet-1k and ImageNet-21k, also observing consistent improvement in measured transfer performance when scaling-up model and data size during pre-training.
Studies mentioned above deal with closely related source and target datasets containing natural images type data. Various works related to testing transfer performance across different target datasets in general often employ targets that are rather close to source data as used in pre-training. Examples are like studies introducing transfer benchmarking datasets that use targets resembling mostly natural image domain [10, 29] or domain specific transfer studies that stay within their given domain, for instance in medical imaging [30].
Only few studies so far attempt to measure transfer performance between datasets that are further apart, for instance natural and medical images, while systematically varying model and data size during pre-training. Work done by Raghu et al. [31] has found no significant difference between models pre-trained on ImageNet-1k and models trained from scratch on target datasets containing medical images. However, it was not using datasets larger than standard ImageNet-1k or networks larger than standard ResNet-50 for pre-training. Another work examines transfer on CheXpert while varying network model size during pre-training [32]. It does find slight benefit for transfer when pre-training with larger models, however it does not vary source data size in the pre-training, using only standard ImageNet-1k as a source. A recent study by Mustafa et al. builds up on Big Transfer work [14] and compares transfer performance of different sized ResNet network models pre-trained on ImageNet-1k, ImageNet-21k, and JFT-300M on different medical imaging target datasets [33]. Slight evidence for transfer improvement was observed when using larger model and larger dataset sizes during pre-training, with inconsistencies across conditions and datasets, where in some cases no significant benefit from larger pre-training scale was seen. The work does not compare models pre-trained on natural images data to models pre-trained on medical imaging data when measuring transfer performance on medical imaging targets.
3 Experiments & Results
In order to test the impact of model and data size during large-scale pre-training on transfer performance in full and few-shot regime under different source and target data type constellations, we
3

conducted experiments on pre-training different sized ResNet models on supervised classification using either large natural image datasets ImageNet-1k or ImageNet-21k, or chest X-Ray medical imaging datasets CheXpert, MIMIC-CXR, PadChest, and NIH Chest X-ray14 (see Suppl. Tab. 3 for comprehensive list and further details of datasets). The pre-trained models were then fine-tuned on different target datasets that contain either natural or medical images. In the following, we describe the experimental procedures and outcomes in more detail.
3.1 Large-scale pre-training
For pre-training, we largely followed the training procedure and used the network architecture of [14]. More concretely, we pre-trained both ResNet-50x1 and ResNet-152x4 (in following R50x1 and R152x4) from [14] on different natural image and medical datasets. Smaller R50x1 has 26M weight parameters, while larger R152x4 has 928M parameters. This substantial difference in size allows us to compare the effect of model scaling in the pre-training on subsequent transfer. The following describes the training procedure and hyper-parameters used for natural image and medical image domain.
3.1.1 Natural image domain
For natural images, we pre-trained the two models (R50x1 and R152x4) on ImageNet-1k ( 1.4 Millions images) and the much larger full ImageNet-21k ( 14 Millions images). For ImageNet-1k models, we used a standard supervised classification setup with softmax as an output activation and cross entropy as a loss. In ImageNet-21k, some images have several labels, for simplicity and to be consistent with ImageNet-1k setup, for each image x with multiple labels, we construct as many training pairs (x, y) as there are labels for x, where the same image x is repeated in different training pairs and is associated each time to one of its labels, simulating a noisy classification setup similar to [34]2. As a result, the ImageNet-21k models are trained using the same setup as ImageNet-1k.
We followed the training hyper-parameters of [14], with the difference that we used stochastic gradient descent (SGD) with adaptive gradient clipping (AGC) from [35], as we found that it helps both pre-training and transfer. With AGC, we found that the default base learning rate used in [14] made training unstable for the ImageNet-21k experiments, so we reduced it from 0.03 to 0.01, but otherwise we used a base learning rate of 0.03. The rest of the hyper-parameters are similar, namely we used a momentum of 0.9, 90 epochs,  5000 warmup iterations, a batch size of 4096, the linear learning rate rescaling rule of [36], and the standard step-wise learning rate schedule for ImageNet [14]. For data augmentation, we used the standard random resized crop data augmentation as in [14]. In ImageNet-1k experiments, we additionally used RandAugment [37] and changed the learning rate schedule from step-wise to cosine annealing [38], as it improved the results. In order to speedup training, we used data parallel training with Horovod [39], using 256 A100 GPUs for R152x4 models and 128 A100 GPUs for R50x1 models. A pre-training on ImageNet-21k with large R152x4 takes about 81 hours using 256 GPUs, while with small R50x1 it needs about 13.5 hours to finish using 128 GPUs on JUWELS Booster supercomputer [21] (see Suppl. Sec. A and Suppl. Fig. 4 for more details on distributed training)
3.1.2 Medical image domain
For medical data, we pre-trained the two models (R50x1 and R152x4) on a concatenation of several medical datasets, which we note CheXpert-MIMIC-NIH-PadChest. The largest source dataset combination contains about 873K X-Ray chest radiogpraphs (see also Suppl. Tab. 3). The medical datasets are multi-label, as each image can be associated to several diseases. The datasets are concatenated by finding intersecting labels (diseases) and using the intersected labels as a target. In order the study the effect of data size as we progressively add more data and different data sources, we also pre-trained the two models (R50x1 and R152x4) on CheXpert only, on CheXpert-MIMIC, and on CheXpert-MIMIC-NIH. For processing the datasets and extracting the labels from raw data, we used TorchXRayVision [40] from the work of [30].
2In both setups (ours and the one in [34]), for a given image x, cross entropy loss is minimized when we predict a uniform probability distribution over the labels of x, and 0 probability for the rest of the labels.
4

We followed the literature on medical datasets [30] and pre-trained using a multi-label setup where we have independent binary tasks, one for each label (disease), and we used sigmoid as an output activation function and binary cross entropy as a loss for each label.
We used the same hyper-parameters as in the natural image domain, except that the base learning rate was set to 0.01 instead of 0.03, as we found that a learning rate of 0.03 led to more overfitting. We followed [30] and used a center crop based on the smallest side, then resized the image to 224 × 224. In order to combat overfitting, we used data augmentation from [30], which included random translation, random rotation, and random scaling. In addition to the data augmentation used in [30], we also do random horizontal flipping. In order to speedup training, we used data parallel distributed training with Horovod [39], using 64 A100 GPUs in all pre-training setups.
3.2 Fine-tuning and transfer evaluation
For fine-tuning, we used the BiT-HyperRule[14], which is a heuristic that selects fine-tuning hyperparameters (learning rate schedule, resolution, usage of MixUp, and total number of steps) based on training set size and image resolution. We used a batch size of 128, and an initial learning rate of 0.001 on all experiments. Like in [14], we do not use weight decay. Like in pre-training, we used stochastic gradient descent (SGD) with adaptive gradient clipping (AGC), as we found it to improve few-shot results. We used a momentum of 0.9. In each experiment, the classification head of the pre-trained model was replaced with a new classification head for the fine-tuning task. We fine-tuned all the layers of the network. For each experiment, we performed 5 independent runs with different seeds to have an estimate of the variance of the performance. We ran each fine-tuning experiment on a single A100 GPU.
As in [14], we consider two kinds of setups, few-shot setups (we used 1 or 5 or 10 or 100 or 500 examples per class) and fine-tuning on the full training set. We used CIFAR-10, CIFAR-100 [41], Flowers-102 [42], and Oxford-IIIT Pet [43] for natural image fine-tuning. We used COVIDx [20], Tuberculosis [44], and PadChest-classification (in following PadChest-cl) for medical image finetuning (see also Suppl. Tab. 3). PadChest-cl is a dataset we constructed from PadChest, where we keep only images where exactly one label is present (disease), so that we could easily perform few-shot experiments like with the other datasets. For Flowers-102 and COVIDx, since the datasets are imbalanced, we use oversampling. We report the final accuracy on the test sets, except for PadChest-Cl where we report the mean AUC over all labels to be comparable to previous studies.
3.3 Results.
The experiments allow us to look on transfer performance following pre-training when varying model size, source data size and source and target dataset alignment. In following we report on the results obtained from experiment runs.
Transfer from natural to natural domain. The results we obtain after pre-training on either ImageNet-1k or much larger ImageNet-21k with either R50x1 or much larger R152x4 network provide a consistent picture showing transfer improvement when increasing either model or data size during pre-training, which is in line with previous work (Tab. 1). On CIFAR-10 and CIFAR-100, which are generic natural images target sets, we see the improvement both due to larger model and data scale in both full and few-shot transfer (Fig. 1a). The same is valid for Flowers-102 (Suppl. Fig. 5a), which is a smaller and more specialized, fine-grained dataset containing only a narrowly defined image category. Remarkably, in the very low data regime, 1- or 5-shot transfer, the transfer improvement is especially strong, reaching in some cases 20% - 30% absolute difference in test accuracy in favor of larger scale (Figs. 1a, 3a). Difference becomes smaller towards full shot transfer regime but is still significant. The observed variance is larger for few-shot transfer experiments, which may suggest less stable fine-tuning in those cases where model has to adapt to target data based on only very limited number of examples.
Transfer from medical to medical imaging domain. When pre-training on medical imaging source data, we cannot afford to increase data scale as strongly as in natural domain. Total size of dataset obtained by concatenating all available X-Ray chest imaging sets is still less than ImageNet-1k and more than order of magnitude below ImageNet-21k. Moreover, the increase in data size from smallest to largest is about 4 fold (when starting with CheXpert) for medical imaging domain, while in natural domain it is about 10 fold. Still, by increasing data size we observe significant transfer improvement
5

Table 1: Comparing transfer for ImageNet-1k and ImageNet-21k pre-training with different

sized ResNets (1) - Top-1 Acc [%] metric; (2) - mean AUC metric. Bold indicates best transfer

performance and pinpoints the effect of scaling on transfer. Italics indicates transfer performance with

no significant difference between data scale. Clear transfer improvement emerges for natural-natural

due to both model and data scale, while no such clear improvement is observed for natural-medical

scenario.

Target Data Type

Dataset

ResNet-50x1

1K

21K

ResNet-152x4

1K

21K

Natural

CIFAR-10(1) CIFAR-100(1) Flowers-102 (1)
Pets (1)

94.26 ± 0.04 75.90 ± 0.05 74.94 ± 0.94 85.21 ± 0.54

95.78 ± 0.09 82.47 ± 0.20 98.21 ± 0.21 87.23 ± 0.17

96.93 ± 0.05 83.90 ± 0.08 89.41 ± 0.24 93.32 ± 0.28

97.82 ± 0.07 88.54 ± 0.13 99.49 ± 0.08 93.21 ± 0.13

Medical

PadChest-Cl(2) COVIDx(1)
Tuberculosis(1)

80.17 ± 0.16 82.03 ± 0.16 76.30 ± 1.30 78.35 ± 1.53 79.83 ± 1.42 83.47 ± 0.78

80.84 ± 0.14 83.31 ± 0.99 78.10 ± 0.89 78.90 ± 0.46 81.49 ± 2.11 80.83 ± 2.36

Table 2: Comparing transfer for different sized medical imaging source pre-training with dif-

ferent sized ResNets (1) - Top-1 Acc [%] metric; (2) - mean AUC metric. "+" indicates addition into

a successively larger source set. Clear transfer improvement is evident due to both model and data

scale across target datasets.

Dataset

CheXpert

ResNet-50x1

+MIMIC

+ NIH

+PadChest

CheXpert

ResNet-152x4

+MIMIC

+NIH

PadChest-Cl(2) COVIDx(1)
Tuberculosis(1)

73.01 ± 0.12 68.50 ± 0.17 79.83 ± 0.43

78.44 ± 0.04 75.10 ± 1.43 78.84 ± 1.18

78.33 ± 0.07 75.60 ± 0.43 81.32 ± 0.70

-- 76.05 ± 0.20 81.65 ± 0.85

81.79 ± 0.06 78.65 ± 0.79 79.01 ± 0.43

83.14 ± 0.04 81.65 ± 0.70 84.63 ± 0.70

82.69 ± 0.05 80.80 ± 1.03 87.93 ± 0.70

+PadChest
-- 83.00 ± 1.09 90.91 ± 0.78

in full shot regime across all medical imaging target datasets (Tab. 2). This improvement is strongly pronounced for COVIDx (Fig. 2b) and is also there in comparable form on Tuberculosis dataset (Suppl. Section B) and to lesser extent on PadChest (Figs. 2c, 1b). Also by increasing model size we get transfer improvement that is pronounced across all targets in full shot regime (Tab. 2, Figs. 2b, 2c, 1b). Improvement in few-shot regime is not consistently observed when varying model or data size (Figs. 1b, 3c, Suppl. Fig. 6) Especially in very low data regime, there is no significant difference in transfer performance - the observed variance there is large. Increasing number of shots and approaching full shot regime, the improvement due to scale becomes more and more visible. This is different from natural-natural transfer scenario, where few-shot transfer shows striking improvement.
Transfer from natural to medical imaging domain. Here we transfer on medical imaging targets after pre-training on natural sources of different size, ImageNet-1k or much larger ImageNet-21k. We do not observe differences in transfer performance due to different model or data size on small data targets (Tuberculosis, COVIDx, see Tab. 1). For larger data target, PadChest, there is significant improvement due to larger data size for full shot transfer (Figs. 1b, 2c, Tab. 1). Larger model size also leads to improvement there, being however very small. When comparing transfer performance to models pre-trained on medical imaging, in most cases medical imaging pre-trained models outperform models pre-trained on natural images. In the few-shot regime, there are cases where ImageNet pretrained models are also on par with models pre-trained on medical imaging, which can be seen on COVIDx and Tuberculosis targets (Figs. 3b, 3c and Suppl. Section B). For PadChest, in the full shot regime largest model pre-trained with ImageNet-21k is on par with best medical image pre-trained models (Figs. 2c, 1b), while in few-shot regime ImageNet pre-trained models are consistently worse than models pre-trained on medical images there. Again, variance observed in few-shot regime is large, and is getting smaller and smaller when increasing number of shots and moving towards full shot transfer.
4 Discussion & Conclusion
Our observations of the transfer performance dependency on the model and data size during pretraining and on alignment of source and target dataset types are not compatible with a straightforward picture of larger pre-training scale being always beneficial for transfer. More differentiated picture
6

Test Accuracy Test Mean AUC

R50x1 on ImageNet-1k

R50x1 on ImageNet-21k

R152x4 on ImageNet-1k

80

R152x4 on ImageNet-21k

60

40

20

R50x1 on CheXpert

R50x1 on CheXpert-MIMIC

R50x1 on CheXpert-MIMIC-NIH

80

R152x4 on CheXpert R152x4 on CheXpert-MIMIC

R152x4 on CheXpert-MIMIC-NIH

R50x1 on ImageNet-1k

R50x1 on ImageNet-21k

70

R152x4 on ImageNet-1k R152x4 on ImageNet-21k

60

50

40

0 1 Shot

5 Shot

1S0hSohtsotduring t1ra0n0sSfehort

500 Shot

Full Shot

30

1 Shot

5 Shot Shots d1u0riSnhgottransfer 100 Shot

Full Shot

(a) CIFAR-100

(b) PadChest-Cl

Figure 1: Few-shot and full shot transfer performance on target datasets when varying model size and dataset size in pre-training.

Pre-training dataset

88

ImageNet-1k ImageNet-21k

86

84

82

80

78

76 R50x1

Pre-training Model architecture

R152x4

84

Pre-training dataset ImageNet-1k

82

ImageNet-21k CheXpert

CheXpert-MIMIC-NIH-PadChest

80

78

76

74

72

70

68 R50x1

Pre-training Model architecture

84

82

80

78

76

74

R152x4

R50x1

Pre-training dataset ImageNet-1k ImageNet-21k CheXpert CheXpert-MIMIC-NIH

Pre-training Model architecture

R152x4

(a) CIFAR-100

(b) COVIDx

(c) PadChest-Cl

Figure 2: Full shot transfer performance on target datasets when varying model and source data size,

taking the smallest and largest pre-training datasets available on each domain.

Test Accuracy Test Accuracy Test Mean AUC

we obtain points to yet other factors, like the type and proximity of source and target datasets, that play a decisive role in determining whether larger pre-training scale will or will not result in transfer improvement.
Larger pre-training scale improves intra-domain transfer. We obtain evidence that either naturalnatural domain transfer or medical-medical domain transfer can be improved when increasing model and data size during pre-training (Tabs. 1, 2). This improvement is strongly evident for natural-natural transfer scenario. There, either increasing data size by using ImageNet-21k instead of ImageNet-1k or increasing network model size by using ResNet-152x4 instead of smaller ResNet-50x1 creates strong, consistent boost in transfer performance across all natural target datasets, which is in line with previous observations [14, 15]. Transfer improvement is strongly pronounced in few-shot regime (e.g, Figs. 1a, 3a), also adding evidence for more data efficient transfer due to larger scale. The situation is less clear for medical-medical transfer scenario. There, transfer improvement due to larger pre-training scale is given for full transfer, without clear benefits for few-shot transfer that is so strongly boosted in natural-natural scenario. This hints that nature of dataset domain itself may play an important role in determining how transfer improvement will be affected by pre-training scale.
Larger pre-training scale has no clear benefit for inter-domain transfer. In contrast to intradomain transfer, where natural source and target datasets are in close proximity, we do not observe such overall improvement when increasing model and data size during pre-training for inter-domain transfer (Tab. 1), with natural source and medical imaging target domains being further apart. For smaller medical imaging targets, COVIDx and Tuberculosis, neither full transfer nor few-shot transfer using models pre-trained on natural images show any substantial differences in performance when comparing different model and data size scales used during pre-training. For larger medical imaging target, PadChest, no transfer improvement due to larger scale is evident in few-shot regime, while in
7

Test Accuracy Test Accuracy Test Accuracy

90 80 70 60 50 40 30 20
1 Shot

5 Shot

S1h0oSthsodturing 1tr0a0nSsfheort

Model R50x1 on ImageNet-1k R50x1 on ImageNet-21k R152x4 on ImageNet-1k R152x4 on ImageNet-21k
500 Shot Full Shot

85

Model

R50x1 on ImageNet-1k

80

R50x1 on ImageNet-21k R152x4 on ImageNet-1k

R152x4 on ImageNet-21k

75

70

65

60

55

50 1 Shot

5 Shot

S1h0oSthsodturing 1tr0a0nSsfheort

500 Shot

Full Shot

80

70

60

50

Model

40

R50x1 on CheXpert R50x1 on CheXpert-MIMIC-NIH-PadChest

R152x4 on CheXpert

R152x4 on CheXpert-MIMIC-NIH-PadChest

1 Shot

5 Shot

S1h0oSthsodturing 1tr0a0nSsfheort

500 Shot

Full Shot

(a) CIFAR-100, natural source

(b) COVIDx, natural source

(c) COVIDx, medical source

Figure 3: Few-shot and full shot transfer performance on targets when we pre-train using different model sizes on different sources (natural or medical datasets).

full shot regime, both larger data and model size lead to better transfer (Figs. 2c, 1b), albeight this improvement being small.
Interestingly, the largest R152x4 model pre-trained on ImageNet-21k shows there in full-shot regime transfer performance that equals the best models pre-trained on large collection of medical imaging source data (Figs. 2c, 1b). This provides evidence that it is also possible to use large generic source data far from domain specific target data to pre-train models that can provide high quality transfer, comparable to transfer quality available otherwise only when having access to large quantities of domain specific source data, which is often not the case in practice in contrast to our experimental setting.
Lack of evidence on benefits of scaling in other conditions does not mean that further scaling up would not change this picture, as for instance we could not use substantially larger datasets like JFT300M [28, 14] which are proprietary and not available publicly. However, it is as well conceivable that there may be also fundamental limitations prohibiting transfer improvement no matter how large pre-training scale may become. Such limitation may for instance have roots in the degree of incompatibilty between source and target datasets which, when becoming large enough, may impose a strict limit on amount of information that can be possibly re-used on target after pre-training on source. If this amount is small because of inherent incompatibilty between the datasets, increasing pre-training scale will be of no use, as all possibly transferable information from source would be already extracted at far smaller scales, without any further benefit to be expected for instance from larger source data sizes. There is some evidence from language modeling studies that hints on such fundamental limitations for transfer improvement on target datasets far from source when doing straightforward scaling without further changes in model architecture. For instance, the work by Hendrycks et al. [45] finds no improvement of transfer when increasing size of large Transformer networks pre-trained on very large text datasets and fine-tuning those on target dataset far apart from the source, containing mathematical text tasks of advanced difficulty.
Limitations of the current study. There are several limitations of the current study that impede more general conclusions about relation between pre-training scale, source and target dataset proximity and transfer performance from the observations that we have made here. In the conducted transfer experiments, we made use of a heuristic hyper-parameter selection rule - hyper-rule, as introduced in [14] - that derives values of pre-training hyper-parameters directly from target datasets on which transfer is to be performed. This rule may be heavily biased towards transfer on natural image datasets, as those were the targets used in the original study. If modifying the rule to take also dataset domain type - natural or medical - into account, the derived hyper-parameters may serve a much better basis for fine-tuning during transfer. In general, performing hyper-parameter tuning for training procedure can strongly boost performance [46], and this is no different for transfer procedure. Therefore, it cannot be excluded that performing hyper-parameter tuning for each transfer task would alter the effect of larger pre-training scale on transfer. Hyperparameter tuning would however also impose further cost on transfer that is avoided by employing the hyper-rule.
We also have not explored other backbone network architectures except the standard ResNet. Although ResNet has proven itself a versatile network architecture for dealing with multitude of various vision tasks, it cannot be ruled out that while for instance the inductive bias inherent to its convolutional design is well suited for working on natural image statistics with strong local spatial correlations,
8

it may be less suited for providing good basis for generalization when dealing with other types of image signals. Scaling up ResNet architecture may thus be a viable strategy to improve generalization capability on natural image data, while other, more generic architectures, may be required to benefit from scaling in the same way across more diverse data types. We also did not experiment with larger datasets than ImageNet-21k - as those are still mostly proprietary and were not publicly available, as it is the case for JFT-300M [28, 14]. For medical imaging domain, we could not experiment with increasing data scale substantially, as amount of available data is currently still limited in general. Finally, we studied the dependence of transfer improvement on pre-training scale exclusively in supervised classification problem setting. Promising work is also done on pre-training in unsupervised fashion with unlabeled data [47, 48, 49], where benefits of scaling up pre-training may turn out to be less dependent on source and target data proximity.
Conclusion and outlook. To summarize, we presented here evidence that scaling up model and data size for pre-training does not necessarily result in improvement of transfer performance on target datasets - the obtained transfer performance depends in addition on type and proximity of the source and target datasets. Transfer improvement due to scaling-up pre-training was found to be substantial in natural-natural or medical-medical, intra-domain transfer scenarios where source and target datasets were closely related, being especially strongly pronounced in the few-shot transfer regime for natural-natural case and concentrated in full-shot scenario in medical-medical case. In contrast to intra-domain transfer, no such clear improvements due to larger pre-training scale were observed in natural-medical inter-domain transfer. As an exception for inter-domain case, we have observed slight improvement due to larger data and model size in full-shot transfer on PadChest, the larger medical imaging target in our study. There, the largest model pre-trained on ImageNet-21k was also on par with best models pre-trained on medical imaging source data. This indicates that when performing large-scale pre-training on generic source data far from target, it is still possible to reach high quality model performance on target via transfer that is otherwise only available with large volumes of domain-specific, target compatible source data that is often hard or impossible to obtain.
The study offers different follow-up directions. Besides further experiments in scaling up data size beyond ImageNet-21k, combining in the pre-training different source datasets that may contain both natural and medical images, taking into account other architectures than standard ResNet as network model backbone or employing various unsupervised learning strategies for pre-training [47, 48, 49] instead of supervised learning, another fruitful path is to provide a measure of dataset and task proximity to allow systematic variation of relatedness between source and target domains. First steps in this direction for language modeling was already undertaken in [22]. Following these directions would also allow to posit scaling laws for transfer in the image domain and extend those to include dependency on type and on proximity of the source and target domains, enabling systematic prediction of transfer performance and improvement due to increase of pre-training scale.
Broader and Social Impact
Our work deals with transfer learning, which has the aim to improve learning and performance for a given target task by re-using models that have already learned to perform well on other, more or less, related tasks before. Advancing transfer learning can make existing learning algorithms perform better and more efficient, using less compute and data to learn solutions for the tasks of interest. The approach to improve transfer learning by increasing scale of the pre-training is generic and has impact far beyond vision domain, for instance in language modeling, and is not bound to any specific application. As any generic method, it can be therefore applied to enhance technologies for sensitive applications, for instance in health domain or in public surveillance, that may have both strong positive and negative social impact, depending on policies introduced on their usage. Special care should be taken about applications in clinical domain where further development of diagnostic tools based on data driven machine learning should be accompanied by a broad panel of experts from corresponding domains. The method depends on computationally heavy large-scale pre-training that is energy demanding on the one hand. On the other hand, it contains a promise to pay off the energy budget put into training by obtaining generic models that can be very efficiently adapted to a large range of problems, saving computational and energy costs that would otherwise incur for their solution.
9

Acknowledgments and Disclosure of Funding
We would like to express gratitude to all the people who are working on making code, models and data publicly available, advancing community based research and making research more reproducible. Special thanks go to creators and maintainers of open available X-Ray medical imaging datasets, some of it gathered under the COVID-19 pandemic situation, that also enabled our research. The authors gratefully acknowledge the Gauss Centre for Supercomputing e.V. (www.gauss-centre.eu) for funding this work by providing computing time through the John von Neumann Institute for Computing (NIC) on the GCS Supercomputers JUWELS, JUWELS Booster at Jülich Supercomputing Centre (JSC). We also acknowledge computing resources from the Helmholtz Data Federation and further computing time provided on supercomputer JUSUF in frame of offer for epidemiology research on COVID-19 by JSC.
References
[1] Lorien Y Pratt, Jack Mostow, Candace A Kamm, and Ace A Kamm. Direct transfer of learned information among neural networks. In AAAI, volume 91, pages 584­589, 1991.
[2] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on knowledge and data engineering, 22(10):1345­1359, 2009.
[3] A. S. Razavian, H. Azizpour, J. Sullivan, and S. Carlsson. CNN features off-the-shelf: An astounding baseline for recognition. In Proc. IEEE Conf. Computer Vision and Pattern Recognition Workshops, pages 512­519, June 2014.
[4] Hossein Azizpour, Ali Sharif Razavian, Josephine Sullivan, Atsuto Maki, and Stefan Carlsson. Factors of transferability for a generic convnet representation. IEEE transactions on pattern analysis and machine intelligence, 38:1790­1802, September 2016.
[5] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097­1105, 2012.
[6] Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, and Yann LeCun. Overfeat: Integrated recognition, localization and detection using convolutional networks. In 2nd International Conference on Learning Representations, ICLR 2014, 2014.
[7] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR, San Diego, CA, USA, 2015.
[8] J. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Proc. IEEE Conf. Computer Vision and Pattern Recognition, pages 248­255, June 2009.
[9] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211­252, 2015.
[10] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. A large-scale study of representation learning with the visual task adaptation benchmark. arXiv preprint arXiv:1910.04867, 2019.
[11] Thomas Mensink, Jasper Uijlings, Alina Kuznetsova, Michael Gygli, and Vittorio Ferrari. Factors of influence for transfer learning across diverse appearance domains and task types. arXiv preprint arXiv:2103.13318, 2021.
[12] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.
[13] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
10

Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 1877­1901. Curran Associates, Inc., 2020.

[14] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. Big transfer (bit): General visual representation learning. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors, Computer Vision ­ ECCV 2020, pages 491­507, Cham, 2020. Springer International Publishing.

[15] Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. Imagenet-21k pretraining for the masses. arXiv preprint arXiv:2104.10972, 2021.

[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR), pages 770­778, June 2016.

[17] Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 590­597, 2019.

[18] Alistair E. W. Johnson, Tom J. Pollard, Seth J. Berkowitz, Nathaniel R. Greenbaum, Matthew P. Lungren, Chih-Ying Deng, Roger G. Mark, and Steven Horng. Mimic-cxr, a de-identified publicly available database of chest radiographs with free-text reports. Scientific data, 6:317, December 2019.

[19] Aurelia Bustos, Antonio Pertusa, Jose-Maria Salinas, and Maria de la Iglesia-Vayá. Padchest: A large chest x-ray image dataset with multi-label annotated reports. Medical image analysis, 66:101797, December 2020.

[20] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald Summers. Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. In IEEE Conference on Computer Vision and Pattern Recognition(CVPR), pages 3462­3471, 2017.

[21] Juelich Supercomputing Center.

JUWELS Booster Supercomputer, 2020.

https://apps.fz-juelich.de/jsc/hps/juwels/configuration.html#

hardware-configuration-of-the-system-name-booster-module.

[22] Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. Scaling laws for transfer. arXiv preprint arXiv:2102.01293, 2021.

[23] Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B Brown, Prafulla Dhariwal, Scott Gray, Chris Hallacy, Benjamin Mann, Alec Radford, Aditya Ramesh, Nick Ryder, Daniel M. Ziegler, John Schulman, Dario Amodei, and Sam McCandlish. Scaling laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701, 2020.

[24] Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machinelearning practice and the classical bias-variance trade-off. Proceedings of the National Academy of Sciences of the United States of America, 116:15849­15854, August 2019.

[25] Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep double descent: Where bigger models and more data hurt. In International Conference on Learning Representations, 2019.

[26] Stéphane d'Ascoli, Levent Sagun, and Giulio Biroli. Triple descent and the two kinds of overfitting: where &amp; why do they appear? In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 3058­3069. Curran Associates, Inc., 2020.

[27] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. Communications of the ACM, 59(2):64­73, 2016.

[28] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable effectiveness of data in deep learning era. arXiv:1707.02968, 2017.

11

[29] Eleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci, Kelvin Xu, Ross Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol, and Hugo Larochelle. Meta-dataset: A dataset of datasets for learning to learn from few examples. In International Conference on Learning Representations, 2020.
[30] Joseph Paul Cohen, Mohammad Hashir, Rupert Brooks, and Hadrien Bertrand. On the limits of cross-domain generalization in automated x-ray prediction. In Medical Imaging with Deep Learning, pages 136­155. PMLR, 2020.
[31] Maithra Raghu, Chiyuan Zhang, Jon Kleinberg, and Samy Bengio. Transfusion: Understanding transfer learning for medical imaging. In Advances in Neural Information Processing Systems 32, pages 3347­3357. 2019.
[32] Alexander Ke, William Ellsworth, Oishi Banerjee, Andrew Y. Ng, and Pranav Rajpurkar. Chextransfer: Performance and parameter efficiency of imagenet models for chest x-ray interpretation. In Proceedings of the Conference on Health, Inference, and Learning, CHIL '21, page 116­124, New York, NY, USA, 2021. Association for Computing Machinery.
[33] Basil Mustafa, Aaron Loh, Jan Freyberg, Patricia MacWilliams, Alan Karthikesalingam, Neil Houlsby, and Vivek Natarajan. Supervised transfer learning at scale for medical imaging. arXiv preprint arXiv:2101.05913, 2021.
[34] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens Van Der Maaten. Exploring the limits of weakly supervised pretraining. In Proceedings of the European Conference on Computer Vision (ECCV), pages 181­196, 2018.
[35] Andrew Brock, Soham De, Samuel L Smith, and Karen Simonyan. High-performance largescale image recognition without normalization. arXiv preprint arXiv:2102.06171, 2021.
[36] Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
[37] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 702­703, 2020.
[38] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016.
[39] Alexander Sergeev and Mike Del Balso. Horovod: fast and easy distributed deep learning in tensorflow. arXiv preprint arXiv:1802.05799, 2018.
[40] Joseph Paul Cohen, Joseph Viviano, Paul Morrison, Rupert Brooks, Mohammad Hashir, and Hadrien Bertrand. TorchXRayVision: A library of chest X-ray datasets and models. https://github.com/mlmed/torchxrayvision, 2020.
[41] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
[42] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In Sixth Indian Conference on Computer Vision, Graphics & Image Processing, pages 722­729. IEEE, 2008.
[43] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In IEEE Conference on Computer Vision and Pattern Recognition, pages 3498­3505, June 2012.
[44] Stefan Jaeger, Sema Candemir, Sameer Antani, Yì-Xiáng J. Wáng, Pu-Xuan Lu, and George Thoma. Two public chest x-ray datasets for computer-aided screening of pulmonary diseases. Quantitative imaging in medicine and surgery, 4:475­477, December 2014.
[45] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.
[46] Christopher J Shallue, Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein, Roy Frostig, and George E Dahl. Measuring the effects of data parallelism on neural network training. Journal of Machine Learning Research, 20:1­49, 2019.
12

[47] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey E Hinton. Big self-supervised models are strong semi-supervised learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 22243­22255. Curran Associates, Inc., 2020.
[48] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student improves imagenet classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10687­10698, 2020.
[49] Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, Ekin Dogus Cubuk, and Quoc Le. Rethinking pre-training and self-training. Advances in Neural Information Processing Systems, 33, 2020.
[50] Linda Wang, Zhong Qiu Lin, and Alexander Wong. Covid-net: a tailored deep convolutional neural network design for detection of covid-19 cases from chest x-ray images. Scientific reports, 10:19549, November 2020.
13

Supplementary: Effect of large-scale pre-training on full and few-shot transfer learning for natural and medical images

A Distributed Training
A.1 JUWELS Booster Supercomputer
Installed in November 2020, JUWELS Booster [21] features 936 compute nodes that host four NVIDIA A100 GPUs each, providing 3744 GPUs in total. The installed A100 Tensor Core GPUs (40 GB) provide 19.5 TFLOP/s of FP64TC computing performance each. The GPUs are hosted by AMD EPYC 7402 CPUs with 2 × 24 cores (SMT-2) per node, clocked with 2.8 GHz. Each node is diskless and is equipped with 512 GB of RAM. The network of JUWELS Booster is based on Mellanox HDR200 InfiniBand, with four Mellanox ConnectX 6 devices per node, each providing 200 Gbit/s bandwidth per direction.
The NVIDIA A100 GPUs installed into JUWELS Booster reach peak efficiency of 48.75 GFLOP/(s W) when utilizing the FP64 Tensor Cores. This makes JUWELS Booster rank highest in the Green500 list of November 2020 as the most energy efficient supercomputer among the first 100 machines of the Top500 list with 25 GFLOP/(s W).

A.2 Scaling and training time

Here, we report scaling behavior during large-scale pre-training for ResNet networks we used in the experiments.

We performed scaling experiments to assess the scalability of data parallel training distributed across

many GPUs on multiple nodes using Horovod. The efficiency in Figure 4b (upper part of the figure

with

percentages)

is

computed

using

the

following

formula:

E(N )

=

100

×

T (N ) N ×T (1)

.

T (N )

is

the

total measured throughput in Im/s for N GPUs. The best achievable efficiency, when scaling is

perfect, is 100%.

We also provide the raw throughput (Im/s) numbers in Figure 4a. On 1024 GPUs, we achieve an efficiency of  93.7% with single precision (FP32). To make sure distributed training is stable, we check the end accuracy of full training for each number of GPUs to reassure we reach target accuracy acceptable for standard ImageNet-1k Top-1 and Top-5 results.

120000

Throughput measured ideal

100000

1000 100.009%7.07%96.55% 96.07% 800

94.52%

93.77%

Throughput (Im/s) Speed up

80000

600

60000 400

40000

20000

0

1

4

8

16

32

64

128 256 512 1024

Number of GPUs

200

0

1 64 128

256

512 Number of GPUs

Speed up measured ideal
1024

(a) Throughput in Im/s

(b) Scaling efficiency

Figure 4: Distributed training for R152x4, scaling behavior on JUWELS Booster using A100 GPUs.

14

Table 3: Datasets used as source for pre-training and target for transfer. The url of each dataset we will use is provided below.

Dataset

Size

Source pre-training

Natural Images ImageNet-1k [8] ImageNet-21k [8] X-Ray Chest Imaging CheXpert [17] NIH Chest X-ray14 [20] PadChest [19] MIMIC-CXR [18] Total X-Ray images

1.4M images, 1000 classes 14M images, 21842 classes
224K radiographs of 65K patients, 14 classes 112K radiographs of 32K patients, 14 classes 160K radiographs of 67K patients, 19 classes 377K radiographs of 65K patients, 14 classes 873K chest radiographs, 229K patients

Target transfer

Natural Images CIFAR-10, 100 [41] Oxford Flowers-102 [42] Oxford-IIIT Pet [43] X-Ray Chest Imaging PadChest [19] COVIDx [50] Tuberculosis [44]

60K images, 10,100 classes 8K images, 102 classes 7.3K images, 37 classes
160K radiographs of 67K patients, 19 / 27 classes 16K radiographs, 15K patients, 2 / 3 classes 800 radiographs, 800 patients, 2 classes

B Additional details on experimental results

B.1 Datasets employed in experiments.
All datasets employed in our experiments are publicly available and can be obtained following links in the Tab. 3

B.2 Further transfer results Here, we present results of further transfer experiments as described in the Sec. 3.3.

100

R50x1 on ImageNet-1k R50x1 on ImageNet-21k

R152x4 on ImageNet-1k

R152x4 on ImageNet-21k

90

80

70

100

R50x1 on ImageNet-1k R50x1 on ImageNet-21k

R152x4 on ImageNet-1k

R152x4 on ImageNet-21k

90

80

70

60

60

50

50

40

40

30

1 Shot

5 ShSohtots during trans1fe0rShot

Full Shot

30 1 Shot

5 Shot

1S0hSohtsotduring t1ra0n0sSfehort

500 Shot

Full Shot

(a) Flowers-102

(b) CIFAR-10

Figure 5: Few-shot and full shot transfer performance on target datasets when varying model size and

dataset size in pre-training.

Test Accuracy Test Accuracy

15

90

80

70

Test Accuracy

60

50 40 30 1 Shot

5 Shot

R50x1 on CheXpert R50x1 on CheXpert-MIMIC R50x1 on CheXpert-MIMIC-NIH R50x1 on CheXpert-MIMIC-NIH-PadChest R152x4 on CheXpert R152x4 on CheXpert-MIMIC R152x4 on CheXpert-MIMIC-NIH R152x4 on CheXpert-MIMIC-NIH-PadChest R50x1 on ImageNet-1k R50x1 on ImageNet-21k R152x4 on ImageNet-1k R152x4 on ImageNet-21k

1S0hSohtsotduring t1ra0n0sSfehort

500 Shot

Full Shot

Figure 6: Few-shot and full shot transfer performance on COVIDx dataset when varying model size and dataset size in pre-training.

90

80

70

Test Accuracy

60

50

40

30

1 Shot

5 Shot

R50x1 on CheXpert R50x1 on CheXpert-MIMIC R50x1 on CheXpert-MIMIC-NIH R50x1 on CheXpert-MIMIC-NIH-PadChest R152x4 on CheXpert R152x4 on CheXpert-MIMIC R152x4 on CheXpert-MIMIC-NIH R152x4 on CheXpert-MIMIC-NIH-PadChest R50x1 on ImageNet-1k R50x1 on ImageNet-21k R152x4 on ImageNet-1k R152x4 on ImageNet-21k

Shots d1u0riSnhgottransfer 100 Shot

Full Shot

Figure 7: Few-shot and full shot transfer performance on Tuberculosis dataset when varying model size and dataset size in pre-training.

16

Test Accuracy Test Accuracy

90

Model R50x1 on ImageNet-1k

R50x1 on ImageNet-21k

R152x4 on ImageNet-1k

R152x4 on ImageNet-21k

80

imagenet21k_bit152x4

70

60

50

90

Model R50x1 on CheXpert

R50x1 on CheXpert-MIMIC-NIH-PadChest

R152x4 on CheXpert

R152x4 on CheXpert-MIMIC-NIH-PadChest

80

70

60

50

1 Shot

5 Shot Shots d1u0riSnhgottransfer 100 Shot

Full Shot

1 Shot

5 Shot Shots d1u0riSnhgottransfer 100 Shot

Full Shot

(a) Tuberculosis, natural sources

(b) Tuberculosis, medical sources

Figure 8: Few-shot and full shot transfer performance on Tuberculosis dataset when pre-training with different model sizes on different sources (natural or medical datasets) of various sizes.

Test Mean AUC Test Mean AUC

Model

R50x1 on ImageNet-1k

80

R50x1 on ImageNet-21k R152x4 on ImageNet-1k

R152x4 on ImageNet-21k

75

imagenet21k_bit152x4

Model

R50x1 on CheXpert

80

R50x1 on CheXpert-MIMIC-NIH R152x4 on CheXpert

R152x4 on CheXpert-MIMIC-NIH

75

70

70

65

65

60

60

55

55

1 Shot

5 Shot Shots d1u0riSnhgottransfer 100 Shot

Full Shot

1 Shot

5 Shot Shots d1u0riSnhgottransfer 100 Shot

Full Shot

(a) PadChest, natural sources

(b) PadChest, medical sources

Figure 9: Few-shot and full shot transfer performance on PadChest when pre-training with different model sizes on different sources (natural or medical datasets) of various sizes.

17

C Code and Data availability
Here, a link to the repository containing code used for running experiments and producing figures in this study will be provided. All datasets used in the study are openly available and are listed together with references to the original work in the Table 3. Further details on the usage in the conducted experiments will be provided in the repository.
18

