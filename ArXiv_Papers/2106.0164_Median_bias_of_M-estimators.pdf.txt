Median bias of M-estimators
Arun Kumar Kuchibhotla arunku@cmu.edu
Department of Statistics & Data Science, Carnegie Mellon University

arXiv:2106.00164v1 [math.ST] 1 Jun 2021

Abstract
In this note, we derive bounds on the median bias of univariate M -estimators under mild regularity conditions. These requirements are not sufficient to imply convergence in distribution of the M-estimators. We also discuss median bias of some multivariate M-estimators.

1 Introduction

Commonly used estimators, known as M-estimators, are obtained as solution to optimization problems. Under certain regularity conditions, properly normalized M-estimators are shown to convergence in distribution to a mean zero Gaussian. This implies that the median of the M-estimator converges to the population parameter, i.e., the M-estimator is asymptotically median unbiased. The aim of this note is to show that in some cases asymptotic median unbiasedness can be proved without proving convergence in distribution. The more interesting aspect is that median unbiasedness can be proved under far less regularity conditions than those required for convergence in distribution.
For any estimator  estimating 0, we define the median bias as

Med-bias0() :=

1 2

-

max

P(  0), P(  0)

,
+

where (x)+ := max{x, 0}. If P(  0)  1/2 and P(  0)  1/2, then Med-bias0() = 0 and  is median unbiased for 0.

Notation. For any function f , we use f and f¨ to denote the first and second derivatives of f .

2 Univariate M-estimators
In this section, we consider median bias of univariate M-estimators and Z-estimators.
2.1 Convex M-estimators
Suppose   R and Mn :   R is a convex function and define n := argmin Mn().

If Mn(·) converges in probability pointwise to M (·), then we can define the target 0 of n as the minimizer of M (). Formally, 0 = argmin M (). With M n(·) representing the derivative of Mn(·), convexity of

1

Mn(·) and an assumption that 0 lies in the interior of  implies that

M n(0) < 0  n  0  M n(0)  0,

(1)

and

M n(0) > 0  n  0  M n(0)  0.

(2)

See, for example, (3.4) of Bentkus et al. (1997) for the proof. Hence, we get that

P(n  0)  P(M n(0) < 0) and P(n  0)  P(M n(0) > 0).

These inequalities imply the following result.

Theorem 1. If 0 lies in the interior of  and   Mn() is convex, then

Med-bias0 (n) 

1 2

-

max

P(M n(0) < 0), P(M n(0) > 0)

.
+

(3)

Theorem 1 implies that the median bias of n can be controlled by studying M n(0). The study of M n(0)
is not enough to prove consistency or convergence in distribution of n. Proving consistency requires assumptions on the curvature of Mn(·) around 0 and similarly proving asymptotic normality requires assumptions on the first derivative of Mn(·).
In many cases, Mn(·) is an average of random variables and hence, M n(0) satisfies a central limit theorem under Lindeberg type conditions. This implies that the right hand side of (3) converges to zero
and proves that n is asymptotically median unbiased. Note that it is relatively straightforward to obtain a finite sample bound on the median bias using (3) and the Berry­Esseen bounds for sum of independent or weakly dependent random variables. For examples of such Berry­Esseen bounds, see Petrov (2012, Chap. V) and Ho¨rmann (2009). Below, we provide some simple applications of Theorem 1.

Median Estimation. The sample median of X1, . . . , Xn is defined as the minimizer of

n

n

Mn() :=

|Xi - |  M n() = {2½{Xi  } - 1} .

i=1

i=1

If X1, . . . , Xn are independent but possibly non-identically distributed observations and 0 is a solution of E[M n()] = 0, then n-1/2M n(0) converges in distribution to a mean zero normal distribution. Proving convergence in distribution of the sample median requires an assumption on the Ho¨lder continuity of the distribution functions. See, for example, Knight (1998) and Knight (1999). Note that the calculations above also apply to quantile estimation and proves asymptotic median unbiasedness.

Lp-median. p  1,

Generalizing the sample median, consider n as a minimizer of Mn() over   R where for

n

n

Mn() :=

|Xi - |p  M n() = p |Xi - |p-1sign( - Xi).

i=1

i=1

Define 0 as a solution to the equation E[M n()] = 0. Inequality (3) along with the Berry­Esseen bounds
shows that n is asymptotically median unbiased for 0. Once again more conditions on the distribution of Xi's is needed (for p  3) to ensure convergence in distribution; see Bentkus et al. (1997).

2

Maximum Likelihood Estimator. Suppose {p :   } is a family of parametric densities parametrized by     R. Consider the maximum likelihood estimator (MLE) n as

n

n := argmin - log p(Xi).



i=1

If   - log p(x) is convex, then Mn() = -

n i=1

log

p (Xi )

is

a

convex

function

of

.

Assuming

differen-

tiability in quadratic mean (DQM) of the parametric family, let u(x) be the likelihood score function; see

Eq. (7.1) of Van der Vaart (2000) for DQM. Define 0 to be a solution to the equation

n i=1

E[u

(Xi)]

=

0.

Assuming X1, . . . , Xn are independent and the Linderberg condition on u0(Xi), 1  i  n, we get that

the MLE is asymptotically median unbiased. Once again a properly normalized MLE need not converge in

distribution without further assumptions that guarantee asymptotic equicontinuity of the likelihood score.

Further under possible misspecification, the Jacobian also needs to be non-zero at 0 for convergence in

distribution.

2.2 Non-differentiable M-estimators
We have assumed the existence of a version of the derivative M n(·) in the previous subsections. It is possible to avoid such assumption. From the convexity of Mn(·), it follows that for any  > 0,

and Hence,

{n > 0 + }  {Mn(0)  Mn(0 + )}, {n < 0 - }  {Mn(0)  Mn(0 - )}.

Now observe that

P(n  0 + )  P(Mn(0) < Mn(0 + )), P(n  0 - )  P(Mn(0) < Mn(0 - )).

Therefore,

P(n



0)

=

lim
0

P(n



0

+

)

and

P(n



0)

=

lim
0

P(n



0

-

).

Med-bias0 ()



lim
0

1 2

-

max

{P(Mn(0)

<

Mn(0

+

)),

P(Mn(0)

<

Mn(0

-

))}

.
+

(4)

In many cases, Mn() is an average of random variables and Mn(0) - Mn(0 + ) converges to a negative quantity for any fixed  > 0.

Maximum Likelihood Estimation. Consider again the problem of median bias of the maximum like-

lihood estimator. In this case, Mn() = -

n i=1

log p(Xi)

and

the

target

of

the

MLE

0

is

defined

as

the

minimizer of   Mn() over   . If 0 lies in the interior of  and 0 ±   , then

P(Mn(0) < Mn(0 + ))

=P

n i=1

log

p0+(Xi) p0 (Xi)

<

0

= P n log p0+(Xi) - E log p0+(Xi)

< - n E log p0+(Xi)

.

i=1

p0 (Xi)

p0 (Xi)

i=1

p0 (Xi)

3

Note that, by definition,

n i=1

E

log(p0+(Xi)/p0

(Xi))



0

and

is

strictly

negative

if



>

0

by

identifiability.

Hence, it follows that

P(Mn(0) < Mn(0 + ))  P

n i=1

log

p0+(Xi) p0 (Xi)

-

E

log

p0+(Xi) p0 (Xi)

0 .

Similarly,

P(Mn(0) < Mn(0 - ))  P

n i=1

log

p0-(Xi) p0 (Xi)

-

E

log

p0-(Xi) p0 (Xi)

0 .

If the log-likelihood ratio satisfies the Linderbeg condition, then (4) implies that the MLE is again asymptotically median unbiased.

2.3 Non-convex M-estimators
Convexity of the objective function Mn(·) is not very crucial for (3). All that is required is that   Mn() is convex in the neighborhood of 0 and with some positive probability the estimator n belongs to that neighborhood. These conditions are same as convexity and consistency assumptions (1.4), (1.5) in Bentkus et al. (1997). Formally, for set

1,n() := 1 - P (  Mn() is convex on [0 - , 0 + ]) , 2n() := P(|n - 0| > ).

On the event n  [0 - , 0 + ]  , inequalities (1) and (2) hold true. Therefore, we get

Med-bias0 (n) 

1 2

-

max

P(M n(0) < 0), P(M n(0) > 0)

+ min [1,n() + 2,n()] .
+ 0

(5)

Note that if   Mn() is convex on , then 1,n() = 0 and 2,n() = 0. Inequality (5) follows from (3.3) of Bentkus et al. (1997).
Alternatively, one can consider the usual Taylor series expansion way and prove a better result. For this, we additionally require absolute continuity of the first derivative of Mn(·). If n solves the equation M n() = 0 and using absolute continuity of   M n(), we get that

1
0 = M n(0) + M¨ n(0 + t(n - 0))dt(n - 0).
0

Now assuming that n is a locally unique solution with probability 1, we conclude that

0))dt = 0 and hence,

n - 0 = -

1 0

M n(0) M¨ n(0 + t(n

-

0))dt

.

This implies that

1 0

M¨ n(0

+

t(n

-

(6)

Med-bias0 (n) =

1 2

-

max

P(M n(0)  0), P(M n(0)  0)

.
+

(7)

Equation (6) provides a more intuitive reason for why median bias of univariate M-estimators can be controlled without requiring conditions to imply convergence in distribution. To prove convergence in distribution, we would require n to be consistent for 0 along with conditions to ensure that the denominator on the right hand side of (6) can be replaced with M¨ n(0).
The bound on median bias (7) readily applies to Z-estimators which are obtained as solutions of estimating equations rather than minimizers of objective functions.

4

3 Multivariate M-estimation

The calculations from previous sections can be used trivially when estimating a parameter in presence of nuisance parameters. Suppose Mn :  ×   R be an objective function and define
(n, n) := argmin Mn(, ).
,
Setting M (, ) as the pointwise limit in probability of Mn(, ), define
(0, 0) := argmin M (, ).
,

To apply the results from previous section to study the median bias of n, observe that

n

:=

argmin


min


Mn(,

).

If (, )  Mn(, ) is a convex function, then   min Mn(, ) is also a convex function. This is called the inf-projection of Mn(, ). However, Wn() = min Mn(, ) and its derivative (sub-gradient) W n() are complicated functions to study, in general. In some special cases, Wn() is available in closed form and might be easily analysed.

3.1 Application 1: Least Squares Linear Regression
Suppose (Yi, Ti, Xi)  R × R × Rd, 1  i  n represent the set of observations on the treatment variable (T ), covariates (X), and the response (Y ). Consider

n

(n, n) := argmin (Yi - Ti - Xi)2.

(8)

, i=1

The targets of n, n are defined as

n
(0, 0) := argmin E[(Yi - Ti - Xi)2].
, i=1

This can be written as

n

n := argmin (Yi - Y,nXi - (Ti - T,nXi))2,



i=1

where

n

n

Y,n := argmin (Yi - Xi)2, and T,n := argmin (Ti - Xi)2.

Rd i=1

Rd i=1

Hence n is the minimizer of a quadratic convex objective function and (3) (or (5)) yields a bound on the
median bias of n. For notational convenience, set RY,i = Yi - Y,nXi and RT,i = Ti - T,nXi. Also, let Y
and T denote the targets of Y,n and T,n, and set RY,i = Yi - YXi and RT,i = Ri - TXi. Then (5) yields

Med-bias0 (n) =

1 2

-

max

P

n

n

RT,i(RY,i - 0RT,i)  0 , P

RT,i(RY,i - 0RT,i)  0

i=1

i=1

.
+
(9)

5

This equality holds true if Ti is not perfectly collinear with Xi. Note that unlike the examples discussed in previous sections, the sums on the right hand side of (9) are not of independent random variables. Observe that

n

n

n

RT,i(RY,i - 0RT,i) = RT,i(RY,i - 0RT,i) + (RT,i - RT,i)(RY,i - 0RT,i)

i=1

i=1

i=1

n

- RT,i(RY,i - RY,i - 0(RT,i - RT,i)).

i=1

From the definition of T,n it follows that

n i=1

RT

,i

Xi

= 0.

Noting

that

RY,i - RY,i

= Xi(Y,n - Y )

and

RT,i - RT,i = Xi(T,n - T ), we conclude that

n
RT,i(RY,i - RY,i - 0(RT,i - RT,i)) = 0.
i=1

We obtain that

n

n

n

RT,i(RY,i - 0RT,i) = RT,i(RY,i - 0RT,i) + (T,n - T ) Xi(RY,i - 0RT,i).

i=1

i=1

i=1

From the definition of definitions of Y,n, T,n

0, it that

can be easily verified that

n i=1

E[Xi(RY,i

-

0RT,i)]

=

0.

n i=1

E[RT,i(RY,i

-

0RT,i)]

Consider the event

=

0

and

from

the

E :=

n
(T,n - T ) Xi(RY,i - 0RT,i)   ,
i=1

and the sum

n
Sn := RT,i(RY,i - 0RT,i).
i=1

Using this event and inequality (9), we obtain the following result.

Proposition 1. For any sequence of random vectors (Yi, Ti, Xi)  R × R × Rd, the estimator n defined by (8) satisfies

Med-bias0 (n)



inf
>0

1 2

-

max

{P

(Sn



-)

,

P

(Sn



)}

+ P(Ec)
+

.

Under mild moment conditions as well as weak dependence assumptions, it can be proved that P(E) converges to 1 as n   for  = O(d). With  = O(d), it suffices for d = o( n) to ensure that the median bias converges to zero. If d = O( n), then the median bias converges to a constant bounded away from 0 and 1/2. The calculations above can also be used with the Neyman orthogonal estimating equation in a partial linear model (Chernozhukov et al., 2018).

3.2 Application 2: Partial Linear Regression with Sample Splitting
Suppose (Yi, Ti, Xi)  R × R × Rd, 1  i  n satisfy the partial linear model
Yi = 0Ti + g0(Xi) + Ui, and Ti = m0(Xi) + Vi, where E[Ui|Ti, Xi] = 0, E[Vi|Xi] = 0.
The parameter of interest is still 0 (the treatment effect). Split the data into two parts D1 and D2. Let m(·) and g(·) be estimators of m0(·) and g0(·), respectively. We will assume that these estimators are computed

6

from D1. For i  D2, set RT,i = Ti - m(Xi) and RY,i = Yi - g(Xi). Then consider the estimator n as a solution to the equation

Zn() :=

RT,i(RY,i - Di) = 0,

iD2

where D2 is the second part of the data. From (7), it follows that

Med-bias0 (n) =

1 2

-

max

{P

(Zn(0)



0) ,

P(Zn(0)



0)}

.
+

(10)

Now set Zcn(0) = Zn(0) - E[Zn(0)|D1]. Then, (10) implies

Med-bias0 (n) 

1 2

-

max

P(Zcn(0)  -|E[Zn(0)|D1]|), P(Zcn(0)  |E[Zn(0)|D1]|)

.
+

(11)

Note that conditional D1 (the first split of the data), Zcn(0) is a sum of centered (mean zero) random variables. We can write

Zn(0) =

RT,i(RY,i - Di0) +

RT,i(RY,i - RY,i)

iD2

iD2

+

(RT,i - RT,i)(RY,i - Di0) +

(RT,i - RT,i)(RY,i - RY,i).

iD2

iD2

Conditional on D1, the first three terms above are mean zero and hence,

E[Zn(0) D1] = E[(g(Xi) - g0(Xi))(m(Xi) - m0(Xi)) D1].
iD2

This can be bounded as

|E[Zn(0) D1]|  |D2| g - g0 2,n m - m0 2,n,

where for PXi (·) representing the probability measure of Xi,

g - g0

2 2,n

:=

|D2|-1

iD2

m - m0

2 2,n

:=

|D2|-1

iD2

(g(x) - g0(x))2dPXi (x), (m(x) - m0(x))2dPXi (x).

Hence if |D2|1/2 g - g0 2,n m - m0 2,n = op(1), then inequality (11) implies that the median bias of n converges to zero. Further, if |D2|1/2 g - g0 2,n m - m0 2,n = Op(1), then the median bias of n is bounded away from 0 to 1/2.

4 Conclusion
In this note, we proved that median bias of several M/Z-estimators can be controlled under conditions weaker than those required for convergence in distribution of these estimators. The control of the median bias implies that the recently proposed confidence interval methodology HulC (Kuchibhotla et al., 2021) can be applied to these estimators. Note that without convergence in distribution none of the usual methods of inference, including bootstrap and subsampling, apply.

7

References
Bentkus, V., Bloznelis, M., and G¨otze, F. (1997). A Berry­Ess´een bound for M-estimators. Scandinavian journal of statistics, 24(4):485­502.
Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., and Robins, J. (2018). Double/debiased machine learning for treatment and structural parameters.
Ho¨rmann, S. (2009). Berry-Esseen bounds for econometric time series. ALEA Lat. Am. J. Probab. Math. Stat, 6:377­397.
Knight, K. (1998). Limiting distributions for L1 regression estimators under general conditions. Annals of statistics, pages 755­770.
Knight, K. (1999). Asymptotics for L1-estimators of regression parameters under heteroscedasticity. Canadian Journal of Statistics, 27(3):497­507.
Kuchibhotla, A. K., Balakrishnan, S., and Wasserman, L. (2021). The HulC: Confidence regions from convex hulls. arXiv preprint arXiv:2105.14577.
Petrov, V. V. (2012). Sums of independent random variables, volume 82. Springer Science & Business Media. Van der Vaart, A. W. (2000). Asymptotic statistics, volume 3. Cambridge university press.
8

