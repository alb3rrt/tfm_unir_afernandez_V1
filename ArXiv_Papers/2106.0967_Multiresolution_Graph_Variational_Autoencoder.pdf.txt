arXiv:2106.00967v1 [cs.LG] 2 Jun 2021

Multiresolution Graph Variational Autoencoder
Truong Son Hy and Risi Kondor Department of Computer Science, Department of Statistics
The University of Chicago {hytruongson,risi}@uchicago.edu
Abstract
In this paper, we propose Multiresolution Graph Networks (MGN) and Multiresolution Graph Variational Autoencoders (MGVAE) to learn and generate graphs in a multiresolution and equivariant manner. At each resolution level, MGN employs higher order message passing to encode the graph while learning to partition it into mutually exclusive clusters and coarsening into a lower resolution. MGVAE constructs a hierarchical generative model based on MGN to variationally autoencode the hierarchy of coarsened graphs. Our proposed framework is end-to-end permutation equivariant with respect to node ordering. Our methods have been successful with several generative tasks including link prediction on citation graphs, unsupervised molecular representation learning to predict molecular properties, molecular generation, general graph generation and graph-based image generation.
1 Introduction
Understanding graphs in a multiscale and multiresolution perspective is essential for capturing the structure of molecules, social networks, or the World Wide Web. Graph neural networks (GNNs) utilizing various ways of generalizing the concept of convolution to graphs [Scarselli et al., 2009] [Niepert et al., 2016b] [Li et al., 2016] have been widely applied to many learning tasks, including modeling physical systems [Battaglia et al., 2016], finding molecular representations to estimate quantum chemical computation [Duvenaud et al., 2015] [Kearnes et al., 2016] [Gilmer et al., 2017b] [Kondor et al., 2018] [Hy et al., 2018], and protein interface prediction [Fout et al., 2017]. One of the most popular types of GNNs is message passing neural nets (MPNNs) that are constructed based on the message passing scheme in which each node propagates and aggregates information, encoded by vectorized messages, to and from its local neighborhood. While this framework has been immensely successful in many applications, it lacks the ability to capture the multiscale and multiresolution structures that are present in complex graphs [Rustamov and Guibas, 2013] [Chen et al., 2014] [Cheng et al., 2015] [Xu et al., 2019]. Ying et al. [2018] approached this issue by introducing a differential pooling operator based on soft assigment matrices, but this method transforms a sparse graph into a dense weighted graph without any actual clustering of the vertices.
For large graphs in particular, it is important to design graph neural networks that are able to learn a multiresolution representation along with learning an actual clustering that favors balanced cuts, increasing the representational power of GNNs while perserving permutation equivariance. To this end, we propose our Multiresolution Graph Network (MGN) architecture with the following key contributions:
 A multiresolution framework that explicitly has a reconstruction target at each granularity level, and employs local graph encoders and decoders to learn graph structures in an autoencoding manner. In addition, a learnable clustering procedure with hard assignments that determines the reconstruction target at the next level of granularity.
Preprint. Under review.

 The graph encoders and decoders are designed to take higher order permutation equivariant features into account. Our proposed framework is end-to-end permutation equivariant with respect to the node ordering.
Learning to generate graphs with deep generative models is a difficult problem because graphs are combinatorial objects with high order correlations between their discrete substructures (subgraphs) [You et al., 2018a] [Li et al., 2018] [Liao et al., 2019] [Liu et al., 2019] [Dai et al., 2020]. Graphbased molecular generation [Gómez-Bombarelli et al., 2018] [Simonovsky and Komodakis, 2018] [Cao and Kipf, 2018] [Jin et al., 2018] [Thiede et al., 2020] involves further challenges, including correctly recognizing chemical substructures, and importantly, ensuring that the generated molecular graphs are chemically valid. MGN allows us to extend the existing model of variational autoencoders (VAEs) with a hierarchy of latent distributions that can stochastically generate a graph in multiple resolution levels. We call this combined architecture Multiresolution Graph Variational Autoencoder (MGVAE). We argue that having a flexible clustering procedure from MGN enables MGVAE to detect, reconstruct and finally generate important graph substructures, especially chemical functional groups.

2 Related work
There have been significant advances in understanding the invariance and equivariance properties of neural networks in general [Cohen and Welling, 2016a] [Cohen and Welling, 2016b], of graph neural networks [Kondor et al., 2018] [Maron et al., 2019b], of neural networks learning on sets [Zaheer et al., 2017] [Serviansky et al., 2020] [Maron et al., 2020], along with their expressive power on graphs [Maron et al., 2019c] [Maron et al., 2019a]. Our work is in line with equivariant networks operating on graphs and sets.
Multiscale, multilevel, multiresolution and coarse-grained techniques have been widely applied in factorizing symmetric matrices [Kondor et al., 2014]; kernel approximation [Ding et al., 2017]; wavelets on graphs [Rustamov and Guibas, 2013]; graph clustering and finding balanced cuts on large graphs [Dhillon et al., 2005] [Dhillon et al., 2007] [Chiang et al., 2012] [Si et al., 2014]; link prediction on social networks [Shin et al., 2012]; and modeling and simulating physical, chemical and biological systems [Riniker et al., 2012] [Kmiecik et al., 2016] [Hirn et al., 2017] [Wang et al., 2019]. Our work is inspired by this literature.
In the field of deep generative models, it is generally recognized that introducing a hierarchy of latents and adding stochasticity among latents lead to a more powerful model which is able to learn more complicated distributions [Blei et al., 2003] [Ranganath et al., 2016] [Ingraham and Marks, 2017] [Klushyn et al., 2019] [Wu et al., 2020] [Vahdat and Kautz, 2020]. Our work exploits these hierarchical generative models.

3 Multiresolution graph network

3.1 Construction

An undirected weighted graph G = (V, E, A, Fv, Fe) with node set V and edge set E is represented by an adjacency matrix A  N|V|×|V|, where Aij > 0 implies an edge between node vi and vj with weight Aij (e.g., Aij  {0, 1} in the case of unweighted graph); while node features are represented by a matrix Fv  R|V|×dv , and edge features are represented by a tensor Fe  R|V|×|V|×de .

Definition 1. A K-cluster partition of graph G is a partition of the set of nodes V into K mutually exclusive clusters V1, .., VK . Each cluster corresponds to an induced subgraph Gk = G[Vk].

Definition 2. A coarsening of G is a graph G~ of K nodes defined by a K-cluster partition in which node v~k of G~ corresponds to the induced subgraph Gk. The weighted adjacency matrix A~  NK×K of G~ is

A~kk =

1 2

vi,vj Vk Aij ,

viVk,vj Vk Aij ,

if k = k , if k = k ,

where the diagonal of A~ denotes the number of edges inside each cluster, while the off-diagonal

denotes the number of edges between two clusters.

2

2 1

3 1

V1

V3

V2

6
Figure 1: Aspirin C9H8O4, its 3-cluster partition and the corresponding coarsen graph

Fig. 3.1 shows an example of Defs. 1 and 2: a 3-cluster partition of the Aspirin C9H8O4 molecular graph and its coarsening graph. Def. 3 defines the multiresolution of graph G in a bottom-up manner in which the bottom level is the highest resolution (e.g., G itself) while the top level is the lowest resolution (e.g., G is coarsened into a single node).

Definition 3. An L-level coarsening of a graph G is a series of L graphs G(1), .., G(L) in which

1. G(L) is G itself. 2. For 1   L - 1, G( ) is a coarsening graph of G( +1) as defined in Def. 2. The number of
nodes in G( ) is equal to the number of clusters in G( +1). 3. The top level coarsening G(1) is a graph consisting of a single node, and the corresponding
adjacency matrix A(1)  N1×1.

Definition 4. An L-level Multiresolution Graph Network (MGN) of a graph G consists of L - 1 tuples of five network components {(c( ), el(oc)al, d(loc)al, d(glo)bal, p( ))}L=2. The -th tuple encodes G( ) and transforms it into a lower resolution graph G( -1) in the higher level. Each of these network components has a separate set of learnable parameters (1( ), 2( ), 3( ), 4( ), 5( )). For simplicity, we collectively denote the learnable parameters as , and drop the superscript. The
network components are defined as follows:

1.

Clustering procedure c(G( ); ), which partitions graph G( ) into K clusters Each cluster is an induced subgraph Gk( ) of G( ) with adjacency matrix A(k ).

V1(

), . . . , VK( ).

2.

Local encoder elocal(Gk( ); ), which is a network that takes as input the subgraph as a matrix of size |Vk( )| × dz.

permutation equivariant Gk( ), and outputs a set of

(see Defs. 5, 6) node latents Zk(

graph neural ) represented

3.

Local decoder reconstruct the

dlocal(Zk( subgraph

); ), which is a permutation equivariant neural network that tries to adjacency matrix Ak( ) for each cluster from the local encoder's output

latents.

4. (Optional) Global decoder dglobal(Z( ); ), which is a permutation equivariant neural network that reconstructs the full adjacency matrix A( ) from all the node latents of K clusters Z( ) = k Zk( ) represented as a matrix of size |V( )| × dz.
5. etPhnoaiontlgitnaggkreanspeththweGos(rek-t p1o)(fZhnaok(sd)ea; dlja)at,cewennthscicyZhmk(ias)tarainxpdAeor(mut-upt1ua)ttsbiouanilstiinnavgsalierniacDlnuetsf(t.es2er,elaaDntedenftsth.ze~5k(c,)o6r)rendsezpu.ornTadlhiennegctownaoordrskefeatures Z( -1) = k z~k( ) represented as a matrix of size K × dz.

Algorithmically, MGN works in a bottom-up manner as a tree-like hierarchy starting from the high-
est resolution graph G(L), going to the lowest resolution G(1) (see Fig. 3.1). Iteratively, at -th level,
MGN partitions the current graph into K clusters by running the clustering procedure c( ). Then, the local encoder e(loc)al and local decoder d(glo)bal operate on each of the K subgraphs separately, and can be executed in parallel. This encoder/decoder pair is responsible for capturing the local structures.
Finally, the pooling network p( ) shrinks each cluster into a single node of the next level. Optionally, the global decoder d(glo)bal makes sure that the whole set of node latents Z( ) is able to capture the inter-connection between clusters.

In terms of time and space complexity, MGN is more efficient than existing methods in the field. The cost of global decoding the highest resolution graph is proportional to |V|2. For example,

3

0 1 0 0 0 1

1 0 1 0 0 0

A(23)

=

0  0

1 0

0 1

1 0

0 1

0 
0





0 0 0 1 0 1

100010

0 1 1

A(13) = 1 1

0 0

0 0

V1(2)

2 1 0 G(1) A(2) = 1 6 1
013

V2(2)

0 1 0 0

V3(2)

A(33)

=

1  0

0 1

1 0

1 
0

0100

v0 v1 v2 v3 v4 v5 v6 v7 v8 v9 v10 v11 v12

Figure 2: Hierarchy of 3-level Multiresolution Graph Network on Aspirin molecular graph

while the encoder can exploit the sparsity of the graph and has complexity O(|E|), a simple dotproduct decoder dglobal(Z) = sigmoid(ZZT ) has both time and space complexity of O(|V|2) which
is infeasible for large graphs. In contrast, the cost of running K local dot-product decoders is O |V|2/K), which is approximately K times more efficient.

3.2 Higher order message passing

In this paper we consider permutation symmetry, i.e., symmetry to the action of the symmetric
group, Sn. An element   Sn is a permutation of order n, or a bijective map from {1, . . . , n} to {1, . . . , n}. The action of Sn on an adjacency matrix A  Rn×n×1 and on a latent matrix Z  Rn×dz are

[ · A]i1,i2 = A , -1(i1),-1(i2)

[ · Z]i,j = Z-1(i),j ,

  Sn.

Here, the adjacency matrix A is a second order tensor with a single feature channel, while the latent

matrix tensor

Z X

is 

a first order Rnk×d (the

tensor with dz feature channels. In general, the action of Sn on last index denotes the feature channels) is defined similarly as:

a

k-th

order

[ · X ]i1,..,ik,j = X-1(i1),..,-1(ik),j ,

  Sn.

Network components of MGN (as defined in Sec. 3.1) at each resolution level must be either equivariant, or invariant with respect to the permutation action on the node order of G( ). Formally, we define these properties in Def. 5.

Definition 5. An Sn-equivariant (or permutation equivariant) function is a function f : Rnk×d  Rnk ×d that satisfies f ( · X ) =  · f (X ) for all   Sn and X  Rnk×d. Similarly,
we say that f is Sn-invariant (or permutation invariant) if and only if f ( · X ) = f (X ).

Definition 6. An Sn-equivariant network is a function f : Rnk×d  Rnk ×d defined as a composition of Sn-equivariant linear functions f1, .., fT and Sn-equivariant nonlinearities 1, .., T :

f = T  fT  ..  1  f1.
On the another hand, an Sn-invariant network is a function f : Rnk×d  R defined as a composition of an Sn-equivariant network f and an Sn-invariant function on top of it, e.g., f = f  f .

Based on Def. 5, we can construct Sn-equivariant/invariant networks as in Def. 6. Example 1 shows an example of an Sn-equivariant encoder and decoder that is first order. Indeed, the message passing propagation model Mt (see Eq. 2) is an Sn-equivariant function, and a MPNN encoder can be written as
fMPNN =   MT  ..    M1.

Example 1. The simplest implementation of the encoder is Message Passing Neural Networks
(MPNNs) [Gilmer et al., 2017b]. The node embeddings (messages) H0 are initialized by the input node features: H0 = Fv. Iteratively, the messages are propagated from each node to its neighborhood, and then transformed by a combination of linear transformations and non-linearities, e.g.,

Ht = (Mt),

(1)

Mt = D-1AHt-1Wt-1,

(2)

4

where  is a element-wise non-linearity function, Dii = j Aij is the diagonal matrix of node degrees, and Ws are learnale weight matrices. The output of the encoder is set by messages of the last iteration: Z = HT . The simplest implementation of the decoder is via a dot-product that estimates the adjacency matrix as A^ = sigmoid(ZZT ). This implementation of encoder and decoder is considered as first order.

In order to build higher order equivariant networks, we revisit some basic tensor operations: the tensor product A  B and tensor contraction Ax1,..,xp (details and definitions are in the Supplement). It can be shown that these tensor operations respect permutation equivariance [Kondor et al., 2018] [Hy et al., 2018]. Based on them, we build our second order message passing network as follows.

Example 2. The second order message passing has the message H0  R|V|×|V|×(dv+de) initialized by promoting the node features Fv to a second order tensor (e.g., we treat node features as self-loop edge features), and concatenating with the edge features Fe. Iteratively,

Ht = (Mt),

34

Mt = Wt

(A  Ht-1)i,j ,

(3)

i=1 j=i+1

where A  Ht-1 results in a fourth order tensor while i,j contracts it down to a second order tensor along the i-th and j-th dimensions,  denotes concatenation along the feature channels, and
Wt denotes a multilayer perceptron on the feature channels. The message HT of the last iteration is still second order, so we contract it down to the first order latent Z:

Z = HT 1  HT 2.
Proposition 1. The higher order message passing is equivariant with respect to the node permutation of its input graph G.

3.3 Learning to cluster

Definition 7. A clustering of n objects into k clusters is a mapping  : {1, .., n}  {1, .., k} in which (i) = j if the i-th object is assigned to the j-th cluster. The inverse mapping -1(j) = {i  [1, n] : (i) = j} gives the set of all objects assigned to the j-th cluster. The clustering is represented by an assignment matrix   {0, 1}n×k such that i,(i) = 1.
Definition 8. The action of Sn on a clustering  of n objects into k clusters and its corresponding assignment matrix  are

[ · ](i) = (-1(i)),

[ · ]i,j = -1(i),j ,

  Sn.

Definition 9. Let N be a neural network that takes as input a graph G of n nodes, and outputs a clustering  of k clusters. N is said to be equivariant if and only if N ( · G) =  · N (G) for all   Sn.
From Def. 9, intuitively the assignement matrix  still represents the same clustering if we permute its rows. The learnable clustering procedure c(G( ); ) is built as follows:
1. A graph neural network parameterized by  encodes graph G( ) into a first order tensor of K feature channels p~( )  R|V( )|×K .
2. The clustering assignment is determined by a row-wise maximum pooling operation:

( )(i) = argmax p~(i,k)

(4)

k[1,K ]

that is an equivariant clustering in the sense of Def. 9.

A composition of an equivariant function (e.g., graph net) and an invariant function (e.g., maximum
pooling given in Eq. 4) is an invariant function with respect to the node permutation. Thus, the learnable clustering procedure c(G( ); ) is permutation equivariant.

In practice, in order to make the clustering procedure differentiable for backpropagation, we replace the maximum pooling in Eq. 4 by sampling from a categorical distribution. Let ( )(i) be a categorical variable with class probabilities p(i,1), .., p(i,K) computed as softmax from p~(i,:). The Gumbel-max

5

trick [Gumbel, 1954][Maddison et al., 2014][Jang et al., 2017] provides a simple and efficient way to draw samples ( )(i):

(i ) = one-hot argmax gi,k + log p(i,k) ,
k[1,K ]

where gi,1, .., gi,K are i.i.d samples drawn from Gumbel(0, 1). Given the clustering assignment matrix ( ), the coarsened adjacency matrix A( -1) (see Defs. 1 and 2) can be constructed as ( )TA( )( ).

It is that

desirable are close

to have to |V(

a
)

balanced K-cluster partition in which clusters V1( ), .., VK( ) have similar sizes |/K. The local encoders tend to generalize better for same-size subgraphs.

We want the distribution of nodes into clusters to be close to the uniform distribution. We enforce

the clustering procedure to produce a balanced cut by minimizing the following Kullback­Leibler

divergence:

K

P (k)

DKL(P ||Q) =

P (k) log Q(k)

where

P=

|V1( |V (

)| )| ,

..,

|VK( )| |V( )|

,

Q=

11 , .., .
KK

k=1

Proposition 2. The whole construction of MGN is equivariant w.r.t node permutations of G.

4 Hierarchical generative model

In this section, we introduce our hierarchical generative model for multiresolution graph generation based on variational principles.

4.1 Background on graph variational autoencoder

Suppose that we have input data consisting of m graphs (data points) G = {G1, .., Gm}. The standard variational autoencoders (VAEs), introduced by Kingma and Welling [2014] have the following generation process, in which each data graph Gi for i  {1, 2, .., m} is generated independently:

1. Generate the latent variables Z = {Z1, .., Zm}, where each Zi  R|Vi|×dz is drawn i.i.d. from a prior distribution p0 (e.g., standard Normal distribution N (0, 1)).
2. Generate the data graph Gi  p(Gi|Zi) from the model conditional distribution p.

We want to optimize  to maximize the likelihood p(G) = p(Z)p(G|Z)dZ. However, this

requires computing the posterior distribution p(G|Z) =

m i=1

p

(Gi

|Zi),

which

is

usually

in-

tractable. Instead, VAEs apply the variational principle, proposed by Wainwright and Jordan [2005],

to approximate the posterior distribution as q(Z|G) =

m i=1

q(Zi

|Gi)

via

amortized

inference

and

maximize the evidence lower bound (ELBO) that is a lower bound of the likelihood:

LELBO(, ) = Eq(Z|G)[log p(G|Z)] - DKL(q(Z|G)||p0(Z))

m

(5)

=

Eq(Zi|Gi)[log p(Gi|Zi)] - DKL(q(Zi|Gi)||p0(Zi)) .

i=1

The probabilistic encoder q(Z|G), the approximation to the posterior of the generative model p(G, Z), is modeled using equivariant graph neural networks (see Examples 1, 2) as follows. Assume the prior over the latent variables to be the centered isotropic multivariate Gaussian

p(Z) = N (Z; 0, I). We let q(Zi|Gi) be a multivariate Gaussian with a diagonal covariance

structure:

log q(Zi|Gi) = log N (Zi; µi, i2I),

(6)

where µi, i  R|Vi|×dz are the mean and standard deviation of the approximate posterior output by two equivariant graph encoders. We sample from the posterior q by using the reparameterization trick: Zi = µi + i , where  N (0, I) and is the element-wise product.

On the another hand, the probabilistic decoder p(Gi|Zi) defines a conditional distribution over the
entries of the adjacency matrix Ai: p(Gi|Zi) = (u,v)Vi2 p(Aiuv = 1|Ziu, Ziv). For example, Kipf and Welling [2016] suggests a simple dot-product decoder that is trivially equivariant: p(Aiuv = 1|Ziu, Ziv) = (ZiTuZiv), where  denotes the sigmoid function.

6

4.2 Multiresolution VAEs

Based on the construction of multiresolution graph network (see Sec. 3.1), the latent vari-

ables are partitioned into disjoint groups, Zi = {Zi(1), Zi(2), .., Zi(L)} where Zi( ) = {[Zi( )]k 

R|[Vi( } )]k|×dz k is the set into a number of clusters

of latents [Gi( )]k.

at

the

-th resolution level in which the graph Gi( ) is partitioned

In the area of normalzing flows (NFs), Wu et al. [2020] has shown that stochasticity (e.g., a chain

of stochastic sampling blocks) overcomes expressivity limitations of NFs. In general, our MGVAE

is a stochastic version of the deterministic MGN such that stochastic sampling is applied at each

resolution level in a bottom-up manner. The prior (Eq. 7) and the approximate posterior (Eq. 8) are

represented by

L

L

p(Zi) = p(Zi( )) =

p([Zi( )]k),

(7)

=1

=1 k

1

q(Zi|Gi) = q(Zi(L)|Gi(L))

q(Zi( )|Zi( +1), Gi( )),

(8)

=L-1

in which each conditional in the approximate posterior are in the form of factorial Normal distributions, in particular

q(Zi( )|Zi( +1), Gi( )) = q([Zi( )]k Zi( +1), [Gi( )]k),
k

where each probabilistic encoder q([Zi( )]k Zi( +1), [Gi( )]k) operates on a subgraph [Gi( )]k as follows:

· The pooling network p( +1) shrinks the latent Zi( +1) into the node features of Gi( ) as in the construction of MGN (see Def. 4).
· The local (deterministic) graph encoder d(loc)al encodes each subgraph [Gi( )]k into a mean vector and a diagonal covariance matrix (see Eq. 6). A second order encoder can produce
a positive semidefinite non-diagonal covariance matrix, that is interpreted as a Gaussian Markov Random Fields (details in the Supplement). The new subgraph latent [Zi( )]k is sampled by the reparameterization trick (similar to as in Sec. 4.1).

The prior (in Eq. 7) can be either the isotropic Gaussian N (0, 1) as in standard VAEs, or be implemented as a parameterized Gaussian N (µ^, ^ ) where µ^ and ^ are learnable equivariant functions (details in the Supplement).

On the another hand, the probabilistic decoder p(Gi|Zi) defines a conditional distribution over all subgraph adjacencies at each resolution level:

p(Gi|Zi) = p(Gi( )|Zi( )) =

p([Ai( )]k|[Zi( )]k).
k

Extended from Eq. 5, we write our multiresolution variational lower bound LMGVAE(, ) on log p(G) compactly as

LMGVAE(, ) =
i

Eq(Zi( )|Gi( ))[log p(Gi( )|Zi( ))] - DKL(q(Zi( )|Gi( ))||p0(Zi( ))) . (9)

5 Experiments

Many more experimental results and details are presented in the Supplement.

5.1 Link prediction on citation graphs
We demonstrate the ability of the MGVAE models to learn meaningful latent embeddings on a link prediction task on popular citation network datasets Cora and Citeseer [Sen et al., 2008]. At training

7

Figure 3: MGVAE generates molecules on QM9 (4 on the left) and ZINC (the rest) equivariantly. There are many more examples of generated molecules in the Supplement and the quality of these can only be surpassed by (non-equivariant) autoregressive methods.

time, 15% of the citation links (edges) were removed while all node features are kept, the models are trained on an incomplete graph Laplacian constructed from the remaining 85% of the edges. From previously removed edges, we sample the same number of pairs of unconnected nodes (non-edges). We form the validation and test sets that contain 5% and 10% of edges with an equal amount of non-edges, respectively. We compare our model MGVAE against other methods from [Kipf and Welling, 2016] on the ability to correctly classify edges and non-edges using two metrics: area under the ROC curve (AUC) and average precision (AP). We repeat the experiments with 5 different random seeds and calculate the average AUC and AP along with their standard deviations. The number of message passing layers ranges from 1 to 4. The size of latent representation is 128. The number of coarsening levels is L  {3, 7}. In the -th coarsening level, we partition the graph G( )
into 2 (for L = 7) or 4 (for L = 3) clusters. We further explore the possibility of fixed clustering algorithms by implementing spectral clustering and K-Means clustering in comparison with learning to cluster (as in Sec. 3). The learning to cluster algorithm returns a more balanced cut relatively to both fixed clustering algorithms. MGVAE outperforms all other methods (see Table 1).

5.2 Molecular graph generation

We examine the generative power of MGN and MGVAE in the challenging task of molecule generation, in which the graphs are highly structured. We demonstrate that MGVAE is the first generative graph model in the field that successfully generates molecules in an equivariant manner. We train on two datasets that are standard in the field:
 QM9 [Ruddigkeit et al., 2012] [Ramakrishnan et al., 2014]: contains 134K organic molecules with up to nine atoms (C, H, O, N, and F) out of the GDB-17 universe of molecules.
 ZINC [Sterling and Irwin, 2015]: contains 250K purchasable drug-like chemical compounds with up to twenty-three heavy atoms.
We only use the graph features as the input, including the adjacency matrix, the one-hot vector of atom types (e.g., carbon, hydrogen, etc.) and the bond types (single bond, double bond, etc.) without any further domain knowledge from chemistry or physics. First, we train autoencoding task of reconstructing the adjacency matrix and node features. We use a learnable equivariant prior (see the Supplement) instead of the conventional N (0, 1). Then, we generate 5, 000 different samples from the prior, and decode each sample into a generated graph (see Fig. 3). We implement our graph construction (decoding) in two approaches:
1. All-at-once: We reconstruct the whole adjacency matrix by running the probabilistic decoder (see Sec. 4). MGVAE enables us to generate a graph at any given resolution level . In this particular case, we select the highest resolution = L. This approach of decoding preserves equivariance, but is harder to train. On ZINC, we extract several chemical/atomic features from RDKit as the input for the encoders to reach a good convergence in training.

Table 1: Citation graph link prediction results (AUC & AP)

Dataset Method
SC DW VGAE MGVAE (Spectral) MGVAE (K-Means) MGVAE

Cora

AUC (ROC)

AP

84.6 ± 0.01 83.1 ± 0.01 90.97 ± 0.77 91.19 ± 0.76 93.07 ± 5.61 95.67 ± 3.11

88.5 ± 0.00 85.0 ± 0.00 91.88 ± 0.83 92.27 ± 0.73 92.49 ± 5.77 95.02 ± 3.36

Citeseer

AUC (ROC)

AP

80.5 ± 0.01 80.5 ± 0.02 89.63 ± 1.04 90.55 ± 1.17 90.81 ± 1.19 93.93 ± 5.87

85.0 ± 0.01 83.6 ± 0.01 91.10 ± 1.02 91.89 ± 1.27 91.98 ± 1.02 93.06 ± 6.33

8

Dataset

Method

Training size Input features Validity Novelty Uniqueness

QM9

GraphVAE CGVAE MolGAN
Autoregressive MGN All-at-once MGVAE

 100K 10K

Graph

61.00% 100% 98.1% 100% 100%

85.00% 94.35% 94.2% 95.01% 100%

40.90% 98.57% 10.4% 97.44% 95.16%

ZINC

GraphVAE CGVAE JT-VAE
Autoregressive MGN All-at-once MGVAE

 200K
1K 10K

Graph Chemical

14.00% 100% 100% 100% 99.92%

100% 100%
99.89% 100%

31.60% 99.82%
99.69% 99.34%

Table 2: Molecular graph generation results. GraphVAE results are taken from [Liu et al., 2018].

alpha Cv G gap H HOMO LUMO mu omega1 R2 U U0 ZPVE

WL

3.75 2.39 4.84 0.92 5.45 0.38

NGF

3.51 1.91 4.36 0.86 4.92 0.34

PSCN 1.63 1.09 3.13 0.77 3.56 0.30

CCN 2D 1.30 0.93 2.75 0.69 3.14 0.23

MGVAE 2.83 0.91 1.78 0.66 1.87 0.34

0.89 1.03 192 154 5.41 5.36 0.51 0.82 0.94 168 137 4.89 4.85 0.45 0.75 0.81 152 61 3.54 3.50 0.38 0.67 0.72 120 53 3.02 2.99 0.35 0.58 0.95 195 90 1.89 1.90 0.14

Method MLP GCN GAT MoNet DiscenGCN FactorGCN GatedGCNE MGN

MAE 0.667 0.503 0.479 0.407

0.538

0.366

0.363

0.290

Table 3: Top: Unsupervised MGVAE on QM9. Bottom: Supervised MGN on ZINC.

2. Autoregressive: The graph is constructed iteratively by adding one edge in each iteration, similarly to [Liu et al., 2018]. But this approach does not respect permutation equivariance.
In our setting for small molecules, L = 3 and K = 2 -1 for the -th level. We compare our methods with other graph-based generative models including GraphVAE [Simonovsky and Komodakis, 2018], CGVAE [Liu et al., 2018], MolGAN [Cao and Kipf, 2018], and JT-VAE [Jin et al., 2018]. We evaluate the quality of generated molecules in three metrics: (i) validity, (ii) novelty and (iii) uniqueness as the percentage of the generated molecules that are chemically valid, different from all molecules in the training set, and not redundant, respectively. Because of high complexity, we only train on a small random subset of examples while all other methods are trained on the full datasets. Our models are equivalent with the state-of-the-art (SOTA), even with a limited training set (see Table 2). For the downstream regression tasks, we extract the molecular representation learned from the autoencoding task for graph generation as our unsupervised features. Then, we apply a simple multilayer perceptron on these unsupervised features for each regression task. Table 3 (top) shows our unsupervised regression result on 13 learning targets of QM9 in comparison with other methods from [Hy et al., 2018] with Mean Average Error (MAE) as the metric. Unsupervised MGVAE outperforms other supervised methods in 8 out of 13 learning targets. We also train MGN in a supervised manner to predict the solubility of molecules in ZINC dataset. Table 3 (bottom) shows MGN outperforms the SOTA on the same 10K training of ZINC from [Yang et al., 2020] by 20%.
6 Conclusion
We introduce MGVAE built upon MGN, the first generative model to learn and generate graphs in a multiresolution and equivariant manner. The key idea of MGVAE is learning to construct a series of coarsened graphs along with a hierarchy of latent distributions in the encoding process while learning to decode each latent into the corresponding coarsened graph at every resolution level. MGVAE achieves state-of-the-art results from link prediction to molecule and graph generation, suggesting that accounting for the multiscale structure of graphs is a promising way to make graph neural networks even more powerful.

9

References
Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, and Koray Kavukcuoglu. Interaction networks for learning about objects, relations and physics. In Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent dirichlet allocation. J. Mach. Learn. Res., 3(null): 993­1022, March 2003. ISSN 1532-4435.
Nicola De Cao and Thomas Kipf. Molgan: An implicit generative model for small molecular graphs, 2018. Xu Chen, Xiuyuan Cheng, and Stephane Mallat. Unsupervised deep haar scattering on graphs. In Advances in
Neural Information Processing Systems, volume 27. Curran Associates, Inc., 2014. Xiuyuan Cheng, Xu Chen, and Stéphane Mallat. Deep haar scattering networks. CoRR, abs/1509.09187, 2015.
Kai-Yang Chiang, Joyce Jiyoung Whang, and Inderjit S. Dhillon. Scalable clustering of signed networks using balance normalized cut. In Proceedings of the 21st ACM International Conference on Information and Knowledge Management, CIKM '12, page 615­624, New York, NY, USA, 2012. Association for Computing Machinery. ISBN 9781450311564. doi: 10.1145/2396761.2396841.
Taco S. Cohen and Max Welling. Group equivariant convolutional networks. Proceedings of The 33rd International Conference on Machine Learning, 48:2990­2999, 2016a.
Taco S. Cohen and Max Welling. Steerable cnns. ICLR'17, 2016b.
Hanjun Dai, Azade Nazi, Yujia Li, Bo Dai, and Dale Schuurmans. Scalable deep generative modeling for sparse graphs. In Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 2302­2312. PMLR, 13­18 Jul 2020.
Inderjit Dhillon, Yuqiang Guan, and Brian Kulis. A fast kernel-based multilevel algorithm for graph clustering. In Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining, KDD '05, page 629­634, New York, NY, USA, 2005. Association for Computing Machinery. ISBN 159593135X. doi: 10.1145/1081870.1081948.
Inderjit S. Dhillon, Yuqiang Guan, and Brian Kulis. Weighted graph cuts without eigenvectors a multilevel approach. IEEE Transactions on Pattern Analysis and Machine Intelligence, 29(11):1944­1957, 2007. doi: 10.1109/TPAMI.2007.1115.
Adji B. Dieng, Francisco J. R. Ruiz, David M. Blei, and Michalis K. Titsias. Prescribed generative adversarial networks, 2019.
Yi Ding, Risi Kondor, and Jonathan Eskreis-Winkler. Multiresolution kernel approximation for gaussian process regression. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.
David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alan AspuruGuzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular fingerprints. In Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015.
Vijay Prakash Dwivedi, Chaitanya K. Joshi, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks. CoRR, abs/2003.00982, 2020.
Jack Edmonds and Richard M. Karp. Theoretical improvements in algorithmic efficiency for network flow problems. Journal of the ACM (JACM), 1972. doi: 10.1145/321694.321699.
Alex Fout, Jonathon Byrd, Basir Shariat, and Asa Ben-Hur. Protein interface prediction using graph convolutional networks. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS'17, page 6533­6542, Red Hook, NY, USA, 2017. Curran Associates Inc. ISBN 9781510860964.
J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl. Neural message passing for quantum chemistry. 70, 2017a.
Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural message passing for quantum chemistry. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1263­1272. PMLR, 06­11 Aug 2017b.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, volume 9 of Proceedings of Machine Learning Research, pages 249­256, Chia Laguna Resort, Sardinia, Italy, 13­15 May 2010. PMLR.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc., 2014.
E. J. Gumbel. Statistical theory of extreme values and some practical applications: a series of lectures. US Govt. Print. Office, Number 33, 1954.
Rafael Gómez-Bombarelli, Jennifer N. Wei, David Duvenaud, José Miguel Hernández-Lobato, Benjamín Sánchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D. Hirzel, Ryan P. Adams, and Alán Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. ACS Central Science, 4(2):268­276, 2018. doi: 10.1021/acscentsci.7b00572. PMID: 29532027.
10

Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.
M. Hirn, S. Mallat, and N. Poilvert. Wavelet scattering regression of quantum chemical energies. Multiscale Modeling & Simulation, 15(2):827­863, Jan 2017. ISSN 1540-3459.
P. Hohenberg and W. Kohn. Inhomogeneous electron gas. Phys. Rev., 136:864­871, 1964. Truong Son Hy, Shubhendu Trivedi, Horace Pan, Brandon M. Anderson, , and Risi Kondor. Predicting molec-
ular properties with covariant compositional networks. The Journal of Chemical Physics, 148, 2018. John Ingraham and Debora Marks. Variational inference for sparse and undirected models. In Proceedings of
the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1607­1616. PMLR, 06­11 Aug 2017. Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In ICLR, 2017. Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for molecular graph generation. In Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 2323­2332. PMLR, 10­15 Jul 2018. Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, and Patrick Riley. Molecular graph convolutions: moving beyond fingerprints. Journal of Computer-Aided Molecular Design, 30(8):595­608, Aug 2016. ISSN 1573-4951. doi: 10.1007/s10822-016-9938-8. Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014. Thomas N. Kipf and Max Welling. Variational graph auto-encoders, 2016. Alexej Klushyn, Nutan Chen, Richard Kurle, Botond Cseke, and Patrick van der Smagt. Learning hierarchical priors in vaes. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. Sebastian Kmiecik, Dominik Gront, Michal Kolinski, Lukasz Wieteska, Aleksandra Elzbieta Dawid, and Andrzej Kolinski. Coarse-grained protein models and their applications. Chemical Reviews, 116(14):7898­ 7936, 2016. doi: 10.1021/acs.chemrev.6b00163. PMID: 27333362. Daphne Koller and Nir Friedman. In Probabilistic Graphical Models: Principles and Techniques. MIT Press, 2009. Risi Kondor, Nedelina Teneva, and Vikas Garg. Multiresolution matrix factorization. In Proceedings of the 31st International Conference on Machine Learning, volume 32 of Proceedings of Machine Learning Research, pages 1620­1628, Bejing, China, 22­24 Jun 2014. PMLR. Risi Kondor, Truong Son Hy, Horace Pan, Brandon M. Anderson, and Shubhendu Trivedi. Covariant compositional networks for learning graphs, 2018. Nils M. Kriege, Pierre-Louis Giscard, and Richard C. Wilson. On valid optimal assignment kernels and applications to graph classification. In Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS'16, page 1623­1631, Red Hook, NY, USA, 2016. Curran Associates Inc. ISBN 9781510838819. Yann LeCun, Corinna Cortes, and Christopher J.C. Burges. The mnist database of handwritten digits. URL http://yann.lecun.com/exdb/mnist/. Yujia Li, Richard Zemel, Marc Brockschmidt, and Daniel Tarlow. Gated graph sequence neural networks. In Proceedings of ICLR'16, April 2016. Yujia Li, Oriol Vinyals, Chris Dyer, Razvan Pascanu, and Peter W. Battaglia. Learning deep generative models of graphs. ICML'18, abs/1803.03324, 2018. Renjie Liao, Yujia Li, Yang Song, Shenlong Wang, Will Hamilton, David K Duvenaud, Raquel Urtasun, and Richard Zemel. Efficient graph generation with graph recurrent attention networks. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. Zinan Lin, Ashish Khetan, Giulia Fanti, and Sewoong Oh. Pacgan: The power of two samples in generative adversarial networks. In Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. Jenny Liu, Aviral Kumar, Jimmy Ba, Jamie Kiros, and Kevin Swersky. Graph normalizing flows. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. Qi Liu, Miltiadis Allamanis, Marc Brockschmidt, and Alexander Gaunt. Constrained graph variational autoencoders for molecule design. In Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.
11

Jianxin Ma, Peng Cui, Kun Kuang, Xin Wang, and Wenwu Zhu. Disentangled graph convolutional networks. In Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 4212­4221. PMLR, 09­15 Jun 2019.
Chris J Maddison, Daniel Tarlow, and Tom Minka. A* sampling. In Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc., 2014.
Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful graph networks. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019a.
Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph networks. In International Conference on Learning Representations, 2019b.
Haggai Maron, Ethan Fetaya, Nimrod Segol, and Yaron Lipman. On the universality of invariant networks. In Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 4363­4371. PMLR, 09­15 Jun 2019c.
Haggai Maron, Or Litany, Gal Chechik, and Ethan Fetaya. On learning sets of symmetric elements. In Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 6734­6744. PMLR, 13­18 Jul 2020.
F. Monti, D. Boscaini, J. Masci, E. Rodola, J. Svoboda, and M. M. Bronstein. Geometric deep learning on graphs and manifolds using mixture model cnns. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5425­5434, Los Alamitos, CA, USA, jul 2017. IEEE Computer Society. doi: 10.1109/CVPR.2017.576.
Kevin P. Murphy. Chapter 19: Undirected graphical models (markov random fields). In Machine Learning: A Probabilistic Perspective, pages 663­707. MIT Press, 2012.
M. Niepert, M. Ahmed, and K. Kutzkov. Learning convolutional neural networks for graphs. In Proceedings of the International Conference on Machine Learning, 2016a.
Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neural networks for graphs. In Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pages 2014­2023, New York, New York, USA, 20­22 Jun 2016b. PMLR.
B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), pages 701­710, 2014.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks, 2016.
Raghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole von Lilienfeld. Quantum chemistry structures and properties of 134 kilo molecules. Scientific Data, 1, 2014.
Rajesh Ranganath, Dustin Tran, and David Blei. Hierarchical variational models. In Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pages 324­333, New York, New York, USA, 20­22 Jun 2016. PMLR.
Sereina Riniker, Jane R. Allison, and Wilfred F. van Gunsteren. On developing coarse-grained models for biomolecular simulation: a review. Phys. Chem. Chem. Phys., 14:12423­12430, 2012. doi: 10.1039/C2CP40934H.
Lars Ruddigkeit, Ruud van Deursen, Lorenz C. Blum, and Jean-Louis Reymond. Enumeration of 166 billion organic small molecules in the chemical universe database gdb-17. Journal of Chemical Information and Modeling, 52(11):2864­2875, 2012. doi: 10.1021/ci300415d. PMID: 23088335.
Havard Rue and Leonhard Held. Gaussian markov random fields: Theory and applications. In Monographs on Statistics and Applied Probability, volume 104, London, 2005. Chapman & Hall.
Raif Rustamov and Leonidas J Guibas. Wavelets on graphs via deep learning. In Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc., 2013.
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61­80, 2009. doi: 10.1109/TNN. 2008.2005605.
Maximilian Seitzer. pytorch-fid: FID Score for PyTorch. https://github.com/mseitzer/pytorch-fid, August 2020. Version 0.1.1.
P. Sen, G. M. Namata, M. Bilgic, L. Getoor, B. Gallagher, , and T. Eliassi-Rad. Collective classification in network data. AI Magazine, 29(3):93­106, 2008.
Hadar Serviansky, Nimrod Segol, Jonathan Shlomi, Kyle Cranmer, Eilam Gross, Haggai Maron, and Yaron Lipman. Set2graph: Learning graphs from sets. In Advances in Neural Information Processing Systems, volume 33, pages 22080­22091. Curran Associates, Inc., 2020.
Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn, and Karsten M. Borgwardt. Weisfeiler-lehman graph kernels. Journal of Machine Learning Research, 12(77):2539­2561, 2011.
12

Donghyuk Shin, Si Si, and Inderjit S. Dhillon. Multi-scale link prediction. In Proceedings of the 21st ACM International Conference on Information and Knowledge Management, CIKM '12, page 215­224, New York, NY, USA, 2012. Association for Computing Machinery. ISBN 9781450311564. doi: 10.1145/2396761. 2396792.
Si Si, Donghyuk Shin, Inderjit S Dhillon, and Beresford N Parlett. Multi-scale spectral decomposition of massive graphs. In Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc., 2014.
Martin Simonovsky and Nikos Komodakis. Graphvae: Towards generation of small graphs using variational autoencoders. CoRR, abs/1802.03480, 2018.
Akash Srivastava, Lazar Valkov, Chris Russell, Michael U. Gutmann, and Charles Sutton. Veegan: Reducing mode collapse in gans using implicit variational learning. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.
Teague Sterling and John J. Irwin. Zinc 15 ­ ligand discovery for everyone. Journal of Chemical Information and Modeling, 55(11):2324­2337, 2015. doi: 10.1021/acs.jcim.5b00559. PMID: 26479676.
L. Tang and H. Liu. Leveraging social media networks for classification. Data Mining and Knowledge Discovery, 23(3):447­478, 2011.
Erik Henning Thiede, Truong Son Hy, and Risi Kondor. The general theory of permutation equivarant neural networks and higher order graph variational encoders. CoRR, abs/2004.03990, 2020.
Arash Vahdat and Jan Kautz. Nvae: A deep hierarchical variational autoencoder. In Advances in Neural Information Processing Systems, volume 33, pages 19667­19679. Curran Associates, Inc., 2020.
Petar Velickovic´, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018.
Martin J. Wainwright and Michael I. Jordan. A variational principle for graphical models. New Directions in Statistical Signal Processing, 2005.
Jiang Wang, Simon Olsson, Christoph Wehmeyer, Adrià Pérez, Nicholas E. Charron, Gianni de Fabritiis, Frank Noé, and Cecilia Clementi. Machine learning of coarse-grained molecular dynamics force fields. ACS Central Science, 5(5):755­767, 2019. doi: 10.1021/acscentsci.8b00913.
Hao Wu, Jonas Köhler, and Frank Noe. Stochastic normalizing flows. In Advances in Neural Information Processing Systems, volume 33, pages 5933­5944. Curran Associates, Inc., 2020.
Bingbing Xu, Huawei Shen, Qi Cao, Yunqi Qiu, and Xueqi Cheng. Graph wavelet neural network. In International Conference on Learning Representations, 2019.
Yiding Yang, Zunlei Feng, Mingli Song, and Xinchao Wang. Factorizable graph convolutional networks. In Advances in Neural Information Processing Systems, volume 33, pages 20286­20296. Curran Associates, Inc., 2020.
Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec. Hierarchical graph representation learning with differentiable pooling. In Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.
Jiaxuan You, Rex Ying, Xiang Ren, William Hamilton, and Jure Leskovec. GraphRNN: Generating realistic graphs with deep auto-regressive models. In Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 5708­5717. PMLR, 10­15 Jul 2018a.
Jiaxuan You, Rex Ying, Xiang Ren, William L. Hamilton, and Jure Leskovec. Code for graphrnn: Generating realistic graphs with deep auto-regressive model. 2018b.
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. Deep sets. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.
13

A Basic tensor operations

In order to build higher order equivariant networks, we revisit some basic tensor operations: tensor product (see Def. 10) and tensor contraction (see Def. 11). It can be shown that these tensor operations respect permutation equivariance. Based on them, we build our second order message passing networks.

Definition 10. The tensor product of A  Rna with B  Rnb yields a tensor C = A  B  Rna+b

where

C = A B i1,i2,..,ia+b

i1,i2,..,ia ia+1,ia+2,..,ia+b

Definition 11. The contraction of A  Rna along the pair of dimensions {x, y} (assuming x < y) yields a (a - 2)-th order tensor

C = i1,..,ix-1,ix+1,..,iy-1,iy+1,..,ia

Ai1 ,..,ia

ix ,iy

where we assume that ix and iy have been removed from amongst the indices of C. Using Einstein notation, this can be written more compactly as

C{i1,i2,..,ia}\{ix,iy} = Ai1,i2,..,ia ix iy

where  is the Kronecker delta. In general, the contraction of A along dimensions {x1, .., xp} yields a tensor C = Ax1,..,xp  Rna-p where

or compactly as

Ax1,..,xp =

...

Ai1 ,i2 ,..,ia

ix1 ix2

ixp

Ax1,..,xp = Ai1,i2,..,ia ix1 ix2 ..ixp

B Markov Random Fields

Undirected graphical models have been widely applied in the domains spatial or relational data, such as image analysis and spatial statistics. In general, k-th order graph encoders encode an undirected graph G = (V, E) into a k-th order latent z  Rnk×dz , with learnable parameters , can be represented as a parameterized Markov Random Field (MRF) or Markov network. Based on the Hammersley-Clifford theorem [Murphy, 2012] [Koller and Friedman, 2009], a positive distribution p(z) > 0 satisfies the conditional independent properties of an undirected graph G iff p can be represented as a product of potential functions , one per maximal clique, i.e.,

1

p(z|) = Z ()

c(zc|c)

(10)

cC

where C is the set of all the (maximal) cliques of G, and Z() is the partition function to ensure the overall distribution sums to 1, and given by

Z() =

c(zc|c)

z cC

Eq. 10 can be further written down as

p(z|)  v(zv|)

st(zst|) · · ·

c(zc|)

vV

(s,t)E

c=(i1 ,..,ik )Ck

where v, st, and c are the first order, second order and k-th order outputs of the encoder, corresponding to every vertex in V, every edge in E and every clique of size k in Ck, respectively. However, factorizing a graph into set of maximal cliques has an exponential time complexity, since the problem of determining if there is a clique of size k in a graph is known as an NP-complete problem. Thus, the factorization based on Hammersley-Clifford theorem is intractable. The second order encoder relaxes the restriction of maximal clique into edges, that is called as pairwise MRF:

p(z|)  st(zs, zt)
st

14

Our second order encoder inherits Gaussian MRF introduced by [Rue and Held, 2005] as pairwise MRF of the following form

p(z|)  st(zs, zt) t(zt)

st

t

where st(zs, zt) = exp

-

1 2

zsstzt

is the edge potential, and t(zt) = exp

-

1 2

tt

zt2

+

tzt

is the vertex potental. The joint distribution can be written in the information form of a multivariate

Gaussian in which

 = -1

 = µ

p(z|)  exp T z - 1 zT z

(11)

2

Sampling z from p(z|) in Eq. 11 is the same as sampling from the multivariate Gaussian N (µ, ). To ensure end-to-end equivariance, we set the latent layer to be two tensors µ  Rn×dz and   Rn×n×dz that corresponds to dz multivariate Gaussians, whose first index, and second index are first order and second order equivariant with permutations. Computation of  is trickier than µ,
simply because  must be invertible to be a covariance matrix. Thus, our second order encoder produces tensor L as the second order activation, and set  = LLT . The reparameterization trick
from Kingma and Welling [2014] is changed to

z=µ+L ,

 N (0, 1)

C Equivariant learnable prior

The original VAE published by Kingma and Welling [2014] limits each covariance matrix  to be
diagonal and the prior to be N (0, 1). Our second order encoder removes the diagonal restriction on the covariance matrix. Furthermore, we allow the prior N (µ^ , ^ ) to be learnable in which µ^ and ^ are parameters optimized by back propagation in a data driven manner. Importantly, ^ cannot be
learned directly due to the invertibility restriction. Instead, similarly to the second order encoder, a matrix L^ is optimized, and the prior covariance matrix is constructed by setting ^ = L^L^T . The Kullback-Leibler divergence between the two distributions N (µ, ) and N (µ^, ^ ) is as follows:

DKL(N (µ, )||N (µ^, ^ ))

=

1 2

tr(^ -1) + (µ^ - µ)T ^ -1(µ^ - µ) - n + ln

det ^ det 

(12)

Even though  is invertible, but gradient computation through the KL-divergence loss can be nu-
merical instable because of Cholesky decomposition procedure in matrix inversion. Thus, we add neglectable noise = 10-4 to the diagonal of both covariance matrices.

Importantly, during training, the KL-divergence loss breaks the permutation equivariance. Suppose

the set of vertices are permuted by a permutation matrix P  for   Sn. Since µ and  are the first

order

and

second

order

equivariant

outputs

of

the

encoder,

they

are

changed

to

P

µ

and

P

 P

T 

accordingly. But

DKL(N

(µ,

)||N

(µ^,

^ ))

=

DKL(N

(P



µ,

P

 P

T 

)||N

(µ^,

^ ))

To address the equivariance issue, we want to solve the following convex optimization problem that is our new equivariant loss function

min
Sn

DKL

(N

(P



µ,

P



P

T 

)||N

(µ^,

^ ))

(13)

However, solving the optimization based on Eq. 13 is computationally expensive. One solution is
to solve the minimum-cost maximum-matching in a bipartite graph (Hungarian matching) with the cost matrix Cij = ||µi - µ^j|| by O(n4) algorithm published by Edmonds and Karp [1972], that can be still improved further into O(n3). The Hungarian matching preserves equiariance, but is
still computationally expensive. In practice, instead of finding a optimal permutation, we apply a free-matching scheme to find an assignment matrix  such that: ij = 1 if and only if j = argminj ||µi - µ^j||, for each i  [1, n]. The free-matching scheme preserves equivariance and can be done efficiently in a simple O(n2) algorithm that is also suitable for GPU computation.

15

Method

Min Max STD KL divergence

Spectral

1 2020 177.52

3.14

K-Means

1 364 40.17

0.84

Learn to cluster 10 36 4.77

0.02

Table 4: Learning to cluster algorithm returns balanced cuts on Cora.

Method

Min Max STD KL divergence

Spectral

1 3320 292.21

4.51

K-Means

1 326 41.69

0.74

Learn to cluster 11 38 4.93

0.01

Table 5: Learning to cluster algorithm returns balanced cuts on Citeseer.

D Experiments

D.1 Link prediction on citation graphs

We compare our model MGVAE against popular methods in the field:

1. Spectral clustering (SC) [Tang and Liu, 2011] 2. Deep walks (DW) [Perozzi et al., 2014] 3. Variational graph autoencoder (VGAE) [Kipf and Welling, 2016]

Numerical results of SC and DW are experimental settings are taken from [Kipf and Welling, 2016]. We reran the implementation of VGAE as in [Kipf and Welling, 2016]. For MGVAE, we initialize weights by Glorot initialization [Glorot and Bengio, 2010]. We train for 2,048 epochs using Adam optimization [Kingma and Ba, 2015] with a starting learning rate of 0.01. Hyperparameters optimization (e.g. number of layers, dimension of the latent representation, etc.) is done on the validation set.
We propose our learning to cluster algorithm to achieve the balanced K-cut at every resolution level. Besides, we also implement two fixed clustering algorithms:

1. Spectral: It is similar to the one implemented in [Rustamov and Guibas, 2013].

·

First, we embed each

where

{n

,

n

}nmax
n=0

node i are the

 V into Rnmax as eigen-pairs of the

(1(i)/1(i), .., nmax graph Laplacian L =

(i)/nmax (i)), D-1(D - A)

where Dii = j Aij. We assume that 0  ..  nmax . In this case, nmax = 10.

· At the -th resolution level, we apply the K-Means clustering algorithm based on the above node embedding to partition graph G( ).

2. K-Means:

· First, we apply PCA to compress the sparse word frequency vectors (of size 1,433 on Cora and 3,703 on Citeseer) associating with each node into 10 dimensions.
· We use the compressed node embedding for the K-Means clustering.

Tables 4 and 5 show that our learning to cluster algorithm returns a much more balanced cut on the highest resolution level comparing to both Spectral and K-Means clusterings. For instance, we have L = 7 resolution levels and we partition the -th resolution into K = 2 clusters. Thus, on the bottom levels, we have 128 clusters. If we distribute nodes into clusters uniformly, the expected number of nodes in a cluster is 21.15 and 25.99 on Cora (2, 708 nodes) and Citeseer (3, 327 nodes), respectively. We measure the minimum, maximum, standard deviation of the numbers of nodes in 128 clusters. Furthermore, we measure the Kullback­Leibler divergence between the distribution of nodes into clusters and the uniform distribution. Our learning to cluster algorithm achieves low KL losses of 0.02 and 0.01 on Cora and Citeseer, respectively.

16

Dataset Method Validity Novelty Uniqueness

QM9

MLP decoder Sn decoder

100% 100%

99.98% 100%

77.62% 95.16%

Table 6: All-at-once MGVAE with MLP decoder vs. second order decoder.

D.2 Molecular graph generation
In this case, MGVAE and MGN are implemented with L = 3 resolution levels, and the -th resolution graph is partitioned into K = 2 -1 clusters. On each resolution level, the local encoders and local decoders are second-order Sn-equivariant networks with up to 4 equivariant layers. The number of channels for each node latent dz is set to 256. We apply two approaches for graph decoding:
1. All-at-once: MGVAE reconstructs all resolution adjacencies by equivariant decoder networks. Furthermore, we apply learnable equivariant prior as in Sec. C. Our second order encoders are interpreted as Markov Random Fields (see Sec. B). This approach preserves permutation equivariance. In addition, we implement a correcting process: the decoder network of the highest resolution level returns a probability for each edge, we sort these probabilities in a descending order and gradually add the edges in that order to satisfy all chemical constraints. Furthermore, we investigate the expressive power of the second order Sn-equivariant decoder by replacing it by a multilayer perceptron (MLP) decoder with 2 hidden layers of size 512 and sigmoid nonlinearity. We find that the higher order decoder outperforms the MLP decoder given the same encoding architecture. Table 6 shows the comparison between the two decoding models.
2. Autoregressive: This decoding process is constructed in an autoregressive manner similarly to [Liu et al., 2018]. First, we sample each vertex latent z independently. We randomly select a starting node v0, then we apply Breath First Search (BFS) to determine a particular node ordering from the node v0, however that breaks the permutation equivariance. Then iteratively we add/sample new edge to the existing graph Gt at the t-th iteration (given a randomly selected node v0 as the start graph G0) until completion. We apply second-order MGN with gated recurrent architecture to produce the probability of edge (u, v) where one vertex u is in the existing graph Gt and the another one is outside; and also the probability of its label. Intuitively, the decoding process is a sequential classification.
We randomly select 10,000 training examples for QM9; and 1,000 (autoregressive) and 10,000 (allat-once) training examples for ZINC. It is important to note that our training sets are much smaller comparing to other methods. For all of our generation experiments, we only use graph features as the input for the encoder such as one-hot atomic types and bond types. Since ZINC molecules are larger then QM9 ones, it is more difficult to train with the second order Sn-equivariant decoders (e.g., the number of bond/non-bond predictions or the number of entries in the adjacency matrices are proportional to squared number of nodes). Therefore, we input several chemical/atomic features computed from RDKit for the all-at-once MGVAE on ZINC (see Table 7). We concatenate all these features into a vector of size 24 for each atom.
We train our models with Adam optimization method [Kingma and Ba, 2015] with the initial learning rate of 10-3. Figs. 4 and 5 show some selected examples out of 5,000 generated molecules on QM9 by all-at-once MGVAE, while Fig. 6 shows the molecules generated by autoregressive MGN. Qualitatively, both the decoding approaches capture similar molecular substructures (bond structures). Fig. 9 shows an example of interpolation on the latent space on ZINC with the all-at-once MGVAE. Fig. 7 shows some generated molecules on ZINC by the all-at-once MGVAE. Fig. 8 and table 8 show some generated molecules by the autoregressive MGN on ZINC dataset with high Quantitative Estimate of Drug-Likeness (QED) computed by RDKit and their SMILES strings. On ZINC, the average QED score of the generated molecules is 0.45 with standard deviation 0.21. On QM9, the QED score is 0.44 ± 0.07.
D.3 Unsupervised molecular properties prediction on QM9
Density Function Theory (DFT) is the most successful and widely used approach of modern quantum chemistry to compute the electronic structure of matter, and to calculate many properties of
17

Feature
GetAtomicNum IsInRing IsInRingSize GetIsAromatic GetDegree GetExplicitValance GetFormalCharge GetIsotope GetMass GetNoImplicit GetNumExplicitHs GetNumImplicitHs GetNumRadicalElectrons GetTotalDegree GetTotalNumHs GetTotalValence

Type
Integer Boolean Boolean Boolean Integer Integer Integer Integer Double Boolean Integer Integer Integer Integer Integer Integer

Number
1 1 9 1 1 1 1 1 1 1 1 1 1 1 1 1

Description
Atomic number Belongs to a ring? Belongs to a ring of size k  {1, .., 9}? Aromaticity? Vertex degree Explicit valance Formal charge Isotope Atomic mass Allowed to have implicit Hs? Number of explicit Hs Number of implicit Hs Number of radical electrons Total degree Total number of Hs Total valance

Table 7: The list of chemical/atomic features used for the all-at-once MGVAE on ZINC. We denote each feature by its API in RDKit.

Figure 4: Some generated examples on QM9 by the all-at-once MGVAE with second order Snequivariant decoders.
molecular systems with high accuracy [Hohenberg and Kohn, 1964]. However, DFT is computationally expensive [Gilmer et al., 2017a], that leads to the use of machine learning to estimate the properties of compounds from their chemical structure rather than computing them explicitly with DFT [Hy et al., 2018]. To demonstrate that MGVAE can learn a useful molecular representations and capture important molecular structures in an unsupervised and variational autoencoding manner, we extract the highest resolution latents (at = L) and use them as the molecular representations for the downstream tasks of predicting DFT's molecular properties on QM9 including 13 learning targets. For the training, we normalize all learning targets to have mean 0 and standard deviation 1. The name, physical unit, and statistics of these learning targets are detailed in Table 9.
The implementation of MGVAE is the same as detailed in Sec. D.2. MGVAE is trained to reconstruct the highest resolution (input) adjacency, its coarsening adjacencies and the node atomic features. In this case, we do not use any chemical features: the node atomic features are just one-hot atomic types. After MGVAE is converged, to obtain the Sn-invariant molecular representation, we average the node latents at the L-th level into a vector of size 256. Finally, we apply a simple Multilayer Perceptron with 2 hidden layers of size 512, sigmoid nonlinearity and a linear layer on top to predict the molecular properties based on the extracted molecular representation. We compare the results
18

Figure 5: Some generated examples on QM9 by the all-at-once MGVAE with a MLP decoder instead of the second order Sn-equivariant one. It generates more tree-like structures.
Figure 6: Some generated examples on QM9 by the autoregressive MGN. in Mean Average Error (MAE) in the corresponding physical units with four methods on the same split of training and testing from [Hy et al., 2018]:
1. Support Vector Machine on optimal-assignment Weisfeiler-Lehman (WL) graph kernel [Shervashidze et al., 2011] [Kriege et al., 2016]
2. Neural Graph Fingerprint (NGF) [Duvenaud et al., 2015] 3. PATCHY-SAN (PSCN) [Niepert et al., 2016a] 4. Second order Sn-equivariant Covariant Compositional Networks (CCN 2D) [Kondor et al.,
2018] [Hy et al., 2018] Our unsupervised results show that MGVAE is able to learn a universal molecular representation in an unsupervised manner and outperforms WL in 12, NGF in 10, PSCN in 8, and CCN 2D in 8 out of 13 learning targets, respectively. There are other recent methods in the field that use several chemical and geometric information but comparing to them would be unfair.
19

Figure 7: Some generated examples on ZINC by the all-at-once MGVAE with second order Snequivariant decoders. In addition of graph features such as one-hot atomic types, we include several chemical features computed from RDKit (as in Table 7) as the input for the encoders. A generated example can contain more than one connected components, each of them is a valid molecule.
D.4 Supervised molecular properties prediction on ZINC
To further demonstrate the comprehensiveness of MGN, we apply our model in a supervised regression task to predict the solubility (LogP) on the ZINC dataset. We use the same split of 10K/1K/1K for training/validation/testing as in [Dwivedi et al., 2020]. The implementation of MGN is almost the same as detailed in Sec. D.2, except we include the latents of all resolution levels into the prediction. In particular, in each resolution level, we average all the node latents into a vector of size 256; then we concatentate all these vectors into a long vector of size 256 × L and apply a linear layer for the regression task. The baseline results are taken from [Yang et al., 2020] including:
1. Multilayer Perceptron (MLP), 2. Graph Convolution Networks (GCN), 3. Graph Attention Networks (GAT) [Velickovic´ et al., 2018], 4. MoNet [Monti et al., 2017], 5. Disentangled Graph Convolutional Networks (DisenGCN) [Ma et al., 2019], 6. Factorizable Graph Convolutional Networks (FactorGCN) [Yang et al., 2020], 7. GatedGCNE [Dwivedi et al., 2020] that uses additional edge information.
20

QED = 0.711

QED = 0.715

QED = 0.756

QED = 0.751

QED = 0.879

QED = 0.805

QED = 0.742

QED = 0.769

QED = 0.710

QED = 0.790

QED = 0.850

QED = 0.859

QED = 0.730

QED = 0.901

QED = 0.786

QED = 0.729

QED = 0.703

QED = 0.855

QED = 0.895

QED = 0.809

Figure 8: Some generated molecules on ZINC by the autoregressive MGN with high QED (druglikeness score).

21

Row Column SMILES

1

O=C1NC(CCCF)c2[nH]nnc21

1

2

OCC(OSBr)c1ccc(-c2cccc(Cl)c2Cl)[nH]1

3

C=CC1=CC=c2c(cc(=C3ONC(Cl)=C3Cl)[nH]c2=O)O1

4

COC(=CN1NC=CN1)C=C1C=CC(Cl)=CO1

1

[NH-]C(CNS1(=O)=NNc2c(F)cccc21)C1CC1

2

2

CS(=O)N1CC[SH](C)C(CNCc2ccccc2)C1

3

C=C(Cl)[SH](=O)(NC)C1c2ccc(Cl)c(n2)CC1O

4

CC(F)C(=C1C[NH2+]C([O-])N1)S(=O)Cc1ccccc1

1

CC1(NC2=CONN2c2ccccc2)C=C(C=O)N[N-]1

3

2

CC(=O)NN1N=C(C(O)c2cccc3ccoc23)C(=O)C1=O

3

C=CC(C)=C1C(F)=CC(C=C2ONN=NS2=O)=C1SCl

4

CCN1ON=C(C=C(Cl)c2ccco2)C(F)(F)C1=O

1

O=C(CCN(c1[nH+]cc(S)s1)c1ccc2cc1SC2)C1=NCC=C1Cl

4

2

CC=CNC1=C2Oc3ccccc3C(C)S2=S=N1

3

O=C(SC1=CC=NS1(=O)=O)c1ccc(Cl)cc1S1=NN=NN=N1

4

COCCNCc1cc2ccccc2[nH]1

1

ClC=C1CON=C(c2ncno2)N1CC(Cl)(Br)Br

5

2

CS(=O)(=O)c1ccc[nH+]c1SNCc1ccccc1Cl

3

O=S1(=O)CNS(=O)(N(c2ccccc2F)c2ccccc2Cl)=N1

4

O=C1NS(c2ccccc2Cl)=S2(=NSN=N2)O1

Table 8: SMILES of the generated molecules included in Fig. 8. Online drawing tool: https: //pubchem.ncbi.nlm.nih.gov//edit3/index.html

Figure 9: Interpolation on the latent space: we randomly select two molecules from ZINC and we reconstruct the corresponding molecular graphs on the interpolation line between the two latents.

Target
 Cv G gap H HOMO LUMO µ 1 R2 U U0 ZPVE

Unit
bohr3 cal/mol/K
eV eV eV eV eV D cm-1 bohr2 eV eV eV

Mean
75.2808 31.6204 -70.8352 6.8583 -77.0167 -6.5362 0.3220 2.6729 3504.1155
1189.4091 -76.5789 -76.1145 4.0568

STD
8.1729 4.0674 9.4975 1.2841 10.4884 0.5977 1.2748 1.5034 266.8982
280.4725 10.4143 10.3229 0.9016

Description
Norm of the static polarizability Heat capacity at room temperature Free energy of atomization Difference between HOMO and LUMO Enthalpy of atomization at room temperature Highest occupied molecular orbital Lowest unoccupied molecular orbital Norm of the dipole moment Highest fundamental vibrational frequency
Electronic spatial extent Atomization energy at room temperature Atomization energy at 0 K Zero point vibrational energy

Table 9: Description and statistics of 13 learning targets on QM9.

22

MODEL
GRAPHVAE DEEPGMG GRAPHRNN GNF MGVAE

COMMUNITY-SMALL

DEGREE CLUSTER ORBIT

0.35

0.98

0.54

0.22

0.95

0.4

0.08

0.12

0.04

0.20

0.20

0.11

0.002

0.01

0.01

EGO-SMALL

DEGREE CLUSTER ORBIT

0.13 0.04 0.09 0.03 1.74e-05

0.17 0.10 0.22 0.10 0.0006

0.05 0.02 0.003 0.001 6.53e-05

Table 10: Graph generation results depicting MMD for various graph statistics between the test set and generated graphs.

Figure 10: Top row includes generated examples and bottom row includes training examples on the synthetic 2-community dataset.
Our supervised result shows that MGN outperforms the state-of-the-art models in the field with a margin of 20%.
D.5 General graph generation by MGVAE
We further examine the expressive power of hierarchical latent structure of MGVAE in the task of general graph generation. We choose two datasets from GraphRNN paper [You et al., 2018a]:
· Community-small: A synthetic dataset of 100 2-community graphs where 12  |V |  20. · Ego-small: 200 3-hop ego networks extracted from the Citeseer network [Sen et al., 2008]
where 4  |V |  18.
The datasets are generated by the scripts from the GraphRNN codebase [You et al., 2018b]. We keep 80% of the data for training and the rest for testing. We evaluate our generated graphs by computing Maximum Mean Discrepancy (MMD) distance between the distributions of graph statistics on the test set and the generated set as proposed by [You et al., 2018a]. The graph statistics are node degrees, clustering coefficients, and orbit counts. As suggested by [Liu et al., 2019], we execute 15 runs with different random seeds, and we generate 1,024 graphs for each run, then we average the results over 15 runs. We compare MGVAE against GraphVAE [Simonovsky and Komodakis, 2018], DeepGMG [Li et al., 2018], GraphRNN [You et al., 2018a], and GNF [Liu et al., 2019]. The baselines are taken from Graph Normalizing Flows paper [Liu et al., 2019]. In our setting of (all-atonce) MGVAE, we implement only L = 2 levels of resolution and K = 2 clusters for each level. Our encoders have 10 layers of message passing. Instead of using a high order equivariant network as the global decoder for the bottom resolution, we only implement a simple fully connected network that maps the latent Z(L)  R|V|×dz into an adjacency matrix of size |V| × |V|. For the ego dataset in particular, we implement the learnable equivariant prior as in Sec. B and Sec.C. Table 10 includes our quantitative results in comparison with other methods. MGVAE outperforms all competing methods. Figs. 10 11 show some generated examples and training examples on the 2-community and ego datasets.
23

Generated examples
Training examples Figure 11: EGO-SMALL.
24

Method
DCGAN VEEGAN PACGAN PresGAN MGVAE

FID (32 × 32)
113.129 68.749 58.535 42.019 39.474

FID (16 × 16) N/A
64.289

FID (8 × 8) N/A
39.038

Table 11: Quantitative evaluation of the generated set by FID metric for each resolution level on
MNIST. It is important to note that the generation for each resolution is done separately: for the -th resolution, we sample a random vector of size dz = 256 from N (0, 1), and use the global decoder d( ) to decode into the corresponding image size. The baselines are taken from [Dieng et al., 2019].

D.6 Additional: Graph-based image generation by MGVAE

In this additional experiment, we apply MGVAE into the task of image generation. Instead of matrix representation, an image I  RH×W is represented by a grid graph of H · W nodes in which each
node represents a pixel, each edge is between two neighboring pixels, and each node feature is the corresponding pixel's color (e.g., R1 in gray scale, and R3 in RGB scale). Fig. 12 demonstrates an
exmaple of graph representation for images. Since images have natural spatial clustering, instead of
learning to cluster, we implement a fixed clustering procedure as follows:

· For the -th resolution level, we divide the grid graph of size H( ) × W ( ) into clusters

of

size

h

×

w

that

results

into

a

grid

graph

of

size

H( h

)

×

W( w

)
,

supposingly

h

and

w

are

divisible by H( ) and W ( ), respectively. Each resolution is associated with an image I( )

that is a zoomed out version of I( +1).

· The global encoder e( ) is implemented with 10 layers of message passing that operates on the whole H( ) × W ( ) grid graph. We sum up all the node latents into a single latent vector Z( )  Rdz . The global decoder d( ) is implemented by the convolutional neural network architecture of the generator of DCGAN model [Radford et al., 2016] to map Z( ) into an approximated image I^( ). The Sn-invariant pooler p( ) is a network operating on each small h × w grid graph to produce the corresponding node feature for the next level
+ 1. MGVAE is trained to reconstruct all resolution images. Fig. 13 shows an example of
reconstruction at each resolution on a test image of MNIST (after the network converged).

We evaluate our MGVAE architecture on the MNIST dataset [LeCun et al.] with 60,000 training examples and 10,000 testing examples. The original image size is 28 × 28. We pad zero pixels to get the image size of 25 × 25 (e.g., H(5) = W (5) = 32). Each cluster is a small grid graph of size 2 × 2 (e.g., h = w = 2). Accordingly, the image sizes for all resolutions are 32 × 32, 16 × 16, 8 × 8, etc. In this case, the whole network architecture is a 2-dimensional quadtree. The latent size dz is selected as 256. We train our model for 256 epochs by Adam optimizer [Kingma and Ba, 2015] with the initial learning rate 10-3. In the testing process, for the -th resolution, we sample a random vector of size dz from prior N (0, 1) and use the decoder d( ) to decode the corresponding image. We generate 10,000 examples for each resolution. We compute the Frechet Inception Distance (FID) proposed by [Heusel et al., 2017] between the testing set and the generated set as the metric to evaluate the quality of our generated examples. We use the FID implementation from [Seitzer, 2020]. We compare our MGVAE against variants of Generative Adversarial Networks (GANs) [Goodfellow et al., 2014] including DCGAN [Radford et al., 2016], VEEGAN [Srivastava et al., 2017], PacGAN [Lin et al., 2018], and PresGAN [Dieng et al., 2019]. Table 11 shows our quantitative results in comparison with other competing generative models. The baseline results are taken from Prescribed Generative Adversarial Networks paper [Dieng et al., 2019]. MGVAE outperforms all the baselines for the highest resolution generation. Figs. 14 and 15 show some generated examples of the 32 × 32 and 16 × 16 resolution, respectively.

25

Figure 12: An image of digit 8 from MNIST (left) and its grid graph representation at 16 × 16 resolution level (right).

32 × 32

16 × 16

8×8

Target

Reconstruction Figure 13: An example of reconstruction on each resolution level for a test image in MNIST.
26

Figure 14: Generated examples at the highest 32 × 32 resolution level. 27

Figure 15: Generated examples at the 16 × 16 resolution level. 28

