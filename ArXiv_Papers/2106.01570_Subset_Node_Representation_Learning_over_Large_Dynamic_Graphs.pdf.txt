Subset Node Representation Learning over Large Dynamic Graphs

arXiv:2106.01570v1 [cs.SI] 3 Jun 2021 Embedding movement

Xingzhi Guo
xingzguo@cs.stonybrook.edu Stony Brook University Stony Brook, USA

Baojian Zhou
baojian.zhou@cs.stonybrook.edu Stony Brook University Stony Brook, USA

Steven Skiena
skiena@cs.stonybrook.edu Stony Brook University Stony Brook, USA

ABSTRACT
Dynamic graph representation learning is a task to learn node embeddings over dynamic networks, and has many important applications, including knowledge graphs, citation networks to social networks. Graphs of this type are usually large-scale but only a small subset of vertices are related in downstream tasks. Current methods are too expensive to this setting as the complexity is at best linear-dependent on both the number of nodes and edges.
In this paper, we propose a new method, namely Dynamic Personalized PageRank Embedding (DynamicPPE) for learning a target subset of node representations over large-scale dynamic networks. Based on recent advances in local node embedding and a novel computation of dynamic personalized PageRank vector (PPV), DynamicPPE has two key ingredients: 1) the per-PPV complexity is O (¯/) where , ¯, and  are the number of edges received, average degree, global precision error respectively. Thus, the per-edge event update of a single node is only dependent on ¯ in average; and 2) by using these high quality PPVs and hash kernels, the learned embeddings have properties of both locality and global consistency. These two make it possible to capture the evolution of graph structure effectively.
Experimental results demonstrate both the effectiveness and efficiency of the proposed method over large-scale dynamic networks. We apply DynamicPPE to capture the embedding change of Chinese cities in the Wikipedia graph during this ongoing COVID-19 pandemic 1. Our results show that these representations successfully encode the dynamics of the Wikipedia graph.

G t-1

v1

v2

4 ×10-5 Joe Biden Donald Trump
3

Election Inauguration

Gt e1

G = {e1, e2, e3, e4}

e3

v1

v2

e4

e2

{wvt1 , wvt2 , . . .} = DynamicPPE (Gt, {v1, v2, . . .})
(a) Dynamic graph model

2
1
0 12 13 14 15 16 17 18 19 20
Year (b) An application of DynamicPPE

Figure 1: (a) The model of dynamic network in two consecutive snapshots. (b) An application of DynamicPPE to keep track embedding movements of interesting Wikipedia articles (vertices). We learn embeddings of two presidents of the United States on the whole English Wikipedia graph from 2012 monthly, which cumulatively involves 6.2M articles (nodes) and 170M internal links (edges). The embedding movement between two time points is defined as 1 - cos( , +1) where cos(·, ·) is the cosine similarity. The significant embedding movements may reflect big social status changes of Donald_Trump and Joe_Biden 2in this dynamic Wikipedia graph.
'21), August 14­18, 2021, Virtual Event, Singapore. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/XXXXXXXXXXXXX
1 INTRODUCTION

KEYWORDS
Dynamic graph embedding; Representation learning; Personalized PageRank; Knowledge evolution
ACM Reference Format: Xingzhi Guo, Baojian Zhou, and Steven Skiena. 2021. Subset Node Representation Learning over Large Dynamic Graphs. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD
Both authors contributed equally to this research 1 https://en.wikipedia.org/wiki/CO VID- 19_pandemic
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. KDD '21, August 14­18, 2021, Virtual Event, Singapore. © 2021 Association for Computing Machinery. ACM ISBN 978-1-4503-8332-5/21/08. . . $15.00 https://doi.org/10.1145/XXXXXXXXXXXXX

Graph node representation learning aims to represent nodes from graph structure data into lower dimensional vectors and has received much attention in recent years [12, 15, 16, 23, 29, 31, 37]. Effective methods have been successfully applied to many real-world applications where graphs are large-scale and static [43]. However, networks such as social networks [4], knowledge graphs [20], and citation networks [7] are usually time-evolving where edges and nodes are inserted or deleted over time. Computing representations of all vertices over time is prohibitively expensive because only a small subset of nodes may be interesting in a particular application. Therefore, it is important and technical challenging to efficiently learn dynamic embeddings for these large-scale dynamic networks under this typical use case.
Specifically, we study the following dynamic embedding problem: We are given a subset  = {1, 2, . . . ,  } and an initial graph G with  = 0. Between time  and  + 1, there are edge events of insertions and/or deletions. The task is to design an algorithm to
2Two English Wikipedia articles are accessible at https://en.wikipedia.org/wiki/ Donald_Trump and https://en.wikipedia.org/wiki/Joe_Biden.

learn embeddings for  nodes with time complexity independent on the number of nodes  per time  where   . This problem setting is both technically challenging and practically important. For example, in the English Wikipedia graph, one need focus only on embedding movements of articles related to political leaders, a tiny portion of whole Wikipedia. Current dynamic embedding methods [10, 28, 47, 49, 50] are not applicable to this large-scale problem setting due to the lack of efficiency. More specifically, current methods have the dependence issue where one must learn all embedding vectors. This dependence issue leads to per-embedding update is linear-dependent on , which is inefficient when graphs are large-scale. This obstacle motivates us to develop a new method.
In this paper, we propose a dynamic personalized PageRank embedding (DynamicPPE) method for learning a subset of node representations over large-sale dynamic networks. DynamicPPE is based on an effective approach to compute dynamic PPVs [45]. There are two challenges of using Zhang et al. [45] directly: 1) the quality of dynamic PPVs depend critically on precision parameter , which unfortunately is unknown under the dynamic setting; and 2) The update of per-edge event strategy is not suitable for batch update between graph snapshots. To resolve these two difficulties, first, we adaptively update  so that the estimation error is independent of , , thus obtaining high quality PPVs. Yet previous work does not give an estimation error guarantee. We prove that the time complexity is only dependent on ¯. Second, we incorporate a batch update strategy inspired from [13] to avoid frequent peredge update. Therefore, the total run time to keep track of  nodes for given snapshots is O (¯). Since real-world graphs have the sparsity property ¯  , it significantly improves the efficiency compared with previous methods. Inspired by InstantEmbedding [30] for static graph, we use hash kernels to project dynamic PPVs into embedding space. Figure 1 shows an example of successfully applying DynamicPPE to study the dynamics of social status in the English Wikipedia graph. To summarize, our contributions are:
(1) We propose a new algorithm DynamicPPE, which is based on the recent advances of local network embedding on static graph and a novel computation of dynamic PPVs. DynamicPPE effectively learns PPVs and then projects them into embedding space using hash kernels.
(2) DynamicPPE adaptively updates the precision parameter  so that PPVs always have a provable estimation error guarantee. In our subset problem setting, we prove that the time and space complexity are all linear to the number of edges  but independent on the number of nodes , thus significantly improve the efficiency.
(3) Node classification results demonstrate the effectiveness and efficiency of the proposed. We compile three large-scale datasets to validate our method. As an application, we showcase that learned embeddings can be used to detect the changes of Chinese cities during this ongoing COVID-19 pandemic articles on a large-scale English Wikipedia.
The rest of this paper is organized as follows: In Section 2, we give the overview of current dynamic embedding methods. The problem definition and preliminaries are in Section 3. We present our proposed method in Section 4. Experimental results are reported in Section 5. The discussion and conclusion will be presented in

Section 6. Our code and created datasets are accessible at https: //github.com/zjlxgxz/DynamicPPE.
2 RELATED WORK
There are two main categories of works for learning embeddings from the dynamic graph structure data. The first type is focusing on capturing the evolution of dynamics of graph structure [49]. The second type is focusing on both dynamics of graph structure and features lie in these graph data [38]. In this paper, we focus on the first type and give the overview of related works. Due to the large mount of works in this area, some related works may not be included, one can find more related works in a survey [22] and references therein. Dynamic latent space models The dynamic embedding models had been initially explored by using latent space model [18]. The dynamic latent space model of a network makes an assumption that each node is associated with an -dimensional vector and distance between two vectors should be small if there is an edge between these two nodes [32, 33]. Works of these assume that the distance between two consecutive embeddings should be small. The proposed dynamic models were applied to different applications [17, 34]. Their methods are not scalable from the fact that the time complexity of initial position estimation is at least O (2) even if the per-node update is log(). Incremental SVD and random walk based methods Zhang et al. [48] proposed TIMERS that is an incremental SVD-based method. To prevent the error accumulation, TIMERS properly set the restart time so that the accumulated error can be reduced. Nguyen et al. [28] proposed continuous-time dynamic network embeddings, namely CTDNE. The key idea of CTNDE is that instead of using general random walks as DeepWalk [29], it uses temporal random walks contain a sequence of edges in order. Similarly, the work of Du et al. [10] was also based on the idea of DeepWalk. These methods have time complexity dependent on  for per-snapshot update. Zhou et al. [49] proposed to learn dynamic embeddings by modeling the triadic closure to capture the dynamics. Graph neural network methods Trivedi et al. [38] designed a dynamic node representation model, namely DyRep, as modeling a latent mediation process where it leverages the changes of node between the node's social interactions and its neighborhoods. More specifically, DyRep contains a temporal attention layer to capture the interactions of neighbors. Zang and Wang [44] proposed a neural network model to learn embeddings by solving a differential equation with ReLU as its activation function. [24] presents a dynamic embedding, a recurrent neural network method, to learn the interactions between users and items. However, these methods either need to have features as input or cannot be applied to largescale dynamic graph. Kumar et al. [24] proposed an algorithm to learn the trajectory of the dynamic embedding for temporal interaction networks. Since the learning task is different from ours, one can find more details in their paper.
3 NOTATIONS AND PRELIMINARIES
Notations We use [] to denote a ground set [] := {0, 1, . . . ,  - 1}. The graph snapshot at time  is denoted as G V , E . The degree of a node  is  () . In the rest of this paper, the average

degree at time  is ¯ and the subset of target nodes is   V . Bold

capitals, e.g. ,  are matrices and bold lower letters are vectors

, . More specifically, the embedding vector for node  at time

 denoted as   R and  is the embedding dimension. The -

th entry of 

is



 

(

)



R. The embedding of node 

for all 

snapshots is written as  = [1, 2, . . . ,  ]. We use  and 

as the number of nodes and edges in G which we simply use 

and  if time  is clear in the context.

Given the graph snapshot G and a specific node , the person-

alized PageRank vector (PPV) is an -dimensional vector   R and the corresponding -th entry is  (). We use   R to stand for a calculated PPV obtained from a specific algorithm. Similarly,

the corresponding -th entry is  (). The teleport probability of the PageRank is denoted as . The estimation error of an embedding vec-

tor is the difference between true embedding  and the estimated

embedding ^ is measure by  · 1 :=

  =1



 

(

)

- ^ ()

.

3.1 Dynamic graph model and its embedding
Given any initial graph (could be an empty graph), the corresponding dynamic graph model describes how the graph structure evolves over time. We first define the dynamic graph model, which is based on Kazemi and Goel [22].

Definition 1 (Simple dynamic graph model [22]). A simple dynamic graph model is defined as an ordered of snapshots G0, G1, G2, . . . , G where G0 is the initial graph. The difference of graph G at time  = 1, 2, . . . , is  (V , E ) :=  \-1 with V := V \V-1 and E := E \E-1. Equivalently, G corre-
sponds to a sequence of edge events as the following

G =





1, 2, . . . , 

,

(1)

where each edge event  has two types: insertion or deletion, i.e,

 = (, , event) where event  {Insert, Delete} 3.


The above model captures evolution of a real-world graph naturally where the structure evolution can be treated as a sequence of edge events occurred in this graph. To simplify our analysis, we assume that the graph is undirected. Based on this, we define the subset dynamic representation problem as the following.

Definition 2 (Subset dynamic network embedding problem). Given a dynamic network model G0, G1, G2, . . . , G define in Definition 1 and a subset of target nodes  = {1, 2, . . . ,  }, the subset dynamic network embedding problem is to learn dynamic embeddings of 
snapshots for all  nodes  where   . That is, given any node   , the goal is to learn embedding matrix for each node   , i.e.



:=

[1, 2, . .

. ,  ]

where 




R

and





.

(2)

3.2 Personalized PageRank
Given any node  at time , the personalized PageRank vector for graph G is defined as the following
Definition 3 (Personalized PageRank (PPR)). Given normalized adjacency matrix  = -1 where  is a diagonal matrix with  (, ) =  () and  is the adjacency matrix, the PageRank vector
3The node insertion can be treated as inserting a new edge and then delete it and node deletion is a sequence of deleting its edges.

 with respect to a source node  is the solution of the following

equation

 =   1 + (1 - )  ,

(3)

where 1 is the unit vector with 1 () = 1 when  = , 0 otherwise.

There are several works on computing PPVs for static graph [1, 2, 5]. The idea is based a local push operation proposed in [2]. Interestingly, Zhang et al. [45] extends this idea and proposes a novel updating strategy for calculating dynamic PPVs. We use a modified version of it as presented in Algorithm 1.

Algorithm 1 ForwardPush [45]

1: Input: , , G, ,  = 0 2: while ,  () >  () do 3: Push()

4: while ,  () < - () do 5: Push()

6: return (,  )

7: procedure Push()

8:  () +=  ()

9: for   Nei() do

10:

 () += (1 - ) () (1 - )/ ()

11:  () = (1 - ) ()

Algorithm 1 is a generalization from Andersen et al. [2] and there are several variants of forward push [2, 5, 26], which are dependent on how  is chosen (we assume  = 0). The essential idea of forward push is that, at each Push step, the frontier node  transforms her  residual probability  () into estimation probability  () and then pushes the rest residual to its neighbors. The algorithm repeats this push operation until all residuals are small enough 4. Methods based on local push operations have the following invariant property.

Lemma 4 (Invariant property [19]). ForwardPush has the follow-

ing invariant property



 () =  () +  () (),   V.

(4)

 

The local push algorithm can guarantee that the each entry of the estimation vector  () is very close to the true value  (). We state this property as in the following

Lemma 5 ([2, 45]). Given any graph G(V, E) with  = 0,  = 1

and a constant , the run time for ForwardLocalPush is at most

1-  1 

and the estimation error of  ()

for each node 

is at most

, i.e. | () -  ()|/ ()|  

The main challenge to directly use forward push algorithm to obtain high quality PPVs in our setting is that: 1) the quality  return by forward push algorithm will critically depend on the precision parameter  which unfortunately is unknown under our dynamic problem setting. Furthermore, the original update of peredge event strategy proposed in [45] is not suitable for batch update

4There are two implementation of forward push depends on how frontier is selected. One is to use a first-in-first-out (FIFO) queue [11] to maintain the frontiers while the other one maintains nodes using a priority queue is used [5] so that the operation cost is O (1/) instead of O (log /).

between graph snapshots. Guo et al. [13] propose to use a batch strategy, which is more practical in real-world scenario where there is a sequence of edge events between two consecutive snapshots. This motivates us to develop a new algorithm for dynamic PPVs and then use these PPVs to obtain high quality dynamic embedding vectors.

4 PROPOSED METHOD
To obtain dynamic embedding vectors, the general idea is to obtain dynamic PPVs and then project these PPVs into embedding space by using two kernel functions [30, 42]. In this section, we present our proposed method DynamicPPE where it contains two main components: 1) an adaptive precision strategy to control the estimation error of PPVs. We then prove that the time complexity of this dynamic strategy is still independent on . With this quality guarantee, learned PPVs will be used as proximity vectors and be "projected" into lower dimensional space based on ideas of Verse [39] and InstantEmbedding [30]. We first show how can we get high quality PPVs and then present how use PPVs to obtain dynamic embeddings. Finally, we give the complexity analysis.

4.1 Dynamic graph embedding for single batch

For each batch update  , the key idea is to dynamically maintain

PPVs where the algorithm updates the estimate from -1 to  and

its

residuals

from

  -1

to



 

.

Our

method

is

inspired

from

Guo

et

al.

[13] where they proposed to update a personalized contribution

vector by using the local reverse push 5. The proposed dynamic

single node embedding, DynamicSNE is illustrated in Algorithm 2.

It takes an update batch G (a sequence of edge events), a target

node  with a precision  , estimation vector of  and residual vector

as inputs. It then obtains an updated embedding vector of  by the

following three steps: 1) It first updates the estimate vector  and  from Line 2 to Line 9; 2) It then calls the forward local push method

to obtain the updated estimations,  ; 3) We then use the hash kernel projection step to get an updated embedding. This projection

step is from InstantEmbedding where two universal hash functions are defined as  : N  [] and sgn : N  {±1} 6. Then the hash

kernel based on these two hash functions is defined as sgn, () :

R  R where each entity  is  -1 ()  sgn ( ). Different 
from random projection used in RandNE [47] and FastRP [6], hash

functions has O (1) memory cost while random projection based

method has O () if the Gaussian matrix is used. Furthermore,

hash kernel keeps unbiased estimator for the inner product [42].

In the rest of this section, we show that the time complexity is O (¯/) in average and the estimation error of learned PPVs

measure by  · 1 can also be bounded. Our proof is based on the

following lemma which follows from Guo et al. [13], Zhang et al.

[45].

Lemma 6. Given current graph G and an update batch G , the total run time of the dynamic single node embedding, DynamicSNE

5One should notice that, for undirected graph, PPVs can be calculated by using the invariant property from the contribution vectors. However, the invariant property does not hold for directed graph. It means that one cannot use reverse local push to get a personalized PageRank vector directly. In this sense, using forward push algorithm is more general for our problem setting. 6For example, in our implementation, we use MurmurHash https://github.com/ aappleby/smhasher

for obtaining embedding  is bounded by the following





-1 1 -  1 

+



2 -  -1 ()   ()

(5)

 G

Proof. We first make an assumption that there is only one edge update (, , event) in G , then based Lemma 14 of [46], the run time of per-edge update is at most:

-1 1

-

-1 1

+

 () ,

(6)





where  ()

=

2- 

-1 ()

 ()

.

Suppose

there

are



edge events in

G . We still obtain a similar bound, by the fact that forward push

algorithm has monotonicity property: the entries of estimates  () only increase when it pushes positive residuals (Line 2 and 3 of Al-

gorithm 1). Similarly, estimates  () only decrease when it pushes negative residuals (Line 4 and 5 of Algorithm 1). In other words, the

amount of work for per-edge update is not less than the amount of

work for per-batch update. Similar idea is also used in [13]. 

Algorithm 2 DynamicSNE(G , G , , -1, -1,  , )

1: Input: graph G , G , target node , precision , teleport 

2: for (, , op)   do

3: if op == Insert(, ) then

4:



=




-1

()/(

()

-

1)

5: if op == Delete(, ) then

6:

 = --1 ()/( () + 1)

7: -1 ()  -1 () +  8: -1 ()  -1 () -  / 9: -1 ()  -1 () +  / - 
10:  = ForwardPush(-1, -1, G ,  , ) 11:  = 0 12: for   { :  ()  0,   V } do 13:  ( ()) += sgn () max log  () , 0

Theorem 7. Given any graph snapshot G and an update batch 
where there are  edge events and suppose the precision parameter is  and teleport probability is , DynamicSNE runs in O ( /2 + ¯ /(2) +  /()) with  = /

Proof. Based lemma 6, the proof directly follows from Theorem

12 of [46].



The above theorem has an important difference from the pre-
vious one [45]. We require that the precision parameter will be
small enough so that  -  1 can be bound ( will be discussed later). As we did the experiments in Figure 2, the fixed epsilon will
make the embedding vector bad. We propose to use a dynamic precision parameter, where   O ( / ), so that the 1-norm can be properly bounded. Interestingly, this high precision parameter strategy will not case too high time complexity cost from O (/2) to O (¯/(2)). The bounded estimation error is presented in the
following theorem.

Theorem 8 (Estimation error). Given any node , define the estima-

tion error of PPVs learned from DynamicSNE at time  as  -  1,

if

we

are

given

the

precision

parameter




=

/ , the estimation

error can be bounded by the following

 -  1  ,

(7)

where we require   2 7 and  is a global precision parameter of

DynamicPPE independent on  and  .

Proof. Notice that for any node , by Lemma 5, we have the following inequality

|

()

-




( ) |






().

Summing all these inequalities over all nodes , we have



 -  1 =




()

-




()

 V













()

=


 

=

 = .

 V





Macro-F1 Micro-F1

0.6

0.5

0.4

0.3

= 10-6

= 10-7

0.2

= 10-8

= 10-9

01 03 05 07 09 11 13 15 17 19 Year

0.7

0.6

0.5

= 10-6

0.4

= 10-7

= 10-8

0.3

= 10-9

01 03 05 07 09 11 13 15 17 19 Year

Figure 2:  as a function of year for the task of node classification on the English Wikipedia graph. Each line corresponds to a fixed precision strategy of DynamicSNE. Clearly, when the precision parameter  decreases, the performance of node classification improves.

The above theorem gives estimation error guarantee of  , which is critically important for learning high quality embeddings. First of all, the dynamic precision strategy is inevitable because the precision is unknown parameter for dynamic graph where the number of nodes and edges could increase dramatically over time. To demonstrate this issue, we conduct an experiments on the English Wikipedia graph where we learn embeddings over years and validate these embeddings by using node classification task. As shown in Figure 2, when we use the fixed parameter, the performance of node classification is getting worse when the graph is getting bigger. This is mainly due to the lower quality of PPVs. Fortunately, the adaptive precision parameter / does not make the run time increase dramatically. It only dependents on the average degree ¯ . In practice, we found  = 0.1 are sufficient for learning effective embeddings.
7By noticing that  -  1   1 +  1  2, any precision parameter larger than 2 will be meaningless.

4.2 DynamicPPE
Our finally algorithm DynamicPPE is presented in Algorithm 3. At every beginning, estimators  are set to be zero vectors and residual vectors  are set to be unit vectors with mass all on one entry (Line 4). The algorithm then call the procedure DynamicSNE with an empty batch as input to get initial PPVs for all target nodes 8 (Line 5). From Line 6 to Line 9, at each snapshot , it gets an update batch G at Line 7 and then calls DynamicSNE to obtain the updated embeddings for every node .

Algorithm 3 DynamicPPE(G0, , , )

1: Input: initial graph G0, target set , global precision , teleport

probability 

2:  = 0

3: for    := {1, 2, . . . ,  } do
4:  = 0,  = 1 5: DynamicSNE(G0, , ,  ,  , 1/0, )

6: for   {1, 2, . . . , } do 7: read a sequence of edge events G := G \G-1

8: for    := {1, 2, . . . ,  } do

9:

 = DynamicSNE(G , G , , -1, -1, / , )

10: return  = [1, 2, . . . ,  ],   .

Based on our analysis, DynamicPPE is an dynamic version of InstantEmbedding. Therefore, DynamicPPE has two key properties observed in [30]: locality and global consistency. The embedding quality is guaranteed from the fact that InstantEmbedding implicitly factorizes the proximity matrix based on PPVs [39].

4.3 Complexity analysis
Time complexity The overall time complexity of DynamicPPE is the  times of the run time of DynamicSNE. We summarize the time complexity of DynamicPPE as in the following theorem

Theorem 9. The time complexity of DynamicPPE for learning a

subset of 

nodes

is

O

(

 2

+



 

¯
2

+ 


+  min{,  })


Proof. We follow Theorem 7 and summarize all run time to-

gether to get the final time complexity.



Space complexity The overall space complexity has two parts:

1) O () to store the graph structure information; and 2) the storage

of keeping nonzeros of  and  . From the fact that local push oper-

ation

[2],

the

number

of

nonzeros

in



is

at

most

1 

.

Thus,

the

total

storage for saving these vectors are O ( min{,  }). Therefore,



the total space complexity is O ( +  min(,  )).



Implementation Since learning the dynamic node embedding

for any node  is independent with each other, DynamicPPE is are

easy to parallel. Our current implementation can take advantage of

multi-cores and compute the embeddings of  in parallel.

8For the situation that some nodes of  has not appeared in G yet, it checks every batch update until all nodes are initialized.

5 EXPERIMENTS
To demonstrate the effectiveness and efficiency of DynamicPPE, in this section, we first conduct experiments on several small and large scale real-world dynamic graphs on the task of node classification, followed by a case study about changes of Chinese cities in Wikipedia graph during the COVID-19 pandemic.
5.1 Datasets
We compile the following three real-world dynamic graphs, more details can be found in Appendix C. Enwiki20 English Wikipedia Network We collect the internal Wikipedia Links (WikiLinks) of English Wikipedia from the beginning of Wikipedia, January 11th, 2001, to December 31, 2020 9. The internal links are extracted using a regular expression proposed in [8]. During the entire period, we collection 6,151,779 valid articles10. We generated the WikiLink graphs only containing edge insertion events. We keep all the edges existing before Dec. 31 2020, and sort the edge insertion order by the creation time. There are 6,216,199 nodes and 177,862,656 edges during the entire period. Each node either has one label (Food, Person, Place,...) or no label. Patent (US Patent Graph) The citation network of US patent[14] contains 2,738,011 nodes with 13,960,811 citations range from year 1963 to 1999. For any two patents  and , there is an edge (, ) if the patent  cites patent . Each patent belongs to six different types of patent categories. We extract a small weakly-connected component of 46,753 nodes and 425,732 edges with timestamp, called Patent-small. Coauthor We extracted the co-authorship network from the Microsoft Academic Graph [35] dumped on September 21, 2019. We collect the papers with less than six coauthors, keeping those who has more than 10 publications, then build undirected edges between each coauthor with a timestamp of the publication date. In addition, we gather temporal label (e.g.: Computer Science, Art, ... ) of authors based on their publication's field of study. Specifically we assign the label of an author to be the field of study where s/he published the most up to that date. The original graph contains 4,894,639 authors and 26,894,397 edges ranging from year 1800 to 2019. We also sampled a small connected component containing 49,767 nodes and 755,446 edges with timestamp, called Coauthor-small. Academic The co-authorship network is from the academic network [36, 49] where it contains 51,060 nodes and 794,552 edges. The nodes are generated from 1980 to 2015. According to the node classification setting in Zhou et al. [49], each node has either one binary label or no label.
5.2 Node Classification Task
Experimental settings We evaluate embedding quality on binary classification for Academic graph (as same as in [49]), while using multi-class classification for other tasks. We use balanced logistic regression with 2 penalty in on-vs-rest setting, and report the Macro-F1 and Macro-AUC (ROC) from 5 repeated trials with 10% training ratio on the labeled nodes in each snapshot, excluding dangling nodes. Between each snapshot, we insert new edges and keep the previous edges.
9We collect the data from the dump https://dumps.wikimedia.org/enwiki/20210101/ 10A valid Wikipedia article must be in the 0 namespace

We conduct the experiments on the aforementioned small and large scale graph. In small scale graphs, we calculate the embeddings of all nodes and compare our proposed method (DynPPE.) against other state-of-the-art models from three categories11. 1) Random walk based static graph embeddings (Deepwalk12 [29], Node2Vec13 [12]); 2) Random Projection-based embedding method which supports online update: RandNE14 [47]; 3) Dynamic graph embedding method: DynamicTriad (DynTri.) 15 [49] which models the dynamics of the graph and optimized on link prediction.
Table 1: Node classification task on the Academic, Patent Small, Coauthor Small graph on the final snapshot

Static Node2Vec method Deepwalk
DynTri. Dynamic RandNE method DynPPE.
Static Node2Vec method Deepwalk
DynTri. Dynamic RandNE method DynPPE.

Academic Patent Small F1 AUC F1 AUC
 = 128 0.833 0.975 0.648 0.917

0.834 0.817 0.587 0.808
0.841 0.842 0.811 0.722 0.842

0.975 0.650
0.969 0.560 0.867 0.428 0.962 0.630  = 512 0.975 0.677 0.975 0.680 0.965 0.659 0.942 0.560 0.973 0.682

0.919 0.866 0.756 0.911
0.931 0.931 0.915 0.858 0.934

Coauthor Small F1 AUC

0.477
0.476
0.435 0.337 0.448

0.955
0.950 0.943 0.830 0.951

0.486 0.495 0.492 0.493

0.955 0.955 0.952 0.895

0.509 0.958

Table 2: Total CPU time for small graphs (in second). RandNE-I is with orthogonal projection, the performance is slightly better, but the running time is significantly increased. RandNE-II is without orthogonal projection.

Deepwalk16 Node2vec DynTri. RandNE-I RandNE-II DynPPE.

Academic 498211.75 4584618.79 247237.55 12732.64 1583.08 18419.10

Patent Small 181865.56 2031090.75 117993.36 9637.15 9208.03 3651.59

Coauthor Small 211684.86 1660984.42 108279.4 8436.79 177.89 21323.74

Table 1 shows the classification results on the final snapshot. When we restrict the dimension to be 128, we observe that static methods outperform all dynamic methods. However, the static methods independently model each snapshot and the running time grows with number of snapshots, regardless of the number of edge changes. In addition, our proposed method (DynPPE.) outperforms other dynamic baselines, except that in the academic graph, where DynTri. is slightly better. However, their CPU time is 13 times
11Appendix D shows the hyper-parameter settings of baseline methods 12 https://pypi.org/project/deepwalk/ 13 https://github.com/aditya- grover/node2vec 14 https://github.com/ZW- ZHANG/RandNE/tree/master/Python 15 https://github.com/luckiezhou/DynamicTriad 16We ran static graph embedding methods over a set of sampled snapshots and estimate the total CPU time for all snapshots.

Macro-F1

Academic

0.8

0.6

DynamicPPE DynamicTriad

RandNE

0.4

Node2Vec DeepWalk

1980 1985 1990 1995 2000 2005 2010 2015

EnWiki20

DynamicPPE

0.4

Commute RandNE

0.3

0.2 2006 2008 2010 2012 2014 2016 2018 2020

Macro-F1

Macro-F1

0.7

Patent-small

0.6

0.5

DynamicPPE DynamicTriad

RandNE

0.4

Node2Vec

DeepWalk

1980 1985 1990 1995 2000
Patent 0.6

0.5 0.4

DynamicPPE Commute

0.3

RandNE

0.2

1986 1988 1990 1992 1994 1996 1998

Macro-F1

Macro-F1

Coauthor-small 0.50

0.45

0.40

DynamicPPE

0.35 0.30

DynamicTriad RandNE Node2Vec

0.25

DeepWalk

1985 1990 1995 2000 2005 2010 2015

Coauthor

0.30

DynamicPPE Commute

0.25

RandNE

0.20

0.15

0.10

1990 1995 2000 2005 2010 2015 2020

Figure 3: Macro-F1 scores of node classification as a function of time. The results of the small and large graphs are on the first and second row respectively (dim=512). Our proposed methods achieves the best performance on the last snapshot when all edges arrived and performance curve matches to the static methods as the graph becomes more and more complete.

Macro-F1

more than ours as shown in Table 2. According to the JohnsonLindenstrauss lemma[9, 21], we suspect that the poor result of RandNE is partially caused by the dimension size. As we increase the dimension to 512, we see a great performance improvement of RandNE. Also, our proposed method takes the same advantage, and outperform all others baselines in F1-score. Specifically, the increase of dimension makes hash projection step (Line 12-13 in Algorithm 2) retain more information and yield better embeddings. We attach the visualization of embeddings in Appendix.E.
The first row in the Figure 3 shows the F1-scores in each snapshot when the dimension is 512. We observe that in the earlier snapshots where many edges are not arrived, the performance of DynamicTriad [49] is better. One possible reason is that it models the dynamics of the graph, thus the node feature is more robust to the "missing" of the future links. While other methods, including ours, focus on an online feature updates incurred by the edge changes without explicitly modeling the temporal dynamics of a graph. Meanwhile, the performance curve of our method matches to the static methods, demonstrating the embedding quality of the intermediate snapshots is comparable to the state-of-the-art.
Table 2 shows the CPU time of each method (Dim=512). As we expected, static methods is very expensive as they calculate embeddings for each snapshot individually. Although our method is not blazingly fast compared to RandNE, it has a good trade-off between running time and embedding quality, especially without much hyper-parameter tuning. Most importantly, it can be easily parallelized by distributing the calculation of each node to clusters.
We also conduct experiment on large scale graphs (EnWiki20, Patent, Coauthor). We keep track of the vertices in a subset containing | | = 3, 000 nodes randomly selected from the first snapshot in each dataset, and similarly evaluate the quality of the embeddings of each snapshot on the node classification task. Due to scale of the graph, we compare our method against RandNE [47] and an fast heuristic Algorithm 4. Our method can calculate the embeddings for a subset of useful nodes only, while other methods have to calculate the embeddings of all nodes, which is not necessary under

our scenario detailed in Sec. 5.3. The second row in Figure 3 shows that our proposed method has the best performance.
Table 3: Total CPU time for large graphs (in second)

Commute RandNE-II DynPPE. DynPPE (Per-node)

Enwiki20 6702.1 47992.81 1538215.88 512.73

Patent 639.94 6524.04 139222.01 46.407

Coauthor 1340.74 20771.19 411708.9 137.236

Table 3 shows the total CPU time of each method (Dim=512). Although total CPU time of our proposed method seems to be the greatest, the average CPU time for one node (as shown in row 4) is significantly smaller. This benefits a lot of downstream applications where only a subset of nodes are interesting to people in a large dynamic graph. For example, if we want to monitor the weekly embedding changes of a single node (e.g., the city of Wuhan, China) in English Wikipedia network from year 2020 to 2021, we can have the results in roughly 8.5 minutes. Meanwhile, other baselines have to calculate the embeddings of all nodes, and this expensive calculation may become the bottleneck in a downstream application with real-time constraints. To demonstrate the usefulness of our method, we conduct a case study in the following subsection.

5.3 Change Detection
Thanks to the contributors timely maintaining Wikipedia, we believe that the evolution of the Wikipedia graph reflects how the real world is going on. We assume that when the structure of a node greatly changes, there must be underlying interesting events or anomalies. Since the structural changes can be well-captured by graph embedding, we use our proposed method to investigate whether anomalies happened to the Chinese cities during the COVID-19 pandemic (from Jan. 2020 to Dec. 2020). Changes of major Chinese Cities We target nine Chinese major cities (Shanghai, Guangzhou, Nanjing, Beijing, Tianjin, Wuhan, Shenzhen, Chengdu, Chongqing) and keep track of the embeddings

Table 4: The top cities ranked by the z-score along time. The corresponding news titles are from the news in each time period.  () is node degree,  () is the degree changes from the previous timestamp.

Date 1/22/20 2/2/20 2/13/20
2/24/20 3/6/20 3/17/20 3/28/20 4/8/20 4/19/20 4/30/20 5/11/20

City

 ()

Wuhan

2890

Wuhan

2937

Hohhot

631

Wuhan

3012

Wuhan

3095

Wuhan

3173

Zhangjiakou 517

Wuhan

3314

Luohe

106

Zunhua

52

Shulan

88

 () 54 47 20

Z-score 2.210 1.928 1.370

38

2.063

83

1.723

78

1.690

15

1.217

47

2.118

15

2.640

17

2.449

46

2.449

Top News Title NBC: "New virus prompts U.S. to screen passengers from Wuhan, China" WSJ: "U.S. Sets Evacuation Plan From Coronavirus-Infected Wuhan" Poeple.cn: "26 people in Hohhot were notified of dereliction of duty for prevention and control, and the director of the Health Commission was removed" (Translated from Chinese). USA Today: "Coronavirus 20 times more lethal than the flu? Death toll passes 2,000" Reuters: "Infelctions may drop to zero by end-March in Wuhan: Chinese government expert" NYT: "Politicians Use of 'Wuhan Virus' Starts a Debate Health Experets Wanted to Avoid" "Logo revealed for Freestyle Ski and Snowboard World Championships in Zhangjiakou" CNN:"China lifts 76-day lockdown on Wuhan as city reemerges from conronavirus crisis" Forbes: "The Chinese Billionaire Whose Company Owns Troubled Pork Processor Smithfield Foods" XINHUA: "Export companies resume production in Zunhua, Hebei" CGTN: "NE China's Shulan City to reimpose community lockdown in 'wartime' battle against COVID-19"

in a 10-day time window. From our prior knowledge, we expect that Wuhan should greatly change since it is the first reported place of the COVID-19 outbreak in early Jan. 2020.

Degree Changes

Wuhan

80

Chengdu

60

40

20

0 1/11/20 3/6/20 4/30/206/24/208/18/2010/12/2012/6/20

Figure 4: The changes of major Chinese cities in 2020. Left:Changes in Node Degree. Right: Changes in Cosine distance

Figure.4(a) shows the node degree changes of each city every 10 days. The curve is quite noisy, but we can still see several major peaks from Wuhan around 3/6/20 and Chengdu around 8/18/20. When using embedding changes 17 as the measurement, Figure.4 (b) provides a more clear view of the changed cities. We observe strong signals from the curve of Wuhan, correlating to the initial COVID-19 outbreak and the declaration of pandemic 18. In addition, we observed an peak from the curve of Chengdu around 8/18/20 when U.S. closed the consulate in Chengdu, China, reflecting the U.S.-China diplomatic tension19. Top changed city along time We keep track of the embedding movement of 533 Chinese cities from year 2020 to 2021 in a 10day time window, and filter out the inactive records by setting the threshold of degree changes (e.g. greater than 10). The final results are 51 Chinese cities from the major ones listed above and less famous cities like Zhangjiakou, Hohhot, Shulan, ....
Furthermore, we define the z-score of a target node  as  () based on the embedding changes within a specific period of time
17Again, the embedding movement Dist( ·, ·) is defined as 1 - cos( ·, ·) 18COIVD-19: https://www.who.int/news/item/27-04-2020-who-timeline---covid-19 19US Consulate: https://china.usembassy-china.org.cn/embassy-consulates/chengdu/

from  - 1 to .

 ( |  ) =  ( , -1) -  



=

1 | |





( 




,

 -1


),

 



=

 1 | |

 ( ( - 1)
 

-

)2

In Table 4, we list the highest ranked cities by the z-score from Jan.22 to May 11, 2020. In addition, we also attach the top news titles corresponding to the city within each specific time period. We found that Wuhan generally appears more frequently as the situation of the pandemic kept changing. Meanwhile, we found the appearance of Hohhot and Shulan reflects the time when COVID-19 outbreak happened in those cities. We also discovered cities unrelated to the pandemic. For example, Luohe, on 4/19/20, turns out to be the city where the headquarter of the organization which acquired Smithfield Foods (as mentioned in the news). In addition, Zhangjiakou, on 3/28/20, is the city, which will host World Snowboard competition, released the Logo of that competition.
6 DISCUSSION AND CONCLUSION
In this paper, we propose a new method to learn dynamic node embeddings over large-scale dynamic networks for a subset of interesting nodes. Our proposed method has time complexity that is linear-dependent on the number of edges  but independent on the number of nodes . This makes our method applicable to applications of subset representations on very large-scale dynamic graphs. For the future work, as shown in Trivedi et al. [38], there are two dynamics on dynamic graph data, structure evolution and dynamics of node interaction. It would be interesting to study how one can incorporate dynamic of node interaction into our model. It is also worth to study how different version of local push operation affect the performance of our method.
ACKNOWLEDGMENTS
This work was partially supported by NSF grants IIS-1926781, IIS1927227, IIS-1546113 and OAC-1919752.

REFERENCES
[1] Reid Andersen, Christian Borgs, Jennifer Chayes, John Hopcraft, Vahab S Mirrokni, and Shang-Hua Teng. 2007. Local computation of PageRank contributions. In International Workshop on Algorithms and Models for the Web-Graph. Springer, 150­165.
[2] Reid Andersen, Fan Chung, and Kevin Lang. 2006. Local graph partitioning using pagerank vectors. In 2006 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS'06). IEEE, 475­486.
[3] Reid Andersen, Fan Chung, and Kevin Lang. 2007. Using pagerank to locally partition a graph. Internet Mathematics 4, 1 (2007), 35­64.
[4] Tanya Y Berger-Wolf and Jared Saia. 2006. A framework for analysis of dynamic social networks. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining. 523­528.
[5] Pavel Berkhin. 2006. Bookmark-coloring algorithm for personalized pagerank computing. Internet Mathematics 3, 1 (2006), 41­62.
[6] Haochen Chen, Syed Fahad Sultan, Yingtao Tian, Muhao Chen, and Steven Skiena. 2019. Fast and Accurate Network Embeddings via Very Sparse Random Projection. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management. 399­408.
[7] Colin B Clement, Matthew Bierbaum, Kevin P O'Keeffe, and Alexander A Alemi. 2019. On the Use of ArXiv as a Dataset. arXiv preprint arXiv:1905.00075 (2019).
[8] Cristian Consonni, David Laniado, and Alberto Montresor. 2019. WikiLinkGraphs: A complete, longitudinal and multi-language dataset of the Wikipedia link networks. In Proceedings of the International AAAI Conference on Web and Social Media, Vol. 13. 598­607.
[9] Sanjoy Dasgupta and Anupam Gupta. 1999. An elementary proof of the JohnsonLindenstrauss lemma. International Computer Science Institute, Technical Report 22, 1 (1999), 1­5.
[10] Lun Du, Yun Wang, Guojie Song, Zhicong Lu, and Junshan Wang. 2018. Dynamic Network Embedding: An Extended Approach for Skip-gram based Network Embedding.. In IJCAI, Vol. 2018. 2086­2092.
[11] David F Gleich. 2015. PageRank beyond the Web. Siam Review 57, 3 (2015), 321­363.
[12] Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining. 855­864.
[13] Wentian Guo, Yuchen Li, Mo Sha, and Kian-Lee Tan. 2017. Parallel personalized pagerank on dynamic graphs. Proceedings of the VLDB Endowment 11, 1 (2017), 93­106.
[14] Bronwyn H Hall, Adam B Jaffe, and Manuel Trajtenberg. 2001. The NBER patent citation data file: Lessons, insights and methodological tools. Technical Report. National Bureau of Economic Research.
[15] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation learning on large graphs. In Advances in neural information processing systems. 1024­1034.
[16] William L Hamilton, Rex Ying, and Jure Leskovec. 2017. Representation learning on graphs: Methods and applications. arXiv preprint arXiv:1709.05584 (2017).
[17] Peter Hoff. 2007. Modeling homophily and stochastic equivalence in symmetric relational data. Advances in neural information processing systems 20 (2007), 657­664.
[18] Peter D Hoff, Adrian E Raftery, and Mark S Handcock. 2002. Latent space approaches to social network analysis. Journal of the american Statistical association 97, 460 (2002), 1090­1098.
[19] Sungryong Hong, Bruno C Coutinho, Arjun Dey, Albert-L Barabási, Mark Vogelsberger, Lars Hernquist, and Karl Gebhardt. 2016. Discriminating topology in galaxy distributions using network analysis. Monthly Notices of the Royal Astronomical Society 459, 3 (2016), 2690­2700.
[20] Guoliang Ji, Shizhu He, Liheng Xu, Kang Liu, and Jun Zhao. 2015. Knowledge graph embedding via dynamic mapping matrix. In Proceedings of the 53rd annual
meeting of the association for computational linguistics and the 7th international
joint conference on natural language processing (volume 1: Long papers). 687­696. [21] William B Johnson and Joram Lindenstrauss. 1984. Extensions of Lipschitz
mappings into a Hilbert space. Contemporary mathematics 26, 189-206 (1984), 1. [22] Seyed Mehran Kazemi and Rishab Goel. 2020. Representation Learning for
Dynamic Graphs: A Survey. Journal of Machine Learning Research 21, 70 (2020), 1­73. [23] Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with Graph Convolutional Networks. In International Conference on Learning Representations (ICLR). [24] Srijan Kumar, Xikun Zhang, and Jure Leskovec. 2019. Predicting dynamic embedding trajectory in temporal interaction networks. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 1269­1278. [25] TG Lei, CW Woo, JZ Liu, and F Zhang. 2003. On the Schur complements of diagonally dominant matrices. [26] Peter Lofgren. 2015. Efficient Algorithms for Personalized PageRank. Ph.D. Dissertation. Stanford University.

[27] Peter Lofgren, Siddhartha Banerjee, and Ashish Goel. 2015. Bidirectional PageRank Estimation: From Average-Case to Worst-Case. In Proceedings of the 12th International Workshop on Algorithms and Models for the Web Graph-Volume 9479. 164­176.
[28] Giang Hoang Nguyen, John Boaz Lee, Ryan A Rossi, Nesreen K Ahmed, Eunyee Koh, and Sungchul Kim. 2018. Continuous-time dynamic network embeddings. In Companion Proceedings of the The Web Conference 2018. 969­976.
[29] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. 701­710.
[30] tefan Postvaru, Anton Tsitsulin, Filipe Miguel Gonçalves de Almeida, Yingtao Tian, Silvio Lattanzi, and Bryan Perozzi. 2020. InstantEmbedding: Efficient Local Node Representations. arXiv preprint arXiv:2010.06992 (2020).
[31] Ryan A Rossi, Brian Gallagher, Jennifer Neville, and Keith Henderson. 2013. Modeling dynamic behavior in large evolving graphs. In Proceedings of the sixth ACM international conference on Web search and data mining. 667­676.
[32] Purnamrita Sarkar and Andrew W Moore. 2005. Dynamic social network analysis using latent space models. Acm Sigkdd Explorations Newsletter 7, 2 (2005), 31­40.
[33] Purnamrita Sarkar and Andrew W Moore. 2005. Dynamic social network analysis using latent space models. In Proceedings of the 18th International Conference on Neural Information Processing Systems. 1145­1152.
[34] Purnamrita Sarkar, Sajid M Siddiqi, and Geogrey J Gordon. 2007. A latent space approach to dynamic embedding of co-occurrence data. In Artificial Intelligence and Statistics. 420­427.
[35] Arnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Darrin Eide, Bo-June Hsu, and Kuansan Wang. 2015. An overview of microsoft academic service (mas) and applications. In Proceedings of the 24th international conference on world wide web. 243­246.
[36] Jie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, and Zhong Su. 2008. Arnetminer: extraction and mining of academic social networks. In Proceedings of
the 14th ACM SIGKDD international conference on Knowledge discovery and data
mining. 990­998. [37] Rakshit Trivedi, Mehrdad Farajtabar, Prasenjeet Biswal, and Hongyuan Zha.
2019. DyRep: Learning Representations over Dynamic Graphs. In International Conference on Learning Representations. [38] Rakshit Trivedi, Mehrdad Farajtabar, Prasenjeet Biswal, and Hongyuan Zha. 2019. DyRep: Learning Representations over Dynamic Graphs. In International Conference on Learning Representations. [39] Anton Tsitsulin, Davide Mottin, Panagiotis Karras, and Emmanuel Müller. 2018. Verse: Versatile graph embeddings from similarity measures. In Proceedings of the 2018 World Wide Web Conference. 539­548. [40] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE. Journal of machine learning research 9, 11 (2008). [41] Ulrike Von Luxburg, Agnes Radl, and Matthias Hein. 2014. Hitting and commute times in large random neighborhood graphs. The Journal of Machine Learning Research 15, 1 (2014), 1751­1798. [42] Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola, and Josh Attenberg. 2009. Feature hashing for large scale multitask learning. In Proceedings of the 26th annual international conference on machine learning. 1113­1120. [43] Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec. 2018. Graph convolutional neural networks for web-scale recommender systems. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 974­983. [44] Chengxi Zang and Fei Wang. 2020. Neural dynamics on complex networks. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 892­902. [45] Hongyang Zhang, Peter Lofgren, and Ashish Goel. 2016. Approximate personalized pagerank on dynamic graphs. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 1315­1324. [46] Hongyang Zhang, Peter Lofgren, and Ashish Goel. 2016. Approximate Personalized PageRank on Dynamic Graphs. arXiv preprint arXiv:1603.07796 (2016). [47] Ziwei Zhang, Peng Cui, Haoyang Li, Xiao Wang, and Wenwu Zhu. 2018. Billionscale network embedding with iterative random projection. In 2018 IEEE International Conference on Data Mining (ICDM). IEEE, 787­796. [48] Ziwei Zhang, Peng Cui, Jian Pei, Xiao Wang, and Wenwu Zhu. 2018. TIMERS: Error-Bounded SVD Restart on Dynamic Networks. In Proceedings of the 32nd AAAI Conference on Artificial Intelligence. AAAI. [49] Lekui Zhou, Yang Yang, Xiang Ren, Fei Wu, and Yueting Zhuang. 2018. Dynamic network embedding by modeling triadic closure process. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 32. [50] Linhong Zhu, Dong Guo, Junming Yin, Greg Ver Steeg, and Aram Galstyan. 2016. Scalable temporal latent space inference for link prediction in dynamic social networks. IEEE Transactions on Knowledge and Data Engineering 28, 10 (2016), 2765­2777.

A PROOF OF LEMMAS
To prove Lemma 4, we first introduce the property of uniqueness of PPR  for any .
Proposition 10 (Uniqueness of PPR [3]). For any starting vector 1 , and any constant   (0, 1], there is a unique vector  satisfying (3).

Proof. Recall the PPR equation
 = 1 + (1 - )-1 .
We can rewrite it as ( - (1-)-1) = 1 . Notice the fact that matrix  - (1 - )-1 is strictly diagonally dominant matrix. To see this, for each   V, we have 1-(1-)  |1/ ()| =  > 0. By [25], strictly diagonally dominant matrix is always invertible. 

Proposition 11 (Symmetry property [27]). Given any undirected graph G, for any   (0, 1) and for any node pair (, ), we have

 () () =  () ().

(8)

Proof of Lemma 7. Assume there are  iterations. For each forward push operation  = 1, 2, . . .  , we assume the frontier node

is  , the run time of one push operation is then  ( ). For total

 push operations, the total run time is

  =1

 ( ).

Notice

that

during each push operation, the amount of -1 1 is reduced at

least  ( ), then we always have the following inequality

 ( ) < -1 1 -  1

Apply the above inequality from  = 1, 2, to  , we will have





  ( )



0 

-

 1

=

1-

 1,

(9)

 =1

where  is the final residual vector. The total time is then O (1/). To show the estimation error, we follow the idea of [26]. Notice that, the forward local push algorithm always has the invariant property by Lemma 4, that is



 () =  () +  () (),   V.

(10)

 

By proposition 11, we have

  () =  () +  () (),   V
 

  () =  () +  ()  ()  (),   V
 
   () +  () (),   V =  () +  (),
 

where the first inequality by the fact that  ()   () and the

last equality is due to  1 = 1.



Proposition 12 ([45]). Let G = ( , ) be undirected and let  be a

vertex of  , then

 

 ()  ()

 1.

Proof. By using Proposition 11, we have



 ()  ()

=



 ()  ()



  ()

= 1.

 

 

 



B HEURISTIC METHOD: COMMUTE
We update the embeddings by their pairwise relationship (resistance distance). The commute distance (i.e. resistance distance)  =  +  , where rescaled hitting time  converges to 1/ (). As proved in [41], when the number of nodes in the graph is large enough, we can show that the commute distance tends to 1/ + 1/ .

Algorithm 4 Commute

1: Input: An graph G0 (V0, E0) and embedding  0, dimension .

2: Output:  3: for  (, ,  )  1 (1, 1, 1), . . . ,  ( ,  ,  ) do

4: Add  to G 5: If    -1 then

6:

generate  = N (0, 0.1 ·  ) or U (-0.5, 0.5)/

7: If    -1 then

8:

generate  = N (0, 0.1 ·  ) or U (-0.5, 0.5)/

9:



=

 ()  ()+1

 -1

+



1 ()



10:



=

 ()  ()+1

 -1

+



1 ()



11: Return 

One can treat the Commute method, i.e. Algorithm 4, as the firstorder approximation of RandNE [47]. The embedding generated by RandNE is given as the following

 = 0 + 1 + 22 + . . . +  ,

(11)

where  is the normalized adjacency matrix and  is the identity matrix. At any time , the dynamic embedding of node  of Commute is given by



=



 () () +

1



-1

+



1 (

)



=

1  () +

1 0

+

1 |N ()|





1 ()



 N ()

C DETAILS OF DATA PREPROCESSING
In this section, we describe the preprocessing steps of three datasets. Enwiki20: In Enwiki20 graph, the edge stream is divided into 6 snapshots, containing edges before 2005, 2005-2008, ..., 20172020. The sampled nodes in the first snapshot fall into 5 categories. Patent: In full patent graph, we divide edge stream into 4 snapshots, containing patents citation links before 1985, 1985-1990,..., 1995-1999. In node classification tasks, we sampled 3,000 nodes in the first snapshot, which fall into 6 categories. In patent small graph, we divide into 13 snapshots with a 2-year window. All the nodes in each snapshot fall into 6 categories.Coauthor graph: In full Coauthor graph, we divide edge stream into 7 snapshots (before 1990, 1990-1995, ..., 2015-2019). The sampled nodes in the first snapshot fall into 9 categories. In Coauthor small graph, the edge stream is divided into 9 snapshots (before 1978, 1978-1983,..., 2013-2017). All the nodes in each snapshot have 14 labels in common.

T-SNE: DeepWalk

T-SNE: Node2vec

60

60

40

History

Geology

20

Economics Geography Chemistry

Philosophy

0

Sociology Materials science

Mathematics

20

Biology Computer science Political science

40

Engineering Psychology Environmental science

Business

60

Physics Medicine Art

40 20 0 20 40 60

40 20 0 20 40 60
60 40 20 0

History Geology Economics Geography Chemistry Philosophy Sociology Materials science Mathematics Biology Computer science Political science Engineering Psychology Environmental science Business Physics Medicine Art
20 40

40 20 0 20 40 60
40

T-SNE: DynamicTriad

20 0

History Geology Economics Geography Chemistry Philosophy Sociology Materials science Mathematics Biology Computer science Political science Engineering Psychology Environmental science Business Physics Medicine Art
20 40 60

T-SNE: RandNE 60

40 20 0 20 40 60
80 60 40 20 0

History Geology Economics Geography Chemistry Philosophy Sociology Materials science Mathematics Biology Computer science Political science Engineering Psychology Environmental science Business Physics Medicine Art
20 40 60 80

60

T-SNE: DynamicPPE

40 20 0 20 40 60
60 40 20 0

History Geology Economics Geography Chemistry Philosophy Sociology Materials science Mathematics Biology Computer science Political science Engineering Psychology Environmental science Business Physics Medicine Art
20 40 60

Figure 6: We randomly select 2,000 nodes from Coauthor-

small graph and visualize their embeddings using T-SNE[40] .

60

T-SNE: DeepWalk

40

20

0

20

40

60

80 100

0 1

125 100 75 50 25 0 25 50

T-SNE: DynamicTriad

40

0 1

20

T-SNE: Node2vec

0

50

1

0

50

100

150

100

50

0

50

40

0 1

T-SNE: RandNE

20

0

20

40

40

20

0

20

40

60

0 1

40

20

0

20

40

0
20
40
60 60 40 20 0 20 40 60
T-SNE: DynamicPPE

40 20 0 20 40 60 80

Figure 7: We select all labeled nodes from Academic graph and visualize their embeddings using T-SNE[40]
.

D DETAILS OF PARAMETER SETTINGS
Deepwalk: number-walks=40, walk-length=40, window-size=5 Node2Vec: Same as Deepwalk, p = 0.5, q = 0.5 DynamicTriad: iteration=10, beta-smooth=10, beta-triad=10. Each input snapshot contains the previous existing edges and newly arrived edges. RandNE: q=3, default weight for node classification [1, 12, 14, 15], input is the transition matrix, the output feature is normalized (l-2 norm) row-wise. DynamicPPE:  = 0.15,  = 0.1, projection method=hash. our method is relatively insensitive to the hyper-parameters.

E VISUALIZATIONS OF EMBEDDINGS
We visualize the embeddings of small scale graphs using T-SNE[40] in Fig.5,6,7.

T-SNE: DeepWalk

40

60

20

40

0

20

20

Chemical

40

Computers & Communications Drugs & Medical

0 20

60

Electrical & Electronic Mechanical

40

Others
60 40 20 0

20 40 60

60

60

T-SNE: DynamicTriad
Chemical

40

Computers & Communications Drugs & Medical

60

Electrical & Electronic

20

Mechanical Others

40 20

0

0

20

20

40

40

60

60 40 20 0

20 40 60

T-SNE: DynamicPPE

60

Chemical Computers & Communications

40

Drugs & Medical Electrical & Electronic

20

Mechanical Others

0

20

40

60 60 40 20 0 20

T-SNE: Node2vec
Chemical Computers & Communications Drugs & Medical Electrical & Electronic Mechanical Others
40 20 0 20 40 60 T-SNE: RandNE
Chemical Computers & Communications Drugs & Medical Electrical & Electronic Mechanical Others
60 40 20 0 20 40 60
40

Figure 5: We randomly select 2,000 nodes from patent-small and visualize their embeddings using T-SNE[40]
.

