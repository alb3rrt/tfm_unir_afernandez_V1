Learning a Single Neuron with Bias Using Gradient Descent
Gal Vardi* , Gilad Yehudai* , and Ohad Shamir Weizmann Institute of Science
{gal.vardi,gilad.yehudai,ohad.shamir}@weizmann.ac.il

arXiv:2106.01101v1 [cs.LG] 2 Jun 2021

Abstract
We theoretically study the fundamental problem of learning a single neuron with a bias term (x  ( w, x + b)) in the realizable setting with the ReLU activation, using gradient descent. Perhaps surprisingly, we show that this is a significantly different and more challenging problem than the bias-less case (which was the focus of previous works on single neurons), both in terms of the optimization geometry as well as the ability of gradient methods to succeed in some scenarios. We provide a detailed study of this problem, characterizing the critical points of the objective, demonstrating failure cases, and providing positive convergence guarantees under different sets of assumptions. To prove our results, we develop some tools which may be of independent interest, and improve previous results on learning single neurons.

1 Introduction

Learning a single ReLU neuron with gradient descent is a fundamental primitive in the theory of deep
learning, and has been extensively studied in recent years. Indeed, in order to understand the success of
gradient descent on complicated neural networks, it seems reasonable to expect a satisfying analysis of
convergence on a single neuron. Although many previous works studied the problem of learning a single
neuron with gradient descent, none of them considered this problem with an explicit bias term.
In this work, we study the common setting of learning a single neuron with respect to the squared loss,
using gradient descent. We focus on the realizable setting, where the inputs are drawn from a distribution D on Rd+1, and are labeled by a single target neuron of the form x  ( v, x ), where  : R  R is some non-linear activation function. To capture the bias term, we assume that the distribution D is such that its first d components are drawn from some distribution D~ on Rd, and the last component is a constant 1. Thus, the input x can be decomposed as (x~, 1) with x~  D~, the vector v can be decomposed as (v~, bv), where v~  Rd and bv  R, and the target neuron computes a function of the form x  ( v~, x~ + bv). Similarly, we can define the learned neuron as x  ( w~ , x~ + bw), where w = (w~ , bw). Overall, we can write the objective function we wish to optimize as follows:

1

2

F (w) :=

E
xD

2

(w x) - (v x)

(1)

1

2

=

E
x~D~

2

(w~

x~ + bw) - (v~

x~ + bv)

.

(2)

*equal contribution

1

Throughout the paper we consider the commonly used ReLU activation function: (x) = max{0, x}. Although the problem of learning a single neuron is well studied (e.g. [16, 21, 5, 4, 9, 17, 12, 13]), none
of the previous works considered the problem with an additional bias term. Moreover, previous works on learning a single neuron with gradient methods have certain assumptions on the input distribution D, which do not apply when dealing with a bias term (for example, a certain "spread" in all directions, which does not apply when D is supported on {1} in the last coordinate).
Since neural networks with bias terms are the common practice, it is natural to ask how adding a bias term affects the optimization landscape and the convergence of gradient descent. Although one might conjecture that this is just a small modification to the problem, we in fact show that the effect of adding a bias term is very significant, both in terms of the optimization landscape and in terms of which gradient descent strategies can or cannot work. Our main contributions are as follows:

· We start in Section 3 with some negative results, which demonstrate how adding a bias term makes the problem more difficult. In particular, we show that with a bias term, gradient descent or gradient flow1 can sometimes fail with probability close to half over the initialization, even when the input distribution is uniform over a ball. In contrast, [21] show that without a bias term, for the same input distribution, gradient flow converges to the global minimum with probability 1.

· In Section 4 we give a full characterization of the critical points of the loss function. We show that adding a bias term changes the optimization landscape significantly: In previous works (cf. [21]) it has been shown that under mild assumptions on the input distribution, the only critical points are w = v (i.e., the global minimum) and w = 0. We prove that when we have a bias term, the set of critical points has a positive measure, and that there is a cone of local minima where the loss function is flat.

· In Sections 5 and 6 we show that gradient descent converges to the global minimum at a linear rate, under some assumptions on the input distribution and on the initialization. We give two positive convergence results, where each result is under different assumptions, and thus the results complement each other. We also use different techniques for proving each of the results: The analysis in Section 6 follows from some geometric arguments and extends the technique from [21, 5]. The analysis in Section 5 introduces a novel technique, not used in previous works on learning a single neuron, and has a more algebraic nature. Moreover, that analysis implies that under mild assumptions, gradient descent with random initialization converges to the global minimum with probability 1 - e(d).

· The best known result for learning a single neuron without bias using gradient descent for an input distri-

bution that is not spherically symmetric, establishes convergence to the global minimum with probability

close

to

1 2

over

the

random

initialization

[21,

5].

With

our

novel

proof

technique

presented

in

Section

5

this result can be improved to probability at least 1 - e(d) (see Remark 5.7).

Related work
Although there are no previous works on learning a single neuron with an explicit bias term, there are many works that consider the problem of a single neuron under different settings and assumptions.
Several papers showed that the problem of learning a single neuron can be solved under minimal assumptions using algorithms which are not gradient-based (such as gradient descent or SGD). These algorithms
1I.e., gradient descent with infinitesimal step size.

2

include the Isotron proposed by [8] and the GLMtron proposed by [7]. The GLMtron algorithm is also analyzed in [3]. These algorithms allow learning a single neuron with bias. We note that these are non-standard algorithms, whereas we focus on standard gradient based methods.
In [12] the authors study the empirical risk of the single neuron problem. However, their analysis does not include the ReLU activation, or adding a bias term. A related analysis is also given in [13], where the ReLU activation is not considered.
Several papers showed convergence guarantees for the single neuron problem with ReLU activation under certain distributional assumptions, although none of these assumptions allows for a bias term. Notably, [18, 16, 9, 1] showed convergence guarantees for gradient methods when the inputs have a standard Gaussian distribution, without a bias term. [4] showed that under a certain subspace eigenvalue assumption a single neuron can be learned with SGD, although this assumption does not allow adding a bias term. [21, 5] use an assumption about the input distribution being sufficiently "spread" in all directions, which does not allow for a bias term (since that requires an input distribution supported on {1} in the last coordinate). [21] showed a convergence result under the realizable setting, while [5] considered the agnostic and noisy settings. In [17] convergence guarantees are given for the absolute value activation, and a specific distribution which does not allow a bias term.
Less directly related, [19] studied the problem of implicit regularization in the single neuron setting. In [20, 10] it is shown that approximating a single neuron using random features (or kernel methods) is not tractable in high dimensions. We note that these results explicitly require that the single neuron which is being approximated will have a bias term. Thus, our work complements these works by showing that the problem of learning a single neuron with bias is also learnable using gradient descent (under certain assumptions). Agnostically learning a single neuron with non gradient-based algorithms was studied in [3, 6].

2 Preliminaries

Notations. We use bold-faced letters to denote vectors, e.g., x = (x1, . . . , xd). For u  Rd we denote

by

u

the Euclidean norm. We denote u¯ =

u u

,

namely,

the

unit

vector

in

the

direction

of

u.

For

1  i  j  d we denote ui:j = (ui, . . . , uj)  Rj-i+1. We denote by 1(·) the indicator function, for

example 1(t  5) equals 1 if t  5 and 0 otherwise. We denote by U ([-r, r]) the uniform distribution over

the interval [-r, r] in R, and by N (0, ) the multivariate normal distribution with mean 0 and covariance

matrix . Given two vectors w, v we let (w, v) = arccos

w,v wv

= arccos( w¯ , v¯ )  [0, ]. For a

vector u  Rd+1 we often denote by u~  Rd the first d components of u, and denote by bu  R its last

component.

Gradient methods. In this paper we focus on the following two standard gradient methods for optimizing our objective F (w) from Eq. (2):
· Gradient descent: We initialize at some w0  Rd+1, and set a fixed learning rate  > 0. At each iteration t  0 we have: wt+1 = wt - F (wt).
· Gradient Flow: We initialize at some w(0)  Rd+1, and for every time t  0, we set w(t) to be the solution of the differential equation w = -F (w(t)). This can be thought of as a continuous form of gradient descent, where the learning rate is infinitesimally small.

3

The gradient of the objective in Eq. (1) is:

F (w) = E (w x) - (v x) ·  (w x)x .

(3)

xD

Since  is the ReLU function, it is differentiable everywhere except for 0. Practical implementations of gradient methods define  (0) to be some constant in [0, 1]. Following this convention, the gradient used by these methods still correspond to Eq. (3). We note that the exact value of  (0) has no effect on our results.

3 Negative results
In this section we demonstrate that adding bias to the problem of learning a single neuron with gradient descent can make the problem significantly harder.
First, on an intuitive level, previous results (e.g., [21, 5, 16, 4, 17]) considered assumptions on the input distribution, which require enough "spread" in all directions (for example, a strictly positive density in some neighborhood around the origin). Adding a bias term, even if the first d coordinates of the distribution satisfy a "spread" assumption, will give rise to a direction without "spread", since in this direction the distribution is concentrated on 1, hence the previous results do not apply.
Next, we show two negative results where the input distribution is uniform on a ball around the origin. We note that due to Theorem 6.4 from [21], we know that gradient flow on a single neuron without bias will converge to the global minimum with probability 1 over the random initialization. The only case where it will fail to converge is when w0 is initialized in the exact direction -v, which happens with probability 0 with standard random initializations.
3.1 Initialization in a flat region
If we initialize the bias term in the same manner as the other coordinates, then we can show that gradient descent will fail with probability close to half, even if the input distribution is uniform over a (certain) origin-centered ball:
Theorem 3.1. Suppose we initialize each coordinate of w0 (including the bias) according to U ([-1, 1]). Let > 0 and let D~be the uniform distribution supported on a ball around the origin in Rd of radius . Then, w.p > 1/2 - d, gradient descent on the objective in Eq. (2) satisfies wt = w0 for all t (namely, it gets stuck at its initial point w0).
Note that by Theorem 6.4 in [21], if there is no bias term in the objective, then gradient descent will converge to the global minimum w.p 1 using this random initialization scheme and this input distribution. The intuition for the proof is that with constant probability over the initialization, bw is small enough so that (w~ x~ + bw) = 0 almost surely. If this happens, then the gradient will be 0 and gradient descent will never move. The full proof can be found in Appendix B.1.
3.2 Targets with negative bias
Theorem 3.1 shows a difference between learning with and without the bias term. A main caveat of this example is the requirement that the bias is initialized in the same manner as the other parameters. Standard deep learning libraries (e.g. Pytorch [14]) often initialize the bias term to zero by default, while using random initialization schemes for the other parameters.

4

Alas, we now show that even if we initialize the bias term to be exactly zero, and the input distribution is uniform over an arbitrary origin-centered ball, we might fail to converge to the global minimum for certain target neurons:

Theorem 3.2. Let D~ be the uniform distribution on B = {x~  Rd : x~  r} for some r > 0. Let

v  Rd+1 such that v~ = (1, 0, . . . , 0)

and bv = -

r

-

r 2d2

. Let w0  Rd+1 such that bw0 = 0 and w~ 0

is

drawn

from

the

uniform

distribution

on

a

sphere

of

radius



>

0.

Then,

with

probability

at

least

1 2

- od(1)

over the choice of w0, gradient flow does not converge to the global minimum.

We prove the theorem in Appendix B.2. The intuition behind the proof is the following: The target

neuron has a large negative bias, so that only a small (but positive) measure of input points are labelled

as non-zero.

By randomly initializing w~ ,

with

probability close to

1 2

there are

no inputs that both v and

w label positively. Since the gradient is affected only by inputs that w labels positively, then during the

optimization process the gradient will be independent of the direction of v, and w will not converge to the

global minimum.

Remark 3.3. Theorem 3.2 shows that gradient flow is not guaranteed to converge to a global minimum

when bv is negative, instead it converges to a local minimum with a loss of F (0). However, the loss F (0) is

determined by the input distribution. Take v =

1, 0, . . . , 0, -

r

-

r 2d2

considered in the theorem. On

one hand, for a uniform distribution on a ball of radius r as in the theorem we have:

1

F (0)

=

2

·

E
x

(v

2
x)

1

=

2

·

E
x

1(v

x  0)

v

2
x

1

=

2

·

E
x

1

r x1  r - 2d2

r x1 - r - 2d2

2

1 r2

1



2

·

4d4

·

Pr
x

x1  r

1 - 2d2

 r2e-(d) .

Thus, for any reasonable r, a local minimum with loss F (0) is almost as good as the global minimum. On

the other hand, take a distribution D~ with a support bounded in a ball of radius r, such that half of its mass

is uniformly distributed in A :=

x~



Rd

:

x1

>

r

-

r 4d2

, and the other half is uniformly distributed in

B \ A. In this case, it is not hard to see that the same proof as in Theorem 3.2 works, and gradient flow will

converge to a local minimum with loss F (0) = 

r d2

, which is arbitrarily large if r is large enough.

Although in the example given in Theorem 3.2 the objective at w = 0 is almost as good as the objective

at

w

=

v,

we

emphasize

that

w.p

almost

1 2

gradient

flow

cannot

reach

the

global

minimum

even

asymp-

totically. On the other hand, in the bias-less case by Theorem 6.4 in [21] gradient flow on the same input

distribution will reach the global minimum w.p 1. Also note that the scale of the initialization of w0 has no

effect on the result.

4 Characterization of the critical points
In the previous section we have shown two examples where gradient methods on the problem of a single neuron with bias will either get stuck in a flat region, or converge to a local minimum. In this section we delve deeper into the examples presented in the previous section, and give a full characterization of the critical points of the objective. We will use the following assumption on the input distribution:

5

Assumption 4.1. The distribution D~ on Rd has a density function p(x~), and there are , c > 0, such that D~ is supported on {x~ : x~  c}, and for every x~ in the support we have p(x~)  .

The assumption essentially states that the distribution over the first d coordinates (without the bias term) has enough "spread" in all directions, and covers standard distributions such as uniform over a ball of radius c. Other similar assumptions are made in previous works (e.g. [21, 5]). We note that in [21] it is shown that without any assumption on the distribution, it is impossible to ensure convergence, hence we must have some kind of assumption for this problem to be learnable with gradient methods. Under this assumption we can characterize the critical points of the objective.
Theorem 4.2. Consider the objective in Eq. (2) with v = 0, and assume that the distribution D~ on the first d coordinates satisfies Assumption 4.1. Then w = 0 is a critical point of F (i.e., is a root of Eq. (3)) if and only if it satisfies one of the following:

· w = v, in which case w is a global minimum.

· w = (w~ , bw) where w~ = 0 and bw < 0.

·

w~ = 0 and -

bw w~

 c.

In the latter two cases, F (w) = F (0). Hence, if 0 is not a global minimum, then w is not a global minimum.

We

note

that

F (0)

=

1 2

Ex[(v

x)2], so 0 is a global minimum only if the target neuron returns 0 with

probability 1.

Remark 4.3 (The case w = 0). We intentionally avoided characterizing the point w = 0, since the objective is not differentiable there (this is the only point of non-differentiability), and the gradient there is determined by the value of the ReLU activation at 0. For  (0) = 0 the gradient at w = 0 is zero, and this is a non-differentiable saddle point. For  (0) = 1 (or any other positive value), the gradient at w = 0 is non-zero, and it will point at a direction which depends on the distribution. We note that in [16] the authors define  (0) = 1, and use a symmetric distribution, in which case the gradient at w = 0 points exactly at the direction of the target v. This is a crucial part of their convergence analysis.

We emphasize that with a bias term, there is a non-zero measure manifold of critical points (corresponding to the third bullet in the theorem). On the other hand, without a bias term the only critical point besides the global minimum (under mild assumptions on the input distribution) is at the origin w = 0 (cf. [21]). The full proof is in Appendix C.
The assumption on the support of D~ is made for simplicity. It can be relaxed to having a distribution with exponentially bounded tail, e.g. standard Gaussian. In this case, some of the critical points will instead have a non-zero gradient which is exponentially small. We emphasize that when running optimization algorithms on finite-precision machines, which are used in practice, these "almost" critical points behave essentially like critical points since the gradient is extremely small.
Revisiting the negative examples from Section 3, the first example (Theorem 3.1) shows that if we do not initialize the bias of w to zero, then there is a positive probability to initialize at a critical point which is not the global minimum. The second example (Theorem 3.2) shows that even if we initialize the bias of w to be zero, there is still a positive probability to converge to a critical point which is not the global minimum. Hence, in order to guarantee convergence we need to have more assumptions on either the input distribution, the target v or the initialization. In the next section, we show that adding such assumptions are indeed sufficient to get positive convergence guarantees.

6

5 Convergence for initialization with loss slightly better than trivial
In this section, we show that under some assumptions on the input distribution, if gradient descent is initialized such that F (w0) < F (0) then it is guaranteed to converge to the global minimum. In Subsection 5.2, we study under what conditions this is likely to occur with standard random initialization.

5.1 Convergence if F (w0) < F (0)
To state our results, we need the following assumption:

Assumption 5.1. 1. The distribution D is supported on {x  Rd+1 : x  c} for some c  1. 2. The distribution D~ over the first d coordinates is bounded in all directions: there is c > 0 such that for
every u~ with u~ = 1 and every a  R and b  0, we have Prx~D~ u~ x~  [a, a + b]  b · c . 3. We assume w.l.o.g. that v = 1 and c  1.

Assumption (3) helps simplifying some expressions in our convergence result, and is not necessary. Assumption (2) requires that the distribution is not too concentrated in a short interval. For example, if D~ is
spherically symmetric then the marginal density of the first (or any other) coordinate is bounded by c . Note that we do not assume that D~ is spherically symmetric.

Theorem 5.2. Under Assumption 5.1 we have the following. Let  > 0 and let w0  Rd+1 such that

F (w0)  F (0) - . Let 

step

size





 c4

.

Then,

for

=

3·122(

3 w0 +2)3c8c

every t we have

2

.

Assume

that

gradient

descent

runs

starting

from

w0

with

wt - v 2  w0 - v 2 (1 - )t .

The formal proof appears in Appendix D, but we provide the main ideas below. First, note that

wt+1 - v 2 = wt - F (wt) - v 2 = wt - v 2 - 2 F (wt), wt - v + 2 F (wt) 2 .

Hence, in order to show that wt+1 - v 2  wt - v 2 (1 - ) we need to obtain an upper bound for

F (wt) and a lower bound for F (wt), wt - v . Achieving the lower bound for F (wt), wt - v is

the challenging part, and we show that in order to establish such a bound it suffices to obtain a lower bound

for Prx wt x  0, v x  0 . We prove that if F (wt)  F (0) -  then Prx wt x  0, v x  0 

 c2 wt

. Hence, if F (wt) remains at most F (0) -  for every t, then a lower bound for

F (wt), wt - v

can be achieved, which completes the proof. However, it is not obvious that F (wt) remains at most F (0)-

throughout the training process. When running gradient descent on a smooth loss function we can choose

a sufficiently small step size such that the loss decreases in each step, but here the function F (w) is highly

non-smooth around w = 0. That is, the Lipschitz constant of F (w) is unbounded. We show that if

F (wt)  F (0) -  then wt is sufficiently far from 0, and hence the smoothness of F around wt can be

bounded, which allows us to choose a small step size that ensures that F (wt+1)  F (wt)  F (0) - .

Hence, it follows that F (wt) remains at most F (0) -  for every t.

As an aside, recall that in Section 4 we showed that other than w = v all critical points of F (w) are in

a flat region where F (w) = F (0). Hence, the fact that F (wt) remains at most F (0) -  for every t implies

7

that wt does not reach the region of bad critical points, which explains the asymptotic convergence to the global minimum.
We also note that although we assume that the distribution has a bounded support, this assumption is mainly made for simplicity, and can be relaxed to have sub-Gaussian distributions with bounded moments. These distributions include, e.g. Gaussian distributions.

5.2 Convergence for Random Initialization

In Theorem 5.2 we showed that if F (w0) < F (0) then gradient descent converges to the global minimum. We now show that under mild assumptions on the input distribution, a random initialization of w0 near zero satisfies this requirement. We will need the following assumption, also used in [21, 5]:
Assumption 5.3. There are ,  > 0 s.t the distribution D~ satisfies the following: For any vector w~ = v~, let D~w~ ,v~ denote the marginal distribution of D~ on the subspace spanned by w~ , v~ (as a distribution over R2). Then any such distribution has a density function pw~ ,v~(x^) over R2 such that infx^: x^  pw~ ,v~(x^)  .

The main technical tool for proving convergence under random initialization is the following:

Theorem 5.4. Assume that the input distribution D is supported on {x  Rd+1 : x  c} for some

c  1, and Assumption 5.3 holds. Let v  Rd+1 such that

v

=

1

and

-

bv v~



·

sin(
4

 8

)

.

Let

M

=

4

 sin3(
256c

 8

)

.

Let w



Rd+1 such that bw

=

0, (w~ , v~)



3 4

and

w

<

2M c2

.

Then, F (w)



F (0) +

w

2

·

c2 2

-

w

· M < F (0).

We prove the theorem in Appendix F. The main idea is that since

1

2

F (w) = E
x

2

(w x) - (v x)

1

2

=

F

(0)

+

2

E
x

(w x)

- E (w x)(v x)
x

 F (0) +

w 2 · c2 - 2

w

· E (w¯ x)(v x)
x

,

then it suffices to obtain a lower bound for Ex (w¯ x)(v x) . In the proof we show that such a bound

can be achieved if the conditions of the theorem hold.

Suppose that w0 is such that w~ 0 is drawn from a spherically symmetric distribution and bw0 = 0. By

standard concentration

of measure

arguments,

it holds w.p.

at

least 1 - e(d)

that

(w~ 0, v~)



3 4

(where

the notation (d) hides only numerical constants, namely, it does not depend on other parameters of the

problem).

Therefore, if w~ 0 is drawn from the uniform distribution on a sphere of radius 

<

2M c2

,

then

the theorem implies that w.h.p. we have F (w0) < F (0). For such initialization Theorem 5.2 implies

that gradient descent converges to the global minimum.

For example, for 

=

M c2

we have w.h.p.

that

F (w0)



F (0) +

2 c2 2

- M

=

F (0) -

M2 2c2

,

and

thus

Theorem

5.2

applies

with



=

M2 2c2

.

Thus,

we

have

the following corollary:

Corollary 5.5.

Under Assumption 5.1

and Assumption 5.3 we have the following.

Let M

=

4 sin3(
256c

 8

)

,

let



=

M c2

,

let 

=

M2 2c2

,

and let



=

3 3·122(+2)3c8c

2

.

Suppose that -

bv v~





·

sin(
4

 8

)

,

and

w0

is

such

that

bw0 = 0 and w~ 0 is drawn from the uniform distribution on a sphere of radius . Consider gradient descent

8

with

step

size





 c4

.

Then,

with

probability

at

least

1

-

e(d)

over

the

choice

of

w0

we

have

for

every

t:

wt - v 2  w0 - v 2 (1 - )t .

We

note

that

a

similar

result

holds

also

if

w~ 0

is

drawn

from

a

normal

distribution

N

(0,

2 d

I ).

Remark

5.6 (The

assumption

on

bv).

The

assumption

-

bv v~





·

sin(

 8

)

4

implies

that

the

bias

term

bv

may

be either positive or negative, but in case it is negative then it cannot be too large. This assumption is indeed

crucial for the proof, but for "well-behaved" distributions, if this assumption is not satisfied (for a large

enough ), then the loss at F (0) is already good enough. For example, for a standard Gaussian distribution

and for every > 0, we can choose  large enough such that for any bias term (positive or negative) we

either: (1) converge to the global minimum with a loss of zero, or; (2) converge to a local minimum with a

loss of F (0), which is smaller then . Moreover, we can show that by choosing  appropriately, and using

the

example

in

Theorem

3.2,

if

-

bv v~

 2 then gradient flow will converge to a non-global minimum with

loss of F (0). This means that our bound on  is tight up to a constant factor. For a further discussion on

the assumption on bv, and how to choose  see Appendix G.

Previous papers have shown separation between random features (or kernel) methods and neural networks in terms of their approximation power (see [20, 10], and the discussion in [11]). These works show that under a standard Gaussian distribution, random features cannot even approximate a single ReLU neuron, unless the number of features is exponential in the input dimension. That analysis crucially relies on the single neuron having a non-zero bias term. In this work we complete the picture by showing that gradient descent can indeed find a near-optimal neuron with non-zero bias. Thus, we see there is indeed essentially a separation between what can be learned using random features and using gradient descent over neural networks.

Remark 5.7 (Learning a neuron without bias). [21] studied the problem of learning a single ReLU neuron

without bias using gradient descent on a single neuron without bias. For input distributions that are not

spherically symmetric they showed that gradient descent with random initialization near zero converges to

the global minimum w.p.

at least

1 2

-

od(1).

Their result is also under Assumption 5.3.

An immediate

corollary from the discussion above is that if we learn a single neuron without bias using gradient descent

with random initialization on a single neuron with bias, then the algorithm converges to the global minimum

w.p. at least 1-e(d). Moreover, our proof technique can be easily adapted to the setting of learning a single

neuron without bias using gradient descent on a single neuron without bias, namely, the setting studied in

[21]. It can be shown that in this setting gradient descent converges w.h.p to the global minimum. Thus, our

technique

allows

us

to

improve

the

result

of

[21]

from

probability

1 2

-

od(1)

to

probability

1

-

e(d).

6 Convergence for spread and symmetric distributions
In this section we show that under a certain set of assumptions, different from the assumptions in Section 5, it is possible to show linear convergence of gradient descent to the global minimum. The assumptions we make for this theorem are as follows:
Assumption 6.1.
1. The target vector v satisfies that bv  0 and v~ = 1. 2. The distribution D~ over the first d coordinates is spherically symmetric.

9

3.

Assumption

5.3 holds, and denoting by



:=

Ex~D~ [|x~1x~2
Ex~D~ [x~21]

|]

,

then





 2.5 2 · max

1,

1 

from Assumption 5.3.

where  is

4. Denote by c := Ex~D~ x~ 4 , then c < .

Under these assumptions, we prove the following theorem:

Theorem 6.2. Assume we initialize w0 such that w0 - v 2 < 1, bw0  0 and that Assumption 6.1

holds. Then, there is a universal constant C, such that using gradient descent on F (w) with step size



<

C

·

 c2

min{1,

}

yields

that

for

every

t

we

have

wt - v

2  (1 - )t

w0 - v

2

,

for



=

C

·

 c2

.

This result has several advantages and disadvantages compared to those of the previous section. The
main disadvantage is that the assumptions are generally more stringent: We focus only on positive target biases (bv  0) and spherically symmetric distributions D~. Also we require a certain technical assumption on the the distribution, as specified by  , which are satisfied for standard spherically symmetric distributions, but is a bit non-trivial2. Finally, the assumption on the initialization ( w0 - v 2 < 1 and bw0  0) is much more restrictive (although see Remark 6.3 below). In contrast, the initialization assumption in the previous
section holds with probability close to 1 with random initialization. On the positive side, the convergence rate does not depend on the initialization, i.e., here by initializing with any w0 such that w0 - v 2 < 1 and bw0  0, we get a convergence rate that only depends on the input distribution. On the other hand, in Theorem 5.2, the convergence rate depends on the parameter  which depends on the initialization. Also,
the distribution is not necessarily bounded ­ we only require its fourth moment to be bounded.

Remark 6.3 (Random initialization). For bv = 0 the initialization assumption ( w0 - v 2 < 1) is satisfied with probability close to 1/2 with standard initializations, see Lemma 5.1 from [21]). For bv > 0, a similar argument applies if bw is initialized close enough to bv.

The proof of the theorem is quite different from the proofs in Section 5, and is more geometrical in

nature, extending previously used techniques from [21, 5]. It contains two major parts: The first part is an

extension of the methods from [21] to the case of adding a bias term. Specifically, we show a lower bound

on F (w), w - v , which depends on both the angle between w~ and v~, and the bias terms bw and bv (see

Theorem A.2). This result implies that for suitable values of w, gradient descent will decrease the distance

from v. The second part of the proof is showing that throughout the optimization process, w will stay in an

area

where

we

can

apply

the

result

above.

Specifically,

the

intricate

part

is

showing

that

the

term

-

bw w~

does

not get too large. Note that due to Theorem 4.2, we know that keeping this term small means that w stays

away from the cone of bad critical points which are not the global minimum. The full proof can be found in

Appendix E.

7 Discussion

In this work we studied the problem of learning a single neuron with a bias term using gradient descent. We showed several negative results, indicating that adding a bias term makes the problem more difficult than without a bias term. Next, we gave a characterization of the critical points of the problem under some assumptions on the input distribution, showing that there is a manifold of critical points which are not the global minimum. We proved two convergence results using different techniques and under different

2For example, for standard Gaussian distribution, we have that 

=

2 



0.63, hence we can take 

=

4.5, and 

=

O(1).

Since the distribution D~ is symmetric, we present the assumption w.l.o.g with respect to the first 2 coordinates.

10

assumptions. Finally, we showed that under mild assumptions on the input distribution, reaching the global minimum can be achieved by standard random initialization.
We emphasize that previous works studying the problem of a single neuron either considered nonstandard algorithms (e.g. Isotron), or required assumptions on the input distribution which do not allow a bias term. Hence, this is the first work we are aware of which gives positive and negative results on the problem of learning a single neuron with a bias term using gradient methods.
In this work we focused on the gradient descent algorithm. We believe that our results can also be extended to the commonly used SGD algorithm, using similar techniques to [21, 15], and leave it for future work. Another interesting future direction is analyzing other previously studied settings, but with the addition of a bias term. These settings can include convolutional networks, two layers neural networks, and agnostic learning of a single neuron.
References
[1] A. Brutzkus and A. Globerson. Globally optimal gradient descent for a convnet with gaussian inputs. In Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.
[2] S. Bubeck. Convex optimization: Algorithms and complexity. arXiv preprint arXiv:1405.4980, 2014.
[3] I. Diakonikolas, S. Goel, S. Karmalkar, A. R. Klivans, and M. Soltanolkotabi. Approximation schemes for relu regression. In Conference on Learning Theory, pages 1452­1485. PMLR, 2020.
[4] S. S. Du, J. D. Lee, and Y. Tian. When is a convolutional filter easy to learn? arXiv preprint arXiv:1709.06129, 2017.
[5] S. Frei, Y. Cao, and Q. Gu. Agnostic learning of a single neuron with gradient descent. arXiv preprint arXiv:2005.14426, 2020.
[6] S. Goel, S. Karmalkar, and A. Klivans. Time/accuracy tradeoffs for learning a relu with respect to gaussian marginals. arXiv preprint arXiv:1911.01462, 2019.
[7] S. M. Kakade, V. Kanade, O. Shamir, and A. Kalai. Efficient learning of generalized linear and single index models with isotonic regression. In Advances in Neural Information Processing Systems, pages 927­935, 2011.
[8] A. T. Kalai and R. Sastry. The isotron algorithm: High-dimensional isotonic regression. In COLT. Citeseer, 2009.
[9] S. M. M. Kalan, M. Soltanolkotabi, and A. S. Avestimehr. Fitting relus via sgd and quantized sgd. In 2019 IEEE International Symposium on Information Theory (ISIT), pages 2469­2473. IEEE, 2019.
[10] P. Kamath, O. Montasser, and N. Srebro. Approximate is good enough: Probabilistic variants of dimensional and margin complexity. In Conference on Learning Theory, pages 2236­2262. PMLR, 2020.
[11] E. Malach, P. Kamath, E. Abbe, and N. Srebro. Quantifying the benefit of using differentiable learning over tangent kernels. arXiv preprint arXiv:2103.01210, 2021.
11

[12] S. Mei, Y. Bai, and A. Montanari. The landscape of empirical risk for non-convex losses. arXiv preprint arXiv:1607.06534, 2016.
[13] S. Oymak and M. Soltanolkotabi. Overparameterized nonlinear learning: Gradient descent takes the shortest path? arXiv preprint arXiv:1812.10004, 2018.
[14] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. arXiv preprint arXiv:1912.01703, 2019.
[15] O. Shamir. A stochastic pca and svd algorithm with an exponential convergence rate. In International Conference on Machine Learning, pages 144­152, 2015.
[16] M. Soltanolkotabi. Learning relus via gradient descent. In Advances in Neural Information Processing Systems, pages 2007­2017, 2017.
[17] Y. S. Tan and R. Vershynin. Online stochastic gradient descent with arbitrary initialization solves non-smooth, non-convex phase retrieval. arXiv preprint arXiv:1910.12837, 2019.
[18] Y. Tian. An analytical formula of population gradient for two-layered relu network and its applications in convergence and critical point analysis. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 3404­3413. JMLR. org, 2017.
[19] G. Vardi and O. Shamir. Implicit regularization in relu networks with the square loss. arXiv preprint arXiv:2012.05156, 2020.
[20] G. Yehudai and O. Shamir. On the power and limitations of random features for understanding neural networks. In Advances in Neural Information Processing Systems, 2019.
[21] G. Yehudai and O. Shamir. Learning a single neuron with gradient methods. arXiv preprint arXiv:2001.05205, 2020.

Appendices

A Auxiliary Results

In this appendix we extend several key results from [21] for the case of adding a bias term. Specifically, we extend Theorem 4.2 from [21] which shows that under mild assumptions on the distribution, the gradient of the loss points in a good direction which depends on the angle between the learned vector w and the target v. We also bound the volume of a certain set in R2, which can be seen as an extension of Lemma B.1 from [21].

Lemma A.1. Let P = {y  R2 : w y > b, v y > b, y  } for b  R and w, v  R2 with

w,

v

= 1 and (w, v)   -  for   [0, ]. If b <  sin

 2

then

Vol(P )



(

sin(

 2

)-b)2

4

sin(

 2

)

.

12

Proof. The volume of P is smallest when the angle is exactly  - , thus we can lower bound the volume by assuming that (w, v) =  - . Next, we can rotate to coordinates to consider without loss of generality the volume of the set

P = (y1, y2)  R2 : ((y1, y2 - b ), e2)  /2, (y1, y2)   ,

where b

=

b sin(/2)

and

e2

=

(0, 1).

Let P

= {(x, y)  R2 : x2 + (y - b )2  ( - b )2} be the

disc of radius  - b around the point (0, b ). It is enough to bound the volume of P  P . We define the

rectangular sets:

( - b ) 



( - b )





P1 =

sin , ( - b ) sin

× b+

cos , b + ( - b ) cos

2

4

4

2

4

4

 ( - b ) 

( - b )





P2 = -( - b ) sin 4 , -

2

sin 4

× b+

2

cos , b + ( - b ) cos

4

4

See Figure 1 for an illustration. We have that P1, P2  P  P . We will show it for P1, the same

argument also works for P2. First, P1  P is immediate by the definition of the two sets. For P , the

straight line in the boundary of P

is defined by y2 = b

+

y1

·

cos( sin(

 2  2

) )

.

It

can

be

seen

that

each

vertex

of

the

rectangle P1, is above this line. Moreover, the norm of each vertex of P1 is at most . Hence all the vertices

are inside P , which means that P1  P . In total we get:

Vol(P )  Vol(P  P )  Vol(P1  P2)

( - b )2





=

sin cos

2

4

4

=

 sin

 2

-b 2

4 sin

 2

Theorem A.2. Let w, v  Rd+1 , denote by w~ , v~ their first d coordinates and by bw, bv their last coordinate. Assume that (w~ , v~)   -  for some   [0, ), and that the distribution D is such that its first d

coordinates satisfy Assumption 4.1 (1) from [21], and that its last coordinate is a constant 1. Denote

b

= max{-bw/

w~

, -bv/

v~

,

0}

·

1
sin(

 2

)

,

and

assume

that

b

< , then:

F (w), w - v

( - b )4 sin



84

 4

3 · min

1 1, 2

w-v 2

Proof. Let x~ be the first d coordinates of x. We have that:

F (w), w - v = ExD  (w x)((w x) - (v x))(w x - v x)

 ExD 1(w x > 0, v x > 0)(w x - v x)2

= w - v 2 · ExD 1(w~ x~ > -bw, v~ x~ > -bv)((w - v) x)2

 w-v 2·

inf

ExD 1(w~ x~ > -bw, v~ x~ > -bv)(u x)2

uspan{w,v}, u =1

13

Figure 1: An illustration of the set P (in red), the circle P (in blue) and the two rectangles P1, P2 (in black), for the case of  = /2,  = 1 and b = 0.3. For b = 0, P would be a pie slice, and the blue circle P will coincide with the red circle.

Let b = max{-bw/ w~ , -bv/ v~ , 0}, then we can bound the above equation by:

w-v 2·

inf

ExD 1(w~ x~ > b, v~ x~ > b)(u x)2

uspan{w,v}, u =1



w-v

2·

inf

uspan{w,v},

u

=1 Ex~D~

1(w~

x~ > b, v~

x~ > b,

x~

 )(u~

x~ + bu)2

(4)

Here bu is the bias term of u, u~ are the first d coordinates of u and D~ is the marginal distribution of x on its first d coordinates. Note that since the last coordinate represents the bias term, then the distribution on the last coordinate of x is a constant 1. The condition that u = 1 (equivalently u 2 = 1) translates to u~ 2 + b2u = 1.
Our goal is to bound the term inside the infimum. Note that the expression inside the distribution depends
just on inner products of x~ with w~ or v~, hence we can consider the marginal distribution Dw~ ,v~ of x~ on the 2-dimensional subspace spanned by w~ and v~ (with density function pw~ ,v~). Let w^ and v^ be the projections
of w~ and v~ on that subspace. Let P = {y  R2 : w^ y > b, v^ y > b, y  }, then we can bound
Eq. (4) with:

14

w-v

2·

inf

uR2,buR: u

E 2+b2u=1 yDw~ ,v~

1(y  P ) · (u

y + bu)2

= w-v 2·

inf

1(y  P ) · (u y + bu)2pw~ ,v~ (y)dy

uR2,buR: u 2+b2u=1 yR2

 w-v 2·

inf

(u y + bu)2dy

uR2,buR: u 2+b2u=1 yP

Combining with Proposition A.3 finishes the proof

Proposition A.3. Let P = {y  R2 : w^ y > b, v^ y > b, y  } for b  R and w^ , v^  R2 with (w^ , v^)   -  for   [0, ]. Then

inf
uR2,buR: u 2+b2u=1

(u
yP

y

+

bu)2dy



(

-

b

)4 sin 84

3
4 · min

1 1, 2

for b

=

b
sin(

 2

)

.

Proof. As in the proof of Lemma A.1, we consider the rectangular sets:

( - b ) 



( - b )





P1 =

sin , ( - b ) sin

× b+

cos , b + ( - b ) cos

2

4

4

2

4

4

 ( - b ) 

( - b )





P2 = -( - b ) sin 4 , -

2

sin 4

× b+

2

cos , b + ( - b ) cos

4

4

with b

=

b sin(/2)

.

Since we have P1  P2

 P , and the function inside the integral is positive, we can lower

bound the target integral by integrating only over P1  P2. Now we have:

inf

(u y + bu)2dy

uR2,buR: u 2+b2u=1 yP



inf

(u1y1 + u2y2 + bu)2dy

u1,u2,buR:u21+u22+b2u=1 yP1P2

=

inf

(u1y1)2dy +

(u2y2 + bu)2dy +

2u1y1(u2y2 + bu)dy

u1,u2,buR:u21+u22+b2u=1 yP1P2

yP1P2

yP1P2

=

inf

(u1y1)2dy +

(u2y2 + bu)2dy

u1,u2,buR:u21+u22+b2u=1 yP1P2

yP1P2

where in the last equality we used that P1  P2 are symmetric around the y2 axis, i.e. (y1, y2)  P1  P2 iff

(-y1, y2)



P1



P2.

By

the

condition

that

u21

+

u22

+

b2u

=

1

we

know

that

either

u21



1 2

or

u22

+

b2u



1 2

.

Using that both integrals above are positive, we can lower bound:

inf

(u1y1)2dy +

(u2y2 + bu)2dy

u1,u2,buR:u21+u22+b2u=1 yP1P2

yP1P2

 min

1 2

yP1P2

y12dy,

inf
u2,u3R:u22+u23=

1 2

(u2y2 + u3)2dy
yP1P2

.

15

We will now lower bound both terms in the above equation. For the first term, note that for every

y



P1



P2

we

have

that

|y1|



(-b 2

)

sin

 4

. Hence we have:

1 2

y12dy 
yP1P2

1

( - b )2

2



sin

dy

2 yP1P2

4

4

( - b )2

 2 ( - b )2





=

sin

·

sin cos

8

4

2

4

4

( - b )4

3

  sin

(5)

16 2

4

where in the last inequality we used that   [0, ], hence /4  [0, /4]. For the second term we have:

inf

(u2y2 + u3)2dy

u2,u3

R:u22

+u23=

1 2

yP1P2

= inf
u - 1 , 1
22

uy2 +
yP1P2

2
1 - u2 dy 2

2



=( - b ) sin

inf

4 u - 1 , 1

uy2 +
y2C

1 - u2 2

dy2 .

(6)

22

The last equality is given by changing the order of integration into integral over y2 and then over y1, denoting

the interval C =

b

+

(-b 2

)

cos

 4

, b + ( - b ) cos

 4

, and noting that the term inside the integral

does not depend on y1.

Fix some u 

- 1 , 1
22

.

If u

= 0, then we can bound Eq. (6) by

(-b 4

)2

sin

 4

cos

 4

.

Assume

u = 0, we split into cases and bound the term inside the integral:

Case I:

1 2

-u2

u

b

+

3 4

·

(

-

b

) cos

 4

.

In this case, solving the inequality for u we have

|u| 

2+2(b

1

+

3 4

(-b

)

cos(

 4

))2

.

Hence,

we

can

also

bound:

1 - u2  2

1

1

-

2

2+2

b

+

3 4

(

-

b

)

cos

 4

2=

b

+

3 4

(

-

b

)

cos

 4

2

2+2

b

+

3 4

(

-

b

)

cos

 4

2

16

In particular, for every y2 

b

+

(-b 2

)

cos

 4

,b

+

5(-b 8

)

cos

 4

we get that:

uy2 +

1 - u2 2



b

+

3 4

(

-

b

)

cos

 4

2+2

b

+

3 4

(

-

b

)

cos

2
 4

2-



( - b ) cos

 4

8

2+2

b + ( - b ) cos

 4

2

b

+

5 8

(

-

b

)

cos

 4

2

2+2

b

+

3 4

(

-

b

)

cos

 4

2

Case II:

1 2

-u2

u

<

b

+

3 4

· ( - b ) cos

 4

.

Using the same reasoning as above, we get for every

y2 

b

+

7(-b 8

)

cos

 4

, b + ( - b ) cos

 4

that:

uy2 +

1 - u2 

( - b ) cos

 4

2

8

2+2

b + ( - b ) cos

 4

2

Combining the above cases with Eq. (6) we get that:

inf

(u2y2 + u3)2dy

u2

,u3

R:u22+u23

=

1 2

yP1P2

  ( - b ) sin
4

( - b )2 cos

2 4

y2C 82(2 + 2

b + ( - b ) cos

 4

2) dy2



( - b )4 cos

 4

3 sin

 4

2 · 82

2+2

b + ( - b ) cos

 4

2





2 · 82 2

( - b )4 sin

3 4

2 + 2 b + ( - b ) cos

 4

2

( - b )4 sin



84

3
4 · min

1 1, 2

(7)

where in the second inequality we used that for   [0, ] we have sin

 4

 cos

 4

, and in the last

inequality we used that b  , and ( - b ) cos

 4

 . Combining Eq. (5) with Eq. (7) finishes the proof.

B Proofs from Section 3
B.1 Proof of Theorem 3.1
Let > 0, for the input distribution, we consider the uniform distribution on the ball of radius . Let bw be the last coordinate of w, and denote by w~ , x~ the first d coordinates of w and x. Using the assumption on
17

the initialization of w0 and on the boundness of the distribution D~ we have:

 | w~ 0, x~ |  w~ 0 x~  d.





Since bw0 is also initialized with U ([-1, 1]), w.p > 1/2 - d we have that bw0 < - d. If this event
happens, since the activation is ReLU we get that  ( w0, x ) = 1( w~ 0, x~ + bw0 > 0) = 0 for every x~ in

the support of the distribution. Using Eq. (3) we get that F (w0) = 0, hence gradient flow will get stuck

at its initial value.

B.2 Proof of Theorem 3.2



Lemma B.1.

Let w

 Rd+1

such that bw

=

0, w~1

< - 4 , and
d

w~ 2:d

2

d. Then,

Pr w x  0, v x  0 = 0 .
xD

Proof. If v

x



0

then

x1



r

-

r 2d2

and

hence

x21



r2

-

r2 d2

.

Since

we

also

have

x~

 r then

x~2:d 2 =

x~ 2 - x21  r2 -

r2

-

r2 d2

r2 = d2 .

Hence,

r

r

Pr
xD

w

x  0, v

x0

 Pr
xD

w

x  0, x1  r - 2d2 ,

x~2:d

 d

.



Since bw = 0,

w~ 2:d

2

d and w~1

<

- 4 ,
d

then for every x~



B

such that x1



r-

r 2d2



r 2

and

x~2:d



r d

we

have

4r r

w

x = w~

x~ = w~1x~1 +

w~ 2:d, x~2:d

< - · + 2 d2

d· =0. d

Therefore, PrxD w x  0, v x  0 = 0.

Lemma B.2.

With probability

1 2

- od(1) over the choice of w0, we have

Pr
xD

w0 x  0, v

x0

=0.

Proof. Let w  Rd+1 such that bw = 0 and w~  N (0, Id). Since w~1 has a standard normal distribution,

then

we

have

w~1

<

- 4
d

with

probability

1 2

-

od(1).

Moreover,

note

that

w~ 2:d 2 has a chi-square distri-

bution and the probability of

w~ 2:d

2



4d

is

1

-

od(1).

Hence,

by

Lemma

B.1,

with

probability

1 2

-

od(1)

over the choice of w, we have

Pr w x  0, v x  0 = 0 .
xD

Therefore,

w

Pr  x  0, v x  0 = Pr w x  0, v x  0 = 0 .

xD w

xD

Since



w w

has the distribution of w0, the lemma follows.

18

Lemma B.3. Assume that w0 satisfies PrxD w0 x  0, v x  0 = 0. Let  > 0 and let w  Rd+1 such that w~ = w~ 0, and bw  0. Then, PrxD w x  0, v x  0 = 0. Moreover, we have

·

If

-

bw w~

<

r, then

dw~ dt

=

-sw~

for some s

>

0, and

dbw dt

<

0.

·

If

-

bw w~



r, then

dw~ dt

=

0

and

dbw dt

=

0.

Proof. For every x we have: If w x = w~ 0 x~ + bw  0 then w~ 0 x~  0, and therefore w0 x = w~ 0 x~  0. Thus

Pr
xD

w

x  0, v

x0

 Pr
xD

w0 x  0, v

x0

=0.

(8)

We have

dw~

- dt

=

w~ F

(w)

=

E
x

(w

x) - (v

x)

 (w

x)x~

= E (w x) - (v x) 1(w x  0)x~ x

= E (w x) - (v x) 1(w x  0, v x < 0)x~ x

+ E (w x) - (v x) 1(w x  0, v x  0)x~ x

(Eq. (8))
=E

(w

x) - (v

x)

1(w

x  0, v

x < 0)x~

x

= E (w x) x~
x
= E 1(w~ x~ > -bw)(w~ x~ + bw)x~ . x~

If

-

bw w~

 r then for every x~  B we have w~ x~ 

-

bw w~

< r, i.e.,

w~ r > -bw, then Prx~ w~

x~ > -bw

obtain

dw~ dt

=

-sw~

for

some

s

>

0.

Next, we have

w~

r



-bw and hence

dw~ dt

=

0.

Note that if

> 0. Since D~ is spherically symmetric, then we

- dbw dt

=

bw F

(w)

=

E
x

(w

x) - (v

x)

 (w

x) · 1

= E (w x) - (v x) 1(w x  0) x

(Eq=. (8)) E (w x) - (v x) 1(w x  0, v x < 0) x

= E (w x)
x
= E 1(w~ x~ > -bw)(w~ x~ + bw) . x~

If

-

bw w~

 r then for every x~  B we have w~

x~ 

w~

r  -bw and hence

dbw dt

= 0. Otherwise, we have

dbw dt

< 0.

Proof of Theorem 3.2. By Lemma B.2 w0 satisfies PrxD w0 x  0, v x  0

=

0 w.p.

at

least

1 2

-

od(1). Then, by Lemma B.3 we have for every t > 0 that w~ t = tw~ 0 for some t > 0, bwt < 0, and

19

-

bwt w~ t

 r. Moreover, we have PrxD

wt x  0, v

x0

= 0. Hence, for every t we have

1

2

F (wt)

=

2

·

E
x

(wt x) - (v

x)

1

21

2

=

2

·

E
x

(wt x)

+

2

·

E
x

(v

x)

-E
x

(wt x)(v

x)

1

21

2

=

2

·

E
x

(wt x)

+

2

·

E
x

(v

x)

1

2



2

·

E
x

(v

x)

= F (0) .

Thus, gradient flow does not converge to the global minimum F (v) = 0 < F (0).

C Proofs from Section 4

Proof of Theorem 4.2. The gradient of the objective is:

F (w) = E (w x) - (v x) ·  (w x)x .
xD
We can rewrite it using that  is the ReLU activation, and separating the bias terms:

F (w) = E (w~ x~ + bw) - (v~ x~ + bv) · 1(w~ x~ + bw > 0)x . x~D~

First, notice that if w~ = 0 and bw < 0 then 1(w~ x~ + bw > 0) = 0 for all x~, hence F (w) = 0. Second,

using Cauchy-Schwartz we have that | w~ , x~ |  c ·

w~

.

Hence,

for

w

with

w~

=

0

and

-

bw w~

 c we have

that 1(w~ x~ + bw > 0) = 0 for all x~ in the support of the distribution, hence F (w) = 0. Lastly, it is

clear that for w = v we have that F (w) = 0. This shows that the points described in the statement of the

proposition are indeed critical points. Next we will show that these are the only critical points.

Let w  Rd+1 which is not a critical point defined above - i.e. either w~ = 0 and bw > 0, or w~ = 0 and

-

bw w~

< c. Then we have:

F (w), w - v = ExD  (w x)((w x) - (v x))(w x - v x)

= ExD 1(w x > 0, v x > 0)((w x) - (v x))(w x - v x) +

+ ExD 1(w x > 0, v x  0)(w x)(w x - v x)

 ExD 1(w x > 0, v x > 0)(w x - v x)2 +

+ ExD 1(w x > 0, v x  0)(w x)2 .

= ExD 1(w~ x~ > -bw, v~ x~ > -bv)(w x - v x)2 +

+ ExD 1(w~ x~ > -bw, v~ x~  -bv)(w x)2 .

(9)

Denote:

A1 := {x~  Rd : w~ x~ > -bw, v~ x~ > -bv, x~ < c} A2 := {x~  Rd : w~ x~ > -bw, v~ x~  -bv, x~ < c}

20

Since w is not a critical point as defined above, we know that the set {x~  Rd : w~ x~ > -bw, x~ < c} has a positive measure, hence either A1 or A2 have a positive measure. Assume w.l.o.g that A1 have a positive measure, the other case is similar. Since both terms inside the expectations of Eq. (9) are positive, we can lower bound it with:

ExD 1(w~ x~ > -bw, v~ x~ > -bv)(w x - v x)2

= w - v 2ExD 1(x~  A1)((w - v) x)2

(10)

Denote u := w - v, and note that w = v, hence u = 1. Denote by p(x~) the pdf of D~, then we can rewrite Eq. (10) as:

w-v 2·

1(x~  A1) · (u~ x~ + bu)2p(x~)dx~

x~Rd

= w-v 2·

(u~ x~ + bu)2p(x~)dx~

(11)

x~A1

Since the set A1 has a positive measure, and the set {x~ : u~ x~ + bu = 0} is of zero measure, there is a point x~0 such that u~ x~ + bu = 0. By continuity, there is a small enough neighborhood A of x~0, such that u~ x~ + bu = 0 for every x~  A. Using Assumption 4.1 we can lower bound Eq. (11) by:

w-v 2·

(u~ x~ + bu)2dx~

x~A

where this integral is positive. This shows that F (w), w - v > 0, which shows that F (w) = 0, hence w is not a critical point.

D Proofs from Section 5

The following lemmas are required in order to prove Theorem 5.2. First, we show that if F (w)  F (0) -  then we can lower bound w and Prx w x  0, v x  0 .

Lemma D.1. Let  > 0 and let w  Rd+1 such that F (w)  F (0) - . Then

 w  c2 ,

and



Pr w x  0, v
x

x0

 c2 w

.

Proof. We have

1

F (0)

-





F (w)

=

2

E((w
x

x) - (v

x))2

1

=

2

E((w
x

x))2

+

1 2

E((v
x

x))2 - E((w
x

x)(v

x))

 F (0) - E((w x)(v x)) .
x

21

Hence
Thus, and

  E (w x)(v x) = E 1(w x  0, v x  0) · w x · v x

x

x

 w c2 · Pr w x  0, v x  0 .
x





w  c2 · Prx [w x  0, v x  0]  c2 ,



Pr w x  0, v
x

x0

 c2 w

.

Using the above lemma, we now show that if F (w)  F (0) -  then w - v decreases.

Lemma D.2. Let  > 0 and let B > 1. Let w  Rd+1 such that F (w)  F (0) -  and w - v  B - 1.

Let  =

3 3·122B3c8c 2

and let 0 <  

 c4

.

Let

w

= w - F (w). Then,

w - v 2  w - v 2 · (1 - )  (B - 1)2 .

Proof. We have

w - v 2 = w - F (w) - v 2

= w - v 2 - 2 F (w), w - v + 2 F (w) 2 .

(12)

We first bound F (w) 2. By Jensen's inequality and since  is 1-Lipschitz, we have:

F (w) 2  E

(w

x) - (v

x)

2
 (w

x)

x

2

x

 c2 E

2
(w x) - (v x)

x

 c2 E

2
w x-v x

x

= c2 E

2
(w - v) x

x

 c4 w - v 2 .

(13)

Next, we bound F (w), w - v . Let u = w - v. We have

F (w), w - v = E (w x) - (v x)  (w x)(w x - v x)
x
= E w x - v x 2 1(w x  0, v x  0)+ x E w x · (w x - v x)1(w x  0, v x < 0) x
 w - v 2 · E 1(w x  0, v x  0)(u x)2 . x

22

Let



=

 12Bc3c

.

The

above

is

at

least

w - v 2 · 2 · Pr w x  0, v x  0, (u x)2  2
x

= w - v 2 · 2 · Pr w x  0, v x  0 - Pr w x  0, v x  0, (u x)2 < 2 .

x

x

By Lemma D.1, and since w  w - v + v  B - 1 + 1 = B, the above is at least

w - v 2 · 2 ·



c2 w

- Pr
x

w

x  0, v

x  0, |u x| < 

 w - v 2 · 2 ·



c2B

-

Pr
x

|u

x|  

= w - v 2 · 2 ·



c2B

-

Pr
x

|u~

x~ + bu|  

.

(14)

We now bound Prx |u~ x~ + bu|   . We denote a =


u~

.

If a



1 4c

,

then

since

u

= 1 we have

bu 

1-

1 16c2



1-

1 16

=

15 4

.

Hence,

for

every

x

with

x

 c we have





15

15 1 1

|u~ x~ + bu|  |bu| - |u~ x~| 

4

- ac 

4

->. 42

Note that

  = 12Bc3c

F (0)  12Bc3c

1 = 12Bc3c

1

·

2

E((v
x

x))2



1 12Bc3c

· 1 c2 = 1 2 24Bcc

1 ,
24

where the last inequality is since B, c, c  1. Therefore, |u~ x~ + bu| > . Thus,

Pr
x

|u~

x~ + bu|  

=0.

Assume

now

that

a



1 4c

.

We

have

Pr
x

|u~

x~ + bu|  

= Pr
x

u~

x~  [- - bu,  - bu]

= Pr

u¯~

x~



 [-

-

bu ,



-

bu ]

x

a aa a

 c ·2·
a

 8cc  .

Combining the above with Eq. (14), we obtain

F (w), w - v  w - v 2 · 2 ·

 c2B - 8cc 

=

w-v

2

2

122B2c6c 2

·





c2B - 8cc · 12Bc3c

=

w-v

2

2

122B2c6c 2

·

 3c2B

=

w-v

2

3

3 · 122B3c8c 2

.

(15)

23

Combining

Eq.

(12),

(13)

and

(15),

and

using



=

3 3·122B3c8c

2

,

we

have

w - v 2  w - v 2 - 2 w - v 2 ·  + 2c4 w - v 2 = w - v 2 · 1 - 2 + 2c4 .

Since





 c4

,

we

obtain

w -v 2 

w-v 2·

1

-

2

+

c4

·

 c4

= w - v 2 · (1 - )  w - v 2  (B - 1)2 .

Next, we show that F (w) remains smaller than F (0) -  during the training. In the following two lemmas we obtain a bound for the smoothness of F in the relevant region, and in the two lemmas that follow we use this bound to show that F (w) indeed remains small.

Lemma D.3. Let w  Rd+1 such that F (w)  F (0). Then, F (w)  c 2F (0).

Proof. By Jensen's inequality, we have

F (w) 2  E

(w

x) - (v

x)

2
 (w

x) x 2

x

 c2 E (w x) - (v x) 2
x

 c22F (w)  2c2F (0) .

Lemma D.4. Let M, B > 0 and let w, w  Rd+1 be such that for every s  [0, 1] we have M  w + s(w - w)  B. Then,

F (w) - F (w )

 w-w

· c2

8(B + 1)c c2 1+

.

M

Proof. We assume w.l.o.g. that w - w



M 2c

.

Indeed,

let

0

=

s0

<

.

.

.

<

sk

=

1

for

some

integer

k,

let

wi = w + si(w - w), and assume that

wi - wi+1



M 2c

for

every

i.

If

the

claim

holds

for

every

pair

wi, wi+1, then we have

F (w) - F (w )

k-1

=

F (wi) - F (wi+1)

i=0

k-1



F (wi) - F (wi+1)

i=0

k-1



wi - wi+1 · c2

8(B + 1)c c2 1+
M

i=0

= c2

8(B + 1)c c2 1+

w-w .

M

24

We have

F (w) - F (w )

= E((w x) - (v x)) (w x)x - ((w x) - (v x)) (w x)x
x
 E 1(w x  0, w x  0) w x - (v x) - w x + (v x) x + x E 1(w x  0, w x < 0) w x - (v x) x + x E 1(w x < 0, w x  0) w x - (v x) x . x

By Jensen's inequality and Cauchy-Shwartz, the above is at most

E 1(w x  0, w
x
E 1(w x  0, w
x
E 1(w x < 0, w
x

x  0) w - w · x · x + x < 0) ( w · x + v · x ) · x + x  0) w · x + v · x · x .

By our assumption we have x  c and w , w  B. Hence, the above is at most

w - w c2 + Pr w x  0, w x < 0 · c2 · (B + 1)
x

+ Pr w x < 0, w x  0 · c2 · (B + 1) .

(16)

x

Now, we bound Prx w x  0, w x < 0 . If w x  0 and w x < 0 then

w x = w x + (w - w ) x < 0 + w - w · x  c · w - w .

Hence, we only need to bound

Pr w x  [0, c ·
x

w-w

]

= Pr w~
x

x~ + bw  [0, c ·

w-w

]

.

We denote a =

w~

. If a 

M 4c

,

then

since

w

 M we have |bw| 

M2 -

M 4c

2=M

1 - 1/(16c2).

Hence for every x we have

|w~ x~ + bw|  |bw| - |w~ x~|  |bw| - ac  M 

1 - 1/(16c2) - M  M 4

15 - 1

=M·

> M/2  c w - w .

4

M 1 - 1/16 -
4

Thus, Prx w~ x~ + bw  [0, c · w - w ] = 0.

Assume

now

that

a



M 4c

.

Hence,

c a



4c2 M

.

Therefore,

we

have

Pr
x

w~

x~ + bw  [0, c ·

w-w

]

= Pr
x

w¯~

x~  [- bw , - bw + c · a aa

w-w

]

 Pr w¯~ x~  [- bw , - bw + 4c2 · w - w ]

x

a aM

4c2 c · · w-w .
M

25

Hence, Prx w

x  0, w

x<0

c

·

4c2 M

·

w-w

. By similar arguments, this inequality holds

also for Prx w x < 0, w x  0 . Plugging it into Eq. (16), we have

F (w) - F (w )  w - w = w-w

c2 + 2 · c2 · (B + 1) · c · 4c2 M

· c2

8(B + 1)c c2 1+

.

M

Lemma D.5. Let f : Rd  R and let L > 0. Let x, y  Rd be such that for every s  [0, 1] we have f (x + s(y - x)) - f (x)  Ls y - x . Then,

f (y) - f (x)  f (x)

L (y - x) +

y-x

2.

2

Proof. The proof follows a standard technique (cf. [2]). We represent f (y) - f (x) as an integral, apply Cauchy-Schwarz and then use the L-smoothness.

1

f (y) - f (x) - f (x) (y - x) = f (x + s(y - x)) (y - x)ds - f (x) (y - x)

0

1

 f (x + s(y - x)) - f (x) · y - x ds

0

1
 Ls y - x 2ds

0

L =

y-x

2.

2

Hence, we have

f (y) - f (x)  f (x)

L (y - x) +

y-x

2.

2

Lemma D.6. Let B,  > 0 and let L = c2

1

+

16(B+1)c 

c4

. Let w  Rd+1 such that F (w)  F (0) - 

and let w = w -  · F (w), where   min

 ,
2c3 2F (0)

1 L

= min

 ,
2c3 2F (0)

 c2+16(B+1)c

c6

.

Assume that w , w

 B. Then, we have F (w ) - F (w)  -

1

-

L 2



F (w) 2, and F (w ) 

F (w)  F (0) - .

Proof.

Let M

=

 2c2

.

By

Lemmas

D.1

and

D.3,

we

have

w



 c2

and

F (w)

c

2F (0). Hence for

every   [0, 1] we have









w - F (w)  c2 -  · c

2F (0)  c2 - 2c3

·c 2F (0)

2F (0) = 2c2 = M .

Since w , w  B, we also have w - F (w)  B. By Lemma D.4, we have for every   [0, 1]

that

F (w) - F (w - F (w))

  F (w) · c2

8(B + 1)c c2 1+

.

M

26

We have L = c2

1

+

16(B+1)c 

c4

= c2

1

+

8(B+1)c M

c2

. By Lemma D.5 we have

F (w - F (w)) - F (w)  - F (w) 2 + L 2 F (w) 2 . 2

Since





1 L

,

we

also

have

F (w

-

F (w))



F (w)



F (0)

-

.

We are now ready to prove the theorem:

Proof of Theorem 5.2. Let B =

w0 + 2. Assume that   min

 ,
2c3 2F (0)

 c2+16(B+1)c

c6

,

 c4

. We

have w0 - v  w0 + v = w0 + 1  B - 1. By Lemmas D.2 and D.6, for every t we have

wt - v  B - 1 (thus, wt  B) and F (wt)  F (0) - . Moreover, by Lemma D.2, we have for every t that wt+1 - v 2  wt - v 2 · (1 - ). Therefore, wt - v 2  w0 - v 2 (1 - )t.

It remains to show that









min 2c3 2F (0) , c2 + 16(B + 1)c c6 , c4 = c4 .

Note

that

we

have





F (0)

=

1 2

Ex((v

x))2



1 2

·

c2.

Thus



3



c4



c4 = 3 · 122B3c12c 2  3 · 122B3c12c 2 · 4 = 123B3c8c 2 .

We have





2c3 2F (0)  2c4  c4 ,

where the last inequality is since B, c, c  1. Finally,











c2 + 16(B + 1)c c6



c4 2

+ 16(B + 1)c c6



17(B + 1)c c6



34Bc c6



c4

.

E Proofs from Section 6

Before proving Theorem 6.2, we first proof two auxiliary propositions which bounds certain areas for which the vector w cannot reach during the optimization process. The first proposition shows that if the norm of w~ is small, and its bias is close to zero, then the bias must get larger. The second proposition shows that if the norm of w~ is small, and the bias is negative, then the norm of w~ must get larger.

Proposition E.1. Assume that w~ - v~ 2  1, and that Assumption 6.1 holds. If w~  0.4 and bw 

0,

3 640

then

(F (w))d+1



-

3 640

.

27

Proof. The d + 1 coordinate of the distribution D is a constant 1. We denote by D~ the first d coordinates of the distribution D. Hence, we can write:

(F (w))d+1 = ExD ((w x) - (v x))1(w x > 0)

= Ex~D~ ((w~ x~ + bw) - (v~ x~ + bv))1(w~ x~ > -bw)

= Ex~D~ (w~ x~ + bw) · 1(w~ x~ > -bw) -

- Ex~D~ (v~ x~ + bv) · 1(w~ x~ > -bw, v~ x~ > -bv)

(17)

We will bound each term in Eq. (17) separately. Using the assumption that D~ is spherically symmetric, we can assume w.l.o.g that w~ = w~ e1, the first unit vector. Hence we have that :

Ex~D~ (w~ x~ + bw) · 1(w~ x~ > -bw)

=Ex~D~

( w~ x1 + bw) · 1

x1

>-

bw w~

= w~ Ex~D~

x11

x1

>

-

bw w~

+ bwEx~D~

1

x1

>

-

bw w~

(a)
 0.4Ex~D~

x11

x1

>

-

bw w~

+ bw

(b)

0.4Ex~D~ [x11(x1 > 0)] + bw .

(18)

Here, (a) is since

w~

 0.4, and Ex~D~

1

x1

>

-

bw w~

 1, (b) is since bw  0, hence

Ex~D~

x11

0

>

x1

>

-

bw w~

0.

For the second term of Eq. (17), we assumed that

w~ -v~

2  1, which shows that (w~ , v~) 

 2

,

and

the

term

is

largest

when

this

angle

is

largest.

Hence,

to

lower

bound

this

term

we

can

assume

that

(w~ , v~)

=

 2

,

and since the distribution is spherically symmetric we can also assume w.l.o.g that v~ = e2, the second unit

vector. Now we can bound:

Ex~D~ (v~ x~ + bv) · 1(w~ x~ > -bw, v~ x~ > -bv)

Ex~D~

(x2 + bv) · 1

x1

>-

bw w~

, x2

> -bv

1  2 Ex~D~

[(x2

+

bv)

·

1

(x2

>

-bv)]

1  2 Ex~D~

[x2

·

1

(x2

>

0)]

+

1 2 Ex~D~

[(x2

+

bv)

·

1

(0

>

x2

>

-bv)]

1  2 Ex~D~

[x2

·

1 (x2

>

0)]

=

1 2 Ex~D~

[x1

·

1 (x1

>

0)]

,

(19)

where we used the assumption bv  0 and the symmetry of the distribution. Combining Eq. (18), Eq. (19) with Eq. (17) we get:

28

(F (w))d+1  bw - 0.1Ex~D~ [x1 · 1(x1 > 0)] .

Let D^ be the marginal distribution of D~ on the plane spanned by e1 and e2, and denote by x^ the projection of x~ on this plane. By Assumption 6.1(3) we have that the pdf of this distribution is at least  in a ball or radius  around the origin. This way we can bound:

Ex~D~ [x1 · 1(x1 > 0)] = Ex^D^ [x1 · 1(x1 > 0)]



 P(/2 < 2

x^

< , x1 > /2)



3

 2 P(x1  [/2, 3/4], x2  [-/4, /4]) = 32 .

Combining the above, and using the assumption on bw we get that:

3

3

(F (w))d+1



bw

-

320

- 640

Proposition E.2. Assume that

w~ - v~

<

1, and Assumption 6.1 holds.

Denote by 

=

Ex~D~ [|x1x2|]
[ ] Ex~D~ x21

where D~ is the projection of the distribution D on its first d coordinates. If

w~



 2

and

bw



0

then

F (w)1:d, w~  0.

Proof. Denote by D~ the projection of the distribution D on its first d coordinates, we have that:

F (w)1:d, w~ =ExD (w x) - (v x) 1(w x > 0)w~ x~

=Ex~D~ (w~ x~ + bw) - (v~ x~ + bv) 1(w~ x~ > -bw)w~ x~

Ex~D~ w~ x~ + bw - (v~ x~) · 1(w~ x~ > -bw)w~ x~ .

(20)

The inequality above is since bv  0. Recall that our goal is to prove that the above term is negative, hence we will divide it by w~ . Also, since the distribution D~ is symmetric we can assume w.l.o.g that
w~ = w~ e1. Hence, it is enough to prove that the following term is non-positive:

w~ Ex~D~ = w~ Ex~D~  w~ Ex~D~

w~ x1 + bw - (v~ x~)

·1

x1

>

-

bw w~

x1

w~ x21 + bwx1

·1

x1

>-

bw w~

- w~ Ex~D~

w~ x21 + bwx1

·1

x1

>-

bw w~

- w~ Ex~D~

x1v~ x~ · 1 x1v~ x~ · 1

x1 > -

bw w~

, v~

x~ > -bv

x1 > -

bw w~

, v~

x~ > 0

.

(21)

We will first bound the second term above. Since the term only depend on inner products between w~ , v~ with x~, we can consider the marginal distribution D^, of D~ on the plane spanned by w~ and v~. Since D~ is symmetric we can assume w.l.o.g that D^ is spanned by the first two coordinates x1 and x2. Let v^~ be the

29

projection of v~ on this plane, then we can write v^~ = (v1, v2) where v12 + v22 = 1. Note that since the distribution D^ is symmetric, we have that E[x21] = E[x22]. By Cauchy-Schwarz we have:

|covD^ (x1, x2)|  varD^ (x1) · varD^ (x2) = varD^ (x1)

Again, by symmetry of D^ we have that E[x1] = E[x2]. Opening up the above terms we get that E[x1 · x2] 

E[x21]. Also, we assumed that

w~ - v~

<

1,

then

(v~, w~ )



 2

which

means

that

v1



0.

Hence,

the

second

term of Eq. (21) is smallest when v~ = e2. In total, we can bound Eq. (21) by:

w~ Ex~D~  w~ Ex~D~ = w~ Ex~D~

w~ x21 + bwx1

·1

x1

>

-

bw w~

- w~ Ex~D~

w~

x21 + bwx1 -

1 2 x1|x2|

·1

x1

>

-

bw w~

1 w~ x1 + bw - 2 |x2|

· x11

x1

>

-

bw w~

x1x2 · 1

x1

>-

bw w~

, x2

>0

(22)

By our assumption, bw  0. Both terms inside the expectation in Eq. (22) are largest when bw = 0. Hence, we can bound Eq. (22) by:

w~ Ex~D~

1 w~ x1 - 2 |x2|

· x11 (x1 > 0)

=

w~ 2 2 Ex~D~

x21

-

w~ 4 Ex~D~ [|x1x2|]



w~ 2 2 Ex~D~

x21

-

w~  4 Ex~D~

x21

= c1

w~ 2 w~ 

-

2

4

.

(23)

In particular, for

w~



 2

,

Eq.

(23)

non-positive.

We are now ready to prove the main theorem:

Proof of Theorem 6.2.

Denote bt

= max{0, -

bwt w~ t

}.

We will show by induction on the iterations of gradi-

ent descent that throughout the optimization process bt < 2.4 · max

1,

1 

and

(w~ t, v~)



 2

for

every

t  0.

By the assumption on the initialization we have that w~ 0 - v~ 2  w0 - v 2 < 1, and also v~ = 1,

hence

(w~ 0, v~)



 2

.

We

also

have

that

bw0



0,

hence

b0

=

0

this

proves

the

case

of

t

=

0.

Assume

this

is true for t. We will bound the norm of the gradient of the objective using Jensen's inequality:

F (w) 2  ExD ((w x) - (v x))21(w x > 0)x x

 ExD (w x - v x)2x x

 w - v 2ExD x 4 = w - v 2c .

(24)

For the (t + 1)-th iteration of gradient descent we have that:

wt+1 - v 2 = wt - F (wt) - v 2

= wt - v 2 - 2 F (wt), wt - v + 2 F (wt) 2

 wt - v 2 - 2 F (wt), wt - v + 2c wt - v 2 .

(25)

30

By Theorem A.2, and the induction assumption on (w~ t, v~) we get that there is a universal constant c0,

such max

that

F (wt), wt - v



c0(- 2

1,

1 

and Assumption 6.1(3) we

2bt) wt - v can bound (

2. Using the induction 
- 2bt)  0.1. In total

assumption we get that

that bt < F (wt),

2.4 · wt -

v



c0 102

wt - v

2.

By

taking





c0 10c2

and

combining

with

Eq.

(25)

we

have

that:

wt+1 - v 2 < wt - v 2 .

In particular,

w~ t+1 - v~ 2 

wt+1 - v 2 <

wt

-v

2

<

1,

which shows that

(w~ t+1, v~)



 2

,

and

concludes the first part of the induction.

The bound for bt is more intricate, for an illustration see Figure 2. Let t be the first iteration for which

w~ t  0.4. First assume that t  t , we will show that in this case bt = 0. Assume otherwise, and let t0

be the first iteration for which bt0 > 0, this means that bwt0 < 0 and bwt0-1  0. We have that:

bwt0 = bwt0-1 - F (wt0 )d+1 .

If bwt0-1 bwt0-1 

 0.

3 640

,

then

by

Proposition

E.1

the

last

coordinate

of

the

gradient

is

negative,

hence

Otherwise, assume that bwt0-1

>

3 640

.

By Eq. (24):

|F (wt0 )d+1|



F (w)

bwt0>  c.

Hence, by taking 

<

 640 c



3 640 c

,

we

get

that

bwt0



0, which is a contradiction (note that by

Assumption 6.1(3), we have   1). We proved that if t  t then bwt  0, which means that bt = 0.

Assume now that t > t . We will need the following calculation: Assume that w~ t = , Then

w~ t - v~ 2  (1 - )2, and the minimum is achieved at w~ = v~. Since we have:

w~ t - v~ 2 + (bwt - bv)2 = wt - v 2  1 ,

we get that (bwt -bv)2  1-(1-)2  2. If we further assume that bwt  0, then b2wt  (bwt -bv)2  2. Combining all the above, we get that if w~ t =  then:

bt = max

0, - bwt w~ t

2 .


(26)

To show the bound on bt we split into cases, depending on the norm of w~ t:

Case I:

2 5

<

w~ t



 2

and

bwt



0.

In

this

case

we

have:

w~ t+1 2 = = 

w~ t - F (wt)1:d 2 w~ t 2 - 2 w~ t, F (wt)1:d w~ t 2 - 2 w~ t, F (wt)1:d

+ 2 F (wt)1:d 2 .

We can use Proposition E.2 to get that w~ t, F (wt)1:d  0, hence w~ t+1 2  w~ t 2. By Eq. (26) we

get that bt+1 

5 



2.4 

.

Case II:

w~ t

 min

0.4,

 2

.

In

this

case,

by

choosing

a

step

size



<

1 40c

min{1,  }we

can

bound

w~ t+1  w~ t 2 - 2 w~ t, F (wt)1:d

 w~ t 2 - 2 w~ t F (wt)1:d

 w~ t 2 - 2 w~ t F (wt)



w~ t 2 - 2 · 2c  min

2 0.39,
5

.

31

Figure 2: A 2-d illustration of the optimization landscape. The x axis represents w~ , and the y-axis

represents bw.

In the figure, for simplicity, we assume that bv

=

0, and 

= 0.1 which means that

2 5

=

0.4.

The red circle represents the area with w - v  1, throughout the optimization process wt stays in this

circle.

The

black

region

represents

the

area

where

bt

=

-

bw w~

can be potentially large, our goal is to show

that wt stays out of this region. Case I shows that wt cannot cross the blue region. Case II shows that if wt

is to the right of the black region, then bt is upper bounded. Case III shows that wt cannot cross the orange

region (sub-cases (a) and (b)), and cannot cross from the green region directly to the black region (sub-case

(c)).



Again, by Eq. (26) we get that bt+1  max

5.2,

2.4 

 2.4·max

1,

1 

. This concludes the induction.

Case III:

w~ t

 min

0.4,

2 5

. We split into sub-cases depending on the previous iteration: (a) If

bwt-1



0,

then

by

Case

I

the

norm

of

w~

cannot

get

below

2 5

,

hence

this

sub-case

is

not

possible;

(b)

If

bwt-1  0 and

w~ t-1

 min

0.4,

2 5

, then by the same reasoning in the case of t < t , bwt cannot get

smaller than zero. Hence, we must have that bwt+1  0; (c) If bwt-1  0 and

w~ t-1

 min

0.4,

2 5

then the bound depend on whether w~ t-1 is larger than 0.4 or not. If w~t-1  0.4, then using the

same reasoning as the case of t < t twice (both for the t - 1 and t iterations) we get that bt+1  0. If

w~ t-1 > 0.4 and bwt  0, then again this is the same case as in the case of t < t (since w~ t  0.4. The

last case is when w~ t-1 > 0.4 and bwt < 0, here using the same calculation as in Case II, we have that

w~ t

 0.39. Since

w~ t

 min

0.4,

2 5

, using Proposition E.2, the norm of w~ t can only grow, hence

by the same reasoning as in Case I we can also bound bt+1 < 2.4 max

1,

1 

.

Until

now

we

have

proven

that

throughout

the

entire

optimization

process

we

have

that

(w~ t, v~)



 2

32

and bt  2.4 · max

1,

1 

. Let  =  - (w~ t, v~), we now use Theorem A.2 and Eq. (24) to get that:

wt+1 - v 2 = wt - F (wt) - v 2

= wt - v 2 - 2 F (wt), wt - v + 2 F (wt) 2

4

 wt - v 2 - 2



-

bt

sin(

 2

)

842

 sin



wt - v

2-

  - 2bt
842

4 sin

 4

 4

3

wt - v 2 + 2c wt - v 2

3
wt - v 2 + 2c wt - v 2



wt - v

2

-

C~ 2

wt - v

2 + 2c

wt - v

2

(27)

where C~ bt  2.4

is some universal constant, and we used the bounds from

· max

1,

1 

 , and by the assumption that   2.5 2 max

the induction

1,

1 

. By

above that choosing 

 



 2

,



,

C~ 2c2

,

and

setting



=

C~ 2c2

we

get

that:

wt - v 2 - C~ min

1 1, 2

wt - v 2 + 2c wt - v 2

(1 - ) wt - v 2  · · ·  (1 - )t w0 - v 2 ,

which finished the proof.

F Proofs from Subsection 5.2

F.1 Proof of Theorem 5.4

We have

1

2

F (w)

=

2

E
x

(w

x) - (v

x)

1

2

=

F

(0)

+

2

E
x

(w

x)

- E (w x)(v x)
x

w 2c2

 F (0) +

2

- w E (w¯ x)(v x) .
x

(28)

Let



=

 4c

sin

 8

. We have

E (w¯ x)(v x)  2 · Pr (w¯ x)(v x)  2

x

x

 2 · Pr

w¯

 x  2 c, v

x

 

.

(29)

x

2c

In the following two lemmas we bound Prx

w¯

 x  2 c, v

x



 2c

.

33

Lemma F.1. If bv  0 then





Pr w¯ x  2 c, v x  

x

2c

 

 sin

 8

 - 2 c

4 sin

 8

2
.

Proof. If

v~



1 4c

,

then

we

have





Pr w¯ x  2 c, v x  

x

2c





 Pr w¯ x  2 c, v~ x~  

x

2c

= Pr

w~¯

x~



 2 c,

v¯~

x~ 



x

2 c v~

 Pr

w~¯

x~



 2 c,

v¯~

 x~  2 c

x

 

 sin

 8

4 sin

 - 2 c
 8

2
,

where

the

last

inequality

is

due

to

Lemma

A.1,

since

(w~ , v~)



3 4

.

If

v~



1 4c

,

then



1

1

15 3

bv 

1 - 16c2 

1- = > , 16 4 4

and hence

1

31



v

x = v~

x~

+

bv

>

- 4c

·

c

+

4

=

2







 2c

.

Therefore,

 Pr w¯ x  2 c, v x 



 = Pr w¯ x  2 c

= Pr

w~¯

 x~  2 c

.

x

2c x

x

For u~  Rd such that

u~

=

1

and

(w~ , u~)

=

3 4

,

Lemma

A.1

implies

that

the

above

is

at

least

Pr
x

w~¯

 x~  2 c, u~

 x~  2 c

 

 sin

 8

4 sin

 - 2 c
 8

2

.

Lemma F.2.

If bv

< 0 and -

bv v~





·

sin(
4

 8

)

,

then





Pr w¯ x  2 c, v x  

x

2c

 

 sin

 8

 - 2 c

4 sin

 8

2
.

Proof.









Pr w¯ x  2
x

c, v x   2c

= Pr w¯ x  2
x

c, v~

x~



 2c

-

bv

= Pr

w~¯

x~



 2 c,

v¯~

x~ 



- bv .

(30)

x

2 c v~ v~

34

Moreover, we have

sin ·

 8

4

2


bv v~

2 1 - v~ 2

1

= v~ 2 = v~ 2 - 1 ,

and hence

v~

2

16

2 sin2

 8

 + 16

16

16

 sin

 8

+ 4 2  (c · 1 + 4c)2 ,

where in the last inequality we used c   and c  1. Thus,

41 v~   .
5c 2c

Combining

the

above

with

Eq.

(30),

and

using

-

bv v~





·

sin(
4

 8

)

,

we

have





Pr w¯ x  2 c, v x  

x

2c

 Pr

w~¯

x~



 2 c,

v¯~



sin

x~  c +  ·

 8

x

4

= Pr

w~¯

x~



 2 c,

v¯~

 x~  2 c

x

 

 sin

 8

4 sin

 - 2 c
 8

2
,

where

the

last

inequality

is

due

to

Lemma

A.1,

since

(w~ , v~)



3 4

.

Combining Eq. (29) with Lemmas F.1 and F.2, we have

E (w¯
x

x)(v

x)  2 · 

 sin

 8

4 sin

 - 2 c

2

2 sin2

 8

= 16c

 8

 ·

 2

sin

4 sin

 8  8

2

4 sin3 =

 8

=M .

256c

Plugging the above into Eq. (28) we have

w 2c2

F (w)  F (0) +

- w ·M .

2

The above expression is smaller than F (0) if

w

<

2M c2

.

G Discussion on the Assumption on bv

In

Corollary

5.5

we

had

an

assumption

that

-

bv v~





·

sin(
4

 8

)

.

This

implies

that

either

the

bias

term

bv

is

positive, or it is negative but not too large. Here we discuss why this assumption is crucial for the proof of

the theorem, and what can we still say when this assumption does not hold.

35

In Theorem 3.2 we showed an example with bv < 0 where gradient descent with random initialization

does not converge w.h.p. to a global minimum even asymptotically3. In the example from Theorem 3.2

we

have

-

bv v~

=r

1

-

1 2d2

, and the input distribution is uniform over a ball of radius r. In this case, we

must

choose



from

Assumption

5.3

to

be

smaller

than

r

(otherwise



=

0)

and

hence

-

bv v~

>

1

-

1 2d2

.

Therefore

it

does

not

satisfy

the

assumption

-

bv v~





·

sin(
4

 8

)

(already

for

d

>

1).

If we choose, e.g.,



=

r 2

,

then

the

example

from

Theorem

3.2

satisfies

-

bv v~

=

2

-

1 d2

 2. It implies that our assumption on

-

bv v~

is

tight

up

to

a

constant

factor,

and

is

also

crucial

for

the

proof,

since

already

for

-

bv v~

= 2 we have

an example of convergence to a non-global minimum.

On

the

other

hand,

if

-

bv v~

>



·

sin(

 8

)

4

(i.e.

the

assumption

does

not

hold)

we

can

calculate

the

loss

at

zero:

1

F (0)

=

2

·

E
x

(v

2
x)

1

=

2

·

E
x

1(v~

x~ + bv  0)

v~

x~ + bv

2

1

=

2

·

E
x

1

v¯~ x~  - bv v~

v~ 2 v¯~ x~ + bv 2 v~



v~ 2

2

·E
x

1

v¯~

sin x~   ·

 8

4

v¯~ x~ 2 .

Let > 0 be a small constant. Suppose that the distribution D~ is spherically symmetric, and that  is large,

such that the above expectation is smaller than

.

For

such

,

we

either

have

-

bv v~





·

sin(
4

 8

)

,

in

which

case

gradient descent converges w.h.p.

to the

global

minimum,

or

-

bv v~

>



·

sin(
4

 8

)

,

in

which

case

the

loss at w = 0 is already almost as good as the global minimum. For standard Gaussian distribution, we can

choose  to be a large enough constant that depends only on (independent of the input dimension), hence

 will also be independent of d. This means that for standard Gaussian distribution, for every constant > 0

we can ensure either convergence to a global minimum, or the loss at 0 is already -optimal.

Note that in Remark 3.3 we have shown another distribution which is non-symmetric and depends on

the target v, such that the loss F (0) is highly sub-optimal, but gradient flow converges to such a point with

probability

close

to

1 2

.

3In Theorem 3.2 we have

v

= 1, but it still holds if we normalize v, namely, replace v with

v v

.

36

