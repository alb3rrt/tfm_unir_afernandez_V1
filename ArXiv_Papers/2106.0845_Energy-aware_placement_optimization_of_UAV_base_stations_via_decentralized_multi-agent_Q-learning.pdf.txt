Energy-aware placement optimization of UAV base stations via decentralized multi-agent Q-learning
Babatunji Omoniwa, Boris Galkin, Ivana Dusparic School of Computer Science and Statistics, CONNECT Centre,
Trinity College Dublin, Ireland, Emails: omoniwab@tcd.ie, galkinb@tcd.ie, ivana.dusparic@scss.tcd.ie

arXiv:2106.00845v1 [cs.MA] 1 Jun 2021

Abstract--Unmanned aerial vehicles serving as aerial base stations (UAV-BSs) can be deployed to provide wireless connectivity to ground devices in events of increased network demand, points-offailure in existing infrastructure, or disasters. However, it is challenging to conserve the energy of UAVs during prolonged coverage tasks, considering their limited on-board battery capacity. Reinforcement learning-based (RL) approaches have been previously used to improve energy utilization of multiple UAVs, however, a central cloud controller is assumed to have complete knowledge of the end-devices' locations, i.e., the controller periodically scans and sends updates for UAV decision-making. This assumption is impractical in dynamic network environments with mobile ground devices. To address this problem, we propose a decentralized Qlearning approach, where each UAV-BS is equipped with an autonomous agent that maximizes the connectivity to ground devices while improving its energy utilization. Experimental results show that the proposed design significantly outperforms the centralized approaches in jointly maximizing the number of connected ground devices and the energy utilization of the UAV-BSs.
Index Terms--Reinforcement learning, UAV base stations, energy management, wireless connectivity.
I. INTRODUCTION
The use of unmanned aerial vehicles (UAVs) to provide wireless connectivity to ground end-devices has received significant research attention [1], [2]. In particular, the adjustable altitude and mobility of UAVs make them suitable candidates for flexible deployment as aerial base stations in events of increased network demand, points-of-failure in existing infrastructure, or disasters. UAVs can adjust their position to establish reliable connectivity with ground end-devices [1], [3]. UAVs play a crucial role in several Internet-of-Things (IoT) applications, one of which is the provision of real-time connectivity for ground devices to the internet. Data acquisition and dissemination for critical applications, rescue missions in disaster scenarios, etc., are also useful applications of UAVs.
In this paper, we focus on UAVs serving as aerial base stations (UAV-BSs) to provide wireless connectivity to ground devices. Ground devices may either be static, e.g. flood alert sensors, smart electric meters, air quality sensors, etc. or mobile, e.g. smartphones interconnected to smart earbuds, trackers and internal heart monitoring sensors in humans, parcel and vehicle tracking sensors, etc. Nevertheless, for UAVs to be effectively deployed to serve as UAV-BSs, several technical challenges must be addressed such as optimal placement [4], trajectory-planning [5] and energy management [1], [2], [7], [8]

of the UAVs. In particular, UAVs have limited on-board battery capacity, making it a challenge to serve as aerial base stations for extended periods of time [1] ­ [6].
In [2] and [6], a single UAV was deployed to support downlink wireless communications to ground devices. Although, these works show significant improvement in energy utilization, they focused on improving the energy utilization of a UAV providing coverage in a static environment. In [1], multiple UAVs were deployed to support wireless coverage in geographically large areas. This work minimized the UAVs' energy consumption, however, the UAVs relied on periodic updates from a central cloud controller for decision making and control, making the approach unsuitable in disaster scenarios. A branch of machine learning called reinforcement learning (RL) was used in [8] and [14] to minimize energy consumption in UAVs. However, these works consider an ideal assumption of complete knowledge of the locations of all ground devices in the network. In a dynamic network environment, this assumption is impractical. This makes the approaches unsuitable in disaster scenarios where the communication bandwidth is quite limited and cannot support much information exchange between UAV and server. Motivated by these findings, our approach proposed in this paper has two main contributions given as follows:
· We propose a decentralized Q-learning with local sensory information (DQLSI) algorithm that relies on local observations from the UAV and the broadcast connectivity observation from neighbouring UAVs for decision making. Here, each UAV is equipped with an autonomous agent that learns to jointly maximize the number of connected ground devices and energy utilization in the network.
· We conduct simulations based on a realistic model of the agent's environment, taking into consideration both randomly distributed static devices, as well as dynamic devices which follow a random walk mobility (RWM) model. Furthermore, our proposed DQLSI approach is compared to state-of-art centralized approaches that assume global spatial knowledge of all ground end-devices in a quickly-evolving network. Results show that our proposed approach is robust in simultaneously providing improved energy utilization while maximizing the number of connected ground devices.
The remainder of this work is organized as follows. In Section

II, we present related work. The environment model is provided in Section III. We discuss the proposed decentralized Qlearning with Local Sensory Information (DQLSI) approach in section IV. In section V, we present the simulation setup, results and discussions. Section VI concludes the paper and outlines future directions.

II. RELATED WORK
In this section, we present related work that addresses energy management challenges of UAVs serving as base stations. In recent work [2], [6], a single UAV deployed as an aerial base station to provide coverage to static ground devices was shown to have a significant improvement in energy utilization. Although, these work are often limited by a single energy management policy, a static and an ideal coverage space. In [1], an iterative algorithm was proposed to jointly optimize the three-dimensional (3D) placement and mobility of the UAVBS, to minimize the total energy consumed by multiple UAVBSs in the network. The work was able to improve energy utilization, however, the periodicity of control and data updates with the central control station may result in increased energy consumption for the UAVs. It is worthwhile to look into alternatives of relying on a central controller in the advent of a disaster scenario where the ground infrastructure may be damaged [8]. In particular, to reduce administrative cost and complexity in network management and control, quicklyevolving networks require some level of autonomy enabled by device organization [9].
To this end, machine learning is a promising and powerful tool to provide autonomous and effective solutions to support intelligent behaviours in UAV-enabled networks [10]. Moreover, a branch of machine learning called RL has shown the capacity to address problems in UAV-enabled networks [8], [10] ­ [12]. The goal of RL is to learn good policies for sequential decision problems, by optimizing a cumulative future reward signal [13]. A deep reinforcement learning (DRL) approach was proposed in [8] and [14] to control a fleet of UAV-BSs, providing coverage while maintaining connectivity with each other and minimizing their energy consumption. A clusteringbased Q-learning algorithm was proposed in [11] to optimize the deployment of multiple UAVs in a pre-partitioned area. However, these approaches considered ideal assumptions of complete knowledge of the locations of all ground devices in the network. That is, the central cloud controller periodically scans the entire network perimeter and sends updates to the UAVs at intervals for local decision-making.
In a dynamic network environment, this ideal assumption is impractical, as it requires significant information exchange between UAV and server. Moreover, it may not be possible to track user locations in a disaster scenario. In this work, we focus on the energy management of multiple UAVs deployed in a dynamic environment to provide wireless connectivity to both static and mobile ground devices in a given geographical space. The contribution of this paper is to propose a DQLSI algorithm that relies on local observations from the UAV and the broadcast

Fig. 1. UAV-BSs providing coverage to ground devices.

connectivity observation from neighbouring UAVs. Here, each UAV is equipped with an autonomous agent that maximizes the connectivity to both static and mobile devices while improving its energy utilization.

III. SYSTEM MODEL
We denote the set of single-antenna wireless end-devices located within a given geographical area as . These enddevices can either be static or mobile. The location of an end-device i   is given by (xi, yi). A set N of quadrotor UAVs are deployed as UAV-BSs within a rectangular area to provide downlink wireless service to ground end-devices, as shown in Figure 1. We consider a specific scenario where existing infrastructure is damaged or unavailable due to disaster, increased network demand or failure in certain parts of the network.

A. Wireless channel model

The location of a UAV serving as UAV-BS j  N is given by the 3D coordinates (xj, yj, hj). We assume guaranteed line-ofsight (LOS) conditions due to the aerial positions of the UAVBS. Each end-device in i   can be served by a single UAVBS in j  N which provides the strongest downlink signalto-interference-plus-noise-ratio (SINR) for the end-device such that i = argmaxjN ij and i  th, where ij is the downlink SINR between the end-device at i and the UAVBS j, and th is the threshold SINR required for successful wireless connectivity with the end-device. The expression for the SINR is given below [1]. The location of a UAV serving as UAV-BS j  N is given by the 3 coordinates (xj, yj, hj). We assume guaranteed LOS conditions due to the aerial positions of the UAV-BS. Each end-device in i   can be served by a single UAV-BS in j  N which provides the strongest downlink signal-to-interference-plus-noise-ratio (SINR) for the end-device such that i = argmaxjN ij and i  th, where ij is the downlink SINR between the end-device at i and the UAV-BS j, and th is the threshold SINR required for successful wireless connectivity with the end-device. The expression for the SINR is given below [1].

ij

=

Pj d-ij zint Pz d-iz

, + 2

(1)

where  and  are the attenuation factor and path loss

exponent that characterizes the wireless channel, respectively.

2 is the power of the additive white Gaussian noise at the receiver, dij is the distance between the i and j, and expressed as dij = (xi - xj)2 + (yi - yj)2 + h2j . int is the set of interfering UAV-BSs within the coverage range. z is the location of other interfering UAV-BSs. Pj and Pz are the transmit power of the UAV-BS at j and the interfering UAV-BS at z. We model the mobility of end-devices using the RWM model [19], i.e., the end-devices may dynamically change their positions, and the UAV-BSs can adjust their locations to provide connectivity to end-devices. Our objective here is to conserve the energy of the UAV-BSs while maximizing the provided connectivity to both static and mobile end-devices in the network.

B. Connectivity model

Network connectivity implies the existence of at least one

communication path between any pair of devices [15]. On

the other hand, coverage is the area around a UAV-BS where

the SINR  exceeds a pre-defined threshold th. In this

interference-limited system, coverage is affected by the SINR.

We compute the connection score of a UAV-BS in location j

at time t as,

Ctj =

wtj (i),

(2)

i

where wtj(i)  [0, 1] denotes whether device i is connected to UAV j at time t. wtj(i) = 1 if i = ij > th, otherwise wtj(i) = 0.

C. Energy consumption model

UAV-BSs consume energy when they are in operation, with an energy cost eT . By this, the total energy consumption of a UAV-BS to complete a task is the sum of propulsion energy consumption and communication energy consumption.

eT = eP + eC ,

(3)

where eP is the propulsion energy to support the UAV-BSs' transition from one point to another and hovering in the air, and eC is the communication energy for wireless signal processing and data transmission. It is stated in [16] that eC is practically much smaller than eP , i.e., eC eP . Hence in this work, we ignore energy consumption from the circuits for signal processing such as channel decoders and analog-todigital converters. A closed-form analytical propulsion power consumption model for a rotary-wing UAV-BS flying at a time t is given as [2],

3V 2 P (t) = 0 1 + Ut2ip + i

V4 V2 1 + 4v04 + 2v02

1 2

+

 sAV 3

2

(4)

where 0 and i are physical constants of the UAV-BS flight

environment (e.g., rotor radius or weight), Utip is the tip speed

of the rotor blade, v0 is the mean rotor induced velocity during

hovering,  is the fuselage drag ratio, s rotor solidity, A is

the rotor disc area, V is the UAV-BSs' speed and  is the air

density. In particular, we take into account the basic operations

of the UAV-BS, such as, hovering and acceleration. Thus, we

can obtain the average propulsion power over T time-steps

as

1 T

T t=1

P (t),

and

the

total

consumed

propulsion

energy

is

given as,

T

eP = t P (t),

(5)

t=1

where t is the duration of each time step.

IV. DECENTRALIZED Q-LEARNING WITH LOCAL SENSORY INFORMATION (DQLSI)
Deploying multiple UAV-BSs in a dynamic and quicklyevolving network makes energy estimation and planning complex and difficult. As such, we propose a decentralized multiagent reinforcement learning-based approach [17] to improve the energy utilization of multiple UAV-BSs, while maximizing the connectivity of both static and mobile ground enddevices. We consider a framework of cooperative multi-agent system (MAS), where agents learn locally and independently. In such environments, the agents share the common interest of maximizing wireless connectivity while improving the energy utilization within the network. In this work, we focus on agents with local observability called independent learners (IL), since the assumption of joint action observability is unrealistic without central/global knowledge [17]. The Q-learning update for agent i is given as,

Qi(si, ai)  (1-)Qi(si, ai)+

ri

+

max
ai

Qi

(si

,

ai

)

,

(6)

where si is the present local state observed by agent i, si is the new local state observed by agent i, ai is the action taken by agent i, ri is the reward received by agent i in that time step,  is the learning rate and   [0, 1] is the discount factor.
In our multi-UAV-BS system, we consider a DQLSI algorithm as seen in Algorithm 1 with strict local observability suitable when there is limited or no access to cellular infrastructure due to disaster. We assume that each UAV-BS is equipped with an autonomous agent which takes an action and in turn, receives a reward and makes a transition to a new state. We explicitly define the states, actions, and reward function of our agent.

A. States
We consider a combination of the three-dimensional (3D) position of the UAV-BS [11] and the distance from neighbouring UAV-BSs. This helps to inform the agent of its position and that of neighbouring agents in each time-step. The agent observes its connectivity to ground devices, energy consumption, and the geographical ground area covered at each position in space. In addition, each agent can locally observe the broadcast of its neighbour's connection score. However, the communication cost incurred by each sensory-exchanging agent per step is bounded by (N -1)×E (Refer to Case-study 2, [18]), where N is the number of UAV-BSs within that locality, E is the number of bits needed to represent the connection sensation observed

Algorithm 1 Decentralized Q-learning with Local Sensory

Information (DQLSI) for Agent i

1: Initialize:

2: for all a  Ai and s  S do: 3: Qi,max(s, a)  0, i(s, a) arbitrarily 4: s  initial state

5: 10000  maxStep

6: %% An episode ends when goal is Reached or

UAV-BS dies or maxStep is reached

7: while goal not Reached and Agent alive and maxStep not reached do

8:

s  MapLocalObservationToState(Env)

9:

%%From s select a according to -greedy

method based on i

10:

a  QLearning.SelectAction(s)

11:

%% AgentExecutesActionInState

12:

a.execute(Env)

13:

if a.execute(Env) is True then

14:

%% MapToNewState

15:

Env.UAV3Dposition [11]

16:

Observation.ConnectionScore (2)

17:

Observation.EnergyConsumed (5)

18:

Observation.GeographicalAreaCovered

19:

Env.UAVneighbourProximity

20:

Observation.ConnectionSensation

21:

%%Observe reward r and next state s

22:

UpdateQLearningProcedure() (6)

23: endwhile

metres. In the static setting, we deploy 400 randomly distributed static ground devices that are not uniformly distributed in the area. We deploy about 200 randomly distributed static ground devices and 200 mobile devices in the dynamic setting. The random walk mobility (RWM) model was used to simulate the unpredictable movement of ground devices in a campus setting [19]. We assume a maximum connection limit of 150 active ground devices per UAV-BS [1]. We consider the cluster-based Q-learning (CQL) [11], iterative search (IS) [1], and exhaustive (brute-force) search (ES) as baselines to our proposed DQLSI algorithm. Using the same energy parameters given in (5) [2] and [21], we consider the total energy consumed by the UAVBS over the range of time-steps T as a metric expressed in Joules. We account for the possibility of coverage overlap in multi-UAV-BS, hence, the number of ground devices connected to a UAV-BS expressed in percentage was used. Lastly, we observed the geographical area covered by UAV-BS, which estimates in square kilometres the actual ground coverage by all UAV-BSs. We used statistical results from 20 independent runs, gathering 5 episodes in each run of experiment.
A. Static setting

by an agent. The agent then discretizes the continuous state space emanating from the environment.

B. Actions
At each time-step t  T , each UAV-BS takes an action at  A. We discretize the agent's actions [11]: Up, Down, Forward, Backward, Left, Right and Stationary.

C. Reward

The goal of each learning agent is to learn a policy that maximizes the connectivity of ground devices while improving its energy utilization. The reward assigned to an agent i in each time-step t  T is given in as,


  

+ + 1, eit-1-eit
eit+ eit-1

if Cti > Cti-1

 Ri =

+ , eit-1-eit
eit+ eit-1

if Cti = Cti-1

(7)

   

+ - 1, eit-1-eit
eit+ eit-1

otherwise,

where Cti and Cti-1 are the connection score and eit and eit-1 are the average energy consumed by agent i in present and previous

time-step, respectively. is a cooperative factor used to evalu-

ate the connectivity among neighbouring UAV-BSs. = +1 if

Cto > Cto-1, otherwise

= -1, where Cko = Cki +

N -i

Ck-i

,

is the overall connectivity score in the locality, and

N -i

Ck-i

is

the connection score from neighbouring UAV-BSs in kth time-

step.

V. EVALUATION, RESULTS AND ANALYSIS
In our experiments, we consider settings with static and dynamic ground devices. In both, four UAV-BSs were deployed in a 1 km2 area, each with its initial starting point at the start of an episode. The mobility step size for each UAV-BS is 20

Figure 2 shows performance metrics of each agent when the proposed DQLSI algorithm is used in a static setting. As seen in Figure 2a, all four agents maximize their accumulated reward. After the 60th episode, we observe significant convergence in the energy consumed by the UAVs, within the range of about 26 kJ - 52 kJ as seen in Figure 2b. Figure 2c show a balance in the connection load across the four UAVs, ranging between 85 - 100 connected ground devices per UAV. The total number of connected ground devices for all UAVs ranges between 365 382. Figure 2d show convergence in the geographical area covered by the UAVs after the 40th episode.
B. Dynamic setting
Figure 3 and Figure 4 shows the performance metrics of each agent when the proposed DQLSI algorithm is applied in a uniformly distributed (DQLSI-D) and unevenly distributed (DQLSI-UD) dynamic setting, respectively. As seen in Figure 3a and Figure 4a, all four agents maximize their accumulated reward. Both Figure 3b and Figure 4b show convergence in the energy consumed by the UAVs after the 60th and 70th episode, within the range of 27 kJ - 66 kJ and 36 kJ - 67 kJ, respectively. Although in Figure 4b, Agent 4 experienced energy spikes of about 114 kJ and 101 kJ during the 86th and 96th episode, respectively. Despite the network dynamics, Figure 3c show balance in the connection load, however, Agent 1 covered slightly lesser number of devices than other UAVs in the network. In Figure 4c, due to the uneven distribution of ground devices, Agent 1 and Agent 3 connected to twice as many devices than Agent 2 and Agent 4, possibly influenced by their initial starting position. The total number of connected ground devices by all UAVs ranges between 347 - 365 and 343 - 360 in the DQLSI-D and DQLSI-UD settings, respectively. Figure

3d and Figure 4d show convergence in the geographical area covered by the UAVs after the 50th and 70th episode.

Energy consumed (Joules)

Norm. Accum. Reward

1

Agent-1

0.8

Agent-2

Agent-3

0.6

Agent-4

Convergence @ 26 kJ - 52 kJ
105

0.4

0.2

0

0

20 40 60 80 100

Episodes

(a) Accumulated rewards per agent for 100 learning episodes.

Connected ground devices

100 90 80 70 60 50 40 30 20 10 0 0

Agent-1 Agent-2 Agent-3 Agent-4
10 20 30 40 50 60 70 80 90 100 Episodes

(c) Number of connected ground device per agent in the network.

100 0

Agent-1 Agent-2 Agent-3 Agent-4
20 40 60 80 100 Episodes

(b) Energy consumed per agent.

1

Area covered (sq. km)

0.9

0.8

0.7

0.6

0.5 0

20 40 60 80 100

Episodes

(d) Geographical area covered by multi-UAVs.

Fig. 2. Four UAV-BSs providing connectivity in static environments with 400 randomly distributed static ground devices.

Norm. Accum. Reward

1

Agent-1

0.8

Agent-2

Agent-3

0.6

Agent-4

0.4

0.2

0

0

20 40 60 80 100

Episodes

(a) Accumulated rewards per agent for 100 learning episodes.

Connected ground devices

100 90 80 70 60 50 40 30 20 10 0 0

Agent-1 Agent-2 Agent-3 Agent-4
10 20 30 40 50 60 70 80 90 100 Episodes

(c) Number of connected ground device per agent in the network.

Energy consumed (Joules)

106

104
Convergence @ 27 kJ - 66 kJ
102

100 0

Agent-1 Agent-2

Agent-3 Agent-4

20 40 60 80 100 Episodes

(b) Energy consumed per agent.

1

Area covered (sq. km)

0.8

0.6

0.4

0.2 0

20 40 60 80 100

Episodes

(d) Geographical area by multi-UAVBS.

Fig. 3. Four UAV-BSs providing connectivity in dynamic environments with 200 randomly distributed static ground devices and 200 mobile RWM-based (random walk mobility model) devices.

C. Comparative analysis
We compare our proposed DQLSI approach with baselines. Figure 5 show the percentage of connected ground devices versus baseline approaches. In this figure, the exhaustive search method achieves the highest number of connected devices in both static (ES-S) and dynamic (ES-D) settings, with an average connection of about 96% and 93%, respectively. The exhaustive search method under the uneven and dynamic (ES-UD) setting

Energy consumed (Joules)

Norm. Accum. Reward

1

Agent-1

0.8

Agent-2

Agent-3

0.6

Agent-4

106

Convergence @

36 kJ - 67 kJ

104

0.4

0.2

0

0

20 40 60 80 100

Episodes

(a) Accumulated rewards per agent for 100 learning episodes.

Connected ground devices

140 130 120 110 100
90 80 70 60 50 40 30 20 10
0 0

Agent-1 Agent-2

Agent-3 Agent-4

10 20 30 40 50 60 70 80 90 100 Episodes

(c) Number of connected ground device per agent in the network.

102 100
0

Agent-1 Agent-2 Agent-3 Agent-4
20 40 60 80 100 Episodes

(b) Energy consumed per agent.

1

Area covered (sq. km)

0.8

0.6

0.4

0.2

0

20 40 60 80 100

Episodes

(d) Geographical area by multi-UAVBS.

Fig. 4. Four UAV-BSs providing connectivity in an uneven dynamic environments with 200 randomly distributed static ground devices and 200 mobile RWM-based (random walk mobility model) devices.

had the least average connection of about 92%. However, this method consumed the most energy in order of hundreds of kiloJoules as seen in Figure 6. This high energy cost is due to the exploration of all possible combinations of the search space by the UAVs. The proposed DQLSI approach performs well in both static (DQLSI-S), dynamic (DQLSI-D) and uneven dynamic (DQLSI-UD) settings with average connections of about 93%, 89% and 88%, respectively. Interestingly, the clusterbased Q-learning [11] approach performs well with average connections of about 89% in the static scenario, however, only about 33% of the average connections was achieved in the dynamic setting. This poor performance was due to the inability of the UAVs to effectively locate the quickly-changing centroids in real-time.

Connected Ground Devices (%)

100

90

80

70

60

50

40

30 Median

20 25% - 75%

9% - 91%

10 Outliers

+

DQLSDI-QSLDSQI-LDSI-UDCQL-CSQLC-QDL-UD IS-S IS-DIS-UDES-SES-EDS-UD

Fig. 5. Comparison of ground device connections in a 4-agent setting vs. baselines in both static and dynamic environments.

Similar to the exhaustive search method, the iterative search

method [1] performs poorly in terms of energy consumption as seen in Figure 6. Though it performs slightly better in the static case (IS-S) than in the dynamic case (IS-D) in the number of connected devices, with an average number of connections of 62% and 48%, respectively. From this figure, both learning-based approaches (DQLSI and CQL) perform well in minimizing the total energy consumed by UAV-BSs in the network, however, some outliers with poor energy utilization were encountered in their dynamic settings. The results in Figure 5 and Figure 6 show that our DQLSI approach is robust in simultaneously improving the number of connections and minimizing the total energy consumed by UAV-BSs in both static and dynamic environments.

104

Median 25% - 75%

9% - 91%

Outliers

+

103

Energy consumed (KJ)

DQLSDI-QSLDSQI-LDSI-UDCQL-CSQLC-DQL-UD IS-S IS-DIS-UDES-SES-EDS-UD
Fig. 6. Comparison of energy consumed in a 4-agent setting vs. baselines in both static and dynamic environments.
VI. CONCLUSION
This paper demonstrates that UAVs serving as UAV-BSs can learn via RL agents to cooperatively maximize the number of connected static and mobile ground devices while improving the energy utilization in the network. Unlike previous work, we do not rely on a central cloud controller with an unrealistic assumption of complete knowledge of the location of ground devices. The performance of the proposed DQLSI approach was compared to state-of-the-art centralized approaches under both static and dynamic network conditions. Our proposed approach is robust in simultaneously improving the number of connections and minimizing the total energy consumed by UAV-BSs in both static and dynamic environments.
We pose pertinent questions that are open to further investigations. What impact will different distributions and mobility patterns have on the performance of our proposed approach? Considering the impact of communication cost, can agents learn to communicate with minimal information? Finally, can other cooperative methods offer faster convergence under dynamic network conditions? These are directions for future work.
ACKNOWLEDGMENT
This work was supported by a research grant from Science Foundation Ireland (SFI) and the National Natural Science

Foundation Of China (NSFC) under the SFI-NSFC Partnership
Programme Grant Number 17/NSFC/5224, as well as SFI
Grants No. 16/SP/3804 and 13/RC/2077.
REFERENCES
[1] M. Mozaffari, W. Saad, M. Bennis and M. Debbah, "Mobile Unmanned Aerial Vehicles (UAVs) for Energy-Efficient Internet of Things Communications," in IEEE Transactions on Wireless Communications, vol. 16, no. 11, pp. 7574-7589, Nov. 2017.
[2] Y. Zeng, J. Xu and R. Zhang, "Energy Minimization for Wireless Communication With Rotary-Wing UAV," in IEEE Transactions on Wireless Communications, vol. 18, no. 4, pp. 2329-2345, April 2019,
[3] B. Galkin, J. Kibilda and L. A. DaSilva, "Coverage Analysis for LowAltitude UAV Networks in Urban Environments," GLOBECOM 2017 2017 IEEE Global Communications Conference, Singapore, 2017, pp. 16.
[4] R. I. Bor-Yaliniz, A. El-Keyi and H. Yanikomeroglu, "Efficient 3-D placement of an aerial base station in next generation cellular networks," 2016 IEEE International Conference on Communications (ICC), Kuala Lumpur, 2016, pp. 1-5,
[5] Q. Wu, Y. Zeng and R. Zhang, "Joint Trajectory and Communication Design for Multi-UAV Enabled Wireless Networks," in IEEE Transactions on Wireless Communications, vol. 17, no. 3, pp. 2109-2121, March 2018.
[6] J. Yao and N. Ansari, "QoS-Aware Power Control in Internet of Drones for Data Collection Service," in IEEE Transactions on Vehicular Technology, vol. 68, no. 7, pp. 6649-6656, July 2019.
[7] Z. Yang, W. Xu and M. Shikh-Bahaei, "Energy Efficient UAV Communication With Energy Harvesting," in IEEE Transactions on Vehicular Technology, vol. 69, no. 2, pp. 1913-1927, Feb. 2020.
[8] C. H. Liu, X. Ma, X. Gao and J. Tang, "Distributed Energy-Efficient Multi-UAV Navigation for Long-Term Communication Coverage by Deep Reinforcement Learning," in IEEE Transactions on Mobile Computing, vol. 19, no. 6, pp. 1274-1285.
[9] C. Prehofer and C. Bettstetter, "Self-organization in communication networks: principles and design paradigms," in IEEE Communications Magazine, vol. 43, no. 7, pp. 78-85, July 2005.
[10] J. Cui, Y. Liu and A. Nallanathan, "Multi-Agent Reinforcement LearningBased Resource Allocation for UAV Networks," in IEEE Transactions on Wireless Communications, vol. 19, no. 2, pp. 729-743, Feb. 2020.
[11] X. Liu, Y. Liu and Y. Chen, "Reinforcement Learning in Multiple-UAV Networks: Deployment and Movement Design," in IEEE Transactions on Vehicular Technology, vol. 68, no. 8, pp. 8036-8049, Aug. 2019.
[12] B. Galkin, E. Fonseca, R. Amer, L. A. DaSilva, and I. Dusparic, "REQIBA: Regression and Deep Q-Learning for Intelligent UAV Cellular User to Base Station Association", Arxiv 2020.
[13] R. S. Sutton and A. G. Barto, Introduction to Reinforcement Learning, 1st ed. Cambridge, MA, USA: MIT Press, 1998.
[14] C. H. Liu, Z. Chen, J. Tang, J. Xu and C. Piao, "Energy-Efficient UAV Control for Effective and Fair Communication Coverage: A Deep Reinforcement Learning Approach," in IEEE Journal on Selected Areas in Communications, vol. 36, no. 9, pp. 2059-2070, Sept. 2018.
[15] H. M., Ammari and S. K., Das, (2006). "Coverage, Connectivity, and Fault Tolerance Measures of Wireless Sensor Networks," in Stabilization, Safety, and Security of Distributed Systems, 2006, pp. 35­49.
[16] S. Eom, H. Lee, J. Park and I. Lee, "UAV-Aided Wireless Communication Designs With Propulsion Energy Limitations," in IEEE Transactions on Vehicular Technology, vol. 69, no. 1, pp. 651-662.
[17] L. Busoniu, R. Babuska and B. De Schutter, "A Comprehensive Survey of Multiagent Reinforcement Learning," in IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), vol. 38, no. 2, pp. 156-172, March 2008.
[18] T. Ming, "Multi-Agent Reinforcement Learning: Independent versus Cooperative Agents," Proceedings of the Tenth International Conference on Machine Learning (ICML 1993), San Francisco, CA, USA, pp. 330­337.
[19] T.Camp, J.Boleng, V. Davies, "A Survey of Mobility Models for Ad Hoc Network Research" on Wireless Communication & Mobile Computing (WCMC): Special issue on Mobile Ad Hoc Networking: Research, Trends and Applications, 2002, pp. 483-502.
[20] P. J. Hoen, K. Tuyls, L. Panait, S. Luke, and J. A. La Poutre´, "An Overview of Cooperative and Competitive Multiagent Learning,"SpringerVerlag, Berlin, Heidelberg, 2005.

[21] M. Hwang, H.-R. Cha, and S. Y. Jung, "Practical Endurance Estimation for Minimizing Energy Consumption of Multirotor Unmanned Aerial Vehicles," Energies, vol. 11, no. 9, p. 2221, Aug. 2018.

