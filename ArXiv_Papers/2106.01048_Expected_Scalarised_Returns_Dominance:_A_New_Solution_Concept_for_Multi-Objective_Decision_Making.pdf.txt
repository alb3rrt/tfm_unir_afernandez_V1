Expected Scalarised Returns Dominance: A New Solution Concept for Multi-Objective Decision Making*
Conor F. Hayes · Timothy Verstraeten · Diederik M. Roijers · Enda Howley · Patrick Mannion

arXiv:2106.01048v1 [cs.LG] 2 Jun 2021

Abstract In many real-world scenarios, the utility of a user is derived from the single execution of a policy. In this case, to apply multi-objective reinforcement learning, the expected utility of the returns must be optimised. Various scenarios exist where a user's preferences over objectives (also known as the utility function) are unknown or difficult to specify. In such scenarios, a set of optimal policies must be learned. However, settings where the expected utility must be maximised have been largely overlooked by the multi-objective reinforcement learning community and, as a consequence, a set of optimal solutions has yet to be defined. In this paper we address this challenge by proposing first-order stochastic dominance as a criterion to build solution sets to maximise expected utility. We also propose a new dominance criterion, known as expected scalarised returns (ESR) dominance, that extends firstorder stochastic dominance to allow a set of optimal policies to be learned in practice. We then define a new solution concept called the ESR set, which is a set of policies that are ESR dominant. Finally, we define a new multi-objective distributional tabular reinforcement learning (MOT-DRL) algorithm to learn the ESR set in a multi-objective multi-armed bandit setting.

* An earlier version of this work was presented at the Adaptive and Learning Agents Workshop 2021 [18]. This article extends our workshop paper with additional theoretical analysis and new empirical results.

Conor F. Hayes, Enda Howley, Patrick Mannion School of Computer Science National University of Ireland Galway Galway, Ireland E-mail: c.hayes13@nuigalway.ie E-mail: enda.howley@nuigalway.ie E-mail: patrickmannion@nuigalway.ie

Timothy Verstraeten AI Lab, Vrije Universiteit Brussel Belgium E-mail: timothy.verstraeten@vub.be
Diederik M. Roijers Vrije Universiteit Brussel, Belgium & HU University of Applied Sciences Utrecht, the Netherlands E-mail: diederik.yamamoto-roijers@hu.nl

2

Hayes et al.

Keywords Multi-objective · Decision making · Distributional · Reinforcement learning · Stochastic dominance

1 Introduction
In multi-objective reinforcement learning (MORL) there are two classes of algorithms: single-policy and multi-policy [38, 31]. Each MORL algorithm has two phases: the learning phase and the execution phase [31]. When using single-policy methods, an agent learns a single optimal policy that maximises a user's utility function where a utility function represents a user's preferences over objectives. The agent then executes the optimal policy during the execution phase. Single-policy methods require the utility function of a user to be known during the learning phase. In certain scenarios a user's preferences over objectives may be unknown; therefore, the utility function is unknown. In this case, a user is said to be in the unknown utility function or unknown weights scenario [31]. In the unknown utility function scenario, multi-policy methods must be used to learn a set of optimal policies during the learning phase. We assume that the utility function of the user will become known during the execution phase. Once the utility function of the user is known, it is possible to select a policy, from the set of learned policies, that will maximise the user's utility function.
In contrast to single-objective reinforcement learning (RL), multiple optimality criteria exist for MORL [31]. In scenarios where the utility of the user is derived from multiple executions of a policy, the scalarised expected returns (SER) must be optimised. However, in scenarios where the utility of a user is derived from a single execution of a policy, the expected utility of the returns (or expected scalarised returns, ESR) must be optimised. The majority of MORL research focuses on the SER criterion and linear utility functions [27], which limits the applicability of MORL to real-world problems. In the real world, a user's utility function may be derived in a linear or non-linear manner. For known linear utility functions, single-objective methods can be used to learn an optimal policy [31]. Non-linear utility functions do not distribute across the sums of the immediate and future returns, which invalidates the Bellman equation [30]. Therefore, to learn optimal policies for non-linear utility functions, strictly multi-objective methods must be used.
For non-linear utility functions, a user can prefer significantly different policies depending on whether the SER or ESR criterion is optimised [27, 28]. Unfortunately, the ESR criterion has received very little attention, to date, from the MORL community. To learn optimal policies in many real-world scenarios where a policy will be executed only once, the ESR criterion must be optimised. For example, in a medical setting where a user has one opportunity to select a treatment, a user will want to maximise the expected utility of a single outcome. However, choosing the wrong optimisation criterion (SER) for such a scenario could potentially lead to a different policy than that which

Expected Scalarised Returns Dominance

3

would be learned under ESR. In the real world, like in the aforementioned scenario, learning a sub-optimal policy could have catastrophic outcomes.
Therefore, it is crucial that the MORL community focuses on developing both single-policy and multi-policy methods that can learn optimal policies under the ESR criterion. Recently, a number of single-policy methods have been implemented that can learn optimal policies under the ESR criterion [30, 17]. Based on the findings of Hayes et al. [17, 16], a distribution over the expected utility of the returns must be used to learn an optimal policy under the ESR criterion in realistic settings where rewards are stochastic1. Traditionally, a single expected value of the returns is used to make decisions. However, the expected value cannot account for the range of positive or adverse effects a decision might have [17]. In the current MORL literature, no multi-policy methods exist for the ESR criterion. In fact, a set of optimal policies for the ESR criterion has yet to be defined.
This paper aims to fill the aforementioned research gaps that exist for the ESR criterion. Due to the lack of existing research for the ESR criterion, a formal definition of the requirements to satisfy the ESR criterion has yet to be determined. In Section 3, we define the requirements necessary to satisfy the ESR criterion. The applicability of MORL to many real-world scenarios under the ESR criterion is limited because no solution set has been defined for scenarios when a user's utility function is unknown. In Section 4, we show how first-order stochastic dominance can be used to define sets of optimal policies under the ESR criterion. However, when a user's utility function is unknown, using FSD to determine a set of optimal policies in practice is difficult because FSD relies on the utility function of a user to be known. We address this challenge in Section 5 and expand first-order stochastic dominance to define a new dominance criterion, called expected scalarised returns (ESR) dominance. This work proposes that ESR dominance can be used to learn a set of optimal solutions, which we define as the ESR set. Finally, we present a novel multiobjective distributional tabular reinforcement learning algorithm (MOTDRL) which aims to learn the ESR set in scenarios when the utility function of the user is unknown. We apply MOTDRL to two different multi-objective multiarmed bandit settings where MOTDRL is able to learn the ESR set in both settings.

2 Background
2.1 Multi-Objective Reinforcement Learning
In multi-objective reinforcement learning [15], we deal with decision making problems with multiple objectives, often modelled as a multi-objective Markov decision process. An MOMDP represents a tuple, M = (S, A, T, , R), where S
1 We note that distributional methods also work well for simple problems with deterministic rewards. In such cases, the value distribution only has a single value vector per state-action pair that occurs with probability 1.0.

4

Hayes et al.

and A are the state and action spaces, T : S × A × S  [0, 1] is a probabilistic transition function,  is a discount factor determining the importance of future rewards and R : S × A × S  Rn is an n-dimensional vector-valued immediate reward function. In multi-objective reinforcement learning, n > 1.

2.2 Multi-Objective Multi-Armed Bandits
Multi-objective multi-armed bandits (MOMAB) [11] are a natural extension of multi-armed bandits, where each arm returns an n-dimensional reward vector Rn, where n is the number of objectives. At each timestep, t, the agent must select an arm, i, and receives a reward vector. The returns in an MOMAB setting can be deterministic [11] or stochastic [3]. Many algorithms focus on the MOMAB setting and learn a set of arms that are optimal [11, 46, 33, 43].
For example, Pareto UCB-1 [11] is an algorithm that can learn a set of optimal policies in an MOMAB setting. Pareto UCB-1 [11] initially plays each arm once, then at each timestep the algorithm computes the mean vector of each of the multi-objective arms and adds the upper confidence bound to the mean return vector. Using this method Pareto UCB-1, can learn the Pareto front in an MOMAB setting.

2.3 Utility Functions

In MORL, utility functions are used to model a user's preferences, and are used

in both single-objective and multi-objective RL. Utility functions are functions

that map returns to a scalar value which represents the user's preferences over

the returns,

u : Rn  R,

(1)

where u is a utility function and Rn is an n-dimensional vector. Linear utility

functions are widely used to represent a user's preferences,

n

u = wiri,

(2)

i=1

where wi is the preference weight and ri is the value at position i of the return vector. However, certain scenarios exist where linear utility functions cannot

accurately represent a user's preferences. In this case, the user's preferences

must be represented using a non-linear utility function.

In this paper, we consider monotonically increasing utility functions [31],

i.e.,

( i, Vi  Vi   i, Vi > Vi ) = ( u, u(V) > u(V )),

(3)

where V and V are the values of executing policies  and  respectively.

A monotonically increasing utility function includes linear utility functions

of the form in Equation 2. It is important to note that in certain scenarios the

utility function may be unknown, therefore we do not know the shape of the

Expected Scalarised Returns Dominance

5

utility function. If we assume the utility function is monotonically increasing we know that, if the value of one of the objectives in the return vector increases, then the utility also increases [31]. This assumption makes it possible to determine an ordering over policies when the shape of the utility function is unknown.

2.4 Scalarised Expected Returns and Expected Scalarised Returns

For MORL, the ability to express a user's preferences over objectives as a utility function is essential when learning a single optimal policy. In MORL, different optimality criteria exist [31]. Additionally, the utility function can be applied to the expectation of the returns, or the utility function can be applied directly to the returns before computing the expectation. Calculating the expected value of the return of a policy before applying the utility function leads to the scalarised expected returns (SER) optimisation criterion:



Vu = u E

trt , µ0 ,

(4)

t=0

where µ0 is the probability distribution over possible starting states.

SER is the most commonly used criterion in the multi-objective (single

agent) planning and reinforcement learning literature [31]. For SER, a coverage

set is defined as a set of optimal solutions for all possible utility functions. If

the utility function is instead applied before computing the expectation, this

leads to the expected scalarised returns (ESR) optimisation criterion [30, 17,

31]:



Vu = E u

trt , µ0 .

(5)

t=0

ESR is the most commonly used criterion in the game theory literature on multi-objective games [27].

2.5 Stochastic Dominance
Stochastic dominance [14, 4] gives a partial order between distributions and can be used when making decisions under uncertainty. Stochastic dominance is particularly useful when a distribution must be taken into consideration rather than an expected value when making decisions. Stochastic dominance is a prominent dominance criterion in finance, economics and decision theory. When making decisions under uncertainty, stochastic dominance can be used to determine the most risk averse decision. Various degrees of stochastic dominance exist, however, in this paper we focus on first-order stochastic dominance (FSD). FSD can be used to give a partial ordering over random variables or random vectors to give an FSD dominant set.

6

Hayes et al.

1

0.8

Probability

0.6

0.4

0.2

FX

0

FY

0

2

4

6

8

10

Utility

Fig. 1: For random variables X and Y , X F SD Y , where FX and FY are the cumulative distribution functions (CDFs) of X and Y respectively. In this case, X is preferable to Y because higher utilities occur with greater frequency in FX .

In Definition 1 we present the necessary conditions for FSD and in Theorem 1 we prove that if a random variable is FSD dominant it has at least as high an expected value as another random variable [42]. We use the work of Wolfstetter [42] to prove Theorem 1.

Definition 1 For random variables X and Y, X F SD Y if:

P (X > z)  P (Y > z),  z

If we consider the cumulative distribution function (CDF) of X, FX , and the CDF of Y, FY , we can say that X F SD Y if:
FX (z)  FY (z),  z.

Theorem 1 If X F SD Y, then X has a greater than or equal expected value as Y.
X F SD Y = E(X)  E(Y ).

Proof By a known property of expected values the following is true for any

random variable:
+

E(X) =

(1 - FX (x)) dx

0

+

E(Y ) =

(1 - FY (x)) dx

0

Therefore, if X F SD Y then:

+

+

(1 - FX (x)) dx 

(1 - FY (x)) dx

0

0

Expected Scalarised Returns Dominance

7

Which gives, [42]

E(X)  E(Y ).

3 Expected Scalarised Returns

In contrast to single-objective reinforcement learning, different optimality criteria exist for MORL. In scenarios where the utility of a user is derived from multiple executions of a policy, the agent should optimise over the SER criterion. In scenarios where the utility of a user is derived from a single execution of a policy, the agent should optimise over the ESR criterion. Let us consider, as an example, a power plant that generates electricity for a city and emits harmful CO2 and greenhouse gases. City regulations have been imposed which limit the amount of pollution that the power plant can generate. If the regulations require that the emissions from the power plant do not exceed a certain amount over an entire year, the SER criterion should be optimised. In this scenario, the regulations allow for the pollution to vary day to day, as long as the emissions do not exceed the regulated level for a given year. However, if the regulations are much stricter and the power plant is fined every day it exceeds a certain level of pollution, it is beneficial to optimise under the ESR criterion.
The majority of MORL research focuses on linear utility functions. However, in the real world, a user's utility function can be non-linear. For example, a utility function is non-linear in situations where a minimum value must be achieved on each objective [25]. Focusing on linear utility functions limits the applicability of MORL in real-world decision making problems. For example, linear utility functions cannot be used to learn policies in concave regions of the Pareto front [40]. Furthermore, if a user's preferences are non-linear, these are fundamentally incompatible with linear utility functions. In this case, strictly multi-objective methods must be used to learn optimal policies for non-linear utility functions. In MORL, for non-linear utility functions, significantly different policies are preferred when optimising under the ESR criterion versus the SER criterion [28]. It is important to note that, for linear utility functions, the distinction between ESR and SER does not exist [27].
For example, a decision maker has to choose between the following lotteries, L1 and L2, which are highlighted in Table 1.
The decision maker has the following non-linear utility function:

u(x) = x21 + x22,

(6)

where x is a vector returned from R in Table 1, and x1 and x2 are the values of two objectives. Note that this utility function is monotonically increasing for x1  0 and x2  0. Under the SER criterion, the decision maker will compute the expected value of each lottery, apply the utility function, and select the lottery that maximises their utility function. Let us consider which lottery the decision maker will play under the SER criterion:

8

Hayes et al.

L1 P(L1= R)
0.5 0.5

R (4, 3) (2, 3)

L2 P(L2=R)
0.9 0.1

R (1, 3) (10, 2)

Table 1: A lottery, L1, has two possible returns, (4, 3) and (2, 3), each with a probability of 0.5. A lottery, L2, has two possible returns, (1, 3) with a probability of 0.9 and (10, 2) with a probability of 0.1.

L1 : E(L1) = 0.5(4, 3) + 0.5(2, 3) = (2, 1.5) + (1, 1.5) L1 : u(E(L1)) = (22 + 1.52) + (12 + 1.52) = 6.25 + 3.25 = 9.5
L2 : E(L2) = 0.9(1, 3) + 0.1(10, 2) = (0.9, 2.7) + (1, 0.2) L2 : u(E(L2)) = (0.92 + 2.72) + (12 + 0.22) = 8.1 + 1.04 = 9.14
Therefore, a decision maker with the utility function in Equation 6 will prefer to play lottery L1 under the SER criterion.
Under the ESR criterion, the decision maker will first apply the utility function to the return vectors, compute the expectation, and select the lottery to maximise their utility function. Let us consider how a decision maker will choose which lottery to play under the ESR criterion:
L1 : u(L1) = u(4, 3) + u(2, 3) = (42 + 32) + (22 + 32) = (25) + (13)
L1 : E(u(L1)) = 0.5(25) + 0.5(13) = 12.5 + 6.5 = 19 L2 : u(L2) = u(1, 3) + u(10, 2) = (12 + 32) + (102 + 22) = (10) + (104)
L2 : E(u(L2)) = 0.9(10) + 0.1(104) = 9 + 10.4 = 19.4
Therefore, a decision maker with the utility function in Equation 6 will prefer to play lottery L2 under the ESR criterion. From the example, it is clear that users with the same non-linear utility function can prefer different policies, depending on which multi-objective optimisation criterion is selected. Therefore, it is critical that the distinction ESR and SER is taken into consideration when selecting a MORL algorithm to learn optimal policies in a given scenario. The majority of MORL research focuses on the SER criterion [27]. By comparison, the ESR criterion has received very little attention from the MORL community [31, 17, 30, 27]. Many of the traditional MORL methods cannot be used when optimising under the ESR criterion. The fact that non-linear utility functions in MOMDPs do not distribute across the sum of immediate and future returns invalidates the Bellman equation [30],



max E u


R-t +

iri

, st =

i=t

(7)



u(R-t )

+

max


E

u

iri

i=t

, st ,

Expected Scalarised Returns Dominance

9

L3 P(L3= R)
0.5 0.5

R (-20, 1) (20, 3)

L4 P(L4=R)
0.9 0.1

R (0, 2) (10, 2)

Table 2: A lottery, L3, has two possible returns, (-20, 1) and (20, 3), each with a probability of 0.5. A lottery, L4, has two possible returns, (0, 2) with a probability of 0.9 and (10, 2) with a probability of 0.1.

where u is a non-linear utility function and R-t =

t-1 i=0

iri.

Hayes et al. [17] implement a distributional Monte Carlo tree search (DM-

CTS) algorithm, which learns a posterior distribution over the expected utility

of individual policy executions. DMCTS achieves state-of-the-art performance

under the ESR criterion. Hayes et al. [17] demonstrate that, when optimising

under the ESR criterion, making decisions based on a distribution over the

expected utility of the returns is crucial to learn optimal policies in realistic

problems where rewards are stochastic. Traditional RL approaches use the

expected value of the future returns to make decisions. The expected value

cannot provide the agent with sufficient critical information to avoid adverse

outcomes and exploit positive outcomes when making a decision [17].

To understand why it is critical to make decisions when optimising under

the ESR criterion using a distribution over the expected utility of the returns,

let us consider the following example in Table 2 regarding a human decision

maker.

The decision maker has the following non-linear utility function:

u(x) = x1 + x22

(8)

where x is a vector returned from R in Table 2, and x1 and x2 are the values of two objectives. Note that this utility function is monotonically increasing for all values of x1 and for values of x2  0.
For the non-linear utility function in Equation 8, under the ESR criterion, both L3 and L4 have the same expected utility value of 5. It is important to note if an agent plays lottery L3, there is 0.5 chance of receiving a utility of -19 and a 0.5 chance of receiving a utility of 29. For a human decision maker, receiving a utility of 29 is an ideal outcome. However, receiving a utility of -19 might represent a severely negative outcome that the decision maker would want to avoid, e.g. going into debt. Instead, the decision maker may prefer lottery L4. As shown in this example, it is crucial that a distribution over the expected utility of the returns is used when making decisions under the ESR criterion.
The current MORL literature on the ESR criterion assumes a scalar expected utility (see Section 2.4) [17, 31, 30, 27]. As demonstrated above, using a single expected value to make decisions under the ESR criterion is not sufficient to avoid choosing policies with undesirable outcomes. Therefore, it is necessary to adopt a distributional approach to ESR problems.

10

Hayes et al.

Firstly, we introduce a multi-objective version of the value distribution [7], Z. A value distribution, Z, is equivalent to a multivariate distribution where a dimension exists per objective. The value distribution, Z, gives the
distribution over returns of a random vector [37] when a policy  is executed,
such that,



E Z = E

trt , µ0 .

(9)

t=0

Moreover, a value distribution can be used to represent policies. Under the ESR
criterion, the utility of the value distribution, Zu, is defined as a distribution over the scalar utilities received from applying the utility function to each vector in the value distribution, Z. Therefore, Zu is a distribution over the scalar utility of vector returns of a random vector received from executing a

policy , such that,



E Zu = E u

trt

t=0

, µ0 .

(10)

The utility of the value distribution can only be calculated when the utility function is known a priori.
In the examples used in Section 3, the utility function of the user is known. However, many scenarios exist where the user's utility function is unknown at the time of learning [31]. In this scenario, a set of policies that are optimal for all monotonically increasing utility functions must be learned. However, for the ESR criterion, a set of optimal solutions has yet to be defined. To learn a set of optimal policies under the ESR criterion we must develop new methods.
To address this challenge, in Section 4 we apply first-order stochastic dominance to determine a partial ordering over value distributions to satisfy the ESR criterion.

4 Stochastic Dominance for ESR
For MORL there are two classes of algorithms: single-policy and multi-policy algorithms [38, 31]. When the user's utility function is known a priori, it is possible to use a single-policy algorithm [17, 30] to learn an optimal solution. However, when the user's utility function is unknown we aim to learn a set of policies that are optimal for all monotonically increasing utility functions. The current literature on the ESR criterion focuses only on scenarios where the utility function of a user is known [17, 30], overlooking scenarios where the utility function of a user is unknown. Moreover, a set of solutions under the ESR criterion for the unknown utility function scenario [31] has yet to be defined.
Various algorithms have been proposed to learn solution sets under the SER criterion (see Section 2.4), for example [41, 23, 32]. Under the SER criterion, multi-policy algorithms determine optimality by comparing policies based on

Expected Scalarised Returns Dominance

11

the utility of vector valued expectations (see Equation 4). In contrast, under the ESR criterion it is crucial to maintain a distribution over the utility of possible vector-valued outcomes. SER multi-policy algorithms cannot be used to learn optimal policies under the ESR criterion because they compute expected value vectors. It is necessary to develop new methods that can generate solution sets for the ESR criterion with unknown utilities. The development of methods that determine an optimal partial ordering over value distributions is a promising avenue to address this challenge.
First-order stochastic dominance (see Section 2.5) is a method which gives a partial ordering over random variables [42, 20]. FSD compares the cumulative distribution functions (CDFs) of the underlying probability distributions of random variables to determine optimality. To satisfy the ESR criterion, it is essential that the expected utility is maximised. To use FSD for the ESR criterion, we must show the FSD conditions presented in Section 2.5 also hold when optimising the expected utility for unknown monotonically increasing utility functions.
For the single-objective case, Theorem 2 proves for random variables X and Y, if X F SD Y, the expected utility of X is greater than, or equal to, the expected utility of Y for monotonically increasing utility functions. In Theorem 2, random variables X and Y are considered, and their corresponding CDFs FX , FY . The work of Mas-Colell et al. [22] is used as a foundation for Theorem 2.

Theorem 2 A random variable, X, is preferred to a random variable, Y, for all decision makers with a monotonically increasing utility function if, and only if, X F SD Y.

X F SD Y = E(u(X))  E(u(Y ))

Proof If X F SD Y, then2,

FX (z)  FY (z),  z

Since,


E(u(X)) = u(z)dFX (z)
-


E(u(Y )) = u(z)dFY (z)
-

When integrating both E(u(X)) and E(u(Y )) by parts, the following results is generated:



E(u(X)) = [u(z)FX (z)] - -

u (z)FX (z) dz

-

2 CDFs with lower probability values for a given z are preferable. Figure 1 explains why this is the case.

12

Hayes et al.



E(u(Y )) = [u(z)FY (z)] - -

u (z)FY (z) dz

-

Given FX (-) = FY (-) = 0 and FX () = FY () = 1, the first terms in E(u(X)) and E(u(Y )) are equal, and thus





E(u(X)) - E(u(Y )) = u (z)FY (z) dz - u (z)FX (z) dz

-

-

Since FX (z)  FY (z) and u (z)  0 for all monotonically increasing utility functions, then
E(u(X)) - E(u(Y ))  0

and thus,

E(u(X))  E(u(Y ))

A utility function maps an input (scalar or vector return) to an output (scalar

utility). Since the probability of receiving some utility is equal to the prob-

ability of receiving some return for a random variable, X, we can write the

following:

P (X > c) = P (u(X) > u(c)),

(11)

where c is a constant. Using the results shown in Theorem 2 and Equation 11, the FSD conditions highlighted in Section 2.5 can be rewritten to include monotonically increasing utility functions:

P (u(X) > u(z))  P (u(Y ) > u(z))

(12)

Definition 2 Let X and Y be random variables. X dominates Y for all decision makers with a monotonically increasing utility function if the following is true:

X F SD Y 

u : v : P (u(X) > u(v))  P (u(Y ) > u(v)).

In MORL, the return from the reward function is a vector, where each

element in the return vector represents an objective. To apply FSD to MORL

under the ESR criterion, random vectors must be considered. In this case, a

random vector (or multi-variate random variable) is a vector whose compo-

nents are scalar-valued random variables on the same probability space. For

simplicity, this paper focuses on the case in which a random vector has two

random variables, known as the bi-variate case. FSD conditions have been

proven to hold for random vectors with n random variables in the works of

Sriboonchitta et al. [36], Levhari et al. [19], Nakayama et al. [24] and Scarsini

[34]. In Theorem 3, the work of Atkinson and Bourguignon [2] is distilled into

a suitable Theorem for MORL. Theorem 3 highlights how the conditions for

FSD hold for random vectors while satisfying the ESR criterion for a monoton-

ically

increasing

utility

function,

u,

where

 2 u(x1 ,x2 )  x1  x2



0

[29].

It

is

important

to note Atikson and Bourguignon [2] have shown the conditions for FSD hold

Expected Scalarised Returns Dominance

13

for

random

vectors

for

utility

functions

where

 2 u(x1 ,x2 )  x1  x2



0.

We

plan

to

ex-

tend these conditions for MORL in a future work. In Theorem 3, X and Y are

random vectors where each random vector consists of two random variables,

X = [X1, X2] and Y = [Y1, Y2]. FX1X2 and FY1Y2 are the corresponding CDFs.

Theorem 3 Assume that u : R0 × R0  R0 is a monotonically in-

creasing

function,

with

 u(x1 ,x2 ) x1

 0,

 u(x1 ,x2 ) x2

0

and

 2 u(x1 ,x2 )  x1  x2

 0.

If,

for

random vectors X and Y, X F SD Y, then X is preferred to Y by all decision

makers, i.e.,

X F SD Y = E(u(X))  E(u(Y))

Proof As X F SD Y, t, z we have

FX(t, z)  FY(t, z), or F (t, z) = FX(t, z) - FY(t, z)  0.

The expected utility of the random variable X can be written as follows:



E (u(X)) =

u(t, z)fX(t, z)dtdz,

00

where f is the probability density function of X. Note that

f (t, z) = fX(t, z) - fY(t, z) = 2F (t, z) . tz

Using

integration-by-parts

(I),

and

the

fact

that

F (t, 0)

=

F (0,z) z

=

0

(Z), we obtain:

E (u(X)) - E (u(Y))



=

u(t, z)f (t, z)dtdz

00

(=I)  u(t, z) F (t, z)  dz -   u(t, z) F (t, z) dtdz

0

z

t=0

00

t

z

(=I )

 0

u(t, z) F (t, z)  dz -

z

t=0

 0

u(t, z)



t

F (t, z) dt+
z=0

  2u(t, z)

00

tz F (t, z)dtdz

(Z)
=

 lim u(t, z) F (t, z) dz -

0 t

z



u(t, z)

lim
0 z

t

F (t, z)dt+

  2u(t, z) 0 0 tz F (t, z)dtdz.

14

Hayes et al.

Given

that

 2 u(t,z ) tz



0,

u(t,z) t



0

and

F (t, z)



0,

we

know

that

the

last two terms are positive. Therefore, we can state that

E (u(X)) - E (u(Y))

=

 lim u(t, z) F (t, z) dz -

0 t

z



u(t, z)

lim
0 z

t

F (t, z)dt+

 0

 2u(t, z) 0 tz F (t, z)dtdz 

 lim u(t, z) F (t, z) dz.

0 t

z

According to Lemma 2 (see Section 10), as u(t, z)F (t, z) is a positive monotonically increasing function in both t and z, we know that:



F (t, z)



F (t, z)

lim u(t, z)

dz = lim u(t, z)

dz.

0 t

z

t 0

z

Using integration-by-parts (I), and the fact that F (t, 0) = 0 (Z), we have:

E (u(X)) - E (u(Y))

 lim  u(t, z) F (t, z) dz

t 0

z

(=I )

lim
t

[u(t,

z)F

(t,

z)] 0

-

lim
t

 u(t, z)

0

z F (t, z)dz

(Z )

 u(t, z)

= lim lim u(t, z)F (t, z) - lim

t z

t 0

z F (t, z)dz.

Finally,

given

that

u(t,z) z



0

and

F (t, z)



0,

we

know

that:

E (u(X)) - E (u(Y))

 u(t, z)



lim
t

lim
z

u(t,

z)F

(t,

z)

-

lim
t

0

z F (t, z)dz

0

Using the results from Theorem 3, Equation 12 can be updated to include random vectors,

P (u(X) > u(z))  P (u(Y) > u(z)).

(13)

Definition 3 For random vectors X and Y, X is preferred over Y by all decision makers with a monotonically increasing utility function if, and only if, the following is true:
X F SD Y 
u : (v : P (u(X) > u(v))  P (u(Y) > u(v))

Expected Scalarised Returns Dominance

15

Using the results from Theorem 3 and Definition 3, it is possible to extend FSD to MORL. For MORL, under the ESR criterion, the value distribution, Z, is considered to be the full distribution of the returns of a random vector received when executing a policy,  (see Section 3). Value distributions can be used to represent policies. In this case, it is possible to use FSD to obtain a partial ordering over policies. For example, consider two policies,  and  , where each policy has the underlying value distribution Z and Z . If Z F SD Z then  will be preferred over  .
Definition 4 Policies  and  have value distributions Z and Z . Policy  is preferred over policy  by all decision makers with a utility function, u, that is monotonically increasing if, and only if, the following is true:
Z F SD Z .
Now that a partial ordering over policies has been defined under the ESR criterion for the unknown utility function scenario, it is possible to define a set of optimal policies.

5 Solution Sets for ESR
Section 4 defines a partial ordering over policies under the ESR criterion for unknown utility using FSD. In the unknown utility function scenario, it is infeasible to learn a single optimal policy [31]. When a user's utility function is unknown, multi-policy MORL algorithms must be used to learn a set of optimal policies. To apply MORL to the ESR criterion in scenarios with unknown utility, a set of optimal policies under the ESR criterion must be defined. In Section 5, FSD is used to define multiple sets of optimal policies for the ESR criterion.
Firstly, a set of optimal policies, known as the undominated set, is defined. The undominated set is defined using FSD, where each policy in the undominated set has an underlying value distribution that is FSD dominant. The undominated set contains at least one optimal policy for all possible monotonically increasing utility functions.
Definition 5 The undominated set, U (), is a sub-set of all possible policies for where there exists some utility function, u, where a policy's value distribution is FSD dominant.
U () =    u,    : Z F SD Z
However, the undominated set may contain excess policies. For example, under FSD, if two dominant policies have value distributions that are equal, then both policies will be in the undominated set. Given both value distributions are equal, a user with a monotonically increasing utility function will not prefer one policy over the other. In this case, both policies have the same expected

16

Hayes et al.

utility. To reduce the number of policies that must be considered at execution time, for each possible utility function we can keep just one corresponding FSD dominant policy; such a set of policies is called a coverage set (CS).
Definition 6 The coverage set, CS(), is a subset of the undominated set, U (), where, for every utility function, u, the set contains a policy that has a FSD dominant value distribution,
CS()  U ()  u,   CS(),    : Z F SD Z
In practice, for scenarios where the utility function is unknown, it is difficult to compute the undominated set or coverage set using FSD because FSD relies on having a user's utility function available to calculate dominance. To address this challenge, expected scalarised returns (ESR) dominance is defined. Multipolicy algorithms can use ESR dominance as a method under the ESR criterion to learn a set of optimal policies.
Definition 7 For random vectors X and Y, X >ESR Y for all decision makers with a monotonically increasing utility function if, and only if, the following is true:
X >ESR Y 
u : (v : P (u(X) > u(v))  P (u(Y) > u(v))
 v : P (u(X) > u(v)) > P (u(Y) > u(v))).
ESR dominance (Definition 7) extends FSD, however, ESR dominance is a more strict dominance criterion. For FSD, policies that have equal value distributions are considered dominant policies, which is not the case under ESR dominance. Therefore, if a random vector is ESR dominant, the random vector has a greater expected utility than all ESR dominated random vectors. Theorem 4 proves that ESR dominance satisfies the ESR criterion when the utility function of the user is unknown for all monotonically increasing utility functions. Theorem 4 focuses on random vectors X and Y where each random vector has two random variables, such that X = [X1, X2] and Y = [Y1, Y2]. FX and FY are the corresponding CDFs and v = [t, z]. However, Theorem 4 can easily be extended for random vectors with n random variables (X = [X1, X2, ..., Xn]).
Theorem 4 A random vector, X, is preferred to a random vector, Y, by all decision makers with a monotonically increasing utility function if, and only if, X ESR Y:
X >ESR Y = E(u(X)) > E(u(Y))
Proof X and Y are random vectors with n random variables. If X >ESR Y the following two conditions must be met for all u:
1. v : P (u(X) > u(v))  P (u(Y) > u(v)) 2.  v : P (u(X) > u(v)) > P (u(Y) > u(v))

Expected Scalarised Returns Dominance

17

From Definition 3, if X F SD Y then the following is true: u : v : P (u(X) > u(v))  P (u(Y) > u(v))

If X F SD Y, then, from Theorem 3, the following is true: E(u(X))  E(u(Y))

If condition 1 is satisfied, the expected utility of X is at least equal to the expected utility of Y, then:



E(u(X)) =

u(z)fX(t, z) dt dz

- -



E(u(Y)) =

u(z)fY(t, z) dt dz

- -

In order to satisfy condition 2, some limits must exist to give the following,

bd

bd

u(t, z)fX(t, z) dt dz >

u(t, z)fY(t, z) dt dz

ac

ac

The minimum requirement to satisfy condition 1 is:





u(t, z)fX(t, z) dt dz =

u(t, z)fY(t, z) dt dz

- -

- -

If condition 1 is satisfied, to satisfy condition 2 some limits must exist:

bd

bd

u(t, z)fX(t, z) dt dz >

u(t, z)fY(t, z) dt dz.

ac

ac

Therefore,

ac

bd

u(t, z)fX(t, z) dt dz +

u(t, z)fX(t, z) dt dz +

- -

ac



ac

u(t, z)fX(t, z) dt dz >

u(t, z)fY(t, z) dt dz +

bd

- -

Finally,

bd



u(t, z)fY(t, z) dt dz +

u(t, z)fY(t, z) dt dz.

ac

bd





u(t, z)fX(t, z) dt dz >

u(t, z)fY(t, z) dt dz

- -

- -

if X >ESR Y, then,

E(u(X)) > E(u(Y)).

18

Hayes et al.

In the ESR dominance criterion defined in Definition 7, the utility of different vectors is compared. However, it is not possible to calculate the utility of a vector when the utility function is unknown. In this case, Pareto dominance [26] can be used instead to determine the relative utility of the vectors being compared.

Definition 8 A Pareto dominates ( p) B if the following is true:

A p B  (i : Ai  Bi)  (i : Ai > Bi).

(14)

For monotonically increasing utility functions, if the value of an element of the vector increases, then the scalar utility of the vector also increases. Therefore, using Definition 8, if vector A Pareto dominates vector B, for a monotonically increasing utility function, A has a higher utility than B. To make ESR comparisons between value distributions, Pareto dominance can be used.
Definition 9 For random vectors X and Y, X >ESR Y for all monotonically increasing utility functions if, and only if, the following is true:

X >ESR Y 

v : P (X >P v)  P (Y >P v)  v : P (X >P v) > P (Y >P v).
It is also possible to calculate ESR dominance by comparing the CDFs of random vectors. Using the CDF guarantees a higher expected utility. Using the CDF we compare the cumulative probabilities for a given vector, where a lower cumulative probability is preferred. ESR dominance with the CDF does not require any information about the utility function of a user and therefore can be used in the unknown utility function scenario.

Definition 10 For random vectors X and Y, X >ESR Y for all monotonically increasing utility functions if, and only if, the following is true:

X >ESR Y 
v : FY(v)  FX(v)  v : FY(v) < FX(v).
Therefore, we can use either Definition 9 or Definition 10 to calculate ESR dominance to give a partial ordering over policies.
Definition 11 For value distributions Z and Z for policies  and  ,  is preferred over  by all decision makers with a monotonically increasing utility function if, and only if, the following is true:
Z >ESR Z
Using ESR dominance, it is possible to define a set of optimal policies, known as the ESR set.

Expected Scalarised Returns Dominance

19

Definition 12 The ESR set, ESR(), is a sub-set of all policies where each policy in the ESR set is ESR dominant,

ESR() = {   |    : Z >ESR Z}.

(15)

The ESR set is a set of non-dominated policies, where each policy in the ESR set is ESR dominant. The ESR set can be considered a coverage set, given no excess policies exist in the ESR set. It is viable for a multi-policy MORL method to use ESR dominance to construct the ESR set, given Pareto dominance is used to determine ESR dominance when the utility function of a user is unknown.

6 Multi-Objective Tabular Distributional Reinforcement Learning
Traditionally in the MORL literature, multi-objective methods learn a set of optimal solutions when the utility function of a user is unknown or hard to specify [31, 15]. The current MORL literature focuses only on methods which learn the optimal set of policies under the SER criterion [23, 41]. As already highlighted, the ESR criterion has largely been ignored by the MORL community, with a few exceptions [30, 17, 39]. In Section 6 we address this research gap and we present a novel multi-objective tabular distributional reinforcement learning (MOTDRL) algorithm that learns an optimal set of policies for the ESR criterion, also known as the ESR set, for multi-objective multi-armed bandit (MOMAB) problems.
MOTDRL learns the value distribution for a policy by sampling each available arm in a MOMAB setting and maintains a multivariate distribution over the returns received. Given MOTDRL only considers MOMAB problem domains, MOTDRL maintains a distribution per arm and updates the distribution after each timestep with the return vector received from executing the sampled arm. To satisfy the ESR criterion it is critical that a MORL method learns the underlying distribution over the returns. Other distributional MORL methods, such as bootstrap Thompson sampling [17], cannot be used to learn a set of optimal policies under the ESR criterion. Such methods learn a distribution over the mean returns. In scenarios where the utility function is unknown or unavailable, such methods would invalidate the ESR criterion as a distribution over mean return vectors would be computed. Given a distribution must be used when learning the ESR set, new distributional MORL methods must be formulated to learn the underlying value distributions.
MOTDRL can learn the underlying value distribution for an arm by maintaining a tabular representation of the underlying multivariate distribution. To maintain a tabular representation of a multivariate distribution we initialise a Z-table for each arm where the Z-table has an axis per objective. The Z-table maintains a count of the number of times a return vector is received for a given arm. The size of each Z-table is initialised using the parameters Vmin and Vmax which are the minimum and maximum values obtainable for any of

20

Hayes et al.

the objectives in the given environment. Therefore, each axis in the Z-table will use Vmin and Vmax to define the length of the axis, where each index value of the Z-table is initialised to 0. Using Vmin and Vmax as initialisation parameters, a Z-table can be constructed which contains an index for all possible return vectors in a given problem domain. Figure 2 visualises an initialised Z-table for a multi-objective problem with two objectives where Vmin = 0 and Vmax = 5.

Z
x1 = 0 x1 = 1 x1 = 2 x1 = 3 x1 = 4 x1 = 5

x2 = 0 0 0 0 0 0 0

x2 = 1 0 0 0 0 0 0

x2 = 2 0 0 0 0 0 0

x2 = 3 0 0 0 0 0 0

x2 = 4 0 0 0 0 0 0

x2 = 5 0 0 0 0 0 0

Fig. 2: An illustration of an initialised Z-table for a problem domain with two objectives, x1 and x2, with each index value set to 0.

Algorithm 1: Z-table Update
1 Input - arm, i 2 Require - Z-table for arm, i, Zi 3 Pull arm, i, and observe return, R. 4 Zi(R) = Zi(R) + 1 5 Ni = Ni + 1 6 return Z-table, Zi.
Each Z-table can be used to calculate the value distribution of an arm, which can be considered as a policy , Z (see Section 3). At each timestep, t, the returns, R, received from pulling arm, i, are used to update the Z-table. The Z-table is used to maintain a count of the number of times the return, R, is received. In MOMAB problem domains, the returns received from the execution of an arm represent the full returns of the execution of a policy. To update the Z-table, the value at the index corresponding to the return R is incremented by one. To correctly calculate the probability of receiving return R when pulling arm i, a counter, Ni, which represents the number of times arm i has been pulled, must be maintained. Each time arm i is pulled, the counter Ni is incremented by one. Algorithm 1 outlines how the Z-table for each arm is updated.
MOTDRL is a multi-policy algorithm that can learn the ESR set using ESR dominance. Using ESR dominance a partial ordering over policies can be determined when the utility function of a user is unknown. Algorithm 2 outlines how MOTDRL learns the ESR set when the utility function of a user in unknown in a MOMAB problem domain. In Algorithm 2 A is defined

Expected Scalarised Returns Dominance

21

as a set of available arms, the ESR set is defined as E, D is the number of
objectives, n is the total number of pulls across all arms, Nl and Ni are the number of pulls of arms j and i, and |E| is the cardinality of the ESR set,
which is known a priori.

Algorithm 2: Multi-Objective Tabular Distributional Reinforcement Learning

1 Pull each arm i in A,  times

2 Z-table Update(i)  i  A

3 repeat

4 Find E such that  i  E,  j





5

Zj +

2ln(n 4 D|E|) Nj

>ESR

Zi +

2ln(n 4 D|E|) Ni

6 Select arm a uniform randomly from E

7 Z-table Update(i)

8 until stopping condition is met;

On initialisation each arm is pulled  times. The hyperparameter  is selected to ensure each arm is pulled sufficiently to build an initial distribution. For optimal performance  is set to greater than 1. For  greater than 1, MOTDRL can build a sufficient initial distribution and can then efficiently explore each arm with the UCB1 statistic. At each timestep, the value distribution of the policies associated with the execution of each arm is calculated. The ESR set, E, is then calculated from the resulting value distributions. Therefore, for all the non-optimal arms l  E, there exists an ESR dominant arm i  E that ESR dominates the arm l.
To calculate ESR dominance required in Algorithm 2 at Line 5, it is critical to compute both the PDF and CDF of the underling value distribution of a policy. The PDF can be calculated by computing the probability of receiving individual returns. Combining the Z-table and N for an arm, i, it is possible to compute the probability of receiving each return in a given problem domain, since the following is true:

fX(x1, x2, ..., xn)

=

P (X

=

x1, X

=

x2, ..., X

=

xn)

=

Zi(x1, x2, ..., xn) Ni

(16)

Once the PDF has been computed using Equation 16, it is possible to compute the CDF. Since the following is true:

FX(x1, x2, ..., xn) =P (X  x1, X  x2, ..., X  xn)

=

...

P (X = xa, X = xb, ..., X = xk)

xax1 xbx2 xkxn

(17)

=

...

Zi(xa, xb, ..., xk)

xax1 xbx2 xkxn

Ni

22

Hayes et al.

Using the PDF and the CDF of a value distribution it is possible to calculate ESR dominance using Definition 9 or Definition 10. Both methods can be used to calculate ESR dominance.
To efficiently explore all available arms, MOTDRL uses the UCB1 statistic presented by Drugan et al. [11]. MOTDRL uses UCB1 to transform the PDF of the underlying value distribution. MOTDRL tranforms the PDF by adding the UCB1 statistic, computed at Line 5 in Algorithm 2, to the PDF. By summing the UCB1 statistic and the PDF, the PDF is shifted relative to the value of the computed UCB1 statistic. The CDF can then calculated based on the transformed PDF and ESR dominance can then be computed.
Transforming the PDF using the UCB1 statistic ensures that there is sufficient exploration of all available arms during experimentation. However, as the number of pulls of a given arm increases the UCB1 statistic decreases, which decreases exploration. Over time the UCB1 statistic's effect on the PDF and CDF becomes negligible. At such a point, MOTDRL can exploit the value distributions learned during exploration and compute the ESR set.

7 Experiments
In order to evaluate the MOTDRL algorithm, we test MOTDRL in multiple settings. Before experimentation we define a metric that can be used to evaluate the performance of multi-policy ESR methods. We then evaluate MOTDRL in a multi-objective multi-armed bandit setting. Finally, we evaluate MOTDRL using a Vaccine Recommender System environment.

7.1 Evaluation Metrics

The standard metrics for MORL [38, 45, 44] are not suitable to evaluate a multipolicy method under the ESR criterion since they are designed to specifically evaluate SER methods. To evaluate MORL algorithms under the ESR criterion, we adapt the coverage ratio metric used by Yang et al. [44] for the ESR criterion. The coverage ratio evaluates the agent's ability to recover optimal solutions in the ESR set (E). If F  Rm is the set of solutions found by the agent, we define the following:

F  E := {Z  F | Z  E s.t sup |FZ (x) - FZ (x) |  },

(18)

x

where x = [x1, x2, ..., xD] and D is equal to the number of objectives. Equation 18 uses the Kolmogorov­Smirnov statistic [10] (Equation 19), where sup is the
x
supremum of the set of distances. The Kolmogorov­Smirnov statistic takes the
largest absolute difference between the two CDFs across all x values,

sup |FZ (x) - FZ (x)|.

(19)

x

Expected Scalarised Returns Dominance

23

The Kolmogorov­Smirnov statistic returns a minimum value of 0 and a maximum value of 1. If two CDFs are equal then the Kolmogorov­Smirnov statistic will return a value of 0.
The coverage ratio is then defined as:

precision · recall

F1

=

2

·

precision

+

, recall

(20)

where precision = |F  E|/|F| indicating the fraction of optimal solutions among the retrieved solutions, and the recall = |F  E|/|E| indicating the fraction of optimal instances that have been retrieved over the total amount of optimal solutions [44].

7.2 Multi-Objective Multi-Armed Bandit Environment
In Section 7.2 we evaluate MOTDRL in a MOMAB setting. To evaluate MOTDRL, we consider a bi-objective MOMAB with five arms. Table 3 outlines the number of possible outcomes obtainable when selecting a given arm and the corresponding probabilities. In the MOMAB setting the ESR set is known a priori where the value distributions for arm 1 and arm 5 are ESR dominant and therefore the ESR set only contains arm 1 and arm 5.
To evaluate MOTDRL in a MOMAB environment we set Vmin = 0, Vmax = 10, D = 2,  = 5 and |E| = 2. To compute the coverage ratio we set = 0.01. All experiments in this setting are averaged over 10 runs.

Arm 1

Arm 2

Arm 3

P(Arm 1 = R) R

P(Arm 2 = R) R

P(Arm 3= R)

0.4

(0, 1)

0.85

(1, 0)

0.75

0.6

(5, 4)

0.15

(3, 2)

0.25

Arm 4

Arm 5

P(Arm 4 = R) R

P(Arm 5 = R) R

0.8

(0, 1)

0.7

(2, 0)

0.2

(1, 2)

0.3

(4, 5)

R (2, 0) (4, 2)

Table 3: A MOMAB with 5 arms where selecting an arm has two outcomes and two objectives.

MOTDRL is able to learn the underlying value distributions for each arm in the MOMAB setting. Using the value distributions for each arm, MOTDRL can learn the ESR set in the MOMAB environment. In Figure 3, we plot the coverage ratio as the F1 score. MOTDRL converges to the optimal F1 score of 1. MOTDRL converges to the optimal F1 score after 100, 000 episodes. An optimal F1 score can only be achieved when all policies in the ESR set have been learned by the agent.
Figure 4 displays the value distributions in the ESR set learned by MOTDRL as heatmaps. Each heatmap in Figure 4 corresponds to the probabilities highlighted for arm 1 (left) and arm 5 (right) in Table 3.

24

Hayes et al.

1.0

0.8

F1 score

0.6

0.4

0.2

0.0 0

40000

80000 120000 160000 200000 Episodes

Fig. 3: Results from the MOMAB environment. MOTDRL is able to learn the ESR set as MOTDRL converges to the optimal coverage ratio since the F1 score reaches the maximum possible value of 1.

objective 1 objective 1

10

9

8

0.5

7

0.4

6

5

0.3

4

3

0.2

2

0.1

1

0 0 1 2 3 4 5 6 7 8 9 10 0.0 objective 2

10

9

0.6

8

7

0.5

6

0.4

5

4

0.3

3

0.2

2

1

0.1

0 0 1 2 3 4 5 6 7 8 9 10 0.0 objective 2

Fig. 4: Heatmaps for each value distribution in the ESR set learned by MOTDRL in the MOMAB environment. The left heatmap describes the value distribution for arm 1 learned by MOTDRL and the right heatmap describes the value distribution for arm 5 learned by MOTDRL.

Figure 5 displays the CDFs for each value distribution in the ESR set learned by MOTDRL. The CDF is used to calculate ESR dominance and the CDFs in Figure 5 correspond to the CDFs of arm 1 and arm 5 in Table 3.
In Figure 6 all value distributions in the ESR set are shown. Figure 6 (left) is a heatmap with the value distributions for each policy in the ESR set. Figure 6 (right) shows the CDFs for the underlying value distribution for each policy in the ESR set. Since the CDF is used to calculate ESR dominance it is clear

Expected Scalarised Returns Dominance

25

probability

1.0

0.8

0.6

0.4

0.2

0.0

10

8

objective 2

0

2obje4ctive61 8

6 4 2
10 0

probability

1.0

0.8

0.6

0.4

0.2

0.0

10

8

objective 2

0

2obje4ctive61 8

6 4 2
10 0

Fig. 5: CDFs for each policy in the ESR set learned by MOTDRL in the MOMAB environment. The left figure describes the CDF for arm 1 learned by MOTDRL and the right figure describes the CDF for arm 5 learned by MOTDRL.

probability

objective 1

10

9

0.6

8

7

0.5

6

0.4

5

4

0.3

3

0.2

2

1

0.1

0 0 1 2 3 4 5 6 7 8 9 10 0.0 objective 2

1.0

0.8

0.6

0.4

0.2

0.0

10

8

objective 2

0

2obje4ctive61 8

6 4 2
10 0

Fig. 6: The left figure shows the combined heatmap for all policies in the ESR set learned by MOTDRL. The right figure shows the CDF of each policy in the ESR set learned by MOTDRL.

that both policies are ESR dominant. At specific points in Figure 6 (right) both CDFs intersect, making both policies ESR dominant.
Figure 7 demonstrates why SER MORL methods cannot be used to learn the optimal set of policies under the ESR criterion. To determine the Pareto front [26] the expectations of each arm in the MOMAB setting are calculated and the Pareto dominant policies are determined. In Figure 7 the policies on the Pareto front (left) have been highlighted in red, all other policies are Pareto dominated. In the MOMAB environment outlined in Table 3, the Pareto front consists of a single policy. Figure 7 (right) displays the expected values of the policies in the ESR set, highlighted in green. By comparing both plots in Figure 7, it is clear that it is not sufficient to use an SER method and Pareto dominance to compute the optimal set of policies under the ESR criterion.

26

Hayes et al.

objective 2 objective 2

5

5

Pareto Front

4

4

ESR Set

3

3

2

2

1

1

00

1

2

3

4

5 00

1

2

3

4

5

objective 1

objective 1

Fig. 7: The policies on the Pareto front (left) are different from the expectations of the policies in the ESR set (right). In this case, a policy that is in the ESR set is not on the Pareto front. This figure illustrate why SER methods cannot be used to learn the ESR set.

When using an SER method, certain policies that are optimal under the ESR criterion are dominated under the SER criterion and therefore not optimal.

7.3 Vaccine Recommender System
In a medical setting a doctor may only have one opportunity to select a treatment for a patient. In this case it is necessary to optimise under the ESR criterion. Consider the following scenario: a patient is travelling to another country where it is required to be vaccinated for a specific disease to gain entry to the country. There are five available vaccines, however, each vaccine will have varying side effects (safety rating) and effectiveness. This problem has two objectives: safety and effectiveness. Both objectives are ranked from 0 to 5, with 0 being the worst rating and 5 being the best rating. None of the available vaccines are 100% effective at treating the disease. When taking each vaccine there is a chance of different outcomes occurring, for example, there is a chance of having severe side effects (low safety rating) and a chance of the vaccine providing the required immunity to the disease (high effectiveness rating). Table 4 outlines each vaccine and the probability of each outcome occurring after taking the vaccine.
Given the utility function of the user is unknown, the MOTDRL algorithm is used to learn the underlying value distributions for each vaccine in Table 4 and determine the ESR set. Once MOTDRL has finished learning a set of optimal polices, in this case the ESR set, is returned to the user. When the user's utility function becomes known, a vaccine that maximises the user's utility function can be selected from the ESR set by the user.
The ESR set for the Vaccine Recommender System (VRS) environment is known a priori. The value distributions for vaccine 1 and vaccine 3 are ESR dominant and therefore vaccine 1 and vaccine 3 are the only distributions in

Expected Scalarised Returns Dominance

27

Vaccine 1 (V1)

P(V1= R) R

0.05

(2, 0)

Vaccine 2 (V2)

P(V2= R) R

0.1

(0, 0)

Vaccine 3 (V3)

P(V3= R) R

0.1

(1, 0)

0.05

(2, 1)

0.1

(1, 1)

0.1

(1, 3)

0.1

(3, 2)

0.5

(2, 0)

0.2

(3, 4)

0.8

(4, 2)

0.3

(2, 1)

0.6

(5, 4)

Vaccine 4 (V4)

P(V4= R) R

0.1

(1, 0)

Vaccine 5 (V5)

P(V5= R) R

0.8

(0, 0)

0.4

(2, 1)

0.05

(1, 1)

0.4

(3, 1)

0.05

(1, 2)

0.1

(3, 2)

0.1

(4, 0)

Table 4: A group of available vaccines that have varying outcomes. Some vaccines have a higher chance of side effects (low safety rating), while others are more effective at providing immunity. The objectives are ordered as follows: R = (safety, effectiveness).

the ESR set. The VRS environment has five arms where each arm corresponds to a vaccine in Table 4. To evaluate MOTDRL in a VRS environment, we set Vmin = 0, Vmax = 10, D = 2,  = 5 and |E| = 2. All experiments in this setting are averaged over 10 runs and each experiment lasts 200, 000 episodes. To compute the coverage ratio, we set = 0.01.

1.0

0.8

F1 score

0.6

0.4

0.2

0.0 0

40000

80000 120000 160000 200000 Episodes

Fig. 8: Results from the VRS environment. MOTDRL is able to learn the full ESR set as it converges the optimal F1 score of 1.

After sufficient sampling, MOTDRL is able to learn the underlying value distributions for each arm in the VRS environment. Given value distributions

28

Hayes et al.

can be used to give a partial ordering over policies, MOTDRL can use the value distributions for each arm to compute the ESR set in the VRS environment. In Figure 8, we plot the coverage ratio as the F1 score. MOTDRL converges to the optimal F1 score after 100, 000 episodes. Given MOTDRL converges to the optimal F1 it is clear MOTDRL is able to learn the ESR set.

objective 1 objective 1

10

9

8 7

0.6

6

5

0.4

4

3 2

0.2

1

0 0 1 2 3 4 5 6 7 8 9 10 0.0 objective 2

10

0.6

9

8

0.5

7

0.4

6

5

0.3

4

3

0.2

2

0.1

1

0 0 1 2 3 4 5 6 7 8 9 10 0.0 objective 2

Fig. 9: Heatmaps for each policy in the ESR set learned by MOTDRL. The left heatmap describes the distribution for vaccine 1 learned by MOTDRL and the right heatmap describes the distribution for vaccine 3 learned by MOTDRL.

Figure 9 presents heatmaps to represent the policies in the ESR set learned by MOTDRL. Each heatmap represents a value distribution learned by MOTDRL and shows the return vectors and the corresponding probabilities. Each heatmap in Figure 9 corresponds to the probabilities highlighted for vaccine 1 (left) and vaccine 3 (right) in Table 4. By repeatedly pulling each arm in the VRS environment and by updating the corresponding arm's Z-table and N value, MOTDRL can learn the underlying value distribution for each arm. Therefore MOTDRL can learn the underlying value distribution for each vaccine outlined in Table 4.
Figure 10 displays the policies in the ESR set learned by MOTDRL and their corresponding CDFs. Each CDF in Figure 10 corresponds to the CDFs of the underlying value distributions of vaccine 1 and vaccine 3 in Table 4.
Figure 11 shows all policies in the ESR set learned by MOTDRL. Figure 11 (left) shows a heatmap with the value distributions learned my MOTDRL for each policy in the ESR set. Figure 11 (right) shows the CDFs for the underlying value distribution for each policy in the ESR set. To calculate ESR dominance MOTDRL uses the CDF. Figure 11 (right) shows how value distributions for vaccine 1 and vaccine 3 are ESR dominant. MOTDRL uses Definition 10 to calculate ESR dominance in terms of the CDFs of value distributions. Since there are two policies in the ESR set, which are shown in Figure 11 (right), we know that both policies do not satisfy Definition 10 and therefore the policies are ESR dominant.

Expected Scalarised Returns Dominance

29

probability

1.0

0.8

0.6

0.4

0.2

0.0

10

8

objective 2

0

2obje4ctive61 8

6 4 2
10 0

probability

1.0

0.8

0.6

0.4

0.2

0.0

10

8

objective 2

0

2obje4ctive61 8

6 4 2
10 0

Fig. 10: CDFs for each policy in the ESR set learned by MOTDRL in the VRS environment. The left figure describes the CDF for vaccine 1 learned by MOTDRL and the right figure describes the CDF for vaccine 3 learned by MOTDRL.

probability

objective 1

10

9

8 7

0.6

6

5

0.4

4

3 2

0.2

1

0 0 1 2 3 4 5 6 7 8 9 10 0.0 objective 2

1.0

0.8

0.6

0.4

0.2

0.0

10

8

objective 2

0

2obje4ctive61 8

6 4 2
10 0

Fig. 11: The left figure shows the combined heatmap for all policies in the ESR set learned by MOTDRL for the VRS. The right figure shows the CDF of each policy in the ESR set learned by MOTDRL.

8 Related Work
The various orders of stochastic dominance have been used extensively as a method to determine the optimal decision when making decisions under uncertainty in economics [8], finance [1, 5], game theory [13], and various other real-world scenarios [6]. However, stochastic dominance has largely been overlooked in systems that learn. Cook and Jarret [9] use various orders of stochastic dominance and Pareto dominance with genetic algorithms to compute optimal solution sets for an aerospace design problem with multiple objectives when constrained by a computational budget. Martin et al. [21] use secondorder stochastic dominance (SSD) with a single-objective distributional RL algorithm [7]. Martin et al. [21] use SSD to determine the optimal action to

30

Hayes et al.

take at decision time, and this approach is shown to learn good policies during experimentation.
To learn the ESR set in sequential decision making processes, like MOMDPs, new distributional MORL methods must be formulated. Distributional Monte Carlo tree search (DMCTS) is a state-of-the-art ESR method and uses a bootstrap Thompson sampling method to approximate a posterior distribution over the returns [17]. However, this method is a single policy method and relies on the utility function of the user to be known at the time of learning or planning. DMCTS would invalidate the ESR criterion in the unknown utility function scenario and would therefore be unable to learn the ESR set. Distributional methods like the C51 algorithm, proposed by Bellemare et al. [7], could potentially be used to learn the underlying distribution of a random vector. However C51 is a single-objective method and defining a multi-objective version of C51 to learn the ESR set could pose significant challenges. Replacing the distribution over returns used by C51 with a multi-variate distribution could cause computation to increase with the number of objectives. In this case, dedicated multi-objective distributional methods must be formulated so that it is possible to efficiently learn the ESR set for the ESR criterion. We highlight this as a new challenge that must be addressed by the MORL community.

9 Conclusion & Future Work
MORL has been highlighted as one of several key challenges that needs to be addressed in order for RL to be commonly deployed in real-world systems [12]. In order to apply RL to the real world the MORL community must consider the ESR criterion. However, the ESR criterion has largely been ignored by the MORL community, with the exception of the works of Roijers et al. [31, 30], Hayes et al. [17, 16] and Vamplew et al. [39]. The works of Hayes et al. [16, 17] and Roijers et al. [30] present single-policy algorithms that are suitable to learn policies under the ESR criterion, however, prior to this work, a formal definition of the necessary requirements to satisfy the ESR criterion had not previously been defined. In Section 3, we outline, through examples and definitions, the necessary requirements to satisfy the ESR criterion. The formal definitions outlined in Section 3 ensure that an optimal policy can be learned when the utility function of the user is known under the ESR criterion. However, in the real world, a user's preferences over objectives (or utility function) may be unknown at the time of learning [31].
Prior to this paper, a suitable solution set for the unknown utility function scenario under the ESR criterion had not been defined. This long-standing research gap has restricted the applicability of MORL in real-world scenarios under the ESR criterion. In Section 4 and Section 5 we define the necessary solution sets required for multi-policy algorithms to learn a set of optimal policies under the ESR criterion when the utility function of a user is unknown. In Section 6 we present a novel multi-policy algorithm, known as multi-objective tabular distributional reinforcement learning (MOTDRL), that can learn the

Expected Scalarised Returns Dominance

31

ESR set in a MOMAB setting when the utility function of a user is unknown at the time of learning. In Section 7 we evaluate MOTDRL in two MOMAB settings and show that MOTDRL can learn the ESR set in MOMAB settings. This work aims to answer some of the existing research questions regarding the ESR criterion. Moreover, we aim to highlight the importance of the ESR criterion when applying MORL to real-world scenarios. In order to successfully apply MORL to the real world, we must implement new single-policy and multi-policy algorithms that can learn solutions for non-linear utility functions in various scenarios.
A promising direction for future work would be to extend the work of Hayes et al. [17] and the work of Wang and Sebag [41]. It may be possible to build on the aforementioned works to implement a multi-objective distributional Monte Carlo tree search algorithm that can learn a set of optimal policies under the ESR criterion. It is important to note that Hayes et al. [16, 17] use bootstrap Thompson sampling to approximate a posterior distribution. This method cannot learn the ESR set when utility function of a user is unknown, therefore a different distributional method must be used to learn the ESR set. Although the distributional method used by Hayes et al. [17] cannot be used to learn the ESR set, this work is still a useful starting point.
A lack of well defined benchmarks is a significant challenge associated with implementing any new single-policy or multi-policy algorithms under the ESR criterion. Currently, very few ESR benchmark environments exist (e.g. Fishwood [30]). In order to accurately evaluate single-policy and multi-policy ESR algorithms, a suite of benchmark problem domains need to be designed. Under the SER criterion, such benchmarks already exist, e.g. Deep Sea Treasure [38]. It is also important to highlight the need to establish new metrics to evaluate multi-policy algorithms under the ESR criterion. As previously mentioned, all metrics used to evaluate multi-objective algorithms are designed for the SER criterion. In order to accurately evaluate multi-policy algorithms under the ESR criterion, new metrics must be determined. We note that extending the work of Zintgraf et al. [45] for the ESR criterion would be a promising starting point.

10 Supplementary Material

Lemma 1 (Beppo Levi's lemma [35]) Consider a point-wise non-decreasing sequence of positive functions fn : X  [0, +], i.e., for every k  1 and every x  X.
0  fn(x)  fn+1(x)  +
Set the point-wise limit of the sequence {fi} to be f . That is, for every x  X,

lim
n+

fn(x)

=

f

(x)

Then f is measurable and:

lim
n+

fn(x)dx =

lim
n+

fn

(x)dx

32

Hayes et al.

Lemma 2 (Monotone convergence) Let u be a non-negative monotonically increasing utility function in x and y, and F the CDF of a random variables X and Y . Then,

lim u(x, y)F (x, y)dx = lim u(x, y)F (x, y)dx.

y+

y+

Proof Let gn(x) = u(x, n)F (x, n). As u and F are positive monotonically increasing functions in n, the function gn is also positive and monotonically increasing, i.e.,
0  gn(x)  gn+1(x)  +
According to Beppo Levi's lemma (see Lemma 1), the limit of the integral of gn(x) in x is the integral of its limit, i.e.,

lim gn(x)dx = lim gn(x)dx.

n+

n+

Acknowledgements Conor F. Hayes is funded by the National University of Ireland Galway Hardiman Scholarship. This research was supported by funding from the Flemish Government under the "Onderzoeksprogramma Artifici¨ele Intelligentie (AI) Vlaanderen" program.

Conflict of interest The authors declare that they have no conflict of interest.

References
1. Ali, M.M.: Stochastic dominance and portfolio analysis. Journal of Financial Economics 2(2), 205­229 (1975). DOI https://doi.org/10.1016/0304-405X(75)90005-7. URL https://www.sciencedirect.com/science/article/pii/0304405X75900057
2. Atkinson, A.B., Bourguignon, F.: The Comparison of Multi-Dimensioned Distributions of Economic Status. The Review of Economic Studies 49(2), 183­201 (1982). DOI 10.2307/2297269. URL https://doi.org/10.2307/2297269
3. Auer, P., Chiang, C.K., Ortner, R., Drugan, M.: Pareto front identification from stochastic bandit feedback. In: A. Gretton, C.C. Robert (eds.) Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, Proceedings of Machine Learning Research, vol. 51, pp. 939­947. PMLR, Cadiz, Spain (2016). URL http://proceedings.mlr.press/v51/auer16.html
4. Bawa, V.S.: Optimal rules for ordering uncertain prospects. Journal of Financial Economics 2(1), 95 ­ 121 (1975). DOI https://doi.org/10.1016/0304-405X(75)90025-2. URL http://www.sciencedirect.com/science/article/pii/0304405X75900252
5. Bawa, V.S.: Safety-first, stochastic dominance, and optimal portfolio choice. The Journal of Financial and Quantitative Analysis 13(2), 255­271 (1978). URL http: //www.jstor.org/stable/2330386

Expected Scalarised Returns Dominance

33

6. Bawa, V.S.: Research bibliography-stochastic dominance: A research bibliography. Manage. Sci. 28(6), 698­712 (1982). DOI 10.1287/mnsc.28.6.698. URL https://doi.org/ 10.1287/mnsc.28.6.698
7. Bellemare, M.G., Dabney, W., Munos, R.: A distributional perspective on reinforcement learning. In: International Conference on Machine Learning, pp. 449­458. PMLR, Sydney (2017)
8. Choi, E., Johnson, S.: Stochastic dominance and uncertain price prospects. Center for Agricultural and Rural Development (CARD) at Iowa State University, Center for Agricultural and Rural Development (CARD) Publications 55 (1988). DOI 10.2307/ 1059583
9. Cook, L., Jarrett, J.: Using stochastic dominance in multi-objective optimizers for aerospace design under uncertainty. In: American Institute of Aeronautics and Astronautics Journal (2018). DOI 10.2514/6.2018-0665
10. Darling, D.A.: The kolmogorov-smirnov, cramer-von mises tests. The Annals of Mathematical Statistics 28(4), 823­838 (1957). URL http://www.jstor.org/stable/2237048
11. Drugan, M.M., Nowe, A.: Designing multi-objective multi-armed bandits algorithms: A study. In: The 2013 International Joint Conference on Neural Networks (IJCNN), pp. 1­8 (2013). DOI 10.1109/IJCNN.2013.6707036
12. Dulac-Arnold, G., Levine, N., Mankowitz, D.J., Li, J., Paduraru, C., Gowal, S., Hester, T.: Challenges of real-world reinforcement learning: definitions, benchmarks and analysis. In: Machine Learning (2021). DOI https://doi.org/10.1007/s10994-021-05961-4
13. Fishburn, P.C.: Non-cooperative stochastic dominance games. International Journal of Game Theory 7(1), 51­61 (1978)
14. Hadar, J., Russell, W.R.: Rules for ordering uncertain prospects. The American Economic Review 59(1), 25­34 (1969). URL http://www.jstor.org/stable/1811090
15. Hayes, C.F., Radulescu, R., Bargiacchi, E., K¨allstr¨om, J., Macfarlane, M., Reymond, M., Verstraeten, T., Zintgraf, L.M., Dazeley, R., Heintz, F., et al.: A practical guide to multi-objective reinforcement learning and planning. arXiv preprint arXiv:2103.09568 (2021)
16. Hayes, C.F., Reymond, M., Roijers, D.M., Howley, E., Mannion, P.: Risk-aware and multi-objective decision making with distributional monte carlo tree search. In: Proceedings of the Adaptive and Learning Agents workshop at AAMAS 2021) (2021)
17. Hayes, C.F., Reymond, M., Roijers, D.M., Howley, E., Mannion, P.: Distributional monte carlo tree search for risk-aware and multi-objective reinforcement learning. In: Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems, vol. 2021. IFAAMAS (2021 In Press)
18. Hayes, C.F., Verstraeten, T., Roijers, D.M., Howley, E., Mannion, P.: Dominance criteria and solution sets for the expected scalarised returns. In: Proceedings of the Adaptive and Learning Agents workshop at AAMAS 2021 (2021)
19. Levhari, D., Paroush, J., Peleg, B.: Efficiency analysis for multivariate distributions. The Review of Economic Studies 42(1), 87­91 (1975). URL http://www.jstor.org/ stable/2296822
20. Levy, H.: Stochastic dominance and expected utility: Survey and analysis. Management Science 38(4), 555­593 (1992). URL http://www.jstor.org/stable/2632436
21. Martin, J., Lyskawinski, M., Li, X., Englot, B.: Stochastically dominant distributional reinforcement learning. In: International Conference on Machine Learning, pp. 6745­ 6754. PMLR (2020)
22. Mas-Colell, A., Whinston, M.D., Green, J.R., et al.: Microeconomic theory, vol. 1. Oxford university press New York (1995)
23. Moffaert, K.V., Now´e, A.: Multi-objective reinforcement learning using sets of pareto dominating policies. Journal of Machine Learning Research 15(107), 3663­3692 (2014). URL http://jmlr.org/papers/v15/vanmoffaert14a.html
24. Nakayama, H., Tanino, T., Sawaragi, Y.: Stochastic dominance for decision problems with multiple attributes and/or multiple decision-makers. IFAC Proceedings Volumes 14(2), 1397 ­ 1402 (1981). DOI https://doi.org/10.1016/S1474-6670(17)63673-5. URL http://www.sciencedirect.com/science/article/pii/S1474667017636735. 8th IFAC World Congress on Control Science and Technology for the Progress of Society, Kyoto, Japan, 24-28 August 1981

34

Hayes et al.

25. O'Callaghan, D., Mannion, P.: Exploring the impact of tunable agents in sequential social dilemmas. arXiv preprint: arXiv:2101.11967 (2021). URL https://arxiv.org/ abs/2101.11967
26. Pareto, V.: Manuel d'Economie Politique, vol. 1. Giard, Paris (1896) 27. Radulescu, R., Mannion, P., Roijers, D.M., Now´e, A.: Multi-objective multi-agent deci-
sion making: a utility-based analysis and survey. Autonomous Agents and Multi-Agent Systems 34(10) (2020) 28. Radulescu, R., Mannion, P., Zhang, Y., Roijers, D.M., Now´e, A.: A utility-based analysis of equilibria in multi-objective normal-form games. The Knowledge Engineering Review 35 (2020) 29. Richard, S.F.: Multivariate risk aversion, utility independence and separable utility functions. Management Science 22(1), 12­21 (1975). URL http://www.jstor.org/ stable/2629784 30. Roijers, D.M., Steckelmacher, D., Now´e, A.: Multi-objective reinforcement learning for the expected utility of the return. In: Proceedings of the Adaptive and Learning Agents workshop at FAIM 2018 (2018) 31. Roijers, D.M., Vamplew, P., Whiteson, S., Dazeley, R.: A survey of multi-objective sequential decision-making. Journal of Artificial Intelligence Research 48, 67­113 (2013) 32. Roijers, D.M., Whiteson, S., Oliehoek, F.A.: Linear support for multi-objective coordination graphs. In: Proceedings of the 2014 International Conference on Autonomous Agents and Multi-Agent Systems, AAMAS '14, p. 1297­1304. International Foundation for Autonomous Agents and Multiagent Systems, Richland, SC (2014) 33. Roijers, D.M., Zintgraf, L.M., Now´e, A.: Interactive thompson sampling for multiobjective multi-armed bandits. In: International Conference on Algorithmic DecisionTheory, pp. 18­34. Springer (2017) 34. Scarsini, M.: Dominance conditions for multivariate utility functions. Management Science 34(4), 454­460 (1988). URL http://www.jstor.org/stable/2631934 35. Schappacher, N.: Beppo levi and the arithmetic of elliptic curves. The Mathematical Intelligencer 18(1), 57­­69 (1996) 36. Sriboonchitta, S., Wong, W.K., Dhompongsa, s., Nguyen, H.: Stochastic Dominance and Applications to Finance, Risk and Economics. Chapman and Hall/CRC (2009). DOI 10.1201/9781420082678 37. Sutton, R.S., Barto, A.G.: Reinforcement Learning: An Introduction. A Bradford Book, Cambridge, MA, USA (2018) 38. Vamplew, P., Dazeley, R., Berry, A., Issabekov, R., Dekker, E.: Empirical evaluation methods for multiobjective reinforcement learning algorithms. Machine Learning 84, 51­80 (2011). DOI 10.1007/s10994-010-5232-5 39. Vamplew, P., Foale, C., Dazeley, R.: The impact of environmental stochasticity on valuebased multiobjective reinforcement learning. In: Neural Computing and Applications (2021). DOI https://doi.org/10.1007/s00521-021-05859-1 40. Vamplew, P., Yearwood, J., Dazeley, R., Berry, A.: On the limitations of scalarisation for multi-objective reinforcement learning of pareto fronts. In: W. Wobcke, M. Zhang (eds.) AI 2008: Advances in Artificial Intelligence, pp. 372­378. Springer Berlin Heidelberg, Berlin, Heidelberg (2008) 41. Wang, W., Sebag, M.: Multi-objective Monte-Carlo tree search. In: S.C.H. Hoi, W. Buntine (eds.) Proceedings of Machine Learning Research, vol. 25, pp. 507­522. PMLR, Singapore (2012) 42. Wolfstetter, E.: Topics in Microeconomics: Industrial Organization, Auctions, and Incentives. Cambridge University Press (1999). DOI 10.1017/CBO9780511625787 43. Yahyaa, S., Manderick, B.: Thompson sampling for multi-objective multi-armed bandits problem. In: Proceedings, p. 47. Presses universitaires de Louvain, Elsevier (2015) 44. Yang, R., Sun, X., Narasimhan, K.: A generalized algorithm for multi-objective reinforcement learning and policy adaptation. In: H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, R. Garnett (eds.) Advances in Neural Information Processing Systems, vol. 32. Curran Associates, Inc. (2019). URL https://proceedings.neurips. cc/paper/2019/file/4a46fbfca3f1465a27b210f4bdfe6ab3-Paper.pdf 45. Zintgraf, L.M., Kanters, T.V., Roijers, D.M., Oliehoek, F., Beau, P.: Quality assessment of morl algorithms: A utility-based approach. In: Benelearn 2015: Proceedings of the 24th Annual Machine Learning Conference of Belgium and the Netherlands (2015)

Expected Scalarised Returns Dominance

35

46. O¨ ner, D., Karakurt, A., Eryilmaz, A., Tekin, C.: Combinatorial multi-objective multiarmed bandit problem (2018)

