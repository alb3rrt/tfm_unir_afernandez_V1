A UNIFIED PAC-BAYESIAN FRAMEWORK FOR MACHINE UNLEARNING VIA INFORMATION RISK MINIMIZATION
Sharu Theresa Jose, Osvaldo Simeone
Department of Engineering King's College London London, WC2R 2LS

arXiv:2106.00265v1 [cs.LG] 1 Jun 2021

ABSTRACT
Machine unlearning refers to mechanisms that can remove the influence of a subset of training data upon request from a trained model without incurring the cost of re-training from scratch. This paper develops a unified PAC-Bayesian framework for machine unlearning that recovers the two recent design principles ­ variational unlearning [1] and forgetting Lagrangian [2]­ as information risk minimization problems [3]. Accordingly, both criteria can be interpreted as PAC-Bayesian upper bounds on the test loss of the unlearned model that take the form of free energy metrics.
Index Terms-- Machine unlearning, PAC-Bayesian bounds, free energy minimization
1. INTRODUCTION
AI tools are increasingly widespread and subject to privacy attacks and data misuse. Recent regulations, such as the European Union's General Data Protection Regulation (GDPR) and the California Consumer Privacy Act, has enshrined in law the right for individuals to withdraw consent to the use of their personal data for training machine learning models. The mere deletion of the requested data from the training data set does not serve the purpose, as information about the deleted data can still be retrieved from already trained machine learning models [4]. Thus, data deletion necessitates the machine learning model to unlearn the contribution of the deleted data to the training process, such that the resulting model behaves as if it has never observed the data in the first place.
A straightforward approach to unlearn is to retrain the model from scratch by using only data remaining after deletion of the data to be unlearnt. However, this is computationally intensive and resource expensive. Machine unlearning refers to mechanisms that can remove the influence of a specific subset of the training data on a trained machine learning model, without incurring the cost of retraining from scratch [5], [6].
Several machine unlearning approaches have been studied since the introduction of the concept in [5], where the problem

was studied in the context of statistical query learning. [7] proposes an unlearning approach that partitions the data set into shards that are used to train multiple models in isolation and finally aggregrated. This allows unlearning to be carried out by aggregating only the remaining shards, avoiding the need for retraining.
Our work is motivated by two recently proposed machine unlearning mechanisms. The first proposes a design criterion, termed Evidence Upper BOund (EUBO), for variational unlearning within a Bayesian setting [1], while the second optimizes over a "scrubbing function" by minimizing a forgetting Lagrangian criterion [2]. Although prima facie these two approaches seem different, we demonstrate that the two design principles can be interpreted in a unified manner in the context of PAC-Bayesian theory [8], [9]. PAC-Bayesian theory develops high-probability upper bounds on the population loss of a learning algorithm in terms of a free energy metric that includes the sum of a training loss and the Kullback-Leibler (KL) divergence between the learning algorithm and a dataindependent prior distribution [9, 10].
The main contributions of the paper are summarized as follows. We develop a unified PAC-Bayesian framework for machine unlearning that explains the unlearning design principles in [1] and [2] through the principle of information risk minimization (IRM) [3]. The PAC-Bayesian formulation makes use of the recent result in [11] that accounts for datadependent priors. We show that the design criteria ­ EUBO and forgetting Lagrangian ­ optimize PAC-Bayesian bounds with appropriate choices of training loss and data-dependent prior. Finally, the proposed framework motivates the design of amortized variants of variational unlearning and forgetting Lagrangian-based mechanisms, which are also described.
2. LEARNING AND UNLEARNING ALGORITHMS
In this section, we start by defining the operation and performance criteria of learning and unlearning algorithms. These are described as stochastic mappings as in the standard PACBayes framework.

2.1. Learning Algorithm

Let D = (Z1, . . . , Zn) denote a training data set of n samples generated i.i.d. according to an unknown population dis-
tribution PZ  P(Z). A learning algorithm uses the data set D to infer a model parameter W belonging to a model class W. We define the learning algorithm as a stochastic mapping, PW |D  P(W)1, from the input training set D to the model class, W. The probabilistic mapping PW |D describes a distribution over all possible outcomes W in the model class W.
Let  : W × Z  R+ denote a loss function. The goal of the learning algorithm is to find a model parameter w  W
that minimizes the population loss,

L(w) = EPZ [(w, Z)],

(1)

which is the average loss of the model parameter w incurred on a new test data point Z  PZ. The population loss (1) is unknown to the learner, since the underlying population distribution PZ is not available. Instead, the learner uses the empirical training loss on the data set D, i.e.,

L(w|D)

=

1 n

n

(w, Zi)

(2)

i=1

as the training criterion. For a given training data set D,
we define the generalization error, L(PW |D), of a learning mechanism PW |D as the average difference between the population loss (1) and the training loss (2), i.e.,

L(PW |D ) = EPW |D [L(W ) - L(W |D)].

(3)

The generalization error (3) quantifies the extent to which the training loss (2) can be reliably used as a proxy measure for the unknown population loss.

2.2. Machine Unlearning
Consider a model Wl  PW |D learned using the data set D. When a request is received to "delete" a subset De  D of m samples, the learned model Wl must be updated so as to "unlearn" the information extracted from the data set De by the learning process. We refer to data set De as the unlearning data set. Ideally, this could be done by re-training from scratch by using the remaining data, Dr = D \ De, i.e., by applying the stochastic mapping PW |Dr . Given the large computational cost of re-training, machine unlearning aims to remove the influence of the data De on the learned model Wl without incurring the full cost of re-training from scratch. Formally, we define an unlearning algorithm as follows [12].
Definition 2.1 (Unlearning Algorithm) An unlearning algorithm PW |Wl,T (D),De is a stochastic mechanism that maps the learned model parameter Wl  PW |D, a statistic T (D) of data set D, and the unlearning data set De to the space of model parameters W.
1We use P(·) to denote the space of all probability distributions on `·'.

The rationale for making the unlearned model W  PW |Wl,T (D),De depend on a statistic T (D) of D is to rule out training from scratch. In fact, if the statistic is T (D) = D, the unlearning algorithm can ignore Wl and re-train from scratch, while more restrictive choices of T (D) make this impossible.
In order to ensure successful unlearning, one needs to impose that the distribution of the unlearned model W be close to that obtained by training from scratch. For fixed data sets D and De, the latter distribution is PW |Dr , while the former is given by the average EPWl|D [PW |Wl,T (D),De ] over the learning mechanism. Note that the expectation marginalizes over
the learned models. This constraint can be formalized as fol-
lows.

Definition 2.2 (-certified unlearning) An unlearning algo-
rithm PW |Wl,T (D),De is said to satisfy -certified unlearning for  > 0 if

DKL(EWlPW |D [PW |Wl,T (D),De ]||PW |Dr )  , (4)
where DKL(P ||Q) denotes the KL divergence between distributions P and Q.

By the biconvexity of the KL divergence, it is easy to see that the unlearning certificate in (4) is implied by the stronger condition that the inequality

DKL(PW |Wl,T (D),De ||PW |Dr )  

(5)

applies for all Wl  W in the support of PW |D.

3. PRELIMINARIES
In this section, we briefly review the classical PAC-Bayesian framework, which underlies the proposed unified approach to machine unlearning. PAC Bayesian theory [8, 13] provides upper bounds on the average population loss, EPW|D [L(W )], of a learning algorithm PW |D in terms of: (a) the average training loss, EPW|D [L(W |D)], and (b) the KL divergence between the distribution PW |D and an arbitrary dataindependent "prior" QW . The PAC-Bayesian bounds hold with high probability over random draws of the training data set D. There has been extensive study on various refinements to the original PAC-Bayesian bound of [8] (see [14] for a review). More recently, PAC-Bayesian bounds have been extended to account for data-dependent priors [15], [11].
In this work, we make use of the general PAC-Bayesian bound derived in Theorem 2 of [11] that allows for datadependent priors. This turns out to be important for unlearning, since the prior will be used to account for the learning algorithm. The next lemma restates Theorem 2 in [11] by using our notation and by adopting a conventional formulation in terms of uniform bounds over all posteriors PW |D. A proof is provided for completeness in Appendix A.

Lemma 3.1 Let QW |D denote a data-dependent prior. For any (measurable) function A : Zn × W  R2 and convex function F : R2  R, let f : Zn × W  R be the compo-
sition of F and A, and let  = EPZn EQW |D [exp(f (D, W ))]. Then, with probability at least 1 - , with   (0, 1), over the random draw of data set D  PZn , the following inequality holds uniformly over all stochastic mappings PW |D

F (EPW |D [A(D, W )])

 DKL(PW |D||QW |D) + log(/).

(6)

In the rest of the paper, we will use Lemma 3.1 by selecting function A(D, W ) to output a tuple including the population loss L(W ) and a training loss metric to be specialized for different unlearning methods. Furthermore, the convex function F will be chosen to output the difference of its inputs, i.e., F (a, b) = a - b. With these choices, the PAC-Bayesian bound in (6) will allow us to relate the empirical training metrics and the unknown population loss.
For reference, in the standard analysis of learning algorithms, the function A(D, W ) is selected to be the two-
dimensional vector [L(W ), L(W |D)]. With this choice, the bound in (6) can be re-written as an upper bound on the population loss that holds for all PW |D:

EPW |D [L(W )]



FIRM

+

1 

log(/),

where,

(7)

FIRM

=

EPW |D [L(W |D)]

+

1 

DKL(PW

|D

||QW

|D

).

Important to our framework is the observation that the PACBayesian bound (6), and hence also (7), hold uniformly over all choices of the learning algorithm PW |D. As such, one can optimize the right-hand side of (7) ove the learning algorithm PW |D by considering the problem minPW |D FIRM. By minimizing an upper bound on the population loss, the learning criterion (7) facilitates generalization. This approach is known as Information Risk Minimization (IRM) [3], and it amounts to the minimization of a free energy criterion [10]. A free energy criterion is given by the sum of a training loss and of an information-theoretic regularization.
The PAC-Bayesian bound in (6) contains a constant term , bounding which ensures non-vacuous bounds on the generalization error. For data-independent priors, under suitable assumptions on the loss function, such as boundedness or subGaussianity, the constant  can be easily upper bounded. An upper bound on  for a data-dependent prior has been recently obtained in [11]. Since we will use (6) to justify unlearning criteria via variants of the IRM problem, we will not be further concerned with bounding .

4. VARIATIONAL UNLEARNING
In this section, we study the Bayesian unlearning framework introduced in the recent work [1]. As we first review, this

paper presents a new unlearning criterion, termed Evidence Upper BOund (EUBO), that enables variational unlearning. To be consistent with Definition 2.1, we specifically describe here an amortized variational unlearning variant of the approach proposed in [1]. We then show that the resulting unlearning algorithm can be interpreted as IRM, which is obtained through a specific instantiation of the PAC-Bayesian bound (6).

4.1. Amortized Variational Unlearning

In order to meet the unlearning requirement (4) for some  > 0, the variational unlearning framework proposed in [1] finds a distribution in the model parameter space W that is closest, in terms of KL divergence, to the distribution PW |Dr resulting from re-training on the remaining data Dr. Optimization is
restricted to a given family of distributions.
The approach requires the variational optimization to be carried out separately for any given selection of data sets D and De. Furthermore, it relies on access to the distribution PW |D and not solely on a trained model Wl. In contrast, an efficient unlearning mechanism conforming to Defini-
tion 2.1 must define a conditional probability distribution PW |Wl,T (D),De that can be instantiated for any choice of learned model Wl, statistic T (D) of the data, and unlearning data set De. To this end, in this section, we develop an amortized variant of variational unlearning [1] that enables optimization over an unlearning mechanism PW |Wl,T (D),De . We refer to this approach as amortized variational unlearning
(AVU).
The proposed AVU framework constrains the unlearning mechanism PW |Wl,T (D),De to belong to a family QAVU of (parameterized) conditional distributions on W. AVU seeks to find the unlearning mechanism PW |Wl,T (D),De that solves the following problem

min E PW |Wl,T (D),De

PD,De PWl|D

DKL(PW |Wl,T (D),De ||PW |Dr )

,

QAVU

(8)

where PWl|D denote the distribution of the learned model Wl  PW |D, and PD,De denote the probability distribution of the training data D  PZn and of the unlearning data set De  PDe|D. The conditional distribution PDe|D describes a uniformly distributed stochastic selection of a subset De of m samples from D. Problem (8) aims at ensuring that the
unlearning condition (5) be satisfied on average over all train-
ing data set D and unlearning data set De for small value of  > 0.
Following [1], the optimization problem in (8) can be
equivalently formulated as

min E PW |Wl,T (D),De PD,De PWl|D
QAVU

EUBO(PW |Wl,T (D),De , PW |D) , (9)

where the Evidence Upper BOund (EUBO) is defined as

EUBO(PW |Wl,T (D),De , PW |D ) =EPW |Wl,T (D),De [log PDe|W ]
+ DKL(PW |Wl,T (D),De ||PW |D ).

(10)

The EUBO (10) comprises of two terms: (i) the average positive log-likelihood of the unlearning data set De obtained after unlearning; and (ii) the deviation of the unlearning mechanism from the learning algorithm PW |D. Intuitively, the first term should be small for effective unlearning, while the sec-
ond is a regularization penalty that accounts for the residual
epistemic uncertainty associated with the training algorithm.

4.2. A PAC-Bayesian View of Variational Unlearning

We now demonstrate that the optimization (10) can be jus-
tified as an IRM obtained from the PAC-Bayesian bound in
(6). To instantiate the PAC-Bayesian bound in (6) for unlearn-
ing, we note that the unlearning mechanism in Definition 2.1 is a cascade of two operations: (a) sample model parameter Wl  PW |D according to the learning mechanism; and then (b) apply the unlearning mechanism PW |Wl,T (D),De on the learned model Wl. This process is subject to the random draw of data D  PZn and to the random selection of subset of data to be removed, De  PDe|D. In line with this observation, we have the following PAC-Bayesian bound for the
unlearning mechanism.

Corollary 4.1 Let the data dependent prior be fixed as the
learning mechanism PW |D. With probability at least 1 - , with   (0, 1), over the random draw of the data set D  PZn and the subset De  D to be removed, the following inequality holds uniformly for all unlearning algorithms
PW |Wl,T (D),De :

EPWl|D EPW |Wl,T (D),De [-EPZ [log PZ|W ]]

EPWl |D

1 m

EUBO(PW

|Wl

,T

(D),De

,

PW

|D

)

+

1 m

log

¯AVU 

,

(11)

where ¯AVU = EPD,De PW|D [exp(m(-EPZ [log PZ|W ] - (1/m) log PDe|W )].

Proof : This result is obtained from Lemma 3.1 by selecting
A(D, W ) as the two-dimensional vector [-mEPZ [log PZ|W ], log PDe|W ] and F (a, b) = a - b. Details can be found in Appendix B.

The left-hand side in (11) is the average test log-loss obtained by the unlearnt model. Therefore, by (11), the variational unlearning mechanism introduced in [1] can be interpreted as minimizing an upper bound on the test log-loss over

the unlearning mechanism PW |Wl,T (D),De (assuming knowledge of PW |D). By (10), this minimization is of the form (7) assumed by IRM problems [3]. As   0, the inequality in
(11) holds almost surely, which justifies taking the average in (10) over the draws of D and De. It follows that the proposed AVU (10) can be similarly interpreted in terms of the min-
imization of a PAC-Bayes upper bound on the average test
log-loss, and hence in terms of an IRM problem.

5. FORGETTING LAGRANGIAN-BASED UNLEARNING
In this section, we first review the unlearning framework introduced in [2], the Forgetting Lagrangian, and show that this can also be intrepreted as an IRM obtained as a specific instantiation of (6).

5.1. Forgetting Lagrangian
Reference [2] considers a stochastic learning mechanism PW |D that trains the model parameter vector W of a deep neural network (DNN) using data set D. The unlearning mechanism PW |Wl,T (D),De ignores the statistic T (D) and yields a stochastic scrubbing function PW |Wl,De that "scrubs off" the influence of the unlearning data set De on the learned model Wl  PW |D.
The scrubbing function PW |Wl,De is designed so as to optimize the Forgetting Lagrangian,

F L(PW |Wl,De , ) = EPW |Wl,De [L(W |Dr)] + DKL(EPWl|D [PW |Wl,De ]||EPWl|Dr [P~W |Wl ])

(12)

where  > 0 denotes a Lagrangian multiplier, and P~W |Wl is a an arbitrary `reference' distribution that maps the model Wl  PW |Dr , obtained by retraining on the data set Dr, to a "noisy" version W  W. The forgetting Lagrangian in (12) thus aims at finding an unlearning mechanism that (a)
minimizes the average training loss L(w|Dr) on the remaining data Dr; while (b) ensuring that the unlearning mechanism PW |Wl,De applied on the learned model Wl  PW |D is close, in terms of KL divergence, to the reference distribution P~W |Wl applied on the model Wl  PW |Dr obtained after retraining from scratch. Thus, the KL divergence term in (12)
ensures a "certificate of unlearning" with respect to the reference P~W |Wl in the sense of Definition 2.2. Moreover, the KL divergence term can be interpreted as an upper bound on the information about the unlearning data set De that can be read out from observing the unlearned model W  PW |Wl,De [2].
As discussed in Section 4.1, designing the unlearning
mechanism via the forgetting Lagrangian in (12) requires
the optimization to be performed for each selection of the learned model Wl and the data sets D and De. Furthermore, it depends directly on the distribution PW |D. Following the discussion in Section 4.1, we could address this problem by

considering an amortized forgetting Lagrangian approach
so as to optimize a conditional distribution PW |Wl,De that can be instantiated for any choice of learned model Wl, and unlearning data De. We do not pursue this here, since reference [2] shows that an approximate solution PW |Wl,De to problem (12) can be found that does not require a separate
optimization for all D and De.

5.2. A PAC-Bayesian view of forgetting Lagrangian

We now show that the forgetting Lagrangian (12) follows from a specific instantiation of the PAC-Bayesian bound (6) for unlearning mechanisms.

Corollary 5.1 Let the data dependent prior be fixed as P~W |D,De = EPWl|Dr [P~W |Wl ]. Then, for all  > 0, with probability at least 1 - , with   (0, 1), over the random draw of the data set D  PZn and the subset De  D to be removed, the following inequality holds uniformly for all
PW |Wl,De ,

EPWl|D PW |Wl,De [L(W )]



EPWl|D [F L(PW |Wl,De , -1)]

+

1 

log

F L 2

,

(13)

where FL = EPD,De P~W |D,De [exp((L(W ) - L(W |Dr))].
Proof : The proof follows in the same steps as the proof of Corollary 4.1 with A(D, W ) = [L(W ), L(W |Dr)]. Details in Appendix C.

The left-hand side of (13) is the average test loss, and hence the forgetting Lagrangian framework introduced in [2] can be again interpreted as minimizing an upper bound on the average test loss.

6. CONCLUSION
The paper presents a unified PAC-Bayesian framework for the design of machine unlearning algorithms. We show that two unlearning design criteria studied in literature ­ EUBO for variational unlearning [1] and Forgetting Lagrangian [2] can be interpreted as IRM obtained via specific instantiation of the proposed PAC-Bayesian framework.

A. PROOF OF LEMMA 3.1
The PAC-Bayesian bound in (6) is obtained by first using a Markov inequality, and then applying change of measure as detailed next. The Markov inequality for a non-negative random variable Y states that with probability at least 1 - , with   (0, 1), we have Y  E[Y ]/. Precisely, the following inequality holds,
Pr(Y  E[Y ]/)  1 - .

We specialize the above Markov inequality to our setting by
taking Y = EQW|D [exp(f (D, W ))]. Note that Y is a function of the random variable D, and that EPZn [Y ] = . Markov's inequality then gives that

PrD

EQW |D [exp(f (D, W )]



 

 1 - .

(14)

Applying change of measure then results in the following inequality

PrD

PW |D ,EPW |D

exp

f (D, W ) - log

PW |D(W |D) QW |D(W |D)



 

 1 - .

(15)

Using Jensen's inequality to take expectation inside the exponential term, and subsequently applying log on both sides of the inequality then results in

PrD PW |D, EPW|D [f (D, W )] - DKL(PW |D||QW |D)



log

 

 1 - .

(16)

Finally, noting that f (D, W ) = F (A(D, W )) where F is convex, and applying Jensen's inequality again results in the PAC-Bayesian bound in (6).

B. PROOF OF COROLLARY 4.1

The required bound follows by instantiating the general PAC-
Bayesian bound in Lemma 3.1 for unlearning. As such, the
unlearning PAC-Bayesian bound depends on the cascade op-
eration of learning a model Wl  PW |D, and subsequent unlearning using PW |Wl,T (D),De . This process is subject to the random draw of D  PZn, and to the random selection of the subset De  D. Consequently, we consider the prior in Lemma 3.1 as QW |D,De , depending on both data sets D and De.
Lemma 3.1 then gives that with probability at least 1 -  over the random draw of data set D, and that of the unlearning data set De, the following inequality holds uniformly over all distributions PW |D,De ,

F (EPW |D,De [A(D, W )]) - DKL(PW |D,De ||QW |D,De )



log

 

.

(17)

In particular, (17) holds for all learning mechanisms PW |D and unlearning mechanisms PW |Wl,T (D),De such that PW |D,De = EPWl|D [PW |Wl,T (D),De ] is the marginal of the joint distribution PWl|D  PW |Wl,T (D),De .
To get to (11), we consider the PAC-Bayesian bound
(17) for a fixed learning algorithm PW |D. Further, we

take QW |D,De = PW |D, A(D, W ) = [-mEPZ [log PZ|W ], log PDe|W ] and F (a, b) = a - b. Noting that PW |D,De = EPWl|D [PW |Wl,T (D),De ], we use the biconvexity of KL divergence to upper bound
DKL(PW |D,De ||PW |D)  EPWl|D [DKL[PW |Wl,T (D),De ||PW |D ].
Using all these in (17) yields the required bound in (11).
C. PROOF OF COROLLARY 5.1
The proof follows the same line as the proof of Corollary 4.1 in Appendix B. To get to (13), we use (17) with PW |D,De = EPWl|D [PW |Wl,T (D),De ], QW |D,De = EPWl|Dr [P~W |Wl ] and A(D, W ) = [L(W ), L(W |Dr)].
D. REFERENCES
[1] Quoc Phong Nguyen, Bryan Kian Hsiang Low, and Patrick Jaillet, "Variational bayesian unlearning," Advances in Neural Information Processing Systems, vol. 33, 2020.
[2] Aditya Golatkar, Alessandro Achille, and Stefano Soatto, "Eternal sunshine of the spotless net: Selective forgetting in deep networks," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 9304­9312.
[3] Tong Zhang, "Information-Theoretic Upper and Lower Bounds for Statistical Estimation," IEEE Transactions on Information Theory, vol. 52, no. 4, pp. 1307­1321, 2006.
[4] Nicholas Carlini, Chang Liu, U´ lfar Erlingsson, Jernej Kos, and Dawn Song, "The secret sharer: Evaluating and testing unintended memorization in neural networks," in 28th {USENIX} Security Symposium ({USENIX} Security 19), 2019, pp. 267­284.
[5] Yinzhi Cao and Junfeng Yang, "Towards making systems forget with machine unlearning," in 2015 IEEE Symposium on Security and Privacy. IEEE, 2015, pp. 463­480.
[6] Antonio Ginart, Melody Y Guan, Gregory Valiant, and James Zou, "Making ai forget you: Data deletion in machine learning," arXiv preprint arXiv:1907.05012, 2019.
[7] Lucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot, "Machine unlearning," arXiv preprint arXiv:1912.03817, 2019.

[8] David A McAllester, "PAC-Bayesian Model Averaging," in Proc. of Annual Conf. Computational Learning Theory (COLT), July 1999, pp. 164­170.
[9] Pascal Germain, Alexandre Lacasse, Franc¸ois Laviolette, and Mario Marchand, "Pac-bayesian learning of linear classifiers," in Proceedings of the 26th Annual International Conference on Machine Learning, 2009, pp. 353­360.
[10] Sharu Theresa Jose and Osvaldo Simeone, "Free energy minimization: A unified framework for modeling, inference, learning, and optimization [lecture notes]," IEEE Signal Processing Magazine, vol. 38, no. 2, pp. 120­ 125, 2021.
[11] Omar Rivasplata, Ilja Kuzborskij, Csaba Szepesva´ri, and John Shawe-Taylor, "Pac-bayes analysis beyond the usual bounds," arXiv preprint arXiv:2006.13057, 2020.
[12] Ayush Sekhari, Jayadev Acharya, Gautam Kamath, and Ananda Theertha Suresh, "Remember what you want to forget: Algorithms for machine unlearning," arXiv preprint arXiv:2103.03279, 2021.
[13] David A McAllester, "PAC-Bayesian stochastic model selection," Machine Learning, vol. 51, no. 1, pp. 5­21, 2003.
[14] Benjamin Guedj, "A primer on PAC-Bayesian learning," arXiv preprint arXiv:1901.05353, 2019.
[15] Gintare Karolina Dziugaite and Daniel M Roy, "Datadependent pac-bayes priors via differential privacy," arXiv preprint arXiv:1802.09583, 2018.

