Near Optimal Stochastic Algorithms for Finite-Sum Unbalanced Convex-Concave Minimax Optimization
Luo Luo  Guangzeng Xie  Tong Zhang § Zhihua Zhang ¶

arXiv:2106.01761v1 [math.OC] 3 Jun 2021

Abstract

This paper considers stochastic first-order algorithms for convex-concave minimax problems of the

form minx maxy f (x, y), where f can be presented by the average of n individual components which are

L-average could find

smooth. For µx-strongly-convex-µy a -saddle point of the problem in

-Os~tronng(ly-cno+ncavxe)(setnti+ng, yw)elopgr(o1p/o)se

a new method which stochastic first-order

complexity, where x L/µx and y L/µy. This upper bound is near optimal with respect to , n, x

and y simultaneously. In addition, the algorithm is easily implemented and works well in practical. Our

methods can be extended to solve more general unbalanced convex-concave minimax problems and the

corresponding upper complexity bounds are also near optimal.

1 Introduction

This paper studies the following finite-sum minimax problem:

1n

min max f (x, y)

xX yY

n

fi(x, y),

(1)

i=1

where X and Y are convex and closed. Our goal is to find the approximate saddle point of problem (1) to

guarantee the duality gap no larger than .

The formulation (1) includes a lot of machine learning applications such as AUC maximization [11, 13,

22, 40], robust optimization [7, 38], adversarial learning [30] and reinforcement learning [33]. We study

the fundamental setting that each component fi(x, y) is convex in x and concave in y; {fi(x, y)}ni=1 are L-average-smooth; and f (x, y) is µx-strongly-convex in x and µy-strongly-convex in y. In particular, we

focus on the unbalanced case that the condition numbers x L/µx and y L/µx could be quite different. Without loss of generality, we suppose x is larger than y.

Most existing algorithms [1, 5, 10, 16, 23, 25, 28] for general strongly-convex-strongly-concave (SCSC)

minimax optimization do not consider the difference between two condition numbers x and y, which leads

to their upper bound complexities depend methods for the unbalanced problem with

Oo~n(maxx{yloxg, 3(y1}/.)L)ingreatdiaeln.t[2c1a]llfis,r1stneparrolpyomseadtcphrionxgimthael

point lower

bound [14, 41] of the deterministic algorithm. Wang and Li [34], Xie et al. [37] improved Lin et al. [19]'s

results under refined smoothness assumption and bilinear setting. Unfortunately, these methods [21, 34] are

based on full gradient oracle and they ignore the finite-sum structure in the objective function.

In practice, the number of components n could be very large. It is natural to use stochastic first-order

oracle (SFO) algorithms to reduce the cost of gradient based methods. The SFO algorithms and their

optimality are well-studied for minimization problems [2, 6, 8, 15, 19, 35, 42], but the related theory for

minimax optimization is still imperfect. In the balanced case of x = y = , Palaniappan and Bach [28]

Equal Contribution Department of Mathematics, The Hong Kong University of Science and Technology; luoluo@ust.hk Academy for Advanced Interdisciplinary Studies, Peking University; smsxgz@pku.edu.cn §Department of Mathematics, The Hong Kong University of Science and Technology; tongzhang@ust.hk ¶School of Mathematical Sciences, Peking University; zhzhang@math.pku.edu.cn 1We use notation O~(·) to hide logarithmic factors of n, x and y in complexities.

1

Algorithm

SFO Complexity

Extragradient

O nmax log

1 

SVRG/SAGA/SVRE

O

n + 2max

log

1 

A-SVRG/A-SAGA

O~

 (n + nmax) log

1 

L-SVRE

O

 (n + nmax) log

1 

MINIMAX-APPA

O~

nxy log3

1 

PBR

O~

nxy log

1 

AL-SVRE

O~





n( n + x)( n + y) log

1 

Lower Bound

the expression of (2)

Loop

Reference

1

[10, 16]

1

[5, 28]

2

[5]

1 [1] + Theorem 2

3

[21]

3

[34]

2

Corollary 1

­

[12]

Table 1: Comparison of SFO complexities in the (µx, µy)-convex-concave setting.

first introduced SVRG/SAGA [6, 15] to solve O~((n + 2) log(1/)) SFO upper bound. If 

thegeneral > n, the

formulation (1) in big data regime and leading term can be reduced to O(n +

obtained  n) by

introducing extragradient (EG) or proximal point iterations [5, 23]. Recently, Vladislav et al. [32]proposed adnesaigcnceelderaatceodmsptolicchaatsetdicamlgeotrhitohdmfowr huincbhalcaonncteadinpsrothbrleeme-l(o1o)pws hiteenrabtoiothns, xobatnadininy garOe (larngerxthya)nSFOn.uTphpeeyr bound under stronger smoothness assumptions2.

In fact, SCSC minimax problems with finite-sum structure can be classified by three types of relationships

among x, y and n (recall we have assumed x  y) :





(a) f (x, y) is extremely ill-conditioned w.r.t two variables: x = ( n) and y = ( n);





(b) f (x, y) is only extremely ill-conditioned w.r.t y: x = O( n) and y = ( n);





(c) the number of components is extremely large: x = O( n) and y = O( n).

Han et al. [12] provided a general SFO lower bound for all three cases as follows

 

n + nxy log(1/) ,





for x = ( n) and y = ( n);

 


n + n3/4y log(1/) ,





for x = O( n) and y = ( n);

(2)

 (n),





for x = O( n) and y = O( n).

We can observe [32]'s algorithm

that has

the the

complexity of SVRG/SAGA upper bound of the form O

([2n8+] is nneaxroyp) tloimg(a1l/in))th,ebcuatseitorfe(qcu)i.reVslathdeislsatvroentgaerl.

assumption that each component fi is L-smooth. We should point out that Case (b) is also an important

setting, which is useful to establish the efficient algorithms for ill-conditioned strongly-convex-nonconcave (or

nonconvex-strongly-concave) minimax optimization [20, 21, 24].

In this paper, we propose a Catalyst-type algorithm that we call accelerated loopless stochastic variance

reduced extragradient (AL-SVRE), whose iteration inexactly solves more well-conditioned minimax problems.

We use loopless stochastic variance reduced extragradient (L-SVRE) [1] as the sub-problem solver and revise

Alacaoglu and Malitsky [1]'s analysis to show L-SVRE solves balanced SCSC minimax problem in optimal

SFO complexity. Combining an appropriate choice of parameters, AL-SVRE could find an approximate

saddle point of our main problem (1) with SFO complexity nearly matching Han et al. [12]'s lower bound

(2). Additionally, AL-SVRE only applies one times Catalyst acceleration on AL-SVRE, which leads to the

algorithm has two-loops of iterations in total and it is easily implemented. The empirical studies on AUC

maximization [13, 29, 40] and wireless communication [3, 9, 39] problems show that AL-SVRE performs better

2Vladislav et al. [32] assume each the weaker assumption that {f }ni=1 is

fi is Li-smooth and f is L-smooth, where L L-average-smooth as Definition 1. Please see

=

1 n

n i=1

Li,

while

this

paper

only

the detailed discussion for these two

requires different

settings in appendix.

2

Algorithm

SFO Complexity

Loop Reference

DIAG MINIMAX-APPA

O~ nx

L 

log2

1 

O~ n

x L 

log3

1 

A-SVRG/A-SAGA

AL-SVRE

O~

Lower Bound

O~

n + (3x/2 + n3/4)

L 

log(

1 

)

n+

nLx 

+

n3/4

 x

+ n3/4

L 

log(

1 

 n+

nLx 

+ n3/4x

+ n3/4

L 

3

[31]

3

[21]

2

[39]

2 Corollary 1

­

[12]

Table 2: Comparison of SFO complexities in the (µx, 0)-convex-concave setting, where the result of ASVRG/A-SAGA [39] requires the assumption of  < µy.

than baselines. In contrast, previous Catalyst-type methods [21, 32, 34] for such an unbalanced problem need twice Catalyst acceleration (three-loops of iterations), making these algorithms almost impractical.
We can also apply AL-SVRE to solve more general finite-sum convex-concave minimax problems. If we suppose f (x, y) is µy-strongly-convex in y and allow it could be non-strongly-convex in x, AL-SVRE could find an -saddle point of (1) in

O~ n + Dx

nLy /

+

n3/4

 y

+

n3/4Dx

L/

(3)

SFO complexity, where Dx is the diameter of X . The upper bound (3) nearly matches the lower bounds with respect to , L, µy and n simultaneously, and we do require any additional assumption on L, µy, n and , while the state-of-the-art method [39] implicitly requires that  < µy.
More general, if we only suppose the objective function is convex in x and concave in y, the algorithm

could find an -saddle point in

O~

n

+

 nLDx

Dy

/

+

n3/4

(Dx

+

Dy

)

L/

(4)

SFO complexity, where Dx and Dy are diameters of X and Y respectively. Note that the upper bound (4) nearly matches the lower bounds [12] in such a setting. Compared with the the best known stochastic SFO algorithm L-SVRE [1], our result additionally trades off the difference between Dx and Dy.
We present comparisons between our results and existing results in Table 1 for strongly-convex-stronglyconcave setting, in Table 2 for strongly-convex-concave setting, and in Table 3 for convex-concave settings. To the best of our knowledge, the proposed AL-SVRE is the first algorithm which attains near optimal SFO complexities for all the above settings.

2 Notation and Preliminaries

In this section, we present the notation and some definitions used in this paper.

Definition 1. For any differentiable function  : Rd  R, we say  is L-smooth for some L > 0 if for any z, z  Rd, it holds that (z) - (z ) 2  L z - z 2.
Definition 2. For any n differentiable functions {i : Rd  R}ni=1, we say {i}ni=1 is L-average smooth for some L > 0 if for any z, z  Rd, it holds that

1n n

i(z) - i(z

)

2 2

 L2

z-z

2 2

.

i=1

Definition 3. For a differentiable function  : Rd  R, we say  is convex if for any z, z  Rd, it holds

that (z )  (z) +

(z), z

-z

.

We

say



is

µ-strongly-convex

for

some

µ>0

if

(·)

-

µ 2

·

2 2

is

convex.

We also say  is concave (µ-strongly-concave) if - is convex (µ-strongly-convex).

3

Algorithm

SFO Complexity

Loop Reference

Extragradient

MINIMAX-APPA

L-SVRE

AL-SVRE

O~

Lower Bound

O

max{Dx2

,

Dy2

}

nL 

O~

nLDx Dy 

log3

1 



O max{Dx2, Dy2}(n +

nL 

)



n+

nLDx Dy 

+ n3/4(Dx

+ Dy)

L 

log

1 



 n+

nLDx Dy 

+ n3/4(Dx

+ Dy)

L 

1

[10, 16]

3

[21]

1

[1]

2 Corollary 1

­

[12]

Table 3: Comparison of SFO complexities in convex-concave setting.

Definition 4. For any function  : Rd  R, we say  is µ-strongly-convex for some µ > 0 if for any

z, z  Rd, it holds that (z )  (z) +

(z), z - z

+

µ 2

z - z 22. Furthermore, we say  is µ-strongly-

concave if - is µ-strongly-convex. If we set µ = 0, then it recovers the definitions of general convexity and

concavity.

Definition 5. For any function f : Rdx × Rdy  R and µx, µy  0, we say f is (µx, µy)-convex-concave if for any x  Rdx and y  Rdy , it holds that f (x, ·) is µy-strongly-concave and f (·, y) is µx-strongly-convex.
In Definition 5, we allow both µx and µy to be zero. The notations (0, 0)-convex-concave means the function is general convex-concave, and (0, µy)-convex-concave means it is µy-strongly-concave in y but possibly non-strongly-convex in x. Similarly, we use (µx, 0)-convex-concave to present the function is µx-strongly-convex in x but possibly non-strongly-concave in y.
We are interested is finding an approximate saddle point which is defined as follows.

Definition 6. For the minimax optimization problem (1), (x^, y^)  X × Y is said to be an -saddle point if

max f (x^, y) - min f (x, y^)  .

yY

xX

We also conduct the projection operator to address the constraints in our concern.

Definition 7. We define the projection of z onto the convex and compact set C as

PC(z) = arg min

u-z

2 2

.

uC

This paper focuses on solving finite-sum minimax problems by SFO algorithms and we give the formal definition as follows.

Definition 8. Consider a stochastic optimization algorithm A to solve Problem (1). Suppose an initial point (x(0), y(0)) is given, and let (x(t), y(t)) denote the point obtained by A at time-step t. The algorithm is said
to be an SFO algorithm if for any t > 0, we have

x(t) = PX x~(t) and y(t) = PY y~(t) ,

where

x~(t)  span x(0), · · · , x(t-1), xfit x(0), y(0) , · · · , xfit x(t-1), y(t-1) , y~(t)  span y(0), · · · , y(t-1), yfit x(0), y(0) , · · · , yfit x(t-1), y(t-1) ,

and it is drawn from {1, . . . , n}.

4

Algorithm 1 L-SVRE ({fi(x, y)}ni=1, (x0, y0), , p, T ) 1: Initialize:  = 1 - p, w0 = z0.

2: for k = 0, 1, . . . , T - 1 do

3: z¯k = zk + (1 - )wk.

4: zk+1/2 = PZ (z¯k -  g(wk)). 5: Draw an index i  [n] uniformly at random.

6: zk+1 = PZ (z¯k -  [g(wk) + gi(zk+1/2) - gi(wk)]).

7: wk+1 = 8: end for

zk+1, wk ,

with probability p, with probability 1 - p.

9: Output: (xT , yT ).

3 Accelerating Unbalanced Convex-Concave Optimization
In this section, we first introduce loopless stochastic variance reduced extragradient (L-SVRE) [1], and show it can be used to solve balanced strongly-convex-strongly-concave (SCSC) minimax with optimal SFO complexity. Then we propose a Catalyst-type scheme to accelerate L-SVRE, deriving our new algorithm Accelerated Loopless Stochastic Variance Reduced Extragradient (AL-SVRE) whose SFO upper bound is near optimal for different types of unbalanced concave-concave minimax problems.

3.1 The Optimal SFO Algorithm for Balanced SCSC Minimax

We start our discussion from the optimality of balanced SCSC problems such that the objective function f (x, y) is (µ, µ)-convex-concave with some µ > 0. Theorem 1 gives an SFO lower bound ((n + n) log(1/)) for finding an approximate saddle point w.r.t. the Euclidean distance under average smooth assumptions. This result is slightly difference from previous work which is based on duality gap convergence [12] or under assumption that each component fi is L-smooth [36].

Theorem dimension

1. For anySFO algorithm d = O(n + nL/µ log(1/))

A and L, µ, n,  such that L/µ and functions {fi(x, y)}ni=1 : Rd

> 2 and  < 0.003, there exist × Rd  R which satisfy {fi}ni=1

a is

L-average smooth, f is (µ, µ)-convex-concave. In order to find an approximate saddle point (x^, y^) such that

E

x^ - x

2 2

+

y^ - y

2 2

 ,

 Algorithm A needs at least (n+ nL/µ log(1/)) steps.

Recently, Alacaoglu and Malitsky [1] proposed the L-SVRE algorithm, which combines the idea of loopless SVRG [17] and extragradient [16]. The original motivation of L-SVRE is to solve variational inequalities. We revise its analysis to adapt the standard finite-sum minimax problem (1) under the (µ, µ)-convex-concave assumption. We present the details of L-SVRE in Algorithm 1 and show its convergence behavior in Theorem 2, where we denote g(z) = (xf (z), -yf (z)), gi(z) = (xfi(z), -yfi(z)) and z = (x, y)  Rdx × Rdy .

Theorem 2. Assume that µx = µy = µ and {fi}ni=1 is L-average-smooth. Then Algorithm 1 with probability

parameter

p=

1 2n

and

stepsize



=

1 4 nL

satisfies

Ek

zk - z

2 2

4

z0 - z

2 2

1

1-



4(n + 2 nL/µ)

k
.

(5)

The result of inequality (5) means that L-SVRE could find (x^, y^) satisfying E[

x^ - x

2 2

+

y^ - y

22]  

with O((n+ nL/µ) log(1/)) iterations. Since we select p = 1/2n in the theorem, each iteration requires

O(1) SFO calls in expectation. Hence, we have proved that the upper bound complexity of L-SVRE matches

the lower bound shown in Theorem 1.

5

Algorithm 2 AL-SVRE {fi(x, y)}ni=1, (x0, y0), , q, K, p, {k}Kk=1, {Tk}Kk=1 1: Initialize:  = 11-+qq and u0 = x0.

2: for k = 1, · · · , K do

3:

(x~k, y~k) = L-SVRE

fk (x,

y)

+

 2

4: xk = PX x~k - kxFk x~k, y~k

x - uk-1

2 2

ni=1, xk-1, yk-1, k, p, Tk

5: yk = PY y~k + kyFk x~k, y~k

6: uk = xk + (xk - xk-1)

7: end for

8: Output: (xK , yK ).

3.2 Acceleration for Unbalanced SCSC Minimax

Note that the convergence result of L-SVRE is not perfect when the objective function is unbalanced. For

example, we consider problem (1) in the case of x =  > n > y = O(1). Then Theorem 2 cannot leverage

tchoemwpleelxl-ictoynOd~i(tnionedloags3s(u1m/p)t)ioanchonievyedanbdy

leads to SFO upper bound O ( deterministic algorithms [21].

n log(1/))), which is worse than

Our key intuition to establish a better SFO algorithm for unbalanced SCSC minimax problems is taking

the advantage of the optimality of L-SVRE in the balanced case. We present the new method AL-SVRE in

Algorithm 2, whose iteration applies L-SVRE to solve the following sub-problem

(x~k, y~k)  arg min max Fk(x, y)
xX yY

 f (x, y) +
2

x - uk-1

2 2

.

(6)

Since Theorem 2 only provides the convergence rate for L-SVRE by distance, we introduce additional projection gradient iterations (Lines 6-7 of Algorithm 2) on the output of the sub-problem solver. These steps help us control the accuracy w.r.t. primal and dual functions, which is helpful to establish the convergence results w.r.t. the duality gap in our main result Theorem 3.

Lemma 1. Suppose the function f (x, y) : X × Y  R is (µx, µy)-convex-concave and L-smooth. Denote the saddle point of minxX maxyY f (x, y) by (x, y) and let the condition numbers of f be x = L/µx and
y = L/µy. Assume that the point (x^, y^) satisfies

x^ - x

2 2

+

y^ - y

2 2



.

Then for x~ = PX (x^ - xf (x^, y^)) and y~ = PY (y^ + yf (x^, y^)), it holds that

max f (x~, y) - f (x, y) 
yY

 2(1 + L) + 2(1 + L)2 + 2

 yL + 2 ,

f (x, y) - min f (x, y~) 
xX

 2(1 + L) + 2(1 + L)2 + 2

 xL + 2 .

Theorem

3.

If

running

Algorithm

2

with

parameters



 0,

q

= µx/(µx + ),

p = 1/2n,

k

=

1 4 n(L+)

and





Tk =

4

2 n(L + ) n+
min{µx + , µy}

log

12

1

2 -



+

µ1x7µ28ym(Lin{+µx,)(µ7y(}L(1+-))+2(2qn-µy)))2

,

where  < q, then we have

E

max f (xk, y) - min f (x, yk)

yY

xX



916f



(xL + n(L +

 µx( q

-

)2

)) 2y (1

-

)k ,

where f = maxyY f (x0, y) - minxX f (x, y0).

6

Theorem 3 shows the -saddle point can be obtained by calling K = O~ (µx+)/µx log(1/) times

AL-SVRE to solve the sub-problem. By minimizing the product of dominant terms (ignore logarithmic



terms)

(µx + )/µx and

n

+

2 n(L+) min{µx+,µy }

in K and Tk respectively, we decide the choice of  as follows

(suppose µx  µy)





µy 

- 

µx,

if

y



n, 

 = L/ n - µx, if x > n > y,

(7)

0,

otherwise,

Combing the result of Theorem 3 and Eq.(7), we immediately obtain the following corollary, which implies AL-SVRE has near optimal SFO upper bound for unbalanced SCSC minimax optimization.

Corollary and letting

1.asSu(7p)p,ose=P0ro.5bleqma(n1d)

satisfies

µx

<

µy .

If

running

AL-SVRE

with

the

setting

of

Theorem

3

K=

2 q log

 10992 nf

y

3x

,

then we have

E max f (xK , y) - min f (x, yK )  

yY

xX

with the number of SFO calls at most





O n( n + x)( n + y) log(nx) log(nf yx/) .

The design of AL-SVRE aims to sufficiently take the advantage of AL-SVRE's optimality for the balanced SCSC minimax problem. Hence, the choice of  makes the sub-problem (6) more balanced than the main problem (1). In contrast, previous catalyst-type methods [21, 32, 34] select a larger  to guarantee the subproblem be well-conditioned in one of the variables. However, such a strategy leads to that the sub-problem is still so unbalanced and we need two-loops of iterations to solve it. As a result, the implementation of the whole procedure is more complicated and almost cannot be used in practical.

4 Extensions to Non-SCSC Minimax
We can also apply AL-SVRE to solve convex-concave minimax without SCSC assumption.

4.1 Convex-Strongly-Concave Case

If the objective function f (x, y) is (0, µy)-convex-concave, we construct the auxiliary SCSC function to approximate it as follows:

f,x0 (x, y)

 f (x, y) + 4Dx2

x - x0

2 2

.

(8)

Then the difference of duality gaps between f,x0 and f should be small. The following lemma presents this fact formally.

Lemma 2. Suppose that f (x, y) is (0, µy)-convex-concave and x  X . Consider the function f,x0 (x, y) defined as (8). Then for any (x^, y^)  X × Y, we have



max f (x^, y)
yY

-

min f (x, y^)
xX



2

+

max
yY

f,x0

(x^,

y)

-

min
xX

f,x0

(x,

y^).

7

Lemma 2 means that any /2-saddle-point of f,x0 is also an -saddle-point of f . Hence, we can directly run AL-SVRE on f,x0 and connect Lemma 2 and Theorem 3 to establish the convergence result for (0, µy)-convex-concave minimax optimization as follows.
Corollary 2. Suppose f (x, y) is convex-concave in Problem (1). The total complexity of SFO calls for finding an /2-saddle-point of f,x0 , which is also an -saddle-point of f , is

O~ n + Dx

nLy 

+

n3/4

 y

+ n3/4Dx

L 

.

Recently, Yang et al. [39] also studied Catalyst acceleration for strongly-convex-concave minimax optimization, but they only considered the special case of  < µy which leads to that their result does not match the lower bound.

4.2 Convex-Concave Case

In general convex-concave setting, the unbalance of the problem comes from the difference between diameters Dx and Dy. We introduce the auxiliary SCSC function as follows:

 f,x0,y0 (x, y) = f (x, y) + 8Dx2

x - x0

2 2

-

 8Dy2

y - y0

2 2

.

(9)

Similarly, Lemma 3 shows that any /2-saddle-point of f,x0,y0 is an -saddle-point of f .

Lemma 3. Suppose that f (x, y) is convex-concave. Consider the function

 f,x0,y0 (x, y) = f (x, y) + 8Dx2

x - x0

2 2

-

 8Dy2

y - y0

2 2

,

where (x0, y0)  X × Y. Then for any (x^, y^)  X × Y, we have



max f (x^, y)
yY

-

min f (x, y^)
xX



2

+

max
yY

f,x0

,y0

(x^,

y)

-

min
xX

f,x0

,y0

(x,

y^).

Combining Lemma 3 and Theorem 3, we obtain the convergence result of Corollary 3.

Corollary 3. Suppose f (x, y) is convex-concave in Problem (1). The total complexity of SFO calls for finding an /2-saddle-point of f,x0,y0 , which is also an -saddle-point of f , is



O~ n +

nLDxDy 

+ n3/4(Dx

+

Dy )

L 

.

(10)

The result of Corollary 3 nearly matches the lower bound w.r.t. , L, n, Dx and Dy simultaneously [12]. Note that the best known upper bound in convex-concave setting [1] is optimal to , L and n, but it regards the diameters as constants and does not consider the potential unbalance arose from Dx and Dy.

5 Experiments
We conduct the experiments on AUC maximization [13, 29, 40] and wireless communication [3, 9, 39] problems. We evaluate the performance of AL-SVRE and compare it with baseline algorithms ExtraGradient (EG) [10, 16] and L-SVRE [1]. We summarize the datasets in Table 4. The empirical results in Figure 1 show that our proposed AL-SVRE performs better than the baselines.

8

a9a w8a wireless-500 wireless-1000

n 32,561 45,546

500

1,000

d 123 300

500

1,000

Table 4: Summary of datasets

5.1 AUC Maximization

AUC maximization [13] considers finding the classifier w  Rd on training set {(ai, bi)}ni=1, where ai  Rd and b  {+1, -1}. We denote n+ and n- be the numbers of positive and negative instances respectively and let p = n+/n. Our experiments focus on the unbalanced SCSC minimax formulation for AUC maximization [29, 40]
as follows

min max f (x, y)
xRd+2 yR

1n n fi(x, y; ai, bi, ),
i=1

(11)

where x = [w; u; v]  Rd+2,  > 0 and each component is defined as

 fi(x, y; ai, bi, ) = 2

x

2 2

-

p(1

-

p)y2

+

(1

-

p)

(w

ai - u)2 - 2(1 + y)w

ai

I{bi=1}

+ p (w ai - v)2 + 2(1 + y)w ai I{bi=-1}.

We set  = 10-10 and evaluate all the algorithms on real-world dataset "a9a" and "w8a" [4]. We tune stepsize of EG and L-SVRE (or as sub-problem solver) from {0.02, 0.05, 0.1, 0.2, 0.5}. For L-SVRE, we let p = 1/2n as the setting of Theorem 1. For AL-SVRE, we let  = 0.01, q = x/( + ); Tk = 0.3n for "a9a" and Tk = 0.5n for "w8a". We present the results of epochs against log f (x, y) 2 in Figure 1 (a) and (b).

5.2 The Wireless Communication Problems

We consider the wireless communication problem [3, 39] as follows

min max f (x, y)

1 -

n
log

1+

bixi

,

(12)

xX yY

n
i=1

ai + yi

where X = {x : x 2  R, xi  0} and Y = {y : 1 y = n, yi  0}. We can verify the problem (12) in convex-concave but possibly be neither strongly-convex nor strongly-concave.
We generate two datasets with (i) n = 500, R = 1, b = 1 and a  R500 uniformly from [0, 10]500; (ii) n = 1000, R = 1, b = 1 and a  R1000 uniformly from [0, 50]1000. We tune the stepsize for all the algorithms
from [0.01, 0.1, 1.0] and set p = 2/n like AUC maximization. For AL-SVRE, we use the auxiliary function as
follows

f^(x, y)

 f (x, y) + 8R2

x

2 2

-

 8n2

y-1

2 2

1 =

n
- log

1+

bixi

n
i=1

ai + yi

+

n 8R2

x2i

-

n 8n2

(yi

-

1)2,

where  = 10-6/n. We also set Tk = 500, q = R2/n2 and  = /(4R2) - /(4n2). Since the problem (12) is constrained, we use epochs against the logarithm of the magnitude of gradient mapping

log x - PX (x -  xf (x, y)) 2 + y - PY (y +  yf (x, y)) 2 / for evaluation, where  is set to be 0.1. We present the empirical results in Figure 1 (c) and (d).

9

0

-5

EG

-10

L-SVRE

AL-SVRE

-15

-20 0

200

400

600

(a) AUC, a9a

-4

-6

-8

EG

L-SVRE

-10

AL-SVRE

-12

-14 0

200

400

600

(b) AUC, w8a

0

EG

L-SVRE

-2

AL-SVRE

-4

-6

-8

-10

0

200

400

600

(c) wireless-500

0

-5

-10 EG

L-SVRE

AL-SVRE

-15

0

200

400

600

(d) wireless-1000

Figure 1: We demonstrate comparison on AUC maximization for SCSC minimax in sub-figures (a) and (b); and wireless communication for general convex-concave minimax in sub-figures (c) and (d).

6 Conclusions
In this paper we have studied unbalanced convex-concave minimax problems with finite-sum structure. We have shown the optimality of L-SVRE for balanced SCSC minimax and proposed a near optimal algorithm ALSVRE for unbalanced problems. AL-SVRE only contains two loops of iterations, making its implementation more simple and practical than the existing methods for unbalanced SCSC minimax. We have also extended our algorithm to more general convex-concave minimax problems and showed the near optimality.
It would be interesting to apply our ideas to solve specific convex-concave minimax with refined smoothness assumptions and specific bilinear settings [27, 34, 37]. It is also possible to design accelerated variance reduced algorithms to solve unbalanced convex-concave minimax problems in online settings.

References
[1] Ahmet Alacaoglu and Yura Malitsky. Stochastic variance reduction for variational inequality methods. arXiv preprint arXiv:2102.08352, 2021.
[2] Zeyuan Allen-Zhu. Katyusha: The first direct acceleration of stochastic gradient methods. Journal of Machine Learning Research, 18(1):8194­8244, 2017.
[3] Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.
[4] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1­27:27, 2011. Software available at http://www.csie.ntu. edu.tw/~cjlin/libsvm.
[5] Tatjana Chavdarova, Gauthier Gidel, François Fleuret, and Simon Lacoste-Julien. Reducing noise in GAN training with variance reduced extragradient. In NeurIPS, 2019.
[6] Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In NIPS, 2014.
[7] John C. Duchi and Hongseok Namkoong. Variance-based regularization with convex objectives. Journal of Machine Learning Research, 20(68):1­55, 2019.
[8] Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. SPIDER: Near-optimal non-convex optimization via stochastic path-integrated differential estimator. In NeurIPS, 2018.
[9] Andrey Garnaev and Wade Trappe. An eavesdropping game with SINR as an objective function. In International Conference on Security and Privacy in Communication Systems, pages 142­162. Springer, 2009.
[10] Gauthier Gidel, Hugo Berard, Gaëtan Vignoud, Pascal Vincent, and Simon Lacoste-Julien. A variational inequality perspective on generative adversarial networks. In ICLR, 2019.

10

[11] Zhishuai Guo, Mingrui Liu, Zhuoning Yuan, Li Shen, Wei Liu, and Tianbao Yang. Communicationefficient distributed stochastic AUC maximization with deep neural networks. In ICML, 2020.
[12] Yuze Han, Guangzeng Xie, and Zhihua Zhang. Lower complexity bounds of finite-sum optimization problems: The results and construction. arXiv preprint arXiv:2103.08280, 2021.
[13] James A Hanley and Barbara J McNeil. The meaning and use of the area under a receiver operating characteristic (ROC) curve. Radiology, 143(1):29­36, 1982.
[14] Adam Ibrahim, Waiss Azizian, Gauthier Gidel, and Ioannis Mitliagkas. Linear lower bounds and conditioning of differentiable games. In ICML, 2020.
[15] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In NIPS, 2013.
[16] GM Korpelevich. Extragradient method for finding saddle points and other problems. Matekon, 13(4): 35­49, 1977.
[17] Dmitry Kovalev, Samuel Horváth, and Peter Richtárik. Don't jump through hoops and remove those loops: Svrg and katyusha are better without the outer loop. In ALT, 2020.
[18] Guanghui Lan and Yi Zhou. An optimal randomized incremental gradient method. Mathematical programming, pages 1­49, 2017.
[19] Hongzhou Lin, Julien Mairal, and Zaid Harchaoui. Catalyst acceleration for first-order convex optimization: from theory to practice. Journal of Machine Learning Research, 18(212):1­54, 2018.
[20] Tianyi Lin, Chi Jin, and Michael I. Jordan. On gradient descent ascent for nonconvex-concave minimax problems. In ICML, 2020.
[21] Tianyi Lin, Chi Jin, and Michael I. Jordan. Near-optimal algorithms for minimax optimization. In COLT, 2020.
[22] Mingrui Liu, Zhuoning Yuan, Yiming Ying, and Tianbao Yang. Stochastic AUC maximization with deep neural networks. In ICLR, 2020.
[23] Luo Luo, Cheng Chen, Yujun Li, Guangzeng Xie, and Zhihua Zhang. A stochastic proximal point algorithm for saddle-point problems. arXiv preprint arXiv:1909.06946, 2019.
[24] Luo Luo, Haishan Ye, Zhichao Huang, and Tong Zhang. Stochastic recursive gradient descent ascent for stochastic nonconvex-strongly-concave minimax problems. In NeurIPS, 2020.
[25] Aryan Mokhtari, Asuman Ozdaglar, and Sarath Pattathil. A unified analysis of extra-gradient and optimistic gradient methods for saddle point problems: Proximal point approach. In AISTATS, pages 1497­1507. PMLR, 2020.
[26] Yurii Nesterov. Lectures on convex optimization, volume 137. Springer, 2018.
[27] Yuyuan Ouyang and Yangyang Xu. Lower complexity bounds of first-order methods for convex-concave bilinear saddle-point problems. arXiv preprint arXiv:1808.02901, 2018.
[28] Balamurugan Palaniappan and Francis Bach. Stochastic variance reduction methods for saddle-point problems. In NIPS, 2016.
[29] Zebang Shen, Aryan Mokhtari, Tengfei Zhou, Peilin Zhao, and Hui Qian. Towards more efficient stochastic decentralized learning: Faster convergence and sparse communication. In ICML, 2018.
[30] Aman Sinha, Hongseok Namkoong, and John Duchi. Certifying some distributional robustness with principled adversarial training. In ICLR, 2018.
[31] Kiran K. Thekumparampil, Prateek Jain, Praneeth Netrapalli, and Sewoong Oh. Efficient algorithms for smooth minimax optimization. In NeurIPS, 2019.
11

[32] Tominin Vladislav, Tominin Yaroslav, Borodich Ekaterina, Kovalev Dmitry, Alexander Gasnikov, and Pavel Dvurechensky. On accelerated methods for saddle-point problems with composite structure. arXiv preprint arXiv:2103.09344, 2021.
[33] Hoi-To Wai, Zhuoran Yang, Zhaoran Wang, and Mingyi Hong. Multi-agent reinforcement learning via double averaging primal-dual optimization. In NeurIPS, 2018.
[34] Yuanhao Wang and Jian Li. Improved algorithms for convex-concave minimax optimization. arXiv preprint arXiv:2006.06359, 2020.
[35] Blake Woodworth and Nathan Srebro. Tight complexity bounds for optimizing composite objectives. In NIPS, 2016.
[36] Guangzeng Xie, Luo Luo, Yijiang Lian, and Zhihua Zhang. Lower complexity bounds for finite-sum convex-concave minimax optimization problems. In ICML, 2020.
[37] Guangzeng Xie, Yuze Han, and Zhihua Zhang. DIPPA: An improved method for bilinear saddle point problems. arXiv preprint arXiv:2103.08270, 2021.
[38] Yan Yan, Yi Xu, Qihang Lin, Lijun Zhang, and Tianbao Yang. Stochastic primal-dual algorithms with faster convergence than O(1/ t) for problems without bilinear structure. arXiv preprint arXiv:1904.10112, 2019.
[39] Junchi Yang, Siqi Zhang, Negar Kiyavash, and Niao He. A catalyst framework for minimax optimization. In NeurIPS, 2020.
[40] Yiming Ying, Longyin Wen, and Siwei Lyu. Stochastic online AUC maximization. In NIPS, 2016. [41] Junyu Zhang, Mingyi Hong, and Shuzhong Zhang. On lower iteration complexity bounds for the saddle
point problems. arXiv preprint arXiv:1912.07481, 2019. [42] Dongruo Zhou and Quanquan Gu. Lower bounds for smooth nonconvex finite-sum optimization. In
ICML, 2019.
12

The appendix are organized as follows. Section A presents several lemmas for further theoretical analysis. Section B provides the proof of Theorem 2, which is the convergence result of L-SVRE for balanced SCSC setting. Section C gives the detailed analysis for AL-SVRE, including the proof of our main results Theorem 3 and Corollary 1. Section D provide the proof of Theorem 1, which implies L-SVRE is an optimal SFO algorithm for balanced finite-sum SCSC minimax. Section E provides the proof of Lemma 2. We can also prove Lemma 3 by almost the same way as Section E. Finally, we discuss the different smoothness assumptions between this paper and Alacaoglu and Malitsky [1]'s work in Section F.

A Technical Lemmas

We first present some useful tools for the analysis of constrained optimization.
Lemma 4 ([26, Theorem 2.2.9 and 2.2.12]). Let z = arg minzC Q(z), where Q is smooth and strongly-convex; C is convex and compact. Then, for any z  C and  > 0, we have
Q(z), z - z  0 and z = PC (z - Q(z)) .
Lemma 5 ([26, Corollary 2.2.3]). Given a convex and compact set C  Rd; and any u, v  Rd, we have PC(u) - PC(v) 2  u - v 2.
Lemma 6. Given a convex and compact set C  Rd, for any u  Rd and v  C, we have

PC(u) - u, PC(u) - v  0.

Proof.

Let

Q(z) =

1 2

z - u 22, then PC(u) = arg minzC Q(z). Using Lemma 4, we have

PC(u) - u, v - PC(u) = Q(PC(u)), v - PC(u)  0.

Then we provides some properties for convex-concave functions.

Lemma 7 ([21, Lemma B.2]). Assume that f (x, y) : X × Y  R is L-smooth and (µx, µy)-convex-concave. We define

yf (·) xf (·)

arg max f (·, y), f (·)
yY
arg min f (x, ·), f (·)
xX

max f (·, y),
yY
min f (x, ·).
xX

Then, there holds that

(a) the function yf(·) is y-Lipschitz, (b) the function f (·) is 2yL-smooth and µx-strongly convex with f (·) = xf (·, yf(·)), (c) the function xf (·) is x-Lipschitz, (d) the function f (·) is 2xL-smooth and µy-strongly concave with f (·) = xf (xf (·), ·).

Lemma 8. Assume that f (x, y) : X × Y  R is differentiable and (µx, µy)-convex-concave, then for any z1 = (x1, y1), z2 = (x2, y2)  X × Y, we have

g(z1) - g(z2), z1 - z2

 µx

x1 - x2

2 2

+

µy

y1 - y2

2 2

.

(13)

where g(x, y) = (xf (x, y), -yf (x, y)).

13

Proof. According to f (·, y) is µx-strongly-convex, we have f (x2, y1) - f (x1, y1)  xf (x1, y1), x2 - x1 f (x1, y2) - f (x2, y2)  xf (x2, y2), x1 - x2

+ µx 2
+ µx 2

x2 - x1 2 , x2 - x1 2 .

Similarly, by strongly-convexity of -f (x, ·), we have

-f (x1, y2) + f (x1, y1)  - yf (x1, y1), y2 - y1 -f (x2, y1) + f (x2, y2)  - yf (x2, y2), y1 - y2

+ µy 2
+ µy 2

y2 - y1 2 , y2 - y1 2 .

The desired result just follows from adding above four inequalities together.

Lemma

9.

Assume

that

{fi}ni=1

is

L-average-smooth,

then

f (z) =

1 n

fi

(z)

is

L-smooth.

Proof. Note that

1n

f (z1) - f (z2) 2 = n (fi(z1) - fi(z2))

i=1

2

1n

 n

fi(z1) - fi(z2) 2

i=1



1n n

fi(z1) - fi(z2)

2 2

i=1



L2

z1 - z2

2 2

=

L

z1 - z2

2,

where the first inequality is according to triangle inequality, the second inequality follows from AM-QM inequality, and the last inequality is due to average-smoothness of {fi}ni=1.

B Proof of Theorem 2

Following proof is adapted from the proof of Theorem 4.9 in [1].

Proof. By Lemma 6, we know that

zk+1/2 - z¯k +  g(wk), zk+1 - zk+1/2  0, zk+1 - z¯k +  [g(wk) + gi(zk+1/2) - gi(wk)], z - zk+1  0.

Summing above two inequalities, we have

zk+1 - z¯k, z - zk+1 + zk+1/2 - z¯k, zk+1 - zk+1/2 + g(wk) + gi(zk+1/2) - gi(wk), z - zk+1/2
+ gi(zk+1/2) - gi(wk), zk+1/2 - zk+1  0.

Since 2 a, b

=

a+b

2 2

-

a

2 2

-

b 22, the first term in inequality (14) can be written as

2 zk+1 - z¯k, z - zk+1 =2 zk+1 - zk - (1 - )wk, z - zk+1 =2 zk+1 - zk, z - zk+1 + 2(1 - ) zk+1 - wk, z - zk+1

=

zk - z

2 2

-

zk+1 - z

2 2

-

zk+1 - zk

2 2

+ (1 - )

wk - z

2 2

-

zk+1 - z

2 2

-

zk+1 - wk

2 2

=

zk - z

2 2

+

(1

-

)

wk - z

2 2

-

zk+1 - z

2 2

-

zk+1 - zk

2 2

-

(1

-

)

zk+1 - wk

2 2

.

(14) (15)

14

Similarly, the second term in inequality (14) can be written as

2 zk+1/2 - z¯k, zk+1 - zk+1/2

=

zk - zk+1

2 2

+

(1

-

)

wk - zk+1

2 2

-

zk+1/2 - zk+1

2 2

-

zk+1/2 - zk

2 2

-

(1

-

)

zk+1/2 - wk

2 2

.

(16)

Using the fact Ek[gi(z)] = g(z), the expectation of third term in inequality (14) can be bounded as

2Ek g(wk) + gi(zk+1/2) - gi(wk), z - zk+1/2

= 2 g(zk+1/2), z - zk+1/2

 2 g(z), z - zk+1/2

- 2µ

zk+1/2 - z

2 2

 -µEk

zk+1 - z

2 2

+ 2µEk

zk+1/2 - zk+1

2 2

,

(17)

where the first inequality follows from Lemma 8, and the last inequality is according to Lemma 4 and

a+b

2 2



2

a

2 2

+

2

b

2 2

.

Moreover, by Young's inequality 2 a, b



a

2 2

+

1 

b

2 2

and

L-average-smoothness

of

{fi}ni=1,

there

holds

Ek 2 gi(zk+1/2) - gi(wk), zk+1/2 - zk+1

 2 2Ek

gi(zk+1/2) - gi(wk)

2 2

1 + 2 Ek

zk+1/2 - zk+1

2 2

(18)

 2 2L2

zk+1/2 - wk

2 2

+

1 2 Ek

zk+1/2 - zk+1

2 2

.

Plugging results of (15), (16), (17) and (18) into inequality (14), we obtain that



zk - z

2 2

+

(1

-

)

wk - z

2 2

-

Ek

zk+1 - z

2 2

- Ek

-

zk+1/2 - zk

2 2

-

(1

-

)

zk+1/2 - wk

2 2

-  µEk

zk+1 - z

2 2

+ 2 µEk

zk+1/2 - zk+1

2 2

+ 2 2L2

zk+1/2 - wk

2 2

+

1 2 Ek

zk+1/2 - zk+1

2 2

 0,

zk+1/2 - zk+1

2 2

that is

(1 +  µ)Ek

zk+1 - z

2 2



zk - z

2 2

+

(1

-

)

wk - z

2 2

-

1 - 2 µ
2

Ek

- (1 -  - 2 2L2)

zk+1/2 - wk

2 2

.

zk+1/2 - zk+1

2 2

Consequently,

with

setting



=

1 - p,

p

=

1 2n

and



=

1 4 nL

,

we

know

that

1 2

- 2 µ



0,

1 -  - 2 2L2



0

and

(1 +  µ)Ek

zk+1 - z

2 2

 (1 - p)

zk - z

2 2

+

p

wk - z

2 2

.

(19)

On the other hand, by definition of wk+1, we have

Ek

wk+1 - z

2 2

= (1 - p)

wk - z

2 2

+

pEk

zk+1 - z

2 2

.

(20)

Following from results of (19) and (20), there holds

(1 +  µ)Ek

zk+1 - z

2 2

+ cEk

wk+1 - z

2 2

15

 (1 - p)

zk - z

2 2

+

p

wk - z

2 2

+

c(1

-

p)

wk - z

2 2

+

cpEk

zk+1 - z

2 2

,

that is

(1 +  µ - cp)Ek

zk+1 - z

2 2

+ cEk

wk+1 - z

2 2

(21)

 (1 - p)

zk - z

2 2

+

(p

+

c(1

-

p))

wk - z

2 2

.

Letting

c

=

2 µ+2p  µ+2p

and

noticing

that

µ

<

1, p

<

1,

we

have

1-p 1 +  µ - cp

=

1-

 µ - p(c - 1) 1 +  µ - cp

=

1

-

µ

-

p µ  µ+2p

1

+



µ

-

p

2 µ+2p  µ+2p

 2µ2 + p µ =1-
 µ(1 +  µ) + 2p(1 - p)

 µ( µ + p)

p µ

1-

1-

.

2 µ + 2p

2( µ + p)

and

p + c(1 - p)

 µ + 2p

p µ

=1-p 1-

=1-

.

c

2 µ + 2p

2( µ + p)

Together with inequality (21), there holds

(1 +  µ - cp)Ek p µ
 1- 2( µ + p)

zk+1 - z

2 2

+ cEk

wk+1 - z

2 2

(1 +  µ - cp)

zk - z

2 2

+

c

wk - z

2 2

,

which implies that

( µ + p) E

zk - z

2 2

 ( µ(1 +  µ) + 2p(1 - p)) E

zk - z

2 2

+ 2 ( µ + p)

wk - z

2 2

p µ k

 1- 2( µ + p)

( µ(1 +  µ) + 2p(1 - p)) E

 4 ( µ + p)

z0 - z

2 2

p µ 1-
2( µ + p)

k
,

z0 - z

2 2

+ 2 ( µ + p)

w0 - z

2 2

where

we

have

recalled

that

2(1 - p)



1

due

to

p

=

1 2n



1 2

and

µ

<

1.

C Convergence Analysis of AL-SVRE
In this section, we aim to give the convergence rate of AL-SVRE. We first give the proof of Lemma 1 and establish the connection between the distance to saddle point and primal-dual gap. Then, we show upper bounds of some auxiliary quantities, which is useful to the analysis for AL-SVRE. Finally, we provide the formal proofs of Theorem 3 and Corollary 1.

C.1 The proof of Lemma 1

Proof. The definition of x~ means

x~ = arg min
xX

1

xf (x^, y^), x - x^

+ 2

x - x^

2 2

.

16

Therefore, for any x  X , we have

1

xf (x^, y^), x - x^

+ 2

x - x^

2 2



1

xf (x^, y^), x~ - x^

+ 2

x~ - x^

2 2

,

that is

1

xf (x^, y^), x - x~

 2

x~ - x^

2 2

-

x - x^

2 2

1 -
2

x - x^

2 2

.

(22)

Let f (x) = maxyY f (x, y) and yf(x) = arg maxyY f (x, y), then it holds that

f (x~) - f (x)

= f (x~) - f (x^) - (f (x) - f (x^))



f (x^), x~ - x^

+ yL

x~ - x^

2 2

-

f (x^), x - x^

=

xf (x^, yf(x^)), x~ - x

+ yL

x~ - x^

2 2

(23)

=

xf (x^, yf(x^)) - xf (x^, y^), x~ - x

+

xf (x^, y^), x~ - x

+ yL

x~ - x^

2 2

L

y^ - yf (x^) 2

x~ - x

1 2 + 2

x^ - x

2 2

+

y L

x~ - x^

2 2

.

where the first inequality is according to f is 2yL-smooth and convex, the last inequality is due to inequality

(22) with setting x = x.

According to the

a+b

2 2



2

a

2 2

+

2

b

2 2

and

the

Lipschitz

continuity

of

yf (·)

(cf.

Lemma

7),

we

observe that

y^ - yf(x^)

22
2

y^ - y

2 2

+

2

y - yf (x^)

2 2

2

y^ - y

2 2

+

22y

x^ - x

2

(24)

 22y.

Next, following from optimality of x such that x = arg minxX f (x) and Lemma 4, we have

x = PX (x - x(x)) = PX (x - xf (x, y)) .

Hence, by smoothness of the function f and Lemma 5, we have

x~ - x 2 = PX (x^ - xf (x^, y^)) - PX (x - xf (x, y)) 2

 x^ - x -  (xf (x^, y^) - xf (x, y)) 2

(25)

 x^ - x 2 + L

x^ - x

2 2

+

y^ - y

2 2



 (1 + L) .

Consequently, plugging inequalities (24) and (25) into (23) yields that

max f (x~, y)
yY

-

f (x, y)



 2(1

+

L)y L

+

 2

+

2y L

x~ - x

2 2

+

x^ - x

2 2



 2(1 + L) + 2(1 + L)2 + 2

 yL + 2

Similarly, by definition of y~, we also have

y~ - y 2 = y^ - y +  (yf (x^, y^) - yf (x, y)) 2 ,

(26)

and

g(x, y) - min g(x, y~) 
xX

 2(1 + L) + 2(1 + L)2 + 2

 xL + 2 .

17

Furthermore, by inequality (25) and (26), we have

x~ - x

2 2

+

y~ - y

2 2

=

PX

(x^ - xf (x^, y^)) - PX

(x - xf (x, y))

2 2

+

PY (y^ + yf (x^, y^)) - PY (y + yf (x, y))

2 2

2

x^ - x

2 2

+

2

y^ - y

2 2

+

22

xf (x^, y^) - xf (x, y)

2 2

+

yf (x^, y^) - yf (x, y)

2 2

2 + 22L2

x^ - x

2 2

+

y^ - y

2 2

 2(1 + 2L2),

(27)

where the second inequality is according to the smoothness of f .

C.2 Connection between Distance and Primal-Dual Gap
The following lemma shows that we can upper bound the distance from given point to the saddle point by its primal-dual gap.
Lemma 10. Suppose the function f (x, y) : X × Y  R is (µx, µy)-convex-concave and L-smooth. Denote the saddle point of f by (x, y). Then for any x^  X , y^  Y, we have

µx

x^ - x

2 2

+

µy

y^ - y

2 2



2

max f (x^, y) - min f (x, y^)

yY

xX

.

Proof. Following the convexity of function f (·, y), we know that

µx 2

x^ - x

2 2



f (x^,

y)

-

f (x,

y).

Similarly, the concavity of function f (x, ·) leads to

µy 2

y^ - y

2 2



f (x,

y)

-

f (x, y^).

Together with these pieces, it holds that

µx

x^ - x

2 2

+

µy

y^ - y

2 2



2

(f (x^,

y)

-

f (x, y^))



2

max f (x^, y) - min f (x, y^)

yY

xX

.

C.3 Upper Bounds of Auxiliary Quantities

The following lemma introduces four auxiliary quantities and their upper bounds, which are useful to the analysis of Theorem 3.

Lemma 11. We use the notation of Algorithm 2 and denote (xk, yk) is the saddle point of Fk and

f = max f (x0, y) - min f (x, y0).

yY

xX

Then, for each k  1, we have

E

xk - xk

2 2

+

yk - yk

2 2

 k,

(28)

E

max
yY

Fk

(xk ,

y)

-

Fk

(xk

,

yk

)

 2f (1 - )k, 9

(29)

E

xk - x

2 2



2k , µx

E

xk - xk+1

2 2

+

yk - yk+1

2 2

 72k-2 , µx min{µx, µy}

(30) (31)

where

k

2µyf (1 - )k  3(L + )(7(L + ) + 2 nµy)

and

k

8f(1 (q

- -

)k+1 )2

.

18

Proof. It is easy to verify that that Fk is (L + )-smooth and (µx + , µy)-convex-concave. We will use induction to prove inequalities (28) to (31) hold for each k  1.
We first assume that inequalities (28) to (31) hold for any k = 1, 2 . . . , K - 1, then we prove the statements for k = K.

Part (a): Inequalities (28) and (29) hold for k = K.

By definition of TK , we know that



eTK  12

1

2 -



+

µ1x7µ28ym(Lin{+µx,)(µ7y(}L(1+-))+2(2qn-µy)))2

R,



where

1 

=

4

n

+

2 n(L+) min{µx+,µy }

.

Let {uK,t, vK,t}t0 be the sequence of using L-SVRE to solve minimax problem

min max FK (u, v)
uX vY

with

initial

point

(uK,0, vK,0)

=

(xK-1, yK-1)

and

stepsize

K

=

1 4 n(L+)

.

Based

on

Theorem

2,

we

have

E

uK,TK - xK

2 2

+

vK,TK - yK

2 2

 4 (1 - )TK E

xK-1 - xK

2 2

+

yK-1 - yK

2 2

 4e-TK E

xK-1 - xK

2 2

+

yK-1 - yK

2 2

4  RE

xK-1 - xK

2 2

+

yK-1 - yK

2 2

.

Note that

E

xK-1 - xK

2 2

+

yk-1 - yK 

2 2

 2E

xK-1 - xK-1

2 2

+

yK-1 - yK  -1

2 2

+ 2E



2K-1

+

144K-3 , µx min{µx, µy}

xK-1 - xK

2 2

+

yK  -1 - yK 

2 2

where we have used induction hypothesis (28) and (31). Plugging (33) into (32), we have

(32) (33)

E

uK,TK - xK

2 2

+

vK,TK - yK

2

4 
R

2K-1

+

144K-3 µx min{µx, µy}

4 =
R

2K-1

+

µx

144 min{µx,

µy }

·

8f(1 (q

- -

)K-2 )2

(34)



4 =
R

1

2 -



+

µ1x7µ28ym(Lin{+µx,)(µ7y(}L(1+-))+2(2qn-µy)))2

1 K  3 K .

Consequently, combing inequality (34) with equation (27) in the proof of Lemma 1, we obtain

E

xK - xK

2 2

+

yK - yK

2 2



2(1

+

K2 (L 3

+

)2) K



2(1

+ 1/16)

3

K



K ,

and

max
yY

FK

(xK

,

y)

-

FK

(xK

,

yK

)

19



 2(1 + K (L + )) + 2(1 + K (L + ))2 + 2

(L + )2 K + K µy 3 6K



5  50 2+ +2

(L + )2  + 2 n(L + )

K

4

16

µy

3



7(L + )  +2 n

(L + ) K

µy

3

= 2f (1 - )K , 9

where

we

have

used

K

=

1 4 n(L+)

and

xK = PX (uK,TK - K xFK (uK,TK , vK,TK )) yK = PY (vK,TK + K yFK (uK,TK , vK,TK )) .
Therefore, we have proved inequalities (28) and (29) hold for 1  k  K.

Part (b): Inequality (30) holds for k = K.

Let (x)

maxyY f (x, y), and k(x)

maxyY

Fk (x,

y)

=

(x)

+

 2

x - uk-1

2 2

.

It is easy to check that

xk = arg minxX k(x) and k is µx-strongly convex by Lemma 7.

Part (a) means we have (29) holds for 1  k  K, which implies that

Ek (xk )

-

k



2f 9

(1

-

)k ,

for 1  k  K.

Consequently, according to the analysis of Catalyst for convex minimization [19, Proposition 5]3, it holds that

E(xk) -   (8q-f)2 (1 - )k+1 = k, for 0  k  K.

(35)

Combing above inequality with the strong convexity of , we have

µx 2

E

xk - x

2 2



E(xk )

-





k ,

for 0  k  K.

Part (c): Inequality (31) holds for k = K. For K  2, the definition of uk and  < 1 means

uK-1 - uK 2 = xK-1 + (xK-1 - xK-2) - xK - (xK - xK-1) 2  (1 + ) xK - xK-1 2 +  xK-1 - xK-2 2  3 max{ xK - xK-1 2 , xK-1 - xK-2 2}.

Thus, we have

uK-1 - uK

2 2

 9 max{

xK - xK-1

2 2

,

xK-1 - xK-2

2 2

}

 9 max{2

xK - x

2 2

+

2

xK-1 - x

2 2

,

2

xK-1 - x

2 2

+

2

xK-2 - x

22 }

 36 max{

xK - x

2 2

,

xK-1 - x

2 2

,

xK-2 - x

2 2

}

 72K-2 , µx

where the last inequality is according to the fact that (30) holds for 0  k  K.

3The original proof of Proposition 5 in Pages 45-46 of [19] should be slightly modified by employing (x0) -  = maxy f (x0, y) - f (x, y)  f . In fact, this result means inequality (30) also holds for k = 0.

20

Next, by strong convexity of FK (·, yK ) and xK = arg minxX FK (x, yK ), we have

µx +  2

xK - xK+1

2 2



FK (xK+1,

yK )

-

FK (xK ,

yK ).

Similarly, by strong concavity of FK (xK , ·) and yK = arg maxyY FK (xK , y), we have

µy 2

yK - yK +1

2 2



FK (xK ,

yK )

-

FK (xK ,

yK +1).

Therefore, we can conclude that

µx +  2

xK - xK+1

2 2

+

µy 2

yK - yK +1

2 2

 FK (xK+1, yK ) - FK (xK , yK +1)

=

f (xK+1,

yK )

+

 2

xK+1 - uK-1

2 2

-

f (xK ,

yK +1)

+

 2

xK - uK-1

2 2

=

f (xK+1,

yK )

+

 2

-

f (xK , yK +1)

+

 2

xK+1 - uK

2 2

+

uK - uK-1

2 2

+2

xK+1

- uK , uK

- uK-1

xK - uK

2 2

+

uK - uK-1

2 2

+

2

xK - uK , uK - uK-1

= FK+1(xK+1, yK ) - FK+1(xK , yK +1) +  xK+1 - xK , uK - uK-1

  xK+1 - xK , uK - uK-1

 
2

xK+1 - xK

 2+ 2

uK - uK-1

2 2

,

(36)

where we have used that FK+1(xK+1, yK )  FK+1(xK+1, yK +1)  FK+1(xK , yK +1). Hence, it holds that

E

xK - xK+1

2 2

+

yK - yK +1

2 2

  min{µx, µy} E

uK - uK-1

2 2



72K-2 . µx min{µx, µy}

We also need to show the induction base to finish the proof of inequalities (28) to (31).

Part (d): Induction base.
Finally, we present the induction base that inequalities (28) to (31) hold for k = 1. Since F1 is (µx + , µy)-convex-concave, we have

µx +  2

x0 - x1

2 2

+

µy 2

y0 - y1

2 2

 F1(x0, y1) - F1(x1, y1) + F1(x1, y1) - F1(x1, y0)

=

f (x0, y1)

-

f (x1,

y0)

-

 2

x1 - x0

2 2

 max f (x0, y) - min f (x, y0) = f ,

yY

xX

which implies

E

u1,T1 - x1

2 2

+

v1,T1 - y1 2

4e-T1 E

x0 - x1

2 2

+

y0 - y1

2 2

4 
R

2f min{µx, µy}



1 3 1.

(37)

Similar to the proof in Part (a), we can use Lemma 1, 9 and inequality (37) to show that (28) and (29) hold for k = 1.
Moreover, according to Proposition 5 in [19] and the strong convexity of F1, we know that (30) holds for k = 1.

21

Note that z1 - z0 = (1 + )(y1 - y0), thus we have

E

u0 - u1

2 2



4E

x1 - x0

2 2



320 µx



72-1 . µx

Therefore, by inequality (36) in Part (c), we have

E

x1 - x2

2 2

+

y1 - y2

2 2

  min{µx, µy} E

u1 - u0

2 2



72-1 . µx min{µx, µy}

As a conclusion, we have proved inequalities (28) to (31) hold for any k  1.

C.4 The proof of Theorem 3
Proof. Consider that the fact

yk = arg max
yY

f (xk,

y)

+

 2

xk - uk-1

2 2

= arg max f (xk, y) = yf(xk),
yY

then we have

yk - y 2 = yf (xk) - yf (x) 2  y xk - x 2

(38)

by using Lemma 7. Therefore, we conclude that

E

uk,Tk - x

2 2

+

vk,Tk - y

2 2

 2E

uk,Tk - xk

2 2

+

vk,Tk - yk

2 2

+ 2E

xk - x

2 2

+

yk - y

2 2



2 3 k

+

2(2y

+

1)E

xk - x

2 2

 k + 4(2y + 1)E

xk - xk

2 2

+

xk - x

2 2

 k + 4(2y + 1)

k

+

2k µx

^,

where the second inequality is due to (34) and (38) and the last one is according to (28) and (30). Note that

yk = PY (vk,Tk + kyFk(uk,Tk , vk,Tk )) = PY (vk,Tk + kyf (uk,Tk , vk,Tk )) . Then, Lemma 1 and 9 implies

E f (x, y) - min g(x, yk)
xX



L

L

2





2 1+ 

+2 1+ 

4 n(L + )

4 n(L + )

+ 2 xL^ + 2 n(L + )^





 7xL^ + 2 n(L + )^  7xL + 2 n(L + )

(42y

+

5)k

+

8(2y + µx

1) k

  7xL + 2 n(L + )

3(L

+

2f (42y + 5)µy )(7(L + ) + 2 nµy)

+

64µf (x(2y +q -1)(1)2-

)



2(42y

+ 5)(µx 3µx

+

µy )

+

1282y

 (7xL + 2 n(L
µx(q - )2

+

))

f (1 - )k

(1 - )k

22

Together with inequality (35), we obtain that

E max f (xk, y) - min f (x, yk)

yY

xX

 f (1 - )k

2(42y

+ 5)(µx 3µx

+

µy )

+

1282y



(7xL + 2 n(L

 µx( q

-

)2

+

))

+

(q

8 -

)2

 f (1 - )k

122y x

+

1282y



(7xL + 2 n(L

 µx( q

-

)2

+

))

+

(q

8 -

)2



916f

 (xL + n(L +
µx(q - )2

)) 2y

(1

-

)k .

C.5 The Proof of Corollary 1
 Proof. First, note that  = min{µy - µx, max{L/ n - µx, 0}}  µy - µx  L and

1 = µx +   µy .

q

µx

µx

Together with Theorem 3, we have

E

max f (xK , y) - min f (x, yK )

yY

xX



e-K

916f

 (xL + n(L
µx(q - )2

+

))

2y

 e-K

274(8qn-f )22y2x

 e-K

 10992 nf

y

3x

 ,

where we use that  = 0.5q. Recall that

Tk =4 4 4

 2 n(L + ) n+ min{µx + , µy}
 4 nL n+ min{µx + , µy} 4 nL n+ min{µx + , µy}



log

12

1

2 -



+

µ1x7µ28ym(Lin{+µx,)(µ7y(}L(1+-))+2(2qn-µy)))2

884736nL3)

log 12 4 +

µ2x µy q

log 10616880n3x ,

where we have noticed that   0.5. Therefore, the total SFO complexity of AL-SVRE is

K
(Tk + n) = O
k=1



2n 2 q + q

4 nL n+
min{µx + , µy}

log(n3x)

 log( nf

y

3x/)

.

Observe that





1. if y  n, we have µy  L/ n,  = µy - µx, which means

n q = n

µy µx

 n3/4x

and





1

nL

nL

·

=

q min{µx + , µy} µy

µy µx

=

 n xy;

23







2. if x > n > y, we have  = L/ n - µx, µx +  = L/ n < µy , which means

n q = n

L 
nµx

=

n3/4

 x

and





1

nL

nL

·

=

q min{µx + , µy} L/ n

L 
nµx

=

n3/4

 x

;

 3. if n  x, we have  = 0, which means

n

1

 nL



q = n

and

·

=

q min{µx + , µy}

nx  n.

Hence, we can conclude that the total SFO complexity of AL-SVRE is

K
(Tk + n) = O

 n

 (n

+

 x)( n

+

y

)

log(n3x)

 log( nf

y

3x

/)

.

k=1

D The Proof of Theorem 1

The construction for the lower bound in Theorem 1 follows from the idea of "zero-chain" property [12, 41].
Since we focus on SFO algorithm without proximal operator, our analysis is simpler than Han et al. [12] and
Xie et al. [36]'s. Without loss of generality, we assume that the SFO algorithm starts iteration at (x(0), y(0)) = (0dx , 0dy ).
Otherwise, we can take the objective function

f^(x, y) = 1 n

n

fi(x + x(0), y + y(0))

i=1

into consideration. Consider following function H : Rd × Rd  R defined as

 H(x, y; , d) =
2

x

2 2

+

x

 (By - c) -
2

y

2 2

,

(39)

where

1



-1 1



 B=


... ...



 



Rd×d,

 

-1

1



 

-1 



c = (, 0, 0, . . . , 0)

and  =

2

+4- 2

.

Furthermore, we define subspaces as follows

span{e1, e2, . . . , ek}, k = 1, . . . , d,

Fk =

{0d},

k = 0,

where {e1, . . . , ed} is the standard basis of Rd. Here, we state some properties of the Function H in the above definition.

24

Lemma 12. For the function H defined in Equation (39), following properties hold. 
1. H is 8 + 22-smooth.

2. The saddle point of function H is

 x = (q, q2, . . . , qd) ,

y = 

q,

q2,

.

.

.

,

qd-1,

1 1-q

qd

,

where

q

=

2+2

 - 2
2

+4

.

3. For k < d, if (x, y)  Fk × Fk, then (xH(x, y), yH(x, y))  Fk+1 × Fk+1.

4. For k  d/2 and (x, y)  Fk × Fk, we have

x - x

2 2

+

x

2 2

+

y - y

y

2 2

2 2



1 q2k. 2

Proof.

1.

Note

that

 =

 2 2 +4+

< 1.

Hence

B 2

B 1 = 2.

Then for any (x1, y1), (x2, y2)  Rd × Rd, we have

xH(x1, y1) - xH(x2, y2) yH(x1, y1) - yH(x2, y2)

2
=
2

(x1 - x2) + B(y1 - y2) 2 B (x1 - x2) - (y1 - y2) 2

 22

x1 - x2

2 2

+

2

B

2 2

y1 - y2

2 2

+

2

B

2 2

x1 - x2

2 2

+

22

y1 - y2

2 2

 (8 + 22)

x1 - x2

2 2

+

y1 - y2

2 2

,

where the first inequality is according to

a+b

2 2

2

a

2 2

+

2

b 22.

2. Letting the gradient of H equal to zero, we get that

xH(x, y) = x + By - c = 0, yH(x, y) = B x - y = 0.

Hence, the saddle point of H satisfies

y

=

1 B

x,

(40)



2I + BB x = c.

(41)

Equation (41) are equivalent to

1 + 2 -1





 -1 2 + 2 -1

 0

  

... ... ...

  x =  

...

 . 

 

-1 2 + 2

-1

 

 

0

 

-1 1 +  + 2

0

Note that q is a root of the equation s2 - (2 + 2)s + 1 = 0, then we have



(1 + 2)q - q2 = 1 - q = 

2 + 4 - 2 = ,

2

-qd-1 + (1 +  + 2)qd = qd-1 -1 + (2 + 2)q - q + q

= qd-1 -1 + (2 + 2)q - q + (1 - q)q = 0,

25

and -qk + (2 + 2)qk+1 - qk+2 = 0, for k = 1, 2, . . . , d - 2,
which implies the solution to Equation (41) is x = (q, q2, . . . , qd) . Additionally, we have

y =

1 B

x = 

q, q2, . . . , qd-1,  1

qd

,



1-q

where we have used that  = 1 - q. 3. For (x, y)  Fk for k < d, note that

c  F1  Fk+1, By  Fk+1, B x  Fk+1.

Therefore we have xH(x, y), yH(x, y)  Fk+1. 4. For (x, y)  Fk, we have xk+1 = · · · = xd = yk+1 = · · · = yd = 0 and

x - x

2 2

+

y - y

2 2



d

d-1

q2i + 2

q2i +

2

q2d

1-q

i=k+1

i=k+1

=

(1

+

2)

q2(k+1)(1 - q2(d-k)) 1 - q2

+

2q q2d. 1-q

Consequently, there holds

x - x

2 2

+

x

2 2

+

y - y

y

2 2

2 2



(1

+

 )2 q2(k+1)(1-q2(d-k))
1-q2

+

2q 1-q

q2d

(1

+

2

)

q2

(1-q2d 1-q2

)

+

2q 1-q

q

2d

q2k(1 - q2(d-k))



1 - q2d

q2k(1 - q2(d-k))  1 - (2qd - 1)

=

1 2

q2k

1

- 1

q2(d-k) - qd



1 q2k, 2

where we have used that 1 + q2d  2qd and 2(d - k)  d according to k  d/2.

With these pieces in hand, we now construct our adversary problem as follows:

1n

1n

min max f (x, y; , , d)

xy

n

fi(x, y; , , d) = n

H(Uix, Uiy),

(42)

i=1

i=1

where f : Rnd × Rnd  R, and matrices U1, . . . , Un  Rd×nd consist a partition of the identity matrix of
order nd such that I = [U1 , · · · , Un ]. The trick of constructing worse objective function for SFO algorithms by matrices U1 , · · · , Un also can be found in the analysis for minimization problem [18, 42].
The smoothness, convexity and concavity of f can be characterized as follows.

Lemma 13. The class of functions {fi} defined in equation (42) is 

function

f

is

(

 n

,

 n

)-convex-concave.

Proof. Using the fact

n i=1

Uix

2 2

=

x 22, we have

8+22 n

-average-smooth;

and

the

 f (x, y) =
n

x

2 2

-

 x
n

n



Ui c

+x n

i=1

n
Ui BUi

 y-
n

y

2 2

.

i=1

26

It

is

clear

that

f

is

(

 n

,

 n

)-convex-concave.

Moreover, for any (x1, y1), (x2, y2)  Rnd × Rnd, it holds that

1n n
i=1

xfi(x1, y1) - xfi(x2, y2)

2 2

+

yfi(x1, y1) - yfi(x2, y2)

2 2

2 n =
n
i=1

2
Ui (xH(Uix1, Uiy1) - xH(Uix2, Uiy2)) 2

+

Ui

(yH(Uix1, Uiy1) - yH(Uix2, Uiy2))

2 2

2 n =
n
i=1

xH(Uix1, Uiy1) - xH(Uix2, Uiy2)

2 2

+

yH(Uix1, Uiy1) - yH(Uix2, Uiy2)

2 2

2(8 + 22) n 
n
i=1

Ui(x1 - x2)

2 2

+

Ui(y1 - y2)

2 2

2(8 + 22) =
n

x1 - x2

2 2

+

y1 - y2

2 2

,

where the first inequality follows from smoothness of H (cf. Property 1 in Lemma 12).

Each component function fi has "zero-chain" property for stochastic first-order oracle. That is, the information provided by an SFO call at the current point (x, y) can at most increase the dimension of the linear space which contains (x, y) by 1. We present the formal statement in Lemma 14.

Lemma 14. Let (x(t), y(t)) be the point obtained by an SFO algorithm A at time-step t and denote ki(t) |{s  t : is = i}|. Then there holds

Uix(t), Uiy(t)  Fki(t) , for t  0, i = 1, . . . , n.

(43)

Proof. For t = 0, Equation (43) holds apparently by x(0) = y(0) = 0.
Now we suppose Equation (43) holds for t < T . It is easily to check that ki(T ) = ki(T -1) for i = iT and ki(TT ) = ki(TT -1) + 1.
Observe that for i = iT we have

which implies

UixfiT (x, y) = UiUiT xH(UiT x, UiT y) = 0,

Uix(T )  span Uix(0), . . . , Uix(T -1), UixfiT (x(0), y(0)), . . . , UixfiT (x(T -1), y(T -1)) = span Uix(0), . . . , Uix(T -1)  Fki(T -1) = Fki(T ) .
Next, Following from Property 3 in Lemma 12, we know that for t  T - 1

UiT xfiT (x(t), y(t)) = UiT UiT xH (UiT x(t), UiT y(t)) = xH (UiT x(t), UiT y(t))  Fki(TT -1)+1 = F , ki(TT )
where we have used the inductive hypothesis that

Therefore, we can conclude that

UiT x(t), UiT y(t)  Fki(Tt)  F . ki(TT -1)

UiT x(T )  span UiT x(0), . . . , UiT x(T -1), UiT xfiT (x(0), y(0)), . . . , UiT xfiT (x(T -1), y(T -1)) Fki(TT ) .
The reason for the of result Uix(T )  Fki(T) is same.

27

Now we can show the lower bound for finding an approximate saddle point of problem (42) by SFO algorithms when L/µ = ( n).



Theorem 4. For the parameter L, µ, n,  such that L/µ >

10n

and



<

1 2

e-5



0.00337,

we

set

8n

nµ

11

=

L2/µ2 - 2n ,

= , 

d = ln  2

- 4.

Then the functions f (x, y; , , d) and fi(x, y; , , d) defined in Problem (42) satisfy f is (µ, µ)-convexconcave and {fi}ni=1 is L-average-smooth. Moreover, when we employ any SFO algorithm A to solve the Problem (42), there holds

2

2

E x(t) - x + y(t) - y >  for t  nd/2.

2

2

Proof. By Lemma 13 and definition of , , it is clear that f is (µ, µ)-convex-concave and {fi(x, y; , , d)}ni=1 is L-average-smooth.
For T = nd/2, let i = arg minj{kj(T )}. It is clear that ki(T )  d/2. Then by Property 4 in Lemma 12, for t  T , we have

2

2

2

2

E x(t) - x + y(t) - y  E Ui(x(t) - x) + Ui(y(t) - y)

2

2

2

2

qki(t) 
2

Uix

2 2

+



qd/2 2

q2

1 - q2d 1 - q2

Uiy

2 2

qd/2+2



,

2

where

q

=

2+2

 - 2
2

+4

,



=



2

+4- 2

,

and

Uix

=

(q, q2, . . . , qd)

by Property 2 in Lemma 12.

Next, note that

 ( + 2 + 4) (2 + d/2) ln(1/q) = (2 + d/2) ln 1 +
2  ( + 2 + 4)  (2 + d/2) 2
< (2 + d/2)( + 1)

 (4 + d)

 ln(1/2)

which means 
from a + b

that 

qd/2+2
2

> .

< a + b, the

The first inequality is according to ln(1 
third inequality is due to L/µ  10n

+ a)  a, and  =

the second
8n L2 /µ2 -2n

inequality follows  1, and the last

inequality is based on the definition of d.

We

remark

that

the

condition



<

1 2

e-5

can

ensure

that

d



1.





Furthermore, Theorem 4 implies < 0.003, there exist a dimension d

that =O

(fornaLn/yµSloFgO(1a/lg)o)riatnhdmfuAncatniodnLs ,{µf,i(nx,,ys)u}cni=h1th: aRtd

L/µ > × Rd 

10n and R which

satisfy {fi}ni=1 is L-average smooth, f is (µ, µ)-convex-concave. In order to find an approximate saddle point

(x^, y^) such that

E

x^ - x

2 2

+

y^ - y

2 2

 ,

28

 algorithm A needs at least(( nL/µ) log(1/)) steps.
For the case L/µ = O( n), we consider following problem

min max f^(x, y) = 1

xy

n

n

f^i(x, y)

i=1

1n n

µ 2

x

2 2

+

nL^ 2

(xi

-

1)2

-

µ 2

y

2 2

-

nL^ 2

(yi

-

1)2

,

(44)

i=1

where f^ : Rn × Rn  R, L^ =

L2 2

-

µ2.

And we provide the lower bound for finding approximate saddle point of Problem (44) as follows.

Theorem

5.

For

the

parameter

L, µ, n, 

such

that

L/µ > 2

and

<

1 8

,

the

functions

f^(x, y)

and

f^i(x, y)

defined in Problem (44) satisfy f^ is (µ, µ)-convex-concave and {f^i}ni=1 is L-average-smooth. Moreover, when

we employ any SFO algorithm A to solve the Problem (44), there holds

2

2

E x(t) - x + y(t) - y >  for t  n/2.

2

2

Proof. It is easily to check that

f^(x, y) = µ 2

x

2 2

+

L^ 2n

x-1

2 2

-

µ 2

y

2 2

-

L^ 2n

y-1

2 2

.

Hence f^ is (µ, µ)-convex-concave and the saddle point (x, y) of f^ satisfies

x

=

y

=

L^

L^  1. + nµ

Next, observe that

xf^i(x, y) =µx + nL^(ei x - 1)ei, yf^i(x, y) = - µy - nL^(ei y - 1)ei.

(45)

Then we have

1n
n
i=1
1n =
n
i=1
1n 
n
i=1

xf^i(x1, y1) - xf^i(x1, y1)

2
+

yf^i(x1, y1) - yf^i(x1, y1) 2

2

2

µ(x1 - x2) + nL^eiei

(x1 - x2)

2
+
2

µ(y1 - y2) + nL^eiei

(y1 - y2)

2 2

2µ2

x1 - x2

2 2

+

2nL^2

eiei

(x1 - x2)

2 2

+

2µ2

y1 - y2

2 2

+

2nL^2

eiei (y1 - y2)

2 2

= 2(L^2 + µ2)

x1 - x2

2 2

+

y1 - y2 2

= L2

x1 - x2

2 2

+

y1 - y2 2

,

where the first inequality is according to

a+b

2 2

2

a

2 2

+

2

b

2 2

and

the

last

equation

follows

from

n

n

eiei u 2 =

u2i =

u

2 2

.

i=1

i=1

Hence, {f^i}ni=1 is L-average-smooth. Moreover, by Equation (45) and definition of SFO algorithm, we know that

x(t), y(t)  span {ei1 , . . . , eit } .

29

Consequently, for t  n/2, there holds

2

2

E x(t) - x + y(t) - y

2

2

L^2

(n

-

t)

·

(L^

+

 nµ)2

n

L^2

 2

·

2L^2

+

2nµ2

n

L2/2 - µ2

= 2

·

L2

-

2µ2

+

2nµ2

n

2n

= 4

1 - L2/µ2 + 2n - 2

n

1



 > ,

4(n + 1) 8

where we have recalled that L^2 = L2/2 - µ2, L/µ > 2 and n  1.

Combining Theorem 4 and 5, we obtain the result of Theorem 1.

E The Proof of Lemma 2

Proof. Just note that



max
(x,y)X ×Y

|f

(x,

y)

-

f,x0

(x,

y)|

=

4Dx2

max
xX

x - x0

2 2



 .
4

Therefore, we can conclude that

max
yY

f

(x^,

y)

-

min
xX

f

(x,

y^)

=

f

(x^,

yf

(x^))

-

f

(xf

(y^),

y)



f,x0 (x^, yf (x^))

+

 4

-

f,x0 (xf (y^),

y)

-

 4





2

+

max
yY

f,x0

(x^,

y)

-

min
xX

f,x0

(x,

y^),

where yf(x^) = arg maxyY f (x^, y) and xf (y^) = arg minxX f (x, y^).

F Different Assumptions on Smoothness

Note that Alacaoglu and Malitsky [1]'s work suppose each fi(x, y) is Li-smooth, that is

fi(z1) - fi(z2) 2  Li z1 - z2 2

(46)

where z1 =

1 n

n i=1

Li.

(x1, y1)

and

z2

=

(x2, y2),

which

leads

to

f (z)

=

1 n

n i=1

fi(z)

is

LG-smooth,

where

LG

=

Recall the functions {f^i}ni=1 and the SCSC minimax problem defined in equation (44). We have shown

that {f^i}ni=1 is L-average-smooth in the proof of Theorem 5. However, each f^i satisfies

f^i(z1) - f^i(z2)

=

(µI (µI

+ +

nnLL^^eeiieeii

2
)(x1 - )(y1 -

x2) y2)

(µ + nL^) z1 - z2 2 ,

2

30

where (µI +

wneL^heaievie

recalled Equation (45) to get the first ) is a diagonalmatrix with max entry

µeq+ualintyL^,.

and the last inequality is according to Therefore each fi is M -smooth where

M = µ + n(L2/2 - µ2) = ( nL).

1 n

Uni=si1nLg ino=tatio(nsnoLf).AlHaceanocgel,u

and their

Malitsky [1] smoothness

on {f^i}ni=1, parameter

we LG

requires Li = M = is larger than the

 ( nL) and LG = average-smoothness

parameter L we used in this paper.

31

