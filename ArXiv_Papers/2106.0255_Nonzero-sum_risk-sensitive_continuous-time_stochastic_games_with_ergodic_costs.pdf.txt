arXiv:2106.00255v1 [math.OC] 1 Jun 2021

NONZERO-SUM RISK-SENSITIVE CONTINUOUS-TIME STOCHASTIC GAMES WITH ERGODIC COSTS.
MRINAL K. GHOSH, SUBRATA GOLUI, CHANDAN PAL, AND SOMNATH PRADHAN
Abstract. We study nonzero-sum stochastic games for continuous time Markov decision processes on a denumerable state space with risk-sensitive ergodic cost criterion. Transition rates and cost rates are allowed to be unbounded. Under a Lyapunov type stability assumption, we show that the corresponding system of coupled HJB equations admits a solution which leads to the existence of a Nash equilibrium in stationary strategies. We establish this using an approach involving principal eigenvalues associated with the HJB equations. Furthermore, exploiting appropriate stochastic representation of principal eigenfunctions, we completely characterize Nash equilibria in the space of stationary Markov strategies. Keywords: Nonzero-sum game, risk-sensitive ergodic cost criterion, stationary strategies, coupled HJB equations, Fan's fixed point theorem, Nash equilibrium.
1. INTRODUCTION
We consider a nonzero-sum stochastic game on the infinite time horizon for continuous time Markov decision processes (CTMDPs) on a denumerable state space. The performance evaluation criterion is exponential of integral cost which addresses the decision makers (i.e., players) attitude towards risk. In other words we address the problem of nonzero-sum risk sensitive stochastic games involving continuous time Markov decision processes. In the literature of stochastic games involving continuous time Markov decision processes, one usually studies the integral of the cost [13], [14], [15] which is the so called risk-neutral situation. In the exponential of integral cost, the evaluation criterion is multiplicative as opposed to the additive nature of evaluation criterion in the integral of cost case. This difference makes the risk sensitive case significantly different from its risk neutral counterpart. The study of risk sensitive criterion was first introduced in [3]; see [29] and the references therein. This criterion is studied extensively in the context of MDP both in discrete and continuous times; see, for instance [5], [6], [7], [9], [17], [18], [26], [30], and the references therein. The corresponding results for stochastic (dynamic) games are limited. Notable exceptions are [1], [2], [10]. In discrete time and discrete state space the risk-sensitive zero-sum stochastic games with bounded cost and transition rates have been studied by Basu and Ghosh [2] and nonzero-sum games in [1]. For CTDMPs, zero-sum stochastic games with risk-sensitive costs for bounded cost and bounded transition rates have been studied in [10]. One can
1

2

M. K. GHOSH, S. GOLUI, C. PAL, AND S. PRADHAN

see [12], [28], and the references therein for finite horizon risk-sensitive nonzero-sum games for CTMDPs. Recently risk sensitive continuous time Markov decision processes have been studied in [4], [11], [24], [25]. In this present paper we extend the results of the above four papers to nonzero-sum stochastic games. Using principal eigenvalue approach, under a Lyapunov type stability assumption, we have shown that the corresponding system of coupled HJB equations admits a solution which in turn leads to the existence of Nash equilibrium in stationary strategies. Also, exploiting the stochastic representation of principal eigenfunction we completely characterize all possible Nash equilibria in the space of stationary Markov strategies. The main motivation for studying this kind of games arises from their applications to many interesting problems, such as controlled birth-and-death systems, telecommunication and queueing systems in which the transition and cost rates may be both unbounded.
Our main contribution in this paper is the following. We establish the existence and characterization of Nash equilibria under a blanket Lyapunov type stability assumption. To be more specific, we study ergodic nonzero sum risk-sensitive stochastic games for CTMDPs having the following features: (a) the transition and the cost rates may be unbounded (b) state space is countable (c) at any state of the system the space of admissible actions is compact (d) the strategies may be history dependent. To our knowledge, these results are new in the literature of ergodic non-zero sum risk-sensitive games for CTMDPs. Similar risk-sensitive game problems for discrete time Markov decision processes have been studied under small costs and geometric ergodicity assumption in [2].
The rest of this paper is organized as follows: Section 2 deals with the problem description and preliminaries. The ergodic cost criterion is analyzed in Section 3. Under a Lyapunov type stability assumption(s), we first establish the existence of a solution to the corresponding coupled Hamilton-Jacobi-Bellman (HJB) equations. This in turn leads to the existence of a Nash equilibrium in stationary strategies (see Theorem 3.2). In Section 4, we present an illustrative example.

2. The game model For the sake of notational simplicity we treat two player game. The N -player game for N  3, is analogous. The continuous-time two-person nonzero-sum stochastic game model which consists of the following elements
{S, U1, U2, (U1(i)  U1, U2(i)  U2, i  S), ¯ij(u1, u2), c¯1(i, u1, u2), c¯2(i, u1, u2)}, (2.1)
where each component is described below:

3

· S, called the state space, is assumed to be the set of all positive integers endowed with the discrete topology, i.e. S =: {1, 2, · · · }.
· U1 and U2 are the action sets for players 1 and 2, respectively. The action spaces U1 and U2 are assumed to be Borel spaces with the Borel -algebras B(U1) and B(U2), respectively.
· For each i  S, U1(i)  B(U1) and U2(i)  B(U2) denote the sets of admissible actions for players 1 and 2 in state i, respectively. Let K := {(i, u1, u2)|i  S, u1  U1(i), u2  U2(i)}, which is a Borel subset of S × U1 × U2. Throughout this paper, we assume that (A1)(a) For each i  S, the admissible action spaces Uk(i), k = 1, 2, are nonempty and compact subsets of Uk.
· The transition rates ¯ij(u1, u2), (u1, u2)  U1(i)×U2(i), i, j  S, satisfy the condition ¯ij(u1, u2)  0 for all i = j, (u1, u2)  U1(i) × U2(i). Also, we assume that: (A1)(b) The transition rates ¯ij(u1, u2) are conservative, i.e.,

¯ij(u1, u2) = 0 for i  S and (u1, u2)  U1(i) × U2(i) .
jS

and

¯i :=

sup

[-¯ii(u1, u2)] <  .

(u1 ,u2 )U1 (i)×U2 (i)

· Finally, the measurable function c¯k : K  R+ denotes the cost rate function for

player k, k = 1, 2.

We consider a continuous time Markov decision processes (CTMDPs) {Y (t)}t0 with
state space S and controlled rate matrix u1,u2 = (¯ij(u1, u2)). To construct the underlying CTMDPs Y (t) (as in [[19], [22], [27]) we introduce some notations: let S := S  {} (with some  / S), 0 := (S × (0, )), m := (S × (0, ))m × S × ({} × {}) for m  1 and  :=  m=0m. Let F be the Borel -algebra on . Then we obtain the measurable space (, F). For some m  1, and sample  := (i0, 1, i1, · · · , m, im, · · · )  , define

T0() := 0,

Tn() := Tn-1() + n,

T()

:=

lim
n

Tn().

Using {Tm}, we define the state process {Y (t)}t0 as

Y (t) :=

I{Tmt<Tm+1}im + I{tT}, for t  0 (with T0 := 0).

m0

(2.2)

Here, IE denotes the indicator function of a set E, and we use the convention that 0+z =: z

and 0z =: 0 for all z  S. Obviously, Y (t) is right-continuous on [0, ). From (2.2), we

see that Tm() (m  1) denotes the m-th jump moment of {Y (t)}t0 and im-1 is the

state of the process on [Tm-1(), Tm()), m() = Tm() - Tm-1() plays the role of

sojourn time at state im-1, and the sample path {Y (t)()}t0 has at most denumerable

4

M. K. GHOSH, S. GOLUI, C. PAL, AND S. PRADHAN

states im(m = 0, 1, · · · ). The process after T is regarded to be absorbed in the state . Thus, let q(·|, u1 , u2 ) : 0, U1 := U1  {u1 }, U2 := U2  {u2 }, U1() := {u1 }, U2() := {u2 }. Also, assume that c¯k(, u1, u2) : 0 (c¯k is the running cost function for kth player) for all (u1, u2)  U1 × U2, where u1 , u2 are isolated points. Moreover, let Ft := ({Tm  s, Y (Tm)  S} : 0  s  t, m  0) for all t  0, Fs- =: t<s Ft, and F~ := (A × {0}, B × (s, ) : A  F0, B  Fs-) which denotes the -algebra of predictable
sets on  × [0, ) related to {Ft}t0.
To complete the specification of a risk-sensitive stochastic game problem, we need, of course,
to introduce an optimality criterion. This requires to define the class of strategies as below.

Definition 2.1. A admissible strategy for player 1, denoted by v1 = {v1(t)}t0, is a transition probability v1(du1|, t) from (×[0, ), F~) onto (U1, B(U1)), such that v1(U1(Y (t-)())|, t) = 1. The set of all admissible strategies for player 1 is denoted by A1. A strategy v1  A1, is called a Markov for player 1 if v1(t)() = v1(t, Y (t-)(w)), i.e., v1(du1|, t) = v1(du1|Y (t-)(w), t) for every w   and t  0, where Y (t-)(w) := limst Y (s)(w). We denote by M1 the family of all Markov strategies for player 1. If the Markov strategy v1 for player 1 does not have any explicit time dependency then it is called a stationary Markov
strategy. The set of such strategies for player 1 is denoted by S1. The sets of all admissible strategies A2, all Markov strategies M2 and all stationary strategies S2 for player 2 are defined similarly.

To avoid the explosion of the state process {Y (t)}t0, we need the following assumption imposed on the transition rates, which had been widely used in CTMDPs; see, for instance, [[17], [18], [19], [20]] and references therein.
Assumption 2.1. There exists a Lyapunov function W~ : S  [1, ) such that (i) jS W~ (j)ij(u1, u2)  C1W~ (i) + C2 for all (u1, u2)  U1(i) × U2(i) and i  S with some constants C1 = 0, C2  0; (ii) ¯i  C3W~ (i) for all i  S with some positive constant C3.
For the rest of this article we are going to assume that Assumption 2.1 holds. Note that if supiS ¯i <  then Assumption 2.1 holds. In this case we can choose W~ to be a suitable constant. Also note that under Assumption 2.1, for any initial state i  S and any pair of strategies (v1, v2)  A1 × A2, Theorem 4.27 in [23] yields the existence of a unique probability measure denoted by Piv1,v2 on (, F). Let Eiv1,v2 be the expectation operator with respect to Piv1,v2. Also, from [[16], pp.13-15], we know that {Y (t)}t0 is a Markov process under any (v1, v2)  M1 × M2 (in fact, strong Markov).
For any compact metric space A, let P(A) denote the space of probability measures on A with Prohorov topology. Let Vk = P(Uk) and Vk(i) = P(Uk(i)) for i  S and k = 1, 2.

5

For each i, j  S, k = 1, 2, v1  V1(i) and v2  V2(i), the associated transition and cost rates are defined, respectively, as follows:

ij(v1, v2) :=

¯ij(u1, u2)v1(du1)v2(du2),

U1(i) U2(i)

ck(v1, v2) :=

c¯k(u1, u2)v1(du1)v2(du2).

U1(i) U2(i)

Note that for k = 1, 2, vk  Sk can be identified with a map vk : S  Vk such that for

each j  S, vk(j)  Vk(j) for each j  S. The sets S1 and S2 are endowed with product

topology.

We list the commonly used notations below.

· For any finite set D  S, we define BD = {f : S  R | f is borel measurable function and f (i) = 0  i  Dc} .

· Given any real-valued function V  1 on S, we define a Banach space (L V ,

·

 V

)

of V-weighted functions by

L V =

u:SR|

u

 V

:=

sup
iS

|u(i)| V (i)

<

.

· L1V, denotes the subset of L V consists of function u such that

u

 V



1.

For k = 1, 2, let c¯k : S × U1 × U2  [0, ) be the running cost function for the kth player, i.e., when state of the system is i and the actions (u1, u2) are chosen by the players, then the cost incurred by the kth player is c¯k(i, u1, u2). By choosing appropriate strategies, each player wants to minimize his/her accumulated cost over infinite time horizon.

For a pair of admissible strategies (v1, v2), the risk-sensitive ergodic cost for player k is

given by

kv1,v2 (i)

:=

lim sup
T 

1 T

ln Eiv1,v2

e

T 0

ck(Y (t),v1(t),v2(t))dt

,

(2.3)

where Y (t) is the CTMDP corresponding to (v1, v2)  A1 × A2 and Eiv1,v2 denotes the

expectation with respect to the law of the process Y (t) with initial condition Y (0) = i.

Since we are allowing our transition and cost rates to be unbounded, to guarantee the finiteness of kv1,v2 for k = 1, 2, we need the following Assumption.

Assumption 2.2. We assume that the CTMDP {Y (t)}t0 is irreducible under every pair of stationary Markov strategies (v1, v2)  S1 × S2. Furthermore, suppose there exist a constant C4 > 0 and a Lyapunov function W : S  [1, ) such that one of the following hold.
(a) When the running cost is bounded: For some positive constant  > max{ c1 , c2 } and a finite set K it holds that

sup

W (j)ij(u1, u2)  C4IK(i) - W (i) i  S.

(u1,u2)U1(i)×U2(i) jS

6

M. K. GHOSH, S. GOLUI, C. PAL, AND S. PRADHAN

(b) When the running cost is unbounded: For some norm-like function  : S  R+ and a finite set K it holds that

sup

W (j)ij(u1, u2)  C4IK(i) - (i)W (i) i  S.

(u1,u2)U1(i)×U2(i) jS

Also, the functions (·) - max(u1,u2)U1(·)×U2(·) ck(·, u1, u2), k = 1, 2, are norm-like.

Definition 2.2. A pair of strategies (v1, v2)  A1 × A2 is called a Nash equilibrium if v11,v2 (i)  1v1,v2 (i) for all v1  A1 and i  S

and v21,v2 (i)  v21,v2 (i) for all v2  A2 and i  S.

We wish to establish the existence of a Nash equilibrium in stationary strategies. To ensure the existence of a Nash equilibrium, we assume the following:

Assumption 2.3. (i) For any fixed i, j  S, k=1,2 , ij(u1, u2) and c¯k(i, u1, u2) are continuous in (u1, u2)  U1(i) × U2(i) .
(ii) W (j)ij(u1, u2) is continuous in (u1, u2)  U1(i) × U2(i) for any given i  S,
jS
where W is as Assumption 2.2.
(iii) There exists i0  S such that i0j(u1, u2) > 0 for all j = i0 and (u1, u2)  U1(j) × U2(j).

We now proceed to establish the existence of a Nash equilibrium in stationary strategies.

To this end we first outline a procedure for establishing the existence of a Nash equilibrium.

Suppose player 2 announces that he is going to employ a strategy v2  S2. In such a

scenario, player 1 attempts to minimize

1v1,v2 (i)

=

lim sup
T 

1 T

ln Eiv1,v2

e

T 0

c1(Y (t),v1(t),v2(Y (t-)))dt

,

over v1  A1. Thus for player 1 it is a continuous time Markov decision problem (CTMDP)

with risk sensitive ergodic cost. This problem has been studied in [4], [11], [24], [25].

In particular under certain assumptions, it is shown in [4], [24], [25], that the following

Hamilton-Jacobi-Bellman (HJB) equation

 1

^1(i)

=

inf
v1 V1 (i)

 

^1(i0) = 1,

ij(v1, v2(i))^1(j) + c1(i, v1, v2(i))^1(i)
jS

has a suitable solution (1, ^1), where 1 is a scalar and ^1 : S  R has suitable growth

rate; i0 is a fixed element of S. Furthermore it is shown in [4], [24], [25] that

1

=

inf
v1 A1

lim sup
T 

1 T

ln Eiv1,v2

e

T 0

c1(Y (t),v1(t),v2(Y (t-)))dt

,

7

and if v1  S1 is such that for i  S

inf

ij(v1, v2(i))^1(j) + c1(i, v1, v2(i))^1(i)

v1V1(i) jS

=

ij(v1(i), v2(i))^1(j) + c1(i, v1(i), v2(i))^1(i),

jS

then v1  S1 is an optimal control for player 1, i.e., for any i  S

1

=

lim sup
T 

1 T

ln Eiv1,v2

e

T 0

c1(Y (t),v1(Y (t-)),v2(Y (t-)))dt

.

In other words, given that player 2 is using the strategy v2  S2, v1  S1 is an optimal response for player 1. Clearly v1 depends on v2 and moreover there may be several optimal
responses for player 1 in S1. Analogous results holds for player 2 if player 1 announces that

he is going to use a strategy v1  S1. Hence given a pair of strategies (v1, v2)  S1 × S2, we can find a set of pairs of optimal responses {(v1, v2)  S1 × S2} via the appropriate pair of HJB equations described above. This defines a set-valued map. Clearly any fixed point of

this set-valued map is a Nash equilibrium.

The above discussion leads to the following procedure for finding a pair of Nash equilib-

rium strategies. Suppose that there exist a pair of stationary strategies (v1, v2)  S1 × S2, a pair of scalars (1, 2) and a pair of functions (^1, ^2) with appropriate growth conditions,
satisfying the following coupled HJB equations:

 1  

^1(i)

=

inf
v1 V1 (i)





ij(v1, v2(i))^1(j) + c1(i, v1, v2(i))^1(i)
jS




  


= ij (v1(i), v2(i))^1(j) + c1(i, v1(i), v2(i))^1(i)



 

jS



  


^1(i0) = 1,

2  

^2(i)

=

inf
v2 V2 (i)



ij(v1(i), v2)^2(j) + c2(i, v1(i), v2)^2(i)
jS





   

= ij (v1(i), v2(i))^2(j) + c2(i, v1(i), v2(i))^2(i)



  

jS



 


^2(i0) = 1,

where as before i0  S is a fixed point. Then it can be shown that (v1, v2) is a pair of Nash equilibrium and (1, 2) is the pair of corresponding Nash values. Thus the main result of
our paper is to establish that the above coupled HJB equations has suitable solutions.

Remark 2.1. Note that the similar stochastic optimal control problem has been studied in [11], [25] for bounded cost and bounded transition rates. But in our game model transition and cost rates are unbounded. Analogous MDP problems are treated in [4].

8

M. K. GHOSH, S. GOLUI, C. PAL, AND S. PRADHAN

3. Coupled HJB Equations and Existence of Nash Equilibrium

By the definition of weak convergence of probability measures, one can easily get the following result, which will be crucial for the existence of Nash equilibrium; we omit the details.

Lemma 3.1. Under Assumptions 2.1, 2.2, and 2.3, the functions

ck(i, v1, v2), k = 1, 2 and

ij (v1, v2)(j)

jS

are continuous on V1(i) × V2(i) for each fixed   L W and i  S.

Let Dn  S be an increasing sequence of finite sets such that nDn = S and i0  Dn for each n  1 . In the next lemma we show the existence of eigenpairs to certain equations in Dn for each n  N .

Lemma 3.2. Grant Assumptions 2.1, 2.2, and 2.3. Then for each n  N, the following

hold.

(1) For v^2  S2, there exists an eigenpair (1,n, 1,n)  R × BD+n, satisfying

 1,n1,n(i) = inf
v1 V1 (i)

 

1,n(i0) = 1.

1,n(j)ij (v1, v^2(i)) + c1(i, v1, v^2(i))1,n(i)
jS

for i  Dn, (3.1)

Moreover, we have

0



lim inf
n

1,n



lim sup 1,n
n



inf
v1 A1

lim sup
T 

1 T

ln Eiv01,v^2

e

T 0

c1(Y (t),v1(t),v^2(Y (t-)))dt

.

(3.2)

(2) Similarly, for v^1  S1, there exists an eigenpair (2,n, 2,n)  R × BD+n, satisfying

 2,n2,n(i) = inf
v2 V2 (i)

 

2,n(i0) = 1.

2,n(j)ij (v^1(i), v2) + c2(i, v^1(i), v2)2,n(i)
jS

for i  Dn, (3.3)

Moreover, we have

0



lim inf
n

2,n



lim sup 2,n
n



inf
v2 A2

lim sup
T 

1 T

ln Eiv^01,v2

e

T 0

c2(Y (t),v^1(Y (t-)),v2(t))dt

.

(3.4)

Proof. Follows by analogous arguments as in [4, Lemma 3.1, Lemma 3.3]. We omit the details.

Next by taking limit n   in the equations we show that the limiting equations admit eigenpairs in appropriate spaces. In particular, we have the following theorem.

Theorem 3.1. Grant Assumptions 2.1, 2.2, and 2.3. Then the following hold.

9

(1) For v^2  S2, there exists a unique minimal eigenpair (1, 1)  R+ × L1W,, 1 > 0, satisfying

 11(i) = inf
v1 V1 (i)

 

1(i0) = 1.

1(j)ij (v1, v^2(i)) + c1(i, v1, v^2(i))1(i)
jS

for i  S,

(3.5)

Moreover, we have

1

=

inf
v1 A1

lim sup
T 

1 T

ln Eiv1,v^2

e

T 0

c1(Y (t),v1(t),v^2(Y (t-)))dt

(:=

v1^2

=

inf
v1 A1

1v1,v^2 ),

and there exists a finite set B1  K, such that

(3.6)

1(i)

=

inf
v1 S1

Eiv1 ,v^2

e

^(B1 ) 0

(c1

(Y

(t),v1

(Y

(t-)),v^2

(Y

(t-)))-1

)dt

1

(Y

(^(B1)))

(:= 1v^2 (i)) i  Bc1,

(3.7)

where ^(B1) =  (Bc1) = inf{t : Y (t)  B1} =: ~1. (2) Similarly, for v^1  S1, there exists a unique minimal eigenpair (2, 2)  R+ ×LW 1,,
2 > 0 satisfying

 22(i) = inf
v2 V2 (i)

 

2(i0) = 1.

2(j)ij (v^1(i), v2) + c2(i, v^1(i), v2)2(i)
jS

for i  S,

(3.8)

Moreover, we have

2

=

inf
v2 A2

lim sup
T 

1 T

ln Eiv^1,v2

e

T 0

c2(Y (t),v^1(Y (t-)),v2 (t))dt

(:=

v2^1

=

inf
v2 A2

2v^1,v2 ),

and there exists a finite set B2  K, such that

(3.9)

2(i)

=

inf
v2 S2

Eiv^1 ,v2

e

^(B2 ) 0

(c2

(Y

(t),v^1

(Y

(t-)),v2

(Y

(t-)))-2

)dt

2

(Y

(^(B2)))

(:= 2v^1 (i)) i  Bc2,

(3.10)

where ^(B2) =  (Bc2) = inf{t : Y (t)  B2} =: ~2.

Proof. Since c1  0, using Assumption 2.2, we deduce that there exists a finite set B1 containig K such that
· Under Assumption 2.2 (a)

(

sup

c1(i, u1, u2) - 1,n) < 

(u1 ,u2 )U1 (i)×U2 (i)

· Under Assumption 2.2 (b)

 i  Bc1

and all n large enough .

(

sup

c1(i, u1, u2) - 1,n) < (i)  i  Bc1 and all n large enough .

(u1 ,u2 )U1 (i)×U2 (i)

Then applying It^o-Dynkin formula, from Assumption 2.2, we have the following estimates:

10

M. K. GHOSH, S. GOLUI, C. PAL, AND S. PRADHAN

· Under Assumption 2.2(a):

Eiv1,v^2 e^(B1) W (Y (^(B1)))  W (i) i  Bc1 .

(3.11)

· Under Assumption 2.2(b):

Eiv1,v^2

e

^(B1 ) 0

(Y

(t))dt

W

(Y

(^(B1)))

 W (i) i  Bc1 .

(3.12)

Now as in [4, Lemma 3.4], using the Lyapunov function W we construct a barrier. Then

following arguments similar to [4, Lemma 3.4] and letting n  , there exists (1, 1)  R+ × L1W,, 1 > 0, satisfying (3.5). By truncating the running cost c1, one can show that 1 satisfies (3.6) (see, [4, Lemma 3.5]) .
Next we prove the stochastic representation (3.7). Applying It^o-Dynkin formula and Fatou's lemma, for any minimizing selector v1 of (3.5) we have

1(i)  Eiv1,v^2

e

^(B1 0

)

(c1

(Y

(t),v1

(Y

(t-)),v^2

(Y

(t-)))-1

)dt

1

(Y

(^(B1)))



inf
v1 S1

Eiv1 ,v^2

e

^(B1 0

)(c1

(Y

(t),v1 (Y

(t-)),v^2

(Y

(t-)))-1 )dt 1 (Y

(^(B1)))

i  Bc1 .

(3.13)

Again, by applying It^o-Dynkin formula, from (3.1) for any v1  S1, T > 0 and i  Dn  Bc1 it follows that

1,n(i)  Eiv1,v^2

e

^(B1 ) 0

(Dn)T

(c1 (Y

(t),v1

(Y

(t-)),v^2 (Y

(t-)))-1,n

)dt 1,n (Y

(^(B1)





(Dn)



T

))

 Eiv1,v^2

e

^(B1 0

)(c1(Y

(t),v1

(Y

(t-)),v^2

(Y

(t-)))-1,n

)dt1,n

(Y

(^(B1)))I{^(B1) (Dn)T }

+ Eiv1,v^2

e

T 0

(c1(Y

(t),v1 (Y

(t-)),v^2 (Y

(t-)))-1,n )dt 1,n (Y

(T

))I{T

^(B1 )

(Dn )}

.

(3.14)

Using (3.11) and the fact that 1,n  W (by our construction), we have

Eiv1,v^2

e

T 0

(c1

(Y

(t),v1

(Y

(t-)),v^2

(Y

(t-)))-1,n

)dt1,n

(Y

(T ))I{T ^(B1) (Dn)}

 e( c1 -1,n-)T Eiv1,v^2 eT  W (Y (T ))I{T ^(B1) (Dn)}  e( c1 -1,n-)T W (i) .

Thus, letting T   from (3.14) we get

1,n(i)  Eiv1,v^2

e

^(B1 0

)

(c1

(Y

(t),v1

((Y

(t-))),v^2

(Y

(t-)))-1,n

)dt

1,n

(Y

(^(B1)))I{^(B1) (Dn)}

Now, since 1,n  W using (3.11) by dominated convergence theorem it follows that

1(i)  Eiv1,v^2

e

^(B1 0

)

(c1(Y

(t),v1

(Y

(t-)),v^2

(Y

(t-)))-1

)dt

1(Y

(^(B1)))

i  Bc1 .

(3.15)

11

Since v1  S1 is arbitrary, combining (3.13) and (3.15), we obtain (3.7). Also, it it clear from the proof that for any minimizing selector v1 of (3.5) we have

1(i) = Eiv1,v^2

e

^(B1 0

)

(c1(Y

(t),v1

(Y

(t-)),v^2

(Y

(t-)))-1

)dt

1

(Y

(^(B1)))

i  Bc1 .

(3.16)

Using (3.12) it is easy to check that the same conclusion holds under Assumption 2.2(b) .
Now exploiting the stochastic representation (3.7), we show that (1, 1)  R+ × L1W, is the minimal eigenpair. Suppose (^1, ^1)  R+ × L1W,, ^1 > 0 is an eigenpair satisfying

 ^1^1(i) = inf
v1 V1 (i)

 

^1(i0) = 1.

^1(j)ij (v1, v^2(i)) + c1(i, v1, v^2(i))^1(i)
jS

for i  S,

(3.17)

We want to show that 1  ^1. If not suppose that 1 > ^1. Then, for any minimizing selector v^1 of (3.17), applying It^o-Dynkin formula and Fatou's lemma, we obtain

^1(i)  Eiv^1,v^2

e

^(B1 0

)

(c1(Y

(t),v^1

(Y

(t-)),v^2

(Y

(t-)))-^1

)dt

^1

(Y

(^(B1)))

i  Bc1 .

(3.18)

Whereas from (3.7), we have

1(i)  Eiv^1,v^2

e

^(B1 0

)(c1(Y

(t),v^1

(Y

(t-)),v^2

(Y

(t-)))-^1

)dt

1

(Y

(^(B1)))

i  Bc1 .

(3.19)

Let

^

:=

minB1

^1 1

.

Hence,

from

(3.18)

and

(3.19)

it

follows

that

(^1 - ^1) 

0

in

S

and

(^1 - ^1)(~i0) = 0 for some ~i0  B1 . Now, combining (3.5) and (3.17) we deduce that

(^1 - ^1)(j)~i0j (v^1(~i0), v^2(~i0))  0 .
j =~i0

(3.20)

Since Y (t) is irreducible under (v^1, v^2), in view of (3.20) it is clear that (^1 - ^1)  0. Again, since ^1(i0) = 1(i0) =1, we get ^1  1. But this is a contradiction to the fact that 1 > ^1. Thus we deduce that (1, 1)  R+ × L1W, is the minimal eigenpair. Following
the above argument one can show that any eigenfunction satisfying (3.7) is unique upto

a scalar multiplication. Also, by the similar argument, one can show that there exists a minimal eigenpair (2, 2)  R+ × L1W, satisfying (3.8), (3.9) and (3.10). This completes the proof.

To proceed further we establish some technical results needed later.
Lemma 3.3. Let Assumptions 2.1, 2.2, and 2.3 hold. Then the maps v^1  2v^1 from S1  L W , v^1  v2^1 from S1  R+, v^2  1v^2 from S2  L W , and v^2  v1^2 from S2  R+ are continuous.

12

M. K. GHOSH, S. GOLUI, C. PAL, AND S. PRADHAN

Proof. Let {v2,n} be a sequence in S2 such that v2,n  v~2 in S2, i.e., for each i  S, v2,n(i)  v~2(i) in V2(i). Now by Theorem 3.1, there exists (v12,n , 1v2,n )  R+ × LW 1,, 1v2,n > 0
satisfying

v12,n 1v2,n (i)

=

inf
v1 V1 (i)

1v2,n (j)ij (v1, v2,n(i)) + c1(i, v1, v2,n(i))1v2,n (i)
jS

,

(3.21)

with 1v2,n (i0) = 1. Now, since 1v2,n  L1W,, by a standard diagonalization argument, there exists a function 1  L1W, such that 1v2,n (i)  1(i) as n   for all i  S. Also, {v12,n }
is a bounded sequence. Hence, along a suitable subsequence (without loss of generality denoting by the same notation) v12,n  1. Now from (3.21), for any v1  V1(i) we deduce
that

v12,n 1v2,n (i)  This implies that

1v2,n (j)ij (v1, v2,n(i)) + c1(i, v1, v2,n(i))1v2,n (i) .
jS

v12,n 1v2,n (i) - 1v2,n (i)ii(v1, v2,n(i)) 

1v2,n (j)ij (v1, v2,n(i)) + c1(i, v1, v2,n(i))1v2,n (i) .
j=i
(3.22)

Note that

1v2,n (j)ij (v1, v2,n(i))  W (j)ij (v1, v2,n(i)).

j=i

j=i

(3.23)

Thus, using Lemma 3.1, generalized Fatou's lemma in [21, Lemma 8.3.7] and taking n  

in (3.22), we get

Hence,

11(i) 

1(j)ij (v1, v~2(i)) + c1(i, v1, v~2(i))1(i) .

jS

11(i)



inf
v1 V1 (i)

1(j)ij (v1, v~2(i)) + c1(i, v1, v~2(i))1(i)
jS

.

Since V1(i) is compact, there exist v1,n, v1  S1 such that v1,n  v1 satisfying

(3.24)

1v2,n 1v2,n (i) =

1v2,n (j)ij (v1,n(i), v2,n(i)) + c1(i, v1,n(i), v2,n(i))1v2,n (i) .

jS

(3.25)

Now, using Lemma 3.1, the dominated convergent theorem and passing n   in (3.25),

we obtain

11(i) =

1(j)ij (v1(i), v~2(i)) + c1(i, v1(i), v~2(i))1(i) ,

jS

13

Therefore

11(i)



inf
v1 V1 (i)

1(j)ij (v1, v~2(i)) + c1(i, v1, v~2(i))1(i)
jS

.

Hence, from (3.24), and (3.26), it follows that

(3.26)

11(i)

=

inf
v1 V1 (i)

1(j)ij (v1, v~2(i)) + c1(i, v1, v~2(i))1(i)
jS

.

(3.27)

Since v1~2 is the minimal eigenvalue corresponding to v~2 of (3.27), we have 1  v1~2. Suppose 1 > v1~2. Now, from Theorem 3.1, for any minimizing v^1  S1 of (3.5), there exists a finite
set B1  K, such that

1(i) = Eiv^1,v~2

e

^(B1 0

)

(c1

(Y

(t),v^1 (Y

(t)),v~2

(Y

(t)))-v1~2

)dt1(Y

(^(B1)))

i  Bc1,

(3.28)

where ^(B1) = inf{t : Y (t)  B1} =: ~1. Since 1 > v1~2, by similar arguments as in [4, Lemma 3.4] we deduce that

1(i)  Eiv^1,v~2

e

^(B1 0

)

(c1

(Y

(t),v^1 (Y

(t)),v~2

(Y

(t)))-v1~2

)dt1(Y

(^(B1)))

i  Bc1.

(3.29)

From (3.28) and (3.29), we obtain

(1 - 1)(i)  Eiv^1,v~2

e

^(B1 0

)(c1(Y

(t),v^1

(Y

(t)),v~2

(Y

(t)))-v1~2

)dt(1

-

1)(Y

(^(B1)))

i  Bc1.

(3.30)

Now

choosing

an

appropriate

constant



(e.g.,



=

maxB1

1 1

),

we

have

(1

-

1)



0

in

B1 and for some ^i0  B1, (1 - 1)(^i0) = 0. Thus, in view of (3.30), we get (1 - 1)  0

in S. Now combining (3.5) and (3.27), we get

v1~2 (1 - 1)(^i0) 

(1 - 1)(j)^i0j (v^1(^i0), v~2(^i0)) + c1(^i0, v^1(^i0), v~2(^i0))(1 - 1)(^i0) .

jS

This implies that

(1 - 1)(j)^i0j(v^1(^i0), v~2(^i0)) = 0 .

(3.31)

j =^i0

Since, {Y (t)}t0 is irreducible under (v^1, v~2)  S1 × S2, from (3.31) it follows that 1  1.

But this is a contradiction to the fact that 1 > v1~2 . Hence, we deduce that 1 = v1~2. This

proves the continuty of the map v^2  v1^2. Since 1v^2,n (i0) = 1 for all n  1, we have

1(i0) = 1. Hence by Theorem 3.1, we have 1 is the unique solution of (3.5). Thus 1 = 1v~2 . This proves the continuity of the map v^2  1v^2 . Continuity of other maps

follows by the similar argument.

Fix v^2  S2. For each i  S, v1  V1(i), set

F~1(i, v1, v^2(i)) =

1v^2 (j)ij (v1, v^2(i)) + c1(i, v1, v^2(i))1v^2 (i) ,

jS

14

M. K. GHOSH, S. GOLUI, C. PAL, AND S. PRADHAN

where 1v^2 is the solution of (3.5) corresponding to the strategy v^2  S2. Let

H~ (v^2) =

v^1



S1

:

F~1(i, v^1(i), v^2(i))

=

inf
v1 V1 (i)

F~1(i, v1, v^2(i))



i



S

.

Then by the compactness of each V1(i), it follows that H~ (v^2) is a non empty subset of S1. It is obvious that, H~ (v^2) is convex and closed. Since S1 is compact, H~ (v^2) is also compact.

Similarly, for i  S, v^1  S1, v2  V2(i), we set

F~2(i, v^1(i), v2) =

2v^1 (j)ij (v^1(i), v2) + c2(i, v^1(i), v2)2v^1 (i) , i  S,

jS

where 2v^1 is the solution of (3.8) corresponding to the strategy v^1  S1. Let

H~ (v^1) =

v^2



S2

:

F~2(i, v^1(i), v^2(i))

=

inf
v2 V2 (i)

F~2(i, v^1(i), v2)



i



S

.

Then by analogous arguments, H~ (v^1) is nonempty, convex and is a compact subset of S2.

Next set

H~ (v^1, v^2) = H~ (v^2) × H~ (v^1).
From the above argument it is clear that H~ (v^1, v^2) is nonempty, convex, and is a compact subset of S1 × S2. Therefore we may define a map from S1 × S2  2S1 × 2S2.

3.1. The existence of Nash equilibria. Next lemma proves upper-semicontinuity of certain set valued map. This result will be useful in establishing existence of a Nash equilibrium in the space of stationary Markov strategies.

Lemma 3.4. Let Assumptions 2.1, 2.2, and 2.3 hold. Then the map (v^1, v^2)  H~ (v^1, v^2) from S1 × S2  2S1 × 2S2 is upper semicontinuous.

Proof. Let {(v1m, v2m)}  S1 × S2 and (v1m, v2m)  (v^1, v^2) in S1 × S2, i.e., for each i  S, (v1m(i), v2m(i))  (v^1(i), v^2(i)) in V1(i) × V2(i). Let vm1  H~ (v2m). Then {vm1 }  S1. Since S1 is compact, it has a convergent subsequence (denoted by the same sequence by an abuse
of notation), such that

vm1  v1 in S1.

Then

(v

m 1

,

v2m)



(v1, v^2)

in

S1

× S2.

Note

that

ij(vm1 , v2m(i))1v2m (j) 

ij (vm1 , v2m(i))W (j).

j=i

j=i

Thus from [[21], Lemma 8.3.7], Assumption 2.3 and the (product) topology of Sk, k = 1, 2, it follows that for each i  S,

ij

(v

m1 ,

v2m

(i))1v2m

(j)

+

c1

(i,

v

m 1

,

v2m

(i))1v2m

(i)

jS

15

converges to Hence we have

ij(v1, v^2(i))1v^2 (j) + c1(i, v1, v^2(i))1v^2 (i).
jS

lim
m

F~1

(i,

vm1 (i),

v2m

(i))

=

F~1

(i,

v1(i),

v^2(i)).

(3.32)

Now fix v~1  S1 and consider the sequence (v~1, v2m). Using the analogous arguments as

above, we conclude that

lim
m

F~1(i,

v~1(i),

v2m(i))

=

F~1(i,

v~1(i),

v^2(i)).

Since vm1  H(v2m), for any m we have

F~1(i, v~1(i), v2m(i))  F~1(i, vm1 (i), v2m(i)).

(3.33)

Thus, in view of (3.32) and (3.33), taking m   in the above equation, for any v~1  S1 we get

F~1(i, v~1(i), v^2(i))  F~1(i, v1(i), v^2(i)).

Therefore, v1  H~ (v^2). Suppose vm2  H~ (v1m) and along a subsequence vm2  v2 in S2. Then, by the similar arguments as above one can show that v2  H~ (v^1). This proves that
the map (v^1, v^2)  H~ (v^1, v^2) is upper-semicontinuous.

Theorem 3.2. Grant Assumptions 2.1, 2.2, and 2.3. Then there exists a Nash equilibrium in the space of stationary Markov strategies S1 × S2.

Proof. By Lemma 3.4 and Fan's fixed point theorem [8], there exists a fixed point (v^1, v^2)  S1 × S2, for the map (v^1, v^2)  H~ (v^1, v^2) from S1 × S2  2S1 × 2S2 , i.e.,

(v^1, v^2)  H~ (v^1, v^2).

This implies that (1v^2 , 1v^2 ), (2v^1 , 2v^1 ) satisfy the following coupled HJB equations:


  

v1^2 1v^2 (i)














 

1v^2 (i0) = 1

= inf
v1 V1 (i)

ij(v1, v^2(i))1v^2 (j) + c1(i, v1, v^2(i))1v^2 (i)
jS

=

jS ij(v^1(i), v^2(i))1v^2 (j) + c1(i, v^1(i), v^2(i))1v^2 (i) ,

(3.34)

and


  

2v^1 2v^1 (i)













  

2v^2 (i0) = 1.

= inf
v2 V2 (i)

ij (v^1(i), v2)2v^1 (j) + c2(i, v^1(i), v2)2v^1 (i)
jS

=

jS ij (v^1(i), v^2(i))2v^1 (j) + c2(i, v^1(i), v^2(i))2v^1 (i) ,

(3.35)

16

M. K. GHOSH, S. GOLUI, C. PAL, AND S. PRADHAN

Now by Theorem 3.1, from (3.34), it follows that

1v^2

=

inf
v1 A1

1v1

,v^2

= v1^1,v^2 .

Similarly, from (3.35), we have

2v^1

=

inf
v2 A2

v2^1

,v2

= v2^1,v^2 .

Thus, from equations (3.36) and (3.37), we get 1v1,v^2  v1^1,v^2 ,  v1  A1,

v2^1,v2  v2^1,v^2 ,  v2  A2. Hence (v^1, v^2)  S1 × S2 is a Nash equilibrium. This completes the proof.

(3.36) (3.37)

Now we prove a converse of Theorem 3.2.

Theorem 3.3. Let Assumptions 2.1, 2.2, and 2.3 hold. If (v1, v2)  S1 × S2 is a Nash equilibrium, i.e.,
v11,v2  v11,v2 ,  v1  A1,
2v1,v2  v21,v2 ,  v2  A2.
Then v1  S1 is a minimizing selector of (3.5) (corresponding to fixed strategy v2  S2 of player 2) and v2  S2 is a minimizing selector of (3.8) (corresponding to fixed strategy v1  S1 of player 1).

Proof. Applying analogous arguments as in [[4], Lemma 3.4 and Remark 3.1], one can prove

that

for

the

given

pair

(v1, v2)



S1

×

S2,

there

exists

a

eigenpair

(v11

,v2

,

1v

 1

,v2

)



R

× L W ,

1v1 > 0 and 1v1,v2  0 satisfying

1v1,v2 1v1,v2 (i) = 1v1,v2 (i0) = 1.

jS

ij (v 1(i),

v

 2

(i))1v

1 ,v2

(j)

+

c1(i,

v

 1

(i),

v

 2

(i))1v1

,v2

(i),

(3.38)

Also, for given v2  S2, there exists a minimal eigenpair (1v2 , 1v2 )  R+ × L W , 1v2 > 0,

satisfying


 

v12 1v2 (i)

=

inf
v1 V1 (i)

ij

(v1

,

v2

(i))1v2

(j

)

+

c1(i,

v1

,

v

 2

(i))1v

 2

(i)

jS

,

 

1v2 (i0) = 1.

(3.39)

17

Since v12 is a minimal eigenvalue of (3.39), corresponding to v2, we have

1v2

=

inf
v1 A1

v11 ,v2

.

Also, we have

(3.40)

v11,v2  1v1,v2 ,  v1  A1.

Hence,

inf
v1 A1

1v1

,v2



v11,v2 .

So, by (3.40) and (3.41), we obtain

(3.41)

1v2  1v1,v2 .

Also, from (3.40), we have

1v2  1v1,v2 .

Hence, we deduce that

1v2 = 1v1,v2 .

(3.42)

Now, applying Ito-Dynkin formula, from (3.38), it follows that

1v1,v2 (i) = Eiv1,v2

e

T 0

^(B1 )

(c1

(Y

(t),v1 (Y

(t)),v 2 (Y

 (t)))-v11,v2 )dt

v1 ,v2 1

(Y

(T

 ^(B1)))

i  Bc1,

where B1 is as in Theorem 3.1. Now, by Fatou's Lemma, taking T   in the above

equation, we get

1v

 1

,v2

(i)



Eiv1 ,v2

e

^(B1 ) 0

(c1

(Y

(t),v 1 (Y

(t)),v 2 (Y

(t)))-v1 1 ,v2

)dt 1v 1 ,v2

(Y

(^(B1)))

i  Bc1.

(3.43)

Again, using (3.39), from Theorem 3.1, it follows that

1v

 2

(i)



Eiv1 ,v2

e

^(B1 0

)

(c1

(Y

(t),v1 (Y

(t)),v 2 (Y

(t)))-1v

 2

)dt1v2

(Y

(^(B1)))

i  Bc1.

(3.44)

So, by (3.43) and (3.44), we obtain

1v1,v2 (i) - 1v2 (i)  Eiv1,v2

e

^(B1 ) 0

(c1(Y

(t),v 1 (Y

(t)),v 2 (Y

(t)))-1v

 2

)dt (1v1 ,v2

-

1v2 )(Y

(^(B1)))

i  Bc1. (3.45)

Now arguing as in the proof of Lemma 3.3, we obtain 1v1,v2 (i)  1v2 . Thus, from (3.38) and (3.39) it follows that v1 is a minimizing selector of (3.5) (for fixed strategy v2  S2 of player 2). Following similar arguments one can show that v2 is a minimizing selector of (3.8) (for fixed strategy v1  S1 of player 1). This completes the proof.

18

M. K. GHOSH, S. GOLUI, C. PAL, AND S. PRADHAN

4. Example

In this section, we present an illustrative example in wherein transition rates are unbounded and cost rates are nonnegative and unbounded.

Example 4.1. Consider a shop which deals with only one type of product for buying and selling. Suppose there are two workers, say, player 1 and player 2 for buying and selling the products, respectively. The number of stocks in the shop is a finite subset of the set of natural numbers N at each time t  0. There are `natural' buying and selling rates, say µ~ and , respectively, and buying parameters h1 controlled by player 1 and selling parameters h2 controlled by player 2. When the state of the system is i  S := {1, 2, · · · } (i.e., number of items in the shop) , player 1 takes an action u1 from a given set U1(i), which may increase (h1(i, u1)  0) or decrease (h1(i, u1)  0) the buying rate. These actions produce a payoff denoted by r1(i, u1) per unit time. Similarly, if the state is i  S, player 2 takes an action u2 from a set U2(i) to decrease (h2(i, b)  0) or to increase (h2(i, b)  0) the selling rate. These actions result in a payoff denoted by r2(i, u2) per unit time. We assume that when the stock of items in the shop becomes 1, the first player may buy any number of stocks of that item as much as he/she likes depending upon the availability of cash. In addition, we assume that player k, (k = 1, 2) `gets' a reward rk(i) := pki or incurs a cost rk(i) := pki for each unit of time during which the system remains in the state i  S, where pk > 0 is a fixed reward fee, and pk < 0, a fixed cost fee, per stock, from the owner. We next formulate this model as a continuous-time Markov game. The corresponding transition rate ij(u1, u2) and payoff rate c¯k(i, u1, u2) for player k(k = 1, 2) are given as follows: for (1, u1, u2)  K (K as in the game model (2.1)).

1j (u1, u2) > 0 j  2, such that 1j (u1, u2) = 0, and 1j (u1, u2)  e-2j  j  2,
jS
(4.1)

where  > 0 is a constant.

Also, for (i, u1, u2)  K with i  2,


   ij (u1, u2) =
  

i + h2(i, u2), if j = i - 1 -µ~i - i - h1(i, u1) - h2(i, u2), if j = i µ~i + h1(i, u1), if j = i + 1 0, otherwise.

c¯1(i, u1, u2) := ip1 - r1(i, u1), c¯2(i, u1, u2) = ip2 - r2(i, u2) for (i, u1, u2)  K. (4.2)
We now investigate conditions under which there exists a Nash-equilibrium. To this end we make following assumptions:
(I) For each i  S, U1(i) = U2(i) = [0, L], L > 0 is a constant.

19

(II) Let   µ~ > 0, µ~i + h1(i, u1) > 0, and i + h2(i, u2) > 0 for all (i, u1, u2)  K with

i  2; and assume that h1(1, u1) > 0 and h2(1, u2) = 0 for all (u1, u2)  U1(i)×U2(i).

(III) The functions h1(i, u1), h2(i, u2), r1(i, u1), r2(i, u2), and 11(u1, u2) are continu-

ous in (u1, u2) for each fixed i  S. Suppose there exists a finite set K such that

hk(i, uk) =

uk ei

IK

(i)

and

1



K.

Also assume that inf(u1,u2)U1(·)×U2(·) rk(·, uk) is

norm like function for k = 1, 2.

(IV) Suppose ipk-rk(i, uk)  0 i  S, (u1, u2)  U1(i)×U2(i) and (1-e-)+(1-e)µ~ >

pk for k = 1, 2.

Proposition 4.1. Under conditions (I)-(IV), the above controlled system satisfies the Assumptions 2.1, 2.2, and 2.3. Hence by Theorem 3.2, there exists a Nash-equilibrium.

Proof. Take a Lyapunov function as V (i) := ei for i  S for some  > 0 as described earlier. Then, we have V (i)  1 for all i  S. Now for each i  2, and (u1, u2)  U1(i) × U2(i), we have

ij (u1, u2)V (j) = i(i-1)(u1, u2)V (i - 1) + V (i)ii(u1, u2) + V (i + 1)i(i+1)(u1, u2)
jS

= ei (i + h2(i, u2))e- - (iµ~ + i + h1(i, u1) + h2(i, u2)) + (µ~i + h1(i, u1))e

= eii

µ~(e - 1) + (e- - 1) +

eh1(i, u1) + e-h2(i, u2) - h1(i, u1) - h2(i, u2) i

= iV (i)[µ~(e - 1) + (e- - 1)] + u1(e - 1) + u2(e- - 1) IK(i)

 iV (i)[µ~(e - 1) + (e- - 1)] + L(e - 1)IK(i).

(4.3)

Now for every  > 0, we know

(e- - 1) + µ~(e - 1) < 0  µ~ < e-.

Let [µ~(e - 1) + (e- - 1)] = - for some  > 0. Also, let (i) = i and C4 = max L(e -

1),

e-2 1-e-

(see (4.5)). Then for i  2,

sup

V (j)ij(u1, u2)  C4IK(i) - (i)V (i) i  S.

(u1,u2)U1(i)×U2(i) jS

Also, we have

1j (u1, u2)V (j) < ¯11(u1, u2)e +

e-2j ej

 ¯11(u1, u2)e +

e-2 1 - e-

< .

jS

j2

Since -(i) < 1 for all i  S. Hence from (4.4) and (4.5), for i  1, we have

(4.4) (4.5)

ij(u1, u2)V (j)  C1V (i) + C2, where C1 = 1 and C2 = C4.
jS

(4.6)

20

M. K. GHOSH, S. GOLUI, C. PAL, AND S. PRADHAN

For i  2,

-ii(u1, u2) = µ~i + i + h1(i, u1) + h2(i, u2)

 i(µ~ + ) + 2L



1 

(µ~

+

)V

(i) + 2LV (i)

=

(2L

+

µ~

+

)

1 

V

(i)

= C3V (i).

Take W = W~ = V . Now for k = 1, 2

(4.7)

(i) -

sup

c¯k(i, u1, u2) = i - ipk + inf rk(i, uk)

(u1 ,u2 )U1 (i)×U2 (i)

uk Uk (i)

= ik + inf rk(i, uk).
uk Uk (i)

(4.8)

We see that from condition (IV), that k = -pk  0. So, (i)-sup(u1,u2)U1(i)×U2(i) c¯k(i, u1, u2) is norm-like function for k = 1, 2. Now by (4.6), we say Assumption 2.1 (i) holds. Also by

(4.1) and (4.7), Assumption 2.1 (ii) is verified.

Now we verify Assumption 2.2. By (4.4), (4.5) and (4.8), it is easy to see that Assumption

2.2 is satisfied.

Now by condition (III) and (4.2), we say ck(i, u1, u2) and ij(u1, u2) are continuous in

(u1, u2)  U1(i) × U2(i) for each fixed i, j  S and for k = 1, 2. So, Assumption 2.3 (i) is

verified. By (4.3) and (4.5) and condition (III), we say that Assumption 2.3 (ii) is verified.

Also, from (4.1) it is easy to see that Assumption 2.3 (iii) is satisfied.

Hence by Theorem 3.2 there exists a Nash-equilibrium for this controlled process.

5. Acknowledgment
The research work of Mrinal K. Ghosh is partially supported by UGC Centre for Advanced Study. The research work of Somnath Pradhan is partially supported by a National Postdoctoral Fellowship PDF/2020/001938.

References
[1] A. BASU AND M. K. GHOSH, Nonzero-sum risk-sensitive stochastic games on a countable state space, Math. Oper. Res., 43(2) (2018), pp. 516-532.
[2] A. BASU AND M. K. GHOSH, Zero-sum risk-sensitive stochastic games on a countable state space, Stochastic Process. Appl., 124(1) (2014), pp. 961-983.
[3] R. BELLMAN, Dynamic Programming, Princeton University Press, Princeton, N. J., (1957). [4] A. BISWAS AND S. PRADHAN, Ergodic risk-sensitive control of Markov processes on countable state
space revisited, ArXiv e-prints 2104.04825 (2021), available at https://arxiv.org/abs/2104.04825. [5] R. CAVAZOS-CADENA AND E. FERNANDEZ-GAUCHERAND, Controlled Markov chains with risk-
sensitive criteria: average cost, optimality equations, and optimal solutions, Math. Methods Oper. Res., 49 (1999), pp. 299-324.

21
[6] G. B. Di MASI AND L. STETTNER, Risk-sensitive control of discrete-time Markov processes with infinite horizon, SIAM J. Control Optim., 38(1)(1999), pp. 61-78.
[7] G. B. Di MASI AND L. STETTNER, Infinite horizon risk-sensitive control of discrete time Markov processes under minorization property, SIAM J. Control Optim., 46(1) (2007), pp. 231-252.
[8] K. FAN, Fixed-point and minimax theorems in locally convex topological linear spaces, Proc. Nat. Acad. Sc., 38 (1952), pp. 121-126.
[9] W. H. FLEMING, AND D. HERNANDEZ-HERNANDEZ, Risk-sensitive control of finite state machines on an infinite horizon, SIAM J. Control Optim. 35(5) (1997), pp. 1790-1810.
[10] M. K. GHOSH, K. S. KUMAR, AND C. PAL, Zero-sum risk-sensitive stochastic games for continuoustime Markov chains, Stoch. Anal. Appl., 34 (2016), pp. 835-851.
[11] M. K. GHOSH AND S. SAHA, Risk-sensitive control of continuous-time Markov chains, Stochastics, 86 (2014), pp. 655-675.
[12] S. GOLUI AND C. PAL, Continuous-time zero-sum games for Markov chains with risk-sensitive finite-horizon cost criterion, Stoch. Anal. Appl., (2021), available at https://doi.org/10.1080/07362994.2021.1889381.
[13] X. P. GUO AND O. HERNANDEZ-LERMA, Zero-sum games for continuous-time Markov chains with unbounded transition and average payoff rates, J. Appl. Probab., 40(2) (2003), pp. 327-345.
[14] X. P. GUO AND O. HERNANDEZ-LERMA, Nonzero-sum games for continuous-time Markov chains with unbounded discounted payoffs, J. Appl. Probab., 42(2) (2005) pp. 303-320.
[15] X. P. GUO AND O. HERNANDEZ-LERMA, Zero-sum games for continuous-time jump Markov processes in Polish spaces: discounted payoffs, Adv. in Appl. Probab. 39(3) (2007) pp. 645-668.
[16] X. P. GUO AND O. HERNANDEZ-LERMA, Continuous-Time Markov decision processes: Theory and Applications, Stoch. modelling and Appl. Probab., Springer, Berlin, 62 (2009).
[17] X. P. GUO AND Z. W. LIAO, Risk-sensitive discounted continuous-time Markov decision processes with unbounded rates, SIAM J. Control Optim., 57 (2019), pp. 3857-3883.
[18] X. P. GUO, Q. LIU, AND Y. ZHANG, Finite horizon risk-sensitive continuous-time Markov decision processes with unbounded transition and cost rates, 4OR, 17 (2019), pp. 427-442.
[19] X. P. GUO AND A. PIUNOVSKIY, Discounted continuous-time Markov decision processes with constraints: Unbounded transition and loss rates, Math. Oper. Res., 36 (2011), pp. 105-132.
[20] X. P. GUO AND X. SONG, Discounted continuous-time constrained Markov decision processes in polish spaces, Ann. Appl. Probab., 21 (2011), pp. 2016-2049.
[21] O. HERNANDEZ-LERMA, J. LASSERRE, Further topics on discrete-time Markov control processes, Springer, New York, (1999).
[22] M. Y. KITAEV, Semi-Markov and jump Markov controlled models: Average cost criterion, SIAM Theory Probab. Appl., 30 (1995), pp. 272-288.
[23] M. Y. KITAEV AND V.V. RYKOV, Controlled Queueing Systems, CRC Press, Boca Raton, (1995). [24] K.S. KUMAR AND C. PAL, Risk-sensitive control of jump process on denumerable state space with
near monotone cost, Appl. Math. Optim., 68 (2013), pp. 311-331. [25] K.S. KUMAR AND C. PAL, Risk-sensitive control of continuous-time Markov processes with denumer-
able state space, Stoch. Anal. Appl., 33 (2015), pp. 863-881. [26] C. PAL AND S. PRADHAN, Risk sensitive control of pure jump processes on a general state space,
Stochastics, 91(2) (2019), pp. 155-174. [27] A. PIUNOVSKIY AND Y. ZHANG, Discounted continuous-time Markov decision processes with un-
bounded rates: The convex analytic approach, SIAM J. Control Optim., 49 (2011), pp. 2032-2061. [28] Q. WEI, Nonzero-sum risk-sensitive finite-horizon continuous-time stochastic games, Statistics & Prob-
ability Letters, 147 (2019), pp. 96-104. [29] P. WHITTLE, Risk-Sensitive Optimal Control, Wiley-Inter science Series in Systems and Optimization,
John Wiley & Sons Ltd., Chichester, (1990). [30] Y. ZHANG, Continuous-time Markov decision processes with exponential utility, SIAM J. Control Op-
tim., 55 (2017), pp. 2636-2660.

22

M. K. GHOSH, S. GOLUI, C. PAL, AND S. PRADHAN

Department of Mathematics, Indian Institute of Science, Bangalore-560012, India. Email address: mkg@iisc.ac.in
Department of Mathematics, Indian Institute of Technology Guwahati, Guwahati, Assam, India
Email address: golui@iitg.ac.in
Department of Mathematics, Indian Institute of Technology Guwahati, Guwahati, Assam, India
Email address: cpal@iitg.ac.in
Department of Mathematics, Indian Institute of Science Education and Research, Pune, Maharashtra-411008, India
Email address: somnath@iiserpune.ac.in

