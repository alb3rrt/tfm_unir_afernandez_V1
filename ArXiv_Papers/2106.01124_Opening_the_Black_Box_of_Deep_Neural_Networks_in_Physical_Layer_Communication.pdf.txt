SUBMITTED TO IEEE WIRELESS COMMUNICATIONS LETTERS, VOL. XX, NO. X, JUNE 2021

1

Opening the Black Box of Deep Neural Networks in Physical Layer Communication
Jun Liu, Kai Mei, Dongtang Ma, Senior Member, IEEE and Jibo Wei, Member, IEEE

arXiv:2106.01124v2 [eess.SP] 6 Jun 2021

Abstract--Deep Neural Network (DNN)-based physical layer techniques are attracting considerable interest due to their potential to enhance communication systems. However, most studies in the physical layer have tended to focus on the application of DNN models to wireless communication problems but not to theoretically understand how does a DNN work in a communication system. In this letter, we aim to quantitatively analyse why DNNs can achieve comparable performance in the physical layer comparing with traditional techniques and their cost in terms of computational complexity. We further investigate and also experimentally validate how information is flown in a DNN-based communication system under the information theoretic concepts.
Index Terms--Deep neural network (DNN), physical layer communication, information theory.
I. INTRODUCTION
D EEP neural networks (DNN) have recently drawn a lot of attention as a powerful tool in science and engineering problems such as protein structure prediction, image recognition, speech recognition and natural language processing that are virtually impossible to explicitly formulate. Although the mathematical theories of communication systems have been developed dramatically since Claude Elwood Shannon's monograph "A mathematical theory of communication" [1] provides the foundation of digital communication, the wireless channel-related gap between theory and practice motivates researchers to implement DNNs in existing physical layer communication. In order to mitigate the gap, a natural thought is to let a DNN to jointly optimize a transmitter and a receiver for a given channel model without being limited to componentwise optimization. In [2], a pure data-driven end-to-end communication system is proposed to jointly optimize transmitter and receiver components. Then, the authors consider the linear and nonlinear steps of processing the received signal as a radio transformer network (RTN) which can be integrated into the end-to-end training process. The ideas of end-to-end learning of communication system and RTN through DNN are extended to orthogonal frequency division multiplexing (OFDM) in [3]. Another natural idea is to recover channel state information (CSI) and estimate the channel as accurate as possible by implementing a DNN so that the effects of fading could be
Manuscript received June 2, 2021; revised X X, 2021; accepted X X, 2021. This work was supported in part by National Natural Science Foundation of China (NSFC) under Grant 61931020, 61372099 and 61601480. (Corresponding author: Jun Liu.)
Jun Liu, Kai Mei, Dongtang Ma, and Jibo Wei are with the College of Electronic Science and Technology, National University of Defense Technology, Changsha 410073, China (E-mail: {liujun15, meikai11, dongtangma, wjbhw}@nudt.edu.cn).

reduced. The authors of [4] propose an end-to-end DNN-based CSI compression feedback and recovery mechanism which is further extended with long short-term memory (LSTM) [5]. In [6], a residual learning based DNN designed for OFDM channel estimation is introduced. Furthermore, in order to mitigate the disturbances, in addition to Gaussian noise, such as channel fading and nonlinear distortion, [7] proposes an online fully complex extreme learning machine-based symbol detection scheme.
Comparing with traditional physical layer communication systems, the above-mentioned DNN-based techniques show competitive performance. However, what has been missing is to understand the dynamics behind the DNN in physical layer communication.
In this paper, we attempt to first give a mathematical explanation to reveal the mechanism of end-to-end DNN-based communication systems. Then, we try to unveil the role of the DNNs in the tasks of CSI recovery, channel estimation and symbol detection. We believe that we have developed a concise way to open as well as understand the "black box" of DNNs in physical layer communication. To summarize, our main contributions of this paper are twofold:
· Instead of proposing a scheme combining a DNN with a typical communication system, we analyse the behaviours of a DNN-based communication system from the perspectives of the whole DNN (communication system), encoder (transmitter) and decoder (receiver). Our simulation results verify that the constellations produced by autoencoders are equivalent to the (locally) optimum constellations obtained by the gradient-search algorithm which minimize the asymptotic probability of error in Gaussian noise under an average power constraint.
· We consider the tasks of CSI recovery, channel estimation and symbol detection as a typical inference problem. The information flow in the DNNs of these tasks is estimated by using matrix-based functional of Renyi's -entropy to approximate Shannon's entropy.
The remainder of this paper is organized as follows. In Section II, we give the system model and formulate the problem. Next, simulation results are presented in Section III. Finally, the conclusions are drawn in Section IV.
Notations: The notations adopted in the paper are as follows. We use boldface lowercase x, capital letters X and calligraphic letters X to denote column vectors, matrices and sets respectively. In addition, and E {·} denote respectively the Hadamard product and the expectation operation.

SUBMITTED TO IEEE WIRELESS COMMUNICATIONS LETTERS, VOL. XX, NO. X, JUNE 2021

2

s

x

y

s^

Information Source

Transmitter

ff ()

s

z

Wireless Channel
p(v | z)
v

Receiver gg ()

Destination s^

Encoder

PDF

Decoder

Error Propagation

Autoencoder

B. Understanding Autoencoders on Message Transmission
From the prospective of the whole autoencoder (communication system), it aims to transmit information to destination with low error probability. The symbol error probability, i.e., the probability that the wireless channel has shifted a signal point into another signal's decision region, is

 =

1


 Pr (s^  s|s transmitted).

(1)



=1

The autoencoder can use the cross-entropy loss function

Fig. 1. Schematic diagram of a general communication system and its corresponding autoencoder representation.

II. SYSTEM MODEL AND PROBLEM FORMULATION
In this section, we first describe the considered system model and then provide a detailed explanation of the problem formulation from three different perspectives.

A. System Model

As shown in the upper part of Fig. 1, let's consider the pro-

cess of message transmission from the perspectives of a typical

communication system and an autoencoder, respectively. We

assume that an information source generates a sequence of -

bit message symbols   {1, 2, · · · ,  } to be communicated

to the destination, where  = 2 . Then the modulation

modules inside the transmitter map each symbol  to a signal

x  R , where  denoted the dimension of the signal space.

The signal alphabet is denoted by x1, x2, · · · , x . During channel transmission, -dimensional signal x is corrupted to

y  R with conditional probability density function (PDF)

 (y|x) =

 =1

 (|).

In

this

paper,

we

use

 /2

band-

pass channels, each with separately modulated inphase and

quadrature components to transmit the -dimensional signal

[8]. Finally, the received signal is mapped by the demodulation

module inside the receiver to ^ which is an estimate of the

transmitted symbol . The procedures mentioned above have

been exhaustively presented by Shannon.

From the point of view of filtering or signal inference,

the idea of autoencoder-based communication system matches

Norbert Wiener's perspective [9]. As shown in the lower part

of the Fig. 1, the autoencoder consists of an encoder and a

decoder and each of them is a feedforward neural network

(NN) with parameters (weights)   and , respectively.

Note that each symbol  from information source usually needs

to be encoded to an one-hot vector s  R and then is fed

into the encoder. Under a given constraint (e.g., average signal

power constraint), the PDF of a wireless channel and a loss

function to minimize error symbol probability, the encoder

and decoder are respectively able to learn to appropriately

represent s as z =   (s) and to map the corrupted signal v to a estimate of transmitted symbol s^ =  (v) where z, v  R . Here, we use z1, z2, · · · , z denoted the transmitted signals

from the encoder in order to distinguish it from the transmitted

signals from the transmitter.

Llog

s^, s;   , 

=

-

1







s()

[]

log

s^() []



=1 =1

(2)

=

-

1




s^ ()

[]


=1

to represent the price paid for inaccuracy of prediction where s() [] denotes the th element of the th symbol in a training set with  symbols. In order to train the autoencoder to minimize the symbol error probability, the optimal parameters could be found by optimizing the loss function

 , 


= arg min

Llog s^, s;   , 

(  ,)

(3)

subject to E

z

2 2

 av

where av denotes the average power. In this paper, we set av = 1/. Now, we must be very curious about how do the mappings z =   (s) look like after the training was done.

C. Encoder: Finding a Good Representation

Let's pay attention to the encoder (transmitter). In the domain of communication, an encoder needs to learn a robust representation z =   (s) to transmit s against channel disturbances, such as thermal noise, channel fading, nonlinear distortion, phase jitter, etc. This is equivalent to find an coded (or uncoded) modulation scheme with the signal set of size  to map a symbol s to a point z for a given transmitted power, which maximizes the minimum distance between any two constellation points. Usually the problem of finding good signal constellations for a Gaussian channel1 is associated with the search for lattices with high packing density which is an old and well-studied problem in the mathematical literature [11].
We use the algorithm proposed in [12] to obtain the optimum constellations. Consider a zero-mean stationary additive white Gaussian noise (AWGN) channel with one-sided spectral density 20. For large signal-to-noise ratio (SNR), the asymptotic approximation of the (1) can be written as

  exp

1 - min
80  

z - z 

2 2

.

(4)

1The problem of constellation optimization is usually considered under the condition of the Gaussian channel. Although the problem under the condition of Rayleigh fading channel has been studied in [10], its prerequisite is that the side information is perfect known.

SUBMITTED TO IEEE WIRELESS COMMUNICATIONS LETTERS, VOL. XX, NO. X, JUNE 2021

3

To minimize , the problem can be formulated as

z

 =1

=

arg min

()

{z }=1

(5)

subject to E

z

2 2

 av

where

z

 =1

denotes

the

optimal

signal

set.

This

optimiza-

tion problem can be solved by using a constrained gradient-

search algorithm. We denote {z}=1 as a  ×  matrix

Z = [z1, z2, · · · , z ] .

(6)

Then, the th step of the constrained gradient-search algorithm can be described by

p(v | z)

v

z

v t1 t2 t3 u t3 t2 t1 z^

... ...

z^ 

PDF Training
Data (a)

gg () Decoder

z

p(v | z)

v

p (t1 | v)

t1

p (t2 | t1 )
t2

p

(t3

|

t2

)

p ...

(ts

|

t s-1

)

...

ts

u

...

s^

z^

p (z^ | t1 ) t1 p (t1 | t2 ) t2

...
p (t2 | t3 ) p (ts-1 | ts )

ts

(b)

Z+1 = Z -  (Z)

Z+1 =

Z+1
2

Z+1 [,  ]



(7a) (7b)

Fig. 2. (a) An inference model with a DNN decoder of size (2 - 1) hidden layers for learning. (b) The graph representation of the decoder with ( - 1) hidden layers in both sub encoder and decoder. The solid arrow denotes the direction of input feedforward propagation and the dashed arrow denotes the direction of information flow in the error back-propagation phase.

where  denotes step size and  (Z)  R× denotes the gradient of  respect to the current constellation points. It can be written as

 (Z) = [g1, g2, · · · , g ]

(8)

where

 g  - exp


-

z - z 80

2 2

1

1

+

z - z

2 2

40

1z-z .

(9)

The vector 1z-z denotes -dimensional unit vector in the direction of z - z.

Comparing (3) to (5), the mechanism of the encoder in an

autoencoder-based communication system has been unveiled.

The mapping function of encoder can be represented as



 (s)





=1

z

 =1

(10)

when the PDF used for generating training samples is multivariate zero-mean normal distribution z^ - z  N (0, ) where 0 denotes -dimensional zero vector and  = (20/) I is a  ×  diagonal matrix.

D. Decoder: Inference
Finally, it is the time to zoom in the lower right corner of the Fig. 1 to investigate what happens inside the decoder (receiver). As Fig. 2(a) shown, for the tasks of DNN-based CSI recovery, channel estimate and symbol detection, the problem can be formulated as an inference model. For the sake of convenience, we can denote the target output of the decoder as z instead of s because we can assume z =   (s) is bijection. If the decoder is symmetric, the decoder also can be seen as a sub autoencoder which consists of a sub encoder and decoder. Its bottleneck (or middlemost) layer codes is denoted as u. Here we use z to denote CSI or transmitted symbol which we desire to predict. The decoder infers a prediction z^ =  (v) according to its corresponding measurable variable v.

If the joint distribution  (v, z) is known, the expected (population) risk C(v,z)  , Llog can be written as



1

E Llog z^, z;  =

 (v, z) log  (z|v)

vV,zZ



=

 (v, z) log

1

+   (v, z) log  (z|v)

 (z|v)

 (z|v)

vV,zZ

vV,zZ

=  (z|v) + KL (  (z|v) || (z|v))

  (z|v) (11)

where  (·|v) = (v)   (Z) and KL (  (z|v) || (z|v)) denotes Kullback-Leibler divergence between  (z|v) and  (z|v) [13]2. If and only if the decoder is given by the con-

ditional posterior  (v) = (z|v), the expected (population)

risk reaches the minimum min C(v,z)  , Llog =  (z|v).

In physical layer communication, instead of perfectly know-

ing the channel-related joint distribution  (v, z), we only have



a set of  i.i.d. samples D := v() , z()

from  (v, z).

=1

In this case, the empirical risk is defined as

C^ (v,z)

 , L, D

1




=

L

z,  (v) .

(12)



=1

Practically, the D from  (v, z) usually is a finite set. This leads the difference between the empirical and expected (population) risks which can be defined as

gen (v,z)  , L, D =C (v,z)  , Llog - (13)
C^(v,z)  , L, D .
We now can preliminarily conclude that the DNN-based receiver is an estimator with minimum empirical risk for a given set D, whereas its performance is inferior to the

2If  is a continuous random variable the sum becomes an integral when its PDF exists.

SUBMITTED TO IEEE WIRELESS COMMUNICATIONS LETTERS, VOL. XX, NO. X, JUNE 2021

4

M=8, N=2

M=16, N=2

M=8, N=2

(a) M=16, N=2

M=16, N=3 M=16, N=3

4.5 4.0 3.5 3.0 2.5 2.0

SNR=3 dB, N=64 SNR=15 dB, N=64 SNR=30 dB, N=64 SNR=15 dB, N=128 SNR=15 dB, N=512

Sa(z|zLS)

1.5

1.0

(b)

0.5

Fig. 3. Comparisons of (a) optimum constellations obtained by gradient-

0.0

search technique and (b) constellations produced by autoencoders.

200

B 400

600

800

1000

Size of Training Set

1200

1400

optimal with minimum expected (population) risk under a given joint distribution  (v, z).
Furthermore, it is necessary to quantitatively understand how information flows inside the decoder. Fig. 2(b) shows the graph representation of the decoder where t and t  (1    ) denote th hidden layer representations starting from the input layer and the output layer, respectively. Here, we use the method proposed in [14] to illustrate layer-wise mutual information by three kinds of information planes (IPs) where the Shannon's entropy is estimated by matrix-based functional of Renyi's -entropy [15]. Its details are given in Appendix.
III. SIMULATION RESULTS
In this section, we provide simulation results to illustrate the behaviour of DNN in physical layer communication.
A. Constellation Comparison
Fig. 3(a) shows the optimum constellations obtained by gradient-search technique proposed in [12]. When  = 2 and 3, the algorithm were run allowing for 1000 and 3000 steps, respectively. The step size  = 2 × 10-4. Fig. 3(b) shows the constellations produced by autoencoders which have the same network structures and hyperparameters with the autoencoders mentioned in [2]. The autoencoders were trained with 106 epochs, each of which contains  different symbols.
When  = 2, the two-dimensional constellations produced by autoencoders have a similar pattern to the optimum constellations which form a lattice of (almost) hexagonal. Specifically, in the case of ( = 8,  = 2), one of the constellations found by the autoencoder can be obtained by rotating the optimum constellation found by gradient-search technique. In the case of ( = 16,  = 2), the constellation found by the autoencoder is different from the optimum constellation but it still forms a lattice of (almost) equilateral triangles. In the case of ( = 16,  = 3), one signal point of the optimum constellation is almost at the origin while the other 15 signal points are almost at the surface of a sphere with radius av and centre 0. This pattern is similar to the surface of a truncated

Fig. 4. The entropy  (z |z^LS) with respect to different values of SNR and .
icosahedron which is composed of pentagonal and hexagonal faces. However, the three-dimensional constellation produced by an autoencoder is a local optima which is form by 16 signal points almost in a plane.
From the perspective of computational complexity, the cost to train an autoencoder is significantly higher than the cost of traditional algorithm. Specifically, an autoencoder which has 4 dense layers respectively with , ,  and  neurons needs to train (2 + 1) ( + ) + 2 parameters for 106 epochs whereas the gradient-search algorithm only needs 2 trainable parameters for 103 steps.
B. Information Flow
We consider a common channel estimation problem for an OFDM system with  subcarriers. Let z = [ [0] ,  [1] , · · · ,  [ - 1]] which denotes frequency impulse response (FIR) vector of a channel. For the sake of convenience, we denotes the measurable variable as v = z^LS where z^LS represents the least-square (LS) estimation of z. Usually, it can be obtained by using training symbol-based channel estimation. In this paper, we use linear interpolation and the number of pilots  = /4 = 16.
According to (11), the minimum logarithmic expected (population) risk for this inference problem is  (z|z^LS) which can be estimated by Renyi's -entropy  (z|z^LS) = (z, z^LS) -  (z^LS) with  = 1.01. Fig. 4 illustrates the entropy  (z|z^LS) with respect to different values of SNR and . As can be seen,  (z|z^LS) monotonically decreases as the size of training set increases. When   ,  (z|z^LS) decreases slowly. It is because the joint distribution  (z, z^LS) can be perfectly learned and therefore the empirical risk is approaching to the expected risk. Interestingly, when  > 580, the lower the SNR or the larger input dimension  is, the smaller  is needed to obtain the same value of  (z|z^LS).
Fig. 5(a), (b), (c) illustrate the behaviour of the IP-I, IPII and IP-III in a DNN-based OFDM channel estimator with topology "128 - 64 - 32 - 16 - 8 - 16 - 32 - 64 - 128" where

SUBMITTED TO IEEE WIRELESS COMMUNICATIONS LETTERS, VOL. XX, NO. X, JUNE 2021

5

' T V
I( ; )

4.5 4.0 3.5 3.0 2.5 2.0 1.5 1.0 0.5 0.0
0.0
4.5 4.0 3.5 3.0 2.5 2.0 1.5 1.0 0.5 0.0
0.0

T 1
T 2
T 3
T 4

0.5

1.0

T

T'
-

1

1

T

T'
-

2

2

T

T'
-

3

3

T

T'
-

4

4

0.5

1.0

1.5

2.0

2.5

T V I( ; )

3.0

(a) IP-I

1.5

2.0

2.5

T V I( ; )

3.0

(c) IP-III

3.5 3.5

4.0 4.0

900 880 860 840 820 800 780 760 740 720 700 680 660 640 620 600 580 560 540 520 500 480 460 440 420 400 380 360 340 320 300 280 260 240 220 200 180 160 140 120 100 80 60 40 20 0
4.5

4.5

900 880 860 840 820 800 780 760 740 720 700 680 660 640 620 600 580 560 540 520 500 480 460 440 420 400 380 360 340 320 300 280 260 240 220 200 180 160 140 120 100 80 60 40 20 0

MSE

'T V

'

I( ; )

4.5
4.0
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0 0.0
1.4 1.3 1.2 1.1 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1

T

T'
-

1

1

T

T'
-

2

2

T

T'
-

3

3

T

T'
-

4

4

0.5

1.0

1.5

2.0

2.5

T V I( ; )

3.0

(b) IP-II

3.5

4.0

4.5

900 880 860 840 820 800 780 760 740 720 700 680 660 640 620 600 580 560 540 520 500 480 460 440 420 400 380 360 340 320 300 280 260 240 220 200 180 160 140 120 100 80 60 40 20 0

0

100

200

300

400

500

600

700

800

900

Iterations

(d) Loss Curve

'T V
I( ; )

Fig. 5. The three IPs and loss curve in a DNN-based channel estimator.

linear activation function is considered and the training sample is constructed by concatenating the real and imaginary parts of the complex channel vectors. Batch size is 100 and learning rate  = 0.001. We use  and  to denote the input and output of the decoder, respectively. The number of iterations is illustrated through a colour bar. From IP-I, it can be seen that the final value of mutual information I (;  ) in each layer tends to be equal to the final value of I (; ), which means that the information from  has been learnt and transferred to  by each layer. In IP-II, I ( ;  ) < I (; ) in each layer, which implies that all the layers are not overfitting. The tendency of I (; ) to approach the value of I ( ; ) can be observed from IP-III. Finally, from all the IPs, it is easy to notice that the mutual information does not change significantly when the number of iterations is larger than 200. Meanwhile, according to Fig. 5(d), the MSE reaches a very low value and also does not change sharply. It means that 200 iterations are enough for the task of 64-subcarrier channel estimation using a DNN with the above-mentioned topology.

IV. CONCLUSION
In this paper, we propose a framework to understand the manner of the DNNs in physical communication. We find that a DNN-based transmitter essentially tries to produce a good representation of the information source. Then, we quantitatively analyse the information flow in a DNN-based communication system. We believe that this framework has the potential for the design of DNN-based physical communication.

APPENDIX A MATRIX-BASED FUNCTIONAL OF RENYI'S -ENTROPY

For a random variable  in a finite set X, its Renyi's entropy

of order  is defined as



()

=

1 1-


log
X





() 

(14)

where  () is the PDF of the random variable . Let

 ()

 =1

be

an

i.i.d.

sample

of



realizations

from

the

random

variable  with PDF  (). The Gram matrix K can be defined

as K [, ] =  ,   where  : X × X  R is a real valued

positive definite and infinitely divisible kernel. Then, a matrix-

based analogue to Renyi's -entropy for a normalized positive

definite matrix A of size  ×  with trace 1 can be given by

the functional

1



(A)

=

1

-

log2 


  (A) 

(15)

=1

where  (A) denotes the th eigenvalue of A, a normalized version of K:

1 K [, ]

A [, ] =

.

(16)

 K [, ] K [ , ]

Now, the joint-entropy can be defined as

 (A, B) = 

A tr (A

B B)

.

(17)

Finally, the matrix notion of Renyi's mutual information can be defined as

 (A; B) =  (A) +  (B) -  (A, B) . (18)

REFERENCES
[1] C. E. Shannon, "A mathematical theory of communication," The Bell system technical journal, vol. 27, no. 3, pp. 379­423, 1948.
[2] T. O'shea and J. Hoydis, "An introduction to deep learning for the physical layer," IEEE Transactions on Cognitive Communications and Networking, vol. 3, no. 4, pp. 563­575, 2017.
[3] A. Felix, S. Cammerer, S. Dörner, J. Hoydis, and S. Ten Brink, "OFDMautoencoder for end-to-end learning of communications systems," in 2018 IEEE 19th International Workshop on Signal Processing Advances in Wireless Communications (SPAWC). IEEE, 2018, pp. 1­5.
[4] C.-K. Wen, W.-T. Shih, and S. Jin, "Deep learning for massive MIMO CSI feedback," IEEE Wireless Communications Letters, vol. 7, no. 5, pp. 748­751, 2018.
[5] T. Wang, C.-K. Wen, S. Jin, and G. Y. Li, "Deep learning-based CSI feedback approach for time-varying massive MIMO channels," IEEE Wireless Communications Letters, vol. 8, no. 2, pp. 416­419, 2018.
[6] L. Li, H. Chen, H.-H. Chang, and L. Liu, "Deep residual learning meets OFDM channel estimation," IEEE Wireless Communications Letters, vol. 9, no. 5, pp. 615­618, 2019.
[7] J. Liu, K. Mei, X. Zhang, D. Ma, and J. Wei, "Online extreme learning machine-based channel estimation and equalization for OFDM systems," IEEE Communications Letters, vol. 23, no. 7, pp. 1276­1279, 2019.
[8] B. Sklar and P. K. Ray, Digital Communications Fundamentals and Applications. Pearson Education, 2014.
[9] S. Yu, M. Emigh, E. Santana, and J. C. Príncipe, "Autoencoders trained with relevant information: blending Shannon and Wiener's perspectives," in 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2017, pp. 6115­6119.
[10] J. Boutros, E. Viterbo, C. Rastello, and J.-C. Belfiore, "Good lattice constellations for both Rayleigh fading and Gaussian channels," IEEE Transactions on Information Theory, vol. 42, no. 2, pp. 502­518, 1996.
[11] G. C. Jorge, A. A. de Andrade, S. I. Costa, and J. E. Strapasson, "Algebraic constructions of densest lattices," Journal of Algebra, vol. 429, pp. 218­235, 2015.
[12] G. Foschini, R. Gitlin, and S. Weinstein, "Optimization of twodimensional signal constellations in the presence of Gaussian noise," IEEE Transactions on Communications, vol. 22, no. 1, pp. 28­38, 1974.
[13] A. Zaidi, I. Estella-Aguerri et al., "On the information bottleneck problems: models, connections, applications and information theoretic views," Entropy, vol. 22, no. 2, p. 151, 2020.
[14] S. Yu and J. C. Principe, "Understanding autoencoders with information theoretic concepts," Neural Networks, vol. 117, pp. 104­123, 2019.
[15] L. G. S. Giraldo, M. Rao, and J. C. Principe, "Measures of entropy from data using infinitely divisible kernels," IEEE Transactions on Information Theory, vol. 61, no. 1, pp. 535­548, 2014.

