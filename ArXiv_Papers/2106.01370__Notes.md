
# q-RBFNN:A Quantum Calculus-based RBF Neural Network

[arXiv](https://arxiv.org/abs/2106.01370), [PDF](https://arxiv.org/pdf/2106.01370.pdf)

## Authors

- Syed Saiq Hussain
- Muhammad Usman
- Taha Hasan Masood Siddique
- Imran Naseem
- Roberto Togneri
- Mohammed Bennamoun

## Abstract

In this research a novel stochastic gradient descent based learning approach for the radial basis function neural networks (RBFNN) is proposed. The proposed method is based on the q-gradient which is also known as Jackson derivative. In contrast to the conventional gradient, which finds the tangent, the q-gradient finds the secant of the function and takes larger steps towards the optimal solution. The proposed $q$-RBFNN is analyzed for its convergence performance in the context of least square algorithm. In particular, a closed form expression of the Wiener solution is obtained, and stability bounds of the learning rate (step-size) is derived. The analytical results are validated through computer simulation. Additionally, we propose an adaptive technique for the time-varying $q$-parameter to improve convergence speed with no trade-offs in the steady state performance.

## Comments

Article is under review. This is a preprint version

## Source Code

Official Code

- [https://github.com/musman88/q-RBFNN](https://github.com/musman88/q-RBFNN)

Community Code

- [https://paperswithcode.com/paper/q-rbfnn-a-quantum-calculus-based-rbf-neural](https://paperswithcode.com/paper/q-rbfnn-a-quantum-calculus-based-rbf-neural)

## Bibtex

```tex
@misc{hussain2021qrbfnna,
      title={q-RBFNN:A Quantum Calculus-based RBF Neural Network}, 
      author={Syed Saiq Hussain and Muhammad Usman and Taha Hasan Masood Siddique and Imran Naseem and Roberto Togneri and Mohammed Bennamoun},
      year={2021},
      eprint={2106.01370},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```

## Notes

Type your reading notes here...

