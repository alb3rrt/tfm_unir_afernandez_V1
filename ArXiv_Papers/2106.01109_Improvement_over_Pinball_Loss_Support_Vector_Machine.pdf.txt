PREPRINT

1

Improvement over Pinball Loss Support Vector Machine
Pritam Anand, Reshma Rastogi and Suresh Chandra.

arXiv:2106.01109v1 [cs.LG] 2 Jun 2021

Abstract--Recently, there have been several papers that discuss the extension of the Pinball loss Support Vector Machine (Pin-SVM) model, originally proposed by Huang et al., [1] [2]. Pin-SVM classifier deals with the pinball loss function, which has been defined in terms of the parameter  . The parameter  can take values in [-1, 1]. The existing Pin-SVM model requires to solve the same optimization problem for all values of  in [-1, 1]. In this paper, we improve the existing Pin-SVM model for the binary classification task. At first, we note that there is major difficulty in Pin-SVM model (Huang et al. [1]) for -1   < 0. Specifically, we show that the Pin-SVM model requires the solution of different optimization problem for -1   < 0. We further propose a unified model termed as Unified Pin-SVM which results in a QPP valid for all -1    1 and hence more convenient to use. The proposed Unified Pin-SVM model can obtain a significant improvement in accuracy over the existing Pin-SVM model which has also been empirically justified by extensive numerical experiments with real-world datasets.
Index Terms--Binary classification, support Vector machine, pinball loss, Pin-SVM.

The standard C-SVM model minimizes the Hinge loss function along with the L2-norm regularization in its formulation. Thus, it minimizes

1 2

||w||22

+

C0

l

LHinge(1 - yi(wT (xi) + b)), (1)

i=1

where LHinge = max(u, 0) is the Hinge loss function and C0  0 is the user supplied parameter. The use of Hinge loss function in C-SVM model makes it ignore the data points which satisfy yi(wT (xi) + b) > 1. There are few data points satisfying yi(wT (xi)+b)  1, which contribute for the empirical risk. These data
points are called `support vectors' and lie near the boundary of the separating hyperplane wT (x)+b = 0.
The separating hyperplane in C-SVM model is only
constructed by using these support vectors. This causes
the sparsity in C-SVM model. But, data points near the
boundary of the separating hyperplane may be noisy
which can mislead the resulting separating hyperplane.
To improve the C-SVM model, Huang et al. [2] have
suggested to use the pinball loss function [6] in SVM
model. For the classification problem, the pinball loss
function is given by

I. INTRODUCTION

Lpin0(u) =

u, if u  0, - u, otherwise,

(2)

Support Vector Machines (SVMs) [3] [4] [5] are popular machine learning algorithms. These algorithms are based on Structural Risk Minimization (SRM) principle [4]. For binary classification problem with given training set T = {(xi, yi) : xi  Rn, yi  {-1, 1}, i = 1.2, ..., l}, SVM models obtain a separating kernel generated decision function wT (xi) + b = 0 by minimizing a good trade-off between the empirical risk and model complexity in its optimization problem. SVM models use a loss function to measure the empirical risk of the given training set. For minimizing the model complexity, SVM models minimize a regularization term in their optimization problem.
Pritam Anand is with the Dhirubhai Ambani Institute of Information Technology, Gandhinagar Gujrat, India ­ 382007. (e-mail: ltpritamanand@gmail.com, pritam anand@daiict.ac.in)
Reshma Rastogi is with the Department of Computer Science, South Asian University, New Delhi-110021. (e-mail: reshma.khemchandani@sau.ac.in)
Suresh Chandra was with the Department of Mathematics, Indian Institute of Technology, Delhi-110016. (e-mail: sureshiitdelhi@gmail.com.)

where 0    1 is its parameter. For  = 0, the pinball loss function reduces to the Hinge loss function. For  = 1, it reduces to the l1 loss function.
The Pin-SVM model (Huang et al., [2]) minimizes the empirical risk using the pinball loss function along with the L2-norm regularization in its formulation. This leads to the following optimization problem

min
(w,b)

1 2

||w||22

+

C0

l

Lpin0(1 - yi(wT (xi) + b)),

i=1

which can be equivalently converted to the following optimization problem

min
(w,b,)

1 2

||w||22

+

C0

l

i

i=1

subject to,

yi(wT (xi) + b)  1 - i,

yi(wT (xi)

+

b)



1

+

i 

,

(3)

where 1, 2, .., l are slack variables and C0, 0    1 are user supplied parameters.

PREPRINT

2

The pinball loss function in Pin-SVM model penalizes (assigns positive risk) every data point but, with different rate. Data points satisfying yi(wT (xi) + b)  1 are penalized with unit rate and other data points are penalized with comparatively lower rate  . This penalization in Pin-SVM model causes it to also minimize the scatter of data points along the separating hyperplane. But then, it takes away the very nice property of SVM, namely sparsity. However, the PinSVM model is a general SVM model in the sense that it can reduce to the standard C-SVM model for its parameter  = 0.
To reduce the effect of the unbalanced class labeling, we consider a l-dimensional vector C = (C1, C2, . . . Cl), rather than a single constant C0, such that

which can obtain the solution of Pin-SVM model without bothering the sign of its parameter  in [-1, 1]. We term this proposed model as Unified Pin-SVM model. Section V presents numerical results which empirically verify that the proposed Unified Pin-SVM model corrects the existing Pin-SVM model by minimizing the pinball loss function in true sense.

II. PINBALL LOSS FUNCTION WITH NEGATIVE 
VALUE AND SVM MODEL
Huang et al. extended the pin-ball loss function for negative  using the same expression in their work (Huang et al. [1]) . The pinball loss function with negative  is given by

Lpin0(u) =

u, if u  0. - u, othewise.

(7)

Ci =

C0, pC0,

yi = +1, yi = -1,

(4)

where p is defined as

p=

number of data points on `Class +1' number of data points in `Class -1'

and seek the solution of following optimization

problem

min
(w,b,)

1 2

||w||22

+

l

Cii

i=1

subject to,

yi(wT (xi) + b)  1 - i,

yi(wT (xi)

+

b)



1

+

i 

,





0.

(5)

Rather than solving the primal problem (5), we prefer to solve its Wolfe's dual problem, which is obtained as follows

1l l

min (,) 2

(j - j)(i - i)yiyjK(xi, xj)

i=1 j=1

l
- (i - i)
i=1

subject to,

l

(i - i)yi = 0,

i=1

Ci

-

i

-

1 

i

=

0,

f or i = 1, 2, .., l,

i  0, i  0, f or i = 1, 2, .., l.

(6)

More information about properties of pinball loss function and Pin-SVM model can be found in (Huang et al., [2]).
We organize the rest of this paper as follows. Section II describes the optimization problem of Pin-SVM model for -1   < 0 as proposed in (Hunag et al., [1]). In section III, we derive the right optimization problem for Pin-SVM model for -1   < 0. In section IV, we propose a unified optimization problem

The above pinball loss function (7) is convex loss function for   -1. Huang et al. have formulated the Pin-SVM model for -1   < 0 using

min
(w,b)

1 2

||w||22

+

C

l

Lpin0(1 - yi(wT (xi) + b)).

(8)

i=1

For minimizing (8), they have chosen to minimize the following Quadratic Programming Problem (QPP)

min
(w,b,)

1 2

||w||22

+

l

Cii

i=1

subject to,

yi(wT (xi) + b)  1 - i,

yi(wT

(xi)

+

b)



1

+

i 

,

(9)

where -1   < 0 is user supplied parameter. It should be noted that, Huang et al. have used the same Pin-SVM optimization problem for both positive and negative values of  in [-1, 1]. Contrary to this, we claim in the next section of this paper that the Pin SVM model for -1   < 0 requires the solution of a QPP which is different from (9).

III. PIN-SVM WITH NEGATIVE  VALUES

This paper improves the existing Pin-SVM model for -1   < 0 (Huang et al., [1]). We shall show that the optimization problem of existing Pin-SVM model for -1   < 0 obtained in (Huang et al., [1]) is not correct and derive the right optimization problem for it.
The pinball loss function (7) has been used in (Huang et al., [1] [2]) for -1    1. At first, we consider the loss function

Lpin(u) = max(u, - u)

(10)

for -1    1. For -1    1, we can obtain u, if u  0.
max(u, - u) = - u, otherwise.

PREPRINT

3

It makes us realize that the pinball loss function is as equivalent to the max(u, - u) for -1    1.

Now, we shall state and justify our claim about the existing Pin-SVM model with -1   < 0. We claim that the Pin-SVM model with -1   < 0 (problem

min
(w,b,)

1 2

||w||22

+

l

Cii

i=1

(8)) is not equivalent to the solving QPP (9) used in

subject to,

(Huang et al., [1]) and vice-versa. The justification of this claim is detailed as follows.
The Pin-SVM for -1   < 0 (problem (8)) is

yi(wT (xi) + b)  1 - i,

yi(wT (xi)

+

b)



1

+

 

,

(15)

equivalent to

where Ci are as defined in (4) and -1   < 0. In

min
(w,b)

1 2

||w||22

+

C0

l

max((1 - yi(wT (xi) + b)),

i=1

order to find the solution of above primal problem, we need to derive its corresponding Wolfe`s dual problem. For this, we construct the Lagrangian function for

- (1 - yi(wT (xi) + b))) where - 1   < 0. (11)primal problem (15) as follows

Let us consider slack variables i = max((1 - yi(wT (xi) + b)), - (1 - yi(wT (xi) + b))), i = 1, 2, ..., l. Then, the optimization problem (11) of Pin-
SVM can be given by

min
(w,b,)

1 2

||w||22

+

C0

l

i

i=1

subject to,

i  1 - yi(wT (xi) + b),

i  - (1 - yi(wT (xi) + b)), -1   < 0. (12)

Since  < 0 in above optimization problem (12), so its second constraint

i  - (1 - yi(wT (xi) + b)) is equivalent to

yi(wT (xi)

+

b)



1

+

i 

.

Similarly, the first constraint of problem (12)

i  1 - yi(wT (xi) + b) is equivalent to

yi(wT (xi) + b)  1 - i.

(13)

L(w, b, i, i, i)

=

1 2

||w||22

+C

l

i

i=1

l
- i(yi(wT (xi) + b) - 1 + i)

i=1

-

l

i(yi(wT

(xi)

+

b)

-

1

-

i 

).

(16)

i=1

We list some relevant Karush-Kuhn-Tucker(KKT) conditions for the optimization problem (15) as follows

l

L w

=

w-

(i + i)yi(xi) = 0,

(17)

i=1

l

L b

=

(i + i)yi = 0,

(18)

i=1

L i

=

C

- i

+

1 

i

=

0, i

=

1, 2, .., l.

(19)

Now, optimization problem (12) can be obtained as

min
(w,b,)

1 2

||w||22

+

C0

l

i

i=1

subject to,

yi(wT (xi) + b)  1 - i,

yi(wT (xi)

+

b)



1

+

i 

,

(14)

where -1   < 0, which is different from QPP (9) used in (Huang et al., [1]). It also infers that QPP (14) is the actual minimizer of the Pin-SVM model with -1   < 0 (problem (8)).

A. Solution of QPP for Pin-SVM with negative 

Using the KKT conditions, the Wolfe's dual of the primal problem (15) can be obtained as follows

1l min (,) 2

l
(j + j)(i + i)yiyj((xi)T (xj))

i=1 j=1

l
- (i + i)
i=1

subject to,

l

(i + i)yi = 0,

i=1

C

-

i

+

1 

i

=

0,

i  0, i  0, i = 1, 2, .., l.

For unbalanced training set, the Pin-SVM optimiza- By using a positive semi-definite kernel K(xi, xj) = tion problem with -1   < 0 can also be modified (xi)T (xj), satisfying Mercer condition (Mercer,

PREPRINT

4

[7]), the above dual problem can be obtained as

1l l

min (,) 2

(j + j)(i + i)yiyjK(xi, xj)

i=1 j=1

l
- (i + i)
i=1
subject to,

l

(i + i)yi = 0,

i=1

C

-

i

+

1 

i

=

0,

i  0, i  0, i = 1, 2, .., l.

(20)

After obtaining the solution of the dual problem (20), the value of w can be obtained from the KKT condition (17) as follows

l

w = (i + i)yi(xi).

(21)

i=1

Let us now define the following set S = {i : i > 0, i > 0},
Using the complementary slackness condition, we compute the values of the b for each i  S, from

l
b = yi - wT (xi) = yi - (j + j )yj K(xj , xi) (22)
j=1
and take their average value as the final value of the bias b. For given test point x  Rn, the decision function is obtained as
f (x) = sign(wT (x) + b)
l
= sign ( (j + j)yjK(xj, x) + b). (23)
j=1

IV. A UNIFIED QPP FOR SOLVING PIN-SVM
PROBLEM

We can observe that minimizing the Pin-SVM problem with positive and negative  value in [-1, 1] results into two different QPPs. Minimizing different QPPs for negative and positive  value in Pin-SVM problem may not be handful for searching best   [-1, 1], which corresponds to the optimal accuracy. Taking motivation from this, we also propose a unified optimization problem which can obtain the solution of Pin-SVM problem without bothering about the sign of its parameter  . For a given   [-1, 1], the Pin-SVM model should minimize

min
(w,b)

1 2

||w||22

+

C0

l
(max(1 - yi(wT (xi) + b)

i=1

, - (1 - yi(wT (xi) + b)).

After introducing the slack variable i = max(1 - yi(wT (xi) + b), - (1 - yi(wT (xi) + b)), the Pin-
SVM problem becomes

min
(w,b,)

1 2

||w||22

+

C0

l

i

i=1

subject to,

i  1 - yi(wT (xi) + b),

i  - (1 - yi(wT (xi) + b)).

(24)

For the unbalanced training set, the suitable Pin-SVM problem can be given by

min
(w,b,)

1 2

||w||22

+

l

Cii

i=1

subject to,

i  1 - yi(wT (xi) + b),

i  - (1 - yi(wT (xi) + b)).

(25)

We obtain the Lagrangian function for the primal problem (25) as follow

L(w, b, i, i, i)

=

1 ||w||2 2

+

l

Cii

i=1

l

- i(yi(wT (xi) + b) - 1 + i)

i=1

l
- i( (1 - yi(wT (xi) + b)) + i). (26)
i=1

We list some relevant KKT optimality conditions for the optimization problem (25) as follows.

l

L w

=

w-

(i -  i)yi(xi) = 0,

(27)

i=1

l

L b

=

(i -  i)yi = 0,

(28)

i=1

L i

= Ci - i - i = 0,

i = 1, 2, .., l

(29)

Using the KKT optimality conditions, the Wolfe's dual of the primal problem (25) is obtained as follows

1l l

min (,) 2

(j -  j)(i -  i)yiyjK(xi, xj)

i=1 j=1

l
- (i -  i)
i=1
subject to,

l
(i -  i)yi = 0,
i=1
Ci - i - i = 0,

i  0, i  0, i = 1, 2, .., l.

(30)

If we consider the replacement of variable  := | | in dual problem (30) and define a signum function

PREPRINT

5

1 if u  0,

su =

then, the dual problem -1, otherwise,

(30) can be given by

1l l

min (,) 2

(j - s j )(i - s i)yiyj K(xi, xj )

i=1 j=1

l
- (i - s i)
i=1
subject to,

l
(i - s i)yi = 0,

i=1

Ci

- i

-

i | |

=

0,

i  0, i  0, i = 1, 2, .., l.

(31)

It is notable that for 0    1, the proposed dual problem (31) is equivalent to dual problem (6) of PinSVM model. For -1   < 0 , the proposed dual problem (31) can be found to be equivalent to the dual problem (20) of Pin-SVM model for -1   < 0. This is because  = s | |.
After obtaining the solution of the dual problem (31), the value of w can be given by

l

w = (i - s i)yi(xi).

(32)

i=1

For finding the value of b, we consider each index i such that i > 0 and i > 0, and compute the value of b using the complementary slackness condition as
follow

l
b = yi-wT (xi) = yi- (j -s j )yj K(xj , xi). (33)
j=1

We consider the final value of b by taking the average
over all possible values of b. For given test point x  Rn, the decision function is obtained as

f (x) = sign(wT (x) + b)
l
= sign ( (j - s j)yjK(xj, x) + b). (34)
j=1

The proposed unified QPP (31) should be solved for minimizing the pinball loss function in SVM for -1    1. Some properties of Pin-SVM models like noise-insensitivity and non-sparsity have been only induced by the use of pinball loss function in the SVM model. Therefore, these properties do not vary in the proposed Unified Pin-SVM model. For clarity, we explicitly describe the algorithm of the proposed Unified Pin-SVM model in Algorithm 1.

V. EXPERIMENTAL RESULTS
In this section, we justify our claims made in this paper empirically. For this, we perform numerical experiments with some commonly real-world benchmark datasets. Table I shows the description of the used

Algorithm 1 Unified Pin-SVM
Input:- Training set T = {(xi, yi) : xi  Rn, yi  {-1, 1}, i = 1.2, ..., l}, test data x  Rn, and parameter  . Output:- Predicted label for test data x.

(i) Select a penalty parameter C0 > 0 and kernel parameter q, if required . These parameter are

commonly selected through validation.

(ii) For i = 1, 2, .., l, compute Ci using (4).

(iii) For the linear kernel compute k(xi, xj) =

xTi xj. For Gaussian kernel compute k(xi, xj) =

exp(

-||xi -xj 2q2

||2

).

(iv) Obtain the solution vectors ,  by solving the

proposed QPP (31). (v) Also obtain the value of bias b using (33). (vi) Predict the label of test point x using (34).

TABLE I: Dataset Description

Dataset Monk 1 Monk 2 Monk 3 Spect Fertility D. Echocardiogram Plrx Sonar Heart Statlog Haberman Votes Ecoil Ionosphere Bupa Liver Pima Indian Breast Cancer Australian Diabetes Spambase

Size 556× 7 601 ×7 554 × 7 267 × 22 100 × 10 131 × 10 182 × 13 208 × 61 270 × 14 306 × 4 435 × 17 327 × 8 351 × 34 345 × 7 768 × 9 569 × 31 690 × 15 768 × 9 4601 × 57

Training points 124 169 122 80 50 80 100 100 150 150 200 200 200 250 300 400 400 500 4000

datasets in our experiments. The first four datasets in

Table I contain the training and testing set provided.

For other datasets, we have divided the training and

testing set in Table I. We have normalized the training

and testing set in [-1, 1].

Now, we describe our experimental setup. We

have performed all experiments in MATLAB 2018

(in.mathworks.com) environment on a Dell Xeon

processor with 16 GB of RAM and Windows 10

operating system. We have solved the dual QPP

(6) of the Pin-SVM model and the proposed dual

QPP (31) of Unified Pin-SVM model with 'quad-

prog' function available in MATLAB. We have

used linear kernel and RBF kernel of the form

exp(

-||x-y||2 2q2

)

in

these

QPPs

of

Pin-SVM

models.

The MATLAB codes of the proposed Unified Pin-SVM

model and existing Pin-SVM models are available at

https://github.com/ltpritamanand/UnifiedPinSVM/

tree/mycode/Unfied-Pin-SVM-master.

Before reporting final numerical results, we have

PREPRINT

6

obtained the best possible choices of parameter C0 and RBF kernel parameter q of Pin-SVM models. For this, we have set  = 0 in Pin-SVM model and searched best possible values of (C, q) from the set {2-7, 2-6, ......, 26, 27} × {2-7, 2-6, ......, 26, 27}. After tunning the value of these parameters, we have obtained the accuracy of the Pin-SVM model and proposed Unified Pin-SVM model for different values of  on different datasets listed in Table I.
Figure 1 shows the plot of accuracy on several datasets obtained by the existing Pin-SVM model and proposed Unified Pin-SVM model against different  values from the set {-1, -0.99, ...., ..0.99, 1} with linear kernel. In these plots, the red-line represents the accuracy obtained by Pin-SVM and the black line represents the accuracy obtained by the proposed Unified Pin-SVM model. It should be noted that at  = 0, the Pin-SVM and proposed Unified Pin-SVM model reduces to the C-SVM model. We can obtain the following observations from plots in Figure 1.
1) In each plot, we can observe that the black line hides the red line on the right side of the Y axis. It confirms that for   0 , the Pin-SVM model and proposed Unified SVM model are equivalent.
2) In each plot, the red line differs from the black line in left side of the Y -axis. It empirically confirms that the Pin-SVM model (9) for -1   < 0 is different from the Unified Pin-SVM model for -1   < 0 .
3) Further, we can observe that the black line appears above the red line on the left side of Y -axis in most of the cases. It means that for -1   < 0, the proposed Unified Pin-SVM can obtain better accuracy than the existing Pin-SVM model (9). It is because of the fact that Unified Pin-SVM minimizes the pinball loss function for -1   < 0 in true spirit.
We have also plotted the accuracy obtained by the proposed Unified Pin-SVM model and existing PinSVM model against different values of parameter C and  in Figure 3 for few datasets. For this, we have varied  and C in the range {-1, -0.9, ..., 0.9, 1} and {2-7, 2-6, ..., 26, 27} respectively. Figure 3 confirms that irrespective of choice of parameters, the proposed Unified Pin-SVM model outperforms the existing PinSVM model for -1   < 0.
Table II lists the optimal performance of the existing C- SVM model, Pin-SVM model and Unified Pin-SVM model along with their training time and tunned parameters value. It can be observed that in the case of several datasets like Spect, Haberman, Echo, Australian, Diabetes, Sonar and Spambase, the use of proposed Unified Pin-SVM over existing Pin-SVM and C-SVM model can result in significant improvement

TABLE II: Pin-SVM models with linear kernel

Dataset Monk1 C0 = 0.0625
Monk2 C0 = 0.0078
Monk3 C0 = 0.0078
Spect C0 = 0.0156
Haberman C0 = 0.0078
Heart Statlog C0 = 0.0625
Ionosphere C0 = 2
Pima C0 = 0.0156
Breast C. C0 = 0.0078
Echo C0 = 0.0078
Australian C0 = 0.0313
Bupa Liver C0 = 0.0156
Votes C0 = 0.0156
Diabetes C0 = 0.0078
Fertility C0 = 0.0078
Sonar C0 = 0.0313
Ecoil C0 = 2
Parlx C0 = 0.0078
Spambase C0 = 0.0078

SVM models Unified Pin-SVM
Pin-SVM C-SVM Unified Pin-SVM Pin-SVM SVM Unified Pin-SVM Pin-SVM C-SVM Unified Pin-SVM Pin-SVM C-SVM Unified Pin-SVM Pin-SVM C-SVM Unified Pin-SVM Pin-SVM C-SVM Unified Pin-SVM Pin-SVM C-SVM Unified Pin-SVM Pin-SVM C-SVM Unified Pin-SVM Pin-SVM C-SVM Unified Pin-SVM Pin-SVM C-SVM Unified Pin-SVM Pin-SVM C-SVM Unified Pin-SVM Pin-SVM C-SVM Unified Pin-SVM Pin-SVM C-SVM Unified Pin-SVM Pin-SVM C-SVM Unified Pin-SVM Pin-SVM C-SVM Unified Pin-SVM Pin-SVM C-SVM Unified Pin-SVM Pin-SVM C-SVM Unified Pin-SVM Pin-SVM C-SVM Unified Pin-SVM Pin-SVM C-SVM

Accuracy 65.28 65.28 65.28 67.13 67.13 67.13 83.10 83.10 81.02 93.58 91.98 91.98 76.28 73.08 73.08 86.67 86.67 86.67 94.04 94.04 94.04 67.31 68.80 67.09 97.63 97.63 85.80 90.20 74.51 74.51 87.24 84.48 84.48 63.16 63.16 63.16 93.62 94.47 85.11 75.75 67.91 67.91 94.00 94.00 94.00 81.48 77.78 75.93 96.85 96.85 96.85 67.07 67.07 67.07 68.39 59.15 59.15

Time(s) 0.17 0.16 0.15 0.29 0.29 0.22 0.17 0.17 0.15 0.07 0.08 0.05 0.19 0.11 0.10 0.09 0.09 0.09 0.16 0.16 0.16 0.89 9.68 0.51 0.95 1.10 0.54 0.04 0.03 0.03 1.05 0.64 0.63 0.19 0.20 0.20 0.29 0.32 0.20 1.72 0.89 0.87 0.19 0.01 0.01 0.07 0.86 0.05 0.15 0.15 0.15 0.75 0.04 0.04
265.35 68.12 68.58

 0.00 0.00
-0.60 -0.99
0.16 0.16
-0.85 -0.99
-0.61 0.00
0.00 0.00
0.00 0.00
-0.99 -1.00
-0.25 0.11
-0.51 0.00
-0.30 0.00
0.00 0.00
-0.08 -0.99
-0.59 0.00
-1.00 0.00
-0.63 -1.00
0.00 0.00
-1.00 -1.00
-0.95
0

of accuracy. It is because of the fact that unlike the existing Pin-SVM model, the proposed Unified PinSVM model also minimizes the pinball loss function for -1   < 0 in true spirit.
We repeat the similar numerical experiments with the existing C-SVM model, Pin-SVM model and proposed Unified Pin-SVM model for RBF kernel also. The numerical results are listed in Table III. Figure 2 shows the plot of accuracy on several datasets obtained by the existing Pin-SVM model and proposed Unified Pin-SVM model against different  values from the set {-1, -0.9, ...., ..0.9, 1} with RBF kernel. These plots and numerical results are consistent with the

PREPRINT

7

(a) Monk 1

(b) Monk 3

(c) Spect

(d) Echo

(e) Parlx

(f) Sonar

(g) Statlog

(h) Ecoil

(i) Bupa liver

(j) Pima

(k) Breast Cancer

(l) Australian

(m) Diabetes

(n) Spambase

(o) Fertility

Fig. 1: Comparison of existing Pin-SVM model and proposed Unified Pin-SVM model with linear kernel.

PREPRINT

8

(a) Monk 1

(b) Monk 2

(c) Monk 3

(d) Spect

(e) Bupa liver

(f) Pima

(g) German

(h) Australian

(i) Diabetes

Fig. 2: Comparison of existing Pin-SVM model and proposed Unified Pin-SVM model with RBF kernel.

TABLE III: Pin-SVM models with RBF kernel

Dataset
Monk1 p=1 C0 = 16 Monk2 p = 0.5 C0 = 1 Monk3 p = 2, C0 = 2 Spect p = 0.0078 C0 = 0.5 Pima p = 0.5 C0 = 0.0625 German p=1 C0 = 2 Australian C0 = 2 C0 = 0.0078 Bupa Liver p = 0.25 C0 = 0.1250 Diabetes p = 0.5 C0 = 0.0313

SVM models Unified Pin-SVM
Pin-SVM C-SVM Unified Pin-SVM Pin-SVM
SVM Unified Pin-SVM
Pin-SVM C-SVM Unified Pin-SVM Pin-SVM C-SVM Unified Pin-SVM Pin-SVM C-SVM Unified Pin-SVM Pin-SVM C-SVM Unified Pin-SVM Pin-SVM C-SVM Unified Pin-SVM Pin-SVM C-SVM Unified Pin-SVM Pin-SVM C-SVM

Acc. 84.95 84.95 83.33 86.11 85.65 85.65 91.67 91.67 91.67 93.58 93.58 93.58 76.07 76.07 75.85 68.80 68.00 68.00 87.59 82.41 82.41 65.26 65.26 64.21 79.10 79.10 78.36

Time(s) 0.22 0.19 0.17 0.28 0.25 0.25 0.17 0.17 0.18 0.06 0.05 0.05 0.65 0.63 0.53 0.95 1.10 0.54 0.95 0.66 0.66 0.34 0.32 0.24 1.80 1.81 0.91

 0.12 0.12
-0.32
0 0 0 -0.59 0 0.45 0.45 -0.14 0 -0.86 0.00 -0.74 0.84 0.01 0.01 -

observations which have been made in the linear kernel case.
VI. CONCLUSIONS
This paper proposes a significant improvement over the Pin-SVM model. For this, it re-look the pinball loss function for -1   < 0 and its corresponding optimization problem used in the Pin-SVM model. It finds that the optimization problem used in (Huang et al, [1]) fails to minimize the pinball loss function for -1   < 0 in its true sense. Thereafter, it develops the right optimization problem which can minimize the pinball loss function for -1   < 0 in its true sense.
It makes us realize that the Pin-SVM model requires to solve different QPP for its positive and negative  values in [-1, 1]. Taking motivation from this, we further propose a Unified Pin-SVM QPP which can be used to solve the Pin-SVM model without bothering the sign of its parameter  in [-1, 1]. The proposed Unified Pin-SVM model can obtain a significant improvement in accuracy over the Pin-SVM model, as it can also minimize the pinball loss function with

PREPRINT

9

(a) Monk 1

(b) Statlog

Fig. 3: Plot of accuracy obtained by Pin-SVM models for different values of its parameters  and Co.

-1   < 0 in true sense. We have performed extensive numerical experiments with nineteen realworld datasets and shown empirically that the proposed Unified Pin-SVM model can always obtain an improvement over the existing Pin-SVM model.
ACKNOWLEDGMENTS
We are extremely grateful to the anonymous reviewers and Editor for their valuable comments that helped us to enormously improve the quality of the paper.
REFERENCES
[1] Xiaolin Huang, Lei Shi, and Johan AK Suykens. Solution path for pin-svm classifiers with positive and negative  values. IEEE transactions on neural networks and learning systems, 28(7):1584­1593, 2017.
[2] Xiaolin Huang, Lei Shi, and Johan AK Suykens. Support vector machine classifier with pinball loss. IEEE transactions on pattern analysis and machine intelligence, 36(5):984­997, 2014.
[3] Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning, 20(3):273­297, 1995.
[4] Vladimir Vapnik. The nature of statistical learning theory. Springer science & business media, 2013.
[5] Steve Gunn. Support vector machines for classification and regression. ISIS technical report, 1998.
[6] Roger Koenker and Gilbert Bassett Jr. Regression quantiles. Econometrica: journal of the Econometric Society, pages 33­ 50, 1978.
[7] James Mercer. Xvi. functions of positive and negative type, and their connection the theory of integral equations. Philosophical transactions of the royal society of London. Series A, containing papers of a mathematical or physical character, 209(441458):415­446, 1909.

