
# Personalizing Pre-trained Models

[arXiv](https://arxiv.org/abs/2106.01499), [PDF](https://arxiv.org/pdf/2106.01499.pdf)

## Authors

- Mina Khan
- P Srivatsa
- Advait Rane
- Shriram Chenniappa
- Asadali Hazariwala
- Pattie Maes

## Abstract

Self-supervised or weakly supervised models trained on large-scale datasets have shown sample-efficient transfer to diverse datasets in few-shot settings. We consider how upstream pretrained models can be leveraged for downstream few-shot, multilabel, and continual learning tasks. Our model CLIPPER (CLIP PERsonalized) uses image representations from CLIP, a large-scale image representation learning model trained using weak natural language supervision. We developed a technique, called Multi-label Weight Imprinting (MWI), for multi-label, continual, and few-shot learning, and CLIPPER uses MWI with image representations from CLIP. We evaluated CLIPPER on 10 single-label and 5 multi-label datasets. Our model shows robust and competitive performance, and we set new benchmarks for few-shot, multi-label, and continual learning. Our lightweight technique is also compute-efficient and enables privacy-preserving applications as the data is not sent to the upstream model for fine-tuning.

## Comments



## Source Code

Official Code



Community Code

- [https://paperswithcode.com/paper/personalizing-pre-trained-models](https://paperswithcode.com/paper/personalizing-pre-trained-models)

## Bibtex

```tex
@misc{khan2021personalizing,
      title={Personalizing Pre-trained Models}, 
      author={Mina Khan and P Srivatsa and Advait Rane and Shriram Chenniappa and Asadali Hazariwala and Pattie Maes},
      year={2021},
      eprint={2106.01499},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```

## Notes

Type your reading notes here...

