Gauss-Seidel Method with Oblique Direction
Fang Wang, Weiguo Li, Wendi Bao, Zhonglu Lv
China University of Petroleum Qingdao, China
liwg@upc.edu.cn
June 2021

arXiv:2106.00594v1 [math.NA] 1 Jun 2021

Abstract
In this paper, a Gauss-Seidel method with oblique direction (GSO) is proposed for finding the least-squares solution to a system of linear equations, where the coefficient matrix may be full rank or rank deficient and the system is overdetermined or underdetermined. Through this method, the number of iteration steps and running time can be reduced to a greater extent to find the least-squares solution, especially when the columns of matrix A are close to linear correlation. It is theoretically proved that GSO method converges to the least-squares solution. At the same time, a randomized version­randomized Gauss-Seidel method with oblique direction (RGSO) is established, and its convergence is proved. Theoretical proof and numerical results show that the GSO method and the RGSO method are more efficient than the coordinate descent (CD) method and the randomized coordinate descent (RCD) method.
Key words: linear least-squares problem, oblique direction, coordinate descent method, randomization, convergence property.

1 Introduction
Consider a linear least-squares problem

ar g min Ax - b 2,

(1)

x Rn

where b  Rm is a real m dimensional vector, and the columns of coefficient matrix A  Rm×n are non-
zero, which doesn't lose the generality of matrix A. Here and in the sequel, · indicates the Euclidean norm of a vector. When A is full column rank, (1) has a unique solution x = A b = (AT A)-1AT b, where A and AT are the Moore-Penrose pseudoinverse [3] and the transpose of A, respectively. One of the iteration methods that can be used to solve (1) economically and effectively is the coordinate descent (CD) method [10], which is also obtained by applying the classical Gauss-Seidel iteration method to the following normal equation (see [22])

AT Ax = AT b.

(2)

In solving (1), the CD method has a long history of development, and is widely used in various fields, such as machine learning [7], biological feature selection [6], tomography [5, 29], and so on. Inspired by the randomized coordinate descent (RCD) method proposed by Leventhal and Lewis [10] and its linear convergence rate analyzed theoretically [12], a lot of related work such as the

1

randomized block versions [11, 13, 21] and greedy randomized versions of the CD method [1, 30] have been developed and studied. For more information about a variety of randomized versions of the coordinate descent method, see [17, 23, 28] and the references therein. These methods mentioned above are based on the CD method, and can be extended to Kaczmarz-type methods. The recent work of Kaczmarz-type method can be referred to [26, 25, 15, 16, 2, 24, 20]. Inspired by the above work, We propose a new descent direction based on the construction idea of the CD method, which is formed by the weighting of two coordinate vectors. Based on this, we propose a Gauss-Seidel method with oblique direction (GSO) and construct the randomized version­randomized Gauss-Seidel method with oblique direction (RGSO), and analyze the convergence properties of the two methods.
Regarding our proposed methods­the GSO method and the RGSO method, we emphasize the efficiency when the columns of matrix A are close to linear correlation. In [14], it is mentioned that when the rows of matrix A are close to linear correlation, the convergence speed of the K method and the randomized Kaczmarz method [27] decrease significantly. Inspired by the above phenomena, we experimented the convergence performance of the CD method and the RCD method when the columns of matrix A are close to linear correlation and it is found through experiments that the theoretical convergence speed and experimental convergence speed of the CD method and the the RCD method will be greatly reduced. The exponential convergence in expectation of the RCD method is as follows:

Ek(x (k+1)) 

1 1 - 2F (A)

(x (k)),

(3)

where (x) = F (x) - minF , F (x) =

Ax - b

2.

Here

and

in

the

sequel,

||A||2

=

max ||Ax||,
x =1

||A||F ,

F (A) = ||A||F · ||A||2 are used to denote Euclidean norm, Frobenius norm and the scaled condition

number of the matrix A, respectively. The subgraph (a) in Figure 1 shows that when the column

of matrix A is closer to the linear correlation, 2F (A) will become larger, which further reduce the

convergence rate of the RCD method. The subgraph (b) in Figure 1 illustrates the sensitivity of

the CD method and the RCD method to linear correlation column of A. This further illustrates the

necessity of solving this type of problem, and the GSO method and the RGSO method we proposed

can be used effectively to solve that one. For the initial data setting, explanation of the experiment

in Figure 1 and the experiment on this type of matrix, please refer to Section 4 in this paper.

In this paper, · stands for the scalar product, and we indicate by ei the column vector with 1

at the ith position and 0 elsewhere.

In addition, for a given matrix

G

= (gi j)  Rm×n,

g

T i

,

Gj

and

min(G), are used to denote its ith row, jth column and the smallest nonzero singular value of G

respectively. Given a symmetric positive semidefinite matrix G  Rn×n, for any vector x  Rn we

define the corresponding seminorm as ||x||G = x T G x. Let Ek denote the expected value conditonal on the first k iterations, that is,

Ek[·] = E[·| j0, j1, ..., jk-1],

where js(s = 0, 1, ..., k - 1) is the column chosen at the sth iteration. The organization of this paper is as follows. In Section 2, we introduce the CD method and
its construction idea. In Section 3, we propose the GSO method naturally and get its randomized version­RGSO method, and prove the convergence of the two methods. In Section 4, some numerical examples are provided to illustrate the effectiveness of our new methods. Finally, some brief concluding remarks are described in Section 5.

2

(a)

(b)

Figure 1: Matrix A is generated by the r and function in the interval [c, 1]. (a): 2F (A) of matrix A changes with c. (b): When the system is consistent, the number of iterations for the CD method
and the RCD method to converge with the change of c, where the maximum number of iterations is
limited to 800,000.

2 Coordinate Descent Method

Consider a linear system

A~x = b,

(4)

where the coefficient matrix A~  Rn×n is a positive-definite matrix, and b  Rn is a real m dimensional vector. x~ = A~-1 b is the unique solution of (4). In this case, solving (4) is equivalent to the following
strict convex quadratic minimization problem

f (x) = 1 x T A~x - bT x. 2

From [10], the next iteration point x(k+1) is the solution to min f (x(k) + t d), i.e.
t R

x (k+1)

=

x (k)

+

(b

- A~x (k))T d d T A~d

d,

(5)

where d is a nonzero direction, and x(k) is a current iteration point. It is easily proved that

f (x (k+1)) - f (x~) = 1 2

x (k+1) - x~

2 A~

=

1 2

x (k) - x~

2 A~

-

(( b

- A~x (k))T 2d T A~d

d )2

.

(6)

One natural choice of a set of easily computable search directions is to choose d by successively cycling through the set of canonical unit vectors {e1, ..., en}, where ei  Rn, i = 1, · · · , n. When A  Rm×n is full column rank, we can apply (2) to (5) to get:

x (k+1)

=

x (k)

+

r(k), Ai ||Ai ||2

ei ,

where i = mod(k, n) + 1. This is the iterative formula of CD method, also known as Gauss-Seidel
method. This method is linearly convergent but with a rate not easily expressible in terms of typical matrix quantities. See [4, 8, 19]. The CD method can only ensure that one entry of AT r is 0 in

3

each iteration, i.e. ATi r(k) = 0 (i = mod(k, n) + 1). In the next chapter, we propose a new oblique direction d for (5), which is the weight of the two coordinate vectors, and use the same idea to get a new method­ the GSO method. The GSO method can ensure that the two entries of AT r are 0 in
each iteration, thereby accelerating accelerating the convergence. Remark 1. When A~ is positive semidefinite matrix, (4) may not have a unique solution, replace
x~ with any least-squares solution, (5), (6) still hold, if d T A~d = 0. Remark 2. The Kaczmarz method can be regarded as a special case of (5) under a different
regularizing linear system

AAT y = b, x = AT y,

(7)

when d is selected cyclically through the set of canonical unit vectors {e1, ..., em}, where ei  Rm, i = 1, 2, · · · , m.

3 Gauss-Seidel Method with Oblique Direction and its Randomized Version

3.1 Gauss-Seidel Method with Oblique Direction

We

propose

a

similar

d,

that

is

d

=

eik+1

-

e , Aik+1 ,Aik 
||Aik ||2 ik

where

ei



Rn,

i

=

1, 2, · · ·

, n.

Using

(2)

and

(5), we get

x (k+1)

=

x (k)

+

(AT

b

-

AT Ax (k)) A(eik+1 -

T (eik+1 -
Aik+1 ,Aik  ||Aik ||2

Aik+1 ,Aik ||Aik ||2
2
eik )



eik

)

(eik+1

-

Aik+1 , Aik ||Aik ||2



eik

)

=

x (k)

+

AT
ik+1

r

(k)

||Aik+1

- ||2

A r Aik+1 ,Aik  T

||Aik ||2

ik

- Aik+1 ,Aik 2
||Aik ||2

(k)

(eik+1

-

Aik+1 , Aik ||Aik ||2



eik

).

(8)

Now we prove that ATik r(k) = 0.

ATik r(k) = Aik , b - Ax (k)

=

Aik ,

r (k-1) 

-

Aik ,

r (k-1) 

+

Aik , Aik-1 ||Aik-1 ||2



AT
ik-1

r (k-1)

=

Aik , Aik-1 ||Aik-1 ||2



AT
ik-1

r

(k-1)

,

k = 2, 3, ...

We only need to guarantee ATi1 r(1) = 0, so we need to take the simplest coordinate descent projection as the first step. (8) becomes

x (k+1)

=

x (k)

+

AT
ik+1

r

(k)

||Aik+1 ||2

-

Aik+1 ,Aik 2 ||Aik ||2

(eik+1

-

Aik+1 , Aik ||Aik ||2



eik

).

The algorithm is described in Algorithm 1. Without losing generality, we assume that all columns of A are not zero vectors.

4

Algorithm 1 Gauss-Seidel method with Oblique Projection (GSO)

Require: A  Rm×n, b  Rm, x(0)  Rn, K, > 0

1: For i = 1 : n, N (i) = Ai 2

2:

Compute r(0) = b - Ax (0), 0 =

, A1 , r (0) 
N (1)

x (1) = x (0) + 0e1, r(1) = r(0) - 0A1, and set ik+1 = 1

3: for k = 1, 2, · · · , K - 1 do

4: Set ik = ik+1 and choose a new ik+1: ik+1 = mod(k, n) + 1

5:

Compute Gik = Aik , Aik+1 

and

gik

=

N (ik+1)

-

G Gik
N (ik) ik

6: if gik > then

7:

Compute

k

=

Aik+1 ,r(k) gik

and

k

=

-

G(ik ) N (ik )

k

8:

Compute x (k+1) = x (k) + k eik+1 + k eik , and r(k+1) = r(k) - kAik+1 - kAik

9: end if

10: end for 11: Output x (K)

It's easy to get

AT
ik-1

r

(k)

=

AT
ik-1

(

r

(k-1)

-

k-1Aik

-

k-1Aik-1 )

=

AT
ik-1

(

r

(k-1)

-

Aik

, r(k-1) gik-1



Aik

+

Aik-1

, Aik Aik , ||Aik-1 ||2 g

r
ik

(k-1)



Aik-1

)

= 0. k = 2, 3, · · ·

The last equality holds due to ATik r(k) = 0, k = 1, 2, · · · . Before giving the proof of the convergence of the GSO method, we redefine the iteration point. For x(0)  Rn as initial approximation, we define
x (0,0), x (0,1), ..., x (0,n)  Rn by


         

x (0,0)

=

x (0)

+

e , AT1 (b-Ax(0))

||A1 ||2

1

x = x + (e - e ), (0,1)

(0,0)

AT2 (b-Ax (0,0))

||A2

||2

-

A2 ,A1 2 ||A1 ||2

2

A2 ,A1  ||A1||2 1

  


x = x + (e - e ), (0,2)

(0,1)

AT3 (b-Ax (0,1))

||A3

||2

-

A3 ,A2 2 ||A2 ||2

3

A3 ,A2  ||A2||2 2

(9)

 ···························

       

x = x + (e - e ), (0,n-1)

(0,n-2)

ATn (b-Ax (0,n-2))

||An

||2

-

An ,An-1 2 ||An-1 ||2

n

An ,An-1  ||An-1||2 n-1

   


x = x + (e - e ). (0,n)

(0,n-1)

AT1 (b-Ax (0,n-1))

||A1

||2

-

A1 ,An 2 ||An ||2

1

A1 ,An  ||An||2 n

For convenience, denote An+1 = A1, bn+1 = b1. When the iteration point x(p,n) p  0 is given, the iteration points are obtained continuously by the following formula

 f or i = 1 : n

 x = x + (e - e ), (p+1,i)

(p+1,i-1)

ATi+1(b-Ax (p+1,i-1))

||Ai+1

||2

-

Ai+1 ||Ai

,Ai ||2

2

i+1

Ai+1,Ai  ||Ai ||2 i

 end

(10)

where x (p+1,0) = x (p,n). Then, we can easily obtain that x (k+1) = x (p,i), and ATik r(k) = ATi+1 r(p,i) = 0, if k = p · n + i, 0  i < n.
The convergence of the GSO is provided as follows.

5

Theorem 1. Consider (1), where the coefficient A  Rm×n, b  Rm is a given vector, and x~ is any least-
squares solution of (1). Let x(0)  Rn be an arbitrary initial approximation, then the sequence {Ax(k)} k=1 generated by the GSO algorithm is convergent , and satisfy the following equation:

lim x (k) - x~ AT A = 0.
k

(11)

Proof. According to (9)-(10) we obtain the sequence of approximations (from top to bottom and left to right, and by also using the notational convention x(p+1,0) = x(p,n)).

 x (0), x (0,0)

   

x (0,1), x (0,2), · · · , x (0,n) = x (1,0)

 

x (1,1), x (1,2), · · · , x (1,n) = x (2,0)

···························



  

x (p,1), x (p,2), · · · , x (p,n) = x (p+1,0)



 ···························

Apply (2) to (6),we get

||x (p,i+1)

-

x~||2AT A

=

||x (p,i)

-

x~||2AT A

-

((AT

b

- AT Ax (p,i))T d)2 d T AT Ad

,

(12)

where

d

=

ei+2

-

e , Ai+2,Ai+1
||Ai+1||2 i+1

ei



Rn,

i

=

1, · · ·

, n.

Obviously,

the

sequence

{

x(p,i) - x

AT A} p=,0n,-i=10,

i.e. { x(k+1) - x AT A} k=1 is a monotonically decreasing sequence with lower bounds. There exists a

  0 such that

lim
p

x (p,i) - x~

AT A =   0,

 i = 0, 1, · · · , n - 1.

(13)

Thus,

take

the

limit

of

p

on

both

sides

of

(12),

and

because

i

was

arbitrary

we

apply

AT
ik+1

r

(p,i)

=

0,

and get

lim
p

ATi+2

r

(p,i)

=

0,

 i = 0, 1, · · · , n - 1.

(14)

The residuals satisfy

r (p,i)

=

r (p,i)

-

Ai+2, r(p,i)

||Ai+2||2

-

Ai+2 ,Ai+1 2 ||Ai+1 ||2

Ai+2

-

Ai+2, Ai+1 ||Ai+1||2

Ai+1

.

Taking the limit of p on both sides of the above equation, we get

lim r(p,i+1) = lim r(p,i),  i = 0, 1, · · · , n - 1.

p

p

Using the above equation and (14), we can easily deduce that lim AT r(p,i) = 0,  i = 0, 1, · · · , n - 1.
p

(15)

Because the sequence { x (p,i) - x~ AT A} p=,0n,-i=10 is bounded, we obtain

x (p,i) AT A  x~ AT A + x (p,i) - x~ AT A  x~ AT A + x (0,1) - x~ AT A, p  0.

(16)

According to (16) we get that the sequence {Ax(p,0)} p=0 is bounded, thus there exists a convergent subsequence {Ax(pj,0)} j=1, let's denote it as

lim Ax (pj,0) = ^b.
j

(17)

6

From (9)-(10), we get

x (pj ,1)

=

x (pj ,0)

-

AT2 (b - Ax (pj,0))

||A2||2

-

A2 ,A1 2 ||A1 ||2

e2

-

A2, A1 ||A1||2

e1

,  j > 0.

By multiplying the both sides of the above equation left by matrix A and using (14), we can get that

lim Ax (pj,1) = ^b.
j

With the same way we obtain

lim Ax(pj,i) = ^b,  i = 0, 1, · · · , n - 1.
j

Then, from (15) we get for any i = 1, · · · , n - 1,

lim AT r(pj,i) = AT (b - ^b) = 0.
j

From (13) and the above equation, we get

lim
j

x (pj,i) - x~

AT A =  = 0,

 i = 0, 1, · · · , n - 1

Hence, then (11) holds.

lim
p

x (p,i) - x~

AT A = 0,

 i = 0, 1, · · · , n - 1

Remark 3. When gik = 0, Aik+1 is parallel to Aik , i.e.  > 0, s.t. Aik = Aik+1 . According to the above derivation, the GSO method is used to solve (1.2) which is consistent, so the following

equation holds:

ATik b = ATik+1 b,

which

means

for

(2)

the

ik th

equation:

Aik , Ax 

=

ATik b,

and

the

ik+1th

equation:

Aik+1 , Ax 

=

AT
ik+1

b

are coincident, and we can skip this step without affecting the final calculation to obtain the least-

squares solution. When gik is too small, it is easy to produce large errors in the process of numerical

operation, and we can regard it as the same situation as gik = 0 and skip this step.

Remark 4.

By the GSO

method,

we have:

||x (k+1) - x~||2AT A

=

||x (k) - x~||2AT A -

, (ATik+1 r k )2
gik

where

gik =

Aik+1

- . 2

Aik+1 ,Aik 2

Aik 2

But the CD method holds:

||x (k+1)

-

x~||2AT A

=

||x (k)

-

x~||2AT A

-

. (ATik+1 r k )2
||Aik+1 ||2

So the GSO method is faster than the CD method unless Aik , Aik+1  = 0. When Aik , Aik+1  = 0, the

convergence rate of the GSO method is the same as that of the CD method. This means that when the

coefficient matrix A is a column orthogonal matrix, the GSO method degenerates to the CD method.

Remark 5. The GSO method needs 8m+5 floating-point operations per step, and the CD method

needs 4m + 1 floating-point operations per step.

Remark 6. When the matrix A is full column rank, let x be the unique least-squares solution

of

(1),

the

sequence

{x (k)} k=1

generated

by

the

GSO

method

holds:

lim
k

x(k) - x

AT A = 0, that is,

lim A(x(k) - x) 2 = 0. Therefore,

k

lim x (k) - x  2 = 0.
k

7

Example 1. Consider the following systems of linear equations

5x1 + 45x2 = 50, 9x1 + 80x2 = 89,

(18)

  x1 + 11x2 = 12,
-2x1 - 21x2 = -23,
 3x1 + 32x2 = 35

(19)

and





x1 + 9x2 =

0,

4x1 + 36x2 = 42.5,

 13x1 + 118x2 = 131,

(20)

(18) is square and consistent, (19) is overdetermined and consistent, and (20) is overdetermined and inconsistent. Vector x = (1, 1)T is the unique solution to the above (18) and (19), is the unique

least-squares solution to (20). It can be found that the column vectors of these systems are close

to linearly correlated. Numerical experiments show that they take 650259, 137317, 3053153 steps

respectively for the CD method to be applied to the above systems to reach the relative solution error

requirement

x (k) - x  x 2

2



1 2

× 10-6,

but

the

GSO

method

can

find

the

objective

solutions

to

the

above

three systems in one step.

3.2 Randomized Gauss-Seidel Method with Oblique Direction

If the columns whose residual entries are not 0 in algorithm 1 are selected uniformly and randomly, we get a randomized Gauss-seidel method with oblique direction (RGSO) and its convergence as follows.

Algorithm 2 Randomized Gauss-Seidel Method with Oblique Direction (RGSO)

Require: A  Rm×n, b  Rm, x(0)  Rn, K, > 0

1: For i = 1 : n, N (i) = Ai 2

2:

Randomly select i1, and compute r(0) = b - Ax(0), 0 =

, Ai1 ,r(0)
||Ai1 ||2

and

x (1) = x (0) + 0ei1

3:

Randomly select i2 = i1, and compute r(1) = r(0) -0Ai1 , 1 =

, and  = -  Ai2 ,r(1)

||Ai2

||2

-

Ai1 ,Ai2 2 ||Ai1 ||2

Ai1 ,Ai2 

1

||Ai1 ||2 1

4: Compute x (2) = x (1) + 1ei2 + 1ei1 ,and r(2) = r(1) - 1Ai2 - 1Ai1

5: for k = 2, 3, · · · , K - 1 do

6: Randomly select ik+1 (ik+1 = ik, ik-1)

7:

Compute Gik = Aik , Aik+1 

and

gik

=

N (ik+1)

-

G Gik
N (ik) ik

8: if gik > then

9:

Compute

k

=

Aik+1 ,r(k) gik

and

k

=

-

G(ik ) N (ik )

k

10:

Compute x (k+1) = x (k) + k eik+1 + k eik , and r(k+1) = r(k) - kAik+1 - kAik

11: end if

12: end for 13: Output x (K)

Lemma 1. Consider (1), where the coefficient A  Rm×n , b  Rm is a given vector, and x~ is any solution to (1) , then we obtain the bound on the following expected conditional on the first k (k  2) iteration

8

of the RGSO

Ek

(ATik+1 g

r (k)
ik

)2



1 n-2

2min(A)||x~ - x (k)||2AT ||A||2F - 2min(A)

A

.

Proof.

For the RGSO mthod, it is easy to get that ATik r(k) = 0

(k

=

1, 2, · · · )

and

AT
ik-1

r

(k)

=

0

(k =

2, 3, · · · ) are still valid.

Ek

(ATik+1 g

r (k)
ik

)2

=

1

n

n - 2 s=1
s=ik ,ik-1

(ATs r(k))2

||As ||2

-

As ,Aik 2 ||Aik ||2

1 
n-2

n
(ATs r(k))2
s=1,s=ik ,ik-1

n s=1

(||As ||2

-

As ,Aik 2 ||Aik ||2

)


= 

1 n-2

n



(ATs r(k))2

s=1



n
(||As ||2
s=1

-

  As,Aik 2 ) ||Aik ||2

s=ik ,ik-1

1 ||AT A(x~ - x (k))||2

= n-2

||A||2F

-

||AT Aik ||2 ||Aik ||2



n

1 -2

2min(A)||x~ - x (k)||2AT ||A||2F - 2min(A)

A

,

k = 2, 3, ...

The

first

inequality

uses

the

conclusion

of

| b1 | |a1 |

+

| b2 | |a2 |



| b1 |+| b2 | |a1 |+|a2 |

(if

|a1|

>

0,

|a2|

>

0),

and

the

second

one uses the conclusion of

AT z

2 2



2min(A)

z

22, if z  R(A).

Theorem 2. Consider (1), where the coefficient A  Rm×n , b  Rm is a given vector, and x~ is any leastsquares solution of (1). Let x(0)  Rn be an arbitrary initial approximation, and define the least-squares
residual and error by F (x) = ||Ax - b||2,

(x) = F (x) - minF,

then the RGSO method is linearly convergent in expectation to a solution in (1). For each iteration:k = 2, 3, ...,

Ek(x (k+1)) 

1 1 - (n - 2)(k2F (A) - 1)

(x (k)).

In particular, if A has full column rank, we have the equivalent property

Ek

x (k+1) - x 

2 AT A



1 1 - (n - 2)(k2F (A) - 1)

where x = A b = (AT A)-1AT b is the unique least-squares solution.

x(k) - x

2 AT

A,

Proof. It is easy to prove that

F (x) - F (x~) = ||x - x~||2AT A = (x).

9

Apply

(2)

to

(6)

with

d

=

eik+1

-

e Aik+1 ,Aik 
||Aik ||2 ik

and

ATik rk

= 0, k

= 1, 2, ..

,we

get

that

F (x (k+1)) - F (x~) = ||x (k+1) - x~||2AT A

=

||x (k)

-

x~||2AT A

-

(ATik+1 r (k))2 gik

.

Making conditional expectation on both sides, and applying Lemma 1, we get

Ek F (x (k+1)) - F (x~) = ||x (k) - x~||2AT A - Ek

(ATik+1 r (k))2 gik



||x (k)

-

x~||2AT A

-

2min(A)||x~ - x (k)||2AT A (n - 2)(||A||2F - 2min(A))

,

that is

Ek(x (k+1))  =

1

-

(n

-

2min(A) 2)(||A||2F - 2min(A))

( x (k))

1 1 - (n - 2)(k2F (A) - 1)

(x (k)).

If A has full column rank, the solution in (1) is unique and the x~ = x. Thus, we get

Ek

x (k+1) - x 

2 AT A



1 1 - (n - 2)(k2F (A) - 1)

x(k) - x

2 AT

A.

Remark 7. In particular, after unitizing the columns of matrix A, we can get from Lemma 1:

Ek

(ATik+1 g

r (k)
ik

)2

=

1

n

n - 2 s=1
s=ik ,ik-1

(ATs r(k))2

||As ||2

-

As ,Aik 2 ||Aik ||2

n
=
s=1 s=ik ,ik-1

As 2

(ATs r(k))2

A

2 F

-

2

||As ||2

-

As ,Aik 2 ||Aik ||2

n

s=1 s=ik ,ik-1

1

(ATs r(k))2

A

2 F

-2

1 - 2ik



2mi n (A)|| x~ (1 - 2ik )(

- A

x (k)||2AT

2 F

-

2)

A

,

k

=

2, 3, ...

where

ik

=

min
s=ik ,ik-1

|As

,

Aik

|.

Then

we

get

from

Theorem

1:

Ek(x (k+1)) 

1

-

(1

-

2min(A) 2ik )(||A||2F

-

2)

(x (k)).

Comparing the above equation with (3), we can get that under the condition of column unitization, the RGSO method is theoretically faster than the RCD method. Note that by Remark 3, we can avoid the occurrence of ik = 1.

10

4 Numerical Experiments

In this section, some numerical examples are provided to illustrate the effectiveness of the coordinate descent (CD) method, the Gauss-Seidel method with oblique direction (GSO), the randomized coordinate descent (RCD) method (with uniform probability) and the randomized Gauss-Seidel method with oblique direction (RGSO) for solving (1). All experiments are carried out using MATLAB (version R2019b) on a personal computer with 1.60 GHz central processing unit (Intel(R) Core(TM) i5-10210U CPU), 8.00 GB memory, and Windows operating system (64 bit Windows 10).
Obtained from [18], the least-squares solution set for (1) is

LSS(A; b) = S(A, bA) = {PN(A)(x (0)) + x LS, x (0)  Rn},

where LSS(A; b) is the set of all least solutions to (1), and x LS is the unique least-squares solution of

minimal Euclidean norm. For the consistent case b  R(A), LSS(A; b) will be denoted by S(A; b). If

b = bA + bA, with

bA = PR(A)(b), bA = PN(AT )(b),

where Ps denotes the orthogonal projection onto the vector subspace S of some Rq. From Theorem 1

and Theorem 2, we can know that the sequence

x (k) - x~

2 AT

A

generated

by

the

GSO

method

and

the

RGSO method converges to 0. Due to

x (k) - x~

2 AT

A

=

=

=

A(x (k) - x~) 2
bA + bA - r(k) - bA 2 bA - r(k) 2,

where bA can be known in the experimental hypothesis, and r(k) is calculated in the iterative process, we can propose a iteration termination rule: The methods are terminated once residual relative error

(RRE), defined by

RRE = bA - r(k) 2 b2

at

the

current

iterate

x (k),

satisfies

RRE

<

1 2

× 10-6

or

the

maximum

iteration

steps

500, 000

being

reached. If the number of iteration steps exceeds 500, 000, it is denoted as "-". IT and CPU are the

medians of the required iterations steps and the elapsed CPU times with respect to 50 times repeated

runs of the corresponding method. To give an intuitive demonstration of the advantage, we define

the speed-up as follows:

speed-up1 =

CPU of CD , speed-up2 =

CPU of RGS .

CPU of GSO

CPU of RGSO

In our implementations, all iterations are started from the initial guess x0 = zer os(n, 1). First, set a least-squares solution x~, which is generated by using the MATLAB function rand. Then set bA = Ax~. When linear system is consistent, bA = 0, b = bA, else bA  nul l(AT ), b = bA + bA. When the column of the coefficient matrix A is full rank, the methods can converge to the only least-squares solution
x under the premise of convergence.

4.1 Experiments for Random Matrix Collection in [0, 1]
The random matrix collection in [0, 1] is randomly generated by using the MATLAB function r and, and the numerical results are reported in Tables 1-9. According to the characteristics of the matrix generated by MATLAB function r and, Table 1 to Table 3, Table 4 to Table 6, Table 7 to Table

11

9 are the experiments respectively for the overdetermined consistent linear systems, overdetermined inconsistent linear systems, and underdetermined consistent linear systems. In Table 1 to Table 6, under the premise of convergence, all methods can find the unique least-squares solution x, i.e. x = (AT A)-1AT b. In Table 7 to Table 9, all methods can find the least-squares solution under the premise of convergence, but they can't be sure to find the same least-squares solution.
From these tables, we see that the GSO method and the RGSO method are more outstanding than the CD method and the RCD method respectively in terms of both IT and CPU with significant speed-up, regardless of whether the corresponding linear system is consistent or inconsistent. We can observe that in Tables 1-6, for the overdetermined linear systems, whether it is consistent or inconsistent, CPU and IT of all methods increase with the increase of n, and the CD method is extremely sensitive to the increase of n. When n increases to 100, it stops because it exceeds the maximum number of iterations. In Tables 7-9, for the underdetermined consistent linear system, CPU and IT of all methods increase with the increase of m.

Table 1: IT and CPU of CD, RCD, GSO and RGSO for m × n matrices A with n = 50 and different m when the overdetermined linear system is consistent

m×n

CD

IT CPU

GSO

IT CPU

speed-up1

RCD

IT CPU

RGSO

IT CPU

speed-up2

1000 × 50 73004 0.1605 11110 0.0379
4.23 1733 0.0125 778 0.0070
1.78

2000 × 50 74672 0.3082 11081 0.0711
4.33 1596 0.0151 752 0.0086
1.75

3000 × 50 74335 0.5200 10915 0.1224
4.25 1505 0.0196 789 0.0145
1.36

4000 × 50 74608 0.9833 10951 0.2412
4.08 1583 0.0322 700 0.0210
1.53

5000 × 50 74520 1.3256 10934 0.3244
4.09 1522 0.0416 685 0.0267
1.56

Table 2: IT and CPU of CD, RCD, GSO and RGSO for m × n matrices A with n = 100 and different m

when the overdetermined linear system is consistent

m×n

CD

IT CPU

GSO

IT CPU

speed-up1

RCD

IT CPU

RGSO

IT CPU

speed-up2

1000 × 100 -
84180 0.2945
3909 0.0278 1657 0.0148
1.88

2000 × 100 -
81595 0.5315
3304 0.0318 1598 0.0204
1.56

3000 × 100 -
80120 0.9227
3564 0.0475 1486 0.0264
1.80

4000 × 100 -
80630 1.7860
3391 0.0719 1751 0.0546
1.32

5000 × 100 -
79131 2.6375
3187 0.0957 1432 0.0631
1.52

4.2 Experiments for Random Matrix Collection in [c, 1]
From example 1, it can be observed that when the columns of the matrix are nearly linear correlation, the GSO method can find the objective solution of the equation with less iteration steps and running time than the CD method. In order to verify this phenomenon, we construct several 3000 × 50 and 1000 × 3000 matrices A, which entries is independent identically distributed uniform
12

Table 3: IT and CPU of CD, RCD, GSO and RGSO for m × n matrices A with n = 150 and different m

when the overdetermined linear system is consistent

m×n

CD

IT CPU

GSO

IT CPU

speed-up1

RCD

IT CPU

RGSO

IT CPU

speed-up2

1000 × 150 -
276537 0.9292
6781 0.0472 2880 0.0241
1.96

2000 × 150 -
270070 1.6746
5375 0.0486 2574 0.0304
1.60

3000 × 150 -
260799 2.7657
5371 0.0660 2466 0.0415
1.59

4000 × 150 -
259227 5.9676
5288 0.1095 2352 0.0741
1.48

5000 × 150 -
259033 9.1506
5358 0.1712 2547 0.1195
1.43

Table 4: IT and CPU of CD, RCD, GSO and RGSO for m × n matrices A with n = 50 and different m

when the overdetermined linear system is inconsistent

m×n

CD

IT CPU

GSO

IT CPU

speed-up1

RCD

IT CPU

RGSO

IT CPU

speed-up2

1000 × 50 73331 0.1591 11124 0.0442
3.60 1736 0.0129 744 0.0067
1.91

2000 × 50 73895 0.3004 10955 0.0716
4.20 1786 0.0164 718 0.0087
1.88

3000 × 50 73910 0.5266 10875 0.1337
3.94 1706 0.0244 762 0.0142
1.72

4000 × 50 74810 1.0081 10984 0.2411
4.18 1599 0.0338 737 0.0223
1.52

5000 × 50 74606 1.4170 10910 0.3376
4.20 1514 0.0414 769 0.0327
1.26

Table 5: IT and CPU of CD, RCD, GSO and RGSO for m × n matrices A with n = 100 and different m

when the overdetermined linear system is inconsistent

m×n

CD

IT CPU

GSO

IT CPU

speed-up1

RCD

IT CPU

RGSO

IT CPU

speed-up2

1000 × 100 -
84415 0.2829
3973 0.0305 1676 0.0142
2.14

2000 × 100 -
84104 0.5457
3511 0.0329 1675 0.0203
1.62

3000 × 100 -
80361 0.9160
3599 0.0473 1596 0.0279
1.70

4000 × 100 -
79462 1.7187
3092 0.0615 1456 0.0427
1.44

5000 × 100 -
79572 2.5587
3221 0.0943 1563 0.0666
1.42

13

Table 6: IT and CPU of CD, RCD, GSO and RGSO for m × n matrices A with n = 150 and different m

when the overdetermined linear system is inconsistent

m×n

CD

IT CPU

GSO

IT CPU

speed-up1

RCD

IT CPU

RGSO

IT CPU

speed-up2

1000 × 150 -
288578 1.0080
6799 0.0478 2834 0.0247
1.94

2000 × 150 -
267841 1.7435
5690 0.0520 2472 0.0300
1.73

3000 × 150 -
265105 2.9230
5340 0.0690 2463 0.0467
1.48

4000 × 150 -
262289 5.8877
4860 0.0977 2475 0.0739
1.32

5000 × 150 -
258320 7.8848
4979 0.1390 2368 0.0979
1.42

Table 7: IT and CPU of CD, RCD, GSO and RGSO for m × n matrices A with n = 1000 and different

m when the underdetermined linear system is consistent

m×n

CD

IT CPU

GSO

IT CPU

speed-up1

RCD

IT CPU

RGSO

IT CPU

speed-up2

100 × 1000 3805 0.0025 1621 0.0016
1.52 4113 0.0210 1876 0.0102
2.05

200 × 1000 11193 0.0089 3544 0.0044
2.01 10926 0.0593 4152 0.0253
2.34

300 × 1000 22638 0.0215 6824 0.0111
1.93 21267 0.1151 7985 0.0497
2.31

400 × 1000 43868 0.0499 12339 0.0224
2.23 39220 0.2219 13158 0.0877
2.53

500 × 1000 82643 0.1102 24149 0.0507
2.17 70545 0.4207 24441 0.1680
2.50

Table 8: IT and CPU of CD, RCD, GSO and RGSO for m × n matrices A with n = 2000 and different

m when the underdetermined linear system is consistent

m×n

CD

IT CPU

GSO

IT CPU

speed-up1

RCD

IT CPU

RGSO

IT CPU

speed-up2

100 × 2000 3285 0.0029 1622 0.0022
1.35 3636 0.0195 1741 0.0114
1.71

200 × 2000 7790 0.0071 3324 0.0051
1.39 8113 0.0488 3580 0.0235
2.08

300 × 2000 13913 0.0160 5027 0.0095
1.68 14382 0.0828 5892 0.0393
2.11

400 × 2000 21575 0.0314 7079 0.0156
2.01 21696 0.1320 8343 0.0598
2.21

500 × 2000 32445 0.0487 9858 0.0231
2.11 31904 0.1988 11745 0.0908
2.19

14

Table 9: IT and CPU of CD, RCD, GSO and RGSO for m × n matrices A with n = 3000 and different

m when the underdetermined linear system is consistent

m×n

CD

IT CPU

GSO

IT CPU

speed-up1

RCD

IT CPU

RGSO

IT CPU

speed-up2

100 × 3000 3215 0.0029 1624 0.0025
1.19 3475 0.0193 1686 0.0107
1.80

200 × 3000 6924 0.0069 3267 0.0051
1.35 7633 0.0427 3499 0.0225
1.90

300 × 3000 11717 0.0138 4940 0.0099
1.39 12272 0.0714 5491 0.0377
1.89

400 × 3000 17393 0.0238 6679 0.0146
1.63 18248 0.1104 7537 0.0532
2.08

500 × 3000 25296 0.0384 8683 0.0214
1.79 25115 0.1587 9966 0.0724
2.19

random variables on some interval [c,1]. When the value of c is close to 1, the column vectors of matrix A are closer to linear correlation. Note that there is nothing special about this interval, and other intervals yield the same results when the interval length remains the same.
From Table 10 to Table 12, it can be seen that no matter whether the system is consistent or inconsistent, overdetermined or underdetermined, with c getting closer to 1, the CD and the RCD method have a significant increase in the number of iterations, and the speed-up1 and the speed-up2 also increase greatly. In Table 10 and Table 11, when c increases to 0.45, the number of iterations of the CD method exceeds the maximum number of iterations. In Table 12, when c increases to 0.6, the number of iterations of the CD method and RCD method exceeds the maximum number of iterations.
In this group of experiments, it can be observed that when the columns of the matrix are close to linear correlation, the GSO method and the RGSO method can find the least-squares solution more quickly than the CD method and the RCD methd.

Table 10: IT and CPU of CD, RCD, GSO and RGSO for A  R3000×50 with different c when the overdetermined linear system is consistent

c

CD

IT CPU

GSO

IT CPU

speed-up1

RCD

IT CPU

RGSO

IT CPU

speed-up2

0.15 141636 0.9638 12201 0.1575
6.12 2196 0.0278 749 0.0145
1.92

0.30 273589 1.8351 12979 0.1625
11.30 3850 0.0483 757 0.0145
3.33

0.45 -
12763 0.1583
6828 0.0851 650 0.0124
6.87

0.60 -
11814 0.1519
13978 0.1752
696 0.0132
13.22

0.75 -
10126 0.1261
36858 0.4506
572 0.0111
40.68

0.90 -
7017 0.0862
216260 2.6451
421 0.0079
336.90

5 Conclusion
A new extension of the CD method and its randomized version, called the GSO method and the RGSO method, are proposed for solving the linear least-squares problem. The GSO method is deduced to be convergent, and an estimate of the convergence rate of the RGSO method is obtained. The GSO method and the RGSO method are proved to converge faster than the CD method and
15

Table 11: IT and CPU of CD, RCD, GSO and RGSO for A  R3000×50 with different c when the overdetermined linear system is inconsistent

c

CD

IT CPU

GSO

IT CPU

speed-up1

RCD

IT CPU

RGSO

IT CPU

speed-up2

0.15 140044 0.9483 12075 0.1602
5.92 2227 0.0301 722 0.0158
1.91

0.30 270445 1.8366 12910 0.1623
11.32 3864 0.0479 713 0.0145
3.32

0.45 -
12678 0.1598
6493 0.0825 650 0.0153
5.39

0.60 -
11689 0.1519
14256 0.1783
646 0.0131
13.58

0.75 -
10118 0.1284
37734 0.4645
557 0.0121
38.52

0.90 -
7112 0.0882
209427 2.5826
474 0.0088
292.21

Table 12: IT and CPU of CD, RCD, GSO and RGSO for A  R1000×3000 with different c when the

underdetermined linear system is consistent

c

CD

IT CPU

GSO

IT CPU

speed-up1

RCD

IT CPU

RGSO

IT CPU

speed-up2

0.15 143373 0.3344 24358 0.1072
3.12 122119 0.9120 28166 0.2711
3.36

0.30 246942 0.5818 23888 0.1048
5.55 194440 1.4251 24936 0.2450
5.82

0.45 441147 1.0359 22310 0.0987
10.49 346301 2.5529 24201 0.2318
11.01

0.60 -
19485 0.0878
22318 0.2082
-

0.75 -
16795 0.0748
18433 0.1813
-

0.90 -
11509 0.0525
13717 0.1306
-

16

the RCD method, respectively. Numerical experiments show the effectiveness of the two methods, especially when the columns of coefficient matrix A are close to linear correlation.
Acknowledgments
This work was supported by the National Key Research and Development Program of China [grant number 2019YFC1408400], and the Science and Technology Support Plan for Youth Innovation of University in Shandong Province [No.YCX2021151].
References
[1] Z. Z. BAI AND W. T. WU, On greedy randomized coordinate descent methods for solving large linear least-squares problems, Numer. Linear Algebra Appl., 26 (2019), pp. 1­15.
[2] Z. Z. BAI AND W. T. WU, On partially randomized extended Kaczmarz method for solving large sparse overdetermined inconsistent linear systems, Linear Algebra Appl., 578 (2019), pp. 225­ 250.
[3] A. BEN-ISRAEL, Generalized inverses: Theory and applications, Pure Appl. Math., 139 (1974), pp. 125­147.
[4] A. BJORCK, Numerical Methods for Least Squares Problems, SIAM, Philadelphia, PA, 1996. [5] C. BOUMAN AND K. SAUER, A unified approach to statistical tomography using coordinate descent
optimization, IEEE Trans. Image Process., 5 (1996), pp. 480­492. [6] P. BREHENY AND J. HUANG, Coordinate descent algorithms for nonconvex penalized regression, with
applications to biological feature selection, Ann. Appl. Stat., 5 (2011) pp. 232­253. [7] K. W. CHANG, C. J. HSIEH AND C. J. LIN, Coordinate descent method for large-scale l2-loss linear
support vector machines, J. Mach. Learn. Res., 9 (2008) pp. 1369­1398. [8] G. GOLUB AND C. V. LOAN, Matrix Computations, Johns Hopkins University Press, 1996. [9] S. KACZMARZ, Angena¨herte auflo¨sung von systemen linearer gleichungen, Bull. Internat. A-cad.
Polon.Sci. Lettres A, 29 (1937) pp. 335­357. [10] D. LEVENTHAL AND A. LEWIS, Randomized methods for linear constraints: convergence rates and
conditioning, Math. Oper. Res., 35 (2010) pp. 641­654. [11] Z. LU AND L. XIAO, On the complexity analysis of randomized block-coordinate descent methods,
Math. Program., 152 (2015) pp. 615­642. [12] A. MA, D. NEEDELL AND A. RAMDAS, Convergence properties of the randomized extended Gauss-
Seidel and Kaczmarz methods, SIAM J. Matrix Anal. Appl., 36 (2015) pp. 1590­1604. [13] I. NECOARA, Y. NESTEROV AND F. GLINEUR, Random block coordinate descent methods for linearly
constrained optimization over networks, J. Optim. Theory Appl., 173 (2017) pp. 227­254. [14] D. NEEDELL AND R. WARD, Two-subspace projection method for coherent overdetermined systems,
J. Fourier Anal. Appl., 19 (2013) pp. 256­269.
17

[15] X. YANG, A geometric probability randomized Kaczmarz method for large scale linear systems, Appl. Numer. Math., 164 (2021) pp. 139­160.
[16] J. J. ZHANG, A new greedy Kaczmarz algorithm for the solution of very large linear systems, Appl. Math. Lett., 91 (2019) pp. 207­212.
[17] Y. NESTEROV AND S. STICH, Efficiency of the accelerated coordinate descent method on structured optimization problems, SIAM J. Optim., 27 (2017) pp. 110­123.
[18] C. POPA, T. PRECLIK, H. Ko¨STLER AND U. Ru¨DE, On kaczmarz's projection iteration as a direct solver for linear least squares problems, Linear. Algebra Appl., 436 (2012) pp. 389­404.
[19] A. QUARTERONI, R. SACCO AND F. SALERI, Numerical Mathematics, Springer New York, 2007. [20] C. G. KANG AND H. ZHOU, The extensions of convergence rates of Kaczmarz-type methods, J. Com-
put.Appl. Math., 382 (2021) 113577. [21] P. RICHTa´RIK AND M. TAKa´c, Iteration complexity of randomized block-coordinate descent methods
for minimizing a composite function, Math. Program., 144 (2014) pp. 1­38. [22] A. RUHE, Numerical aspects of gram-schmidt orthogonalization of vectors, Linear Alg. Appl., 52
(1983) pp. 591­601. [23] S. SHALEV-SHWARTZ AND A. TEWARI, Stochastic methods for 1-regularized loss minimization, J.
Mach. Learn. Res., 12 (2011) pp. 1865­1892. [24] Y. LIU AND C. Q. GU, Variant of greedy randomized Kaczmarz for ridge regression, Appl. Numer.
Math., 143 (2019) pp. 223­246. [25] K. DU AND H. GAO, A new theoretical estimate for the convergence rate of the maximal residual
Kaczmarz algorithm, Numer. Math. Theor. Meth. Appl., 12 (2019) pp. 627­639. [26] Y. J. GUAN, W. G. LI, L. L. XING AND T. T. QIAO, A note on convergence rate of randomized Kaczmarz
method, Calocolo, 57 (2020). [27] T. STROHMER AND R. VERSHYNIN, A randomized kaczmarz algorithm with exponential conver-
gence, J. Fourier Anal. Appl., 15 (2009) pp. 262­278. [28] S. WRIGHT, Coordinate descent algorithms, Math. Program., 151 (2015) pp. 3­34. [29] J. YE, K. WEBB, C. BOUMAN AND R. MILLANE, Optical diffusion tomography by iterative coordinate-
descent optimization in a bayesian framework, J. Opt. Soc. Am. A, 16 (1999) pp. 2400­2412. [30] J. H. ZHANG AND J. H. GUO, On relaxed greedy randomized coordinate descent methods for solving
large linear least-squares problems, Appl. Numer. Math., 157 (2020) pp. 372­384.
18

