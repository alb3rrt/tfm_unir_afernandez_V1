Towards Explainable Convolutional Features for Music Audio Modeling

arXiv:2106.00110v1 [cs.SD] 31 May 2021

Anna K. Yanchenko Duke University

Mohammadreza Soltani Duke University

Robert J. Ravier Duke University

Sayan Mukherjee Duke University

Vahid Tarokh Duke University

Abstract
Audio signals are often represented as spectrograms and treated as 2D images. In this light, deep convolutional architectures are widely used for music audio tasks even though these two data types have very different structures. In this work, we attempt to "open the black-box" on deep convolutional models to inform future architectures for music audio tasks, and explain the excellent performance of deep convolutions that model spectrograms as 2D images. To this end, we expand recent explainability discussions in deep learning for natural image data to music audio data through systematic experiments using the deep features learned by various convolutional architectures. We demonstrate that deep convolutional features perform well across various target tasks, whether or not they are extracted from deep architectures originally trained on that task. Additionally, deep features exhibit high similarity to hand-crafted wavelet features, whether the deep features are extracted from a trained or untrained model.
1 Introduction
Deep convolutional architectures are successfully and widely used for many audio tasks in the music domain, including generation [18], genre classification [14, 19], audio tagging [7], audio to audio alignment [16] and note prediction [27]. Audio signals are often represented as spectrograms and treated as natural images in deep modeling approaches. Mel-spectrograms, in particular, have been foundational as an audio representation in all types of classical audio modeling, from speech processing to music applications. The mel-scale is a logarithmic scale that is based on perceptual pitch differences and is designed to represent more perceptual information with fewer frequency bands than spectrograms on the linear pitch scale [25, 5]. The related mel-frequency cepstral coefficients, also based on the mel-spectrogram, have long been a preferred representation for speech recognition [22] and capture aspects of timbre for music applications [17]. In the deep learning paradigm, convolutional architectures on (mel-)spectrograms excel, especially at discriminative tasks, even though spectrograms do not have the same structure as natural images. Spectrograms are not translationally invariant, the time (x) and frequency (y) axes represent inherently different quantities, and often there is a substantial loss in information by ignoring phase information. The success of convolutions on mel-spectrograms is thus not obvious a priori and the question remains: Why do deep convolutions that model spectrograms as natural images perform so well on music audio tasks?
We hypothesize that deep convolutional architectures trained on mel-spectrograms learn features that correspond to classical signal processing and wavelet features. This may explain their high performance on discriminative tasks. In order to test this hypothesis, we investigate the following Questions: What audio features, both deep and hand-crafted, are useful for discriminative music tasks? Are the learned deep features from convolutional architectures similar to the hand-crafted
Preprint. Under review.

features? Do untrained convolutional architectures have a useful inductive bias for music audio discriminative tasks? These questions are relevant for the broader machine learning community as they provide a starting point for a more general understanding of the nature of features learned by deep CNNs for audio modeling. Our experiments may contribute to the explainability of deep features. They may also contribute towards further democratization of deep learning by suggesting possibilities for reduced-size architectures for audio modeling that utilize known feature information and could be much smaller and faster to train than the majority of current approaches, e.g. [18]. Finally, our experiments may further inform ongoing work in useful inductive biases and deep priors for audio modeling tasks, e.g. [31].
1.1 Contributions
Recent efforts in explainability of learned deep features have focused almost exclusively on natural image data, e.g. [10, 1, 24]. The main contribution of this work is expanding the explainability discussion to music audio data through systematic experiments across various datasets, architectures, tasks, layers and initializations to understand the deep features learned by convolutional architectures on discriminative music audio tasks. Specifically, we explore deep audio features both for a simplified audio dataset [8] and two real-world, full orchestral audio recording datasets. Across datasets, we find that:
1. Convolutional deep features are robust to discriminative tasks and achieve high classification accuracy and low predictive error on several distinct tasks, whether or not the deep features were extracted from architectures originally trained on that specific task. In general, deep features from deformable convolutions [6], and from the last convolutional layer of several architectures give the highest classification accuracy across tasks.
2. Deep features from earlier layers in the convolutional architecture are highly similar to interpretable hand-crafted features that are motivated by classical signal processing.
3. Earlier layers have high similarity even when extracted from untrained convolutional models, suggesting that convolutions may have a useful inductive bias for these hand-crafted features.
This work attempts to "open the black-box" on deep convolutional models for music audio modeling and can consequently inform future architecture and feature designs for music audio learning tasks.
1.2 Related Work
Explainability and interpretability in deep learning are growing areas of research, especially for natural image modeling, and are important topics for building trust in powerful black-box methods. Some recent approaches are interpretable by design [3], while numerous post-hoc explainability measures have been developed and compared for understanding deep architectures and visualizing classification decisions by convolutional models [24, 15, 23, 9, 4, 11, 1, 26]. Many of these methods are designed specifically for image tasks (for example, utilizing image segmentation methods [9]) and often visualize "important" parts of input images using saliency map techniques, e.g. [1] for a comparison of approaches. However, there is very little work that focuses on understanding deep features learned on audio data in general, and music data in particular.
Prior work for analyzing deep audio features includes [20], which used pre-trained ImageNet weights for music genre classification and used integrated gradients to analyze mel-spectrograms, finding that convolutional deep architectures were able to detect sound events. Additionally, [31] explored convolutions for denoising and sound separation for general audio signals and proposed a new type of convolution to better represent the structure in audio signals. We take a different approach than [31], and instead incorporate ideas from recent work for deep natural image feature understanding [10] to understand deep features in the context of decoding deep features (Section 3.1) and relating these deep features to meaningful and interpretable hand-crafted features (Sections 3.2, 3.3). Unlike in image classification tasks, it is challenging or impossible to visually inspect a spectrogram and determine the composer of the piece being performed, necessitating approaches other than saliency-based techniques to understand deep music audio features.
2

2 Methodology
In this section, we discuss the datasets and audio tasks for our study, the different convolutional architectures, the extracted features, and a measure of similarity to compare the hand-crafted features with the deep convolution features; full details are given in the Supplementary Materials.
2.1 Data and Tasks
We consider three different music audio datasets to explore the ability of deep convolutional features to represent both low-level musical concepts (i.e. note pitch, instrument, timbre) and high-level concepts relating to musical and performance style. We examine one widely used benchmark dataset [8] and two datasets comprised of real, orchestral audio recordings. Full orchestral audio recordings for machine learning tasks are relatively under-explored in the literature (e.g., [28] uses symbolic representations), likely due to the complexity in terms of multiple instruments playing simultaneously. It is important to analyze orchestral audio recordings to evaluate the utility of various features in musical settings that are much more complex than a single instrument [8] or in popular music genre classification settings [14, 19]. The three datasets and tasks of interest are:
(1) NSynth [8]: This dataset consists of examples of single instruments playing a single note pitch at a single velocity (volume). We limit to only acoustic recordings and are mainly focused on classifying the instrument family, which is an 8-class classification task (the classes are brass, flute, guitar, keyboard, mallet, reed, string, vocal). Related discriminative tasks discussed in Section 3.1 include predicting the note pitch and note velocity performed in each audio sample. Our dataset consists of 50000 total examples and the focus of this dataset is the recovery of low-level musical features (e.g., instrument, pitch, volume).
(2) Composer: This dataset consists of symphonic recordings by five composers (Beethoven, Brahms, Haydn, Sibelius, Tchaikovsky) performed by the Berlin Philharmonic under Herbert von Karajan. The main task is 5-class composer classification, with the goal of recovering differences in musical style between these composers. This is a much higher-level musical concept than what single instrument is playing a single pitch, as in the NSynth dataset. This dataset consists of 19667 total examples.
(3) Beethoven: This dataset consists of recordings of all nine Beethoven symphonies recorded by 10 different orchestras and is the same dataset used in [30]. The main task is 10-class classification of the orchestra performing each piece, focusing on the evaluation of performance style following [30]. This dataset consists of 52999 total examples.
All datasets start as 4 second audio clips that are transformed into mel-spectrograms with 128 mels on the dB scale. Mel-spectrograms are widely used to represent audio data, especially with CNNs [7, 5, 21]. All datasets have a 70%-30% train-test split and the mel-spectrograms are normalized to have mean 0 and standard deviation 1 prior to training. The NSynth dataset is originally 4 second long audio clips with a sampling rate of 16 kHz, resulting in mel-spectrograms of dimension 128 x 128. The Composer and Beethoven datasets are formed by dividing each piece into non-overlapping 4 seconds of audio and both have sampling rates of 22050 Hz, resulting in mel-spectrograms of dimension 128 x 176. The same pre-processing procedure to form the mel-spectrograms is used for all datasets; additional details are given in Section S2.1 of the Supplementary Materials.
2.2 Convolutional Architectures
We explore 5 different deep convolutional architectures that are selected to be fast to train and reflective of commonly used convolutions for audio modeling. We select smaller networks to (a) explore minimal architectures for extracting features, and (b) because of the moderate size of the datasets that we consider. Especially in music audio modeling, it is not always possible to obtain more data; for example, we already consider all 6 Tchaikovsky symphonies in the Composer dataset, and meaningful data augmentation is non-trivial for orchestral music audio.
Each architecture consists of 3 convolutional layers with batch normalization, and max pooling after the first 2 convolutional layers, similar to [16]. The deep layers of interest for extracting features are then: conv1 (10 channels), pool1, conv2 (20 channels), pool2, and conv3 (30 channels). The last convolutional layer is followed by a fully connected layer with 120 units and then a fully connected layer to a softmax output with the appropriate dimension for each dataset's classification task. The
3

5 architectures considered all have the same aforementioned structure and differ only in the type of convolution used. The first architecture uses Regular convolutions and the second architecture uses Deformable convolutions [6] for the last convolutional layer, conv3. The Dilated architecture uses dilated convolutions at all three layers and is motivated by the success of dilated convolutions in the WaveNet architecture [18]. The last two architectures are musically motivated, following [21]; the 1dF architecture utilizes 1 dimensional dilated convolutions along the frequency dimension only, while the 1dT architecture uses dilated convolutions along the time dimension only. The 1dF architecture specializes in learning frequency features, like pitch or timbre, while the 1dT architecture is meant for learning temporal features like rhythm and tempo [21]. For all deep architectures, we use 5 different initializations, as the features learned by deep architectures have been found to vary slightly based on initialization [13, 29]; we confirm this finding for our data in Section S6.1 in the Supplementary Materials.
All architectures are trained with cross entropy loss, the SGD optimizer with learning rate 0.005 and momentum 0.9, for 150 epochs with batch size 64. The input mel-spectrograms are treated as a single channel to the convolutions. Full details are given in Section S2.2 of the Supplementary Materials. Again, these architectures are intended to be deep enough to learn meaningful features and perform the main task well, but not too large to overfit the moderate size datasets used here and in music audio modeling in general. Deep architectures like ResNet50 are likely to overfit our data.
2.3 Extracted Features
We consider both deep features from the convolutional architectures and hand-crafted features that are based on wavelets and classical signal processing. The deep features correspond to the layer activations after each convolutional and pooling layer, and are extracted with the weights of the network frozen [10]. We conduct experiments across all layers and focus on the deep features from the last convolutional layer (conv3). We compare the deep features to interpretable, hand-crafted wavelet and signal processing features. We explore numerous different hand-crafted features (see Section S2.3 in the Supplementary Materials) and highlight a selection of top features in terms of classification accuracy here. All features are calculated from the mel-spectrograms directly. These features are: (1) Mean Power of the mel-spectrogram over time (relates to the volume of the audio signal). (2) Time to -70 dB: for each mel-frequency bin, at what time does that frequency reach -70 dB in power or less? This is a measure of the decay of the audio signal for each frequency bin and relates to envelope features (i.e. the ADSR, attack, decay, sustain, release model) commonly used in music processing [17]. (3) Mean Wavelet (25): the mean coefficients across time from a Ricker wavelet with bandwidth 25 and centered at 0 applied to each frequency bin of the mel-spectrogram. (4) Combined Wavelet: the same procedure as feature (3), but instead of taking the mean of the coefficients across time, the standard deviation, variance, kurtosis, 25th quantile and 75th quantile are calculated and concatenated together. These features capture additional information about the shape of the wavelet transform over time beyond just the mean coefficients. (5) Top Combined: all four of the above features are concatenated together. The wavelet features considered here are fairly robust to the choice of bandwidth parameter, and taking the summary wavelet statistics over frequency instead of time gives higher classification accuracy (Section S3.1 in the Supplementary Materials).
2.4 Analysis Methods
Following [10], all features are analyzed in terms of their ability to achieve high classification accuracy on the main discriminative tasks for each dataset, using simple multi-class logistic regression. All hand-crafted and deep features are normalized prior to training the logistic regression models. The hand-crafted features are 1 dimensional vectors and fed directly into the logistic regression model, while the deep features are flattened over channel and dimension for the decoding. That is, if the features after conv3 are C × H × W dimensional, the deep features fed to the logistic regression models are CHW dimensional. The decoding results are averaged over the classification accuracies for each of the deep feature architecture initializations; logistic regression models with 5 different initializations are trained for each of the hand-crafted features.
Ultimately, we wish to relate the deep features to the hand-crafted features to explain what deep architectures are learning from music audio data; we use linear centered kernel alignment (CKA) [12] to do so. Let X  Rn×p1 be one set of features, and Y  Rn×p2 be another set of features, where n is the number of training examples and p1 and p2 are the dimensions of each feature. Assume that
4

the columns of X and Y are centered. Then, the linear CKA similarity measure can be calculated as

[12]:

sim(X, Y

)

=

||Y T X||2F ||XT X||F ||Y T Y

||F

,

(1)

where || · ||F is the Frobenius norm. The authors of [12] explore many similarity measures and find the linear CKA approach to be easy to calculate and to give the best results; we confirm these findings with additional similarity measures in Section S4.3 of the Supplementary Materials.

3 Experimental Results
We hypothesize that deep convolutional architectures trained on mel-spectrograms learn features that correspond to classical signal processing and wavelet features, which enables such high performance on discriminative tasks. Our main experiments focus on understanding which features are useful for the main discriminative tasks, if these features are useful for multiple related tasks, and how similar the deep features from trained and untrained architectures are to the hand-crafted features.
3.1 What Features Are Useful For the Discriminative Tasks?
The experiments in this section focus on what features are useful for the main discriminative tasks, in the sense of achieving high test accuracy. In particular, we try to understand if these features are useful across multiple tasks [10]. Table 1 shows that across all three datasets, the hand-crafted features are able to achieve comparable accuracy to some of the deep features. Among the deep features from the last convolutional layer (conv3), the Deformable features achieve the highest classification accuracy across datasets, with the Regular convolutions second best. The Regular and Deformable architectures will be the main focus of the remaining results. The baseline single hidden layer, fully connected Feed Forward architecture in Table 1 uses mel-spectrograms as features directly and is outperformed by both the hand-crafted and deep features, especially on the Composer and Beethoven datasets. Furthermore, the deep convolutional features provide a significant dimensionality reduction compared to the mel-spectrograms themselves, for example 30 x 4 x 6 (Regular conv3) vs. 128 x 176 (mel-spectrograms) for the Composer and Beethoven datasets.
Next, we examine the robustness of deep features following [10]. Specifically, we compare how the deep features necessary for different targets are enhanced or suppressed when the discriminative task changes. The tasks for the NSynth dataset include one classification task defined as multiclassification of instrument family and two regression tasks defined as the note pitch and note velocity (volume) prediction for each mel-spectrogram. We first train Regular and Deformable architectures separately for each of the above three tasks, and again extract the deep features from each layer for these architectures (the architectures for the regression tasks are identical to those described in Section 3.2 above, but with a linear last layer with one output and MSE loss for the regression tasks). Then, the goal is to explore how well all of these deep features perform at each discriminative task, whether or not the features are extracted from deep architectures that were originally trained on that task. For example, we want to see how the deep features that are trained for the note pitch regression perform on the note velocity task.
As illustrated in Figure 1, we find that the deep features are not enhanced or suppressed based on the target feature of interest. This is in contrast with the results in [10] for natural images. That is, whether the deep features are trained on classifying the instrument family (pink curves), predicting the note pitch (green curves) or predicting the note velocity (blue curves), the test accuracy and root mean squared error (RMSE) are nearly identical across target tasks. Even though our original deep architectures are trained to classify the instrument family, these extracted deep features are able to predict both the note pitch and the note velocity as well as the architectures that were trained directly on those tasks; this is true of both the Regular and Deformable architectures. The robustness of the deep features to target task is also seen for the Composer and Beethoven datasets (Section S3.3 of the Supplementary Materials); that is, deep features trained on one audio task are able to achieve accuracy comparable to identical deep architectures trained on another task directly. The hand-crafted features are also useful for multiple tasks; the Top Combined wavelet features achieve RMSE values of 11.57 and 33.53 for the note pitch and note velocity tasks, respectively, comparable to the deep features at later layers in Figure 1. In addition, Figure 1 confirms previous results for natural images that test accuracy by layer for the deep features improves with depth, with the last convolutional layer

5

(conv3) achieving the highest test accuracy and lowest test RMSE. This is also true for the Composer dataset (Figure 5b).

Table 1: Test set accuracy for the top hand-crafted features and the deep features from the last convolutional layer (conv3) for each dataset. The mean values across 5 initializations are reported with 1 standard deviation. The hand-crafted features are able to achieve comparable accuracy to some of the deep features. A Feed Forward, fully connected architecture with one hidden layer of 120 units is used as a baseline.

Random Guessing Mean Power Time to -70 dB Mean Wavelet (25) Wavelet Combined Top 4 Combined Regular Deformable Dilated 1dF 1dT Feed Forward

NSynth (8 Classes) 26.50%
66.81 ± 0.37% 56.95 ± 0.03% 60.69 ± 0.08% 74.44 ± 0.18% 84.05 ± 0.07% 96.35 ± 0.19% 97.80 ± 0.08% 95.02 ± 0.42% 95.60 ± 0.12% 94.28 ± 0.69% 96.91 ± 0.06%

Composer (5 Classes) 25.20%
27.50 ± 0.04% 35.02 ± 0.12% 62.65 ± 0.05% 59.56 ± 0.15% 64.04 ± 0.09% 72.41 ± 1.21% 75.76 ± 0.68% 68.10 ± 0.57% 65.42 ± 0.21% 70.08 ± 0.98% 64.86 ± 2.07%

Beethoven (10 Classes) 10.4%
12.93 ± 0.07% 23.85 ± 0.10% 74.14 ± 0.04% 66.32 ± 0.05% 73.33 ± 0.04% 82.32 ± 0.40% 84.11 ± 0.70% 75.98 ± 0.58% 72.84 ± 1.08% 82.11 ± 0.75% 76.28 ± 0.36%

3.2 Are the Learned Deep Features Similar to the Hand-Crafted Features?
We next explore the similarity between the learned deep features and the hand-crafted features (which are useful for the main discriminative task, Table 1) using the linear CKA similarity measure [12]. We first compare the similarity between the deep features from the last convolutional layer (conv3) across architectures to the top hand-crafted features in Figure 20. Across all three datasets, there is relatively high similarity between all deep features and the mean wavelet feature, especially for the 1dT architecture. This makes sense, since the 1dT architecture takes convolutions over time only and is thus very similar to the mean wavelet feature, which takes the mean wavelet coefficient for each frequency over time. This suggests that hand-crafted wavelet features could be used in lieu of the musically motivated architectures, 1dF and 1dT [21]. Furthermore, the Deformable deep features tend to be more similar to the hand-crafted features than the Regular deep features. We confirm this similarity by concatenating the deep features to each hand-crafted feature for the Beethoven dataset. The Regular deep features are most similar to the Mean Power and Mean Wavelet features, and when these features are concatenated together, the logistic regression accuracy on the main classification task does not improve beyond that of the deep Regular features alone (82.32 ± 0.40% for the Regular deep features, vs. 82.35 ± 0.34% for the deep features concatenated with Mean Power). However, the Regular features are less similar to the Wavelet Combined and Top 4 Combined features, and indeed, when combined with the Regular deep features, these hand-crafted features can further improve the classification accuracy (83.44 ± 0.42% and 84.23 ± 0.31%, respectively).
We also compare the deep features at each layer to the top hand-crafted features for the Regular and Deformable convolutions on the NSynth (Figure 21) and Composer (Figure 4) datasets. For the NSynth dataset, the deep features from the earlier layers exhibit a very high similarity with the hand-crafted features, while for the Composer dataset, the middle layers exhibit a very high similarity with the hand-crafted features. Overall, the last convolutional layer (conv3) for the Deformable architectures tends to be more similar to the hand-crafted features than the same Regular layer (conv3 is the Deformable layer in this architecture, the earlier convolutions are regular).
3.3 Do Untrained Convolutional Architectures Have a Useful Inductive Bias for Music Audio Discriminative Tasks?
Finally, we explore how well features from the untrained deep architectures can perform the main classification tasks and how similar the untrained features are to the hand-crafted features. These experiments explore the utility of the inductive bias of convolutions for music audio tasks, taking
6

(a) Regular
(b) Deformable Figure 1: Discriminative task results for deep features by layer for (a) Regular (Top row) and (b) Deformable (Bottom row) architectures trained on the NSynth data. Three identical CNN architectures are trained to perform each of the three target discriminative tasks: the pink curves are extracted from architectures trained to predict instrument family, the green curves are extracted from architectures trained to predict note pitch and the blue curves are extracted from architectures trained to predict note velocity. Then, these extracted features from all 3 models are used to classify instrument family (left column, 8-class test accuracy), note pitch (center column, RMSE) and note velocity (right column, RMSE). For each target task or column, the three curves are overlapping or identical. The deep features perform well across target tasks, whether or not they were extracted from deep architectures originally trained on that task. conv1 and pool1 features are averaged over channels, all other deep features are flattened over channels. The mean accuracies/RMSE values are shown with one standard deviation as error bars; plots following [10].

(a) NSynth

(b) Composer

(c) Beethoven

Figure 2: Linear CKA similarity between the deep features from each architecture from the last convolutional layer (conv3) with the top hand-crafted features for the (a) NSynth, (b) Composer and (c) Beethoven datasets. We baseline with Random standard normal noise in the last column. The 1dT features are especially similar to the mean wavelet features by design. Plots are the similarity value averaged across the deep features from each initialization.

7

(a) Regular

(b) Deformable

Figure 3: Linear CKA similarity between the deep features from each layer for the NSynth dataset for the (a) Regular and (b) Deformable architectures. The earlier layers exhibit very high similarity with the hand-crafted features.

(a) Regular

(b) Deformable

Figure 4: Linear CKA similarity between the deep features from each layer for the Composer dataset for the (a) Regular and (b) Deformable architectures.

a different perspective than [31]. First, in Figure 5 we look at how well each layer for the Regular and Deformable untrained architectures performs for the main classification tasks. We find that for both the NSynth and Composer datasets, all untrained layers are able to classify well above random guessing, and that only the later layers improve in accuracy after training. This again confirms earlier results that the last convolutional layers are the most accurate and extract higher-level concepts than the earlier layers. Next, in Figure 6 we compare the untrained deep features from the last convolutional layer to the hand-crafted features for all architectures. Across all datasets, the untrained deep features are more similar to the hand-crafted features than the trained deep features in Figure 20. The untrained deep features from the earlier convolutional layers for the NSynth (Figure 7) and Composer datasets (Figure 7c) are especially similar to the hand-crafted features, while the later layers become less similar to the hand-crafted features after training.

(a) NSynth

(b) Composer

Figure 5: Logistic regression classification accuracies by layer for the (a) NSynth and (b) Composer datasets using untrained deep features from the Regular and Deformable architectures (right side of each plot) compared to the trained deep features (left side of each plot).

8

(a) NSynth

(b) Composer

(c) Beethoven

Figure 6: Linear CKA similarity between the untrained deep features from each architecture from the last convolutional layer (conv3) with the top hand-crafted features for the (a) NSynth, (b) Composer and (c) Beethoven datasets.

(a) Regular - NSynth

(b) Deformable - NSynth

(c) Regular - Composer

Figure 7: Linear CKA similarity between the untrained deep features from each layer for the NSynth dataset for the (a) Regular and (b) Deformable architectures, and for the Composer dataset for the (c) Regular architecture. The earlier layers exhibit very high similarity with the hand-crafted features.

4 Conclusion
In this work, we explore why deep convolutional architectures perform so well for music audio discriminative tasks, with a focus on understanding deep features in terms of feature classification and similarity to interpretable features. We find that deep features from Regular and Deformable convolutions in particular are useful for several discriminative tasks, whether or not originally trained on a specific task. We also find that deep features are very similar to hand-crafted wavelet features, whether the deep features are extracted from a trained or untrained architecture. Earlier convolutional layers do not improve in the accuracy of their deep features after training, while the later layers do. Indeed, the later layers become less similar to the hand-crafted features after training, while the deep features from earlier layers remain highly similar to the hand-crafted features. This suggests that deep convolutions may perform so well for discriminative audio tasks, even though mel-spectrograms are not natural images, because they recover hand-crafted features from classical signal processing that are known (and shown) to be useful for discriminative tasks. Additional results in the Supplementary Materials confirm the robustness of deep features for the Composer and Beethoven datasets and explore the similarity of deep features across architectures, layers and initializations.
The similarity between the untrained deep features and the hand-crafted features, in particular, suggests future avenues of research in refining deep priors for music audio data, relating to and expanding on [31], for example. Additionally, incorporating knowledge of the deep features into future deep models could enable reduced size architectures for related audio tasks, which currently tend to be massive in terms of parameters and required data, e.g., [18].
A limitation of this work may be the lack of mathematical analysis for the empirical findings presented here; further explorations of the theoretical aspects of this paper would be an interesting future research direction. Additionally, potential negative social impacts of this work include more informed adversarial attacks on music audio systems.
9

References
[1] J. Adebayo, J. Gilmer, M. Muelly, I. Goodfellow, M. Hardt, and B. Kim. Sanity checks for saliency maps. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/ file/294a8ed24b1ad22ec2e7efea049b8737-Paper.pdf.
[2] Brian McFee and Colin Raffel and Dawen Liang and Daniel P. W. Ellis and Matt McVicar and Eric Battenberg and Oriol Nieto. librosa: Audio and Music Signal Analysis in Python. Proceedings of the 14th Python in Science Conference, pages 18­25, 2015.
[3] C. Chen, O. Li, D. Tao, A. Barnett, C. Rudin, and J. K. Su. This looks like that: Deep learning for interpretable image recognition. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d' Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/ file/adf7ee2dcf142b0e11888e72b43fcb75-Paper.pdf.
[4] I. Covert, S. M. Lundberg, and S.-I. Lee. Understanding global feature contributions with additive importance measures. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 17212­ 17223. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/ 2020/file/c7bf0b7c1a86d5eb3be2c722cf2cf746-Paper.pdf.
[5] J. Cramer, H. Wu, J. Salamon, and J. P. Bello. Look, listen, and learn more: Design choices for deep audio embeddings. In ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 3852­3856, 2019.
[6] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei. Deformable convolutional networks. In 2017 IEEE International Conference on Computer Vision (ICCV), pages 764­773, 2017. doi: 10.1109/ICCV.2017.89.
[7] S. Dieleman and B. Schrauwen. End-to-end learning for music audio. In 2014 IEEE International Conference on Acoustic, Speech and Signal Processing (ICASSP), 2014.
[8] J. H. Engel, C. Resnick, A. Roberts, S. Dieleman, D. Eck, K. Simonyan, and M. Norouzi. Neural audio synthesis of musical notes with wavenet autoencoders. CoRR, abs/1704.01279, 2017. URL http://arxiv.org/abs/1704.01279.
[9] A. Ghorbani, J. Wexler, J. Y. Zou, and B. Kim. Towards automatic concept-based explanations. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d' Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/ 77d2afcb31f6493e350fca61764efb9a-Paper.pdf.
[10] K. Hermann and A. Lampinen. What shapes feature representations? Exploring datasets, architectures, and training. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 9995­10006. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/ file/71e9c6620d381d60196ebe694840aaaa-Paper.pdf.
[11] S. Hooker, D. Erhan, P.-J. Kindermans, and B. Kim. A benchmark for interpretability methods in deep neural networks. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d' Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/ file/fe4b8556000d0f0cae99daa5c5c5a410-Paper.pdf.
[12] S. Kornblith, M. Norouzi, H. Lee, and G. Hinton. Similarity of neural network representations revisited. In K. Chaudhuri and R. Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 3519­3529. PMLR, 09­15 Jun 2019. URL http://proceedings.mlr.press/v97/ kornblith19a.html.
10

[13] Y. Li, J. Yosinski, J. Clune, H. Lipson, and J. E. Hopcroft. Convergent learning: Do different neural networks learn the same representations? In Y. Bengio and Y. LeCun, editors, 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv.org/abs/1511. 07543.
[14] C. Liu, L. Feng, G. Liu, H. Wang, and S. Liu. Bottom-up broadcast neural network for music genre classification. Multimedia Tools and Applications, 80:7313­7331, 2021.
[15] S. M. Lundberg and S.-I. Lee. A unified approach to interpreting model predictions. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/ 8a20a8621978632d76c43dfd28b67767-Paper.pdf.
[16] Y. Luo and L. Su. Learning domain-adaptive latent representations of music signals using variational autoencoders. In E. Gómez, X. Hu, E. Humphrey, and E. Benetos, editors, Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018, Paris, France, September 23-27, 2018, pages 653­660, 2018. URL http://ismir2018.ircam.fr/doc/pdfs/169_Paper.pdf.
[17] M. Müller. Fundamentals of Music Processing. Springer, 2015.
[18] A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. W. Senior, and K. Kavukcuoglu. WaveNet: A generative model for raw audio. CoRR, abs/1609.03499, 2016. URL http://arxiv.org/abs/1609.03499.
[19] S. Oramas, O. Nieto, F. Barbieri, and X. Serra. Multi-label music genre classification from audio, text and images using deep features. 18th International Society for Music Information Retrieval Conference, Suzhou, China, 2017. URL https://ismir2017.smcnus.org/wp-content/ uploads/2017/10/126_Paper.pdf.
[20] K. Palanisamy, D. Singhania, and A. Yao. Rethinking CNN models for audio classification. CoRR, abs/2007.11154, 2020. URL https://arxiv.org/abs/2007.11154.
[21] J. Pons, T. Lidy, and X. Serra. Experimenting with musically motivated convolutional neural networks. In 2016 14th International Workshop on Content-Based Multimedia Indexing (CBMI), pages 1­6, 2016. doi: 10.1109/CBMI.2016.7500246.
[22] L. R. Rabiner and B.-H. Juang. Fundamentals of Speech Recognition. PTR Prentice Hall, 1993.
[23] A. Shrikumar, P. Greenside, and A. Kundaje. Learning important features through propagating activation differences. In D. Precup and Y. W. Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 3145­3153. PMLR, 06­11 Aug 2017. URL http://proceedings.mlr.press/v70/ shrikumar17a.html.
[24] K. Simonyan, A. Vedaldi, and A. Zisserman. Deep inside convolutional networks: Visualizing image classification models and saliency maps. CoRR, abs/1312.6034, 2014. URL https: //arxiv.org/abs/1312.6034.
[25] S. S. Stevens, J. Volkman, and E. B. Newman. A scale for the measurement of the psychological magnitude pitch. Journal of the Acoustical Society of America, 8(3):185­190, 1937.
[26] M. Sundararajan, A. Taly, and Q. Yan. Axiomatic attribution for deep networks. In D. Precup and Y. W. Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 3319­3328. PMLR, 06­11 Aug 2017. URL http://proceedings.mlr.press/v70/sundararajan17a.html.
[27] J. Thickstun, Z. Harchaoui, and S. M. Kakade. Learning features of music from scratch. In International Conference on Learning Representations (ICLR), 2017.
11

[28] H. Verma and J. Thickstun. Convolutional composer classification. In A. Flexer, G. Peeters, J. Urbano, and A. Volk, editors, Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019, Delft, The Netherlands, November 4-8, 2019, pages 549­556, 2019. URL http://archives.ismir.net/ismir2019/paper/000066. pdf.
[29] L. Wang, L. Hu, J. Gu, Z. Hu, Y. Wu, K. He, and J. Hopcroft. Towards understanding learning representations: To what extent do different neural networks learn the same representation. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/ 5fc34ed307aac159a30d81181c99847e-Paper.pdf.
[30] A. K. Yanchenko and P. D. Hoff. Hierarchical multidimensional scaling for the comparison of musical performance styles. Annals of Applied Statistics, 14(4):1581 ­ 1603, 2020.
[31] Z. Zhang, Y. Wang, C. Gan, J. Wu, J. B. Tenenbaum, A. Torralba, and W. T. Freeman. Deep audio priors emerge from harmonic convolutional networks. In International Conference on Learning Representations (ICLR), 2020.
12

S1 Summary of Additional Details and Results
We include additional details about the data, architectures and results included in the main paper in these Supplementary Materials, as well as additional supporting experiments. Specific details about the three datasets, the pre-processing procedure to calculate the mel-spectrograms and hand-crafted features, as well as details about all deep learning architectures, computing resources and code are discussed in Section S2. In Section S3, we include feature classification results for numerous additional hand-crafted features, supporting our selection of the top 4 features discussed in the main paper. The wavelet features, in particular, are found to be robust to the choice of the bandwidth parameter. We also demonstrate that the deep features trained on one discriminative task are able to achieve high classification accuracy on other tasks that they were not trained on for the Composer and Beethoven datasets. In Section S4, we compare the similarity between the deep features for the last convolutional layer by architecture and the deep features from all layers for the Regular and Deformable architectures to all of the hand-crafted features. We also explore additional similarity measures for comparison to the Linear CKA similarity of main focus. Untrained deep features are compared by layer and by architecture to all of the hand-crafted features in Section S5. We again find that the untrained features from the last convolutional layer across architectures and for all layers for the Regular and Deformable architectures are highly similar to the hand-crafted features, especially the wavelet features. Finally, we include additional experiments in Section S6 that explore the similarity of the deep features across architectures, layers and initializations. We find that learned deep features are highly similar across initializations, but not identical, reinforcing the need for multiple initializations. Code for the NSynth experiments is available at https: //github.com/aky4wn/convolutions-for-music-audio.
S2 Methodology
S2.1 Data and Tasks
All of the mel-spectrograms used as input data in this work are calculated from 4 second audio clips. The sampling rate of the NSynth audio is 16 kHz and the sampling of the Composer and Beethoven datasets is 22050 Hz. For all three datasets, the mel-spectrograms are calculated with 128 mels, a maximum frequency of 8000 Hz, a hop length of 502 and a Hanning window with FFT window length 2048 using the librosa package in Python [2]. This pre-processing results in mel-spectrograms of dimension 128 x 128 for the NSynth dataset, and of dimension 128 x 176 for the Composer and Beethoven datasets. The mel-spectrogram is first calculated on the power scale, then converted to dB, with reference of the maximum power; the phase information is not used. All datasets use a 70-30% train-test split. PCA decompositions colored by class for the Beethoven dataset are given in Figure 4 and are similar for the other datasets; the classes overlap indicating that the classification tasks for each dataset are non-trivial.
S2.1.1 NSynth
The NSynth dataset has 50000 total data examples. The main task is an 8-class classification of instrument family; the class balance for the train and test set are given in Table 1. The related tasks are Note Pitch and Note Velocity (volume) regression, with the distribution of these tasks given in Figure 1. The Note Pitch is an integer value that can be (in theory) between 0 and 127, inclusive, while the Note Velocity is also an integer velocity between 0 and 127 inclusive, though only 5 distinct values are observed among the acoustic instruments (Figure 1). The data used is taken from the training portion of the NSynth dataset [8], where 50000 randomly sampled acoustic examples are selected and the bass and organ instrument families are dropped since they do not have very many examples. The acoustic examples are most similar to the real orchestral recordings and were selected for that reason. The training data was subset to approximately match the amount of data for the Composer and Beethoven datasets. Example mel-spectrograms are shown in Figure 2. The NSynth dataset is under a Creative Commons Attribution 4.0 International (CC BY 4.0) license (https://magenta.tensorflow.org/datasets/nsynth#files).
13

Train Test

Table 1: Train and test sets class balance for the NSynth dataset.
Brass Flute Guitar Keyboard Mallet Reed String 12.3% 6.3% 11.3% 8.0% 26.5% 13.0% 18.6% 12.7% 6.0% 11.4% 7.9% 26.6% 12.7% 19.3%

Vocal 4.0% 3.5%

Figure 1: Histograms of train (left) and test (right) distributions for the note pitch (top row) and note velocity = volume (bottom row) related tasks for the NSynth dataset.

Figure 2: Example mel-spectrograms for several instrument families for the NSynth dataset. There are obvious visual differences in the mel-spectrograms based on the instrument family, especially between percussive (keyboard), string (guitar) and wind (reed, flute and brass) instruments.
14

S2.1.2 Composer
The Composer dataset is made up of 19667 total examples. All of these orchestral pieces are from classical composers, recorded by the Berlin Philharmonic under Herbert von Karajan to eliminate performance style differences by different orchestras, which can vary considerably [30] and will be considered with the Beethoven dataset. Before calculating the mel-spectrograms, each audio recording of a piece is divided into 4 second non-overlapping chunks, with any chunks less than 4 seconds at the end of pieces dropped. The class balance for the main task of 5-class composer classification is given in Table 2. The related task for the Composer dataset is classifying whether each example came from the beginning, middle or end of its respective piece, where the classes are determined by evenly splitting the total length of the audio recording of each piece in thirds and labeling each spectrogram accordingly. The class balance for this task is given in Table 3 and again evaluates musical style differences across the length of each piece. Example composer spectrograms are given in Figure 3. A list of pieces used in the Composer dataset is below, and primarily consists of symphonic works by each composer.

Table 2: Train and test sets class balance for the Composer dataset.

Beethoven Brahms Haydn Sibelius Tchaikovsky

Train 22.8% 12.0% 25.2% 18.0%

22.1%

Test 23.1% 11.4% 25.5% 18.1%

21.9%

Table 3: Train and test sets class balance for the related task of part of piece classification for the Composer dataset.
Beginning Middle End Train 33.3% 33.3% 33.4% Test 33.6% 33.6% 32.8%

Figure 3: Example mel-spectrograms for each composer for the Composer dataset. There are not obvious visual differences in the mel-spectrograms based on the composer, indicating that visual explanation techniques are not likely to be meaningful for this type of data.
Beethoven Pieces The Beethoven pieces in the Composer dataset are: Fidelio Overture Op. 72, Coriolan Overture Op. 62, Leonore No. 3 Overture Op. 72b, Egmont Overture, Symphony No 1 Op. 21 (4 movements), Symphony No 2 Op. 36 (4 movements), Symphony No 3 Op. 55 (4 movements), Symphony No 4 Op. 60 (4 movements), Symphony No 5 Op. 67 (4 movements), Symphony No
15

6 Op. 68 (5 movements), Symphony No 7 Op 92 (4 movements) and Symphony No 8 Op 93 (4 movements).
Brahms Pieces The Brahms pieces in the Composer dataset are: Symphony No 1 Op. 68 (4 movements), Symphony No 2 Op. 73 (4 movements), Symphony No 3 Op. 90 (4 movements) and Symphony No 4 Op. 98 (4 movements).
Haydn Pieces The Haydn pieces in the Composer dataset are: Symphony No 93 H. 1/94 (4 movements), Symphony No 94 H. 1/94 (4 movements), Symphony No 95 H. 1/95 (4 movements), Symphony No 96 H. 1/96 (4 movements), Symphony No 97 H. 1/97 (4 movements), Symphony No 98 H. 1/98 (4 movements), Symphony No 99 H. 1/99 (4 movements), Symphony No 100 H. 1/100 (4 movements), Symphony No 101 H. 1/101 (4 movements), Symphony No 102 H. 1/102 (4 movements), Symphony No 103 H. 1/103 (4 movements) and Symphony No 104 H. 1/104 (4 movements).
Sibelius Pieces The Sibelius pieces in the Composer dataset are: Karelia Suite Op. 11 (3 movements), En Saga Op. 9, Finlandia Op. 26, Swan of Tuonela Op. 22-3, Tapiola Op. 112, Valse Triste Op. 44-1, Symphony No 1 Op. 39 (4 movements), Symphony No 2 Op. 43 (4 movements), Symphony No 4 Op. 63 (4 movements) and Symphony No 5 Op. 82 (3 movements).
Tchaikovsky Pieces The Tchaikovsky pieces in the Composer dataset are: Capriccio Italien Op. 45, Symphony No 1 Op. 13 (4 movements), Symphony No 2 Op. 17 (4 movements), Symphony No 3 Op. 29 (4 movements), Symphony No 4 Op. 36 (4 movements), Symphony No 5 Op. 64 (4 movements) and Symphony No 6 Op. 74 (4 movements).
S2.1.3 Beethoven
The Beethoven dataset is made up of 52999 total examples and uses the same audio recordings as in [30]. The main classification task is the 10-class classification of orchestras performing each of the 9 Beethoven symphonies to focus on performance style of different orchestras. The 10 orchestras considered are the Academy of Ancient Music under Hogwood, the Berlin Philharmonic under Rattle, the Berlin Philharmonic under von Karajan, the Chicago Symphony Orchestra under Solti, the Leipzig Gewandhaus Orchestra under Masur, the London Symphony Orchestra (LSO) under Haitink, the NBC Symphony Orchestra under Toscanini, the New York Philharmonic under Bernstein, the Philadelphia Orchestra under Muti and the Vienna Philharmonic under Rattle. The class balance is given in Table 4. The recordings of each piece are of slightly different lengths (due to differences in tempo, for example), which is why the class balance is not exactly 10%. The two related tasks are classifying the symphony number (9-classes, musical style task) and whether the orchestra is American or European (2-class, 41.4% - 58.6% test balance, performance style task), as European orchestras can have common performance styles that differ from American orchestras. The class balance for the piece classification is given in Table 5.

Table 4: Train and test sets class balance for the Beethoven dataset.

Train Test

Ancient Music
9.8% 9.5%

BerlinRattle
9.7% 9.7%

Berlinvon Karajan 10.1% 10.0%

Chicago Leipzig LSO NBC NY Phil Philadelphia Vienna

10.4% 10.1% 9.7% 9.6% 10.3% 11.4% 9.8% 9.8% 9.6% 10.2%

10.2% 10.2%

10.1% 9.8%

Table 5: Train and test sets class balance for the related task of symphony number classification for the Beethoven dataset.
No 1 No 2 No 3 No 4 No 5 No 6 No 7 No 8 No 9 Train 7.3% 9.4% 14.3% 9.5% 9.1% 12.1% 11.5% 7.2% 19.5% Test 7.3% 9.4% 14.2% 9.6% 9.7% 11.9% 11.2% 7.1% 19.6%

16

Figure 4: PCA decomposition for the Beethoven training dataset, colored by the class. The dataset is clearly not linearly separable when represented by the first two principal components of the flattened mel-spectrograms, so the orchestra classification task is non-trivial.
S2.2 Convolutional Architectures
The exact convolutional architectures used for all three datasets are given below, displayed as the PyTorch model specification (https://pytorch.org/). The architectures are slightly different between the NSynth and the Composer and Beethoven datasets due to the difference in input mel-spectrogram dimensions. The Deformable convolution implementation used is from https://github.com/4uiiurz1/pytorch-deform-conv-v2. The 1d Frequency and 1d Time architectures are motivated by [21].
S2.2.1 NSynth Architectures
Regular Convolution
CNN( (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1)) (conv1_bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(3, 3), padding=(3, 3)) (conv2_bn2): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (conv3): Conv2d(20, 30, kernel_size=(5, 5), stride=(2, 2)) (conv3_bn3): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (fc1): Linear(in_features=480, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=8, bias=True)
)
Deformable Convolution
CNN_deform( (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1)) (conv1_bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(3, 3), padding=(3, 3)) (conv2_bn2): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (conv3): DeformConv2d( (zero_padding): ZeroPad2d(padding=(0, 0, 0, 0), value=0.0) (conv): Conv2d(20, 30, kernel_size=(5, 5), stride=(5, 5), bias=False) (p_conv): Conv2d(20, 50, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) ) (conv3_bn3): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (fc1): Linear(in_features=1080, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=8, bias=True)
)
Dilated Convolution
CNN_dilated( (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1), dilation=(3, 3)) (conv1_bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
17

(pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(2, 2), padding=(3, 3), dilation=(2, 2)) (conv2_bn2): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (conv3): Conv2d(20, 30, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1)) (conv3_bn3): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (fc1): Linear(in_features=1080, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=8, bias=True) )
1d Frequency
CNN_1dF( (conv1): Conv2d(1, 10, kernel_size=(5, 1), stride=(1, 1), dilation=(3, 1)) (conv1_bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (pool1): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False) (conv2): Conv2d(10, 20, kernel_size=(5, 1), stride=(2, 1), padding=(3, 0), dilation=(2, 1)) (conv2_bn2): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (pool2): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False) (conv3): Conv2d(20, 30, kernel_size=(5, 1), stride=(2, 1), padding=(1, 0)) (conv3_bn3): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (fc1): Linear(in_features=23040, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=8, bias=True)
)
1d Time
CNN_1dT( (conv1): Conv2d(1, 10, kernel_size=(1, 5), stride=(1, 1), dilation=(1, 3)) (conv1_bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (pool1): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False) (conv2): Conv2d(10, 20, kernel_size=(1, 5), stride=(1, 2), padding=(0, 3), dilation=(1, 2)) (conv2_bn2): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (pool2): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False) (conv3): Conv2d(20, 30, kernel_size=(1, 5), stride=(1, 2), padding=(0, 1)) (conv3_bn3): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (fc1): Linear(in_features=23040, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=8, bias=True)
)
S2.2.2 Composer and Beethoven Architectures
Note: for the Beethoven dataset experiments, out_features = 10 for all architectures.
Regular Convolution
CNN( (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1)) (conv1_bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(3, 3), padding=(3, 3)) (conv2_bn2): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (conv3): Conv2d(20, 30, kernel_size=(5, 5), stride=(2, 2)) (conv3_bn3): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (fc1): Linear(in_features=720, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=5, bias=True)
)
Deformable Convolution
CNN_deform( (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1)) (conv1_bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(3, 3), padding=(3, 3)) (conv2_bn2): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (conv3): DeformConv2d( (zero_padding): ZeroPad2d(padding=(0, 0, 0, 0), value=0.0) (conv): Conv2d(20, 30, kernel_size=(5, 5), stride=(5, 5), bias=False) (p_conv): Conv2d(20, 50, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) ) (conv3_bn3): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (fc1): Linear(in_features=1440, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=5, bias=True)
)
18

Dilated Convolution
CNN_dilated( (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1), dilation=(3, 3)) (conv1_bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(2, 2), padding=(3, 3), dilation=(2, 2)) (conv2_bn2): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (conv3): Conv2d(20, 30, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1)) (conv3_bn3): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (fc1): Linear(in_features=1620, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=5, bias=True)
)
1d Frequency
CNN_1dF( (conv1): Conv2d(1, 10, kernel_size=(5, 1), stride=(1, 1), dilation=(3, 1)) (conv1_bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (pool1): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False) (conv2): Conv2d(10, 20, kernel_size=(5, 1), stride=(2, 1), padding=(3, 0), dilation=(2, 1)) (conv2_bn2): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (pool2): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False) (conv3): Conv2d(20, 30, kernel_size=(5, 1), stride=(2, 1), padding=(1, 0)) (conv3_bn3): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (fc1): Linear(in_features=31680, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=5, bias=True)
)
1d Time
CNN_1dT( (conv1): Conv2d(1, 10, kernel_size=(1, 5), stride=(1, 1), dilation=(1, 3)) (conv1_bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (pool1): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False) (conv2): Conv2d(10, 20, kernel_size=(1, 5), stride=(1, 2), padding=(0, 3), dilation=(1, 2)) (conv2_bn2): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (pool2): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False) (conv3): Conv2d(20, 30, kernel_size=(1, 5), stride=(1, 2), padding=(0, 1)) (conv3_bn3): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (fc1): Linear(in_features=34560, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=5, bias=True)
)
All architectures are trained with cross entropy loss, the SGD optimizer with learning rate 0.005 and momentum 0.9, for 150 epochs with a batch size of 64. The overall accuracies of each network architecture, as well as the number of parameters and the dimensions of the features of the last convolutional layer (conv3) are given for the NSynth (Table 6), Composer (Table 7) and Beethoven (Table 8) datasets below. For a baseline, a feed forward network with one hidden layer of 120 units with batch normalization and a softmax output with the appropriate number of classes is also trained (where the mel-spectrogram is flattened for the input). All networks for all datasets are trained identically for 5 different initializations.

Table 6: Train and test accuracy for all 5 architectures trained on the NSynth dataset (8 classes). Mean accuracy and standard deviation over 5 initializations is reported. The number of parameters and feature dimensions of the last convolutional layer (conv3) are also reported.

Architecture Regular Deformable Dilated 1dF 1dT Feed Forward

Train Acc. 98.91 ± 0.02% 98.96 ± 0.02% 96.11 ± 2.75% 98.78 ± 0.07% 98.83 ± 0.15% 98.89 ± 0.02%

Test Acc. 97.78 ± 0.05% 98.20 ± 0.10% 94.79 ± 2.46% 97.87 ± 0.12% 96.99 ± 0.28% 96.91 ± 0.06%

# of Parameters 79118 160138 151118 2770118 2770118
1967408

Feature Dim. conv3 (30, 4, 4) (30, 6, 6) (30, 6, 6) (30, 6, 128) (30, 128, 6)

19

Table 7: Train and test accuracy for all 5 architectures trained on the Composer dataset (5 classes). Mean accuracy and standard deviation over 5 initializations is reported. The number of parameters and feature dimensions of the last convolutional layer (conv3) are also reported.

Architecture Regular Deformable Dilated 1dF 1dT Feed Forward

Train Acc. 96.31 ± 0.15% 97.34 ± 0.14% 95.72 ± 0.13% 96.62 ± 0.18% 98.24 ± 0.13% 97.15 ± 0.09%

Test Acc. 75.78 ± 1.13% 79.79 ± 0.88% 70.64 ± 0.64% 70.24 ± 0.72% 74.30 ± 1.16% 64.86 ± 2.07%

# of Parameters 107555 202975 215555 3806555 4152155 2704325

Feature Dim. conv3 (30, 4, 6) (30, 6, 8) (30, 6, 9) (30, 6, 176) (30, 128, 9)

Table 8: Train and test accuracy for all 5 architectures trained on the Beethoven dataset (10 classes). Mean accuracy and standard deviation over 5 initializations is reported. The number of parameters and feature dimensions of the last convolutional layer (conv3) are also reported.

Architecture Regular Deformable Dilated 1dF 1dT Feed Forward

Train Acc. 95.49 ± 3.62% 96.39 ± 3.99% 86.76 ± 0.13% 97.04 ± 0.19% 99.03 ± 0.07% 96.94 ± 0.09%

Test Acc. 85.85 ± 2.51% 87.50 ± 3.72% 75.92 ± 0.37% 79.34 ± 0.30% 89.08 ± 0.44% 76.28 ± 0.36%

# of Parameters 108160 203580 216160 3807160 4152760 2704930

Feature Dim. conv3 (30, 4, 6) (30, 6, 8) (30, 6, 9) (30, 6, 176) (30, 128, 9)

S2.2.3 Number of Channels Comparison
For the NSynth dataset and the Regular convolutional architecture shown above, we explore the impact of the number of channels on the overall classification accuracy, with all other details fixed. The architectures with more channels result in a higher classification accuracy (Table 9).

Table 9: Train and test accuracy for all the Regular convolution architecture trained on the NSynth dataset (8 classes). Mean accuracy and standard deviation over 3 initializations is reported for a different number of channels at each convolutional layer. The number of parameters is also reported. The selected (10, 20, 30) channels gives the highest accuracy and has a reasonable number of parameters.

# of Channels (5, 10, 10) (10, 10, 10) (10, 20, 30)

Train Acc. 89.41 ± 1.45% 95.46 ± 2.32% 98.93 ± 0.03%

Test Acc. 88.84 ± 1.20% 94.28 ± 1.95% 97.82 ± 0.04%

# of Parameters 24238 25628 79118

S2.3 Features
Deep Features The dimensions of the deep features for the last convolutional layers are given in Table 6, Table 7 and Table 8.
Hand-Crafted Features A variety of hand-crafted features are calculated and compared. The top hand-crafted features are discussed in the main paper, with the remaining features discussed here. Classification results for all features are given in Section 3.1 below. Several of the hand-crafted features are calculated using the librosa package [2] in Python. These features are the root mean square for each spectrogram frame (RMS), spectral centroid, spectral bandwidth, spectral flatness and spectral rolloff (threshold is 85%). All of these features are calculated on the raw audio, with the same parameters as used to calculate the mel-spectrograms (i.e. hop length is 502, etc.). See https://librosa.org/doc/main/feature.html for details about each feature.
20

The remaining features are calculated directly from the unnormalized mel-spectrograms. These features include the median power over time, the mean power over time and the time to -80 dB (approximately 0 power), -75 dB and -70 dB. These last three features capture the decay of the power in the audio signal and are calculated for each frequency bin as the first time at which that frequency goes to each of the respective dB thresholds.
Several wavelet features are also considered. All wavelet features are calculated with the 1D Ricker mother wavelet centered at 0. Various bandwidths are considered. The wavelet features are either calculated over frequency (included in the main paper) or over time. That is, for each frequency (time) the wavelet transform is calculated over time (frequency) and a summary statistic of the wavelet coefficients is calculated. The summary statistics considered are mean, median, standard deviation, variance, kurtosis, 25th quantile and 75th quantile.
Plots of a subset of these features are plotted for a single input mel-spectrogram for the NSynth dataset (Figure 5) and Composer dataset (Figure 6). The mutual information between a subset of the wavelet features and the output class for the Composer dataset is plotted as well (Figure 7).

Figure 5: Mel-spectrogram and various hand-crafted features for an example Reed instrument family for the NSynth dataset. The librosa features are plotted in the top left, the median and mean power over time are plotted in the middle left, the time to specific dB features are plotted in the middle right and the mean wavelet coefficients (by frequency) for 4 different bandwidths are plotted in the bottom left.

S2.4 Analysis Methods
Feature Classification All features, both deep and hand-crafted, are normalized to have mean 0 and standard deviation 1 prior to the feature logistic regression. The multi-class logistic regression is implemented in PyTorch, and the models for all features are trained using cross entropy loss, with the SGD optimizer with a learning rate of 0.01 and momentum 0.9 for 100 epochs with a batch size of 64. The deep features are flattened across channels as input to the multi-class logistic regression. The same initialization of the logistic regression classifier is used for each of the five different initializations of the deep features for each architecture, while five different initializations of the logistic regression classifier are used for each hand-crafted feature.

Feature Similarity In addition to the linear CKA feature similarity, several additional similarity

measures defined in [12] are also considered in Section 3.2 below. The linear regression similarity

measure is:

RL2 R

=

sim(X, Y

)

=

||QTY X||2F ||X ||2F

,

(1)

21

Figure 6: Mel-spectrogram and various hand-crafted features for an example Brahms training melspectrogram for the Composer dataset. The librosa features are plotted in the top left, the median and mean power over time are plotted in the middle left, the time to specific dB features are plotted in the middle right and the mean wavelet coefficients (by frequency) for 4 different bandwidths are plotted in the bottom left.

Figure 7: Mutual information between the Composer dataset hand-crafted features of mean wavelet coefficient over frequency for various bandwidths and the output class (5 composer classes). The x-axis is labeled by the mel frequency bin (0-128).

where Y = QY RY is the QR decomposition of Y and || · ||F is the Frobenius norm. Two Canonical Correlation Analysis (CCA) similarities are:

RC2 CA

=

||QTY QX ||2F p1

,

¯CCA

=

||QTY QX || , p1

(2)

where X = QX RX is the QR decomposition of X, p1 is the number of columns of X and || · || is

the Nuclear norm, following the notation of [12]. Singular vector CCA (SVCCA) is also calculated,

as follows:

RS2 V CCA

=

||UYT UX ||2F , min(dX , dY )

¯SV CCA

=

||UYT UX || , min(dX , dY )

(3)

where UY and UX are the left-singular vectors of Y and X, respectively, sorted in decreasing order and the first dY and dX singular vectors are selected to explain 99% of the variance, following [12].

22

That is, UY is of dimension n × dY and UX is of dimension n × dX , where the largest dY singular vectors explain 99% of the variation in Y and the largest dX singular vectors explain 99% of the variation in X.
S2.5 Computing Details
All pre-processing of the data and analysis of results is performed on CPUs. All deep models and the logistic regression models for decoding are performed on 1 GeForce RTX 2080 Ti GPU. The computation of the mel-spectrograms and all the hand-crafted features took approximately 1 hour for the Composer dataset (smallest) and 2 hours for the Beethoven dataset (largest). Extracting the deep features takes approximately 1 hour for all 5 trained and untrained initializations for each dataset and each architecture. The linear CKA calculations for comparing the last layer of the deep features for each architecture to the top hand-crafted features takes approximately 1.5 hours for the Composer dataset (smallest) and 2.5 hours for the Beethoven dataset (largest) using the linear algebra functionality in PyTorch. Other linear CKA calculations that use smaller dimensional features are faster. Exact training times for the deep architectures are given in Table 10. Training times for the classification of the deep features, as well as training times for the hand-crafted features averaged across all features for all datasets are given in Table 11; training times for the related tasks are included in the deep feature times and are similar for the hand-crafted features. To classify all layers for the NSynth dataset main and related tasks took 30065 seconds for the Regular architecture and 22278 seconds for the Deformable architecture, and to classify all layers over all initializations for the main task for the Composer dataset took 12711 seconds for the Regular architecture and 10680 seconds for the Deformable architecture. The CPUs and GPU used are on an internal cluster.

Table 10: Exact training times in seconds for 5 initializations of the 5 deep architectures for each dataset (times are rounded to the nearest second).

Regular Deformable Dilated 1dF 1dT Feed Forward

NSynth (8 Classes) 10943 18516 9825 9376 9010 6148

Composer (5 Classes) 9529 14268 9454 5061 4782 2106

Beethoven (10 Classes) 22379 14315 25315 13862 19206 7720

Table 11: Training time in seconds for the logistic regression models to classify the deep and handcrafted features. The hand-crafted features results are averaged across the training times for all of the top features. The training times are for all five initializations for each feature. Times are rounded to the nearest second and the deep feature training times include the main task and all related tasks. The times for training the related tasks for the hand-crafted features are similar to those reported here (the linear regression times for the hand-crafted features for the NSynth related tasks are very fast).

Hand-Crafted Regular Deformable Dilated 1dF 1dT

NSynth (8 Classes) 2420 2330 2493 2490 12163 17743

Composer (5 Classes) 935 2013 2249 2285
11749 12600

Beethoven (10 Classes) 2722 8440 11558 12004 18570 39184

S2.6 Code
Several existing code resources are used in this work. Much of the data pre-processing uses code from the librosa package in Python [2], https://librosa.org/doc/latest/index.html, package version 0.8.0. The wavelet features are calculated using the scipy.signal package https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.cwt.html,
23

package version 1.1.0. Loading the data into PyTorch uses code from https://discuss. pytorch.org/t/input-numpy-ndarray-instead-of-images-in-a-cnn/18797/3, and PyTorch version 1.4.0 is used throughout. The Deformable convolution implementation is from https://github.com/4uiiurz1/pytorch-deform-conv-v2, under an MIT License. Extracting the deep features after each layer uses code from https://discuss.pytorch.org/t/ how-can-l-load-my-best-model-as-a-feature-extractor-evaluator/17254/6. The logistic regression model in PyTorch uses code following https://pytorch.org/tutorials/ beginner/nlp/deep_learning_tutorial.html. Finally, information about the NSynth dataset can be found at https://magenta.tensorflow.org/datasets/nsynth#files. Code for the NSynth experiments is available at https://github.com/aky4wn/ convolutions-for-music-audio. The Composer and Beethoven experiments are analogous, with the updated architectures as shown above.

S3 What Features are Useful for the Discriminative Tasks?
Classification accuracies for all of the considered hand-crafted features on the main classification tasks, as well as the related tasks for all datasets are discussed below. Training procedures for the logistic regressions for all of the hand-crafted features are identical to those for the top features considered in the main paper. Confusion matrices for the deep architectures are also included.
S3.1 Hand-Crafted Features
Classification accuracies using the hand-crafted features described in Section S2.3 for the main classification tasks for all datasets are given in Table 12.

Table 12: Test accuracies using a logistic regression classifier for the hand-crafted features on the main classification tasks for each dataset. Mean values over five initializations of the logistic regression classifier and 1 standard error are reported. Top features included in the main paper are in bold. The wavelet features are the mean value of the wavelet coefficients over time for each frequency and the bandwidth is the number in parentheses.

Random Guessing
RMS Spectral Centroid Spectral Bandwidth Spectral Flatness Spectral Rolloff Median Power Mean Power Time to -80 dB Time to -75 dB Time to -70 dB Mean Wavelet (1) Mean Wavelet (5) Mean Wavelet (10) Mean Wavelet (25)

NSynth (8 Classes) 26.50%
53.43 ± 1.08% 47.95 ± 1.01% 55.68 ± 0.10% 42.02 ± 0.04% 48.11 ± 0.10% 58.90 ± 0.72% 66.81 ± 0.37% 56.07 ± 0.05% 57.18 ± 0.17% 56.95 ± 0.03% 54.79 ± 1.11% 61.76 ± 0.10% 62.76 ± 0.08% 60.69 ± 0.08%

Composer (5 Classes) 25.20%
26.98 ± 0.22% 32.08 ± 0.07% 30.51 ± 0.11% 31.01 ± 0.09% 31.76 ± 0.07% 28.23 ± 0.08% 27.50 ± 0.04% 33.70 ± 0.06% 34.93 ± 0.10% 35.02 ± 0.12% 51.80 ± 0.03% 58.20 ± 0.05% 59.91 ± 0.07% 62.65 ± 0.05%

Beethoven (10 Classes) 10.4%
12.96 ± 0.07% 17.75 ± 0.10% 17.94 ± 0.11% 16.10 ± 0.04% 17.54 ± 0.20% 13.13 ± 0.13% 12.93 ± 0.07% 18.65 ± 0.04% 22.29 ± 0.09% 23.85 ± 0.10% 56.38 ± 0.03% 66.94 ± 0.05% 70.37 ± 0.03% 74.14 ± 0.04%

S3.2 Wavelet Features
Wavelet features are explored for various bandwidths and various summary statistics for the 1D Ricker mother wavelet centered at 0. Wavelets are either calculated for each frequency and then a summary statistics is taken over time (results for main paper), or for each time and a summary statistic is taken over frequency. The test set accuracies for each dataset for the frequency wavelet features used in the main paper (bandwidth is 25) is given in Table 13. The mean wavelet coefficients and the combined other summary statistics give the highest test set accuracies across datasets. The frequency wavelet coefficients result in higher classification accuracies across bandwidths as compared to the
24

time-based wavelet features (Figure 8 vs. Figure 9). Additionally, the top features used in the main paper, the mean wavelet coefficients and the combined wavelet summary features, are relatively consistent in test set accuracy across bandwidths. Finally, for an additional comparison, we calculate 2D discrete wavelet transforms using the db10 and bior1.3 wavelet families from Version 0.5.2 of the PyWavelets package (https://pywavelets.readthedocs.io/en/latest/). The horizontal and vertical components of each type of wavelet are separately flattened and fed into the Logistic Regression classifier. The classification results for the Composer dataset are given in Table 14; the 2D wavelets do not improve over the 1D wavelet features considered, though again, the horizontal coefficients outperform the vertical coefficients, as seen for the 1D wavelets with the frequency summary statistics outperforming the time summary wavelet statistics.

Table 13: Test accuracies using a logistic regression classifier for the hand-crafted wavelet features on the main classification tasks for each dataset. Mean values over five initializations of the logistic regression classifier and 1 standard error are reported. Top features included in the main paper are in bold. The wavelet features are the summary statistic calculated for each frequency over time for a bandwidth of 25. The median, standard deviation, variance, kurtosis and 25th and 75th quantiles are combined to form the last row.

Random Guessing Mean Median Standard Deviation Variance Kurtosis 25th Quantile 75th Quantile Combined (No Mean)

NSynth (8 Classes) 26.50%
60.69 ± 0.08% 59.04 ± 0.06% 55.08 ± 0.06% 54.11 ± 0.14% 49.81 ± 0.12% 59.34 ± 0.06% 58.98 ± 0.08% 74.44 ± 0.18%

Composer (5 Classes) 25.20%
62.65 ± 0.05% 48.05 ± 0.05% 42.56 ± 0.10% 41.96 ± 0.14% 34.08 ± 0.25% 47.94 ± 0.10% 37.40 ± 0.10% 59.56 ± 0.15%

Beethoven (10 Classes) 10.4%
74.14 ± 0.04% 46.44 ± 0.04% 33.19 ± 0.09% 33.63 ± 0.05% 25.69 ± 0.10% 48.52 ± 0.06% 31.40 ± 0.05% 66.32 ± 0.05%

Figure 8: Test accuracy for the main Composer 5-class classification task for wavelet features calculated for each frequency, then a summary statistic is taken over time, for several different bandwidths. The different colors are each of the different summary statistics. The main features used, mean coefficients and combined, are relatively robust in accuracy for different bandwidths.
S3.3 Related Tasks
The deep and hand-crafted features are also used for several related tasks for each dataset. For the NSynth dataset, the related tasks are predicting the Note Pitch and the Note Velocity. Both of these are linear regression tasks and the root mean squared error (RMSE) is reported for the hand-crafted features in Table 16 and Table 17. The linear regression is performed once for each input feature and does not have error bars. The related task for the Composer dataset is classifying which part of the piece (beginning, middle or end) each mel-spectrogram comes from. Finally, the related tasks for the
25

Figure 9: Test accuracy for the main Composer 5-class classification task for wavelet features calculated for each time, then a summary statistic is taken over frequencies, for several different bandwidths. The different colors are each of the different summary statistics.

Table 14: Test accuracy for 2D wavelets using either the horizontal or vertical coefficients on the Composer dataset. These 2D coefficients do not improve over the summary statistics of the 1D wavelets.

Horizontal

Vertical

bior1.3 44.90 ± 0.35 25.68 ± 0.20

db10

43.69 ± 0.70 25.06 ± 0.51

Beethoven dataset are classifying the number of the symphony of each mel-spectrogram (10 classes) and whether each example is by an American or European orchestra. Results for the Composer and Beethoven datasets are shown in Table 18 and Table 19. All classification results are again using the same logistic regression procedure as for the main classification tasks. Top features included in the main paper are in bold throughout and the wavelet features especially perform well in the related tasks, and can even outperform some of the deep features (Table 15). As seen in the NSynth results, the deep features trained for one task perform well when used for another task, that is, the deep features in Table 15 achieve similar accuracies to Regular CNNs trained on each related task directly (displayed in the last row of Table 15).
S3.4 Confusion Matrices for Deep Features
Confusion matrices for the deep architectures overall for the Composer dataset are given in Figure 10. In general, Haydn and Beethoven are mistaken for each other the most, which makes sense from a musical style perspective as these composers were closest to each other in time and were both from the Classical era. Confusion matrices for the Regular (Figure 11) and Deformable architectures (Figure 12) for the Beethoven dataset are also shown. LSO and Vienna tend to be confused with each other the most; this also makes sense, as these are the two most recent recordings considered and are both by European orchestras with similar performance styles.
S4 Are the Learned Deep Features Similar to the Hand-Crafted Features?
S4.1 Similarity by Architecture
We first compare the similarity between the deep features from the last convolutional layer (conv3) across architectures to all of the hand-crafted features for the NSynth (Figure 13), Composer (Figure 14) and Beethoven (Figure 15) datasets. We again find that the 1dT features are especially similar to the mean wavelet features, by design. We can confirm this similarity by concatenating the deep Deformable features to the top hand-crafted feature (Table 21). Compared to the Regular deep
26

Table 15: Test RMSE for the related tasks of note pitch prediction and note velocity (volume) prediction for the NSynth dataset and test accuracies using a logistic regression classifier for the hand-crafted features on the related classification tasks for the Composer (part of piece, 3-class) and Beethoven (symphony number, 9-class and continent of orchestra, 2-class) datasets. The baseline RMSE is predicting the mean value on the training data for all test examples. The hand-crafted features are the top features included in the main paper. The last row is a regular CNN trained directly on each related task. (Lower values of RMSE and higher classification accuracies are better).

Baseline / Random Guessing Mean Power Time to -70 dB Mean Wavelet (25) Wavelet Combined Top 4 Combined Regular Deformable Dilated 1dF 1dT Regular (Trained)

NSynth

Note Pitch Note Velocity

(RMSE)

(RMSE)

21.00

36.03

18.97 16.01 14.87 12.57 11.57 9.52 ± 0.11 8.17±0.31 9.25 ± 0.17 62.18 ± 5.71 37.06 ± 2.80 7.84 ± 0.11

34.68 35.34 35.37 34.52 33.53 32.74 ± 0.10 31.93 ± 0.07 33.16 ± 0.15 247.07 ± 31.06 125.23 ± 5.29 32.33 ± 0.10

Composer Part of Piece (3-class)
33.5%
37.64 ± 0.09% 35.85 ± 0.14% 43.48±0.09% 40.59 ± 0.24% 41.34±0.35% 40.33 ± 0.32% 40.43 ± 0.46% 40.13 ± 0.43% 40.73 ± 0.35% 40.54 ± 0.53% 38.63 ± 0.44%

Beethoven

Symphony #

Orchestra

(9-class)

Continent

(2-class)

19.5%

59.5%

20.57 ± 0.07% 24.74 ± 0.13% 53.07 ± 0.08% 50.47 ± 0.11% 53.61 ± 0.24% 47.41 ± 0.30% 49.29 ± 0.65% 51.51 ± 0.52% 49.89 ± 0.56% 58.45 ± 0.73% 62.04 ± 1.37%

58.53 ± 0.03% 63.09 ± 0.01% 79.79 ± 0.01% 76.87 ± 0.02% 80.39 ± 0.19% 85.11 ± 0.44% 86.02 ± 0.74% 82.65 ± 0.28% 80.34 ± 0.57% 86.62 ± 1.17% 91.41 ± 0.33%

Table 16: Test RMSE for the related tasks of note pitch prediction and note velocity (volume) prediction for the NSynth dataset. The baseline RMSE is predicting the mean value on the training data for all test examples. Top features included in the main paper are in bold. The wavelet features are the mean value of the wavelet coefficients over time for each frequency and the bandwidth is the number in parentheses. (Lower values are better).

Baseline (Mean)
RMS Spectral Centroid Spectral Bandwidth Spectral Flatness Spectral Rolloff Median Power Mean Power Time to -80 dB Time to -75 dB Time to -70 dB Mean Wavelet (1) Mean Wavelet (5) Mean Wavelet (10) Mean Wavelet (25)

Note Pitch RMSE 21.000
19.886 14.647 19.079 20.069 16.658 19.636 18.972 16.853 16.427 16.007 14.528 13.139 13.599 14.866

Note Velocity RMSE 36.025
35.858 35.122 34.519 35.507 34.960 35.020 34.676 35.423 35.320 35.340 34.826 34.889 35.097 35.365

27

Table 17: Test RMSE for the related tasks of note pitch prediction and note velocity (volume) prediction for the NSynth dataset. The baseline RMSE is predicting the mean value on the training data for all test examples. Top features included in the main paper are in bold. The wavelet features are the summary statistic calculated for each frequency over time for a bandwidth of 25. The median, standard deviation, variance, kurtosis and 25th and 75th quantiles are combined to form the last row.

Baseline (Mean) Mean Median Standard Deviation Variance Kurtosis 25th Quantile 75th Quantile Combined (No Mean)

Note Pitch RMSE 21.000 14.866 15.118 16.477 16.561 19.224 16.792 15.190 12.565

Note Velocity RMSE 36.025 35.365 35.381 35.603 35.610 35.507 35.373 35.528 34.520

Table 18: Test accuracies using a logistic regression classifier for the hand-crafted features on the related classification tasks for the Composer (part of piece, 3-class) and Beethoven (symphony number, 9-class and continent of orchestra, 2-class) datasets. Mean values over five initializations of the logistic regression classifier and 1 standard error are reported. Top features included in the main paper are in bold. The wavelet features are the mean value of the wavelet coefficients over time for each frequency and the bandwidth is the number in parentheses.

Random Guessing
RMS Spectral Centroid Spectral Bandwidth Spectral Flatness Spectral Rolloff Median Power Mean Power Time to -80 dB Time to -75 dB Time to -70 dB Mean Wavelet (1) Mean Wavelet (5) Mean Wavelet (10) Mean Wavelet (25)

Composer Part of Piece (3-class)
33.5% 37.94 ± 0.11% 35.61 ± 0.13% 35.72 ± 0.09% 37.75 ± 0.01% 36.04 ± 0.12% 36.94 ± 0.084% 37.64 ± 0.09% 35.14 ± 0.11% 35.61 ± 0.32% 35.85 ± 0.14% 41.94 ± 0.06% 43.25 ± 0.08% 43.05 ± 0.08% 43.48 ± 0.09%

Beethoven

Symphony # (9-class) Orchestra Continent

(2-class)

19.5%

59.5%

19.75 ± 0.15% 59.07 ± 0.03%

19.75 ± 0.09% 63.03 ± 0.02%

19.22 ± 0.14% 60.99 ± 0.03%

19.29 ± 0.06% 61.83 ± 0.02%

19.43 ± 0.17% 63.39 ± 0.01%

20.53 ± 0.07% 58.50 ± 0.02%

20.57 ± 0.07% 58.53 ± 0.03%

21.11 ± 0.07% 58.96 ± 0.04%

22.34 ± 0.06% 62.23 ± 0.03%

24.74 ± 0.13% 63.09 ± 0.01%

43.91 ± 0.05% 73.71 ± 0.01%

48.47 ± 0.07% 76.95 ± 0.01%

50.62 ± 0.08% 78.23 ± 0.01%

53.07 ± 0.08% 79.79 ± 0.01%

28

Table 19: Test accuracies using a logistic regression classifier for the hand-crafted features on the related classification tasks for the Composer (part of piece, 3-class) and Beethoven (symphony number, 9-class and continent of orchestra, 2-class) datasets. Mean values over five initializations of the logistic regression classifier and 1 standard error are reported. Top features included in the main paper are in bold. The wavelet features are the summary statistic calculated for each frequency over time for a bandwidth of 25. The median, standard deviation, variance, kurtosis and 25th and 75th quantiles are combined to form the last row.

Random Guessing Mean Median Standard Deviation Variance Kurtosis 25th Quantile 75th Quantile Combined (No Mean)

Composer Part of Piece (3-class)
33.5% 43.48 ± 0.09% 38.77 ± 0.03% 37.85 ± 0.05% 37.52 ± 0.01% 36.19 ± 0.12% 39.46 ± 0.03% 36.73 ± 0.02% 40.59 ± 0.24%

Beethoven

Symphony # (9-class) Orchestra Continent

(2-class)

19.5%

59.5%

53.07 ± 0.08% 79.79 ± 0.01%

39.99 ± 0.09% 69.24 ± 0.00%

30.85 ± 0.07% 65.28 ± 0.00%

30.24 ± 0.10% 65.42 ± 0.00%

25.32 ± 0.07% 63.61 ± 0.00%

38.43 ± 0.06% 70.16 ± 0.00%

30.58 ± 0.08% 64.23 ± 0.00%

50.47 ± 0.11% 76.87 ± 0.02%

(a) Regular

(b) Deformable

(c) Dilated

(d) 1dF

(e) 1dT

(f) Mean Wavelet (25)

Figure 10: Confusion matrices for all of the deep architectures and the mean wavelet coefficients (bandwidth = 25) with logistic regression for the Composer dataset on the test set. The counts are divided by the overall number of test examples. In general, Haydn and Beethoven are mistaken for each other the most, which makes sense from a musical style perspective. The confusion matrices of each architecture appear similar.

29

Figure 11: Confusion matrices on the test set for the Beethoven dataset for the Regular deep architecture. The counts on the diagonal are set to 0 for display purposes.
Figure 12: Confusion matrices on the test set for the Beethoven dataset for the Deforamble deep architecture. The counts on the diagonal are set to 0 for display purposes.
features, the Deformable deep features are more similar to the hand-crafted features, and indeed, we see less improvement when these hand-crafted features are concatenated to the Deformable deep features in Table 21 vs Table 20. However, the Deformable deep features are less similar to the Top 4 Combined features, and the last row of Table 21 indicates that when combined with the Deformable deep features, these hand-crafted features can further improve the classification accuracy. S4.2 Similarity by Layer We next compare the similarity between the deep features from all layers to all of the hand-crafted features for the NSynth (Figure 16) and Composer (Figure 19) datasets for the Regular and Deformable convolutions. The NSynth similarities trends hold whether the deep features are extracted from architectures trained to predict Instrument Family (Figure 16), Note Pitch (Figure 17) or Note Velocity (Figure 18), confirming the robustness of the deep features to multiple tasks. S4.3 Additional Similarities We also look at the Linear Regression similarity defined in [12] and Equation 1 for all datasets. The results across architectures again show high similarity between the deep features and the hand-crafted features, though there are differences to the plots with Linear CKA as the similarity (Figure 20), and similarly when comparing deep features across layers (Figure 21). Linear CKA is found to be the
30

Figure 13: Linear CKA similarity between the deep features from each architecture from the last convolutional layer (conv3) with all hand-crafted features for the NSynth dataset. We baseline with Random N (0, 1) noise in the last column. Plots are the similarity value averaged across the deep features from each initialization.
Figure 14: Linear CKA similarity between the deep features from each architecture from the last convolutional layer (conv3) with all hand-crafted features for the Composer dataset. We baseline with Random N (0, 1) noise in the last column. Plots are the similarity value averaged across the deep features from each initialization.
Figure 15: Linear CKA similarity between the deep features from each architecture from the last convolutional layer (conv3) with all hand-crafted features for the Beethoven dataset. We baseline with Random N (0, 1) noise in the last column. Plots are the similarity value averaged across the deep features from each initialization.
31

(a) Regular
(b) Deformable Figure 16: Linear CKA similarity between the deep features from all layers for the NSynth dataset and all hand-crafted features for the (a) Regular and (b) Deformable deep features (trained to predict Instrument Family). We baseline with Random N (0, 1) noise in the last column. Plots are the similarity value averaged across the deep features from each initialization.
(a) Regular
(b) Deformable Figure 17: Linear CKA similarity between the deep features from all layers for the NSynth dataset and all hand-crafted features for the (a) Regular and (b) Deformable deep features trained to predict Note Pitch.
32

Table 20: Regular convolution deep features are concatenated to each top hand-crafted feature and classified with logistic regression on the main classification tasks for each dataset (number in parentheses for each dataset is the number of classes). Hand-crafted features that are less similar to the Regular deep features (Figure 20), especially the Wavelet Combined and Top Combined features improve the classification accuracy when combined with the deep Regular features.

Random Guessing Deep Features conv3 Deep + Mean Power Deep + Time to -70 dB Deep + Mean Wavelet (25) Deep + Wavelet Combined Deep + Top 4 Combined

NSynth (8) 26.50%
96.35 ± 0.19% 96.36 ± 0.20% 96.41 ± 0.18% 96.50 ± 0.18% 96.59 ± 0.20% 96.61 ± 0.17%

Composer (5) 25.20%
72.41 ± 1.21% 72.35 ± 1.26% 72.85 ± 1.19% 73.24 ± 1.09% 74.04 ± 0.87% 74.71 ± 0.78%

Beethoven (10) 10.4%
82.32 ± 0.40% 82.35 ± 0.34% 82.60 ± 0.45% 83.58 ± 0.50% 83.44 ± 0.42% 84.23 ± 0.31%

Table 21: Deformable convolution deep features are concatenated to each top hand-crafted feature and classified with logistic regression on the main classification tasks for each dataset (number in parentheses for each dataset is number of classes). Hand-crafted features that are less similar to the Deformable deep features, especially the Top Combined features improve the classification accuracy when combined with the deep Deformable features.

Random Guessing Deep Features conv3 Deep + Mean Power Deep + Time to -70 dB Deep + Mean Wavelet (25) Deep + Wavelet Combined Deep + Top 4 Combined

NSynth (8) 26.50%
97.80 ± 0.08% 97.75 ± 0.09% 97.81 ± 0.05% 97.79 ± 0.06% 97.84 ± 0.12% 97.87 ± 0.09%

Composer (5) 25.20%
75.76 ± 0.68% 75.92 ± 0.85% 76.23 ± 0.65% 76.42 ± 0.79% 77.23 ± 0.44% 77.72 ± 0.67%

Beethoven (10) 10.4%
84.11 ± 0.70% 84.17 ± 0.79% 84.49 ± 0.72% 85.11 ± 0.66% 85.10 ± 0.80% 85.70 ± 0.64%

(a) Regular
(b) Deformable Figure 18: Linear CKA similarity between the deep features from all layers for the NSynth dataset and all hand-crafted features for the (a) Regular and (b) Deformable deep features trained to predict Note Velocity.
33

(a) Regular
(b) Deformable Figure 19: Linear CKA similarity between the deep features from all layers for the Composer dataset and all hand-crafted features for the (a) Regular and (b) Deformable deep features. We baseline with Random N (0, 1) noise in the last column. Plots are the similarity value averaged across the deep features from each initialization. best similarity measure by [12], which is why it is the focus of the main paper, though the similarities are expected to change with the similarity measure used.

(a) NSynth

(b) Composer

(c) Beethoven

Figure 20: Linear Regression similarity between the deep features from each architecture from the last convolutional layer (conv3) with the top hand-crafted features for the (a) NSynth, (b) Composer and (c) Beethoven datasets. We baseline with Random N (0, 1) noise in the last column. Plots are the similarity value averaged across the deep features from each initialization.

Finally, for the Composer dataset, we compare the Linear CKA similarity used in the main paper to the linear regression, RC2 CA, ¯CCA, RS2 V CCA and ¯SV CCA similarities defined in Equations 2 and 3 above. Again, these results confirm the findings in [12]: the linear regression and SVCCA similarity
measures give similar results to the Linear CKA measure, while the CCA similarities appear less
useful overall. Similarities are calculated across architectures for conv3 (Figure 22), and for all
layers for the Regular (Figure 23) and Deformable (Figure 24) architectures.

34

(a) Regular

(b) Deformable

Figure 21: Linear Regression similarity between the deep features from each layer for the NSynth dataset for the (a) Regular and (b) Deformable architectures. The earlier layers exhibit very high similarity with the hand-crafted features.

(a) Linear CKA

(b) RC2 CA

(c) RS2 V CCA

(d) Linear Regression, RL2 R

(e) ¯CCA

(f) ¯SV CCA

Figure 22: Similarity measures between the deep features from each architecture from the last

convolutional (c) RS2 V CCA,

layer (conv3) with the top hand-crafted features for the (a) Linear (d) Linear Regression, RL2 R, (e) ¯CCA and (f) ¯SV CCA similarities

CKA, for the

(b) RC2 CA, Composer

dataset. We baseline with Random N (0, 1) noise in the last column. Plots are the similarity value

averaged across the deep features from each initialization.

35

(a) Linear CKA

(b) RC2 CA

(c) RS2 V CCA

(d) Linear Regression, RL2 R

(e) ¯CCA

(f) ¯SV CCA

Figure 23: Similarity measures between the deep features from each layer for the Regular architecture

with the top hand-crafted features Regression, RL2 R, (e) ¯CCA and (f)

for the (a) Linear CKA, (b) RC2 CA, (c) RS2 V CCA, ¯SV CCA similarities for the Composer dataset.

(d)

Linear

(a) Linear CKA

(b) RC2 CA

(c) RS2 V CCA

(d) Linear Regression, RL2 R

(e) ¯CCA

(f) ¯SV CCA

Figure 24: Similarity measures between the deep features from each layer for the Deformable architecture with the top hand-crafted features for the (a) Linear CKA, (b) RC2 CA, (c) RS2 V CCA, (d) Linear Regression, RL2 R, (e) ¯CCA and (f) ¯SV CCA similarities for the Composer dataset.

36

S5 Do Untrained Convolutional Architectures Have a Useful Inductive Bias for Music Audio Discriminative Tasks?
We first compare the trained features from each initialization to the untrained features in terms of Linear CKA similarity for the Regular (Figure 25) and Deformable (Figure 26) architectures across datasets.

(a) NSynth

(b) Composer

(c) Beethoven

Figure 25: Linear CKA similarity between untrained and trained features from the last convolutional layer (conv3) of the Regular architectures by initialization for the (a) NSynth, (b) Composer and (c) Beethoven datasets. Across initializations, the untrained features are similar to the trained features, though not identical.

(a) NSynth

(b) Composer

(c) Beethoven

Figure 26: Linear CKA similarity between untrained and trained features from the last convolutional layer (conv3) of the Deformable architectures by initialization for the (a) NSynth, (b) Composer and (c) Beethoven datasets. Across initializations, the untrained features are similar to the trained features, though not identical.

We additionally compare the untrained deep features by architecture to all of the hand-crafted features for the NSynth (Figure 27), Composer (Figure 28) and Beethoven (Figure 29) datasets, again finding that the untrained features from the last convolutional layer for all architectures are highly similar to the hand-crafted features, especially the wavelet features. Additionally, across layers the untrained deep features are very similar to the hand-crafted wavelet features for the NSynth (Figure 30) and Composer (Figure 31) datasets.
Finally, we can compare the trained features by layer to the untrained features for the NSynth dataset for the Regular and Deformable architectures (Figure 32). The middle layers tend to be highly similar to each other, whether trained or not, while the last convolutional layer (conv3) is the least similar to the other layers and to itself before and after training.

S6 How do Different Architectures, Layers and Initializations Differ in their Learned Deep Features?
In this section, we use the Linear CKA similarity measure to compare learned deep features across different architectures, layers, initializations and channels.

37

Figure 27: Linear CKA similarity between the untrained deep features from each architecture from the last convolutional layer (conv3) with all hand-crafted features for the NSynth dataset. We baseline with Random N (0, 1) noise in the last column. Plots are the similarity value averaged across the deep features from each initialization.
Figure 28: Linear CKA similarity between the untrained deep features from each architecture from the last convolutional layer (conv3) with all hand-crafted features for the Composer dataset. We baseline with Random N (0, 1) noise in the last column. Plots are the similarity value averaged across the deep features from each initialization.
Figure 29: Linear CKA similarity between the untrained deep features from each architecture from the last convolutional layer (conv3) with all hand-crafted features for the Beethoven dataset. We baseline with Random N (0, 1) noise in the last column. Plots are the similarity value averaged across the deep features from each initialization.
S6.1 Initializations We calculate the Linear CKA similarity between the learned features for the last convolutional layer (conv3) for different initializations for all datasets for the Regular (Figure 33) and Deformable architectures (Figure 34). Similar to the findings of [13, 29], the deep features for different initializations are highly similar, but not identical, which reinforces the need to consider multiple initializations when exploring and using deep features.
38

(a) Regular
(b) Deformable Figure 30: Linear CKA similarity between the untrained deep features from all layers for the NSynth dataset and all hand-crafted features for the (a) Regular and (b) Deformable deep features. We baseline with Random N (0, 1) noise in the last column. Plots are the similarity value averaged across the deep features from each initialization.
We also explore the similarity between initializations for different layers for the NSynth dataset for the Regular (Figure 35) and Deformable (Figure 36) architectures. In general, the deep features for layers conv2 and pool2 are very similar to each other, while the learned features differ more for the earlier and later layers.
S6.2 Architectures
We also compare the similarity of the deep features from the last convolutional layer (conv3) between architectures, averaged over initializations, and find that the 1dT deep features tend to be the least similar to the features from the other architectures (Figure 37). The Regular features are similar to the Deformable and Dilated features, while the Deformable deep features are less similar to the Dilated and 1dF features than the Regular features are.
S6.3 Channel Similarity
For computational reasons, for the main layer experiments, we average the features over channels for layers conv1 and pool1. This reduces the dimensionality of the learned features at these layers by a factor of 10 and allows for the matrix multiplications necessary for the Linear CKA calculations and for the logistic regression decoding experiments to occur in a reasonable time. The dimensions of the learned features are each layer for the NSynth dataset are: conv1-(10, 124, 124), pool1 - (10, 62, 62), conv2 - (20, 22, 22), pool2 - (20, 11, 11) and conv3 - (30, 4, 4); averaging across channels reduces the dimensionality of the conv1 features from n × 153760 to n × 15376. This is reasonable, as except for one or two channels, the features across channels exhibit very high similarity for layers conv1 and pool1 (Figure 38). The later layers, however, are of smaller dimension and exhibit less similarity between channels (Figure 39), so we flatten over channels for the experiments using deep features from the conv2, pool2 and conv3 layers.
39

(a) Regular
(b) Deformable Figure 31: Linear CKA similarity between the untrained deep features from all layers for the Composer dataset and all hand-crafted features for the (a) Regular and (b) Deformable deep features. We baseline with Random N (0, 1) noise in the last column. Plots are the similarity value averaged across the deep features from each initialization.

(a) Regular

(b) Deformable

Figure 32: Linear CKA similarity between the untrained deep features from all layers and the trained deep features for the NSynth dataset and all hand-crafted features for the (a) Regular and (b) Deformable deep features.

S6.4 Layer Similarity
Finally, we compare the Linear CKA similarity of the deep features by layer for the Regular and Deformable architectures for the NSynth dataset to explore how learned features compare across layers within the same architecture (Figure 40) . The middle layers, conv2 and pool2, tend to be most similar to all other layers, while the earlier and later layers exhibit less similarity to each other. We also compare the deep features by layer for the Regular architecture to the deep features by layer for the Deformable architecture for the NSynth data (Figure 41). As expected, the actual Deformable convolution layer (conv3) tends to have the least similarity with the same layer in the Regular architecture, as well as the earlier layers in the Regular architecture. In general, all convolutional
40

(a) NSynth

(b) Composer

(c) Beethoven

Figure 33: Linear CKA similarity between initializations for Regular deep features from the last convolutional layer for the (a) NSynth, (b) Composer and (c) Beethoven datasets. Deep features across initializations are highly similar, but not identical.

(a) NSynth

(b) Composer

(c) Beethoven

Figure 34: Linear CKA similarity between initializations for Deformable deep features from the last convolutional layer for the (a) NSynth, (b) Composer and (c) Beethoven datasets. Deep features across initializations are highly similar, but not identical.

(a) conv1

(b) pool1

(c) conv2

(d) pool2

(e) conv3

Figure 35: Linear CKA similarity between initializations for the deep features from each layer for the Regular architectures on the NSynth dataset.

layers tend to differ more from each other between the Regular and Deformable architectures than the pooling layers do.

41

(a) conv1

(b) pool1

(c) conv2

(d) pool2

(e) conv3

Figure 36: Linear CKA similarity between initializations for the deep features from each layer for the Deformable architectures on the NSynth dataset.

(a) NSynth

(b) Composer

(c) Beethoven

Figure 37: Linear CKA similarity between architectures for deep features from the last convolutional layer for the (a) NSynth, (b) Composer and (c) Beethoven datasets. The similarity is averaged over initializations.

(a) conv1

(b) pool1

Figure 38: Linear CKA similarity between channels for one initialization of the Regular deep features on the NSynth dataset for the (a) conv1 and (b) pool1 layers. In general, the deep features are very similar across channels.

42

(a) conv2

(b) pool2

(c) conv3

Figure 39: Linear CKA similarity between channels for one initialization of the Regular deep features on the NSynth dataset for the (a) conv2, (b) pool2 and (c) conv3 layers. In general, the deep features are very similar across channels.

(a) Regular

(b) Deformable

Figure 40: Linear CKA similarity between layers for the (a) Regular and (b) Deformable architectures trained on the NSynth dataset. The similarity is averaged over initializations.

Figure 41: Linear CKA similarity between layers from the Regular architecture with layers from the Deformable architecture on the NSynth dataset. The similarity is averaged over initializations.
43

