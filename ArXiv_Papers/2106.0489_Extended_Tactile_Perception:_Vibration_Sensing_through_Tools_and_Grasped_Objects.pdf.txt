Extended Tactile Perception: Vibration Sensing through Tools and Grasped Objects

Tasbolat Taunyazov°, Luar Shui Song°, Eugene Lim°, Hian Hian See:, David Lee:;, Benjamin C.K. Tee:;, and Harold Soh°
°Dept. of Computer Science, National University of Singapore :Dept. of Materials Science and Engineering, National University of Singapore ;Institute for Health Technology and Innovation, National University of Singapore
Email: tasbolat@comp.nus.edu.sg, luarss@comp.nus.edu.sg, elimwj@nus.edu.sg mseshh@nus.edu.sg,
david.leekh@nus.edu.sg, benjamin.tee@nus.edu.sg, harold@comp.nus.edu.sg

arXiv:2106.00489v1 [cs.RO] 1 Jun 2021

Abstract--Humans display the remarkable ability to sense the world through tools and other held objects. For example, we are able to pinpoint impact locations on a held rod and tell apart different textures using a rigid probe. In this work, we consider how we can enable robots to have a similar capacity, i.e., to embody tools and extend perception using standard grasped objects. We propose that vibro-tactile sensing using dynamic tactile sensors on the robot fingers, along with machine learning models, enables robots to decipher contact information that is transmitted as vibrations along rigid objects. This paper reports on extensive experiments using the BioTac micro-vibration sensor and a new event dynamic sensor, the NUSkin, capable of multitaxel sensing at 4 kHz. We demonstrate that fine localization on a held rod is possible using our approach (with errors less than 1 cm on a 20 cm rod). Next, we show that vibro-tactile perception can lead to reasonable grasp stability prediction during object handover, and accurate food identification using a standard fork. We find that multi-taxel vibro-tactile sensing at sufficiently high sampling rate (above 2 kHz) led to the best performance across the various tasks and objects. Taken together, our results provides both evidence and guidelines for using vibrotactile perception to extend tactile perception, which we believe will lead to enhanced competency with tools and better physical human-robot-interaction.
I. INTRODUCTION
Our proficiency with our hands is crucially supported by our sense of touch. Mechanoreceptors in our skin enable us to finely perceive tactile information, which is invaluable when directly manipulating objects and when using tools. Indeed, humans can accurately localize contact not only on our skin, but also on grasped objects [30], and are able to discriminate textures through rigid links [22]. Unlike a majority of tactile perception, which has focussed on determining properties of grasped objects, we are inspired by our remarkable ability to sense the world through tools and other held items.
In this work, we seek to extend robot tactile perception by using standard objects (e.g., a stick or a fork) and without the use of mounted accelerometers. Prior work has demonstrated that contact events -- whether/when a contact occurred on a held object -- can be reliably detected with existing sensors. For example, tactile signals have been used to trigger motions during human-robot handover [46] and to detect when a held soda-can has been placed on a table [34]. Here, we go beyond existing research and ask: can we decipher the properties of

A.

Robot Gripper

Dynamic Tactile Sensor

B.
Signal Capture
(e.g., spikes, AC pressure)

Vibrations travel up the
rod

Feature Extraction
(e.g., spike-counts, FFT)
Regression
(e.g., SVR, MLP)

C.

Tap position

Prediction

Fig. 1: We study Extended Tactile Perception -- our goal is to enable robots to extend their tactile perception through standard objects such as tools. (A) We show that robots are able to accurately localize taps on an acrylic rod using fast vibro-tactile sensing and machine learning. Vibrations caused by the tap travel up the rod where they are picked up by a dynamic tactile sensor (the NUSkin in this image). The signal is captured and mapped into a tap position using simple models and learned features. We provide results on two additional tasks: (B) grasp stability classification during object handover and (C) food classification through a fork.

such contacts? Where on a held tool's surface did the contact occur? What type of object did the tool make contact with?
Addressing this challenge is important for enabling competency with tools -- sensing via tools can help robots determine the properties of objects that are out of reach, or that are too difficult or dangerous to interact with directly. In humans, recent evidence suggests vibration sensing via fast-adapting (FA) somatic sensory receptors and neural processing of "vibratory motifs" underlies our capacity to embody tools and extend perception beyond our body [30]. Likewise, we hypothesize that extended tactile perception (beyond the boundary of the robot) may be achieved using a combination of high-frequency vibrotactile sensing via artificial skin on the robot and processing via statistical learning or neural models (Fig. 1).
A key feature of our work is that we only use compact tactile sensors on the robot gripper, rather than speciallycrafted sensorized tools (e.g., [1, 33, 41]). We propose a

system that senses and learns to decode contact information that is transmitted as vibrations (or movements) along standard rigid objects, such as a rod/fork/spoon or a handled item such as a plate. Accurately detecting these vibrations requires a sensitive dynamic tactile sensor with high frequency response. In this work, we experiment with the BioTac, specifically its hydrophone pressure sensor, which has a frequency response that exceeds human mechanoreceptors. However, the BioTac currently has one such sensor. We contribute NUSkin; a 40-taxel event sensor based on NeuTouch [44] that has been augmented to have a higher 4 kHz sampling rate. Unlike the BioTac, the NUSkin taxels can fire asynchronously -- each taxel generates positive or negative spikes depending on changes in the contact pressure.
To interpret the vibro-tactile signals, we adopt a learning approach. However, with high sampling rates, the collected data comprise long sequences that can be difficult to learn from. This problem is exacerbated in data constrained settings such as tactile sensing where data collection is relatively costly. Moreover, the event sensor only outputs sparse spike trains (not pressure values) that can be difficult to handle in standard machine learning methods. We find that recent neural models achieved the best performance in our tests, but simple (binned/smoothed) spike-count and FFT features together with "classical" statistical learning methods also worked well.
Experiments show that by enabling robots to quickly perceive and make sense of vibratory features, they are also capable of accurately localizing contact on a grasped rod -- on a 20 cm rod, the localization error was only «1 cm. Further experiments show that our approach can be applied to tasks such as stablegrasp detection during human-robot handover (where static force detection is usually applied), and food classification for robot feeding. Taken together, our results indicate that with fine vibro-tactile perception, a robot can extend its capacity to sense the world, akin to having "virtual sensors" on the held objects.
To summarize, our key contributions are:
, A novel method of vibration perception that enables robots to extend tactile perception through everyday objects;
, The NUSkin, a modification of a recently proposed event tactile sensor capable of higher sampling rates;
, Experimental results that validate the extended tactile perception system on a real-world robot, along with findings that can impact future designs.
In addition, we have made our datasets publicly available1; they comprise the tactile signals captured during our experiments and ground-truth annotations, which we hope will facilitate future work on extended tactile perception. We have focussed primarily on vibrations in this work, and a plausible next step is examine multiple sensors in multi-modal framework. More broadly, we believe this paper brings us closer towards general tool embodiment for robotics.
1https://github.com/clear-nus/ext-sense.

II. BACKGROUND AND RELATED WORK
Tactile perception in robotics is a broad research area that spans sensing, learning, and control. Here, we give a brief overview of closely-related work and refer interested readers to survey articles [27, 46, 36, 20] for more a comprehensive information. On the whole, prior work has mainly focused on perceiving properties of objects that are in direct contact with the sensor. In contrast, we aim to perceive the properties of interactions that occur on the held/touched object.
Dynamic Tactile Sensors. There has been significant progress on tactile sensing and a range of sensors -- from piezoelectric to optical [48] -- have been developed over the years [20, 46]. Here, we focus on dynamic tactile sensing, i.e., the measurement of rapidly changing tactile information. Examples of dynamic tactile sensors include accelerometers [12] and robot whiskers [40]. The latter is notable since the whiskers themselves do not possess tactile sensing elements. Rather, deformations of the whiskers (sensed via the whisker's torque and angle at the base) can be used to sense the presence and shape of objects.
Dynamic tactile sensing has also been performed using attached sensors or specially-designed tools. For example, notable work showed that vibrations (captured using an accelerometer or contact mic mounted on a container) obtained by shaking a vessel can reveal the type and approximate number of items within [7]. Prior work has devised haptic recording tools that employ accelerometers, microphones, and force sensors [33, 41] to accurately discriminate textures, and in [1], a dinner fork instrumented with a 6-axis force/torque sensor (the "Forque") was used to perform food classification.
In this work, we focus on the more general setting where dynamic tactile sensing is performed using sensors on the robot fingers. We work with two modern compact sensors: the SynTouch BioTac and the NUSkin, a new variant of the NeuTouch [44]. In addition to impedance sensing electrodes, the BioTac has a hydro-acoustic pressure sensor that senses vibrations that propagate through the skin and incompressible conductive fluid in the finger [14]. The sensor is capable of detecting micro-vibrations (frequency response up to 1040 Hz) and can discriminate textures better than humans [13]. A multitaxel alternative is the NeuTouch, which uses a piezoresistive graphene transducer with 39 taxels. Unlike the BioTac, the NeuTouch transmits spikes, which are generated by changes in pressure. These two sensors operate differently and comparing them in our experiments allowed us to examine different sensing modes, the role of multiple taxels, and type of transmitted data. In this work, we developed a modified version of the NeuTouch with a higher sampling rate of 4kHz (described in Sec. IV). Our methodology could be applied to other sensors that are fast and sensitive enough to detect vibrations on the grasped object.
Features for tactile-sensing. To make sense of vibration and tactile signals, prior work has used a variety of features -- from handcrafted [49, 32, 41] and statistical descriptors [39, 38] to

features learned via dictionary methods [35, 28, 26, 7, 25] and neural networks [27, 18]. In addition to raw temporal data, we work with a variety of features: Fast Fourier Transform (FFT) [4] features, learnt neural representations via autoencoding [23] and the recently proposed Event Spike Tensor [15].

Applications of Vibro-Tactile Signals. Within the context of robotics, tactile sensing is widely recognized as important for dexterous manipulation [2] and a variety of robot tasks (e.g., grasping [34, 6], slip-detection [9, 44], handovers [16, 17], and robot feeding [1]). However, prior work has focused on identifying contact events or properties of the grasped object (e.g., its texture [43], identity [1, 39, 38], or even its contents [7, 29]). In this work, we focus on decoding vibratory signals that are transferred through the held/touched object. Vibration analysis has long been applied in numerous fields, e.g., in the monitoring and diagnosis of machines, but the precise problem in this paper -- the interpretation of vibrations through grasped tools -- remains relatively unexplored in robotics.
III. PROBLEM STATEMENT: EXTENDED TACTILE PERCEPTION
In this work, our overarching goal is to accurately perceive the properties of contacts on a held/touched rigid object; a problem that we call extended tactile perception.
Consider a simple straight rod that has touched another object, say a piece of fruit. Information about point(s) of contact and properties of the fruit (e.g., softness, roughness) is transmitted along the rod as mechanical transients [11, 30]. The vibrations arising from an impulse along the rod can be modeled using partial differential equations (PDEs) from EulerBernoulli Beam theory [45]. However, finding solutions to the PDE is difficult in general and requires precise knowledge of the rod's material properties. Moreover, complex interactions (e.g., arising from further manipulation) or with non-uniformly shaped tools (e.g., a fork) are difficult to model analytically.
To sidestep these issues, we adopt a learning-based approach. A tactile sensor produces a stream or sequence of data x " rpx1, t1q, px2, t2q, . . . , pxT , tT qs P X , where each xj P Rd is the observed tactile data and tk is the time it was observed. In general, the timings tk may not be regular and each sequence xpiq can have a difference length T piq. Our task is to associate each observed signal x with a contact property y P Y, e.g., the contact location. Given a dataset D " tpxpiq, ypiqquNi"1 of N tuples, we seek to learn a parameterized function f : X Ñ Y that well captures this association.
For our perception system to work well, there are several major challenges to address. The sensor must be sufficiently sensitive with a frequency response high enough to detect vibrations. The Pacinian and Meissner's corpuscles in human skin detect vibration frequencies as high as 400Hz [19]. To achieve similar (or better) performance, sampling should be performed at least above the Nyquist rate of 800Hz2. But at these high rates, the tactile data collected are long time series
2The Nyquist rate is a sufficient condition and sub-Nyquist rate sampling may be possible for sparse or compressible signals.

Fig. 2: Spatial distribution of the 40 taxels on NUSkin.
that may be difficult to interpret. Key questions arise about what methods and features may be suitable. It is the goal of this paper to provide a crucial first-step towards answering these questions.
IV. EVENT-BASED TACTILE SENSOR
Motivated by the requirements of vibration sensing, we contribute the NUSkin, a variant of the recently proposed NeuTouch event tactile sensor [44].
We refer readers to [44] for details, but in summary, the NeuTouch is a neuromorphic tactile sensor that is similar to a human fingertip in size (37 x 21 x 13 mm), with Ecoflex0030 as "skin" and 3D-printed "bone" as a base. Sensing is performed using an electrode array of 39 radially-arranged taxels and a graphene-based piezoresistive thin film for vibrotactile sensing. The graphene-based pressure transducer has a frequency response of up to 1500 Hz [47], similar to the BioTac. This high-sensitive and low-hysteresis transducer helps to reduce the sensors' response time and enables the capture of the high frequency vibratory signals. Transmission is performed using the Asynchronously Coded Electronic Skin (ACES) [24], which enables each taxel to asynchronously transmit pressure changes in the form of positive and negative spikes (without time synchronization). This is achieved by encoding the taxels with unique electrical pulse signatures that are designed to overlap robustly for deconvolution.
The NUSkin revises the NeuTouch in three specific ways:
, Higher 4 kHz Sampling Rate. The sampling rate was increased from 1 kHz up to 4 kHz, which better accommodates the frequency response of the graphene-based pressure transducer. This is made possible by reducing the pulse signature duration from 1 ms to 0.25 ms;
, Regular taxel configuration. The NUSkin has an electrode array of 40 taxels (v.s. 39 on the NeuTouch) organized in a lattice structure -- this structure is simple and allows the sensors to produce matrix-structured data, which facilities the application of deep learning methods;
, Softer Skin. The material to emulate the "skin" was replaced with Ecoflex 00-10 that has a lower Shore A Hardness value. This softer material helps to further amplify the stimuli exerted on the sensors and results in larger deformation during contact. The former allows more spatial-temporal features to be collected [5] and the latter provides a more stable grasp of tools and objects [21], which aids in the transmission of vibratory signals to the transducer.

Tap Localization

A.

Robotiq

2F-140

NUSkin

Grasp Stability
B.

Optitrack Markers

NUSkin

Optitrack Markers

Rod
Tap position

C. Optitrack
Markers

Food Identification
D.

Depth Camera

NUSkin

E.

BioTac
Fig. 3: Our experiments involved 3 different tasks: (A) Tap localization on a rod, (B and C) Grasp stability prediction on the a plate, a 30cm rod and a box. In the shown images, the human has touched the plate, but has not yet achieved a stable grasp. In comparison, the box has been adequately grasped by the human. (D and E) Food identification through a standard fork with six different items (and a control class with nothing on the plate). Please see main text for details.

V. EXPERIMENT SETUP: TASKS AND ROBOTS
In this section, we describe our experiments that were designed to test our primary hypothesis that fast vibro-tactile perception enables sensing through held objects. We seek to answer fundamental questions related to vibration-based extended tactile perception: Can robots sense the world through tools by interpreting vibrations? How fast does sensing need to be performed? Is there a strong reason to use multiple taxels?.
To that end, we conducted a systematic study using three different tasks and objects. In the remainder of this section, we give details on the tasks, robot setup, and methods used in our experiments. As a quick overview, our tasks along with their corresponding experimental setups are shown in Fig. 3.
Robot Tasks. Our experiments involved three tasks:
, Tap Localization, where the robot is tasked to predict the position of a human tap on a held acrylic rod. We tested three rod lengths: 20cm, 30cm and 50cm;
, Grasp Stability Prediction, where the goal is to classify whether a human has a stable grasp on an object during robot-to-human handover. We used three different objects, i.e., a rod, a plate, and a box;
, Food Identification, where the robot has to classify the type of food using a standard fork. There were 7 classes in total: one control (nothing), one bowl of liquid (water), and 5 foods (slices of soft tofu, watermelon, banana, apple, and green pepper).
The Tap Localization task was inspired by recent work showing humans can localize contact on held rods [30], and serves as a controlled setting where we could more easily isolate the effect of different factors. The grasp stability and food identification problems are prototypical real-world tasks that involve tactile signals. Grasp stability prediction may appear an unusual task for dynamic sensing; stability may be better inferred from the static forces. Nonetheless, vibration perception may be

useful, e.g., by implicitly inferring the position of the fingers in contact with the object. Prior work on food identification using a sensorized fork [1] showed that reasonable haptic-based classification is possible. Here, we use a standard dinner fork and dynamic tactile data is sensed using the BioTac hydrophone sensor or the NUSkin.
Robot Setup. For all the tasks above, we used either a BioTac3 or NUSkin sensor, retrofitted onto each finger of a Robotiq 2F-140 gripper attached to a Franka-Emika Panda arm. For the tap localization and grasp stability prediction tasks, we selected a stable position and held the objects in place using position control on the Robotiq fingers. For the food classification task, we used position control to hold the fork at a tilted angle (12 degrees) and programmed the arm to lower the fork at a speed of 1 cm/s. The arm stopped either when it reached a pre-set height from the table surface, or it detected a Cartesian collision (threshold of 10N/m).
Data Collection and Preprocessing. The number of data points varied between tasks: for tap localization, we collected 1000 taps per rod; for grasp stability prediction, 50 grasps were collected per object and per class, and for the food identification task, the robot would skewer each item 50 times. The experiments were performed twice, i.e., once for each sensor. Ground truth annotations for the tap localization task were obtained using an Optitrack motion-caption system. Grasp stability and food type were manually annotated. For the tap localization and grasp stability tasks, we extracted signals from 50 ms before to 250 ms after first contact with the object was detected. For the identification task, we used sequences starting from 2 s after the arm started moving to the 8 s mark.
Vibro-tactile Features. To evaluate the role of feature learning, we evaluated four different types of features:
, Baseline. We used the raw PAC signal for the BioTac. For the NUSkin, using the raw spike trains resulted in very poor performance in preliminary experiments. As such, we use binned spike count features instead, i.e., the number of spikes in 5 ms intervals, for the localization and grasp stability tasks. The food identification task used 50 ms bin intervals.
, Fast-Fourier Transform (FFT). Similar to prior work [30], we used FFT features obtained from the BioTac PAC values and the binned spike trains from the NUSkin.
, Neural Autoencoder. Autoencoding is a popular unsupervised feature learning method based on a reconstruction loss [23]. Our encoder comprised a MLP with two hidden layers of 128 and 64 neurons, respectively, that output a 32 dimensional real-vector code. The decoder was also a MLP with two hidden layers, each with 64 and 128 neurons. Both encoder and decoder used ReLU activation units. The reconstruction was performed on the PAC values and spike counts for the BioTac and NUSkin, respectively.
, Event Spike Tensor [15]. We generated features by convolv-
3We could only use one BioTac due to cost constraints.

TABLE I: MAE scores for the Tap Localization Task.

Methods SVR Linear SVR RBF MLP RNN + MLP

Features
Baseline FFT Autoencoder EST Baseline FFT Autoencoder EST Baseline FFT Autoencoder EST Baseline EST

20cm

BioTac (PAC) NUSkin (2F)

2.5492 ± 0.1873 1.7102 ± 0.0913

2.2397 ± 0.0821 1.3864 ± 0.0448

2.6015 ± 0.2132 2.1976 ± 0.1058

-

1.5260 ± 0.1176

2.0729 ± 0.1506 1.5411 ± 0.1029

1.9021 ± 0.0516 1.2053 ± 0.0376

2.3188 ± 0.1957 1.8579 ± 0.1395

-

1.4472 ± 0.1299

2.3579 ± 0.2896 1.9006 ± 0.1239

1.7079 ± 0.0278 1.6415 ± 0.3577

2.4047 ± 0.2135 2.0130 ± 0.1565

-

1.3964 ± 0.1488

2.2773 ± 0.4501 0.9288 ± 0.2015

-

1.3320 ± 0.2319

30cm

BioTac (PAC) NUSkin (2F)

3.8219 ± 0.2584 3.2682 ± 0.1861

3.4202 ± 0.2153 2.7135 ± 0.1811

4.3844 ± 0.1394 4.0754 ± 0.3331

-

2.7310 ± 0.0862

3.0808 ± 0.1454 2.8592 ± 0.1871

2.7636 ± 0.0370 2.3250 ± 0.1395

3.5110 ± 0.2027 3.9692 ± 0.3306

2.6060 ± 0.1661

3.8654 ± 0.5083 2.9053 ± 0.1040

2.7636 ± 0.0370 2.3250 ± 0.1395

4.2544 ± 0.3960 4.1346 ± 0.3691

-

2.0273 ± 0.1546

4.7522 ± 1.1578 1.2339 ± 0.1775

-

1.7354 ± 0.2097

50cm

BioTac (PAC) NUSkin (2F)

6.3491 ± 0.5526 4.2323 ± 0.4525

6.5452 ± 0.3566 3.8361 ± 0.5234

7.4146 ± 0.4647 6.0886 ± 0.4546

-

3.8520 ± 0.4105

5.2913 ± 0.4026 3.5130 ± 0.4029

6.2178 ± 0.3066 3.4561 ± 0.4067

6.5044 ± 0.4685 4.7812 ± 0.3819

3.4290 ± 0.3368

5.1097 ± 0.5556 3.8222 ± 0.5322

5.2028 ± 0.6282 4.1149 ± 0.6749

6.5187 ± 0.5216 5.5296 ± 0.6103

-

3.4227 ± 0.3272

4.7450 ± 0.8914 3.2920 ± 1.2359

-

3.0750 ± 0.4830

ing the learnt kernel proposed in [15] on the raw NUSkin spike data. The kernel consists of two fully connected layers, each comprising 30 Leaky ReLU neurons. This convolution resulted in a smoothed sequence, from which we sampled 50 points at equal intervals.
Machine Learning Methods. We tested three popular ML methods:
, Support Vector Machine/Regression (SVM/R), which are popular kernel-based methods based upon the maximum margin principle [3, 37]. We experimented with both linear and radial basis function (RBF) kernels;
, Multi-layer perceptron (MLP), i.e., fully connected neural networks with two-hidden layers of 16 and 8 neurons with ReLU activations [31];
, Recurrent Neural Network (RNN) comprising 16 Gated Recurrent Unit (GRU) [10] cells connected to an MLP with the structured described above.
Each of the features above were fed into a machine learning (ML) model to perform regression (for localization) or classification (for stability or food type prediction). The exceptions were the Autoencoder and FFT features, which were a representation of entire time series and hence, were not used together with the RNN.
Testing Methodology. Each task-sensor-model-feature combination using a standardized setup. We report the average and standard deviation of K scores obtained by repeating the following steps K times:
1) Split the dataset randomly into 90% training and 10% testing samples.
2) Perform grid-search and 4-fold cross-validation on the training samples to obtain model hyperparameters.
3) Using the hyperparameters, optimize a model on all the training samples.
4) Test the learned model on the testing samples to obtain a performance score.
We set K based on available computational resources; K " 5 for the tap localization task and all RNN models, and K " 20 for all other combinations.

VI. RESULTS
In this section, we report our main findings. Briefly, the experimental results support our hypothesis that extended tactile sensing can be achieved with fast vibro-tactile sensing and feature learning. Moreover, using multiple taxels at a high enough sampling rate leads to performance gains. The following provides additional details.
Can the robot localize contacts using the vibro-tactile signals? Localization can indeed be performed to a relatively high accuracy; Table I shows that the best models achieved low localization errors of approximately 1 cm, 1.7 cm, and 3.1 cm for the 20, 30, and 50 cm rods, respectively. Among the learned models, the best performance was achieved using the neural models (either the MLP or RNN). That said, good results were also obtained using the SVR (RBF) with FFT features, which we found easier to tune and faster to train. The NUSkin generally achieved better performance compared to the BioTac PAC sensor, especially on the 50cm rod where the NeuTouch localization was «1 cm better.
Is vibration perception useful for grasp stability prediction and food identification? Next, we turn our attention to the "higher-level" robot perception tasks. Accuracies for grasp stability classification (Table II) indicate that useful information can be obtained from the dynamic signals; accuracy for the best models was around 60 ´ 80% across the three objects. The accuracies were reasonable but could be improved using static tactile sensing. The performance across the objects and sensors were mixed; the BioTac (PAC) scores were higher for the plate, but the NUSkin performed better with the box.
For the food identification task, Table II shows high accuracies of close to 90% were obtained using the NUSkin and FFT/EST features. Fig. 4 shows the confusion matrices for the SVM (RBF) with FFT features obtained from the NUSkin and BioTac. With the NUSkin, the most frequent errors were between the control class (empty plate) and water. Even so, the robot's ability to distinguish between the two classes was surprisingly good (« 80%) given the small forces involved when touching water with a fork. The other errors are reasonable, i.e., between the apple and pepper which had

TABLE II: Accuracy Scores for the Grasp Stability Prediction Task.

Methods SVM Linear SVM RBF MLP RNN + MLP

Features
Baseline FFT EST Baseline FFT EST Baseline FFT EST Baseline EST

Rod

BioTac (PAC) NUSkin (2F)

0.4792 ± 0.1488 0.3682 ± 0.1332

0.6500 ± 0.1225 0.5727 ± 0.1351

-

0.6727 ± 0.1477

0.6375 ± 0.1376 0.5364 ± 0.1072

0.6125 ± 0.1520 0.6409 ± 0.1479

-

0.6364 ± 0.1348

0.5667 ± 0.1434 0.4000 ± 0.1164

0.6125 ± 0.1401 0.6045 ± 0.1416

-

0.6318 ± 0.1451

0.5000 ± 0.1394 0.5091 ± 0.0445

-

0.3818 ± 0.1060

Box

BioTac (PAC) NUSkin (2F)

0.3792 ± 0.1455 0.5318 ± 0.1445

0.5917 ± 0.1205 0.7273 ± 0.0813

-

0.7773 ± 0.1055

0.5042 ± 0.1192 0.7909 ± 0.1041

0.5958 ± 0.1187 0.7182 ± 0.1072

-

0.7955 ± 0.0948

0.5000 ± 0.1208 0.5545 ± 0.1905

0.6333 ± 0.1776 0.6818 ± 0.1132

-

0.7409 ± 0.1007

0.6000 ± 0.2261 0.5455 ± 0.0813

-

0.5091 ± 0.1686

Plate

BioTac (PAC) NUSkin (2F)

0.3583 ± 0.1152 0.5773 ± 0.1608

0.7583 ± 0.1341 0.6773 ± 0.1663

-

0.5455 ± 0.1286

0.4708 ± 0.1850 0.6500 ± 0.1633

0.6167 ± 0.1275 0.6591 ± 0.1673

-

0.6136 ± 0.1313

0.5000 ± 0.1394 0.5318 ± 0.1007

0.7667 ± 0.1137 0.5955 ± 0.1874

-

0.4591 ± 0.1587

0.5167 ± 0.0624 0.5273 ± 0.0680

-

0.3091 ± 0.0445

TABLE III: Accuracy Scores for the Food Identification Task.

Methods SVM Linear SVM RBF MLP RNN + MLP

Features Baseline FFT EST Baseline FFT EST Baseline FFT EST Baseline EST

BioTac (PAC) 0.5129 ± 0.0897 0.7486 ± 0.0709
0.5171 ± 0.0711 0.6186 ± 0.0773
0.5114 ± 0.0908 0.6843 ± 0.0764
0.3486 ± 0.0492
-

NUSkin (2F) 0.6371 ± 0.0640 0.8800 ± 0.0561 0.8971 ± 0.0355 0.7471 ± 0.0445 0.8857 ± 0.0670 0.8871 ± 0.0465 0.7043 ± 0.0615 0.8186 ± 0.1039 0.8343 ± 0.0988 0.6914 ± 0.0686 0.6400 ± 0.0983

NUSkin

BioTac (PAC)

NUSkin

BioTac (PAC)

Fig. 5: MAE errors versus the sampling frequency (log-scale) for the NUSkin and BioTac (PAC). Errors fell significantly as the sampling frequency was increased from 5 Hz to 1 kHz, after which improvements were smaller. The NUSkin obtained lower errors in general, with the best performance in the 2 to 3 kHZ range.

6

NUSkin (2F)

NUSkin (1F)

BioTac (PAC) 4

MAE

Fig. 4: Confusion Matrices for the NUSkin and BioTac (PAC) on the Food Identification Task. Overall accuracy was 90% and the errors were clustered around similar objects.

2

0

20cm

30cm

50cm

similar consistency, hardness, and texture.
How fast should tactile data be sampled? To answer this question, we simulated different sampling rates using our obtained data. For the BioTac, we performed standard downsampling via decimation. The Neutouch -- due to its neuromorphic nature -- required a different approach; we iterated through each spike sequence and dropped spikes occurring within a fixed time interval  after an "accepted" spike. Each taxel was processed independently and the interval  was adapted to suit the desired sampling rate. After downsampling, we applied the same testing methodology described in Sec. V.
Fig. 5 plots localization error for the rod task against the sampling rate when using the SVR (RBF) with FFT features. We see diminishing returns with increasing sampling rate -- error rates fell dramatically as sampling rates increased from

Fig. 6: MAE scores for the NUSkin with 1 or 2 fingers, and the BioTac (PAC). Using two fingers (80-taxels) resulted in lower errors compared to the single finger (40-taxel) version and the BioTac (single hydrophone sensor).
5 to 1000 Hz, after which performance differences were small. Nevertheless, the best average performance was obtained in the 2 kHz to 3 kHz range. This trend was fairly consistent across all the methods and features, and we observed a similar curves for the grasp stability prediction and food identification tasks. Overall, for the tasks examined, a sampling frequency of 2 kHz (alternatively, a frequency response of 1 kHz) was sufficient for good performance.
Do multiple taxels lead to better performance? For this question, we focus on the rod task with the NUSkin sensor. Fig. 6 shows the average localization error achieved by the BioTac

20cm Rod 30cm Rod 50cm Rod

sensing. We believe extended tactile perception is possible when using non-rigid objects or those with multiple links if sufficient information can be transmitted along the object to the tactile sensors. However, the signals may be non-linearly transformed and aliasing may occur, which can hamper performance and require new learning methods. A full investigation with appropriate experiments remains future work.
Localization on surfaces and other complex objects. Our first experiment demonstrated that localization can be accurately performed on a rod, which can be regarded as a onedimensional prediction task. How accurately we can localize on surfaces such as the plate or 3D objects such as the box is an open question.

Fig. 7: MAE scores for the NUSkin with 1 or 2 fingers, and the BioTac (PAC). Using two fingers (80-taxels) resulted in lower errors compared to the single finger (40-taxel) version and the BioTac (single hydrophone sensor).
(PAC) and NUSkin sensors. We further split the NUSkin data into two portions, i.e., the signals gathered by the left and right fingers. The plot shows the performance from using only the left finger (40-taxels), which gave slightly better performance than the right finger in preliminary tests. Overall, we see that using all 80 taxels resulted in better localization. This trend was consistent across the ML methods and features.
We ran a second test where we trained a SVM (RBF) model using data from single taxel only. Fig. 7 shows the histogram plot of the MAEs obtained by the single taxel models, with corresponding scores obtained by the 80-taxel and 40-taxel models marked as vertical lines. Over the three rod lengths, using all 80 taxels (or even just 1 finger) was better than any individual taxel -- this finding suggests the importance of spatially-distributed vibro-tactile sensing.
VII. DISCUSSION AND CONCLUSION
In this work, we showed that tactile perception could be extended beyond the boundary of the robot via vibration perception. By using appropriate vibratory features, our system is able to accurately localize contacts and classify different contact types. Although vibratory sensing has been used before (e.g., for texture classification), to our knowledge, this is the first demonstration of highly accurate perception through standard objects. We hope this work will bring more attention to vibration sensing and its use in robotics, particularly for tool use. Certainly, there are a variety of unsolved problems to address, three of which we highlight below:
Non-rigid and multiple-link objects. In all our experiments, we have worked with rigid objects, which facilitates vibration

Multi-modal perception. Vibration sensing is but one mode
of sensing, and a coherent representation of the world requires
other modalities (e.g., force sensing, vision, auditory). For
example, our models for grasp stability prediction on the
handover task could potentially be improved by using a
combination of static and dynamic tactile sensing, along with
multi-modal machine learning models [8, 42].
REFERENCES
[1] Tapomayukh Bhattacharjee, Gilwoo Lee, Hanjun Song, and Siddhartha S. Srinivasa. Towards Robotic Feeding: Role of Haptics in Fork-Based Food Manipulation. IEEE Robotics and Automation Letters, 4(2):1485­1492, 2019.
[2] Aude Billard and Danica Kragic. Trends and challenges in robot manipulation. Science, 364(6446):eaat8414, 2019.
[3] Bernhard E Boser, Isabelle M Guyon, and Vladimir N Vapnik. A training algorithm for optimal margin classifiers. In Proceedings of the fifth annual workshop on Computational learning theory, pages 144­152, 1992.
[4] E. O. Brigham and R. E. Morrow. The fast fourier transform. IEEE Spectrum, 4(12):63­70, 1967. doi: 10.1109/MSPEC.1967.5217220.
[5] Thierri Callier, Aneesha K Suresh, and Sliman J Bensmaia. Neural coding of contact events in somatosensory cortex. Cerebral Cortex, 29 (11):4613­4627, 2019.
[6] Yevgen Chebotar, Karol Hausman, Zhe Su, Gaurav S. Sukhatme, and Stefan Schaal. Self-supervised regrasping using spatio-temporal tactile features and reinforcement learning. Intl. Conf. on Intelligent Robots and Systems, 2016-November:1960­1966, 2016.
[7] Carolyn L. Chen, Jeffrey O. Snyder, and Peter J. Ramadge. Learning to identify container contents through tactile vibration signatures. 2016 IEEE International Conference on Simulation, Modeling, and Programming for Autonomous Robots, SIMPAR 2016, pages 43­48, 2017.
[8] Kaiqi Chen, Yong Lee, and Harold Soh. Multi-modal mutual information (mummi) training for robust self-supervised deep reinforcement learning. In IEEE International Conference on Robotics and Automation (ICRA), 2021.
[9] Wei Chen, Heba Khamis, Ingvars Birznieks, Nathan F. Lepora, and Stephen J. Redmond. Tactile Sensors for Friction Estimation and Incipient Slip Detection - Toward Dexterous Robotic Manipulation: A Review. IEEE Sensors Journal, 18(22):9049­9064, 2018.
[10] Kyunghyun Cho, Bart Van Merrie¨nboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.
[11] Mark R. Cutkosky. Dynamic Tactile Sensing: Perception of Fine Surface Features with Stress Rate Sensing. IEEE Trans. on Robots and Auto., 9 (2):140­151, 1993.
[12] Mark R Cutkosky and John Ulmen. Dynamic tactile sensing. In The Human Hand as an Inspiration for Robot Hand Development, pages 389­403. Springer, 2014.
[13] Jeremy A. Fishel and Gerald E. Loeb. Bayesian exploration for intelligent identification of textures. Frontiers in Neurorobotics, 6(JUNE):1­20, 2012.

[14] Jeremy A Fishel, Veronica J Santos, and Gerald E Loeb. A robust micro-vibration sensor for biomimetic fingertips. In IEEE RAS & EMBS Intl. Conf. on Biomedical Rob. and Biomechatronics, pages 659­663. IEEE, 2008.
[15] Daniel Gehrig, Antonio Loquercio, Konstantinos G Derpanis, and Davide Scaramuzza. End-to-end learning of representations for asynchronous event-based data. In CVPR, pages 5633­5643, 2019.
[16] A. Go´mez Egu´iluz, I. Ran~o´, S. A. Coleman, and T. M. McGinnity. Reliable robotic handovers through tactile sensing. Autonomous Robots, 43(7):1623­1637, 2019.
[17] Norman Hendrich, Hannes Bistry, Johannes Liebrecht, and Jianwei Zhang. Natural Robot-Human Handover Combining Force and Tactile Sensors. Workshop on Assistance and Service Robotics in a Human Environment, IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems, pages 1­7, 2014.
[18] Heiko Hoffmann, Zhichao Chen, Darren Earl, Derek Mitchell, Behnam Salemi, and Jivko Sinapov. Adaptive robotic tool use under variable grasps. Robots and Autonomous Systems, 62(6):833­846, 2014.
[19] Roland S Johansson and J Randall Flanagan. Coding and use of tactile signals from the fingertips in object manipulation tasks. Nature Reviews Neuroscience, 10(5):345­359, 2009.
[20] Zhanat Kappassov, Juan Antonio Corrales, and Ve´ronique Perdereau. Tactile sensing in dexterous robot hands - Review. Robots and Autonomous Systems, 74:195­220, 2015.
[21] Akhtar Khurshid, Abdul Ghafoor, and M Afzaal Malik. Robotic grasping and fine manipulation using soft fingertip. In Advances in Mechatronics. IntechOpen, 2011.
[22] Roberta L. Klatzky and Susan J. Lederman. Tactile roughness perception with a rigid link interposed between skin and surface. Perception and Psychophysics, 61(4):591­607, 1999.
[23] Mark A Kramer. Nonlinear principal component analysis using autoassociative neural networks. AIChE journal, 37(2):233­243, 1991.
[24] Wang Wei Lee, Yu Jun Tan, Haicheng Yao, Si Li, Hian Hian See, Matthew Hon, Kian Ann Ng, Betty Xiong, John S Ho, and Benjamin C K Tee. A neuro-inspired artificial peripheral nervous system for scalable electronic skins. Science Robotics, 4(32), 2019.
[25] Huaping Liu and Fuchun Sun. Material Identification Using Tactile Perception: A Semantics-Regularized Dictionary Learning Method. IEEE/ASME Transactions on Mechatronics, 23(3):1050­1058, 2018.
[26] Shan Luo, Wenxuan Mou, Min Li, Kaspar Althoefer, and Hongbin Liu. Rotation and translation invariant object recognition with a tactile sensor. Proceedings of IEEE Sensors, 2014-December(December):1030­1033, 2014.
[27] Shan Luo, Joao Bimbo, Ravinder Dahiya, and Hongbin Liu. Robotic tactile perception of object properties: A review. Mechatronics, 48:54­67, 2017.
[28] Marianna Madry, Liefeng Bo, Danica Kragic, and Dieter Fox. St-hmp: Unsupervised spatio-temporal feature learning for tactile data. In Intl. Conf. on Robotics and Automation, pages 2262­2269. IEEE, 2014.
[29] Carolyn Matl, Robert Matthew, and Ruzena Bajcsy. Haptic Perception of Liquids Enclosed in Containers. Intl. Conf. on Intelligent Robots and Systems, pages 7142­7149, 2019.
[30] Luke E. Miller, Luca Montroni, Eric Koun, Romeo Salemme, Vincent Hayward, and Alessandro Farne`. Sensing with tools extends somatosensory processing beyond the body. Nature, 561(7722):239­242, 2018.
[31] Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on International Conference on Machine Learning, pages

807­814, 2010. [32] Zachary Pezzementi, Erion Plaku, Caitlin Reyda, and Gregory D
Hager. Tactile-object recognition from appearance information. IEEE Transactions on Robotics, 27(3):473­487, 2011. [33] Joseph M. Romano and Katherine J. Kuchenbecker. Methods for robotic tool-mediated haptic surface recognition. IEEE Haptics Symposium, HAPTICS, pages 49­56, 2014. [34] Joseph M. Romano, Kaijen Hsiao, Gu¨nter Niemeyer, Sachin Chitta, and Katherine J. Kuchenbecker. Human-inspired robotic grasp control with tactile sensing. IEEE Transactions on Robotics, 27(6):1067­1079, 2011. [35] Alexander Schneider, Ju¨rgen Sturm, Cyrill Stachniss, Marco Reisert, Hans Burkhardt, and Wolfram Burgard. Object identification with tactile sensors using bag-of-features. In Intl. Conf. on Intelligent Robots and Systems, pages 243­248. IEEE, 2009. [36] David Silvera-Tawil, David Rye, and Mari Velonaki. Artificial skin and tactile sensing for socially interactive robots: A review. Robots and Autonomous Systems, 63(P3):230­243, 2015. [37] Alex J Smola and Bernhard Scho¨lkopf. A tutorial on support vector regression. Statistics and computing, 14(3):199­222, 2004. [38] Harold Soh and Yiannis Demiris. Incrementally learning objects by touch: Online discriminative and generative models for tactile-based recognition. IEEE Transactions on Haptics, 7(4):512­525, 2014. [39] Harold Soh, Yanyu Su, and Yiannis Demiris. Online spatio-temporal Gaussian process experts with application to tactile classification. In Intl. Conf. on Intelligent Robots and Systems, pages 4489­4496. IEEE, 2012. [40] Joseph H. Solomon and Mitra J. Hartmann. Biomechanics: Robotic whiskers used to sense features. Nature, 443(7111):525, 2006. [41] Matti Strese, Clemens Schuwerk, Albert Iepure, and Eckehard Steinbach. Multimodal Feature-Based Surface Material Classification. IEEE Transactions on Haptics, 10(2):226­239, 2017. [42] Zhi-Xuan Tan, Harold Soh, and Desmond Ong. Factorized inference in deep markov models for incomplete multimodal time series. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34. [43] Tasbolat Taunyazov, Hui Fang Koh, Yan Wu, Caixia Cai, and Harold Soh. Towards effective tactile identification of textures using a hybrid touch approach. In Intl. Conf. on Robotics and Automation, pages 4269­4275. IEEE, 2019. [44] Tasbolat Taunyazoz, Weicong Sng, Hian Hian See, Brian Lim, Jethro Kuan, Abdul Fatir Ansari, Benjamin Tee, and Harold Soh. Event-driven visual-tactile sensing and learning for robots. In Proceedings of Robotics: Science and Systems, July 2020. [45] Stephen Timoshenko. History of strength of materials: with a brief account of the history of theory of elasticity and theory of structures. McGraw-Hill, 1953. [46] Akihiko Yamaguchi and Christopher G. Atkeson. Recent progress in tactile sensing and sensors for robotic manipulation: can we turn tactile sensing into vision?1. Advanced Robotics, 33(14):661­673, 2019. [47] Haicheng Yao, Pengju Li, Wen Cheng, Weidong Yang, Zijie Yang, Hashina Parveen Anwar Ali, Hongchen Guo, and Benjamin CK Tee. Environment-resilient graphene vibrotactile sensitive sensors for machine intelligence. ACS Materials Letters, 2(8):986­992, 2020. [48] Wenzhen Yuan, Siyuan Dong, and Edward H Adelson. Gelsight: Highresolution robot tactile sensors for estimating geometry and force. Sensors, 17(12):2762, 2017. [49] Mabel M. Zhang, Monroe D. Kennedy, M. Ani Hsieh, and Kostas Daniilidi. A triangle histogram for object classification by tactile sensing. Intl. Conf. on Intelligent Robots and Systems, 2016-November:4931­4938, 2016.

