Improved Rates for Differentially Private Stochastic Convex Optimization with Heavy-Tailed Data

Gautam Kamath

Xingtu Liu June 4, 2021

Huanyu Zhang§

arXiv:2106.01336v2 [cs.LG] 3 Jun 2021

Abstract

We study stochastic convex optimization with heavy-tailed data under the constraint of

differential privacy. Most prior work on this problem is restricted to the case where the loss

function is Lipschitz. Instead, as introduced by Wang, Xiao, Devadas, and Xu [WXDX20], we

study general convex loss functions with the assumption that the distribution of gradients has

bounded k-th moments. We provide improved upper bounds on the excess population risk under

approximate differential privacy of O~

+ d

d

k-1 k

n

n

and O~

+ d

d

2k-2 k

n

n

for convex and

strongly convex loss functions, respectively. We also prove nearly-matching lower bounds under the constraint of pure differential privacy, giving strong evidence that our bounds are tight.

1 Introduction

Stochastic convex optimization (SCO) is a classic optimization problem in machine learning. The

goal is, given a loss function  and a dataset X1, . . . , Xn drawn i.i.d. from some unknown distribution

D,

to

output

a

parameter

vector

w

which

minimizes

the

population

risk

LD (w)

=

E [(w; x)].
xD

The

quality of a solution w^ is measured in terms of the excess risk over the minimizer in the parameter set

W, LD(w^) - argwmiWn LD(w). We study SCO under the constraint of differential privacy [DMNS06] (DP), a rigorous notion of privacy which guarantees that an algorithm's output distribution is

insensitive to modification of a small number of datapoints.

The field of DP optimization has seen a significant amount of work. Early results focused

on differentially private empirical risk minimization (ERM), a non-statistical problem in which

the goal is to privately output a parameter vector w which minimizes the loss function  over

a fixed dataset X1, . . . , Xn:

that

is

we

would

like

to

optimize

minw

1 n

n i=1

(w,

Xi

).

See, for

example, [CM08, CMS11, RBHT12, KST12, TS13, SCS13, JT14, BST14, TTZ15, KJ16, WLK+17,

WYX17, INS+19, WJEG19, ZMH21, WZGX21]. The first result to address the statistical problem

of DP SCO was [BST14], using generalization properties of differential privacy and regularized

ERM. However, the excess risk bounds were suboptimal. [BFTT19] addressed this and closed the

gap by providing tight upper bounds on DP SCO. Following this result there has been renewed

Authors are in alphabetical order. Cheriton School of Computer Science, University of Waterloo. g@csail.mit.edu. Supported by an NSERC
Discovery Grant. Cheriton School of Computer Science, University of Waterloo. x563liu@uwaterloo.ca. §Facebook. Some of this work was done while the author was a graduate student at Cornell University, supported
by NSF #1815893. huanyuzhang@fb.com.

1

interest in DP SCO, with works reducing the gradient complexity and running time [FKT20, KLL21], and deriving results for different geometries [AFKT21, BGN21].
Despite the wealth of work in this area, a significant restriction in almost all results is that the loss function is assumed to be Lipschitz. This assumption bounds the magnitude of each datapoint's gradient, a very convenient property for restricting the sensitivity in the design of differentially private algorithms. While convenient, it is often an unrealistic assumption which does not hold in practice, and DP optimizers resort to heuristic clipping of gradients to enforce a bound on their magnitude [ACG+16]. One can remove the strong Lipschitz assumption by instead assuming that the distribution of gradients is somehow well-behaved. In this vein, [WXDX20] introduces and studies the problem of DP SCO with heavy-tailed data. Their work removes the requirement that the loss function is Lipschitz, and instead assumes that the distribution of the gradient has bounded second moments. However, they leave open the question of whether the rates of their algorithms can be improved.

1.1 Results

We answer this question affirmatively, giving algorithms with much better rates for DP SCO with heavy-tailed gradients. We also prove lower bounds which provide strong evidence that these algorithms can not be significantly improved.
Our main upper bound is the following.

Theorem 1.1 (Informal, see Theorems 4.7 and 4.8). Suppose we have a loss function  : W × Rd  R and there exists a distribution D over Rd such that for any parameter vector w  W, when x  D, the k-th moment of (w, x) is bounded. Then there exists a computationally efficient 2-
zero-concentrated differentially private algorithm which, given x1, . . . , xn  D, outputs a parameter vector wpriv satisfying the following:

E[LD(wpriv) - LD(w)]  O~

k-1
d+ d k , n n

where w = arg minw LD(w). Furthemore, if  is strongly convex and smooth, a similar algorithm guarantees the following:





2(k-1)

E[LD (wpriv )

-

LD(w)]



O~

d n

+

d n

k .

This theorem is stated under the constraint of 2-concentrated differential privacy, which also

implies the more common notion of (O( log(1/)), )-differential privacy for any  > 0 (see

Lemma 2.3). Thus, ignoring factors which are logarithmic in 1/, the same rates in Theorem 1.1

also hold under the weaker notion of (, )-differential privacy.

Prior work on DP SCO with heavy tailed data is due to [WXDX20]. Their main results are

algorithms for a case with bounded second moments (k = 2), guaranteeing excess risk bounds of

O~

d2 1/3 2n

and O~

d2 2n

for the convex and strongly convex cases, respectively. We comment

that their bounds are not immediately comparable to ours, due to differences in assumptions.

Namely, they assume that the second moment is bounded only in axis-aligned directions, whereas

we assume that it is bounded in all directions. While our assumption is stronger, we chose it as

the focus of our work since it is rotation invariant, and thus we consider it to be more natural.

2

Nonetheless, a slight variant of our analysis (described in Appendix B) allows us to similarly derive

excess risk bounds under their bounded moment assumption. These bounds are O~

d3/2 n

and

O~

d3/2 n

for the convex and strongly convex cases, respectively, which significantly improves upon

their excess risk bound for both cases. In addition to improving their error and refining their

moment bound, we also derive results that are applicable to distributions with bounded moment

conditions of all orders k, while [WXDX20] only applies to distributions with bounded second

moments (k = 2). Finally, while it may appear that one advantage of our upper bounds is that

they hold under the stronger notion of concentrated DP, the results of [WXDX20] could easily be

analyzed under concentrated DP as well.

To complement our upper bounds, we provide lower bounds with rates which match our upper

bounds up to polylogarithmic factors.

Theorem 1.2 (Informal, see Theorems 5.1 and 5.4). Let  : W × Rd  R be a convex loss function and D be a distribution over Rd, such that for any parameter vector w  W, when x  D, the k-th
moment of (w, x) is bounded. Suppose there exists an -differentially private algorithm which is given x1, . . . , xn  D and outputs a parameter vector wpriv. Then there exists a choice of convex loss function  and distribution D such that

E[LD(wpriv) - LD(w)]  

k-1
d+ d k , n n

where w = arg minw LD(w). Furthermore, there exists a choice of strongly convex loss function 

and distribution D such that





2(k-1)

E[LD (wpriv )

-

LD(w)]





d n

+

d n

k .

We note that our upper bounds are for (, )-DP, whereas our lower bounds are for (, 0)-DP. Thus while the rates match, there is still a gap with respect to the privacy notions. However, the same gap exists in the current literature even for the simplest DP SCO problem of private mean estimation. Specifically, [KSU20] similarly shows (, )-DP upper bounds, (, 0)-DP upper bounds,1 and (, 0)-DP lower bounds for private heavy-tailed mean estimation, all of which match. They conjecture that a matching (, )-DP lower bound should also exist, appealing to results [KLSU19, BKSW19] showing that private Gaussian mean estimation (which can be seen as a limiting case of heavy tails when k = ) enjoys the same rate under either (, )-DP or (, 0)-DP. Thus, we conjecture the same, and in fact a positive resolution to their conjecture would imply the same for us.

1.2 Techniques
Our upper bounds operate using a gradient-descent-based method, relying upon an algorithm for private mean estimation. In particular, we instantiate an oracle which can output an estimate of the true gradient at a point. This oracle is based on the algorithms of [KSU20], which address the problem of private mean estimation of heavy-tailed distributions. That said, for several reasons,
1One might be curious why [KSU20] shows upper bounds under both pure and approximate DP, since the former is a stronger privacy notion than the latter. However, their pure DP algorithm is not computationally efficient, while their approximate DP algorithm is.

3

a naive black-box application of their results are insufficient to achieve the rates in Theorem 1.1. First, the accuracy guarantees in [KSU20] give a prescribed 2-error with high probability. While such guarantees for an oracle allow one to achieve non-trivial rates for DP SCO, they are far from optimal. Instead, we can get better results when the estimator is known to have low bias. This is where the intersection of privacy and heavy-tailed data gives rise to a new technical challenge: no unbiased mean estimation algorithm for this setting is known to exist. To deal with these issues, we explicitly derive bounds on the bias and variance of the estimator. While this was implicitly done in [KSU20], the terms were balanced to minimize the 2-error in a high-probability bound, whereas we need to decouple the bias and the variance for our application. Even with these changes in place, the bound would still be lossy ­ as a final modification, we find that a different bias-variance tradeoff is required in each iteration to achieve the best possible error. Namely, if we tolerate additional variance to reduce the bias of each step, this results in an improved final accuracy.

2 Preliminaries

2.1 Privacy Preliminaries
In our work we consider a few different variants of differential privacy. The first is the standard notion of differential privacy.
Definition 2.1 (Differential Privacy (DP) [DMNS06]). A randomized algorithm M : X n  Y satisfies (, )-differential privacy if for every pair of neighbouring datasets X, X  X n (i.e., datasets that differ in exactly one entry),
Y  Y P[M (X)  Y ]  e · P[M (X)  Y ] + .

When  = 0, we say that M satisfies -differential privacy or pure differential privacy.

The second is concentrated differential privacy [DR16], and its refinement zero-concentrated differential privacy [BS16]. Since in this work we exclusively concern ourselves with the latter, in a slight overloading of nomenclature, we refer to it more concisely as concentrated differential privacy.

Definition 2.2 (Concentrated Differential Privacy (CDP) [BS16]). A randomized algorithm M : X n  Y satisfies -CDP if for every pair of neighboring datasets X, X  X n,

  (1, ) D(M (X) M (X))  ,

where D(M (X) M (X)) is the -R´enyi divergence between M (X) and M (X).

Roughly speaking, concentrated differential privacy lives between pure (, 0)-differential privacy and approximate (, )-differential privacy, formalized in the following lemma.

Lemma 2.3 ([BS16]). For every  > 0, if M is -DP, then M is

1 2

2

-CDP.

If

M

is

1 2

2

-CDP,

then M is

1 2

2

+



2 log(1/),  -DP for every  > 0.

Differential privacy enjoys various nice properties, including post-processing and adaptive composition.

Lemma 2.4 (Post-Processing [DMNS06, BS16]). If M : X n  Y is (, )-DP, and P : Y  Z is any randomized function, then the algorithm P  M is (, )-DP. Similarly if M is -CDP then the algorithm P  M is -CDP.

4

Lemma 2.5 (Composition of DP [DMNS06, DRV10, BS16]). If M is an adaptive composition of differentially private algorithms M1, . . . , MT , then the following all hold:
· If M1, . . . , MT are (1, 0), . . . , (T , 0)-DP then M is (, 0)-DP for  = t t.
· If M1, . . . , MT are 1, . . . , T -CDP then M is -CDP for  = t t.

2.2 Optimization Preliminaries

Definition 2.6. A function f : W  R is L-Lipschitz if for all w1, w2  W we have |f (w1) - f (w2)| L w1 - w2 2.

Definition 2.7. A function f is a-strongly convex on W if for all w1, w2  W we have f (w1) 

f (w2) +

f (w2), w1 - w2

+

a 2

w1 - w2

2 2

.

Definition 2.8. A function f is b-smooth on W if for all w1, w2  W, f (w1)  f (w2)+ f (w2), w1-

w2

+

b 2

w1 - w2

22.

Definition 2.9. Given a convex set W, we denote the projection of any   Rd to the convex set

W

by

P

rojW

()

=

arg min
wW

-w

2.

2.3 Problem Setup
Definition 2.10 (Stochastic Convex Optimization (SCO)). Let D be some unknown distribution over X and X = {x1, . . . , xn} be i.i.d. samples from D. Given a convex constraint set W  Rd and a convex loss function  : W × X  R, the goal of stochastic convex optimization (SCO) is to find a minimizer wpriv for the population risk LD(wpriv) = ExD[(wpriv, x)]. The utility of an algorithm A is measured by the expected excess population risk

E
X Dn ,A

LD

(wpriv

)

-

min
wW

LD

(w)

.

We use the following definition to denote that a distribution has bounded moments. Since it enforces a constraint on every univariate projection of the distribution, it is rotation invariant. This is in comparison to the axis-aligned notion studied by [WXDX20], which is less common in the literature.

Definition 2.11. Let D be a distribution over Rd with mean µ. We say that for k  2, the k-th moment of D is bounded by , if for every unit vector v  Sd-1,

E[| X - µ, v |k]  ,

where Sd-1 is the surface of a d-dimensional sphere.

Let Br(c)  Rd be the ball of radius r > 0 centered at c  Rd. All our theorems rely on the following set of assumptions.

Assumption 1. We assume the following: 1. The loss function (w, x) is non-negative, differentiable and convex for all w  W and x  X . 2. The convex constraint set W is bounded with diameter M . 3. The gradient of the loss function at the optimum is zero. 4. For any w  W, the distribution of the gradient of the loss function has bounded k-th moments: (w, x)  P satisfies Definition 2.11 with  = 1. 5. For any w  W, the distribution of the gradient has bounded mean: E[(w, x)]  BR(0), where R = (1).

5

The first three points in Assumption 1 are standard when studying convex learning problems. The fourth is a relaxation of the Lipschitz condition in non-heavy-tailed SCO problems, in which the gradient is assumed to be uniformly bounded by a constant L. While the gradient in our setting is unbounded, it is realistic to assume that the expected gradient is inside a ball with some radius R. In fact, packing lower bounds for private mean estimation necessitate such an assumption under most notions of DP [KV18]. As a direct corollary, LD(w) 2  R, so LD(w) is R-Lipschitz.

3 A Framework for Stochastic Convex Optimization
In this section, we present a general framework for DP SCO. Before diving into the details, we first provide some intuition on how we approach this problem, via the classic optimization model.
Let LD(·) be the expected loss function to minimize. Note that although the data x  X , the loss function (·), and its gradient (·) may be heavy-tailed, LD(·) is well-behaved (it is both convex and R-Lipschitz), which directly comes from Assumption 1. Therefore, if LD(·) were known beforehand, the problem would reduce to a classical convex optimization problem, which could be solved by gradient descent (GD).
Next, we consider the case when LD(·) is not known to the optimizer. Note that directly applying GD is infeasible, since it is impossible to compute the exact gradient LD(·). An alternative way is to estimate LD(·) from the samples X, which incurs an additional loss due to error in our estimate.
Algorithm 1 SCO algorithmic framework SCOF,T,MeanOracle(X) Input: X = {xi}ni=1, xi  Rd, algorithm MeanOracle, parameters , T .
1: Initialize w0  W. 2: for t = 1, 2, . . . , T do 3: LD(wt-1) = MeanOracle({(wt-1, xi)}ni=1) 4: wt = ProjW (wt-1 - t-1LD(wt-1))
Output: {w1, w2, . . . , wT }.

Our framework, SCOF, is presented in Algorithm 1, where we generally follow the framework of GD. The main difference is in that the updating rule, we replace LD(wt-1) by its estimate, LD(wt-1), which is attained by a mean estimation algorithm.
First, we note that Algorithm 1 is differentially private if the mean estimator MeanOracle is differentially private, a consequence of the composition and post-processing properties of differential privacy. Furthermore, in Theorems 3.1 and 3.2, we describe the relationship between the population risk of Algorithm 1 and the accuracy of MeanOracle. In Theorem 3.1, we consider a general class of convex loss functions, while in Theorem 3.2, we consider a less general class of functions, by assuming they are both smooth and strongly convex. Although the proof techniques are standard and similar to the previous work, e.g., [ASY+18], we include the analysis for completeness. The proofs are in Appendix C.1, and Appendix C.2, respectively.

Theorem 3.1 (Convex). Suppose that MeanOracle guarantees that, for any w  W, E[LD(w)]-

LD(w)

2

B

and E[

LD (w)-LD (w)

2 2

]



G2.

Under Assumption 1, for any 

> 0 the output

wpriv

=

1 T

t[T ] wt produced by SCOF satisfies

E
X Dn ,A

LD(wpriv) - LD(w)



M2 2T

+

R2 2

+

G2 2

+ MB,

6

where w = arg minw LD(w).

Theorem 3.2 (Strongly convex and smooth). Suppose that MeanOracle guarantees that, for any

w  W, E[ LD(w) - LD(w) 2]  G. Under Assumption 1, and the further assumption that the

population

risk

is

a-strongly

convex

and

b-smooth,

if



=

1 a+b

,

the

output

wpriv

= wT

produced

by

SCOF satisfies

X

E
Dn

,A[LD

(wpriv

)

-

LD (w )]



1

-

(a

ab + b)2

T

M

+

(a

+ b)G ab

.

Specifically, if T = log

(a+b)G ab

/ log

a2 +b2 +ab (a+b)2

, the output wpriv satisfies

E
X Dn ,A

LD(wpriv) - LD(w)



(a

+

b)2(M + 2a2b

1)2G2 ,

where w = arg minw LD(w).

4 Algorithms for SCO with Heavy-Tailed Data
We present our algorithms for -CDP SCO. We first introduce our mean estimation oracle in Section 4.1. Then we leverage it to obtain our main algorithms, which are presented in Section 4.2.

4.1 Mean Estimation Oracle

Generally speaking, we leverage CDPHDME as our oracle, proposed in [KSU20] to privately estimate the mean of a heavy-tailed distribution. However, it is insufficient to use it as a black box, as this would result in loose bounds for DP SCO. Therefore, we provide a novel analysis of this algorithm, which differs from theirs in two crucial aspects.
First, their analysis only applies for a specific choice of the truncation parameter ( in Theorem 4.1), which is selected to be optimal for their one-round algorithm. However, note that our problem is different from theirs, since GD requires multiple steps instead of only one round. If we naively follow the same parameter setting as they do, we will get a loose bound on the excess risk. Therefore, we generalize their analysis to accommodate a range of values for  to fit our needs.
Second, while their analysis provides 2-error guarantees which hold with constant probability, our results for the stochastic oracle model require us to instead derive bounds on the expectation and variance of the estimator. At the same time, we must refine their approach so that most steps in the algorithm fail with exponentially small probability, as compared to constant probability in their analysis.
Our new analysis can be summarized by Theorem 4.1, which reveals a trade-off between the bias and variance of the estimator. Note that by choosing different values for the truncation parameter  , we can achieve a different balance between the bias and variance. The proof appears in Appendix C.4. The algorithm is described in Algorithm 3, for which Algorithm 2 is a key subroutine.

Theorem 4.1. Let D be a distribution over R with mean µ  BR 0 and k-th moment bounded

by 1. Let   60, there exists a polynomial-time, -CDP algorithm CDPHDME(,  ) that takes

nO hold:

d log R 

+

log

R

samples from D, and outputs µ  Rd, such that the following conditions

7

1. E [µ] - µ 2  O

d n

+

1 k-1 

.

2. E

µ-µ

2 2

O

d n

+

 2d2 n2

+

1 2k-2 

.

Algorithm 2 -CDP Range Estimator CDPRE,,R(X) [KSU20] Input: Samples X = {xi}ni=1, xi  R. Parameters , R > 0,   10 Output: [a, b]  R
1: Divide [-R - 2, R + 2 ] into buckets [-R - 2, -R), ..., [-2, 0), [0, 2 ), ..., [R, R +  ] 2: For each bucket B, assign a noisy count |{xi|xi  B}| + N (0, 1/) 3: Let [a, b] be the bucket with the greatest noisy count 4: return [a, b]

Algorithm 3 CDP High-Dimensional Mean Estimator CDPHDME, (X) [KSU20]

Input: Samples X = {xi}ni=1, xi  Rd. Parameters , R > 0,   60

1: procedure CDPHDME,r,R(X)

2: Set parameters: Y  (X1, ..., Xn) Z  (Xn+1, ..., X2n)

3: for i  1, ..., d do

4:

ci



CDPRE

 d

,10,R

(Y

i)

5: Let c  (c1, . . . , cd) 6: Let Z  

7: for z  Z  Bd(c) do

8:

Z  Z  {z - c}

9:

Let   max

Z

,

3 4

n

10:

µ



1 

zZ z + N

-0 ,

32 2d 9n2

Id×d

Output: µ + c  Rd

4.1.1 Algorithm Subroutine

In this section, we overview the key components in the proof of Theorem 4.1. While the proof is based on the original approach in [KSU20], we crucially sharpen and simplify their analysis at several points.
The first step of Algorithm 3 coarsely estimates the mean of the distribution, using a histogrambased approach described in Algorithm 2. The guarantees of this procedure are described in Theorem 4.2, and proven in Appendix C.3. A weaker statement is proved in [KSU20] for a fixed threshold  and a constant probability of success, we generalize it to handle arbitrary thresholds  and to have sharp exponential tail bounds.

Theorem 4.2. Let D be a distribution over R with mean µ  [-R, R] and k-th moment bounded

by 1. There exists an -CDP algorithm that takes samples from D, and outputs I  R, such that

with probability at least 1 - 2 ·

R 

+

2

· e-

n2 1024

-

e-

n 50

,

µ

-

a+b 2

 3 .

After performing coordinatewise coarse mean estimation, Algorithm 3 truncates points based on the magnitude of their 2-norm. While the algorithm simply discards points which are too far

8

from the coarse estimate of the mean, we consider an alternative notion of truncation which sets these distant points to the true (unknown) mean of the distribution. Since the mean is unknown, this is clearly not implementable, but it serves as a useful tool in the analysis.

Definition 4.3 (Definition 4.2 in [KSU20]). Let c, x  Rd, and r > 0. Then we define trunc(c, r, x) as follows.
trunc(c, r, x, µ) = x if c - x 2  r, µ if c - x 2 > r.
Similarly, for a dataset X = (X1, . . . , Xn)  Rn×d, we define trunc(c, r, S, µ) as the dataset (X1 , . . . , Xn ), where for each 1  i  n, Xi = trunc(c, r, Xi, µ).

The following lemma quantifies the bias induced if this (fictional) truncation were performed.

Lemma 4.4 (Fictional truncation, Lemma 4.3 in [KSU20]). Let D be a distribution over Rd with mean µ, and k-th moment bounded by 1, where k  2. Let c  Rd, and   1. Let x  D, and Z

be the following random variable:

 Z = trunc(c,  d, x, µ).

If


µ - c 2   2 d , then

µ - E [Z] 2 

C 

k-1 for a constant C.

We use Lemma 4.5 to quantify the bias induced by the actual truncation procedure. The proof

relies upon bounding the bias introduced by the fictional truncation, as quantified in Lemma 4.4.

The statement of Lemma 4.5 is similar to Lemma 4.5 in [KSU20], but the proof is novel. We

consider both to be cleaner than the equivalents in [KSU20], and thus more broadly applicable.

Lemma 4.5. Let   20, and D be a distribution over Rd with mean µ and k-th moment bounded by 1. Suppose x  D, and µ - c 2   2 d ,

µ - E x | x  Bd(c)

3·
2

C 

k-1
.





Proof. Let Z = trunc(c,  d, x, µ). Note that when x / Bd(c), Z = trunc(c,  d, x, µ) = µ.

E [Z - µ] = Pr x  Bd(c) · E x - µ | x  Bd(c) + Pr x / Bd(c) · (µ - µ)

= Pr x  Bd(c) · E x - µ | x  Bd(c) .

(1)

Therefore,

E [Z] - E x | x  Bd(c)

= E [Z - µ] - E 2

x

-

µ

|

x



B

 d

(c)



2

= Pr

1 x  Bd(c)

- 1 ·

E [Z - µ] 2

 2 · Pr x / Bd(c) · E [Z] - µ 2 ,

where the second equality comes from (1), and the last inequality comes from the fact that  <

1 2

,

1 1-



1

+

2.

Finally, by Lemma 4.4, E [Z] - µ 2 

C 

k-1. By triangle inequality,

µ - E x | x  Bd(c)


2

3·

C 

k-1.

9

The proof of Theorem 4.1 depends primarily upon Theorem 4.2 and Lemma 4.5, and appears in Section C.4.

4.2 Main Algorithm
With this tool in place, we present our main algorithm in Algorithm 4. This instantiates our oracle framework (SCOF) using the private mean estimation algorithm (CDPHDME in Theorem 4.1) as our oracle (MeanOracle).
Algorithm 4 CDP-SCO algorithm with heavy-tailed data Input: X = {xi}ni=1, xi  Rd, parameters , , T
1: {wt}Tt=1 = SCOF,T,CDPHDME(/T, )(X) Output: wpriv

Since each of the T steps of the algorithm is /T -CDP, composition of CDP gives the following privacy guarantee.

Lemma 4.6. Algorithm 4 is -CDP.

We provide accuracy guarantees of Algorithm 4 in the following two theorems. In Theorem 4.7, we consider a general class of convex loss functions, and in Theorem 4.8, we consider the functions that are both strongly convex and smooth. The proofs follow by appropriately selecting the truncation parameter  , and balancing the bias and variance in SCOF. We defer the proofs to Appendix C.5 and Appendix C.6, respectively.

Theorem 4.7 (Convex). Suppose we have a stochastic convex optimization problem which satisfies

Assumption

1.

Algorithm

4,

instantiated

on

this

problem

with

parameters

T

=

1 4 log R

·

n d

2k-2
k,



=

M RT

,

and



=

n d

1/k
,

will

output

wpriv

=

1 T


t[T ] wt. If n  O dTlog R + log R , then

X

E
Dn

,A[LD

(wpriv

)

-

LD(w)]



MR

log R · O

k-1

d n

+

d n

k

,

where w = arg minw LD(w).

Theorem 4.8 (Strongly Convex and Smooth). Suppose we have a stochastic convex optimization

problem which satisfies Assumption 1, and additionally, the loss function  is a-strongly convex and

b-smooth. Algorithm 4, instantiated on this problem with parameters T = log

(a+b)G ab

/ log

=

1 a+b

and



=

n dT

1/k
,

will

output

wpriv

=

wT .

If

n

O


dTlog R + log R , then

a2 +b2 +ab (a+b)2

,

XEDn,A[LD(wpriv) - LD(w)]



(M

+

1)2(a a2b

+ b)2

· O~

d n

+

dn

2k-2 k

,

where w = arg minw LD(w).

10

Remark 4.9. Our non-standard choice of the truncation parameter  in Theorem 4.7 is crucial to

obtaining our results. If one were to na¨ively use the prior setting of  =

n

1
k from [KSU20], we

dT

would achieve much worse bounds. Instead, in order to reduce bias we truncate far less aggressively

than they do, which comes at the cost of increased variance. Roughly speaking, if we were to use

their choice of  for the convex case, the error would be O

1 T

+

1 k-1 

=O

1 T

+

 dTn

k-1 k

.

Fixing T =

n d

2k-2
, 2k-1 we obtain the bound O

k-1
dn 2k-1 . Considering the case k = 2, this

1

1

gives O dn 3 instead of our bound of O dn 2 in Theorem 4.7. In the limit as k  ,

our bound is quadratically better.

5 Lower Bounds for DP SCO with Heavy-Tailed Data
In this section, we present our lower bounds for -DP SCO. Our results are generally attained by reducing from DP SCO to DP mean estimation. Similar connections have been explored when proving lower bounds for DP empirical risk minimization [BST14].

5.1 Strongly-Convex Loss Functions
Theorem 5.1 (Strongly convex case). Let n, d  N and  > 0. There exists a strongly convex loss function  : W × Rd, such that for every (, 0)-DP algorithm A (whose output on input X is denoted by wpriv = A(X)), there exists a distribution D on Rd such that w  W, supv: v 2=1 ExD | (w, x) - E [(w, x)] , v |k  1, which satisfies

2k-2

X EDn ,A[LD (wpriv )

-

LD(w))]



d n

+



min

1,

d n

k

,

where w = arg minw LD(w).
Proof. The following lemma shows a reduction from DP mean estimation to DP SCO. The proof is deferred to Appendix C.7.
Lemma 5.2. Let n, d  N, and  > 0. There exists a strongly convex loss function  : W × Rd, such that for every (, 0)-DP algorithm A (whose output on input X is denoted by wpriv = A(X)), and every distribution D on Rd with E[D] = µ,

X

E
Dn

,A[LD

(wpriv

)

-

LD(w)]

=

E
X Dn ,A

1 2

wpriv - µ

2 2

,

where w = arg minw LD(w). Furthermore, if E[D] = µ and supv: v 2=1 ExD[| x - µ, v |k]  1,  satisfies that

w, sup ExD | (w, x) - E [(w, x)] , v |k  1.
v: v 2 =1

The following lemma provides lower bounds for DP mean estimation. The first term is the non-private sample complexity, and is folklore for Gaussian mean estimation. The second term is Proposition 4 in [BD14].

11

Lemma 5.3. Let n, d  N and  > 0. For every (, 0)-DP algorithm A, there exists a distribution D on Rd with E[D] = µ and supv: v 2=1 ExD[| v, x - µ |k]  1, such that

2k-2

E[
X Dn ,A

A(X) - µ

2 2

]



d n

+

min

1,

d n

k

.

Combining Lemma 5.2 and Lemma 5.3 yields Theorem 5.1.

5.2 Convex Loss Functions
The convex case is more challenging than the strongly convex case, as it can not be reduced to DP mean estimation in a black-box fashion. Technically, we apply a DP version of Fano's inequality (Theorem 2 in [ASZ21]), based on the packing of distributions employed by [BD14]. The proof appears in Appendix C.8.

Theorem 5.4 (Convex case). Let n, d  N and  > 0. There exists a convex loss function  : W × Rd, such that for every (, 0)-DP algorithm (whose output on input X is denoted by wpriv = A(X)),
there exists a distribution D on Rd with w, supv: v 2=1 ExD | (w, x) - E [(w, x)] , v |k  1, which satisfies

k-1

XEDn,A[LD(wpriv) - LD(w))] 

d n

+



min

1,

d n

k

,

where w = arg minw LD(w).

References

[ACG+16]

Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM Conference on Computer and Communications Security, CCS '16, pages 308­318, New York, NY, USA, 2016. ACM.

[AFKT21] Hilal Asi, Vitaly Feldman, Tomer Koren, and Kunal Talwar. Private stochastic convex optimization: Optimal rates in 1 geometry. arXiv preprint arXiv:2103.01516, 2021.

[ASY+18]

Naman Agarwal, Ananda Theertha Suresh, Felix Xinnan X Yu, Sanjiv Kumar, and Brendan McMahan. cpSGD: Communication-efficient and differentially-private distributed SGD. In Advances in Neural Information Processing Systems 31, NeurIPS '18, pages 7575­7586. Curran Associates, Inc., 2018.

[ASZ21] Jayadev Acharya, Ziteng Sun, and Huanyu Zhang. Differentially private assouad, fano, and le cam. In Algorithmic Learning Theory, pages 48­78. PMLR, 2021.

[BD14]

Rina Foygel Barber and John C Duchi. Privacy and statistical risk: Formalisms and minimax bounds. arXiv preprint arXiv:1412.4451, 2014.

[BFTT19] Raef Bassily, Vitaly Feldman, Kunal Talwar, and Abhradeep Guha Thakurta. Private stochastic convex optimization with optimal rates. In Advances in Neural Information Processing Systems 32, NeurIPS '19, pages 11282­11291. Curran Associates, Inc., 2019.

12

[BGN21] Raef Bassily, Crist´obal Guzm´an, and Anupama Nandi. Non-euclidean differentially private stochastic convex optimization. arXiv preprint arXiv:2103.01278, 2021.

[BKSW19] Mark Bun, Gautam Kamath, Thomas Steinke, and Zhiwei Steven Wu. Private hypothesis selection. In Advances in Neural Information Processing Systems 32, NeurIPS '19, pages 156­167. Curran Associates, Inc., 2019.

[BS16]

Mark Bun and Thomas Steinke. Concentrated differential privacy: Simplifications, extensions, and lower bounds. In Proceedings of the 14th Conference on Theory of Cryptography, TCC '16-B, pages 635­658, Berlin, Heidelberg, 2016. Springer.

[BST14]

Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient algorithms and tight error bounds. In Proceedings of the 55th Annual IEEE Symposium on Foundations of Computer Science, FOCS '14, pages 464­473, Washington, DC, USA, 2014. IEEE Computer Society.

[Bub15]

S´ebastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends® in Machine Learning, 8(2­3):231­357, 2015.

[CM08]

Kamalika Chaudhuri and Claire Monteleoni. Privacy-preserving logistic regression. In Advances in Neural Information Processing Systems 21, NIPS '08, pages 289­296. Curran Associates, Inc., 2008.

[CMS11]

Kamalika Chaudhuri, Claire Monteleoni, and Anand D. Sarwate. Differentially private empirical risk minimization. Journal of Machine Learning Research, 12(29):1069­1109, 2011.

[DMNS06] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In Proceedings of the 3rd Conference on Theory of Cryptography, TCC '06, pages 265­284, Berlin, Heidelberg, 2006. Springer.

[DR16]

Cynthia Dwork and Guy N. Rothblum. Concentrated differential privacy. arXiv preprint arXiv:1603.01887, 2016.

[DRV10]

Cynthia Dwork, Guy N. Rothblum, and Salil Vadhan. Boosting and differential privacy. In Proceedings of the 51st Annual IEEE Symposium on Foundations of Computer Science, FOCS '10, pages 51­60, Washington, DC, USA, 2010. IEEE Computer Society.

[FKT20]

Vitaly Feldman, Tomer Koren, and Kunal Talwar. Private stochastic convex optimization: Optimal rates in linear time. In Proceedings of the 52nd Annual ACM Symposium on the Theory of Computing, STOC '20, New York, NY, USA, 2020. ACM.

[INS+19]

Roger Iyengar, Joseph P Near, Dawn Song, Om Thakkar, Abhradeep Thakurta, and Lun Wang. Towards practical differentially private convex optimization. In Proceedings of the 40th IEEE Symposium on Security and Privacy, SP '19, pages 299­316, Washington, DC, USA, 2019. IEEE Computer Society.

[JT14]

Prateek Jain and Abhradeep Guha Thakurta. (near) dimension independent risk bounds for differentially private learning. In Proceedings of the 31st International Conference on Machine Learning, ICML '14, pages 476­484. JMLR, Inc., 2014.

13

[KJ16]

Shiva Prasad Kasiviswanathan and Hongxia Jin. Efficient private empirical risk minimization for high-dimensional learning. In Proceedings of the 33rd International Conference on Machine Learning, ICML '16, pages 488­497. JMLR, Inc., 2016.

[KLL21]

Janardhan Kulkarni, Yin Tat Lee, and Daogao Liu. Private non-smooth empirical risk minimization and stochastic convex optimization in subquadratic steps. arXiv preprint arXiv:2103.15352, 2021.

[KLSU19] Gautam Kamath, Jerry Li, Vikrant Singhal, and Jonathan Ullman. Privately learning high-dimensional distributions. In Proceedings of the 32nd Annual Conference on Learning Theory, COLT '19, pages 1853­1902, 2019.

[KST12]

Daniel Kifer, Adam Smith, and Abhradeep Thakurta. Private convex empirical risk minimization and high-dimensional regression. In Proceedings of the 25th Annual Conference on Learning Theory, COLT '12, pages 25.1­25.40, 2012.

[KSU20]

Gautam Kamath, Vikrant Singhal, and Jonathan Ullman. Private mean estimation of heavy-tailed distributions. In Proceedings of the 33rd Annual Conference on Learning Theory, COLT '20, pages 2204­2235, 2020.

[KV18]

Vishesh Karwa and Salil Vadhan. Finite sample differentially private confidence intervals. In Proceedings of the 9th Conference on Innovations in Theoretical Computer Science, ITCS '18, pages 44:1­44:9, Dagstuhl, Germany, 2018. Schloss Dagstuhl­LeibnizZentrum fuer Informatik.

[RBHT12] Benjamin Rubinstein, Peter Bartlett, Ling Huang, and Nina Taft. Learning in a large function space: Privacy-preserving mechanisms for SVM learning. The Journal of Privacy and Confidentiality, 4(1):65­100, 2012.

[SCS13]

Shuang Song, Kamalika Chaudhuri, and Anand D Sarwate. Stochastic gradient descent with differentially private updates. In Proceedings of the 2013 IEEE Global Conference on Signal and Information Processing, GlobalSIP '13, pages 245­248, Washington, DC, USA, 2013. IEEE Computer Society.

[SSBD14] Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning - From Theory to Algorithms. Cambridge University Press, 2014.

[TS13]

Abhradeep Guha Thakurta and Adam Smith. Differentially private feature selection via stability arguments, and the robustness of the lasso. In Proceedings of the 26th Annual Conference on Learning Theory, COLT '13, pages 819­850, 2013.

[TTZ15]

Kunal Talwar, Abhradeep Thakurta, and Li Zhang. Nearly-optimal private LASSO. In Advances in Neural Information Processing Systems 28, NIPS '15, pages 3025­3033. Curran Associates, Inc., 2015.

[WJEG19] Lingxiao Wang, Bargav Jayaraman, David Evans, and Quanquan Gu. Efficient privacypreserving stochastic nonconvex optimization. arXiv preprint arXiv:1910.13659, 2019.

[WLK+17] Xi Wu, Fengan Li, Arun Kumar, Kamalika Chaudhuri, Somesh Jha, and Jeffrey Naughton. Bolt-on differential privacy for scalable stochastic gradient descent-based analytics. In Proceedings of the 2017 ACM SIGMOD International Conference on Management of Data, SIGMOD '17, pages 1307­1322, New York, NY, USA, 2017. ACM.

14

[WXDX20] Di Wang, Hanshen Xiao, Srinivas Devadas, and Jinhui Xu. On differentially private stochastic convex optimization with heavy-tailed data. In Proceedings of the 37th International Conference on Machine Learning, ICML '20, pages 10081­10091. JMLR, Inc., 2020.

[WYX17] Di Wang, Minwei Ye, and Jinhui Xu. Differentially private empirical risk minimization revisited: Faster and more general. In Advances in Neural Information Processing Systems 30, NIPS '17, pages 2722­2731. Curran Associates, Inc., 2017.

[WZGX21] Di Wang, Huangyu Zhang, Marco Gaboardi, and Jinhui Xu. Estimating smooth glm in non-interactive local differential privacy model with public unlabeled data. In Algorithmic Learning Theory, pages 1207­1213. PMLR, 2021.

[ZJS19]

Banghua Zhu, Jiantao Jiao, and Jacob Steinhardt. Generalized resilience and robust statistics. arXiv preprint arXiv:1909.08755, 2019.

[ZMH21] Huanyu Zhang, Ilya Mironov, and Meisam Hejazinia. Wide network learning with differential privacy. arXiv preprint arXiv:2103.01294, 2021.

A Useful Inequalities

Lemma A.1 (Chebyshev's Inequality). Let D be a distribution over R with mean µ, and k-th moment bounded by M. Then the following holds for any a > 1.

P [|X
XD

-

µ|

>

aM

1 k

]



1 ak

.

Lemma A.2 (Gaussian Empirical Variance Concentration). Let (X1, . . . , Xm)  N (0, 2) be in-

dependent.

If

m



8 2

ln(2/),

for





(0, 1),

then

P

1 m

m i=1

Xi2

-

2

> 2

 .

Lemma A.3 (Laplace Concentration). Let Z  Lap(t). Then P[|Z| > t · ln(1/)]  .

Lemma A.4 (Concentration in High Dimensions [ZJS19]). Let D be a distribution over Rd with

mean 0 and k-th moment bounded by M. Then the following holds for any t > 0,

k

P[
X D

X

2 > t]  M

d t

.

B Algorithms under Axis-Aligned Bounded Moments
In this section, we discuss the alternate moment condition of [WXDX20], which we denote as axis-aligned bounded moments, and sketch that our approach can achieve better rates than their algorithms. Roughly speaking, a distribution D over Rd with mean µ has axis-aligned bounded k-th moments if for all i  [d], we have
E[| X - µ, ei |k]  1,

15

where ei is the standard unit basis vector in coordinate i. Compare this with Definition 2.11, which bounds the k-th moment in every projection, rather than just the coordinate axes. Since [WXDX20] restrict their attention to the case k = 2, as do we for the sake of this comparison. Furthermore, to allow for the cleanest comparison, we elide the dependence on parameters M and R.
The first observation is that axis-aligned bounded moments is a weaker notion that that of Definition 2.11. This can be easily observed by considering the distribution which places probability 0.5 on the all 0's vector and probability 0.5 on the all 1's vector. In any axis-aligned direction this distribution is Bernoulli with parameter 0.5, thus satisfying the definition of axis-aligned bounded moments. However, in the direction connecting these two points, the bound will only satisfy Definition 2.11 with  = d. Since a weaker assumption requires an algorithm to handle a broader set of inputs, we expect the rates for DP SCO with axis-aligned bounded moments to degrade in comparison to general bounded moments. Nonetheless, we will show that the a similar approach can outperform [WXDX20], simply by changing the mean estimation algorithm.
Specifically, we will modify Algorithm 3. The main difference will be, rather than truncating and noising the dataset based on an 2-ball, we will instead use an -ball. This is a natural alteration given the axis-aligned nature of the alternative moment bound. Given the tools we have already developed, this algorithm can be succinctly described as the invocation of Algorithm 3 on each one-dimensional (d = 1) axis-aligned problem with privacy parameter /d. Composition of CDP gives that this overall procedure will be -CDP. For dimension i, Theorem 4.1 tells us that

|E [µi] - µi|  O

1 n

+

1 

E (µi - µi)2  O

1 n

+

 2d n2

+

12 

By combining these coordinatewise guarantees, we get a -DP mean estimation algorithm under axis-aligned bounded moments with the following guarantees, which will be plugged into our SCO framework.

E [µ] - µ 2  O



d n

+

d 

E



µ-µ

2 2



O

d n

+

 2d2 n2

+




2

d



To derive bounds for the strongly convex and smooth case, we employ Theorem 3.2, setting

G2 to be the above variance bound and balancing the terms by setting truncation parameter

=

n2  d

1/4
.

This

produces

an

expected

excess

risk

bound

of

O~

d3/n2 , which roughly corresponds

to an (, )-differential privacy guarantee of O~

d3/2 n

. This improves upon the O~

d2 2n

guarantee

of [WXDX20] in both d and .

For the convex case, we use Theorem 3.1, setting B and G2 to be the above bias and variance

bounds.

We

set

parameters

T

=

1/2 nR2 d3/2

,



=

n2 d

1/4
,

=

M RT

,

giving

an

excess

risk

bound

of

O~

d3/n2 . Again, this roughly corresponds to a guarantee of O~

d3/2 n

under (, )-differential

privacy, improving upon the O~

d2 1/3 2n

bound of [WXDX20].

16

C Omitted Proofs

C.1 Proof of Theorem 3.1

We

let

LD(wt)

=

E [(wt, x)].
xD

By

Assumption

1,

for

all

t,

LD(wt) 2 =

 E [(wt, x)] =

xD

2

E [(wt, x)]  R.

xD

2

Let w^t = wt-1 - LD(wt-1), and wt denotes its projection to W. By the convexity of LD(·) (see, e.g., Section 14.1.1 in [SSBD14]), we have

E
A,X Dn

LD(wpriv) - LD(w)

=E
A,X Dn

LD

1 T

T

wt

t=1

- LD(w)

E
A,X Dn

1 T

T

LD wt

- LD(w)

(2)

t=1

=E
A,X Dn

1 T

T

LD wt - LD(w)

t=1

E
A,X Dn

1 T

T

1 

LD(wt), wt - w

(3)

t=1

where (2) is by the Jensen's inequality and (3) is by the convexity of LD. Continuing the proof,

E
A,X Dn

LD(wpriv) - LD(w)

E
A,X Dn

1 T

T

1 

LD(wt) + LD(wt) - LD(wt), wt - w

t=1

=E
A,X Dn

1 T

T

LD(wt) - LD(wt), wt - w

+E
A,X Dn

1 T

T

1 

LD(wt), wt - w

.

t=1

t=1

We bound the first term, note that wt - w 2  M , and E[LD(w)] - LD(w) 2  B,

E
A,X Dn

1T T t=1

LD(wt) - LD(wt), wt - w

=

1 T

T

LD (wt )

-

E
A,X Dn

LD(wt)

, wt - w

 BM.

(4)

t=1

17

Then we move to the second term.

E
A,X Dn

1 T

T

1 

LD(wt), wt - w

t=1

=E
A,X Dn

1T T
t=1

1 2

-

wt - w - LD(wt)

2
+

wt - w 2

+

 2

LD(wt) 2

(5)

=

1 T

T

1 2

-E

w^t+1 - w 2 + E

wt - w 2

+

 2

·

E

LD(wt) 2

(6)

t=1



1 T

T t=1

1 2

-E

wt+1 - w 2 + E

wt - w 2

+

 2

·

E

LD(wt) 2

(7)

=

1 2T

-E

wT - w 2 + E

w1 - w 2

+

 2T

·E

T t=1

LD(wt) 2

(8)



M2 2T

+

 2T

·E

T t=1

LD(wt) 2 .

(9)

where (5) comes from the fact that a, b  Rd,

a, b

=

1 2

a

2 2

+

b

2 2

-

a-b

2 2

, and (6) is by

the updating rule, (7) comes from the fact that w^t+1 - w 2  wt+1 - w 2, and (8) is by the telescopic sum.

Finally, for all t  [T ],

E LD(wt) 2 = E LD(wt) - LD(wt) + LD(wt) 2



1 2

·

E

LD(wt) - LD(wt) 2 + LD(wt) 2



G2

+ 2

R2 ,

(10)

where

we

note

that

E[

LD(w) - LD(w)

2 2

]



G2,

and

LD(wt) 2  R2.

We conclude the proof by combining (4), (9), and (10).

C.2 Proof of Theorem 3.2

The argument is broadly similar to the proof of Theorem 5 in [WXDX20], albeit with some minor modifications.
Let w^t = wt-1 - LD(wt-1). Now we have
w^t - w 2 = wt-1 - LD(wt-1) - w 2  wt-1 - LD(wt-1) - w 2 +  LD(wt-1) - LD(wt-1) 2.

It should be noticed that, the second term is bounded by G in expectation, since E[ LD(wt-1) - LD(wt-1) 2]  G. For the first term, by the coercivity of strongly convex functions (Lemma 3.11 in [Bub15])

wt-1 - w, LD(wt-1)



ab a+b

wt-1 - w

2 2

+

a

1 +

b

LD (wt-1 )

2 2

18

and

by

taking



=

1 a+b

we

have

wt-1 - LD(wt-1) - w

2 2

=

wt-1 - w

2 2

+

LD (wt-1 )

2 2

-

2

wt-1 - w, LD(wt-1)



1

-

2ab (a + b)2

wt-1 - w

2 2

-

(a

1 +

b)2

LD (wt-1 )

2 2



1

-

2ab (a + b)2

wt-1 - w

2 2

.

Now

using

the

inequality

 1-x

1-

x 2

we

combine

two

terms

together

to

have

E[ w^t - w 2] 

1

-

(a

ab + b)2

E[

wt-1 - w

2]

+

a

G +

b.

Recall that wt is the projection of w^t on W, which implies wt - w 2  w^t - w 2. Therefore,

E[ wt - w 2] 

1

-

(a

ab + b)2

E[

wt-1 - w

2]

+

a

G +

b.

After T multiplications and simplifying the geometric series,

E[ wT - w 2] 

1

-

(a

ab + b)2

T

M

+

(a

+ b)2 ab

G a+

b.

Letting T = log

(a+b)G ab

/ log

a2 +b2 +ab (a+b)2

,

E[

wT

- w

2] 

(a + b)(M ab

+ 1)G .

Since LD(w) is b-smooth, we have

E
A,X Dn

LD(wT )

-

LD(w)



b 2

E[

wT - w

2 2

]



(a

+

b)2(M + 2a2b

1)2G2 .

which concludes the proof.

C.3 Proof of Theorem 4.2

We first prove the privacy guarantees. Note that Algorithm 2 can be viewed as a post-processing

of the private histogram. Therefore, it naturally satisfies -CDP.

Now we analyze the accuracy.

We define the following two events. We will show that it is highly likely that neither of them

happens.

Let S1 =

|i



[n]

:

Xi

/

[µ

-

, µ

+

 ]|



1 5

n

.

Given x  D, by Lemma A.1,

Pr (|x

- µ|



10)



1 10k

.

Note that   10, and k  1, we have

Pr (|x

-

µ|



)



1 10

.

19

Now we can bound the number of samples which fall outside of [µ - , µ +  ]. By the Hoeffding's

inequality,

Pr (S1) = Pr

|i



[n]

:

Xi

/

[µ

-

, µ

+

 ]|



1 5

n



e-

1 50

n

.

the

Then we define the second j-th bucket in the private

event. Given j histogram. We



[

R 

have

+ 2], that

let j

Nj denote



[

R 

+

2],

the Gaussian noise

Nj



N (0,

1 

).

added

to

Let

S2

=

{j



[

R 

+ 2], |Nj|



n 16

}.

Now

we

bound

the

probability

of

event

S2.

By

the

Gaussian

tail

bound,

j



[

R 

+

2],

Pr

|Nj |



n 16



2

·

e-

n2 1024

.

Finally

by

union

bound,

Pr (S2)  2 ·

R 

+

2

·

e-

n2 1024

.

In the following proof, we suppose neither event happens. We note that the remaining proof is

almost the same as the original analysis in [KSU20], and we keep it here for completeness. First,

there must exist a bucket containing

bucket that contains at least 0.5 · (1 the maximum number of points has

-

1 5

)n



n 4

to contain

samples, which implies that the

at

least

n 4

points.

According

to

the

definition

of

event

S2,

the

noise

added

to

each

bucket

is

no

more

than

1 16

n.

Therefore,

the

noisy

value

for

the

largest

bucket

has

to

be

at

least

3 16

n.

Since

all

these

points

lie

in

a

single

bucket,

and

include points that are not in the tail of the distribution, the mean lies in either the same bucket,

or in an adjacent bucket because the distance from the mean is at most  . Hence, the interval

[a - 2, b + 2 ]

contains

the

mean

and

at

least

4 5

n

points.

Note

that

the

interval

has

length

6

and

the mid-point is at most 3 away from µ.

C.4 Proof of Theorem 4.1

We separate the proofs of privacy and accuracy. We note that the proof of privacy is exactly the

same as the original analysis in [KSU20], and we omit it for simplicity.

Next we show the accuracy guarantees. In the first step, the objective of the algorithm is to

find a centre ci S1i is the event

for each that the

dimension, such that |ci i-th CDPRE succeeds.

- µi| Note

 30. Given i  that suppose S1

[d], let S1 happens,

= c

i[d]S1i , -µ 2 

where 30 d.

Furthermore, by Theorem 4.2 and union bound,

Pr (S1)  1 - 2d ·

R 10

+

2

· e-

n2 1024d

+

e-

n 50

.

We

define

S2

to

be the

event

that





3 4

n.

Suppose

S1

happens,

by

Lemma

A.4,

Pr

x

/

B

 d

(c)

|

S1



1 10

,

since



 60.

By

the

Hoeffding's

inequality,

Pr (S2

|

S1)



1

-

e-

1 50

n.

Therefore,

Pr (S2  S1)  1 - (2d + 1) ·

R 10

+

2

· e-

n2 1024d

+

e-

n 50

.

(11)

We prove the first part. By triangle inequality, we are able to separate E [µ] - µ 2 into the sum of two terms.

E [µ] - µ 2  Pr (S1  S2) · E µ - µ S1  S2 2 + (1 - Pr (S1  S2)) · E µ - µ | S1  S2 2 .

20

Now we bound the first term. Note that

E [µ - µ | S1  S2]

2

 E[ µ-µ 

2

| S1  S2]



 E

1 

Xi - µ

Xi



B

 d

(c)







3 4

n



i[]

2 i[]

2

d n

+

3

·

C 

k-1
,

(12)

where the first inequality comes from Jensen's inequality, and the last inequality comes from Lemma 4.4 in [KSU20] and Lemma 4.5.
Finally, by combining (11) and (12), we have

E [µ] - µ 2  2

d n

+

3

·

C 

k-1
+ (4d + 2) · R ·

R 10

+

2

· e-

n2 1024d

+

e-

n 50

,

which concludes the first part. Next we prove the second part. Note that

E

µ-µ

2 2

 Pr (S1  S2) · E

µ-µ

2 2

|

S1



S2

+ 4(1 - Pr (S1  S2))R2

E

µ-µ

2 2

|

S1



S2

+ 4(1 - Pr (S1  S2))R2.

(13)

We bound the first term. By triangle inequality,

E

µ-µ

2 2

| S1  S2

E

µ - E [µ | S1  S2]

2 2

| S1  S2

+

µ - E [µ | S1  S2]

2 2



32 2d2 9n2

+

8d n

+

18

·

C 

2k-2
,

(14)

where the last inequality comes from the fact that E [µ | S1  S2] - µ 2  2

d n

+

3

·

C 

k-1.

Finally, we note that Pr (S1  S2)  1 - (2d + 1) ·

R 10

+

2

· e-

n2 1024d

+

e-

n 50

. Combined

with (13) and (14), we have the intermediate results:

1.

E [µ] - µ 2  2

d n

+

3

·

C 

k-1 + (4d + 2) · R ·

R 10

+

2

· e-

n2 1024d

+

e-

n 50

.

2. E

µ-µ

2 2



8d n

+

32 2d2 9n2

+

18 ·

C 

2k-2 + 4R2 · (2d + 1) ·

R 10

+

2

· e-

n2 1024d

+

e-

n 50

.

 By elementary calculations, we note that when n  O dlog(R) + log R ,

R3d max

e-

n 50

,

e-

n2 d



d n

,

which eliminates all exponential terms above and concludes the proof.

21

C.5 Proof of Theorem 4.7

Theorem 4.1 guarantees the following lemma.

Lemma C.1. Consider Algorithm 1 instantiated with CDPHDME

 T

,

cle, where we set  =

n d

1/k
Under Assumption 1 and with n  O

following holds for all w  W:

n 1/k d

as MeanOra-



dTlog(R) + log R , the

E[LD(w)] - LD(w) 2  O

k-1

d n

+

d n

k

,

and

E[

LD(w) - LD(w)

2 2

]



O

d n

+

 2d2T n2

+

1 2k-2 

,

where LD(w) is the estimated gradient in Algorithm 1.

The proof now follows by choosing the right  and T in Theorem 3.1. To balance the first two

terms,

M2 T

+

 2

R2,

in

Theorem

3.1,

we

let



=

M RT

.

Suppose T

=

1 4 log R

·

n d

2k-2



k

, note that  =

2M

log R R

·

k-1

dn

k

and T

=

M 2R log R

·

n d

k-1
k , we have

M2 T

=

O

MR

log R ·

dn

k-1 k

,

k-1

 2

R2

=

O

MR

log R ·

dn

k

,

k-1

BM = O M · dn

k
+M ·

d n

,



G2

=

2M

log R R

  2M log R
R

dn dn

k-1
k · G2

k-1 k
·O

d n

+

1 

2k-2

+

 2d2T n2



k-1

2k-2

k-1

=

2M

log R R

dn

k
·O

d n

+

nd 

k

+

M R

·O

nd 

k

k-1
 O M R log R · dn k .

To justify the last inequality, observe that (3) in the proof of Theorem 3.1 implies that M R is a vacuous bound on the excess risk, and thus to obtain a non-trivial bound we implicitly have the

22

condition that the LHS of Theorem 4.7 is bounded by M R, i.e.,

log R · O

k-1

d n

+

dn

k

1

and thus

2k-2

O

d n

+

d n

k



1 log

R

.

Combining this with R = (1) from Assumption 1 justifies the inequality. Putting the various terms together completes the proof.

C.6 Proof of Theorem 4.8

Note that Theorem 4.1 immediately guarantees the following accuracy when CDPHDME is instantiated as MeanOracle in SCOF:

Lemma C.2. Consider Algorithm 1 instantiated with CDPHDME 

 T

,

n dT

1/k

as MeanOracle.

Under Assumption 1, and with n  O dTlog(R) + log R , the following holds for all w  W:

 E[ LD(w) - LD(w) 2]  O 

d n

+

 dT n

 k-1
k
,

where LD(w) is the estimated gradient in Algorithm 1. The proof then follows by Theorem 3.2.

C.7 Proof of Lemma 5.2

Let

x



D,

and

(w; x)

=

1 2

w-x

22.

Note

that

w

=

arg

min

LD (w)

=

E [x]
xD

=

µ.

Further

using

the expansion

a-b

2 2

=

a

2 2

-

2

a, b

+

b 22,

LD(w) - LD(w) =

1 E[ 2 xD

w-x

2 2

-

w - x 22]

=1 E 2 xD

w

2 2

-

2

w, x

+

x

2 2

-

w

2 2

+

2

w, x

-

x

2 2

=

1 2

w

2 2

-

2

w, w

-

w

2 2

+

2

w, w

=

1 2

w

2 2

-

2

w, w

+

w

2 2

=

1 2

w - w

2 2

Notice that  is strongly convex and the expected risk of wpriv is

X

E
Dn

,A[LD

(wpriv

)]

-

LD (w )

=

E
X Dn ,A

1 2

wpriv - µ

2 2

,

which implies the result.

23

Now we prove the second half, note that (w, x) = w - x, and E [(w, x)] = w - µ,
sup ExD | v, (w, x) - E [(w, x)] |k
v: v 2=1
= sup ExD | v, w - x - (w - µ) |k
v: v 2=1
= sup ExD | v, x - µ |k  1.
v: v 2=1

C.8 Proof of Theorem 5.4

We first prove the private term (the second term) in Theorem 5.4.

We adopt the packing set defined in the proof of Proposition 4 in [BD14]. Given   V, with

 2 = 1, let Q = (1 - p)P0 + pP for some p  [0, 1], where P0 is a point mass on {D = 0} and

P is a point mass on {D = p-1/k}.

Given Q, we define µ  Rd to be the mean of Q, i.e., µ = ExQ [x]. Additionally, we define

w to be its normalization, i.e., w =

µ µ

.
2

Note that w

is in the same direction as µ, with

w 2 = 1.

As a corollary of standard Gilbert-Varshamov bound for constant-weight codes (e.g., see Lemma

6 in [ASZ21]), there exists a set V with cardinality at least

V

2



2

d 8

,

with

 2 = 1 for all   V,

and with

 - 

2

1 2

for all  =   V.

Suppose p = min

1,

d n

, we first compute the norm of µ.

Note that   V,

µ 2 is the

same, which is denoted by µ 2.

k-1

µ 2 = ExQ [x] 2 = min

1,

d n

k

:= µ 2 .

Without loss of generality, we assume the parameter space W 2 = 1, which is a unit ball. Then we define the loss function (w; x). Given   V, and x  Q, we let
(w; x) = - w, x ,

and

LQ

(w)

=

E [(w;
xQ

x)]

=

-

w, µ

.

Let x  Q. Note that (w, x) = -x, and E [(w, x)] = -µ,

sup ExQ | v, (w, x) - E [(w, x)] |k
v: v 2 =1

= sup ExQ | v, -x + µ) |k
v: v 2 =1

 ExQ

x - µv

k 2

p·

p-1/k

k
= 1.

Now we are able to bound the error of SCO.

24

E

LD

(wpriv

)

-

min
w^W

LD

(w^)



1 |V |

E

LQ

(wpriv

)

-

min
w^W

LQ

(w^)

V



1 |V |

E

V

µ µ

2

,

µ

- wpriv, µ

=

1 |V |

E µ 2 - wpriv, µ

V



1 |V |

E

1 2

·

µ 2·

wpriv - w

2 2

,

V

(15) (16)
(17)

where (15) comes from the fact that the worst case loss is no smaller than the average loss, (16)

comes from w = argminLQ (w^), and (17) comes from the fact that
w^W

wpriv 2  1, and

w 2  1.

Let w^priv := arg minw w - wpriv 2. By triangle inequality,

w - w^priv 2  w - wpriv 2 + w^priv - wpriv 2  2 w - wpriv 2 .

Therefore,

E LD(wpriv) - min LD(w^)
w^W



µ 8

2

·

1 |V |

·

E

V

w^priv - w

2 2

.

Note

that

|V |



2

d 8

.

Furthermore,



= ,

w - w 2 = (1); dTV(w, w ) = p, indicating

that there exists a coupling between w and w with a coupling distance np. By DP Fano's

inequality (Theorem 2 in [ASZ21]), it can be shown that

1 |V |

E

w^priv - w

2 2

= (1).

V

Thus,

k-1

E

LD

(wpriv

)

-

min
w^W

LD

(w^)

 (1) ·

µ 2=

min

1,

d n

k

.

(18)

Now we prove the first term. We generally follow the lower bound proof of -DP estimating

Gaussians [ASZ21]. p  [0, 1]. Similarly,

Given   {0, 1}d, we

we define w =

µ µ

2

.

define

Q

=

N (µ, Id),

where

µ

=

p d

· ,

for

some

As a standard Gilbert-Varshamov bound for constant-weight codes (e.g., see Lemma 6 in [ASZ21]),

there exists a set V with cardinality at least

V

2



2

d 8

,

with

| |1

=

d 2

for

all





V,

and

with

dHam(, )



d 2

for

all



=





V.

Suppose p = min 1,

d n

, we can compute the norm of the distribution mean.

Note that

| |1

=

d 2

,



µ 2 =

2 2

min

1,

d n

:= µ 2 .

25

By a similar argument with the private case, it can be shown that

E

LD (wpriv )

-

min
w^W

LD(w^)



µ 8

2

·

1 |V |

·

E

V

w^priv - w

2 2

,

where w^priv := arg minw w - wpriv 2. Note that this is indeed a multi-way classification problem, where w 's are well-separated. By
classical Fano's inequality,

Thus,

1 |V |

E

w^priv - w

2 2

= (1).

V

E LD(wpriv) - min LD(w^)
w^W

 (1) · µ 2 = 

min

1,

d n

.

(19)

Combining (18) and (19), we conclude the proof.

26

