arXiv:2106.01792v1 [stat.ME] 3 Jun 2021

Conformal Prediction Bands for Multivariate Functional Data
Jacopo Diquigiovanni1,*, Matteo Fontana2,3, Simone Vantini2
1 Department of Statistical Sciences, University of Padova, Italy 2 MOX - Department of Mathematics, Politecnico di Milano, Italy 3 now at Joint Research Centre - European Commission, Ispra (VA), Italy
* jacopo.diquigiovanni@phd.unipd.it
Abstract
Motivated by the pressing request of methods able to create prediction sets in a general regression framework for a multivariate functional response and pushed by new methodological advancements in non-parametric prediction for functional data, we propose a set of conformal predictors that produce finite-sample either valid or exact multivariate simultaneous prediction bands under the mild assumption of exchangeable regression pairs. The fact that the prediction bands can be built around any regression estimator and that can be easily found in closed form yields a very widely usable method, which is fairly straightforward to implement. In addition, we first introduce and then describe a specific conformal predictor that guarantees an asymptotic result in terms of efficiency and inducing prediction bands able to modulate their width based on the local behavior and magnitude of the functional data. The method is investigated and analyzed through a simulation study and a real-world application in the field of urban mobility. Keywords: Functional data; Conformal Prediction; Prediction band; Exact prediction set; Distribution-free prediction set; Finite-sample prediction set
1 Introduction
Functional Data Analysis (FDA, Ramsay and Silverman 2005) is now a fairly established, but still very ebullient field of statistics whose goal is to develop theory and methods to treat datasets composed of smooth functions. Since the first seminal paper by Jim O. Ramsay (Ramsay 1982), many standard multivariate tools have been translated to the functional realm: among those Functional Principal Component Analysis (Ramsay and Silverman 2005, Chapter 10), Functional Linear Regression (Ramsay and Silverman 2005, Chapter 12) and functional boxplots (Sun and Genton 2011), just to give a very partial and non-exhaustive list.
A crucial challenge in FDA is the issue of uncertainty quantification in prediction. Intuitively, we are interested in creating prediction sets, namely subsets of the sample space including a new functional observation with a certain nominal confidence level
1

1 - . Only very recent works in FDA provide some knowledge into this theoretical

(but yet full of applied repercussions) issue, all of them focusing on the univariate

setting (i.e. a framework in which the functional observation consists of a single real-

valued function defined over a domain). These approaches can be classified in three

groups: the first one consists of works principally based on parametric bootstrapping

techniques (e.g., Degras 2011, Cao et al. 2012), the second one is characterized by the

application of dimensionality reduction techniques to manage the naturally infinite

dimensionality (e.g., Hyndman and Shahid Ullah 2007, Antoniadis et al. 2016). These

first two groups carry obvious drawbacks since they are either based on not easily

provable distributional assumptions and/or on asymptotic results. In addition, the

first class of approaches is computationally demanding, whereas the second one relies

on the approximations induced by basis projection. The third group is based on

a novel approach to forecasting in the framework of Conformal Prediction (CP)

(Diquigiovanni et al. 2021). This approach is able to output either exact or valid

prediction bands under minimal distributional assumptions and in an efficient way,

thus bypassing the methodological shortcomings identified in the previous literature.

However, this is done in the setting of univariate i.i.d. functional data. The objective

of the present work is to build from that contribution by extending the method to

multivariate functional data and to a regressive framework.

Formally, we will consider independent and identically distributed regression

pairs Z1, . . . , Zn  P , with Zi = (Xi, Y i) consisting of a multivariate functional

response variable Y i and a set of (not necessarily scalar) covariates Xi i = 1, . . . , n.

Let Y i = (Yi1, Yi2, . . . , Yip) be a multivariate random function such that its j-th

component Yij (j = 1, . . . , p) is a random function taking values in L(Tj), which

is the family of limited functions y : Tj  R with Tj closed and bounded subset of

Rdj , dj  N>0. For the sake of brevity, later in the discussion we will indicate the

space L(T1) × · · · × L(Tp) in which Y i takes values as

p j=1

L

(Tj

).

Note

that

the framework considered is extremely wide since both the domain Tj and the image

of Yij are allowed to be very different when j varies. Xi = (Xi1, Xi2, . . . , Xip) is a

set of covariates such that its element related to the j-th component Xij (which is a

set of covariates itself) belongs to a measurable space and can be very general: for

example, Xij can be the usual vector of predictors, or it can be a set of functional covariates allowing for a functional-on-functional regression model, or it can contain

both scalar and functional predictors. Let µj(xij) = E(Yij|Xij = xij) denote the regression function for the j-th component of the i-th observation, and consistently

with this notation let us define the scalar value [µj(xij)](t) = E(Yij(t)|Xij = xij). The aim of the article is to build a procedure able to output exact (or at least

valid) multivariate functional prediction bands under no assumptions on P and

µ1(·), . . . , µp(·) other than i.i.d. regression pairs. A multivariate functional prediction

band is a specific kind of prediction set that can be defined, consistently with the

well-known definition of univariate functional prediction band (L´opez-Pintado and

Romo 2009, Degras 2017), as

p
y = (y1, . . . , yp)  L(Tj) : yj(t)  Bj(t), j  1, . . . , p, t  Tj,
j=1

with Bj(t) interval j, t. Prediction bands are so relevant in the functional set prediction framework due to their conceptual simplicity and because they can be plotted in parallel coordinates (Inselberg 1985). A detailed discussion of the topic

2

is provided by Diquigiovanni et al. (2021). For the sake of simplicity, later in the discussion the term prediction band will be used to indicate a multivariate functional prediction band, unless otherwise specified.
The terms valid prediction set and exact prediction set are instead used to indicate the coverage ensured by a prediction set.

Valid prediction set Consistently with the notation of Lei et al. (2018), a valid prediction set for Zn+1 = (Xn+1, Y n+1) - which is independent from and identically distributed to Z1, . . . , Zn - is the set Cn,1- based on Z1, . . . , Zn such that

P (Y n+1  Cn,1- (Xn+1))  1 - 
for any significance level   (0, 1), with Cn,1- (x) = {y  Cn,1-}

(1)

p j=1

L (Tj )

:

(x,

y)



Exact prediction set An exact prediction set for Zn+1 = (Xn+1, Y n+1) - which is independent from and identically distributed to Z1, . . . , Zn - is the set Cn,1- based on Z1, . . . , Zn such that

P (Y n+1  Cn,1- (Xn+1)) = 1 - 

(2)

for any significance level   (0, 1) and with Cn,1- (x) defined as above. It is important to notice that the left side of Inequality (1) and of Equality (2)
refers to the unconditional coverage reached by the prediction set, i.e. the probability is taken over the i.i.d. draws Z1, . . . , Zn+1. In view of this, later in the discussion the term coverage will be used to indicate the unconditional coverage and the term empirical coverage will be used to indicate an estimate of the coverage.
The article is organized as follows: in Section 2 we introduce the CP framework; in Section 3 we present the method developed; in Section 4 we discuss three simulation studies aimed at investigating different aspects of the method; in Section 5 we apply our method to a real-world application; in Section 6 we provide an overview of the main findings and sketch directions of future research.

2 The Conformal Prediction Framework
Conformal Prediction is an innovative method to build either valid or exact prediction sets under no assumptions other than exchangeable data (Vovk et al. 2005). Moreover, the CP framework ensures that valid/exact prediction sets are obtained regardless the sample size n (i.e. not only asymptotically), a fact that allows Conformal Prediction to be used in an extremely wide range of different scenarios. In this article we consider the Semi-Off-Line Inductive Conformal framework, also known as Split Conformal (Papadopoulos et al. 2002), which represents a computationally and methodologically convenient alternative to the original Transductive framework. Split Conformal approach is characterized by two sub-frameworks: Non-Smoothed Split Conformal framework and Smoothed Split Conformal framework 1. The two procedures are defined below.
1since the term `Split Conformal' itself is used to indicate `Non-Smoothed Split Conformal', later in the discussion we will use the following two terms to indicate the two sub-frameworks: Split Conformal, Smoothed Split Conformal
3

Split Conformal method Let z1, . . . , zn be realizations of Z1, . . . , Zn, and let

{1, . . . , n} be randomly splitted into two sets I1, I2 of size m and l respectively

such that n = m + l, m, l  N>0. Let us also define the set {zh : h  I1} as

training set, the set {zd : d  I2} as calibration set and the nonconformity measure,

which represents the key aspect of Conformal Prediction, as any measurable function

A({zh : h  I1}, z) taking values in R¯ . The Split Conformal approach defines the

prediction set for Y n+1 as Cn,1- (xn+1) := y 

p j=1

L (Tj )

:

y

>



, with

y

:=

|{d



I2



{n

+ 1} : l+1

Rd



Rn+1}| ,

and nonconformity scores Rd := A({zh : h  I1}, zd) for d  I2, Rn+1 := A({zh : h  I1}, (xn+1, y)). Intuitively, nonconformity score Rd (Rn+1 respectively) scores how different zd ((xn+1, y) respectively) is from the training set, and so y indicates the conformity of (xn+1, y) to the training set compared to the conformity of the elements of the calibration set to the same training set (i.e. it is the p-value of

(xn+1, y), Vovk et al. 2005). The Split Conformal method is particulary appealing since it outputs - by

construction - finite-sample, valid prediction sets by only assuming exchangeable

data. In fact, Diquigiovanni et al. (2021) show that, under the mild assumption that

{Rd : d  I2} have a continuous joint distribution (an assumption that we will made hereafter), the coverage ensured by Split Conformal prediction set is equal to an

easy-to-compute fixed quantity, i.e. P (Y n+1  Cn,1- (Xn+1)) = 1 -

(l+1) l+1

,

and

it

is not only greater than or equal to 1 - . As a consequence, exact (and not only

valid) prediction sets are automatically obtained whenever (l + 1) = (l + 1).

Smoothed Split Conformal method Moving from the Split Conformal frame-

work, let us consider a single realization of a uniform random variable in [0, 1], called

n+1. The Smoothed Split Conformal approach defines the prediction set for Y n+1

as Cn,1-,n+1 (xn+1) := y 

p j=1

L (Tj )

:

y,n+1

>



, with

y,n+1

:=

|{d



I2

:

Rd

>

Rn+1}|

+

n+1 |{d  l+1

I2



{n

+

1}

:

Rd

=

Rn+1}| .

By introducing the element of randomization n+1, the Smoothed Split Conformal method outputs finite-sample, exact prediction sets by only assuming exchangeable

data (Vovk et al. 2005). In order to avoid redundancy, later in the discussion we will mainly focus on Split

Conformal method, but the generalization of the main findings of this article to the

Smoothed Split Conformal method is reported in Appendix A.

3 Proposed Methodology

3.1 Nonconformity Measure

Moving from Diquigiovanni et al. (2021), we propose the following nonconformity measure and nonconformity scores:

As({zh : h  I1}, z~) = sup
j{1,...,p}

sup y~j(t) - [µ^jI1(x~j)](t)

tTj

sj,I1 (t)

(3)

4

Rds = sup
j{1,...,p}

sup ydj(t) - [µ^jI1(xdj)](t)

tTj

sj,I1 (t)

, d  I2

(4)

Rns +1 = sup
j{1,...,p}

sup yj(t) - [µ^jI1(xn+1,j)](t)

tTj

sj,I1 (t)

with z~ = (x~, y~), y~ = (y~1, . . . , y~p), x~ = (x~1, . . . , x~p), yj the j-th component of y, [µ^jI1(xdj)](t) estimate of [µj(xdj)](t) based on {zh : h  I1}, sI1 = {sj,I1}pj=1 set of modulation functions with sj,I1 : Tj  R>0 a (strictly positive) function belonging to L(Tj) based on {zh : h  I1} called modulation function, and with the superscript
s introduced in order to emphasize the role of sI1. It is fundamental to notice that no specific assumptions are made on the estimators [µ^1I1(·)](t), . . . , [µ^pI1(·)](t) (considered in this case as random variables instead of observed values) since the Conformal

framework only requires the nonconformity scores Rds and Rns+1 to be computed on the basis of the observations belonging to the training set and on zd and (xn+1, y)
respectively. As a consequence, finite-sample, either valid or exact prediction sets

are obtained regardless the choice of the regression estimators, allowing Conformal

Inference to be satisfactorily performed also when the underlying model is completely

misspecified.

By considering the Split Conformal method and the nonconformity measure (3),

if   (0, 1/(l + 1)) then Cns,1-(xn+1) =

p j=1

L (Tj )

since

ys

is

always

greater

or

equal than 1/(l + 1). If   [1/(l + 1), 1) (representing the scenario on which we will

focus on hereafter), then

p
Cns,1-(xn+1) := y  L(Tj) : yj(t)  [µ^jI1(xn+1,j)](t) - ks · sj,I1(t),
j=1
[µ^jI1(xn+1,j)](t) + ks · sj,I1(t)] (5)
j  {1, . . . , p}, t  Tj ,

with ks the (l+1)(1-) th smallest value in the set {Rds : d  I2}. The computation

needed to find analytically Cns,1-(xn+1) is provided in Appendix A.1, together with

the

definition

of

Cs n,1-,n+1

(xn+1),

i.e.

the

Smoothed

Split

Conformal

prediction

set

induced by nonconformity measure (3).

From a practical point of view, first of all the observed sample z1, . . . , zn is used
to compute ks and s1,I1, . . . , sp,I1, and after that the prediction set is built around the regression estimates [µ^jI1(xn+1,j)](t), j  {1, . . . , p}. Despite the fact that no specific constraints on [µ^jI1(·)](t) are required by the Split Conformal framework, the choice of the regression estimators is fundamental in providing small prediction sets,

a key topic that will be investigated in Section 3.2: indeed, intuitively one is justified

in expecting prediction sets to be smaller when improved regression estimators are

chosen since they typically provide smaller nonconformity scores and so a smaller

value of ks (Lei et al. 2018). However, later in the discussion (and specifically in

Section 4 and Section 5) we will always consider the regression estimators as given by

the application at hand: in fact, our aim is to construct valid/exact prediction sets

in general and arbitrary prediction scenarios and not only in specific, well informed

frameworks.

5

Under the exchangeability assumption of the regression pairs and regardless the choice of sI1 and [µ^jI1(·)](t), the prediction sets induced by nonconformity measure (3)

· are either finite-sample valid (Split Conformal method) or finite-sample exact (Smoothed Split Conformal method) for any distribution P ;

· are in closed form;

· are bands;

· are scalable 2.

Note that nonconformity measure (3) ensures multivariate simultaneous bands,

i.e. bands guaranteeing the desired coverage globally (i.e. for the multivariate

random function Y n+1). Proper multivariate simultaneous coverage represents a leap forward with respect to univariate simultaneous coverage (i.e. coverage

holding for Yn+1,j) and pointwise coverage (i.e. coverage holding for Yn+1,j(t)). Conformal prediction bands for multivariate functional data (5) can be proven

to be a superset of the multivariate functional bands found by concatenating the

p univariate prediction bands obtained by applying the nonconformity measure suptTj yj(t) - [µ^jI1(xj)](t) /sj,I1(t) to the p components separately (Diquigiovanni et al. 2021), and also a superset of the multivariate functional bands found by

concatenating the pointwise prediction intervals obtained by applying the pointwise nonconformity measure yj(t) - [µ^jI1(xj)](t) /sj,I1(t) j  {1, . . . , p}, t  Tj (see Appendix A.1 for the proof). In other words, multivariate functional simultaneous

bands (5) ensure also both univariate simultaneous and pointwise validity, while the

converse is not guaranteed. The topic is further addressed by means of a simulation

study in Section 4.2.

Alongside the choice to base the nonconformity measure on the supremum metric,

the set sI1 of (strictly positive) modulation functions sj,I1 represents the core of our approach. First of all, one can notice that prediction bands induced by {sj,I1}pj=1 and by { · sj,I1}pj=1 coincide   R>0 (see Appendix A.1 for the proof), and so later

in the discussion we will consider, for any equivalence class, the set of modulation

functions such that

p j=1

Tj sj,I1(t)dt = 1. In the next Section, we detail the role

of sI1 by highlighting its impact on the efficiency (i.e. the size) of the prediction

bands and we propose a specific set of modulation functions able to guarantee an

asymptotic result in terms of efficiency.

3.2 The Choice of the Set of Modulation Functions
Intuitively, in addition to the appealing properties presented in Section 3.1, a prediction band should modulate its width over T1, . . . , Tp according to the local variability of the data. Specifically, the aim is to obtain prediction bands able to properly manage the fact that: focusing on the j-th component, the pointwise evaluations of functional data may be characterized by highly different variability
2Indeed, conditional on the computational cost required to calculate the regression estimates and the set of modulation functions (a set that can be chosen to be computationally parsimonious), and by keeping the ratio l/n fixed when n grows, the time required to compute ks (and therefore to output the prediction set) increases linearly with l, and so linearly with n

6

y1(t)

3

3

y2(t)

30

30

-3

-3

0.00

0.25

0.50

0.75

1.00

0.00

0.25

0.50

0.75

1.00

0

t

0

t

y2(t)

y1(t)

3

3

-30

-30

y2(t)

y1(t)

-3

-3

0.00

0.25

0.50

0.75

1.00

0.00

0.25

0.50

0.75

1.00

t

t

Figure 1. Split Conformal multivariate prediction band for Y 201 = (Y201,1, Y201,2) obtained by considering {s0j }2j=1 (at the top) and {sj,I1}2j=1 (at the bottom) as set of modulation functions. The dashed yellow lines represent the regression estimates.
 = 0.25, n = 200, m = l = 100.

when t  Tj varies; the p components may be characterized by different magnitude. In order to achieve these two purposes, a careful choice of a data-driven set of

modulation functions sI1 is recommended. In order to clarify this concept, let us consider the following example: let p = 2 with y1, . . . , y200 independent realizations of Y 1, . . . , Y 200 such that Yi1(t) = 1(t) + i1(t) and Yi2(t) = 2(t) + i2(t) (i =
1, . . . , 200, T1 = T2 = [0, 1]), with the systematic components defined simply as
1(t) = 1, 2(t) = 0 t  [0, 1] and the independent functional error components {i1}2i=001 ({i2}2i=001 respectively) obtained by means of a B-spline basis expansion (Fourier basis expansion respectively) with normally distributed random vectors as coefficients. In full generality, we consider [µ^jI1(xn+1,j)](t) = ^j(t), j = 1, 2, with ^1(t), ^2(t) the estimates (based on {zh : h  I1}) obtained by fitting the two
concurrent functional-on-functional linear models (Ramsay and Silverman 2005).

This example represents the simplest, almost trivial regression scenario which allows

to - hopefully - easily understand the crucial role of sI1, but the discussion presented hereafter naturally holds also when decidedly more complex regression functions

and regression estimators are taken into account. Figure 1 shows the multivariate

prediction band for Y 201 = (Y201,1, Y201,2) obtained by considering two different sets of modulation functions: the two panels at the top of the Figure 1 show the multivariate

prediction band obtained by not modulating (i.e. by setting s1,I1(t) = s2,I1(t) =

1/

2 j=1

|Tj

|



1

t



[0, 1]),

whereas

the

two

panels

at

the

bottom

of

the

same

Figure show the prediction band obtained by considering the two standard deviation

functions of the functional residuals as modulation functions (after normalization

in order to meet the condition

2 j=1

Tj sj,I1(t)dt = 1). In order to distinguish the

two sets of modulation functions, later in the discussion we will denote the first

7

set by s0 := {s0j }pj=1 (whose notation excludes the subscript I1 to remark its lack of dependence on the training set) and the second one by sI1 := {sj,I1}pj=1. The prediction sets are obtained by considering the Split Conformal framework and by setting  = 0.25, m = l = 100. Focusing on the two panels at the top of Figure 1, it is possible to notice that the two univariate prediction bands are far from desirable: specifically, the univariate prediction band related to Y201,1 is large along all the domain T1, whereas the one related to Y201,2 contains almost all the pointwise evaluations of the functional data in the low-variance parts of T2 but excludes many pointwise evaluations in the other, high-variance parts of the domain. In this specific case, the absence of a modulation process does not allow to take into account: first of all, the different variability of the data over T1 and T2 respectively; secondly, the different magnitude that characterizes the two components. In so doing, one is justified in expecting that a procedure based on {s0j }2j=1, although able to output a valid prediction band, may be of limited practical use in real applications. Vice versa, the set of modulation functions {sj,I1}2j=1 properly adapts the width of the prediction band according to the local variability of functional data, allowing for a meaningful, interpretable and useful prediction band.
Beyond these common-sense considerations, a criterion that is both reasonable and well-established in Conformal Prediction to discriminate between procedures able to guarantee validity is the minimization of the size of the prediction sets outputted (also known, in the Conformal framework, as maximization of efficiency, Balasubramanian et al. 2014): this choice is due to the fact that desirable prediction sets should include subsets of the sample space where the probability mass is highly concentrated (Lei et al. 2013). In the context of our article, the aim would be to find the nonconformity measure As({zh : h  I1}, ·) (and so, practically, the set of modulation functions sI1) inducing the smallest prediction bands. The first, fundamental step in assessing the size of a prediction band for multivariate functional data is the definition of the concept of `size', a nontrivial task if compared to the traditional univariate and multivariate statistical settings. By generalizing the definition given in Diquigiovanni et al. (2021) to the multivariate case, we define the size of a multivariate prediction band as the sum of the p areas between the upper and lower bound of the p univariate prediction bands:

p

Q(sI1) :=

2 · ks · sj,I1(t)dt = 2 · ks.

(6)

j=1 Tj

Since Q(sI1) is a random variable depending on Z1, . . . , Zn, the task of finding the set of modulation functions minimizing the risk functional E[Q(sI1)] is unfeasible in the case of no assumptions on P . A simplification of such a complex task consists of considering the quantity to be minimized ks( Q(sI1)) as an observed value depending on z1, . . . , zn instead of on Z1, . . . , Zn according to the empirical risk
minimization principle (Vapnik 1992). In so doing, the optimization problem is
certainly simplified, but its resolution still remains unfeasible due to the specific structure of ks. Indeed, ks is a specific empirical quantile of {Rds : d  I2}, and Rds (see Equation (4)) depends by construction both on the training set through {zh : h  I1} and on the calibration set through zd. Since by construction the set
of modulation functions sI1 depends only on the training set (as its dependence on the calibration set would imply not to obtain closed-form valid prediction bands),

8

no rule minimizing ks only by combining the elements of the training set (i.e. by
varying sI1) can be found for general z1, . . . , zn. In view of this, we propose an alternative, unconventional strategy to build a set
of modulation functions able to guarantee an asymptotic result in terms of efficiency. Specifically, the purpose is to find a couple of sets of functions (s¯I1, s¯cI1,I2) such that:
· s¯cI1,I2 := {s¯cj,I1,I2}pj=1 is a set of functions such that s¯cj,I1,I2 meets the definition of modulation function, but depends also on the calibration set through {zd : d  I2}, j  {1, . . . , p}

· prediction bands obtained by using s¯cI1,I2 as set of modulation functions are smaller than or equal to (in terms of Equation (6)) those induced by the set of modulation functions s0 for every possible value of n and for every possible
observed sample z1, . . . , zn

· s¯I1 = {s¯j,I1}pj=1 is a set of modulation functions such that s¯j,I1 is equal to s¯cj,I1,I2, but in which the dependence on {zd : d  I2} is replaced by the
dependence on {zh : h  I1} j  {1, . . . , p}

· s¯cj,I1,I2 and s¯j,I1 converge to the same function when m, l  +, j  {1, . . . , p}

In so doing, prediction bands induced by the set of modulation functions s¯I1 are characterized by all the appealing properties presented in Section 3.1 (including validity) and are asymptotically not wider than those induced by s0 regardless the specific sample z1, . . . , zn. In order to find (s¯I1, s¯cI1,I2) satisfying the aforementioned conditions, let us consider the structure of ks: operationally, ks computes a summary

of the multivariate functional residual for every observation in the calibration set, and selects the (l + 1)(1 - ) th smallest value among them. In particular: the summary

is naturally induced by the specific nonconformity measure used, which searches the

greatest value of the absolute value of the modulated multivariate functional residual
over the p domains T1, . . . , Tp; ks is not affected by the l - (l + 1)(1 - ) greatest values of {Rds : d  I2}. In view of this, a proper candidate for s¯cI1,I2 should ignore the elements of {zd : d  I2} leading to the l - (l + 1)(1 - ) greatest values of {Rds : d  I2} and should modulate data based on the most extreme value observed t  Tj, j  {1, . . . , p}.
Therefore, the couple of sets of functions (s¯I1, s¯cI1,I2) we propose - which represents a generalization of the finding of Diquigiovanni et al. (2021) in the univariate case -
is defined below. Formally, the set of functions s¯cI1,I2 is such that

s¯cj,I1,I2 (t) :=

maxdH2 |ydj(t) - [µ^jI1(xdj)](t)|

p j=1

Tj maxdH2 |ydj(t) - [µ^jI1(xdj)](t)|dt

 j = 1, . . . , p, t  Tj with

H2 := d  I2 : sup sup |ydj(t) - [µ^jI1(xdj)](t)|  k j{1,...,p} tTj

and k = ks0/

p j=1

|Tj |

the

(l + 1)(1 - ) th smallest value in the set

sup sup ydj(t) - [µ^jI1(xdj)](t) : d  I2 .
j{1,...,p} tTj

9

For the sake of simplicity, we assumed maxdH2 |ydj(t) - [µ^jI1(xdj)](t)| = 0 j 

{1, . . . , p}, t  Tj. If this condition does not hold for at least one couple (t, j) but

the condition

p j=1

Tj maxdH2 |ydj(t) - [µ^jI1(xdj)](t)|dt = 0 still holds (the case in

which

p j=1

Tj maxdH2 |ydj(t) - [µ^jI1(xdj)](t)|dt = 0 represents a pathological case

of no practical interest), in order to have that s¯cj,I1,I2(t) > 0 j = 1, . . . , p, t  Tj it

is sufficient to add a small, positive value to s¯cj,I1,I2(t) and to normalize accordingly.

The set of modulation functions s¯I1 is such that

s¯j,I1(t) :=

maxhH1 |yhj(t) - [µ^jI1(xhj)](t)|

p j=1

Tj maxhH1 |yhj(t) - [µ^jI1(xhj)](t)|dt

 j = 1, . . . , p, t  Tj with H1 = I1 if (m + 1)(1 - ) > m, otherwise

H1 := h  I1 : sup sup |yhj(t) - [µ^jI1(xhj)](t)|   j{1,...,p} tTj
and  the (m + 1)(1 - ) th smallest value in the set

sup sup yhj(t) - [µ^jI1(xhj)](t) : h  I1 .
j{1,...,p} tTj
If (t, j) such that maxhH1 |yhj(t) - [µ^jI1(xhj)](t)| = 0, the adjustment used for s¯cj,I1,I2 is implemented.
Specifically, the fact that the set of modulation functions s¯I1 depends on  (through ) allows for a procedure able to modulate data according to the specific value 1 - , i.e. the desired nominal coverage. In addition, such an unconventional set of modulation functions is particularly useful when functional residuals show a non-standard behavior (e.g. there are outliers). The following two theorems show that (s¯I1, s¯cI1,I2) satisfies the aforementioned conditions.
Theorem 1. Let m/n =  with 0 <  < 1 and let Var [µ^jI1(Xij)](t)  0 i  {1, . . . , n}, t  Tj, j  {1, . . . , p} when m  +. Then s¯cj,I1,I2 and s¯j,I1 converge to the same function j  {1, . . . , p} when n  +.
Theorem 2. If at least one of the functions {s¯cj,I1,I2(t)}pj=1 is not constant almost everywhere over its domain, then Q(s0) > Q(s¯cI1,I2). Otherwise, Q(s0) = Q(s¯cI1,I2).
See Appendix A.2 for both proofs, together with the generalization of (s¯I1, s¯cI1,I2), Theorem 1 and Theorem 2 to the Smoothed Split Conformal framework. Due to the very mild conditions required by the two theorems to hold, the set of modulation functions s¯I1 can be used in many general frameworks and provides a new, we believe appealing data-driven alternative to other solutions (e.g. sI1). In the next Section, the set of modulation functions s¯I1 is compared to other sets of modulation functions in different simulated scenarios.

4 Simulation Study
In this Section we perform three simulation studies aimed at evaluating different practical aspects of the method presented in Section 3. Since, to our knowledge, there
10

are no methods dealing with building prediction bands in a multivariate functional setting, the simulations will focus on exploring the empirical properties of our method. In Section 4.1, the empirical coverage provided by the prediction bands is evaluated in different scenarios, considering different sample sizes and different kinds of model misspecification. In Section 4.2, we compare the multivariate prediction bands obtained by the method presented in this article with those obtained by concatenating the p univariate prediction bands induced by the Conformal approach of Diquigiovanni et al. (2021). Finally, in Section 4.3, the three sets of modulation functions presented in Section 3 ({s0j }pj=1, {sj,I1}pj=1, {s¯j,I1}pj=1) are compared in terms of efficiency in order to highlight their strengths and weaknesses.
In all simulation studies, some quantities are kept fixed: p = 2, T1 = T2 = [0, 1],  = 0.10. Three possible sample sizes are taken into account: n = 20, n = 200, n = 2000. We focus on the Split Conformal method and since the coverage reached by Split Conformal prediction set is 1 - (l + 1) /(l + 1) (see Section 2), the size of the calibration set is set equal to l = 9, l = 99, l = 999 respectively in order to obtain 1 - (l + 1) /(l + 1) = 1 -  and consequently to facilitate the readability of the results. A possible alternative would be to consider a different value of l (e.g. n/2) and to evaluate the empirical coverage taking into account the coverage 1 - (l + 1) /(l + 1). Each combination of simulation study, scenario, sample size, regression estimators and set of modulation functions is evaluated based on N = 5000 replications. Specifically, for each replication, a sample z1, . . . , zn+1 is generated and n randomly chosen elements are assigned to the training and calibration sets, whereas the remaining element is considered as the one we aim to predict (however, for the sake of simplicity, hereafter we will simply define the two sets as {zi}ni=1 and zn+1). All simulations are computed using the R Programming Language (R Core Team 2020).
4.1 Simulation Study 1: Coverage
The aim of the simulation study in this Section is to evaluate the empirical coverage (computed as the fraction of the N = 5000 replications in which yn+1 belongs to Cn,1- (xn+1)) reached by the method presented in Section 3 in different scenarios and for different values of n.
Specifically, the simulation study consists of two scenarios. In the first one, the systematic component generating data is linear and, in addition to the case in which the model is correctly specified, two different kinds of model misspecification are taken into account: misspecification due to omitted relevant variable and misspecification due to inclusion of irrelevant variable (see Rao 1971). In the second scenario, a third kind of model misspecification is evaluated, i.e. functional form misspecification (see Wooldridge 1994). The two scenarios are formally defined as follows:
· Scenario 1
Yi1(t) =0(t) + 1(t)wi + i1(t), i  {1, . . . , n + 1}, t  [0, 1] Yi2(t) =0(t) + 2(t)wi2 + i2(t), i  {1, . . . , n + 1}, t  [0, 1]
with wi = i/(n + 1), 0(t), 1(t), 2(t) generated by means of a B-spline basis expansion of order four, with six basis functions, equally spaced knots,
11

Scenario 1

Set of Cov. 1

Set of Cov. 2

Set of Cov. 3

n = 20 0.894[0.886,0.903] 0.896[0.888,0.905] 0.904[0.896,0.912]

n = 200 0.902[0.894,0.910] 0.894[0.885,0.903] 0.901[0.893,0.909]

n = 2000 0.899[0.890,0.907] 0.906[0.898,0.914] 0.902[0.894,0.911]

Scenario 2

Set of Cov. 1

Set of Cov. 2

Set of Cov. 3

n = 20 0.907[0.899,0.915] 0.899[0.890,0.907] 0.904[0.896,0.913]

n = 200 0.899[0.891,0.907] 0.898[0.890,0.907] 0.901[0.893,0.909]

n = 2000 0.893[0.884,0.901] 0.893[0.884,0.901] 0.899[0.891,0.908]

Table 1. Simulation study 1: empirical coverage and related 95% confidence interval

in brackets for each combination of scenario, sample size and set of covariates.  = 0.10, set of modulation functions {sj,I1}2j=1.

coefficients generated independently by a standard normal random variable and i1(t), i2(t) independent functional errors obtained by means of the same B-spline basis expansion with independent standard normal random variables as coefficients. It is important to notice that regression coefficient functions 0, 1, 2 are generated only once, i.e. they do not vary between the N = 5000 replications.
· Scenario 2

Yi1(t) = exp(0(t) + 1(t)wi + i1(t)), i  {1, . . . , n + 1}, t  [0, 1] Yi2(t) = exp(0(t) + 2(t)wi2 + i2(t)), i  {1, . . . , n + 1}, t  [0, 1]
with wi, 0(t), 1(t), 2(t), i1(t), i2(t) defined as in Scenario 1.
Both scenarios are evaluated considering the following three regression estimates:
· Set of Covariates 1. [µ^1I1(xi,1 = {1})](t) = [µ^2I1(xi,2 = {1})](t) = ^0(t)
· Set of Covariates 2. [µ^1I1(xi,1 = {1, wi})](t) = ^0(t) + ^1(t)wi and [µ^2I1(xi,2 = {1, wi2})](t) = ^0(t) + ^2(t)wi2
· Set of Covariates 3. [µ^1I1(xi,1 = {1, wi, wi2})](t) = [µ^2I1(xi,2 = {1, wi, wi2})](t) = ^0(t) + ^1(t)wi + ^2(t)wi2
with ^0(t), ^1(t), ^2(t) the estimates (based on {zh : h  I1}) obtained by fitting each time the corresponding functional-on-scalar linear model. Focusing on Scenario 1, `Set of Covariates 1' represents the omitted relevant variable case, `Set of Covariates 2' represents the case in which the model is correctly specified and `Set of Covariates 3' represents the case in which an irrelevant variable is included, whereas Scenario 2 is characterized by functional form misspecification.
Table 1 shows the empirical coverage p^, as well as the 95% confidence interval [p^ ± 1.96 p^(1 - p^)/N ], obtained for each combination of scenario, sample size and set of covariates considering the set of modulation functions {sj,I1}2j=1. The results
12

are decidedly satisfactory, as the empirical coverages are really close to 1 -  = 0.90 and the observed confidence intervals always include the desired coverage regardless the specific combination of scenario, sample size and set of covariates considered. Specifically, the method presented in Section 3 is able to guarantee the desired coverage also when the sample size is small and the model misspecified.
4.2 Simulation Study 2: Univariate and Multivariate Prediction Bands
The simulation study of this Section is aimed at comparing the Multivariate Prediction Bands outputted by the method presented in this article (MPB method ) to the bands obtained by Concatenating the p Univariate prediction Bands provided by the Conformal approach presented in Diquigiovanni et al. (2021) (CUB method, see Section 3.1 for further details). We focus on two aspects: empirical coverage and efficiency. The empirical coverage is evaluated as described in Section 4.1, whereas for each of the N = 5000 replications the size of the observed prediction band Cn,1- (xn+1) is defined as the average value Q(·)/2 (see Equation (6)).
Two different scenarios are considered: in the first one, the two components Yi,1, Yi,2 share the systematic component, but they are characterized by independent error terms; in the second one, the two components both share the systematic component and the error term in the first half of the domain. In so doing, one is justified in expecting the method presented in this article not to be affected by the different specification of the error terms in terms of coverage, while the CUB method to provide different empirical coverages according to the scenario considered. The two scenarios are:
· Scenario 1

Yi1(t) =0(t) + 1(t)wi + 2(t)wi2 + i1(t), i  {1, . . . , n + 1}, t  [0, 1] Yi2(t) =0(t) + 1(t)wi + 2(t)wi2 + i2(t), i  {1, . . . , n + 1}, t  [0, 1]
with wi, 0(t), 1(t), 2(t), i1(t), i2(t) defined as in Section 4.1.
· Scenario 2

Yi1(t) =0(t) + 1(t)wi + 2(t)wi2 + i1(t), i  {1, . . . , n + 1}, t  [0, 1] Yi2(t) =0(t) + 1(t)wi + 2(t)wi2 + i2(t), i  {1, . . . , n + 1}, t  [0, 1]

with i1(t) = i1(t),

i2(t) =

i1(t) i2(t)

t  [0, 0.5] t  (0.5, 1]

and wi, 0(t), 1(t), 2(t), i1(t), i2(t) defined as in Section 4.1.

As in the previous simulation study, three regression estimates are considered:

· Set of Covariates 1. [µ^1I1(xi,1 = {1})](t) = [µ^2I1(xi,2 = {1})](t) = ^0(t)

13

Scenario 1

MPB method

Set of Cov. 1

Set of Cov. 2

Set of Cov. 3

n = 20 0.902[0.894,0.910] 0.897[0.889,0.906] 0.902[0.893,0.910]

n = 200 0.907[0.899,0.915] 0.909[0.901,0.917] 0.899[0.890,0.907]

n = 2000 0.899[0.891,0.908] 0.897[0.888,0.905] 0.904[0.896,0.913]

CUB method

Set of Cov. 1

Set of Cov. 2

Set of Cov. 3

n = 20 0.812[0.801,0.822] 0.811[0.800,0.822] 0.813[0.802,0.824]

n = 200 0.820[0.809,0.830] 0.823[0.813,0.834] 0.805[0.794,0.816]

n = 2000 0.808[0.797,0.819] 0.801[0.790,0.812] 0.815[0.804,0.826]

Scenario 2

MPB method

Set of Cov. 1

Set of Cov. 2

Set of Cov. 3

n = 20 0.899[0.890,0.907] 0.888[0.879,0.897] 0.905[0.894,0.915]

n = 200 0.900[0.892,0.908] 0.895[0.887,0.904] 0.893[0.882,0.904]

n = 2000 0.906[0.898,0.914] 0.899[0.891,0.908] 0.906[0.895,0.916]

CUB method

Set of Cov. 1

Set of Cov. 2

Set of Cov. 3

n = 20 0.838[0.828,0.848] 0.829[0.819,0.840] 0.844[0.834,0.854]

n = 200 0.853[0.843,0.863] 0.843[0.833,0.853] 0.846[0.836,0.856]

n = 2000 0.857[0.848,0.867] 0.852[0.843,0.862] 0.863[0.854,0.873]

Table 2. Simulation study 2: empirical coverage and related 95% confidence interval

in brackets for each combination of scenario, method, sample size and set of covariates.  = 0.10, set of modulation functions {sj,I1}2j=1.

· Set of Covariates 2. [µ^1I1(xi,1 = {1, wi})](t) = [µ^2I1(xi,2 = {1, wi})](t) = ^0(t) + ^1(t)wi
· Set of Covariates 3. [µ^1I1(xi,1 = {1, wi, wi2})](t) = [µ^2I1(xi,2 = {1, wi, wi2})](t) = ^0(t) + ^1(t)wi + ^2(t)wi2
Note that `Set of Covariates 3' represents the case in which the model is correctly specified, while the other two sets of covariates represent a case of misspecification.
Table 2 shows the empirical coverage p^, together with the 95% confidence interval defined as in Section 4.1, obtained for each combination of scenario, method, sample size and set of covariates considering the set of modulation functions {sj,I1}2j=1. In accordance with the results provided by the first simulation study, the MPB method presented in this article ensures empirical coverages very close to 0.90, with only two confidence intervals not including the target value 1 - . As regards the CUB method, in both scenarios the empirical coverages are far from 1 -  as expected, and moving from Scenario 1 to Scenario 2 they grow due to the fact that the error terms are dependent. In particular, in the first scenario almost all the confidence intervals include the value (1 - )2 = 0.81, that is the coverage we expect from the CUB method since the error terms are independent. In view of this, the simulation study fully confirms the quite obvious conjecture that a carefully chosen multivariate approach must be considered in order to obtain proper multivariate simultaneous bands.
14

Scenario 1

Set of Cov. 1

Set of Cov. 2

Set of Cov. 3

n = 20 5.606[4.940,6.539] 5.373[4.704,6.322] 6.007[5.106,7.212]

n = 200 4.270[4.137,4.407] 3.820[3.706,3.943] 3.831[3.714,3.954]

n = 2000 4.166[4.126,4.206] 3.707[3.673,3.741] 3.702[3.666,3.737]

Scenario 2

Set of Cov. 1

Set of Cov. 2

Set of Cov. 3

n = 20 5.279[4.623,6.186] 5.063[4.357,6.005] 5.637[4.788,6.814]

n = 200 4.133[3.996,4.279] 3.690[3.571,3.815] 3.698[3.575,3.823]

n = 2000 4.032[3.992,4.073] 3.583 [3.547,3.618] 3.578[3.543,3.614]

Scenario 3

Set of Cov. 1

Set of Cov. 2

Set of Cov. 3

n = 20 4.820[4.184,5.649] 4.568[3.932,5.446] 5.052[4.273,6.141]

n = 200 3.852[3.708,4.000] 3.442[3.319,3.565] 3.450[3.327,3.578]

n = 2000 3.773[3.731,3.817] 3.354[3.315,3.391] 3.347[3.310,3.385]

Table 3. Simulation study 2: median size (first and third quartile in brackets)

for each combination of scenario, sample size and set of covariates. MPB method,  = 0.10, set of modulation functions {sj,I1}2j=1.

Since the CUB method does not guarantee the desired coverage, hereafter we will only focus on the efficiency of the prediction bands outputted by the MPB method. Table 3 shows the median size (together with the first and third quartile in brackets) of the prediction bands analyzed in Table 2. In addition to the two scenarios considered so far, Table 3 also analyzes a third scenario in which i1(t) = i2(t) i, t (and so Yi1 = Yi2): despite its limited practical utility, this scenario represents an edge case that can provide useful information. First of all, for each combination of scenario and set of covariates, the size decreases when n grows, both because improved regression estimates generally provide smaller nonconformity scores and because the value of ks is less dependent on random fluctuations. Focusing now on each combination of sample size and set of covariates, it is possible to notice that Scenario 1 typically provides the biggest prediction bands, whereas Scenario 3 the smallest. From a practical point of view, this evidence is due to the nonconformity measure used: indeed, it searches the most extreme value of (ydj(t) - [µ^jI1(xdj)](t))/sj,I1(t) over T1, T2, and so the nonconformity score computed when yd1 = yd2 as in Scenario 3 will always be less than or equal to that computed when yd1 = yd2. Scenario 2 represents an intermediate case between Scenario 1 and Scenario 3 as regards the structure of the error terms, and this is confirmed by the evidence provided by Table 3.
4.3 Simulation Study 3: Efficiency
The aim of the simulation study of this Section is to compare the three sets of modulation functions ({s0j }pj=1, {sj,I1}pj=1, {s¯j,I1}pj=1) in terms of efficiency. To do that, three different scenarios are taken into account: focusing just for now on the error terms and ignoring the systematic components, in the first scenario the error terms are characterized by a constant variability over the domains, in the second scenario the variability differs whereas in the third scenario the presence of outliers further complicates their specification. Formally, the three scenarios are:
15

· Scenario 1. The two systematic components are defined as in the first scenario of Section 4.1, while the independent functional errors i1(t), i2(t) are defined as follows:
ij (t) =Bi+(n+1)(j-1),1+ Bi+(n+1)(j-1),2 cos 10 t + Ui+(n+1)(j-1) + Bi+(n+1)(j-1),3 sin 10 t + Ui+(n+1)(j-1)
i  {1, . . . , n+1}, j  {1, 2}, t  [0, 1], with i.i.d. random vectors B1, . . . , B2(n+1)  N3(0, ),  having the entries on the main diagonal equal to 1 and the entries outside the main diagonal equal to 0.7, i.i.d. random variables U1, . . . , U2(n+1)  U [-0.5, 0.5].
· Scenario 2. The two systematic components are defined as in the first scenario of Section 4.1, while the independent functional errors i1(t), i2(t) are obtained by means of a B-spline basis expansion of order four, with 13 basis functions, equally spaced knots and normal random vectors as vectors of coefficients. Specifically, the 2 · (n + 1) (observed) vectors of coefficients are independent realizations of C = (C1, . . . , C13)  N13(0, ) with  diagonal matrix such that Var[Ca] = 0.001 a = 7, Var[C7] = 9 · 10-6.
· Scenario 3

Yi1(t) =0(t) + i1(t), i  {1, . . . , n + 1}, t  [0, 1] Yi2(t) =0(t) + i2(t), i  {1, . . . , n + 1}, t  [0, 1]
with 0(t) = 0 t  [0, 1],
ij(t) =1(t)wij + ij(t), i  {1, . . . , n + 1}, j  {1, 2}, t  [0, 1]

with 1(t) obtained by means of a B-spline basis expansion of order four, with 13 basis, equally spaced knots and all coefficients equal to 0 but the seventh equal to 0.5, ij(t) defined as in Scenario 2, and if n = 20 then wij = 0 {i, j} = {1, 1}, w1,1 = 1, whereas if n  {200, 2000} then

wij =

1 0

if i 

j

+

40

·



:





{0, 1, 2, . . . ,

n 40

-

1}

otherwise

Despite the complex notation, the introduction of wij is aimed at obtaining that  5% of the multivariate functions y1, . . . , yn+1 (i.e. 1 out of 21 when n = 20, 10 out of 201 when n = 200, 100 out of 2001 when n = 2000) is characterized,
in one of the two components, by the anomalous behavior induced by 1(t). We propose such an unconventional structure for the error terms to simulate, for
example, a regression framework in which relevant variables are not available.

All three scenarios are evaluated considering only one set of covariates each, namely the case in which the corresponding model is correctly specified. Figure 2 shows,

16

i1(t)

i1(t)

4

2

0

-2

0.00

0.25

0.50

0.75

1.00

t

0.4

0.3

0.2

0.1

0.0

-0.1

0.00

0.25

0.50

0.75

1.00

t

0.4

0.3

0.2

0.1

0.0

-0.1

0.00

0.25

0.50

0.75

1.00

t

i1(t)

Figure 2. Example of realization of the error term related to {Yi1}ni=+11. First scenario at the top, second scenario in the middle, third scenario at the bottom. n = 20.

for each scenario, a realization of the error terms {i1}ni=+11 ({i1}ni=+11 for Scenario 3) when n = 20.
Table 4 shows the median size (defined as in Section 4.2; first and third quartile
in brackets) of the N = 5000 prediction bands obtained for each combination of
scenario, sample size and set of modulation functions. All three scenarios share the evidence that the prediction bands induced by {s0j }2j=1 are typically smaller than those induced by the other two sets of modulation functions when the sample size is
very small (n = 20). This is due to the fact that regression estimates obtained with
a small training set size likely provide an unreliable (and potentially misleading) set
of modulation functions, leading to a preference for a set of modulation functions
not depending on I1. As proof of that, it is not surprising that the two data-driven sets of modulation functions {sj,I1}2j=1, {s¯j,I1}2j=1 deliver the worst performance in the most complex Scenario, i.e. Scenario 3. Focusing on the other two sample
sizes, in Scenario 1 the choice of not modulating seems appropriate due to the equal
magnitude of the two components and the constant variability over T1, T2, but, as expected, the difference between the three alternative sets of modulation functions
decreases when n grows. Differently from Scenario 1, Scenario 2 is characterized by
multivariate residuals showing a lower variability in the central portion of T1 and T2: as a consequence, {s0j }2j=1 provides large prediction bands since it is not able to adapt the width of the band according to the local variability of the residuals, whereas {sj,I1}2j=1 is particularly effective since it induces a modulation process based on the two standard deviation functions. Finally, {s¯j,I1}2j=1 represents the best solution in Scenario 3 given its ability to focus on the `least extreme'  (1 - ) · 100% of data: indeed, differently from {s0j }2j=1 it is able to reduce the width of the band in the central part of the domains, and differently from {sj,I1}2j=1 it does not uselessly enlarge the band in the same subinterval of T1, T2. Consequently, the simulation study seems to confirm the statistical intuition given in Section 3.2 that the newly launched set of modulation functions {s¯j,I1}pj=1 represents an interesting solution when functional residuals show a non-standard behavior and a modulation process
driven by the value 1 -  is needed.

17

n = 20

Scenario 1

{s0j }2j=1

{sj,I1 }2j=1

{s¯j,I1 }2j=1

9.599[8.348,11.116] 12.205[10.121,14.853] 14.241[11.730,17.798]

n = 200 8.658[8.289,9.077] 8.835[8.442,9.246] 9.315[8.892,9.784]

n = 2000 8.568[8.449,8.692] 8.587[8.469,8.712] 8.681[8.561,8.806]

n = 20

{s0j }2j=1 0.168[0.152,0.188]

Scenario 2
{sj,I1 }2j=1 0.190[0.167,0.221]

{s¯j,I1 }2j=1 0.213[0.186,0.249]

n = 200 0.148[0.144,0.153] 0.126[0.123,0.130] 0.139[0.135,0.144]

n = 2000 0.146[0.145,0.148] 0.122[0.121,0.123] 0.134[0.133,0.136]

n = 20

{s0j }2j=1 0.201[0.157,0.667]

Scenario 3
{sj,I1 }2j=1 0.294[0.212,1.767]

{s¯j,I1 }2j=1 0.407[0.277,1.869]

n = 200 0.162[0.155,0.170] 0.167[0.161,0.172] 0.151[0.145,0.158]

n = 2000 0.160[0.157,0.162] 0.161[0.160,0.163] 0.145[0.143,0.147]

Table 4. Simulation Study 3: median size (first and third quartile in brackets) for

each combination of scenario, sample size and set of modulation functions.  = 0.10.

5 Case Study: Analysis of Bike Mobility in the
City of Milan
In order to illustrate the application potential of the method presented in this article, in this section we focus on a case study concerning urban mobility, and specifically the usage of a bike-sharing system in the Italian city of Milan. Moving from the raw data and the context presented in Torti et al. (2021), the aim is to study the behavior of subscribers of Bikemi, a bike sharing system active in the city in which bikes are picked up and dropped off in specific docking stations located through the city. Starting from raw data providing various information about picked up bikes (simply pickups hereafter) and dropped off bikes (simply dropoffs hereafter) for each day considered, and focusing our attention - as an example - on the Duomo district only (i.e. the area in which Milan's cathedral is), the multivariate functional response variable yi = (yi1, yi2) representing the rate of dropoffs (yi1) and pickups (yi2) is obtained via a standard kernel density estimation smoothing method (Hastie et al. 2009). In so doing, yi1(t) (yi2(t)) represents the dropoff (pickup) rate at time t, with t ranging from 7 a.m. day i to 1 a.m. the next day (consequently, we assume that day i ends at 1 a.m. the next day). The period considered starts on 25 January 2016 and ends on 6 March 2016: due to an error in the data collection, 25 February is removed from the dataset in accordance with Torti et al. (2021), and so the sample size is n = 41. Data are shown in the two top panels of Figure 3.
Like in Torti et al. (2021), the regression estimates are obtained by fitting a concurrent functional-on-functional linear model (Ramsay and Silverman 2005). The model hereby used includes as covariates a functional intercept, the temperature function (after subtracting the average daily temperature function of the period considered) in degrees Celsius, and a dummy variable indicating whether day i is a weekday or not. Since the rates cannot be negative in any subinterval of the domain, the predicted functions are truncated to 0. However, as discussed in Section 3.1, the
18

600

600

Pickup rate [bikes/hour]

Dropoff rate [bikes/hour]

400

400

200

200

0 7 a.m.
600

10 a.m.

1 p.m.

4 p.m. t

7 p.m.

10 p.m.

1 a.m

0 7 a.m.

600

10 a.m.

1 p.m.

4 p.m. t

7 p.m.

10 p.m.

1 a.m

Predicted pickup rate [bikes/hour]

Predicted dropoff rate [bikes/hour]

400

400

200

200

Residual - Dropoff rate [bikes/hour]

0 7 a.m.

10 a.m.

1 p.m.

100 0
-100 -200 -300 -400
7 a.m.

10 a.m.

1 p.m.

4 p.m. t
4 p.m. t

7 p.m. 7 p.m.

10 p.m. 10 p.m.

1 a.m

0 7 a.m.

10 a.m.

1 p.m.

100

Residual - Pickup rate [bikes/hour]

0

-100

-200

-300

1 a.m

-400 7 a.m.

10 a.m.

1 p.m.

4 p.m. t
4 p.m. t

7 p.m. 7 p.m.

10 p.m. 10 p.m.

1 a.m 1 a.m

Figure 3. Dropoff and pickup rates (top left, top right respectively), corresponding functional predictions (center left, center right) and functional residuals (bottom left, bottom right). Yellow curves refer to 29 February; continuous curves refer to the observations in the training set, dashed curves to those in the calibration set.

purpose is to construct valid, meaningful and interpretable prediction bands also when simple regression estimators are specified, and so the choice of the covariates, as well as the functional form of the model, represents an aspect of limited interest in the framework considered.
The method presented in Section 3 is performed by considering the three sets of modulation functions {s0j }2j=1, {sj,I1}2j=1, {s¯j,I1}2j=1,  = 0.25 and m = 22, l = 19 in order to assign, as in the simulation studies, about half of the observations to the training set and to obtain the value 1 - (l + 1) /(l + 1) equal to 1 - . To remain as neutral as possible, we will consider the case in which - after having labeled the days considered with numbers from 1 to 41 - the observations referring to an odd day are assigned to the training set and those referring to an even day to the calibration set, with the observation related to day 20 assigned to the training set to satisfy m = 22. Two possible prediction scenarios are taken into account for the scope of visualization: in the first, we construct the multivariate prediction band for a weekday having the average temperature function of the period as temperature function; in the second, we construct it for a warmer than usual weekday (see Figure B.1 in Appendix B for a graphical representation of the two functional covariates, together with those observed). Figure 4 shows, for each of the three sets of modulation functions ({s0j }2j=1 in the first row, {sj,I1}2j=1 in the second row, {s¯j,I1}2j=1 in the third row), the prediction sets induced by the two scenarios (first set of covariates in the first column, second in the second column). In particular, each panel shows the prediction band for the dropoff rate (light blue band) and the pickup rate (red band), with

19

Dropoff/Pickup rate [bikes/hour]

Dropoff/Pickup rate [bikes/hour]

750

750

500

500

250

250

0 7 a.m.

10 a.m.

1 p.m.

4 p.m. t

7 p.m.

10 p.m.

1 a.m

0 7 a.m.

10 a.m.

1 p.m.

4 p.m. t

7 p.m.

10 p.m.

1 a.m

Dropoff/Pickup rate [bikes/hour]

Dropoff/Pickup rate [bikes/hour]

750

750

500

500

250

250

0 7 a.m.

10 a.m.

1 p.m.

4 p.m. t

7 p.m.

10 p.m.

1 a.m

0 7 a.m.

10 a.m.

1 p.m.

4 p.m. t

7 p.m.

10 p.m.

1 a.m

Dropoff/Pickup rate [bikes/hour]

Dropoff/Pickup rate [bikes/hour]

750

750

500

500

250

250

0 7 a.m.

10 a.m.

1 p.m.

4 p.m. t

7 p.m.

10 p.m.

1 a.m

0 7 a.m.

10 a.m.

1 p.m.

4 p.m. t

7 p.m.

10 p.m.

1 a.m

Figure 4. Prediction bands for the dropoff rate (light blue band) and the pickup
rate (red band). Each panel refers to a combination of set of modulation functions ({s0j }2j=1 at the top, {sj,I1}2j=1 in the middle, {s¯j,I1}2j=1 at the bottom) and scenario (first set of covariates on the left, second on the right). The dashed lines indicate
the corresponding regression estimates.  = 0.25. Split into calibration/training set:
even/odd(+ day 20) days.

the two dashed lines representing the corresponding regression estimates. As for the
predicted functions, the prediction bands are truncated to 0, as the rates cannot be
negative in any subinterval of the domain. Note that this truncation does not involve
any kind of drawback since the coverage reached by the prediction sets remains
unchanged if a null probability portion of the bands is removed from the prediction bands. It is evident that the prediction bands for dropoffs induced by {sj,I1}2j=1 are quite large in the initial portion of the domain compared to those obtained by not modulating (i.e. {s0j }2j=1) and by the proposed set {s¯j,I1}2j=1. In order to clarify this aspect, let us consider Figure 3. Focusing on the residual functions of the dropoff
rates (i.e. the panel at the bottom left of the figure), it is easily noticeable that
the yellow curve (referring to weekday 35, i.e. 29 February, which is assigned to
the training set) shows an anomalous behavior in the initial part of the domain.
The panel at the top left of the same figure suggests that this is due to the fact
that day 35 was characterized by an unusually low dropoff rate compared to that
observed in the other weekdays (which are the curves showing a pick around 9 a.m.). Consequently, by using {sj,I1}2j=1 it is natural to obtain prediction bands for dropoffs extremely wide in the first portion of the domain since this outlier has a huge impact
on the modulation process, while the corresponding prediction bands obtained by not modulating are not adversely affected as {s0j }2j=1 does not modulate the width of the band according to the local variability of the residuals. In view of this, the set of modulation functions {s¯j,I1}2j=1 represents an intriguing solution since, in addition to modulate the width of the band along the domains, induces a modulation
process which is not misled by the anomalous behavior of day 35. However, similar

20

considerations would have been made also if other observations than the one related to day 35 had been assigned to the training set, as can be noticed by analyzing the functional residuals of the observations assigned to the calibration set in the two panels at the bottom of Figure 3 (dashed curves for the calibration set; continuous curves for the training set). Despite the small sample size, the prediction sets of Figure 4 can provide profitable information: first of all, subscribers of Bikemi seem to mainly use bikes to go to Duomo in the morning, whereas in the early evening the bike flow is reversed. Moving from the first set of covariates (weekday-temperature equal to the mean temperature of the period) to the second one (weekday-warm day), we notice that a higher temperature does not strongly affect people's behavior in the morning, whereas it involves a moderate increase in dropoffs and, at the same time, a big increase in pickups in the period of time around 7 p.m.. The information provided by the prediction bands can be indeed very useful to fleet managers in identifying the periods of time in which the imbalance between pickups and dropoffs could become critical based on the day of the week, the temperature function and other possible carefully chosen covariates.
6 Conclusion and Further Developments
In the present work we have developed a procedure aimed at creating prediction bands for multivariate functional data in a regression framework. Despite the paramount importance of this topic both from the methodological and applied point of view, to the best of our knowledge our method represents the first proposal in this direction. Moving from the approach proposed by Diquigiovanni et al. (2021) for univariate i.i.d. functional data, the method presented in this article builds finite-sample either exact or valid prediction bands under the only assumption of exchangeable regression pairs with multivariate functional response. These properties, together with the fact that the procedure is scalable and the bands can be easily found in closed form, allow to obtain meaningful prediction bands regardless the regression estimator used, leading to a methodology which can be applied in a wide range of application scenarios. Moreover, we have introduced a specific set of modulation functions (namely {s¯j,I1}pj=1) achieving an asymptotic result in terms of efficiency regardless the sample observed z1, . . . , zn and inducing prediction bands whose width varies along the domains and across the components according to the local behavior. The simulation study and the real-world application provided in Section 4 and 5 respectively confirm the potential of the approach. Nevertheless, many possible directions still remain unexplored. Among these, we plan to modify the methodology in order to apply it when regression data are dependent (as in the case, for example, of a functional time series); and, we plan to explore the impact of the regression estimator on the size of the prediction sets.
Supplementary material
A Technical Proofs
A.1 Proof of Section 3.1
Computation to find Cns,1-(xn+1)
21

Since

ys =

d  I2  {n + 1} : Rds  Rns +1 l+1

,

p

Cns,1- (xn+1) = y  L(Tj) : ys >  ,

j=1

if   [1/(l + 1), 1), then y  Cns,1-(xn+1)  Rns +1  ks, with ks the (l + 1)(1 - ) th smallest value in the set {Rds : d  I2}. Then

sup

sup yj(t) - [µ^jI1(xn+1,j)](t)

j{1,...,p} tTj

sj,I1 (t)

 ks



yj(t) - [µ^jI1(xn+1,j)](t)  ks sj,I1 (t)

j  {1, . . . , p}, t  Tj

 yj(t)  [µ^jI1(xn+1,j)](t) - ks · sj,I1(t), [µ^jI1(xn+1,j)](t) + ks · sj,I1(t)] j  {1, . . . , p}, t  Tj.

As a consequence, the Split Conformal prediction set is

Cns,1-(xn+1) :=

p
y  L(Tj) : yj(t)  [µ^jI1(xn+1,j)](t) - ks · sj,I1(t),
j=1
[µ^jI1(xn+1,j)](t) + ks · sj,I1(t)]

j  {1, . . . , p}, t  Tj .

Computation

to

find

Cs n,1-,n+1

(xn+1

)

Consistently with the Split Conformal scenario, let us define

s y,n+1

:=

Cs n,1-,n+1

(xn+1)

:=

d  I2 : Rds > Rns +1 + n+1 d  I2  {n + 1} : Rds = Rns +1

l+1

p

y

L (Tj )

:

s y,n+1

>



.

j=1

By definition, Cns,1-,1(xn+1) = Cns,1-(xn+1).

Since

s y,n+1



[n+1/(l

+

1), (l

+

n+1)/(l

+

1)],

we

will

focus

on

the

scenario

in

which   [n+1/(l + 1), (l + n+1)/(l + 1)). Let us define ws the l + n+1 - (l + 1) th

smallest value in the set {Rd : d  I2}, and rns (vns respectively) the number of

elements in the set {Rd : d  I2} that are equal to ws and that are to the right (left

respectively) of ws in the sorted version of the set. Note that rns = vns = 0 when

the assumption about the continuous joint distribution of {Rd : d  I2} is satisfied,

but generally speaking we will consider rns , vns  N0 such that rns + vns  l - 1. By

replicating calculations similar to those performed in the Split Conformal framework,

we obtain that:

· if

n+1

>

(l

+

1)

-

(l + 1) - rns + vns + 2

n+1

+ rns

22

then

y



Cs n,1-,n+1

(xn+1)



Rns +1  ws and so

Cs n,1-,n+1

(xn+1)

:=

p
y  L(Tj) : yj(t)  [µ^jI1(xn+1,j)](t) - ws · sj,I1(t),
j=1
[µ^jI1(xn+1,j)](t) + ws · sj,I1(t)]

j  {1, . . . , p}, t  Tj .

· if

n+1



(l

+

1)

-

(l + 1) - rns + vns + 2

n+1

+ rns

then

y



Cs n,1-,n+1

(xn+1)



Rn+1 < ws and so

Cs n,1-,n+1

(xn+1)

:=

p
y  L(Tj) : yj(t) 
j=1

[µ^jI1(xn+1,j)](t) - ws · sj,I1(t), [µ^jI1(xn+1,j)](t) + ws · sj,I1(t)

j  {1, . . . , p}, t  Tj .

Proof that the concatenation of the p univariate prediction bands obtained by applying the nonconformity measure suptTj yj(t) - [µ^jI1(xj)](t) /sj,I1(t) to the p components separately is a subset of (5)
Let us define Uns,1-(xn+1) as the multivariate prediction band obtained by concatenating the p univariate prediction bands induced by applying the nonconformity measure suptTj yj(t) - [µ^jI1(xj)](t) /sj,I1(t) to the p components separately. Let us define, with a slight abuse of notation,
R~dsj := sup ydj(t) - [µ^jI1(xdj)](t) /sj,I1(t) , d  I2, j  {1, . . . , p} tTj
and k~js the (l + 1)(1 - ) th smallest value in the set {R~dsj : d  I2}. By construction Rds = supj{1,...,p} R~dsj, and so Rds  R~dsj j  {1, . . . , p}, d  I2 and then ks  k~js j  {1, . . . , p}. In view of this, if y  Uns,1-(xn+1), i.e.
yj(t)  [µ^jI1(xn+1,j)](t) - k~js · sj,I1(t), [µ^jI1(xn+1,j)](t) + k~js · sj,I1(t)] j  {1, . . . , p}, t  Tj,
then
yj(t)  [µ^jI1(xn+1,j)](t) - ks · sj,I1(t), [µ^jI1(xn+1,j)](t) + ks · sj,I1(t)] j  {1, . . . , p}, t  Tj,
i.e. y  Cns,1-(xn+1). As y  Cns,1-(xn+1) does not necessarily imply y  Uns,1-(xn+1), then Uns,1-  Cns,1-.
Proof that the concatenation of the pointwise prediction intervals obtained by applying the pointwise nonconformity measure yj(t) - [µ^jI1(xj)](t) /sj,I1(t) j  {1, . . . , p}, t  Tj is a subset of (5)
23

Let us define Uns,1-(xn+1) as the multivariate prediction band obtained by
concatenating the pointwise prediction intervals obtained by applying the pointwise nonconformity measure yj(t) - [µ^jI1(xj)](t) /sj,I1(t) j  {1, . . . , p}, t  Tj. Let us define, with a slight abuse of notation,

R~dsj(t) := ydj(t) - [µ^jI1(xdj)](t) /sj,I1(t) , d  I2, j  {1, . . . , p}, t  Tj
and k~js(t) the (l + 1)(1 - ) th smallest value in the set {R~dsj(t) : d  I2}. By construction Rds = supj{1,...,p} suptTj R~dsj(t) , and so Rds  R~dsj(t) j  {1, . . . , p}, d  I2, t  Tj and then ks  k~js(t) j  {1, . . . , p}, t  Tj. In view of this, if y  Uns,1-(xn+1), i.e.
yj(t)  [µ^jI1(xn+1,j)](t) - k~js(t) · sj,I1(t), [µ^jI1(xn+1,j)](t) + k~js(t) · sj,I1(t)] j  {1, . . . , p}, t  Tj,
then

yj(t)  [µ^jI1(xn+1,j)](t) - ks · sj,I1(t), [µ^jI1(xn+1,j)](t) + ks · sj,I1(t)] j  {1, . . . , p}, t  Tj,
i.e. y  Cns,1-(xn+1). As y  Cns,1-(xn+1) does not necessarily imply y  Uns,1-(xn+1), then Uns,1-  Cns,1-.
Proof that prediction bands induced by {sj,I1}pj=1 and by { · sj,I1}pj=1 coincide   R>0
Let Cn,·1s-(xn+1) be the prediction band induced by the set of modulation functions { · sj,I1}pj=1. The nonconformity scores are:

Rd·s = sup
j{1,...,p}
Rn·+s1 = sup
j{1,...,p}

sup ydj(t) - [µ^jI1(xdj)](t)

tTj

 · sj,I1(t)

=

1 

Rds ,

d  I2

sup yj(t) - [µ^jI1(xn+1,j)](t)

tTj

 · sj,I1(t)

=

1 

Rns +1.

Moreover, let us define:

y·s :=

d  I2  {n + 1} : Rd·s  Rn·+s1 , l+1

with, as usual, Cn,·1s-(xn+1) := y 

p j=1

L (Tj )

:

y·s

>



.

As a consequence,

y  Cn,·1s-(xn+1)  Rn·+s1  k·s, with k·s the (l + 1)(1 - ) th smallest value in the set {Rd·s : d  I2}. Since Rd·s = Rds/ d  I2, then k·s = ks/. Then:

Rn·+s1  k·s



1 

Rns +1



ks 

 Rns +1  ks,

and since y  Cns,1-(xn+1)  Rns +1  ks, then Cn,·1s-(xn+1) coincides with Cns,1-(xn+1).
24

A.2 Proof of Section 3.2

Proof of Theorem 1 Let us consider s¯j,I1(t), with j  {1, . . . , p}. Since m/n =  with 0 <  < 1,
if n  + then m  +. The scalar  is the empirical quantile of order (m + 1)(1 - ) ) of {supj{1,...,p} suptTj yhj(t) - [µ^jI1(xhj)](t) : h  I1}. First of all note that

(m + 1)(1 - )

m + 1 - (m + 1)

lim

= lim

m+

m

m+

m

and since

(m + 1) - 1 (m + 1) (m + 1)

m





m

m

m  N,

(m + 1) - 1

(m + 1)

lim

= lim

=

m+

m

m+

m

then by the squeeze theorem we know that

(m + 1)

lim

=

m+

m

and then

(m + 1)(1 - ) )

lim

= 1 - .

m+

m

Consequently,  is the empirical quantile of order 1 -  when m  +. Let us define wh := supj{1,...,p} suptTj yhj(t) - [µ^jI1(xhj)](t)  h  I1. The random variables {Wh : h  I1} from which {wh : h  I1} are drawn are continuous and after Var [µ^jI1(Xhj)](t)  0 j  {1, . . . , p} they become i.i.d.. The GlivenkoCantelli theorem guarantees that the empirical distribution function of these variables converges uniformly and almost surely pointwise to its distribution function, and so also the empirical quantiles converge in distribution - and so in probability - to the corresponding theoretical quantiles (see, for example, Van der Vaart 2000, chap. 21). In so doing, empirical quantile  converges to q1-, the theoretical quantile of order 1 - . As a consequence:

H1 := {h  I1 : sup sup yhj(t) - [µ^jI1(xhj)](t)  q1-} j{1,...,p} tTj
when m  +, with q1- non-random quantity. Let us consider the numerator of s¯j,I1(t) j  {1, . . . , p} as the denominator is a normalizing constant. t  Tj, the sequence {maxhH1 |yhj(t) - [µ^jI1(xhj)](t)|}m is eventually bounded by q1- and is eventually increasing since {|H1|}m is eventually increasing. Therefore the sequence converges to its supremum by the monotone convergence theorem.
As regards s¯cj,I1,I2, first of all it is possible to notice that if n  + then l = n(1 - )  +. In order to show the convergence of the numerator of s¯cj,I1,I2 to the same limit function, it is sufficient to consider the previous calculations by substituting  with k, m with l, H1 with H2 and I1 with I2 (except for [µ^jI1(xhj)](t) that is not substituted by [µ^jI2(xhj)](t)). Finally, as the numerators of s¯j,I1,I2 and s¯cj,I1
25

converge to the same function j  {1, . . . , p}, also the two normalizing constants converge to the same value.
Proof of Theorem 2 For the sake of simplicity, let us focus on the case in which |H2| = (l + 1)(1 - ) . Under the assumption concerning the continuous joint distribution of {Rd : d  I2} made in Secton 2 such condition is always satisfied, but for the sake of completeness the proof when this assumption is violated is addressed below.

· d  H2, j  {1, . . . , p} the following relationship holds t  Tj:

ydj(t) - [µ^jI1(xdj)](t) s¯cj,I1,I2 (t)

p
=
j=1

Tj

max
dH2

|ydj (t)

-

[µ^jI1 (xdj )](t)|dt

·

ydj(t) - [µ^jI1(xdj)](t) maxdH2 |ydj(t) - [µ^jI1(xdj)](t)|

p


j=1

Tj

max
dH2

|ydj (t)

-

[µ^jI1 (xdj )](t)|dt,

and then

Rds¯c := sup
j{1,...,p}

sup
tTj

ydj(t) - [µ^jI1(xdj)](t) s¯cj,I1,I2 (t)

p


j=1

Tj

max
dH2

|ydj (t)

-

[µ^jI1 (xdj )](t)|dt.

Specifically,  d  H2 such that Rds¯c =

p j=1

Tj maxdH2 |ydj(t)-[µ^jI1(xdj)](t)|dt

since j  {1, . . . , p} and t  Tj at least one function yd,j satisfies ydj(t) - [µ^jI1(xdj)](t) =

maxdH2 |ydj(t) - [µ^jI1(xdj)](t)|.

· Let us define CH2 := I2 \ H2 and let (td, jd) be the couple of values such that

ydjd (td) - [µ^jId1 (xdjd )](td) = sup j{1,...,p}

sup ydj(t) - [µ^jI1(xdj)](t)
tTj

d  I2.

If (td, bd) is not unique, it is randomly chosen from the couples satisfying that condition.
b  CH2, by definition of H2 it is possible to notice that ybjb(tb ) - [µ^jIb1(xbjb)](tb ) > maxdH2 |ydjb(tb ) - [µ^Ijb1(xdjb)](tb )| and so the following relationship holds:

ybjb (tb ) - [µ^Ijb1 (xbjb )](tb )

s¯cjb,I1,I2 (tb )

p
=
j=1

Tj

max
dH2

|ydj (t)

-

[µ^jI1 (xdj )](t)|dt

·

ybjb (tb ) - [µ^jIb1 (xbjb )](tb ) maxdH2 |ydjb (tb ) - [µ^jIb1 (xdjb )](tb )|

p

>
j=1

Tj

max
dH2

|ydj (t)

-

[µ^jI1 (xdj )](t)|dt.

26

Consequently,

Rbs¯c := sup
j{1,...,p}

sup
tTj

ybj(t) - [µ^jI1(xbj)](t) s¯cj,I1,I2 (t)

p

>
j=1

Tj

max
dH2

|ydj (t)

-

[µ^jI1 (xdj )](t)|dt.

Since:

· |H2| = (l + 1)(1 - )

· d  H2 Rds¯c 

p j=1

Tj maxdH2 |ydj(t) - [µ^jI1(xdj)](t)|dt and  d  H2 such

that Rds¯c =

p j=1

Tj maxdH2 |ydj(t) - [µ^jI1(xdj)](t)|dt

· b  CH2 Rbs¯c >

p j=1

Tj maxdH2 |ydj(t) - [µ^jI1(xdj)](t)|dt

we conclude that ks¯c =

p j=1

Tj maxdH2 |ydj(t) - [µ^jI1(xdj)](t)|dt, with ks¯c the

(l + 1)(1 - ) th smallest value in the set {Rds¯c : d  I2}.

If |H2| > (l + 1)(1 - ) , then Rds¯c =

p j=1

Tj maxdH2 |ydj(t) - [µ^jI1(xdj)](t)|dt

is valid d  H2 such that supj{1,...,p} suptTj |ydj(t) - [µ^jI1(xdj)](t)| = k and we

can conclude also in this case that ks¯c =

p j=1

Tj maxdH2 |ydj(t) - [µ^jI1(xdj)](t)|dt.

Focusing now on the set of modulation functions s0, d  I2:

Rds0 := sup
j{1,...,p}

sup
tTj

ydj(t) - [µ^jI1(xdj)](t) s0j (t)

= sup sup ydj(t) - [µ^jI1(xdj)](t) j{1,...,p} tTj

p
· |Tj| .
j=1

Since ks0 is the (l + 1)(1 - ) th smallest value in the set {Rds0 : d  I2}, by definition of H2 we can notice that

ks0

=

max
dH2

Rds0

= max dH2

sup
j{1,...,p}

sup ydj(t) - [µ^jI1(xdj)](t)
tTj

= sup
j{1,...,p}

sup
tTj

max
dH2

ydj(t) - [µ^jI1(xdj)](t)

p
· |Tj|
j=1 p
· |Tj| .
j=1

Since by the integral mean value theorem we know that j  {1, . . . , p}

sup
tTj

max
dH2

ydj(t) - [µ^jI1(xdj)](t)

· |Tj| 

max
Tj dH2

ydj(t) - [µ^jI1(xdj)](t)

dt,

then the following relationship is valid:

p

sup
j=1 tTj

max
dH2

ydj(t) - [µ^jI1(xdj)](t)

p
· |Tj| 
j=1

In addition, by definition j  {1, . . . , p}

max
Tj dH2

ydj(t) - [µ^jI1(xdj)](t)

dt.

(7)

sup
j{1,...,p}

sup
tTj

max
dH2

ydj(t) - [µ^jI1(xdj)](t)

 sup
tTj

max
dH2

ydj(t) - [µ^jI1(xdj)](t)

27

and so:

p

sup
j=1 j{1,...,p}

sup
tTj

max
dH2

ydj(t) - [µ^jI1(xdj)](t)

· |Tj|

= sup
j{1,...,p}

sup
tTj

max
dH2

ydj(t) - [µ^jI1(xdj)](t)

p

 sup
j=1 tTj

max
dH2

ydj(t) - [µ^jI1(xdj)](t)

· |Tj| .

p
· |Tj|
j=1

(8)

By combining (7) and (8) we can notice that

sup
j{1,...,p}

sup
tTj

max
dH2

ydj(t) - [µ^jI1(xdj)](t)

p

p

· |Tj| 

j=1

j=1

max
Tj dH2

ydj(t) - [µ^jI1(xdj)](t)

dt,

i.e. ks0  ks¯c. Then, Q(s0)  Q(s¯cI1,I2). Specifically, the integral mean value theorem guarantees that j  {1, . . . , p}

sup
tTj

max
dH2

ydj(t) - [µ^jI1(xdj)](t)

· |Tj| =

max
Tj dH2

ydj(t) - [µ^jI1(xdj)](t)

dt



max
dH2

ydj(t) - [µ^jI1(xdj)](t)

is constant almost everywhere,

i.e. if and only if s¯cj,I1,I2(t) is constant almost everywhere over Tj. Consequently, if at least one of the functions s¯c1,I1,I2(t), . . . , s¯cp,I1,I2(t) is not constant almost everywhere over its domain then the left side of (7) is strictly greater than the right side (implying Q(s0) > Q(s¯cI1,I2)); otherwise, Q(s0) = Q(s¯cI1,I2).
Generalization of (s¯I1, s¯cI1,I2), Theorem 1 and Theorem 2 to the Smoothed Split Conformal framework
The functions s¯cI1 and s¯I1 are defined as in the Split Conformal framework, except for: k ( respectively) that is the l + n+1 - (l + 1) th ( m + n+1 - (m + 1) th
respectively) smallest value in the corresponding set; similarly to the Split Conformal
framework, if m+n+1-(m+1) > m then H1 = I1 and if m+n+1-(m+1)  0 we arbitrarily set s¯j,I1 = s0j . Theorem 1 and Theorem 2 still hold by substituting (l + 1)(1 - ) , (m + 1)(1 - ) with l + n+1 - (l + 1) , m + n+1 - (m + 1) .

B Supplementary Figures
Acknowledgements
Prof. Vantini and Dr. Fontana acknowledge the financial support from Accordo Quadro ASI-POLIMI "Attivita` di Ricerca e Innovazione" n. 2018-5-HH.0, collaboration agreement between the Italian Space Agency and Politecnico di Milano. The authors are deeply grateful to Clear Channel Italia S.p.A, which provided the data for the case study, and to Agostino Torti for providing part of the code used in the case study.

28

4

Centered temperature function

0

-4

-8 7 a.m.

10 a.m.

1 p.m.

4 p.m.
t

7 p.m.

10 p.m.

1 a.m

Figure B.1. Temperature function (after subtracting the average daily temperature function of the period considered) in degrees Celsius for the observed days (blue curves) and for the two hypothetical days (yellow curves).

References
Antoniadis, A., Brossat, X., Cugliari, J., Poggi, J.M., 2016. A prediction interval for a function-valued forecast model: Application to load forecasting. Int. J. Forecast. 32, 939­947.
Balasubramanian, V., Ho, S.S., Vovk, V., 2014. Conformal prediction for reliable machine learning: theory, adaptations and applications. Newnes.
Cao, G., Yang, L., Todem, D., 2012. Simultaneous Inference For The Mean Function Based on Dense Functional Data. J. Nonparametr. Stat. 24, 359­377.
Degras, D.A., 2017. Simultaneous confidence bands for the mean of functional data. Wiley Interdiscip. Rev. Comput. Stat. 9.
Degras, D.A., 2011. Simultaneous confidence bands for nonparametric regression with functional data. Statist. Sinica 21.
Diquigiovanni, J., Fontana, M., Vantini, S., 2021. The importance of being a band: Finite-sample exact distribution-free prediction sets for functional data. arXiv:2102.06746.
Hastie, T., Tibshirani, R., Friedman, J., 2009. The elements of statistical learning: data mining, inference, and prediction. Springer Science & Business Media.
Hyndman, R.J., Shahid Ullah, M., 2007. Robust forecasting of mortality and fertility rates: A functional data approach. Comput. Statist. Data Anal. 51, 4942­4956.
Inselberg, A., 1985. The plane with parallel coordinates. Vis Comput 1, 69­91.

29

Lei, J., G'Sell, M., Rinaldo, A., Tibshirani, R.J., Wasserman, L., 2018. Distributionfree predictive inference for regression. J. Amer. Statist. Assoc. 113, 1094­1111.
Lei, J., Robins, J., Wasserman, L., 2013. Distribution-free prediction sets. J. Amer. Statist. Assoc. 108, 278­287.
L´opez-Pintado, S., Romo, J., 2009. On the concept of depth for functional data. J. Amer. Statist. Assoc. 104, 718­734.
Papadopoulos, H., Proedrou, K., Vovk, V., Gammerman, A., 2002. Inductive confidence machines for regression, in: European Conference on Machine Learning, Springer. pp. 345­356.
R Core Team, 2020. R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing. Vienna, Austria. URL: https://www. R-project.org/.
Ramsay, J.O., 1982. When the data are functions. Psychometrika 47, 379­396. Ramsay, J.O., Silverman, B.W., 2005. Functional data analysis. Springer series in
statistics. second edition ed., Springer, New York, NY. Rao, P., 1971. Some notes on misspecification in multiple regressions. Amer. Statist.
25, 37­39. Sun, Y., Genton, M.G., 2011. Functional Boxplots. J. Comput. Graph. Statist. 20,
316­334. Torti, A., Pini, A., Vantini, S., 2021. Modelling time-varying mobility flows using
function-on-function regression: Analysis of a bike sharing system in the city of milan. J. R. Stat. Soc. Ser. C. Appl. Stat. 70, 226­247. Van der Vaart, A. W., 2000. Asymptotic statistics, Vol.3. Cambridge University press. Vapnik, V., 1992. Principles of risk minimization for learning theory, in: Advances in neural information processing systems, pp. 831­838. Vovk, V., Gammerman, A., Shafer, G., 2005. Algorithmic learning in a random world. Springer Science & Business Media. Wooldridge, J.M., 1994. A simple specification test for the predictive ability of transformation models. Rev. Econ. Stat. 76, 59­65.
30

