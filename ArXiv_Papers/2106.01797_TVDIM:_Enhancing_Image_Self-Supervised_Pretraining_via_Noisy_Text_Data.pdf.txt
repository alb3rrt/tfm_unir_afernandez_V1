TVDIM: Enhancing Image Self-Supervised Pretraining via Noisy Text Data
Pengda Qin1 , Yuhong Li1 , Kefeng Deng1 , Qiang Wu1 1Alibaba Group
{pengda.qpd, daniel.lyh, kefeng.deng, qiangwu.wq}@alibaba-inc.com

arXiv:2106.01797v2 [cs.CL] 13 Jun 2021

Abstract
Among the multi-modal signals of the real world, language is the modality nearest to human understanding level; in contrast, vision reflects the real world honestly and objectively. When in a visual scene, machines are expected to understand visual information at the human level. Inspired by this, we propose a novel self-supervised visual learning method, named Text-enhanced Visual Deep InfoMax (TVDIM) strategy, to learn visual representations enhanced by language-modality information. The co-occurrence of textual and visual information commonly exists around us, such as books and internet; More importantly, this text provides the annotation-free information for better understanding its corresponding images. Based on that, we make full use of this information overlap, and utilize the concept of mutual information maximization to integrate textual information into visual representations in a self-supervised way. Considering the information gap between different modalities, we adopt contrastive learning to measure mutual information by Ranking. During evaluation, we directly use the pretrained visual representations to complete various image classification tasks. Experimental results show that, with the assistance of annotation-free text information, TVDIM significantly outperforms previous visual self-supervised methods by processing the same set of images.
1 Introduction
Human access the real world via multi-modal signals everytime and everywhere. Most part is the original signal that is equivalent to all living things in this real world, like vision and sound, while a small part is unique to humans. Language is obviously in the latter camp, which is the product of human evolution, and it is ever-evolving. Therefore, compared with vision, the information expressed by language is quite closer to be the human understanding level. Not only that, theoretically a visual scene can provide infinite information along with the unlimited increasing of resolution, while human just selectively receive information to make reasonable reactions. To promote machine intelligence to hu-

Figure 1: Our overall pipeline consists of two stages. The first stage is to do self-supervised training for image encoder via the proposed TVDIM. Noted that the training image-text pairs naturally exist in various medias and can be obtained without human annotation. The second stage is to freeze image encoder and directly do featurebased fine-tuning and prediction for downstream tasks.
man level, many advanced architectures have been proposed and their excellent learning ability has been fully proved. Deep neural networks, especially convolutional neural networks (CNNs), have drastically advanced the development of vision-related tasks. The conventional way is to train deep neural networks in a fully supervised manner; However, not only it is prohibitively expensive due to a large amount of efforts on manual labeling, but also the randomly-initialized parameters violate human learning process and can not provide any prior valuable knowledge for the following tasks. Under this background, self-supervised learning (SSL) is a rational solution to overcome these issues. As for visual representation learning, the typical paradigm is to pretrain a large model via well-designed pretext tasks from a large annotation-free dataset. The definitions of pretext tasks in previous works are devoted to exploring rational supervised signals from the image spatial structure [Doersch et al., 2015; Noroozi and Favaro, 2016], multi-view of image [Tian et al., 2019] or relationships between local and global representations [Bachman et al., 2019]. We believe that these visionlevel pretext tasks can encode useful prior knowledge, such as color, position and spatial structure; However, we argue that it merely stays at vision-level (intra-modality learning), and inter-modality prior knowledge is also conducive to the performance of various visual tasks.
In this paper, we present a novel image self-supervised

pretraining strategy TMDIM that considers both the intramodality representation learning and inter-modality representation learning. The intra-modality component focuses on modeling the vision-level knowledge, where we adopt the recently proposed method of Augmented Multiscale Deep InfoMax (AMDIM) [Bachman et al., 2019]. The pretext task of AMDIM is to maximize mutual information between a global summary feature vector and a collection of local feature vectors pulled from intermediate layers. Noise-contrastive estimation (NCE) [Gutmann and Hyva¨rinen, 2010; McAllester and Statos, 2018; Poole et al., 2019] is used to model mutual information maximization. The inter-modality component integrates the image-related text to enhance image encoder at the human understanding level. Instead of predictive learning [Joulin et al., 2016; Li et al., 2017], we adopt contrastive learning [Tian et al., 2019] to realize mutual information maximization between image-text pairs from both global and local feature level. It is worthy of note that, in this inter-modality case, NCE just yields the suboptimal performance, because the image-text pairs exist information gap even though their information is highly matched. To solve this problem, we decide to model the inter-modality mutual information maximization in a pairwise ranking way [Kiros et al., 2014]. Unlike the infinite penalty of NCE loss function, pairwise ranking loss function defines a margin value to limit the boundary of penalty level, and our experimental results validate that pairwise ranking loss function achieves better performance and makes the training process more stable. Besides the superiorities mentioned above, the integration of inter-modality learning increases the difficulty of selfsupervised learning, which is also beneficial to more robust and generic visual representations. Our main contributions are three-fold:
· TVDIM is the first to consider both intra-modality and inter-modality self-supervised learning for visual representation learning;
· We prove that pairwise ranking loss is more suitable to model inter-modality mutual information maximization;
· TVDIM is able to enhance visual representations at the human understanding level, and it significantly improves the performance of various downstream tasks.
2 Related Work
As an important member of unsupervised-learning family, the objective of self-supervised learning is more clear and definite. Self-supervised learning is to make subsequent problem solving easier via learning transformations of the data. In the field of visual representation learning, previous research works are designed either in an intra-modality way or in an inter-modality way. Figure 2(a) concludes two different kinds of intra-modality learning. The left one introduces a traditional unsupervised learning technique that aims to rebuild inputs from visual representations [Rumelhart et al., 1985; Hinton and Salakhutdinov, 2006; Erhan et al., 2010]. However, it is proved that these works just achieve the goal of data compression, and merely a small amount of valuable knowledge for downstream task is learned. To overcome this

Figure 2: Previous related works can be divided into two camps, intra-modality learning (a) and inter-modality learning (b). Based on them, TVDIM (c) combines their merits to further robust selfsupervised visual representation learning.
problem, current works (right part of Figure 2(a)) resort to the reasonable transformations (f (), g(), h()) of data to encode more valuable information without human intervention, including counting the objects in a image [Noroozi et al., 2017], recovering colors from grayscale images [Zhang et al., 2016], or predicting image spatial structure [Noroozi and Favaro, 2016; Kim et al., 2018; Doersch et al., 2015; Oord et al., 2018; Gidaris et al., 2018]. These pretext tasks belong to predictive learning. Recently, contrastive learning is also considered to increase the diversity and difficulty of pretext tasks. Contrastive Multiview Coding (CMC) [Tian et al., 2019] learns representations that capture information shared between multiple sensory views of an image. Deep InfoMax (DIM) [Hjelm et al., 2018] and AMDIM [Bachman et al., 2019] perform representation learning by maximizing mutual information between feature representations from different encoder layers. Besides that, the concept of inter-modality learning has already been proposed previously (Figure 2(b)). Kiros et al. [2014] present a contrastive learning approach for learning visual-semantic embeddings for cross-modality retrieval, and their text part merely consists of words or phrases. Mahajan et al. [2018] utilize the hashtags as the supervised signals and perform the predictive learning on billions of social media images. Different from these works, we are the first work to consider both intra-modality and inter-modality learning for robust visual representations. More than that, we are in a more complicated setting: our image-related text could be a sentence or a paragraph. We believe that, visual intra-modality learning is beneficial to explore more visual information, and vision-text inter-modality learning enables image encoder to better understand visual information at the human understanding level.
Currently, with the significant advance of transformerbased language pretraining [Devlin et al., 2018; Yang et al., 2019], a series of works [Tsai et al., 2019; Lu et al., 2019; Tan and Bansal, 2019; Tan and Bansal, 2019; Sun et al., 2019]

Figure 3: Overview of TVDIM. Our self-supervised pretraining consists of intra-modality learning (red arrows) and inter-modality learning (green arrows). Our ultimate goal is to learn a sophisticated image encoder (red part) for better visual representations. In terms of intramodality, we integrate visual-related prior knowledge into image encoder by maximizing NCE-based infomax from multiple scales. Data augmentation part is not presented here for clarity. In terms of inter-modality, we first encode text into text representations, and then maximize the mutual information between text and image via pairwise ranking. This inter-modality mutual information maximization is calculated in both textual feature space (light green arrows) and visual feature space (dark green arrows).

aim to do multi-modality self-supervised learning based on transformer network. They all utilize self-attention mechanism [Vaswani et al., 2017] to model intra-modality learning. In the meantime, Tan and Bansal [2019] propose a crossattention mechanism to model inter-modality learning; Lu et al. [2019] propose a co-attention mechanism to enable the fusion of cross-modality information. In terms of motivation, these works focus on better multi-modality representation, while TVDIM aims to obtain better visual representations. Besides that, the image input formats are different. TVDIM is based on the original image signals. To satisfy the transformer structure, these works need to preprocess images into a series of ROI features via the pretrained R-CNN [Ren et al., 2015] network; Thus, error propagation is inevitable.
3 Methodology
The self-supervised training of TVDIM consists of two parallel processes (Figure 3), intra-modality learning and intermodality learning. Accordingly, the following descriptions are divided into two parts. The first part presents a brief introduction of visual intra-modality learning. Given a positive image pair and a set of negative pairs, we aim to do multi-scale infomax across multiple views via NCE loss function. The second part detailedly introduces vision-text intermodality learning. Given a positive image-text pair and a set of negative image-text pairs, we use pairwise ranking loss to maximize the multi-scale mutual information between the positive pair.
3.1 Visual Intra-modality Learning
Our intra-modality learning follows the infomax idea of DIM and AMDIM. AMDIM is the extended version of DIM. Given an image input xv, the infomax process is implemented within the interior structure of image encoder V. The core concept is to maximize mutual information between antecedent features va = Va(xv) and consequent features

vicj  {Vc(xv)ij : i, j}. Antecedent features are the features that encode the image input to condition on, and consequent features are the features to be predicted. The subscripts i and j index the two spatial dimensions of the array of activations in the intermediate layer of image encoder V. AMDIM constructs a distribution p(va, vicj) over (antecedent, consequent) feature pairs via ancestral sampling: First step is to use a data distribution D to sample input xv  D, and second is based on two uniform distributions u(i), u(j) to select the valid spatial indices i, j. Given p(va, vicj) and marginal distributions p(va), p(vicj), AMDIM seeks an image encoder V that maximizes the mutual information I(va, vicj) in p(va, vicj). AMDIM equally considers three different scales of encoder layers: layer 1, 5 and 7. Respectively, they have the spatial dimension 1 × 1, 5 × 5 and 7 × 7. As for mutual information maximization, they correspond to three different antecedentconsequent pairs, including 1-to-5, 1-to-7 and 5-to-5.
Various strategies can be used to realize mutual information maximization. DIM mainly compares two different ways, Jensen-Shannon Divergence (JSD) [Lin, 1991] and Noise-Contrastive Estimation. Their experimental results suggest that infoNCE [Oord et al., 2018] achieves better performance on downstream tasks in most cases, especially when training on less challenging data. On balance, both DIM and AMDIM regard infoNCE as the optimal loss function. Noted that AMDIM adopts data augmentation technique [Lim et al., 2019] to further increase the difficulty of of selfsupervised learning. During each computation of infoNCE loss, antecedent features va and consequent features vicj are from augmented views of each input. For a image input xv, AMDIM samples a pair of augmented images x1v  A(xv) and x2v  A(xv). Here, A(x) denotes a distribution of images generated by applying stochastic data augmentation. Therefore, for one kind of antecedent-consequent pairs, the infoNCE-based infomax objective can be formulated as be-

low:

Lintra =

L(Va(x1v), Vc(x2v)ij , Nc) (1)

xv i j

where Nc denotes the negative samples. Nc acts as many distractor consequent features drawn independently from the marginal distribution pA(Vc(x2v)ij). Here, L() denotes the infoNCE which is a softmax-based version of NCE. For
brevity, we omit the data augmentation symbols and spatial
indices, then the overall infoNCE loss function can be rewrite
as follows:

Lintra =
vi
L(va, vc, Nc) = -log

L(va, vc, Nc)

j

exp((va, vc))

(2)

,

v~cNc{vc} exp((va, v~c))

Empirically, the matching score () is calculated by dot product operation.

3.2 Text-Enhanced Visual Inter-Modality Learning
Simply, self-supervised learning is to find annotation-free supervised signals to do supervised learning. The above subsection merely focuses on the intra-modality signals, while we believe that there exist massive valuable self-supervised signals of inter-modality and this type of information is able to comprehensively enrich visual representations. Compared with vision modality, language modality can express higherlevel information and is the nearest modality to human understanding level. Self-supervised learning is to promote downstream tasks, and downstream tasks are defined by human and at the service of human. Therefore, for the purpose of enhancing visual representation learning, we consider language modality as the auxiliary modality. Text is one of the core formats of language modality, and the co-occurrence of text and image is common in various media sources. More importantly, the prerequisite of co-occurrence is the existence of high information overlap between two modalities. This overlapping information provides sufficient conditions to produce rational supervised signals for self-supervised learning.
Figure 3 presents the framework we use to enrich visual representations via textual information. We first utilize a text encoder T to model text information. Due to the role of auxiliary modality, we just consider the text global features t = T (xt). On the other side, we still utilize both global visual features and local visual features; For brevity, they are uniformly denoted as v. The prerequisite of mutual information calculation is the same feature space. In other words, t and v should be projected into the same unified space. We consider two different spaces: the image feature space and the text feature space. To calculate mutual information in the image space, we adopt a feed-forward layer ftimg(t) which consists of a fully-connected layer and a batch-normalization layer; For v, we directly use a batch-normalization layer fvimg(v). As for the text feature space, there are opposite settings: fttxt(t) is a batch-normalization layer and fvtxt(v) includes a fully-connected layer and a batch-normalization

layer. Under the same feature space, we leverage the dot product operation to calculate the matching scores:

img(t, v) = ftimg(t) fvimg(v)

(3)

txt(t, v) = fttxt(t) fvtxt(v)

Our experimental results demonstrate that batch normalization plays a crucial role in multimodal mutual information calculation.
Even though infoNCE is regarded as a good choice for visual intra-modality learning, we argue that there are several drawbacks to apply infoNCE in our inter-modality learning. InfoNCE is formulated in the softmax-based version, which means that the optimal situation is that the probability of positive sample is 1. In other words, infoNCE imposes infinite rewards to positive samples and infinite penalty to negative samples. However, the information overlap between image and text is not equal to perfect alignment. Thus, a certain part of textual information is noise information. Our experimental results also demonstrate that, the performance of infoNCE begins to get the downward trend when the training time exceeds a certain value. To overcome this problem, we decide to substitute it with pairwise ranking (P wR) loss. Pairwise ranking loss adopts a margin to define the gap between positive samples and negative samples. When the difference of mutual information is larger than this predefined margin, no penalty will be imposed. Based on that, our pairwise ranking loss is formulated as follows:

Ltv =

max{0,  - (t, v) + (t, vk)},

vk

(4)

Lvt =

max{0,  - (v, t) + (v, tk)},

vk

where k denotes the index of negative samples, and we omit the calculations in different feature spaces for brevity. To sum up, the overall inter-modality loss function is presented below:

Linter = Ltv + Lvt,

(5)

4 Experiments
According to the conventional training procedure of selfsupervised system, the following evaluation is two-stage, including the pre-training stage and the feature-based finetuning stage.
4.1 Pre-Training
For better evaluation, We use two different image-text datasets to do the self-supervised pretraining. For the selection of dataset, there are two prerequisites: obtaining without human annotation and the high information overlap within each image-text pair. Two datasets shown below have different data scale and different average lengths of text sequence. Below presents the dataset descriptions and the implementation details.

Model
CIFAR10 CIFAR100 STL10

TVDIM SMALL

0.8M

3.3M

AMDIM

TVDIM

AMDIM

TVDIM

(Lin., MLP) (Lin., MLP) (Lin., MLP) (Lin., MLP)

66.4, 71.7 43.1, 45.9 66.8, 70.1

68.7, 72.3 45.0, 47.5 70.1, 71.9

66.9, 71.8 45.2, 48.2 68.6, 71.8

70.5, 73.4 47.6, 49.8 71.7, 73.5

TVDIM LARGE

0.8M

3.3M

AMDIM

TVDIM

AMDIM

TVDIM

(Lin., MLP) (Lin., MLP) (Lin., MLP) (Lin., MLP)

69.6, 73.1 45.0, 47.3 68.2, 72.1

70.5, 73.8 48.3, 49.7 72.9, 74.1

71.4, 74.8 46.6, 49.9 68.5, 74.1

72.2, 75.0 50.0, 51.3 73.2, 75.5

Table 1: Top-1 classification accuracy (%) of various image classification tasks. We adopt both linear layer (Lin.) and MLP layer (M LP ) as the classification layer to evaluate the performance. Noted that the pretraining setting and fine-tuning setting are different from that of AMDIM. Therefore, the results shown here are not comparable with the results presented in the AMDIM paper. AMDIM unifies the training dataset for both pretraining stage and fine-tuning stage, and their Linear layer and MLP layer are also fine-tuned without gradient backpropagation to image encoder during pretraining stage. But these settings is not applicable to our case. Here we consider a more challenge setting that image encoder is pretrained from a unified large-scale dataset, and then applied to various downstream tasks.

Conceptual Captions Dataset
Conceptual Captions[Sharma et al., 2018] is a collection of 3.3 million image-caption pairs automatically scraped from alt-text enabled web images. The average length of captions is around 10. It is easy to collect, and each image-text pair exists high information overlap. However, the automatic collection and sanitation process leaves some noise and the captions are sometimes not human-like or short on details. Following the design standard of AMDIM, we use two network structures with different parameter scales, TVDIMsmall: (ndf=64, nrkhs=512, ndepth=8)1, TVDIMlarge: (ndf=96, nrkhs=768, ndepth=8). The pretraining process is on 4 P100 GPUs with a total batch size of 480 for 15 epochs. The negative samples for each processing image-text pair are selected from the current batch it belongs to. We use the Adam optimizer [Kingma and Ba, 2014] with initial learning rates  of 1e-4, 1 = 0.9, 2 = 0.999, weight decay of 1e-4. The margin value  of pairwise ranking loss is set as 0.5. In terms of data augmentation, we apply resized cropping, color jitter, and random conversion to grayscale for each image input.
MM-IMDB Dataset
MM-IMDb dataset [Arevalo et al., 2017] is built from the Movielens 20M dataset2. The MM-IMDb dataset comprises 25,959 movies along with their plot, poster, genres and other 50 additional metadata fields. In our setting, we use the (poster, plot) as our image-text pair. Note that each plot contains on average 92.5 words, which is quite longer than that of Conceptual Captions Dataset. Considering its small scale, we adopt a relatively small-scale image encoder structure:
TVDIMMM-IMDb: (ndf=64, nrkhs=512, ndepth=1). We set the batch size as 256 and pre-train for 80 epochs. The negative samples are also selected from training batches. The optimizer has the same setting as Conceptual Captions dataset, except for the learning rate  of 5e-4. The margin value  is 0.5 as well.
1rkhs is the abbreviation of Reproducing Kernel Hilbert Spaces, which denotes the dimension of visual representation.
2http://grouplens.org/datasets/movielens/

Model
CNN Supervised L. AMDIM TVDIM

sample
35.0 37.2 37.7 39.8

F-Score
micro macro
34.0 21.0 37.4 21.2 37.7 20.7 39.5 24.1

weight
37.0 36.8 37.4 39.5

Table 2: Comparison results of MM-IMDb dataset. The MM-IMDb training dataset is applied for both the pretraining stage and the finetuning stage. CNN denotes the model from Arevalo et al. [2017].

4.2 Fine-Tuning on Downstream Tasks
Following the evaluation protocol of previous works [Kolesnikov et al., 2019; Bachman et al., 2019], we evaluate TVDIM on several image classification benchmarks, including CIFAR10, CIFAR100 and STL10. The input images are uniformly resized to 128*128 before encoding. Similar to the conventional setting, the self-supervised pretrained image encoder (from training dataset Dssl) is frozen and directly used to generate visual representations, and then we train a linear classifier or a MLP classifier on the downstream image classification dataset Dcl. Unlike previous settings, two training datasets Dssl, Dcl are not the same one: Dssl is uniformly the Conceptual Captions dataset. Therefore, our evaluation method belongs to the transfer task that is also widely used for evaluating self-supervised visual representation learning [Bachman et al., 2019]. The comparison results are presented in Table 1. Besides the different scales of image encoder, we also compare the performance from two different sizes of self-supervised training dataset. Here we assign the full Conceptual Captions dataset as the big one 3.3M, and randomly select a quarter of it as the small one 0.8M. It is obvious that TVDIM outperforms AMDIM on various image classification benchmarks.
Besides that, we also perform the comparison experiments on the multi-modal dataset MM-IMDb, where Dssl and Dcl is unified into the same dataset. The downstream task is still the image classification task. The performance is shown in Table 2. We present the result of supervised learning of our image encoder, which achieves slightly better performance improvement compared with the previously-published result.

Model
TVDIM w/o BN w/o Local w/o V2T

CIFAR10 Accuray(%)
72.2
69.5 70.7 71.2

STL10 Accuray(%)
73.2
70.1 70.8 71.9

MM-IMDb F1-macro
24.1
22.1 22.8 23.5

Table 3: The comparison results of ablation study. Here BN denotes batch normalization; Local denotes that the local visual representations are considered for inter-modality learning; V 2T denotes the inter-modality mutual information maximization in text feature space. The results of CIFAR10 and STL10 are from T V DIMlarge on 3.3M dataset.

Then, we compare the performance of AMDIM and TVDIM, and TVDIM yields the significant improvement with the help of inter-modality learning.
To sum up, the proposed TVDIM is able to effectively encode the image-related text information into visual representations, and further promote the visual understanding ability of machine. More than that, image-text pairs are accessible and massive without human annotation, which is crucial for self-supervised learning.
4.3 Analysis
Ablation Studies We conduct ablation studies from three aspects in Table 3. CIFAR10 and Place205 are selected as the representatives for the transfer setting. For MM-IMDb, we only perform the F1-macro score because it is the most widely-used F1 score. First, we can see that batch normalization is vital to the calculation of inter-modality mutual information. Second, with the assistance of visual local features, the inter-modality mutual information maximization achieves better performance. Finally, we find that the performance get further improved by maximizing mutual information in both image feature space and text feature space.
Analysis of Pairwise Ranking Loss Considering the information gap between vision modality and text modality, we adopt pairwise ranking loss for intermodality self-supervised learning. Table 4 proves the superiority of pairwise ranking loss in this case. According to the definitions, NCE is from the angle of probability, which assigns infinite reward to positive samples and infinite penalty to negative samples. It is reasonable for our visual intramodality learning, because global features and local features are derived from the same image. For inter-modality, imagetext pairs have high information overlap while the information gap also exists. Therefore, if we infinitely maximize their mutual information, it would run counter to our desire. Pairwise ranking loss is defined from the difference of information amount. More specifically, the boundary of reward and penalty is that the difference reaches the requirement of margin.
Impact of Text Encoder We have tried several different settings for text encoder. Here we consider MM-IMDb dataset to evaluate the influence of

Model
NCE Pairwise R.

CIFAR10 Accuray(%)
67.7 72.2

STL10 Accuray(%)
69.8 73.2

MM-IMDb F1-macro
22.0 24.1

Table 4: Comparison results of different contrastive learning method for inter-modality learning. The results of CIFAR10 and STL10 are from T V DIMlarge on 3.3M dataset.

Text Encoder
CNN LSTM BERTmean W2Vmean

sample
38.2 38.5 39.6 39.8

F-Score
micro macro
38.2 23.0 38.4 22.4 39.2 23.5 39.5 24.1

weight
38.5 38.5 39.2 39.5

Table 5: Analysis of the selection of text encoder on MM-IMDb dataset. W 2V mean and BERT mean denote the mean operation on the pretrained word embedding, and word embedding is frozen during self-supervised learning.

text encoder T (). Compared with Conceptual Captions dataset, the text descriptions of MM-IMDb dataset is more complex; Therefore, the impact of text encoder is more obvious. Table 5 illustrates that the mean of word embedding3 achieves the best performance, even though it is the simplest way. Our explanation is that the trainable parameters added from text encoder (CNN or LSTM) reduce the difficulty of self-supervised learning. To be specific, it becomes relatively easier to maximize the mutual information between image and text. Consequently, the performance goes down. We also have tried the contextualized word embedding from BERT [Devlin et al., 2018], but it still obtains the suboptimal performance. It proves that contextualized word embeddings should be used via fining-tuning BERT model rather than directly used.
4.4 Discussion
We have shown that pairwise ranking loss is an effective contrastive learning loss for self-supervised learning. However, when we attempt to adopt it to the visual intra-modality learning, the performance is quite poor. We have checked the intermediate results of the pretraining stage and find that the loss value drops sharply, which means that it is quite easy to convergence. In other words, the learning process of selfsupervised pretraining is lack of difficulty, so our image encoder cannot learn sufficient knowledge. We conclude that it is crucial to control the difficulty of self-supervised learning in an appropriate range. Within this range, the increase of difficulty is beneficial for more robust visual representations. This point of view is also reflected by the impact of text encoder in Section 4.3. Without introducing extra trainable parameters into TVDIM, the mean of word embeddings achieves the best performance. But, for the design of text encoder, we need to acknowledge that the mean of word embed-
3https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit

dings is just the better one in our current solutions but not the best, it still has a shortage of poor diversity. We will continue to find better solutions for text encoder in future work.
5 Conclusion
In this paper, we present a novel self-supervised visual representation learning method by considering both visual intramodality learning and vision-text inter-modality learning. Various media provide massive image-text pairs that exist high information overlap and are easily obtained without human annotations. The proposed TVDIM is able to effectively encode valuable information from text modality into visual representations. Considering the existence of information gap, we adopt pairwise ranking loss to maximize the mutual information for inter-modality learning. Theoretically, TVDIM is a model-agnostic technique that can be applied to any existing self-supervised visual representation learning frameworks regardless of network structures. Experimental results show that TVDIM is the better self-supervised strategy for visual representation learning and significantly outperforms previous methods.
References
[Arevalo et al., 2017] John Arevalo, Thamar Solorio, Manuel Montes-y Go´mez, and Fabio A Gonza´lez. Gated multimodal units for information fusion. arXiv preprint arXiv:1702.01992, 2017.
[Bachman et al., 2019] Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing mutual information across views. arXiv preprint arXiv:1906.00910, 2019.
[Devlin et al., 2018] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
[Doersch et al., 2015] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context prediction. In Proceedings of the IEEE International Conference on Computer Vision, pages 1422­ 1430, 2015.
[Erhan et al., 2010] Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent, and Samy Bengio. Why does unsupervised pre-training help deep learning? Journal of Machine Learning Research, 11(Feb):625­660, 2010.
[Gidaris et al., 2018] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. arXiv preprint arXiv:1803.07728, 2018.
[Gutmann and Hyva¨rinen, 2010] Michael Gutmann and Aapo Hyva¨rinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pages 297­304, 2010.

[Hinton and Salakhutdinov, 2006] Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks. science, 313(5786):504­507, 2006.
[Hjelm et al., 2018] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. arXiv preprint arXiv:1808.06670, 2018.
[Joulin et al., 2016] Armand Joulin, Laurens van der Maaten, Allan Jabri, and Nicolas Vasilache. Learning visual features from large weakly supervised data. In European Conference on Computer Vision, pages 67­84. Springer, 2016.
[Kim et al., 2018] Dahun Kim, Donghyeon Cho, Donggeun Yoo, and In So Kweon. Learning image representations by completing damaged jigsaw puzzles. In 2018 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 793­802. IEEE, 2018.
[Kingma and Ba, 2014] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
[Kiros et al., 2014] Ryan Kiros, Ruslan Salakhutdinov, and Richard S Zemel. Unifying visual-semantic embeddings with multimodal neural language models. arXiv preprint arXiv:1411.2539, 2014.
[Kolesnikov et al., 2019] Alexander Kolesnikov, Xiaohua Zhai, and Lucas Beyer. Revisiting self-supervised visual representation learning. arXiv preprint arXiv:1901.09005, 2019.
[Li et al., 2017] Ang Li, Allan Jabri, Armand Joulin, and Laurens van der Maaten. Learning visual n-grams from web data. In Proceedings of the IEEE International Conference on Computer Vision, pages 4183­4192, 2017.
[Lim et al., 2019] Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and Sungwoong Kim. Fast autoaugment. arXiv preprint arXiv:1905.00397, 2019.
[Lin, 1991] Jianhua Lin. Divergence measures based on the shannon entropy. IEEE Transactions on Information theory, 37(1):145­151, 1991.
[Lu et al., 2019] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. arXiv preprint arXiv:1908.02265, 2019.
[Mahajan et al., 2018] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining. In Proceedings of the European Conference on Computer Vision (ECCV), pages 181­196, 2018.
[McAllester and Statos, 2018] David McAllester and Karl Statos. Formal limitations on the measurement of mutual information. arXiv preprint arXiv:1811.04251, 2018.
[Noroozi and Favaro, 2016] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations

by solving jigsaw puzzles. In European Conference on Computer Vision, pages 69­84. Springer, 2016.
[Noroozi et al., 2017] Mehdi Noroozi, Hamed Pirsiavash, and Paolo Favaro. Representation learning by learning to count. In Proceedings of the IEEE International Conference on Computer Vision, pages 5898­5906, 2017.
[Oord et al., 2018] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.
[Poole et al., 2019] Ben Poole, Sherjil Ozair, Aaron van den Oord, Alexander A Alemi, and George Tucker. On variational bounds of mutual information. arXiv preprint arXiv:1905.06922, 2019.
[Ren et al., 2015] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing systems, pages 91­99, 2015.
[Rumelhart et al., 1985] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning internal representations by error propagation. Technical report, California Univ San Diego La Jolla Inst for Cognitive Science, 1985.
[Sharma et al., 2018] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556­2565, 2018.
[Sun et al., 2019] Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A joint model for video and language representation learning. arXiv preprint arXiv:1904.01766, 2019.
[Tan and Bansal, 2019] Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from transformers. arXiv preprint arXiv:1908.07490, 2019.
[Tian et al., 2019] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. arXiv preprint arXiv:1906.05849, 2019.
[Tsai et al., 2019] Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, J Zico Kolter, Louis-Philippe Morency, and Ruslan Salakhutdinov. Multimodal transformer for unaligned multimodal language sequences. arXiv preprint arXiv:1906.00295, 2019.
[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998­6008, 2017.
[Yang et al., 2019] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237, 2019.

[Zhang et al., 2016] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In European conference on computer vision, pages 649­666. Springer, 2016.
References
[Arevalo et al., 2017] John Arevalo, Thamar Solorio, Manuel Montes-y Go´mez, and Fabio A Gonza´lez. Gated multimodal units for information fusion. arXiv preprint arXiv:1702.01992, 2017.
[Bachman et al., 2019] Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing mutual information across views. arXiv preprint arXiv:1906.00910, 2019.
[Devlin et al., 2018] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
[Doersch et al., 2015] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context prediction. In Proceedings of the IEEE International Conference on Computer Vision, pages 1422­ 1430, 2015.
[Erhan et al., 2010] Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent, and Samy Bengio. Why does unsupervised pre-training help deep learning? Journal of Machine Learning Research, 11(Feb):625­660, 2010.
[Gidaris et al., 2018] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. arXiv preprint arXiv:1803.07728, 2018.
[Gutmann and Hyva¨rinen, 2010] Michael Gutmann and Aapo Hyva¨rinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pages 297­304, 2010.
[Hinton and Salakhutdinov, 2006] Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks. science, 313(5786):504­507, 2006.
[Hjelm et al., 2018] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. arXiv preprint arXiv:1808.06670, 2018.
[Joulin et al., 2016] Armand Joulin, Laurens van der Maaten, Allan Jabri, and Nicolas Vasilache. Learning visual features from large weakly supervised data. In European Conference on Computer Vision, pages 67­84. Springer, 2016.
[Kim et al., 2018] Dahun Kim, Donghyeon Cho, Donggeun Yoo, and In So Kweon. Learning image representations by completing damaged jigsaw puzzles. In 2018 IEEE Winter

Conference on Applications of Computer Vision (WACV), pages 793­802. IEEE, 2018.
[Kingma and Ba, 2014] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
[Kiros et al., 2014] Ryan Kiros, Ruslan Salakhutdinov, and Richard S Zemel. Unifying visual-semantic embeddings with multimodal neural language models. arXiv preprint arXiv:1411.2539, 2014.
[Kolesnikov et al., 2019] Alexander Kolesnikov, Xiaohua Zhai, and Lucas Beyer. Revisiting self-supervised visual representation learning. arXiv preprint arXiv:1901.09005, 2019.
[Li et al., 2017] Ang Li, Allan Jabri, Armand Joulin, and Laurens van der Maaten. Learning visual n-grams from web data. In Proceedings of the IEEE International Conference on Computer Vision, pages 4183­4192, 2017.
[Lim et al., 2019] Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and Sungwoong Kim. Fast autoaugment. arXiv preprint arXiv:1905.00397, 2019.
[Lin, 1991] Jianhua Lin. Divergence measures based on the shannon entropy. IEEE Transactions on Information theory, 37(1):145­151, 1991.
[Lu et al., 2019] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. arXiv preprint arXiv:1908.02265, 2019.
[Mahajan et al., 2018] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining. In Proceedings of the European Conference on Computer Vision (ECCV), pages 181­196, 2018.
[McAllester and Statos, 2018] David McAllester and Karl Statos. Formal limitations on the measurement of mutual information. arXiv preprint arXiv:1811.04251, 2018.
[Noroozi and Favaro, 2016] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In European Conference on Computer Vision, pages 69­84. Springer, 2016.
[Noroozi et al., 2017] Mehdi Noroozi, Hamed Pirsiavash, and Paolo Favaro. Representation learning by learning to count. In Proceedings of the IEEE International Conference on Computer Vision, pages 5898­5906, 2017.
[Oord et al., 2018] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.
[Poole et al., 2019] Ben Poole, Sherjil Ozair, Aaron van den Oord, Alexander A Alemi, and George Tucker. On variational bounds of mutual information. arXiv preprint arXiv:1905.06922, 2019.
[Ren et al., 2015] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances

in neural information processing systems, pages 91­99, 2015.
[Rumelhart et al., 1985] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning internal representations by error propagation. Technical report, California Univ San Diego La Jolla Inst for Cognitive Science, 1985.
[Sharma et al., 2018] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556­2565, 2018.
[Sun et al., 2019] Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A joint model for video and language representation learning. arXiv preprint arXiv:1904.01766, 2019.
[Tan and Bansal, 2019] Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from transformers. arXiv preprint arXiv:1908.07490, 2019.
[Tian et al., 2019] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. arXiv preprint arXiv:1906.05849, 2019.
[Tsai et al., 2019] Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, J Zico Kolter, Louis-Philippe Morency, and Ruslan Salakhutdinov. Multimodal transformer for unaligned multimodal language sequences. arXiv preprint arXiv:1906.00295, 2019.
[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998­6008, 2017.
[Yang et al., 2019] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237, 2019.
[Zhang et al., 2016] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In European conference on computer vision, pages 649­666. Springer, 2016.

