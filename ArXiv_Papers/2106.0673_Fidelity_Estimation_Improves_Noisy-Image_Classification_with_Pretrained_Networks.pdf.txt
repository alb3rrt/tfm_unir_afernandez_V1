SUBMISSION TO IEEE SIGNAL PROCESSING LETTERS

1

Fidelity Estimation Improves Noisy-Image Classification with Pretrained Networks
Xiaoyu Lin, Deblina Bhattacharjee, Majed El Helou, Members, IEEE, and Sabine Su¨sstrunk, Fellow, IEEE.

arXiv:2106.00673v1 [cs.CV] 1 Jun 2021

Abstract--Image classification has significantly improved using deep learning. This is mainly due to convolutional neural networks (CNNs) that are capable of learning rich feature extractors from large datasets. However, most deep learning classification methods are trained on clean images and are not robust when handling noisy ones, even if a restoration preprocessing step is applied. While novel methods address this problem, they rely on modified feature extractors and thus necessitate retraining. We instead propose a method that can be applied on a pretrained classifier. Our method exploits a fidelity map estimate that is fused into the internal representations of the feature extractor, thereby guiding the attention of the network and making it more robust to noisy data. We improve the noisy-image classification (NIC) results by significantly large margins, especially at high noise levels, and come close to the fully retrained approaches. Furthermore, as proof of concept, we show that when using our oracle fidelity map we even outperform the fully retrained methods, whether trained on noisy or restored images.
Index Terms--Noisy-Image Classification, Deep Learning, Image Restoration, Data Fidelity

Fig. 1. High-level overview of our FG-NIC method. During training, we freeze the pretrained classification and restoration networks, highlighted with green blocks. The trainable modules are highlighted with blue blocks. (*), (+), (), and (·) denote multiplication, addition, concatenation and the fully connected part of a classification network, respectively.

I. INTRODUCTION
Traditionally, image classification methods have relied on a limited set of hand-crafted features [1]­[6]. Deep learning approaches, especially supervised methods using convolutional neural networks (CNNs) [7], [8] trained on large annotated datasets [9], are capable of extracting rich feature representations [9], [10] and thus improve performance. However, obtaining reliable and generalizable feature extractors remains a challenge [11]­[14]. It is thus desirable to exploit existing, pretrained feature extractors in a modular way across datasets or applications without the need to retrain them. Here, we exploit the feature extractor pretrained on clean images to work with noisy input.
Classification methods trained on datasets with clean images show a significant performance drop on noisy or restored images [15]. Yet, in many applications, the captured images are inherently noisy, for instance, in low-light conditions, short exposure capture, in multi-spectral acquisition, and when employing inexpensive surveillance cameras, etc. A recent study [16] confirmed that image classification accuracy drops significantly when the input images are noisy. Furthermore, even a straight-forward restoration preprocessing step failed to improve classification.
Recently, two studies specifically addressed Noisy-Image Classification (NIC). However, they either require costly re-
Submitted for review on June 1, 2021. School of Computer and Communication Sciences, EPFL, Switzerland. {xiaoyu.lin, deblina.bhattacharjee, majed.elhelou, sabine.susstrunk} @epfl.ch All code and supplementary material at https://github.com/IVRL/FG-NIC

training [17] or even necessitate an entire redesign of the CNNbased classifier [18].
In contrast, our proposed approach can be extended to any feature extractor without retraining, enabling existing networks to also classify noisy images. Our Fidelity-Guided NoisyImage Classification (FG-NIC) method exploits an internal integration of a data fidelity estimator within the feature extractor to adjust the features at test time. We estimate the fidelity map with a deep network. We then exploit the fidelity map to guide the attention of the pretrained feature extractor by fusing its weights into the different network layers (see Fig. 1). Experimental evaluations show that our method improves the baseline by large margins and even rivals fully retrained classifiers. Furthermore, our proof-of-concept method with an oracle fidelity map outperforms even all the fully retrained classification networks.
II. RELATED WORK Deep networks outperform human classification on clean images [19]. However, their performance deteriorates on noisy images on which humans do better [19]. A recent study [16] demonstrates the negative effect of degraded images, including hazy images, underwater images, and motion-blurred images, on classification accuracy. Their results parallel those of an earlier work [20]. Both studies emphasize the need to develop robust NIC methods. Validating these observations, two recent approaches, DeepCorrect [17] and WaveCNets [18], address the problem of NIC under additive white Gaussian noise (AWGN). The authors of DeepCorrect [17] analyze how

SUBMISSION TO IEEE SIGNAL PROCESSING LETTERS

2

feature extractor activations are affected by noise. To do so, DeepCorrect estimates how susceptible each filter is to the AWGN degradation and learns to replace these filters by retraining them. WaveCNets proposes a wavelet-decomposition network architecture [18]. As noise mostly affects the highfrequency components of an image, WaveCNets trains a more noise-robust network that focuses on low frequencies. Both these methods need retraining and thus do not benefit from pretrained classifiers and feature extractors.
The most recent NIC method proposes an ensemble network consisting of a restoration network followed by two classification ones for clean and for degraded images [21]. Though requiring retraining, this method does exploit one pretrained network for some clean images. To be modular and be able to deploy novel pretrained classifiers despite noise, our solution exploits only pretrained classifiers. This way, we avoid the need for computationally expensive retraining, and can directly make use of future, potentially improved, classifiers.
Another way to approach NIC is to apply a restoration preprocessing step. However, when preprocessing with a restoration network, for instance the DnCNN [22] denoiser, the performance of the classification network is still severely affected, which we show in our experimental section. There is an observable trade-off between the perceptual quality of the restoration output and the classification performance that was recently highlighted in [23]. For that reason, we rely on a fidelity map that we can either learn explicitly or, as we show in our supplementary material, learn internally in an end-to-end approach [24]. We use this data fidelity to guide the attention of the network [25], [26] and account for potential distortions coming from the degradation or from the restoration steps. Learning an internal indicator for various degradation levels was recently shown to improve the performance and robustness of deep denoisers [27]. In our case, we use fidelity as an attention guide for adjusting the activations of the feature extractor in a classifier.
III. METHOD We present our proposed FG-NIC method by first introducing our fidelity map that we fuse into the pretrained classifier. We discuss the high-level manipulation of the pretrained classifier and present the details of our fidelity fusion. The overall pipeline is shown in Fig. 1. An extended figure with the specific fidelity fusion details is in the supplementary material.
A. Fidelity map A fundamental element of our approach is the fidelity map
that we use to adjust the feature extractor's output and to guide the attention of the network. Given an image restored by a pretrained network, some image components are more faithful to the original clean image than others. This can depend on the noise (possibly spatially varying), the frequency components or the texture across the image, and on the pretrained restoration network itself. We define the fidelity map in terms of the element-wise 1 distance between the original clean image X and the restored image X = R(X^ ), where R(·) is the restoration network and X^ is the noisy image

corresponding to X. In the supplementary material, we further test fidelity maps based on 2 and cosine distances. While the 2 distance performs comparably to the 1 distance for lower noise levels, the 1 distance outperforms it at higher noise levels, due to its robustness to outliers. We hence rely on a fidelity map based on the 1 distance. This fidelity map at pixel (i, j) is

F(i,j)

=

1

-

1 C

C
|X(i,j,c) - X(i,j,c)|

(1)

c=1

where X(i,j,c) is the pixel intensity of the restored image at pixel (i, j) and channel c, and similarly X(i,j,c) is the pixel intensity of the clean image X, and C is the number of channels (3 for RGB). All images and the fidelity map are defined over [0, 1]. From our definition for the fidelity map, it indicates our confidence level in each restored pixel.
The ground-truth oracle fidelity map is naturally not available at test time. Therefore, we train a deep network to act as the fidelity map estimator for the deep restoration network that we use. In our experiments, we implement the fidelity estimator using a state-of-the-art residual architecture [22].

B. Manipulated classifier For a given CNN-based image classification network f (·),
we refer to its feature extractor part that outputs the highlevel features for an image as (·). And we refer to its feature classifier, which mainly consists of fully connected layers for predicting the class likelihoods, as (·). We formulate our method as a manipulation over the pretrained feature extractor (·). As we discuss in detail in the following section, we exploit our fidelity map to guide the attention of the pretrained feature extractor by fusing its information into the different network layers. This enables us to adapt the importance that the network assigns to different features based on the varying quality of the degraded-then-restored image. The manipulated classification network can then be formulated generally as y = ( (X)) where  (X) is the output of the manipulated feature extractor, relative to the original extractor (X), and (·) are the fully connected layers that output the class likelihoods y. Our approach enables us to rely on (·) and (·) taken directly from a classification network that is pretrained on clean images. Hence, without any retraining, we can integrate a novel classification network in a straight-forward manner by obtaining  (·) as described in the following section.

C. Fidelity fusion To obtain the manipulated feature extractor,  (·), relative
to the original pretrained extractor (·), we build our fusion over (·) and describe our Algorithm 1 in what follows.
Spatial linear fusion. The input restored image X is affected by the degradation and restoration process. Even when noise is spatially uniform, different spatial regions in an image are affected to varying degrees based on their frequency content [28]. The objective of our fidelity map is thus to guide the attention of the feature extractor accordingly. To that

SUBMISSION TO IEEE SIGNAL PROCESSING LETTERS

3

Algorithm 1 FG-NIC: Fidelity fusion.1

Input: X # Input restored image

Input: F # Our fidelity map

Input: (·) = LayerN (...(Layer1(·))) # Pretrained extractor

Input: [Blockm 0 (·), ..., Blockm T (·)] # CNN for multiplication

Input: Input:

[Blocka0(·), ..., BlockaT (·)] # CNN for addition Blockf (·) # FC block for channel fusion

Input: Blockc(·) # FC block for channel concatenation

# Spatial linear fusion

1: fm  Blockm 0 (F); fa  Blocka0(F); t  1

2: x  (2 × Sigmoid(fm)  X)  fa 3: for n = 1, ..., N do

4: if Layern(·)  PoolingLayers then

5:

fm  DownSampling(Blockm t (fm) + fm)

6:

fa  DownSampling(Blockat (fa) + fa)

7:

x  (2 × Sigmoid(fm)  x)  fa

8:

tt+1

9: end if

10: x  Layern(x)

11: end for

# Channel fusion

12: fc  DownSampling(F latten(F)) 13: x  2 × Sigmoid(Blockf (fc))  x
# Channel concatenation

14: x  Blockc(Concatenate(fc, x)) Output: Modified feature x =  (X)

end, we apply an element-wise spatial linear fusion on feature layers using our fidelity map weight. This translates into an element-wise multiplication and addition of the features with the fidelity map. This mapping is extended across all the feature channels of the pretrained network. We add a trainable block between two sequential spatial fusion operations to increase the flexibility of our fusion.
Channel fusion. The aforementioned modules are all applied spatially. However, the feature extractor also stores information across channels. For example, the output of the ResNet-50 [29] feature extractor has 2048 channels. For that reason, it is important to also guide the channel attention of the feature extractor. We could, in theory, train a channel-based fidelity map with 2048 channels, however, the complexity and computational cost would significantly increase. We thus flatten and downsample our fidelity map to obtain a new fidelity feature having the same length as the pretrained network's feature space. As with the spatial fusion, the features we obtain by manipulating and reshaping our fidelity map are passed through a trainable block for more flexibility. We fuse the output of this block with the image features through channel-wise multiplication, to guide the attention of the feature extractor across channels.
Channel concatenation. We improve the flexibility of our fusion method by preserving the fidelity information and enabling a trainable block to perform further integration of both the fidelity map and the image features. We concatenate our fidelity map and the image features together and pass the concatenated features (of size 2×2048 for the ResNet-50 architecture) into a trainable block. To enable the use of the

TABLE I TOP-1 ACCURACY (%) OF STATE-OF-THE-ART APPROACHES AND OUR FG-NIC FOR NOISY IMAGES, AVERAGED OVER ALL NOISE LEVELS, ON CALTECH-256. THE BEST AND SECOND BEST RESULTS aside from the oracle configuration ARE IN BOLD AND UNDERLINED RESPECTIVELY.

Methods

AlexNet ResNet-18 ResNet-50

Pretrained (test on noisy) Pretrained (test on restored [22]) Retrain on noisy WaveCNets [18] (Haar) 2 DeepCorrect [17] FG-NIC (pretrained + single) FG-NIC (pretrained + ensemble)

13.08 44.42 52.87
21.90 44.53 54.61

22.11 46.15 65.54 22.94 42.14 57.99 56.99

23.27 52.33 72.95 24.68 45.54 68.69 67.04

FG-NIC (oracle + single) FG-NIC (oracle + ensemble)

57.49 58.51

68.22 68.59

76.10 75.42

pretrained network, the resulting feature set retains the same size as the original one.
Ensembling. We also investigate using an ensemble of {(·),  (·)} rather than only our manipulated features. Our ensemble module assigns element-wise weights to the two feature layers to combine them. This approach enables our method to exploit the best of the two feature spaces, without having to modify the pretrained fully connected layers. As we show in our experimental results, preserving the original features can be beneficial at low degradation levels.
IV. EXPERIMENTS A. Data processing
We use all categories of the Caltech-256 dataset [7], and perform the standard processing and augmentation used in the state-of-the-art methods [16], [30]. For our AWGN degradation, the additive noise is independent across color channels. For different degradation levels, we change the standard deviation () of the Gaussian noise. For further evaluation, we implement spatially varying degradation with 1D and 2D variations. The noise level linearly changes with the number of rows or columns for 1D varying noise, and for 2D varying noise, the noise level linearly changes with respect to the Euclidean distance from a random point on the image. Fig. 2 illustrates some examples with different AWGN degradations on an image along with their corresponding fidelity maps.
B. Baseline methods Aside from the pretrained baseline, we compare against
multiple fully retrained networks. Each of our setups is then tested on images with and without restoration preprocessing. In the Pretrained baseline, the classification network is only trained on clean images, and we do not modify it. We also retrain the classification network on a mixture of noisy and clean images (Retrain on noisy), i.e.,   {0, 0.1, 0.2, 0.3, 0.4, 0.5}. For the Retrain on restored baseline, the training set consists of the outputs of the restoration network. The restored images are obtained by processing the same images as in the previous setup with the DnCNN denoiser [22].

1 is the Hadamard product,  the Hadamard product spatially with broadcasting to all channels, and  the element-wise addition in spatial dimensions with channel broadcasting. FC refers to fully-connected layers.

2Pretrained models: https://github.com/LiQiufu/WaveCNet. Note, we do not report the AlexNet perfomance on the WaveCNets model as the authors do not provide the pretrained model.

SUBMISSION TO IEEE SIGNAL PROCESSING LETTERS

4

Fig. 2. Illustrative example with images degraded by AWGN, from the 'butterfly' category in Caltech-256. For the spatially varying noise,  = (0, 0.5) indicates that the degradation level varies linearly from 0 to 0.5. The bottom row shows our fidelity maps averaged over the RGB channels, corresponding to each image in the top row. We apply a gamma correction to the fidelity maps for better visualization.

TABLE II CLASSIFICATION ACCURACY (%) OF THE BASELINES AND OUR FG-NIC METHODS (BASED ON RESNET-50) ON CALTECH-256. THE BEST AND SECOND BEST RESULTS aside from the oracle configuration ARE IN BOLD
AND UNDERLINED RESPECTIVELY.

Methods

Experimental

Uniform degradation ()

Varying

setup

0.1 0.2 0.3 0.4 0.5 1D 2D

Pretrained

Test on noisy 67.60 32.68 11.25 3.50 1.35 30.25 27.15 Test on restored 77.99 67.06 53.00 38.08 25.53 64.08 62.55

Retrain on Test on noisy 79.17 76.21 73.08 69.90 66.39 74.71 74.71

noisy

Test on restored 78.44 74.14 67.47 59.15 50.15 71.69 71.43

Retrain on Test on noisy 71.33 46.07 20.02 7.43 2.26 42.32 39.06 restored Test on restored 80.32 77.85 75.04 71.90 68.37 76.88 76.70

FG-NIC Single (Pretrained) Ensemble

79.57 74.90 69.37 63.19 56.44 73.29 72.87 80.32 74.84 68.18 60.31 51.56 73.34 72.68

FG-NIC (Oracle)

Single Ensemble

80.45 78.14 76.13 74.06 71.75 77.64 77.52 81.14 78.28 75.52 72.64 69.51 77.77 77.41

Our proposed FG-NIC method, as presented in Sec. III, exploits a fidelity map to manipulate the image features obtained by a pretrained classification network. The pretrained classifier is the same network used in the first Pretrained baseline, along with a pretrained preprocessing denoiser [22]. We use a pretrained fidelity estimator, which we obtain with the same training setup as that of the denoiser [22]. In our FGNIC method we also design a proof-of-concept configuration that uses an oracle fidelity map. This is the ground-truth fidelity map, computed following Eq. 1, that we use instead of an estimated fidelity map. This is to study the performance of our proposed approach independent of the quality of the fidelity map estimator. As an ablation study, we allow the network to fine-tune its own learned fidelity estimation with end-to-end training, and present these extended results in the supplementary material. We also provide in the the supplementary material, the number of operations and parameters used by the different approaches.

C. Experimental results We compare the accuracy of the state-of-the-art methods
DeepCorrect [17] and WaveCNets [18] with our FG-NIC on noisy images. The results in Table I are averaged over all uniform noise levels (0.1 to 0.5). We evaluate the models with AlexNet, ResNet-18, and Resnet-50. We report the

performance of pretrained models, pretrained models with restoration preprocessing, retrained models, WaveCNets, and DeepCorrect. We observe that our FG-NIC (pretrained), outperforms all other models, and comes close to the one retrained on noisy data. Our FG-NIC (oracle), which is the proof-ofconcept method, consistently reports the best performance.
In Table II, we show classification accuracy on various levels of image noise, both uniform and spatially varying. Our methods significantly outperform the pretrained baseline, whether the latter is tested on noisy or restored images, consistently over all noise levels. Comparing with the fully retrained networks, we note that the accuracy of our method comes close, even though the gap increases at high noise levels. Again, our proof-of-concept method with the groundtruth oracle fidelity map outperforms even the fully retrained networks across all noise levels. We also observe that although our ensemble module can slightly boost the classification performance at low noise levels, it quickly becomes detrimental to the results at higher levels. This indicates that our fidelity-based manipulated features  (X) are better than the original ones when noise is present. Interestingly, the retrained baselines also tend to overfit their training distribution and are consequently not robust. For instance, the network retrained on noisy images performs significantly worse when given restored images, and the network retrained on restored images performs significantly worse when given noisy images.
V. CONCLUSION We propose the first method that manipulates the layers of a feature extractor to enable the classification of noisy images by a pretrained classifier. By internally fusing data fidelity information, our FG-NIC model guides the attention of the classifier to improve its robustness to noise. We thus avoid retraining a classification network and still significantly improve over the baseline. With our oracle proof-of-concept approach we even consistently outperform, over all noise levels, the fully retrained classifier that is retrained specifically on the outputs of the restoration network. Our method can be readily extended to different degradation types in future work. It is also modular and can be used with future deep classifiers or restoration networks without the retraining overhead.

SUBMISSION TO IEEE SIGNAL PROCESSING LETTERS

5

Acknowledgement: This work was supported in part by the Swiss National Science Foundation via the Sinergia grant number CRSII5-180359.
REFERENCES
[1] O. Boiman, E. Shechtman, and M. Irani, "In defense of nearest-neighbor based image classification," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2008, pp. 1­8.
[2] J. Yang, K. Yu, Y. Gong, and T. Huang, "Linear spatial pyramid matching using sparse coding for image classification," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009, pp. 1794­ 1801.
[3] F. Perronnin, J. Sa´nchez, and T. Mensink, "Improving the fisher kernel for large-scale image classification," in European Conference on Computer Vision (ECCV), 2010, pp. 143­156.
[4] T. Kobayashi, "Bfo meets hog: feature extraction based on histograms of oriented pdf gradients for image classification," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2013, pp. 747­754.
[5] D. G. Lowe, "Distinctive image features from scale-invariant keypoints," International journal of computer vision, vol. 60, no. 2, pp. 91­110, 2004.
[6] N. Dalal and B. Triggs, "Histograms of oriented gradients for human detection," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2005, pp. 886­893.
[7] G. Griffin, A. Holub, and P. Perona, "Caltech-256 object category dataset," 2007.
[8] A. Krizhevsky, I. Sutskever, and G. Hinton, "Imagenet classification with deep convolutional neural networks," in Conference on Neural Information Processing Systems (NeurIPS), 2012, pp. 1097­1105.
[9] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, "Imagenet: A large-scale hierarchical image database," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009, pp. 248­255.
[10] T. Schlegl, J. Ofner, and G. Langs, "Unsupervised pre-training across image domains improves lung tissue classification," in International MICCAI Workshop on Medical Computer Vision, 2014, pp. 82­93.
[11] M. El Helou, F. Du¨mbgen, and S. Su¨sstrunk, "AL2: Progressive activation loss for learning general representations in classification neural networks," in IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP), 2020, pp. 4007­4011.
[12] X. Li, S. Chen, X. Hu, and J. Yang, "Understanding the disharmony between dropout and batch normalization by variance shift," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 2682­2690.
[13] M. Blot, T. Robert, N. Thome, and M. Cord, "Shade: Information-based regularization for deep learning," in IEEE International Conference on Image Processing (ICIP), 2018, pp. 813­817.
[14] A. S. Morcos, D. G. Barrett, N. C. Rabinowitz, and M. Botvinick, "On the importance of single directions for generalization," in International Conference on Learning Representations (ICLR), 2018.
[15] C. Tian, L. Fei, W. Zheng, Y. Xu, W. Zuo, and C.-W. Lin, "Deep learning on image denoising: An overview," Neural Networks, vol. 131, pp. 251­ 275, 2020.
[16] Y. Pei, Y. Huang, Q. Zou, X. Zhang, and S. Wang, "Effects of image degradation and degradation removal to cnn-based image classification," IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 4, pp. 1239­1253, 2021.
[17] T. S. Borkar and L. J. Karam, "Deepcorrect: Correcting dnn models against image distortions," IEEE Transactions on Image Processing, vol. 28, no. 12, pp. 6022­6034, 2019.
[18] Q. Li, L. Shen, S. Guo, and Z. Lai, "Wavelet integrated CNNs for noiserobust image classification," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 7245­7254.
[19] T. Tadros, N. C. Cullen, M. R. Greene, and E. A. Cooper, "Assessing neural network scene classification from degraded images," ACM Transactions on Applied Perception, vol. 16, no. 4, pp. 1­20, 2019.
[20] P. Roy, S. Ghosh, S. Bhattacharya, and U. Pal, "Effects of degradations on deep neural network architectures," arXiv preprint arXiv:1807.10108, 2018.
[21] K. Endo, M. Tanaka, and M. Okutomi, "Classifying degraded images over various levels of degradation," in IEEE International Conference on Image Processing (ICIP), 2020, pp. 1691­1695.
[22] K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. Zhang, "Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising," IEEE Transactions on Image Processing, vol. 26, no. 7, pp. 3142­3155, 2017.

[23] D. Liu, H. Zhang, and Z. Xiong, "On the classification-distortionperception tradeoff," in Advances in Neural Information Processing Systems (NeurIPS), vol. 32, 2019.
[24] M. El Helou and S. Su¨sstrunk, "BIGPrior: Towards decoupling learned prior hallucination and data fidelity in image restoration," arXiv preprint arXiv:2011.01406, 2020.
[25] J. Fu, J. Liu, J. Jiang, Y. Li, Y. Bao, and H. Lu, "Scene segmentation with dual relation-aware attention network," IEEE Transactions on Neural Networks and Learning Systems, pp. 1­14, 2020.
[26] X. Ma, X. Lin, M. El Helou, and S. Su¨sstrunk, "Deep Gaussian denoiser epistemic uncertainty and decoupled dual-attention fusion," in IEEE International Conference on Image Processing (ICIP), 2021.
[27] M. El Helou and S. Su¨sstrunk, "Blind universal Bayesian image denoising with Gaussian noise level learning," IEEE Transactions on Image Processing, vol. 29, pp. 4885­4897, 2020.
[28] M. El Helou, R. Zhou, and S. Su¨sstrunk, "Stochastic frequency masking to improve super-resolution and denoising networks," in European Conference on Computer Vision (ECCV), 2020, pp. 749­766.
[29] K. He, X. Zhang, S. Ren, and J. Sun, "Deep residual learning for image recognition," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 770­778.
[30] T. He, Z. Zhang, H. Zhang, Z. Zhang, J. Xie, and M. Li, "Bag of tricks for image classification with convolutional neural networks," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 558­567.

SUPPLEMENTARY MATERIAL

1

Fidelity Estimation Improves Noisy-Image Classification with Pretrained Networks
Xiaoyu Lin, Deblina Bhattacharjee, Majed El Helou, Members, IEEE, and Sabine Su¨sstrunk, Fellow, IEEE.

arXiv:2106.00673v1 [cs.CV] 1 Jun 2021

work or module, highlighting the computational efficiency of our proposed approach.

TABLE IV #MACS IS THE NUMBER OF MULTIPLY­ACCUMULATE OPERATIONS (IN BILLION), AND #PARAMS IS THE NUMBER OF TRAINABLE PARAMETERS
(IN MILLION).

Network model ResNet-50 (Classification) Ensemble module Our FG-NIC

#MACs (G) 4.11 4.11 0.08

#Params (M) 24.03 0.21 10.49

Fig. 3. Detailed module architecture (without ensemble) of our proposed method based on ResNet-50 [1]. FC, Conv and ReLU indicate one fully connected layer, one convolutional layer and a rectified linear unit activation, respectively. The pretrained module (feature extractor) is shown in green, and the modules containing trainable parameters are shown in blue.

TABLE III CLASSIFICATION ACCURACY (%) OF THE BASELINE AND OUR FG-NIC METHODS (BASED ON RESNET-50) ON CALTECH-101. THE BEST AND SECOND BEST RESULTS aside from the oracle configuration ARE IN BOLD
AND UNDERLINED RESPECTIVELY.

Methods

Experimental

Uniform degradation ()

Varying

setup

0.1 0.2 0.3 0.4 0.5 1D 2D

Pretrained

Test on noisy 83.87 56.95 27.13 10.95 5.50 54.23 50.18 Test on restored 92.03 84.83 70.16 50.17 31.18 81.38 80.17

Retrain on Test on noisy 89.62 87.92 86.23 84.50 82.04 87.59 87.38

noisy

Test on restored 89.85 87.90 82.16 73.98 62.19 85.86 85.68

Retrain on Test on noisy 84.18 64.52 36.82 15.53 5.85 59.62 56.74 restored Test on restored 91.89 89.86 87.84 85.39 82.64 89.49 89.11

FG-NIC Single (Pretrained) Ensemble

89.85 87.76 84.68 80.49 75.39 86.42 86.64 91.79 89.08 84.72 78.81 70.80 87.71 87.60

FG-NIC (Oracle)

Single Ensemble

90.91 89.46 87.75 86.07 83.83 89.02 88.89 91.97 90.29 88.02 85.82 82.87 89.61 89.53

I. EXTENDED EXPERIMENTAL RESULTS The Caltech-101 [2] dataset is similar to Caltech-256 with fewer images and categories. We select 30 images per class as the training set and keep the same procedure as for Caltech256. The results are consistent and given in Table III.
II. COMPUTATIONAL COMPLEXITY We show in Table IV the number of Multiply­accumulate operation (MAC) and trainable parameters used in each net-

III. ABLATION STUDY We run a series of experiments on Caltech-256 and conduct an in-depth analysis of the results to show the improvement of each module in our proposed method. For the end-toend setup, we train our FG-NIC and fidelity map estimator together. For the different fidelity map inputs and outputs, downsampling methods, and generally the ablation studies, we use the oracle setup with the single model. The results are given in Table V and support the statements and design decisions we make in our main manuscript.

TABLE V IN-DEPTH ANALYSIS AND ABLATION STUDY RESULTS (CLASSIFICATION ACCURACY IN %). BOLD NUMBERS SHOW THE BEST ACCURACY AND
UNDERLINED NUMBERS SHOW THE NEXT BEST ACCURACY.

Methods

Uniform degradation () 0.1 0.2 0.3 0.4 0.5

Our FG-NIC: oracle + single

80.45 78.14 76.13 74.06 71.75

Our FG-NIC: end-to-end + single 80.57 75.21 68.22 60.01 51.03

Pretrained on clean test on restored 77.99 67.06 53.00 38.08 25.53

Fidelity map input: restored

80.41 78.15 76.13 74.12 71.77

Fidelity map output: 2 distance Fidelity map output: cosine

80.46 78.13 76.08 73.90 71.55 79.45 75.07 69.27 62.56 55.18

Downsampling: bicubic Downsampling: nearest neighbor

80.69 78.25 76.09 73.81 71.43 80.32 78.01 75.81 73.65 71.37

w/o spatial multiplication 80.17 77.40 74.89 72.34 69.49

Ablation w/o spatial addition

80.09 76.93 73.82 70.47 67.08

study w/o channel multiplication 80.32 77.72 75.70 73.51 70.95

w/o channel concatenation 80.55 77.55 74.68 71.77 68.63

REFERENCES
[1] K. He, X. Zhang, S. Ren, and J. Sun, "Deep residual learning for image recognition," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 770­778.
[2] L. Fei-Fei, R. Fergus, and P. Perona, "One-shot learning of object categories," IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 28, no. 4, pp. 594­611, 2006.

