Astronomy & Astrophysics manuscript no. UPSILoN-T June 2, 2021

©ESO 2021

arXiv:2106.00187v1 [astro-ph.IM] 1 Jun 2021

Deep Transfer Learning for Classification of Variable Sources
Dae-Won Kim1,, Doyeob Yeo1,, Coryn A.L. Bailer-Jones2, and Giyoung Lee1
1 Electronics and Telecommunications Research Institute (ETRI), 218 Gajeong-ro, Daejeon, Yuseong-gu, South Korea 2 Max-Planck Institute for Astronomy (MPIA), Königstuhl 17, D-69117 Heidelberg, Germany
June 2, 2021
ABSTRACT
Ongoing or upcoming surveys such as Gaia, ZTF, or LSST will observe light-curves of billons or more astronomical sources. This presents new challenges for identifying interesting and important types of variability. Collecting a sufficient number of labelled data for training is difficult, however, especially in the early stages of a new survey. Here we develop a single-band light-curve classifier based on deep neural networks, and use transfer learning to address the training data paucity problem by conveying knowledge from one dataset to another. First we train a neural network on 16 variability features extracted from the light-curves of OGLE and EROS-2 variables. We then optimize this model using a small set (e.g. 5%) of periodic variable light-curves from the ASAS dataset in order to transfer knowledge inferred from OGLE/EROS-2 to a new ASAS classifier. With this we achieve good classification results on ASAS, thereby showing that knowledge can be successfully transferred between datasets. We demonstrate similar transfer learning using Hipparcos and ASAS-SN data. We therefore find that it is not necessary to train a neural network from scratch for every new survey, but rather that transfer learning can be used even when only a small set of labelled data is available in the new survey.
Key words. methods: data analysis - stars: variables: general - surveys - techniques: miscellaneous

1. Introduction
In recent years, deep learning has achieved outstanding success in various research areas and application domains. For instance, Convolutional Neural Networks (CNNs) (Lecun et al. 2015) use convolutional layers to regularize and build space-invariant neural networks to classify visual data (Krizhevsky et al. 2012; Goodfellow et al. 2014; Szegedy et al. 2015). AlphaGo, which is one of the most astonishing achievements of deep learning and is based on reinforcement learning, defeated several world-class Go players (Silver et al. 2016). In astronomy deep learning has become popular and has been used in many studies such as star-galaxy classification (Kim & Brunner 2017), asteroseismology classification of red giants (Hon et al. 2017), photometric redshift estimation (D'Isanto & Polsterer 2018), galaxygalaxy strong lens detection (Lanusse et al. 2018), exoplanet finding (Pearson et al. 2018), point source detection (Vafaei Sadr et al. 2019), and fast-moving object identification (Duev et al. 2019), to name just a few examples.
Training a deep learning classification model requires a huge amount of labeled data which are not always readily available. This can be addressed using "transfer learning", a method that preserves prior knowledge inferred from one problem having sufficient samples (the "source") and apply it to another but related problem (the "target") (Hinton et al. 2015; Yim et al. 2017; Yeo et al. 2018). Transfer learning starts with a machine learning model that has been trained on a lot of labeled samples from the source.
 dwk@etri.re.kr  yeody@etri.re.kr

It then optimizes parameters in the model using a small number of labeled samples from the target. The source and target samples can be different but need to be related such that extracted features to train the model are general and relevant in both samples. Period, amplitude, and variability indices could be such features when the problem is one of classifying light-curves of variable stars.
Transfer learning has been used to solve a variety of problems such as medical image classification (Shin et al. 2016; Maqsood et al. 2019; Byra et al. 2020), recommender-system applications (Pan et al. 2012; Hu et al. 2019), bioinformatics applications (Xu et al. 2010; Petegrosso et al. 2016), transportation applications (Di et al. 2018; Wang et al. 2018; Lu et al. 2019), and energy saving applications (Li et al. 2014; Zhao & Grace 2014). Some studies in astronomy also used transfer learning such as Ackermann et al. (2018); Domínguez Sánchez et al. (2019); Lieu et al. (2019); Tang et al. (2019). These studies used transfer learning to analyze image datasets of either galaxies or solar system objects, and confirmed that transfer learning is successful even when using a small set of data.
The advantages of transfer learning are as follows:
­ Transfer learning uses accumulated knowledge from the source. Therefore it works with a small set of samples from the target.
­ Transfer learning uses a pre-trained model, obviating the need to design a deep neural network from scratch for every survey, which is a challenging and timeconsuming task. A pre-trained model furthermore contains better initialization parameters than randomly initialized parameters. Thus it is likely that transferring a

Article number, page 1 of 18

A&A proofs: manuscript no. UPSILoN-T

pre-trained model shows faster convergence than training from scratch. ­ It is possible to either add or remove certain output classes from the pre-trained model during transferlearning processes. This is particularly useful because the classes in the target dataset are mostly different from the classes of the source dataset.
Our previous work, UPSILoN: AUtomated Classification for Periodic Variable Stars using MachIne LearNing (Kim & Bailer-Jones 2016), we provided a software package that automatically classifies light-curve into one of seven periodic variable classes. Other groups have used UPSILoN to classify light-curves of periodic variable stars (e.g. Jayasinghe et al. 2018; Kains et al. 2019; Hosenie et al. 2019).
In this paper, we introduce UPSILoN-T: UPSILoN using Transfer Learning. UPSILoN-T is substantially different from UPSILoN in several respects:
­ UPSILoN-T can be transferred to other surveys whereas the UPSILoN's Random Forest model cannot be.
­ UPSILoN-T is not only able to classify periodic or semiperiodic variables but also non-periodic variables such as QSOs or blue variables.
­ UPSILoN-T uses MCC (Matthews Correlation Coefficient; Matthews 1975) rather than F1 as a performance metric. MCC is known to give a relatively robust performance measure for imbalanced samples (Boughorbel S 2017). We use the generalized MCC (Chicco 2017) for multi-class classification.
­ UPSILoN-T uses deep learning whereas UPSLIoN used a Random Forest. The UPSILoN-T model has three hidden layers with 64, 128, and 256 neurons, respectively. This is a much smaller model (200 KB) than UPSILoN's Random Forest model (3 GB).
As far as we know, no previous work has applied transfer learning based on deep learning to time-variability features extracted from light-curves. In addition to developing the method, we provide a python library so that readers can easily adapt our method to their datasets (see Appendix). The library classifies a single-band light-curve into one of nine variable classes. If multiple optical bands exist, the library can be applied to each of them independently. Note that one of the goals of our method, as with its predecessor UPSILoN, is not to be dependent on colours and so to be independent of the availability or number of bands in the survey.
We give an introduction of a neural network in Section 2. In section 3.1 we introduce the training set, and in section 3.2 we explain 16 time-variability features that we use to train a classification model. From section 3.3 to 3.5, we explain training processes and feature importance of the trained model. Section 4 demonstrates the application of transfer learning to the other light-curve datasets: ASAS, Hipparcos, and ASAS-SN. Section 5 gives the classification performance of the transferred model as a function of lightcurve lengths and sampling. Section 6 gives a summary.
2. Artificial Neural Network
An artificial neural network (ANN) consists of multiple layers comprising an input layer, hidden layer(s), and an out-

!!!

!!!

!!!

"#$%&' #()*+

-.))*#' ,%&$%&' #()*+ #()*+

Fig. 1. DNN architecture with one hidden layer.

put layer. ANN with at least two hidden layers are usually called a deep neural network (DNN). Each layer contains one or more nodes (neurons) that are connected to the nodes in the next layer. Each connection has a particular weight that can be interpreted as how much impact that node has on the connected node in the next layer. Data are propagated through the network and the value on each node is computed using an activation function (Agatonovic-Kustrin & Beresford 2000). The Heaviside function, hyperbolic tangent function, and rectified linear unit (ReLU) (Nair & Hinton 2010) are typical examples of nonlinear activation functions that are often used between intermediate layers. The softmax function is widely used in the output layer when solving a classification problem (Nair & Hinton 2010). Due to the nonlinearity, the function space that can be expressed using ANN is diverse.
An example of an ANN architecture is shown in Fig. 1. We define input nodes, hidden nodes and output nodes as x, h and y, respectively, that are vectors. Let the weights and biases between x and h, and weights between h and y be W1, b1 and W2, b2, respectively. Then, W1, W2 are matrices and b1, b2 are vectors. We can then express x, h, and y as follows:

h = 1 xT W1 + b1 y = 2 hT W2 + b2

(1)

where 1(·) and 2(·) denote the activation function between x and h, and between h and y, respectively. In classification problems, 1 and 2 are typically the ReLU function and the softmax function, respectively. The values of the output nodes give the probabilities that input (i.e. x) belongs to each of the output classes. The goal of a classification problem is learning the weights so that the true classes receive the highest probabilities. Given an input X = {x1, x2, · · · , xm}, this learning process is done by

Article number, page 2 of 18

D.-W. Kim et al.: Deep Transfer Learning for Classification of Variable Sources

minimizing a loss function such as cross entropy defined as 3. UPSILoN-T Classifier

follows:

3.1. Training Set

L(X; W )



-

1 m

m

log P (li|yi)

i=1

=

-1 m

m

li · log(yi)

(2)

i=1

where li the true label for each xi and a one-hot encoded vector, which is a vector representation of categorical data such as class labels. That is, lk = [lk,1, lk,2, · · · , lk,c] and lk,j  {0, 1} for k = {1, 2, · · · , m} and j = {1, 2, · · · , c}. c is the number of classes. W = {W1, b1, W2, b2} and · denotes an inner product. In the cases of imbalanced dataset
classification, a weighted cross entropy is often used, which is defined as follows:

L(X; W )



-

1 m

m

 · (li  log(yi))

i=1

1 li,1 log(yi,1)

=

-1 m

m i=1

2

  

...

  

·

li,2 



log(yi,2)

...

  

(3)

c

li,c log(yi,c)

where  = [1, 2, · · · , c] and  denotes an element-wise

product. i is a weight for ative real number, where

a given

c i=1

i

class = 1.

which

is

a

nonneg-

Inferring the weights (i.e. learning) is done by minimiz-

ing a loss function such as that in Equation (2) or (3) using

a gradient descent method:

Wnew  Wold - W L(X ; Wold)

(4)

where  is a learning rate for the gradient descent optimization which is a positive constant. The minimization is often done by using a stochastic gradient descent optimization (SGD) (Hinton et al. 2012; Graves et al. 2013) or a modified stochastic gradient descent optimization (Kingma & Ba 2014). In the case of SGD, the input data are split into several batches (i.e. mini-batch) and then SGD is applied to each mini-batch as the pseudo-code shown in Algorithm 1.

Algorithm 1 ANN learning procedure
Input: Initialized weights of ANN to be learned: W = {W1, b1, W2, b2} Output: Learned weights of ANN: W

1: while W does not converge do 2: Choose a mini-batch XB of size n
XB = {xB,1, · · · , xB,n}  X 3: Wnew  Wold - W LCE(XB; Wold)
 Update W using SGD
4: end while

The training set consists of single-band light-curves of variable sources from two independent surveys, OGLE (Udalski et al. 1997) and EROS-2 (Tisserand et al. 2007). We mix variable sources from the two surveys in order to build a rich training set. The training set comprises non-variables and seven classes of periodic variables:  Scuti stars, RR Lyraes, Cepheids, Type II Cepheids, eclipsing binaries, and long period variables. We also use subclasses (e.g. Cepheid F, 1O, and Other) of each class if these exist. These seven classes of variable sources are from Kim & Bailer-Jones (2016) who originally compiled and cleaned the list of periodic variable stars from several sources including Soszynski et al. (2008); Soszyski et al. (2008, 2009); Soszyñski et al. (2009); Poleski et al. (2010); Graczyk et al. (2011); Kim et al. (2014). As described in these papers, light-curves are visually examined and cleaned in order to remove light-curves that do not show variability. In the present work we also add to the training set QSOs and blue variables from Kim et al. (2014). These two types of variables generally show non-periodic and irregular variability. Note that the training set is not exhaustive since it does not contain all types of variable sources in the real sky. Thus a model trained on the training set will classify a light-curve that does not belong to the training classes into a most similar training class based on variability.
The observation duration is about eight years for the OGLE light-curves, and about seven years for the EROS-2 light-curves. We use OGLE I-band light-curves and EROS2 blue-band, BE, light-curves because these generally have more data points and better photometric accuracy than OGLE V-band and EROS-2 red-band, RE, respectively. The average number of data points in the light-curves is about 500. Table 1 shows the number of objects per class in the training set. There is a total of 21 variable classes.
3.2. Variability Features
We extract 16 variability features from each light-curve in the training set. These features, listed in Table 2, are taken from von Neumann (1941); Shapiro & Wilk (1965); Lomb (1976); Ellaway (1978); Grison (1994); Stetson (1996); Long et al. (2012); Kim et al. (2014); Kim & Bailer-Jones (2016). These features have proven to be useful for classifying both periodic and non-periodic variables (e.g. QSOs) in our previous works (Kim et al. 2011, 2012, 2014; Kim & Bailer-Jones 2016). They are generic and can be extracted from any single-band light-curves. Four of the features (, CS, mp90, and mp10) are based on a phasefolded light-curve. We excluded survey-dependent features, such as colours, because we want to transfer accumulated knowledge from one survey to another.
We take logarithm base 10 of nine of the features (period, 1, 2, , CS, Q3-1, A, H1, and mp90) because we found empirically that a model trained using these log10 applied features improved the performance over using the features directly. We scale each of the nine feature set as follows:

1. Derive the minimum value of a feature set.

Article number, page 3 of 18

A&A proofs: manuscript no. UPSILoN-T

Table 1. The number of sources per true class in the OGLE and EROS-2 training set

Superclass (Acronym) QSO
 Scuti (DSCT) Type II Cepheid (T2CEPH)
Blue Variable (BV) Non-variable (NonVar)
RR Lyrae (RRL)
Cepheid (CEPH)
Eclipsing Binary (EB)
Long Period Variable (LPV)
Total

Subclass
ab c d e
F 1O Other
EC ED ESD
Mira AGB C Mira AGB O OSARG AGB OSARG RGB SRV AGB C SRV AGB O

Number 165
3209 300 735
8050
19 921 4974 1077 1327
2981 2043
443
1398 19 075
7339
1361 783
25 284 29 516
6062 8780 144 823

Note
fundamental first overtone
contact detached semi-detached carbon-rich oxygen-rich carbon-rich oxygen-rich

Table 2. 16 Variability features

Feature Period
1 2  CS Q3-1 A H1 mp90 mp10 R21 R31 21 31 K W

Description
Period derived by the Lomb-Scargle algorithm Skewness Kurtosis  (von Neumann 1941) of a phase-folded light-curve Cumulative sum (Ellaway 1978) index of a phase-folded light-curve 3rd quartile (75%) - 1st quartile (25%) Ratio of magnitudes brighter or fainter than the average magnitude Amplitude derived using the Fourier decomposition 90% percentile of slopes of a phase-folded light-curve 10% percentile of slopes of a phase-folded light-curve 2nd to 1st amplitude ratio derived using the Fourier decomposition 3rd to 1st amplitude ratio derived using the Fourier decomposition Difference between 2nd and 1st phase derived using the Fourier decomposition Difference between 3rd and 1st phase derived using the Fourier decomposition Stetson K index Shapiro­Wilk normality test statistics

Reference Lomb (1976)
Kim et al. (2014) Kim et al. (2014) Kim et al. (2014) Kim & Bailer-Jones (2016)
Grison (1994) Long et al. (2012) Long et al. (2012)
Grison (1994) Grison (1994) Grison (1994) Grison (1994) Stetson (1996) Shapiro & Wilk (1965)

2. Subtract the minimum value minus one from each feature in the feature set.
3. Take logarithm base 10 of the features.
Performance did not improve when taking the logarithm of the other features, so they were used directly. Finally, we normalize each 16 feature set by calculating the standard score.

3.3. Model Training
3.3.1. Classification Performance Metric
In order to measure the classification performance of a trained model and then select the one that gives the best performance, we use the MCC (Matthews Correlation Coefficient; Matthews 1975) metric rather than the mode tra-

Article number, page 4 of 18

D.-W. Kim et al.: Deep Transfer Learning for Classification of Variable Sources

ditional F1 or accuracy. For two-class problems, the MCC, accuracy, and F1 are defined as follows:

MCC = 

TP × TN - FP × FN

(5)

(TP + FP )(TP + FN )(TN + FP )(TN + FN )

Accuracy

=

TP

+

TP TN

+ +

TN FP

+ FN

(6)

F1

=

2

×

precision precision

× +

recall recall

(7)

where T P, T N, F P, and T N is the number of true positives, true negatives, false positives, and false negatives, respectively. Precision and recall in the equation of F1 are defined as follows:

precision

=

TP TP +FP

,

recall

=

TP TP +FN

(8)

The MCC is a more informative measure for imbalanced datasets than the other two measures because the MCC utilizes all four categories of a confusion matrix (Powers 2011; Boughorbel S 2017). F1 does not take account of true negative and thus is less informative for imbalanced datasets. Accuracy is inappropriate for imbalanced datasets because high accuracy does not necessarily indicate high classification quality, due to the "Accuracy Paradox" (Fernandes et al. 2010; Valverde-Albacete & Peláez-Moreno 2014). The MCC ranges from -1.0 to 1.0, where 1.0 indicates that the predictions match exactly with the true labels, 0.0 indicates that the predictions are random, and -1.0 means that the predictions are completely opposite to the true labels. It is claimed that the MCC is the most informative value to measure classification quality using a confusion matrix (Chicco 2017). In the case of multi-class problems, the MCC is generalized as follows (Gorodkin 2004):

MCC =

CkkClm - CklCmk

k lm





-

1 2

×

Ckl 

Ckl 

(9)

k

l

k|k=k l





-

1 2

×
k

Clk 

Clk 

l

k|k=k l

For convenience, we map MCC to the range [0, 1] as suggested in Chicco & Jurman 2020:

Mc

=

1

+

MCC 2

(10)

Hereinafter, we use Mc to report classification quality of neural networks.

3.3.2. Training DNN Models
In order to find the best architecture for classifying the OGLE and EROS-2 training set, we trained many DNN models with different combinations of the number of hidden layers, the number of neurons in each hidden layer, loss functions, and activation functions between layers. The model training processes are as follows:
1. Randomly split the dataset shown in Table 1 into 80% training and 20% validation set while preserving class ratio (i.e. stratified sampling).
2. Train a DNN model from scratch using the training set with the batch size of 1024 and learning rate = 0.1. This counts as one training epoch. Mc is then calculated using the validation set.
3. Run the training process in step 2 for 500 iterations (i.e. 500 training epochs). If Mc is not improved for three consecutive epochs, decrease the learning rate by 10. If the learning rate becomes lower than 10-10, increase it back to 0.1, according to a technique called the cyclical learning rate (CLR; Smith 2015).
4. Get the highest Mc value (i.e. the best Mc value) during the 500 epochs.
5. Repeat steps 1 to 4 30 times and calculate the mean of the best Mc values and the standard error of this mean (SEM), which is defined as:

SEM

=

 n

(11)

where  is standard deviation of the n Mc values from the n=30 cycles.

The loss function that we chose is cross entropy explained in Section 2. While training DNN models, we use the SGD (stochastic gradient descent; Hinton et al. 2012, Graves et al. 2013) optimizer because we empirically found SGD gives better classification performance than other optimizers (e.g. ADAM: Kingma & Ba 2014; ADADELTA: Zeiler 2012; and ADAGRAD: Duchi et al. 2011). We also use a batch normalization method that converts the inputs to layers into a normal distribution in order to mitigate "internal covariate shift" (Ioffe & Szegedy 2015). Ioffe & Szegedy (2015) showed that the method makes training faster and more stable.
After training many DNN models, we finally chose the architecture that have the highest validation-set Mc, which was 0.9043. This is shown in Fig. 2. Note that we use Mc to choose the best model but not as a loss function to optimize neural networks. We will refer to this best model as the "U model", an abbreviation of "UPSILoN-T model". The number of trainable parameters in the U model is 48,799. Given that the number of training samples is 144,823 and there are 16 features per sample, we presume that there exist enough features to train the parameters. Furthermore, the parameters in a network are not, and need not be, independent. Therefore it is not theoretically necessary to have more training features than the number of trainable parameters.
Some examples of the DNN architectures that we trained are shown in Table 3 where the U model is shown in bold text. As the table shows, increasing the number of layers and the number of neurons in each hidden layer

Article number, page 5 of 18

A&A proofs: manuscript no. UPSILoN-T

8#96)
:;<'=%&"%>"3")5',$%)6&$?@
!"#$%&'(#")':<A@
B$!(
!"#$%&'(#")':;CD@

*+ !"#$%&'(#")
,-
*+.'*%)/0'+1&2%3"4%)"1#'!%5$& ,-.',6335'-1##$/)$7'!%5$&

B$!(

!"#$%&'(#")':CE<@

B$!(

!"#$%&'(#")':C;@

*+
F1G)H%I
J6)96)
:-3%??'K&1>%>"3")"$?@

Fig. 2. The neural network architecture that we adopted in this work. The input is a vector of 16 variability features. Each linear unit contains one batch normalization layer and one fully-connected layer. We use ReLU as an activation function between layers. The last linear unit has 21 outputs, which is the number of variable classes in our training set. We use the softmax function before the output layer in order to scale the outputs to lie between 0 and 1, to represent probabilities for the 21 output classes.

increases classification performance. On the other hand, using too many neurons degrades classification performance as can be seen in the bottom three rows due to overfitting of the training data. The Mc differences between the models are statistically significant at the several sigma level. Whether the differences are practically significant or not is, however, another issue. In order to confirm that the differences are practically significant, we would need to apply every trained DNN model to many different time-series databases having labels for light-curves and then check their classification performances. We have not done this, as we just chose the best model, but this does not preclude that a simpler model is in fact almost as good in practice.
We classify each light-curve according to the highest probability of all the classes. The top panel in Fig. 3 shows the highest and the second highest probabilities for each source in the training sample. As the panel shows, the majority of the highest probabilities are larger than 0.8 while the majority of the second highest probabilities are lower than 0.2. In the bottom panel, we show the histogram of differences between the two. As expected, the differences are generally larger than 0.5. This means that the classifier is reasonably confident about its class assignments. The U model returns both the predicted class and the probabilities of all classes, so users can set a threshold on a probability if they desire.

3.3.3. Classification Quality of the U model
In Fig. 4, we show the U model's confusion matrix normalized by the number of true objects per class. The numbers on the leading diagonal represent recall values for each variable class. Most of the recall values are higher than 0.8 except for a few classes such as T2CEPH or QSO. The figure also shows confusion within subclasses of a certain variable class such as LPV or EB. This confusion is expected because variability patterns residing in their light-curves are likely to be similar.
In the top panel of Fig. 5, we show Mc and learning rate as a function of training epoch. The figure shows 500 epochs of the n=18 cycle (see step 5 in Section 3.3.2) where the highest validation-set Mc (annotated with the red arrow) is achieved. As the figure shows, the training-set Mc keeps increasing as a function of epoch. This indicates that the model eventually starts to overfit the training samples. Note that the U model is chosen based on the validation-set Mc, not based on the training-set Mc. The lower panel of Fig. 5 shows the change in the learning rate during the training processes. We see the learning rate cyclying decreasing and increasing, a consequence of our using CLR. Mc value (the top panel) derived using the training sets and validation sets vary according to the changes of learning rate. The

Article number, page 6 of 18

D.-W. Kim et al.: Deep Transfer Learning for Classification of Variable Sources

Table 3. Classification performance of the neural network models

The Number of Hidden Layers and Neurons in Each Layer1 32 64 128 256 32  64 64  128 128  256 256  128 32  64  128 64  128  256 128  256  512 256  512  1024 512  1024  2048

Best Mc
0.8878 0.8926 0.8940 0.8957 0.9005 0.9017 0.9027 0.8996 0.9027 0.9043 0.9031 0.9018 0.9032

Avg. of Mc
0.8848 0.8891 0.8925 0.8929 0.8970 0.8999 0.9002 0.8934 0.9004 0.9015 0.9008 0.9002 0.9001

SEM2
3.9×10-4 3.1×10-4 1.9×10-4 2.1×10-4 2.2×10-4 1.9×10-4 2.4×10-4 1.6×10-4 1.9×10-4 2.2 × 10-4 2.2×10-4 1.5×10-4 2.4×10-4

Model Size
9 KB 14 KB 24 KB 44 KB 21 KB 55 KB 171 KB 169 KB 63 KB 202 KB 726 KB 3 MB 11 MB

1 The integer values indicate the number of neurons in each hidden layer. For instance "32  64" means that there are two hidden layers, which have 32 and 64 neurons. 2 n in Equation (11) is 30.

highest validation-set Mc is obtained after several iterations of CLR.
For comparison purposes, we trained Random Forest (Breiman 2001) models with the same dataset and variabil-
ity features. We trained the models using 80% of the data as a training set and then derived Mc using the remaining 20% data, just as we did for the DNN model training. We
optimized the model hyper-parameters using the 80% training set and using ten-fold cross-validation and brute-force grid search over a two-dimensional grid of t and m, where
t is the number of trees and m is the number of randomly selected features for each node in trees. The grid values of t and m are [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200] and [4, 6, 8, 10, 12, 14], respectively.1. We repeated the training processes ten times. The highest Mc after the training is 0.8996, which is lower than DNN's Mc. The average of Mc and SEM2 are 0.8963 and 7.7×10-4, respectively. Thus we concluded that the Random Forest model gives slightly worse performance than the U model.
Nevertheless, this result does not mean that DNN is always superior than Random Forest for variable source classification using variability features.
In the case of random light-curves that do not belong to
any of the training classes, we found empirically that most of them are classified as non-variables as follows:

­ We generated 1,000 light-curves consisting of purely random values, extracted 16 variability features from them, and predicted their classes using the U model. The U model predicted 800 of them as non-variables, 100 as long-period variables. The remaining 100 light-curves' predicted classes were dispersed through other variable classes.
­ We randomly shuffled data points in each light-curve of the ASAS variable stars (Table 4 in Section 3.4), extracted 16 variability features, and used the U-model to predict their classes. As a result, 7,000 light-curves

1 The grid values are from our
Kim & Bailer-Jones (2016). 2 In this case, n in the equation 11 is 10.

previous

work,

were classified as non-variables, and 3,000 light-curves were classified as EB EDs. The remaining 1,000 lightcurves' predicted classes were dispersed through other classes.
3.4. Application to ASAS light-curves of Periodic Variable Stars
To validate the classification quality of the U model in general, we applied it to the ASAS light-curves of periodic variable stars that contain  Scuti stars, RR Lyraes, Cepheids, eclipsing binaries, and long-period variables (Pojmanski 1997). The duration of the light-curves is about nine years. The average number of data points is about 500. We collected the light-curves from Kim & Bailer-Jones (2016). The number of samples per true variable class and the classification quality is shown in Table 4. We derived the F1 measure of the same dataset as well, and it was 0.87. This is slightly higher than F1=0.85 from Kim & Bailer-Jones (2016) that used the same training set and the 16 variability features to train a Random Forest. Nevertheless, this does not necessarily mean that the U model is always superior to the Random Forest. For instance, we applied the U model to light-curves from the MACHO dataset that Kim & Bailer-Jones (2016) also used. In this case, the F1 measures for both the U model and the Random Forest model are identical at 0.92.
3.5. Feature Importance
We use the SHAP (SHapley Additive exPlanations; Lundberg & Lee 2017) method to measure the feature importance in the U model. SHAP values represents how strongly each feature impacts on model's predictions. They are derived from the Shapley value (Shapley 1953) used in cooperative game theory. Conceptually, the Shapley value measures the degree of contribution of each player to a game where all the players cooperate. Although there are few SHAP applications in physics, Pham et al. (2019); Amacker et al. (2020); Scillitoe et al. (2020) used SHAP to

Article number, page 7 of 18

A&A proofs: manuscript no. UPSILoN-T

Number of sources

60000 50000 40000 30000 20000 10000
0 0.0
50000 40000

Highest probability Second highest probability

0.2

0.4

0.6

0.8

Probability Difference

Fig. 6 shows the SHAP feature importance for the U model. This shows the mean absolute value of SHAP over all samples for each feature. We see that the period is the most important feature, which has also been found in other studies (e.g. Kim et al. 2014; Kim & Bailer-Jones 2016; Elorrieta et al. 2016). All other features also contribute to the classification of variable classes although their contributions are lower than period's contribution.
We trained two independent DNN models with the same architecture as shown in Figure 2. Both are trained with periods removed, the second with the period-related features (, CS, mp90, and mp10) also removed. As a result, the Mc drops from 0.90 to 0.87 in the first case and to 0.82 in the second case. Given that the SEM values (Table 3) are 1.0 quite small, these decreases are significant, again confirming that periods are critical for variable classification. Note, however, that periods of some training classes do not have a strict physical meaning as a period such as for quasars. In such cases, they are just used as a variability feature.

Number of sources

4. Transferring the U model's Knowledge to Other

30000

Light-Curve Datasets of Variable Sources

20000

10000

0

0.0

0.2

0.4

0.6

0.8

1.0

Probability

Fig. 3. Class probability distribution of the light-curves in the training set. The upper panel shows the highest and the second highest probabilities. The bottom panel shows the differences between the two.

Table 4. The number of objects per true class in the ASAS dataset

Superclass Subclass Number Mc

EB

EC

2550 0.91

ED

2167 0.96

ESD

823 0.86

RRL

ab

1218 0.97

c

305 0.82

CEPH

F

567 0.84

1O

179 0.78

DSCT

520 0.88

LPV

2450 0.97

Total

10 779 0.91

measure feature importances for their machine learning applications such as predicting solar flare activities, Higgs selfcoupling measurements, and data-driven turbulence modelling.

4.1. Transfer Learning
Transfer learning is a research field in machine learning that aims to preserve knowledge accumulated while solving one problem (source), then to utilize this to solve other but related problem (target) (Cowan et al. 1994; Pan & Yang 2010). Depending on the problem at hand, conditions of source and target could vary in several ways. In this work, which aims to transfer the U model to data from other surveys, we want to deal with the following issues:
1. Different class label space. The number of variable classes (21 as shown in Table 1) of the OGLE and EROS-2 training set is different from the number of variable classes of the ASAS dataset (9 as shown in Table 4). Moreover, not only the number of variable classes but also the variable types are different. For instance, there are no QSOs or blue variables in the ASAS dataset.
2. Different frequency of class members. The number of contacted eclipsing binaries (EC) in the OGLE and EROS-2 training set is 1398 (see Table 1), which is less than 1% of the dataset, but their number in the ASAS dataset is 2550 (see Table 4), which is 24% of the dataset.
3. Different distribution of variability features. Magnitude ranges, filter wavelengths, and noise characteristics are different per survey. This implies that the distribution of 16 variability features extracted from light-curves is not identical between source and target.
The widely-used transfer-learning approach to deal with these issues is to transfer the weights from the pre-trained model. That is, we start with the same neural network model with the same weights (i.e. the U model). We then continue with the training in one or two different ways. The first is to optimize the weights of every layer, and the second is to optimize the weights of only the last fullyconnected layer while freezing the other layers. In Fig. 2, the last fully-connected layer is in the fourth linear unit (i.e. the last linear unit). In both approaches we adjust the

Article number, page 8 of 18

D.-W. Kim et al.: Deep Transfer Learning for Classification of Variable Sources

BV CEPH 1O CEPH F CEPH Other DSCT EB EC EB ED EB ESD LPV Mira AGB C LPV Mira AGB O LPV OSARG AGB LPV OSARG RGB LPV SRV AGB C LPV SRV AGB O NonVar QSO RRL ab RRL c RRL d RRL e T2CEPH

BV 0.80

0.01

0.11 0.01 0.03 0.02 0.02

CEPH_1O

0.92 0.01 0.02

0.03

CEPH_F

0.01 0.99

CEPH_Other DSCT

0.13 0.83 0.94

0.02 0.01

0.8

0.04

0.01

EB_EC

0.01

0.77 0.15

0.01

0.01 0.01

0.03

0.01

EB_ED

0.94 0.03

0.02

EB_ESD

0.02 0.18 0.77

0.01

0.01

LPV_Mira_AGB_C

0.88 0.03

0.06 0.03

0.6

LPV_Mira_AGB_O

0.08 0.75

0.01 0.16

LPV_OSARG_AGB

0.67 0.26 0.02 0.05

LPV_OSARG_RGB

0.18 0.81 0.01 0.01

LPV_SRV_AGB_C

0.01 0.14 0.02 0.67 0.16

0.4

LPV_SRV_AGB_O

0.01 0.13 0.03 0.11 0.72

NonVar

0.01 0.05

0.01

0.91

QSO 0.09

0.03 0.09

0.12 0.67

RRL_ab RRL_c

1.00 0.2
0.95 0.01 0.03

RRL_d

0.03 0.16 0.81

RRL_e

0.02

0.08 0.89

T2CEPH 0.05 0.10 0.02 0.05 0.07

0.02

0.08

0.03

0.58 0.0

Fig. 4. The confusion matrix of the U model. Labels on the vertical axis are the true classes and labels on the horizontal axis are the predicted classes. Each row is divided by the number of true objects per variable class. Thus the values on each diagonal are recall. We show the value if it is larger than or equal to 0.01.

number of output nodes at the last linear unit so that the number of output nodes is equal to the number of variable classes from the target dataset.3 By doing this, we can obtain appropriate prediction results with regard to the target dataset's classes.
In order to show the benefit of transfer learning, we define a base model (step 2 in the following list) as the non-transferred and directly trained model, and compare it to the transferred models (step 4). We will also compare a model directly trained on a small dataset (scratch models, step 3) with the transferred models. The process for the transfer learning that we apply to each target dataset (i.e. ASAS, Hipparcos, and ASAS-SN) is as follows:
3 Technically, we replace the fully-connected layer in the fourth linear unit with a new fully-connected layer having the same number of output nodes with the number of target's variable classes.

1. Randomly split the target dataset into 80% and 20% while preserving the class ratio (stratified sampling). We call the 80% dataset D0.8, and the 20% dataset the test set.
2. Train a DNN model from scratch with the same architecture shown in Fig. 2. We use the entire D0.8 to train the model. We call this the base model. The training process is the same as the ones from steps 1 to 3 in Section 3.3.2. We then calculate Mc of the base model using the test set. Note that this base model does not use any accumulated knowledge from the U model (i.e. weights in the U model).
3. Randomly choose a 3% subset of D0.8 and then use this subset to train a DNN model from scratch, which has the same architecture as the one shown in Fig. 2. We use the same training process in Section 3.3.2. We call this the scratch model. Also note that we do not transfer any of the U model's accumulated knowledge. After the

Article number, page 9 of 18

Mc

0.925 0.900 0.875 0.850 0.825 0
0.0 -2.5 -5.0 -7.5 -10.0
0

A&A proofs: manuscript no. UPSILoN-T

Training-set Mc Validation-set Mc

100

200

300

400

500

100

200

300

400

500

Training Epoch

log10

Fig. 5. The top panel shows training-set Mc (blue line) and validation-set Mc (orange line) as a function of training epochs. The bottom panel shows changes of learning rate as a result of cyclical learning rate (CLR) application. The red arrow indicates the training epoch where the best validation-set Mc is achieved.

training, we compute Mc for the scratch model on the test set. 4. Use the same 3% subset of D0.8 used for training the scratch model to transfer the U model. As mentioned above, we transfer the U model according two different approaches 1) optimizing weights of every layers, and 2) optimizing weights of only the last fully-connected layer while freezing weights of all other layers. We call the former a every-layer model, and the latter an lastlayer model. Note that both approaches start from the initial weights of the U model rather than from random initialization. We again use the same training process in Section 3.3.2 to optimize the weights. We compute the Mc of these models using the test set. 5. Repeat steps 3 to 4 using 5%, 10%, 15%, 20%, 50%, and 100% of D0.8. Theoretically, classification quality of the base model should be identical to the one of the scratch model trained using 100% of D0.8 because the base model is also trained using the same 100% of D0.8. 6. Repeat steps 1 to 5 30 times and use these to compute the mean and SEM value of Mc.
In the following three sections, we show and analyze the results of using this transfer learning procedure on three different datasets.
4.2. Application to the ASAS Dataset
The results of applying the transfer learning process to the ASAS dataset described in Section 3.4 are given in Fig. 7. The base model's average Mc is 0.9501 and SEM is 1.8×10-3. The figure also shows the results for the other models (described in the previous section), which we now briefly discuss.

1. Scratch model. The scratch models' Mc increase as the size of a subset of D0.8 increases. The one trained using 100% of D0.8 shows Mc = 0.9513, which is very similar to the Mc of the base model.
2. Every-layer model. The every-layer models' Mc are generally higher than the scratch models' Mc, even when their training set is small. This result shows the benefit of transfer learning, even when the target dataset is small. For larger samples (50%-100% subset), the results of the transferred models and scratch models are similar.
3. Last-layer model. The last-layer models preserve more knowledge from the U model than the every-layer models do. For the same reason, the last-layer model could give worse performance because new knowledge from the target dataset cannot be transmitted to every layer. The results show that which effect dominates depends on the training sample size. For small training sets, the last-layer models produce a higher Mc, and vice versa for large training sets. We also see that the SEM values of the last-layer models are smaller than other models, because optimizing the weights in only the last layer gives less freedom to update the weights than optimizing the weights in every layer.
Overall we see that transfer learning produces good results when the size of the target dataset is small, which is one of the advantages of using transfer learning. Transferring only the last layer is not a good choice when there is a relatively large number of target samples. In such cases, training from scratch (base model) or transferring every layer works better, although it requires more computation

Article number, page 10 of 18

D.-W. Kim et al.: Deep Transfer Learning for Classification of Variable Sources

0.96

0.94

Period
H1 mp90 Q3 - 1 
1 2 W K R21 A  CS mp10 R31 31 21
0

BV NonVar RRL ab DSCT RRL e LPV SRV AGB O RRL d QSO CEPH Other RRL c EB EC T2CEPH EB ED EB ESD CEPH 1O CEPH F LPV OSARG AGB LPV OSARG RGB LPV SRV AGB C LPV Mira AGB O LPV Mira AGB C

10

20

30

40

50

60

70

mean(|SHAP value|)

Average Mc

0.92 0.90 0.88 0.86 0.84 0.82 0.80
3

Last-layer models Every-layer models Scratch models

5

10

15

20

50

100

% subset of D0.8

Fig. 7. Average Mc with SEM as error bars for the last-layer, every-layer, and scratch models. The every-layer models' Mc are slightly higher than the scratch models' Mc. The last-layer models show higher Mc for small training sets, and vice versa for large training sets. We add a small offset on the x-axis to the scratch models in order to prevent its error bars from lying on top of the other error bars.

CEPH 1O CEPH F DSCT EB EC EB ED EB ESD LPV RRL ab RRL c

Fig. 6. SHAP (Shapley additive explanations) feature importance Lundberg & Lee (2017). The larger the SHAP value, the more important a feature. The mean absolute SHAP value shows the accumulated feature importance for all variable classes depicted in different colors.

CEPH_1O 0.86 0.04

0.04

0.07

time. In addition, we show a confusion matrix of the every-

CEPH_F 0.03 0.89 0.01

0.01 0.02 0.01 0.01 0.01

0.8

layer model trained on the 100% subset in Fig. 8. This shows DSCT

0.88 0.10 0.02

good classification performance.

EB_EC

0.01 0.01 0.96

0.01

0.6

4.3. Application to the Hipparcos Dataset
We now apply transfer learning to the Hipparcos dataset (Dubath et al. 2011) which has more classes but fewer samples than the ASAS dataset (see Table 5). The 1570 Hipparcos light-curves that we extracted from Kim & Bailer-Jones (2016) is substantially smaller than the 10 779 light-curves in the ASAS dataset. The duration of the light-curves varies from one to three years, and the average number of data points is about 100.
Overall training process is the same as the one mentioned in Section 4.1. To ensure sufficient samples for training, however, we split the Hipparcos data into 50%/50% training/testing sets (D0.5), rather than the 80%/20% that we used for ASAS. We use the entire set of D0.5 to train scratch models and also to transfer the U model. We did not train separate base models since these are now the same as the scratch models.
The result of transfer learning is shown in Table 6. The scratch model's average Mc is 0.7398 whereas the everylayer model's average Mc is 0.7770. The last-layer model' achieves the highest average Mc of all models tested, of 0.8230. Due to the small number of target samples, learning from scratch is significantly inferior to transfer learning in this case. Fig. 9 is a confusion matrix of the last-layer model, which shows relatively large confusion between the classes because of the insufficient target samples as well.

EB_ED

0.01

0.96 0.03

EB_ESD 0.01 0.01 0.01 0.10 0.05 0.82

0.01

0.4

LPV RRL_ab 0.01 0.02

0.01 0.01

0.99 0.2
0.96 0.01

RRL_c

0.14 0.02

0.02 0.82 0.0

Fig. 8. The confusion matrix of the ASAS transferred model. Labels are as in Table 4.

4.4. Application to the ASAS-SN Dataset
In this section, we apply transfer learning to the ASASSN database of variable stars (Shappee et al. 2014; Jayasinghe et al. 2019). We obtained a catalog of the variable stars and their corresponding light-curves from the ASAS-SN website (https://asas-sn.osu.edu/variables). We selected only the light-curves with class probabilities higher than 95%, and whose classification results are not uncertain. The average number of data points in the light-curves is about 200, and the duration is about four years. We extracted 16 time-variability features from each light-curve,

Article number, page 11 of 18

A&A proofs: manuscript no. UPSILoN-T

Table 5. The number of objects per true class in the Hipparcos dataset

Type Eclipsing binary
detached semi-detached contact RR Lyrae
 Cepheid fundamental mode first overtone multi-mode
 Scuti low amplitude
Long-period variable Ellipsoidal RS Canum Venaticorum
+ BY Draconis Slowly pulsating B star  Cygni 2 Canum Venaticorum  Cephei  Doradus B emmission line star
+  Cassiopeiae W Virginis
long-period (> 8 days) short period (< 8 days) SX Arietis RV Tauri Total

Hipparcos Class
EA EB EW RRAB RRC
DCEP DCEPS CEP(B)
DSCT DSCTC
LPV ELL RS+BY
SPB ACYG
ACV BCEP GDOR BE+GCAS
CWA CWB SXARI
RV

Number
222 246
99 71 19
170 28 11 45 80
254 27 35
78 17 75 29 25 12
9 6 7 5 1570

UPSILoN-T class1
EB ED EB ESD
EB EC RRL ab
RRL c
CEPH F CEPH 1O CEPH Other
DSCT DSCT
LPV

1 Corresponding classes in the training set.

Table 6. Transfer learning application using the Hipparcos dataset

Model Scratch Model
Transfer Model (every-layer model)
Transfer Model (last-layer model)

Avg. of Mc 0.7398 0.7770 0.8230

SEM1 1.3×10-2 8.6×10-3 1.8×10-3

1 n in Equation (11) is 30.

excluding light-curves having fewer than 100 data points. In Table 7 we show the number of selected light-curves for each variable class and their corresponding classes4 in the training set (if any). The larger number of samples, 288 698, is sufficient to train DNNs from scratch. Yet even in such cases transfer learning could be useful because it could achieve the highest Mc faster than training from scratch does.
4 We manually assigned the corresponding classes.

We train scratch, every-layer, and last-layer models using the entire set of D0.8 according to the training process mentioned in Section 4.1. Because base models are now the same as the scratch models, we did not train them. The average of Mc and SEM is shown in Table 8. The average Mc of the scratch model and the every-layer model are similar as expected, whereas that of the last-layer model is smaller. Note that we did not train models using a subset of the ASAS-SN dataset because its result would be similar to what we explained in Section 4.2 and 4.3. Fig. 11 is a confusion matrix of the every-layer model. The classification quality is relatively good, but a lot of the SRD, VAR, YSO, LSP, and L are misclassified as SR. Most of these variables are either semi-regular or irregular variables, and thus their variability characteristics are expected to be similar to one another (i.e. no clear periodicities in their light-curves). Furthermore, the number of SRD, VAR, YSO, and LSP is relatively small, a few hundreds per class, whereas the number of SR is 108 943, which could cause the bias toward the majority (SR).
Fig. 10 shows four examples of validation-set Mc at the first 40 training epochs from the n=30 cycles (see steps 1 to 5 in Section 3.3.2). From the figure, we see that the everylayer models reach the highest Mc faster than the scratch models do. We confirmed that the remaining 26 cases out

Article number, page 12 of 18

D.-W. Kim et al.: Deep Transfer Learning for Classification of Variable Sources

ACV ACYG BCEP BE+GCAS CEP(B) CWA CWB DCEP DCEPS DSCT DSCTC EA EB ELL EW GDOR LPV RRAB RRC RS+BY RV SPB SXARI

1.0 ACV 0.68 0.11 0.16 0.05

ACYG 0.25 0.25

0.50

BCEP

0.71

0.14 0.14

BE+GCAS

0.33

0.33

0.33

CEP(B)

1.00

0.8

CWA

1.00

CWB

1.00

DCEP

0.05 0.86 0.02

0.07

DCEPS

0.14

0.14

0.71

DSCT

0.50 0.33

0.08

0.08

0.6

DSCTC

0.10

0.05 0.85

EA 0.04

0.02

0.02

0.82 0.11

EB 0.02 0.02 0.02 0.02 0.03 0.02 0.02 0.05 0.60 0.06 0.08 0.02 0.02 0.02

0.03

ELL 0.17 0.17 EW

0.33 0.17

0.17

0.4

0.08

0.08 0.04 0.76

0.04

GDOR

0.14

0.43

0.14 0.29

LPV

0.03

0.92 0.02

0.03

RRAB

1.00

RRC

0.25

0.25 0.50

0.2

RS+BY 0.11 0.11

0.22

0.11

0.33 0.11

RV

1.00

SPB 0.20 0.05

0.05

0.10

0.60

SXARI

1.00 0.0

Fig. 9. The confusion matrix of the Hipparcos transferred model. Labels are as in Table 5.

of the n=30 cycles show similar behavior. However the difference is not significant, which implies that given enough training samples and computing resources, transfer learning is not really necessary because training from scratch gives as good classification performance as transfer learning does.
5. Performance of a Transferred Model as a
Function of Light-Curve Length and Sampling
In this section we examine the classification performance of the transferred model as a function of the observation durations, d, and the number of data points in a light-curve, n. For this experiment, we used the every-layer model transferred using the 20% subset of ASAS D0.8 (see Section 4.2). Mc of the model is 0.94.
We constructed a set of light-curves by resampling the ASAS light-curves shown in Table 4 using d = 30, 60, 90, 180, 365, 730, and 1470 days, and n = 20, 40, 80, 100, 150, 200, and 300. In order to resample a given light-curve, we first extract measurements observed between the start-

ing epoch and d days after the starting epoch. We then randomly select n unique data points from the extracted measurements to make a resampled light-curve. The left panel of Fig. 12 shows the classification quality of the transferred model applied to these resampled light-curves. As the panel shows, Mc quickly reaches its maximum value once the number of data points reaches 300. Even with lower n = 100 or 150, or lower d = 365 or 730, Mc is fairly high. When n = 20, we see that Mc decreases as d increases. This is because n = 20 is too small number of data points to construct a well-sampled light-curve.
From this experiment, we see that if the number of data points, n, is larger than or equal to 100 or 150, Mc is not significantly lower than what we can achieve using more data points. For comparison, we show Mc of the U model for the same dataset in the right panel of Fig. 12. Mc of the U model is lower than the transferred model, as expected, but the overall patterns of Mc are similar with those of the transferred model. For the Hipparcos and ASAS-SN datasets, we did not resample their light-curves to carry

Article number, page 13 of 18

A&A proofs: manuscript no. UPSILoN-T

Table 7. The number of objects per true class in the ASAS-SN dataset

Type Cepheid  Cephei-type variable
with a symmetrical light-curve  Scuti High amplitude  Scuti RR Lyrae
with an asymmetric light-curve with a symmetric light-curve Double mode RR Lyrae Eclipsing binary W Ursae Majoris type Detached Algol-type Beta Lyrae type W Virginis type variable with period > 8 days Mira variable Red irregular variable Semi-regular variable Yellow semi-regular variable Young stellar object Suspected rotational variable Suspected semi-regular variable Uncertain type of variable Total

ASAS-SN Class DCEP
DCEPS
DSCT HADS
RRAB RRC RRD
EW EA EB CWA
M L LSP SRD YSO ROT SR VAR

Number 743 207
1412 1901
23 891 6226 296
38 284 22 483 10 857
230
7498 53 598
160 135 189 11 395 108 943 250 288 698

UPSILoN-T class1 CEPH F
CEPH 1O
DSCT DSCT
RRL ab RRL c RRL d
EB EC EB ED EB ESD T2CEPH
LPV

1 Corresponding classes in the training set.

Table 8. Transfer learning application using ASAS-SN dataset

Model Scratch Model
Transfer Model (every-layer model)
Transfer Model (last-layer model)

Avg. of Mc 0.8744 0.8711 0.8380

SEM1 7.7×10-4 1.1×10-3 2.6×10-4

1 n in Equation (11) is 30.

Validation-set Mc

Blue: Scratch model, Red: every-layer model

0.90

0.90

0.85

0.85

0.80

0.80

0.75

0.75

0.70 0
0.90

0.70

10 20 30 40

0

0.90

10 20 30 40

0.85

0.85

0.80

0.80

0.75

0.75

out the same experiment due to a lack of either n or d. Nevertheless, if enough n and d is given, we expect to see similar variations in Mc for other datasets considered, because those models are also derived from the U model using transfer learning.
6. Summary

0.70 0

0.70

10 20 30 40

0

10 20 30 40

Training Epoch

Fig. 10. Examples of ASAS-SN validation-set Mc of the scratch models (blue line) and the every-layer models (orange line). As the figure shows, transferring the UT model reaches the highest Mc faster than training from scratch does.

We have introduced deep transfer learning for classifying light-curves of variable sources in order to diminish difficulties in collecting sufficient training samples with labels, designing an individual neural network for every survey, and applying inferred knowledge from one dataset to another. Our approach starts with training a deep neural network using 16 variability features that are relatively survey-

independent. These features are extracted from a training set composed of the light-curves of OGLE and EROS-2 variable sources. We then apply transfer learning to optimize the weights in the trained network so that we can extend the original model to new datasets, here ASAS, Hipparcos, and ASAS-SN. From these applications we see that trans-

Article number, page 14 of 18

D.-W. Kim et al.: Deep Transfer Learning for Classification of Variable Sources

CWA DCEP DCEPS DSCT EA EB EW HADS L LSP M ROT RRAB RRC RRD SR SRD VAR YSO

CWA 0.86 0.05

0.08

DCEP 0.04 0.90 0.02

0.03

0.01

DCEPS

0.09 0.79

0.12

DSCT

0.88

0.06 0.02

0.03

0.8

EA

0.90 0.03 0.01

0.01

0.04

EB

0.10 0.76 0.11

0.01

0.02

EW

0.02 0.94

0.02

0.02

HADS

0.02

0.09 0.86

0.03

0.6

L

0.46

0.53

LSP

0.54

0.46

M

0.97

0.03

ROT

0.01

0.03

0.78

0.17

0.4

RRAB

0.99

0.01

RRC

0.12

0.86

RRD

0.91

0.09

SR

0.10

0.88

0.2

SRD

0.23

0.77

VAR

0.02

0.12

0.05 0.05 0.12

0.62

YSO

0.07

0.20

0.73

0.0

Fig. 11. The confusion matrix of the ASAS SN transferred model. Labels are as in Table 7.

fer learning successfully utilizes knowledge inferred from the source dataset (i.e. OGLE and EROS-2) to the target dataset even with a small number of the target samples. In particular, we see that:
­ Transferring only the last layer of the network shows better classification quality than transferring every layer when there are few samples. For instance, the last-layer models' classification performance is 3%-7% higher than the scratch models when a small number of ASAS samples (i.e. 3%-15% of the entire ASAS dataset) are used (see Fig. 7).
­ As shown in Fig. 7, when there exists a sufficient number of samples (i.e. 20%-100% of the entire ASAS dataset), the every-layer models give better classification performance 1%-2% higher than the last-layer models.
­ Transfer learning successfully works even when label space are different between the source and the target. For instance, the Hipparcos dataset contains many types of variable sources different from the training samples (i.e. OGLE and EROS-2) as shown in Table 5. Transfer learning successfully works as the every-layer and the last-layer models show 4% and 9% better classification performance, respectively, than the scratch models (see Table 6).

From these results of using transfer learning, we see that transfer learning is useful for classification of variable sources from time-domain surveys that have a few training samples. This is a significant benefit since it is not always possible to build a robust training set with a sufficient number of samples. For instance, many ongoing and upcoming time-domain surveys such as Gaia (Gaia Collaboration et al. 2016), ZTF (Bellm et al. 2019), SkyMapper (Keller et al. 2007), and Pan-STARRS (Chambers et al. 2016) will produce a large number of lightcurves of astronomical sources. Assembling a sufficiently large set of labelled light-curves for training for each survey, however, is a difficult and time-consuming task, especially in the early stage of the survey. Given that transfer learning works even when using a small set of data, transfer learning would prove useful for classifying light-curves and thus building an initial training set for such surveys. Nevertheless, if enough training samples are readily available, transfer learning does not give much benefit but at least it does not do any worse as can be seen from the results of every-layer models (e.g. Fig. 7 and Table 8).
We have not used any survey-specific features such as colours. It would nonetheless be interesting to test whether including such features improves classification performance.

Article number, page 15 of 18

1.0

20

40

0.9

80

100

150

200

0.8

300

0.7

A&A proofs: manuscript no. UPSILoN-T

1.0

20

40

0.9

80

100

150

200

0.8

300

0.7

Mc Mc

0.6

0.6

0.5

30

60 90 180 365 730 1460

Full

Length of light-curves (in days)

0.5

30

60 90 180 365 730 1460

Full

Length of light-curves (in days)

Fig. 12. Classification quality, Mc, of the resampled ASAS light-curves as a function of duration, d (horizontal axis), and the number of data points n (different lines) in the light-curve. Left: Mc of the transferred model. Right: Mc of the U model.

For instance, one could replace the lowest important feature 21 (see Fig. 6) with a colour. One could then train a DNN model from scratch with this and transfer the trained model to another survey that uses different colours.
We provide a python software package containing a pretrained network (the U model) and functionality to transfer the U model to any time-domain surveys. The package also can train a DNN model from scratch and deal with imbalanced datasets. For details about the package, see Appendix.
Acknowledgement
This work was supported by a National Research Council of Science & Technology (NST) grant by the Korean government (MSIP) [No. CRC-15-05-ETRI].
References
Ackermann, S., Schawinski, K., Zhang, C., Weigel, A. K., & Turp, M. D. 2018, MNRAS, 479, 415
Agatonovic-Kustrin, S. & Beresford, R. 2000, Journal of pharmaceutical and biomedical analysis, 22, 717
Amacker, J., Balunas, W., Beresford, L., et al. 2020, arXiv e-prints, arXiv:2004.04240
Bailer-Jones, C. A. L., Fouesneau, M., & Andrae, R. 2019, MNRAS, 490, 5615
Bellm, E. C., Kulkarni, S. R., Graham, M. J., et al. 2019, PASP, 131, 018002
Boughorbel S, Jarray F, E.-A. M. 2017, PLoS ONE Breiman, L. 2001, Machine Learning, 45, 5 Byra, M., Wu, M., Zhang, X., et al. 2020, Magnetic Resonance in
Medicine, 83, 1109 Chambers, K. C., Magnier, E. A., Metcalfe, N., et al. 2016, arXiv
e-prints, arXiv:1612.05560 Chawla, N., Bowyer, K., Hall, L., & Kegelmeyer, W. 2002, J. Artif.
Intell. Res. (JAIR), 16, 321 Chicco, D. 2017, BioData Mining, 10, 35 Chicco, D. & Jurman, G. 2020, BMC Genomics, 21, 6 Cowan, J., Tesauro, G., & Alspector, J. 1994, Advances in Neural
Information Processing Systems 6, Advances in Neural Information Processing Systems (Morgan Kaufmann) Di, S., Zhang, H., Li, C., et al. 2018, IEEE Transactions on Intelligent Transportation Systems, 19, 745 D'Isanto, A. & Polsterer, K. L. 2018, A&A, 609, A111 Domínguez Sánchez, H., Huertas-Company, M., Bernardi, M., et al. 2019, MNRAS, 484, 93

Dubath, P., Rimoldini, L., Süveges, M., et al. 2011, MNRAS, 414, 2602
Duchi, J., Hazan, E., & Singer, Y. 2011, Journal of Machine Learning Research, 12, 2121
Duev, D. A., Mahabal, A., Ye, Q., et al. 2019, MNRAS, 486, 4158 Ellaway, P. 1978, Electroencephalography and Clinical Neurophysiol-
ogy, 45, 302 Elorrieta, F., Eyheramendy, S., Jordán, A., et al. 2016, A&A, 595,
A82 Fernandes, J. A., Irigoien, X., Goikoetxea, N., et al. 2010, Ecological
Modelling, 221, 338 Gaia Collaboration, Prusti, T., de Bruijne, J. H. J., et al. 2016, A&A,
595, A1 Goodfellow, I., Pouget-Abadie, J., Mirza, M., et al. 2014, in Advances
in Neural Information Processing Systems 27, ed. Z. Ghahramani,
M. Welling, C. Cortes, N. D. Lawrence, & K. Q. Weinberger (Cur-
ran Associates, Inc.), 2672­2680 Gorodkin, J. 2004, Comput. Biol. Chem., 28, 367 Graczyk, D., Soszyski, I., Poleski, R., et al. 2011, Acta Astronomica,
61, 103 Graves, A., Mohamed, A.-r., & Hinton, G. 2013, arXiv e-prints,
arXiv:1303.5778 Grison, P. 1994, A&A, 289, 404 He, H., Bai, Y., Garcia, E. A., & Li, S. 2008, 2008 IEEE International
Joint Conference on Neural Networks (IEEE World Congress on
Computational Intelligence), 1322 Hinton, G., Deng, l., Yu, D., et al. 2012, Signal Processing Magazine,
IEEE, 29, 82 Hinton, G., Vinyals, O., & Dean, J. 2015, arXiv e-prints,
arXiv:1503.02531 Hon, M., Stello, D., & Yu, J. 2017, MNRAS, 469, 4578 Hosenie, Z., Lyon, R. J., Stappers, B. W., & Mootoovaloo, A. 2019,
MNRAS, 488, 4858 Hu, G., Zhang, Y., & Yang, Q. 2019, arXiv e-prints, arXiv:1901.07199 Ioffe, S. & Szegedy, C. 2015, arXiv e-prints, arXiv:1502.03167 Jayasinghe, T., Kochanek, C. S., Stanek, K. Z., et al. 2018, MNRAS,
477, 3145 Jayasinghe, T., Stanek, K. Z., Kochanek, C. S., et al. 2019, MNRAS,
485, 961 Kains, N., Calamida, A., Rejkuba, M., et al. 2019, MNRAS, 482, 3058 Keller, S. C., Schmidt, B. P., Bessell, M. S., et al. 2007, PASA, 24, 1 Kim, D.-W. & Bailer-Jones, C. A. L. 2016, A&A, 587, A18 Kim, D.-W., Protopapas, P., Bailer-Jones, C. A. L., et al. 2014, A&A,
566, A43 Kim, D.-W., Protopapas, P., Byun, Y.-I., et al. 2011, ApJ, 735, 68 Kim, D.-W., Protopapas, P., Trichas, M., et al. 2012, ApJ, 747, 107 Kim, E. J. & Brunner, R. J. 2017, MNRAS, 464, 4463 Kingma, D. P. & Ba, J. 2014, arXiv e-prints, arXiv:1412.6980 Krizhevsky, A., Sutskever, I., & E. Hinton, G. 2012, Neural Informa-
tion Processing Systems, 25 Lanusse, F., Ma, Q., Li, N., et al. 2018, MNRAS, 473, 3895 Lecun, Y., Bengio, Y., & Hinton, G. 2015, Nature, 521, 436

Article number, page 16 of 18

D.-W. Kim et al.: Deep Transfer Learning for Classification of Variable Sources

Li, R., Zhao, Z., Chen, X., Palicot, J., & Zhang, H. 2014, IEEE Transactions on Wireless Communications, 13, 2000
Lieu, M., Conversi, L., Altieri, B., & Carry, B. 2019, MNRAS, 485, 5831
Lomb, N. R. 1976, Ap&SS, 39, 447
Long, J. P., El Karoui, N., Rice, J. A., Richards, J. W., & Bloom, J. S. 2012, PASP, 124, 280
Lu, C., Hu, F., Cao, D., et al. 2019, IEEE Transactions on Intelligent Transportation Systems, 1
Lundberg, S. M. & Lee, S.-I. 2017, in Advances in Neural Information Processing Systems 30, ed. I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, & R. Garnett (Curran Associates, Inc.), 4765­4774
Maqsood, M., Nazir, F., Khan, U., et al. 2019, Sensors (Basel, Switzerland), 19, 2645
Matthews, B. 1975, Biochimica et Biophysica Acta (BBA) - Protein Structure, 405, 442
Nair, V. & Hinton, G. E. 2010, in Proceedings of the 27th international conference on machine learning (ICML-10), 807­814
Pan, S. J. & Yang, Q. 2010, IEEE Trans. on Knowl. and Data Eng., 22, 1345
Pan, W., Xiang, E. W., & Yang, Q. 2012, in Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence, AAAI'12 (AAAI Press), 662­668
Pearson, K. A., Palafox, L., & Griffith, C. A. 2018, MNRAS, 474, 478
Petegrosso, R., Park, S., Hwang, T. H., & Kuang, R. 2016, Bioinformatics, 33, 529
Pham, C., Pham, V., & Dang, T. 2019, in 2019 IEEE International Conference on Big Data (Big Data), 5844­5853
Pojmanski, G. 1997, Acta Astronomica, 47, 467
Poleski, R., Soszyñski, I., Udalski, A., et al. 2010, Acta Astronomica, 60, 1
Powers, D. M. W. 2011, Journal of Machine Learning Technologies, 2, 37
Scillitoe, A., Seshadri, P., & Girolami, M. 2020, arXiv e-prints, arXiv:2003.01968
Shapiro, S. S. & Wilk, M. B. 1965, Biometrika, 52, 591
Shapley, L. S. 1953, Princeton University Press CY - Princeton, 2
Shappee, B. J., Prieto, J. L., Grupe, D., et al. 2014, ApJ, 788, 48
Shin, H.-C., Roth, H. R., Gao, M., et al. 2016, arXiv e-prints, arXiv:1602.03409
Silver, D., Huang, A., Maddison, C. J., et al. 2016, Nature, 529, 484
Smith, L. N. 2015, arXiv e-prints, arXiv:1506.01186
Soszyñski, I., Udalski, A., Szymañski, M. K., et al. 2009, Acta Astronomica, 59, 239
Soszynski, I., Poleski, R., Udalski, A., et al. 2008, Acta Astronomica, 58, 163
Soszyski, I., Udalski, A., Szymaski, M. K., et al. 2008, Acta Astronomica, 58, 293
Soszyski, I., Udalski, A., Szymaski, M. K., et al. 2009, Acta Astronomica, 59, 1
Stetson, P. B. 1996, PASP, 108, 851
Szegedy, C., Liu, W., Jia, Y., et al. 2015, in Computer Vision and Pattern Recognition (CVPR)
Tang, H., Scaife, A. M. M., & Leahy, J. P. 2019, MNRAS, 488, 3358
Tisserand, P., Le Guillou, L., Afonso, C., et al. 2007, A&A, 469, 387
Udalski, A., Kubiak, M., & Szymanski, M. 1997, Acta Astronomica, 47, 319
Vafaei Sadr, A., Vos, E. E., Bassett, B. A., et al. 2019, MNRAS, 484, 2793
Valverde-Albacete, F. J. & Peláez-Moreno, C. 2014, PLoS One, 9, e84217
von Neumann, J. 1941, Ann. Math. Statist., 12, 367
Wang, J., Zheng, H., Huang, Y., & Ding, X. 2018, IEEE Transactions on Intelligent Transportation Systems, 19, 2913
Xu, Q., Xiang, E. W., & Yang, Q. 2010, in 2010 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), 62­67
Yeo, D., Bae, J., Kim, N., et al. 2018, in 2018 25th IEEE International Conference on Image Processing (ICIP), 674­678
Yim, J., Joo, D., Bae, J., & Kim, J. 2017, in 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 7130­7138
Zeiler, M. D. 2012, arXiv e-prints, arXiv:1212.5701
Zhao, Q. & Grace, D. 2014, in 1st International Conference on 5G for Ubiquitous Connectivity, 152­157

Appendix

How to Predict Variable Classes Using UPSILoN-T
The UPSILoN-T python software package is available from https://etrioss.kr/ksb/upsilon-t5 The following pseudocode shows how to use the package to extract variability features from a set of light-curves and then to predict their variable classes.
from u p s i l o n t import UPSILoNT from u p s i l o n t . f e a t u r e s import V a r i a b i l i t y F e a t u r e s

# Extract f e a t u r e s from a s e t of l i g h t -cu rves . feature_list = [] for light_curve in set_of_light_curves :

# Read a l i g h t -cu rve . date = np . array ( [ : ] ) mag = np . a r r a y ( [ : ] ) e r r = np . array ( [ : ] )

# Extract features . v a r _ f e a t u r e s = V a r i a b i l i t y F e a t u r e s ( date , mag , err ) . get_features ()

# Append to the l i s t . f e a t u r e _ l i s t . append ( var_features )

# Convert to Pandas DataFrame . f e a t u r e s = pd . DataFrame ( f e a t u r e _ l i s t )

# Classify . ut = UPSILoNT ( ) label , prob = ut . predict ( features ,
True )

return_prob=

label and prob is a list of predicted classes and class probabilities, respectively.
How to Transfer UPSILoN-T's Knowledge to Another Dataset
The UPSILoN-T package can transfer the U model as follows:
# Get f e a t u r e s and l a b e l s . features = . . . labels = ...
# T r a n s f e r UPSILoN-T . ut = UPSILoNT ( ) ut . transfer ( features , labels , "/path/to/
transf erred /model/")

features is a list of features extracted from a set of lightcurves, and labels is a list of corresponding labels. The package writes the transferred model and other modelrelated parameters to a specified location (i.e. "/path/to/transferred/model/" in the above pseudocode). The transferred model can predict variable classes as follows:
# Get f e a t u r e s . features = . . .
# Predict . ut = UPSILoNT ( )
5 Permission is required to access the repository. Please send an email to one of the authors of this paper, either D.-W. Kim or D. Yeo.

Article number, page 17 of 18

A&A proofs: manuscript no. UPSILoN-T

ut . load ( "/path/to / transf erred /model/")
label , prob = ut . predict ( features , return_prob= True )

# Do both . ut . t r a i n ( f e a t u r e s , l a b e l s , b a l a n c e d _ s a m p l i n g=True ,
w e i g h t _ c l a s s=True )

How to Deal with an Imbalanced Dataset
As shown in Table 1, training set is often imbalanced. For instance, the number of OSARG RGBs is about 179 times more than the number of QSOs (i.e. 29 516 / 165  179). Due to such class imbalance, training of a network could be biased towards the more dominant classes, even though this dominance is just a consequence of the relative frequency of available training samples that we do not want to learn.
There are several approaches to deal with imbalanced datasets such as weighting a loss function according to the sample frequencies, synthesizing artificial samples (e.g. SMOTE introduced at Chawla et al. 2002, or ADASYN introduced at He et al. 2008), over- or under-sampling methods that repeat or remove samples in order to balance the sample frequency per class, or Bayesian methods (Bailer-Jones et al. 2019). Bailer-Jones et al. (2019) introduces a method to accommodate class imbalance in probabilistic multi-class classifiers. The UPSILoN-T library provides two of these approaches when training or transferring a model:
­ Weighting a loss function In Equation 3, we weight a loss function using , which is proportional to the number of samples per class. In brief, we give higher weight to the minority class, and vice versa.  for each class is defined as follows:

i =

fi

c i=1

fi

,

where fi

=

1 Ni

,

(12)

where Ni is the number of samples per class and c is the number of variable classes.
­ Over-sampling technique Conceptually, an over- or under-sampling method builds an artificial training set by balancing the number of each class in the original training set. Among them, the oversampling method resamples the original training set by randomly duplicating samples from the minority class. For instance, if there are 20 samples of class A and 40 samples of class B, over-sampling randomly selects twice as many samples from class A as class B. The disadvantage of this approach to addressing imbalance is that the training time is increased. Another disadvantage is that a trained model could overfit the minority class since it just duplicates samples.

The following pseudocode shows how to use these two methods.
ut = UPSILoNT ( )

# Over-s am p l i n g . ut . t r a i n ( f e a t u r e s , l a b e l s , b a l a n c e d _ s a m p l i n g=True )

# Weighting a l oss function . ut . t r a i n ( f e a t u r e s , l a b e l s , w e i g h t _ c l a s s=True )

Article number, page 18 of 18

