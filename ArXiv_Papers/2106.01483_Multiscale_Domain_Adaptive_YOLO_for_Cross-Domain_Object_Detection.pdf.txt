Multiscale Domain Adaptive YOLO for Cross-Domain Object Detection
Mazin Hnewa and Hayder Radha Michigan State University
East Lansing, Michigan, 48824, United States
(mazin,radha)@msu.edu

arXiv:2106.01483v1 [cs.CV] 2 Jun 2021

Abstract
The area of domain adaptation has been instrumental in addressing the domain shift problem encountered by many applications. This problem arises due to the difference between the distributions of source data used for training in comparison with target data used during realistic testing scenarios. In this paper, we introduce a novel MultiScale Domain Adaptive YOLO (MS-DAYOLO) framework that employs multiple domain adaptation paths and corresponding domain classifiers at different scales of the recently introduced YOLOv4 object detector to generate domaininvariant features. We train and test our proposed method using popular datasets. Our experiments show significant improvements in object detection performance when training YOLOv4 using the proposed MS-DAYOLO and when tested on target data representing challenging weather conditions for autonomous driving applications.

1. Introduction
Convolutional Neural Networks (CNNs) have been achieving exceedingly improved performance for object detection in terms of classifying and localizing a variety of objects in a scene [17, 14, 16, 13]. However, under a domain shift, when the testing data has a different distribution from the training data distribution, the performance of state-of-the-art object detection methods drop noticeably and sometimes significantly. Such domain shift could occur due to capturing the data under different lighting or weather conditions, or due to viewing the same objects from different view points leading to changes in object appearance and background. For example, training data used for autonomous vehicles is normally captured under favorable clear weather conditions whereas testing could take place under more challenging weather (e.g. rain, fog). Consequently, methods fail to detect objects as shown in the examples of Figure 1(b). In that context, the domain under which training is done is known the source domain while the new domain under which testing is conducted is referred

Figure 1. Visual detection examples using the original YOLOv4 method on: (a) clear images and (b) foggy images. (c) Our proposed MS-DAYOLO applied onto foggy images.
to as the target domain. One of the challenges that aggravates the domain shift
problem is the lack of annotated target domain data. This led to the emergence of the area of domain adaptation [4, 11, 21, 15], which has been widely studied to solve the problem of domain shift without the need to annotate data for new target domains. Recently, domain adaptation has been used to improve the performance of object detection due to domain shift [12]. It attempts to learn a robust object detector using labeled data from the source domain and unlabeled data from the target domain. Most domain adaptation approaches in literature employ adversarial training strategy [8]. In particular, a domain classifier is optimized to identify whether a data point from the source or target

1

domain, while the feature extractor of object detector is optimized to confuse the domain classifier. This strategy makes the feature extractor learn domain invariant features. Domain adaptive Faster R-CNN [2] is the first work that employed adversarial training for domain adaptation based object detection. After that, many adversarial-based methods were developed for domain adaptation object detection [24, 22, 18, 10].
Equally important, the particular domain adaptation solution used is influenced greatly by the underlying object detection method architecture. In that context, within the area of object detection, domain adaptation has been studied rather extensively for Faster R-CNN object detection and its variants [2, 24, 22, 18, 10].
Despite its popularity, Faster R-CNN suffers from long inference time to detect objects. As a result, it is arguably not the optimal choice for time-critical, real-time applications such as autonomous driving. On the other hand, onestage object detectors, and in particular YOLO, can operate quite fast, even much faster than real-time, and this makes them invaluable for autonomous driving and similar timecritical applications. Furthermore, domain adaptation for the family of YOLO architectures have received virtually no attention. Besides the computational advantage of YOLO, the latest version, YOLOv4, has many salient improvements and its object detection performance has improved rather significantly relative to prior YOLO architectures and more important in comparison to Faster R-CNN. All of these factors motivated our focus on the development of a new domain adaptation framework for YOLOv4.
In this paper, we propose a novel MultiScale Domain Adaptive YOLO (MS-DAYOLO) that supports domain adaptation at different layers of the feature extraction stage within the YOLOv4 backbone network. In particular, MSDAYOLO includes a Domain Adaptive Network (DAN) with multiscale feature inputs and multiple domain classifiers. We conducted extensive experiments using popular datasets. These experiments show that our proposed MSDAYOLO framework provides significant improvements to the performance of YOLOv4 when tested on target domain as shown in the examples of Figure 1(c). To the best of our knowledge, this is the first proposed work that improves the performance of YOLO for cross domain object detection.
2. Proposed MS-DAYOLO
YOLOv4 [1] has been released recently as the latest version of the family of the YOLO object detectors. Relative to its predecessor, YOLOv4 has incorporated many new revisions and novel techniques to improve the overall detection accuracy. YOLOv4 has three main parts: backbone, neck, and head as shown in Figure 2. The backbone is responsible for extracting multiple layers of features at different scales. The neck collects these features from three different

scales of the backbone using upsampling layers and feed them to the head. Finally, the head predicts bounding boxes surrounding objects as well as class probabilities associated with each bounding box.
The backbone (i.e.feature extractor) represents a major module of the YOLOv4 architecture, and we believe that it makes a significant impact on the overall performance of the detector. In addition to many convolutional layers, it has 23 residual blocks [9], and five downsampling layers to extract critical layers of features that are used by the subsequent detection stages. Here, we consecrate on the features (F1, F2, and F3 in Figures 2) because they are fed to the next stage (neck module). In particular, our goal is to apply domain adaptation to these three features to make them robust against domain shifts at different scales, and hence, have them converge toward domain invariance during domainadaptation based training.
2.1. Domain Adaptive Network for YOLO
The proposed Domain Adaptive Network (DAN) is attached to the YOLOv4 object detector only during training in order to learn domain invariant features. Indeed, YOLOv4 and DAN are trained in an end-to-end fashion. For inference, and during testing, domain-adaptive trained weights are used in the original YOLOv4 architecture (without the DAN network). Therefore, our proposed framework will not increase the underlying detector complexity during inference, which is an essential factor for many real-time applications such as autonomous driving.
DAN uses the three distinct scale features of the backbone that are fed to the neck as inputs. It has several convolutional layers to predict the domain class (either source or target). Then, domain classification loss (Ldc) is computed via binary cross entropy as follows:
Ldc = - [ti ln pi(x,y) + (1 - ti) ln(1 - p(ix,y))] (1)
i,x,y
Here, ti is the ground truth domain label for the i-th training image, with ti = 1 for source domain and ti = 0 for target domain. p(ix,y) is predicted domain class probabilities for i-th training image at location (x, y) of the feature map.
DAN is optimized to differentiate between the source and target domains by minimizing this loss. On the other hand, the backbone is optimized to maximize the loss to learn domain invariant features. Thus, features of the backbone should be indistinguishable for the two domains. Consequently, this should improve the performance of object detection for the target domain.
To solve the joint minimization and maximization problem, we employ adversarial leaning strategy[8]. In particular, we achieve this contradictory objectives by using a Gradient Reversal Layer (GRL) [6, 7] between the backbone

2

Figure 2. Architecture of the proposed MS- DAYOLO. Domain adaptation network (DAN) is attached to the YOLO object detector only during training in order to learn domain invariant features.

and the DAN network. GRL is a bidirectional operator that is used to realize two different optimization objectives. In the feed-forward direction, the GRL acts as an identity operator. This leads to the standard objective of minimizing the classification error when performing local backpropagation within DAN. On the other hand, for backpropagation toward the backbone network, the GRL becomes a negative scalar (). Hence, in this case, it leads to maximizing the binary-classification error; and this maximization promotes the generation of domain-invariant features by the backbone.
To compute the detection loss (Ldet) [16], only source images are used because they are annotated with groundtruth objects. Consequently, all three parts of YOLOv4 (i.e. backbone, neck and head) are optimized via minimizing Ldet. On the other hand, both source labeled images and target unlabeled images are used to compute the domain classification loss (Ldc) which is used to to optimize DAN via minimizing it, and the backbone via maximizing it. As a result, both Ldet and Ldc are used to optimize the backbone. In other words, the backbone is optimized by minimizing the following total lose:

Lt = Ldet + Ldc

(2)

where  is a negative scalar of GRL that balances a trade-off between the detection loss and domain classification loss. In fact,  controls the impact of DAN on the backbone.
2.2. DAN Architecture Instead of applying domain adaptation for only the fi-
nal scale of the feature extractor as done in the Domain

Adaptive Faster R-CNN architecture [2], we develop domain adaptation for three scales separately to solve gradient vanishing problem. In other word, applying domain adaptation only to the final scale (F3) does not make significant impact to the previous scales (F1 and F2) due to gradient vanishing problem as there are many layers between them. As a result, we employ a multiscale strategy that connects the three features F1, F2, and F3 of the backbone to the DAN through three corresponding GRLs as shown in Figure 2. For each scale, there are two convolutional layers after GRL, the first one reduces the feature channels by half, and the second one predict the domain class probabilities. Finally, a domain classifier layer is used to compute the domain classification loss.
3. Experiments
In this section, we evaluate our proposed MS-DAYOLO. We modified the official source code of YOLOv4 that is based on the darknet platform1; and we developed new code to implement our proposed method.
3.1. Setup For training, we used the default settings and hyper-
parameters that were used in the original YOLOv4 [1]. The network is initialized using the pre-trained weights file. The training data includes two sets: source data that has images and their annotations (bounding boxes and object classes), and target data without annotation. Each batch has 64 images, 32 from the source domain and 32 from target domain. We set  = 0.1 for all experiments.
1https://github.com/AlexeyAB/darknet

3

For evaluation, we report Average Precision (AP) for each class as well as mean average precision (mAP) with threshold of 0.5 [5] using testing data that has labeled images of target domain. We compare our proposed method with the original YOLOv4, both applied to the same target domain validation set.
Domain shifts due to changes in weather conditions is one of the most prominent reasons for discrepancy between the source and target domains. Reliable object detection systems in different weather conditions are essential for many critical applications such an autonomous driving. As a result, we focus on presenting our evaluation results of our proposed MS-DAYOLO by studying domain shifts under changing weather condition for autonomous driving. To achieve this, we use different driving datasets: Cityscapes [3], Foggy Cityscapes [19], BDD100K [23], and INIT [20].
3.2. Results and Discussion Clear  Foggy: we discuss the ability of our proposed
method to adapt from clear to foggy weather as has been done by many recent works in this area [2, 24, 22, 18, 10]. Original YOLOv4 is trained using the Cityscapes training set. While MS-DAYOLO is trained using the Cityscapes training set as source domain and the Foggy Cityscapes training set without labels as target domain. The Foggy Cityscapes validation set is used for testing and evaluation. Because Foggy Cityscapes training set is annotated, we are able to train the original YOLOv4 with this set to show the ideal performance (oracle). The Cityscapes dataset has eight classes. However, because the number of ground-truth objects for some classes (truck, bus, and train) is small (i.e. less than 500 in training set, and 100 in testing set), the performance measure will be inaccurate for these classes. As a result, we exclude them and compute mAP based on the remaining classes.
To show the important of applying domain adaptation to three distinct scales of the backbone network, we conducted an ablation study. First, we applied domain adaptation, separately, to each of the three scales of features that are fed into the neck of the YOLOv4 architecture. Then we apply domain adaptation to different combinations of two scales at a time. Finally, we compared the results with the performance of applying these combinations of the study with the performance of applying our MS-DAYOLO to all three scales.
Table 1 summarizes the performance results. It is clear that based on these results, we can conclude that applying domain adaptation to all three feature scales improves the detection performance on target domain, and achieves the best result. Moreover, our proposed MS-DAYOLO outperforms the original YOLOv4 approach by significant margin, and it almost reaches the performance of the ideal (oracle) scenario, especially for some object classes in terms of average precision and overall mAP. Figure 1 shows examples

Table 1. Quantitative results on adaptation from clear to foggy

weather of the Cityscapes dataset. means that domain adapta-

tion is applied to the feature scale(s) using our MS-DAYOLO. The

classes are P:Person, R:Rider, C:Car, M: Motorcycle, and B: Bi-

cycle. Results in red are obtained using the baseline YOLO for

comparison with our method.

Method F1 F2 F3 P R C M B mAP

YOLO

31.57 38.27 46.93 16.75 30.32 32.77

36.84 42.84 53.69 24.77 32.35 38.10

37.08 41.49 54.49 26.22 32.43 38.34

36.28 44.22 53.10 25.81 35.87 39.06

Ours

36.62 42.68 55.70 26.09 33.52 38.92

37.50 42.48 54.53 27.84 34.75 39.42

36.41 46.06 52.19 22.48 34.99 38.43

38.62 45.52 55.85 28.82 36.46 41.05

Oracle

42.35 49.50 63.59 31.10 39.68 45.24

Table 2. Quantitative results on adaptation from sunny to rainy

weather of the BDD100K and INIT datasets. The classes

of BDD100K are V: Vehicle, P:Person, TS:Traffic Sign, and

TL:Traffic Light. The classes of INIT are P:Person, C: Car, and

SLS: Speed Limit Sign.

BDD100K

Method V

P

TS

TL

mAP

YOLO 72.54 41.54 56.06 47.07 54.30

Ours 73.74 45.37 58.32 48.00 56.36

Method P YOLO 44.52 Ours 48.80

INIT C 74.48 76.03

SLS 48.39 50.00

mAP 55.80 58.28

of detection results of the proposed method as compared to the original YOLOv4.
Sunny  Rainy: we also discuss the ability of our proposed method to adapt from sunny to rainy weather using BDD100K [23] and INIT [20] datasets. We extracted "sunny weather" labeled images for the source data, and "rainy weather" unlabeled images to represent the target data. As before, the original YOLOv4 is trained using only source data (i.e. labeled sunny images). Meanwhile, our proposed MS-DAYOLO is trained using both source and target data (i.e. labeled sunny images and unlabeled rainy images). In addition, we extracted labeled images from the rainy-weather data for testing and evaluation. The results are summarized in Table 2. A clear performance improvement is achieved by our method over the original YOLO in both datasets.
4. Conclusion
In this paper, we proposed a multiscale domain adaptation framework for the popular real time object detector YOLO. Specifically, under our MS-DAYOLO, we applied domain adaptation to three different scale features within

4

the YOLO feature extractor that are fed to the next stage. The proposed method improves the overall detection performance under the target domain because it produces robust domain invariant features that reduce the impact of domain shift. Based on various experimental results, our framework can successfully adapt YOLO to target domains without any need for annotation. Furthermore, the proposed MSDAYOLO outperformed state-of-the-art YOLOv4 under diverse testing scenarios for autonomous driving applications.
References
[1] Alexey Bochkovskiy, Chien-Yao Wang, and HongYuan Mark Liao. Yolov4: Optimal speed and accuracy of object detection. arXiv preprint arXiv:2004.10934, 2020. 2, 3
[2] Yuhua Chen, Wen Li, Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Domain adaptive faster r-cnn for object detection in the wild. In Proc. CVPR, pages 3339­3348, 2018. 2, 3, 4
[3] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In Proc. CVPR, pages 3213­3223, 2016. 4
[4] Lixin Duan, Ivor W Tsang, and Dong Xu. Domain transfer multiple kernel learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(3):465­479, 2012. 1
[5] Mark Everingham and John Winn. The pascal visual object classes challenge 2012 development kit. Pattern Analysis, Statistical Modelling and Computational Learning, Tech. Rep, 2011. 4
[6] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In Proc. ICML, pages 1180­ 1189. PMLR, 2015. 2
[7] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Franc¸ois Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The Journal of Machine Learning Research, 17(1):2096­2030, 2016. 2
[8] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Proc. NeurIPS, pages 2672­2680, 2014. 1, 2
[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proc. CVPR, pages 770­778, 2016. 2
[10] Zhenwei He and Lei Zhang. Multi-adversarial faster-rcnn for unrestricted object detection. In Proc. ICCV, pages 6668­ 6677, 2019. 2, 4
[11] Brian Kulis, Kate Saenko, and Trevor Darrell. What you saw is not what you get: Domain adaptation using asymmetric kernel transforms. In CVPR 2011, pages 1785­1792. IEEE, 2011. 1
[12] Wanyi Li, Fuyu Li, Yongkang Luo, and Peng Wang. Deep domain adaptive object detection: a survey. arXiv preprint arXiv:2002.06797, 2020. 1

[13] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dolla´r. Focal loss for dense object detection. In Proc. ICCV, pages 2980­2988, 2017. 1
[14] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg. Ssd: Single shot multibox detector. In Proc. ECCV, pages 21­37. Springer, 2016. 1
[15] Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Unsupervised domain adaptation with residual transfer networks. In Proc. NeurIPS, pages 136­144, 2016. 1
[16] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In Proc. CVPR, pages 779­788, 2016. 1, 3
[17] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Proc. NeurIPS, pages 91­99, 2015. 1
[18] Kuniaki Saito, Yoshitaka Ushiku, Tatsuya Harada, and Kate Saenko. Strong-weak distribution alignment for adaptive object detection. In Proc. CVPR, pages 6956­6965, 2019. 2, 4
[19] Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Semantic foggy scene understanding with synthetic data. International Journal of Computer Vision, 126(9):973­992, 2018. 4
[20] Zhiqiang Shen, Mingyang Huang, Jianping Shi, Xiangyang Xue, and Thomas Huang. Towards instance-level image-toimage translation. In Proc. CVPR, 2019. 4
[21] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In Proc. CVPR, pages 7167­7176, 2017. 1
[22] Tao Wang, Xiaopeng Zhang, Li Yuan, and Jiashi Feng. Fewshot adaptive faster r-cnn. In Proc. CVPR, pages 7173­7182, 2019. 2, 4
[23] Fisher Yu, Wenqi Xian, Yingying Chen, Fangchen Liu, Mike Liao, Vashisht Madhavan, and Trevor Darrell. Bdd100k: A diverse driving video database with scalable annotation tooling. arXiv preprint arXiv:1805.04687, 2018. 4
[24] Xinge Zhu, Jiangmiao Pang, Ceyuan Yang, Jianping Shi, and Dahua Lin. Adapting object detectors via selective crossdomain alignment. In Proc. CVPR, pages 687­696, 2019. 2, 4

5

