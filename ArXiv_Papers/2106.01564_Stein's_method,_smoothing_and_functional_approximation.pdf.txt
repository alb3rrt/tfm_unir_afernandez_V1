STEIN'S METHOD, SMOOTHING AND FUNCTIONAL APPROXIMATION
A. D. Barbour, Nathan Ross, Guangqu Zheng Universit¨at Zu¨rich, University of Melbourne, University of Kansas

arXiv:2106.01564v1 [math.PR] 3 Jun 2021

Abstract
Stein's method for Gaussian process approximation can be used to bound the differences between the expectations of smooth functionals h of a c`adla`g random process X of interest and the expectations of the same functionals of a well understood target random process Z with continuous paths. Unfortunately, the class of smooth functionals for which this is easily possible is very restricted. Here, we prove an infinite dimensional Gaussian smoothing inequality, which enables the class of functionals to be greatly expanded -- examples are Lipschitz functionals with respect to the uniform metric, and indicators of arbitrary events -- in exchange for a loss of precision in the bounds. Our inequalities are expressed in terms of the smooth test function bound, an expectation of a functional of X that is closely related to classical tightness criteria, a similar expectation for Z, and, for the indicator of a set K, the probability P(Z  K \ K-) that the target process is close to the boundary of K.
Keywords: Weak convergence; rates of convergence; smoothing inequalities; Stein's method; Gaussian processes

1 INTRODUCTION
Stein's method [Stein, 1972, 1986] is a powerful method of obtaining explicit bounds on the distance between a probability distribution L(X) of interest and a well-understood approximating distribution L(Z). Here, "distance" is represented by a bound on the differences |Eh(X) - Eh(Z)|, for all functions in some class H of test functions:

dH(L(X), L(Z)) := sup E[h(X)] - E[h(Z)] .
hH
For example, if H is the class of Lipschitz functions h with

sup
x=y

|h(x) |x

- -

h(y)| y|

1,

the distance is the Wasserstein metric. The general method was treated in monograph form in [Stein, 1986], its application to approximation by the Poisson and normal distributions is described in the books [Barbour, Holst, and Janson, 1992] and [Chen, Goldstein, and Shao, 2011], respectively, and its many uses in combination with the Malliavin calculus are

1

presented in the monograph [Nourdin and Peccati, 2012]. Stein's method is not restricted to approximating the distributions of random variables, but can be used for multivariate distributions, as introduced in [Barbour, 1988] for the Poisson and [Go¨tze, 1991] for the normal, as well as for entire processes, as developed by [Barbour, 1988] and [Arratia, Goldstein, and Gordon, 1989] for Poisson processes and [Barbour, 1990] for Brownian motion.
A feature of Stein's method is that, in applications, there is often a class of functions H that is particularly well adapted for use with the method, resulting in a distance that is easily bounded. For normal approximation in one dimension, the family of bounded Lipschitz functions is typically amenable, leading to approximation with respect to a bounded Wasserstein distance. This distance is very natural in the context of weak convergence, but is not well suited for approximating tail probabilities, where the appropriate test functions are indicators of half lines, and hence are not Lipschitz. Nonetheless, by approximating the indicator functions above and below by Lipschitz functions with steep gradient, a bounded Wasserstein distance of  easily implies an approximation bound of order O(1/2) for the probability of a half line. Thus smoothing the indicator function, and then using the error bound for smooth functions, immediately results in bounds for the probabilities of half lines, albeit at the cost of an inferior rate of approximation. If better rates of approximation are required for tail probabilities, then (much) more work usually has to be done.
For process approximation by Brownian motion, there are classes of functions Mc0, c > 0, used in [Barbour, 1990] and [Kasprzak, 2020a,b], and given in (1.2) and (1.3) below, that are particularly well adapted for use with Stein's method. However, the classes are not rich enough to directly imply bounds for the distributions of functionals, such as the supremum, that have immediate practical application. This limits the usefulness of the results obtained. There is a quite separate approach, used in [Coutin and Decreusefond, 2013], [Besan¸con, Decreusefond, and Moyal, 2020] and [Bourguin and Campese, 2020], in which the problem is addressed on an abstract Gaussian space. Once again, there are analogous problems in translating the results into practical consequences. As an example, it would be advantageous to know that, if X belonged to the space D of c`adl`ag processes indexed by [0, 1] equipped with the Skorokhod topology, and if

Zc (X) :=

sup |Eh(X) - Eh(Z)|

hMc0 : h M0 1

were small, then differences of the form

Z(X, K) := P(X  K) - P(Z  K) ,

(1.1)

for K with P(Z  K) = 0, would also be small. Then, at least, if (Xn)n 1 were a sequence of processes in D for which Zc (Xn) converged to zero, this would imply that Xn converged weakly to Z, something that is shown only under some additional, mild assumptions in
[Barbour, 1990] and [Kasprzak, 2020a,b].
The aim of this note is to show how to use smoothing to obtain error bounds for the
distribution of rather general functionals of X, provided that a bound for functions in the class Mc0 is available. In addition to the value of Zc (X), the bounds involve some quantities that can be deduced from the properties of the limiting process Z, which, for Z(X, K), also involve the set K. In addition, they require an estimate of the uniform difference between
X and a smoothed version X of X, which, in asymptotic settings, can be thought of as a quantitative version of tightness.

2

1.1 Test functions

In order to state the main result, we need some further definitions. Let D := D [0, 1]; Rd be the set of functions from [0, 1] to Rd that are right continuous with left limits. The space D
endowed with the sup norm · is a Banach space (though not separable), and we denote the Fr´echet derivatives of functions h : D  R by Dh, D2h, . . ..
As in [Barbour, 1990] and [Kasprzak, 2020a], let M0 be the set of functions h : D  R
such that

h M0

:= sup|h(w)|+sup

wD

wD

Dh(w)

+ sup
wD

D2h(w)

+ sup
w,vD

D2h(w + v) - D2h(w) v

(1.2)

is finite, where we write A := supw: w =1|A[w, . . . , w]| for any k-linear form A. Letting It  D([0, 1]; R) be defined by It(u) := I[u t], and given c > 0, we define the subset Mc0 of M0 to consist of all functions h  M0 such that, for all r, s, t  [0, 1] and x1, x2  Rd,

sup D2h(w)[x1Ir, x2(Is - It)]
wD

c |x1| |x2| |s - t|1/2.

(1.3)

For  > 0 and a measurable set K  D, we define K := w : dist(w, K) <   K and K- := (Kc) c  K,

where dist(w, K) := inf{ w - v : v  K}. For w  D with w < , we can define the -regularized path w for  > 0 as follows:

w(s) := E[w(s + U)],

(1.4)

where U has the uniform distribution U(-1, 1) on the interval (-1, 1), and we define w(t) = w(1) for t > 1 and w(t) = w(0) for t < 0. Then, for h : D  R that is bounded and measurable with respect to the Skorokhod topology, and for any ,  > 0, we define an (, )-smoothed version of h by

h,(w) := E[h(w + B)],

(1.5)

where B is a standard d-dimensional Brownian motion on [0, 1], and w is defined as in (1.4).

As is shown in Lemma 1.6 and Remark 1.7, if supyD|h(y)|

1,

then

h,



M0 ()-2

for

any

positive  and , and if h is differentiable with Dh

1,

then

h,



M . 0 -2-1

Our main result is as follows.

Theorem 1.1. Let X, Z be random elements of D such that Z has almost surely continuous sample paths. Suppose that there are 1, 2 0 such that for any h  Mc0, we have

|Eh(X) - Eh(Z)| 1 h M0 + c 2.

(1.6)

Then, for any K  D that is measurable with respect to Skorokhod topology, and for any positive , , , , we have

P(X  K) - P(Z  K)

C, 1 + ()-22 + P X - X  + P

+

4de-

2 2d2

+ P(Z



K 2(+ )

\ K-2(+)),

Z - Z

 (1.7)

3

where

 C, := 1 + ()-1 + 2()-2 + 50/()-3 .

Furthermore, letting dLP denote the L´evy-Prokhorov metric, we have

dLP L(X), L(Z) max 2( + ), C, 1 + ()-22 + P X - X

 + P Z - Z



+

4de-

2 2d2

.

If h : D  R is bounded and Lipschitz with h 1 and Dh 1, then

E h(X) - h(Z) (1.8)
E X - X + E Z - Z + 2 E B + 2 -1 + 2-3-2 1 + -1/2-2-12. for any ,   (0, 1).

Remark 1.2.
1. The quantity |Eh(X) - Eh(Z)| with h  Mc0 can frequently be bounded effectively in the form (1.6), using Stein's method.

2. The bound (1.7) fits in well with weak convergence. Suppose that (Xn)n 1  D is a sequence of processes for which (1.6) holds with i = (in)  0 as n  , i = 1, 2, and write Xn, = (Xn). Then letting n   in (1.7), it follows that, for all , , ,  > 0
and for all measurable K  D,

lim sup P(Xn  K) - P(Z  K) n

lim sup P Xn, - Xn  + P( Z - Z ) n

+

4de-

2 2d2

+ P(Z



K 2(+ )

\ K-2(+)).

Now, since Z has almost surely continuous sample paths, Z - Z  0 a.s. as   0. Hence, letting  and  tend to zero for fixed , , and then letting  and  tend to zero, it follows that

lim sup P(Xn  K) - P(Z  K) n

lim sup lim sup lim sup P

0

0 n

Xn, - Xn

 + P(Z  K).

Thus lim supn P(Xn  K) - P(Z  K) = 0 for all K with P(Z  K) = 0, and hence Xn converges weakly to Z, provided that

lim sup lim sup P Xn, - Xn  = 0 for each  > 0. 0 n

(1.9)

Thus, with this condition in addition to (in)  0, i = 1, 2, it follows that Xn converges weakly to Z.

4

Now, by the definition of Xn,, Xn, - Xn MXn(), where

Mw() :=

sup |w(t) - w(s)|

0 s<t 1 : t-s<

denotes the modulus of continuity of w. It is well known that any sequence (Xn, n  N)  D that converges weakly to a limit with continuous sample paths satisfies the tightness condition

lim sup lim sup P MXn()  = 0 for all  > 0.

0

n

Hence, given (in)  0, i = 1, 2, the condition (1.9) is a necessary and sufficient condition for weak convergence to Z.

The main use for the bound given in Theorem 1.1 is to obtain explicit bounds on the error in approximating probabilities and expectations of functionals involving the process X by the corresponding values for the process Z. These follow from (1.7) and (1.8) by optimizing the choice of , , , . In the case of a sequence of processes indexed by n, rates of convergence can be deduced, as illustrated in Example 1.5 below. The following lemma provides a useful bound for probabilities of the form P( Y - Y ). It is a quantitative version of the classical condition of [Chentsov, 1956], and is proved using standard approximations from, for example, [Billingsley, 1999, Chapter 3].

Lemma 1.3. Let Y  D be a random process and let Ji(Y ) denote the size of the largest jump of its i-th component Y (i). If there are positive constants ,  and C such that, for any
0 r s t 1 and  > 0, and for each 1 i d, we have

P min |Y (i)(t) - Y (i)(s)|, |Y (i)(s) - Y (i)(r)| 

C-|t - r|1+,

(1.10)

then there is an explicit universal constant L such that

P( Y - Y )

d
P Ji(Y )



 2d1/2

+ C 2+1d

96 d 

 2L + 2 .

i=1

Remark 1.4. The following bound, using the inequality min{x, y} xy for x, y 0,

together with Markov's inequality, may be useful in practice: for any  > 0,

P min |Y (t) - Y (s)|, |Y (s) - Y (r)| 
Furthermore, if Z is a Gaussian process with E Z(v) - Z(u) 2

E |Y (t) - Y (s)|/2|Y (s) - Y (r)|/2



.

k|v - u| ,

(1.11)

for some positive constants k and  , then, for any  2,

E |Z(t) - Z(s)|/2|Z(s) - Z(r)|/2

E |Z(t) - Z(s)| E |Z(s) - Z(r)|

k/2E |G| |t - r|/2,

where G  N (0, 1) is standard normal. This is not the strongest possible moment bound under (1.11), but it is totally explicit. When applied in Lemma 1.3, it typically yields bounds of order no larger than those of the analogous term for X.

5

We are not aware of any general theory for bounding the final term P(Z  K \ K-), even for restricted classes of sets and continuous Gaussian processes. For finite dimensional Gaussian measures and convex sets, such enlargements have order  as   0; see, for example, [Ball, 1993] and [Go¨tze, Naumov, Spokoiny, and Ulyanov, 2019, Section 1.1.4]. For Gaussian processes with values in Hilbert space, there are some results when K is an open ball [Go¨tze et al., 2019]. That being said, for certain K and Z, it may nonetheless be possible to obtain quantitive results. For example, if Z is Brownian motion and g : D  R is a function that is Lipschitz on C[0, 1] and is such that g(Z) has a bounded density (an example of such a function is g(w) := sup0 s 1 ws), then for K = {w  D : g(w) y}, it is easy to see that
P(Z  K \ K-) c,
where c is a constant depending on the density bound and the Lipschitz constant of g. In such an example, Theorem 1.1 can be used to provide bounds on the Kolmogorov distance between L(g(X)) and L(g(Z)).
Theorem 1.1 and the discussion following should be compared to [Barbour, 1990, Theorem 2] and [Kasprzak, 2020a, Proposition 2.3], which give criteria for weak convergence assuming a bound of the form

E[h(Xn)] - E[h(Z)]

h M0 (n)

for all functions h in the larger class M0, which are not assumed to satisfy the smoothness
condition (1.3) (the statement in [Barbour, 1990, Theorem 2] is not correct, and the bound
must hold for functions without the smoothness condition). One advantage of working with a smaller class of functions Mc0 is that a discretization step can be avoided, which in turn can remove a log-term from the convergence rate; see [Barbour, 1990, Remark 2 and (2.29)].
More importantly, as will be shown in future work, applying Stein's method restricted to
test functions in the smaller class has wider applicability.

Example 1.5. As a proof of concept, we explore the quality of result that can be obtained

with Theorem 1.1 in the classical case where Xn(s) = n-1/2

ns i=1

Wi

with

W1, W2, . . . ,

real

i.i.d. random variables with mean zero, variance one and E |W1|/2 <  for some  4.

Donsker's theorem implies that the limit Z is standard Brownian motion. First, [Barbour,

1990, Theorem 1 and Remark 2] implies that, for  6, we can bound the first two terms

by

C,(1n) + ()-22(n) = O ()-3n-1/2 , as n   and ,   0. (1.12)

Noting that, for Brownian motion Z, (1.11) is satisfied with  = k = 1, Lemma 1.3 implies that, for any  > 2,

P Z - Z  = O (-2)/2- , as   0.

(1.13)

To bound P Xn, - Xn  , we appeal to (1.10) directly. If |t - r| < 1/n, then for r < s < t,
min |Xn(t) - Xn(s)|, |Xn(s) - Xn(r)| = 0,

6

since at least one term in the minimum must be zero. If |t - r| 1/n, then for  > 4, Rosenthal's inequality [Rosenthal, 1970, Theorem 3] implies that

E |Xn(t) - Xn(s)|/2

C

n(t - s) /4 n

2

 4

C (t

-

r)

 4

,

where C is a constant depending on  and E[|W1|/2]. We also have to consider the term corresponding to the size of the largest jump,



P(J (Xn)

/2)

=

P

max
1in

Wi

(/2) n

 n P |W1| (/2) n

n

E |W1|/2 (/2)/2n/4

= 2/2E

|W1|/2

n1-

 4

-/2

.

Altogether, we deduce from Lemma 1.3 with  = ( - 2)/2 > 0 that

P Xn, - Xn



=

O

(-2)/2-

+O

n1-

 4

-/2

,

where the implicit constant in the O depends on . Now, assuming that the set K is such that P(Z  K2(+) \ K-2(+)) c( + ), as,
for example, if K is the event that the maximum lies in some fixed interval, we deduce from (1.12), (1.13) and (1.7) that

P(Xn  K) - P(Z  K)

=O

()-3n-1/2

+

(-2)/2-

+

n1-

 4

-/2

+

4e-

2 22

+ O( + ),



as ,   0 and n  order O(n-5), which

. We first is negligible

choose  =  compared to

10 the

log n, so that the exponential first term. Now choose 4 =

term 1/(3

isno)f,

essentially to balance  with the first term above, and  = (-2)/{2(+1), to balance  with

the second term, to give

P(X  K) - P(Z  K)

= O -3/4n-1/8

-2
log n + O  2(+1)

+ O n1-/4-/2 .

Now approximately balance the first and second terms by choosing (-2)/{2(+1)}+3/4 = n-1/8,

leading to = n-(+1)/(2(5-1)),  = order O(n log n); note that, for 

n

:= n-(-2)/(4(5-1)) and to first and second terms 6, n n-1/20. The remaining term n1-/4-/2

of

n1-9/40 n-7/20 is thus much smaller than n. As a result, we have established a rate of

convergence of

P(Xn  K) - P(Z  K)

=O

n-

-2 4(5-1)

log n .

we

When  = can obtain

t6he(croartreesOponnd-i2n10g+ato lfiogninte

third

moments),

the

rate

is

O

n-

1 29

 log

n

,

for arbitrarily small a > 0, if we assume W1

and has

enough moments. This rate is still far from optimal, but, as is typical with Stein's method,

the power of the approach is that the method can be used in situations with non-trivial

dependencies, where rates of convergence are not otherwise available.

7

The key to proving Theorem 1.1 is the following lemma on Gaussian smoothing, for which we need the (, )-smoothing of h, defined in (1.5). The lemma is an infinite-dimensional analog of finite-dimensional Gaussian smoothing inequalities found, for example, in [Raic, 2018, Section 4.2].

Lemma 1.6. Let h : D  R be bounded and measurable, and let  and  be positive. Then h, is measurable with respect to the Skorokhod topology, and has infinitely many bounded Fr´echet derivatives satisfying

Dkh,

sup|h(y)| Ck()-k,
yD

where Ck is a constant depending only on k. Moreover,



h, M 0

sup|h(y)| 1 + ()-1 + 2()-2 +

yD

50/()-3 ,

(1.14)

and, for z, x  D,

D2h,(w)[z, x]

 2-2-1 z

sup|h(y)|
yD

1

1/2

|x(u)|2du .

0

(1.15)

If, in addition, h is continuous with Dnh <  for some n  N, then for any k = 0, 1, 2, . . .,

Dk+nh,

Dnh Ck-n-k-k,

(1.16)

and if Dh < , then for z, x  D,

D2h,(w)[z, x]

2/()-1 z Dh

1

1/2

|x(u)|2du .

0

(1.17)

Remark 1.7. For x1, x2  Rd and for functions z = x1Ir and x = x2(Is - It), as in (1.3), the inequality (1.15) implies the inequality

D2h,(w)[x1Ir, x2(Is - It)]

|x1| |x2| sup|h(y)| ()-2|t - s|1/2.
yD

This is because, with x(u) = x2(Is(u) - It(u)) = x2I[s

1
|x(u)|2 du =
0

|x2|2 42

1
I[s
0

u +  < t] - I[s

Similarly, if Dh < , then (1.17) implies

u < t],
2
u -  < t] du

|x2|2

(t - s) 22

.

D2h,(w)[x1Ir, x2(Is - It)]

-1/2|x1| |x2| Dh -2-1|t - s|1/2.

An expression for the Fr´echet derivatives and bounds can be found at (2.3) and (2.10). They are not complicated, but require some set-partition notation, stemming from Fa`a di Bruno's formula for the derivatives of an exponential. The proof begins with the easy fact that w is absolutely continuous, and is thus in the Cameron-Martin space of Brownian motion. We can then use Girsanov's theorem to write h,(w + x) - h,(w) as a single expectation with respect to Brownian motion which, roughly speaking, is smooth in x due to the change of measure formula.

8

2 PROOFS
Let us first recall the Cameron­Martin­Girsanov theorem [Cameron and Martin, 1944].

Theorem 2.1. Let B(t) : t  [0, 1] be a standard d-dimensional Brownian motion and g = (g(1), ..., g(d)) : [0, 1]  Rd be a deterministic, absolutely continuous function such that

1
|g(t)|2dt =
0

1d 0 i=1

d dt

g(i)(t)

2
dt

<

.

Then, for any real bounded measurable function h defined on the path space C([0, 1]; Rd), we have

E h(B + g) = E h(B) exp

1 0

g(t)

dB(t)

-

1 2

1
|g(t)|2 dt
0

.

(2.1)

Note that the Wiener integral

1

d

1

g(t) dB(t) :=

0

i=1 0

d dt

g(i)(t)

dB(i)(t)

is normally distributed with mean zero and variance

1 0

|g(t)|2

dt.

The proof of

Theorem

2.1

follows from the real case, using the independence between the different components B(i)

of B. In our setting, the -regularised path w plays the role of g in (2.1).

Proof of Lemma 1.6. To establish the measurability of h,, note that x  x is continuous (and hence measurable with respect to the Skorohod topology) and then that (x, y)  h(x + y) is measurable with respect to the product topology. Therefore h, is also measurable.
We first give an informal computation to indicate where the formulas below come from. Note that for w  D, w is absolutely continuous from [0, 1] to Rd and

w

=

1 2

w(· + ) - w(· - )

1 

w

.

(2.2)

Thus we can apply Girsanov's formula (2.1) to write

h,(w) = E h(B) exp((w)) ,

where (w) is a random element given by

(w) :=

1 

1 0

w(t)

dB(t)

-

1 22

1
|w(s)|2 ds.
0

Now, informally, we ought to have

Dkh,(w)[x1, . . . , xk] = E h(B)Dk exp((w))[x1, . . . , xk] ,

9

and then Dk exp((w))[x1, . . . , xk] can be understood as exp((w)) times a polynomial of the derivatives of (w), based on Fa`a di Bruno's formula. Looking at the expression

(w + v) - (w) =

1 

1
v(t) d B(t) - -1w(t)
0

-

1 22

1
|v(t)|2 dt,
0

it follows that

D(w)[v] =

1 

1
v(s) d B(s) - -1w(s) ,
0

whose dependence on w is linear. Then it is also easy to see that

D2(w)[x, y]

=

-

1 2

1
x(s), y(s) ds
0

and

Dk(w)[x1, . . . , xk] = 0, k

3,

where · denotes the inner product on Rd; these higher derivatives are no longer random.
The derivatives only depend on w through (B - -1w), which under the Girsanov change of measure e(w) is distributed as B. Altogether, this leads to the following claim:







Dnh,(w)[x1, ..., xn] = E h(w + B) 

D|b|[xb] ,

Pn,2 b

(2.3)

where

· Pn,2 is the set of all partitions of {1, ..., n} whose blocks have at most 2 elements, · b   means that b is a block of  whose cardinality is denoted by |b|,

· we define

D1[x]

=

1 

1
x(t) dB(t)
0

and

D2[x, y]

=

-

1 2

1
x(s), y(s) ds;
0

· if b = {i1, ..., i|b|}, the expression D|b|[xb] means D|b|[xi1, ..., xi|b|].

For its proof, we also need the equivalent formulation of the right hand side of (2.3), using

the change of measure formula, given by







Dnh,(w)[x1, ..., xn] = E h(B)e(w) 

D|b|(w)[xb] ,

Pn,2 b

(2.4)

where D(w) and D2(w) are as before.
With the convention that the sum of products over Pn,2 for n = 0 is equal to one, both formulas (2.3) and (2.4) are obviously true for n = 0. Now assume that they are true for all n k; we want to show that they hold also for n = k + 1. For z  D([0, 1]; Rd),

Dkh,(w + z)[x1, ..., xk] - Dkh,(w)[x1, ..., xk]

10

=

E h(B) e(w+z) - e(w)

D|b|(w + z)[xb]

Pk,2

b

(2.5)

+

E h(B)e(w)

D|b|(w + z)[xb] - D|b|(w)[xb] .

Pk,2

b

b

(2.6)

Now, from the definitions of  and D, and from (2.2), we have

(w + z) - (w) - D(w)[z]

1 22

1
|z(s)|2 ds
0

and

1

1

y(s) dB(s)  N 0, |y(s)|2 ds .

0

0

It is thus straightforward to see from Taylor's expansion that

z2 222

,

(2.7)

e(w+z) - e(w) = e(w)D(w)[z] + O( z 2),

where the linear-in-z term e(w)D(w)[z] = O( z ). Here and in what follows, the big-O is to be understood in the Lp()-sense; for example, T (z) = O( z ) means that T (z) is a random variable that depends on z and (E|T |p)1/p = O( z ), in the usual sense of O, for
any p  [2, ). We also note that D2(w)[x, y] does not depend on w and that

D(w + z)[xi] = D(w)[xi] + D2(w)[xi, z],
with D2(w)[xi, z] = O( z ), in the usual sense of O. In view of these facts, we can write the first sum (2.5) as

(2.8)

E h(B)e(w)D(w)[z] D|b|(w)[xb] + O( z 2).

Pk,2

b

(2.9)

Concerning the second sum (2.6), the difference of expectations vanishes if   Pk,2 is a partition with all blocks having exactly 2 elements, since D2(y) does not depend on y.
Suppose now that the partition   Pk,2 contains  blocks with exactly one element, for some   {1, ..., k} (say the blocks {1}, ..., {}). Then, using (2.8), it follows that

D|b|(w + z)[xb] - D|b|(w)[xb]

b

b 



=

1{|b|=2}D2(w)[xb]  D2(w)[z, xj]

 D(w)[xi] + O( z 2).

b

j=1

i{1,...,}\{j}

11

As a consequence, we can write the second sum (2.6) as

E h(B)e(w) D|b|(w)[xb] + O( z 2),

Pk +1,2

b

where Pk +1,2 is the set of all partitions of {1, ..., k, k + 1} (xk+1 = z) whose blocks have at most 2 elements, and such that k + 1 (that corresponds to z) belongs to a block of size 2. Since the first term (2.9) accounts for the partitions where k + 1 is in a block of size 1, we have established formula (2.4) for n = k + 1, and hence for all n.
For the bounds on the derivative, we assume without loss of generality that supyD|h(y)| = 1. Using (2.7), (2.2) and (2.3), we easily have that

DkE[h(w + B)] ()-kE |G|k × card(Pk,2).

(2.10)

So the constant Ck can be chosen to be E |G|k × card(Pk,2) and therefore C1 1, C2 = 3. Finally, we can do better than the crude bound of (2.10) for k = 2, 3, to bound h, M0.
Using the bound (2.10) directly implies that

Dh,(w)[x]

()-1 x ,

(2.11)

and so

sup Dh,(w)
wD

()-1.

(2.12)

Using the formula (2.3) with n = 2 implies that

1

1

D2h,(w)[x, y] = -2 E h(w + B)

x(s) dB(s)

y(s) dB(s)

0

0

1

- -2 E h(w + B)

x(s), y(s) ds

0

1

1

= -2 Cov h(w + B),

x(s) dB(s)

y(s) dB(s) . (2.13)

0

0

From this, it easily follows from Cauchy­Schwarz that

D2h,(w)[x, y]

-2 Var 
2()-2 x

1
x(s) dB(s)
0
y,

1
y(s) dB(s)
0

(2.14)

 where the constant 2 arises because, for (U, V ) bivariate normal with mean zero, variances 1 and correlation , Var(UV ) = 1 + 2 2. Thus, immediately,

sup D2h,(w)
wD

 2()-2.

12

For the Lipschitz constant of the second derivative, we show that

sup
wD

D2h,(w + v) - D2h,(w) v

50 

()-3.

(2.15)

Using the definition of the derivative and (2.3), we have

D2h,(w + x3)[x1, x2] - D2h,(w)[x1, x2]

1
D3h,(w + tx3)[x1, x2, x3] dt

0

3

3

-3 E

D1[xi]D2 (xj)j=i + E

D1[xj ]

i=1

j=1

3

3

3E|G| + E|G|3 ()-3 xi = 50/ ()-3 xi ,

i=1

i=1

(2.16)

and this implies (2.15). It remains to verify the inequality (1.15). This follows simply from the Cauchy­Schwarz inequality and (2.13) in almost the same way as in the calculations leading to (2.14), but using (2.7) for the final bound.
Finally we show the bound (1.16), under the additional assumption that Dnh < . First, we note that the sum inside the expectation in (2.3) does not depend on w, and we denote it by Tn. We now show by induction that, for 0 r n,

Dk+rh,(w)[x1, . . . , xk, z1, . . . , zr] = E Drh(w + B)[z1,, . . . , zr,]Tk ,

(2.17)

where zj, = (zj) is defined according to (1.4), the case r = 0 being just (2.3). Then, assuming that (2.17) is true for r,

Dk+r+1h,(w)[x1, . . . , xk, z1, . . . , zr, v]

=

d dt

Dk+rh,(w + tv)[x1, . . . , xk, z1, . . . , zr]
t=0

=

d dt

E
t=0

Drh(w + tv + B)[z1,, . . . , zr,]Tk

.

For r < n, since Dnh < ,

sup Dr+1h(w + y + B)
y: y 1

is bounded by a polynomial of degree n - r - 1 in  B , and hence its product with |Tk| is integrable, by Fernique's theorem. Hence, by dominated convergence, we deduce that

d dt

E
t=0

Drh(w + tv

+ B)[z1,, . . . , zr,]Tk

= E Dr+1h(w + B)[z1,, . . . , zr,, v]Tk ,

establishing (2.17) for r + 1 also. The bound (1.16) follows from (2.17) with r = n, using (2.7) and (2.2), as in proving (2.10). Finally, the inequality (1.17) follows from (2.17) with k = r = 1, and using (2.7).

13

Proof of Theorem 1.1. First note that, if h : D  R is the indicator of a measurable set and

,



are

positive,

then

Lemma

1.6

and

Remark

1.7

imply

that

h,



M0 ()-2

,

and

that

h, M 0

 1 + ()-1 + 2()-2 + 50/()-3 =: C,.

(2.18)

Next, we upper and lower bound P(X  K) - P (Z  K) for K a measurable subset of D. For any ,  > 0, we have

P(X  K)

P(X + B  K+, X + B - X <  + ) + P( X + B - X P(X + B  K+) - P(Z + B  K+)
+ P( X + B - X  + ) + P(Z + B  K+).

 + ) (2.19) (2.20)

The first term (2.19) is of the form E h,(X) - E h,(Z) for h as above, and is thus upper bounded by C,1 + ()-22. The first part of the second term (2.20) is upper bounded by

P( X + B - X

 + )

P( X - X P( X - X

) + P( B ) ) + 2de-2/(2d2);

in the second inequality we have used the bound

PB z

d
P

B(i)

z

i=1

d

d P B(1)

z

d

2d P max B(1)(t) zd-1/2 0t1

4d P B(1)(1) zd-1/2

2d exp

- z2 2d

,

for any z > 0, which follows from well known facts about Brownian motion. For the second term of (2.20), we have

P(Z + B  K+)

P( Z + B - Z  + ) + P(Z  K2(+))

P( Z - Z ) + P( B ) + P(Z  K2(+))

P( Z - Z

) + P(Z  K2(+)) + 2de-2/(2d2).

Combining the last displays, we find that

(2.21)

P(X  K)

C,1 + ()-22 + P( X - X )

+ P( Z - Z

)

+

P(Z



K 2(+ ) )

+

4de-

2 2d2

.

Subtracting P(Z  K2(+)) from both sides of this inequality implies the bound on the
L´evy­Prokhorov distance, and subtracting P(Z  K) from both sides gives an upper bound on P(X  K) - P(Z  K) of the form of the bound of Theorem 1.1, with P(Z  K2(+) \ K) in place of P(Z  K2(+) \ K-2(+)). A lower bound of the same magnitude follows in
analogous fashion.

Finally, suppose that h : D  R is bounded and Lipschitz with supyD|h(y)| 1 and

Dh

1.

Then

for

any

, 

>

0,

Lemma

1.6

and

Remark

1.7

imply

that

h,



M , 0  -1/2 -2 -1

and we also have

E h(X) - h(Z) = E h(X) - h(X + B) + E h,(X) - h,(Z) + E h(Z + B) - h(Z) .

14

The first expectation is bounded by E X - X +  B , and the third by E Z - Z +  B ; the second is bounded by 1 h, M0 + -1/2-2-12, by assumption. Applying (1.16) of Lemma 1.6 with h 1 and Dh 1, we find

h,

1;

Dh,

-1;

D2h,

Dh C1-2-1

-2-1,

while we deduce from (2.16) and (1.16) that, for xi 1 (i = 1, 2, 3)

D2h,(w + x3)[x1, x2] - D2h,(w)[x1, x2]

D3h,

Dh C2-3-2

3-3-2.

Thus, if ,   (0, 1), we see that

h, M 0

1 + -1 + -2-1 + 3-3-2

2-1 + 4-3-2.

Altogether, the above bounds lead us to the desired estimate (1.8).
Proof of Lemma 1.3. Letting Mw() denote the modulus of continuity of a function w  D at level , we can bound

P Y - Y 

d
P
i=1

Y(i) - Y (i)

 d1/2

d
P(MY (i)()
i=1

d-1/2).

Thus, from now on, it suffices to consider the case where d = 1. Now, defining

MY () := sup min |Y (t) - Y (s)|, |Y (s) - Y (r)| , |t-r|< 0 r<s<t 1

we have from (12.9) and (12.32) of [Billingsley, 1999] that

MY () J(Y ) + 48 max MY (2), |Y (2) - Y (0)|, |Y (1-) - Y (1 - 2)| . Thus we can bound

P MY () 

P(J(Y ) /2) + P(MY (2) /96) + P(|Y (2) - Y (0)| /96) + P(|Y (1-) - Y (1 - 2)| /96).

The third term can be bounded by using (1.10) with t - r = 2 and  = /96:

P |Y (2) - Y (0)| /96

C 21+ 1+

96 


.

Similarly for the fourth term: with 1 - 2 < tn < 1 such that tn  1,

P(|Y (1-) - Y (1 - 2)|

/96)

lim sup lim inf P 0 n+

|Y (tn) - Y (1 - 2)| > (/96) - 

lim sup
0

lim inf
n+

C (tn

-

1

+

2)1+2

 96

-



-

15

= C21+1+

96 


.

For the second term, from the proof of Theorem 13.5 of [Billingsley, 1999], (1.10) implies

that there is an explicit constant L (read from the proof of Theorem 10.3 of [Billingsley,

1999]), such that

P(MY (2) )

2C L 

(4).

Applying this with  = /96 yields the final bound.

REFERENCES
R. Arratia, L. Goldstein, and L. Gordon, 1989. Two moments suffice for Poisson approximations: the Chen-Stein method. Ann. Probab., 17(1):9­25.
K. Ball, 1993. The reverse isoperimetric problem for Gaussian measure. Discrete Comput. Geom., 10(4):411­420.
A. D. Barbour, 1988. Stein's method and Poisson process convergence. J. Appl. Probab., (Special Vol. 25A):175­184. A celebration of applied probability.
A. D. Barbour, 1990. Stein's method for diffusion approximations. Probab. Theory Related Fields, 84(3):297­322.
A. D. Barbour, L. Holst, and S. Janson, 1992. Poisson approximation, volume 2 of Oxford Studies in Probability. Oxford University Press, Oxford.
E. Besan¸con, L. Decreusefond, and P. Moyal, 2020. Stein's method for diffusive limits of queueing processes. Queueing Syst., 95(3-4):173­201.
P. Billingsley, 1999. Convergence of probability measures. Wiley Series in Probability and Statistics: Probability and Statistics. John Wiley & Sons, Inc., New York, second edition. A Wiley-Interscience Publication.
S. Bourguin and S. Campese, 2020. Approximation of Hilbert-valued Gaussians on Dirichlet structures. Electron. J. Probab., 25:Paper No. 150, 30.
R. H. Cameron and W. T. Martin, 1944. Transformations of Wiener integrals under translations. Ann. of Math. (2), 45:386­396.
L. H. Y. Chen, L. Goldstein, and Q.-M. Shao, 2011. Normal approximation by Stein's method. Probability and its Applications (New York). Springer, Heidelberg.
N. N. Chentsov, 1956. Weak convergence of stochastic processes whose trajectories have no discontinuities of the second kind and the "heuristic" approach to the kolmogorov-smirnov tests. Theory Probab. Appl., 1(1):140­144.
L. Coutin and L. Decreusefond, 2013. Stein's method for Brownian approximations. Commun. Stoch. Anal., 7(3):349­372.
16

F. Go¨tze, 1991. On the rate of convergence in the multivariate CLT. Ann. Probab., 19(2): 724­739.
F. Go¨tze, A. Naumov, V. Spokoiny, and V. Ulyanov, 2019. Large ball probabilities, Gaussian comparison and anti-concentration. Bernoulli, 25(4A):2538­2563.
M. J. Kasprzak, 2020a. Stein's method for multivariate Brownian approximations of sums under dependence. Stochastic Process. Appl., 130(8):4927­4967.
M. J. Kasprzak, 2020b. Functional approximations via Stein's method of exchangeable pairs. Ann. Inst. Henri Poincar´e Probab. Stat., 56(4):2540­2564.
I. Nourdin and G. Peccati, 2012. Normal approximations with Malliavin calculus, volume 192 of Cambridge Tracts in Mathematics. Cambridge University Press, Cambridge. From Stein's method to universality.
M. Raic, 2018. A multivariate central limit theorem for Lipschitz and smooth test functions. Preprint https://arxiv.org/abs/1812.08268.
H. P. Rosenthal, 1970. On the subspaces of Lp (p > 2) spanned by sequences of independent random variables. Israel J. Math., 8:273­303.
C. Stein, 1972. A bound for the error in the normal approximation to the distribution of a sum of dependent random variables. In Proceedings of the Sixth Berkeley Symposium on Mathematical Statistics and Probability (Univ. California, Berkeley, Calif., 1970/1971), Vol. II: Probability theory, pages 583­602. Univ. California Press, Berkeley, Calif.
C. Stein, 1986. Approximate computation of expectations. Institute of Mathematical Statistics Lecture Notes--Monograph Series, 7. Institute of Mathematical Statistics, Hayward, CA.
17

