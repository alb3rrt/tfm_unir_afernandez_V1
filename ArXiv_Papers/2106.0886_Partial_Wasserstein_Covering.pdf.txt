arXiv:2106.00886v1 [cs.LG] 2 Jun 2021

Partial Wasserstein Covering
Keisuke Kawano Satoshi Koide Keisuke Otaki Toyota Central R&D Labs., Inc. Nagakute, Aichi, Japan
{kawano, koide, otaki}@mosk.tytlabs.co.jp
Abstract
We consider a general task called partial Wasserstein covering with the goal of emulating a large dataset (e.g., application dataset) using a small dataset (e.g., development dataset) in terms of the empirical distribution by selecting a small subset from a candidate dataset and adding it to the small dataset. We model this task as a discrete optimization problem with partial Wasserstein divergence as an objective function. Although this problem is NP-hard, we prove that it has the submodular property, allowing us to use a greedy algorithm with a 0.63 approximation. However, the greedy algorithm is still inefficient because it requires linear programming for each objective function evaluation. To overcome this difficulty, we propose quasi-greedy algorithms for acceleration, which consist of a series of techniques such as sensitivity analysis based on strong duality and the socalled C-transform in the optimal transport field. Experimentally, we demonstrate that we can efficiently make two datasets similar in terms of partial Wasserstein divergence, including driving scene datasets.
1 Introduction
A major challenge in real-world machine learning applications is to cope with mismatches between the data distributions used for development and actual applications. If there are regions in the development data distribution with densities lower than those in the application dataset, we have potential risks such as a lack of evaluation or high generalization error in such regions. Our motivation is to resolve issues related to low data density by adding additional data to development datasets. However, the limited budget for labeling new data allows us to add only a limited number of data.
Our research question is formulated as follows. To resolve a lack of data density in development datasets, how and with which metric can we select data from a candidate dataset with a limited budget? One reasonable approach is to define the discrepancy between data distributions and select data to minimize this discrepancy. Wasserstein distance has attracted significant attention as a metric for data distributions [1, 2]. However, the Wasserstein distance is not capable of representing the asymmetric relationship between application datasets and development datasets; our interest is on areas with densities in the development dataset lower than those in the the application dataset, whereas we disregard areas with higher densities.
In this paper, we propose partial Wasserstein covering (PWC), which selects a small number of data from candidates by minimizing the partial Wasserstein divergence [3] between an application dataset and the union of the development dataset and the selected data. An important feature of partial Wasserstein divergence is asymmetry (i.e., it is sensitive to areas with densities in the development dataset lower than those in the application dataset; but not sensitive for the opposite direction). The concept of PWC is illustrated in Fig. 1, where PWC selects data from the application dataset to fill the gaps between the two distributions. As shown in Fig. 1, PWC selects data from areas with fewer development data than application data in the data distributions (lower-right area in the blue
Preprint. Under review.

Original data distribution

Partial Wasserstein covering

Anomaly detection (LOF)

Application dataset

Development dataset

Selected data

Figure 1: Concept of PWC. PWC extracts some data from an application dataset by minimizing the partial Wasserstein divergence between the application dataset and the union of the selected data and a development dataset. PWC focuses on regions in which development data are lacking compared to the application dataset, whereas anomaly detection extracts irregular data.

distribution in Fig. 1), while ignoring areas with sufficient development data (upper-middle of the orange distribution). We also highlight the data selected by an anomaly detection method (LOF, [4]), where irregular data (upper-right points) are selected, but major density difference is ignored.
Our main contributions can be summarized as follows.
· We propose PWC, which extracts data that are lacking in a development dataset from an application dataset by minimizing the partial Wasserstein divergence between an application dataset and development dataset.
· We prove that PWC is a maximization problem involving a submodular function whose inputs are the set of selected data. This allows us to use a greedy algorithm with a guaranteed approximation ratio. Additionally, we propose fast heuristics based on sensitivity analysis and the Sinkhorn derivative for acceleration of computation.
· Experimentally, we demonstrate that PWC extracts data that are lacking in a development data distribution from an application distribution more efficiently than several baselines.
The remainder of this paper is organized as follows. Section 2 briefly introduces the partial Wasserstein divergence, linear programming (LP), and submodular functions. In Sect. 3, we detail the proposed PWC, and fast algorithms for approximating the optimization problem, as well as some theoretical results. Section 4 presents a literature review of related work. We demonstrate PWC in some numerical experiments in Sect. 5 and conclude this paper in Sect. 6.
2 Preliminaries
In this section, we introduce the notations used throughout this paper. We then describe partial Wasserstein divergences, sensitivity analysis for LP, and submodular functions.
Notations Vectors and matrices are denoted in bold (e.g., x and A), where xi and Aij denote the ith and (i, j)th elements, respectively. To clarify the elements, we use notations such as x = (f (i))i and A = (g(i, j))ij, where f and g specify the element values depending on their subscripts. ·, ·
denotes the inner product of matrices or vectors. 1n  Rn is a vector with all elements equal to one.
For a natural number n, we define [[n]] := {1, · · · , n}. For a finite set V , we denote its power set as 2V and its cardinality as |V |. The L2-norm is denoted as · . The delta function is denoted as (·). R+ is a set of real positive numbers and [a, b]  R denotes a closed interval. I[·] denotes an indicator function (zero for false and one for true).
Partial Wasserstein divergence In this paper, we consider partial Wasserstein divergence [5, 3, 6] as an objective function.1 Partial Wasserstein divergence is designed to measure the discrepancy between two distributions with different total masses by considering a variation of optimal transport. Throughout this paper, datasets are modeled as empirical distributions that are represented by mixtures of delta functions without necessarily having the same total mass. Suppose two empirical distributions
1Partial optimal transport problems in the literature contains a wider problem definition than Eq.(1) as summarized in Table 1 (b) of [3], but this paper employs this one-side relaxed Wasserstein divergence corresponding to Table 1 (c) in [3] without loss of generality.
2

(or datasets) X and Y with probability masses a  Rn+ and b  Rm + , which are respectively denoted

as X =

n i=1

ai(x(i))

and

Y

=

m j=1

bj (y(j)).

Without

loss

of

generality,

the

total

mass

of

Y

is greater than or equal to that of X (i.e.,

n i=1

ai

=

1

and

m j=1

bj



1).

Based on the definitions above, we define the partial Wasserstein divergence as follows:

PW2(X, Y ) := min P, C , where U (a, b) = {P  [0, 1]n×m | P1m = a, P 1n  b}, (1) PU (a,b)

where Cij := x(i) - y(j) 2 is the transport cost between x(i) and y(j), and Pij is the amount of mass flowing from x(i) to y(j) (to be optimized). Unlike standard Wasserstein distance, the second constraint in Eq. (1) is not defined with "=", but with "". This modification allows us to treat
distributions with different total masses. The condition P 1n  b indicates that the mass in Y does not need to be transported, while the condition P1m = a indicates that all of the mass in X
should be transported without excess or deficiency (just as in the original Wasserstein distance). This
property is useful for the problem defined below, which treats datasets with vastly different sizes.

To compute the partial Wasserstein divergence, we must solve the minimization problem in Eq.(1). In this paper, we consider the following two methods. (i) LP with the simplex method. (ii) Generalized Sinkhorn iteration with entropy regularization (with a small regularization parameter  > 0) [7, 8].
As will be detailed in Sect. 3, an element in the mass b varies when adding data to development datasets. A key to our algorithm is quickly estimating how much the partial Wasserstein divergence will change when an element in b varies. If we compute PW2 using LP, we can employ sensitivity analysis, which will be described in the following paragraph. If we use generalized Sinkhorn iterations, we can use automatic differentiation techniques to obtain a partial derivative w.r.t. bj (see Sect. 3.3).

LP and sensitivity analysis The sensitivity analysis of LP plays an important role in our algorithm. Given a variable x  Rm and parameters c  Rm, b  Rn, and A  Rn×m, the standard form of LP can be written as follows: min c x, s.t. Ax  b, x  0. Sensitivity analysis is a framework for estimating changes in a solution when the parameters c, A, and b of the problem vary. We consider sensitivity analysis for the right-hand side of the constraint (i.e., b). When bj changes as bj + bj, the optimal value changes by yjbj if a change bj in bj lies within (bj, bj), where y is the optimal solution of the following dual problem corresponding to the primal problem: max b y s.t. A y  c, y  0. We refer readers to [9] for the details of calculating the upper
bound bj and lower bound bi.
Submodular function Our covering problem is modeled as a discrete optimization problem involving submodular functions, which are a subclass of set functions that play an important role in discrete optimization. A set function  : 2V  R is called submodular iff (S  T ) + (S  T )  (S) + (T ) (S, T  V ). A submodular function is monotone iff (S)  (T ) for S  T . An important property of monotone submodular functions is that a greedy algorithm provides a (1 - 1/e)  0.63-approximate solution for the maximization problem under a budget constraint |S|  K [10]. The contraction ~ : 2V  R of a (monotone) submodular function , which is defined as ~(S) := (S  T ) - (T ), where T  V , is also a (monotone) submodular function.

3 Partial Wasserstein covering problem

3.1 Formulation

As discussed in the introduction, our goal is to emulate a large dataset Dapp by adding some data from a candidate dataset Dcand to a small dataset Ddev2. We model this task as a discrete optimization problem called the partial Wasserstein covering problem.

Given

a

dataset

for

development

Ddev

=

{y(j

)

}Ndev
j=1

,

a

dataset

obtained

from

an

application

Dapp

=

{x(i)

}Napp
i=1

,

and

a

dataset

containing

candidates

for

selection

Dcand

=

{s(j

)

}Ncand
j=1

,

where

Napp



Ndev,

2We will consider Dcand = Dapp eventually, but we herein consider the most general formulation.

3

the PWC problem is defined as the following optimization problem3:

max -PW2(Dapp, S  Ddev) + PW2(Dapp, Ddev), s.t. |S|  K.

(2)

SDcand

We select a subset S from the candidate dataset Dcand under the budget constraint |S|  K( Ncand) and then add that subset to the development data Ddev to minimize the partial Wasserstein divergence between the two datasets Dapp and S  Ddev. The second term is a constant w.r.t. S, which is included to make the objective non-negative.

In Eq. (2), we must specify the probability mass (i.e., a and b in Eq. (1)) for each data point. Here, we
employ a uniform mass, which is a natural choice because we do not have prior information regarding
each data point. Specifically, we set the weights to ai = 1/Napp for Dapp and bj = 1/Ndev for SDdev. With this choice of masses, we can easily show that PW2(Dapp, S  Ddev) = W2(Dapp, Ddev) when S = , where W2(Dapp, Ddev) is the Wasserstein distance. Therefore, based on the monotone property (Sect. 3.2), the objective value is positive for any S.

The PWC problem in Eq.(2) can be written as a mixed integer program (MIP) as follows. Instead
of using the divergence between Dapp and S  Ddev, we consider the divergence between Dapp and Dcand  Ddev. Hence, the mass b is an Ncand + Ndev-dimensional vector, where the first Ncand elements correspond to the data in Dcand and the remaining elements correspond to data in Ddev. In this case, we never transport to data points in Dcand \ S, meaning we use the following mass that depends on S:

bj(S) =

, I [s(j ) S ]
Ndev
, 1
Ndev

if 1  j  Ncand if Ncand + 1  j  Ncand + Ndev.

(3)

As a result, the problem is a mixed-integer linear programming (MIP) problem with an objective function P, C w.r.t. S  Dcand and P  U (a, b(S)) with |S|  K and ai = 1/Napp.
One may wonder why we use the partial Wasserstein divergence in Eq.(2) instead of the standard Wasserstein distance. This is because the asymmetry in the conditions of the partial Wasserstein divergence (described in Sect. 2) enables us to extract only the parts with a density lower than that of the application dataset, whereas the standard Wasserstein distance becomes large for parts with sufficient density in the development dataset.

3.2 Submodularity of the PWC problem

In this section, we prove the following theorem to guarantee the approximation ratio of our proposed algorithms, which are introduced in Section 3.3, for the PWC problem.
Theorem 1. Given datasets X = {x(i)}Ni=x1 and Y = {y(j)}Nj=y1, and a subset of a dataset S  {s(j)}Nj=s1, (S) = -PW2(X, S  Y ) + PW2(X, Y ) is a monotone submodular function.
To prove Theorem 1, we reduce our problem to the partial maximum weight matching problem [11], which is known to be submodular. First, we present the following lemmas.
Lemma 1. Let Q be the set of all rational numbers, and m and n be positive integers with n  m. Consider A  Qm×n and b  Qm. Then, the extreme points x of a convex polytope defined by the linear inequalities Ax  b are also rational, meaning x  Qn.

Proof of Lemma 1. It is well known [9] that any extreme point of the polytope is obtained by (i) choosing n inequalities from Ax  b and (ii) solving the corresponding n-variable linear system Bx = b~, where B and b~ are the corresponding sub-matrix/vector (when replacing  with =). As all elements in B-1 and b~ are rational, 4 we conclude that any extreme points are also rational.
Lemma 2. Let l, m, and n be positive integers and Z = [[n]]. Given a positive-valued m-by-n matrix R > 0, the following set function  : 2Z  R is a submodular function:

3We herein consider the partial Wasserstein divergence between the unlabled datasets, because the application

and candidate datasets are usually unlabeled. If they are labeled, we can use the labels information as in [2].

4This immediately follows from the fact that B-1 = B~ / det B, where B~ is the adjugate matrix of B and

det

B

is

the

determinant

of

B.

Since

B



n×n
Q

,

B~

and

det

B

are

both

rational

by

definition.

4

(S) =

max

R, P ,

PU (1m/m,b(S))

where

bj (S )

=

I [j S ] l

j  [[n]].

Here, U (a, b(S)) is a set defined by replacing the constraint P1m = a in Eq. (1) with P1ma.

Proof of Lemma 2. For any fixed S  Z, there is an optimal solution P of (S), where all

elements Pij are rational because all the coefficients in the constraints are in Q ( Lemma 1). Thus,

there

is

a

natural

number

M



Z+

that satisfies

(S)

=

1 mlM

maxPUQ(S)

R, P

where the set

UQ(S) is the set of possible transports when the transport mass is restricted to the rational numbers

Q (i.e., UQ(S) = P  Zm +×n | P1n  lM · 1m, P 1m  mM · I[j  S] · 1n ).

The problem of (S) can be written as a network flow problem on a bipartite network following

Sect 3.4 in [8]. Since the elements of P are integers, we can reformulate the problem of (S)

using maximum-weight bipartite matching (i.e., an assignment problem) on a bipartite graph whose

vertices are generated by copying the vertices lM and mM times with modified edge weights.

Therefore, G(V  W, E) is a bipartite graph, where V = {v1,1, . . . , v1,lM , . . . , vm,1, . . . , vm,lM }

and W = {w1,1, . . . , w1,mM , . . . , wn,1, . . . , wn,mM }. For any i  [[n]], j  [[m]], the transport mass

Pij  Z+ is encoded by using the copied vertices {vi,1, . . . , vi,lM } and {wj,1, . . . , wj,mM }, and the

weights

of

the

edges

among

them

are

1 mlM

Rij

>

0

to

represent

Rij .

The

partial

maximum

weight

matching function for a graph with positive edge weights is a submodular function [11]. Therefore,

(S) is a submodular function.

Lemma 3. If |S|  l, there exists an optimal solution P in the maximization problem of  satisfying

P1n

=

1m
m

(i.e., P

:=

arg

maxPU  (

1m
m

,b(S))

R, P

=

arg

maxPU (

1m
m

,b(S))

R, P

).

Proof of Lemma 3. Given an optimal solution P, suppose there exists i satisfying

n j=1

Pi

j

<

1 m

.

This implies that

i,j Pij

<

1.

We denote

the

gap as 

:=

1 m

-

n j=1

Pi

j

>

0.

Based

on the

assumption of |S|  l, we have

n j=1

bj (S )



1.

Hence,

there

must

exist

j

such that

m i=1

Pij

<

bj . We denote the gap as  := bj -

m i=1

Pij

> 0. We also define ~ = min{,  } > 0. Then,

a matrix P = (Pij + ~I[i = i , j = j ])ij is still a feasible solution. However, Rij > 0 leads to

R, P > R, P , which contradicts P being the optimal solution. Thus,

n j=1

Pij

=

1 m

,

i.

Proof of Theorem 1. Given Cmax = maxi,j Cij + , where  > 0, the following is satisfied.

(S) = (-Cmax + max R, P ) - (-Cmax + max R, P ),

(4)

PU (a,b(S))

PU (a,b())

where R = (Cmax - Cij)ij > 0, m = Nx, n = Ns + Ny, and l = Ny. Here, |S  Y |  Ny = l and Lemma 3 yield (S) = (S  Y ) - (Y ). Since (S) is a contraction of the submodular function (S) (Lemma 2), (S) = -PW2(X, S  Y ) + PW2(X, Y ) is a submodular function.

We now prove the monotonicity of . For S  T , we have U (a, b(S))  U (a, b(T )) because b(S)  b(T ). This implies that (S)  (T ).

Finally, we prove Proposition 1 to clarify the computational intractability of the PWC problem. Proposition 1. The PWC problem with marginals a and b(S) is NP-hard.

Proof of Proposition 1. The decision version of the PWC problem, denoted as PWC(a, b(S), C, K, x), asks whether a subset S, |S|  K such that (S)  x  R exists. We discuss a reduction to the PWC from the well-known NP-complete problems called set cover problems [12]. Given a family G = {Gj  I}M j=1 from the set I = [[N ]] and an integer k, SetCover(G, I, k) is a decision problem to answer Yes iff there exists a sub-family G  G such that |G |  k and GG G = I. Our reduction is defined as follows. Given SetCover(G, I, k) for i  [[N ]] and j  [[M ]], we set ai = 1, bj(S) = |Gj| · I[j  S], Cij = I[i  Gj], K = k, and x = 0. Based on this polynomial reduction, the given SetCover instance is Yes iff the reduced PWC instance is Yes.
This reduction means that the decision version of PWC is NP-complete and now Proposition 1 holds.

3.3 Algorithms
MIP and greedy algorithms (PW-MIP, PW-greedy-{LP, ent}) The PWC problem in Eq. (2) can be solved directly as an MIP problem. We refer to this method as PW-MIP. However, this is extremely inefficient when Napp, Ndev, and Ncand are large.

5

As a consequence of Theorem 1, a simple greedy algorithm gives 0.63 approximation because the problem is a type of monotone submodular maximization with a budget constraint |S|  K [10]. Specifically, this greedy algorithm selects data from the candidate dataset Dcand in a sequential manner in each iteration to maximize . We consider two baseline methods called PW-greedy-LP and PW-greedy-ent. PW-greedy-LP solves the LP problem using the simplex method. PW-greedy-ent computes the optimal P using Sinkhorn iterations [7].

Even for the greedy algorithms mentioned above, the computational cost is still expensive because
we need to calculate the partial Wasserstein divergences for all candidates on Dapp in each iteration, yielding a computational time complexity of O(K ·Ncand ·CP W ). Here, CP W is the complexity of the partial Wasserstein divergence with the simplex method for LP or Sinkhorn iteration.

To reduce the computational cost, we propose heuristic approaches called quasi-greedy algorithms.

A high-level description of the quasi-greedy algorithms is provided below. In each step of greedy

selection, we use a type of sensitivity value that allows us to estimate how much the partial Wasserstein

divergence varies when adding candidate data, instead of performing the actual computation of

divergence. Specifically, if we add a data point s(j) to S, denoted as T = {s(j)}  S, we have

b(T )

=

b(S)

+

, ej
Ndev

where

ej

is

a

one-hot

vector

with

the

corresponding

jth

element

activated.

Hence, we can estimate the change in the divergence for data addition by computing the sensitivity

with respect to b(S). If efficient sensitivity computation is possible, we can speed up the greedy

algorithms. As mentioned previously, we consider both a simplex method and Sinkhorn iteration

to compute partial Wasserstein divergence. We describe the concrete methods for computing the

sensitivity values for each approach in the following paragraphs.

Quasi-greedy algorithm for the simplex method (PW-sensitivity-LP) When we use the simplex method for partial Wasserstein divergence computation, we employ sensitivity analysis for LP (Section 2). The dual problem of partial Wasserstein divergence computation is defined as follows.

max

f , a + g, b(S) , s.t. gj  0, fi + gj  Cij, i, j,

(5)

f RNapp ,gRNcand+Ndev

where f and g are dual variables. By using sensitivity analysis, the changes in the partial Wasserstein

divergences can be estimated as gjbj, where gj is an optimal dual solution of Eq. (5) and bj is

the change in bj. It is worth noting that bj now corresponds to I[s(j)  S] in Eq. (3) and smaller Eq. (5) results larger Eq. (2). We thus propose a heuristic algorithm that iteratively selects s(j)

satisfying j = arg minj[[Ncand]],s(j)/S gj(t) at iteration t, where gj(t) is the optimal dual solution

at

t.

We

emphasize

that

we

have

-

1 Ndev

gj

=

({s(j)}

 S) -

(S)

as

long

as

bj

- bj



1/Ndev

holds, where bj is the upper bound obtained from sensitivity analysis, leading to the same selection as

that of the greedy algorithm. The computational complexity of the heuristic algorithm is O(K·CP W ) because we can obtain g by solving the dual simplex. It should be noted that we add a small value to

bj when bj = 0 to avoid undefined values in g. We refer to this algorithm as PW-sensitivity-LP

and summarize the overall algorithm in Algorithm 1.

For computational efficiency, we use the solution matrix P from the previous step for the initial value in the simplex algorithm in the current step. Any feasible solution P  U (a, b(S)) is also feasible because P  U (a, b(T )) for all S  T . The previous solutions of the dual forms f and g are also utilized for the initialization of the dual simplex and Sinkhorn iteration.
Faster heuristic using C-transform (PW-sensitivity-Ctrans) The heuristics above still require solving a large LP in each step, where the number of dual constraints is Napp ×(Ncand +Ndev). Here, we aim to reduce the size of constraints in the LP to Napp × (|S| + Ndev) Napp × (Ncand + Ndev) using C-transform (Sect.3.1 in [8]).

Algorithm 1 Data selection for the PWC problem with sensitivity analysis (PW-sensitivity-LP)

1: Input: Dapp, Ddev, Dcand

2: Output: S

3: S  {}

4: t  0

5: while |S| < K do

6: Calculate PW2(Dapp, S  Ddev) , and ob-

tain gj(t) from sensitivity analysis.

7:

j = arg minj[[Ncand]],s(j)/S gj(t)

8: S  S  {s(j)}

9: t  t + 1

10: end while

6

To derive the algorithm, we consider the dual Eq.(5). Considering the fact bj(S) = 0 for s(j) / S, we first solve the smaller LP problem by ignoring j such that s(j) / S, whose system size is Napp × (|S| + Ndev). Let the optimal dual solution of this smaller LP be (f , g), where f   RNapp , g  R|S|+Ndev . For each s(j) / S, we consider an LP in which s(j) is added to S. Instead of solving each LP like PW-greedy-LP, we approximate the optimal solution using a technique called C-transform. To be more specific, for each j such that s(j) / S, we only optimize gj and fix the other variables to be the dual optimal above. This is done by gjC := min{0, mini[[Ndev]] Cij - fi}. Note that this is the largest value of gj satisfying the dual constraints. This gjC gives the estimated increase of the objective function when s(j) / S is added to S. Finally, we select an instance j = arg minj[[Ncand]],s(j)/S gjC. As shown later, this heuristic experimentally works efficiently in terms of computational times. We refer to this algorithm as PW-sensitivity-Ctrans.
Quasi-greedy algorithm for Sinkhorn iteration (PW-sensitivity-ent) When we use the generalized Sinkhorn iterations, we can easily obtain the derivative of the entropy-regularized partial Wasserstein divergence j := PW2(Dapp, S  Ddev)/bj, where PW2 denotes partial Wasserstein divergence, using automatic differentiation techniques such as those provided by PyTorch [13]. Based on this derivative, we can derive a quasi-greedy algorithm for Sinkhorn iteration as j = arg minj[[Ncand]],s(j)/S j (i.e., we replace Line 6 and Line 7 of Algorithm 1 with this formula). Because the derivative can be obtained with the same computational complexity as the Sinkhorn iterations, the overall complexity is O(K · CP W ). It is noteworthy that we also add a small value to bj for the computation of the derivative.5 We refer this algorithm PW-sensitivity-ent.
4 Related work
Instance selection and data summarization Tasks for selecting an important portion of a large dataset are common in machine learning. Instance selection involves extracting a subset of a training dataset while maintaining the target task performance. Accoring to [14], two types of instance selection methods have been studied. One is model-dependent methods (e.g., [15, 16, 17, 18, 19]) and the other is label-dependent methods (e.g., [20, 21, 22, 23, 24, 25]). Unlike instance selection methods, PWC does not depend on both model nor ground-truth labels. Data summarization involves finding representative elements in a large dataset [26]. Here, the submodularity plays an important role [27, 28, 29, 30]. Unlike data summarization, PWC focuses on resolving mismatches between application and development datasets.
Anomaly detection Our covering problem can be considered as the task of selecting a subset of a dataset that is not included in a development dataset. In this regard, anomaly detectors (e.g., LOF [4], one-class SVM [31], and deep learning-based methods [32, 33] ) are related to our method. However, our goal is to extract patterns that are not included in the development data distribution, but have a certain volume in the application data distribution, rather than selecting rare data, which would be considered as observation noise or infrequent patterns that are less important than frequent patterns.
Facility location problem (FLP) and k-median The FLP and related k-median [34] are similar to our problem in the sense that we select data S  Dcand to minimize an objective function. FLP is a mathematical model for finding a desirable location for facilities (e.g., stations, schools, and hospitals). While FLP models facility opening costs, our data covering problem does not consider data selection costs, making it more similar to k-median problems. However, unlike k-median, our covering problem allows relaxed transportations for distribution, which is known as Kantorovich relaxation in the optimal transport field [8]. Hence, our approach using partial Wasserstein divergence enable us to model flexible assignment among distributions beyond naïve assignments among data.
5 Experiments
In our experiments, we considered a scenario, in which we wished to select some data in the application dataset Dapp for the PWC problem (i.e., Dapp = Dcand). This is because our motivation
5An important feature of PW-greedy-ent and PW-sensitivity-ent is that they can be executed without using any commercial solvers.
7

is to resolve the mismatch between two datasets as discussed in the introduction. We implemented the solvers for the PWC problem as follows. For PW-*-LP and PW-sensitivity-Ctrans, we used IBM ILOG CPLEX 20.01 [35] for the dual simplex algorithm, where the sensitivity analysis for LP is also available. For PW-*-ent, we computed the entropic regularized version of partial Wasserstein divergence using PyTorch [13] on a GPU. We computed the sensitivity using automatic differentiation. We set the coefficient for entropy regularization = 0.01 and terminated the Sinkhorn iterations when the divergence did not change by at least maxi,j Cij × 10-12 compared to the previous step or the number of the iterations reached 50,000. All experiments were conducted with an Intel® Xeon® Gold 6142 CPU and an NVIDIA® TITAN RTXTM GPU. For baselines, we implemented the following two simple algorithms called random and farthest. For the random setting, we randomly selected data from Dapp \ S and iteratively add them to S. The farthest setting iteratively selects the datum in Dapp \ S having the farthest neighbor in Ddev  S.
5.1 Comparison of accuracy and computational time
We first compare the developed algorithms described in Sect. 3.3, namely the greedy-based PW-greedy-*, sensitivity-based PW-sensitivity-*, and two baselines random and farthest. For the evaluation, we generated two-dimensional random values following a normal distribution for Dapp and Ddev, respectively. Figure 2 (left) presents the partial Wasserstein divergences between Dapp and S  Ddev while varying the number of the selected data when Napp = Ndev = 30 and Fig. 2 (right) presents the computational times when K = 30 and Napp = Ndev = Ncand varied. As shown in Fig. 2 (left), the proposed algorithms achieve lower partial Wasserstein divergences than the baselines. In this case, PW-greedy-LP and PW-sensitivity-LP select exactly the same data as the global optimal solution obtained by PW-MIP. For quantitative evaluation, we define the empirical approximation ratio as (SK )/(SK ), where SK is the optimal solution obtained by PW-MIP when |S| = K. As shown in Fig. 2 (right), considering the logarithmic-scale, PW-sensitivity-* significantly reduce the computational time compared to PW-MIP, while the naïve greedy algorithms do not scale. In particular, PW-sensitivity-ent can be calculated fast as long as the GPU memory is sufficient, while its CPU version (PW-sensitivity-ent(CPU)) is significantly slower. PW-sensitivity-Ctrans is the fastest among the methods without GPUs. It is 8.67 and 3.07 times faster than PW-MIP and PW-sensitivity-LP, respectively. Figure 3 shows the empirical approximation ratio (Napp = Ndev = 30, K = 15) for 50 different random seeds. One can see that our approaches significantly outperform the baselines. Both PW-*-LP achieve approximation ratios close to one. The optimal solutions derived by PW-MIP can be obtained only for small datasets.
5.2 Finding a missing category in unlabeled dataset
Here, we demonstrate an application of PWC using the MNIST dataset [36], where the development dataset contains 0 labels at a rate of only 0.5%, whereas all labels are included in the application data in equal ratios (i.e., 10%). For evaluation, we randomly sampled 500 images from the training split for each of Dapp and Ddev. We employed LOF [4] novelty detection provided by scikit-learn [37] with the default parameters as a baseline to capture the mismatches between datasets in addition to random and farthest. For the LOF, we selected top-K data according to the anomaly scores. We conducted 10 trials using different random seeds for the proposed algorithms and baselines, except that we only performed one trial for the greedy algorithms, because they took over 24h. Figure 4 presents a histogram of the selected labels when K = 30, where x-axis corresponds to labels of selected data (i.e., 0 or not 0) and y-axis shows relative frequencies of selected data. The proposed methods (i.e., PW-*) extract more data corresponding to the label 0 (0.78 for PW-MIP) than the baselines, including LOF (0.29). We can conclude that the proposed methods successfully selected data from the candidate dataset Dcand(= Dapp) to fill in lacking areas in the distribution.
5.3 Missing scene extraction in real driving datasets
Finally, we demonstrate PWC in a realistic scenario, using driving scene images. We adopted two datasets called BDD100K [38] and KITTI (Object Detection Evaluation 2012) [39] for the application dataset and development dataset, respectively. The major difference between these two datasets is that KITTI (Ddev) contains only daytime images, whereas BDD100k (Dapp) contains both daytime and nighttime images. To reduce computational time, we randomly selected 1,000 data for the development dataset from the test split of KITTI dataset and 2,000 data for the application dataset
8

PartidialveWragsesnecrestein

Partial Wasserstein divergence

0.02

0.01

0.00 0

#10selected d2at0a

30

Comtipmutea[tis]onal

Computational Time
103 102 101
1#00data 2(N0a0pp = N3d0e0v = Nc4a0nd0) 500

PW-MIP PW-greedy-LP PW-greedy-ent PW-sensitivity-LP PW-sensitivity-Ctrans PW-sensitivity-ent PW-sensitivity-ent(CPU) farthest random

Figure 2: Partial Wasserstein divergence PW2(Dapp, S  Ddev) when data are sequentially selected

and added to S (left). Computational time, where colored areas indicate standard deviations (right).

1.0

1.0

PW-MIP

Relative frequency

Approrxaitimoation

0.8 0.6

0.8

PW-greedy-LP

0.6

PW-greedy-ent PW-sensitivity-LP

Figure 3PW:-gAreepdyPp-WLP-rgoreexdPyiW-em-nstenasPittWiivoi-tsyen-nLsPitrivaitPyWt-Ci-storeansnssitievivty-aefnaltrutheastterdandeomm-

0.4

0.2

0.0

0 Label not 0

PW-sensitivity-Ctrans PW-sensitivity-ent farthest random LOF

Figure 4: Histogram for labels of selected data.

pirically for K = 15 and 50 random instances

Black bars denote standard deviations.

(a) Partial Wasserstein covering

(b) LOF

Figure 5: Covering results when the application dataset (BDD100k) contains nighttime scenes, but the development dataset (KITTI) does not. PWC (PW-sensitivity-LP) extracts the major difference between the two datasets (i.e., nighttime images), whereas LOF only extracts some rare cases.

from the test split of BDD100k. To compute the transport costs between images, we calculated squared L2 norms between the feature vectors extracted by a pretrained ResNet with 50 layers [40] obtained from Torchvision [41]. Before inputting the images into the ResNet, each image was resized to a height of 224 pixels and then center-cropped to a width of 224, followed by normalization. As a baseline method, we adopted the LOF [4] as in Sect. 5.2. Figure 5 presents the obtained top-3 images. One can see that PWC (PW-sensitivity-LP) selects the major pattern (i.e., nighttime scenes) that is not included in the development data, whereas LOF only extracts specific rare scenes (e.g., a truck crossing a street or a sunset scene).
The above results indicate that the PWC problem enables us to accelerate updating or incrementally learning ML models when the distributions of application and development are different because our method reveals patterns, not a possibly isolated anomaly, as shown in Fig. 5 and Fig. 1. The ML workflow using our method could efficiently find a pattern whose density is low in terms of data distributions and incrementally update the model (e.g., designing a special subroutine for the pattern) using the selected data. This process could enhance the reliability and generalization ability of ML-based systems, such as image recognition in autonomous vehicles.
6 Conclusion
In this paper, we proposed PWC, which fills the gaps between development datasets and application datasets based on partial Wasserstein divergence. We also proved the submodularity of PWC, leading to a greedy algorithm with the guaranteed approximation ratio. Additionally, we proposed quasigreedy algorithms based on sensitivity analysis and the derivatives of Sinkhorn. Experimentally, we demonstrated that the proposed method covers the major areas of development datasets with densities lower than thoes of the corresponding areas of application datasets. The main limitation of PWC is scalability; the space complexity of the dual simplex or Sinkhorn iteration is at least O(Napp · Ndev), which might require approximation (e.g., stochastic optimizations [42] and neural networks [43]). As a potential negative societal impact, we also note that if infrequent patterns in application datasets are as important as frequent patterns, the proposed method may ignore some important cases, and it may be desirable to use it with methods that focuses on fairness.
9

References
[1] Arjovsky, M.; Chintala, S.; Bottou, L. Wasserstein Generative Adversarial Networks. Proceedings of the 34th International Conference on Machine Learning. 2017; pp 214­223.
[2] Alvarez-Melis, D.; Fusi, N. Geometric Dataset Distances via Optimal Transport. Advances in Neural Information Processing Systems 33. 2020; pp 21428­21439.
[3] Bonneel, N.; Coeurjolly, D. SPOT: sliced partial optimal transport. ACM Transactions on Graphics (TOG) 2019, 38, 1­13.
[4] Breunig, M. M.; Kriegel, H.-P.; Ng, R. T.; Sander, J. LOF: identifying density-based local outliers. Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data. 2000; pp 93­104.
[5] Figalli, A. The optimal partial transport problem. Archive for rational mechanics and analysis 2010, 195, 533­560.
[6] Chapel, L.; Alaya, M.; Gasso, G. Partial optimal transport with applications on positiveunlabeled learning. Advances in Neural Information Processing Systems 33. 2020; pp 2903­ 2913.
[7] Benamou, J.-D.; Carlier, G.; Cuturi, M.; Nenna, L.; Peyré, G. Iterative Bregman projections for regularized transportation problems. SIAM Journal on Scientific Computing 2015, 37, A1111­ A1138.
[8] Peyré, G.; Cuturi, M. Computational optimal transport: With applications to data science. Foundations and Trends® in Machine Learning 2019, 11, 355­607.
[9] Vanderbei, R. J. Linear programming; Springer, 2015; Vol. 3.
[10] Nemhauser, G. L.; Wolsey, L. A.; Fisher, M. L. An analysis of approximations for maximizing submodular set functions--I. Mathematical programming 1978, 14, 265­294.
[11] Bar-Noy, A.; Rabanca, G. Tight approximation bounds for the seminar assignment problem. International Workshop on Approximation and Online Algorithms. 2016; pp 170­182.
[12] Karp, R. M. Reducibility among combinatorial problems. Complexity of computer computations. 1972; pp 85­103.
[13] Paszke, A. et al. PyTorch: An Imperative Style, High-Performance Deep Learning Library. Advances in Neural Information Processing Systems 32. 2019; pp 8024­8035.
[14] Olvera-López, J. A.; Carrasco-Ochoa, J. A.; Martínez-Trinidad, J. F.; Kittler, J. A review of instance selection methods. Artificial Intelligence Review 2010, 34, 133­143.
[15] Hart, P. The condensed nearest neighbor rule (corresp.). IEEE transactions on information theory 1968, 14, 515­516.
[16] Ritter, G. An Algorithm for a selective nearest neighbor decision rule. IEEE Trans. Information Theory 1975, 21, 665­669.
[17] Chou, C.-H.; Kuo, B.-H.; Chang, F. The generalized condensed nearest neighbor rule as a data reduction method. Proceedings of the 18th International Conference on Pattern Recognition. 2006; pp 556­559.
[18] Wilson, D. L. Asymptotic properties of nearest neighbor rules using edited data. IEEE Transactions on Systems, Man, and Cybernetics 1972, 408­421.
[19] Vázquez, F.; Sánchez, J. S.; Pla, F. A stochastic approach to Wilson's editing algorithm. Iberian conference on pattern recognition and image analysis. 2005; pp 35­42.
[20] Wilson, D. R.; Martinez, T. R. Reduction techniques for instance-based learning algorithms. Machine Learning 2000, 38, 257­286.
[21] Brighton, H.; Mellish, C. Advances in instance selection for instance-based learning algorithms. Data Mining and Knowledge Discovery 2002, 6, 153­172.
[22] Riquelme, J. C.; Aguilar-Ruiz, J. S.; Toro, M. Finding representative patterns with ordered projections. Pattern Recognition 2003, 36, 1009­1018.
[23] Bezdek, J. C.; Kuncheva, L. I. Nearest prototype classifier designs: An experimental study. International Journal of Intelligent Systems 2001, 16, 1445­1473.
10

[24] Liu, H.; Motoda, H. On issues of instance selection. Data Mining and Knowledge Discovery 2002, 6, 115­130.
[25] Spillmann, B.; Neuhaus, M.; Bunke, H.; Pekalska, E.; Duin, R. P. Transforming strings to vector spaces using prototype selection. Joint IAPR international workshops on statistical techniques in pattern recognition (SPR) and structural and syntactic pattern recognition (SSPR). 2006; pp 287­296.
[26] Ahmed, M. Data summarization: a survey. Knowledge and Information Systems 2019, 58, 249­273.
[27] Mitrovic, M.; Kazemi, E.; Zadimoghaddam, M.; Karbasi, A. Data summarization at scale: A two-stage submodular approach. Proceedings of the 35th International Conference on Machine Learning. 2018; pp 3596­3605.
[28] Lin, H.; Bilmes, J. A class of submodular functions for document summarization. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. 2011; pp 510­520.
[29] Mirzasoleiman, B.; Zadimoghaddam, M.; Karbasi, A. Fast Distributed Submodular Cover: Public-Private Data Summarization. Advances in Neural Information Processing Systems 29. 2016; pp 3594­3602.
[30] Tschiatschek, S.; Iyer, R. K.; Wei, H.; Bilmes, J. A. Learning mixtures of submodular functions for image collection summarization. Advances in Neural Information Processing Systems 27. 2014; pp 1413­1421.
[31] Schölkopf, B.; Williamson, R. C.; Smola, A. J.; Shawe-Taylor, J.; Platt, J. C., et al. Support vector method for novelty detection. Advances in Neural Information Processing Systems 12. 1999; pp 582­588.
[32] Kim, K. H.; Shim, S.; Lim, Y.; Jeon, J.; Choi, J.; Kim, B.; Yoon, A. S. Rapp: Novelty detection with reconstruction along projection pathway. International Conference on Learning Representations. 2019.
[33] An, J.; Cho, S. Variational autoencoder based anomaly detection using reconstruction probability. Special Lecture on IE 2015, 2, 1­18.
[34] ReVelle, C. S.; Swain, R. W. Central facilities location. Geographical analysis 1970, 2, 30­42. [35] IBM ILOG CPLEX Optimization Studio, CPLEX User's manual. Vers 2013, 12, 207­258. [36] LeCun, Y.; Cortes, C. MNIST handwritten digit database. 2010, [37] Pedregosa, F. et al. Scikit-learn: Machine Learning in Python. Journal of Machine Learning
Research 2011, 12, 2825­2830. [38] Yu, F.; Chen, H.; Wang, X.; Xian, W.; Chen, Y.; Liu, F.; Madhavan, V.; Darrell, T. BDD100K:
A Diverse Driving Dataset for Heterogeneous Multitask Learning. IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020. [39] Geiger, A.; Lenz, P.; Stiller, C.; Urtasun, R. Vision meets Robotics: The KITTI Dataset. International Journal of Robotics Research 2013, [40] He, K.; Zhang, X.; Ren, S.; Sun, J. Deep residual learning for image recognition. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016; pp 770­778. [41] Marcel, S.; Rodriguez, Y. Torchvision the machine-vision package of torch. Proceedings of the 18th ACM International Conference on Multimedia. 2010; pp 1485­1488. [42] Aude, G.; Cuturi, M.; Peyré, G.; Bach, F. Stochastic optimization for large-scale optimal transport. arXiv preprint arXiv:1605.08527 2016, [43] Xie, Y.; Chen, M.; Jiang, H.; Zhao, T.; Zha, H. On scalable and efficient computation of large scale optimal transport. International Conference on Machine Learning. 2019; pp 6882­6892.
11

