JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

1

Semi-supervised Conditional Density Estimation for Imputation and Classification of Incomplete Instances
Huang Buliao

arXiv:2106.01708v1 [cs.LG] 3 Jun 2021

Number of observed h

Abstract--Incomplete instances with various missing attributes in many real-world scenes have brought challenges to the classification task. There are some missing values imputation methods to fill the missing values with substitute values before classification. However, the separation between imputation and classification may lead to inferior performance since label information are ignored during imputation. Moreover, these imputation methods tend to initialize these missing values with strong prior assumptions, while the unreliability of such initialization is rarely considered. To tackle these problems, a novel semi-supervised conditional normalizing flow (SSCFlow) is proposed in this paper. SSCFlow explicitly utilizes the observed labels to facilitate the imputation and classification simultaneously by employing a semi-supervised algorithm to estimate the conditional probability density of missing values. Moreover, SSCFlow takes the initialized missing values as corrupted initial imputation and iteratively reconstructs their latent representations with an overcomplete denoising autoencoder to approximate the true conditional probability density of missing values. Experiments have been conducted with real-world datasets to demonstrate the robustness and efficiency of the proposed algorithm.
Index Terms--missing value, imputation and classification, normalizing flow, conditional probability density estimation, semisupervised.

Probability

Probability

distribution of observed h with label y=1 distribution of observed h with label y=2
0.2

less promising imputation point with max

(lower probability)
0.15

density likelihood

2400
p(h)
2100
1800

imputation is located at

1500

0.1

mixture area and is difficult 1200

to be classified reasonably

900

0.05

600

300

0

0

-6

0

6

h

(a) imputation with probability density p(h)

0.2

more promising imputation point with max

(higher probability)

density likelihood

0.15

2400
p(h|y=1) p(h|y=2) 2100
1800

imputation could be classified 1500

0.1

simultaneously with better 1200

interpretability

900

0.05

600

300

0

0

-6

0

6

h

(b) imputation with conditional probability density p(h|y)

Number of observed h

I. INTRODUCTION
M ISSING value problem is ubiquitous in real-world applications [1]. Inappropriate treatment for incomplete instances with missing values might consequently degrade the machine learning performance. Therefore, how to appropriately handle missing values in classification problems has still been a challenge for the machine learning community.
In order to handle missing values in classification, one straightforward approach is to employ a "two-step" strategy: first applying unsupervised missing value imputation methods [2], [3], [4], [5], [6] to replace the missing values with substitute values, and then employing supervised classification methods on the imputed dataset1. Recently, normalizing flow2 [7], [8], [9], has led to great advances in these unsupervised missing value imputation methods thanks for its
M. Shell was with the Department of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, 30332 USA e-mail: (see http://www.michaelshell.org/contact.html).
J. Doe and J. Doe are with Anonymous University. Manuscript received April 19, 2005; revised August 26, 2015. 1The unsupervised missing value imputation methods are trained on both labelled and unlabelled incomplete instances to impute their missing values, while the supervised classification methods are trained on the labelled imputed instances to predict the unobserved labels of unlabelled imputed instances. 2Detailed introduction of normalizing flow could be viewed in Sec. II.

Fig. 1: Examples3for the imputation of missing value h with its estimated probability density p(h) and conditional probability density p(h|y) by maximizing the density likelihood.
direct optimization of the exact data likelihood. For example, Richardson et al. [5] proposed Monte Carlo Flow (MCFlow), which leverages normalizing flow to estimate the probability density of the incomplete instances and impute the missing values by maximizing their likelihood over observed values with Monte Carlo sampling. However, the separation between imputation and classification in these "two-step" methods may lead to inferior performance since label information are ignored during imputation. Moreover, these methods often need to initialize the missing values of the incomplete instances since their models are unable to handle incomplete input. The unreliability of such initialization is rarely considered and biased assumptions about the missing values may be made by initializing them with special default values [12].
To tackle the aforementioned problems, this paper proposes a novel semi-supervised conditional normalizing flow (SS-
3The mixture area in the examples relates to the local data area where the distribution of missing values with different categories are similar and could not be reasonably distinguished.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

2

input
incomplete features
unobserved labels

initial

corrupted full

imputation

features

missing values

additional features

observed labels

forward

imputation

inverse

reconstructed latent representations

overcomplete denoising autoencoder
additional prior

Bayes classifier

reconstructed incomplete features
reconstructed unobserved labels
classification

main components of iterative denoising mechanism
Details could be viewed in Fig. 3

reconstruction components of semisupervised conditional network Details could be viewed in Fig. 4

prediction components of semisupervised conditional network Details could be viewed in Fig. 5

Fig. 2: The architecture of the proposed SSCFlow, where normalizing flow T(x) estimates the conditional probability density of missing values for imputation and classification. SSCFlow explicitly takes the observed labels as additional features and prior to facilitate the conditional probability density estimation in T(x). The unobserved labels are reconstructed (predicted) from the view of imputation and classification respectively to accomplish classification. Meanwhile, SSCFlow iteratively reconstructs the corrupted full features to approximate the true conditional probability density of missing values for imputation.

CFlow), which accomplishes both imputation and classification simultaneously by estimating the conditional probability density of missing values conditioned on their labels. As illustrated in Fig. 1, compared with the probability density estimation in [5], the conditional probability density estimation could facilitate the imputation by explicitly taking label information as additional features to model more exact distribution of missing data [13]. Meanwhile, the classification could be accomplished with better interpretability by modelling Bayes decision rule with conditional probability density. To this end, the aforementioned "two-step" tasks (unsupervised imputation + supervised classification) is formed as a novel semisupervised task, which need to simultaneously estimate the conditional probability density of both labelled and unlabelled incomplete instances4 for their imputation and classification. However, it remains a challenging problem to estimate the conditional probability density of unlabelled incomplete instances in existing semi-supervised normalizing flows [14], [15], [16]. First, different flow models are required for each class to condition unlabeled instances on the class labels, which does not share weights between classes and could not utilize training data effectively [14], [16]. Additionally, most of them assume that the dataset is completely observed and could not handle incomplete input.
To tackle these challenges, SSCFlow employs a novel semisupervised conditional network, whose weights are shared over labelled and unlabelled incomplete instances. In the semisupervised conditional network, the observed labels are taken as additional input features and prior knowledges to direct the conditional probability density estimation. As for the
4The labelled (resp. unlabelled) incomplete instances refer to the train (resp. test) instances with missing values in the classification task. Specifically, the labelled incomplete instances contain missing values that need to be imputed, while the unlabelled incomplete instances contain both missing values that need to be imputed and unobserved labels that need to be predicted.

unobserved labels, they are taken as missing values to participate in the training process and reconstructed (predicted)5 by maximizing the joint likelihood over labelled and unlabelled instances. Moreover, since the incomplete input remains a challenge and the unreliability of the initialization is rarely considered, a novel iterative denoising mechanism is proposed for SSCFlow. The iterative denoising mechanism takes those initialized missing values as corrupted initial imputation and iteratively reconstructs their latent representations6 with an overcomplete denoising autoencoder to approximate the true conditional probability density of missing values.
In summary, this paper makes the following contributions:
· This paper proposes a novel semi-supervised conditional normalizing flow (SSCFlow). To the best of our knowledge, SSCFlow is the first method that estimates the conditional probability density of missing values to accomplish their imputation and classification simultaneously.
· A novel semi-supervised conditional network is proposed, whose weights are shared over labelled and unlabelled incomplete instances and facilitates their imputation and classification with the observed labels.
· A novel iterative denoising mechanism is proposed to make SSCFlow robust for the unreliability of initialization.
· The Bayes decision rule based classification in the proposed method naturally encodes clustering principle and has strong interpretability.
The rest of this paper is organized as follows. The background of the normalizing flow, which is the base model
5From the view of imputation task, the unobserved labels are reconstructed. From the view of classification task, the unobserved labels are predicted.
6Normalizing flows estimate probability density by transforming the data to its latent representation with invertible transformations and computing the relative change of local density volume [10]. Details could be viewed in Sec. II.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

3

of SSCFlow, is illustrated in Section II. Then, a review of the related work is provided in Section III. In Section IV, the task of imputation and classification with missing values is described. Section V introduces the proposed methods in details. Section VI details the experimental settings and empirical analysis. Finally, the conclusion and future work are provided in Section VII.

II. BACKGROUND: NORMALIZING FLOW

SSCFlow builds on normalizing flow [7], [8], [9], which is a kind of deep generative model while not as well-known as variational auto-encoder (VAE) [11] and generative adversarial nets (GAN) [17]. Compared with VAE and GAN, normalizing flow has highly desirable properties like exact density estimation and exact latent-variable inference. Normalizing flow models data distribution by transformating of a simple prior distribution (e.g., a standard normal [10]) in the latent space into a more complex distribution in the input space with a sequence of invertible and differentiable transformations. Let x be a D-dimensional real vector, and suppose the task is to find a joint distribution over x. The main idea of flow-based methods is to express x as a sequence of transformations T -1 of a latent representation vector z sampled from a latent prior distribution pZ (z).

x = T-1(z) where z  pZ (z)

(1)

where T -1 is parameterized by a set of parameters . The
defining property of flow-based methods is that the transformations T-1 must be invertible and both T and T-1 must be differentiable. Under these conditions, the density of x could
be evaluated exactly by transforming it back to the latent prior
distribution and then computing change of variables [18] as
following :

pX (x) = pZ (T(x)|detJT (x)|

(2)

where JT refers to the Jacobian of T. Since flow models give the exact density likelihood in Eq. 2,  can be trained by
directly optimizing the log likelihood [19] as following:

 = arg max pX (x)


= arg max pZ (T(x)|detJT (x)|

(3)



= arg max log(pZ (T(x)) + log(|detJT (x)|)


In addition, thanks to the invertibility of the transformations, one can draw samples by simply inverting the transformations over a set of samples from the latent space.

III. RELATED WORK
This section first reviews the related work about the missing value imputation methods. Then, the related work about classconditional normalizing flow is introduced.

A. Missing value Imputation Methods
Many missing value imputation methods have been proposed to replace the missing values with substituted values. For example, Hapfelmeier et al. [3] proposed the multivariate imputation by chained equations (MICE). which creates multiple imputations (replacement values) for multivariate missing data. The method is based on Fully Conditional Specification, where each incomplete variable is imputed by a separate model. Stekhoven et al. [4] proposed missForest, which is a nonparametric, mixed-type imputation method for basically any type of data. By averaging over many unpruned classification or regression trees, missForest intrinsically constitutes a multiple imputation scheme, using the built-in out-of-bag error estimates of random forest. Recently, deep generative models have led to great advances in unsupervised missing value imputation. Greg et al. [6] proposes GAIN, which imputes missing data by adapting the well-known Generative Adversarial Nets (GAN) framework. Q. Shi et al. [20] proposed MCFlow, which leverages normalizing flow generative models and Monte Carlo sampling for imputation.
B. Learning in the Model Space
The proposed iterative denoising mechanism could be viewed as a model to be fitted on the latent representations of the incomplete instances in the latent space to have more stable and parsimonious representations of them to estimate their probability density. This coincides with one new trend, termed "learning in the model space" in the machine learning community. The idea of learning in the model space (LiMS) [21] is to use models fitted on parts of data as more stable and parsimonious representations of the data. Learning is then performed directly in the model space instead of the original data space to achieve more robust and more targeted learning on diverse data collections. The LiMS origins from time series learning [22] and fault diagnosis [23], and it has been improved by considering both representation ability of models, and the discrimination ability in the model space [24]. Later, LiMS has been successfully extended to symbolic sequence learning [25], multi-objective learning [26], Imbalanced learning [27], dynamic state modeling [28], and short sequences [29], etc.
C. Semi-Supervised Normalizing Flow
There have been some normalizing flows to estimate the conditional probability density of unlabelled instances in semisupervised manner. Trippe et al. [14] propose to use different flows for each class to condition unlabeled instances on the class labels, which does not share weights between classes and could not utilize training data effectively [16]. To promote the former method, Atanov et al. [16] propose a conditional coupling layer with a multi-scale architecture. Although Atanov et al. [16] share weights between classes in a certain degree, its conditional coupling layer still need L runs for L classes. Izmailov et al. [15] proposed FlowGMM, which could achieve one shot running by modelling the density in the latent space as a Gaussian mixture with each mixture component

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

4

input incomplete
features

forword

initial

corrupted

imputation full features

iterative updating

imputation

latent representations

overcomplete denoising autoencoder
dropout

inverse
reconstructed latent representations

reconstructed features

update based on corrupted full features

denoising and reconstruction

generate reconstructed features based on

Fig. 3: High-level view of iterative denoising mechanism. To tackle the incomplete input, random initialization is employed as initial imputation to generate corrupted full features. The iterative denoising mechanism takes the unreliability of the initialization into account and iteratively reconstructs the corrupted full features with an overcomplete denoising autoencoder to approximate the true probability density of missing values for imputation.

corresponding to a class. However, since the label information only participates in the loss computation and does not affect the generative process, its improvement for imputation may be limited. In addition, the parameters of the Gaussian mixture in FlowGMM [15] is fixed after random initialization, which may make it difficult for the model to converge stably. Moreover, none of these semi-supervised normalizing flows take the missing value scenes into consideration.
IV. PROBLEM DESCRIPTION
Assuming that there is an incomplete dataset X = {x1, x2, ..., xN } with N instances. The i-th instance xi in X contains d attribute values and can also be written as (x1i , x2i , ..., xdi ). A binary masking vector M (xi)  (0, 1)d is assigned to xi to indicate the location of missing attributes with their corresponding masking attributes set to 0. Generally, X could be divided into two subset as training dataset Xtr with label set Ytr  1, ..., L observed to indicate its labels (L classes) and test dataset Xte with their unobserved labels Yte. The task of this paper is to assign each missing value in the incomplete dataset X with its substituted value and predict the unobserved labels Yte. Considering that both the training dataset Xtr and test dataset Xte may contains missing values that need to be imputed, this paper forms the task in a semisupervised manner and take the instances in the test dataset as unlabelled instances when they are entered into the imputation algorithm.
V. SEMI-SUPERVISED CONDITIONAL NORMALIZING FLOW
In this section, the SSCFlow model is introduced. Given an incomplete instances, SSCFlow imputes its missing values and predicts its class label simultaneously. In this section, the normalizing flow with a novel iterative denoising mechanism, which imputes the missing values and get robust for the unreliability of the initialization, is first introduced. Then, to utilize the observed labels to facilitate the imputation and classification of both labelled and unlabelled incomplete instances, a novel semi-supervised conditional network is proposed. The semi-supervised conditional network could be divided into two parts, named semi-supervised conditional invertible

transformation and semi-supervised conditional latent space respectively. Finally, the training of SSCFlow with novel loss functions is illustrated. The procedure of SSCFlow is provided in Fig. 2.
A. Iterative Denoising Mechanism
Since normalizing flow is a generative model, data imputation is its natural application by sampling from the data distribution learned by the generative model. Assuming that there is a complete set C of training data that could optimize the parameters  of the invertible transformation T in Sec. II to learn the data distribution of C, the missing values Cm of C could be imputed by sampling from their marginal distribution based on the observed values Co and the trained normalizing flow T. However, a more reasonable assumption is that training data itself may have missing entries, which makes direct estimation of the data distribution of the incomplete data not possible and brings a challenge to the training of normalizing flow.
Following MCFlow [20], SSCFlow addresses this challenge with a alternating algorithm. Specifically, an initialization is first employed to fill the incomplete instances x as imputed instances x . Then SSCFlow iteratively updates the generative network parameters  according to the imputed instances x and updates the imputation of x according to the updated . In MCFlow, a multi-layer perceptron (MLP) with fixed hidden units in its each layer are employed in the latent space to find the latent representation z of x with the maximum density likelihood and whose entries matching the observed values xo of x.
However, the unreliability of the initialization has not been considered in MCFlow [20] and biased assumptions about the missing values may be made by initializing them with special default values [12]. Moreover, since the MLP only requires the found vector's entries to match the observed values xo, it's easy to overfit the observed values xo, which means that MCFlow only tries to fit on the marginal distribution of the observed values xo rather than learning the data distribution of the unobserved values xm. Therefore in this work, SSCFlow employs a iterative denoising mechanism (IDM). The iterative

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

5

input incomplete
features
unobserved labels
observed labels

initial imputation

[] corrupted full features

one-hot encoder

imputation

overcomplete

-

denoising autoencoder

=

=

+ +

half of input features in k-th layer
half of latent representations
in k-th layer

addition
- subtraction

]|[

[]

operations to split or concentrate vectors

multiplication

division

=

equal

neural networks that parameterized by

reconstructed incomplete features
]|[
reconstructed unobserved labels
classification

Fig. 4: High-level view of semi-supervised conditional invertible transformation. The observed labels are taken as additional features to direct the conditional probability density estimation, while the unobserved labels are taken as missing values and reconstructed (predicted) during imputation.

denoising mechanism treats the initializated missing values as corrupted initial imputation and redesingns the MLP as an overcomplete denoising autoencoder (ODAE), whereby projecting the latent representation z of corrupted initial imputation x to a higher dimensional subspace from where the latent representation z is then reconstructed as z^ to be robust for the unreliability of the initialization. The overcomplete representation of DAE means that more units in successive hidden layers during encoding phase compared to the input layer. This mapping of z to a higher dimensional subspace creates representations capable of adding lateral connections, aiding in data recovery [30]. The usefulness of the overcomplete denoising autoencoder empirically validated in the Sec. VI-D. The detailed procedure of IDM could be viewed in Fig. 3.
B. Semi-supervised Conditional Network
In order to incorporate the label information to help model the data distribution of the incomplete dataset, SSCFlow need to estimate the conditional probability density p(x|y) rather than p(x). Although there are some existing semi-supervised normalizing flows to estimate conditional probability density with normalizing flow, they do not share weights between classes and could not utilize training data effectively [16] when estimate the conditional probability density of unlabelled instances. The main reason for this limitation is that L times running is needed for the unlabelled instances with L potential classes since the generative process for the unlabelled instances is conditioned on their class labels that are not observed. To take this challenge, this paper proposes a new semisupervised conditional network, whose weights are shared over labelled and unlabelled incomplete instances and facilitates their imputation and classification with the observed labels. Such semi-supervised conditional network could be divided into two parts, named semi-supervised conditional invertible transformation and semi-supervised conditional latent space, which respectively tackle the challenge from the view of imputation and classification task.
The semi-supervised conditional invertible transformation tackles this challenge from the view of imputation task in

the input space of normalizing flow by explicitly taking the observed labels as additional features to direct the conditional probability density estimation. In the semi-supervised conditional invertible transformation, the label y would be encoded as a one-hot class vector y with yk = 1 if y = k, otherwise yk = 0. The one-hot class vector would be taken as additional feature of the imputed instances x and they are concentrated to form a new input variable x^ = [x , y], which allows to model complex dependencies between data distribution and the class labels. As for these unlabelled instances, their onehot label vector are treated as missing values and initialized to be reconstructed during IDM. Then, this layer splits the input variable x^ into two non-overlapping parts x^1 , x^2 and applies affine transformation based on the first x^1 to the other x^2. The semi-supervised conditional invertible transformation T could be defined as follow:

1 : y = OneHotEncoder(y)

2 : x^ = [x , y]

3 : x^1, x^2 = Split(x^)

(4)

4 : z1 = x^1

5 : z2 = x^2 exp(s(x^1)) + t(x^1)

6 : z = Concentrate(z1, z2)

where s and t are arbitrary neural networks that parameterized by the parameters  and with  = {, }, Split denotes the operation that splits the input variable x^ into two nonoverlapping parts x^1 and x^2, Concentrate is the inverse operation of Split to concentrate z1 and z2 into the latent representation z. To enhance the performance of the model, steps 3-6 could be stacked by using z of the k-th x^k as x^k+1 of the next stack, which follows the designing of RealNVP [8]. The detailed procedure of the semi-supervised conditional invertible transformation could be viewed in Fig. 4. As the semi-supervised conditional invertible transformation could reconstruct the class label during the imputation of IDM by modelling Bayes posterior distribution with conditional probability density, the class labels always participate the generative process of the unlabelled instances, which saves L times running for them and makes the semi-supervised conditional invertible transformation in one shot. Moreover,

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

6

observed labels

additional prior

latent space

dynamic parameters
setting

latent space

Bayes classifier

latent space

latent representations after reconstruction

latent representations of labelled instances with different labels
latent representations of unlabeled instances

dynamical centers of each GMM mixture component
classifier decision boundary

Fig. 5: High-level view of class-conditional latent space. The observed labels are taken as additional prior knowledges to model the latent space distribution conditioned on the labels as Gaussian mixture model, while the unobserved labels are predicted by maximizing the joint likelihood.

the label of the unlabelled instances could also be predicted based on the reconstructed class label.
As for the semi-supervised conditional latent space, it explicitly taking the observed labels as additional prior knowledges to direct the conditional probability density estimation from the view of classification task in the latent space of normalizing flow. Traditional normalizing flows model data x as a deterministic and invertible transformation x = T-1(z) of a simple random latent variable z, e.g. standard normal [10], in the latent space, which may not take the additional supervised information into consideration. In this paper, SSCFlow follows FlowGMM [15] to take the labels as additional prior knowledges and model the latent space distribution conditioned on the label k as Gaussian with mean µk and covariance covk:

pZ (z|y = k) = N (z|µk, covk).

(5)

and the marginal distribution of z is then a Gaussian mixture model (GMM), whose distribution could be formed as:

L

pZ (z) = N (z|µk, covk)p(y = k).

(6)

k=1

. Combining Eq. 2 and 6 , the likelihood for labelled data is

pX (x|y = k) = N (T(x)|µk, covk) · |detJT (x)|. (7)

and the likelihood for data with unobserved label is

L

pX (x) = pX (x|y = k)p(y = k)

k=1

(8)

L

= N (T(x)|µk, covk) · |detJT (x)|p(y = k)

k=1

. The label y of the unlabelled instances could also be predicted by a Bayes classifier, which maximizes the above likelihood with Bayes decision rule according to

y = arg max pX (y = k|x)
k

= arg max pX (x|y = k)p(y = k)

(9)

k

pX (x)

= arg max pX (x|y = k)p(y = k)
k
.

Although theoretically the prior distribution should not

matter since it could be transformed from the standard normal,

the resulting transformations may become easier to learn in

practice if some prior knowledges is provided in the prior

distribution [10]. In [15], the means µ are randomly sampled

from the standard normal distribution µk  N (0, I), and

the covariance matrices cov are set to identity i = I for all classes, which are both fixed throughout training. How-

ever, since the dataset keeps being upgraded with the IDM,

the fixed setting of the parameters µ, cov for the Gaussian

mixture may not apply to SSCFlow. Therefore, SSCFlow

introduces a dynamic parameters setting algorithm, which

dynamically upgrade the parameters along with the IDM and

makes SSCFlow more interpretable. Specifically, after each

iteration of IDM, the means µk of class k is set as the mean

point of latent representations of labelled data in each class

: µk = (1/nk)

nk m=1

T

(xm k

),

where

xk

represents

labelled

data points from class k and nk is the total number of labelled

points in that class. As shown in Sec. VI-C, the dynamically

upgrading of the means µ reflected the learning process of

SSCFlow naturally encodes the clustering principle, which not

only improves the effect of SSCFlow, but also makes SSCFlow

more interpretable. The detailed procedure of class-conditional

latent space could be viewed in Fig. 5.

C. Training of SSCFlow

The parameters of SSCFlow that need to be trained consist of two parts:  of semi-supervised conditional invertible transformation T and  of the overcomplete denoising autoencoder D. Correspondingly, two novel loss functions to train the parameters of SSCFlow is illustrated in this subsection. The training stage of SSCFlow can be formalized as follows: assume N training samples, xi, i = 0, 1, ..., N - 1 with their masking vector M (xi) are available. For each incomplete instance xi, a full training instance x i is formed by replacing the missing values with its imputed values x i according to its mark vector M (xi) as:

x i = xi M (xi) + x i (1 - M (xi))

(10)

where denotes the Hadamard product. The imputed values x i is randomly initialized at the first running and iteratively updated by IDM. The class label yi of incomplete instance xi

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

7

is encoded as a one-hot vector yi and the unlabelled instance's class label vector would be filled with 0.5 in each attribute to indicate the uncertainty of its category. The input feature x^i would be computed by concentrating the full training instance x i with its one-hot class vector yi according to x^i = [x i, yi]. With a full training set constructed, learning the optimal set  of parameters  of the semi-supervised conditional invertible transformation T could be accomplished by directly optimizing the log likelihood as in Eq. 3 or minimizing the following loss function L as in Eq. 11:

1 N-1

L

=

- N

(log(|detJT (x^i)|) + log(pX (x^i)) (11)

i=0

where pX (x^i) is illustrated in Eq. 7, 8 for labelled and unlabelled instances respectively.
The first cost term in Eq. 11 conditions on the class label in the features aspect introduced in the semi-supervised conditional invertible transformation since the computation of Jacobian relies on the additional class features. The second cost term in Eq. 11 refers to the semi-supervised conditional latent space with the Gaussian mixture conditioned on the class labels in the prior aspect. Such two aspects forms the semisupervised condition structure in our SSCFlow.
As for the  of the overcomplete denoising autoencoder D to be trained, SSCFlow finds the optimal  by minimizing the following loss function L:

L

=

1 -
N

N -1
M SE(xoi , x oi )

-

iCEE(yi, yi)

-

pX (x i)

i=0

(12) where xoi = xi M (xi) refers to the observed attributes of xi, x oi refers to the reconstructed observed attributes of xi, M SE denotes the mean-squared error operator, yi denotes the reconstructed class vector, CEE denotes the cross entropy error operator, and i = 1 (resp. 0) when xi is labelled (resp. unlabelled). The first cost term in Eq. 12 encourages D to reconstruct the latent representation of x that matches the training sample at the observed attributes xoi . The second cost term encourages D to reconstruct the latent representation to match the observed labels. The third cost term encourages D to reconstruct the latent representation of x with the highest density value according to the current density estimate in the latent space. The pseudocode of the training process is provided in Algorithm. 1. After training of the model, the SSCFlow imputes missing values and classify the unlabelled instances in an unified framework according to Algorithm 2.
It could be found in Algorithm 1,2 that the computational cost of training SCCFlow is O(nIterations  nEpochs  n), which is consistent with the computational cost of training MCFlow [5]. In our training process, The longest the model took to converge was 10 iterations and the epoch at each iteration is a power of 2. When employing SCCFlow for imputation and classification, the computational cost is O(n), which is relatively lower.
In order to verify the robustness and effectiveness of the SSCFlow proposed in this paper, the following experiments

Algorithm 1 Training Process.

Input:

Data:xi, i = 0, 1, ..., N - 1 with their masking vector M (xi) and their class label yi

1: Randomly initialize the missing values in xi indicated by

M (xi) to form x i. 2: Encode class label yi as one-hot vector with unlabelled

instances filled with 0.5 in each attributes to form y^i.

3: x^i = [x i, yi]

4: for iter = 1 to nIterations do

5: for n = 1 to nEpochs do

6: Forward Pass:

7:

zi = T(x^i)

8:

z^i = D (zi)

9:

x~i = T(z^i)

10:

[x t, yt] = x~i

11:

zi = T-1(x~i)

12:

Back propagation:

13: Compute loss according to Eq. 11

14: Compute loss according to Eq. 12

15: Update  and  by backpropagating loss

16: end for

17: Updating x^i with x~i according to Eq. 10

18: end for

Algorithm 2 Imputation and Classification Process.
Input:
Data: Test xt with its masking vector M (xt) 1: Randomly initialize the missing values in xt indicated by
M (xt) to form x t. 2: Encode class label of xt as unlabelled one-hot vector with
0.5 filled in each attributes to form y^t. 3: x^t = [x t, y^t] 4: zt = T(x^t) 5: z^t = D (zt) 6: x~t = T-1(z^t) 7: [x t, yt] = x~t 8: Impute the missing values of xt with x t according to Eq.
10
9: Classify xt based on the reconstructed one-hot class vector yt and the Gaussian mixture model N (µk, covk) in the latent space by yt = arg maxk N (z^t|µk, covk) + ytk

will be performed on nine real-world datasets and compared with five kinds of state-of-the-art methods.
VI. EXPERIMENTAL DESIGN In this section, the datasets used in the experiment is introduced first. Then the competing methods and some specific experimental settings are described. After that, the comparisons between SSCFlow and these competing methods are reported. Finally, the ablation experiments of SSCFlow is analysed.
A. Datasets The experimental datasets in this paper comes from UCI
repository [31], one of most commonly used dataset collection.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

8

These UCI datasets consist of 9 datasets (iris, parkinsons, Sonar, ecoli, banknote, breast, vertebral column, accent recognition, wifi localization, occupancy wine white, sat, HTRU2, avila, Sensorless) [31]. The basic characteristics of these UCI datasets are summarized in Table I.

(MICE, missForest, GAIN and MCFlow) since SSCFlow could also be confirmed as a generative classification algorithm. All experiments are implemented with the software of Pycharm 2018.

TABLE I: UCI Datasets Characteristics

Dataset
iris parkinsons
Sonar ecoli banknote breast vertebral column accent recognition wifi localization occupancy wine white sat HTRU2 avila Sensorless

Samples
150 195 208 336 1372 569 310 329 2000 8143 4898 6435 17898 20867 58509

Attributes
4 22 60 7 4 32 6 12 7 5 11 36 8 10 48

Classes
3 2 2 8 2 2 3 6 4 2 7 6 2 12 11

In order to evaluate the performance of SSCFlow and the competing methods on missing value imputation, missing values are artificially generated in all these UCI datasets with a fraction of 50%. The missing mechanism of missing completely at random (MCAR) [32] is introduced to generate artificial missing values. MCAR randomly eliminates some values in these dataset and sets them missing. That is, the possibility of missing is not influenced by values of any other attributes or the attribute itself.

B. Comparison Setting
The proposed SSCFlow is compared with XGBoost [33], missForest [4], MICE [34], MCFlow [5], GAIN [6] using fivefold cross validation on the incomplete dataset.
The comparisons between SSCFlow and other methods consist of three parts. The first comparison to report is the imputation performance between SSCFlow and the competing methods. Imputation performance on UCI datasets is measured using root mean squared error (RMSE) between the missing values introduced into the dataset an the actual vales. Then the comparisons of the classification performance are described, which is measured using classification accuracy (ACC). The mean and standard deviation of above comparisons result are reported across all folds. Finally, a qualitative comparison between SSCFlow and other methods is conducted to illustrate the interpretability of SSCFlow. In the qualitative comparison, multi-dimensional scaling (MDS) [35] is employed to reduce the dimensionality of the latent representations in SSCFlow to 2D so that they could be illustrated in figures. The same process is also employed on the imputed dataset of the competing methods to compared them with the latent representations in SSCFlow.
To be fair, a generative classification algorithm, which is implemented as Bayesian Gaussian mixture, is used for the classification after the imputation step of these MVI methods

C. Comparison Results and Discussion
Firstly, the comparisons of the imputation performance between SSCFlow and the competing methods are illustrated in Tabel II. For all datasets, SSCFlow outperforms all other methods at imputation performance from the perspective of the RMSE between artificial missing values and the ground truth values. It could be observed in Tabel II that SSCFlow produces an reduction in RMSE as compared to its base framework MCFlow [5] and the other state-of-the-art methods. Moreover, as for the datasets which MCFlow [5] misses the best imputation performance, SSCFlow also achieves the best imputation performance, which demonstrates that the additional label information introduced by the semi-supervised conditional network really helps SSCFlow to better estimate the probability density of missing values.
Then, the comparisons of the classification performance between SSCFlow and the competing methods are illustrated in Tabel. III. SSCFlow produces an improvement in acc as compared to its base framework MCFlow [5] and the other state-of-the-art methods. As for the XGBoost [33] that could directly classify the incomplete datasets, it achieves the lowest classification accuracy, which demonstrates our previous opinions that ignores the potential contribution of these missing values may degrade the classification performance. On the contrary, Tabel. III demonstrates that SSCFlow could improve the classification at the assistance of taking the distribution of missing values into account. A win-win situation is achieved in SSCFlow, which accomplishes both imputation task and classification task simultaneously and makes them promote each other.
Finally, the comparisons of the latent representations of SSCFlow and the imputed dataset with the other MVI methods in Fig. 6 demonstrate the interpretability of SSCFlow. These comparisons are illustrated by MDS in Fig. 6. It's obvious in Fig. 6 that the separability of the imputed datasets in the original data space has no significant improvement after the imputation and the biased values from the imputation methods even make it worse. However, it could be observed in Fig. 6 that, the classification boundaries could be clearly determined and naturally encodes clustering principle, which offers better interpretability. This phenomenon is owed to the additional label information, which enables SSCFlow to better estimate the probability density of missing values since the Gaussian mixture model in the latent space guide the latent representation generation with supervised information from the labelled instances. Moreover, it could be observed that SSCFlow is able to benefit from the unlabelled data to push the decision boundary to a low-density region during training, as expected. As a result, better classification performance could be achieved in SSCFlow with the clear classification boundaries.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

9

1.0

0.5

0.0

label_0 label_1

0.5

1.0 0.5 0.0 0.5

2
1
0
1 10
1.0

label_0 label_1 12

0.5

0.0

label_0

label_1

0.5

label_2 label_3

0.5 0.0 0.5 1.0

1.5

label_0

1.0

label_1

0.5

0.0

0.5 0.5 0.0 0.5 1.0

(a) MICE

0.75 0.50

label_0 label_1

0.25

0.00

0.25

0.50

0.50 0.25 0.00 0.25 0.50 0.75

1

label_0 label_1

0

1

2

10

1

2

label_0

0.5

label_1 label_2

label_3

0.0

0.5

0.5 0.0 0.5 1.0

1.0

label_0 label_1

0.5

0.0

0.5 0.50 0.25 0.00 0.25 0.50 0.75

(b) missForest

0.5

0.0

0.5

label_0

label_1

0.50 0.25 0.00 0.25 0.50 0.75

1

0

1

2

label_0 label_1

2

10

1

1.0

label_0

label_1

0.5

label_2

label_3

0.0

0.5 0.5 0.0 0.5

0.5

0.0

0.5

1.0

label_0

label_1

1.0 0.5 0.0 0.5

(c) GAIN

0.5

0.0

0.5 1.0

label_0 label_1

0.5 0.0

0.5

2

1

0

1 10

1.0

label_0

label_1

0.5

label_2 label_3

0.0

label_0 label_1 12

0.5

0.5 0.0 0.5 1.0 1.5

1.0

0.5

0.0

0.5

label_0

1.0

label_1

1.0 0.5 0.0 0.5 1.0 1.5

(d) MCFlow

3

mean_0

2 1

label_0 mean_1 label_1

0

1

2

5.0 2.5 0.0 2.5 5.0

7.5

5.0

2.5

0.0

2.5

5.0

5

0

mean_0 label_0 mean_1 label_1 5

7.5

mean_0

5.0

label_0 mean_1

2.5

label_1 mean_2

0.0

label_2

2.5

mean_3 label_3

5.0

5

0

5

2

mean_0

1 0

label_0 mean_1 label_1

1

2

3

4 20 2 4 6

(e) SSCFlow

Fig. 6: The comparisons between the latent representations of SSCFlow and the imputed dataset of the other MVI methods.

TABLE II: Comparisons of the imputation performance with RMSE (lower better) on different datasets

dataset/methods iris
parkinsons Sonar ecoli
banknote breast
vertebral column accent recognition wifi localization
occupancy wine white
sat

XGboost -

MICE 0.1834 ± 0.0063 0.1741 ± 0.0057 0.2288 ± 0.0009 0.2022 ± 0.0071 0.1746 ± 0.0004 0.1241 ± 0.0121 0.1505 ± 0.0082 0.1594 ± 0.0058 0.1438 ± 0.0024 0.1502 ± 0.0034 0.1119 ± 0.0022 0.1008 ± 0.0140

missForest 0.1908 ± 0.0004 0.1185 ± 0.0028 0.1685 ± 0.0006 0.1885 ± 0.0023 0.1694 ± 0.0001 0.0880 ± 0.0003 0.1553 ± 0.0000 0.1399 ± 0.0011 0.1415 ± 0.0001 0.1275 ± 0.0011 0.1080 ± 0.0001 0.0466 ± 0.0001

GAIN 0.2288 ± 0.0653 0.1608 ± 0.0038 0.3517 ± 0.0036 0.1901 ± 0.0073 0.2135 ± 0.0295 0.0909 ± 0.0022 0.1396 ± 0.0038 0.1517 ± 0.0065 0.1639 ± 0.0174 0.1731 ± 0.0104 0.1058 ± 0.0022 0.0589 ± 0.0017

MCFlow 0.1661 ± 0.0116 0.1183 ± 0.0020 0.1483 ± 0.0028 0.1799 ± 0.0016 0.1835 ± 0.0067 0.0710 ± 0.0007 0.1311 ± 0.0099 0.1265 ± 0.0027 0.1293 ± 0.0017 0.1538 ± 0.0049 0.0998 ± 0.0013 0.0449 ± 0.0002

SSCFlow 0.1531 ± 0.0100 0.1134 ± 0.0027 0.1423 ± 0.0010 0.1733 ± 0.0017 0.1708 ± 0.0055 0.0772 ± 0.0033 0.1234 ± 0.0028 0.1231 ± 0.0018 0.1225 ± 0.0020 0.1424 ± 0.0086 0.0988 ± 0.0015 0.0442 ± 0.0038

Since XGboost directly classifies the incomplete dataset without imputation, its imputation performance is replaced as -.

TABLE III: Comparisons of the classification performance with ACC (higher better) on different datasets

dataset/methods iris
parkinsons Sonar ecoli
banknote breast
vertebral column accent recognition wifi localization
occupancy wine white
sat

XGboost 0.8333 ± 0.0431 0.7397 ± 0.0476 0.5504 ± 0.0433 0.5863 ± 0.0323 0.7795 ± 0.0158 0.9091 ± 0.0218 0.6524 ± 0.0212 0.3746 ± 0.0471 0.8100 ± 0.0222 0.9367 ± 0.0019 0.4568 ± 0.0115 0.7945 ± 0.0388

MICE 0.8527 ± 0.0341 0.7077 ± 0.0765 0.5425 ± 0.0593 0.5942 ± 0.0380 0.7124 ± 0.0123 0.9218 ± 0.0132 0.6406 ± 0.0257 0.3924 ± 0.0843 0.8096 ± 0.0133 0.9112 ± 0.0017 0.3796 ± 0.0334 0.7583 ± 0.0248

missForest 0.8303 ± 0.0171 0.7290 ± 0.0509 0.5406 ± 0.0809 0.6088 ± 0.0275 0.7306 ± 0.0168 0.9301 ± 0.0102 0.6242 ± 0.0305 0.3952 ± 0.0813 0.8389 ± 0.0110 0.9229 ± 0.0034 0.3896 ± 0.0307 0.7643 ± 0.0269

GAIN 0.8007 ± 0.0771 0.7151 ± 0.0602 0.5325 ± 0.0656 0.5211 ± 0.0544 0.6798 ± 0.0350 0.9191 ± 0.0151 0.6308 ± 0.0386 0.3962 ± 0.0704 0.7804 ± 0.0189 0.9091 ± 0.0041 0.3869 ± 0.0297 0.7596 ± 0.0270

MCFlow 0.8400 ± 0.0320 0.7131 ± 0.0566 0.5431 ± 0.0833 0.5488 ± 0.0868 0.6997 ± 0.0262 0.9284 ± 0.0105 0.6363 ± 0.0467 0.3944 ± 0.1034 0.8084 ± 0.0126 0.9085 ± 0.0034 0.3711 ± 0.0394 0.7613 ± 0.0318

SSCFlow 0.8683 ± 0.0291 0.8024 ± 0.0496 0.6318 ± 0.1177 0.6525 ± 0.0110 0.7704 ± 0.0437 0.9837 ± 0.0082 0.6500 ± 0.0777 0.5236 ± 0.0207 0.8225 ± 0.0708 0.9326 ± 0.0017 0.4886 ± 0.0549 0.8616 ± 0.0089

D. Ablation Experiments
In this subsection, ablation experiments are conducted to demonstrate the improvement provided by the several com-

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

10

ponents of the SSCFlow. The ablation experiments divide the SSCFlow into three part: SSCFlow-IDM keeps the IDM in SSCFlow, SSCFlow-IT keeps the semi-supervised conditional invertible transformation in SSCFlow, and SSCFlow-LS keeps the semi-supervised conditional latent space in SSCFlow. The classification setting in SSCFlow-IDM follows the aforementioned imputation methods with a Bayesian Gaussian mixture classifier. The comparisons result are illustrated in Tabel IV and V.
It could be observed from Tabel. IV and V that SSCFlow achieves the best performance among these ablation methods, which demonstrates that all components of SSCFlow improve the performance. Moreover, the SSCFlow-LS achieves the worst performance among these methods, which demonstrates that the improvement would be limited if the label information only participate in the loss computation (density estimation) and do not affect the generative (imputation) process.
VII. CONCLUSION
This paper proposed a novel algorithm SSCFlow for missing values imputation and classification. SSCFlow accomplishes both imputation and classification simultaneously by estimating the conditional probability density of missing values conditioned on their labels. This paper found that the conditional probability density estimation could effectively improve the performance of both imputation and classification since the gap between these two tasks is replaced by joint weights optimization and features sharing. Moreover, it is discovered that the improvement may be limited when the label information only participate in the loss computation (density estimation) and do not affect the generative (imputation) process.
Even though the proposed method shows promising performance on various incomplete datasets, this paper has some limitations that is left for future work. For example, the covariance matrices of the Gaussian mixture model in the latent space remain fixed during training, which may limits the expressive ability of the model. Our following work would design novel algorithms to update the covariance matrices during training.
REFERENCES
[1] H. Wang, Z. Yuan, Y. Chen, B. Shen, and A. Wu, "An industrial missing values processing method based on generating model," Computer Networks, vol. 158, pp. 61­68, 2019.
[2] W. E. Strawderman, "Statistical analysis with missing data (roderick j. a. little and donald b. rubin)," SIAM Rev., vol. 31, no. 2, pp. 348­349, 1989. [Online]. Available: https://doi.org/10.1137/1031083
[3] A. Hapfelmeier, T. Hothorn, C. Riediger, and K. Ulm, "Mice: multivariate imputation by chained equations in r," International Journal of Biostats, vol. 45, no. 2, pp. 1­67, 2014.
[4] D. J. Stekhoven and P. Bu¨hlmann, "Missforest--non-parametric missing value imputation for mixed-type data," Bioinformatics, vol. 28, no. 1, pp. 112­118, 2011.
[5] T. W. Richardson, W. Wu, L. Lin, B. Xu, and E. A. Bernal, "Mcflow: Monte carlo flow models for data imputation," in 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 14 193­14 202.
[6] G. C. G. Wei and M. A. Tanner, "Gain: Missing data imputation using generative adversarial nets," in Proceedings of the 35th International Conference on Machine Learning volume 80 of Proceedings of Machine Learning Research, 10­15 Jul 2018, pp. 5689­5698.

[7] L. Dinh, D. Krueger, and Y. Bengio, "NICE: non-linear independent components estimation," in 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Workshop Track Proceedings, Y. Bengio and Y. LeCun, Eds., 2015. [Online]. Available: http://arxiv.org/abs/1410.8516
[8] L. Dinh, J. Sohl-Dickstein, and S. Bengio, "Density estimation using real NVP," in 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.
[9] D. P. Kingma and P. Dhariwal, "Glow: Generative flow with invertible 1×1 convolutions," in Proceedings of the 32nd International Conference on Neural Information Processing Systems, ser. NIPS'18. Red Hook, NY, USA: Curran Associates Inc., 2018, p. 10236­10245.
[10] I. Kobyzev, S. Prince, and M. Brubaker, "Normalizing flows: An introduction and review of current methods," IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 1­1, 2020.
[11] D. P. Kingma and M. Welling, "Auto-encoding variational bayes," in 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, Y. Bengio and Y. LeCun, Eds., 2014. [Online]. Available: http://arxiv.org/abs/1312.6114
[12] J. You, X. Ma, D. Y. Ding, M. J. Kochenderfer, and J. Leskovec, "Handling missing data with graph representation learning," in Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds., 2020.
[13] M. Mirza and S. Osindero, "Conditional generative adversarial nets," CoRR, vol. abs/1411.1784, 2014. [Online]. Available: http: //arxiv.org/abs/1411.1784
[14] B. L. Trippe and R. E. Turner, "Conditional density estimation with bayesian normalising flows," 2018.
[15] P. Izmailov, P. Kirichenko, M. Finzi, and A. G. Wilson, "Semisupervised learning with normalizing flows," in Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, ser. Proceedings of Machine Learning Research, vol. 119. PMLR, 2020, pp. 4615­4630. [Online]. Available: http://proceedings.mlr.press/v119/izmailov20a.html
[16] A. Atanov, A. Volokhova, A. Ashukha, I. Sosnovik, and D. P. Vetrov, "Semi-conditional normalizing flows for semi-supervised learning," CoRR, vol. abs/1905.00505, 2019. [Online]. Available: http://arxiv.org/abs/1905.00505
[17] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. C. Courville, and Y. Bengio, "Generative adversarial nets," in Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, Eds., 2014, pp. 2672­ 2680.
[18] V. Bogachev, Measure Theory. Springer-Verlag Berlin Heidelberg, 2007.
[19] Y. Li, S. Akbar, and J. Oliva, "Acflow: Flow models for arbitrary conditional likelihoods," in Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, ser. Proceedings of Machine Learning Research, vol. 119. PMLR, 2020, pp. 5831­5841. [Online]. Available: http://proceedings.mlr.press/v119/li20a.html
[20] Q. Shi, Y. Cheung, Q. Zhao, and H. Lu, "Feature extraction for incomplete data via low-rank tensor decomposition with feature regularization," IEEE Transactions on Neural Networks and Learning Systems, vol. 30, no. 6, pp. 1803­1817, 2019.
[21] H. Chen, P. Tino, A. Rodan, and X. Yao, "Learning in the model space for cognitive fault diagnosis," IEEE transactions on neural networks and learning systems, vol. 25, no. 1, pp. 124­136, 2013.
[22] H. Chen, F. Tang, P. Tino, and X. Yao, "Model-based kernel for efficient time series analysis," in Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ser. KDD '13. New York, NY, USA: Association for Computing Machinery, 2013, p. 392­400. [Online]. Available: https://doi.org/10.1145/2487575.2487700
[23] H. Chen, P. Tino, and X. Yao, "Cognitive fault diagnosis in tennessee eastman process using learning in the model space," Computers & Chemical Engineering, vol. 67, no. AUG.4, pp. 33­42, 2014.
[24] H. Chen, F. Tang, P. Tino, A. G. Cohn, and X. Yao, "Model metric co-learning for time series classification." in Proceedings of the TwentyFourth International Joint Conference on Artificial Intelligence. AAAI Press, 2015, pp. 3387­3394.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

11

TABLE IV: Ablation Experiments of the imputation performance with RMSE(lower better) on different dataset

dataset/methods iris
parkinsons Sonar ecoli
banknote breast
vertebral column accent recognition wifi localization
occupancy wine white
sat

SSCFlow-IDM 0.1620 ± 0.0035 0.1143 ± 0.0015 0.1443 ± 0.0006 0.1740 ± 0.0017 0.1774 ± 0.0030 0.0780 ± 0.0009 0.1266 ± 0.0020 0.1241 ± 0.0018 0.1286 ± 0.0021 0.1531 ± 0.0032 0.1012 ± 0.0006 0.0451 ± 0.0005

SSCFlow-IT 0.1630 ± 0.0080 0.1174 ± 0.0022 0.1488 ± 0.0018 0.1789 ± 0.0051 0.1777 ± 0.0060 0.0790 ± 0.0012 0.1258 ± 0.0058 0.1289 ± 0.0050 0.1261 ± 0.0012 0.1487 ± 0.0037 0.1019 ± 0.0009 0.0449 ± 0.0009

SSCFlow-LS 0.1732 ± 0.0155 0.1182 ± 0.0048 0.1498 ± 0.0013 0.1835 ± 0.0073 0.1773 ± 0.0077 0.0794 ± 0.0004 0.1264 ± 0.0042 0.1292 ± 0.0017 0.1297 ± 0.0018 0.1549 ± 0.0100 0.1012 ± 0.0016 0.0454 ± 0.0001

SSCFlow 0.1531 ± 0.0100 0.1134 ± 0.0027 0.1423 ± 0.0010 0.1733 ± 0.0017 0.1708 ± 0.0055 0.0772 ± 0.0033 0.1234 ± 0.0028 0.1231 ± 0.0018 0.1225 ± 0.0020 0.1424 ± 0.0086 0.0988 ± 0.0015 0.0442 ± 0.0038

TABLE V: Ablation Experiments of the classification performance with ACC(higher better) on different dataset

dataset/methods iris
parkinsons Sonar ecoli
banknote breast
vertebral column accent recognition wifi localization
occupancy wine white
sat

SSCFlow-IDM 0.8367 ± 0.0267 0.7258 ± 0.0485 0.5347 ± 0.0922 0.5547 ± 0.0692 0.7083 ± 0.0115 0.9306 ± 0.0161 0.6232 ± 0.0261 0.3784 ± 0.0951 0.8208 ± 0.0130 0.9053 ± 0.0066 0.3739 ± 0.0370 0.7699 ± 0.0288

SSCFlow-IT 0.8067 ± 0.0216 0.8011 ± 0.0275 0.5574 ± 0.1221 0.6411 ± 0.0259 0.7591 ± 0.0100 0.9403 ± 0.0188 0.6281 ± 0.0321 0.3905 ± 0.0681 0.8095 ± 0.0145 0.9207 ± 0.0120 0.4433 ± 0.0200 0.8276 ± 0.0459

SSCFlow-LS 0.8383 ± 0.0304 0.6463 ± 0.2325 0.5695 ± 0.0945 0.4361 ± 0.1307 0.7303 ± 0.0178 0.9354 ± 0.0171 0.5871 ± 0.0634 0.3538 ± 0.0098 0.7889 ± 0.0199 0.9144 ± 0.0087 0.3531 ± 0.0368 0.7868 ± 0.0311

SSCFlow 0.8683 ± 0.0291 0.8024 ± 0.0496 0.6318 ± 0.1177 0.6525 ± 0.0110 0.7704 ± 0.0437 0.9837 ± 0.0082 0.6308 ± 0.0386 0.5236 ± 0.0207 0.8225 ± 0.0708 0.9326 ± 0.0017 0.4886 ± 0.0549 0.8616 ± 0.0089

[25] Y. Li, B. Jiang, H. Chen, and X. Yao, "Symbolic sequence classification in the fractal space," IEEE Transactions on Emerging Topics in Computational Intelligence, vol. PP, pp. 1­10, 11 2018.
[26] Z. Gong, H. Chen, B. Yuan, and X. Yao, "Multiobjective learning in the model space for time series classification," IEEE Transactions on Cybernetics, vol. PP, no. 3, pp. 1­15, 2018.
[27] Z. Gong and H. Chen, "Model-based oversampling for imbalanced sequence classification," in the 25th ACM International, 2016.
[28] ----, "Sequential data classification by dynamic state warping," Knowledge and Information Systems, 2017.
[29] Y. Li, J. Hong, and H. Chen, "Short sequence classification through discriminable linear dynamical system," IEEE Transactions on Neural Networks and Learning Systems, pp. 3396­3408, 2019.
[30] L. Gondara and K. Wang, "Mida: Multiple imputation using denoising autoencoders," in Pacific-Asia Conference on Knowledge Discovery and Data Mining. Springer, 2018, pp. 260­272.
[31] A. Asuncion and D. Newman, "Uci machine learning repository," 2007. [32] W.-C. Lin and C.-F. Tsai, "Missing value imputation: a review
and analysis of the literature (2006­2017)," Artificial Intelligence Review, pp. 1­23, 2019. [Online]. Available: https://doi.org/10.1007/ s10462-019-09709-4 [33] T. Chen and C. Guestrin, "Xgboost: A scalable tree boosting system," in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ser. KDD '16. New York, NY, USA: Association for Computing Machinery, 2016, p. 785­794. [Online]. Available: https://doi.org/10.1145/2939672.2939785 [34] U. Garciarena and R. Santana, "An extensive analysis of the interaction between missing data types, imputation methods, and supervised classifiers," Expert Systems with Applications, vol. 89, pp. 52­65, 2017. [35] J. B. Kruskal and M. Wish, Multidimensional scaling. Sage, 1978, vol. 11.

