
# Rethinking Cross-modal Interaction from a Top-down Perspective for Referring Video Object Segmentation

[arXiv](https://arxiv.org/abs/2106.01061), [PDF](https://arxiv.org/pdf/2106.01061.pdf)

## Authors

- Chen Liang
- Yu Wu
- Tianfei Zhou
- Wenguan Wang
- Zongxin Yang
- Yunchao Wei
- Yi Yang

## Abstract

Referring video object segmentation (RVOS) aims to segment video objects with the guidance of natural language reference. Previous methods typically tackle RVOS through directly grounding linguistic reference over the image lattice. Such bottom-up strategy fails to explore object-level cues, easily leading to inferior results. In this work, we instead put forward a two-stage, top-down RVOS solution. First, an exhaustive set of object tracklets is constructed by propagating object masks detected from several sampled frames to the entire video. Second, a Transformer-based tracklet-language grounding module is proposed, which models instance-level visual relations and cross-modal interactions simultaneously and efficiently. Our model ranks first place on CVPR2021 Referring Youtube-VOS challenge.

## Comments



## Source Code

Official Code



Community Code

- [https://paperswithcode.com/paper/rethinking-cross-modal-interaction-from-a-top](https://paperswithcode.com/paper/rethinking-cross-modal-interaction-from-a-top)

## Bibtex

```tex
@misc{liang2021rethinking,
      title={Rethinking Cross-modal Interaction from a Top-down Perspective for Referring Video Object Segmentation}, 
      author={Chen Liang and Yu Wu and Tianfei Zhou and Wenguan Wang and Zongxin Yang and Yunchao Wei and Yi Yang},
      year={2021},
      eprint={2106.01061},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```

## Notes

Type your reading notes here...

