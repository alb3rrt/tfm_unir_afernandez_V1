Tight High Probability Bounds for Linear Stochastic Approximation with Fixed Stepsize

arXiv:2106.01257v1 [stat.ML] 2 Jun 2021

Alain Durmus  ENS Paris-Saclay alain.durmus@ens-paris-saclay.fr

Eric Moulines Ecole Polytechnique and HSE University eric.moulines@polytechnique.edu

Alexey Naumov HSE University anaumov@hse.ru

Sergey Samsonov HSE University
svsamsonov@hse.ru

Kevin Scaman INRIA, DI/ENS, PSL Research University
kevin.scaman@gmail.com

Hoi-To Wai The Chinese University of Hong Kong
htwai@se.cuhk.edu.hk

Abstract
This paper provides a non-asymptotic analysis of linear stochastic approximation (LSA) algorithms with fixed stepsize. This family of methods arises in many machine learning tasks and is used to obtain approximate solutions of a linear system A¯ = ¯b for which A¯ and ¯b can only be accessed through random estimates {(An, bn) : n  N}. Our analysis is based on new results regarding moments and high probability bounds for products of matrices which are shown to be tight. We derive high probability bounds on the performance of LSA under weaker conditions on the sequence {(An, bn) : n  N} than previous works. However, in contrast, we establish polynomial concentration bounds with order depending on the stepsize. We show that our conclusions cannot be improved without additional assumptions on the sequence of random matrices {An : n  N}, and in particular that no Gaussian or exponential high probability bounds can hold. Finally, we pay a particular attention to establishing bounds with sharp order with respect to the number of iterations and the stepsize and whose leading terms contain the covariance matrices appearing in the central limit theorems.

1 Introduction

This paper provides a detailed analysis of Linear Stochastic Approximation (LSA) schemes which aim at finding a solution  for a linear system of the form A¯ = ¯b. In particular, we analyze LSA with a fixed stepsize  > 0 which consists in defining a sequence of estimates {n : n  N} for  by the recursion

n+1 = n - {An+1n - bn+1} , n  N ,

(1)

where {(An, bn) : n  N} is a sequence of i.i.d. random variables used as proxy for A¯  Rd×d and ¯b  Rd which are typically unknown. This class of algorithms and the corresponding setting

have a long history and important applications in signal processing such as channel equalization and

echo cancellation [3, 22]. It has renewed interests in machine learning and computational statistics

Authors listed in alphabetical order.

Preprint. Under review.

especially for least-square estimation, Reinforcement learning (RL) and Q-learning [4, 7, 38, 35]. The recursion (1) has already been studied in depth in several works which derive asymptotic [31, 22, 6, 3] and non-asymptotic [33, 25, 2, 19, 20, 5, 23, 34, 9, 12] guarantees.
However, in most cases, there is a consistent gap between these two types of analyses. While asymptotic analysis gives important insights on the qualitative convergence of (1) based on statistical key quantities of the problem on hand, they do not provide finite-time convergence, or high probability bounds, necessary to obtain non-asymptotic confidence sets, see [26, 10] and the references therein. On the other hand, non-asymptotic studies are in general too coarse and lose significant statistical information in their derivation. Further, their upper bounds are generally loose when used in predicting the actual performance of LSA. We aim at filling this gap and provide conditions on {(An, bn) : n  N} ensuring tight high probability bounds on the sequence {n : n  N}.
This problem has been addressed in several contributions but at the expense of strong conditions on the sequence {(An, bn) : n  N}. [13] provided concentration bounds for non-linear stochastic algorithms under a log-Sobolev condition which turns out to be hard to verify for most applications except for the Euler-Maruyama discretization scheme applied to Stochastic Differential Equation. [27] derived concentration inequalities but assuming that the innovations in (1) are uniformly bounded. In contrast, we aim at giving simple and mild conditions ensuring high probability bounds. More precisely, one of our key contributions (Theorem 1) is to show that under mild conditions on the sequence {(An, bn) : n  N}, for any   (0, 1), n  N and u  Sd-1,

 P |u(n - )|  c{ uu + }

log(1/)

+

c{n

+

p20}-

1 p0

 1- ,

(2)

where   (0, 1), c > 0 is a constant independent of n, , , and p0 = o(-1/4). In the above,  is the unique solution of the Lyapunov equation which naturally appears in central limit theorems for LSA with diminishing stepsize. In addition, we show that the bound we get is tight with respect to  and  in the case where we only assume that -E[A1] = -A¯ is Hurwitz. Indeed, we provide counterexamples illustrating that for a fixed stepsize  and under the conditions that we consider, logarithmic dependence in 1/ cannot hold in (2) but only a polynomial one. Regarding the dependence with respect to , we extend [28] and show that for  small enough, {n : n  N} admits a unique stationary distribution  and establish a central limit theorem for this family of distribution as   0 at rate  and with asymptotic covariance matrix  appearing in (2).
Finally, our proofs rely on a new analysis of product of matrices, extending the recent work of [17]. In particular, we establish conditions ensuring uniform bounds in n of the p-th moments of Yn · · · Y1, where {Yn : n  N} is a sequence of independent matrices whose expected values have a spectral radius less than 1. In comparison to existing results, the main challenge that we address here is that the random matrices {An : n  N} are not assumed to be almost surely symmetric.
The paper is organized as follows. Section 2 formally discusses the assumptions on LSA for our analysis. Section 3 presents the moment bound for product of random matrices. Using this result, Section 4 shows the high probability concentration inequality (2) and Section 5 shows the tightness of the bounds by deriving a central limit theorem for LSA.

Notations Denote N = N \ {0} and N- = Z \ N. Let d  N and Q be a symmetric positive

definite d × d matrix. For x  Rd, we denote x Q = {xQx}1/2. For brevity, we set x = x Id .

We denote A Q = max x Q=1 Ax Q, and the subscriptless norm A = A I is the standard

spectral norm. We denote the condition number of Q as Q = -mi1n(Q)max(Q). We denote Sd-1 =

{x  Rd| x = 1}. Let A1, . . . , AN be d-dimensional matrices. We denote

j =i

A

=

Aj

. . . Ai

if i  j and with the convention

j =i

A

=

Id

if

i

>

j.

We

say

that

a

centered

random

variable

(r.v.) X is sub-Gaussian with variance factor 2 and we denote X  SG(2) if for all   R,

log E[eX ]  22/2. We define the Wasserstein distance of order 2 between two probabilities

measure µ and  on Rd as W2(µ, ) = inf(µ,) R2d x - y 2d(x, y), where (µ, ) is the set of probability measures on (R2d, B(R2d)) with marginals µ and  respectively. Denote by P2(Rd)

the set of all probability measures on Rd with the finite second moment.

2

2 Linear Stochastic Approximation: Setting and Assumptions

Consider the LSA recursion (1) with a deterministic initial point 0. The main assumption required in this paper is as follows:
A1. {(An, bn)}nN is an i.i.d. sequence satisfying the following conditions.
(i) E[b1] = ¯b and there exists Cb > 0 such that, for any u  Sd-1, u(b1 - ¯b)  SG(C2b). (ii) There exists CA > 0 such that A1  CA almost surely. (iii) The matrix -A¯ = -E[A1] is Hurwitz, i.e. for any eigenvalue  of A¯, Re() > 0.

Both conditions A1-(i), (ii) are standard in analysis of LSA, e.g., in [11, 34, 24]. Meanwhile, A1(iii) guarantees the existence of a unique solution  to A¯ = ¯b. It is also a sufficient and necessary

condition to  [18,

for the solution of the ordinary differential equation Lemma 4.1.2]. The same kind of result holds for the

dtis=cre-teA¯sytstteomconndv+e1rg-eendxp=on-entAi¯allndy.

Proposition 1. Assume that -A¯ is a Hurwitz matrix. Then there exists a unique positive definite matrix Q satisfying the Lyapunov equation A¯Q + QA¯ = I. In addition, setting

a = Q -1/2 ,

and

 = (1/2)

A¯

-2 Q

Q

-1 ,

(3)

then for any   [0, ], we get

I - A¯

2 Q

 1 - a. If in addition  

Q 2 then 1 - a  1/2.

This result is well known but its proof can be found in Appendix A.1 for above proposition implies that the discrete system converges exponentially as

condm+1pleteness.Q

The (1 -

a)n/2 0d for   (0, ).

Recall that the aim of this paper is to derive high probability bounds on u{n - } for any n  N, u  Sd-1. Below, we present a counterexample to show that under only A 1, if  > 0 is fixed, then there exists p¯ > 0 such that limn+ E[ n -  p] = + for p  p¯. As a corollary, it is impossible to obtain exponential high probability bounds for { n -  : n  N}.

Example 1. Consider (1) with d = 1 taking bn = 0 for any n  N and for {An : n  N} an i.i.d. sequence of biased Rademacher r.v.s with parameter qA  (1/2, 1):

An =

1 -1

with probability qA , with probability 1 - qA .

(4)

This choice is associated with  = 0 and corresponds to the recursion: n =

n k=1

(1

-

Ak )0 ,

for some 0 = 0. For any p  1 and   (0, 1), we have by definition,

E [|n|p] = {qA(1 - )p + (1 - qA)(1 + )p}n|0|p .

Using the lower bounds (1 - )p  1 - p and (1 + )p  1 + p + p(p - 1)2/2, we get for any p  1 and   (0, 1),

E [|n|p]  {1 - p[(2qA - 1) - (p - 1)(1 - qA)/2]}n|0|p .

If   (0, 1) is fixed, then for any p > p¯q, = 1 + 2(2qA - 1)/[(1 - qA)], we have limn+ E [|n|p] = +. On the other hand, if   (0, 2(2qA - 1)/(1 - qA)), then limn+ E[n2 ] = 0. Therefore {n : n  N} converges in distribution to the Dirac measure at 0 which corresponds to the unique stationary distribution of this sequence as a Markov chain. In such a case, this distribution admit p moments for any p  0.

However, this result is specific to this particular case and does not hold if only A1 holds. Consider {n : n  N} defined by (1) with {An : n  N} given in (4) and {bn : n  N} be an
i.i.d. sequence of zero-mean Gaussian random variables with unit variance independent of {An : n  N}. We show in Appendix A.2 that there exists 2, such that for any   (0, 2,], the
Markov chain {n : n  N} admits a unique invariant distribution  for any  > 0. Further, for any   (0, 2,] there exists p  1 such that R ||pd() = + for any p  p.

It is, however, possible to obtain any p-th moment uniform bound for { n -  : n  N} by strengthening A1-(iii) to:

3

A2. There exist a~  (0, 1), ~ > 0 and a definite positive d-dimensional matrix Q~ such that almost surely, for any   (0, ~], I - A1 Q~ < 1 - a~.

Examples such that A2 holds include regularized linear regression, where we take A1 = I + a1a1 , for some  > 0 and under the assumption that a1 is bounded almost surely. The LSA recursion (1) approximates the solution to (I + E[a1a1 ]) = ¯b which is guaranteed to have a unique solution.
On the other hand, examples where A 2 does not hold are common. For this, we consider TD(0) learning with linear function approximation. For a Markov Reward Process with X as the state space, P : X × X  [0, 1] as the transition probability, R : X  R as the reward function, and   (0, 1) as a discount factor, TD(0) learning is described as in (1) with

An = (xn){(xn) - (xn)}, bn = R(xn)(xn) ,

(5)

where  : X  Rd is a feature map. A typical setting is when xn is drawn from the stationary distribution of P and xn  P(xn, ·). It is easy to verify A1 provided that (x) , R(x) are bounded for all x  X [36]. However, A2 is violated as An is only rank-one.

Our next endeavor is to establish moment estimates on the product below:

(m:)n = ni=m(I - Ai) , m, n  N, m  n .

(6)

We also define its expected value as G(m:)n = E[(m:)n] = (I - A¯)n-m+1. The above product naturally appears after re-centering the LSA recursion (1). For any n  N,

n -  = I - An {n-1 - } + n , n = bn - ¯b - {An - A¯} .

(7)

An easy induction implies that

n -  = ~n(tr) + ~n(fl) ,

~n(tr) = (1:n){0 - } ,

~n(fl) = 

n j=1

(j+)1:n j

.

(8)

The decomposition (8) highlights the two sources of error in the estimation of  by {n : n  N} which will be separately tackled: {~n(tr) : n  N} corresponds to the transient (or bias) term and {~n(fl) : n  N} to the fluctuation term. Both errors are controlled by the product of matrices (m:)n, thereby motivating the study of the moment bound on (1:n) as we present next.

3 Moment and High-probability Bounds for Products of Random Matrices

Recall from Proposition 1 that the expected value G(1:n) = E[(1:n)] decays exponentially with n, here we expect a similar phenomenon to hold for the moment bound of (1:n). Precisely, in this section, we show that if p is fixed, then there exists p > 0 such that for any   (0, p], the p-th moment of (m:)n decays exponentially with n - m.

To facilitate our discussions, we introduce the following notations. For B  Rd×d, we denote

by ((B))d=1 its singular values. For p  1, the Schatten p-norm is denoted by B p =

{

d =1

p(B)}1/p.

For

p,

q



1

and

random

matrix

X,

we

write

X p,q = {E[ X qp]}1/q.

In the following, we present the main technical result on the product of general random matrices. The proof is based on the framework introduced in [17].

Proposition 2. Let {Y :   N} be an independent sequence and P be a positive definite matrix.

Assume that for each  Y - E[Y] P  

 N there exist m  almost surely. Define

(Z0n, 1=) andn=0

> 0 such that E[Y] Y = YnZn-1, for n

2 P


 1

1 - m and and starting

from Z0. Then, for any 2  q  p and n  1,

n

Zn

2 p,q



P

(1 - m + (p - 1)2)

P 1/2Z0P -1/2

2 p,q

,

(9)

=1

where P = m-i1n(P )max(P ).

4

Proof of Proposition 2 . Let 2  q  p. Consider the following decomposition Zn = YnZn-1 = (Yn - E[Yn])Zn-1 + E[Yn]Zn-1, Therefore, we obtain for any n  N,

fP (Zn) = An + Bn , An = fP ((Yn - E[Yn])Zn-1) , Bn = fP (E[Yn])fP (Zn-1) ,

where fP : Rd×d  Rd×d is defined for any B  Rd×d by fP (B) = P 1/2BP -1/2. Since E[An|Bn] = 0, [17, Proposition 4.3] (see Proposition 10 in Appendix B) implies that

fP (Zn)

2 p,q



Bn

2 p,q

+

(p

-

1)

An

2 p,q

.

(10)

It remains to bound the two terms on the right hand side. To this end, we use [16, Theorem 6.20] which implies that for any B1, B2  Rd×d,

B1B2 p,q  B1 B2 p,q .

(11)

As a result and using that for any B  Rd×d, B P = fP (B) , and Yn - E[Yn] P  n we get

An p,q = E E

fP (Yn - E[Yn])fP (Zn-1)

q p

1/q

Yn - E[Yn]

q P

fP (Zn-1)

q p

1/q  n fP (Zn-1) p,q .

(12)

Similarly, applying

E[Yn]

2 P

 1 - mn

Bn

2 p,q

=

E

E

fP (E[Yn])fP (Zn-1)

q p

2/q

E[Yn]

q P

fP (Zn-1)

q p

2/q

 (1 - mn)

fP (Zn-1)

2 p,q

.

(13)

Combining (12) and (13) in (10) yields for any n  N,

fP (Zn)

2 p,q



(1 - mn + (p -

1)n2 )

fP (Zn-1)

2 p,q



n i=1

(1

-

upon using (11) which implies that

mn Zn

+ (p - 1)n2 ) fP (Z0) p,q = P -1/2fP (Zn)P

2 p,q

.

1/2

The p,q 

prooPf

is then completed fP (Zn) p,q.

AInso-rdAe¯ritsoHbouurwnditz,(1a:np) pulsyiinnggPPrrooppoossiittiioonn21,

we identify the yields E[Y]

latter

2 Q

=

wIit-h YA¯=2QI

-A,    1 - a.

1, Y0 = I. Further, A

1-(ii) ensures that almost surely,

Y - E[Y] Q =  A - A¯ Q  2Q CA = bQ .

Therefore, (9) holds with m = a and  = bQ. Noting that as I p = d1/p, we obtain the following corollary.

Corollary 1. Assume A1-(ii)-(iii). Then, for any   [0, ], 2  q  p, and n  N,

E1/q (1:n) q  (1:n) p,q  Qd1/p(1 - a + (p - 1)b2Q2)n/2 ,

(14)

where  was defined in (3), and

bQ = 2Q CA .

(15)

Note that Corollary 1 shows supnN E[ (1:n) p] < + for any   (0, p,], where

p, =   a/(2b2Q(p - 1)) .

(16)

This kind of condition relating the choice  with the order p of the moment to be bounded is necessary as illustrated in Example 1. The above corollary further leads to the high-probability bound:

Corollary 2. Assume A 1-(ii)-(iii). Then, for any   (0, ) where  was defined in (3),   (0, 1) and n  N, with probability at least 1 - ,

(1:n)  Q exp -(an - 2b2Qn)/2 + bQ 2n log(d/) .

Proof. The result follows from combining Corollary 1 with p = q and Lemma 1 in Appendix B applied with A = (- log(Q)+an+b2Q2n)/2, B = 2b2Qn/2 and C = d, p0 = 2, p1 = +.

5

The result which we obtain in Corollary 2 is tight with respect to , as illustrated via the following example that continues from Example 1.
Example (Continuation of Example 1). Consider {n : n  N} defined by (1) with {An : n  N} given in (4) and bn = 0 for any n  N. Define

q() = qA log

1+ 1-

- log(1 + ),

¯q = sup{¯ > 0 : q() > 0,    (0, ¯)} . (17)

Note that q()  (2qA - 1) as   0. Therefore since qA > 1/2, {¯ > 0 : q() > 0 for any   (0, ¯)} =  and ¯q is well-defined. Consider also ~q() = q() log-1[(1 + )/(1 - )]. Then, we show in Appendix B that for any ¯  e-2n~q(), 1 and   (e-n~2q()/(qA(1-qA))-2-1 log(n), 1),

P

n  exp

-q()n + log

1+ 1-

n log(1/¯) 2

 ¯ , (18)

P

n  exp

-q()n + log

1+ 1-

nqA(1

-

qA) log(1/)

+

n

log(n) 2

  . (19)

Note that the bound given by (18) and (19) shows that the tail distribution associated with n behaves as a log-normal one. Indeed, if  is a zero-mean one dimensional Gaussian distribution with unit variance, then an easy computation shows that for any  > 0, P(e  t)  (22)-1/2 log-1(t) exp(-(22)-1t2) as t  , therefore, to have P(e  t)   for a small
 > 0, then t has to be of order exp( log(1/)).

We conclude the section with a complementary result of Corollary 1 that does not require A1-(ii):

Proposition 3. Assume A1-(iii), A1 -A¯  SG(CA) for some CA > 0. Then, for any   (0, ) where  was defined in (3), 2  q  p, and n  N,

E1/q (1:n) q  (1:n) p,q  Qd1/p(1 - a + q(p - 1)(bQ)22)n/2 ,

(20)

where bQ = 2Q CA.

The proof is similar to that of Proposition 2 and it can be found in Appendix B.

4 Finite-time High-probability Bounds for LSA

Relying on the results established in Section 3 and the decomposition (8), we derive in this section high probability bounds on u{n - } for any n  N and u  Sd-1, where {n : n  N} is defined in (1). We begin our study with the transient term ~n(tr) defined in (8). The proof of the following statement is given in Appendix C.1.
Proposition 4. Assume A1 and let p0  2. Then, for any n  N,   (0, p0,), where p0, is defined in (16), u  Sd-1 and   (0, 1) it holds with probability at least 1 -  that
|u(1:n)(0 - )|  Qd1/p0 (1 - a/4)n 0 -  -1/p0 ,
where a was defined in (3).

Proposition 4 only provides a polynomial high probability bound with respect to . This is due to the fact that only polynomial moments of (1:n) up to a maximal order are uniformly bounded in the number of iterations n.

We now turn on the fluctuation term ~n(fl) defined in (8). We note that under A 1, the sequence

{n : n  N} defined in (7) is i.i.d.. From this observation and following [12], we consider the

decomposition

n

~n(fl) = 

(j+)1:nj = Jn(,0) + Hn(,0) ,

(21)

j=1

6

where {(Jn(,0), Hn(,0)) : n  N} are defined by induction for n  0 as:

Jn(+,10) = I - A¯ Jn(,0) + n+1 , Hn(+,10) = (I - An) Hn(,0) - (An+1 - A¯)Jn(,0) ,

J0(,0) = 0 , H0(,0) = 0 .

(22)

The latter recurrence can be written as

n

Jn(,0) = 

G(j+)1:nj ,

j=1

n

Hn(,0) = -

(j+)1:n(Aj - A¯)Jj(-,10) .

j=1

Note that Jn(,0) is a linear statistics of the random variables {j : j  {1, . . . , n}} which are
centered and i.i.d. under A 1. In our next results, we show that Jn(,0) is the leading term as the stepsize   0. Denote for any n  N and  > 0, the covariance matrix of Jn(,0) as

n = Cov(Jn(,0)) .

(23)

Proposition 5. Assume A 1. Then for any n  N,   (0, ], where  is defined in (3), u  Sd-1 and   (0, 1), it holds with probability at least 1 - ,

uJn(,0) < D1 {unu} log(2/) +  1 + log(1/(a))D2 log3/2(2/) ,

(24)

 where D1 = 60 3e4/3 and D2 is defined in (50).

The proof of Proposition 5 is postponed to Appendix C.2.

We analyze further the covariance associated with Jn(,0) and its dependence with respect to n and

. First,  = 

no kt=e0thGa1t:kfor aGny1:kisth(e0,uni2q, ue],so{lutnion:

n  N} converges to of the Ricatti equation



as

n





where

A¯ + A¯ - A¯A¯ =  , with  = E[11 ] .

(25)

Indeed, using Proposition 1, we easily get that for any n  0,

n -   2

G1:k 2   a-1Q  (1 - a)n .

(26)

k>n

We now give an expansion of  with respect to . It is well-known that as   0,  converges to , the unique solution of the Lyapunov equation (see [32, Lemma 9.1])

A¯ + A¯ =  .

(27)

Our next result states this convergence is of the order of the stepsize .

Proposition 6. Assume that A1-(iii) holds. Then, for any   (0, ], where  is defined in (3),

 -  Q  a-1 A¯A¯ Q , where  and  are defined in (25) and (27) respectively and a is given in (3).

The proof is given in Appendix C.3. The last step in bounding ~n(fl) is to consider Hn(,0). We proceed similarly to (22) and consider the decomposition Hn(,0) = Jn(,1) + Hn(,1), where {(Jn(,1), Hn(,1)) : n  N} are defined by induction for n  0 as:

Jn(+,11) = (I - A¯)Jn(,1) - (An+1 - A¯)Jn(,0), Hn(+,11) = (I - An+1)Hn(,1) - (An+1 - A¯)Jn(,1),

J0(,1) = 0 , H0(,1) = 0 .

(28)

In our next results, we bound each term of this decomposition separately.
Proposition 7. Assume A1 and let p0  2. Then, for any n  N,   (0, p0,), where p0, is defined in (16), u  Sd-1 and   (0, 1/2), with probability at least 1 - 2, it holds

uJn(,1) < eD3 log2(1/) , uHn(,1) < D4p20-1/p0 ,

(29)

where D3 and D4 are given in (58) and (61), respectively.

7

The proof of Proposition 7 is postponed to Appendix C.4. Now we are ready to combine the previous bounds and to state the main result of this section.
Theorem 1. Assume A 1 and let p0  2. Then, for any n  N,   (0, p0,), where p0, is defined in (16), u  Sd-1 and   (0, 1/4), with probability at least 1 - 4, it holds

-1/2|u(n - )| < D1 {uu} log(2/) + 1/2q(1)(, ) + (1 - a/4)n(1)(, ) , (30) where  is the unique solution of (25), D1 = 603e4/3, a is defined in (3),

q(1)(, ) = eD3 log2(1/) + 1 + log(1/a)D2 log3/2(2/) + D4p20-1/p0 , (1)(, ) = D1 a-1Q  log(2/) + Qd1/p0 0 -  -1/2-1/p0 ,

(31)

where Q and  are defined in (3) and (25) respectively.

Proof. The proof follows from the decomposition
u(n - ) = u(1:n)(0 - ) + uJn(,0) + uJn(,1) + uHn(,1) , where Jn(,0), Jn(,1) and Hn(,1) are defined in (22)-(28), the union bound and Proposition 4, Proposition 5, (26) and Proposition 7.

We now discuss the high probability bound (30). First, note that the term (1)(, ), and in particular the initial condition vanishes exponentially fast in the number of iterations n. In addition, q(1)(, ) and (1)(, ) are of order -1/p0 as   0 and therefore (30) provides polynomial high probability bounds on LSA. However, this conclusion is expected as illustrated in Example 1. Finally, the discussion of (30) with respect to  is postponed to the next section.
Under A2 we can provide a better bound for Hn(,1).
Proposition 8. Assume A1 and A2. Then, for any n  N,   (0,   ~), where  is defined in (3), u  Sd-1 and   (0, 1/2), with probability at least 1 - 2, it holds

uJn(,1) < eD3 log2(1/) , uHn(,1) < eD5 log2(1/) ,

(32)

where D3 and D5 are given in (58) and (62) respectively.

As a result, we can establish exponential high probability bounds with respect to . Theorem 2. Assume A 1 and A 2. Then, for any n  N,   (0,   ~), u  Sd-1 and   (0, 1/4), with probability at least 1 - 4, it holds

-1/2|u(n - )| < D1 {uu} log(2/) + 1/2q(2)(, ) + (1 - a~)n/2(2)(, ) , where D1 = 603e4/3,  is solution of (25),

q(2)(, ) = e(D3 + D5) log2(1/) + 1 + log(1/a~)D2 log3/2(2/) ,

(2)(, ) = D1 a~-1Q~  log(2/) + Q1~/2 0 -  -1/2 ,

(33)

where  is defined in (25).

Proof. The proof follows the lines of Theorem 1 with Proposition 8 used instead of Proposition 7.

5 Optimality of the derived bounds with respect to : analysis of (n)nN as a Markov chain
In this section, we study the sequence {n : n  N} defined in (1) as a Markov chain. This perspective will allow us to show that the bounds that we derived in Theorem 1 are near-Berstein high probability bounds with respect to the stepsize . Denote by R the Markov kernel associated with {n : n  N}. First, we show that if  is small enough then R is geometrically ergodic with respect to the Wasserstein distance of order 2 denoted by W2 and give a representation of its stationary distribution as an infinite sum.
8

Theorem 3. Assume A1. Then, for any   (0, 2,), where 2, is defined in (16), R admits a unique stationary distribution   P2(Rd) and for any n  N,

W22(Rn, )  Qd(1 - a/2)n

~ -  2d(~) .

(34)

Rd

Further, if {(Ak, bk) : k  N-} is any sequence of i.i.d. random variables with the same distribution as (A1, b1), then the following limit exists almost surely and in L2 and has distribution :

 ()

=

lim
n-

n(,)

,

1

n(,) = 

k:0bk-1 ,

k=n

0
k:0 = (Id - Ai) .
i=k

(35)

The proof is postponed to Appendix E.1. Based on Theorem 1, we easily get concentration bounds for the family of distributions { :   (0, 2,)} around .
Theorem 4. Assume A1 and let p0  2. Then, for any   (0, p0,), where p0, is defined in (16), u  Sd-1 and   (0, 1/4), with probability at least 1 - 4, it holds

-1/2|u( () - )| < D1

{uu} log(2/) + 1/2[a-1/2 

A¯A¯

1/2 Q

+

q (1) (,

)]

,

(36)

where  is the unique solution of (27), D1 = 60 3e2/3, a is defined in (3), and q(1)(, ) in (31).

Proof. The proof follows from Theorem 1, the Portmanteau theorem [21, Theorem 13.16], and the fact that convergence in W2 implies weak convergence.

Our results is only polynomial in  and we cannot expect improving this dependency as illustrated

in Example 1 for fixed . The leading term in (36) as   0 is D1{uu}. In our next result, we establish a central limit theorem for the family ( ())(0,2,] where  plays the role of the asymptotic covariance matrix. As a result, (36) is a Bernstein-type high probability bound with
respect to  and therefore (36) is sharp. Define for any   (0, 2,],

~ () = -1/2{ () - } .

(37)

Theorem 5. Assume A1. Then, the family {~ () :   (0, 2,]} converges in law as   0 to a zero-mean Gaussian random variable with covariance matrix  defined by (27).

Note that this result was established in [28, Theorem 1] for general stochastic approximation schemes but under stronger conditions on the sequence {n : n  N}. In particular, it is assumed that the distribution of 1 admits a density with respect to the Lebesgue measure. We relax
this condition and provide a new proof for this result. In particular, our strategy to establish The-
orem 5 is to consider the decomposition (21) of {n : n  N} with 0 = 0, since in such case n = ~n(fl) for any n  N. Define {Jn(,) : n  N-} by

1

Jn(,) = 

Gk:0k-1 ,

k=n

0
Gk:0 = (I - A¯) .
i=k

(38)

Note that for any n  N, -(n, +1) has the same distribution as n() starting from 0 = 0 and Jn(,0) as J-(n,+1). In contrast to Jn(,0), J-(n,+1) admits a limit in L2 and almost surely denoted by J (,). Then, we get for any u  Sd-1,   (0, 2,], bounded and Lipschitz functions f : R  R, with Lipschitz constant smaller than 1, by the Lebesgue dominated convergence theorem

|E[f (u~ (,))] - E[f (-1/2uJ (,))]|

=

lim
n+

|E[f (-1/2u[-(n, +1)

-

])]

-

E[f (-1/2uJ-(n,+1))]|

=

lim
n+

|E[f (-1/2u[n()

-

])]

-

E[f

(-1/2 u Jn(,0) )]|



lim sup
n+

E[|-1/2 u Hn(,0) |]

.

Using the decomposition Hn(,0) = Jn(,1) + Hn(,1), where {(Jn(,1), Hn(,1)) : n  N} are defined

in (28) and plugging the bounds provided by Proposition 11 and Proposition 12 in Appendix C.4

shows that

lim sup |E[f (u~ (,))] - E[f (-1/2uJ (,))]| = 0 .
0

9

Therefore by the Cramer Wold device and the Portmanteau theorem [21, Theorem 13.16], Theorem 5 follows from the next result. Proposition 9. Assume A1. Then, for any u  Sd-1, {-1/2uJ (,) :   (0, 2,]} converges in distribution to the zero-mean Gaussian distribution with variance uu where  is given in (27).
The proof is postponed to Appendix E.2.
6 Conclusion
In this paper, we provided a novel non-asymptotic analysis of LSA algorithms with fixed stepsize. For any   (0, 1), we obtain bounds on the sequence { n -  : n  N} that holds with probability at least 1 - . The bounds are proven to be tight with respect to the stepsize, and we show that these bounds necessarily have polynomial dependency in . Importantly, our results do not require the matrices An to be symmetric but only Hurwitz, which enables one to apply them to various scenarios such as reinforcement learning. Future work includes extending our high probability bounds to a larger panel of random noise, e.g., with heavy tailed distribution, Markovian dependency, as well as Polyak-Ruppert averaging.
References
[1] R. B. Ash. Information theory. Tracts in Pure & Applied Mathematics. John Wiley & Sons Inc, 1966. ISBN 0470034459,9780470034453.
[2] F. Bach and E. Moulines. Non-strongly-convex smooth stochastic approximation with convergence rate o(1/n). In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc., 2013. URL https://proceedings.neurips.cc/paper/2013/file/7fe1f8abaad094e0b5cb1b01d712f708-Paper.pdf.
[3] A. Benveniste, M. Métivier, and P. Priouret. Adaptive algorithms and stochastic approximations, volume 22. Springer Science & Business Media, 2012.
[4] D. P. Bertsekas and J. N. Tsitsiklis. Parallel and distributed computation: numerical methods. 2003.
[5] J. Bhandari, D. Russo, and R. Singal. A finite time analysis of temporal difference learning with linear function approximation. In Conference On Learning Theory, pages 1691­1692, 2018.
[6] V. S. Borkar. Stochastic Approximation: A Dynamical Systems Viewpoint. Cambridge University Press, 2008.
[7] L. Bottou, F. E. Curtis, and J. Nocedal. Optimization methods for large-scale machine learning. Siam Review, 60(2):223­311, 2018.
[8] S. Boucheron, G. Lugosi, and P. Massart. Concentration inequalities: A nonasymptotic theory of independence. Oxford University Press, 2013.
[9] S. Chen, A. Devraj, A. Busic, and S. Meyn. Explicit mean-square error bounds for monte-carlo and linear stochastic approximation. In International Conference on Artificial Intelligence and Statistics, pages 4173­4183. PMLR, 2020.
[10] X. Chen, J. D. Lee, X. T. Tong, Y. Zhang, et al. Statistical inference for model parameters in stochastic gradient descent. The Annals of Statistics, 48(1):251­273, 2020.
[11] G. Dalal, B. Szörényi, G. Thoppe, and S. Mannor. Finite sample analyses for TD(0) with function approximation. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.
[12] A. Durmus, E. Moulines, A. Naumov, S. Samsonov, and H.-T. Wai. On the stability of random matrix product with markovian noise: Application to linear stochastic approximation and td learning, 2021.
[13] N. Frikha, S. Menozzi, et al. Concentration bounds for stochastic approximations. Electronic Communications in Probability, 17, 2012.
10

[14] S. Guo, F. Qi, and H. M. Srivastava. Necessary and sufficient conditions for two classes of functions to be logarithmically completely monotonic. Integral Transforms and Special Functions, 18(11):819­826, 2007. doi: 10.1080/10652460701528933. URL https://doi.org/10.1080/10652460701528933.

[15] P. Hall and C. Heyde. Martingale Limit Theory and Its Application. Academic Press, 1980.

[16] F. Hiai and D. Petz. Introduction to Matrix Analysis and Applications. Universitext. Springer International Publishing, 2014. ISBN 9783319041506.

[17] D. Huang, J. Niles-Weed, J. A. Tropp, and R. Ward. Matrix concentration for products. arXiv preprint arXiv:2003.05437, 2020.

[18] B. Jacob and H. Zwart. Linear Port-Hamiltonian Systems on Infinite-dimensional Spaces. Number 223 in Operator Theory: Advances and Applications. Springer, 2012. ISBN 978-3-03480398-4. doi: 10.1007/978-3-0348-0399-1. 10.1007/978-3-0348-0399-1.

[19] P. Jain, S. M. Kakade, R. Kidambi, P. Netrapalli, and A. Sidford. Accelerating stochastic gradient descent for least squares regression. In Conference On Learning Theory, pages 545­ 604. PMLR, 2018.

[20] P. Jain, D. Nagaraj, and P. Netrapalli. Making the last iterate of sgd information theoretically optimal. In A. Beygelzimer and D. Hsu, editors, Proceedings of the Thirty-Second Conference on Learning Theory, volume 99 of Proceedings of Machine Learning Research, pages 1752­ 1755, Phoenix, USA, 25­28 Jun 2019. PMLR.

[21] A. Klenke. Probability Theory: A Comprehensive Course. Universitext. Springer London, 2013. ISBN 9781447153603.

[22] H. Kushner and G. G. Yin. Stochastic approximation and recursive algorithms and applications, volume 35. Springer Science & Business Media, 2003.

[23] C. Lakshminarayanan and C. Szepesvari. Linear stochastic approximation: How far does constant step-size and iterate averaging go? In A. Storkey and F. Perez-Cruz, editors, Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics, volume 84 of Proceedings of Machine Learning Research, pages 1347­1355. PMLR, 2018.

[24] O. Macchi and E. Eweda. Second-order convergence analysis of stochastic adaptive linear filtering. IEEE Transactions on Automatic Control, 28(1):76­85, 1983.

[25] A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on optimization, 19(4):1574­1609, 2009.

[26] Y. Nesterov and J.-P. Vial. Confidence level solutions for stochastic programming. Automatica, 44(6):1559­1568, 2008.

[27] B. Pepin. Concentration inequalities for additive functionals: A martingale approach. Stochastic Processes and their Applications, 135:103­138, 2021.

[28] G. Pflug. Stochastic Minimization with Constant Step-Size: Asymptotic Laws. SIAM Journal on Control and Optimization, 24(4):655­666, 1986. doi: 10.1137/0324039. URL https://doi.org/10.1137/0324039.

[29] I. Pinelis.

An Approach to Inequalities for the Distributions of Infinite-

Dimensional Martingales, pages 128­134.

Birkhäuser Boston, Boston, MA,

1992. ISBN 978-1-4612-0367-4. doi: 10.1007/978-1-4612-0367-4_9. URL

https://doi.org/10.1007/978-1-4612-0367-4_9.

[30] I. Pinelis. Optimum Bounds for the Distributions of Martingales in Banach Spaces. The Annals of Probability, 22(4):1679 ­ 1706, 1994. doi: 10.1214/aop/1176988477. URL https://doi.org/10.1214/aop/1176988477.

[31] B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM journal on control and optimization, 30(4):838­855, 1992.

[32] A. S. Poznyak. Advanced Mathematical Tools for Automatic Control Engineers: Deterministic Techniques. Elsevier, Oxford, 2008.

[33] A. Rakhlin, O. Shamir, and K. Sridharan. Making gradient descent optimal for strongly convex stochastic optimization. In Proceedings of the 29th International Coference on International Conference on Machine Learning, pages 1571­1578, 2012.

11

[34] R. Srikant and L. Ying. Finite-Time Error Bounds For Linear Stochastic Approximation and TD Learning. In Conference on Learning Theory, 2019.
[35] R. S. Sutton. Learning to predict by the methods of temporal differences. Machine Learning, 3(1):9­44, Aug 1988. ISSN 1573-0565. doi: 10.1007/BF00115009.
[36] J. N. Tsitsiklis and B. Van Roy. An analysis of temporal-difference learning with function approximation. IEEE Transactions on Automatic Control, 42(5):674­690, May 1997. ISSN 2334-3303. doi: 10.1109/9.580874.
[37] C. Villani. Optimal transport : old and new. Grundlehren der mathematischen Wissenschaften. Springer, Berlin, 2009. ISBN 978-3-540-71049-3.
[38] C. J. Watkins and P. Dayan. Q-learning. Machine learning, 8(3-4):279­292, 1992.

A Proofs of Section 2

A.1 Proofs of Proposition 1

The existence and uniqueness of Q follows from [32, Lemma 9.1, p. 140]. Regarding the second statement, note that for any x  Rd \ {0}, we have

x(I

-

A¯)Q(I xQx

-

A¯)x

=

1

-



x2 xQx

+

2

xA¯QA¯x xQx

.

Hence, we get that for all   [0, ],

1

-



x2 xQx

+

2

xA¯QA¯x xQx

1-

Q

-1 + 2

A¯

2 Q



1

-

(1/2)

Q

-1 .

The proof is completed using that for any matrix A¯  Rd×d, A¯ Q  1Q/2 A¯ .

A.2 Proof for Example 1

The existence and uniqueness of the stationary distribution  is a consequence of Theorem 3 noting
that A1 is satisfied for the particular case that we consider. We now show the second statement. Let
  (0, 2,). First, note that since b1 is a zero-mean Gaussian random variables with unit variance independent of A1, we have for any p  1,

2p
E[12p] =
k=0

2p k

E[02p-k]E[(1 - A1)2p-k]E[bk1 ]

p
=

2p 2k

E[02(p-k)]E[(1 - A1)2(p-k)]E[b21k]  E[02p]E[(1 - A1)2p] .

k=0

This shows that taking 0 with distribution  that if R ||2pd() < +, then it is necessary that
E[(1 - A1)2p]  1. However, using that E[(1 - A1)2p] = {qA(1 - )2p + (1 - qA)(1 + )2p} and (1 - )2p  1 - 2p and (1 + )2p  1 + 2p + 2p(2p - 1)2/2, we get for any p  1, E[(1 - A1)2p]  {1 - 2p[(2qA - 1) - (2p - 1)(1 - qA)/2]}, therefore E[(1 - A1)2p]  1 does not hold for 2p > p¯q, = 1 + 2(2qA - 1)/[(1 - qA)].

B Technical and supporting results for Section 3

Proposition 10 ([17, Proposition 4.3]). Consider two random matrices X, Y  Rd×d that satisfy E[Y|X] = 0. Then for 2  q  p,

where Cp = p - 1.

X+Y

2 p,q



X

2 p,q

+

Cp

Y

2 p,q

,

12

Lemma 1. Let A  R, B > 0, C  1, p0, p1  R such that 1  p0  p1 < +, and X a real random variable satisfying, for any p  [p0, p1],

E[|X|p]  C exp(-Ap + Bp2) .

(39)

Then, for all   (0, 1], we have, with probability at least 1 - ,

|X|  exp -A + Bp0 + 2 B log(C/) + log(C/)/p1 ,

(40)

with the convention c/ = 0 for c > 0. In addition if (39) is satisfied for any p  p0, then with probability at least 1 - ,

|X|  exp -A + Bp0 + 2 B log(C/) .

Proof. Note that by the monotone convergence theorem, it is sufficient to show (39). By Markov's inequality, we have, for any t > 0 and p  [p0, p1],

P(|X|  t)  E[|X|p]/tp  C exp (-p(log(t) + A - Bp)) .

(41)

Taking t = exp(-A + 2Ba) for a  R and maximizing over p  [p0, p1], we obtain

P(|X|  exp(-A + 2Ba))  C exp (-Bp(2a - p))  C exp (-B(a)) ,

(42)

where

(a)

=

max
p[p0 ,p1 ]

p(2a-p)

=

(2p0a-p20 ½) (-,p0](a)+(a)2½(p0,p1)(a)+(2p1a-p21)½[p1,+)(a)

.

Note that for any t  R, the inverse of  is given by

(t)

=

½ ½ p20 + t
2p0

(-,p20](t) + t1/2

(p20,p21)(t) +

p21 + 2p1

t

½[p21,+)(t)

.

For  > 0, taking a = (log(C/)/B) gives

P(|X|  exp[-A + 2B(log(C/)/B)])   .

The

proof

then

follows

from

the

fact

that

for

any

t



R,

(t)



p0/2

+

 t

+

t/(2p1).

Proof of (18) and (19)

Let   (0, ¯q). Note that by definition of {n : n  N} with (4), for any n  N,

n = (1 - )Nn (1 + )n-Nn ,

n
where Nn = ½{1}(Zk) .
k=1

Then, for any  > 0, we get

P n  e-n = P (log(n)  -n) = P Nn log

1- 1+

 -n - n log(1 + )

=P

Nn  n log-1

1+ 1-

{ + log(1 + )}

=P

Nn - qAn  -n qA - log-1

1+ 1-

{ + log(1 + )}

. (43)

Let ,q = -1 [qA log {(1 + )/(1 - )} - log(1 + )]. Note that with the condition,   (0, ¯q), ,q > 0 and therefore for any   (0, ,q),

x, = qA - log-1

1+ 1-

{ + log(1 + )}  (0, ~q()) ,

(44)

We now show (18). From (43), it follows using Hoeffding's inequality that for any   (0, ,q),

P n  e-n  e-2nx2, .

(45)

13

Hence, for ¯  (e-2n~2q(), 1), there exists x  (0, ~q()) such that e-2nx2 = ¯ given by x = log(1/¯)/2n, which corresponds by (44) to

 = -1 qA -

log(1/¯)/(2n)

log

1+ 1-

- -1 log(1 + )  (0, ,q) .

This completes the proof of (18) using (45).

We now show (19). Using [1, Lemma 4.7.2] and (43), it holds that for any   (0, ,q),

P n  e-n  exp(-nKL(qA - x,|qA) - 2-1 log(n)) ,

(46)

where for any q~  (0, 1),

KL(q~|qA) = q~log(q~/qA) + (1 - q~) log((1 - q~)/(1 - qA)) .

Note that for any q~  (0, 1), q~  qA, using log(1 + t)  t for any t > -1, we get

KL(q~|qA)  (qA - q~)2/(qA(1 - qA)) .

Therefore, plugging this result into (46) yields for any   (0, ,q),

P n  e-n  exp(-nx2,/(qA(1 - qA)) - 2-1 log(n)) .

(47)

Hence, for   (e-n~2q()/(qA(1-qA))-2-1 log(n), 1) there exists x  (0, ~q()) such that e-nx2/(qA(1-qA))-2-1 log(n) = , given by x = 2-1 log(n) + qA(1 - qA) log(1/)/n, which corresponds by (44) to

 = -1{qA -

2-1 log(n) + qA(1 - qA) log(1/)/n} log

1+ 1-

- -1 log(1 + ) .

This completes the proof of (19) using (47).

Proof of Proposition 3. It suffices to repeat the argument of Corollary 1. We need a version of

Proposition 2 for the product Zn =

n =0

Y

where

{Y

:



 N} are an independent and for

each , q  N there exist m  (0, 1) and ,q > 0 such that

E[Y]

2 Q

 1 - m and E1/q[

Y -

E[Y] qQ]  ,q. We use notations of An, Bn from Proposition 2. Applying independence of Zn-1

and Yn and E1/q [

Y - E[Y]

q Q

]



,q

we

estimate

An p,q  E

Yn - E[Yn]

q Q

fQ(Zn-1)

q p

1/q
 n,q fQ(Zn-1) p,q.

(48)

The bound for

Bn

2 p,q

remains

the

same:

Bn

2 p,q



(1 - mn)

fQ(Zn-1)

2 p,q

.

Combining

this inequality with (48) and (10) yields for any n  N,

fQ(Zn)

2 p,q



(1 - mn + (p -

1)n2 ,q )

fQ(Zn-1)

2 p,q

upon using (11) which

 ni=1(1 - mi implies that Zn

+ (p - p,q =

1)i2,q )

fQ(Z0)

2 p,q

.

Q-1/2fQ(Zn)Q1/2

The proof is then completed p,q  Q fQ(Zn) p,q. Fi-

nally, yields

it remains

E[Y]

2 Q

take =

IY-=A¯

I - A,   1, Y0 = I. As -A¯

2 Q



1

-

a.

Further,

since

A -

is A¯

Hurwitz, applying Proposition  SG(CA) we get by Lemma

1 3

E1/q [

Y - E[Y]

q Q

]

=

E1/q [

A - A¯

qQ]  2Qq CA = bQq .

Taking m = a and ,q = bQq we get the claim of the proposition.

C Proofs of Section 4
For ease of presentation, we drop in this section the dependence of Jn(,0), Hn(,0), Jn(,1), Hn(,1) with respect to  and simply write Jn(0), Hn(0), Jn(1), Hn(1), respectively. We denote A~ n = An - A¯.
14

C.1 Proof of Proposition 4
Let n  N,   (0, p0,], u  Sd-1 and   (0, 1). Under A1, applying Corollary 2 with p = p0 yields
E1/p0 [|u(1:n)(0 - )|p0 ]  E1/p0 [ (1:n) p0 ] 0 -   Qd1/p0 (1 - a + (p0 - 1)b2Q2)n/2 0 -  .
Since   a/(2b2Q(p0 - 1)), using (1 - t)1/2  1 - t/2 for t  [0, 1], we get E1/p0 [|u(1:n)~0|p0 ]  Qd1/p0 0 -  (1 - a/4)n .
Applying Markov's inequality easily completes the proof.

C.2 Proof of Proposition 5
Let n  N,   (0, p0,], u  Sd-1 and   (0, 1). Using (22) and applying Rosenthal's inequality [30, Theorem 4.1]2 for sum of centered independent random variables we get for any p  2,

E[|uJn(0)|p]  (60e)ppp/2{unu}p/2 + p60pppE

max |uG+1:n|p
=1,...,n

.

Applying Lemma 5, we obtain for any p  2,

E[|uJn(0)|p]  (60e)ppp/2{unu}p/2 + (9{1 + log[1/(a)]}Q C2)p/2p60pp3p/2 , (49)
where the constant C is given in (63). Applying Markov's inequality, we get for any p  2, c1, c2 > 0,

P(|uJn(0)|  c1{unu}1/2 + c2)  {c1{unu}1/2 + c2}-p (60e)ppp/2{unu}p/2 + (9{1 + log[1/(a)]}Q C2)p/2p60pp3p/2  (60e)ppp/2c-1 p + (9{1 + log[1/(a)]}Q C2)p/2p60pp3p/2c-2 p .

Taking p = 3 log (2/), c1 = D1(log (2/))1/2 and c2 =  1 + log(1/(a))D2 log3/2(2/)

yields the statement, where

 D1 = 60 3e4/3,

D2 = 5403e1/31Q/2 C .

(50)

C.3 Proof of Proposition 6
Lemma 2. Assume that A1-(iii) holds. Then, for any   (0, ], where  is defined in (3),  -  Q  a-1 A¯A¯ Q ,
where  and  are defined in (25) and (27) respectively and a is given in (3).

Proof. Let   (0, ]. By definition, (25) and (27) imply A¯( - ) + ( - )A¯ - A¯( - )A¯ = A¯A¯ ,

which writes

 -  - (I - A¯)( - )(I - A¯) = 2A¯A¯ .

This implies, by Proposition 1,

 -  Q  (1 - a)  -  Q + 2 A¯A¯ Q ,

Rearranging terms completes the proof.

2Note that the specific universal constants CR,1 = 60e and CR,2 = 60 are not given in the statement, but a close inspection of the proof provide the given estimates.

15

C.4 Proof of Proposition 7

Proposition 7 is a direct consequence of the following statements.

Proposition 11. Assume A 1. Then, for any n  N,   (0, ), where p0, is defined in (16),

u  Sd-1 and p  2,

E[ uJn(1) p]  Dp3pp2p ,

(51)

where D3 is given in (58). Moreover, for any   (0, 1) with probability at least 1 - ,

uJn(1)  eD3 log2(1/) .

(52)

Proposition 12. Assume A1 and let p0  2. Then, for any n  N,   (0, p0,), where p0, is

defined in (16), u  Sd-1,

E[ uHn(1) p0 ]  Dp40 p0 p02p0 ,

(53)

where D4 is given in (61). Moreover, for any   (0, 1) with probability at least 1 - ,

uHn(1)  D4p20-1/p0 .

(54)

Proof of Proposition 11. First, we note that (28) implies

n-1

n

Jn(1) = 2

S+1:n, with S+1:n =

(I - A¯)n-k-1A~ k(I - A¯)k-1- .

=1

k=+1

It is easy to check that the sequence (2S+1:n, F+1:n)n=-11 is a martingale-difference, where F+1:n =  (Aj, bj)j{+1,...,n} . We may use Burkholders's inequality [15, Theorem 2.10] to
get

E[ uJn(1) p]  (36p)p2pE

n-1
(uS+1:n)2

p/2

.

(55)

=1

Using the Minkowski inequality,

E[ uJn(1) p]  (36p)p2p

n-1
E2/p [(u S+1:n  )p ]

p/2
.

=1

(56)

Denote V+1 = uS+1:n. Note that by Assumption A1-(ii) and Lemma 1, (I - A¯)n-k-1A~ k(I - A¯)k-1-  Q CA(1 - a)(n--2)/2. Applying [29, Theorem 3]3, we get for any t  0

P V+1  t  2 exp -t2/ 22Q C2A(n - )(1 - a)n--2 .

Using Lemma 3,

E2/p [

V+1

p]



 22

C2A

2Q(n

-

)(1

-

a)(n--2)p.

(57)

Since S+1:n and  are independent,

E[|uS+1:n|p]  E[ V+1 p] sup E[|u|p] .
uSd-1

Assumption A1-(i), Lemma 3 and Lemma 5 imply, that for any u  Sd-1, E[|u|p]  pp/2 Cp(22)p/2 .

Combining this bound with (57),

E[|uS+1:n|p]  pp(22)p CpA pQ Cp (n - )p/2(1 - a)(n--2)p/2 .

(58)

This inequality and (56) imply

E[ uJn(1) p]  p2p2p(722)p CpA pQ Cp

n-1
(n - )(1 - a)(n--2)

p/2

 pDp3p2p,

where

D3

=

=1 (72 2)

CA

Q

C

a-1(1

-

a)-1.

(59)

3with X = Rd equipped with the Euclidean norm · . Note that x , x  Rd is twice Gateaux differentiable and X  D(A1, A2) with A1 = A2 = 1.

16

Now the equation (29) follows from Markov's inequality. Namely, for any c1 > 0 it holds

P

uJn(1)  c1D3



p Dp3 p2p cp1 p Dp3

= c-1 pp2p .

Taking p = log (1/) and c1 = e log2 (1/), we obtain (29).

Proof of Proposition 12. With the decomposition (28), we represent

n

uHn(1) = -

u(+)1:nA~ J(-1)1.

=1

Using Minkowski's inequality,

n

E1/p[ uHn(1) p]  

E1/p[ u(+)1:nA~ J(-1)1 p] .

=1

Now, using the independence of (+)1:n, A~ , J(-1)1, and Item (ii),

E1/p[ u(+)1:nA~ J(-1)1 p]  E1/p u(+)1:nA~ 

sup E1/p |vJ(-1)1|

vSd-1

 2 CA E1/p[ (+)1:n p] sup E1/p |vJ(-1)1| .
vSd-1

(60)

Hence, applying Corollary 2 to E1/p[ (+)1:n p], and (59) to supvSd-1 E1/p[ vJ(-1)1 p],

E1/p[ uHn(1) p]  2 CA Qd1/pD32p2 n 1 - a + (p - 1)b2Q2 (n-)/2 .
=1

Since p0 - 1  a/(2b2Q), from the previous estimate it follows

E1/p0 [ uHn(1) p0 ]  2 CA Qd1/p0 D32p20

n
(1 - a)(n-)/2

=1
 4 CA Qd1/p0 D3p20/a .

Hence,

E[ uHn(1) p0 ]  Dp40 p0 p02p0 , where D4 = 4 CA Qd1/p0 D3/a .

(61)

Using Markov's inequality, we get with probability at least 1 - ,

uHn(1)  D4p20/1/p0 .

C.5 Proof of Proposition 8

The proof is along the same lines as the proof of Hn(1) in Appendix C.4, with the better bound for E1/p[ (+)1:n p]. For reader's convenience, we provide the proof below. Starting with equation (60), we note that under A2,
E1/p[ (+)1:n p]  Q~ 1 - a~ n- .
We also apply (59) to supvSd-1 E1/p[ vJ(-1)1 p]. Then

E1/p[ uHn(1) p]  2

n

Q~ CA D32p2

1 - a~ n-  D5p2 ,

=1

where we have defined

D5 = 2 Q~ CA D3/a~ .

(62)

Now the equation (32) follows from Markov's inequality.

17

D Concentration results for sub-Gaussian random variables
Lemma 3. Random variable X  SG(2) for some  > 0 if and only if for all t  0 the condition P(|X|  t)  2 exp{-t2/(22)} holds. In addition, in such a case, for any p  2, we have
E[|X|p]  2e(2/e)p/2pp/2p.

Proof. The first statement is well-known, see for example [8, Theorem 2.1]. We now show the

second statement. By the Fubini theorem, E[|X|p] = p

+ 0

up-1P(|X

|

>

u)

du,

we

get



E[|X|p] = 2p

up-1e-u2/(22) du = p2p/2p(p/2),

0

using the change of variable t = u2/(22). Now, using an upper bound (p/2) 
(p/2)(p-1)/2e1-p/2 (see e.g. [14, Theorem 2]), and p1/2  2p/2, we finally get E[|X|p]  2e(2/e)p/2pp/2p .

Lemma 4. Let {X :   N} be a sequence of random variables such that X  SG(2) for any   N and some 2 > 0. Then for any p  2,

E

max

|X|/

p
1 + log 

 3pppp/2 .

=1,...,n

Proof. Set ak = (1 + log k)1/2 for k  N. Using the Fubini's theorem, E[||p] =

p

+ 0

up-1P(||

>

u)du,

the

union

bound,

and

Lemma

3,

we

get

+

E

max {|Xk|p
k=1,...,n

/apk}

=p

0

up-1P max |Xk|  uak du
k=1,...,n

+

 2pp + p

up-1P max |Xk|  uak du

2

k=1,...,n

+

n

 2pp + p

up-1 P |Xk|  uak du

2

k=1

+

n

 2pp + 2p

up-1 exp -u2a2k/(22) du

2

k=1

+

n

 2pp + 2pp

yp-1 exp -y2/2

k-y2/2 dy

2

k=1



2pp

+

2pp 3

+
yp-1 exp -y2/2
2

dy



2pp

+

2 p p 2p/2-1 3

(p/2)

.

Using (p/2)  (p/2)(p-1)/2e1-p/2 (see [14, Theorem 2]), we get

E k=m1,a..x.,n{|Xk|p/apk}  2pp + 2pp(p+1)/2e1-p/2/(32) . Since pe-p/2  e-1/2 and 2p  4pp/2,

E k=m1,a..x.,n{|Xk|p/apk}  ppp/2 4 + 2e1/2/(32) < 9ppp/2 .

Lemma 5. Assume A1. Then, for any n  N and v  Sd-1, vn defined by (7) is a sub-Gaussian random variable with parameter

C2 = 2 C2b +8 C2A  2 .

(63)

18

In addition, for any n  N, p  2, u  Sd-1 and   (0, ), it holds

E

max
=1,...,n

|uG(+)1:n

|p



9Qp C2{1 + log[1/(a)]} p/2 ,

where , a and Q are defined in (3).

Proof. First we prove (63). Using the representation (7), for any   R,

E exp vn  E exp v(bn - ¯b - {An - A¯})  E1/2 exp 2v(bn - ¯b) E1/2 exp 2v(A¯ - An) .
Note that A 1-(ii) implies v(A¯ - An)  2 CA  . Hence, using the Hoeffding inequality, v(A¯ - An)  SG(4 C2A  2). Combining this result with A1-(i),
E exp vn  exp 2 C2b exp 42 C2A  2 ,
yielding the first statement of the lemma. To prove the second part, let us denote v = (I - A¯)n-u/ (I - A¯)n-u  Sd-1. Using Proposition 1,

E[ max
=1,...,n

|uG(+)1:n|p]

=

E[ max
=1,...,n

|v|p

G(+)1:nu

p]



pQ/2

E[ max
=1,...,n

|v|p(1

-

a)p(n-)/2 ]

 pQ/2E

max
=1,...,n

(1

+

|v|p log (n -  +

1))p/2

p/2
max(1 + log(x + 1))e-ax)
x>0

p/2

 pQ/2(9 C2 p)p/2

max[(1 + log(x + 1))e-ax]
x>0

,

where in the last inequality we used Lemma 4. Set f (x) = (1 + log(x + 1))e-cx with c = a  1 over x > 0. First, note that f (x) = e-cx(1/(1 + x) - c - c log(x + 1)) < 0 for all x > 1/c - 1, and thus the maximum is attained for x  [0, 1/c - 1]. Moreover, for any x  1/c - 1, we have f (x)  1 + log(1 + x)  1 + log(1/c), leading to the desired result.

E Proof of Section 5

E.1 Proof of Theorem 3

Let   (0, 2,) and 1, 2  P2(Rd). By [37, Theorem 4.1], there exists a couple of random variables 0(1), 0(2) such that W22(1, 2) = E[ 0(1) - 0(2) 2] independent of {(An, bn) : n  N}. We introduce then a synchronous coupling between 1Rn and 2Rn as follows. Let {(n(1), n(2)) : n  N} starting from 0(1) and 0(2) respectively and for all n  0,

n(1+)1 = (I - An+1)n(1) + bn+1 n(2+)1 = (I - An+1)n(2) + bn+1 .

(64)

Since for all n  0, the distribution of (n(1), n(2)) belongs to (1Rn, 2Rn), by definition of the Wasserstein distance we get for any n  N,

W2(1Rn, 2Rn)  E1/2[ n(1) - n(2) 2] = E1/2[ 1:n[0(1) - 0(2)] 2]  D2(1 - a/2)n/2W2(1, 2) , (65)
where we have used Corollary 1 for the last inequality. By [37, Theorem 6.16], the space P2(Rd) endowed with W2 is a Polish space. Then, (1Rn)n0 is a Cauchy sequence and converges to a limit 1  P2(Rd), limn+ W2(1Rn, 1 ) = 0. We show that the limit 1 does not depend

19

on 1. Assume that there exists 2 such that limk+ W2(2Rn, 2 ) = 0. By the triangle inequality

W2(1 , 2 )  W2(1 , 1Rn) + W2(1Rn, 2Rn) + W2(2 , 2Rn) .

Thus by (65), taking the limits as n  +, we get W2(1 , 2 ) = 0 and 1 = 2 . The limit is thus the same for all initial distributions and is denoted by . Moreover,  is invariant for R. Indeed for all k  N, W2(R, )  W2(R, Rn) + W2(Rn, ), Using (65) again, we get taking n  +, W2(R, ) = 0 and R = . The fact that  is the unique stationary distribution is straightforward by contradiction and using (65). (34) is a simple consequence of (65) taking 2 = .

It remains to show that  () is well-defined and has distribution . Since {(Ak, bk) :

k  N-} is i.i.d., n-1 E1/2[ n - n+1 2] =

n-1 E1/2[ n:0bn-1 2] =

n-1 E1/2[ n:0 2]E1/2[ bn-1 2] and therefore A1-(i) combined with Corollary 1 ensures that

this series is finite and therefore (n)nN- defined in (35) is a Cauchy sequence almost surely and

in L2 which ensures its convergence. Finally, assume now that {(Ak, bk) : k  N-} is independent

of {(Ak, bk) : k  N}. To conclude it is then sufficient to note that if 0 =  (), then 1 has the

same distribution as  () by definition of the recursion (1).

E.2 Proof of Proposition 9

Consider a sequence {n : n  N} converging to 0 such that for any n  N n  (0, 2,], and let u  Sd-1. For ease of notation, we simply denote G(kn:0) = G(k:0n). Note that

1

n-1/2uJ (n,) =

Mn,k ,

k=-

Mn,k = 1n/2uGk(n:0)k-1 .

By [15, Theorem 3.6], it is sufficient to show that

sup
k1

Mn,k

-P
n+

0

(66)

Mn2,k

-P
n+

uu

k1

(67)

sup E[sup Mn2,k] < + .

(68)

nN k1

First, by Markov inequality and Proposition 1 and A1-(i), we have for any  > 0 that

P(sup Mn,k  )  -4E[sup Mn4,k]  -42n E[ G(kn:0) 4 k-1 4]

k1

k1

k1

 -42nE[ 0 4]2Q (1 - an)-2(k-1)  -42nE[ 0 4]2Q(1 - (1 - an)2)-1 ,
k1

which shows that (66) holds.

Denote by n the unique solution of the Ricatti equation (25) with   n. We get by Lemma 2 that there exists C  0 such that for any n  N,

 - n  Cn .

Therefore, we obtain that

| Mn2,k - uu|  n| ((G(kn:0))u)[k-1k-1 - ](Gk(n:0))u| + Cn .

k1

k1

Then, to establish (67), it remains to show that

n|

((G(kn:0) ) u) [k-1 k-1

-

 ](G(kn:0) ) u|

-P
n+

0

.

k1

(69)

20

This follows from A1-(ii)-(i) and Proposition 1 which shows that

E[| k1((G(kn:0))u)[k-1k-1 - ](G(kn:0))u|2]

=

E |((G(kn:0))u)[k-1k-1 - ](G(kn:0))u|2

k1



G(kn:0) 4E[ k-1k-1 -  2]  E[ 00 -  2]2Q(1 - (1 - an)2)-1 .

k1

Therefore,

lim
n+

2nE[|

((G(kn:0))u)[k-1k-1 - ](G(kn:0))u|2] = 0 ,

k1

which completes the proof of (69).

Finally, we show (68) which follows from A1-(ii)-(i) and Proposition 1,

E[sup Mn2,k] 

E[Mn2,k] = n ((G(kn:0))u)E[k-1k-1](G(kn:0))u

k1

k1

k1

 nE[00 ]

G(kn:0) 2  E[00 ]Q/a .

k1

21

