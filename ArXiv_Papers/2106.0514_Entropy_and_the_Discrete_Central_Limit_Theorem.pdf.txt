Entropy and the discrete central limit theorem

Lampros Gavalakis 

Ioannis Kontoyiannis 

June 2, 2021

arXiv:2106.00514v1 [math.PR] 1 Jun 2021

Abstract
A strengthened version of the central limit theorem for discrete random variables is established, relying only on information-theoretic tools and elementary arguments. It is shown that the relative entropy between the standardised sum of n independent and identically distributed lattice random variables and an appropriately discretised Gaussian, vanishes as n  .
Keywords -- Central limit theorem, entropy, Fisher information, relative entropy, Bernoulli part decomposition, lattice distribution, convolution inequality
2020 Mathematics subject classification -- 60F05; 94A17; 60E15

1 Introduction

Suppose X1, X2, . . . are zero-mean, independent and identically distributed (i.i.d.), continuous

random variables, with finite variance 2. The study of the entropy h(S^n) of the standardised

sums

S^n

=

1 n

n i=1

Xi

has a long history, starting with the 1959 work of Linnik [21].

Recall

that the entropy of a continuous random variable Y with density f is h(Y ) = - f log f , where

`log' denotes the natural logarithm.

Barron [3] showed that, as n  ,

h(S^n)



h(Z )

=

1 2

log(2e2),

(1)

where Z  N (0, 2) is a zero-mean Gaussian random variable with variance 2. Barron's proof

combined earlier results by Brown [6] together with an integral form of de Bruijn's identity for

the entropy and a convolution inequality for the Fisher information [29, 4].

The fact that the Gaussian has maximal entropy among all random variables with variance

no greater than 2 invites an appealing analogy between (1) and the second law of thermody-

namics. Indeed, this analogy was carried further when it was shown that the entropy h(S^n) in

fact increases to the

maximum

entropy

h(Z) =

1 2

log(2e2

).

This was

first established

using

analytical tools by Artstein et al. [1], and later using information-theoretic techniques in [32]

and [22].

Department of Engineering, University of Cambridge, Trumpington Street, Cambridge CB2 1PZ, U.K. Email: lg560@cam.ac.uk. L.G. was supported in part by EPSRC grant number RG94782.
Statistical Laboratory, DPMMS, University of Cambridge, Centre for Mathematical Sciences, Wilberforce Road, Cambridge CB3 0WB, U.K. Email: yiannis@maths.cam.ac.uk. I.K. was supported in part by the Hellenic Foundation for Research and Innovation (H.F.R.I.) under the "First Call for H.F.R.I. Research Projects to support Faculty members and Researchers and the procurement of high-cost research equipment grant," project number 1034.

1

Let D(f g) = f log(f /g) denote the relative entropy between two probability densities f, g on R. For a continuous random variable Y with density f and variance 2 < , we write D(Y ) for D(f ), the relative entropy between f and the Gaussian density  with the same mean and variance as Y . Then we always have,

D(Y

)

=

1 2

log(2e2)

-

h(Y

),

(2)

which implies that the convergence of h(S^n) to h(Z) is equivalent to,

D(S^n)  0, as n  .

(3)

In view of Pinsker's inequality, 2

f -g

2 TV

 D(f

g) [9, 20], the relative entropy

convergence

in (3) is enough, e.g., to recover the central limit theorem (CLT) in the sense of total variation

convergence.

Note that, not only do the results in (1) and (3) not rely on the CLT, but they imply a

strong form of the CLT, established without using any of the usual probabilistic techniques.

In the case of discrete random variables {Xn}, there is no immediately obvious starting point

for identifying a corresponding connection between the CLT and the entropy of the standardised sums S^n; for example, the distribution of S^n is orthogonal to the Gaussian and the relative

entropy between them is always infinite. The main contribution of this work is the development

of natural discrete analogs of the "entropic" CLTs in (1) and (3).

For i.i.d. random variables {Xn}, write Sn for the partial sums X1 + X2 + · · · + Xn, so that

S^n

=

1 n

Sn.

If

the

{Xn}

are

continuous

with

finite

variance,

then

by

the

elementary

scaling

property of the entropy [8], (1) can equivalently be written,

lim
n

 h(Sn) - log n

=

1 2

log(2e2).

(4)

The entropy of a discrete random variable Y with probability mass function p on a countable
set A is H(Y ) = - yA p(y) log p(y). Our first result is the analog of (4) for lattice random variables. We say that Y has a lattice distribution with span h > 0 if its support is a subset of
{a + kh : k  Z} for some a  R; the span h is maximal if it is the largest such h.

Theorem 1.1 (Entropy convergence) If Sn, n  1, are the partial sums of a sequence {Xn} of i.i.d. lattice random variables with finite variance 2 and maximal span h, then:



lim
n

H(Sn) - log

n h

=

1 2

log(2e2).

(5)

Since the discrete entropy does not scale in the same way as the continuous entropy (e.g., H(Sn) = H(S^n)), the equivalence between the convergence in (5) and a discrete version of the entropic CLT D(S^n)  0 is no longer immediate. Nevertheless, it is possible to establish a result analogous to that in (3) in the discrete case, as shown in Theorem 1.2 below.
For discrete random variables X, Y with probability mass functions p, q, respectively, on
the same countable set A, the relative entropy D(p q) between p and q is defined as D(p q) =
xs p(x) log(p(x)/q(x)), where the sum is interpreted as the Lebesgue integral of log(p/q) with respect to the probability measure induced by p on R.

2

Suppose Y is lattice random variable maximal span h, values in A = {a + kh : k  Z}, mean µ, and finite variance 2. We write D(Y ) for the relative entropy D(p q) between the
probability mass function p of Y and the probability mass function q of a Gaussian random variable Z  N (µ, 2) quantised on A as,

a+(k+1)h

q(a + kh) =

(x)dx, k  Z,

(6)

a+kh

where  is the N (µ, 2) density. Observe that, by definition, D(Y +c) = D(Y ) for any constant c.

Theorem 1.2 (Discrete entropic CLT) If S^n, n  1, are the standardised sums of a sequence {Xn} of i.i.d. lattice random variables with finite variance, then:

D

1n

n
(Xi - µ)

= D(S^n)  0,

as n  .

(7)

i=1

As in the continuous case, Pinsker's inequality combined with the triangle inequality for the total variation norm imply a strong version of the CLT: Taking µ = 0 without loss of generality, let Z  N (0, 2) and let Zn be the quantised Gaussian as in the definition of D(S^n). Then,

S^n - Z TV 

1 2

D(S^n

)

+

Zn - Z

TV  0,

as n  ,

since the first term vanishes by Theorem 1.2 and the second term vanishes by the definition of Zn. Alternatively, the fact that S^n - Zn TV  0 implied by Theorem 1.2 readily translates to local-CLT-like results.

Paper outline and proof ideas. In the end of this Introduction we discuss the intriguing

connection between the CLT and Shannon's entropy power inequality. In Section 2 we prove that

the entropic CLT statements (5) and (7) in Theorems 1.1 and 1.2, respectively, are equivalent.

There we also establish a relation between D(S^n) and D(S^n + U ), when S^n are the standardised

sums of lattice random variables Xi and and U is an appropriate (continuous) uniform random

variable, independent of the Xi (Lemma 2.3). In Section 3 we establish two special cases of

Theorem 1.1; first, when the Xi in Sn =

n i=1

Xi

are

symmetric

Bernoulli

random

variables,

Xi  Bern(1/2), and then when each Xi can be written as the sum Xi = Vi + Bi of a lattice

random variable Vi with maximal span h = 1 and a Bi  Bern(1/2) independent of Vi. At first sight it might be tempting to hope that the result of Theorem 1.2 could be de-

rived from its continuous counterpart (3) via a simple quantisation argument using the "data

processing" property of relative entropy [8], but this does not appear to be the case. Instead,

Barron's continuous result (3) is employed in a more indirect way in the proof of the special

case of Theorem 1.1 given in Section 3. This is then used in the proof of our main result, the

general case of Theorem 1.1, in Section 4. In addition to Barron's result (3), and to simple

information-theoretic properties and some well-known bounds and identities for the Fisher in-

formation, the other main ingredient in the proof of Theorem 1.1 is an elementary technique

known as "Bernoulli part decomposition," described in Section 4. Theorem 1.2 is an immediate

consequence of Theorem 1.1 combined with Theorem 2.1.

The CLT for discrete random variables has been investigated from an information-theoretic point of view by, among others, Shimizu [28] and Brown [6], who obtained the convergence of S^n

3

to a Gaussian in distribution (but not for entropy or relative entropy) by proving convergence for the Fisher information of smoothed versions of S^n. The Bernoulli part decomposition technique was first used (implicitly) by Mineka [25] in a different context, and by McDonald [24] and Davis and McDonald [10], who derived conditions under which the standardised sums of independent discrete random variables satisfy the local CLT [11, 13]. In the reverse direction, Takano [30] used the local CLT to derive entropy expansions for the standardised sums S^n as in our Theorem 1.1.
There is a significant line of work re-examining core probabilistic results through the lens of information theory. In terms of ideas as well as techniques, perhaps the works closest in spirit to the present development are those providing information-theoretic treatments of Poisson approximation [15, 19] and compound Poisson approximation [2, 18].

The CLT and the entropy power inequality. The earliest indication of a nontrivial con-

nection between the CLT and information-theoretic ideas comes from Shannon's entropy power

inequality (EPI) [27, 29, 4]. For i.i.d. continuous random variables X1, X2, the EPI states that,

h(X1

+

X2)



h(X1)

+

1 2

log

2,

with equality if and only if X1, X2 are Gaussian. Using the scaling property of the entropy, this implies that h(S^2n)  h(S^n) for all n, which likely provided some of the initial motivation for

the works [21, 28, 6, 3] mentioned earlier. Further, a generalisation of the EPI was used to prove

the

monotonic

increase

of

the

entire

sequence

{h(S^n)}

to

1 2

log(2e2)

in

[22].

For i.i.d. discrete random variables X1, X2, it is easy to see (by considering random variables

with

entropy

close

to

zero)

that

the

obvious

discrete

analog,

H (X1

+

X2)



H (X1 )

+

1 2

log 2,

fails to hold in general. On the other hand, Tao [31] showed that, for any  > 0,

H (X1

+

X2)



H (X1 )

+

1 2

log

2

-

,

for all i.i.d. pairs X1, X2 such that H(X1) is large enough depending on . Tao's proof relies on the inverse sumset theory for entropy developed in [31]. A careful examination of the proof

shows that not only is the lower bound on H(X1) at least,



11

1 





,

(8)

but the implied absolute constants are also very large. Although the answer to the natural

question of how much this bound can be improved remains unclear (see, e.g., [14] for some

related bounds), in view of the results in this paper, particularly the nonasymptotic versions of

Theorem 2.1 and Lemma 2.3, we expect that perhaps if one restricts attention to lattice random

variables with finite variance, it may be possible to significantly improve on (8).

Interestingly, Tao [31] further conjectured that, for any n  2,

H (X1

+

.

.

.

+

Xn)



H (X1

+

.

.

.

+

Xn-1)

+

1 2

log

n n-1

- ,

(9)

as long as H(X1) is sufficiently large depending on n and . The present results again suggest that this conjecture might be easier to prove if attention is restricted to lattice random variables with finite variance. Specifically, in this setting (9) can be interpreted as an "approximate monotonicity" refinement of our Theorems 1.1 and 1.2: By the nonasymptotic form of Theorem 2.1 and the fact that H(X1)   implies Var(X1)  , for lattice X1 with finite variance the conjecture (9) is equivalent to:
D(S^n)  D(S^n-1) + .

4

2 Entropy, relative entropy, and Fisher information

Let X1, X2, . . . , be i.i.d. lattice random variables with values in {a + kh : k  Z}, mean µ, and

finite variance 2. As before, write Sn for the partial sums

n i=1

Xi

and

S^n

for

the

standardised

sums

1 n

Sn

,

and

recall

the

definition

of

the

relative

entropy

D(Y )

between

a

lattice

random

variable Y and and an appropriately quantised Gaussian as in (6).

Our first observation is that the "entropy deficit,"



1 2

log

(2e2

)

-

H(Sn) - log

n h

,

can be viewed as a measure of the "Gaussianity" of the lattice sum Sn. Theorem 2.1 shows that the entropic CLTs stated in Theorems 1.1 and 1.2 are equivalent.

Theorem 2.1 (Entropy and relative entropy solidarity) Suppose {Xn} are i.i.d. lattice

random variables with finite variance 2 and maximal span h > 0. Then the partial sums Sn

and the standardised sums S^n of the Xi satisfy, as n  ,



D(S^n)

=

1 2

log (2e2)

-

H(Sn) - log

n h

+O

1n

.

 In fact, for all n  1, the O(1/ n) error term is absolutely bounded by:

h n

1

+

h 2 n

.

Proof. Because H(Y ) is translation invariant and, as noted in the introduction, so is D(Y ),

we may assume that µ = 0 without loss of generality.

S^n

Since the Xi take in { na + kh/ n

values in {a + kh : k  Z}. Let p, q

: k  Z}, denote the

Sn takes values in {na + kh probability mass functions of

: k  Z} and Sn and of the

quantised Gaussian in the definition of D(S^n), respectively. Writing  for the standard normal

density, for each k  Z we have,

q

 na

+

k

hn

=

na+(k+1)h/n na+kh/n

1 



x 

dx = h n 

na +kh n

,

for some k  [k, k + 1]. Using this, we can bound the absolute difference,



n :=

D(S^n) -

1 2

log

(2e2)

-

H (Sn )

+

log

n h





=

kZ

p(na

+

kh)

log



p(na

h n

(

+ kh)
na+k h n

)



-

kZ

p(na

+

kh)

log

p(na + kh) h n ( na+knh )

,

where the second sum contains the last three terms in n. Simplifying we obtain,

n



1 2n2

p(na + kh) (na + kh)2 - (na + kh)2

kZ



1 2n2

p(na + kh) 2h|na + kh| + h2



h n

+

h2 22n

,

kZ

as required, where the last step follows from the Cauchy-Schwarz inequality and the fact that the variance of Sn is n2.

5

By the nonnegativity of relative entropy we obtain the following standard upper bound,

which can be viewed as a discrete analog of the maximum entropy property of the Gaussian:

H

(Sn)

-

log

n h



1 2

log

(2e2)

+

h n

1

+

h 2 n

.

(10)

In fact, we can easily obtain a stronger bound. Let U be an independent uniform random

variable on (-1/2, 1/2). Then, by the definitions of the continuous and discrete entropies,

H (Sn )

-

log

n h

=

h

S^n + hn U

.

(11)

And using the maximum maximum entropy property of the Gaussian yields:

Proposition 2.2 If Sn is the sum of n i.i.d. lattice random variables with maximal span h > 0

and finite variance 2, then:



H(Sn) - log

n h



1 2

log

2e

2

+

h2 12n

.

(12)

In the special case h = 1, n = 1, the bound (12) appeared in [23]. It was recently exploited further in [26], where an improved inequality was also established for large 2 via the Poisson summation formula. For any n and h = 1, (12) also appeared in [5], as a special case of an inequality for R´enyi entropies.
The following lemma will be used in the proof of Theorem 3.2. It highlights the asymptotic equivalence between the discrete and continuous versions of the relative entropy D(S^n).

Lemma 2.3 Under the assumptions of Theorem 2.1, let U be an independent uniform random variable on (-1/2, 1/2). Then, as n  ,

D(S^n) = D S^n + hn U + O 1n . In fact, for all n  1, the O(1/n) error term is absolutely bounded by:

h n 1 + 2413h n .

Proof. have zero

mAseainn.thNeopterotohfaot fS^Tnh+eo(hre/mn2.)1U,

we may assume has variance 2

without loss + h2/(12n).

of generality that Using the finite-n

the Xi bound

in Theorem 2.1 and the general property (2) of the relative entropy,

D(S^n) - D S^n + hn U



D

S^n

+

h n

U



-

1 2

log

(2e2)

+

H(Sn) - log

n h

+

h n

+

h2 22n

=

1 2

log

1

+

h2 12n2

- h S^n + hn U

+

H (Sn )

-

log

n h

+

h n

+

h2 22n

,

and using (11),

D(S^n) - D

S^n

+

h n

U



1 2

log

1

+

h2 12n2

+

h n

+

h2 22n



h n

+

13h2 242n

,

where the last inequality follows from the elementary bound log (1 + x)  x, x > 0.

6

We close this section by recalling some simple convolution inequalities that will be used in the following sections. If X, Y are independent discrete random variables, then [8]:

H(X + Y )  H(X).

(13)

Similarly, if X is a continuous random variable and Y an arbitrary independent random variable,

then [8]:

h(X + Y )  h(X).

(14)

Finally, for a continuous random variable X with a continuously differentiable density f , we define the Fisher information of X as I(X) = (f )2/f . If the independent random variables
X, Y have continuously differentiable densities with bounded derivatives, then [6, Lemma 5.5]:

I(V + W )  I(V ).

(15)

3 Binomial sums and Bernoulli smoothing

We first establish a nonasymptotic version of Theorem 1.1 in the special case when Sn is the sum of independent Bern(1/2) random variables, so that Sn  Bin(n, 1/2) has a binomial distribution with parameters n and 1/2. Although this elementary result is largely known [7, 16, 17], we state and prove it explicitly as it is the first step towards the proof of our main result, Theorem 1.1. Also, as earlier proofs of (16) actually use the CLT, relying on such arguments would defeat our main claim, namely, that of obtaining a complete proof of the entropic CLT without using any of the standard probabilistic normal approximation techniques or, of course, the CLT itself.

Proposition 3.1 (Binomial entropy) If Sn  Bin(n, 1/2), then for all n  2:

 H(Sn) - log n

-

1 2

log

1 2

e

 4n .

(16)

Note that (16) combined with the finite-n version of Theorem 2.1 also yields:

D(S^n)



8 n

,

n  2.

Proof. The general upper bound in (12) in this particular case gives,

H (Sn )

-

 log n



1 2

log

1 2

e

+

1 2

log

1

+

1 12n



1 2

log

1 2

e

+

1 24n

.

(17)

For the proof of the corresponding lower bound we only consider even n; the case of odd n

is similar.

ak

=

bn(

n 2

Let bn(k) =

n k

+ k), for -n/2

2-n k

denote  n/2.

the Bin(n, Following

1/2) probabilities, and for fixed n  2 write a simple argument by Feller [12, VII, 2], we

first observe that for k  1,

ak

=

a0

×

n 2

(

n 2

(

n 2

+

-

1)

·

·

·

(

n 2

-

k

+

1)

1)(

n 2

+

2)

·

·

·

(

n 2

+

k)

=

a0

×

(1 -

2 n

)(1

-

(1

+

2 n

4 n

)

)··

· ·

··

(1

-

2(k-1) n

(1

+

2k n

)

)

,

and then use the elementary bounds, 1 - x  e-x and 1 + x  ex-x2, for x  [0, 1), to obtain

that,

ak  a0 exp

-

4 n

1 + · · · + (k - 1)

-

2k n

+

3

k3 n2

=

a0

e-

2k2 n

+

3k3 n2

.

7

By Robbins' finite-n version of Stirling's formula, e.g. [12, II, (9.15)], we can easily bound,

a0 

n 2

-1/2

e

1 12n

,

so that, for k  0,

ak 

n 2

e e . -1/2

-

2k2 n

3k3 n2

+

1 12n

(18)

Since ak = a-k, the same bound holds for all -n/2  k  n/2, with |k| in place of k. And substituting (18) into the logarithmic term in the definition of H(Sn) gives,

n/2

H(Sn) = -

ak log ak

k=-n/2



log

 n

+

1 2

log

1 2



+

2 n

n/2

akk2

-

3 n2

n/2

ak |k|3

-

1 12n

k=n/2

k=-n/2



log

 n

+

1 2

log

1 2



+

1 2

-

4n ,

(19)

where in the last step we used the fact that the variance of Sn is n/4 and its third absolute central moment is bounded above by n3/2.
The result follows from (17) and (19).

Next, we extend the result of Proposition 3.1 to the case when each Xi in Sn can be written as the independent sum Xi = Vi + Bi of a lattice random Vi and a Bi  Bern(1/2). The proof of Theorem 3.2 is a key step towards the proof of the general case of Theorem 1.1 in the next
section. We refer to the addition of an independent Bernoulli to a lattice random variable as
"Bernoulli smoothing," in analogy to the Gaussian smoothing step used in [28, 6, 3] along the corresponding development in the continuous case. There, one considers Xi + tZi, where the Zi are standard normals, so that the resulting random variables have differentiable densities that smoothly interpolate between the distribution of Xi and the Gaussian, as t varies. In our case, the addition of a binomial random variable to the partial sums Sn facilitates the use of Proposition 3.1, and also allows us to establish a uniform integrability property which can be
used to exploit the fact that Fisher information decreases on convolution.

Theorem 3.2 (Bernoulli smoothing) Suppose {Vn} are i.i.d. lattice random variables with
finite variance V2 and maximal span h = 1, and let {Bn} be i.i.d. Bern(1/2), independent of {Vn}. Then:

lim H
n

n
Vi + Bi

 - log n

=

1 2

log

2e

V2

+

1 4

.

i=1

For a continuous random variable with continuously differentiable density f , the score function  of Y is  = f /f , so that the Fisher information I(Y ) can be expressed I(Y ) = f 2. In particular, if  is the N (µ, 2) density, then its score function  is linear, (z) = -(z - µ)/2, z  R. For the proof of Theorem 3.2, we will find it convenient to use the standardised Fisher information J(Y ), which, when Y has mean µ and variance 2, is defined as J(Y ) = 2 f (-)2, or, equivalently,

J(Y ) = 2I(Y ) - 1.

(20)

8

Proof. Let U be an independent random variable, uniformly distributed on (-1/2, 1/2), and

write Sn for the binomial sum Sn =

n i=1

Bi

.

In

view

of

Theorem

2.1

and

Lemma

2.3,

it

suffices

to show that, as n  ,

D 1n n Vi + 1n Sn + 1n U  0.
i=1

Using Barron's integral form of de Bruijn's identity [3, Eq. (4.1)], this can be expressed as,

D

1 n

n
Vi + Sn + U

i=1

=D

1 2n

n
Vi + Sn + U
i=1

+ 1 Z 2

(21)

1/2

+

J

0

1-t n

n
Vi + Sn + U

 + tZ

dt 2(1 -

t)

,

(22)

i=1

where Z is an independent normal random variable with the same mean and variance as,

1n n Vi + 1n Sn + 1n U.
i=1

Writing µV

1n

n i=1

Zi

, V2 for the mean + Wn, where the

and variance Zi are i.i.d. N

of the (µV +

Vi, 1/2,

respectively, Z V2 + 1/4) and

can be expressed,

Wn



N (0,

1 12n

)

is

Z= inde-

pendent of the Vi. Therefore, the argument of the relative entropy in the right-hand side of (21)

can be written,

Tn

=

1 2n

n
[Vi
i=1

+

Bi

+

Zi]

+

1 U 2n

+

1 2

Wn

.

Write

Yi

for

the

continuous

i.i.d.

random

variables

Yi

=

 (Vi + Bi + Zi)/ 2

and

let

T2

and

Y2

denote the variances of Tn and of Yi, respectively. By (2) and the convolution inequality (14)

we have, as n  ,

D(Tn)

=

1 2

log(2eT2

)

-

h(Tn)



1 2

log

2e

Y2

+

1 12n

=

D

1 n

n
Yi

+ o(1),

i=1

-h

1n

n i=1

Yi

where the last relative entropy is also o(1) by the continuous entropic CLT (3). Therefore, the relative entropy in (21) vanishes as n  , and now it suffices to show that so does the integral in (22).
An analogous argument to the one used above for the relative entropy can be used to show that, for each t, the standardised Fisher information in the integrand in (22) vanishes with n. For fixed t  (0, 1), let Rn = Rn(t) denote the argument of the standardised Fisher information in (22), so that Rn can be written,

Rn =

1-t n

n [Vi + Bi] + tZ^n +

i=1

1

- n

tU

+

 tWn,

9

where

now

Z^n



 N ( n(µV

+1/2),

V2

+1/4).

Write

Yi

for

the

i.i.d.

random

variables

Yi

:=

Vi +Bi

and let R2 and Y2  denote the variances of Rn and Yi, respectively. By the representation (20)

and the convolution inequality (15), we have that,

0  J (Rn) = R2 I(Rn) - 1 

Y2



+

1 12n

I

1-t n

n

Yi + tZ^n

- 1,

i=1

which vanishes as n   by the Fisher information convergence in [3, Lemma 2], since J(·) is

translation invariant.

Finally, we show that the nonnegative sequence {J(Rn(t)) ; n  1} is uniformly integrable

with

respect

to

the

probability

measure

(dt)

proportional

to

dt 2(1-t)

on

(0, 1/2).

In

fact,

we

will

tshhaowtLZetth=ZatZit +isNZb(o.u2nnT,dh41eed+naw1b21eonv)ceaanbnydwtrZhitee ,uniNfor(mlnyµiVnt, egV2r)abblee

sequence {J(Rn (t))} independent random

defined next. variables such

Rn = Rn +

1-t n

n

Vi

+

 tZ



,

i=1

where,

Rn = Rn (t) =

1

- n

t

[Sn

+

U

]

+

 tZ

,

so that, by the convolution inequality (15) and using the the representation (20) twice,

J (Rn) = R2 I(Rn) - 1  (R2  + V2 )I(Rn ) - 1 =

1

+

V2 R2 

J (Rn )

+

V2 R2 

,

where

R2 

=

1 4

+

1 12n

is

the

variance

of

Rn .

But by Proposition 3.1, Lemma 2.3 and de Bruijn's integral identity,

0

1/2

J

(Rn (t))

dt 2(1 -

t)

,

vanishes as n  . Therefore, {J(Rn (t))} is uniformly integrable with respect to the probability

measure

(dt)



dt 2(1-t)

on

(0, 1/2),

and

hence

so

is

{J (Rn (t))}.

The result follows.

4 Bernoulli part decomposition
At the end of this section we give the proof of Theorem 1.1. In view of (10), our goal is to obtain an appropriate lower bound on the entropy H(Sn). The main idea is to show that Sn can be asymptotically approximately decomposed as a sum involving a Bin(n, 1/2) random variable and then apply Theorem 3.2. The required decomposition will be based on the following elementary technique.
Let X be an integer-valued random variable with probability mass function p on Z and maximal span h = 1. The Bernoulli part decomposition of X is the representation,
X =D V + W B,
10

where V takes values in Z, W  Bern(q), and B  Bern(1/2) is independent of (V, W ). The joint probability mass function of V and W is given by,

pV,W (k, 1) = min{p(k), p(k + 1)},

pV,W

(k,

0)

=

p(k)

-

1 2

[pV,W

(k

-

1,

1)

+

pV,W

(k,

1)],

and the parameter q is,

k  Z,

q := min{p(k), p(k + 1)} > 0,

(23)

kZ

where the positivity of q follows from the fact that the maximal span is 1. For the proof we need the following elementary lemma. It says that, if we wait long enough,
there will be an (approximately) symmetric Bernoulli step hidden in Sn.

Lemma 4.1 Under the assumptions of Theorem 1.1, suppose the Xi have zero mean and take
values in {a + k : k  Z}, for some a  R, with maximal span h = 1. Then, for each n  1, there is a random variable V (n) with values in {na + k : k  Z} and a W (n)  Bern(q(n)), such

that,

Sn =D V (n) + W (n)B,

(24)

where B  Bern(1/2) is independent of (V (n), W (n)) and q(n)  1 as n  . Furthermore,

Var Sn W (n) = 1 = n2(1 + o(1)) as n  .

(25)

Proof. Using the Bernoulli part decomposition Xi = Vi + WiBi for each Xi, we can write,

where Nn =

n

Nn

Sn =D

Vi + Bi,

i=1

i=1

n i=1

Wi



Bin(n, q).

But

also,

n

Nn

n

Vi + Bi =D

Vi +

i=1

i=1

i=1

Nn -1
Bi
i=1

I{Nn1} + I{Nn1}B,

where B  Bern(1/2) is independent of everything else. This is exactly of the required form (24),

with V (n) =

n i=1

Vi

+

(

Nn -1 i=1

Bi)I{Nn1}

and

W (n)

=

I{Nn1},

where

q(n)

=

1

-

(1

-

q)n



1

as n   by (23).

For (25) we only have to consider the case q < 1, since otherwise the result holds trivially.

For the mean we have,

n

0 = ESn = E

Vi W (n) = 0 (1 - q)n + E Sn W (n) = 1 [1 - (1 - q)n].

i=1

On the event {W (n) = 0} = {W1 = · · · = Wn = 0} the Vi are i.i.d., so, E[

n i=1

Vi|W (n)

=

0]

=

O(n), and since [1 - (1 - q)n]  1, we must have,

E Sn W (n) = 1 = o(1), as n  .

(26)

11

For the second moment we similarly have,


n

2



n2 = ESn2 = E 

Vi W (n) = 0 (1 - q)n + E Sn2 W (n) = 1 [1 - (1 - q)n],

i=1

and since the Vi are i.i.d. on {W (n) = 0}, we have, E[(

n i=1

Vi)2|W

(n)

=

0] = O(n2).

Therefore,

E Sn2 W (n) = 1 = n2(1 + o(1)),

as n  .

(27)

The result follows from (26) and (27).

We can finally give the proof of the general case of our main result.

Proof of Theorem 1.1. In view of (10), we only need to show that, as n  ,



H(Sn)  log

n h

+

1 2

log(2e2)

+

o(1).

(28)

Without loss of generality, we assume that the Xi have mean zero and maximal span h = 1.

Let  > 0 be arbitrary and M a large integer to be chosen later. For 1  i  n/M , let

Si(M) =

iM j=(i-1)M +1

Xj ,

so

that

Sn

=

n/M i=1

Si(M ) .

In

the

notation

of

Lemma

4.1,

for

n



M,

n/M

H(Sn) = H

Si(M )

i=1

=H

n/M
Vi(M) + Wi(M)Bi
i=1

n/M

H

Vi(M) + Wi(M)Bi W1(M), . . . , Wn(M/M) .

i=1

Let W (M) denote the vector (W1(M), . . . , Wn(M/M) ) and write AM the collection of vectors w =
(w1, . . . , wn/M )  {0, 1}M/n with wi = 1 for at least n(q(M)-/2)/M indices i, where q(M) = qi(M) is the parameter in the Bernoulli decomposition of Lemma 4.1. Then we can bound,

n/M

H(Sn) 

P W (M) = w H

Vi(M) + Wi(M)Bi W (M) = w .

wAM

i=1

(29)

Now observe that, on the event {W (M) = w}, the n/M random variables {Vi(M) + Wi(M)Bi} are independent, though not necessarily identically distributed. But by (13), we can leave out of
the sum inside the entropy in (29) the summands that correspond indices i for which wi = 0. Thus, writing W¯ (M) for the vector consisting of Wi with 1  i  n(q(M) - /2)/M , and 1 for the vector of all 1s,

H(Sn) 

P W (M) = w H

Vi(M) + Wi(M)Bi W (M) = w

wAM

1i

n M

: wi=1

P

n/M
Wi(M )



n M

q(M )

-

 2

i=1

n(q(M ) -/2)/M

H

Vi(M) + Bi W¯ (M) = 1 ,

i=1

(30)

12

where the second inequality follows form another application of (13), and the fact that, for different i, the distribution of Vi(M) + Wi(M)Bi only depends on Wi(M).
Since each Wi(M)  Bern(q(M)), the probability in (30) converges to 1 exponentially fast. And since the summands inside the entropy in (30) are i.i.d. with variance O(1), from the upper
bound in (10) it follows that, as n  ,

H(Sn)  H

n(q(M ) -/2)/M i=1

Vi(M) + Bi

W¯ (M) = 1

- o(1).

To complete the proof, we apply Theorem 3.2 to the sequence of i.i.d. random variables {Vi(M)} conditional on {Wi(M) = 1}, and the independent sequence {Bi}, to obtain that, as n  ,

H (Sn )



1 2

log

n M

q(M )

-

 2

+

1 2

log

2eVar V1(M) + B1 W1(M) = 1

- o(1),

and using the variance bound in Lemma 4.1,

H (Sn )



1 2

log

n M

+

1 2

log

2eM 2

+

1 2

log(1

-

)

+

1 2

log

q(M )

-

 2

- o(1),

where M is taken large enough for the o(1) term in Lemma 4.1 to be smaller than . And taking M large enough so that q(M) > 1 - /2,

H (Sn )



1 2

log n

+

1 2

log (2e2)

+

log (1

-

)

-

o(1).

Since  > 0 was arbitrary, this gives (28) and completes the proof. Finally we remark that, in order to avoid non-essential technicalities, throughout the proof
we have implicitly assumed that both n/M and n(q(M) - /2)/M are integers. This does not harm generality as we could have replaced these quantities with their integer parts and " =" with " " where necessary to obtain exactly the same result.

References
[1] S. Artstein, K. Ball, F. Barthe, and A. Naor. Solution of Shannon's problem on the monotonicity of entropy. J. Amer. Math. Soc., 17(4):975­982, 2004.
[2] A.D. Barbour, O. Johnson, I. Kontoyiannis, and M. Madiman. Compound Poisson approximation via information functionals. Electron. J. Probab, 15:1344­1369, 2010.
[3] A.R. Barron. Entropy and the central limit theorem. Ann. Probab., 14(1):336­342, January 1986.
[4] N.M. Blachman. The convolution inequality for entropy powers. IEEE Trans. Inform. Theory, 11(2):267­271, April 1965.
[5] S.G. Bobkov, A. Marsiglietti, and J. Melbourne. Concentration functions and entropy bounds for discrete log-concave distributions. ArXiv e-prints, 2007.11030 [math.PR], April 2020.
13

[6] L.D. Brown. A proof of the central limit theorem motivated by the Cram´er-Rao inequality. In Statistics and Probability: Essays in Honor of C. R. Rao, pages 141­148. North-Holland, Amsterdam, 1982.
[7] S.-C. Chang and E Weldon. Coding for T-user multiple-access channels. IEEE Trans. Inform. Theory, 25(6):684­691, November 1979.
[8] T.M. Cover and J.A. Thomas. Elements of information theory. J. Wiley & Sons, New York, second edition, 2012.
[9] I. Csisz´ar. Information-type measures of difference of probability distributions and indirect observations. Studia Sci. Math. Hungar., 2:299­318, 1967.
[10] B. Davis and D.R. McDonald. An elementary proof of the local central limit theorem. J. Theoret. Probab., 8(3):693­701, 1995.
[11] C.-G. Ess´een. Fourier analysis of distribution functions. A mathematical study of the Laplace-Gaussian law. Acta Mathematica, 77(1):1­125, 1945.
[12] W. Feller. An introduction to probability theory and its applications. Vol. I. John Wiley & Sons Inc., New York, N.Y., 1950.
[13] B.V. Gnedenko. On the local limit theorem of probability theory. Russian Math. Surveys, 3(3):187­194, 1948.
[14] S. Haghighatshoar, E. Abbe, and I.E. Telatar. A new entropy power inequality for integervalued random variables. IEEE Trans. Inform. Theory, 60(7):3787­3796, July 2014.
[15] P. Harremo¨es. Binomial and Poisson distributions as maximum entropy distributions. IEEE Trans. Inform. Theory, 47(5):2039­2041, July 2001.
[16] B.L. Hughes and A.B. Cooper. Nearly optimal multiuser codes for the binary adder channel. IEEE Trans. Inform. Theory, 42(2):387­398, March 1996.
[17] P. Jacquet and W. Szpankowski. Entropy computations via analytic depoissonization. IEEE Trans. Inform. Theory, 45(4):1072­1081, May 1999.
[18] O. Johnson, I. Kontoyiannis, and M. Madiman. Log-concavity, ultra-log-concavity, and a maximum entropy property of discrete compound Poisson measures. Discrete Applied Mathematics, 161(9):1232­1250, 2013.
[19] I. Kontoyiannis, P. Harremo¨es, and O. Johnson. Entropy and the law of small numbers. IEEE Trans. Inform. Theory, 51(2):466­472, February 2005.
[20] S. Kullback. A lower bound for discrimination information in terms of variation. IEEE Trans. Inform. Theory, 13(1):126­127, January 1967.
[21] Ju.V. Linnik. An information-theoretic proof of the central limit theorem with Lindeberg conditions. Theory Probab. Appl., 4:288­299, 1959.
[22] M. Madiman and A.R Barron. Generalized entropy power inequalities and monotonicity properties of information. IEEE Trans. Inform. Theory, 53(7):2317­2329, July 2007.
14

[23] J.L. Massey. On the entropy of integer-valued random variables. In 1988 IEEE Workshop on Information Theory (ITW), Beijing, China, July 1988.
[24] D.R. McDonald. On local limit theorem for integer-valued random variables. Theory Probab. Appl., 24(3):613­619, 1980.
[25] J. Mineka. A criterion for tail events for sums of independent random variables. Z. Wahrsch. Verw. Gabiete, 25(3):163­170, 1973.
[26] O. Rioul. Variations on a theme by Massey. ArXiv e-prints, 2102.04200 [cs.IT], February 2021.
[27] C.E. Shannon. A mathematical theory of communication. Bell System Tech. J., 27(3):379­ 423, 623­656, 1948.
[28] R. Shimizu. On Fisher's amount of information for location family. In G.P. Patil, S. Kotz, and J.K. Ord, editors, A Modern Course on Statistical Distributions in Scientific Work, pages 305­312. Springer, Dordrecth, Netherlands, 1975.
[29] A.J. Stam. Some inequalities satisfied by the quantities of information of Fisher and Shannon. Information and Control, 2(2):101­112, 1959.
[30] S. Takano. Convergence of entropy in the central limit theorem. Yokohama Mathematical Journal, 35:143­148, 1987.
[31] T. Tao. Sumset and inverse sumset theory for Shannon entropy. Combinatorics, Probability and Computing, 19:603­639, 2010.
[32] A.M. Tulino and S. Verdu´. Monotonic decrease of the non-Gaussianness of the sum of independent random variables: A simple proof. IEEE Trans. Inform. Theory, 52(9):4295­ 4297, September 2006.
15

