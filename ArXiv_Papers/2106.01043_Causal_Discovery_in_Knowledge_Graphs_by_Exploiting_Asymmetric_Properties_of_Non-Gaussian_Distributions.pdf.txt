Causal Discovery in Knowledge Graphs by Exploiting Asymmetric Properties of Non-Gaussian Distributions

arXiv:2106.01043v1 [cs.LG] 2 Jun 2021

Rohan Giriraj, Sinnu Susan Thomas Department of Computer Science and Engineering
Digital University Kerala (IIITMK) India 695317
rohan.mi19, sinnu.thomas@iiitmk.ac.in

Abstract
In recent years, causal modelling has been used widely to improve generalization and to provide interpretability in machine learning models. To determine causeeffect relationships in the absence of a randomized trial, we can model causal systems with counterfactuals and interventions given enough domain knowledge. However, there are several cases where domain knowledge is almost absent and the only recourse is using a statistical method to estimate causal relationships. While there have been several works done in estimating causal relationships in unstructured data, we are yet to find a well-defined framework for estimating causal relationships in Knowledge Graphs (KG). It is commonly used to provide a semantic framework for data with complex inter-domain relationships. In this work, we define a hybrid approach that allows us to discover cause-effect relationships in KG. The proposed approach is based around the finding of the instantaneous causal structure of a non-experimental matrix using a non-Gaussian model, i.e; finding the causal ordering of the variables in a non-Gaussian setting. The non-experimental matrix is a low-dimensional tensor projection obtained by decomposing the adjacency tensor of a KG. We use two different pre-existing algorithms, one for the causal discovery and the other for decomposing the KG and combining them to get the causal structure in a KG.

1 Introduction

Inclusion of causality in fields such as machine learning has shown improved results when it comes to problems such as generalizations and countering adversarial attacks [39]. Causality has been applied to the different modalities of data required in machine learning, be it images [41], text [16] or numerical data [31]. However, of these modalities, much work is not done in terms of causal discovery in KG --semantically rich method for storing and retrieving relational data. They describe entities based on the relationships they hold with other entities. These entities and relationships combined are represented in the form of a tuple (s, p, o) (subject, predicate, object) or (h, r, t) (head, relation, tail), which is called a triple. Eq. (1) shows the diagrammatic representation of a triple.

subject -p-r-ed-i-ca-te object

(1)

KG possesses statistical properties that show how some triples affect other triples i.e; the existence of some triples affects the existence of certain other triples. This statistical dependence between the triples of a KG leads us to postulate the existence of a cause-effect relationship that governs how the triples affect each other. Our assumption is based on the Common Cause Principle [12], which states that when two variables X and Y are statistically dependent, there exists a third variable Z that

Preprint. Under review.

influences both X and Y . This third variable is called a "confounder" and in most cases, remains ambiguous. In the proposed method, we assume that there are no latent confounders, and only the observed data is taken into consideration. The assumption is necessary for the causal discovery algorithm, Linear Non-Gaussian Acyclic Model (LiNGAM) [28] that discovers the causal order using Independent Component Analysis (ICA) [13] under non-Gaussian assumption on observational data. LiNGAM exploits the asymmetry of non-Gaussian data in higher order statistics, to determine the causal direction among the data points.
The implications of causal discovery in the context of KG are huge, allowing better path discovery for explainable reasoning and querying. In this work, we define a hybrid theoretical approach to attempt solving the problem of causal discovery in KG. The contributions of this work are threefold. At first, we use TuckER [5], a method for embedding a KG after decomposing its adjacency tensors into a core tensor and constituent matrices. Secondly, we take the decomposed tensor, and we project them into a matrix, Q. Finally, the projected matrix is then passed to the DirectLiNGAM [29] algorithm that finds the causal ordering from the given data matrix.
The remainder of this paper is organized as follows. Section 2 contains a brief literature review and underscores the proposed contributions. Section 3 introduces the causal discovery in KG. Section 1 features numerical experiments and Section 5 highlights the limitations of the proposed work. Finally, Section 6 concludes the work.
2 Literature Review
The proposed work tries to define a framework that can infer causal relationships from KG --a multirelational graph composed of entities and relationships which are regarded as nodes and different types of edges respectively [36]. They are known to possess certain statistical properties such as transitivity and type constraints [15] and also some softer statistical patterns can be seen in larger KG. YAGO [32], DBPedia [3], Freebase [7], WordNet [7] etc. have given researches access to large KGs which allowed them to experiment and discover more statistical patterns within them. We need to represent the KG in such a way that it becomes easier for us to capture the statistical relationships present in the KG. Most models represent triples via latent entities. Such representations can be broadly classified into three main categories: point-wise space, complex space, Gaussian space and Manifold space [15].
Of these, we concentrate on pointwise space representation of the KG that includes vector, matrix, and tensor space in this work. TransE [7] represents the translational representation of entities in d-dimensions h + r  t. TransR [17] was introduced to combat the problem of representing both the entities and relationships in single space and defines a matrix Mr  Rk×d, where k is the entity embedding space and d is the relation space. TransH [37] is another example where the translational model is extended to work with hyperplanes. Other methods such as HolE [23] which is based on semantic matching, uses a plain vector space for its representation. Encoding models are just better versions of the point-wise space models, where simple models such as bilinear models [14] achieve state-of-the-art performance compared to the existing point-wise space models such as TransE, RESCAL [21], DistMult [38], and ComplEx [34]. Of these models, we are most interested in RESCAL, which explains triples as pairwise interactions of latent features. RESCAL can be further extended to work with higher dimensional data to handle entities properly [22]. A better method for encoding relationships is TuckER that was introduced to decompose the KG using three-way tucker decomposition [5] to a core tensor and constituent matrices of entities and relationships.
We choose TuckER over other methods since it allows full-expressivity and other encoding models such as RESCAL that can be considered as a special case of TuckER. It also takes the asymmetry of the KG into consideration since the proposed causal inference method is dependent on that. While there are several other encoding methods such as the neural network based NTN [30] and NAM [18], we follow Occam's Razor and a relatively simpler linear model of TuckER architecture for the experiments. Causal discovery algorithms can be classified into three categories. First, constraint based (CB) algorithms learn a set of causal graphs that satisfy the conditional independence in the data. Statistical tests can be used to verify if a candidate graph is faithful [31]. A popular example of this is the Peter-Clark algorithm [31]. Second, score based (SB) algorithms check for the goodness of the fit tests instead of testing for conditional independence. It uses a scoring function, Bayesian Information Criterion (BIC) [27] and maximizes the criterion. There are certain hybrid algorithms
2

that combine both SB and CB methods, for achieving a better result. Third, structural causal model (SCM) based algorithms in which a variable can be written as a function of the directed causes and some noise term.

We consider the Pearlean [25] approach for causal inferences, in the context of SCMs [25] in this work. SCM provides a comprehensive theory of causality. Eq.(2) shows how SCM can be represented as a function, also called as Functional Causal Model (FCM)

Xi = fi(PAi, Ui)

(2)

where fi is a deterministic function, PAi is the parent(s) of the Xith observable random variable. Ui is an exogenous variable that is assumed to be jointly independent. SCMs consist of two components: the causal graph and the structural equation [6]. In causal graphs, each node denotes a random variable and each directed edge from X to Y denotes the causal influence of X on Y . Causal graphs are usually assumed to fulfill the Markov Property such that the implied joint probability factorizes into a "disentangled representation" following recursive decomposition. In the case of causal graphs, there can be cases where multiple graphs can satisfy the conditional independencies. To identify the scenarios where the true graph is identifiable, for which the most common example is a linear system based on non-Gaussian errors [28].

Non-Gaussianity is asymmetric when we take higher order statistics into account [9, 10]. We use an algorithm that specifically exploits this asymmetric property to find the causal order, LiNGAM. The task of learning in LiNGAM comes down to estimating the lower triangular matrix that shows the causal order k(j) where no variable precedes its cause, similar to topologically sorting Directed Acyclic Graph (DAG) [24]. LiNGAM uses ICA to decompose the data matrix X = BS, where B is the mixing matrix and S is the source matrix. The matrix B is then used to compute W = B-1, which is the unmixing matrix. W can then be used to find the lower-triangular matrix. ICALiNGAM gets stuck at the local optima rather than the global ones. To counter this, DirectLiNGAM was introduced, which guaranteed convergence. Although its approach is more or less similar to ICALiNGAM, it uses Kernelized ICA [4] by kernelizing the canonical correlation [1] of the variables.

The proposed method is heavily inspired by the multi-dimensional causal discovery [26] that introduces a tensor decomposition based around tucker decomposition and HOSVD [35] for discovering causal structures in temporal data. Another work that explicitly discussed the problem of causal inference in KG [20] which is based on the idea of pruning KG and having a probabilistic relational model learn the causal structures within the pruned graph. However, pruning the graph requires explicit domain knowledge and insight on how the KG is designed. This is where the proposed method is different, as we don't require any domain data for causal discovery. This makes the proposed approach more robust and generalizable for multiple datasets and domains. Other methods are very interactive, requiring human input occasionally, but the proposed method can find the causal relationships between the embeddings of the KG.

3 Methodology

3.1 Causal Discovery Framework

We formulate the causal discovery framework for the given variables to discover their causal order. We use a non-Gaussian version of Structural Equation Model (SEM) and Bayesian Network (BN) called LiNGAM which approximates a causal order k(i) for a set of observed variables modeled as a DAG. LiNGAM is based around some key assumptions:

· The set of observed variables x1, ..., xn can be arranged in a causal order, such that no later variable causes any earlier variable. A causal order is denoted as k(i).
· Each observable xi can be designed as a linear combination of its earlier variables.

xi =

bij xj + ei

(3)

k(j)>k(i)

Here, bij represents the connection strength of the variable xi and xj, ei represents a noise term. In the case of KG, we assume the relationships between the triples to be of linear nature.

3

· The disturbances ei are continuous, independent random variables with non-Gaussian distributions. This is the same as the noise variable Ui from Eq. 2.
The above assumptions imply that there are no latent confounders involved in the proposed system, and this is called "causally sufficient". In our specific case, it means that there are no unobserved triples in a KG.

3.2 Why Non-Gaussianity?

According to Shimizu et al. [28], algorithms based around second-order statistics are unable to

discern the entire causal structure in most cases. A simple example would be the consideration of two

variables a and b where they both are statistically dependent on each other. We know for a fact that in

the case of a Gaussian distribution, ab -- the Pearson correlation coefficient, is symmetric which means that the direction of dependence, a  b or b  a, is unidentifiable. However, in higher order

statistics, the correlation coefficients exhibit properties of asymmetry [9, 10]. The fourth standard

moment- kurtosis of a distribution is defined as

x = E

X - µx x

4

=

µ4 4

(4)

where µ4 is the standardized central moment, E is the expectation and  is the standard deviation. Excess Kurtosis can be defined as x - 3. The fourth power of XY can be written as the ratio of the excess kurtosis of response and predictor, where response is Y and predictor is X, when X  Y is

considered.

4xy

=

y x

(5)

xy is bounded by the interval [-1, 1]. So when we consider a Gaussian distribution, the excess

kurtosis will be x

=

0, y

=

0, resulting in an irrational value

0 0

.

Therefore, we know that

asymmetry exists in the case of non-Gaussian distributions when higher moments are taken into

consideration.

Traditional LiNGAM models the problem of causal discovery in the form of ICA. ICA uses a set of
inverse linear basis transformations to generate the constituent components of a mixed signal. ICA can be modeled as matrix multiplication x(t) = Ms(t), where x(t) and s(t) are the observed and source signals respectively and M is the unknown mixing matrix. Eq. (3) can be modeled in the same form as above, x = Ae where A = (I - B)-1 where B is the mixing matrix in the ICA problem setting and I is the identity matrix. This form, taken along with the non-Gaussian and independent components of e, is called the linear independent component analysis model.

However, there are problems with the ICA approach. The calculation of gradients, to minimize the diagonal elements of the unmixing matrix W, does not guarantee convergence of the solution. There is a risk of the algorithm getting stuck at the local optima. Secondly, the permutations performed to obtain the lower-triangular matrix are scale-invariant, leading to the finding of the wrong causal order. To mitigate these issues, we prefer DirectLiNGAM that guarantees convergence. Compared to the pre-existing methods this algorithm requires no algorithmic parameters and is guaranteed to converge to the right solution within the fixed number of steps, provided it follows the model strictly along with all the assumptions and the sample size is infinite.

In DirectLiNGAM, the input is a p-dimensional vector x with variable subscripts U and a data matrix X of shape p × n. Then two ordered lists, K :=  and m := 1 are initialized. Then until p - 1 subscripts have been appended to K:

· Least square regression is done on variables xi and xj, where i  U \ K(i = j) and the residual vectors r(j) and the residual data matrix R(j) from X. Then a variable xm is computed by minimizing an independence measure.

xm = arg min Tkernel(xj ; U \ K)

(6)

jU \K

Tkernel is a kernelized measure of independence computed by calculating the mutual information between the variable xj and residual vector ri

Tkernel(xj ; U ) =

M Ikernel(xj , ri(j))

(7)

iU,i=j

4

The kernel-based mutual information estimator can be written as:

M

I kernel (y1 ,

y2)

=

-

1 2

log

det det

K D

(8)

where  is a small positive constant and K and D are just matrices whose blocks are

(K )ij

=

KiKj

for

i

=

j,

and

(K )ii

=

(Ki

+

N 2

I

)2

and

D

is

just

a

block

diagonal

matrix

with

blocks

(Ki

+

N 2

I )2 .

K1

and

K2

are

just

Gram

matrices

whose

elements

are

RBF kernels of the sets of observations of y1 and y2 respectively (in the two variable case).

· m is appended to K.

· x := r(m), X := R(M)

Once p - 1 subscripts have been appended to K the remaining variable is appended to K and a strict lower-triangular matrix B is formed using the order in K, and the connection strengths bij is estimated by a conventional regression method such as least squares or maximum likelihood based on covariance.

3.3 KG Representation
KG are usually represented in the form of triples. Triples consist of entities and relationships. Let the set of all entities be E = {e1, ..., eNe }, and the set of all relationships be R = {r1, ..., rNr }. The triples are usually represented as tuples (ei, rk, ej). Each possible triple can also be modeled as a binary random variable yijk  {0, 1}.

yijk =

1, 0

if the triple (ei, rk, ej) exists otherwise

This can also be represented as a third order adjacency tensor of shape Y= {0, 1}Ne×Ne×Nr . Estimation of the joint probability distribution of the observed triples can help us derive the entire KG. For our particular application, we assume that the KG is built in Closed World Assumption (CWA) [19]. We know that some triples are statistically dependent on others, meaning that the existence of some triples influences the existence of other triples. To model this correlation, we need to assume that all triples yijk are conditionally independent given the latent features associated with the subject, predicate, object and observed graph features and parameters. These score-based models predict the feasibility of a triple based on a scoring function f (xijk, ), where the score denotes the confidence that a triple exists given the parameters . In most cases, the triples are explained by the latent features, where latent features are the elements in a vectorized form of an entity/relationship in a triple. These latent features can come from different embedding algorithms that essentially convert text into vectorized form. For example: consider a system with only two entities then the latent features can be written in vectorized form as:

e1 =

0.98 0.

, e2 =

0. 0.2

(9)

There are many more complicated methods for embedding entities, which we discussed in the Section 2. These are language models that take in some text and output the resultant vector with a certain number of latent features. The main intuition behind the latent feature model is the fact that the relationships between the different entities can be deduced by the interactions between their latent features. In the proposed approach, we use these latent features to predict the causal order of the system. There are several score-based methods such as RESCAL, DistMult, and ComplEx but we prefer TuckER for the proposed approach. It is fully expressive, meaning that it is capable of capturing all the information present within the data. It allows us to represent all the other score-based, tensor factorization models as special cases of itself. It is a linear model, which is important because one of the major assumptions required for LiNGAM is linearity.

Tucker decomposition decomposes a tensor into its constituent matrices and a smaller core tensor. In a three mode scenario a tensor X  RI×J×K outputs a core tensor Z, and factor matrices A, B and
C. Elements of the core tensor show the level of interaction between the different components.

X  Z ×1 A ×2 B ×3 C

(10)

5

(a) Core tensor of TuckER

(b) Visualization of tensor Q

Figure 1: 1a: Visualization of the TuckER architecture, where eo, es  Rde and wr  Rdr . 1b: Visualization of the tensor Q  Rnr×de×de

We take the binary adjacency tensor representation of the KG and use Tucker decomposition to reduce
it to the core tensor and factor matrices. We have an entity embedding matrix E which is equivalent for both subject and object entities, i.e; E = A = C  Rne×de . The second factor matrix is the relation embedding matrix R = B  Rnr×dr where ne and nr are the number of entities and relations respectively and de, dr are the dimensions of the entity and relation matrices. These dimensions are the same as the latent features we mentioned above. Finally, we have a core tensor W  Rde×dr×de ,
which is diagrammatically represented in Fig. 1a. The scoring function of the TuckER is:

(es, r, eo) = W ×1 es ×2 wr ×3 eo

(11)

where es, eo  Rde are rows of the entity embedding matrix E denoting the subject and object entities of a triple. wr  Rdr are the rows of relation embedding matrix R denoting the relations or the predicate values between the different entities. The result of the scoring function is then passed
through the logistic sigmoid function so that the probability of a triple being true can be calculated.
The final score obtained is ((es, r, eo)). We need to have a matrix that can be used to represent the decomposed form of a tensor apart from the core tensor and different factor matrices. To achieve this, we introduce a projection tensor Q  Rnr×de×de defined as

Q = (W ×1 E ×2 wr)E.

(12)

This tensor, as shown in Fig. 1b captures the relationship between the dimensions of the entity
embeddings de, since we are more interested in the latent variables that make up the entities. The resultant tensor Q  Rnr×de×de is converted into a matrix Q  Rnr×(de×de) suitable for DirectLiNGAM.

3.4 The Hybrid Algorithm
We take the matrix Q and use that as a p × n dimensional data matrix as input for DirectLiNGAM. As a final check, we perform a kurtosis test and find that the matrix is non-Gaussian. DirectLiNGAM has a run time complexity of O(np3M 2 + p4M 3), so depending on the size of p, the runtime of the algorithm increases polynomially. For higher values of p, we reshaped the values of Q accordingly. The proposed approach gives the freedom to choose the number of relationships we can consider for
6

the algorithm i.e; we can choose the number of rows of R, wr for some added flexibility as given in Algorithm 1. Algorithm 1: Hybrid Algorithm Result: Causal order k(i)
· Initialize the DirectLiNGAM algorithm with the right parameters. · Initialize the triples of a KG as a third order tensor. · Apply the TuckER model on the tensor to obtain the decomposed components.
Z, A, B, C  TuckER(adjacency tensor) · Choose the number of rows wr of relation matrix R we need for the execution. · Get the projection tensor Q  (W ×1 E ×2 wr)E. · Matricize the tensor Q into the matrix Q. · Causal order k(i)  DirectLiNGAM(Q) · return k(i)
4 Experimental Results
As the proposed approach is a highly specific application of causal discovery, there are no known benchmarks to check for causality in a KG that we can compare the proposed approach to. We find that it does give us a causal ordering of de, and an adjacency matrix, which we could then plot as a DAG. The code for the implementation is available here.
4.1 Implementation · Datasets: For the testing, we chose two datasets FB15K-237 [33], and WN18-RR [8] subsets of the FB15K [7] and WN18 [7] datasets respectively, but have their inverse relations removed. The removal of inverse relations helps us avoid cycles in the data. For our experimentation purposes, we chose a wr value of 100 for FB15K-237 and 10 for WN18RR. Fig. 2 shows an example of an inverse relation in a KG. Note that the entities and the relationships are the same.
Figure 2: Inverse relations in a KG.
· Libraries: We use a combination of different open-source libraries to implement the algorithm. We use numpy [11] for the tensor operations, pykg2vec [40] for overall embedding and tuning with TuckER, lingam [28] library for working with DirectLiNGAM and graphviz for visualizing the DAG from the causal order.
· Hardware: For training of the TuckER architecture and the implementation of DirectLiNGAM, we use 9th generation Intel Core i5 9300H processor, with 8 Gigabytes of RAM. This didn't particularly slow the experimentation down, as we considered smaller dimensions and smaller subsets of the larger KG datasets.
7

4.2 Results

Dataset FB15K-237 WN18-RR

ndim
de = 5 de = 7 de = 10 de = 5 de = 7 de = 10

Convergence

Execution Time

Mean p-values

ICALiNGAM DirectLiNGAM ICALiNGAM DirectLiNGAM ICALiNGAM DirectLiNGAM

No

Yes

1.417

3.957

0.158

0.200

No

Yes

8.821

28.27

0.162

0.123

Yes

Yes

23.85

240.4

0.150

0.183

No

Yes

N/A

3.330

N/A

0.335

No

Yes

N/A

24.08

N/A

0.260

No

Yes

N/A

202.5

N/A

0.153

Table 1: Quantitative Analysis with different parameters.

Table 1 shows four primary columns with different parameters specified in each.
· ndim: This specifies the dimensions of the embedding, i.e; de, dr. For the sake of simplicity, we are considering a case where de = dr. We tested with 5, 7 and 10 dimensions. Since the dimensions grow quadratically, it makes sense to experiment with smaller values of de. The significance of smaller dimensions is downplayed by the non-convergence of the algorithms due to lack of data.
· Convergence: This shows whether the algorithm in question converges to provide a solution. We observe that ICALiNGAM fails to converge in most cases, especially in the case of smaller datasets like WN18-RR. This further proves the significance of a method like DirectLiNGAM, which is guaranteed to converge. In all the trials, we found that the ICA algorithms like FastICA were unable to converge despite early stopping. Since it converges at the local minima, there are chances for the causal order of ICALiNGAM to be wrong.
· Execution time: This specifies the execution time taken by both ICALiNGAM/LiNGAM and DirectLiNGAM to compute the causal order k(j). Here we observe that in all cases, the execution time of ICALiNGAM is far lesser than that of DirectLiNGAM. This is because there is a trade-off between execution speed and convergence. While LiNGAM is faster, it failed to converge. If given enough computing resources and time, DirectLiNGAM is the better choice.
· Mean p-value: This is the mean of the p-values obtained by testing for independence of the error variables. This is a fairly important test that checks if the LiNGAM assumption holds true for the proposed case. Essentially, we want the test to fail at rejecting the null hypothesis, which in this case is the independence of the error/exogenous variables of LiNGAM.

Figure 3: DAG with wr = 237 for FB15K-237 dataset.
In Fig. 3, we see that when ndim = 3, the total number of nodes is nine. This is because matrix Q  Rnr×(de×de) has de × de as columns. So for any dimension de, there is a quadratic increase in the number of features. This is precisely why we limited ourselves to small values of de like 5, 7, and 10.
8

We further check the validity of the LiNGAM assumption by checking the p-values of independence between the error variables of the data matrix. This is because the most important assumption in LiNGAM is that the errors/exogenous variables follow a non-Gaussian distribution and are independent of each other so that there are no latent confounder variables. For the experiments, we take  = 0.01 of p-values. The hypotheses for the proposed results are: H0  The error variables are independent of each other. Ha  The error variables are not independent. For all cases, we find that the p-values are higher than the  value. This means that the test has failed to reject the null hypothesis.
5 Limitations
The theoretical approach we proposed in the work has its share of limitations. We broadly classify the limitations into two categories:
· Dimensionality: This work suffers from the curse of dimensionality. We take the tensor shown in Eq. (12) and we project it to a smaller dimension Q. Ignoring the loss of information, one of the things we do to achieve this is by increasing the number of features by two-fold. In this case, by making the matrix have the shape nr × (de × de). This is especially problematic for DirectLiNGAM due to its polynomial time complexity.
Figure 4: Time complexity vs. dimensions.
Fig. 4 shows how the increase in the number of dimensions leads to a polynomial increase in the time complexity of the respective algorithm. The complexity of DirectLiNGAM is O(np3M 2 + p4M 3) and that of ICALiNGAM is O(np3 + p4). For plotting the graph, we assume that the values of n and M are constant, where n is the number of samples and M is the maximal rank found by the low-rank decomposition used in the kernel-based independence measure [M ( n)]. This polynomial increase in the time-complexity of the algorithm with just an increase in the dimension de is detrimental in cases where we want to examine large, feature-packed KGs for causality. Another problem is that standard dimensionality reduction techniques like Principal Component Analysis (PCA) or Singular Value Decomposition (SVD) fail to capture the causal relationships among the variables. We address this problem in the future works. · Extrapolation of useable causal relationships Proposed method is a definite success when it comes to discovery of causal structures in embedding matrices, but extrapolating from these causal relationships to ensure its application is not something we covered in this work. In this work, we mostly focus on the methodology to check for causal relationships in a Knowledge Graph. It is more difficult, considering that we look for relationships among the embedded features of a KG. In Fig. 3, we see elements x0  x8. These 9 variables are 9 features embedded by KG when we consider dimensionality de × de. It is not possible to
9

perform an inverse operation to get the required result, as the embeddings for each entity vary greatly. Further analysis will give us more data on how data can be embedded in a way where it makes sense to use causal discovery for working with applications like recommendation systems, etc. We will be able to develop practical applications like causalenforced reasoning.
6 Conclusions
The approaches presented in this work were the result of us trying to formally explore a previously unexplored avenue of causal discovery. Through the results, we could come to the conclusion that it is, in fact, plausible to discover causal structures in KG. We tried to approach this problem of causal discovery from a purely statistical perspective without the requirement of any background knowledge or problem/dataset specific ontologies. Future improvements include using better methods of tensor decomposition and representation such as LowFER [2], which is a generalized form of TuckER, better forms of decomposition and dimensionality reduction that can capture and project the causal structure into lower dimensions. Though we could get the results for the proposed approach, more in-depth analysis and experimentation is required to bring out the applications for the proposed approach on KG-based tasks (such as link prediction or KG completion).
References
[1] Shotaro Akaho. A Kernel Method for Canonical Correlation Analysis. CoRR, abs/0609071, 2006.
[2] Saadullah Amin, Stalin Varanasi, Katherine Ann Dunfield, and Günter Neumann. LowFER: Low-rank Bilinear Pooling for Link Prediction. In ICML, pages 257­268, 2020.
[3] Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives. DBpedia: A Nucleus for a Web of Open Data. In ISWC, page 722­735, 2007.
[4] Francis R Bach and Michael I Jordan. Kernel Independent Component Analysis. JMLR, Jul: 1­48, 2002.
[5] Ivana Balazevic´, Carl Allen, and Timothy M. Hospedales. TuckER: Tensor Factorization for Knowledge Graph Completion. In EMNLP-IJCNLP, pages 5185­5194, November 2019.
[6] Kenneth A Bollen and Pamela Paxton. Interactions of Latent Variables in Structural Equation Models. Structural Equation Modeling: A Multidisciplinary Journal, 5(3):267­293, 1998.
[7] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. Translating Embeddings for Modeling Multi-relational Data. In NIPS, pages 1­9, 2013.
[8] Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. Convolutional 2D Knowledge Graph Embeddings. In AAAI, volume 32, 2018.
[9] Yadolah Dodge and Valentin Rousson. On Asymmetric Properties of the Correlation Coeffcient in the Regression Setting. The American Statistician, 55(1):51­54, Feb 2001.
[10] Yadolah Dodge and Iraj Yadegari. On Direction of Dependence. Metrika, 72(1):139­150, Aug 2009.
[11] Charles R. Harris, K. Jarrod Millman, Stéfan J. van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fernández del Río, Mark Wiebe, Pearu Peterson, Pierre Gérard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array Programming with NumPy. Nature, 585(7825):357­362, Sep 2020.
[12] Gábor Hofer-Szabó, Miklós Rédei, and László E Szabó. On Reichenbach's Common Cause Principle and Reichenbach's Notion of Common Cause. The British Journal for the Philosophy of Science, 50(3):377­399, 1999.
10

[13] A. Hyvärinen and E. Oja. Independent Component Analysis: Algorithms and Applications. Neural Networks, 13(4-5):411­430, Jun 2000.
[14] Rodolphe Jenatton, Nicolas Roux, Antoine Bordes, and Guillaume R Obozinski. A Latent Factor Model for Highly Multi-relational Data. In NIPS, 2012.
[15] Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and Philip S. Yu. A Survey on Knowledge Graphs: Representation, Acquisition and Applications. IEEE TNNLS, pages 1­21, 2021.
[16] Katherine Keith, David Jensen, and Brendan O'Connor. Text and Causal Inference: A Review of Using Text to Remove Confounding from Causal Estimates. In ACL, July 2020.
[17] Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. Learning Entity and Relation Embeddings for Knowledge Graph Completion. In AAAI, page 2181­2187, 2015.
[18] Quan Liu, Hui Jiang, Andrew Evdokimov, Zhen-Hua Ling, Xiaodan Zhu, Si Wei, and Yu Hu. Probabilistic Reasoning via Deep Learning: Neural Association Models. CoRR, abs/1603.07704, 2016.
[19] Jack Minker. On Indefinite Databases and the Closed World Assumption. In International Conference on Automated Deduction, pages 292­308. Springer, 1982.
[20] Melanie Munch, Juliette Dibie, Pierre-Henri Wuillemin, and Cristina Manfredotti. Interactive Causal Discovery in Knowledge Graphs. In PROFILES-SEMEX, pages 78­93, 2019.
[21] Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. A Three-Way Model for Collective Learning on Multi-Relational Data. In ICML, 2011.
[22] Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. Factorizing YAGO: Scalable Machine Learning for Linked Data. In WWW, page 271­280, 2012.
[23] Maximilian Nickel, Lorenzo Rosasco, and Tomaso Poggio. Holographic Embeddings of Knowledge Graphs. In AAAI, page 1955­1961, 2016.
[24] Chaoyi Pang, Junhu Wang, Yu Cheng, Haolan Zhang, and Tongliang Li. Topological Sorts on DAGs. Information Processing Letters, 115(2):298­301, 2015.
[25] Judea Pearl. Causality. Cambridge University Press, 2009.
[26] Ulrich Schaechtle, Kostas Stathis, and Stefano Bromuri. Multi-dimensional Causal Discovery. In IJCAI, 2013.
[27] Gideon Schwarz et al. Estimating the Dimension of a Model. Annals of Statistics, 6(2):461­464, 1978.
[28] Shohei Shimizu, Patrik O. Hoyer, Aapo Hyvarinen, and Antti Kerminen. A Linear Non-Gaussian Acyclic Model for Causal Discovery. JMLR, 7(72):2003­2030, 2006.
[29] Shohei Shimizu, Takanori Inazumi, Yasuhiro Sogawa, Aapo Hyvarinen, Yoshinobu Kawahara, Takashi Washio, Patrik O. Hoyer, and Kenneth Bollen. DirectLiNGAM: A Direct Method for Learning A Linear Non-Gaussian Structural Equation Model. JMLR, 12:1225­1248, July 2011.
[30] Richard Socher, Danqi Chen, Christopher D. Manning, and Andrew Y. Ng. Reasoning with Neural Tensor Networks for Knowledge Base Completion. In NIPS, 2013.
[31] Peter Spirtes, Clark N Glymour, Richard Scheines, and David Heckerman. Causation, Prediction, and Search. MIT press, 2000.
[32] Fabian M Suchanek, Gjergji Kasneci, and Gerhard Weikum. YAGO: A Core of Semantic Knowledge. In WWW, pages 697­706, 2007.
[33] Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoifung Poon, Pallavi Choudhury, and Michael Gamon. Representing Text for Joint Embedding of Text and Knowledge Bases. In EMNLP, pages 1499­1509, September 2015.
11

[34] Théo Trouillon, Johannes Welbl, Sebastian Riedel, Éric Gaussier, and Guillaume Bouchard. Complex Embeddings for Simple Link Prediction. In ICML, page 2071­2080, 2016.
[35] Miaoyan Wang and Yun Song. Tensor Decompositions via Two-mode Higher-order SVD (HOSVD). In AISTATS, pages 614­622. PMLR, 2017.
[36] Quan Wang, Zhendong Mao, Bin Wang, and Li Guo. Knowledge Graph Embedding: A Survey of Approaches and Applications. IEEE TKDE, 29(12):2724­2743, 2017.
[37] Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. Knowledge Graph Embedding by Translating on Hyperplanes. In AAAI, 2014.
[38] Bishan Yang, Scott Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Embedding Entities and Relations for Learning and Inference in Knowledge Bases. In ICLR, May 2015.
[39] Chao-Han Huck Yang, Yi-Chieh Liu, Pin-Yu Chen, Xiaoli Ma, and Yi-Chang James Tsai. When Causal Intervention meets Adversarial Examples and Image Masking for Deep Neural Networks. In ICIP, pages 3811­3815, 2019.
[40] Shih-Yuan Yu, Sujit Rokka Chhetri, Arquimedes Canedo, Palash Goyal, and Mohammad Abdullah Al Faruque. Pykg2vec: A Python Library for Knowledge Graph Embedding. JMLR, 22(16):1­6, 2021.
[41] Bin Zhu and Chong-Wah Ngo. CookGAN: Causality based Text-to-Image Synthesis. In CVPR, pages 5519­5527, 2020.
12

