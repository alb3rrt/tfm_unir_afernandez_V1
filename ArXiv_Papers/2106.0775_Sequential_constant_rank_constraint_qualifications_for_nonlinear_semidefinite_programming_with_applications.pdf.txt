Sequential constant rank constraint qualifications for nonlinear semidefinite programming with applications

Roberto Andreani 

Gabriel Haeser  Leonardo M. Mito  June 8, 2021

H´ector Ram´irez C. 

arXiv:2106.00775v2 [math.OC] 7 Jun 2021

Abstract
We present new constraint qualification conditions for nonlinear semidefinite programming that extend some of the constant rank-type conditions from nonlinear programming. As an application of these conditions, we provide a unified global convergence proof of a class of algorithms to stationary points without assuming neither uniqueness of the Lagrange multiplier nor boundedness of the Lagrange multipliers set. This class of algorithm includes, for instance, general forms of augmented Lagrangian, sequential quadratic programming, and interior point methods. We also compare these new conditions with some of the existing ones, including the nondegeneracy condition, Robinson's constraint qualification, and the metric subregularity constraint qualification.
Keywords: Constant rank, Constraint qualifications, Semidefinite programming, Algorithms, Global convergence.
1 Introduction
Constraint qualification (CQ) conditions play a crucial role in optimization. They permit to establish first- and second-order necessary optimality conditions for local minima and support the convergence theory of many practical algorithms (see, for instance, a unified convergence analysis for a whole class of algorithms by Andreani et al. [8, Thm. 6]). Some of the well-known CQs in nonlinear programming (NLP) are the constant-rank constraint qualification (CRCQ), introduced by Janin [22], and the constant positive linear dependence (CPLD) condition. The latter was first conceptualized by Qi and Wei [26], and then proved to be a constraint qualification by Andreani et al. [12]. Moreover, it has been a source of inspiration for other authors to define even weaker constraint qualifications for NLP, such as the constant rank of the subspace component (CRSC) [9], and the relaxed versions of CRCQ [23] and CPLD [8]. Our interest in constant ranktype conditions is motivated, mainly, by their applications towards obtaining global convergence results of iterative algorithms to stationary points without relying on boundedness or uniqueness of Lagrange multipliers. However, several other applications that we do not pursue in this paper may be expected to be extended to the conic context, such as the computation of the derivative of the value function [22, 24] and the validity of strong second-order necessary optimality conditions that do not rely on the whole set of Lagrange multipliers [1]. Besides, their ability of dealing with redundant constraints, up to some extent, gives modellers some degree of freedom without losing regularity or convergence guarantees on algorithms. For instance, the standard NLP trick of replacing one nondegenerate equality constraint by two inequalities of opposite sign does not violate CRCQ, while violating the standard Mangasarian-Fromovitz CQ (MFCQ).
Constant-rank type CQs have been proposed in conic programming only very recently. The first extension of CRCQ to nonlinear second-order cone programming (NSOCP) appeared in [32], but it was shown to be incorrect in [2]. A second proposal [7], which encompasses also nonlinear semidefinite programming (NSDP) problems, consists of transforming some of the conic constraints into NLP constraints via a reduction function, whenever it was possible, and then demanding constant linear dependence of the reduced constraints, locally. This was considered by the authors a naive extension, since it basically avoids the main difficulties that are expected from a conic framework. What both these works have in common is that they somehow neglected the conic structure of the problem.
The authors received financial support from FAPESP (grants 2017/18308-2, 2017/17840-2, and 2018/24293-0), CNPq (grants 301888/2017-5, 303427/2018-3 and 404656/2018-8), and ANID (FONDECYT grant 1201982 and Basal Program CMM ANID PIA AFB170001).
Department of Applied Mathematics, University of Campinas, Campinas, SP, Brazil. Email: andreani@unicamp.br Department of Applied Mathematics, University of S~ao Paulo, S~ao Paulo, SP, Brazil. Emails: ghaeser@ime.usp.br, leokoto@ime.usp.br Departamento de Ingenier´ia Matema´tica and Centro de Modelamiento Matema´tico (AFB170001 - CNRS IRL2807), Universidad de Chile, Santiago, Chile. Email: hramirez@dim.uchile.cl
1

In a recent article [6], we introduced weak notions of regularity for nonlinear semidefinite programming (NSDP) that were defined in terms of the eigenvectors of the constraints ­ therein called weak-nondegeneracy and weak-Robinson's CQ. These conditions take into consideration only the diagonal entries of some particular transformation of the matrix constraint. Noteworthy, weak-nondegeneracy happens to be equivalent to the linear independence CQ (LICQ) when an NLP constraint is modeled as a structurally diagonal matrix constraint, unlike the standard nondegeneracy condition [30], which in turn is considered the usual extension of LICQ to NSDP. Moreover, the proof technique we employed in [6] induces a direct application in the convergence theory of an external penalty method. In this paper, we use these conditions to derive our extension proposals for CRCQ and CPLD to NSDP, which also recover their counterparts in NLP when it is modelled as a structurally diagonal matrix constraint. These CQs are called, in this paper, as weak-CRCQ and weak-CPLD, respectively.
However, to provide support for algorithms other than the external penalty method, we present stronger variants of these conditions, called sequential-CRCQ and sequential-CPLD (abbreviated seq-CRCQ and seqCPLD, respectively), by incorporating perturbations in their definitions. This makes them robust and easily connectible with algorithms that keep track of approximate Lagrange multipliers, but also more exigent. Nevertheless, seq-CRCQ is still strictly weaker than nondegeneracy, and independent of Robinson's CQ, while seq-CPLD is strictly weaker than Robinson's CQ. On the other hand, weak-CRCQ is strictly weaker than seq-CRCQ, while weak-CPLD is strictly weaker than weak-CRCQ and seq-CPLD. Moreover, we show that seq-CPLD implies the metric subregularity CQ.
The content of this paper is organized as follows: Section 2 introduces notation and some well-known theorems and definitions that will be useful in the sequel. Our main results for NSDP are presented in Sections 3 and 4. Indeed, Section 3 is devoted to the study of weak-CRCQ and weak-CPLD and their properties, which in turn need to invoke weak-nondegeneracy and weak-Robinson's CQ as a motivation. Section 4 studies seq-CRCQ and seq-CPLD ­ the main CQs of this paper ­ and some algorithms supported by them. In Section 5, we discuss the relationship between seq-CPLD and the metric subregularity CQ. Lastly, some final remarks are given in Section 6.

2 A nonlinear semidefinite programming review

In this section, Sm denotes inner product defined as M, N

th=.e

linear space of trace(M N ) =

all m × m real symmetric matrices equipped with the

m i,j=1

Mij

Nij

for

all

M, N



Sm,

and

Sm +

is

the

cone

of

Ball(Mpo,sit)i=v.e norm M

semidefinite matrices in Sm

{=.Z



Sm : M - Z M, M , and

<  } the its closure

. Additionally, for every M  Sm and every  > open ball centered at M with radius  with respect will be denoted by B(M,  ).

0, to

we the

denote by Frobenius

We consider the NSDP problem in standard (dual) form:

Minimize f (x), xRn
subject to G(x) 0,

(NSDP)

where f : Rn  R and G : Rn  Sm are continuously differentiable functions, and is the partial order induced by Sm + ; that is, M N if, and only if, M - N  Sm + .
Equality constraints are omitted in (NSDP) for simplicity of notation, but our definitions and results are
flexible regarding inclusion of such constraints, which should be done in the same way as in [7]. Moreover, throughout the whole paper, we will denote the feasible set of (NSDP) by F.
Let us recall that the orthogonal projection of an element M  Sm onto Sm + , which is defined as

Sm(M ) =. argmin M - N ,

+

N Sm +

is a convex continuous function of M since Sm + is nonempty, closed, and convex. Furthermore, since Sm + is self-dual, every M  Sm has a Moreau decomposition [25, Prop. 1] in the form

M = Sm (M ) - Sm (-M )

+

+

with Sm(M ), Sm (-M ) = 0, and a spectral decomposition in the form

+

+

M = 1(M )u1(M )u1(M ) + . . . + m(M )um(M )um(M ),

(1)

where u1(M ), . . . , um(M )  Rm are arbitrarily chosen orthonormal eigenvectors associated with the eigenval-

ues 1(M ), . . . , m(M ), respectively. In turn, these eigenvalues are assumed to be arranged in non-increasing

oisrdueir(.ME)q, uanivdalDent=.lyD, wiaeg(ca1n(Mwr)i,t.e.

(1) as M = U DU , where U . , m(M )) is a matrix whose

is an orthogonal matrix whose diagonal entries are 1(M ), . . . ,

i-th column m(M ) and

the remaining entries are zero.

2

A convenient property of the orthogonal projection onto Sm + is that, for every M  Sm, we have

Sm +

(M

)

=

[1(M

)]+u1(M

)u1(M

)

+

.

.

.

+

[m(M )]+um(M

)um(M

),

where [ · ]+ =. max{ · , 0}.

Given a sequence of sets {Sk}kN, recall its outer limit (or upper limit) in the sense of Painlev´e-Kuratowski

(cf. [28, Def. 4.1] or [15, Def. 2.52]), defined as

Lim supkNSk =. y : I  N, {yk}kI  y, k  I, yk  Sk ,

which is the collection of all cluster points of sequences {yk}kN such that yk  Sk for every k  N. The
notation I  N means that I is an infinite subset of the set of natural numbers N. We denote the Jacobian of G at a given point x  Rn by DG(x), and the adjoint operator of DG(x) will
be denoted by DG(x). Moreover, the i-th partial derivative of G at x will be denoted by Dxi G(x), and the gradient of f at x will be written as f (x), for every x  Rn.

2.1 Classical optimality conditions and constraint qualifications

As usual in continuous optimization, we drive our attention towards local solutions of (NSDP) that satisfy the so-called Karush-Kuhn-Tucker (KKT) conditions, defined as follows:

Definition 2.1. We say that the Karush-Kuhn-Tucker conditions hold at x  F when there exists some
Y 0 such that xL(x, Y ) = 0 and G(x), Y = 0,
where L(x, Y ) =. f (x) - G(x), Y is the Lagrangian function of (NSDP). The vector Y is called a Lagrange
multiplier associated with x, and the set of all Lagrange multipliers associated with x will be denoted by (x).

Of course, not every local minimizer satisfies KKT in the absence of a CQ. In order to recall some

classical CQs, it is necessary to use the (Bouligand) tangent cone to Sm + at a point M 0. This object can be characterized in terms of any matrix E  Rm×m-r, whose columns form an orthonormal basis of KerM ,

as follows (e.g., [15, Ex. 2.65]):

TSm (M ) = {N  Sm : EN E 0},

(2)

+

where r denotes the rank of M . So, its lineality space, defined as the largest linear space contained in

TSm (M ), is computed as follows: +

lin(TSm(M )) = {N  Sm : EN E = 0}.

(3)

+

The latter is a direct consequence of the identity lin(C) = C  (-C), satisfied for any closed convex cone C. One of the most recognized constraint qualifications in NSDP is the nondegeneracy (or transversality)
condition introduced by Shapiro and Fan [30], which can be characterized [15, Eq. 4.172] at a point x  F when the following relation is satisfied:

ImDG(x) + lin(TSm (G(x))) = Sm. +

If x is a local solution of (NSDP) that satisfies nondegeneracy, then (x) is a singleton, but the converse is not necessarily true unless G(x) + Y  intSm + holds for some Y  (x) [15, Prop. 4.75]. This last condition is known as strict complementarity in this NSDP framework. Here, intSm + stands for the topological interior of Sm + . By (2) it is possible to characterize nondegeneracy at x by means of any given matrix E with
orthonormal columns that span KerG(x). Indeed, following [15, Sec. 4.6.1], nondegeneracy holds at x if, and only if, either KerG(x) = {0} or the linear mapping x : Rn  Sm-r given by

x( · ) =. EDG(x)[ · ]E

(4)

is surjective, which is in turn equivalent to saying that the vectors

vij (x, E) =.

ei Dx1 G(x)ej , . . . , ei Dxn G(x)ej


,

1  i  j  m - r,

(5)

are linearly independent [29, Prop. 6], where ei denotes the i-th column of E and r is the rank of G(x).
Another widespread constraint qualification is Robinson's CQ [27], which can be characterized at x  F by the existence of some d  Rn such that

G(x) + DG(x)[d]  intSm + .

(6)

It is known (e.g., [15, Props. 3.9 and 3.17]) that when x is a local solution of (NSDP), then (x) is nonempty and compact if, and only if, Robinson's CQ holds at x.
Given the properties and characterizations recalled above, the nondegeneracy condition is typically considered the natural extension of LICQ from NLP to NSDP, while Robinson's CQ is considered the extension of MFCQ.

3

2.2 A sequential optimality condition connected to the external penalty method

If we do not assume any CQ, every local minimizer of (NSDP) can still be proved to satisfy at least a sequential type of optimality condition that is deeply connected to the classical external penalty method. Namely:
Theorem 2.1. Let x be a local minimizer of (NSDP), and let {k}kN  +. Then, there exists some {xk}kN  x, such that for each k  N, xk is a local minimizer of the regularized penalized function

F

(x)

=.

f (x)

+

1 2

x-x

2 2

+

k 2

Sm (-G(x)) +

2.

Proof. See [10, Thm. 3.2]. For a more general proof, see the first part of the proof of [4, Thm. 2].

Note that Theorem 2.1 provides a sequence {xk}kN  x such that each xk satisfies, with an error k  0+, the first-order optimality condition of the unconstrained minimization problem

Minimize xRn

f (x)

+

k 2

Sm (-G(x)) 2, +

so {xk}kN characterizes an output sequence of an external penalty method.

{Y k}kN  Sm + , where

Y

k

=.

k

Sm +

(-G(xk

))

Moreover, the sequence

for every k  N, consists of approximate Lagrange multipliers for x, in the sense that xL(xk, Y k)  0 and
complementarity and feasibility are approximately fulfilled, in view of Moreau's decomposition ­ indeed, note that G(xk) + k, Y k = 0 and G(xk) + k 0, with k = -Sm (-G(xk))  0, for every k  N.
+
These sequences will suffice to obtain the results of the first part of this paper (Section 3), but in order

to extend their scope to a larger class of iterative algorithms, in Section 4, we will need a more general

sequential optimality condition, which will be presented later on.

2.3 Reviewing constant rank-type constraint qualifications for NLP

This section is meant to be a brief review of the main results regarding the classical nonlinear program-

ming problem:

Minimize f (x), xRn
subject to g1(x)  0, . . . , gm(x)  0,

(NLP)

where f, g1, . . . , gm : Rn  R are continuously differentiable functions. As far as we know, the first constant rank-type constraint qualification was introduced by Janin [22], to

obtain directional derivatives for the optimal value function of a perturbed NLP problem. Janin's condition

is defined as follows:

Definition 2.2. Let x  F. The constant rank constraint qualification for (NLP) (CRCQ) holds at x if there exists a neighborhood V of x such that, for every subset J  {i  {1, . . . , m} : gi(x) = 0}, the rank of the family {gi(x)}iJ remains constant for all x  V.

As noticed by Qi and Wei [26] it is possible to rephrase Definition 2.2 in terms of the "constant linear dependence" of {gi(x)}iJ for every J. That is, CRCQ holds at x if, and only if, there exists a neighborhood V of x such that, for every J  {i  {1, . . . , m} : gi(x) = 0}, if {gi(x)}iJ is linearly dependent, then {gi(x)}iJ remains linearly dependent for every x  V. Based on this characterization, Qi and Wei proposed a relaxation of CRCQ, which they called constant positive linear dependence (CPLD) condition,
but this was only proven to be a constraint qualification a few years later, in [12]. To properly define CPLD, recall that a family of vectors {zi}iJ of Rn is said to be positively linearly independent when

zii = 0, i  0, i  J  i = 0, i  J.
iJ
Next, we recall the CPLD constraint qualification:
Definition 2.3. Let x  F. The constant positive linear dependence condition for (NLP) (CPLD) holds at x if there exists a neighborhood V of x such that, for every J  {i  {1, . . . , m} : gi(x) = 0}, if the family {gi(x)}iJ is positively linearly dependent, then {gi(x)}iJ remains linearly dependent for all x  V.
Clearly, CPLD is implied by CRCQ, which is in turn implied by LICQ and is independent of MFCQ. Moreover, CPLD is implied by MFCQ, and all those implications are strict [12, 22]. To show that our extensions of CRCQ and CPLD to NSDP are indeed constraint qualifications (Theorem 3.1), we shall take inspiration in [8], where the authors employ Theorem 2.1 together with the well-known Carath´eodory's Lemma :

4

Lemma 2.1 (Exercise B.1.7 of [13]). Let z1, . . . , zp  Rn, and let 1, . . . , p  R be arbitrary. Then, there exists some J  {1, . . . , p} and some scalars ~i with i  J, such that {zi}iJ is linearly independent,

p

izi = ~izi,

i=1

iJ

and i~i > 0, for all i  J.
See also [19]. If one considers equality constraints in (NSDP) separately, one should employ an adapted version of Carath´eodory's Lemma that fixes a particular subset of vectors, which can be found in [8, Lem. 2]. In our current setting, Lemma 2.1 will suffice as is.

3 Constant rank constraint qualifications for NSDP

Based on the relationship between LICQ and CRCQ, the most natural candidate for an extension of CRCQ to NSDP is to demand every subset of

{vij (x, E) : 1  i  j  m - r}

to remain with constant rank (or constant linear dependence) in a neighborhood of x. However, this candidate cannot be a CQ, as shown in the following counterexample, adapted from [2, Eq. 2]: Example 3.1. Consider the problem to minimize f (x) =. -x subject to

G(x) =.

x x + x2

x + x2 x

0.

For this problem, x =. 0 is the only feasible Since G(x) = 0, the columns of the matrix

pEoi=n. t

and, therefore, the unique global I2 form an orthonormal basis of

minimizer of the problem. KerG(x) (the whole space

R2). For this choice of E, we have

v11(x, E) = v22(x, E) = 1 and v12(x, E) = 1 + 2x.

Since they are all bounded away from zero, the rank of every subset of {vij (x, E) : 1  i  j  2} remains

constant for every x around x. However, Note that x does not satisfy the KKT conditions because any

Y =.

Y 11 Y 12

Y 12 Y 22

 (x) would necessarily be a solution of the system

Y 11  0,

Y 22  0,

Y

11 Y

22

-

Y

2 12



0,

Y 11 + 2Y 12 + Y 22 = -1,

which has no solution.

Besides, it is well-known that even if G is affine, not all local minimizers of (NSDP) satisfy KKT, but in

this case every subfamily of {vij (x, E) : 1  i  j  m - r} remains with constant rank for every x  Rn.

What Example namely, denote the

3.1 tells columns

us of

is E

that by e1

E=. =[aI, 2b]maayndbee2a=.ba[cd,

choice of E. d], and take

In fact, let us choose a differentE, a = -1/ 2 and b = c = d = 1/ 2.

This election of E happens to diagonalize G(x) for every x, but it follows that

v11(x, E) = 1 + 2ab(1 + 2x) = -2x; v22(x, E) = 1 + 2cd(1 + 2x) = 2(1 + x); v12(x, E) = (ad + bc)(1 + 2x) = 0,

and the rank of {v11(x, E)} does not remain constant in a neighborhood of x = 0. In light of our previous work [6], the situation presented above is not surprising. Therein, we already
noted that identifying the "good" matrices E allows us to obtain relaxed versions of nondegeneracy and Robinson's CQ for NSDP. This identification can also be used to extend constant-rank type conditions to NSDP and is the starting point for the results we will present in the current manuscript.
For the sake of completeness, let us quickly summarize a discussion raised in [6] before presenting the results of this paper. Consider a feasible point x  F and denote by r the rank of G(x). Observe that r(M ) > r+1(M ) for every M  Sm close enough to G(x). Thus, when r < m, define the set

Er(M ) =.

E  Rm×m-r :

M E = EDiag(r+1(M ), . . . , m(M )) EE = Im-r

,

(7)

5

which consists of all matrices whose columns are orthonormal eigenvectors associated with the m-r smallest

eigenvalues denotes the

of M , which is well defined whenever r(M ) > r+1(M ). diagonal matrix whose diagonal entries are r+1(M ), . . . ,

In (7), m(M ).

Diag(r+1(M ), . By convention,

.E.r,(Mm)(M=.

)) 

when r = m. By construction, Er(M ) is nonempty provided r < m and M is close enough to G(x). In

particular, in this situation, Er(G(x)) is the set of all matrices with orthonormal columns that span KerG(x).

We showed, in [6, Prop. 3.2], that nondegeneracy can be equivalently stated as the linear indepen-

dence of the smaller family, {vii(x, E)}i{1,...,m-r}, as long as this holds for all E  Er(G(x)) instead of a fixed one. Similarly, Robinson's CQ can be translated as the positive linear independence of the family

{vii(x, E)}i{1,...,m-r} for every E  Er(G(x)) [6, Prop. 5.1]. This characterization suggested a weak form of nondegeneracy (and Robinson's CQ) that takes into account only a particular subset of Er(G(x)) instead

of the whole set, which reads as follows:

Definition 3.1 (Def. 3.2 and Def. 5.1 of [6]). Let x  F and let r be the rank of G(x). We say that x satisfies:

· Weak-nondegeneracy condition for NSDP if either r = m or, for each sequence {xk}kN  x, there exists some E  Lim supkNEr(G(xk)) such that the family {vii(x, E)}i{1,...,m-r} is linearly independent;

· Weak-Robinson's CQ condition for NSDP if either r = m or, for each sequence {xk}kN  x, there exists some E  Lim supkNEr(G(xk)) such that the family {vii(x, E)}i{1,...,m-r} is positively linearly independent.

Note that, in general, Lim supkNEr(G(xk))  Er(G(x)), but the reverse inclusion is not always true, meaning Er(G(x)) is not necessarily continuous at x as a set-valued mapping. It then follows that weak-

nondegeneracy is indeed a strictly weaker CQ than nondegeneracy [6, Ex. 3.1]. Moreover, in contrast with

mnoantdriexgecnoenrsatcrya,inwt eiankt-hneonfodremgenGe(rxa)cy=. hDaipapge(ng1s(xto),f.u.l.ly, gm re(cxo)v)er[6L, IPCrQop.w3h.e3n].

G(x) is a Similarly,

structurally diagonal weak-Robinson's CQ

is implied by Robinson's CQ and coincides with MFCQ when G(x) is diagonal.

3.1 Weak constant rank CQs for NSDP
A straightforward relaxation of weak-nondegeneracy and weak-Robinson's CQ, likewise NLP, leads to our first extension proposal of CRCQ and CPLD to NSDP:
Definition 3.2 (weak-CRCQ and weak-CPLD). Let x  F and let r be the rank of G(x). We say that x satisfies the:
· Weak constant rank constraint qualification for NSDP (weak-CRCQ) if either r = m or, for each sequence {xk}kN  x, there exists some E  Lim supkNEr(G(xk)) such that, for every subset J  {1, . . . , m-r}: if the family {vii(x, E)}iJ is linearly dependent, then {vii(xk, Ek)}iJ remains linearly dependent, for all k  I large enough.
· Weak constant positive linear dependence constraint qualification for NSDP (weak-CPLD) if either r = m or, for each sequence {xk}kN  x, there exists some E  Lim supkNEr(G(xk)) such that, for every subset J  {1, . . . , m - r}: if the family {vii(x, E)}iJ is positively linearly dependent, then {vii(xk, Ek)}iJ remains linearly dependent, for all k  I large enough.
For both definitions, I  N, and {Ek}kI is a sequence converging to E and such that Ek  Er(G(xk)) for every k  I, as required by the Painlev´e-Kuratowski outer limit.
Clearly, weak-nondegeneracy implies weak-CRCQ, which in turn implies weak-CPLD. Also, the condition weak-Robinson's CQ implies weak-CPLD as well. However, Robinson's CQ and its weak variant are both independent of weak-CRCQ. In fact, the next example shows that weak-CRCQ is not implied by either (weak-)Robinson's CQ or weak-CPLD.
Example 3.2. Let us consider the constraint

G(x) =.

2x1 + x22 -x22

-x22 2x1 + x22

and note that, for every orthogonal matrix E in the form

E =.

a b

c d

,

we have

v11(x, E) =

2 2(a - b)2x2

and

v22(x, E) =

2 2(c - d)2x2

.

6

Then, at x = 0, we have v11(x, E) = v22(x, E) = [2, 0], so they are linearly dependent, but positively linearly independent for all E  Er(G(x)). However, choosing any sequence {xk}kN  0 such that xk2 = 0 for all k, it follows that the eigenvalues of G(xk):
1(G(xk)) = 2(x1 + x22) and 2(G(xk)) = 2x1,

are simple, with associated orthonormal eigenvectors

u1(G(xk)) =

- 1 , 1 22

and

u2(G(xk)) =

1 , 1 22

,

respectively, for every k  N. Then, the only sequence {Ek}kN such that Ek  Er(G(xk)) for every k, up to sign, is given by a = -1/ 2 and b = c = d = 1/ 2. However, keep in mind that vii(x, E), i  {1, 2}, is invariant to the sign of the columns of E, so v22(xk, Ek) = [2, 0] and v11(xk, Ek) = [2, 4xk2 ] are linearly independent for all large k. Therefore, we conclude that (weak-)Robinson's CQ holds at x, and consequently weak-CPLD also holds, but weak-CRCQ does not hold at x.
Conversely, we show with another counterexample, that weak-CRCQ does not imply (weak-)Robinson's CQ, and neither does weak-CPLD.
Example 3.3. Let us consider the constraint

G(x) =.

x x2

x2 -x

and the point x = 0. Take any sequence {xk}kN  x such that xk = 0 for every k, and consider two subsequences of it, indexed by I+ and I-, such that xk > 0 for every k  I+, and xk < 0 for every k  I-. Then, for every k  I+, we have that:
1(G(xk)) = xk (xk)2 + 1 and 2(G(xk)) = -xk (xk)2 + 1,
are simple, with associated orthonormal eigenvectors uniquely determined (up to sign) by

u1 (G(xk ))

=

1 1k

1+

(xk)2 + 1

xk

,1

and

u2 (G(xk ))

=

1 2k

1-

(xk)2 + 1

xk

,1 ,

where

1k =.

2

1 + (xk)2 + 1 xk

+ 1 and 2k =.

2

1 - (xk)2 + 1 xk

+ 1.

Moreover, one can verify that whenever I+ is an infinite set,

lim u1(G(xk)) = (1, 0) and lim u2(G(xk)) = (0, 1).

kI+

kI+

Then, we have that for all E  Lim supkI+ Er(G(xk)), the vectors

v11(x, E) = 1 and v22(x, E) = -1

are positively linearly dependent. And, in addition, since 1k   and 2k  0, the vectors

v11(xk, Ek) = 1k + 4

(xk)2 + 1 - 2 1k

and

v22(xk, Ek) = 2k - 4

(xk)2 + 1 - 2 2k

are nonzero and have opposite signs; and thus, remain positively linearly dependent, for all large k  I+.

For the indices k  I- the order of 1(G(xk)) and 2(G(xk)) is swapped, together with their respective

eigenvectors, and we have limkI u1(G(xk)) = (0, 1) and limkI u2(G(xk)) = (-1, 0). Hence, for all

-

-

E  Lim supkI Er(G(xk)), the vectors -

v11(x, E) = -1 and v22(x, E) = 1

are also positively linearly dependent. The order of v11(xk, Ek) and v22(xk, Ek) is also swapped, so they
remain positively linearly dependent for all large k  I-. By the above reasoning, observe that any sequence {xk}kN  x, such that xk = 0 for every k  N,
shows that (weak-)Robinson's CQ fails at x. Moreover, if xk = 0 for infinitely many indices, we may simply take Ek = E = I2 for every k, and then v11(xk, Ek) = v11(x, E) = 1 and v22(xk, Ek) = v22(x, E) = -1 are
positively linearly dependent for every k  N. This completes checking that weak-CPLD and weak-CRCQ
both hold at x, while (weak-)Robinson's CQ does not.

7

Just as it happens in NLP, the weak-CPLD condition is strictly weaker than (weak-)Robinson's CQ, and also weaker than weak-CRCQ, which are in turn, independent. Furthermore, let us establish a formal relationship between weak-CRCQ and weak-CPLD, and their NLP counterparts: Proposition 3.1. Let G(x) =. Diag(g1(x) . . . , gm(x)) be a structurally diagonal constraint and let x be such that g1(x)  0, . . . , gm(x)  0. Then, the following statements are equivalent:
1. weak-CRCQ holds at x;

2. For every J  A(x), if the set {gi(x) : i  J} is linearly dependent, then {gi(x) : i  J} is also linearly dependent, for every x close enough to x;
where A(x) =. {i  {1, . . . , m} : gi(x) = 0} is the set of active indices at x.
Proof. Let r =. rank(G(x)), and note that the result follows trivially if m = r. Hence, we will assume that r < m. For simplicity, we will also assume that A(x) = {1, . . . , m - r}.

· 1  2: By contradiction, suppose that there is some J  A(x) and a sequence {xk}kN  x such that {gi(xk) : i  J} is linearly independent for every k, but {gi(x) : i  J} is not. Let {Ek}kN and E be the sequence and its limit point described in Definition 3.2, for this particular {xk}kN. Note that any other set J that contains J such that {gi(xk) : i  J} is linearly independent also fits this
description, so let us assume that J is maximal.

Since G(xk) is diagonal, every eigenvector vk associated with an eigenvalue k must satisfy Gjj (xk)vjk = kvjk for every j  {1, . . . , m}, which implies k = Gjj (xk) or vjk = 0. Moreover, since G is continuous, the m - r smallest eigenvalues of G(xk) converge to zero, and consequently, they are bounded from

above by



=.

1 2

min{Gii(x) :

i



{m - r

+ 1, . . . , m}}

for k large enough. On the other hand, by continuity of G again, the r largest eigenvalues of G(xk)
are bounded from below by  for all k large enough. Hence, it necessarily holds that vjk = 0 for all j  {m - r + 1, ..., m} and for all k large enough. That is, Ek has the form

Ek =

Qk 0

, where Qk  Rm-r×m-r is orthogonal,

(8)

for every k large enough. A simple computation shows us that

m-r

m-r

vii(xk, Ek) =

gj(xk)(Qkji)2, and vii(x, E) =

gj (x)Q2ji

(9)

j=1

j=1

for every i  {1, . . . , m-r}, where Q is the submatrix of E correspondent to the indices of Qk. Observe that
span({gi(xk) : i  J}) = span({gi(xk) : i  {r + 1, . . . , m}}),
for all k large enough; otherwise, there would be a subsequence {xk}kI  {xk}kN and another index j  J such that {gi(xk) : i  J  {j}} is linearly independent for every k  I, contradicting the maximality of J. Hence, for every S  {1, . . . , m - r} we have

span({vii(xk, Ek) : i  S})  span({gi(xk) : i  J})

(10)

for every large enough k. In particular, there exists some S  {1, . . . , m - r} with the same cardinality as J, such that (10) holds with equality for every large k. On the other hand, it follows from (9) that

span({vii(x, E) : i  S})  span({gi(x) : i  J}),
and this implies span({vii(x, E) : i  S}) is a linearly dependent set. However, since {vii(xk, Ek) : i  S} is linearly independent for all k, by weak-CRCQ, we obtain a contradiction. · 2  1: Take Qk = Im-r and Ek as in (8), so we have vii(xk, Ek) = gi(xk) for every i  {1, . . . , m-r} and every k  N, and the result follows immediately.

Using analogous arguments to the proposition above, we can also prove the following: Corollary 3.1. Under the same hypotheses of the previous proposition, the following are equivalent:
1. weak-CPLD holds at x; 2. For every J  A(x), if the set {gi(x) : i  J} is positively linearly dependent, then {gi(x) : i  J}
is linearly dependent, for every x close enough to x.
8

Proof. Note, in (9), that vii(xk, Ek) is generated by a nonnegative linear combination of gi(xk), i  {1, . . . , m - r}. Therefore, every argument in the proof of Proposition 3.1 can be adapted to prove Corol-
lary 3.1. It suffices to consider positive linear independence, instead of linear independence; and the smallest cone generated by {vii(xk, Ek)}iS , instead of the smallest subspace.

Advancing to the main result of this section, which is to prove that weak-CPLD (and therefore, weakCRCQ) guarantees the existence of Lagrange multipliers at all local solutions of (NSDP), we get inspiration in the proof of [12, Thm. 3.1] for NLP, and the proof of [6, Thm. 3.2]. That is, we analyse the sequence from Theorem 2.1 in terms of the spectral decomposition of its approximate Lagrange multiplier candidates, under weak-CPLD. Then, we use Carath´eodory's Lemma 2.1 to construct a bounded sequence from it, that converges to a Lagrange multiplier. As an intermediary step, we also obtain a convergence result of the external penalty method to KKT points under weak-CPLD, a fact that is emphasized in the statement of the next theorem.
Theorem 3.1. Let {k}kN   and {xk}kN  x  F be such that

xL

xk

,

k

Sm +

(-G(xk

))

 0.

If x satisfies weak-CPLD, then x satisfies the KKT conditions. In particular, every local minimizer of (NSDP) that satisfies weak-CPLD also satisfies KKT.

Proof.

Let

Y

k

=.

k

Sm +

(-G(xk

)),

for

every

k



N.

Recall that we assume 1(-G(xk))  . . .  m(-G(xk)),

for every k, and denote by r the rank of KerG(x). Note that when k is large enough, say greater than some

k0, we necessarily have i(-G(xk)) = -m-i+1(G(xk)) < 0 for all i  {m - r + 1, . . . , m}. Let I  N, and

{Ek}kI  E be such that Ek  Er(G(xk)) for every k  I, as described in Definition 3.2. Then, for each

k  I greater than k0, the spectral decomposition of Y k is given by

m-r

Yk =

ki eki (eki ),

i=1

where ki =. [ki(-G(xk))]+  0 and eki denotes the i-th column of Ek, for every i  {1, . . . , m - r}. Since xL(xk, Y k)  0, we have

m-r

f (xk) -

ki DG(xk) eki (eki )  0,

(11)

i=1

but note that

DG(xk )

eki (eki )

 Dx1 G(xk), eki (eki )  (eki )Dx1 G(xk)eki 

= 

...

= 

...

 

=

vii (xk ,

E k ),

Dxn G(xk), eki (eki )

(eki )Dxn G(xk)eki

so we can rewrite (11) as

m-r

f (xk) -

ki vii(xk, Ek)  0.

i=1

Using Carath´eodory's Lemma 2.1 for the family {vii(xk, Ek)}i{1,...,m-r}, for each fixed k  I, we obtain some Jk  {1, . . . , m - r} such that {vii(xk, Ek)}iJk is linearly independent and

m-r

f (xk) -

ki vii(xk, Ek) = f (xk) -

~ki vii(xk, Ek),

i=1

iJ

(12)

where ~ki  0 for every k  I and every i  Jk. By the infinite pigeonhole principle, we can assume Jk is

the same, say equal to J, for all k  I large enough. We claim that the sequences {~ki }kI are all bounded.

In order to prove this, suppose that

mk =. miaJx{~ki }

is unbounded with k  I, divide (12) by mk and note that mk   on a subsequence implies that the

vectors vii(x, E), i  J, are positively linearly dependent. On the other hand, the vectors vii(xk, Ek), i  J,

are linearly independent for all large k, which contradicts weak-CPLD. Finally, note that every collection

of limit points associated with

{i : i  x, which

J} is

Yof=.theiirJrespiueic(tGiv(ex)s)e.quTehnucse,s

{~ki }kN, i x is a KKT

 J, generates point.

a

Lagrange

multiplier

The second part of the statement of the theorem follows from Theorem 2.1.

9

Back to Example 3.1, observe that weak-CPLD does not hold at x = 0, as expected. Indeed, for any sequence {xk}kN  0 such that xk < 0 for all k, the matrix G(xk) has only simple eigenvalues, for all large k, so Ek  Er(G(xk)) is unique up to sign. Without loss of generality, we can assume

Ek =. 1 2

-1 1

1 1

,

and then we have v11(xk, Ek) = -2xk > 0, which is linearly dependent for all k while v11(x, E) = 0 is positively linearly dependent. Thus Definition 3.2 is not satisfied.

Remark 3.1. In [7], we presented a different extension proposal of CRCQ (and CPLD) to NSDP problems with multiple constraints, which is weaker than nondegeneracy (respectively, Robinson's CQ) for a single constraint as in (NSDP) only when the zero eigenvalue of G(x) is simple. We called this definition the "naive extension of CRCQ (and CPLD)". We remark that Definition 3.2 coincides with the naive extension of CRCQ (and CPLD) when zero is a simple eigenvalue of G(x), which makes Definition 3.2 an improvement of it, or a "non-naive variant" of it.

The phrasing of Theorem 3.1 was chosen to call the reader's attention to the fact that it is, essentially, a convergence proof of the external penalty method to KKT points, under weak-CPLD. To obtain a more general convergence result, in the next section we introduce new constant rank-type CQs for NSDP that support every algorithm that converges with a more general type of sequential optimality condition. Then, we prove some properties of these new conditions, and we compare them with weak-CPLD and weak-CRCQ.

4 Stronger sequential-type constant rank CQs for NSDP and
global convergence of algorithms
A more general sequential optimality condition, which was brought from NLP to NSDP by Andreani et al. [10], is the so-called Approximate Karush-Kuhn-Tucker (AKKT) condition. Let us recall one of its many characterizations1 .
Definition 4.1 (Def. 4 of [4]). We say that a point x  F satisfies the AKKT condition when there exist sequences {xk}kN  x and {Y k}kN  Sm + , and perturbation sequences {k}kN  Rn and {k}kN  Sm, such that:
1. xL(xk, Y k) = k, for every k  N; 2. G(xk) + k 0 and G(xk) + k, Y k = 0, for every k  N; 3. k  0 and k  0. Note that {Y k}kN is a sequence of approximate Lagrange multipliers of x, in the sense that Y k is an exact Lagrange multiplier, at x = xk, for the perturbed problem
Minimize f (x) + x - x, k , xRn
subject to G(x) + k 0.
The main goal in enlarging the class of approximate Lagrange multipliers Y k and perturbations k as in Definition 4.1 instead of considering only the ones given by Theorem 2.1, is to capture the output sequences of a larger class of iterative algorithms. In the next two subsections, we illustrate the previous statement. What is remarkable is that the proof of Theorem 3.1 can still be somewhat conducted considering this more general class of sequences, arriving at strong global convergence results for such algorithms (Theorem 4.2).

4.1 Example 1: A safeguarded augmented Lagrangian method

Let us briefly recall a variant of the Powell-Hestenes-Rockafellar augmented Lagrangian algorithm that

employs a safeguarding technique, which is the direct generalization of the one studied in [14]. The variant

we use is also a generalization of [8, Pg. 13] and [10, Alg. 1], for instance. For an arbitrary penalty parameter  > 0 and a safeguarded multiplier Y~
as the Augmented Lagrangian function of (NSDP), which is given by

0, we define L,Y~ : Rn  R

L,Y~

(x)

=.

f (x)

+

 2

Sm +

-G(x)

+

Y~ 

2

-

1 2

Y~

2
.

Since it will be useful in the convergence proof, we compute the gradient of L,Y~ at x below:

L,Y~ (x) = f (x) - DG(x)

Sm +

-G(x)

+

Y~ 

.

(13)

Now, we state the algorithm:

1Definition 4.1 coincides with the AKKT condition presented in [10, Def. 3.1]. See, for instance, [4, Prop. 4].

10

Algorithm 1 Safeguarded augmented Lagrangian method
Input: A sequence {k}kN of positive scalars such that k  0; a nonempty convex compact set B  Sm + ; real parameters  > 1,   (0, 1), and 1 > 0; and initial points (x0, Y~ 1)  Rn × B. Also, define V 0 = .

Initialize k  1. Then:
Step 1 (Solving the subproblem): Compute an approximate stationary point xk of Lk,Y~ k (x), that is, a point xk such that
Lk,Y~ k (xk)  k;
Step 2 (Updating the penalty parameter): Calculate

V k =. Sm +

-G(xk) + Y~ k k

- Y~ k ; k

(14)

Then, a. If k = 1 or V k   V k-1 , set k+1 =. k;
b. Otherwise, take k+1 such that k+1  k.

Step 3 (Estimating a new safeguarded multiplier): Choose any Y~ k+1  B, set k  k + 1 and go to Step 1.

By

the

definition

of

projection

we

have

that

Y~ k

=

Sm (Y~ k +

-

k G(xk ))

if,

and

only

if,

Y~ k, G(xk)



Sm +

and Y~ k, G(xk) = 0, which means that V k = 0 if, and only if, the pair (xk, Y~ k) is primal-dual feasible and

complementary. Moreover, note that Algorithm 1 does not necessarily keep a record of the approximate

multiplier sequence associated with {xk}kN, which is

Y

k

=.

k

Sm +

-G(xk )

+

Y~ k k

.

(15)

Nevertheless, with these multipliers at hand, it is very easy to prove that any feasible limit point x of {xk}kN must satisfy AKKT:

Theorem 4.1. Fix any choice of parameters in Algorithm 1 and let {xk}kN be the output sequence generated by it. If {xk}kN has a convergent subsequence {xk}kI  x, then:

1.

The point x is stationary for the problem of minimizing

1 2

Sm (-G(x)) +

2;

2. If x is feasible, then x satisfies AKKT.

Proof. Let {k}kN  0, {Y~ k}kN  B  Sm + ,  > 1,   (0, 1), and 1 > 0 be the fixed input parameters of Algorithm 1. Moreover, let {k}kN and {V k}kN computed as in Step 2. For simplicity, let us also assume
that I = N.

1. This part of the proof is standard; see, for instance, [4, Prop. 4.3];
2. Define {Y k}kN as in (15) and take k =. V k for all k  N, where V k is as given in (14). Then, it follows from Step 1 that xL(xk, Y k) = Lk,Y~ k (xk)  0. We also have

G(xk) + k = Sm +

G(xk)

-

Y~ k k

for every k  N, which yields

Y k, G(xk) + k

=

0

for

every

k.

If

k



,

then

Vk



Sm (-G(x)) +

by

definition

and

Sm (-G(x)) +

=

0

because

x

is

assumed

to

be

feasible;

on

the

other

hand,

if

k

remains

bounded, then V k  0 due to Step 2-a. Therefore, k  0 and x satisfies AKKT.

Note that when Y~ k is set as zero for every k, then Algorithm 1 reduces to the external penalty method, meaning Theorem 4.1 also covers this method.
11

4.2 Example 2: A sequential quadratic programming method
Next, we recall Correa and Ram´irez's [17] sequential quadratic programming (SQP) method:

Algorithm 2 General SQP method

Input: A real parameter  > 1, a pair of initial points (x1, Y 1)  Rn × Sm + , and an approximation of 2xL(x1, Y 1) given by H1.

Initialize k  1. Then:

Step 1 (Solving the subproblem): Compute a solution dk, together with its Lagrange multiplier

Y k+1, of the problem

Minimize dHkd + f (xk)d,
dRn

(Lin-QP)

subject to G(xk) + DG(xk)d  Sm + ,

and if dk = 0, stop;

Step 2 (Step corrections): Perform line search to find a steplength k  (0, 1) satisfying Armijo's

rule

f (xk + kdk) - f (xk)   kf (xk)dk.

(16)

Step 3 (Approximating the Hessian): Set xk+1  xk + kdk, compute a positive definite approximation Hk+1 of 2xL(xk+1, Y k+1), set k  k + 1, and go to Step 1.

The SQP algorithm generates AKKT sequences as well, as it can be seen in the following proposition:
Proposition 4.1. If there is an infinite subset I  N such that limkI dk = 0 and { Hk }kI is bounded, then any limit point x of {xk}kI satisfies AKKT.

Proof. By the KKT conditions for (Lin-QP), there exists some Y k 0 such that

f (xk) + Hkdk - DG(xk)[Y k] = 0

(17)

G(xk) + DG(xk)dk, Y k = 0.

(18)

Set k =. DG(xk)dk for every k  I and since dk  0, we obtain that limkI Hkdk = 0 and limkI k = 0. Moreover, since dk is feasible, G(xk) + k 0. Thus, x satisfies AKKT.

The hypothesis on the convergence of a subsequence of {dk}kN to zero, directly or indirectly, is somewhat common regarding some types of SQP methods, as well as the boundedness of Hk ­ see, for instance, [9, 17,
26].

4.3 Sequential constant rank CQs for NSDP

Inspired by AKKT, we are led to introduce a small perturbation in weak-CPLD and weak-CRCQ, which makes it stronger, but also brings some useful properties in return. At first, we present it in a form that most resembles Definition 3.2, for comparison purposes. Later, for convenience, we will provide a characterization of it without sequences.

Definition 4.2 (seq-CRCQ and seq-CPLD). Let x  F and let r be the rank of G(x). We say that x satisfies the

1. Sequential CRCQ condition for NSDP (seq-CRCQ) if r = m or, for all sequences {xk}kN  x and {k}kN  Sm with k  0, there exists {Ek}kI  E, I  N, such that Ek  Er(G(xk) + k) for
every k  I and, for every subset J  {1, . . . , m - r}: if the family {vii(x, E)}iJ is linearly dependent, then {vii(xk, Ek)}iJ remains linearly dependent, for all k  I large enough.

2. Sequential CPLD condition for NSDP (seq-CPLD) if r = m or, for all sequences {xk}kN  x and {k}kN  Sm with k  0, there exists {Ek}kI  E, I  N, such that Ek  Er(G(xk) + k) for
every k  I and, for every subset J  {1, . . . , m - r}: if the family {vii(x, E)}iJ is positively linearly dependent, then {vii(xk, Ek)}iJ remains linearly dependent, for all k  I large enough.

Note that the particular, set k

=o.n0lyfodriffevereernycke

between Definitions 3.2 and 4.2 is the perturbation matrix to see that seq-CRCQ and seq-CPLD imply weak-CRCQ and

k  0. In weak-CPLD,

respectively. Moreover, both implications are strict, as we can see in the following example:

12

Example 4.1. Consider the constraint

G(x) =.

x 0

0 -x

at the point x = 0, so in this case r = 2. For every sequence {xk}kN  x, we have (up to sign)

Er(G(xk)) =

1 0

0 1

,

0 1

1 0

,

for every k  N such that xk = x, whereas if xk = x, then Er(G(xk)) is the set of all orthogonal 2 × 2 matrices. Take Ek = I2 for every k  N to see that both, weak-CRCQ and weak-CPLD, hold at x, since
v11(xk, Ek) = 1 and v22(xk, Ek) = -1

are nonzero and (positively) linearly dependent for every k  N. On the other hand, take

k

=.

1

1 + (xk

+

1)2

-xk(xk - 1)2 xk(xk + 1)

xk(xk + 1) xk + 2xk(xk + 1)2

,

and note that the eigenvectors of G(xk) + k are uniquely determined up to sign. Then, since vii(x, E),
i  {1, 2}, is invariant to the sign of the columns of E, we can assume without loss of generality that any Ek  Er(G(xk) + k) has the form

Ek =

1

-1 xk + 1

1 + (xk + 1)2 xk + 1

1

for every k  N, if xk = 0. Then, for any sequence {Ek}kN such that Ek  Er(G(xk) + k) for every k, we have
v11(xk, Ek) = 1 - (xk + 1)2 and v22(xk, Ek) = (xk + 1)2 - 1,
which are both nonzero whenever xk = 0, but if E is a limit point of {Ek}kN, then v11(x, E) = v22(x, E) = 0. Thus, neither seq-CRCQ nor seq-CPLD hold at x.
Furthermore, since nondegeneracy can be characterized as the linear independence of vii(x, E), i  {1, . . . , m - r}, for every E  Er(G(x)) [6, Prop. 3.2], we observe that it implies seq-CRCQ (see also Remark 4.1 at the end of this section), but this implication is also strict. Let us show this with a counterexample.
Example 4.2. We analyse the constraint

G(x) =.

x 0

0 x

at the point x =. 0. For any x  R and any arbitrary orthogonal matrix E  R2×2, note that E has the form

E=

a b

-b a

,

if

det(E) = 1

or

E=

a b

b -a

, if det(E) = -1

(19)

where a2 + b2 = 1. In both cases, we have v11(x, E) = v22(x, E) = a2 + b2 = 1.

That is, v11(x, E) and v22(x, E) are nonzero and linearly dependent, regardless of x and E. Thus, seq-CRCQ holds at x, although nondegeneracy does not. Note that weak-nondegeneracy also fails at x, in this example.
By Example 3.2, we verify that Robinson's CQ does not imply seq-CRCQ; because otherwise, it would also imply weak-CRCQ, contradicting the example. As for the converse, the counterexample below shows that seq-CRCQ does not imply Robinson's CQ either.
Example 4.3. Consider the constraint

G(x) =.

x1 x2

x2 -x1

.

Clearly, the only feasible point is x = 0. Then, due to the linearity of G, it is immediate to see that Robinson's CQ does not hold at x = 0. On the other hand, for any x  R2 and any orthogonal matrix E  R2×2, note
that regardless of the form of E as in (19), we have v11(x, E) = 0, v22(x, E) = 0, and

v11(x, E) = -v22(x, E).

Thus, seq-CRCQ holds at x = 0; see also the characterization of Proposition 5.2.

13

Another important consequence of Example 4.3 is that seq-CPLD is strictly weaker than Robinson's CQ. Next, we will show that seq-CPLD (and, consequently, seq-CRCQ) is enough to establish equivalence between AKKT and KKT with a small adaptation of the proof of Theorem 3.1. Note that in view of Theorem 2.1, any condition that establishes that an AKKT point is also a KKT point is, in particular, a CQ; in addition, such a CQ necessarily supports the global convergence of any algorithm supported by AKKT to KKT points. This includes the algorithms presented in Subsections 4.1 and 4.2, and Yamashita, Yabe, and Harada's primal-dual interior point method for NSDP [31] ­ for details on the latter, see [3]. We should also stress that this convergence result neither assumes compactness of the Lagrange multiplier set nor that it is a singleton.
Theorem 4.2. Let x  F be an AKKT point that satisfies seq-CPLD. Then, x satisfies KKT.

Proof. Let {xk}kN  x, {Y k}kN  Sm + , and {~ k}kN  0 be the AKKT sequences from Definition 4.1. Since i(G(xk)) > 0 for every i  {1, . . . , r}, where r is the rank of G(x), then i(G(xk) + ~ k) > 0 and

m-i+1(Y k) = 0 for every such i and all k large enough. Hence, the spectral decomposition of Y k can be

represented in the format

m-r

Yk =

i(Y k)uki (uki )

i=1

where uk1 , . . . , ukm-r are shared orthonormal eigenvectors between Y k and G(xk) + ~ k, associated with the m - r largest eigenvalues of Y k and the m - r smallest eigenvalues of G(xk) + ~ k, respectively. Defining Ek = [uk1 , . . . , ukm-r] for every k, we obtain

m-r

xL(xk, Y k) = f (xk) -

i(Y k)vii(xk, Ek)  0.

i=1

For each k  N, let P k  Rm×r be a matrix whose columns are orthonormal eigenvectors associated with the r largest eigenvalues of G(xk), and construct





M k =. U k  Diag(1(G(xk)), . . . , r(G(xk)))

0  (U k), (20)





0

Diag((r + 1) xk - x , . . . , m xk - x )

where U k =. [P k, Ek] for every k  N. Note that M k  G(x) and that the m - r smallest eigenvalues of M k are simple, if xk = x, meaning their associated eigenvectors are unique up to sign, when k is large enough. Consequently, vii(xk, Ek) is invariant to the choice of Ek  Er(M k), for all such k, and every
i  {1, . . . , m - r}. The rest of this proof follows the exact same lines as the proof of Theorem 3.1.

Remark 4.1. The "perturbed versions" of weak-nondegeneracy and weak-Robinson's CQ, in the sense of Def-
inition 4.2, are nondegeneracy and Robinson's CQ, respectively. In other words, nondegeneracy (respectively, Robinson's CQ) holds at x  F if, and only if, for every sequence {xk}kN  x and every {k}kN  Sm such that k  0, there is some E  Lim supkNEr(G(xk) + k) such that {vii(x, E) : i  {1, . . . , m - r}} is (positively) linearly independent, where r = rank(G(x)). For more details, see [6, Rem. 3.1].

5 Relationship with metric subregularity CQ
Besides convergence of algortihms, the CQs we present also have implications towards stability and error analysis. We make this link by means of establishing a relationship between seq-CPLD (and seq-CRCQ) and the so-called metric subregularity CQ (also known as the error bound CQ in NLP), defined in our SDP framework as follows:
Definition 5.1 (e.g., Def. 1.1 of [18]). We say that a feasible point x of (NSDP) satisfies the metric subregularity CQ when there exists some  > 0 and a neighborhood V of x such that
dist(x, F)   Sm + (-G(x))
for every x  V. That is, when the set-valued mapping G : Rn  Sm that maps x  G(x) - Sm + is metric subregular at (x, 0)  graph(G). Here dist(x, F) denotes the distance between x and F, and graph(G)  R × Sm is the graph of G.
The metric subregularity CQ is implied by Robinson's CQ, which in turn coincides with a similar condition called metric regularity CQ, and it has relevant implications on the stability analysis of optimization problems ­ for details, we refer to Ioffe's survey [20, 21]. Besides, there are several works addressing the relationship between constant rank constraint qualifications and the metric subregularity CQ in NLP, such as Minchenko and Stakhovski [23], Andreani et al. [8], and others.

14

We will use a sufficient condition for metric subregularity CQ to hold, originally proposed by Minchenko and Stakhovski [23, Thm. 2] for NLP problems. We made a simple extension of it to NSDP, which seems not to have been done before in the literature. It is worth mentioning, nevertheless, that the proof we present is essentially the same as the original one, with some minor adaptations to the NSDP context via Moreau's decomposition.
Proposition 5.1. Let x  F and assume that G is twice differentiable around x. For every given x  Rn, let (x) denote the set of Lagrange multipliers of the problem of minimizing z - x subject to G(z) 0, z  Rn. If there exist numbers  > 0 and  > 0 such that (x)  B(0,  ) =  for every x  B(x, ), then x satisfies metric subregularity CQ.

Proof. Let  and  be as described in the hypothesis. Following the proof of [23, Thm. 2], note that if

x  intF, then it trivially satisfies metric subregularity CQ, so we will assume that x  bdF. Let 0  (0, )

be such that

4 0

In

-

D2G(z)[Y

]

0

for all z  B(x, ) and all Y  cl(B(0, 2 )). Let x  B(x, 0/2) be such that x  F. Although F (x) may

not be well-defined as a function of x, we will use the notation F (x) to denote an arbitrary minimizer

of z - x subject to G(z) 0. Then, by definition, we have that F (x) - x  x - x < 0/2, so

F (x)  B(x, 0/2) and, therefore, F (x) - x  F (x) - x + x - x < 0. Let h : Rn × Sm  R be

defined as

h(z, Y ) =.

z - x, z - F (x) x - F (x)

- G(z), Y

and note that

2zh(z, Y ) =

2 x - F (x)

In - D2G(z)[Y ]

4 0

In

-

D2G(z)[Y

]

0

whenever z  B(x, ) and Y  cl(B(0, 2 )). Thus, h(z, Y ) is convex with respect to its first variable z  B(x, ), for every Y  cl(B(0, 2 )). Now let us fix an arbitrary Y  (x)  cl(B(0,  )), which is nonempty by hypothesis. Recall that, by definition of the set (x), we have that Y is a Lagrange multiplier of the projection problem associated with the point F (x). Hence, 2Y is a Lagrange multiplier of the problem:

Minimize f~x(z) =.

z-x

+

z - x, z - F (x) x - F (x)

,

subject to G(z)

0

(21)

associated with the point F (x), which is a local minimizer of f~x since it is elementary to check that f~x(F (x))  z - x for every z  F, by the definition of projection (for details, see [23, Lem. 3]), with

equality at F (x). Writing the KKT conditions for the problem (21) at F (x) with the Lagrange multiplier

2Y  cl(B(0, 2 )), we obtain

2(F (x) - x) x - F (x)

-

DG(F (x))[2Y

]

=

0

(22)

with G(F (x)), 2Y = 0, which yields

x - F (x) = - x - F (x) - DG(F (x))[2Y ], x - F (x)

 G(F (x)) - G(x), 2Y

(23)

= - G(x), 2Y

after taking inner products of both sides of (22) with x - F (x). The middle inequality follows from the definition of adjoint and the convexity of h(z, Y ) in the first variable. Taking Moreau's decomposition for G(x), we obtain from (23) that

x - F (x)

 - Sm(G(x)), 2Y +

+ Sm(-G(x)), 2Y +

 Sm(-G(x)), 2Y , +

because Y 0, which is self-dual, so Sm(G(x)), 2Y  0; then +

dist(x, F) = x - F (x)  2Y

Sm (-G(x))  2 Sm (-G(x)) .

+

+

Since x was chosen arbitrarily, set  =. 2 and we are done.

Now, to compare metric subregularity CQ with seq-CRCQ and seq-CPLD, we first need to show that they are robust, in the sense they are preserved in a neighborhood of the point of interest. This property may not be clear from Definition 4.2, but it becomes clear after we exhibit a characterization of it without sequences, as follows:
Proposition 5.2. Let x  F and let r be the rank of G(x).

15

· seq-CRCQ holds at x if, and only if, r = m or, for every E  Er(G(x)), there exists some neighborhood V of (x, E) such that for all J  {1, . . . , m - r}, we have that if the family {vii(x, E)}iJ is linearly dependent, then {vii(x, E)}iJ remains linearly dependent for every (x, E)  V;
· seq-CPLD holds at x if, and only if, r = m or, for every E  Er(G(x)), there exists some neighborhood V of (x, E) such that for all J  {1, . . . , m - r}, we have that if the family {vii(x, E)}iJ is positively linearly dependent, then {vii(x, E)}iJ remains linearly dependent for every (x, E)  V.

Proof. We will prove only item 1, since item 2 follows analogously. Let x satisfy seq-CRCQ; by contradiction:

suppose that there exists some E  Er(G(x)), some J  {1, . . . , m - r}, and some sequence {(xk, Ek)}kN 

(x, E) such that {vii(x, E)}iJ is linearly dependent, but {vii(xk, Ek)}iJ is linearly independent for every

large k  N. Let P k r largest eigenvalues

 of

RGm(x×kr),bdeeafinme aUtrkix=.w[hPoks,eEcko]l,umannds

are orthogonal consider M k as

eigenvectors in (20). Set

asskoc=.iatMedk

with the - G(xk)

and note that vii(xk, Ek) is invariant to Ek  Er(k + G(xk)) when k is large, provided that xk = x. This

contradicts seq-CRCQ.

Conversely, let {xk}kN  x and k  0 be any sequences, and let J  {1, . . . , m - r} be any subset.

For each k, pick any Ek  Er(G(xk) + k) and consider the sequence {Ek}kN, which is bounded. Let

I  N and E be arbitrary, as long as {Ek}kI  E. Then, by hypothesis, there exists a neighborhood V

of (x, E) such that if {vii(x, E)}iJ is linearly dependent, then {vii(xk, Ek)}iJ is also linearly dependent

for all large enough k  I, since (xk, Ek)  V for all such k.

In light of the equivalence of Proposition 5.2, we obtain the robustness property.
Proposition 5.3. If seq-CPLD holds at x, then there exists a neighborhood V of x such that seq-CPLD also holds for every x  V  F. Moreover, the same property holds for seq-CRCQ.

Proof. Direct from Proposition 5.2.

Now, using Proposition 5.3, it is possible to prove that seq-CPLD (and seq-CRCQ) implies metric subregularity CQ. We shall do this in the same style as Andreani et al. [8]:
Theorem 5.1. If seq-CPLD holds at x and G is twice differentiable around x, then x satisfies metric subregularity CQ.

Proof. Suppose that metric subregularity CQ does not hold at x. In view of Proposition 5.1, there exist

sequences { k}kN   and {yk}kN  x such that (yk)  cl(B(0,  k)) =  for every k  N.

Now let {zk}kN be such that zk = F (yk) for each k and note that zk  x. By the previous proposition,

zk satisfies metric subregularity for all k large enough. Consequently, there exists a sequence {Y k}kN  Sm +

such that

zk - yk zk - yk

- DG(zk)[Y k] = 0

and G(zk), Y k = 0 for every k, which implies that i(Y k) = 0 for every i  {m - r + 1, . . . , m} and every k  N. Let U k be an arbitrary matrix that diagonalizes Y k and let Ek be the part of it that corresponds to the m - r smallest eigenvalues of G(zk). So

zk - yk zk - yk

m-r

-

i(Y k)vii(xk, Ek) = 0.

i=1

(24)

Again, by Caratheodory' lemma (cf. Lemma 2.1) and the infinite pigeonhole principle, we obtain a set

J  {1, . . . , m - r} such that {vii(xk, Ek) : i  J} is linearly independent and

m-r i=1

i(Y

k)vii(xk, Ek)

=

iJ ki vii(xk, Ek) for Y k  (yk), so Y k

every > k

k where ki  . Let

i(Y mk

k) =.

> 0 for all max{ki : i

i  J. Then, recall from  J} and divide (24) by

the mk

definition to obtain

that that

{vii(x, E) : i  J} is linearly dependent for every limit point E of {Ek}kN, which contradicts seq-CPLD at

x.

6 Conclusion
There are few constraint qualifications available for NSDP, and as far as we know, the use of CQs in the global convergence of algorithms is somewhat limited to nondegeneracy and Robinson's CQ. In contrast, several constraint qualifications have been defined for NLP over the past decades, mostly improving the global convergence of algorithms beyond the case when the set of Lagrange multipliers is bounded. We are in a path to extend these CQs to conic contexts, such as NSDP, that started in [7]. In fact, the results of this paper can be considered a significant improvement of [7] based on our previous developments in [6]. We introduced two weak constant rank CQs for NSDP, called weak-CRCQ and weak-CPLD, which are essentially "diagonal extensions" of their NLP counterparts, in the sense of Proposition 3.1. Namely, one can embed an NLP problem using a structurally diagonal semidefinite constraint and both conditions are preserved. This

16

is a fairly unusual property as this approach usually induces a degenerate NSDP problem; we however believe that this, in some sense, provides a sound mathematical consistency to our approach. These conditions were used to prove convergence of an external penalty method to stationary points, but any application beyond that, besides the mere existence of Lagrange multipliers, is still a subject for investigation. However, they were the starting points for introducing stronger constant rank CQs, called seq-CRCQ and seq-CPLD, with more interesting properties, such as the convergence theory of a larger class of algorithms such as augmented Lagrangians, sequential quadratic programming, and interior point methods, and a property related with the ability to compute error bounds under these conditions. We believe that several other applications of constant rank CQs will appear in the literature, such as the computation of the derivative of the value function of a parameterized NSDP problem and the computation of second-order necessary optimality conditions. In NLP, constant rank CQs are used to define a strong second-order necessary optimality condition that depends on a single Lagrange multiplier, rather than on the full set of Lagrange multipliers, which we believe will be the case for conic problems as well. It is also the case that constant rank conditions provide the adequate assumptions for guaranteeing global convergence of algorithms to second-order stationary points, which has not been considered yet in the conic programming literature.
This paper leaves several interesting open questions that can be addressed in future works, such as the use of weak-CRCQ and weak-CPLD in algorithms other than external penalty methods, and the analysis of some stability properties under the conditions introduced in this manuscript. It is also worth recalling that although our conditions were defined by means of sequences, which seems appropriate when talking about convergence of algorithms, we also provided characterizations of them without sequences, in a more classical way, which should foster new applications.
The relationship among the CQs we presented in this paper, and existing ones, is summarized in the following diagram, where (solid) arrows represent (strict) implications, existing CQs are in blue boxes, and new CQs are in green boxes.

nondegeneracy

weak-nondegeneracy

seq-CRCQ

Robinson's CQ

weak-CRCQ

weak-Robinson's CQ

seq-CPLD

weak-CPLD

metric subreg. CQ

Figure 1: Relationship among the new constraint qualifications and some of the existing ones.

References
[1] R. Andreani, C. E. Echagu¨e, and M. L. Schuverdt, Constant-rank condition and second-order constraint qualification, Journal of Optimization theory and Applications, 146 (2010), pp. 255­266, https://doi.org/10.1007/s10957- 010- 9671- 8 .
[2] R. Andreani, E. H. Fukuda, G. Haeser, H. Ram´irez C., D. O. Santos, P. J. S. Silva, and T. P. Silveira, Erratum to: New constraint qualifications and optimality conditions for second order cone programs, To appear in Set-Valued and Variational Analysis, (2021), https://doi.org/10.1007/s11228- 021- 00573- 5 .
[3] R. Andreani, E. H. Fukuda, G. Haeser, D. O. Santos, and L. D. Secchin, On the use of Jordan algebras for improving global convergence of an augmented Lagrangian method in nonlinear semidefinite programming, To appear in Computational Optimization and Applications, (2021), https://doi.org/10.1007/s10589- 021- 00281- 8 .
[4] R. Andreani, W. Go´mez, G. Haeser, L. M. Mito, and A. Ramos, On optimality conditions for nonlinear conic programming, tech. report, 2020, http://www.optimization-online.org/DB_HTML/2020/03/7660.html (accessed 2020/05/18).
17

[5] R. Andreani, G. Haeser, and J. M. Mart´inez, On sequential optimality conditions for smooth constrained optimization, Optimization, 60 (2011), pp. 627­641, http://dx.doi.org/10.1080/02331930903578700 .
[6] R. Andreani, G. Haeser, L. M. Mito, and H. Ram´irez, Weak notions of nondegeneracy in nonlinear semidefinite programming, tech. report, 2020, https://arxiv.org/abs/2012.14810v1 (accessed 2021/02/16).
[7] R. Andreani, G. Haeser, L. M. Mito, H. Ram´irez, D. O. Santos, and T. P. Silveira, Naive constant rank-type constraint qualifications for multifold second-order cone programming and semidefinite programming, To appear in Optimization Letters, (2021), https://doi.org/10.1007/s11590-021-01737-w (accessed 2021/02/16).
[8] R. Andreani, G. Haeser, M. L. Schuverdt, and P. J. S. Silva, A relaxed constant positive linear dependence constraint qualification and applications, Mathematical Programming, Series A, 135 (2012), pp. 255­273, https://doi.org/10.1007/s10107-011-0456-0.
[9] R. Andreani, G. Haeser, M. L. Schuverdt, and P. J. S. Silva, Two new weak constraint qualifications and applications, SIAM Journal on Optimization, 22 (2012), pp. 1109­1135, http://dx.doi.org/10.1137/110843939 .
[10] R. Andreani, G. Haeser, and D. S. Viana, Optimality conditions and global convergence for nonlinear semidefinite programming, Mathematical Programming, Series A, 180 (2020), pp. 203­235, http://dx.doi.org/10.1007/s10107- 018- 1354- 5 .
[11] R. Andreani, J. Mart´inez, A. Ramos, and P. J. S. Silva, Strict constraint qualifications and sequential optimality conditions for constrained optimization, Mathematics of Operations Research, 43 (2018), pp. 693­717, https://doi.org/10.1287/moor.2017.0879.
[12] R. Andreani, J. M. Mart´inez, and M. L. Schuverdt, On the relation between constant positive linear dependence condition and quasinormality constraint qualification, Journal of Optimization Theory and Applications, 125 (2005), pp. 473­485, https://doi.org/10.1007/s10957-004-1861-9.
[13] D. P. Bertsekas, Nonlinear Programming, Athenas Scientific. Belmont, Mass, 1999.
[14] E. Birgin and J. M. Mart´inez, Practical Augmented Lagrangian Methods for Constrained Optimization, SIAM Publications. Philadelphia, 2014.
[15] J. F. Bonnans and A. Shapiro, Pertubation Analysis of Optimization Problems, Springer-Verlag. Berlin, 2000.
[16] S. Boyd and L. Vandenberghe, Convex Optimization, Cambridge University Press, 2004.
[17] R. Correa and H. Ram´irez C., A global algorithm for nonlinear semidefinite programming, SIAM Journal on Optimization, 15 (2004), pp. 303­318, https://doi.org/10.1137/S1052623402417298.
[18] H. Gfrerer, First order and second order characterizations of metric subregularity and calmness of constraint set mappings, SIAM Journal on Optimization, 21 (2011), pp. 1439­1474, https://doi.org/10.1137/100813415 .
[19] G. Haeser, On the global convergence of interior-point nonlinear programming algorithms, Computational and Applied Mathematics, 29 (2010), pp. 125­138, https://doi.org/10.1590/S1807- 03022010000200003 .
[20] A. D. Ioffe, Metric regularity - A survey. Part I. Theory, Journal of the Australian Mathematical Society, 101 (2016), pp. 188­243, https://doi.org/10.1017/S1446788715000701.
[21] A. D. Ioffe, Metric regularity - A survey. Part II. Applications, Journal of the Australian Mathematical Society, 101 (2016), pp. 376­417, https://doi.org/10.1017/S1446788715000695.
[22] R. Janin, Directional derivative of the marginal function in nonlinear programming, Mathematical Programming Studies, 21 (1984), pp. 127­138, https://doi.org/10.1007/BFb0121214.
[23] L. Minchenko and S. Stakhovski, On relaxed constant rank regularity condition in mathematical programming, Optimization, 60 (2011), pp. 429­440, https://doi.org/10.1080/02331930902971377.
[24] L. Minchenko and S. Stakhovski, Parametric nonlinear programming problems under the relaxed constant rank condition, SIAM Journal on Optimization, 21 (2011), pp. 314­332, https://doi.org/10.1137/090761318 .
[25] J. J. Moreau, D´ecomposition orthogonale d'un espace hilbertien selon deux cones mutuellement polaires, Comptes Rendus de l'Academie des Sciences de Paris, 255 (1962), pp. 238­240.
[26] L. Qi and Z. Wei, On the constant positive linear dependence conditions and its application to SQP methods, SIAM Journal on Optimization, 10 (2000), pp. 963­981, https://doi.org/10.1137/S1052623497326629 .
[27] S. M. Robinson, First-order conditions for general nonlinear optimization, SIAM Journal on Applied Mathematics, 30 (1976), pp. 597­610, https://doi.org/10.1137/0130053.
18

[28] R. T. Rockafellar and R. Wets, Variational Analysis, Grundlehren der mathematischen Wissenschaften, v. 317. Springer-Verlag Berlin Heidelberg. Berlin, 2009.
[29] A. Shapiro, First and second order analysis of nonlinear semidefinite programs, Mathematical Programming, Series B, 77 (1997), pp. 301­320, https://doi.org/10.1007/BF02614439.
[30] A. Shapiro and M. K. H. Fan, On eigenvalue optimization, SIAM Journal on Optimization, 5 (1995), pp. 552­569, https://doi.org/10.1137/0805028.
[31] H. Yamashita, H. Yabe, and K. Harada, A primal-dual interior point method for nonlinear semidefinite programming, Mathematical Programming, Series A, 135 (2012), pp. 89­121, https://doi.org/10.1007/s10107- 011- 0449- z .
[32] Y. Zhang and L. Zhang, New constraint qualifications and optimality conditions for second order cone programs, Set-Valued and Variational Analysis, 27 (2019), pp. 693­712, https://doi.org/10.1007/s11228- 018- 0487- 2 .
19

