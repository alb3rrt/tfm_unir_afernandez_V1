COV-ECGNET: COVID-19 detection using ECG trace images with deep convolutional neural network.
Tawsifur Rahman1, Alex Akinbi2, Muhammad E. H. Chowdhury1*, Tarik A. Rashid3, Abdulkadir engür4, Amith Khandakar1, Khandaker Reajul Islam1, Aras M. Ismael5*
1Department of Electrical Engineering, Qatar University, Doha-2713, Qatar 2School of Computer Science and Mathematics, Liverpool John Moores University, United Kingdom 3Computer Science and Engineering Department, School of Science and Engineering, University of Kurdistan Hewler, Erbil, KRG, Iraq. 4Firat University, Technology Faculty, Electrical-Electronics Engineering Department, Elazig, Turkey 5Sulaimani Polytechnic University, College of Informatics, Information Technology Department, Sulaymaniyah, Iraq
*Correspondence: Aras M. Ismael (aras.masoud@usa.com);
Muhammad E. H. Chowdhury (mchowdhury@qu.edu.qa)
Abstract
The reliable and rapid identification of the COVID-19 has become crucial to prevent the rapid spread of the disease, ease lockdown restrictions and reduce pressure on public health infrastructures. Recently, several methods and techniques have been proposed to detect the SARSCoV-2 virus using different images and data. However, this is the first study that will explore the possibility of using deep convolutional neural network (CNN) models to detect COVID-19 from electrocardiogram (ECG) trace images. In this work, COVID-19 and other cardiovascular diseases (CVDs) were detected using deep-learning techniques. A public dataset of ECG images consists of 1937 images from five distinct categories, such as Normal, COVID-19, myocardial infarction (MI), abnormal heartbeat (AHB), and recovered myocardial infarction (RMI) were used in this study. Six different deep CNN models (ResNet18, ResNet50, ResNet101, InceptionV3, DenseNet201, and MobileNetv2) were used to investigate three different classification schemes: i) two-class classification (Normal vs COVID-19); ii) three-class classification (Normal, COVID19, and Other CVDs), and finally, iii) five-class classification (Normal, COVID-19, MI, AHB, and RMI). For two-class and three-class classification, Densenet201 outperforms other networks with an accuracy of 99.1%, and 97.36%, respectively; while for the five-class classification, InceptionV3 outperforms others with an accuracy of 97.83%. ScoreCAM visualization confirms that the networks are learning from the relevant area of the trace images. Since the proposed method uses ECG trace images which can be captured by smartphones and are readily available facilities in low-resources countries, this study will help in faster computer-aided diagnosis of COVID-19 and other cardiac abnormalities.

Keywords: Electrocardiogram (ECG); COVID-19; Deep learning; Convolutional neural networks, Cardiovascular diseases (CVDs).
1. Introduction
Coronavirus Disease 2019 (COVID-19) has rapidly spread with increased fatalities across the world leading to a long-lasting global pandemic. Over 166 million cases have been recorded as of May 21, 2021, with over 3.4 million fatalities documented worldwide [1]. The Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) virus mostly affects the respiratory system, but it can also lead to multi-organ failure. It has a severe impact on the cardiovascular system [26]. The advancement of artificial intelligence in biomedical applications has helped in developing trained networks for reliable computer-aided diagnostic decisions and thus reducing the pressure from the healthcare facilities (such as medical doctors, healthcare staff, etc.) [7]. Several deep learning models have been proposed in recent studies to identify abnormalities from medical images, including chest X-ray images and computerized tomography (CT) scans [8]. Degerli et al. in [9], introduced a novel approach for the combined localization, severity grading, and detection of COVID-19 from 15495 CXR pictures by constructing so-called infection maps, which can accurately localize and grade the severity of COVID-19 infection with 98.69 percent accuracy. For chest X-ray image classification, Kesim et al. proposed a novel convolutional neural network (CNN) model [10]. Since pre-trained CNN models have difficulty in practical applications, the authors designed a small-sized CNN architecture that showed very promising performance in classifying twelve different abnormalities from chest X-ray images (Atelectasis, Cardiomegaly, Consolidation, Edema, Effusion, Emphysema, Fibrosis, Infiltration, Mass, Nodule, Pleural Thickening, Pneumothorax) and reported an accuracy score of 86 %. Liu et al. proposed a tuberculosis (TB) detection technique using chest X-ray and deep learning models[11]. The authors proposed a new CNN model and utilized shuffle sampling to deal with the imbalanced dataset issue and yielded an accuracy score of 85.68 %. Rahman et al.[12] applied various pretrained CNNs to categorize CXR pictures as having pulmonary tuberculosis (TB) symptoms or as being healthy. A dataset of 3,500 infected and 3,500 normal CXR pictures were used to train the suggested model. DenseNet201, the best-performing model, achieved a high detection performance of 98.57 percent sensitivity and 98.56 percent specificity. Chowdhury et al. [13] have

created a public dataset consisting of Normal, Viral Pneumonia and COVID-19 chest X-ray images and used deep CNN models for binary and three class classifications. On the created dataset, transfer learning using pre-trained Squeezenet, Mobilenetv2, Inceptionv3, Chexnet, ResNet, and Densenet201 models were examined. While binary classification had an accuracy score of 99.7 %, three-class classification tasks showed an accuracy of 97.9%. Xu et al. in [14] devised a method for detecting abnormalities in the chest X-ray images. To avoid the over-fitting problem in transfer learning, the authors suggested a hierarchical-CNN model called CXNet-m1. The proposed CNN models were shallower than the pre-trained CNN models. Moreover, a novel loss function and CNN kernel improvement were introduced with an overall accuracy of 67.6 %. In the study by Rahman et al. [15], the authors reported three schemes of classifications: normal vs. pneumonia, bacterial vs. viral pneumonia, and normal, bacterial, and viral pneumonia. Normal and pneumonia images, bacterial and viral pneumonia images, and normal, bacterial, and viral pneumonia images had classification accuracy of 98 %, 95%, and 93.3%, respectively. Chouhan et al. [16] used deep learning models to detect pneumonia in chest X-ray images using five deep transfer learning models and their ensemble. The accuracy of the ensemble deep learning model was 96.4 %. Rajpurkar et al. created ChexNet, a 121-layer CNN architecture for stratifying fourteen distinct lung diseases using chest X-ray images [17]. The authors used the chest X-ray dataset to train the 121-layered DenseNet-121 CNN model. This is the first pre-trained ImageNet model which has been made public re-trained on chest X-ray images. The proposed model produces an area under the curve (AUC) values ranging from 0.704 to 0.944. Li et al. proposed a multi-resolution CNN (MR-CNN) for lung nodule identification [18]. To extract the features, a patch-based MR-CNN model was utilized, and multiple fusion approaches were applied for classification. Free Response Receiver Operating Characteristics (FROC) curve was used for performance evaluation with AUC and Refined Competition Performance Metric (R-CPM) measures of 0.982 and 0.987, respectively. Bhandary et al. tweaked the AlexNet model to detect lung anomalies using chest X-ray images [19]. A new threshold filter and feature ensemble technique were deployed to achieve a classification accuracy of 96%. Ucar et al. [20] employed Laplacian Gaussian filters to improve the classification performance of the CNN models in chest X-ray image classification, which achieved a classification accuracy of 82.43%. Ismael and engür [21] demonstrated different deep learning approaches to detect COVID-19 from chest X-ray images using a Kaggle dataset and

obtained the highest accuracy score 92.63%, which was produced by the ResNet50 model. Their COVID-19 detection was carried out using a variety of multiresolution techniques (Contourlet transform and Wavelet and Shearlet). Extreme Learning Machine (ELM) was applied in the classification stage and the experiment results showed that Wavelet and Shearlet can obtain higher accuracy of 92%.
COVID-19 infection can cause acute myocarditis in apparently healthy people [22]. Up to 27.8% of COVID-19 patients had an increased troponin level beyond the 99th percentile of the upper reference limit, indicating acute myocardial injury in an early case reported from China [23,24]. This is about ten times greater than the influenza rate (2.9%) [25]. Most COVID-19 patients, even those who have biochemical evidence of acute myocardial damage, have a moderate illness history and recover without overt cardiac problems. It is unclear if COVID-19 survivors with no overt cardiac signs have any subclinical or hidden cardiac injury that might impair longterm results. As the pandemic slows, it is critical to figure out if cardiac monitoring in COVID-19 survivors is necessary or not. The potential to screen the general population and give an extra opinion for health care practitioners is the benefit of automated 12-lead electrocardiogram (ECG) diagnostic techniques. Since 1957, attempts have been made to automate the interpretation of ECG recordings, with a focus on findings links to atrial fibrillation (AF). However, the performance of currently available automated methods has been mediocre [26]. Subsequently, despite current technological advancements, notably in the fields of sophisticated machine learning and artificial intelligence (AI) methodologies, the clinical value of automated ECG interpretations remains limited [27,28], and cardiologists continue to analyze and interpret 12-lead ECG recordings using traditional methods. In a recent work by Du et al. in [29], the approach of deep learning on ECG trace images have been explored with promising result. This work will further explore the possibility of COVID-19 and other cardiac abnormality detection with the help of deep learning techniques.
The remainder of this paper is organized as follows: Section 2 discusses the material and methods of the study, while Section 3 outlined the experimental pipeline and evaluation metrics. Section 4 discusses the results and finally, Section 5 concludes the paper.

1.1. Deep convolutional neural networks-based transfer learning Six popular deep learning pre-trained CNN models have been used for COVID-19 detection using ECG trace images. These are ResNet18, ResNet50, ResNet101 [6], DenseNet201 [30], InceptionV3 [31], and MobileNetV2 [32], which are initially trained on ImageNet database. The Residual Network (also known as ResNet) was created to overcome the vanishing gradient and degradation problem [6]. ResNet has different variants based on the number of layers in the residual network: ResNet18, ResNet50, ResNet101, and ResNet152. ResNet is widely utilized for transfer learning in biomedical image classification. During training, deep neural network layers typically learn low or high-level features, whereas ResNet learns residuals rather than features [22]. Figure 1 shows the architecture of a convolutional neural network.
Figure 1: Architecture of a Convolutional Neural Network.
A Dense Convolutional Network (or DenseNet) [30] requires fewer parameters than a traditional CNN as this does not require training on redundant feature maps. The DenseNet has very narrow layers, hence it only adds a small number of new feature maps. DenseNet has four different known variants: DenseNet264, DenseNet169, DenseNet121 and DenseNet20. DenseNet provides straight access to the original input image as well as gradients from the loss function in each layer. As a result, the computational cost of DenseNet has been significantly lowered, making it a superior choice for image classification. Alternatively, MobileNetv2 [32] is not comparable to other networks in-depth, rather this is a compact network. Except for the first layer, which is a full convolution, the rest of the layers are non-convolutional. Except for the last fully connected layer, which has no nonlinearity and feeds into a Softmax layer for classification, the MobileNet structure is constructed on depth-wise

separable convolutions. Batch normalization and Rectified Linear Units (ReLU) nonlinearity are applied to all layers. Before the fully connected layer, a final average pooling reduces the spatial resolution to 1. MobileNet has 28 layers when depth-wise and pointwise convolutions are counted separately. Inception Networks use inception blocks to allow for deeper networks and more efficient computation by reducing dimensionality with layered convolutions.
1.2. Visualization Techniques There is an increased interest in the internal mechanics of CNNs and the rationale for the models' judgments for classification. The visualization techniques aid the interpretation of CNN decisionmaking processes by providing a more visual representation. These also improve the model's transparency by presenting the reason behind the inference in a way that is easily understandable by human, hence enhancing confidence in the neural network's conclusions. SmoothGrad [11], Grad-CAM [12], Grad-CAM++ [13], and Score-CAM [25] are examples of visualization approaches. Because of its promising performance, Score-CAM was used in this study. The outcome is formed by a linear combination of weights and activation maps, with each activation map's weight determined by its forward passing score on the target class and eliminates the dependency of gradients. A sample image visualization with Score-CAM is shown in Figure 2, where the heat map indicates that the region dominantly contributed to the decision making in CNN. This can be useful for understanding how the network makes decisions and for enhancing end-user confidence when it can be confirmed that the network makes decisions using the important segment of ECG trace image all the time.
Figure 2: Score-CAM heat map on ECG trace images to show the important region for making the decision by the CNN.

1.3. Different abnormalities in ECG images In this study, five distinct types of ECG trace images were used in this study, where four out of five are abnormal (COVID-19, myocardial infarction, abnormal heartbeat, and recovered myocardial infarction) and the other one is normal ECG trace images. In clinical terms, a normal ECG trace image represents the ECG of the normal person, who has no abnormality in the ECG trace. Myocardial Infarction (MI), often known as a heart attack, is a form of acute coronary syndrome that defines a sudden or short-term reduction or disruption of blood flow to the heart, causing significant damage to the heart and can be detected by ECG sensing for correct patient diagnosis [33]. Chest pain or discomfort is the most prevalent symptom, which might spread to the shoulder, arm, back, neck, or jaw. Other than the MI ECG trace images of individuals, the dataset includes ECG traces images of the patients who have just recovered from COVID-19 and are experiencing symptoms of shortness of breath or respiratory sickness and the patients who are suffering from other abnormal heartbeats. Moreover, ECG trace images of the patients who are recently recovered from myocardial infarction were also available. Most types of cardiac abnormalities have slight variances in ECG signals, nonetheless, these tiny distinctions (e.g., a peak-peak interval or a particular wave) are frequently used for defining the variables in abnormalities classification, such as ST-segmentation change, P wave height, and T wave abnormality. Figure 3 shows two examples of aberrant kinds that may be recognized by important components. Due to its inability to efficiently gather important and discriminative aspects, deep CNN models' effectiveness is restricted when dealing with picture data challenges.

(A)

(B)

Figure 3: Illustration of two examples of ECG classes: abnormal ECG trace image for the (a) COVID-19 and (b) Myocardial Infarction patients. The subtle signs identified (highlighted in red and pointing via arrows) as key parts to detect the abnormalities.

2. Methodology
Figure 4 summarizes the methodology of this study. As explained earlier, this work has presented three different experimental schemes: i) binary or two-class classification (Normal vs COVID-19); ii) three-class classification (Normal, COVID-19, and Cardiac abnormalities) and finally, iii) fiveclass classification (Normal, COVID-19, MI, AHB, and RMI). Six state-of-the-art CNN models were trained, validated, and tested to detect abnormality from ECG trace images for each of the classification schemes.

Input ECG trace image

Data preprocessing & Data augmentation

Deep
Pre-trained Model  Mobilenetv2  Resnet18  Resnet50  Resnet101  DenseNet201  Inceptionv3

Two-class Three-class Five-class

Normal
COVID-19
Normal COVID-19 Cardiac Abnormality Normal COVID-19 Myocardial Infarction (MI) Abnormal Heartbeat Recovered MI

Figure 4: Overview of the methodology.

The methodology of this research work is described in the following subsections.

2.1. Dataset description

In this study, an ECG image dataset [34] of cardiac and COVID-19 patients is used, which consists

of 1937 distinct patient records with five distinct categories (Normal, COVID-19, myocardial

infarction (MI), abnormal heartbeat (AHB), and recovered myocardial infarction (RMI). All the

data were collected using the ECG device `EDAN SERIES-3' installed in Cardiac Care and

Isolation Units of different health care institutes across Pakistan. Twelve lead ECG trace images

were collected and were manually reviewed by medical professors using a telehealth ECG

diagnostic system, under the supervision of senior medical professionals with experience in ECG

interpretation. Table 1 shows the number of images for different categories in the dataset and some

sample images is shown in Figure 5.

Table 1: Dataset description Category Normal COVID-19

Number of images Sample rate

859

500 Hz

250

Leads 12 leads

Myocardial infarction 77

Abnormal Heartbeat

548

Recovered MI

203

Figure. 5. Sample ECG trace images from the dataset. The horizontal axis represents time, and each time step is represented by a vertical line that lasts 0.04 seconds. Signal magnitudes in millivolts (mV), are represented on the vertical axis.
2.2. Preprocessing To improve the ECG image quality, the files are preprocessed using a gamma correction enhancement technique [35]. In image normalization, linear operations, individual pixels are frequently subjected to operations such as scalar multiplication, addition, and subtraction. Gamma correction is a non-linear procedure that is applied to the pixels of a source image. To improve the image, gamma correction employs the projection relationship between the pixel value and the gamma value according to the internal map, as shown in Figure 6. If A represents the pixel value within a range of 0-255, which represents an angle value. If X represents the grayscale value of the pixel (A), then Equation (1-5) is correct. Let Xm be the midpoint of the range [0, 255]. P is the linear map from group consists of the following elements:

 :   ,  = {| = ()}, () =  () The mapping from  to  is defined as:

:    ,  = {| = ()}

()

{(())==+((()))

() ()

Based on this map, group A can be related to  group pixel values. The arbitrary pixel value is calculated with a given Gamma number. Let  (X) = h(X), and the Gamma correction function is as follows:

 /()

() =  ()

()

Where s(x) represents the output pixel correction value in grayscale. After gamma correction, the dataset is processed to resize the ECG images to fit the input image-size requirements of CNN networks (e.g., 224 by 224 for residual and dense networks, and 299 by 299 for inception network). Using the mean and standard deviation of the images, Z-score normalization of the image was carried out [36].

(A)

(B)

Figure 6: Preprocessing the input image: original ECG trace image (A) and Gamma corrected image (B).

2.3. Augmentation Since the dataset is not balanced and the dataset does not have a similar number of images for the different categories, training with an imbalanced dataset can produce a biased model. Thus, data augmentation for the training set can help in having a similar number of images in the various classes, which can provide reliable results as stated in many recent publications [12,13,15,35,37,38]. In this study, three augmentation strategies (rotation, scaling, and translation) were utilized to balance the training images. The rotation operation used for image augmentation was done by rotating the images in the clockwise and counterclockwise direction with an angle between 5 to 10 degrees. The scaling operation is the magnification or reduction of the frame size of the image and 2.5% to 10% image magnifications were used in this work. Image translation was done by translating images horizontally and vertically by 5% to 20%.
3. Experiments
As discussed in Section 2, three different classification schemes were carried out in this study: two classes (normal vs COVID-19), three classes (normal, COVID-19, and cardiac abnormality), and five classes (normal, COVID-19, myocardial infarction, abnormal heartbeat, and recovered myocardial infarction) classification using different deep learning algorithms. Five-fold crossvalidation was used and therefore, 80% of data were used for training and 20 % for testing. Out of the training dataset subset, 10% were utilized for validation to avoiding overfitting issues [39]. Finally, the results were a weighted average of five folds. Table 2 shows the details of the number of training, validation, and test ECG images used.

Table 2: Details of training, validation, and test set for different classification problem

Classification Types

Total No. of Train

set Validation

images/ class count/fold

count/fold

Two-class

Normal COVID-19

859

619*4=2476

68

250

180*14=2520

20

Normal

859

619*4=2476

68

Three-class COVID-19

250

180*14=2520

20

Five-class

Abnormal Normal

828

597*4=2388

66

859

619*4=2476

68

set Test set count/ fold 172 50 172 50 165 172

COVID-19

250

180*14=2520

20

50

Myocardial Infarction

77

56*43=2408

6

15

Abnormal HB

548

395*6=2370

44

109

Recovered MI

203

147*17=2499

16

40

The networks were built with the PyTorch library and Python 3.7 on an Intel® Xeon® CPU E52697 v4 @ 2,30GHz with 64 GB RAM and a 16 GB NVIDIA GeForce GTX 1080 GPU. All networks were trained using the Adam optimizer with a learning rate of 10-3, a dropout rate of 0.2, a momentum update of 0.9, a mini-batch size of 16 images with 15 backpropagation epochs, and an early stopping threshold of 8 maximum epochs when no improvement in validation loss was seen. Table 3 summarizes the training settings used in the categorization studies.

Table 3: Summary of training parameters for classification experiments

Training parameter

Learning rate 0.001

Batch size 16

Epochs 15

Epoch patience 8

Stopping criteria 8

Optimizer ADAM

3.1. Performance Matrices for Classification

In this study, six CNN models were trained and assessed using five-fold cross-validation. After

the training phase, the performance of multiple networks for the testing dataset was assessed and

compared using six performance indicators, such as accuracy, sensitivity or recall, specificity,

precision (PPV), and F1 score. Equations (6-10) [35] indicate the different matrices for

performance evaluation:

 + 

 = ( + ) + ( + )

(6)

()

 = ( + )

(7)

()

 = ( + )

(8)

()

 = ( + )

(9)

(2  )

1_ = (2   +  + )

(10)

Here, for two-class, true positive (TP) is the number of correctly classified COVID-19 ECG images and true negative (TN) is the number of correctly classified normal images. False-positive (FP) and false-negative (FN) are the misclassified normal and COVID-19 ECG images, respectively. For the three-class, true positive (TP) is the number of correctly classified COVID19 ECG images and true negative (TN) is the number of correctly classified other two classes (normal and abnormal images). False-positive (FP) and false-negative (FN) are the misclassified other two classes (normal and abnormal images) and COVID-19 ECG images, respectively. For the five-class, true positive (TP) is the number of correctly classified COVID-19 ECG images and true negative (TN) is the number of correctly classified other four classes (normal, myocardial infarction, abnormal heartbeat, and recovered myocardial infarction images). False-positive (FP) and false-negative (FN) are the misclassified other four classes (normal, myocardial infarction, abnormal heartbeat, and recovered myocardial infarction images) and COVID-19 ECG images, respectively.

The performance of deep CNNs was assessed using different evaluation metrics with 95% confidence intervals (CIs). Accordingly, CI for each evaluation metric was computed, as shown in

equation (11):

=(ric(1-)/)

(11)

where, N is the number of test samples, and  is the level of significance that is 1.96 for 95% CI.

In addition to the above metrics, the various classification networks were compared in terms of

elapsed time per image, or the time it took each network to classify an input image, as shown in

equation (12).

 = 2 - 1

(12)

In this equation, T1 is the starting time for a network to classify an image, I, and T2 is the end time

when the network has classified the same image, I.

4. Results and Discussion
This section describes the performance of the different classification networks' performance on ECG trace image classification. The comparative performance of different CNNs for two-class, three-class, five-class classification schemes was shown in Table 4. It can be noted that for two

and three class classification schemes DenseNet201 is outperforming while for the five-class

classification InceptionV3 is showing the best performance.

Table 4: Comparison of the performances of the different CNN models for different classification schemes (Best result

is presented as bold)

Classification Model

Overall

Accuracy

2 Class

MobieneV2

98.74 ± 0.52

Resnet18

98.62 ± 0.45

Resnet50

98.92 ± 0.48

Resnet101

99.01 ± 0.46

Densenet201 99.1 ± 0.44

InceptionV3 98.78 ± 0.52

3 Class

MobieneV2

90.79 ± 1.34

Resnet18

92.81 ± 1.19

Resnet50

93.01 ± 1.18

Resnet101

93.02 ± 1.18

Densenet201 97.36 ± 0.74

InceptionV3

96.89 ± 0.8

5 Class

MobieneV2

96.22 ± 0.88

Resnet18

95.34 ± 0.97

Resnet50

96.43 ± 0.86

Resnet101

97 ± 0.79

Densenet201

97.2 ± 0.76

InceptionV3 97.83 ± 0.67

Result with 95% CI

Weighted

Precision Sensitivity F1-score

98.74 ± 0.52 98.74 ± 0.52 98.73 ± 0.52

98.56± 0.35 98.6 ± 0.4 98.8 ± 0.44

98.93 ± 0.48 98.92 ± 0.48 98.91 ± 0.48

99.02 ± 0.46 99.01 ± 0.46 99 ± 0.46

99.11 ± 0.43 99.1 ± 0.44 99.09 ± 0.44

98.62 ± 0.54

100 ± 0

99.31 ± 0.38

91.26 ± 1.3 90.79 ± 1.34 90.76 ± 1.34

92.83 ± 1.19 92.81 ± 1.19 92.8 ± 1.19

93.09 ± 1.17 93.01 ± 1.18 93.01 ± 1.18

93.23 ± 1.16 93.01 ± 1.18 92.99 ± 1.18

97.4 ± 0.74 97.36 ± 0.74 97.36 ± 0.74

96.9 ± 0.8

96.9 ± 0.8 96.89 ± 0.8

96.29 ± 0.87 96.22 ± 0.88 96.2 ± 0.88

95.44 ± 0.96 95.34 ± 0.97 95.28 ± 0.98

96.43 ± 0.86 96.43 ± 0.86 96.4 ± 0.86

97.07 ± 0.78 97 ± 0.79 96.95 ± 0.79

97.2 ± 0.76 97.21 ± 0.76 97.2 ± 0.76

97.82 ± 0.67 97.83 ± 0.67 97.82 ± 0.67

Specificity 96.23 ± 0.88 96.21 ± 0.78 96.28 ± 0.87 96.59 ± 0.84
96.9 ± 0.8 95.2 ± 0.99 92.75 ± 1.2 94.44 ± 1.06 94.59 ± 1.05 94.54 ± 1.05 97.93 ± 0.66 97.6 ± 0.71 97.73 ± 0.69 97.02 ± 0.79 97.93 ± 0.66 97.97 ± 0.65 98.63 ± 0.54 98.86 ± 0.49

Inference time
0.18 0.26 0.44 0.85 1.34 1.26 0.22 0.31 0.48 0.88 1.4 1.36 0.25 0.33 0.52 0.92 1.61 1.68

For two-class (Normal vs COVID-19) classification, overall test accuracy was 99.1% using Densenet201, while for three-class (Normal, COVID-19 and other cardiac abnormality) classification, it was 97.36% using Densenet201, and for five-class (normal, COVID-19, myocardial infarction, abnormal heartbeat, and recovered MI) classification, it was found to be 97.83% with InceptionV3. Figure 7 depicts the area under the curve (AUC) / receiver-operating characteristics (ROC) curve (also known as AUROC (area under the receiver operating characteristics) for various classification schemes, which is one of the most essential assessment metrics for determining the success of any classification model. In two-class and three-class classification, DenseNet201 shows better performance than the other techniques, as shown in Figure 7, where InceptionV3 outperforms the other algorithms for five-class classification.

Figure 7: ROC curves for two-class, three-class, and five-class classifications for ECG images.
Figure 8 illustrates the confusion matrix for the outperforming model for ECG trace image classification schemes: two classes (Densenet201), three classes (Densenet201), and five classes (Inceptionv3). It is worth noting that with the top-performing network, 10 out of 250 COVID-19 ECG images were incorrectly categorized as normal for two-class classification, however, none of the COVID-19 ECG images was incorrectly categorized as normal or other classes for three-class or five-class classification. This is an outstanding performance from any computer-aided classifier,

and this can significantly help in the fast diagnosis of COVID-19 by the clinicians immediately after acquiring the ECG images.
Figure 8: Confusion matrix for (A) Normal and COVID-19, (B) Normal, COVID-19, and Abnormal classification for Densenet201 model, and (C) Normal, COVID-19, Myocardial infarction, Abnormal heartbeat, and Recovered MI classification for Inceptionv3 model.
Figure 9 shows the comparison of accuracy versus the elapsed time per image for different CNN models for two-class, three-class, and five-class classification. While Densenet201 outperforms other networks for two-class, and three-class classification and Inceptionv3 outperform other networks for five-class classification, these are the slowest networks; however, these networks took approximately a second to take decision. For two-class and five-class classification, all

network performances are comparable, where for three-class, Densenet201 outperforms other networks by 4-7%.

Two-class Classification

Three-class Classification

(A)

(B)

Five-class Classification

(C)
Figure 9: Accuracy vs inference time plot for two-class (A), three-class (B), and five-class (C) classifications.
Figure 10 shows the training and validation loss versus epochs for the three best-performing networks for two-class (DenseNet201), three-class (DenseNet201), and five-class (InceptionV3) classification. It can also be seen that the networks reach and stabilize with the lowest loss earlier after few epochs.

Figure 10: Training and Validation Losses versus Epoch for (A) two-class, (B) three-class, and (C) five-class classification.
As mentioned previously, it is critical to determine if the network is learning from the relevant area of the ECG trace images or from somewhere else and any non-related data for taking the decision. Heat maps based on the Score-CAM technique were created for distinct classes of the ECG trace images. Figure 11 depicts samples ECG trace images for 3-class classification as well as heat maps created using the best-performing DenseNet201 model. CNN learns from the regions where various waves change for various classes and the areas that are most important in determining abnormal ECG images in each of the ECG trace images. In Figure 11 (A-C), we can see that ST-segment and J-point elevation, and abnormal heartbeat occurred for COVID-19, myocardial infarction, and abnormal heartbeat. Reliability of how the network is taking decisions for classification is important to increase the confidence of the end-user in the AI performance. It is easily noticeable that the network learned from the area where ECG waves are changing compared to normal ECG images rather than the outside area of the ECG waves.

Figure 11: Score-CAM visualization of abnormal (COVID-19, myocardial infarction, and abnormal heartbeat) ECG images using the best performing model. The subtle signs are identified as key parts to detect abnormalities (highlighted in red and pointing via arrows).
5. Conclusion
This paper provides a deep Convolutional Neural Networks-based transfer learning strategy for the automated diagnosis of COVID-19 and other cardiac disorders using ECG trace images. The performance of the six different CNN models was evaluated for the classification of three different schemes: two-class classification (Normal and COVID-19), three-class classification (Normal, COVID-19, and Cardiac abnormality) and five-class classification (Normal, COVID-19, myocardial infarction (MI), Abnormal heartbeat (HB), and recovered MI). Densnet201 model outperforms other deep CNN models for two-class, and three-class classifications whereas Inceptionv3 outperform other networks for five-class classification. The best classification accuracy, precision, and recall for the two-class, and three-class classifications were found to be

99.1%, 99.11%, 99.1%, and 97.36%, 97.4%, 97.36%, respectively. For five-class classification, the best classification accuracy, precision, and recall were 97.82%, 97.83%, and 97.82%, respectively. The Score-CAM visualization output demonstrates that the important signal changes in the ECG trace contribute to the decision-making of the network. Automatic abnormality detection from ECG images has a very crucial application in computer-aided diagnosis for critical healthcare problems like this one. This state-of-the-art performance can be a very useful and fast diagnostic tool, which can save a significant number of people who died every year due to delayed or improper diagnosis.
References
[1] S. Christodoulidis, M. Anthimopoulos, L. Ebner, A. Christe, S. Mougiakakou, Multisource Transfer Learning with Convolutional Neural Networks for Lung Pattern Analysis, IEEE J. Biomed. Heal. Informatics. (2017). doi:10.1109/JBHI.2016.2636929.
[2] H. Yang, S. Mei, K. Song, B. Tao, Z. Yin, Transfer-Learning-Based Online Mura Defect Classification, IEEE Trans. Semicond. Manuf. (2018). doi:10.1109/TSM.2017.2777499.
[3] S. Akçay, M.E. Kundegorski, M. Devereux, T.P. Breckon, Transfer learning using convolutional neural networks for object classification within X-ray baggage security imagery, in: Proc. - Int. Conf. Image Process. ICIP, 2016. doi:10.1109/ICIP.2016.7532519.
[4] N. Tajbakhsh, J.Y. Shin, S.R. Gurudu, R.T. Hurst, C.B. Kendall, M.B. Gotway, J. Liang, Convolutional Neural Networks for Medical Image Analysis: Full Training or Fine Tuning?, IEEE Trans. Med. Imaging. (2016). doi:10.1109/TMI.2016.2535302.
[5] S.J.P. and Q. Yang, "A Survey on Transfer Learning," Knowledge and Data Engineering, IEEE Trans. (2010).
[6] CV-Tricks.com, ResNet, AlexNet, VGGNet, Inception: Understanding various architectures of Convolutional Networks, (2021). https://cv-tricks.com/cnn/understandresnet-alexnet-vgg-inception/ (accessed May 16, 2021).
[7] M. Dahmani, M.E.H. Chowdhury, A. Khandakar, T. Rahman, K. Al-Jayyousi, A. Hefny, S. Kiranyaz, An intelligent and low-cost eye-tracking system for motorized wheelchair control, Sensors (Switzerland). (2020). doi:10.3390/s20143936.
[8] Y. Qiblawey, A. Tahir, M.E.H. Chowdhury, A. Khandakar, S. Kiranyaz, T. Rahman, N.

Ibtehaz, S. Mahmud, S. Al-Madeed, F. Musharavati, Detection and severity classification of COVID-19 in CT images using deep learning, ArXiv Prepr. ArXiv2102.07726. (2021). [9] A. Degerli, M. Ahishali, M. Yamac, S. Kiranyaz, M.E.H. Chowdhury, K. Hameed, T. Hamid, R. Mazhar, M. Gabbouj, COVID-19 infection map generation and detection from chest x-ray images, ArXiv. (2020). doi:10.1007/s13755-021-00146-8. [10] E. Kesim, Z. Dokur, T. Olmez, X-ray chest image classification by a small-sized convolutional neural network, in: 2019 Sci. Meet. Electr. Biomed. Eng. Comput. Sci. EBBT 2019, 2019. doi:10.1109/EBBT.2019.8742050. [11] C. Liu, Y. Cao, M. Alcantara, B. Liu, M. Brunette, J. Peinado, W. Curioso, TX-CNN: Detecting tuberculosis in chest X-ray images using convolutional neural network, in: Proc. - Int. Conf. Image Process. ICIP, 2018. doi:10.1109/ICIP.2017.8296695. [12] T. Rahman, A. Khandakar, M.A. Kadir, K.R. Islam, K.F. Islam, R. Mazhar, T. Hamid, M.T. Islam, S. Kashem, Z. Bin Mahbub, M.A. Ayari, M.E.H. Chowdhury, Reliable tuberculosis detection using chest X-ray with deep learning, segmentation and visualization, IEEE Access. (2020). doi:10.1109/ACCESS.2020.3031384. [13] M.E.H. Chowdhury, T. Rahman, A. Khandakar, R. Mazhar, M.A. Kadir, Z. Bin Mahbub, K.R. Islam, M.S. Khan, A. Iqbal, N. Al Emadi, M.B.I. Reaz, M.T. Islam, Can AI Help in Screening Viral and COVID-19 Pneumonia?, IEEE Access. (2020). doi:10.1109/ACCESS.2020.3010287. [14] S. Xu, H. Wu, R. Bie, CXNet-m1: Anomaly Detection on Chest X-Rays with ImageBased Deep Learning, IEEE Access. (2019). doi:10.1109/ACCESS.2018.2885997. [15] T. Rahman, M.E.H. Chowdhury, A. Khandakar, K.R. Islam, K.F. Islam, Z.B. Mahbub, M.A. Kadir, S. Kashem, Transfer learning with deep Convolutional Neural Network (CNN) for pneumonia detection using chest X-ray, Appl. Sci. (2020). doi:10.3390/app10093233. [16] V. Chouhan, S.K. Singh, A. Khamparia, D. Gupta, P. Tiwari, C. Moreira, R. Damasevicius, V.H.C. de Albuquerque, A novel transfer learning based approach for pneumonia detection in chest X-ray images, Appl. Sci. (2020). doi:10.3390/app10020559. [17] P. Rajpurkar, J. Irvin, R.L. Ball, K. Zhu, B. Yang, H. Mehta, T. Duan, D. Ding, A. Bagul, C.P. Langlotz, B.N. Patel, K.W. Yeom, K. Shpanskaya, F.G. Blankenberg, J. Seekins, T.J. Amrhein, D.A. Mong, S.S. Halabi, E.J. Zucker, A.Y. Ng, M.P. Lungren, Deep learning for

chest radiograph diagnosis: A retrospective comparison of the CheXNeXt algorithm to practicing radiologists, PLoS Med. (2018). doi:10.1371/journal.pmed.1002686. [18] X. Li, L. Shen, X. Xie, S. Huang, Z. Xie, X. Hong, J. Yu, Multi-resolution convolutional networks for chest X-ray radiograph based lung nodule detection, Artif. Intell. Med. (2020). doi:10.1016/j.artmed.2019.101744. [19] A. Bhandary, G.A. Prabhu, V. Rajinikanth, K.P. Thanaraj, S.C. Satapathy, D.E. Robbins, C. Shasky, Y.D. Zhang, J.M.R.S. Tavares, N.S.M. Raja, Deep-learning framework to detect lung abnormality ­ A study with chest X-Ray and lung CT scan images, Pattern Recognit. Lett. (2020). doi:10.1016/j.patrec.2019.11.013. [20] M. Uçar, E. Uçar, Computer-Aided Detection of Lung Nodules in Chest X-Rays using Deep Convolutional Neural Networks, Sak. Univ. J. Comput. Inf. Sci. (2019). doi:10.35377/saucis.02.01.538249. [21] A.M. Ismael, A. engür, Deep learning approaches for COVID-19 detection based on chest X-ray images, Expert Syst. Appl. (2021). doi:10.1016/j.eswa.2020.114054. [22] Y. LeCun, K. Kavukcuoglu, C. Farabet, Convolutional networks and applications in vision, in: ISCAS 2010 - 2010 IEEE Int. Symp. Circuits Syst. Nano-Bio Circuit Fabr. Syst., 2010. doi:10.1109/ISCAS.2010.5537907. [23] D. Smilkov, N. Thorat, B. Kim, F. Viégas, M. Wattenberg, SmoothGrad: Removing noise by adding noise, ArXiv. (2017). [24] R.R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, D. Batra, Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization, Int. J. Comput. Vis. (2020). doi:10.1007/s11263-019-01228-7. [25] A. Chattopadhay, A. Sarkar, P. Howlader, V.N. Balasubramanian, Grad-CAM++: Generalized gradient-based visual explanations for deep convolutional networks, in: Proc. - 2018 IEEE Winter Conf. Appl. Comput. Vision, WACV 2018, 2018. doi:10.1109/WACV.2018.00097. [26] Q. Ruan, K. Yang, W. Wang, L. Jiang, J. Song, Clinical predictors of mortality due to COVID-19 based on an analysis of data of 150 patients from Wuhan, China, Intensive Care Med. (2020). doi:10.1007/s00134-020-05991-x. [27] T.R. Nabil Ibtehaz, Muhammad E. H. Chowdhury, Amith Khandakar, Serkan Kiranyaz, M. Sohel Rahman, Anas Tahir, Yazan Qiblawey, EDITH:ECG biometrics aided by Deep

learning for reliable Individual auTHentication, ArXiv.Org. (2021). https://arxiv.org/abs/2102.08026. [28] M.G. Muhammad Uzair Zahid, Serkan Kiranyaz, Turker Ince, Ozer Can Devecioglu, Muhammad E. H. Chowdhury, Amith Khandakar, Anas Tahir, Robust R-Peak Detection in Low-Quality Holter ECGs using 1D Convolutional Neural Network, ArXiv.Org. (2020). https://arxiv.org/abs/2101.01666. [29] N. Du, Q. Cao, L. Yu, N. Liu, E. Zhong, Z. Liu, Y. Shen, K. Chen, FM-ECG: A finegrained multi-label framework for ECG image classification, Inf. Sci. (Ny). (2021). doi:10.1016/j.ins.2020.10.014. [30] G. Huang, Z. Liu, L. Van Der Maaten, K.Q. Weinberger, Densely connected convolutional networks, in: Proc. - 30th IEEE Conf. Comput. Vis. Pattern Recognition, CVPR 2017, 2017. doi:10.1109/CVPR.2017.243. [31] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, Z. Wojna, Rethinking the Inception Architecture for Computer Vision, in: Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit., 2016. doi:10.1109/CVPR.2016.308. [32] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, L.C. Chen, MobileNetV2: Inverted Residuals and Linear Bottlenecks, in: Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit., 2018. doi:10.1109/CVPR.2018.00474. [33] D.A. Morrow, Myocardial Infarction: A Companion to Braunwald's Heart Disease, 2016. [34] A.H. Khan, M. Hussain, M.K. Malik, ECG Images dataset of Cardiac and COVID-19 Patients, Data Br. (2021). doi:10.1016/j.dib.2021.106762. [35] T. Rahman, A. Khandakar, Y. Qiblawey, A. Tahir, S. Kiranyaz, S. Bin Abul Kashem, M.T. Islam, S. Al Maadeed, S.M. Zughaier, M.S. Khan, M.E.H. Chowdhury, Exploring the effect of image enhancement techniques on COVID-19 detection using chest X-ray images, Comput. Biol. Med. 132 (2021) 104319. doi:10.1016/j.compbiomed.2021.104319. [36] P. Liu, J.Y. Wang, Z.K. Yin, Study on illumination compensation method for images, Xi Tong Gong Cheng Yu Dian Zi Ji Shu/Systems Eng. Electron. (2008). [37] M.E.H.C. Anas Tahir, Yazan Qiblawey, Amith Khandakar, Tawsifur Rahman, Uzair Khurshid, Farayi Musharavati, M. T. Islam, Serkan Kiranyaz, Coronavirus: Comparing COVID-19, SARS and MERS in the eyes of AI, ArXiv.Org. (2021).

https://arxiv.org/abs/2005.11524v5. [38] M.E.H. Chowdhury, T. Rahman, A. Khandakar, S. Al-Madeed, S.M. Zughaier, S.A.R.
Doi, H. Hassen, M.T. Islam, An early warning tool for predicting mortality risk of COVID-19 patients using machine learning, ArXiv. (2020). [39] Elite Data Science, Overfitting in Machine Learning: What It Is and How to Prevent It, (2021). https://elitedatascience.com/overfitting-in-machine-learning (accessed May 16, 2021).

