arXiv:2106.01150v1 [stat.ME] 2 Jun 2021

Feature Extraction for Functional Time Series: Theory and Application to NIR Spectroscopy Data
Yang Yanga,
aDepartment of Econometrics and Business Statistics, Monash University, Melbourne, VIC 3145, Australia
Yanrong Yangb
bResearch School of Finance, Actuarial Studies and Statistics, Australian National University, Canberra, ACT 2601, Australia
Han Lin Shangc
cDepartment of Actuarial Studies and Business Analytics, Macquarie University, Sydney, NSW 2109, Australia
Abstract We propose a novel method to extract global and local features of functional time series. The global features concerning the dominant modes of variation over the entire function domain, and local features of function variations over particular short intervals within function domain, are both important in functional data analysis. Functional principal component analysis (FPCA), though a key feature extraction tool, only focus on capturing the dominant global features, neglecting highly localized features. We introduce a FPCA-BTW method that initially extracts global features of functional data via FPCA, and then extracts local features by block thresholding of wavelet (BTW) coefficients. Using Monte Carlo simulations, along with an empirical application on near-infrared spectroscopy data of wood panels, we illustrate that the proposed method outperforms competing methods including FPCA and sparse FPCA in the estimation functional processes. Moreover, extracted local features inheriting serial dependence of the original functional time series contribute to more accurate forecasts. Finally, we develop asymptotic properties of FPCA-BTW estimators, discovering the interaction between convergence rates of global and local features.
Keywords:Functional Principal Component Analysis; Long-run Covariance Estimation; Near-infrared Spectroscopy Data; Regularized Wavelet Approximation.
Corresponding author. Email address: yang.yang3@monash.edu
1

1 Introduction
The rapid improvements in automated data acquisition technology allow researchers to access functional data more frequently. Functional data sequentially recorded over time are often considered as finite realizations of a functional stochastic process {Xt(u)}tZ, where the time parameter t is discrete, and the parameter u is a continuum bounded within a finite interval domain [a, b]. Observations {Xt(u)}tZ are commonly referred to as functional time series. Functional time series can arise when a continuous-time record is separated into natural consecutive time intervals. Examples include daily concentration curves of particulate matter with an aerodynamic diameter of less than 10 µm (e.g., H¨ormann et al. 2015) and monthly sea surface temperature in the "Nin~o region" (e.g., Shang & Hyndman 2011). Alternatively, functional time series can arise when observations that are continuous functions in nature are repeatedly sampled in a period. For example, Figure 1a displays near-infrared (NIR) spectra recorded in monitoring glue curing process of wood panels in 72 experimental trials. The curves in the plot are ordered chronologically according to the colors of the rainbow (Hyndman & Shang 2010).
Functional time series methods and theory have witnessed an upsurge in literature contributions in the past two decades (see, e.g., Bosq 2000, Bosq & Blanke 2007, Kokoszka & Reimherr 2013, Aue et al. 2015, Hyndman & Shang 2009, Klepsch & Klu¨ppelberg 2017, Klepsch et al. 2017, Li et al. 2020). Most existing functional time series modeling methods, including those in the references cited above, rely on functional principal component analysis (FPCA) to project the intrinsically infinite-dimensional functional objects onto directions of a small number of leading functional principal components. FPCA extracts only the dominant modes of variation of a functional object over its entire domain, with captured information referred to as the "main features" of the considered process. However, the "minor" components neglected by FPCA often have highly localized features possessing information on functional variations over particular short intervals within the function domain. A relatively recent dynamic FPCA introduced by Ho¨rmann et al. (2015)
2

employs long-run covariance to include serial dependence of the data, but suffers the same problem of loss of local features in dimension reduction. The problem of FPCA inadequately extracting local features is illustrated in Figure 1. The eigenanalysis results shown in Figure 1c indicate that the first two leading dynamic functional components explain most functional variation of smoothed NIR spectra (see Section 6 for details of smoothing). Removing the empirical functional principal components from observations, residual functions of dynamic FPCA still contain sharp features around 1300 nm and 1900 nm of wavelength, as shown in Figure 1d. Thus, local features are important for the

1.5

1.5

1.0

1.0

Absorbance

Absorbance

0.5

0.5

0.0

0.0

500

1000

1500

2000

Wavelength

(a) Observed NIR spectra.

q

500

1000

1500

2000

Wavelength

(b) Smoothed NIR spectra.

6

-0.06 -0.04 -0.02 0.00 0.02 0.04

5

4

3

Eigenvalues

2

1

q

0

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

0

10

20

30

40

50

60

70

(c) Sample long-run covariance.

500

1000

1500

2000

Wavelength

(d) Dynamic FPCA residuals.

Figure 1: NIR absorption spectra of wood panels and residual functions after extraction of the first two dynamic functional components associated with largest empirical eigenvalues of sample long-run covariance function. Using rainbow plots, curves from the distant past are shown in red, and the most recent curves are in violet.

3

estimation of functional time series, typically in the study of NIR spectroscopy data that possess multiple significant local features.
Based on molecular overtones and combination vibrations of the investigated molecule, NIR spectroscopy generates complex absorption spectra over a region of the electromagnetic spectroscopy. Since many chemical compounds are known to have characteristic absorption bands over certain spectrum regions between 780­2500 nm, to determine composition materials of an object requires studying particular wavelength ranges (i.e., narrow bands with extreme absorption intensity) of the observed NIR spectrum together instead of examining absorptions one frequency at a time (Burns & Ciurczak 2007). Thus, a computational method that can extract "local features" covering multiple frequencies of absorption spectrum is important for NIR spectroscopy analysis in practice. Use the wood panel NIR spectrum illustrated in Figure 1 above as an example. The observed local features between 350­2300 nm linked to composition materials, namely, the wood substrate, curing resin, and moisture content (Cao et al. 2018). Subtle changes in experimental conditions such as temperature and pressure lead to variations of absorption bands over a series of trials. Hence, extracting and modeling local features are essential for monitoring the glue curing process of wood panels. Moreover, local features inheriting serial dependence of the original NIR curves can be used to make forecasts for future experiments. In this paper, we aim at developing a methodology for recovering local features that are ignored by FPCA and for using these extracted local features to make more accurate estimations and forecasts for functional time series.
Most existing feature extraction methods attempt to capture local features of functional data by either restricting function domain (see, e.g., Hall & Hooker 2016, Gellar et al. 2014) or introducing sparseness penalty parameters (see, e.g., Huang et al. 2009, Allen & Weylandt 2019) during dimension reduction. However, truncating function domains to specific intervals to enhance local feature extraction requires well aligned curves with most local features occurring in the same region. Thus, truncating methods are not suitable
4

for analysis of NIR spectroscopy data that generally focus on identifying non-overlapping absorption spikes in observed spectra. In contrast, sparse FPCA methods impose sparsity penalties in regularized eigendecomposition to identify basis functions with local features. However, a single penalty parameter in practice is not sufficient to accommodate for local features of various magnitudes at different scales. As a result, solving optimization problems to identify the optimal penalty parameter can be tricky: a small penalty results in a significant amount of observation noise falsely identified as local features, while a large penalty fails to preserve peak heights of high-magnitude local features.
Unlike the feature extraction methods mentioned above, Johnstone & Lu (2009) considered extracting principal components of high-dimensional data in wavelet domains. The wavelet bases are considered to be natural for uncovering sparse local features in the signal for the following four reasons. First, wavelet transform is a spatially varying decomposition that adapts its effective "window width" to magnitudes of local oscillations in FPCA residual functions. As a result, wavelet-based algorithms can accurately estimate local features at various scales. Second, orthonormal bases of compactly supported wavelets are particularly good at estimating sharp, highly localized features. This character of wavelet transform allows effective detection of local features associated with chemicals that have very narrow absorption bands (i.e., short intervals of wavelength frequencies) but high intensities (i.e., large absorbance coefficients) in NIR spectroscopy data (Burns & Ciurczak 2007). Third, the wavelet transform is computationally efficient. For a given orthonormal wavelet basis, feature extraction can be completed in one step of matrix multiplication known as the "discrete wavelet transform" (for further detail on discrete wavelet transform, see Strang (1989), Daubechies (1992)). Fourth and the most important, many types of functional forms encountered in practice, including NIR absorption spectrum, can be sparsely and uniquely represented by a series of wavelet coefficients. Thus, wavelet transform allows a parsimonious representation of local features using only a relatively small number of estimated coefficients.
5

We propose a two-step algorithm that captures global and local features of functional time series sequentially. Initially, dynamic FPCA is applied to extract global features from the smoothed functional time series. Residuals of dynamic FPCA are then transformed into wavelet domains and block thresholding of wavelet (BTW) coefficients are conducted. Advantages of the FPCA-BTW method over sparse FPCA methods in relation to local feature extraction are demonstrated using simulated data in Section 5.1, and via an empirical application in Section 6. It should be noticed that neither conducting the BTW alone, or conducting the BTW before dynamic FPCA, would effectively capture most global and local features of functional time series in a parsimonious set of estimated wavelet coefficients: First, wavelet approximations requires a fairly large number of coefficients (e.g., 211 for the wood panel spectra, and the number of coefficients would increase if more spectrum frequencies are considered) to summarize all global and local features of a continuous function consisting of non-zero signals over its entire domain. More details of wavelet approximations will be presented in Section 2.3 later. Second, implementing BTW leads to a trade-off between preserving the overall smoothness and attaining to fine details of the true signal (see, page 942, Figure 1 in Antoniadis & Fan 2001, for a depiction of this trade-off). As a result, in practice many local features need to be sacrificed to minimize estimation errors measured by an L2 norm for functional time series. In contrast, after conducting FPCA in the initial step of our proposed FPCA-BTW method isolates significant local features in the format of sparse "spikes" over short segments of a function that contains no signal but noise elsewhere. Then, performing the BTW in the second step yields only a small number of non-zero estimated wavelet coefficients containing information on local features as the thresholding algorithm reduces the remaining least important coefficients to zero.
To the best of our knowledge, there is no precedent research focusing on improving FPCA estimation performance via adequately extracting local features contained in "minor" functional components. The principal orthogonal complement thresholding method of
6

Fan et al. (2013) for the estimation of a high dimensional covariance with a conditional sparsity structure is closely analogous to our work as both methods attempt to produce improved estimation performance for processes consisting of finite common global features and sparse local features.
The rest of the paper is organized as follows. In Section 2, we provide necessary background on FPCA and wavelet approximation, before introducing the FPCA-BTW feature extraction method. Implementation details of the proposed method in estimation and forecasting of functional time series are given in Section 3. Section 4 presents asymptotic properties of FPCA-BTW estimators. In Section 5, we use Monte Carlo simulations to illustrate finite sample performances of FPCA-BTW estimators regarding estimation and forecasting of functional time series. Section 6 presents real data applications on NIR spectroscopy data of wood panels. Finally, Section 7 concludes the paper and provides some discussion and directions for future research.

2 Methodology

2.1 Notations

We start by fixing the notations used in this paper. Let {Xt(u)}tZ denote random

functions defined on a rich enough probability space (, A, P ). Observations {Xt(u)}tZ

are elements of the Hilbert space H = L2([0, 1]) equipped with the inner product x , y =

1 0

x(u)y(u)du.

Each

Xt

is

a

square

integrable

function

satisfying

Xt 2 =

1 0

Xt2(u)du

<

,

where the standard norm on L2([0, 1]) is defined as x = x, x 1/2. Define a notation

X  LpH(, A, P ) such that, for some p > 0, E X p < .

We consider functional time series {Xt(u)}tZ with a general representation given by

K

Xt(u) = µ(u) + t,kk(u) + Zt(u) + t(u), u  [0, 1],

(1)

k=1

7

where µ(u) = E[X (u)] is the mean function; {k(u)}Kk=1 are real-valued orthogonal functions with K a fixed positive integer; a set of pairwise uncorrelated real numbers

{t,k}Kk=1 = {t,1, . . . , t,K} satisfy that var(t,i, t,j) = 0 for any i = j; {Zt(u)}tZ is a set

of functions uncorrelated with {k(u)}Kk=1; {t(u)}tZ is H-white noise with E {t(u)} = 0.

(See Chapter 3 of Bosq (2000) for further detail about strong white noise function in

Hilbert space.) The

K k=1

t,k

k

(u)

in

(1)

containing

dominant

modes

of

variation

of

{Xt(u)}tZ are referred to as "global features", whereas {Zt(u)}tZ with sparse localized

spikes over the function domain [0, 1] are referred to as "local features". We assume

that all eigenvalues of long-run covariance of local features are bounded, and the first K

eigenvalues of long-run covariance function of global features decrease at the rate of O(1).

Extraction of global features and local features from functional time series {Xt(u)}tZ are introduced in Sections 2.2 and 2.3, respectively.

2.2 Extraction of global features
A weakly stationary functional time series {Xt(u)}tZ satisfies that, for all t  Z, (a) Xt(u)  L2([0, 1]), (b) E[Xt(u)] = E[X0(u)] = µ(u), and (c) for all  Z and u, s  [0, 1],

c (u, s) = cov[X0(u), X (s)] = cov[Xt+ (u), Xt(s)],

(2)

with cov[X (u), X (s)] = E[{X (u)-µ(u)}{X (s)-µ(s)}]. C induces an operator L2([0, 1])  L2([0, 1]) given by

1
C (x)(u) = C (u, s)x(s) ds,
0

x  L2([0, 1]), u, s  [0, 1].

When = 0, the autocovariance operator C has a special case of covariance operator C0

defined by C0(x) =

1 0

C0(u,

s)x(s)

ds

for

x



L2([0,

1])

and

u,

s



[0,

1].

In practice, {Xt(u)}tZ often consists of serially correlated observed trajectories. To

incorporate serial dependence carried by lagged observations, recent studies (see, e.g., Rice

8

& Shang 2017, Shang 2019) suggest computing a long-run covariance function C(u, s) as



C(u, s) =

c (u, s), u, s  [0, 1].

(3)

=-

A long-run covariance operator C is then defined as

1
C(x)(u) = C(u, s)x(s) ds
0

x  L2([0, 1]), u, s  [0, 1].

The symmetric positive-definite Hilbert-Schmidt operator C admits a decomposition as


C(x) = k k , x k,
k=1

x  L2([0, 1]),

where {k}kZ+ are the nonincreasing eigenvalues, and {k}kZ+ the corresponding orthonormal eigenfunctions such that C(k) = kk, and i , j = 1 iff i = j. The Karhunen­Lo`eve expansion of a stochastic process Xt(u) is then given by


Xt(u) = µ(u) + t,kk(u),
k=1

where the kth functional component score t,k is a projection of X t(u) = Xt(u) - µ(u) in the direction of the kth eigenfunction k(u), that is, k = X t , k .
According to (1), the main features of the infinite-dimensional {Xt(u)}tZ can be summarized by its first K leading components as

K

Xt(u) = µ(u) + t,kk(u) + et(u),

(4)

k=1

where {et(u)}tZ are error functions after truncation. According to Theorem 2 of Ho¨rmann

et al. (2015), the linear combination of

K k=1

t,kk(u)

obtained

by

dynamic

FPCA

satisfies

9

that, for any other orthonormal basis {k}kZ+ of Hilbert space H,







K

2

K

2

E  X t - t,kk   E  X t -

Xt , k k  .

(5)

k=1

k=1

In rare cases, functional time series {Xt(u)}tZ may possess weak serial dependence. The significance of serial dependence can be determined according to the hypothesis test of Horva´th et al. (2016). Functional observations are treated as independent if c of (2) at all lags apart from = 0 are tested to be negligible. A process that decomposes the covariance operator C0 to extract global features is often referred to as static FPCA to distinguish it from dynamic FPCA. In the remaining of this paper, we present feature extraction results obtained by dynamic FPCA and include feature extraction results associated with static FPCA in the Supplementary document. The aim of this paper is to demonstrate the proposed local feature extraction method can be applied to improve performances of static FPCA and dynamic FPCA, instead of comparing performances of the two versions of FPCA.
It can be seen from (5) that dynamic FPCA can find an optimal representation of global features of {Xt(u)}tZ, but ignores most local features {Zt(u)}tZ. Our proposed two-step feature extraction method will continue to capture any remaining local features from FPCA residuals, as described in Section 2.3.

2.3 Extraction of local features
To extract sharp and highly localized features from FPCA residuals, we consider an orthonormal system of wavelet functions. Wavelet functions combine compact support with various degrees of smoothness, which enables the extraction of signals at a variety of different scales. It has been tested that wavelets can effectively isolate signals from noisy functions in statistical applications (see, e.g., Antoniadis 2007, Ogden 1997). Most recent wavelet applications in statistics adopt the approach of Daubechies (1992) to define two
10

related and specially selected orthonormal parent wavelet functions: the scaling function  and the mother wavelet . Wavelets can then be generated by dilation and translation as

j,p = 2j/2(2jt - p), j,p = 2j/2(2jt - p),

(j  Z+, p = 1, . . . , 2j),

where the index j represents resolution level in wavelet decomposition. This wavelet system

produces wavelet functions forming an orthonormal wavelet basis in L2([0, 1]). With a

primary decomposition level j0  0, local features {Zt(u)}tZ admit a decomposition given

by

2j0

 2j

Zt(u) =

Dj0,p,tj0,p(u) +

Dj,p,tj,p(u),

(6)

p=1

j=j0 p=1

where wavelet coefficients are defined as

1

1

Dj0,p,t = Zt(u)j0,p(u) du, Dj,p,t = Zt(u)j,p(u) du.

0

0

"Approximations" and "details" of Zt(u) are stored in wavelet coefficients Dj0,p,t and Dj,p,t, respectively (Mallat 2009).
According to (1), residual functions {et(u)}tZ consist of highly localized features Zt(u) and random noise t(u) given by

et(u) = Zt(u) + t(u).

The wavelet transform of et(u) can be expressed as

2j0

 2j

et(u) =

Dj0,p,tj0,p(u) +

Dj,p,tj,p(u),

p=1

j=j0 p=1

11

where the empirical wavelet coefficients Dj0,p,t and Dj,p,t are given by

1

1

Dj0,p,t = et(u)j0,p(u) du, Dj,p,t = et(u)j,p(u) du.

0

0

Wavelet coefficients related to detailed structure of et(u) and Zt(u) thus satisfy that, for

any t  Z,

Dj,p,t = Dj,p,t + j,p,t,

(j  Z+, p = 1, . . . , 2j),

(7)

where

j,p,t =

1 0

t(u)j,p(u) du

represents

a

wavelet

transform

of

contamination

noise.

Since local features {Zt(u)}tZ are sparse, a vector of wavelet coefficients Dt = {Dj0,1,t, . . . ,

Dj0,2j0,t, . . .} contains many zeros. Extracting local features is then equivalent to de-

termining non-zero wavelet coefficients Dj,p,t. From a statistical modeling perspective,

the denoising problem of (7) has been commonly approached by shrinking the empirical

wavelet coefficients {Dj,p,t}jZ+,p=1,...,2j one by one (see, e.g., Donoho & Johnstone 1994, Antoniadis & Fan 2001). However, local features of functional data often occur over

short intervals within the function domain that correspond to several consecutive wavelet

coefficients at fine resolution levels. To determine chemical content of an object by NIR

spectroscopy, simultaneously considering the non-zero wavelet coefficients corresponding

to certain distinctive absorption bands of known chemical compounds provides more

accurate composition results than examining absorption value at any single frequency. For

example, local features depicting extreme absorption bands of approximately 1900 nm,

shown in Figure 1d, are summarized into 21 consecutive empirical wavelet coefficients

at the resolution level j = 11. Thus, to enhance extraction of local features, adjacent

wavelet coefficients should be modeled together as a group. For this purpose, we adopt a

block thresholding approach of Cai (2002) to make simultaneous selection of empirical

wavelet coefficients in groups as follows. At each resolution level j, divide the empirical

wavelet coefficients Dj,p,t into non-overlapping blocks of length L. Denote indices of the coefficients in the ath block at level j by ja, i.e., ja = {(j, p) : (a - 1)L + 1  p  aL}.

12

Let Sj2a = pja Dj2,p,t denote the sum of squares of the empirical coefficients in the block. A block is significant if its Sj2a is larger than a threshold Tw =  L2/2J , where  is a threshold constant and  is the noise level. Retaining significant wavelet coefficients while discarding the remaining negligible coefficients leads to a local feature estimator as

2j0

J -1

Zt(u) =

Dj0,p,tj0,p(u) +

Dj,p,tj,p(u)1(Sj2a > Tw) ,

(8)

k=0

j=j0 a pja

where a varies for different resolution levels and 1(·) represents the binary indicator function.
In (8), the block length L and the threshold constant Tw together control global and local adaptivity of the estimator Zt(u). A global adaptive estimator adjusts to the overall regularity of the target function, and a locally adaptive estimator focuses on optimally adapting to subtle and highly localized features along the curve. The optimal selection of parameters L and Tw, together with other implementation details about the FPCA-BTW feature extraction method, are described in Section 3.

3 Implementation details

3.1 Long-run covariance estimation

We first present technical details of extracting global features of a finite sample functional time series. To consider serial dependence of stationary functional observations {Xt(u)}Tt=1, we compute the empirical long-run covariance function as

T

Ch,q(u, s) =

Wq h c (u, s),

(9)

=-T

13

where Wq is a symmetric weight function with bounded support of order q, and h is a

bandwidth parameter; the estimator of c (u, s) is defined in the form of



 1

c

(u,

s)

=

T
 1

T

T- j=1

[Xj (u)

-

µ(u)]

[Xj+

(s)

-

µ(s)]

,

T j=1-

[Xj(u) - µ(u)] [Xj+

(s) - µ(s)] ,

 0; < 0.

The optimal bandwidth parameter h is selected via the "plug-in" algorithm proposed in Rice & Shang (2017). More details about estimating the corresponding Chopt(u, s) are provided in Appendix ?? in the Supplementary document. The empirical long-run covariance operator is then given by

1
C(x)(u) = Chopt(u, s)x(s) ds, 0

x  L2([0, 1]).

Performing eigendecomposition on the empirical long-run covariance operator yields


C(x) = k k , x k,
k=1

x  L2([0, 1]),

where {k}kZ+ are the empirical eigenfunctions, and {k}kZ+ are associated eigenvalues. To facilitate dimension reduction, the dimension of global features K need to be empirically determined. Existing functional time series methods generally select K by requiring that retained functional components should explain a certain level of the total variance, approximately 85% (see, e.g., Chiou 2012, Ho¨rmann et al. 2015, Shang 2019). However, this criterion of cumulative percentage of explained variation has the disadvantage of incorrectly selecting too many components as global features when fast-diverging eigenvalues are present in FPCA analysis. To precisely extract global features, following Li et al. (2020), the value of K is determined as the integer minimizing ratios of two adjacent empirical

14

eigenvalues given by

K = argmin k+1 × 1 k   + 1 k <  ,

1kkmax

k

1

1

(10)

where kmax is a prespecified positive integer,  is a prespecified small positive number, and 1(·) is the indicator function. When without priori information about a possible

maximum of K, it is unproblematic to choose a relatively large kmax, e.g., kmax = #{k|k 

T k=1

k

/T

,

k



1}

(Ahn

&

Horenstein

2013).

Given

that

the

small

empirical

eigenvalues

k for some K < k < kmax are likely to be practically zero, we adopt the threshold constant

 = 1/ ln(max {1, T }) to ensure consistency of K. As described in Section 2.2, it is possible to have nearly independent {Xt(u)}Tt=1 in
practice. The sample covariance operator of independent observations is computed as

1
C0(x)(u) = c0(u, s)x(s) ds,
0

x  L2([0, 1]),

where

c0(u, s)

=

1 T

Tt=1[Xt(u) - µ(u)][Xt(s) - µ(s)] is the empirical covariance kernel with

the

empirical

mean

function

µ(u)

=

1 T

T t=1

Xt(u).

For

such

data

static

FPCA

is

applied

to extract global features from C0(x)(u) using the same criterion as (10).

With FPCA results, functional time series can be estimated by

K
Xt(u) = µ(u) + t,kk(u),
k=1

where

K k=1

t,kk(u)

represents

the

extracted

global

features,

with

the

empirical

principal

component scores defined by t,k =

1 0

[Xt(u)

-

µ(u)]

k

(u)

du.

Removing

the

estimated

mean function and the extracted global features from functional observations leaves residual

functions given by

K
et(u) = Xt(u) - µ(u) - t,kk(u).
k=1

15

In Section 3.2, we present details of recovering local features from {et(u)}Tt=1 through block thresholding of wavelet coefficients.

3.2 Estimation of wavelet coefficients
The continuous wavelet transform formalized by Grossmann & Morlet (1984) can be implemented in computer software such as R (R Core Team 2020) to extract local features from FPCA residuals {et(u)}Tt=1. However, in practice, we most likely only observe discretized values {Xt(ui)}ni=t 1, with nt denoting the number of grid points in the t-th curve. Removing global features evaluated at each grid point leaves discrete residuals {et(ui)}ni=t 1. When equally spaced grids satisfy nt = 2J for t = 1, . . . , T , wavelet transform of {et(u)}Tt=1 can be performed in O(2J ) operations (Mallat 1989).
In situations when functional observations have nondyadic, varying or unequally spaced grid points, the non-linear regularized Sobolev interpolator of Antoniadis & Fan (2001) is adopted to perform the wavelet transform. Local feature extraction can then be completed in the following steps. First, select an orthonormal wavelet family to obtain an orthogonal DWT base matrix W with dimension N × N , where N = 2J  max(n1, . . . , nT ) is a dyadic integer. There are many discrete wavelet families available in the literature. We follow Zhao et al. (2012) and consider the Daubechies least asymmetric wavelets with 10 vanishing moments in the analysis of NIR spectroscopy data. Denote A as a matrix of dimension nt × N whose ith row corresponds to the row of the matrix W T. We interpolate the vector et = [et(u1), . . . , et(unt)]T as

Dt = ATet,

(11)

where Dt = [Dj0,1,t, . . . , Dj0,2j0 ,t, Dj0,1,t, . . . , Dj0,2j0 ,t, . . . , DJ-1,1,t, . . . , DJ-1,2J-1,t]T is a vector of size N (Antoniadis & Fan 2001). The optimal parameters for block thresholding are then selected according to Cai (2002). Specifically, for the block size L and the threshold

16

constant  in (8), L = 2 log2(ln(2J)) and  = 4.5052 are chosen. Noise level of residual functions are estimated by taking the median absolute deviation (MAD) as



=

MAD{DJ-1,p,t/vJ1/-21,p,t : vJ-1,p,t 0.6745

>

0.0001} ,

where {DJ-1,p,t}p=1,...,2J-1 are the empirical wavelet coefficients at the resolution level J - 1, and {vJ-1,p,t}p=1,...,2J-1 are diagonal elements of the matrix V = ATA (Antoniadis & Fan 2001). Next, the first-round block thresholding is implemented according to (8), and intermediate results are denoted as Dt. Subtracting the inverse transform of Dt from discrete residuals gives
et = et - ADt.
The second round empirical wavelet coefficients are then computed as

Dt = Dt + ATet .
Finally, performing block thresholding again on Dt yields the final BTW coefficients Dt = [Dj0,1,t, . . . , Dj0,2j0 ,t, Dj0,1,t, . . . , Dj0,2j0 ,t, . . . , DJ-1,1,t, . . . , DJ-1,2J-1,t]T with many zero entries reflecting the sparseness of the local features. Note that we keep the "approximation" wavelet coefficients unchanged as Dj0,p,t = Dj0,p,t for all p = 1, . . . , 2j0. According to Solo (2001), implementing the estimation method of Antoniadis & Fan (2001) through a tworound block thresholding process simplifies computation. Applying the above procedure to each discrete residual function et leads to a sparse N × T matrix of BTW coefficients D = [D1, . . . , DT ]. The extracted local features are then given by a product AD.
Using the extracted global and local features, we can make improved estimation of the considered functional process and its covariance structure, and produce more accurate forecasts. We demonstrate applications of the proposed feature extraction method using simulated samples in Section 5 and real NIR spectroscopy data in Section 6. Additional

17

technical details about long-run covariance estimation and applications of the FPCA-BTW method are provided in Appendix ?? in the supplementary document.

4 Asymptotic properties

Before presenting assumptions and asymptotic results of long-run covariance based FPCABTW estimators, we introduce some notations. Let L = L(H, H) be the space of bounded linear operators from H to H. We define the operator norm A L = sup x 1 A(x) for A  L. The operator A is compact if there exists two orthonormal bases {k} and {vk}, and a real sequence {k} converging to zero, such that


A(x) = k x , k vk,
k=1

x  H.

A compact operator is said to be a Hilbert-Schmidt operator if

 k=1

2k

<

.

We

denote

the Hilbert-Schmidt norm by A S. For any Hilbert-Schmidt operator A, one can show

that

A

2 S

=

k1 2k and A L  A S (Horva´th & Kokoszka 2012, Chapter 2).

Assumption 1. Functions {Xt(u), u  [0, 1]}tZ are L4 - m -approximable, taking values in L2([0, 1]), satisfying the following conditions:

(i) Xt admits the representation Xt = f (t, t-1, t-2 . . . , t-m+1, t-m, t-m-1, . . .) with i i.i.d. elements taking values in a measurable space S and a measurable function f : S  H.

(ii) E X0 4+d <  for some d > 0, and

(iii) {Xt(u), u  [0, 1]}tZ can be approximated by m-dependent sequences

Xt(m) = f (t, t-1, t-2, . . . , t-m+1, t(,mt-)m, t(,mt-)m-1, . . .),

where {t(,mi )} are independent copies of sequence {t}-<t< defined on the same 18

measurable space S such that

E

Xt - Xt(m) 4

1/4
.

 m=1

4(Xt

-

Xt(m))

<



with

4(Xt

-

Xt(m))

=

Remark 1. Assumption 1 follows the dependence concept for functional time series introduced in H¨ormann & Kokoszka (2010). This assumption is often considered as equivalent conditions to the classic mixing conditions in function spaces (see, e.g., Berkes et al. 2016, Horv´ath et al. 2016, Rice & Shang 2017). Condition (iii) specifies the level of dependence that is allowed within process {Xt(u)}tZ in relation to how well it can be approximated by finite m-dependent processes. Condition (iii) can also be satisfied when 4(Xt - Xt(m)) = O(m-) for some  > 4. Roughly speaking, the Xt(m) defined by the coupling construction in Condition (iii) can be determined by the first m elements t, t-1, . . . , t-m+1. When the measurable space S coincides with H, the sequence {Xt(m)} given by
Xt(m) = f (t, t-1, t-2, . . . , t-m+1, 0, 0, . . .)

is also strictly stationary and m-dependent, satisfying

 m=1

4(Xt

-

Xt(m))

<

.

Assumption 2. The kernel function Wq(·) in (9) satisfies the following standard conditions:

Wq(0) = 1, Wq(u)  1, Wq(u) = Wq(-u), Wq(u) = 0 if |u| > g for some constant g > 0,

and Wq(u) is Lipschitz continuous on [-g, g].

(12)

There exists a q > 0 such that

0

<

lim
u0

Wq(u) - |u|q

1

=

Wq

<

,

and there exists q > q such that

| |q c < ,
=-
19

where c is the lag- autocovariance function defined in (2).
Remark 2. Assumption 2 limits the growing rate of Wq(u) at u = 0, with q referred to as the characteristic exponent of the kernel function by Parzen (1957). The smoother the kernel Wq(u) at zero, the larger the value of q for which Wq is finite. This assumption has been widely adopted in studies on limit behaviors of the long-run covariance estimator (e.g., Berkes et al. 2016, Rice & Shang 2017).
The conditions in Assumptions 1 and 2 can be easily verified for most stationary time series models based on independent innovations. In the following example we illustrate the applicability of Assumptions 1 and 2 using a standard functional linear process (Bosq 2000).
Example 1. (Functional autoregressive process). Suppose   L satisfies  L < 1. Let {t}tZ be a sequence of i.i.d. random elements of mean zero taking values in L2([0, 1]) satisfying E 0 2 < . There exists a unique stationary sequence of random process {Xt(u), u  [0, 1]}tZ taking the form

Xt(u) = (Xt-1)(u) + t(u),

which is referred to as functional autoregressive process of order one (FAR(1)). The

FAR(1) process admits the expansion Xt(u) =

 j=0

j

(t-j

)(u)

where

j

is

the

jth

iterate of the operator . According to Condition (iii), we can define an approximation

Xt(m)(u) =

m-1 j=0

j (t-j )(u)

+

expressed as Xt(u) - Xt(m)(u) =

 j=m

j

((t-mj))(u).

The

approximation

error

can

then

be

 j=m(j(t-j)(u) - j((t-mj))(u)). Using Cauchy­Schwarz

inequality, it can be verified that every operator A  L satisfies 4(A(X ))  A L 4(X ).

Then, it follows that 4(Xt - Xt(m))  2

 j=m



j L

4(Xt

-

Xt(m))

=

O(



m L

4(0)).

This

shows that for the FAR(1) process, Assumption 1 holds as long as 0 has moments

up to order 4 + d for some d > 0. In addition, Lemma 3.2 of Bosq (2000) indicates

20

that E c 2   L E X0 2. Assumption 2 then holds since we have assumed that

E X0 2 

 j=0



j L

E

0 2 < .

To ensure the consistency of the long-run covariance estimator Ch,q in (9), we impose the following condition on the bandwidth parameter h.

Assumption 3. The bandwidth parameter h of long-run covariance estimator in (9)

satisfies

h=

h(T ) 



and

h(T ) T

 0,

as

T



.

We use the optimal value of h selected according to the plug-in bandwidth selection

procedure of Rice & Shang (2017) in developing asymptotic results and conducting

simulation and empirical studies in this paper.

Remark 3. Assumption 3 is a weaker and more standard condition compared to the one used in the Theorem 4.2 of H¨ormann & Kokoszka (2010), that is, h2/T  0. Details of the plug-in algorithm are provided in Appendix ?? in the supplementary document.

Assumption 4. The eigenvalues of the long-run covariance operator C are finite, positive, and distinctive, i.e.,  > 1 > 2 > . . .. There exists a positive integer K such that

 k=K +1

k

K k=1

k

= o(1).

(13)

Remark 4. Distinctive eigenvalues of covariance operators are commonly adopted in the literature to ensure identification of eigenfunctions (see, e.g., H¨ormann & Kokoszka 2010, H¨ormann et al. 2015). Assumption 4 requires that the sum of the "insignificant" eigenvalues {K+1, K+2 . . .} tend to zero sufficiently rapidly. Thus, the K-dimensional global feature contains "most information" of Xt(u) (see e.g., Hall & Vial 2006, Bathia et al. 2010). Roughly speaking, Assumption 4 requires that the first K eigenvalues
21

{1, . . . K} have greater orders than the remaining eigenvalues in the sense of (13). For example, denoting a/b  1 by "a  b", Li et al. (2020) proposed that eigenvalues of

long-run covariance function satisfying the conditions (a) k  kT 3-2k for k = 1, . . . , K

with coefficients  > 1  2  . . . K > 0 and 1/2 < 1 < 2 < . . . < K < 1,

and (b)

 k=K +1

k

=

O(T ).

Given that T 3-21 > T 3-22 > . . . T 3-2K > T for a

fixed K, the sum of

K k=1

k

has an order of T 3-21.

It can then be readily seen that

 k=K +1

k/

K k=1

k

=

O(T )/T 3-21

=

o(1)

as

T



.

Hence,

Assumption

4

is

satisfied

with non-zero "insignificant" eigenvalues {K+1, K+2 . . .}. We are going to identify

K and estimate the dynamic space M spanned by the (deterministic) eigenfunctions

1(u), . . . , K(u).

Assumption 5. The dynamic FPC scores {t,k} are uncorrelated across k at all different lags, i.e., cov(t,i, t+h,j) with i = j, i, j = 1, · · · , K, and h  Z.

Remark 5. Assumption 5 specifies the uncorrelatedness of dynamic FPC scores, which is considered as one of the important properties of dynamic FPC scores (see, e.g., page 329, proposition 3(b) in Ho¨rmann et al. 2015).

Assumption 6. The empirical eigenfunctions are in the same direction of the true eigenfunction, i.e., k , k > 0.
Remark 6. Under Assumption 4, the empirical eigenfunctions k recovered are in the same direction, or in the opposite direction, with the true eigenfunction k, i.e., sign( k , k ) = ±1. With Assumption 6, the derivations of equations and proofs are simplified. Note that Assumption 6 is optional for conducting the Karhunen­Lo`eve expansion of a stochastic process X (u) given that X ,   and X , - (-) are identical.

Assumption 7. Let n denote the number of observations on each curve. Let N = 2J  n be a dyadic integer. As N, n  , we assume that (n loga n)-1N tends to a constant for some a > 0. Let Gn be the empirical distribution function of the grid points {u1, . . . , un}.

22

Suppose that there exists a distribution G(u) with density g(u), which is bounded away from 0 and infinity such that
Gn(u)  G(u) for all t  [0, 1] as n  .
Further, g(u) has the th bounded derivative.
Remark 7. Assumption 7 specifies technical conditions ensuring the estimator of (11) is closely approximate to the true signal over the Besov space BP,Q (see Appendix ??). The same assumption was adopted in Antoniadis & Fan (2001) in the development of their Theorem 6. Functional data {Xt(u1), . . . , Xt(un)} measured at dense grids {u1, . . . , un} can easily satisfy Assumption 7. Since the global feature extraction is conducted before the local feature extraction, selections of N and n have no impact on convergence of global feature estimators.
Proposition 1. Under Assumptions 1 to 6, as T   there is
Pr(K = K)  1,
where K is determined by (10).
Remark 8. The estimation approach of (10) has one similarity with the "scree plot" method of Chiou (2012): the estimated dimension of functional principal component is chosen to be the point at which the ordered eigenvalues drop substantially. Similar decision rules are often used to estimate the number of factors for high-dimensional factor models; see Lam & Yao (2012), Lam et al. (2011), and Ahn & Horenstein (2013). For functional time series with short memory, Bathia et al. (2010) adopted an estimator similar to (10) in analysis of the lagged autocovariance operator C for = 0 of the K-dimensional functions satisfying K = 0 and K+1 = 0. Most recently, Li et al. (2020) used an estimator similar to (10) to identify the dimension of the dominant subspace in the long memory functional
23

time series. We fill in the literature gap by using the estimator of (10) when estimating the dimension of long-run covariance operator for short memory functional time series.

We are now ready to present consistency properties of global and local feature estimators in the following theorems.

Theorem 1. Denote the kth empirical functional component by t,k and its associated score by k. Under Assumptions 1 to 6, as T  ,

K

K

t,kk(u) - t,kk(u) = OP T -2/5 .

k=1

k=1

Remark 9. The convergence rate of global feature estimators depends on the weight function Wq and the bandwidth h in (9). We use a flat-top weight function with quadratic spectral kernel (more details see Appendix ??) that has been considered by Andrews (1991), together with the optimal bandwidth selected according to the plug-in method of Rice & Shang (2017). The order of T -2/5 associated with the selected Wq matches findings of Politis & Romano (1996) when the optimal bandwidth is used.
Theorem 2. Denote the number of dyadic points in wavelet transform by N . Under Assumptions 1 to 7, as N, T  , and for some  > 0,
Zt(u) - Zt(u) = OP (N -/(1+2) + T -2/5),
where Zt(u) and Zt(u) are defined in (6) and (8), respectively.
Remark 10. Theorem 1 states the convergence rate for FPCA-based global feature estimators when the optimal bandwidth selected by the plug-in algorithm of Rice & Shang (2017) is used. Here,  indicates the degree of smoothness of the true signal of local features in a Besov ball BP,Q (see ?? in Appendix ?? for the definition of BP,Q). Loosely speaking,
24

the true signal in the Besov space BP,Q has  bounded derivatives in LP space, with finer gradation of smoothness further controlled by the parameter Q (see, e.g., Meyer 1992, for definitions and properties of Besov spaces). Given that local features are estimated after the extraction of global features, convergence of local feature estimators should depend on global feature estimators. This conjecture is confirmed by the term of OP T -2/5 , i.e., the convergence rate of FPCA global feature estimators, in the derived convergence rate for BTW local feature estimators. Theorem 3. Under Assumptions 1 to 7, as N, T  ,
Xt(u) - Xt(u) = OP (N -/(1+2) + T -2/5).
Remark 11. Theorem 3 indicates the estimation error for Xt(u) includes a component from the estimation of global features, and another component from the estimation of local features. As N, T  , both components converge to zero, and we have the total estimation error subsequently converges to zero.
5 Monte Carlo experiments
Finite sample performances of FPCA-BTW estimators are examined through two Monte Carlo experiments. The FPCA-BTW method is applied to make estimation of the functional process and its covariance structure, and produce out-of-sample forecasts. The data generating process for each experiment is calibrated according to a real NIR dataset. Throughout this section, the dimension of global features K is a fixed integer estimated by (10).
25

5.1 Experiment 1

Many common chemical compounds (e.g., chlorinated alkanes) have complex NIR spec-

troscopy spectra consisting of mixed sharp spikes and lower peaks (see, e.g., Burns &

Ciurczak 2007, Figure 21.3). As an example, Figure 2a illustrates the NIR spectrum

of chloroform with formula CHCl3 consisting of two sharp and highly localized features at approximately 1750 nm and 2400 nm, and several lower peaks scattering between

1000 nm and 1300 nm. To generate functional data imitating NIR spectroscopy spectra

of such chemical compounds, we select 1(u) = sin(u) as a basis function for global features, and extend the "bumps" function of Donoho & Johnstone (1994) to simulate

local features. Specifically, choose a kernel function fkernel(u) = (1 + |u|)-4 to gener-

ate 2(u) = fBumps(u) =

11 j=1

sj

fkernel

((u

-

uj

)/wj

),

where

uj ,

wj

and

sj

are

location,

bandwidth and scaling parameters, respectively.

2.5

basis function 1 basis function 2

-0.1 0.0 0.1 0.2 0.3 0.4 0.5

2.0

1.5

Absorbance

1.0

0.5

0.0

1000

1500

2000

2500

Wavelength

(a) NIR spectrum of chloroform.

0

20

40

60

80

100

(b) Orthonormalized basis functions.

Figure 2: Motivation data and designed basis functions for Experiment 1.

Figure 2b presents the orthonormalized basis functions used in this experiment. Coefficients {t,k : k = 1, 2}Tt=1 are generated from autoregressive models of order 1 (AR(1)) of the form t,k = kt-1,k + t,k. Select 1 = 0.8 and t,1  N (0, 4) for {t,1}Tt=1, while choosing 2 = 0.2 and t,2  N (0, 0.01) for {t,2}Tt=1. Combining the generated global and local features gives the true simulated process as XtTRUE = t,11(u) + Zt(u) =
26

t,11(u) + t,22(u). Generate independent noise as t(u) = 0.01Bt(u) with Bt(u) i.i.d. standard Brownian motion {Bt(u)}Tt=1. Finally, functional time series {Xt(u)}Tt=1 is calculated as Xt(u) = XtTRUE + t(u) for u  [0, 1].
For each sample size T  {25, 50, 100}, dynamic FPCA is applied to extract global features, with obtained results denoted by gtF P CA(u). The BTW method, together with competing methods including the unified sparse and functional PCA (SFPCA) method of Allen & Weylandt (2019) and the two-way FPCA (TWFPCA) method of Huang et al. (2009), are applied to extract local features Zt(u) from FPCA residuals. SFPCA and TWFPCA are implemented with a grid search parameter selection approach provided by the MoMA package (Weylandt et al. 2018) in R (R Core Team 2020). Estimation accuracy is assessed by relative squared error (RSE) defined in a simple Riemann sum as

T
RSE =
t=1

2

XtTRUE - gtFPCA - Zt

T 100

XtTRUE - gtFPCA 2

=
t=1 i=1

2

XtTRUE(ui) - gtFPCA(ui) - Zt(ui)

|XtTRUE(ui) - gtFPCA(ui)|2

,

where i = {1, . . . , 100} denote equally spaced discrete realizations over [0, 1]. Given that

the denominator of RSE corresponds to the reconstruction accuracy of the FPCA estimator,

any estimation method with RSE < 1 has a more accurate estimation performance than

the conventional FPCA method. Moreover, the numerator of RSE is proportional to

mean squared estimation error defined by T -1

T t=1

2
Xt(u) - Xt(u) . Thus, small RSE

indicates an efficient local feature extraction method.

Table 1: Mean RSE and running time of various local feature extraction methods (standard
errors in parentheses). The bold entries highlighting the best performing method for each setting.

Sample size T = 25 T = 50 T = 100

RSE Time RSE Time RSE Time

SFPCA
0.687 (0.077) 15.462 (0.603) 0.659 (0.070) 34.288 (1.289) 0.649 (0.052) 89.555 (2.579)

TWFPCA
0.749 (0.058) 21.085 (2.872) 0.737 (0.047) 20.206 (3.686) 0.731 (0.034) 21.089 (2.881)

BTW
0.663 (0.079) 0.154 (0.104) 0.639 (0.067) 0.143 (0.023) 0.629 (0.052) 0.244 (0.041)

27

Table 1 presents RSE averaged over 100 replications for three considered local feature extraction methods, together with computation time (in seconds) for a single iteration in R (R Core Team 2020) on an AMD Ryzen Threadripper 1950X CPU at 3.40GHz. It can be seen that the BTW local feature extraction method consistently outperforms competing methods in estimation accuracy and computation efficiency. All three methods report RSE significantly less than 1, indicating that extracting local features after FPCA dramatically improves estimation accuracy. We note the existence of a greedy "coordinatewise" Bayesian Information Criterion (BIC) optimization scheme by Allen & Weylandt (2019) that can significantly reduce computation time for the SFPCA and TWFPCA methods. However, in this experiment the BIC optimization approach produces RSEs around 1, suggesting that inappropriate penalty parameters are being selected. We present RSEs in relation to static FPCA in Appendix ?? in the Supplementary document.

5.2 Experiment 2

Significant spikes of local features of functional time series are often visible in surface

plots of long-run covariance functions. A good example of such data is spectroscopy of

absorbance on samples of ground pork recorded on a Tecator infrared spectrometer in

the region 850 to 1050 nm (Thodberg 1996). Following Ferraty & Vieu (2006), we apply

dynamic FPCA to 77 Tecator NIR spectroscopy spectra corresponding to samples with

large fat content. Fitting the leading empirical scores associated with the leading functional

components to an AR(1) model returns an estimated coefficient 0.2487. Using analysis

results of the Tecator data, we calibrate the data generating process for Experiment 2 by

choosing

1(u)

=

1 2

exp

{-

u2 2

}

and

generating

{t,1}Tt=1

from

t,1

=

0.2487t-1,1

+

t,1,

where t,1  N (0, 1). To amplify bumps in covariances, local features are generated as



Zt(u)

=

0.5Zt-1(u) 0,

+

0.1Bt(u),

0.25  u < 0.5 ,
elsewhere

28

where i.i.d. Brownian motion innovations {Bt(u), u  [0.25, 0.5]}Tt=1 satisfy Bt(0.25) = 0. 
Finally, independent noise is generated as t(u) = 0.001Bt(u) with Bt(u) i.i.d. standard Brownian motion {Bt(u), u  [0, 1]}Tt=1. Functional time series is computed as Xt(u) = t1(u) + Zt(u) + t(u) for u  [0, 1].
For each simulated time series {Xt(u)}Tt=1, we apply the FPCA-BTW method to extract global and local features, and use the extracted features to reconstruct long-run covariance functions. The dynamic FPCA estimators are considered as comparison benchmarks. Estimation accuracy for covariance is assessed according to relative error (RE) given by

RE =

2

40 40 C(ui, sj) - C(ui, sj)

i=1 j=1

|C(ui, sj)|2

,

where C(u, s) is the theoretical long-run covariance function, and C(u, s) is the reconstructed estimator using extracted features; i, j = {1, . . . , 40} denote equally spaced grid points over [0, 1].
For each T  {200, 500, 1000}, we replicate the experiment 100 times. Throughout the experiment, the empirical dimension of global features is determined to be K = 1 by (10). Figure 3 shows that the FPCA-BTW method produces smaller reconstruction errors than the FPCA method. Hence, the extracted local features are tested to improve long-run covariance estimation accuracy. Finally, it can be easily observed that both FPCA and FPCA-BTW methods report smaller estimation errors when sample sizes increase.
Figure 4 visualizes the advantage of FPCA-BTW estimators in long-run covariance estimation when sample size T = 200. Figure 4c presents the theoretical long-run covariance function that has a "pyramid-shaped bump" corresponding to local features Zt(u). Estimators depicted by Figure 4a fail to capture the "bump" of local features. In contrast, FPCA-BTW estimators successfully recover most information about local features in the presence of intentionally added noise. This experiment shows that local features are essential for the estimation of the long-run covariance function of functional
29

T=200 0.6

T=500

T=1000

0.4

Relative Distance

0.2

0.0 FPCA

FPCA-BTW

FPCA

FPCA-BTW

FPCA

FPCA-BTW

Figure 3: Relative errors of long-run covariance estimators.

time series.

FPCA

FPCA-BTW

Long-run Covariance

0.25

0.20

0.15

0.10

0.0 0.2 0.4 0.6
0.8

1.0 0.8 0.6 0.4
0.2

1.0 0.0

0.25

0.20

0.15

0.10 0.0 0.2 0.4
0.6
0.8

1.0 0.8 0.6 0.4
0.2
1.0 0.0

0.25

0.20

0.15

0.0 0.2 0.4 0.6
0.8

1.0 0.8 0.6 0.4
0.2
1.0 0.0

(a)

(b)

(c)

Figure 4: Surface plots of the mean long-run covariance estimators over 100 simulations obtained by FPCA (blue) and FPCA-BTW (red) for T = 200, along with the true theoretical functions (cyan).

Monte Carlo experiments introduced in Section 5 prove that FPCA-BTW produces the best feature extraction performance among considered methods. We also design experiments to show that local features extracted by BTW help to improve point forecast accuracy, with details included in Appendix ?? in the Supplementary document. In the next section, advantages of the FPCA-BTW method at feature extraction and forecasting functional time series are demonstrated using the empirical wood panel NIR spectroscopy
30

data.

6 Empirical application
The wood panel NIR spectroscopy data illustrated in Figure 1 consists of spectra of absorbance (in negative base ten logarithm of the transmittance) recorded at wavelengths from 350 to 2500 nm in 1 nm intervals in a series of 72 experimental trials. Removing observations from 2301 to 2500 nm because of considerable noise gives n = 1951 discrete realizations on each curve. Figure 1a indicates that raw spectra curves are contaminated by observational noise. Denoting the observed NIR absorbance values at wavelength i in the tth curve as Yt(ui), the data can be expressed as

Yt(ui) = Xt(ui) + t(ui), i = 1, . . . , 1951, t = 1, . . . , 72,

where Xt(u) is the true underlying smooth process, and t(u) is a random noise function. The smoothed functions displayed in Figure 1b are obtained by minimizing the penalized residual sum of squares (PENSSE) given by

72 1951

72

PENSSE =

[Xt(ui)-Yt(ui)]2+BS

t=1 i=1

t=1

1 0

d2Xt(u) du2

du,

t = 1, . . . , 72, u  [0, 1],

where BS is a B-spline smoothing parameter selected by the fda.usc package in R (R Core Team 2020) through generalized cross-validations. The functional KPSS test of Horv´ath et al. (2014) confirms that {Xt(u)}7t=2 1 is stationary at the 5% significance level with a p-value of 0.053.

6.1 Feature extraction performance
We compare feature extraction performances of the FPCA-BTW method with competing sparse FPCA methods. The sample long-run covariance function for {Xt(u)}7t=2 1 is computed
31

following the procedures described in Section 3.1, and is presented in Figure 5. It can be seen that sharp spikes mainly occur between 1300 and 1900 nm, indicating that autocovariance functions at non-zero of lags {Xt(u)}7t=2 1 also possess information exclusive to local features.
Sample long-run covariance

0.03

0.02

0.01

0.00 500 1000 1500
2000

2000 1500 1000
500

Figure 5: Sample long-run covariance function of smoothed wood panel NIR spectroscopy spectra.

Decomposing the sample long-run covariance operator reports the dimension of global features as K = 2 by (10), and this result is confirmed by the empirical eigenvalues presented in Figure 1c. Using only global features we compute a long-run covariance estimator CFPCA(u, s). Next, we apply the BTW method, and also the SFPCA and TWFPCA methods mentioned in Section 5.1, to recover local features from dynamic FPCA residuals. With the extracted local features, we compute another estimator Clocal(u, s). The true process of NIR absorbance is not observable in practice. To assess effectiveness of various local feature extraction methods, a sample relative error is defined as

Sample relative error =

500 500 i=1 j=1

2

Csample(ui, sj) - CFPCA(ui, sj) - Clocal(ui, sj)

2

,

Csample(ui, sj) - CFPCA(ui, sj)

32

where i and j denote equally spaced grid points over [0, 1]. To accelerate the computation, we pick n = 500 equally spaced grids on each Xt(u) and get sample relative errors of 0.255, 0.386 and 7.866e - 5 for the sparse FPCA method (Allen & Weylandt 2019), the two-way FPCA method (Huang et al. 2009) and the BTW method, respectively. A sample RE close to 0 indicates that nearly all relevant information of the sample long-run covariance has been utilized in modeling the functional time series {Xt(u)}7t=2 1. The obtained sample relative errors indicate that BTW is an optimal method for recovering sharp and highly localized features for functional data.

6.2 Forecasting performance

The forecasting performance of various global and local feature extraction methods are compared. First, the smoothed functions {Xt(u)}7t=2 1 are divided into a training set {X1(u), . . . , X62(u)} and a testing set {X63(u), . . . , X72(u)}. We apply the FPCA-BTW method to the training set, and use the obtained global and local features to make outof-sample forecasts. Adopting the expanding window approach of Zivot & Wang (2006), in total we produce ten one-step-ahead forecasts, nine two-step-ahead forecasts, and so on, up to one 10-step-ahead forecast. Point forecasts obtained without considering local featires under the same expanding window setting serve as comparison benchmarks in this application.
To accelerate computation, we pick n = 500 equally spaced grids on each Xt(u), and compute the mean absolute forecast error (MAFE) and the root mean squared forecast error (RMSFE) as

MAFE(h)

=

500

×

1 (11

-

h)

10

500
|X62+ (ui) - X62+|62+-h(ui)|,

=h i=1

RMSFE(h) =

1

10 500

500 × (11 - h) =h i=1

2
X62+ (ui) - X62+|62+-h(ui) ,

33

where X62+(ui) represents the actual holdout sample at the ith wavelength of the th curve, and X62+(ui) is the corresponding point forecasts. Averaging over ten forecast horizons, we obtain summary statistics given by

Median

(MAFE)

=

1 2

[MAFE(h

=

5) +

MAFE(h

=

6)],

Mean

(RMSFE)

=

1 10

10

RMSFE(h).

h=1

The median statistic is suitable for handling the absolute error MAFE while the mean statistic is good at handling the squared error RMSFE (Gneiting 2011).
Point forecast evaluation results are reported in Tables 2. The forecasts constructed using only global features are shown in the columns with the heading "None", with the remaining columns reporting forecasts produced with global and local features extracted by various methods. It can be easily seen that forecasts produced with local features are consistently more accurate. This result highlights the importance of incorporating local features in forecasting NIR spectroscopy spectra time series. Further, it can be seen that BTW consistently outperforms the competing methods in recovering local features relevant to forecasting. Thus, we recommend FPCA-BTW method in modeling and forecasting functional time series in practice. In addition, a comparison with point forecast evaluation results shown in Appendix ?? indicates that dynamic FPCA produces more accurate point forecasts than static FPCA for the NIR spectroscopy data. This finding indicates that incorporating serial dependence carried by lagged NIR spectroscopy observations improves point forecast accuracy.
Table 2 shows that the FPCA-BTW method produces the most accurate point forecasts. Therefore, we do not further consider other competing feature extraction methods. To access the forecast uncertainty of FPCA-BTW method, we adapt the approach of Aue et al. (2015) and compute pointwise prediction intervals at the 100(1 - a)% nominal coverage probability. Technical details of interval forecasts are provided in Appendix ??. Pointwise

34

Table 2: Mean MAFEs and RMSFEs of point forecasts averaged over 100 replications. The bold
entries highlight the feature extraction method with higher forecast accuracy.

h
1 2 3 4 5 6 7 8 9 10 Mean Median

None
0.482 0.502 0.528 0.537 0.543 0.580 0.598 0.645 0.704 0.593 0.571 0.561

MAFE BTW SFPCA

0.430 0.449 0.475 0.486 0.491 0.533 0.552 0.596 0.646 0.531 0.519 0.511

0.450 0.473 0.498 0.511 0.513 0.556 0.575 0.627 0.685 0.548 0.544 0.530

TWFPCA
0.450 0.475 0.502 0.516 0.518 0.566 0.592 0.650 0.717 0.577 0.556 0.542

None
0.870 0.882 0.910 0.918 0.939 0.988 1.016 1.041 1.115 1.144 0.982 0.963

RMSFE BTW SFPCA

0.837 0.841 0.872 0.870 0.891 0.938 0.959 0.972 1.044 1.082 0.931 0.915

0.841 0.852 0.878 0.884 0.902 0.951 0.976 1.003 1.072 1.109 0.942 0.927

TWFPCA
0.846 0.857 0.884 0.891 0.910 0.962 0.994 1.030 1.105 1.133 0.961 0.936

predictions intervals are evaluated using the interval score of Gneiting & Raftery (2007) given by

Sa XTlb+h(ui), XTu+b h(ui); XT +h(ui) = XTu+b h(ui) - XTlb+h(ui)

+

2 a

XTlb+h(ui) - XT +h(ui) 1

XT +h(ui) < XTlb+h(ui)

+

2 a

XT +h(ui) - XTu+b h(ui) 1

XT +h(ui) > XTu+b h(ui)

,

where XTlb+h(ui) and XTu+b h(ui) denote lower and upper bounds of a symmetric 100(1 - a)% prediction interval, and the level of significance is customarily selected as a = 0.2. To accelerate computation, we again pick n = 500 equally spaced grids on each Xt(u). Averaging over different points in a curve and different forecast horizons, the mean interval score is defined as

S¯(h)

=

500

×

1 (11

-

h)

10

500
Sa

XTlb+h(ui), XTu+b h(ui); XT +h(ui)

,

=h i=1

where Sa XTlb+h(ui), XTu+b h(ui); XT +h(ui) denotes the interval score at the th curve in

35

the testing set. The interval scores summarized in Figure 6 confirm that incorporating the local features produces more accurate interval forecasts. Moreover, extracting local

K=1

K=2

0.06

Interval Score

0.04

0.02

FPCA

FPCA-BTW

Method

FPCA

FPCA-BTW

Figure 6: Scores of pointwise interval forecasts produced by FPCA method and FPCABTW method.

features after retaining only one empirical functional component gives decent interval forecasts, which further highlights that the BTW method can effectively recover nearly all relevant information of functional data.

7 Conclusion
We propose a novel feature extraction method for functional time series. The proposed FPCA-BTW method improves the feature extraction performance of FPCA by recovering sharp, highly localized features from dimension reduction residuals. Local features extracted by BTW possess information of functional variations over particular short intervals within function domain, contributing to improved estimation results and more accurate forecasts. Theoretical properties of FPCA-BTW method are developed. Superior estimation and
36

forecasting performances of FPCA-BTW estimators in finite samples are verified by Monte Carlo experiments and an empirical application to wood panel NIR spectroscopy data.
There are several ways in which the present paper can be further extended. First, this paper employed Cai's (2002) parametric blockwise threshold approach to select wavelet coefficients. To make the proposed FPCA-BTW a nonparametric feature extraction method, the block size and threshold level at different resolution levels need to be selected based on characteristics of observations. A possible extension of the current method is adopting the data-driven block thresholding approach of Cai & Zhou (2009) to enhance extraction of local features. Moreover, this paper considered forecasting functional time series with extracted linear features. Non-linear extensions of functional regression, for example the continuously additive model of Mu¨ller et al. (2013), provide enhanced flexibility and structural stability. Another possible extension of the FPCA-BTW method may consider functional additive model (Mu¨ller & Yao 2008) as the main dimension reduction tool. Since the inspirational work on functional manifold models by Donoho & Grimes (2005), functional manifold models have witnessed increasing contributions in methodology and applications (see, e.g., Lin & Yao 2019).
Acknowledgment
The authors thank Professor Jiguo Cao from Simon Fraser University for providing us with the lumber data set.
SUPPLEMENTARY MATERIAL
Supplementary document: Document containing detailed proofs of the theoretical results and additional technical details of implementing FPCA-BTW method.
37

References
Ahn, S. C. & Horenstein, A. R. (2013), `Eigenvalue ratio test for the number of factors', Econometrica 81(3), 1203­1227.
Allen, G. I. & Weylandt, M. (2019), `Sparse and functional principal components analysis', arXiv preprint arXiv:1309.2895v5 . URL: https://arxiv.org/abs/1309.2895
Andrews, D. (1991), `Heteroskedasticity and autocorrelation consistent covariant matrix estimation', Econometrica 59(3), 817­858.
Antoniadis, A. (2007), `Wavelet methods in statistics: Some recent developments and their applications', Statistics Surveys 1, 16­55.
Antoniadis, A. & Fan, J. (2001), `Regularization of wavelet approximations', Journal of the American Statistical Association: Theory and Methods 96(455), 939­967.
Aue, A., Norinho, D. D. & Ho¨rmann, S. (2015), `On the prediction of stationary functional time series', Journal of the American Statistical Association: Theory and Methods 110(509), 378­392.
Bathia, N., Yao, Q. & Ziegelmann, F. (2010), `Identifying the finite dimensionality of curve time series', The Annals of Statistics 38(6), 3352­3386.
Berkes, I., Horva´th, L. & Rice, G. (2016), `On the asymptotic normality of kernel estimators of the long run covariance of functional time series', Journal of Multivariate Analysis 144, 150­175.
Bosq, D. (2000), Linear Processes in Function Spaces, Lecture Notes in Statistics, New York.
Bosq, D. & Blanke, D. (2007), Inference and Prediction in Large Dimensions, John Wiley & Sons, West Sussex, England.
Burns, D. A. & Ciurczak, E. W. (2007), Handbook of Near-Infrared Analysis, CRC press, Boca Raton, Florida.
Cai, T. T. (2002), `On block thresholding in wavelet regression: Adaptivity, block size, and threshold level', Statistica Sinica 12, 1241­1273.
Cai, T. T. & Zhou, H. H. (2009), `A data-driven block thresholding approach to wavelet estimation', The Annals of Statistics 37(2), 569­595.
Cao, J., Sang, P., Groves, K., Feng, M. & FPInnovations (2018), Stopping time detection in functional time series: An application to wood panel glue curing process. Joint Statistical Meetings 2018, Vancouver, Canada.
Chiou, J.-M. (2012), `Dynamical functional prediction and classification with application to traffic flow prediction', The Annals of Applied Statistics 6(4), 1588­1614.
38

Daubechies, I. (1992), Ten Lectures on Wavelets, Society for Industrial and Applied Mathematics, Philadelphia, PA.
Donoho, D. L. & Grimes, C. (2005), `Image manifolds which are isometric to euclidean space', Journal of mathematical imaging and vision 23(1), 5­24.
Donoho, D. L. & Johnstone, J. M. (1994), `Ideal spatial adaptation by wavelet shrinkage', Biometrika 81(3), 425­455.
Fan, J., Liao, Y. & Mincheva, M. (2013), `Large covariance estimation by thresholding principal orthogonal complements', Journal of the Royal Statistical Society: Series B (Statistical Methodology) 75(4), 603­680.
Ferraty, F. & Vieu, P. (2006), Nonparametric Functional Data Analysis: Theory and Practice, Springer Science & Business Media, New York.
Gellar, J. E., Colantuoni, E., Needham, D. M. & Crainiceanu, C. M. (2014), `Variabledomain functional regression for modeling ICU data', Journal of the American Statistical Association: Applications and Case Studies 109(508), 1425­1439.
Gneiting, T. (2011), `Making and evaluating point forecasts', Journal of the American Statistical Association: Review Article 106(494), 746­762.
Gneiting, T. & Raftery, A. E. (2007), `Strictly proper scoring rules, prediction and estimation', Journal of the American Statistical Association: Review Article 102(477), 359­378.
Grossmann, A. & Morlet, J. (1984), `Decomposition of hardy functions into square integrable wavelets of constant shape', SIAM Journal on Mathematical Analysis 15(4), 723­ 736.
Hall, P. & Hooker, G. (2016), `Truncated linear models for functional data', Journal of the Royal Statistical Society: Series B (Statistical Methodology) 78(3), 637­653.
Hall, P. & Vial, C. (2006), `Assessing the finite dimensionality of functional data', Journal of the Royal Statistical Society (Series B) 68(4), 689­705.
Ho¨rmann, S., Kidzin´ski, L. & Hallin, M. (2015), `Dynamic functional principal components', Journal of the Royal Statistical Society: Series B (Statistical Methodology) 77(2), 319­ 348.
Ho¨rmann, S. & Kokoszka, P. (2010), `Weakly dependent functional data', The Annals of Statistics 38(3), 1845­1884.
Horva´th, L. & Kokoszka, P. (2012), Inference for Functional Data with Applications, Vol. 200, Springer Science & Business Media, New York.
Horv´ath, L., Kokoszka, P. & Rice, G. (2014), `Testing stationarity of functional time series', Journal of Econometrics 179(1), 66­82.
39

Horv´ath, L., Rice, G. & Whipple, S. (2016), `Adaptive bandwidth selection in the long run covariance estimator of functional time series', Computational Statistics & Data Analysis 100, 676­693.
Huang, J. Z., Shen, H. & Buja, A. (2009), `The analysis of two-way functional data using two-way regularized singular value decompositions', Journal of the American Statistical Association: Theory and Methods 104(488), 1609­1620.
Hyndman, R. J. & Shang, H. L. (2009), `Forecasting functional time series (with discussions)', Journal of the Korean Statistical Society 38(3), 199­221.
Hyndman, R. J. & Shang, H. L. (2010), `Rainbow plots, bagplots, and boxplots for functional data', Journal of Computational and Graphical Statistics 19(1), 29­45.
Johnstone, I. M. & Lu, A. Y. (2009), `On consistency and sparsity for principal components analysis in high dimensions', Journal of the American Statistical Association: Theory and Methods 104(486), 682­693.
Klepsch, J. & Klu¨ppelberg, C. (2017), `An innovations algorithm for the prediction of functional linear processes', Journal of Multivariate Analysis 155, 252­271.
Klepsch, J., Klu¨ppelberg, C. & Wei, T. (2017), `Prediction of functional ARMA processes with an application to traffic data', Econometrics and Statistics 1, 128­149.
Kokoszka, P. & Reimherr, M. (2013), `Determining the order of the functional autoregressive model', Journal of Time Series Analysis 34(1), 116­129.
Lam, C. & Yao, Q. (2012), `Factor modeling for high-dimensional time series: inference for the number of factors', The Annals of Statistics 40(2), 694­726.
Lam, C., Yao, Q. & Bathia, N. (2011), `Estimation of latent factors for high-dimensional time series', Biometrika 98(4), 901­918.
Li, D., Robinson, P. M. & Shang, H. L. (2020), `Long-range dependent curve time series', Journal of the American Statistical Association: Theory and Methods 115(530), 957­971.
Lin, Z. & Yao, F. (2019), `Intrinsic riemannian functional data analysis', The Annals of Statistics 47(6), 3533­3577.
Mallat, S. G. (1989), `A theory for multiresolution signal decomposition: the wavelet representation', IEEE Transactions on Pattern Analysis and Machine Intelligence 11(7), 674­693.
Mallat, S. G. (2009), A Wavelet Tour of Signal Processing: the Sparse Way, 3rd edn, Elsevier/Academic Press, Amsterdam; Boston.
Meyer, Y. (1992), Wavelets and operators, Vol. 1, Cambridge university press, Cambridge.
Mu¨ller, H.-G., Wu, Y. & Yao, F. (2013), `Continuously additive models for nonlinear functional regression', Biometrika 100(3), 607­622.
40

Mu¨ller, H.-G. & Yao, F. (2008), `Functional additive models', Journal of the American Statistical Association: Theory and Methods 103(484), 1534­1544.
Ogden, T. (1997), Essential Wavelets for Statistical Applications and Data Analysis, Springer, Boston.
Parzen, E. (1957), `On consistent estimates of the spectrum of a stationary time series', The Annals of Mathematical Statistics 28(2), 329­348.
Politis, D. N. & Romano, J. P. (1996), `On flat-top kernel spectral density estimators for homogeneous random fields', Journal of Statistical Planning and Inference 51(1), 41­53.
R Core Team (2020), R: A Language and Environment for Statistical Computing, R Foundation for Statistical Computing, Vienna, Austria. URL: http://www.R-project.org/
Rice, G. & Shang, H. L. (2017), `A plug-in bandwidth selection procedure for long-run covariance estimation with stationary functional time series', Journal of Time Series Analysis 38(4), 591­609.
Shang, H. L. (2019), `Dynamic principal component regression: Application to age-specific mortality forecasting', ASTIN Bulletin: The Journal of the IAA 49(3), 619­645.
Shang, H. L. & Hyndman, R. J. (2011), `Nonparametric time series forecasting with dynamic updating', Mathematics and Computers in Simulation 81(7), 1310­1324.
Solo, V. (2001), `Regularization of wavelet approximations: Discussion', Journal of the American Statistical Association: Theory and Methods 96(455), 963­964.
Strang, G. (1989), `Wavelets and dilation equations: A brief introduction', SIAM review 31(4), 614­627.
Thodberg, H. H. (1996), `A review of Bayesian neural networks with an application to near infrared spectroscopy', IEEE Transactions on Neural Networks 7(1), 56­72.
Weylandt, M., Allen, G. & Liao, L. (2018), MoMA: MoMA - Modern Multivariate Analysis in R. R package version 0.1. URL: https://github.com/DataSlingers/MoMA
Zhao, Y., Ogden, R. T. & Reiss, P. T. (2012), `Wavelet-based lasso in functional linear regression', Journal of Computational and Graphical Statistics 21(3), 600­617.
Zivot, E. & Wang, J. (2006), Modeling Financial Time Series with S-PLUS, Springer, New York.
41

Supplementary to "Feature Extraction for Functional Time Series: Theory and Application to NIR Spectroscopy Data"

arXiv:2106.01150v1 [stat.ME] 2 Jun 2021

Appendix A provides detailed proofs of theoretical results stated in the main document. Appendix B present additional technical details of estimation long-run covariance function as well as applications of the FPCA-BTW feature extraction method.
Appendix A
In Appendix A.1, we provide proofs of consistency of FPCA global feature estimators. In Appendix A.2, we present proofs of consistency of BTW local feature estimators. Finally, Appendix A.3 provides proof of consistency of FPCA-BTW estimators for functional time series. The preliminary lemmas, together with additional notations facilitating development, are presented before proofs of the main results throughout Appendix A.
A.1 Consistency of global feature estimators
Lemma 1. Assume that {Xt}tZ is L4 - m -approximable. Then (i) the autocovariance operator C defined by the kernel function c (u, s) in (??) satisfies
C S < .
Z

(ii) The sample mean function µ(u) = T -1

T t=1

Xt(u)

satisfies

E

µ(u) - µ(u) 2 = O(1/T ).

Proof of Lemma 1. For L4 - m -approximable processes {Xt}tZ, the proof of (i) follows the Proposition 6 in Ho¨rmann et al. (2015), and the proof (ii) follows the Theorem 4.1 in Ho¨rmann & Kokoszka (2012).
1

Since estimation of the long-run covariance function Ch,q(u, s) does not depend on its mean function E[Xt(u)] = E[X0(u)] = µ(u), in the following derivations we can assume E[X0(u)] = 0 without losing generality.

Lemma 2. Under Assumptions ?? to ??, the estimated long-run covariance operator

1
Ch,q(x)(u) = Ch,q(u, s)x(s) ds,
0

x  L2([0, 1])

and the true long-run covariance operator

1
C(x)(u) = C(u, s)x(s) ds,
0

x  L2([0, 1])

satisfy

Ch,q - C = oP (1). S

Proof of Lemma 2. Under Assumption ??, by Theorem 2.3 in Berkes et al. (2016) we have

E

Ch,q - C

2 S

=

h T

C2(u, s) du ds +

1

2

C(u, u) du

0


Wq2(x) dx
-

+ h-2q

Wq



2

| |q (u, s)

+

o(

h T

+

h-2q )

=-

=:

h T

c1

+

h-2q c2

+

o

h T

+ h-2q

,

(A.1)

where

Wq (·)

is

a

symmetric

kernel

function

and

Wq

=

limu0

. Wq (u)-1 |u|q

First, consider the first term c1h/T in (A.1). Under Assumption ??, the true long-run covariance

function C(u, s) is an element of L2([0, 1]2) satisfying

C2(u, s) du ds <  and

1 0

C (u,

u)

du

<

;

see Appendix A.2 in Horva´th et al. (2013). By condition (??) in Assumption ??, the kernel function

Wq(u) over a bounded support [-g, g] satisfies

 -

Wq2(x)

dx

=

g -g

Wq2

(x)

dx

<

.

We

then

have

c1h/T = O

h T

.

Next, consider the second term h-2qc2 in (A.1). Andrews (1991) shows that the considered flat-top

kernel in (A.19) has q > 0 while the considered QS kernel in (A.21) has q = 2. By Assumption ??,

Wq

 =-

|

|q (u, s)

2

= Wq2

 =-

|

|q (u, s)

2

< .

We then have h-2qc2 = O(h-2q).

By

2

Assumption ??, we then have

2
E Ch,q - C = O S

h T

+

h-2q

.

The Chebyshev's inequality and Assumption ?? imply that

Ch,q - C = OP S

1

h

2
+ h-q

.

T

Lemma 3. (Theorem 2.2 in Rice & Shang (2017)). Under Assumption ??, using a flat-top weight function and a quadratic spectral kernel function in the estimation of (??) gives
hopt = O(T 1/5).

Proof of Lemma 3. When a flat-top weight function and a quadratic spectral kernel function (see Section B.1 for more details) is used in the "plug-in" method, by Theorem 2.2 in Rice & Shang (2017), the estimated optimal bandwidth satisfies

hopt = hopt 1 + OP

log5/2(T ) T

.

Applying

L'Hospital's

rule,

we

can

easily

verify

that

limT 

log5/2(T ) T

=

15 T 1/2 log1/2(T )

=

0.

Moreover,

equation (2.15) in Berkes et al. (2016) suggests that hopt = O(T 1/5),. Hence, we have hopt =

O(T 1/5).

Lemma 4. (Lemma 3.2 in H¨ormann & Kokoszka (2010)). Under Assumption ??, for two compact operators P, Q  L with singular value decompositions


P (x) = k x , vk vk,
k=1


Q(x) = k x , k k,
k=1

with P (vk) = kvk and k k2 < . Consequently k are eigenvalues of P and k the corresponding eigenfunctions. We also define

vk = skvk,

sk = sign( k , vk ). 3

If P has its eigenvalues satisfy

1 > 2 > · · · > K > K+1,

then



k - vk

2 2 k

Q-P L,

1  k  K,

where 1 = 1 - 2 and k = min(k-1 - k, k - k+1), 2  k  K.

Proof of Lemma 4. H¨ormann & Kokoszka (2010) outlined the proof to their Lemma 3.2, and later provided more detailed proof of this lemma in their book (Horva´th & Kokoszka 2012, page. 35).

Lemma 5. Under Assumptions ?? to ??, for each 1  k  K, the empirical eigenfunctions and the true eigenfunctions satisfy k(u) - k(u) = OP T -2/5 as T  .

Proof of Lemma 5. For the proof of this lemma, we replace operators P and Q in Lemma 4 by the true

long-run covariance operator C and the estimated long-run covariance operator C, respectively. vk and k in Lemma 4 can then be replaced by the true eigenfunction k and the empirical eigenfunction k, respectively. By Assumption ??, k and k are in the same direction, and hence sk = 1. Conditioning on K = K, we have





k(u) - k(u)



22 k

C -C

L



22 k

C-C , S

1  k  K,

where 1 = 1 - 2 and k = min(k-1 - k, k - k+1). Assumption ?? indicates the first K + 1

eigenvalues 1, · · · , K+1 are distinctive. So k does not equal to 0. In the estimation of long-run

covariance function, the hopt of (A.20) and the quadratic spectral kernel function are used. Hence,

by Lemma 2 and 3, we have



1



k(u) - k(u) = OP 

hopt T

2
+ h-op2t

= OP T -2/5 + T -2/5 = OP T -2/5

Lemma 6. The estimated FPC scores t,k and the true FPC scores t,k associated with the tth 4

(t  Z) function Xt(u) satisfy

|t,k - t,k| = OP T -2/5 ,

1  k  K.

Proof of Lemma 7. For each 1  k  K,

|t,k - t,k| = | Xt(u) , k(u) - Xt(u) , k(u) |  | Xt(u) , k(u) - k(u) |  k(u) - k(u) Xt(u) (Cauchy-Schwarz inequality)

By Lemma 5,

|t,k - t,k| = OP (T -2/5) ( Xt(u) )

It remains to show that Xt(u) = OP (1). By Assumption ??, the weak stationarity assumption requires that E[ Xt(u) 2] = E[ X1(u) 2]. Hence, for any x > 0, Markov's inequality implies that

Pr ( Xt(u)

> x)  E

Xt(u) x



E[

X1(u) x

2]

< .

We define some notations before introducing the next lemma. Let X be a subspace (closed linear manifold) of a separable Hilbert space H, and let Y be the orthogonal complement of X . Let A be a closed linear operator on H. Let X and Y be the injections of X and Y into H. Then X is an invariant subspace of A if and only if X  D(A) (the domain of A) and
Y AX = 0.
We denote the set of all linear operators P : X  Y by B(X , Y ), and set B(X ) = B(X , X ). Let F  B(X ) and G  B(Y ). Then F and G define an operator T  B[B(X , Y )] by
T (P ) = P F - GP, P  B(X , Y ).

5

We now define the separation of two operators F and G as



 T -1 sep(F, G) = 0,

-1 L

,

if 0 / (T ) ,
if 0  (T )

where (T ) denotes the spectrum of T .
Lemma 7. (Theorem 4.1 and Lemma 3.2 in Stewart (1971)). Let A be a closed operator defined on a separable Hilbert space H whose domain is dense in H. Let X  D(A) (the domain of A) be a subspace, and let Y be its orthogonal complement. Let YA be the projection of D(A) onto Y . Let X, Y , and YA be the injections of X , Y , and YA into H. The adjoints of X and YA, denoted by X and YA, satisfy XX = I (the identity on X ), YAYA = IA (the identity on YA), XYA = 0, and YAX = 0. Let

B11 = XAX, B21 = YAAX,

B12 = XAYA, B22 = YAAYA.

Let a perturbation E  B(H) satisfy

E11 = XEX, E21 = Y EX,

E12 = XEY, E22 = Y EY.

(A.2)

Let  = B12 L + E12 L, and  = sep(B11, B22) - E11 L - E22 L. If X is an invariant subspace of A and 1 = -2 E21 L < 1/4, there is a P  B(X , YA) satisfying

P L

E21 

L (1 + 1) < 2

E21 

L

such that R(X + YAP ) (the range of X + YAP ) is an invariant subspace of A + E. Let

X = (X + YAP )(I + P P )-1/2,

and X = R(X ). Then X  D(A) is a subspace.
Proof of Proposition ??. Denote a complete set of orthonormal basis on L2([0, 1]) by {k}kZ+. The long-run covariance operator C and its estimator Ch,q are both positive-definite Hilbert-Schmidt

6

operators, and thus admit the decomposition


C(x) = k x , k k,
k=1 
Ch,q(x) = k x , k k,
k=1

and x  L2([0, 1]).

We then have

T



k =

k , Ch,q(k) ,

k=1

k=1

and





k = k , C(k) .

k=1

k=1

Deducting (A.4) from (A.3) gives

T





(k - k) = k , (Ch,q - C)(k) +

k.

k=1

k=1

k=T +1

By definition, the long-run covariance operator C can be expressed as



C(x) =

E[ Xt , x Xt+ ]

=-





= E[ Xt , x Xt] + E[ Xt , x Xt+ ] + E[ Xt , x Xt- ]

| |1

| |1





=: C0(x) + C (x) + C- (x).

| |1

| |1

It can be seen that C is self adjoint because, by a direct verification,

(A.3) (A.4) (A.5)

C(x)  E[ Xt , x Xt- ] = C- (x),

where the superscript · denotes the adjoint operator. Similarly, we can prove that Ch,q is also self

7

adjoint. In addition, using the relations C(k) = kk and Ch,q(k) = kk, we have for k = 1, · · · , K,

k , (Ch,q - C)(k) - (k - k) = k , Ch,q(k) - k , C(k) - (k - k) = k , Ch,q(k) - C(k) , k - (k - k) (self adjoint C) = (k - k)( k , k - 1) = k - k|| k , k - 1) = k - k|| k , k - k  k - k k k - k = k - k k - k .

By Lemma 4.2 in Bosq (2000), supk1 |k - k| 

Ch,q - C

. When the optimal bandwidth hopt is
S

used, by Lemma 2 and Lemma 3, we then have

k = k + OP T -2/5 ,

k = 1, · · · , K.

(A.6)

Now consider k = K + 1, · · · , kmax. Define the following operators

kmax

Q1(x) :=

x , k k,

k=K +1

K

Q2(x) := x , k k,

k=1

kmax

Q1(x) :=

x , k k,

k=K +1

x  L2([0, 1]).

Then Q1, Q2, the long-run covariance operator C, and the difference of operators Ch,q - C correspond

to X, YA, A and E in Lemma 7. Using the fact that CQ2 =

K k=1

k k

and

j , k

= 0 j = k, we

can get

K

B12 L := Q1CQ2 L = Q1( kk) = 0.

k=1

L

8

Next, by results of Lemma 2 and Lemma 3,

E11 L := Q1(Ch,q - C)Q1 L



Q1

2 L

Ch,q - C S

 OP (T -2/5),

where the last inequality is due to

Q1

2 L

=

sup

x 1

< sup
x 1

kmax
x , k 2
k=K +1 
x , k 2
k=1

= sup x 2 .
x 1

In Lemma 7, Y is the closure of YA and Y the extension of YA to Y . We define a Q3(x) :=

K k=1

x , k

k +

 k=kmax+1

x , k

k, with x  L2([0, 1]) corresponding to Y

in the lemma.

It can

be easily seen that

K



2

Q3

2 L

=

x , k k +

x , k k

k=1

k=kmax+1

L

K



 sup

x , k 2 +

x , k 2

x 1 k=1

k=kmax+1



< sup

x , k 2

x 1 k=1

= sup x 2 .
x 1

We then have

E21 L := Q3(Ch,q - C)Q1 L  Q3 L Ch,q - C S Q1 L = OP (T -2/5),

E12 L := Q1(Ch,q - C)Q3 L  Q1 L Ch,q - C S Q3 L = OP (T -2/5),
9

and

E22 L := Q3(Ch,q - C)Q3 L



Q3

2 L

Ch,q - C

= OP (T -2/5).
S

We have previously checked that the long-run covariance operator C is self-adjoint. This corresponds to the well-known fact that if C is Hermitian and  is an approximate normalized eigenvector, then C is an approximate eigenvalue. Thus we have B11 and B22 := Q2CQ2. The separation between of B11 and B22 satisfies

sep(B11, B22) 

min

|i - j|

i(B11),j (B22)

 |K - K+1| > 0

where the last inequality due to Assumption ?? requiring that K > 0 and K+1/K = o(1). Now we readily have the condition in Lemma 7 satisfied such that

-2 E21 L 

Q1CQ2 L + Q1(Ch,q - C)Q3 L Q3(Ch,q - C)Q1 L

2

|K+1 - K | -

Q1(Ch,q - C)Q1

-
L

Q3(Ch,q - C)Q3

L



OP (T -2/5) + OP (T -2/5) OP (T -2/5) (|K+1 - K | - OP (T -2/5) - OP (T -2/5))2

<

1 4

.

By Lemma 7, we can then write

Q1 = (Q1 + Q2P )(I + P P )-1/2,

with

P

L



2 E21 L sep(Q1CQ1, Q2CQ2) - E11

L-

E22 L



2 Q3(Ch,q - C)Q1 L

|K+1 - K | -

Q1(Ch,q - C)Q1

-
L

Q3(Ch,q - C)Q3

L



|K+1

-

2 × OP (T -2/5) K | - OP (T -2/5) -

OP (T -2/5)

=

OP (T -2/5).

(A.7) (A.8)

10

We can then compute the difference between Q1 and its estimator as

Q1 - Q1 = L

(Q1 + Q2P )(I + P P )-1/2 - Q1 L

= Q1 + Q2P - Q1(I + P P )1/2 (I + P P )-1/2 L

 Q1 I - (I + P P )1/2 (I + P P )-1/2 L + Q2P (I + P P )-1/2 L

 [I - (I + P P )1/2](I + P P )-1/2 L + P (I + P P )-1/2 L

 I - (I + P P )1/2 L + P L

2 P L

= OP (T -2/5).

(A.9)

Using the linearity and symmetric properties of inner product, and the fact that C(K+j) = K+jK+j, for j = 1, · · · kmax - K, we have

K+j = K+j , Ch,q(K+j )

= K+j - K+j + K+j , (Ch,q - C + C)(K+j - K+j + K+j)

= K+j - K+j , (Ch,q - C)(K+j - K+j)

+ K+j - K+j , C(K+j - K+j) + 2 K+j - K+j , (Ch,q - C)(K+j)

+ 2 K+j - K+j , C(K+j) + | K+j , C(K+j) |

 |K+j| + K+j - K+j , (Ch,q - C)(K+j - K+j)

+ K+j - K+j , C(K+j - K+j) + 2 K+j - K+j , (Ch,q - C)(K+j)

+ 2 K+j - K+j , C(K+j) (triangle inequality)

2
 |K+j| + K+j - K+j Ch,q - C S 2 + K+j - K+j C S + 2 K+j - K+j

K+j

Ch,q - C S

+ 2 |K+j| K+j K+j - K+j (Cauchy-Schwarz inequality)

= |K+j| + OP (T -4/5),

(A.10)

where the last inequality follows from (A.9) and Lemmas 2 to 5. Denoting a b if a = OP (b) and b = OP (a), conditions in Assumption ?? indicate that k+1/k 1
for k = 1, · · · , K - 1, and K+1/K o(1), with K > 0. By (A.6), for k = 1, · · · , K - 1, we then

11

have

k+1 k

=

k+1 + OP T -2/5 k + OP (T -2/5)

1.

Similarly, when k = K, by (A.6) and (A.10), we have

(A.11)

K+1 K

=

K+1 + OP T -4/5 K + OP (T -2/5)

P 0,

(A.12)

and

K 1

=

K + OP T -2/5 1 + OP (T -2/5)

1.

Since K+1 > K+2 > · · · > kmax, by (A.10) we have, for k = K + 1, · · · , kmax,

k 1



K+1 + OP T -4/5 1 + OP (T -2/5)

= oP (1),

(A.13) (A.14)

which is less than the threshold  in (??). With (A.11)-(A.14), we complete the proof of Proposition ??.

Proof of Theorem ??. We prove this theorem firstly assuming that K is known. We then have

K

K

t,kk(u) - k,tk(u)

k=1

k=1

K



t,kk(u) - k,tk(u)

k=1

K

=

t,kk(u) + t,kk(u) - t,kk(u) - k,tk(u)

k=1

K



|t,k| k(u) - k(u) + |t,k - t,k| k(u)

k=1

K


2

K

|t,k|2 k(u) - k(u) +

{t,k - t,k}2

k=1

k=1

(triangle inequality)
2
k(u) (Cauchy-Schwarz inequality)

= OP T -2/5 ,

where we have used the fact that the estimated eigenfunctions have unit length due to normalization,
2
i.e., k(u) = 1, and results of Lemma 5 and Lemma 6 in the last step. By Proposition ??, Pr(K = K)  1. We readily have the unconditional arguments, completing
the proof of Theorem ??.

12

A.2 Consistency of local feature estimators
Lemma 8. Define the Besov space ball BP,Q(M ) as

BP,Q = f  LP :

(2j(+1/2-1/P ) Dj· P )Q < M ,

j0

1  P, Q  

where  > 0 and Dj· is the vector of wavelet coefficients at the resolution level j. Define the NRSI wavelet coefficient estimator as D = Afn, where fn = (f (u1), · · · , f (un)) , and A is defined in (??). Under Assumption ??, D  BP,Q.

Proof of Lemma 8. This lemma is contributes to Theorem 5 and Theorem 6 in Antoniadis & Fan (2001). Derivation of this lemma is given in the proof to Theorem 6 in Antoniadis & Fan (2001).

Remark. Lemma 8 indicates that we interest in performances of wavelet approximation over the Besov space BP,Q. Roughly speaking, the the Besov space BP,Q contains functions having  bounded derivatives in LP space, the second parameter Q gives a finer gradation of smoothness. Generally, we use  to indicate the degree of smoothness of the underlying signal f . See Meyer (1992) for definitions and properties of Besov space. Following this lemma, we can apply the results of Hall et al. (1999) to derive the consistency of wavelet estimators.

Lemma 9. (Proposition 1 in Cai (2002)) Suppose that ui ind. N (Di, 2), i = 1, · · · , L. Let Di =

ui1(S2 > L2), where S2 =

L i=1

x2i .

Let

DL

=

[D1, · · ·

, DL],

and

DL

=

[D1, · · ·

, DL].

In

addition,

 = 4.5052, the root of  - log  - 3 = 0, L = log N and 2 = 2/N , then

E

DL - DL

2
 (2 + 2)(
2

DL

2 2



L2)

+

22N

-2

log

N.

Proof of Lemma 9. This lemma corresponds to Proposition 1 in Cai (2002) when the optimal choice of parameters  and L are used.

Remark. Lemma 9 gives a risk measure (mean squared error) of the estimated wavelet coefficients after block thresholding. The second term on the right hand side of the risk inequality is negligible, indicating the estimator achieves, within a constant factor, the optimal balance between the variance and the squared bias over the blocks.

Definition

1.

Let

H

=

H(1, , , M1, M2, M3, r, v),

where

0



1







r,

0





<

1+21 1+2

,

and

13

M1, M2, M3, v  0, denote the class of functions f such that for any j  j0 > 0 there exists a set of integers Aj with cardinality card(Aj)  M32j for which the following are true:

· for each p  Aj, there exist constants a0 = f (2-jp), a1, · · · , ar-1 such that for all u 

[2-jp, 2-j(p + v)], |f (u) -

r-1 m=0

am(u

-

2-j p)m |



M12-j1 ;

· for each p / Aj, there exist constants a0 = f (2-jp), a1, · · · , ar-1 such that for all u 

[2-jp, 2-j(p + v)], |f (u) -

r-1 m=0

am(u

-

2-j p)m |



M22-j.

Remark. Broadly speaking, the intervals with indices in Aj are "bad" intervals which contain less smooth parts of the function. Each function f  H(1, , , M1, M2, M3, r, v) can be approximated by a regular smooth function f1 in the Besov space B ,(M2) and an irregular perturbation f2: f = f1 + f2. The perturbation f2 can be, for example, jump discontinuities or high frequency oscillations. Convergence rates are determined by the smooth component f1. The function class H(1, , , M1, M2, M3, r, v) contains the Besov class B ,(M2) as a subset for any given 1, , M1, M3, r and v (see also Section 3.1 in Cai 2002). We hold 1, , , M1, M2, r and v fixed, but allow M3 to depend on the sample size. Detailed explanation of each parameter an be found in Hall et al. (1999).
After introducing the function class H, the following lemma shows how the conditions defining H have direct implications for the wavelet expansion of a function f  H.
Lemma 10. (Lemma 1 (i) in Cai (2002)) Let f be a function belonging to the function class H(1, , , M1, M2, M3, r, v). Assume the wavelets {, } with supports supp() = supp()  [0, v]. Let N = 2J . For the wavelet coefficients DJ,p and Dj,p defined in (??), we have

|DJ,p - N -1/2f (p/N )|  M1  1 N -(1/2+1) for all p  AJ ; |DJ,p - N -1/2f (p/N )|  M2  1 N -(1/2+) for all p / AJ ;
|Dj,p|  M1  1 2-j(1/2+1) for all p  Aj; |Dj,p|  M2  1 2-j(1/2+) for all p / Aj.

Proof of Lemma 10. This is Lemma 1 (i) in Cai (2002), which follows directly Proposition 1 in Hall et al. (1999). The proof is therefore omitted.
14

Lemma 11. Let N = 2J denote the number of realizations on curve f (u). Denote L = log(2J ) and  = 4.5052 as the optimal parameters for the blockwise thresholding. Use the notation Dj,p for the estimated wavelet coefficients after the optimal blockwise thresholding, and Dj,p for the true coefficients of f  BP,Q. We then have

J -1
E

(Dj,p - Dj,p)2 = O N -2/(1+2) ,

j=j0 p

where  > 0 is fixed.

Proof of Lemma 11. This lemma is part of global adaptivity results of Cai (2002), with detailed proof provided in the proof to Theorem 4 in Section 8.2 of Cai (2002).
Proof. Proof of Theorem ?? The residual function after FPCA is given by

K
et(u) = Xt(u) - µ(u) - t,kk(u)

k=1

K

K

= Zt(u) + µ(u) - µ(u) + t,kk(u) - t,kk(u) + t(u)

k=1

k=1

The NRSI initially interpolates residual observations on grids {u1, · · · , unt} into a vector et = [et(u1), · · · , et(uN )] with N = 2J > nt equally spaced points. According to Antoniadis & Fan (2001), approximation errors in this step caused by moving nondyadic points to dyadic points are negligible. Hence, discretized FPCA residuals can be expressed as

K

K

et(ui) = Zt(ui) + µ(ui) - µ(ui) + t,kk(ui) - t,kk(ui) + t(ui),

k=1

k=1

(A.15)

where i = 1, · · · , N , ui = i/N . It follows from Definition 3.1 in Bosq (2000) that strong H-white noise process (independently and identically distributed sequence of random variables with mean 0 and constant variance taking values in H) can be expressed as

t(ui) = (W (ui) - W (ui-1)) · , 0  ui-1 < ui  1, t  Z,

where W (u), for u  0 with W (0) = 0 is a measurable bilateral Wiener process, and  is the noise level. By definition, the Wiener process has independent Gaussian increments, i.e., W (u) - W (0) 

15

Normal(0, u), and for 0  ua < ub < uc < ud  1, W (ub) - W (ua) independent of W (ud) - W (uc). Since ui = i/N for i = 1, · · · , N , we have equally sized increments ui - ui-1 = 1/N . The sequence t(u1), · · · , t(uN ) therefore follows an i.i.d. Normal(0, 2/N ) distribution.
We consider least asymmetric wavelets {, } constructed by Daubechies (1992). Using the "subband filtering schemes" discussed by Daubechies (1992, Chapter 5), the true function Zt(u) can be approximated by discretized observations as

N
Zt(u) = N -1/2Zt(ui)J,i(u).
i=1

Let Dj0,p,t and Dj,p,t denote the true wavelet coefficients of Zt(u), i.e., Dj0,p,t = Zt , j0,p and Dj,p,t = Zt , j,p . Plugging Zt(ui) from (A.15) into the last equation, we have









N



K

K



Zt(u) =

N -1/2 et(ui) - [µ(ui) - µ(ui)] -  t,kk(ui) -

t,kk(ui) - t(ui) J,i(u)

i=1

k=1

k=1

N

=

DJ,i,t + [N -1/2et(ui) - DJ,i,t] - N -1/2[ iN -1/2] J,i(u)

i=1





N

K

K

+ µ(ui) - µ(ui) + t,kk(ui) - t,kk(ui) J,i(u)

i=1

k=1

k=1

2j0

J-1 2j

=

Dj0,p,t + aj0,p,t +  j0,p,t/N j0,p(u) +

{Dj,p,t + aj,p,t +  j,p,t/N } j,p(u)

p=1



j=j0 p=1



N

K

K

+ µ(ui) - µ(ui) + t,kk(ui) - t,kk(ui) J,i(u),

(A.16)

i=1

k=1

k=1

where i's are i.i.d. Normal(0, 1) such that var(t(ui)) = var( iN -1/2) = 2/N . In (A.16), Dj0,p,t and Dj,p,t are the orthogonal transform of {DJ,i,t}Ni=1 via the DWT base matrix W , likewise aj0,p,t and aj,p,t the transform of {N -1/2et(ui) - DJ,i,t}Ni=1, and j0,p,t and j,p,t the transform of { i}Ni=1. The j0,p,t and j,p,t are i.i.d. Normal(0, 1) since i's are i.i.d. Normal(0, 1). By Lemma 10, the approximation errors satisfy

2j0

J-1 2j

N

(aj0,p,t)2 +

a2j,p,t = [N -1/2et(ui) - DJ,i,t]2 = o(N -2/(1+2)).

p=1

j=j0 p=1

i=1

(A.17)

More details about the derivation of this result can be found in Page 43 of Hall et al. (1999). Let Dj0,p,t = Dj0,p,t + aj0,p,t +  j0,p,t/N and Dj,p,t = Dj,p,t + aj,p,t +  j,p,t/N denote the NRSI
wavelet coefficients. By Lemma 8, Dt  BP,Q. According to Definition 1, the Besov space BP,Q is a

16

subset of the function class H(1, , , M1, M2, M3, r, v) (more details see Example 3.1 in Hall et al.

(1999)). Thus, we can apply Lemma 10 and Lemma 11 in the following derivations involving NRSI

estimator Dt. Denoting the wavelet coefficients after blockwise thresholding as Dt as in Section ??, according to (??) we have Dj0,p,t = Dj0,p,t and Dj,p,t = Dj,p,t1(Sj2a > L2/N ) for (j, p)  ja. The orthonormal wavelet functions satisfy  =  = 1. By the isometry of the function norm and the

sequence norm, then by triangle inequality we have



2

 2j0

J-1 2j

 2j

E Zt(u) - Zt(u)  c0  E(Dj0,p,t - Dj0,p,t)2 +

E(Dj,p,t - Dj,p,t)2 +

Dj2,p,t

p=1

j=j0 p=1

j=J p=1

N



N

K

K

2

+

E[µ(ui) - µ(ui)]2 +

E  t,kk(ui) -

t,kk(ui)  ,

i=1

i=1

k=1

k=1

(A.18)

where c0 is a constant.
2
We need to show that E Zt(u) - Zt(u)  0 as T, N  . The result of convergence can be
easily confirmed since each summand in (A.18) converges to zero:

· By Lemma 10 and (A.17),

2j0

 2j

E(Dj0,p,t - Dj0,p,t)2 +

Dj2,p,t = o(N -2/(1+2)).

p=1

j=J p=1

· By Lemma 11 and (A.17),

J-1 2j
E(Dj,p,t - Dj,p,t)2 = O(N -2/(1+2)).
j=j0 p=1

· By Proposition 1 (ii),

N
E[µ(ui) - µ(ui)]2 < E µ(u) - µ(u) 2 = O(1/T ).
i=1

· By Theorem ??,



N

K

2
K

E  t,kk(ui) - t,kk(ui) < E

i=1

k=1

k=1

2

K

K

t,kk(u) - t,kk(u) = O(T -4/5).

k=1

k=1

17

Hence, the MSE of the estimator Zt(u) satisfies E Zt(u) - Zt(u) 2 = O(N -2/(1+2) + T -4/5).
The Chebyshev's inequality then implies that, Zt(u) - Zt(u) = OP (N -/(1+2) + T -2/5) = oP (1).

A.3 Consistency of functional time series estimators

Proof. Proof of Theorem ??

This theorem can be easily proved with results of Theorems ?? and ??. By triangle inequality, we

have

2
E Xt(u) - Xt(u) =



2

K

K

t,kk(u) + Zt(u) + t(u) -  t,kk + Zt(u)

k=1

k=1

2

K

K

2

E

t,kk(u) - t,kk + Zt(u) - Zt(u)

k=1

k=1

= OP (T -4/5) + O(N -2/(1+2) + T -4/5).

The Chebyshev's inequality then implies that

Xt(u) - Xt(u) = OP (T -2/5) + OP (N -/(1+2) + T -2/5).

Since N is a positive integer, we have N -/(1+2) > 0 for  > 0. Thus,

Xt(u) - Xt(u) = OP (N -/(1+2) + T -2/5).

18

Appendix B
Appendix B.1 first provides additional details about estimating the empirical long-run covariance function of (??). We then present technical details of applying a static version of the FPCABTW method in estimating functional time series and its covariance structure in Appendix B.2. Appendix B.3 presents procedures of making point and interval forecasts using global and local features extracted by the FPCA-BTW method.

B.1 Estimation of long-run covariance

The optimal bandwidth parameter hopt in (??) is estimated via the "plug-in" algorithm of Rice & Shang (2017) as follows. Initially, a pilot estimate of the long-run covariance function is computed

utilizing the flat-top weight function WFT (Politis & Romano 1996) of the form

 1, WFT (x) = 20,- 2|x|,

0  |x|  0.5 0.5 < |x| < 1 , |x|  1

(A.19)

with an initial bandwidth h1 = T 1/5, and p = 0, as:

T

Ch(p1),FT(u, s) =

WFT

=-T

h1

| |pc (u, s).

Then, the bandwidth parameter can be estimated by

h^opt = C0(h1)T 1/5,

(A.20)

where C0(h1) = 4 Ch(21),FT 2 1/5

Ch(01),FT

2
+

1

2

Ch(01),FT(u, u) du

0



-1/5

WQS(x) dx

,

-

and  = 182/125; the quadratic spectral (QS) kernel function WQS is defined as

WQS(x)

=

25 122x2

sin(6x/5) 6x/5

-

cos(6x/5)

.

(A.21)

19

Finally, plugging in the estimated bandwidth parameter into (??) yields

T

Chopt(u, s) =

WQS

=-T

hopt

c (u, s).

B.2 Static FPCA-BTW method

Global and local features extracted by the proposed FPCA-BTW method has been proved to make

improved estimation of functional time series in the main document. When functional data possess

weak serial dependence, static FPCA would adequately extract global features of the data. We

illustrate that the static FPCA-BTW also contributes to more accurate estimation of the functional

process and its covariance structure. This appendix serves as supplementary to Appendix A that

presents main proofs of theoretical results involving the dynamic FPCA-BTW method.

We illustrate that local features contribute to improved estimation of the functional process via

an example involving functional time series {Xt(u)}Tt=1 as defined in (??). For simplicity, we assume a zero mean function and weak serial dependence in the data.Non-zero mean functions in practice

are handled by centralizing the functional observations.

Consider functional time series Xt(u) =

K k=1

t,kk(u)

+

Zt(u)

+

t(u)

as

in

(??)

with

a

fixed

integer K. The sample covariance function for the observed {Xt(u)}Tt=1 is computed as

c0(u, s)

=

1 T

T
[Xt(u) - µ(u)][Xt(s) - µ(s)],

t=1

u, s  [0, 1],

where

µ(u)

=

1 T

T t=1

Xt(u)

is the empirical mean function.

Decomposing c0(u, s)

yields global

features

K k=1

t,k

k

(u)

of

the

considered

functional

time

series.

Applying

regularized

wavelet

approximations

to FPCA residuals recover local features Zt(u). With the extracted global and local features, we

obtain FPCA estimators XtFPCA(u) and FPCA-BTW estimators XtFW(u) given by

K

XtFPCA(u) = t,kk(u),

and

k=1

K
XtFW(u) = t,kk(u) + Zt(u).
k=1

20

Computing the mean squared error (MSE) for both estimators then yields that

1T T

Xt(u) - XtFW(u) 2

t=1

=

1 T

T

t=1

2

K

K

t,kk(u) + Zt(u) + t(u) - t,kk(u) - Zt(u)

k=1

k=1

1 T T
t=1

2

K

K

t,kk(u) - t,kk(u)

+

1 T

T

k=1

k=1

t=1

2
Zt(u) - Zt(u)

1 T T
t=1

2

K

K

t,kk(u) - t,kk(u)

+1 T T

k=1

k=1

t=1

Zt(u) 2

1 T T
t=1

2

K

K

t,kk(u) + Zt(u) + t(u) - t,kk(u)

k=1

k=1

=

1 T

T

Xt(u) - XtFPCA(u)

2
.

t=1

Thus, incorporating the extracted local features Zt(u) produces a more efficient estimator XtFW(u) than the FPCA estimator XtFPCA(u).
The true covariance function for the process Xt(u) can be expressed as

c0(u, s) = E =E

K

K

t,kk(u) + Zt(u) + t(u)

t,kk(s) + Zt(s) + t(s)

k=1

k=1

K
t2,kk(u)k(s) + E [Zt(u)Zt(s)] .

k=1

(A.22) (A.23)

Derivation of (A.23) follows that {k(u)}Kk=1 are real-valued orthogonal functions with K a fixed positive integer; a set of real numbers {t,k}Kk=1 = {t,1, · · · , t,K} satisfy that var(jDj,p,tk) = 0 for any j = k; {Zt(u)}tZ is a set of functions pairwise orthogonal with {k(u)}Kk=1; {t(u)}tZ is Gaussian H-white noise with E {t(u)} = 0. Using only the extracted global features, we have an FPCA estimator for the covariance function given by

cF0 PCA(u, s)

=

1 T

T

K
t2,kk(u)k(s).

t=1 k=1

21

Another estimator constructed with the extracted global and local features is given by





cF0 W(u, s)

=

1 T

T

K



 t2,kk(u)k(s) + Zt(u)Zt(s) .

t=1 k=1

It can be easily seen that

c0 - cF0 w

=E

K
t2,kk(u)k(s)

+

E

[Zt(u)Zt(s)]

-

1 T

T

K

t2,kk(u)k(s)

-

1 T

T

Zt(u)Zt(s)

k=1

t=1 k=1

t=1

E

K
t2,kk(u)k(s)

+

E

[Zt(u)Zt(s)]

-

1 T

T

K
t2,kk(u)k(s)

k=1

t=1 k=1

= c0 - cF0 ,

since Zt(u) has sparse and highly localized spikes over u  [0, 1]. Hence, incorporating the extracted local features Zt(u) gives improved estimator for covariance of the considered process.
The static FPCA-BTW can be applied to extract features of generated data described in Monte Carlo experiments in Section ?? of the main article. Table 1 reports mean RSEs and running time of combinations of static FPCA and various local feature extraction methods. It can be seen that the BTW method outperforms both competing methods at recovering local features when applied to static FPCA residuals. Given that all RSEs reported are smaller than 1, extracted local features are tested to improve static FPCA performance.
Table 1: Mean RSE and running time of various local feature extraction methods (standard errors in parentheses). The bold entries highlighting the best performing method for each setting.

Sample size T = 25 T = 50 T = 100

RSE Time
RSE Time
RSE Time

SFPCA
0.665 (0.073) 15.286 (0.537)
0.636 (0.057) 33.890 (1.222)
0.620 (0.040) 87.710 (3.008)

TWFPCA
0.732 (0.056) 20.692 (2.946)
0.720 (0.040) 19.494 (3.601)
0.710 (0.029) 19.927 (3.075)

BTW
0.644 (0.070) 0.092 (0.012)
0.620 (0.053) 0.135 (0.016)
0.604 (0.039) 0.261 (0.096)

Using the static FPCA-BTW method in Experiment 2 in Section ?? of the main article produces smaller reconstruction errors than the static FPCA method, as shown in Fig. 1. Theoretical covariance functions of generated data have "pyramid-shaped bumps" corresponding to local features assumed in data generating processes. Figure 2(c) visualize the theoretical covariance function for sample size
22

T = 200. As shown in Fig. 2(a) and 2(c), extracted local features significantly improve covariance function estimation accuracy.

T=200

T=500

T=1000

0.20

0.15

Relative Distance

0.10

0.05

0.00 FPCA

FPCA-BTW

FPCA

FPCA-BTW

FPCA

FPCA-BTW

Figure 1: Relative errors of covariance estimators.

FPCA

0.16

0.14

0.12

0.10

0.08

0.06 0.0 0.2 0.4
0.6
0.8

1.0 0.8 0.6 0.4
0.2

1.0 0.0

FPCA-BTW

0.16

0.14

0.12

0.10

0.08

0.0 0.2 0.4 0.6
0.8

1.0 0.8 0.6 0.4
0.2

1.0 0.0

Covariance

0.16

0.14 0.12 0.10
0.08

0.0 0.2 0.4 0.6
0.8

1.0 0.8 0.6 0.4
0.2

1.00.0

(a)

(b)

(c)

Figure 2: Surface plots of the mean covariance estimators over 100 simulations obtained by FPCA (blue) and FPCA-BTW (red) for T = 200, along with the true theoretical functions (cyan).

We have demonstrated that BTW can be used to improve feature extraction performance of static FPCA. In the next section, we show that FPCA-BTW can be used to obtain more accurate forecasts Section B.3.

B.3 Forecasting functional time series
We can produce an h-step-ahead point forecast for functional time series {Xt(u)}Tt=1 utilizing the extracted global and local features. The empirical principal component scores {t,k}Kk=1 are uncorrelated satisfying that var(t,jt,k) = 0 for any j = k. The estimated wavelet coefficients at different resolution levels are also uncorrelated such that var(Dj1,p,tDj2,p,t) = 0 for any j1 = j2. Within a
23

particular resolution level j, non-zero wavelet coefficients Dj,p,t after the blockwise thresholding of (??) show very weak correlations since sparse local features rarely overlap. For these reasons, we use univariate time series models, such as autoregressive integrated moving average (ARIMA) models, to make forecasts for t,k and Dt (see, e.g., Hyndman & Ullah 2007, Hyndman & Shang 2009).
Conditioning on the estimated  = {1(u), · · · , K(u)}, and D = {D1, · · · , DT }, an out-ofsample h-step-ahead point forecast can be obtained as

XT +h(u) = E[XT +h|, D]
K
= µ(u) + T +h|T,kk(u) + A DT +h|T ,
k=1

(A.24) (A.25)

where T +h|T,k and DT +h|T represents the time series forecasts of the principal component scores and wavelet coefficients, respectively.

We now present a Monte Carlo experiment to demonstrate that extracted local features help to

improve point forecast accuracy. The FPCA extracted global features have been commonly used in

forecasting functional time series (see, e.g., Hyndman & Ullah 2007, Shang 2019). We investigate

if the empirically extracted local features contribute to more accurate point forecasts. Applying

FPCA to smoothed near-infrared spectroscopy spectra of wood panels data as illustrated in Fig. ????

in the main article yields two FPCs for global features, with serially correlated residuals following

an autoregressive integrated moving average process of order (0,2,0) (ARIMA(0,2,0)). The data

generating process for this experiment is then calibrated using the analysis results of the wood

near-infrared spectroscopy data as follows. Choose 1(u) = sin(u) and 2(u) = sin(2u) as basis functions for global features, with their associated coefficients generated independently from AR(1)

models of the form t,k = kt-1,k + t,k. Select 1 = 0.2 and t,1  N (0, 10) for {t,1}Tt=1, while

choosing 2 = 0.8 and t,2  N (0, 4) for {t,2}Tt=1. Construct the basis function for local features as



3(u)

=

sin(

u-a1 b1-a1



),

2

sin(

u-a2 b2-a2



),

a1  u < b1 ,
a2  u < b2

where a1  U (0.05, 0.4) and a2  U (0.55, 0.8), and b1 - a1 = b2 - a2 = 0.1. Generate {t,3}Tt=1
from an ARIMA(0,2,0) model with i.i.d. Normal(0, 1) innovations. Generate independent noise 
t(u) = 0.1Bt(u) with Bt(u) i.i.d. standard Brownian motion {Bt(u), u  [0, 1]}Tt=1. Finally,
orthonormalize three basis functions and calculate simulated functional time series as Xt(u) =

24

t,11(u) + t,22(u) + t,33(u) + t(u) for u  [0, 1]. For each T  {25, 45, 85}, apply the FPCA-BTW method to extract global and local features of
{Xt(u)}Tt=1. h-step-ahead point forecasts can be computed using the obtained features as

K
XT +h(u) = µ(u) + T +h|T,kk(u) + A DT +h|T ,
k=1

where A is defined in (??), and {k}Kk=1 are the empirical functional components; T +h|T,k and DT +h|T are the forecasts obtained via univariate time series methods. We consider one- to five-step-ahead point forecasts, i.e., h = 1, · · · 5, and adopt the expanding window method of Zivot & Wang (2006) to increase the training size by 1 in each iteration. Forecasts obtained in 100 replications for each sample size are then evaluated by the mean absolute forecast error (MAFE), and the root mean squared forecast error (RMSFE). For a particular forecast horizon h, the evaluation measures are given by

MAFE(h)

=

1 (6 - h) ×

100

5

100
|XT -5+ (ui) - XT -5+|T -5+-h(ui)|,

=h i=1

RMSFE(h) =

1 (6 - h) × 100

5

100

2
XT -5+ (ui) - XT -5+|T -5+-h(ui) ,

=h i=1

where XT -5+|T -5+-h represents the h-step-ahead forecast obtained based on a training set {Xt}Tt=-15+-h, and XT -5+ is the corresponding actual observation; i = {1, · · · , 100} denote equally spaced grid points over [0, 1]
Table 2 presents point forecast evaluation results for the FPCA-BTW method and the benchmark FPCA method under different settings. Because the local features extracted from the sample (longrun) covariance function inherit serial dependence of the original data, the FPCA-BTW extraction method produces more accurate point forecasts at each forecast horizon h than the FPCA method. Note that the data generating process for this experiment uses a small AR coefficient 1 = 0.2 in generating {t,1}Tt=1. The resulted functional time series possess only mild serial dependence, for which data the static version of FPCA performs well. In practice, NIR spectra recorded by spectrometers over time often have moderate-to-strong serial dependence, such as the wood panel NIR data illustrated in Figure ???? in the main article.

25

Table 2: Mean MAFEs and RMSFEs of point forecasts averaged over 100 replications. The bold entries highlight the feature extraction method with higher forecast accuracy.

Covariance
Long-run Covariance

MAFE Sample size h FPCA FPCA-BTW

T = 25 T = 45 T = 85

1 0.420 2 0.446 3 0.473 4 0.498 5 0.496 1 0.404 2 0.431 3 0.448 4 0.464 5 0.476 1 0.390 2 0.427 3 0.439 4 0.446 5 0.467

0.404 0.427 0.452 0.479 0.473 0.380 0.406 0.420 0.432 0.442 0.362 0.399 0.411 0.417 0.431

T = 25 T = 45 T = 85

1 0.415 2 0.427 3 0.470 4 0.488 5 0.488 1 0.405 2 0.424 3 0.438 4 0.458 5 0.478 1 0.383 2 0.424 3 0.439 4 0.448 5 0.470

0.410 0.391 0.461 0.479 0.476 0.390 0.407 0.420 0.437 0.454 0.369 0.415 0.429 0.435 0.453

RMSFE FPCA FPCA-BTW

0.531 0.559 0.588 0.610 0.599 0.415 0.547 0.568 0.581 0.579 0.507 0.545 0.554 0.557 0.569

0.505 0.529 0.558 0.579 0.559 0.479 0.510 0.526 0.534 0.525 0.465 0.501 0.509 0.507 0.509

0.525 0.554 0.585 0.600 0.590 0.514 0.537 0.552 0.570 0.580 0.494 0.538 0.550 0.553 0.571

0.513 0.535 0.570 0.584 0.567 0.492 0.513 0.524 0.539 0.543 0.472 0.518 0.529 0.528 0.537

26

Interval forecasts can be used to assess forecast uncertainty of models involving the extracted global and local features. Supplement to point forecasts, we adopt the method of Aue et al. (2015) and construct pointwise prediction intervals as follows.
1) Using all observed data, compute the empirical FPCs {1(u), · · · , K(u)} with their associated estimated principal component scores {1, · · · , K}, where k = [1,k, · · · , T,k]. Compute the regularized wavelet coefficients D = {D1, · · · , DT }. In-sample forecasts are then constructed as

X+h(u) = +h,11(u) + · · · + +h,K K (u) + A D+h|,

  {K, · · · , T - h},

where {+h,1, · · · , +h,K} and D+h| are h-step-ahead forecasts produced by univariate time series models based on {k}Kk=1 and {Dt}t=1, respectively.
2) With the in-sample point forecasts, we calculate the in-sample point forecasting errors

(u) = X+h(u) - X+h(u),

where   {1, 2, · · · , M } and M = T - h - K + 1.

3) Based on these in-sample forecasting errors, we sample with replacement to obtain a series of bootstrapped forecasting errors. Denote upper bounds and lower bounds for point forecasts by lb(u) and ub(u), respectively. We seek a tuning parameter  such that  × 100% of in-sample forecasting errors satisfy  × lb(u)  (u)   × ub(u).
In-sample forecasting errors { 1(u), · · · , M (u)} are expected to be approximately stationary, and by the law of large numbers, to satisfy

1 M

M

1

 × lb(u) 

(x)   × ub(u)

=1

 Pr  × lb(u)  XT +h(u) - XT +h|n(u)   × ub(u) .

Instead of computing the standard deviation of { 1(u), · · · , M (u)} as done in Aue et al. (2015), we follow Shang (2018) and use the nonparametric bootstrap approach to calculate pointwise prediction

27

intervals. Specifically, we determine a  such that  × 100% of in-sample forecasting errors satisfy  × lb(ui)  (ui)   × ub(ui), i = 1, . . . , n.
Then, the h-step-ahead pointwise prediction intervals are given as  × lb(ui)  XT +h(ui) - XT +h|T (ui)   × ub(ui),
where i symbolizes the discretized data points.
28

References
Andrews, D. (1991), `Heteroskedasticity and autocorrelation consistent covariant matrix estimation', Econometrica 59(3), 817­858.
Antoniadis, A. & Fan, J. (2001), `Regularization of wavelet approximations', Journal of the American Statistical Association: Theory and Methods 96(455), 939­967.
Aue, A., Norinho, D. D. & H¨ormann, S. (2015), `On the prediction of stationary functional time series', Journal of the American Statistical Association: Theory and Methods 110(509), 378­392.
Berkes, I., Horv´ath, L. & Rice, G. (2016), `On the asymptotic normality of kernel estimators of the long run covariance of functional time series', Journal of Multivariate Analysis 144, 150­175.
Bosq, D. (2000), Linear Processes in Function Spaces, Lecture Notes in Statistics, New York.
Cai, T. T. (2002), `On block thresholding in wavelet regression: Adaptivity, block size, and threshold level', Statistica Sinica 12, 1241­1273.
Daubechies, I. (1992), Ten Lectures on Wavelets, Society for Industrial and Applied Mathematics, Philadelphia, PA.
Hall, P., Kerkyacharian, G. & Picard, D. (1999), `On the minimax optimality of block thresholded wavelet estimators', Statistica Sinica 9(1), 33­49.
Ho¨rmann, S., Kidzin´ski, L. & Hallin, M. (2015), `Dynamic functional principal components', Journal of the Royal Statistical Society: Series B (Statistical Methodology) 77(2), 319­348.
H¨ormann, S. & Kokoszka, P. (2010), `Weakly dependent functional data', The Annals of Statistics 38(3), 1845­1884.
H¨ormann, S. & Kokoszka, P. (2012), Functional time series, in `Handbook of statistics', Vol. 30, Elsevier, pp. 157­186.
Horva´th, L. & Kokoszka, P. (2012), Inference for Functional Data with Applications, Vol. 200, Springer Science & Business Media, New York.
Horva´th, L., Kokoszka, P. & Reeder, R. (2013), `Estimation of the mean of functional time series and a two-sample problem', Journal of the Royal Statistical Society: Series B (Statistical Methodology) 75(1), 103­122.
Hyndman, R. J. & Shang, H. L. (2009), `Forecasting functional time series (with discussions)', Journal of the Korean Statistical Society 38(3), 199­221.
Hyndman, R. J. & Ullah, M. S. (2007), `Robust forecasting of mortality and fertility rates: A functional data approach', Computational Statistics & Data Analysis 51(10), 4942­4956.
Meyer, Y. (1992), Wavelets and operators, Vol. 1, Cambridge university press, Cambridge.
Politis, D. N. & Romano, J. P. (1996), `On flat-top kernel spectral density estimators for homogeneous random fields', Journal of Statistical Planning and Inference 51(1), 41­53.
Rice, G. & Shang, H. L. (2017), `A plug-in bandwidth selection procedure for long-run covariance estimation with stationary functional time series', Journal of Time Series Analysis 38(4), 591­609.
Shang, H. L. (2018), `Bootstrap methods for stationary functional time series', Statistics and Computing 28(1), 1­10.
29

Shang, H. L. (2019), `Dynamic principal component regression: Application to age-specific mortality forecasting', ASTIN Bulletin: The Journal of the IAA 49(3), 619­645.
Stewart, G. (1971), `Error bounds for approximate invariant subspaces of closed linear operators', SIAM Journal on Numerical Analysis 8(4), 796­808.
Zivot, E. & Wang, J. (2006), Modeling Financial Time Series with S-PLUS, Springer, New York.
30

