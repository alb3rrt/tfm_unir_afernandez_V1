arXiv:2106.01125v1 [stat.AP] 2 Jun 2021

Interpolation and linear prediction of data - three kernel selection criteria
Azzouz Dermoune1, Mohammed Es.Sebaiy2 and Jabrane Moustaaid3 Lille University, Kuwait University and Cadi Ayyad University
Abstract
Interpolation and prediction have been useful approaches in modeling data in many areas of applications. The aim of this paper is the prediction of the next value of a time series (time series forecasting) using the techniques in interpolation of the spatial data, for the tow approaches kernel interpolation and kriging. We are interested in finding some sufficient conditions for the kernels and provide a detailed analyse of the prediction using kernel interpolation. Finally, we provide a natural idea to select a good kernel among a given family of kernels using only the data. We illustrate our results by application to the data set on the mean annual temperature of France and Morocco recorded for a period of 115 years (1901 to 2015).
Keyword: Kernel interpolation, stochastic interpolation, linear algebra interpolation, cubic spline interpolation, climate change detection.
1 Introduction
Interpolation and prediction have been useful approaches in modelling data in many areas of applications such as the prediction of the meteorological variables, surface reconstruction and Interpolation of spatial data [1] among many more. For more details see [5], [6], [7] and [8]. In this work we extend the results of Scheuerer [1] to the linear prediction approach of time series. We also cite the work of Dermoune et all [2] where the parametrizations and the cubic spline were used as a model of prediction and we extend this results to the kernel interpolation framework.
Interpolation of spatial data is a very general mathematical problem and it's precise mathematical formulation as defined in [1] is to reconstruct a function f : T  R with T is is a domain in Rd, based on its values at a finite set of data points X = {x1, . . . , xn}  T , the values f (x1), . . . , f (xn) assumed to be known. But, in our case we are interested in the time series forecasting problem we have T = {x1, . . . , xn, xn+1} represent the time and the time series is f (x1), . . . , f (xn) with the unknown value is f (xn+1). In other words, we want to predict effectively the value f (xn+1) using the known values f (x1), . . ., f (xn). From [1]
1Laboratoire Paul Painleve´, USTL-UMR-CNRS 8524. UFR de Mathe´matiques, Ba^t. M2. 59655 Villeneuve d'Ascq Ce´dex, France. Email: azzouz.dermoune@univ-lille1.fr
2National School of Applied Sciences-Marrakech, Cadi Ayyad University, Marrakesh, Morocco. E-mail: mohammedsebaiy@gmail.com
3National School of Applied Sciences-Marrakech, Cadi Ayyad University, Marrakesh, Morocco. E-mail: jabrane.mst@gmail.com
1

we have that both approaches kernel interpolation and kriging have the same approximant for the interpolation of spatial data problem, even with the different model assumption, a general overview in both approaches can be fond in [9].

2 Linear prediction and kernel interpolation

Let R{x1,...,xn+1} be the Hilbert space of real functions on {x1, . . . , xn+1} with inner product (., .) and norm N (.). The dual of R{x1,...,xn+1} is spanned by the point evaluation linear forms x : f  f (x), x  {x1, . . . , xn+1}, that is
(R{x1,...,xn+1}) = (x1 , . . . , xn+1 ).
Moreover, the dual norm N  is defined by
(N (µ))2 = sup{|µ(f )|2 : N (f )  1},

for all µ  (R{x1,...,xn+1}). Now, for any function f  R{x1,...,xn+1} and any sequence of real numbers (w1, . . . , wn), we define the linear prediction of f (xn+1)
n
f^(xn+1) = wif (xi),
i=1
with the error
n
Errn(f ) := |f (xn+1) - wif (xi)|,
i=1

and the worst error in the unit ball w.r.t. the norm N (.)

n
Werr(f ) := sup{|f (xn+1) - wif (xi)|2 :
i=1

n
N (f )  1} = (N (xn+1 - wixi))2. (1)
i=1

In the rest oh this paper, we endow the vector space R{x1,...,xn+1} with the scalar inner product

n+1 n+1

(f, f ) = (f, f )K-1 =

f (xi)f (xj)k(-1)(xi, xj)

i=1 j=1

= f K-1f ,

with f = (f (x1), . . . , f (xn+1)) and K = [k(xi, xj) : i, j = 1, . . . , n + 1] is a fixed (n + 1) × (n + 1) symmetric positive definite matrix, with k(-1)(xi, xj) denotes the (i, j) entry of K-1. The norm defined by K is given by N (f ) = K-1/2f , with · denotes the
Euclidean norm.

2

2.1 Min-max prediction and kernel interpolation

Definition 2.1 (Min-max prediction). A linear prediction f (xn+1) of f (xn+1) is called minmax if

n

f (xn+1) = wif (xi),

(2)

i=1

where (w1, ..., wn) are given by the minimization of the Werr(f ) 1.

The following result give us the optimal weights associate to the min-max prediction w.r.t. to the norm K-1/2 · .

Proposition 2.2. The the worst error in the unit ball, Werr(f ), w.r.t. to the norm K-1/2 · is equals

n

Werr(f ) = xn+1 -

wixi )

2 K1/2

(3)

i=1

where · K1/2 denotes the dual norm defined by the dual scalar inner product

(xi, xj )K = k(xi, xj), i, j = 1, . . . , n + 1.

Proof. From the general theory of reproducing kernel Hilbert spaces,see [9, 1], we have

n

sup {|f (xn+1) - wif (xi)|2}

K-1/2f 1

i=1

= sup {[K-1/2f ] [K1/2(-w1, . . . , -wn, 1) (-w1, . . . , -wn, 1)K1/2][K-1/2f ]}
K-1/2f 1

= the largest eigenvalue of [K1/2(-w1, . . . , -wn, 1) (-w1, . . . , -wn, 1)K1/2]

= K1/2(-w1, . . . , -wn, 1) 2 = (-w1, . . . , -wn, 1)K(-w1, . . . , -wn, 1)

n

= xn+1 -

wixi )

. 2
K1/2

i=1

Corollary 2.3. The optimal weights of the min-max linear prediction of f (xn+1) are given by

w = (w1, . . . , wn) = [k(xn+1, x1), . . . , k(xn+1, xn)][k(xi, xj) : i, j = 1, . . . , n]-1. (4)

Proof. The optimal weights are given by the minimization

n

arg min{ xn+1 -

wixi

: 2
K1/2

w1, . . . , wn  R},

(5)

i=1

3

which is the solution of the system

n

wjk(xi, xj) = k(xn+1, xi), i = 1, . . . , n,

(6)

j=1

it follows easily that w is given by 4.

Remarks 2.4. 1) The worst case linear prediction error in the ball with the radius r > 0 w.r.t. to the norm K-1/2 · is equal to

n

sup {|f (xn+1) - wif (xi)|2}

K-1/2f r

i=1

= r2(-w1, . . . , -wn, 1)K(-w1, . . . , -wn, 1) ,

as a result the optimal weights (4) do not depend on the radius of the ball.

2) The prediction using the spline interpolating w.r.t. the norm K-1/2 · (see, e.g., [9]) defined by the minimizer :

S(f ) = arg min{ K-1/2f : f (x1), . . . , f (xn) are fixed},

coincide with the prediction (2).

3) The min-max prediction (2) is equal to

n

f (xn+1) = jk(xn+1, xj),

(7)

j=1

where j, j = 1, ..., n is the solution of the system

n

jk(xi, xj) = f (xi), i = 1, . . . , n,

(8)

j=1

Now, we turn to the interpolation of the function f at the set {x1, . . . , xn} using span(k1, . . . , kn) where kj denotes the j-th column of the matrices K. Then the interpolation of the function f equals

n
I(f ) = jkj
j=1
with the weights  are given by (8). The following Proposition gives the error of interpolation.

Proposition 2.5 (Interpolation error ). The error of interpolation, IErr, is given by

n
IE rr(f ) := f (xn+1) - f (xn+1) = [k(n-+11)f ][k(xn+1, xn+1) - wik(xi, xn+1)]. (9)
i=1

4

Proof. First, observe that we can write the coordinates of f in the basis K as

n+1
f = [k(j-1)f ]kj.
j=1

with k(j-1) denotes the j-th row of K-1. Therefore

n+1

I(f ) =

[k(j-1)f ]I(kj)

j=1

n

=

[k(j-1)f ]kj + [k(n-+11)f ]I(kn+1),

j=1

because the interpolation of kj is exact for j = 1, . . . , n. Thus,
f - I(f ) = [K-n+11f ](kn+1 - I(kn+1))
n
= [k(n-+11)f ](0, . . . , 0, [k(xn+1, xn+1) - wik(xi, xn+1)]) ,
i=1
which completes the proof.

2.2 Min-max linear prediction with constraint

In this section, we consider the optimization (5) under the constraint

n

wipk(xi) = pk(xn+1), k = 1, . . . , q,

(10)

i=1

where p1, . . . , pq  R{x1,...,xn+1} are given. Solve the minimization (5) under the constraint (10) is equivalent to solve the system

n j=1

k(xi,

xj )wj

+

q k=1

kpk(xi)

=

k(xi,

xn+1),

i = 1, . . . , n,

n j=1

wj pk (xj )

=

pk(xn+1),

k = 1, . . . , q,

(11)

where 1, . . ., q are the Lagrange multiplier. The solution is unique if the homogeneous system

n

q

k(xi, xj)wj + kpk(xi) = 0,

j=1

k=1

n

wjpk(xj) = 0, k = 1, . . . , q,

j=1

i = 1, . . . , n,

5

has a unique solution w1 = . . . = wn = 0, 1 = . . . = q = 0. This is equivalent to say that the columns (pk(x1), . . . , pk(xn)) , k = 1, . . . , q, are linearly independent and that K is conditionally positive w.r.t. p1, . . ., pq, i.e. the system

nn
k(xi, xj)wjwi = 0,
j=1 j=1

n
wipk(xi) = 0,
i=1

k = 1, . . . , q,

has a unique solution w1 = . . . = wn = 0. Observe that this is true if K is definite positive,
but it is not necessary. Let w1, . . ., wn, 1, . . ., q be the solution of the system (11). Then the optimal prediction under the constraint (10) is

n

f (xn+1) = wif (xi).

(12)

i=1

Constraint's parametrization
Now, let z(1) = (z1(1), . . . , zn(1)) be a particular solution of the system (10) and z1 = (z11, . . . , zn1) , . . ., zn-q = (z1n-q, . . . , znn-q) , n - q independent solutions of the corresponding homogeneous system. Then the general solution of the system (10) has the form

n-q
w = z(1) + w~lzl.
l=1

Let us consider the basis

n

n

B = [p1, . . . , pq, zi1ki, . . . , zin-qki, bn+1]

(13)

i=1

i=1

=: [b1, . . . , bn+1]

such that the expansion of f in the basis B is given by

q

n-q

f = lf pl + q+lf bq+l + n+1f bn+1,

l=1

l=1

with n+1 = (-(z(1)) , 1). The unknows are the rows 1, . . ., n, and the column bn+1. They are solution of the system

q

n-q

(i = j) = pl(xi)lj + bq+l(xi)(q+l)j + bn+1(xi)(n+1)j

l=1

l=1

i, j = 1, . . . , n + 1.

(14)

6

The interpolation of any function g at the set {x1, . . . , xn} using span(b1, . . . , bn) is given by the map
n
I(g) = jbj,
j=1

with  is the solution of the system
n
jbj(xi) = g(xi),
j=1

i = 1, . . . , n.

Using similar arguments as in proposition 2.5, we can deduce the following result.

Proposition 2.6. The value

n

n

zj(1)k(xj, xn+1) + I(f - zj(1)kj)(xn+1)

j=1

j=1

coincides with f (xn+1) given by (12). In addition, the error is equal to

n

n

f (xn+1) - f (xn+1) = f (xn+1) - zj(1)k(xj, xn+1) - I(f - zj(1)kj)(xn+1)

j=1

j=1

n
= [b(n-+11)(f - zj(1)kj)][bn+1(xn+1) - I(bn+1)(xn+1)].

j=1

If K is invertible and pl = kl with l = 1, . . . , n, then l = k(l-1) for l = 1, . . . , n, and the

basis

(14)

is

given

by

bl

=

kl

with

l

=

1, . . . , n,

and

bn+1

=

. kn+1
k(xn+1 ,xn+1 )-I (kn+1 )(xn+1 )

Constraint's effect on the kernel

From the notations above the general solution of the system (10) has the form

n-q
w = z(1) + w~lzl.
l=1
As a consequence the quadratic form

n

xn+1 -

wixi

2 K

=

i=1

n-q

µn+1-q -

w~lµl 2K~ ,

l=1

with µ1 =

n i=1

zi1xi

,

. . .,

µn-q

=

n i=1

zin-q

xi

,

µn+1-q

=

xn+1

-

entries of the (n + 1 - q) × (n + 1 - q) kernel K~ are given by

k~(l1, l2) = (µl1, µl2)K, l1, l2 = 1, . . . , n + 1 - q.

n i=1

zi(1)xi ,

and

the

7

Observe that K~ is positive definite if and only if the columns (pk(x1), . . . , pk(xn)) , k = 1, . . . , q, are linearly independent and K is conditionally positive w.r.t. p1, . . ., pq. It follows that

n
sup{|f (xn+1) - wif (xi)|2 :

i=1

n-q

= µn+1-q -

w~lµl 2K~ ,

l=1

f~ K~ -1f~  1}

where f~ = (f~(1), . . . , f~(n + 1 - q))  Rn+1-q are defined by

n

n

f~(1) = zi1f (xi), . . . , f~(n - q) = zin-qf (xi),

i=1

i=1

n
f~(n + 1 - q) = f (xn+1) - zi(1)f (xi).

i=1

The map f  R{x1,...,xn+1}  f~ K~ -1f~ is a semi kernel having the null space spanned by p1,
. . ., pq. That being the case, the optimal weights w~ are given by

n-q

w~ = arg min{ µn+1-q -

w~lµl

2 K~

:

l=1

w~1, . . . , w~n-q  R},

and then predict f (xn+1) is equal to

n

n-q

n

zi(1)f (xi) + w~l( zilf (xi)).

i=1

l=1

i=1

The latter predictor coincides with (12). Moreover, the spline

S(f~) = arg min {f~ K~ -1f~ : f~(1), . . . , f~(n - q) are fixed} f~(n+1-q)

is such that
n
S(f~)(n + 1 - q) = f (xn+1) - zi(1)f (xi)
i=1
with f (xn+1) is the optimal prediction under the constraint (12). From the expansion of f in the basis B = [b1, . . . , bn+1] (13), we can conclude the following result.

8

Proposition 2.7. If the weights w satisfy the constraint (10), then

n

n+1-q

n

|f (xn+1) - wif (xi)|2 = |

f~l{bq+l(xn+1) - wibq+l(xi)}|2.

i=1

l=1

i=1

It follows that

n
sup{|f (xn+1) - wif (xi)|2 : f~ K~ -1f~  1}

i=1

n

n

= (bq+1(xn+1) - wibq+1(xi), . . . , bn+1(xn+1) - wibn+1-q(xi))K~ -1

i=1 n

i=1 n

(b1+q(xn+1) - wib1+q(xi), . . . , bn+1(xn+1) - wibn+1(xi))

i=1

i=1

= (-w1, . . . , -wn, 1)RK~ R (-w1, . . . , -wn, 1)

= (-w~1, . . . , -w~n-q, 1)K~ (-w~1, . . . , -w~n-q, 1)

n-q

= µn+1-q -

w~lµl 2K~ ,

l=1

with the (n + 1) × (n + 1 - q) matrix

R = [bq+1, . . . , bn+1].

2.3 Semi-kernel and constraint

Now, conversely we consider a semi-kernel Q on R{x1,...,xn+1} with the null space spanned by q functions p1, . . ., pq and let

S(f ) = arg min{Q(f, f ) : f (x1), . . . , f (xn) are fixed}

be the spline defined by the semi-norm Q, and
n
S(f )(xn+1) = wif (xi).
i=1
We consider a basis B = [b1, . . . , bn+1] such that bl = pl with l = 1, . . . , q and let (1, . . . , q, u1, . . . , un+1-q) be the coordinates of f , i.e.

q

n+1-q

f = lpl +

ulbq+l.

l=1

l=1

It follows that

n+1-q n+1-q

Q(f, f ) =

ul1 ul2 Q(bq+l1 , bq+l2 ) =

l1=1 l2=1

Q1/2u 2,

9

and the kernel Q =: [Ql1,l2 : l1, l2 = 1, . . . , n + 1 - q] is invertible. If the weights w satisfy the constraint (10), then

n

n+1-q

n

|f (xn+1) - wif (xi)|2 = |

ul{bl(xn+1) - wibl(xi)}|2,

i=1

l=1

i=1

therefore,

n

sup{|f (xn+1) - wif (xi)|2 : Q(f, f )  1}

i=1 n

= sup{|f (xn+1) - wif (xi)|2 : u Qu  1}

i=1

n

n

= (bq+1(xn+1) - wibq+1(xi), . . . , bn+1(xn+1) - wibn+1-q(xi))Q-1

i=1 n

i=1 n

(b1+q(xn+1) - wib1+q(xi), . . . , bn+1(xn+1) - wibn+1(xi))

i=1

i=1

= (-w1, . . . , -wn, 1)RQ-1R (-w1, . . . , -wn, 1) ,

where the (n + 1) × (n + 1 - q) matrix

R = [bq+1, . . . , bn+1].

3 Stochastic approach

The statistical counterpart to the kernel interpolation is known as kriging (see e.g. [1]). It is
based on the modeling assumption that (f (x1), . . . , f (xn), f (xn+1)) is a realization of random vector Yx1, . . ., Yxn+1 over the same probability space (, F , P). To predict Yxn+1 known Yx1, . . ., Yxn we need the mean and the covariance matrix of the random vector (Yx1, . . . , Yxn+1). We assume that the mean (m(x1), . . . , m(xn+1)) (also called the trend) and the covariance function

k(xi, xj) = cov(Yxi, Yxj )

of the random vector (Yx1, . . . , Yxn+1) exist. If Yx1, . . ., Yxn are assumed to be known, then the best linear unbiased predictor (BLUP) of Yxn+1 is given by

n

Y^xn+1 =

wiYxi ,

i=1

where the weights wi are the solution of the following optimization problem

n

n

min{var(Yxn+1 - wiYxi) : w1, . . . , wn  R, wim(xi) = m(xn+1)}. (15)

i=1

i=1

10

If the mean function m is modeled as
q
m(xi) = kpk(xi) :
k=1

i = 1, . . . , n + 1,

and if we consider the weights such that

n
wipl(xi) = pl(xn+1), l = 1, . . . , q,
i=1

then the optimal predictor

n
f^(xn+1) = wif (xi)
i=1

of f (xn+1) in stochastic sense coincides with the interpolation (12).

4 Three kernel selection criteria

Kernel interpolation and prediction approaches are based on the knowledge of a symmetric

positive definite matrix K and the trend p1, . . ., pq. To apply kernel interpolation it amounts to the assumption that one knows the degree of smoothness of the function f . In the context

of partial differential equations, the function f belongs to some Sobolev space. In stochas-

tic approach the covariance matrix and the trend are chosen using the maximum likelihood

method or the Bayesian method.

Here we propose three natural criteria to compare two kernels K(1) and K(2). Known f (x1),

. . ., f (xr), we predict f (xr+1) using the kernel [k(l)(i, j) : i, j = 1, . . . , r], and we obtain the predictor f^(l)(xr+1), with l = 1, 2, and r = 2, . . ., n - 1. We propose the following three

criteria to measure the performance of the Kernel K(l):

1) M SP E(l) =:

. n-1
j=1

|f

(xj+1

)-f^(l) (xj +1 )|2

n-1

We say

that K(1)

is

better than

K(2)

w.r.t.

the

MSPE criterion if

M SP E(1) < M SP E(2).
2) M AXP E(l) =: max{|f (xj+1) - f^(l)(xj+1)| : j = 1, . . . , n - 1}. We say that K(1) is better than K(2) w.r.t. the MAXPE criterion if

M AXP E(1) < M AXP E(2).

3) We say that K(1) is statistically better than K(2) if
n-1
1 > 1/2. j=1 [|f (xj+1)-f^(1)(xj+1)|<|f (xj+1)-f^2)(xj+1)|] n-1
These criteria were also used in [?].

11

5 Application

In the climate change problem we are interested in the mean temperature f (t) at the time t. The data are the years taken into account t1 < . . . < tn+1 and the mean temperature f (t1), . . ., f (tn), and we are interested in the prediction of f (tn+1). We recall that

tn+1

arg min{

|g (t)|2dt : g(t1) = f (t1), . . . , g(tn+1) = f (tn+1) are fixed}

t1

is the natural C2 cubic spline s which interpolates the points (ti, f (ti)), i = 1, . . . , n + 1. See [10, 11]. We assume that f (t1), . . ., f (tn+1) are the values of a natural C2 cubic spline. We
are going to predict f (tn+1) using three kernels, and we need some notations.

5.1 Kernel and semikernels using cubic splines
Let S = S3(t1, . . . , tn+1) be the set of C2 cubic splines having the knots t1 < · · · < tn+1. Note that every element s  S is a C2 map on [t1, tn+1] and is a polynomial of degree three on each interval [ti, ti+1) for i = 1,. . . , n.
More precisely, let

p1 = s(t1), . . . , pn+1 = s(tn+1), q1 = s (t1), . . . , qn+1 = s (tn+1), u1 = s (t1), . . . , un+1 = s (tn+1), v1 = s (t1+), . . . , vn = s (tn+)

be respectively the values of s and its derivatives up to order three at the knots. We have for every i = 1, . . . , n,

s(t) = pi + qi(t - ti) + (t - ti)2ui/2 + (t - ti)3vi/6, t  [ti, ti+1).

The following constraint for hi = ti+1 - ti, i = 1, . . . , n ensures the hypothesis that s is C2:

pi + qihi + uih2i /2 + vih3i /6 = pi+1,

(16)

qi + uihi + vih2i /2 = qi+1,

(17)

vi = s(3)(ti) = (ui+1 - ui)/hi.

(18)

It is well known (see [?]) that S has the dimension n + 3, and the set of natural spline Snat has the dimension n + 1. Hence an element s  S (respectively s  Snat) is completely defined by n + 3 (respectively n + 1 parameters) independent parameters.
Now we need to parametrize the set S in order to define properly an element s  S. A parametrization of S is a one-to-one linear map
 : s  S    Rn+3.

12

Defining a parametrization  is equivalent to the existence of the basis B = (b1, . . . , bn+3) of S such that, for all s  S,
n+3
s = ibi = B.
i=1
The parametrization 002 = (p1, p2, u1, . . . , un+1) defines the basis B002 = (b0102, . . . , b0n0+23). The subscript notation 002 is justified by the fact that

p1 = s(t1) = s(0)(t1), p2 = s(t2) = s(0)(t2), u1 = s (t1) = s(2)(t1), . . . , un+1 = s (tn+1) = s(2)(tn+1).

See [2, 3, 4] for more details. It follows for s  Snat that

n

s = p1b0102 + p2b0202 +

uib020+2i,

i=2

and then s = (s(t1), . . . , s(tn+1) is given by
n
s = [b0102(t), b0202(t)](p1, p2) + R(u2, . . . , un) ,
i=2

Here the column b0i 02(t) = (b0i 02(t1), . . . , b0i 02(tn+1)) , with i = 4, . . . , n + 2, and the n + 1 × n + 1 matrix

R = [b0402(t), . . . , b0n0+22(t)].

Observe that span(b0102, b0202) = span(1, t) with the column 1 = (1, . . . , 1) , t = (t1, . . . , tn+1) . We can show that

tn+1
|s (t)|2dt =

t1

n

ti+1

|ui + t(ui+1 - ui)/hi|2dt

i=1 ti

n

=

(u2i + uiui+1 + u2i+1)hi/3

i=1

= (u2, . . . , un)Q(u2, . . . , un) ,

(19)

with Q is a known n - 1 × n - 1 invertible matrix see [2]. We also recall that

(u2, . . . , un) = U(p1, . . . , pn+1) ,

13

with U is a known n - 1 × n + 1 matrix see [2]. Therefore

(u2, . . . , un)Q(u2, . . . , un) = (p1, . . . , pn+1)U QU(p1, . . . , pn+1)

(20)

=: (p1, . . . , pn+1)P(p1, . . . , pn+1)

(21)

(22)

Now we propose the following predictors for f (tn+1). 0) We assume that s is Gaussian centred with the covariance matrix K(0) = (Q(0))-1 with Q(0) is defined by
tn+1
|s(t)|2dt = s Q(0)s.
t1
1) We consider the spline

S(f ) = arg min{(f (t1), . . . , f (tn+1))P(f (t1), . . . , f (tn+1)) : f (t1), . . . , f (tn) are fixed},(23)
defined by the kernel P (21) and the predictor f (tn+1) = S(f )(tn+1) of f (tn+1). We assume that s is Gaussian with the mean p1b0102(t) + p2b0202(t) = 11 + 1t and the covariance matrix K(1) = RQ-1R with the kernel Q is given by (20). The predictor f^(1)(tn+1) of f (tn+1) (12) using the kernel K(1) coincides with S(f )(tn+1).
2) We assume that s is Gaussian with the mean p1b0102(t) + p2b0202(t) = 11 + 1t and the covariance matrix K(2) = RQR .
Let f^(i)(tn+1) be the predictor of f (tn+1) (12) using the kernel K(i) with i = 0, 1, 2. Using real data, we are going to compare these three predictors.

5.2 Real data Application
As application in the climate change area we are interested in the annual mean temperature observed in France and Morocco from 1901 to 2015, the data are presented in Figure 1. We illustrate the importance of the kernel choice by considering the kernels K(0), K(1), K(2). The three kernel selection criteria are presented in Table 1. The mean annual temperature of the year 2015 and 2016 (i.e. f^(i)(tn) and f^(i)(tn+1), n = 114) are given in Tables (2, 3), as for Figure 2 it shows the splines of the predictors f^(0), f^(1), f^(2) and the true temperature. The w1, . . ., wn of (12) for the kernels K(0), K(1), K(2) are presented in Figure 3.

14

Mean annual temperature from 1901 to 2015 in France

13.5

12.5

Mean annual temperature

11.5

10.5

1900

1920

1940

1960 Years

1980

2000

Mean annual temperature from 1901 to 2015 in Morocco

Mean annual temperature 15 16 17 18 19

1900

1920

1940

1960 Years

1980

2000

Figure 1: Mean annual temperatures in France and Morocco from 1901 to 2015.

Country Kernel M SP E M AXP E Statistically

France

K(0)

K(1)

K(2)

0.3301302 2.090779 5.21788

1.3961 3.344106 5.312125

K(0) with 0.8198198 for K(1)

and 0.8288288 for K(2)

Morocco

K(0)

K(1)

K(2)

0.7727975 5.110724 11.70042

2.251341 6.171007 9.438383

K(0) with 0.8288288 for K(1)

and 0.8468468 for K(2)

Table 1: The three kernel selection criteria for the kernels K(0),K(1), K(2) using Morroco and France data.

15

18

^f (0) ^f (1) ^f (2) True temperature

France

16

14

12

10

8

6

0

20

40

60

80

100

t

^f (0) ^f (1) ^f (2) True temperature

Morocco

25

20

15

10

0

20

40

60

80

100

t

Figure 2: The splines of the predictors f^(0), f^(1), f^(2) and the true temperature.

Country Kernel Prediction True temperature

K(0) 13.17656

France K(1) 15.48813
13.5

K(2) 15.61992

K(0) 18.06526

Morocco K(1) 21.18307 18.9008

K(2) 20.41619

Table 2: The predictors f^(i)(tn), n = 114 (the mean annual temperature of the year 2015).

Country

France

Morocco

Kernel

K(0)

K(1)

K(2)

K(0)

K(1)

K(2)

Prediction 12.91553 12.54049 11.40698 17.86737 18.99740 18.49113

Table 3: The predictors f^(i)(tn+1), n = 114 (the mean annual temperature of the year 2016).

16

Remark 5.1. Table 1 shows that the kernel K(0) wins against K(1) and K(2) with respect to the three kernel selection criteria.
K(0)

0.3

w*

0.1

-0.1

0

20

40

60

80

100

i

K(1)

w* -1 0 1 2

0

20

40

60

80

100

i

K(2)

w* -3 -2 -1 0 1 2 3

0

20

40

60

80

100

i

Figure 3: The w1, . . ., wn of (12) for the kernels K(0), K(1), K(2).
5.3 Concluding remarks
The numerical results shows the three kernel selection criteria are stable, form Table 1 we have that the best kernel among the three kernels is K(0) w.r.t. all the three criteria for both France and Morocco data. Moreover, the representation of the splines (Figure 2) shows that too.
From Table 1 and Figure 2 we have that the kernel K(1) wins against K(2). Considering the second derivative (u2, . . . , un) as Gaussian with the covariance matrix Q-1 is a good stochastic modelization, at least is better than the assumption that (u2, . . . , un) as Gaussian with the covariance matrix Q. Equivalently measuring the worst error in the unit ball using the norm Q1/2u is better than the norm Q-1/2u .
17

References
[1] M. Scheuerer, R. Schaback, M. Schlather, Interpolation of spatial data-a stochastic or a deterministic problem, European Journal of Applied Mathematics 24 (4) (2013) 601­629.
[2] Azzouz Dermoune, Khalifa Es-Sebaiy, Mohammed Es.Sebaiy, Jabrane Moustaaid. Parametrizations, weights, and optimal prediction (2021). Communication in StatisticsTheory and Methods, 50(4), 815-836. https://www.tandfonline.com/doi/full/ 10.1080/03610926.2019.1642489
[3] A. Dermoune, C. Preda, Parametrizations, fixed and random effects, Journal of Multivariate Analysis 154 (2017) 162­176.
[4] A. Dermoune, C. Preda, Estimation of noisy cubic spline using a natural basis, Annals of the University 265 of Craiova, Mathematics and Computer Science Series 43 (1) (2016) 33­52.
[5] J.P. Chile`s, How to adapt kriging to non-classical problems: three case studies, In: M. Guarascio, M. David and C. Huijbregts (editors), Advanced Geostatistics in the Mining Industry, D. Reidel, Dordrecht, Holland (1976) 69­89.
[6] J.P. Chile`s, P. Delfiner, Geostatistics, Modeling Spatial Uncertainty, John Wiley, New York (2009).
[7] H. Wendland, Scattered data approximation, Cambridge Monographs on Applied and Computational Mathematics, Cambridge University Press, Cambridge, UK (2005).
[8] M. G. Mardikis, D. P. Kalivas, V. J. Kollias, Comparison of interpolation methods for the prediction of reference evapotranspiration--an application in greece, Water Resources Management 19 (3) (2018) 250 251­278.
[9] A. Berlinet, C. Thomas-Agnan, Reproducing Kernel Hilbert Spaces in Probability and Statistics, Kluwer, Berlin, Germany, (2004).
[10] I. J. Schoenberg, Contributions to the problem of approximation of equidistant data by analytic functions, quart, Appl. Math. 4 (1946) 44­99 and 112­141.
[11] C. H. Reinsch, Smoothing by spline functions, Numerische Mathematik 10 (1967) 177­ 183.
[12] C. de Boor, A practical guide to splines, Applied Mathematical Sciences. 27 (1978) xxiv+392.
18

