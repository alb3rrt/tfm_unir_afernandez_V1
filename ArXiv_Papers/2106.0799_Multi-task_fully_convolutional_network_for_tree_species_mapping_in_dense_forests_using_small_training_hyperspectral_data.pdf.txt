arXiv:2106.00799v1 [cs.CV] 1 Jun 2021

Multi-task fully convolutional network for tree species mapping in dense forests using small training hyperspectral data
Laura Elena Cue´ La Rosaa,b,, Camile Sothec, Raul Queiroz Feitosaa, Cla´udia Maria de Almeidad, Marcos Benedito Schimalskie, Dario Augusto Borges Oliveiraf
aComputer Vision Lab, Pontifical Catholic University of Rio de Janeiro, Rio de Janeiro 22451263, Brazil bIBM Research, Avenida Pasteur, 138, Rio de Janeiro, Brazil
cSchool of Geography and Earth Sciences, McMaster University General Sciences Building, Room 326 1280 Main Street West, Hamilton, Ontario, Canada
dDepartment of Remote Sensing, National Institute for Space Research (INPE), Av. dos Astronautas 1758, Sa~o Jose´ dos Campos SP 12227-010, Brazil
eDepartment of Forest Engineering, College of Agriculture and Veterinary, Santa Catarina State University (UDESC), Avenida Luiz de Camo~es 2090, Lages 88520-000, SC, Brazil fIBM Research, Rua Tuto´ia, 1157, Sa~o Paulo, Brazil
Abstract
This work proposes a multi-task fully convolutional architecture for tree species mapping in dense forests from sparse and scarce polygon-level annotations using hyperspectral UAV-borne data. Our model implements a partial loss function that enables dense tree semantic labeling outcomes from non-dense training samples, and a distance regression complementary task that enforces tree crown boundary constraints and substantially improves the model performance. Our multi-task architecture uses a shared backbone network that learns common representations for both tasks and two task-specific decoders, one for the semantic segmentation output and one for the distance map regression. We report that introducing the complementary task boosts the semantic segmentation performance compared to the single-task counterpart in up to 10% reaching an overall F1 score of 87.5% and an overall accuracy of 85.9%, achieving state-of-art performance for tree species classification in tropical forests.
Keywords: Semantic segmentation, Tree species identification, Multi-task learning, Fully convolutional network, Sparse annotations
1. Introduction
Mapping tree species using remote sensing has consolidated as a cheaper, faster, and more practical way to inventory forest areas in comparison with traditional fieldworks (Fassnacht et al., 2016). Identifying individual trees even in dense forest canopies of tropical forests is
Corresponding author Email addresses: lauracue@ele.puc-rio.br,lauracue.rosa@ibm.com (Laura Elena Cue´ La Rosa), sothec@mcmaster.ca (Camile Sothe), raul@ele.puc-rio.br (Raul Queiroz Feitosa), almeida@dsr.inpe.br (Cla´udia Maria de Almeida), marcos.schimalski@udesc.br (Marcos Benedito Schimalski), dariobo@br.ibm.com (Dario Augusto Borges Oliveira)
1

currently possible due to improved spatial and spectral resolution of remote sensing data associated with the increase in computational capacity and the advancements in classification methods. Hyperspectral data collected by airborne platforms or unmanned aerial vehicles (UAV) enable the discrimination of individual tree crowns (ITC) and the classification of tree species in tropical environments by capturing slight differences in reflectance patterns among them (Clark and Roberts, 2012; Fe´ret and Asner, 2012; Baldeck et al., 2015; Ferreira et al., 2016; Shen and Cao, 2017; Sothe et al., 2019; Zhang et al., 2020).
When using hyperspectral or multisource data, most studies involving tree species classification apply machine learning algorithms, such as support vector machine (SVM) and random forest (RF) (Fassnacht et al., 2016). Such algorithms are robust and usually work well for different class distributions or high dimensional data (Ghosh et al., 2014). However, most of them depend on intricate heuristics that limit their transferability, and often it is hard to achieve an optimal balance between discrimination and robustness for many types of data (Zhang et al., 2016, 2020).
Deep learning methods proved to be a robust alternative for remote sensing image classification, as they can learn optimal features and classification parameters to handle hyperspectral data (Signoroni et al., 2019). Different works successfully applied convolutional neural networks (CNN) (Krizhevsky et al., 2012) for tree species classification (Po¨lo¨nen et al., 2018; Hartling et al., 2019; Fricker et al., 2019; Natesan et al., 2020; Ma¨yra¨ et al., 2021), including tropical and subtropical environments (Sothe et al., 2019, 2020; Zhang et al., 2020; Tian et al., 2020). Sothe et al. (2019) employed a CNN based on image patches classification to classify hyperspectral data pixels and reached significantly higher accuracies (84.4%) when compared to SVM (62.7%) and RF (59.2%). In (Zhang et al., 2020), the authors proposed a three-dimensional CNN (3D-CNN) for tree species classification in hyperspectral data, reaching an accuracy of 93.14%. However, patch-based approaches are extremely inefficient for large-scale remote sensing imagery, as they infer each pixel classification using its corresponding patch (surrounding neighboring pixels).
More efficient approaches use Fully Convolutional Networks (FCN) (Long et al., 2015), which classify all pixels in the input patch at once. Several works explored FCN in the context of tree species mapping from UAV RGB images (Kattenborn et al., 2019; Lobo Torres et al., 2020; Ferreira et al., 2020; Brandt et al., 2020), but only few of them used hyperspectral data. Fricker et al. (2019) employed FCN to classify tree species in a mixed-conifer forest from hyperspectral and pseudo-RGB data reporting an average F-score of 0.87 and 0.64, respectively. In (Wagner et al., 2019) the authors assess a U-net FCN to identify and segment one tree species, Cecropia hololeuca, using very high-resolution images, obtaining an overall accuracy of 97% and an intersection over union (IoU) of 0.86.
The studies mentioned above depend on a large number of densely annotated ITC samples (i.e. training samples) to deliver good results, which may restrict their application in classifying a large number of tree species in diverse forests. For instance, in tropical forests, tree species are not equally distributed over the forest canopy where, for a given region, some might be dominant and others rare. That often results in an imbalanced training set, where only a few samples are available for under-represented classes (Mellor et al., 2015; Fassnacht et al., 2016). Besides, commonly, the few ITC samples are sparsely distributed across the images (Fassnacht et al., 2016), which represents an additional challenge for such methods.
In the last few years, some alternatives arose to train FCN with weak supervision, enabling low-cost annotations using points and scribbles (Alonso et al., 2017; Maggiolo et al., 2018; Wu et al., 2018; Tang et al., 2018). In (Tang et al., 2018), the authors proposed to train a FCN from
2

scribbles using a partial cross-entropy (pCE) loss. The proposed loss only back-propagates gradients for the scribble annotated pixels, emerging as an effective approach to deal with low-cost annotations. Considering remote sensing applications, Wu et al. (2018) employed the same loss function to train an FCN for segmenting aerial building footprints, achieving an IoU of 68.4% with only 5% scribble samples. Despite these approaches' success in training dense semantic labeling networks using sparse low-cost annotation, they are still unexplored for tree species classification.
Building on that literature, this work proposes a novel network architecture to train FCN with sparse and scarce polygon-level annotations (ITC samples) for tree species mapping in dense forest canopies using hyperspectral UAV-borne data. First, we propose implementing a partial loss function to train FCN to perform dense tree semantic labeling from non-dense training samples. Second, we modify the architecture to implement a distance regression complementary task that substantially improves the model performance by enforcing tree crown boundary constraints. The proposed multi-task fully convolutional architecture uses a shared backbone network that learns common representations for both tasks and two task-specific decoders, one for the semantic segmentation output and one for the distance map regression. We report that introducing the complementary task boosts the semantic segmentation performance compared to the single-task counterpart. Our Multi-Task Fully Convolutional Network for Sparse Polygon-level annotations (MTFCsp) network is trained end-to-end and delivers dense predictions.
The paper is organized as the following. Section 2 presents related works with focus on FCN, distance map generation and multi-task learning. Section 3 describes the study area. Section 4 details the proposed method. Section 5 presents the experimental design and evaluation measures. Sections 6 and 7 present the results and discussion, respectively. Finally, Section 8 presents the main conclusions of this paper.
2. Related works
Semantic segmentation (or semantic labelling) is the process of classifying each pixel in an image. Currently, state-of-the-art methods for semantic segmentation in remote sensing images are based on FCN architecture (Zhu et al., 2017; Ma et al., 2019) usually implementing land-cover usage applications. In FCN (Long et al., 2015), typical CNN fully connected layers are replaced by convolutional layers and upsampling operations, avoiding redundant operations in overlapping image tiles and performing structured predictions. These networks consist of an encoder stage that extracts high- and low-level semantic features from convolutions, non-linear activation functions, and downsampling layers; and a decoder stage that uses convolutions, non-linear activation functions, and upsampling layers to produce a target output with the spatial dimension of the input image. The network is trained end-to-end by example without the need for user-specific knowledge. The decoder module allows to explore multi-level context information and learn shape and inter- and intra-class variability in the training images. However, blurred boundaries and low spatial resolution that affect the discrimination of object details are common problems (Ma et al., 2019). Many strategies have been proposed to tackle those issues, such as skip connections (Ronneberger et al., 2015), atrous convolutions (Chen et al., 2017) and pyramid scene parsing pooling (Zhao et al., 2017). Moreover, residual connections enabled training deeper networks bypassing the vanishing gradients problem (He et al., 2016).
3

Many applications benefited from these advances in remote sensing image analysis, including tree species identification and segmentation (Lobo Torres et al., 2020; Ferreira et al., 2020; Kattenborn et al., 2019; Brandt et al., 2020). Lobo Torres et al. (2020) evaluated the performance of several variants of FCNs combined with a conditional random field (CRF) post-processing step for single tree species in an urban environment from high-resolution UAV optical imagery, reporting an overall accuracy ranging from 88.9% to 96.7%. In (Ferreira et al., 2020), the authors employed an encoder composed of several residual blocks and a decoder based on an Atrous Spatial Pyramid Pooling (ASPP) (Chen et al., 2017) for individual tree detection and tree species classification of Amazonian palms from UAV images. The authors further proposed a post-processing step based on morphological operations for boundary refinement and individual trees separation.
Recent works focused on the importance of using the boundary information in the learning process. Specifically, a distance map estimation as a supplementary task has emerged as an effective tool for improving the semantic segmentation task. In this context, Bischke et al. (2019) proposed a multi-task FCN architecture for improving the segmentation prediction of building footprints and incorporate geometric properties in the network training. They proposed to include, in addition to the semantic segmentation output, a second output for estimating the segmentation mask distance transform and preserve building boundaries. More recently, Diakogiannis et al. (2020) proposed the ResUNet-a, which combines a UNet encoder-decoder backbone with residual connections, atrous convolutions, pyramid scene parsing pooling, and multi-task for semantic segmentation of urban regions from RS data. The multi-task network also used the distance transform of the segmentation mask to refine objects and improve the semantic segmentation results.
Considering that such methods require a large number of densely annotated training samples, several recent works proposed alternatives for learning from weak-supervision such as points and scribbles (Alonso et al., 2017; Maggiolo et al., 2018; Wu et al., 2018; Tang et al., 2018) or bounding boxes (Khoreva et al., 2017). In (Tang et al., 2018; Wu et al., 2018) the authors propose to use partial cross-entropy (pCE) loss to train a FCN from scribbles. pCE can be interpreted as a sampled loss that only considers the cross-entropy for annotated pixels. That simple yet effective function allows training the network to perform dense predictions, reporting impressive results. Wang et al. (2019) designed an FCN with a shared-encoder and two task-specific decoders, one for segmentation refinement (PRN) and one for boundary regression (BRN). To train the semantic segmentation branch from scribble annotations, the authors also used the pCE loss. Later, Hua et al. (2021) also tested a partial cross-entropy loss for segmentation of objects considering point, scribble-, and polygon-level annotations.
The aforementioned weakly supervised learning methods still need to be explored for semantic segmentation of tree species in dense forests, considering that collecting samples in this application is particularly costly, time-consuming, and requires specific domain expertise. The next sections present an approach for handling efficient segmentation of tree species using small training data.
3. Materials
3.1. Study Area
The study area is located in the municipality of Curitibanos, Santa Catarina state, a southern region of Brazil (Figure 1). The area comprises an extension of approximately 30 ha and belongs
4

Figure 1: Study area location. (a) Brazil and South America, (b) Santa Catarina state and Curitibanos municipality, (c) Hyperspectral image with the location of individual tree crown (ITC) samples.
to the Atlantic Rain Forest biome and the Mixed Ombrophilous Forest phytophysiognomy. The region is dominated mainly by broadleaves species, having only two conifer species. One of them, Araucaria angustifolia, is a physiognomic marker of this forest type (Backes and Nilson, 1983).
3.2. Hyperspectral images
The hyperspectral images were acquired from a frame format camera based on a Fabry-Perot Interferometer (FPI), model 2015 (DT-0011) boarding a quadcopter UAV (UX4 model). The camera has two CMOSIS CMV400 sensors with an adjustable air gap that can be flexibly configured to select up to 25 spectral bands between 500 and 900 nm. The first preprocessing stage of the images comprised the radiometric calibration (in which digital numbers were transformed into radiance values) and the dark signal correction. The geometric processing involved each band's orientation using the interior orientation parameters (IOPs) and the exterior orientation parameters (EOPs), which were estimated using the so-called on-the-job calibration. For the camera positions, the initial values were assessed by the GNSS receiver and involved latitude, longitude, and altitude (flight height plus the average terrain elevation) data. The coordinates of six ground control points (GCPs) were added to the project and measured in the corresponding reference images in the sequence. Finally, the orthorectification of each band was performed separately. This process also co-registers the bands of the same image regarding their slight positioning difference caused by the camera's time-sequential operating principle (Honkavaara et al. (2013)). The final dataset is composed of 25 spectral bands with 11 cm of spatial resolution. More details about the flight, camera, and preprocessing steps can be found in Sothe et al. (2020).
3.3. Individual tree crown samples
Samples of 14 species totaling 80 ITCs were acquired in fieldwork conducted in December 2017 (Table 1). Only ITCs visited and identified in the fieldwork were used as samples for semantic segmentation. Due to the dense forest canopy with many species in a small area, it was not possible to collect a large number of samples for each species. This resulted in an imbalanced sample set sparsely distributed over the study area, with dominant species having 8
5

Table 1: List of tree species: species names, number of ITCs, number of pixels, mean number of pixels per ITCs, and the standard deviation (in pixels) of ITCs sizes.

ID
a b c d e f g h i j k l m n
Total

Species names
Luhea Araucaria Mimosa Lithrae Campomanesia Cedrela Cinnamodendron Cupania Matayba Nectandra Ocotea Podocarpus Schinus sp1 Schinus sp2

Nº of ITCs
5 8 7 5 5 5 5 5 8 8 9 6 2 2
80

Nº of pixels
23,624 27,191 25,449 17,458 18,837 24,368 6,927 12,475 48,231 11,247 101,884 12,387 4,491 6,083
340,652

mean Nº of pixels/ITC
4,725 3,399 3,636 3,492 3,767 4,874 1,385 2,495 6,029 1,406 11,320 2,065 2,246 3,042

std/ITC
3,019 3,210 3,349 4,287 1,663 3,445 837 1,654 6,955 1,241 4,969 2,723 371 4,799

to 9 ITC samples and the minority ones having only 2 ITCs. To aid delineate the ITC polygons, UAV-RGB data with 4 cm spatial resolution was used together with the hyperspectral data. The ITCs of each species were randomly split in 50% training and 50% test, always keeping the ITC identity. From the training ITCs we took 1% for validation.
4. Method
Our approach implements a novel multi-task FCN trained with sparse polygon-level annotations (MTFCsp) for dense semantic segmentation of tree species. Our architecture can be viewed as an encoder-focused architecture (Crawshaw, 2020), a type of multi-task learning architecture that learns a generalizable representation in the encoding phase (i.e., backbone network) and task-specific representations in the decoder phase. It takes hyperspectral images and produces two outputs, a class probability map and a distance map, with the input images' resolution. The class probability map holds semantic label posterior probabilities for each pixel in the image, whereas the distance map gives the distance of each pixel within the tree crowns to the closest boundary.
Since our training set comprises a small number of ITC samples, training solely for the semantic segmentation task would most likely cause over-fitting on the training set and deliver poor performance in the test set. We hypothesize that introducing a distance map regression as a secondary task will act as a regularizer for the semantic segmentation task and ultimately improve its generalization. For our multi-task learning approach, the loss function is defined as the linear combination of the two task-specific losses Ltotal = L1 + L2, where  represents a weight parameter. To train the network with sparse annotations, we use the definition of partial loss function for both tasks. Sections 4.1, 4.2 and 4.3 present a detailed description of the proposed loss functions and the network architecture.

6

4.1. Semantic segmentation with sparse polygon-level annotation

Semantic segmentation networks are commonly trained using the cross-entropy loss function

between the predicted label y^ and the ground truth label y. Given an input image I  RW×H,

the categorical cross-entropy function for a multi-class semantic segmentation problem can be

written as follows:

LC

=

1 -
|L|

iL

jC

yi, j log(y^i, j)

(1)

where L, with |L| = WH, is the set of labeled pixels, C is the number of classes, yi, j is a binary indicator of pixel i belonging to class j, and y^i, j corresponds to the model's estimated class probability of pixel i belonging to class j. The class probability is commonly calculated by applying the softmax activation function to the network output.
To effectively train a dense semantic segmentation network with partial annotations, i.e., |L| WH, we used the definition of partial loss (Wu et al., 2018; Tang et al., 2018). This type of loss only back-propagates the losses from pixels i that belong to the annotated set L and ignores the loss in other pixels. In (Wu et al., 2018), the authors found that adopting such a simple modification to the loss function improves results substantially for scarce labeling sets.
To tackle class imbalance, we use the categorical focal loss (Lin et al., 2017) instead of the cross-entropy loss. The focal loss addresses the class imbalance by forcing the standard crossentropy to down-weight the contribution of well-classified examples, focusing on those samples difficult to classify. Combining both concepts, the task-specific L1 loss, called partial categorical focal loss (LPCFL), can be written as the following:

LPC F L

=

1 -
|L|

i

i

jC

yi j


1 - y^i j log(y^i j)

(2)

where i = 1 if pixel i  L and 0 otherwise, and   0 is the focusing parameter. The focusing parameter weights the loss depending on how well the samples are classified to prioritize hard examples learning. When  = 0, LPCFL is equivalent to the partial cross-entropy loss (Wu et al., 2018). Notice that in the above definition, L  , with || = WH. Figure 3 illustrates the ground truth labeled image (GTs) for an input image with two annotated polygons (red, blue) where the white pixels correspond to the unknown region.

4.2. Distance map as auxiliary task
The distance map estimation auxiliary task aims to improve the segmentation network's generalization. The network will learn a regression function that maps the input image to a distance map where each pixel within the tree crowns holds the distance to the closest crown boundary. We compute such distances from the ITCs reference and train the network using a standard loss for regression. We implement a simple pipeline for creating the training set for distance maps. First, we generate the corresponding binary mask for each annotated polygon. Second, we compute the distance map d for the resulting binary image using the Euclidean Distance Transform (EDT). This operation results in a distance map when each pixel value within the polygons is the Euclidean distance to the closest boundary. Third, we smooth the distance map by applying a 2D Gaussian kernel. Finally, we normalize all distances for each polygon between 0 and 1. Since the target region is highly heterogeneous, with different crown sizes even for the same tree species, we deliberately opted for an ITC-level normalization to

7

(a)

(b)

(c)

Figure 2: Extracting the distance transform of a single tree. (a) hyperespetral image, (b) binary mask of the ITC sample, (c) distance map.

ensure that the maximum values for each ITC are 1. Figure 2 illustrates the distance map result for a single-tree. Note that the peak of the distance map is at the center of the tree crowns.
Our intuition is that forcing the network to assign low values to the tree crown edges could help identify individual trees, even those of the same semantic class. It is worth noticing that the segmentation branch does not include any restriction about the polygon's edges, and therefore this extra training signal can potentially foster more accurate results.
As in the semantic segmentation branch, we employed a partial loss function to train the distance map estimation branch. We modeled the problem as a regression problem employing the standard Mean Square Error (MSE) as the L2 loss function in our multi-task problem. MSE is the sum of squared distances between the reference map and the estimated map, and our modified version takes the following form:

LPMS E

=

1 -
|L|

i

i(di

- d^i)2

(3)

where i = 1 if pixel i  L and 0 otherwise, d is the reference map and d^ is the network's estimated distance map.

4.3. Architecture Design
Figure 3 depicts the architecture design of our approach. MTFCsp consists of a single shared encoder that learns general low-level features and two task-specific decoders that learn spectral and textural features concerning each specific task. The network is trained in a weakly-supervised end-to-end fashion using the total loss function L = LPCFL + LPMS E, based on the previously described partial loss functions. In our experiments we set  = 1.

4.3.1. Proposed shared encoder
The shared encoder was designed based on the ResNet architecture (He et al., 2016). These networks learn not an underlying mapping function H(x) (Figure 4 (left)) but a residual function H(x) - x that is expected to be more discriminant. In its final form, the residual block learns a function H(x) = F(x) + x (Figure 4 (right)), and uses a shortcut connection that handles the gradient vanishing problem without adding any extra parameter to the network.
Even though residual blocks enable deeper architectures, we experimentally setup a network, called henceforth ResNet-9 (Figure 3 encoder phase), composed of only nine convolutional operations in 3 residual blocks. In an exploratory test, we found that this shallow
8

Figure 3: ResNet and DeepLabv3+ based network architecture. C stands for the number of classes, r stands for the atrous rate, and GTs and GTd stands for the ground truth and reference distance map, respectively.
Figure 4: A regular block (left) and a residual block (right).
configuration provided a better trade-off between computational cost and accuracy than standard versions of ResNet, including ResNet-18 and ResNet-50. Each residual block has two 3×3 convolutional layers, and before each convolution within the residual blocks, we apply Batch Normalization (BN) and ELU activation function. The spatial dimension was reduced by using convolution operations with stride 2. As in ResNet architecture, we used a first convolutional layer that did not reduce the spatial dimension. The number of filters doubles periodically at each residual block, and the spatial resolution of the output feature maps is four times smaller than the input resolution. 4.3.2. Proposed decoder for semantic segmentation
For the semantic segmentation branch, we feed the ResNet-9 output feature map to a decoder based on the DeepLabv3+ architecture (Chen et al., 2018), which is considered the state-of-the-art for this task. We first applied the Atrous Spatial Pyramid Pooling (ASPP) module (Chen et al., 2017), which consists of parallel atrous convolutions operations. The atrous convolution's fundamental characteristic is the filters that have r - 1 rows/columns of zeros separating neighbouring learnable weights, as shown in Figure 5, when r is the atrous rate
9

Figure 5: Example of atrous convolutions with different atrous rates.

that determines the minimum distance between two learnable filter weights. With x as the input, the atrous convolution is defined as:

K

y[i] = x[i + r  k]w[x]

(4)

k=1

where y[i] is the output feature map at pixel i, w is the convolutional filter and r is the atrous rate. Notice that when r = 1, atrous convolution is equivalent to the standard convolution. This type of convolution allows a larger receptive field without increasing the number of parameters or loss in spatial resolution. Performing these operations in parallel permits to capture context at multiple scales.
Similar to Chen et al. (2018), our ASPP module (see Figure 3) consists of 5 parallel operations: an image pooling, a 1×1 convolution, and three 3×3 atrous convolution with r equal 3, 6 and 9, respectively. We concatenate the five outputs and apply a BN and an ELU activation function. Then, we used two convolutional blocks (CB) consisting of a 3×3 convolution followed by a BN, an ELU activation function, and a bilinear upsampling to recover the input spatial resolution. We also use skip-connections by concatenating the first CB's output with the corresponding encoder low-level features (depicted in Figure 3 with the dotted black lines). The last two layers of our decoder network consist of a Dropout layer and a classification layer implemented with a 1×1 convolution with a softmax activation function that delivers the class membership probabilities. We use this probability map to calculate the LPCFL as described in subsection 4.1. It is worth pointing out that the errors obtained in the loss function are propagated only through the semantic segmentation branch and the shared encoder.

4.3.3. Proposed decoder for distance map estimation
For the distance map branch, we feed the ResNet-9 output feature map to a decoder composed of two CB, which also consist of a 3×3 convolution followed by a BN, an ELU activation function, and a bilinear upsampling. Again, we used skip connections by concatenating the first CB's output with the corresponding low-level features (Figure 3 dotted black line). Finally, we employed a last 3×3 convolutional layer with a sigmoid activation function to obtain a regression map (i.e., the distance map). The output is then used to compute the LPMS E as described in subsection 4.2, which is propagated only through the regression branch and the shared encoder. In preliminary experiments we found that this configuration provided a good response for the distance map estimation.

10

Figure 6: Scheme of the random-crop strategy based on cropping random image tiles guaranteeing at least 10% of cover proportion of one of the target classes.
5. Experiment design
We proposed two experiments in this study. First, we run an experiment to evaluate the partial loss using the segmentation branch uniquely deriving the single-task fully convolutional architecture for sparse annotation (STFCsp). Second, we run the experiment using both branches, i.e., the proposed MTFCsp architecture, and compared their results. This paired experiment's objective is to evaluate the benefits of including the second task as a regularizer.
5.1. Experimental protocol To run our experiments, we first selected 50% of the ITC samples to compose the training
set. We trained and validated STFCsp and MTFCsp models using image tiles of size 128×128. To extract the tiles, we used a random-crop strategy (see Figure 6) with the following pipeline. Firstly, we used a regular grid to sample the tiles' central coordinates. Since our training set consists of a small number of annotated ITCs (40), we set the grid spacing to derive 98% of overlap between neighboring tiles and create sufficient training samples. Secondly, we cropped square tiles of 128×128 from the orthoimage, the digitized polygons, and the distance map (only for the MTFCsp model). Using this strategy, we randomly cropped 140, 000 image tiles on the fly at each epoch, guaranteeing that at least 10% of each tile is annotated with a reference ITC.
As reported in Table 1, some species outnumber others by a large margin in the number of annotated pixels. Class imbalanced datasets significantly reduce deep learning models' accuracy, creating a bias to learn those features that best discriminate among the classes with the higher number of samples. Thus, we proposed a strategy that targets ensuring that all classes have similar probabilities of appearing in a cropped tile by oversampling the under-represented classes. We implemented an image tiles selection process that produces multiple views (total or partial) of the same tree, operating a data augmentation process. Besides, we employed random
11

rotations (90, 180, 270) and horizontal and vertical flips during training to improve generalization. It is worth pointing out that this strategy does not ensure the same number of labeled pixels per class, as annotated trees with higher diameters have more annotated pixels within a tile than trees with smaller diameters.
The network architecture, as detailed in Figure 3 and subsection 4.3, includes a dropout layer before the classification layer. To deal with overfitting, we set a high dropout rate value of 0.65 that gave the best segmentation performance. For training, the stochastic gradient descent (SGD) optimizer with a momentum of 0.9 was used with an initial learning rate of 0.1 and an inverse time decay schedule with a decay rate of 0.1 every five epochs. For the LPCFL function, we selected  = 2 as in (Lin et al., 2017). We run each experiment 25 times, using 25 different random seeds to split the available training tiles into training and validation sets. For the validation set, we randomly selected a 1% hold-off of the training tiles at each run. We trained the models for ten epochs, with eight image tiles per batch. We used ten epochs since both models converged before the 10th epoch. STFCsp usually converged before the 3rd epoch, whereas the MTFCsp model converged at the first five epochs. This behavior was not unexpected since we used heavily overlapped tiles, which implies similar validation and training sets. Even so, we monitored the average F1 score and applied early stop when no improvements higher than 0.9e-4 in the validation set happened in a sequence of five epochs.
At inference time, we applied the trained network to overlapping image tiles using a sliding window strategy and keeping the patch's central region, minimizing border effects. In preliminary experiments, we found that averaging the prediction outcomes considering different overlapping rates improved the result. Hence, we generated classification outputs for 10%, 30%, and 50% overlap and took the average as the final outcome. Finally, we concatenated the outputs to obtain an outcome with the input orthoimage dimensions.
The experiments ran on a Linux workstation (Mint 19.2 Cinnamon) with an Intel Core i74790, 32Gb RAM, and an NVIDIA GeForce Titan GTX 1080 graphics card (11Gb RAM). The processing chain was implemented on the Python platform using the Tensorflow library.
5.2. Performance metrics
We used confusion matrices to investigate the semantic segmentation quality for ITC test samples, which were unseen samples corresponding to 50% of total ITCs. We also report several derived classification metrics: overall accuracy, F1 score per class, per class accuracy, and kappa index. The overall accuracy (OA) is a global metric very much influenced by more extensive classes, with values ranging from 0 (perfect miss-classification) to 1 (perfect classification). It is calculated from the total correctly classified samples divided by the total number of samples. The accuracy per class is the proportion of correctly classified observations for each class Ci regarding each class's total number of samples. The Kappa index (Cohen, 1960), is a measure of the magnitude of agreement between two operators, in this case, the deep learning classier and the human-based reference. The computation considers the difference between the actual agreement and the expected agreement by chance, with values ranging between -1 and 1. Negative values imply an agreement worse than the one occurring by chance, 0 values indicate a total random classification, and a value of 1 a perfect agreement between the reference and the classified pixels.
The F1 score were calculated using the true-positive (tp), false-positive (fp), and false-negative (fn) rates extracted from the confusion matrices. For a given class Ci, tp is the number of pixels correctly classified as Ci, fp is the number of pixels mistakenly classified as Ci, and fn is the number of pixels mistakenly classified as other class when they actually belong to
12

Figure 7: Overall accuracy (OA), Kappa, average class accuracy (AA) and macro-average F1 score (F1) for both models displayed as violin plots. Dotted lines give the upper and lower quartiles and the dashed line gives the median. The stretch of the violins varies according to the variability of the metrics.

class Ci. From those, we compute the precision or producer's accuracy (PA), which represents the probability that a given class on the reference is correctly classified and is calculated as PA = tp/(tp + f p); and the recall or user's accuracy (UA), which represents the probability that a pixel classified as a given class belongs to that class on the reference, and is calculated as UA = tp/(tp + f n). Finally, the F1 score (F1) is defined as the harmonic mean of producer's and user's accuracy as follows:

F1

=

2

×

PA PA

× +

UA UA

(5)

F1 score measures the performance at class level, inferring the similarity between producer's

and user's accuracy (for more details refer to Congalton and Green (2008)).

6. Results
Figure 7 presents the overall accuracy (OA), Kappa score (Kappa), average class accuracy (AA), and the average F1 score through various violin plots. Besides, in Table 2 we report the mean values for both methods and the baseline results for a CNN patch classification approach and an SVM classifier having as input the raw hyperspectral data.
Compared to the baseline CNN approach (Sothe et al., 2020), which reported 84.37% and 82% for OA and Kappa score, respectively; we observed that STFCsp presented a performance drop, with 77.29% for OA and 73.51% for Kappa score (see Table 2). That might have derived from the balancing strategy adopted in both methods. The CNN baseline approach adopted a
13

Table 2: Mean values for each analyzed metric for both DTFCsp and MTFCsp methods and the baseline methods.

Metric
AA F1 OA Kappa

STFCsp
80.96 76.07 77.29 73.51

MTFCsp
88.60 87.51 85.91 83.50

CNN (Sothe et al., 2020)
84.37 82

SVM (Sothe et al., 2020)
62.67 57

data augmentation process during training that balances the number of samples per class at a pixel level. In contrast, STFCsp is a fully convolutional approach, and, as discussed in 5.1, the balancing strategy works at tile level, which does not guarantee the same number of annotated pixels per class. Nonetheless, STFCsp reported superior performance compared to the SVM baseline approach, which indicates the benefits of using the partial loss function to train a fully convolutional approach with scarce and sparse annotated tiles.
We report that the multi-task approach MTFCsp outperformed the single-task model STFCsp for all analyzed metrics. The performance for the MTFCsp model ranged from 83.50% (mean Kappa score) to 88.60% (mean AA), which is significantly superior to STFCsp, with values ranging from 73.51% (mean Kappa score) to 80.96% (mean AA); among the 25 realizations. That represents an increase of up to 11.44%, indicating that adding the auxiliary task was beneficial over the single-task method. However, we found that for both methods, the performance varied when repeating the training and classification procedures 25 times with random initialization for training and validation samples (for all the 25 realizations, the test set remained the same). The OA of MTFCsp varied between 81% and 92%, and the OA of STFCsp varied between 73% and 80%. We observed similar variations for Kappa score and AA. The difference between realizations for the average F1 score in the STFCsp method reached 13%.
To better understand the classification variability, we report the violin plots (see Figure 8) and mean values (Table 3) for accuracy and F1 score per class for the 25 realizations. As reported in the plots, MTFCsp model outperformed by large the STFCsp model for almost all classes according to both metrics. For example, an improvement of more than 10% in accuracy was verified for Matayba (27.64%), Cedrela (27.64%), Schinus sp2 (23.8), and Podocarpus (15.52%). More remarkable improvements were observed in F1 score, with an increase of more than 10% for Schinus sp2 (31.91%), Matayba (22.75%), Schinus sp1 (15.3%), Cedrela (14.54%), Lithrae (13.54%), Nectandra (11.54%), and Cupania (10.02%). However, we also observed that the STFCsp method outperformed the MTFCsp method for some of species. Araucaria, Campomanesia, Cinnamodendron, and Cupania had accuracy values aproximately 1.4% higher, while Mimosa achieved 4.2% of improvement using the STFCsp method. In terms of F1 score, the STFCsp method outperformed the multi-task method in up to 1.63% for Araucaria and Mimosa.
Figure 8 depicts that the variability is different among species and methods. For the STFCsp model, we observed a higher variability for Cedrela in terms of accuracy, and for Schinus sp2 for both accuracy and F1 score values. In contrast, for MTFCsp model, Matayba presents the highest variability for both metrics, followed by Schinus sp2 with important variations in F1 score. These results might be a consequence of the varying number of samples per species and pixels per crown. Schinus sp2 species has only one ITC with 1, 001 pixels to test models (see the last column in Table 3) and, therefore, any miss-classification can cause a significant variation in the accuracy metrics. Analyzing Matayba, one observes that it has the highest number of
14

Figure 8: Accuracy (top) and F1 score (bottom) per class for both models displayed as violin plots. Dotted lines give the upper and lower quartiles, and the dashed line gives the median. The stretch of the violins varies according to the variability of the metrics. Note: Species ID according to Table 1.
15

Table 3: Mean values for accuracy and F1 score per class for the 25 realizations. The columns stands for the number of pixels in the test set.

Species name
Luhea Araucaria Mimosa Lithrae Campomanesia Cedrela Cinnamodendron Cupania Matayba Nectandra Ocotea Podocarpus Schinus sp1 Schinus sp2

STFCsp (%) Acc F1-score
84.8 87.2 84.1 87.1 94.5 95.4 75.2 70.6 99.9 81.9 69.9 78.5 100 85.8 99.9 86.0 34.1 49.4 71.6 71.9 90.6 80.6 66.0 75.2 89.2 78.7 73.6 36.7

MTFCsp (%) Acc F1-score
88.5 90.4 83.3 85.5 90.3 94.0 78.0 84.1 98.5 86.0 97.5 95.1 99.8 95.5 99.7 96.0 61.7 72.2 77.4 83.4 94.2 85.7 81.5 84.7 92.5 94.0 97.4 68.6

Nº Pixels
7,582 13,928
9,901 10,290
6,753 12,333
1,321 2,113 20,229 6,539 38,939 5,899 2,341 1,001

annotated pixels but also the highest variation in the number of pixels per crown (see Table 1), which can affect the MTFCsp model performance. These classes also present high variability in the crown size, which may be related to the high variability in the semantic segmentation outcome.
Figure 9 shows the heatmap of the normalized confusion matrices yielded by the models with the highest and lowest performance among the 25 realizations. For the STFCsp method, the worst and best realization results reached an average class accuracy (AA) of 74.8% and 85.1%, respectively, and for the MTFCsp method, they reached 82.7% and 92.6%, respectively.
Considering the STFCsp model, one observes the highest error associated with the Matayba class, with accuracy values under 35% for both the confusion matrices (9a and 9b), and the Schinus sp2 class, with an accuracy of 32% for the model with the lowest performance (9a). Others species also presented relatively low accuracy values, with values ranging from 60% to 75% for Luhea, Araucaria, Lithrae, Cedrela, Nectandra and Podocarpus. We reported the highest misclassification values (> 24%) for Schinus sp2 × Cupania, Matayba × Ocotea, Araucaria × Campomanesia, Cedrela × Lithrae, Luhea × Campomanesia, Nectandra × Schinus sp2, and Podocarpus × Schinus sp2. Campomanesia, Cinnamodendron, Cupania, Ocotea and Schinus sp1 were the best classified species, reaching accuracy values above 88% for both matrices. The high difference in the classification accuracy for the Schinus sp1 class when comparing both matrices explains the variability observed in the violin-plots.
Considering the MTFCsp model, one observes the number of errors associated with the Luhea and Araucaria classes with 36.9% and 45,7% of accuracy, respectively, followed by Nectandra and Podocarpus with values around 70% for the realization with the lowest average accuracy (Figure 9c). The rest of the species presented accuracy values above 77%. High misclassification rates were observed between Araucaria × Ocotea (46.6%), Matayba × Ocotea (18.5%), Luhea × Campomanesia (19.7%), Podocarpus × Ocotea (18.0%), Luhea × Ocotea
16

(a)

(b)

(c)

(d)

Figure 9: Heatmap of the normalized confusion matrices of the models with the lowest and highest performance using the STFCsp ((a) and (b)) and the MTFCsp ((c) and (d)) models evaluated in the test set. Note: Species ID according to Table 1.
17

(a) STFCsp classification (c) MTFCsp classification

(b) Orthoimage with labels (d) Tree species

Figure 10: Classification images and representative enlarged area. (a) the STFCsp classifier; (b) the MTFCsp classifier; (c) orthoimage with test ITC labels and (d) colormap of the tree species. The white and black circles show correctly classified and miss-classified ITC samples, respectively. Note: non-treed areas were removed using a canopy height model mask.
(19.2%), and Luhea × Nectandra (17.4%). In contrast, for the realization with the best average accuracy (Figure 9d), all species reached high accuracy values, with 10 species presenting accuracy of 88% (Luhea, Araucaria, Mimosa, Campomanesia, Cedrela, Cinnamodendron, Cupania, Ocotea, Schinus sp1, Schinus sp2) and 4 species with values among 75% and 84% (Lithrae, Matayba, Nectandra, Podocarpus).
6.1. Species map
Figure 10 shows the classification images for the area of interest. It is possible to observe that both methods produced visually appealing results considering the ITCs samples from fieldwork. The Figure presents a zoom in the central region of the classified images containing 8 ITC test samples for better visualization.
Looking at the white circles in Figure 10, it turns out that both methods correctly detected and classified most ITC samples. However, many pixels were misclassified for both methods, as shown in the black circles in Figure 10. For the STFCsp method, we observed a Cedrela sample with close to 50% of pixels wrongly classified as Lithrae, and a Nectandra sample with most pixels classified as Matayba and Cupania. For the MTFCsp method, we also observed misclassification in 2 ITCs, with some pixels of Lithrae erroneously classified as Luhea, and Nectandra as Cedrela and Ocotea, respectively.
Besides, we noticed that MTFCsp managed to detect species but not classify all pixels accordingly every time, whereas STFCsp misclassified all the ITC pixels in some cases. A close inspection of the classification map allows to identify the abovementioned remark for Matayba and Cedrela, were the multi-task method correctly detected and classified these species, whilst
18

the single-task method assigned Ocotea and Lithrae, respectively. As expected, in both maps, Araucaria was the dominant species, but the MTFCsp map had more areas classified as Ocotea than STFCsp, which is among the dominant species of that forest phytophysiognomy (Higuchi et al., 2012; Manfredi et al., 2015).
Finally, we observed that the MTFCsp model resulted in a more homogeneous classification map. In contrast, the STFCsp model presented more salt and pepper effect, with different species being assigned within one ITC, highlighting the benefits of the distance map to improve the classification and the segmentation of the ITCs.
7. Discussion
7.1. Considerations about tree species classification
This work proposed a new network architecture for semantic segmentation of tree species in dense forests from hyperspectral data using small training sets. To enable that, we proposed a partial loss function to train an FCN with scarce and sparse ITC training samples. Similar to Wu et al. (2018), we found that the partial loss function improved the semantic segmentation results in our scarce label use case. Furthermore, the multi-task model introducing the distance regression branch improved the semantic segmentation accuracy and produced a visually more appealing tree species map of the study area. Numerically, we observed that the complementary task boosted the performance by more than 10% for OA, Kappa value, average class accuracy (AA), and average F1 score. Although previous works used a regression branch in a multi-task FCN for improving the segmentation of impervious surfaces, buildings, cars, low vegetation, trees, and background from remote sensing images (Diakogiannis et al., 2020), also in combination with a partial loss function (Wang et al., 2019), we believe this is the first work that combines both techniques for tree species mapping in a dense tropical forest region.
An essential contribution in this work is the reduction of the demand for ITC annotated samples to train an FCN that delivers dense semantic segmentation. This is a significant contribution considering the time and effort needed for identifying tree species in fieldwork. Furthermore, the use of a FCN architecture demands less computational effort at inference time which is desirable for large scale remote sensing image analysis. We further believe this proposal is also applicable to other dense forests where ITCs are typically scarce and sparsely annotated over the study area (Ferreira et al., 2016; Ferraz et al., 2016; Ferreira et al., 2019).
Another critical aspect regards the risk of overfitting, when annotated training sets are small such as the one used in this work. Common approaches to tackle that in tree species classification involve data splitting, k-fold cross-validation, leave-one-out cross-validation, and bootstrap-resampling (Fassnacht et al., 2016). The dataset used in our analysis consists of roughly 80 annotated ITC samples, and for some species, only one ITC was available for training and one for testing. We run 25 realizations, leaving out a small percent of the available training tiles as the validation set, and, for each realization, we applied a data augmentation procedure that allowed that training tiles from all classes, regardless of their frequency, had the same probability of composing a minibatch. Also, we used random rotations and horizontal and vertical flips online, which proved to improve the network's generalization (Sothe et al., 2020; Ferreira et al., 2020). We also employed some methods for network regularization, such as drop-out and L2 norm regularization.
Finally, the experimental protocol adopted allowed to verify that variations in the training samples due to factors, as lighting conditions and spectral changes within the tree crowns (e.g.,
19

(a) STFCsp probability map

(b) MTFCsp probability map

Figure 11: Probability output for the realization with the best average class accuracy (AA) for both methods. Note: non-treed areas were removed using a canopy height model mask.
shadows, background), significantly affected the network's performance. Note that we randomly selected 1% hold-off of the training tiles to monitor the network and applied early stopping for each realization. As a result, the tree species were not equally classified, and some of them presented more variability in accuracy and F1 score values among the 25 realizations. Similar results have been reported in previous works, where 5-10% of variations were reported after applying iterative data-sampling (Fassnacht et al., 2016). Our experiments also revealed that species with high variability in the crown's size have the classification performance affected in the multi-task approach. Since the distance map estimation considers the object size, this introduces a new source of variability. Nonetheless, overall, the introduction of the complementary task significantly improved the classification map.
7.2. Considerations about the distance map estimation as regularizer Regarding the role of the complementary task as a regularizer, we observed that the distance
map estimation introduces an inductive bias into the learning process (Ruder, 2017), which leads the model to learn features capable of explain both tasks and therefore generalize better. This inductive bias acts as a regularization term reducing the risk of overfitting for the main task. While both networks fit the training ITC samples almost perfectly, generally, MTFCsp performs better on the test ITC samples, which points to a model with better generalization (Zhang et al., 2018).
Figure 11 shows the probability output for each method's predicted class, and one can note that the single-task model delivers results closer to one-hot predictions, especially if compared to MTFCsp, and such low entropy outcomes are often regarded as an indicator of overfitting (Szegedy et al., 2016). To better understand the networks' behavior, we also report the histogram of softmax probabilities (considering the top 3 classes ranked) for some of the species (see Figure 12). Generally, the STFCsp model leads to softmax distributions with a higher concentration of 0 and 1 probability values. MTFCsp model, on the other hand, delivers smoother output distributions (see Luhea, Mimosa and Cupania in Figure 12). Even if that was observed for 8 species, 6 species presented the opposite behavior, including Matayba and Schinus sp2 (see Figure 12). Overall, the global entropy of the MTFCsp method was 8% higher than the STFCsp method, which indicates that the complementary task can also act as entropy-regularization for the architecture.
20

(a) Luhea

(b) Mimosa

(c) Cupania

(d) Matayba

(e) Schinus sp2

Figure 12: Distribution of the magnitude of softmax probabilities on the correctly classified test pixels for some of the species for both methods. Top row: STFCsp, bottom row: MTFCsp method.
7.3. Potential use of the distance map for ITC detection and delineation
Analyzing and processing the distance map prediction is beyond the scope of this paper. However, we can consider its potential use for other tasks, such as ITC detection, tree count, ITC delineation, ITC statistics, and others. Information about tree counts in tropical forests play an essential role in understanding forest diversity and dynamics (Davies et al., 2021; Cavaleri et al., 2015). Many applications require an accurate estimation of the total number of trees, including protection of natural forests, wildlife habitat mapping, conservation, and forest management (Onishi and Ise, 2021). However, estimating the tree count from a classification map is challenging, particularly in forest regions where tree crowns often overlap. As observed in Figure 10, the classification map fails to identify individual trees when neighboring trees pertain to the same species. In contrast, the distance map predicted by the second branch of the multi-task method could be potentially used to count the number of trees using their centers, as highlighted in yellow.
ITC delineation is a prerequisite for individual tree inventory over large spatial extents (Clark et al., 2005), providing information such as tree location, crown size and distance between individuals (Fassnacht et al., 2016). Nevertheless, most of the studies exploring ITC delineation have been developed for temperate or boreal forests (Ke and Quackenbush, 2011; Duncanson et al., 2014; Dalponte et al., 2015; Lee et al., 2017), and their application to deciduous and tropical forests has proven to be much more challenging (Tochon et al., 2015; Wagner et al., 2018). We advocate that the distance map estimated in our approach may be further explored for ITC delineation in highly diverse forests.
In Figure 13 we report the distance map where the white circles indicate 7 test ITCs correctly detected and delineated, and the black circles correspond to ITC samples with lower detection values. A visual inspection of the prediction map reveals that the great majority of the trees were correctly detected (yellow pixels located in the crowns' center region), and the crowns' edges and background regions were also identified (dark blue pixels). Another important finding is that the distance map roughly delineates the shape of a species characterized by a specific crown geometry, e.g., Araucaria (red circle in Figure 13). That species, for instance, is critically endangered according to the "List of Threatened Species" of the International Union for Conservation of Nature (IUCN 2017) (IUCN, 2017) and, therefore, knowing the number of those trees and their spatial distribution would be very attractive for
21

Figure 13: Distance map resulted from the regression branch of the proposed MTFCsp method and representative enlarged area with the overlaid contour of 8 test ITC samples. Yellow pixels are located in the crowns' center region, while dark blue pixels are located at the crowns' edges. White circles indicate correctly delineated ITCs, while the black circle indicates a bad delineation result. The red circle indicates two ITCs of the species Araucaria that were correctly delineated
conservation purposes.
8. Conclusion
This paper presented a novel methodology to train FCNs from a sparse and scarce ITC sample set for tree species classification in dense forests canopies such as that found in tropical regions. We proposed a partial loss function to train a multi-task network that uses an auxiliary task for distance map regression to improve the semantic segmentation performance. Our results demonstrated that including the complementary task improved the semantic segmentation performance by more than 10% compared to the single-task counterpart.
Our proposed deep learning architecture uses an encoder composed of residual blocks units and two decoders, one for each task. The semantic segmentation decoder builds on the DeepLabv3+ architecture with parallel atrous convolutions and image pooling, whereas the distance map decoder builds on traditional convolution operations. We introduced a partial loss function for training the models, which only back-propagates the losses from pixels that pertain to the annotated region and enables training dense semantics segmentation networks from weakly annotated samples.
We experimented two FCN approaches using UAV-hyperspectral data for tree species classification in a subtropical forest area. The single-task network outperformed an SVM baseline method without the need of relying on hand-engineered features, reaching an overall accuracy (OA) of 77.29%. However, the performance is much lower than the one observed in a CNN-patch baseline method (OA of 84%), highlighting the burden of training FCN with sparse and scarce ITC samples.
Furthermore, we reported that our proposed multi-task model outperformed not only the single-task counterpart but also the CNN-patch baseline, achieving an OA of 85.91%, which indicates that including a complementary task for the semantic segmentation of tree species is beneficial. Being a FCN architecture, our approach also demands less computational effort at inference time than CNN-patch approaches. We also demonstrated that the distance map transform acts as a regularizer, improving the semantic segmentation generalization. Moreover,
22

we discussed how the distance map predictions could help ITC detection and delineation and tree species classification, which ultimately helps monitoring forest biodiversity and endangered tree species like Araucaria.
Finally, we point to further studies regarding model generalization since the network architecture design may have to be adjusted to different forest types. Testing the method in other diverse areas will enable more conclusive and generic outcomes. Acknowledgments
We gratefully acknowledge the financial support offered to this research by the Coordination for the Improvement of Higher Education Personnel (CAPES); the Brazilian National Council for Scientific and Technological Development (CNPq); the Foundation for Support of Research and Innovation, Santa Catarina State (FAPESC) and the Foundation for Support of Research and Innovation, Rio de Janeiro State (FAPERJ).
23

References
Alonso, I., Cambra, A., Munoz, A., Treibitz, T., Murillo, A.C., 2017. Coral-segmentation: Training dense labeling models with sparse ground truth, in: Proceedings of the IEEE International Conference on Computer Vision Workshops, pp. 2874­2882.
Backes, A., Nilson, A.D., 1983. Araucaria angustifolia (Bert.) O. Kuntze, o pinheiro brasileiro. IHERINGIA. Se´rie Bota^nica .
Baldeck, C.A., Asner, G.P., Martin, R.E., Anderson, C.B., Knapp, D.E., Kellner, J.R., Wright, S.J., 2015. Operational tree species mapping in a diverse tropical forest with airborne imaging spectroscopy. PloS one 10, e0118403.
Bischke, B., Helber, P., Folz, J., Borth, D., Dengel, A., 2019. Multi-task learning for segmentation of building footprints with deep neural networks, in: 2019 IEEE International Conference on Image Processing (ICIP), IEEE. pp. 1480­ 1484.
Brandt, M., Tucker, C.J., Kariryaa, A., Rasmussen, K., Abel, C., Small, J., Chave, J., Rasmussen, L.V., Hiernaux, P., Diouf, A.A., Others, 2020. An unexpectedly large count of trees in the West African Sahara and Sahel. Nature 587, 78­82.
Cavaleri, M.A., Reed, S.C., Smith, W.K., Wood, T.E., 2015. Urgent need for warming experiments in tropical forests. Global Change Biology 21, 2111­2121.
Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L., 2017. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE transactions on pattern analysis and machine intelligence 40, 834­848.
Chen, L.C., Zhu, Y., Papandreou, G., Schroff, F., Adam, H., 2018. Encoder-decoder with atrous separable convolution for semantic image segmentation, in: Proceedings of the European conference on computer vision (ECCV), pp. 801­818.
Clark, M.L., Roberts, D.A., 2012. Species-level differences in hyperspectral metrics among tropical rainforest trees as determined by a tree-based classifier. Remote Sensing 4, 1820­1855.
Clark, M.L., Roberts, D.A., Clark, D.B., 2005. Hyperspectral discrimination of tropical rain forest tree species at leaf to crown scales. Remote sensing of environment 96, 375­398.
Cohen, J., 1960. A coefficient of agreement for nominal scales. Educational and psychological measurement 20, 37­46. Congalton, R.G., Green, K., 2008. Assessing the accuracy of remotely sensed data: principles and practices. CRC press. Crawshaw, M., 2020. Multi-Task Learning with Deep Neural Networks: A Survey. arXiv preprint arXiv:2009.09796 . Dalponte, M., Reyes, F., Kandare, K., Gianelle, D., 2015. Delineation of individual tree crowns from ALS and
hyperspectral data: a comparison among four methods. European Journal of Remote Sensing 48, 365­382. Davies, S.J., Abiem, I., Salim, K.A., Aguilar, S., Allen, D., Alonso, A., Anderson-Teixeira, K., Andrade, A., Arellano,
G., Ashton, P.S., Others, 2021. ForestGEO: Understanding forest diversity and dynamics through a global observatory network. Biological Conservation 253, 108907. Diakogiannis, F.I., Waldner, F., Caccetta, P., Wu, C., 2020. Resunet-a: a deep learning framework for semantic segmentation of remotely sensed data. ISPRS Journal of Photogrammetry and Remote Sensing 162, 94­114. Duncanson, L.I., Cook, B.D., Hurtt, G.C., Dubayah, R.O., 2014. An efficient, multi-layered crown delineation algorithm for mapping individual tree structure across multiple ecosystems. Remote Sensing of Environment 154, 378­386. Fassnacht, F.E., Latifi, H., Steren´czak, K., Modzelewska, A., Lefsky, M., Waser, L.T., Straub, C., Ghosh, A., 2016. Review of studies on tree species classification from remotely sensed data. doi:10.1016/j.rse.2016.08.013. Fe´ret, J.B., Asner, G.P., 2012. Tree species discrimination in tropical forests using airborne imaging spectroscopy. IEEE Transactions on Geoscience and Remote Sensing 51, 73­84. Ferraz, A., Saatchi, S., Mallet, C., Meyer, V., 2016. Lidar detection of individual tree size in tropical forests. Remote Sensing of Environment 183, 318­333. Ferreira, M.P., de Almeida, D.R.A., de Almeida Papa, D., Minervino, J.B.S., Veras, H.F.P., Formighieri, A., Santos, C.A.N., Ferreira, M.A.D., Figueiredo, E.O., Ferreira, E.J.L., 2020. Individual tree detection and species classification of Amazonian palms using UAV images and deep learning. Forest Ecology and Management 475, 118397. Ferreira, M.P., Wagner, F.H., Araga~o, L.E.O.C., Shimabukuro, Y.E., de Souza Filho, C.R., 2019. Tree species classification in tropical forests using visible to shortwave infrared WorldView-3 images and texture analysis. ISPRS journal of photogrammetry and remote sensing 149, 119­131. Ferreira, M.P., Zortea, M., Zanotta, D.C., Shimabukuro, Y.E., de Souza Filho, C.R., 2016. Mapping tree species in tropical seasonal semi-deciduous forests with hyperspectral and multispectral data. Remote Sensing of Environment 179, 66­78. Fricker, G.A., Ventura, J.D., Wolf, J.A., North, M.P., Davis, F.W., Franklin, J., 2019. A convolutional neural network classifier identifies tree species in mixed-conifer forest from hyperspectral imagery. Remote Sensing 11, 2326. Ghosh, A., Fassnacht, F.E., Joshi, P.K., Koch, B., 2014. A framework for mapping tree species combining hyperspectral and LiDAR data: Role of selected classifiers and sensor across three spatial scales. International Journal of Applied Earth Observation and Geoinformation 26, 49­63.
24

Hartling, S., Sagan, V., Sidike, P., Maimaitijiang, M., Carron, J., 2019. Urban tree species classification using a WorldView-2/3 and LiDAR data fusion approach and deep learning. Sensors 19, 1284.
He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning for image recognition, in: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778.
Higuchi, P., da Silva, A.C., Ferreira, T.d.S., de Souza, S.T., Gomes, J.P., da Silva, K.M., dos Santos, K.F., Linke, C., Paulino, P.d.S., 2012. Influe^ncia de varia´veis ambientais sobre o padra~o estrutural e flor´istico do componente arbo´reo, em um fragmento de floresta ombro´fila mista montana em lages, SC. Ciencia Florestal 22, 79­90. doi:10.5902/ 198050985081.
Honkavaara, E., Saari, H., Kaivosoja, J., Po¨lo¨nen, I., Hakala, T., Litkey, P., Ma¨kynen, J., Pesonen, L., 2013. Processing and assessment of spectrometric, stereoscopic imagery collected using a lightweight UAV spectral camera for precision agriculture. Remote Sensing 5, 5006­5039.
Hua, Y., Marcos, D., Mou, L., Zhu, X.X., Tuia, D., 2021. Semantic Segmentation of Remote Sensing Images with Sparse Annotations. arXiv preprint arXiv:2101.03492 .
IUCN, P.S., 2017. Guidelines for using the IUCN red list categories and criteria, version 13. Prepared by the Standards and Petitions Subcommittee, Cambridge UK .
Kattenborn, T., Eichel, J., Fassnacht, F.E., 2019. Convolutional Neural Networks enable efficient, accurate and finegrained segmentation of plant species and communities from high-resolution UAV imagery. Scientific reports 9, 1­9.
Ke, Y., Quackenbush, L.J., 2011. A comparison of three methods for automatic tree crown detection and delineation from high spatial resolution imagery. International Journal of Remote Sensing 32, 3625­3647.
Khoreva, A., Benenson, R., Hosang, J., Hein, M., Schiele, B., 2017. Simple does it: Weakly supervised instance and semantic segmentation, in: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 876­885.
Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012. ImageNet Classification with Deep Convolutional Neural Networks, in: Pereira, F., Burges, C.J.C., Bottou, L., Weinberger, K.Q. (Eds.), Advances in Neural Information Processing Systems, Curran Associates, Inc.. pp. 1097­1105.
Lee, J., Coomes, D., Schonlieb, C.B., Cai, X., Lellmann, J., Dalponte, M., Malhi, Y., Butt, N., Morecroft, M., 2017. A graph cut approach to 3D tree delineation, using integrated airborne LiDAR and hyperspectral imagery. arXiv preprint arXiv:1701.06715 .
Lin, T.Y., Goyal, P., Girshick, R., He, K., Dolla´r, P., 2017. Focal loss for dense object detection, in: Proceedings of the IEEE international conference on computer vision, pp. 2980­2988.
Lobo Torres, D., Queiroz Feitosa, R., Nigri Happ, P., Elena Cue´ La Rosa, L., Marcato Junior, J., Martins, J., Ola~ Bressan, P., Gonc¸alves, W.N., Liesenberg, V., 2020. Applying Fully Convolutional Architectures for Semantic Segmentation of a Single Tree Species in Urban Environment on High Resolution UAV Optical Imagery. Sensors 20, 563.
Long, J., Shelhamer, E., Darrell, T., 2015. Fully convolutional networks for semantic segmentation, in: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3431­3440.
Ma, L., Liu, Y., Zhang, X., Ye, Y., Yin, G., Johnson, B.A., 2019. Deep learning in remote sensing applications: A meta-analysis and review. ISPRS journal of photogrammetry and remote sensing 152, 166­177.
Maggiolo, L., Marcos, D., Moser, G., Tuia, D., 2018. Improving maps from CNNs trained with sparse, scribbled ground truths using fully connected CRFs, in: IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium, IEEE. pp. 2099­2102.
Manfredi, S., Pereira Gomes, J., Iaschitzki Ferreira, P., Lopes da Costa Bortoluzzi, R., Mantovani, A., 2015. DISSIMILARIDADE FLOR´ISTICA E ESPE´ CIES INDICADORAS DE FLORESTA OMBRO´ FILA MISTA E ECO´ TONOS NO PLANALTO SUL CATARINENSE. FLORESTA 45, 497­506. URL: https://revistas.ufpr. br/floresta/article/view/36960, doi:10.5380/rf.v45i3.36960.
Ma¨yra¨, J., Keski-Saari, S., Kivinen, S., Tanhuanpa¨a¨, T., Hurskainen, P., Kullberg, P., Poikolainen, L., Viinikka, A., Tuominen, S., Kumpula, T., Others, 2021. Tree species classification from airborne hyperspectral and LiDAR data using 3D convolutional neural networks. Remote Sensing of Environment 256, 112322.
Mellor, A., Boukir, S., Haywood, A., Jones, S., 2015. Exploring issues of training data imbalance and mislabelling on random forest performance for large area land cover classification using the ensemble margin. ISPRS Journal of Photogrammetry and Remote Sensing 105, 155­168.
Natesan, S., Armenakis, C., Vepakomma, U., 2020. Individual tree species identification using Dense Convolutional Network (DenseNet) on multitemporal RGB images from UAV. Journal of Unmanned Vehicle Systems 8, 310­333.
Onishi, M., Ise, T., 2021. Explainable identification and mapping of trees using UAV RGB image and deep learning. Scientific reports 11, 1­15.
Po¨lo¨nen, I., Annala, L., Rahkonen, S., Nevalainen, O., Honkavaara, E., Tuominen, S., Viljanen, N., Hakala, T., 2018. Tree Species Identification Using 3D Spectral Data and 3D Convolutional Neural Network, in: 2018 9th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS), IEEE. pp. 1­5.
Ronneberger, O., Fischer, P., Brox, T., 2015. U-net: Convolutional networks for biomedical image segmentation, in:
25

International Conference on Medical image computing and computer-assisted intervention, Springer. pp. 234­241. Ruder, S., 2017. An overview of multi-task learning in deep neural networks. arXiv preprint arXiv:1706.05098 . Shen, X., Cao, L., 2017. Tree-species classification in subtropical forests using airborne hyperspectral and LiDAR data.
Remote Sensing 9, 1180. Signoroni, A., Savardi, M., Baronio, A., Benini, S., 2019. Deep learning meets hyperspectral image analysis: a
multidisciplinary review. Journal of Imaging 5, 52. Sothe, C., Dalponte, M., de Almeida, C.M., Schimalski, M.B., Lima, C.L., Liesenberg, V., Miyoshi, G.T.,
Tommaselli, A.M.G., 2019. Tree species classification in a highly diverse subtropical forest integrating UAV-based photogrammetric point cloud and hyperspectral data. Remote Sensing 11, 1338. Sothe, C., De Almeida, C.M., Schimalski, M.B., La Rosa, L.E.C., Castro, J.D.B., Feitosa, R.Q., Dalponte, M., Lima, C.L., Liesenberg, V., Miyoshi, G.T., Others, 2020. Comparative performance of convolutional neural network, weighted and conventional support vector machine and random forest for classifying tree species using hyperspectral and photogrammetric data. GIScience & Remote Sensing 57, 369­394. Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z., 2016. Rethinking the inception architecture for computer vision, in: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2818­2826. Tang, M., Djelouah, A., Perazzi, F., Boykov, Y., Schroers, C., 2018. Normalized cut loss for weakly-supervised cnn segmentation, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1818­1827. Tian, X., Chen, L., Zhang, X., Chen, E., 2020. Improved Prototypical Network Model for Forest Species Classification in Complex Stand. Remote Sensing 12, 3839. Tochon, G., Feret, J.B., Valero, S., Martin, R.E., Knapp, D.E., Salembier, P., Chanussot, J., Asner, G.P., 2015. On the use of binary partition trees for the tree crown segmentation of tropical rainforest hyperspectral images. Remote sensing of environment 159, 318­331. Wagner, F.H., Ferreira, M.P., Sanchez, A., Hirye, M.C.M., Zortea, M., Gloor, E., Phillips, O.L., de Souza Filho, C.R., Shimabukuro, Y.E., Araga~o, L.E.O.C., 2018. Individual tree crown delineation in a highly diverse tropical forest using very high resolution satellite images. ISPRS journal of photogrammetry and remote sensing 145, 362­377. Wagner, F.H., Sanchez, A., Tarabalka, Y., Lotte, R.G., Ferreira, M.P., Aidar, M.P.M., Gloor, E., Phillips, O.L., Aragao, L.E.O.C., 2019. Using the U-net convolutional network to map forest types and disturbance in the Atlantic rainforest with very high resolution images. Remote Sensing in Ecology and Conservation 5, 360­375. Wang, B., Qi, G., Tang, S., Zhang, T., Wei, Y., Li, L., Zhang, Y., 2019. Boundary Perception Guidance: A ScribbleSupervised Semantic Segmentation Approach., in: IJCAI, pp. 3663­3669. Wu, W., Qi, H., Rong, Z., Liu, L., Su, H., 2018. Scribble-Supervised Segmentation of Aerial Building Footprints Using Adversarial Learning. IEEE Access 6, 58898­58911. Zhang, B., Zhao, L., Zhang, X., 2020. Three-dimensional convolutional neural network model for tree species classification using airborne hyperspectral images. Remote Sensing of Environment 247, 111938. Zhang, L., Zhang, L., Du, B., 2016. Deep learning for remote sensing data: A technical tutorial on the state of the art. IEEE Geoscience and Remote Sensing Magazine 4, 22­40. Zhang, Y., Xiang, T., Hospedales, T.M., Lu, H., 2018. Deep mutual learning, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4320­4328. Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J., 2017. Pyramid scene parsing network, in: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2881­2890. Zhu, X.X., Tuia, D., Mou, L., Xia, G.S., Zhang, L., Xu, F., Fraundorfer, F., 2017. Deep learning in remote sensing: A comprehensive review and list of resources. IEEE Geoscience and Remote Sensing Magazine 5, 8­36.
26

