arXiv:2106.00114v1 [math.PR] 31 May 2021

KERNEL BASED DIRICHLET SEQUENCES

PATRIZIA BERTI, EMANUELA DREASSI, FABRIZIO LEISEN, LUCA PRATELLI, AND PIETRO RIGO

Abstract. Let X = (X1, X2, . . .) be a sequence of random variables with values in a standard space (S, B). Suppose

X1  

and

P Xn+1  · | X1, . . . , Xn

 + =

n i=1

K

(Xi

)

n+

a.s.

where  > 0 is a constant,  a probability measure on B, and K a random

probability measure on B. Then, X is exchangeanble whenever K is a reg-

ular conditional distribution for  given any sub--field of B. Under this

assumption, X enjoys all the main properties of classical Dirichlet sequences,

including Sethuraman's representation, conjugacy property, and convergence

in total variation of predictive distributions. If µ is the weak limit of the em-

pirical measures, conditions for µ to be a.s. discrete, or a.s. non-atomic, or

µ   a.s., are provided. Two CLT's are proved as well. The first deals with

stable convergence while the second concerns total variation distance.

1. Introduction

Throughout, S is a Borel subset of a Polish space and B the Borel -field on S. All random elements are defined on a common probability space, say (, A, P ). Moreover,

X = (X1, X2, . . .) is a sequence of random variables with values in (S, B) and

Fn = (X1, . . . , Xn).

We say that X is a Dirichlet sequence, or a Polya sequence, if its predictive distri-

butions are of the form

P

Xn+1  · | Fn

=  P (X1  ·) +

n i=1

Xi

(·)

n+

a.s.

for all n  1 and some constant  > 0. The finite measure  P (X1  ·) is called the parameter of X. Here and in the sequel, for each x  S, we denote by x the unit mass at x.
Let L0 be the class of Dirichlet sequences. As it can be guessed from the definition, each element of L0 is exchangeable. Since Ferguson, Blackwell and Mac Queen, L0 played a prevailing role in Bayesian statistics. It was for a long time the basic ingredient of Bayesian nonparametrics.
And still today, the Bayesian nonparametrics machinery is greatly affected by L0

2020 Mathematics Subject Classification. 60G09, 60G25, 60G57, 62F15, 62M20. Key words and phrases. Bayesian nonparametrics, Central limit theorem, Dirichlet sequence, Exchangeability, Predictive distribution, Random probability measure, Regular conditional distribution.
1

2PATRIZIA BERTI, EMANUELA DREASSI, FABRIZIO LEISEN, LUCA PRATELLI, AND PIETRO RIGO

and its developments. In addition, L0 plays a role in various other settings, including population genetics and species sampling. The literature on L0 is huge, and we do not try to summarize it here. We just mention a few seminal papers and recent textbooks: [1], [8], [9], [11], [13], [16], [17], [18], [19], [20].
The object of this paper is a new class of exchangeable sequences, say L, such that L  L0. There are essentially two reasons for taking L into account. First, all main features of L0 are preserved by L, including the Sethuraman's representation, the conjugacy property and the simple form of predictive distributions. Thus, from the point of view of a Bayesian statistician, L can be handled as simply as L0. Second, L is more flexible than L0 and allows to model more real situations. For instance, if X  L, the weak limit of the empirical measures is not forced to be a.s. discrete, but it may be a.s. non-atomic or even a.s. absolutely continuous with respect to a reference measure.

1.1. Definition of L. Obviously, the notion of Dirichlet sequence can be extended in various ways. In this paper, for X to be an extended Dirichlet sequence, two conditions are essential. First, X should be exchangeable. Second, the predictive distributions of X should have a known (and possibly simple) structure. Indeed, to define a sequence X via its predictive distributions has various merits. It is technically convenient (see the proof of Theorem 12) and makes the dynamics of X explicit. Furthermore, having the predictive distributions in closed form makes straightforward the Bayesian predictive inference on X; see e.g. [7] and [14]. We also note that, as claimed in [15]: "There are very few models for exchangeable sequences X with an explicit prediction rule".

Let P be the collection of all probability measures on B and C the -field over P generated by the maps p  p(A) for all A  B. A kernel on (S, B) is a measurable map K : (S, B)  (P, C). Thus, K(x)  P for each x  S and x  K(x)(A) is a B-measurable map for fixed A  B.

A quite natural extension of L0, among the possible ones, consists in replacing  with any kernel K in the predictive distributions of X. If K is arbitrary, however, X may fail to be exchangeable. More precisely, fix   P, a constant  > 0 and a kernel K on (S, B). By the Ionescu-Tulcea theorem, there is a sequence X such that

(1)

X1  

and

P Xn+1  · | Fn

=  +

n i=1

K (Xi )

n+

a.s.

for all n  1. Generally, however, X is not exchangeable. As an obvious example, take the trivial kernel K(x) =  for all x  S, where   P but  = . Then,
condition (1) implies that X2 is not distributed as X1. Our starting point is that, for X to be exchangeable, it suffices condition (1) and

(2)

K is a regular conditional distribution (r.c.d.) for  given G

for some sub--field G  B. We recall that, since K(x)  P for each x  S, condition (2) means that K is G-measurable and (A  G) = G K(x)(A) (dx) for all A  B and G  G. Condition (2) makes the next definition operational.

Say that X is a kernel based Dirichlet sequence if it is exchangeable and satisfies condition (1) for some   P, some constant  > 0 and some kernel K on (S, B).

DIRICHLET SEQUENCES

3

In particular, X is a kernel based Dirichlet sequence if conditions (1)-(2) hold. In the sequel, L denotes the collection of all X satisfying conditions (1)-(2). If X  L and G = B, then K =  and X  L0. At the opposite extreme, if G = {, S}, then K(x) =  for -almost all x  S and X is i.i.d. Various other examples come soon to the fore. The following are from [7] (even if, when writing [7], we didn't know yet that X is exchangeable).

Example 1. Let G = (H), where H  B is a countable partition of S such that (H) > 0 for all H  H. A r.c.d. for  given G is

K(x) = 1H(x) (· | H) =  · | H(x)
HH

where H(x) denotes the only H  H such that x  H. Therefore, X  L whenever

 + X1   and P Xn+1  · | Fn =

n i=1



· | H(Xi)

n+

a.s.

Note that

P Xn+1  · | Fn  (·) a.s.
This fact highlights a stricking difference between L and L0. In this example, if  is non-atomic, the probability distributions of X and Y are singular for any Y  L0.

Example 2. Let S = R2 and G = (f ) where f (u, v) = u for all (u, v)  R2. Moreover, let B0 be the Borel -field on R and N (u) the Gaussian law on B0 with mean u and variance 1. Fix a probability measure r on B0 and define

(A × B) = N (u)(B) r(du)
A
Then, a r.c.d. for  given G is

for all A, B  B0.

K(u, v) = u × N (u) for all (u, v)  R2.

Hence, letting Xi = (Ui, Vi), one obtains X  L provided (U1, V1)   and

P Un+1  A, Vn+1  B | Fn = (A × B) +

n i=1

1A(Ui)

N

(Ui

)(B

)

n+

a.s.

Example 3. Let f : S  S be a measurable map. If  is f -invariant, that is  =   f -1, it may be reasonable to take
G = A  B : f -1(A) = A .

As a trivial example, if S = R, f (x) = -x and  is symmetric, then

K (x)

=

x

+ -x 2

is a r.c.d. for  given G. Hence, X  L whenever X1   and

P Xn+1  · | Fn

= 2  +

n i=1

(Xi

+

-Xi )

2 (n + )

a.s.

4PATRIZIA BERTI, EMANUELA DREASSI, FABRIZIO LEISEN, LUCA PRATELLI, AND PIETRO RIGO

1.2. Sethuraman's representation and conjugacy for L0. Before going on, a few basic properties of L0 are to be recalled. A random probability measure on (S, B) is a measurable map µ : (, A)  (P, C).
Let X be exchangeable. Since (S, B) is a standard space, there is a random proba-
bility measure µ on (S, B) such that

µ(A)

a=.s.

lim
n

1 n

n

1A(Xi)

a=.s.

lim
n

P

Xn+1  A | Fn

i=1

for each fixed A  B. Moreover, X is i.i.d. conditionally on µ, in the sense that

P X  B | µ = µ(B) a.s. for all B  B

where µ = µ × µ × . . . Suppose now that X  L0 and define
D(C) = P (µ  C)

for all C  C.

Such a D is a probability measure on C, called the Dirichlet prior, and admits the following representation. Define a random probability measure µ on (S, B) as

µ = Vj Zj ,
j

where (Zj) and (Vj ) are independent sequences, (Zj) is i.i.d. with Z1  , and (Vj ) has the stick-breaking distribution with parameter ; see Section 2. Then,
D(C) = P (µ  C) for all C  C.

Thus, D can be also regarded as the probability distribution of µ. This fact, proved by Sethuraman [20], is fundamental in applications; see e.g. [10]. Finally, we recall the conjugacy property of L0. Write D() (instead of D) if X  L0 has parameter . In this notation, if X has parameter , then

n
P (µ  C | Fn) = D  + Xi (C)
i=1

a.s. for all C  C and n  1.

Roughly speaking, the posterior distribution of µ given (X1, . . . , Xn) is still of the

Dirichlet type but the parameter turns into  +

n i=1

Xi

.

Once again,

this fact

plays a basic role in applications.

1.3. Our contribution. As claimed above, this paper aims to introduce and investigate the class L. Our first result is that conditions (1)-(2) suffice for exchangeability of X. Thus, each X  L is a kernel based Dirichlet sequence, as defined in Subsection 1.1.
The next step is to develop some theory for L. The obvious hope is that, at least to a certain extent, such a theory is parallel to that of L0. This is exactly the case. All main results concerning L0 extend nicely to L. To illustrate, we assume X  L and we mention a few facts.
· Up to replacing  with K, the Sethuraman's representation remains exactly the same. Precisely, P (µ  C) = P (µ  C) for all C  C, where
µ = Vj K(Zj)
j

DIRICHLET SEQUENCES

5

and (Vj) and (Zj) are as in Subsection 1.2.

· The predictive distributions converge in total variation, that is

sup P Xn+1  A | Fn - µ(A) -a.s. 0
AB

as n  .

· If X  L0, it is well known that µ is a.s. discrete. This result extends to L as follows. Denote by D1, D2, D3 the collections of elements of P which are, respectively, discrete, non-atomic, or absolutely continuous with
respect to . Then, for each 1  j  3,

P (µ  Dj) = 1  K(x)  Dj for -almost all x  S.

Since x  D1 for all x  S, the classical result is recovered. But now, with a suitable K, one obtains P (µ  D2) = 1 or P (µ  D3) = 1. This fact may be useful in applications.

· The conjugacy property of L0 is still available. For each n  1, let

V (n) = Vj(n) : j  1 and Z(n) = Zj(n) : j  1

be two sequences such that

(i) V (n) and Z(n) are conditionally independent given Fn; (ii) V (n) has the stick-breaking distribution, with parameter n + , con-
ditionally on Fn; (iii) Z(n) is i.i.d., conditionally on Fn, with

P (Z1(n)  · | Fn) = P

Xn+1  · | Fn

=  +

n i=1

K

(Xi

)

n+

a.s.

Then,

where

P (µ  · | Fn) = P (µn  · | Fn)

µn =

Vj(n) K Zj(n) .

j

Again, if K = , this result reduces to the classical one.
· A stable CLT holds true. Let S = Rp and x 2 (dx) < , where · is the Euclidean norm. Suppose that K has mean 0, in the sense that

yi K(x)(dy) = 0 for all x  Rp and i = 1, . . . , p

where yi denotes the i-th coordinate of a point y  Rp. Then, n-1/2

n i=1

Xi

converges stably (in particular, in distribution) to the Gaussian kernel

Np(0, ), where  is the (random) covariance matrix

 = yi yj µ(dy) : 1  i, j  p .

Moreover, under some additional conditions, n-1/2

n i=1

Xi

converges

in

total variation as well.

6PATRIZIA BERTI, EMANUELA DREASSI, FABRIZIO LEISEN, LUCA PRATELLI, AND PIETRO RIGO
A last remark is in order. To prove the above results, our main tool is that K(Xn) : n  1
is a classical Dirichlet sequence with values in (P, C); see Theorem 7.

2. Preliminaries

For all   P and bounded measurable f : S  R, the notation (f ) stands for (f ) = f d. Moreover, Np(0, ) denotes the p-dimensional Gaussian law (on the Borel -field of Rp) with mean 0 and covariance matrix . Let  > 0 be a constant, (Wn) an i.i.d. sequence with W1  beta(1, ) and

T1 = W1,

n-1
Tn = Wn (1 - Wi) for n > 1.
i=1

A sequence (Vn) of real random variables has the stick-breaking distribution with parameter  if (Vn)  (Tn). Note that Vn > 0 for all n and n Vn = 1 a.s.
Stable convergence is a strong form of convergence in distribution. Let N be a random probability measure on (S, B). Then, Xn converges to N stably if

E N (f ) | H

=

lim E
n

f (Xn)

|

H

for all bounded continuous f : S  R and all H  A with P (H) > 0. In particular, Xn converges in distribution to the probability measure A  E N (A) .

We next report a useful characterization of exchangeability due to [12]; see also [5] and [7]. Let F0 = {, } be the trivial -field and

n(x) = P Xn+1  · | (X1, . . . , Xn) = x

for all x  Sn.

Theorem 4. ([12, Theorem 3.1]). The sequence X is exchangeable if and only if

P (Xn+1, Xn+2)  · | Fn = P (Xn+2, Xn+1)  · | Fn

a.s.

for all n  0 and

n(x) = n((x))
for all n  2, all permutations  : Sn  Sn, and almost all x  Sn. (Here, "almost all" is with respect to the marginal distribution of (X1, . . . , Xn)).

We conclude this section with a technical lemma. Let

(K) = x  S : K(x)  C : C  C

be the -field over S generated by the kernel K.

Lemma 5. Under condition (2), there is a set F  (K) such that (F ) = 1 and

K(y)(B) K(x)(dy) = K(x)(A) K(x)(B)
A
Moreover,

for all x  F and A, B  B.

K(y)(B) (dy) = K(y)(A) (dy)

A

B

for all A, B  B.

DIRICHLET SEQUENCES

7

Proof. Just note that (K) is countably generated and K is a r.c.d. for  given (K). Hence, there is F  (K) satisfying (F ) = 1 and

K(x) = x on (K) for each x  F ;

see e.g. [4]. Fix x  F and A, B  B. Define G = y  S : K(y)(B) = K(x)(B) . Since x  F  G and G  (K),

Therefore,

K(x)(G) = x(G) = 1.

K(y)(B) K(x)(dy) = K(x)(B) K(x)(dy) = K(x)(A) K(x)(B).

A

A

Finally,

K(y)(B) (dy) = E 1B | (K) d = E 1A | (K) d = K(y)(A) (dy).

A

A

B

B

3. Results

Recall that L is the class of sequences satisfying conditions (1)-(2). In this section, X  L and µ is a random probability measure on (S, B) such that

µ(A)

a=.s.

lim
n

1 n

n

1A(Xi)

a=.s.

lim
n

P

Xn+1  A | Fn

i=1

for all A  B.

Existence and essential uniqueness of µ depends on X is exchangeable and (S, B) is a standard space; see Subsection 1.2. Our starting point is the following.

Theorem 6. Under condition (1), X is exchangeable if and only if

(3)

K(y)(B) (dy) = K(y)(A) (dy)

A

B

and

(4)

K(y)(B) K(x)(dy) = K(y)(A) K(x)(dy)

A

B

for all A, B  B and -almost all x  S. In particular, X is exchangeable whenever

X  L (because of Lemma 5).

Proof. For all A, B  B, condition (1) implies

Therefore,

P (X1  A, X2  B) = E 1A(X1) P (X2  B | F1)

=E

1A(X1)

(B)

+ 1

K (X1 )(B ) +

=

1

 +



(B)

(A)

+

1 1+

K(y)(B) (dy).
A

condition (3)  (X1, X2)  (X2, X1).

8PATRIZIA BERTI, EMANUELA DREASSI, FABRIZIO LEISEN, LUCA PRATELLI, AND PIETRO RIGO

Similarly, under (1), one obtains

P X2  A, X3  B | F1 = E 1A(X2) P (X3  B | F2) | F1

=E

1A(X2)

(B)

+

K (X1)(B ) 2+

+

K (X2 )(B )

|

F1

=

1+ 2+

P (X2

B

| F1) P (X2

 A | F1)

+

1 2+

E

1A(X2) K(X2)(B) | F1

a.s.

and

E 1A(X2) K(X2)(B) | F1

=

1

 +



A

K (y )(B )

(dy)

+

1

1 +



K(y)(B) K(X1)(dy)
A

a.s.

Next, if X is exchangeable, condition (3) follows from (X1, X2)  (X2, X1). Moreover, P X2  A, X3  B | F1 = P X2  B, X3  A | F1 a.s. implies

E 1A(X2) K(X2)(B) | F1 = E 1B(X2) K(X2)(A) | F1 a.s.

Therefore, (4) follows from (3) and the above condition.

Conversely, assume conditions (3)-(4). Define

n(x) =



+

n i=1

K

(xi

)

n+

for all n  1 and x = (x1, . . . , xn)  Sn.

By (1), P (Xn+1  · | Fn) = n(X1, . . . , Xn) a.s. Moreover, n(x) = n((x)) for all n  2, all permutations  : Sn  Sn and all x  Sn. Hence, by Theorem 4, it suffices to show that

P (Xn+1, Xn+2)  · | Fn = P (Xn+2, Xn+1)  · | Fn

a.s. for all n  0.

For n = 0, the above condition is equivalent to (3) (recall that F0 is the trivial -field). Therefore, it is enough to show that

(5)

n+1(x, y)(B) n(x)(dy) = n+1(x, y)(A) n(x)(dy)

A

B

for all n  1, all A, B  B and almost all x  Sn (where "almost all" refers to the
marginal distribution of (X1, . . . , Xn)). Fix m  1 and A  B. If Xi   for i = 1, . . . , m, then

E K(Xi)(A) = K(y)(A) (dy) = (A) for i = 1, . . . , m,

where the second equality is by (3) (apllied with B = S). Hence,

P (Xm+1  A) = E P (Xm+1  A | Fm)

=

(A) m+

+

m i=1

E

K (Xi )(A)

m+

= (A).

By induction, it follows that Xi   for all i  1. Finally, fix n  1 and A, B  B. By (4), there is a set M  B such that (M ) = 1
and

K(y)(B) K(x)(dy) = K(y)(A) K(x)(dy) for all x  M.

A

B

DIRICHLET SEQUENCES

9

Thanks to this fact and condition (3), if x = (x1, . . . , xn)  M n, one obtains

 K(y)(B) n(x)(dy) =
A

A K(y)(B) (dy) +

n i=1

n+

A K(y)(B) K(xi)(dy)

 =

B K(y)(A) (dy) +

n i=1

n+

B K(y)(A) K(xi)(dy) =

K(y)(A) n(x)(dy).
B

It follows that

n+1(x, y)(B) n(x)(dy) = (B) +

A

A

n i=1

K

(xi)(B)

n+1+

+

K (y )(B )

n(x)(dy)

=

n+ n+1+



n(x)(B) n(x)(A)

+

A K(y)(B) n(x)(dy) n+1+

=

n+ n+1+

n(x)(B) n(x)(A)

+

B K(y)(A) n(x)(dy) n+1+

= n+1(x, y)(A) n(x)(dy).
B
Therefore, equation (5) holds for each x  M n. To conclude the proof, it suffices to note that, since (M ) = 1 and Xi   for all i,
P (X1, . . . , Xn)  M n = 1.

After proving Theorem 6, the next step is to develop some theory for L. To this end, the following result is fundamental.

Theorem 7. If X  L, the sequence K(Xn) : n  1 is a Dirichlet sequence with values in (P, C) and parameter the image measure    K-1.

Proof. As already noted, (K) is countably generated and K is a r.c.d. for  given (K). Hence, there is a set F  (K) such that

(F ) = 1 and K(x)(B) = x(B) for all B  (K) and x  F ;

see e.g. [4]. Since P (Xn  F ) = (F ) = 1 for all n, it follows that

P Xn+1 

B | Fn

=

(B)

+ n

n i=1
+

Xi (B)

for all B  (K) a.s.

Having noted this fact, define

Kn =  K(X1), . . . , K(Xn) .

Since Kn  Fn and P Xn+1  · | Fn is Kn-measurable,

P Xn+1  · | Kn = P Xn+1  · | Fn

a.s.

Finally, fix C  C and define B = {K  C}. Since B  (K), one obtains

P K(Xn+1)  C | Kn = P Xn+1  B | Kn = P Xn+1  B | Fn

=

(B)

+ n

n i=1
+

Xi (B)

=





K-1(C) + n+

n i=1


K (Xi ) (C )

a.s.

1P0ATRIZIA BERTI, EMANUELA DREASSI, FABRIZIO LEISEN, LUCA PRATELLI, AND PIETRO RIGO

We next turn to a Sethuraman-like representation for L. Let µ be the random probability measure on (S, B) defined as

µ = Vj K(Zj),
j
where (Zj) and (Vj ) are independent sequences, (Zj) is i.i.d. with Z1  , and (Vj ) has the stick-breaking distribution with parameter ; see Section 2.

Theorem 8. If X  L, then P (µ  C) = P (µ  C)

for all C  C.

Proof. Let µ0 and µ0 be the restrictions of µ and µ on (K). Then, µ0  µ0 by [20] and since K(Xn) : n  1 is a classical Dirichlet sequence. Hence,

µ(g1), . . . , µ(gk)  µ(g1), . . . , µ(gk)

whenever g1, . . . , gk : S  R are bounded and (K)-measurable. In addition, for fixed A  B, one obtains

K(x)(A) µ(dx) = lim
n

n i=1

K (Xi )(A) n

=

lim
n

P

(Xn+1



A

|

Fn)

=

µ(A)

a.s.

Similarly, Lemma 5 (applied with B = S) implies

K(x)(A) K(Zj)(dx) = K(Zj)(A) a.s. for all j  1.

Thus,
K(x)(A) µ(dx) = Vj K(x)(A) K(Zj )(dx)
j
= Vj K(Zj)(A) = µ(A) a.s.
j
Having noted these facts, fix k  1, A1, . . . , Ak  B, and define gi(x) = K(x)(Ai) for all x  S and i = 1, . . . , k. Then,
µ(A1), . . . , µ(Ak) a=.s. µ(g1), . . . , µ(gk)  µ(g1), . . . , µ(gk) a=.s. µ(A1), . . . , µ(Ak) .
This concludes the proof.

Theorem 8 plays for L the same role played by [20] for L0. Among other things, it provides a simple way to approximate the probability distribution of µ and to obtain its posterior distribution; see forthcoming Theorem 12 and its proof. For a further implication, define

D1 = p  P : p discrete , D2 = p  P : p non-atomic , D3 = p  P : p   . Then, Theorem 8 implies the following result.

Theorem 9. If j  {1, 2, 3} and X  L, then P (µ  Dj)  {0, 1} and

P (µ  Dj) = 1  K(x)  Dj for -almost all x  S.

In addition,

(6)

sup P Xn+1  A | Fn - µ(A) -a.s. 0 as n  .

AB

DIRICHLET SEQUENCES

11

Proof. Fix j  {1, 2, 3} and define aj = {x : K(x)  Dj}. If aj = 1, Theorem 8 yields

P (µ  Dj) = P (µ  Dj) = P K(Zi)  Dj for all i  1 = 1.

Similarly, if aj < 1, P (µ  Dj)  P K(Zi)  Dj for 1  i  n = anj - 0

as n  .

It remains to prove (6). Define the random probability measure

n

=

1 n

n
K (Xi ).

i=1

To prove (6), it is enough to show that limn supAB |n(A) - µ(A)| a=.s. 0 and this limit relation is actually true if X  L0; see e.g. [18, Prop. 11]. Hence, since
K(Xn) : n  1 is a classical Dirichlet sequence, one obtains

sup |n(A) - µ(A)| -a.s. 0.
A(K )

Now, we argue as in the proof of Theorem 8. Precisely, for each A  B, Lemma 5 (applied with B = S) yields

K(x)(A) n(dx)

=

1 n

n

i=1

K(x)(A) K(Xi)(dx)

=

1 n

n
K(Xi)(A) = n(A)

a.s.

i=1

Similarly, K(x)(A) µ(dx) = µ(A) a.s. Therefore, after fixing a countable field B0 such that B = (B0), one finally obtains

sup |n(A) - µ(A)| = sup |n(A) - µ(A)|

AB

AB0

a=.s. sup K(x)(A) n(dx) - K(x)(A) µ(dx)
AB0
 sup |n(A) - µ(A)| -a.s. 0.
A(K )

It is worth noting that, for an arbitrary exchangeable sequence X, convergence in total variation of P Xn+1  · | Fn is not guaranteed; see e.g. [6].
A further consequence of Theorem 8 is a stable CLT (stable convergence is briefly recalled in Section 2). For each y  Rp, let yi denote the i-th coordinate of y.
Theorem 10. Let S = Rp and X  L. Suppose x 2 (dx) < , where · is the Euclidean norm, and

yi K(x)(dy) = 0 for all x  Rp and i = 1, . . . , p.

Then,

n
i=1

Xi

n

s-tably

Np(0, )

where  is the random covariance matrix

as n  ,

 = yi yj µ(dy) : 1  i, j  p .

1P2ATRIZIA BERTI, EMANUELA DREASSI, FABRIZIO LEISEN, LUCA PRATELLI, AND PIETRO RIGO

Proof. By standard arguments, it suffices to show that

n
i=1

bXi

n

s-tably

N1(0,

b b)

for each b  Rp,

where points of Rp are regarded as column vectors and b denotes the transpose of b. Define

b2 = E (bX1)2 | µ - E(bX1 | µ)2. For fixed b  Rp, one obtains

n

n-1/2

bXi - E(bX1 | µ) s-tably N1(0, b2);

i=1

see e.g. [3, Th. 3.1] and the subsequent remark. Furthermore,

p

E(bX1 | µ) = (by) µ(dy) = bi yi µ(dy)

a.s. and

i=1

pp

E (bX1)2 | µ = (by)2 µ(dy) =

bibj yi yj µ(dy) = b b a.s.

i=1 j=1

Hence, it suffices to show that yi µ(dy) a=.s. 0 for all i, and this follows from Theorem 8. In fact, yi µ(dy)  yi µ(dy) and

yi µ(dy) = Vj
j
This concludes the proof.

yi K(Zj)(dy) = 0.

Theorem 10 applies to Examples 3 and 14. In fact, in Example 3, one has p = 1

and K(x) = (x + -x)/2. Hence, y K(x)(dy) = 0 for all x  R. Example 14

is discussed below. Here, we give conditions for convergence in total variation of

n-1/2

n i=1

Xi.

Theorem 11. In addition to the conditions of Theorem 10, suppose that K(x) is not singular, with respect to Lebesgue measure, for -almost all x  Rp. Define

n
Yn = n-1/2 Xi and (A) = E Np(0, )(A)
i=1

for all A  B, where  =

yi yj µ(dy) : 1  i, j  p .

Then,

lim sup
n AB

P (Yn  A) - (A)

= 0.

Proof. Let D be the collection of elements of P which are not singular with respect to Lebesgue measure. By Theorem 8, P (µ  D) = P (µ  D) = 1. Hence,
conditionally on µ, the sequence X is i.i.d. and the common distribution µ be-
longs to D a.s. Arguing as in Theorem 10, one also obtains yi µ(dy) = 0 and

DIRICHLET SEQUENCES

13

y 2µ(dy) <  a.s. for all i. Thus, conditionally on µ, Yn converges to Np(0, ) in total variation (see e.g. [2]) that is
sup P (Yn  A | µ) - Np(0, )(A) -a.s. 0.
AB
Finally,    implies (·) = E Np(0, )(·) . Hence,

sup P (Yn  A) - (A) = sup P (Yn  A) - E Np(0, )(A)

AB

AB

 E sup P (Yn  A | µ) - Np(0, )(A) - 0 as n  .
AB

Our last result deals with the posterior distribution of µ. We aim to find the conditional distribution of µ given Fn = (X1, . . . , Xn). To this end, for each n  1, we denote by
V (n) = Vj(n) : j  1 and Z(n) = Zj(n) : j  1
two sequences such that:

(i) V (n) and Z(n) are conditionally independent given Fn;

(ii) V (n) has the stick-breaking distribution with parameter n +  conditionally on Fn;

(iii) Z(n) is i.i.d. conditionally on Fn with

P (Z1(n)  · | Fn) = P

Xn+1  · | Fn

=  +

n i=1

K

(Xi

)

n+

a.s.

Moreover, we let

µn =

Vj(n) K Zj(n) .

j

Theorem 12. If X  L, then

P (µ  C | Fn) = P (µn  C | Fn) a.s. for all C  C and n  1.

Note that, if X  L0 and X has parameter  (i.e., if K = ) then
n
P (µ  C | Fn) = D  + Xi (C) = P (µn  C | Fn) a.s.
i=1
Hence, Theorem 12 extends to L the conjugacy property of L0. Such a property is clearly useful as regards Bayesian statistical inference. On one hand, the Bayesian analysis of X  L is as simple as that of X  L0. On the other hand, L is able to model much more situations than L0. As an obvious example, for X  L, it may be that P (Xi = Xj) = 0 if i = j. See e.g. Example 1 and Theorem 9.
Theorem 12 can be proved in various ways. We report here the simplest and most direct proof. Such a proof relies on Theorem 8 and the definition of L in terms of predictive distributions.

1P4ATRIZIA BERTI, EMANUELA DREASSI, FABRIZIO LEISEN, LUCA PRATELLI, AND PIETRO RIGO

Proof of Theorem 12. Throughout this proof, if X satisfies conditions (1)-(2), we say that X  L and X has parameter (, K).

Fix n  1 and define the sequence

X(n) = Xi(n) : i  1 = Xn+i : i  1 .

Define also the random measure
n
Jn =  + K(Xi).
i=1
It suffices to show that, conditionally on Fn, one obtains

(7)

X(n)  L and X(n) has parameter (Jn, K) a.s.

In fact, under (7), Theorem 8 implies that µ  µn conditionally on Fn, namelly P (µ  · | Fn) = P (µn  · | Fn) a.s.

In turn, condition (7) follows directly from the definition. Define in fact

P Fn X (n)  B = P X (n)  B | Fn

for all B  B a.s.

Then,

P Fn X1(n)  ·

=P

Xn+1  · | Fn

=  +

n i=1

K (Xi )

n+

=

Jn n+

a.s.

and

P Fn Xm(n+) 1  · | X1(n), . . . , Xm(n) = P Xn+m+1  · | Fn+m

=



+

n+m i=1

K

(Xi

)

n+m+

=

Jn

+ (n

m i=1

K

+ ) +

Xi(n) m

a.s. for all m  1.

This concludes the proof.

4. Concluding remarks and examples

Obviously, the class L could be further enlarged. In this case, however, some of the

basic properties of L0 would be lost. As an example, suppose that

X1   and P Xn+1  · | Fn = cn  + (1 - cn)

n i=1

K

(Xi)

n

a.s.,

where the kernel K satisfies condition (2) and cn  [0, 1] is a constant. To make X closer to L0, suppose also that limn cn = 0. Then, X is exchangeable and X  L provided cn = /(n + ). Furthermore, various properties of L0 are preserved, including µ  j Vj K(Zj) where (Vj ) and (Zj) are independent sequences and (Zj) is i.i.d. with Z1  . Unlike Theorem 8, however, the probability distribution
of (Vj ) is unknown (to us). Similarly, we do not know whether some form of Theorem 12 is still valid.

Another issue is to be mentioned. Every X  L is a kernel based Dirichlet sequence, as defined in Subsection 1.1. Whether the converse is true, however, is an open problem. Precisely, the question is whether there is an exchangeable sequence satisfying condition (1) but not condition (2). Lemma 5 and Theorem 6 may be useful to address this question.

DIRICHLET SEQUENCES

15

Finally, we close the paper with two examples.

Example 13. (Example 1 continued). Let H  B be a countable partition of

S such that (H) > 0 for all H  H. Then, K(x) =  · | H(x) is a r.c.d. for 

given (H), where H(x) is the only H  H such that x  H. Therefore, X  L

provided X1   and

 + P Xn+1  · | Fn =

n i=1



· | H(Xi)

n+

a.s.

In this example, for each A  B, one obtains

µ(A) = lim P
n

Xn+1  A | Fn

=

µ(H) (A | H) a.s.

HH

where µ(H) a=.s. limn(1/n)

n i=1

1H

(Xi

).

To grasp further information about µ,

define

b(H) = Vj 1H (Zj),
j

H  H,

where (Vj) and (Zj) are independent, (Zj) is i.i.d. with Z1  , and (Vj ) has the stick breaking distribution with parameter . Then, Theorem 8 yields

Therefore,

µ  µ = b(H) (· | H).
HH
µ(H) : H  H  µ(H) : H  H = b(H) : H  H .

To evaluate the posterior distribution of µ, fix n  1 and take two sequences V (n) = Vj(n) : j  1 and Z(n) = Zj(n) : j  1 satisfying conditions (i)-(ii)-(iii). Recall that, by (iii), Z(n) is i.i.d. conditionally on Fn with

P (Z1(n)  · | Fn) = P Xn+1  · | Fn

a.s.

Define

bn(H) =

Vj(n) 1H Zj(n)

j

and µn =

bn(H) (· | H).

HH

Then, Theorem 12 implies µ  µn conditionally on Fn.

Example 14. Let · be the Euclidean norm on S = Rp. For t  0, let Ut  P be uniform on the spherical surface {x : x = t} (with U0 = 0) and



(A) =

Ut(A) e-t dt

0

for all A  B.

Then, K(x) = U x is a r.c.d. for  given ( · ). Hence, X  L whenever X1  

and

P Xn+1  · | Fn

 + =

n i=1

U

Xi

n+

a.s.

Theorem 10 applies to this example. To see this, first note that


x 2 (dx) =
0



x 2 Ut(dx) e-tdt =

t2 e-tdt < .

0

1P6ATRIZIA BERTI, EMANUELA DREASSI, FABRIZIO LEISEN, LUCA PRATELLI, AND PIETRO RIGO

Moreover, since Ut is invariant under rotations,

(8)

yi Ut(dy) = yi yj Ut(dy) = 0 and

yi2 Ut(dy) = t2/p

for all t, all i and all j = i. (Recall that yi denotes the i-th coordinate of a point y  Rp). Because of (8),

yi K(x)(dy) = yi U x (dy) = 0 for all x  Rp and i = 1, . . . , p.

Therefore, Theorem 10 yields

n
i=1

Xi

n

s-tably

Np(0, )

where  is the random covariance matrix with entries

ij =

yi

yj

µ(dy)

=

lim
n

1 n

n

yi yj U Xr (dy)

a.s.

r=1

It is even possible be more precise about . In fact, using (8) again, one obtains

ij = 0 for i = j and

ii

=

lim
n

1 n

n

r=1

yi2 U

Xr

(dy) =

1 p

lim
n

1 n

n

Xr

2=

1 p

r=1

x 2 µ(dx) a.s.

Hence, if I denotes the p × p identity matrix,

 = 11 I where 11 = (1/p) x 2 µ(dx).

Two last remarks are in order. First, in the notation of Theorem 8,

x 2 µ(dx) 

x 2 µ(dx) = Vj Zj 2.
j

Second, exploiting stable convergence and 11 > 0 a.s., one also obtains

p

n i=1

Xi

n i=1

Xi

2

= p

n-1/2 n-1

n i=1

Xi

n i=1

Xi

2

s-tably Np(0, I).

References
[1] Antoniak C. (1974) Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems, Ann. Statist., 2, 1152-1174.
[2] Bally V., Caramellino L. (2016) Asymptotic development for the CLT in total variation distance, Bernoulli, 22, 2442-2485.
[3] Berti P., Pratelli L., Rigo P. (2004) Limit theorems for a class of identically distributed random variables, Ann. Probab., 32, 2029-2052.
[4] Berti P., Rigo P. (2007) 0-1 laws for regular conditional distributions, Ann. Probab., 35, 649662.
[5] Berti P., Pratelli L., Rigo P. (2012) Limit theorems for empirical processes based on dependent data, Electronic J. Probab., 17, 1-18.
[6] Berti P., Pratelli L., Rigo P. (2013) Exchangeable sequences driven by an absolutely continuous random measure, Ann. Probab., 41, 2090-2102.
[7] Berti P., Dreassi E., Pratelli L., Rigo P. (2021) A class of models for Bayesian predictive inference, Bernoulli, 27, 702-726.
[8] Blackwell D., Mac Queen J.B. (1973) Ferguson distributions via Polya urn schemes, Ann. Statist., 1, 353-355.

DIRICHLET SEQUENCES

17

[9] Ewens W.J. (1972) The sampling theory of selectively neutral alleles, Theor. Popul. Biol., 3, 87-112.
[10] Favaro S., Lijoi A., Pruenster I. (2012) On the stick-breaking representation of normalized inverse Gaussian priors, Biometrika, 99, 663-674.
[11] Ferguson T.S. (1973) A Bayesian analysis of some nonparametric problems, Ann. Statist., 1, 209-230.
[12] Fortini S., Ladelli L., Regazzini E. (2000) Exchangeability, predictive distributions and parametric models, Sankhya A, 62, 86-109.
[13] Ghosal S., van der Vaart A. (2017) Fundamentals of nonparametric Bayesian inference, Cambridge University Press, Cambridge.
[14] Hahn P.R., Martin R., Walker S.G. (2018) On recursive Bayesian predictive distributions, J.A.S.A., 113, 1085-1093.
[15] Hansen B., Pitman J. (2000) Prediction rules for exchangeable sequences related to species sampling, Stat. Prob. Letters, 46, 251-256.
[16] Hjort N.L., Holmes C., Muller P., Walker S.G. (2010) Bayesian nonparametrics, Cambridge University Press, Cambridge.
[17] Lo A.Y. (1984) On a class of Bayesian nonparametric estimates: I. Density estimates, Ann. Statist., 12, 351-357.
[18] Pitman J. (1996) Some developments of the Blackwell-MacQueen urn scheme, Statistics, Probability and Game Theory, IMS Lect. Notes Mon. Series, 30, 245-267.
[19] Pitman J., Yor M. (1997) The two-parameter Poisson-Dirichlet distribution derived from a stable subordinator, Ann. Probab., 25, 855-900.
[20] Sethuraman J. (1994) A constructive definition of Dirichlet priors, Stat. Sinica, 4, 639-650.
Patrizia Berti, Dipartimento di Matematica Pura ed Applicata "G. Vitali", Universita` di Modena e Reggio-Emilia, via Campi 213/B, 41100 Modena, Italy Email address: patrizia.berti@unimore.it
Emanuela Dreassi, Dipartimento di Statistica, Informatica, Applicazioni, Universita` di Firenze, viale Morgagni 59, 50134 Firenze, Italy Email address: emanuela.dreassi@unifi.it
Fabrizio Leisen, School of Mathematical Sciences, University of Nottingham, University Park, Nottingham, NG7 2RD, UK Email address: fabrizio.leisen@gmail.com
Luca Pratelli, Accademia Navale, viale Italia 72, 57100 Livorno, Italy Email address: pratel@mail.dm.unipi.it
Pietro Rigo (corresponding author), Dipartimento di Scienze Statistiche "P. Fortunati", Universita` di Bologna, via delle Belle Arti 41, 40126 Bologna, Italy Email address: pietro.rigo@unibo.it

