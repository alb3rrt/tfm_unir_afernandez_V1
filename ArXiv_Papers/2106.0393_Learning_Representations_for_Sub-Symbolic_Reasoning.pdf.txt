arXiv:2106.00393v1 [cs.AI] 1 Jun 2021

Learning Representations for Sub-Symbolic Reasoning
Giuseppe Marra1, Michelangelo Diligenti2, Francesco Giannini2 and Marco Maggini2 1Department of Computer Science, KU Leuven, Leuven, Belgium
2Department of Information Engineering and Mathematical Sciences, Siena, Italy giuseppe.marra@kuleuven.be, {francesco.giannini, marco.maggini, michelangelo.diligenti}@unisi.it
Abstract
Neuro-symbolic methods integrate neural architectures, knowledge representation and reasoning. However, they have been struggling at both dealing with the intrinsic uncertainty of the observations and scaling to real world applications. This paper presents Relational Reasoning Networks (R2N), a novel end-to-end model that performs relational reasoning in the latent space of a deep learner architecture, where the representations of constants, ground atoms and their manipulations are learned in an integrated fashion. Unlike flat architectures like Knowledge Graph Embedders, which can only represent relations between entities, R2Ns define an additional computational structure, accounting for higher-level relations among the ground atoms. The considered relations can be explicitly known, like the ones defined by logic formulas, or defined as unconstrained correlations among groups of ground atoms. R2Ns can be applied to purely symbolic tasks or as a neurosymbolic platform to integrate learning and reasoning in heterogeneous problems with both symbolic and feature-based represented entities. The proposed model bridges the gap between previous neuro-symbolic methods that have been either limited in terms of scalability or expressivity. The proposed methodology is shown to achieve state-of-the-art results in different experimental settings.
1 Introduction
Enriching neural architectures with the ability of developing data representations in presence of relational knowledge is a major goal of neuro-symbolic methods. A powerful class of methodologies [Raedt et al., 2016] uses logic formulas as templates for probabilistic graphical models, which explicitly encode statistical dependencies among entities and their relationships. However, the application of these methods has been strongly limited by its computational complexity. An interesting growing trend [Wang et al., 2017] is represented by solutions focusing on reasoning over a set of objects represented via trainable embeddings, like Knowledge Graph Embeddings (KGEs). By implicitly encoding the statistical dependencies in latent spaces, these methods have been proved to scale to very large domains. However, a limitation of representation learning approaches is that they model statistical regularities among relations and/or entities but they fail to detect and exploit higher level of relational or logical knowledge, like formulas or programs.
In this paper, we introduce Relational Reasoning Networks (R2N), a class of models that exploits relational knowledge to produce meaningful embedded representations. By compiling the relations into the model structure, R2N jointly develops integrated representations for constants, ground atoms and ground knowledge by computing multiple sub-symbolic reasoning steps in a latent space. As automated reasoning derives new facts from known ones using multiple inference steps, R2N performs multiple sub-symbolic reasoning steps by manipulating atom embeddings into refined ones, accounting for the relational structure of the task. The learning of latent representations allows the
Preprint. Under review.

KGE

One-hop One-hop One-hop

Constants Embeddings

Atom Embeddings

Transformed Atom
Embeddings

Atom Predictions

Constants
Formulas Relations
Input Atom Embedding Layers

Reasoning Layers

Clique Predictions
Clique Embeddings
Output Layers

Figure 1: The overall structure of the model. a) The Input Atom Embedding Layer maps symbolic ground atoms into their embeddings, e.g. smokes(Alice)  Emb0s(A). b) The Reasoning layers recursively aggregate and manipulate these representations into refined or transformed atom representations; e.g. Emb0s(A)  Emb1s(A) · · ·  Embns(A). Reasoning layers exploit relational correlations between atoms, i.e. the cliques, defined in the Knowledge Base KB. The reasoning
block also computes representations for the cliques. c) Finally, the Output Layer uses these trans-
formed representations to make the final predictions.

model to select whether a given correlation (or explicit logic knowledge, if available) is useful for the specific task at hand. R2N maintains enough expressive power to perform relational reasoning under uncertainty, while allowing to scale up to large relational networks.
R2N inference mechanism is divided into three phases. In the first phase, symbolic ground atoms, e.g. smokes(Alice), are embedded in a latent space using representation learning techniques, e.g. KGE. In the second phase, the relations among atoms provided in the KB are used to subsymbolically aggregate and refine atom embeddings in a multi-layer fashion. This phase resembles multi-hop automated reasoning but in the latent space. Embedded representations of the related groups of atoms, i.e. cliques, are also computed during this phase. Finally, in the third phase, the final atoms and cliques embeddings are used to make predictions. These predictions will then be used to train the model end-to-end with respect to a supervised task. Figure 1 shows a high level representation of the model.
Contributions. The main contribution of the paper is the introduction of a neuro-symbolic architecture performing relational reasoning in a latent space. Distilling the relational structure at embedding level allows a more flexible representation to perform both learning and reasoning in a scalable way. Experimental results show the effectiveness of the approach achieving state-of-the-art results on different experimental settings. The proposed methodology is very general, as it can be used in relational tasks where the prior knowledge is explicitly available, even if noisy (e.g. predictions can deviate from the knowledge with a variable degree that must be co-learned), or where the relational structure is present but left latent. Last but not least, the methodology provides a general platform for neuro-symbolic integration [De Raedt et al., 2020], as it can be transparently applied to pure symbolic inputsor to cases where the inputs have a feature-based representation like images.
The outline of the paper is the following: Section 2 presents an overview of related works. Section 3 presents the model and Section 4 shows the experimental results on multiple datasets and learning tasks. Finally, some conclusion and remarks on future works are drawn in Section 5.
2 Related Work
Probabilistic reasoners define a more flexible inference process than their counterparts based on standard logic. For example, Markov Logic Networks (MLN) [Richardson and Domingos, 2006] translate a First-Order Logic (FOL) knowledge base into an undirected graphical model. Neuro-symbolic methods [De Raedt et al., 2020] bridge the reasoning capabilities of logic reasoners with the ability of sub-symbolic systems to deal with the feature-based representations that typically represent
2

sensorial inputs like video, images or sounds. In particular, neuro-symbolic distillation methods like Semantic-based Regularization [Diligenti et al., 2017] and Logic Tensor Networks [Donadello et al., 2017] inject logic knowledge into the network weights by enforcing the knowledge on the predictor outputs. Relational Reasoning Networks define a more flexible employment of the logic knowledge, since reasoning over latent representation learns how and where to apply the available knowledge. Another class of neuro-symbolic approaches, like Deep ProbLog [Manhaeve et al., 2018], Semantic Loss [Xu et al., 2018], Deep Logic Models (DLM) [Marra et al., 2019] and Relational Neural Machines (RNM) [Marra et al., 2020a] define directed or undirected graphical models to jointly train predicates approximated by deep learners and reasoning layers.Exact inference over the graphical models is generally intractable and these systems rely on heuristics to apply to any real world problem [Marra et al., 2020b]. Lifted Relational Neural Networks [Sourek et al., 2018] and Neural Theorem Provers (NTP) [Minervini et al., 2020] realize a soft forward or backward chaining via an end-to-end gradient-based scheme. NTP allows soft-unification among symbols using a tensorial representation to allow a more flexible matching. However, this introduces scalability issues as reasoning paths can not be easily pruned. Neural LP [Yang et al., 2017] learns rules using differentiable operators defined in TensorLog [Cohen et al., 2020]. Unlike this class of models, the proposed approach approximates relational inference using a forward step in a neural architecture and can scale up to much larger problems.
With the emergence of Knowledge Graphs (KG), representation learning has played a pivotal role in learning embedding spaces, which allow efficient inference. Knowledge Graph Embeddings (KGE), initially built from the statistical co-occurrences of constants and relations, have been later enriched with semantic information to improve embedding accuracy and explainability. As an example of this line of research, both Qu and Tang [2019] and Niu et al. [2020] inject logic knowledge into a KGE. However, both approaches require strong assumptions on the form of the distributions to make inference tractable. ExpressGNN [Zhang et al., 2020] attempts at overtaking the scalability limitations of the previous approaches using a Graph Neural Network (GNN) defined over the KG to embed the constants. Since the GNN has not enough expressive power, reasoning is performed via an external variational process. A similar relational structure is considered by Discriminative Gaifman models [Niepert, 2016], which translate FOL formulas into a set of features. Unlike this class of models, the proposed architecture directly compiles the relational structures required to approximate the reasoning process into the model architecture. Therefore, R2N can embed the constants and the atoms via the knowledge in the forward step of the network without employing an external inference step.
Another related class of approaches uses KGs for question answering by modeling multi-hop reasoning in latent space. For example, MINERVA [Das et al., 2018] learns to do query answering by first walking on a knowledge graph conditioned on a query. DeepPath [Xiong et al., 2017] uses reinforcement learning to find paths in a KG but, like Ren and Leskovec [2020], its applicability is limited to cases where the target entity is known in advance.
3 The Model
We assume a relational framework that can be described with a function-free First-Order Logic (FOL) language, including finite sets of constants (e.g. Alice, Bob) for specific domain entities, variables (e.g. x, y) for anonymous entities and n-ary predicates (e.g. the binary F atherOf (, )) for relations among constants. Given an n-ary predicate P and a tuple (t1, . . . , tn) such that, for every i, ti is a constant or a variable, then P (t1, . . . , tn) is called an atom (e.g. F atherOf (Bob, x)). If t1, . . . , tn are constants, P (t1, . . . , tn) is called a ground atom (e.g. F atherOf (Bob, Alice)). Given a FOL language, the set of all the expressible ground atoms is called Herbrand Base (HB).
Relational Reasoning Networks (R2N) make use of a relational knowledge KB on a certain HB, that can be expressed either as a semantically meaningful logic theory (e.g. {xyz F atherOf (x, y)  F atherOf (y, z)  Ancestor(x, z), . . . , }) or as a set of lists of atoms (e.g. {[F atherOf (x, y), Ancestor(x, y)], . . .}) that we only know, as a prior, to be correlated by some ignored relationship. In any case, the provided logic theory and/or lists of atoms are used as a template for building an R2N by grounding all the variables of the occurring predicates in their domain. In particular, the network structure is defined as a graph where, there is a clique for any tuple of ground atoms occurring in a certain ground formula (like it would happen for a MLN) or in a ground list in KB. Therefore, a clique of nodes is the basic relational structure defining the
3

a. Undirected Model

F(a, b)

L(a)

F(a, c)

b. Factor Graph

F(a, b)

L(a)

F(a, c)

c. Basic Reasoning Block
F(a, b) L(b) L(a) L(c) F(a, c) F(b, c)

L(b)

L(c)

L(b)

L(c)

Clique 1

Clique 2

Clique 3

F(b, c)

F(b, c)

F(a, b) L(b) L(a) L(c) F(a, c) F(b, c)

Figure 2: a. An undirected model representing the logic rule F (x, y)  L(x)  L(y), grounded for the constants {a, b, c}, where each clique is highlighted with a different colour. Please note that some cliques have been omitted to improve the readability of the figure. b. inference on the model can be performed via message passing on the factor graph. c. a reasoning block maps the atom embeddings into new refined embeddings using the same factorization dictated by the factor graph. In the picture, each connection has the colour of the clique it comes from.

correlation among connected nodes. A factor graph can be defined by replacing one factor for each clique in the graphical model, and connecting each node in the model to the factors for the cliques it is associated to. For example, the graph corresponding to xy F (x, y)  L(x)  L(y) with domain D = {a, b, c} is represented in Figure 2-a, where each clique is highlighted with a different color. The associated factor graph is shown in Figure 2-b.
The resulting architecture encodes the inference process by manipulating the latent representations of the atoms as a forward step within the model. The model is composed of three high-level components. The Input Atom Embedding Layer maps the input symbolic ground atoms (like F ather(Bob, Ann)) into input embeddings. Reasoning Layers (recursively) manipulate the input atom embeddings into transformed atom embeddings and, in turn, into clique embeddings using the relational structure of the factor graph. Output Layers use the last transformed atom embeddings to take decisions both at the level of the atoms or cliques. The next section will provide a detailed description of these components.
3.1 Input Atom Embedding Layer
Ground atoms are the fundamental objects of logic-based systems and their representation is a critical choice in determining the behaviour of the resulting reasoner. The proposed methodology can take as input either the Boolean values of the ground atoms, like in symbolic methods, or the atom representations in some latent space with semantics that are not a-priori defined. In the latter case, a vast range of representation learning approaches can be employed and co-trained. Particularly relevant to this paper are Knowledge Graph Embeddings and latent representations of a supervised multi-task classifier.
Knowledge Graph Embeddings. Knowledge graphs (KGs) are a representation of a portion of human knowledge, consisting of facts (ground atoms) in form of triples formed by two entities (constants) and a relation (predicate). A KG can be seen as a graph, where each entity is a node and each relation establishes an edge between the entities in the fact. For example, the fact F atherOf (Bob, Alice) links the entities Bob, Alice with an edge labeled by the relation F atherOf . KGs are incomplete and Knowledge Graph Embeddings (KGEs) are a powerful approach for populating KGs by mapping entities and relations to a latent representation, which generalizes the assignments to unknown facts. Let R(c1, c2) be a fact and e1, e2, WR be the trainable embeddings for c1, c2 and the relation R, respectively. KGE methods learn the entity and relation embeddings by defining scoring functions that are trained to match the supervisions. Some popular choices are: DistMult [Yang et al., 2015] computing the dot product among entity and relation embeddings < e1, e2, WR >, ComplEx [Trouillon et al., 2016] using the Hermitian dot product over complex embeddings to also model asymmetric relations: Re(< e1, e2, WR >), and Neural Tensor Networks [Socher et al., 2013] modelling asymmetric relations via a bilinear formulation: uTRf (eT1 WRe2 +VR[e1,e2]+b) with VR,uR,b additional weight vectors.
Latent Representations for Supervised Learning. A neural network used for supervised learning learns a representation of its input at the hidden layer. This allows to perform higher level reasoning on entities represented as feature vectors (like images, video, etc.) without any modification to the reasoner. Regardless of the specific selection of input embedded for a single atom, all the
4

Input Atom Embeddings

Aggregation
Clique MLPs

Transformed Atom Embeddings

Clique Embeddings

Previous Layer

One-hop reasoning Layer

Next Layer

Figure 3: Representation of a one-hop reasoning block, mapping the input atom embeddings into new embeddings accounting for the relational information. The blue and red rectangles indicate two separate cliques sharing one ground atom.

ground atoms in the HB are embedded, passed to reasoning layer and co-trained with the reasoning blocks.

3.2 Reasoning Block
This block takes a HB representation as input and provides a transformed HB representation as output, where each ground atom of the HB is represented using an n-dimensional embedding. In particular, the relational reasoning layer builds a bipartite graph, where each atom of the HB corresponds to one node in the input and one in the output layers. The reasoner links each atom in the output layer to the atoms in the input layer that are co-occurring in at least one clique with the considered atom, as shown in Figure 2-c. Therefore, one forward step in the architecture performs a one-to-one mapping of the latent representations of the atoms into a new set of representations updated with one inference step, where the latent representations of each clique nodes are transformed in a new set of embeddings that depend on the embeddings of the other elements in the clique. This mechanism can be iterated by stacking multiple blocks to perform multi-hop reasoning.
Let C(, g) be the clique corresponding to the relation  over the tuple of ground atoms g. All cliques of  have the same number of nodes |C()| as they are produced by grounding the same relation. The embedding Emb(C(, g))(i) of the clique C(, g) at the i-th reasoning block is computed as positional concatenation of embeddings of ground atoms participating to the clique:

Emb(C(, g))(i) = ConcatmC(,g) Emb(m)(i-1)

(1)

where Emb(m)(i-1) is the embedding for node m at the previous i - 1 block, while Emb(n)(0) is the initial representation of the atom n as computed by the Input Embedding Block.
The embedding Emb(n)(i) of an atom n by the i-th reasoning block is computed in two steps. First, the embedding Emb(n, C(, g))(i) of the atom is computed for each single clique containing it as:

Emb(n, C(, g))(i) =  W,pos(n) · Emb(C(, g))(i)

(2)

where  is an activation function, and the trainable weights W,i  W are shared across all nodes of the cliques that correspond to the groundings of the relation . The applied weights depend on the position of the incoming link pos(m) in the formula. For example, for the formula  = A(x, y)  B(x), pos(A(a, b)) = 1 and the embedding of the atom A(a, b) is processed by W,1, while pos(B(a)) = 2 and the embedding of B(a) is processed by W,2. Additional fully connected layers can be added on top of this basic processor to create a more complex clique level MLP. Since these basic reasoning layers can be stacked, a single layer MLP has been used in the experiments, as the architecture was showing sufficient computation power to perform the required reasoning in all the considered tasks.

5

Finally, the new representation of each atom is aggregated as mean of the embeddings computed from all the cliques containing it:

1

Emb(n)(i) = |(n)|

Emb(n, C(, g))(i)

(3)

C(,g)(n)

where (n) is the set of cliques where atom n occurs across all formulas. In particular, we note that:

· Equation 2 performs one reasoning step at the clique level by recomputing the embedding of a node based on its neighbours in the clique.
· Equation 3 performs an aggregation step by averaging the per-clique embeddings of the atoms that are replicated across multiple formulas and/or groundings to obtain the final representation of the atoms.

Figure 3 provides a graphical representation of how the presented basic reasoning block works. Since each node in a clique is connected to all the nodes in the transformed layer corresponding to the same clique, a clique establishes a number of connections equal to the square of its number of atoms |C()|. Because of full weight sharing among the cliques of a formula, the number of parameters of the model is equal to the sum over the formulas of the number of connections for one clique times the embedding size |E|, e.g.  |W| = |E| ·  |C()|2.
The model can employ logical knowledge to build the cliques, or general knowledge about some unknown correlation among group of nodes defining a higher relational structure among atoms. These latter relations are typically less specific than the ones coming from logic knowledge and, therefore, generate larger cliques requiring more weights, making this solution more prone to overfitting. However, general correlation relation can be employed even for learning tasks where no specific logic knowledge is available as shown in the experimental section.

3.3 Output Layers
One output layer takes the atom representations and outputs a single atom prediction for each ground atom. In our experiments, this has been implemented by a fully connected layer with sigmoidal activation.
In case of logic knowledge available for the learning task, a clique output layer is added to predict the truth values of logic rules for the considered cliques. This layer takes as input the concatenation of the atom representations of a clique and predicts the truth value of the corresponding ground formula, and it is used to inject the desired semantic into the network. This layer is trained by providing supervisions for ground formulas in the training set to force the network to compute the atom predictions with full awareness of the provided logic knowledge, instead of just learning generic atom correlations. The experimental results show that this semantic regularization helps model generalization. We will refer to a model trained with logic knowledge as R2NS, whereas we indicate with R2NC a R2N model exploiting generic correlations among groups of atoms.

3.4 Training

The overall architecture is trained to predict the supervised atom labels and, optionally, the truth value of each grounded formula. In the experimental results, KGE tasks are provided with positive examples, whereas the negative examples are obtained as corruptions of the positive patterns as commonly done in the literature. The negative softmax-loss was used for these experiments:







-f (x) + log 

exp (f (x )) ,

xX

x C(x)

where f is the function computed by the model and X is the training set of positive examples and C(x) is the set of corrupted patterns of x  X . The standard crossentropy loss was instead used for the other experiments and in the semantic loss: - (x,y)X y · log f (x), where the training set X is formed by input and desired output pairs.

The overall architecture of the model is illustrated in Figure 1 for the case of a KGE input block co-trained with the rest of the architecture.

6

Table 1: AUC-PR metric on the 3 tasks of the Countries dataset using different KGE and neural

reasoning systems, R2N indicates the relational reasoner network proposed in this paper. A bold

font indicates the best method for each task.

Task ComplEx ConvE DistMult NTP NTP- NeuralLP Minerva

R2N

S1

0.994 1.000 0.979 0.908 1.000 1.000

1.000 1.000±0.000

S2

0.879 0.990 0.692 0.874 0.930 0.751

0.924 0.998±0.001

S3

0.484 0.860 0.158 0.567 0.773 0.922

0.951 0.971±0.002

Limitations. While the scalability of the presented model is a huge step forward to classical Statistical Relational Learning methods, application on large domains can still be impractical when working on a fully grounded HB, as the number of possible ground atoms grows polynomially on the arity of the considered relations. For example a relation R(x, y) has a quadratic number of groundings with respect of the size of the domains of x, y. This issue is not specific to the presented methodology but it is common to all neuro-symbolic methods based on grounded logics, like LTN or SBR. Even if beyond the scope of this paper, automatic selection of a subset of the ground atoms within the learning process is an important line of research, currently explored by the authors.
4 Experiments
The experiments explore the capability of the model to perform reasoning in different contexts1. All the employed datasets do not include personal data. In Section 4.1, the methodology is tested in a symbolic reasoning task with explicit logic knowledge. In Section 4.2, the model is applied on top of KGEs to perform a relational reasoning tasks, when the relational knowledge is implicit and the model can exploit the higher-order correlations among groups of atoms. Finally, in Section 4.3 the model is used to perform symbolic/sub-symbolic integration, by learning to take collective decisions by reasoning over the output of a set of co-trained neural classifiers. The inference graphs have been pruned using standard techniques [Shavlik and Natarajan, 2009] to remove the portions having no effect on the returned predictions. Experiments have been executed on a machine running Linux with a Volta-V100 GPU, Xeon 2.1Ghz CPU and 48GB of RAM. The size of the embedded space has been selected by maximizing the target metric on the validation set for each experiments, using the grids reported in the provided code package.
4.1 Countries Dataset
The Countries dataset (ODbL licence) defines a set of countries, regions and sub-regions as basic entities. We used splits and setup from Rockta¨schel and Riedel [2017], which reports the basic statistics of the dataset and defines 3 tasks named S1, S2, S3, each requiring reasoning chains of increasing length. The task consists in predicting the unknown facts LocIn(country, continent), stating country location within a continent, given the evidence in form of country neighbourhoods and some known country/region locations. The model is provided with knowledge about the task like the hierarchical nature of the LocIn (located in) relation crkLocIn(c, r)  LocIn(r, k)  LocIn(c, k) and the manifold created across neighbouring countries: cc1kN eighOf (c, c1)  LocIn(c, k)  LocIn(c1, k). The first rule is always correct in the data, while the second is only partially correct and the probabilistic reasoning has to determine how and where to employ it. A R2NS model with 3 stacked reasoning blocks was trained for 300 epochs, Adam optimizer with initial learning rate equal to 0.01 and the reasoning embedding size was selected via a validation set in the [5, 50] range. Table 1 reports the area under the precision-recall curve (AUC-PR) metric and one standard error for the different reasoning tasks as an average over 5 different runs.
4.2 KGE datasets: Nations, Kinship, UMLS
The Nations, Kinship, and UMLS datasets [Kok and Domingos, 2007] (CC0 licence) are popular datasets for relational reasoning, where a set of triples (entity, relation, entity) express known true facts and the goal is to infer the unknown true facts. We used the setup and splits defined by Minervini et al. [2020], which also reports the basics statistics for the datasets. These tasks
1Code/data available at url_hidden_for_blind_review
7

Table 2: Results and training/inference times for KGE datasets. Missing values indicate values not

reported in the original papers, for which it was not possible to re-run the experiments. The best

model for each metric is shown in bold. Dataset Metric ComplEx DistMult NTP GTP CTP NeuralLP Minerva R2NC

Nations Hits@1

0.627

0.617 0.45 0.493 0.562

-

-

0.672

Hits@3

0.858

0.868 0.73 0.781 0.813

-

-

0.887

Hits@10 0.998

1.000 0.87 0.985 0.995

-

-

1.000

MRR

0.749

0.754 0.61 0.658 0.709

-

-

0.783

Time(s) 63/0.02 53/0.02 -

-

-

-

-

170/0.03

Kinship Hits@1

0.623

0.352 0.24 0.586 0.646 0.475

0.605

0.755

Hits@3

0.843

0.574 0.37 0.815 0.859 0.707

0.812

0.925

Hits@10 0.965

0.967 0.57 0.959 0.958 0.912

0.924

0.985

MRR

0.745

0.508 0.35 0.658 0.709 0.619

0.720

0.843

Time(s) 827/0.02 507/0.02 -

-

-

-

-

2442/0.04

UMLS Hits@1

0.877

0.341 0.70 0.761 0.752 0.643

0.728

0.885

Hits@3

0.987

0.542 0.88 0.947 0.947 0.869

0.900

0.987

Hits@10 0.998

0.736 0.95 0.983 0.984 0.962

0.968

0.998

MRR

0.923

0.479 0.80 0.857 0.852 0.778

0.825

0.929

Time(s) 109/0.02 125/0.03 -

-

-

-

-

877/0.12

can be addressed using KGE approaches, which learn the atom representations based on the constant and relation correlations over the true facts. No logic knowledge is explicitly provided for these datasets, therefore an R2NC model is instructed to correlate all predicates over each pair of constants: [P1(x, y), P2(x, y), . . . , Pn(x, y)]. The model exploits the correlations to correct and improve the KGE predictions via an implicit and latent reasoning process.
As a preliminary step, the different KGE methods have been tested for each dataset (ComplEx, DistMult and NTP), searching the embedding size maximizing the MRR metric on the validation set over a grid in the [10, 100] range. The best performing KGE was employed as first layer of the model and co-trained for 700 epochs with an initial learning rate equal to 0.01, Adam optimizer and reasoning embedding size selected using the validation set using the same grid in [10, 100]. Table 2 reports training/inference times and the standard metrics for this task, where the proposed methodology overperforms KGEs and state-of-the-art reasoning systems on all datasets. The model co-trains a KGE layer, therefore adding some time overhead. In spite of the extra reasoning, running times were in the same order of magnitude for all experiments. The results for the competitors have been extracted from Minervini et al. [2020].
4.3 Symbolic and Sub-Symbolic Integration: Cora
The Cora dataset (CC0 licence) defines a deduplication task which aims at reconciling small differences in paper citations. Each paper is associated to its author, title and venue attributes. We used the setup defined by Zhang et al. [2020], which measures the performance of the model in terms of the prediction accuracy of the predicates SameAuthor(author, author), SameT itle(title, title), SameV enue(venue, venue), detecting whether two entries refer to the same author, title and venue, respectively. Symbolic methods like ExpressGNN can process the logical knowledge and the textual information in titles/authors/venues is mapped to one binary feature to model the presence of each term in the corresponding text. A main limitation of this class of models is that they can not easily capture complex term correlations, required to construct powerful text similarity distances. Pure sub-symbolic models, like neural networks, can learn arbitrary complex functions to approximate the SameAuthor, SameT itle, SameV enue predicates. However, they can not easily model the relational dependencies among tasks and/or groundings.
The proposed model reasons over the latent representations generated by the underlying classifiers processing the textual information. Hence, they can simultaneously optimize the classifiers learning the text similarity predicates and the inference process defined by the available logic knowledge. Table 3 shows the logic knowledge used for this task, which is equivalent to the one used by ExpressGNN. The input text was represented by a bag-of-word representation and processed by an input MLP with 2 layers and 35 neurons per layer. The classifier and reasoning blocks have been co-trained for 400 epochs, with an initial learning rate equal to 0.01, Adam optimizer and reasoning embedding size equal to 35 (validated on the validation set). An R2NC model was also built by
8

Table 3: Logic knowledge used by presented model for the Cora dataset. SameP aper(x, y) is defined on the union of Author, Title and Venue domains and expresses that two entities x, y are related to a common paper in the known facts.
SameP aper(x, y)  SameAuthor(x, y), SameP aper(x, y)  SameT itle(x, y) SameP aper(x, y)  SameV enue(x, y), SameV enue(x, y)  SameV enue(y, x) SameAuthor(x, y)  SameAuthor(y, x), SameT itle(x, y)  SameT itle(y, x) SameP aper(x, y)SameP aper(y, z)  SameAuthor(x, z) SameP aper(x, y)SameP aper(y, z)  SameT itle(x, z) SameP aper(x, y)SameP aper(y, z)  SameV enue(x, z) SameAuthor(x, y)SameAuthor(y, z)SameAuthor(x, z) SameT itle(x, y)SameT itle(y, z)  SameT itle(x, z) SameV enue(x, y)SameV enue(y, z)  SameV enue(x, z)

Table 4: AUC-PR on test for the 5 folds and global average, training/inference mean running time for the Cora dataset for different tested models. A dash indicates a missing result not reported in the original paper. Statistically significant (95%) best results reported in bold.

Split S1 S2 S3 S4 S5 Avg Time(sec)

ExpressGNN -
0.640 -

MLP 0.830 ± 0.007 0.781 ± 0.007 0.843 ± 0.009 0.774 ± 0.011 0.838 ± 0.006 0.812 ± 0.007
120.2/0.8

R2NS 0.891±0.011 0.882±0.016 0.933±0.007 0.927±0.010 0.896±0.013 0.906±0.012
809.8/3.6

R2NC 0.851 ± 0.019 0.823 ± 0.008 0.882 ± 0.017 0.842 ± 0.016 0.808 ± 0.023 0.841 ± 0.037
1142/3.6

discarding the available logic knowledge by defining the cliques correlating all predicates over each pair of constants, as done for the experiment in Section 4.2.
Table 4 reports the average AUC-PR scores and 95% confidence error over 10 different runs for the five folds. The Avg split is the average of all the runs on all the folds. R2NS outperforms all the other baselines. ExpressGNN is separated by a large gap, which provides further evidence on the importance of exploiting low-level embedding functions to correlate the terms in each title, venue or author name. This is evident by also looking at the good performances of a simple MLP with no relational information. R2NS outperforms also the R2NC model, which however performs better than the MLP classifier. This is a confirmation of what previously discussed on the advantages of using more specific knowledge, when available. The accuracy of the clique predictions is 0.822 ± 0.031 (95% confidence error) for cliques of grounded formulas with at least one test atom, which means that the predictions of the network are consistent with the semantics of the provided formulas.
5 Conclusions
This paper presents a neural architecture to perform relational reasoning using latent representations. The model can be applied on top of different input feeds like KGEs or the embeddings computed by supervised classifiers. The presented model provides a flexible and expressive platform for neurosymbolic integration, which can be used either when the knowledge is explicitly available or when the form of the relational knowledge is not fully known. Future work will focus on improving the scalability by incrementally expanding the grounding set used during inference. Finally, we plan to employ explainability methods to extract new knowledge from the embedded representations.
References
William Cohen, Fan Yang, and Kathryn Mazaitis. Tensorlog: A probabilistic database implemented using deep-learning infrastructure. Journal of Artificial Intelligence Research, 67:285­325, 2020.
Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, Luke Vilnis, Ishan Durugkar, Akshay Krishnamurthy, Alex Smola, and Andrew McCallum. Go for a walk and arrive at the answer: Reasoning over paths in KBs using reinforcement learning. In ICLR, 2018.
9

Luc De Raedt, Sebastijan Dumancic´, Robin Manhaeve, and Giuseppe Marra. From statistical relational to neuro-symbolic ai. In IJCAI, 2020.
Michelangelo Diligenti, M. Gori, and C. Sacca. Semantic-based regularization for learning and inference. Artificial Intelligence, 244:143­165, 2017.
Ivan Donadello, Luciano. Serafini, and d'Avila Garcez. Logic tensor networks for semantic image interpretation. In IJCAI, 2017.
Stanley Kok and Pedro Domingos. Statistical predicate invention. In International Conference on Machine learning (ICML), 2007.
Robin Manhaeve, Sebastijan Dumancic, Angelika Kimmig, Thomas Demeester, and Luc De Raedt. Deepproblog: neural probabilistic logic programming. In NeurIPS, 2018.
Giuseppe Marra, Francesco Giannini, Michelangelo Diligenti, and Marco Gori. Integrating learning and reasoning with deep logic models. In ECML, 2019.
Giuseppe Marra, Francesco Giannini, Michelangelo Diligenti, Marco Gori, and Marco Maggini. Relational neural machines. In ECAI, 2020.
Giuseppe Marra, Francesco Giannini, Lapo Faggi, Michelangelo Diligenti, Marco Gori, and Marco Maggini. Inference in relational neural machines. In International Workshop on New Foundations for Human-Centered AI, volume 2659, pages 71­74, 2020.
Pasquale Minervini, Sebastian Riedel, Pontus Stenetorp, Edward Grefenstette, and Tim Rockta¨schel. Learning reasoning strategies in end-to-end differentiable proving. In ICML, 2020.
Mathias Niepert. Discriminative gaifman models. In NeurIPS, 2016.
Guanglin Niu, Yongfei Zhang, Bo Li, Peng Cui, Si Liu, Jingyang Li, and Xiaowei Zhang. Ruleguided compositional representation learning on knowledge graphs. In AAAI, pages 2950­2958, 2020.
Meng Qu and Jian Tang. Probabilistic logic neural networks for reasoning. In NeurIPS, volume 32, 2019.
Luc De Raedt, Kristian Kersting, Sriraam Natarajan, and David Poole. Statistical relational artificial intelligence: Logic, probability, and computation. Synthesis Lectures on Artificial Intelligence and Machine Learning, 10(2):1­189, 2016.
Hongyu Ren and Jure Leskovec. Beta embeddings for multi-hop logical reasoning in knowledge graphs. In NeurIPS, volume 33, 2020.
Matthew Richardson and Pedro Domingos. Markov logic networks. Machine learning, 62(1):107­ 136, 2006.
Tim Rockta¨schel and Sebastian Riedel. End-to-end differentiable proving. In NeurIPS, 2017.
Jude Shavlik and Sriraam Natarajan. Speeding up inference in markov logic networks by preprocessing to reduce the size of the resulting grounded network. In IJCAI, 2009.
Richard Socher, Danqi Chen, Christopher Manning, and Andrew Ng. Reasoning with neural tensor networks for knowledge base completion. In NeurIPS, 2013.
Gustav Sourek, Vojtech Aschenbrenner, Filip Zelezny, Steven Schockaert, and Ondrej Kuzelka. Lifted relational neural networks: Efficient learning of latent relational structures. Journal of Artificial Intelligence Research, 62:69­100, 2018.
The´o Trouillon, Johannes Welbl, Sebastian Riedel, Eric Gaussier, and Guillaume Bouchard. Complex embeddings for simple link prediction. In ICML, 2016.
Quan Wang, Zhendong Mao, Bin Wang, and Li Guo. Knowledge graph embedding: A survey of approaches and applications. IEEE Transactions on Knowledge and Data Engineering, 29(12):2724­2743, 2017.
10

Wenhan Xiong, Thien Hoang, and William Yang Wang. Deeppath: A reinforcement learning method for knowledge graph reasoning. arXiv preprint arXiv:1707.06690, 2017.
Jingyi Xu, Zilu Zhang, Tal Friedman, Yitao Liang, and Guy Van den Broeck. A semantic loss function for deep learning with symbolic knowledge. In ICML, 2018.
Bishan Yang, W. Yih, X. He, J. Gao, and Li Deng. Embedding entities and relations for learning and inference in knowledge bases. In ICLR, 2015.
Fan Yang, Zhilin Yang, and William W. Cohen. Differentiable learning of logical rules for knowledge base reasoning. In NeurIPS, 2017.
Yuyu Zhang, Xinshi Chen, Yuan Yang, Arun Ramamurthy, Bo Li, Yuan Qi, and Le Song. Efficient probabilistic logic reasoning with graph neural networks. In ICLR, 2020.
11

