Product Catalog A, Shoe Mart

Ember: No-Code Context Enrichment via ITEMID ID

A78

4

Color Black

Brand Asics

Sizes 13

Similarity-Based Keyless Joins Product Catalog B, Shoe Shoppe

Product Catalog C, Shoes "R" Us

Model

ID

Brand

ITEM ID

ITEM ID ID

Description

Base Table (aggregated user ratings) AVG(Rating) COUNT(USER) ITEM ID

Scorch Runner 3

Puma

A79

A81

3 For the first time the Pegasus li...

8.5/10

5

A80

Gel-Nimbus 21 9

Asics

A80

A82

4 The blue Asics Gel-Nimbus 22...

0

0

A82

Sahaana Suri, Ihab F. Ilyas, Christopher Ré, Theodoros RekatsinaGsiven Catalogs A, B, C

StanNofo-CrdodUe nCoivneterxstiEtyn,riUchnmievnetrsity of Waterloo, UW-Madison

Normalize USER A U888
Normalize USER U888

arXiv:2106.01501v1 [cs.DB] 2 Jun 2021

ABSTRACT

Structured data, or data suffer from fragmented

tchoanttaedxht:eInrinepsfuotrtomaaptiroen-ddeefisncerdibsinchgeamsai,ncgalne

entity can be scattered across mBausletipDlaetadatasets or tables tailored

for specific business needs, with no explicit linking keys (e.g., pri-

mary key-foreign key relationships or heuristic functions). Context

enrichment, or rebuilding fragmSeunpteervdiscioonntext, using keyless joins is an implicit or explicit step in machine learning (ML) pipelines over structured data sources. This process is tediousR, edcoommamine-nspdecific,

and lacks support in now-prevalent no-code ML systems that let

users create ML pipelines usingAjuuxsitliainrypDutatdaata and high-level config-

uration files. In response, we propose Ember, a system that abstracts

and automates keyless joins to generalize context enrichment. Our

key insight is that Ember can enable a general keyless join operator

by constructing an index populated with task-specific embeddings.

Ember learns these embeddings by leveraging Transformer-based

representation learning techniques. We describe our core archi-

tectural principles and operators when developing Ember, and

empirically demonstrate that Ember allows users to develop no-

code pipelines for five domains, including search, recommeAnudxialitairoynData and question answering, and can exceed alternatives by up to 39%

recall, with as little as a single line configuration change.

PVLDB Reference Format: Sahaana Suri, Ihab F. Ilyas, Christopher Ré, Theodoros Rekatsinas. Ember: No-Code Context Enrichment via Similarity-Based Keyless Joins. PVLDB, 15(1): XXX-XXX, 2022. doi:XX.XX/XXX.XX

1 INTRODUCTION
Machine learning (ML) systems that extract structural and semantic context from unstructured datasets have revolutionzed domains such as computer vision [34] and natural language processing [18, 49]. Unfortunately, applying these sytems to structured and semistructured data repositories that consist of datasets with pre-defined schemas is challenging as their context is often fragmented: they frequently scatter information regarding a data record across domainspecific datasets with unique schemas. For instance, in Figure 1B, information regarding Asics shoes is scattered across three catalogs with unique schemas. These datasets adhere to fixed schemas that are optimized for task-specific querying, and often lack explicit linking keys, such as primary key-foreign key (KFK) relationships.
This work is licensed under the Creative Commons BY-NC-ND 4.0 International License. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of this license. For any use beyond those covered by this license, obtain permission by emailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights licensed to the VLDB Endowment. Proceedings of the VLDB Endowment, Vol. 15, No. 1 ISSN 2150-8097. doi:XX.XX/XXX.XX

(A) Goal: Downstream ML

Normalized Ratings Table

USER U888 ???

ITEM A80 A82

Rating 8/10 ???

(B) Challenge: Info scattered across three catalogs

Proprietary Catalog A

ITEM A81 A82

Description The newest Pegasus... Blue Asics GT-1000 9...

ShoeShop, Ext Catalog B
ID Brand Model P5 Puma UltraRide P8 Asics GT-1000 8

Recommend

Predict

KFK Join.

Keyless Join.

ShoeMart, Ext Catalog C

ITEM A79 A80

Size 7 8

Make Puma Asics

Color Black Blue

(C) Existing Approach: Manual Joins (D)EMBER: replace & automate

ITEM is a joinable key, Make == Brand, Description 
{Make, Color}, {Brand, Model}
Domain-specific analyses to join A, B, C

Enriched Data

Downstream Tasks

Figure 1: An end-to-end task requiring context enrichment. EMBER
Predicting the rating of and recommending a new product
preprocessing
(A82), requires relating the Asics products (highlighted in
dark gray) via a keyless join (top). This process is manual
representation
due to data heterogeneity--we aim to automate it (bottom).
learning

Base Data
This constrains users to a single view of an entity that is specialized for a specific business need.
Associating these fragmented data contexts is critical in enabling ML-powered applications over such datasets--a process we denote as context enrichment--yet is a heavily manual endeavor due to task and dataset heterogeneity. Engineers develop solutions for context enrichment tailored to their task, such as similarity-based blocking in data integration [47], retriever models in question answering [67], or retrieval functions in search [53] (see Section 2). Constructing these independent solutions is repetitive, time-consuming, and results in a complicated landscape of overlapping, domain-specific methods. For instance, consider the scenario depicted in Figure 1:

joining

An e-commerce company has a proprietary product catalog, and aggregates product information from several external vendors to perform market analysis. Each vendor uses a unique product catalog, each with unique product representations (i.e., schema) ranging from free-form text to tabular product descriptions; products may overlap and evolve (Figure 1B). Given normalized tables containing user and rating data, an engineer wishes to estimate the rating for a candidate new product (A82) and identify users to recommend the product to (Figure 1A).

The engineer must first perform context enrichment by joining information across tables to extract features that capture similarities between the new (A82) and existing (A80, P8) products. They can then estimate the product rating, and recommend the new product to users based on how they rated related products.

Downstream

The classic data management approach is to denormalize datasets using KFK joins. This fails to solve the problem due to two reasons. First, not all tables can be joined when relying on only KFK relationships (e.g., there is no KFK relationship between Catalog B and Catalogs A or C). Second, even when KFK relationships exist, as between Catalogs A and C (ITEM), relying on only KFK semantics fails to capture the similarity between A82 and A80.
Alternatively, the engineer can rely on similarity-based join techniques, as in data blocking [47], built to avoid exhaustive, pairwise comparison of potentially joinable records. However, as we show in Section 8.3, the optimal choice of join operator to maximize recall of relevant records is task-dependent, and may not scale at query time when new records arrive. The engineer must first note that the Description column in Catalog A relates to the Brand and Model columns in Catalog B and the Size, Make, and Color columns in Catalog C. They can then select a custom join based on table properties: for matching primarily short, structured data records, they may want to join based on Jaccard similarity, whereas BM25 may be better suited for purely textual records (see Table 3). As database semantics do not natively support these keyless joins that require similarity-based indexing, joining arbitrary catalogs remains heavily manual--even large companies rely on vendors to categorize listings, which results in duplicate listings.1
To counter this manual process, we draw inspiration from recent no-code ML systems such as Ludwig [42], H20.ai [4], and Data Robot [3] that are rapidly shifting practitioners towards higher-level configuration-based abstractions for developing ML applications. Despite their success, these systems leave context enrichment as a user-performed data preparation step.2 In this paper, we evaluate how to bring no-code semantics to context enrichment.
No out-of-the-box query interface surfaces the relatedness between A80, A82, and P8 (all Asics, two GT-1000, and two blue), and links these records to the Ratings table with minimal intervention. The challenge in developing a no-code context enrichment system to enable this is to construct an architecture that is simultaneously:
(1) General: Applicable to a wide variety of tasks and domains. (2) Extensibile: Customizable for domain or downstream needs. (3) Low Effort: Usable with minimal configuration.
Our key insight to enable such a system is to simplify context enrichment by abstracting an interface for a new class of join: a learned keyless join that operates over record-level similarity. Just as traditional database joins provide an abstraction layer for combining structured data sources given KFK relationships, we formalize keyless joins as an abstraction layer for context enrichment.
We then propose Ember: a no-code context enrichment framework that implements a keyless join abstraction layer. Ember creates an index populated with task-specific embeddings that can be quickly retrieved at query time, and can operate over arbitrary semistructured datasets with unique but fixed schema. To provide generality, Ember relies on Transformers [58] as building blocks for embedding generation, as they have demonstrated success across textual, semi-structured, and structured workloads [18, 40, 49, 57, 64]. To provide extensibility, Ember is composed of a modular, three step architecture with configurable operators. To provide ease of
1 https://sell.amazon.com/sell.html 2 https://cloud.google.com/automl-tables/docs/prepare

Input
Base Data
Supervision
Auxiliary Data

EMBER
preprocessing
representation learning
joining

Output
Retrieve Related

Downstream Models

Models

Figure 2: Ember's interface for context enrichment. Ember
retrieves related auxiliary records for each base record.
use, Ember can be configured using a simple json-based configuration file, and provides a default configuration that works well across five tasks with a single line change per task.
As input, users provide Ember with: (1) a base data source, (2) an auxiliary data source, (3) a set of examples of related records across the sources. For each record in the base data source, Ember returns related records from the auxiliary data source as characterized by the examples, which can be post-processed for downstream tasks (e.g., concatenated or averaged). We present Ember's three-step modular architecture that enables index construction via Transformer-based embeddings to facilitate related record retrieval: preprocessing, representation learning, and joining (Figure 2).
Preprocessing. Ember transforms records from different data sources to a common representation, which allows us to apply the same methods across tasks. By default, Ember uses operators that convert each record to a natural language sentence for input to a Transformer-based encoder, which we optionally pretrain using self-supervision. We demonstrate that depending on the data vocabulary, encoder pretraining to bootstrap the pipeline with domain knowledge can increase recall of relevant records by up to 20.5%.
Representation Learning. Ember tunes the preprocessed representations to identify contextually similar records as defined by supervised examples. Learning this representation is task-dependent: we show that a natural approach of using pretrained Transformerbased embeddings with no fine-tuning performs up to three orders of magnitude worse than a fine-tuned approach, often returning less than 10% of relevant records. Pretraining in the first step can increase recall by over 30%, but this is still insufficient. Thus, Ember relies on operators that use supervised examples of related records and a contrastive triplet loss [60] to learn a representation that encourages similar records to be close while pushing away dissimilar records in the induced embedding space. We evaluate how many labeled examples Ember needs to match the performance of using all provided examples, and find that at times, 1% of the labels is sufficient, and that operators for hard negative sampling improve performance by up to 30% recall, further improving ease of use.
Joining. Ember quickly retrieves related records using the tuned representations to perform the keyless join. Ember populates an index with the embeddings learned to capture record similarity, and uses optimized maximum inner product search (MIPS) to identify

2

the nearest neighbors of a given record. This procedure allows Ember to capture one-to-one and one-to-many join semantics across a variety of downstream applications with low query-time overhead. We demonstrate that Ember meets or exceeds the performance of eight similarity-based join baselines including BM25 [53], Jaccard-similarity-based joins, Levenshtein distance-based joins, and AutoFuzzyJoin [37] with respect to recall and query runtime.

We represent each dataset  as an ni ×di matrix of ni data points (or records) and di columns. We denote  as the  column in dataset . We denote   as the  row in dataset . Columns are
numeric or textual, and we refer to the domain of   as Di. Our goal is to enrich, or identify all context for, each 0 in 0:
auxiliary records  (  0) that are related to 0 , as per task  .

Although conventional wisdom in data management often says to rely on domain-specific pipelines for context enrichment, we introduce no-code ML semantics to context enrichment and empirically show how a single system can generalize to heterogeneous downstream tasks. We report on our experiences in deploying Ember across five tasks: fuzzy joining, entity matching, question answering, search, and recommendation. We demonstrate that Ember generalizes to each, requires as little as a single line configuration change, and extends to downstream similarity-based analyses for

2.2 Motivating Applications
Context enrichment is a key component for applications across a broad array of domains. Thus, a context enrichment system would reduce the developer time and effort needed to construct domainspecific pipelines. We now describe a subset of these domains, which we evaluate in Section 8. Additional applicable domains include entity linkage or disambiguation [46, 55], nearest neighbor machine translation [28], and nearest neighbor language modeling [29].

search, entity matching, and recommendation as in Figure 1. In summary, we present the following contributions in this paper:

Entity Matching and Deduplication. Entity matching identifies data points that refer to the same real-world entity across two

· We propose keyless joins with a join specification to serve as an abstraction layer for context enrichment. To our knowledge, this is the first work to generalize similarity-based tasks across data integration, natural language processing, search, and recommendation as a data management problem.
· We design and develop Ember, the first no-code framework for context enrichment that implements keyless joins, and provides an API for extending and optimizing keyless joins.
· We empirically demonstrate that Ember generalizes to five workloads by meeting or exceeding the recall of baselines while using 6 or fewer configuration line changes, and evaluate the modular design of Ember's default architecture.

different collections of entity mentions [33]. An entity denotes any distinct real-world object such as a person or organization, while its entity mention is a reference to this entity in a structured dataset or a text span. Entity deduplication identifies entities that are found multiple times in a single data source, thus is a special case of entity matching where both collections of entity mentions are the same.
We frame entity matching as a context enrichment problem where 0 and 1 represent the two collections of entity mentions. Context enrichment aligns entities 1 in the auxiliary dataset 1 with each entity 0 in the base dataset 0. In entity deduplication, 0 = 1. The downstream task  is a binary classifier over the retrieved relevant records, as the aim is to identify matching entities.
Fuzzy Joining. A fuzzy join identifies data point pairs that are

2 CONTEXT ENRICHMENT
In this section, we define context enrichment, and provide example workloads that can be framed as a form of context enrichment.

similar to one another across two database tables, where similar records may be identified with respect to a similarity function (e.g., cosine similarity, Jaccard similarity, edit distance) and threshold [15, 59] defined over a subset of key columns. Though related to entity

matching, fuzzy joins can be viewed as a primitive used to efficiently

2.1 Problem Statement

mine and block pairs that are similar across the two data sources.

Structured data, or data that adheres to a fixed schema formatted in rows and columns, suffers from context fragmentation. We broadly define structured data to include semi-structured and textual data sources that may be stored and keyed in data management systems, such as content from Wikipedia articles or web search results. Unlike unstructured data, structured data follows a scattered format that is efficient to store, query, and manipulate for specific business needs. This may be in the form of an e-commerce company storing datasets cataloging products and user reviews as in Figure 1. To use this fragmented data for ML tasks, practitioners implicitly or explicitly join these dataset to construct discriminative features.
We refer to this joining process as context enrichment, which we define as follows: Given a base dataset in tabular form, 0, context enrichment aligns 0 with context derived from auxiliary data sources,  = {1, ...,  }, to solve a task  . We focus on a text-based, two-dataset case (| | = 1), but propose means for multi-

We frame fuzzy joining as a context enrichment problem where 0 and 1 represent the two database tables. We let 0 and 1 denote the set of key columns used for joining. Records 0 and 1 are joined if their values in columns 0 and 1 are similar, to some threshold quantity. This is equivalent to a generic context enrichment task with a limited number of features used. The downstream task  is a top-k query, as the aim is to identify similar entities.
Recommendation. A recommender system predicts the rating that a user would give an item, typically for use in content recommendation or filtering [41]. We also consider the broader problem of uncovering the global rating for an item, as in the e-commerce example in Figure 1, as being a form of recommendation problem-- namely, the global score may be predicted by aggregating the predicted per-user results obtained via classic recommender systems.
We frame such recommendation workloads as a context enrichment

dataset extension Sections 5-7, and defer their evaluation to future work. We explore context enrichment in regimes with abundant

problem where 0 represents all information regarding the entity to be ranked (e.g., a base product table, or new candidate products), and

and limited labeled relevant pairs  to learn source alignment.

1 represents the information of the rankers, or other auxiliary data

3

(e.g., catalogs of previously ranked products). Context enrichment
aligns rankers 1 in the auxiliary dataset 1 with those that are related to each entity to be ranked 0 in the base dataset 0. The downstream task  is to return the top-1 query, or to perform
aggregation or train an ML model over the returned top-k entries.

base_table_ref [ join_type ] "KEYLESS JOIN" aux_table_ref "LEFT SIZE" integer "RIGHT SIZE" integer "USING" supervision ;
join_type = "INNER" | "LEFT" | "RIGHT" | "FULL" ;

Search. An enterprise search engine returns information relevant to a user query issued over internal databases, documentation, files, or web pages [26, 44]. A general web retrieval system displays information relevant to a given search query [12, 21]. Both rely on information retrieval techniques and relevance-based ranking over an underlying document corpus as building blocks to develop complex, personalized pipelines, or for retrospective analyses [10].
We frame retrieval and ranking in search as a context enrichment problem where 0 represents the set of user queries, and 1 represents the underlying document corpus. Context enrichment aligns documents 1 in the auxiliary dataset 1 that map to each query 0 in the base dataset 0. The downstream task  is to return the top-k documents for each query, sorted by their query relatedness.
Question Answering. Question answering systems answer natural language questions given a corresponding passage (e.g., in reading comprehension tasks) or existing knowledge source (e.g., in open domain question answering) [50, 52]. Multi-hop question answering generalizes this setting, where the system must traverse a series of passages to uncover the answer [63]. For instance, the question "what color is the ocean?" may be provided the statements "the sky is blue" and "the ocean is the same color as the sky."
We frame the retriever component [67] of a classic retriever-reader model in open domain question answering as a context enrichment problem. Our task is to identify the candidate text spans in the provided passages that contain the question's answer (the retriever component), and a downstream reader task can formulate the answer to the question. We let dataset 0 represent the set of questions, and 1 represent the provided passages or knowledge source, split into spans (e.g., based on sentences, or fixed word windows). Context enrichment aligns spans 1 in the auxiliary dataset 1 that contain the answer to each question 0 in the base dataset 0. Multi-hop question answering requires an extra round of context enrichment for each hop required. Each round selects a different passage's spans as the base table 0; to identify context relevant to the question, the user can traverse the related spans extracted from each passage. The downstream task  is to learn a reader model to answer questions using the enriched data sources.

Listing 1: Keyless join specification (inner join default).

3.1 Keyless Joins: Definition and Objective

Context enrichment requires a similarity-based join that operates

at the record-, not schema-, level to retrieve related records without

primary key-foreign key (KFK) relationships. We propose learned

keyless joins as an abstraction to enable this functionality. The goal

of a keyless join is to quantify the relatedness of records across

different datasets to identify records referring to similar entities.

A keyless join must learn a common embedding space X for all

records   across datasets 0   that reflects entity similarity. For each Di, a keyless join learns a function  : Di  X that maps elements of  to the space X. We denote each transformed data point

 (  ) as   . This mapping must be optimized such that related val-

ues map to similar feature vectors in X and unrelated values map to

distant feature vectors in X: (  ,  ) > (  , ) implies

that

the




entity

in

the




dataset

is

more

closely

related

to

the




entity in the  dataset than the  entity in the  dataset. For in-

stance, if we define similarity with respect to an  -norm, the above

condition is equal to optimizing for   -   <   -   .

3.2 Join Specification for Context Enrichment
We propose a minimal join specification for our applications using keyless joins as a building block (Listing 1). Given a pair of data sources to join (base_table_ref, aux_table_ref), and examples of similar records across them (supervision), users first specify the join type: an inner join to only return enriched records, or outer join (left, right, full) to return enriched and unenriched records. Users then specify the join size: how many records from one data source joins to a single record in the other data source to indicate one-to-one, one-to-many, or many-to-many semantics.
As output, joined tuples (matches) between the two tables are the most similar records across them as learned using the keyless join objective. Results are ranked in order of greatest entity similarity, and the top k results are returned based on join size.
For instance, an entity matching application is written as follows to retrieve a single matching record between each data source:

3 LEARNED KEYLESS JOINS FOR CONTEXT

entity_mentions_A INNER KEYLESS JOIN entity_mentions_B LEFT SIZE 1 RIGHT SIZE 1 USING matching_mentions;

ENRICHMENT
Although the applications in Section 2 implicitly or explicitly perform the same context enrichment, many state-of-the-art systems for these tasks re-implement and rediscover primitives across domains spanning machine learning, databases, information retrieval, and natural language processing [29, 30, 38, 56]. We propose learned keyless joins as an abstraction to unify context enrichment across these tasks and provide a vehicle to develop a no-code system with out-of-the-box recall exceeding naïve baselines. In this section, we define keyless joins, provide a keyless join API specification, and introduce Transformer models that enable general keyless joins.

A search application is written as follows to retrieve 10 documents for each search query, else return the unenriched query:
query_corpus LEFT KEYLESS JOIN document_corpus LEFT SIZE 1 RIGHT SIZE 10 USING relevant_docs_for_query;
A recommendation application is written as follows to retrieve 10 items for each user, and 20 users who like each item:
user_database INNER KEYLESS JOIN product_database LEFT SIZE 20 RIGHT SIZE 10 USING relevant_docs_for_query;
In the remainder of paper, we describe our prototype system that implements this keyless join abstraction layer for enrichment.

4

Training Data Curation (B; §4)

Input Data D0 (Base)

Preprocessing (A; §5)

Data Preparing (A.1)

Optional Self-Supervised Pretraining (A.2)

encoder

D1 (Auxiliary)

dij (record) Brand = Nike;
Model = Winflo

preparer's sentence
sij = "[CLS] Brand Nike [SEP] Model Winflo [SEP]"

Self-Supervision BM25-based retrieval, embedding similarity, etc.

Pretraining Masked Language
Modeling

Representation Learning (B; §6)

Fully-Supervised Contrastive Learning

anchor

+ positive + - negative -

encoder

anchor

+ positive + - negative -

anchor

+ positive + - negative -

anchor

+ positive + - negative -

Fine-tune encoder using input supervision

as a labeler (e.g. [sij , skl ]  positive) and sampler to find hard negatives.

+ +
-- -
loss Triplet loss

Joining (C; §7)

Indexing, Query Retrieval, Processing

query left join

left size: 1

index

right size: 3

embeddings retrieved based on join size and type
represented via retriever

Figure 3: Ember system architecture.

Data Types
record sentence embedding

Dict[str, Union[str, int, float]] str List[float]

Operators preparer labeler sampler loss
encoder retriever

Step

record  sentence

1

Tuple[record, record]  bool

1, 2

List[Generic[T]]  Generic[T]

2

Tuple[emb., Union[emb.,

1, 2

Tuple[emb., emb.]]]  float

sentence  embedding

1, 2

List[emb.]  List[Tuple[record, emb.]] 3

Table 1: Ember data types and operators used in each step

3.3 Background: Transformer-Based Encoders
Our main tools for creating a representation optimized for the objective in Section 3.1 are Transformers [58], as they have demonstrated success across a range of structured, semi-structured, and unstructured domains [18, 40, 49, 57, 64]. They consist of an encoderdecoder architecture, where the Transformer first encodes the input to an intermediate representation, and then decodes this representation to produce the end output. Stacking encoder (e.g., BERT [18]) or decoder (e.g., GPT-2 [49]) modules allows us to learn high-quality word embeddings that can either be used for a wide array of downstream applications. We focus on BERT-based embeddings.
BERT embeddings used for a downstream task are trained in a two-step procedure. The first step is self-supervised pretraining using a masked language model (MLM) objective: a random subset of tokens are masked, and the model must predict each masked token given the surrounding text as context. Pretraining is performed with general purpose corpora such as Wikipedia. The second step is task-specific fine-tuning. Additional layers are appended to the final layer of the pretrained model based on the downstream task, and all model parameters are updated given small amounts of downstream supervision. Rather than relying solely on fine-tuning, an additional MLM pretraining step can be performed prior to fine-tuning to introduce additional domain knowledge.
4 EMBER
We develop Ember,3 an open-source system for no-code context enrichment. Ember implements a keyless join abstraction layer that meets the specification in Section 3.2. Ember first represents input
3 https://github.com/sahaana/ember

records using transformer-based embeddings directly optimized for the condition in Section 3.1. Ember then populates a reusable index with these embeddings based on the join type, and configures index retrieval based on the join sizes. We now provide an overview of Ember's usage, API and architecture; data types and operators are in Table 1, with an architecture overview in Figure 3.
4.1 Usage
As input, Ember requires the join specification parameters: a base data source 0 ("left"), auxiliary data sources  = {1, ...,  } ("right"), labeled examples of related data points that represent similar entities, join type, and join sizes. Labeled examples are provided in one of two supervision forms: pairs of related records, one from each table, or a triple where a record from 0 is linked to a related and unrelated record from each  . That is, unrelated examples are optional, but related ones are required. Recall that we focus on the  = 1 case, but describe extensions in Sections 5-7.
As output, Ember retrieves the data points enriched based on the join type and join sizes as a list of tuples. Users configure Ember using a json configuration file that exposes the join specification and lower-level Ember-specific parameters (see Section 8.5).
4.2 API and Architecture
Keyless joins learn a representation that quantifies data point relatedness, and retrieve these points regardless of the input schema. We propose a modular system with three architectural elements to enable this: data preprocessing, representation learning, and data joining. Ember consists of dataflow operators that transform inputs into the formats required for each step (see Table 1).
Ember represents input data records   as records in keyvalue pairs. Supervision is represented via labelers. An Ember pipeline consists of preparers, encoders, samplers, losses, and retrievers. Ember uses preparers to transform records into sentences. Ember uses encoders to transform sentences into embeddings that are optimized per a provided loss; samplers mine negative examples if the provided supervision only provides examples of related records. The trained embeddings are stored in an index, and are retrieved using a retriever. We provide an API over these operators in the form of a customizable, high-level configuration, as described in Section 8.6. To enable functionality beyond that described, users must implement additional preparers, encoders, samplers, or retrievers. Users configure Ember using pre-defined or custom operators, or use the following pre-configured default:

5

USER U888 U993

Normalized metadata table: nile.com

AGE 42 38

GENDER F F

STATE HI HI

Base Table

Preprocessing (Figure 3A, §5).

Ember ingests any text or nu- AvgRating 7.5/10

ITEM ID A80

meric data source with a pre-defined???schemaA82, and converts its

records to a common representation. By default, Ember converts

records to sentences using sentence preparer modules, which

are fed as input to the pipeline's encoders. Ember optionally pre-

trains the encoders in this step using self-supervision.

ITEM A82

Product Catalog A Description
Blue Asics GT-1000 9 now...

Product Catalog B

ID

Brand

Model

P8

Asics

GT-1000 8

sentence preparer

Pretrain encoder

sentence A: "Description Blue Asics GT-1000..." MLM: "[CLS] Description Blue Asics GT-1000 9 now sentence B: "Brand Asics [SEP] Model GT-1000 8" [SEP] Brand Asics [SEP] Model [MASK] 8 [SEP] "

Representation Learning (Figure 3B, §6). Ember learns a mapping for each input data source's records such that the transformed data is clustered by relatedness. To learn this mapping, Ember's encoders are fine-tuned with the input supervision (in the form of a labeler) and loss. Ember applies the learned mapping to each of the sentences to generate embeddings passed to the final step.

Joining (Figure 3C §7). Ember populates an index with the NormalizedUserRatings learned

USER

ITEM ID

RATING

embeddings

using

Faiss

[27].

A

keyless

join

canU888

be

completed A80

7/10

by

is-

U993

A80

8/10

suing a similarity search query given an input record (transformed

to an embedding) against this index. The dataset that is indexed

is determined by the join type. Ember uses a k-NN retriever

module to retrieve as many records specified by the join sizes.

5 PREPROCESSING
In this section, we describe the first step of Ember's pipeline: preprocessing. Users provide base and auxiliary datasets with examples of related records across the two. Ember first processes the input datasets so all records are represented in a comparable format. Ember then pretrains the encoders for the representation learning step. We describe these phases, and then multi-dataset extension.
Data Preparing (Figure 3A.1). This phase converts each record   into a format that can be used for downstream Transformerbased encoders regardless of schema. Motivated by recent work in representing structured data as sentences [57, 64], Ember's default pipeline uses a sentence preparer  to convert input records   into sentences  (  ) =   . Each key-value pair in   is converted to a string and separated by the encoder's separator token as "key_1 value_1 [SEP] key_2 value_2 [SEP]..." (Figure 4).
Optional Pretraining (Figure 3A.2). Our default BERT-based encoder is trained over natural language corpora (Wikipedia and BookCorpus [8, 68]). Structured data is often domain-specific, and is of a different distribution than natural language. Thus, we find that bootstrapping the pipeline's encoders via additional pretraining can improve performance by up to 2.08×. Ember provides a pretraining configuration option that is enabled by default.
Users can pretrain the pipeline's encoders via BM25-based selfsupervision (out-of-the-box) or by developing weak supervisionbased labelers (custom-built). We add a standard Masked Language Modeling (MLM) head to each pipeline encoder, and pretrain it following the original BERT pretraining procedure that fits a reconstruction loss. To encourage the spread of contextual information across the two tables, we concatenate one sentence from each table to one other as pretraining input as "  [SEP]  " (Figure 4). We select sentence pairs that are likely to share information in an unsupervised manner via BM25, a bag-of-words relevance ranking function [53], though any domain-specific unsupervised similarity join can be used, such as from Magellan [32] or AutoFJ [37]. We evaluate BM25-based MLM pretraining in Section 8.4. During development, we explored learning conditional representations of each

Figure 4: Examples of the data preparing and pretraining phases of the preprocessing architecture step.

table given the other by selectively masking one table's sentences, USER

Normalized Metadata Table

AGE

GENDER

STATE

U888

42

F

HI

but

found

no

change

in

performance

compared

to

uniform U993

38

maskF ing.

HI

Multi-Dataset Extension. Depending on the encoder configuration used in the multi-dataset case, the BM25-based pretraining step will be applied to all sentences across all datasets at once (anchored from the base table), or each dataset pairwise.

6 REPRESENTATION LEARNING
In this section, we describe the second step of Ember's pipeline: representation learning. Given the sentences and optionally pretrained encoders from the first step, Ember fine-tunes the encoders such that embeddings from related records are close in embedding space. These embeddings are then passed to the final pipeline step. Users can choose how many encoders to configure, the encoder architecture and output dimension, and how to train the encoders, which we now detail prior to describing multi-dataset extension.
Encoder configuration. Ember provides a choice of training independent encoders,  for each data source, or using a single encoder 0 common to all data sources. In all of our scenarios, using a single, common encoder performs best, sometimes by over an order of magnitude, and is set as Ember's default configuration. In our tasks, we found that using separate encoders perturbs the representations such that exact matches no longer share a representation--thus, the encoders must relearn correspondences, and often fail to. However, to extend Ember to non-textual data (e.g., images, video, text-image for image captioning or joint sentiment analysis, or text-audio tasks), users must leverage source-specific encoders--thus, we provide them as a configurable option.
Encoder architecture. Ember lets users configure each encoder's base architecture and the size of the learned embeddings. Users can choose BERTbase or DistilBERTbase as the core architecture; Ember's default is the 40% smaller, and 60% faster DistilBERT model. We use models from HuggingFace Transformers [61]; thus, integrating new architectures requires few additional lines of code.
We remove the MLM head used in the preprocessing step for optional pretraining, and replace it with a fully connected layer that transforms BERT's default 768-dimensional output to a userspecified embedding output dimension, . The output of the fully connected layer is a -dimensional embedding for each input token. Users can choose one of two types of aggregation methods that will return a single -dimensional output per input sentence: averaging all of the embeddings, or using the embedding of the leading CLS token that BERT appends to its input data (Figure 4). Ember defaults to CLS-based aggregation with a 200-dimensional output.
Encoder training. The encoders' goal is to learn a representation (embedding), for the data sources such that sentences that refer to

6

similar entities are grouped in the underlying embedding space. Recall that to perform a keyless join under an  -norm, each encoder must learn a function that maps elements of  to the space X, such that   -   <   -   when the  entity in the 
dataset is more closely related to the  entity in the  dataset than the  entity in the  dataset. We directly optimize for this objective function by training encoders using a contrastive, triplet loss together with user-provided supervision.
Given an anchor record 0a from 0, and records 1p and 1n in 1 that are related and unrelated to 0a, respectively, let 0 (0a) = 0a, 1 (1p) = 1p, and 1 (1n) = 1n be their embeddings following the composition of the sentence preparer  and encoder  operators ( =   ). We minimize the triplet loss, defined as:
L (0a, 1p, 1n) = max{0a - 1p , 0a - 1n  + , 0}
where  is a hyperparameter controlling the margin between related and unrelated embeddings. Users could use an alternative loss function: a cosine similarity loss would provide representations that similarly encourage related points to be close and unrelated points to be far, and a standard binary cross-entropy loss may be used as well, where the final layer in the trained encoder would be the end embeddings. However, they do not explicitly optimize for relative distances between related and unrelated records.
Users often have examples of related pairs, but no unrelated examples to form triples. In our tasks, only the search workload provides triples, while the rest provide lists of related pairs, which we represent with a labeler. As a result, we provide negative sampler operators to convert labelers that operate over pairs to the triples required by the triplet loss. If the user-provided labeler already contains a related and unrelated example for a record, we form a triple using these examples. If a record does not contain a related and unrelated example, for each record 0 in 0 with a labeled related record, we use either a random sampler or a stratified hard negative sampler. The random sampler selects a record at random (that is not a supervised related pair) as an example unrelated record. The stratified sampler provides tiers for different degrees of relatedness for the negative examples--a user can specify a region of hard examples to sample from, given prior domain knowledge. We provide a default stratified sampler with a single tier of relatedness defined using BM25 or Jaccard similarity, though any unsupervised join method can be used (including AutoFJ [37]). We show that a hard negative sampler improves recall by up to 30% compared to a random sampler in Section 8.4.
Multi-Dataset Extension. The encoder configuration and architecture are unchanged in the multi-dataset case. There are two options for the encoder training procedure. The first scenario follows the context enrichment problem definition--where a base table 0 must be augmented with several auxiliary data sources . In this case, each encoder can be trained pairwise with 0 and  as described above. In the case where multiple data sources must be aligned in sequence, a user can successively apply Ember over a data DAG created using their prior knowledge--effectively creating several, sequential context enrichment problems as described in Section 2.2 for multi-hop question answering. Else, Ember can induce a DAG and sequentially traverse the space of possible keyless joins based on the cardinality of each dataset, in ascending order.

7 JOINING
In this section, we describe the last step of Ember's pipeline: joining. Given the embeddings output by the trained encoders, Ember executes the keyless join by identifying related points across the input datasets, and processing them for downstream use. Ember indexes the learned embeddings and queries this index to find candidate related records. The join sizes determine the number of returned results per query, and we also allow users to configure a similarity threshold for each candidate match. We now detail the indexing and retrieval procedure, post-processing, and multi-dataset extension.
Indexing and Query Retrieval. Given two records, Ember computes the similarity between their embeddings to determine if the records are related. Many traditional solutions to our motivating applications perform such pairwise checks across all possible pairs either naïvely or with blocking [43, 45, 47] to identify related records. However, the choice of operator is both domain-specific, and scales quadratically at query time with the size of the input datasets and blocking mechanisms used (see Section 8.3). We eliminate the need for pairwise checking by indexing our embeddings, which are optimized for clustering related records, and rely on efficient libraries for maximum inner product search (MIPS) [27].
For a LEFT or RIGHT OUTER JOIN, Ember constructs an index over the base (0) or auxiliary (1) datasets, respectively. Ember then queries the index with each embedding of the remaining dataset, and returns the record and embedding corresponding to the most similar records using a retriever operator. For a FULL OUTER or INNER JOIN, Ember may jointly index and query both datasets to identify related entries in either direction. By default, Ember only indexes the larger dataset to reduce runtime--an optimization we evaluate in Section 8.4 that improves query runtime by up to 2.81×. Ember's default configuration is an INNER JOIN.
Post-Processing. The user-provided join size configures the retriever to return the top- records with the closest embeddings in the indexed dataset. Ember additionally supports threshold-based retrieval. The former is useful for applications such as search, where a downstream task may be to display the top- search results to a user in sorted order. The latter is useful for applications where there may not be related records for each record in the base dataset. Ember's default retriever is configured for a 1-to-10 join.
If the task  is not to simply return related records, users can construct pipelines on top of Ember's outputs, relying on keylessjoin-based context enrichment as a key primitive. Examples of such workloads include the recommendation example from Figure 1, which we simulate as a case study in Section 8.5, open domain question answering, data augmentation, or applications that involve humans in the loop to verify matches or drive business needs.
Multi-Dataset Extension. The joining step may vary in the multidataset case in two ways. In a first scenario, each auxiliary data source  is indexed, and the query retrieval phase will query each of these indexes and return candidate related data points for each pairwise interaction. If multiple data sources must be sequentially aligned and a DAG can be specified over this sequence, a user can chain context enrichment subroutines by querying the next index with the records retrieved by the previous subroutine; we provide a toy example of this scenario in Section 8.5 under Recommendation.

7

Task
FJ FJ QA S R EM-T EM-T EM-S EM-S EM-S EM-S EM-S EM-S EM-S EM-D EM-D EM-D EM-D

Dataset
IMDb IMDb-hard SQuAD MS MARCO IMDb-wiki Abt-Buy Company BeerAdvo-RateBeer iTunes-Amazon Fodors-Zagat DBLP-ACM Amazon-Google DBLP-Scholar Walmart-Amazon DBLP-ACM DBLP-Scholar iTunes-Amazon Walmart-Amazon

# base
50000 50000 92695 508213 47813 1081 28200 4345 6907
533 2616 1363 2616 2554 2616 2616 6907 2554

# aux
10000 10000 64549 8.8M 47813 1092 28200 3000 55923
331 2294 3226 64263 22074 2294 64263 55923 22074

# + train
40000 40000 86668 418010 38250
616 16859
40 78 66 1332 699 3207 576 1332 3207 78 576

# + test
10000 10000 6472 7437 9563
206 5640
14 27 22 444 234 1070 193 444 1070 27 193

# LoC
1 1 2 1 1 (6) 2 2 1 1 1 1 1 1 1 1 1 1 1

Table 2: Data source record count, related pair (supervision)

count, and number of configuration lines changed for the

best recall@k, and for downstream tasks in parenthesis.

8 EVALUATION
In this section, we demonstrate that Ember and its operators are:
(1) General: Ember enables context enrichment across five domains while meeting or exceeding similarity-join baseline recall and query runtime performance (Section 8.3).
(2) Extensible: Ember provides a modular architecture, where each component affects performance (Section 8.4). Ember enables task-specific pipelines for various similarity-based queries, and provides task performance that can be finetuned by state-of-the-art systems (Section 8.5).
(3) Low Effort: Ember requires no more than five configuration changes (Table 2) from its default, and does not always require large amounts of hand-labeled examples (Section 8.6).

8.1.1 Fuzzy Join (FJ). We build two workloads using a dataset and generation procedure from a 2019 scalable fuzzy join VLDB paper [15]. The first dataset consists of the Title, Year, and Genre columns from IMDb [5]. The second dataset is generated by perturbing each row in the first by applying a combination of token insertion, token deletion, and token replacement. The task  , is to join each perturbed row with the row that generated it. We generate two dataset versions: 5 perturbations per row (IMDb) and 15 perturbations per row (IMDb-hard), up to 25% of the record length. As we focus on generalizablity more than scalability, to form a dataset, we randomly sample 10,000 movies and generate 5 perturbed rows for each. We hold out 20% of records as the test set; no records from the same unperturbed record are in both the train and test sets.
8.1.2 Entity Matching (EM). We use all 13 benchmark datasets [1] released with DeepMatcher [43], spanning structured (EM-S), textual (EM-T), and dirty (EM-D) entity matching. The base and auxiliary datasets always share the same schema. In EM-T, all data records are raw text entries or descriptions. In EM-S, each data record is drawn from a table following a pre-defined schema, where text-based column values are restricted in length. In EM-D, records are similar to EM-S, but some column values are injected into the incorrect column. The task  is to label a record pair (one from each dataset) as representing the same entity or not. Train, validation, and test supervision are lists of unrelated, and related pairs--we only use the related pairs for training Ember, as we identified mislabeled entries (false negatives) when using Ember to explore results.
8.1.3 Search (S). We use the MS MARCO passage retrieval benchmark [10]. MS MARCO consists of a collection of passages from web pages that were gathered by sampling and anonymizing Bing logs from real queries [10]. The task  is to rank the passage(s) that are relevant to a given query as highly as possible. Supervision is a set of 397M triples, with 1 relevant and up to 999 irrelevant passages for most queries. Irrelevant passages are retrieved via BM25. We report results over the publicly available labeled development set.

8.1 Evaluation Metric and Applications
Context Enrichment Evaluation Metric. Context enrichment identifies related records across a base and auxiliary dataset. We can view related records between the two datasets as forming edges in a bipartite graph: each record in each dataset represents a vertex in the graph. Under this framing, a context enrichment system must retrieve all of the outgoing edges (i.e., related records) for each record in the base dataset. This is equivalent to maximizing recordlevel recall, or the fraction of records for which we recover all related records. We choose to define recall at the record-level, rather than the edge-level, as we view context enrichment systems as being repeatedly queried for new incoming data once instantiated.
A naïve means to optimize for recall is to return all auxiliary records as being related to each record in the base dataset. A precision metric greatly drops when we retrieve multiple records. Thus, our evaluation metric is recall@k, for small k (i.e., join size).
Applications. We evaluate Ember against workloads from five application domains: fuzzy joining, entity matching, search, question answering, and recommendation (summarized in in Table 2). We make all our datasets publicly available post-Ember processing.

8.1.4 Question Answering (QA). We modify the Stanford Question Answering Dataset (SQuAD) [50]. SQuAD consists of Wikipedia passages and questions corresponding to each passage. The task  is to identify the beginning of the text span containing the answer to each question. As described in Section 2, a retriever module, used in retriever-reader models for QA [67], performs context enrichment. We modify SQuAD by splitting each each passage at the sentencelevel, and combining these sentences to form a new dataset. The modified task is to recover the sentence containing the answer.
8.1.5 Recommendation (R). We construct a workload using IMDb and Wikipedia to mimic the e-commerce example from Figure 1. For the first dataset, we denormalize four IMDb tables using KFK joins: movie information (title.basics), principal cast/crew for each movie (title.principals) and their information (name.basics), and movie ratings (title.ratings) [5]. For the second dataset, we extract the summary paragraphs of the Wikipedia entry for each IMDb movie by querying latest Wikipedia snapshot; we extract 47813 overlapping records [8]. We remove the IMDb ID that provides a KFK relationship to induce a need for keyless joins. We construct this workload to enable two applications. In Application A, we show that Ember can join datasets with dramatically different schema. In

8

Application B, we show how to estimate the rating for movies in the test set given ratings for the train set by performing similaritybased analyses enabled by Ember. Supervision is provided as exact matches and movie ratings with an 80-20 train-test set split.

8.2 Experimental Setup
Baselines. We first evaluate all workloads compared to seven similarity-based joins with respect to recall@1 and recall@10. Our baselines are joins using Levenshtein distance, four variations of Jaccard Similarity, BM25, Auto-FuzzyJoin [37], and a pretrained embedding-based approach. We evaluate downstream EM and search workloads with respect to previously reported state-of-the-art and benchmark solutions [6, 20, 38, 43, 45]. The remainder of our workloads were constructed to isolate context enrichment from the downstream task, and do not have standard baselines.
Ember Default Configuration. Tasks use a sentence preparer, and perform 20 epochs of self-supervised pretraining. Ember uses a single DistilBERTbase encoder trained with a triplet loss and stratified hard negative sampler. Self-supervision and the sampler use BM25 to identify similar sentences to concatenate from each data source, and to mark as hard negatives, respectively. The output embedding size is 200, and models are trained with a batch size of 8 using an ADAM optimizer [31] with initial learning rate of 1e-5. The default join sizes are 1 and 10. Results are five-trial averages.
Implementation. We use a server with two Intel Xeon Gold 6132 CPUs (56 threads) with 504GB of RAM, and four Titan V GPUs with 12GB of memory. We implement Ember in Python and PyTorch [48], with pretrained models and tokenizers from HuggingFace Transformers [61]. We use Faiss for MIPS [27], Magellan for similarity join baselines [32], rank-BM25 for BM25 [7], and AutoFuzzyJoin [37].

8.3 Generalizability

We show that Ember's recall and runtime meets or outperforms that of the following similarity-join baselines in nearly all tasks:

Levenshtein-Distance (LD). LD is the number of character edits needed to convert one string to another. This join returns the closest records with respect to single-character edits over provided key columns. We filter and only return results under a 30 edit threshold.

Jaccard-Similarity, Specified Key (JK-WS, JK-2G). The Jaccard

similarity between sets  and  is  (, )

=

| |

| |

.

Defining

a

Jaccard-similarity based join over textual inputs requires tokenizer

selection. We consider a whitespace tokenizer (WS) and a 2-gram

tokenizer (2G) to capture different granularities. JK-WS and JK-2G

return the closest records with respect to Jaccard similarity over

provided key columns using a WS or 2G tokenizer. We set a filtering

threshold to return results with at least 0.3 Jaccard similarity.

Jaccard-Similarity, Unspecified Key (J-WS, J-2G). J-WS and J-2G return the closest records with respect to Jaccard similarity using a WS or 2G tokenizer, after a sentence preparer. We set a filtering threshold to return results with over 0.3 Jaccard similarity.

BM25 (BM25). BM25 is a bag-of-words ranking function used in retrieval [53]. This join returns the closest records with respect to the Okapi BM25 score using default parameters k1=1.5 and b=0.75.

9

Pretrained-Embedding (BERT). BERT generates embeddings for each prepared sentence via a pretrained DistilBERTbase model, and returns the closest records based on the 2-norm between them.
Auto-FuzzyJoin (AutoFJ). AutoFJ automatically identifies join functions for unsupervised similarity joins [37] by assuming one input is a "reference table" with few or no duplicates, which does not always hold in context enrichment. If duplicates are present, or records in the reference table are not sufficiently spread, precision estimation may break down (Ember trivially accounts for these scenarios). AutoFJ optimizes for a precision target, and does not expose any join size semantics. We find a single result is typically returned per record, so we only consider recall@1 at a low precision target of 0.5. We comment on workloads that completed in 1.5 days on our machines using all cores. As AutoFJ exhaustively computes record similarities for each considered join configuration, this means we omit results for MS MARCO, EM-T Company, EMS BeerAdvo-RateBeer, and several others consisting of large text spans, or where blocking may not be as effective. Users must manually align matching columns, which do not always exist thus ignore information in our tasks (e.g., IMDb-wiki): as input, we pass the output of a sentence preparer if column names are not identical.
For multi-column datasets, we provide a plausible key column if the method requires. In FJ, EM, and R, this is a title or name column.
8.3.1 Retrieval Quality. We show that Ember is competitive with or exceeds the best performing alternatives with respect to Recall@1 and Recall@10 (Table 3). In both tables, we highlight the methods that are within 1% of the top performing method. No method dominates the others across the different workloads, while BERT underperformed all alternatives. Broadly, character-based joins (LD, JK-2G, J-2G) tend to perform well in scenarios where a join key exists but may be perturbed (FJ, EM-S, EM-D); word-based joins (J-WS, BM25) tend to perform well in scenarios with word-level join keys (EM-S), or where a join key does not exist but common phrases still link the two datasets (R, EM-D). Neither performs well otherwise (S, EM-T). Ember most consistently outperforms or is comparable with alternatives across the tasks: a learned approach is necessary for generalization. The exception is with the synthetic FJ workload that is, by construction, suited for character-based joins.
We do not report AutoFJ in Table 3 due to incompleteness. AutoFJ is tailored for the EM and FJ: many-to-one joins with identical schema. Of those completed, Ember is sometimes outperformed by up to 8% recall@1 (Amazon-Google), but is otherwise comparable or far exceeds AutoFJ when considering recall@10. AutoFJ's recall is 37.9% for QA, and 38.5% for R. As AutoFJ over the Wikipedia summary (task R) did not terminate, we only use the title column.
8.3.2 Query Runtime. Using optimized MIPS routines improved query runtime performance by up to two orders of magnitude. We note that this reflects Ember joining time, not pretraining and representation learning, which may require several hours. As the two FJ datasets are synthetic and follow identical generation methods, we consider just IMDb-hard. For our largest dataset, MS MARCO (S) with 8.8M auxiliary records, Ember indexes and retrieves results from the entire corpus, but each baseline only considers each of the up to 1000 relevant records provided by the MS MARCO dataset

Task FJ FJ QA S R EM-T EM-S EM-D

Dataset IMDb IMDb-hard SQuAD MS MARCO IMDb-wiki Average Average Average

LD
99.32
95.54 1.37 0.26 58.96 18.20 52.26 18.70

J-2g
100.00
98.87 34.70 1.66 11.01 20.06 73.50 64.69

JK-2g
99.42
95.02 41.26 1.66 82.57 36.56 72.30 52.05

Recall@1

JK-ws
71.54 33.42 29.47 1.15 86.08 36.90 69.25 53.41

J-ws
91.61 52.29 27.26 1.15 18.30 37.21 72.71 61.94

BM25
91.81 54.58 49.17 2.31 63.25 61.09
76.59
65.47

bert
45.85 8.72 11.37 0.01 0.04 6.88 38.98 30.61

emb 97.86 88.92 52.91 16.34 97.02 71.93 77.20 66.56

LD
99.99
99.04 1.50 0.96 64.62 25.73 70.26 24.60

J-2g
100.00
99.82 52.82 7.38 18.80 37.57 94.41 90.86

JK-2g
99.96 98.26 61.18 7.38 93.12 49.00
97.90 80.92

Recall@10

JK-ws
96.13 42.68 44.79 5.11 91.93 56.15 93.51 79.41

J-ws
96.30 64.34 43.18 5.11 18.30 53.11 93.33 89.85

BM25
96.33 65.82 67.08 4.10 96.25 71.90 95.19 92.31

bert
69.83 21.11 27.03 0.10 0.26 21.45 55.88 43.92

emb 99.79 98.37 78.85 46.98 98.89 83.29 97.58 97.83

Table 3: Baseline comparison, where we highlight all methods within 1% of the best for each task. No single baseline dominates

the other, but Ember (emb) is competitive with or better than alternatives in nearly all tasks with respect to Recall@k.

for each base record. Even when using CPU-only Faiss [27], Ember takes 7.24 seconds on average for the MIPS-based joining step. Embedding the query table takes 24.52 seconds on average. The most expensive dataset with respect to MIPS runtime is MS MARCO (S), which required 1.97 minutes when indexing all 8.8M records; excluding it reduces the average to 0.31 seconds. For embedding time, this was EM-T Company, requiring 3.96 minutes, whose exclusion reduces the average to 11.19 seconds. In contrast, the fastest baseline method (JK-WS), took 5.02 minutes on average; excluding MS MARCO (79.73 minutes) reduces the average to 21.18 seconds. The slowest method (LD), took 23.64 minutes on average, with the most expensive dataset, EM-T Company, requiring 321.98 minutes.

8.4.3 Remove Transformer fine-tuning (-ft). We replace RL with fixed MLM pretrained embeddings followed by a learned fullyconnected layer (i.e., we do not fine tune the entire BERT-based encoder, just the final output layer). We report our results in Figure 5. Without end-to-end Transformer fine-tuning, Ember slightly outperforms using just a pretrained encoder (-rl) at times. We primarily observe benefits in text-heavy workloads, where the pretrained encoder provides meaningful representations (QA, S, R, EMT) without relying on positional information from Transformers. While we do not perform an exhaustive comparison of pretrained embeddings or non-Transformer-based architectures, this shows that Ember's architecture is a strong out-of-the-box baseline.

8.4 Extensibility: Architecture Lesion
We demonstrate modularity and justify our architectural defaults by evaluating Ember's performance when modifying components as follows, each of which requires a single line configuration change (all reported percentages are relative unless explicitly specified):
(1) Replacing MLM pretraining and representation learning (RL) with a pretrained encoder.
(2) Removing RL, leaving only MLM pretraining. (3) Replacing the fine-tuned Transformer model with a frozen
pretrained embedding and a learned dense layer. (4) Removing MLM pretraining. (5) Random negatives compared to hard negative sampling (6) Using an encoder for each dataset versus a single encoder. (7) Removing the joining index-query optimization.
8.4.1 Pretrained encoder (-mlm, rl). We remove MLM pretraining and RL, and use a pretrained DistilBERTbase encoder--i.e., the same as BERT in Table 3. We report our results in Figure 5 normalized by Ember's default. Ember's performance dramatically declines, sometimes by three orders of magnitude (e.g., recall dropped to 0.04 in R). It is only feasible when the contents of the datasets to be joined are similar, as in certain EM-S and EM-D tasks, and FJ-IMDb.
8.4.2 Removing Representation Learning (-rl). We remove representation learning (RL) and only pretrain an encoder using the BM25-based MLM procedure from Section 5. We report our results in Figure 5. Ember's performance declines by up to an order of magnitude, though it improves performance compared to -mlm, rl on tasks whose datasets do not match the original natural text Wikipedia corpus (e.g., EM-S, FJ). Primarily textual datasets do not see as large an improvement over -mlm, rl, and for QA, where the corpus is derived from Wikipedia, -rl performs slightly worse.

8.4.4 Remove MLM Pretraining (-mlm). We eliminate MLM pretraining from the pipeline, and report our results in Figure 5 as -mlm. Ember meets or exceeds this setting in all but QA, though by a smaller margin than the previous experiments (up to 20.5% in IMDb-hard). MLM pretraining is not effective for QA as the corpus is drawn from Wikipedia--one of the two corpora BERT was trained with--and both base and auxiliary datasets consist of the same, primarily textual vocabulary. In such cases, users may wish to disable MLM pretraining, although performance is not significantly impacted. In contrast, of the non-EM tasks, MLM pretraining is most helpful for FJ IMDb-hard, where random perturbations result in many records with words that are not in the original vocabulary.
8.4.5 No Negative Sampling (-NS). We replace hard negative sampling with random sampling of unrelated records, and report our results in Figure 5. Hard negative sampling tends to improve performance by a similar margin as MLM pretraining, and only negatively impacted the EM-T Company dataset (by up to 8.72% absolute recall). We find larger improvements when the join condition is more ambiguous than recovering a potentially obfuscated join key, as in tasks S (30% R@1), QA (15% R@1), and EM-D (15% R@1). We use BM25-based sampling for all but FJ and QA, where we have prior knowledge, and develop custom samplers based on Jaccard similarity, and sentence origin, respectively. This improved performance over the BM25-based sampler by 1% absolute recall for FJ, and 8.7% and 5.6% absolute recall@1 and recall@10, respectively, for QA.
8.4.6 encoder Configuration (te). We use two encoders, one for each dataset, instead of our default single encoder configuration, and report our results in Figure 5. We find that using two encoders performs up to two orders of magnitude worse than using a single encoder, especially for strictly structured datasets. We observe that through the course of the encoder training procedure, the

10

NRoercamllali@z1e0d NRorecmalalliz@e1d

All

-MLM,RL

-RL

-FT

-MLM

-NS

TE

1.0

0.5

0.0 1.0

0.5

0.0 FJ: IMDb FJ: IMDb-hard QA: SQuAD S: MS MARCO R: IMDb-wiki EM-T: Avg EM-S: Avg EM-D: Avg

Figure 5: Architecture lesion displaying recall@k normalized to the default seen in Table 3 (All) for: a pretrained DistilBERTbase model with no representation learning (-mlm,rl), MLM pretraining with no RL (-rl), RL with no Transformer fine tuning (-ft), RL with no MLM pretraining (-mlm), no hard negative sampling (-ns), and one encoder per dataset (te).

performance of using two identically initialized encoders often degrades--inspecting the resulting embeddings even when running Ember over two identical tables shows that the exact same terms diverge from one another in the training process. However, we still provide the option to use a distinct encoder for each data source for future extension to non text-based data, such as audio or image.
8.4.7 Index Optimization. For INNER and FULL OUTER JOINs, we optimize for join execution time by indexing the larger dataset, which reduces the number of queries made by the system. Due to encoder training, this reflects a small fraction of the end-to-end pipeline execution time at our data sizes, but can be a substantial cost at scale. We evaluate this optimization by running the joining step while indexing the larger dataset, and then the smaller dataset for all tasks. On average, this optimization reduces join time by 1.76×, and up to 2.81×. However, this improvement is only meaningful in the MS MARCO workload, saving 2.7 minutes.
8.5 Extensibility: End-to-End Workloads

Task
EM-T EM-T EM-S EM-S EM-S EM-S EM-S EM-S EM-S EM-D EM-D EM-D EM-D

Dataset
Abt-Buy Company Beer iTunes-Amzn Fodors-Zagat DBLP-ACM Amazon-Google DBLP-Scholar Walmart-Amzn DBLP-ACM DBLP-Scholar iTunes-Amazon Walmart-Amzn

R@10
96.70 69.89 92.86
100 100 100 98.94 96.29 94.97 99.96 95.99 100 95.39

emb
85.05 74.31 91.58 84.92 88.76 98.05 70.43 57.88 69.60 97.58 58.08 64.65 67.43

DMRNN
39.40 85.60 72.20 88.50
100 98.30 59.90 93.00 67.60 97.50 93.00 79.40 39.60

DMatt
56.80 89.80 64.00 80.80 82.10 98.40 61.10 93.30 50.00 97.40 92.70 63.60 53.80

DMhyb 62.80 92.70 78.80 91.20 100 98.45 70.70 94.70 73.60 98.10 93.80 79.40 53.80

Ditto
89.33 93.85 94.37 97.06
100 98.99 75.58 95.60 86.76 99.03 95.75 95.65 85.69

Table 4: Recall@10 for Ember (R@10) and F1 Scores for

the EM workloads using Ember with join size = 1 (emb)

compared to deep-learning-based EM solutions from Deep-

Matcher (DM) and Ditto. Ember meets or exceeds (high-

lighted) at least one recent, state-of-the-art classifier in most

workloads. Ember has a low false negative rate (1 - R@10),

and can be used with these methods to increase precision

We show how to extend and use Ember in an end-to-end context.
Entity Matching. Recent deep-learning-based EM systems focus on the matching phase of that two-part (i.e., blocking and matching) end-to-end EM pipeline [20, 38, 43]: given a pair of candidate records, these systems must identify if the pair correspond to the same entity. For end-to-end EM, Ember must generate candidate blocks with a low rate of false negatives such that it can be efficiently followed by downstream matchers; we verify this in Table 4 (R@10). Perhaps surprisingly, we find that treating Ember results from a top-1 query as binary classifier achieves performance that is comparable to or better than previous, custom-built state-of-the-art with respect to F1 score. We believe incorporating general operators for data augmentation and domain knowledge integration to enable the custom advances presented in the current state-of-the-art EM system, Ditto, may allow Ember to entirely bridge this gap [38].
Search. In MS MARCO passage ranking, results are ranked in relevance order, and rankings are evaluated via mean reciprocal rank (MRR) of the top 10 results. We rank Ember results based on their query distance, and compare MRR@10 with existing MS MARCO baselines. We obtain MRR@10 of 0.266 on the dev set after just 2.5M training examples, outperforming the official Anserini BM25 baseline solution of 0.167. Our results exceed K-NRM, a kernel

based neural model, with a dev set MRR@10 of 0.218 [62], and is slightly below the state-of-the-art from the time, IRNet, with 0.278 MRR@10 [6]. For comparison, the first state-of-the-art BERT-based model uses BERTlarge to achieve MRR@10 of 0.365 after training with 12.8M examples [45], and current state-of-the-art is 0.439 [19]. By developing an additional joining operator, Ember can implement ColBERT [30], a previous state-of-the-art method that achieves 0.384 MRR@10 in MS MARCO document ranking that operates on larger input passages: we must remove the pooling step from Ember's encoder (1 line config change), and develop a retriever that indexes and retrieves bags of embeddings for each record.
Recommendation. In Application B of task R, we estimate the IMDb ratings of movies in the test set given a training dataset and Wikipedia corpus. We report the mean squared error (MSE) between the rating estimates and the true rating. The task is easy: predicting the average training rating returns 1.19 MSE, and a gradient-boosted decision tree (GBDT) over the joined data returns 0.82 MSE. We aim to show extensibility while meeting GBDT performance.
There are two approaches to enable this analysis with Ember by reusing the representation learned for the IMDb-wiki task: similarity defined in a single hop in terms of the labeled training data, or by first going through the Wikipedia data in two hops.

11

" data_dir " : "IMDb-wiki " , " join_type " : "INNER" , # or "LEFT" , "RIGHT" , "FULL" "left_size": 1, "right_size" : 1,
Listing 2: Core configuration lines annotated with options.
In the first method, users configure Ember to index labeled IMDb training data (with ratings) and retrieve records related to the test data (without ratings) via an INNER JOIN. Users then post-process the output by averaging the labels of the returned records. In the second method, users require two LEFT JOINs that index the Wikipedia training data against the IMDb test dataset, and the IMDb train dataset against the Wikipedia training data. Following a two-hop methodology, users first retrieve the closest Wikipedia summary to each test IMDb record, and then retrieve the closest labeled IMDb instances to each previously retrieved Wikipedia summary, which is post-processed via averaging.
Both approaches reuse the pretrained encoder from the IMDbwiki task, and require at most 6 configuration changes and a 7line post-processing module. We report results when aggregating with a join size of 1, 10, 20, and 30; both approaches improve performance as the number of neighbors increase, until a plateau is reached. The two-hop approach attains MSE of 1.69, 0.92, 0.89, and 0.89 for join size 1, 10, 20, and 30, respectively, while the one-hop approach performs better with 1.59, 0.89, 0.86, and 0.85.
8.6 Low Development Effort
We describe Ember's configuration to show the low number of config changes we made, and comment on the supervision required.
8.6.1 Configuration. Perhaps surprisingly, we found that exposing only join specification parameters (Listing 2) is a strong out-of-thebox baseline. We require input datasets and supervision to follow a fixed naming convention, under which obtaining the results in Table 3 relies on a default set of configurations options, where only the data directory must be changed. In Table 2, we list how many config changes are required for the best results from Figure 5.
We generate each result in Figure 5 by altering the following unsurfaced options: number of encoders, encoder intialization, encoder finetuning (boolean), fraction of supervision, negative sampler. Users can additionally specify lower-level hyperparameters, which we fix across our experiments: epochs, batch size, embedding size, pooling type, tokenizer, learning rate, loss function parameters.

9 RELATED WORK
Similarity Joins. Similarity-based, or fuzzy, joins often focus on the unsupervised setting [15, 59, 65]. State-of-the art systems such as AutoFJ [37] are tailored for the case where tables can be joined using exact keys or shared phrases that do not differ greatly in distribution (e.g., our EM tasks)--not complex context enrichment. Ember generalizes to arbitrary notions of similarity and join semantics at the expense of supervised pairs, and can also leverage unsupervised methods for negative sampling or MLM pretraining.
Automated Machine Learning. Automated Machine Learning (AML) systems aim to empower users to develop high-performance ML pipelines minimal intervention or manual tuning. They support various types of data preprocessing, feature engineering, model training and monitoring modules. Examples of AML tools include Ludwig [42], Overton [51], Google Cloud AutoML [2], and H20.ai [4]. However, these platforms do not focus on context enrichment, leaving it as an exercise for users to perform prior to data ingestion.
Relational Data Augmentation. Relational data augmentation systems seek to find new features for a downstream predictive task by deciding whether or not to perform standard database joins across a base table and several auxiliary tables [16, 35, 54]. Similar to context enrichment, these systems aim to augment a base table for a downstream ML workload. However, they assume the existence of KFK relationships that simply must be uncovered.
Data Discovery. Data discovery systems find datasets that may be joinable with or related to a base dataset, and to uncover relationships between different datasets using dataset schema, samples, and metadata [11, 13, 14, 17, 22­24]. These systems typically surface KFK relationships, and do not tune for downstream ML workloads.
NLP and Data Management. A recurring aim in data management is to issue natural language commands to interface with structured data [25, 36]. Recent work in Neural Databases [57] aims to replace standard databases with Transformer-powered schemaless data stores that are updated and queried with natural language commands. Related to this work are systems that leverage advances in NLP to provide domain-specific functionality, such as converting text to SQL [39, 66], correlating structured and unstructured data [64], enabling interpretable ML over structured data [9], or automating data preparation [56]. We focus on the broader problem of context enrichment of entities for downstream tasks. To our knowledge, this has not been formally addressed in the literature, which often assumes entity information has been resolved [57].

8.6.2 Dependence on Labeled Data. A key user input is labeled examples of relevant record pairs. With the exception of MS MARCO, we use all available labeled examples of relevant pairs in Table 3 and Figure 5. In MS MARCO, we use 2.5M of the 397M of the provided labeled triples, which is just 0.63% of the provided triples. For the remaining datasets, we evaluate how many labeled examples are required to match performance of our reported results. We found that 9 of the 17 datasets required all of the provided examples. The remaining datasets required 54.4% of labeled relevant data on average to meet Recall@1 performance, and 44.5% for Recall@10. In the best case, for EM-S DBLP-ACM, only 30% and 1% of the data is required to achieve the same Recall@1 and Recall@10, respectively.

10 CONCLUSION
We demonstrate how seemingly unrelated tasks spanning data integration, search, and recommendation can all be viewed as instantiations of context enrichment. We propose keyless joins as a unifying abstraction that can power a system for general context enrichment, which allows us to view context enrichment as a data management problem. Consequently, we developed and applied Ember, a first-of-its kind system that performs no-code context enrichment via keyless joins. We evaluate how developing a keyless join enrichment layer empowers a single system to generalize to five downstream applications, with no ML code written by the user.

12

REFERENCES
[1] 2018. Datasets for DeepMatcher paper. https://github.com/anhaidgroup/ deepmatcher/blob/master/Datasets.md
[2] 2021. Cloud AutoML. https://cloud.google.com/automl [3] 2021. Data Robot. https://www.datarobot.com/platform/automated-machine-
learning/ [4] 2021. h20.ai. https://www.h2o.ai/ [5] 2021. IMDb Datasets. https://datasets.imdbws.com/ [6] 2021. MS MARCO. https://microsoft.github.io/msmarco/ [7] 2021. rank-bm25. https://pypi.org/project/rank-bm25/ [8] 2021. Wikimedia Downloads. https://dumps.wikimedia.org/ [9] Sercan O Arik and Tomas Pfister. 2019. Tabnet: Attentive interpretable tabular
learning. arXiv preprint arXiv:1908.07442 (2019). [10] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong
Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, et al. 2016. MS MARCO: A human generated machine reading comprehension dataset. arXiv preprint arXiv:1611.09268 (2016). [11] Anant Bhardwaj, Souvik Bhattacherjee, Amit Chavan, Amol Deshpande, Aaron J Elmore, Samuel Madden, and Aditya G Parameswaran. 2014. Datahub: Collaborative data science & dataset version management at scale. arXiv preprint arXiv:1409.0798 (2014). [12] Andrei Broder. 2002. A taxonomy of web search. In ACM Sigir forum, Vol. 36. ACM New York, NY, USA, 3­10. [13] Michael J Cafarella, Alon Halevy, and Nodira Khoussainova. 2009. Data integration for the relational web. Proceedings of the VLDB Endowment 2, 1 (2009), 1090­1101. [14] Raul Castro Fernandez, Dong Deng, Essam Mansour, Abdulhakim A Qahtan, Wenbo Tao, Ziawasch Abedjan, Ahmed Elmagarmid, Ihab F Ilyas, Samuel Madden, Mourad Ouzzani, et al. 2017. A demo of the data civilizer system. In Proceedings of the 2017 ACM International Conference on Management of Data. 1639­1642. [15] Zhimin Chen, Yue Wang, Vivek Narasayya, and Surajit Chaudhuri. 2019. Customizable and scalable fuzzy join for big data. Proceedings of the VLDB Endowment 12, 12 (2019), 2106­2117. [16] Nadiia Chepurko, Ryan Marcus, Emanuel Zgraggen, Raul Castro Fernandez, Tim Kraska, and David Karger. 2020. ARDA: Automatic Relational Data Augmentation for Machine Learning. Proceedings of the VLDB Endowment 13, 9 (2020). [17] Fernando Chirigati, Rémi Rampin, Aécio Santos, Aline Bessa, and Juliana Freire. 2021. Auctus: A Dataset Search Engine for Data Augmentation. arXiv preprint arXiv:2102.05716 (2021). [18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Association for Computational Linguistics, Minneapolis, Minnesota, 4171­4186. https://www.aclweb.org/anthology/N19- 1423 [19] Yingqi Qu Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. 2020. RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering. arXiv preprint arXiv:2010.08191 (2020). [20] Muhammad Ebraheem, Saravanan Thirumuruganathan, Shafiq Joty, Mourad Ouzzani, and Nan Tang. 2018. Distributed Representations of Tuples for Entity Resolution. VLDB (2018). [21] Christos Faloutsos and Douglas W Oard. 1998. A survey of information retrieval and filtering methods. Technical Report. [22] Raul Castro Fernandez, Ziawasch Abedjan, Famien Koko, Gina Yuan, Samuel Madden, and Michael Stonebraker. 2018. Aurum: A data discovery system. In 2018 IEEE 34th International Conference on Data Engineering (ICDE). IEEE, 1001­1012. [23] Hector Gonzalez, Alon Halevy, Christian S Jensen, Anno Langen, Jayant Madhavan, Rebecca Shapley, and Warren Shen. 2010. Google fusion tables: data management, integration and collaboration in the cloud. In Proceedings of the 1st ACM symposium on Cloud computing. 175­180. [24] Alon Halevy, Flip Korn, Natalya F Noy, Christopher Olston, Neoklis Polyzotis, Sudip Roy, and Steven Euijong Whang. 2016. Goods: Organizing google's datasets. In Proceedings of the 2016 International Conference on Management of Data. 795­ 806. [25] Alon Y Halevy, Oren Etzioni, AnHai Doan, Zachary G Ives, Jayant Madhavan, Luke K McDowell, and Igor Tatarinov. 2003. Crossing the Structure Chasm. CIDR. [26] David Hawking. 2004. Challenges in Enterprise Search.. In ADC, Vol. 4. Citeseer, 15­24. [27] Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2017. Billion-scale similarity search with GPUs. arXiv preprint arXiv:1702.08734 (2017). [28] Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020. Nearest neighbor machine translation. arXiv preprint arXiv:2010.00710 (2020). [29] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2019. Generalization through memorization: Nearest neighbor language models. arXiv preprint arXiv:1911.00172 (2019).

[30] Omar Khattab and Matei Zaharia. 2020. Colbert: Efficient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43rd
International ACM SIGIR Conference on Research and Development in Information Retrieval. 39­48. [31] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014). [32] Pradap Konda, Sanjib Das, Paul Suganthan GC, AnHai Doan, Adel Ardalan, Jeffrey R Ballard, Han Li, Fatemah Panahi, Haojun Zhang, Jeff Naughton, et al. 2016. Magellan: Toward building entity matching management systems. Proceedings of the VLDB Endowment 9, 12 (2016), 1197­1208. [33] Hanna Köpcke and Erhard Rahm. 2010. Frameworks for entity matching: A comparison. Data & Knowledge Engineering 69, 2 (2010), 197­210. [34] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems 25 (2012), 1097­1105. [35] Arun Kumar, Jeffrey Naughton, Jignesh M Patel, and Xiaojin Zhu. 2016. To join or not to join? Thinking twice about joins before feature selection. In Proceedings of the 2016 International Conference on Management of Data. 19­34. [36] Fei Li and HV Jagadish. 2014. Constructing an interactive natural language interface for relational databases. Proceedings of the VLDB Endowment 8, 1 (2014), 73­84. [37] Peng Li, Xiang Cheng, Xu Chu, Yeye He, and Surajit Chaudhuri. 2021. AutoFuzzyJoin: Auto-Program Fuzzy Similarity Joins Without Labeled Examples. SIGMOD (2021). [38] Yuliang Li, Jinfeng Li, Yoshihiko Suhara, AnHai Doan, and Wang-Chiew Tan. 2020. Deep entity matching with pre-trained language models. arXiv preprint arXiv:2004.00584 (2020). [39] Xi Victoria Lin, Richard Socher, and Caiming Xiong. 2020. Bridging Textual and Tabular Data for Cross-Domain Text-to-SQL Semantic Parsing. arXiv preprint arXiv:2012.12627 (2020). [40] Zifan Liu, Zhechun Zhou, and Theodoros Rekatsinas. 2020. Picket: Self-supervised Data Diagnostics for ML Pipelines. arXiv preprint arXiv:2006.04730 (2020). [41] Jie Lu, Dianshuang Wu, Mingsong Mao, Wei Wang, and Guangquan Zhang. 2015. Recommender system application developments: a survey. Decision Support Systems 74 (2015), 12­32. [42] Piero Molino, Yaroslav Dudin, and Sai Sumanth Miryala. 2019. Ludwig: a typebased declarative deep learning toolbox. arXiv preprint arXiv:1909.07930 (2019). [43] Sidharth Mudgal, Han Li, Theodoros Rekatsinas, AnHai Doan, Youngchoon Park, Ganesh Krishnan, Rohit Deep, Esteban Arcaute, and Vijay Raghavendra. 2018. Deep learning for entity matching: A design space exploration. In Proceedings of the 2018 International Conference on Management of Data. 19­34. [44] Rajat Mukherjee and Jianchang Mao. 2004. Enterprise Search: Tough Stuff: Why is it that searching an intranet is so much harder than searching the Web? Queue 2, 2 (2004), 36­46. [45] Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with BERT. arXiv preprint arXiv:1901.04085 (2019). [46] Laurel Orr, Megan Leszczynski, Simran Arora, Sen Wu, Neel Guha, Xiao Ling, and Christopher Re. 2020. Bootleg: Chasing the Tail with Self-Supervised Named Entity Disambiguation. arXiv preprint arXiv:2010.10363 (2020). [47] George Papadakis, Dimitrios Skoutas, Emmanouil Thanos, and Themis Palpanas. 2020. Blocking and filtering techniques for entity resolution: A survey. ACM Computing Surveys (CSUR) 53, 2 (2020), 1­42. [48] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019. Pytorch: An imperative style, high-performance deep learning library. arXiv preprint arXiv:1912.01703 (2019). [49] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI blog 1, 8 (2019), 9. [50] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250 (2016). [51] Christopher Ré, Feng Niu, Pallavi Gudipati, and Charles Srisuwananukorn. 2019. Overton: A data system for monitoring and improving machine-learned products. arXiv preprint arXiv:1909.05372 (2019). [52] Matthew Richardson, Christopher JC Burges, and Erin Renshaw. 2013. Mctest: A challenge dataset for the open-domain machine comprehension of text. In Proceedings of the 2013 conference on empirical methods in natural language processing. 193­203. [53] Stephen Robertson and Hugo Zaragoza. 2009. The probabilistic relevance framework: BM25 and beyond. Now Publishers Inc. [54] Vraj Shah, Arun Kumar, and Xiaojin Zhu. 2017. Are Key-Foreign Key Joins Safe to Avoid when Learning High-Capacity Classifiers? Proceedings of the VLDB Endowment 11, 3 (2017). [55] Wei Shen, Jianyong Wang, and Jiawei Han. 2014. Entity linking with a knowledge base: Issues, techniques, and solutions. IEEE Transactions on Knowledge and Data Engineering 27, 2 (2014), 443­460.

13

[56] Nan Tang, Ju Fan, Fangyi Li, Jianhong Tu, Xiaoyong Du, Guoliang Li, Sam Madden, and Mourad Ouzzani. 2020. Relational Pretrained Transformers towards Democratizing Data Preparation [Vision]. arXiv preprint arXiv:2012.02469 (2020).
[57] James Thorne, Majid Yazdani, Marzieh Saeidi, Fabrizio Silvestri, Sebastian Riedel, and Alon Halevy. 2020. Neural Databases. arXiv preprint arXiv:2010.06973 (2020).
[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In NIPS.
[59] Jiannan Wang, Guoliang Li, and Jianhua Fe. 2011. Fast-join: An efficient method for fuzzy token matching based string similarity join. In 2011 IEEE 27th International Conference on Data Engineering. IEEE, 458­469.
[60] Kilian Q Weinberger and Lawrence K Saul. 2009. Distance metric learning for large margin nearest neighbor classification. Journal of machine learning research 10, 2 (2009).
[61] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. 2019. HuggingFace's Transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771 (2019).
[62] Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. 2017. End-to-end neural ad-hoc ranking with kernel pooling. In Proceedings of the 40th
International ACM SIGIR conference on research and development in information

retrieval. 55­64. [63] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan
Salakhutdinov, and Christopher D Manning. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600 (2018). [64] Pengcheng Yin, Graham Neubig, Wen-tau Yih, and Sebastian Riedel. 2020. TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data. In
Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 8413­8426. [65] Minghe Yu, Guoliang Li, Dong Deng, and Jianhua Feng. 2016. String similarity search and join: a survey. Frontiers of Computer Science 10, 3 (2016), 399­417. [66] Jichuan Zeng, Xi Victoria Lin, Caiming Xiong, Richard Socher, Michael R Lyu, Irwin King, and Steven CH Hoi. 2020. Photon: A Robust Cross-Domain Text-toSQL System. arXiv preprint arXiv:2007.15280 (2020). [67] Fengbin Zhu, Wenqiang Lei, Chao Wang, Jianming Zheng, Soujanya Poria, and Tat-Seng Chua. 2021. Retrieving and Reading: A Comprehensive Survey on Open-domain Question Answering. arXiv preprint arXiv:2101.00774 (2021). [68] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision. 19­27.

14

