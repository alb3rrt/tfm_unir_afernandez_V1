Fine-grained Generalization Analysis of Structured Output Prediction
Waleed Mustafa1 , Yunwen Lei2 , Antoine Ledent1 and Marius Kloft1 1TU Kaiserslautern
2 University of Birmingham mustafa@cs.uni-kl.de, y.lei@bham.ac.uk, {ledent, kloft}@cs.uni-kl.de

arXiv:2106.00115v1 [cs.LG] 31 May 2021

Abstract
In machine learning we often encounter structured output prediction problems (SOPPs), i.e. problems where the output space admits a rich internal structure. Application domains where SOPPs naturally occur include natural language processing, speech recognition, and computer vision. Typical SOPPs have an extremely large label set, which grows exponentially as a function of the size of the output. Existing generalization analysis implies generalization bounds with at least a square-root dependency on the cardinality d of the label set, which can be vacuous in practice. In this paper, we significantly improve the state of the art by developing novel high-probability bounds with a logarithmic dependency on d. Moreover, we leverage the lens of algorithmic stability to develop generalization bounds in expectation without any dependency on d. Our results therefore build a solid theoretical foundation for learning in large-scale SOPPs. Furthermore, we extend our results to learning with weakly dependent data.

1 Introduction

Structured output prediction (SOP) refers to a broad class of

machine learning problems with a rich structure in the output

space. For instance, the output may be a sequence of tags in

part-of-speech (POS) tagging, a sentence in machine transla-

tion, or a grid of segmentation labels in image segmentation.

A distinguishing property of these tasks is that the loss

function admits a decomposition along the output structures.

For instance, if the output is a sequence of partial labels, the

loss function could be the Hamming distance. The output

structure makes those problems substantially different,

both algorithmically and theoretically, from well-studied

machine-learning methods such as binary classification.

Algorithms specifically targeted at SOPPs have been put

forward in [Lafferty et al., 2001; Ciliberto et al., 2016;

Taskar et al., 2003;

Tsochantaridis et al., 2005;

Vinyals et al., 2015; Lucchi et al., 2013; Chen et al., 2017],

to mention but a few.

To appear in IJCAI 2021

Whilst the subject of SOP is well explored from a practical point of view, existing theoretical analyses have several limitations. For instance, the results in [Taskar et al., 2003; Collins, 2001] apply only to specific factor graphs and bound errors measured only by the Hamming loss, while other losses such as edit distance and BLUE scores are more natural in many applications. [McAllester, 2007] introduced guarantees that apply to general losses but only to randomized linear algorithms and admit only a square-root dependence on the size of substructures. In [Cortes et al., 2016], the authors introduced general bounds that apply to general factor graphs and general losses from the viewpoint of function class capacity. However, the associated bounds exhibit a square-root dependence on the number d of categories a subset of substructures can take, which can become vacuous when applied to extreme multi-class contexts [Lei et al., 2019] or models that assume a large dependence between the substructures.
In this paper, we aim to advance the state of the art in the theoretical foundation of SOP by developing generalization bounds applicable to large-scale problems with millions of labels. Our contributions are as follows.
1. We apply the celebrated technique of Rademacher complexity to develop high-probability generalization bounds with a log dependency on the size of the label set. This substantially improves the existing state of the art, which comes with at least a square-root dependency. We achieve this improvement by using covering numbers measured by the norm, which can exploit the Lipschitz continuity of loss functions with respect to (w.r.t.) the -norm. For comparison, the existing complexity analysis uses the Lipschitz continuity w.r.t. the 2-norm [Cortes et al., 2016], which does not match the regularity of loss functions in structured output prediction and thus leads to suboptimal bounds.
2. We leverage the framework of algorithmic stability to further remove the log dependency for generalization bounds in expectation. We consider two popular methods for structured output prediction: stochastic gradient descent (SGD) and regularized risk minimization (RRM). We adapt the existing stability analysis in a way to exploit the Lipschitz continuity w.r.t. the -norm of loss functions in SOP. 3. We extend our discussion to learning with weakly dependent training examples, which are widespread in SOPPs. For example, in natural language processing (NLP), a data set can come in the form of sets of documents, while learning

is performed at the sentence level. While assuming that the sentences are independent is inaccurate, it is reasonable to assume that the dependency between sentences decreases when their distance in a document increases.
The remaining parts of the paper are structured as follows. We discuss some related work in Section 2 and present the problem formulation in Section 3. We present our main results on generalization bounds in Section 4, which are extended to learning with dependent examples in Section 5. We conclude the paper in Section 6.

2 Related Work

We first review some work on structured output predic-

tion. Many algorithms have been developed to solve struc-

tured output prediction problems. Early techniques consid-

ered generative probabilistic models (e.g., hidden Markov

models [Rabiner and Juang, 1986]). Motivated by the suc-

cess of support vector machines (SVM), large-margin mod-

els for structured data were proposed in [Taskar et al., 2003;

Tsochantaridis et al., 2005]. To reduce the model complex-

ity, conditional random fields (CRFs) [Lafferty et al., 2001]

model the conditional distribution of the structured outputs

rather than modeling the joint probability of the input and

output. A key property of these models is that their pre-

diction step can be viewed as maximising a scoring func-

tion. Such a scoring function enjoys a decomposition over

the substructure so that the maximisation can be done effi-

ciently. CRFs were combined with convolutional neural net-

works (CNNs) in [Chen et al., 2017] to approach semantic

segmentation problems, achieving better performance than

CNNs alone.

In [Collins, 2001; Taskar et al., 2003], the authors showed

a generalization bound for their proposed models. However,

they considered restricted models and losses (Hamming loss).

A PAC-Bayesian bound is proved in [McAllester, 2007] for

Bayesian prediction algorithms. In [Cortes et al., 2016] the

authors introduced a more general generalization bound that

applies to general losses and models. Their bound scales as

the square root of the number of classes. This can lead to

vacuous bounds when the number of classes per substructure

and their dependence on each other continue to increase.

[Ciliberto et al., 2016] introduced the implicit embedding

approach to structured output prediction where the label is

encoded into a vector in some Hilbert space via an encoding

function. A decoding function is also defined so that predic-

tion is performed by composing a regression function and the

decoding function, thus establishing a connection between

structured output prediction and regression. They provided

generalization

bounds

of

the

order

of

O(m-

1 4

),

where

m

is

the number of samples, which can be a problem for large

m. Recently, [Ciliberto et al., 2019] introduced the setting

of localized structured output prediction, where they assume

a form of weak dependence between substructures. Their

model utilizes such assumption by treating each part of the

structure as an independent sample. They prove bounds of

the

order

O((ml)-

1 4

)

for

their

method,

where

l

is

the

number

of substructures, under weakly dependent samples.

We now review the related work for multi-class clas-

sification (MCC), which is a specific case of structured
output prediction. Various capacity measures of function
classes were used to study generalization bounds of MCC, including Rademacher complexities [Lei et al., 2015;
Maurer, 2016; Li et al., 2018; Maximov et al., 2018; Musayeva et al., 19], covering numbers [Zhang, 2004; Lei et al., 2019; Ledent et al., 2019] and the fat-shattering dimension [Guermeur, 2017]. While initial analyses implied
generalization bounds with at least a linear dependency on the number of classes [Koltchinskii and Panchenko, 2002],
the couplings among class components were ex-
ploited recently to get a dependency that can be as mild as square-root [Cortes et al., 2016; Li et al., 2018] or even logarithmic [Lei et al., 2019; Wu et al., 2021].

3 Problem Formulation
SOP refers to machine learning problems with an internal structure in the outputs (and potentially also in the inputs). For example in sequence-to-sequence prediction, both the input and output are sequences. In syntax analysis, the inputs are sequences of words and the output is a parse tree.
Let X be an input space (e.g., sentences in a given language) and Y be an output space (e.g., POS tags for the input sentences). In structured output prediction, the output space can often be decomposed into a number of substructures. Take POS tags as an example, where each word tag represents a substructure and the sequence of tags constitutes the structured output. Formally we define Y = Y1 × · · · × Yl, where Yk is the set of possible classes a substructure k can take. For a point (x, y)  X × Y, let yk denote the k-th element in y (i.e., y = (y1, . . . , yl)).
We aim to learn a scoring function h : X × Y  R based on which we can perform the prediction as y^(x) = arg maxyY h(x, y). The score function in structured output prediction can be described via a factor graph G = (V, F, E), where V = [l] := {1, . . . , l} is the set of variable nodes, F is a set of factor nodes, and E is a set of undirected edges between a variable node and a factor node. Let N (f ) be the set of nodes connected to the factor f by an edge and Yf = kN (f)Yk. For brevity, we assume that |Yf | = d for all f , where |Yf | denotes the cardinality of Yf . Now we define the scoring function h(x, y) for x  X and y  Y as
h(x, y) = hf (x, yf ),
f F
where yf := {yj : j  N (f )} and hf : X × Yf  R. Figure 1 gives an example of factor graphs and scoring functions.

Let S = {(xi, yi)}m i=1 be a training set with (xi, yi)  X × Y being independently drawn from a distribution D over

X × Y. We use a loss function L : Y × Y  R+ to measure the performance of prediction models, based on which we can

define the margin loss [Cortes et al., 2016] as L : X × Y × H  R:

L(x,

y,

h)

=

(max{L(y,
y=y

y)

-

1 

[h(x,

y)

-

h(x,

y)]}),

(1)

3

4

f1

1

2

(a)

f1 f2 f3
12345 (b)

3

f2 5

2

f3 6

f1 4

1

(c)

Figure 1: Examples of factor graphs. Panel (a) represents a factor graph with only one factor node. Note that N (f1) = {1, 2, 3, 4}

and hf1

(Yx,fy1 1,=y

case is h(x,

Y1 × Y2 2, y3, y4).

× Y3 Panel

× Y4. If Yi (b) depicts an

= {1, 2, 3} for all i, then |Yf1 | = example of factor graph that assumes

34. The corresponding scoring function is h(x, y) = a sequence-like structure. The scoring function in this

y) = hf1 (x, y1, y2, y3) + hf2 (x, y2, y3, y4) + hf3 (x, y3, y4, y5). Panel (c) depicts an example of tree-like factor graph.

where (r) = min(M, max(0, r)), M = maxy,y L(y, y) and H  {h : X × Y  R} is some hypothesis class. Note that L(x, y, h)  L(y^(x), y). Therefore, the obtained bounds for L will also hold for L. We then define the population risk R(h) and empirical risk RS(h) to quantify the performance of a model h on testing and training examples,
respectively as:

R(h) = ED[L(x, y, h)],

RS (h)

=

1 m

m

L(xi, yi, h).

i=1

Let  be a feature function which maps an input-output example (x, y)  X × Y to RD, where D is the dimension of feature space. In structured output prediction, the fea-
ture extractor takes a composite form according to the factor graph G, that is, (x, y) = fF f (x, yf ), where f : X × Yf  R. We consider a linear scoring function hw(x, y) = w, (x, y) indexed by a w  RD. Then the
hypothesis space becomes

Hp = (x, y)  w, (x, y) : w p  , (x, y)  X ×Y ,

(2)

where

wp = (

D i=1

|wi|d)

1 p

is the p-norm of w

=

(w1, . . . , wD). We also define the class of loss functions

Fp,, := (x, y)  L(x, y, hw) : hw  Hp . (3)

4 Main Results
In this section, we present our main results on generalization bounds for structured output prediction. We consider two types of generalization bounds: complexity-based bounds and stability-based bounds. Our aim is to develop bounds with a very mild dependency on the size of the label set, thus laying a solid foundation for structured output prediction, where the size of label set Y is often extremely large in practice. A key discovery to both our stability-based and complexity-based analysis is to note the Lipschitz continuity of loss functions w.r.t. infinity-norm · .
Definition 1 (Lipschitz continuity). We say that a loss function L(x, y, h) is (, )-Lipschitz in the last argument if, for any h, h~  H and all (x, y)  X × Y, we have:
|L(x, y, h) - L(x, y, ~h)|   max |h(x, y) - ~h(x, y)|.
yY

The existing analysis [Cortes et al., 2016] uses the (2, 2) Lipschitz continuity of loss functions:

|L(x, y, h)-L(x, y, ~h)|  2

|h(x, y)-h~(x, y)|2

1/2
.

yY

Note that the Lipschitz continuity w.r.t. -norm is much stronger than that w.r.t. 2-norm. Indeed, if L is (, )Lipschitz then it is also (, 2) Lipschitz since ·   ·
2. As a comparison, a (2, 2)-Lipschitz function can be
(2 |Y|, )-Lipschitz due to the norm relationship · 2 

|Y| ·  (the equality can hold in some cases). In Lemma 1 we build the -Lipschitz continuity of L for structured output prediction. A remarkable property is that
the involved Lipschitz constant is independent of |Y|. This

shows that the loss function in structured output prediction is

well behaved in the sense of Lipschitz continuity. However, the existing analysis based on the (2, 2)-Lipschitz continuity fails to exploit this strong regularity, and therefore only

implies suboptimal bounds with at least a square-root depen-

dency on the size of the label set. The proof of Lemma 1

below is given in the appendix.

Lemma 1.

The loss function L

is

(

2 

,

)-Lipschitz

with

re-

spect to the scoring function h for all x  X and y  Y.

4.1 Complexity-based Generalization Bounds

We develop generalization bounds with high probability here. Our basic tool to this aim is the Rademacher complexity.

Definition 2. The empirical Rademacher complexity of a function class H of real-valued functions is defined as:

RS (H)

=

E[ sup
f H

1 n

n i=1

if (xi)],

(4)

where {i} are random variables with equal probability of being either +1 or -1.

Theorem 1. Let  > 0 be. Then the Rademacher complexity

of the loss class Fp,, is bounded as follows:

R(Fp,,)



4 m

+

 144 q

-1|F

m

|

L~,

(5)

where L~ = log(2md|F |[8m|F |/ + 3] + 1) log(m),  = supfF,yYf ,xX f (x, y) q, and q = p/(p - 1).

The proof strategy is to to relate the complexity of the loss class Fp,, to a complexity of a scalar linear function class on an extended set of size m|F |d, thus moving contribution of d to the complexity from the output dimension to the size of training set. We then utilize standard bounds [Zhang, 2002]
that admit log dependency on the size of training set. The
detailed proof is given in the appendix.

Remark 1. We now compare our results with related work.

In [Cortes et al., 2016], the authors bounded the Rademacher

complexity of Fp,, by a factor graph Rademacher com-

plexity. R(Fp,,

)Spe2cifi2cR^alGSly(Hfopr),

the loss class where R^ GS (Hp

)

(3) they proved is defined as

1 m E

sup
hH i[m],f F,yYf

|F |i,f,y w, f (xi, y) .

Here  = (i,f,y)i[m],fF,yYf and each i,f,y is an independent Rademacher variable. Combining the result from

Theorem 2 in [Cortes et al., 2016], we get the following

bound forlearning with 2-regularization: R(F2,,) 

O

|F | m

d

. Note the bound exhibit a square-root depen-

dence on the number of classes per factor d = |Yf |. Thus it is

vacuous for typical SOPPs, where the number of class labels

grows exponentially w.r.t. the size of the output. For compar-

ison, our bounds enjoy a log dependency on d and therefore

still imply meaningful generalization bounds in this setting.

As a direct corollary, we use the connection between generalization and Rademacher complexity to get Theorem 2.
Theorem 2 (Generalization Bounds). For any  > 0,   (0, 1), and h  Hp, with probability at least 1 -  over the draw of training data S, the following bound holds:

R(h)



RS (h)

+

8 m

+

 288 q

-1|F

|

L~

m

+

M

log

1 

2m

.

4.2 Stability-based Generalization Bounds
In this section, we present generalization bounds in expectation for structured output prediction by leveraging the lens of algorithmic stability. Algorithmic stability is a fundamental concept in statistical learning theory, which measures the sensitiveness of output models when the training dataset of an algorithm A is slightly perturbed. For any algorithm A, we use A(S) to denote the model produced by running A over the training examples S.
Definition 3 (Uniform Stability). A stochastic algorithm A is -uniformly stable if, for all training datasets S, S  Zn that differ by at most one example, we have

sup EA L(x, y, A(S)) - L(x, y, A(S))  . (6)
x,y

Algorithmic stability naturally implies quantitative generalization bounds, as shown in the following lemma [Shalev-Shwartz et al., 2010].
Lemma 2 (Generalization via uniform stability). Let A be uniformly stable. Then ES,A R(A(S)) - RS(A(S))  .

We will apply algorithmic stability to study two representative algorithms for SOP: regularization and stochastic gradient descent. For brevity, we use the abbreviation R(w) = R(hw), RS(w) = RS(hw), etc. We also write w = arg infw R(w) for the minimizer of the population risk.
Regularized Risk Minimization. RRM is a popular scheme to overcome overfitting in machine learning. The basic idea is to add a regularizer to the empirical risk and build a regularized empirical risk RS. Then we minimize the resulting objective function to obtain a model wS as follows:

wS

=

arg

min
w

RS (hw)

:=

RS (hw)

+

 2

w

2 2

.

(7)

Here we omit the dependency of wS on the regularization

parameter for brevity. In the following lemma to be proved

in the appendix, we show that the above regularization algo-

rithm is uniformly stable. Let  := supx,y (x, y) 2.

Lemma 3. Let A be defined as (7), i.e., A(S) = wS . Then A

is

162 m2 

-uniformly

stable.

This lemma is a variant of the stability bound in

[Bousquet and Elisseeff, 2002], which, however, requires the

loss function to be admissible. We adapt their technique to the

setting of structured output prediction and a key step in our

analysis is again the Lipschitz continuity of the loss function

w.r.t. the  norm. A use of the classical Lipschitz continuity w.r.t. 2 norm would incur a bound with at least a square-root dependency on d. For comparison, the consideration of Lip-

schitz continuity w.r.t. the  norm allows us to get stability bounds independent of the size of the label set.

We can combine the Lipschitz continuity of loss functions,

the stability of regularization schemes established in Lemma

3 and Lemma 2 together to get the following generalization

bounds for structured output prediction. Let

w = arg inf
w

R(w)

:=

R(hw)

+

 2

w

2 2

be the minimizer of the regularized risk. We have the follow-

ing result, whose proof is given in the appendix.

Theorem 3. Let wS be defined in (7). Then

E R(wS ) - R(w)



162 m2

.

(8)



Furthermore, E

if we choose  = R(wS) - R(w

)m442w22m,wthen2

.

(9)

Stochastic Gradient Descent. We now turn to the per-
formance of SGD for structured output prediction. SGD is
a popular optimization algorithm with wide applications in learning in a big data setting. Let w(1) be the initial point and {t} be a sequence of positive step sizes. At the t-th iteration, we first randomly select an index it according to the uniform distribution over [m], which is used to build a stochastic gradient L(xit , yit , hw(t) ) (L(xit , yit , hw(t) ) denotes a subgradient of L(xit , yit , hw) at w = w(t)). Then we update the model along the negative direction of the stochastic gradient

w(t+1) = w(t) - tL(xit , yit , hw(t) ).

(10)

This scheme of selecting a single example to build a stochastic gradient allows SGD to get sample-size independent iteration complexity, and is especially appealing if m is large. Since we consider a linear scoring function hw, the loss function L is convex w.r.t. w. In the following lemma to be proved in the appendix, we build the uniform stability of SGD for structured output prediction. Note here we do not require the loss function to be smooth [Lei and Ying, 2020].
Lemma 4. Let S = {z1, . . . , zm} and S = {z1 , . . . , zm } be two datasets that differ only by a single example. Let {w(t)} and w^(t) be two sequences produced by SGD based on S and S, respectively. Then

t

EA

w(t+1) - w^(t+1)

2 2

 16e(1 + t/m2)2-2

j2.

j=1

According to Lemma 4, we know that the algorithm becomes more and more unstable as we run more and more iterations. We can use this stability bound to derive generalization bounds of SGD for structured output prediction. The proof is given in the appendix.

Theorem 4. Let{w(t)} be produced by (10) with t = . Then

E R(w¯(T )) -R(w)  O

 (T

+T

/m)2

+

1

+

T T

22 

,

(11)

where

w¯(T )

=

1 T

T t=1

w(t)

is

an

average

of

iterates.

 The upper bound (11) involves two terms. The first term

T + T /m comes from controlling the generalization error

R(w¯(T )) - RS (w¯(T )),

while

the second term

1+T 2 T

comes

from controlling the optimization error RS (w¯(T )) - RS(w).

It is clear the optimization error decreases w.r.t. T , while the

generalization error grows in the learning process. Therefore,

we need to trade-off these two terms by early-stoping SGD as

done by the following corollary. We write B  B if there are

absolute constants c1 and c2 such that c1B  B  c2B.

Corollary 1. Let {w(t)} be the sequence produced by (10)

with

t

=

.

If

we

choose

T



m2

and





T

-

3 4

/,

then

E R(w¯(T ))

-

R(w)



O(m-

1 2

).

Remark 2. According to Theorem 3 and Corollary 1, we know that both the regularization methodand SGD are able to achieve the generalization bound O(1/ m), which is minimax optimal. While RRM requires the objective function to be strongly convex, SGD only requires the objective function to be convex. Remarkably, these generalization bounds do not admit any dependency on the size of the label set, and provide a convincing explanation on why SOP often works well even if the problem has more class labels than training examples. To our best knowledge, these are the first labelsize free generalization bounds. As compared to Theorem 2 on high-probability bounds, our generalization bounds here are stated in expectation. It should be noted that our bounds in expectation require the loss functions to be convex, while the high-probability analysis also applies to nonconvex cases.

4.3 Applications
In this section we discuss applications of our bounds and compare them to those of [Cortes et al., 2016].

Example 1. Consider pair-wise Markov networks with fixed number of substructures l [Taskar et al., 2003]. Specifically, we have Y = Y1 × . . . × Yl and Yk  [c] for k  [l]. Further, we have sequence-like connections, i.e., there is an arrangement of output nodes such that if a factor f  F is connected

to two nodes then they are neighbors in that arrangement.

Therefore we have |F | = l-1 and d = c2. We further assume

an unnormalized hamming loss L(y, y) =

l k=1

Iyk =yk

so

that we normalize later in the bound to get rid of the depen-

dence on l = |F | + 1. For regularized learning with these

Markov networks, the Rademacher complexity of loss function classes was bounded in [Cortes et al., 2016]

R(F2,,)



O(

c m

).

As a comparison, our Rademacher complexity bound in Theorem 1 reduces to an upper bound on R(F2,,) that has the form

O

 log m

log(2mc2l[8m/+3]+1) m

.

Therefore, our bound significantly outperforms their bound

by dropping their linear dependency on c to a logarithmic de-

pendency. If we further extend the model so that each factor

f a

is connected to v nodes function of v, as O(cv/2

instead ) while

of 2, ours

their bound grows, as increase only O( v).

Faluizrtahtieornmbooreu,nadcscOor(di/ngtmo )Tihneeoxrepmecsta3t,io4n,

we for

can get generboth RRM and

SGD, where the log dependency is further removed.

Example 2. As the second example we consider multi-class

classification. In this case we have no substructures and

therefore |F | = 1, Y1 = Y where Y = [c], d = c. In [Cortes et al., 2016], the Rademacher complexity for multi-

class learning with 2 regularization was shown to satisfy

R(F2,,)  O

c m

.

Our analysis instead shows R(F2,,) is bounded by

O



log(2mc[8m/+3]+1) log m m

.

It is clear that we drop the square root dependency in c in [Cortes et al., 2016] to a log dependence. Analogous to Example 1, the log dependency can be further removed if we consider generalization bounds in expectation, as shown in Theorems 3 and 4.
Example 3. In this example we explore the possibility of combining SOP models above with a learned feature extraction function  as was practically explored in [Chen et al., 2017; Hinton et al., 2012]. Consider the case where  is a CNN that takes x as input and outputs different D-dimensional vector f (x, yf ) for each factor f and label yf . Chaining the covers, one can bound the Rademacher

complexity of the combined class as follows:

O

 q

-

1|F

|

m

+O

D¯ m

log(G~)

,

where the notation O hides logarithmic factors, D¯ is the number of network parameters and G~ is a product of norms of network weight matrices. The details of the bound and its derivation of this bound are given in the appendix.

5 Learning Weakly Dependent Sequences

In the above bounds we assumed that the examples are sampled independently from each other. However, this assumption is often violated in practice. For example, consider the problem of POS tagging. We are usually given a dataset of documents each of which contains a sequence of sentences. There are two natural assumptions. (1) We may assume that each document is a long sequence of dependent words. This assumption is too pessimistic. The considered sample size becomes too small, and the prediction complexity increases while, as sentences get further apart, the dependence between them decreases, and thus the effective sample size increases. (2) We may assume that each sentence is independent of the others within and across documents. This assumption on the other hand is too optimistic. Sentences following each other in the same document indeed have some degree of dependence. We formalize this dependence in a hierarchical manner, thus providing a trade-off between these two assumptions. Namely, we assume that the documents are independent of each other while sentences within a document are only weakly dependent. We note that the term document here does not necessarily mean an actual text document but rather any sequence of examples (e.g., for a dataset of videos, one video is a document as it is a sequence of images).
We now formalize the idea above. We are given a training set of independent documents {Di}m i=1. Each document Di is a sequence of weakly dependent examples Di = (zij)Jj=1. Since the structured output prediction framework in the above section subsumed the usual classification paradigm, we assume that the sequence elements classes follows it. That is, zji  X × Y =: Z, where X and Y defined as above.
Now we define precisely how the examples within each document Di are weakly dependent. We assume that each example within a given document is sampled from a -mixing process, defined below, at times 1, 2, . . . , J.

Definition 4 (Stationary -mixing Stochastic Process). Let

(zk) k=- be a stationary stochastic process and L =

((zk)Lk=1) and L+a = ((zk) k=L+a) be the sigma alge-

bras and

generated by the zL+a = (zL+a, .

random variables . . , Z). Define

Z1L = (z1, . the -mixing

. . , zL) coeffi-

cient (a) := supL1 E supBL+a |P (B|L) - P (B)| . The process is called -mixing if lima (a) = 0. It is called exponentially mixing if (a)  0 exp (-1ar) or algebraically mixing if (a)  0/ar, for positive 0, 1 and r.

Some examples of exponentially mixing process include a class of Autoregressive Moving Average (ARMA)

[Mokkadem, 1988] and a class of Markov process
[Rosenblatt, 2012].
Now let H be a structured output prediction function class as defined in the previous section (see (2)). For h  H, let Lh : Z  [0, M ] be a loss function over elements of the sequence zk. An example for such a loss is given in equation
(1). Again we are interested in high probability bounds on the difference between two quantities: the empirical risk R^S (h) and the true risk R(h), which are defined as

RS (h)

=

1 mJ

m

J
Lh(zij ),

i=1 j=1

R(h) = ES[RS(h)].

Theorem 5 summarizes the main results of this section.

Theorem 5. Let Fp,, be the loss class defined in (3) and let

S be a set of independent and identically distributed docu-

ments Di, i = 1, . . . , m, where each document is a sequence of examples (zij) j = 1, . . . , J drawn from a -mixing process. For any integer a > 0 such that J is a multiple of 2a.

Let the

fol>low2min(g2Jiane-q1u)ali(tya)h,othldesnuwniitfhorpmrolybaobvielritaylal thleasHt 1p

-

,

R(h) R^S(h) + O

2a(q -1)|F | L~  mJ

M a +

log

2

-2m(


J 2a

-1)

(a)

,

mJ

(12)

where L~ = log(2mJd|F |[8mJ/ + 3] + 1) log(mJ).

Remark 3. Note that the bound unsurprisingly depends on

the same main quantities as the bound in Theorem 2. To bet-

ter interpret it consider the following two extreme cases. (1)

The elements inside each document are independent of each

other. Note that in this case (a) = 0, for all a, hence a

can be chosen to be 1 and the bound boils down to the bound

in Theorem 2. (2) The elements inside each document are

strongly dependent. Thus, (a)  1 for all a and therefore

selecting

a

=

J 2

leads

to

the

bound

training examples. We further note

in Theorem 2 with only m that (a)  0 as a  ,

therefore, for any process admitting a fast decaying (a) the

term

2m(

J 2a

-

1)(a)

approaches

0

fast

for

moderate

a.

6 Conclusion
In this paper, we advance the state of the art in the generalization analysis of structured output prediction. We consider two types of generalization bounds: complexity-based and stability-based bounds. Our complexity-based approach produces bounds with high probability that admit a log dependency on the size of the label set. The stability-based approach further removes this log dependency for generalization bounds in expectation. This significantly improves the existing bounds, which have at least a square root dependency. We also extend our discussion to the setting of learning with weakly dependent training examples.
A very interesting question is to investigate whether the log dependency in the high probability analysis is an artefact

of our analysis or is really essential. Another question is to extend our generalization bounds in expectation to learning with nonconvex functions.
Acknowledgments
MK, AL and WM acknowledge support by the German Research Foundation (DFG) award KL 2698/2-1 and by the German Federal Ministry of Science and Education (BMBF) awards 01IS18051A, 031B0770E, and 01MK20014U. YL acknowledges support by NSFC under Grant No. 61806091.
References
[Bartlett et al., 2017] Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. In NeurIPS, 2017.
[Bottou et al., 2018] Le´on Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning. SIAM Review, 60(2):223­311, 2018.
[Bousquet and Elisseeff, 2002] Olivier Bousquet and Andre´ Elisseeff. Stability and generalization. JMLR, 2:499­526, 2002.
[Chen et al., 2017] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs. TPAMI, 40(4):834­848, 2017.
[Ciliberto et al., 2016] Carlo Ciliberto, Alessandro Rudi, and Lorenzo Rosasco. A consistent regularization approach for structured prediction. In NeurIPS, 2016.
[Ciliberto et al., 2019] Carlo Ciliberto, Francis Bach, and Alessandro Rudi. Localized structured prediction. In NeurIPS, 2019.
[Collins, 2001] Michael Collins. Parameter estimation for statistical parsing models: Theory and practice of. In IWPT, pages 4­15, 2001.
[Cortes et al., 2016] Corinna Cortes, Vitaly Kuznetsov, Mehryar Mohri, and Scott Yang. Structured prediction theory based on factor graph complexity. In NeurIPS, 2016.
[Guermeur, 2017] Yann Guermeur. Lp-norm sauer­shelah lemma for margin multi-category classifiers. Journal of Computer and System Sciences, 89:450­473, 2017.
[Hinton et al., 2012] Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal processing magazine, 29(6):82­97, 2012.
[Koltchinskii and Panchenko, 2002] Vladimir Koltchinskii and Dmitry Panchenko. Empirical margin distributions and bounding the generalization error of combined classifiers. Annals of Statistics, pages 1­50, 2002.

[Lafferty et al., 2001] John D Lafferty, Andrew McCallum, and Fernando CN Pereira. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML, pages 282­289, 2001.
[Ledent et al., 2019] Antoine Ledent, Waleed Mustafa, Yunwen Lei, and Marius Kloft. Norm-based generalisation bounds for multi-class convolutional neural networks. CoRR, abs/1905.12430, 2019.
[Lei and Ying, 2020] Yunwen Lei and Yiming Ying. Finegrained analysis of stability and generalization for stochastic gradient descent. In ICML, 2020.
[Lei et al., 2015] Yunwen Lei, Urun Dogan, Alexander Binder, and Marius Kloft. Multi-class svms: From tighter data-dependent generalization bounds to novel algorithms. In NeurIPS, 2015.
[Lei et al., 2019] Yunwen Lei, U¨ ru¨n Dogan, Ding-Xuan Zhou, and Marius Kloft. Data-dependent generalization bounds for multi-class classification. IEEE Transactions on Information Theory, 65(5):2995­3021, 2019.
[Li et al., 2018] Jian Li, Yong Liu, Rong Yin, Hua Zhang, Lizhong Ding, and Weiping Wang. Multi-class learning: from theory to algorithm. In NeurIPS, 2018.
[Long and Sedghi, 2020] Philip M. Long and Hanie Sedghi. Size-free generalization bounds for convolutional neural networks. In ICLR, 2020.
[Lucchi et al., 2013] Aurelien Lucchi, Y. Li, and P. Fua. Learning for structured prediction using approximate subgradient descent with working sets. In CVPR, 2013.
[Maurer, 2016] Andreas Maurer. A vector-contraction inequality for rademacher complexities. In ALT, 2016.
[Maximov et al., 2018] Yury Maximov, Massih-Reza Amini, and Zaid Harchaoui. Rademacher complexity bounds for a penalized multi-class semi-supervised algorithm. JAIR, 61:761­786, 2018.
[McAllester, 2007] David McAllester. Generalization bounds and consistency. Predicting structured data, pages 247­261, 2007.
[Meir, 2000] Ron Meir. Nonparametric time series prediction through adaptive model selection. Machine learning, 39(1):5­34, 2000.
[Mohri and Rostamizadeh, 2008] Mehryar Mohri and Afshin Rostamizadeh. Rademacher complexity bounds for non-iid processes. NeurIPS, 21, 2008.
[Mohri et al., 2018] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT press, 2018.
[Mokkadem, 1988] Abdelkader Mokkadem. Mixing properties of arma processes. Stochastic Processes and Their Applications, 29(2):309­315, 1988.
[Musayeva et al., 19] Khadija Musayeva, Fabien Lauer, and Yann Guermeur. Rademacher complexity and generalization performance of multi-category margin classifiers. Neurocomputing, 342:6­15, 19.

[Neyshabur et al., 2015] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural networks. In Peter GrA~ OEnwald, Elad Hazan, and Satyen Kale, editors, COLT, volume 40 of Proceedings of Machine Learning Research, Paris, France, 03­06 Jul 2015. PMLR.

[Rabiner and Juang, 1986] Lawrence Rabiner and B Juang. An introduction to hidden markov models. IEEE ASSP Magazine, 3(1):4­16, 1986.

[Rosenblatt, 2012] Murray Rosenblatt. Markov Processes, Structure and Asymptotic Behavior: Structure and Asymptotic Behavior, volume 184. Springer Science & Business Media, 2012.

[Shalev-Shwartz et al., 2010] Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Learnability, stability and uniform convergence. JMLR, 11:2635­ 2670, 2010.

[Taskar et al., 2003] Ben Taskar, Carlos Guestrin, and Daphne Koller. Max-margin markov networks. NeurIPS, 16, 2003.

[Tsochantaridis et al., 2005] Ioannis

Tsochantaridis,

Thorsten Joachims, Thomas Hofmann, and Yasemin

Altun. Large margin methods for structured and in-

terdependent output variables. JMLR, 6:1453­1484,

2005.

[Vinyals et al., 2015] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image caption generator. In CVPR, 2015.

[Wu et al., 2021] Liang Wu, Antoine Ledent, Yunwen Lei, and Marius Kloft. Fine-grained generalization analysis of vector-valued learning. In AAAI, 2021.

[Yu, 1994] Bin Yu. Rates of convergence for empirical processes of stationary mixing sequences. The Annals of Probability, pages 94­116, 1994.

[Zhang, 2002] Tong Zhang. Covering number bounds of certain regularized linear function classes. JMLR, 2, 2002.

[Zhang, 2004] Tong Zhang. Statistical analysis of some multi-category large margin classification methods. JMLR, 5:1225­1251, 2004.

Supplementary Material for "Fine-grained Generalization Analysis of Structured Output Prediction"

A Proof of Lemma 1
In this section, we present the proof of Lemma 1.

Proof of Lemma 1. Let h, ~h  H be arbitrary scoring functions. Given arbitrary (x, y)  X × Y,

|L(x, y, h) - L(x, y, h~)|

 (max L(y, y) - 1 [h(x, y) - h(x, y)]) - (max L(y, y) - 1 [h~(x, y) - ~h(x, y)])

y=y



y=y



 max - 1 [h(x, y) - h(x, y)] + 1 [h~(x, y) - h~(x, y)]

y=y 





1 

max |h(x, y) - h~(x, y)| + |h(x, y) - h~(x, y)|
y=y



1 

max |h(x, y) - h~(x, y)| + max |h(x, y) - h~(x, y)|

yY

yY



2 

max |h(x, y)
yY

-

h~(x, y)|.

This establishes the Lipschitz continuity.

B Proofs on Generalization Bounds with High Probability
In this section we sketch the proof of Theorem 1. A key step is to control the complexity of the loss function class (3), which is highly nonlinear and therefore is challenging to deal with. Our basic idea is to relate this complexity to a complexity of a linear function class, which is easier to deal with. Indeed, define the following extended function class based on the training sample S
Hp, := {v  w, v : w  RD, w p  , v  S},

where S is defined by

S := {f (x, y) : x  S|X , y  Yf , f  F }.

The basic idea in constructing S is as follows. For each input x in the original training set and each feature function f (., .), we construct |Yf | training examples as f (x, y), for all y  Yf . Therefore, the cardinality of S is md|F |. A remarkable discovery is that the complexity of the highly nonlinear loss function class Fp,, on a dataset of size m can be upper bounded
by that of the much simpler linear function class Hp, on a dataset of size md|F |, via the tool of  covering numbers.
Definition 5. Let H be a class of real-valued functions defined over a space Z and S := {z1, . . . , zm}  Z. For any  > 0, the empirical -covering number denoted by N(, H, S) with respect to S is defined as the minimum cardinality N of collection of vectors v1, . . . , vN  Rm, which we refer to as a cover, such that

sup
hH

min
j=1,...,N

max
i=1,...,m

|h(zi)

-

vij |



.

We now formally start building the connection between the complexity of Fp,, and that of Hp,.

Theorem B.1. Given the notation above, the following holds

log N(, Fp,,, S)  log N

 2|F

|

,

Hp,,

S

.

(B.1)

This result shows that one can use the covering numbers of the class H, to control the covering numbers of Fp,. The advantage of this result is that we now only need to control a covering number of a scalar real-valued function class as opposed to a multi-class-multi-factored class. This reduces the complexity of the problem significantly as controlling covering numbers of scalar function classes is a well-studied problem. In particular, we refer to a covering number bound of linear function classes [Zhang, 2002]. Note considering a large dataset only comes at a slight penalty since the covering number bound in the following lemma enjoys only a log dependency on the cardinality of dataset.

Lemma 5 ([Zhang, 2002]). Let L be a class of linear functions on a set of size n. That is, L = { w, x , x, w  RN }. If x q  b and w p  a, where 2  q <  and 1/p + 1/q = 1, then  > 0,

log

N(,

L,

n)



36(q

-

1)

a2b2 2

log[24ab/

+

2n

+

1],

where N(, L, n) is the worst case covering number of the class L on a dataset of size n.
The result controls the covering numbers by norms of the data and weights. As a direct corollary of Lemma 5 and Theorem B.1, we derive the following bound on the covering numbers of loss function classes.

Corollary 2. Given the notation above, the following holds

log

N(,

Fp,, ,

S)



C

()22|F 22

|2

Llog ,

where C = 144(q - 1), Llog = log[28|F |/ + 2md|F | + 1].
Now that we established bounds on the log of covering numbers, we use these bounds to give bounds on the Rademacher complexity of Fp,,. To that extent, we use Dudley's theorem to obtain a bound on the Rademacher complexities given bonds on the log covering numbers and thus establishing Theorem 1. The scoring function h(x, y) can be viewed as the probability for the class y given an input x. In our analysis we would like to treat the function h(x, .) as a real-valued vector. That is the vector of class probabilities for each y  Yk This is possible since the set of classes Yk is finite. Formally for any general function f : X × Y0  R (here, we assume a general finite output space Y0), where Y0 = {c1, . . . , cK } is some finite set with size K, we denote the vector (f (x, c1), . . . , f (x, cK ))  RK by [f (x, Y0)]. Therefore, we have [f (x, Y0)]k = f (x, ck). In what follows we always use the notation f (x, ck) instead of [f (x, Y0)]k. Note that for any c  Y0, there is a corresponding kc  [K] such that c = ckc and therefore we have f (x, c) = [f (x, Y0)]kc where kc is the corresponding index of c.

Proof of Theorem B.1. We start by proving (B.1). The idea is to start with an (, )-cover for Hp,, and use it to construct a

(

 |F |

,

)-cover

for

Fp,,

with

the

same

size.

To

that

extent,

let

C := {rj = (r1j,1,1, . . . , rmj ,d,l) : j = [N ]}  Rmdl,

be an (, )-cover for Hp,, where N is the cardinality of the cover. Now we use C to construct a cover for Fp1,,.

To

simplify

notation,

denote

by

j
ri,.,f

the vector (rji,1,f , . . . , ri,d,f )



Rd.

Further let f (x, .) denote the matrix

(f (x, y1)T , . . . , f (x, yd)T )  Rd×D, where y1, . . . , yd are the elements of Yf and define the corresponding dot product

as w, fF f (x, .) := w, fF f (x, y1) , . . . , w, fF f (x, yd)  Rd.

Now we claim that the set











L(x1, y1,

j
r1,.,f

),

.

.

.

,

L(xm

,

ym

j
rm,.,f

)

:

j



[N ]



Rm

f F

f F

is

an

(

 |F

|

,

)-

cover

to

the

set

   L x1, y1,

w, f (x1, .)
f F





 , . . . , L xm, ym,

w, f (xm, .)
f F







 : w  w p   .

Indeed by the construction of C, we have for any w  RD such that w p  , there exists j(w) such that

max
i[m]

max
j[d]

max
f F

|rji,(jw,f)

-

w, f (xi, yj)

|  .

(B.2)

Therefore,

max
i[m]

L

xi, yi,

w, f (xi, .)
f F

-L(xi, yi,

j(w)
ri,.,f

)

f F



2 

max
i[m]

f F

w, f (xi, .)

-

j(w)
ri,.,f

f F



=

2 

max max | (
i[m] j[d] f F

w, f (xi, yj)

- rij,(jw,f))|



2|F | 

max max max |
i[m] j[d] f F

w, f (xi, yj)

- rij,(jw,f)|



2|F 

|

,

where the first inequality is from -Lipschitzness of L and the second inequality is from triangule inequality. The last inequality is from the choice of j(w) to satisfy (B.2). Therefore we can conclude that

log

N(,

Fp,, ,

S)



log

N(

 2|F

|

,

Hp,,

S).

Before we prove Theorem 1, we first state the classical result of Dudley's entropy integral. The result is classic. Proofs can be found in [Bartlett et al., 2017].

Theorem B.2. Let F be a real-valued function class taking values in [0, 1], and assume that 0  F . Let S be a finite sample of size m. We have the following relationship between the empirical Rademacher complexity R(F ) and the covering number N(, F , S).

R(F )  inf
>0

4 + 12n

1 

log N(, F , S) .

Proof of Theorem 1. The proof is a straight forward application of the Dudley's entropy integral to the upper bounds obtained

in Corollary 2. To simplify notation let B = 12

(q-1)|F | 

,

and

A

=

16md|F

|2/,

and

D

=

6md|F

|

+

1

R(Fp,,)



4 m

+

12 m



4 m

+

12mB



4 m

+

12B m

1
log N(, F1,, S)d
1 m

1
1

log

(

A 



+

D) d

m

log(Am + D) log m

Where the first inequality is the Dudley's entropy integral. The second inequality follows from substituting the upper bounds

obtained in Corollary 2.

The

last

inequality

follows

by

noticing

that

log

A 



log Am for 



[1/m, 1] and therefore can be

taken outside of the integral, meaning it suffices to evaluate

1
1 m

1 

d.

We then substitute the values of B, A, and D to get:

R(Fp,,)

 =

4 m 4 m

+ +

 144 q

-1|F

|

144q-1m|F |

m

log(16m2d|F |2/ + 6md|F | + 1) log m log(2md|F |[8m|F |/ + 3] + 1) log m.

C Proofs on Generalization Bounds in Expectation

C.1 Regularized Risk Minimization We say a function g is -strongly convex if

g(w)  g(w) +

w - w, g(w)

+

 2

w - w

2 2

,

where g(w) is a subgradient of g at w = w.

Proof of Lemma 3. Let S = {z1 loss of generality, we assume zi

,..., = zi

zm} for i

and S = {z1 , . . . , zm } be two datasets that  [m - 1]. Since RS is convex (note L is

differ only by convex since

a single example. Without we consider linear models

here and a maximum of convex functions is again convex) we know that RS is -strongly convex. It then follows that

RS (wS ) - RS (wS )



 2

wS - wS

22.

Furthermore, it follows from the definition of wS and the (2/, )-Lipschitz continuity of L that

RS (wS )

-

RS (wS )

=

RS (wS )

-

RS (wS )

+

L(xn, yn , hwS ) - L(xn, yn, hwS ) m

+

L(xn, yn, hwS ) - L(xn, yn , hwS ) m



2 m

max
yY

|hwS (xn, y) - hwS (xn, y)| + |hwS (xn, y) - hwS (xn, y)|

=

2 m

max
yY

| wS - wS , (xn, y) | + | wS - wS , (xn, y) |



2 m

max
yY

wS - wS

2

(xn, y) + (xn, y)

2



4 m

wS - wS

2,

where we have used the assumption maxx,y (x, y) 2  . We can combine the above two inequalities to show

wS - wS

2



8 m

.

By using the (2/, )-Lipschitz continuity again, we get

sup
x,y

L(x, y, hwS ) - L(x, y, hwS )



2 

sup
x,y

hwS (x, y) - hwS (x, y)

=

2 

sup
x,y

wS - wS , (x, y)



2 

wS - wS

2



162 m2

.

Proof of Theorem 3. According to Lemma 2 and Lemma 3, we know

E R(wS ) - RS (wS )

= E R(wS ) - RS(wS )



162 m2

.

(C.1)

By the definition of wS we know RS(wS )  RS(w). Note w is independent of the sample, and we know E RS(w) = R(w). It then follows that

E R(wS ) - R(w) = E R(wS ) - RS (wS) + E RS (wS ) - RS (w) + E RS (w) - R(w)  E R(wS ) - RS (wS) .
We can combine the above inequality and (C.1) together and get (8). This proves the first inequality. By the definition of w we know R(w)  R(w). We can plug this inequality back into (8) and get

E R(wS )

-

R(w)



 2

w

2 2

+

162 m2

.



If we choose  =

m4

2 w

, then we get
2



E R(wS)

- R(w)  4

2 w m

2.

The proof is complete.

C.2 Stochastic Gradient Descent

Proof of Lemma 4. According to Lemma 1, for any w, w there holds

|L(x, y, hw)

-

L(x, y, hw )|



2 

max |
yY

w

-

w, (x, y)

|



2 

w - w

2.

It then follows that

L(x, y, hw)

2

2 

.

(C.2)

Without loss of generality, we assume zi = zi for i  [m-1]. We now analyse how w(t) -w^(t) 2 changes along the iterations. Consider two cases at the t-th iteration. If it = m, then

w(t+1) - w^(t+1)

2 2

=

w(t) - tL(xit , yit , hw(t) ) - w^(t) + tL(xit , yit , hw^(t) )

=

w(t) - w^(t)

2 2

+

t2

L(xit , yit , hw(t) ) - L(xit , yit , hw^(t) )

2 2

-

2t

w(t) - w^(t), L(xit , yit , hw(t) ) - L(xit , yit , hw^(t) ) .

The convexity of L implies that

w(t) - w^(t), L(xit , yit , hw(t) ) - L(xit , yit , hw^(t) )  0.

Together with (C.2), this implies

w(t+1) - w^(t+1)

2 2



w(t) - w^(t)

2 2

+

16t22-2.

If it = m, then

w(t+1) - w^(t+1) 2  w(t) - w^(t) 2 + t L(xm, ym, hw(t) ) - L(xm, ym , hw^(t) ) 2.

It then follows from C.2 and the elementary inequality (a + b)2  (1 + r)a2 + (1 + 1/r)b2 that

(C.3)

w(t+1) - w^(t+1)

2 2



(1

+

r)

w(t) - w^(t)

2 2

+

(1

+

1/r)8t22-2.

Since with probability 1 - 1/m we have it = m and with probability 1/m we have it = m, we can combine the above two

cases to show

Eit

w(t+1) - w^(t+1)

2 2

 (1 + r/m)

w(t) - w^(t)

2 2

+

(1

+

1/(rm))8t22-2.

We can apply the above inequality recursively and get

t

EA

w(t+1) - w^(t+1)

2 2

 16(1 + 1/(rm))2-2

j2(1 + r/m)t-j

j=1

If we choose r = m/t we know that for any j  t,

(1 + r/m)t-j  (1 + 1/t)t  e.

and therefore The proof is complete.

t

EA

w(t+1) - w^(t+1)

2 2

 16e(1 + t/m2)2-2

j2.

j=1

Proof of Theorem 4. According to Lemma 4 we know EA w(t+1) - w^(t+1)

2



 16e(1

+

t/m)-1t.

It then follows from the convexity of the 2 norm that EA w¯(T ) - w¯^(T )

2



7(1

+

 T

/m)-1T

.

This

together

with

Lemma 1 sup

shows the following EA L(x, y, hw¯(T ) )

uniform stability - L(x, y, hw^¯(T )

bounds )  14(1

+

 T

/m)2-2T

.

z

It then follows from Lemma 2 that



E R(w¯(T )) - RS(w¯(T ))  O 2( T + T /m) .

Furthermore, standard convergence rate analysis of SGD shows that [Bottou et al., 2018]

EA RS (w¯(T )) - RS (w)

O

1 + T 22 T

.

It then follows that

E R(w¯(T )) - R(w) = E R(w¯(T )) - RS(w¯(T )) + E RS(w¯(T )) - RS(w) + E RS (w) - R(w)

  O ( T + T /m)2

+O

1 + T 22 T

.

The proof is complete.

D Proofs on Learning with Weakly Dependent Examples

In this section we prove our results on Weakly Dependent Examples. We follow the standard analysis that was introduced in [Yu, 1994] and followed by [Mohri and Rostamizadeh, 2008; Meir, 2000]. Recall that the goal is to bound the following
probabilit:

P sup R(h) - R^S(h) >  .
hH

(D.1)

Standard uniform convergence results can not be employed here since zik and zik are not independent for k = k and all i  [m]. Uniform convergence results for mixing sequence have been studied in the literature, e.g., [Meir, 2000; Mohri and Rostamizadeh, 2008]). Bounds in the aforementioned works are proved via the so called independent blocks technique. The main difference here is that in our settings we have a hierarchical dependence. At the high level, we have inde-
pendence between documents, while at the low level, we have weak dependence between the sequence of examples within each of the documents. This is unlike previous work which considered only learning with one level of weakly dependent data. Our work can also be extended to the case when we have a higher hierarchy where different levels are weakly dependent with
different levels of weak dependence.
Most of the following is based on the standard analysis of independent blocks technique for mixing sequences that was introduced in [Yu, 1994] and later employed by [Mohri and Rostamizadeh, 2008; Meir, 2000].
For simplicity, assume that J = 2µa for positive integers µ and a. Let Hj = {i : 2(j - 1)a + 1  i  (2j - 1)a} and Tj = {i : (2j - 1)a + 1  i  2ja}. Further let ZiHj = (zik)kHj and similarly ZiTj = (zik)kTj , for j  [µ]. We then define the even dataset S0

S0 = {(ZiHj )j[µ]}m i=1,

(D.2)

and the odd dataset S1

S1 = {(ZiTj )j[µ]}m i=1.

(D.3)

That is we divide each document into 2µ blocks each containing a-consecutive elements. The even dataset S0 is formed as

follows: for each document in the data set S, include only blocks that have even index while the odd dataset is formed similarly.

Therefore, any document in S0 has blocks that are sampled a time steps apart.

Let Lah(ZHj )

=

1 a

kHj Lh(zk) and similarly Lah(ZTj )

=

1 a

kTj Lh(zk). The first step is to bound the probability

(D.1) by another probability that depends only on the even dataset S0. In this way we can work with sequences of blocks that

are a steps apart from each other and therefore the dependence between those blocks becomes weaker. The next lemma is a

standard lemma [Yu, 1994]. It relates the probability of estimation error on the full set of sequences to that on the even set of

sequences.

Lemma 6. With the notaion above, we have

P sup R(h) - R^S (h) >   2P sup R(h) - R^S0(h) >  .

hH

hH

Proof. According to basic properties of probabilities, we have

(D.4)

P sup R(h) - R^S(h) > 
hH

=P sup
hH

R(h)

- R^S0 (h) 2

+

R(h)

- R^S1 (h) 2

>

P sup R(h) - R^S0 (h) + sup R(h) - R^S1(h) > 2

hH

hH

P sup R(h) - R^S0 (h) >  + P sup R(h) - R^S1 (h) > 

hH

hH

=2P sup R(h) - R^S0(h) >  .
hH

Here the first inequality is due to the convexity of the sup, the second is combination of the union bound and the fact that for the sum to exceed 2, at least one of the summands has to exceed , and the last inequality is due to stationarity of the process generating the documents.

Now that we have reduced the problem to the study of the even blocks, the remaining problem is that those blocks are still
weakly dependent. Thus, the next step is to reduce the problem further to one with independent blocks so that we can apply standard techniques for independent data. The main idea is to replace the blocks in S0 with another set of blocks each of which has the same marginal distribution but are independent of each other. Specifically, for j  [µ] and i  [m], we introduce the random variable Z~iHj = (z~ik)im,kHj to have the same distribution as ZiHj and such that the set of random variables {Z~iHj }i[m],j[µ] are independent and similarly we construct {Z~iTj }i[m],j[µ]. In what follows we use S, S0, S1 to denote the datasets with independent blocks. The goal then is to establish a connection between (D.4) and a similar quantity that depends
only on the independent blocks. A key result introduced in [Yu, 1994] relates the means of the original data to that of the independent counterpart. The
original result was introduced in the case of only one sequence. We present here a slightly modified lemma that accounts for a
set of independent documents. The proof is provided at the end of this section.
Lemma 7. Let f be a measurable function of the set of even blocks S0 that is bounded by M , then we have,

|ES0 [h] - ES~0 [h]|  m(µ - 1)M (a) The next direct corollary controls the probability on the weakly dependent blocks to that of independent blocks.

(D.5)

Corollary 3. With the notation above, we have

P

sup R(h) - R^S0 (h)
hH

>

 P~

sup
hH

R(h) - R^S0 (h)

>

+ m(µ - 1)(a),

where P~ denote the probability measure on independent data.

(D.6)

Proof. We can apply Lemma 7 with f being I suphH R(h) - R^S0 >  to get the inequality.

We note also that ES0[R^S0 ] = R(h) due to the linearity of expectation and the fact that the blocks in S0 have the

same distributions as blocks of S0.

We further note that R^S0 (h)

=

1 mµ

i[m],j[µ] Lah(Z~iHj ). The first term in the right

hand side of (D.6) can be bounded via standard uniform convergence techniques, because we have the probability over in-

dependent Mohri and

blocks, by considering the class (Lah  H) = Rostamizadeh, 2008]). Therefore, we can apply

{

1 a

a k=1

Lh(.)

:

h



standard analysis to get

H} (see for any 

for example > 0,

[Mohri

et

al.,

2018;

P~

sup
hH

R(h) - R^S0 (h)

- 2ES0 [RS~0 ((Lah  H))] > 

 2 exp

-2µm2 M2

.

For the left hand side of the last inequality to match the right hand side of (D.6), we set  =  - 2ES0[RS~0 ((Lah  H))], thus,

P~

sup
hH

R(h) - R^S0 (h)

>

 2 exp

-2µm2 M2

(D.7)

Theorem D.1. Let  > 2m(µ - 1)(a) with probability at least 1 - , for all h  H, we have

M R(h)  R^S(h) + 2ES0 [RS~0 ((Lah  H))] +

log

2 -2m(µ-1)(a)

2µm

.

Proof. We combine inequalities (D.7), (D.6), and (D.4), and get

P

sup R(h) - R^S(h) > 
hH

 2 exp

-2µm2 M2

+ 2m(µ - 1)(a).

Therefore, for  > 2m(µ - 1)(a) with probability at least 1 - , for all h  H, we get

(D.8) (D.9)

M R(h)  R^S(h) + 2ES0 [RS~0 ((Lah  H))] +

log

2 -2m(µ-1)(a)



.

2µm

(D.10)

We now aim to control the second term in (D.8) by the worst-case -covering number of the dataset. To that end, we first show that the covering number of the class (Lah  H) on the dataset S0 is indeed upper bounded by the covering number of the class (Lh  H) on the set S. This is summerized in the following lemma.

Lemma 8. Given the notation above the following inequality holds

N(, (Lah  H), S0)  N(, (Lh  H), S).
Proof. First we show that the covering number defined on S indeed upper bounds the covering number defined on S0 . To see this, let C := {rj = (r1j,1, . . . , rmj ,l) : j  [N ]}  Rml be an  cover for the class (Lh  H) on the dataset S, therefore, for each h  H, there is a j(h)  [N ] such that,

max
k[l],i[m]

|Lh(z~ik)

-

rij,(kh) |



.

Hence,

max
imkµ

|Lah

(Z~iHj

)

-

1 a

rij,(th) |

=

max
im,kµ

|

1 a

Lh(z~it)

-

1 a

rij,(th) |

tHj

tHj

tHj



max
im,kµ,tHj

|Lh(z~it)

-

rij,(th) |



max
im,t[J ]

|Lh(z~it)

-

rij,(th) |

=

.

It then follows that

N(, (Lah  H), S0)  N(, (Lh  H), S).

Furthermore, let N(, (Lh  H), mJ) = supSZmJ N(, (Lh  H), S) be the worst case covering number on a dataset of size mJ.
Now we are ready to give the proof of Theorem 5.
Proof of Theorem 5. By Dudley's entropy integral we have

RS0 ((Lh



H))



4 mµ

+

12 mµ

1 



4 mµ

+

12 mµ

1 



4 mµ

+

1m2µ

1 

log N(, (Lah  H), S0)d log N(, (Lh  H), S)d log N(, (Lh  H), mJ)d,

where the second inequality is from lemma 8 and the third inequality is by definition of worst case covering number. Now substituting in (D.8), we get the following bound, for  > 2m(µ - 1)(a) with probability at least 1 - , for all h  H

R(h)



R^S (h)

+

8 mµ

+

2m4µ

1 

M log(N(, (Lh  H), mJ))d +

log

2 -2m(µ-1)(a)

 2µm

.

(D.11)

Recall

that

µ

=

J 2a

,

the

bound can

be

written

as



R(h)



R^S (h)

+

16a mJ

+

24 2a mJ

1 

 Ma log(N(, (Lh  H), mJ))d +

log

2

-2m(


J 2a

-1)

(a)

Jm

.

(D.12)

Now we would like to apply this theory to models that decompose as a factor graphs as above. Thus, for the loss class Fp,,, we have the following bound:

R(h)



R^S (h)+

32a mJ

+

288

2a(q- 1)|F |  mJ

M a log(2mJd|F |[8mJ/ + 3] + 1) log mJ+

log

2

-2m(


J 2a

-1)(a)

.

Jm

Now we give the proof of lemma 7. Before we prove it, we begin by defining some basic quantities and stating a useful basic
lemma. Let Q be a measure defined on the a product space (1 × 2, 1 × 2), where (k, k) for k  [2] are two measurable spaces. Let Qk be the marginal distribution of Q on (k, k). We define the following quantity:

(1, 2, Q) = E sup |Q(B|1) - Q2(B)|.
B2

The following lemma provides the basic building block for our proof. It controls the difference of expectation with respect to Q and Q1 × Q2 of bounded functions by their -coefficient.
Lemma 9. [Yu, 1994] Let h : 1 × 2  R be bounded by M and measurable. Let P be the product measure Q1 × Q2. Then, the following holds
|EQ[h] - Ep[h]|  M (1, 2, Q)

The following is a modification of Corollary 2.7 in [Yu, 1994]. Here, we give a fine grained analysis, that is useful if the sequence of random variables has different mixing coefficients that change over time.

Corollary 4. Let m  1 Q a probability measure of Q defined on (i, i)

and h :
defined and Qi

boenm i=tthh1eempiraordguiRncatblseppaaroc(bea(bm i=ilm i1i=ty1im, ie,am i=su1m i=re1io)f-imQ).eFaosnuur(rtahbeijrl=el1efut njQ,citiboijen=1tthheajtm)i,sadrbegofiiunnnaedl epdrobbyaMbil.itDy emneoatseubrye

Let P = m i=1Qi. Then

i(Q) = (ij=1j , i+1, Qi+1).
m-1
|EP [h] - EQ[h]|  M i(Q)
i=1

Proof. The proof is by induction.

· The base case m = 2 is by Lemma 9.

· Step: assume that the statement holds for m - 1, and let P^ = m i=-11Qi. Then

|EP [h] - EQ[h]| = |EQm EP^ [h] - EQm EQm-1 [h] + EQm EQm-1 [h] - EQ[h]|  |EQm EP^ [h] - EQm EQm-1 [h]| + |EQm EQm-1 [h] - EQ[h]|  EQm |EP^ [h] - EQm-1 [h]| + |EQm EQm-1 [h] - EQ[h]|

m-2

m-1

 M i(Q) + M m-1 = M i(Q),

i=1

i=1

where in the first equality we introduce the expectation with respect to Qm × Qm-1, the first inequality is by the triangule inequality, the second inequality is by Jensen's inequality while in the last inequality, the first term follows from the
induction hypothesis and the second is by Lemma 9.

Note that if set (Q) = maxi[m-1] i(Q), we get the same result as in corollary 2.7 in [Yu, 1994]. Our corollary can be useful when i is different for each i.
Now we are ready to prove lemma 7.

Proof of Lemma 7. The statement directly follows from the last corollary. Recall that S0 is a set of m sequence of blocks each with length a, that it can be arranged as the sequence, S0 = (Z1H1 , . . . , ZmHj ). Let i = Za, and i = (ZkHl ) for i  [µm], where (Z) denotes the sigma algebra generated by the random variable Z and k = i/µ and l = (i mod µ). Now we note
that i(Q) = 0, whenever i is divisible by µ and i(Q) = (a) otherwise. Hence the following holds

|ES0 [h] - ES~0 [h]|  m(µ - 1)M (a).

(D.13)

E Features Extracted From Neural Networks

In this section, we sketch how one can combine the strategies presented in this paper with other bounds to obtain generalization
guarantees for structured output prediction when the features are obtained via Deep Neural Networks.
Let F be a class of functions from X to a space Z endowed with the norm . z and let H be the class of linear functions  from Z to R such that     where .  denotes the dual norm to . z. For a sample set S = {x1, . . . , xm}  X denote by Nz,(, F , S) be the covering number of F with  and . z norms, i.e., the smallest N such that we have cover {f 1, . . . , f N }  F such that f  F there exists j  N such that for all i  n:

f (xi) - f j(xi) z  .

The maximum of this quantity over any choice of S will be denoted Nz,(, F , m). The following lemma relies on classic concatenation techniques [Bartlett et al., 2017].

Lemma 10. Suppose that F is such that f (x) z   for any x  X , then

N(,

H



F

,

m)



Nz,(

 2

,

F

,

m)

×

N(/2,

H,

,

m),

(E.1)

where N(/2, H, , m) denotes the maximum value of N(/2, H, S) over all S  Z with |S| = m and |s|z   s  S.

Proof. Let N1 = Nz,(/2, F , m) and N2 = N(/2, H, m). Let S = {x1, . . . , xm}  X be any sample set. Let {f 1, . . . , f N1} be the corresponding (z, ) cover. For each j  N1,
we can define the "training set" Sj := {f j(x1), . . . f j(xm)} and the corresponding  cover Cj = {hj1, . . . , hjN2} of H with granularity /2. Let Dj = Cj  f j := {hj1  f j, . . . , hjN2  f j}. We will show that the cover Nj=11Dj = {hjk  f j : j  N1, k  N2} is an  cover of H  F with respect to the , which implies the stated result. To see this, observe that for any h  f  H  F , we can choose j  N1 such that f j(xi) - f (xi) z  /2 for all i  n. We can now also choose an element hjk (k  N2) from the cover Cj such that for all i  n, |h(f j(xi)) - hk(f j(xi))|  /2. We now have that for any i  n,

|(h  f )(xi) - (hk  f j)(xi)|  |(h  f )(xi) - (h  f j)(xi)| + |(h  f j)(xi) - (hk  f j)(xi)|

= | h, f (xi) - f j(xi) | + |h((f j)(xi)) - hk(f j)(xi))|



h

  2

+

/2





 2

+

/2

=

,

as expected. At the last line, we have used the duality between the norms . z and . .

Back in our structured output prediction setting, let us consider the situation where the features f (x, y) are obtained from a neural network or another parametric method: the features f (x, yf ) can be read from the (f, yf , .) components of the threeway tensor W (x), where W denotes a parameter set chosen from a set W  RD¯ . We will write F for the function class F = {W : W  W}. For instance, W can be a vectorization of the weights of the neural network.

Define the augmented dataset

S¯ := {(x, f, yf ) : x  S|X , f  F, y  Yf }.

Define the function classes

H¯ := {(x, f, yf )  w, f (x, yf ) : w  RD, w p  , (x, f, yf )  S¯}

(E.2)

and

F¯p,, = {(x, y)  L(x, y, h) : w p  , W  W, (x, y)  X × Y}.

(E.3)

Similarly to Theorem B.1, it is easy to show that:

log N(, F¯p,,, S))  log N

 2|F

|

,

H¯ p, ,

S¯

.

(E.4)

Thus, assuming we have a way of obtaining bounds for the covering number of the function class corresponding to W, we
can use Lemma 10 in combination with the other techniques in this paper to obtain a generalisation bound valid for all choices of W and all choices of w  RD.
For instance, suppose that W is a D¯ -dimensional ball of radius 1 with respect to some norm w, and the Lipschitz constant of the map from RD¯ to (RD)X ×F ×Yf which maps Wf to W . (., .) is B¯-Lipschitz with respect to the norms . w and
. q, (we choose . z = . q). This means that for any W1, W2  W and for any x1  X , f  F and y1  Yf , W f 1(x1, y1) - W f 2 (x2, y2) q  B¯ W1 - W2 w. When this property holds, we say that the corresponding function class is (D¯ , L¯)-parametrised w.r.t. the relevant norm . q. We write  = supfF,yYf ,xX ,W W W f (x, y) q. Our assumption on the Lipschitz constant B¯ implies that for any , an -cover of W gives rise to an B¯-cover of F with respect to the q, norm (in this case, the same cover works for any training set). Such a cover of the ball W can easily be obtained from classic results such as Lemma A.8 in [Long and Sedghi, 2020]:

Lemma 11. Let d be a positive integer, . be a norm,  be the metric induced by it, and ,  > 0. A ball of radius  in Rd

w.r.t.



can

be

covered

by

(

3 

)d

balls

of

radius

.

The following is then immediate:

Lemma 12. Let F be a function class with outputs in a space Z endowed with the norm . z. Suppose that F is (D¯ , B¯)-
parametrized with respect to the norm z. Then, for any training set S = {x1, . . . , xm}, we have the following bound on the covering number of F :
log Nz,(, F , S)  D¯ log(3B¯/).

Note that the Lipschitz constant B¯ only shows up in log terms, which means that any reasonable control on B¯ is enough to yield satisfying generalisation bounds, and the dominant term in the first term of equation (E.6) will be be D¯ , except in

pathological cases. In [Long and Sedghi, 2020], the Lipschitz constant B¯ of convolutional neural networks was bounded in terms of the norms

of the weight matrices. Adapting their results (section 3.1) and combining with Lemma 11 above, we obtain

Lemma 13. Consider a neural network architecture with D¯ parameters and D1 outputs where the output layer is equipped with the L norm. We suppose that the L2 norms of the inputs are bounded by , and consider for each ,  > 0 the class F of networks with  layers whose weights satisfy the following conditions: (1) the spectral norms of each layers are bounded by
1 + , and (2) the sum of the spectral norms of the differences between the weight matrices and their initialised values is less than . For any  and any training set S = {x1, . . . , xm}, we have

log N(, F , S)  D¯ log(3N/),

(E.5)

where N = (1 +  + /).

In our structured output prediction setting, we now precisely define W to be the set of weights satisfying the conditions (1)

and (2) above. After noting that the .  and . q norms on the feature space Z are within a factor of D of each other and applying our Lemma 10, we have the following bound on the covering number of F¯p,,:

log(N(,

F¯p, S¯))



D¯

log[12N D/]

+

 576 q

-

1()22|F |2 22

log[216|F |/

+

2md|F |

+

1].

(E.6)

Plugging this back into E.6 and applying Dudley's entropy theorem similarly to the proof of Theorems 1 and 2, it is straightforward to obtain

R(h)



RS

(h)

+

O(

 q

- 1|F m

|

)

+

O(

D¯

log

1 2

(N |F

|D/))

+

O(

m

log m

1 

),

where the notation O hides logarithmic factors, and as above, N = (1 +  + /) with  = supxX x 2, the spectral norms of each layers are bounded by 1 + ,  is the number of layers, and the sum of the spectral norms of the differences between the weight matrices and their initialised values is less than .
Remark: Whilst the above bounds rely on the parameter-counting strategy from [Long and Sedghi, 2020] to bound the com-
plexity of the feature-extracting network, other approaches to that sub problem are perfectly compatible with our framework. For instance, norm-based bounds on the feature-extracting network, relying on results from from [Bartlett et al., 2017;
Neyshabur et al., 2015] etc. (for fully connected networks) or from [Ledent et al., 2019] (for CNNs) can also be plugged into
our proof, yielding results with the properties as above in terms of the (lack of) dependence on the number of factors but with various norms of the weights of feature-extracting network replacing the parameter-count term D¯ .

