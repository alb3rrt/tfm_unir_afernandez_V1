arXiv:2106.01584v1 [stat.ME] 3 Jun 2021

A Subspace-based Approach for Dimensionality Reduction and Important Variable Selection
Di Bo1, Hoon Hwangbo1,*, Vinit Sharma1, Corey Arndt1, and Stephanie C. TerMaath1
1University of Tennessee, Knoxville *Corresponding author: Hoon Hwangbo, hhwangb1@utk.edu
Abstract An analysis of high dimensional data can offer a detailed description of a system but is often challenged by the curse of dimensionality. General dimensionality reduction techniques can alleviate such difficulty by extracting a few important features, but they are limited due to the lack of interpretability and connectivity to actual decision making associated with each physical variable. Important variable selection techniques, as an alternative, can maintain the interpretability, but they often involve a greedy search that is susceptible to failure in capturing important interactions. This research proposes a new method that produces subspaces, reduced-dimensional physical spaces, based on a randomized search and forms an ensemble of models for critical subspaces. When applied to high-dimensional data collected from a composite metal development process, the proposed method shows its superiority in prediction and important variable selection.
keywords: Dimensionality reduction; Important variable selection; Subspace-based modeling
1 Introduction
With the advance of sensor technologies and the rapid breakthrough in computing, real-world data analysis has evolved to use voluminous high-dimensional datasets for modeling various systems with great complexity [21, 12]. A major challenge of analyzing a high-dimensional dataset comes from the curse of dimensionality [16]. That is, in a high-dimensional space, data tend to be sparser than in a low dimensional space so signals are much weaker but noises have far greater impacts producing misleading analysis outcomes. A fundamental solution for such a challenge is dimensionality reduction or variable selection, i.e., to identify a low-dimensional subset of features that can represent the entire data with a minimal loss of information [16].
Dimensionality reduction techniques, mapping high-dimensional data into a low-dimensional space, can alleviate the curse of dimensionality (e.g., the difficulty in estimating the variable importance) and other undesired properties (e.g., the sparse high dimensional space) of high-dimensional data analysis [31, 12, 16]. Dimensionality reduction is crucial in many domains, such as biological engineering and material science [31]. However, dimensionality reduction techniques are limited
1

due to the lack of interpretability and connectivity to the original physical variables. Even after extracting low-dimensional features, it is difficult to determine which physical variables are important as the features being artificial. In engineering applications, identifying influential physical variables is critical since it provides ways to reduce the number of occasions in operational controls and experimental designs to consider for determining the optimal policy. Variable selection, also referred to as model selection or feature selection, is a good alternative to the dimensionality reduction because this technique can pinpoint which physical variables are important by producing a subset of the original variables that are significant [32]. Variable selection typically involves a heuristic search, so important interactions between physical variables can easily be missed. In addition, some backward selection approaches begin with a full-dimensional model which are not free of the curse of dimensionality.
This paper develops subspace-based modeling that can achieve both dimensionality reduction and variable selection. To this end, we model a regression problem as an ensemble of multiple base learners where each base learner is responsible for representing a lower-dimensional input space only; we will referred to this reduced-dimensional physical space as a subspace. The core idea is to include only important subspaces in the model to achieve dimensionality reduction. Instead of applying an existing heuristic for subspace selection, we investigate randomly generated subspaces to cover a wide area in the exploration of important subspaces. We prescribe the subspaces to be an input space of a fixed (low) dimension considering that a high-order interaction exceeding a certain order is rarely significant in practice. By construction, an important subspace indicates that not only the variables therein but also potential interactions between them are significant. In addition, by keeping all subspaces defined at a low dimension, functional evaluation is always performed at a low-dimensional space, without having a risk of suffering from the curse of dimensionality.
The potential of this method to significantly impact our characterization and understanding of advanced engineering systems is demonstrated using the challenging problem of identifying the most influential material properties on damage tolerance for a layered hybrid structure and formulating a reduced order model based on these most sensitive parameters. This example was chosen due to the high dimensional parameter space and wide range of parameter values. Additionally, the high fidelity model used to predict damage tolerance requires a prohibitive amount of computational time to characterize just a small number of parameters in a narrow subspace of the parameter value ranges. The approach presented herein enables a novel method to rapidly reduce and characterize this vast and complex parameter space to solve a challenging physics-based material science problem.
The remainder of this article is organized as follows. Section 2 review relevant studies on dimensionality reduction and important variable selection. Section 3 presents our approach, subspacebased method. It utilizes the structure of generalized additive model, including randomized search for subspaces, critical subspace selection, and hyperparameter selection. In Section 4, discusses the results of the experiments and show the effectiveness of our approach. Conclusion are given in Section 5.
2 Literature Review
In the past decades, dimensionality reduction has been an active research area. Dimensionality reduction techniques transform high-dimensional data into a meaningful representation of reduced dimensionality, ideally close to its intrinsic dimensions [31]. The intrinsic dimensions of data are the minimum features needed to account for the observed properties of the data [6]. High-dimensional data typically involves a significant amount of noises that can easily mislead to wrong conclusions,
2

and analyzing high-dimensional data is often computationally intractable. For this reason, applying dimensionality reduction techniques is common in many applications involving a large number of variables, including digital photographs, speech recognition, and bioinformatics [31].
Traditional dimensionality reduction techniques include Principal Component Analysis (PCA), Independent Component Analysis (ICA), and Multidimensional Scaling (MDS). PCA is one of the most popular linear reduction technique and dates back to Karl Pearson in 1901 [19]. This technique tries to find orthogonal directions explaining as much variance of data as possible [28]. Li et al. [15] applied PCA to evaluate energy security of multiple East Asian countries with respect to vulnerability, efficiency, and sustainability. Salo et al. [23] proposed a network anomaly detection method that first reduced dimensionality by combining information gain and PCA and fed the extracted features into an ensemble classifier based on support vector machine (SVM), instance-based learning algorithms, and multilayer perceptron for the detection. Due to its attractive advantages of minimal information loss and generation of uncorrelated dimensions, PCA is still popular for use in a broad range of application areas [34]. ICA, on the other hand, tries to extract independent pieces of information from high-dimensional data mainly for the purpose of source blind separation, which has been popular in signal processing [9, 13]. ICA can be useful in other areas of study. For example, Sompairac et al. [27] discussed the benefits of ICA to unravel the complexity of cancer biology, particularly in analyzing different types of omics datasets. Different from PCA and ICA, MDS is a nonlinear dimensionality reduction technique. It provides useful graphical representation of data based on similarity information of individual data points. MDS is a common technique for analyzing network-structured data as shown in Saeed et al. [22] that applied MDS to wireless networks localization.
These traditional dimensionality reduction techniques are widely used. Recently, many new techniques have been proposed [14, 25]. Kernel PCA is the reformulation of traditional linear PCA in a high-dimensional space that is constructed using a kernel function [26]. When applied PCA and kernel PCA to a nonlinear system, Choi et al. [5] found PCA was inefficient and problematic. Xu et al. [33] propose a defect prediction framework that combines kernel (PCA) and Weighted Extreme Learning Machine to extract representative data features and learn an effective defect prediction model. Nonlinear dimensionality reduction techniques are notably able to avoid overcrowding of the representation, wherein distinct clusters are represented on an overlapping area [2]. High-dimensional data can be transformed into low-dimensional codes by training a multilayer neural network with a small central layer. Hinton and Salakhutdinov [11] presented that the deep autoencoder networks worked better than PCA if the initial weights were close to a good solution. Tenenbaum et al. [29] proposed the isometric feature mapping (Isomap) technique. Their method based on classical MDS but seeks to retain the intrinsic geometry of data sets, as captured in the geodesic manifold distances between all pairs of data points. It achieves the goal of nonlinear dimensionality reduction by using easily measured local metric information to learn the underlying global geometry of a data set. Non-parametric dimensionality reduction techniques such as t-distributed stochastic neighbor embedding (t-SNE) is a powerful tool for high-dimensional data [3]. Gisbrecht et al. [7] presented kernel t-SNE as an extension of t-SNE to a parametric framework, which enables explicit out-of-sample extensions. It is demonstrated that this technique yields satisfactory results for large data sets. McInnes et al. [17] proposed Uniform Manifold Approximation and Projection (UMAP), constructing from a theoretical framework based in Riemannian geometry and algebraic topology. Compared with t-SNE, UMAP arguably preserves more of the global structure. UMAP can be used as a general purpose dimension reduction technique, since it does not have computational restrictions on embedding dimension.
3

After applying dimensionality reduction techniques, the above methods will obtain artificial features. It is not beneficial for analyzing and interpreting the data. Measuring variable importance is a critical task in many applications, such as bioinformatics [32]. Important variable selection techniques can maintain the analysis and interpretability. Researchers have developed sensitivity analysis for variable importance [24, 10]. However, these articles focus only on the variance of the output. For sensitivity analysis, it is hard to include the best practice for correlated input variables. Raymer et al. [20] proposed a dimensionality reduction approach using genetic algorithm, performing feature selection, feature extraction and classifier training simultaneously. The key advantage of this technique is that the relationship between the original features and the output feature of the GA feature extractor remains explicit, and easy to identify and analyze. It has been proved effective to handle a classification problem.
Machine learning algorithm can be used for variable importance analysis, such as Random Forest (RF) [4] and Neural Network (NN) [18]. RF has referred as a useful tool to handle the feature selection issue even with high-dimensional datasets [4]. Gregorutti et al. [8] explored RF algorithm in presence of correlated predictors. Their results motivated the use of the recursive feature elimination (REF) algorithm using permutation importance measure as a ranking criterion. Chen et al. [4] compared the result of three popular datasets (Bank Marketing, Car Evaluation Database, Human Activity Recognition Using Smartphones) with and without important feature selection by RF method varImp(), Boruta, and REF. Experimental results demonstrated that RF achieved a better performance in all experiment groups. These experiments proved important feature selection is essential for classification problem. Furthermore, the best feature selection method is varImp() compared to Boruta and REF. varImp() is included in the classification and regression training (Caret) package. This package contains effective tools for data splitting, preprocessing, feature selection, model tuning with resampling, variable importance estimation, as well as other functionality. NN have received great attention in recent years. However, they are labeled as a "black box" since they provide little explanatory insight into the contributions of input variables in the prediction process [18]. Olden et al. [18] compared nine methodologies for quantifying variable importance in artificial NN using simulated data. Their results show Connection Weight Approach (CWA) provided the best accuracy. Furthermore, CWA was the only method identifying the ranked importance of all predictors. The other methods either only identified the first few important variables in the network or no variables at all. Liu [16] presented a deep Baysian Rectified Linear Unit network (ReLU) for high-dimensional regression and variable selection. The result showed their method outperforms existing classic and NN-based variable selection methods.
In high-dimensional regression or classification frameworks, variable selection is a difficult task, that becomes even more challenging in the presence of highly correlated predictors [8]. Most of the methods mentioned above can deal with high-dimensional data and find the important individual variables. They don't consider the interaction among input features. Our approach is an ensemble of models of critical subspaces, incorporating the interaction among variables for high-dimensional regression. The critical subspace originates from a randomized search and is verified by selection criterion. The approach will keeps adding critical subspaces until it meets the termination criterion.
3 Subspace-Based Modeling and Critical Subspaces
In this section, we develop subspace-based modeling for a general supervised learning problem that can identify critical variables and interactions. The dataset of interest contains data pairs of {xi, yi}ni=1 where xi is a p-dimensional input vector and yi is the corresponding response. For
4

subspace-based modeling, we form a subspace, a space spanned by a subvector of input x, determine the significance of a subspace in modeling response, and use critical subspaces as a basic unit for model building. A critical subspace implies that the variables forming the space and their interactions are important, so it naturally identifies important variables and interactions. In the following sections, we describe the proposed model structure, a randomized search for generating subspaces, a significance evaluation of a subspace, and an overall model learning process.

3.1 Subspace-based model

Considering a general supervised learning problem, we estimate a function f : p  that relates

p-dimensional input x and output y as y = f (x) +  where  is an additive noise. We model the

unknown function f as an additive mixture of subfunctions gj : k  for j = 1, . . . , m defined in

a lower dimensional space of dimension k p, instead of estimating the full-dimensional function

directly, as

y = f (x) +   g1(z1) + g2(z2) + ... + gm(zm) + ,

(1)

where zj is a subvector of x of size k. Each zj for j = 1, . . . , m takes different components of x. For example, suppose k = 3 and x = x1, x2, . . . , xp T where p > 20. Then, we may have
z1 = x3, x8, x12 T and z2 = x7, x11, x20 T . We define a reduced-dimensional space spanned by

each subvector zj as a subspace, and gj estimates the response y within a subspace formed by zj for j = 1, . . . , m. As such, Eq. (1) shows that we model the function f as a mixture of subspace-based

models.

The advantage of the subspace-based modeling is obvious. By evaluating y within a low-

dimensional subspace, there is no risk of suffering from the curse of dimensionality (opposed to

the case of evaluating y in the original full-dimensional space). In addition, to keep the dimension-

ality of each subspace low, the subspace-based modeling does not require using intrinsic artificial

dimensions but relies on a reduced-dimensional physical space providing physical interpretation of

the model. One major shortcoming of the subspace-based modeling is that it is not capable of

modeling interaction between more than k input factors by construction. However, in many cases

of real-world problems, a higher order interaction is almost negligible in modeling response. From

a preliminary study, we found that k = 3, i.e., to model up to 3-factor interactions, provided good

prediction results.

The key to the success in the subspace-based modeling lies in how to form the subspaces, i.e.,

how to form the subvectors of zj for j = 1, . . . , J. For an exhaustive search, if we assume p = 41

and k = 3, the number of all subspace candidates is

41 3

= 10660. Evaluating models for this many

subspaces and determining whether to include each of them or not requires a considerable amount

of computational efforts. On the other hand, simple variable selection techniques, such as greedy

forward selection and greedy backward selection, are not proper for finding critical subspaces, and

yet they are limited in terms of the exploration capability as they rely on a greedy search. In the

next section, we discuss how to generate subspaces, how to evaluate their significance, and how to

form a final model.

3.2 Subspace generation and extraction
To explore a broad area covering potential subspace configurations and not to rely on a greedy search, we generate subspaces randomly. In particular, at each draw, we randomly choose k variables

5

Figure 1: A flow chart describing the process of subspace generation and extraction
out of p variables (full-dimension) and evaluate whether to include this randomly generated subspace into a model or not. For each random draw, we apply sampling "without replacement," but we allow duplicated selection of a single variable at multiple draws. In other words, an input variable x1 could be a part of a subspace z1 and this subspace could be included in the model, but the variable x1 can be chosen again at later draws to form other subspaces. This is to ensure that different interactions caused by the same variable are not lost but all of them can be considered when modeling the response as long as they have significant impacts.
We determine the significance of a randomly generated subspace by evaluating the percentage reduction in the prediction error with respect to root mean square error (RMSE). To evaluate outof-sample prediction error and avoid possible overfitting, we apply 5-fold cross validation (CV) to calculate the error. For a 5-fold CV, the dataset assigned for training and validation (excluding that for testing) is split into 5 small subsets of the data. Each subset can serve as a validation dataset whereas the remaining four subsets can be used to train a model. We can generate 5 different train/validation combinations of data, for each of which train and validation datasets account for 80% and 20% of the original dataset, respectively, as shown in Fig. 1. After the data split, a subspace-based model (gj) for the randomly generated subspace (zj) will be added into the model, and the prediction error of this expanded model will be evaluated in terms of out-of-sample RMSE. The model is applied to the five different train and validation datasets producing RM SEt,q for q = 1, . . . , 5 for each train/validation dataset where t denotes the current iteration number for the
6

subspace generation. The RMSE is calculated as

RM SEt,q =

1 nvq

{xi ,yi }Vq

(yi

- f~t(xi))2

(2)

where Vq is the validation dataset for the qth train/validation split, nvq is the number of observations in the validation dataset. f~t = jJ (t) gj(zj) where J (t) is the set of indices for the selected critical subspaces until the tth iteration. The CV score is then the average of the five prediction

errors, i.e.,

5

CVt = RM SEt,q.

(3)

q=1

We use the percentage reduction of the prediction error for the selection of a critical subspace and also for the termination of the iterative search. The percentage error reduction is defined as

e

=

CVt-1 - CVt C Vt-1

(4)

where CVt-1 is the minimum (best) CV score in Eq. (3) obtained until the previous iteration, t - 1. The initial prediction error, CV0, is calculated from a model that only includes a constant term (the sample mean of yi's in the training dataset). For the selection of a critical subspace, we set a selection threshold , and if e > , the subspace generated at the tth iteration is accepted and included in the model. Otherwise, this subspace is disregarded. The search for a new critical subspace continues until the percentage reduction e becomes less than a termination threshold, µ. In other words, if e < µ, the subspace searching process terminates. If none of the selection criterion and the termination criterion are met, the iterative search continues for the next possible critical subspace. Fig. 1 illustrates the overall process of the subspace generation and extraction.

3.3 Subspace model learning and hyperparameter selection

This section describes how to estimate each subspace-based model gj for j = 1, . . . , J. For this

estimation, we use a randomly generated subvector, zj as predictors and the current residual as a response to estimate, i.e., y~i = yi - f~t(xi) for i = 1, . . . , n. In other words, we consider a regression

problem written as

y~i = gj (zj,i) + ~i

(5)

where zj,i for i = 1, . . . , n is a subvector for the ith observation, and ~i for i = 1, . . . , n is a modified noise.
To learn the function gj, we use support vector machine (SVM) as a base learner. It is a supervised learning method which can be used for classification and regression. For a regression analysis, it is also known as support vector regression (SVR). To train a model based on SVR, we use " -insensitive" error function which is a symmetric loss function. For this function, a flexible tube with the radius of is formed symmetrically around the estimate. Errors of size less than are ignored, and overestimation and underestimation are penalized equally. In other words, some penalty is assigned to the points outside the tube based on the distance between the points and the tube, but no penalty is applied to the points within the tube [1].

7

For SVR, the function gj is expressed as a linear combination of basis functions, hm for m = 1, . . . , M , as
M

gj(zj) = mhm(zj) + 0,

(6)

m=1

where m for m = 1, . . . , M is a coefficient of each basis function and 0 is a constant. To estimate gj, we minimize

H(, 0) =

n

 V (y~i - g^j(zj,i)) + 2

M

m2

(7)

i=1

m=1

0,

if |r| < ,

where V (r) =

|r| - , otherwise.

 = 1, . . . , M T is a coefficient vector, and  is a regularization parameter. The minimizer of Eq. (7) provides the estimate of gj in a form of

n

g^j (zj,i) = iKj (zj , zj,i).

(8)

i=1

where i for i = 1, . . . , n is a Lagrangian dual variable for the Lagrangian primal shown in Eq. (7), and Kj(·, ·) is a kernel function that represents the inner product of the unknown basis functions [9].
To fully specify the estimate of gj, some hyperparameters need to be determined. This includes the regularization parameter,  (related to the cost parameter in a typical SVM), the radius of the tube, , for the loss function, and some kernel related parameters. For this hyperparameter learning, we apply grid search based on generalized cross-validation (GCV) criterion. The proposed method already employs 5-fold CV for the selection of critical subspaces and the termination of the iterative search, so another layer of out-of-sample prediction will complicate the data structure and the training dataset may not suffer from the lack of a sufficient number of data points. While measuring in-sample error using the training data only, GCV provides a convenient approximation to leave-one-out CV [9], so it is a proper criterion for the hyperparameter learning of the proposed method. In general, GCV is calculated as

GCV (f^) = 1 n

yi - f^(xi) 2

(9)

n 1 - trace(S)/n)

i=1

where S is an n × n hat matrix that performs a linear projection of y = y1, . . . , yn T to achieve the estimate of y, i.e., y^ = Sy. For the proposed subspace-based model in Eq. (1), deriving this expression of linear projection is not straightforward. Still, Theorem 1 provides a good approximation of trace(S) that can be derived from the assumption of a squared loss function.

Theorem 1 Assuming a squared loss function, i.e., V (r) = r2, the estimate of the response y can

be expressed as y^ = Sy, and trace(S) =

J j=1

trace(Sj

)

where

Sj

=

Kj (Kj

+ I)-1

and

Kj

is

the

n × n kernel matrix with {Kj}i,i = Kj(zj,i, zj,i ) for i, i = 1, . . . , n.

We apply a grid search for the hyperparameter learning, and the set of the parameters that minimizes the GCV criterion in Eq. (9) will be chosen as the optimal parameter values.

8

3.4 Overall algorithm
Alg. 1 shows the overall process of the proposed subspace-based modeling. We first split the entire data based on 5-fold CV to leave some portion of data for testing. From the 80% of data assigned for model learning, we further apply 5-fold CV to split training data and validation data (used for applying the selection and termination criteria). By using training data only, we establish a model that includes a single constant term to evaluate CV0. The hyperparameter learning will be performed once the full model is established, i.e., once all critical subspaces are determined. We set up a grid search for the hyperparameters, so the hyperparameter values will be fixed within each evaluation of the grid search. For each fixed set of hyperparameters, we generate a subspace randomly and build a SVR model to estimate gj. The estimation quality is evaluated via 5-fold CV with respect to the CV score and the percentage error reduction in Eq. (3) and (4). If a subspace meets the selection criterion, it will be added into the model, and the residual information, y~i for i = 1, . . . , n, will be updated. This process of generating and evaluating a random subspace will continue until the termination criterion is met. Once the algorithm terminates, the GCV value will be computed according to Eq. (9) After the grid search is completed, we find the optimal hyperparameter values and apply this optimal setting to the entire training/validation dataset to build a final model. This final model will be evaluated by a separate test set for the comparison with other methods in Section 4.
4 Case Study
The data has 200*41 data points. The high-dimensional data collected from a composite metal development process. In order to compare the results more effectively, 5-fold cross-validation is used here, that is, it is used for comparing prediction ability and important variable selection. The metric criterion for prediction error is root mean square error (RMSE). Standard deviation is calculated based on different folds. Since the current machine learning methods focus on individual variable importance, the individual variable importance is compared among our method and certain popular machine learning methods. In each fold, important variables are captured. We use the common variables in five folds as the important variable of the data.
In this case study, our method have two 5-folds. The second 5-fold cross-validation, 64% of the samples are used to select the critical subspaces. The proportion of the validation set is 16%. For the first 5-fold cross-validation, 80% of the samples are used to train the model and find the optimized parameters, and the rest of the data (20% of the data) is used for evaluating the performance of the model.
The computer is Intel(R) Xeon(R) Gold 6140 CPU @ 2.30GHz 2.29 GHz (2 processors). In this case study, one subspace is three variables, that is, k=3. The values of parameters are as follows,  = 1%, µ = 0.001%.
4.1 Metal/Composite damage data
Design and analysis of layered hybrid structure is challenged by the many possible choices of materials and configurations. This vast parameter space is impractical to explore through physical testing and is computationally prohibitive due to the analysis time required due to the large number and ranges of input parameters. The randomized subspace approach is applied to overcome this limitation and provide an efficient and accurate approach to computationally characterize the
9

Algorithm 1: Subspace-based algorithm

1 Split the entire data into train data(80% of the data) and test data(20% of the data) using

5-fold cross-validation;

2 Build a model with constants using train data and calculate the prediction error;

3 Set different sets of hyperparameters;

4 for each set of hyperparameter do

5 Select a subspace based on a ranomized search;

6 Build a SVR model using train data only including the selected subspace;

7 Calculate the average prediction error;

8 if the difference between average RMSE and initial RMSE( p) is greater than  then

9

Store this subspace as important subspace;

10

Assign the intial RMSE to average RMSE;

11

Update the response to the model's residual;

12 else

13

Go to Step 5;

14 end

15 Repeat step 5 to step 13 until decreased percentage is less than µ;

16 for each stored subspace do

17

Add one stored subspace to the model using train dataset;

18

Assign the response to the model's residual;

19 end

20 Calculate GCV based on train and validation dataset;

21 Calculate prediction error;

22 end

23 Find the minimum GCV;

24 Find the corresponding hyperpameters, important subspaces and prediction error;

parameter space and inform limited physical testing to define parameters. The reduced order model can then be used to rapidly explore the parameter space to customize and optimize layered designs. The specific problem of metal and composite layered structures was chosen due to its complexity and fit for the demonstration objectives.
A 3D high fidelity finite element model explicitly captures each layer in the hybrid structure as well as the interfaces (Fig. 2). This physics-based model was validated for four point bending loading through experimental testing (Fig. 3) [30]. Damage tolerance was evaluated by the total energy absorbed by the structure­­an output parameter that is calculated during analysis and readily extracted using an automated approach. This model captures multiple, interacting damage mechanisms including the plastic deformation in aluminum, shear plasticity in each lamina, the intralaminar fracture of each lamina, delamination within the patch and disbond at the interface.
4.2 Comparison criterion: prediction error
In order to illustrate the superiority of our method, the popular predictors (e.g. RF, NN, Lasso) are used as baselines. For the methods above, the models include two 5-fold cross-validations. The first 5-fold is used to compute the average test error and the second 5-fold is used to identify the

10

Figure 2: High fidelity finite elment model used to collect data
Figure 3: Loading conditions investigated for damage tolerance of the layered hybrid structure
critical subspaces. Grid search is utilized to find the optimal parameters. GCV is the metric, that is, the optimal parameter provides the lowest GCV.
Excluding NN and our method, all methods are executed in R using train() function in "caret" package. NN is implemented in python with "keras" package. One hidden layer structure and two hidden layers structure are trained separately. Three activation function are used, "linear", "sigmoid" and "relu" (rectified linear unit). Hyperparameters are the numbers of neurons, batch size, and epochs. By comparing the NN results, we will pick the one with lowest prediction error.
It is noticed that linear models perform better than nonlinear models in Table 1. The standard deviations of linear models are lower than nonlinear models. Our model gives the lowest prediction error and standard deviation. The lowest prediction error indicates a more accuracy model. Our model provides a better prediction compared with other methods. The lowest standard deviation demonstrates that our method is more stable. When applied our approach, it provides a more accurate model with stronger robustness. In addition, by introducing subspace-based modeling, it can reduce the dimensionality of the data, mitigating the curse of dimensionality.
4.3 Important variable selection
The important individual variables in our model is the common variables in the critical subspaces that are chosen to add to the model. For other methods, the first twenty important variables
11

Table 1: Model performance

Method
Linear model Lasso regression Ridge regression Principal component regression Partial least squares regression Random forest K-nearest neighbors SVM (radial kernel) SVM (polynomial kernel) Neural network Subspace-SVM

Average RMSE
12.61 12.02 11.98 12.21 11.89 14.70 15.27 17.49 12.01 12.45 11.64

Standard deviation
1.05 1.45 1.20 1.64 1.15 3.00 3.51 3.77 1.20 1.13 0.94

are recorded for each fold. The variable importance of methods (excluding NN) is calculated by varImp() function in "caret" package in R. For NN, Connection Weight Approach is used to identify the important variables[18]. We still refer the common variables as important variables for other methods. The results are shown in the Table 2.

Table 2: Important variables of linear method

Method A Aln XS E P XT X7781 G1200

GII 12 E1800 GI X7500 XiT v E7500 XiS

LM Lasso Ridge PCR PLS
RF KNN SVM (RBF) SVM(poly) NN Our method

oo o o
oo oo oo o

o

o

o

o

oo o

o

o

o

o

oo o

o

oo

o

oo o

o

o oo o

o

o

o

o

o

o

oo o

oo o

o

o

o

o

oo o

o

o

oo o

o

o

oo o

o

o

oo o

o

o

oo o

o

o

o

o

o

oo o

o

The methods compared with our method include methods with reduced dimensionality(e.g., Lasso Regression and PCA) and methods with full dimensionality(e.g., RF and NN). As we can seen, certain variables are identified as important variables in all methods, while certain variables are rarely identified as important variables. Our method are able to capture the variables that identified as important variables in all methods, such as the first, second, fourth and seventh variables. The third and eighth variables are identified as important variables except a certain method. Our method can capture these two variables. Variables(e.g., the fifth, sixth and tenth variables) including in almost half or more than half methods are marked as important variables in our method. Certain variables(e.g., the ninth, eleventh and last variables) are rarely identified as important variables. Our method does not include these variables. In summary, compared with methods with reduced dimensionality and methods with full dimensionality, our method effectively captures important variables, including actual important variables and half important variables, while getting rid of not important variables.

12

Figure 4: The process of identifying important variables from the proposed method
4.4 Critical Subspace Analysis
The main novelty of our approach is that our method can identify the critical subspaces, automatically including the important physical variables and interaction. In Sections 4.2 and 4.3, it proves that our method is efficient to predict the data and identify the important variable. We can use the entire data to identify the critical subspaces, that is, (X1800, X7781, BKi), (XS, XiT, E1200), (G1800, A, v), (sigmaY, E, G1800), (XiT, E, GII), (SS, XS, B), (E, E7500, Aln), (G1200, GII, v7500). For the individual variable selection, it definitely contains the real important variables. Certain variables do not show in Table 2. These variables are not important variables but they have important interactions with other variables.
5 Conclusion
To deal with high-dimensional datasets and identify the important physical variables and interactions among input variables, the subspace-based modeling is proposed in this paper. The new method is an ensemble of models with critical subspaces, reduced-dimensional physical spaces. The critical subspaces come from a randomized search and are evaluated by the selection criterion. When applied this method and popular machine learning methods, the case study analysis results confirm the following conclusions:
(1) Compared with the existing predictors (e.g. RF, NN and Lasso), the RMSE is the lowest, indicating that our method is very suitable for the prediction of high-dimensional datasets. The lowest standard deviation shows that our method is stable.
13

(2) Our method can identify the critical subspaces, which means our method is able to capture important physical variables and interactions.
Our method can identify the important physical variables and interactions, so it is beneficial to experiment designing. For future work, we may get rid of fully randomized search. We can assign the variable importance to each predictor, that is, the probability of different predictors is different in the searching pool. Or, we build a model that only contains important predictors.
References
[1] Mariette Awad and Rahul Khanna. Support vector regression. In Efficient learning machines, pages 67­80. Springer, 2015.
[2] Etienne Becht, Leland McInnes, John Healy, Charles-Antoine Dutertre, Immanuel WH Kwok, Lai Guan Ng, Florent Ginhoux, and Evan W Newell. Dimensionality reduction for visualizing single-cell data using umap. Nature biotechnology, 37(1):38­44, 2019.
[3] Anna C Belkina, Christopher O Ciccolella, Rina Anno, Richard Halpert, Josef Spidlen, and Jennifer E Snyder-Cappione. Automated optimized parameters for t-distributed stochastic neighbor embedding improve visualization and analysis of large datasets. Nature communications, 10(1):1­12, 2019.
[4] Rung-Ching Chen, Christine Dewi, Su-Wen Huang, and Rezzy Eko Caraka. Selecting critical features for data classification based on machine learning methods. Journal of Big Data, 7(1): 1­26, 2020.
[5] Sang Wook Choi, Changkyu Lee, Jong-Min Lee, Jin Hyun Park, and In-Beum Lee. Fault detection and identification of nonlinear processes based on kernel pca. Chemometrics and intelligent laboratory systems, 75(1):55­67, 2005.
[6] Keinosuke Fukunaga. Introduction to statistical pattern recognition. Elsevier, 2013.
[7] Andrej Gisbrecht, Alexander Schulz, and Barbara Hammer. Parametric nonlinear dimensionality reduction using kernel t-sne. Neurocomputing, 147:71­82, 2015.
[8] Baptiste Gregorutti, Bertrand Michel, and Philippe Saint-Pierre. Correlation and variable importance in random forests. Statistics and Computing, 27(3):659­678, 2017.
[9] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The elements of statistical learning: data mining, inference, and prediction. Springer Science & Business Media, 2009.
[10] Jon C Helton, Clifford W Hansen, and C´edric J Sallaberry. Conceptual structure and computational organization of the 2008 performance assessment for the proposed high-level radioactive waste repository at yucca mountain, nevada. Reliability Engineering & System Safety, 122: 223­248, 2014.
[11] Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks. science, 313(5786):504­507, 2006.
14

[12] Luis O Jimenez and David A Landgrebe. Supervised classification in high-dimensional space: geometrical, statistical, and asymptotical properties of multivariate data. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 28(1):39­54, 1998.
[13] Yao Junliang, Ren Haipeng, and Liu Qing. Fixed-point ica algorithm for blind separation of complex mixtures containing both circular and noncircular sources. The Journal of China Universities of Posts and Telecommunications, 23(2):15­23, 2016.
[14] John A Lee and Michel Verleysen. Nonlinear dimensionality reduction. Springer Science & Business Media, 2007.
[15] Yingzhu Li, Xunpeng Shi, and Lixia Yao. Evaluating energy security of resource-poor economies: A modified principle component analysis approach. Energy Economics, 58:211­ 221, 2016.
[16] Jeremiah Zhe Liu. Variable selection with rigorous uncertainty quantification using bayesian deep neural networks. In Bayesian Deep Learning Workshop at NeurIPS, 2019.
[17] Leland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426, 2018.
[18] Julian D Olden, Michael K Joy, and Russell G Death. An accurate comparison of methods for quantifying variable importance in artificial neural networks using simulated data. Ecological modelling, 178(3-4):389­397, 2004.
[19] Karl Pearson. Liii. on lines and planes of closest fit to systems of points in space. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 2(11):559­572, 1901.
[20] Michael L Raymer, William F. Punch, Erik D Goodman, Leslie A Kuhn, and Anil K Jain. Dimensionality reduction using genetic algorithms. IEEE transactions on evolutionary computation, 4(2):164­171, 2000.
[21] G Thippa Reddy, M Praveen Kumar Reddy, Kuruva Lakshmanna, Rajesh Kaluri, Dharmendra Singh Rajput, Gautam Srivastava, and Thar Baker. Analysis of dimensionality reduction techniques on big data. IEEE Access, 8:54776­54788, 2020.
[22] Nasir Saeed, Haewoon Nam, Tareq Y Al-Naffouri, and Mohamed-Slim Alouini. A state-of-theart survey on multidimensional scaling-based localization techniques. IEEE Communications Surveys & Tutorials, 21(4):3565­3583, 2019.
[23] Fadi Salo, Ali Bou Nassif, and Aleksander Essex. Dimensionality reduction with ig-pca and ensemble classifier for network intrusion detection. Computer Networks, 148:164­175, 2019.
[24] Andrea Saltelli, Marco Ratto, Terry Andres, Francesca Campolongo, Jessica Cariboni, Debora Gatelli, Michaela Saisana, and Stefano Tarantola. Global sensitivity analysis: the primer. John Wiley & Sons, 2008.
[25] Lawrence K Saul, Kilian Q Weinberger, Fei Sha, Jihun Ham, and Daniel D Lee. Spectral methods for dimensionality reduction. Semi-supervised learning, 3, 2006.
[26] Bernhard Sch¨olkopf, Alexander Smola, and Klaus-Robert Mu¨ller. Nonlinear component analysis as a kernel eigenvalue problem. Neural computation, 10(5):1299­1319, 1998.
15

[27] Nicolas Sompairac, Petr V Nazarov, Urszula Czerwinska, Laura Cantini, Anne Biton, Askhat Molkenov, Zhaxybay Zhumadilov, Emmanuel Barillot, Francois Radvanyi, Alexander Gorban, et al. Independent component analysis for unraveling the complexity of cancer omics datasets. International Journal of molecular sciences, 20(18):4414, 2019.
[28] Carlos Oscar S´anchez Sorzano, Javier Vargas, and A Pascual Montano. A survey of dimensionality reduction techniques. arXiv preprint arXiv:1403.2877, 2014.
[29] Joshua B Tenenbaum, Vin De Silva, and John C Langford. A global geometric framework for nonlinear dimensionality reduction. science, 290(5500):2319­2323, 2000.
[30] Stephanie C TerMaath. Probabilistic multi-scale damage tolerance modeling of composite patches for naval aluminum alloys. Technical Report DTIC AD1074284, University of Tennessee, Knoxville, United States, 2018.
[31] Laurens Van Der Maaten, Eric Postma, and Jaap Van den Herik. Dimensionality reduction: a comparative review. J Mach Learn Res, 10(66-71):13, 2009.
[32] Pengfei Wei, Zhenzhou Lu, and Jingwen Song. Variable importance analysis: A comprehensive review. Reliability Engineering & System Safety, 142:399­432, 2015.
[33] Zhou Xu, Jin Liu, Xiapu Luo, Zijiang Yang, Yifeng Zhang, Peipei Yuan, Yutian Tang, and Tao Zhang. Software defect prediction based on kernel pca and weighted extreme learning machine. Information and Software Technology, 106:182­200, 2019.
[34] Hui Zou, Trevor Hastie, and Robert Tibshirani. Sparse principal component analysis. Journal of computational and graphical statistics, 15(2):265­286, 2006.
16

