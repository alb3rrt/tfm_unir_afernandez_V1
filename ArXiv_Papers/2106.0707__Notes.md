
# An Entropy Regularization Free Mechanism for Policy-based Reinforcement Learning

[arXiv](https://arxiv.org/abs/2106.0707), [PDF](https://arxiv.org/pdf/2106.0707.pdf)

## Authors

- Changnan Xiao
- Haosen Shi
- Jiajun Fan
- Shihong Deng

## Abstract

Policy-based reinforcement learning methods suffer from the policy collapse problem. We find valued-based reinforcement learning methods with {\epsilon}-greedy mechanism are capable of enjoying three characteristics, Closed-form Diversity, Objective-invariant Exploration and Adaptive Trade-off, which help value-based methods avoid the policy collapse problem. However, there does not exist a parallel mechanism for policy-based methods that achieves all three characteristics. In this paper, we propose an entropy regularization free mechanism that is designed for policy-based methods, which achieves Closed-form Diversity, Objective-invariant Exploration and Adaptive Trade-off. Our experiments show that our mechanism is super sample-efficient for policy-based methods and boosts a policy-based baseline to a new State-Of-The-Art on Arcade Learning Environment.

## Comments

arXiv admin note: text overlap with arXiv:2105.03923

## Source Code

Official Code



Community Code



## Bibtex

```tex
@misc{xiao2021entropy,
      title={An Entropy Regularization Free Mechanism for Policy-based Reinforcement Learning}, 
      author={Changnan Xiao and Haosen Shi and Jiajun Fan and Shihong Deng},
      year={2021},
      eprint={2106.00707},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```

## Notes

Type your reading notes here...

