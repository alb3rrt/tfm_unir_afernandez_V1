Learning Football Body-Orientation as a Matter of Classification
Adria` Arbue´s Sangu¨ esa1 , Adria´n Mart´in1 , Paulino Granero2 , Coloma Ballester1 and Gloria Haro1
1Universitat Pompeu Fabra, 2Russian Football Union adria.arbues@upf.edu

arXiv:2106.00359v1 [cs.LG] 1 Jun 2021

Abstract
Orientation is a crucial skill for football players that becomes a differential factor in a large set of events, especially the ones involving passes. However, existing orientation estimation methods, which are based on computer-vision techniques, still have a lot of room for improvement. To the best of our knowledge, this article presents the first deep learning model for estimating orientation directly from video footage. By approaching this challenge as a classification problem where classes correspond to orientation bins, and by introducing a cyclic loss function, a well-known convolutional network is refined to provide player orientation data. The model is trained by using ground-truth orientation data obtained from wearable EPTS devices, which are individually compensated with respect to the perceived orientation in the current frame. The obtained results outperform previous methods; in particular, the absolute median error is less than 12 degrees per player. An ablation study is included in order to show the potential generalization to any kind of football video footage.
1 Introduction
Although deep learning (DL) has been an active field of research over the last decade, its application on top of sports data has had a slow start. The lack of universal sports datasets made it an impossible challenge for a lot of researchers, professional clubs were not aware about the unlocked potential of data-driven tools, and companies were highly focused on manual video analysis. However, during this last lustrum, the whole paradigm shifted, and for instance, in the case of football, complete datasets like SoccerNet have been publicly shared [Giancola et al., 2018; Delie`ge et al., 2020], hence providing researchers with valid resources to work with [Cioppa et al., 2019; Cioppa et al., 2020]. At the same time, top European clubs created their own departments of data scientists while publishing their findings [Fernandez and Bornn, 2018; Llana et al., 2020], and companies also shifted to data-driven products based on trained large-scale models. Companies such as SciSports [Decroos et al., 2019; Bransen and Haaren, 2020],

Sport Logiq [Sanford et al., 2020], Stats Perform [Sha et al., 2020; Sto¨ckl et al., 2022] or Genius Sports [Quiroga et al., 2020] made a huge investment in research groups (in some cases, in collaboration with academia), and other companies are also sharing valuable open data [MetricaSports, 2020; StatsBomb, 2020; SkillCorner, 2020]. All these facts prove that, DL is currently both trendy and useful within the context of sports analytics, thus creating a need for plug-and-play models that could be exploited either by researchers, clubs or companies. Recently, expected possession value (EPV) and expected goals models proved to produce realistic outcomes [Spearman et al., 2017; Spearman, 2018; Ferna´ndez et al., 2019], which can be directly used by coaches to optimize their tactics. Furthermore, in this same field, Arbue´s-Sangu¨esa et al. spotted a specific literature gap regarding the presented models: player body-orientation. The authors claimed that by merging existing methods with player orientation, the precision of existing models would improve [Arbue´s-Sangu¨esa et al., 2020a], especially in pass events. By defining orientation as the projected normal vector right in the middle of the upper-torso, the authors propose a sequential computer vision pipeline to obtain orientation data [Arbue´s-Sangu¨esa et al., 2020b]. Their model stems from pose estimation, which is obtained with existing models [Ramakrishna et al., 2014; Wei et al., 2016; Cao et al., 2017], and achieves an absolute median error of 28 degrees, which indicates that, despite being a solid baseline, there is still room for improvement.
Therefore, in this article, a novel deep learning model to obtain orientation out of any player's bounding boxes is presented. By: (1) using sensor-based orientation data as ground truth, (2) turning this estimation into a classification problem, (3) compensating angles with respect to the camera's viewing direction, and (4) introducing a cyclic loss function based on soft labels, the network is able to estimate orientation with a median error of fewer than 12 degrees per player. The rest of the paper is organized as follows: in Section 2 the main data structures and types of datasets are detailed; the proposed fine-tuning process is explained in Section 3 together with the appropriate details about the loss function and angle compensation. Results are shown in Section 4, and finally, conclusions are drawn in Section 5.

Figure 1: Several domains are merged in this research: (left) sensor-, (middle) field-, and (right) image-domain. By using corners and intersection points of field lines, the corresponding homographies are used to map data across domains into one same reference system.

2 Data Sources
Before introducing the proposed method, a detailed description of the required materials to train this model is given. Similarly, since we are going to mix data from different sources, their corresponding domains should be listed as well:
· Image-domain, which includes all kinds of data related to the associated video footage. That is: (i1) the video footage itself, (i2) player tracking and (i3) corners' position. Note that the result of player tracking in the imagedomain consists of a set of bounding boxes, expressed in pixels; similarly, corners' location is also expressed in pixels. In this research, full HD resolution (1920 x 1080) is considered, together with a temporal resolution of 25 frames per second.
· Sensor-domain, which gathers all pieces of data generated by wearable EPTS devices. In particular, data include: (s4) player tracking, and (s5) orientation data. In this case, players are tracked according to the universal latitude and longitude coordinates, and orientation data are captured with a gyroscope in all XYZ Euler angles. In this work, sensor data were gathered with RealTrack Wimu wearable devices [RealTrack, 2018], which generate GPS/Orientation data at 100/10 samples per second respectively.
· Field-domain, which expresses all variables in terms of a fixed two-dimensional football field, where the top-left corner is the origin.
Once data are gathered and synchronized from different sources, two possible scenarios are faced:
· The complete case, in which all variables (i1, i2, i3, s4, s5) are available. Note that both image- and sensor-data include unique identifiers, which are easy to match by inspecting a small subset of frames.
· The semi-complete case, where only part of the information is available (i1,s4,s5). In order to estimate the missing pieces (i2, i3) and match data across domains, a sequential pipeline is proposed in Section 2.2.
In this article, both a complete and a semi-complete datasets are used, each one containing data from single games. In particular, the complete dataset contains a full game of F.C. Barcelona's Youth team recorded with a tactical camera with almost no panning and without zoom; this dataset will be

named FCBDS. The semi-complete dataset contains a full preseason match of CSKA Moscow's professional team, recorded in a practice facility (without fans) with a single static camera that zooms quite often and has severe panning. Similarly, this second dataset will be named CSKADS. Furthermore, intersection and corner image-coordinates were manually identified and labelled in more than 4000 frames of CSKADS (1 frame every 37, i.e. 1.5 seconds), with a mean of 8.3 ground-truth field-spots per frame (34000 annotations).
2.1 Homography Estimation
Since the reference system of the image- and the sensordomain is not the same, corners' positions (or line intersections) are used to translate all coordinates into the field-domain. On the one hand, obtaining field locations in the sensor domain is pretty straightforward: since its gathered coordinates are expressed with respect to the universal latitude/longitude system, the corners' locations are fixed. By using online tools such as the Satellite View of Google Maps, and by accurately picking field intersections, the corners' latitude and longitude coordinates are obtained. On the other hand, corner's positions in the image domain (in pixels) depend on the camera shot and change across the different frames; although several literature methods [Citraro et al., 2020] can be implemented in order to get the location of these field spots or the camera pose, our proposal leverages homographies computed from manual annotations. From now on, the homography that maps latitude/longitude coordinates into the field will be named HSF , whereas the one that converts pixels in the image into field coordinates will be named HIF . The complete homography-mapping process is illustrated in Figure 1.
2.2 Automatic Dataset Completion
In this Subsection, the complete process to convert a semi-complete dataset into a complete one is described. It has to be remarked that the aim is to detect players in the image-domain and to match them with sensor data, hence pairing orientation and identified bounding boxes. Note that this procedure has been applied to CSKADS, which did not contain ground-truth data in the image-domain. The proposed pipeline is also displayed in Figure 2.

Figure 2: Proposed pipeline to match sensor orientation data with bounding boxes. Different input sources are merged: (top, image-domain) video footage, which is used for player detection and jersey filtering; the resulting bounding boxes are later mapped into the field-domain. (middle, image-domain) Corner's location, which is used for building the corresponding mapping homographies, and (bottom, sensor-domain) ground-truth data, which are also mapped into the field-domain. Finally, players in the 2D-domain are matched through pairwise distances.

Player Detection: the first step is to locate players' position in the image. In order to do so, literature detection models can be used, such as OpenPose [Cao et al., 2017] (used in this research) or Mask R-CNN [He et al., 2017]. Once identified all different targets in the scene, detections are converted into bounding boxes. Note that this step does not exploit any temporal information across frames. Jersey Filtering: since sensor data are only acquired for one specific team, approximately half of the detected bounding boxes (opponents) are filtered out. Given that the home/away teams of football matches are required to wear distinguishable colored jerseys, a simple clustering model can be trained. Specifically, by computing and by concatenating quantized versions of the HSV / LAB histograms, a single 48-feature vector is obtained per player. Having trained a K-Means model, with K = 3, boxes with three different types of content are obtained: (1) home team, (2) away team, and (3) outliers. Mapping: in order to establish the same reference system for both sensor and image data, all tracking coordinates are mapped into the field domain. More specifically, cornerbased homographies HSF and HIF are used; in the latter, since we are dealing with bounding boxes, the only point being mapped for each box is the middle point of the bottom boundary of the box. Matching: once all points are mapped into the field-domain, a customized version of the Hungarian method [Kuhn, 1955] is implemented, thus matching sensor and image data in terms of pairwise field-distances.
3 Proposed Method
In this Section, the complete adaptation and fine-tuning procedure of a state-of-the-art convolutional neural network are detailed, hence resulting in a model capable of estimating

Figure 3: Orientation references in the field-domain.
body orientation directly from bounding boxes containing players. By default, all orientations are expressed in the fielddomain reference system. In this bi-dimensional field it can be assumed that 0º / 90º / 180º / 270º are the corresponding orientations of players facing towards the right / top / left / bottom sides of the fields, respectively, as shown in Figure 3.
3.1 Angle Compensation The apparent orientation of each player is influenced by the current image content, which is drastically affected by the camera pose and its orientation. This means that, if a bounding box of a particular player is cropped without taking into account any kind of field-reference around him/her, it is not possible to obtain an absolute orientation estimation. As displayed in Figure 4 (top), the appearance of three players oriented towards the same direction (0 degrees) can differ a lot. Since the presented classification model only takes a bounding box as input, we propose to compensate angles a priori,

Figure 4: (Top) Three players oriented towards 0º can look really different depending on the camera pose and orientation. (Bottom) Proposed technique for angle compensation: (left) detected player together with his orientation {red} and apparent zero-vector {cyan}; (middle-left) mapped apparent zero-vector in the field-domain {dashed axes - apparent reference system, continous axes - absolute reference system} (middle-right) Applied compensation on the original orientation {purple}; (right) resulting compensated absolute orientation {purple}.

thus assuming that all orientations have been obtained under the same camera pose; i.e. the pink camera displayed in Figure 3. For instance, if the full chest of a player is spotted in a particular frame, its orientation must be approximately 270, no matter what the overall image context is. In order to conduct this compensation, as seen in the bottom row of Figure 4, the orientation vector of the player is first mapped into the field-domain. Then, the apparent zerovector is considered in the image-domain; for the reference camera, i.e. the pink one in Figure 3, this vector would point to the right side of the field whilst being parallel to the sidelines. By using HIF , the apparent zero-vector is mapped into the field-domain, and the corresponding compensation is then found by computing the angular difference between the mapped apparent zero-vector and the reference zero-vector in the field-domain. According to Figure 3, this difference indicates how much does the orientation vector differ from the apparent zero-vector. Formally, for a player i with non-compensated orientation i at position Pi = (Pi,x, Pi,y) and being the (unitary) apparent zero-vector Z described by (1, 0), another point is defined towards the zero direction:

Pi0 = Pi + Z = (Pi,x + 1, Piy)

(1)

Both points Pi and Pi0 are mapped into the field domain by using HIF , thus obtaining their 2D position Fi and Fi0, respectively. The final compensated angle is then found as:

---

i = i - (FiFi0),

(2)

--- where expresses the angle of the vector FiFi0 with respect to the reference zero-vector.

3.2 Network
Once all bounding boxes have an associated compensated body-orientation value, the model is set to be trained. In this work, orientation estimation has been approached as a classification task, where each bounding box is classified within a certain number of orientation bins. As detailed in Section 4, orientation data are grouped into 12 bins, each one containing an orientation range of 30 degrees (e.g. bin 1 goes from 0º to 30º, bin 2 form 30º to 60º, until bin 12, which goes from 330º to 360º). Consequently, the above-mentioned bounding boxes in the image-domain were automatically labeled with their corresponding class according to their compensated orientation. Another reason for grouping similar angles into the same class is the noisy raw orientation signals generated by the EPTS devices. In particular, the chosen network to be fine-tuned in this research is a VGG-19 [Simonyan and Zisserman, 2014]; this type of network has also been used as a backbone in existing literature methods such as OpenPose [Cao et al., 2017]. However, in order to further analyze and to justify our choice, alternative results are shown in Section 4 when using Densenet [Huang et al., 2017]. The original architecture of VGG-19 is composed of 5 convolutional blocks -each one containing either 2 or 4 convolutional layers-, and a final set of fully connected layers with a probability output vector of 1000 classes. For the presented experiments, as seen in Figure 5, the architecture adaptation and the proposed method consists of: (1) changing the dimensions of the final fully-connected layer, thus obtaining an output with a length equal to 12, the desired number of classes, (2) freezing the weights of the first couple of convolutional blocks, (3) re-training the convolutional layers of the third block and the fully connected layers of the classifier, and (4) omitting both the fourth and fifth convolutional blocks. By visualizing the final network weights with

Figure 5: Proposed architecture for fine-tuning a VGG according to the main blocks of the original network.

Figure 6: Obtained ScoreCam responses. While the 1st block detects mainly edges and shapes, the 3rd one has a high response over the upper-torso of players. The last row shows how the 4th block learns specific features that have little to with orientation.
Score-CAM [Wang et al., 2020] (Figure 6), it can be spotted how the most important body parts regarding orientation (upper-torso) are already being vital for the sake of classification after the third block; in fact, the responses of the fourth block do not provide useful information in terms of orientation. Therefore, omitting blocks 4 and 5 is a safe choice to have an accurate model whilst decreasing the total number of parameters to be trained. Let us finally remark that bounding boxes' values are converted into grayscale, thus improving the overall capability of generalization, since the model will not be learning the specific jersey colors, which seemed to be one of the drawbacks in [Arbue´s-Sangu¨esa et al., 2020b]. In terms of data augmentation, brightnes, and contrast random changes are performed for all boxes in the training set.
3.3 Cyclic Loss
An important aspect of the training process is the definition of the loss function. A priori, state-of-the-art loss functions such as binary cross-entropy could be a valid resource, but in general classification scenarios, the order and the distance within classes is not taken into account. Nonetheless, in this particular scenario, we have 12 ordered-cyclic classes and a distance between them that can be well-defined. Besides, in this classification problem, since similar orientations have been grouped into bins, enforcing a one-hot encoding is not the best solution. For example, imagine a player P1 oriented towards 31º and another P2 oriented towards 59º; both players are included in the second bin, which encompasses all orientations between 30-60. With one-hot encoding, it would

be assumed that since both P1 and P2 are in the second bin,

both of them have the same orientation (45º). However, al-

ternatives such as soft labels [Diaz and Marathe, 2019] can

describe the players' class as a mixture; in the given exam-

ple, the soft labels of P1/P2 would indicate that these players are right between the first-second/second-third bins, respec-

tively. The other challenge to be solved is the need for this

loss function to be cyclic, as the first bin (number 1, 0-30º)

and the last one (12, 345-360º) are actually really close.

Let {b1, b2, ..., b12} be the set of orientation classes and let  = {r1, r2, . . . , r12} be the set such that each rj denotes the central angle of bin bj, for all j  {1, . . . , 12}. Then, for a player i with compensated ground-truth orientation i, the

soft labels representing the ground-truth probability distribu-

tion is defined as the vector with coordinates:

yij =

exp(-(i, rj))

K k=1

exp(-(i,

rk

))

,

for j

=

1, . . . , 12

(3)

where  is cyclic distance between the ground-truth player's

orientation i and the angle corresponding to the jth bin, rj:

(i, rj)

=

min(|i

-

rj|, 360 90

-

|i

-

rj |)2 .

(4)

Let us denote as xi the estimated probability distribution of orientation of player i obtained by applying the softmax func-

tion to the last layer of the network. Finally, our loss is the

cross-entropy between xi and the ground-truth soft labels yi.

3.4 Training Setting
As mentioned in Section 2, bounding boxes were gathered from the only two games at our disposal (Section 2), both recorded under different camera shot conditions. Consequently, as seen in Figure 7, the content inside both bounding boxes differs a lot: while in F CBDS players are seen from a tactical camera and have small dimensions, players in CSKADS are spotted from a camera that is almost in the same height as the playing field, thus resulting in big bounding boxes. Although all bounding boxes are resized as a preprocessing stage of the network, the raw datasets suffer from concept drift [Webb et al., 2016].
The proposed solution in this article is to build an unbalanced-mixed train set; that is, merging bounding boxes from both datasets with an unbalanced distribution in the train set, whilst using the remaining instances from F CBDS and CSKADS on their own to build the validation and the test set, respectively. In particular, the presented experiments have been carried out with a 90-10 distribution in the training set; for each class, a total of 4500 bounding boxes corresponding to the first half of the games- are included, where 4000 of them are obtained from F CBDS and the 500 remaining ones are gathered from CSKADS. Both the validation and test set include 500 samples per class, all of them belonging to data from the second half.

Figure 7: Resized bounding boxes of both datasets; several artifacts can be spotted in F CBDS (e.g. JPEG, ringing, aliasing).

t12 t24 t12CE t12nC t12den tCV

MEAEv 17.37 13.13 22.34 21.47 15.22 -

MDAEv 9.90 7.70 17.00 14.16 10.46 -

MEAEt 18.92 24.34 28.98 31.75 25.27 38.23

MDAEt 11.60 13.01 23.00 24.54 17.29 32.09

Table 1: Obtained results in all experiments, expressed in terms of the mean/median absolute error, both in the validation and test set.

4 Results
The obtained classification results will be shown in terms of angular difference and confusion matrices. Nonetheless, it has to be remarked that, when grouping orientations, an intrinsic error is introduced: assuming that each bin contains d degrees, and that a players' orientation is equal to the central bin value, properly classified players may have an associated absolute error up to d/2. The results of six different experiments are shown in Table 1: (1) t12 and (2) t24 use a VGG architecture that classifies into 12 and 24 orientation bins, respectively, both trained with compensated angles; (3) t12nC uses the same network as in t12 but trained without angle compensation, and (4) t12den uses a DenseNet architecture -fine-tuning of the fourth dense block- that performs a 12-bin classification, (5) t12CE uses binary cross-entropy instead of the proposed cyclic loss, and (6) t12CV shows the performance of the existing state-of-the-art method [Arbue´sSangu¨esa et al., 2020b] (12 bins as well). Table 1 contains the mean absolute error (MEAE) and the median absolute error (MDAE) of the estimated angles in each experiment. As it can be spotted, the test of 12 classes is the one providing the most reliable test results in terms of generalization; in particular, classifying orientation into 24 classes produces better results in the validation set, but seemingly the model overfits and learns specific features that do not seem to generalize properly. Moreover, the model benefits from the cyclic loss implementation, as binary cross-entropy introduces errors both in the validation and in the test set due to the unknown distance between classes and the non-cyclic angular behavior. Actually, the obtained boost with this cyclic loss is displayed in the confusion matrices of Figure 8. The addition of angle compensation also proves to be vital, especially in the test set, where the corresponding video footage contained a lot of panning and zooming. Besides, the performance of DenseNet does not seem to generalize either; however, it is likely that with an exhaustive trial-error procedure of freezing weights of particular layers and performing small changes in the original DenseNet structure, this architecture should be able to generalize as well. Finally, the existing computer-vision-based method, implemented without the model in charge of the coarse corroboration, performs the worst, obtaining more than 32 degrees in both absolute errors.
5 Conclusions
In this article, a novel DL model to estimate the orientation of football players is presented. More concretely, the fine-tuned model learns how to classify players' crops into orientation

Figure 8: First and last rows of the obtained confusion matrix (test set) when using the (top) proposed cyclic and (bottom) binary crossentropy as a loss function (t12 and t12CE respectively).
bins. The core of this method combines a VGG structure with frozen and re-trained layers, an angle compensation strategy to get rid of the camera behavior, and a cyclic loss function based on soft labels that take the intra-class distance into account. The obtained results outperform (by a large margin of more than 20 degrees) the existing state-of-the-art computer-vision method with a MDAE of 11.60 degrees in the test set. Moreover, since complete datasets are difficult to gather, a sequential-based pipeline has also been proposed, which fuses data from different domains in order to establish the ground truth orientation of the player (sensor-domain) in each bounding box (image-domain). The main limitation of the presented model is that only two different games were used in the given dataset, as ground-truth sensor data (together with high-quality frames) are difficult to obtain. This research shows that even with unbalanced training sets it is possible to train a model with certain generalization capabilities, hence promising results should be obtained with a more varied and balanced dataset in terms of different games. As future work, apart from (a) analyzing the model extension to other football datasets and (b) testing the same approach in a regression-based fashion, its potential intra-sport generalization will be studied as well. Besides, the inclusion of fine-grained orientation into existing EPV models could lead to better performance.
Acknowledgements
The authors acknowledge partial support by MICINN/FEDER UE project, ref. PGC2018-098625B-I00, H2020-MSCA-RISE-2017 project, ref. 777826 NoMADS, EU H-2020 grant 952027 (project AdMiRe), and RED2018-102511-T. Besides, we also acknowledge RealTrack Systems' (in particular, Carlos Padilla's) and F.C. Barcelona's data support.

References
[Arbue´s-Sangu¨esa et al., 2020a] Adria` Arbue´s-Sangu¨esa, Adrian Martin, Javier Ferna´ndez, Coloma Ballester, and Gloria Haro. Using player's body-orientation to model pass feasibility in soccer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 886­887, 2020.
[Arbue´s-Sangu¨esa et al., 2020b] Adria` Arbue´s-Sangu¨esa, Adria´n Mart´in, Javier Ferna´ndez, Carlos Rodr´iguez, Gloria Haro, and Coloma Ballester. Always look on the bright side of the field: Merging pose and contextual data to estimate orientation of soccer players. In Proceedings of the International Conference on Image Processing, 2020.
[Bransen and Haaren, 2020] Lotte Bransen and Jan Van Haaren. Player chemistry: Striving for a perfectly balanced soccer team, 2020.
[Cao et al., 2017] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. Realtime multi-person 2d pose estimation using part affinity fields. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7291­7299, 2017.
[Cioppa et al., 2019] Anthony Cioppa, Adrien Deliege, Maxime Istasse, Christophe De Vleeschouwer, and Marc Van Droogenbroeck. Arthus: Adaptive real-time human segmentation in sports through online distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 0­0, 2019.
[Cioppa et al., 2020] Anthony Cioppa, Adrien Deliege, Silvio Giancola, Bernard Ghanem, Marc Van Droogenbroeck, Rikke Gade, and Thomas B Moeslund. A context-aware loss function for action spotting in soccer videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13126­ 13136, 2020.
[Citraro et al., 2020] Leonardo Citraro, Pablo Ma´rquezNeila, Stefano Savare`, Vivek Jayaram, Charles Dubout, Fe´lix Renaut, Andre´s Hasfura, Horesh Ben Shitrit, and Pascal Fua. Real-time camera pose estimation for sports fields. Machine Vision and Applications, 31(3):1­13, 2020.
[Decroos et al., 2019] Tom Decroos, Lotte Bransen, Jan Van Haaren, and Jesse Davis. Actions speak louder than goals: Valuing player actions in soccer. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 1851­1861, 2019.
[Delie`ge et al., 2020] Adrien Delie`ge, Anthony Cioppa, Silvio Giancola, Meisam J Seikavandi, Jacob V Dueholm, Kamal Nasrollahi, Bernard Ghanem, Thomas B Moeslund, and Marc Van Droogenbroeck. Soccernetv2: A dataset and benchmarks for holistic understanding of broadcast soccer videos. arXiv preprint arXiv:2011.13367, 2020.
[Diaz and Marathe, 2019] Raul Diaz and Amit Marathe. Soft labels for ordinal regression. In Proceedings of the

IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4738­4747, 2019.
[Fernandez and Bornn, 2018] Javier Fernandez and Luke Bornn. Wide open spaces: A statistical technique for measuring space creation in professional soccer. In Sloan Sports Analytics Conference, volume 2018, 2018.
[Ferna´ndez et al., 2019] Javier Ferna´ndez, Luke Bornn, and Dan Cervone. Decomposing the immeasurable sport: A deep learning expected possession value framework for soccer. In 13 th Annual MIT Sloan Sports Analytics Conference, 2019.
[Giancola et al., 2018] Silvio Giancola, Mohieddine Amine, Tarek Dghaily, and Bernard Ghanem. Soccernet: A scalable dataset for action spotting in soccer videos. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 1711­1721, 2018.
[He et al., 2017] Kaiming He, Georgia Gkioxari, Piotr Dolla´r, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961­2969, 2017.
[Huang et al., 2017] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700­4708, 2017.
[Kuhn, 1955] Harold W Kuhn. The hungarian method for the assignment problem. Naval research logistics quarterly, 2(1-2):83­97, 1955.
[Llana et al., 2020] Sergio Llana, Pau Madrero, and Javier Ferna´ndez. The right place at the right time: Advanced off-ball metrics for exploiting an opponent's spatial weaknesses in soccer. In Proceedings of the 14th MIT Sloan Sports Analytics Conference, 2020.
[MetricaSports, 2020] MetricaSports. Metrica Sports - Football Open Data. https://metrica-sports.com/ football-open-data/, 2020. Accessed: 23-04-21.
[Quiroga et al., 2020] Julian Quiroga, Henry Carrillo, Edisson Maldonado, John Ruiz, and Luis M Zapata. As seen on tv: Automatic basketball video production using gaussianbased actionness and game states recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 894­895, 2020.
[Ramakrishna et al., 2014] Varun Ramakrishna, Daniel Munoz, Martial Hebert, James Andrew Bagnell, and Yaser Sheikh. Pose machines: Articulated pose estimation via inference machines. In European Conference on Computer Vision, pages 33­47. Springer, 2014.
[RealTrack, 2018] RealTrack. RealTrack Systems. http: //www.realtracksystems.com/, 2018. Accessed: 23-04-21.
[Sanford et al., 2020] Ryan Sanford, Siavash Gorji, Luiz G Hafemann, Bahareh Pourbabaee, and Mehrsan Javan. Group activity detection from trajectory and video data in soccer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 898­899, 2020.

[Sha et al., 2020] Long Sha, Jennifer Hobbs, Panna Felsen, Xinyu Wei, Patrick Lucey, and Sujoy Ganguly. End-toend camera calibration for broadcast videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13627­13636, 2020.
[Simonyan and Zisserman, 2014] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.
[SkillCorner, 2020] SkillCorner. SkillCorner - Open Data. https://github.com/SkillCorner/opendata, 2020. Accessed: 23-04-21.
[Spearman et al., 2017] William Spearman, Austin Basye, Greg Dick, Ryan Hotovy, and Paul Pop. Physics-based modeling of pass probabilities in soccer. In Proceeding of the 11th MIT Sloan Sports Analytics Conference, 2017.
[Spearman, 2018] William Spearman. Beyond expected goals. In Proceedings of the 12th MIT sloan sports analytics conference, pages 1­17, 2018.
[StatsBomb, 2020] StatsBomb. StatsBomb - Open Data. https://github.com/statsbomb/open-data, 2020. Accessed: 23-04-21.
[Sto¨ckl et al., 2022] Michael Sto¨ckl, Thomas Seidl, Daniel Marley, and Paul Power. Making offensive play predictable - using a graph convolutional network to understand defensive performance in soccer. In Proceedings of the 15th MIT Sloan Sports Analytics Conference, 2022.
[Wang et al., 2020] Haofan Wang, Zifan Wang, Mengnan Du, Fan Yang, Zijian Zhang, Sirui Ding, Piotr Mardziel, and Xia Hu. Score-cam: Score-weighted visual explanations for convolutional neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 24­25, 2020.
[Webb et al., 2016] Geoffrey I Webb, Roy Hyde, Hong Cao, Hai Long Nguyen, and Francois Petitjean. Characterizing concept drift. Data Mining and Knowledge Discovery, 30(4):964­994, 2016.
[Wei et al., 2016] Shih-En Wei, Varun Ramakrishna, Takeo Kanade, and Yaser Sheikh. Convolutional pose machines. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 4724­4732, 2016.

