arXiv:2106.00978v1 [cs.AI] 2 Jun 2021

A Span Extraction Approach for Information Extraction on Visually-Rich Documents
Tuan-Anh D. Nguyen, Hieu M. Vu, Nguyen Hong Son, and Minh-Tien Nguyen
Cinnamon AI, 10th floor, Geleximco building, 36 Hoang Cau, Dong Da, Hanoi, Vietnam.
{tadashi,ian,levi,ryan.nguyen}@cinnamon.is
Abstract. Information extraction (IE) from visually-rich documents (VRDs) has achieved SOTA performance recently thanks to the adaptation of Transformer-based language models, which demonstrates great potential of pre-training methods. In this paper, we present a new approach to improve the capability of language model pre-training on VRDs. Firstly, we introduce a new IE model that is query-based and employs the span extraction formulation instead of the commonly used sequence labelling approach. Secondly, to further extend the span extraction formulation, we propose a new training task which focuses on modelling the relationships between semantic entities within a document. This task enables the spans to be extracted recursively and can be used as both a pre-training objective as well as an IE downstream task. Evaluation on various datasets of popular business documents (invoices, receipts) shows that our proposed method can improve the performance of existing models significantly, while providing a mechanism to accumulate model knowledge from multiple downstream IE tasks.
Keywords: Information Extraction · Span Extraction · Pre-trained model · Visually-rich document.
1 Introduction
Information Extraction (IE) for visually-rich documents (VRDs) is different from plain text documents in many aspects. These documents contain sparser textual content, have strictly defined visual structures and complex layouts which is not present in plain text documents. Due to these characteristics, existing works on IE for VRDs mostly employ Computer Vision (CV) [1] and/or graph-based [8,6,10] techniques. Recently, the language modelling capability of BERT provides more perspectives on VRDs from the Natural Language Processing (NLP) point of view [12,11,13,4,3,14].
LayoutLM [12] emerges as a simple yet potential approach. It can be seen as the equivalent of BERT for VRDs, since it extends the BERT architecture by adding 2D positional embedding and visual embedding to BERT's input embedding [2]. To train the model, the Masked Visual Language Modelling (MVLM) objective [12] is used, which is also an adaptation of positional embedding and

2

Tuan-Anh et. al.

visual embedding to similar objective [2] of BERT. More recently, LayoutLMv2 [11] was introduced with more focus on better leveraging 2D positional and visual information. There is also the addition of two new pre-training objectives that are designed for modeling image-text correlation.
While the formulation of LayoutLM is very efficient to model multi-modal data like visually-rich documents, it bears a few drawbacks regarding practicality: (1) pre-training a language model requires a large amount of data, and unlike plain text documents, visually-rich document data is not readily available in such large quantity, especially for languages other than English; (2) previous method mostly utilizes sequence labelling formulation for IE settings, but we argue that such method might not work well for short entities due to imbalance class and training loss [9,5] (3) downstream IE tasks requires explicit definition of entity tags/classes which hinders application of same IE model on multiple datasets to accumulate model knowledge.
To address the aforementioned problems, we (1) propose a span extraction method for IE in visually-rich documents, which combines the fine-grained entity extraction capability of QA-based span extraction with the multi-value extraction capability using a novel recursive relation predicting scheme (2) introduce a pre-training procedure based on existing semantic entities from IE datasets to apply LayoutLM more effectively on both English and non-English settings. This new method also allows seamless accumulation of knowledge from downstream tasks through one common formulation across datasets.

2 Information Extraction as Span extraction
The most common approach for extracting information from visually-rich documents is sequence labelling which assigns a label for each token of the document text [12,11,13,4,3,14]. Even though this solution showed promising results, we observed an important disadvantage of this type of formulation when applying on some special cases. The sequence labelling tends not to work well when some entity types have only a small amount of samples or when the targeted entities are short spans of text in a dense documents. These are both resulted in an imbalance of token labels which we argue is the reason for the decrease in performance. Moreover, the sequence tagging formulation requires explicit definition of token classes, which diminishes the reusability of the model on other task.
To overcome this challenge, we decided to follow the span extraction approach, also known as Extractive Question Answering (QA) in NLP context [2]. Different from sequence labelling models which assign label for every token based on their embedding, QA models predict the start position and the end position of the corresponding answer for each query. In IE problem, a query represents a required field/tag that we want to extract from original document context. Each query is represented as a learnable embedding vector and kept separately from main language model encoder.

A Span Extraction Approach for Information Extraction on VRDs

3

w0 w1

wi

wn

Pretrained LM

h0 h1

hi

hn

Hidden size
T

Query/context interaction

Hidden size

Start/end

Fig. 1. Span extraction model.

Our span extraction model is described in Figure 1 with D = {w1, w2, ..., wn} is the input sequence. Let H = [h1, ..., hj, ..., hn] = f (D|) is the hidden rep-
resentation of the input sequence produced by the pretrained encoder f () with hi  Rc is the representation vector of the token ith, T  Rc is the vector repre-
sentation of the query (field embedding) and c is the hidden size of the encoder,
the start and end position of the corresponding answer is calculated as:

start, end = g(T, H)

(1)

with g() is the query-context interaction layer. To calculate the span extraction loss, the sof tmax operation is applied over
all tokens in the context instead of over the token classes for each token as in sequence tagging. Softmax loss on all sequence tokens eliminates the class imbalance problem for shorter answer entities. The separation of question (query) embedding and context embedding also open the potential of accumulating and reusing them on different datasets.

3 Pre-training objective with Recursive Span extraction
3.1 Transferred LayoutLM pre-training to other languages
This section describe some effective transfer methods for pre-training of 2D language model (LayoutLM) on other languages. Pre-training a language model from scratch with MLM objective normally requires millions of data and can take days to train. While such amount of data can be acquired in plain text, visually rich data does not naturally exist in large quantity due to the lack of word-level OCR annotations. Since pre-trained weights are available for English language only, applying LayoutLM on other languages becomes less practical. To overcome this, we propose a simple transferred pre-training procedure for LayoutLM that takes advantages of available pre-trained weights of BERT and transfers their semantic modelling capability to LayoutLM.

4

Tuan-Anh et. al.

Overcoming data shortage. First, the model's word embedding layer is initialized from a pre-trained BERT of the targeted language and the rest of its layers (including positional embedding layers, encoder layers and the MVLM head) are initialized from the LayoutLM for English which is publicly available. After that, the model is trained with the MVLM objective using a much smaller dataset (about 17000 samples in our case) for 100 epochs. The final model can then be used to fine-tune on downstream tasks such as sequence labeling, document classification or question answering.
The word embedding layer maps plain text tokens into a semantically meaningful latent space, and while the latent space can be similar, the mapping process itself is strictly distinct from language to language. Oppositely, we argue that the spatial information is language-independent and can be useful among languages with similar reading order. Similarly, the encoder layers are taken from the LayoutLM weights to leverage its ability of capturing attention from both semantic and positional inputs. Our experiments on internal datasets show that this transferred pre-training setting yields significantly better results than not using word embedding weights from BERT. We plan to verify our approach on the recently introduced XFUN dataset [13] when it is publicly available.

Overcoming inadequate annotations. To make up for the lack of word-level OCR annotations, we use an OCR reader to extract line-level annotations. Then, word-level annotations are approximated by dividing the bounding box of each line proportionally to the length of the words in that line.

3.2 Span extraction pre-training
To resolve the limitation of training data requirement when pre-training LayoutLM model, we present a new pre-training objective that re-use existing entities' annotations from downstream IE tasks. Our pre-train task is based on a recursive span extraction formulation that extends traditional QA method to work with multiple answer spans from a single query.

Recursive Span extraction. As opposed to sequence labelling, our span extraction formulation allows flexible query (field) definition independent of the context, which enable knowledge accumulation from multiple datasets with different set of entity types. However, one limitation of our span extraction model described in Figure 1 is that the model can not extract more than one answer (entity) for each query, which is arguably crucial in many IE use cases. We, therefore, propose a strategy to extend the capability of the model by using recursive link decoder mechanism which is described in Figure 2.
Using the same notation as in Section 2, our recursive mechanism is as follows:

s1, e1 = g(T, H)

(2)

si+1, ei+1 = g(hsi , H)

(3)

A Span Extraction Approach for Information Extraction on VRDs

5

Context

T

First answer

Second answer

Third answer

Fig. 2. The recursive span extraction mechanism.

Fig. 3. Recursive link prediction demonstrated on a receipt example [7]. (green and blue arrow denotes start and end position of answer span corresponding to the green query tokens' bounding-box.
The first answer span is extracted using the the vector representation T of the query and the hidden representation H of the context (Eq. 2). After that, the hidden representation corresponding to the start token of the i-th answer is used as the query vector to extract the (i + 1)-th answer recursively (Eq. 3). This recursive decoding procedure will stop if si = ei = 0 or when si = sj, ei = ej with j < i. Using this method, we can decode all answer spans that belong to the same query/entity tag in a document context.
Pre-training with semantic entities. The recursive span extraction task forms a generic relation structure between entities in visually-rich documents, which reinforces the language model capability to model document context and spatial information. Sample visualization of a document structure can be viewed in Figure 3. By forcing the model to reconstruct the relation between continuous entities belongs to the same field, we hypothesize that the model can produce better performance when being fine-tuned on downstream IE tasks. Experiment results in Section 4 will validate our hypothesis.

6

Tuan-Anh et. al.

With the proposed query-based mechanism and recursive span decoding, we can pre-train the LayoutLM model on multiple IE datasets with different keyvalue fields seamlessly. Recall that the query embedding is built separately from main pre-trained model, the main LayoutLM encoder will focus on modelling document context and interactions between its entities. We named this pretraining objective as Span extraction semantic pre-training, which allows us to pretrain the LayoutLM model with a small-sized IE-annotated dataset instead of large scale document data such as [11].

4 Experiments
4.1 Experiment setup
Dataset. We use two internal self-collected datasets: one for pre-training the language model and one for evaluating IE performance as downstream task. We also evaluate the performance of our proposed method on a public dataset consists of shop receipts [7], which represents a typical example of document understanding task in commercial systems. Table 1 shows the statistics of all datasets used in our experiments.

Table 1. 3 VRD document datasets. Italic is internal.

Data

#train #test #words/doc #fields

Japanese Pre-training BizDocs 5170

-

503

123

Japanese Internal Invoice

700 300

421

25

CORD

800 100

42

30

Japanese Pre-training BizDocs is a self-collected dataset consist of 5k document images and their OCR transcriptions. The dataset contains various types of business document such as: financial report, invoice, medical receipt, insurance contract, . . . Each document type is annotated with an unique set of key-value fields which represents the important information that end-users want to extract. There are over 100 distinct fields from all documents and around 80k labeled entities. We use this dataset to pre-train our LayoutLM model with the proposed span-based objectives (Section 3.2). Japanese Invoice is another Invoice documents dataset that disjoints with the aforementioned pre-training data. The data consists of 1,200 document images of Shipping Invoice and is annotated with 25 type of key-value fields. We divide the data into 700/200/300-sample set that correspond the training/validation/testing data respectively. CORD [7] consists of Indonesian receipts collected from shops and restaurants. The dataset includes 800 receipts for training, 100 for validation, and 100 for testing. An image and a list of OCR annotations are provided alongside each receipt. There are 30 fields defined with most important classes are: store information, payment information, menu, total. The objective of the model is to label each semantic entity in the document and extract their values (Information

A Span Extraction Approach for Information Extraction on VRDs

7

Extraction) from input OCR annotations. The dataset is available publicly at https: // github. com/ clovaai/ cord .

Training and Inference. The English-based LayoutLM model starts from pre-trained weight of [12] which is provided alongside with the paper, dubbed as EN-LayoutLM-base. Our choice is the LayoutLM-base version from the original paper (113M parameters) due to hardware limitations. Another Japanese-based model (JP-LayoutLM-base) is pre-trained following the procedure described in Section 3.1 with the same configuration as LayoutLM-base, using MVLM task on our internal data. This model is intended to be used with Japanese datasets.
To conduct our experiments, we firstly pre-train the JP-LayoutLM-base model on the Japanese Pre-training BizDocs dataset with the proposed span-extraction objectives. Then, we perform fine-tuning on two IE datasets: Japanese Invoice and CORD with the pre-trained weight JP-LayoutLM-base and EN-LayoutLMbase respectively. We compare the results of span-extraction with sequence labeling formulation on these downstream IE tasks. Also, we measure the fine-tuning result with LayoutLM-base model without span-based pre-training to demonstrate the effect of the proposed method.

Evaluation metrics. We adopt the popular entity-level F1-score (micro) for evaluation, which is commonly used in previous studies [12,11,4,3]. Entity F1score (macro) is used as the second metric to represent the overall result on all key-value fields, since it emphasizes on model performance on some rare fields/tags. This situation is commonly appeared in industrial setting where data collection is costly for some infrequent fields.

4.2 Results and Discussion
CORD. We report the results on the test and dev sub-sets of CORD in Table 3 and Table 2 respectively. Span extraction method consistently improves the performance of IE in compare to existing sequence labeling method. We can observe 1% - 3% improvement in both metrics which is significant given competitive base result of default LayoutLM model. Interestingly, when compares to the result from LayoutLMv2 [11], our model performs better than both LayoutLMlarge and LayoutLM-base-v2, even though we only use a much lighter version of the model. This demonstrates the effectiveness of our span extraction formulation for downstream IE tasks. Note that the span extraction formulation is independent from model architecture, thus we can expect the same improvement when starting from more complex pre-trained models. We leave this experiment as future extension of our work.
Another finding from the results is that F1-score macro is increased by an significant amount alongside with entity-level score (micro) in both subsets. This illustrates that our method span-extraction helps improve performance on all fields/tags, not only common tags that has more entities' annotation.

8

Tuan-Anh et. al.

Table 2. Results on CORD (test) dataset(*: result from original paper [11])

Methods

F1-score (macro) F1-score (micro) #Parameters

EN-LayoutLM-base (seq labeling) EN-LayoutLM-base (span extraction) EN-LayoutLM-large (seq labeling) * EN-LayoutLM-base-v2 (seq labeling) *

80.08 83.46
-

94.86 95.71 94.93 94.95

113M 113M 343M 200M

Table 3. Results on CORD (dev) dataset

Methods

F1-score (macro) F1-score (micro)

EN-LayoutLM-base (seq labeling) EN-LayoutLM-base (span extraction)

80.74 82.13

96.16 97.35

Japanese Invoice. Table 4 presents the result on Japanese Invoice data. We compare our method with sequence labeling formulation, which shows similar improvements in term of both F1-score metrics. Additionally, when adding spanbased pre-training task to the model, the performance of downstream IE task increases by another substantial amount (+1.2% in F1-score (micro)). The improvements illustrates that our pre-training strategy can effectively improve the IE performance, given only annotated semantic entities from other document types. This also means that we can accumulate future IE datasets when perform fine-tuning to further improve the base pre-train LayoutLM model.

Table 4. Ablation study on Japanese Invoice dataset

Methods

F1-score (macro) F1-score (micro)

JP-LayoutLM-base (seq labeling) JP-LayoutLM-base (span extraction) JP-LayoutLM-base + span pre-training (seq labeling) JP-LayoutLM-base + span pre-training (span extraction)

85.67 87.55 88.02
89.76

90.15 91.34 91.84
92.55

5 Conclusion
This paper introduces a novel span extraction formulation for Information Extraction task on VRD documents. A pre-training scheme based on semantic entities annotation from IE data is also presented. We demonstrate promising results in two IE datasets of business documents with the proposed approach. Our method can be applied to most existing pre-trained model for IE and opens new direction to continuously improve the performance of pre-trained model through downstream tasks fine-tuning.

A Span Extraction Approach for Information Extraction on VRDs

9

References

1. Dang, T.A.N., Thanh, D.N.: End-to-end information extraction by character-level embedding and multi-stage attentional u-net. In: BMVC. p. 96 (2019)
2. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018)
3. Hong, T., Kim, D., Ji, M., Hwang, W., Nam, D., Park, S.: {BROS}: A pre-trained language model for understanding texts in document (2021), https://openreview. net/forum?id=punMXQEsPr0
4. Hwang, W., Yim, J., Park, S., Yang, S., Seo, M.: Spatial dependency parsing for semi-structured document information extraction (2020)
5. Li, X., Sun, X., Meng, Y., Liang, J., Wu, F., Li, J.: Dice loss for data-imbalanced nlp tasks. arXiv preprint arXiv:1911.02855 (2019)
6. Liu, X., Gao, F., Zhang, Q., Zhao, H.: Graph Convolution for Multimodal Information Extraction from Visually Rich Documents (mar 2019), https://arxiv.org/ abs/1903.11279http://arxiv.org/abs/1903.11279
7. Park, S., Shin, S., Lee, B., Lee, J., Surh, J., Seo, M., Lee, H.: Cord: A consolidated receipt dataset for post-ocr parsing (2019)
8. Qian, Y., Santus, E., Jin, Z., Guo, J., Barzilay, R.: GraphIE: A Graph-Based Framework for Information Extraction (2018), http://arxiv.org/abs/1810. 13083
9. Tomanek, K., Hahn, U.: Reducing class imbalance during active learning for named entity annotation. In: Proceedings of the Fifth International Conference on Knowledge Capture. p. 105­112. K-CAP '09, Association for Computing Machinery, New York, NY, USA (2009). https://doi.org/10.1145/1597735.1597754, https://doi.org/10.1145/1597735.1597754
10. Vedova, L.D., Yang, H., Orchard, G.: An Invoice Reading System Using a Graph Convolutional Network 2, 434­449 (2019). https://doi.org/10.1007/978-3030-21074-8
11. Xu, Y., Xu, Y., Lv, T., Cui, L., Wei, F., Wang, G., Lu, Y., Florencio, D., Zhang, C., Che, W., Zhang, M., Zhou, L.: Layoutlmv2: Multi-modal pre-training for visuallyrich document understanding. In: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics (ACL) 2021 (August 2021)
12. Xu, Y., Li, M., Cui, L., Huang, S., Wei, F., Zhou, M.: Layoutlm: Pre-training of text and layout for document image understanding. In: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. pp. 1192­1200 (2020)
13. Xu, Y., Lv, T., Cui, L., Wang, G., Lu, Y., Florencio, D., Zhang, C., Wei, F.: Layoutxlm: Multimodal pre-training for multilingual visually-rich document understanding (2021)
14. Yu, W., Lu, N., Qi, X., Gong, P., Xiao, R.: PICK: Processing key information extraction from documents using improved graph learning-convolutional networks. In: 2020 25th International Conference on Pattern Recognition (ICPR) (2020)

