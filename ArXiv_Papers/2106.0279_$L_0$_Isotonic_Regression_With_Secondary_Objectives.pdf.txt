L0 Isotonic Regression With Secondary Objectives
Quentin F. Stout
qstout@umich.edu www.eecs.umich.edu/ ~qstout/
Abstract: We provide algorithms for isotonic regression minimizing L0 error (Hamming distance). This is also known as monotonic relabeling, and is applicable when labels have a linear ordering but not necessarily a metric. There may be exponentially many optimal relabelings, so we look at secondary criteria to determine which are best. For arbitrary ordinal labels the criterion is maximizing the number of labels which are only changed to an adjacent label (and recursively apply this). For real-valued labels we minimize the Lp error. For linearly ordered sets we also give algorithms which minimize the sum of the Lp and weighted L0 errors, a form of penalized (regularized) regression. We also examine L0 isotonic regression on multidimensional coordinate-wise orderings. Previous algorithms took (n3) time, but we reduce this to o(n3/2).
Keywords: isotonic regression, monotonic relabeling, L0, Hamming distance, ordinal response, distance to monotonicity

arXiv:2106.00279v1 [cs.DS] 1 Jun 2021

1 Introduction

There are many scenarios when one expects that an attribute of objects increases as a function of other attributes. For example, if people's weight is in {underweight, normal, overweight, obese}, one would expect that if the daily intake of fat is held constant then the weight increases as the carbohydrate intake increases, and similarly if the daily carbohydrate intake is held constant and the fat intake increases. No assumptions are made about the weight of a person with higher fat, but lower carbohydrate, vs. one with lower fat but higher carbohydrates. However, noisy datasets may have data points that violate these assumptions. Constructing a representation of the data which obeys the assumptions and has minimal changes to the data is generically known as isotonic regression. In this example the dependent variable, weight, is a label with no assumed metric, only an ordering, and the problem is more commonly known as monotonic relabeling.
As datasets become increasingly complex, often with only ordinal ordering on some attributes, such nonparametric regressions become increasingly more useful, but are often nonunique and difficult to compute. We will use optimizations based on secondary criteria to choose among the possible regressions, and give algorithms to find the resulting regressions.
More precisely, let V be a set with a partial order , and L = {(1), . . . , ()} be a set of labels, where (1) < . . . < (). A label function f is a mapping f : V  L. Given label functions f, g, the L0 distance between them, f - g 0, is
1 · (f (v) = g(v))
vV
This is also known as the Hamming distance, 0-1 loss, or Kronecker delta loss. It is similar to the well-known Lp distance when the labels are real values:

vV |f (v) - g(v)|p 1/p 1  p < 

maxvV |f (v) - g(v)|

p=

A label function g is monotonic (isotonic) if whenever u  v, for u, v  V , then g(u)  g(v), i.e., it is a weakly order-preserving mapping from V to L. Let p(f ) = min{||f - g||p : g is monotonic}. This is

1

the Lp distance of f to monotonicity, and producing an isotonic function matching this distance is known as isotonic regression. The term distance to monotonicity, without mention of p, typically means L0 distance, and often the goal is to quickly estimate it, rather than determine it exactly [7, 9, 12].
A monotonic function g is an Lp-optimal monotonic relabeling of label function f iff ||f - g||p = p(f ). For p  1 this requires that the labels are real values. L0-optimal monotonic relabelings need not be unique, e.g., on a sequence of just 2 points, if f is overweight, normal, then normal, normal and overweight, overweight are both optimal monotonic relabelings. Since optimal L0 monotonic relabelings are not always unique, in Section 3 we give algorithms for real-valued labels where the L1, L2, or L distances are used to select among them. We also introduce strong L0 optimality.
As will be shown in Section 2, most of the computational work involves a "violator dag" (a directed acyclic graph). To help keep track of which ordering or dag is being used, we use G = (V, E) to denote the initial dag, and G = (V , E) to denote the violator dag. We assume V is connected, and if it isn't then the construction is used on each component independently. Let n = |V |, m = |E|, n = |V | and m = |E|.
By multidimensional ordering we mean that the vertices are from some d-dimensional space, where each dimension has a linear ordering, and the vertices are ordered by component-wise ordering, i.e., x = (x1, . . . , xd)  y = (y1, . . . , yd) iff x = y and xi  yi for all 1  i  d. This is also known as the product order of linear orders. These orderings are the ones of most interest in many applications. Previous algorithms took (n3) time, but in Section 4 we use a Steiner dag (a dag with additional vertices) as the violator graph and reduce the time to o(n1.5).
For the label set L with  labels we assume only labels that occur in the data are included, and hence even if the labels are arbitrary real numbers   n. In many applications  might be quite small and independent of n, such as in the {underweight, normal, overweight, obese} example, and so some of the results are stated in terms of . To simplify exposition we assume that the labels are 1 . . . , but for Lp secondary optimization where the labels are arbitrary real numbers we use the true values.
We do not examine any specific application, concentrating instead on the algorithms and reducing the time so that they can be applied to larger problems. Previous algorithms [8, 13, 18] are reviewed in Section 2. Examples and applications of L0 isotonic regressions and monotonic relabeling appear in many papers, such as [8, 16, 17, 18, 23]. There are many applications of it to monotonic learning and classification, see [4, 5] and the extensive references therein. However, in some classification settings one must be careful about how monotonicity is applied [3].
2 Background
There are some special cases which have simple solutions. Minimizing the number of changed labels is equivalent to maximizing the number of unchanged ones, and hence if  is a simple linear ordering then the problem is the same as finding a nondecreasing subsequence of maximum length. This well-known problem has an easy (n log ) solution. When  is an arbitrary order, but  = 2, then the problem is equivalent to L1 isotonic regression, for which numerous algorithms are known [1, 2, 21, 22]. These algorithms vary widely in their time and construction as a function of the underlying order. If it is linear then the regression can be found in (n) time, a 2-dimensional grid in (n) time, an arbitrary set of vertices in d dimensions with the natural d-dimensional ordering in (n1.5 logd n) time, and an arbitrary dag in (nm + n2 log n). Most of these results appear in [21, 22], some based on the algorithm for general dags appearing in [2].
We will see that there is similarly a range of times and algorithms for L0 isotonic regression depending on the underlying dag, but an extra aspect, namely secondary criteria, causes yet more complexity. The general case, without such criteria, was analyzed in [8, 15, 17], while here we introduce a faster algorithm for multidimensional vertices (Section 4) and various algorithms optimizing secondary criteria.
2

1. Create a violator dag G = (V, E), where for all u, v  V , there is a path from u to v in G iff u  v and f (u) > f (v).
2. Find a maximum antichain C of G (this is also a maximum f -isotonic set in V );
(a) Create a flow graph Gf from G.
(b) Find a minimum flow on Gf and use this to determine C.
3. Determine f :
(a) For all v  V determine the window [w (v), w (v)] induced by C. (b) Let f  be any isotonic function on V that goes through the windows. Since w (v) = w (v) =
f (v) for v  C, f  = f on C.
Figure 1: L0-optimal monotonic relabeling f  of label function f on G = (V, E) with order  (see [8, 15, 18])
For the general case, [8, 15, 17] used an approach based on violating pairs. Given a label function f , vertices p, q  V are a violating pair iff p  q and f (p) > f (q), i.e., they violate the monotonicity requirement. Let p v q denote that p, q are a violating pair. Note that v is transitive, and hence is a partial ordering on the vertices in V .
Their approach is outlined in Figure 1. Detailed proofs appear in their papers, but the basic idea is to create a dag based on violating pairs and find an antichain of maximum size in it. Let G = (V, E) be a dag where there is a path from p to q in G iff p v q in G. G is a violator dag. While V is well-defined, there may be multiple sets of edges which give the same partial ordering. The construction of G in step 1) will be analyzed later. If  is given via dag G = (V, E) then this can be done via topological sorting, taking (nm) time. For multidimensional vertices it can be done by pairwise comparisons, taking (n2) time, and in Section 4 it is shown that this time can be reduced.
Vertices in V \ V are not in any violating pair, i.e., they are monotonic with respect to all other vertices, and hence in any optimal relabeling their label does not change. Step 2) determines those elements C of V where the label will not be changed. [8, 18] borrow an idea from [14] to determine C. No two elements of C can be a violating pair, i.e., are not comparable in the v ordering. A set of incomparable elements in a partial order is an antichain, and hence in an optimal relabeling C is a maximum antichain.
Here, rather than emphasizing the antichain's graph properties we look at its subsequent role in constructing an isotonic function. A set U  V is f -isotonic iff f is isotonic on U . Thus a set of vertices is an antichain of maximum size iff it is an f -isotonic set of maximum size. An isotonic function g is consistent with an f -isotonic set C iff g = f on C.
Mohring [14] showed how to find C via minimal flows. Figure 2 shows a dag G and one of its violator dags G, which happens to be its transitive reduction. G is transformed into a flow graph Gf as follows: each vertex u of G is replaced by two vertices uin, uout, where each incoming edge to u becomes an incoming edge to uin with 0 minimum flow, each outgoing edge from u becomes an outgoing edge from uout with minimum flow 0, and there is an edge from uin to uout with minimum flow 1.
Let F be a minimum flow on Gf , i.e., a flow where on each edge is at least the weight on that edge, and the total flow is as small as possible. From F one can determine a maximal cut, and the edges of weight 1 in this cut correspond to C. The size of C is the flow, and it is straightforward to show that C is an antichain. A topological sort and inverse sort can be used to determine a feasible flow (one where the flow on each
3

4

5

4

3

2

1

2

4

4

3

2

1

2

1

0

1

0

1

0

1

0
1

0
1

4

5

4

3

2

1

2

Figure 2: Label function on dag G, one of its violator dags G, flow graph Gf , resulting C

edge is at least the edge's lower bound) with flow |V | and all edge flows are an integer. Using a maximal

flow algorithm to remove the excess generates a minimal flow. Thus the time of step 3) is the time to find a

maximal flow on Gf , where the flow is at most |V | and Gf has (|G|) vertices and (|E|) edges. Previous authors used the Ford-Fulkerson algorithm, which finds a maximum flow in (nm) time. However, there

are algorithms that are far faster (in O-notation), though far more complex. Many have poly-log factors in terms of the number of vertices and their time is typically expressed in terms of ~ . Some have time which is

logarithmic in the maximum capacity on any edge, but since this is never larger than the number of vertices it is absorbed in the ~ . Because the previous authors used the basic Ford-Fulkerson algorithms, in the time
analyses we give the time if it is used, and T (n, m) if the fastest known algorithm is used. Currently

the Goldberg-Rao algorithm [11], combined with the Gao-Liu-Peng algorithm [10], shows that T is not

worse

than

~ (m

·

min{n2/3,

m1 2

-

1 328

}).

The

algorithms

are

still

undergoing

improvement,

and

the

wikipedia

page [24] is usually up-to-date on the latest improvements. However, in practice simpler algorithms are faster

on datasets of moderate size. Step 3) determines f . At each such vertex v there is a "window" of label values [b(v), t(v)] into which
the new label must fall. This is the range of values that are monotonically compatible with the values in V ,

i.e., the values at vertices where the function will not change. The windows are isotonic in that the bottom

values are an isotonic function on V , as are the top values. Any isotonic function within these windows (such as always using the lower bound) can be used as f , and simple topological sort can be used to finish this step in (m) time. However, one may want to find an f  optimizing secondary objectives, the main contribution

of this paper, which adds complexity.

Returning to step 2), there are several ways G can be created. First, the ordering  on V may be given

explicitly via a dag G = (V, E), or implicitly, such as when V is points in a multidimensional space with

coordinate-wise ordering. In the former case, finding the vertices in violating pairs can be done via topological

sort and reverse topological sort in (m) time, and all violating pairs (the transitive closure of v) can be found in (nm) time. In the latter case the vertices in violating pairs, and all violating pairs, can be found in

(n2) time by pairwise comparisons. Throughout, when analyzing d-dimensional vertices the analysis is in

terms of d fixed and n  , so the implied constants in the O-notation depend on d but are not given.

While a dag specifies a partial ordering, there may be multiple dags specifying the same partial ordering.

For example, a dag with an edge for each pair of vertices in the transitive closure, and one with an edge

for each pair in the transitive reduction, specify the same partial order, but the latter may be significantly

smaller. Both can be constructed in (nm) time using repeated topological sorting, and therefore [18] uses

the transitive reduction for G.

If the data is almost in order and there are few violating pairs then G may be much smaller than G.

However, the transitive reduction can be quite large. For example, on the linear order on V = {1, 2, . . . , n},

for

n

even,

suppose

f

is

n2+1,

n2+2,

.

.

.

n,

1,

2

.

.

.

n 2

.

Then

for

p,

q



V

,

p

v

q

iff

p



[1,

n 2

]

and

q



[1+n2 ,

n].

The transitive closure and reduction are the same, with n = n and m = n2/4. Here m  m. Further, there

4

can be exponentially maximum f -isotonic sets, as 2, 1, 4, 3, 6, 5, . . . shows since one needs to choose 1 from each pair of the form k, k - 1.
Note that [2] use an approach based on violating pairs and flows for L1 isotonic regression.
3 Secondary Optimality
For general label functions there can be many possible choices of maximum f -isotonic set, and for each there may be many possible values for the remaining vertices, as long as they satisfy monotonicity. For example, given label values small < medium < large, for a linearly ordered set with data values large, medium, small, any single value provides a maximum f -isotonic set. However, many researchers would prefer using {medium}, with regression values medium, medium, medium. However, even with this choice of maximum f -isotonic set, small, medium, large is also L0 optimal, as is small, medium, X-large if X-large is a valid label. This raises the questions: which maximum f -isotonic set should be used, and what should the regression values be for the remaining vertices?
If the labels merely have an ordering, with no metric, one can decide this as follows: let C0 be an f isotonic set of maximal size. Values at other vertices must be 1 or more labels away from their trimmed value. Among all vertices  C0 find an f -isotonic set C1 of maximum size where all regression values change to adjacent values, and for these vertices pin their values to their trimmed value. This shrinks the windows, and now all vertices that aren't pinned are 2 or more away from their trimmed value. Find an f -isotonic set C2 fitting within the windows which maximizes the number of vertices 2 labels away from their trimmed value, etc. An isotonic regression obtained this way is called a weak L0,0|C regression.
While the size of C0 is unique, the size of C1 depends on the choice of C0, the size of C2 depends on C0 and C1, etc. Let ni = |Ci| for 0  i   - 1. Viewing (n0, n1, ..., n-1) as a word of  letters, a strong L0,0 isotonic regression is one which is lexically last among all isotonic functions (this may not be unique).
When the labels are real numbers, another way to choose the regression way to minimize Lp error. That is, given 1  p  , for a given label function f and maximum f -isotonic set C, choose an isotonic function g where f = g on C and g minimizes the Lp error among all such functions. For weak optimality, denoted L0,p|C, the L0 maximum f -isotonic set C is specified and g minimizes the Lp error, while strong optimality, denoted L0,p, minimizes the Lp error of the L0,p|C functions, minimizing over all maximum f -isotonic sets C. In the previous example 3, 3, 3 is weakly optimal, given C = {3}, and 2, 2, 2 is strongly optimal, using C = {2}. It is L0,p optimal for any 1  p  . L0,p optimal functions need not be unique, as the simple example 2, 1 shows. We examine the most important values of p, namely p  {1, 2, }.
Note that if the labels are 1, 2, . . . , , there is a natural metric but even here, in general L0,0 = L0,1. For example, for 0, 5, 5, -1, 3, 3 the L0,0 regression is 0, 0, 0, 0, 3, 3, while the L0,1 regression is 0, 3, 3, 3, 3, 3.
Given an f -isotonic set C, for every vertex v  V , its window is [w (v), w (v)], where w (v) = max{f (u) : u  C, u v} and w (v) = min{f (u) : u  C, u v}, i.e., it is the range of possible values an isotonic regression can have at v, given that the regression values on C are the values of f . If C is a maximum f -isotonic set and v  C then f (v) is not in v's window, for if it were then v could be added to C to give a yet larger f -isotonic set. The trim of value y at v is the closest value in v's window, i.e., it is y if y is in v's window, w (v) if y < w (v), and w (v) if y > w (v).
3.1 Weak L0,p|C Optimality, p  {1, 2, }
There are two natural approaches for generating an L0,p|C isotonic regression: trim all the f values and determine an Lp isotonic regression of the trimmed values, or determine an Lp isotonic regression of the original values and then trim the result. The advantage of these approaches is that they can utilize standard Lp
5

isotonic regression algorithms, and each generates an isotonic regression. Unfortunately they do not always produce an optimal one.
For L0,1|C, finding an L1 optimal regression and trimming may not be optimal. On a linear order, suppose the values are 0, 3, 1, -1, -2, -3, -4, 2, where  indicates the unique maximum isotonic set. The unique L1 optimal isotonic regression has values -1, -1, -1, -1, -1, -1, 2, which, when trimmed, is 0, 0, 1, 1, 1, 1, 1, 2, with L1 error 13. However, the unique L0,1|C isotonic regression is 0, 1, 1, 1, 1, 1, 1, 2, with error 12. For L0,|C, trimming then finding an optimal L regression may not be optimal. Suppose the values are 0, 0, 8, -2, 2, 2, where again the unique optimal C is indicated. Trimming gives 0, 0, 2, 0, 2, 2, then optimizing gives 0, 0, 1, 1, 2, 2, with L error 7, while the L0,|C optimal is 0, 0, 2, 2, 2, 2, with error 6. Similar examples show that for L0,2|C neither approach always gives the optimal.
Despite these examples, the approaches are useful.
Theorem 1 Given a set of real-valued labels, a label function f on a dag G = (V, E), and an f -isotonic set C , an L0,1|C isotonic regression can be obtained by trimming f and finding an L1 isotonic regression of the trimmed values.
Proof: Any L0,1|C isotonic regression g must have its values in [w (v), w (v)] for all v, and hence if f (v) < w (v) then the error at v is |f (v) - w (v)| + |w (v) - g(v)|. Similarly if f (v) > w (v) then the error at v is the distance to the trimmed value plus the distance from w (v) to g(v). Therefore a function minimizing the sum of the distances from the trimmed values to their regression values is also a function minimizing the L1 error of an isotonic regression consistent with C, and vice versa.
Trimming is just a topological sort operation, so can be completed in (m) time. The total time is determined by the time to find the L1 regression. Currently the fastest algorithm known for general dags takes (nm + n2 log n) time ([2]).
There is a simple lower bound on the error that trimming imposes, true for any p. For v  V , let
trim-err(f, v) = max {max{f (w)-f (v) : w v, f (w)  f (v)}, max{f (v)-f (w) : w v, f (w)  f (v)}}
This is the error due to the fact that isotonic functions must be no greater than f (v) at predecessors of v, and no less for successors. For an arbitrary subset C  V let trim-err(f, C) = max{trim-err(f, v) : v  C}.
Lemma 2 Give a set of real-valued labels, a dag G = (V, E), a label function f and isotonic label function g on G, and an f -isotonic set C  V , let g be g trimmed to C. Then ||f - g||  max{||f - g||, trim-err(f, C)}. Further, if g is an optimal L isotonic regression of f then the inequality is an equality.
Proof: For any vertex v  V , if g(v) is in v's window then g and g have the same error at v, which is  ||f - g||. Suppose g(v) < w (v). If f (v) < w (v) then g(v) = w (v), and hence the error of g(v)  trim-err(f, C), while if f (v)  w (v) then |f (v) - g(v)| < |f (v) - g(v)|  ||f - g||. Similar results hold if g(v) > w (v).
As far as equality is concerned, when ||f - g|| is optimal both terms in the max are lower bounds.
Theorem 3 Given a set of real-valued labels, a label function f on a dag G = (V, E), and an f -isotonic set C  V , an L0,|C isotonic regression of f can be obtained in (m) time by finding an L regression and trimming it to C.
Proof: As noted above, the windows at every vertex can be obtained in (m) time by topological sort. Further, it is well-known that an optimal isotonic regression g can be obtained by setting g(v) = (max{f (u) : u v} + min{f (u) : u v})/2. These values too can be computed by topological sort in (m) time. The lemma shows that the result is optimal.
6

Gf = (Vf , Ef ) {the flow graph of a violator dag G = (V, E) for G} W =  {the initial set of vertices v where their regression value f(v) has been determined} for d = 0,  - 1
g = trim of f, trimmed using the values of f on W V = vertices of V where g and f differ by exactly d steps G = flow graph constructed from Gf , where all vertices in V - V are collapsed C = an f -isotonoic set of maximum size determined by G for all v  C, f(v) = g(v) W = WC
Figure 3: Determining an f  which is an L0,0|C regression of f on G

Corollary 4 Given a set of real-valued labels, a label function f on a dag G = (V, E), and a violator graph G = (V , E), an L0, isotonic regression of f can be obtained in (T (n, m) log n) time,

Proof: The proof of the theorem shows that we merely need to find an f -isotonic set of maximum size S that minimizes trim-err among all such sets. There is an f -isotonic set of size S with trim-err  t iff the minimum satisfying flow is S on an adjusted violator graph where for every v where r(v) > t, the required flow from vin to vout is set to 0. A simple binary search on the trim-err values of the vertices can be used to find t.

Finally, for L0,2|C there doesn't seem to be a straightforward approach. However, given C, to find an

L0 isotonic regression g where at every vertex the value is no more than  away from that of L0,2|C, let



=

1 

and let g

vV |f (v)|, let g be the trim of g.

be the

weighted

L2

isotonic

regression

of

f

with weight 1 on V

\C

and  on C,

3.2 Weak L0,0|C Optimality
Determining L0,0|C appears to require a different approach, given in Figure 3. The time analysis is straightforward.
Theorem 5 Given a label function f on a graph G, and a violator dag G = (V , E), the algorithm in Figure 3 produces an L0,0|C regression of f in (nm) time if the Ford-Fulkerson algorithm is used, and (T (n, m)) time if the fastest algorithm is used.
Determining the best flow algorithm to use depends on the relationship of n to m. In some cases this is unknown, while in others it is and can be exploited. See Section 4 for such a case.

4 Multidimensional Orderings
Given points x = (x1, . . . , xd), y = (y1, . . . , yd), x = y, in a d-dimensional space, y dominates x iff xi  yi for 1  i  d. Domination is a partial ordering also known as multi-dimensional ordering. There is no requirement that the dimensions are the same, merely that each is linearly ordered. All of the datasets examined in [8, 18], and a vast number of others in different applications, use component-wise orderings. Given a set V of d-dimensional points, with a label f (x) at each point, we create the violator graph by using a (d + 1)-dimensional ordering where (x, f (x))  (y, f (y)) iff y dominates x and f (x) > f (y).
Here the partial ordering on the violator dag is given in terms of its vertices, but the edges are only implied, not given explicitly. One simple approach to construct the violator dag is to do pairwise comparisons of the
7

points and add an edge from (x, f (x)) to (y, f (y)) iff x  y. This is the transitive closure and can result in (n2) edges, where a large number of edges would be expected in many applications. This is a concern because the time of flow algorithms, used in step 3 of Figure 1, depend on the number of edges and vertices. To reduce the time, one can first determine the transitive reduction, i.e., the smallest subset of edges in the transitive closure that preserve the ordering. It is well-known that this can be found in O(min(n, nm) time, where m is the number of edges in the transitive closure and  is such that matrix multiplication can be done in O(n) time. While it is known how to multiply in o(n2.4) time, algorithms achieving this are decidedly impractical, and so in practice people use either standard matrix multiplication (or perhaps Strassen's) or a breadth-first search of the transitive closure dag. In the worst case this is (n3) time. Further, the transitive reduction may have (n2) edges, so no reduction in time, as measured in O-notation, is insured, though in practice the reduction is likely significant.
However, more concise violator graphs are possible by embedding the violators V^ into a larger dag G¯ = (V¯ , E¯), where V^  V¯ , that preserves the ordering on V^ . I.e., if x, y  V^ then x  y iff there is a path in G¯ from x to y. The vertices in V¯ \ V^ are sometimes known as Steiner vertices.
An explicit construction for coordinate-wise orderings appears in [22], where it is shown how to embed into a dag with (n logd+1 n) vertices and edges, with the construction taking time proportional to this. Using this gives:
Theorem 6 Given a label function f on n d-dimensional points, L0, and L0,|C isotonic regressions can be found in
· ~ (n2) time if the Ford-Fulkerson flow algorithm is used, and
· T (n logd+1 n, n logd+1 n) time if the fastest algorithm is used.
Proof: The results in [22] show that a violator dag G can be constructed in (n logd+1 n) time, with (n logd+1 n) edges and nodes. The standard conversion of G into a flow graph is used, except that the Steiner vertices are not expanded into a pair and all edges into and out of them have 0 minimal flow. Using this, a maximum f -isotonic set C can be found in the time claimed.
Given C, an L0 regression (Figure 1) and an L0,|C regression can be found in ~ (n) time (Theorem 3).
Using the Gao-Liu-Peng [10] and Goldberg-Rao [11] algorithms shows that C can be found in o(n1.5) time. Given C, an L0,1|C regression can be found in ~ (n1.5) time [22].
5 Strong L0,p Optimality on Linear Orders
For L0,p on linear orders we don't determine an optimal f -isotonic set and then determine other regression values. Rather, we use a left-right scan, but cannot use PAVA (pool adjacent violators) since it does not hold for L0 error. E.g., if L = {0, 1, 2} and the data values are 2, 2, 2, 0, 0, 1, 1, then after processing the first 5 values the unique optimal regression is the single level set 2, 2, 2, 2, 2. When the 6th is processed the result could be either 2, 2, 2, 2, 2, 2 or 0, 0, 0, 0, 0, 1, and when the last is processed the unique answer is 0, 0, 0, 0, 0, 1, 1. Thus whichever choice is used at the end of the 6th, either 5 to 6, or 6 to 7, results in merging adjacent violators but getting 2 level sets, i.e., they are not pooled.
To determine L0,p on linear orders we instead use an approach based on the standard algorithm for maximal nondecreasing subsequences. At each vertex i we determine a pair (c, s), where c is the size of the largest f -isotonic set from 1 to i, and s is the smallest error among all such sets. If the previous best value at i was
8

There are no data points in the blank areas (including the lower left and upper right on the right figure) or on the boldface lines (other than vertices of C or possible successors). For both figures there might be data points in the background, or for the left figure perhaps on the dotted lines (points there aren't successors).
Figure 4: Left: A point (lower left) and all of its possible successors. Right: C for some data set.

(c1, s1) and now it is determined it can be reached in (c2, s), then the best value at i is given by the maxmin

operation, where

  (c1, s1)

if c1 > c2

maxmin{(c1, s1),

(c2, s2)}

=



(c2, s2) (c1, min{s1, s2)})

if c1 < c2 if c1 = c2

We will prove

Theorem 7 Given a real-valued label function f on a linear ordering of length n, where there are  different
label values and regression values can be arbitrary real numbers, an L0,p isotonic regression can be found in (n) time for p  {1, 2} and (n log ) for p = .

and

Theorem 8 Given a label function f on a linear ordering of length n, where there are  different labels, a strong L0 isotonic regression (L0,0) can be found in (n2) time.

Since the order is linear we can assume the vertices are 1, . . . , n. To simplify the algorithm we add vertices 0 and n + 1, and define f (0) = -, f (n + 1) = .
Suppose an L0,p isotonic regression f  is equal to f on a maximum f -isotonic set C = {i0 < . . . < ik}, for some k  0. For 0  j < k consider the closed rectangle with lower left coordinate (ij, f (ij)) and upper right coordinate (ij+1, f (ij+1)). Except at these corners there cannot be any point (x, f (x)) in the rectangle for if there were then C would not have maximum size since the point could be added to it. Thus if we have an optimal f -isotonic regression from 0 to i, and at i the regression value is f (i), then for the next index j, in increasing order, where the regression value is the same as f (j) we need only consider those j where f (j) > f (i), and f (j) < f (k) for all i + 1  k < j. To simplify notation, at a vertex i the potential successors is the unique sequence of vertices i < m1 < . . . < mk of maximum length where f (m1) > f (i) and f (m1) > f (m2) > . . . > f (mk). See Figure 4. The Lp-error from i to potential successor j is the minimum Lp distance from f to f  on i . . . j, over all isotonic functions f  where f (i) = f (i) and f (j) = f (j). This observation immediately gives the algorithm in Figure 5.
To prove Theorem 7 we need to prove that the Lp errors can be computed efficiently. We give the proof by considering different ways to compute these errors, where the details of the approach used depend on p. For p =  the algorithm in Section 5.2 is not based on Figure 5. The proof of Theorem 8 is quite different
and given in the appendix.

9

array T(0 : n+1) of (c, s) pairs {also keep track of predecessor giving optimal value} f(0) = -  f(n + 1) =  for j = 0, n + 1
T(j) = (0, 0) for i = 0, n
successorvalue =  for j = i + 1, n
if ((f(j) < f(i)) or (f(j)  successorvalue)) then loop T(j) = maxmin{T(j), T(i) + (1, Lp -error(i, j)} if (f(i) = f(j)) then exit for-loop successorvalue = f(j)
Figure 5: Determining an L0,p Isotonic Regression on a Linear Order, p < 

5.1 p  {1, 2}

The simplest case is p = 2, with p = 1 being similar but slightly more complicated. For p = 2 the prefix

algorithm in [20] is utilized, incrementally building an isotonic regression using a left-right scan. The algo-

rithm maintains a stack of level sets, ordered by their initial index and also containing their regression value.

When the next index i is reached, f (i) is compared to the regression value of the most recent level set. If f (i)

is larger than this then a new level set is started at i and the process moves on to i + 1, while if it is less then or

equal to it then i is merged with the level set and the regression value of this enlarged level set is determined.

If the regression value is larger than its predecessor then the process moves on to i + 1, while otherwise it is

merged with the predecessor, with the cascading mergers stopping when the regression value of the resulting

set is greater than its predecessor's.

Before any calculations of L2 errors we first compute S1(j) =

j k=1

f

(k)

and

S2(j)

=

j k=1

f

(k)2

for 1  j  n, and let S1(0) = S2(0) = 0. For L1 we precompute, for all i  j  n, A(j) =

j k=i

1

·

(f (j)  f (i)), B(j) =

j k=i

f

(j)

·

(f (k)



f (i)),

and

C (j )

=

j k=i

f

(j)

·

(f (k)

>

f (i)),

and

define

A(i-1) = B(i-1) = C(i-1) = 0.

Lemma 9 For any vertex i, 1  i  n, for p  {1, 2} the Lp-error from i to all of its potential successors can be computed in () time.

Proof: In the prefix approach for L2 each level set's regression value is the average of its f values. Here, however, when the potential successor j is reached, if the level set preceding j has value > f (j) then they have to be combined and given the value f (j), and this might cascade. For any level set on c . . . d, the L2 error of using y as the regression value is just S2(d) - S2(c - 1) - 2y(S1(d) - S1(c - 1)) + y(d - c + 1), so calculating a level set's L2 error, even when the mean is not used as the regression value, takes constant time
For L1, here the prefix algorithm is faster than the (n log n) prefix algorithm in [20] for the general case. We exploit the property that the L1 error of a level set is minimized when it is a median value, which can always be chosen to be one of the data values. If the regression g is such that g(i) = f (i) and j is a potential successor, then for all level sets in i . . . j, then either f (i) or f (j) is an (perhaps not unique) optimal choice of regression value since all other f values are outside the real interval [f (i), f (j)] that their regression value must be in. For any level set on a . . . b, let na = A(b)- A(a) and nb = b - a + 1- na. Then the L1 error of using y as the regression value, for f (i)  y  f (j), is nay -(B(b)-B(a-1))+(C(b)-C(a-1))-nby.

10

This can be computed in constant time, and hence determining whether a level set should have value f (i) or f (j) can be determined in constant time. In case of ties use f (i) for consistency.
Since for both p = 1 and p = 2 each iteration of the i loop in the algorithm in Figure 5 takes O(n) time, this completes the proof of Theorem 7 for p  {1, 2}.
5.2 p = 
There is a very simple algorithm for p =  which runs in (n log ) time. First determine the trim error of every point, taking (n) time. Then do the standard algorithm for finding a longest nondecreasing sequence, inserting points into a balanced tree, where each node stores the length of the longest nondecreasing decreasing sequence reaching that point, and the minimum maximum trim error of such a sequence. When a new point is inserted it adds 1 to the length of its predecessor, and computes the max of its trim error and the predecessors. A successor point is eliminated iff it has length less than the new point, or the same length and equal or greater trim error.
This completes the proof of Theorem 7.
6 Lp Isotonic Regression with Hamming Distance Penalty
In some estimation problems the objective is similar to L0,p, namely, given a function f and constant  > 0, find an isotonic function g that minimizes ||f -g||p + ||f -g||0, i.e, an Lp isotonic regression with an L0, or Hamming distance, penalty function. It is a form of regularized regression. We will show
Theorem 10 Given a real-valued function f on a linear ordering of length n and a constant  > 0, a realvalued isotonic function g which minimizes ||f -g||p + ||f -g||0 among all isotonic functions can be found in (n2 log n) time for p  {1, 2} and (n2) time for p = .
For p  {1, 2} we again utilize the algorithm in Figure 5, determining the error between successive indices where the regression has the same value as the original function. An important difference is that if j is a successor of i in this sequence then there may be f values that are in the rectangle with lower left corner (i, f (i)) and upper right corner (j, f (j)). We have to consider every j > i as a potential successor if f (j)  f (i) and cannot exploit the property that for a fixed i the potential successors form a monotonically decreasing sequence. Thus we modify the algorithm to remove any use of successorvalue and eliminate breaking out of the loop if f (i) = f (j). We also have to replace the update of T (j) with T (j) = min{T (j), (j -i -1) + Lp-error(i, j)}.
As before, to determine L2 regression errors we precompute the S1 and S2 functions mentioned in Section 5.1. For L1 a more complicated approach is needed because we can longer exploit "no points in the rectangle". We use a simple augmented binary search tree where the keys are the f values. These are known in advance so we can use a perfectly balanced tree and no rotations are needed. This doesn't change the time, but makes the implementation faster and easier. Initially nodes are "ghosts", but will be occupied as information is added. At each node we have augmenting fields containing the number of nodes in the subtree, and the sum of their f values, both of which are initially 0. We go through the indices and add each function value as it is reached. We do this using a persistent tree, adding a new root for each index and updating nodes with new values as new nodes, at each step of the insertion pointing into the previously tree for whichever child is unchanged. This takes logarithmic time per index, for a total of (n log n).
Lemma 11 For any index i, 1  i  n, the Lp-errors of optimal isotonic functions from i to all of its potential successors can be computed in (n log n) time for p  {1, 2}.
11

Proof: For each i we form an Lp isotonic regression starting there and moving leftward, using the prefix approach, just as in Lemma 9, always forcing the initial level set to have value f (i). When a j is reached where f (j)  f (i) we conceptually trim the isotonic regression so that the final level set has value f (j). However, since later possible successors may have larger values, we don't keep this trimmed final level set, and instead continue to build the isotonic regression as if the trim didn't occur.
To compute the error that would be induced if we had created a final level set of value f (j) requires finding the leftmost level set M with value  f (i) and merging it and all indices until j and giving this level set the value f (j). Instead of the sequential merging used in Lemma 9, we find M by binary search on the values of the ievel sets, and then determine the regression error of the the trimmed level set from M to j. We assume that the stack of level sets is maintained as the initial portion of an array, rather than as a list, so that each probe of the binary search takes constant time.
For p = 2 determining the error is quite simple, using the same calculations as in Section 5.1. The time for each potential successor j is now (log n), due to the binary search, hence there is (n log n) time for the successors, plus the linear time to find the standard untrimmed isotonic regression.
For p = 1 we use the augmented tree. It's a common homework problem to show that the median key y and the L1 error of using it as the value of the level set can be computed in logarithmic time. More generally, if x is used as the regression value, let n be the number of indices in c . . . d that have f value  x, let B be the sum of all f values below  x and C the number of values > x. Then the L1 error is nx-B+C -(d-c+1)x, and these values can be determined from the tree in (log n) time. We use the standard pool adjacent violators to keep track of the level sets of the unrestricted isotonic regression.
To compute the median on c . . . d we traverse the tree with root d, at each node conceptually subtracting the corresponding node in the tree for c - 1. The path traversed is determined by the tree for root d. The result is exactly the same as if the tree only consisted of the keys and augmented fields corresponding to c . . . d.
Finally, as for L2, a binary search is used to find the leftmost level set > f (j), and then the tree is used to find the error of the resulting level set. Since the stack of level sets includes the regression value of each here too each probe to find M takes constant time.
For L, the dynamic programming in Figure 5 will not always work since the errors from one successor to the next are not independent. For example, suppose the values are 0, 3, -3, -3, 0, -6, 6, 0 and  > 0. The isotonic function minimizing the L + L0 error for the 1st 5 values is 0, 0, 0, 0, 0, with L error 3 and L0 error 3. The optimal L + L0 error on the last 4 values is 0, 0, 0, 0, with L error is 6 and L0 error 2. Combining these gives a function which has L error 6 and L0 error 5. However, the fact that the 2nd part has larger L error allows the first part to have a larger L error than it had originally. The optimal regression is -3, -3, -3, -3, 0, 0, 0, 0, with L error 6 and L0 error 4.
Lemma 12 Given a real-valued function f on a linear ordering of length n and a constant  > 0, a realvalued isotonic function g which minimizes ||f -g|| + ||f -g||0 among all isotonic functions can be found in (n2) time.
Proof: To simplify notation let f (h) = ||f - h|| + ||f - h||0 and let g be an L isotonic regression of f . Given a f -isotonic subset C of 1 . . . n, let h be g trimmed to C. Lemma 2 shows that ||f - h|| = max{||f - g||, trim-err(C)}. Since trim-err(C) is the maximum trim-err of any of its vertices, this gives a straightforward approach to minimizing f (·). For any given vertex v let  be its trim-err(.) In the set of all vertices with trim-err   find a maximal nondecreasing sequence, i.e., an f -increasing set C of maximal size among all this with trim-err  . Thus trimming g to C minimizes f (·) among all isotonic functions where ||f - h||0 = n - |C|. Note we only need to determine |C|, not C itself, to determine this value. Doing this for all vertices will give an  minimizing f among all functions, and then a standard process for finding a
12

maximum nondecreasing subsequence among all vertices of trim-err   can be used to determine a specific function with this f value.
Thus the problem has been reduced to finding the length of a longest increasing subsequence (LIS) of vertices where the trim-err is no more than the trim-err of a specific vertex v, for all v. Sorting the vertices by their trim-err and inserting them one at a time, and determining the longest increasing subsequence is the problem of finding longest increasing subsequences of a growing set of f values. This problem, called the dynamic longest increasing subsequence problem has been studied by several authors [7, 9, 12] in the more difficult setting where there can be deletions as well as insertions. When there are only insertions a (n2) algorithm appears in [6]. This proves the lemma.
A randomized algorithm in [12] for the fully dynamic problem takes ~ (n1.8) time but has a tiny chance of producing an incorrect answer. In our case we only do insertions and don't need to produce an LIS every step, just determine its length, so it is likely that an exact algorithm taking o(n2) time exists. This would immediately improve the time to find an optimal ||f -g|| + ||f -g||0 isotonic regression.
7 Final Remarks
Datasets are quickly becoming vastly larger and more complex, often with linearly ordered but not real-valued indices and values. Because of this, and algorithmic advances, nonparametric regression, particularly isotonic regression, are becoming increasingly important. When optimizing the Hamming distance, in general there will be many L0 isotonic regressions and choosing among them may be useful but difficult. We choose ones which optimize secondary criteria based on other Lp metrics. We gave algorithms for computing them, and many have already shown the utility of isotonic regression for L0 and other metrics [3, 4, 5, 7, 8, 9, 12, 16, 17, 18, 23].
In Lemma 11 a persistent tree is used for the prefix L1 calculations. An algorithm for L1 prefix isotonic regression appeared in [20], taking (n log n) time using a somewhat complicated implementation via mergeable heaps. G. Rote [19] developed an interesting and much simpler geometric approach, taking the same time, but the persistent tree implementation also seems simple. Further, once constructed, given any segment of indices the median of that segment, and the L1 error of using some given value as the regression value of the segment, can be determined in (log n) time.
Finally, we introduced strong L0 (L0,0), which appears to be a natural choice among L0 isotonic regressions with arbitrary non-numeric labels. We gave an efficient algorithm for determining it on linear orders, but no algorithm for general dags nor even multidimensional ones. Computing it efficiently seems far more difficult than the other problems considered here and is beyond the scope of this paper.
References
[1] Ahuja, RK and Orlin, JB (2001), "A fast scaling algorithm for minimizing separable convex functions subject to chain constraints", Operations Research 49, pp. 784­789.
[2] Angelov, S, Harb, B, Kannan, S, and Wang, L-S (2006), "Weighted isotonic regression under the 1 norm", Symp. on Discrete Algorithms (SODA), pp. 783­791.
[3] De Baets, B (2019), "Monotonicity, a deep property in data science", SFC2019.
[4] Brabant, Q, Couceiro, M, Dubois, D, Prade, H, and Rico, Q (2020), "Learning rule sets and Sugeno integrals for monotonic classification problems", Fuzzy Sets and Systems, 401, pp 4­37.
13

[5] Cano, J-R, Gutierrez, PA, Krawcsyk, B, Wozniak, M, Garc´ia, S (2018), "Monotonic classification: an overview on algorithms, performance measures and data sets". arXiv:1811.07155
[6] Chen, A, Chu, T, and Pinsker, N (2013), "The dynamic longest increasing subsequence problem", arXiv:1309.7724v4
[7] Fattal, S and Ron, D (2010), "Approximating the distance to monotonicity in high dimensions", ACM Trans. Algorithms, article 52.
[8] Feelders, A, Velikova, M, and Daniels, H (2006), "Two polynomial algorithms for relabeling nonmonotone data", Tech. Report UU-CS-2006-046, Dept. Info. Com. Sci., Utrecht Univ.
[9] Fischer, E, Lehman, E, Newman, I, Raskhodnikova, S, Rubinfeld, R., and Samorodnitsky, A (2002), "Monotonicity testing over general poset domains", STOC'02.
[10] Gao, Y., Liu, Y.. Peng, R. (2021). "Fully dynamic electrical flows: sparse maxflow faster than GoldbergRao", arXiv:2101.07233
[11] Goldberg, A. V.; Rao, S. (1998), "Beyond the flow decomposition barrier", J. ACM 45 (5): 783.
[12] Kociumaka, T and Seddighin, S (2020), "Improved dynamic algorithms for longest increasing subsequence", arXiv:2011.10874
[13] Lin, T-C, Kuo, C-C, Hsieh, Y-H. and Wang, B-F (2009), "Efficient algorithms for the inverse sorting problem with bound constraints under the L-norm and the Hamming distance", J. Comp. and Sys. Sci. 75, pp. 451­464.
[14] Mo¨hring, R (1985), "Algorithmic aspects of comparability graphs and interval graphs", Proc. NATO Adv. Study Inst. on Graphs and Order, Rival, I, ed., 41­101.
[15] Pijls, W and Porharst, R (2013), "Another note on Dilworth's decomposition theorem", J. Discrete Mathematics 2013
[16] Pijls, W and Potharst, R (2014), "Repairing non-monotone ordinal data sets by changing class labels", Econometric Inst. Report EI 2014­29.
[17] Rademaker, M, De Baets, B, and De Meyer, H (2009), "Loss optimal monotone relabeling of noisy multi-criteria data sets", Information Sciences 179 (2009), pp. 4089­4097.
[18] Rademaker, M, De Baets, B, and De Meyer, H (2012), "Optimal monotone relabeling of partially nonmonotone ordinal data", Optimization Methods and Soft. 27, 17­31.
[19] Rote, G (2019), "Isotonic regression by dynamic programming" , Proc. 2nd Symp. on Simplicity in Algorithms (SOSA 2019), 69, pp. 1:1­1:10
[20] Stout, QF (2008), "Unimodal regression via prefix isotonic regression", Comp. Stat. and Data Anal. 53, 289­297.
[21] Stout, QF (2013), "Isotonic regression via partitioning", Algorithmica 66, pp. 93­112.
[22] Stout, QF (2015), "Isotonic regression for multiple independent variables", Algorithmica 71, pp. 450­ 470.
14

[23] Verbeke, W, Martens, D, and Baesens, B (2017), "RULEM: A novel heuristic rule learning approach for ordinal classification with monotonicity constraints", Applied Soft Computing 60 (2017), pp. 858­873
[24] https://en.wikipedia.org/wiki/Maximum flow problem
A Proof of Theorem 8
We sketch an algorithm for finding a strong L0 isotonic regression (L0,0) of a label function f on a linear order of length n. The algorithm goes in stages s = 0 . . . -1, where at stage s we determine all isotonic functions which maximize the number of regression values s labels away from f , among those isotonic functions which were optimal for all previous stages. An (index, label) pair is live if it might be in such an optimal isotonic function, and dead otherwise. Initially every pair is live, but as the algorithm progresses there may be a stage where a pair is not in any optimal isotonic function, in which case it becomes dead throughout the rest of the algorithm
At stage s, for index i and label j, let As(i, j) denote the largest number of f (·) ± s values of any isotonic function from index 1 (with any value) to (i, j) which is an initial segment of an isotonic function which was optimal on 1 . . . i and reached (i, j) for all earlier stages, and let Bs(i, j) denote the largest number on any isotonic function which was optimal at all previous stages from (i, j) to a value at index n. If (i, j) is dead then A(i, j) = B(i, j) = -. For live (i, j),
As(i, j) = max{As(i-1, k) : k  j, (i-1, k) alive} + 1 · (j = f (i) ± s) Bs(i, j) = max{Bs(i+1, k) : k  j, (i+1, k) alive} + 1 · (j = f (i) ± s) Ts(i, j) = As(i, j) + Bs(i, j) - 1 · (f (i) = j ± s)
(the -1 in T is to account for the fact that it is counted in both A and B). Let ms = max{Bs(1, j) : 1  j  } = max{A(n, j) : 1  j  }. Then (i, j) is on an optimal isotonic function and stays alive for this stage iff it was alive and Ts(i, j) = ms. If Ts(i, j) < ms then (i, j) dies. The approach is basically the standard one of finding the maximum number of function values in an isotonic function (via A) and tracing backwards, via B, to find such a function, but this seems a bit simpler when one needs to keep track of all such functions. As an example, Figure 6 gives the function values (circled). Part a) shows the calculations for A0 and indicates the (index,label) pairs that die. Part b) shows the nonsense values of A1 if one ignored the deaths in the prior stage, and c) the values if the deaths are incorporated. Keeping an (index,value) dead enforces the fact that later stages can only use functions that were optimal in all prior stages.
While this seems straightforward, there is an additional subtlety shown in part d). Numerous (i, j) pairs remain alive at the end of stage 0. For example, (5, d) is alive. For s = 1, if A1 is calculated taking deaths into account, A1(5, d) = 2 because it is a successor of (4, b) and A1(4, b) = 2. This is the value indicated in part c). However, this is not correct because there was no optimal isotonic function in stage 0 that included (4, b) and (5, d). The only optimal isotonic function in stage 0 that reached (5, d) was the one that started at (1, d). The general problem is that for an (i, j) pair in stage 0, i > 1, if x = max{A0(i-1, k) : k  a . . . j}, then any optimal isotonic function passing through (i, j) must have a predecessor at some (i - 1, k) where a  k  j and A0(i-1, k) = x. Let p0(i, j) be the smallest such k, and q0(i, j) the largest. In this range all the values of A0 are -, at dead pairs, or x. Larger than x is impossible because it is the maximum. Smaller than x is impossible because if it occurred at (i-1, k) then Bs-1(i, k) > Bs-1(i, j) since their T values must be the same (otherwise one or both would be dead). If p0(i, j)  k  q0(i, j) then a function passing through (i-1, p0(i, j)) then continuing from (i, k) would have larger T value, which is impossible.
This property recursively applies throughout all stages, and for the B function as well, so one need only keep track of intervals, not all locations within them where x occurred. In general, for any stage s, for a
15

d1 2

2

2

2

c0 0

0

1

1

b0 0

1

1

2

a0 0

0

1

1

12

3

4

5

a) A_0 and deaths by end of stage 0

d0 1

2

3

3

c1 2

3

3

4

b0

0

0

2

2

a0 0

1

1

2

12

3

4

5

b) A_1 ignoring death

d0 0

0

0

2

c

b0 0

0

2

2

a0 0

1

1

12

3

4

5

c) A_1 incorporating death

d1 2

2

2

2

(a,d) (d,d) (d,d) (d,d) (d,d)

c0 0

0

1

1

(a,c) (a,c) (a,c) (b,b) (b,c)

b0 0

1

1

2

(a,b) (a,b) (a,b) (b,b) (a,b)

a0 0

0

1

1

(a,a) (a,a) (a,a) (a,a)

(a,a)

12

3

4

5

d) A_0 and the intervals

d0 0

0

0

0

c

b0 0

0

1

1

(a,b) (a,b) (a,b) (b,b) (a,b)

a

0
(a,a)

(a0,a)

(a1,a)

1
(a,a)

12

3

4

5

e) Correct A_1 via intervals and death: also shows intervals, deaths end stage 1

d

c

b1 2

2

2

2

a0 0

0

0

0

12

3

4

5

f) A_2 and deaths by end of stage 2. Strong L0 is remaining b, b, b, b, b

Figure 6: Steps, and Missteps, for Generating Strong L0. Original function values circled.

live (i, j) one keeps track of the interval ps(i, j) . . . qs(i, j), where As(i - 1, j) achieved the maximum in the calculation of As(i, j). At the next stage only values in this range are used to calculate As+1(i, j), and ps+1(i, j)  ps(i, j) and qs+1(i, j)  qs(i, j), i.e., the successive intervals can only shrink, not expand.
For each stage, the calculations at each index can be done in () time, giving the time claimed.

16

