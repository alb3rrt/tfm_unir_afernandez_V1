Language Independent Speech Emotion and Non-invasive Early Detection of Neurocognitive Disorder

arXiv:2106.01684v1 [cs.SD] 3 Jun 2021

Susmita Bhaduri1, Anirban Bhaduri2, Rajib Sarkar3
1Moonshot Analytics and Design 3/1/1, Dhakruia Station Lane, Kolkata - 700031, West Bengal, India 2Moonshot Analytics and Design 3/1/1, Dhakruia Station Lane, Kolkata - 700031, West Bengal, India 3Rajarhat Road, R. Gopalpur Kolkata -700136,West Bengal, India 1ORCID:0000-0003-1246-9124, E-mail: susmita.sbhaduri@moonshot.net.in
2ORCID:0000-0002-7787-3550, E-mail: bhaduri.anirban@moonshot.net.in
3E-mail: rjbsarkar@gmail.com

Abstract
Emotions(like fear,anger,sadness,happiness etc.) are the fundamental features of human behavior and governs his/her mental health. The subtlety of emotional fluctuations can be examined through perturbation in conversations or speech. Analysis of emotional state of a person from acoustical features of speech signal leads to discovery of vital cues determining his/her mental health. Hence, it's an important field of research in the area of Human Computer Interaction(HCI). In a recent work[1] we have shown that how the contrast in Hurst-Exponent calculated from the non-stationary and nonlinear aspects of angry and sad speech(spoken in English language) recordings in the Toronto-Emotional-Speech-Set(TESS)[2] can be used for early detection and diagnosis of Alzheimer's Disease. In this work we have extended the work and extracted Hurst-exponent for the speech-signals of similar emotions but spoken in German language(Emo-DB [3]). It has been observed that the Hurst-exponent efficiently segregates the contrasting emotions of anger and sadness in the speech spoken in German language, in similar fashion it has been doing for English speech. Hence it can be concluded that the Hurst-exponent can differentiate among speech spoken out of different emotions in language-independent manner. We propose algorithm for a language-independent application for early non-invasive detection of various severe neurocognitivedisorders like Alzheimer's Disease, MND(motor-neurondisorder), ASD(autism-spectrum-disorder), depression, suicidal-tendency etc. which is not possible with the state of the art medical science.
Keywords: Speech Emotion, Non-linearity, Language

Independence, Neurocognitive Disorder. PCAS Nos.: 10, 11.30.Pb, 24.60.Ky, 24.60.Lz
1 Introduction
Emotions are tangible expression of brain activity finely expressed through speech. Other than speech they are expressed via facial distortions, physical gestures and so on. Speech emotions are expressed both voluntarily or involuntarily. But recognizing the subtle content of emotion in human speech is one of the latest challenges in speech processing [4]. Besides human facial expressions, speech has proven as one of the most promising modalities for the automatic recognition of human emotions. Speech emotion recognition process revolves around the speech signal processing to detect and highlight the non-verbal features of speech. The data analysis process based of the parameters related to speech emotion is extremely helpful in various fields like psychology, medical science, lie detection tool, video games and security systems and so on.
Speech of any language is essentially form of expression of a person's emotion based on the physical and mental situation he or she may be in. Hence, emotion elicited in speech does not depend on the language [5]. Emotion is entwined with the personality, nature, temperament etc. of a person, which again changes from situation to situation. Neurocognitive disorders involve deterioration of cognitive abilities like memory, problem solving, perception, judgement etc. These disorders result from temporary or permanent damage of the brain, from degenerative processes like Alzheimer's or Parkinson's dis-

1

ease, motor neuron disorder dementia and also from affective disorders like depression, suicidal tendency, pathological anxiety and even from bipolar disorder, autism and dyslexia [6].
Among the various types of dementia, noways, most alarming one for elderly people is Alzheimer's Disease with symptoms like irreversible and growing cognitive deterioration, gradual memory loss, weakened judgment, speech impairment among other cognitive deficiencies. Eventually the patient becomes incapable of leading normal professional and social life. The symptoms are normally moderate during the early stage and normally related to age related issues like infrequent memory disorders, impairment in speech, sentence construction etc. Naturally 2 - 3 years are spent to take medical step after the beginning of more severe symptoms, that too after expensive neuro-psychological diagnosis. This is a major challenge faced for early detection of any neurocognitive disorder in a non-invasive manner for people in different countries and speaking different languages.
Traditionally different emotions were considered as categorically different and mutually exclusive from each other while devising models for emotion categorization. According to Cornelius The Big Six Emotions are: fear, anger, sadness, happiness, surprise and disgust [7]. But later as it was established that emotion is a continuous process, several two-dimensional models have been proposed of which Russell's [8] and Thayer's [9] are widely used which are defined by valence (whether the emotion elicits positivity or negativity) and arousal (activity, intensity of the emotion). The obvious criticism of this model is that starkly varying emotions with respect to cognitive, semantic and psychological significance associated with different languages may be close and indistinguishable in this rigidly defined emotional space.
There have been attempts to analyze various stationary aspects(like the power spectrum [10], energy distribution [11], prosody, formants, and glottal ratio spectrum [12] of speech signals to define non-invasive techniques of early detection of neurocognitive disorders, as speech is the most unsolicited, natural and instinctive means of communication and so it defines one's responsive and cognitive ability. Moreover, apart from Alzheimer's Disease, other types of dementia and motor neuron disorders, other neurocognitive disorders stated above, affect the tonal quality, energy-content impaired expression of emotion in speech irrespective of the languages it's spoken in. Non-invasive methodologies like Automatic Spontaneous Speech Analysis (ASSA), Emotional Response Analysis (ERA) for detecting neurocognitive disorders, are based on stationary features of speech signal like in-

tensity, short term energy, pitch, spectral centroid etc.
Speech contains a large variety of complex sounds with varying temporal grain, periodic and aperiodic components, noise, frequency and amplitude modulations etc. Most of works have been done to devise speech-emotion analysis system using the conventional stationary acoustic features. They deal with time domain features like Zero Crossing Rate, Short Time Energy and frequency domain features like signal bandwidth, spectral centroid, signal energy, fundamental frequency, Mel-Frequency Cepstral Co-efficients(MFCC). Linear discriminate classifier [13], the k-nearest neighbor (k-NN) [14] and Support Vector Machine(SVM) [15] have mostly been used in this context. Most of these stationary techniques involve Fourier spectral analysis which is based on linear super-positions of trigonometric functions. Secondary harmonic components which is common in natural non-stationary timeseries, may generate a distorted wave outline for these natural signals. These distortions are the consequence of nonlinear contributions which are not normally extracted from the non-stationary signals, when analyzed using these stationary techniques. Moreover, none of these attempts are language independent and largely depends on local dialects of various languages.
All the organs of human body behave nonlinearly due to their inherent complex dynamic nature. The process of speech production and cognition by human beings is complex phenomena [16]. The theory of complexity is rooted in chaos theory [17] and have various parameters whose combined behavior refers to a border between order and randomness, termed as the edge of chaos [18]. As per chaos theory, a chaotic system is extremely sensitive to initial conditions, does not repeat itself and however is deterministic. The chaos-based complexity theory attempts to decode behavior of dynamic nonlinear systems [19, 20, 21].
To provide order or definite properties to an structural form inherent in the chaotic system, fractal geometry has been evolved [22]. According to Mandelbrot [23, 24], fractal is a geometric scheme which repeats itself at smaller or larger scales to generate self-similar, irregular shapes or surfaces that can not be represented by Euclidean geometry. Fractal systems can extend to infinitely large values of their coordinates, in all direction from the center towards the outside. The principal feature of fractal -s is its self-similarity. It is a phenomenon where smaller and bigger fragments of a system looks very alike, but not necessarily exactly the same, to the whole fractal system. Power-law (as per statistics, a power-law is a functional relationship between two quantities, where one quantity varies as a power of another) is applied to represent the

2

self-similarity of the large and small fragments of a fractal system. This power-law exponent is defined as the scaling exponent of the self-similarity or the fractal dimension of the system.
Detrended Fluctuation Analysis(DFA) method[25, 26] has been implemented for analyzing the long-range correlations in noisy and non-stationary time series and determining their monofractal scaling exponents. If the time series is long-range correlated, its DFA function shows a power-law relationship with its scale parameter. If we denote the DFA function of the time series by F (s) and its scale parameter by s, F (s) will vary with a power of s as per the equation F (s)  sH , here the exponent H is termed as Hurst exponent. If DF is the fractal dimension, it is related with H-Hurst exponent as per the equation DF = 2 - H [27]. Results obtained by DFA method are proved to be more reliable compared to the methods like Wavelet Analysis, Discrete Wavelet Transform, Wavelet Transform Modulus Maxima, Detrending Moving Average, Band Moving Average, Modified Detrended Fluctuation Analysis etc. [28, 29, 30]. We have applied this method successfully for analysing various kinds of time series formed from natural signals like speech signals[1] and biological signals like EEG and ECG signals [31, 32, 33, 34].
The speech production process exhibits fractal characteristics. The quasi-static oscillations of the vocal folds and the adaptation process of the vocal tract are both nonlinear processes [35]. Fractal nature of speech has been explored for automatic speech recognition [36], speaker recognition [37], speech decomposition [38], speech segmentation, representation and characterization [39]. Regarding the speech-emotion recognition from nonlinear perspective, classifiers like Artificial Neural Network (ANN) and decision trees are implemented due to their consistent performances in specific cases [40]. However, it has been detected that the same feature vector produces completely different classification performance using varried algorithms [41]. Moreover, extraction and selection of features is a crucial parameter in creating language independent speech emotion-detection model and in determining on what emotion a specific speech should be tagged to. Other characteristic features should cover speaking/listening process, linguistics specific to a language and so on.
Hence, we should define a speech-emotion classification system by analysing speech as complex system using state of the art methods in fractal domain, in contrast with the conventional stationary techniques. This way all aspects of speech signal can be understood at the deepest level. In our earlier work [42], we have ap-

plied multi-fractal method to the time-displacement profile of speech(non-musical), drone(periodically musical) and Indian art music samples having different musicality(emotions) and showed that the value of the width of the multi-ractal spectrum is substantially different for speech and music signals.
In another work [1], we have applied the similar approach over speech signal and proposed a quantitative parameter for categorizing various emotions based on the Hurst exponent, by analyzing the non-stationary details of the dynamics of speech signal, generated out of differing emotions. A non-invasive system has been proposed using this parameter for early detection of Alzheimer's Disease. Complex network based Visibility Graph method has been applied to analyze the audio signals of speech and music(drone) and a non-invasive model for early detection and monitoring of autism spectrum disorder has been proposed using non-stationary acoustic cues for differentiating speech and music signals in [43]. Modified Visibility Graph analysis has been done over speech signals spoken out of various emotions and application for detecting suicidal tendency using nonlinear and nonstationary aspects of speech is proposed in [44]. However all these applications are designed to work for speech spoken in English language. In this work we have extended the work and extracted Hurst exponent for the speech signals generated out of similar emotions but spoken in German language. It has been observed that the Hurst Exponent efficiently segregates the contrasting emotions of anger and sadness in the speech spoken in German language, in a similar fashion it has been doing for the speech signal of English language. Hence it can be concluded that the Hurst exponent can differentiate among speech spoken out of different emotions in a language independent manner.
Based on the nonlinear and non-stationary parameters of speech(indepenedent of the language it's spoken in) and earlier works [42, 1, 44, 43], we can model non-invasive language independent applications for early detection and constant monitoring of various neurocognitive disorders which would have substantially lesser infrastructural and technological cost.
This would be a positive step towards defining various applications based on speech emotions in terms nonlinear and non-stationary acoustic parameters of speech signals irrespective of languages they are spoken in. This would make the applications language independent and hence portable.
The rest of the paper is organized as follows. The details of data is in Section 2, the method of analysis is elaborated and the inferences from the test results are

3

3.1 Hurst Exponent

presented in Section 3.2. The inferences are discussed and an example of the application for early detection and monitoring a neurocognitive disorder in non-invasive and language independent manner is elaborated in Section 4.
2 Data
In this work, we have used two datasets consisting of emotional speech-audio clips, namely Emo-DB [3] for German language and TESS [2] for English(USA) language.
The recordings of Emo-DB [3] dataset were done in the anechoic chamber with high quality recording equipment at the Technical University Berlin, department of Technical Acoustics. Ten German emotional utterances, mostly used in everyday communication and could be interpreted in all the applied emotions, were produced in German language by ten actors. The actors(five men and five women) simulated the emotions. The age of all actors were between 21 - 35 years. The recorded speech signal-files of Emo-DB dataset are in .wav format with sampling rate of 16kHz and of single channel. Duration of the audio files are around 2 to 5 seconds. In this experiment, we have taken 127 recorded speech signal-files spoken out of anger emotion and 62 files created out of sad emotion.
The recordings of Toronto Emotional Speech Set (TESS) [2] dataset created by Kate Dupuis and M. Kathleen Pechora-Fuller at the University of Toronto Psychology Department. We have obtained it from the website:TESS-data. In TESS dataset, there are voice samples of 26 and 64 year old actresses speaking a set of 200 target words in English. The recorded speech signal-files are in .wav format with sampling rate of 24.4kHz and of single channel. In this experiment, we have taken 200 recorded speech signal-files spoken out of anger emotion and 200 files created out of sad emotion. Moreover, we have considered the speech signal-files created for young actresses because the actors of Emo-DB dataset are of the same range of age.
The amplitude waveforms of the sound files are taken for the experiment. Empirical mode decomposition method [45] is applied over the original signal to remove noise. Then each of the .wav file is converted to a corresponding text file.
3 Method
For each of the text files obtained from the .wav file as per the process described in the Section 2, Hurst exponenet values are calculated as per the method of Peng et.al. [25]

and Kantelhardt et.al. [26]. This method is described in Section 3.1. For the files created from emotional English speech-signals, two sets of 200 Hurst exponents are calculated for each of the angry and sad emotions. For the files created from German recordings 127 and 62 Hurst exponents are calculated for angry and sad emotions respectively. After that with all the Hurst exponent values frequency histogram is created in Section 3.2 and Hurst exponent values for anger and sad emotional speech spoken in English and German language are deduced and parameters for language independent non-invasive application for diagnosis of neurocognitive disorders are defined.

3.1 Hurst Exponent
Detrended Fluctuation Analysis(DFA) defined by Peng et.al. [25] has been applied successfully over various nonstationary time series for detecting long-range correlations.

1. Let us denote the input data series as x(i) for i =

1, 2, . . . , N , with N number of points. The mean val-

ues

of

this

series

is

calculated

as

x¯

=

1 N

N i=1

x(i).

Then accumulated deviation series for x(i) is calcu-

lated as per the below equation.

i
X(i)  [x(k) - x¯], i = 1, 2, . . . , N
k=1
This subtraction of the mean(x¯) from the data series, is a standard way of removing noisy data from the input data series. The effect of this subtraction would be eliminated by the detrending in the fourth step.

2. X(i) is divided into Ns non-overlapping segments, where Ns  int(N/s), s is the length of the segment. In our experiment s varies from 16 as minimum to 1024 as maximum value in log-scale.

3. For each s, we denote a particular segment by v(v =

1, 2, . . . , Ns). For each segment least-square fit is per-

formed to obtain the local trend of the particular seg-

ment [25]. Here xv(i) denotes the least square fitted

polynomials for the segment v in X(i). xv(i) is calcu-

lated as per the equations xv(i) =

m k=0

Ck

(i)m-k

,

where Ck is the kth coefficients of the fit polynomi-

als with degree m. For fitting linear, quadratic, cubic

or higher m-order polynomials may be used [26, 46].

For this experiment m is taken as 1 for linear fitting.

4

3.2 Analysis of Hurst Exponents

4. To detrend the data series, we have to subtract the polynomial fit from the data series. There is presence of slow varying trends in natural data series. Hence to quantify the scale invariant structure of the variation around the trends, detrending is required. Here for each s and segment v  1, 2, . . . , Ns, detrending is done by subtracting the least-square fit xv(i) from the part of the data series X(i), for the segment v to determine the variance, denoted by F 2(s, v) calculated as per the following equation.

and vice versa, in the series. In the Figure 1, the comparison of the Hurst exponents calculated for 30 samples for each of the anger and sad emotions for both the English and German languages are shown. It's evident that the speech signals spoken out of anger emotion has antipersistent long-range correlations, whereas, those of sad emotion have persistent long-range correlations, for both the languages.
3.2 Analysis of Hurst Exponents

F 2(s, v)



1 s

s
{X[(v - 1)s + i] - xv(i)}2,

i=1

Hurst exponents are calculated for · Emotional English speech-signals

where s  16, 32, . . . , 1024 and v  1, 2, . . . , Ns.
5. Then the qth-order fluctuation function, denoted by Fq(s), is calculated by averaging F 2(s, v) over all the segments(v) generated for each of the s  16, 32, . . . , 1024 and for a particular q, as per the equation below.

1

Fq(s) 

1

Ns

[F

2

(s,

v

)]

q 2

Ns v=1

q
,

for q

=

0

because in

that

case

1 q

would blow

up. For q = 2, calculation of Fq(s) boils down to

standard method of Detrended Fluctuation Analy-

sis(DFA) [25].

6. The above process is repeated for different values of s  16, 32, . . . , 1024 and it can be seen that for a specific q, Fq(s) increases with increasing s. If the series is long range power correlated, the Fq(s) versus s for a particular q, will show power-law behavior as below.

Fq(s)  sh(q)

If this kind of scaling exists, log2[Fq(s)] would depend linearly on log2 s, where h(q) is the slope which depends on q. h(2) is similar to the well-known
Hurst exponent [26]. So, in general, h(q) is the
generalized Hurst exponent.

If h(2) = 0.5 then there is no correlation. Further, if h(2) > 0.5 then there is persistent long-range crosscorrelations, where the large value of one variable is likely to be followed by a large value of another variable in the series, whereas, h(2) < 0.5 then there is anti-persistent long-range cross-correlations, where a large value of one variable is most likely to be followed by a small value

1. 200 Hurst exponent values for speech-signal files recorded for anger emotion.
2. 200 Hurst exponent values for speech-signal files recorded for sad emotion.
· Emotional German speech-signals
1. 127 Hurst exponent values for speech-signal files recorded for anger emotion.
2. 62 Hurst exponent values for speech-signal files recorded for sad emotion.
Then for each of the languages, the frequency histogram of occurrence of a specific Hurst exponent for each of the two emotions are formed. The bin width of the histograms is decided on the basis of the range of the Hurst exponents in a particular category of emotion for each of the languages. The peak of a particular histogram formed for the particular set of Hurst exponents calculated for a particular emotion and language, is considered as the Hurst exponent calculated for that emotional speech in that language. Figure 2 shows the comparison of Hurst exponents extracted from the histograms for each emotion and language. The Figure shows that Hurst exponent calculated for anger emotion is startlingly less than sad emotion, consistently for both the languages.
4 Conclusion
The most striking finding of present investigation based on robust nonlinear methods is that the fundamental emotions like anger and sadness are clearly distinguishable from the speech signals in a language independent manner, which was not reported in the earlier studies. As the emotions expressed in speech varies from subject to subject a range of values or threshold for the Hurst exponent calculated for each of the emotions of anger

5

4.1 Proposed Exemplary Application

EngHurst

GerHurst

EngHurst

GerHurst

angry

angry

sad

sad

1

0.8

0.6

0.4

0.2

0 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31

Figure 1: Comparison of the Hurst exponents calculated for 30 samples for each of the anger and sad emotions for both the English and German languages.

Hurst exponents

0.7

0.6

English German

0.5

0.4

0.3

0.2

0.1

0

Angry

Sad

Figure 2: Trend of Hurst exponents for anger and sad emotion for both the English and German languages.

and sadness would be defined. If the Hurst exponent calculated for an emotional speech signal spoken in any language falls within any of the two ranges(for anger or sadness) then we can detect and tag the emotion of the speech as any of the two. This range should be defined by analyzing the fractal properties of time-displacement profile using the method in Section 2 and Section 3.2, for large number of speech samples of different emotions and different language. Using this method, non-invasive applications for detecting various severe neorocognitive disorders like Alzheimer's disease, motor neuron disorder, autism spectrum disorder, disorder of consciousness, suicidal tendency etc., can be devised. As an example, we have broadly outlined a framework for one such application for early detection of neorocognitive disorders in the Section 4.1.
4.1 Proposed Exemplary Application
As already mentioned in the Section 1, neurocognitive disorders involve cognitive impairment restricting proper emotional expression in speech, memory problem, issues involving problem solving, perception, judgment etc. and these disorders result from temporary or permanent damage to the brain, degenerative processes like Alzheimer's or Parkinson's disease, dementia, motor neu-

ron disorders and also from affective disorders like depression, pathological anxiety and even from bipolar disorder, autism and dyslexia [6]. Recently we have proposed a quantitative parameter for categorizing various emotions based on the Hurst exponent, by analyzing the non-stationary details of the dynamics of speech signal spoken in English language, generated out of differing emotions. A non-invasive system has been framed using this parameter for early detection of Alzheimer's Disease [1]. Further we have applied complex network based Visibility Graph and Modified Visibility Graph methods to analyze non-stationary aspects of speech(spoken in English language) and proposed non-invasive application for early detection of autism spectrum disorder and suicidal tendency [43, 44]. The initial version of the proposed algorithm for speech emotion detection is in the website:Speech-Emotion. The present experiment confirms that the Hurst exponent is language independent for emotional(anger and sadness) speech signals.
In this work, we propose a quantitative framework to capture the change in intricate dynamics of speech spoken in any language by normal subject and a subject suffering from neurocognitive disorders. According to the steps elaborated below, three types of control elements are calculated.
· One is for normal subjects

6

4.1 Proposed Exemplary Application

· Second for subjects who have already been diagnosed with some kind of neurocognitive disorder
· Third one is for any subject to be diagnosed for similar disorder.
Then depending upon the proximity of the third one to the first or second one, proneness or onset of disorder can be decided.
1. First emotional speech signals generated by large number of normal subjects speaking varying languages, would be collected. Here we have considered the two most fundamental emotions of anger and sadness.Then, after doing the scaling analysis of amplitude-profile of the speech signals using the method described in Section 3 the range of Hurst exponents for normal subjects, say denoted by Hnorm±norm, can be base-lined. This would be first control element for this application for monitoring and early detection of any neorocognitive disorder.
2. Similarly, the audio clip of the emotional speech signals generated by the subjects already been diagnosed with any of the neorocognitive disorders, would be recorded. People suffering from neorocognitive disorders elicit emotion in their speech differently than normal, irrespective of the language they are speaking in. Hence, the speech spoken out of fundamental emotions by such subjects, would definitely display different scaling behavior in all acoustic aspects, than those of normal subjects. Therefore, using the same method of Step 1, the second control element, say denoted by Hdis ±dis, for diseased subjects suffering from neorocognitive disorders, can be base-lined.
3. Different ranges of the first two control elements with varying norm and dis, would be defined, to reflect the proneness or the severity of neorocognitive disorders. One sample set of ranges is given below.
(a) Severity 1:First range for deciding that whether the subject to be diagnosed is at all prone to neorocognitive disorders or not.
(b) Severity 2:Second range for deciding the onset of neorocognitive disorders.
(c) Severity 3:Third for prognosis of neorocognitive disorders.
4. Simple and lightweight android application would be framed where the two types of control elements(calculated for normal and diseased subjects)

containing ranges of language independent Hurst exponents with varying  values calculated from emotional speech signals for fundamental emotions, would be stored locally.
When the subject would be speaking over the phone, his/her speech signal would be recorded in .wav format and the Hurst exponent would be calculated from its amplitude-profile. This would be the third control element as stated before. This would be matched against the first two control elements, and according to the severity ranges of norm and dis defined in Step 3 to decide the proneness to any neorocognitive disorder, is decided for the subject. The application may be scheduled to be run as per the default frequency preset in the application or it might run as a background application and the third control element would be calculated each time the subject would make a phone call. Third control element would be checked against the Severity 1 -range, which would signify the proneness of the subject to the neorocognitive disorder.
If this range of values is obtained for the third element for more than certain present frequency, and the third control element is getting nearer to Severity 2 -range an alarm would be generated and the onset of the disorder can be confirmed and accordingly further prognosis may be started. This would be a real-time, non-invasive, language independent and routine check-up framework for self-assessment as well as monitoring of any neurocognitive disorder.
The Severity 3 -range would be decided based on the prognosis parameters. Once the onset of the disease is confirmed the subject would be under necessary treatment and then the third control element would be matched against the first two control elements according to this severity range. This way continuous monitoring of the patient would be done during prognosis period until the proximity with Severity 1 -range would be met, which would in turn set a guideline for the treatment of the patient.
5. This would be definitely an cost effective application and an efficient one because the control elements are defined on the basis of the fundamental and most prominent emotions of anger and sadness, so the subjects speaking any language would certainly elicit these emotions most of the time. And, as we have already discussed, the less prone a subject would be to any neurocognitive disorder the more prominently the fundamental emotions would be expressed

7

REFERENCES

in his/her speech and vice-verse. So, the control elements defined based on these emotions would be the most effective ones and the scaling disparity between normal and person prone to these disorders, would enable us in the most robust and efficient way, to detect the disorder in non-invasive and language independent manner.
6. For the first two control elements, feedback system would be implemented as more and more number of speech samples of normal and diseased subjects speaking various languages would be analyzed for emotion detection, and the control elements would be revised accordingly and eventually the performance of the android application would be refined.

[7] R. R. Cornelius, The Science of Emotion: Research and Tradition in the Psychology of Emotion, Pearson, 1995.
[8] J. Russell, A circumplex model of affect, Journal of personality and social psychology 39 (6) (1980) 1161­ 1178.
[9] R. E. Thayer, The biopsychology of mood and arousal, Oxford University Press, 1990.
[10] W. A. Hargreaves, J. A. Starkweather, K. H. Blacker, Voice quality in depression., Journal of Abnormal Psychology 70 (3) (1965) 218­220. doi:10.1037/h0022151. URL http://content.apa.org/journals/abn/70/3/218

References References

[11] F. Tolkmitt, H. Helfrich, R. Standke, K. Scherer, Vocal indicators of psychiatric treatment effects in depressives an Journal of Communication Disorders 15 (3) (1982) 209­222. doi:10.1016/0021-9924(82)90034-X. URL http://linkinghub.elsevier.com/retrieve/pii/002199

[1] S. Bhaduri,

R. Das,

D. Ghosh, [12] R.

Sun,

E.

Moore,

Non-Invasive Detection of Alzheimer's Disease-MultifractalIitnyveosftiEgmatointigongaloltStapleepcahr,ameters and teager energy operators in e

Journal of Neurology and Neuroscience 7 (2:84)

in: S. D'Mello, A. Graesser, B. Schuller, J.-C. Mar-

(2016) 1­7. doi:10.21767/2171-6625.100084.

tin (Eds.), Lecture Notes in Computer Science

URL http://www.jneuro.com/neurology-neuroscience/(ninocnliundvinagsivseu-bdseertieesctiLoenc-tuorfe-aNlzohteesimeinrs-Adritisfiecaiaslemultifract

[2] K. Dupuis, M. K. Pichora-Fuller, Toronto emotional speech set (tess), abcd (2010).

Intelligence and Lecture Notes in Bioinformatics), Vol. 6975 LNCS, Springer Berlin Heidelberg, Berlin, Heidelberg, 2011, pp. 425­434.

[3] F. Burkhardt, A. Paeschke, M. Rolfes, W. F.

doi:10.1007/978-3-642-24571-8{\_}54.

Sendlmeier, B. Weiss, A database of german emo-

URL http://dx.doi.org/10.1007/978-3-642-24571-8{_}54

tional speech., Interspeech 5 (2005) 1517­1520.

[13] D.

Roy,

A.

Pentland,

[4] M. El Ayadi, M. S. Kamel, F. Karray, Survey on

Automatic spoken affect classification and analysis,

speech emotion recognition: Features, classification

Proceedings of the Second International Conference

schemes, and databases, Pattern Recognition 44 (3)

on Automatic Face and Gesture Recognition (1996)

(2011) 572­587.

363­367doi:10.1109/AFGR.1996.557292.

[5] H.

K.

Maganti,

M.

Matassoni,

URL http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.h

Auditory processing-based features for improving spee[c1h4]reJc.ogRn.iAtiovnerinll,aIdnvtehreseeayceosuosfttichecobnedhiotlidoenrs,, The nature

Eurasip Journal on Audio,

Speech,

of emotion: Fundamental questions (1994) 7­14.

adUnoRdiL:1h0t.t1Mp1su8:6s/i/c/1a6s8m7p-P-4er7ou2cr2eas-ss2iin0pg1j4o-u2r1n.2a0l1s4.spri(n2g0e1r4o)p. en[.1c5o] mMta/inoa.dnrYtErioexcucpol,oge,Cns2i/.t01ioC00n6.h1eIf1rnEo8,Em6EJ/.1nI6onB8itsu7ey,-rn4sJap7.te2ie2oLcn-ihua2,,l0i1CnJ4:o.-nM2Tf1eauroelt,nimcEeemdoonia-,

[6] M. Ganguli, D. Blacker, D. G. Blazer, I. Grant, D. V.

IEEE, 2006, pp. 1653­1656.

Jeste, J. S. Paulsen, R. C. Petersen, P. S. Sachdev, Classification of neurocognitive disorders in DSM-5: The American journal of geriatric psychiatry

a[w16o]rkTSii.mnVppalrenogaZrneadsnsCd.,to,mRpl.exWS.yPstreomctso, r2,0H08u.man

Factors

in

: official journal of the American Association [17] H. Poincar´e, Sur le probleme des trois corps et les {´e}quations de

for Geriatric Psychiatry 19 (3) (2011) 205­10.

Acta Mathematica 13 (1) (1890) 5­7.

doi:10.1097/JGP.0b013e3182051ab4.

doi:10.1007/BF02392506.

URL http://www.pubmedcentral.nih.gov/articlerendeUrR.fLcghit?tapr:t/i/dl=i3n0k7.6s3p7r0i{n&g}etro.oclo=mp/m1c0e.n1t0r0e7z/{B&F}0r2e3n9d2e5r0t6ype=abs

8

REFERENCES

[18] J. Horgan, From Complexity to Perplexity, Scientific American 272 (6) (1995) 104­109. doi:10.1038/scientificamerican0695-104.

E - Statistical, Nonlinear, and Soft Matter Physics 74 (1) (2006). arXiv:0504608, doi:10.1103/PhysRevE.74.016103.

[19] Richard Gallagher,

Tim Appenzeller, [29] E. Serrano, A. Figliola, Wavelet Leaders: A

Beyond Reductionism, Science 284 (5411) (1999)

new method to estimate the multifractal singu-

79.

larity spectra, Physica A: Statistical Mechanics

URL http://www.sciencemag.org/content/284/5411/79a.nfdulilts Applications 388 (14) (2009) 2793­2805.

doi:10.1016/j.physa.2009.03.043. [20] D. C. Mikulecky, The emergence of complexity: science coming of age or science growing old?,

Computers & Chemistry 25 (4) (2001) 341­348. [30] Y. X. Huang, F. G. Schmitt, J. P. Hermand,

doi:10.1016/S0097-8485(01)00070-5.

Y. Gagne, Z. M. Lu, Y. L. Liu, Arbitrary-order

URL http://linkinghub.elsevier.com/retrieve/pii/SH0i0l9b7er8t48s5p0e1c0t0ra0l70a5nalysis for time series possess-

ing scaling statistics: Comparison study with de-

[21] J. P. Higgins, Nonlinear systems in medicine, Yale

trended fluctuation analysis and wavelet leaders,

Journal of Biology and Medicine 75 (5-6) (2003) 247­

Physical Review E - Statistical, Nonlinear, and Soft

260.

Matter Physics 84 (1) (2011). arXiv:1107.3611,

[22] H.-O. Peitgen, H. Ju¨rgens, D. Saupe,

doi:10.1103/PhysRevE.84.016208.

Chaos and Fractals, Springer New York, New [31] S.

Bhaduri,

D.

Ghosh,

York, NY, 2004. doi:10.1007/b97624.

Electroencephalographic Data Analysis With Visibility Graph Te

URL http://www.springer.com/in/book/9780387202297Chltintpic:a/l/liEnEkG.spraindger.nceoumr/o1sc0i.e1n0ce07/b(29071642)4 3­

8doi:10.1177/1550059414526186. [23] B. Mandelbrot, How Long Is the Coast of Britain? StatisticUalRSLehlft-Stipm:i/la/rwiwtyw.anncdbiF.rancltmi.onniahl .Dgiomve/npsuiobnm,ed/24781371
Science 156 (3775) (1967) 636­638.

doi:10.1126/science.156.3775.636.

[32] A.

Bhaduri,

D.

Ghosh,

URL http://www.sciencemag.org/cgi/doi/10.1126/sciQenucaen.ti1t5a6ti.v3e7a7s5s.e6ss3m6ent of heart rate dynamics during meditatio

[24] B. B. Mandelbrot, The Fractal Geometry of Nature,

Vol. 51, 1983.

arXiv:arXiv:1011.1669v3,

doi:10.1017/CBO9781107415324.004.

Frontiers in Physiology 7 (FEB) (feb 2016). doi:10.3389/fphys.2016.00044. URL http://journal.frontiersin.org/Article/10.3389/fph

URL http://link.aip.org/link/?AJP/51/286/1{&}[A3g3g] =Nd.oi P, A. B, S. B, D. G,

[25] C. K. Peng, S. V. Buldyrev, S. Havlin, M. Si-

Non-Invasive Alarm Generation for Sudden Cardiac Arrest: A P Translational Biomedicine 7 (3) (2016).

mons, H. E. Stanley, A. L. Goldberger, Mosaic organization of DNA nucleotides, Physical Review E 49 (2) (1994) 1685­1689.

doi:10.21767/2172-0479.100079. URL http://www.transbiomedicine.com/translational-bio

doi:10.1103/PhysRevE.49.1685.

[34] A. Bhaduri, S. Bhaduri, D. Ghosh,

URL http://link.aps.org/doi/10.1103/PhysRevE.49.1V6i8s5ibility graph analysis of heart rate time series and bio-marker

Physica A: Statistical Mechanics and

[26] J. W. Kantelhardt, E. Koscielny-Bunde,

its Applications 482 (2017) 786­795.

H. H. A. Rego, S. Havlin, A. Bunde,

doi:10.1016/j.physa.2017.04.091.

Detecting long-range correlations with detrended fluctuatioUnRaLnahltytsips:, //linkinghub.elsevier.com/retrieve/pii/S03784

Physica A 295 (3­4) (2001) 441­454. URL http://www.sciencedirect.com/science/arti[c3l5e] /Wpi.i/JS. 0L37ev8e4l3t7, 1M01o0d0e1l4s 4o3f{w%o}r5dCnppraopdeurcsti2o:n/,/pTurbelnidcsation/uuid/
in Cognitive Sciences 3 (6) (1999) 223­232.

[27] B. B. Mandelbrot, Self-affine fractals and fractal dimension,doi:10.1016/S1364-6613(99)01319-4.

Physica Scripta 32 (4) (1985) 257­260.

URL http://linkinghub.elsevier.com/retrieve/pii/S13646

doi:10.1088/0031-8949/32/4/001. URL http://stacks.iop.org/1402-4896/32/i=4/a=[03061] ?Pk.ey=crosMsraerfa.gbo3s2, 8d75d869A80. 4a0b0e8P9oat9aam91ia7n7o9s6,a4
Fractal dimensions of speech sounds: Computation and applicati

[28] P. O´swicimka, J. Kwapien´, S. Drozda,

The Journal of the Acoustical Society of America

Wavelet versus detrended fluctuation analy-

105 (3) (1999) 1925. doi:10.1121/1.426738.

sis of multifractal structures, Physical Review

URL http://scitation.aip.org/content/asa/journal/jasa/

9

REFERENCES

[37] D. C. Gonz´alez, L. Luan Ling, F. Violaro,

doi:10.21767/2171-6625.1000100.

Analysis of the multifractal nature of speech signals,

URL http://www.jneuro.com/neurology-neuroscience/spee

in: Lecture Notes in Computer Science (in-

cluding subseries Lecture Notes in Artificial [45] N. E. Huang, Z. Shen, S. R. Long, M. C. Wu, H. H.

Intelligence and Lecture Notes in Bioinfor-

Shih, Q. Zheng, N.-C. Yen, C. C. Tung, H. H. Liu,

matics), Vol. 7441 LNCS, 2012, pp. 740­748.

The empirical mode decomposition and the Hilbert spectrum for

doi:10.1007/978-3-642-33275-3{\_}91.

Proceedings of the Royal Society A: Mathematical,

URL http://link.springer.com/10.1007/978-3-642-33P2h7y5s-ic3a{l_a}n9d1 Engineering Sciences 454 (1971) (1998)

903­995. doi:10.1098/rspa.1998.0193.

[38] A. Langi, K. Soemintapura, W. Kinsner,

URL http://rspa.royalsocietypublishing.org/cgi/doi/10.

Multifractal processing of speech signals, in: Proceedings of ICICS, 1997 International Conference

[46]

on Information, Communications and Signal Pro-

cessing. Theme: Trends in Information Systems

Engineering and Wireless Multimedia Communi-

cations (Cat. No.97TH8237), Vol. 1, IEEE, pp.

527­531. doi:10.1109/ICICS.1997.647154.

URL http://ieeexplore.ieee.org/document/647154/

J. W. Kantelhardt, S. A. Zschiegner, E. KoscielnyBunde, S. Havlin, A. Bunde, H. Stanley, Multifractal detrended fluctuation analysis of nonstationary time Physica A: Statistical Mechanics and its Applications 316 (1-4) (2002) 87­114. doi:10.1016/S0378-4371(02)01383-3. URL http://linkinghub.elsevier.com/retrieve/pii/S03784

[39] W. Kinsner, W. Grieder, Speech segmentation using multifractal measures and amplification of signal features, Proceedings of the 7th IEEE International Conference on Cognitive Informatics, ICCI 2008 (2008) 351­ 356doi:10.1109/COGINF.2008.4639188.

[40] B. Schuller, A. Batliner, D. Seppi, S. Steidl, T. Vogt, J. Wagner, L. Devillers, L. Vidrascu, N. Amir, L. Kessous, et al., The relevance of feature type for the automatic classification of emotional user states: Low level descriptors and functionals, in: Eighth Annual Conference of the International Speech Communication Association, 2007.

[41] Y. Kim, H. Lee, E. M. Provost, Deep learning for robust feature generation in audiovisual emotion recognition, in: Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, IEEE, 2013, pp. 3687­3691.

[42] S.

Bhaduri,

D.

Ghosh,

Speech, music and multifractality,

Cur-

rent Science 110 (9) (2016) 1817­1822.

doi:10.18520/cs/v110/i9/1817-1822.

URL http://www.currentscience.ac.in/Volumes/110/9/1822.pdf

[43] S. Bhaduri, D. Ghosh, Speech and Music ­ Nonlinear Acoustical Decoding in Neurocognitive Scenario, Archives of Acoustics In-press (2018).

[44] S. Bhaduri, A. Chakraborty, D. Ghosh, Speech Emotion Quantification with Chaos-Based Modified Visibility Graph- Possible Precursor of Suicidal Tendency, Journal of Neurology and Neuroscience 7 (3) (2016).

10

