arXiv:2106.01577v1 [cs.LG] 3 Jun 2021

A Provably-Efficient Model-Free Algorithm for Constrained Markov Decision Processes
Honghao Wei, Xin Liu, and Lei Ying University of Michigan, Ann Arbor

Abstract

This paper presents the first model-free, simulator-free reinforcement learning algorithm for Con-

strained Markov Decision Processes (CMDPs) with sublinear regret and zero constraint violation.

The algorithm is named Triple-Q because it has three key components: a Q-function (also called

action-value function) for the cumulative reward, a Q-function for the cumulative utility for the

constraint, and a virtual-Queue that (over)-estimates the cumulative constraint violation. Under

Triple-Q, at each step, an action is chosen based on the pseudo-Q-value that is a combination of the

three "Q" values. The algorithm updates the reward and utility Q-values with learning rates that

depend on the visit counts to the corresponding (state, action) pairs and are periodically reset. In the

episodic CMDP setting, Triple-Q achieves O~

1 

H

4S

1 2

A

1 2

K

4 5

regret, where K is the total number

of episodes, H is the number of steps in each episode, S is the number of states, A is the number of

actions, and  is Slater's constant. Furthermore, Triple-Q guarantees zero constraint violation when

K is sufficiently large. Finally, the computational complexity of Triple-Q is similar to SARSA for

unconstrained MDPs, and is computationally efficient.

1 Introduction
Reinforcement learning (RL), with its success in gaming and robotics, has been widely viewed as one of the most important technologies for next-generation, AI-driven complex systems such as autonomous driving, digital healthcare, and smart cities. However, despite the significant advances (such as deep RL) over the last few decades, a major obstacle in applying RL in practice is the lack of "safety" guarantees. Here we use "safety" to refer to a wide range of operational constraints. The objective of an RL algorithm is to maximize the expected cumulative reward, but in practice, many applications need to be operated under a variety of constraints, such as collision avoidance in robotics and autonomous driving [18, 12, 11], legal and business restrictions in financial engineering [1], and resource and budget constraints in healthcare systems [31]. These applications with operational constraints can often be modeled as Constrained Markov Decision Processes (CMDPs), in which the agent's goal is to learn a policy that maximizes the expected cumulative reward subject to the constraints.
Earlier studies on CMDPs assume the model is known. A comprehensive study of these early results can be found in [3]. RL for unknown CMDPs has been a topic of great interest recently because of its importance in Artificial Intelligence (AI) and Machine Learning (ML). The most noticeable advances recently are model-based RL for CMDPs, where the transition kernels are learned and used to solve the linear programming (LP) problem for the CMDP [23, 6, 15, 10], or the LP problem in the primal component of a primal-dual algorithm [21, 10]. If the transition kernel is linear, then it can be learned in a sample efficient manner even for infinite state and action spaces, and be used in the policy evaluation and improvement in a primal-dual algorithm [8].

1

The performance of a model-based RL algorithm depends on how accurately a model can be estimated. For some complex environments, building accurate models could be challenging computationally and data-wise [25]. For such environments, model-free RL algorithms often are more desirable.
However, there has been little development on model-free RL algorithms for CMDPs with provable optimality or regret guarantees, with the exceptions [9, 30, 7], all of which require simulators. In particular, the sample-based NPG-PD algorithm in [9] requires a simulator which can simulate the MDP from any initial state x, and the algorithms in [30, 7] both require a simulator for policy evaluation. It has been argued in [4, 5, 13] that with a perfect simulator, exploration is not needed and sample efficiency can be easily achieved because the agent can query any (state, action) pair as it wishes. Unfortunately, for complex environments, building a perfect simulator often is as difficult as deriving the model for the CMDP. For those environments, sample efficiency and the exploration-exploitation tradeoff are critical and become one of the most important considerations of RL algorithm design. Therefore, this paper considers model-free algorithms for CMDPs without a simulator.

1.1 Main Contributions

In this paper, we consider the online learning problem of an episodic CMDP with a model-free approach without a simulator. We develop the first model-free RL algorithm for CMDPs with sublinear regret and zero constraint violation (for large K). The algorithm is named Triple-Q because it has three key components: (i) a Q-function (also called action-value function) for the expected cumulative reward, denoted by Qh(x, a) where h is the step index and (x, a) denotes a state-action pair, (ii) a Q-function for the expected cumulative utility for the constraint, denoted by Ch(x, a), and (iii) a virtual-Queue, denoted by Z, which (over)estimates the cumulative constraint violation so far. At step h in the current episode, when observing state x, the agent selects action a based on a pseudo-Q-value that is a combination of the three "Q" values:

a  arg max
a

Qh(x,

a)

+

Z 

Ch(x,

a)

,

pseudo-Q-value of state (x, a) at step h

where  is a constant. Triple-Q uses UCB-exploration when learning the Q-values, where the UCB bonus

and the learning rate at each update both depend on the visit count to the corresponding (state, action)

pair as in [13]). Different from the optimistic Q-learning for unconstrained MDPs (e.g. [13, 27, 29]), the

learning rates in Triple-Q need to be periodically reset at the beginning of each frame, where a frame

consists of K consecutive episodes. The value of the virtual-Queue (the dual variable) is updated

once in every frame. So Triple-Q can be viewed as a two-time-scale algorithm where virtual-Queue is

updated at a slow time-scale, and Triple-Q learns the pseudo-Q-value for fixed Z at a fast time scale

within each frame. Furthermore, it is critical to update the two Q-functions (Qh(x, a) and Ch(x, a))

following a rule similar to SARSA [22] instead of Q-learning [28], in other words, using the Q-functions

of the action that is taken instead of using the max function.

We prove Triple-Q achieves O~

1 

H

4

S

1 2

A

1 2

K

4 5

reward regret and guarantees zero constraint viola-

tion when the total number of episodes K 

 16 SAH63


5
, where  is logarithmic in K. Therefore, in

terms of constraint violation, our bound is sharp for large K. To the best of our knowledge, this is the

first model-free, simulator-free RL algorithm with model-based approaches, it has been shown that a

sublinear regret and zero constraint model-based algorithm achieves both

vO~io(latHio4nS. AFKor)

regret and constraint violation (see, e.g. [10]). It remains open whether a model-free RL algorithm can

achieve the same regret bound order-wise.

2

We remark that a key difference between our analysis and the analysis of the optimistic Q-learning for unconstrained MDPs [13, 27, 29, 26, 14] is that our proof relies heavily on the Lyapunov-drift analysis of virtual-Queue Z. The drift analysis on Lyapunov function Z2 relates the difference between the optimal reward Q-function and the learned reward Q-function to the difference between the optimal pseudo-Qfunction and the learned pseudo-Q-function. For fixed Z, Triple-Q can be regarded as optimistic SARSA for the pseudo-Q-function, so the relationship enables us to establish the regret bound by analyzing the pseudo-Q-function. Furthermore, the Lyapunov-drift analysis on the moment generating function of Z, i.e. E[erZ] yields an upper bound on Z that holds uniformly over the entire learning horizon. This upper bound, together with a fundamental relationship between Z and constraint violation, leads to the constraint violation bound.
As many other model-free RL algorithms, a major advantage of Triple-Q is its low computational complexity. The computational complexity of Triple-Q is similar to SARSA for unconstrained MDPs, so it retains both its effectiveness and efficiency while solving a much harder problem. While we consider a tabular setting in this paper, Triple-Q can easily incorporate function approximations (linear function approximations or neural networks) by replacing the Q(x, a) and C(x, a) with their function approximation versions, making the algorithm a very appealing approach for solving complex CMDPs in practice.
Notation: f (n) = O~(g(n)) denotes f (n) = O(g(n)logkn) with k > 0. The same applies to ~ . R+ denotes non-negative real numbers. [H] denotes the set {1, 2, · · · , H}.

2 Problem Formulation

We consider an episodic CMDP, denoted by (S, A, H, P, r, g), where S is the state space with |S| = S,
A is the action space with |A| = A, H is the number of steps in each episode, and P = {Ph}Hh=1 is a collection of transition kernels (transition probability matrices). At the beginning of each episode, an

initial state x1 is sampled from distribution µ0. Then at step h, the agent takes action ah after observing state xh. Then the agent receives a reward rh(xh, ah) and incurs a utility gh(xh, ah). The environment then moves to a new state xh+1 sampled from distribution Ph(·|xh, ah). Similar to [13], we assume that rh(x, a)(gh(x, a)) : S × A  [0, 1], are deterministic for convenience.
Given a policy , which is a collection of H functions {h : S  A}Hh=1, the reward value function Vh at step h is the expected cumulative rewards from step h to the end of the episode under policy  :

H

Vh(x) = E

ri(xi, i(xi)) xh = x .

i=h

The (reward) Q-function Qh(x, a) at step h is the expected cumulative rewards when agent starts from a state-action pair (x, a) at step h and then follows policy  :

Qh(x, a) = rh(x, a) + E

H
ri(xi, i(xi))
i=h+1

xh = x ah = a

.

Similarly, we use Wh(x) : S  R+ and Ch(x, a) : S × A  R+ to denote the utility value function and utility Q-function at step h:

H

Wh(x) = E

gi(xi, i(xi)) xh = x ,

i=h

Ch(x, a) = gh(x, a) + E

H
gi(xi, i(xi))
i=h+1

xh = x ah = a

.

3

For simplicity, we adopt the following notation (some used in [13, 8]):

PhVh+1(x, a) = ExPh(·|x,a)Vh+1(x), PhWh+1(x, a) = ExPh(·|x,a)Wh+1(x),

Qh(x, h(x)) = Qh(x, a)P(h(x) = a)
a
Ch(x, h(x)) = Ch(x, a)P(h(x) = a).
a

From the definitions above, we have

Vh(x) = Qh(x, h(x)), Qh(x, a) = rh(x, a) + PhVh+1(x, a), and

Wh(x) = Ch(x, h(x)), Ch(x, a) = gh(x, a) + PhWh+1(x, a).
Given the model defined above, the objective of the agent is to find a policy that maximizes the expected cumulative reward subject to a constraint on the expected utility:

maximize


E [V1(x1)]

subject

to:E [W1(x1)]  ,

(1)

where we assume   [0, H] to avoid triviality and the expectation is taken with respect to the initial distribution x1  µ0.

Remark 1. The results in the paper can be directly applied to a constraint in the form of

E [W1(x1)]  .

(2)

Without loss of generality, assume   H. We define g~h(x, a) = 1 - gh(x, a)  [0, 1] and ~ = H -   0, the the constraint in (2) can be written as

E W~ 1(x1)  ~,

(3)

where

H

E W~ 1(x1) = E

g~i(xi, i(xi)) = H - E [W1(x1)] .

i=1

Let  denote the optimal solution to the CMDP problem defined in (1). We evaluate our model-free RL algorithm using regret and constraint violation defined below:

K

Regert(K) = E

(V1(xk,1) - V1k (xk,1)) ,

(4)

k=1

K

Violation(K) = E

( - W1k (xk,1)) ,

(5)

k=1

where V1(x) = V1(x), k is the policy used in episode k and the expectation is taken with respect to the distribution of the initial state xk,1  µ0.
In this paper, we assume the following standard Slater's condition hold.

4

Assumption 1. (Slater's Condition). Given initial distribution µ0, there exist  > 0 and policy  such that
E [W1(x1)] -   .
In this paper, Slater's condition simply means there exists a feasible policy that can satisfy the constraint with a slackness . This has been commonly used in the literature [8, 9, 10, 19]. We call  Slater's constant. While the regret and constraint violation bounds depend on , our algorithm does not need to know  under the assumption that K is large (the exact condition can be found in Theorem 1). This is a noticeable difference from some of works in CMDPs in which the agent needs to know the value of this constant (e.g. [8]) or alternatively a feasible policy (e.g. [2]) .

3 Triple-Q

In this section, we introduce Triple-Q for CMDPs. The design of our algorithm is based on the primal-

dual approach in optimization. While RL algorithms based on the primal-dual approach have been

developed for CMDPs [8, 9, 21, 10], a model-free RL algorithm with sublinear regrets and zero constraint

violation is new.

The design of Triple-Q is based on the primal-dual approach in optimization. Given Lagrange

multiplier , we consider the Lagrangian of problem (1) from a given initial state x1 :

max


V1

(x1)

+



(W1(x1

)

-

)

(6)

H

= max E


rh(xh, h(xh)) + gh(xh, h(xh)) - ,

h=1

which is an unconstrained MDP with reward rh(xh, h(xh)) + gh(xh, h(xh)) at step h. Assuming we solve the unconstrained MDP and obtain the optimal policy, denoted by , we can then update the dual variable (the Lagrange multiplier) using a gradient method:

   +  - E W1 (x1)

+
.

While primal-dual is a standard approach, analyzing the finite-time performance such as regret or sample complexity is particularly challenging. For example, over a finite learning horizon, we will not be able to exactly solve the unconstrained MDP for given . Therefore, we need to carefully design how often the Lagrange multiplier should be updated. If we update it too often, then the algorithm may not have sufficient time to solve the unconstrained MDP, which leads to divergence; and on the other hand, if we update it too slowly, then the solution will converge slowly to the optimal solution and will lead to large regret and constraint violation. Another challenge is that when  is given, the primal-dual algorithm solves a problem with an objective different from the original objective and does not consider any constraint violation. Therefore, even when the asymptotic convergence may be established, establishing the finite-time regret is still difficult because we need to evaluate the difference between the policy used at each step and the optimal policy.
Next we will show that a low-complexity primal-dual algorithm can converge and have sublinear regret and zero constraint violation when carefully designed. In particular, Triple-Q includes the following key ideas:

· A sub-gradient algorithm for estimating the Lagrange multiplier, which is updated at the beginning

of each frame as follows:

Z

Z

+



+



-

C¯ K

+
,

(7)

5

where (x)+ = max{x, 0} and C¯ is the summation of all C1(x1, a1)s of the episodes in the previous

frame. We call Z a virtual queue because it is terminology that has been widely used in stochastic

networks (see e.g. [17, 24]). If we view  +  as the number of jobs that arrive at a queue within each

frame and C¯ as the number of jobs that leave the queue within each frame, then Z is the number of

jobs that are waiting at the queue. Note that we added extra utility  to . By choosing  = 8

SAH K 0.2

63

,

the virtual queue pessimistically estimates constraint violation so Triple-Q achieves zero constraint

violation when the number of episodes is large.

·

A

carefully

chosen

parameter



= K0.2

so

that

when

Z 

is

used

as

the

estimated

Lagrange

multiplier,

it balances the trade-off between maximizing the cumulative reward and satisfying the constraint.

· Carefully chosen learning rate t and Upper Confidence Bound (UCB) bonus bt to guarantee that

the estimated Q-value does not significantly deviate from the actual Q-value. We remark that the

learning rate and UCB bonus proposed for unconstrained MDPs [13] do not work here. Our learning

rate

is

chosen

to

be

K 0.2 +1 K 0.2 +t

,

where

t

is

the

number

of

visits

to

a

given

(state,

action)

pair

in

a

particular

step.

This

decays

much

slower

than

the

classic

learning

rate

1 t

or

H +1 H +t

used

in

[13].

The

learning

rate

is further reset from frame to frame, so Triple-Q can continue to learn the pseudo-Q-values that vary

from frame to frame due to the change of the virtual-Queue (the Lagrange multiplier).

We now formally introduce Triple-Q. A detailed description is presented in Algorithm 1. The

algorithm only needs to know the values of H, A, S and K, and no other problem-specific values are

needed. Furthermore, Triple-Q includes updates of two Q-functions per step: one for Qh and one for Ch; and one simple virtual queue update per frame. So its computational complexity is similar to SARSA.

The next theorem summarizes the regret and constraint violation bounds guaranteed under Triple-Q.

Theorem 1. Assume K 

 16 SAH63


5



, where  = 128 log( 2SAHK). Triple-Q achieves the follow-

ing regret and constraint violation bounds:

Regret(K ) Violation(K )

 

13 

H

4

 S

A3

K

54H 4 K 0.6 

log

01.68 H+24KH14.+2



4

H 

2

K

0.8

-

 5 SAH63K0.8.

If

we

further

have

K



e

1 

,

then

Violation(K)  0.

4 Proof of the Main Theorem
We now present the complete proof of the main theorem.
4.1 Notation
In the proof, we explicitly include the episode index in our notation. In particular, · xk,h and ak,h are the state and the action taken at step h of episode k. · Qk,h, Ck,h, Zk, and C¯k are the reward Q-function, the utility Q-function, the virtual-Queue, and the value of C¯ at the beginning of episode k. · Nk,h, Vk,h and Wk,h are the visit count, reward value-function, and utility value-function after they are updated at step h of episode k (i.e. after line 9 of Triple-Q).
6

Algorithm 1: Triple-Q

1 Choose  = K0.2,  = K0.2,  = 128 log

 2S AH K



,  = 0.6 and  = 8

S AH 6 3 K 0.2

;

2 Initialize Qh(x, a) = Ch(x, a)  H and Z = C¯ = Nh(x, a) = VH+1(x) = WH+1(x)  0 for all

(x, a, h)  S × A × [H];

3 for episode k = 1, . . . , K do

4 Sample the initial state for episode k : x1  µ0; 5 for step h = 1, . . . , H + 1 do

6

if h  H ;

// take a greedy action based on the pseudo-Q-function

7

then

8

Take action ah  arg maxa

Qh(xh,

a)

+

Z 

Ch(xh,

a)

;

9

Observe rh(xh, ah), gh(xh, ah), and xh+1 ;

10

Nh(xh, ah)  Nh(xh, ah) + 1, Vh(xh)  Qh(xh, ah), Wh(xh)  Ch(xh, ah);

11

if h  2 ;

// update the Q-values for (xh-1, ah-1) after observing (sh, ah)

12

then

13

Set

t

=

Nh-1(xh-1, ah-1), bt

=

1 4

H

2 (+1) +t

,

t

=

+1 +t

;

14

Update the reward Q-value:

Qh-1(xh-1, ah-1)  (1 - t)Qh-1(xh-1, ah-1) + t (rh-1(xh-1, ah-1) + Vh(xh) + bt);

15

Update the utility Q-value:

Ch-1(xh-1, ah-1)  (1 - t)Ch-1(xh-1, ah-1) + t (gh-1(xh-1, ah-1) + Wh(xh) + bt);

16

if h = 1 then

17

C¯  C¯ + C1(x1, a1) ;

// Add C1(x1, a1) to C¯

18 if k mod (K) = 0 ;

// Reset the visit counts, add extra bonuses, and update the

virtual-queue at the beginning of each frame

19 20

then Nh(x,

a)



0,

Qh(x,

a)



Qh(x,

a)

+

2H 3 

 

,

(x,

a,

h);

21

if Qh(x, a)  H or Ch(x, a)  H then

22

Qh(x, a)  H and Ch(x, a)  H;

23

Z

Z

+



+



-

C¯ K

+ , and C¯  0 ;

// update the virtual-queue length

We also use shorthand notation

{f - g}(x) = f (x) - g(x),

when f (·) and g(·) take the same argument value. Similarly

{(f - g)q}(x) = (f (x) - g(x))q(x).

In this shorthand notation, we put functions inside { }, and the common argument(s) outside. A summary of notations used throughout this paper can be found in Table 1 in the appendix.

7

4.2 Regret
To bound the regret, we consider the following offline optimization problem as our regret baseline [3, 20]:

max
qh

h,x,a

qh

(x,

a)rh

(x,

a)

(8)

s.t.: qh(x, a)gh(x, a)  

(9)

h,x,a

qh(x, a) = Ph-1(x|x, a)qh-1(x, a)

a

x,a

(10)

qh(x, a) = 1, h  [H]

(11)

x,a

q1(x, a) = µ0(x)

(12)

a

qh(x, a)  0, x  S, a  A, h  [H].

(13)

Recall that Ph-1(x|x, a) is the probability of transitioning to state x upon taking action a in state x at step h - 1. This optimization problem is linear programming (LP), where qh(x, a) is the probability of (state, action) pair (x, a) occurs in step h, a qh(x, a) is the probability the environment is in state x in step h, and
qh(x, a) a qh(x, a)
is the probability of taking action a in state x at step h, which defines the policy. We can see that
(9) is the utility constraint, (10) is the global-balance equation for the MDP, (11) is the normalization
condition so that qh is a valid probability distribution, and (12) states that the initial state is sampled from µ0. Therefore, the optimal solution to this LP solves the CMDP (if the model is known), so we use the optimal solution to this LP as our baseline.
To analyze the performance of Triple-Q, we need to consider a tightened version of the LP, which is
defined below:

max
qh

h,x,a

qh

(x,

a)rh

(x,

a)

(14)

s.t.: qh(x, a)gh(x, a)   + 
h,x,a

(10) - (13),

where  > 0 is called a tightness constant. When   , this problem has a feasible solution due to Slater's condition. We use superscript  to denote the optimal value/policy related to the original CMDP (1) or the solution to the corresponding LP (8) and superscript , to denote the optimal value/policy
related to the -tightened version of CMDP (defined in (14)).
Following the definition of the regret in (4), we have

K

K

Regret(K) = E

V1(xk,1) - V1k (xk,1) = E

k=1

k=1

{Q1q1} (xk,1, a) - Q1k (xk,1, ak,1) .
a

8

Now by adding and subtracting the corresponding terms, we obtain

Regret(K )

K

=E

Q1q1 - Q1,q1, (xk,1, a) +

k=1 a

K

E

Q1,q1, (xk,1, a) - Qk,1(xk,1, ak,1) +

k=1 a

K

E

{Qk,1 - Q1k } (xk,1, ak,1) .

k=1

(15) (16) (17)

Next, we establish the regret bound by analyzing the three terms above. We first present a brief outline.

4.2.1 Outline of the Regret Analysis

· Step 1: First, by comparing the LP associated with the original CMDP (8) and the tightened LP (14), Lemma 1 will show

E

a

Q1q1 - Q1,q1,

(xk,1, a)



H 

,

which implies that under our choices of , , and ,

(15)



KH 

=

O~

1 

H

4

S

1 2

A

1 2

K

4 5

.

· Step 2: Note that Qk,h is an estimate of Qhk , and the estimation error (17) is controlled by the learning rates and the UCB bonuses. In Lemma 2, we will show that the cumulative estimation error

over one frame is upper bounded by

H2SA

+

H 3 K  

+

H4SAK( + 1).

Therefore, under our choices of , , and , the cumulative estimation error over K episodes satisfies

(17)



H 2 S AK 1-

+

H 3 K 

+

H4SAK2-( + 1) = O~

H

3

S

1 2

A

1 2

K

4 5

.

The proof of Lemma 2 is based on a recursive formula that relates the estimation error at step h to the estimation error at step h + 1, similar to the one used in [13], but with different learning rates and UCB bonuses.

· Step 3: Bounding (16) is the most challenging part of the proof. For unconstrained MDPs, the

oovpetrimesitsitmicatQe -loefarQnhi,n(gx,ina)[)13fo] rgaulalra(xn,taee,sh,tkh)atsimQku,lht(axn,eao)usislyawnitohvearehstigimhapterobofabQilhit(yx.,

a) (so also an However, this

result does not hold under Triple-Q because Triple-Q takes greedy actions with respect to the pseudo-

Q-function instead of the reward Q-function. To overcome this challenge, we first add and subtract

9

additional terms to obtain

K
E
k=1 a
=E
ka
+E
k

Q1,q1, (xk,1, a) - Qk,1(xk,1, ak,1)

Q1,q1,

+

Zk 

C1,q1,

(xk,1, a) -

Qk,1q1,

+

Zk 

Ck,1q1,

Qk,1q1, (xk,1, a) - Qk,1(xk,1, ak,1)
a

+E

Zk ka

(xk,1, a) Ck,1 - C1, q1,

(18)
(xk,1, a) . (19)

We can see (18) is the difference of two pseudo-Q-functions. Using a three-dimensional induction (on

step, episode, and frame), we will prove in Lemma 3 that

Qk,h

+

Zk 

Ck,h

(x, a) is an overestimate of

Qh,

+

Zk 

Ch,

(x, a) (i.e. (18)  0) for all (x, a, h, k) simultaneously with a high probability. Since

Zk changes from frame to frame, Triple-Q adds the extra bonus in line 21 so that the induction can

be carried out over frames.

Finally,

to

bound (19),

we

use

the

Lyapunov-drift

method

and

consider

Lyapunov

function LT

=

1 2

ZT2

,

where T is the frame index and ZT is the value of the virtual queue at the beginning of the T th frame.

We will show in Lemma 4 that the Lyapunov-drift satisfies

E[LT +1

-

LT ]



a

negative

drift

+

H4

+

2

-

 K

k

,

(20)

where

k = E

Qk,1q1, (xk,1, a) - Qk,1(xk,1, ak,1)
a

+E

Zk 

a

Ck,1 - C1, q1, (xk,1, a) ,

and we note that (19) = k k. Inequality (20) will be established by showing that Triple-Q takes actions to almost greedily reduce virtual-Queue Z when Z is large, which results in the negative drift
in (20). From (20), we observe that

E[LT +1

-

LT ]



H4

+

2

-

 K

k .

(21)

So we can bound (19) by applying the telescoping sum over the K1- frames on the inequality above:

(19) =

k



KE [L1

- LK1-+1] 

+

K(H4 + 

2)



K

(H

4 

+

2)

,

k

where the last inequality holds because L1 = 0 and LT  0 for all T. Combining the bounds on (18) and (19), we conclude that under our choices of ,  and ,

(16)

=

O~ (H 4 S

1 2

A

1 2

K

4 5

).

Combining the results in the three steps above, we obtain the regret bound in Theorem 1.

10

4.2.2 Detailed Proof
We next present the detailed proof. The first lemma bounds the difference between the original CMDP and its -tightened version. The result is intuitive because the -tightened version is a perturbation of the original problem and   .
Lemma 1. Given   , we have

E

a

Q1q1 - Q1,q1,

(xk,1, a)



H 

.

Proof. Given qh(x, a) is the optimal solution, we have

qh(x, a)gh(x, a)  .
h,x,a

Under Assumption 1, we know that there exists a feasible solution {qh1(x, a)}Hh=1 such that
qh1 (x, a)gh(x, a)   + .
h,x,a

We

construct

qh2 (x,

a)

=

(1

-

 

)qh

(x,

a)

+

 

qh1

(x,

a),

which

satisfies

that

qh2(x, a)gh(x, a) =

(1

-

 

)qh (x,

a)

+

 

qh1

(x,

a)

gh(x, a)   + ,

h,x,a

h,x,a

qh2(x, a) =

ph-1(x|x, a)qh2-1(x, a),

h,x,a

x,a

qh2(x, a) = 1.
h,x,a

Also we have optimization

qh2 (x, a) problem

 0 for all (14). Then

(h, x, a). Thus given {qh,(x,

{qh2(x, a)}Hh=1 is a feasible solution to the -tightened a)}Hh=1 is the optimal solution to the -tightened opti-

mization problem, we have

qh(x, a) - qh,(x, a) rh(x, a)
h,x,a



qh(x, a) - qh2(x, a) rh(x, a)

h,x,a



qh(x, a) -

1

-

 

h,x,a



qh(x, a) -

1

-

 

h,x,a



 

qh(x, a)rh(x, a)

h,x,a

qh(x,

a)

-

 

qh1 (x,

a)

qh(x, a) rh(x, a)



H 



,

rh(x, a)

11

where the last inequality holds because 0  rh(x, a)  1 under our assumption. Therefore the result follows because

Q1(xk,1, a)q1(xk,1, a) =

qh(x, a)rh(x, a)

a

h,x,a

Q1,(xk,1, a)q1,(xk,1, a) =

qh,(x, a)rh(x, a).

a

h,x,a

The next lemma bounds the difference between the estimated Q-functions and actual Q-functions in a frame. The bound on (17) is an immediate result of this lemma.

Lemma 2. Under Triple-Q, we have for any T  [K1-],


T K
E

{Qk,1

-

 Q1k } (xk,1, ak,1)



H2SA

+

H

3

 K





+

k=(T -1)K+1


T K
E

{Ck,1

-

 C1k } (xk,1, ak,1)



H2SA

+

H 3 K  

+

k=(T -1)K+1

H2SAK( + 1), H2SAK( + 1).

Proof. We will prove the result on the reward Q-function. The proof for the utility Q-function is almost identical. We first establish a recursive equation between a Q-function with the value-functions in the earlier episodes in the same frame. Recall that under Triple-Q, Qk+1,h(x, a), where k is an episode in frame T, is updated as follows:

Qk+1,h(x, a) =

(1 - t)Qk,h(x, a) + t (rh(x, a) + Vk,h+1(xk,h+1) + bt) Qk,h(x, a)

if (x, a) = (xk,h, ak,h) , otherwise

where t = Nk,h(x, a). Define kt to be the index of the episode in which the agent visits (x, a) in step h for the tth time in the current frame.
The update equation above can be written as:

Qk,h(x, a) =(1 - t)Qkt,h(x, a) + t (rh(x, a) + Vkt,h+1(xkt,h+1) + bt) . Repeatedly using the equation above, we obtain

Qk,h(x, a) =(1 - t)(1 - t-1)Qkt-1,h(x, a) + (1 - t)t-1 rh(x, a) + Vkt-1,h+1(xkt-1,h+1) + bt-1 + t (rh(x, a) + Vkt,h+1(xkt,h+1) + bt)
=···

where 0t =

t
=0t Q(T -1)K+1,h(x, a) + it (rh(x, a) + Vki,h+1(xki,h+1) + bi)
i=1
t
0t H + it (rh(x, a) + Vki,h+1(xki,h+1) + bi) ,
i=1

(22)

tj=1(1 - j) and it = i tj=i+1(1 - j). From the inequality above, we further obtain

T K

T K

T K

Nk,h (x,a)

Qk,h(x, a) 

0t H +

iNk,h (rh(x, a) + Vki,h+1(xki,h+1) + bi) .

k=(T -1)K+1

k=(T -1)K+1

k=(T -1)K+1 i=1

(23)

12

The notation becomes rather cumbersome because for each (xk,h, ak,h), we need to consider a corresponding sequence of episode indices in which the agent sees (xk,h, ak,h). Next we will analyze a given sample path (i.e. a specific realization of the episodes in a frame), so we simplify our notation in this
proof and use the following notation:

Nk,h =Nk,h(xk,h, ak,h) ki(k,h) =ki(xk,h, ak,h),

where ki(k,h) is the index of the episode in which the agent visits state-action pair (xk,h, ak,h) for the ith

time. Since in a given sample path, (k, h) can uniquely determine (xk,h, ak,h), this notation introduces

no ambiguity. Furthermore, we will replace

T K k=(T -1)K+1

with

k because we only consider episodes

in frame T in this proof.

We note that

Nk,h

k

i=1 iNk,h Vki(k,h),h+1

xki(k,h),h+1



 Vk,h+1(xk,h+1)

Nt k,h 

k

t=Nk,h

1

+

1 

Vk,h+1(xk,h+1),
k
(24)

where the first inequality holds because because Vk,h+1(xk,h+1) appears in the summation on the left-

hand side each time when in episode k > k in the same frame, the environment visits (xk,h, ak,h) again,

i.e. (xk,h, ak,h) = (xk,h, ak,h), and the second inequality holds due to the property of the learning rate

proved in Lemma 7-(d). By substituting (24) into (23) and noting that

Nk,h(x,a) i=1

iNk,h

=

1

according

to Lemma Lemma 7-(b), we obtain

Qk,h(xk,h, ak,h)

k



0t H +

(rh(xk,h,

ak,h)

+

Vk,h+1(xk,h+1))

+

1 

Vk,h+1(xk,h+1) +

Nk,h
iNk,h bi



k

k
(rh(xk,h, ak,h) + Vk,h+1(xk,h+1)) + HSA +

H2Kk  

+

1 2

k i=1
H2SAK( + 1),

k

where the last inequality holds because (i) we have

0Nk,h H =

H I{Nk,h=0}  H SA,

k

k

(ii) Vk,h+1(xk,h+1)  H2 by using Lemma 8, and (iii) we know that

Nk,h
iNk,h bi

=

1 4

T K

Nk,h
iNk,h

k i=1

k=(T -1)K+1 i=1

H2( + +i

1)



1 2

T K

k=(T -1)K+1

H2( + 1)  + Nk,h

=

1 2

NT K,h(x,a)

x,a

n=1

H2( + 1) +n



1 2

x,a

NT K,h(x,a) n=1

H 2 ( n

+

1)

(1)


H2SAK( + 1),

where the last inequality above holds because the left hand side of (1) is the summation of K terms

and

H 2 (+1) +n

is

a

decreasing

function

of

n.

13

Therefore, it is maximized when NT K,h = K/SA for all x, a, i.e. by picking the largest K terms. Thus we can obtain

Qk,h(xk,h, ak,h) -

Qhk (xk,h, ak,h)

k

k

k
Vk,h+1(xk,h+1) - PhVh+k1(xk,h, ak,h)

+

HSA

+

H

2

 K





+

H2SAK( + 1)



Vk,h+1(xk,h+1) - PhVh+k1(xk,h, ak,h) + Vh+k1(xk,h+1) - Vh+k1(xk,h+1)

k
+ HSA

+

H 2 K  

+

H2SAK( + 1)

=

Vk,h+1(xk,h+1)) - Vh+k1(xk,h+1) - PhVh+k1(xk,h, ak,h) + P^khVh+k1(xk,h, ak,h)

k
+ HSA

+

H 2 K  

+

H2SAK( + 1)

=

Qk,h+1(xk,h+1, ak,h+1) - Qhk+1(xk,h+1, ak,h+1) - PhVh+k1(xk,h, ak,h) + P^khVh+k1(xk,h, ak,h

k
+ HSA

+

H 2 K  

+

H2SAK( + 1).

Taking the expectation on both sides yields

E

Qk,h(xk,h, ak,h) -

Qhk (xk,h, ak,h)

k

k

E

Qk,h+1(xk,h+1, ak,h+1)) - Qhk+1(xk,h+1, ak,h+1)

+

HSA

+

H 2 K  

+

H2SAK( + 1).

k

Then by using the inequality repeatably, we obtain for any h  [H],

E

Qk,h(xk,h, ak,h) -

Qhk (xk,h, ak,h)

H 2 S A

+

H 3 K  

+

k

k

H4SAK( + 1),

so the lemma holds.

From the lemma above, we can immediately conclude:

E

K
{Qk,1 - Q1k } (xk,1, ak,1)



H 2 S AK 1-

+

H 3 K 

+

k=1

E

K
{Ck,1 - C1k } (xk,1, ak,1)



H 2 S AK 1-

+

H 3 K 

+

k=1

H4SAK2-( + 1) H4SAK2-( + 1).

14

We now focus on (16), and further expand it as follows:

(16)
K
=E
k=1

Q1,q1, (xk,1, a) - Qk,1(xk,1, ak,1)
a

=E
ka

Fk,,1 - Fk,1 q1, (xk,1, a)

+E
k

Qk,1q1, (xk,1, a) - Qk,1(xk,1, ak,1)
a

+E

Zk ka

(25)
Ck,1 - C1, q1, (xk,1, a) , (26)

where

Fk,h(x, a)

=

Qk,h(x,

a)

+

Zk 

Ck,h(x, a)

Fh,(x, a)

=

Qh,(x, a)

+

Zk 

Ch,(x,

a).

We first show (25) can be bounded using the following lemma. This result holds because the choices of the UCB bonuses and the additional bonuses added at the beginning of each frame guarantee that Fk,h(x, a) is an over-estimate of Fh,(x, a) for all k, h and (x, a) with a high probability.

Lemma

3.

With

probability

at

least

1-

1 K3

,

the

following

inequality

holds

simultaneously

for

all

(x, a, h, k)  S × A × [H] × [K] :

{Fk,h - Fh} (x, a)  0,

(27)

which further implies that

K
E
k=1 a

Fk,,1 - Fk,1

q1,

(xk,1, a)



4H 4  K

.

(28)

Proof. Consider frame T and episodes in frame T. Define Z = Z(T -1)K+1 because the value of the virtual queue does not change during each frame. We further define/recall the following notations:

Fk,h(x,

a)

=

Qk,h(x, a)

+

Z 

Ck,h(x,

a),

Fh (x,

a)

=

Qh (x,

a)

+

Z 

Ch (x,

a),

Uk,h(x)

=

Vk,h(x)

+

Z 

Wk,h(x),

Uh (x)

=

Vh (x)

+

Z 

Wh(x).

15

According to Lemma 9 in the appendix, we have

{Fk,h - Fh}(x, a)

=0t F(T -1)K+1,h - Fh (x, a)

t
+ it
i=1

Uki,h+1 - Uh+1 (xki,h+1) + {(P^khi - Ph)Uh+1}(x, a) +

1

+

Z 

bi

t

(a)0t F(T -1)K+1,h - Fh (x, a) +

it Uki,h+1 - Uh+1 (xki,h+1)

i=1

t

=(b)0t F(T -1)K+1,h - Fh (x, a) +

it

max
a

Fki,h+1

(xki

,h+1,

a)

-

Fh+1

(xki

,h+1,



(xki

,h+1))

i=1

t

0t F(T -1)K+1,h - Fh (x, a) +

it Fki,h+1 - Fh+1 (xki,h+1, (xki,h+1)),

(29)

i=1

where inequality (a) holds because of the concentration result in Lemma 10 in the appendix and

t

it(1

+

Z 

)bi

=

1 4

t

it(1

+

Z 

)

i=1

i=1

H2( + +t

1)





+Z 4

H2( + 1) +t

by using Lemma 7-(c), and equality (b) holds because Triple-Q selects the action that maximizes Fki,h+1(xki,h+1, a) so Uki,h+1(xki,h+1) = maxa Fki,h+1(xki,h+1, a).
The inequality above suggests that we can prove {Fk,h - Fh}(x, a) for any (x, a) if (i)
F(T -1)K+1,h - Fh (x, a)  0,

i.e. the result holds at the beginning of the frame and (ii)

Fk,h+1 - Fh+1 (x, a)  0 for any k < k
and (x, a), i.e. the result holds for step h + 1 in all the previous episodes in the same frame. We now prove the lemma using induction. We first consider T = 1 and h = H i.e. the last step in
the first frame. In this case, inequality (29) becomes

{Fk,H - FH }(x, a)  0t

H

+

Z1 

H

-

Fh

(x, a)  0.

(30)

Based on induction, we can first conclude that

{Fk,h - Fh}(x, a)  0
for all h and k  K + 1, where {FK+1,h}h=1,··· ,H are the values before line 20, i.e. before adding the extra bonuses and thresholding Q-values at the end of a frame. Now suppose that (27) holds for any episode k in frame T, any step h, and any (x, a). Now consider

{FT K+1,h

-

Fh }

(x,

a)

=

QT K+1,h(x,

a)

+

ZT K+1 

CT K+1,h(x,

a)

-

Qh(x,

a)

-

ZT K+1 

Ch (x,

a).

(31)

16

Note that if Q+T K+1,h(x, a) Q+T K+1,h(x, a) = Q-T K+1,h

= CT+K (x, a) +

+2H1,h3 (x ,


a) <

= H

H, then (31)  0. and CT+K+1,h(x,

Otherwise, from line a) = CT-K+1,h(x, a)

21-24, we have < H. Here, we

use superscript - and + to indicate the Q-values before and after 21-24 of Triple-Q. Therefore, at the

beginning of frame T + 1, we have

{FT

K  +1,h

-

Fh }

(x,

a)(=+a)Q22HH-T K33+1+-,h(2Zx|,TZaKT)K++1+Z-1C-ZT-KZC|T-H+K1,h+(1x,h,

a) - Qh (x, a) -

(x, ZT

a)

-

Z 

Ch(x,

a)

K  +1


-

Z

Ch(x,

a)

(b)0,

(32)

where inequality (a) holds due to the induction assumption and the fact CT-K+1,h(x, a) < H, and (b) holds because according to Lemma 8,

|ZT K+1 - ZT K|  max  + ,

T K k=(T

-1)K

+1

Ck,1(xk,1,

ak,1)

K

 H2.

Therefore, by substituting inequality (32) into inequality (29), we obtain for any T K + 1  k  (T + 1)K + 1,

t

{Fk,h - Fh}(x, a) 

it Fki,h+1 - Fh+1 (xki,h+1, (xki,h+1)) .

i=1

(33)

Considering h = H, the inequality becomes

{Fk,H - FH }(x, a)  0.

(34)

By applying induction on h, we conclude that

{Fk,h - Fh}(x, a)  0.

(35)

holds for any T K + 1  k  (T + 1)K + 1, h, and (x, a), which completes the proof of (27). Let E denote the event that (27) holds for all k, h and (x, a). Then based on Lemma 8, we conclude
that

K
E
k=1 a

Fk,,1 - Fk,1 q1, (xk,1, a)

K
=E
k=1 a

Fk,,1 - Fk,1 q1, (xk,1, a) E Pr(E )

K

+E

Fk,,1 - Fk,1 q1, (xk,1, a) E c

2K

k=1 a

1

+

K 1- H 

2

 

H

2

1 K3



4H 4 K



.

Pr(E c )

(36)

17

Next we bound (26) using the Lyapunov drift analysis on virtual queue Z. Since the virtual queue is updated every frame, we abuse the notation and define ZT to be the virtual queue used in frame T. In particular, ZT = Z(T -1)K+1. We further define

C¯T =

T K

Ck,1(xk,1, ak,1).

k=(T -1)K+1

Therefore, under Triple-Q, we have

ZT +1 =

ZT

+



+



-

C¯T K

+

Define the Lyapunov function to be

LT

=

1 2

ZT2

.

The next lemma bounds the expected Lyapunov drift conditioned on ZT .

Lemma 4. Assume   . The expected Lyapunov drift satisfies

E [LT +1 - LT |ZT = z]



1 K

T K

k=(T -1)K+1

-E

a

Qk,1q1, (xk,1, a) - Qk,1(xk,1, ak,1) ZT = z

+zE
a

C1, - Ck,1 q1, (xk,1, a) ZT = z

+ H4 + 2.

(37)

Proof. Based on the definition of LT , the Lyapunov drift is

LT +1 - LT ZT



+



-

C¯T K

+

C¯T K

+-



2

2

ZT



+



-

C¯T K

+ H4 + 2



ZT K

(T +1)K

( +  - Ck,1(xk,1, ak,1)) + H4 + 2

k=T K+1

where the first inequality is a result of the upper bound on |Ck,1(xk,1, ak,1)| in Lemma 8. Let {qh }Hh=1 be a feasible solution to the tightened LP (14). Then the expected Lyapunov drift
conditioned on ZT = z is

E [LT +1 - LT |ZT = z]



1 K

T K

(E [ z ( +  - Ck,1(xk,1, ak,1)) - Qk,1(xk,1, ak,1)| ZT = z] + E [ Qk,1(xk,1, ak,1)| ZT = z])

k=(T -1)K+1

+ H4 + 2.

(38)

18

Now we focus on the term inside the summation and obtain that

(E [ z ( +  - Ck,1(xk,1, ak,1)) - Qk,1(xk,1, ak,1)| ZT = z] + E [ Qk,1(xk,1, ak,1)| ZT = z])

(a)z( + ) - E 

a

z 

Ck,1q1

+

Qk,1q1

(xk,1, a)

ZT = z + E [ Qk,1(xk,1, ak,1)| ZT = z]

=E z  +  - Ck,1(xk,1, a)q1 (xk,1, a) ZT = z
a

- E  Qk,1(xk,1, a)q1 (xk,1, a) - Qk,1(xk,1, ak,1) ZT = z
a

=E z  +  - C1(xk,1, a)q1 (xk,1, a) ZT = z
a

- E  Qk,1(xk,1, a)q1 (xk,1, a) - Qk,1(xk,1, ak,1) ZT = z + E z {(C1 - Ck,1)q1 } (xk,1, a) ZT = z

a

a

 - E

Qk,1(xk,1, a)q1 (xk,1, a) - Qk,1(xk,1, ak,1) ZT = z + E z {(C1 - Ck,1)q1} (xk,1, a) ZT = z ,

a

a

where

inequality

(a)

holds

because

ak,h

is

chosen

to

maximize

Qk,h(xk,h, a) +

ZT 

Ck,h(xk,h,

a)

under

Triple-Q, and the last equality holds due to that {qh (x, a)}Hh=1 is a feasible solution to the optimization

problem (14), so





 +  - C1(xk,1, a)q1 (xk,1, a) =  +  - gh(x, a)qh (x, a)  0.

a

h,x,a

Therefore, we can conclude the lemma by substituting qh (x, a) with the optimal solution qh,(x, a).
After taking expectation with respect to Z, dividing  on both sides, and then applying the telescoping sum, we obtain

K
E
k=1

Qk,1q1, (xk,1, a) - Qk,1(xk,1, ak,1)
a

+E

K Zk 
k=1

a

KE

[L1

- 

LK 1- +1 ]

+

K

H4 + 2 

K H4 + 2





,

Ck,1 - C1, q1, (xk,1, a) (39)

where the last inequality holds because that L1 = 0 and LT +1 is non-negative. Now combining Lemma 3 and inequality (39), we conclude that

K (16) 

H4 + 2 

+

4H 4  K

.

Further combining inequality above with Lemma 1 and Lemma 2,

Regret(K )



KH 

+

H 2 S AK 1-

+

H 3 K 

+

H4SAK2-( + 1) + K

H4 + 2 

+

4H 4  K

.

(40)

19



By choosing  = 0.6, i.e each frame has K0.6 episodes,  = K0.2,  = K0.2, and  = 8

conclude that when K 

 8 SAH63


5
, which guarantees that  < /2, we have

SAH K 0.2

6

3

,

we

Regret(K )



13 

H

4

 S

A3

K

0.8

+

4H 4  K 1.2

=

O~

1 

H

4

S

1 2

A

1 2

K

0.8

.

(41)

4.3 Constraint Violation

4.3.1 Outline of the Constraint Violation Analysis

Again, we use ZT to denote the value of virtual-Queue in frame T. According to the virtual-Queue update defined in Triple-Q, we have

ZT +1 =

ZT

+



+



-

C¯T K

+



ZT

+



+



-

C¯T K

,

which implies that

T K

T K

(-C1k (xk,1, ak,1) + )  K (ZT +1 - ZT ) +

({Ck,1 - C1k } (xk,1, ak,1) - ) .

k=(T -1)K+1

k=(T -1)K+1

Summing the inequality above over all frames and taking expectation on both sides, we obtain the following upper bound on the constraint violation:

K

K

E

 - C1k (xk,1, ak,1)  -K + KE [ZK1-+1] + E

{Ck,1 - C1k } (xk,1, ak,1) ,

k=1

k=1

(42)

where we used the fact Z1 = 0. In Lemma 2, we already established an upper bound on the estimation error of Ck,1 :

E

K
{Ck,1 - C1k } (xk,1, ak,1)



H 2 S AK 1-

+

H 3 K 

+

k=1

H4SAK2-( + 1).

(43)

Next, we study the moment generating function of ZT , i.e. E erZT for some r > 0. Based on a

Lyapunov drift analysis of this moment generating function and Jensen's inequality, we will establish

the following upper bound on ZT that holds for any 1  T  K1- + 1

E[ZT

]



54H 4  

log

16H 2  



+

16H 2  K2

+

4

H 

2



.

(44)

Under our choices of , , ,  and , it can be easily verified that K dominates the upper bounds in (43) and (44), which leads to the conclusion that the constraint violation because zero when K is sufficiently large in Theorem 1.

4.3.2 Detailed Proof
To complete the proof, we need to establish the following upper bound on E[ZT +1] based on a bound on the moment generating function.

20

Lemma

5.

Assuming



 2

,

we

have

for

any

1T



K 1-

E[ZT

]



54H 4  

log

16H 2  



+

16H 2  K2

+

4

H 

2



.

(45)

The proof will also use the following lemma from [16].

Lemma 6. Let St be the state of a Markov chain, Lt be a Lyapunov function with L0 = l0, and its drift t = Lt+1 - Lt. Given the constant  and v with 0 <   v, suppose that the expected drift E[t|St = s] satisfies the following conditions:

(1) There exists constant  > 0 and t > 0 such that E[t|St = s]  - when Lt  t.

(2) |Lt+1 - Lt|  v holds with probability one.

Then we have

where

r

=

v2

 +v/3

.

E[erLt ]



erl0

+

2er(v+t ) r

,

Proof of Lemma 5. We apply Lemma 6 to a new Lyapunov function:

L¯T = ZT .

To

verify

condition

(1)

in

Lemma

6,

consider

L¯T

=

ZT



T

=

4(

4H 2 K2



 + H

2

+H

4+2

)



and

2



.

The conditional expected drift of

E [ZT +1 - ZT |ZT = z] 
=E ZT2 +1 - z2 ZT = z

 (a)


1 2z -
-

E
 2 

ZT2 +1 - z2 ZT = z

+

4H 2  K2

 +  H2 +

+

4H 2  K2

z +  H2 +

H4 H4

+ +

2 2

2

T

=

-

 4

,

where inequality (a) is obtained according to Lemma 11; and the last inequality holds given z  T . To verify condition (2) in Lemma 6, we have

ZT +1 - ZT  |ZT +1 - ZT | 

 +  - C¯T





 (H2 + H4) +   2 H4,

where the last inequality holds

Now

choose



=

 4

and

v

=

because 2 H4.

2    1. From Lemma

6,

we

obtain

E erZT



erZ1

+

2er(v+T ) r

,

where

r

=

v2

 + v/3

.

By Jensen's inequality, we have

erE[ZT ]  E erZT ,

21

which implies that

E[ZT ]



1 r

log

1

+

2er(v+T ) r

=

1 r

log

1

+

6v2 + 2v 32

er(v+T )



1 r

log

1

+

8v2 32

er(v+T )



1 r

log

11v2 32

er(v+T

)



4v2 3

log

11v2 32

er(v+T

)



3v2 

log

2v 

+ v + T

 3v2 log

2v

+

v

+

4(

4H 2 K2



+

  H2

+

H4

+

2)







=

48H 

4

log

16H

2

 



+

 2 H4

+

4(

4H 2 K2



+

  H2


+

H4

+

2)



54H 

4

log

16H

2

 





+

16H 2  K2

+

4

H2 

= O~

H 

,

which completes the proof of Lemma 5.

Substituting the results from Lemmas 2 and 5 into (42), under assumption K 


 16 SAH63


5
,

which

guarantees





 2

.

Then

by

using

the

facts

that



=

8

SAH K 0.2

63

,

we

can

easily

verify

that

Violation(K )



54H 4 K 0.6 

log

16H 2  

+



4

H 

2

K

0.8

-

 5 SAH63K0.8.

If

further

we

have

K



e

1 

,

we

can

obtain

Violation(K )



54H 4 K 0.6 

log

16H 2  

-

 S AH 6 3 K 0.8

=

0.

which completes the proof of our main result.

5 Conclusions
This paper considered CMDPs and proposed a model-free RL algorithm without a simulator, named Triple-Q. From a theoretical perspective, Triple-Q achieves sublinear regret and zero constraint violation. We believe it is the first model-free RL algorithm for CMDPs with provable sublinear regret, without a simulator. From an algorithmic perspective, Triple-Q has similar computational complexity with SARSA, and can easily incorporate recent deep Q-learning algorithms to obtain a deep Triple-Q algorithm, which makes our method particularly appealing for complex and challenging CMDPs in practice.

22

While we only considered a single constraint in the paper, it is straightforward to extend the algorithm and the analysis to multiple constraints. Assuming there are J constraints in total, Triple-Q can maintain a virtual queue and a utility Q-function for each constraint, and then selects an action at each step by solving the following problem:





max
a

Qh(xh,

a)

+

1 

J

Z(j)Ch(j)(xh, a) .

j=1

References
[1] Naoki Abe, Prem Melville, Cezar Pendus, Chandan K Reddy, David L Jensen, Vince P Thomas, James J Bennett, Gary F Anderson, Brent R Cooley, Melissa Kowalczyk, et al. Optimizing debt collections using constrained reinforcement learning. In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 75­84, 2010.
[2] Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In International Conference on Machine Learning, pages 22­31. PMLR, 2017.
[3] Eitan Altman. Constrained Markov decision processes, volume 7. CRC Press, 1999.
[4] Mohammad Gheshlaghi Azar, R´emi Munos, and Hilbert J. Kappen. On the sample complexity of reinforcement learning with a generative model. In Int. Conf. Machine Learning (ICML), Madison, WI, USA, 2012.
[5] Mohammad Gheshlaghi Azar, R´emi Munos, and Hilbert J. Kappen. Minimax pac bounds on the sample complexity of reinforcement learning with a generative model. Mach. Learn., 91(3):325­349, June 2013.
[6] Kiant´e Brantley, Miroslav Dudik, Thodoris Lykouris, Sobhan Miryoosefi, Max Simchowitz, Aleksandrs Slivkins, and Wen Sun. Constrained episodic reinforcement learning in concave-convex and knapsack settings. arXiv preprint arXiv:2006.05051, 2020.
[7] Yi Chen, Jing Dong, and Zhaoran Wang. A primal-dual approach to constrained Markov decision processes. arXiv preprint arXiv:2101.10895, 2021.
[8] Dongsheng Ding, Xiaohan Wei, Zhuoran Yang, Zhaoran Wang, and Mihailo Jovanovic. Provably efficient safe exploration via primal-dual policy optimization. In Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, 2021.
[9] Dongsheng Ding, Kaiqing Zhang, Tamer Basar, and Mihailo Jovanovic. Natural policy gradient primal-dual method for constrained markov decision processes. Advances in Neural Information Processing Systems, 33, 2020.
[10] Yonathan Efroni, Shie Mannor, and Matteo Pirotta. Exploration-exploitation in constrained MDPs. arXiv preprint arXiv:2003.02189, 2020.
[11] Jaime F Fisac, Anayo K Akametalu, Melanie N Zeilinger, Shahab Kaynama, Jeremy Gillula, and Claire J Tomlin. A general safety framework for learning-based control in uncertain robotic systems. IEEE Transactions on Automatic Control, 64(7):2737­2752, 2018.
23

[12] Javier Garcia and Fernando Ferna´ndez. Safe exploration of state and action spaces in reinforcement learning. Journal of Artificial Intelligence Research, 45:515­564, 2012.
[13] Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is Q-learning provably efficient? In Advances Neural Information Processing Systems (NeurIPS), volume 31, pages 4863­ 4873, 2018.
[14] Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with linear function approximation. In Conference on Learning Theory, pages 2137­2143. PMLR, 2020.
[15] Krishna C. Kalagarla, Rahul Jain, and Pierluigi Nuzzo. A sample-efficient algorithm for episodic finite-horizon MDP with constraints. arXiv preprint arXiv:2009.11348, 2020.
[16] M. J. Neely. Energy-aware wireless scheduling with near-optimal backlog and convergence time tradeoffs. IEEE/ACM Transactions on Networking, 24(4):2223­2236, 2016.
[17] Michael J. Neely. Stochastic network optimization with application to communication and queueing systems. Synthesis Lectures on Communication Networks, 3(1):1­211, 2010.
[18] Masahiro Ono, Marco Pavone, Yoshiaki Kuwata, and J Balaram. Chance-constrained dynamic programming with application to risk-aware robotic space exploration. Autonomous Robots, 39(4):555­ 571, 2015.
[19] Santiago Paternain, Luiz Chamon, Miguel Calvo-Fullana, and Alejandro Ribeiro. Constrained reinforcement learning has zero duality gap. In Advances in Neural Information Processing Systems, 2019.
[20] Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons, 2014.
[21] Shuang Qiu, Xiaohan Wei, Zhuoran Yang, Jieping Ye, and Zhaoran Wang. Upper confidence primaldual reinforcement learning for CMDP with adversarial loss. In Advances in Neural Information Processing Systems, 2020.
[22] Gavin A Rummery and Mahesan Niranjan. On-line Q-learning using connectionist systems, volume 37. University of Cambridge, Department of Engineering Cambridge, UK, 1994.
[23] Rahul Singh, Abhishek Gupta, and Ness B Shroff. Learning in markov decision processes under constraints. arXiv preprint arXiv:2002.12435, 2020.
[24] R. Srikant and Lei Ying. Communication Networks: An Optimization, Control and Stochastic Networks Perspective. Cambridge University Press, 2014.
[25] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
[26] Daniel Vial, Advait Parulekar, Sanjay Shakkottai, and R Srikant. Regret bounds for stochastic shortest path problems with linear function approximation. arXiv preprint arXiv:2105.01593, 2021.
[27] Yuanhao Wang, Kefan Dong, Xiaoyu Chen, and Liwei Wang. Q-learning with UCB exploration is sample efficient for infinite-horizon MDP. In International Conference on Learning Representations, 2020.
24

[28] Christopher John Cornish Hellaby Watkins. Learning from Delayed Rewards. PhD thesis, King's College, Cambridge, UK, May 1989.
[29] Chen-Yu Wei, Mehdi Jafarnia Jahromi, Haipeng Luo, Hiteshi Sharma, and Rahul Jain. Model-free reinforcement learning in infinite-horizon average-reward markov decision processes. In International Conference on Machine Learning, pages 10170­10180. PMLR, 2020.
[30] Tengyu Xu, Yingbin Liang, and Guanghui Lan. A primal approach to constrained policy optimization: Global optimality and finite-time analysis. arXiv preprint arXiv:2011.05869, 2020.
[31] Chao Yu, Jiming Liu, and Shamim Nemati. Reinforcement learning in healthcare: A survey. arXiv preprint arXiv:1908.08796, 2020.
25

In the appendix, we summarize notations used throughout the paper in Table 1, and present a few lemmas used to prove the main theorem.
A Notation Table

Table 1: Notation Table

Notation Definition

K
S
A
H
[H ]
Qk,h(x, a) Qh(x, a) Vk,h(x) Vh (x) Ck,h(x, a) Ch(x, a) Wk,h(x) Wh (x) Fk,h(x, a)
Uk,h(x)
rh(x, a) gh(x, a) Nk,h(x, a)
Zk qqhh, 
bt I(·)

The total number of episodes

The number of states

The number of actions

The length of each episode

Set {1, 2, . . . , H}

The estimated reward Q-function at step h in episode k

The reward Q-function at step h in episode k under policy 

The estimated reward value-function at step h in episode k.

The value-function at step h in episode k under policy 

The estimated utility Q-function at step h in episode k

The utility Q-function at step h in episode k under policy 

The estimated utility value-function at step h in episode k

The utility value-function at step h in episode k under policy 

Fk,h(x,

a)

=

Qk,h(x,

a)

+

Zk 

Ck,h(x,

a)

Uk,h(x)

=

Vk,h(x)

+

Zk 

Wk,h(x)

The reward of (state, action) pair (x, a) at step h.

The utility of (state, action) pair (x, a) at step h.

The number of visits to (x, a) when at step h in episode k (not including k)

The dual estimation (virtual queue) in episode k.

The optimal solution to the LP of the CMDP (8).

The optimal solution to the tightened LP (14).

Slater's constant.

the UCB bonus for given t

The indicator function

B Useful Lemmas

The first lemma establishes some key properties of the learning rates used in Triple-Q. The proof closely follows the proof of Lemma 4.1 in [13].

Lemma

7.

Recall

that

the

learning

rate

used

in

Triple-Q

is

t

=

+1 +t

,

and

t

t

0t = (1 - j ) and it = i

(1 - j).

j=1

j=i+1

(46)

26

The following properties hold for it :

(a) 0t = 0 for t  1, 0t = 1 for t = 0.

(b)

t i=1

it

=

1

for

t



1,

t i=1

it

=

0

for

t

=

0.

(c) 1+t 

t i=1

it +i



2+t .

(d)

 t=i

it

=1+

1 

for

every

i  1.

(e)

ti=1(it)2



+1 +t

for

every

t



1.

Proof. The proof of (a) and (b) are straightforward by using the definition of it. The proof of (d) is the same as that in [13].

(c): We next prove (c) by induction.

For t = 1, we have

t i=1

it +i

=

11 +1

=

1 +1

,

so

(c)

holds

for

t

=

1.

Now suppose that (c) holds for t - 1 for t  2, i.e.

1

t-1


it

 2

.

 + t - 1 i=1  + i - 1

+t-1

From the relationship it = (1 - t)it-1 for i = 1, 2, . . . , t - 1, we have

t i=1

it +

i

=

 t +

t

+

(1

-

t)

t-1 i=1

it-1 +

. i

Now we apply the induction assumption. To prove the lower bound in (c), we have

 t +

t

+

(1

-

t)

t-1

it-1 +

i



 t +

t

+

 1 - t +t-

1



 t +

t

+

1 - t +t



1+

t.

i=1

To prove the upper bound in (c), we have

 t +

t

+

(1

-

t)

t-1

it-1 +

i



 t +

t

+

2(1 - t) +t-1

=

(

+t)+1

+

t

+

(

2(t - + t) 

1) +t

-

1,

i=1

=

(1+-t)-2+t

t

+

(

+

2(t - t) 

1) +

t

-

1

+

2+

t

 ( + t-) -+1 t - 1 + 2+ t  2+ t .

(47)

(e) According to its definition, we have

it

=

+1 i+

·

i

+

i 1

+



i

i+1 +2+



·

·

·

t t

- +

1 

=

+1 t+

·

i

i +



i

i +

+1 1+



·

·

·

t

t -

-1 1+





 

+ +

1 t

.

(48)

27

Therefore, we have

because

t i=1

it

=

1.

t
(it)2
i=1



[max
i[t]

it]

·

t i=1

it



 

+ +

1 t

,

The next lemma establishes upper bounds on Qk,h and Ck,h under Triple-Q.

Lemma 8. For any (x, a, h, k)  S × A × [H] × [K], we have the following bounds on Qk,h(x, a) and

Ck,h(x, a) :

0 0

 

CQkk,h,h((xx,,aa))HH22.

Proof. and x

We first consider the last by its definition and Q0,H

step =H

of 

anepisode, i.e. h = H. Recall H . Suppose Qk,H (x, a)  H

thaftorVak,nHy+k1( x)

= 0 for any k k - 1 and any

(x, a). Then,

Qk,H(x, a) = (1 - t)Qkt,H (x, a) + t (rH (x, a) + bt)  max

 H , 1

+

 H
4

  H ,

where t = Nk,H(x, a) is the number of visits to state-action pair (x, a) when in step H by episode k (but

not include episode k) and kt is the index of the episode of the most recent visit. Therefore, the upper

bound holds for h = H.



Note that Q0,h = H  H(H - h + 1) . Now suppose the upper bound holds for h + 1, and also

holds for k  k - 1. Consider step h in episode k :

Qk,h(x, a) =(1 - t)Qkt,h(x, a) + t (rh(x, a) + Vkt,h+1(xkt,h+1) + bt) ,

where t = Nk,h(x, a) is the number of visits to state-action pair (x, a) when in step h by episode k (but

not include episode k) and kt is the index of the episode of the most recent visit. We also note that Vk,h+1(x)  maxa Qk,h+1(x, a)  H(H - h) . Therefore, we obtain

Qk,h(x, a)  max

H (H

-

h

+

 1) ,

1

+

H (H

-

 h) 

+

H  4

  H(H - h + 1) .

Therefore, we can conclude that Qk,h(x, a)  H2 for any k, h and (x, a). The proof for Ck,h(x, a) is

identical.

Next, we present the following lemma from [13], which establishes a recursive relationship between Qk,h and Qh for any . We include the proof so the paper is self-contained.
Lemma 9. Consider any (x, a, h, k)  S × A × [H] × [K], and any policy . Let t=Nk,h(x, a) be the number of visits to (x, a) when at step h in frame T before episode k, and k1, . . . , kt be the indices of the episodes in which these visits occurred. We have the following two equations:

(Qk,h - Qh)(x, a) =0t Q(T -1)K+1,h - Qh (x, a)

t
+ it

Vki,h+1 - Vh+1 (xki,h+1) + P^khi Vh+1 - PhVh+1 (x, a) + bi ,

i=1

(Ck,h - Ch)(x, a) =0t C(T -1)K+1,h - Ch (x, a)

t
+ it

Wki,h+1 - Wh+1 (xki,h+1) + P^khi Wh+1 - PhWh+1 (x, a) + bi

i=1

(49) , (50)

28

where P^khVh+1(x, a) := Vh+1(xk,h+1) is the empirical counterpart of PhVh+1(x, a) = ExPh(·|x,a)Vh+1(x). This definition can also be applied to Wh as well.
Proof. We will prove (49). The proof for (50) is identical. Recall that under Triple-Q, Qk+1,h(x, a) is updated as follows:

Qk+1,h(x, a) =

(1 - t)Qk,h(x, a) + t (rh(x, a) + Vk,h+1(xh+1,k) + bt) Qk,h(x, a)

if (x, a) = (xk,h, ak,h) . otherwise

From the update equation above, we have in episode k,

Qk,h(x, a) =(1 - t)Qkt,h(x, a) + t (rh(x, a) + Vkt,h+1(xkt,h+1) + bt) . Repeatedly using the equation above, we obtain

Qk,h(x, a) =(1 - t)(1 - t-1)Qkt-1,h(x, a) + (1 - t)t-1 rh(x, a) + Vkt-1,h+1(xkt-1,h+1) + bt-1 + t (rh(x, a) + Vkt,h+1(xkt,h+1) + bt)
=···

t
=0t Q(T -1)K+1,h(x, a) + it (rh(x, a) + Vki,h+1(xki,h+1) + bi) ,
i=1

(51)

where the last equality holds due to the definition of it in (46) and the fact that all Q1,h(x, a)s are

initialized to be H. Now applying the Bellman equation Qh(x, a) = rh + PhVh+1 (x, a) and the fact

that

t i=1

it

=

1,

we

can

further

obtain

Qh(x, a) = 0t Qh(x, a) + (1 - 0t )Qh(x, a)
t
= 0t Qh(x, a) + it r(x, a) + PhVh+1(x, a) + Vh+1(xki,h+1) - Vh+1(xki,h+1)
i=1
t
= 0t Qh(x, a) + it rh(x, a) + PhVh+1(x, a) + Vh+1(xki,h+1) - P^khiVh+1(x, a)
i=1
t
= 0t Qh(x, a) + it rh(x, a) + Vh+1(xki,h+1) + PhVh+1 - P^khiVh+1 (x, a) .
i=1

(52)

Then subtracting (52) from (51) yields

(Qk,h - Qh)(x, a) =0t Q(T -1)K+1,h - Qh (x, a)

t

+

it Vki,h+1 - Vh+1 (xki,h+1) + P^khi Vh+1 - PhVh+1 (x, a) + bi .

i=1

Lemma 10. Consider any frame T. Let t=Nk,h(x, a) be the number of visits to (x, a) at step h before

episode k in the current frame and let k1, . . . , kt < k be the indices of these episodes. Under any policy

,

with

probability

at

least

1

-

1 K3

,

the

following

inequalities

hold

simultaneously

for

all

(x, a, h, k)



29

S × A × [H] × [K]

t
it (P^khi - Ph)Vh+1
i=1

t
it
i=1

(P^khi - Ph)Wh+1

(x, a)



1 4

(x, a)



1 4

H

2( + 1) ( + t)

,

H

2( + 1) ( + t)

.

Proof. Without loss of generality, we consider T = 1. Fix any (x, a, h)  S × A × [H]. For any n  [K],

define

n
X(n) = i · I{kiK} (P^khi - Ph)Vh+1 (x, a).
i=1

Let Fi be the -algebra generated by all the random variables until step h in episode ki. Then

E[X(n + 1)|Fn] = X(n) + E n +1I{kn+1K} (P^khn+1 - Ph)Vh+1 (x, a)|Fn = X(n),

which shows that X(n) is a martingale. We also have for 1  i  n,

|X(i) - X(i - 1)|  i (P^khn+1 - Ph)Vh+1 (x, a)  i H

 Then let  = 8 log 2SAHK

 i=1

(i

H

)2.

By

applying

the

Azuma-Hoeffding

inequality,

we

have

with probability at least 1 - 2 exp - 2

2 i=1(i H)2

=

1

-

1 SAH

K

4

,

|X( )| 





8 log 2SAHK

(i H)2 

i=1

 16

H

2



(i )2



1 4

i=1

H

2( +

+ 

1)

,

where the last inequality holds due to

 i=1

(i

)2



+1 +

from Lemma 7.(e).

Because this inequality

holds for any   [K], it also holds for  = t = Nk,h(x, a)  K, Applying the union bound, we obtain

that

with

probability

at

least

1-

1 K3

the

following

inequality

holds

simultaneously

for

all

(x, a, h, k)



S × A × [H] × [K],:

t

it

(P^khi - Ph)Vh+1

(x, a)



1 4

i=1

H

2( + 1) ( + t)

.

Following

a

similar

analysis

we

also

have

that

with

probability

at

least

1-

1 K3

the

following

inequality

holds simultaneously for all (x, a, h, k)  S × A × [H] × [K],:

t

it

(P^khi - Ph)Wh+1

(x, a)



1 4

i=1

H

2( + 1) ( + t)

.

Lemma 11. Given   2, under Triple-Q, the conditional expected drift is

E [LT +1

- LT |ZT

=

z]



-

 2

ZT

+

4H 2  K2

 +  H2 + H4 + 2

(53)

30

Proof.

Recall

that

LT

=

1 2

ZT2

,

and

the

virtual

queue

is

updated

by

using

ZT +1 =

ZT

+



+



-

C¯T K

+
.

From inequality (38), we have

E [LT +1 - LT |ZT = z]



1 K

T K

E [ZT ( +  - Ck,1(xk,1, ak,1)) - Qk,1(xk,1, ak,1) + Qk,1(xk,1, ak,1)|ZT = z] + H4 + 2

k=(T -1)K+1

(a)

1 K

T K

E ZT

k=(T -1)K+1

 +  - {Ck,1q1} (xk,1, a)
a

-  {Qk,1q1}(xk,1, a) + Qk,1(xk,1, ak,1)|ZT = z
a

+ 2 + H4



1 K

T K

E ZT

k=(T -1)K+1

 +  - {C1q1} (xk,1, a)
a

-  {Qk,1q1}(xk,1, a) + Qk,1(xk,1, ak,1)|ZT = z
a

+

1 K

T K

E

k=(T -1)K+1

ZT

a

{C1q1} (xk,1, a) - ZT

{Ck,1q1} (xk,1, a)|ZT = z
a

+

1 K

T K

E

k=(T -1)K+1



a

{Q1 q1} (xk,1, a) - 

{Q1 q1} (xk,1, a)|ZT = z
a

+ H4 + 2

(b)

-

 2

z

+

1 K

T K

E

k=(T -1)K+1



a

{(F1 - Fk,1)q1} (xk,1, a) + Qk,1(xk,1, ak,1)|ZT = z

(c)

-

 2

z

+

4H 2  K2

+

  H2

+

H4

+

2.

+ H4 + 2

Inequality (a) holds because of our algorithm. Inequality (b) holds because a {Q1 q1} (xk,1, a) is nonnegative, and under Slater's condition, we can find policy  such that

+-E





a

C1(xk,1, a)q1(xk,1, a)

= +-E

qh (x,

a)gh (x,

a)



-

+





-

 2

.

h,x,a

Finally, inequality (c) is obtained similar to (36), and the fact that Qk,1(xk,1, ak,1) is bounded by using Lemma 8

31

