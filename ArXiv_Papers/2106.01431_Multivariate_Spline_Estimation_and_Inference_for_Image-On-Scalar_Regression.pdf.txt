arXiv:2106.01431v1 [stat.ME] 2 Jun 2021

Multivariate Spline Estimation and Inference for Image-On-Scalar Regression
Shan Yua, Guannan Wangb, Li Wangc and Lijian Yangd
aUniversity of Virginia, bCollege of William & Mary, cIowa State University and dTsinghua University
Abstract: Motivated by recent analyses of data in biomedical imaging studies, we consider a class of image-on-scalar regression models for imaging responses and scalar predictors. We propose using flexible multivariate splines over triangulations to handle the irregular domain of the objects of interest on the images, as well as other characteristics of images. The proposed estimators of the coefficient functions are proved to be root-n consistent and asymptotically normal under some regularity conditions. We also provide a consistent and computationally efficient estimator of the covariance function. Asymptotic pointwise confidence intervals and data-driven simultaneous confidence corridors for the coefficient functions are constructed. Our method can simultaneously estimate and make inferences on the coefficient functions, while incorporating spatial heterogeneity and spatial correlation. A highly efficient and scalable estimation algorithm is developed. Monte Carlo simulation studies are conducted to examine the finite-sample performance of the proposed method, which is then applied to the spatially normalized positron emission tomography data of the Alzheimer's Disease Neuroimaging Initiative.
Key words and phrases: Multivariate splines; Coefficient maps; Confidence corridors; Image Analysis; Triangulation.
1. Introduction
Medical and public health studies collect massive amount of imaging data using methods such as functional magnetic resonance imaging (fMRI), positron emission tomography (PET) imaging, computed tomography (CT), and ultrasonic imaging. Much of these data can be characterized as functional data. Compared with traditional one-dimensional (1D) functional data, these imaging data are complex, high-dimensional, and structured, which poses challenges to traditional statistical methods.
We propose a unifying approach to characterize the varying associations between imaging responses and a set of explanatory variables. Three types of statistical methods are widely used to investigate such associations. The first category includes the univariate approaches and pixel-/voxel-based methods (Worsley et al., 2004; Stein et al., 2010; Hibar et al., 2015), which take each pixel/voxel as a basic analytic unit. Because all pixels/voxels are treated as independent, a major drawback of these methods is that they ignore correlation between
Address for correspondence: Li Wang, Department of Statistics and the Statistical Laboratory, Iowa State University, Ames, IA, USA. Email: lilywang@iastate.edu

the pixels/voxels. The second category is the tensor regression. This approach considers an image as a multi-dimensional array (Zhou et al., 2013; Li and Zhang, 2017), which is then changed to a vector to perform the regression. However, doing so naively yields an ultrahigh dimensionality and requires a novel dimension-reduction technique and highly scalable algorithms (Li and Zhang, 2017). The third category is the functional data analysis (FDA) approach, in which an image is viewed as the realization of a function defined on a given domain (Zhu et al., 2012, 2014; Reiss et al., 2017). Using an FDA, we are able to combine information both across and within functions.
We adopt the FDA approach in this study. Functional linear models (FLMs) are widely used to model the regression relationship between a response and some set of predictors from multiple subjects. In the literature (Ramsay and Silverman, 2005; Mu¨ller, 2005; Morris, 2015; Wang et al., 2016), FLMs are often categorized based on whether the outcome, the predictor, or both are functional: (i) functional predictor regression (scalar-on-function) (Cardot et al., 1999, 2003; Hall and Horowitz, 2007); (ii) functional response regression (function-on-scalar) (Morris and Carroll, 2006; Reiss et al., 2010; Staicu et al., 2010; Zhu et al., 2014; Zhang and Wang, 2015; Chen et al., 2017); and (iii) function-on-function regression (Ramsay and Dalzell, 1991; Yao et al., 2005; Sentu¨rk and Mu¨ller, 2010; Wu and Mu¨ller, 2011).
Motivated by the structure of brain imaging data, we propose a novel image-on-scalar regression model with spatially varying coefficients that captures the varying associations between imaging phenotypes and a set of explanatory variables. Figure 1.1 shows a schematic diagram of the proposed modeling approach. Specifically, let  be a two-dimensional bounded domain, and let z = (z1, z2) be the location point on . For the ith subject, i = 1, . . . , n, let Yi(z) be the imaging measurement at location z  , and let Xi , for = 0, 1, . . . , p, with Xi0  1, be scalar predictors, for example, clinic variables (such as age and sex) and genetic factors. The spatially varying coefficient regression characterizes the associations between imaging measures and covariates, and is given by the following model:
Yi(z) = Xi o(z) + i(z) + (z)i(z), i = 1, . . . , n, z  ,
where Xi = (Xi0, Xi1, . . . , Xip) , o = (0o, 1o, . . . , po) is a vector of some unknown bivariate functions, i(z) characterizes the individual image variations, i(z) represents additional measurement errors, and (z) is a positive deterministic function. In the following, we assume that i(z) and i(z) are mutually independent. Moreover, we assume that i(z), for i = 1, . . . , n, are independent and identically distributed (i.i.d.) copies of an L2 stochastic process with mean zero and covariance function G(z, z ) = cov{i(z), i(z )}. Furthermore, i(z), for i = 1, . . . , n, are i.i.d. copies of a stochastic process with zero mean. and covariance function G(z, z ) = cov{i(z), i(z )} = I(z = z ).
2

0.06

0.10

0.05

0.04

=

+1.0

× +(, 0.00 +

×0.02$-./0+

×0.0$5 -./1

0.5

0.00

0.00

-0.05

-0.02

0.050

0.025

0.05

0.005

+

× #$ + 0.000 -0.025

× $& + 0.00

× $'( + ())*) 0.000

-0.050

-0.05

-0.005

Figure 1.1: A schematic diagram of proposed modeling approach.

For a 1D function-on-scalar regression, Chapter 13 of Ramsay and Silverman (2005) provides a common model-fitting strategy, in which the coefficient functions are expanded using some sets of basis functions, and the basis coefficients are estimated using the ordinary least squares withmethod. However, it is not trivial to extend this to an image-on-scalar regression, particularly with biomedical imaging responses. For biomedical images, the objects (e.g., organs) on the images are usually irregularly shaped (e.g., breast tumors). Another example is that of brain images, as shown in Figure 1.1, especially slices from the bottom and the top of the brain. Even though some images seem to be rectangular, the true signal comes only from the domain of an object, and the image contains only noise outside the boundary of the object. Many smoothing methods, such as, tensor product smoothing (Reiss et al., 2017; Chen et al., 2017), kernel smoothing (Zhu et al., 2014), and wavelet smoothing (Morris and Carroll, 2006), provide poor estimations over difficult regions because they smooth inappropriately across boundary features, referred to as the "leakage" problem in the smoothing literature; see Ramsay (2002) and Sangalli et al. (2013). Next, for technical reasons, imaging data often have different visual qualities. The general characteristics of medical images are determined and limited by the technology for each specific modality. As a result, there is a great interest in developing a flexible method with varying smoothness to adaptively smooth biomedical imaging data.
In this study, we tackle the above challenges using bivariate splines on triangulations (Lai and Wang, 2013) to effectively model the spatially nonstationary relationship and preserve the important features (shape, smoothness) of the imaging data. a triangulation can represent any two-dimensional (2D) geometric domain effectively because any polygon can be decomposed into triangles. We study the asymptotic properties of the bivariate spline estimators of the coefficient functions, and show that our spline estimators are root-n consistent and asymptotically normal. The asymptotic results are used as a guideline to construct pointwise confidence intervals (PCIs) and simultaneous confidence corridors (SCCs; also referred to as "simultaneous confidence bands/regions") for the true coefficient functions. Figure 1.2 shows

3

0.05
± ()*0+.0/0-

0.03

0.02 100(1 - &)%

0.01

PCIs

Coefficient Map

0.00
-0.05
Standard Deviation Map

Lower SCC Upper SCC

Bootstrap Adjustment

0.10 0.05 0.00 -0.05 -0.10

0.10 0.05 0.00 -0.05 -0.10

100(1 - &)% SCC

Significance Map

Figure 1.2: A schematic diagram of proposed inferential approach.

the proposed inferential approach. Our method is statistically more efficient than the tensor regression (Li and Zhang, 2017) and the three-stage estimation (Zhu et al., 2014), because it is able to accommodate complex domains of arbitrary shape and adjust the individual smoothing needs of different coefficient functions using multiple smoothing parameters. In addition, our method does not rely on estimating the spatial similarity and adaptive weights repeatedly, as in Zhu et al. (2014); thus, it is much simpler.
The remainder of the paper is structured as follows. Section 2 describes the spline estimators for the coefficient functions, and establishes their asymptotic properties. Section 3 describes the bootstrap method used to construct the SCC and how to estimate the unknown variance functions involved in the SCC. Section 4 presents the implementation of the proposed estimation and inference. Section 5 reports our findings from two simulation studies. In Section 6, we illustrate the proposed method using PET data provided by the Alzheimer's Disease Neuroimaging Initiative (ADNI). Section 7 concludes the paper. All technical proofs of the theoretical results and additional numerical results are deferred to the Appendices A and B.

2. Models and Estimation Method

2.1. Image-on-scalar regression model

Let zj   be the center point of the jth pixel in the domain , and let Yij be the imaging response of subject i at location j. the actual data set consists of {(Yij, Xi, zj), i = 1, . . . , n, j = 1, . . . , N }, which can be modeled as follows:

p
Yij = Xi o(zj) + i(zj) + (zj)ij.

(2.1)

=0

Denote the eigenvalues and eigenfunctions of the covariance operator G(z, z ) as {k} k=1

and {k(z)} k=1, respectively, where 1  2  · · ·  0,

 k=1

k

<

,

and

{k } k=1

forms

an

orthonormal basis of L2 (). It follows from spectral theory that G(z, z ) =

 k=1

k k (z )k (z

).

The ith trajectory {i(z), z  } allows the Karhunen­Lo´eve L2 representation (Li and Hs-

ing, 2010; Sang and Huang, 2012): i(z) =

 k=1

1k/2 ik k

(z),

k1/2ik

=

z i(z)k(z)dz,

where the random coefficients ik are uncorrelated random variables with mean zero and

4

E(ikik ) = I(k = k ), referred to as the kth functional principal component score (FPCA) of the ith subject. Thus, the response measurements in (2.1) can be represented as follows:

p



Yij = o(zj)Xi + 1k/2ikk(zj) + (zj)ij.

=0

k=1

(2.2)

2.2. Spline approximation over triangulations and penalized regression

Note that the objects of interest on many biomedical images are often distributed over an irregular domain . Triangulation is an effective strategy to handle such data. For example, the spatial smoothing problem over difficult regions in Ramsay (2002) and Sangalli et al. (2013) was solved using the finite element method (FEM) on triangulations, which was developed primarily to solve partial differential equations. Here, we approximate each coefficient function in (2.2) using bivariate splines over triangulations (Lai and Schumaker, 2007). The idea is to approximate each function  (·) using Bernstein basis polynomials that are piecewise polynomial functions over a 2D triangulated domain. Compared with the FEM, the proposed approach is appealing in the sense that its spline functions are more flexible and it uses various smoothness settings to better approximate the coefficient functions. In this section, we briefly introduce the triangulation technique and describe the bivariate penalized spline smoothing (BPST) method used to approximate the spatial data.
Triangulation is an effective tool to deal with data distributed over difficult regions with complex boundaries and/or interior holes. In the following, we use T to denote a triangle that is a convex hull of three points not located on one line. A collection = {T1, . . . , TH } of H triangles is called a triangulation of  = Hh=1Th, provided that any nonempty intersection between a pair of triangles in is either a shared vertex or a shared edge. Given a triangle T  , let |T | be its longest edge length and T be the radius of the largest disk inscribed in T . Define the shape parameter of T as the ratio T = |T |/ T . When T is small, the triangles are relatively uniform in the sense that all angles of the triangles in are relatively the same. Denote the size of by | | = max{|T |, T  }, that is, the length of the longest edge of . For an integer r  0, let Cr() be the collection of all rth continuously differentiable functions over . Given , let Sdr( ) = {s  Cr() : s|T  Pd(T ), T  } be a spline space of degree d and smoothness r over , where s|T is the polynomial piece of spline s restricted on triangle T , and Pd is the space of all polynomials of degree less than or equal to d. Note that the major difference between the FEM and the BPST is the flexibility of the smoothness, r, and the degree of the polynomials, d. Specifically, the FEM in Sangalli et al. (2013) requires that r = 0 and d = 1 or 2, whereas the BPST allows smoothness r  0 and various degrees of polynomials.
We use Bernstein basis polynomials to represent the bivariate splines. For any =

5

0, 1, . . . , p, denote by the triangulation of the th component. Define

G(p+1)  G(p+1)( 0 × · · · × p) = g = (g0, . . . , gp) , g  Sdr( ), = 0, . . . , p ,

and let {B m}mM be the set of degree-d bivariate Bernstein basis polynomials for Sdr( ), where M is an index set of Bernstein basis polynomials. Denote by B the evaluation matrix

of the Bernstein basis polynomials for the th component, and let the jth row of B is given

by B (zj) = {B m(zj), m  M }. We approximate each  (·) using  (zj)  B (zj) , for

= 0, 1, . . . , p, where  = ( m, m  M ) is the spline coefficient vector.

Penalized spline smoothing has gained in popularity over the last two decades; see Hall

and Opsomer (2005); Claeskens et al. (2009); Schwarz and Krivobokova (2016). To define

the penalized spline method, for any direction zq, q = 1, 2, let vzq s(z) denote the vth­order derivative in the direction zq at the point z. We consider the following penalized least squares

problem:

nN
min
(0,...,p) G(p+1) i=1 j=1

p

2p

Yij - Xi  (zj) + n, E( ),

=0

=0

where E(s) =

T

T

i+j=2

2 i

(iz1 jz2 s)2dz1dz2

is

the

roughness

penalty,

and

n,

is the

penalty parameter for the th function.

To satisfy the smoothness condition of the splines, we need to impose some linear con-

straints on the spline coefficients  : H  = 0, for = 0, 1, . . . , p. Thus, we have to minimize

the following constrained least squares:

nN i=1 j=1

p
Yij - Xi B (zj)
=0

2p
+ n,  P  , subject to H  = 0,
=0

where P is the block diagonal penalty matrix satisfying  P  = E(B  ).

We first remove the constraint using a QR decomposition of the transpose of the constraint

matrix H . Applying a QR decomposition on H , we have H

=QR

= (Q ,1

Q ,2)

R ,1 R ,2

,

where Q is an orthogonal matrix and R is an upper triangular matrix. the submatrix Q ,1

represents the first r columns of Q , where r is the rank of matrix H , and R ,2 is a matrix

of zeros. We reparametrize this using  = Q ,2 , for some  . Then, it is guaranteed that

H  = 0. Thus, the minimization problem is converted to the following conventional penalized

regression problem, without restrictions:

nN i=1 j=1

p
Yij - Xi B (zj)Q ,2
=0

2p
+ n,  D  ,
=0

(2.3)

where D = Q ,2P Q ,2.

6

Let Yi = (Yi1, Yi2, . . . , YiN ) , B (z) = {B m(z), m  M } , Y = (Y1 , . . . , Yn ) , and U = (U11, U12, . . . , UnN ) , where

Uij = {Xi0B0(zj) Q0,2, Xi1B1(zj) Q1,2, · · · , XipBp(zj) Qp,2} .

(2.4)

Let  = (0 , 1 , . . . , p ) and D(n,0, . . . , n,p) = diag{n,0D0, . . . , n,pDp}. Minimizing (2.3) is then equivalent to minimizing Y - U 2 +  D(n,0, . . . , n,p). Hence,
 = (0 , 1 , . . . , p ) = {U U + D(n,0, . . . , n,p)}-1U Y.
Thus, the estimators of  and  (·) are

 = Q ,2 ,  (z) = B (z)  .

(2.5)

2.3. Asymptotic properties of the BPST estimators

This section examines the asymptotics of the proposed estimators. Given random variables

Un for n  1, we write Un = OP (bn) if limc lim supn P (|Un|  cbn) = 0. Similarly, we write Un = oP (bn) if limn P (|Un|  cbn) = 0, for any constant c > 0. Next, to facilitate discussion,

we introduce some notation of norms. For any function g over the closure of domain ,

denote

g

2 L2()

=

 g2(z)dz as the regular L2 norm of g, and

g , = supz |g(z)| as

the supremum norm of g. Further denote g ,, = max0 p |g |,,, where |g|,, = maxi+j= iz1jz2g , is the maximum norm of all th­order derivatives of g over . Let Wd,() = {g : |g|k,, < , 0  k  d} be the standard Sobolev space. Next, we introduce

some technical conditions.

(A1) For any = 0, . . . , p, o(·)  Wd+1,(), for an integer d  1. (A2) For any i = 1, . . . , n, j = 1, . . . , N , ij's are independent with mean zero and variance one,

and for any k  1, ik are uncorrelated random variables with mean zero and variance

one. (A3) For any = 0, 1, . . . , p, there exists a positive constant C , such that E|X |8  C . The

eigenvalues of X = E(XX ) are bounded away from zero and infinity. (A4) The function (z)  C(1)(), with 0 < c  (z)  C  , for any z  ; for any k,

k(z)  C(1)() and 0 < cG  G(z, z)  CG  , for any z  . (A5) Let | | = min0 p | | and | | = max0 p | |. the triangulations

satisify that

lim supn(| |/| |) < . The triangulations are -quasi-uniform; that is, there exists a positive constant , such that max0 p{(minT  T )-1| |}  . (A6) As N  , n  , for some 0 <  < 1, N -1n1/(d+1)+  0, n1/2| |d+1  0,

N 1/2| |  , and the smoothing parameters satisfy that n-1/2N -1| |-3n  0, where

n = max0 p n, .

7

The above assumptions are mild conditions that are satisfied in many practical situations. Assumption (A1) describes the usual requirement on the coefficient functions described in the literature on nonparametric estimation. Assumption (A1) can be relaxed to Assumption (A1 ) in Section 2.4, which only requires o(·)  C(0)() when dealing with imaging data with sharp edges; see Section 2.4. Assumptions (A1) and (A2) are similar to Assumptions (A1) and (A2) in Gu et al. (2014) and Assumptions (A1)­(A3) in Huang et al. (2004). Assumption (A3) is analogous to Assumption (A5) in Gu et al. (2014), ensuring that Xi is not multicollinear. Assumption (A5) requires that be of similar size, and suggests the use of more uniform triangulations with smaller shape parameters. Assumption (A6) implies that the number of pixels for each image N diverges to infinity and the sample size n grows as N  , a welldeveloped asymptotic scenario for dense functional data (Li and Hsing, 2010). Assumption (A6) also describes the requirement of the growth rate of the dimension of the spline spaces relative to the sample size and the image resolution. This assumption is easily satisfied because images measured using current technology are usually of sufficiently high resolution.
The following theorem provides the L2 convergence rate of  (·), for = 0, 1, . . . , p. a detailed proof is given in Appendix A.

Theorem 1. Suppose Assumptions (A1)­(A5) hold and N 1/2| |   as N  . Then,

for any = 0, 1, . . . , p, the BPST estimator  (·) is consistent and satisfies  - o L2() =

OP

n nN | |3

o

2, +

1

+

n nN |

|5

|

|d+1 o d+1, + n-1/2 .

Theorem 2 states the asymptotic normality of  at any given point z  , for = 0, 1, . . . , p. See Appendix A for a detailed proof. Denote





n(z) = B(z)

E

 -n,1


1 n2N

2

nN
Uij Uij
i=1 j,j =1

G(zj, zj

 )-n,1 B(z),


(2.6)

where Uij and n, are given in (2.4) and (A.17), respectively, in Appendix 1, B (z) = Q2, B (z) for = 0, . . . , p, and B(z) = diag{B0(z), · · · , Bp(z)}.

Theorem 2. Suppose Assumptions (A1)­(A6) hold. If for any = 0, 1, . . . , p, |Xi |  C < , then n-1/2(z){(z) - o(z)} -L N 0, I(p+1)×(p+1) as N   and n  , where n(z)

is given in (2.6). Furthermore, there exist positive constants cV < CV < +, such that

cV n-1

1

+

n nN |

|4

-2
 Var{ (z)}  CV n-1, for any

= 0, 1, . . . , p.

2.4. Piecewise constant spline over triangulation smoothing
Many imaging data can be regarded as a noisy version of a piecewise-smooth function of z   with sharp edges, which often reflect the functional or structural changes. The

8

penalized bivariate spline smoothing method introduced, in Section 2.2, assumes some degree of smoothness over the entire image. To relax this assumption while preserving the features of sharp edges, we make the following less stringent assumption on the smoothness of the coefficient functions:
(A1 ) For any = 0, . . . , p, the bivariate function o(·)  C(0)().

For the estimation, we consider the piecewise constant spline over triangulation (PCST)

method. For any = 1, . . . , p, denote by PC( ) the space of piecewise constant functions

over each Tm, for m  M . The bivariate spline basis functions of PC( ) are denoted as {B m(z)}mM , which are simply indicator functions over triangle Tm, B m(z) = I(z  Tm), m  M . Assumption (A1 ) controls the bias of the piecewise constant spline estimator for o

and leads to the estimation consistency.

When using the constant bivariate spline basis functions, we have E(s) = 0 for all s 

PC( ), and for any z  m = (0m, 1m, . . . , pm)

, =

B (z)B (z) Vm-1 (nN )-1

=

diag{B2m(z), m

n i=1

N j=1

B

m(zj

 M }. Then, we
p
)Xi Yij , where
=0

can

write



p

1 Vm = nN

N

B 2m (zj )

n

XiXi

j=1

i=1

1 =
 nN

n i=1

N
B 2m (zj )Xi
j=1

Xi



,

.
=0

(2.7)

By simple linear algebra, for any = 0, . . . , p, the PCST estimator is given by

c(z) =

 mB m(z).

mM

(2.8)

For any z  , define the index of the triangle containing z as m(z); that is, m(z) = m

if z  Tm.

Then,  (z) =  m(z)

and

c
 (z)

=

(0c(z), .

.

.

, pc(z))

= (0m(z), . . . , pm(z))

=

m(z). For any z  , denote

n(z) = n-1-X1G (z, z) .

(2.9)

Theorem 3 shows the asymptotic normality of the piecewise constant estimators (z). See the Appendix A for detailed proofs. To obtain the asymptotic variance-covariance function, we also need the following assumption:

(C1) The variables ik and ij are independent and satisfy E |ik|4+1 < + for some 1 > 0, and E |ij|4+2 <  for some 2 > 0.

Theorem 3. Under Assumptions (A1 ), (A2)­(A5), and (C1), as N   and n  , if for

some 0 <  < 1, N -1n1+  0, N -1/2 | |  | | n1/4N -1/2, and

 k=1

1k/2k



<

9

, then for any z  , -n 1/2(z){c(z) - o(z)} -L N 0, I(p+1)×(p+1) , where n(z) is (2.9); pr (nc, )-1(z)  (z) - o(z)  Z1-/2  1 - , for any   (0, 1), as N  , n  , where nc, (z) is the square root of the ( , )th entry of the matrix n(z), and Z1-/2 is the 100 (1 - /2)th percentile of the standard normal distribution.

3. Variance Function Estimation and Simultaneous Confidence Corridors

3.1. Estimation of the variance function

Define the estimated residual Rij = Yij -

p =0

Xi



(zj )

or

Yij

-

p =0

Xi

 c (zj ),

for

any

i = 1, . . . , n, j = 1, . . . , N . We apply the bivariate spline smoothing method to {(Rij, zj)}Nj=1.

Specifically, we define

N

2

i(z) = arg min

Rij - gi(zj) , i = 1, . . . , n,

giSdr( ) j=1

(3.1)

as the spline estimator of i(z), where the triangulation  may differ from that introduced in Section 2 when estimating o(z). Next, let ij = Rij - i(zj). Define the estimators of G(z, z ) and 2(zj) as

n

n

G(z, z ) = n-1 i(z)i(z ) and 2(zj) = n-1

ij ij ,

i=1

i=1

(3.2)

p

respectively. In general, for spline estimators (d  0), denote n(z) =

n2,

(z)

, where
, =0

1

n

n(z) = n2N 2 B(z)

i=1

N

N

-n,1Uij Uij G(zj , zj )-n,1 + Uij Uij 2(zj ) B(z). (3.3)

j,j =1

j=1

Note that the estimation can be much simplified if PCST smoothing is applied. In this case,

the variance-covariance matrix n(z) can be simply estimated using

n(z) =

(nc ,

)2(z)

p ,

1 =0 = n

n
n-1 XiXi
i=1

-1

2(z)

G(z, z) + N Am(z)

,

where Am(z) is the area of triangle Tm(z) divided by the area of the domain. The following conditions (C2)­(C3) are required for the bivariate spline approximation in the covariance

estimation and to establish the estimation consistency. The proofs of the results in this section

are provided in the Appendix A.

(C2) For any k  1, k(z)  Ws+1, for an integer s  0, and for a sequence {Kn} n=1

of increasing positive integers with limn Kn  , | |s+1

Kn k=1

k1/2

k

s+1,  0 as

N  , n  .

10

(C3) As N  , n  , for some 0 <  < 1, N -1n1/(d+1)+  0, N | |2  , and n| |2/(log n)1/2  .

Assumption (C2) concerns the bounded smoothness of the principal components that bound the bias terms in the spline covariance estimator.

Theorem 4. Under Assumptions (A1)­(A6) and (C1)­(C3), G(z, z ) uniformly converges to G(z, z ) in probability; that is, sup(z,z )2 |G(z, z ) - G(z, z )| = oP (1).

Corollary 1. Under Assumptions (A1)­(A6), (C1)­(C3), the estimator of n(z) uniformly converges to to n(z) in probability; that is, supz |n(z) - n(z)| = oP (1).

Denote 
nc , (z) = n-1/2 e

n
n-1 XiXi
i=1

-1
e

1/2 2(z) G(z, z) + N Am(z)  .

(3.4)

From Corollary 1, nc, (z) is a consistent estimator of nc, (z) in (2.9). 3.2. Bootstrap simultaneous confidence corridors (SCCs)

From Theorems 2 and 3 and Slutzky's Theorem, we have the following asymptotic PCIs.

Corollary 2. (a) For the BPST estimators, under Assumptions (A1)­(A6), for any = 0, . . . , p,   (0, 1), as N  , n  , an asymptotic 100(1 - )% PCI for o(z), is  (z)±n, (z)Z1-/2, for any z  , where n2, (z) is the ( , )th entry of the matrix -n 1/2(z), and Z1-/2 is the 100 (1 - /2)th percentile of the standard normal distribution.
(b) For the PCST estimators, under Assumptions (A1 ) and (A2)­(A6), if for some 0 <  < 1, N -1n1+  0, an asymptotic 100(1 - )% PCI for o(z) is c(z) ± nc, (z)Z1-/2, for any z  , where nc , (z) is the standard deviation function of c(z) in Theorem 3.

Next, we introduce a simple bootstrap approach to extend the PCIs to the SCCs. Our approach is based on the nonparametric bootstrap method used in Hall and Horowitz (2013). We triangulate the domain  using quasi-uniform triangles, obtaining a set of approximate 100(1 - )% PCIs. In the following, 0 denotes the nominal confidence level of the desired SCCs. We recalibrate the PCIs using the following bootstrap method.

N,n

Step 1. Based on (Xi, Yij)

, obtain the coefficient functions o(z) using the BPST es-

j=1,i=1

timators  (z) in (2.5) or the PCST estimators c(z) in (2.8), for = 0, . . . , p. Let

µ(z) =

p =0

Xi



(z)

or

p =0

Xi

c(z).

11

Step 2. Obtain i(z) and ij presented in (3.1)­(3.2), and estimate G(z, z), 2(z), and n2, (z) using G(z, z) and 2(z) in (3.2) and n2, (z) in (3.3) or (3.4), respectively.

Step 3. Obtain an adjusted nominal confidence level  (0).

(i) Generate an independent random sample i(b) and i(jb) from {-1, 1} with probability 0.5 each, and define Yij(b) = µ(zj) + i(b)i(zj) + i(jb)ij.

(ii) Based on

(Xi, Yij(b))

N,n , obtain (b)(z) using (2.5) or (2.8), and calculate
j=1,i=1

n(,b) using (3.3) or (3.4).

(iii) Construct SCCs for the resampled data

(Xi, Yij(b))

N,n

:
j=1,i=1

B(b)(),

b = 1, · · ·

, B,

B(b)() = {(z, y) : z  , (b)(z)-n(,b)(z)Z1-/2  y  (b)(z)+n(,b)(z)Z1-/2}.

(iv) Estimate the coverage rate  (zj, ) = P {(zj,  (zj))  B()|X} using  (zj, ) =

1 B

B b=1

I

{(zj

,



(zj ))



B(b)()}.

(v) Find the root of the equation  (zj, ) = 1 - 0, for j = 1, . . . , N , and denote it as { (zj, 0)}Nj=1. The root can be found using the grid method by repeating the last

two steps for different values of .

(vi) Take the minimum of { (zj, 0)}Nj=1 and denote it as    (0).

Step 4. Construct the final SCCs: B( ) = {(z, y) : z  ,  (z) - n, (z)Z1- /2  y   (z) + n, (z)Z1- /2}.

4. Implementation

The proposed procedure can be implemented using our R package "FDAimage" (Yu et al.,

2019), in which the bivariate spline basis is generated using the R package "BPST" (Wang et al.,

2019). When the response imaging seems to be a realization from some smooth function, we

suggest using the smoothing parameter r = 1 and degree d  5, which achieves full estimation

power asymptotically (Lai and Schumaker, 2007). In contrast, if there are sharp edges on the

images, we suggest considering the PCST presented in Section 2.4.

Selecting suitable values for the smoothing parameters is important to good model fitting.

To select n, , for = 0, . . . , p, we used K-fold cross-validation (CV). The individuals are randomly partitioned into K groups, where one group is retained as a test set, and the remaining

K - 1 groups are used as training sets. The CV process is repeated K times (the folds), with

each of the K groups used exactly once as the validation data. Then, the K-fold CV score is

K

N

CV(n,0, . . . , n,p) = K-1 (|Vk|N )-1

{Yij - Xi -k(zj)}2,

k=1

iVk j=1

12

where Vk is the kth testing set for k = 1, . . . , K, and -k is the corresponding estimator after removing the kth testing set. We use K = 5 in our numerical examples.
To determine an optimal triangulation, the criterion usually considers the shape, size, or number of triangles. In terms of shape, a "good" triangulation usually refers to one with wellshaped triangles without small angles and/or obtuse angles. Therefore, for a given number of triangles, Lai and Schumaker (2007) and Lindgren et al. (2011) recommended selecting the triangulation according to "max-min" criterion, which maximizes the minimum angle of all the angles of the triangles in the triangulation. With respect to the number of triangles, our numerical studies show that a lower limit of the number of triangles is necessary to capture the features of the images. However, once this minimum number has been reached, refining the triangulation further usually has little effect on the fitting process. In practice, when using higher-order BPST smoothing, we suggest taking the number of triangles as Hn = min{ c1n1/(2d+2)N 1/2 , N/10}, where c1 is a tuning parameter. We find that c1  [0.3, 2.0] works well in our numerical studies. When using the PCST, we suggest taking the number of triangles as Hn = min{ c2n-1/4N , N/2}, with c2  [0.3, 2.0]. Once Hn is chosen, we can build the triangulation using typical triangulation construction methods, such as Delaunay triangulation and DistMesh (Persson and Strang, 2004).
5. Simulation Studies
In this section, we conduct two Monte Carlo simulation studies using our R package "FDAimage" (Yu et al., 2019) to examine the finite-sample performance of the proposed methodology. The triangulations used here can be found in the data set in the "FDAimage" package. To illustrate the performance of our estimation method, we compare the proposed spline method with the kernel method proposed by Zhu et al. (2014) (Kernel) and the tensor regression method of Li and Zhang (2017) (Tensor). To implement the kernel method, we use the R Package SVCM, which is publicly available at https://github.com/BIG-S2/SVCM. For the tensor method, the accompanying MATLAB code at https://ani.stat.fsu.edu/ ~henry/TensorEnvelopes_html.html is used. We compare the proposed method with the tensor regression approach in Li and Zhang (2017) and the three-stage FDA approach in Zhu et al. (2014).
5.1. Example 1
To illustrate the advantage of the proposed method over a complex domain, we study the horseshoe domain in Sangalli et al. (2013). The response images are generated from the following model: Yij = 0o(zj) + Xi1o(zj) + i(zj) + ij, for i = 1, . . . , n, j = 1, . . . , N , and zj  . To understand the advantages and disadvantages of different methods, we consider
13

1.0

0.5

0.0

v1

-1.0 -0.5

Case I (jump functions)

Case II (smooth functions)

1.0

1.0

1.0

0.5

0.5

0.5

0.0

v1

0.0

v1

0.0

v1

2 1.2 2.4 3.6
1
0
1 -1.2 -2.4 -3.6
2

-1.0 -0.5

-1.0 -0.5

-1.0 -0.5

-1

0

o
10 2

3

-41

0

o
112

3

4 -1

0

o
10 2

3

-41

0

o
11 2

3

4

u1

u1

u1

u1

Figure 5.1: The true coefficient functions in Simulation Example 1.

Table 5.1: Estimation errors of the coefficient estimators,  = 2.0.

Function Type

n

Method

1 = 0.03, 2 = 0.006

0

1

BPST 0.0139 0.0182

50

PCST 0.0088 Kernel 0.0801

0.0090 0.0819

Jump

Tensor 0.0799 BPST 0.0090

0.0248 0.0118

PCST 0.0044 100 Kernel 0.0400

0.0044 0.0405

Tensor 0.0395 0.0166

BPST 0.0026 0.0032

50

PCST 0.0088 Kernel 0.0801

0.0090 0.0819

Smooth

Tensor 0.0799 BPST 0.0016

0.0256 0.0019

100

PCST Kernel

0.0070 0.0400

0.0086 0.0405

Tensor 0.0399 0.0168

1 = 0.2, 2 = 0.05

0

1

0.0145 0.0189

0.0094 0.0097

0.0807 0.0826

0.0799 0.0254

0.0093 0.0122

0.0047 0.0047

0.0403 0.0409

0.0399 0.0171

0.0032 0.0041

0.0119 0.0139

0.0807 0.0826

0.0806 0.0271

0.0019 0.0022

0.0073 0.0090

0.0403 0.0409

0.0402 0.0179

two types of coefficient functions in the above image-on-scalar regression model: (I) functions

with jumps; and (II) smooth functions. The true coefficient functions are shown in Figure 5.1.

For each image, we set the resolution as 100 × 50 (pixels). The true signal falls only within

the horseshoe domain (3182 pixels); outside the domain is pure noise. We generate the scalar

covariate Xi  N (0, 1), and then truncate it by [-3, +3]. We set i(z) =

2 k=1

k1/2ik

k

(z),

where (1, 2) = (0.1, 0.02) or (0.2, 0.05) and i1 and i2  N (0, 1), 1(z) = c1 sin(2z1), and

2(z) = c2 cos(2z2). Let c1 = 0.56 and c2 = 0.61, such that 1 and 2 are orthonormal

functions on . The measurement error ij is independently generated from N (0, 1) and

 = 1.0, 2.0.

To fit the model, we consider the BPST and PCST methods presented in Section 2. To

obtain the BPST estimators, we set d = 5 and r = 0 when generating the bivariate spline basis

functions. Figure B.1 in the Appendix B illustrates the triangulations used for the BPST and

14

PCST. The triangulation used for the BPST ( 1) contains 90 triangles (73 vertices), and the triangulation used for the PCST ( 2) contains 346 triangles (226 vertices).
We quantify the estimation accuracy of the coefficient functions using the mean squared error (MSE). Table 5.1 provides the average MSE (across 500 Monte Carlo experiments) for two types of coefficient functions. To save space, we present the results for  = 2.0 only; the results for  = 1.0 are presented in Table B.1 in the Appendix B. As expected, the estimation accuracy of all the methods improves as the sample size increases or the noise level decreases. In both scenarios, the BPST and PCST outperform the other two competitors, reflecting the advantage of our method over a complex domain. When the true coefficient functions are smooth, the BPST provides the best estimation, followed by the PCST. On the other hand, when the true coefficient function contains jumps, the PCST provides a better result. For the tensor regression, the estimator of 1o(·) is much more accurate than that of 0o(·), owing to the design of the coefficient function. Figure 5.1 shows that, in contrast to the intercept function of 0o(·), the true slope function of 1o(·) is still smooth across the complex boundary. Moreover, when the coefficient function is smooth across the boundary, the estimation accuracy is also affected by the domain of the true signal. The performance of the kernel method is not affected by the design of the coefficient functions. Instead, it depends heavily on the noise level, owing to the three-stage structure.

5.2. Example 2

In this example, we simulate the data by considering the domains of the fifth and 35th

slices of the brain images illustrated in Section 6 as the domain . We generate response images

based on a set of smooth coefficient functions from the following model: Yij =

2 =0

Xi

 o (zj )+

i(zj) + ij, for i = 1, . . . , n, j = 1, . . . , N , and zj  , where 0o(z) = 5{(z1 - 0.5)2 + (z2 -

0.5)2}, 1o(z) = -1.5z13 + 1.5z23 and 2o(z) = 2 - 2 exp[-8{(z1 - 0.5)2 + (z2 - 0.5)2}]. the true

coefficient images are shown in the first columns of Figures B.5 and B.6 in the Appendix B for

the fifth and 35th slices, respectively. For each image, we simulate the data at all 79×95 pixels.

To mimic real brain images, the true signals are generated only on the pixels/voxels (3476 or

5203 pixels in total) within the brain domain; outside the boundary of the brain, the image

contains only noise. We set Xi0 = 1 and generate Xi = (Xi1, Xi2)  N (0, ), with  =

1.0 0.5

0.5 1.0

and Xi

truncated by [-3, +3]. For the error terms, we set i(z) =

2 k=1

k1/2ik

k

(z),

where i1 and i2  N (0, 1), 1(z) = 1.488{sin(z1) - 1.5}, 2(z) = 1.939 cos(2z2), and

(1, 2) = (0.1, 0.02) or (0.2, 0.05). The measurement error ij is independently generated

from N (0, 1) and  = 0.5, 1.0. To conserve space, we show only the results for the domain of

the fifth slice for  = 1.0 here. The results for  = 0.5 and those based on the domain of the

35th slice are shown in Appendix B.

15

Table 5.2: Estimation errors of the coefficient function estimators,  = 1.0.

n

Method

1 = 0.1, 2 = 0.02 1 = 0.2, 2 = 0.05

0

1

2

0

3

2

BPST( 3) 0.003 0.005 0.005 0.007 0.011 0.010

50

BPST( 4) 0.003 0.005 0.005 0.007 0.010 0.009 Kernel 0.023 0.032 0.032 0.026 0.037 0.037

Tensor 0.023 0.013 0.019 0.026 0.017 0.024

BPST( 3) 0.002 0.002 0.002 0.003 0.005 0.005

100

BPST( 4) Kernel

0.002 0.011

0.002 0.015

0.002 0.015

0.003 0.013

0.004 0.018

0.004 0.018

Tensor 0.011 0.007 0.011 0.013 0.009 0.013

Because the functions in this example are smooth, for the bivariate spline approach, we consider only the BPST method. To further study the effect of different triangulations, we consider 3 and 4; see Figure B.4 in the Appendix B. Similarly to Section 5.1, we summarize the MSE for different coefficient functions based on 500 Monte Carlo experiments in Table 5.2. Columns 2­5 in Figure B.5 in the Appendix B show the estimated coefficient functions using the kernel, tensor and BPST methods, respectively. Table 5.2 and Figure B.5 in the Appendix B show that the estimation accuracy improves for all methods as the sample size increases or the noise level decreases. In all settings, the BPST method has the smallest MSE compared with the kernel and tensor methods, reflecting the advantage of our method in estimating the coefficient functions and, hence, the regression function. Because the kernel and tensor methods are both designed for a rectangle domain, the estimation accuracy can be affected by the noise outside the domain. Futhermore, the MSE is invariable across two triangulations, thus, 3 might be sufficient to capture the feature in the data set. This also implies that when this minimum number of triangles is reached, further refining the triangulation has little effect on the fitting process, but makes the computational burden unnecessarily heavy.
Finally, we illustrate the finite-sample performance of the proposed SCCs for the coefficient functions described in Section 3. In particular, we report the empirical coverage probabilities of the nominal 95% SCCs using triangulation 3. We evaluate the coverage of the proposed SCCs over all pixels on the interior of , and test whether the true functions are entirely covered by the SCCs at these pixels. Table 5.3 summarizes the empirical coverage rate (ECR) for 500 Monte Carlo experiments of the 95% SCCs and the average width of the SCCs. The results clearly show that the ECRs of the SCCs are well approximated to 95%, particularly as the sample size increases. Table 5.3 also reveals that the SCCs tend to be narrower when the sample size becomes larger or the noise level decreases.
6. ADNI Data Analysis

16

Table 5.3: The coverage rate of the 95% SCCs for the coefficient functions.

Coverage

Width

n





0

1

2

0

1

2

50

(0.1,0.02)

0.5 1.0

0.976 0.976

0.928 0.940

0.938 0.952

0.332 0.358

0.362 0.392

0.377 0.413

(0.2,0.05)

0.5 1.0

0.962 0.970

0.918 0.930

0.932 0.940

0.445 0.478

0.497 0.527

0.513 0.544

0.5 0.970 0.956 0.956 0.234 0.250 0.267

100

(0.1,0.02) (0.2,0.05)

1.0 0.5 1.0

0.978 0.956 0.966

0.968 0.958 0.964

0.978 0.936 0.954

0.262 0.313 0.344

0.285 0.348 0.378

0.297 0.357 0.389

To illustrate the proposed method, we consider the spatially normalized FDG (fludeoxyglucose) PET data of the Alzheimer's Disease Neuroimaging Initiative (ADNI). As pointed out in Marcus et al. (2014), FDG-PET images have been shown to be a promising modality for detecting functional brain changes in Alzheimer's Disease (AD). The data can be obtained from the ADNI database at http://adni.loni.usc.edu/. The database contains spatially normalized PET images of 447 subjects. Of these 447 subjects, 112 have normal cognitive functions, considered to be the control group, 213 are diagnosed as mild cognitive impairment (MCI), and 122 are diagnosed as AD. Table B.5 in the Appendix B summarizes the distribution of patients by diagnosis status and sex.
In this study, we examine several patient-level features: (i) demographical features, such as age (Age) and sex (Sex); (ii) a dummy variable for the abnormal diagnosis status "MA" (1 = "AD" or "MCI", zero otherwise); (iii) a dummy variable for "AD" (1 = "AD," zero otherwise); and (iv) dummy variables for the APOE genotype, the strongest genetic risk factor for "AD"; see Corder et al. (1993). We code APOE1 as a dummy variable for subjects with one epsilon 4 allele, and APOE2 as subjects who have two alleles.
Noting that the PET images are 3D, we select the 5th, 8th, 15th, 35th, 55th, 62nd, and 6fifth horizontal slices (bottom to up) of the brain from a total of 68 slices to illustrate our method. Each slice of the image contains 79×95 pixels, but the domains of different brain slices are quite different. Specifically, the domain boundary for the bottom slices and upper slices are much more complex than the slices in the middle; more examples can be found in Figure B.7 in the Appendix B. For each slice, we consider the following image-on-scalar regression:
Yi(zj) =0(zj) + 1(zj)MAi + 2(zj)ADi + 3(zj)Agei + 4(zj)Sexi + 5(zj)APOE1i + 6(zj)APOE2i + i(zj) + (zj)i(zj), i = 1, . . . , n.
We fit the above model using the BPST method for each slice; see Figure B.7 in the Appendix B for the set of triangulations used for the BPST method. The image maps in Figure
17

Table 5.1: 10-fold CV results for the ADNI dataset. (×10-2)

Method BPST Kernel Tensor

Slice 5 1.4508 1.4533 1.5010

Slice 8 1.4809 1.4828 1.5260

Slice 15 1.5013 1.5021 1.5400

Slice 35 1.5633 1.5638 1.5900

Slice 55 2.0693 2.0715 2.1000

Slice 62 2.3020 2.3060 2.3340

Slice 65 2.6239 2.6303 2.6400

5.1 and Figures B.8 and B.9 in the Appendix B present the estimated coefficient functions using the BPST (d = 5, r = 1) method. To evaluate the predictive performance, Table 5.1 reports the 10-fold CV (parts of the images are left out as training sets) MSPE results for the BPST method, kernel method in Zhu et al. (2014), and tensor regression method in Li and Zhang (2017). The table shows that the MSPEs of the BPST method are uniformly smaller than those of the kernel method and tensor regression methods.
Next, we construct the 95% SCCs to check whether the covariates are significant. The yellow and blue colors on the "significance" map in Figure 5.1 indicate the regions in which zero is below the lower SCC or above the upper SCC, respectively. Using these estimated coefficient functions and the 95% SCCs, we can assess the impact of the covariates on the response images. Taking the fifth slice as an example, the main impact of "AD" on in the PET images is an increase in activity in the cerebellum compared with a normal individual. The cerebellum obtains information from the sensory systems, spinal cord, and other parts of the brain, and then regulates motor movements, resulting in smooth and balanced muscular activities. The significance map of "Age" also shows an increase in activity in the cerebellum, and "Sex" shows different effects in the male and female brain images. The significance maps of the covariates for all other slices of the PET image are shown in Figures B.10 ­ B.11 in the Appendix B. From these figures, we can see that the effect of the covariates on the brain activity level varies between slices, depending on the location of the slice; see the Appendix B for further details.
7. Conclusion
We examine a class of image-on-scalar regression models to efficiently explore the spatial nonstationarity of a regression relationship between imaging responses and scalar predictors, allowing the regression coefficients to change with the pixels. We have proposed an efficient estimation procedure to carry out statistical inference. We have developed a fast and accurate method for estimating the coefficient images, while consistently estimating their standard deviation images. Our method provides coefficient maps and significance maps that highlight and visualize the associations with brain and the potential risk factors, adjusted for other patientlevel features, as well as permitting inference. In addition, it allows an easy implementation

18

Intercept
1.0 0.5
AD
0.05 0.00 -0.05
Sex
0.05 0.00 -0.05
APOE2
0.10 0.05 0.00

MA
0.050 0.025 0.000 -0.025 -0.050
Age
0.005
0.000
-0.005
APOE1
0.06 0.04 0.02 0.00 -0.02

Figure 5.1: The BPST estimate and significance map of the coefficient functions for the fifth slice of the PET images. The yellow and blue colors in the significance map indicate the regions in which zero is below the lower SCC or above the upper SCC, respectively.

19

of piecewise polynomial representations of various degrees and smoothness over an arbitrary triangulation, and therefore can handle irregular-shaped 2D objects with different visual qualities. This provides enormous flexibility, accommodating various types of nonstationarity that are commonly encountered in imaging data analysis. Our methodology is extendable to 3D images to fully realize its potential usefulness in biomedical imaging. Instead of using bivariate splines over triangulation, the trivariate splines over tetrahedral partitions introduced in Lai and Schumaker (2007) could be well suited, because they have many properties in common with the bivariate splines over triangulation. However, this is a nontrivial task, because the computation is much more challenging for high-resolution 3D images than it is for 2D images, and thus warrants further investigation. Acknowledgements
The authors wish to thank the editor, associate editor, and two reviewers for their constructive comments and suggestions. Shan Yu's research was partially supported by the Iowa State University Plant Sciences Institute Scholars Program. Li Wang's research was partially supported by National Science Foundation grants DMS-1542332 and DMS-1916204. Lijian Yang's research was partially supported by National Natural Science Foundation of China award 11771240. The study data were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (ADNI.loni.usc.edu). As such, the investigators within the ADNI contributed to the design and implementation of the ADNI and/or provided data, but did not participate in the analysis or the writing of this report. A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ ADNI_Acknowledgement_List.pdf.
20

Appendix A

In the following, we use c, C, c1, c2, C1, C2, etc. as generic constants, which may

be different even in the same line. For any sequence an and bn, we write an bn if

there exist two positive constants c1, c2 such that c1|an|  |bn|  c2|an|, for all n 

1. For a real valued vector a, denote a its Euclidean norm. For a matrix A =

(aij), denote A  = maxi,j |aij|. For any positive definite matrix A, let min(A) and

max(A) be the smallest and largest eigenvalues of A. For a vector valued function g =

(g0, . . . , gp) , denote

g L2() = {

p =0

g

2 L2

()

}1/2

and

g , = max0 p g ,,

where g L2, and g , are the L2 norm and supremum norm of g defined at the

beginning of Section 2.2. Further denote g ,, = max0 p |g |,,, where |g |,, =

maxi+j= iz1jz2g (z) ,. For notation simplicity, we drop the subscript  in the rest of the paper. For g(1)(z) = (g0(1)(z), . . . , gp(1)(z)) and g(2)(z) = (g0(2)(z), . . . , gp(2)(z)) ,

define the empirical inner product as

g(1), g(2)

n,N

=

1 nN

p

nN
Xi Xi g(1)(zj)g(2)(zj),

, =0 i=1 j=1

(A.1)

and the theoretical inner product as

p

g(1), g(2) =

E(X X ) g(1)(z)g(2)(z)dz,

, =0



(A.2)

and denote the corresponding empirical and theoretical norms · n,N and · . Furthermore, let · E be the norm introduced by the inner product ·, · E , where,
for g(1)(z) and g(2)(z),

p
g(1), g(2) E = , =0 

i+j=2

2 i

(iz1 jz2 g(1))

2 i

(iz1 jz2 g(2))

dz1dz2.

i+j=2

Let A() be the area of the domain , and without loss of generality, we assume A() = 1 in the rest of the article. Note that the triangulation for different coefficient function can be different from each other. For notational convenience in the proof below, we consider a common triangulation for all the explanatory variables: B0(z) = B1(z) = · · · = Bp(z) = B(z), and  (zj) = B (zj) .
A.1. Properties of bivariate splines

We cite two important results from Lai and Schumaker (2007).

21

Lemma A.1 (Theorem 2.7, Lai and Schumaker (2007)). Let {Bm}mM be the Bernstein

polynomial basis for spline space Sdr( ) defined over a -quasi-uniform triangulation .

Then there exist positive constants c, C depending on the smoothness r, d, and the shape

parameter  such that c| |2 mM m2 

mM mBm

2  C|
L2

|2

mM m2 .

Lemma A.2 (Theorems 10.2 and 10.10, Lai and Schumaker (2007)). Suppose that | |

is a -quasi-uniform triangulation of a polygonal domian , and g(·)  Wd+1,().

(i) For bi-integer (a1, a2) with 0  a1 + a2  d, there exists a spline g(·)  Sd0( )

such that

  a1 a2 z1 z2

(g

-

g)





C|

|d+1-a1-a2|g|d+1,, where C is a constant

depending on d, and the shape parameter .

(ii) For bi-integer (a1, a2) with 0  a1 + a2  d, there exists a spline g(·)  Sdr( )

(d  3r + 2) such that

  a1 a2 z1 z2

(g

-

g)

  C|

|d+1-a1-a2 |g|d+1,, where C is

a constant depending on d, r, and the shape parameter .

Lemma A.2 shows that Sd0( ) has full approximation power, and Sdr( ) also has full approximation power if d  3r + 2. For any g(·) in Sobolev space C(0)(), there exists a
spline g(·)  PC( ) such that g - g   C| | g .

Lemma A.3. Let g(z) = (g0(z), . . . , gp(z)) , where g (z) = mM  mBm(z). Then,

under Assumptions (A3) and (A5), g

p =0

g

L2 .

Proof. By (A.1), g 2 =

p ,

=0 E(X X

)

 g (z)g

(z)dz =

 g (z)Xg(z)dz. Ac-

cording to Assumptions (A3) and (A5), g 2  g (z)g(z)dz

p =0

g

L2 .

Lemma A.4. Under Assumptions (A4) and (A5), for any Bernstein basis polynomials

Bm(z), m  M, of degree d  0, we have

max
mM

1 N

N

Bmk (zj) -

j=1

Bmk (z)dz = O |


|N -1/2 , 1  k < ,

(A.3)

1N

max m,m M N

Bm(zj)Bm (zj) -

j=1

Bm(z)Bm (z)dz = O |


|N -1/2 ,

(A.4)

1N

max
m,m M

N2

G(zj, zj )Bm(zj)Bm (zj )-

j,j =1

G(z, z )Bm(z)Bm (z )dzdz
2

= O N -1/2| |3 ,

(A.5)

max
mM

Bm

2 N,L2

-

Bm

2 L2

= max mM

1 N

N

Bm2 (zj)2(zj)-

j=1

2(z)Bm2 (z)dz


= O N -1/2| | .

(A.6)

22

Proof. Note that there are d = (d + 1)(d + 2)/2 Bernstein basis polynomials on each
triangle and  Bmk (z)dz = T m/d Bmk (z)dz, for any k  1. For piecewise constant basis functions, we have Bm(z) = I(z  Tm), then

1 N

N

Bmk (zj) -

j=1

Bmk (z)dz


=

1N N I(zj  Tm) - A(Tm) .
j=1

According to Assumption (A5),

max
mM

1 N

N

Bmk (zj) -

j=1

Bmk (z)dz  CN -1/2|


|.

For any j = 1, . . . , N , let Vj be the jth pixel, and it is clear that

1 N

N

Bmk (zj) -

j=1

Bmk (z)dz 


N j=1

{Bmk (zj) - Bmk (z)}dz +
Vj

Bmk (z)dz.
\Vj

If d  1, by the properties of bivariate spline basis functions in Lai and Schumaker (2007), \Vj Bmk (z)dz = O(N -1/2| |), and

N

{Bmk (zj) - Bmk (z)}dz 

|Bmk (zj) - Bmk (z)|dz

j=1 Vj

{j:zj T m/d } Vj

 C(N | |2) × N -1 × (N -1/2| |-1)  CN -1/2| |.

Thus, (A.3) holds. The proof of (A.4) is similar to the proof (A.3), thus omitted. Next, for any m, m  M,

1NN

N2

G(zj, zj )Bm(zj)Bm (zj ) -

j=1 j =1

G(z, z )Bm(z)Bm (z )dzdz
2

NN

=

{G(zj, zj )Bm(zj)Bm (zj ) - G(z, z )Bm(z)Bm (z )} dzdz

j=1 j =1 Vj ×Vj

+

{G(zj, zj )Bm(zj)Bm (zj ) - G(z, z )Bm(z)Bm (z )} dzdz .

2\j,j Vj ×Vj

As N  ,

| |3

{G(zj, zj )Bm(zj)Bm (zj ) - G(z, z )Bm(z)Bm (z )} dzdz = O  .

2\j,j Vj ×Vj

N

23

Notice that
NN
{G(zj, zj )Bm(zj)Bm (zj ) - G(z, z )Bm(z)Bm (z )} dzdz
j=1 j =1 Vj ×Vj



jj (GKm, 2N -1/2)dzdz ,

{(j,j ):zj T m/d ,zj T m/d } Vj ×Vj

where Km(z, z ) = Bm(z)Bm(z ) and

jj (g, ) =

sup

|g(z1, z1) - g(z2, z2)|

(z1,z1),(z2,z2)Vj ×Vj ,

z1-z2 2+ z1-z2 2= 2

is the modulus of continuity of g on Vj × Vj . Therefore, by Assumption (A4), we have

NN
{G(zj, zj )Bm(zj)Bm (zj ) - G(z, z )Bm(z)Bm (z )} dzdz
j=1 j =1 Vj ×Vj

 (N | |2)2 × N -2 × (N -1/2| |-1) = O(N -1/2| |3).

Thus, (A.5) follows. Finally, note that

1 N

N

Bm2 (zj)2(zj) -

j=1

2(z)dz 


N j=1

{Bm2 (zj)2(zj) - Bm2 (z)2(z)}dz
Vj

+

|Bm2 (zj)2(zj) - Bm2 (z)2(z)|dz.

\Vj

It is easy to see that \Vj |Bm2 (zj)2(zj) - Bm2 (z)2(z)|dz = O(N -1/2| |). Denote j(g, ) = supz,z Vj, z-z = |g(z) - g(z )| is the modulus of continuity of g on the jth
pixel Vj, then by Assumption (A4), we have

N

{Bm2 (zj)2(zj) - Bm2 (z)2(z)}dz 

j(Bm2 2, 2N -1/2)dz

j=1 Vj

{j:zj T m/d } Vj

 C(N | |2) × N -1 × (N -1/2| |-1)  CN -1/2| |.

We obtain (A.6).

Lemma A.5. For any m  M, 0  ,  p, let m, , = E(X X )  Bm2 (z)dz. Suppose Assumptions (A3) and (A5) hold, and N 1/2| |   as N  , then with

probability 1, one has

1n max max mM 0 , p nN

N
Bm2 (zj)Xi Xi - m, ,

i=1 j=1

= O n-1/2| |2(log n)1/2 + N -1/2| | .

24

Proof. Let i,m  i,m, ,

=

1 N

N j=1

Bm2 (zj)Xi

Xi

.

If N 1/2|

|   as N  , then

by

(A.3),

we

can

show

that

E(i,m)

=

1 N

N j=1

Bm2 (zj)E(X

X

)

| |2, and E (i,m)2 =

1 N

N j=1

Bm2 (z

j

)

2
E (X X

)2

| |4.

Next define a sequence Dn = n with   (1/3, 1/2). We make use of the following

truncated and tail decomposition Xi = Xi Xi = XiDn,1 + XiDn,2, where XiDn,1 =

Xi Xi I {|E(Xi Xi | > Dn}, XiDn,2 = Xi Xi I {|Xi Xi |  Dn}. Correspondingly the

truncated and tail parts of i,m are i,m,v  i,m,v, ,

=

1 N

N j=1

Bm2

(z

j

)XiDn,v

,

v

=

1, 2.

According to Assumption (A3), for any , = 0, . . . , p,


P {|Xn Xn
n=1

|

>

Dn}



 n=1

E

|Xn Xn Dn3

|3


 Cb Dn-3 < .
n=1

By

Borel-Cantelli

Lemma,

1 N

N j=1

Bm2 (zj

)XiDn,1

= 0, almost surely.

So for any

k

 1,

supm, , |n-1

n i=1

i,m,1|

=

Oa.s. (n-k ).

Since

N 1/2|

|   as N  ,

|E(i,m,1)| = |E(XiDn,1)|

1 N

N

Bm2 (zj)

j=1

 Dn-2E |Xi Xi |3

Bm2 (z)dz + O(N -1/2| |)  CDn-2| |2.



Next, we consider the truncated part i,m,2. Define i,m,2 = i,m,2-E(i,m,2), then Ei,m,2 = 0, and

E(i,m,2)2 = E(i,m,2)2 - (Ei,m,2)2 =

1 N

N
Bm2 (zj)

2

E(XiDn,2)2 - (EXiDn,2)2

.

j=1

Note that E(XiDn,1)2  Dn-1E |Xi Xi |3  cDn-1, thus, E(XiDn,2)2 = E(Xi )2-E(XiDn,1)2 =

E(Xi )2 - o(1). Therefore, there exists c such that for large n, we have E(i,m,2)2 

c E(Xi

)2 ×

1 N

N j=1

Bm2 (zj

)

2
. Next for any k > 2,

E i,m,2 k = E |i,m,2 - E (i,m,2)|k  2k-1 E |i,m,2|k + |E(i,m,2)|k

= 2k-1 E XiDn,2 k + O(1)

1 N

N

Bm2 (zj)

k
,

j=1

25

then there exists C > 0 such that for any k > 2 and large n,

E i,m,2 k  2k-1 Dnk-2E (Xi )2 + O(1)

1 N

N

Bm2 (zj)

k

j=1

 2kDnk-2E i,m,2 2

1 N

N

Bm2 (zj)

k-2


C Dn|

j=1

|2 k-2 k!E i,m,2 2 ,

which implies that

i,m,2

n i=1

satisfies

Cram´er's

condition

with

constant

C Dn|

|2. Ap-

plying Bernstein's inequality to

n i=1

i,m,2,

for

k

>

2

and

any

large

enough



>

0,

P

1 n

n
i,m,2  n-1/2|

|2(log n)1/2

i=1

2 log(n)  2 exp -
4 + 2C Dn(log n)1/2n-1/2

.

Assume that | |-2 n for some 0 <  < , we have




 P max
mM

1 n

n

i,m,2

 n-1/2|

n=1 0 , p i=1



 |2(log n)1/2  2

n-2- < .



n=1 mM 0 , p

Thus, supm, , n-1

n i=1

i,m,2

= Oa.s.

n-1/2|

|2(log n)1/2

as n  , by Borel-Cantelli

Lemma. Furthermore,

n

n

n

max n-1
m, ,

i,m - Ei,m  max n-1 m, ,

i,m,1 + max n-1 m, ,

i,m,2

+ max |Ei,m,1| m, ,

i=1

i=1

i=1

= Oa.s.(n-k) + Oa.s. n-1/2| |2(log n)1/2 + O Dn-2| |2 = Oa.s. n-1/2| |2(log n)1/2 .

Finally, we notice that

max
mM

1n nN

N
Bm2 (zj)Xi Xi - m, ,

0 , p

i=1 j=1

= max mM

n-1

n

i,m - Ei,m

+ |EXi Xi

| max mM

1 N

N
Bm2 (zj) -

0 , p

i=1

j=1

Bm2 (z)dz


=Oa.s. n-1/2| |2(log n)1/2 + O(N -1/2| |).

We obtain the desired result.

The following lemma provide the uniform convergence rate at which the empirical inner product in (A.1) approximates the theoretical inner product in (A.2).

26

Lemma A.6. Let g(1)(z) = mM c(1m)Bm(z), g(2)(z) = mM c(2m)Bm(z) be any spline functions in Sdr( ). Denote g(z) = (g0(z), . . . , gp(z)) with g  Sdr( ), = 0, . . . , p. Suppose Assumptions (A3) and (A5) hold, and N 1/2| |   as N  , then

Rn,N = sup
g(1),g(2)Sdr ( )

g(1), g(2) n,N - g(1), g(2) g(1) g(2)

Proof. It is easy to see

= OP {n-1/2(log n)1/2 + N -1/2| |-1}.

g(1), g(2) n,N

=

1n nN

N

i=1 j=1

p
c(1m)Xi Bm(zj)
=0 mM

p
c(2m) Xi Bm (zj)
=0 m M

=

c(1m) c(2m)

1 nN

n

N
Xi Xi Bm(zj)Bm (zj).

,m ,m

i=1 j=1

Note that g(r) 2 = ,m ,m c(rm)c(rm) E(X X )  Bm(z)Bm (z)dz, r = 1, 2. It follows from Assumptions (A1), (A2), Lemmas A.1 and A.3 that,

C1| |2

cv| |2 {c(vm)}2 

,m

1/2

{c(1m)}2

{c(2m) }2



,m

,m

g(v) 2  Cv| |2 {c(vm)}2,
,m

g(1) g(2)  C2| |2

{c(1m)}2

,m

1/2

{c(2m) }2

.

,m

With the above preparation, we have

Rn,N  C1|

, ,|m-m |(d+2)(d+1)/2 |c(1m)c(2m) |

|2

,m{c(1m)}2

,m {c(2m) }2 1/2

(A.7)

1 nN

× max
m,m M
0 , p

nN

Bm(zj)Bm (zj)Xi Xi

i=1 j=1

- E(X X )

Bm(z)Bm (z)dz


C

1 nN

 |

max |2 m,m M nN

Bm(zj)Bm (zj)Xi Xi

0 , p

i=1 j=1

- E(X X )

Bm(z)Bm (z)dz .


The desired result follows from (A.7) and Lemma A.5.

As a direct result of Lemma A.6, we can see that

sup

g

2 n,N

gSdr( )

g 2 - 1 = OP {n-1/2(log n)1/2 + N -1/2| |-1}.

(A.8)

27

A.2. Uniform convergence of the unpenalized spline estimators

In this section, we consider the unpenalized spline smoothing approach. The unpenalized bivariate spline estimator of o = (0o, . . . , po) is defined as

nN
 = (0, . . . , p) = arg min
G(p+1) i=1 j=1

p

2

Yi(zj) - Xi  (zj) .

=0

(A.9)

Denote

µ = (µ,0, . . . , µ,p)

=

-n,10

1 nN

n

N

Xi  B(zj) Xi o(zj)

i=1 j=1

 = (,0, . . . , ,p)

=

-n,10

1 nN

n

N

Xi  B(zj) i(zj),

i=1 j=1

 = (,0, . . . , ,p)

=

-n,10

1 nN

n

N

Xi  B(zj) (zj)ij,

i=1 j=1

where

1 nN

n,0 = nN

(XiXi )  {B(zj)B (zj)}.

i=1 j=1

(A.10)

Lemma A.7. Under Assumptions (A3) and (A5), if N 1/2| |   as N  , then
there exist constants 0 < c < C < , such that with probability approaching 1, as N  , n  , c| |2  min(n,0)  max(n,0)  C| |2, where n,0 is in (A.10).

Proof. Note that for any vector  = (0 , · · · , p ) with  = ( m, m  M) ,

1  n,0 = nN 

nN

(XiXi )  {B(zj)B (zj)} =

g

2 n,N

,

i=1 j=1

(A.11)

where  = Q2, and g = (g0, . . . , gp) with g = mM  mBm. By (A.8), we have

c(1-Rn,N )|

|2  2  (1-Rn,N ) g 2 

g

2 n,N

=

(1+Rn,N )

g

2  C(1+Rn,N )|

|2  2,

in which we have used the stability conditions in Lemma A.1.

Next, we consider the following decomposition (z) = µ(z) + (z) + (z), where

µ(z) = (µ,0(z), . . . , µ,p(z)) = {I  B(z)} µ, (z) = (0(z), . . . , p(z)) = {I  B(z)} , (z) = (0(z), . . . , p(z)) = {I  B(z)} .

(A.12) (A.13) (A.14)

28

Lemma A.8. Under Assumptions (A2)­(A5) and (C1), if N 1/2| |   as N  ,

 k=1

1k/2k

 <  and n1/(4+2)

n1/2N -1/2| |-1 for some 2, then for  and  in

(A.13) and (A.14),   = OP {n-1/2(log n)1/2} and   = OP {(nN )-1/2(log n)1/2| |-1}.

Proof. Note that for any = 0, 1, . . . , p,  (z) = mM , ,mBm(z) for some coefficients , ,m, so the order of  (z) is related to that of , ,m. In fact

  = max 0 p



  C ,

=

(e  1) -n,10

1n nN

N

Xi  B(zj) i(zj)

,

i=1 j=1



where  = (, ,m)mM with M being an index set of the transformed Bernstein basis polynomials Bm(z) and n,0 is the symmetric positive definite matrix defined in (A.10).
Thus, by Lemma A.7,

   C|

|-2 max max 0 p mM

1 nN

nN
Xi i(zj)Bm(zj)
i=1 j=1

,

almost surely. Next, we show that with probability 1,

1 nN

max max
0 p mM

nN

Xi i(zj)Bm(zj)
i=1 j=1

=O

n-1/2|

|2(log n)1/2

.

(A.15)

To prove (A.15), let i = i,m = 0 and

 k=1

k1/2Xi

ik

1 N

N j=1

Bm (z j )k (z j ),

where

E(

i) =

E(

i2)

=

E(Xi2 ) N2

N

N
Bm(zj)Bm(zj )G(zj, zj )

j=1 j =1

G(z, z )Bm(z)Bm(z )dzdz | |4.
2

We decompose the random variable i into a tail part and a truncated part,



Dn i,1

=

1k/2

k=1



Dn i,2

=

1k/2

k=1



µDi n =

1k/2

k=1

1N

N

Bm (z j )k (z j )

j=1

1N

N

Bm (z j )k (z j )

j=1

1N

N

Bm (z j )k (z j )

j=1

Xi ikI {|Xi ik| > Dn} , Xi ikI {|Xi ik|  Dn} - µDi n, E [Xi ikI {|Xi ik|  Dn}] ,

29

where Dn = n (1/(4 + 1) <  < 1/2). At first, we show that tail part vanishes almost surely. Note that, for any k  1,


P
n=1

{|Xn

nk| > Dn} 

 n=1

E |Xn nk|4+1 Dn4+1

 1


Dn-(4+1)
n=1

< .

By the Borel-Cantelli's lemma, we can show that E

1 n

n i=1

Dn i,1

= O (n-r), for any

r > 0.

As E(

i) = 0, then it is straightforward to verify that µDi n = -E(

Dn i,1

)

=

O(Dn-2| |2).

Next, notice that E(

Dn i,2

)

=

0.

Then, Var(

Dn i,2

)

=

E(

i2) - E(

Dn i,1

)2

-

(µDn )2

| |4. Also, we have, for any r  3,

E|

Dn i,2

|r

=

E



1k/2

1 N

N

r

Bm(zj)k(zj) [Xi ikI {|Xi ik|  Dn}] - µDi n

k=1

j=1

 2r-1 E



1r/2

1 N

N

r

Bm(zj)k(zj)Xi ikI {|Xi ik|  Dn} + µDi n r

k=1

j=1



1 2Dn N

N



Bm(zj) 1k/2k(zj)

r-2
E|

Dn i,2

|2



(C Dn |

|2 )r-2 E |

Dn i,2

|2

.

j=1

k=1

Thus, E | i,2/n|r  {Cn-1Dn| |2}r-2r!E( i2,2/n2) <  with the Cramer constant c = Cn-1Dn| |2. By the Bernstein inequality, for any large enough  > 0,

P

1n n
i=1

Dn i

 n-1/2|

|2(log n)1/2

-2 log n  2 exp
4c + 2CDn(log n)1/2n-1/2

 2n-3.

Hence,


P
n=1

1n max max 0 p mM n
i=1

i  n-1/2| |2(log n)1/2


 C| |-2 n-3 < 
n=1

for such  > 0. Thus, Borel-Cantelli's lemma implies that   = OP {n-1/2(log n)1/2}. The result of   = OP {(nN )-1/2(log n)1/2| |-1} can be established similarly, thus
omitted.

For (z) defined in (A.9), Theorem A.5 below provides its uniform convergence rate to o.
Theorem A.5. Under Assumptions (A1)­(A6), for (z) defined in (A.9), -o  = OP {| |d+1 o d+1, + n-1/2(log n)1/2}.

30

Proof. Note that  - o   µ - o  +   +  , where

nN
µ = arg min
gG(p+1) i=1 j=1

p

2

Xi (o - g )(zj) .

=0

Let  = (0, . . . , p)  G(p+1), where 's are the best approximation to o's with the approximation rate  - o   C| |d+1 o d+1, for any = 0, . . . , p. By Lai and
Wang (2013),

µ - o   µ -   +  - o   C| |d+1 o d+1,.

(A.16)

The desired result follows from Lemma A.8.

A.3. Asymptotic properties of penalized spline estimators

Let B(z) = Q2 B(z), then for U = X  (BQ2) defined in Section 2.2, we have

U = (X1  B(z1), . . . , X1  B(zN ), . . . , Xn  B(z1), . . . , Xn  B(zN )),

and U U =

n i=1

Nj=1(XiXi )  {B(zj)B (zj)}, U Y =

n i=1

N j=1

{Xi



B(z j )}Yij .

Let

1 n, = nN

n

N
(XiXi )  {B(zj)B

(z j )}

+

n nN

Ip



Q2

[

Bm, Bm

E ]m,m MQ2,

i=1 j=1

(A.17)

which is a symmetric positive definite matrix.

Next, we define

µ = (µ,0, . . . , µ,p)

=

-n,1

1 nN

n

N

Xi  B(zj) Xi o(zj),

i=1 j=1

 = (,0, . . . , ,p)

=

-n,1

1 nN

n

N

Xi  B(zj)


k1/2 ik k (z j ),

i=1 j=1

k=1

 = (,0, . . . , ,p)

=

-n,1

1 nN

n

N

Xi  B(zj) (zj)ij.

i=1 j=1

Note that, for any written as:

= 0, . . . , p, the penalized bivariate spline estimator  can be

 (z) = µ, (z) +  (z) +  (z),

(A.18)

31

where

µ, (z) = B(z) µ, ,  (z) = B(z) , ,  (z) = B(z) , ,

Therefore, we have

 (z) - o(z) = µ, (z) - o(z) +  (z) +  (z).

(A.19)

Lemma A.9. Under Assumptions (A3)­(A5), if N 1/2| |   as N  , then there

exist constants 0 < c < C < , such that with probability approaching 1 as N  

and n  , c|

|2  min(n,)  max(n,)  C

|

|2

+

n nN |

|2

.

Proof. By (A.11), it is easy to see that, for any vector  = (0 , · · · , p ) ,



n, =

g

2 n,N

+

n nN

p
 [ Bm, Bm E ]m,m M ,

=0

where  = (0, . . . , p) = Q2 with  = ( m, m  M) . Using the Markov's inequality in the supplement of Lai and Wang (2013) and Lemma A.1, we have

p

2

Cp

 mBm

 |

|4

2

 mBm

C

 |

|2



2.

=0 mM

E

=0 mM

L2

Thus, the largest eigenvalue of the matrix n, in (A.17) satisfies that max(n,) 

C {(1 + Rn,N )| |2 + (nN | |2)-1n}. Thus, we have with probability approaching 1,

max(n,)  C {| |2 + (nN | |2)-1n} for some positive constant C. On the other

hand, we use Lemma A.1 and equation (A.8) to have

g

2 n,N

= (1 - Rn,N )

g

2



c(1 - Rn,N )| |2  2.

Therefore, min(n,)  c(1 - Rn,N )| |2 = c| |2.

Lemma A.10. Under Assumptions (A1), (A3) and (A5), if N 1/2| |  , one has

µ - o  = OP

n nN | |3

o

2, +

1

+

n nN |

|5

|

|d+1 o d+1, .

Proof. Define

An = sup
gG(p+1)

g , g n,N

g

n,N = 0

,

An = sup
gG(p+1)

gE , g n,N

g

n,N = 0

,

(A.20)

where random variables An and An depend on the collection of Xi 's, i = 1, . . . , n, = 0, . . . , p. It is clear that o - µ   o - µ  + µ - µ , where µ is
given in (A.12), and µ - o   C| |d+1 o d+1, according to (A.16).

32

By the definition of An in (A.20), we have

µ - µ   An µ - µ n,N .

(A.21)

Note that the penalized spline µ of o is characterized by the orthogonality relations

nN o - µ, g n,N = n µ, g E , for all g  G(p+1),

(A.22)

while µ is characterized by

o - µ, g n,N = 0, for all g  G(p+1).

(A.23)

By (A.22) and (A.23), we have nN µ - µ, g n,N = n µ, g E , for all g  G(p+1). Inserting g = µ - µ yields that

nN

µ - µ

2 n,N

=

n

µ, µ - µ

E.

(A.24)

Thus, by Cauchy-Schwarz inequality and the definition of An.

nN

µ - µ

2 n,N



n

µ

E

µ - µ

E  nAn

µ

E

µ - µ

n,N .

Similarly,

using

(A.24),

nN

µ - µ

2 n,N

=

n{ µ, µ E -

µ, µ E }  0.

Thus,

by Cauchy-Schwarz inequality,

µ

2 E



µ, µ E 

µ E µ E , which implies that

µ E  µ E . Therefore,

µ - µ n,N  n(nN )-1An µ E .

(A.25)

Combining (A.21) and (A.25) yields that

µ - µ   An µ - µ n,N  n(nN )-1AnAn µ E .

By Lemma A.2, we have

µ E = C1{ o 2, +

a1 z1

a2 z2

(

o

-

µ

)

}  C2(

o

2, + |

|d-1 o d+1,).

a1+a2=2

It follows

µ - µ  = n(nN )-1AnAnC2( o 2, + | |d-1 o d+1,).

(A.26)

Next we derive the order of An and An. By Markov's inequality, for any g  G(p+1), g   C| |-1 g , g E  C| |-2 g . Equation (A.8) implies that

sup { g n,N / g }  1 - OP (log n)1/2n-1/2 + N -1/2| |-1 1/2 .
gG( )

33

Thus, we have
An  C| |-1 1 - OP (log n)1/2n-1/2 + N -1/2| |-1 -1/2 = OP | |-1 , An  C| |-2 1 - OP (log n)1/2n-1/2 + N -1/2| |-1 -1/2 = OP | |-2 .

Plugging the order of An and An into (A.26) yields that

µ - µ  = OP

C2n nN | |3

(

o

2, + |

|d-1 o d+1,)

.

Hence,

µ - o   C1| |d+1 o d+1, + OP

C2n nN | |3

o 2, + | |d-1 o d+1,

.

Therefore, Lemma A.10 is established.

Lemma A.11. Suppose Assumptions (A2)­(A5) hold and N 1/2| |   as N  , then  2 = OP (n-1| |-2).

Proof.

Note

that



=

-n,1

1 nN

n i=1

N j=1

Xi  B(zj)

to Lemma A.9,

 k=1

1k/2

ik

k

(zj

).

According

 2

1

nN

n2N 2| |4 i,i =1 j,j =1



×

1k/2 ik k (z j )

k=1

Xi  B(zj) Xi  B(zj )


k1/2i kk(zj ).
k=1

Note that


Xi  B(zj) 1k/2ikk(zj)
k=1





= Xi0B(zj)

1k/2ikk(zj), . . . , XipB(zj)

1k/2ikk(zj) ,

k=1

k=1

so one has

 2

1

p

n2N 2| |4

n

N



Xi Xi B(zj) B(zj )

(kk )1/2ikk(zj)i k k (zj ).

=0 i,i =1 j,j =1

k,k =1

34

Because the eigenvalues of Q2Q2 are either 0 or 1, under Assumptions (A2) and (A3), for any , i,

1NN E
N2 j=1 j =1



Xi2 B(zj) B(zj )

(kk )1/2ikk(zj)ik k (zj )

k,k =1

1NN

C N2

Bm(zj)Bm(zj )G(zj, zj ).

mM

j=1 j =1

Assumption (A4) and (A.5) imply that

1

N 2 Bm(zj)Bm(zj )G(zj, zj ) = j=j

G(z, z )Bm(z)Bm(z )dzdz
Tm×Tm

× {1 + O(N -1/2| |3)} = O(| |4).

Thus,

1N N2

N
EXi2 B(zj) B(zj )


(kk )1/2ikk(zj)ik k (zj )  C|

|2.

j=1 j =1

k,k =1

Next for any , i = i , j, j , we have



E Xi Xi B(zj) B(zj )

(kk )1/2ikk(zj)ik k (zj )

k,k =1

= E(Xi Xi )

Bm2 (zj)Bm2 (zj ) E (kk )1/2iki k k(zj)k (zj ) = 0.

mM

k,k

Therefore, E  2  Cp(n-1| |-2). The conclusion of the lemma follows.
Lemma A.12. Suppose Assumptions (A2)­(A5) hold and N 1/2| |   as N  , then  2 = OP (n-1N -1| |-4).

Proof. By the definition of  in (A.37), we have



2=

1 n2N 2|

|4

|

n
|-2n, -1

N

Xi  B(zj)

i=1 j=1

(z j )ij

nN

× | |-2n, -1

Xi  B(zj ) (zj )ij .

i=1 j =1

35

By Lemma A.9,

 2

1

nN

n2N 2| |4

Xi  B(zj)

i,i =1 j,j =1

(zj)ij Xi  B(zj ) (zj )i j .

Note that

XiB(zj)(zj)ij = Xi0B(zj) (zj)ij, Xi1B(zj) (zj)ij, . . . , XipB(zj) (zj)ij ,

so one has  2

1

pn N

n2N 2| |4

Xi Xi B(zj) B(zj )(zj)(zj )iji j.

=0 i,i =1 j,j =1

Because the eigenvalues of Q2Q2 are either 0 or 1, under Assumption (A2), for any , i, by (A.6),

1 N

N
B(z j )

B(zj)2(zj) = B(zj) Q2Q2 B(zj)2(zj)  C

1 N

N

Bm2 (zj)2(zj)

j=1

mM j=1

C

2(z)Bm2 (z)dz{1 + O(N -1/2| |-1)}  C.

mM T m/d

Next note that for any , i, j = j , E{Xi2 B(zj) B(zj )ijij } = 0, and for any , i = i , j, j , E{Xi Xi B(zj) B(zj )(zj)(zj )iji j } = 0. Therefore,

E  2

1 nN |

|4

p

E

(Xi2

)

1 N

N
B(zj) B(zj)2(zj)  Cp(nN )-1|

|-4.

=0

j=1

The conclusion of the lemma follows.

Proof of Theorem 1. By Lemma A.11, Lemma A.12, and the properties of the

bivariate spline basis functions in Lemma A.1,



2 L2

| |2 , 2 = OP (n-1) and



2 L2

| |2 , 2 = OP (n-1N -1| |-2), for any = 0, 1, . . . , p. It is clear that



-o

2 L2



µ,

-o

2 L2

+



2 L2

+



2 L2

,

where

the

asymptotic

order

of

µ, - o L2

is the same as µ, - o . The desired result follows from Lemma A.10.

Lemma A.13. Under Assumptions (A1)­(A6), if for any = 0, 1, . . . , p, |Xi |  C <
, then as N   and n  , one has for any vector a = (a0 , . . . , ap ) with a a = 1, [Var{a ( + )}]-1/2{a ( + )} -L N (0, 1), where  and  are given in (A.37).

36

Proof. For coefficient vectors ,  and the matrix n, defined in (A.17), Var{a ( + )} = a {E( ) + E( )}a. Denote  = (, , ) , and  = ( , , ) , , with

, , , ,

1 nNN

= n2N 2

Xi Xi B(zj)B (zj )G(zj, zj ),

i=1 j=1 j =1

1n =
n2N 2

N
Xi Xi B(zj)B (zj)2(zj),

i=1 j=1

then, we have

a

E( )a =Ea

-n,1

1 n2N

2

n

N

Xi  B(zj)

i=1 j,j =1

=Ea -n,1-n,1a,

a

E( )a =Ea

-n,1

1 n2N

2

n

N

Xi  B(zj)

i=1 j=1

=Ea -n,1-n,1a.

Xi  B(zj ) G(zj, zj )-n,1a Xi  B(zj) 2(zj)-n,1a

Note that for any vector a with a a = 1, we can rewrite as a (+) =

n i=1

ai +zi,

where

(ai +)2 =a

-n,1

1 n2N

2

N

N

Xi  B(zj)

j=1 j =1

+a

-n,1

1 n2N

2

N

Xi  B(zj)

j=1

Xi  B(zj ) G(zj, zj )-n,1a Xi  B(zj) 2(zj)-n,1a = (ai )2 + (ai )2,

and conditional on {Xi, i = 1, . . . , n}, zi are independent with mean zero and variance

one. Thus,

ni=1(ai )2 = a -n,1-n,1a and

n i=1

(ai

)2

=

a

-n,1-n,1a.

According to Lemma A.9, Assumptions (A2) and (A4),

Ea -n,1-n,1a  c- 2

|

|2 + n nN |

|2

-2
Ea a,

where

1

p nNN

a a = n2N 2

Xi Xi a B(zj)B(zj ) a G(zj, zj )

, =0 i=1 j=1 j =1

1 n =
n2 k=1 i=1

1p N

N

2

k1/2Xi g (zj)k(zj)

=0 j=1

37

with g (z) = a B(z). Therefore, by Assumption (A3), we have

1 p Ea a = n
k=1 =0

1 N

N

1k/2g (zj)k(zj)

2

j=1

c pNN

 nN 2

g (zj)g (zj )G(zj, zj )

=0 j=1 j =1

1p

n

=0

g (z)g (z )G(z, z )dzdz .
2

Noting that the eigenvalues of G are strictly positive, we have

p

Ea a  c1n-1

g2(z)dz  c2n-1| |2 a 2.

=0 

-2

Therefore, we have Ea -n,1-n,1a  cn-1

1

+

n nN |

|4

| |-2. Similarly, one can

-2

show that Ea -n,1-n,1a  c(nN )-1

1

+

n nN |

|4

| |-2. In addition,

max(ai )2



|

C |4 a

1 n2N 2

N

N

(XiXi )  B(zj)B(zj )

j=1 j =1

G(zj, zj )a

C 
| |4

p

1

N

n2N 2

max i

|Xi

Xi

|

N
g (zj)g (zj )G(zj, zj )  Cn-2|

|-2,

, =1

j=1 j =1

max(ai )2



|

C a
|4

1 n2N 2

N

(XiXi )  B(zj)B(zj)

j=1

2 (z j )a

C

 |

|4

p

1

n2N 2

max i

|Xi

Xi

|

N

g (zj)g

(zj )2(zj)  Cn-2N -1|

|-2.

, =1

j=1

Thus, if nn-1N -1| |-4  0, we have

max1in(ai + ai )2 ni=1(ai + ai )2



C n-1

1 + n nN |

|4

2
 0,

which satisfies the Lindeberg condition.

Theorem A.6. Under Assumptions (A1)­(A6), if for any = 0, 1, . . . , p, |Xi |  C < , supz[Var{ (z)}]-1/2(µ, (z) - o(z)) = oP (1), for = 0, . . . , p.

38

Proof. Using similar arguments as in the proof of Lemma A.13 and the result of Lemma
A.9, we have for any a = 1, Ea -n,1-n,1a  C-2| |-4Ea a  Cn-1| |-2, and Ea -n,1-n,1a  C-2| |-4Ea a  C(nN )-1| |-2. Therefore, based on the proof of Lemma A.13, for any a = 1,

cn-1|

|-4

1 + n nN | |4

-2
 Ea -n,1-n,1a  Cn-1|

|-2,

c(nN )-1

1 + n nN | |4

-2
|

|-2  Ea -n,1-n,1a  C(nN )-1|

|-2.

Thus,

Var( ) = {e  B(z)} E{-n,1( + )-n,1}{e  B(z)}

{e  B(z)} E(-n,1-n,1){e  B(z)} {e  B(z)}

×E

-n,1

1 n2N

2

n

N

Xi  B(zj)

i=1 j,j =1

Xi  B(zj )

G(zj, zj )-n,1

{e  B(z)}.

By Lemma A.9, we have

Var( ) Var( )

1

NN

nN 2| |2

{e  B(z)} B(zj)B(zj ) {e  B(z)}G(zj, zj ),

j=1 j =1

1

NN

nN 2| |2

{e  B(z)} B(zj)B(zj ) {e  B(z)}G(zj, zj )

j=1 j =1

× 1 + n

-2
,

nN | |4

-2

and according to Lemmas A.1 and A.4, we have cn-1

1

+

n nN |

|4

 Var( )  Cn-1.

According to Lemma A.10, if nn-1/2N -1| |-3  0 and n1/2| |d+1  0, the bias term

in (A.19) is negligible compared to the order of [Var{ (z)}]1/2.

Proof of Theorem 2. Theorem 2 follows from (A.19), Lemma A.13 and Theorem A.6.

A.4. Asymptotic properties of piecewise constant spline estimators
In this section, we study the asymptotic properties of the piecewise constant spline estimators defined in the spline space PC( ). Define piecewise constant bivariate spline

39

functions

c
µ(z)

=

(µc,0(z),

.

.

.

,

µc,p(z))

= Vm-1(z)

1 nN

nN

Bm(z) (z j )Xi

i=1 j=1

p

p

o (zj)Xi

,

=0

=0

(A.27)

(z) = (0(z), . . . , p(z)) = Vm-1(z) (z) = (0(z), . . . , p(z)) = Vm-1(z)

1 nN



nN

Bm(z) (z j )Xi

ik k (z j )

i=1 j=1

k=1

1 nN

nN

Bm(z)(zj)Xi ij

i=1 j=1

p
,
=0

p
,
=0
(A.28)
(A.29)

where Vm(z) is defined in (2.7). The next two theorems concern the functions µc, (z),  (z),  (z), = 0, . . . , p, given
in (A.27), (A.28) and (A.29). Theorem A.7 gives the uniform convergence rate of µ, (z) to o(z).

Theorem A.7. Under Assumptions (A1 ), (A2)­(A6), the constant spline functions µc, (z), = 0, . . . , p, satisfy supz sup0 p µc, (z) - o(z) = OP (| |).
In the following, we provide detailed proofs of Theorems A.7. For the random matrix Vm defined in (2.7), the lemma below shows that its inverse can be approximated by the inverse of a deterministic matrix A-m1-X1, where Am =  Bm(z)dz.
Lemma A.14. Under Assumptions (A3) and (A5), for any m  M, we have

Vm-1 = A-m1-X1 + OP n-1/2| |2(log n)1/2 + N -1/2| |) .

(A.30)

Proof. By Lemma A.5, Vm - AmX = OP n-1/2| |2(log n)1/2 + N -1/2| |) . Us
ing the fact that for any matrices A and B, (A + B)-1 = A-1 - A-1BA-1 + O(2), we obtain (A.30).

Proof of Theorem A.7. According to Lemma A.2, there exist functions   PC( )

that satisfies  - o  = O(| |) for = 0, 1, . . . , p. By the definition of µ, (z) in

c
(A.27), µ(z) =

µc,0(z), µc,1(z), . . . , µc,p(z)

= m(z),0, . . . , m(z),p

= m(z), where

m = Vm-1 (nN )-1

n i=1

N j=1

Bm

(zj

)Xi

Let

p =0 o (zj)Xi

p
for Vm defined in (2.7).
=0

(z) = (0(z), 1(z), . . . , p(z))

= Vm-1(z)

1 nN

n

N
Bm(z) (z j )Xi

p
 (zj)Xi

p

,

i=1 j=1

=0

=0

40

then

c
µ(z)

-

(z)

=

Vm-1(z)

1n nN

N
Bm(z) (z j )Xi

i=1 j=1

p
{o (zj) -  (zj)} Xi
=0

p
.
=0

Observing that    as   PC( ), µc, (z) = µc, (z) -  (z) + (z), = 0, 1, . . . , p. It is easy to see µc, -   = OP (| |). Hence, for = 0, 1, . . . , p, µc, - o  
µc, - + o-  = OP (| |), which completes the proof.

By Lemma A.14, the inverse of the random matrix Vm can be approximated by that

of a deterministic matrix AmX. Substituting Vm with AmX in (A.28) and (A.29),

we define the random vectors

(z) = (0(z), . . . , p(z)) = A-m1(z)-X1

1 nN



nN

Bm(z) (z j )Xi

ik k (z j )

i=1 j=1

k=1

p
,
=0

(A.31)

(z) = (0(z), . . . , p(z)) = A-m1(z)-X1

1 nN

nN

Bm(z)(zj)Xi ij

i=1 j=1

p
.
=0

(A.32)

The next lemma implies that the difference between (z) and (z) and the differ-

ence between (z) and (z) are both negligible uniformly over z  .

Lemma A.15. Under Assumptions (A2)­(A5) and (C1), if N 1/2| |   as N  ,

 k=1

1k/2k

 <  and n1/(4+2)

n1/2N -1/2| |-1 for some 2, then for (z), (z)

given in (A.28), (A.29) and (z), (z) given in (A.31), (A.32), as N   and

n  , we have

sup (z) - (z)  = OP n-1| |4 log(n) + n-1/2N -1/2| |3(log n)1/2 ,
z

(A.33)

sup (z) - (z)  = OP n-1N -1/2| |3 log(n) + n-1/2N -1| |2(log n)1/2 . (A.34)
z

Proof. Comparing (z) and (z) given in (A.28) and (A.31), we have

(z) - (z) = Vm-1 - A-m1(z)-X1

1 nN



nN

Bm(z) (z j )Xi

ik k (z j )

i=1 j=1

k=1

p
.
=0

Now let i,m,  i = n-1 Xi



1

k=1 N

N j=1

Bm(zj

)k

(zj

)

ik , then it is easy to

see

that

1 nN

n i=1

N j=1

Bm(z

j

)Xi

 k=1

ik

k

(zj

)

=

1 N

n i=1

i,m,

.

It

is

easy

to

see

that

E(i) = 0, and

2i,n = E i2 = n-2E(X2)

G (u, v) dudv{1 + O(N -1/2| |-1)}.

Tm×Tm

41

Note that

-i,1ni

n are uncorrelated random variables with mean 0. Assume that
i=1

| |-2 n for some 0 <  < , we can show that for any large enough  > 0,

P|

n i=1

i|





{C

log(n)n-1|

|4E(Xi2 )}1/2  2n-2- . Therefore,


P
n=1

n

sup

i,m,

mM,0 p i=1

 n-1/2|

|2(log n)1/2

< .

Thus, supm, |

n i=1

i,m,

|

=

OP

n-1/2|

|2(log n)1/2 as n   by Borel-Cantelli Lemma.

It follows that supm, |n-1

n i=1

i,m,

|

=

OP

n-1/2|

|2(log n)1/2 . Finally, according to

(A.30), we obtain (A.33). The result in (A.34) can be proved similarly.

Lemma A.16. For any z  , the covariance matrices of (z) and (z) are

(z) = E

(z) (z)

=

A-m2(z)-X1

1 nN 2



k

k=1

N

2

Bm(z)(zj)k(zj) ,

j=1

(z) = E

(z) (z)

=

A-m2(z)-X1

1 nN

2

N

Bm2 (z)(zj)2(zj),

j=1

in addition,

sup (z) + (z) - n(z)  = O(n-1N -1/2| |-1),
z

(A.35)

where n(z) is given in (2.9).

Proof. Note that A2m(z)(z) (z) is equal to

-X1

1 nN



nN

n2N 2

Bm(z) (z j )Xi

ik k (z j )

Bm(z)(zj )Xi

i=1 j=1

k=1

i =1 j =1


i k k (zj )
k =1

p
-X1.
, =0

Thus,

(z) = E (z) (z)

=

A-m2(z)-X1

1 nN 2



k

k=1

N

2

Bm(z)(zj)k(zj) .

j=1

Similarly,

we

can

derive

the

covariance

of

(z):

(z)

=

A-m2(z)

-X1

1 nN

2

N j=1

Bm2 (z)

(zj

)2(zj

).

Observe that


k
k=1

1N

N

Bm(z) (z j )k (z j )

j=1

2

1NN

= N2

G(zj, zj )Bm(z)(zj)Bm(z)(zj ).

j=1 j =1

42

Hence, by (A.5) and (A.6) in Lemma A.4, (A.35) holds. Therefore,

(z) + (z) =(nA2m(z))-1-X1

G (u, v) dudv{1 + O(N -1/2| |-1)}

Tm(z) ×Tm(z)

+ (nN A2m(z))-1-X1

2(u)du{1 + O(N -1/2|

Tm(z)

=n-1-X1G (z, z) {1 + O(N -1/2| |-1)}.

|-1)}

Therefore, supz (z) + (z) - n-1-X1G (z, z)  = O(n-1N -1/2| |-1). The desired result in (A.35) follows.

Proof of Theorem 3. Note that, for any vector a = (a0, . . . , ap)  R(p+1), we have

E[

p =0

a

{(z) + (z)}] = 0,

and

p
a (z) = a
=0 p
a (z) = a
=0

A-m1(z)-X1 n nN

N



n

Bm(z) (z j )

ikk(zj)Xi =

a A-m1(z)-X1zi ,

i=1 j=1

k=1

i=1

A-m1(z)-X1 n nN

N

n

Bm(z)(zj)ijXi =

a A-m1(z)-X1zi ,

i=1 j=1

i=1

where

zi

=

1 nN

N j=1

Bm(z)

(zj

)

 k=1

ik

k

(zj

)Xi

and

zi

=

1 nN

N j=1

Bm(z)

(zj

)ij

Xi

are

independent

sequences

with

variances

Var(zi )

=

1 n2N 2

j,j Bm(z)(zj)Bm(z)(zj)G(zj, zj )X

and

Var(zi )

=

1 n2N 2

N j=1

Bm(z) (z j )Bm(z) (z j

)2 (z j )X

,

respectively.

Therefore,

we have

Var a A-m1(z)-X1zi Var a A-m1(z)-X1zi

1 =a
n

(z)a

=

A-m2(z) n2N 2

N

Bm(z)(zj)Bm(z)(zj )G(zj, zj )a -X1a,

j,j =1

1 =a
n

(z)a

=

A-m2(z) n2N 2

N

Bm(z) (z j )Bm(z) (z j )2 (z j )a

-X1a.

j=1

Using central limit theorem, we have

p
a {(z) + (z)} a -1/2 a {(z) + (z)} -L N (0, 1).
=0

By (A.35), as N   and n  , {a n(z)a}-1/2

p =0

a

{(z) + (z)}

-L

N (0, 1). Therefore, {a n(z)a}-1/2

p =0

a

{c(z)

-

o(z)}

-L

N (0, 1)

follows

from

(A.18), Theorem A.7, Lemma A.15 and Slutsky's Theorem. Applying Cram´er-Wold's

device, we obtain -n 1/2(z){c(z) - o(z)}p=0 -L N 0, I(p+1)×(p+1) , as N   and n  , and consequently, n-,1 (z){c(z) - o(z)} -L N (0, 1), for any z   and

= 0, . . . , p.

43

A.5. Convergence of the covariance estimator

For any i = 1, . . . , n, and estimated residuals Rij = Yij -

p =0

Xi



(z j ),

denote

i

=

arg min

N j=1

Rij - B (zj)Q,2

2
, where B(z) is the set of bivariate spline basis

functions used to estimate i(z), and Q,2 is given in the following QR decomposition of

the transpose of the smoothness matrix H:

H

= QR = (Q,1 Q,2)

R,1 R,2

.

Then, the

bivariate spline estimator of i(z) can be written as i(z) = B(z) Q,2i = B(z) i.

Let

1N

n = N

B(zj)B (zj),

j=1

then we have

i

=

-n 1

1 N

N

B (z j )Rij

j=1

=

-n 1

1 N

N

B (z j )

j=1

p
Xi {o(zj) -  (zj)} + i(zj) + (zj)ij .
=0

(A.36)

Lemma A.17. Under Assumptions (A3)­(A5), if (N 1/2| |)/ log(| |-1)   as N  , then there exist constants 0 < c < C < , such that with probability approaching 1 as N  , n  , c| |2  min(n)  max(n)  C| |2.

The proof is similar to the proof of A.7, thus omitted.

Next we define

bi(z) = B(z)

-n 1

1 N

N

B (z j )

p

Xi {o(zj) -  (zj)},

j=1

=0

(A.37)

i(z) = B(z)

-n 1

1 N

N

B (z j )i (z j ),

i(z) = B(z)

-n 1

1 N

N

B (z j )(z j )ij .

j=1

j=1

Then, the estimation error Di(z) = i(z) - i(z) in (3.1) can be decomposed as the following:
Di(z) = bi(z) + i(z) + i(z).

For any z, z  , denote
n
G(z, z ) = n-1 i(z)i(z ).
i=1

The following lemma shows the uniform convergence of G(z, z ) to G(z, z ) in probability over all (z, z )  2.

44

Lemma A.18. Under Assumptions (A1)­(A5) and (C1)­(C3), sup(z,z )2 |G(z, z ) - G(z, z )| = OP {n-1/2(log n)1/2}.

Proof. Let ¯·kk = n-1

n i=1

ik ik

,

then


G(z, z ) - G(z, z ) = kk(z)k(z ) ¯·kk - 1 +

¯·kk (kk )1/2k(z)k (z ).

k=1

k=k

As E

 k=1

kk(z)k(z

)

¯·kk - 1

= 0, then E{G(z, z ) - G(z, z )} = 0. Note

that E{2(z)2(z )} = G(z, z)G(z , z ) + 2G2(z, z ) +

 k=1

2k E (14k

-

3)k2(z)k2(z

).

Next,

2

1n

2

E G(z, z ) - G(z, z )

=E n

i(z)i(z ) - G(z, z )

i=1

1 =
n


G(z, z)G(z , z ) + G2(z, z ) + 2kE(14k - 3)k2(z)k2(z )

.

k=1

2

Therefore, E G(z, z ) - G(z, z )

n-1. Hence, following from Bernstein inequal-

ity, sup(z,z )2 G(z, z ) - G(z, z ) = OP {n-1/2(log n)1/2}, and the desired result follows.

Proof of Theorem 4. Note that

sup |G(z, z )-G(z, z )|  sup {|G(z, z )-G(z, z )|+|G(z, z )-G(z, z )|},

(z,z )2

(z,z )2

where sup(z,z )2 |G(z, z ) - G(z, z )| = oP (1) according to Lemma A.18, and

n

sup |G(z, z ) - G(z, z )|  sup n-1 i(z)Di(z )

(z,z )2

(z,z )2

i=1

n

n

+ sup n-1 i(z )Di(z) + sup n-1 Di(z)Di(z ) .

(z,z )2

i=1

(z,z )2

i=1

With some simple calculations, we have

n

n

n

n

i(z)Di(z ) = i(z)bi(z ) + i(z)i(z ) + i(z)i(z ),

i=1

i=1

i=1

i=1

where i = i - i. According to (A.39), (A.42) and (A.47), we have

n

n

sup n-1 i(z)Di(z ) + n-1 i(z )Di(z) = oP (1).

(z,z )2

i=1

i=1

45

Note that

n

n

n

n

n

Di(z)Di(z ) = bi(z)bi(z ) + i(z)i(z ) + bi(z)i(z ) + i(z)i(z )

i=1

i=1

i=1

i=1

i=1

n

n

+ i(z)i(z ) + bi(z)i(z ).

i=1

i=1

It follows from (A.38), (A.41), (A.43)­(A.46) that sup(z,z )2 |n-1

n i=1

Di(z)Di(z

)|

=

oP (1). The desired result is established.

Lemma A.19. Under Assumptions (A1)­(A5), (C1)­(C3), we have

n

sup n-1 bi(z)bi(z ) = OP {n-1| |-2(log n)1/2},

(z,z )2

i=1

n

sup

i(z)bi(z ) = OP {n-1(log n)1/2}.

(z,z )2 i=1

(A.38) (A.39)

Proof. According to (A.19) and (A.37), we have

bi(z) = B(z)

-n 1

1 N

N

B(z j )

p

Xi {o(zj) -  (zj)}

j=1

=0

= B(z)

-n 1

1 N

N

B(z j )

p

Xi {o(zj) - µ, (zj) -  (zj) -  (zj)}.

j=1

=0

(A.40)

Thus,

1 n

n

1

bi(z)bi(z ) = n

n
B(z) -n 1

1 N

N
B(z j )

p

Xi {o(zj) -  (zj)}

i=1

i=1

j=1

=0

1N

× N

B(zj )

j =1

p
Xi {o (zj ) -  (zj )} -n 1B(z )
=0

1

n

n|

 |4

B (z )
i=1

1N

N2

B(zj)B(zj )

p
Xi Xi {o(zj) -  (zj)}

j,j =1

, =0

× {o (zj ) -  (zj )} B(z ).

Therefore, by Theorem 1, we have

1n

E n

bi(z)bi(z )

i=1

pp
| |-2 o - 
=0 =0

o - 

46

n-1| |-2.

We have E n-1

n i=1

bi(z)bi(z

)

2

=

1 n2

n i,i

=1

E

bi(z)bi(z )bi (z)bi (z ) , where

E bi(z)bi(z )bi (z)bi (z ) | |-8

× EB(z)

1N

N2

B(zj)B(zj )

p
Xi Xi {o(zj) -  (zj)}{o (zj ) -  (zj )} B(z )

j,j =1

, =0

× B(z)

1N

N2

B(zj)B(zj )

p
Xi Xi {o(zj) -  (zj)}{o (zj ) -  (zj )} B(z )

j,j =1

, =0

n-2| |-4.

Thus, (A.38) follows from the Bernstein inequality after the discretization. Following from (A.40), we have, for any i, i = 1, . . . , n,

E bi(z )bi (z )i(z)i (z)

|

|-4B(z )

1 N2

N

B(zj)B(zj ) E

j,j =1

p
Xi Xi  (zj) (zj )i(z)i (z) B(z ),
, =0

E

p

1

Xi Xi  (zj) (zj )i(z)i (z)

= n2N 2

n
E

Xi  B(zj)

-n,1

, =0

i ,i =1

N

×

Xi Xi  B(zj )B(zj ) -n,1Xi  B(zj ) E {i(z)i (z)i (zj )i (zj )}

j ,j =1

1 = n2N 2 E

Xi  B(zj)

N

-n,1

Xi Xi  B(zj )B(zj ) -n,1 Xi  B(zj )

j ,j =1

× E {i(z)i (z)i(zj )i (zj ) + i(z)i (z)i (zj )i(zj )} n-2.

Therefore, E

1 n

n i=1

i(z)bi(z

)

2

=

1 n2

n i,i

=1

E{bi(z

)bi

(z

)i(z)i

(z)}

=

O(n-2).

Lemma A.20. Under Assumptions (A1)­(A5), (C1)­(C3), we have

n

sup n-1 i(z)i(z ) = OP

(z,z )2

i=1

Kn



| |2(s+1)

k

k

2 s+1,

+

k

k

2 

,

k=1

k=Kn+1

(A.41)

n

Kn



sup n-1 i(z)i(z ) = OP | |s+1 k k s+1, k  +

k

k

2 

,

(z,z )2

i=1

k=1

k=Kn+1

(A.42)

47

n

sup

i(z)bi(z ) = OP

(z,z )2 i=1

+ OP

Kn
(log n)1/2n-1| |s+1 k k s+1, k 

k=1



(log n)1/2n-1

k

k

2 

.

(A.43)

k=Kn+1

Proof.

For

any

k



1,

denote

k(z)

=

B(z)

-n 1

1 N

N j=1

B(zj

)k

(zj

),

and

k

=

k - k. According to Assumption (C2), we hav(C3)e, for any k  1, k  

C| |s+1 k s+1, and k   k  + k   2 k , as n  . It is easy to

see that i(z ) =

 k=1

1k/2 ik k (z

).

We first show (A.41). Let ¯·kk = n-1

n i=1

ik

ik

,

where

E(¯·kk

)

=

I (k

=

k

)

and

E(¯·kk )2



(Ei4kEi4k )1/2



C.

Simple

calculation

yields

that

1 n

n i=1

i(z)i(z

)

=

 k,k

=1 ¯·kk

(kk

)1/2k(z)k

(z

).

Thus,

by

Assumption

(C2),

we

have

sup E
(z,z )2

1n n i(z)i(z )
i=1



= sup

kk(z)k(z )

(z,z )2 k=1

Kn



 | |2(s+1)

k

k

2 s+1,

+

C

k k 2.

k=1

k=Kn+1

In addition, we have

2





sup E i(z)i(z )
z,z 

= sup E

i2kk(k)2(z) i2k k (k )2(z )

z,z 

k=1

k =1

Kn



2

n-1 | |2(s+1)

k

k

2 s+1,

+

k

k

2 

.

k=1

k=Kn+1

Thus,

1n

sup Var
z,z 

n i(z)i(z ) i=1

Kn



2

| |2(s+1)

k

k

2 s+1,

+

k

k

2 

.

k=1

k=Kn+1

Therefore, using the discretization method and Bernstein inequality

1n

sup
(z,z )2

n i(z)i(z ) - E{i(z)i(z )} i=1

Kn



= OP (log n)1/2n-1/2| |2(s+1)

k

k

2 s+1,

(log

n)1/2n-1/2

k

k

2 

.

k=1

k=Kn+1

48

Next we derive (A.42). Noting that n-1

n i=1

i(z)i(z

)

=

·kk

(kk

)1/2 k (z

)(k

)(z

),

we have

sup E
(z,z )2

1n n i(z)i(z )
i=1

n
var n-1 i(z)i(z )
i=1



 k k  k 

k=1

Kn



 C| |s+1 k k s+1, k  +

k k 2,

k=1

k=Kn+1

= n-1 E i2(z)i(z )2 - {Ei(z)i(z )}2 ,

sup E
z,z 

i2(z)i(z )2



= sup

Ei4k2kk2(z)(k)2(z ) + kk k2(z)(k)2(z )

z,z  k=1

k=k

Kn



 C | |2(s+1)

k

k

2 s+1,

+

k

k

2 

,

k=1

k=Kn+1

and

Kn



sup |E {i(z)i(z )}|  C | |s+1 k k s+1, k  +

k

k

2 

.

z,z 

k=1

k=Kn+1

Therefore,

n

2

Kn



2

sup E n-1 i(z)i(z ) = O  | |s+1 k k s+1, k  +

k

k

2 

.

z,z 

i=1

k=1

k=Kn+1

Hence,

n

sup n-1 i(z)i(z ) - E {i(z)i(z )}

(z,z )2

i=1

Kn



= OP (log n)1/2n-1/2| |2(s+1)

k

k

2 s+1,

+

(log

n)1/2n-1/2

k

k

2 

k=1

k=Kn+1

using the discretization method and Bernstein inequality.

Finally, we provide the proof of (A.43). Note that

E bi(z )bi (z )i(z)i (z) | |-4

1N

× B(z ) N 2

B(zj)B(zj ) E

j,j =1

p
Xi Xi  (zj) (zj )i(z)i (z) B(z ),
, =0

49

and by (A.42),

E

p

1

Xi Xi  (zj) (zj )i(z)i (z)

= n2N 2

n
E

Xi  B(zj)

-n,1

, =0

i ,i =1

N

×

Xi Xi  B(zj )B(zj ) -n,1Xi  B(zj ) E {i (zj )i (zj )i(z)i (z)}

j ,j =1

1

=

E

n2N 2

Xi  B(zj)

N

-n,1

Xi Xi  B(zj )B(zj ) -n,1 Xi  B(zj )

j ,j =1

× E {i(zj )i (zj )i(z)i (z) + i (zj )i(zj )i(z)i (z)} .

If i = i , we have

E i(zj )i (zj )i(z)i (z) + i (zj )i(zj )i(z)i (z)

Kn



2

k| |s+1 k s+1, k  +

k

k

2 

.

k=1

k=Kn+1

If i = i , then we have

E Thus,


i(zj )i(zj )i(z)i(z) = 2kEi4kk(z )k(z )k(z)k(z)

k=1

Kn





2k |

|2s+2

k

2 s+1,

k

2 

+

2k k 4.

k=1

k=Kn+1

Therefore,

p

E

Xi Xi  (zj) (zj )i(z)i (z)

, =0

Kn



k| |s+1 k s+1, k  +

2

k

k

2 

.

k=1

k=Kn+1

1n

2

1n

E n

i(z)bi(z )

= n2

Ebi(z )bi (z )i(z)i (z)

i=1

i,i =1

Kn



= O n-2

2k |

|2s+2

k

2 s+1,

k

2 

+

n-2

2k

k

4 

.

k=1

k=Kn+1

Thus, (A.43) is obtained.

50

Lemma A.21. Under Assumptions (A1)­(A5), (C1)­(C3), we have

n

sup n-1 i(z)i(z ) = OP (N -1| |-2),

(z,z )2

i=1

(A.44)

n

sup n-1 i(z)i(z ) = OP

(z,z )2

i=1

n-1/2N -1/2(log n)1/2|

Kn

 |s

k1/2 k s+1,

k=1



+ OP n-1/2N -1/2| |-1(log n)1/2

k1/2 k  , (A.45)

k=Kn+1

n

sup n-1 bi(z)i(z ) = OP {n-1N -1| |-2(log n)1/2},

(z,z )2

i=1

(A.46)

n

sup n-1 i(z)i(z ) = OP {n-1/2N -1/2| |-1(log n)1/2}.

(z,z )2

i=1

(A.47)

Proof. We first show (A.44). Let ¯·jj = n-1

n i=1

ij ij

,

where

E(¯·jj )

=

I (j

=

j ).

Note that

1 n

n

i(z)i(z ) = B(z)

-n 1

i=1

1NN

N2

B(zj)B(zj ) (zj)(zj )¯·jj

j=1 j =1

-n 1B(z ).

It is easy to see that,

E

1n n i(z)i(z )

= B(z) -n 1

1 N2

N

B(zj)B(zj )

2 (z j )

-n 1B(z ).

i=1

j=1

Therefore, sup(z,z )2 E

1 n

n i=1

i(z)i(z

)

= O(N -1| |-2). In addition, note that

E {i(z)i(z )} =B(z) -n 1

1 N2

N

B(zj)B(zj )

2 (z j )

-n 1B(z ) = O(N -1| |-2),

j=1

E {i(z)i(z )}2 =E B(z) -n 1

1NN

N2

B(zj)B(zj ) (zj)(zj )ijij

j=1 j =1

2
-n 1B(z )

| |-8 N4

N

B(zj)B(zj ) B(zj )B(zj )

j,j ,j ,j =1

× (zj)(zj )(zj )(zj )ijij ij ij N -2| |-4.

Thus, var

1 n

n i=1

i(z)i(z

)

=

1 n2

n i=1

var

{i(z)i(z

)}

n-1N -2| |-4. Therefore,

n

sup n-1 i(z)i(z ) - E {i(z)i(z )} = OP {n-1/2N -1(log n)1/2| |-2}

(z,z )2

i=1

51

using the discretization method and Bernstein inequality. Next we derive (A.45). Note that

1n

1n

n i(z)i(z ) = n


ik1k/2k(z)B(z ) -n 1

1N

N

B (z j )(z j )ij

,

i=1

i=1 k=1

j=1

1n

2

1n

n

i(z)i(z )

= n2

n


iki k (kk )1/2k(z)

i=1

i=1 i =1 k=1 k =1

× k (z)B(z ) -n 1

1NN

N2

B(zj)B(zj ) (zj)(zj )iji j

j=1 j =1

-n 1B(z ).

Next observe that E

1 n

n i=1

i(z)i(z

)

= 0 and

E

1 n

n

i(z)(k)2(z)

1n =
n2


k(k)2(z)

i=1

i=1 k=1

× B(z ) -n 1

1 N2

N

B (z j )B (z j )

2 (z j )

-n 1B(z )

j=1

So,

E

1 n

n

i(z)(k)2(z)

 C1| |-2 nN

Kn

| |2(s+1)

k

k

2 s+1,

+



k

k

2 

.

i=1

k=1

k=Kn+1

Thirdly, we prove (A.46). Note that for any i, i , j, j , we have

E bi(z)ijbi (z)i j

=E

B (z )

-n 1

1 N2

N

B(zj )B(zj )

j ,j =1

= O(n-2N -2| |-4).

p
Xi  (zj )Xi  (zj )iji j -n 1B(z)
, =0

Therefore,

E bi(z)i(z )bi (z)i (z )

= B(z )

-n 1

1 N2

N
E bi(z)ijbi (z)i j

-n 1B(z )

j,j =1

= O(n-2N -2| |-4),

E

n-1

n

2
1 bi(z)i(z ) = n2

n

E

bi(z)i(z )bi (z)i (z )

= O(n-2N -2|

|-4).

i=1

i,i =1

52

Finally, we show (A.47). Note that

n

i(z)i(z ) =

n

B(z )

-n 1

1 N

N

B (z j )(z j )ij


ik 1k/2 k (z ),

i=1

i=1

j=1

k=1

where E {n-1

n i=1

i(z)i(z

)}

=

0,

and

n

2

E n-1 i(z)i(z ) = n-1E{i(z)2}E{i(z )2} = n-1G(z, z )B(z )

i=1

×

-n 1

1 N2

N
B(zj)B(zj) 2(zj)-n 1B(z ) = O(n-1N -1|

 |-2 ).

j=1

Thus, (A.47) is obtained.

Appendix B
In this section, we provide some additional results from simulation studies and real application analysis.
B.1. More results of simulation studies
In Section 5.1 of the main paper, we illustrated the advantage of the proposed method over the complex horseshoe domain in Sangalli et al. (2013). Figure B.1 shows the two triangulations used for the horseshoe domain in this example. For implementation, the BPST method is conducted over triangulation, 1, while triangulation, 2, is used for PCST method. To visually compare different methods, we display the estimated coefficient functions for Case I (jump function) and Case II (smooth function) in Figures B.2 and B.3, respectively. The plots are obtained based on the setting: n = 50, 1 = 0.2, 2 = 0.05,  = 1.0. Table B.1 summarizes the estimation results based on the noise level  = 1.0.
From these figures, one sees that the BPST and PCST estimates are both very close to the true coefficient functions. When the true coefficient functions are smooth, BPST provides the best estimation, while when the true coefficient function contains jumps, PCST provides a better estimation. The performance of the Tensor method will be affected by the design of the coefficient function. Moreover, from Figure B.2 and B.3, one can see that even when the coefficient function is smooth across the boundary, the estimation accuracy is also affected by the domain of the true signal, especially the pixels which are closed to the boundary. The performance of the Kernel method is not affected by the design of the coefficient functions, instead, it heavily depends on the noise level

53

due to the three-stage structure. As the noise level increases, the Kernel estimates are getting more blurred.

1

2

Figure B.1: Triangulations for the horseshoe domain.

TRUE

Tensor

Kernel

BPST ( 1) PCST ( 2)

2.5

2.5

2.5

2.5

2.5 2.5

0.0

0.0

0.0

0.0

0.0 0.0

-2.5

-2.5

-2.5

-2.5

-2.5 -2.5

0

2.5

2.5

2.5

2.5

2.5 2.5

2.0

2.0

2.0

2.0

2.0 2.0

1.5

1.5

1.5

1.5

1.5 1.5

1.0

1.0

1.0

1.0

1.0 1.0

0.5

0.5

0.5

0.5

0.5 0.5

0.0

0.0

0.0

0.0

0.0 0.0

1

Figure B.2: True coefficient functions and their different estimators for Case I in Example 1.

In Section 5.2 of the main paper, we conduct a simulation study based on the domain of the 5th slice of the brain images illustrated in Section 6. Table B.2 demonstrates the estimation results for  = 0.5. In this example, we focus on the domain of the 35th slices of the brain image. Based on this domain, we consider two types of triangulations: 5 and 6; see Figure B.4. Table B.3 summarizes the MSE results of the BPST, kernel and tensor methods. The findings are similar to those described in Section 5.2. Tables 5.3 and B.4 summarize the ECRs of the 95% SCCs for the 5th and 35th slices, respectively, and they are all close to 95%. As the sample size increases, the ECRs are getting closer to 95%. Figures B.5 and B.6 show the true coefficient functions and an example of their estimators and 95% SCCs based on the 5th and 35th slices, respectively. The plots are generated based on the setting: n = 50, 1 = 0.1, 2 = 0.02 and  = 0.5.
54

TRUE

Tensor

Kernel

BPST ( 1) PCST ( 2)

2.5

2.5

2.5

2.5

2.5 2.5

0.0

0.0

0.0

0.0

0.0 0.0

-2.5

-2.5

-2.5

-2.5

-2.5 -2.5

0

2.5

2.5

2.5

2.5

2.5 2.5

2.0

2.0

2.0

2.0

2.0 2.0

1.5

1.5

1.5

1.5

1.5 1.5

1.0

1.0

1.0

1.0

1.0 1.0

0.5

0.5

0.5

0.5

0.5 0.5

0.0

0.0

0.0

0.0

0.0 0.0

1

Figure B.3: True coefficient functions and their different estimators for Case II in Example 1.

3

4

5

6

Figure B.4: Triangulations for the fifth slice ( 3, 4) and 35th slice ( 5, 6) of the brain image in Simulation Example 2.

B.2. Additional ADNI data analysis results
For the ADNI data described in Section 6 in the main paper, Table B.5 below summarizes the distribution of patients by diagnosis status and sex. Next, Figure B.7 displays the triangulations of slices used for the BPST method in the model fitting and constructing the SCCs. Finally, Figures B.8 and B.9 provide the image maps of the estimated coefficient functions for eighth, 15th, 35th, 55th, 62nd, and 65th slices, and Figures B.10 and B.11 show the corresponding significance maps. The significance maps in the eighth and 15th slice show that the increase of age increases the brain activities in the cerebellum and temporal lobe, and people with the Alzheimer's disease are more active in the cerebellum, while less active in the temporal lobe. The significance maps of the 35th slide display that the age has a negative effect on the brain activities in the
55

Table B.1: Estimation errors of the coefficient estimators,  = 1.0.

Function Type

n

Method

1 = 0.03, 2 = 0.006

0

1

BPST 0.0059 0.0075

PCST 0.0023 50 Kernel 0.0201

0.0023 0.0206

Jump

Tensor 0.0201 BPST 0.0038

0.0132 0.0050

100

PCST Kernel

0.0011 0.0100

0.0011 0.0102

Tensor 0.0099 0.0112

BPST 0.0010 0.0012

50

PCST 0.0049 Kernel 0.0201

0.0065 0.0206

Smooth

Tensor 0.0189 BPST 0.0006

0.0132 0.0007

100

PCST Kernel

0.0037 0.0100

0.0054 0.0102

Tensor 0.0100 0.0113

1 = 0.2, 2 = 0.05

0

1

0.0066 0.0082

0.0028 0.0030

0.0207 0.0213

0.0206 0.0142

0.0042 0.0054

0.0014 0.0015

0.0104 0.0105

0.0103 0.0120

0.0016 0.0019

0.0054 0.0072

0.0207 0.0213

0.0207 0.0153

0.0009 0.0010

0.0040 0.0057

0.0104 0.0105

0.0103 0.0128

anterior cingulate gyrus, corpus callosum, and part of the cerebral white matter, while the female has a higher level of activities in these regions. These regions connect the left and right cerebral hemispheres and enabling communication between them. From the significance maps of the 55th, 62nd, and 65th slices, we could see an increase of brain activities in the frontal gyrus, precentral gyrus and postcentral gyrus for people with Alzheimer's disease. Our findings are consistent with the findings in the literature, see Andersen et al. (2012), Bernard and Seidler (2014), and Dubb et al. (2003).
References
Andersen, K., Andersen, B. B., and Pakkenberg, B. (2012), "Stereological quantification of the cerebellum in patients with Alzheimer's disease," Neurobiology of aging, 33, 197­e11.
Bernard, J. A. and Seidler, R. D. (2014), "Moving forward: age effects on the cerebellum underlie cognitive and motor declines," Neuroscience & Biobehavioral Reviews, 42, 193­207.
Cardot, H., Ferraty, F., and Sarda, P. (1999), "Functional linear model," Statistics & Probability Letters, 45, 11­22.

56

Table B.2: Estimation errors of the coefficient function estimators,  = 0.5.

n

Method

1 = 0.1, 2 = 0.02 1 = 0.2, 2 = 0.05

0

1

2

0

3

2

BPST( 3) 0.003 0.005 0.005 0.007 0.011 0.010

50

BPST( 4) 0.003 0.005 0.005 0.006 0.009 0.009 Kernel 0.008 0.011 0.011 0.011 0.016 0.016

Tensor 0.008 0.007 0.010 0.011 0.012 0.014

BPST( 3) 0.002 0.002 0.002 0.003 0.005 0.005

100

BPST( 4) Kernel

0.002 0.004

0.002 0.005

0.002 0.005

0.003 0.005

0.004 0.008

0.004 0.007

Tensor 0.004 0.005 0.005 0.005 0.007 0.009

-- (2003), "Spline estimators for the functional linear model," Statistica Sinica, 13, 571­591.
Chen, K., Delicado, P., and Mu¨ller, H.-G. (2017), "Modelling function-valued stochastic processes, with applications to fertility dynamics," Journal of the Royal Statistical Society: Series B (Statistical Methodology), 79, 177­196.
Claeskens, G., Krivobokova, T., and Opsomer, J. D. (2009), "Asymptotic properties of penalized spline estimators," Biometrika, 96, 529­544.
Corder, E., Saunders, A., Strittmatter, W., Schmechel, D., Gaskell, P., Small, G., Roses, A., Haines, J., and Pericak-Vance, M. (1993), "Gene dose of apolipoprotein E type 4 allele and the risk of Alzheimer's disease in late onset families," Science, 261, 921­923.
Dubb, A., Gur, R., Avants, B., and Gee, J. (2003), "Characterization of sexual dimorphism in the human corpus callosum," Neuroimage, 20, 512­519.
Gu, L., Wang, L., H¨ardle, W. K., and Yang, L. (2014), "A simultaneous confidence corridor for varying coefficient regression with sparse functional data," Test, 23, 806­ 843.
Hall, P. and Horowitz, J. (2013), "A simple bootstrap method for constructing nonparametric confidence bands for functions," The Annals of Statistics, 41, 1892­1921.
Hall, P. and Horowitz, J. L. (2007), "Methodology and convergence rates for functional linear regression," The Annals of Statistics, 35, 70­91.
Hall, P. and Opsomer, J. D. (2005), "Theory for penalised spline regression," Biometrika, 92, 105­118.
57

Table B.3: Estimation errors of the coefficient function estimators in the 35th slice.

n



Method

1 = 0.1, 2 = 0.02 1 = 0.2, 2 = 0.05

0

1

2

0

1

2

BPST( 5) 0.003 0.005 0.005 0.007 0.011 0.011

0.5

BPST( 6) Kernel

0.003 0.008

0.005 0.012

0.005 0.012

0.007 0.018

0.011 0.018

0.010 0.017

Tensor 0.008 0.009 0.011 0.012 0.015 0.015

50

BPST( 5) 0.003 0.005 0.005 0.007 0.011 0.011

1.0

BPST( 6) Kernel

0.003 0.023

0.005 0.033

0.005 0.033

0.007 0.027

0.011 0.039

0.011 0.038

Tensor 0.023 0.012 0.019 0.027 0.017 0.023

BPST( 5) 0.002 0.002 0.002 0.003 0.005 0.005

0.5

BPST( 6) Kernel

0.002 0.004

0.002 0.006

0.002 0.006

0.003 0.006

0.005 0.008

0.005 0.008

100

Tensor 0.004 0.006 0.007 0.006 0.010 0.009 BPST( 5) 0.002 0.002 0.002 0.003 0.005 0.005

1.0

BPST( 6) Kernel

0.002 0.012

0.002 0.016

0.002 0.016

0.003 0.013

0.005 0.018

0.005 0.018

Tensor 0.013 0.010 0.013 0.011 0.007 0.011

Hibar, D. P., Stein, J. L., Renteria, M. E., Arias-Vasquez, A., Desrivi`eres, S., Jahanshad, N., et al. (2015), "Common genetic variants influence human subcortical brain structures," Nature, 520, 224­229.
Huang, J. Z., Wu, C. O., and Zhou, L. (2004), "Polynomial spline estimation and inference for varying coefficient models with longitudinal data," Statistica Sinica, 14, 763­788.
Lai, M.-J. and Schumaker, L. L. (2007), Spline functions on triangulations, Cambridge University Press.
Lai, M. J. and Wang, L. (2013), "Bivariate penalized splines for regression." Statistica Sinica, 23, 1399­1417.
Li, L. and Zhang, X. (2017), "Parsimonious tensor response regression," Journal of the American Statistical Association, 112, 1131­1146.
Li, Y. and Hsing, T. (2010), "Uniform convergence rates for nonparametric regression and principal component analysis in functional/longitudinal data," The Annals of Statistics, 38, 3321­3351.

58

Table B.4: The coverage rate of the 95% SCCs for the coefficient functions defined over the 35th slice.

Coverage

Width

n





0

1

2

0

1

2

50

(0.1,0.02)

0.5 1.0

0.962 0.964

0.916 0.926

0.934 0.940

0.307 0.331

0.344 0.368

0.347 0.371

(0.2,0.05)

0.5 1.0

0.952 0.96

0.920 0.920

0.930 0.934

0.426 0.449

0.490 0.512

0.492 0.512

0.5 0.956 0.952 0.940 0.214 0.240 0.244

100

(0.1,0.02) (0.2,0.05)

1.0 0.5 1.0

0.962 0.946 0.952

0.952 0.954 0.954

0.948 0.932 0.938

0.239 0.298 0.317

0.262 0.340 0.359

0.265 0.346 0.365

Table B.5: Distribution of patients by diagnosis status and gender.

CN MCI AD All

Male 70 136 72 278 Female 42 77 50 169

All

112 213 122 447

Lindgren, F., Rue, H., and Lindstr¨om, J. (2011), "An explicit link between Gaussian fields and Gaussian Markov random fields: the stochastic partial differential equation approach," Journal of the Royal Statistical Society: Series B (Statistical Methodology), 73, 423­498.
Marcus, C., Mena, E., and Subramaniam, R. M. (2014), "Brain PET in the diagnosis of Alzheimer's disease," Clinical Nuclear Medicine, 39, e413­e422.
Morris, J. S. (2015), "Functional regression," Annual Review of Statistics and Its Application, 2, 321­359.
Morris, J. S. and Carroll, R. J. (2006), "Wavelet-based functional mixed models," Journal of the Royal Statistical Society: Series B (Statistical Methodology), 68, 179­199.
Mu¨ller, H.-G. (2005), "Functional modelling and classification of longitudinal data," Scandinavian Journal of Statistics, 32, 223­240.
Persson, P. O. and Strang, G. (2004), "A simple mesh generator in MATLAB." SIAM Review, 46, 329­345.

59

TRUE

Kernel
1.5 1.0 0.5 0.0

Tensor
1.5 1.0 0.5 0.0

BPST( 1) BPST( 2) Lower SCC Upper SCC

1.5

1.5

1.5

1.5

1.5 1.5

1.0

1.0

1.0

1.0

1.0 1.0

0.5

0.5

0.5

0.5

0.5 0.5

0.0

0.0

0.0

0.0

0.0 0.0

0

1.5

1.5

1.5

1.5

1.5

1.5

1.0

1.0

1.0

1.0

1.0

1.0

0.5

0.5

0.5

0.5

0.5

0.5

0.0

0.0

0.0

0.0

0.0

0.0

-0.5

-0.5

-0.5

-0.5

-0.5

-0.5

-1.0

-1.0

-1.0

-1.0

-1.0

-1.0

-1.5

-1.5

-1.5

-1.5

-1.5

-1.5

1

2.0

2.0

2.0

2.0

2.0

2.0

1.5

1.5

1.5

1.5

1.5

1.5

1.0

1.0

1.0

1.0

1.0

1.0

0.5

0.5

0.5

0.5

0.5

0.5

0.0

0.0

0.0

0.0

0.0

0.0

1.5 1.5 1.0 1.0 0.5 0.5 0.0 0.0 -0.5 -0.5 -1.0 -1.0 -1.5 -1.5
2.0 2.0 1.5 1.5 1.0 1.0 0.5 0.5 0.0 0.0

2

Figure B.5: True coefficient functions and their estimators and 95% SCCs based on the fifth slice.

Ramsay, J. O. and Dalzell, C. J. (1991), "Some tools for functional data analysis," Journal of the Royal Statistical Society. Series B (Methodological), 53, 539­572.
Ramsay, J. O. and Silverman, B. W. (2005), Functional Data Analysis, New York, Springer.
Ramsay, T. (2002), "Spline smoothing over difficult regions." Journal of the Royal Statistical Society: Series B (Statistical Methodology), 64, 307­319.
Reiss, P. T., Goldsmith, J., Shang, H. L., and Ogden, R. T. (2017), "Methods for scalaron-function regression," International Statistical Review, 85, 228­249.
Reiss, P. T., Huang, L., and Mennes, M. (2010), "Fast function-on-scalar regression with penalized basis expansions," The International Journal of Biostatistics, 6, Article 28.
Sang, H. and Huang, J. Z. (2012), "A full scale approximation of covariance functions for large spatial data sets," Journal of the Royal Statistical Society: Series B (Statistical Methodology), 74, 111­132.

60

TRUE

Kernel
2.0 1.5 1.0 0.5 0.0 -0.5

2 1 0 -1 -2

2 1 0

Tensor
2.0 1.5 1.0 0.5 0.0 -0.5
2 1 0 -1 -2

BPST( 1) BPST(

2.0 1.5 1.0 0.5 0.0 -0.5

2.0 1.5 1.0 0.5 0.0 -0.5
0

2

2

1

1

0

0

-1

-1

-2

-2

1

2) Lower SCC Upper SCC

2.0 1.5 1.0 0.5 0.0 -0.5

2.0 1.5 1.0 0.5 0.0 -0.5

2.0
2.0
1.5
1.5
1.0 1.0
0.5 0.5
0.0
-0.5 0.0
-0.5

2

2

1

1

0

0

-1

-1

-2

-2

2
2
1
1

0

0

-1
-1
-2

-2

2

2

2

2

2

1

1

1

1

1

0

0

0

0

0

2

2
2

1

1

0
0

Figure B.6: True coefficient functions and their estimators and 95% SCCs based on the 35th slice.

Sangalli, L. M., Ramsay, J. O., and Ramsay, T. O. (2013), "Spatial spline regression models," Journal of the Royal Statistical Society: Series B (Statistical Methodology), 75, 681­703.
Schwarz, K. and Krivobokova, T. (2016), "A unified framework for spline estimators," Biometrika, 103, 121­131.
Sentu¨rk, D. and Mu¨ller, H.-G. (2010), "Functional varying coefficient models for longitudinal data," Journal of the American Statistical Association, 105, 1256­1264.
Staicu, A. M., Crainiceanu, C. M., and Carroll, R. J. (2010), "Fast methods for spatially correlated multilevel functional data," Biostatistics, 11, 177­194.
Stein, J. L., Hua, X., Lee, S., Ho, A. J., Leow, A. D., Toga, A. W., Saykin, A. J., Shen, L., Foroud, T., Pankratz, N., et al. (2010), "Voxelwise genome-wide association study (vGWAS)," Neuroimage, 53, 1160­1174.
Wang, G., Wang, L., and Lai, M. J. (2019), "Package `BPST'," R package version 1.0. Available at "https://github.com/funstatpackages/BPST".

61

Slice 5

Slice 8

Slice 15

Slice 35

Slice 55

Slice 62

Slice 65

Figure B.7: Triangulation sets used in the ADNI data analysis.
Wang, J.-L., Chiou, J.-M., and Muller, H.-G. (2016), "Functional data analysis," Annal Review of Statistics and Its Applications, 3, 257­295.
Worsley, K. J., Taylor, J. E., Tomaiuolo, F., and Lerch, J. (2004), "Unified univariate and multivariate random field theory," NeuroImage, 23, S189­S195.
Wu, S. and Mu¨ller, H.-G. (2011), "Response-Adaptive Regression for Longitudinal Data," Biometrics, 67, 852­860.
Yao, F., Mu¨ller, H.-G., and Wang, J.-L. (2005), "Functional linear regression analysis for longitudinal data," The Annals of Statistics, 33, 2873­2903.
Yu, S., Wang, G., and Wang, L. (2019), "Package `FDAimage'," R package version 1.0. Available at "https://github.com/funstatpackages/FDAimage".
Zhang, X. and Wang, J.-L. (2015), "Varying-coefficient additive models for functional data," Biometrika, 102, 15­32.
Zhou, H., Li, L., and Zhu, H. (2013), "Tensor regression with applications in neuroimaging data analysis," Journal of the American Statistical Association, 108, 540­552.
62

Intercept
1.5 1.0 0.5
1.5 1.0 0.5 0.0
2.0 1.5 1.0 0.5

MA
0.03 0.00 -0.03 -0.06
0.06 0.03 0.00 -0.03 -0.06
0.05 0.00 -0.05

AD
0.05 0.00 -0.05
0.05 0.00 -0.05 -0.10
0.10 0.05 0.00 -0.05 -0.10

Age
0.005 0.000 -0.005
Slice 8
0.004 0.000 -0.004
Slice 15
0.005 0.000 -0.005 -0.010
Slice 35

Sex

0.12 0.08 0.04 0.00 -0.04

APOE1
0.04 0.02 0.00 -0.02

APOE2
0.05 0.00 -0.05

0.05 0.00 -0.05 -0.10 -0.15

0.050 0.025 0.000 -0.025 -0.050

0.10 0.05 0.00 -0.05

0.10 0.05 0.00 -0.05 -0.10

0.04 0.02 0.00 -0.02 -0.04

0.08 0.04 0.00 -0.04

63

Figure B.8: The BPST estimates of the coefficient functions for the ADNI data based on the eighth, 15th and 35th slices, respectively.

Intercept
1.5 1.0 0.5
1.6 1.2 0.8 0.4
1.5 1.0 0.5

MA
0.050 0.025 0.000 -0.025 -0.050
0.050 0.025 0.000 -0.025 -0.050
0.03 0.00 -0.03

AD

0.10 0.05 0.00 -0.05 -0.10

0.10 0.05 0.00 -0.05

0.05 0.00 -0.05 -0.10

Age

0.005 0.000 -0.005

Slice 55

0.0025 0.0000 -0.0025 -0.0050
Slice 62

Slice 65

0.002 0.000 -0.002 -0.004 -0.006

Sex

0.10 0.05 0.00 -0.05

APOE1
0.02 0.00 -0.02 -0.04

APOE2
0.05 0.00 -0.05 -0.10

0.05 0.00 -0.05
0.10 0.05 0.00 -0.05

0.025 0.000 -0.025 -0.050
0.04 0.02 0.00 -0.02 -0.04

0.05 0.00 -0.05 -0.10
0.05 0.00 -0.05

64

Figure B.9: The BPST estimates of the coefficient functions for the ADNI data based on the 55th, 62nd and 65th slices, respectively.

Intercept

MA

AD

Age

Sex

APOE1 APOE2

Slice 8

65

Slice 15

Slice 35
Figure B.10: The "significance" map (based on the 95% SCC) for the coefficient functions for the ADNI data. The yellow color and blue color on the map indicate the regions that zero is below the lower SCC or above the upper SCC, respectively.

Intercept

MA

AD

Age

Sex

APOE1

APOE2

Slice 55

66

Slice 62

Slice 65
Figure B.11: The "significance" map (based on the 95% SCC) for the coefficient functions for the ADNI data. The yellow color and blue color on the map indicate the regions that zero is below the lower SCC or above the upper SCC, respectively.

Zhu, H., Fan, J., and Kong, L. (2014), "Spatially varying coefficient model for neuroimaging data with jump discontinuities," Journal of the American Statistical Association, 109, 1084­1098.
Zhu, H., Li, R., and Kong, L. (2012), "Multivariate varying coefficient model for functional responses," The Annals of statistics, 40, 2634­2666.
67

