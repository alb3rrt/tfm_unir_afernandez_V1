arXiv:2106.00919v1 [eess.IV] 2 Jun 2021

Self-supervised Lesion Change Detection and Localisation in Longitudinal Multiple Sclerosis
Brain Imaging 
Minh-Son To1,2, Ian G Sarno2, Chee Chong2,3, Mark Jenkinson4,5, and Gustavo Carneiro5
1 FHMRI, Flinders University, Australia minhson.to@flinders.edu.au 2 SAMI, Flinders Medical Centre, Australia
3 Dr Jones & Partners Medical Imaging, Australia 4 FMRIB Centre, University of Oxford, United Kingdom
5 AIML, University of Adelaide, Australia
Abstract. Longitudinal imaging forms an essential component in the management and follow-up of many medical conditions. The presence of lesion changes on serial imaging can have significant impact on clinical decision making, highlighting the important role for automated change detection. Lesion changes can represent anomalies in serial imaging, which implies a limited availability of annotations and a wide variety of possible changes that need to be considered. Hence, we introduce a new unsupervised anomaly detection and localisation method trained exclusively with serial images that do not contain any lesion changes. Our training automatically synthesises lesion changes in serial images, introducing detection and localisation pseudo-labels that are used to self-supervise the training of our model. Given the rarity of these lesion changes in the synthesised images, we train the model with the imbalance robust focal Tversky loss. When compared to supervised models trained on different datasets, our method shows competitive performance in the detection and localisation of new demyelinating lesions on longitudinal magnetic resonance imaging in multiple sclerosis patients. Code for the models will be made available on GitHub.
Keywords: Change detection · Siamese networks · Multiple sclerosis.
1 Introduction
Diagnostic imaging interpretation routinely involves comparisons with prior imaging to identify new lesions or detect changes to existing structures and lesions. Change detection can be a tedious and difficult task for the radiologist [3] and importantly, the presence of change can alter clinical management.
 This paper was partially supported by an Avant Doctor in Training Research Scholarship and the Australian Research Council through grants DP180103232 and FT190100525.

2

M. To et al.

For example, magnetic resonance imaging (MRI) plays a central role in monitoring disease progression in multiple sclerosis [30] and new demyelinating lesions on follow-up imaging may require modification to immunotherapy [25]. Change detection requires a balance between highlighting relevant differences, and suppressing trivial perceptual differences. The latter may be related to imperfect image registration or technical differences in acquisition, such as different scanners, different magnetic field strength (e.g. 1.5T versus 3T MRI), and different imaging spatial resolution.
Deep learning models have gained significant attention in recent years due to their unprecedented performance in a variety of computer vision tasks [31]. Many of these have been translated to medical imaging applications [23], including supervised lesion detection and classification [14], organ and structural segmentation [16, 36], and image enhancement [27]. A drawback of supervised deep learning models is the requirement for a sufficiently large and balanced annotated training set [13]. Unfortunately, certain types of lesion appearance and change detection, such as MS in brain MRI, are anomalous events, where a large variety of changes can occur. For such problems, it is prohibitively inefficient to annotate every type of change to collect a large and balanced training set. Thus, supervised models for change detection may not generalise well for broad application in medical imaging.
In this paper, we propose a solution to mitigate the lack of annotated lesion change samples with a new unsupervised anomaly detection and localisation method trained only with serial images that do not contain any change. Our training simulates changes in the serial images, enabling the production of detection and localisation pseudo-labels that are used to self-supervise the training of our model. The images are synthesised by mixing super-pixels [2] from the original image and its reconstructed image generated by a variational autoencoder [18]. To tackle the class imbalance problem, we employ a focal Tversky loss [1, 32]. Experiments on the problem of detection and localisation of new demyelinating lesions on longitudinal MRI in multiple sclerosis patients show that our method produces competitive detection and localisation results compared with other fully-supervised methods [9, 19] trained on different datasets.
2 Related Work
Automated change detection is a long-standing problem in medical imaging [6] and other fields [28, 17]. Previous work on detecting change in longitudinal multiple sclerosis imaging include subtraction techniques to visualise areas of change [26], statistical modelling [34], and deep learning [5, 24, 19, 35]. Approaches that jointly solve image registration and change detection have also been devised [7, 11].
We propose a data augmentation strategy based on mixing super-pixels [2]. Unlike CutOut [10] and CutMix [38], mixing based on super-pixels utilises a segmentation that is not restricted to horizontally- or vertically-oriented edges. Furthermore, as super-pixels group pixels with perceptually similar characteristics, mixing super-pixels is more likely to preserve the integrity of anatomical and lesional boundaries and maintain the saliency of transposed portions.

Change Detection in Longitudinal Multiple Sclerosis Brain Imaging

3

3 Materials and Methods
3.1 Data Acquisition and Processing
Research ethics for this study was granted by Bellberry Limited. Longitudinal volumetric T2 FLAIR examinations performed by a private imaging provider (Anonymous Medical Imaging) between August 2018 and October 2020 across multiple sites were extracted. Based on findings in the radiologist report, pairs of scans were separated into two sets, namely: pairs containing lesion changes (Change), and pairs without lesion changes (NoChange). The training set contains pairs of scans in the NoChange set and is denoted by D = {xi, x^i}|iD=|1, where xi, x^i  X  RH×W ×D represent a pair of the MRI scans from the same patient. The testing set, with pairs of scans from the Change set, is represented by T = {(xi, x^i, y^i)}|iT=|1, where xi, x^i  X denote a pair of scans from the same patient with a lesion change indicated in the binary map y^i  Y  {0, 1}H×W ×D.
The images in both sets were pre-processed with a resampling to 1.0mm × 1.0mm × 1.0mm isometric voxels, greyscale intensity normalisation to [0, 1] (0th and 99th percentile), skull stripping [15], rigid body registration [12], and N4 bias correction [22]. For pairs of scans in the Change set, the ground truth masks for regions of change were manually annotated by a trainee radiologist (Anonymous) using ITK-SNAP [39] based on the findings in the report as a guide. In total, 237 pairs of NoChange scans were used for model training, while 94 pairs of Change scans were used for model testing.
3.2 Methods
The training of our model follows a two-stage process that uses only the NoChange dataset D. The first stage consists of a variational auto-encoder (VAE) [18] combined with a new data augmentation strategy related to CutMix [38], that automatically generates images and annotation maps with synthetic lesion changes. The second step comprises training of a siamese 3D U-net [29] that takes two serial images of the same patient and returns a segmentation map of the lesion change.
Generating Synthetic Lesion Changes The generation of synthetic lesion changes starts with a VAE [18] that relies on an encoder f : X  M×S, a sampling from the Gaussian model to generate the embedding z  N (µ, ), where z  Z, µ  M and   S, and a reconstruction from the decoder g : Z  X . This VAE is trained with the images x  D using an voxel-wise L1 reconstruction loss and a regularisation loss to minimise the Kulback-Leibler divergence between the embedding distribution and a zero-mean identity-covariance Gaussian. To increase the diversity of the reconstructions, a perturbation is also applied to the embedding, ~z = z × , where   U(-, ) (U(.) denoting a uniform distribution). Hence, for each image xi  D, we sample new reconstructions, denoted by x~i = g(zi × ), with zi  N (µi, i), and µi, i = f(xi).
To synthesise lesion changes, we propose a super-pixel mixing data augmentation strategy, referred to as SuperMix. Using the scan xi and its sampled reconstruction x~i, we first produce a super-pixel segmentation [2] that tesselates xi to produce nseg binary maps, each represented by bi,t  {0, 1}H×W ×D for

4

M. To et al.

t  {0, ..., nseg - 1} (in this map, a voxel is labelled with 1, if it belongs to the tth super-pixel). The synthesised image and lesion change are defined by

nseg -1

xi =

(t)(bi,t  xi) + (1 - t)(bi,t  x~i),

t=0

(1)

nseg -1

y^i =

(t)(0H×W ×D ) + (1 - t)(bi,t),

t=0

where t = 1 if u <  , with u  U (0, 1), and 0H×W ×D denotes a volume of size H × W × D containing only zeros. This allows us to build a synthesised set of Change, represented by D^ = {(xi, x^i, y^i)}|iD=|1. Fig. 1 shows examples of the synthesised lesion changes for different values of nseg.
Siamese 3D U-net Our proposed siamese 3D U-net takes a sample (xi, x^i, y^i)  D^ to form two inputs and one output. The inputs are x(i1) = (xi, |xi - x^i|)  X 2 and x(i2) = (x^i, |xi - x^i|)  X 2, and the output is the segmentation map y^i. Siamese network configurations enable independent processing of image inputs
that may improve feature extraction prior to fusion and comparison of fea-
tures [8]. Our siamese 3D U-net consists of a down-sampling network, represented by e : X 2 × X 2  K, followed by an up-sampling network, denoted by d : K  [0, 1]H×W ×D. As shown in Fig. 2 the model contains skip connections that propagate data directly from the encoder to decoder at the same spatial
resolution, providing local contextual information to the up-sampling process.
Each block of the model is formed by inception modules [37] that incorporate
convolutional filters of different sizes with dimensionality reduction achieved by 1 × 1 × 1 convolutional layers to enable efficient processing of features at multiple
scales. Since areas of change typically occupy small regions in the entire image,
the ability to focus the network to only relevant areas improves utilisation of the
network resources. To that end, attention gates were built into the network to fil-
ter information being passed through the skip connections [33]. Down-sampling

VAE

nseg = 200

nseg = 500

nseg = 1000

nseg = 2000

nseg = 5000

Real

Fig. 1: SuperMix examples showing the effect of nseg. The perturbed VAE reconstruction ( = 5) and overlaid SuperMix masks (red) are shown in the top row. The resultant synthesised lesion changes are shown in the bottom row.

Change Detection in Longitudinal Multiple Sclerosis Brain Imaging

5

Fig. 2: Schematic of proposed siamese U-net model.

operations in the U-net encoder can result in loss of information flow in the deeper layers. To mitigate this, we provided down-sampled inputs to the deep encoding layers. We also encouraged the intermediate layers of the U-net decoder to learn meaningful segmentations by incorporating deep supervision [20]. This was implemented by passing the outputs of the intermediate layers through a 1 × 1 × 1 convolutional layer with sigmoid activation.
The training of this siamese 3-D model is based on the minimisation of the focal Tversky loss [1, 32] that can handle the class imbalance problem associated with segmenting small and localised areas of change. This loss is defined by

(D^, , )

=

1 |D|

|D|
(1 - tv(y^i, y~i)) ,

(2)

i=1

where   R+, y~i = d(e(x(i1), x(i2)))), and tv(.), the Tversky index, generalises the Dice coefficient, with

tv(y^i, y~i)

=

TP

+

TP F N

+

F P

,

(3)

6

M. To et al.

where T P , F N and F P are the numbers of true positive, false negative and false positive between the annotation y^i and the model output y~i, respectively. In (3), the parameters  and  can be tuned to emphasise recall over precision ( > ), or vice versa ( < ), such that  +  = 1 (Supplemental Fig. 3).
After training the model, inference is performed by running it on a test sample composed of a pair of serial images (x, x^) from T . The model produces y~ = d(e(x(1), x(2))) with x(1) = (x, |x - x^|) and x(2) = (x^, |x - x^|). Then, we threshold this output to produce a binary output with y~ > , which is then used by connected component analysis (CCA) to produce segmentation blobs of size  {20, 100}. This binarised result can then be compared with the ground truth annotation in y^ from T .
4 Experiments and Evaluation
Training for the VAE consisted of 200 iterations in batches of 4096 volume crops randomly sampled from the set of NoChange scans, with mini batch size of 2. Each crop measured 192 × 192 × 16 voxels. The effect of  to paramaterise the uniform distribution that influences the VAE generation is linked to the VAE encoder architecture, such that replacing max-pooling with strided convolutions results in greater mottling in the generator output (Supplemental Fig. 1). For all experiments we utilised the VAE (MaxPool) architecture and  was fixed at 5.0. The training for the lesion change detector consisted of 60 iterations in batches of 100 NoChange pairs augmented by SuperMix, with mini batch size of 2. The SuperMix mixing parameter  in (1) was fixed at 0.98, with log nseg  U(log 200, log 5000). The recall/precision balance of the model is tunable by the , , and  parameters of the focal loss in (2) and (3) (Supplemental Fig. 3). For all outputs  was set to 0.75, while  = 0.75 for intermediate layers, and we tested  = {0.75, 1.0} for the final layer. In a series of ablation experiments, we explored the contributions of network architecture (non-siamese or siamese U-net), loss function (binary cross-entropy or focal Tversky loss), multi-scale inputs, deep supervision, attention gates, and Inception modules. We included L2 weight regularisation in the siamese U-net.
Adam optimiser was used. A learning rate of 0.00005 was used to train the VAE. For the lesion change detector, an initial learning rate of 0.0002 and learning decay rate of 0.001 were used. Models were written in Keras/Tensorflow and trained on a GeForce RTX 3090 GPU with 24GB of memory.
Model performance was evaluated by generating prediction masks on the Change pairs, thresholding  = 0.1, and comparing against ground truth lesions. The lesion-wise true positive rate (LTPR), lesion-wise false positive rate (LFPR) and positive predictive value (PPV) were calculated for lesion changes in each Change pair and averaged across all pairs (Supplemental D). Overlap between a ground truth change and predicted change lesion was significant (i.e. a lesionwise true positive) if the intersection over union (IoU) of the blobs was 0.01 or greater. Others have considered overlap of a single voxel to be sufficient [4], but we decided to be more conservative with an IoU .01.

Change Detection in Longitudinal Multiple Sclerosis Brain Imaging

7

Table 1: Performance of selected model configurations. AG, attention gate; MS,

multiscale input; DS, deep supervion; BCE, binary cross-entropy; FTL, focal

Tversky loss. See also Fig. 3.

Model

Loss  L2 reg Blob LTPR LFPR PPV

U-net

BCE -

-

20 0.330 0.932 0.0684

U-net

FTL 1.0

U-net-AG-MS-DS

FTL 1.0

U-net-Inc-AG-MS-DS FTL 1.0

Siamese U-net

FTL 1.0

Siamese U-net

FTL 1.0

Siamese U-net

FTL 0.75

Siamese U-net

FTL 0.75

-
-
10-6 10-4 10-6 10-4

20 0.618 0.960 0.0404 20 0.664 0.945 0.0546 20 0.645 0.909 0.0914 20 0.758 0.942 0.0579 20 0.708 0.884 0.116 20 0.747 0.946 0.0541 20 0.634 0.874 0.126

Li et al. 2018 [21]

-

-

-

- 0.133 0.997 0.0031

4.1 Model performance
Ablation experiments show the incremental improvements in change detection provided by incorporation of the focal Tversky loss, attention gates, multiscale inputs and deep supervision (Fig. 3 and Table 1). Connected components post-processing with a larger blob size (100 compared to 20) generally improve PPV, with a corresponding reduction in LTPR. Siamese architectures yielded the best performance. The focal loss parameter  did not strongly influence segmentation performance. Example change mask predictions are shown in Fig. 4, demonstrating the model's ability to detect changes in different parts of the brain, as well as changes to existing lesions. High LFPR values may correspond to other regions of change (Fig. 4(f)), distinct from lesion changes. Some may be changes unrelated to multiple sclerosis, but their relevance is not explored here.
The lesion change recall (i.e. LTPR) of the siamese models is comparable to published supervised models [9, 19]. We also test a publicly-available deep fully convolutional network model for white matter hyperintensities (WMH) segmentation that was trained on different datasets [21]. In this case, the difference between the predicted WMH segmentations of each input image is taken to be the predicted change segmentation. Despite reporting an average recall of 0.84 on the original data, that model [21] performs poorly on our data. We observed that lesions tended to be under-segmented and lesion changes were frequently missed, resulting in a low LTPR the model in [21] (Table 1).
5 Conclusions
The generation of images and annotation maps with synthetic lesion changes and the siamese U-net are the main contributions of this paper. These contributions mitigate the lack of annotated lesion change datasets with a self-supervised training of the anomaly detection and localisation methods. Change detection is optimised by the incorporation of the focal Tversky loss, attention gates, multiscale inputs and deep supervision. Our approach, which does not require detailed, voxel-level annotated training sets, demonstrates high detection rates for lesion changes in multiple sclerosis imaging, comparable to fully-supervised models.

8

M. To et al.

LTPR

1.00 0.75 0.50 0.25 0.00
1.00 0.75 0.50 0.25 0.00

PPV

U-BCE (B20) U-FTL (B20) U-AG-MS-DS-FTL (B20) U-Inc-AG-MS-DS-FTL (B20) S-g=1.0-L2=1e-6 (B20) S-g=1.0-L2=1e-4 (B20) S-g=0.75-L2=1e-6 (B20) S-g=0.75-L2=1e-4 (B20) U-BCE (B100) U-FTL (B100) U-AG-MS-DS-FTL (B100) U-Inc-AG-MS-DS-FTL (B100) S-g=1.0-L2=1e-6 (B100) S-g=1.0-L2=1e-4 (B100) S-g=0.75-L2=1e-6 (B100) S-g=0.75-L2=1e-4 (B100)

Model
Fig. 3: Performance of different model configurations. U, U-net, BCE, binary cross-entropy; FTL, focal Tversky loss; AG, attention gate; MS, multiscale input; DS, deep supervision; B, minimum blob size; g,  parameter of final layer FTL; L2, L2 weight regularisation factor.

Time point 1

Time point 2

Time point 1

Time point 2

Time point 1

Time point 2

(a) Cortical

Time point 1

Time point 2

(b) Subcortical

Time point 1

Time point 2

(c) Peri-ventricular

Time point 1

Time point 2

(d) Brainstem

(e) Lesion expansion

(f) False detection

Fig. 4: Example predicted change masks. Ground truth (blue) and predicted (red) change masks are shown overlaid the scan at time point 2.

Change Detection in Longitudinal Multiple Sclerosis Brain Imaging

9

References
1. Abraham, N., Khan, N.M.: A novel focal Tversky loss function with improved attention U-Net for lesion segmentation. arXiv:1810.07842 (2018)
2. Achanta, R., Shaji, A., Smith, K., et al.: SLIC superpixels compared to state-ofthe-art superpixel methods. IEEE Transactions on Pattern Analysis and Machine Intelligence 34, 2274­2282 (2012)
3. Altay, E.E., Fisher, E., Jones, S.E., et al.: Reliability of classifying multiple sclerosis disease activity using magnetic resonance imaging in a multiple sclerosis clinic. JAMA Neurology 70, 338 (2013)
4. Aslani, S., Dayan, M., Storelli, L., et al.: Multi-branch convolutional neural network for multiple sclerosis lesion segmentation. NeuroImage 196, 1­15 (2019)
5. Birenbaum, A., Greenspan, H.: Multi-view longitudinal CNN for multiple sclerosis lesion segmentation. Engineering Applications of Artificial Intelligence 65, 111­118 (2017)
6. Bosc, M., Heitz, F., Armspach, J.P., et al.: Automatic change detection in multimodal serial MRI: application to multiple sclerosis lesion evolution. NeuroImage 20(2), 643­656 (2003)
7. Bu, S., Li, Q., Han, P., et al.: Mask-CDNet: A mask based pixel change detection network. Neurocomputing 378, 166­178 (2020)
8. Daudt, R.C., Saux, B.L., Boulch, A.: Fully convolutional siamese networks for change detection. arXiv:1810.08462 (2018)
9. Denner, S., Khakzar, A., Sajid, M., et al.: Spatio-temporal learning from longitudinal data for multiple sclerosis lesion segmentation. arXiv:2004.03675 (2020)
10. DeVries, T., Taylor, G.W.: Improved regularization of convolutional neural networks with cutout. arXiv:1708.04552 (2017)
11. Dufresne, E., Fortun, D., Kumar, B., et al.: Joint registration and change detection in longitudinal brain MRI. In: 2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI). pp. 104­108. IEEE (2020)
12. Garyfallidis, E., Brett, M., Amirbekian, B., et al.: Dipy, a library for the analysis of diffusion MRI data. Frontiers in Neuroinformatics 8, 8 (2014)
13. Hofmanninger, J., Prayer, F., Pan, J., Ro¨hrich, S., Prosch, H., Langs, G.: Automatic lung segmentation in routine imaging is primarily a data diversity problem, not a methodology problem. European Radiology Experimental 4 (2020)
14. Huang, X., Shan, J., Vaidya, V.: Lung nodule detection in CT using 3D convolutional neural networks. In: 2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017) (2017)
15. Isensee, F., Schell, M., Pflueger, I., et al.: Automated brain extraction of multisequence MRI using artificial neural networks. Hum Brain Mapp 40(17), 4952­4964 (2019)
16. Kayalibay, B., Jensen, G., van der Smagt, P.: CNN-based segmentation of medical imaging data. arXiv:1701.03056 (2017)
17. Khelifi, L., Mignotte, M.: Deep learning for change detection in remote sensing images: Comprehensive review and meta-analysis. IEEE Access 8, 126385­126400 (2020)
18. Kingma, D.P., Welling, M.: Auto-encoding variational bayes. arXiv:1312.6114 (2013)
19. Kru¨ger, J., Opfer, R., Gessert, N., et al.: Fully automated longitudinal segmentation of new or enlarged multiple sclerosis lesions using 3D convolutional neural networks. NeuroImage: Clinical 28, 102445 (2020)

10

M. To et al.

20. Lee, C.Y., Xie, S., Gallagher, P., et al.: Deeply-supervised nets. arXiv:1409.5185 (2014)
21. Li, H., Jiang, G., Zhang, J., et al.: Fully convolutional network ensembles for white matter hyperintensities segmentation in MR images. NeuroImage 183, 650­665 (2018)
22. Lowekamp, B.C., Chen, D.T., Iba´n~ez, L., Blezek, D.: The design of SimpleITK. Frontiers in Neuroinformatics 7, 45 (2013)
23. Lundervold, A.S., Lundervold, A.: An overview of deep learning in medical imaging focusing on mri. Zeitschrift fu¨r Medizinische Physik 29, 102­127 (2019)
24. McKinley, R., Wepfer, R., Grunder, L., et al.: Automatic detection of lesion load change in multiple sclerosis using convolutional neural networks with segmentation confidence. NeuroImage: Clinical 25, 102104 (2020)
25. McNamara, C., Sugrue, G., Murray, B., MacMahon, P.J.: Current and emerging therapies in multiple sclerosis: Implications for the radiologist, part 1--mechanisms, efficacy, and safety. AJNR 38, 1664­1671 (2017)
26. Patel, N., Horsfield, M.A., Banahan, C., et al.: Detection of focal longitudinal changes in the brain by subtraction of mr images. AJNR 38, 923­927 (2017)
27. Plassard, A.J., Davis, L.T., Newton, A.T., et al.: Learning implicit brain MRI manifolds with deep learning. arXiv:1801.01847 (2018)
28. Radke, R.J., Andra, S., Al-Kofahi, O., Roysam, B.: Image change detection algorithms: a systematic survey. IEEE transactions on image processing 14(3), 294­307 (2005)
29. Ronneberger, O., Fischer, P., Brox, T.: U-Net: Convolutional networks for biomedical image segmentation. arXiv:1505.04597 (2015)
30. A` lex Rovira, Wattjes, M.P., Tintor´e, M., et al.: MAGNIMS consensus guidelines on the use of MRI in multiple sclerosis--clinical implementation in the diagnostic process. Nature Reviews Neurology 11, 471­482 (2015)
31. Russakovsky, O., Deng, J., Su, H., et al.: ImageNet large scale visual recognition challenge. International Journal of Computer Vision volume 115, 211­252 (2015)
32. Salehi, S.S.M., Erdogmus, D., Gholipour, A.: Tversky loss function for image segmentation using 3D fully convolutional deep networks. arXiv:1706.05721 (2017)
33. Schlemper, J., Oktay, O., Schaap, M., et al.: Attention gated networks: Learning to leverage salient regions in medical images. Medical Image Analysis 53, 197­207 (2019)
34. Schmidt, P., Pongratz, V., Ku¨ster, P., et al.: Automated segmentation of changes in FLAIR-hyperintense white matter lesions in multiple sclerosis on serial magnetic resonance imaging. NeuroImage: Clinical 23, 101849 (2019)
35. Sepahvand, N.M., Arnold, D.L., Arbel, T.: CNN detection of new and enlarging multiple sclerosis lesions from longitudinal MRI using subtraction images. In: 2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI). pp. 127­130 (2020)
36. Snaauw, G., Gong, D., Maicas, G., et al.: End-to-end diagnosis and segmentation learning from cardiac magnetic resonance imaging. In: 2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019). pp. 802­805. IEEE (2019)
37. Szegedy, C., Liu, W., Jia, Y., et al.: Going deeper with convolutions. In: 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 1­9 (2015)
38. Yun, S., Han, D., Oh, S.J., et al.: CutMix: Regularization strategy to train strong classifiers with localizable features. arXiv:1905.04899 (2019)
39. Yushkevich, P.A., Piven, J., Hazlett, H.C., et al.: User-guided 3D active contour segmentation of anatomical structures: significantly improved efficiency and reliability. Neuroimage 31(3), 1116­1128 (2006)

Supplemental Material
A VAE perturbation

arXiv:2106.00919v1 [eess.IV] 2 Jun 2021

VAE (Strided Conv)

Original

 = 0.0

(a) VAE architectures

 = 1.0

 = 2.0

 = 5.0

 = 10.0

Original

 = 0.0

 = 0.5

 = 1.0

 = 2.0

 = 5.0

(b) Corresponding images produced by VAE peturbation
Fig. 1: Variational auto-encoder architectures used to generate perturbations of input image.

2

B Deep supervision

Time point 1

Time point 2

Ground truth

Scale 1 (192,192,16)

Scale 2 (96,96,8)

Scale 3 (48,48,4)

Scale 4 (24,24,2)

Fig. 2: Intermediate predictions with deep supervision.

C Tuning recall and precision

Time point 1

Time point 2

Ground truth

 = 0.1,  = 0.9

 = 0.3,  = 0.7

 = 0.5,  = 0.5

 = 0.7,  = 0.3

 = 0.9,  = 0.1

Fig. 3: The effect of tuning  and  parameters of the focal Tversky loss.

D Evaluation metrics

The lesion-wise true positive rate (LTPR), lesion-wise false positive rate (LFPR) and positive predictive value (PPV) are defined as

LT P R

=

LT P ,
LT P + LF N

LF P R

=

LF P LT P + LF P

,

PPV

=

LT P LT P + LF P ,

where LT P , LF N , and LF P denote lesion-wise true positives, false negatives, and false positives, respectively.

