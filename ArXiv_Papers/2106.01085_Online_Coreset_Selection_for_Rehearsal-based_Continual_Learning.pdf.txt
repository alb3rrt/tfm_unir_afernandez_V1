arXiv:2106.01085v1 [cs.LG] 2 Jun 2021

Online Coreset Selection for Rehearsal-based Continual Learning
Jaehong Yoon1, Divyam Madaan1, Eunho Yang1,2, Sung Ju Hwang1,2 KAIST 1, South Korea AITRICS 2, South Korea
{jaehong.yoon, dmadaan, eunhoy, sjhwang82}@kaist.ac.kr
Abstract
A dataset is a shred of crucial evidence to describe a task. However, each data point in the dataset does not have the same potential, as some of the data points can be more representative or informative than others. This unequal importance among the data points may have a large impact in rehearsal-based continual learning, where we store a subset of the training examples (coreset) to be replayed later to alleviate catastrophic forgetting. In continual learning, the quality of the samples stored in the coreset directly affects the model's effectiveness and efficiency. The coreset selection problem becomes even more important under realistic settings, such as imbalanced continual learning or noisy data scenarios. To tackle this problem, we propose Online Coreset Selection (OCS), a simple yet effective method that selects the most representative and informative coreset at each iteration and trains them in an online manner. Our proposed method maximizes the model's adaptation to a target dataset while selecting high-affinity samples to past tasks, which directly inhibits catastrophic forgetting. We validate the effectiveness of our coreset selection mechanism over various standard, imbalanced, and noisy datasets against strong continual learning baselines, demonstrating that it improves task adaptation and prevents catastrophic forgetting in a sample-efficient manner.
1 Introduction
Humans possess the ability to learn a large number of tasks by accumulating knowledge and skills over time. Building a system resembling human learning abilities is a deep-rooted desire since sustainable learning over a long-term period is essential for general artificial intelligence. In light of this need, continual learning (CL) [40], or lifelong learning, tackles a learning scenario where a model continuously learns over a sequence of tasks [18, 25] within a broad research area, such as classification [15, 9], image generation [47], language learning [24, 4], clinical application [20, 22, 3], speech recognition [34], and federated learning [46]. A well-known challenge for continual learning is catastrophic forgetting [27], where the continual learner loses the fidelity for past tasks after adapting the previously learned knowledge to future tasks.
Recent rehearsal-based continual learning methods adapt the continual model to the previous tasks by maintaining and revisiting a small replay buffer [9, 41, 28]. However, the majority of these methods store random-sampled instances as a proxy set to mitigate catastrophic forgetting, limiting their practicality to real-world applications (see Figure 1a). However, all the training instances are not equally useful, as some of them can be more representative or informative for the target task, and others can lead to performance degeneration for previous tasks. Furthermore, these unequal potentials could be more severe under practical scenarios containing imbalanced, streaming, or noisy instances (see Figure 2). This leads to an essential question in continual learning:
Preprint. Under review.

redundancy 
noise

 Train 

 

 ,

 

 



Minibatch Similarity

 ,  ,

Select , , and    samples
Train 


... ...

Replay Buffer 

-1

Cross-batch Diversity

 ,

 

 

Coreset Affinity

Replay Buffer 

-1

(a) Existing rehearsal-based CL

(b) Online Coreset Selection (OCS)

Figure 1: Illustration of existing rehearsal-based CL and Online Coreset Selection (OCS): (a) Existing

rehearsal-based methods train on all the arrived instances and memorize a fraction of them in the replay buffer,

which results in a suboptimal performance due to the outliers (noisy or biased instances). (b) OCS obtains the

coreset by leveraging our three selection strategies, which discard the outliers at each iteration. Consequently,

the selected examples promote generalization and minimize interference with the previous tasks.

...

...





Train 





 

Train 





 

(a) Imbalanced continual learning

(b) Noisy continual learning

Figure 2: Realistic continual learning scenarios: (a) Each task consists of class-imbalanced instances.

(b) Each task has uninformative noise instances, which hamper training.

How can we obtain a coreset to promote task adaptation for the current task while minimizing catastrophic forgetting on previously seen tasks?
To address this question, we propose Online Coreset Selection (OCS), a novel method for continual learning that selects representative training instances for the current and previous tasks based on our following three selection strategies: (1) Minibatch similarity selects samples that are representative to the target task Tt. (2) Cross-batch diversity encourages minimal redundancy among the samples of target task Tt. (3) Coreset affinity promotes minimum interference between the selected samples and knowledge of the previous tasks Tk, k < t. To this end, OCS minimizes the catastrophic forgetting on the previous tasks by utilizing the obtained coreset for future training, and also encourages the current task adaptation by updating the model parameters on the top- selected data instances. The overall concept is illustrated in Figure 1b.

Our method is simple, intuitive, and is generally applicable to any rehearsal-based continual learning method. We evaluate the performance of OCS on various continual learning scenarios and show that it outperforms state-of-the-art rehearsal-based techniques on balanced, imbalanced, and noisy continual learning benchmarks of varying complexity. We also show that OCS is general and exhibits collaborative learning with the existing rehearsal-based methods, leading to increased task adaptation and inhibiting catastrophic forgetting. To summarize, our contributions are threefold:

· We address the problem of coreset selection for realistic and challenging continual learning scenarios, where the data continuum is composed of class-imbalanced or noisy instances that deteriorate the performance of the continual learner during training.
· We propose Online Coreset Selection (OCS), a simple yet effective online coreset selection method to obtain a representative and diverse subset that has a high affinity to the previous tasks from each minibatch during continual learning. Specifically, we present three gradient-based selection criteria to select the coreset for current task adaptation while mitigating catastrophic forgetting.
· We demonstrate that OCS is applicable to any rehearsal-based continual learning method and experimentally validate it on multiple benchmark scenarios, where it largely improves the performance of the base algorithms across various performance metrics.

2 Related Work
Continual learning. In the past few years, there has been significant progress in continual learning to alleviate catastrophic forgetting [27]. The regularization approaches [15, 21, 37] modify the model parameters with additional regularization constraints to prevent catastrophic forgetting. The architecture approaches [33, 44, 43, 23, 45] utilize network isolation or expansion during continual learning to improve network performance. Another line of research uses rehearsal approaches, which memorize or generate a small fraction of data points for previous tasks and utilizes them to retain the

2

task knowledge [26, 9, 2, 5]. For example, Gradient-based Sample Selection (GSS) [2] formulates the selection of the replay buffer as a constraint selection problem to maximize the variance of gradient direction. ER-MIR [1] iteratively constructs the replay buffer using a loss-based criterion, where the model selects the top- instances that increase the loss between the current and previous iteration. However, the existing rehearsal-based methods [32, 2, 1, 9, 10] do not select the coreset before the current task adaptation and update the model on all the arriving data streams, which makes them susceptible to real-world applications that include noisy and imbalanced data distributions. In contrast, OCS selects the instances before updating the model using our proposed selection criteria, which makes it robust to past and current task training across various CL scenarios.
Coreset selection. There exist various directions to obtain a coreset from a large dataset. Importance sampling [13, 14, 38] strengthens the loss/gradients of important samples based on influence functions. Kool et al. [16] connect stochastic Gumbel-top-k trick and beam search to hierarchically sample sequences without replacement. Rebuffi et al. [32] propose a herding based strategy for coreset selection. Nguyen et al. [31] formulate the coreset summarization in continual learning using online variational inference [35, 6]. Aljundi et al. [2] select the replay buffer to maximize the variance in the gradient-space. Contrary to these methods, OCS considers the diversity, task informativity and relevancy to the past tasks. Recently, Borsos et al. [5] propose a bilevel optimization framework with cardinality constraints for coreset selection. However, their method is extremely limited in practice and inapplicable in large-scale settings due to the excessive computational cost incurred during training. In contrast, our method is simple, and scalable since it can construct the coreset in the online streaming data continuum without additional optimization constraints.

3 Rehearsal-based Continual Learning

We consider learning a model over a sequence of tasks {T1, . . . , TT } = T , where each task is
composed of independently and identically distributed datapoints and their labels, such that task
Tt includes Dt = {xt,n, yt,n}Nn=t 1  Xt × Yt, where Nt is the total number of data instances, and Xt × Yt is an unknown data generating distribution. We assume that an arbitrary set of labels for task Tt, yt = {yt,n}Nn=t 1 has unique classes, yt  yk = , t = k. In a standard continual learning scenario, the model learns a corresponding task at each step and t-th task is accessible at step t only. Let neural network f : X1:T  Y1:T be parameterized by a set of weights  = {l}Ll=1, where L is the number of layers in the neural network. We define the training objective at step t as follows:

Nt

minimize

(f(xt,n), yt,n),

(1)



n=1

where (·) is any standard loss function (e.g., cross-entropy loss). The naive CL design cannot retain the knowledge of previous tasks and thus results in catastrophic forgetting. To tackle this problem, rehearsal-based methods [31, 9, 41] update the model on a randomly sampled replay buffer Ck constructed from the previously observed tasks, where Ck = {xk,j, yk,j}Jj=k 1  Dk, k < t and Jk Nk. Consequently, the quality of the selected instances is essential for rehearsal-based continual learning. For example, some data instances can be more informative and representative than others to describe a task and improve model performance. In contrast, some data instances can degrade the model's memorization of previous tasks' knowledge. Therefore, obtaining the most beneficial examples for the target task is crucial for the success of rehearsal-based CL methods.

To validate our hypothesis, we design a continual learning scenario with a sequence of two tasks,
Permuted MNIST (T1)  MNIST (T2). After training on T1, we update the model parameters through a single backpropagation step using a data point from T2 and measure the test accuracy of all the corresponding classes and T1 performance degeneration. The results for all individual data points from T2 are described in Figure 3a and Figure 3b as box plots. The influence of each data point from T2 has a large disparity not only on the corresponding class accuracy but also on past task's average forgetting, which will further accumulate when trained on a large number of data points and tasks.

Based on this motivation, our objective is to select the data instances that can promote current task adaptation while minimizing catastrophic forgetting on the previous tasks. However, obtaining the most representative data points from the entire dataset is computationally expensive and intractable for online continual learning. Hence, we postulate and empirically validate a straightforward assumption to select a representative coreset for a target dataset.

3

Gradient Distance (x 1e-4)

Grad. dist. to full dataset during training

minibatch:10

0.8

minibatch:20 minibatch:50

0.6

minibatch:100 minibatch:500

0.4

Perm-MNIST

0.2

0.0
10 It2e0ratio30n (x 410000)50 60

(a) Classwise adaptation for T2 (b) Classwise forgetting for T1

(c) Classwise forgetting for T1

Figure 3: (a-b) Classwise accuracy gain and average forgetting when a model trained on Permuted MNIST (T1) is updated on a single data point at class c on MNIST (T2). (c) Empirical validation of Assumption 1 on MNIST dataset measuring the mean squared distance between the gradients of a minibatch and the whole dataset. The distance decreases with a larger minibatch, and we observe that a small minibatch has a comparable distance to the larger minibatch and can approximate the whole dataset with a small .

More formally, for a given subset Bt, dataset Dt, and > 0, we state our assumption formally. Assumption 1. The gradient of any arbitrary subset approximates the gradient of the whole dataset,

1

1

Nt f(Dt) - |Bt| f(Bt)  .

(2)

In principle, obtaining a representative subset from a large dataset is still computationally expensive; therefore, we consider a minibatch as an approximation of the whole dataset and select few representative data instances at each minibatch iteration. While the approximation quality depends on the dataset and minibatch size, we empirically validate that the above assumption generally holds across various minibatch sizes. In particular, we conduct an experiment on the MNIST dataset, where we measure the mean squared distance between the gradients of a batch and whole dataset per iteration in Figure 3c. Notably, even the gradients of an arbitrary subset with a small-sized minibatch are similar to the whole dataset with a small , and it is more evident when compared to the irrelevant gradients from a different dataset (Permuted MNIST).
Under Assumption 1, we propose a selection criterion that selects the subset with the smallest possible . Specifically, we select the coreset that maximizes the gradient similarity between the representative instances and the target dataset. More formally:

u = maximize S
uN

1

1

Nt f (Dt) ,  nu f (xt,n, yt,n) , where u = {n : n  N<Nt },

(3)

where S is any arbitrary similarity function and u is an index set that selects top- informative samples without replacement. Consequently, the model iteratively updates the parameters to find the optimal local minima of the loss, and informative data points obtain similar gradient directions with the averaged gradients of the dataset. In the next section, we propose OCS which consists of a simple similarity criterion to achieve this objective. However, similarity criterion is not sufficient to select the representative coreset for online continual learning; hence, we also propose our diversity and coreset affinity criteria to mitigate catastrophic forgetting.

4 Online Coreset Selection
In this section, we introduce our selection strategies and propose Online Coreset Selection (OCS) to strengthen current task adaptation and mitigate catastrophic forgetting. Thus far, the rehearsal-based continual learning methods [32, 2, 1, 9, 10] populate the replay buffer to preserve the knowledge on the previous tasks. However, we argue that some instances may be non-informative and inappropriate to construct the replay buffer under realistic setups (such as video streaming or imbalanced continual learning scenarios), leading to the degradation of the model's performance. Moreover, it is critical to select the valuable samples for current task training since the model can easily overfit to the biased and noisy data stream, which negatively affects the model generalization. To satisfy these desiderata, we propose minibatch similarity (S) and cross-batch diversity (V) criteria based on our aforementioned assumption to adaptively select the useful instances without the influence of outliers.

4

Definition 1 (Minibatch similarity). Let bt,n = {xt,n, yt,n}  Bt denote n-th pair of data point with gradient f (bt,n) and its corresponding label at task Tt. Let ¯ f(Bt) denote the averaged gradient vector of Bt. The minibatch similarity S (bt,n | Bt) between bt,n and Bt is given by

S (bt,n | Bt) =

f (bt,n) ¯ f (Bt) f (bt,n) · ¯ f (Bt)

.

(4)

Definition 2 (Cross-batch diversity). Let bt,n = {xt,n, yt,n}  Bt denote n-th pair of a data point with gradient f (bt,n) and its corresponding label at task Tt. The cross-batch diversity

V bt,n | Bt\bt,n between bt,n and all other instances in Bt (Bt\bt,n ) is given by

V

bt,n | Bt\bt,n

-1 Nt-1 =
Nt - 1 p=n

f (bt,n) f (bt,p) . f (bt,n) · f (bt,p)

(5)

In particular, minibatch similarity considers a minibatch as an approximation of the target dataset and compares the minibatch-level similarity between the gradient vector of a data point b and its minibatch B. It aligns with Assumption 1 and measures how well a given data instance describes the target task at each training step. Note that selecting examples with the largest minibatch similarity is reasonable when the variance of task instances is low; otherwise, it increases the redundancy among coreset items. In contrast, cross-batch diversity compares the diversity of each data point as the negative averaged similarity with other peer instances in the same minibatch.

4.1 Online Coreset Selection for Current Task Adaptation

The model receives a data continuum during training, including noisy or redundant data instances in real-world scenarios. Consequently, the arriving data instances can interrupt and hurt the performance of the model. To tackle this problem, we consider an amalgamation of minibatch similarity and cross-batch diversity to select the most helpful instances for current task training. More formally, our online coreset selection for the current task adaptation can be defined as follows:

u = argmax() S (bt,n | Bt) + V bt,n | Bt\bt,n n  {0, . . . , |Bt| - 1} .

(6)

n

We emphasize that we can obtain the top- valuable instances for the target task by computing

Equation 6 in an online manner. Once the representative coreset is selected, we optimize the

following objective for the current task training at each iteration:

1 minimize


(f (^x) , y^) , where Bt = Bt[u].

(7)

(^x,y^)Bt

We consider a selected coreset at each iteration as a candidate for the replay buffer. After the

completion of each task training, we choose a coreset Ct among the collected candidates, or we may also iteratively update Ct for continual learning as described in the following subsection.

4.2 Online Coreset Selection for Continual Learning

We now formulate OCS for online continual learning, where our objective is to obtain the coreset to retain the knowledge of the previous tasks using our proposed similarity and diversity selection criteria. However, continual learning is more challenging as the model suffers from catastrophic forgetting and coreset size is smaller than the size of the arriving data streams. Thus, inspired by our observation in Figure 3a and Figure 3b, we aim to train the continual learner on the selected instances that are representative of the current task and prevent the performance degeneration of previous tasks.

We achieve our goal by introducing our Coreset affinity criterion A to Equation 6. In particular, A computes the gradient vector similarity between a training sample and the coreset for previous tasks (C). More formally, A can be defined as follows:

Definition 3 (Coreset affinity). Let bt,n = {xt,n, yt,n}  Bt denote the n-th pair of a data point with gradient f (bt,n) and its corresponding label at task Tt. Further let ¯ f(BC) is the averaged gradient vector of BC, which is randomly sampled from the coreset C. The coreset affinity A (bt,n | BC  C) between bt,n and BC is given by

A (bt,n | BC  C) =

f (bt,n) ¯ f (BC) f (bt,n) · ¯ f (BC)

.

(8)

5

Algorithm 1 Online Coreset Selection (OCS)

input Dataset {Dt}Tt=1, neural network f, learning rate , hyperparameters ,  , replay buffer C  {}.

1: for task Tt = T1, . . . , TT do

2: Ct  {}

Initialize coreset for current task

3: for batch Bt  Dt do

4:

BC  SAMPLE(C)

Randomly sample a batch from the replay buffer

5:

u = argmax() S (bt,n | Bt) + V bt,n | Bt\bt,n +  A (bt,n | BC )

n{0,...,|Bt |-1}

Coreset selection

6:

Bt  Bt[u]

7:

   - f(Bt  BC) with Equation (10)

Model update with selected instances

8:

Ct  SELECT(Ct  Bt) with Equation (9)

9: end for

Coreset update

10: C  C  Ct 11: end for

Memorize coreset in the replay buffer

While the past task is inaccessible after the completion of its training, our selectively populated replay buffer can be effectively used to describe the knowledge of the previous tasks. The key idea is to select the examples that minimize the angle between the gradient vector of the coreset containing previous task examples and the current task examples. Instead of randomly replacing the candidates in the coreset [26, 9, 2, 5], A promotes the selection of examples that do not degenerate the model performance on previous tasks. To this end, we select the most beneficial training instances which are representative and diverse for current task adaptation while maintaining the knowledge of past tasks. In summary, our OCS for training task Tt during CL can be formulated as:

u = argmax() S (bt,n | Bt) + V bt,n | Bt\bt,n +  A (bt,n | BC) n  {0, . . . , |Bt| - 1} . (9)
n

 is a hyperparameter that controls the degree of model plasticity and stability. Note that, during the first task training, we do not have the interference from previous tasks and we select the top- instances that maximize the minibatch similarity and cross-batch diversity. Given the obtained coreset Bt = Bt[u], our optimization objective reflecting the coreset C, is as follows:

1



|BC |

minimize 
(^x,y^)Bt

(f(^x), y^) + |BC| (x,y)BC

(f(x), y),

(10)

where BC is a randomly sampled minibatch from the coreset C and  is a hyperparameter to balance the adaptation between the current task and past task coreset. Overall training procedure for Online Coreset Selection (OCS) is described in Algorithm 1. To the best of our knowledge, this is the first work that utilizes selective online training for the current task training and incorporates the relationship between the selected coreset and the current task instances to promote current task adaptation while minimizing the interference with previous tasks.

5 Experiments

5.1 Experimental Setup
Datasets. We validate OCS on class-incremental CL for Balanced and Imbalanced Rotated MNIST using a single-head two-layer MLP with 256 ReLU units in each layer, task-incremental CL for Split CIFAR-100 and Multiple Datasets (a sequence of five datasets) with a multi-head structured ResNet18 following prior works [9, 28, 29]. We perform five independent runs for all the experiments and provide further details on the experimental settings and datasets in Appendix A.
Baselines. We compare OCS with regularization-based CL methods: EWC [15] and Stable SGD [28], rehearsal-based CL methods using random replay buffer: A-GEM [9] and ER-Reservior [10], coresetbased methods using CL algorithms: Uniform Sampling, k-means Features [31] and k-means Embeddings [36], and coreset-based CL methods: iCaRL [32], Grad Matching [8], GSS [2], ERMIR [1], and Bilevel Optim [5]. We limit the buffer size for the rehearsal-based methods to one example per class per task. Additionally, we compare with Finetune, a naive CL method learnt on a sequence of tasks, and Multitask, where the model is trained on the complete data.

6

Table 1: Performance comparison of OCS and other baselines on balanced and imbalanced continual learning. We report the mean and standard-deviation of the average accuracy (Accuracy) and average forgetting (Forgetting) across five independent runs. The best results are highlighted in bold.

Method

Rotated MNIST

Split CIFAR-100

Multiple Datasets

Accuracy

Forgetting

Accuracy

Forgetting

Accuracy

Forgetting

Finetune EWC [15] Stable SGD [28]

46.3 (± 1.37) 0.52 (± 0.01) 40.4 (± 2.83) 0.31 (± 0.02) 49.8 (± 2.14) 0.23 (± 0.03) 70.7 (± 1.74) 0.23 (± 0.01) 48.5 (± 1.24) 0.48 (± 0.01) 42.7 (± 1.89) 0.28 (± 0.03) 70.8 (± 0.78) 0.10 (± 0.02) 57.4 (± 0.91) 0.07 (± 0.01) 53.4 (± 2.66) 0.16 (± 0.03)

Balanced CL

A-GEM [9] ER-Reservoir [10] Uniform Sampling iCaRL [32] k-means Features [31] k-means Embedding [36] Grad Matching [8] GSS [2] ER-MIR [1] Bilevel Optim [5]

55.3 (± 1.47) 69.2 (± 1.10) 79.9 (± 1.32) 80.7 (± 0.44) 79.1 (± 1.50) 80.6 (± 0.54) 78.5 (± 0.86) 76.0 (± 0.58) 80.7 (± 0.72) 80.7 (± 0.44)

0.42 (± 0.01) 0.21 (± 0.01) 0.14 (± 0.01) 0.13 (± 0.00) 0.14 (± 0.01) 0.13 (± 0.01) 0.15 (± 0.01) 0.19 (± 0.01) 0.14 (± 0.01) 0.14 (± 0.00)

50.7 (± 2.32) 46.9 (± 0.76) 58.8 (± 0.89) 60.3 (± 0.91) 59.3 (± 1.21) 55.5 (± 0.70) 60.0 (± 1.24) 59.7 (± 1.22) 60.2 (± 0.72) 60.1 (± 1.07)

0.19 (± 0.04) 0.21 (± 0.03) 0.05 (± 0.01) 0.04 (± 0.00) 0.06 (± 0.01) 0.06 (± 0.01) 0.04 (± 0.01) 0.04 (± 0.01) 0.04 (± 0.00) 0.04 (± 0.01)

-
-
56.0 (± 2.40) 59.4 (± 1.43) 53.6 (± 1.98) 55.4 (± 1.46) 57.8 (± 1.35) 60.2 (± 1.00) 56.9 (± 2,25) 58.1 (± 2.26)

-
-
0.11 (± 0.02) 0.07 (± 0.02) 0.14 (± 0.02) 0.11 (± 0.02) 0.08 (± 0.02) 0.07 (± 0.01) 0.11 (± 0.03) 0.08 (± 0.02)

OCS (Ours)

82.5 (± 0.32) 0.08 (± 0.00) 60.5 (± 0.55) 0.04 (± 0.01) 61.5 (± 1.34) 0.03 (± 0.01)

Multitask

89.8 (± 0.37)

-

71.0 (± 0.21)

-

57.4 (± 0.84)

-

Finetune Stable SGD [28]

39.8 (± 1.06) 0.54 (± 0.01) 45.3 (± 1.38) 0.17 (± 0.01) 27.6 (± 3.66) 0.22 (± 0.04) 52.0 (± 0.25) 0.19 (± 0.00) 48.7 (± 0.64) 0.03 (± 0.00) 29.5 (± 4.09) 0.20 (± 0.02)

Imbalanced CL

Uniform Sampling iCaRL [32] k-means Features [31] k-means Embedding [36] Grad Matching [8] GSS [2] ER-MIR [1] Bilevel Optim [5]

61.6 (± 1.72) 71.7 (± 0.69) 52.3 (± 1.48) 63.2 (± 0.90) 55.6 (± 1.86) 68.7 (± 0.98) 69.3 (± 1.01) 63.2 (± 1.04)

0.15 (± 0.01) 0.09 (± 0.00) 0.24 (± 0.01) 0.13 (± 0.02) 0.18 (± 0.02) 0.18 (± 0.01) 0.16 (± 0.01) 0.22 (± 0.01)

51.0 (± 0.78) 51.2 (± 1.09) 50.6 (± 1.52) 50.4 (± 1.39) 51.1 (± 1.14) 44.5 (± 1.35) 44.8 (± 1.42) 44.0 (± 0.86)

0.03 (± 0.00) 0.02 (± 0.00) 0.04 (± 0.01) 0.03 (± 0.01) 0.02 (± 0.00) 0.04 (± 0.01) 0.03 (± 0.01) 0.03 (± 0.01)

35.0 (± 3.03) 43.6 (± 2.95) 36.1 (± 1.75) 35.6 (± 1.35) 34.6 (± 0.50) 32.9 (± 0.90) 32.3 (± 3.49) 35.1 (± 2.78)

0.11 (± 0.03) 0.05 (± 0.03) 0.09 (± 0.02) 0.11 (± 0.02) 0.12 (± 0.01) 0.13 (± 0.01) 0.15 (± 0.03) 0.12 (± 0.02)

OCS (Ours)

76.5 (± 0.84) 0.08 (± 0.01) 51.4 (± 1.11) 0.02 (± 0.00) 47.5 (± 1.66) 0.03 (± 0.02)

Multitask

81.0 (± 0.95)

-

48.2 (± 0.72)

-

41.4 (± 0.97)

-

Balanced Rotated MNIST
95

90

85

80

75

Stable SGD Uniform

70

iCaRL Ours

0

4

T8ask inde1x2

16

20

Average test accuracy

Imbalanced Rotated MNIST
90

80

70

Stable SGD

60

Uniform iCaRL

Ours

50 0

4

T8ask inde1x2

16

20

First task test accuracy First task test accuracy

Balanced Rotated MNIST

90

80

70

Stable SGD

60

Uniform iCaRL

Ours

50 0

4

T8ask inde1x2

16

20

Imbalanced Rotated MNIST
90

80

70

Stable SGD

60

Uniform iCaRL

Ours

50 0

4

T8ask inde1x2

16

20

(a) Average test accuracy

(b) First task test accuracy

Figure 4: (a) Average accuracy (b) First task accuracy for balanced/imbalanced Rotated MNIST during CL.

Metrics. We evaluate all the methods on two metrics following the CL literature [9, 29].

1. Average Accuracy (At) is the averaged test accuracy of all tasks after the completion of CL at

task

Tt.

That

is,

At

=

1 t

t i=1

at,i,

where

at,i

is

the

test

accuracy

of

task

Ti

after

learning

task

Tt.

2. Average Forgetting (F ) is the averaged disparity between the peak and final task accuracy after

the completion of continual learning. That is, F

=

1 T -1

T -1 i=1

maxt{1,...,T -1}(at,i

-

aT ,i ).

5.2 Quantitative Analysis for Continual Learning

Balanced continual learning. Table 1 shows the results on the balanced CL benchmarks. First, observe that compared to the random replay based methods (A-GEM and ER-Reservoir), OCS shows 19% relative gain in average accuracy, 62% and 79% reduction in forgetting over the strongest baseline on Rotated MNIST and Split CIFAR-100, respectively. Second, OCS reduces the forgetting by 38% and 57% on Rotated MNIST and Multiple Datasets respectively over the coreset-based techniques, demonstrating that it selects valuable samples from the previous tasks. We further illustrate this in Figure 4, where OCS consistently exhibits superior average accuracy and first task accuracy. Third, we show the scalability of OCS with larger episodic memory in Figure 5. Interestingly, iCaRL shows lower performance than uniform sampling with a larger memory buffer for Rotated MNIST, while OCS outperforms across all memory sizes on both datasets. Furthermore, we note that ER-MIR, GSS, and Bilevel Optim require 0.9×, 3.9×, and 4.2× training time than OCS (see Table 5) on TITAN Xp, showing a clear advantage of OCS for the online streaming scenarios.

Average test accuracy

7

Table 2: Performance comparison of OCS and other baselines on varying proportions of noise instances during noisy continual learning. We report the mean and standard-deviation of the average accuracy (Accuracy) and average forgetting (Forgetting) across five independent runs. The best results are highlighted in bold.

Method

0%

40%

60%

Accuracy

Forgetting

Accuracy

Forgetting

Accuracy

Forgetting

Stable SGD [28] Uniform sampling iCaRL [32] k-means embedding [36] GSS [2] ER-MIR [1]

70.8 (± 0.78) 79.9 (± 1.32) 80.7 (± 0.44) 80.6 (± 0.54) 76.0 (± 0.58) 80.7 (± 0.72)

0.10 (± 0.02) 0.14 (± 0.01) 0.13 (± 0.00) 0.13 (± 0.01) 0.19 (± 0.01) 0.14 (± 0.01)

56.2 (± 0.95) 74.9 (± 2.45) 77.4 (± 0.60) 78.5 (± 0.86) 71.7 (± 0.95) 76.0 (± 1.34)

0.40 (± 0.01) 0.20 (± 0.03) 0.18 (± 0.01) 0.17 (± 0.00) 0.19 (± 0.01) 0.17 (± 0.01)

56.1 (± 0.62) 68.3 (± 3.68) 71.4 (± 2.63) 77.5 (± 1.67) 68.8 (± 1.02) 73.5 (± 0.94)

0.40 (± 0.01) 0.26 (± 0.03) 0.23 (± 0.03) 0.26 (± 0.03) 0.17 (± 0.02) 0.18 (± 0.01)

Average test accuracy Average test accuracy Average test accuracy Average test accuracy

OCS (Ours)

82.5 (± 0.32) 0.08 (± 0.00) 80.4 (± 0.20) 0.14 (± 0.00) 80.3 (± 0.75) 0.10 (± 0.01)

86

Balanced Rotated MNIST

85

84

83

82

81

Uniform

80

iCaRL Ours

79200

400Coreset size600

800

Imbalanced Rotated MNIST

80

75

70

65

Uniform iCaRL

Ours

60200

400Coreset size600

800

Balanced Multiple Datasets
62

59

56

Uniform

iCaRL

53

Ours

80

220Coreset size360

500

Imbalanced Multiple Datasets

44

Uniform

iCaRL

39

Ours

34 80 115 150 C1o8r5ese2t2s0ize 255 290 325

(a) Rotated MNIST

(b) Multiple Datasets

Figure 5: Performance comparison on various coreset sizes for balanced/imbalanced continual learning.

Imbalanced continual learning. To demonstrate the effectiveness of OCS in challenging scenarios, we evaluate on imbalanced CL in Table 1. We emphasize that compared to balanced CL, OCS shows significant gains over all the baselines for Rotated MNIST and Multiple Datasets. Notably, it leads to a relative improvement of  7% and  9% on the accuracy,  11% and 40% reduction on the forgetting compared to the best baseline for each dataset, respectively. The poor performance of the baselines in this setup is largely attributed to their lack of current task coreset selection, which results in a biased estimate degenerating model performance (see Figure 8). Moreover, we observe that OCS outperforms Multitask for complex imbalanced datasets, perhaps due to the bias from the dominant classes and the absence of selection criteria in Multitask. Similar to balanced CL, OCS leads to superior performance for larger episodic memory in imbalanced CL (see Figure 5).

Noisy continual learning. Next, we evaluate on noisy Rotated MNIST dataset, which is constructed by perturbing a proportion of instances of the original dataset with Gaussian noise N (0, 1). Table 2 shows that the addition of noise significantly degrades the performance on all the baselines. In contrast, OCS leads to a relative gain of 43% on accuracy, 20% and 35% reduction in forgetting on 40% and 60% proportion of noisy data. Note that the performance gap is more significant for the higher distribution of noisy examples, supporting our claim that the similarity and diversity across the training examples in the coreset play an essential role for the task adaptation in continual learning.

5.3 Ablation Studies
Effect of gradients. In Table 3, we empirically justify the utilization of gradients (Grad-OCS) for Assumption 1 compared to the raw inputs (Input-OCS) and feature-representations (Feat-OCS). We observe that Grad-OCS significantly outperforms Input-OCS and Feat-OCS on balanced and imbalanced CL, demonstrating that the gradients are a better metric to approximate the dataset.
Effect of individual components. We further dissect Minibatch similarity (S), Cross-batch diversity (V) and Coreset affinity (A) in Table 4. Note that selection using S shows reasonable performance as the model can select valuable data points; however, it may select redundant samples, which degrades its performance. In addition, V in isolation is insufficient since it can select non-redundant and non-representative instances. The combination of S and V improves the average accuracy, but it shows a marginal improvement on the forgetting. To further gain insight into S and V, we interpolate between S and V in Figure 6, where we can observe that an optimal balance of S and V (indicated by the arrows) can further improve the performance of our proposed selection strategy.
Furthermore, A improves the accuracy and forgetting, since the selected candidates have similar gradient direction to the coreset of the previous tasks maximizing their performance. However, A does not consider the current task distribution explicitly and depends on the quality of the memorized replay buffer. Finally, we can observe that the amalgamation of the three criteria improves the current task adaptation while alleviating the catastrophic forgetting simultaneously.

8

Table 3: Ablation study for analyzing the effect of gradients selection for OCS.

Method Balanced Rotated MNIST Imbalanced Rotated MNIST

Accuracy Forgetting Accuracy

Forgetting

Input-OCS Feat-OCS Grad-OCS

72.7 (± 0.47) 71.7 (± 0.62) 82.5 (± 0.32)

0.13 (± 0.01) 0.17 (± 0.01) 0.08 (± 0.00)

50.6 (± 1.74) 30.6 (± 0.40) 76.5 (± 0.84)

0.04 (± 0.00) 0.03 (± 0.01) 0.08 (± 0.01)

Table 6: Collaborative learning with rehearsal-based CL on various datasets with 20 tasks each.

MC-SGD [29]

MC-SGD + OCS

Dataset

Accuracy Forgetting Accuracy Forgetting

Per-MNIST Rot-MNIST Split CIFAR

84.6 (± 0.54) 82.3 (± 0.68) 58.4 (± 0.95)

0.06 (± 0.01) 0.08 (± 0.01) 0.02 (± 0.00)

86.6 (± 0.42) 85.1 (± 0.27) 59.1 (± 0.55)

0.02 (± 0.00) 0.04 (± 0.00) 0.00 (± 0.00)

Table 4: Ablation study to investigate the impact of

selection criteria S, V, and A on OCS.

Method Noisy Rot-MNIST (60%)

Multiple Datasets

S V A Accuracy Forgetting Accuracy Forgetting

- - 64.0 (± 1.18) 0.33 (± 0.01) 56.3 (± 0.97) 0.10 (± 0.02)

-

- 40.1 (± 1.32) 0.06 (± 0.02) 49.8 (± 1.30) 0.12 (± 0.01)

--

79.6 (± 0.87) 0.10 (± 0.01) 58.1 (± 0.96) 0.05 (± 0.01)

- 66.8 (± 1.39) 0.30 (± 0.01) 58.6 (± 1.91) 0.09 (± 0.02)

80.3 (± 0.75) 0.10 (± 0.01) 61.5 (± 1.34) 0.03 (± 0.01)

0

0

0

0

5

5

5

5

10

10

10

10

15

15

15

15

20

20

20

20

25

25

25

25

0 5 10 15 20 25

0 5 10 15 20 25

0 5 10 15 20 25

0 5 10 15 20 25

(a) Uniform (imbalanced)

0

0

0

0

5

5

5

5

10

10

10

10

15

15

15

15

20

20

20

20

25

25

25

25

0 5 10 15 20 25

0 5 10 15 20 25

0 5 10 15 20 25

0 5 10 15 20 25

(d) Uniform (noisy)

0

0

0

0

5

5

5

5

10

10

10

10

15

15

15

15

20

20

20

20

25

25

25

25

0 5 10 15 20 25

0 5 10 15 20 25

0 5 10 15 20 25

0 5 10 15 20 25

(b) iCaRL (imbalanced)

0

0

0

0

5

5

5

5

10

10

10

10

15

15

15

15

20

20

20

20

25

25

25

25

0 5 10 15 20 25

0 5 10 15 20 25

0 5 10 15 20 25

0 5 10 15 20 25

(e) iCaRL (noisy)

0

0

0

0

5

5

5

5

10

10

10

10

15

15

15

15

20

20

20

20

25

25

25

25

0 5 10 15 20 25

0 5 10 15 20 25

0 5 10 15 20 25

0 5 10 15 20 25

(c) Ours (imbalanced)

0

0

0

0

5

5

5

5

10

10

10

10

15

15

15

15

20

20

20

20

25

25

25

25

0 5 10 15 20 25

0 5 10 15 20 25

0 5 10 15 20 25

0 5 10 15 20 25

(f) Ours (noisy)

Figure 7: Randomly picked coreset examples. Top: Imbalanced Rotated MNIST. Bottom: Noisy Rotated MNIST with 60% of noisy instances.

Table 5: Running time on Balanced Rot-MNIST.

Method

Training Time

Figure 6: Interpolation between S and V.

ER-MIR [1] GSS [2] Bilevel [5] OCS (Ours)

0.38 h (×0.87) 1.71 h (×3.89) 1.83 h (×4.17) 0.44 h (×1.00)

5.4 Further Analysis

Figure 8: T-SNE visualization of the selected samples on Imbalanced Rotated MNIST.

Coreset visualization. Next, we visualize the coreset selected by different methods for imbalanced and noisy rotated MNIST in Figure 7. We observe that uniform sampling selects highly biased samples representing the dominant classes for imbalanced CL and noisy instances for noisy CL. In contrast, iCaRL selects the representative samples per class for imbalanced CL; however, it selects noisy instances during noisy CL. In comparison, OCS selects the beneficial examples for each class during imbalanced CL and discards uninformative noisy instances in the noisy CL training regime.
T-SNE visualization. We further compare the T-SNE visualization of the selected coreset by Bilevel Optim, GSS and OCS in Figure 8. We observe that the samples chosen by OCS are diverse, whereas Bilevel Optim and GSS select the majority of the samples from the dominant classes. We attribute the representative clusters and diversity in the samples selected by OCS to our proposed S (selects the valuable samples) and V (minimizes the redundancy among the selected samples) criteria.
Collaborative learning with MC-SGD. We remark that OCS can be applied to any rehearsal-based CL method with a replay buffer during training. We empirically demonstrate the effect of collaborative learning with other CL methods in Table 6. In particular, we use Mode Connectivity SGD (MCSGD) [29], which encourages the mode connectivity between model parameters for continual and multitask loss and approximates the multitask loss through randomly selected replay buffer. Note that OCS leads to a relative gain of 1.2% to 3.4% on accuracy over MC-SGD on Permuted MNIST, Rotated MNIST, and Split CIFAR-100 datasets. Furthermore, MC-SGD + OCS shows considerably lower forgetting, illustrating that OCS prevents the loss of prior task knowledge.

6 Conclusion
We propose Online Coreset Selection (OCS), a novel approach for coreset selection during online continual learning. Our approach is modelled as a gradient-based selection strategy that selects representative and diverse instances, which are useful for preserving the knowledge of the previous tasks at each iteration. This paper takes the first step to utilize the coreset for improving the current task adaptation, while mitigating the catastrophic forgetting on previous tasks. Our experimental evaluation on the standard balanced continual learning datasets against state-of-the-art rehearsal-based techniques demonstrates the efficiency of our approach. We also show promising results on various realistic and challenging imbalanced and noisy continual learning datasets. We further show the natural extension of our selection strategy to existing rehearsal-based continual learning using a random-replay buffer. Our future work will focus on improving the selection strategies and exploring ways to utilize unlabelled data stream during training.

9

References
[1] R. Aljundi, E. Belilovsky, T. Tuytelaars, L. Charlin, M. Caccia, M. Lin, and L. Page-Caccia. Online continual learning with maximal interfered retrieval. In Advances in Neural Information Processing Systems (NeurIPS), 2019.
[2] R. Aljundi, M. Lin, B. Goujaud, and Y. Bengio. Gradient based sample selection for online continual learning. In Advances in Neural Information Processing Systems (NeurIPS), 2019.
[3] C. Baweja, B. Glocker, and K. Kamnitsas. Towards continual learning in medical imaging. arXiv preprint arXiv:1811.02496, 2018.
[4] M. Biesialska, K. Biesialska, and M. R. Costa-jussà. Continual lifelong learning in natural language processing: A survey. arXiv preprint arXiv:2012.09823, 2020.
[5] Z. Borsos, M. Mutny`, and A. Krause. Coresets via bilevel optimization for continual learning and streaming. In Advances in Neural Information Processing Systems (NeurIPS), 2020.
[6] T. Broderick, N. Boyd, A. Wibisono, A. C. Wilson, and M. I. Jordan. Streaming variational bayes. In Advances in Neural Information Processing Systems (NeurIPS), 2013.
[7] Y. Bulatov. Not-mnist dataset. 2011.
[8] T. Campbell and T. Broderick. Automated scalable bayesian inference via hilbert coresets. Journal of Machine Learning Research (JMLR), 2019.
[9] A. Chaudhry, M. Ranzato, M. Rohrbach, and M. Elhoseiny. Efficient lifelong learning with a-gem. In Proceedings of the International Conference on Learning Representations (ICLR), 2019.
[10] A. Chaudhry, M. Rohrbach, M. Elhoseiny, T. Ajanthan, P. K. Dokania, P. H. Torr, and M. Ranzato. Continual learning with tiny episodic memories. arXiv preprint arXiv:1902.10486, 2019.
[11] Y. Cui, M. Jia, T.-Y. Lin, Y. Song, and S. Belongie. Class-balanced loss based on effective number of samples. In Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), 2019.
[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
[13] T. B. Johnson and C. Guestrin. Training deep models faster with robust, approximate importance sampling. In Advances in Neural Information Processing Systems (NeurIPS), 2018.
[14] A. Katharopoulos and F. Fleuret. Not all samples are created equal: Deep learning with importance sampling. In Proceedings of the International Conference on Machine Learning (ICML), 2018.
[15] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, page 201611835, 2017.
[16] W. Kool, H. Van Hoof, and M. Welling. Stochastic beams and where to find them: The gumbeltop-k trick for sampling sequences without replacement. In Proceedings of the International Conference on Machine Learning (ICML), 2019.
[17] A. Krizhevsky. Learning multiple layers of features from tiny images. University of Toronto, 05 2012.
[18] A. Kumar and H. Daume III. Learning task grouping and overlap in multi-task learning. In Proceedings of the International Conference on Machine Learning (ICML), 2012.
[19] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 1998.
[20] C. S. Lee and A. Y. Lee. Clinical applications of continual learning machine learning. The Lancet Digital Health, 2020.
[21] S.-W. Lee, J.-H. Kim, J. Jun, J.-W. Ha, and B.-T. Zhang. Overcoming catastrophic forgetting by incremental moment matching. In Advances in Neural Information Processing Systems (NeurIPS), 2017.
10

[22] M. Lenga, H. Schulz, and A. Saalbach. Continual learning for domain adaptation in chest x-ray classification. In Medical Imaging with Deep Learning, 2020.
[23] X. Li, Y. Zhou, T. Wu, R. Socher, and C. Xiong. Learn to grow: A continual structure learning framework for overcoming catastrophic forgetting. In Proceedings of the International Conference on Machine Learning (ICML), 2019.
[24] Y. Li, L. Zhao, K. Church, and M. Elhoseiny. Compositional language continual learning. In Proceedings of the International Conference on Learning Representations (ICLR), 2019.
[25] Z. Li and D. Hoiem. Learning without forgetting. In Proceedings of the European Conference on Computer Vision (ECCV), 2016.
[26] D. Lopez-Paz and M. Ranzato. Gradient episodic memory for continual learning. In Advances in Neural Information Processing Systems (NeurIPS), 2017.
[27] M. McCloskey and N. J. Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. In Psychology of learning and motivation. 1989.
[28] S. I. Mirzadeh, M. Farajtabar, R. Pascanu, and H. Ghasemzadeh. Understanding the role of training regimes in continual learning. In Advances in Neural Information Processing Systems (NeurIPS), 2020.
[29] S. I. Mirzadeh, M. Farajtabar, D. Gorur, R. Pascanu, and H. Ghasemzadeh. Linear mode connectivity in multitask and continual learning. In Proceedings of the International Conference on Learning Representations (ICLR), 2021.
[30] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in natural images with unsupervised feature learning. 2011.
[31] C. V. Nguyen, Y. Li, T. D. Bui, and R. E. Turner. Variational continual learning. In Proceedings of the International Conference on Learning Representations (ICLR), 2018.
[32] S.-A. Rebuffi, A. Kolesnikov, G. Sperl, and C. H. Lampert. icarl: Incremental classifier and representation learning. In Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), 2017.
[33] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, and R. Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016.
[34] S. Sadhu and H. Hermansky. Continual learning in automatic speech recognition. In Interspeech, 2020.
[35] M.-A. Sato. Online model selection based on the variational bayes. Neural computation, 2001.
[36] O. Sener and S. Savarese. Active learning for convolutional neural networks: A core-set approach. In Proceedings of the International Conference on Learning Representations (ICLR), 2018.
[37] J. Serrà, D. Surís, M. Miron, and A. Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. In Proceedings of the International Conference on Machine Learning (ICML), 2018.
[38] S. Sinha, J. Song, A. Garg, and S. Ermon. Experience replay with likelihood-free importance weights. arXiv preprint arXiv:2006.13169, 2020.
[39] J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel. The german traffic sign recognition benchmark: a multi-class classification competition. In The 2011 international joint conference on neural networks, 2011.
[40] S. Thrun. A Lifelong Learning Perspective for Mobile Robot Control. Elsevier, 1995.
[41] M. K. Titsias, J. Schwarz, A. G. d. G. Matthews, R. Pascanu, and Y. W. Teh. Functional regularisation for continual learning with gaussian processes. In Proceedings of the International Conference on Learning Representations (ICLR), 2020.
[42] H. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.
[43] J. Xu and Z. Zhu. Reinforced continual learning. In Advances in Neural Information Processing Systems (NeurIPS), 2018.
11

[44] J. Yoon, E. Yang, J. Lee, and S. J. Hwang. Lifelong learning with dynamically expandable networks. In Proceedings of the International Conference on Learning Representations (ICLR), 2018.
[45] J. Yoon, S. Kim, E. Yang, and S. J. Hwang. Scalable and order-robust continual learning with additive parameter decomposition. In Proceedings of the International Conference on Learning Representations (ICLR), 2020.
[46] J. Yoon, W. Jeong, G. Lee, E. Yang, and S. J. Hwang. Federated continual learning with weighted inter-client transfer. In Proceedings of the International Conference on Machine Learning (ICML), 2021.
[47] M. Zhai, L. Chen, F. Tung, J. He, M. Nawhal, and G. Mori. Lifelong gan: Continual learning for conditional image generation. In Proceedings of the International Conference on Computer Vision (ICCV), 2019.
12

Appendix
Organization. The appendix is organized as follows: We first provide the experimental setups, including the dataset construction for balanced, imbalanced, noisy continual learning, and the hyperparameter configurations for OCS and all baselines in Appendix A. Next, we evaluate the selection criteria of the baselines for current task training and provide an additional ablation study comparing OCS with current task training to uniform selection in Appendix B.

A Experimental Details

Datasets. We evaluate the performance of OCS on the following benchmarks:

1. Balanced and Imbalanced Rotated MNIST. These datasets are MNIST handwritten digits dataset [19] variants containing 20 tasks, where each task applies a fixed random image rotation (between 0 and 180 degrees) to the original dataset. The imbalanced setting contains a different number of training examples for each class in a task, where we randomly select 8 classes over 10 at each task and each class contains 10% of training instances for training. For example, the total amount of training instances at each class can be [5900, 670, 590, 610, 580, 5400, 590, 620, 580, 590], where bold fonts denote the reduced number of instances for selected classes. The size of the replay buffer is 200 for all the rehearsal-based methods.
2. Balanced and Imbalanced Split CIFAR-100. These datasets are CIFAR-100 dataset [17] variants, where each task consists of five random classes out of the 100 classes. We use the Long-Tailed CIFAR-100 [11] for Imbalanced Split CIFAR-100 consisting of n = niµi samples for each class, where i is the class index, ni is the original number of training images, and µ = 0.05. It contains 20 tasks of five random classes out of the 100 classes. The size of the replay buffer is 100 (one example per class) for all the rehearsal-based methods.
3. Balanced and Imbalanced Multiple Datasets. This dataset contains a sequence of five benchmark datasets: MNIST [19], fashion-MNIST [42], NotMNIST [7], Traffic Sign [39], and SVHN [30], where each task contains randomly selected 1000 training instances from each dataset. This dataset contains five tasks and 83 classes. We use the same strategy as Long-Tailed CIFAR-100 to construct the imbalanced Multiple Datasets with µ = 0.1. The size of the replay buffer is 83 (one example per class) for all rehearsal-based methods.

Network Architectures. We use a MLP with 256 ReLU units in each layer for the Rotated MNIST experiments ResNet-18 [12] architecture for Split CIFAR-100 datasets following Mirzadeh et al. [28]. For Rotated MNIST experiments, we use a single-head architecture, where the final classifier layer is shared across all the tasks, and the task identity is not provided during inference. In contrast, we use the multi-head structured ResNet-18 for CIFAR-100 and Multiple Datasets experiments, where the task identifiers are provided, and each task consists of its individual linear classifier.

Implementations. We follow the design of Mirzadeh et al. [28] for evaluating all the methods. We utilize their implementation for Finetune, EWC [15], Stable SGD [28], A-GEM [9], ER-Reservoir [10] and MC-SGD [28]. We adapt the implementation released by Borsos et al. [5] for Uniform Sampling, iCaRL [32], k-means Features [31], k-means Embedding [36], Grad Matching [8], and Bilevel Optim [5]. Further, we implement GSS [2] and ER-MIR [1] following the official code relased by the authors. Following iCaRL [32], we store a balanced coreset Ct (equal number of examples per class) among the collected coreset candidates.

Hyperparameter configurations. Table A.7 shows the

initial learning rate, learning rate decay, and batch size Table A.7: Shared Hyperparameter configufor each dataset that are shared among all the meth- rations among our method and baselines for

ods. Further, we report the best results obtained for three datasets.

  {0.01, 0.05, 0.1, 1, 10, 50, 100} for all the experi-

ments. For OCS, we use  = 1000 for all the experiments and batch size as 100 for Rotated MNIST and 20 for Split CIFAR-100 and Multiple Dataset. The running time reported in Table 5 was measured on a single

Parameter
Initial LR LR decay Batch size

Rotated MNIST
0.005 [0.75, 0.8]
10

Split CIFAR-100
0.15 0.875
10

Multiple Datasets
0.1 0.85 10

NVIDIA TITAN Xp. Due to the significant computational

cost incurred by the training of Bilevel Optim [5] for on-

line continual learning, we restrict the bilevel optimization procedure to construct the replay buffer at

the end of each task training.

13

Table B.8: Performance comparison of baselines for current task adaptation. We report the mean and standard-deviation of the average accuracy (Accuracy) and average forgetting (Forgetting) across five independent runs.

Method

Balanced Rotated MNIST

Imbalanced Rotated MNIST

Accuracy

Forgetting

Accuracy

Forgetting

Finetune Stable SGD

46.3 (± 1.37) 70.8 (± 0.78)

0.52 (± 0.01) 0.10 (± 0.02)

39.8 (± 1.06) 52.0 (± 0.25)

0.54 (± 0.01) 0.19 (± 0.00)

Uniform Sampling iCaRL k-means Features k-means Embedding Grad Matching

78.9 (± 1.16) 70.9 (± 0.82) 77.9 (± 1.08) 78.1 (± 1.53) 79.0 (± 1.11)

0.14 (± 0.00) 0.12 (± 0.01) 0.15 (± 0.01) 0.14 (± 0.01) 0.15 (± 0.01)

63.5 (± 1.09) 70.0 (± 0.60) 66.6 (± 2.42) 67.2 (± 0.16) 52.4 (± 1.34)

0.14 (± 0.02) 0.10 (± 0.01) 0.12 (± 0.02) 0.10 (± 0.01) 0.19 (± 0.02)

OCS (Ours)

82.5 (± 0.32) 0.08 (± 0.00) 76.5 (± 0.84) 0.08 (± 0.01)

Multitask

89.8 (± 0.37)

-

81.0 (± 0.95)

-

Table B.9: Performance comparison between uniform training with OCS coreset and original OCS method.

(a) Balanced Continual Learning

(b) Imbalanced Continual Learning

Uniform + OCS

OCS

Uniform + OCS

OCS

Dataset

Accuracy Forgetting Accuracy Forgetting

Dataset

Accuracy Forgetting Accuracy Forgetting

Rot-MNIST CIFAR
Mul. Datasets

80.4 (± 0.61) 60.0 (± 1.30) 56.3 (± 1.42)

0.14 (± 0.01) 0.04 (± 0.00) 0.08 (± 0.03)

82.5 (± 0.32) 60.5 (± 0.55) 61.5 (± 1.34)

0.08 (± 0.00) 0.04 (± 0.01) 0.03 (± 0.01)

Rot-MNIST CIFAR
Mul. Datasets

73.6 (± 2.31) 51.3 (± 1.31) 41.4 (± 2.51)

0.11 (± 0.01) 0.03 (± 0.01) 0.05 (± 0.03)

76.5 (± 0.84) 51.4 (± 1.11) 47.5 (± 1.66)

0.08 (± 0.00) 0.02 (± 0.00) 0.03 (± 0.02)

B Additional Experiments
Current task adaptation with the baselines. One of our main contributions is the selective online training that selects the important samples for current task training. Therefore, we investigate the application of the other baselines for current task training in Table B.8. It is worth noting that all the rehearsal-based baselines that utilize their coreset selection criteria for current task adaptation decrease the performance (1.0 - 9.8%p ), except Grad Matching (0.5%p ) on Balanced Rotated MNIST. Moreover, for Imbalanced Rotated MNIST, Uniform Sampling, k-means Features, and k-means Embedding increase the performance 1.9%p, 13.3%p, and 4.0%p compared to Table 1 respectively. In contrast, iCaRL and Grad Matching criteria decrease the performance by 1.7%p and 3.2%p on Imbalanced Rotated MNIST, respectively. On the contrary, OCS improves the performance for both the balanced and imbalanced scenarios. In light of this, we can conclude that efficient coreset selection plays a crucial role in imbalanced and noisy continual learning; therefore, the future rehearsal-based continual learning methods should evaluate their method on realistic settings rather than the standard balanced continual learning benchmarks.
Uniform training with OCS coreset. We further analyze the effect of online coreset selection for current task training in Table B.9a. In particular, we compare uniform sampling for the current task while utilizing the coreset constructed by OCS for the previous tasks (Uniform + OCS) with our original selection scheme utilizing OCS for current and previous tasks. First, observe that Uniform + OCS shows 2.6% and 9.2% relative decrease in performance on Rotated MNIST and Multiple datasets respectively compared to our original OCS selection strategy. Second, note that Uniform + OCS significantly deteriorates the catastrophic forgetting for all datasets since uniformly sampled examples do not preserve the previous tasks knowledge. Moreover, imbalanced continual learning shows a similar trend in Table B.9b, where Uniform + OCS leads to a drop in both the accuracy and forgetting across all benchmarks. This further strengthens our claim that OCS is essential for the previous tasks and plays a vital role in encouraging current-task adaptation.
C Limitations
Although our simple yet effective selection strategy improves the performance across various realistic continual learning scenarios, it is still far from the desirable performance. However, we believe that we have provided a novel insight into the utility of gradients for summarizing the dataset, which improves the current task adaptation and mitigates catastrophic forgetting. We hope that our work will lead to follow-up works that exploit our findings and develop methods, which are scalable to practical continual learning scenarios. Furthermore, our method is limited to supervised learning, where arriving data points require a large amount of annotated class labels. We leave the exploration of OCS for unsupervised data streams for future research.

14

D Societal Impact
In this paper, we introduce Online Coreset Selection, a novel rehearsal-based continual learning method to select representative and diverse samples for online continual learning. Our proposed selection strategy is likely to have a high impact on continual learning as most of the existing methods do not select the coreset before the current task training, which can introduce bias in the model in the realistic continual learning scenarios. Additionally, our selective online training would be vital for memory-constrained applications such as surveillance systems, autonomous driving, social media, and e-commerce, where the data stream is often composed of biased and noisy samples and the system's security plays an essential role in the practicality of the system. Regarding the risks of our proposed system, an adversary might try to deceive the privacy of the system given the knowledge of the coreset. To mitigate this, we encourage the research community to investigate the robustness of the rehearsal-based continual learning methods against membership inference and model stealing attacks to make the system exhaustive and robust to diverse attacks.
15

