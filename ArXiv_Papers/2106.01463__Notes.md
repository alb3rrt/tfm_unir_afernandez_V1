
# Lightweight Adapter Tuning for Multilingual Speech Translation

[arXiv](https://arxiv.org/abs/2106.01463), [PDF](https://arxiv.org/pdf/2106.01463.pdf)

## Authors

- Hang Le
- Juan Pino
- Changhan Wang
- Jiatao Gu
- Didier Schwab
- Laurent Besacier

## Abstract

Adapter modules were recently introduced as an efficient alternative to fine-tuning in NLP. Adapter tuning consists in freezing pretrained parameters of a model and injecting lightweight modules between layers, resulting in the addition of only a small number of task-specific trainable parameters. While adapter tuning was investigated for multilingual neural machine translation, this paper proposes a comprehensive analysis of adapters for multilingual speech translation (ST). Starting from different pre-trained models (a multilingual ST trained on parallel data or a multilingual BART (mBART) trained on non-parallel multilingual data), we show that adapters can be used to: (a) efficiently specialize ST to specific language pairs with a low extra cost in terms of parameters, and (b) transfer from an automatic speech recognition (ASR) task and an mBART pre-trained model to a multilingual ST task. Experiments show that adapter tuning offer competitive results to full fine-tuning, while being much more parameter-efficient.

## Comments

Accepted at ACL-IJCNLP 2021

## Source Code

Official Code



Community Code

- [https://paperswithcode.com/paper/lightweight-adapter-tuning-for-multilingual](https://paperswithcode.com/paper/lightweight-adapter-tuning-for-multilingual)

## Bibtex

```tex
@misc{le2021lightweight,
      title={Lightweight Adapter Tuning for Multilingual Speech Translation}, 
      author={Hang Le and Juan Pino and Changhan Wang and Jiatao Gu and Didier Schwab and Laurent Besacier},
      year={2021},
      eprint={2106.01463},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```

## Notes

Type your reading notes here...

