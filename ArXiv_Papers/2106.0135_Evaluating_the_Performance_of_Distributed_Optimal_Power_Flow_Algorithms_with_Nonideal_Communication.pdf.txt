1
Evaluating the Performance of Distributed Optimal Power Flow Algorithms with Nonideal Communication
Mohannad Alkhraijah, Student Member, IEEE, Carlos Menendez, Student Member, IEEE, and Daniel K Molzahn, Senior Member, IEEE

arXiv:2106.00135v1 [eess.SY] 31 May 2021

Abstract--Power system operators are increasingly looking toward distributed optimization to address various challenges facing electric power systems. To assess their capabilities in environments with nonideal communications, this paper investigates the impacts of data quality on the performance of distributed optimization algorithms. Specifically, this paper compares the performance of the Alternating Direction Method of Multipliers (ADMM), Analytic Target Cascading (ATC), and Auxiliary Principal Problem (APP) algorithms in the context of DC Optimal Power Flow (DC OPF) problems. Using several test cases, this paper characterizes the performance of these algorithms in terms of their convergence rates and solution quality under three data quality nonidealities: (1) additive Gaussian noise, (2) false data, and (3) intermittent communication failure.
Index Terms--Distributed Optimization, Nonideal Communication, Optimal Power Flow.
I. INTRODUCTION
T RADITIONALLY, power system operations are primarily conducted centrally, where the tasks of modeling the system's components, solving an optimization problem, and dispatching setpoints are performed by the system operator. However, as power systems move toward a more decentralized paradigm with many locally controlled microgrids, scaling these tasks becomes challenging when using centralized methods. Further, modeling connected systems is difficult, especially for microgrids in a distribution network.
Distributed algorithms address these challenges by allowing interconnected systems with local controllers to cooperatively solve large optimization problems. This increases flexibility as each microgrid solves a local subset of the original optimization problem. Distributed algorithms thus have various potential advantages, such as allowing parallel computations, reducing the requisite communication infrastructure, and possibly maintaining the subsystems' autonomy and data privacy.
These advantages motivate solving Optimal Power Flow (OPF) problems in a distributed fashion. OPF problems seek optimal setpoints for a power system. The decision variables are typically the generators' power outputs and the voltage phasors. Several distributed optimization algorithms have been proposed for solving OPF problems [1]. The Alternating Direction Method of Multipliers (ADMM) applies Augmented Lagrangian Relaxation to decompose the centralized problems into smaller problems which permit a distributed implementation [2]. Augmented Lagrangian Relaxation is also used in other distributed OPF solution algorithms, such as Analytic Target Cascading (ATC) [3], Auxiliary Principle Problem (APP) [4], and Dual Decomposition. Other distributed algorithms directly consider the Karush-Kuhn-Tucker (KKT) optimality conditions for OPF problems. These include the Optimality Condition Decomposition and Consensus+Innovation

algorithms. Numerical comparisons among these distributed OPF algorithms are presented in [5] and [6]. Further, a numerical analysis presented in [7] shows that distributed optimization algorithms might converge to suboptimal solutions for nonconvex problems, depending on the initialization.
Communication plays a major role in implementing distributed optimization since regions share data with their neighbors [8]. The power systems literature predominantly assumes that the communication is ideal, with each region receiving the exact values sent from neighboring regions. However, shared data between interconnected regions can be subject to communication errors and malicious attacks. The authors of [6] investigate the impact of injecting false values into the shared data during the ATC algorithm's iterative process and show that the error injection increases the convergence time. Nevertheless, it is difficult to derive a solid conclusion about the robustness of the algorithms when considering one type of communication nonideality with a single erroneous value injection. False data injection is also studied in the context of distributed algorithms for state estimation problems in [9].
In more general distributed optimization settings, the impacts of communication noise due to quantization effects have been investigated in [10]­[12]. The authors of [13] provide analytic upper and lower bounds on the ADMM algorithm's performance in the presence of random errors and validate the results using randomly generated networks. The authors of [14] study the robustness of distributed algorithms based on dual averaging to additive communication noise. The authors of [15] investigate the ADMM algorithm's convergence under additive communication noise and recommend modifications to the underlying optimization problem. Packet loss is another communication issue that has been discussed in the literature. The impact of packet loss on unconstrained convex distributed optimization is investigated in [16]. A relaxed ADMM algorithm is proposed in [17] to optimize integrated electrical and heating systems considering communication packet loss.
In this paper, we compare three distributed algorithms (ADMM, ATC, and APP) with respect to their performance in solving OPF problems in the presence of nonideal communications. We characterize the algorithms' performance in terms of convergence speed and solution quality. To serve as a benchmark, we first compare the performance of the three algorithms with ideal communications. We then consider three noise models that impact the shared data between neighboring regions: (1) additive Gaussian noise, (2) false data, and (3) intermittent communication loss. We use three test cases to evaluate the performance of the algorithms with nonideal communication. This paper considers the DC OPF formulation which uses the DC power flow linearization [18]. Thus, this paper's main contribution is an extensive empirical

2

study regarding the impacts of nonideal communication on various distributed solution algorithms for DC OPF problems.
We note that the convexity of the DC OPF formulation studied in this paper provides advantages in terms of theoretical convergence guarantees for the distributed algorithms. These guarantees are useful in the context of this paper as they ensure that the distributed algorithms should all converge to the same solution, thus permitting consistent comparisons that primarily focus on the speed and robustness of the algorithms. However, there are applications where the DC power flow approximation is inappropriate, thus requiring alternative power flow representations [19]. This paper forms a basis for future extensions of our analyses for these applications.
The rest of the paper is organized as follows. Section II introduces the mathematical notation and the DC OPF problem. Section III formulates the problem decomposition and the distributed solution algorithms. Section IV describes the noise models we use in the analysis. Section V presents numerical results and discusses the distributed algorithms' performance under nonideal communication. Section VI summarizes the main findings and discusses future work.

generators' power outputs. Constraint (1b) is the DC approximation of the power flow equations [18]. Constraints (1c) and (1d) limit the generators' power outputs and the line flows. Constraint (1e) sets the reference angle,  re f .
III. DISTRIBUTED OPTIMIZATION ALGORITHMS
This section overviews the distributed algorithms considered in this paper. These algorithms decompose the centralized DC OPF problem into subproblems corresponding to a partition of the original system. In the DC OPF formulation (1), tie-lines between regions couple the power flow equations (1b) and the line flow limits (1d). For each tie-line, we duplicate the boundary variables and assign a local copy to each region as shown in Fig. 1. Each region solves a local optimization problem with the copied variables for neighboring regions.

Remark on Notation
Throughout the paper, we use bold letters to indicate vectors. We denote a local subproblem and regions with the letter m and let M indicate the set of all subproblems. The set Nsm contains the boundary variables for region m. We use the letters n and c to denote variables from the neighboring regions and the central coordinator. Further, we use the hat in x^ to indicate that the variables x are evaluated with constant values from the previous iteration as received from neighboring regions or obtained from the prior local solution.
II. DC OPTIMAL POWER FLOW FORMULATION
OPF problems typically minimize generation costs while satisfying the power flow equations and limits on generator outputs, line flows, etc. The optimization variables are the generators' outputs and the bus voltages. We consider the DC power flow approximation [18] to obtain convergence guarantees for the distributed algorithms, which are discussed in Section III. The DC OPF formulation used in this paper is:

 min

fi ( pi )

 ,p iG

(1a)

 s.t. pi - di =

Bi j(i -  j),

(i, j)L

i  B (1b)

Pimin  pi  Pimax,

i  G (1c)

- Pimj ax  Bi j(i -  j)  Pimj ax, (i, j)  L (1d)

 re f = 0

(1e)

where the sets B, G , and L denote the buses, generators,
and lines, respectively. The generation costs are denoted by
fi. The decision variables are  , the bus voltage angles, and p, the generators' active power outputs. We denote the power
demands by d, and B denotes the lines' susceptances. Bounds on the power output of generator i are Pimax and Pimin, and Pimj ax defines the flow limit of the line between buses i and j. The objective function in (1a) minimizes the total cost of the

Fig. 1. Tie-line model before (a) and after (b) applying the decomposition.

To obtain a feasible solution to the centralized problem, we need to ensure consistency between the duplicated variables in each subproblem. To accomplish this, we consider algorithms that use an Augmented Lagrangian method to enforce consistency using a penalized objective function:

  L (p, , ) := iG

fi(

pi)

+

iNs

i(i

-

i

)

+

 2

||i

-

i

||22,

(2)

where  are the Lagrange multipliers of the consistency constraints,  are copies of the boundary variables, and  is a parameter. The set Ns contains all copies of the bus voltage angle variables corresponding to the tie-lines between regions.
The three distributed algorithms we consider are similar in their application of an Augmented Lagrangian to decompose the original problem. However, they use different processes for evaluating the objective function and updating the Lagrange multiplier  . Generally, the three algorithms use the values received from neighboring regions to evaluate the shared variables in the objective function. This makes the subproblems separable and independent from each other. The local controllers can then solve their subproblems independently and share the result of the boundary variables with the neighboring regions. In the next iteration, the shared variables received from the neighboring regions are used to evaluate the relaxed consistency constraint in the objective function of each subproblem. The algorithms repeat these steps until all regions reach a consensus on the shared variables' values. We next provide more detail for these distributed algorithms.

A. Alternating Direction Method of Multipliers (ADMM)
First introduced in the 1970s [20], [21], ADMM is a wellknown algorithm for solving large optimization problems. The

3

ADMM algorithm solves the augmented Lagrangian problem in a distributed fashion that is similar to the Gauss-Siedel iterative method. Solving the OPF problem using ADMM in a distributed way involves a central coordinator to achieve consensus among the local controllers. In the ADMM algorithm, the local controllers iteratively solve their local problems to obtain a solution for the local variables  m while considering the neighboring regions' variables received from the central coordinator,  c, as constants. The value of  c is provided by the central coordinator from the previous iteration. The local problem of region m at iteration k + 1 is:

  min
pk+1, m,k+1 iG m

fi(pki +1) +

ik (^ic,k

iNsm

- im,k+1)

(3)

+

 2

||(^ic,k

-

im,k+1)||22

subject to the DC OPF constraints (1b)­(1e),

where  is a tuning parameter. The decision variable  m includes the local variables, i.e., the voltage angles for both the local and shared buses. The variable ^ c denotes the shared variables evaluated with values received from the central coordinator, and  are the Lagrange multipliers. The first term in (3) consists of the local objective functions, while the second and third terms are the relaxed consistency constraints.
After solving the subproblem (3), the local controllers share their solutions with the central coordinator. Then, the central coordinator solves an unconstrained optimization problem:

  min
 c,k+1

mM

iNsm

i(ic,k+1

-

^im,k+1)

+

 2

||(ic,k+1

-

^im,k+1)||22,

(4)

where  c are the decision variables and ^ m are the shared variables received from the local controllers. The central coordinator's problem seeks values for the shared variables that reduce the mismatch between the neighboring regions. The shared variable values obtained from the central coordinator's problem are then sent to the local controllers to update the Lagrange multipliers according to (5):

 k+1 =  k + ( c,k+1 -  m,k+1),

(5)

where  is the same parameter used in the local optimization problem (3). Note that  m and  c are the solutions of (3) and (4), respectively. The local controllers then use the updated Lagrange multipliers and the central coordinator's solution to update the local problems. This process is repeated until the shared variables agree to within an specified tolerance.
There are several variants of this ADMM implementation that have been proposed for solving OPF problems. These include extensions that eliminate the need for a central coordinator [22], [23]. A proximal massage passing (PMP) method proposed in [24] also permits a fully distributed ADMM implementation. With PMP, local controllers share variable values with adjacent local controllers instead of a central coordinator and solve the portion of the optimization problem corresponding to their own region. For the calculations in this paper, we implemented ADMM using a modified version of PMP where local controllers share variable values with all other local controllers as this was easier to implement with the data structures that we used. However, calculations from

each local controller are only dependent on variable values from their own regions and adjacent regions. This means that the extra shared information is unused and therefore, our implementation of ADMM is mathematically and structurally identical to PMP. The ADMM algorithm has been proven to converge to the optimal solution if the subproblems are convex. We refer the reader to [2] for more details about ADMM implementations and convergence guarantees.

B. Analytic Target Cascading (ATC)

ATC is another distributed algorithm based on augmented Lagrangian relaxation that solves a large optimization problem via dividing it into hierarchically connected subproblems with multiple levels. Two levels of subproblems are connected if they share coupling variables. Each subproblem can be connected to a higher-level subproblem, called its parent, and lower-level subproblems, called children. Subproblems without a parent are called root subproblems. The ATC algorithm first solves the root subproblems and shares the solutions with their children as target values for the coupling variables. The target variables are used to evaluate a relaxed consistency constraint with a penalty function. Local subproblems in the next level in the hierarchy are then solved, and the results are shared with both their parents, as a response, and their children, as a target. The target-response communication continues until solving the last subproblem. In the next iteration, both the parent and children use the target-response variables to evaluate the consistency constraints. This process continues until reaching consensus on the target-response variables.
To solve the OPF problem, we use a two-level ATC structure. The first level consists of a central coordinator as the root subproblem, and the local subproblems are in the second level. The local subproblem m for iteration k + 1 is:

  min
pk+1, m,k+1 iG m

fi(pi) +

i(^ic,k

iNsm

- im,k+1)

(6)

+ || (^ic,k - im,k+1)||22

subject to DC OPF constraints (1b)­(1e),

where  are the Lagrange multipliers and  is a tuning parameter. Shared variables that are fixed to their values from the central coordinator are denoted as ^ c. The local controllers communicate the resulting shared variable values with a central coordinator. The coordinator solves an unconstrained optimization problem that minimizes the differences between the boundary variables for neighboring regions:

  min
 c,k+1

mM

iNsm

i(ic,k+1

-

^im,k+1

)

+

||

(ic,k+1

-

^im,k+1

)||22

.

(7)

The coordinator shares the target results  c,k+1 with the local

controllers. Next, the local controllers update the Lagrange

multipliers and the parameters  using the target variables:

 k+1 =  k + 2( k)2( c,k+1 -  m,k+1),

(8a)

 k+1 =  k.

(8b)

Variants of ATC use different functions besides the Augmented Lagrangian to relax the consistency constraints [25]. Further, the ATC variant proposed in [6] is fully distributed,

4

eliminating the need for a central coordinator. In this paper, we use the ATC implementation described above, but the same analysis is applicable for other ATC variants. The ATC algorithm is proven to converge for convex problems [26].

C. Auxiliary Problem Principle (APP)

The APP algorithm is also based on augmented Lagrangian decomposition [27]. The APP algorithm solves a sequence of auxiliary problems in distributed fashion without the need for a central coordinator. In contrast to ADMM and ATC which directly use the Augmented Lagrangian, APP linearizes the quadratic term in the augmented Lagrangian around the previous iteration and introduces a regularization term in the objective function [28]. Using the APP algorithm, the OPF formulation for region m and iteration k + 1 is:

  min
pm,k+1, k+1

iG

fi(pki +1) +
iNsm

 2

||im,k+1

-

^in,k ||22

(9)

+



im,k+1(^im,k

-

^in,k

)

+



m,k+1
i

subject to DC OPF constraints (1b)­(1e),

where ,  , and  are specified parameters. We define ^ m,k as the value of the shared variable obtained by the region m in the previous iteration, and ^ n,k denotes the values of the shared variables received from neighboring regions. After each region solves its associated problem and exchanges the results with the neighboring regions, each region updates the values of the Lagrange multipliers as follows:

 k+1 =  k + ( m,k+1 -  n,k+1).

(10)

To summarize, each region iteratively solves (9), shares the results with neighboring regions, and updates the Lagrange multiplier using (10) until reaching consensus on the shared variables. If the local objectives are convex and differentiable, selecting parameters satisfying the condition  < 2 <  guarantees the convergence of the APP algorithm [4].

IV. NONIDEAL COMMUNICATIONS MODELS
The communication requirements for a distributed algorithm depend on the shared variables. During each iteration, each region shares the results of its local optimization by communicating with the neighboring regions. Since communication networks are not ideal, the shared data may suffer from data quality issues or interruptions that impact the distributed algorithms' performance. In this section, we introduce three models for nonideal communications: noisy data, false data, and intermittent loss of communication.

A. Noisy Communications
The data shared between connected regions may be subject to noise resulting from imperfect communication. This noise could be due to quantization error [29] or added to enforce data privacy requirements [30]. To model noisy shared data, we inject additive Gaussian noise into the shared variables:

 noisy =  noiseless + N (0, noise)

(11)

where  noisy is the data that is actually communicated to the neighbors,  noiseless is the true data, and N (0, noise) is a vector of normally distributed random numbers with zero mean and
standard deviation of noise.

B. False Data
Neighboring regions may occasionally receive "false data", i.e., data with large errors. False data may be due to an instantaneous bit error [31] or a malicious adversarial agent [9]. We model a random injection of false data at a specified occurrence probability as shown in the following model:

 noisy =  noiseless + 2 M r, where r =

X 1 - 0.5 0

if X 2 < p, otherwise,

(12)

where M is the maximum magnitude of the error and r is a

vector of multipliers that randomly selects the error magnitude.

In (12), X 1 and X 2 are vectors of uniformly distributed random numbers between [0, 1], and p is the probability of false data

occurrence per iteration.

C. Intermittent Communication Loss
Communications between the agents may occasionally fail entirely for multiple iterations. For instance, communication transmission collisions or instability of the communication link can cause packet loss preventing the agents from sharing data [16], [17]. To model communication loss, we consider an exponential failure distribution with a constant failure rate. This model is commonly used to model component failures [32]. We define the failure rate, denoted  f , as the probability of a component failure per iteration given that the component was in service during the previous iteration. Similar to the failure rate, we use a constant repair rate, denoted r, to model the repair time with an exponential distribution. The communication availability, i.e., the probability that the system will be in a successful state at any iteration, is:

Availability (A) = r × 100 [%]

(13)

 f + r

To model intermittent communication loss with constant failure and repair rates,  f and r, we introduce a state variable s, where s = 1 is a success state and s = 0 is a failure state. We also use an indicator function 1{x}, which equals to 1 if x is true and 0 otherwise. We model the state of the communication link by sampling a uniformly distributed random number at each iteration. If the controller detects an interruption from a neighboring region, it uses the value from the last successful data transmitted from this region. The following pseudocode describes our intermittent communication loss model:

Model for Intermittent Communication Loss 1: generate a random number X 2: if s = 1 then s = 1 -1{ f  X} 3: else s = 1{r  X} 4: if s = 0 then set skh+ar1ed = skhared

V. PERFORMANCE ANALYSES
This section presents the numerical results of solving the DC OPF problem using the three selected distributed algorithms. To investigate the impacts of nonideal communication, we first present the results of each algorithm with ideal communication. We then compare the performance of the distributed algorithms with the nonideal communication models described

5

Fig. 2. ADMM convergence with different parameters for the IEEE 118-bus Fig. 3. ATC convergence with different parameters for the IEEE 118-bus

system with ideal communications.

system with ideal communications.

in Section IV. We use three case studies: the 5-bus system WB5 from [33] and the IEEE 14- and 118-bus systems from MATPOWER [34]. We decompose the three systems into two areas. We present figures for the IEEE 118-bus system results that are representative of the results for the other test cases.

A. Performance with Ideal Communications

To investigate the performance of the three algorithms

with ideal communication, we run the algorithms assuming

each region shares the exact results with neighboring regions

at each iteration. We use the two-norm of the mismatches

between the values of the shared variables to measure the

consensus and set the stopping criteria. We use the results of

the ideal communication to tune the parameters and evaluate

the convergence rates of the algorithms.

1) Alternating Direction Method of Multipliers (ADMM):

The ADMM algorithm's performance depends on the value

selected for the parameter . Fig. 2 shows the convergence of

the ADMM algorithm for the IEEE 118-bus system. For this

test case, we observe that the subsystems reach consensus on

the shared variables the fastest when the value of  is around 106. For WB5 and the IEEE 14-bus system, we tune the value of  to be 103 and 104.6, respectively. Selecting smaller values

for  increases the number of iterations to achieve consensus,

while selecting significantly larger values cause oscillations

that prevent the subsystems from reaching consensus.

2) Analytic Target Cascading (ATC): Similar to ADMM,

the ATC algorithm requires tuning one parameter . The con-

vergence of the shared variables for the IEEE 118-bus system

is shown in Fig. 3. We observe that setting the parameter

 = 1.01 increases the convergence time by a factor of five

compared to  = 1.06. We observe similar behaviour for the

WB5 and IEEE 14-bus systems. Furthermore, the consensus

on the shared variables diverges if the stopping criteria is not

met. This behaviour occurs due to the update criteria (8b),

which increases the penalty on the shared variable consistency

term as the number of iterations increases.

3) Auxiliary Principal Problem (APP): Unlike ADMM and

ATC, the APP algorithm contains three parameters that need

to

be

tuned.

We

adopt

the

condition



=

=

1 2



from

prior

Fig. 4. APP convergence with different parameters for the IEEE 118-bus system with ideal communications.

TABLE I CONVERGENCE RESULTS WITH IDEAL COMMUNICATION

Case WB5 14-Bus 118-Bus

Description
Time (sec) Iteration RG (%) PI (p.u.) Time (sec) Iteration RG (%) PI (p.u.) Time (sec) Iteration RG (%) PI (p.u.)

ADMM
2.85 29 1.1 × 10-3 3.6 × 10-3
3.93 41 -3.0 × 10-3 -7.9 × 10-3
5.68 59 -2.9 × 10-3 -1.3 × 10-1

ATC
0.34 54 5.1 × 10-3 -1.7 × 10-4
1.02 119 9.3 × 10-3 1.8 × 10-4
1.65 149 7.7 × 10-4 -2.9 × 10-4

APP
0.67 20 -5.8 × 10-3 -1.9 × 10-4
7.24 251 7.3 × 10-1 -1.0 × 10-3
15.80 225 1.0 × 10-1 -2.8 × 10-3

literature [4] in order to simplify the parameter tuning. For
the IEEE 118-bus system with two areas, the APP algorithm converges when  = 104 as shown in Fig. 4. The algorithm does not converge with the value of  = 103 after 1000
iterations. For the WB5 and IEEE 14-bus systems, we find a value of  equal to 102 and 103, respectively, yield a
fast convergence rate. While selecting larger parameter values
improves the convergence rate, this also degrades the mismatch

6

Fig. 5. ADMM convergence for the IEEE 118-bus system with noisy data. Fig. 6. ATC convergence for the IEEE 118-bus system with noisy data.

error in the shared variables achieved by the algorithm. 4) Remark about the Algorithms' Performance: In addition
to the convergence rate, the quality of the final solution is another metric for selecting parameters. We use the Relative Gap (RG), defined as relative difference between the cost of the distributed and centralized solutions, to evaluate the final solution quality:

Cost Distributed -Cost Centralized

RG =

× 100 [%]. (14)

Cost Centralized

Further, we use the Power Imbalance (PI), defined as the relative difference between the total power generation and the total load demand, to capture violations of the power balance constraints. In the lossless DC power flow approximation, the Power Imbalance should be zero upon convergence.
Table I summarizes the results obtained from the three algorithms in terms of the convergence rate and solution quality after reaching consensus on the shared variables, with tolerance equal to 1 × 10-4 radians (5.7 × 10-3 degrees).
Comparing the results of the three algorithms, ATC has the fastest convergence rate, while APP has the slowest. Furthermore, APP has the largest relative gap compared to the other two algorithms, which is especially noticeable for the IEEE 14- and 118-bus test cases.

Fig. 7. APP convergence for the IEEE 118-bus system with noisy data.

B. Performance with Noisy Communications
To evaluate the performance of the distributed algorithms, we first assume the regions send shared variable information with additive Gaussian noise as described in (11). We vary the standard deviation of the noise noise to compare the performance of the algorithms under different noise levels. The consensus on the voltage angles achieved by the three algorithms for the IEEE 118-bus system is shown in Figs. 5­7. The results indicate that small noise levels do not significantly impact the convergence rate of the algorithms. Further, the three distributed algorithms converge to a similar level of accuracy as the level of the injected noise. However, the ATC algorithm's convergence pattern exhibits a sudden ramp on the norm of the mismatch during the iterations.
The mean and standard deviation of the shared variables mismatch in the final solution, µ|||| and ||||, for the three

Fig. 8. Mean of the mismatch in the final solution for the IEEE 118-bus system with different noise levels.
algorithms with three noise levels are shown in Table II. To

7

TABLE II MEAN (µsolution ) AND STANDARD DEVIATION (solution ) OF THE MISMATCH IN THE FINAL SOLUTION WITH DIFFERENT VALUES OF NOISE (noise )

Algorithm

Case

noise

WB5 14-Bus 118-Bus

µ|| || || || µ|| || || || µ|| || || ||

10-5
4.4 × 10-5 1.1 × 10-5 4.8 × 10-5 1.2 × 10-5 6.5 × 10-5 1.2 × 10-5

ADMM 10-4
4.3 × 10-4 1.1 × 10-4 4.9 × 10-4 1.2 × 10-4 6.2 × 10-4 1.2 × 10-4

10-3
4.6 × 10-3 1.2 × 10-3 4.9 × 10-3 1.1 × 10-3 6.5 × 10-3 1.1 × 10-3

10-5
2.3 × 10-5 7.6 × 10-6 3.1 × 10-5 7.6 × 10-6 5.7 × 10-5 1.2 × 10-6

ATC 10-4
2.5 × 10-4 6.1 × 10-5 2.7 × 10-4 6.5 × 10-5 3.6 × 10-4 5.6 × 10-5

10-3
2.7 × 10-3 5.4 × 10-4 2.3 × 10-3 5.2 × 10-4 3.3 × 10-3 5.9 × 10-4

10-5
4.1 × 10-5 1.8 × 10-5 4.6 × 10-5 9.7 × 10-6 6.4 × 10-5 1.2 × 10-5

APP 10-4
3.4 × 10-4 1.1 × 10-4 4.9 × 10-4 1.4 × 10-4 6.8 × 10-4 1.8 × 10-4

10-3
3.3 × 10-3 5.9 × 10-4 4.2 × 10-3 7.8 × 10-4 4.8 × 10-3 1.7 × 10-3

visualize the performance differences, Fig. 8 shows the mean of the shared variable mismatches, µ||||, in the final solutions for the IEEE 118-bus system as the noise standard deviation, noise, varies from 10-5 to 10-3. The three algorithms achieve consensus on the shared variables with an error that increases approximately linearly with the standard deviation of the added noise, with a slightly lower mismatch observed in the case of APP and ATC compared to ADMM.
It is also worth mentioning that the solution quality for both ADMM and APP is not impacted by the noise, while the ATC algorithm's final solution can have a high relative gap from the optimal solution if consensus is not achieved, i.e., the stopping criteria are not met after large number of iterations. This happens due to the parameter update for the ATC algorithm (8b), which continuously increases the penalty on the consistency term in the objective function as the number of iterations increase. This degrades the solution quality if the regions do not reach consensus.

the case for ADMM and ATC. Note that even when consensus is achieved on the shared variables for ADMM and ATC, false data can cause a large relative gap in the resulting solution.
To quantitatively compare the performance of the three algorithms, we estimate the probability of achieving the optimal solution using 100 runs of the algorithm for varying probabilities of false data occurrence. We consider an algorithm to have achieved the optimal solution if both RG and PI are below 1% within a maximum of 1000 iterations. Table III on the next

C. Performance with False Data
For the second type of noise, we inject false data into the shared variables as described in (12). We set the value for the false data magnitude M = 2 radians and vary the probability of the injected errors. Figs. 9­11 show the mismatches of the shared variables with different probabilities of false data occurrence for the IEEE 118-bus system.
We observe that the APP mismatches return to the same convergence pattern after a large error is injected, which is not

Fig. 10. ATC convergence for the IEEE 118-bus system with false data.

Fig. 9. ADMM convergence for the IEEE 118-bus system with false data. Fig. 11. APP convergence for the IEEE 118-bus system with false data.

8

TABLE III PERFORMANCE OF THE DISTRIBUTED ALGORITHMS WITH FALSE DATA

Algorithm Probability of false data

WB5 14-Bus 118-Bus

Success rate (%) Avg. Iteration Avg. RG (%) Avg. PI (%) Success rate (%) Avg. Iteration Avg. RG (%) Avg. PI (%) Success rate (%) Avg. Iteration Avg. RG (%) Avg. PI (%)

NC: Not converged.

ADMM

0.1%

1%

88
31 8.7 × 10-4 -8.7 × 10-4
68
42 -4.1 × 10-3
3.9 × 10-3
44
61 1.0 × 10-2 3.0 × 10-3

30 53 -1.4 × 10-7 2.3 × 10-7
15 370 -4.6 × 10-2 1.2 × 10-2
0 NC
NC
NC

ATC

0.1%

1%

100
61 4.2 × 10-3 -3.9 × 10-3
98
145 1.5 × 10-2 -9.6 × 10-3
55
193 1.68 × 10-2 -2.2 × 10-3

97 78 3.0 × 10-3 -1.6 × 10-3
42 164 5.5 × 10-2 -5.8 × 10-4
0 NC
NC
NC

APP

0.1%

1%

100
21 -5.1 × 10-3 -3.9 × 10-4
42
991 -2.1 × 10-1 -1.7 × 10-1
100
404 1.0 × 10-1 -6.6 × 10-3

100
28 -2.3 × 10-3 -1.8 × 10-4
10
985 9.2 × 10-2 6.3 × 10-2
14
1000 2.2 × 10-1 8.8 × 10-2

Fig. 12. Success rates for the IEEE 118-bus system with different rates of false data injection.
page summarizes these results. The three algorithms can fail to attain the optimal solution even with false data probability as low as 0.1%. Further, Fig. 12 shows the success rates when varying the probability of false data from 0.1% to 1% for the IEEE 118-bus system. Generally, the convergence success rate decreases approximately linearly as the probability of false data increases.
D. Performance with Intermittent Communication Loss
This section compares the distributed algorithms' performance under the intermittent communication loss model described in Section IV-C. We vary the failure rate from 0.01 to 0.15 while fixing the repair rate to 0.10. Figs. 13­15 on the next page show the convergence characteristics for the shared variable mismatches for the three algorithms.
The results indicate that the APP algorithm fails to achieve consensus for the IEEE 118-bus case when the failure rate is above 0.01 per iteration or the availability is lower than 50%. The ADMM and ATC algorithms, on the other hand, achieve consensus on the values of the shared variables and the mismatch almost always decreases as the number of iterations increases for all cases. However, the final solutions can be far from optimal. To qualitatively compare algorithmic

performance during intermittent communication loss, Table IV on the next page describes how the probability of achieving the optimal solution changes with failure rates equal to 0.01 and 0.05 per iteration. Each of the results in this table are computed from 100 runs of the algorithm with a constant repair rate of 0.10 per iteration. We again consider an algorithm to have achieved the optimal solution if both RG and PI are below 1% within a maximum of 1000 iterations.
Focusing on the IEEE 118-bus case, Fig. 16 on the next page compares the three algorithms' performance when subjected to intermittent communication loss by varying the failure rate from 0.005 to 0.15. We observe that the APP algorithm starts to fail to converge when the communication failure rate is above 0.08 (corresponding to an availability below 55%). While the ADMM and ATC algorithms fail to obtain the optimal solution around 50% of the time when the failure rate is 0.005 (corresponding to an availability of 95%), they both reach consensus on the shared variables. Overall, the results suggest that the APP algorithm outperforms the other two algorithms with the intermittent communication loss model for the three case studies.
E. Discussion and Comparison
Parameter tuning plays a major role in the performance of distributed algorithms as it strongly impacts the convergence rate. The parameters in the selected algorithms are associated with the penalty terms for the relaxed consistency constraints. We observe that all three algorithms converge for a certain range of parameter values. Generally speaking, selecting large parameter values will prevent the algorithm from achieving the optimal solution and, in some cases, large values might cause the algorithm to diverge. On the other hand, small values reduce the convergence speed and, in extreme cases, the algorithm can diverge. Furthermore, the impacts of the parameter selection differ among the three algorithms. For both ADMM and APP, the algorithms converge toward the optimal solution if consensus on the shared variables is achieved. On the other hand, the ATC algorithm can have a relative gap that increases even if the mismatch for the values of the shared variable is decreasing. We also observe that different systems and cost functions might require repeating the parameter tuning step.
The results shown in this paper indicate the importance of data integrity on the performance of the distributed algorithms. We observe various responses from the distributed algorithms

9

TABLE IV PERFORMANCE OF THE DISTRIBUTED ALGORITHMS WITH INTERMITTENT COMMUNICATION LOSS

Algorithm Failure rate (failure/iteration)
Availability (%)

WB5 14-Bus 118-Bus

Success rate (%) Avg. Iteration Avg. RG (%) Avg. PI(%) Success rate (%) Avg. Iteration Avg. RG (%) Avg. PI (%) Success rate (%) Avg. Iteration Avg. RG (%) Avg. PI (%)

NC: Not converged.

ADMM

0.01

0.05

0.91

0.67

97
43 1.0 × 10-2 -7.5 × 10-4
61
50 1.1 × 10-2 3.5 × 10-3
8
56 4.7 × 10-1 1.5 × 10-3

70 81 2.2 × 10-5 -2.1 × 10-5
8 68 7.9 × 10-2 -8.0 × 10-3
0 NC
NC
NC

ATC

0.01

0.05

0.91

0.67

59
66 3.9 × 10-3 -5.5 × 10-4
100
154 2.2 × 10-2 -6.4 × 10-3
45
193 -1.3 × 10-3 -3.1 × 10-3

100
94 4.0 × 10-3 9.5 × 10-4
99
185 4.4 × 10-3 -3.6 × 10-3
11
206 -1.2 × 10-2 -1.1 × 10-2

APP

0.01

0.05

0.91

0.67

100
29 -2.7 × 10-3 -8.3 × 10-4
100
997 -1.9 × 10-1 -1.3 × 10-1
100
248 1.1 × 10-1 1.1 × 10-1

100
98 4.4 × 10-4 1.4 × 10-4
100
1000 -1.5 × 10-1 -1.1 × 10-1
100
435 -1.3 × 10-3
3.7 × 10-4

Fig. 13. ADMM convergence for the IEEE 118-bus system with intermittent Fig. 15. APP convergence for the IEEE 118-bus system with intermittent

communication loss.

communication loss.

Fig. 14. ATC convergence for the IEEE 118-bus system with intermittent Fig. 16. Success rates for the IEEE 118-bus system with different commu-

communication loss.

nication failure rates. Communication repair rates are held constant at 10%.

to the error models. With Gaussian communication noise, Gaussian noise, this algorithm converges to the wrong values all three algorithms converge to comparable accuracy of the if the stopping criterion is not met. This suggests that reliable standard deviation of noise. Although ATC is least impacted by performance of the ATC algorithm in the presence of Gaussian

10

communication noise requires either using another stopping criterion or modifying the update step in the algorithm.
Among the three algorithms considered in this paper, the APP algorithm has the best performance with respect to false data and intermittent communication loss in terms of the final solution quality. Further, the ADMM algorithm has the lowest success rate compared to the other two algorithms with respect to these two noise types. Although the ADMM and ATC algorithms have a more stable convergence pattern for these two types of noise, they might converge to a suboptimal solution.
VI. CONCLUSION AND FUTURE WORK
Distributed algorithms have many attractive features for solving power system optimization problems, especially for systems with many independent microgrids. Distributed algorithms allow interconnected systems to cooperatively solve large optimization problems while maintaining their autonomy. However, the performance of a distributed algorithm strongly depends on the quality of the shared data. In this paper, we show that different distributed algorithms have various responses to data quality issues. The ADMM and APP algorithms have superior convergence patterns with respect to communication noise compared to ATC. After we stop injecting noise, both the ADMM and APP algorithms return to the same convergence pattern and achieve the optimal solution. In contrast, solutions obtained from the ATC algorithm might be suboptimal even when the neighboring regions achieve consensus on the values of the shared variables.
As extensions to this work, there are other communication and data integrity issues requiring detailed investigations. For power systems applications, these include asynchronous data sharing between neighboring regions and communication latency. Another direction for future work is studying how different power flow representations affect the convergence rates for problems where the DC approximation is inapplicable.
REFERENCES
[1] D. K. Molzahn, F. Do¨rfler, H. Sandberg, S. H. Low, S. Chakrabarti, R. Baldick, and J. Lavaei, "A survey of distributed optimization and control algorithms for electric power systems," IEEE Trans. Smart Grid, vol. 8, no. 6, pp. 2941­2962, 2017.
[2] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, "Distributed optimization and statistical learning via the alternating direction method of multipliers," Found. Trends Mach. Learn., vol. 3, no. 1, pp. 1­122, 2010.
[3] A. Kargarian, M. Mehrtash, and B. Falahati, "Decentralized implementation of unit commitment with analytical target cascading: A parallel approach," IEEE Trans. Power Syst., vol. 33, no. 4, pp. 3981­3993, 2018.
[4] B. H. Kim and R. Baldick, "Coarse-grained distributed optimal power flow," IEEE Trans. Power Syst., vol. 12, no. 2, pp. 932­939, 1997.
[5] B. H. Kim and R. Baldick, "A comparison of distributed optimal power flow algorithms," IEEE Trans. Power Syst., vol. 15, no. 2, pp. 599­604, 2000.
[6] A. Kargarian, M. Mehrtash, and B. Falahati, "Decentralized implementation of unit commitment with analytical target cascading: A parallel approach," IEEE Trans. Power Syst., vol. 33, no. 4, pp. 3981­3993, 2017.
[7] I. Murzakhanov, A. Malakhov, and E. Gryazina, "Suboptimality of decentralized methods for OPF," in IEEE Milan PowerTech, 2019.
[8] J. Mohammadi, G. Hug, and S. Kar, "Role of communication on the convergence rate of fully distributed DC optimal power flow," in IEEE Int. Conf. Smart Grid Commun. (SmartGridComm), 2014, pp. 43­48.
[9] F. Pasqualetti, R. Carli, and F. Bullo, "A distributed method for state estimation and false data detection in power networks," in IEEE Int. Conf. Smart Grid Commun. (SmartGridComm), 2011, pp. 469­474.

[10] H. Li, C. Huang, G. Chen, X. Liao, and T. Huang, "Distributed consensus optimization in multiagent networks with time-varying directed topologies and quantized communication," IEEE Trans. Cybern., vol. 47, no. 8, pp. 2044­2057, 2017.
[11] A. Nedic, A. Olshevsky, A. Ozdaglar, and J. N. Tsitsiklis, "Distributed subgradient methods and quantization effects," in 47th IEEE Conf. Decis. Control (CDC), 2008, pp. 4177­4184.
[12] D. Yuan, S. Xu, H. Zhao, and L. Rong, "Distributed dual averaging method for multi-agent optimization with quantized communication," Syst. Control Lett., vol. 61, no. 11, pp. 1053­1061, 2012.
[13] L. Majzoobi, F. Lahouti, and V. Shah-Mansouri, "Analysis of distributed ADMM algorithm for consensus optimization in presence of node error," IEEE Trans. Signal Process., vol. 67, no. 7, pp. 1774­1784, 2019.
[14] C. Shi and G. Yang, "Distributed optimization under unbalanced digraphs with node errors: Robustness of surplus-based dual averaging algorithm," IEEE Trans. Control Netw. Syst., pp. 1­1, 2020.
[15] H. Li, B. Jin, and W. Yan, "Distributed model predictive control for linear systems under communication noise: Algorithm, theory and implementation," Automatica, vol. 125, p. 109422, 2021.
[16] S. Chen, W. Lan, J. Ma, and X. Yu, "Distributed optimization design of multi-agent systems with packet losses," in IEEE 16th Int. Conf. Control Autom., 2020, pp. 1241­1246.
[17] X. Liang, Z. Li, W. Huang, Q. H. Wu, and H. Zhang, "Relaxed alternating direction method of multipliers for hedging communication packet loss in integrated electrical and heating system," J. Mod. Power Syst. Clean Energy, vol. 8, no. 5, pp. 874­883, 2020.
[18] B. Stott, J. Jardim, and O. Alsac, "DC power flow revisited," IEEE Trans. Power Syst., vol. 24, no. 3, pp. 1290­1300, 2009.
[19] D. K. Molzahn and I. A. Hiskens, "A Survey of Relaxations and Approximations of the Power Flow Equations," Found. Trends Electric Energy Syst., vol. 4, no. 1-2, pp. 1­221, February 2019.
[20] D. Gabay and B. Mercier, "A dual algorithm for the solution of nonlinear variational problems via finite element approximation," Comput. Math. Appl., vol. 2, no. 1, pp. 17­40, 1976.
[21] R. Glowinski and A. Marroco, "Sur l'approximation, par e´le´ments finis d'ordre un, et la re´solution, par pe´nalisation-dualite´ d'une classe de proble`mes de dirichlet non line´aires," ESAIM: Mathematical Modelling and Numerical Analysis-Mode´lisation Mathe´matique et Analyse Nume´rique, vol. 9, no. R2, pp. 41­76, 1975.
[22] A. X. Sun, D. T. Phan, and S. Ghosh, "Fully decentralized AC optimal power flow algorithms," in IEEE Power & Energy Society General Meeting, 2013, pp. 1­5.
[23] T. Erseghe, "Distributed optimal power flow using ADMM," IEEE Trans. Power Syst., vol. 29, no. 5, pp. 2370­2380, 2014.
[24] M. Kraning, E. Chu, J. Lavaei, and S. Boyd, "Dynamic network energy management via proximal message passing," Found. Trends Optimiz., vol. 1, no. 2, pp. 70­122, 2013.
[25] S. Tosserams, L. Etman, P. Papalambros, and J. Rooda, "An augmented Lagrangian relaxation for analytical target cascading using the alternating direction method of multipliers," Struct. Multidiscip. Optimiz., vol. 31, no. 3, pp. 176­189, 2006.
[26] N. Michelena, H. Park, and P. Y. Papalambros, "Convergence properties of analytical target cascading," AIAA J., vol. 41, no. 5, pp. 897­905, 2003.
[27] G. Cohen, "Auxiliary problem principle and decomposition of optimization problems," J. Optimiz. Theory App., vol. 32, no. 3, pp. 277­305, 1980.
[28] L. Zhao, D. Zhu, and B. Jiang, "Auxiliary problem principle of augmented Lagrangian with varying core functions for large-scale structured convex problems," arXiv:1512.04175, 2015.
[29] D. Marco and D. L. Neuhoff, "The validity of the additive noise model for uniform scalar quantizers," IEEE Trans. Inf. Theory, vol. 51, no. 5, pp. 1739­1755, 2005.
[30] T. Zhang and Q. Zhu, "Dynamic differential privacy for ADMM-based distributed classification learning," IEEE Trans. Inf. Forensics Security, vol. 12, no. 1, pp. 172­187, 2017.
[31] M. Jeruchim, "Techniques for estimating the bit error rate in the simulation of digital communication systems," IEEE J. Sel. Areas Commun., vol. 2, no. 1, pp. 153­170, 1984.
[32] R. Billinton and R. N. Allan, Reliability Evaluation of Engineering Systems. Springer, 1992.
[33] W. A. Bukhsh, A. Grothey, K. I. M. McKinnon, and P. A. Trodden, "Local solutions of the optimal power flow problem," IEEE Trans. Power Syst., vol. 28, no. 4, pp. 4780­4788, 2013.
[34] R. D. Zimmerman, C. E. Murillo-Sa´nchez, and R. J. Thomas, "MATPOWER: Steady-state operations, planning, and analysis tools for power systems research and education," IEEE Trans. Power Syst., vol. 26, no. 1, pp. 12­19, 2011.

