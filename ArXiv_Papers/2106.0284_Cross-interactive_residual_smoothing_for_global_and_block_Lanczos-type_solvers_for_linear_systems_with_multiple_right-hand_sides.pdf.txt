Cross-interactive residual smoothing for global and block Lanczos-type solvers for linear systems with multiple right-hand sides
Kensuke Aihara Akira Imakura Keiichi Morikuni

arXiv:2106.00284v1 [math.NA] 1 Jun 2021

Abstract
Global and block Krylov subspace methods are efficient iterative solvers for large sparse linear systems with multiple right-hand sides. However, global or block Lanczos-type solvers often exhibit large oscillations in the residual norms and may have a large residual gap relating to the loss of attainable accuracy of the approximations. Conventional residual smoothing schemes suppress the oscillations but do not aid in improving the attainable accuracy, whereas a novel residual smoothing scheme enables the attainable accuracy for single right-hand side Lanczos-type solvers to be improved. The underlying concept of this scheme is that the primary and smoothed sequences of the approximations and residuals influence one another, thereby avoiding the severe propagation of rounding errors. In the present study, we extend this cross-interactive residual smoothing to the case of solving linear systems with multiple right-hand sides. The resulting smoothed methods can reduce the residual gap with few additional costs compared to their original counterparts. We demonstrate the effectiveness of the proposed approach through rounding error analysis and numerical experiments.

Keywords. multiple right-hand sides, global Lanczos-type solver, block Lanczos-type solver, residual smoothing, residual gap
AMS subject classifications. 65F10, 65F45

1 Introduction

We consider linear systems with multiple right-hand sides

AX = B,

(1)

where A  Rn×n is a large sparse nonsymmetric and nonsingular matrix and B := [b1, b2, . . . , bs]  Rn×s is a rectangular matrix with s  n. This problem appears in various fields of scientific com-

puting (e.g., see [4, 12, 14, 15, 23] and their references), and several types of iterative solvers

have been studied [11, section 12.4]. Global Krylov subspace methods such as the global bi-

conjugate gradient stabilized method (Gl-BiCGSTAB) [10] and generalized global conjugate gra-

dient squared methods including Gl-CGS2 [23] have been developed. Global methods generate the

approximations using the matrix Krylov subspace KkG(A, R0) :=

k-1 i=0

ciAiR0

|

ci



R

, where

R0 := B - AX0  Rn×s is the initial residual with an initial guess X0. Moreover, global methods

Funding: This work was supported by grant numbers JP16K17639, JP17K12690, JP18H03250, JP18K18064, JP19KK0255, JP20K14356, JP21H03451, and JP21K11925 from the Grants-in-Aid for Scientific Research Program
(KAKENHI) of the Japan Society for the Promotion of Science (JSPS). Department of Computer Science, Faculty of Information Technology, Tokyo City University, 1-28-1 Tamazut-
sumi, Setagaya-ku, Tokyo 158-8557, Japan (aiharak@tcu.ac.jp). Faculty of Engineering, Information and Systems, University of Tsukuba, 1-1-1 Tennodai, Tsukuba, Ibaraki
305-8573, Japan (imakura@cs.tsukuba.ac.jp, morikuni@cs.tsukuba.ac.jp).

1

correspond to standard Krylov subspace methods applied to a linear system (Isn  A)x = b, where  denotes the Kronecker product, b := [b1 , b2 , . . . , bs ], and X := [x1, x2, . . . , xs] for x := [x1 , x2 , . . . , xs ]. Thus, many results of Krylov subspace methods for a single linear system can be naturally extended to the case of solving (1). Furthermore, global meth-

ods can easily be applied to general linear matrix equations such as the Sylvester equation;

for example, see [2, 9]. Another approach is the use of block Krylov subspace methods, such

as the block BiCGSTAB method (Bl-BiCGSTAB) [4], which uses the block Krylov subspace

KkB (A, R0) :=

k-1 i=0

AiR0i

|

i



Rs×s

.

As the search subspace for a column of the ap-

proximation Xk expands with s dimensions at each iteration, block methods can achieve faster

convergence than their single counterparts [13]. Block methods may be numerically unstable for

a larger s; thus, stabilization strategies that orthonormalize the iteration matrices have also been

developed; for example, see [12, 13].

We consider the standard recursion formulas for updating the approximation and the corre-

sponding residual of global Lanczos-type solvers

Xk+1 = Xk + kPk, Rk+1 = Rk - k(APk),

(2)

where Pk  Rn×s is a direction matrix and k  R. Starting with the initial residual R0 := B - AX0, the equality Rk = B - AXk holds for k = 1, 2, . . . in exact arithmetic, but this equality may not hold in finite precision arithmetic owing to the accumulation of rounding errors in Xk and Rk. This difference between the recursively updated residual Rk and explicitly computed residual B - AXk is referred to as the residual gap. As in the case of solving a single linear system, global (and block) Lanczos-type solvers often suffer from a large residual gap as a result of rounding
errors. The residual gap GRk := (B - AXk) - Rk is important because the explicitly computed residual norm B - AXk (referred to as the true residual norm) is bounded as follows:

GRk - Rk  B - AXk  GRk + Rk ,
where · denotes the Frobenius norm. Therefore, when Rk becomes sufficiently small, the attainable accuracy of Xk in terms of the true residual norm is dependent on GRk .
The residual gap that is observed in standard Lanczos-type solvers, such as the BiCGSTAB [20] and CGS-type [5] methods, has been thoroughly analyzed and remedies have been proposed. In particular, if the maximum of the residual norms is relatively large compared to the initial residual norm, a large residual gap and a loss of attainable accuracy will appear, and refined techniques to avoid a large increase in the residual norms have been studied (see, for example, [1, 17]). As we demonstrate later, a large relative residual norm leads to a large residual gap in (2). Therefore, in the current study, we focus on a novel residual smoothing scheme [1] to reduce the residual gap. Residual smoothing is a well-known technique for generating a smoothed residual sequence from the primary residual sequence. However, as noted in [7], conventional smoothing schemes [16, 21, 24] do not aid in improving the attainable accuracy. This is due to the fact that the smoothed sequences are computed from the primary (non-smoothed) sequences, and they do not affect the primary iterations; thus, rounding errors that are accumulated in the primary sequences propagate to the smoothed sequences one-sidedly. However, in the alternative scheme presented in [1], the primary and smoothed sequences influence one another and severe propagation of rounding errors can be avoided. We refer to this scheme as cross-interactive residual smoothing (CIRS) and extend it to be applicable for global and block Lanczos-type solvers. Following [1], we present rounding error analysis and numerical experiments to show that CIRS is effective in reducing the residual gap when solving systems with multiple right-hand sides.
We note that the recursion formulas used in block Lanczos-type solvers are partially different from (2); the following forms are often used:

Xk+1 = Xk + Pk~k, Rk+1 = Rk - (APk)~k,

(3)

where ~k  Rs×s. In this case, a large residual gap occurs not only because of the large relative residual norm [19]. When using the stabilization strategy; that is, the orthonormalization of Pk,

2

we can show that the large relative residual norm is a significant factor for the large residual gap. Moreover, the block BiCGGR method (Bl-BiCGGR) was proposed in [19] as a variant of Bl-BiCGSTAB to reduce the residual gap. Bl-BiCGGR is designed to update the residual in the form Rk+1 = Rk - A(Pk~k). However, the alternative recursion is essentially the same as (2); that is, the residual gap may increase significantly when a large relative residual norm exists. We demonstrate that CIRS can be applied to both Bl-BiCGSTAB and Bl-BiCGGR as well as these algorithms in combination with stabilization strategies, and that the resulting smoothed methods generate further accurate approximations.
The remainder of this paper is organized as follows. In section 2, we discuss the residual gap when using (2) and (3). In section 3, we present simple and cross-interactive schemes for residual smoothing. In section 4, rounding error analysis shows that CIRS is effective in reducing the residual gap. In section 5, we propose several smoothed algorithms for specific global and block Lanczos-type solvers. In section 6, we describe the application of global algorithms with CIRS to the Sylvester equation. In section 7, numerical experiments demonstrate the effectiveness of the proposed methods. Finally, concluding remarks are presented in section 8.
Throughout, we use the inner product X, Y F := tr(XY ) for matrices X, Y  Rn×s and · denotes the associated Frobenius norm; that is, X := X, X F . For ease of discussion, we also assume that B = 1 and X0 = O; that is, R0 = B.

2 Influence of rounding errors in standard recursion formulas

Based on [1, 6, 17], we present a rounding error analysis to demonstrate that a large residual gap may occur when the maximum of the recursively updated residual norms in (2) is relatively large.
For matrix operations in finite precision arithmetic, we use the following models [8]:

fl(P + Q) = P + Q + E, E  u( P + Q ), fl(P ) = P + E, E  u P , fl(P ~) = P ~ + E~, E~  su P ~ , fl(AP ) = AP + E, E  mu A P ,

for given P, Q  Fn×s,   F, and ~  Fs×s. Here, F  R is a set of floating point numbers, fl(·) denotes the result of floating point computations, u is the unit roundoff, and m is the maximum number of nonzero entries per row of A. We omit terms of O(u2) and regard u fl() as u  . Note that, unlike the model of matrix­vector multiplication used in [1, section 3], no matrix  exists such that fl(AP ) = AP + P and   mu A holds (cf. [8, section 3.5]).
Similarly to [1, sections 3.1 and 3.2], we can evaluate the upper bound of the norm of the residual gap when using (2); see also [6, section 2] and [17, section 3.1].

Theorem 2.1. Let Xk  Fn×s and Rk  Fn×s be the kth approximation and residual, respectively,
generated by (2) in finite precision arithmetic. Then, the norm of the residual gap GRk := (B - AXk) - Rk is bounded as follows:

GRk

 k(5 + 2m)u A max Xj
0<jk

+ 5(k + 1)u max
0jk

Rj .

(4)

Proof. According to the above matrix operation models, the local errors in the updated approximation Xk = fl(Xk-1 + fl(k-1Pk-1)) and residual Rk = fl(Rk-1 - fl(k-1fl(APk-1))) can be

3

evaluated as follows:

Xk = Xk-1 + fl(k-1Pk-1) + EX k , EX k  u(2 Xk-1 + Xk ) = Xk-1 + k-1Pk-1 + EX k + EX k , EX k  u( Xk-1 + Xk ) = Xk-1 + k-1Pk-1 + EXk , EXk  u(3 Xk-1 + 2 Xk ),
Rk = Rk-1 - fl(k-1fl(APk-1)) - ER k , ER k  u(2 Rk-1 + Rk ) = Rk-1 - k-1fl(APk-1) - ER k - ER k , ER k  u( Rk-1 + Rk ) = Rk-1 - k-1APk-1 - ER k - ER k - k-1ERk , k-1ERk  mu A ( Xk-1 + Xk ) = Rk-1 - k-1APk-1 - ERk ,
ERk  u(3 Rk-1 + 2 Rk ) + mu A ( Xk-1 + Xk ),
where EXk := EX k + EX k and ERk := ER k + ERk + k-1ERk . Thus, the norm of the residual gap can be bounded as follows:

(B - AXk) - Rk

= B - A(Xk-1 + k-1Pk-1 + EXk ) - (Rk-1 - k-1APk-1 - ERk ) = (B - AXk-1) - Rk-1 - AEXk + ERk

k

k

= (B - AX0) - R0 - A EXj + ERj

j=1

j=1

(5)

k

k

A

EXj +

ERj

j=1

j=1

k

k

 (5 + 2m)u A

Xj + 5u Rj ,

j=1

j=0

where (B - AX0) - R0 = O holds under the assumption that X0 = O. The proof is completed by bounding the approximation and residual norms by their maximums.

Theorem 2.1 implies that it is important to suppress maxj Xj and maxj Rj to be as small as possible during the iterations to avoid a large residual gap. In our experience, u(maxj Rj ) provides a practical estimation of GRk when Rj  B = 1 for some j < k. Therefore, following [1], we focus on the error terms related to the residual norm, which can be suppressed by a residual smoothing scheme, rather than on the approximation norms. Note that, similarly to the single right-hand side case, (4) is not necessarily sharp for estimating GRk in the actual computation.
Theorem 2.1 can specifically be applied to Lanczos-type solvers, such as Gl-BiCGSTAB, GlCGS2, and Bl-BiCGGR. Note that, for example, in Gl-BiCGSTAB and Bl-BiCGGR, the approximation and residual are updated in two parts (the BiCG part and polynomial part) at each iteration by using the forms (2). Refer to the paragraph after the proof of Theorem 2.2 for details; also see section 5.2.
Moreover, we can identify the cause of a large residual gap for the recursion formulas (3), similarly to (4), assuming that the column-orthonormal matrices Pk, the ranges of which are equal to those of the direction matrices Pk, are available. We leave showing the corresponding result in the general case open.

Theorem 2.2. Let Xk  Fn×s and Rk  Fn×s be the kth approximation and residual, respectively, generated by (3) in finite precision arithmetic. If the columns of Pj are (exactly) orthonormal for

all j < k, the norm of the residual gap GRk := (B - AXk) - Rk is bounded as follows:





GRk

 k(3 + 4s

s + 2m

s)u A max
0<jk

Xj

+ 3(k + 1)u max
0jk

Rj .

(6)

4

Proof. Noting that Pj = s and Pj

~j

 = s ~j

= s Pj~j , the local errors in the up-

dated approximation Xk = fl(Xk-1+fl(Pk-1~k-1)) and residual Rk = fl(Rk-1-fl(fl(APk-1)~k-1))

can be evaluated as follows:

Xk = Xk-1 + fl(Pk-1~k-1) + EX k , EX k  u(2 Xk-1 + Xk )

= =

Xk-1 + E~X k
Xk-1 +

Pk-1~k-1 +  su Pk-1 Pk-1~k-1 +

EX k + ~k-1 EXk ,

E~X k, = s su

Pk-1 ~ k-1





EXk  u[(2 + s s)

 s su( Xk-1

Xk-1 + (1 +

+ Xk )  s s) Xk

],

Rk = Rk-1 - fl(fl(APk-1)~k-1) - ER k , ER k  u(2 Rk-1 + Rk )

=

Rk-1 - E~R k

fl(APk-1 )~ k-1  su APk-1

-PkE-1R ~k k--1E~Rk ,ssu

A

(

Xk-1

+

Xk )

=

Rk-1 - APk-1~k-1 ERk ~k-1  mu

- A

ER k - Pk-1

E~R k

-

ERk

~k-1 

,

~k-1  m su

A

(

Xk-1

+

Xk )

= Rk-1 - APk-1~k-1 - ERk ,



ERk  u(2 Rk-1 + Rk ) + (s + m) su A ( Xk-1 + Xk ),

where EXk := EX k + E~X k and ERk := ER k + E~Rk + ERk ~k-1. Thus, similarly to the evaluation of (5), the norm of the residual gap can be bounded as follows:





k

k

(B - AXk) - Rk  (3 + 4s s + 2m s)u A

Xj + 3u Rj .

j=1

j=0

The proof is completed by bounding the first and second sums with maxj Xj and maxj Rj , respectively.

The above discussions are useful for evaluating the residual gap in a specific method, namely Bl-BiCGSTAB, in combination with a strategy for orthonormalizing the columns of the direction matrices [12, Algorithm 2.1]; this method is referred to as Bl-BiCGSTABpQ. The method uses the forms (3) and (2) in the BiCG and polynomial parts, respectively, in each iteration as follows:

(BiCG part) Xk := Xk + Pk~k, Rk := Rk - (APk)~k, (polynomial part) Xk+1 = Xk + kRk , Rk+1 = Rk - k(ARk ),

(7)

where k  R, ~k  Rs×s, and Pk is a column-orthonormal matrix. Thus, we can evaluate the residual gap using Theorems 2.1 and 2.2, as follows.

Corollary 2.3. Let Xk  Fn×s and Rk  Fn×s be the kth approximation and residual, respectively, generated by Bl-BiCGSTABpQ with (7) in finite precision arithmetic. If the columns of Pj are (exactly) orthonormal for all j < k, then the norm of the residual gap GRk := (B - AXk) - Rk is bounded as follows:





GRk  k(4 + 2s s + m + m s)u A

max
0j<k

Xj

+ max
0<jk

Xj

(8)

+ 4(k + 1)u

max
0j<k

Rj

+ max Rj
0jk

.

Proof. As in the proofs of Theorems 2.1 and 2.2, we can obtain the bound of (B - AXk) - Rk by evaluating the local errors in Xk, Rk, Xk -1, and Rk -1.

Exact orthonormality of the columns of Pj is required to evaluate the local errors in Theorems 2.2 and 2.3. This assumption is not satisfied in finite precision arithmetic owing to rounding errors. However, the orthonormalization of the columns of Pj can be performed in a backward stable manner, e.g., by using the Householder transformation [8, Theorem 19.4] and Givens rotations [8, Theorem 19.10], and the evaluations (6) and (8) can almost capture the actual computations.

5

3 Residual smoothing for global and block methods
In this section, we present residual smoothing for global and block Lanczos-type solvers. All of the discussions in sections 3.1 and 3.2 assume exact arithmetic.

3.1 Simple smoothing scheme

We first present a simple residual smoothing scheme, which is a naive extension of the classical smoothing technique in [16, 21] to the case of multiple right-hand sides (cf. [22]).
Let {Xk} and {Rk} be the primary sequences of the approximations and residuals, respectively, obtained by a global or block method. Then, new sequences of approximations Yk and the corresponding smoothed residuals Sk := B - AYk are generated by

Yk = (1 - k)Yk-1 + kXk, Sk = (1 - k)Sk-1 + kRk,

(9)

where Y0 := X0 is the initial guess, S0 := R0 is the initial residual, and k  R is a smoothing parameter. Based on the typical strategy, we select the parameter k such that the updated smoothed residual norm Sk is locally minimized; that is,

k := -

Sk-1, Rk - Sk-1 Rk - Sk-1 2

F.

(10)

Thus, we obtain Sk  Rk and Sk  Sk-1 . The former inequality implies that Sk decreases not slower than Rk ; however, as is well known in the classical residual smoothing scheme, the actual convergence speed will be almost the same. The latter inequality implies that
Sk decreases monotonically and plays an important role in reducing the residual gap in CIRS. We note that the smoothing parameter k can be selected using a different technique, such
as quasi-minimal residual smoothing [24]. With this scheme, if Sk decreases smoothly, we can obtain numerical results that are similar to the case of selecting (10). As the effects of CIRS are not significantly dependent on the parameter itself, we do not elaborate on its selection in this study.

3.2 Cross-interactive smoothing scheme
We present a cross-interactive residual smoothing scheme. The original CIRS that was proposed in [1] is an improvement of the alternative smoothing scheme that was introduced in [24], whereby the primary and smoothed sequences influence one another in the iteration process. We apply this concept to the case of solving systems with multiple right-hand sides (1).
Algorithm 1 is an extension of CIRS that can be applied to global and block methods. The smoothing process begins with the computation of the direction matrix P^k in line 3 using the primary method. If the kth primary approximation is updated in the form of Xk+1 = Xk + kPk with k  R or Xk+1 = Xk + Pk~k with ~k  Rs×s, then P^k is replaced by kPk or Pk~k, respectively. In line 5, although a matrix multiplication by A is required, the total number of multiplications by A per iteration in the primary method and corresponding smoothed method can be the same through a sophisticated formulation; for details, see section 5. The parameter ^k+1 is selected as (10). Note that the quantities computed in line 8 coincide with the primary approximation and residual, respectively, as shown in Proposition 3.1 below. These values are generated following the updates of the new approximation Y^k+1 and corresponding smoothed residual S^k+1 using the smoothed sequences, and are returned to the primary method. Thus, the primary and smoothed sequences influence one another, in contrast to the sequences generated by (9).
The propositions below show the equivalence between Algorithm 1 and (9) with (10), and can be derived from the statements in [1, 7, 24]. We leave the proof to the reader.
Proposition 3.1. Let {Xk} and {Rk} be the primary sequences of the approximations and residuals, respectively, which are generated by (2) or (3), and let P^k := Xk+1 - Xk be the direction

6

Algorithm 1 CIRS for global and block methods
Require: An initial guess X0 and the initial residual R0 := B - AX0. 1: Set Y^0 := X0, S^0 := R0, V^0 := O, and ^0 := 0. 2: for k = 0, 1, 2, . . . until convergence do 3: Compute P^k (corresponding to Xk+1 - Xk) using the primary method. 4: V^k+1 = (1 - ^k)V^k + P^k 5: Compute U^k+1 = AV^k+1 with an explicit multiplication by A. 6: ^k+1 = S^k, U^k+1 F / U^k+1, U^k+1 F 7: Y^k+1 = Y^k + ^k+1V^k+1, S^k+1 = S^k - ^k+1U^k+1 8: return Xk+1 = Y^k+1 + (1 - ^k+1)V^k+1, Rk+1 = S^k+1 - (1 - ^k+1)U^k+1 9: end for

matrix. Then, for the iteration matrices S^k, U^k (= AV^k), V^k, and Y^k that are generated by Algorithm 1, the identities

Y^k + (1 - ^k)V^k = Xk, S^k - (1 - ^k)U^k = Rk

(11)

hold for k = 0, 1, 2, . . . , where ^k  R is a smoothing parameter.
Proposition 3.2. For given primary sequences {Xk} and {Rk}, let Y^k, S^k, and ^k be the kth approximation, smoothed residual, and smoothing parameter, respectively, generated by Algorithm 1, and let Yk, Sk, and k be those generated by (9) with (10). Then, the identities Y^k = Yk and S^k = Sk hold for k = 0, 1, 2, . . . and ^k = k for k = 1, 2, . . . .

As a result, we can observe that the smoothed residual does not converge slower than the primary one S^k  Rk and that the smoothed residual norm decreases monotonically S^k 
S^k-1 .

4 Advantage of CIRS

We discuss the residual gap for the presented residual smoothing schemes in finite precision arithmetic. We refer to section 2 for matrix operations, taking into account the rounding errors.
We first consider the simple smoothing scheme outlined in section 3.1. Although smooth convergence behavior can be achieved by (9), the residual gap cannot be reduced because of the following property. Theorem 4.1 below is a naive extension of several results for a single right-hand side in [7, sections 4.1 and 5.1] to the case of multiple right-hand sides. We leave the proof to the reader.

Theorem 4.1 (cf. [7]). Let {Xk} and {Rk} be the primary sequences of the approximations and residuals, respectively, which are generated by (2) or (3), and let {Yk} and {Sk} be the corresponding smoothed sequences that are generated by (9) in finite precision arithmetic. Then, the norm of the residual gap GSk := (B - AYk) - Sk is bounded as follows:

GSk  |1 - k| GSk-1 + |k| GRk + A EYk + ESk ,

(12)

where EYk and ESk are the local errors in the updated approximation Yk and residual Sk, respectively, and these errors satisfy

EYk  3u|1 - k| Yk-1 + 2u|k| Xk ,

(13)

ESk  3u|1 - k| Sk-1 + 2u|k| Rk .

In actual computations, the residual norm Rk of a Lanczos-type solver often increases significantly in the early iterations. According to the property of residual smoothing, |k| is selected to be small when Rk becomes large; that is, it can be expected from (12) and (13) that the residual

7

gap and local errors in the smoothed sequences are relatively small, although Rk  B = 1 holds. However, after Rk increases significantly, the subsequent iterations move into a stage where Rk  B and |k|  1. Furthermore, |k| GRk is the dominant part of the right-hand side in (12), and GSk increases drastically to the same order of magnitude as GRk . We present such a phenomenon in the numerical experiments in section 7.1.
Next, we consider the new CIRS displayed in Algorithm 1. Theorems 4.2 and 4.3 are extensions of the main results for the original CIRS; that is, [1, Eqs. (3.12) and (3.18)], for the case of multiple right-hand sides.
As the recursion formulas for updating Y^k and S^k in line 7 of Algorithm 1 have the same forms as (2), we obtain the following result for the residual gap of the smoothed sequences, as in Theorem 2.1 for the primary sequences.

Theorem 4.2. Let Y^k  Fn×s and S^k  Fn×s be the kth approximation and smoothed residual, respectively, which are generated by Algorithm 1 in finite precision arithmetic. Then, the norm of the residual gap GS^k := (B - AY^k) - S^k is bounded as follows:

GS^k

 k(5 + 2m)u A max
0<jk

Y^j

+ 5(k + 1)u max
0jk

S^j .

(14)

As the smoothed residual norm S^k decreases monotonically, the upper bound in (14) is relatively small compared to that in (4), and the residual gap is expected to be reduced. Moreover, as the primary and smoothed sequences influence one another in Algorithm 1, the evaluation of the residual gap for the primary sequences is also improved over (4) as follows:

Theorem 4.3. Let Xk  Fn×s and Rk  Fn×s be the kth approximation and residual, respectively, which are generated by Algorithm 1 in finite precision arithmetic. Then, the norm of the residual gap GRk := (B - AXk) - Rk is bounded as follows:

GRk

 k(5 + 2m)u A max Y^j
0<jk

+ 5(k + 1)u max S^j
0jk

(15)

+ (2 + m)u A Xk + 2u Rk .

Proof. Similarly to the proof of Theorem 2.1, the local errors in Y^k, S^k, Xk, and Rk can be evaluated as follows:

Y^k = Y^k-1 + ^kV^k + EY^k , EY^k  u(3 Y^k-1 + 2 Y^k ), S^k = S^k-1 - ^kAV^k - ES^k , ES^k  u(3 S^k-1 + 2 S^k ) + mu A ( Y^k-1 + Y^k ), Xk = Y^k + ^kV^k + EXk , EXk  u(3 Y^k + 2 Xk ), Rk = S^k - ^kAV^k - ERk , ERk  u(3 S^k + 2 Rk ) + mu A ( Y^k + Xk ),
where ^k := fl(1 - ^k). Subsequently, the norm of the residual gap can be bounded as follows:

(B - AXk) - Rk

 (B - AY^k) - S^k + A EXk + ERk

 (B - AY^k-1) - S^k-1 + A EY^k + ES^k + A

k

k

A

EY^j +

ES^j + A EXk + ERk

j=1

j=1

EXk + ERk

k

k

 (5 + 2m)u A

Y^j + 5u

S^j + (2 + m)u A Xk + 2u Rk .

j=1

j=0

The proof is completed by bounding the first and second sums with maxj Y^j and maxj S^j , respectively.

8

Algorithm 2 Smoothed global CGS2 (S-Gl-CGS2) 1: Select an initial guess X and compute R = B - AX. 2: Set Y^ := X, S^ := R, V^ := O, and ^ := 0. 3: Select R~0 and R0, and compute Z~0 = AR~0 and Z0 = AR0. 4: Set P := R, U := R, and T := R. 5: while S^ > tol do 6: V = AP,  = R~0, V F ,  = R0, V F 7:  = R~0, R F /,  = R0, R F / 8: W = T - V, Q = U - V 9: P^ = U + W, V^ = ^V^ + P^, U^ = AV^ 10: ^ = S^, U^ F / U^ , U^ F , ^ = 1 - ^ 11: Y^ = Y^ + ^V^ , S^ = S^ - ^U^ 12: X = Y^ + ^V^ , R = S^ - ^U^ 13:  = Z~0, W F /,  = Z0, Q F / 14: U = R - Q, T = R - W, P = T - (Q - P ) 15: end while
The upper bound in (15) is dependent on maxj Y^j and maxj S^j for j  k, and only the kth norms Xk and Rk . Therefore, similarly to the case of the single right-hand side [1], GRk can increase when Rk increases, but it can be reduced as Rj decreases for j > k. The upper bounds in (14) and (15) of the final residual gap are of the same order of magnitude when the primary and smoothed sequences converge, and Y^k and Xk in Algorithm 1 are expected to attain the same level of accuracy.
5 Specific smoothed algorithms
In this section, we apply CIRS to Gl-CGS2 [23], Gl-BiCGSTAB [10], Bl-BiCGSTAB [4], and Bl-BiCGGR [19]. For the block methods, we consider their stabilized variants that are incorporated with strategies that orthonormalize the columns of the iteration matrices, namely BlBiCGSTABpQ [12, Algorithm 2.1] and Bl-BiCGGRrQ [18, Fig. 2].
5.1 Smoothed variant of Gl-CGS2
As Gl-CGS2 uses the forms (2), we can apply CIRS directly by setting P^k := kUk + ~kSk in Algorithm 1, where the matrix kUk + ~kSk is provided in [23, line 13 of Algorithm 5].
Algorithm 2 displays the resulting smoothed Gl-CGS2 method (S-Gl-CGS2), where ~kSk is renamed as kWk. Lines 9­12 of Algorithm 2 correspond to CIRS and the other lines follow from Gl-CGS2. In line 9, we use an explicit multiplication by A, but we do not need to compute AP^k, which is required in Gl-CGS2. Thus, the smoothed variant can be implemented without additional multiplications by A. Note that Gl-CGS2 [23, Algorithm 5] uses four multiplications by A per iteration, but two of these can be reduced by computing AR~0 and AR0 in advance and storing them if an operator A is available, where R~0 and R0 are the initial shadow residuals used in Gl-CGS2. This efficient approach is also incorporated into Algorithm 2.
Note that the derivation of Gl-CGS2 in [23] is different from that of the original CGS2 for a single linear system in [5]. Another Gl-CGS2 algorithm and its smoothed variant can be derived based on [5], and this approach also requires no additional multiplications by A. However, as our main purpose is to illustrate the effectiveness of CIRS, we do not discuss alternative algorithms of the primary method further.
9

Algorithm 3 Smoothed global BiCGSTAB (S-Gl-BiCGSTAB)
1: Select an initial guess X and compute R = B - AX. 2: Set Y^ := X, S^ := R, V^ := O, and ^ := 0. 3: Select R~0 and compute Z~0 = AR~0. 4: Set P := R, R := O, and  := 0. 5: while S^ > tol do 6:  = Z~0, P F ,  = R~0, R F / 7: P^ = R + P, V^ = ^V^ + P^, U^ = AV^ 8: ^ = S^, U^ F / U^ , U^ F , ^ = 1 - ^ 9: Y^ = Y^ + ^V^ , S^ = S^ - ^U^ 10: X = Y^ + ^V^ , R = S^ - ^U^ 11: V = (R - R)/, T = AR,  = R, T F / T, T F 12: X = X + R, R = R - T 13:  = R~0, T F /, P = R - (P - V )
14: end while

5.2 Smoothed variant of Gl-BiCGSTAB

Gl-BiCGSTAB updates Rk+1 = Rk - kARk in

two residuals in the the polynomial part,

forms (2): where k

=Rk R~:=0,

Rk Rk

- kAPk in F / R~0, APk

the BiCG F with an

part and iteration

matrix Pk and k = Rk , ARk F / ARk , ARk F . Therefore, we can apply CIRS to both the BiCG

and polynomial parts. However, this requires two additional multiplications by A per iteration.

To circumvent this issue, based on [1, section 4], we reformulate the updating process so that no

additional multiplications by A are required; see also [17, section 4.6]. We consider the recursion

formulas of the approximation and residual in the BiCG part

Xk = Xk -1 + (k-1Rk -1 + kPk), Rk = Rk -1 - A(k-1Rk -1 + kPk),

and perform CIRS by setting P^k := k-1Rk -1 + kPk. Thereafter, we compute k = R~0, Rk F / Z~0, Pk F , where Z~0 := AR~0 is computed and stored in advance. To update Pk
to Pk+1 via APk, we use the backward formulation APk := (Rk - Rk )/k following the computation of Rk . The resulting smoothed Gl-BiCGSTAB method (S-Gl-BiCGSTAB), which requires no additional multiplications by A, is presented in Algorithm 3.

5.3 Smoothed variant of Bl-BiCGSTABpQ
We apply CIRS to Bl-BiCGSTABpQ using the same approach as in section 5.2. We rewrite (7) as
Xk = Xk -1 + (k-1Rk -1 + Pk~k), Rk = Rk -1 - A(k-1Rk -1 + Pk~k),
and set P^k := k-1Rk -1 + Pk~k in Algorithm 1. The matrix ~k  Rs×s is obtained by solving an s-dimensional linear system (Z~0Pk)~k = R~0Rk, where Z~0 := AR~0 is computed and stored in advance. Following the computation of Rk , the matrix APk is provided as a solution of the system (APk)~k = Rk - Rk . The resulting smoothed Bl-BiCGSTABpQ method (S-Bl-BiCGSTABpQ), which also requires no additional multiplications by A, is displayed in Algorithm 4. Note that qf(·) in line 6 denotes the Q-factor of the QR factorization of a matrix.

5.4 Smoothed variant of Bl-BiCGGRrQ
Finally, we apply CIRS to Bl-BiCGGRrQ, which is a stabilized variant of Bl-BiCGGR. The original Bl-BiCGGR aims to reduce the residual gap of Bl-BiCGSTAB. The basic concept

10

Algorithm 4 Smoothed block BiCGSTABpQ (S-Bl-BiCGSTABpQ)
1: Select an initial guess X and compute R = B - AX. 2: Set Y^ := X, S^ := R, V^ := O, and ^ := 0. 3: Select R~0 and compute Z~0 = AR~0. 4: Set P := R, R := O, and  := 0. 5: while S^ > tol do 6: P = qf(P ),  = Z~0P 7: Solve  = R~0R for . 8: P^ = R + P , V^ = ^V^ + P^, U^ = AV^ 9: ^ = S^, U^ F / U^ , U^ F , ^ = 1 - ^ 10: Y^ = Y^ + ^V^ , S^ = S^ - ^U^ 11: X = Y^ + ^V^ , R = S^ - ^U^ 12: Solve V  = R - R for V . 13: T = AR,  = R, T F / T, T F 14: X = X + R, R = R - T 15: Solve  = R~0T for . 16: P = R - (P - V )
17: end while

is to reformulate the recursion formulas as follows:
Xk+1 = Xk + kRk = Xk + Pk~k + kRk - kAPk~k = (Xk + kRk) + Uk, Uk := (Pk - kAPk)~k,
Rk+1 = [Rk - k(ARk)] - AUk.
Thus, Bl-BiCGGR can use the forms (2). However, as discussed in section 2, the residual gap may become large when the maximum of the residual norms is relatively large. Note that k  R is determined by minimizing Rk - k(ARk) instead of Rk - k(ARk ) .
However, as Bl-BiCGGR exhibits numerical instabilities for a large s, a stabilized variant BlBiCGGRrQ, in which the columns of Rk are orthonormalized, has also been developed. This method updates the approximation using the form
Xk+1 = Xk + (kQk + Uk)k,
where Qkk corresponds to the QR factorization of the residual Rk. In the actual computation, the residual is not computed explicitly; instead, the column-orthonormal matrix Qk and upper triangular matrix k are computed by
[Qk+1, k+1] = qr(Qk - kAQk - AUk), k+1 = k+1k,
where qr(·) denotes the QR factorization of a matrix, and the first and second values of qr(·) are the Q- and R-factors, respectively. For details, we refer the reader to [18].
Subsequently, we apply CIRS to Bl-BiCGGRrQ by setting P^k := (kQk + Uk)k. The resulting smoothed Bl-BiCGGRrQ method (S-Bl-BiCGGRrQ), which also requires no additional multiplications by A, is presented in Algorithm 5.
As Rk+1 is explicitly obtained instead of the matrix Qk - kAQk - AUk in CIRS, we solve the system Tk+1k = Rk+1 to obtain the matrix Tk+1 := Qk+1k+1 in line 12, and compute Zk := AUk = Qk - kWk - Tk+1 in line 13, where Wk := AQk.

6 Application of global methods to Sylvester equation

Global methods can easily be applied to general linear matrix equations [2, 9, 23]. We describe how to apply the presented global algorithms to the Sylvester equation

AX - XC = B,

(16)

11

Algorithm 5 Smoothed block BiCGGRrQ (S-Bl-BiCGGRrQ)
1: Select an initial guess X and compute R = B - AX. 2: Set Y^ := X, S^ := R, V^ := O, and ^ := 0. 3: Select R~0 and compute [Q, ] = qr(R), W = AQ, and - = R~0Q. 4: Set P := Q and V := W . 5: while S^ > tol do 6: Solve (R~0V ) = - for . 7:  = W, Q F / W, W F , U = (P - V ) 8: P^ = (Q + U ), V^ = ^V^ + P^, U^ = AV^ 9: ^ = S^, U^ F / U^ , U^ F , ^ = 1 - ^ 10: Y^ = Y^ + ^V^ , S^ = S^ - ^U^ 11: X = Y^ + ^V^ , R = S^ - ^U^ 12: Solve T  = R for T . 13: Z = Q - W - T 14: [Q, ] = qr(R), W = AQ, + = R~0Q 15: Solve - = +/ for . 16: - = +, P = Q + U , V = W + Z
17: end while

where A  Rn×n, C  Rs×s, B := [b1, b2, . . . , bs]  Rn×s, and X := [x1, x2, . . . , xs]  Rn×s with

s  n. Moreover, we discuss the residual gap when solving (16).

The Sylvester equation (16) can be represented by a standard linear system A~x = b, where

A~ := by A~

IsA-CIn  Rns×ns for an arbitrary v := [v1

and , v2,

b .

:= ..,

[b1 , b2 vs] 

,... Rns

, ,

bs ] vi 

. Subsequently, the linear Rn can be expressed by a

transformation linear operator

A that is defined as A(V ) := AV - V C, where V := [v1, v2, . . . , vs]. Therefore, the application

of the standard Krylov subspace methods to A~x = b corresponds to the application of their

global counterparts to A(X) = B. Such global methods can be implemented by replacing the

multiplications with A in (1) by the transformations with A; for example, the initial residual is defined as R0 := B - A(X0) = B - (AX0 - X0C). Moreover, as A~ = Is  A - C  In, the adjoint of A with respect to the inner product ·, · F can be defined as A(V ) := AV - V C.

At this point, we reconsider the standard recursion formulas used in the global methods. When

solving (16), the recursions (2) are replaced with

Xk+1 = Xk + kPk, Rk+1 = Rk - kA(Pk) = Rk - k(APk - PkC),

(17)

where k  R and Pk  Rn×s. Using these reformulations, we can implement the global methods and their smoothed variants for solving (16). Thereafter, similar to Theorem 2.1, we can evaluate the residual gap in finite precision arithmetic.

Theorem 6.1. Let Xk  Fn×s and Rk  Fn×s be the kth approximation and residual, respectively, which are generated by (17) in finite precision arithmetic. Then, the norm of the residual gap GRk := (B - A(Xk)) - Rk is bounded as follows:

GRk

 ku[(7 + 2m) A

+ (7 + 2s) C ] max Xj
0<jk

+ 5(k + 1)u max Rj .
0jk

(18)

Proof. As in the proof of Theorem 2.1, the local errors in the updated approximation and residual can be evaluated as follows:

Xk = Xk-1 + k-1Pk-1 + EXk , EXk  u(3 Xk-1 + 2 Xk ),
Rk = Rk-1 - fl(k-1fl(A(Pk-1))) - ER k , ER k  u(2 Rk-1 + Rk ) = Rk-1 - k-1fl(A(Pk-1)) - ER k - ERk , ERk  u( Rk-1 + Rk ).

12

The result of k-1fl(A(Pk-1)) = k-1fl(fl(APk-1) - fl(Pk-1C)) is expressed as

k-1 fl(A(Pk-1 )) = k-1fl(APk-1) - k-1fl(Pk-1C) - k-1E1, k-1E1  u|k-1|( A + C ) Pk-1  u( A + C )( Xk-1 + Xk ) = (k-1APk-1 + k-1E2) - (k-1Pk-1C + k-1E3) - k-1E1, k-1E2  mu|k-1| A Pk-1  mu A ( Xk-1 + Xk ), k-1E3  su|k-1| Pk-1 C  su C ( Xk-1 + Xk ) = k-1A(Pk-1) + k-1ERk , k-1ERk  u [(1 + m) A + (1 + s) C ] ( Xk-1 + Xk ),
where ERk := E2 - E3 - E1. Thus, the local errors in Rk are evaluated as follows:
Rk = Rk-1 - k-1A(Pk-1) - ERk , ERk  u(3 Rk-1 + 2 Rk ) + u [(1 + m) A + (1 + s) C ] ( Xk-1 + Xk ),
where ERk := ER k + ERk + k-1ERk . Using the linearity of the operator A, the norm of the residual gap can be bounded as follows:

(B - A(Xk)) - Rk

= B - A(Xk-1 + k-1Pk-1 + EXk ) - (Rk-1 - k-1A(Pk-1) - ERk )

= (B - A(Xk-1)) - Rk-1 - (AEXk - EXk C) + ERk

k

k

( A + C )

EXj +

ERj

j=1

j=1

k

k

 u[(7 + 2m) A + (7 + 2s) C ] Xj + 5u Rj .

j=1

j=0

Note that (B - A(X0)) - R0 = O holds for X0 = O. The proof is completed by bounding

k j=1

Xj

and

k j=0

Rj

with maxj Xj

and maxj

Rj , respectively.

As the upper bound in (18) contains the term maxj Rj similarly to (4), it is important to suppress the maximum of the residual norms to reduce the residual gap. When using CIRS, as Xj and Rj in (18) are replaced by Y^j and S^j, respectively, the smoothed method is expected to have a smaller residual gap.

7 Numerical experiments
We present numerical experiments that were conducted to demonstrate that a large residual gap occurs when a large relative residual norm exists in the global and block Lanczos-type solvers, which can be improved by CIRS. We compared the convergence of Gl-CGS2, Gl-BiCGSTAB, BlBiCGSTABpQ, and Bl-BiCGGRrQ and their smoothed variants S-Gl-CGS2, S-Gl-BiCGSTAB, S-Bl-BiCGSTABpQ, and S-Bl-BiCGGRrQ (i.e., Algorithms 2 to 5) using several model problems.
Numerical calculations were carried out in double-precision floating-point arithmetic on a PC (Intel Core i7-8650U CPU with 16 GB of RAM) equipped with MATLAB R2018a. The iterations were started with X0 = O and were stopped when the relative norms of the recursively updated residuals ( Rk / B for the primary methods and S^k / B for the smoothed methods) were less than 10-14. We used several test matrices. One of these was a Toeplitz matrix A = [aij ]  R2000×2000 that was defined as aii := 2, ai,i+1 := 1, and ai+4,i := -1 for each i (otherwise, aij := 0). The other matrices were obtained from the SuiteSparse Matrix Collection [3]. Table 1 presents the dimension (n), number of nonzero entries (nnz), maximum number of nonzero entries

13

Table 1: Characteristics of test matrices from SuiteSparse Matrix Collection [3].

Target problem

Matrix

n nnz m 2(A)

Linear system (1)

cdde2 pde2961 bfwa782

961 4,681 5 5.5e+01 2,961 14,585 5 6.4e+02
782 7,514 24 1.7e+03

Sylvester equation (16)

fs_680_1 can_24

680 2,184 8 1.5e+04 24 160 9 7.8e+01

Figure 1: Convergence histories of non-smoothed Gl-CGS2 (left) and smoothed Gl-CGS2 using SRS (right) for cdde2 with s = 16.
Figure 2: Convergence histories of proposed S-Gl-CGS2 for cdde2 with s = 16: primary sequences (left) and smoothed sequences (right).
per row (m), and the two-norm condition number (2(A)). The Toeplitz matrix, cdde2, pde2961, and bfwa782 were used as the coefficient matrices of (1), and fs_680_1 and can_24 were used for A and C, respectively, in (16). The right-hand side B was provided as a random matrix and the initial shadow residual R~0 was set to R0 (= B) for all of the methods. Another shadow residual R0 that was used in Gl-CGS2 and S-Gl-CGS2 was set to a random matrix. The conditions that were set for each experiment are described in the following.
The implementation of the compared methods naively followed the provided algorithms. In particular, we used the slash or backslash command in MATLAB to solve the small linear systems that appeared in the block methods. Moreover, a MATLAB command qr(·) was used to perform the QR factorization.
7.1 Differences between simple residual smoothing and CIRS
Following [1], we first present the advantages of CIRS compared to the simple residual smoothing scheme. We compared the convergence of Gl-CGS2 and its smoothed variants for (1), where A was set to cdde2 and s was set to 16. In the following, the simple residual smoothing (9) with (10) is referred to as SRS.
14

Figure 3: Convergence histories of Gl-CGS2 and S-Gl-CGS2 (left), and Gl-BiCGSTAB and S-GlBiCGSTAB (right) for pde2961 with s = 16.

Table 2: Number of iterations and true relative residual norm of Gl-CGS2, S-Gl-CGS2, GlBiCGSTAB, and S-Gl-BiCGSTAB for test matrices.

s=8

s = 16

s = 32

Matrix Solver

Iter. True res. Iter. True res. Iter. True res.

Toeplitz

Gl-CGS2 S-Gl-CGS2 Gl-BiCGSTAB S-Gl-BiCGSTAB

1373 1155 1277 1306

3.5e-12 1.9e-14 1.1e-12 2.2e-14

1257 1097 1252 1289

4.0e-12 2.0e-14 3.9e-12 2.2e-14

1183 1120 1243 1304

5.9e-13 1.9e-14 5.9e-14 2.1e-14

cdde2

Gl-CGS2 S-Gl-CGS2 Gl-BiCGSTAB S-Gl-BiCGSTAB

87 2.9e-11 82 9.0e-15 89 3.3e-14 88 1.1e-14

81 2.5e-10 79 1.1e-14 96 3.8e-14 94 1.2e-14

84 4.7e-11 81 1.2e-14 90 7.5e-14 90 1.3e-14

Gl-CGS2

221 1.6e-10 248 5.4e-10 228 5.6e-10

pde2961

S-Gl-CGS2 Gl-BiCGSTAB

214 8.4e-14 244 8.8e-14 235 8.5e-14 220 2.9e-13 227 1.3e-11 225 2.1e-13

S-Gl-BiCGSTAB 220 1.0e-13 223 1.0e-13 213 9.8e-14

Gl-CGS2

356 3.5e-11 354 2.3e-11 356 3.7e-11

bfwa782

S-Gl-CGS2 Gl-BiCGSTAB

357 1.6e-13 354 1.7e-13 331 1.7e-13

 8.3e-13

 1.1e-12 1551 2.4e-12

S-Gl-BiCGSTAB

 3.5e-13 1561 3.7e-13

 4.0e-13

The left panel of Figure 1 depicts the histories of the relative norms of the recursively updated
residuals Rk, true residuals B - AXk, and residual gap GRk of the non-smoothed Gl-CGS2. The plots indicate the number of iterations on the horizontal axis versus log10 of the relative norms on the vertical axis. The right panel of Figure 1 depicts the corresponding histories of the
smoothed Gl-CGS2 using SRS. Here, the evaluated quantities are associated with Yk and Sk in the smoothed sequences instead of Xk and Rk in the primary sequences, respectively. Figure 2 displays the histories for the proposed S-Gl-CGS2 using CIRS; the left and right panels present
the primary and smoothed sequences obtained in lines 12 and 11, respectively, in Algorithm 2.
The following can be observed from Figures 1 and 2: Gl-CGS2 has a large relative residual norm, which results in a large residual gap, leading to a loss of attainable accuracy of the ap-
proximations, as indicated by Theorem 2.1. In particular, Rk increases drastically at the first and 29th iterations, and GRk increases accordingly. The smoothed Gl-CGS2 using SRS exhibits smooth convergence behavior, but the residual gap is not improved; although GSk is relatively small for Rk  B holds, it increases after 30 iterations and reaches the same order of magnitude as GRk of Gl-CGS2 at the 47th iteration for Rk  B . This phenomenon follows

15

Table 3: Number of iterations and true relative residual norm of Gl-CGS2, S-Gl-CGS2, GlBiCGSTAB, and S-Gl-BiCGSTAB for Sylvester equation.

Solver

Iter. True res.

Gl-CGS2 S-Gl-CGS2 Gl-BiCGSTAB S-Gl-BiCGSTAB

466 484 1113 1133

5.5e-11 1.1e-14 6.0e-14 1.8e-14

Theorem 4.1. In contrast, the behavior of GRk in the proposed S-Gl-CGS2 is similar to that

of Rk ; that is, as indicated by Theorem 4.3, GRk increases drastically with a large increase

in Rk , but decreases as much smaller than that of

Rk becomes smaller. In Gl-CGS2. Furthermore,

the S^k

final iterations, GRk of S-Gl-CGS2 is of S-Gl-CGS2 converges smoothly with

a small residual gap throughout the iterations, as indicated by Theorem 4.2. It should be noted

that the final sizes of GRk and GS^k in S-Gl-CGS2 are of the same order of magnitude.

7.2 Experiments on smoothed global Lanczos-type solvers
We compared the convergences of Gl-CGS2, S-Gl-CGS2, Gl-BiCGSTAB, and S-Gl-BiCGSTAB for (1) to demonstrate the effectiveness of the smoothed global methods, where A was set to the Toeplitz matrix, cdde2, pde2961, and bfwa782 and s was set to 8, 16, and 32. We set the maximum number of iterations to 2n.
Figure 3 depicts the convergence histories of the relative norms of the recursively updated residuals and true residuals for pde2961 with s = 16. The plots illustrate the number of iterations on the horizontal axis versus log10 of the relative residual norm on the vertical axis. For GlBiCGSTAB in the right panel, the maximum of the residual norms in the polynomial part was of the same order of magnitude as that in the BiCG part. For S-Gl-CGS2 and S-Gl-BiCGSTAB, we plotted the smoothed residual norms that were obtained from lines 11 and 9 of Algorithms 2 and 3, respectively. The true residual norms were plotted with markers using every 6th point. Table 2 displays the number of iterations required for successful convergence and the true relative residual norm at termination. The symbol  indicates that no convergence occurred within 2n iterations.
The following can be observed from Figure 3 and Table 2: Gl-CGS2 often has a large residual gap and exhibits a loss of attainable accuracy of the approximations. Moreover, S-Gl-CGS2 has a relatively smaller residual gap and provides more accurate approximations in all cases. Although Gl-BiCGSTAB often has a smaller residual gap than Gl-CGS2, the residual gaps of S-GlBiCGSTAB and S-Gl-CGS2 are not larger than those of Gl-BiCGSTAB. The convergence speed of the smoothed methods is not significantly different from that of their non-smoothed counterparts. Gl-BiCGSTAB and S-Gl-BiCGSTAB fail to converge for bfwa782. For this problem, Gl-CGS2 and S-Gl-CGS2 are more robust, and S-Gl-CGS2 has a smaller residual gap.

7.3 Experiments on Sylvester equation
We applied Gl-CGS2, S-Gl-CGS2, Gl-BiCGSTAB, and S-Gl-BiCGSTAB to the Sylvester equation (16). Following [23], we set the matrices A and C in (16) to fs_680_1 and can_24, respectively. Table 3 displays the number of iterations required for successful convergence and the true relative residual norm at termination.
It can be observed from Table 3 that although Gl-BiCGSTAB and S-Gl-BiCGSTAB have a small residual gap, many iterations are required. However, Gl-CGS2 converges much faster than Gl-BiCGSTAB, as observed in [23]. Moreover, Gl-CGS2 has a large residual gap and exhibits a loss of attainable accuracy of the approximations. S-Gl-CGS2 converges as rapidly as Gl-CGS2 and has a smaller residual gap, and thus, it provides more accurate approximations.

16

Figure 4: Convergence histories of Bl-BiCGSTABpQ and S-Bl-BiCGSTABpQ (left), and BlBiCGGRrQ and S-Bl-BiCGGRrQ (right) for cdde2 with s = 32.

Table 4: Number of iterations and true relative residual norm of Bl-BiCGSTABpQ, S-BlBiCGSTABpQ, Bl-BiCGGRrQ, and S-Bl-BiCGGRrQ for test matrices.

s=8

s = 16

s = 32

Matrix Solver

Iter. True res. Iter. True res. Iter. True res.

Bl-BiCGSTABpQ 253 1.4e-13 123 1.4e-13

Toeplitz

S-Bl-BiCGSTABpQ Bl-BiCGGRrQ

224 227

1.1e-14 9.3e-14

123 132

9.1e-15 1.2e-13

S-Bl-BiCGGRrQ

220 1.3e-14 121 1.0e-14

69 3.0e-13 67 5.4e-15 70 4.8e-14 68 6.7e-15

cdde2

Bl-BiCGSTABpQ S-Bl-BiCGSTABpQ Bl-BiCGGRrQ S-Bl-BiCGGRrQ

65 2.9e-14 64 1.0e-14 63 1.3e-12 63 1.0e-14

55 1.6e-13 54 8.0e-15 53 1.3e-13 52 1.1e-14

38 6.7e-11 38 6.0e-15 36 5.7e-11 37 6.3e-15

Bl-BiCGSTABpQ 113 3.7e-13 86 5.0e-12

pde2961

S-Bl-BiCGSTABpQ Bl-BiCGGRrQ

120 136

7.4e-14 2.3e-13

87 126

6.1e-14 3.7e-13

S-Bl-BiCGGRrQ

123 7.6e-14 120 7.4e-14

65 5.0e-13 62 5.1e-14 74 1.1e-12 92 6.5e-14

bfwa782

Bl-BiCGSTABpQ S-Bl-BiCGSTABpQ Bl-BiCGGRrQ S-Bl-BiCGGRrQ

80 2.9e-12 86 8.5e-14 97 2.5e-12 88 8.7e-14

57 1.2e-12 66 7.8e-14 63 9.8e-13
 4.6e-10

37 1.6e-12 43 6.6e-14 34 1.2e-12
 3.0e-13

7.4 Experiments on smoothed block Lanczos-type solvers
We compared the convergences of Bl-BiCGSTABpQ, S-Bl-BiCGSTABpQ, Bl-BiCGGRrQ, and SBl-BiCGGRrQ for (1) to demonstrate the effectiveness of the smoothed block methods. We refer the reader to section 7.2 for the computational conditions.
Figure 4 displays the convergence histories for cdde2 with s = 32. Table 4 presents the number of iterations and the true relative residual norm at termination. For the plots of the figures and the notations in the table, we refer the reader to section 7.2.
The following can be observed from Figure 4 and Table 4: Bl-BiCGSTABpQ and Bl-BiCGGRrQ converge in all cases and their convergence speeds are comparable. However, there are cases in which both non-smoothed methods have a large relative residual norm as well as a large residual gap. Furthermore, S-Bl-BiCGSTABpQ and S-Bl-BiCGGRrQ exhibit smooth convergence behavior, a reduced residual gap, and more accurate approximations than their non-smoothed counterparts in most cases. We note that S-Bl-BiCGGRrQ sometimes decelerates the convergence speed for a larger s; in particular, it does not converge for bfwa782 with s = 16 and 32. In the final subsection, we consider the cause of this phenomenon and present a strategy for its improvement.

17

Figure 5: Convergence histories of Bl-BiCGGRrQ and S-Bl-BiCGGRrQ with and without switching strategy, and history of the condition number of k for bfwa782 with s = 16.

Table 5: Number of iterations and true relative residual norm of S-Bl-BiCGSTABpQ and S-BlBiCGGRrQ with switching strategy for test matrices.

s=8

s = 16

s = 32

Matrix Solver

Iter. True res. Iter. True res. Iter. True res.

Toeplitz

S-Bl-BiCGSTABpQ S-Bl-BiCGGRrQ

228 239

1.2e-14 1.1e-14

119 113

1.1e-14 1.0e-14

67 5.3e-15 64 1.7e-14

cdde2

S-Bl-BiCGSTABpQ S-Bl-BiCGGRrQ

64 1.1e-14 63 8.4e-15

53 9.5e-15 52 7.7e-15

37 1.1e-14 36 1.1e-14

pde2961

S-Bl-BiCGSTABpQ S-Bl-BiCGGRrQ

122 121

9.4e-14 7.5e-14

86 107

7.7e-14 7.0e-14

61 6.5e-14 73 5.6e-14

bfwa782

S-Bl-BiCGSTABpQ S-Bl-BiCGGRrQ

84 1.1e-13 90 9.2e-14

64 9.7e-14 61 7.5e-14

43 8.4e-14 37 6.0e-14

7.5 Strategy of partial application of CIRS
In S-Bl-BiCGGRrQ, the system Tk+1k = Rk+1 needs to be solved for Tk+1 at each iteration, where k is the R-factor of the QR factorization of Rk. This may cause numerical instability when the condition number of k becomes large, leading to a loss of convergence speed, as can be observed in Table 4. This difficulty can probably be remedied by the partial application of CIRS. Specifically, we perform CIRS only when the condition S^k / B >  holds for a threshold value ; otherwise, we use the standard updating process. This simple strategy enables us to avoid the inversion of an ill-conditioned matrix k, and the convergence speed is expected to be maintained. Moreover, the residual gap is expected to be reduced even in the partially smoothed iteration process, because the essence of CIRS is to suppress the maximum of the residual norms. This strategy can also be applied to S-Bl-BiCGSTABpQ.
We conducted numerical experiments to demonstrate the effectiveness of the smoothed methods using the above switching strategy. The threshold value  was set to 10-2 and we refer the reader to section 7.4 for the other computational conditions.
Figure 5 displays the convergence histories of the relative norms of the recursively updated residuals of Bl-BiCGGRrQ and S-Bl-BiCGGRrQ with and without the switching strategy for bfwa782 with s = 16. The plots indicate the number of iterations on the horizontal axis versus log10 of the relative residual norm on the left vertical axis. In Figure 5, we also display the history of the condition number of k, where log10 of the condition number is plotted on the right vertical axis. Table 5 depicts the number of iterations and the true relative residual norm at termination for S-Bl-BiCGSTABpQ and S-Bl-BiCGGRrQ with the switching strategy.
It can be observed from Figure 5 that S-Bl-BiCGGRrQ with the switching strategy converges at

18

the same speed as Bl-BiCGGRrQ, whereas S-Bl-BiCGGRrQ without the strategy decelerates the convergence speed with an increase in the condition number of k. By comparing Tables 4 and 5, it can be observed that the switching strategy is useful for maintaining the convergence speed while improving the attainable accuracy. We note that the selection of the threshold value  does not cause a severe problem. In our experience, similar effects can be observed for  = 10-1, 10-2, and 10-3, for example.
8 Concluding remarks
We have presented a rounding error analysis to show that global and block Lanczos-type solvers may have a large residual gap when a large relative residual norm exists during the iterations. To reduce the residual gap, we extended cross-interactive residual smoothing for a single linear system to the case of multiple right-hand sides and designed several smoothed algorithms for these solvers. The proposed algorithms can be implemented with few additional costs compared to their non-smoothed counterparts. The numerical experiments demonstrated that the smoothed variants have a smaller residual gap and provide more accurate approximations than their original counterparts.
In this study, we restricted the rounding error analysis to the case of real numbers. However, our results are also valid for the complex case, because the error bounds for basic complex arithmetic have similar forms to those of real arithmetic (cf. [8, section 3.6]). A detailed discussion on this point will be provided in future studies.
References
[1] K. Aihara, R. Komeyama, and E. Ishiwata, Variants of residual smoothing with a small residual gap, BIT, 59 (2019), pp. 565­584, https://doi.org/10.1007/s10543-019-00751-w.
[2] F. P. A. Beik and D. K. Salkuyeh, On the global Krylov subspace methods for solving general coupled matrix equations, Comput. Math. Appl., 62 (2011), pp. 4605­4613, https://doi.org/10.1016/j.camwa.2011.10.043.
[3] T. A. Davis and Y. Hu, The University of Florida sparse matrix collection, ACM Trans. Math. Softw., 38 (2011), pp. 1­25, https://doi.org/10.1145/2049662.2049663.
[4] A. El Guennouni, K. Jbilou, and H. Sadok, A block version of BiCGSTAB for linear systems with multiple right-hand sides, Electron. Trans. Numer. Anal., 16 (2003), pp. 129­142.
[5] D. R. Fokkema, G. L. G. Sleijpen, and H. A. van der Vorst, Generalized conjugate gradient squared, J. Comput. Appl. Math., 71 (1996), pp. 125­146, https://doi.org/10.1016/0377-0427(95)00227-8.
[6] A. Greenbaum, Estimating the attainable accuracy of recursively computed residual methods, SIAM J. Matrix Anal. Appl., 18 (1997), pp. 535­551, https://doi.org/10.1137/S0895479895284944.
[7] M. H. Gutknecht and M. Rozlozník, Residual smoothing techniques: Do they improve the limiting accuracy of iterative solvers?, BIT, 41 (2001), pp. 86­114, https://doi.org/10.1023/A:1021917801600.
[8] N. J. Higham, Accuracy and Stability of Numerical Algorithms, SIAM, Philadelphia, 2nd ed., 2002.
[9] K. Jbilou, A. Messaoudi, and H. Sadok, Global FOM and GMRES algorithms for matrix equations, Appl. Numer. Math., 31 (1999), pp. 49­63, https://doi.org/10.1016/S0168-9274(98)00094-4.
19

[10] K. Jbilou, H. Sadok, and A. Tinzefte, Oblique projection methods for linear systems with multiple right-hand sides, Electron. Trans. Numer. Anal., 20 (2005), pp. 119­138.
[11] G. A. Meurant and J. D. Tebbens, Krylov Methods for Nonsymmetric Linear Systems: From Theory to Computations, Springer Nature, Switzerland AG, 2020.
[12] Y. Nakamura, K. I. Ishikawa, Y. Kuramashi, T. Sakurai, and H. Tadano, Modified block BiCGSTAB for lattice QCD, Comput. Phys. Commun., 183 (2012), pp. 34­37, https://doi.org/10.1016/j.cpc.2011.08.010.
[13] D. P. O'Leary, The block conjugate gradient algorithm and related methods, Linear Algebra Appl., 29 (1980), pp. 293­322, https://doi.org/10.1016/0024-3795(80)90247-5.
[14] T. Sakurai and H. Sugiura, A projection method for generalized eigenvalue problems using numerical integration, J. Comput. Appl. Math., 159 (2003), pp. 119­128, https://doi.org/10.1016/S0377-0427(03)00565-X.
[15] T. Sakurai, H. Tadano, and Y. Kuramashi, Application of block Krylov subspace algorithms to the Wilson-Dirac equation with multiple right-hand sides in lattice QCD, Comput. Phys. Commun., 181 (2010), pp. 113­117, https://doi.org/10.1016/j.cpc.2009.09.006.
[16] W. Schönauer, Scientific Computing on Vector Computers, Elsevier, Amsterdam, 1987. [17] G. L. G. Sleijpen and H. A. van der Vorst, Reliable updated residuals in hybrid Bi-CG
methods, Computing, 56 (1996), pp. 141­163, https://doi.org/10.1007/BF02309342. [18] H. Tadano, S. Saito, and A. Imakura, Accuracy improvement of the shifted
block BiCGGR method for linear systems with multiple shifts and multiple righthand sides, Lecture Notes in Comput. Sci. Eng., 117 (2017), pp. 171­185, https://doi.org/10.1007/978-3-319-62426-6_12. [19] H. Tadano, T. Sakurai, and Y. Kuramashi, Block BiCGGR: a new Block Krylov subspace method for computing high accuracy solutions, JSIAM Lett., 1 (2009), pp. 44­47, https://doi.org/10.14495/jsiaml.1.44. [20] H. A. van der Vorst, Bi-CGSTAB: a fast and smoothly converging variant of Bi-CG for the solution of nonsymmetric linear systems, SIAM J. Sci. Stat. Comput., 13 (1992), pp. 631­644, https://doi.org/10.1137/0913035. [21] R. Weiss, Parameter-Free Iterative Linear Solvers, Akademie Verlag, Berlin, 1996. [22] J. Zhang and H. Dai, Global CGS algorithm for linear systems with multiple right-hand sides (in Chinese), Numer. Math. A: J. Chin. Univ., 30 (2008), pp. 390­399. [23] J. Zhang, H. Dai, and J. Zhao, Generalized global conjugate gradient squared algorithm, Appl. Math. Comput., 216 (2010), pp. 3694­3706, https://doi.org/10.1016/j.amc.2010.05.026. [24] L. Zhou and H. F. Walker, Residual smoothing techniques for iterative methods, SIAM J. Sci. Comput., 15 (1994), pp. 297­312, https://doi.org/10.1137/0915021.
20

