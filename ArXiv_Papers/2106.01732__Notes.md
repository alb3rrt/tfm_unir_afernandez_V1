
# Bilingual Alignment Pre-training for Zero-shot Cross-lingual Transfer

[arXiv](https://arxiv.org/abs/2106.01732), [PDF](https://arxiv.org/pdf/2106.01732.pdf)

## Authors

- Ziqing Yang
- Wentao Ma
- Yiming Cui
- Jiani Ye
- Wanxiang Che
- Shijin Wang

## Abstract

Multilingual pre-trained models have achieved remarkable transfer performance by pre-trained on rich kinds of languages. Most of the models such as mBERT are pre-trained on unlabeled corpora. The static and contextual embeddings from the models could not be aligned very well. In this paper, we aim to improve the zero-shot cross-lingual transfer performance by aligning the embeddings better. We propose a pre-training task named Alignment Language Model (AlignLM), which uses the statistical alignment information as the prior knowledge to guide bilingual word prediction. We evaluate our method on multilingual machine reading comprehension and natural language interface tasks. The results show AlignLM can improve the zero-shot performance significantly on MLQA and XNLI datasets.

## Comments

4 pages

## Source Code

Official Code



Community Code

- [https://paperswithcode.com/paper/bilingual-alignment-pre-training-for-zero](https://paperswithcode.com/paper/bilingual-alignment-pre-training-for-zero)

## Bibtex

```tex
@misc{yang2021bilingual,
      title={Bilingual Alignment Pre-training for Zero-shot Cross-lingual Transfer}, 
      author={Ziqing Yang and Wentao Ma and Yiming Cui and Jiani Ye and Wanxiang Che and Shijin Wang},
      year={2021},
      eprint={2106.01732},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```

## Notes

Type your reading notes here...

