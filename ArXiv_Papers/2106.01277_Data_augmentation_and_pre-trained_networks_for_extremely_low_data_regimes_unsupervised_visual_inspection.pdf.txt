arXiv:2106.01277v1 [cs.CV] 2 Jun 2021

Data augmentation and pre-trained networks for extremely low data regimes unsupervised visual inspection
Pierre Gutierrez1, Antoine Cordier1, Tha¨is Caldeira2, and Th´eophile Sautory1
1Scortex, 22 rue Berbier du Mets, Paris, France 2School of Electrical and Computer Engineering, University of Campinas, Brazil
ABSTRACT
The use of deep features coming from pre-trained neural networks for unsupervised anomaly detection purposes has recently gathered momentum in the computer vision field. In particular, industrial inspection applications can take advantage of such features, as demonstrated by the multiple successes of related methods on the MVTec Anomaly Detection (MVTec AD) dataset. These methods make use of neural networks pre-trained on auxiliary classification tasks such as ImageNet. However, to our knowledge, no comparative study of robustness to the low data regimes between these approaches has been conducted yet. For quality inspection applications, the handling of limited sample sizes may be crucial as large quantities of images are not available for small series. In this work, we aim to compare three approaches based on deep pre-trained features when varying the quantity of available data in MVTec AD: KNN, Mahalanobis, and PaDiM. We show that although these methods are mostly robust to small sample sizes, they still can benefit greatly from using data augmentation in the original image space, which allows to deal with very small production runs.
Keywords: quality control, visual inspection, deep learning, anomaly detection, data augmentation, pre-training
1. INTRODUCTION
Deep learning is currently revolutionizing automated visual inspection. Practitioners usually adopt either a supervised learning approach (where the system is trained with labelled defective images) or an unsupervised learning one (where any anomaly on a test image is considered as a potential defect). In the case of very limited series and custom-manufactured goods, the amount of available data to train an automatic inspection system is often quite low (typically, below 50 images). This prevents the adoption of a supervised approach, since defective images cannot be collected in sufficient quantities for training. At the same time, training the system should not impact the production cycle times. For small series, this means that only a few minutes, or even less, are available for training.
A recent unsupervised approach1 from the literature leverages deep features from pre-trained networks for anomaly detection. Using such a network as a frozen feature extractor can effectively reduce training time, as well as the required amount of training data. In this work, we focus on the extremely low data regime. First, we review and benchmark several unsupervised methods from the literature which make use of these pre-trained networks, by studying the robustness of these methods to the low data regimes. To improve this robustness, we then propose applying data augmentation, by synthetically generating modified images: we find that data augmentation has a strong and positive impact in such regimes.
Results are reported on the publicly available MVTec AD dataset.2 An average AUC of above 0.90 can be achieved with only a few images, which allows to tackle very small series. One may notice that the AUCs in lowdata regimes for the reported methods are on par with the ones obtained with autoencoders in maximum-data regimes.2
For further author information, send correspondence to P. Gutierrez E-mail: pgutierrez@scortex.io

2. RELATED WORK
Quality control can be automated using supervised deep learning.3, 4 To do so, convolutional neural networks (CNNs) such as U-Net5 or RetinaNet6 are trained to explicitly recognize known defects. One of the major downsides of this method is that it may require a high volume of annotated images. Another drawback is that it cannot handle unusual or theoretical defects. Two main approaches are being proposed to solve these issues: simulations7, 8 and anomaly detection (via unsupervised or semi-supervised learning). In this work, we focus on the latter.
Anomaly detection in general and on images in particular has been a very popular field in the past few years. We thus focus this review on the papers that are the closest to our task at hand, and refer the reader to a broader review of literature9 for further details. Generally speaking, most approaches fall in any of the three following categories: reconstruction of the input, classification of anomalies and modelling of normality.
Autoencoders for image reconstruction have been among the first methods to make use of deep learning for anomaly detection.2, 10 The idea is to reconstruct the input image using a network that was trained on normal data solely, before computing the anomaly score as a distance between the input and the output reconstruction. Because the simple pixel-wise L2 difference is a poor choice of similarity measure between images, the approach has been extended using structural similarity,11 perceptual loss, variational autoencoders (VAEs)12 and generative adversarial networks (GANs).13 For an extensive overview and experimental comparison of some of these methods, see Baur and al.12
Another possibility is to leverage the classification confidences of a CNN trained in a supervised or self-supervised fashion in order to predict anomalies. For example, data augmentation techniques can be used to assess a network confidence zone: in GeoTrans,14 the authors propose to learn and predict geometric transformations performed on an input image, like translations or rotations. The intuition is that at test time, if an image is out-of-distribution, the network should not be able to predict the transformation correctly. This is in line with the idea that self-supervised techniques can improve uncertainty estimation, even as an auxiliary task.15 The idea from GeoTrans has been extended in GOAD,16 using ideas from the deep one-class17 literature. However, the performance of both of these approaches depends on what can be considered as acceptable data augmentations with regards to the dataset of interest. CSI18 builds on this idea and improves the method by using contrastive learning and defining what are acceptable and out-of-distributions transformations. To some extent, this can be seen as a way to do outlier exposure,19 i.e. a supervised training of the network so that it learns to separate inliers from outliers (which can come from either synthetic generation or external datasets).
In order to model normality, and inspired by the one-class SVM technique,20 Deep SVDD17 projects the input data on an hyper-sphere. The main issue with the Deep SVDD method is that the network does not necessarily learn discriminative features. One way to fix this is to extend Deep SVDD to the semi-supervised case (Deep SAD),21, 22 potentially using synthetic anomalies.23 Another way to force the network to be descriptive is to simultaneously learn an auxiliary supervised task on another unrelated dataset.17
A recent trend in modelling normality for anomaly detection is to directly take advantage of deep feature representations from a pre-trained network. Using a pre-trained CNN, images are encoded in the embedding space of the network where traditional anomaly detectors can then be used. This guarantees descriptiveness of the features, and avoids mode collapses since the pre-training is performed on a large, generalist dataset such as ImageNet.24 Typical methods for computing the final anomaly score in the embedding space are: distance to the k-nearest neighbors (KNN),25 Mahalanobis distance,1 and kernel density estimation.26 Because these methods only provide an anomaly score at the image level and do not necessarily offer anomaly localization, they have been improved in several ways. SPADE27 extends the KNN approach by looking at the nearest neighbors locally, in the pixel-embedding space. Correspondingly, PaDiM28 extends the Mahanalobis approach by locally modeling patch-wise gaussian covariances. Finally, in "Deep Feature Reconstruction" (DFR),29 Yang et. al. use a reconstruction autoencoder in the pixel-embedding space of the pre-trained feature extractor. Note that the idea of feature reconstruction was first proposed without the need of a frozen pre-trained network. It was introduced in the "Uninformed Students" paper,30 where an ensemble of CNNs is trained to mimic a teacher network. The idea has since been extended by using several feature maps from a single network,31, 32 as well as by blurring the input.33

The methods benefiting from frozen pre-trained networks often perform very well because they cannot forget the richness of the pre-trained feature representations, which often happens when fine-tuning such networks on different data due to catastrophic forgetting.34 By taking advantage of descriptive features, combined with the compactness that the traditional anomaly detectors bring, these methods allow to get state-of-the-art performance on many standard anomaly detection datasets for computer vision, such as MVTec. It is worth noting that techniques such as KNN, Mahalanobis, and PaDiM are also very fast to train and to test (seconds or minutes) compared to other methods that require an additional CNN training, such as autoencoders or self-supervised networks (hours or days). This makes the said methods prime for small series in quality inspection use cases.

3. MATERIALS AND METHODS
3.1 Pre-trained networks for anomaly detection
In this article, as we focus on very small training sample sizes, we only consider methods leveraging pre-trained feature extractors. We compare the following methods: KNN25 (and therefore SPADE,27 for which the image anomaly score is identical), Mahalanabis1 and PaDim.28 Figure 1 summarizes all three approaches. All methods output an anomaly score. In practice, a threshold is set to classify anomalous from normal images.

3.1.1 KNN
The DN2 (Deep Neareast Neighbor Anomaly Detection) method developed by Bergman et al.25 uses a k nearestneighbor approach in the global-average pooled embedding space of a feature extractor for anomaly detection: a training set constituted of normal images is first embedded (and pooled), before embedding (and pooling) test images. The anomaly score associated with a given test image y can then be calculated using the average euclidean distance of its associated embedded vector fy to the k nearest training embeddings Nk(fy):

1 d(y) =
k

||f - fy||2

(1)

f Nk(fy )

While in the original paper, only the last feature level embeddings are used, the work is extended with SPADE (Sub-Image Anomaly Detection with Deep Pyramid Correspondences),27 where spatial features coming from multiple levels are aligned and concatenated for computing an additional pixel-wise score with a second KNN. The method will not be discussed in detail here since its resulting image score is in fact identical to the original KNN25 implementation. However, the idea of using feature levels coming from different scales is of interest, as it has been shown to bring value for anomaly detection using pre-trained networks.1 Consequently, and for fairness of comparison with the two other methods (which also use multiple feature levels), for the KNN approach we decide to compute every f embedding vector by concatenating the global average pooled features coming from the different levels we preliminarily chose, in order to obtain the final embedded vector. In this work, we set the number of nearest neighbors k to 1, as it gives the best results in our experiments.

3.1.2 Mahalanobis
Instead of using euclidean distance to the nearest neighbors in the latent space, Rippel et al.1 proposed to model the embedded training set distribution as a multivariate gaussian and to use the Mahalanobis distance as the anomaly score. After computing the embedding vectors for the training set using an EfficientNet feature extractor, a covariance matrix for the features is estimated. The Mahalanobis distance of an embedded test set sample fy to the mean of the embedded training set distribution µ can then be computed:

d(y) = (fy - µ)T S-1(fy - µ)

(2)

where S is the estimated covariance matrix. In practice, in order to make use of the multiple available feature levels, a covariance matrix is estimated for each feature level that was preliminarily chosen, and the final anomaly score for a given image y is calculated as the sum of the Mahalanobis distances for all feature levels. Alternatively (and similarly to what we do for the KNN approach), embeddings from the different feature levels can first be concatenated after pooling, and a single covariance matrix for all feature levels can then be estimated to compute

Figure 1. Schematic overview of the three pre-trained network based methods that we compare in this work: KNN, Mahalanobis, and PaDiM. In the KNN and PaDiM approaches, features from the different feature levels are concatenated. In both the KNN and the Mahalanobis approaches, global average pooling aggregates the information spatially and allows to manipulate smaller vectors. In PaDiM, spatial aggregation is done a posteriori, by taking the maximum local Mahalanobis distance over all pixel-embeddings.
a unique Mahalanobis distance as the anomaly score. In practice, we find that both methods lead to similar results (data not shown). Hence, we decide to stick with the original implementation, i.e. the sum of the different Mahalanobis distances over the multiple feature levels.
The advantage of using the Mahalanobis distance is that unlike classical euclidean distance, components with the highest variance will be weighting less than their lowest variance counterparts in the final computed distance. In that regard, work from Kamoi et al.35 suggests that the few features that explain most of the variance are not the ones that contribute to anomaly detection in high-dimensional cases, but rather the ones useful for the original classification pre-training task. This makes Mahalanobis distance a metric of choice when working with distributions in multi-dimensional spaces. However, the drawback is that it implicitly assumes the underlying embedded training sample distribution to be unimodal (i.e. distributed uniformly around a unique center), which might not be true. Note that the Mahalanobis distance is identical to the euclidean distance in the case where the co-variance matrix S is equal to the identity.
To compute the covariance matrix, we use two different estimators: Empirical, and Ledoit-Wolf.36 While the Empirical (or maximum likelihood) estimation works well given enough samples are provided, Ledoit's method goes further by shrinking the empirical covariance matrix: diagonal coefficients of the covariance matrix are artificially boosted, lessening the impact of the non-diagonal ones. This is expected to be valuable in low data regimes, where the number of available datapoints can become lower than the number of features for which to compute the covariance matrix. Errors in estimations will therefore be reduced, and invertibility of the matrix is expected to improve.
3.1.3 PaDiM
PaDiM (Patch Distribution Modeling Framework for Anomaly Detection and Localization)28 can be seen as a pixel-wise extension of the Mahalanobis approach, where one covariance matrix is estimated per spatial embedded location. Similarly to what is done for the SPADE27 anomaly localization module, the extracted spatial embeddings are first aligned and concatenated over feature levels. Because these concatenated spatial embeddings are not global-average-pooled like for the KNN or Mahalanobis method, one covariance matrix Si,j can

then be estimated per spatial location (or patch) (i, j) in the concatenated embedding space, leading to one anomaly score per location (or patch). The final image anomaly score can then be taken as the maximum of the anomaly scores for all locations:

d(y) = max
i,j

(fyi,j - µi,j )T Si-,j1(fyi,j - µi,j )

(3)

The advantage of this approach is that it exploits direct correlations between the features at different scales through the concatenation of feature levels, and allows to localize anomalies on heatmaps without relying on gradient-based visual explanations such as Grad-CAM.37 PaDiM outperforms other existing methods on the MVTec dataset, but has a significant drawback: its number of covariance matrices (equal to the number of spatial locations in the aligned and concatenated embedding space). This can significantly increase both training and inference times in practice, due to the estimation of the covariance matrices and the computation of corresponding Mahalanobis distances, respectively.

3.2 Data augmentation
Data augmentation is used in the original Mahalanobis method1 paper. The authors find that although it does not improve the detection performances significantly, data augmentation still is paramount to automatically set a working point (threshold). Data augmentation has been widely used as a regularization process in the training of neural networks to improve generalization, but it is not obvious that it can help when the feature extractor is frozen. Notably, there is a priori no reason that data augmentation maintains the embeddings gaussianity assumption of the Mahalanobis method. In this work, we use data augmentation as it is susceptible to help each of the three methods detailed above in the extremely low data regimes by enriching the training set with new samples.
Because images invariances are different for each class of object and texture of the MVTec dataset, we use different data augmentations for each. We design them manually, by visualizing augmented images from the training sets. The set of available transformations from which we pick are: vertical and horizontal flips, translations, rotations, zoom, and brightness change. For brightness, we either add to each image a random value for all pixels, or multiply each image by a random multiplicative factor. For rotations in texture classes, we mostly use multiples of 90° since they do not require any type of padding for squared images. For each transformation, we define whether they are applied or not, and if yes in which range. When selected for a given class, these data augmentations are applied systematically: for example, for the capsule class, each image is translated by a different randomly sampled number of pixels in the range [-10, 10] on both the x and y axis. Nonetheless, combining different affine transformations can lead to unrealistic cases. To avoid this, we simply look at the most extreme augmentation examples and consequently derive functional bounds for the transformation parameters. This explains why some of the chosen parameters may look peculiar. Chosen data augmentation parametrization for each class is available in Appendix A (Table 4).
Defining a proper data augmentation is expected to be crucial. One should notice that we design our strategies only once at the beginning: we do not optimize them with regards to the validation sets in order to improve performances. This is in order to simulate what a practitioner would do on a new use case. Note that the data augmentations are also designed as a compromise for all three methods. Indeed, a better data augmentation could probably be found for each individual method: for example, large translations on the object classes may improve Mahalanobis and KNN but could deteriorate PaDiM, since the latter does not discard the location information like the other two do via global average pooling.

4. EXPERIMENTS
4.1 Dataset and metrics
Since we are trying to solve a quality inspection task, we apply the methods to the MVTec AD dataset,2 a now very common benchmark dataset in the anomaly detection field. This dataset is composed of 3629 training images and 1725 test images belonging to 15 different categories (or classes). The categories are often split into two types: textures (5 categories) and objects (10 categories). We train and validate our models on each category independently.

When reporting the area under the ROC curve (AUC), we solely focus on image-wise detection performances, and do not report any pixel-wise AUC metric, which usually aims to assess quality of the anomaly segmentation. This is because image-wise AUC is actually the main metric of interest in the industrial set-up. Additionally, pixel-wise AUC can easily reach high values, even when there still are false positives left on every single localization map, making the metric useless for choosing the right model to deploy to production.
4.2 Implementation details
For a fair comparison between KNN, Mahalanobis, and PaDiM, we use the same feature extractor for all three methods: we choose EfficientNet-B4 pre-trained on the ImageNet dataset,38 since it provided good results in previous anomaly detection works,1, 28 as well as in our own network ablations (data not shown). For the feature levels, we decide to use the output of the blocks 4, 6, and 7 of the chosen feature extractor as our embedding layers, similarly to PaDiM original paper. These choices may be slightly detrimental to some methods and can explain the differences of performances with the ones reported in the original papers.
We implement all methods using Tensorflow / Keras for embedding the images with EfficientNetB4, and Scikit-Learn39 for the traditional anomaly detection techniques (k nearest-neighors, covariance estimation, Mahalanobis distance). For data augmentation, we rely on Imgaug.40 Contrarily to other previous approaches, we use almost no pre-processing, and only resize the images to the input resolution which was used by the feature extractor network for training. As we are using EfficientNet-B4, we resize all images to 380 x 380 pixels.
4.3 How to measure robustness?
In order to assess the methods robustness to low data regimes, we apply the following process. On a grid of sample sizes, we sample N samples randomly without replacement. We fit a model with the chosen method on these N samples. We replicate the process M times for each of the chosen N sample value in order to have a robust estimation of the performance for every N .
We iterate with models from all three methods, and use the same samples for each method to keep the comparison as fair as possible. We make the computation effective by encoding all training and evaluation embeddings with the feature extractor in advance. This way, for each new model fitting, only the later outlier detection module (KNN, Mahalanobis, or PaDiM) requires computation, which does not need to rely on a GPU or other intensive resources. In practice, for the MVTec dataset, we use a reproduction factor of M = 5. We use sample sizes ranging from 5 to the maximum sample size, with a step of 5 for sample sizes N below 50, and of 10 for sample sizes above. This is because increasing the sample size provides logarithmically diminishing returns. Over the MVTec dataset, we hence compute 3094 models for each method. The mere fact that we are able to fit and evaluate these numerous models is a strong argument in favor of the adoption of these methods, since they are blazing fast to train and do not require demanding compute resources.
In order to evaluate the robustness of the methods to the low data regime, we introduce a new metric that we call the "area under the AUC-percent curve". The area under the AUC-percent curve is defined as the area under the curve that gives the AUC as a function of the percentage of the total original data that was used for training the corresponding model. Defining such a metric allows us to average it across all MVTec categories, since the latter do not share the same amount of total available training images.
4.4 Robustness to low data regimes
Table 1 shows the AUC obtained for each method using all available training images of the MVTec dataset. For the Mahalanobis method, we compare the Empirical and the Ledoit estimators of the covariance matrix. Since our experiments show Ledoit perform betters in small setups, we used this estimator for PaDiM. Results per category are available in the Appendix B (Table 5). The following can be noticed. First, the KNN approach is outperformed by all other methods. Second, the choice of the covariance matrix estimator in the Mahalanobis method has a rather low impact, with Ledoit only being slightly better than Empirical. This is expected since the amount of data is reasonably high, making the shrinking of the covariance matrix unnecessary. Finally, PaDiM appears clearly as the best method for textures, with an impressive AUC of 0.996. However, it is outperformed by Mahalanobis on object categories on average.

Table 1. Averaged AUCs at maximum sample size on the MVTec AD dataset.

all textures objects

KNN
0.911 0.893 0.919

Mahalanobis (Empirical)
0.957 0.963 0.954

Mahalanobis (Ledoit) 0.962 0.957 0.964

PaDiM (Ledoit)
0.957 0.996 0.938

Figure 2. Images from the hazelnut category validation set (first row), with associated heatmaps obtained using PaDiM (second row). Notice how small local variations of the background induce false detections from PaDiM.
When looking at the per-class breakdown, the under-performance of PaDiM on objects is mostly due to the hazelnut category, which reaches an AUC of only 0.81. This is because the background is sometimes "polluted" by small particles for this category, as small coloured dots can be present on the images. Figure 2 gives an example of the issue. PaDiM tends to detect these as anomalies very strongly. Our hypothesis is that this is due to the fact that PaDiM models the normality in a local manner ("per patch"), while the other methods do not because of the global average pooling. Since the background is most of the time black, any slight local variation of the background will hence be detected as anomalous by PaDiM. Consequently, PaDiM is expected to be less robust to any kind of local perturbations compared to the other methods. Cropping, like it is done in the original PaDiM implementation,28 can solve the issue. Indeed, when using a centered crop of size 600 x 600 pixels on the original images, it is possible to reach a much higher AUC of above 0.99. We hypothesize that data augmentation may also help to partially solve it, while methods making a more explicit use of spatial information such as DFR29 are expected to work even better in these kinds of pollution setups.
Note that these curves also partially answer the question "how much data do we need?". By looking at them, it is possible to know if the AUC can be improved by adding more data (ex: PaDiM on "capsule") or not (ex: PaDiM on "hazelnut", KNN on "metal nut", Mahalanobis on "pill", ...).
Table 2 recapitulates the averaged areas under the AUC-percent curves for the MVTec AD dataset. The differences between approaches are clearer. Notably, these results demonstrate the interest of using the Ledoit covariance estimator instead of the Empirical one in the case of small training sample sizes. We complement these results with Figure 3, which displays the average AUC obtained on all MVTec classes as a function of the absolute sample size used (the breakdown for objects vs textures is available in figure 7 in appendix B). Though Mahalanobis and PaDiM obtain similar performances when the training sample size is near 200 images, they behave very differently in low data regimes. In particular, Mahalanobis with Ledoit covariance estimator appears as the most robust to low data regimes of all methods, with an average AUC close to 0.85 when using only 5 training images: this is actually close to the performance of autoencoders with the use of the full training set.2 In fact, with only 50 samples, the method reaches an average AUC of 0.927, beating most of the existing anomaly detection methods.
As expected, using the Empirical covariance estimator is way more sensible to small training sample sizes. We also notice a consistent drop of performances when the sample size is in the neighborhood of the number of

Table 2. Averaged areas under the AUC-percent curve on the MVTec AD dataset.

all textures objects

KNN
0.870 0.864 0.873

Mahalanobis (Empirical)
0.876 0.882 0.873

Mahalanobis (Ledoit) 0.920 0.925 0.918

PaDiM (Ledoit)
0.899 0.973 0.861

Figure 3. Average AUC across all MVTec classes, as a function of sample size. We removed the toothbrush category from the aggregation due to its low amount of datapoints.
features of the first feature level (the output of the block4 level has dimension 112). A common interpretation for this phenomena is that the inverse of the empirical covariance estimator seems particularly poor at estimating the actual inverse of the true covariance matrix (a.k.a. the precision matrix) in the case where the number of samples is close to the number of features.41 For large covariance matrices with only few samples, the estimated matrix is singular, and the non-diagonal terms that actually represent correlation between features are indistinguishable from noise. While Ledoit estimator solves this by fixing the covariance matrix estimator, one could try to directly estimate the precision matrix in order to tackle the issue. Note that we expect the issue to arise for other feature levels if we were to use more data (since the other feature levels have a higher number of dimensions).
Comparatively, PaDiM seems to require more images to perform well. We hypothesize that this is due to the relative complexity of the method, as it estimates several covariance matrices instead of one. However, it is extremely stable for all textures: with only 5 images, the technique leads to an average AUC of 0.986, with an AUC of 1.0 on two texture categories: leather and carpet.
4.5 Data augmentation for low data regimes
We now show the results when augmenting the data by a factor of 10, making the effective sample size 11 times bigger than the original datasets. Figure 4 shows the AUC as a function of the original sample size for all methods, with and without the 10-times data augmentation. The areas under the AUC-percent curves are presented in Table 3. In general, all methods benefit from data augmentation. As hypothesized, because PaDiM is a more complex model, it generally benefits more from additional data --and hence from data augmentation-- than the other methods. It can be noticed that using data augmentation makes the Ledoit covariance estimation slightly worse than the Empirical one as soon as the original sample size exceeds 50. Data augmentation almost always help in very low data regimes (less than 50 images). When dealing with a larger sample size (on average more than 200 images), data augmentation can however be detrimental, as shown by the Mahalanobis results on the grid, capsule or screw categories. Per class details can be found in Appendix B (Figure 8).
At maximum sample size (see also Table 7 in Appendix B), data augmentation does not help much Mahalanobis, as mentioned in the original paper.1 However, we observe a significant improvement for PaDiM, which is mainly due to the gain on the hazelnut category (AUC increases from 0.81 to 0.89): this makes PaDiM with data augmentation the best performing method overall.

Table 3. Averaged areas under the AUC-percent curve on the MVTec AD dataset, for both the original dataset and its 10-times augmented counterpart.

augmentation factor all textures objects

KNN

0 0.870 0.864 0.873

10 0.877 0.867 0.882

Mahalanobis

(Empirical)

0

10

0.876 0.921

0.882 0.933

0.873 0.916

Mahalanobis

(Ledoit)

0

10

0.920 0.926

0.925 0.925

0.918 0.927

PaDiM

(Ledoit)

0

10

0.899 0.930

0.973 0.974

0.861 0.908

Figure 4. Average AUC across all MVTec classes as a function of the original sample size, for both the original data and its 10-times augmented counterpart. We removed the toothbrush category from the aggregation due to its low amount of datapoints.
When focusing on small sample sizes, we can see that: a) using 30 images, data augmentation allows to gain 0.02 and 0.08 of AUC for Mahalanobis with Ledoit and PaDiM respectively, which corresponds to the performance obtained with 70 and 160 original images without data augmentation for the same methods respectively; and that b) using only 10 images and with data augmentation, we can reach an AUC higher than 0.9 for both Mahalanobis Ledoit and PaDiM, which is better than most existing autoencoder based approaches.2 Corresponding tables are available in Appendix B (Table 8 and Table 9).
The results also validate our hypothesis from section 4.4 that data augmentation can help with PaDiM failure cases such as hazelnut, for which better results are obtained with 10 images and a 10-factor data augmentation rather than with all of the available 391 images. To some extent, data augmentation, if chosen accordingly to semantic invariances, can translate into a location invariance to background noise, which, in the other methods, is achieved by global average pooling.
4.6 How much data augmentation is needed?
On Figure 5, we show results for varying amounts of data augmentation applied: each curve represents the AUC as a function of the original sample size for a given augmentation factor. Because of compute time, we discard PaDiM from this experiment. As expected, increasing data augmentation provides diminishing returns. For Mahalanobis with Ledoit, it can even be harmful when dealing with more than 200 images. When working with more than 50 images, little difference is observed between a 10 and a 20 augmentation factor. Under 50 images, pushing the data augmentation further may still help improve the AUC slightly. For Mahalanobis with Empirical, a heavy data augmentation allows to avoid the performance drops discussed in 4.4. Finally, the KNN method always benefits from data augmentation, though its performance remains weaker than that of the other methods. This may be because the KNN method is by construction more dependent on the number of training datapoints than the other methods, in the hypothesis that a larger number of datapoints is required to properly separate the normal distribution from the anomalies (no distribution is estimated in the KNN case).

Figure 5. Average AUC across all MVTec classes as a function of original sample size, for the original data and for multiple factors of data augmentation (one curve per augmentation factor). We removed the toothbrush category from the aggregation due to its low amount of datapoints.
Figure 6. Average AUC across all MVTec classes as a function of the original sample size, for both the original data and its 1-time augmented counterpart (removing the original datapoints). We removed the toothbrush category from the aggregation due to its low amount of datapoints.
One other question that arises is the relationship between data augmentation and a possible increase in training time. For the KNN and the Mahalanobis techniques, training time is mostly spent on image embedding, which grows linearly with the amount of training images. To minimize training time, one could augment each image once and discard the original images for training (thus only keeping the augmented ones). That way, training time would remain stable while diversity of the training set would increase. Figure 6 shows that this strategy does not work in practice. We hypothesize that this is due to the shift induced by our data augmentation.
5. CONCLUSION In this work, we compared three different approaches that all make use of deep pre-trained feature extractors. We showed that although these methods are in general quite robust to small sample sizes, they can still be improved further by using data augmentation. Our results show that without training any deep learning model, one can achieve very good results, even with few images. Notably, with only 10 images per class and in less than 2 seconds of training, we achieve better results than most auto encoders based techniques on the MVtec dataset.
A question that arises for real time applications like automated quality control is the trade-off between the increase in training time caused by the data augmentation, and the possibility to manually gather and annotate more images. We argue that the latter cannot be done for very small series of parts, since the whole production in these cases may not exceed 500 parts, most of which a human operator can inspect instead of annotate.

One of the downsides of our current methodology is that the data augmentation policy has to be designed manually. A possibility in the future could be to use reinforcement learning to generate optimal augmentation policies: this could be made possible thanks to the low training time that the pre-trained networks based methods offer. However, as we have shown, the data augmentation policy for a given sample size may not be optimal for another one, which increases the complexity of the search policy.
REFERENCES
[1] Rippel, O., Mertens, P., and Merhof, D., "Modeling the distribution of normal data in pre-trained deep features for anomaly detection," arXiv preprint arXiv:2005.14140 (2020).
[2] Bergmann, P., Fauser, M., Sattlegger, D., and Steger, C., "Mvtec ad­a comprehensive real-world dataset for unsupervised anomaly detection," in [Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ], 9592­9600 (2019).
[3] Ren, R., Hung, T., and Tan, K. C., "A generic deep-learning-based approach for automated surface inspection," IEEE transactions on cybernetics 48(3), 929­940 (2017).
[4] Cordier, A., Das, D., and Gutierrez, P., "Active learning using weakly supervised signals for quality inspection," arXiv preprint arXiv:2104.02973 (2021).
[5] Ronneberger, O., Fischer, P., and Brox, T., "U-net: Convolutional networks for biomedical image segmentation," in [International Conference on Medical image computing and computer-assisted intervention ], 234­241, Springer (2015).
[6] Lin, T.-Y., Goyal, P., Girshick, R., He, K., and Doll´ar, P., "Focal loss for dense object detection," in [Proceedings of the IEEE international conference on computer vision ], 2980­2988 (2017).
[7] Zambal, S., Heindl, C., Eitzinger, C., and Scharinger, J., "End-to-end defect detection in automated fiber placement based on artificially generated data," in [Fourteenth International Conference on Quality Control by Artificial Vision], 11172, 111721G, International Society for Optics and Photonics (2019).
[8] Gutierrez, P., Luschkova, M., Cordier, A., Shukor, M., Schappert, M., and Dahmen, T., "Synthetic training data generation for deep learning based quality inspection," arXiv preprint arXiv:2104.02980 (2021).
[9] Pang, G., Shen, C., Cao, L., and Hengel, A. v. d., "Deep learning for anomaly detection: A review," arXiv preprint arXiv:2007.02500 (2020).
[10] Huang, C., Ye, F., Cao, J., Li, M., Zhang, Y., and Lu, C., "Attribute restoration framework for anomaly detection," arXiv preprint arXiv:1911.10676 (2019).
[11] Bergmann, P., L¨owe, S., Fauser, M., Sattlegger, D., and Steger, C., "Improving unsupervised defect segmentation by applying structural similarity to autoencoders," arXiv preprint arXiv:1807.02011 (2018).
[12] Baur, C., Denner, S., Wiestler, B., Navab, N., and Albarqouni, S., "Autoencoders for unsupervised anomaly segmentation in brain mr images: A comparative study," Medical Image Analysis , 101952 (2021).
[13] Schlegl, T., Seeb¨ock, P., Waldstein, S. M., Schmidt-Erfurth, U., and Langs, G., "Unsupervised anomaly detection with generative adversarial networks to guide marker discovery," in [International conference on information processing in medical imaging ], 146­157, Springer (2017).
[14] Golan, I. and El-Yaniv, R., "Deep anomaly detection using geometric transformations," arXiv preprint arXiv:1805.10917 (2018).
[15] Hendrycks, D., Mazeika, M., Kadavath, S., and Song, D., "Using self-supervised learning can improve model robustness and uncertainty," arXiv preprint arXiv:1906.12340 (2019).
[16] Bergman, L. and Hoshen, Y., "Classification-based anomaly detection for general data," arXiv preprint arXiv:2005.02359 (2020).
[17] Perera, P. and Patel, V. M., "Learning deep features for one-class classification," IEEE Transactions on Image Processing 28(11), 5450­5463 (2019).
[18] Tack, J., Mo, S., Jeong, J., and Shin, J., "Csi: Novelty detection via contrastive learning on distributionally shifted instances," arXiv preprint arXiv:2007.08176 (2020).
[19] Hendrycks, D., Mazeika, M., and Dietterich, T., "Deep anomaly detection with outlier exposure," arXiv preprint arXiv:1812.04606 (2018).
[20] Sch¨olkopf, B., Williamson, R. C., Smola, A. J., Shawe-Taylor, J., Platt, J. C., et al., "Support vector method for novelty detection.," in [NIPS ], 12, 582­588, Citeseer (1999).

[21] Ruff, L., Vandermeulen, R. A., G¨ornitz, N., Binder, A., Mu¨ller, E., Mu¨ller, K.-R., and Kloft, M., "Deep semi-supervised anomaly detection," arXiv preprint arXiv:1906.02694 (2019).
[22] Ruff, L., Vandermeulen, R. A., Franks, B. J., Mu¨ller, K.-R., and Kloft, M., "Rethinking assumptions in deep anomaly detection," arXiv preprint arXiv:2006.00339 (2020).
[23] Liznerski, P., Ruff, L., Vandermeulen, R. A., Franks, B. J., Kloft, M., and Mu¨ller, K.-R., "Explainable deep one-class classification," arXiv preprint arXiv:2007.01760 (2020).
[24] Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L., "Imagenet: A large-scale hierarchical image database," in [2009 IEEE Conference on Computer Vision and Pattern Recognition], 248­255 (2009).
[25] Bergman, L., Cohen, N., and Hoshen, Y., "Deep nearest neighbor anomaly detection," arXiv preprint arXiv:2002.10445 (2020).
[26] Erdil, E., Chaitanya, K., and Konukoglu, E., "Unsupervised out-of-distribution detection using kernel density estimation," arXiv preprint arXiv:2006.10712 (2020).
[27] Cohen, N. and Hoshen, Y., "Sub-image anomaly detection with deep pyramid correspondences," arXiv preprint arXiv:2005.02357 (2020).
[28] Defard, T., Setkov, A., Loesch, A., and Audigier, R., "Padim: a patch distribution modeling framework for anomaly detection and localization," arXiv preprint arXiv:2011.08785 (2020).
[29] Yang, J., Shi, Y., and Qi, Z., "Dfr: Deep feature reconstruction for unsupervised anomaly segmentation," arXiv preprint arXiv:2012.07122 (2020).
[30] Bergmann, P., Fauser, M., Sattlegger, D., and Steger, C., "Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings," in [Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition], 4183­4192 (2020).
[31] Wang, G., Han, S., Ding, E., and Huang, D., "Student-teacher feature pyramid matching for unsupervised anomaly detection," arXiv preprint arXiv:2103.04257 (2021).
[32] Salehi, M., Sadjadi, N., Baselizadeh, S., Rohban, M. H., and Rabiee, H. R., "Multiresolution knowledge distillation for anomaly detection," arXiv preprint arXiv:2011.11108 (2020).
[33] Choi, S. and Chung, S.-Y., "Novelty detection via blurring," arXiv preprint arXiv:1911.11943 (2019). [34] Goodfellow, I. J., Mirza, M., Xiao, D., Courville, A., and Bengio, Y., "An empirical investigation of catas-
trophic forgetting in gradient-based neural networks," arXiv preprint arXiv:1312.6211 (2013). [35] Kamoi, R. and Kobayashi, K., "Why is the mahalanobis distance effective for anomaly detection?," (2020). [36] Ledoit, O. and Wolf, M., "A well-conditioned estimator for large-dimensional covariance matrices," Journal
of Multivariate Analysis 88(2), 365­411 (2004). [37] Selvaraju, R. R., Das, A., Vedantam, R., Cogswell, M., Parikh, D., and Batra, D., "Grad-cam: Why did you
say that? visual explanations from deep networks via gradient-based localization," CoRR abs/1610.02391 (2016). [38] Tan, M. and Le, Q. V., "Efficientnet: Rethinking model scaling for convolutional neural networks," CoRR abs/1905.11946 (2019). [39] Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E., "Scikit-learn: Machine learning in Python," Journal of Machine Learning Research 12, 2825­2830 (2011). [40] Jung, A. B., "imgaug." https://github.com/aleju/imgaug (2018). [Online; accessed 30-Oct-2018]. [41] Senneret, M., Malevergne, Y., Abry, P., Perrin, G., and Jaffr`es, L., "Covariance versus precision matrix estimation for efficient asset allocation," IEEE Journal of Selected Topics in Signal Processing 10(6), 982­ 993 (2016).

APPENDIX A. DATA AUGMENTATION DETAILS
Table 4. Data augmentation details per MVTec AD category. "Rotate (90°s)" indicates rotations of multiples of 90° (hence 90°, 180°, and 270°). Each data augmentation was manually and visually designed by a human.

carpet tile leather grid wood capsule cable pill transistor metal nut toothbrush screw hazelnut zipper bottle

Flip (horizontal)
Yes Yes Yes Yes Yes
Yes Yes Yes Yes Yes -

Flip (vertical)
Yes Yes Yes Yes Yes
Yes Yes -

Translate (±x, ±y)
(5, 5) (5, 5) (5, 5) (5, 5) (5, 5) (10, 10) (10, 10) (10, 10) (5, 5) (10, 10) (10, 10) (10, 10) (10, 10) (30, 0) (5, 5)

Rotate (range in °)
(-2, +2) (-3, +3) (-5, +5) (-3, +3) (-2, +2) (-10, +10) (-10, +10) (-20, +20) (-10, +10)

Rotate (90°s) Yes Yes Yes Yes
Yes Yes Yes Yes

Zoom (range)
(0.98, 1.02)
(0.98, 1.02) -
(0.98, 1.02) (0.98, 1.02)
-

Add (range) (-10, 10) (-10, 10) (-10, 10) (-10, 10) (-10, 10) (-10, 10) (-10, 10) (-10, 10) (-10, 10) (-10, 10) (-10, 10) (-10, 10) (-10, 10) (-10, 10) (-10, 10)

Multiply (range) (0.9, 1.1) (0.9, 1.1) (0.9, 1.1) (0.9, 1.1) (0.9, 1.1) (0.9, 1.1) (0.9, 1.1) (0.9, 1.1) (0.9, 1.1) (0.9, 1.1) (0.9, 1.1) (0.9, 1.1) (0.9, 1.1) (0.9, 1.1) (0.9, 1.1)

APPENDIX B. RESULTS DETAILS

Table 5. AUC at maximum sample size per MVTec AD category.

carpet tile leather grid wood capsule cable pill transistor metal nut toothbrush screw hazelnut zipper bottle

KNN
0.982 0.995 0.994 0.625 0.870 0.948 0.938 0.816 0.927 0.868 0.919 0.884 0.952 0.945 0.996

Mahalanobis (Empirical)
1.000 1.000 1.000 0.833 0.982 0.960 0.975 0.811 0.976 0.952 0.983 0.911 0.997 0.974 1.000

Mahalanobis (Ledoit) 1.000 1.000 1.000 0.825 0.961 0.966 0.977 0.893 0.976 0.956 0.961 0.937 0.997 0.974 1.000

PaDiM (Ledoit)
1.000 0.997 1.000 0.988 0.993 0.959 0.980 0.947 0.949 0.953 0.864 0.964 0.809 0.956 0.994

Table 6. Area under the AUC-percent curve per MVTec AD category.

carpet tile leather grid wood capsule cable pill transistor metal nut toothbrush screw hazelnut zipper bottle

KNN
0.964 0.969 0.972 0.572 0.843 0.892 0.914 0.778 0.891 0.843 0.834 0.773 0.919 0.918 0.967

Mahalanobis (Empirical)
0.925 0.961 0.960 0.710 0.857 0.886 0.867 0.809 0.876 0.869 0.834 0.761 0.938 0.928 0.965

Mahalanobis (Ledoit) 0.980 0.976 0.979 0.762 0.928 0.926 0.934 0.850 0.931 0.912 0.875 0.852 0.976 0.944 0.975

PaDiM (Ledoit)
0.982 0.968 0.980 0.967 0.970 0.849 0.929 0.878 0.880 0.855 0.762 0.855 0.721 0.922 0.964

Figure 7. Average AUC across all MVTec AD texture and object categories as a function of sample size. We removed the toothbrush category from the aggregation due to its low amount of datapoints.

Table 7. Averaged AUCs at maximum sample size for MVTec, for both the original data and its 10-times augmented counterpart.

augmentation factor all textures objects

KNN

0 0.911 0.893 0.919

10 0.915 0.897 0.924

Mahalanobis

(Empirical)

0

10

0.957 0.961

0.963 0.962

0.954 0.961

Mahalanobis

(Ledoit)

0

10

0.962 0.959

0.957 0.949

0.964 0.963

PaDiM

(Ledoit)

0

10

0.957 0.966

0.996 0.995

0.938 0.951

Table 8. Averaged AUCs at original sample size 30 for MVTec, for both the original data and its 10-times augmented counterpart.

augmentation factor all textures objects

KNN

0 0.868 0.867 0.868

10 0.879 0.872 0.882

Mahalanobis

(Empirical)

0

10

0.815 0.936

0.809 0.948

0.818 0.930

Mahalanobis

(Ledoit)

0

10

0.913 0.938

0.923 0.936

0.908 0.939

PaDiM

(Ledoit)

0

10

0.864 0.941

0.990 0.993

0.801 0.915

Table 9. Averaged AUCs at original sample size 10 for MVTec, for both the original data and its 10-times augmented counterpart.

augmentation factor all textures objects

KNN

0 0.846 0.853 0.842

10 0.859 0.857 0.859

Mahalanobis

(Empirical)

0

10

0.687 0.767

0.701 0.788

0.680 0.757

Mahalanobis

(Ledoit)

0

10

0.872 0.908

0.892 0.912

0.863 0.905

PaDiM

(Ledoit)

0

10

0.809 0.904

0.989 0.991

0.719 0.860

Figure 8. AUC as a function of the original sample size for all MVTec categories.

