IMPERCEPTIBLE ADVERSARIAL EXAMPLES FOR FAKE IMAGE DETECTION
Quanyu Liao1, Yuezun Li3, Xin Wang2, Bin Kong2, Bin Zhu4, Siwei Lyu3, Youbing Yin2, Qi Song2, Xi Wu1
1 Chengdu University of Information Technology, Chengdu, China 2 Keya Medical, Seattle, USA
3 University at Buffalo, State University of New York, USA 4 Microsoft Research Asia, Beijing, China

arXiv:2106.01615v1 [cs.CV] 3 Jun 2021

ABSTRACT
Fooling people with highly realistic fake images generated with Deepfake or GANs brings a great social disturbance to our society. Many methods have been proposed to detect fake images, but they are vulnerable to adversarial perturbations ­ intentionally designed noises that can lead to the wrong prediction. Existing methods of attacking fake image detectors usually generate adversarial perturbations to perturb almost the entire image. This is redundant and increases the perceptibility of perturbations. In this paper, we propose a novel method to disrupt the fake image detection by determining key pixels to a fake image detector and attacking only the key pixels, which results in the L0 and the L2 norms of adversarial perturbations much less than those of existing works. Experiments on two public datasets with three fake image detectors indicate that our proposed method achieves state-ofthe-art performance in both white-box and black-box attacks.
Index Terms-- Deepfake, Adversarial Example.
1. INTRODUCTION
The development of generative adversarial networks (GANs) [1] enables generating high-quality images. Different GANs have been proposed to fit different applications. One of these applications is DeepFake [2], which can swap the face of a source subject with that of a target subject while retaining original facial expressions with high realism. It can be easily abused for malicious purposes, such as replacing the original face in a pornographic video with the victim's face, which can cause a significant social disturbance.
Many methods [3, 4] have been proposed to detect fake images and videos. These methods are usually based on CNNs and trained on public DeepFake datasets such as UADFV [5], CelebDF-v2 [6], and Faceforensics++ [4]. Despite they can achieve high performance, these methods are vulnerable to adversarial perturbations [7, 8], which are intentionally designed noises that can fool these methods.
 Corresponding authors: Xin Wang (xinw@keyamedna.com), Xi Wu (xi.wu@cuit.edu.cn).

(a)

(b)

(c)

(d)

(e)

Fig. 1. Comparison of generated adversarial perturbations of FGSM, DeepFool, and our Key Region Attack (KRA), for fake image detection. (a) is a fake image generated by DeepFake and detected as a fake image by the detector [4]. (b) is an adversarial perturbation generated by FGSM [7]. (c) is an adversarial perturbation generated by DeepFool [16]. (d) is an adversarial perturbation generated by our method KRA. (e) is an adversarial example generated by adding perturbation (d) on image (a), which is detected as a real image by the detector [4]. (b)(c)(d) are obtained after normalizing the pixel intensity of perturbations to [0, 255]).

Several anti-forensics methods [9, 10] have been proposed recently to evade fake detection using adversarial perturbations. They are based on either minimizing the distortion of perturbations on the entire image [7, 11] or reducing the size of the attacking area without considering the distortion and the time consumption [12]. More specifically, gradient-based attack methods [10] and L2-distortion minimizing attack [9] change almost every pixel, which is unnecessary and makes resulting adversarial perturbations more perceptible. Other adversarial methods [12, 9, 13, 14, 15] aim to reduce the L0 norm of generated perturbations. They have high time consumption, and their generated perturbations have a higher L2 norm than that of perturbations generated by gradient-based attack methods or L2-distortion minimizing attacks.
To address the aforementioned limitations of existing adversarial attacks, we propose in this paper a novel method, called Key Region Attack (KRA), to attack only a small portion of an input image while minimizing the distortion of added perturbations. Existing methods [10, 9] minimize either L0 or L2 norm of perturbations. They cannot make perturbations both sparse and imperceptible. To make perturbations sparse and imperceptible, KRA aims to minimize both L2 and L0 norms of perturbations simultaneously. More specifically, KRA leverages the gradient of multiple convolutional layers and spatial information to efficiently extract

Multi-layers Semantic Key Region Selection from Heatmaps

Fake

Fake Image Detector

Mask

Fig. 2. Illustration of Multi-layers SemanticKey Region Selection (MLSKRS): MLSKRS extracts mask mi for each convolutional layer li and combines masks mi to generates a final mask M.

key pixels that fake detection relies on, and then apply a conventional adversarial method to generate adversarial perturbations by modifying only the key pixels. Experimental results show that KRA achieves state-of-the-art attack performance with high efficiency and significantly reduce both L0 and L2 norms of generated disturbances.
Our method has the following key features, which are also our main contributions: 1) We propose to attack only pixels key to fake detection in an image in generating adversarial disturbances. Attacking key pixels alone can greatly disrupts fake detection while minimizing both L0 and L2 of perturbations at the same time. 2) Instead of using the heatmap of the last layer as key pixels, which leads to a large L0 norm, we propose Multi-Layers Key Semantic Region Selection (MLSKRS) to combine heatmaps from both shallow and deep layers to extract key pixels in an image. The resulting set of key pixels is much smaller yet effective. 3) KRA is designed as a container to support integrating different adversarial attack methods, include FGSM, Deepfool, etc.

2. KRA METHOD

2.1. Problem Definition
As mentioned before, we aim to minimize both L2 and L0 norms of perturbations simultaneously to generate sparse and imperceptible adversarial disturbances of fake detectors. This can be formulated as follows:

minimize r 2 + r 0

(1)

subject f (x + r) = f (x)

where x is a clean input image, f (x) is the predicted result of the fake detector, and r is an adversarial perturbation.
Minimizing the L0 norm of perturbations is an N P problem [13]. To address this problem, KRA does not minimize the L0 norm directly. Instead, it finds key pixel regions of the input image that fake detection relies on and generates adversarial perturbations by modifying only the found key pixels.

Algorithm 1 Multi-layers Semantic Information Key Region

Selection (MLSKRS) Input: image x, attack threshold t, fake image detector
f (·), set of target layers L, operation of upsampling 

Output: final mask M

1: for li : L do

2: for hji : li do

3:

gij

=

f (x)  hji

4: end for

5:

Gi =

k j=1

gij

6:

G^i

=

(

Gi -min(Gi ) max(Gi )-min(Gi

)

)

7: mi = (G^i > t)

8: end for

9: M =

n i=1

mi

10: return M

In this way, the L0 norm of perturbations can be efficiently reduced, and resulting adversarial examples are imperceptible. We reformulate the problem as follows:

minimize ||r||2 + ||M||0

(2)

subjectto f (x + (r · M)) = f (x)

where M is the set of key pixels of the input image.

2.2. Multi-layers Semantic Key Region Selection
Convolutional layers retain spatial information [17, 18], which can indicate which regions of an input image have a higher response to the output of the classifier. This information is exploited in KRA to extract key pixels to fake detection. Only key pixels are disturbed.
A straightforward method is to generate a heatmap based only on the last convolutional layer as in prior work [18] (see the top right heatmap in Fig. 2). This method does not suppress the L0 norm of adversarial perturbations well since deeper convolutional layers correspond to a larger receptive field, leading to a large and continuous region of high response in the generated heatmap. To address this problem, we propose Multi-layers Semantic Key Region Selection (MLSKRS) to generate heatmaps based on multiple convolution layers (see the heatmaps in Fig. 2), including both shallow layers and deep layers. Since shallow layers correspond to a small receptive field, MLSKRS can significantly reduce the L0 norm of the heatmap while maintaining effective gradients generated on deep layers.
Denote L = {li}ni=1 as the set of layers we consider and li = {hji }kj=1 as the i-th layer that contains k channels. For layer li, MLSKRA first calculates the gradient of channel hji

Algorithm 2 Key Region Attack (KRA) Input: image x, fake detector f , attack method , initial at-

tack threshold t, lowest attack threshold t , reduction

step size of threshold 

Output: perturbation r

1: Initialize: u = 0, t0 = t, r-1  [0]W ×H 2: while True do

3: Mu = MLSKRS(x, tu)

4: ru = (x + ru-1)

5: ru = ru-1 + ru · Mu

6: if f (x + ru) = f (x) then

7:

break

8: end if

9: tu+1 = max(tu - , t )

10: u = u + 1

11: end while

12: return ru

and then sums up the gradients of all channels as

gij

=

f (x) hji ,

k
Gi = gij
j=1

(3)

then gradient Gi is normalized to [0, 1] and upsampled to the size of image x:

G^i

=

( Gi - min(Gi) ) max(Gi) - min(Gi)

(4)

where (·) denotes the operation of upsampling. The mask

corresponding to layer li can be obtained using an attack

threshold t:

mi = (G^i > t)

(5)

where  is an impulse response that turns a pixel greater than t to 1, otherwise to 0. MLSKRS combines all masks as

n

M = mi

(6)

i=1

where M denotes the final mask. The procedure of MLSKRS is summarized in Alg. 1.

2.3. Key Region Attack
Key Region Attack (KRA) attacks the key regions obtained from MSKRS iteratively. At each iteration, KRA dynamically updates the attack threshold and calculates key regions using MSKRS, and then utilizes an adversarial attack method to attack the key regions. Various state-of-the-art adversarial

attack methods can be used, e.g., PGD [11], DeepFool [19]), etc. As a result, KRA is a container that can flexibly integrate various attack methods to meet different requirements.
Let the selected adversarial attack method be  and u be the iteration index. The threshold is initialized as t0 = t, where t is a constant. At each iteration, the key region mask is obtained: Mu = MLSKRS(x, tu). Attacking method  is employed to generate an adversarial perturbation ru: ru = (x + ru-1). Then ru is masked by Mu and then added with ru-1 to generate ru. Finally, x + ru is sent to detector f to see whether this attack succeeds. If the attack succeeds, the iteration is terminated. Otherwise the threshold is updated as tu+1 = max(tu - , t ) to gradually enlarge key regions, where  is the reduction step size, and t is the lower bound of threshold. The procedure of KRA is shown in Alg. 2.
3. EXPERIMENTAL EVALUATION
Datasets. We validate attack methods on two public datasets, FaceForensics++ and CNN-Synthesis. FaceForensics++ [4] includes 1000 real videos and 1000 fake videos generated by Deepfake [2]. CNN-Synthesis [3] is generated with a variety of GANs on real images collected from the Internet. Fake Image Detectors. CNN-Synthesis Detector [3] is based on Resnet50 [16] and trained on CNN-Synthesis. Xception [4] is based on XceptionNet and trained on FaceForensics++. We also train three additional fake image detectors, Inceptionv3 [20], Resnet50, and Resnet101 [16], as attacking targets. Implementation Details. Our method is implemented using Pytorch 1.1.0. The experiments were run on Ubuntu 18.04 with one GPU of Nvidia Tesla T4 and the following values of the parameters in our method: t = 0.8, t = 0.1,  = 0.1. Evaluation Metrics. We use three metrics to evaluate the attacking performance of generated adversarial examples: (1) Attack Success Rate (ASR) to measures the attack success rate of adversarial examples: ASR = 1-(accattack/accclean), where accattack is the accuracy of the detector with adversarial examples, accclean is the accuracy of clean inputs. (2) Attack Transfer Ratio (ATR) to measure the black-box performance: AT R = ASRtarget/ASRorigin, where ASRtarget represents the ASR of attacking the target detector, and ASRorigin denotes the ASR of attacking the detector with which the adversarial examples are generated from. (3) Perceptibility. We use both L0 and L2 norms of perturbations to measure perceptibility, denoted as PL0 and PL2 , respectively. A lower value indicates more imperceptible. White-Box Attack. The attack performance of KRA is evaluated using PGD [11], denotes as KRA-PGD, and Deepfool [19], denoted as KRA-Deepfool. The white-box attack results are summary in Table 1. KRA achieves the state-of-the-art white-box attack performance: the accuracy of the three fake detectors is reduced to lower than 0.01 under KRA's attack. Nearly all images of FaceForensics++ and CNN-Synthesis

Attack Method

DataSet

L2 - attack [9] Cnn-Synth-all

Network Acc (Clean) Acc (Attack) ASR

PL2

Resnet-50

0.83

0.001

0.99 1 × 10-1

PL0 Time (s)

--

--

L0 - attack [9] Cnn-Synth-Fake Resnet-50

0.83

0.010

0.99

--

11%

--

L0 - attack [9] Cnn-Synth-Real Resnet-50

0.83

0.021

0.75

--

11%

--

KRA-PGD

Cnn-Synth-all Resnet-50

0.83

0.001

0.99 4 × 10-4 0.1% 0.13

KRA-PGD FaceForensics-all Xception

0.99

0.006

0.99 8 × 10-4 0.9% 0.15

KRA-PGD FaceForensics-all Resnet-50

0.99

0.001

0.99 1.5 × 10-3 0.9% 0.22

KRA-PGD FaceForensics-all Resnet-101

0.99

0.001

0.99 1.7 × 10-3 1%

0.4

KRA-DeepFool FaceForensics-all Xception

0.99

0.001

0.99 6 × 10-5 21%

2

Table 1. Results of White-Box Attack. Acc (Clean) means the accuracy obtained from clean inputs. Acc (Attack) denotes the accuracy obtained from adversarial examples. The 'Time' column shows the average attack time. FaceForensics-all and Cnn-Synth-all mean that both real images and fake images are used in the attack. Cnn-Synth-Fake means attacking only fake images, while Cnn-Synth-Real means attacking only real image.

Clean Xception Resnet-50 Resnet-101

Xception Acc ATR 0.99 -- 0.006 1.00 0.49 0.50 0.48 0.51

Resnet-50 Acc ATR 0.99 -- 0.43 0.56 0.001 1.00 0.42 0.57

Resnet-101 Acc ATR 0.99 -- 0.51 0.49 0.46 0.53 0.001 1.00

Inception-v3 Acc ATR 0.99 -- 0.36 0.63 0.56 0.43 0.59 0.40

Table 2. Results of black-box attack using KRA-PGD. 'Clean' means the accuracy obtained from clean inputs. The first column denotes the original detector that adversarial perturbations are generated with. The first row denotes the target detector that the adversarial examples are used to attack. have been attacked successfully, including both attacking real images into the fake or attacking fake images into the real. Compared with the L0-attack and L2-attack [9], KRA has significantly reduced both L0 norm and L2 norm of adversarial perturbations, indicating that KRA generates much more imperceptible adversarial perturbations. We can see from Table 1 that the L2 norm of the L2-attack is nearly 100 times higher than that of KRA, and the L0 norm of the L0-attack is 10 times higher. We can also see that the L2 norm of adversarial perturbations generated with KRA using Deepfool is reduced 10 times as compared with that of KRA using PGD. Black-Box Attack. In this evaluation, adversarial examples are generated from a detector, called original detector, and used to attack another detector, called target detector. Blackbox attack results are summarized in Table 2. We can see from the table that ATRs of KRA on different detectors are all within 50% ± 15%. Adversarial examples generated from Xception have a higher ASR on Inception-v3 than that on Resnet. Adversarial examples generated from the one Resnet have a high ASR on attacking the other Resnet, which can be explained that the two Resnet detectors learn relatively similar features due to their similar structures.
Both black-box and white-box attacks have the same setting. That means the PL0 and PL2 of perturbations in the black-box attack are identical to those in the white-box attack. Table 2 shows that KRA has a good black-box attack performance although adversarial disturbances generated by KRA have low L2 norm and L0 norm. The black-box attack performance of KRA can be further improved by loosening the L2 and L0 restriction at the cost of worsen perceptibility or using more aggressive attack methods. Qualitative Analysis. Fig. 3 shows qualitative examples generated by KRA using PGD and Deepfool as the integrated at-

(a)
(b)
(c)
(d)
(e)
(f)
(g)
Fig. 3. Qualitative results of KRA using PGD and Deepfool on FaceForensics [4]. The left three columns are fake images, the right three columns are real images. (a) are clean images without any perturbation. (b) are generated by PGD. (c) are generated by Deepfool. (d) are generated by KRA-PGD. (e) are generated by KRADeepfool. (f) and (g) are adversarial examples generated by KRAPGD and KRA-Deepfool, respectively. (b),(c),(d) and (e) are obtained by normalizing the pixel intensity of perturbations to [0, 255]. tack method. We can see that perturbations appear only on key-regions, and a generated adversarial perturbation is hard for humans to distinguish.
4. CONCLUSION
In this paper, we have presented the Key Region Attack (KRA) that generates imperceptible adversarial examples for attacking fake image detectors. KRA can flexibly integrate various attack methods to meet different requirements. Compare with previous methods, our method achieves the state-of-the-art performance for both white-box and black-box attacks, and

adversarial examples generated with our methods are more imperceptible than those with previous methods.
5. REFERENCES
[1] Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio, "Generative adversarial networks," NIPS, 2014.
[2] github, "deepfake" Accessed Apr 2, 2020.
[3] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, and Alexei A Efros, "Cnn-generated images are surprisingly easy to spot... for now," in CVPR, 2020.
[4] Andreas Rossler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, and Matthias Nießner, "Faceforensics++: Learning to detect manipulated facial images," in ICCV, 2019.
[5] Xin Yang, Yuezun Li, and Siwei Lyu, "Exposing deep fakes using inconsistent head poses," in ICASSP, 2019.
[6] Yuezun Li, Xin Yang, Pu Sun, Honggang Qi, and Siwei Lyu, "Celeb-df: A large-scale challenging dataset for deepfake forensics," in CVPR, 2020.
[7] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy, "Explaining and harnessing adversarial examples," ICLR, 2015.
[8] Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo Li, "Boosting adversarial attacks with momentum," in CVPR, 2018.
[9] Nicholas Carlini and Hany Farid, "Evading deepfakeimage detectors with white-and black-box attacks," in Proceedings of the CVPRW, 2020.
[10] Apurva Gandhi and Shomik Jain, "Adversarial perturbations fool deepfake detectors," in IJCNN, 2020.
[11] Nicholas Carlini and David Wagner, "Towards evaluating the robustness of neural networks," in 2017 ieee symposium on security and privacy (sp), 2017.
[12] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu, "Towards deep learning models resistant to adversarial attacks.," ICLR, 2018.
[13] Apostolos Modas, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard, "Sparsefool: a few pixels make a big difference," in CVPR, 2019.
[14] Jiawei Su, Danilo Vasconcellos Vargas, and Sakurai Kouichi, "One pixel attack for fooling deep neural networks," IEEE Transactions on Evolutionary Computation, 2017.

[15] Francesco Croce and Matthias Hein, "Sparse and imperceivable adversarial attacks," in International Conference on Computer Vision.
[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, "Deep residual learning for image recognition," in CVPR, 2016.
[17] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba, "Learning deep features for discriminative localization," in CVPR, 2016.
[18] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra, "Grad-cam: Visual explanations from deep networks via gradient-based localization," in ICCV, 2017.
[19] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard, "Deepfool: a simple and accurate method to fool deep neural networks," in ICCV, 2016.
[20] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna, "Rethinking the inception architecture for computer vision," in CVPR, 2016.

