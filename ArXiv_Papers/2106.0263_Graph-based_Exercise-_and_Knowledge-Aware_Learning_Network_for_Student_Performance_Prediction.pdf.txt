arXiv:2106.00263v1 [cs.AI] 1 Jun 2021

Graph-based Exercise- and Knowledge-Aware Learning Network for Student Performance Prediction
Mengfan Liu1,2, Pengyang Shao1,2, and Kun Zhang1,2,
1 Key Laboratory of Knowledge Engineering with Big Data, Hefei University of Technology, China
2 School of Computer Science and Information Engineering, Hefei University of Technology, China
{liumengff,shaopymark,zhang1028kun}@gmail.com
Abstract. Predicting student performance is a fundamental task in Intelligent Tutoring Systems (ITSs), by which we can learn about students' knowledge level and provide personalized teaching strategies for them. Researchers have made plenty of efforts on this task. They either leverage educational psychology methods to predict students' scores according to the learned knowledge proficiency, or make full use of Collaborative Filtering (CF) models to represent latent factors of students and exercises. However, most of these methods either neglect the exercise-specific characteristics (e.g., exercise materials), or cannot fully explore the high-order interactions between students, exercises, as well as knowledge concepts. To this end, we propose a Graph-based Exercise- and Knowledge-Aware Learning Network for accurate student score prediction. Specifically, we learn students' mastery of exercises and knowledge concepts respectively to model the two-fold effects of exercises and knowledge concepts. Then, to model the high-order interactions, we apply graph convolution techniques in the prediction process. Extensive experiments on two real-world datasets prove the effectiveness of our proposed Graph-EKLN.
Keywords: Education Data Mining · Intelligent Tutoring System · Collaborative Filtering · Graph Neural Network
1 Introduction
Intelligent Tutoring Systems (ITSs) aim at providing personalized guidance for students [2,5], which can be treated as an important supplementary for traditional offline teaching mode. It has attracted enormous attention from both industry and academics [12,22].
Usually, researchers consider the issue from the educational psychology perspective and propose cognitive diagnosis models to discover students' knowledge proficiency [2]. Among them, the Deterministic Inputs, Noisy "And" Gate
Kun Zhang is the corresponding author.

2

M. Liu et al.

(DINA) model is a representative method which uses multi-dimensional factors to represent students' knowledge states on specific knowledge concepts [10]. However, they ignore the influence of other exercise-specific characteristics. knowledge proficiency is not the only factor that affects students' final scores. For example, exercise materials can also influence exercises' difficulty [17].
Moreover, motivated by the observation that students and exercises are collaboratively correlated, researchers borrow success of Matrix Factorization (MF) techniques in recommender systems to model the interactions between students and exercises [25,26]. For example, In [25], the authors applied MF to learn the latent embeddings of students and exercises and predicted the scores based on the inner products of them. Although MF based models achieve great success in ITSs, they still have some weaknesses. First of all, MF based methods are still inadequate in utilizing knowledge concept information, which is very important for student performance prediction. Second, MF based methods cannot deal with the high-order collaborative information between student and exercises.
Since students and exercises naturally form a bipartite graph structure, it is natural to apply Graph Convolutional Network (GCN) to model the high-order collaborative information in the student-exercise-knowledge graph. However, different from scale-free networks, distribution of exercise-knowledge data does not satisfy the power law distribution (i.e., shown in the middle part of Fig. 1). More specifically, the degree distribution of exercise is uniform, i.e., most exercises are related to around 1 to 2 knowledge concepts, as shown in the right part of Fig. 1. Furthermore, the number of knowledge concepts is relatively small. Therefore, knowledge concept nodes would link to many exercise nodes, leading to over smoothing when propagating information through dense links. To this end, in this paper, we propose a novel Graph-based Exercise- and KnowledgeAware Learning Network (Graph-EKLN ), which takes the both influences of exercises and knowledge concepts into consideration for student performance prediction. For the effect of exercises, we apply GCN with link-specific aggregation functions [23] onto the student-exercise bipartite graph to explore the high-order collaborative information. For the effect of knowledge concepts, we replace exercises with their related knowledge concepts, and predict students' performance scores on knowledge concepts by MF. Along this line, the highorder graph structure information and knowledge concept information can be fully explored for the final student performance prediction.

Part 1 Part 2

students exercises
knowledge concepts

Input data (student-exercise-knowledge)

Exercise degree distribution in Part 1 Exercise degree distribution in Part 2

Fig. 1. Data structure of ITSs

Graph-based Exercise- and Knowledge-Aware Learning Network

3

2 Related Work

2.1 Educational Psychology
Educational psychology models are mainly discussed from two sides: cognitive diagnosis models and knowledge tracing models [17,20]. Cognitive diagnosis models, assuming students' knowledge states are static throughout their practice, aim to discover students' proficiency to predict their future performance [18,5,28,35,10,8]. Item Response Theory (IRT) [11,5] was a typical and straightforward cognitive diagnosis model which used a one-dimensional continuous variable  to indicate each student's knowledge state and used  to indicate each exercise's difficulty. In this way, ( - ) was proportional to the predicted probability of the question being answered correctly. Another typical model was DINA [10]. DINA was a multi-dimensional discrete model to represent each student with a binary latent vector. We can know whether the student has mastered related knowledge concepts from students' knowledge states ('1' indicates the student has mastered the target knowledge concepts and vice versa). Recently, deep neural networks have been used for cognitive diagnosis. For example, Cheng et al. leveraged deep learning to enhance the process of diagnosing parameters [8]. Wang et al. proposed to incorporate neural networks to learn interaction functions between students and exercises [28]. Knowledge tracing models aims to track the changes of students' knowledge states during practice [9,17,14,24]. Researchers proposed a first-order Markov process model, in which knowledge states will change with transition probabilities after a learning opportunity [9]. Piech et al. introduced a recurrent neural network to describe the change of knowledge states [21]. Liu et al. explored the text content of exercises by integrating a bidirectional LSTM model [17].

2.2 Collaborative Filtering in Recommender Systems
Recommender systems have been widely utilized to help users find their potential interests in many areas [1,7,30]. Classical models utilize MF techniques to learn user and item embeddings [16]. Motivated by the observation that users and items naturally form a bipartite graph, researchers proposed to utilize Graph Convolution Networks (GCNs) to model high-order collaborative signals in recommender systems [6,23,31,32,33,34]. E.g., Wang et al. used the graph convolution technique to encode collaborative signals in the propagation process [29]. Wu et al. modeled social diffusion process by propagating embeddings in the social network [32]. Chen et al. enhanced graph based recommendation by empirically removing non-linearities and proposed a residual network based structure [6].

2.3 Collaborative Filtering in ITS
Motivated by the observation that students and exercises are collaboratively correlated, researchers mapped educational data to user-item-rating triple data in recommender systems, then applied MF for predicting student performance [25].

4

M. Liu et al.

To improve prediction results, Thai et al. proposed MRMF to explore the multiple relationships between students, exercises, and knowledge concepts by MF techniques [26]. Similarly, CRMF integrated the course relationships to update representations of exercises [15]. Moreover, researchers were inspired by social recommendation systems and used the SocialMF technique to improve the prediction accuracy [27]. Furthermore, Nakagawa et al. proposed GKT that viewed knowledge concepts and their dependencies as nodes and links in a graph, so that students' knowledge states on the answered concepts and their related concepts can be both updated over time [19]. Note that, students and exercises naturally form an interaction graph in ITSs. Considering that GCN can enhance recommendation performance in the user-item bipartite graph, we aim to propose a model that applies GCN onto the student-exercise bipartite graph in ITSs.

3 The Proposed Model
3.1 Problem Formulation
Suppose there are M students, N exercises, and O knowledge concepts. Interactions between students and exercises are represented with matrix R = {rsp}M×N , where rsp represents the performance score that student s has on exercise p. In most cases, the observed part of R consists of 0 and 1, where rsp = 1 if student s's answer to exercise p is correct and rsp = 0 otherwise. As for relations between exercises and knowledge concepts, educational experts manually label each exercise with several knowledge concepts. We use matrix Q = {qpk}N×O to denote the relations, where qpk = 1 if exercise p is related to knowledge concept k and qpk = 0 if there are no relations between them. Given observed interactions R and relations between exercises and knowledge concepts Q, we aim to predict unobserved r^sp, namely student s's score on non-interactive exercise p.

3.2 Overall Structure
The overall structure of Graph-based Exercise- and Knowledge-Aware Learning Network (Graph-EKLN ) is shown in Fig. 2. In the left part of Fig. 2., links between students and exercises are established according to matrix R and links between exercises and knowledge concepts are established according to matrix Q. There two challenges in our task: how to handle with the different links (correct answer/wrong answer) between students and exercises and how to utilize knowledge concept information in MF based models.
To address these two challenges, we divide the task into two sub-tasks, as shown in the middle part of Fig. 2. The first sub-task is to predict student s's proficiency on exercise p itself r^sPp. Note that, P denotes the predicted score r^sPp is in the exercise space. The second sub-task is to predict a student's proficiency on an exercise's related knowledge concepts r^sKp. Note that, K denotes the predicted score r^sKp is in the knowledge space. The following two subsections describe the two sub-tasks respectively.

Graph-based Exercise- and Knowledge-Aware Learning Network

5

 
 



1

2

3

p1

p2

p3

p4



k1 k2
vector dot product vector add

predicting 34
3
4
exercise space predicting 34
3 1 2
knowledge space

layer 1
...
layer 
...
layer L GCN
3
1
2

14

... 13

4

...

4

3

3

 34

 3 4

 3 4

Fig. 2. The overall structure of our proposed Graph-EKLN.

3.3 Modeling High-order Collaborative Information
Student embeddings and exercise embeddings in the exercise space are represented with U = [u1, ..., us, ..., uM ]  RM×D, V = [v1, ..., vp, ..., vN ]  RN×D respectively. D denotes the embedding size and us, vp represent the initial embeddings of student s and exercise p. To address the different-link problem mentioned in subsection 3.2, we follow R-GCN [23], which is proposed to learn representations of multi-relational graph.

1 ... 
s
+1 ... +

...

1





+1

+
...

MLP 1
 MLP 

+1

MLP 0

Fig. 3. The graph convolution layer in our model. Suppose student s has good performance scores on a exercises and bad performance scores on other b exercises. We show the information propagation process of the graph convolution layer on student s.

Fig. 3. provides an overview of the one-layer GCN propagation process in

our model. Specifically, we utilize link-specific aggregation functions based on

multilayer perceptrons (MLPs) for two kinds of links, thus getting high-order

students' and exercises' embeddings. Students' and exercises' initial embedding

can be formulated as

h0s = us, h0p = vp.

(1)

6

M. Liu et al.

Suppose there are L propagation layers. The student s's embedding at the (l+1)th layer can be formulated as:

h(sl+1)

=

n{0,1}

pNsn

1 |Nsn

|

Fn(hlp

)

+

F (hls),

(2)

where Nsn denotes student s's n-th type of neighbors, i.e.,the exercises that student s answered correctly and incorrectly. Eq.(2) uses three types of functions

F, F0, F1 to differentiate the aggregation process of student s's neighbors and

itself with MLPs:

Fn(hl) = 2(1(hlW1))W2,

(3)

where 1, 2 denote activation functions and W1, W2 denote the linear transformation. The exercise p's embedding can be obtained by aggregating embeddings
of its neighbor nodes in the same way. After propagating information, we can obtain [h0s, ..., hls, .., hLs ] and [h0p, ..., hlp, .., hLp ] as students' and exercises' embeddings of each layer. We concatenate embeddings of each layer as follows:

hs = h0s||...||hls||...||hLs , hp = h0p||...||hlp||...||hLp ,

(4)

where || is the concatenation operation. By calculating the inner dot of con-

catenated embeddings, we can get student s's proficiency on exercise p in the

exercise space:

r^sPp = hTs hp.

(5)

3.4 Modeling Information of Knowledge Concepts

As for predicting students' proficiency on knowledge concepts, we project stu-
dents and knowledge concepts to knowledge-space. Note that, X = [x1, ..., xs, ..., xM ]  RM×D and Y = [y1, ..., yk, ..., yO]  RO×D respectively denote the representations of students and knowledge concepts. Then, we use the inner product to predict r^sKp in the knowledge concept space:

r^sKp

=

1 |Kp|

xTs yk,
kKp

(6)

where |Kp| denotes the set of knowledge concepts related to exercise p. It can be formulated as Kp = {k|qpk = 1}, while qpk  Q. Please note that, we do not utilize GCN layer here. In fact, the number of knowledge concepts is much smaller than that of exercises. Thus, relations between students and knowledge concepts are not sparse enough. Therefore, we keep their original embeddings rather than utilizing GCN layers in Eq.(6) to avoid over smoothing.

3.5 Performance Prediction
In subsection 3.3, we model the effect of exercises by R-GCN technique. In subsection 3.4, we model the effect of knowledge concepts by inner product of

Graph-based Exercise- and Knowledge-Aware Learning Network

7

student and knowledge concepts related to the target exercise. After obtaining r^sPp and r^sKp, we can easily calculate student s's final predicted scores on exercise p. Thus the overall predicted function is defined as:

r^sp = r^sPp + r^sKp,

(7)

where  is a hyper-parameter used to control the balance between the two subtasks. We use the point-wise based squared loss to optimize our model:

1 L=
T

(rsp - r^sp)2,

(8)

(s,p,rsp )(S,P,R)

where T denotes the number of (s, p, rsp) triplets in training data.

4 Experiments
4.1 Dataset Description

Table 1. The statistics of the two datasets
Dataset Students Exercises Concepts Logs Density ASISST 4,163 17,746 123 278,868 0.37% KDDcup 574 173,650 437 609,979 0.61%

We choose two widely-used datasets in our experiments. One dataset is ASSIST (ASSISTments 2009-2010 "skill builder")1 provided by the online educational service ASSISTments. The other dataset is Algebra 2005-2006 from the Educational Data Mining Challenge of KDDCup2. The detailed statistics of two datasets are summarized in Table 1. Note that, we only consider exercises that are related to at least one knowledge concept. Specifically, we filter out exercises without related knowledge concepts for the two datasets. Because the number of concepts in KDDcup dataset is too small to make full use of concept information, we combine the knowledge concepts related to the same exercise as a new single concept. Please note that, these two datasets are extremely different. Specifically, in ASISST, each student has nearly 67 logs on average while in KDDcup, each student has nearly 1,063 logs.
4.2 Experimental Settings
Evaluation Metrics. We adopt three widely used metrics (Accuracy, RMSE, and AUC) to measure the error between true ratings and predicted ratings [28,19,17]. Root mean square error (RMSE) is used to measure the absolute difference between predicted labels and real labels [3]. As students' performance scores are binary, we utilize the area under the curve (AUC) [4] as a metric.
1 https://sites.google.com/site/assistmentsdata/home 2 http://pslcdatashop.web.cmu.edu/KDDCup/

8

M. Liu et al.

Baselines. We compare our model with the following methods:
· Student Average: This method calculates students' average scores in training data and uses them as the predicted scores on exercises in testing data.
· MF [16]: It is a classical CF model in recommendations. This model utilizes MF techniques and learns latent representations of students and exercises. Note that knowledge concepts are not used in MF.
· IRT [11]: A classical cognitive diagnosis model that uses one-dimensional continuous variables to represent students' knowledge proficiency and exercises' difficulty, and uses the difference between them for score prediction.
· NeuralCDM [28]: This is an improved multi-dimensional cognitive diagnosis model that utilizes neural networks as the interaction function.
· CRMF [15]: This MF based model takes knowledge concepts into consideration by assuming that representations of exercises with the same knowledge concepts are more similar.
· R-GCN [23]: A substructure of our model that only uses R-GCN to predict scores but neglects students' proficiency on knowledge concepts.
· R-GCN (hetero): In this method, We apply GCN in the whole studentexercise-knowledge heterogeneous graph in Fig. 1.

Parameter Settings. We implement our model in PyTorch-1.6.0. The embedding dimension is set to 128 for our model and other CF models. We initialize all parameters with Xavier initialization [13]. The learning rate is set to 0.001. We set the depth of GCN as two layers. We choose 2-layer MLPs to serve as F in Eq.(3) and LeakyReLU as the activation function. We also set the balancing parameter  = 1.

4.3 Experimental Results
We list the results of our model and other baselines in Table 2. We have several observations from this table.
First, Student Average is the simplest baseline. It assumes that students have the same scores on different exercises, resulting in the worst performance. Second, classical MF based models (MF and CRMF) perform worse than our proposed Graph-EKLN on both two datasets. An obvious reason is that they ignore highorder collaborative information. Simultaneously, CRMF performs better than MF for considering course relations. Third, as for two cognitive diagnosis models (NeuralCDM and IRT), we observe that cognitive diagnosis models perform worse than all MF based model on the ASSIST dataset. MF based models explore similarity among students/exercises, and then provide suggested guidance for students based on the similarities. The reason is that lack of sufficient data brings trouble in cognitive diagnosis models while it has fewer effects on CF based models (MF, CRMF, R-GCN, and Graph-EKLN ). Fourth, R-GCN (hetero) even performs worse than R-GCN on KDDcup. The reason is that data in the heterogeneous graph doesn't obey power law distribution, therefore, common

Graph-based Exercise- and Knowledge-Aware Learning Network

9

graph based methods cannot be directly applied onto ITSs as mentioned in Section 1. Finally, our proposed Graph-EKLN has the best performance on both two datasets. The reason is that Graph-EKLN simultaneously utilizes information of high-order collaborative signals and related knowledge concepts.

Table 2. Overall performance.  /  denotes that the higher/lower, the better.

Model
Student Average MF IRT
NeuralCDM CRMF R-GCN
R-GCN (hetero) Graph-EKLN

ASSIST

Accuracy  RMSE 

0.6942

0.4483

0.7399

0.4205

0.7181

0.4647

0.7249

0.4329

0.7612

0.4134

0.7705

0.3982

0.7748

0.3973

0.7782 0.3938

AUC  0.6816 0.8105 0.7394 0.7561 0.8136 0.8230 0.8232 0.8298

KDDcup

Accuracy  RMSE 

0.7679

0.4190

0.7927

0.3841

0.7762

0.4835

0.8060

0.3713

0.8014

0.3750

0.8205

0.3619

0.8201

0.3642

0.8271 0.3591

AUC  0.5891 0.8062 0.7607 0.8093 0.7968 0.8239 0.8187 0.8291

4.4 Detailed Model Analyses
Ablation Study We perform an ablation study to demonstrate the effectiveness of each component in our model. Specifically, we conduct four experiments to figure out whether graph based techniques (denoted as GCN) and utilizing knowledge concepts (KLG) are effective in ITSs. The basic model is MF, which only utilizes MF techniques. Besides MF techniques, MF-TEM follows subsection 3.4 to use knowledge concepts. MF and R-GCN are recorded in subsection 4.2. As shown in Table 4.4, MF-TEM performs better than MF. It proves that utilizing knowledge concepts is effective. Similarly, R-GCN also performs better than MF, which proves that capturing high-order collaborative signals is helpful for student score prediction. Finally, our proposed Graph-EKLN performs best to prove that simultaneously considering R-GCN and knowledge concepts has the most performance improvements.

Table 3. The ablation study

Model

Components

ASSIST

KDDcup

KLG GCN Accuracy  RMSE  AUC  Accuracy  RMSE  AUC 

MF

××

0.7399 0.4205 0.8105 0.7927 0.3841 0.8062

MF-TEM

×

0.7664 0.3984 0.8288 0.8255 0.3626 0.8246

R-GCN ×

0.7705 0.3982 0.8230 0.8205 0.3619 0.8239

Graph-EKLN

0.7782 0.3938 0.8298 0.8271 0.3591 0.8291

Performance under different balancing parameter  As mentioned in Eq. (7),  is a hyper-parameter that controls balance between two-fold effects.

10

M. Liu et al.

We try the parameter  in the range {0,0.1,1,5,10}. Note that,  = 0 denotes only taking the exercise space into consideration, and Graph-EKLN degenerates to R-GCN. As shown in Fig. 4, when   0, the results become worse; when  becomes higher (e.g.,  = 5, 10), the results also become worse. Finally, GraphEKLN has the best performance on two datasets when  = 1.

Accuracy, AUC RMSE
Accuracy, AUC RMSE

0.83

0.82

0.81

0.80

0.79

0.78

0.77 0

0.1

1

0.398

0.396

Accuracy

AUC RMSE

0.394

5

10 0.392

(a) Performance on ASSIST

0.830 0.825 0.820 0.815 0.810
0 0.1 1

Accuracy 0.372 AUC RMSE
0.368

0.364

0.360

5

10

(b) Performance on KDDcup

Fig. 4. Results of accuracy, AUC, and RMSE with different 

5 Conclusions
In this paper, we proposed a Graph-based Exercise- and Knowledge-Aware Learning Network to improve student performance prediction in ITSs. We borrowed the success of neural graph based models in recommender systems and successfully modeled two-fold effects of exercises and related knowledge concepts. Experimental results on two datasets showed the effectiveness of our model. Note that, we assumed that students' knowledge states are static in this paper. In the future, we are interested in extending our model to a dynamic model which can track the changes in knowledge states.
Acknowledgments. This work is supported in part by by grants from iFLYTEK, P.R. China (Grant No. COGOS-20190002) and the Open Project Program of the National Laboratory of Pattern Recognition (NLPR).

Graph-based Exercise- and Knowledge-Aware Learning Network

11

References

1. An, M., Wu, F., Wu, C., Zhang, K., Liu, Z., Xie, X.: Neural news recommendation with long-and short-term user representations. In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 336­345 (2019)
2. Anderson, A., Huttenlocher, D., Kleinberg, J., Leskovec, J.: Engaging with massive online courses. In: Proceedings of WWW. pp. 687­698 (2014)
3. Barnston, A.G.: Correspondence among the correlation, rmse, and heidke forecast verification measures; refinement of the heidke score. Weather and Forecasting 7(4), 699­709 (1992)
4. Bradley, A.P.: The use of the area under the roc curve in the evaluation of machine learning algorithms. Pattern recognition 30(7), 1145­1159 (1997)
5. Burns, H., Luckhardt, C.A., Parlett, J.W., Redfield, C.L.: Intelligent tutoring systems: Evolutions in design. Psychology Press (2014)
6. Chen, L., Wu, L., Hong, R., Zhang, K., Wang, M.: Revisiting graph based collaborative filtering: A linear residual graph convolutional network approach. In: Proceedings of AAAI. vol. 34, pp. 27­34 (2020)
7. Chen, L., Wu, L., Zhang, K., Hong, R., Wang, M.: Set2setrank: Collaborative set to set ranking for implicit feedback based recommendation. arXiv preprint arXiv:2105.07377 (2021)
8. Cheng, S., Liu, Q., Chen, E., Huang, Z., Huang, Z., Chen, Y., Ma, H., Hu, G.: Dirt: Deep learning enhanced item response theory for cognitive diagnosis. In: Proceedings of CIKM. pp. 2397­2400 (2019)
9. Corbett, A.T., Anderson, J.R.: Knowledge tracing: Modeling the acquisition of procedural knowledge. User modeling and user-adapted interaction 4(4), 253­278 (1994)
10. De La Torre, J.: Dina model and parameter estimation: A didactic. Journal of educational and behavioral statistics 34(1), 115­130 (2009)
11. Embretson, S.E., Reise, S.P.: Item response theory. Psychology Press (2013) 12. Feng, M., Heffernan, N., Koedinger, K.: Addressing the assessment challenge with
an online system that tutors as it assesses. User modeling and user-adapted interaction 19(3), 243­266 (2009) 13. Glorot, X., Bengio, Y.: Understanding the difficulty of training deep feedforward neural networks. In: Proceedings of AISTATS. pp. 249­256 (2010) 14. Huang, Z., Liu, Q., Chen, Y., Wu, L., Xiao, K., Chen, E., Ma, H., Hu, G.: Learning or forgetting? a dynamic approach for tracking the knowledge proficiency of students. ACM Transactions on Information Systems (TOIS) 38(2), 1­33 (2020) 15. Huynh-Ly, T.N., Le, H.T., Nguyen, T.N.: Integrating courses' relationship into predicting student performance. International Journal 9(4) (2020) 16. Koren, Y., Bell, R., Volinsky, C.: Matrix factorization techniques for recommender systems. Computer 42(8), 30­37 (2009) 17. Liu, Q., Huang, Z., Yin, Y., Chen, E., Xiong, H., Su, Y., Hu, G.: Ekt: Exerciseaware knowledge tracing for student performance prediction. IEEE Transactions on Knowledge and Data Engineering 33(1), 100­115 (2019) 18. Liu, Q., Wu, R., Chen, E., Xu, G., Su, Y., Chen, Z., Hu, G.: Fuzzy cognitive diagnosis for modelling examinee performance. ACM Transactions on Intelligent Systems and Technology 9(4), 1­26 (2018) 19. Nakagawa, H., Iwasawa, Y., Matsuo, Y.: Graph-based knowledge tracing: modeling student proficiency using graph neural network. In: IEEE/WIC/ACM International Conference on Web Intelligence. pp. 156­163 (2019)

12

M. Liu et al.

20. Pandey, S., Srivastava, J.: Rkt: Relation-aware self-attention for knowledge tracing. In: Proceedings of CIKM. pp. 1205­1214 (2020)
21. Piech, C., Spencer, J., Huang, J., Ganguli, S., Sahami, M., Guibas, L., SohlDickstein, J.: Deep knowledge tracing. arXiv preprint arXiv:1506.05908 (2015)
22. Romero, C., Ventura, S., Pechenizkiy, M., Baker, R.S.: Handbook of educational data mining. CRC press (2010)
23. Schlichtkrull, M., Kipf, T.N., Bloem, P., Van Den Berg, R., Titov, I., Welling, M.: Modeling relational data with graph convolutional networks. In: ESWC. pp. 593­607 (2018)
24. Shen, S., Liu, Q., Chen, E., Wu, H., Huang, Z., Zhao, W., Su, Y., Ma, H., Wang, S.: Convolutional knowledge tracing: Modeling individualization in student learning process. In: Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. pp. 1857­1860 (2020)
25. Thai-Nghe, N., Drumond, L., Krohn-Grimberghe, A., Schmidt-Thieme, L.: Recommender system for predicting student performance. Procedia Computer Science 1(2), 2811­2819 (2010)
26. Thai-Nghe, N., Schmidt-Thieme, L.: Multi-relational factorization models for student modeling in intelligent tutoring systems. In: IEEE KSE. pp. 61­66 (2015)
27. Thanh-Nhan, H.L., Huy-Thap, L., Thai-Nghe, N.: Toward integrating social networks into intelligent tutoring systems. In: IEEE KSE. pp. 112­117 (2017)
28. Wang, F., Liu, Q., Chen, E., Huang, Z., Chen, Y., Yin, Y., Huang, Z., Wang, S.: Neural cognitive diagnosis for intelligent education systems. In: Proceedings of AAAI. vol. 34, pp. 6153­6161 (2020)
29. Wang, X., He, X., Wang, M., Feng, F., Chua, T.S.: Neural graph collaborative filtering. In: Proceedings of ACM SIGIR. pp. 165­174 (2019)
30. Wu, L., He, X., Wang, X., Zhang, K., Wang, M.: A survey on neural recommendation: From collaborative filtering to content and context enriched recommendation. arXiv preprint arXiv:2104.13030 (2021)
31. Wu, L., Li, J., Sun, P., Hong, R., Ge, Y., Wang, M.: Diffnet++: A neural influence and interest diffusion network for social recommendation. IEEE Transactions on Knowledge and Data Engineering (2020)
32. Wu, L., Sun, P., Fu, Y., Hong, R., Wang, X., Wang, M.: A neural influence diffusion model for social recommendation. In: Proceedings of the 42nd international ACM SIGIR conference on research and development in information retrieval. pp. 235­ 244 (2019)
33. Wu, L., Yang, Y., Chen, L., Lian, D., Hong, R., Wang, M.: Learning to transfer graph embeddings for inductive graph based recommendation. In: Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. pp. 1211­1220 (2020)
34. Wu, L., Yang, Y., Zhang, K., Hong, R., Fu, Y., Wang, M.: Joint item recommendation and attribute inference: An adaptive graph convolutional network approach. In: Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. pp. 679­688 (2020)
35. Zhu, T., Liu, Q., Huang, Z., Chen, E., Lian, D., Su, Y., Hu, G.: Mt-mcd: A multitask cognitive diagnosis framework for student assessment. In: International Conference on Database Systems for Advanced Applications. pp. 318­335. Springer (2018)

