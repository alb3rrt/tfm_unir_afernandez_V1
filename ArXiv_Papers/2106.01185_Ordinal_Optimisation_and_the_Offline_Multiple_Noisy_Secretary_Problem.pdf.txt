ORDINAL OPTIMISATION AND THE OFFLINE MULTIPLE NOISY SECRETARY PROBLEM

arXiv:2106.01185v2 [math.OC] 10 Jun 2021

Robert Chin

Jonathan E. Rowe

Iman Shames

Chris Manzie§

Dragan Nesic´¶

ABSTRACT
We study the success probability for a variant of the secretary problem, with noisy observations and multiple offline selection. Our formulation emulates, and is motivated by, problems involving noisy selection arising in the disciplines of stochastic simulation and simulation-based optimisation. In addition, we employ the philosophy of ordinal optimisation - involving an ordinal selection rule, and a percentile notion of goal softening for the success probability. As a result, it is shown that the success probability only depends on the underlying copula of the problem. Other general properties for the success probability are also presented. Specialising to the case of Gaussian copulas, we also derive an analytic lower bound for the success probability, which may then be inverted to find sufficiently large sample sizes that guarantee a high success probability arbitrarily close to one.
Keywords Ordinal optimization · Secretary problem · Copulas
1 Introduction
Numerous problems studied in operations research are themed around the objective of `picking the best' from a finite number of alternatives, under uncertainty. The branch of ranking and selection problems [1] typically considers finding the population with the best mean from a collected sample. Online (i.e. sequential) variations of ranking and selection have also appeared, e.g. [2], which are also closely related to the pure exploration (best-arm identification) setting in multi-armed bandits [3]. Common motivators for this line of work include stochastic search/simulation [4] and the design of clinical trials [5].
On the other hand, the secretary problem (also known as the "best choice" problem) is an optimal stopping problem that is commonly framed in the context of selecting the best candidate for a job, when the quality of interviewed candidates are observed sequentially in random order [6]. The well-known asymptotically optimal solution for maximising the probability of best selection features the reciprocal of Euler's number (approximately 0.37) as a stopping rule [7, Equation (2a-10)]. Variations of the secretary problem are abundant in the literature, for instance when multiple candidates may be selected [8], or when observations of candidate quality are corrupted by noise [9].
Ordinal optimisation (OO) is another paradigm, originally introduced in [10], that is proposed for softening difficult problems in stochastic search and optimisation, motivated by the simulation-based design of discrete-event dynamical systems. It is offered as a complementary approach to conventional optimisation techniques whenever there is `little hope' of finding the global optimum solution, and its operation rests on two underlying principles: 1) by selecting the subset according to order, the selection is more `robust' to noise; and 2) by goal softening (i.e. increasing the degree of sub-optimality), chances of success can be improved.
Department of Electrical and Electronic Engineering, The University of Melbourne, Australia & School of Computer Science, University of Birmingham, United Kingdom. Email: r.chin4@student.unimelb.edu.au
School of Computer Science, University of Birmingham, United Kingdom & The Alan Turing Institute, United Kingdom College of Engineering & Computer Science, Australian National University, Australia §Department of Electrical and Electronic Engineering, The University of Melbourne, Australia ¶Department of Electrical and Electronic Engineering, The University of Melbourne, Australia

Ordinal Optimisation and the Offline Multiple Noisy Secretary Problem
In the offline (i.e. non-sequential) formulation of OO [11, Chapter II], the technique boils down to "randomly sample many candidate solutions, simulate their performances, and pick the best observed". This is also known as the horse-race selection rule, which is formally proven to be optimal under the setup considered in [12, Theorem 3.1]. The outcome provided by OO is a high probability guarantee that one or more out of a selected subset of candidate solutions is an acceptable sub-optimal solution. It is interesting to note that the same philosphy of "sample many candidate solutions and pick the best observed" also independently manifests itself in a corner of the probabilistically robust control literature [13], justified by rigorous sample complexity analysis. A research area with roots from OO is the optimal computing budget allocation (OCBA) framework, which addresses the problem of efficient sequential allocation of simulation resources [14]. This work on OCBA can also be regarded as an early precursor to results in online ranking and selection, and pure exploration bandits [15, §33.5].
In this paper, we investigate a secretary-like problem that is motivated by simulation-based optimisation. Specifically, we consider contexts where noisy offline simulations are relatively cheap/plentiful compared to `true' evaluations of performance, which are relatively expensive/limited. This may be represented, for example, by the learning of a robotic controller that is primarily outsourced to a computer simulation prior to physical deployment, under the sim-to-real methodology [16]. To this end, we introduce the framework of an offline noisy secretary problem, and regard the probability of success as the probability in which at least one selected candidate performs acceptably well. In addition, we employ in our formulation the two cornerstones championed by OO: an ordinal horse-race selection rule with fixed selection size for multiple secretaries, and goal softening as another degree of freedom to control the probability of success.
We study the properties of the success probability in general settings. In particular, our formulation leads to a success probability which only depends on the bivariate copula of the underlying distributions. Note that while copula modelling has been previously applied in stochastic simulation [17], it was used for a different purpose (namely, to model the time-series of simulation inputs). By keeping to the motivational context that the allowable number of offline evaluations is plentiful, we also address the inversion problem, that is to determine a sufficiently large sample size of candidates, so that for fixed selection size, a prescribed high probability of success is guaranteed.
The paper is structured as follows. In Section 2, we setup the technical premise of the paper, and define the success probability for the offline multiple noisy secretary problem. The inversion problem is also to be stated formally. In Section 3, we provide an explicit integral expression for the success probability that only depends on the underlying copula, and use this to illustrate various properties of the success probability for general copula models. In Section 4, the success probability is specialised to the case of Gaussian copula models, in which we provide an analytic lower bound for the success probability. This analytic lower bound is then used to address the inversion problem, where we demonstrate finding sufficiently large sample sizes to guarantee a high probability of success. Concluding remarks are given and future work is proposed in Section 5.

2 Preliminaries

2.1 Notation

The set R denotes the real numbers, and N denotes the set of natural numbers. The logarithm log (·) is taken to

mean the natural logarithm, while cot (·) and sinh (·) are the cotangent and hyperbolic sine functions respecrively.

The floor and ceiling operators are given by · and · respectively. Whenever the symbols  and < are used

between vectors, they refer to element-wise inequalities. The standard Gaussian probability density function (PDF),

cumulative distribution function (CDF), inverse CDF (i.e. quantile function) and Q-function (complementary CDF) are canonically represented using  (·),  (·), -1 (·), Q (·) respectively. We write X  N (µ, C) to denote that

X is Gaussian distributed with mean µ and covariance C. The probability of an event is measured by Pr (·) with

respect to a probability space that is clear from context. The abbreviation i.i.d. stands for mutually independent and

identically distributed. The symbol E denotes mathematical expectation. We use I{·} to denote an indicator variable.

The notation [X|Y = y] is understood to mean a random variable that is equal in law to the conditional distribution

of X given Y = y. Following the notation of [18], stochastic dominance is denoted by and the symbol = between

st

st

random elements denotes equality in law. The kth order statistic of a i.i.d. sample of size n from parent distribution Z

will be denoted by Zk:n.

2

Ordinal Optimisation and the Offline Multiple Noisy Secretary Problem

2.2 Setup

Let the pair (Z, X) have a continuous distribution with joint CDF FZ,X (z, x) and respective marginal CDFs FZ (z), FX (x). Consider n i.i.d. copies (Zi, Xi) drawn from the distribution of (Z, X). The variables Z1, . . . , Zn (i.e. noisily observed candidate qualities) are ordered from best to worst, denoted by Z1:n  · · ·  Zn:n. The best m (with m  n) are selected, given by Z1:n, . . . , Zm:n, with respectively attached X-values (i.e. actual candidate qualities) denoted as X 1 , . . . , X m . More explicitly, the pairs Z1:n, X 1 , . . . , Zm:n, X m have been chosen. For this offline multiple noisy secretary problem, we define the corresponding success probability as follows.
Definition 2.1 (Success probability). The success probability psuccess is the probability that at least one of X 1 , . . . , X m is in the best 100 percentile of the population of X, and denoted

pFsuZc,cXess (n, m, ) := Pr

m
X i  x
i=1

(2.1)

= Pr

min X
i{1,...,m}

i

 x

,

(2.2)

where x with   (0, 1] is such that Pr (X  x) = .

This is an offline variant of the secretary problem, because the selection is made after all the Z1, . . . , Zn are observed.

Also, the classical secretary problem is trivial if selections can be made offline, so one purpose of having noisy

observations is to `un-trivialise' the problem. Painting this setup in a simulation-based optimisation context, we could

for example take each Zi

as

the empirical average Zi

=

1 T

T t=1

J

(i,

Wt)

over

T

simulation

replications,

where

J (i, Wt) is a system performance function of a randomly generated design parameter i, and the Wt are stochastic

simulation input variables. Naturally, Zi is then a noisy estimate of the actual expectation Xi = EW [J (i, W )|i]. In

essence, simulating a candidate solution is akin to conducting an `interview' for it. Hence Definition 2.1 is compatible,

provided the random variables are continuous.

A noisy secretary problem was also studied by [9], but instead considered the probability that the best out of
X1, . . . , Xn was selected. It was demonstrated that, even in the offline setting, there exist distributions whereby this probability tends to zero as n  . In this paper, the role of  is for goal-softening, i.e. controlling
degree of sub-optimality as defined by a threshold of the best 100 percentile of X. The results for our formulation imply that  can be controlled to make psuccess arbitrarily close to one, while psuccess > 0 always when   (0, 1].

We also focus attention on the problem of inverting the success probability, i.e. finding a sample size n so that psuccess is sufficiently high. Problem 2.1 (High probability inversion). Prescribe   (0, 1], and fix m  N,   (0, 1]. Given the distribution for (Z, X):
(a) does there exist a finite n which yields pFsuZc,cXess (n, m, )  1 - ?
(b) If so, find such an n.

In the context of simulation-based optimisation, n represents the quantity of simulations performed offline. Thus Problem 2.1 could be phrased as "find the number of offline simulations, so that at least one candidate actually performs acceptably well, with high probability".

2.3 Dependence Modelling

Since Z is a noisily corrupted version of X, a reasonable condition that might be desired on (Z, X) is some notion of positive dependence, i.e. roughly speaking, low (high) Z is predictive of low (high) X. One such formal notion of positive dependence is called stochastically increasing positive dependence (SIPD), defined as follows [19, §2.8.2].

Definition 2.2 (Stochastically increasing positive dependence). The random variable X is said to be stochastically increasing positive dependent in Z if

Pr (X > x|Z = z)  Pr (X > x|Z = z)

(2.3)

for all z, z in the support of Z, such that z  z.

3

Ordinal Optimisation and the Offline Multiple Noisy Secretary Problem

The SIPD condition may be satisfied, for instance, under a particular additive noise causal representation. Additive noise is the same type of noise considered in [9] and areas of stochastic simulation such as [10, 20]. Further elaboration is provided in Appendix A.

Due to their utility in modelling dependence structure and flexibility to describe wide classes of distributions, copulas will be convenient for modelling the pair (Z, X).
Definition 2.3 (Bivariate copula). A bivariate copula is a bivariate distribution, with both marginal distributions being the Uniform (0, 1) distribution.

Through the probability integral transform, which says that both FZ (Z) and FX (X) are Uniform (0, 1) distributed [21, Theorem 1], every multivariate distribution can be represented with just its marginal distributions and a copula.
Moreover, Sklar's theorem [19, Theorem 1.1] asserts that if the distribution is continuous, then the choice of copula in this representation is unique. If the continuous pair (Z, X) is represented with copula CZ,X , then we say that (Z, X) `has' copula CZ,X and write the joint copula CDF as

CZ,X (z, x) := Pr (FZ (Z)  z, FX (X)  x) .

(2.4)

In addition, the conditional copula CDF of X given Z is denoted by

CX|Z (x|z) := Pr (FX (X)  x|FZ (Z) = z) .

(2.5)

3 Properties of Success Probability

In this section, we present several properties of the success probability when (Z, X) has a general copula model. Our first result gives an expression to compute the success probability by evaluating an m-dimensional integral.

Theorem 3.1 (Integral expression for success probability). We have





pFsuZc,cXess (n, m, )

=

n! (n - m)!

1 0

zm
···
0

z2

m

1 -

1 - CX|Z (|zj ) 

0

j=1

× (1 - zm)n-m dz1 . . . dzm-1dzm. (3.1)

The proof may be found in Appendix C. Theorem 3.1 reveals that when calculating the success probability given n, m and , only the copula of (Z, X) matters. Or alternatively, we can without loss of generality assume that (Z, X) is a copula distribution, and write pFsuZc,cXess = pCsuZc,cXess. Therefore, we see why the "ordinal" term from OO can be a fitting
qualifier - the success probability will be invariant to univariate monotonic (i.e. strictly increasing) transformations of
Z and X, because this does not modify the underlying copula.

The m-dimensional integral in (3.1) is analytically intractable, except for simple special cases exemplified as follows.

For the bivariate Clayton copula [19, §4.6.1] with parameter  = 1 (denoted Clay ()), and n = 3, m = 1,  = 1/2,

we have

pCsulcacye(s1s)

3,

1,

1 2

=3

1 0

(1 (z

- +

z)2 1)2

dz

=

9

-

12

log

2



0.6822.

(3.2)

Or using Clay (2) instead, we have

pCsulcacye(s2s)

3,

1,

1 2

=3

1 0

(1 (3z2

- +

z)2 1)3/2

dz

=

sinh-1

 3

3

 0.7603.

(3.3)

Note that it is also possible to consider an alternative selection rule - which is to instead select all the candidates below the threshold zm /n, where zm /n is the 100m/n percentile of Z. This selection rule gives a randomised selection size equal to m in expectation, and it can be shown that the analogous success probability is the somewhat more `elegant-looking' expression of 1 - (1 - CZ,X (m/n, ))n. In comparison to a fixed selection size however, this randomised selection size suffers from three issues: 1) if the marginal distribution of Z is not known in practice,
then this selection rule is non-operational; 2) a randomised selection size yields a strictly smaller asymptotic success
probability; and 3) in applications (e.g. simulation-based optimisation), it may be undesirable to have the selection

4

Ordinal Optimisation and the Offline Multiple Noisy Secretary Problem

size be randomised, especially as the selection size is able to be zero with non-zero probability, and has an asymptotic variance of m. Further details are provided in Appendix B.

Under a SIPD regularity condition, we can also confirm that the horse-race selection rule (i.e. selecting the best m observed) is indeed the optimal choice.
Proposition 3.1 (Optimality of horse-race selection). If X is SIPD in Z, then the selection of the first m order statistics maximises the success probability in Definition 2.1, compared to any other selection of size m from the sample.

The success probability can also be shown to be non-decreasing in each of its arguments.
Proposition 3.2 (Monotonicity of success probability). The success probability pCsuZc,cXess satisfies the following monotonicity properties.

(a) (Monotonicity in n) If X is SIPD in Z, then for any n  N and fixed m¯  {1, . . . , n} and fixed ¯  (0, 1], we

have

pCsuZc,cXess (n, m¯ , ¯)  psCuZc,cXess (n + 1, m¯ , ¯) .

(3.4)

(b) (Monotonicity in m) For fixed n¯  N, fixed ¯  (0, 1] and any m  {1, . . . , n - 1}, we have pCsuZc,cXess (n¯, m, ¯)  pCsuZc,cXess (n¯, m + 1, ¯) .

(3.5)

(c) (Monotonicity in ) For fixed n¯  N, fixed m¯  {1, . . . , n¯} and any ,   (0, 1] such that   , we have

pCsuZc,cXess (n¯, m¯ , )  pCsuZc,cXess (n¯, m¯ , ) .

(3.6)

For any copula satisfying the SIPD condition, we also have the following general upper and lower bounds.

Proposition 3.3 (General bounds for success probability). If X is SIPD in Z, the success probability pCsuZc,cXess satisfies

1 - (1 - )m  pCsuZc,cXess (n, m, )  1 - (1 - )n .

(3.7)

These bounds are tight, as some of the following limiting cases show.
Proposition 3.4 (Limiting forms of success probability). If X is SIPD in Z, the success probability pCsuZc,cXess satisfies the following.

(a) When m = n:

pCsuZc,cXess (n, n, ) = 1 - (1 - )n .

(3.8)

(b) When  = 1:

pCsuZc,cXess (n, m, 1) = 1.

(3.9)

(c) When   0+:

lim
0+

pCsuZc,cXess

(n,

m,

)

=

0.

(3.10)

(d) If Z and X are comonotonic (i.e. perfect positive dependence, such that FZ (Z) = FX (X) for every

realisation of (Z, X)), then

pCsuZc,cXess (n, m, ) = 1 - (1 - )n .

(3.11)

(e) If Z and X are independent, then pCsuZc,cXess (n, m, ) = 1 - (1 - )m .

(3.12)

5

Ordinal Optimisation and the Offline Multiple Noisy Secretary Problem

(f) When m   and n   in any way such that m  n:

lim
m,n

pCsuZc,cXess

(n,

m,

)

=

1.

(g) When m is finite and n  :

lim
n

pCsuZc,cXess

(n,

m,

)

=

1

-

1 - CX|Z (|0) m .

(3.13) (3.14)

Due to the specification of Problem 2.1, we are primarily interested in the regime of large n and relatively small fixed m.

Sadly, the property in Proposition 3.4(g) indicates that in general, for finite m we have limn pCsuZc,cXess (n, m, ) < 1,

whenever CX|Z (|0) < 1. For example, the bivariate Frank copula [19, §4.5.1] with parameter  > 0 has boundary

conditional CDF

CXFr|aZnk (|0; ) =

1 - e- 1 - e-

< 1.

(3.15)

Thus to address Problem 2.1(a) in general, we can only attain a high success probability with a combination of suffi-
ciently large n and m. However, there do exist classes of copulas where limn psuccess (n, m, ) = 1 for all finite m  1. In these cases, it is possible to address Problem 2.1(b), i.e. for a prescribed high probability 1 -  (with any   (0, 1] and m,  fixed), to find an n such that pCsuZc,cXess (n, m, )  1 - . A naive approach would be to increment n and numerically evaluate pCsuZc,cXess using the m-dimensional integral (3.1) until such an n is found. In the
next section, we study one such class of copula which satisfies this property, and explore an alternative approach for finding n using an analytic lower bound for psuccess.

4 Gaussian Copula Success Probability

The naive approach to address Problem 2.1(b) is by numerically evaluating the m-dimensional integral iteratively (discussed at the end of Section 3). However, several numerical issues pervade this approach.

· Firstly, m-dimensional integrals will become hard to compute (even numerically) for sufficiently large m.
· Secondly, even by taking m = 1 (which lower bounds the success probability for m  1 due to Proposition 3.2(b)), we do not know the vicinity of the order of magnitude for n to begin evaluating pCsuZc,cXess. For example, we could begin evaluating pCsuZc,cXess by incrementing n starting from n = 100, but it would actually require n  109 before pCsuZc,cXess  1 - , resulting in an excessive number of evaluations.
· Lastly, even if we did know (or can narrow down) the vicinity of the order of magnitude for n, it may not be possible to obtain an accurate numerical evaluation of pCsuZc,cXess for very large n, since the factor (1 - zm)n-m n!/ (n - m)! related to the probability density in (3.1) tends towards a degenerate distribution concentrated over zero as n  , for fixed m.

In this section, we address the above issues by specialising the success probability to the case when (Z, X) has a bivariate Gaussian copula. This allows us to develop an analytic lower bound for the success probability, which may be inverted to directly address Problem 2.1(b). The bivariate Gaussian copula is defined as follows.

Definition 4.1 (Bivariate Gaussian copula). Let (Z, X) be a bivariate standard Gaussian vector with correlation   [-1, 1], i.e.

Z X

N

0 0

,

1 

 1

.

(4.1)

Then the bivariate Gaussian copula with correlation , denoted N C (), is the distribution of ( (Z) ,  (X)).

The class of multivariate distributions with Gaussian copulas are referred to as non-paranormal distributions [22], and
alternatively as meta-Gaussian distributions [23]. A bivariate Gaussian copula is entirely specified by the correlation parameter , which also neatly summarises the dependence within the distribution. Thus, we denote the success probability with a Gaussian copula as pNsucCc(es)s (n, m, ). The bivariate Gaussian copula is known to satisfy the stronger condition of positive likelihood ratio dependence for  > 0 [19, §4.3.1], which implies SIPD of X in Z [24, Theorem

6

Ordinal Optimisation and the Offline Multiple Noisy Secretary Problem

5.2.19]. Thus all the properties in Section 3 hold for pNsucCc(es)s. It is also known that the boundary CDF for the bivariate Gausssian is CXN|Z (·|0; ) = 1 for all  > 0 [19, §4.3.1], so by Proposition 3.4(g), we immediately have

lim
n

pNsucCc(es)s

(n,

m,

)

=

1

(4.2)

when  > 0, for any m  1. The Gaussian copula model for (Z, X) also admits an additive independent Gaussian noise representation (see Appendix A).

4.1 Analytic Lower Bound for Success Probability

Under a Gaussian copula model for (Z, X) with positive correlation  > 0, we can derive the following analytic lower bound on the success probability.

Theorem 4.1 (Success probability analytic lower bound). Given some  

0,

 2

, let

c1

=

1 2

-

 

,

c2

=

cot   - 2

(4.3)

and

µn = - log (nc1) c2

n2

=

2c2

- log log 2 (log (nc1) - log

log

2)

.

(4.4) (4.5)

Then for any  

0,

 2

, there exists an n ()

 N such that for all n



n (), m

 {1, . . . , n}, 

 (0, 1], and

  (0, 1], we have

pNsucCc(es)s (n, m, )  pNsucCc(es)s (n, 1, )  

-1 () - µn . 1 - 2 + 2n2

(4.6)

The proof of this result is found in Appendix C. Furthermore, given any n  N, m  {1, . . . , n},   (0, 1], and

  (0, 1], we can optimise the bound with respect to  by





pNsucCc(es)s (n, m, )  psNucCc(es)s (n, 1, )  sup  
n

-1 () - µn ()  , 1 - 2 + 2 [n ()]2

(4.7)

where n 

0,

 2

is the set of all  such that n  n (), while µn (), [n ()]2 are (4.4), (4.5) respectively but

with dependence on  explicitly denoted. For a given , it is also worthwhile to consider the smallest integer n ()

such that (4.6) is valid. It is clear that we must have n () > 1/c1, otherwise it possibly allows for log (nc1) < 0

in (4.4) and (4.5). Given n and , one can numerically certify whether n  n (), using sufficient conditions from

the proof of Theorem 4.1. We have empirically observed that n () can be quite small; we are usually able to accept

n () = 1/c1. Using this certification, the optimised lower bound (4.7) can also be implemented via a numerical

procedure, noting that we need only conduct search over a univariate bounded interval. Further discussion and

pseudocode for these implementations can be found in Appendix D.

Throughout Figures 1a-1d, we plot the optimised lower bound (4.7) from Theorem 4.1, with baseline values m = 1,  = 0.05,  = 0.4 and n = 100, while varying s single quantity. In Figure 1a, this is compared against a numerical
evaluation of the success probability using (3.1). However, since the density in the integrand concentrates over zero as n increases, numerical integration becomes inaccurate for large n (as discussed at the beginning of this section). In Figure 1b, we instead plot the lower bound over a semi-log horizontal axis scale for large n, to illustrate the convergence of the success probability to one. These plots demonstrate that the behaviour of the lower bound is reasonably close to the actual probability. As the lower bound has been derived with m = 1 while the bound itself does not change with m, this means the bound is least conservative for m = 1, and will generally become more conservative as m grows. For instance with  = 0.05 and m = 32, the lower bound from Proposition 3.3 yields pNsuccess  0.806 for any n  32, already surpassing the lower bound with n = 109 from Figure 1a. However, the bound (4.6) is still useful in the regime of small m and large n, e.g. for addressing Problem 2.1(b).

7

Ordinal Optimisation and the Offline Multiple Noisy Secretary Problem

1 0.8 0.6 0.4 0.2
0 10 0

Numerical integration Optimised lower bound

10 5

10 10

10 15

1 0.99 0.98 0.97 0.96 0.95
10 15

Optimised lower bound

10 20

10 25

10 30

10 35

(a) Comparison of the optimised lower bound in (4.7) to numer- (b) Comparison of the lower bound (4.7) when n is varied over ical integration via Theorem 3.1, as n is varied. In addition, this a semi-log horizontal axis scale. In addition, this figure demonfigure demonstrates monotonicity in n from Proposition 3.2(a). strates convergence to one in n from (4.2).

1

1

Numerical integration

0.8

Optimised lower bound 0.8

0.6

0.6

0.4

0.4

0.2

0.2

Numerical integration

Optimised lower bound

0

0

0

0.2

0.4

0.6

0.8

1

0

0.2

0.4

0.6

0.8

1

(c) Comparison of the optimised lower bound in (4.7) to numer- (d) Comparison of the optimised lower bound in (4.7) to numer-

ical integration via Theorem 3.1, as  is varied.

ical integration via Theorem 3.1, as  is varied. In addition, this

figure demonstrates monotonicity in  from Proposition 3.2(c).

Figure 1: Numerical results for the lower bound.

4.2 Inversion of Analytic Lower Bound
We now use the analytic lower bound in (4.6) of Theorem 4.1 to address Problem 2.1(b), in the case that (Z, X) has a Gaussian copula. Putting this into the contexts discussed earlier, we may address questions of the nature "how many job candidates should the hiring manager interview", or "how many simulations should be conducted offline" in order to guarantee a prescribed high success probability. Observe that the analytic lower bound in (4.6) tends to one, as n  . So to address Problem 2.1(b) using the lower bound (given ,  and ), we aim to invert for n in terms of  with the expression

 -1 () - µn = 1 - . 1 - 2 + 2n2

(4.8)

Putting the definitions of µn and n from (4.4) and (4.5) respectively, this equation can be rearranged into a quartic equation in x = log (nc1), of the form

a4x4 + a3x3 + a2x2 + a1x + a0 = 0,

(4.9)

8

Ordinal Optimisation and the Offline Multiple Noisy Secretary Problem

where

a4

=

-

22 log log

2

a3

=

- 4-1 () c2 log log 2

a2 = 22 - 2c2 -1 () 2 - a1 = 4c2-1 () 

-1 (1 - ) 2 + 2 log log 2

-1 (1 - ) 2

a0 = 2c2 -1 () 2 - -1 (1 - ) 2 + 2 -1 (1 - ) 2 - 2 -1 (1 - ) 2 .

(4.10) (4.11)
(4.12) (4.13) (4.14)

Therefore we take the solution for n corresponding to x = log (nc1) as the greatest real root of the quartic equation. Let this solution for n in terms of  be denoted n (). According to the monotonicity and convergence properties from Proposition 3.2(a) and (4.2) respectively, then provided n ()  n (), we guarantee

pNsucCc(es)s (n () , m, )  1 - ,

(4.15)

since n () upper bounds the smallest n needed such that psuccess  1 - . Moreover, in a similar way to (4.7), one can numerically optimise with respect to  to find the smallest n () that guarantees a high probability of success. Pseudocode implementing this (which also takes into account the requirement n ()  n ()) can be found in

Algorithm 3 of Appendix D.

Table 1: Computed values of n which guarantees pNsucCc(es)s (n () , m, )  1 - , with fixed  = 0.01 and valid for any m  1.

 = 0.01

 = 0.05

 = 0.1

 = 0.01 8.144 × 1047007 5.427 × 1034246 8.943 × 1028267

 = 0.3

3.289 × 1051

1.619 × 1038

8.775 × 1031

 = 0.6 8.703 × 1011

1.988 × 109

1.078 × 108

 = 0.9

16744

4338

2188

 = 0.99

893

505

372

Table 1 lists computed values of n numerically optimised with respect to , using the aforementioned approach. The table is valid for all m  1 (however are values are least conservative when m = 1), for fixed  = 0.01 and a variety of values for  and . The values for n trend downwards as  increases, which is intuitive (as fewer samples might
be required if noisy observations are strongly correlated with the actual values). Of particular note, the case with extremely small correlation  = 0.01 requires n to be at an impractical order of magnitude, namely 1047007 when  = 0.01. This highlights the utility of the analytic lower bound in Theorem 4.1, which allows for a computationally cheap procedure to find a sufficiently high n. If Problem 2.1(b) were attempted to be solved by evaluating expression (3.1), then large n such as in the order of 1047007 would have rendered the evaluation of such probabilities to be
intractable.

Also, Table 1 illustrates the value of having a strong positive dependence in , since it reduces the sample size required to reach a prescribed high probability of success. For instance with  = 0.1, increasing from  = 0.01 to  = 0.6 reduces the order of magnitude required for n from 1028267 to a more practical 108. As one may have reasonably guessed, increasing the strength of correlation between Z and X (which is in effect, decreasing the amount of noise in
the interview/simulation of candidates) has a favourable effect on the success probability.

5 Conclusion
Motivated by methods in simulation-based optimisation, we studied the success probability for an offline multiple noisy secretary problem. As a consequence of applying an ordinal optimisation selection rule with a notion of goal softening, the success probability was found to depend only on the underlying copula. One key condition for the copula was that of stochastically increasing positive dependence, which is sufficient to ensure that psuccess is non-decreasing in n. An analytic lower bound for psuccess was developed in the case of the Gaussian copula model, and Figures 1a-1d illustrate that this lower bound is close to the true success probability. The lower bound may also

9

Ordinal Optimisation and the Offline Multiple Noisy Secretary Problem
be inverted to compute sufficiently large values of n which guarantees success probabilities arbitrarily close to one. This is demonstrated in numerical examples by successfully finding such values of n, even in regimes which require extremely high orders of magnitude for n, whereby numerical integration would not be successful.
The following directions are proposed for future work. In order to address Problem 2.1(b) for a non-Gaussian copula, invertible analytic lower bounds for other classes of copulae could be investigated. It may also be interesting to study how strength of dependence affects psuccess for wider classes of copulas (analogous to the role of  in the Gaussian copula), and how the success probability could be estimated, whenever the copula is not specified exactly in practice. Another direction would be to consider a relaxed problem where (Z, X) is allowed to have a discrete distribution. The present approach may then need to be modified, since the underlying copula for (Z, X) would no longer be unique.
References
[1] E. J. Dudewicz, "Ranking (ordering) and selection: An overview of how to select the best," Technometrics, vol. 22, no. 1, p. 113, 1980.
[2] Y. Peng, E. K. P. Chong, C.-H. Chen, and M. C. Fu, "Ranking and selection as stochastic control," IEEE Transactions on Automatic Control, vol. 63, no. 8, pp. 2359­2373, 2018.
[3] S. Bubeck, R. Munos, and G. Stoltz, "Pure exploration in multi-armed bandits problems," in International Conference on Algorithmic Learning Theory, pp. 23­37, Springer Berlin Heidelberg, 2009.
[4] M. C. Fu, ed., Handbook of Simulation Optimization. Springer, 2014. [5] C. Shen, Z. Wang, S. Villar, and M. V. D. Schaar, "Learning for dose allocation in adaptive clinical trials with
safety constraints," in International Conference on Machine Learning, 2020. [6] F. T. Bruss, "A unified approach to a class of best choice problems with an unknown number of options," The
Annals of Probability, vol. 12, no. 3, 1984. [7] J. P. Gilbert and F. Mosteller, "Recognizing the maximum of a sequence," Journal of the American Statistical
Association, vol. 61, no. 313, pp. 35­73, 1966. [8] J. Preater, "On multiple choice secretary problems," Mathematics of Operations Research, vol. 19, no. 3, pp. 597­
602, 1994. [9] A. M. Krieger and E. Samuel-Cahn, "The noisy secretary problem and some results on extreme concomitant
variables," Journal of Applied Probability, vol. 49, no. 3, pp. 821­837, 2012. [10] Y. C. Ho, R. S. Sreenivas, and P. Vakili, "Ordinal optimization of DEDS," Discrete Event Dynamic Systems,
vol. 2, no. 1, pp. 61­88, 1992. [11] Y.-C. Ho, Q.-C. Zhao, and Q.-S. Jia, Ordinal Optimization: Soft Optimization for Hard Problems. Springer,
2007. [12] M. Yang and L. Lee, "Ordinal optimization with subset selection rule," Journal of Optimization Theory and
Applications, vol. 113, no. 3, pp. 597­620, 2002. [13] M. Vidyasagar, "Randomized algorithms for robust controller synthesis using statistical learning theory," Auto-
matica, vol. 37, no. 10, pp. 1515­1528, 2001. [14] H.-C. Chen, C.-H. Chen, and E. Yucesan, "Computing efforts allocation for ordinal optimization and discrete
event simulation," IEEE Transactions on Automatic Control, vol. 45, no. 5, pp. 960­964, 2000. [15] T. Lattimore and C. Szepesvári, Bandit Algorithms. Cambridge University Press, 2020. [16] X. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel, "Sim-to-real transfer of robotic control with dynam-
ics randomization," in IEEE International Conference on Robotics and Automation (ICRA), IEEE, 2018. [17] B. Biller, "Copula-based multivariate input models for stochastic simulation," Operations Research, vol. 57,
no. 4, pp. 878­892, 2009. [18] M. Shaked and J. G. Shanthikumar, Stochastic Orders. Springer, 2007. [19] H. Joe, Dependence Modeling with Copulas. CRC Press, 2014. [20] W. Xie, B. L. Nelson, and R. R. Barton, "A bayesian framework for quantifying uncertainty in stochastic simula-
tion," Operations Research, vol. 62, no. 6, pp. 1439­1452, 2014. [21] J. E. Angus, "The probability integral transform and related results," SIAM Review, vol. 36, no. 4, pp. 652­654,
1994.
10

Ordinal Optimisation and the Offline Multiple Noisy Secretary Problem

[22] H. Liu, F. Han, M. Yuan, J. Lafferty, and L. Wasserman, "High-dimensional semiparametric gaussian copula graphical models," The Annals of Statistics, vol. 40, no. 4, pp. 2293­2326, 2012.
[23] B. Storvik, G. Storvik, and R. Fjortoft, "On the combination of multisensor data using meta-gaussian distributions," IEEE Transactions on Geoscience and Remote Sensing, vol. 47, no. 7, pp. 2372­2379, 2009.
[24] R. B. Nelsen, An Introduction to Copulas. Springer, 1st ed., 1999.
[25] S. L. Morgan and C. Winship, Counterfactuals and Causal Inference: Methods and Principles for Social Research. Cambridge University Press, 2nd ed., 2015.
[26] M. Harder, Exchangeability of Copulas. PhD thesis, Ulm University, 2016.
[27] H. A. David and H. N. Nagaraja, Order Statistics. John Wiley, 2005.
[28] M.-W. Wu, Y. Li, M. Gurusamy, and P.-Y. Kam, "A tight lower bound on the gaussian Q-function with a simple inversion algorithm, and an application to coherent optical communications," IEEE Communications Letters, vol. 22, no. 7, pp. 1358­1361, 2018.
[29] M. Chiani, D. Dardari, and M. Simon, "New exponential bounds and approximations for the computation of error probability in fading channels," IEEE Transactions on Wireless Communications, vol. 24, no. 5, pp. 840­ 845, 2003.
[30] B. C. Arnold, N. Balakrishnan, and H. N. Nagaraja, A First Course in Order Statistics. SIAM, 2008.
[31] C. E. Rasmussen and C. K. I. Williams, Gaussian Processes for Machine Learning. MIT Press, 2006.
[32] V. Krishnamurthy, Partially Observed Markov Decision Processes: From Filtering to Controlled Sensing. Cambridge University Press, 2016.

Appendices

A Additive Noise Representations

Consider the following causal mechanism (depicted in Figure 2a) for generating (Z, X): by first generating Z, and

then generating X given Z = z though

X = z + Y,

(A.1)

where Y is independent of Z. An alternative additive causal representation is the reverse (depicted in Figure 2b): first X is generated, and then Z is generated given X = x through

Z = x + Y,

(A.2)

where Y is independent of X.

Z

Y

X

Y

X

Z

(a) Causal additive noise representation for X.

(b) Causal additive noise representation for Z.

Figure 2: Causal graphs [25, §3.2.2] for additive noise representations.

Proposition A.1 (Additive noise implies SIPD). If either of the following conditions hold:
· (Z, X) is generated via the causal representation (A.1), or
· (Z, X) is generated via the causal representation (A.2), and moreover the copula of (Z, X) is exchangeable (i.e. its distribution function CZ,X (z, x) is permutation symmetric),

11

Ordinal Optimisation and the Offline Multiple Noisy Secretary Problem

then X is SIPD in Z.

Proof. For the first causal additive representation, as Z and Y are independent, we have equality in law

Thus

[X|Z = z] = z + Y.
st

(A.3)

Pr (X > x|Z = z) = Pr (Z + Y > x|Z = z)

(A.4)

= Pr (Y > x - z|Z = z)

(A.5)

 Pr (Y > x - z|Z = z)

(A.6)

= Pr (X > x|Z = z) .

(A.7)

for all z  z in the support of Z. As for the second causal additive representation, assume without loss of generality

that (Z, X) is a copula, and denote CZ,X (z, x) = Pr (Z  z, X  x). By analogous arguments to the first causal

additive representation, we have

Pr (Z > z|X = x)  Pr (Z > z|X = x)

(A.8)

for all z, x, x  (0, 1) such that x  x. Due to exchangeability, CZ,X (z, x) = CZ,X (x, z) and we have the copula conditional CDF (computed via [19, §2.1.3]) as

Pr (Z

>

z|X

=

x)

=

CZ,X (z, x) x

=

CZ,X (x, z) x

= Pr (X > z|Z = x) .

(A.9)
(A.10) (A.11)

Performing the same for the right-hand side of (A.8), we get Pr (X > z|Z = x)  Pr (X > z|Z = x)
for all z, x, x  (0, 1) such that x  x, which is the required condition for (2.3).

(A.12)

Note that many classes of parametric copulas (with one or two parameters) are exchangeable [19, §2.15], as are all members of the Archimedean family of copulas [26], so the SIPD condition is satisfied for a variety of models with additive noise.

A.1 Additive Noise Representation for Gaussian Copulas

Suppose observations Z are generated by additive Gaussian noise to Gaussian X in the way of (A.2). Specifically, let

Z = X + Y,

(A.13)

where X  N (0, 1) and Y  N 0, 2 independent of X. The variable 2 may be interpreted here as the noise-to-

signal ratio. Note that (Z, X) is then bivariate Gaussian with correlation  = 1 + 2 -1/2 and naturally, a bivariate

Gaussian has a bivariate Gaussian copula. Hence this representation corresponds with the Gaussian copula success

N C 1/ 1+2

probability psuccess

(n, m, ), in terms of the noise-to-signal ratio.

B Randomised Selection Size

Consider the alternative selection rule: to select all the candidates below the threshold zm /n, where zm /n is the 100m/n percentile of Z. This selection rule gives a randomised selection size that is binomial distributed with parameters n and m/n, and hence has an expectation of m, and asymptotic variance of limn n (m/n) (1 - m/n) = m. The success probability with this alternative selection rule, defined in the analogous way to (2.1), is equal to 1 - (1 - CZ,X (m/n, ))n, which is explained as follows. Note that there will be a success if at least one of the candidates satisfies both Zi  zm /n and Xi  x; the former is the condition that it is selected the first place, and the latter is the condition it actually performs acceptably well. Thus, the failure probability is the probability that none of the samples satisfies both Zi  zm /n and Xi  x. Since

c := CZ,X (m/n, ) = Pr X  x, Z  zm /n ,

(B.1)

12

Ordinal Optimisation and the Offline Multiple Noisy Secretary Problem

then the failure probability is (1 - c)n, and so the success probability is 1 - (1 - c)n. We compute the limit of this success probability as n   for fixed m. Assume without loss of generality that (Z, X) is a copula. Then we have

lim
n

[1

-

(1

-

c)n]

= =

lim 1 - 1 - Pr

n



Pr

1 - lim 1 -

n

X  x Z  zm /n
X  x Z  zm /n n

Pr m

Z n


zm /n

n

(B.2) (B.3)

= 1 - exp

-m lim Pr
n

X  x Z  zm /n

= 1 - exp -mCX|Z (|0) ,

(B.4) (B.5)

which depends on the copula conditional boundary CDF CX|Z (|0)  1, thus

1 - exp -mCX|Z (|0)  1 - e-m

(B.6)

which is always strictly less than one for finite m. On the other hand, the fixed size selection rule has the limiting success probability from Proposition 3.4(g) of

lim
n

pCsuZc,cXess

(n,

m,

)

=

1

-

1 - CX|Z (|0) m ,

(B.7)

which can achieve one for finite m, provided CX|Z (|0) = 1. Furthermore, this is shown to be asymptotically strictly greater than that for the randomised selection size. Let c := CX|Z (|0), and use the fact that log (1 - c)  -c for all c  0 (refer to Lemma C.2) so that

1 - e-mc  1 - (1 - c)m ,

(B.8)

with equality if and only if c = 0, i.e. both sides are equal to zero. However the lower bound in Proposition 3.3 implies a strictly positive success probability, so it must be that

1 - e-mc < 1 - (1 - c)m .

(B.9)

C Proofs of Main Results

C.1 Proof of Theorem 3.1

Observe that

min X 1 , . . . , X m  x

(C.1)

if and only if

min FX X 1 , . . . , FX X m  .

(C.2)

Moreover, the ordering of FZ (Z1) , . . . , FZ (Zn) compared to Z1, . . . , Zn is unchanged. Therefore using the probability integral transform, we can assume without loss of generality that (Z, X) is a copula distribution, so that Z and
X are each Uniform (0, 1) distributed. With the law of total probability, write

1

1

pFsuZc,cXess (n, m, ) = · · · Pr min X 1 , . . . , X m

0

0

  Z1:n = z1, . . . , Zm:n = zm × fZ1:n,...,Zm:n (z1, . . . , zm) dz1 . . . dzm, (C.3)

where

fZ1:n ,...,Zm:n

(z1, . . . , zm)

=

n! (n - m)!

(1 - zm)n-m I{z1···zm}

(C.4)

is the joint PDF of the first m order statistics of the i.i.d. Uniform (0, 1) sample (Z1, . . . , Zn), for which the form may be deduced via [27, §2.2]. Further note that since each pair (Zi, Xi) is sampled independently, then each X j is
conditionally independent of all the variables

X 1 , . . . , X j-1 , X j+1 , . . . , X m , Z1:n, . . . , Z(j-1):n, Z(j+1):n, . . . , Zm:n ,

(C.5)

13

Ordinal Optimisation and the Offline Multiple Noisy Secretary Problem

given Zj:m. Denote the event Z = {Z1:n = z1, . . . , Zm:n = zm} for brevity. Then

Pr min X 1 , . . . , X m

Z

= 1 - Pr X 1 > , . . . , X m >  Z
m
= 1 - Pr X j >  Zj:n = zj
j=1 m
= 1 - Pr (X > |Z = zj)
j=1 m
= 1 - (1 - Pr (X  |Z = zj))
j=1 m
= 1 - 1 - CX|Z (|zj) .
j=1

Plugging (C.4) and (C.10) into (C.3), we have the result claimed.

(C.6) (C.7) (C.8) (C.9) (C.10)

C.2 Proof of Proposition 3.1

Starting from (C.3) and applying (C.10), we have

1

1

pCsuZc,cXess (n, m, ) = · · · Pr min X 1 , . . . , X m   Z1:n = z1, . . . , Zm:n = zm

0

0



× f  Z1:n,...,Zm:n (z1, . . . , zm) dz1 . . . dzm

m

= 1 - EZ1:n,...,Zm:n  Pr (X > |Z = Zj:n) .

j=1

(C.11) (C.12)

Consider some arbitrary ordered selection of size m from (Z1, . . . , Zn), denoted (Z1 , . . . , Zm ). Then we have multivariate stochastic dominance [18, §6.B.1]

(Z1:n, . . . , Zm:n) st (Z1 , . . . , Zm ) ,

(C.13)

since

Z1:n  Z1 , Z2:n  Z2 , . . . , Zm:n  Zm

(C.14)

for every realisation

m j=1

Pr

(X

>

|Z

=

of zj )

(Z1, . . . , Zn), so is a non-decreasing

[18, Theorem function in (z1, .

.

6.B.1] . , zm),

holds. so by the

Applying the SIPD definition of stochastic

condition, dominance

[18, §6.B.1], we have









m

m

E  Z1:n,...,Zm:n Pr (X > |Z = Zj:n)  EZ1 ,...,Zm   Pr X >  Z = Zj  .

(C.15)

j=1

j=1

Hence the success probability is maximised when (Z1 , . . . , Zm ) is chosen as the first m order statistics.

C.3 Proof of Proposition 3.2

For the proof of Proposition 3.2, we require the following lemma.

Lemma C.1. Let Z[m]:n := (Z1:n, . . . , Zm:n) denote the joint first m order statistics of an i.i.d. sample of size n from parent distribution Z. Then

Z[m]:(n+1) Z[m]:n.
st

(C.16)

Proof. Consider the following construction on the same probability space. Form an i.i.d. sample of size n + 1 from
Z and take the first m order statistics. This will be equal in law to Z[m]:(n+1). Now delete one element uniformly at random, and re-compute the first m order statistics. This will be equal in law to Z[m]:n. Moreover, for every realisation (denoted in lowercase), we have

z1:(n+1), . . . , zm:(n+1)  (z1:n, . . . , zm:n) .

(C.17)

Therefore from the characterisation of stochastic dominance in [18, Theorem 6.B.1], (C.16) holds.

14

Ordinal Optimisation and the Offline Multiple Noisy Secretary Problem

For (a), it follows from Lemma C.1, via the same technique and analogous arguments as in the proof of Proposition 3.1.

For (b), since

min X 1 , . . . , X m  min X 1 , . . . , X m+1 ,

(C.18)

then

pCsuZc,cXess (n, m, ) = Pr min X 1 , . . . , X m  

(C.19)

 Pr min X 1 , . . . , X m+1  

(C.20)

= pCsuZc,cXess (n, m + 1, ) .

(C.21)

For (c), by De Morgan's laws (i.e. complement of the union is the intersection of the complements), put the definition of pCsuZc,cXess (n, m, ) in terms of

m

pCsuZc,cXess (n, m, ) = 1 - Pr

X i > x .

i=1

Then apply the properties that x is non-decreasing in  and Pr

m i=1

Xi

> x

(C.22) is non-increasing in x.

C.4 Proof of Proposition 3.3

For the lower bound, consider the success probability where the selection of size m is uniformly random without replacement from (Z1, . . . , Zn). This selection is identical in law to (Z1, . . . , Zm), which are i.i.d. Then the success probability can be computed by

m
Pr (min {X1, . . . , Xm}  x ) = 1 - Pr (Xj > x)
j=1
= 1 - (1 - )m .

(C.23) (C.24)

Based on similar arguments provided from the proof of Proposition 3.1, it follows that this lower bounds pCsuZc,cXess.

For the upper bound, we can use the fact

min Xi  min
i{1,...,n}

X 1 ,...,X m

to bound

pCsuZc,cXess (n, m, ) = Pr min X 1 , . . . , X m

 Pr

min
i{1,...,n}

Xi



x

n

= 1 - Pr (Xi > )

i=1
= 1 - (1 - )n .

 x

(C.25)
(C.26) (C.27) (C.28) (C.29)

C.5 Proof of Proposition 3.4

For cases (a), (b), (c), the results are immediate from applying the general upper and lower bounds in Proposition 3.3. For (d), assuming without loss of generality that (Z, X) is a copula and applying the property of comotonicity, then

pCsuZc,cXess (n, m, ) = Pr min X 1 , . . . , X m   = Pr (min {Z1:n, . . . , Zm:n}  ) = Pr (Z1:n  )
n
= 1 - Pr (Zi > )
i=1
= 1 - (1 - )n .

(C.30) (C.31) (C.32)
(C.33)
(C.34)

15

Ordinal Optimisation and the Offline Multiple Noisy Secretary Problem

For (e), by independence of Z and X we have from (C.10):

m

m

1 - (1 - Pr (X  |Z = zj)) = 1 - (1 - Pr (X  ))

j=1

j=1

= 1 - (1 - )m .

(C.35) (C.36)

This can be taken outside of the integral in (C.3) (where the integral evaluates to one), so the result follows. Case
(f) follows from the lower bound in Proposition 3.3. Lastly for (g), as n  , the joint density (C.4) converges
to the Dirac delta function in argument zm. Considering the region of integration in the expression (3.1) where 0  z1  · · ·  zm, we have that zm = 0 implies z1 = · · · = zm-1 = 0. Therefore

m

lim
n

psuccess

(n,

m,

)

=

1

-

(1 - Pr (X  |Z = 0))

j=1

(C.37)

= 1 - 1 - CX|Z (|0) m .

(C.38)

C.6 Proof of Theorem 4.1

For the proof of Theorem 4.1, we require the following lemmas.

Lemma C.2. We have

-p log 4  log (1 - p)  -p,

where the lower bound applies for all p 

0,

1 2

, and the upper bound applies for all p  0.

(C.39)

Proof. graph.

The lower The upper

bound can bound can

be established over p  also be established over

p0, 210

via via

concavity of concavity.

log

(1

-

p),

i.e.

line

secants

lie

below

the

Lemma C.3. For the Gaussian Q-function given by Q (x) = 1 -  (x), we have

over x  0, with

for any  

0,

 2

.

c1 exp -c2x2



Q (x)



1 2

exp

-

x2 2

c1

=

1 2

-

 

c2

=

cot   - 2

(C.40)
(C.41) (C.42)

Proof. The lower bound is due to [28, Equation (2)] and the upper bound is found in [29, Equation (5)].

Lemma C.4. Let Z1:n denote the first order statistic of an i.i.d. standard Gaussian sample of size n. For any



0,

 2

, let

c1

=

1 2

-

 

c2

=

cot   - 2

.

(C.43) (C.44)

Consider Z1:n  N µn, n2 where

µn = - log (nc1) c2

n2

=

2c2

- log log 2 (log (nc1) - log

log

2)

.

(C.45) (C.46)

Then there exists some integer n () such that for all n  n (), we have Z1:n Z1:n, i.e. Z1:n stochastically
st
dominates Z1:n.

16

Ordinal Optimisation and the Offline Multiple Noisy Secretary Problem

Proof. If Z1:n stochastically dominates Z1:n, then by definition Pr Z1:n  z  Pr (Z1:n  z) for all z  R. Or in terms of the Gaussian Q-function Q (z) = 1 -  (z), we require

Q (z)n  Q

z - µn n

(C.47)

for all z  R, where the left-hand side follows from well-known form of the distribution for the first order statistic [30, Equation (2.2.11)]. The idea is to show that this bound holds over three different intervals whose union is R, being (-, µn], [µn, 0] and [0, ). We begin with z  (-, µn]. Since µn  0, then via the lower bound in Lemma C.3

Q (z)n  1 - c1 exp -c2z2 n .

(C.48)

Since 0  c1 exp -c2z2  1/2, then putting p = c1 exp -c2z2 in the upper bound from Lemma C.2, we get

Q (z)n  exp -nc1 exp -c2z2 = exp - exp - c2z2 - log (nc1) .

(C.49) (C.50)

Now using the upper bound in Lemma C.3, we have for z  µn:

1

-

1 2

exp

-

(z

- µn)2 2n2

Q

z - µn n

.

(C.51)

The lower bound in

Lemma C.2 implies exp (-p log 4)



1 - p.

Applying this

with p

=

1 2

exp

- (z-µn)2
2n2

and after

some manipulation, we arrive at

Q

z - µn n

 exp - exp -

(z

- µn)2 2n2

-

log log 2

.

(C.52)

Thus a sufficient condition for Q (z)n  Q

z-µn n

over z  (-, µn] is

exp - exp - c2z2 - log (nc1)

 exp

- exp

-

(z

- µn)2 2n2

-

log log 2

(C.53)

or equivalently,

1 n2

- 2c2

The roots of this quadratic are at

z2

-

2

µn n2

z

+

µ2n n2

+

2 log (nc1)

-

2 log log 2



0.

(C.54)

z = µn ±

µ2n - (1 - 2c2n2 ) (µ2n + 2n2 log (nc1) - 2n2 log log 2) 1 - 2c2n2

(C.55)

with discriminant  calculated by

 = n4 (4c2 log (nc1) - 4c2 log log 2) + n2 2c2µ2n - 2 log (nc1) + 2 log log 2 .

(C.56)

Under the same choice of , note 2c2µ2n = 2 log (nc1) and the discriminant becomes

 = n4 (4c2 log (nc1) - 4c2 log log 2) + n2 (2 log log 2) .

(C.57)

The quadratic inequality is satisfied everywhere if the discriminant is non-positive, so put  = 0 and taking the

positive solution for n2 , giving

n2

=

2c2

- log log 2 (log (nc1) - log

log

2)

.

(C.58)

Therefore the inequality is satisfied provided nc1 > 1, which occurs for sufficiently large n, since c1 > 0. Next we

show that the stochastic dominance condition above. Over this interval, we can use the same

uisppsaetrisbfioeudndfoornzQ(z[)µnna,s0b],eufonrdee,ratnhdenporwopwoseehdavcheothiceeloowf µernbaonudndn

Q

z - µn n

 exp - c2

z - µn n

2
- log c1

.

(C.59)

17

Ordinal Optimisation and the Offline Multiple Noisy Secretary Problem

Thus we want to show that

Fix z, and recognise that

µ2n n2

=

O

nc1 exp (log n)2

-c2z2  c2

z - µn n

2
- log c1.

(C.60)

in the right-hand side, while the left-hand side is O (n). Therefore

O (n)  (log n)2

(C.61)

since O (en)  O n2 . Lastly for the interval z  [0, ), we use the upper bound in Lemma C.3 to give

Q (z)n



1 2n

exp

-

nz 2

2

(C.62)

and we can use the same lower bound as in the preceding interval. In the same vein as above, we want to show

n 2

-

c2 n2

z2

-

2c2µn n2

+ n log 2 +

µ2n n2

- log c1



0.

The discriminant of the quadratic is non-positive when

(C.63)

n 2

-

c2 n2

n log 2

+

µ2n n2

-

log c1



c22 µ2n n4

.

(C.64)

The left-hand side is O n2 and the right-hand side is O (log n)3 , thus this inequality is also satisfied for sufficiently

large n.

To establish the right inequality in (4.6), assume without loss in generality that (Z, X) are bivariate standard Gaussian

with correlation , as in (4.1). By establishing Z1:n Z1:n via Lemma C.4, then via the same technique as in the
st
proof of Proposition 3.1, we have

pNsucCc(eps)s (n, 1, ) = 


Pr X  -1 () Z = z
- 
Pr X  -1 () Z = z
-

fZ1:n (z) dz fZ1:n (z) dz,

(C.65) (C.66)

where fZ1:n (·) is the density of the first order statistic of the standard Gaussian, and fZ1:n (·) is the density of N µn, n2 . Using well-known conditioning formulae for Gaussians [31, Equation (A.6)], we also have

[X|Z = z]  N z, 1 - 2 .

(C.67)

Thus, the integral (C.66) may be computed analytically with well-known marginalisation formulae for linear-Gaussian

systems [32, Theorem 2.3.1]. In particular, if Z1:n  N µn, n2 and X Z1:n = z  N z, 1 - 2 , then X  N µn, 1 - 2 + 2n2 . Hence



Pr
-

X  -1 () Z = z

fZ1:n (z) dz = 

-1 () - µn . 1 - 2 + 2n2

The left inequality in (4.6) is a result of monotonicity in m from Proposition 3.2(b).

(C.68)

D Algorithms
The optimised lower bound in (4.7) can be implemented numerically. This is done by using sufficient conditions found in the proof of Lemma C.4 to check whether n  n () for a given n and . We are required to check whether the inequality (C.47) is satisfied over each of the intervals (, µn], [µn, 0] and [0, ). The inequality is satisfied over (, µn] by construction provided nc1 > 1, whereas (C.64) contains the sufficient condition for the interval [0, ). As for the bounded interval [µn, 0], we can directly evaluate (up to the available numerical precision) whether (C.47) is satisfied. Pseudocode to implement this numerical certificate is provided in Algorithm 1. Using this certificate, we can implement the optimised lower bound (4.7), with pseudocode for this found in Algorithm 2.

Algorithm 3 implements an optimised n to address Problem 2.1(b) for the Gaussian copula success probability, as described in Section 4.2. The procedure also calls the numerical certificate from Algorithm 1.

18

Ordinal Optimisation and the Offline Multiple Noisy Secretary Problem

Algorithm 1 Numerical certification of sufficient conditions for n  n () in Theorem 4.1

1: function NUMERICALCERT(n, )

2:

c1



1 2

-

 

,

c2



cot   - 2

3:

µn  -

log (nc1) , c2

n2



2c2

- log log 2 (log (nc1) - log log 2)

4: if nc1  1 then

 Check sufficient condition for the interval (, µn]

5:

return False

6: else if (C.47) fails over [µn, 0] then

 Check sufficient condition for the interval [µn, 0]

7:

return False

8: else if (C.64) fails then

 Check sufficient condition for the interval [0, )

9:

return False

10: else

11:

return True

Algorithm 2 Implementation of lower bounds in Theorem 4.1 and (4.7)

1: function LOWERBOUND(n, , , )

2: if NUMERICALCERT(n, ) then

3:

return Right-hand side of (4.6)

4: else

5:

return 0

6: function OPTIMISEDLOWERBOUND(n, , )

7:   machine epsilon

8: return max[,/2-]LOWERBOUND(n, , , )

 Lower bound in (4.6)  Optimised lower bound in (4.7)

Algorithm 3 Implementation of optimised n for solving Problem 2.1(b) with Gaussian copula

1: function NSTAR(, , , )

2: Solve quartic (4.9) for greatest real root x

3: n  exp x2 - log c1

4: if NUMERICALCERT(n, ) then

5:

return n

6: else

7:

return 

8: function NSTAROPTIMISED(, , )

9:   machine epsilon

10: return min[,/2-]NSTAR(, , , )

19

