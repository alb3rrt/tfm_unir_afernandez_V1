Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000. Digital Object Identifier 10.1109/ACCESS.2017.DOI
Heart Sound Classification Considering Additive Noise and Convolutional Distortion
FARHAT BINTE AZAM1, (Student Member, IEEE), MD. ISTIAQ ANSARI1, IAN MCLANE2, (Member, IEEE) AND TAUFIQ HASAN1 (Senior Member, IEEE)
1mHealth Laboratory, Department of Biomedical Engineering, Bangladesh University of Engineering and Technology (BUET), Dhaka - 1205, Bangladesh. 2Sonavi Labs Inc., Baltimore, MD 21230, USA.
Corresponding author: Taufiq Hasan (e-mail: taufiq@bme.buet.ac.bd).

arXiv:2106.01865v1 [cs.SD] 3 Jun 2021

ABSTRACT Cardiac auscultation is an essential point-of-care method used for the early diagnosis of heart diseases. Automatic analysis of heart sounds for abnormality detection is faced with the challenges of additive noise and sensor-dependent degradation. This paper aims to develop methods to address the cardiac abnormality detection problem when both types of distortions are present in the cardiac auscultation sound. We first mathematically analyze the effect of additive and convolutional noise on short-term filterbankbased features and a Convolutional Neural Network (CNN) layer. Based on the analysis, we propose a combination of linear and logarithmic spectrogram-image features. These 2D features are provided as input to a residual CNN network (ResNet) for heart sound abnormality detection. Experimental validation is performed on an open-access heart sound abnormality detection dataset involving noisy recordings obtained from multiple stethoscope sensors. The proposed method achieves significantly improved results compared to the conventional approaches, with an area under the ROC (receiver operating characteristics) curve (AUC) of 91.36%, F-1 score of 84.09%, and Macc (mean of sensitivity and specificity) of 85.08%. We also show that the proposed method shows the best mean accuracy across different source domains including stethoscope and noise variability, demonstrating its effectiveness in different recording conditions. The proposed combination of linear and logarithmic features along with the ResNet classifier effectively minimizes the impact of background noise and sensor variability for classifying phonocardiogram (PCG) signals. The proposed method paves the way towards developing computer-aided cardiac auscultation systems in noisy environments using low-cost stethoscopes.
INDEX TERMS Additive and convolutional distortion, stethoscope variability, heart sound classification.

C ARDIOVASCULAR Diseases (CVDs) pose a significant burden on the public health sector, causing about 17.9 million deaths every year, which is 31% of all deaths worldwide [1]. According to the Center for Disease Control (CDC), in the US alone, one person dies of CVDs every 36 seconds [2]. With the shortage of trained physicians and scarcity of diagnostic equipment, the burden of CVDs is much higher in the low- and medium-income countries [1]. Since early diagnosis is the key to reducing the burden of CVDs, low-cost screening tools such as digital stethoscopes with automatic murmur analysis algorithms are becoming more widely available [3]. With the recent advancement in embedded systems, it is now possible to incorporate complex machine learning models within built-in microprocessors
VOLUME 4, 2016

within the stethoscope [4], [5]. However, distortions introduced by the stethoscope sensor (e.g., diaphragm, amplifier) and background noise are of particular concern in designing computer-aided cardiac auscultation frameworks, especially in the underserved communities.
A considerable amount of research has been performed in the area of automatic heart sound analysis for the detection of CVDs. Publicly available datasets [6], [7] have accelerated the progress of research in this area, in particular the 2016 Physionet/CinC Challenge [8], [9] dataset. This dataset provides an archive of 4430 PCG recordings acquired using seven different stethoscopes/sensors and are annotated as either physiological (normal) or pathological (abnormal). Various approaches have already been attempted for heart sound
1

Heart sound%[#]

Respiratory sound

Additive noise Other body sounds

&[#]

Motion/friction noise

Environmental noise

Convolutional distortion

[#]

· ·

Sensor transduction Transmission channel

Recorded heart sound/PCG
' # = % # + &[#]  [#]

FIGURE 1: Schematic model showing the sources of additive noise and convolutional distortion during cardiac auscultation.

abnormality detection using this dataset. A diverse set of front-end features have been examined including, time, frequency and statistical features [10], Mel-frequency Cepstral Coefficients (MFCC) [11]­[14], and Continuous Wavelet Transform (CWT) [15], [16] features. Time-frequency based features including MFCC and Mel-Spectrograms are most commonly being used in recent studies. A wide array of classifiers have been attempted including the k-Nearest Neighbor (k-NN) [12], Support Vector Machine (SVM) [17], Random Forest [10], Multilayer Perceptron (MLP) [16], [18], deep learning approaches with 1D & 2D CNNs [19]­[23], and Recurrent Neural Network (RNN) [24]. While [20] addressed the issue of domain variability for heart sound classification, the issue of additive noise was not addressed. A summary of recent research works relevant to the proposed method are provided in Table 1. Note that different authors used different train-test splits using the PhysioNet data, thus, the results are not comparable in most cases.
A major limitation of the previous approaches is that while only sensor variability was addressed in [20], none of the works have considered the simultaneous presence of channel/sensor and additive noise distortion for this task. Sensor/channel variability affects the signal through timeconvolution, whereas additive noise affects the signal linearly. Thus it is very difficult to disentangle the two components of distortion. Traditional signal processing methods have addressed mainly the channel distortion issue through the log-filterbank analysis, where the convolutional distortion becomes additive [26]. This also explains the effectiveness of MFCC features for the heart sound classification task. Previous work on addressing simultaneous presence of additive and convolutional noise has been mainly focused on speech processing applications [27]­[29]. However, this issue is yet to be addressed for heart sound analysis in the literature.
In the case of heart sounds, the major sources of signal degradation are: (i) the variability due to stethoscopes/sensor [20] affecting the PCG as a convolutional degradation (channel distortion), and (ii) the effect of additive noise (e.g., from the environment or the patient). These effects are depicted in Fig. 1. In particular, noise is a major problem in automatic assessment of cardiac diseases using stethoscopes, especially in low-resource hospitals. In [20], it was shown that the state-of-the-art algorithms for heart sound classification are not capable of providing a consistent performance when different stethoscopes are encountered in the test data. While
2

the overall performances were impressive, the algorithms easily overfit to the data recorded using the most prevalent stethoscope. This is the reason why 10-fold cross-validation [25], frequently performed on this dataset, provides overoptimal performance as the test sets cannot provide sufficient variability due to the severe domain imbalance present in the Physionet data (Fig. 6). In [20], a mini-batch balancing method was proposed to address the performance issue due to domain variability, and consistent improvement of performance across different domains was found on a domain and class balanced test set. However, this dataset also contains additive noise due to human speech, environmental sounds, stethoscope motion, breathing, intestinal activity, etc. [9], which was not considered in [20].
This paper aims to address the simultaneous presence of additive noise and convolutional distortion for the heart sound classification problem. In the case of cardiac auscultation, additive noise mainly includes respiratory sounds and environmental noise. The convolutional distortion represents the transduction effect of the stethoscope sensor. The main contributions of this paper are:
· We introduce a model of the acoustic environment during auscultation, considering the additive and convolutional distortions present in the system.
· Perform analysis of additive and convolutional distortion on heart sounds in the linear and logarithmic filterbank domain and show that both distortions can be converted into an additive distortion.
· Analyze the effect of additive uncorrelated random noise in the first layer of a conventional CNN model.
· Proposed a simple feature fusion technique combining linear and logarithmic filterbank features to simultaneously address additive noise and convolutional distortion.
· Experimentally validate that the proposed feature fusion scheme can effectively improve heart sound classification performance compared to existing methods using a deep residual network.
The remainder of this paper is organized as follows. In Section I-II we elaborate on our motivation and implementation details of our proposed method. Section III describes the datasets used, different experiments performed, analysis of the results obtained by the proposed method and performance comparisons. Finally, in Section IV and V, we discuss the limitations of our method, future directions and summarize our findings.
I. BACKGROUND A. ADDITIVE AND CONVOLUTIONAL DISTORTION
Inspired by [27], we consider a parametric model of the stethoscope as shown in Fig. 2. This assumes that the original PCG signal and an uncorrelated noise component are first added before being processed through a time-convolution operation due to the stethoscope's impulse response. For a given short-time segment of length N , we assume that the clean
VOLUME 4, 2016

TABLE 1: Summary of recent research works on heart sound classification using 2016 PhysioNet/CinC Database [9]

Author Potes et al. [19] Rubin et al. [11] Bozkurt et al. [13] Ren et al. [15] Noman et al. [14] Humayun et al. [20] Deng et al. [25]

Year 2016 2016 2018 2018 2019 2020 2020

Features Raw waveform MFCC
MFCC
Scalogram
MFCC
Raw waveform MFCC

Classifier Branched 1DCNN 2D-CNN
2D-CNN
Pre-trained 2D-CNN 1D-CNN, 2DCNN Learnable filter & 1D-CNN Recurrent 2DCNN

Sen. 0.88 0.765 0.845 0.246 0.8994 0.8695 0.9866

Spe. 0.82 0.931 0.785 0.878 0.8635 0.7602 0.9801

MAcc. 0.85 0.848
0.562 0.8815 0.8149
-

Acc. -
0.815 -
0.8922 -
0.9834

Remarks Top scoring system in [8]. In-house test performance reported. Official test set results are reported (not publicly available). Noise and/or sensor variability effects not considered. Noise and/or sensor variability effects not considered. Custom data split used. Domain variability or noise not considered. Addressed domain variability. Did not consider additive noise. 10-fold cross-validation used which shows overoptimistic results.

This study uses a balanced test data across domains and pathology class using the 2016 PhysioNet validation set. This allows for better evaluation of the performance across different data subsets. However, the results are not comparable with other studies using different data-splits or a 10-fold cross validation.

%[#]
Clean PCG

Additive noise
![#]
Noisy PCG

Stethoscope impulse response
[#]

'#
Noise & channel degraded PCG

FIGURE 2: Parametric model of the PCG signal acquisition process showing additive noise and convolutional distortion.

PCG, noisy PCG, additive noise and stethoscope impulse response are given by, s[t], x[t], n[t] and h[t], respectively. According to the parametric model of Fig. 2,

x[t] = s[t] + n[t]  h[t].

(1)

Note that we assume the stethoscope impulse response is independent of the short-time segment as it is a time-invariant property of the device. We also assume that the impulse response h[t] has a length less than N . Expressing (1) in the Discrete Fourier Transform (DFT) domain, in each segment, using the convolution property of DFT, we obtain

X[k] = S[k] + N [k] H[k].

(2)

In this step, we assume a K-point DFT with k = 0, 1 · · · , K representing the frequency indices, whereas X[·], S[·], N [·] and H[·] represent the DFT coefficients of the corresponding time-domain signals. Note that the convolutional distortion H[k] due to the stethoscope transfer function has now become multiplicative. Next, we estimate the power spectrum of this short-time PCG segment by squaring the absolute value of the DFT coefficients and obtain
|X[k]|2 = |S[k]|2 + |N [k]|2 |H[k]|2 (3)
+ 2Re{S[k]H[k]N [k]H[k]}.

B. LINEAR FILTERBANK ENERGY ANALYSIS Audio signals can be conveniently analyzed in different frequency bands by using a filterbank. Let us assume a predefined filterbank of M filters and denote the i-th filter energy

VOLUME 4, 2016

coefficient of the noisy PCG signal as X[i]. We assume that

the filterbank-energy term X[i] is related to X[k] by the

expression

|X[i]|2 = Eik |X[k]|2 .

(4)

Here, Eik indicates that the expected value operation is performed over the k frequency indices that belong to the i-th
filter. Substituting the values from (3) we obtain,

|X[i]|2 = Eik |S[k]|2|H[k]|2 + |N [k]|2|H[k]|2

+2Re{S[k]H[k]N [k]H[k]} .

(5)

Statistically, heart sound s[t] and additive noise n[t] are independent and uncorrelated, and therefore the expected value of the last term of (5) can be approximated to be zero. According to this assumption and using the definition of filterbank energy, we simplify (5) to obtain a similar expression derived in [27] given by

|X[i]|2 = (|S[i]|2 + |N [i]|2)|H[i]|2.

(6)

Here, S[i], N [i] and H[i] represent the filterbank-energy values obtained from the corresponding DFT coefficients. Thus, according to (6), the background noise component N [i] is additive to the signal component S[i] whereas the channel distortion component H[i] is multiplicative in the filterbank energy domain.

C. LOG-FILTERBANK ENERGY ANALYSIS In filterbank analysis, logarithm operation is usually performed on the energy coefficients to increase the dynamic range and transform multiplicative operation into addition. Taking natural logarithm over (6) we obtain
log |X[i]|2 = log |H[i]|2 + log |S[i]|2 + |N [i]|2 . (7)
As expected, (7) reveals that in the log-filterbank domain, the channel or transducer/sensor distortion becomes additive while the heart sound signal and background noise becomes non-linear functions of the output log-energy coefficient.
3

This was one of the motivations of Cepstral analysis to separate the channel effect from the signal [26], [30]. We may write (7) to express the log-energy vectors for each frame using matrix notation as

EX = EH + ES+N ,

(8)

where

T
EX = log |X[1]|2, log |X[2]|2 · · · log |X[M ]|2 ,

T
EX = log |H[1]|2, log |H[2]|2 · · · log |H[M ]|2 , and

 log(|S[1]|2 + |N [1]|2) 

 log(|S[2]|2 + |N [2]|2) 

ES+N

=

 



...

.  

log(|S[M ]|2 + |N [M ]|2)

In (8), each vector consists of M elements obtained from the individual front-end filters. To summarize our analysis on filterbank and log-filterbank energy analysis, when both additive and convolutional distortion is present, it is not possible to decompose them into additive terms in the feature domain. However, noise component as in (6) or the convolutional distortion component as in (7) can become additive separately. A static additive distortion component from features can be easily reduced by mean normalized which was the original motivation for the traditional Cepstral mean normalization (CMN) [31] used for robust speech recognition.

D. CEPSTRAL FEATURE DOMAIN ANALYSIS
MFCC features are frequently utilized in speech and audio processing. After the extraction of log-filterbank features, an additional step of Discrete Cosine Transform (DCT) is applied to obtain the well-known MFCC coefficients [26]. The DCT step is performed to reduce the correlation among the different filterbank energy coefficients and compress the overall energy of the features to the leading few components. Therefore, after this step, the higher-order DCT coefficients are usually eliminated, and thus, the feature dimensionality is reduced. Since DCT is a linear operation, it follows from (8) that the convolutional distortion component will still be an additive component after this operation. Thus we have

DCT (EX ) = DCT (EH ) + DCT (ES+N )

(9)

Similarly, we can easily show that by removing the higher order DCT coefficients, (9) still holds for the remaining coefficients. However, the PCG signal and additive noise is still entangled within the component ES+N where their relationship is non-linear.

E. ADDITIVE DISTORTION IN A CNN MODEL
Although CNNs are primarily used for image classification, in many 1D signal processing applications, 2D spectrogramimages are used as input features to CNN models for classification [32]. In addition, previous methods have utilized additive noise in training for data augmentation that generally

improves the robustness of the models [33], [34]. In this sub-

section, we perform a mathematical analysis of the CNN

input layer when an additive noise component is present in

the input feature data. In the 0th (first) layer of a CNN model, for a 2D input

feature matrix Fcn and the j-th 2D kernel Kj, the output j-

th feature map before the nonlinear activation layer is given

by

Gj = Fcn  Kj .

(10)

Here,  represents the correlation operation performed in a CNN layer. Eq. (10) can be expressed for each individual matrix elements of row m and column n as,

Gj[m, n] = Fcn  K[m, n]

l

l

=

Fcn[m + , n + ]Kj[, (]1. 1)

=-l =-l

Here, we assume a kernel size of (2l + 1) × (2l + 1), and

 and  are dummy variables. When spectrogram images

from heart sounds are considered, m represent the short-time

frame index whereas n represent the frequency-band index.

Let us now assume that the input feature matrix Fcn can

be expressed as

Fcn = Fc + Fn

(12)

where Fc is dependent on the class c  C where C defines the set of disease classes, and Fn is an additive noise component independent and uncorrelated to Fc (and thus independent of c). The source of Fn may environmental noise or sensor degradation. Considering the presence of this additive distor-
tion, we obtain from (10)

Gj = (Fc + Fn)  Kj

= Fc  Kj + Fn  Kj.

(13)

Assuming that the model has converged after being trained on a sufficiently large amount of data, we may compute the expected value of the feature map as

E{Gj} = E{Fc  Kj} + E{Fn  Kj}. (14)

At this stage, we will make a few simplifying assumptions. Since CNN models are trained using a loss-function that minimizes the classification error, it can be expected when trained on sufficient training data, the model parameters will be dependent on the disease classes c. In contrast, the additive component Fn by definition is independent and uncorrelated of Fc and thus the disease class c. Therefore, we may assume that after the training is converged, E{Fn  Kj}  0 and (13) can be written as

E{Gj}  E{Fc  Kj}.

(15)

Thus, we may conclude that in case of an additive uncorrelated noise present in the data, CNN model parameters can be assumed to be independent of the noise when sufficient training data is present. In contrast, for multiplicative noise in the feature matrix, it is obvious from (11) that a decomposition similar to (14) will not be possible and the learning of the network will be directly affected by the distortion.

4

VOLUME 4, 2016

F. FUSION OF LINEAR AND LOGARITHMIC FEATURES It is clear that linear filterbank energy features are linearly affected by the additive noise and non-linearly affected by channel/sensor noise, as shown in (6). In contrast, logarithmic filterbank energy features are linearly affected by channel/sensor noise and non-linearly affected by additive noise, as demonstrated in (7). Thus, in this respect, the linear and logarithmic filterbank energy features are complimentary. However, if we concatenate the two types of features, both additive noise and convolutional distortion can be separated as an additive component in the fused feature space. As shown in (15), a CNN model is less affected by additive distortion during classification, assuming that the noise is independent and uncorrelated with the signal component. However, in the proposed fused feature space, both noise and channel distortion components become additive. Therefore, we hypothesize that the fused feature set will provide improved performance in the presence of both additive and convolutional/channel noise. The following sections of this paper will focus on designing the relevant features and validating this hypothesis through experimental evaluation.
II. PROPOSED METHOD In this section, we discuss the development of the proposed feature extraction method and the CNN architecture for effective classification of heart sounds, considering both additive and convolutional distortions.
A. PRE-PROCESSING AND SEGMENTATION The heart-sound segments are first re-sampled to 1000Hz, followed by band-pass filtering between 25-400Hz, and cardiac cycle segmentation according to [35]. In the next step, we ensure that each heart sound training segment consists of a single cardiac cycle of a fixed duration of 2.5s. Zeropadding is applied if the cardiac cycle is shorter than 2.5s.
B. FEATURE EXTRACTION FRAMEWORK In order to validate our hypothesis, we needed to extract four different varieties of filter-bank based acoustic features from the PCG signal segments. These are based on equations (6), (7) and (9). The features can be effectively extracted using a unified framework outlined in Fig. 3. The extraction procedure is detailed below.
First, the PCG signal segments are divided into short-time frames of length 0.5s with a hop size 0.01s between successive frames. Filterbank analysis is performed on the 0.5s segments using 26 mel-scaled filters. The features extracted from these segments are summarized in Table 2. To validate our hypothesis, it is essential to perform classification experiments on all these features for the following reasons. According to (6), the Fbank features should be able perform better in case of additive noise. In the case of Log-Fbank, we can expect better results in case of convolutional distortion as per our analysis in (7). Since CNN models are able to model the correlation among the input features, it is unclear if the DCT step will improve the performance in our case. For this
VOLUME 4, 2016

TABLE 2: Description of acoustic features

Name Fbank Log-Fbank MFCC-26 MFCC-13

Feature description Filterbank energy features as shown in eq. (6) Log-filterbank energy features as shown in eq. (7) MFCC retaining all coefficients after DCT compression MFCC retaining 13 coefficients after DCT compression

Dimension 26 26 26 13

reason, we retain both MFCC-13 and MFCC-26 variants of the traditional MFCC features to observe the effect of DCT in the classification performance.
C. PREPARING 2D FEATURE MATRICES From each 2.5s segment, the extracted feature (column) vectors are stacked horizontally to form a 2D feature matrix (input) for classification using our CNN model. Each of these input feature matrices has a dimension of d × 246, where d represents the acoustic feature dimension. The four features as described in Table 2 are visualized in Fig. 4. Before feeding into the CNN model, features from each cardiac cycle are normalized by subtracting the mean of the full cycle and dividing by their standard deviation (STD).
D. MODEL ARCHITECTURE A residual neural network (ResNet) architecture based on [36] is used as the classification model. The model is modified to fit the proposed input features. The model has four resnet layers, each containing two residual blocks, the first block is with down-sampling, and the latter is with direct skip connection. Fig. 5 shows the model architecture used. Each residual block generally consists of two convolution blocks, which also perform the down-sampling operation. Each convolution block has one convolutional layer of kernel size 3, followed by a 2D batch normalization layer and Rectified Linear Unit (ReLU) activation functions. Weights of each convolutional layer have are initialized using the Xavier method [37]. The number of filters of these four

PCG segments
x[t]

Framing &
Windowing

Short-time Fourier
Transform

Mel-Filterbank Analysis

Fbank

MFCC-13
13 × 246

DCT compression

DCT (.)

Log(.)

26 × 246

MFCC-26
26 × 246

Log-Fbank
26 × 246

FIGURE 3: A flow diagram of the acoustic features extraction framework. Different features are extracted from different stages as depicted.

5

26

(a)

0 26

(b)

0 26

(c)

0
13 (d)
0

0

25

50

75

100

FIGURE 4: 2D visualization of the acoustic features as input to the proposed CNN model. (a) Filterbank energy (Fbank), (b) Log-Filterbank energy (log-Fbank), (c) Full-dimension MFCC (MFCC-26), (d) Dimension reduced MFCC (MFCC13).

both clinical and non-clinical environments. It consists of a total of 3,157 recordings from 764 patients and contains a total of 84,425 cardiac cycles ranging from heart rates 35 to 159 beats-per-minute (bpm). The PCG recordings have been collected from seven different research groups denoted as {a-g,i}. A brief summary of this dataset is shown in Table 3. Among seven categories, six {a-f} sets are distributed in training data, which is publicly available. The recordings generally have a duration between 5­120 seconds. Metadata of recording quality, annotations for the onset of S1, S2, systole, diastole are also provided.
As shown in Table 3, this dataset is unbalanced both in terms of pathology-class (normal vs. abnormal) and the data subsets/domains ({a-f}). We use 2,856 (90.47%) for training and the remaining 301 (9.53%) for test. This test set is based on the original Physionet challenge validation set which is domain and class balanced [20]. The train-test split is also patient independent by design. The class imbalance, variability of stethoscope sensors, and noisy environment are all challenging issues to be addressed in this dataset, making it suitable for experimental validation of the proposed approach.

TABLE 3: Data distribution of PhysioNet/CinC Challenge Database.

Subset

Total

Normal Abnormal

Subject recordings recordings

Used device

a

121

b

106

c

31

d

38

e (Norm.) 174

e (Abn.) 3351

f

112

117 385 7 27 1867 0 80

292

Welch Allyn Meditron

104

3M Littmann E4000

24

AUDIOSCOPE

28

Infral Corp. Prototype

0

MLT201/Piezo

151

3M Littmann

34

JABES

FIGURE 5: Residual model architecture used for heart sound abnormality detection.
resnet layers is 16, 32, 64, 128. The input feature matrix goes through a convolutional block with a max-pooling of 2 before the resnet layer. Extracted features from the fourth resnet layer are then flattened, and two neurons are added with softmax activation for binary classification (normal and abnormal PCG). The code is available online at https://github. com/mHealthBuet/CepsNET.
III. EXPERIMENTS AND RESULTS A. DATASET We use the 2016 PhysioNet/CinC Challenge Database [9] for our experiments. It is an archive of PCG recordings from
6

B. TRAINING REGIME In the training-set, the e-subset has the highest number of recordings (about 68%) as can be seen in Fig. 6. Thus we use the domain balance training (DBT) method that prepares mini-batches of data that are balanced for both class and domain [20]. This method was found to be effective in case of domain variability and data imbalance. We also apply shifting on both right and left directions to avoid the effect of segmentation errors.
To examine the effect of the input feature variation on classification performance, we perform our experiments in two steps. First, we use 2D matrices extracted from the individual features described in Table 2 as input to our model. The first three features (Fbank, Log-Fbank, MFCC-26) in Fig. 4 are of shape (246, 26) whereas the fourth feature (MFCC-13) is of the shape (246, 13). The proposed model is
1We only used 151 usable abnormal recordings from 'e' subset and the rest are removed due to data errors [20].
VOLUME 4, 2016

Acc.

Macc

F1

FIGURE 6: Distribution of sensor-dependent subsets of the PhysioNet heart sound data.
designed to be able to handle different input dimensions. The overall results of the experiments are shown in Table 4. In the second step, we used multiple feature combination of the four selected features (Fig. 4) for performance analysis. In this case, individual features are vertically concatenated, which increases the height of the input (minimum 26 to maximum 65) to our model.
C. OPTIMIZATION AND HYPERPARAMETER TUNING We use the cross-entropy loss from the final softmax layer output and perform optimization by stochastic gradient descent (SGD) using the Nesterov momentum method [38]. We also use a cyclic learning rate (CLR) scheduler to change the learning rate in a triangular shape with the same height. Hyper-parameters of the scheduler, which includes minimum and maximum learning rate and step size, are calculated according to [39]. The model generally converges to the optimal point within 50-60 epochs while trained using these parameters. Majority voting is used to obtain the final abnormality prediction of the heart sound recordings obtained from individual 2.5s segments.
D. PERFORMANCE METRICS The final decision provided by the network is used to compute the following performance matrices: AUC, F1-score, and modified accuracy (Macc) [20]. The Macc value is obtained by averaging sensitivity and specificity [9], and thus can be considered a reliable metric for the overall system performance.
In addition, we also calculate the mean and STD of the accuracy metric for each system within individual data subsets {a-f} [9] distributed as shown in Fig. 5. These data subsets are collected in different environmental locations (varying noisy conditions) and using different stethoscopes/sensors (varying convolutional channel effect) [9] and are summarized in Table 3. This subset-wise performance evaluation is essential as it shows if the overall performance is being skewed towards a specific subset due to data imbalance as seen in Fig. 5. The mean and STD accuracy over the subsets measure the robustness and consistency of the performance across different domains.
VOLUME 4, 2016

AUC

70% 75% 80% 85% 90% 95% 100%

Fbank

Log-Fbank

FIGURE 7: Comparison of performance between Fbank and Log-Fbank features depicting the effect of logarithm operation on the features.

E. BASELINES METHODS AND IMPLEMENTATION In this work, we use the best performing system in the Physionet 2016 CinC Challenge developed by Potes et al. [19]. This branched CNN model (Potes-CNN) is implemented as described in [19] and with the addition of the proposed DBT training scheme to address the domain variability present in the dataset. The network uses a set of finite impulse response (FIR) filters as a filterbank front-end and provides inferences for each segmented cardiac cycle [20]. The branched CNN model [19] with including the DBT method to handle domain variability is denoted as the Potes-CNN-DBT method. The results of these systems are summarized in Table 4.
F. PERFORMANCE OF INDIVIDUAL FEATURES First, we look at the performance of the individual features in the second section of Table 4. The best performing individual feature is the Log-Fbank with AUC, F1-score, and Macc values of 90.37%, 84.40%, and 84.55%, respectively. We hypothesized the logarithm operation on the Fbank features would improve the system performance since it can express the sensor variation as an additive component in the feature space. Observing the results of the Fbank and Log-Fbank features separately in Fig. 7, we indirectly validate this hypothesis as the logarithm operation provides an overall improvement in the classification performance while also improving the robustness of the system across the data subsets (increase in average domain-wise accuracy and reducing the STD). It should be emphasized that both the features Fbank and Log-Fbank carry the same information content since logarithm is a one-to-one function. Therefore, we assume the improvement is due to the model's ability to fit the data better.
Next, we analyze the performance of the system after the DCT operation leading to MFCC-26, and later MFCC13 due to dimensionality reduction. The DCT operation is generally used in feature dimensionality reduction as it performs compression of the data into a few coefficients and also de-correlates them. In theory, deep learning architectures should be able to model complex relation among the input features and thus decorrelating the features using a data-
7

TABLE 4: Experimental results of the baseline and proposed systems for the heart sound abnormality detection task.

Method

AUC F1 Score Macc

Accuracy in data subsets / domains

a

b

c

d

e

Domain avg. acc.

1D-CNN based End-to-End systems

Potes-CNN [19]

79.50 74.99 73.50 60.00 63.26 71.43 40

Potes-CNN DBT [20]

84.20 80.67 79.79 71.25 70.41 85.71 66.66

Humayun tConv-CNN DBT [20] 83.04 81.91 81.49 76.25 71.43 71.42 80

2D-CNN (ResNet) based systems using spectro-temporal features

Fbank

89.33 82.18 82.72 71.25 76.53 71.43 90

Log-Fbank

90.37 84.40 84.55 76.25 77.55 85.71 80

MFCC-26

87.82 79.54 81.15 77.50 69.39 85.71 80

MFCC-13

89.07 84.07 83.63 73.75 76.53 85.71 80

MFCC-13+

89.53 80.30 81.56 71.25 72.45 85.71 90

MFCC-13++

87.51 78.29 80.09 72.50 68.37 85.71 80

2D-CNN (ResNet) based systems using feature-level fusion (Proposed)

Fbank & Log-Fbank

90.67 83.22 83.18 72.50 75.51 85.71 90

Fbank & MFCC-13

91.36 84.09 85.08 78.75 76.53 85.71 90

Log-Fbank & MFCC-13

89.93 82.53 83.37 73.75 76.53 85.71 80

Fbank, Log-Fbank & MFCC-13 90.61 82.84 83.71 77.50 75.51 85.71 80

100 97.5 97.75
100 100 100 100 100 100
100 100 100 98.88

66.94±21.80 78.31±12.95 79.37±9.73
81.84±11.36 83.90±8.68 82.52±10.20 83.20±9.30 83.88±10.87 81.32±11.10
84.74±9.96 86.20±8.42 83.20±9.30 83.52±8.41

independent linear transformation, i.e. DCT, should not have an impact. However, in practice we observe that most of the performance metrics drop due to the DCT operation without dimensionality reduction. The performance of Log-Fbank and MFCC-26 are graphically compared in Fig. 8 where we can better observe the performance difference.
The performance drop from Log-Fbank to MFCC-26 may be explained as follows. Since the DCT operation performs feature compression, the high frequency DCT coefficients should be of of negligible value and carry very little information about the PCG signal [30]. Although the 26 components of Log-Fbank and MFCC-26 theoretically contain the exact same information, since the features are normalized before being fed in to the ResNet model, it is possible that these insignificant frequency components are being amplified out of proportion and confusing the model. This is consistent with our observation that by removing the 13 higher order coefficients from MFCC-26, we improve the performance as evident from the results of the MFCC-13 feature vector. This feature set provides the best overall performance among the individual features with a sensitivity, specificity, F1-score and Macc values of 89.86%, 77.40%, 84.07%, and 83.63%, respectively. The performance of MFCC-13 closely follows as expected since this feature configuration widely validated in speech and audio processing applications [30], [40].
8

Acc.

Macc

F1

AUC

70% 75% 80% 85% 90% 95% 100%

Log-Fbank

MFCC-26

FIGURE 8: Comparison of performance between Log-Fbank and MFCC-26 features depicting the effect of DCT operation without dimensionality reduction.

G. EFFECT OF VELOCITY AND ACCELERATION
Appending the velocity () and acceleration features () is very common along with MFCC-13 features, and thus we also examine the effect of these simple feature modifications. From the results in Table 4 and Fig. 9, we observe that the effect of adding these features do not improve the performance of the static MFCC-13 coefficients. We presume that since the CNN model inputs the successive MFCC-13 vectors as a 2D image, it can model the velocity and acceleration features quite well, and thus the addition of these features is
VOLUME 4, 2016

redundant.

Acc.

Macc

F1

AUC

70%

75%

MFCC-13

80% 85% MFCC-13 + 

90%

95% 100%

MFCC-13 +  + 

FIGURE 9: Comparison of performance among MFCC-13, MFCC-13+ and + features depicting the effect of appending velocity and acceleration features on the heart sound classification task.

TABLE 5: McNemar's Chi-squared test for statistical significance analysis compared to the baseline Potes-CNN-DBT [19], [20] system.

Input Feature

2

Significant p-value

(p < 0.05)

Fbank Log-Fbank MFCC-26 MFCC-13 MFCC-13+ MFCC-13++

24.00 0.2892

No

24.00 0.0980

No

0.28 0.5962

No

21.00 0.1690

No

0.4464 0.5040

No

0.0167 0.8973

No

Fbank & Log-Fbank

29.00 0.2750

No

Fbank & MFCC-13

18.00 0.0365

Yes

Log-Fbank & MFCC-13

19.00 0.1524

No

Fbank, Log-Fbank & MFCC-13 18.00 0.1114

No

H. FEATURE LEVEL FUSION
The previous sub-sections analysis reveals that Log-Fbank and MFCC-13 are the best two features for heart sound abnormality detection for classification using the ResNet model described. In Sec. I-F we argued that Fbank and LogFbank features have complimentary properties since they can express additive and convolutional distortions, respectively, as an additive component in the feature space. Therefore, our hypothesis is that fusion of the Fbank and Log-Fbank features would significantly improve heart sound classification performance, particularly on this task since it involves both stethoscope/sensor and background noise variability in the dataset [9]. However, in practice, the MFCC-13 and MFCC26 features have the same property of expressing convolution as a linear operation, as shown in (9), and thus can be equally effective in the presence of sensor variability.
In this section of our analysis, we discuss our feature-level fusion experiments, as shown in the third section of Table 4. These experiments use several combinations of Fbank, LogFbank, and MFCC-13 features, being the most promising individual feature sets. The results show that, indeed, the feature-level fusion of Fbank and MFCC-13 provides the overall best results and the domain-wise performance. This configuration yields an Macc score of 85.08% which shows an absolute gain of 5.29% and 11.58% over the Potes-CNN [19] and Potes-CNN DBT [20] baseline systems, respectively. In terms of domain-wise performance, it performs better in almost all of the data subsets {a-e} with a mean accuracy of 86.2%(±8.42%). Thus, the average accuracy over different domains has increased, but the STD of the accuracy is also reduced compared to individual spectrotemporal features. This validates our hypothesis that this particular feature combination is complementary and effectively addresses the issues of convolutional (sensor variability) and additive distortion (noise) components.
VOLUME 4, 2016

We want to point out several other observations from the results in Table 4. Firstly, the feature level fusion of all three features (Fbank, Log-Fbank, MFCC-13) does not show any additional benefit, as evident from Fig. 4. Secondly, a significant improvement in reducing domain-variability is achieved by introducing the DBT training scheme, which was already noted in [20]. This is evident from the reduction of STD values in the domain-wise performance evaluation. The Potes-CNN [19] with and without DBT yields domainwise accuracies of 66.94%(±21.80) and 78.31%(±12.95%), respectively. Therefore, the results have already been improved by the DBT scheme. However, the proposed feature fusion scheme provides an additional improvement over the Potes-CNN DBT system, which further validates the effectiveness of the proposed method. We have also provided the t-distributed Stochastic Neighbor Embedding (t-SNE) of the extracted features of our model trained with this fused feature set in Fig. 11, where we observe that the different domains are almost indistinguishable in this higher-dimensional space. This depiction is consistent with the observations presented in [20].
I. STATISTICAL SIGNIFICANCE ANALYSIS
In this section, we perform statistical significance tests to evaluate if the improvements obtained by the proposed method is significantly better than the baseline or not. We use McNemar's chi-squared test [41] to perform a pair-wise comparison of the system performances and validate whether the system performance is significantly different. The results are summarized in Table 5. According to this analysis, it is indeed confirmed that the performance improvement achieved by the proposed method with fusion of Fbank and MFCC-13 is significant (p < 0.05) compared to the baseline system Potes-CNN DBT [19], [20]. This feature-fusion
9

Acc.

Macc

F1

AUC

70%

75%

80%

85%

90%

Fbank MFCC-13 Fbank + MFCC-13

95% 100%

FIGURE 10: Improvement obtained by fusing linear and logarithmic features (Fbank and MFCC-13) in heart sound abnormality detection in the presence of noise and sensor variation.

FIGURE 11: TSNE visualization of the last layer of the proposed model showing the distribution of different data subsets. No meaningful clusters have formed with the features of different source domains.
combination provides the lowest p-value among all the other system comparisons and further justifies the effectiveness of the method in heart sound datasets with sensor and noise variability. The proposed system also provides an absolute 3.59% improvement in Macc compared to Humayun tConvCNN DBT [20]. However, this improvement is not significant according to the McNemar's chi-squared test (p > 0.05).
IV. DISCUSSION In this work, we have indirectly validated the hypothesis that feature-level fusion of filter-bank energy features and log-filter-bank energy features provide robust performance in the heart sound abnormality detection task in the presence of noise and sensor variability. However, a major limitation of the proposed approach is that the validation is empirical by nature. Although it is known that the Physionet data [9] contains environmental noise, it is not possible to precisely identify the sensor dependent (convolutional distortion) and the background noise (additive distortion) components from the PCG signals. However, since the results achieved are significantly improved compared to existing methods, especially
10

methods that already deal with reducing domain-variability [20], the proposed method is promising and justifies further investigation. A more thorough analysis could have been performed by artificially generating stethoscope effects using its impulse response [42] and adding environmental noise to clean heart sound recordings. In such a scenario, it would have been possible to observe the effect of the features in the presence or absense of additive and convolutional distortions more systematically. However, such analysis is outside the scope of this current work where we focus on a novel feature fusion scheme to handle channel and noise variability. The proposed method is simple and can also be applied to other biomedical signal analysis tasks that involve noise and channel distortion.
V. CONCLUSIONS
This paper has first mathematically analyzed the simultaneous effect of additive and convolutional distortions for heart sound abnormality detection. In the context of PCG signals, we have hypothesized that the additive distortion components represent environmental and other body sounds, whereas convolutional distortion includes sensor variability and transmission channel effects. We have shown that effectively designed features using a combination of filter-bank energy and its natural logarithm can significantly reduce domain variability due to noise and sensor degradation in a heart sound classification task. A residual network architecture has been proposed and evaluated to classify the feature-stream cascaded to form 2D input feature matrices. Experimental results have demonstrated that the proposed method achieves a significant (p < 0.05) absolute improvement of 5.29% in Macc performance metric compared to a competitive baseline system that utilizes a domain-balanced training (DBT) scheme.
REFERENCES
[1] W. H. O. fact sheet 317. (2017, May) Cardiovascular diseases (CVDs). [Online] Available: https://www.who.int/en/news-room/fact-sheets/detail/ cardiovascular-diseases-(cvds). Accessed: September 2019.
[2] Centers for Disease Control (CDC), "Heart disease facts | cdc.gov," https://www.cdc.gov/heartdisease/facts.htm, Sep 2020, (Accessed on 09/13/2020).
[3] S. Leng, R. San Tan, K. T. C. Chai, C. Wang, D. Ghista, and L. Zhong, "The electronic stethoscope," Biomed. Eng. Online, vol. 14, no. 1, pp. 1­ 37, 2015.
[4] E. West, I. McLane, D. McLane, D. Emmanouilidou, M. Elhilali, J. E. West, A. Ward, I. Busch-Vishniac, J. McLane, and B. Dottin-Haley, "Introducing feelix, a digital stethoscope incorporating active noise control and automatic detection of lung sound abnormalities," J. Acoust. Soc. Am., vol. 145, no. 3, pp. 1923­1923, 2019.
[5] I. M. Mclane, D. Emmanouilidou, J. West, and M. Elhilali, "Design and comparative performance of a robust lung auscultation system for noisy clinical settings," IEEE Journal of Biomedical and Health Informatics, 2021.
[6] A. L. Goldberger, L. A. Amaral, L. Glass, J. M. Hausdorff, P. C. Ivanov, R. G. Mark, J. E. Mietus, G. B. Moody, C.-K. Peng, and H. E. Stanley, "Physiobank, physiotoolkit, and physionet: components of a new research resource for complex physiologic signals," Circulation, vol. 101, no. 23, pp. e215­e220, 2000.
[7] B. Schuller, S. Steidl, A. Batliner, E. Bergelson, J. Krajewski, C. Janott, A. Amatuni, M. Casillas, A. Seidl, M. Soderstrom et al., "The interspeech
VOLUME 4, 2016

2017 computational paralinguistics challenge: Addressee, cold & snoring," in Interspeech 2017, 2017, pp. 3442­3446. [8] G. D. Clifford, C. Liu, B. Moody, D. Springer, I. Silva, Q. Li, and R. G. Mark, "Classification of normal/abnormal heart sound recordings: The physionet/computing in cardiology challenge 2016," in 2016 Computing in cardiology conference (CinC). IEEE, 2016, pp. 609­612. [9] C. Liu, D. Springer, Q. Li, B. Moody, R. A. Juan, F. J. Chorro, F. Castells, J. M. Roig, I. Silva, A. E. Johnson et al., "An open access database for the evaluation of heart sound algorithms," Physiol. Meas., vol. 37, no. 12, p. 2181, 2016. [10] M. N. Homsi and P. Warrick, "Ensemble methods with outliers for phonocardiogram classification," Physiol. Meas., vol. 38, no. 8, p. 1631, 2017. [11] J. Rubin, R. Abreu, A. Ganguli, S. Nelaturi, I. Matei, and K. Sricharan, "Classifying heart sound recordings using deep convolutional neural networks and mel-frequency cepstral coefficients," in 2016 Computing in cardiology conference (CinC). IEEE, 2016, pp. 813­816. [12] I. J. D. Bobillo, "A tensor approach to heart sound classification," in Proc. IEEE CinC. IEEE, 2016, pp. 629­632. [13] B. Bozkurt, I. Germanakis, and Y. Stylianou, "A study of time-frequency features for cnn-based automatic heart sound classification for pathology detection," Computers in biology and medicine, vol. 100, pp. 132­143, 2018. [14] F. Noman, C.-M. Ting, S.-H. Salleh, and H. Ombao, "Short-segment heart sound classification using an ensemble of deep convolutional neural networks," in ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019, pp. 1318­1322. [15] Z. Ren, N. Cummins, V. Pandit, J. Han, K. Qian, and B. Schuller, "Learning image-based representations for heart sound classification," in Proceedings of the 2018 International Conference on Digital Health, 2018, pp. 143­147. [16] E. Kay and A. Agarwal, "Dropconnected neural networks trained on timefrequency and inter-beat features for classifying heart sounds," Physiol. Meas., vol. 38, no. 8, p. 1645, 2017. [17] B. M. Whitaker, P. B. Suresha, C. Liu, G. D. Clifford, and D. V. Anderson, "Combining sparse coding and time-domain features for heart sound classification," Physiol. Meas., vol. 38, no. 8, p. 1701, 2017. [18] M. Zabihi, A. B. Rad, S. Kiranyaz, M. Gabbouj, and A. K. Katsaggelos, "Heart sound anomaly and quality detection using ensemble of neural networks without segmentation," in Proc. IEEE CinC. IEEE, 2016, pp. 613­616. [19] C. Potes, S. Parvaneh, A. Rahman, and B. Conroy, "Ensemble of featurebased and deep learning-based classifiers for detection of abnormal heart sounds," in Proc. IEEE CinC. IEEE, 2016, pp. 621­624. [20] A. I. Humayun, S. Ghaffarzadegan, M. I. Ansari, Z. Feng, and T. Hasan, "Towards domain invariant heart sound abnormality detection using learnable filterbanks," IEEE J. Biomed. Health Inform., 2020. [21] V. Maknickas and A. Maknickas, "Recognition of normal­abnormal phonocardiographic signals using deep convolutional neural networks and mel-frequency spectral coefficients," Physiol. Meas., vol. 38, no. 8, p. 1671, 2017. [22] A. I. Humayun, S. Ghaffarzadegan, Z. Feng, and T. Hasan, "Learning front-end filter-bank parameters using convolutional neural networks for abnormal heart sound detection," in 2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). IEEE, 2018, pp. 1408­1411. [23] A. I. Humayun, M. Khan, S. Ghaffarzadegan, Z. Feng, T. Hasan et al., "An ensemble of transfer, semi-supervised and supervised learning methods for pathological heart sound classification," arXiv preprint arXiv:1806.06506, 2018. [24] T.-c. I. Yang and H. Hsieh, "Classification of acoustic physiological signals based on deep learning neural networks with augmented features," in Proc. IEEE CinC. IEEE, 2016, pp. 569­572. [25] M. Deng, T. Meng, J. Cao, S. Wang, J. Zhang, and H. Fan, "Heart sound classification based on improved mfcc features and convolutional recurrent neural networks," Neural Networks, vol. 130, pp. 22­32, 2020. [26] A. V. Oppenheim and R. W. Schafer, "From frequency to quefrency: A history of the cepstrum," IEEE Signal Process. Mag., vol. 21, no. 5, pp. 95­106, 2004. [27] A. Acero, L. Deng, T. Kristjansson, and J. Zhang, "Hmm adaptation using vector taylor series for noisy speech recognition," in Proc. ISCA ICSLP, 2000. [28] V. Stouten, H. Van Hamme, and P. Wambacq, "Joint removal of additive and convolutional noise with model-based feature enhancement," in 2004
VOLUME 4, 2016

IEEE International Conference on Acoustics, Speech, and Signal Processing, vol. 1. IEEE, 2004, pp. I­949. [29] Y. Gong, "A method of joint compensation of additive and convolutive distortions for speaker-independent speech recognition," IEEE transactions on speech and audio processing, vol. 13, no. 5, pp. 975­983, 2005. [30] J. R. Deller, J. G. Proakis, and J. H. Hansen, Discrete-time processing of speech signals. IEEE, 2000. [31] F.-H. Liu, R. M. Stern, X. Huang, and A. Acero, "Efficient cepstral normalization for robust speech recognition," in HUMAN LANGUAGE TECHNOLOGY: Proceedings of a Workshop Held at Plainsboro, New Jersey, March 21-24, 1993, 1993. [32] R. Hyder, S. Ghaffarzadegan, Z. Feng, J. H. Hansen, and T. Hasan, "Acoustic scene classification using a cnn-supervector system trained with auditory and spectrogram image features." in Interspeech, 2017, pp. 3073­ 3077. [33] T. Tran, T. Pham, G. Carneiro, L. Palmer, and I. Reid, "A bayesian data augmentation approach for learning deep models," in Advances in neural information processing systems, 2017, pp. 2797­2806. [34] A. Mikolajczyk and M. Grochowski, "Data augmentation for improving deep learning in image classification problem," in Proc. IEEE IIPhDW. IEEE, 2018, pp. 117­122. [35] D. B. Springer, L. Tarassenko, and G. D. Clifford, "Logistic regressionhsmm-based heart sound segmentation," IEEE. Trans. Biomed. Eng., vol. 63, no. 4, pp. 822­832, 2015. [36] K. He, X. Zhang, S. Ren, and J. Sun, "Deep residual learning for image recognition," in Proc. IEEE CVPR, 2016, pp. 770­778. [37] X. Glorot and Y. Bengio, "Understanding the difficulty of training deep feedforward neural networks," in Proc. AiSTATS, 2010, pp. 249­256. [38] Y. E. Nesterov, "A method for solving the convex programming problem with convergence rate o (1/k^ 2)," in Dokl. akad. nauk Sssr, vol. 269, 1983, pp. 543­547. [39] L. N. Smith, "Cyclical learning rates for training neural networks," in Proc. IEEE WACV. IEEE, 2017, pp. 464­472. [40] J. H. Hansen and T. Hasan, "Speaker recognition by machines and humans: A tutorial review," IEEE Signal Process. Mag., vol. 32, no. 6, pp. 74­99, 2015. [41] T. G. Dietterich, "Approximate statistical tests for comparing supervised classification learning algorithms," Neural Comp., vol. 10, no. 7, pp. 1895­ 1923, 1998. [42] V. Rennoll, I. M. McLane, D. Emmanouilidou, J. West, and M. Elhilali, "Electronic stethoscope filtering mimics the perceived sound characteristics of acoustic stethoscope," IEEE Journal of Biomedical and Health Informatics, 2020.
11

