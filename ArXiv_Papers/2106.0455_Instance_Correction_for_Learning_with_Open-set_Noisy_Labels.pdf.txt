arXiv:2106.00455v1 [cs.LG] 1 Jun 2021

Instance Correction for Learning with Open-set Noisy Labels
Xiaobo Xia1, Tongliang Liu1, Bo Han2, Mingming Gong3, Jun Yu4, Gang Niu5, Masashi Sugiyama5,6
1The University of Sydney; 2Hong Kong Baptist University; 3The University of Melbourne; 4University of Science and Technology of China;
5RIKEN; 6The University of Tokyo
Abstract The problem of open-set noisy labels denotes that part of training data have a di erent label space that does not contain the true class. Lots of approaches, e.g., loss correction and label correction, cannot handle such open-set noisy labels well, since they need training data and test data to share the same label space, which does not hold for learning with open-set noisy labels. The state-of-the-art methods thus employ the sample selection approach to handle open-set noisy labels, which tries to select clean data from noisy data for network parameters updates. The discarded data are seen to be mislabeled and do not participate in training. Such an approach is intuitive and reasonable at rst glance. However, a natural question could be raised "can such data only be discarded during training?". In this paper, we show that the answer is no. Speci cally, we discuss that the instances of discarded data could consist of some meaningful information for generalization. For this reason, we do not abandon such data, but use instance correction to modify the instances of the discarded data, which makes the predictions for the discarded data consistent with given labels. Instance correction are performed by targeted adversarial attacks. The corrected data are then exploited for training to help generalization. In addition to the analytical results, a series of empirical evidences are provided to justify our claims.
1

1 Introduction
Noisy labels are ubiquitous in real-world data, which always arise in mistakes of manual or automatic annotators [17, 16, 45, 47, 41, 29]. Learning with noisy labels can impair the performance of models, especially over-parameterized deep networks which have large learning capacities and strong memorization power [4, 48, 55, 54, 14, 31]. Therefore, it is of great importance to achieve robust training against noisy labels [10, 9, 18, 39, 26].
The types of noisy labels studied so far can be divided into two categories: closed-set and openset noisy labels. Closed-set noisy labels occur when instances have noisy labels that are contained within the known class label set in the training data [37]. On the other hand, open-set noisy labels occur when instances have noisy labels that are not contained within the known class label set in the training data [37]. A large body of work proposed various methods for learning with closed-set noisy labels, such as loss correction [20, 30, 19, 43, 44, 49, 3, 11], label correction [34, 55, 53], and sample selection [9, 13, 50, 25, 36]. In contrast, learning with open-set noisy labels is less explored, which is our focus in this paper.
It is challenging to handle the open-set noisy label problem. Prior e ects on learning with open-set noisy labels concentrated on the sample selection approach [37, 50]. The reason for this is straightforward. That is to say, for open-set noisy labels, both loss correction and label correction are unreachable since the true class label is unknowable for some training data [37]. Consequently, prior e ects have exploited sample selection to lter out mislabeled examples, and have used selected "clean" examples for robust training. The ltered examples are regarded as useless and discarded directly during the training procedure.
Such a way of combating open-set noisy labels is intuitive but arguably suboptimal, as the discarded data may contain meaningful information for generalization. For instance, let us consider the annotation process on the crowdsourcing platform [46, 52], where the label space has been de ned by experts. In this annotation process, the label "dog" is within the de ned label space, but the label "wolf" is outside the de ned label space. Given an image, the annotators need to select one class label from the de ned label space and assign it to this image. When there is an image of the wolf needed to be annotated, yet the label "wolf" is not within the label space, we have to select the label "dog" and assign it to the image of the wolf. The reason for this is that the semantic part label [44] of the wolf image can be seen as "dog", since the wolf and dog have many similar features, e.g., ears, eyes, and legs. Such features are termed robust features, which are comprehensible to we humans [12]. Although the wolf image is mislabeled as "dog", robust features still are meaningful information for generalization. However, prior sample selection methods [50, 37, 36] directly discard them.
In this paper, to relieve the issue of ignoring meaningful information of discarded data in learning with open-set noisy labels, we perform instance correction on discarded data to make use of them. Speci cally, we employ targeted adversarial attacks [24] which attempt to change the output classi cation of the input to a special target class. For the discarded data, the adversarial targeted attacks make their instances match given labels actively. In this way, we can achieve instance correction to combat open-set noisy labels. The illustration of our method is provided in Figure 1.
The proposed instance correction is motivated by the fact that the features of an instance can be divided into robust features and non-robust features [12]. Compared with robust features which have been discussed, non-robust features are brittle and incomprehensible to we humans. Training
2

Figure 1: The illustration of the algorithm ow of the proposed method. When learning with open-set noisy labels, we rst exploit the sample selection approach to distinguish the clean example "dog" and mislabeled example "wolf". Then we employ targeted adversarial attacks to perform instance correction, which reserves robust features of the wolf and changes non-robust features of the wolf to those of the dog. Finally, we utilize both the clean example and corrected example for training with the supervision of the label "dog".
with robust or non-robust features can yield great test accuracy [12]. Besides, non-robust features are easier to be changed with adversarial attacks than the robust ones [12]. For the instances with labels that are not contained within the known class label set in the training data, they may have some robust features like those in clean data to help generalization as discussed. However, they may have di erent non-robust features compared with clean ones, which do not help generalization. Therefore, we use targeted adversarial attacks to change such non-robust features to match given labels.
Before delving into details, we summarize the main contributions of this paper in two folds. First, we identify the issues of prior approaches on learning with open-set noisy labels and propose instance correction to relieve the issue of ignoring meaningful information in discarded data. Second, we conduct a series of experiments to justify our claims well. The rest of this paper is organized as follows. In Section 2, we introduce the background of learning with open-set noisy labels. In Section 3, we present how to make use of discarded data to enhance deep networks step by step. Experimental results are provided in Section 4. Finally, we conclude the paper in Section 5.
2 Learning with Open-set Noisy Labels
Notations. Vectors and matrices are denoted by bold-faced letters. We use · p as the p norm of vectors or matrices. Let [z] = {1, 2, . . . , z}.
Preliminaries. Consider a classi cation task, where there are c classes in total. Let X and Y
R be the instance space and label space respectively, where X  d with d being the dimensionality,
and Y = [c]. In traditional supervised learning, the given training labels are clean. By employing such a clean dataset S = {(xi, yi)}ni=1, where n denotes the sample size or the number of training data, our goal is to learn a classi er that can assign labels precisely for given instances. However, in many real-world applications, the clean labels cannot be observed and the observed labels are
3

noisy. Consider a subset of S (denoted by S ) is corrupted and denote the corrupted version of the subset S by S . Let Y be the label space of S (with Y  Y = )--this means that the instances in S no longer have labels in Y. Therefore, the whole noisy dataset can be denoted by S~ = (S\S )  S . The aim is changed to learn a robust classi er that could assign clean labels to test data by exploiting the noisy dataset S~.
Here, we provide an intuitive example for better understanding of the above notations (i.e., S, S , S , and S~) and problem setting, which is shown in Figure 2. We exploit the images of CIFAR-10 [15] and SVHN [27]. More speci cally, the clean examples come from CIFAR-10, and the mislabeled examples (i.e., the examples in the set S ) come from SVHN. In the generation process of open-set noisy labels, part of images of CIFAR-10 are corrupted by the images of SVHN. Besides, the examples of CIFAR-10 and SVHN do not have the same label space. For this reason, we only have a noisy sample set, i.e. S~, and need to learn a robust classi er that can assign labels to test data.
Figure 2: The illustration of the used notations and problem setting.
Limitations of loss/label correction approaches. The methods of loss correction and label correction cannot handle noisy labels well [37]. Here, we discuss the issues in more detail. For the loss correction approach, we need to model the ip processes from clean labels to noisy labels [43, 49, 18, 8, 32, 20], which are within the class label set of training data. When learning with open-set noisy labels, some instances do not have clean class labels which are within the class label set of training data. Therefore, when we tend to use the loss correction approach to handle open-set noisy labels, we will mistakenly assign labels to mislabeled data. If this happens, the correction of the training losses will be incorrect, following bad classi cation performance.
For the label correction approach [34, 55, 53], we need to recalibrate the labels of mislabeled data based on the predictions of classi ers. As the classi er is trained only with the known class label set, it assigns labels to mislabeled data from this set. Apparently, such an assignment is incorrect, since the true labels of the mislabeled data are outside the known class label set. As a result, we still train the classi er on the dataset which contains incorrect labels. Hence, the obtained classi er cannot be robust under this circumstance.
Shortcomings of the sample selection approach. Prior e ects exploited sample selection to handle open-set noisy labels [37, 50], which only used the "clean" examples (with relatively small losses) from each mini-batch for training. Such approaches inherit the memorization e ects of deep networks [1], which show that they would rst memorize training data with clean labels and then those with noisy labels under the assumption that clean labels are of the majority in a noisy class. We use the self-teach model [13, 48] as a representative example. The procedure is shown in Algorithm 1. Let f be the classi er with learnable parameters. At the t-th iteration, when a mini-batch S¯ is formed (Step 5), a subset of small-loss examples S¯f is selected from the minibatch S¯ (Step 6). The size of S¯f is determined by R(T ), which always depends on the noise rate
4

Algorithm 1 General procedure on using sample selection to combat open-set noisy labels.

1: Input: initialized classi er f , xed  , epoch Tk and Tmax, iteration tmax.

2: for T = 0, . . . , Tmax - 1 do 3: Shu le training set S~;

4: for t = 0, . . . , tmax - 1 do

5:

Draw a mini-batch S¯ from S~;

6:

Select R(T ) small-loss examples S¯f from S¯ based on classi er's predictions;

7:

Update classi er parameter only using S¯f ;

8: end for

9:

Update

R(T

)

=

1

-

min{

T Tk



,



};

10: end for

11: Output: trained classi er f .

 [9]. Note that in [9], the function R(T ) is designed as 1 - min{T /Tk  ,  }. Such a design can help us make better use of the memorization e ects of deep networks for sample selection. More speci cally, deep networks will learn clean and easy pattern in the initial epochs [1]. Therefore, they have the ability to lter out mislabeled examples using their loss values at the beginning of training. When the number of epochs goes large, they will eventually over t to noisy labels [9]. For this reason, at the start of training, we set R(T ) to a large value. Then, we gradually increase the drop rate, i.e., reduce the value of R(T ).
The selected "clean" examples are then used to update the network parameters in Step 7. Those "mislabeled" examples are discarded directly and do not participate in training. To the end, since we select less noisy data for parameters updates, the classi er f will be more robust. Note that in Step 6, we select small-loss examples S¯f from S¯ for parameter updates, where the examples in S¯\S¯f are seen to be useless and discarded from training. Nevertheless, as discussed, such data could consist of meaningful information for generalization. It thus is not a great choice to abandon them directly during training, which makes the sample selection approach achieve sub-optimal performance. We discuss how to address the issue using the proposed method in next section.
3 Method
In this section, we introduce the proposed method in more detail. Recall Algorithm 1, the set of discarded data in each mini-batch can be denoted by S¯\S¯f . We focus on how to make use of the training data in S¯\S¯f to help generalization.
Recent work [12] showed that adversarial examples can be directly attributed to the presence of non-robust features, which are derived from patterns in the data distribution and highly predictive. Note that non-robustness does not mean that the model is brittle, but part of features of images are easy to be changed via adversarial attacks. Also, adversarial attacks mainly change the non-robust features [12]. Although non-robust features may be incomprehensible, they still can be used to achieve great classi cation performance. Therefore, for our method, it is reasonable to generalize well with corrected examples which are obtained by target adversarial attacks. We therefore can exploit targeted adversarial attacks on discarded data during training.
R R Let : c × Y  + be a surrogate loss function for c-class classi cation. The loss function
can be used to measure the degree of certainty that f classi es the input to speci c classes. In this
5

paper, we exploit the softmax cross entropy loss. Given a discarded example (xi, y~i)  S¯\S¯f , we construct an adversarial example from a benign example xi by adding a perturbation vector xi by solving the following optimization problem:

min
xi

(f (xi + xi), y~i) subject to

xi

2 <  and f (xi + xi) = y~i,

(1)

where  is the budget of targeted adversarial attacks [2, 40, 21, 38]. When the classi er f outputs
probabilities pf (·|xi) associated with each class, the used adversarial loss is (f (xi + xi), y~i) = -pf (y~i|xi + xi), which essentially maximizes the probability of classi cation into the class y~i. Here, we denote the corrected instance via targeted adversarial attacks by x~i, i.e., x~i = xi + xi. The set that includes the pairs of the corrected instances and given labels, i.e., (x~j, y~j), is denoted as S¯p.
In this way, although the clean label of the instance xi is not within the class label set of the training data, the clean label of the corrected instance x~i can be seen as the given label y~i from
the perspective of classi er outputs. As such corrected instances can be highly predictive and
help generalization, after targeted adversarial attacks, both the corrected examples and reserved examples in S¯f are employed for network parameter updates. In this work, we seek a balance between corrected examples and reserved examples during training. Our objective function is
given as follows:

min L =   (xi,y~i)S¯f (f (xi), y~i) + (1 - )  (x~j,y~j)S¯p (f (x~j ), y~j ),

(2)

where   0 is a hyperparameter to balance the contributions of corrected examples and reserved
examples for training. More discussions about this hyperparameter are provided in Section 4.
Overall procedure. We summarize the overall procedure of the proposed method based on the self-teach model [13, 48]. Note that the proposed method can also be added to the methods
which exploit two networks for sample selection, e.g., Co-teaching [9, 50]. We employ the self-
teaching model as a representative example to be compared with Algorithm 1 more intuitively.
We summarize the overall procedure of the proposed method in Algorithm 2. Speci cally, we rst
use the sample selection approach to obtain an initialized classi er (Step 3). Then we divide all training examples into a clean set S¯f and a mislabeled set S\S¯f (Step 5). After this, we perform instance correction on the instances in S\S¯f by using targeted adversarial attacks to make them match given labels and obtain the set S¯p (Step 6). The new training example set which consists of both the corrected examples and selected examples is obtained for parameter updates (Step 8).
Comparing Algorithm 1 with Algorithm 2 further, we can see that the proposed method are simply implemented based on the prior sample selection procedure. Note that the hyperparameter Tc can be determined by using a noisy validation set. Although the noisy validation set cannot perform
as good as the clean one, it does not introduce the assumption that some clean data are available,
which is more realistic [28].

4 Experiments
In this section, we justify our claims from two folds. First, we conduct experiments on the benchmark dataset with synthetic open-set noisy labels (Section 4.1). Second, we conduct experiments on the real-world dataset which contains open-set noisy labels (Section 4.2).

6

Algorithm 2 Overll procedure of the proposed method. 1: Input: initialized classi er f , xed  , epoch Tc and Tmax. 2: for T = 0, . . . , Tc - 1 do 3: Use the sample selection approach as did in Algorithm 1; 4: end for 5: Divide all training examples into clean and mislabeled examples using predictions of f ; 6: Perform instance correction for the mislabeled examples; 7: for T = Tc, . . . , Tmax - 1 do 8: Exploit both clean and corrected examples for training as did in Eq. (2); 9: end for 10: Output: trained classi er f .
4.1 Experiments on the benchmark dataset
Dataset. CIFAR-10 [15] is employed to verify the e ectiveness of the proposed method, which is popularly used for evaluation of noisy labels in the literature [37, 42, 50, 35, 23, 22]. CIFAR-10 has 10 classes of color images including 50,000 training images and 10,000 test images. The size of color images is 32×32.
Open-set noisy label generation. Following [37], open-set noisy datasets are built by replacing some training images in CIFAR-10 by outside images, while keeping the labels and the number of images per class unchanged. We consider two types of noise, i.e., Type I and Type II noise. Type I noise includes images from SVHN [27], CIFAR-100 [15], and ImageNet32 (32×32 ImageNet images) [5], and only those images whose labels exclude the 10 classes in CIFAR-10 are considered. Type II noise includes images damaged by Gaussian random noise, corruption, resolution distortion, fog distortion, and motion blur distortion. Some examples of the type I and type II open-set noisy labels are given in Figure 3. The overall noise rate  is set to 20%, 40%, 60%, and 80%. We leave out 10% of the noisy training data as a validation set for model selection.
Figure 3: Illustrations of examples of the noise injected to CIFAR-10. Baselines. We compare the proposed method with the state-of-the-arts and the most related techniques for learning with open-set noisy labels: (1) Forward [30], which estimates the noise transition matrix to combat noisy labels. (2) Joint [34], which jointly optimizes the labels of examples and the network parameters for label correction. (3) SIGUA [10], which exploits stochastic integrated gradient underweighted ascent to handle noisy labels. We use self-teach SIGUA in this paper. (4) MentorNet [13], which learns a curriculum to lter out noisy data. We used self-teach MentorNet in this paper. (5) S2E [48], which exploits the AutoML technology to handle noisy labels. Note that the baseline (1) belongs to the loss correction approach. The baselines (2) belong to the label correction approach. Besides, the baselines (3), (4), and (5) belong the sample selection
7

approach.

Besides, we add a comparison called Mix, which directly involves discarded data into training

without instance correction. Mix is based on the initialization of S2E. Our method, i.e., instance

correction, is denoted as InsCorr in experiments. Since the proposed method is built on the sample

selection approach, i.e., MentorNet (M) and S2E (S), we denote the proposed method as M-InsCorr

and S-InsCorr respectively. Also, all methods Mix, M-InsCorr, and S-InsCorr are proposed in this

paper, as prior works on sample selection do not make use of discarded data during training. We

mark our methods with a symbol . Note that we do not compare our method with some state-

of-the-art methods, e.g., SELF [28] and DivideMix [16]. It is because their proposed methods are

aggregations of multiple techniques. The comparison with our method is thus not fair.

Experimental setup. For the fair comparison, we implement all methods with default pa-

rameters by PyTorch, and conduct all the experiments on NVIDIA Tesla V100 GPUs. A 7-layer

CNN architecture is exploited following [37, 50], which is standard test bed for weakly-supervised

learning. The network architecture consists of 6 convolutional layers and 1 fully-connected layer.

Batch normalization is applied in each convolutional layer before the ReLU activation, and a max-

pooling layer is implemented every two convolutional layers. For all experiments, Adam optimizer

(momentum=0.9) is with an initial learning rate of 0.001, and the batch size is set to 128. We run

200 epochs in total. As for targeted adversarial attacks, we employ targeted LinfPGD-Attacks [24]

with the budget 8/255 and borrow the implementation of the Advertorch toolbox [7]1.

For

MentorNet

and

M-InsCorr,

we

set

R(T )

=

1

-

min{

T Tk

,

 }.

Here,

we

set

Tk

to

10

as

did in [50]. If the noise rate  is not known in advanced, it can be inferred using validation sets

[19, 51]. For S2E and S-InsCorr, we set R(T ) as did in [48], and exploit the o cial code 2. Note

that R(T ) only depends on the memorization e ect of deep networks but not any speci c datasets

[9]. As for the performance measurement, we use the test accuracy, i.e., test accuracy = (# of correct

prediction) / (# of testing). All experiments on CIFAR-10 are repeated ve times. We report the

mean and standard deviation of experimental results. Intuitively, if a method can achieve higher

classi cation performance, it can better handle open-set noisy labels.

4.1.1 Analyses of experimental results
Overall results. The experimental results with Type I and Type II open-set noise are provided in Table 1 and 2 respectively. With Type I open-set noise, the sample selection approach (e.g., MentorNet) always outperforms other types of approaches. When the noise level is high, the advantage is very obvious. For example, when the noise rate is 80%, MentorNet always achieves a lead of more than 10% over Joint. In addition, MentorNet achieves a lead of more than 20% over Forward. The experimental results show the superiority of the sample selection approach in combating open-set noisy labels.
With Type II open-set noise, the sample selection approach S2E achieves the best classi cation performance in most cases. When the noise level is high, the performance of another sample selection approach MentorNet is not promising. The reason is that MentorNet exploits a xed sample selection procedure, i.e., R(T ). Also, when the noise level is high, it easily chooses the mislabeled examples, since the mislabeled ones are more similar with original ones in Type II open-set noise. What's worse, once MentorNet makes the wrong choice during training, the errors will be accumu-
1The o cial repository of Advertorch: https://github.com/borealisai/advertorch 2The o cial repository of S2E: https://github.com/AutoML-4Paradigm/S2E

8

Noise setting 20%
C+S 40% 60% 80% 20%
C+C 40% 60% 80% 20%
C+I 40% 60% 80%

Forward 74.26 ±0.49 71.37 ±0.52 53.83 ±2.37 36.25 ±0.84 76.72 ±0.93 72.07 ±1.39 54.95 ±1.06 32.93 ±2.44 77.12 ±0.38 70.74 ±1.17 60.30 ±2.56 35.60 ±1.49

Joint 79.11 ±1.53 74.71 ±1.78 68.09 ±1.31 39.06 ±3.12 75.37 ±1.47 67.30 ±0.39 52.58 ±4.62 42.95 ±1.29 76.79 ±1.36 65.76 ±2.64 57.62 ±1.50 37.09 ±4.06

SIGUA 70.88 ±2.33 61.57 ±2.00 54.71 ±2.04 20.77 ±2.16 74.27 ±0.16 68.08 ±0.29 53.45 ±1.46 26.66 ±4.37 72.16 ±0.92 70.93 ±1.40 59.54 ±1.65 20.58 ±0.96

Mix 80.68 ±0.19 75.71 ±0.56 60.47 ±0.44 59.61 ±0.64 77.79 ±0.27 68.79 ±0.26 58.89 ±0.07 40.41 ±0.89 80.66 ±0.21 73.47 ±1.01 60.63 ±0.47 40.09 ±3.61

MentorNet 82.77 ±0.30 79.67 ±0.21 72.68 ±3.95 64.90 ±2.19
82.14 ±0.15 79.17 ±0.15 74.58 ±0.31 57.53 ±0.23 82.15 ±0.77 78.60 ±0.46 73.06 ±2.31 58.06 ±3.76

M-InsCorr 82.81 ±0.60 80.03 ±0.19 72.05 ±0.62 62.35 ±0.32 81.64 ±0.52 79.65 ±1.03 74.86 ±0.28 57.73 ±0.93
82.47 ±0.06 79.05 ±0.15 71.33 ±0.62 56.24 ±2.24

S2E 82.32 ±0.80 78.85 ±1.86 73.82 ±2.69 63.03 ±1.11 80.34 ±0.79 73.47 ±5.60 60.77 ±7.46 42.73 ±7.61 80.25 ±1.02 77.33 ±2.72 64.09 ±2.88 46.95 ±2.62

S-InsCorr 82.58 ±1.09 79.16 ±1.35 73.72 ±1.19 62.36 ±1.55 80.02 ±0.90 74.52 ±2.73 62.09 ±3.43 46.24 ±2.65 80.74 ±0.91 77.87 ±2.24 64.01 ±2.60 49.62 ±3.04

Table 1: Mean and standard deviation of test accuracy (%) of di erent methods for learning with Type I open-set noisy labels over last ten epochs. "C+S" is the abbreviation of CIFAR10+SVHN. "C+C" is the abbreviation of CIFAR-10+CIFAR-100. "C+S" is the abbreviation of CIFAR10+ImageNet32. The methods with the mark are our methods. The best results are in bold.
lated, which seriously hurts generalization [50]. By contrast, S2E exploits the AutoML technique to control the sample selection procedure, which better exploits the memorization e ects of deep networks, following better classi cation performance than MentorNet. Although SIGUA also focuses on sample selection, it uses stochastic integrated gradient underweighted ascent to handle open-set noisy labels, which highly relies on suitable selection of hyperparameters. Therefore, SIGUA does not achieve competitive performance with other sample selection methods.
Mix vs. loss/label correction. As shown in Table 1, on C+S and C+C, Mix can outperform Forward and Joint in most cases. On C+I, Mix always achieves better classi cation performance than Forward and Joint. Such experimental results show the vulnerability of the loss and label correction approaches to handle open-set noisy labels. Similarly, as shown in Table 2, Mix can obtain better results compared with Forward and Joint in most circumstances.
InsCorr vs. sample selection. We rst analyze the results with Type I open-set noise. In the most cases, the proposed instance correction can bring positive e ects for generalization, which show the e ectiveness of the proposed method. In some cases, instance correction cannot bring better performance. The reason is that Type I open-set noise is generated too randomly or even groundlessly. Therefore, the mislabeled examples consist of too many distinct robust features, which make it di cult to correct them by the proposed method. For instance, the images of CIFAR-

9

Noise setting 20%
C+G 40% 60% 80% 20%
C+O 40% 60% 80% 20%
C+R 40% 60% 80% 20%
C+F 40% 60% 80% 20%
C+M 40% 60% 80%

Forward 74.62 ±0.88 71.45 ±1.17 60.62 ±1.86 33.06 ±1.78 75.79 ±1.05 75.44 ±0.93 64.05 ±0.87 47.92 ±2.37 77.46 ±0.94 74.09 ±1.88 65.63 ±1.05 41.65 ±3.93 75.45 ±0.93 75.10 ±1.52 66.08 ±0.83 53.22 ±1.17 77.79 ±0.75 75.13 ±0.68 64.75 ±1.62 60.40 ±1.08

Joint 79.00 ±0.84 75.11 ±0.12 70.30 ±0.42 53.17 ±3.94 78.93 ±1.36 75.82 ±1.25 74.36 ±0.63 67.32 ±0.88 82.22 ±0.50 78.80 ±1.56 75.39 ±2.31 52.65 ±1.79 81.18 ±0.44 78.91 ±0.56 73.37 ±0.82 68.06 ±1.78 81.36 ±0.32 78.24 ±1.05 75.36 ±0.61 64.39 ±2.82

SIGUA 75.97 ±0.21 60.88 ±0.15 37.69 ±5.62 35.66 ±1.92 66.33 ±0.31 60.08 ±0.83 30.08 ±1.00 28.75 ±1.06 66.19 ±0.22 57.88 ±0.58 28.84 ±1.86 20.06 ±3.43 70.09 ±0.43 62.39 ±0.70 40.24 ±0.95 38.62 ±0.73 68.50 ±0.23 61.05 ±0.90 33.44 ±2.60 28.77 ±1.06

Mix 80.33 ±0.65 75.15 ±0.35 59.84 ±1.10 53.98 ±1.77 82.41 ±0.49 80.17 ±0.02 77.84 ±0.27 69.54 ±0.25 82.48 ±0.49 79.00 ±0.16 76.04 ±0.60 65.19 ±1.87 83.48 ±0.24 81.05 ±0.24 78.18 ±0.14 69.89 ±0.73 81.45 ±0.21 78.56 ±0.19 75.13 ±0.24 68.58 ±0.64

MentorNet 80.56 ±0.20 73.35 ±0.38 56.93 ±0.68 27.86 ±1.30 81.81 ±0.31 75.16 ±0.37 57.09 ±0.60 29.75 ±1.64 82.58 ±0.12 74.43 ±0.25 56.91 ±0.84 29.62 ±0.77 82.70 ±0.23 81.84 ±0.54 76.72 ±0.32 60.25 ±0.31 82.61 ±0.42 74.71 ±0.41 57.34 ±2.38 28.65 ±1.62

M-InsCorr 81.17 ±0.43 76.32 ±0.27 64.64 ±0.71 48.44 ±3.05
82.55 ±0.35 77.51 ±0.09 72.00 ±0.68 64.75 ±0.91
82.91 ±0.13 79.07 ±0.12 72.51 ±0.79 62.91 ±1.15 82.81 ±0.92 82.03 ±0.09 77.23 ±0.52 61.02 ±0.39
82.66 ±0.14 77.30 ±0.92 70.06 ±0.49 61.72 ±0.66

S2E 79.53 ±2.14 76.06 ±1.52 71.80 ±1.46 62.74 ±2.83 79.47 ±2.96 79.21 ±2.41 77.73 ±1.80 70.26 ±1.34 79.18 ±1.41 78.75 ±2.65 76.65 ±2.24 70.73 ±2.84 83.15 ±1.34 82.83 ±0.23 78.76 ±0.17 70.55 ±0.54 79.98 ±1.32 78.58 ±2.09 76.34 ±1.85 69.36 ±1.25

S-InsCorr 79.80 ±1.35 76.33 ±0.32 72.25 ±1.83 63.50 ±2.11 79.65 ±2.36 79.44 ±2.63 77.90 ±1.08 71.05 ±0.30 79.77 ±0.69 79.07 ±2.39 77.25 ±2.21 71.31 ±2.12
83.52 ±1.37 82.87 ±0.35 79.03 ±1.09 71.19 ±0.67 80.16 ±1.11 79.30 ±1.77 76.49 ±2.68 71.02 ±0.63

Table 2: Mean and standard deviation of test accuracy (%) of di erent methods for learning with Type II open-set noisy labels over last ten epochs. "C+G" is the abbreviation of CIFAR10+Gaussian. "C+O" is the abbreviation of CIFAR-10+Corruption. "C+R" is the abbreviation of CIFAR-10+Resolution. "C+F" is the abbreviation of CIFAR-10+Fog. "C+M" is the abbreviation of CIFAR-10+Motion Blur. The methods with the mark are our methods. The best results are in bold.

10

­ Type I ­ Test Accuracy (%)

C+S

C+C

C+I

80

80

80

Test Accuracy (%)

Test Accuracy (%)

75

70

70

70

65

60

60

60

50

50

20%

55

40%

60%

40

50

80%

40

0.05

0.10

0.15

0.20

0.25

0.30

0.05

0.10

0.15

0.20

0.25

0.30

0.05

0.10

0.15

0.20

0.25

0.30

C+O

C+R

C+F

Test Accuracy (%)

Test Accuracy (%)

82

84

80

80

82

80

78

75

78

75

70

76

72

74

70

20% 40%

65

68

60% 80%

60

72 70 68

0.05

0.10

0.15

0.20

0.25

0.30

0.05

0.10

0.15

0.20

0.25

0.30

0.05

0.10

0.15

0.20

0.25

0.30

­ Type II ­ Test Accuracy (%)

Figure 4: Illustrations of the test accuracy of the method Mix with di erent . The error bar for standard deviation in each gure has been shaded.

10 is mainly about the objects in our daily life, e.g., cats, dogs, and frogs. However, the images of SVHN is about the house numbers in Google Street View imagery. The images in two datasets are much di erent, which causes that instance correction sometimes does not work well.
We then analyze the results with Type II open-set noise, where the noise is arguably generated more reasonably. In such cases, instance correction can change the non-robust features of mislabeled examples to match given labels, which brings better classi cation performance. The results justify our claims well, i.e., the discard data may have meaningful information, and can be used for training.
4.1.2 Ablation study
We conduct an ablation study to analyze the in uence of the hyperparameter  and show the impact of the discarded data. The experiments are conducted on the Type I noise datasets (i.e., C+S, C+C, and C+I ) and the Type II noise datasets (i.e., C+O, C+R, and C+F). The experimental settings such as the network architecture and optimizer have been introduced before. The value of  is chosen from the range {0.05, 0.10, 0.15, 0.20, 0.25, 0.30}. The illustrations of results are provided in Figure 4.
The discarded data have di erent in uences in the experiments with Type I and Type II openset noise. More speci cally, with Type I open-set noise, we can see that the test accuracy decreases if we increase the the value of  in almost every case. Such results mean that the mislabeled examples in Type I cannot be directly involved into training, since they would hurt generalization. We need to rst correct them and then can utilize them. As a comparison, with Type II openset noise, the curve of test accuracy is not monotonically decreasing with the increase of . The results mean that the discarded data have positive e ects to help generalization. Hence, we can make use of them appropriately. Note that such di erences of results lie in the di erences between the two noise generation mechanisms, which also show that the direct use of discarded data may be invalid.

11

4.2 Experiments on the real-world dataset
Experimental setup. The WebVision [17] dataset is employ in this paper. WebVision contains 2.4 million images crawled from the websites using the 1,000 concepts in ImageNet ILSVRC12 [6]. Following the "Mini" setting in [13, 3, 23], we take the rst 50 classes of the Google resized image subset, and evaluate the trained networks on the same 50 classes of the ILSVRC12 validation set, which is exploited as a test set. Following prior works [30, 42], we leave out 10% training data as a validation set, which is for model selection. For WebVision, we use an Inception-ResNet v2 [33] with batch size 128. The initial learning rate is set to 10-1. We set 100 epochs in total for all the experiments on the real-world dataset.

Forward Joint SIGUA Mix MentorNet M-InsCorr S2E S-InsCorr

56.39 47.60 40.35 54.39 57.66

57.89 57.05 57.75

Table 3: Top-1 validation accuracies (%) on clean ILSVRC12 validation set of Inception-ResNet v2 models trained on WebVision, under the "Mini" setting in [13, 3, 23]. The best result is in bold.

Experimental results. The experimental results are provided in Table 3. Comparing MInsCorr with MentorNet, we achieve an improvement of +0.23%. Besides, comparing S-InsCorr with S2E, we achieve an improvement of +0.70%. Note that directly involving discarded data can outperform the baselines such as Joint clearly, which means this type of approach is somewhat vulnerable in the practical problem. The results on the real-world dataset mean that the proposed method can handle classi cation tasks well in actual scenarios.

5 Conclusion
In this paper, we focus on the problem of learning with open-set noisy labels, where part of training data have a di erent label space that does not contain the true class. We rst point out the weaknesses of the approaches such as loss correction, label correction, and sample selection for handling open-set noisy labels. Then we propose to use instance correction which is performed by targeted adversarial attacks to corrected the instances of discarded data to utilize them for training. A series of experimental results justify our claims well and verify the e ectiveness of the proposed method. We believe that this paper opens up new possibilities in the topic of learning with open-set noisy labels. In the future, we will explore more ways to make use of discarded data and investigate more manners to perform instance correction to improve the robustness against open-set noisy labels.

Acknowledgement
TL was supported by Australian Research Council Project DE-190101473. BH was supported by the RGC Early Career Scheme No. 22200720 and NSFC Young Scientists Fund No. 62006202. JY was supported by USTC Research Funds of the Double First-Class Initiative (YD2350002001). GN was supported by JST AIP Acceleration Research Grant Number JPMJCR20U3, Japan. MS was supported by JST CREST Grant Number JPMJCR18A2, Japan.

12

References
[1] Devansh Arpit, Stanislaw Jastrzbski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look at memorization in deep networks. arXiv preprint arXiv:1706.05394, 2017.
[2] Yang Bai, Yuyuan Zeng, Yong Jiang, Shu-Tao Xia, Xingjun Ma, and Yisen Wang. Improving adversarial robustness via channel-wise activation suppressing. arXiv preprint arXiv:2103.08307, 2021.
[3] Pengfei Chen, Ben Ben Liao, Guangyong Chen, and Shengyu Zhang. Understanding and utilizing deep neural networks trained with noisy labels. In ICML, pages 1062­1070, 2019.
[4] Jiacheng Cheng, Tongliang Liu, Kotagiri Ramamohanarao, and Dacheng Tao. Learning with bounded instance-and label-dependent label noise. In ICML, 2020.
[5] Patryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter. A downsampled variant of imagenet as an alternative to the cifar datasets. arXiv preprint arXiv:1707.08819, 2017.
[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, pages 248­255. Ieee, 2009.
[7] Gavin Weiguang Ding, Luyu Wang, and Xiaomeng Jin. AdverTorch v0.1: An adversarial robustness toolbox based on pytorch. arXiv preprint arXiv:1902.07623, 2019.
[8] Bo Han, Jiangchao Yao, Gang Niu, Mingyuan Zhou, Ivor Tsang, Ya Zhang, and Masashi Sugiyama. Masking: A new perspective of noisy supervision. In NeurIPS, pages 5836­5846, 2018.
[9] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In NeurIPS, pages 8527­8537, 2018.
[10] Bo Han, Gang Niu, Xingrui Yu, Quanming Yao, Miao Xu, Ivor W Tsang, and Masashi Sugiyama. Sigua: Forgetting may make learning with noisy labels more robust. In ICML, 2020.
[11] Dan Hendrycks, Mantas Mazeika, Duncan Wilson, and Kevin Gimpel. Using trusted data to train deep networks on labels corrupted by severe noise. In NeurIPS, pages 10456­10465. 2018.
[12] Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander Madry. Adversarial examples are not bugs, they are features. arXiv preprint arXiv:1905.02175, 2019.
[13] Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. MentorNet: Learning data-driven curriculum for very deep neural networks on corrupted labels. In ICML, pages 2309­2318, 2018.
13

[14] Lu Jiang, Di Huang, Mason Liu, and Weilong Yang. Beyond synthetic noise: Deep learning on controlled noisy labels. In ICML, pages 4804­4815, 2020.
[15] Alex Krizhevsky and Geo rey Hinton. Learning multiple layers of features from tiny images. Technical report, 2009.
[16] Junnan Li, Richard Socher, and Steven C.H. Hoi. Dividemix: Learning with noisy labels as semi-supervised learning. In ICLR, 2020.
[17] Wen Li, Limin Wang, Li Wei, Eirikur Agustsson, and Luc Van Gool. Webvision database: Visual learning and understanding from web data. arXiv preprint arXiv:1708.02862, 2017.
[18] Xuefeng Li, Tongliang Liu, Bo Han, Gang Niu, and Masashi Sugiyama. Provably end-to-end label-noise learning without anchor points. In ICML, 2021.
[19] Tongliang Liu and Dacheng Tao. Classi cation with noisy labels by importance reweighting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38(3):447­461, 2016.
[20] Yang Liu and Hongyi Guo. Peer loss functions: Learning from noisy labels without knowing noise rates. In ICML, 2020.
[21] Xingjun Ma, Bo Li, Yisen Wang, Sarah M Erfani, Sudanthi Wijewickrema, Grant Schoenebeck, Dawn Song, Michael E Houle, and James Bailey. Characterizing adversarial subspaces using local intrinsic dimensionality. arXiv preprint arXiv:1801.02613, 2018.
[22] Xingjun Ma, Yisen Wang, Michael E Houle, Shuo Zhou, Sarah M Erfani, Shu-Tao Xia, Sudanthi Wijewickrema, and James Bailey. Dimensionality-driven learning with noisy labels. In ICML, pages 3361­3370, 2018.
[23] Xingjun Ma, Hanxun Huang, Yisen Wang, Simone Romano, Sarah Erfani, and James Bailey. Normalized loss functions for deep learning with noisy labels. In ICML, 2020.
[24] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In ICLR, 2018.
[25] Eran Malach and Shai Shalev-Shwartz. Decoupling" when to update" from" how to update". In NeurIPS, pages 960­970, 2017.
[26] Aditya Krishna Menon, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv Kumar. Can gradient clipping mitigate label noise? In ICLR, 2020.
[27] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y.Ng. Reading digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011.
[28] Duc Tam Nguyen, Chaithanya Kumar Mummadi, Thi Phuong Nhung Ngo, Thi Hoai Phuong Nguyen, Laura Beggel, and Thomas Brox. Self: Learning to lter noisy labels with selfensembling. In ICLR, 2020.
[29] Curtis G Northcutt, Lu Jiang, and Isaac L Chuang. Con dent learning: Estimating uncertainty in dataset labels. Journal of Arti cial Intelligence Research, 2021.
14

[30] Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making deep neural networks robust to label noise: A loss correction approach. In CVPR, pages 1944­1952, 2017.
[31] Geo Pleiss, Tianyi Zhang, Ethan R Elenberg, and Kilian Q Weinberger. Identifying mislabeled data using the area under the margin ranking. arXiv preprint arXiv:2001.10528, 2020.
[32] Jun Shu, Qian Zhao, Zengben Xu, and Deyu Meng. Meta transition adaptation for robust deep learning with noisy labels. arXiv preprint arXiv:2006.05697, 2020.
[33] Christian Szegedy, Sergey Io e, Vincent Vanhoucke, and Alex Alemi. Inception-v4, inception-resnet and the impact of residual connections on learning. arXiv preprint arXiv:1602.07261, 2016.
[34] Daiki Tanaka, Daiki Ikami, Toshihiko Yamasaki, and Kiyoharu Aizawa. Joint optimization framework for learning with noisy labels. In CVPR, 2018.
[35] Sunil Thulasidasan, Tanmoy Bhattacharya, Je Bilmes, Gopinath Chennupati, and Jamal Mohd-Yusof. Combating label noise in deep learning using abstention. In ICML, pages 6234­ 6243, 2019.
[36] Xiaobo Wang, Shuo Wang, Jun Wang, Hailin Shi, and Tao Mei. Co-mining: Deep face recognition with noisy labels. In ICCV, pages 9358­9367, 2019.
[37] Yisen Wang, Weiyang Liu, Xingjun Ma, James Bailey, Hongyuan Zha, Le Song, and Shu-Tao Xia. Iterative learning with open-set noisy labels. In CVPR, pages 8688­8696, 2018.
[38] Yisen Wang, Xingjun Ma, James Bailey, Jinfeng Yi, Bowen Zhou, and Quanquan Gu. On the convergence and robustness of adversarial training. In ICML, volume 1, page 2, 2019.
[39] Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James Bailey. Symmetric cross entropy for robust learning with noisy labels. In ICCV, pages 322­330, 2019.
[40] Dongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust generalization. NeurIPS, 2020.
[41] Songhua Wu, Xiaobo Xia, Tongliang Liu, Bo Han, Mingming Gong, Nannan Wang, Haifeng Liu, and Gang Niu. Class2simi: A noise reduction perspective on learning with noisy labels. In ICML, 2021.
[42] Xiaobo Xia, Tongliang Liu, Bo Han, Chen Gong, Nannan Wang, Zongyuan Ge, and Yi Chang. Robust early-learning: Hindering the memorization of noisy labels. In ICLR.
[43] Xiaobo Xia, Tongliang Liu, Nannan Wang, Bo Han, Chen Gong, Gang Niu, and Masashi Sugiyama. Are anchor points really indispensable in label-noise learning? In NeurIPS, pages 6838­6849, 2019.
[44] Xiaobo Xia, Tongliang Liu, Bo Han, Nannan Wang, Mingming Gong, Haifeng Liu, Gang Niu, Dacheng Tao, and Masashi Sugiyama. Part-dependent label noise: Towards instancedependent label noise. In NeurIPS, 2020.
15

[45] Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang Wang. Learning from massive noisy labeled data for image classi cation. In CVPR, pages 2691­2699, 2015.
[46] Yan Yan, Rómer Rosales, Glenn Fung, Ramanathan Subramanian, and Jennifer Dy. Learning from multiple annotators with varying expertise. Machine learning, 95(3):291­327, 2014.
[47] Shuo Yang, Lu Liu, and Min Xu. Free lunch for few-shot learning: Distribution calibration. In ICLR, 2021.
[48] Quanming Yao, Hansi Yang, Bo Han, Gang Niu, and James T Kwok. Searching to exploit memorization e ect in learning with noisy labels. In ICML, 2020.
[49] Yu Yao, Tongliang Liu, Bo Han, Mingming Gong, Jiankang Deng, Gang Niu, and Masashi Sugiyama. Dual t: Reducing estimation error for transition matrix in label-noise learning. In NeurIPS, 2020.
[50] Xingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor W Tsang, and Masashi Sugiyama. How does disagreement bene t co-teaching? In ICML, 2019.
[51] Xiyu Yu, Tongliang Liu, Mingming Gong, Kayhan Batmanghelich, and Dacheng Tao. An e cient and provable approach for mixture proportion estimation using linear independence assumption. In CVPR, pages 4480­4489, 2018.
[52] Xiyu Yu, Tongliang Liu, Mingming Gong, and Dacheng Tao. Learning with biased complementary labels. In ECCV, pages 68­83, 2018.
[53] Yikai Zhang, Songzhu Zheng, Pengxiang Wu, Mayank Goswami, and Chao Chen. Learning with feature-dependent label noise: A progressive approach. In ICLR, 2021.
[54] Zhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for training deep neural networks with noisy labels. In NeurIPS, pages 8778­8788, 2018.
[55] Songzhu Zheng, Pengxiang Wu, Aman Goswami, Mayank Goswami, Dimitris Metaxas, and Chao Chen. Error-bounded correction of noisy labels. In ICML, 2020.
16

