Junta Distance Approximation with Sub-Exponential Queries

Vishnu Iyer

Avishay Tal

Michael Whitmeyer

June 2, 2021

arXiv:2106.00287v1 [cs.DS] 1 Jun 2021

Abstract

Leveraging tools of De, Mossel, and Neeman [FOCS, 2019], we show two different results
pertaining to the tolerant testing of juntas. Given black-box access to a Boolean function f : {±1}n  {±1}:

1.

We

give

a

poly(k,

1 

)

query

algorithm

that

distinguishes

between

functions

that

are

-close

to

k-juntas

and

(

+

)-far

from

k-juntas,

where

k

=

O(

k 2

).



2. In the non-relaxed setting, we extend our ideas to give a 2O( k/) (adaptive) query

algorithm that distinguishes between functions that are -close to k-juntas and ( + )-far

from k-juntas. To the best of our knowledge, this is the first subexponential-in-k query

algorithm for approximating the distance of f to being a k-junta (previous results of Blais,

Canonne, Eden, Levi, and Ron [SODA, 2018] and De, Mossel, and Neeman [FOCS, 2019]

required exponentially many queries in k).

Our techniques are Fourier analytical and make use of the notion of "normalized influences" that was introduced by Talagrand [Tal94].

1 Introduction
The study of property testing, initiated by Blum, Luby, and Rubinfeld in their seminal work on linearity testing [BLR90], is concerned with making fast decisions about a global object having some global property, while only accessing (or "querying") parts of it. This notion was further explored by Goldreich, Goldwasser, and Ron [GGR96], who drew connections to the areas of learning theory and approximation algorithms in the context of graph properties. We focus on properties of Boolean functions, i.e., f : {±1}n  {±1}. First, we state the definition of a property testing algorithm A. Given  > 0 and a class of functions C, we say that A is a property tester for C if it satisfies the following two conditions:
1. if f  C, then A accepts f with probability at least 2/3;
2. if dist(f, g)   for all g  C, then A rejects with probability at least 2/3.
In the above definition, dist(f, g) = Pr[f (x) = g(x)] is the fraction of inputs on which f and g disagree under the uniform distribution. The primary measure of efficiency for such property testing algorithms is the algorithms query complexity, or the number of times it must use its black box access to f . Such query algorithms can be adaptive in that the coordinates on which they query f
UC Berkeley. Email: vishnu.iyer@berkeley.edu UC Berkeley. Email: atal@berkeley.edu UC Berkeley. Email: mwhitmeyer@berkeley.edu.

1

depend on previous answers, or they can be nonadaptive in that the algorithm always queries f in a predetermined manner.
In this writeup, our algorithms will be adaptive, and we will focus on testing the particular class of functions known as k-juntas. Juntas comprise a simple and natural class of functions: those that depend only on a smaller subset of their input variables. More precisely, a Boolean function f : {±1}n  {±1} is said to be a k-junta if there exists k coordinates i1, . . . , ik  [n] such that f (x) only depends on xi1, . . . , xik . In essence, juntas capture the existence of many irrelevant variables, and arise naturally in the context of feature selection in machine learning and many computational biology problems. A canonical example is the problem of determining the relationship between genes and phenotypes; for example, one might wish to test whether a particular physical trait is a function of many genes or only a small number.
The fundamental problem of learning and/or testing juntas has been given much attention in recent years. We refer the reader to the works of Mossel, O'Donnell, and Servedio [MOS03] and Valiant [Val12] for the most recent work on learning k-juntas. In this paper, we focus on the problem of testing juntas. Testing 1-juntas (aka dictators) and related functions had initial theoretical interest in the context of long-code testing in PCPs [H°as01; BGS98], and was first formally explored in [PRS01], which gave algorithms for testing dictators, monomials, and monotone DNFs. The more general problem of testing k-juntas was first studied by Fischer et. al. [Fis+04], where they exhibited a k-junta tester with query complexity O(k2) queries to f . Crucially, their upper bound lacked any dependence on the ambient dimension n. More recently, it was shown in [Bla09] that O(k log k + k/) adaptive queries suffice to test k-juntas, and this is tight for constant  [Sag18; CG04]. There has also been recent interest in the distribution free setting for junta testing (wherein the distribution on inputs is not assumed to be uniform). Liu et al. [Liu+19] initially gave a O(k2/)-query algorithm with one-sided error, which was quickly followed up by the works of Bshouty [Bsh19] and Zhang [Zha19] who gave O(k/)-query algorithms with two-sided and one-sided error, respectively. The methods utilized by Bshouty extend those of Diakonikolas et al. [Dia+07] and result in algorithms not only for junta testing but also several subclasses of juntas. We note that while we solve a similar problem in a different setting, some of our techniques resemble those of [Bsh19]: notably, an idea introduced in [Bsh19] is to find a witness such that, if all coordinates outside a subset of the coordinates are fixed to this witness' values, then f becomes a dictator on a single coordinate within that subset. This can be thought of as obtaining oracle access to a relevant coordinate, an idea pervasive throughout the work of [DMN19] and ours. The techniques in [Dia+07; Dia+08; Bsh19] can all be categorized in the "testing via implicit learning" paradigm, as surveyed in [Ser10].
1.1 Tolerant Junta Testing
One of the first relaxations of the standard property testing model considered (sometimes referred to as the "parameterized" regime) were testers that distinguished between f  H and f being -far from H  H. This notion was introduced by Kearns and Ron [KR00] in the context of testing decision trees and certain classes of neural networks. We note that if H is a strict superset of H, then the job of the tester becomes easier, and smaller query or sample complexity is often achievable than in the regular testing model. Indeed, our Theorem 1.3 is an example of a (tolerant) parameterized tester. Tolerant testing is another generalization of the standard property testing model. The notion was first introduced by Parnas, Ron, and Rubinfeld [PRR04]. Normal property testing entails distinguishing between functions that exactly satisfy a certain property, and functions that are -far from satisfying said property. This is somewhat restrictive, and the tolerant testing problem seeks to more generally distinguish functions that are c close to having the desired
2



c

cu

Figure 1: A visualization of the tolerant property testing paradigm. Assuming the outermost oval represents all functions f : {±1}n  {±1} and the property at hand is represented by a class of
functions , the goal is to distinguish between the light grey (at most c close to a function in ) and the dark grey (at least cu far from all functions in ) regions.

property, and those that are at least cu far from having the property, for some 0 < c < cu < 1. We also note that the notion of tolerant testing is closely related to the notion of distance approximation ­ indeed, if one can estimate dist(f, C) up to additive error (cu - c)/2 with probability at least 2/3, then one has solved the tolerant testing problem for that class.1 In general, tolerant testing (and therefore distance approximation), is much more challenging than traditional property testing. Figure 1 provides a visualization of the tolerant testing problem. Tolerant testing has received a lot of attention recently, see for example [BLT20] for work on tolerant testing of decision trees and [Ail+07] [PRW20] for work on tolerant testing of monotonicity. For the case of k-juntas, we have the following (relaxed) definition of a tolerant tester. In the following we denote by Jn,k the class of k-juntas, and for a class of functions C, we denote dist(f, C) := mingC dist(f, g).
Definition 1.1. For constants 0 < c < cu < 1/2 and a given k, k  N with k  k, a (k, k, c, cu) tolerant junta tester is an algorithm that, given oracle access to f : {±1}n  {±1},

1. if dist(f, Jn,k)  c accepts with probability 2/3;

2. if dist(f, Jn,k)  cu rejects with probability 2/3.

Our definition incorporates both tolerant and parameterized testers; when c = 0 the tester is non-tolerant and when k = k the tester is non-parameterized. We note that in the above

definition we upper bound cu < 1/2 since k-juntas are closed under complements, meaning if g  Jn,k, then -g  Jn,k. Parnas, Ron, and Rubinfeld in their seminal work [PRR04] showed that while standard property testers, when querying uniformly, are weakly tolerant, entirely new

algorithms are usually needed to tolerant test with better parameters. Tolerant junta testing was

first considered by Diakonikolas et al. [Dia+07] which used the aforementioned observation from

[PRR04]

to

show

that

a

standard

tester

from

[Fis+04]

actually

gave

a

(k,

k,

poly(

 k

),

)

tolerant

tester. Chakraborty et al. [Cha+12] subsequently showed that a similar analysis to that of Blais

[Bla09] gave a (k, k, /C, ) tolerant junta tester (for some constant C) using exp(k/) queries.

1The reverse direction is also true ­ given a tolerant tester it is possible to estimate the distance to that property. See for example section 3 in [Ail+07].

3

More recently, Blais et al. [Bla+19, Theorem 1.2] showed a tradeoff between query complexity

and the amount of tolerance. In particular, they gave an algorithm which, given k, , and   (0, 1),

is a (k, k, /16, ) tolerant junta tester.

The query complexity of the algorithm is O

k log k (1-)k

.

In particular, note that when  is a constant bounded away from zero, this yields an exp(k) query

algorithm, but when  = 1/k this yields a poly(k) query algorithm. We also note that there is an

undesirable multiplicative "gap" between cu and c that precludes one from tolerantly testing for

arbitrary close values of cu and c (i.e., in [Bla+19], cu  16c for all choices of ). The recent work

of

[DMN19]

addressed

this,

giving

an

algorithm

for

any

arbitrary

,



>

0

that

required

2k poly(k,

1 

)

queries and was a (k, k, ,  + ) tolerant junta tester.

In the relaxed setting (when k = k), [Bla+19, Theorem 1.1] also gave an algorithm which used

poly(k,

1 

)

queries

to

f

and was

a (k, 4k, /16, )

tolerant

junta

tester.

This

once

again

posed

the

issue of not allowing for arbitrary cu and c values, which was resolved by [DMN19, Corollary 1.6],

which

gave

a

(k, O(k2/2),

,



+

)

tolerant

junta

tester

with

query

complexity

poly(k,

1 

).

It is interesting to note that the techniques used to obtain the results from [Bla+19]

and [DMN19] are actually quite different, and yield results that are qualitatively similar but

quantitatively incomparable. The results from [Bla+19] extend the techniques of [Bla09], which

partition the n input coordinates into poly(k) disjoint sets or "parts". It is immediate that any

k-junta is a k-part junta, but in [Bla09] it was shown that with high probability a function that is

far from being a k-junta is also far from being a "k-part junta" (for a definition of this and more

details we refer the reader to [Bla09]). The results of [Bla+19] extend the idea of considering the

relationship between k-juntas and k-part juntas in the context of tolerant testing.

The techniques in [DMN19] suggest a new way of attacking the problem of tolerant k-junta

testing. The core idea in [DMN19] was to get access to "oracles" to coordinates of f which have large

low-degree influence. These coordinate oracles are obtained with high probability via a combination

of random restrictions and noise operators to the original function, and once obtained, can be used

to search, in a brute force manner, for the nearest k-junta.

In terms of lower bounds for tolerant testing of juntas, two recent works addressed the non-

adaptive case. Levi and Waingarten [LW19] demonstrated that there exists 0 < 1 < 2 < 1/2 such that any (k, k, 1, 2) tolerant junta tester requires (k2) non-adaptive queries to f . In particular,

this result demonstrated that the tolerant testing regime is quantitatively harder than the standard

testing regime, in which a O(k3/2)-query non-adaptive query algorithm is known [Bla08] (and indeed

optimal due to [Che+18]). Subsequently, Pallavoor, Raskhodnikova, and Waingarten [PRW20]

demonstrated that for any k  n/2 there exists 2 = (1/ k)) such that every nonadaptive (k, k,

0 < 1 < 2 < 1, 2)-tolerant

1/2 (with 1 = O(1/k1-) junta tester requires at least

and 2k

queries to f , for any 0 <  < 1/2.2

1.2 Our Results
Our first result is a subexponential-in-k query tolerant junta tester in the standard (non-relaxed) setting. In fact, we obtain an -accurate estimate of the distance of f to the class of k-juntas.
Theorem 1.2. Given a Boolean function f : {±1}n  {±1}, it is possible to estimate the distance of f from the class of k-juntas to within additive error  with probability 2/3 using 2O( k/) adaptive queries to f . In particular, when  is constant, this yields a 2O( k)-query algorithm. However, the algorithm still requires exp(k/) time.
2We note that this lower bound does not necessarily rule out poly(k) exp(1/) nonadaptive query (k, k, 1, 2) (where  = 2 - 1) tolerant junta testers due to the setting of 1 and 2 in their hard instance.

4

A simple corollary of the above theorem is that for any 0 < c < cu < 1/2, we have a (cu, c, k, k) tolerant junta tester with the same query complexity as in Theorem 1.2, where  = (cu - c)/2.
This is an improvement of the results of [DMN19; Bla+19], whose tolerant junta testers when k = k required exponential query complexity in k in the worst case. We note that although we
obtain this improvement, our algorithm still requires exp(k) time. In the appendix, we show a result solving a similar problem3 with an improved dependence on , giving an algorithm requiring only 2O( k log(1/))-queries and exp(k log(1/)) time (see Theorem 5.10).
In the relaxed/parameterized setting when k = k, we give a polynomial-in-k query tolerant junta tester that is valid for any setting of cu and c, and reduces k dependence on k to be linear
instead of quadratic due to the result of [DMN19, Corollary 1.6].

Theorem 1.3. For any ,  > 0 and k  N, there is an algorithm with query complexity poly(k, 1/) that is a (k, O(k/2), ,  + )-tolerant junta tester.

Theorem 1.3 is a simple corollary of the following theorem we prove.

Theorem 1.4. Let  > 0, k  N, and k = O(k/2). Then, there exists an algorithm that given parameters k,  and oracle access to f makes at most poly(k, 1/) queries to f and returns a number  such that with high probability (at least 0.99)

1.   dist(f, Jn,k) +  2.   dist(f, Jn,k) - 

Indeed, to solve the problem in Theorem 1.3 we can apply the algorithm from Theorem 1.4 with

phra=ovbe(actbuhia-littcyw)i/th3ahcnigd+hapcrc<oepb12ta(bicfiulait+nydco)nalyncudif-we<w>i12ll(21c(auccu+ce+pctc)..)OIafnnddtishtwe(feo,twJhinell,rkr)heajencdct,.

we have if dist(f,

that Jn,k

with high )  cu we

Both of the algorithms used to prove Theorem 1.2 and Theorem 1.4 rely on the fact that we

can get approximate oracle access to influential coordinates of f using techniques from [DMN19].

From there, we analyze the Fourier coefficients of f after a series of random restrictions in order

gain more information about the relevant coordinates of f at different Fourier levels. Along the

way, we give an algorithm which provides us with oracle access to a junta in the following sense:4

Theorem 1.5 (Informal). Let f : {±1}n  {±1}, D = {g1, . . . , gk} be a set of functions giving oracle access to a certain set of coordinates. Let g be a function from {±1}k  [-1, 1] defined by g(x) = E[f (y)|g1(y) = x1, . . . , gk(y) = xk]. Then g can be computed by a randomized algorithm that runs in expected time poly(k).

We note that one can view this as an oracle access to the junta, without even figuring out the coordinates on which the junta depends. More details on the ideas behind both algorithms can be found in Section 3.

1.3 Structure of this Paper
Section 2 surveys some necessary preliminaries. Section 3 gives high level overviews of the techniques and ideas that go into the proofs of Theorem 1.4 and Theorem 1.2. Section 4 first describes how to
3In particular, this problem is the problem of finding the subset of k inputs that "contain" the most Fourier mass ­ see Section 2 and Theorem 5.10 for more details.
4A similar technique appeared in [DMN19, Section 5.1] to sample two inputs on which the coordinate oracles agree. We note that our algorithm allows to specify the values the coordinate oracles attain.

5

get obtain "oracle access" to a junta (see Theorem 1.5) using only oracles for relevant coordinates of the junta, and then provides all the details of the algorithm and proof for Theorem 1.4. Finally, Section 5 provides all the details of the algorithm and proof for Theorem 1.2.

2 Preliminaries

Throughout the paper we adopt certain notation conventions. For a positive integer n, we denote by [n] the set {1, . . . , n}. For a distribution D, we denote that a random variable x is sampled
according to D by x  D. In the case that x is sampled uniformly at random from a set S, we will
abuse notation slightly and write x  S. The binomial distribution with n trials and probability
p per trial will be denoted Bin(n, p). We denote the set {-1, 1} with the shorthand {±1}. For functions f, g from {±1}n to {±1} we define dist(f, g) = Prx{±1}n[f (x) = g(x)]: that is, the fraction of inputs on which f and g differ. For a set S  [n] we will denote by {±1}S the set of
possible assignments to the variables {xi}iS .

2.1 Probability

We recall the following Chernoff/Hoeffding bounds.

Fact

2.1.

If

X1, . . . , XN

are

independent

random

variables

bounded

in

[0, 1]

and

X¯

:=

1 N

then we have

Pr[|X¯ - E[X¯ ]|  ]  2 exp(-2N 2),

Furthermore, denoting by p = E[X¯ ], we have

Pr[X¯  p - ]  exp(-2N 2),

Pr[X¯  (1 - )p] 

e- (1 - )1-

pN
 exp

-

2pN 2

.

N i=1

Xi,

2.2 Boolean Functions
In this section we recall some tools in the analysis of Boolean functions. For a more thorough introduction to the field, we refer the reader to [ODo14].
For every subset S  [n], we define the parity function on the bits in S, denoted by S : {±1}n  {±1} as S(x) = iS xi. It is a well-known fact that we can express uniquely any f : {±1}n  R as a linear combination of {S}S[n]:

f (x) =

f (S)S(x).

S[n]

The coefficients {f (S)}S[n] are referred to as the Fourier coefficients of f , and can be calculated

by f (S) = E[f (x)S(x)]. We say Fourier coefficients are on level s if they correspond to subsets of

size s.

Given a function f : {±1}n  {±1} and a coordinate i  [n], we define the influence of the

i-th coordinate on f to be

Inf i[f

]

=

Pr [f
x{±1}n

(x)

=

f

(xi)].

6

It is a well-known fact (see, e.g., [ODo14, Theorem 2.20]) that Inf i[f ] = Si f (S)2. The latter definition naturally extends to functions f : {±1}n  R. We naturally extend this notion and define the low-degree influence (up to level k) of coordinate i on f as

Inf i k[f ] =

f (S)2.

Si,|S|k

For a set T  [n] we define the projection of the function f to T , denoted f T , as the partial Fourier expansion restricted to sets contained in T , i.e., f T (x) = S:ST f (S)S(x). We observe that f T depends only on coordinates in T and that it can be alternatively defined as f T (x) = Ey{±1}n [f (y)|yT = xT ]. As suggested by the last identity, we also denote f T by favg,T .
In the regime of property testing, we will need a notion of "closeness" of functions.
Definition 2.2. For functions f, g : {±1}n  {±1} and a set of functions G, all from {±1}n  {±1} we say that

1. f is -close to g if dist(f, g)  ;

2. f is -close to G if mingG dist(f, g)  ;
3. f and g are c-correlated if Ex{±1}n [f (x)g(x)] = c;
4. f and G are c-correlated (denoted corr(f, G) = c) if maxgG Ex{±1}n[f (x)g(x)] = c.
In the paper, we will occasionally abbreviate the correlation between f and g as E[f g] when the domain is implied. Observe that when f and g are Boolean-valued (in ±1) we have E[f g] = 1 - 2dist(f, g).
Fact 2.3. For functions f, g : {±1}n  R, we have Plancheral's identity:

E [f (x)g(x)] = f (S)g(S) .
x{±1}n S[n]

When f = g, this fact is known as Parseval's identity. Definition 2.4. For a function f : {±1}n  R we define:

Wk[f ] =

f (S)2 .

|S|k

The definitions of Wk[f ], W=k[f ], and similar follow from a natural extension. Now, we define some classes of Boolean functions with properties that will be useful to us.
Definition 2.5 (Junta). Let T  [n]. A function f : {±1}n  R is called a junta on T if f depends only on coordinates in T . I.e., there exists a function g : {±1}T  R such that f (x) = g(xT ). A function is called a k-junta if it is a junta on T for some T  [n] of size k. Following the notation of [DMN19], we denote the class of k-juntas on n inputs as Jn,k. We also denote JU,k as the set of k-juntas with inputs inside of U , and when |U | = k then we often denote JU := JU,k for brevity.
Definition 2.6 (Dictator, Anti-Dictator). The i-th dictator function is given by Dicti(x) = xi, for x  {±1}n. The i-th antidictator function is simply the negation -Dicti(x).

7

Claim 2.7 (Nearest k-junta on a Subset). For a function f : {±1}n  [-1, 1] and a subset T  [n], the Boolean-valued junta-on-T most correlated with f is given by

sgn(favg,T (x)) = sgn

E
y{±1}n

[f

(y)|yT

=

xT ]

.

Furthermore, the correlation between f and sgn(favg,T (x)) is simply Ex{±1}n [|favg,T (x)|]. We keep the proof for this well-known claim for completeness.

Proof. Let g : {±1}n  [-1, 1] be any junta-on-T . It suffices to show that Ex[f (x)g(x)]  E[f (x)sgn(favg,T (x))], as we do next. Indeed, for any g(x) that is a junta-on-T we have g(x) = g(xT ) for some g : {±1}T  [-1, 1]. Thus, we have

E [f (x)g(x)]
x{±1}n

=

E
x{±1}n

[f

(x)g

(xT

)]

=E
x{±1}n

g(xT

)

·

E
y{±1}n

[f

(y)|xT

=

yT ]

=

E
x{±1}n

[g

(xT

)favg,T

(x)]



E
x{±1}n

[|favg,T

(x)|]

=

E
x{±1}n

[sgn(favg,T

(x))

·

favg,T

(x)]

=

E [f
x{±1}n

(x)sgn(favg,T

(x))].

A useful tool in Boolean Function Analysis is the noise operator T. For a vector x  {±1}n we denote by N(x) the distribution over vectors y  {±1}n such that for each coordinate i  [n] independently yi = xi with probability (1+)/2 and yi = -xi otherwise (alternatively, E[xiyi] = ). For a function f : {±1}n  R we denote by Tf : {±1}n  R the function defined by

Tf (x) = E [f (y)]
yN(x)

There's also a nice Fourier expression for the function Tf given by Tf (x) = S[n] f (S)|S|. We will need a simple fact about the noise operator.

Fact 2.8 ([ODo14, Exercise 2.33]). For any function f : {±1}n  R and any   [-1, 1] we have that E[|Tf |]  E[|f |].

2.3 Estimating Fourier Coefficients
The following claim is a standard tool in many learning algorithms. It establishes that estimating Fourier coefficients of a Boolean function f can be done with a few queries to f .
Claim 2.9 ([ODo14, Proposition 3.39]). Suppose f : {±1}n  {±1} and S  [n] then there exists an algorithm that estimates f (S) up to additive error  with probability at least 1 -  that makes O((1/2) · log(1/)) samples.
The next claim generalizes the claim to a bounded function f : {±1}n  [-1, 1]. For that generalization, we need the definition of a randomized algorithm computing a bounded function f .

8

Definition 2.10 (Randomized Algorithm for a Bounded Function). Let f : {±1}n  [-1, 1] be a bounded function. We say that algorithm A is a randomized algorithm for f if on any fixed input x algorithm A outputs a random bit y  {±1} with E[y] = f (x).

Claim 2.11. Let f : {±1}n  [-1, 1], and let A be a randomized algorithm for f . Then, there exists an algorithm making O((1/2) · log(1/)) calls to A that estimates f (S) up to additive error
 with probability at least 1 - .

Proof Sketch. We estimate f (S) by sampling m = O((1/2) · log(1/)) uniformly random inputs

x(1), . . . , x(m), applying A to each of them to get random bits (y1, y2, . . . , ym), and taking the

empirical

mean

of

1 m

m i=1

yi

·

S (x(i)).

Note

that

for

each

i

[m]

we

have

that

yi · S(x(i)) is

a

{±1} random variable with expectation

E [yi · S(x(i))] = E

x(i) ,yi

x(i)

Eyi[yi|x(i)] · S (x(i))

= E [f (x(i)) · S(x(i))] = f (S).
x(i)

The claim follows from Fact 2.1.

2.4 Random Restrictions
Definition 2.12 (Restriction). Consider the class of functions on {±1}n. A restriction is a pair (J, z) where J  [n], and z  {±1}J . Given a function f : {±1}n  R, and a restriction (J, z), the restricted function fT z : {±1}T  R is defined by fT z(x) = f (y) where yT = x and yT = z.
Definition 2.13 (-Random Restriction). For   [0, 1] we say that J is a -random subset of S if it is formed by including each element independently with probability , which we denote as J  S. A -random restriction, denoted (J, z)  R, is sampled by taking J to be a -random subset J on [n], and taking z to be a uniformly random string in {±1}J .
Occasionally, we will abuse notation and think of fTz as a function from {±1}n to {±1} that ignores bits outside T . For example, fT z : {±1}n  {±1} is given by fT z(x) = f (xT , zT ). Finally, we will use the following fact on random restrictions:
Fact 2.14 ([ODo14, Corollary 3.22]). For a function f : {±1}n  R and sets S  J  [n] we have

E
z{±1}J

[fJ z (S )2 ]

=

R[n],RJ =S

f

(R)2.

3 Overview of Techniques
Both of our algorithms rely on only having to consider a subset of influential coordinates, rather than all n input variables. This is obtained using results from [DMN19], and is discussed further in Section 4. For now, we simply assume that we are only dealing with poly(k, 1/) coordinates S. For simplicity of presentation, we ignore dependence on , and focus only the dependence on k. Thus, in this section, assume that  is a small universal constant, e.g.,  = 0.01.

3.1 Techniques for Establishing Theorem 1.4
Our first result shows how to further reduce the number of coordinates we need to consider down to O(k/2), while only losing at most  amount of correlation with the maximally correlated k-junta. In establishing Theorem 1.4, we first develop intuition behind a notion of normalized influence that we introduce next:

9

Definition 3.1 (Normalized Influence). Let f : {±1}n  R. We define the normalized influence

of coordinate i on f as

NInf i[f ] =

f

(S)2 |S|

.

Si

We also naturally define the normalized influence below level k:

NInf i k[f ] :=

f

(S)2 |S|

.

|S|k

Si

We note that while the term "normalized influence" is new, the quantity itself is not. It first appeared in a work of Talagrand [Tal94] (expressed as M (if )2) which generalized the famous KKL theorem [KKL88; Kel+20], and subsequently appeared in followup works extending Talagrand's
theorem to Schreier graphs [OW13]. As far as we know, this is the first use of this quantity in a
learning or testing setting.
The next claim states that the sum of normalized influences of f equals its variance.

Claim 3.2. For any function f : {±1}n  R, we have that i NInf i[f ] = Var[f ].

Proof. We have that

NInf i[f ] =

i[n]

i[n] Si

f (S)2 |S|

=
S[n] iS

f (S)2 |S|

=

|S|

f

(S)2 |S|

S[n]

=

f (S)2

S[n]

= Var[f ],

S=

S=

S=

where the last equality follows from Parseval's identity.

Remark 3.3. We note that for a balanced Boolean function f (that is, one where Ex[f (x)] = 0) the normalized influences form a probability distribution on the coordinates i.
The idea behind establishing Theorem 1.4 begins with the observation the these normalized influences can be thought of as defining a sub-probability distribution over the input coordinates of f , since these are non-negative numbers whose sum is at most 1. The weight assigned to coordinate i, similar to the regular influence, captures how important i is to f , but assigns a higher relative weight to the coordinates with Fourier mass coming from the lower levels of the Fourier decomposition.
The second important observation for us is that for any set T of size at most k we can write

iT

NInf

k i

[f

]

=

iT

|S|k

f (S)2 |S|



iT

ST

f (S)2 |S|

=

f (S)2.

=ST

(1)

=Si

Si

Intuitively, this shows that if some set of coordinates captures large amount of Fourier mass, then
this same subset of coordinates also is very likely to be sampled by our sub-probability distribution defined by the normalized influences. Our idea follows this line of thought ­ we get decent estimates
for all of the normalized influences, and sample coordinates from this estimated distribution. Let
T be the "target set" of size k, i.e., the one for which the closest k-junta to f is a junta on T .
Without loss of generality we can assume that T captures constant fraction of the Fourier mass, meaning =ST f (S)2  (1). Otherwise, the best correlation of f with a k-junta is o(1) <  and the task of -accurately estimating the distance to the set of k-juntas becomes trivial. Assuming

10

T captures constant fraction of the Fourier mass, Equation (1) tells us that we will sample i  T

with constant probability mass. Thus, sampling from this distribution O(k) times means we will

have seen most of T up to a small loss in correlation.

To actually estimate these normalized influences, we apply a series of log 10k random restrictions

to our function f (first take 1-random restrictions, then 1/2-random restrictions, then 1/4-random

restrictions, and so on), and then show that summing

sandwiched

between

NInf

k i

[f

]

and

NInf i[f ]:

fJ¯z ({i})2

for

each

of

these

restrictions

is

1 2

NInf

k i

[f

]



log 10k i=0

E
(J,z)R2-i

fJ¯z ({i})2

 2NInf i[f ].

This would allow us to effectively sample from a proxy distribution that still samples i  T with constant probability.
We repeat the process iteratively, sampling coordinates one at a time, until we either sampled all of T or sampled a subset T   T for which we have that the best junta on T  is almost as correlated with f as the best junta on T . Since the process samples a coordinate in T with constant probability in each round, after O(k) iterations we are likely to succeed, giving us a set U of O(k) coordinates that contains either T or T  (as above). Finally, we show we can estimate, up to a small additive error, the best correlation of a junta-on-U with f , given only approximate oracle access to the coordinates in S. By the above discussion the estimate we get is lower bounded by the best correlation with a k-junta up to a small additive error. It is also upper bounded (trivially) with the best correlation of f with a O(k)-junta, since |U | = O(k).

3.2 Techniques for Theorem 1.2

A limitation of the algorithm we described in the previous subsection is that it only samples one coordinate at a time. In particular, suppose we want to find T exactly, instead of a superset U of T . Then, the naive algorithm would need to consider all subsets of U of size k, estimating the best correlation with a junta on each of them. This gives a exp(O(k))-query algorithm. It would be nicer if we can devise a sampling algorithm that outputs, with constant probability, many coordinates of T at a time. Such a sampling algorithm would reduce the number of possibilities for T in the second stage. In particular, consider the case that the nearest k-junta to f had significant amount of Fourier mass on higher levels, say at level  k or maybe  k. In this case it would be nice to be able to sample from the Fourier distribution of f , that would give us a large subset of T with constant probability. We note that sampling from the Fourier distribution of a Boolean function is easy for a quantum algorithm but hard for a randomized algorithm. Nevertheless, the (classical) algorithm we describe in this section takes inspiration from this, and samples subsets of size k according to the Fourier mass of f above level k of each subset, in time and query complexity exp(O( k)).
We will start with the preliminary that we have reduced to the case of only having to consider the coordinates in S  [n] with |S|  O(k/2), using our aforementioned algorithm from the previous section, incurring only a small additive loss in correlation with the closest k-junta. We start with the following definition that generalizes normalized influences of coordinates to normalized influences of sets of coordinates.

Definition 3.4. For a given subset U  [n], we define its normalized influence as follows:

NInf U [f ] :=

f

(S)2
|S|

.

S: U S |U |

11

We

also

have

the

natural

extension

of

NInf

k U

[f

]

=

S: |S|k,U S

f (S)2
( ) |S| |U |

,

analogous

to

Definition

3.1.

This is a direct generalization of the quantity in Definition 3.1. In particular, we consider taking |U | = k. Note there are 2O( k) such U within the coordinates in S, and we can think of these normalized influences as once again defining a sub-probability distribution over subsets of size k. It likely does not sum to 1, but rather sums to WS k[f ]  1. We show that these normalized influences at exactly level k can once again be approximated to within a constant factor via a sequence of random restrictions to f :



1 2

NInf

k U

[f

]



2

k log 10k
E
i=0 (J,z)Rpi

fJ¯z(U )2

 3NInf U [f ],

where p = 1 - 1 . For more details on this statement, see Theorem 5.1.
2k
We are now ready to outline the overall algorithm in Section 5. Suppose T  S is the subset on which the nearest k-junta (within S) is defined. Our algorithm can then be broken down into two phases:
 Phase 1. We get a proxy for NInf U for all |U | = k. This is achieved by performing a series of random
restrictions to f .
We consider these proxies as a distribution, and sample a constant (this constant is actually dependent on , see Section 5 for details) number of subsets of size k. With high probability, one of these is inour set of interest T , provided T has a non-negligible amount of Fourier mass above level k.
We don't know which of the subsets we sample are actually in T , so we start a branching process. For each subset we sampled, we restrict f 's values in that subset, and recursively sample from sets of size k using the steps described above. Our branching process will have depth at most k since at each level we sample k new coordinates, and T can have at most k relevant coordinates. This phase of our algorithm produces 2O( k) possible subsets of our target set T .

Phase 2. With high probability, one of the branches in theabove process will have captured most of the coefficients of T that are relevant above level k on the Fourier spectrum. Each branch of this process represents a different possibility for what T may be, so for each branch we randomly restrict f so that the coordinates sampled in that branch are fixed, which effectively moves most of the mass of T to levels below k. We then estimate all the Fourier coefficients of this restricted f below level k, allowing us to get an estimate for the closest k-junta on anysubset using these estimated coefficients. Each estimation of a Fourier coefficient requires 2O( k)-queries to estimate to the desired accuracy, and there are 2O( k) Fourier coefficients to estimate, so overall we make at most 2O( k) queries. From there, for each possible subset of B  T outputted by phase one, we brute force over all possible subsets of size k containing B, estimating the correlation f has with the closest k-junta on that subset using our estimated Fourier coefficients. This last step takes exponential time in k. We emphasize that while our runtime is exponential in k, our query complexity is only exponential in O( k).

In the entire above explanation, we have eliminated the dependence on  for simplicity. We also only consider T for conceptual and analytic simplicity ­ in reality, we have no idea what T is, and indeed it is exactly what we are looking for. Therefore, more work must be done in order to show

12

that we do not accidentally pick the wrong set, for which our estimates may be inaccurate. To get around this subtle issue, we further apply a noise operator in order to ensure that the significant parts of f lie below level roughly k. We discuss this further in Section 5.2.

4 Finding a Small(er) Set of Influential Coordinate Oracles
In this section, we detail the process of constructing oracles to coordinates with large low-degree influence. We expand upon the techniques in [DMN19], reducing the number of coordinates one needs to consider to produce a highly correlated k-junta (assuming one exists).

4.1 Approximate Oracles to Influential Coordinates
In this subsection we outline and generalize the methods used by [DMN19] to achieve oracle access to coordinates with large low-degree infuence in f . We start with the following definitions from their paper, repeated here for clarity:
Definition 4.1 ([DMN19, Def. 3.1]). Let D be a set of functions mapping {±1}n to {±1}. We say that D is an oracle for the coordinates in S if
· for every g  D, there is some i  S such that g = ±Dicti; and
· for every i  S, there is some g  D such that g = ±Dicti.
In other words, D is an oracle for S if D = {Dicti : i  S} "up to sign".
However, it is not tractable to achieve perfect access to such oracles, so we have to settle for the following weaker notion of approximate oracles:
Definition 4.2 ([DMN19, Def. 3.2]). Let D be a set of functions mapping {±1}n to {±1}. We say that D is an -oracle for the coordinates in S if

· for every g  D, there is some i  S such that g is -close to ±Dicti; and

· for every i  S, there is exactly one g  D such that g is -close to ±Dicti; and

· For every g  D, and  > 0, there is a randomized algorithm that compute g(x) correctly on

any

x



{±1}n

with

probability

at

least

1

-

,

using

poly(k, log

1 

)

queries

to

f.

Lemma 3.6 in [DMN19] establishes that we can achieve access to a set D of approximate oracles to S  {i : Inf i k[f ]  2/k} of bounded size.
More specifically, we have the following corollary:

Corollary

4.3

([DMN19,

Lemma

3.6]).

With

poly(k,

1 

,

log

1 

)

·

1 

queries

to

f,

we

can

gain

access

to

an

approximate

oracle

set

D

in

the

sense

that

for

every

coordinate

i

such

that

Inf

k i

[f

]



2 k

,

there exists a g  D such that g is -close to ±Dicti with probability at least 1 - . Furthermore,

|D|



poly(k,

1 

,

log(1/)).

For

our

purposes,

we

take



=

0.1

and



=

2-poly(k,

1 

)

in

all

our

algorithms.

Since

we

will

make

much fewer than 2poly(k/)-many queries to the coordinate oracles, we can assume that all of our

oracles are indeed  = 0.1 close to dictators/anti-dictators, since by a union bound this is true with

high probability.

13

It is important to note that we do not have a description of which coordinates are influential:

from an information theoretic standpoint this would require query complexity dependent on n.

What we do have is oracle access to these coordinates in the sense that for all i such that

Inf

k i

[f

]



2/k,

there

exists

gi



D

such

that

gi(x)



±Dicti(x),

that

is,

D

contains

dictators

or

anti-dictators to every influential coordinate. Using simple techniques of local correction we can

simplify this: we need only consider dictators to each coordinate in the oracle. Also, we can convert

closeness on average x to high probability correctness for all x (i.e., a worst-case guarantee).

Lemma 4.4. Suppose f is -close to ±Dicti. For any x  {±1}n, LocalCorrect(f, x) samples a random y  {±1}n and outputs f (y)f (x · y), where x · y is pointwise multiplication. Then,

x

:

Pr [LocalCorrect(f,
y{±1}n

x)

=

Dicti(x)]



2.

Proof. Suppose that f is  close to Dicti. Then we have Pry{±1}n [f (y) = Dicti(y)]  , and since x · y has the same distribution as y, Pry{±1}n[f (x · y) = Dicti(x · y)]  . Let A be the event that f (y) = Dicti(y) and let B be the event that f (x · y) = Dicti(x · y).
Clearly if LocalCorrect(f, x) = Dicti(x) then at least one of A and B must have occurred (since Dicti(x) = Dicti(x · y) · Dicti(y)). Thus, by the union bound, we have

Pr
y{±1}n

[LocalCorrect(f,

x)

=

Dicti(x)]



Pr[A



B]



Pr[A]

+

Pr[B]



2

A similar argument shows that if f is  close to -Dicti, then LocalCorrect(f, x) is not equal to (-Dicti(y))(-Dicti(x · y)) = Dicti(y)Dicti(x · y) = Dicti(x) with probability at most 2.
Given a noisy black box computing h which is -close to g = ±Dicti, local correction will compute Dicti with high probability, on every input x. Critically, we can treat potentially faulty ±Dicti oracles as correct Dicti oracles provided suitably many repetitions.
Corollary 4.5. If f is -close to ±Dicti for  = 0.1, then repeating LocalCorrect(f, x) independently poly(k, 1/) times and taking the majority outcome results in an incorrect value for Dicti(x) with probability at most 2-poly(k,1/).

Proof. Clear from applying the first bound in Fact 2.1 with N = O(poly(k/)) and  = (1 - 2 - 0.5) = 0.3 in this case.

We also show that restricting our attention to S we have not lost more than  in the best correlation of f with a k-junta. This is proved in the following claim.
Claim 4.6. Let f : {±1}n  {±1} and let g : {±1}n  {±1} be a k-junta on U . Let  > 0. Take

S=

iU

Inf

k i

[f

]



2 k

Then, there is a junta on S with correlation at least E[f g] -  with f .

Proof. To prove this claim, we define a function on the set S such that the loss in correlation is at

most  . Consider:

g(x) = gavg,S(x) = Ey [g(y)|yS = xS]

First, we note g is a function over only the variables in S. Second, it is bounded in [-1, 1], so it

is not quite Boolean, but it can be randomized rounded to a Boolean function, with the expected correlation with f equaling E[f g]. Thus, it suffices to show that E[f g]  E[f g] -  to deduce that

14

there exists a randomize rounding of g to a Boolean function g with E[f g]  E[f g] -  . We also recall that
g(T ) = g(T ) if T  S 0 otherwise
We thus have:

E[f g] - E[f g] =

f (T )g(T ) 

TS T U



f (T )2 

f (T )2

TS T U

iU \S T i T U

Inf

k i

(f

)



iU \S

k

·

2 k

=



Finally, the below corollary summarizes what we have achieved in this section.

Corollary

4.7.

With

poly(k,

1 

,

log

1 

)

queries

to

f,

we

can

gain

access

to

an

approximate

oracle

set

D

for

a

set

of

coordinates

{i

:

Inf

k i



2 k

}



S



[n].

Moreover,

these

coordinates

and

oracles

satisfy the following properties.

· For every coordinate i  S, there exists a g  D such that g is 0.1-close to Dicti with probability at least 1 - .

· dist(f, Jn,k) - dist(f, JS,k)  .

· |S|  poly(k, 1/, log(1/)).

· For any algorithm A that uses at most q queries to D, we can use LocalCorrect from Lemma 4.4 with error /q to assume that we actually have perfect access to each coordinate oracle, up to an additive loss of  in confidence and a multiplicative overhead of poly(log(q/)) in query complexity.

Proof. The first and the third bullet point follow from Corollary 4.3. The second bullet point follows from Claim 4.6. To achieve the last point, we can use Corollary 4.5 every time we make a "query" to an oracle in our algorithm. Thus every "query" to an oracle g  ±Dicti at x involves poly(log(q/)) many repetitions of LocalCorrect(g, x), which results in an incorrect value with probability at most /2q, as noted above. Recall that Corollary 4.3 guarantees that we can output g(x) correctly with probability 1 - /2q with only a poly(k, log(q/)) queries to f . Since we only ever make at most q queries to our coordinate oracles, we can assume that LocalCorrect(g, x) = Dicti(x) in all queries. This happens with probability at least 1 -  by the union bound.

Therefore, for the rest of this paper, we will assume that we have oracle access to exact dictators.

4.2 Implicit Access to an Underlying Junta

An important consequence of having coordinate oracles is that it allows us to reduce the input

size of the function dramatically. Suppose f : {±1}n  {±1} and we have D = {g1, . . . , gk} are

randomized algorithms that for any x  {±1}n output gi(x) = Dictji(x) = xji. We have that j1, . . . , jk  [n] are a set of k distinct coordinates.

Let U = {j1, . . . , jk}. We want to get access to the following function: g(x1, . . . , xk) =

E[f (y)|yj1 uniformly

= x1, yj2 from all y

= x2, . .  {±1}n

.

, yjk that

= xk]. More precisely, satisfy yj1 = x1, yj2 = x2,

given x1, . . . , . . . , yjk = xk

xk we want and apply f

to on

sample this y.

15

The following algorithm that runs in poly(k, log(1/)) time samples y from such a distribution.

Algorithm 1: Sampling a uniformly random input consistent with the oracles' values

Input: f (target function), D = {g1, . . . , gk } (coordinate oracles), (x1, . . . , xk )  {±1}k Output: A vector y  {±1}n with (g1(y), . . . , gk (y)) = (x1, . . . , xk) 1 Sample y  {±1}n and let z  {±1}k be the vector of evaluations of {g1, . . . , gk} on y; 2 while z = x do

3 repeat

4 5

Let Let

y z

be be

a copy of y, but flip each bit independently the vector of evaluations of {g1, . . . , gk} on

with y;

probability

1 k

;

6 until dist(x, z) < dist(x, z)

7 y = y;

8 z = z;

9 return y

Theorem 4.8. Algorithm 1 with probability 1 -  runs in time poly(k, log(1/)).

Proof. We focus on the number of iterations of the inner repeat loop. Given (y, z) with z = x we

analyze the time it takes to find a (y, z) with dist(z, x) < dist(z, x). Since x = z without loss

of generality we can assume that x1 = z1. To get (y, z) with dist(z, x) < dist(z, x), it suffices

to sample a vector y with are flipping each coordinate

yj1 = x1 and yj2 = yj2, yj3 = yj3, . . with probability 1/k the probability

. , yjk = yjk of sampling

. Indeed, since we such a y is exactly

1/k · (1 - 1/k)k-1  1/(ek). Thus, we get that the runtime of the repeat loop is stochastically

dominated by a geometric random variable with success probability 1/(ek). Thus with probability

at least 1 - /k, it finishes after O(k · log(k/)) iterations. We run the inner repeat loop at most

k-times, thus by union bound, with probability at least 1 -  the entire process end after at most

O(k2 · log(k/)) executions of line 5. We note that execution line 5 actually requires k queries to

g1, . . . , gk, each of them takes poly(k) = poly(k) time. thus overall, with probability at least 1 - , our algorithm run in time poly(k, log(1/)).

Theorem 4.9. Algorithm 1 samples uniformly from the set of inputs {y : (g1(y), . . . , gk(y)) = (x1, . . . , xk ))}.
Proof. Let U = {j1, . . . , jk} be the set of coordinates for which {g1, . . . , gk} are oracles to. Algorithm 1 certainly samples a vector y with yj1 = x1, . . . , yjk = xk. We want to show additionally that Algorithm 1 samples yU uniformly at random. In fact, at any point in the algorithm the distribution over yU is uniform. This is clearly true in the first step where y  {±1}n, and remains true along the algorithm as we apply independent noise to coordinates in U and decide whether to apply the noise or not according to the value of yU which is independent of yU .

We will consider algorithms computing non-Boolean function like g = favg,S for some subset S  [n]. Note that g is a function whose range in [-1, 1], but not necessarily a Boolean function.

Theorem 4.10 (Formal version of Theorem 1.5). Let f : {±1}n  {±1}, D = {g1, . . . , gk} be a set of coordinate oracles. Let g be a function from {±1}k  [-1, 1] defined by g(x) = E[f (y)|g1(y) = x1, . . . , gk(y) = xk]. Then g has a randomized algorithm in the sense of Definition 2.10 computing it that runs in expected time poly(k).

Proof. Given x = (x1, . . . , xk) apply Algorithm 1 on f , D and x to get a vector y  {±1}n. Return f (y). It is clear that since y is a uniform input subject to g1(y) = x1, . . . , gk(y) = xk that our algorithm is a randomized algorithm for g.

16

4.3 Influential Coordinate Oracles

As above, denote as S the superset of the low-degree influential coordinates of f , and D as the set of

approximate oracles to said coordinates, obtained via Corollary 4.3 with parameter  = 0.1. As we

discussed in Section 4.1, we assume (with a small loss in error probability, and a small multiplicative

factor on query complexity) that we have exact access to dictators for each influential coordinate.

We work towards proving the following improved version of a corollary that appeared in [DMN19]:

The idea will be to take D, a set of k = poly

k,

1 

coordinate oracles, and somehow "prune" it

down

to

a

set

D

of

at

most

O(

k 2

)

coordinate

oracles,

such

that

that

the

loss

in

the

most

correlated

junta on this smaller set of coordinates is at most 

max E[f g] - max E[f g]  .

gJD,k

gJD,k

4.4 Reducing the Number of Oracles to Consider

Starting with a set of poly(k/) set of oracles D for a set S containing the influential coordinates of

f , our goal in this section is to prune the number of oracles to O(k/2) in a way that incurs only

a small loss in correlation with the nearest k-junta. [DMN19] achieved their theorem by noting

that applying a standard noise operator to f did not affect its proximity to the nearest k-junta

significantly,

while

also

guaranteeing

that

at

most

k2 2

coordinates

could

have

large

influence.

They

then were able to estimate the influence of every coordinate in D despite only having (approximate)

oracle access to the influential coordinates, and thus were able to determine which oracles were

actually oracles to influential coordinates, of which there were less than k2/2.

Our approach, as explained at a high level in Section 3, is to estimate the normalized influence

of each coordinate in S, which is done via a sequence of random restrictions to f . In words, the

below algorithm estimates for each coordinate i  S the quantity i 2d = E(J,z)R2-d [fJ¯z({i})2], where (J, z)  R2-d parameterize a 2-d-random restriction to f . Then, i is defined to be sum over a series of random restrictions d = 0, ..., log 10k of i 2d. The core idea of our algorithm is that this sum over Fourier coefficients on the first level of restricted versions of f is a proxy for

NInf i[f ]. In other words, we have the following theorem:

Theorem 4.11. Let f : {±1}k  R, where k = |D|. Let i  [k]. Let

log(10k)

i[f ] =

i 2m [f ],

m=0

where

i 2m

[f

]

=

E
(J,z)R2-m

[fJ¯z

({i})2

].

Then,

1 2

NInf

k i

[f

]



i[f ] 

2NInf i[f ].

We postpone the proof of Theorem 4.11 to Section 4.5. The definition of i naturally gives rise to an algorithm for estimating i that we present next. The algorithm would return for each

17

i  [k] an estimate i that would be close to i with high probability.

Algorithm 2: Estimating i Input: f : {±1}k  [-1, 1] along with randomized algorithm A computing f (recall

Def. 2.10). Parameters 1 -  (confidence),  (additive error) and k.

Output: Estimates (1, . . . , k) for (1, . . . , k ). 1 Let m = poly(k, k, 1/, log(1/))

2 Initialize i = 0 for all i  [k];

3 for d = 0 to log 10k do

4 Initialize i 2d = 0 for all i  [k];

5 repeat m times

6

Let (J, z)  R2-d be a 2-d-random restriction.

7

Estimate

fJ¯z ({j })

for

all

j



J

up

to

additive

error

 6 log(10k)

with

probability

1 - /poly(k, k, m) using Claim 2.11 and algorithm A.

8

Denote by fJ¯z({j}) the estimated Fourier coefficient.

9

Update j2d = j2d + fJ¯z({j})2 for all j  J .

10

Let i 2d = i 2d /m for all i  [k];

11 Let i = d i 2d ;

12 return (1, 2, . . . , k)

Lemma 4.12. With probability at least 1 -  we have that for all i  [k] it holds that |i - i|  .
Proof. If j / J the Fourier coefficient of fJ¯z is 0 and so our estimate is correct in that case. In the case j  J, each estimation of the Fourier coefficient is correct up to additive error  = /6 log(10k) with probability at least 1 - /poly(k, k, m). Thus, we get that fJ¯z({j})2 = (fJ¯z({j}) ± )2 = fJ¯z({j})2 ± 2|fJ¯z({j})| ± 2 = fJ¯z({j})2 ± 3. Furthermore, we have that E(J,z)R2-d [fJ¯z({j})2] = j2d, thus by Fact 2.1 we have that the empirical mean of m = poly(1/, log(k), log(k), log(1/)) copies of fJ¯z({j})2 is within additive error /(2 log(10k)) from j 2d with probability at least 1 - /(k log(10k)). By union bound, all these estimates are within the error bound, and we get that |j2d -j2d|  3 +/(2 log(10k))  /(log(10k)). Overall, we get that |j - j|   for all j  [k] with probability at least 1 - .

18

With Algorithm 2 in hand, we are ready to present the pruning procedure.

Algorithm 3: Reduce Number of Oracles

Input: f (target function), D (influential coordinate oracles, where D are oracles for S).

Parameters  and .

1

Output: A subset Initialize D = ;

D

D

of

size

O(

k 2

)

such

that

we

lose

at

most



in

correlation

with

f.

2 Let m = O((k + log(1/))/2)

3 repeat m times 4 Let {g1, . . . , gk} = D - D, and {gk+1, . . . , g|D|} = D 5 Sample z  {±1}|D|. Let f  : {±1}k  R be the function defined by

f (x1,

.

.

.

,

xk )

=

E
y{±1}n

[f

(y

)|g1

(y)

=

x1,

.

.

.

,

gk (y)

=

xk ,

gk+1(y)

=

z1,

.

.

.

,

gk+|D|(y)

=

z|D|].

and let A be the randomized algorithm for f  from Theorem 4.10.

6 Apply Algorithm 2 on f  using the randomized algorithm A for f  with confidence

1-

 2m

and

accuracy

2 48·|S |

=

 = (1, . . . , k ).

7 Let our distribution P be defined by , normalized appropriately.

8 Sample i  P , and add gi to D.

9 return D

Lemma 4.13. With probability at least 1 - , Algorithm 3 returns a set of oracles D to a subset of coordinates S  S, such that

max E[f g] - max E[f g]  .

gJS,k

g JS  ,k

To prove Lemma 4.13, which tells us our algorithm succeeds and directly implies Theorem 1.4,
we will need a few more lemmas. We denote the event E that in the entire execution of Algorithm 3 all i were 2/(48 · |S|) close
to the real i. We note that by union bound this event happens with probability at least 1 - /2. Suppose T is the (unknown) set of k oracles for which the best-k junta approximating f is a
junta on T . We want to show that our algorithm either samples all the coordinates in T , or it samples a subset T  of T that captures all but 2/4 of the Fourier mass of f on T .

Claim 4.14. Assume the event E happens. Then, with probability at least 1-/2, after m iterations, we will have either:

1. sampled i for all i  T , our target set;
2. sampled i for all i  T   T , where ST  f (S)2  ST f (S)2 - 2/4.
Proof. In each iteration, assume we have not yet satisfied either items. Let V be the subset of coordinates in T that we have not yet sampled. Let T  = T \ V . By assumption,

2/4 <

f (S)2 -

f (S)2 =

f (S)2.

ST

ST 

ST :SV =

Let S = S \ S. We have that |S| = k. Now note that up to relabeling of coordinates f  from Algorithm 3 is the same as (favg,S)Sz, where z was randomly chosen. For brevity, denote by

19

fz = (favg,S)Sz. Note that for any fixed z, fz is a function that depends only on the coordinates in S.

By Fact 2.14, we have





E
z

fz(S)2 =

favg,S (R)2 =

f (R)2 

f (R)2

=SV

R:=(RS  )V

RS :

RT :RV =

=(RS  )V

> 2/4.

(2)

Next, by applying Theorem 4.11, for any fixed z, we have

i [fz ]



1 2

NInf

k i

[fz

]



1 2

fz (S )2 .

iV

iV

=SV

By

the

assumption

that

E

happens,

the

i

are

2 48·|S

|

-accurate,

and

we

get

that

i [fz ]



1 2

fz (S )2

-

2 48 · |S|

·

|V

|



1 2

fz (S )2

-

2 48

.

iV

=SV

=SV

On the other hand by applying Theorem 4.11 again we see that

i[fz]  2 · NInf i[fz] = 2 · Var[fz]  2

iS 

iS 

and thus

iS i[f ]



2

+

k

·

2 48·|S |



2+

2 48



3

(under

the

assumption

that

E

happens).

Overall,

the probability to sample an element from V is at least





1 3

·

1 2

fz (S )2

-

2  48

=

1 6

fz (S )2

-

3

2 · 48

=SV

=SV

By taking expectation over z, and using Equation (2) we see that the probability to sample an

element from V overall is at least





E
z



1 6

fz (S )2

-

2  3 · 48



1 6

·

2 4

-

2 3 · 48

>

2 30

.

=SV

We get that in each iteration as long as we don't satisfy Items (1) and (2) above, we sample

an element from times we would

i  T with sample all

probability at least 2/30. of T , or get stuck at some

By T

repeating satisfying

the

process

m

=

O(

k+log(1/) 2

)

Item (2), with probability at

least 1 - /2, using Fact 2.1.

Next, we show that finding T  is almost as good as finding T in the sense that the best correlation by juntas-on-T  with f is up to small additive error the best correlation by juntas-on-T with f .

Lemma 4.15. Suppose we have some subset T such that ST f (S)2 = c, and we then identified

a subset T   T such that

ST  f (S)2

c-

2 4

.

Then

max E[f g] - max E[f g]  

g JT ,k

gJT ,k

20

Proof. We know that argmaxgJT,k E[f g] = sgn(favg,T ) and similarly argmaxgJT,k E[f g] = sgn(favg,T ). Then we have that

max E[f g] - max E[f g]

g JT ,k

gJT ,k

= E[f (x)(sgn(favg,T (xT )) - sgn(favg,T (xT  ))]

=E
xT

E
xT

[f

(xT

,

xT

)]

sgn(favg,T (xT )) - sgn(favg,T (xT  )

=E
xT

favg,T (xT )

sgn(favg,T (xT )) - sgn(favg,T (xT  )

2E
xT

favg,T (xT ) - favg,T  (xT  )

(Since z(sgn(z) - sgn(z))  2|z - z| for all z, z  R)

2 E
xT

favg,T (xT ) - favg,T  (xT  ) 2

=2

f (S)2 - 2 f (S)2 + f (S)2

ST

ST 

ST 

 2 2 =  . 4

Proof of Lemma 4.13. Let g be the k-junta that maximizes E[f g] among all k-juntas on S. Let T

be the set of variables on which g depends. By Claim 4.14 we either sample oracles to all of T or

to a subset T  for which

f (S)2  f (S)2 - 2/4.

ST 

ST

In the second case, by Lemma 4.15, we incur a loss in correlation of at most  with our nearest k-junta. In the first case, we lose no correlation with the closest k-junta, and by a union bound our probability of failure is at most .

The above concludes the proof of Lemma 4.13. Finally, Theorem 1.4 is implied by Lemma 4.13, as shown below.
Theorem 4.16 (Theorem 1.4, restated). Let  > 0, k  N, and k = C(k/2) for some universal constant C. Then, there exists an algorithm that given f, k,  makes at most poly(k, 1/) queries to f and returns a number  such that with probability at least 0.99

1.   maxgJn,k E[f g] + O() 2.   maxgJn,k E[f g] - O()

Proof. Set  = 2-poly(k,1/). We first apply Corollary 4.3 from [DMN19]. This gives us

poly(k,

1 

,

log(1/))

=

poly(k/)

coordinate

oracles

D

to

coordinates

S

that

includes

all

coordinates

i

with

Inf

k i

[f

]



2 k

.

By

Claim

4.6

we

see

that

max E[f g]  max E[f g] - 

gJS,k

gJn,k

Next, we apply Algorithm 3 to get a subset D  D to coordinates S  S such that with high

probability

max E[f g]  max E[f g] - 

g JS  ,k

gJS,k

21

We take  to be the estimation of the correlation of the best junta on S with f . By Claim 2.7 we

have that

that maxgJS E[f g] = E[|favg,S(x)|]. To estimate the computes favg,S given by Theorem 4.10. We randomly

latter, we use a sample O(1/2)

randomized algorithm many values for x and

estimate for each of them |favg,S(x)| up to additive error /2 via the randomized algorithm with

expected value favg,S(x).

Assume that  is a -additive approximation to maxgJS E[f g]. In this case, we claim that  satisfies both items from the theorem's statement. Indeed,

1.   maxgJS E[f g] +   maxgJn,k E[f g] + .
2.   maxgJS E[f g]-  maxgJS,k E[f g]-  maxgJS,k E[f g]-2  maxgJn,k E[f g]-3.
Next, we analyze the number of queries of our algorithm. Obtaining the initial set of coordinate oracles D takes poly(k, 1/, log(1/)) = poly(k, 1/) queries. Then, we go on to run Algorithm 3 that makes m = O((k + log(1/))/2) iterations, each making poly(k, 1/, log(1/)) queries. Next, to estimate E[|favg,S(x)|] we require poly(1/) samples from randomized algorithm for favg,S(x) each such sample translate to poly(k, 1/) samples to f . Finally, we note that each "query" to an oracle incurs an overhead of poly(log(k, 1/)) queries to f along with an o(1) additive loss in confidence by Corollary 4.7. Overall, we make poly(k, 1/) queries.

4.5 Proof of Theorem 4.11

We now present the proof of Theorem 4.11.

Proof of Theorem 4.11. We express i in terms of the Fourier spectrum of f . Using Fact 2.14,

log(10k)

i =

m=0

f (S)2 · Pr [S  J = {i}]

S:Si

J 2-m [k]

=

log(10k) m=0

S:Si

f (S)2

·

Pr [|S
J 2-m [k]



J|

=

1]

·

1 |S|

=

S:Si

f (S)2 |S|

·

log(10k)
Pr [|S
m=0 J 2-m [k]



J|

=

1]

It therefore suffices to show that for any non-empty set S such that |S|  k it holds that

1 2



log(10k)
Pr [|S
m=0 J (m)2-m [k]

 J (m)|

=

1]



2

.

(3)

From which it is clear that i  2 ·

S:Si

f (S)2 |S|

=

2 · NInf i[f ]

and

similarly

i



1 2

Si,

f (S)2 |S|

=

|S|k

1 2

NInf

k i

[f

].

We move to prove Equation (3). The first observation is that an equivalent way to sample

J (m) 2-m [k] is to sample m independent set J1(m), . . . , Jm(m) 1/2 [k] and take their intersection

J (m) = J1(m)  · · ·  Jm(m). Furthermore, by linearity of expectation





Pr [|SJ| = 1] =

E

m=0 J (m)2-m [k]

m=0 J1(m)1/2[k],

J2(m) ..1./2 [k ],

½|SJ1(m)···Jm(m)|=1

=E
J1 1/2 [k ], J2 1./..2 [k ],


½|SJ1···Jm|=1
m=0

22

which in essence means that the choices for J1(1), J1(2), . . . can be the same set J1, and similarly for any Ji.
To analyze the latter expectation, we note that it can be described as the expected value of the
following random process:

1 X0

2 for i = 1, 2, . . . , log(10k) do

3 if S =  then

4

halt!;

5 if |S| = 1 then

6

increment X;

7 Sample Ji 1/2 [k]; 8 S  S  Ji;

It therefore suffices to show that the expected value of the above random process is bounded

in [1/2, 2]. In the analysis, we consider also the infinite horizon process that keeps on going until

S = . We observe that the expected values of both processes depend only on the size of the initial

S from symmetry. For any t  {0, 1, . . . , k}, denote by Ft the expected value of the infinite horizon

process starting with a set S of size t. For the finite horizon process with i iterations, we let the

expected value be denoted by Ft(i). We observe that F0 = 0, and furthermore that F1 = 2 since

starting from a set of size 1 the random variable X would behave like geometric random variable

with

p

=

1/2.

Similarly,

F1(i)

=

2-

1 2i-1

as

it

is

the

minimum

of

i

and

a

geometric

random

variable

with p = 1/2.

Furthermore, for the infinite horizon process, we observe that we have the following recurrence

tt

Ft =

a
2t

· Fa,

a=0

for t  2 or equivalently

t-1 t

Ft · (1 - 2-t) =

a
2t

· Fa.

a=0

We show by induction that 1/2 < Ft(log 10k)  2 for t  1. The base case t = 1 was discussed above. Applying the induction hypothesis we have

t-1 t

t-1 t

Ft · (1 - 2-t) =

a
2t

· Fa 

a
2t

· 2  (1 - 2-t) · 2.

a=0

a=0

Dividing both sides by (1 - 2-t) gives the inequality Ft  2, which implies that Ft(log 10k)  2. For the lower bound, we consider the indicator random variable Yt(i), where t = |S|, which
equals 1 if |S| = 1 at some point during the above process before iteration i. We note that Yt(log 10k) is a lower bound for the value of X in the finite horizon process, and Yt is a lower bound for the
value of X at the end of the infinite horizon process. First, we claim that E[Yt] = Pr[Yt = 1]  2/3
for all t  1. The base case of t = 1 is certainly true, and we also have, similar to before, that

t-1 t

E[Yt] · (1 - 2-t) =
a=0

a
2t

E[Ya]

23



0·

1 2t

+

1·

t 2t

+

2 3

t-1
·
a=2

t a
2t

1-

2+t 2t

=

2 3

+

t

-

2 3

(2

2t

+

t)



2 3

+

t/3

- 4/3 2t



2 3

-

2/3 2t

=

2 3

·

(1

-

2-t)

which holds for all t  2, and thus Pr[Yt = 1]  2/3. However, this only holds for the infinite

horizon random process. Let A be the event that S =  by iteration log 10k, and note that

Pr[A]

=

Pr[Bin(|S|,

1 10k

)

=

0]



Pr[Bin(k,

1 10k

)

=

0]

=

1

-

1 10k

k



1-

k 10k

= 0.9.

Finally, we

claim that for all t  2 we have that Pr[Yt(log 10t)]  1/2. Note that for Yt to happen, it must be

the case that either A happens or Yt(log 10t) happens. Thus, by a union bound

2 3



Pr[Yt

=

1]



Pr[Yt(log 10t)

=

1] + Pr[A]



Pr[Yt(log 10t)

=

1]

+ 0.1

,

which implies Pr[Yt(log 10t) = 1] > 1/2. Finally, Ft(log 10t)  Pr[Yt(log 10t) = 1] > 1/2 as desired. 
5 A 2O( k)-query Tolerant Junta Tester

In this section, we prove Theorem 1.2. Throughout this section, we assume that we already applied

Algorithm 3 to reduce the number of coordinate oracles to O(k/2). We denote by D the set of

oracles we get, and by S  [n] the set of coordinate to which they are oracles to. Suppose that the

best k-junta approximation of f is a junta-on-T , for a set T  S of size k. We call T the "target

set". Note that T is unknown to the algorithm, and in fact, identifying T (or a close approximation

to T ) from all subsets of size k of S is the crux of the problem.

We start with the observation that if we were somehow able to identify all of the variables of T

that capture most of the Fourier mass above level , then we could simply restrict f by randomly

fixing these variables, leaving us with the task of identifying the best k-junta approximation of

f , given that we know the best k-junta has most its Fourier mass below level . For the latter

case, there are only

|S | 

Fourier coefficients to estimate, and estimating these to sufficient accuracy

allows one to estimate the the correlation f has with any subset U  S such that |U |  k.

We are now ready to present the details of the algorithm. The algorithm can be broken down

into two main steps. First, we find, with high probability, a set B  T that captures almost all

Fourier mass of T above level . This first step, which we call "phase one", closely resembles the

techniques in Section 4 in that we utilize a series of random restrictions to estimate normalized

influences. The main difference is that rather than considering normalized influences of individual

coordinates, we now consider normalized influences of sets of size . The goal of phase one is to

produce at least one subset B of our target set T which effectively captures most of the Fourier

mass within T above level . Once we have done that, we have reduced to the scenario of the closest

k-junta to f having most of its Fourier mass below level , which can be solved via estimating all

of the Fourier coefficients below level .

5.1 Phase One: The Higher Levels
First, we prove an analogous theorem to Theorem 4.11, which relates U [f ] to NInf U [f ] for all U :

24

Theorem 5.1. Let f : {±1}  R. Let U  [], where  = |D| and |U |  k. Let

2|U | log(10k)

U [f ] =

Up-m [f ],

m=0

where

Up-m [f ]

=

(J,z

E
)Rpm

[fJ¯z

(U

)2

]

for

p

=

1-

1 2|U

|

.

Then,

1 2

·

NInf

k U

[f

]



U [f ]



3 · NInf U [f ].

Again, we postpone the proof of this to the end of this section in Section 5.3. The definition of

U [f ] is naturally algorithmic, and therefore we can design the following algorithm to approximate the values of U [f ] for all sets U of size  = k.

Algorithm 4: Estimating U 's Input: f : {±1}k  [-1, 1] along with a randomized algorithm A computing f (recall

Def. 2.10). Parameters 1 -  (confidence),  (additive error) and k.

Output: Estimates {U }|U|= for {U }|U|=.

1 2

Let m = poly(k, k, 1/, log(1/)) Initialize U = 0 for all U  [k],

 |U | =  = k

3 Let p =

1

-

1 2

4 for d = 0 to 2 log 10k do

5 Initialize Up-d = 0 for all U  [k] such that |U | =  6 repeat m times

7

Let (J, z)  Rpd be a pd-random restriction.

8

Estimate

fJ¯z(U )

for

all

U



J

of

size



up

to

additive

error

 12 log(10k)

with

probability

1-



(k 

)m·2

log(10k)

using

Claim

2.11

and

algorithm

A.

Denote

by

fJ¯z(U ) the estimated Fourier coefficient.

9

Update Up-d = Up-d + fJ¯z(U )2 for all U  J of size .

10

Let Up-d = Up-d/m for all U  J of size ;

11 Let U = d Up-d ;

12 return {U }|U|=

Lemma 5.2. With probability at least 1 -  we have that for all U  [k] of size  it holds that |U - U [f ]|  .

Proof. This proof closely follows that of Lemma 4.12. If U  J the Fourier coefficient of fJ¯z(U ) is 0 and so our estimate is correct in that case. In the case U  J, each estimation of the Fourier

coefficient

is

correct

up

to

additive

error



=

 12 log(10k)

with

probability

at

least

1-/ exp(k, k, m).

Thus, we get that fJ¯z(U )2 = (fJ¯z(U ) ± )2 = fJ¯z(U )2 ± 2|fJ¯z(U )| ± 2 = fJ¯z(U )2 ± 3. Furthermore, we have that E(J,z)Rpd [fJ¯z(U )2] = Up-d, thus by Fact 2.1 we have that the

empirical mean of m = poly(1/, poly(k), poly(k), log(1/)) copies of fJ¯z(U )2 is within additive

error

/(4 log(10k))

from

Up-d

with

probability

at

least

1-

(k 


)m·2

log(10k)

.

By

union

bound,

all

these estimates are within the error bound, and we get that

Up-d - Up-d  3 + /(4 log(10k))  /(2 log(10k)).

Overall, we get that |U - U [f ]|   for all |U | =  with probability at least 1 - .

25

Since we are sampling sets of size , we need to sample at most k/ = k/ =:  distinct subsets of T of size  in order to capture all the potential mass of T above level .
Algorithm 5: Branching Process
Input: f (target function), D (where D are coordinate oracles for S) a current depth t, a current subset D  D of coordinate oracles, , 
Output: Return collection of subsets of D of size at most k. 1 Let  = k/ = k/ 2 Let r = O(1/2) and  = 2(r + 1)3+log(2/)
/* r + 1 is the branching factor, and  is an upper bound on the number of nodes in the branching process (the process depth is 3 + log(2/)). */
3 if t = 3 + log(2/) or |D| > k -  then 4 return {D}
5 Let {g1, ..., gk } = D - D and {gk+1, ..., g|D|} where k = |D| - |D| 6 Sample z  {±1}|D|. Let f  : {±1}k  R be the function defined by

f

(x1,

.

.

.

,

xk )

=

E
y{±1}n

[f

(y)|g1

(y)

=

x1,

.

.

.

,

gk (y)

=

xk ,

gk+1(y)

=

z1,

.

.

.

,

g|D|(y)

=

z|D|],

and let A be the randomized algorithm for f  from Theorem 4.10.

7

Apply

Algorithm

4

on

f

using the randomized

algorithm

A

for

f

with

confidence

1-

 2

and

accuracy

2
48·(|D |)

=  = {U }|U|=.

8 Let our distribution P be defined by , normalized appropriately

9 Sample M1, ..., Mr   10 Let L = {}.
11 for M = , M1, ..., Mr do 12 L = L  BranchingProcess(f, D, t + 1, D  {gi : i  M }, , )

13 return L

Lemma 5.3. With probability at least 1 - , at least one of the subsets Algorithm 5 returns is a set of coordinate oracles to B  T such that

E
z

fBz(S)2  2/4.

(4)

ST \B

|S|>

The reason for Equation (4) becomes clear in Section 5.2, where we show that assuming the

inequality, we lose at most an additive error of /2 to the nearest k-junta if we ignore the Fourier

mass above level  after restricting B. As before, in order to prove the above lemma, we prove a

claim capturing the algorithm's progress towards satisfying Equation (4).

We denote the event E that in the entire execution of Algorithm 5 all of the U

were 2/48 ·

|D| 

close to the real U . We note that by a union bound, this happens with probability at least 1 - /2.

Suppose again that T is the (unknown) set of k coordinates for which the best k-junta

approximating f is a junta on T . If T has Fourier mass less than 2/4 above level  then one

of the subsets that Algorithm 5 will return is the empty set, which satisfies the claim. Therefore,

henceforth we assume that T has at least 2/4 Fourier mass above level . We show that in such

a case, each Mi for i = 1, . . . , r will be a subset of T with probability at least (2).

26

Claim 5.4. Assume D are coordinate oracles to S  T . Suppose also that

E
z

fSz(S)2 > 2/4.

ST \S

|S|>

Then, conditioned on E, when running the Branching Process on D, each Mi will be with probability at least 2/40 a collection of  new coordinate oracles to coordinates in T .

Proof. Similar to the proof of Claim 4.14, denote by fz = (favg,S )Sz, and note that f  is up to relabeling of coordinates the same function as fz. Denote V  T as the part of the target set we have not yet sampled, so V = T \ S. Then, using our assumption, we have that

2/4 < E
z

fS  z (S )2

SV

|S|>

=

f (R)2

SV |S|>
=

R[n]: RS  =S
f (R)2

SV RS: |S|> RS=S

=
SV |S|>

favg,S (R)2
RS : RS  =S

(Fact 2.14) (if R  S then R  S = S)

=E
z

fz(S)2 .

SV

|S|>

Next, by applying Theorem 5.1, we have that

U [fz]



1 2

NInf Uk[fz]



1 2

fz (S )2
|S|

=

1 2

fz (S )2

U V

U V

U V :|U |= S:U SV |U |

|S|>

|U |=

SV

Then,

using

the

assumption

that

E

happens,

the

U

are

2
48·(|S|)

-accurate,

and

we

get

that

U V

U [fz]



1 2

fz (S )2

|S|>

-

2

48 ·

|S | 

·

k 



1 2

fz (S )2

-

2 48

.

|S|>

|U |=

SV

SV

On the other hand, again by applying Theorem 5.1, we have that

This implies that least

U [fz]  3

NInf U [fz]  3W[fz]  3.

U S |U |=

U S |U |=

U U



3+

2
48·(|S|)

·

|S | 

 4. Overall, the probability to sample U  V is at





1 4



1 2

fz (S )2

-

2 48



=

1 8

fz (S )2

-

4

2 · 48

.

|S|>

|S|>

SV

SV

27

Taking an expectation over z, we see that the probability to sample a subset of V is at least

1

E
z

8

fz (S )2

-

4

2 · 48



1 8

·

2 4

-

2 4 · 48



2 40

.

|S|>

SV

We are now ready to prove Lemma 5.3.

Proof of Lemma 5.3. By Claim 5.4, if our special set T has at least 2/4 mass on the levels above , then if we sample according to our distribution  = {U }|U|=, we will see U  T with probability at least 2/40. Then, if we sample r = O(-2) subsets in Algorithm 5, applying the multiplicative Chernoff bound in Fact 2.1, we see at least one subset of T with probability at least p  0.9 each time we sample M1, ..., Mr in Algorithm 5. In order for Algorithm 5 to successfully find Bi with the desired property, it suffices to have sampled from T at least  times in our branching process. Therefore, we can treat our N := (3 + log(2/)) depth branching process as a X = Bin(N, p) random variable. Applying a standard Chernoff bound (second case in Fact 2.1), we have that our probability of failure is

Pr[X

<

]

= Pr[X

<

 N

]

= Pr[X

<

0.9

-

(0.9

-

 N

)]



exp(-2N (0.9

-

 N

)2)



exp(-2N

(0.81

-

2

 N

))

 exp(-1.5N + 4)

 exp(- log(2/)) = /2.

(Using Fact 2.1)

This shows that, by a union bound with event E, one of the branches of our algorithm find's a Bi satisfying Equation (4) with probability at least 1 - .

Claim5.5. The query complexity of phase one of the algorithm for constant  (failure probability) is 2O( k/).

Proof. All of our queries to f in phase one come from estimating fourier coefficients using Claim 2.11 in Algorithm 4. We require that the estimatedFourier coefficients be accurate to within 1/poly(k, 1/) with confidence 1 - O(1/) = 1 - 2-( k/), which is possible via Fact 2.1 with query complexity poly(k/). However, we do this O() = 2O( k/) times during the branching process, which yields the final overall query complexity.

5.2 Phase Two: The Lower Levels
Now, we are ready to use Algorithm 5. Our strategy will be to take the subsets outputted from Algorithm 5 one at time, randomly fixing those coordinates, and then treating this restricted version of f as if all its Fourier mass were below level  (recall that  = k). Let T be the target set of size k on which there exists a k-junta which best approximates f . Assume that the first part of the algorithm is successful in yielding at least one B  T such that:

E

fBz(S)2  2/4.

(5)

z{±1}B ST \B

|S|>

28

Let g be the maximizer of maxgJT E[f g]. Recall that by Claim 2.7 we have that g = sgn(favg,T ) and

corr(f, JT ) = E[f g] = E [|favg,T (y)|] = E

E

(favg,T )Bz(x)

(6)

y{±1}T

z{±1}B x{±1}T \B

=E

E

fBz(S)S(x) (7)

z{±1}B x{±1}T \B ST \B

Furthermore, using the assumption in Eq. (5) it is an easy calculation to show that (7) equals

E

E

fBz(S)S(x) ± /2.

z{±1}B x{±1}T \B ST \B,|S|

Similarly, for any set U  S of size k containing B (think of U as a candidate for T ) we have that the best correlation between a junta-on-U and f is

corr(f, JU ) = E

E

fBz(S)S (x) .

(8)

z{±1}B x{±1}U\B SU \B

Now, however, the right hand side in Eq. (8) is not necessarily approximated by the low-degree counterpart as above for T . Indeed, we would like to estimate Eq. (8) for all candidates U  S of size k containing B, and pick the set with best estimated correlation. Based on our assumption
on T , we can replace SU\B fBz(S)S(x) with its low-degree part SU\B,|S| fBz(S)S(x) for U = T , but its not clear whether we can do it in general.
In particular, if U satisfies

E

fBz(S)2 > 2/4,

(9)

z{±1}B SU \B,

|S|>

then taking the low-degree part can give an overestimate to the correlation with the best junta on U .5 We settle for an estimate that is -accurate for the target set T assuming it satisfies
Equation (5), and is not overestimating by more than  for any other set U  B of size k. Towards
this goal, we first apply a noise operator that would essentially eliminate most of the contribution
from sets larger than k/ log(1/) regardless of whether U satisfies Eq. (15) or not. This is
captured by the following claim. 
Claim 5.6. Let  = 1 - /k, z  {±1}B and denote by h = fBz and hlow = h( k/)·log(1/) (i.e., hlow is the truncated Fourier expansion of h that zeroes out all Fourier coefficients above level
( k/) · log(1/)). For any U : B  U  S it holds that

corr (Th, JU ) - corr Thlow, JU  .
Proof. We have
corr (Th, JU ) - corr Thlow, JU
5To see a simple example of how this can happen, consider f (x, y) = 1 - x - y + xy. Then one can verify that E[|f (x, y)|] = 1 < 1.5 = E[|1 - x - y|].

29

=E

h(S )S (x)S

x{±1}U\B SU \B

-E
x{±1}U \B

h(S )S (x)S SU \B,
|S|( k/)·log(1/)

E
x{±1}U \B

h(S )S (x)|S | SU \B,
|S|>( k/)·log(1/)

2



E

h(S )S (x)|S |

x{±1}U \B

SU \B,

|S|>( k/)·log(1/)



=

h(S)22|S|  2( k/)·log(1/)  .

SU \B,
|S|>( k/)·log(1/)

Next, we show that applying a noise operator to f does not affect its correlation with a set

U of size k, under the condition that most of the Fourier mass of fBz falls on the lower levels,

i.e., Ez

 SU \B,|S| k

fBz (S )2

 2/4. Recall that this is what was guaranteed with high

probability from the output of Algorithm 5 for our target set T .

Claim 5.7. Let  = 1 - k/. Given U : B  U  S such that Ez SU\B, fBz(S)2

we have that

|S|

E
z

corr(T(fBz

),

JU

)

-

E
z

corr(fBz ,

JU

)

 1.2.

 2/4,

Proof. Similar to the proof of Claim 5.6, we have

E

fBz(S)S (x) - E

fBz(S)S (x) · |S|

z{±1}B x{±1}U \B

SU \B

z{±1}B x{±1}U \B

SU \B

E
z{±1}B x{±1}U \B

fBz(S)S(x)(1 - |S|)
SU \B



2



E

fBz(S)S (x)(1 - |S|)

z{±1}B x{±1}U \B

SU \B

=

E

fBz(S)2 · (1 - |S|)2

z{±1}B SU \B



E
z{±1}B

fBz(S)2 · (1 - |S|)2 +

fBz(S)2 · (1 - |S|)2

SU \B:|S|

SU \B:|S|>

 (1 - )2 + 2/4  2 + 2/4  1.2 · .

The next lemma gives an algorithm that on any B, satisfying Equation (5), outputs U : B  U  S with corr(f, JU )  corr(f, JT ) - O(), with high probability.

30

Lemma 5.8 (Algorithm and Analysis for Phase-Two). Let ,  > 0. There's an algorithm that with probability at least 1 - , gives -accurate estimates cU to

cU = E

E

z{±1}B x{±1}T \B



fBz(S)S (x)|S|

SU \B:|S| k/·log(1/)

for all U : B  U  S of size k simultaneously. We return (U, cU ) for the set U with maximal cU . 
Complexity The procedure uses log(1/)2O( k/) queries and runs in time log(1/)2k·O(1/).
Correctness In the case where all estimates are -accurate, the following holds. If B  T satisfies Equation (5), the above procedure would return (U, cU ) with cU  corr(f, JT )-3.2. Moreover, regardless of whether T and B satisfy Equation (5), we have cU  corr(f, JU ) + 2.

Proof. First we show that we can estimate all cU up to error  simultaneously with high probability using the aforementioned query complexity and running time. We sample t = O(log(1/)/2)

different z  {±1}B, and estimate for each value of z the Fourier coefficientsof fBz(S) of all sets

S  S of size at most  =

k/

·

log(

2 

)

up

to

additive

error

/

k 

= 2-(

k/) with probability

1

-


t·(k)

,

which

is

possible

via

Fact

2.1

with

log(1/)2O(

k/) queries. Fact 2.1 guarantees that

with probability 1 -  for all sampled z, all estimated low-degree Fourier coefficients are within the

additive error bound, in which case we have estimates for all cU up to error  simultaneously with

probability 1 - .

Next, we show the correctness of the procedure. On the one hand, in the assumed case, i.e.,

that T satisfies Ez

ST \B,|S| fBz(S)2



2 4

,

we

will

have

by

Claim

5.6

and

Claim

5.7

that

cT  corr(f, JT ) - 2.2

(10)

Since we output the set U with maximal cU , and since all estimates are correct up to  we know

that we output U with

cU  cT  cT - .

(11)

Combining Equations (10) and (11) together we get

cU  cT -   corr(f, JT ) - 3.2.

We move to prove the furthermore part, i.e., that cU  corr(f, JU ) + 2 regardless of whether T and B satisfy Equation (5). We start by showing that for any set U (whatsoever) we have that corr(f, JU )  cU - . Indeed, by Claim 5.6 we have

cU  E

fBz(S)S (x)|S|

z{±1}B x{±1}U \B

SU \B

and since the noise operator can only reduce 1-norm (see Fact 2.8), we see that for all z  {±1}B it holds that

E

fBz(S)S(x)|S|  E

fBz(S)S (x)

x{±1}U\B SU \B

x{±1}U\B SU \B

31

Thus,

cU   + E

fBz(S)S (x)|S|

z{±1}B x{±1}U \B

SU \B

+ E

fBz(S)S (x) =  + corr(f, JU )

z{±1}B x{±1}U \B

SU \B

Since |cU - cU |  , we get that cU  cU +   corr(f, JU ) + 2.
After phase one, we can apply Lemma 5.8 to each B from phase one, and get a set UB : B  UB  S of size k, along with an estimate of the correlation of f to JUB . This leads to the proof of Theorem 1.2 which we restate next.
Theorem 5.9. Given a Boolean function f : {±1}n  {±1}, it is possible to estimate the distance of f from the class of k-juntas to within additive error  with probability 2/3 using 2O( k/) adaptive queries to f . In particular, when  is constant, this yields a 2O( k)-query algorithm. However, the algorithm still requires exp(k/) time.

Proof. Let 0 = /6
1. We first apply the result of [DMN19] to reduce the down to only poly(k, 1/0) coordinates. This incurs a loss in correlation of at most 0, and fails with probability at most 1, which we can set to be 1/20, by Corollary 4.3.

2. Next, we apply our Theorem 1.4, which reduces the number of oracles we have to consider down to O(k/20), incurs an additive loss in correlation of at most 0, and fails with probability at most 2 = 1/20.
3. Then, we run phase 1 of our algorithm, which fails with probability at most 3 = 1/20 by Lemma 5.3.

4. Finally, we apply Lemma 5.8 to every B outputted by Algorithm 5 to get a set UB and an

estimate CUB for the correlation of f with JUB We iterate on all sets B returned by phase-1

and return UB with the highest estimate of correlation.

There

are



=

O(

1 20

)3

k/0+log(2/3) = 2O(

k/0) branches, and thus if we apply the algorithm

from lemma 5.8 with  = 1/(20), we get that all this step fail with probability at most 1/20

by a union bound.

By a union bound, each of these steps succeeds with probability at least 1 - 4/20  2/3. In the

case all steps succeeds, we return a set U with cU  corr(f, Jn,k) - 5.20. In addition, the moreover

part in Lemma 5.8 guarantees that cU  corr(f, JU ) + 20  corr(f, Jn,k) + 20. We get that the

returned value is within 5.20 <  of corr(f, Jn,k).

Finally, since dist(f, Jn,k) =

1+corr(f ,Jn,k ) 2

we

get that

1+cU 2

is an /2-accurate approximation of dist(f, Jn,k).

Finally, we note that the query

complexities of phase 1 and phase 2 are both 2O( k/), but the runtime is exponential due to

lemma 5.8.

Finally, we mention that if our goal is not to estimate to correlation with the nearest k-junta to f , but rather to simply estimate the most amount of Fourier mass any subset of k variables contains, then we have the following theorem with an improved dependence on :

32

Theorem 5.10. Given a Boolean function f : {±1}n  {±1}, it is possible to estimate the most mass any subset of at most k variables of f has to within additive error  with probability 2/3 using 2O( k log(1/)) adaptive queries to f . In particular, when  is constant, this yields a 2O( k)-query algorithm. However, the algorithm still requires exp(k log(1/)) time.
We leave the proof of this theorem, which involves simple modifications to the algorithm presented in this section, to Appendix A.

5.3 Proof of Theorem 5.1
We now present the proof of Theorem 5.1.

Proof of Theorem 5.1. The proof is very similar to the previous proof of Theorem 4.11, so we explain how to modify it to this case.
We express U in terms of the Fourier spectrum of f .

2|U | log(10k)

U =

f (S)2 · Pr [S  J = U ]

m=0 S:SU

J pm []

2|U | log(10k)

=

f (S)2 · Pr [|S  J| = |U |] ·

m=0 S:SU

J pm []

1
|S| |U |

=

S:SU

f (S)2
|S| |U |

·

2|U | log(10k)

m=0

Pr [|S
J pm []



J|

=

|U |]

It suffices to show that for any non-empty set S of size at least |U | and at most k it holds that

2|U | log(10k)

Pr [|S  J| = |U |]  [1/2, 3] .

m=0

J pm []

(12)

Again, we can analyze the sum on the left hand side of Equation (12) as the expected final value of X in the following random process:

1 X0

2 for i = 1, 2, . . . , 2|U | log(10k) do

3 if |S| < |U | then

4

halt!

5 if |S| = |U | then

6

increase X

7 Sample Ji p [] 8 S  S  Ji

By symmetry the expected value depends only on the size of the initial set S. As before, we denote by Ft its expected value starting with a set S of size t with an infinite horizon, and Ft(i) as
the expected value of X at the end of the above process with finite horizon i. We start by analyzing

33

F|U|. In this case, X is a geometric random variable with stopping probability 1 - p|U|. Thus, its
expectation is F|U| = 1/(1 - p|U|) = 1/(1 - (1 - 1/2|U |)|U|)  [2, 3].

This implies that F|(U2||U| log(10k))  F|U|  3. For t > |U | in the infinite horizon case we have the recurrence

t

t-1

Ft = Fa · Pr[Bin(t, p) = a] =

Fa · Pr[Bin(t, p) = a] + Ft · Pr[Bin(t, p) = t] (13)

a=0

a=|U |

or equivalently
t-1

Ft · Pr[Bin(t, p) < t] =

Fa · Pr[Bin(t, p) = a]

(14)

a=|U |

We prove by induction that for t  |U | it holds that Ft  F|U|. The claim clearly holds for t = |U |. For t > |U | we can apply induction and get

t-1

Ft · Pr[Bin(t, p) < t] 

F|U| · Pr[Bin(t, p) = a]  F|U| · Pr[Bin(t, p) < t],

a=|U |

and thus Ft  F|U|. This immediately implies that Ft(2|U| log(10k))  Ft  3. On the other hand we prove that Ft(2|U| log(10k))  1/2 as long as t  k. To do so, we once again introduce the indicator random variable Yt(i), where t = |S|, and which equals 1 if |S| = |U | at some point during the above process before iteration i. We note that Yt(2|U| log(10k)) is a lower bound for the value of X
in the above process, and Yt is a lower bound for the value of X at the end of the infinite horizon

process. We note that the case |U | = 1 was already lower bounded in Section 4.5, where it was

shown that E[Yt(log(10k))]  1/2, and therefore E[Yt(2|U| log(10k))]  1/2. It remains to show that the

E[Yt(2|U| log(10k))]  1/2 is true for any set |U |  2.

First, we show that Pr[Bin(t, p) < |U |] 

1 2

Pr[Bin(t, p)

=

|U |].

Towards this goal,

it would

suffice to prove that 3  Pr[Bin(t, p) = i + 1]/ Pr[Bin(t, p) = i] for i < |U | and t  |U | + 1. This

would suffice since in this case

|U |-1
Pr[Bin(t, p)
i=0

=

i]



|U |-1 i=0

3i 3|U |

Pr[Bin(t, p)

=

|U |]



1 2

·

Pr[Bin(t, p)

=

|U |].

Indeed, The ratio between the two aforementioned probabilities is

Pr[Bin(t, p) = i + 1] Pr[Bin(t, p) = i]

=

t i+1
t i

·

pi+1(1 - p)t-(i+1) pi(1 - p)t-i

=

t i

- +

i 1

·

1

p -

p



2 |U

|

·

1

- 1/2|U | 1/2|U |

=

2 - 1/|U | 1/2

3

as needed. Now, we claim that E[Yt] = Pr[Yt = 1]  2/3 for all t  1. The base case of t = 1 is

certainly

true.

Assuming

we

have

Pr[Bin(t, p)

<

|U |]



1 2

Pr[Bin(t, p)

=

|U |]

we

have

t-1

E[Yt] · Pr[Bin(t, p) < t] =

E[Ya] · P r[Bin(t, p) = a]

a=|U |

t-1

 Pr[Bin(t, p) = |U |] +

Pr[Bin(t, p) = a] E[Ya]

a=|U |+1

34



Pr[Bin(t,

p)

=

|U |]

+

2 3

Pr[Bin(t,

p)



[|U |

+

1,

t

-

1]]

=

2 3

Pr[Bin(t, p)

<

t]

-

2 3

Pr[Bin(t,

p)

<

|U |]

+

1 3

Pr[Bin(t,

p)

=

|U |]



2 3

Pr[Bin(t, p)

<

t]

which implies that E[Yt]  2/3. Finally, let A be the event that S =  by iteration 2|U | log(10k), and note that

Pr[A]

=

Pr[Bin(|S|, (1

-

1 2|U

|

)2|U

|

log(10k)

)

=

0]



Pr[Bin(k,

e-

log(10k))

=

0]

=

Pr[Bin(k,

1 10k

)

=

0]



0.9

as was shown in the proof for Theorem 4.11 in Section 4.5. Finally, we claim that for all t  2 we have that Pr[Yt(2|U| log 10k)]  1/2. Indeed, we have that

Pr[Yt(2|U | log 10k))

=

1]



Pr[Yt

=

1] -

Pr[A]



2 3

- 0.1



1 2

.

as desired, provided |S|  k.

6 Conclusions and Open Problems
We conclude by mentioning some future research directions. First, we believe some of the techniques discussed in this paper could lead to other interesting work in property testing, learning theory, or Boolean function analysis in general. In particular, the procedure in Algorithm 1 makes use of a random process to get access to an underlying junta, a subprocedure that could be useful in other learning or testing algorithms. In addition, we are able to approximate the quantities NInf i and NInf U , that serve as key steps in our algorithms. These quantities seems natural on their own, and would likely find further applications in Analysis of Boolean functions. In particular, they seem to capture more accurately the intuition that "influences measures the importance of coordinates". While the total influence of a Boolean function can be any number between Var[f ] and n · Var[f ] the total normalized influence equals exactly Var[f ], and thus normalized influences can be seen as a distribution of the variance among the coordinates.
Interestingly, our algorithms strongly resemble certain quantum algorithms. In particular, the sampling of coordinates is done through the Fourier distribution, a process which can be done much more efficiently with a quantum algorithm (querying f in superposition, applying the Hadamard transform, and measuring). This idea was leveraged in [Amb+16] to provide fast quantum algorithms for testing juntas in the standard property testing regime. Indeed, if the nearest k-junta to f has its mass on higher levels (say above k or even k/2), then Fourier sampling is extremely effective and provides a cleaner way of sampling subsets according to the Fourier distribution than the related classical technique we provided in Section 5. However, the issue arises when the nearest k-junta has Fourier mass on lower levels (below log k or even a constant, for example). In this case, it is not clear to us how quantum algorithms provide any advantage over classical ones. An open question is whether quantum Fourier sampling techniques can be applied in a more clever way to give faster algorithms in the tolerant testing paradigm.
Finally, a clear open question is how good of a lower bound one can prove on the query complexity of the tolerant junta testing problem. Our main result Theorem 1.2, rules out strictly exponential-in-k query lower bounds for k-junta distance approximation. [PRW20] proved a nonadaptive query complexity lower bound of 2k for (k, k, 1, 2)-tolerant junta testing (given a
35

particular choice of0 < 1 < 2 < 1/2), for any 0 <  < 1/2. While this is quite close to our upper bound of 2O( k), our algorithm is highly adaptive, while the lower bound due to [PRW20] applies only to nonadaptive algorithms. Therefore, another interesting direction would be to explore whether any nontrivial lower bounds apply to adaptive algorithms for tolerant (junta) testing and distance approximation.
Acknowledgements
We thank Anindya De, Shafi Goldwasser, Amit Levi, and Orr Paradise for very helpful discussions.

References

[Ail+07]

Nir Ailon, Bernard Chazelle, Seshadhri Comandur, and Ding Liu. "Estimating the distance to a monotone function". In: Random Structures & Algorithms 31.3 (2007), pp. 371­383. doi: https://doi.org/10.1002/rsa.20167. eprint: https://onlinelibrary.wiley.co url: https://onlinelibrary.wiley.com/doi/abs/10.1002/rsa.20167.

[Amb+16] Andris Ambainis, Aleksandrs Belovs, Oded Regev, and Ronald de Wolf. "Efficient Quantum Algorithms for (Gapped) Group Testing and Junta Testing". In: SODA. SIAM, 2016, pp. 903­922.

[BGS98]

Mihir Bellare, Oded Goldreich, and Madhu Sudan. "Free Bits, PCPs, and Nonapproximability-Towards Tight Results". In: SIAM J. Comput. 27.3 (1998), pp. 804­915.

[Bla+19]

Eric Blais, Cl´ement L. Canonne, Talya Eden, Amit Levi, and Dana Ron. "Tolerant Junta Testing and the Connection to Submodular Optimization and Function Isomorphism". In: ACM Trans. Comput. Theory 11.4 (2019), 24:1­24:33.

[Bla08]

Eric Blais. "Improved Bounds for Testing Juntas". In: APPROX-RANDOM. Vol. 5171. Lecture Notes in Computer Science. Springer, 2008, pp. 317­330.

[Bla09] Eric Blais. "Testing juntas nearly optimally". In: STOC. ACM, 2009, pp. 151­158.

[BLR90] Manuel Blum, Michael Luby, and Ronitt Rubinfeld. "Self-Testing/Correcting with Applications to Numerical Problems". In: STOC. ACM, 1990, pp. 73­83.

[BLT20] Guy Blanc, Jane Lange, and Li-Yang Tan. "Testing and reconstruction via decision trees". In: CoRR abs/2012.08735 (2020).

[Bsh19]

Nader H. Bshouty. "Almost Optimal Distribution-Free Junta Testing". In: Computational Complexity Conference. Vol. 137. LIPIcs. Schloss Dagstuhl - Leibniz-Zentrum fu¨r Informatik, 2019, 2:1­2:13.

[CG04]

Hana Chockler and Dan Gutfreund. "A lower bound for testing juntas". In: Inf. Process. Lett. 90.6 (2004), pp. 301­305.

[Cha+12] Sourav Chakraborty, Eldar Fischer, David Garcia-Soriano, and Arie Matsliah. "JuntoSymmetric Functions, Hypergraph Isomorphism and Crunching". In: Computational Complexity Conference. IEEE Computer Society, 2012, pp. 148­158.

[Che+18]

Xi Chen, Rocco A. Servedio, Li-Yang Tan, Erik Waingarten, and Jinyu Xie. "Settling the Query Complexity of Non-adaptive Junta Testing". In: J. ACM 65.6 (2018), 40:1­ 40:18.

36

[Dia+07] [Dia+08] [DMN19] [Fis+04] [GGR96]
[H°as01] [Kel+20] [KKL88] [KR00] [Liu+19] [LW19] [MOS03] [ODo14] [OW13] [PRR04] [PRS01] [PRW20]

Ilias Diakonikolas et al. "Testing for Concise Representations". In: FOCS. IEEE Computer Society, 2007, pp. 549­558.
Ilias Diakonikolas, Homin K. Lee, Kevin Matulef, Rocco A. Servedio, and Andrew Wan. "Efficiently Testing Sparse GF(2) Polynomials". In: ICALP (1). Vol. 5125. Lecture Notes in Computer Science. Springer, 2008, pp. 502­514.
Anindya De, Elchanan Mossel, and Joe Neeman. "Junta Correlation is Testable". In: FOCS. IEEE Computer Society, 2019, pp. 1549­1563.
Eldar Fischer, Guy Kindler, Dana Ron, Shmuel Safra, and Alex Samorodnitsky. "Testing juntas". In: J. Comput. Syst. Sci. 68.4 (2004), pp. 753­787.
Oded Goldreich, Shafi Goldwasser, and Dana Ron. "Property Testing and Its Connection to Learning and Approximation". In: 37th Annual Symposium on Foundations of Computer Science, FOCS '96, Burlington, Vermont, USA, 14-16 October, 1996. IEEE Computer Society, 1996, pp. 339­348. doi: 10.1109/SFCS.1996.548493. url: https://doi.org/10.1109/SFCS.1996.548493.
Johan H°astad. "Some optimal inapproximability results". In: J. ACM 48.4 (2001), pp. 798­859.
Esty Kelman, Subhash Khot, Guy Kindler, Dor Minzer, and Muli Safra. "Theorems of KKL, Friedgut, and Talagrand via Random Restrictions and Log-Sobolev Inequality". In: Electron. Colloquium Comput. Complex. 27 (2020), p. 9.
Jeff Kahn, Gil Kalai, and Nathan Linial. "The Influence of Variables on Boolean Functions (Extended Abstract)". In: FOCS. IEEE Computer Society, 1988, pp. 68­80.
Michael J. Kearns and Dana Ron. "Testing Problems with Sublearning Sample Complexity". In: J. Comput. Syst. Sci. 61.3 (2000), pp. 428­456.
Zhengyang Liu, Xi Chen, Rocco A. Servedio, Ying Sheng, and Jinyu Xie. "Distributionfree Junta Testing". In: ACM Trans. Algorithms 15.1 (2019), 1:1­1:23.
Amit Levi and Erik Waingarten. "Lower Bounds for Tolerant Junta and Unateness Testing via Rejection Sampling of Graphs". In: ITCS. Vol. 124. LIPIcs. Schloss Dagstuhl - Leibniz-Zentrum fu¨r Informatik, 2019, 52:1­52:20.
Elchanan Mossel, Ryan O'Donnell, and Rocco A. Servedio. "Learning juntas". In: STOC. ACM, 2003, pp. 206­212.
Ryan O'Donnell. Analysis of Boolean Functions. Cambridge University Press, 2014. isbn: 978-1-10-703832-5. url: http://www.cambridge.org/de/academic/subjects/computer-scie
Ryan O'Donnell and Karl Wimmer. "Sharpness of KKL on Schreier graphs". In: Electronic Communications in Probability 18.none (2013), pp. 1­12. doi: 10.1214/ECP.v18-1961. url: https://doi.org/10.1214/ECP.v18-1961.
Michal Parnas, Dana Ron, and Ronitt Rubinfeld. "Tolerant Property Testing and Distance Approximation". In: Electron. Colloquium Comput. Complex. 010 (2004).
Michal Parnas, Dana Ron, and Alex Samorodnitsky. "Proclaiming Dictators and Juntas or Testing Boolean Formulae". In: RANDOM-APPROX. Vol. 2129. Lecture Notes in Computer Science. Springer, 2001, pp. 273­284.
Ramesh Krishnan S. Pallavoor, Sofya Raskhodnikova, and Erik Waingarten. "Approximating the Distance to Monotonicity of Boolean Functions". In: SODA. SIAM, 2020, pp. 1995­2009.

37

[Sag18] [Ser10] [Tal94]
[Val12] [Zha19]

Mert Saglam. "Near Log-Convexity of Measured Heat in (Discrete) Time and Consequences". In: FOCS. IEEE Computer Society, 2018, pp. 967­978.
Rocco A. Servedio. "Testing by Implicit Learning: A Brief Survey". In: Property Testing. Vol. 6390. Lecture Notes in Computer Science. Springer, 2010, pp. 197­210.
Michel Talagrand. "On Russo's Approximate Zero-One Law". In: The Annals of Probability 22.3 (1994), pp. 1576­1587. doi: 10.1214/aop/1176988612. url: https://doi.org/10.1214/aop/1176988612.
Gregory Valiant. "Finding Correlations in Subquadratic Time, with Applications to Learning Parities and Juntas". In: FOCS. IEEE Computer Society, 2012, pp. 11­20.
Xiaojin Zhang. "Near-Optimal Algorithm for Distribution-Free Junta Testing". In: CoRR abs/1911.10833 (2019).

A Maximum k-Subset Fourier Mass Approximation

In this section, we sketch a proof of Theorem 5.10, which involves simple modifications and observations about our algorithm. The main difference is that we sample from the normalized influence subdistribution at a different Fourier level ­ namely, we let  := k and  = k/ = k in Algorithm 4 and Algorithm 5, respectively (recall that before,  = k). This improves the query complexity dependence on  in Phase 1.

Claim A.1. The query complexity of phase one of the algorithm for constant  (failure probability) is 2O( k log(1/)).

Proof. The proof is analogous to the proof of Claim 5.5, so we just point out the differences.

We still require our Fourier coefficients to be accurate to within 1/poly(k, 1/), and we require

confidence O( k), so

1 - O(1/) = 1 - 2( we need only repeat

k log(1/)). this O()

However, now our branching process now has depth only = 2O( k log(1/)) times, which yields the improved query

complexity.

In Phase 2, we argue that it is not necessary to apply a noise operator in order to only consider Fourier mass below level  after Phase 1. Recall that we applied this noise operator in Section 5.2 in order to deal with the case that a particular U satisfied

E

fBz(S)2 > 2/4.

z{±1}B SU \B,

|S|>

(15)

If this happened, then we could not rule out the possibility that taking the low-degree part of f within U gives an overestimate to the correlation with the best k-junta. However, now we are not concerned with the junta correlation, but rather which set has the most mass, so we claim we do not have to worry about this possibility anymore. To see this, suppose we have identified B  U , and note that

f (S)2

=

E[f
x

(x)favg,U

(x)]

SU

=E
z{±1}B

Ex [fBz(x)(favg,U )Bz (x)]

38

=E
z

fBz(S)2 +

fBz (S )2

SU

SU

|S|

|S|>

E
z

fBz(S)2 .

SU

|S|

Therefore, we no longer have to apply any noise operator, which negates the necessity of Claim 5.6 and Claim 5.7. It therefore suffices in Lemma 5.8 to estimate the mass of each set, rather than the correlation, as

mU

=

E
z

fBz(S)2 .

SU

|S|

To do so, as in the proof of Lemma 5.8 we let t = O(log(1/)/2) be the number of random

samples of z we take. Then we estimate all the Fourier coefficients below level . This requires

estimating

f (S)

for

all

S



S

of

size

at

most



up

to

additive

error

/


k 

= 2( k log(1/)) with

probability

1

-


t·(k)

,

which

is

possible

via

Fact

2.1

with

log(1/)2O(

k log(1/)) queries.

The rest

of our argument and algorithm is exactly the same as in Section 5.

39

