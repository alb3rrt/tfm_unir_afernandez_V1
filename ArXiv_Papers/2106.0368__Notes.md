
# Natural Statistics of Network Activations and Implications for Knowledge Distillation

[arXiv](https://arxiv.org/abs/2106.0368), [PDF](https://arxiv.org/pdf/2106.0368.pdf)

## Authors

- Michael Rotman
- Lior Wolf

## Abstract

As a direct implication of our discoveries, we present a method for performing Knowledge Distillation (KD). While classical KD methods consider the logits of the teacher network, more recent methods obtain a leap in performance by considering the activation maps. This, however, uses metrics that are suitable for comparing images. We propose to employ two additional loss terms that are based on the spectral properties of the intermediate activation maps. The proposed method obtains state of the art results on multiple image recognition KD benchmarks.

## Comments

Accepted to ICIP 2021

## Source Code

Official Code



Community Code



## Bibtex

```tex
@misc{rotman2021natural,
      title={Natural Statistics of Network Activations and Implications for Knowledge Distillation}, 
      author={Michael Rotman and Lior Wolf},
      year={2021},
      eprint={2106.00368},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```

## Notes

Type your reading notes here...

