AI-assisted quantum many-body computation beyond Markov-chain Monte Carlo
Hongyu Lu,1 Chuhao Li,2, 3 Wei Li,4, 5 Yang Qi,6, 7 and Zi Yang Meng1 1Department of Physics and HKU-UCAS Joint Institute of Theoretical and Computational Physics,
The University of Hong Kong, Pokfulam Road, Hong Kong SAR, China 2Being National Laboratory for Condensed Matter Physics and Institute of Physics,
Chinese Academy of Sciences, Being 100190, China 3School of Physical Sciences, University of Chinese Academy of Sciences, Being 100190, China
4School of Physics, Beihang University, Being 100191, China 5Institute of Theoretical Physics, Chinese Academy of Sciences, Being 100190, China 6State Key Laboratory of Surface Physics, Fudan University, Shanghai 200438, China 7Center for Field Theory and Particle Physics, Department of Physics, Fudan University, Shanghai 200433, China
(Dated: June 9, 2021)
We find artificial neural networks can constructively help the Monte Carlo computations to provide better sampling and complete absence of autocorrelation between configurations in the study of classical and quantum many-body systems. We design generic generative neural-network architecture for the Ising and Hubbard models on two-dimensional lattices and demonstrate it can overcome the traditional computational complexity as well as the difficulty in generating uncorrelated configurations, irrespective of the system locating at the classical critical point, antiferromagnetic Mott insulator, correlated Dirac semimetal or the Gross-Neveu quantum criticality. Our work therefore paves the avenue for highly efficient AI-assisted quantum many-body computation beyond the Markov-chain Monte Carlo.

arXiv:2106.00712v3 [cond-mat.str-el] 8 Jun 2021

Introduction -- Neural network (NN) represents the prototypical deep learning artificial intelligence architectures with many successes, for example, in physical systems ranging from condensed matter to quantum material research [1­3]. However, early numerical studies have been mainly accessing the wavefunctions of (quantum) spin systems [4­6] and do not capture the partition function and path-integral of interacting fermion systems such as the Hubbard models where the connection with Mott insulator, non-Fermi liquid, pseudogap, unconventional superconductivity, as well as many other frontier and fundamental quantum many-body questions, lies in.
On the other hand, the previous attempts of learning effective low-energy Hamiltonian that could help with accelerating the (quantum) Monte Carlo sampling process, dubbed selflearning Monte Carlo scheme [7­13], have shown the power of AI-assisted quantum many-body computation in the aforementioned interacting electron systems such as non-Fermi liquid and quantum critical points [14, 15]. It inspired many extensions in both methological developments [16] in related questions such as Holstein model [17­19] and the exploration of fundametnal questions such as the hot-spots non-Fermi liquid at antiferromagnetic quantum critical point (QCP) [16, 20] as well as pseudogap emerging from quantum magnetic fluctuations [21]. Yet, at the bottom technical level, these developments still rely on the computational complexity of the underlying Markov-chain Monte Carlo (MCMC) schemes and have not enjoyed the fast technical developments in the deep NNs. Despite some recent efforts in designing advanced Monte Carlo updating schemes with the help of NNs [18, 22, 23], many of them are confined to classical systems and the intrinsic long autocorrelation time of MCMC has not been eliminated.
This is the knowledge gap we would like to fill in. In this work, we combine the merits of the above two aspects and propose a pivotal leap forward in the usage of deep NNs as an improvement over the simplified effective models in self-learning

scheme for direct sampling in quantum many-body systems. The obtained NN-based MC scheme, dubbed NNMC in this work, is markedly different from the traditional MCMC: First, being a direct sampling instead of a Markov-chain method, it avoids the autocorrelation problem in MCMC; second, based on the latest advances in generative NN models, it fully captures the statistics of the interacting electron systems. Therefore, after trainning, it provides legitimate configurations with minimal efforts that can be used to directly measure physical observables with much less computational complexity.
Below, we first demostrate how NNs can be trained to provide legitimate configurations for the lattice quantum Monte Carlo simulations. We then show how NNMC offers direct and ... sampling either in the spin or the auxiliary field bases. We further show that, with the help of a few additional MC steps, the NN-generated configurations obey the correct probability distribution associated with the interaction Hamiltonian, transcending the precision of the training data. The NNMC for classical and quantum spin and fermion systems do not suffer from the complexity and autocorrelation of the traditional MCMC, irrespective of the system locating at the classical critical point, antiferromagnetic Mott insulator, correlated Dirac semimetal or the Gross-Neveu QCP. Our work therefore paves the avenue for highly efficient AI-assisted quantum many-body computation for challenging and realistic interacting electron systems.
Model -- Our goal in this work is to train a generative NN that can easily generate MC configurations according to the distribution of the original Hamiltonian. To this end, we use random-noise configurations as input into the NN and obtain output of configurations with the shape as those from MC simulations (cf. Fig. 1). We examine the quality of the generative NN model by directly performing physical measurements over the generated configurations and compare the results with those obtained with MCMC simulations. For different phys-

(a)

( PBC )

Input 16×16

1@23×23

64@16×16

32@16×16

32@21×21

2
Output 16×16

PBC(7)

Conv2d 64@8×8

(b) Auxiliary Field Configuration

(c) Input
10×2×2

Conv2d 32@1×1

PBC(5)

20@20×5×5

Conv2d 1@6×6
Output 40×8×8

L L

Conv3dTranspose 20@11×4×4

Conv3dTranspose 1@21×4×4

FIG. 1. Schematic figures for the generative NNs used in this study. (a) shows an example of periodic boundary condition on the left, and
we use the NN structure on the right for the training of the 2d Ising model, where Conv2d stands for 2d convolutional layer. (b) displays the auxiliary field configuration in determinant quantum Monte Carlo (DQMC) for the Hubbard model, with the (2 + 1) space-time of  ×  × . The auxiliary fields of ±1 live on each space-time lattice site. (c) demonstrates the schematic structure of 3d transposed convolutional network we employed for the training of the 2d Hubbard model on 4 × 4, 8 × 8 (as example here) square lattices and 6 × 6 × 2 honeycomb lattice.

ical models studied, we construct different model structures implemented in Tensorflow [6, 18, 24­27], as we now explain.
For the 2d Ising model, we study the system at the critical point with system sizes of 16×16 and 32×32 on the square lattice. The input random configurations are of the same shapes. As shown in Fig. 1 (a) for the NN structure, we use three 2d convolutional layers, with 64 filters of size 8 × 8 for the first layer, 32 filters of size 1 for the second layer, and 1 filter of size 6 × 6 for the third layer. Besides, we choose rectified linear function ReLU(x) = max(0, x) in the first and second convolutional layers and sigmoid function () = 1/(1 + -) in the third. Inspired by the network structure in a previous work [27], we also apply periodic boundary condition (PBC) layers each time before the convolutional layers if the kernel dimension is larger than one, which provides the configuration tensors with paddings of boundary elements instead of 0s (the padding size is decided by the size of filters in the corresponding convolutional layer), so as to ensure the shape invariance of the configuration tensors after convolutional operations. In order to train such a NN, we take 1000 input random configurations while each will be processed into another generated configuration . In the meantime, we also prepare 1000 MC configurations  as well, and then design the loss function to compare the physical observables measured from those configurations { } and {}, i.e.,
({ }, { }) = 1 [| |({ }) - | |({ })]2

+ 2 [2 ({ }) - 2 ({ })]2 + 3 [ ({ }) -  ({ })]2 (1)
where 1, 2, and 3 are constants to balance each part of the loss,  refers to magnetization   , and  refers to energy   of the Ising system. We optimize the network parameters using Adam [28], which is an algorithm for first-order gradient-based optimization of stochastic objective functions,

based on adaptive estimates of lower-order moments with a

batch size of 5, and the training process takes 150 epochs.

In the case of 2d Hubbard model on the square lattice (8 × 8

for example), we switch from classical Ising configurations to

the auxiliary field configurations of the determinant quantum

MC (DQMC) [9, 29], with the configuration space of  ×  × 

where  = 1/ is the inverse temperature. We set  = 4 and

the Trotter discretization  = 0.1, so the DQMC auxiliary

field configurations are of shape 40 × 8 × 8, as schematically

shown in Fig. 1 (b). Inspired by the structure of generator in

typical generative adversarial network [30], here we replace

the previous PBC-based NN structure to two 3d transposed

convolutional layers with sigmoid function as activation and

valid padding. We find such NN structure behaves better than

the previous one for interacting fermion systems. There are 20

filters of size 11 × 4 × 4 in the first layer and 1 filter of size 21

× 4 × 4 in the second layer. The 1000 random input configu-

rations are of shape 10 × 2 × 2, and the predicted output are of

shape 40 × 8 × 8, shown in Fig. 1 (c). To optimize the network,

we directly use lists of physical observables { } (the antifer-

romagnetic structure factor (, ) 

  -Q·(r-r  )





 

with Q = (, ) obtained from equal time spin correlation

function) and {  } (kinetic energy     ) measured from

DQMC simulations of 1000 continuous sweeps for comparison

with the same DQMC measurement applied on the configura-

tions generated from the network. Therefore the loss function

is defined as

({ }, { }, {  }) =1 [({ }) - ]2

+ 2 [  ({ }) -  ]2,

(2) with constants 1 and 2, and Adam is again used as the optimizer, and we run 15 epochs with a batch size of 3.

3

Autocorrelation of  Autocorrelation of (, ) Autocorrelation of (0, 0)

(a) Ising on Square  = 2.269

1.0

 = 16 MC

0.8

 = 16 NN

 = 32 MC

0.6

 = 32 NN

0.4

0.2

0.0

0

100 200 300 400 500 600

/sweep(s)

(b) Hubbard on Square / = 4

1.0

 = 4 DQMC

0.8

 = 4 NN

 = 8 DQMC

0.6

 = 8 NN

0.4

0.2

0.0

0

2

4

6

8

10

/sweep(s)

(c) Hubbard on Honeycomb / = 3.83

1.0

 = 3 DQMC

0.8

 = 6 DQMC

 = 9 DQMC

0.6

 = 12 DQMC

0.4

 = 6 NN

0.2

0.0

0

2

4

6

8

10

/sweep(s)

FIG. 2. Comparison of autocorrelation functions. (a) shows the autocorrelation of magnetization  for 2d Ising model from both MCMC and NNMC at critical point  = 2.269 for  = 16, 32. The MC results exhibit the typical critical slowing down whereas the NN results have zero autocorrelation, i.e., direct ... sampling. (b) shows the autocorrelation for (, ) of Hubbard model on square lattice from DQMC and NN, we fix / = 4,  = 4,  = 0.05 for  = 4 and  = 0.1 for  = 8. The DQMC results exhibit autocorrelation as  increases whereas the NN
results have zero autocorrelation. (c) shows the autocorrelation for (0, 0) of Hubbard model on honeycomb lattice at the QCP between Dirac fermion and antiferromagnetic phase, at / = 3.83 from DQMC with  = 3, 6, 9, 12, the critical slowing down manifest. The NN results
again show no sign of autocorrelation.

For the Hubbard model on the honeycomb lattice, it is known that the model hosts a (2 + 1) Gross-Neveu Heisenberg quantum critical point between the Dirac semimetal and the antiferromagnetic Mott insulator [31­34]. The phase transition happens at /  3.83. We choose a system size of 6 × 6 at  and take  = 6,  = 0.1, so the auxiliary field configuration is of shape 60 × 12 × 6 (note the 2 site per unit cell for the honeycomb lattice). The training process and NN structure are the same as that for the square lattice, i.e., we directly take the physical observables { } (the antiferromagnetic order is within the unit cell of honeycomb lattice, so we take (0, 0) in this case) and {  } into the loss function in Eq. (2), and with minor changes in parameters of the NN layers due to difference in system size.
It is important to point out that after the generative NN is well trained, one can generate as many legitimate configurations ­ the configurations that follow the distribution in the path-integral of the Hamiltonian ­ as possible immediately by putting random configurations into the NN, in this way, the NNMC is superior in computational complexity than the traditional MCMC. For example, in the case of fermion models, the update of DQMC inevitably invoke matrix operations which at the best gives  (6) complexity for 2d Hubbard systems [9], such burden in generating legitimate configurations is removed in NNMC, and one can directly use the output configurations from NNMC to carry out measurements for physical observables, as we now show.
Results and Analysis -- In MCMC, the autocorrelation time of Markov chain, associated with the particular update scheme in the MC process, can be extremely long in the physically interesting parameter regime, for example, classical and quantum critical points. And it is even worse when one performs the finite size analysis as the autocorrelation time usually increase with system size with a high power [9, 17]. Since different configurations generated by NNMC are transformed from uncorrelated random inputs, they are completely free of

autocorrelation, i.e., they are ..., and the direct samplings of physical observables thus become possible. As demonstrated in Fig. 2(a), we compare the autocorrelation function of magnetization between MCMC and NNMC results at  = 2.269 of 2d Ising model; we show similar comparisons of autocorrelation function of (, ) on square lattice Hubbard model at / = 4 in Fig. 2(b) where the system acquires a ground state of antiferromagnetic Mott insulator and the (0, 0) autocorrelation on honeycomb lattice at the Gross-Neveu QCP (/ = 3.83) in Fig. 2(c), between DQMC and NNMC results. It is obvious that, for the MCMC results, the autocorrelation functions decay slowly in Monte Carlo steps (sweeps over the configurations) and the autocorrelation time for classical and quantum critical points increases drastically with the system size, but for the NNMC results, autocorrelation is totally eliminated. Thus, we denote the well-trained generative NN as a method for exact and direct sampling, regardless of classical systems such as the Ising model or the quantum many-body systems such as Hubbard model inside either Mott insulator phase or QCP.
After showing the autocorrelation-free property in NNMC, we next demonstrate how it can be utilized to assist and speed up the MC simulations. In particular, we show that the inevitable statistical error contained in the training sample can be eliminated by adding a few MCMC sweeps to each NNgenerated configuration. In the standard MCMC, the Markov chain must first be well thermalized such that it converges to the correct distribution, and then generate enough numbers of statistically uncorrelated configurations to yield expectation values of observables with satisfactory statistical error. Therefore, it usually requires a rather long Markov chain. In NNMC, however, we can take configurations from a short Markov chain ­ much shorter than chains used in conventional MCMC ­ as the training samples to train a generative NN aiming to reproduce the results measured on the training samples but with much narrower variance due to the ... of NN configurations.

4

(a)

(b)

Histogram

0.200 0.175

MC Train NN NNMC

Variables

Square Ising ( = 16,  = 2.269)

Data

Training Samples

NN Results

NNMC

Good MC

Good MC



0.600(7) 0.608(3) 0.712(6) 0.713(3)

0.150



-1.373(6) -1.315(4) -1.454(5) -1.453(2)

0.125

Square Hubbard ( = 8, / = 4)

0.100

Variables

Data

Training Samples

NN Results

NNMC

Good MC

0.075



-1.298(10) -1.294(4) -1.265(12) -1.268(4)

0.050

(, )

2.21(8) 2.14(2) 2.26(8) 2.26(4)

Honeycomb Hubbard ( = 6, / = 3.83)

0.025 0.000

Variables

Data

Training Samples

NN Results

NNMC

Good MC

0.0

0.2

0.4

0.6

0.8

1.0

1.2

1.4

||

 (0, 0)

-1.360(4) -1.380(3) -1.357(3) -1.358(1) 3.11(10) 3.14(9) 2.99(10) 3.00(4)

FIG. 3. Superior performance of NNMC. (a) shows the distribution of magnetization | | from Ising model with  = 16 at . We include four sets of data: (i) MC Train results are measured from the 1000 configurations we used to train the NN model, which are not thermalized. (ii) NN results are measured from 1000 NN-generated configurations after training. (iii) NNMC results are measured after taking 30 sweeps of MC simulation for the 1000 NN configurations. (iv) Good MC are the average from 1000 bins (200 configurations in each bin) of totally thermalized results. One sees the NNMC is as good as Good MC. (b) summarizes the average values with errors of both Ising model and Hubaard models. (i) The values of Ising model are corresponding to the distribution figure on the left. (ii) For the average values of Hubbard Model on square lattice with  = 8, / = 4 and  = 4 and on honeycomb lattice with  = 6, / = 3.83 and  = 6, the first three are taken from 1000 measurements while the Good DQMC are from 1000 bins (10 configurations in each bin) of thermalized data (NNMC, i.e., NN generated configuration plus 20 DQMC sweeps for square lattice and 15 sweeps for honeycomb lattice). Again, the NNMC results give the identical expectation values of physical observables as the Good DQMC.

To further improve the results, we take a few more MC sweeps starting from the NN-generated configurations to push the NN output towards the correct distribution beyond the precision of the training sample. Such a NNMC scheme turns out to give the best performance.
The results are shown in Fig. 3. We again begin with 2d Ising model with  = 16 at  = , and compare the magnetization | | and energy  measured from four sets of data: (i) 1000 training samples referred to as MC Train. To exaggerate the statistical error in the training sample, we deliberately choose unthermalized data; (ii) The 1000 NN-generated configurations trained from MC Train configurations; (iii) The 1000 NNMC results; (iv) 1000 bins (200 configurations in each bin) of well thermalized results which is referred as Good MC. For better illustration, we plot the distribution of magnetization | | from the four sets of data. One can see that, the distribution of the MC training data set is far from correct, as they are not thermalized. Learning these unthermalized configurations, the NN generated configurations give the same mean values yet with better distribution (smaller variance). Most significantly, by further incorporating a few more MC sweeps [30 steps in Fig. 3(a)], the 1000 NNMC generated configurations gives mean values of observables and distribution identical to those of Good MC measurements with overall many more MC sampling steps. As NNMC is direct sampling and the generated configurations do not suffer from the typical critical slowing down, we thus conclude that the NNMC helps to minimize the thermalization process and provides a direct and exact sampling with better samples and faster speed.

Beyond classical Ising model, for the 2d Hubbard model on the square (/ = 4,  = 8) and honeycomb lattices (/ = 3.83,  = 6), we make similar comparisons. The mean values and errorbars of physical observables are listed in the table in Fig. 3(b), the NNMC results again give almost identical expectation values of physical observables as the Good DQMC. Despite that the autocorrelation in 2d Hubbard DQMC computations turn out to be less prominent as compared to 2d Ising near its transition temperature, through the process of NNMC we obtain very precise results both inside the antiferromagnetic Mott insulator phase of square lattice Hubbard model and the Gross-Neveu QCP of the honeycomb lattice Hubbard model.
We have to clarify that we do not necessarily need the NN to generate up to 1000 configurations(as we show here) to capture the distribution. By using less configurations in the process of NNMC, we can further reduce the computational complexity.
Discussion and outlook -- We develop an AI-assisted quantum many-body computation scheme beyond MCMC. Our numerical results support firmly that neural networks can constructively help improve the Monte Carlo scheme to provide better sampling and completely get rid of autocorrelation between configurations in the study of classical and quantum many-body systems.
Since the network training can even start with small MC samples from a short Markov chain containing both thermalization and statistical errors, our NNMC method therefore provides a generic scheme for avoiding long Markov chains in the simulation of many classical and quantum systems with frus-

5

tration, competing interactions, etc, and thus speeding up the MC simulation. Furthermore, after training, NNMC can generate legitimate configurations (with ...) for measurement with minimal efforts compared with its MCMC counterpart, this method can be used to directly measured physical observables with better accuracy and faster speed.
From the perspective of neural networks, we verify that the complex large-scale quantum many-body computation, such as DQMC, can be well merged into the loss function of network with modern optimizers, the powerful learning ability of NN in frontier physics systems therefore manifests. The next step from here is to use NN to provide efficient and accurate system-size extrapolations of MC configurations and in this way, many more complex yet fundamental computation problems in condensed matter and quantum material research, such as the twisted bilayer grephene and quantum Moiré materials, where the DQMC has just been shown to be able to solve the lattice models [35­39] but with heavy computational complexity, could be improved in NNMC both in efficiency and accuracy.
Acknowledgments -- We thank Zheng Yan, Shangqiang Ning, Bin-bin Mao, Jiarui Zhao, Chengkang Zhou, and Xu Zhang for the enjoyable discussions and happy conversations in the No. 16 Pavillion of Lung Fu Shan Country Park. We acknowledge support from the RGC of Hong Kong SAR of China (Grant Nos. 17303019, 17301420 and AoE/P-701/20), MOST through the National Key Research and Development Program (Grant No. 2016YFA0300502), NSFC (Grant Nos. 11974036, 11874115 and 11834014), and the Strategic Priority Research Program of the Chinese Academy of Sciences (Grant No. XDB33000000). We thank the Center for Quantum Simulation Sciences in the Institute of Physics, Chinese Academy of Sciences, the Computational Initiative at the Faculty of Science and the Information Technology Services at the University of Hong Kong for their technical support and generous allocation of GPU/CPU time. This is work is also supported by the Seed Funding "Quantum-Inspired explainable-AI" at the HKU-TCL Joint Research Centre for Artificial Intelligence, Hong Kong.
[1] G. Carleo, I. Cirac, K. Cranmer, L. Daudet, M. Schuld, N. Tishby, L. Vogt-Maranto, and L. Zdeborová, Rev. Mod. Phys. 91, 045002 (2019).
[2] J. Carrasquilla, Advances in Physics: X 5, 1797528 (2020). [3] E. Bedolla, L. C. Padierna, and R. Castañeda-Priego, Journal of
Physics: Condensed Matter 33, 053001 (2020). [4] G. Carleo and M. Troyer, Science 355, 602 (2017). [5] H. Saito and M. Kato, Journal of the Physical Society of Japan
87, 014001 (2018). [6] O. Sharir, Y. Levine, N. Wies, G. Carleo, and A. Shashua, Phys.
Rev. Lett. 124, 020503 (2020). [7] J. Liu, Y. Qi, Z. Y. Meng, and L. Fu, Phys. Rev. B 95, 041101(R)
(2017). [8] J. Liu, H. Shen, Y. Qi, Z. Y. Meng, and L. Fu, Phys. Rev. B 95,
241104(R) (2017).

[9] X. Y. Xu, Y. Qi, J. Liu, L. Fu, and Z. Y. Meng, Phys. Rev. B 96, 041119(R) (2017).
[10] Y. Nagai, H. Shen, Y. Qi, J. Liu, and L. Fu, Phys. Rev. B 96, 161102(R) (2017).
[11] L. Huang and L. Wang, Phys. Rev. B 95, 035105 (2017). [12] L. Huang, Y.-f. Yang, and L. Wang, Phys. Rev. E 95, 031301(R)
(2017). [13] K. Endo, T. Nakamura, K. Fujii, and N. Yamamoto, Phys. Rev.
Research 2, 043442 (2020). [14] X. Y. Xu, K. Sun, Y. Schattner, E. Berg, and Z. Y. Meng, Phys.
Rev. X 7, 031058 (2017). [15] Z. H. Liu, X. Y. Xu, Y. Qi, K. Sun, and Z. Y. Meng, Phys. Rev.
B 98, 045116 (2018). [16] Z. H. Liu, X. Y. Xu, Y. Qi, K. Sun, and Z. Y. Meng, Phys. Rev.
B 99, 085114 (2019). [17] C. Chen, X. Y. Xu, J. Liu, G. Batrouni, R. Scalettar, and Z. Y.
Meng, Phys. Rev. B 98, 041102(R) (2018). [18] S. Li, P. M. Dee, E. Khatami, and S. Johnston, Phys. Rev. B 100,
020302(R) (2019). [19] C. Chen, X. Y. Xu, Z. Y. Meng, and M. Hohenadler, Phys. Rev.
Lett. 122, 077601 (2019). [20] Z. H. Liu, G. Pan, X. Y. Xu, K. Sun, and Z. Y. Meng, Proceedings
of the National Academy of Sciences 116, 16760 (2019). [21] W. Jiang, Y. Liu, A. Klein, Y. Wang, K. Sun, A. V. Chubukov,
and Z. Y. Meng, arXiv:2105.03639 . [22] B. McNaughton, M. V. Milosevi, A. Perali, and S. Pilati, Phys.
Rev. E 101, 053312 (2020). [23] D. Wu, R. Rossi, and G. Carleo, arXiv e-prints ,
arXiv:2105.05650 (2021), arXiv:2105.05650 [cond-mat.statmech]. [24] M. Abadi et al., arXiv e-prints , arXiv:1603.04467 (2016), arXiv:1603.04467 [cs.DC]. [25] K. Ch'ng, J. Carrasquilla, R. G. Melko, and E. Khatami, Phys. Rev. X 7, 031038 (2017). [26] P. Broecker, J. Carrasquilla, R. G. Melko, and S. Trebst, Scientific Reports 7, 8823 (2017). [27] S. Efthymiou, M. J. S. Beach, and R. G. Melko, Phys. Rev. B 99, 075113 (2019). [28] D. P. Kingma and J. Ba, 3rd International Conference on Learning Representations, Conference Track Proceedings, (2015). [29] J. E. Hirsch, Phys. Rev. B 31, 4403 (1985). [30] I. J. Goodfellow et al., Proceedings of the 27th International
Conference on Neural Information Processing Systems - Volume
2, NIPS'14, 2672­2680 (2014). [31] Z. Y. Meng, T. C. Lang, S. Wessel, F. F. Assaad, and A. Mura-
matsu, Nature 464, 847 (2010). [32] S. Sorella, Y. Otsuka, and S. Yunoki, Scientific Reports 2, 992
(2012). [33] F. F. Assaad and I. F. Herbut, Phys. Rev. X 3, 031010 (2013). [34] Y. Liu, W. Wang, K. Sun, and Z. Y. Meng, Phys. Rev. B 101,
064308 (2020). [35] X. Zhang, G. Pan, Y. Zhang, J. Kang, and Z. Y. Meng, arXiv
e-prints , arXiv:2105.07010 (2021), arXiv:2105.07010 [condmat.str-el]. [36] Y.-D. Liao, X.-Y. Xu, Z.-Y. Meng, and J. Kang, Chinese Physics B 30, 017305 (2021). [37] J. S. Hofmann, E. Khalaf, A. Vishwanath, E. Berg, and J. Y. Lee, arXiv e-prints , arXiv:2105.12112 (2021), arXiv:2105.12112 [cond-mat.str-el]. [38] Y. D. Liao, J. Kang, C. N. Breiø, X. Y. Xu, H.-Q. Wu, B. M. Andersen, R. M. Fernandes, and Z. Y. Meng, Phys. Rev. X 11, 011014 (2021). [39] Y. DaLiao, Z. Y. Meng, and X. Y. Xu, Phys. Rev. Lett. 123, 157601 (2019).

