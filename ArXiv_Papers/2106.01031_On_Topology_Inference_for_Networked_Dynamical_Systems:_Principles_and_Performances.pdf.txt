1
On Topology Inference for Networked Dynamical Systems: Principles and Performances
Yushan Li, Jianping He, Cailian Chen and Xinping Guan

arXiv:2106.01031v1 [eess.SP] 2 Jun 2021

Abstract--Topology inference for networked dynamical systems (NDSs) plays a crucial role in many areas. Knowledge of the system topology can aid in detecting anomalies, spotting trends, predicting future behavior and so on. Different from the majority of pioneering works, this paper investigates the principles and performances of topology inference from the perspective of node causality and correlation. Specifically, we advocate a comprehensive analysis framework to unveil the mutual relationship, convergence and accuracy of the proposed methods and other benchmark methods, i.e., the Granger and ordinary least square (OLS) estimators. Our method allows for unknown observation noises, both asymptotic and marginal stabilities for NDSs, while encompasses a correlation-based modification design to alleviate performance degradation in small observation scale. To explicitly demonstrate the inference performance of the estimators, we leverage the concentration measure in Gaussian space, and derive the non-asymptotic rates of the inference errors for linear timeinvariant (LTI) cases. Considering when the observations are not sufficient to support the estimators, we provide an excitationbased method to infer the one-hop and multi-hop neighbors with probability guarantees. Furthermore, we point out the theoretical results can be extended to switching topologies and nonlinear dynamics cases. Extensive simulations highlight the outperformance of the proposed method.
Index Terms--Topology inference, Distributed cooperation, Networked systems, Causality and correlation modeling, Excitation-based inference.
I. INTRODUCTION
Networked dynamical systems (NDSs) are characterized by the locality of information exchange between individual nodes (described by a topology) [2], and the cooperative capability to solve a common task [3]. Inferring the interaction topology structure from observations over the system emerges in various applications in last decades, including social networks [4], brain connectivity patterns [5] and multi-robot formation [6], to name a few. As topology inference helps better understanding the systems and implementing coordinated tasks, it brings significant benefits for numerous applications of NDSs. For instance, tracing the information flow over a social network [7], group testing and identification of defective items [8], or anomaly detection in communications networks [9].
Mathematically, topology inference can be regarded as a typical inverse modeling problem. In the literature, a large body of researches has been developed to tackle the problem
The authors are with Dept. of Automation, Shanghai Jiao Tong University, Key Laboratory of System Control and Information Processing, Ministry of Education of China, and Shanghai Engineering Research Center of Intelligent Control and Management, Shanghai, Chin. E-mail address: {yushan li, jphe, cailianchen, xpguan}@sjtu.edu.cn. The preliminary result of this paper was submitted to the 60th IEEE Conference on Decision and Control, 2021 [1].

due to their massive employments [10]. For example, [11]­ [13] utilize Granger estimator to capture the casual relationships between agents. Spectral decomposition based method is also a popular tool to estimate the topology [14]­[16], which rely on the diagonalization of the sample matrices and then find the most suitable eigenvalues and eigenvectors to reconstruct the topology matrix. Kernel-based methods are widely used to identify nonlinear dynamic topology [17]­[19], where the key idea is to select appropriate kernel basis functions to approximate the nonlinear dynamics and the selection of kernels critically affects the inference performance.
Despite the prominent contributions of the pioneering works, there still remain some notable issues. First, numerous effective algorithms are designed for symmetric topology structure. The symmetry brings nice tractability in the inference procedure, e.g., in eigendecomposition for the Graph Laplacian matrix and its powers, only a group of left eigenvectors along with their corresponding eigenvalues are sufficient, avoiding large computation costs. Nevertheless, the results are hard to be generalized to account for the directed dependency between nodes, like [20], [21]. Second, most well-established techniques focus on the asymptotic performance with a large number of observation rounds or horizons (like [13], [14]), yet making it unclear how the inference error involves with the observation scale grows. Since small observation scale will give rise to poor inference results, it is also meaningful to investigate feasible techniques to improve the inference accuracy. Third, many methods are developed to interpret the latent regularity contained in the observation data by imposing edge sparsity or smoothness [22]­[24]. Although efficient, the applied technique which can be regarded as using prior knowledge, is not necessarily associated with an actual NDS structure and fails to work for general NDSs. Even for inferring the topology of NDSs, the prior knowledge about the system is assumed available beforehand, which is hard to meet in practice (e.g., the duration time of a topology in dynamic topology inference is priorly known [25]). How to relax such dependence on the prior knowledge and find effective ways to acquire it from observations are also worth consideration.
The above issues have motivated the study of this paper. We aim to present a systematic framework that well accounts for the basic principles of inferring the topology of NDSs, their mutual relationships, performances of convergence and accuracy in terms of the observation scale, along with possible revised measures to enhance the inference performance. Different from traditional system identification literatures which aim to identify the system's Markov parameters from known system input/output data (e.g., [26], [27]), the input in our

2

NDSs

Benchmark Methods

System Model

xt Wxt1 t1 yt  xt t

y11 y12

yT1

y12 y22

yT2

y1n y2n

yTn

Single Trajectory
Insufficient Observations

Multiple Trajectories

Single Trajectory

Granger Estimator

OLS Estimator

Approximation

De-regularization

Causality and Correlation based Inference

Modeling by State Causality

Modification by Node Correlation

Non-asymptotic Performance Analysis

Improve

Excitation based Inference

One-hop Neighbor Recursive Probability

Multiple-hop

Inference

Guarantees

Neighbor Inference

Extensions
Identify Topology Switching Points
Multiple Excitation with Constrained Input
Binary Topology Inference under Nonlinear Dynamics

Fig. 1. The main framework of this paper.

scenario is unavailable, incurring two major challenges. On the one hand, only noisy observations over the system evolution are available, making every consecutive pair are undesirably corrupted by correlated noises. On the other hand, the observations are simultaneously influenced by the system structure, stability and process noises. The influence of noises can accumulate and incur large inference costs.
To start with, we advocate an inference method from the perspective of node causality and correlation, which account for the interaction dependence. Along this line, we reveal its close relations with popular Granger and ordinary-leastsquare (OLS) estimators. Then, the non-asymptotic performance of the proposed method is investigated, as well as explicit comparison with the OLS estimator and covering both marginally and asymptotically stable system dynamics. Considering the situations where the observations are not sufficient to support the estimators, the inference goal is reduced to obtain the binary connection structure contained in the topology. To practice, we propose an excitation-based method to improve the inference performance by resorting to hypothesis testing. Furthermore, we extend the estimator to identify the switching moments of dynamic topology cases, and provide a linear approximation approach to infer the binary connection structure of nonlinear system model cases. The main results are as follows.
· This work contributes to the existing body of research by revealing the principles and (non-)asymptotic performance of inferring the topology of the NDS, from the perspective of node causality and correlation. By characterizing the stability of the NDS, we demonstrate that the Granger estimator for multiple observation rounds can be approximated by a single observation round. In particular, we propose a causality-based method to effectively infer the topology from highly correlated and noisy observations, applying to both directed and undirected topologies. In addition, a correlation-based modification design is tailored for the setting on small observation scale, effectively alleviating the performance degradation by using the original causality-based method.
· By exploiting concentration of measure in Gaussian space and Chebyshev inequality, we deduce the close relations between our proposed method with the Granger and OLS estimators. Specifically, towards the Granger and proposed estimators, we prove the equivalence condi-

tions of the sample matrices in asymptotically stable

system cases, and clarify the deviation growth of the

matrices with the observation scale in marginally stable

cases. Then, taking the OLS estimator as the benchmark

of a single observation trajectory case, we derive the

convergence rates of both asymptotically and marginally

stable cases, i.e., O(

1 T

)

and

O(

log T T

),

respectively.

Concerning the inference accuracy, we enlighten in sharp

contrast that the error bound in our method will converge

to zero, while that of the OLS estimator converges to a

bounded constant.

· Considering the situation where the observations are

not sufficient to support the estimators, we develop an

excitation-based approach to identify the binary connec-

tion structure in the NDS instead. Utilizing hypothe-

sis testing, we obtain the excitation input boundary to

discriminate both the one-hop and multi-hop neighbors

of a node, along with the probability guarantees. Fur-

thermore, we revise and extend the above results to

more complicated topology inference scenarios, including

switching topologies, general nonlinear dynamics, and

bounded excitation constraints, providing a simple and

efficient algorithm design. Performance study by simula-

tion illustrates that our proposed method outperforms the

OLS estimator, and verifies our theoretical results.

This paper provides deeper insights into the close relationships and the performance analysis of different inference methods. The theoretical results can serve as instructions to design the inference procedures for various observation scales, and also beckon further research to explore more advanced methods for general time-varying and nonlinear NDSs. The framework of this paper is summarized in Fig. 1.
The remainder of this paper is organized as follows. Section II presents related literatures. Section III gives basic preliminaries and describes the problem of interest. The inference methods along with their relationships are presented in Section IV, The convergence rate and accuracy of our method are analyzed in Section V. Section VI demonstrates the excitation-based inference method. Section VII discusses some extensions to more complicated cases. Simulation results are shown in Section VIII, followed by the concluding remarks and further research issues in Section IX.

3

II. RELATED WORK
There have been extensive researches on topology inference in the literature. Generally, it can be mainly cast into two categories: static and dynamic topology inference.
Static topology inference. In [28], [29], the authors consider the casual dynamics model and focus on learning the causal relationships by means of functional dependencies. [30] considers the network link tomography and infer the detailed topology of a measurement tree induced by multicast probing. Optimization algorithms are designed in [31], [32] to infer the Graph Laplacian matrix of the network from the nodal observations, by considering the stationary signals are smoothly evolving. [33] investigates the identifiability conditions for unknown dynamical networks from output secondorder statistics, where the network is driven by stochastic inputs. In relation to the inference of networks from consensus dynamics, topology is reconstructed by measuring the power spectral density of the network response to input noises, and node removal strategies are designed [34], [35]. Aiming at the adaptive diffusion process of the network, the correlation methods are proposed to achieve the progressive approximation over partially observed networks [13], [36]. Note that it is difficult to accurately obtain the direct interaction between pairs of nodes using correlation methods [37]. In general, most of these works focus on specific algorithm design, and do not consider the observations can also be corrupted by independent noises, leaving it an open issue to investigate its influence and the inference performance (e.g., non-asymptotic rates and asymptotic accuracy).
Dynamic topology inference. In many applications, the observations entail a time-varying graph and static graph inference methods will fail to capture the dynamic characteristic. To account for the time-varying issue, numerous methods have been developed mainly based on the extensions of graphical Lasso-based methods [20], [38], [39] and SEM models [40]­ [42]. Note that the graphical Lasso-based approaches only apply to identify undirected dynamic topologies while SEMbased methods are feasible for both undirected and directed cases. Since multiple time slots are involved, the dynamic topology inference is usually transformed into a sequential optimization problem with multiple topology variables. For example, [43] proposes a primal-dual optimization algorithm with the prior that the topology edges change smoothly. [44] addresses the inverse covariance matrix of a sequence of multivariate observations, which encodes the conditional independence between different nodes. Also, several works focus on detecting the switching points between different topologies, which is a critical premise in dynamic inference. It is usually assumed that the graphs at each instance come from specific distributions, e.g., generalized hierarchical random graph model [45], Markov random field [46].
In addition, if the dynamics of an NDS involves nonlinearities, linear models cannot accurately capture the nonlinearities despite their simplicity. Some extensions of linear models have been proposed to deal with the nonlinear dependencies, e.g., [47], [48]. However, the topology structure is assumed priorly known in these works assume, and they mainly aim to

infer the unknown edge weights. Some kernel-based methods are developed to well account for general nonlinearities, like [49]. Unfortunately, they cannot unveil the essential topology structure explicitly.
Different from the aforementioned works, this paper focus on demonstrating the basic principles of topology inference for general NDSs and exploring their (non-)asymptotic performance in terms of observation scale. With these issues settled, we further provide their extensions on switching topology and nonlinear dynamics cases.
III. PRELIMINARIES AND PROBLEM FORMULATION
A. Graph Basics and Notations
Let G = (V, E) be a directed graph that models the networked system, where V = {1, · · · , n} is the finite set of nodes and E  V × V is the set of interaction edges. An edge (i, j)  E indicates that i will use information from j. The adjacency matrix A = [aij]N×N of G is defined such that aij > 0 if (i, j) exists, and aij = 0 otherwise. Denote Ni = {j  V : aij > 0} as the in-neighbor set of i, and di = |Ni| as its in-degree. A directed path is a sequence of nodes {r1, r2, · · · , rj} such that (ri+1, ri)  E, i = 1, 2, · · · , j - 1. A directed graph has a (directed) spanning tree if there exists at least a node having a directed path to all other nodes. G must have a spanning tree to guarantee that at least one node's information can reach all other nodes.
Throughout this paper, the set variable, vector and matrix are expressed in Euclid, lowercase, and uppercase font. The subscript [·]ij([·]i) denotes the ij-th(i-th) entry of a matrix (vector). Let 0 be all-zero matrix in compatible dimensions, min(M ) and max(M ) be the smallest and largest eigenvalues of the matrix M , respectively. For square matrices Ma and Mb in the same dimensions, Ma Mb (Ma Mb) means Ma-Mb is positive-semidefinite (negative-semidefinite). Unless otherwise noted, · and · F represent the spectral and Frobenius norm of a matrix, respectively.

B. System Model Consider the following networked dynamical model

xt = W xt-1 + t-1,

(1)

yt = xt + t,

where xt and yt represents the system state and corresponding observation at time t (t = 1, 2, · · · , T ), W  Rn×n is the unknown interaction matrix related to the adjacent matrix A,
and t and  represent the process and observation noises, satisfying the following Gauss-Markov assumption.

Assumption 1. t and t are i.i.d. Gaussian noises, subject

to N (0, 2I) and N (0, 2I), respectively. They are also

independent

of

{xt

}tt

=t =0

and

{yt

}tt

==t0.

Next, we present asymptotically stable matrix class Sa and the (strictly) marginally stable matrix Sm as follows:

Sa ={Z  Rn×n, max(Z) < 1}, Sm ={Z  Rn×n, max(Z) = 1 and the geometric (2)

multiplicity of eigenvalue 1 equals to one}.

4

Model of the NDSs xt  Wxt1  t1, yt  xt  t

t N (0,2I ), t N (0,2I )

Multiple Trajectories

Large Scale Observation

Single Trajectory

Insufficient Observation

Granger Estimator W^g  R1x (t)(R0x (t 1))1

Causality-based Estimator W^c  1(T )(0 (T 1) 2I )1

OLS Estimator W^o  1(T )(0 (T 1))1

R0x , R1x

0 1

W^c

(Theorem 3) de-regularization

W^o

Sample Equivalence (Theorem 2)
W  a : 0 ()  R0x ()  2I
Sample Deviation (Theorem 4) W  m : 0 (T )  R0x (T ) ~ ( T )

Convergence and Accuracy (Theorem 5-6)

(

1 ) (W  T

a) &

(

logT ) (W  T

m)

W^c W  0 & W^o W  K2

Excitation-based Method

 Misjudgement

e

Probability

Critical Excitation Bound | e j |  2 2erf 1(1 e ) / wij
(1-Hop, Theorem 7) | e j |  2 2erf 1(1 e ) / ij (h)
(h-Hop, Theorem 8)

Fig. 2. Roadmap of the main theoretical results in this paper.

In terms of the setup of W , some useful and popular choices are the Laplacian and the Metropolis rules, which are defined as follows [50]. For i = j,

wij =

aij/ max{di, i  V}, by Laplacian rule,

aij/ max{di, dj},

by Metropolis rule,

(3)

where the auxiliary parameter  satisfies 0 <   1. For both rules, the self-weights are given by

wii = 1 - wij .

(4)

j=i

Note that if W is specified by either one of the two rules, then W  Sm. A typical matrix in Sa can be directly obtained via multiplying (3) and (4) by a factor 0 <  < 1, which is common in adaptive diffusion networks [36]. Considering different stabilities, it holds that

lim W t =
t

0, W ,

if W  Sa if W  Sm,

(5)

where 0 represents all-zero matrix in compatible dimensions and W  < . In a recursive form, (1) is rewritten as

t

yt = xt + t = W tx0 +

W m-1t-m + t. (6)

m=1

C. Topology Inference Modeling and Problem of Interest
Given the observation sequence {yt}Tt=0 adhering to (1), the goal of this paper is to infer the unknown interaction matrix W . Mathematically, one aims to find an associated mapping

M : {yt}Tt=0  W.

(7)

Around the above goal, this paper is dedicated to investigating the following problems.
· From what perspectives we can design the inference methods to obtain the topology, and what are their explicit relationships among these methods.
· How is the applicability of these methods in different observation scales (e.g., single or multiple observation rounds, observation horizon, etc.), and how are their convergence and accuracy performances.
· How to infer the topology structure when the observations are not sufficient, or when the system is in more complex cases, like switching topologies and nonlinear dynamics.

To address these issues, the core challenges lie in alleviating the undesirable influence brought by highly correlated observation noises, and characterizing the inference error with limited known information. For the former one, we are able to establish an interpretable inference model borrowing the idea of node causality and correlation. For the latter, an probability analysis framework is promising to be employed by resorting to the concentration measure in Gaussian space. The main theoretical results are present in Fig. 2.

IV. TOPOLOGY INFERENCE METHODS

In this section, we first introduce two benchmark methods, the Granger and OLS estimators. Then, we propose a causality-based inference method for a single observation round setting, following by its correlation-based modification design for cases when the observation size is small. Finally, we demonstrate the close relationships among these methods.
To ease notation, we organize the state/observation/noise as

XT- = [x0, x2, · · · , xT -1], XT+ = [x1, x2, · · · , xT ], YT- = [y0, y2, · · · , yT -1], YT+ = [y1, y2, · · · , yT ], (8)
T = [0, 1 · · · , T -1], T = [1, 2 · · · , T ].

Then, the whole evolution process is compactly written as

XT+ = W XT- + T , YT+ = W XT+ + T .

(9)

A. Benchmark Methods
During the running process of the NDS, the system states become highly correlated after continuous exchange of information. Therefore, the connectivity between two nodes can be revealed by the state correlation. From this perspective, the famous Pearson correlation coefficient provides a way to quantify the correlation degree, given by

0ij

=

T t=0

(xit - x¯i) 0i

(xjt

- x¯j) 0j ,

(10)

where 0i =

T t=0

(xit

-

x¯i)2

is

the

sample

standard

devi-

ations of {xit}Tt=0 and x¯i =

T t=0

xit/T ,

i



V.

The

larger

0ij is, the more confident one can determine that there exists

an edge between node i and j.

Note that the coefficient 0ij directly describes the (linear)

correlation between two nodes. However, due to its symmetry,

5

it cannot reveal the directionality (i.e., causality) of an existing edge between two nodes. The following lemma presents a way to overcome the causality issue.

Lemma 1 (Granger causality [11], [13]). If multiple observation round are available over the system (1), then we have

R1x(t) = W R0x(t-1),

(11)

where R0 = E xtxTt and R1 = E xtxTt-1 are the autocorrelation and one-lag autocorrelation matrices.

This result is straightforward since R1x(t) = E xtxTt-1 = E[(W xt-1 + t)xTt-1] = W R0x(t - 1). Note that R0(t) can be explicitly represented as

t-1

R0x(t) = W tx0xT0 (W t)T + 2

W m(W m)T. (12)

m=0

According to Lemma 1, the Granger estimator is given by · Granger estimator:

W^ g = R1x(t)(R0x(t - 1))-1.

(13)

Note that (13) can be interpreted as finding the coefficients
{wij} that provide the best linear prediction of xt given the past state xt-1.
Next, we present the popular OLS estimator, which is
derived from the perspective of least square optimization. Then, inferring the W from {yt}Tt=0 is formulated to solve the least square problem

T

P1 : min

yt - W yt-1 2.

(14)

W

t=1

Note that the objective function of P1 can be rewritten as

min
W

YT+ - W YT-

2 F

.

Then,

by

finding

the

derivative,

one

obtains the optimal solution as

· OLS estimator:

W^ o = YT+(YT-)T(YT-(YT-)T)-1 = 1(T )-0 1(T ). (15)

B. Causality-based Inference Method

Although the Granger estimator presents a direct and analytic expression for inferring W , it is based on observations over multiple process rounds and the observation noises are often ignored. It cannot be directly applied in single observation round case. Nevertheless, it provides beneficial modeling ideas from the perspective of node causality. Similar with R0x(t) and R1x(t), we define the following sample covariance matrix and its one-lag version as

0(T ) =

1 T

(YT-)(YT-

)T,

1(T ) =

1 T

(YT+

)(YT-)T.

(16)

Before demonstrating the relationship between R0x/R1x and 0/1, we first present the asymptotic performance of 0/1.

Lemma 2 (Mutual independence between states and noises
in a single round). Given arbitrary ZT  Rn×T and noise matrix T  Rn×T with i.i.d. Gaussian entries. Let |z|m = max{|ztj| : t  N+, j  V}. If |z|m < , then we have

Pr

lim
T 

1 T

T

ZTT

=

0

= 1.

(17)

Proof. The proof is provided in Appendix A.

Lemma 2 illustrates the independence of the sample matrix
on the noise matrix in a single observation round. The result (17) also applies to linear transform B, where B  Rn×n and B < . Since only {yt}Tt=0 are directly available, for every two adjacent observations, it follows that

yt = W xt-1 + t-1 + t

= W yt-1 - W t-1 + t-1 + t

= W yt-1 + t,

(18)

where t = -W t-1 + t-1 + t, satisfying N (0, 2W W T + 2I + 2I), which is highly auto-correlated. Besides, t is independent of all {xt }t <t and {t }t <t-1. Note that (18) only represents the quantitative relationship between adjacent
observations, not a causal dynamical process. Based on the
observations, we present the following theorem.

Theorem 1 (Causality in single observation round). Given observations {yt}Tt=1, if W  Sa, we have

1() = W (0() - 2I),

(19)

where 1() = lim 1(T ) and 0() = lim 0(T ).

T 

T 

Proof. The proof is provided in Appendix B.

Different from the Granger causality in Lemma 1, Theorem 1 relaxes the dependence on multiple observation rounds, and presents the observation causality for a single round, while taking the observation noises into consideration. Then, given finite horizon T , we propose the causality-based estimator as
· Causality-based estimator:

W^ c = 1(T )(0(T ) - 2I)-1.

(20)

Remark 1. We demonstrate that although the estimator W^ c is derived from Theorem 1 where W  Sa holds, it is also applicable when W  Sm. In fact, Theorem 1 is directly based on the Chebyshev inequality, where the bounded state
constraint precludes us from proving the convergence and accuracy of W^ c when W  Sm. To tackle this issue, we can resort to the concentration measure in Gaussian space. The
details will be given in Section V.

C. Correlation-based Modification Design
Like Granger estimator W^ g, the proposed causality-based estimator W^ c is a asymptotic solving manner, i.e., it will approximate the real W as T  . When T is small, the performance of W^ c may largely degenerate. The main cause is that directly using 2I to filter the influence of observation noises is not an appropriate choice, for 2I is a meaningful statistical characteristic in the sense of large noise samples.
Inspired by the correlation measurement (10), an alterna-
tive way to alleviate the influence of observation noises is
to implement correlation coefficient calculation, which also
directly represents the linear correlation between nodes. Then,

6

Algorithm 1 Causality and correlation based inference method
Input: Observations {yt}Tt=0, observation noise variance 2, and the preset observation scale threshold Tp.
Output: Estimation of the topology matrix W^ . 1: Collect all the observations into YT- = [y0, y2, · · · , yT -1],
YT+ = [y1, y2, · · · , yT ]; 2: if T > Tp then 3: Compute 0(T ) and 1(T ) by (16); 4: Estimate W by W^ c = 1(T )(0(T ) - 2I)-1. 5: else 6: For all i  V, compute [y~t-]i and [y~t+]i by (22). 7: Compute the sample matrices S0(T ) and S1(T ) by (21);
8: Estimate W by W^ s = S1(T )S0-1(T ); 9: end if 10: return the estimation of the topology matrix W^ .

we define the following correlation-based sample matrix and its one-lag version as

S0(T )

=

1 T

T -1
y~t-(y~t-)T,

S1(T )

=

1 T

T

y~t+(y~t--1)T, (21)

t=0

t=1

where the elements of y~t- and y~t+ are given by

[y~t-]i

=

[yt

-

YT-1T T

]i/-i ,

[y~t+]i

=

[yt

-

YT+1T T

]i/+i ,

(22)

where 1T  RT is the all-one vector and

-i =

T -1
(yti

-

[ YT-1T T

]i)2, +i

=

t=0

T

(yti

-

[

YT+1T T

]i)2.

t=1

Finally, the correlation-based modified causality estimator is designed as
· Correlation-modified estimator:

W^ s = S1(T )S0-1(T ).

(23)

Remark 2. The main merit of W^ s lies that it subtly takes the noise filtering and the node correlation into account at

the same time by the correlation coefficient calculation. In

statistics, it can be seen as a normalization operation to

quantity the observations in the same measurement space.

Here we point out that correlation-based modification W^ s improves the inference performance of estimator W^ c in small observation scale, and its inference accuracy is no worse than that of W^ o (this will be verified in section VIII). Specifically, the performance improvement is significant when W  Sa. The reason is that the state of an asymptotically stable NDS
will always converge zero, which indicates the system is
mainly driven by noises regardless of the initial states. In this situation, W^ s enhances the correlation and causality between two observations.
At last, the proposed causality and correlation based in-
ference method is summarized as Algorithm 1. Note that
we introduce a preset parameter Tp to describe whether the observation scale is large, which can be set as Kn2(K  R+). If T > Tp, the estimator W^ c is adopted, or W^ s otherwise.

Causality-based Estimator

Asymptotic

W^c  1(T )(0 (T 1) 2I )1

Approximation W  a : 0 ()  R0x ()  2I

  2

De-regularization

T

 min W t 1

yt Wyt1 2   W

2 F

Granger Estimator

OLS Estimator

W^g  R1x (t)(R0x (t 1))1
Multiple Observation Rounds

Relax the Dependence On Multiple Rounds

W^o  1(T )(0 (T 1))1 Single Observation Round

Fig. 3. An overview of the mutual relationships of W^ c, W^ g and W^ o.

D. Relationships between Different Estimators
In this part, we first demonstrate the relation between the causality-based estimator W^ c and the Granger estimator W^ g, by revealing the equivalence condition of their observation matrices in single and multiple observation rounds, respectively. The overview of the mutual relationships is shown in Fig. 3.

Theorem 2 (Equivalence condition between 0 and R0). If W  Sa, when T  , we have
0() = R0x() + 2I, 1() = R1x() + 2W. (24)
Proof. The proof is provided in Appendix C.

Theorem 2 demonstrates the equivalent condition between estimators W^ g and W^ c. It reveals that the expected state
covariance matrix of T   is identical with the sample

covariance matrix along all the single time horizon, which is

an interesting result that describes the relationship between

multiple and single observation rounds.

Next, consider the relationship between the causality-based estimator W^ c and OLS estimator W^ o. Note that in optimization

field, it is common to use some modification on the objective

function of the OLS problem to improve the solving per-

formance, typically by adding a regularization term. Taking

W

2 F

as the regularizator, P1 is transformed to

T

P2 : min
W

yt - W yt-1

2+

W

2 F

,

(25)

t=1

where  is a regularization parameter. Then, we present the following theorem.

Theorem 3 (Relationship between W^ c and W^ o). The causality-based estimator W^ c is equivalent to solving P2 with  = -2, and is a de-regularization form of W^ o.

The proof of Theorem 3 is straightforward. Note that the

objective function in P2 is equivalent to min
W

YT+-W YT-

2 F

+



W

2 F

,

whose

optimal

solution

is

W^ = 1(T )(0(T ) + I)-1.

(26)

Apparently, we have W^ = W^ c when  = -2 or W^ = W^ o when  = 0.
Theorem 3 reveals the close relation of the proposed W^ c with W^ o. On the one hand, it provides a new interpretation for using LS methods to infer the interaction topology from the
perspective of node causality. On the other hand, it illustrates
the idea about how to set a reasonable regularization term
and parameters for the LS problem modeling when both the
input and output data are corrupted. Specifically, in the latter

7

case,  = -2 essentially is not a typical regularization but a de-regularization form, which is quite different from normal regularization situations where  > 0.
Remark 3. In summary, the four estimators W^ g, W^ o, W^ c and W^ s approximate W from different angles. From a pure statistical viewpoint, W^ g implements the inference over multiple rounds of observations at a single moment, while the
remaining three do that over a sequence of observations in a
single observation round, which is more common in practice. Specifically, W^ s is a modified version of W^ c for small horizon T , whose inference accuracy is no worse than that of W^ o.

V. INFERENCE PERFORMANCE ANALYSIS: CONVERGENCE AND ACCURACY

In this section, we mainly analyze the inference performance of the proposed causality-based estimator W^ c in terms of convergence speed and accuracy, by comparing with W^ g and W^ o. Our analysis is based on non-asymptotic analysis of
random matrices of Gaussian entries.
To begin with, we provide the supplementary results of
Theorem 2 when W  Sm, by clarifying the non-asymptotic deviation of the observation matrices used in W^ c and W^ g.

R Lemma 3 (Concentration measure in Gaussian space [51]).
Let   n×T be a matrix with independent standard normal

entries. With probability at least 1 - 2 exp -r2/2 , one has





T - n - r  min()  max()  T + n + r. (27)

Based on Lemma 3, we demonstrate that 0() = R0x() + 2I in Theorem 1 does not hold when W  Sm,
but gradually deviates as T increases.

Theorem 4 (Sample matrix deviation between 0 and R0). If W  Sm, the deviation norm 0(T ) - R0x(T ) - 2I is at least in O( T ) scale.

Proof. The proof is provided in Appendix D.

Theorem 4 reveals that when W  Sm, the influence of the process noises will consistently accumulate over the system evolution, and one cannot use the 0() to approximate the ideal factor R0x(). However, we will demonstrate this defect does not hinder us from using the estimator to infer the
topology matrix. The key question here is what is the exact influence of whether W  Sm or W  Sa over the inference performance. A direct intuition is that it needs extra cost to
overcome the accumulated influence of process noises when W  Sm, compare with that when W  Sa. To answer this question, we let T = T 0(T -1) and introduce the following lemma first.

Lemma 4 (Proposition 3.1 in [52]). Let V

0 be a

deterministic matrix and ~ T = T + V . Given 0 <  < 1 and

{t, xt}Tt=1 defined as before, we have with probability 1 - 

T -1

~ -T

1 2

yttT 

t=0



1

5 det ~ T V -1 + I 2n

8n log  

1 n

. 

(28)

Lemma 4 shows that there always exists an upper bound

for

~ -T

1 2

T -1 t=0

yt

tT

, and the invertibility of ~ T

is where

most of the proof lies. In this paper, it is clear that under the

Gauss-Markov assumptions, ~ T is invertible. Therefore, we

can always find a dn and up such that

0  dn XT-(XT-)T ~ T up.

(29)

Following Lemma 4 and (29), we present the non-asymptotic bound of W^ o, paving the way for subsequent comparisons.
Theorem 5 (Error bound by W^ o). Given {yt, t, t}Tt=0 defined before, with probability at least 1 - , the following nonasymptotic bound holds,

W^ o - W

11 

n

log(

5

det(up - dn1 1/n

+I

)

1 2n

)

+

 5T 
4 min(dn)

.

min(dn)

(30)

Proof. The proof is provided in Appendix E.

Theorem 5 demonstrates that the non-asymptotic performance is mainly determined by min(dn) and up . The non-asymptotic bound of W^ c is in the same form with that of W^ o and is omitted here. It is straightforward that if the term min(dn) grows faster than the numerator in (30) as T increases, the inference accuracy also increases. Next, we explicitly characterize the convergence and accuracy of the two estimators.
Theorem 6 (Convergence speed and accuracy of W^ o and W^ c). With probability at least 1 - , the non-asymptotic bound of the OLS estimator W^ o satisfies

W^ o - W



O( 

log T

T

)

+

O(2

),

O(

1 T

)

+

O(2 ),

if W  Sm, (31)
if W  Sa.

and the non-asymptotic bound of the causality-based estimator W^ c satisfies



log T

W^ c - W

O( 

T

1

),

if W  Sm,

(32)

O(

 T

),

if W  Sa.

Proof. The proof is provided in Appendix F.

In terms of sample scale T , Theorem 6 demonstrates the
convergence speed of the inference error bound by using W^ o and W^ c. Now back to the question before Lemma 4, we can conclude that the extra cost for the estimators when W  Sm is longer error converging time (or larger observation sample scale), requiring O( log T ) times than that when W  Sa. In terms of accuracy, when T  , the inference error will converge to a constant by W^ o, while that of W^ c will converge to zero, which shows the latter one has better
inference accuracy.

8

VI. EXCITATION-BASED INFERENCE METHOD As previous sections reveal, the performance of W^ c relies on large sample scales. When the observations are extremely limited and not sufficient, it is hard to obtain a reliable W . However, we are still able to infer the logic structure of the system topology (i.e., the binary adjacent matrix). Analogous to that a stone thrown into the water will cause spreading ripples, if we apply an excitation in the node i, the influence of the excitation will quickly spread to i's neighbor and then outer neighbors. Motivated by this, we will illustrate how to use just one-step excitation to infer the topology structure and evaluate the accuracy in this section. This method can also be applied as auxiliary measure to enhance the accuracy of the inference methods in Section IV (e.g., by treating the excitation result as a constraint in the optimization problem modeling of W^ o/W^ c).

A. Observation Model under Excitation
First, node i is called a h-hop (out)-neighbor of node j if there exists at least h edges such that

h
ailil+1 = ai1i2 ai2i3 ai3i4 ...aih-1ih aihj > 0,
l=1

(33)

where node i1 = i and ih+1 = j. Formally, the h-hop out-
neighbor set can be formulated by the following recursive definition. Let (h) = W h and Nje,h be the node set whose elements are connected with j within h hops, then the h-hop neighbors of j, Njo,uht, is given by

Njo,uht = Nje,h\

h-1

l=1

Njo,ul t

.

(34)

Note that when h = 1, Njo,u1t = Nje,h. In the following, Njo,u1t is directly represented by Njout, and the excitation-based method is based on the following assumption for simplicity.

Assumption 2. W  Sa  {W : W  Sm, W 1 = 1}.

Under Assumption 2 which is quite common in NDSs, when t becomes large, the difference between the elements in yt will converge to zero in sense of expectation. In situations without excitation, two consecutive observations satisfy (18), and the observation deviation of for node i is given by

yti+1 - yti = (W yt)i - yti + ti+1  ytmax + ti+1, (35)

where ytmax = max{|yti - ytj| : i, j  V}, ti  N (0, 2 (i)) and 2 (i) is given by

n

2 (i) = (1 + wi2j )2 + 2.

(36)

j=1

Next, let ejt be the excitation input on node j at time t and y~ti+1
be the observation of i under excitation. Then, the observation deviation of i under ejt is given by

y~ti+,1 = y~ti+1 - yti 

ytmax +wij ejt +ti, ytmax +ti,

if wij > 0, (37) if wij = 0,

Note that the term wijejt in (37) represents the influence of the excitation input ej over i, and all the one-hop layer neighbors

of j will have a response to the excitation in next iteration

step. Then, inferring the topology by excitation is transferred

to identify the one-hop neighbor set of a node j, which

can be modeled as a typical binary hypothesis testing. The

null hypothesis and the alternative hypothesis are respectively

defined as

H0 : i / Njout, H1 : i  Njout.

(38)

In the following parts, we will drop the subscript t in the
variables if it does not cause confusion and temporarily assume ytmax = 0, for illustrating how to use (38) to infer the topology structure in detail.

B. One-hop Neighbor Inference
Denote Pr{H0|y~i,} (Pr{H1|y~i,}) as the probability that H0 (H1) holds given the observation y~i,. Then, we have the following decision criterion

Pr{H1|y~i,}  Pr{H0|y~i,}  H1 holds, Pr{H1|y~i,} < Pr{H0|y~i,}  H0 holds,

(39)

which is also called the maximum posterior probability criterion. However, it is possible that (39) is misjudged in the test, for example, H0 is true but H1 is decided (Type I Error) or H1 is true but H0 is decided (Type II Error). Accordingly, let Pr{D1|H0} be the false alarm probability and Pr{D0|H1} be the missed detection probability, respectively. Therefore, the overall misjudgement probability is given by

e = Pr{D1|H0} + Pr{D0|H1}.

(40)

Suppose the inference center has no prior information about H1 and H0, i.e., Pr(H1) = Pr(H0) = 0.5. Under (39), the inference performance by the hypothesis testing is demonstrated by the following result.

Theorem 7 (Critical excitation for one-hop neighbors). To ensure the misjudgement probability within a threshold ¯e, the

excitation ej should satisfy



|ej|  2

2(i) wij

erf-1(1

-

¯e),

(41)

where

the

Gaussian

error

erf(z)

=

2 

z 0

exp

(-r2)dr

and

erf-1(·) is the reverse mapping of erf(z).

Proof. The proof is provided in Appendix G.

Theorem 7 gives the lower magnitude bound of the excita-

tion input to guarantee the specified misjudgment probability

in a single time. Given the excitation input ej satisfying

(41), one has with probability at least (1 - ¯e) to accurately

discriminate whether i  Njout. Note that the interaction

weight wij is not priorly known in reality, thus the decision

threshold

in

theory,

, wij ej
2

is

unavailable.

However,

we

can

enable the hypothesis test by specifying the least interaction

weight that one wishes to discriminate between two nodes. To practice, since W F  n W  n, thus we have
n
wi2j  n and
j=1

2 (i)  (1 + n)2 + 2 = ¯2 .

(42)

9

Algorithm 2 Excitation-based inference method
Input: Observations yt, target excited node j, desired lower bound of interaction weight w, upper bound ¯, and tolerant error probability ¯e.
Output: Estimation of the one-hop out-neighbor of j, N^jout.

1: Initialize N^jout = .



2:

Calculate

the

critical excitation ej

=

2

2¯ w

erf-1

(1

-

¯e

).

3: Excite node j with ej and obtain the observation yt+1.

4: Compute ytmax = max{|yti - ytj| : i, j  V}.

5: for i  V do

6: Compute the observation deviation y~ti+,1 = yti+1 - yti.

7:

if

y~ti+,1

>

ytmax

+

wej 2

then

8:

N^jout = N^jout  {i}.

9: end if

10: end for 11: return The one-hop neighbor set estimation N^jout.

hypothesis (38), we first define the following hypothesis that tests whether i  Nje,h, i.e.,

H0(h) : i / Nje,h, H1(h) : i  Nje,h.

(44)

Lemma 5 (Critical excitation for neighbors within h hops).

Under hypothesis test (44), to ensure the misjudgement probability for all the neighbor within h-hop is lower than ¯e, the

excitation ej should satisfy



|ej |



2 2 ij (h)

erf-1(1

-

¯e),

(45)

Proof. Directly focusing on the h-step node response after the

excitation input is injected on j, (h) becomes the equivalent

topology that corresponds tothe h-step process. Based on

Theorem 7, when |ej|



2

2 ij

erf-1(1

-

¯e)

ensures

the

misjudgement probability is no more than (1 - ¯e), which

completes the proof.

Specifically, if W is row-stochastic, then the upper bound ¯2

can be further reduced to 22 + 2. Next, suppose that one

aims to judge whether i  Njout such that wij > wij, where

wij is the specified weight threshold. Given the desired error

probability bound ¯e and the excitation input ej such that ej =

2

2¯ w

erf-1(1

-

¯e),

then

with

probability

at

1 - ¯e

one

can

discriminate whether i  Njout by

 i  Njout,

if

|y~ti+,1|



ytmax

+

wij |ej | , 2

(43)

i / Njout, else,

where the parameters in (43) are all computable or known. Applying (43) to all other node and one can obtain an estimated set of Njout. The whole procedures are summarized in Algorithm 2.
Remark 4. The key point of using the excitation-based method lies in that it requires certain prior knowledge about |(W yt)i- yti| or the evolution characteristic of yt. In this paper, we utilize Assumption 2 to bound |(W yt)i-yti| for simplicity. Many other ways are also applicable and are not the interest of this paper. Besides, if a lower bound estimator of an edge weight wij is available, the inference accuracy can be further improved.
A direct result from Theorem 7 is lim e = 0, which
|ej |
corresponds to the common intuition. As long as the excitation input is large enough, the one-hop neighbors of j can always be inferred. Under this situation, it is also very likely that the two-hop (even more) out-neighbors of the excited node can be also identified by just single excitation.

C. Multi-hop Neighbor Inference
In this part, we will demonstrate how to identify multi-hop out-neighbors of a node by single excitation. Similar with the

Note that Lemma (5) only illustrates how to reduce the misjudgement probability of i  Nje,h, and does not provide information about whether i  Njo,uht. A key insight is that if i is decided not in Nje,h-1 but in Nje,h, then it is very likely that i  Njo,uht is true. Starting from this point, we utilize a single-time excitation input and do h-rounds tests to achieve
the inference goal. Two auxiliary functions are defined as

F0(z, ej) =

+ 1

r2

zej 2

 2

exp

(- 22

)dr,

(46)

F1(z, ej) =

+ 1

(r - zej)2

 exp (-

zej 2

2

22

)dr,

(47)

where z  [0, 1]. Based on F0(z) and F1(z), the inference probability of multi-hop out-neighbors is presented as follows.

Theorem 8 (Lower probability bound of neighbor inference). Given the maximum false alarm probability , implement the

test (44) from input ej  ejm

1=-h2op2toerf- hm ij1i-n(h1o-p2a)n,dwtehehasvinegle-time

excitation

Pr{i  Njo,uht}  F1(mijin, ejm)(2--F1(mijax, ejm)), (48)

where mijin and mijax are given by

mijin = min{ij(l), l = 1, · · · , h}, mijax = max{ij(l), l = 1, · · · , h}.

(49)

Proof. The proof is provided in Appendix H.

Theorem 8 provides the lower probability bounds for Pr{i  Njo,uht} given the maximum false alarm probability  of the test (44) in each round. A notable characteristic of the bounds is that with just a single time excitation input, they can be calculated recursively. The higher the hop number is, the lower the probability bound is. The practical application of this test is similar to (43) and omitted here.
As a summary of the theoretical analysis in aforementioned sections, we present the performance comparisons of our proposed causality-based and excitation based methods, and other typical algorithms in Table I.

10

TABLE I COMPARISONS OF THE PROPOSED METHOD WITH OTHER REPRESENTATIVE ALGORITHMS

Method

Topology Structure

System Stability

Noise Consideration

Undirected Directed Asymptotical Marginal Process Observation

Convergence Speed

Asymptotic Error

Granger estimator in [13] Spectral method in [21]

O(

1 L

1
)

O(e-L

2
)

constant zero (noise-free)

OLS estimator

up to O(

log T T

3
)

constant

Our method Excitation-based method

up to O(

log T

T

)

zero

4

at least O(erf(ej ))

zero

1 L here refers to the number of multiple trajectories. 2 In [21] the authors implement multiple times of observation over the system, with the same

initial state while ending at different moments, and no noise terms are involved. 3 T here refers to the number of the observation horizon in single

trajectory.

4

We

remark

the

convergence

here

refers

to

the

probability

that

the

positive

hypothesis

is

right,

corresponding

to

1

-

e

|ej |
-

1.

VII. EXTENDED DISCUSSIONS
In this section, we provide some extended the results in Section V and VI to more complex cases of NDSs.
A. Identification of Switching Topologies
In many cases, the topology structure dynamically changes with time due to factors, e.g., the communication link between two nodes lost. It is a quite challenging issue to infer the internal topologies series from the observations. Most existing works generally assume that the switching moment between two adjacent topologies is priorly known, which is impractical in many situations. Based on the following assumption, we propose a switching-identification method to illustrate how to find the switching moments and evaluate the performance.

identify whether the system topology has changed. In practice,

if the y^t+1 - yt+1 2 exceeds the bound given by (50),

then with high probability the original topology has switched.

Consequently, the switching moment is also found, which lays

the foundation to infer the topologies in different time slots.

Note that if the topology matrix is row-stochastic, then the

upper bound is further reduced to

W^ -W

2 F

yt 2 +22 +.

Remark 5. Using (50) only plays a sufficient role to identify the topology switches. In fact, it is totally possible that even a topology changes, the resulting state is still the same as the one by the original topology. Therefore, we claim that there is no necessary condition for an identifiable topology switching from the perspective of the state deviation.

Assumption 3. The topology does not quickly switch and each topology should maintain at least for a certain slot (e.g., n+1 moments). All the possible topologies must contain at least one spanning tree.
Corallary 1 (Error bound of state prediction). Given yt, suppose we have an estimate W^ of W and predict yt+1 by y^t+1 = W^ yt, then we have
E{ y^t+1 - yt+1 2}  W^ - W 2 yt 2 + 22 + 2. (50)
Proof. Following from the observation relationship (18), the key point is that given yt, t, t, t+1 are all independent of each other and their prediction errors are uncorrelated. Then, given y^t+1 = W^ yt, one has
y^t+1 - yt+1 = (W^ - W )yt + W t - t+1 - t. (51)
Then, it follows from (51) that
E{(y^t+1 -yt+1)(y^t+1 -yt+1)T} =(W^ - W )ytytT(W^ - W )T + 2(W W T - I)+2I. (52)
Hence, by utilizing W  1 one has E{ y^t+1 -yt+1 2} = W^ - W 2 yt 2 + 22 + 2. The proof is completed.
Corollary 1 gives the observation prediction error bound in sense of expectation, potentially providing a beneficial tool to

B. Inference by Multiple Excitations

Theorem 7 and 8 illustrate the conditions and performances of using just one-time excitation, however, there are also situations where a large excitation input is not allowed in the network dynamics, making the methods not directly available. To overcome this deficiency, multi-excitation is a promising alternative to achieve the inference goal. In this part, we will briefly show how to address the issue.

Suppose node j is excited m times with the same excitation

input ej, the inference center obtains the average observation

m

deviation

of

m

rounds

by

y~mi¯,

=

1 m

yi,(l).

l=1

Corallary 2 (Upper bound of the misjudgement probability under multiple excitations). Given excitation input ej > 0 and implement m times of excitations, the misjudgement
probability satisfies

+

1

z2

e(m)  2

q0 ej

 2/ m

exp

(-

22/m

)dz,

(53)

2

where q0 = min{wij : j  V}.

Proof. Based on the independent identically distributed char-

acteristic

of

yi,(l),

yi,(l)

is

subject

to

N (0,

2 m

).

Then,

the

11

misjudgement probability is calculated by

e(m) = Pr{D1|H0} + Pr{D0|H1}

+

1

z2

=

wij ej 2





2/ m

exp

(-

22 /m

)dz

+

wij ej 2
-

1





2/ m

(y exp (-

- wij ej 22 /m

)2

)dz

+

1

z2

=2

wij ej 2





2/ m

exp

(-

22 /m

)dz

+

1

z2

2

q0 ej 2





2/ m

exp

(-

22 /m

)dz,

(54)

which completes the proof.

It is easy to see from Corallary 2 that, as m grows the

variance 2 /m decrease and so does e(m). Therefore, it follows that

lim
m

e(m)

=

0.

(55)

Corollary 2 illustrates that even when the magnitude of the
excitation input is constrained, the misjudgment probability
can be significantly reduced by increasing the excitation times. Due to wij is not priorly known, we can relax the decision threshold as in (43). Given the maximum available excitation input ejmax and specified weight threshold wij, one has with probability at least 1 - ¯e,m to discriminate whether i  Njout by the following multiple excitation testing

 i  Njout, if |y~mi¯,| 

m l=1

ymax(l)

+

wij ejmax ,

m

2

i / Njout, else,

(56)

where ¯e,m = 2

+
wij ejmax

 1 2 / m

exp

(-

z2 22 /m

)dz

.

A

mi-

2
nor drawback of this method is that if the weight between

two nodes is small and the excitation time is also limited, an

existing edge may be regarded as not existing.

C. Nonlinear Cases

The nonlinearities of the NDS model mainly come from two aspects. First, the magnitude of the system state cannot be unbounded, thus the input torque is bounded [53]. Second, the edge weight in the topology matrix is not necessarily static, and it can be highly dependent on the state difference of its associated two nodes [54]. Mathematically, the two kinds of nonlinearities can be uniformly formulated by

xit+1 = xit +

n j=1

ij (xjt

-

xit),

(57)

where ij(z) is a continuous and strictly-bounded function, and ij(z) = 0 if aij = 0 or z = 0. As for the conditions of ij to guarantee the convergence and stability of the NDS, the readers are referred to [54]. Note that it is difficult to obtain the actual input form of each agent and find the internal edge weight, however, their internal adjacent structure is unchanged, which is also critical knowledge about the NDS.
Next, we will illustrate how to use our proposed revised casualty-based estimator to infer the adjacent structure. The

Algorithm 3 Infer the topology structure of nonlinear cases

Input: Observations {yt}Tt=0, and node set V. Output: Binary adjacent matrix estimator A^ = [a^ij]ji==11::nn. 1: Calculate the total regression time T = T - n + 1.

2: for l  1 to T do

3: for i  V do

4:

-i (l) =

l+n-1 t=l

(yti

-

[ Yl-:l+n-11n
n

]i)2,

+i (l) =

l+n t=l+1

(yti

-

[ Yl-+1:l+n1n
n

]i)2.

5:

[y~t-]i = [yt - Yl-:l+n-11n/n]i/-i ,

[y~t+]i = [yt - Yl-+1:l+n1n/n]i/+i .

6: end for

l+n-1

7:

W~

(l)

=

arg

min

1 n

y~t+1 - W (l)y~t 22.

W (l)

t=l

8: Adopt k-means method to W~ (l) and obtain its corre-

sponding binary adjacent matrix A~(l) = [a~ij(l)]ij==11::nn.

9: end for

10: for i, j  V do 11: Ai0j = {a~ij(l) = 0 : l = 1, · · · , T },
Ai1j = {a~ij(l) > 0 : l = 1, · · · , T }. 12: a^ij = 1 if |Ai1j| > |Ai0j| or a^ij = 0 otherwise.
13: end for

key idea is as follows. First, we adopt linearization over a local time horizon sequentially and calculate the topology matrix by estimator (23). Since W contains n2 element, at least n + 1 groups of consecutive observations y are needed to obtain a least square solution of W (suppose yt+1 = W yt). Therefore, we set the local time horizon as n + 1. Then, all the estimated topology matrices are integrated to discriminate whether an edge between two nodes exists in sense of statistics. Specifically, a clustering procedure (e.g., kmeans cluster method) is adopted to automatically classify the regressed weights into connected and disconnected ones. To strengthen the classification accuracy, a voting rule is proposed to determine the connectivity of every pair of nodes. The whole procedures are summarized as Algorithm 3.
It is remarkable that since ij(xj - xi) = 0 if aij = 0 and xj - xi = 0, then the excitation-based method in Section VI is also available to identify the topology structure in nonlinear cases, especially when the observations are limited. The process is likewise to that of the linear case and is omitted here due to the space limit.
VIII. NUMERICAL EXPERIMENTS
In this section, we present extensive numerical experiments to demonstrate the effectiveness of our proposed methods, with reference to the classical Granger estimator and ordinary least estimator. First, we display the basic setup of the whole experiment. Then, we conduct groups of experiments under different conditions, including the system stability, observation scale and noise variance. Detailed analysis is also provided to demonstrate the performance of our method.
A. Simulation Setup
The most critical components are the adjacent matrix A and the interaction matrix W . We randomly generate a directed

12

(a) W  Sa.

(b) W  Sm.

(c) W  Sa

(d) W  Sm

Fig. 4. The sample matrix deviation and inference performance of EG utilizing multiple observation trajectories and EC utilizing single observation trajectory. The same topology structure and initial system state are set for two groups of experiments. (a)(b): Sample matrix deviation. (c)(d): Inference performance.

(a) W  Sa

(b) W  Sm

(c) W  Sa

(d) W  Sm

Fig. 5. Performance comparison of EO, EC and ECM utilizing single observation trajectory. The same topology structure and initial system state are set for two kinds of experiments. (a)(b): Results in small observation scale. (c)(d) Non-asymptotic rates as T increases.

topology structure with |V| = 20, and the weight of W is
designed by the Laplacian rule. Both W  Sa and W  Sm are considered. For generality, the initial states of all agents
are randomly selected from the interval [400, 600], and the variance of the process and observation noise satisfy 2 = 1 and 2 = 1. To evaluate the inference accuracy, we use the following index as

error = W^ - W .

(58)

For simple expression, hereafter we denote the Granger estimator (13), the OLS estimator (15), the proposed causalitybased method (20) and the correlation-based modification method (23) by EG, EO, EC and ECM , respectively.

B. Results and Analysis
Let us begin with examining the sample matrix deviation in EG and EC , i.e., conclusions in Theorem 2 and 4. The results are reported in Fig. 4(a) and Fig. 4(b). When W  Sa, the sample matrix R0x(T ) from multiple observation trajectories can be approximated by the sample matrix 0(T ) from single observation trajectory as T  . When W  Sm, the deviation norm between R0x(T ) and 0(T ) goes to infinity as T   as shown in Fig. , and this is because the influence of the process noise will remain as the system evolves. Then, we directly present the inference errors of the methods in terms of sample size, covering both marginal and asymptotic cases. By joint inspection of Fig. 4(c) and Fig. 4(d), the accuracy magnitudes of EG and EC are very close, which verifies the conclusion of Theorem 1 that the inference result from a single observation round becomes equivalent with that from multiple

observation rounds when T  . Besides, it is found that

EC behaves worse than EO when in small observation scale, but outperforms EO asymptotically, while the accuracy of EC remains stable as the data size increase. The main reason is that

the statistical characteristic of observation noises will matter

a lot when the observation scale is large, which is not taken

into account by the OLS method.

Next, the inference performance of EO, EC and ECM

are evaluated under the same single observation round, as

demonstrated in Fig. 5. For a asymptotic stable NDS, ECM outperforms EO asymptotically but is still worse than EC , as illustrated in Fig. 5(a). For a marginal stable NDS, ECM has almost the same inference performance as EO, shown in Fig. 5(b). We note that EC applies to cases with a large observation scale, while ECM applies to other situations with no worse performance than EO. In Fig. 5(c) and Fig. 5(d), the

analysis of the non-asymptotic inference errors in Theorem

6 is verified. The upper bounds of the inference errors of

two estimators are drawn in the figures, providing explicit

expressions in terms of the observation scale T . From the inset

plots, we can appreciate that the proposed EC exhibits better performance than EO. Remarkably, the inference error of EC converges to zero while that of EO converges to constant, as T increases.

Now we move on to verify the performance excitation-based

method, as shown in Fig. 6. Specifically, we excite a target

node j and wish to find its one-hop out-neighbor i subject

to wij  0.4 in case of W  Sa and wij  0.5 in case of

W  Sm. Given the lower probability bound ¯e, the critical

excitation input is calculated by |ej| = 2

2(i) wij

erf-1

(1

-

¯e

).

Then, we use the input to conduct the test 1000 times and

13

(a) W  Sa

(b) W  Sm

(a) Case 1 of ij

(b) Case 2 of ij

Fig. 6. Example of inferring single-hop neighbors of a node using excitationbased inference results. The accuracy is obtained by implementing the test 1000 times and computing the ratio of positive results.

Fig. 7. Examples of inferring the topology structures of nonlinear dynamics

xit+1 = xit +

n j=1

ij (xjt

-

xit )

+

ti .

computing the ratio of positive results. As one expects, larger

excitation input ensures higher accuracy of the decision results,

and given the same level probability lower bound, smaller

topology weight requires larger excitation input magnitude

(comparing Fig. 6(a) with Fig. 6(b)). We note that the dashed

lines are lower bounds of the accuracy in theory, thus it makes

sense that the actual accuracy in experiments is higher than

that bound. The multi-hop neighbor inference and multiple

excitation cases are likewise and are omitted here.

Finally, we focus on the evolution of Algorithm 3 in non-

linear dynamics cases, namely, inferring the binary topology

structure in the NDS. To this end, we adopt two representative

cases of nonlinear model xit+1 = xit +

n j=1

ij

(xjt

-

xit

)

+

ti

,

where ij is given by

Case

1

:

ij

=

sign(aij )|xjt 1+

- xit|(xjt - xjt - xit 2

xit) ,

jNi

(59)

Case

2

:

ij

=

aij (xjt

-

xit)( 1

+

2 exp{-(xjt

-

xit)}

-

1).

According to the sufficient conditions in [54], both the two systems will reach stable states when noise-free. We repeat the experiments under different process noise level with segment number in Algorithm 3 increases, where  is set as 0.02, 0.2, 0.5 and 1, respectively. Here to evaluate the performance of Algorithm 3, we adopt th structure error index, given by

errors = abs(A^ - A) 0/N 2,

(60)

where abs(·) means obtaining the absolute value of each entry in the matrix variable, and · 0 represents the number of nonzero entries of matrix. As we can see, the accuracy generally grows with the segment number at first and remains stable when the number is large. Perhaps unexpectedly, given the same segment number, the inference accuracy trend in the former case does exhibit the trend in the latter one, where the accuracy increases with . However, we remark that this behavior is determined by the intrinsic characteristic of nonlinear dynamics, and when the noise variance exceeds a certain value, the accuracy will not change a lot.

IX. CONCLUSIONS
In this paper, we investigated the principles and performances of inferring the topology of NDSs, from the perspective of node causality and correlation. First, we propose

a causality-based estimator that allows for the presence of unknown observation noises, along with its correlation-based modification design to alleviate performance degradation in a small observation scale. By relating the proposed estimator for a single observation round with the Granger estimator for multiple rounds, we prove their equivalence when the NDS is asymptotically stable, and demonstrate the incremental characteristic of their sample matrices deviation in marginally stable cases. Then, we rigorously analyzed the convergence rate and accuracy of the proposed estimator by utilizing concentration measure, demonstrating that our method has superior inference performance to the OLS estimator. Besides, for cases where the observations are not sufficient to support the above estimators, an excitation-based method was designed to infer the binary topology structure in the NDSs by leveraging hypothesis testing. Finally, some extensions on switching topologies, nonlinear dynamics and multi-step excitation inference scenarios were discussed. Performance study by simulations verified our performance analysis.
The study of this paper provides meaningful insights into the topology inference problem, and paves the way for several interesting avenues of future research, including i) investigating a richer class of NDS models, including non-stochastic input and generative switching topologies cases; ii) developing more novel nonlinear inference algorithms (e.g., online, distributedly) with certain node causality and correlation as priors; iii) analyzing the performance of the topology inference methods for NDSs under more complicated dynamic settings; and iv) using the inference methods to topology-related applications, like anomaly detection and state prediction in NDSs.

APPENDIX

A. Proof of Lemma 2

Proof. The proof is conducted in element-wise analysis. To

T

ease

notation,

we

denote

(T )

=

1 T

T

ZTT

=

t-1ztT-1,

t=1

and

the

element

ij (T )

=

1 T

[T

ZTT]ij

is

calculated

by

1 ij(T ) = T

T

ti-1ztj-1.

(61)

t=1

14

Since t  N (0, 2), it follows that

1 E[ij(T )] = T

T

E[ti-1]ztj-1 = 0,

(62)

t=1

D[ij(T )] =

T

( ztj-1 )22  (|z|jmax)22 ,

T

T

(63)

t=1

where |z|jmax = max{|ztj|, t = 0, 1, · · · , T -1}. By the famous Chebyshev inequality, given arbitrary > 0, we have

Pr{|ij(T )| <

}1

-

D[ij(T )]
2

1

-

(|z|jmax)22 T2

.

(64)

Consequently, i, j  V, Pr{|ij(T )| <

}1-

(|z |jmax )2  2 T2



1

-

(|z |max )2  2 T2

,

which

completes

the

first

statement.

Next, if |z|m < , T  N+, when T  , it yields that

lim Pr{|ij(T )| < } = 1.

(65)

T 

Finally, in the matrix form, (65) is equivalent to Pr{ lim (T ) = 0} = 1. The proof is completed.
T 

B. Proof of Theorem 1 Proof. Substitute (18) into 1(T ) and it follows that

1(T )

=

1 T

(YT+

)(YT-)T

=

1 T

T

ytytT-1

t=1

W =
T

T

yt-1ytT-1

+

1 T

T

(t-1 +t - W t-1) ytT-1. (66)

t=1

t=1

Note

that when

W



Sa,

lim
t

yt

<  holds almost

surely. Since t-1 and t are independent of yt, applying

Lemma 2 on 1(T ), it yields that

1 lim T  T

T

t-1ytT-1 = 0,

1 lim T  T

T

tytT-1 = 0.

(67)

t=1

t=1

Recalling t-1 is independent of xt-1, it follows that

1 lim T  T

T

t-1 ytT-1

=

lim
T 

1 T

T

t-1(xt-1 + t-1)T = 2 I.

t=1

t=1

Then, one infers that

lim 1(T ) = W
T 

lim
T 

0

(T

)

-

2

I

.

(68)

Hence, the proof is completed.

C. Proof of Theorem 2

Proof. Without losing generality, we first consider 0(T ). Substituting the expanded form (6) of yt into ytytT, one obtains

ytytT = (W tx0 + t)(W tx0 + t)T,

(69)

t

where t =

W m-1t-m + t and 0 = 0. Then, ytytT

m=1

is expanded as

ytytT = W tx0xT0 (W t)T+W tx0tT+txT0 (W t)T+ttT . (70)

Qt1

Qt2

Qt3

Qt4

Based on (70), the proof is equivalent to separately find the
average of the summation of each part in (70) along the observation horizon T .
First, consider taking the average of all {Qt1}Tt=-01. Note that when W  Sa  Sm, W x0 converge to a constant vector. Therefore, it yields that

lim
T 

1 T

T -1
Qt1

= lim
T 

1 T

T -1
W tx0xT0 (W t)T

t=0

t=0

=W x0xT0 (W )T.

(71)

T -1

T -1

Next,

consider

1 T

Qt2

=

1 T

W tx0tT. Since t is a

t=0

t=0

typical linear combination of Gaussian noises {m}tm-=10 and

t, and is independent of W tx0, by Lemma 2, one infers that

lim
T 

1 T

T -1
Qt2

=

lim
T 

1 T

T -1
(W tx0)tT

=

0.

(72)

t=0

t=0

The average of all {Qt3}Tt=-01 is likewise, i.e.,

lim
T 

1 T

T -1
Qt3

=

0.

(73)

t=0

T -1

Then,

focus

on

the

calculation

of

1 T

Qt4. Since

t=0

E(ttT) = 2I and E(ttT) = 2I, one can divide Qt4 as





t

t

Qt4 =

W 

m-1t-m(t

+

tT-m1

(W

m1

-1

)T

) 

m=0

m1 =0, m1 =m

t

+ t

tT-m1 (W m1-1)T

m1 =0

t

+

W m-1t-mtT-m(W m-1)T + ttT.

(74)

m=0

Consider the first term in Qt4. For simple expression, define

t

ta(m) = W m-1t-m, tb(m) =

W m1-1t-m1 . (75)

m1 =0, m1 =m

As W  Sa, one can infer that lim
t

tb(m)

< . Therefore,

by the famous Lebesgue's dominated convergence theorem and

Lemma 2, it follows that with probability one

1T lim T  T

t
ta(m)(tb(m))T

t=1 m=0

t



ta(m)(tb(m))T

=

lim m=0

= 0.

(76)

T 

T

t=0

Likewise, for the second term in (74), it also holds that

1 T -1

lim T  T

t

t=0

t
tT-m1 (W m1-1)T
m1 =0

= 0.

(77)

15

As for the last two parts in (74), recalling D[t] = 2I and D[t] = 2I, and one infers that

T -1 t

T -1

W m-1t-mtT-m(W m-1)T +

ttT

lim t=0 m=0

t=0

T 

T


= W t lim
T  t=0

T -t
T -mTT-m/T
m=0

(W t)T + 2I

=2



W t(W t)T

+

2 I

=

lim
T 

1 T

T -1
Qt4.

t=0

t=0

(78)

Finally, taking (71)-(73) and (78) into lim 0(T ) implies
T 
that

lim 0(T )
T 

=

lim
T 

1 T

T -1
(Qt1

+

Qt2

+ Qt3

+ Qt4)

t=0

=

lim
T 

1 T

T -1
(Qt1

+

Qt4)

t=0



= W x0xT0 (W )T + 2 W t(W t)T + 2I

t=0

= R0x() + 2I.

(79)

The proof of 1() = R1x()+2W is likewise and omitted here. The proof is completed.

D. Proof of Theorem 4

Proof. We proceed this proof based on the analysis in Theorem 2. The key point is to reveal the growing characteristic of the deviation norm in terms of T . Recall t =
t
W m-1t-m + t (0 = 0), and the deviation matrix
m=1
(0(T ) - R0x(T ) - 2I) is expanded as

0(T ) - R0x(T ) - 2 I

=

1 T

T
(

-1
(W

T
tx0)tT+

-1

txT0 (W

T
t)T+

-1

ttT)-2

T

-1

W

t(W

t )T

t=0

t=0

t=0

t=0

+

1 T

T -1
W tx0xT0 (W t)T

- W T -1x0xT0 (W T -1)T - 2 I.

t=0

(80)

Due to W  Sm, lim W m exists, i.e., |[W m]ij| < , m 
m
R+. Therefore, the spectral norm of the last three terms in (80)
is bounded. Then, the major focus is laid on the first three terms. Considering an element-wise analysis of W m-1t-m, the noise variance in every dimension is given by

n
D([W m-1t-m]i) = 2 ([W m-1]ij )2, i  V. (81)
j=1

Since [W m]ij is strictly bounded, there exists a n-dimension state vector x~ and a Gaussian noise ~  N (0, ~2I) such that

W mx0  x~ < , D([W m-1t-m]i)  D(~[i]) = ~2. (82)

Based on (82), we define a revised version of t by ~t =
t
~t + t (~0 = 0), and the alternative of the first three
m=1
terms in (80) is given by

E

=

1 T

T -1

T -1

(x~ ~tT + ~tx~T +

T

~t~tT).

(83)

t=0

t=0

t=1

J1(T )

J2(T )

J3(T )

In the sequel, we turn to analyze the the asymptotic performance of E to demonstrate that of 0(T )-R0x(T )-2I .
First, look at the each entry in J1[ij](T ) = x~[i] T -1 ~t[j],
t=0
which satisfies

E{J1[ij](T )} = 0, D{J1[ij](T )} = (x~[i])2~2T.

(84)

By the Chebyshev inequality, given 0 <  < 1, one has

Pr{|J1[ij](T )| 

T ~|x~[i]|}  1 - . 

(85)

Therefore, it yields that at least with probability 1 - ,

|J1[ij](T )/T | 

1 ~x~m, i, j  V, T

(86)

where x~m = max{|x~[i]|, i  V}. Note that J1(T ) = J2T(T ), thus the bound in (86) also applies to J2[ij](T ).
Next, consider the entries in J3(T ). Since the autocorrelation of {~t}Tt=1} and {~t}Tt=1} are involved, it can be further expanded as

T -1 t t

t

J3(T ) = (

~t1 ~tT2 +

~t1 ~tT1

t=0 t1=0 t2=0,
t2 =t1

t1 =0

t

t

+

~t1 tT1 +

~t1 tT1 + ttT).

t1 =0

t1 =0

(87)

Recall that the product of two independent Gaussian variable also subjects to Gaussian distribution, thus it follows that

E{(~t1 ~tT2 )[ij]} = 0, E{(~t1 ~tT1 )[ij]} = 0,

D{(~t1 ~tT2 )[ij]}

=

~2 ,
2

D{(~t1 ~tT1 )[ij]}

=

~22 ~2 + 2

,

(88)

where t1 = t2. Applying the Chebyshev inequality again, for each entry in J3(T ), one has with probability at least 1 - 

|J3[ij](T )| 

T -1

t(t + 1)

t=1
2

~ + 2

T
t
t=1


T
+(

~2 t)

+

T 2 .



t=1

~22 ~2 + 2
(89)

Then, divide T into J3(T ) and do series summation, yielding

| J3[ij](T ) |  T

2T 3 - 3T 2 + T 12T 2 ~ +

2T 2 - T T 2

+ T + 1 ~2 + 2 J¯(T ).

2



~22 ~2 + 2
(90)

16

Finally, utilizing the inequality B  B F 

B[ij]

i,jV

(B  Rn×n), it is induced that with probability at least 1 - 

J3(T ) n2J¯(T )  O( T ).

(91)

T



Note that R0x(T ) = 2

T -1 t=0

W

t(W

t

)T

 T 2, then

there

exists

a

possibility

that

the

part

with

factor

T

in

J3(T ) T

can be offset with R0x(T ) + 2I, i.e.,

T -1 t
W m-1t-mtT-m(W m-1)T

R0x(T )+2 I- t=0 m=0

T

T- O(1). (92)

However, even if the situation in (92) happens, by (90) one can infer that 0() - R0x() - 2I  O( T ) holds, indicating the deviation still goes to infinity with the increase

of T . Hence, the spectral norm of the deviation (80) at least satisfies O( T ), which completes the proof.

E. Proof of Theorem 5

Proof. The proof is rather similar to that of Theorem 2. The key idea is to prove the upper bounds of the estimation error, by leveraging the concentration measure in Gaussian space. Since W^ o = YT+(YT-)T(YT-(YT-)T)-1, one obtains the estimation error matrix EW by
EW = W^ o - W = (T + +T - W -T )(YT-)T-T 1. (93)
Due to W  1 and YT- = XT- + -T holds, EW is upper bounded by

EW  T (YT-)T-T 1 + +T (YT-)T-T 1 + -T (XT-)T-T 1 + -T (-T )T-T 1 .

(94)

Then, the proof is turned to bound each term of the right-hand
side in (94) individually. · Part 1: Upper Bounding T (YT-)T~ -T 1 . Utilizing 0  dn ~ T up, it yields that

T + dn 2T = (T + dn)-1 -T 1/2, (95)

(T

+

dn

)-

1 2



-T

1 2

 / 2.

(96)

By Lemma 4, one has probability at least 1 -  that

T

~ -T

1 2





1

5 det ~ T -dn1 + I 2n

16n log  

1/n

. (97) 

Further, substitute ~ T

up into in (97) and use

~ T-

1 2



1

and one obtains

min (dn )

T ~ -T 1 

1 min(dn)

 5 det
16n log 

up-dn1 + I 1/n

1
2n
.

(98)

· Part 2: Upper Bounding T (YT-)T~ -T 1 . Since {t, yt-1}Tt-1 are dependent with other, thus the same upper of (98) also applies to +T (YT-)T~ -T 1 .

· Part 3: Upper Bounding -T (XT-)T~ -T 1 . XT-N(ote-T )tTh+at-TYT(-X(TY-T)-T)+T-T=(X-TT-)T(XT-0.)TTh+enV~, o, newchaenrediVr~ectl=y apply Lemma 4 and obtain

-T 1/2XT--T 



1

5 det T V~ -1 + I 2n

8n log  

1/n

. (99) 

Further taking -T 1XT--T ,

dn XT-(XT-)T a loose upper bound

~ T up and is given by

(99)

into

-T 1XT--T 

1 min(dn)

9n log

5

det(up-dn1

+

I

)

1 2n

.

1/n

(100)

· Part 4: Upper Bounding -T (-T )T~ -T 1 . Applying Lemma 3 to -T (-T )T, and it is not difficult to obtain that with probability at least 1 - 2 exp -r2/2

 (T

-

 n

-

r)2 2 I

-T (-T )T

 (T

+

 n

+

r)2 2 I .

(101)

Let r =

2

log

2 

and

focus

on

the

right

side

of

-T (-T )T.

 When T  T = ( n +



2

log

2 

)2/(

5/2 - 1)2, one has

with probability 1 -  that

3T 2 I 4

-T (-T )T

5T 2 I. 4

(102)

It follows from (102) and ~ -T 1  1/min(dn) that

-T (-T )T~ -T 1  -T (-T )T

~ -T 1

 5T 2 . 4min(dn)

(103)

Finally, combining the upper bounds of four parts leads to

 11 EW 

n log

( ) 5 det up- dn1+I

1 2n

1/n

min(dn)

which completes the proof.

 +  5T  
4 min(dn)
,
(104)

F. Proof of Theorem 6

Proof. The key point of this proof is to provide a group of
explicit dn and up about T . First we focus on analyzing the case of W^ o and then easily extend the analysis to that of W^ C . Note that we are not interested in finding the best dn and up rather illustrate their existence.
For dn, it has been proved that (in Proposition 8.5. [52]) there exists some scalar functions n that depends only on n, such that ~ T nT I. Then, it follows that

-dn1

1

1

1



=

 O( ).

min(nT I) nT

T

(105)

For up, note that the following inevitable always holds

T -1

~ T tr

ytytT I = up

t=0

(106)

17

When W  Sm, part of the upper bound of [up]ij is

given by (89) in the proof of Theorem 4, demonstrating

[ in

T -1 t=0

ytytT][ij]



Theorem 2 that

O(T 2). ~ T /T

When T-

W  Sa, it is also proved

2 W t(W t)T + 2I ,

t=0

which is strictly bounded. Therefore, one can easily infer that

O(T 2), up  O(T ),

if W  Sm, if W  Sa.

(107)

Combine the two factors (105) and (107), and it follows that

det(up-dn1 + I) 

O(T ), O(1),

if W  Sm, if W  Sa.

(108)

Taking (105) and (108) into (11

n

log( 5

det(up

- dn1

+I

)

1 2n

1/n

)+

 5T  )/
4 min(dn)

min(dn) yields the relationship given by

(31), which completes the proof of the first statement.

Next, consider the the non-asymptotic bound of W^ c -W .

Let T, = T 0(T - 1) - T 2W , and the causality-based

estimator (20) is rewritten as

W^ c = YT+(YT-)T-T,1 .

(109)

Then, the inference error is given by

W^ c - W =(T + +T - W -T )(YT-)T-T,1 + T 2W -T,1

=(T + +T )(YT-)T-T,1 - W -T (XT-)T-T,1

+ W (T 2I - -T (-T )T)-T,1 .

(110)

Consequently, the upper bound is given by

W^ c - W  T (YT-)T-T,1 + +T (YT-)T-T,1
+ -T (XT-)T-T,1 + (T 2 I - -T (-T )T)-T,1 . (111)

Note that the first three terms in (111) share the same upper bound forms as T (YT-)T-T 1 , +T (YT-)T-T 1 and -T (XT-)T-T 1 in W^ o - W . The proof is similar to that of Theorem 5 and is omitted here. As for the last term in RHS
of (111), one has with high probability that

lim
T 

T 2I - -T (-T )T

/T = 0.

(112)

Therefore, the upper bound of W^ c -W is determined by the first three terms in (111), which converge to zero as T  . The second statement in Theorem 6 is proved.

G. Proof of Theorem 7

Proof. The proof consists of two steps. First, we prove the

decision threshold z0

is given by z0

=

. wij ej
2

Then,

we

demonstrate the critical excitation magnitude under the z0.

For simplicity without losing generality, we begin with the

case where the excitation input ej > 0. Note that i is a

continuous random variable, the likelihood ratio lr(z) in the

test is given by

lr (z )

=

f(z|H1) , f (z |H0 )

(113)

where f(·) is the probability density function of i. Due to the prior probabilities Pr(H1) = Pr(H0), the decision threshold z0 satisfies

lr (z0 )

=

f (z0 |H1 ) f (z0 |H0 )

=

Pr{H1} Pr{H0}

=

1.

(114)

Since wi  N (0, 2 ), substituting f(y) =

1 2

exp

(-

z2 22

)

into

(114),

it

yields

that

lr (z0 )

=

exp

(-

(z0

-wij 22

ej

)2

)

exp

(-

z02 22

)

=

1.

(115)

It follows from (115) that z02 - (z0 - wijej)2 = 0, leading to

z0

=

wij ej 2

.

(116)

Next, by the definition of e, one has

+ 1

z2

e = Pr{D1|H0} + Pr{D0|H1} =

z0

 2

exp (- 22

)dz

z0

1

+

exp (- (z - wij ej )2 )dz.

- 2

22

(117)

Substitute z = z

+ wij ej
2

=z

+ z0

into (117), yielding

e = Pr{D1|H0} + Pr{D0|H1}

=

+


1

exp (- (z + z0)2 )dz

0 2

22

+

0

1



- 2

exp (- (z

- z0 22

)2

)dz

+ 1 =2 

exp (- (z + z0)2 )dz

0 2

22

+ 1

z2

=2 

exp (- )dz.

z0 2

22

(118)

Note that

+ 1 z0 2

exp

(-

z2 22

)dz

=

(1 - erf( z0 ))/2,
2

thus

it yields that

e = 1 - erf( z0 ). 2

(119)

Substituting z0 =

wij ej 2

and e = ¯e

into (119), we obtain



ej

=

2

2 wij

erf-1(1

-

¯e).

(120)

The result is likewise when ej < 0 due to the symmetry of

Gaussian distribution. By the monotone increasing property of

erf(z), to guarantee e  ¯e, the excitation input must satisfy

|ej |



2

2 wij

erf-1

(1

-

¯e).

The

proof

is

completed.

H. Proof of Theorem 8
Proof. The proof consists of three steps. Denote the false alarm probability by f (h) = Pr{D1(h)|H0(h)} and the missed detection probability by m(h) = Pr{D0(h)|H1(h)}. We first prove the critical excitation magnitude for identifying the neighbors within h-hops. Then, we find the lower and upper bounds of f (l) and d(l).

18

Based on the famous Neyman-Pearson rule, with a specified f = , one has

=

+ 1 
z0 2

z2 exp (- 22

)dz =

(1

-

erf( z0
2
2

)) .

(121)

It follows from (121) that 
z0 = 2erf-1(1 - 2).

(122)

Due to the prior probabilities Pr{H1} = Pr{H0} and based

on Lemma 5, z0

=

ij (h)ej 2

also holds at h-step response.

Substituting it into (122), it yields that

ej

=

 2 2

erf-1(1

-

2) .

ij (h)

(123)

Next, note that F0(z, ej) decreases with zej increasing. If

the excitation input is designed such that

ejm

=

22erf-1(1 - 2) ,
min{ij(h), h = 1, · · · , n}

(124)

then one infers that

f (l) = F0(ij(l), ejm)  , 1  l  h.

(125)

Meanwhile, recall the detection probability d(h) = Pr{D1(h)|H1(h)} is calculated by

d(h) =


ij (h)ej 2

1 2

(z exp (-

-

ij (h)ej 22

)2

)dz.

(126)

Since d(h) increases with ij(h)ejmax increasing, one has

F1(mijin, ejm)  d(h)  F1(mijax, ejm).

(127)

Finally, utilizing the Law of Total Probability, the probability that i is decided as member of Njo,uht is calculated by
Pr{i  Njo,uht} = Pr{D1(h)|H1(h)} Pr{D0(h - 1)|H0(h - 1)} + Pr{D1(h)|H1(h)} Pr{D0(h - 1)|H1(h - 1)}. (128)

Substitute Pr{D0(h - 1)|H0(h - 1)} = 1 - f (h - 1) and Pr{D0(h - 1)|H1(h - 1)} = 1 - d(h - 1) into (128), and it yields that

Pr{i  Njo,uht} =d(h)(2 - f (h - 1) - d(h - 1)) F1(mijin, ejm)(2--F1(mijax, ejm)). (129)

The proof is completed.

REFERENCES
[1] Y. Li and J. He, "Topology inference for networked dynamical systems: A causality and correlation perspective," in 60th IEEE Conference on Decision and Control. submitted, 2021.
[2] R. Olfati-Saber, J. A. Fax, and R. M. Murray, "Consensus and cooperation in networked multi-agent systems," Proceedings of the IEEE, vol. 95, no. 1, pp. 215­233, 2007.
[3] M. Nokleby and W. U. Bajwa, "Stochastic optimization from distributed streaming data in rate-limited networks," IEEE Transactions on Signal and Information Processing over Networks, vol. 5, no. 1, pp. 152­167, 2018.
[4] A. Ahmed and E. P. Xing, "Recovering time-varying networks of dependencies in social and biological studies," Proceedings of the National Academy of Sciences, vol. 106, no. 29, pp. 11 878­11 883, 2009.

[5] R. P. Monti, P. Hellyer, D. Sharp, R. Leech, C. Anagnostopoulos, and G. Montana, "Estimating time-varying brain connectivity networks from functional MRI time series," NeuroImage, vol. 103, pp. 427­443, 2014.
[6] C. Liu, J. He, S. Zhu, and C. Chen, "Dynamic topology inference via external observation for multi-robot formation control," in 2019 IEEE Pacific Rim Conference on Communications, Computers and Signal Processing (PACRIM). IEEE, 2019, pp. 1­6.
[7] S. Mahdizadehaghdam, H. Wang, H. Krim, and L. Dai, "Information diffusion of topic propagation in social media," IEEE Transactions on Signal and Information Processing over Networks, vol. 2, no. 4, pp. 569­581, 2016.
[8] M. Cheraghchi, A. Karbasi, S. Mohajer, and V. Saligrama, "Graphconstrained group testing," IEEE Transactions on Information Theory, vol. 58, no. 1, pp. 248­262, 2012.
[9] M. Mardani, G. Mateos, and G. B. Giannakis, "Dynamic anomalography: Tracking network anomalies via sparsity and low rank," IEEE Journal of Selected Topics in Signal Processing, vol. 7, no. 1, pp. 50­ 66, 2013.
[10] G. B. Giannakis, Y. Shen, and G. V. Karanikolas, "Topology identification and learning over graphs: Accounting for nonlinearities and dynamics," Proceedings of the IEEE, vol. 106, no. 5, pp. 787­807, 2018.
[11] C. W. Granger, "Investigating causal relations by econometric models and cross-spectral methods," Econometrica: Journal of the Econometric Society, pp. 424­438, 1969.
[12] A. Brovelli, M. Ding, A. Ledberg, Y. Chen, R. Nakamura, and S. L. Bressler, "Beta oscillations in a large-scale sensorimotor cortical network: Directional influences revealed by granger causality," Proceedings of the National Academy of Sciences, vol. 101, no. 26, pp. 9849­9854, 2004.
[13] A. Santos, V. Matta, and A. H. Sayed, "Local tomography of large networks under the low-observability regime," IEEE Transactions on Information Theory, vol. 66, no. 1, pp. 587­613, 2020.
[14] S. Segarra, A. G. Marques, G. Mateos, and A. Ribeiro, "Network topology inference from spectral templates," IEEE Transactions on Signal and Information Processing over Networks, vol. 3, no. 3, pp. 467­483, 2017.
[15] M. T. Schaub, S. Segarra, and H.-T. Wai, "Spectral partitioning of timevarying networks with unobserved edges," in 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019, pp. 4938­4942.
[16] Y. Zhu, M. T. Schaub, A. Jadbabaie, and S. Segarra, "Network inference from consensus dynamics with unknown parameters," IEEE Transactions on Signal and Information Processing over Networks, vol. 6, pp. 300­315, 2020.
[17] G. Karanikolas, G. B. Giannakis, K. Slavakis, and R. M. Leahy, "Multikernel based nonlinear models for connectivity identification of brain networks," in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 6315­6319.
[18] G. V. Karanikolas, O. Sporns, and G. B. Giannakis, "Multi-kernel change detection for dynamic functional connectivity graphs," in 2017 51st Asilomar Conference on Signals, Systems, and Computers. IEEE, 2017, pp. 1555­1559.
[19] S. Wang, E. D. Herzog, I. Z. Kiss, W. J. Schwartz, G. Bloch, M. Sebek, D. Granados-Fuentes, L. Wang, and J.-S. Li, "Inferring dynamic topology for decoding spatiotemporal structures in complex heterogeneous networks," Proceedings of the National Academy of Sciences, vol. 115, no. 37, pp. 9300­9305, 2018.
[20] J. Friedman, T. Hastie, and R. Tibshirani, "Sparse inverse covariance estimation with the graphical lasso," Biostatistics, vol. 9, no. 3, pp. 432­ 441, 2008.
[21] S. Segarra, M. T. Schaub, and A. Jadbabaie, "Network inference from consensus dynamics," in 2017 IEEE 56th Annual Conference on Decision and Control (CDC). IEEE, 2017, pp. 3212­3217.
[22] N. Shahid, N. Perraudin, V. Kalofolias, G. Puy, and P. Vandergheynst, "Fast robust PCA on graphs," IEEE Journal of Selected Topics in Signal Processing, vol. 10, no. 4, pp. 740­756, 2016.
[23] M. Onuki, S. Ono, M. Yamagishi, and Y. Tanaka, "Graph signal denoising via trilateral filter on graph spectral domain," IEEE Transactions on Signal and Information Processing over Networks, vol. 2, no. 2, pp. 137­148, 2016.
[24] H. E. Egilmez, E. Pavez, and A. Ortega, "Graph learning from data under Laplacian and structural constraints," IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 6, pp. 825­841, 2017.
[25] V. N. Ioannidis, Y. Shen, and G. B. Giannakis, "Semi-blind inference of topologies and dynamical processes over dynamic graphs," IEEE Transactions on Signal Processing, vol. 67, no. 9, pp. 2263­2274, 2019.

19

[26] A. Tsiamis and G. J. Pappas, "Finite sample analysis of stochastic system identification," in 2019 IEEE 58th Conference on Decision and Control (CDC). IEEE, 2019, pp. 3648­3654.
[27] S. Oymak and N. Ozay, "Non-asymptotic identification of LTI systems from a single trajectory," in 2019 American control conference (ACC). IEEE, 2019, pp. 5655­5661.
[28] C. J. Quinn, N. Kiyavash, and T. P. Coleman, "Directed information graphs," IEEE Transactions on Information Theory, vol. 61, no. 12, pp. 6887­6909, 2015.
[29] J. Etesami and N. Kiyavash, "Measuring causal relationships in dynamical systems through recovery of functional dependencies," IEEE Transactions on Signal and Information Processing over Networks, vol. 3, no. 4, pp. 650­659, 2017.
[30] R. Bowden and D. Veitch, "Finding the right tree: Topology inference despite spatial dependences," IEEE Transactions on Information Theory, vol. 64, no. 6, pp. 4594­4609, 2018.
[31] X. Dong, D. Thanou, P. Frossard, and P. Vandergheynst, "Learning Laplacian matrix in smooth graph signal representations," IEEE Transactions on Signal Processing, vol. 64, no. 23, pp. 6160­6173, 2016.
[32] B. Pasdeloup, V. Gripon, G. Mercier, D. Pastor, and M. G. Rabbat, "Characterization and inference of graph diffusion processes from observations of stationary signals," IEEE Transactions on Signal and Information Processing over Networks, 2017.
[33] D. Hayden, Y. Yuan, and J. Gonc¸alves, "Network identifiability from intrinsic noise," IEEE Transactions on Automatic Control, vol. 62, no. 8, pp. 3717­3728, 2017.
[34] S. Shahrampour and V. M. Preciado, "Topology identification of directed dynamical networks via power spectral analysis," IEEE Transactions on Automatic Control, vol. 60, no. 8, pp. 2260­2265, 2014.
[35] ----, "Reconstruction of directed networks from consensus dynamics," in 2013 American Control Conference. IEEE, 2013, pp. 1685­1690.
[36] V. Matta and A. H. Sayed, "Consistent tomography under partial observations over adaptive networks," IEEE Transactions on Information Theory, vol. 65, no. 1, pp. 622­646, 2019.
[37] G. Mateos, S. Segarra, A. G. Marques, and A. Ribeiro, "Connecting the dots: Identifying network structure via graph signal processing," IEEE Signal Processing Magazine, vol. 36, no. 3, pp. 16­43, 2019.
[38] S. Zhou, J. Lafferty, and L. Wasserman, "Time varying undirected graphs," Machine Learning, vol. 80, no. 2-3, pp. 295­319, 2010.
[39] A. J. Gibberd and J. D. Nelson, "High dimensional changepoint detection with a dynamic graphical lasso," in 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2014, pp. 2684­2688.
[40] E. Fox, E. Sudderth, M. Jordan, and A. Willsky, "Nonparametric Bayesian learning of switching linear dynamical systems," Advances in Neural Information Processing Systems, vol. 21, pp. 457­464, 2008.
[41] B. Baingana, G. Mateos, and G. B. Giannakis, "Proximal-gradient algorithms for tracking cascades over social networks," IEEE Journal of Selected Topics in Signal Processing, vol. 8, no. 4, pp. 563­575, 2014.
[42] B. Baingana and G. B. Giannakis, "Tracking switched dynamic network topologies from information cascades," IEEE Transactions on Signal Processing, vol. 65, no. 4, pp. 985­997, 2016.
[43] V. Kalofolias, A. Loukas, D. Thanou, and P. Frossard, "Learning time varying graphs," in 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2017, pp. 2826­2830.
[44] D. Hallac, Y. Park, S. Boyd, and J. Leskovec, "Network inference via the time-varying graphical Lasso," in Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2017, pp. 205­213.
[45] L. Peel and A. Clauset, "Detecting change points in the large-scale structure of evolving networks," in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 29, no. 1, 2015.
[46] S. Roy, Y. Atchade´, and G. Michailidis, "Change point estimation in high dimensional Markov random-field models," Journal of the Royal Statistical Society. Series B, Statistical methodology, vol. 79, no. 4, p. 1187, 2017.
[47] X. Jiang, S. Mahadevan, and A. Urbina, "Bayesian nonlinear structural equation modeling for hierarchical validation of dynamical systems," Mechanical Systems and Signal Processing, vol. 24, no. 4, pp. 957­ 975, 2010.
[48] J. R. Harring, B. A. Weiss, and J.-C. Hsu, "A comparison of methods for estimating quadratic effects in nonlinear structural equation models." Psychological Methods, vol. 17, no. 2, p. 193, 2012.
[49] Y. Shen, B. Baingana, and G. B. Giannakis, "Kernel-based structural equation models for topology identification of directed networks," IEEE Transactions on Signal Processing, vol. 65, no. 10, pp. 2503­2516, 2017.

[50] A. H. Sayed et al., "Adaptation, learning, and optimization over networks," Foundations and Trends® in Machine Learning, vol. 7, no. 4-5, pp. 311­801, 2014.
[51] K. R. Davidson and S. J. Szarek, "Local operator theory, random matrices and banach spaces," Handbook of the geometry of Banach spaces, vol. 1, no. 317-366, p. 131, 2001.
[52] T. Sarkar and A. Rakhlin, "Near optimal finite time identification of arbitrary linear dynamical systems," in International Conference on Machine Learning. PMLR, 2019, pp. 5610­5618.
[53] R. O. Saber and R. M. Murray, "Consensus protocols for networks of dynamic agents," in 2003 American Control Conference, vol. 2. IEEE, 2003, pp. 951­956.
[54] L. Moreau, "Stability of multiagent systems with time-dependent communication links," IEEE Transactions on Automatic Control, vol. 50, no. 2, pp. 169­182, 2005.
Yushan Li (S'19) received the B.E. degree in School of Artificial Intelligence and Automation from Huazhong University of Science and Technology, Wuhan, China, in 2018. He is currently working toward the Ph.D. degree with the Department of Automation, Shanghai Jiaotong University, Shanghai, China. He is a member of Intelligent of Wireless Networking and Cooperative Control group. His research interests include robotics, security of cyberphysical system, and distributed computation and optimization in multi-agent networks.
Jianping He (SM'19) is currently an associate professor in the Department of Automation at Shanghai Jiao Tong University. He received the Ph.D. degree in control science and engineering from Zhejiang University, Hangzhou, China, in 2013, and had been a research fellow in the Department of Electrical and Computer Engineering at University of Victoria, Canada, from Dec. 2013 to Mar. 2017. His research interests mainly include the distributed learning, control and optimization, security and privacy in network systems.
Dr. He serves as an Associate Editor for IEEE Open Journal of Vehicular Technology and KSII Trans. Internet and Information Systems. He was also a Guest Editor of IEEE TAC, International Journal of Robust and Nonlinear Control, etc. He was the winner of Outstanding Thesis Award, Chinese Association of Automation, 2015. He received the best paper award from IEEE WCSP'17, the best conference paper award from IEEE PESGM'17, and was a finalist for the best student paper award from IEEE ICCA'17.
Cailian Chen (M'06) received the B.E. and M.E. degrees in Automatic Control from Yanshan University, P. R. China in 2000 and 2002, respectively, and the Ph.D. degree in Control and Systems from City University of Hong Kong, Hong Kong SAR in 2006. She joined Department of Automation, Shanghai Jiao Tong University in 2008 as an Associate Professor. She is now a Full Professor. Before that, she was a postdoctoral research associate in University of Manchester, U.K. (2006-2008). She was a Visiting Professor in University of Waterloo, Canada (2013-2014). Prof. Chen's research interests include industrial wireless networks, computational intelligence and situation awareness, Internet of Vehicles.
Prof. Chen has authored 3 research monographs and over 100 referred international journal papers. She is the inventor of more than 20 patents. She received the prestigious "IEEE Transactions on Fuzzy Systems Outstanding Paper Award" in 2008, and Best Paper Award of WCSP17 and YAC18. She won the Second Prize of National Natural Science Award from the State Council of China in 2018, First Prize of Natural Science Award from The Ministry of Education of China in 2006 and 2016, respectively, and First Prize of Technological Invention of Shanghai Municipal, China in 2017. She was honored Changjiang Young Scholar in 2015 and Excellent Young Researcher by NSF of China in 2016. Prof. Chen has been actively involved in various professional services. She serves as Associate Editor of IEEE Transactions on Vehicular Technology, Peerto-peer Networking and Applications (Springer). She also served as Guest Editor of IEEE Transactions on Vehicular Technology, TPC Chair of ISAS19, Symposium TPC Co-chair of IEEE Globecom 2016 and VTC2016-fall, Workshop Co-chair of WiOpt18.

20
Xinping Guan (F'18) received the B.S. degree in Mathematics from Harbin Normal University, Harbin, China, in 1986, and the Ph.D. degree in Control Science and Engineering from Harbin Institute of Technology, Harbin, China, in 1999. He is currently a Chair Professor with Shanghai Jiao Tong University, Shanghai, China, where he is the Dean of School of Electronic, Information and Electrical Engineering, and the Director of the Key Laboratory of Systems Control and Information Processing, Ministry of Education of China. Before that, he was the Professor and Dean of Electrical Engineering, Yanshan University, Qinhuangdao, China.
Dr. Guan's current research interests include industrial cyber-physical systems, wireless networking and applications in smart factory, and underwater networks. He has authored and/or coauthored 5 research monographs, more than 270 papers in IEEE Transactions and other peer-reviewed journals, and numerous conference papers. As a Principal Investigator, he has finished/been working on many national key projects. He is the leader of the prestigious Innovative Research Team of the National Natural Science Foundation of China (NSFC). Dr. Guan is an Executive Committee Member of Chinese Automation Association Council and the Chinese Artificial Intelligence Association Council. Dr. Guan received the First Prize of Natural Science Award from the Ministry of Education of China in both 2006 and 2016, and the Second Prize of the National Natural Science Award of China in both 2008 and 2018. He was a recipient of IEEE Transactions on Fuzzy Systems Outstanding Paper Award in 2008. He is a National Outstanding Youth honored by NSF of China, Changjiang Scholar by the Ministry of Education of China and State-level Scholar of New Century Bai Qianwan Talent Program of China.

