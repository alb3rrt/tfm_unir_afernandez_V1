arXiv:2106.01202v1 [stat.ML] 2 Jun 2021

Framing RNN as a kernel method: A neural ODE approach
Adeline Fermanian1 Pierre Marion1 Jean-Philippe Vert2 Gérard Biau1 1 Sorbonne Université, CNRS, LPSM, Paris
{adeline.fermanian, pierre.marion, gerard.biau}@sorbonne-universite.fr 2 Google Research, Brain team, Paris jpvert@google.com
Abstract
Building on the interpretation of a recurrent neural network (RNN) as a continuoustime neural differential equation, we show, under appropriate conditions, that the solution of a RNN can be viewed as a linear function of a specific feature set of the input sequence, known as the signature. This connection allows us to frame a RNN as a kernel method in a suitable reproducing kernel Hilbert space. As a consequence, we obtain theoretical guarantees on generalization and stability for a large class of recurrent networks. Our results are illustrated on simulated datasets.
1 Introduction
Recurrent neural networks (RNN) are among the most successful methods for modeling sequential data. They have achieved state-of-the-art results in difficult problems such as natural language processing (e.g., Mikolov et al., 2010; Collobert et al., 2011) or speech recognition (e.g., Hinton et al., 2012; Graves et al., 2013). This class of neural networks has a natural interpretation in terms of (discretization of) ordinary differential equations (ODE), which casts them in the field of neural ODE (Chen et al., 2018). This observation has led to the development of continuous-depth models for handling irregularly-sampled time-series data, including the ODE-RNN model (Rubanova et al., 2019), GRU-ODE-Bayes (De Brouwer et al., 2019), or neural CDE models (Kidger et al., 2020; Morrill et al., 2020a). In addition, the time-continuous interpretation of RNN allows to leverage the rich theory of differential equations to develop new recurrent architectures (Chang et al., 2019; Herrera et al., 2020; Erichson et al., 2021), which are better at learning long-term dependencies.
On the other hand, the development of kernel methods for deep learning offers theoretical insights on the functions learned by the networks (Cho and Saul, 2009; Belkin et al., 2018; Jacot et al., 2018). Here, the general principle consists in defining a reproducing kernel Hilbert space (RKHS)--that is, a function class H --, which is rich enough to describe the architectures of networks. A good example is the construction of Bietti and Mairal (2017, 2019), who exhibit a RKHS for convolutional neural networks. This kernel perspective has several advantages. First, by separating the representation of the data from the learning process, it allows to study invariances of the representations learned by the network. Next, by reducing the learning problem to a linear one in H , generalization bounds can be more easily obtained. Finally, the Hilbert structure of H provides a natural metric on neural networks, which can be used for example for regularization (Bietti et al., 2019).
Contributions. By taking advantage of the neural ODE paradigm for RNN, we show that RNN are, in the continuous-time limit, linear predictors over a specific space associated with the signature of the input sequence (Levin et al., 2013). The signature transform, first defined by Chen (1958) and
Equal contribution
Preprint. Under review.

central in rough path theory (Lyons et al., 2007; Friz and Victoir, 2010), summarizes sequential inputs by a graded feature set of their iterated integrals. Its natural environment is a tensor space that can be endowed with a RKHS structure (Király and Oberhauser, 2019). We exhibit general conditions under which classical recurrent architectures such as feedforward RNN, Gated Recurrent Units (GRU, Cho et al., 2014), or Long Short-Term Memory networks (LSTM, Hochreiter and Schmidhuber, 1997), can be framed as a kernel method in this RKHS. This enables us to provide generalization bounds for RNN as well as stability guarantees via regularization. The theory is illustrated with some experimental results.

Related works. The neural ODE paradigm was first formulated by Chen et al. (2018) for residual neural networks. It was then extended to RNN in several articles, with a focus on handling irregularly sampled data (Rubanova et al., 2019; Kidger et al., 2020) and learning long-term dependencies (Chang et al., 2019). The signature transform has recently received the attention of the machine learning community (Levin et al., 2013; Kidger et al., 2019; Liao et al., 2019; Toth and Oberhauser, 2020; Fermanian, 2021) and, combined with deep neural networks, has achieved state-of-the-art performance for several applications (Yang et al., 2016, 2017; Perez Arribas, 2018; Wang et al., 2019; Morrill et al., 2020b). Király and Oberhauser (2019) use the signature transform to define kernels for sequential data and develop fast computational methods. The connection between continuous-time RNN and signatures has been pointed out by Lim (2021) for a specific model of stochastic RNN. Deriving generalization bounds for RNN is an active research area (Zhang et al., 2018; Akpinar et al., 2019; Tu et al., 2019). By leveraging the theory of differential equations, our approach encompasses a large class of RNN models, ranging from feedforward RNN to LSTM. This is in contrast with most existing generalization bounds, which are architecture-dependent. Close to our point of view is the work of Bietti and Mairal (2017) for convolutional neural networks.

Mathematical context. We place ourselves in a supervised learning setting. The input data is a sample of n i.i.d. vector-valued sequences {x(1), . . . , x(n)}, where x(i) = (x(1i), . . . , x(Ti))  (Rd)T , T  1. The outputs of the learning problem can be either labels (classification setting)
or sequences (sequence-to-sequence setting). Even if we only observe discrete sequences, each x(i) is mathematically considered as a regular discretization of a continuous-time process X(i)  BV ([0, 1], Rd), where BV ([0, 1], Rd) is the space of continuous functions from [0, 1] to Rd of finite
total variation. Informally, the total variation of a process corresponds to its length. Formally, for any [s, t]  [0, 1], the total variation of a process X  BV ([0, 1], Rd) on [s, t] is defined by

k

X T V ;[s,t] =

sup

Xtj - Xtj-1 ,

(t0,...,tk)Ds,t j=1

where Ds,t denotes the set of all finite partitions of [s, t] and · the Euclidean norm. We therefore have that x(ji) = Xj(/iT) , 1  j  T , where Xt(i) := X(i)(t). We make two assumptions on the processes X(i). First, they all begin at zero, and second, their lengths are bounded by L  (0, 1).
These assumptions are not too restrictive, since they amount to data translation and normalization, common in practice. Accordingly, we denote by X the subspace of BV ([0, 1], Rd) defined by

X = X  BV ([0, 1], Rd) | X0 = 0 and X T V ;[0,1]  L

and assume therefore that X(1), . . . , X(n) are i.i.d. according to some X  X . The norm on all spaces Rm, m  1, is always the Euclidean one. Observe that assuming that X  X implies that, for any t  [0, 1], Xt = Xt - X0  X T V ;[0,1]  L.

Recurrent neural networks. Classical RNN are defined by a sequence of hidden states h1, . . . , hT  Re, where, for x = (x1, . . . , xT ) a generic data sample,

hj+1 = f (hj , xj+1), h0 = 0.

At each time step 1  j  T , the output of the network is zj = (hj), where  is a linear function. In the present article, we rather consider the following residual version, which is a natural adaptation
of classical RNN in the neural ODE framework (see, e.g., Yue et al., 2018):

hj+1

=

hj

+

1 T

f (hj , xj+1),

h0 = 0.

(1)

2

The simplest choice for the function f is the feedforward model, say fRNN, defined by

fRNN(h, x) = (U h + V x + b),

(2)

where  is an activation function, U  Re×e and V  Re×d are weight matrices, and b  Re is the bias. The function fRNN, equipped with a smooth activation  (such as the logistic or hyperbolic tangent functions), will be our leading example throughout the paper. However, the GRU and LSTM
models can also be rewritten under the form (1), as shown in Appendix A.1. Thus, model (1) is
flexible enough to encompass most recurrent networks used in practice.

Overview. Section 2 is devoted to framing RNN as linear functions in a suitable RKHS. We start by embedding iteration (1) into a continuous-time model, which takes the form of a controlled differential equation (CDE). This allows, after introducing the signature transform, to define the appropriate RKHS, and, in turn, to show that model (1) boils down, in the continuous-time limit, to a linear problem on the signature. This framework is used in Section 3 to derive generalization bounds and stability guarantees. We conclude with some experiments in Section 4. All proofs are postponed to the supplementary material.

2 Framing RNN as a kernel method

Roadmap. First, we quantify the difference between the discrete recurrent network (1) and its continuous-time counterpart (Proposition 1). Then, we rewrite the corresponding ODE as a CDE (Proposition 2). Under appropriate conditions, Proposition 4 shows that the solution of this equation is a linear function of the signature of the driving process. Importantly, these assumptions are valid for a feedforward RNN, as stated by Proposition 5. We conclude in Theorem 1.

2.1 From discrete to continuous time

Recall that h0, . . . , hT denote the hidden states of the RNN (1), and let H : [0, 1]  Re be the

solution of the ODE

dHt = f (Ht, Xt)dt, H0 = h0.

(3)

By bounding the difference between Hj/T and hj, the following proposition shows how to pass from discrete to continuous time, provided f satisfies the following assumption:

(A1) The function f is Lipschitz continuous in h and x, with Lipschitz constants Kh and Kx. We let Kf = max(Kh, Kx).

Proposition 1. Assume that (A1) is verified. Then there exists a unique solution H to (3) and, for

any 0  j  T ,

Hj/T - hj



c1 T

,

where c1 = Kf eKf L +

sup

f (h, x) eKf and M = sup f (h0, x) eKf . Moreover,

h M, x L

x L

for any t  [0, 1], Ht  M .

Then, following Kidger et al. (2020), we show that the ODE (3) can be rewritten under the form of a CDE. At the cost of increasing the dimension of the hidden state from e to e + d, this allows us to reframe model (3) as a linear model in dX, in the sense that X has been moved `outside' of f .

Proposition 2. Assume that (A1) is verified. Let H : [0, 1]  Re be the solution of (3), and let

X¯

:

[0, 1]



Rd+1

be the time-augmented process X¯t

=

(Xt

,

1-L 2

t)

. Then there exists a tensor

field F : Re¯  Re¯×d¯, e¯ = e + d, d¯ = d + 1, such that if H¯ : [0, 1]  Re¯ is the solution of the CDE

dH¯t = F(H¯t)dX¯t, H¯0 = (H0 , X0 ) ,

(4)

then its first e coordinates are equal to H.

Equation (4) can be better understood by the following equivalent integral equation:
t
H¯t = H¯0 + F(H¯u)dX¯u,
0

3

where the integral should be understood as Riemann-Stieljes integral (Friz and Victoir, 2010, Section I.2). Thus, the output of the RNN can be approximated by the solution of the CDE (4), and, according to Proposition 1, the approximation error is O(1/T ).
Example 1. Consider fRNN as in (2). If  is Lipschitz continuous with constant K, then, for any h1, h2  Re, x1, x2  Rd,

fRNN(h1, x1) - fRNN(h2, x1) = (U h1 + V x1 + b) - (U h2 + V x1 + b)  K U op h1 - h2 ,

where · op denotes the operator norm--see Appendix A.3. Similarly, f (h1, x1) - f (h1, x2)  K V op x1 - x2 . Thus, assumption (A1) is satisfied. The tensor field FRNN of Proposition 2 corresponding to this network is defined for any h¯  Re¯ by

FRNN(h¯) =

0e×d Id×d

2 1-L

(W

h¯

+

b)

0d×1

,

where

W = (U

V )  Re×e¯.

(5)

2.2 The signature

An essential ingredient towards our construction is the signature of a continuous-time process, which we briefly present here. We refer to Chevyrev and Kormilitzin (2016) for a gentle introduction and to Lyons et al. (2007); Levin et al. (2013) for details.

Tensor Hilbert spaces. We denote by (Rd)k the kth tensor power of Rd with itself, which is a Hilbert space of dimension dk. The key space to define the signature and, in turn, our RKHS, consists
in infinite square-summable sequences of tensors of increasing order:



T = a = (a0, . . . , ak, . . . ) ak  (Rd)k,

ak

2 (Rd )k

<



.

(6)

k=0

Endowed with the scalar product a, b T :=

 k=0

ak, bk

, (Rd)k T

is a Hilbert space, as shown in

Appendix A.4.

Definition 1. Let X  BV ([0, 1], Rd). For any t  [0, 1], the signature of X on [0, t] is defined by S[0,t](X) = (1, X1[0,t], . . . , Xk[0,t], . . . ), where, for each k  1,

Xk[0,t] = k!

···

dXu1  · · ·  dXuk  (Rd)k.

0u1 <···<uk t

Although this definition is technical, the signature should simply be thought of as a feature map that embeds a bounded variation process into an infinite-dimensional tensor space. The signature has several good properties that make it a relevant tool for machine learning (e.g., Levin et al., 2013; Chevyrev and Kormilitzin, 2016; Fermanian, 2021). In particular, under certain assumptions, S(X) characterizes X up to translations and reparameterizations, and has good approximation properties. We also highlight that fast libraries exist for computing the signature (Reizenstein and Graham, 2020; Kidger and Lyons, 2021).

The expert reader is warned that this definition differs from the usual one by the normalization of Xk[0,t] by k!, which is more adapted to our context. When the signature is taken on the whole
interval [0, 1], we simply write S(X) and Xk. In the sequel, for any index (i1, . . . , ik)  {1, . . . , d}k, S[(0i,1t,]...,ik)(X) denotes the term associated with the coordinates (i1, . . . , ik) of Xk[0,t].

Example 2. Let X be the d-dimensional linear path defined by Xt = (a1 + b1t, . . . , ad + bdt) , ai, bi  R. Then S(i1,...,ik)(X) = bi1 · · · bik and Xk = bk.

The next proposition, which ensures that S[0,t](X¯ )  T , is an important step.

Proposition 3. Let X  X and X¯t S[0,t](X¯ ) T  2(1 - L)-1.

=

(Xt

,

1-L 2

t)

as in Proposition 2. Then, for any t  [0, 1],

4

The signature kernel. By taking advantage of the structure of Hilbert space of T , it is natural to introduce the following kernel:

K :X ×X R (X, Y )  S(X¯ ), S(Y¯ ) T ,

which is well defined according to Proposition 3. We refer to Király and Oberhauser (2019) for a

general presentation of kernel methods with signatures and to Cass et al. (2020) for a kernel trick. The RKHS associated with K is the space of functions

H =  : X  R | (X) = , S(X¯ ) T ,   T ,

(7)

with scalar product ,  H = ,  T (see, e.g., Schölkopf and Smola, 2002).

2.3 From the CDE to the signature kernel

An important property of signatures is that the solution of the CDE (4) can be written, under certain assumptions, as a linear function of the signature of the driving process X. This operation can be thought of as a Taylor expansion for CDE. More precisely, let us rewrite (4) as

d

dHt = F(Ht)dXt = F i(Ht)dXti,

(8)

i=1

where heavy

Xt = (Xt1, . notation, we

. . , Xtd) , F : Re  Re×d, momentarily write e, d, H,

and and

Fi X

: Re  instead

Re are of e¯, d¯,

the H¯ ,

columns of F--to avoid and X¯ . Throughout, the

bold notation is used to distinguish tensor fields and vector fields. We recall that a vector field F : Re  Re or a tensor field F : Re  Re×d are said to be smooth if each of their coordinates is C .

Definition 2. Let F, G : Re  Re be smooth vector fields and denote by J(·) the Jacobian matrix. Their differential product is the smooth vector field F G : Re  Re defined, for any h  Re, by

(F

G)(h)

=

e j=1

G hj

(h)Fj (h)

=

J (G)(h)F

(h).

In differential geometry, F G is simply denoted by F G. Since the operation is not associative, we take the convention that it is evaluated from right to left, i.e., F 1 F 2 F 3 := F 1 (F 2 F 3).

Taylor expansion. Let H be the solution of (8), where F is assumed to be smooth. We now show that H can be written as a linear function of the signature of X, which is the crucial step to embed the RNN in the RKHS H . The step-N Taylor expansion of H (Friz and Victoir, 2008) is defined by

HtN = H0 +

N

1 k!

S[(0i,1t,]...,ik)(X)F i1 · · · F ik (H0).

k=1 1i1,...,ikd

Throughout, we let

k(F) =

sup

F i1 · · · F ik (h) .

h M,1i1,...,ikd

Example 1id

3. +

1L,eFt RiFNN=(h¯)F=RNWN idh¯efi+nbeid,

by (5) where

with an bi is the

identity activation. Then, for any h¯ (i + d)th vector of the canonical basis

 of

Re¯, Re¯,

and

Wi = 0e¯×e¯,

Wd+1 =

2 1-L

W

0d×e¯

,

and

bd+1 =

2 1-L

b

0d

.

The vector fields FRiNN are then affine, J(FRiNN) = Wi, and the iterated star products have a simple expression: for any 1  i1, . . . , ik  d, FRi1NN · · · FRikNN(h¯) = Wik · · · Wi2 (Wi1 h¯ + bi1 ).

The next proposition shows that the step-N Taylor expansion HN is a good approximation of H. Proposition 4. Assume that the tensor field F is smooth. Then, for any t  [0, 1],

Ht - HtN



dN +1 (N + 1)!

N

+1

(F).

(9)

5

Thus, provided that N (F) is not too large, the right-hand side of (9) converges to zero, hence

Ht = H0 +



1 k!

S[(0i,1t,]...,ik)(X)F i1 · · · F ik (H0).

k=1 1i1,...,ikd

We conclude from the above representation that the solution H of (8) is in fact a linear function of the signature of X. A natural concern is to know whether the upper bound of Proposition 4 vanishes with N for standard architectures. This property is encapsulated in the following more general assumption:


(A2) The tensor field F is smooth and

dk k!

k

(F)

2
< .

k=0

Clearly, if (A2) is verified, then the right-hand side of (9) converges to 0. The next proposition states formally the conditions under which (A2) is verified for FRNN. It is further illustrated in Figure 1, which shows that the convergence is fast with two common activation functions. We let   = sup h M, x L (U h + V x + b) and (k)  = sup h M, x L (k)(U h + V x + b) .
Proposition 5. Let FRNN be defined by (5). If  is the identity function, then (A2) is satisfied. In the general case, (A2) holds if  is smooth and there exists a > 0 such that, for any k  0,

(k)   ak+1k! and

W

F

<

1-L 8a2d

,

(10)

where

·

 F is the Frobenius norm. Moreover, N (FRNN)  2a

8a2 W F 1-L

N -1
N!.

The proof of Proposition 5, based on the manipulation of higher-order derivatives of tensor fields, is highly non-trivial. We highlight that the conditions on  are mild and verified for common smooth activations. For example, they are verified for the logistic function (with a = 2) and for the hyperbolic tangent function (with a = 4)--see Appendix A.5. The second inequality of (10) puts a constraint on
the norm of the weights, and can be regarded as a radius of convergence for the Taylor expansion.

Putting everything together. We now have all the elements at hand to embed the RNN into the
RKHS H . To fix the idea, we assume in this paragraph that we are in a ±1 classification setting. In other words, given an input sequence x, we are interested in the final output zT = (hT )  R, where hT is the solution of (1). The predicted class is 1(zT > 0).

By Propositions which outputs a

1 and 2, zT is Re+d-valued

approximated by the first process H¯ . According to

e coordinates of the solution Proposition 4, H¯ is a linear

of the CDE function of

(4), the

signature of the time-augmented process X¯ . Thus, on top of H¯ , it remains to successively apply the

projection Proj on the e first coordinates followed by the linear function  to obtain an element of the

RKHS H . This mechanism is summarized in the following theorem.

Theorem 1. Assume that (A1) and (A2) are verified. Then there exists a function   H such that

|zT - (X)| 



op

c1 T

,

(11)

where (X) =

, S(X¯ ) T

and X¯t

=

(Xt

,

1-L 2

t)

.

We have  =

(k) k=0, where each

k  (Rd)k is defined by

k(i1 ,...,ik )

=

1 k!





Proj

F i1

···

F ik (H¯0) .

Moreover,



2 T





2 op

 k=0

dk k!

k

(F)

2
.

We conclude that in the continuous-time limit, the output of the network can be interpreted as a scalar product between the signature of the (time-augmented) process X¯ and an element of T . This interpretation is important for at least two reasons: (i) it facilitates the analysis of generalization of RNN by leveraging the theory of kernel methods, and (ii) it provides new insights on regularization
strategies to make RNN more robust. These points will be explored in the next section. Finally, we
stress that the approach works for a large class of RNN, such as GRU and LSTM. The derivation of conditions (A1) and (A2) beyond the feedforward RNN is left for future work.

6

3 Generalization and regularization

3.1 Generalization bounds

Learning procedure. A first consequence of framing a RNN as a kernel method is that it gives natu-

ral generalization bounds under mild assumptions. In the learning setup, we are given an i.i.d. sample

of n random pairs of observations (x(i), y(i))  (Rd)T × Y , where x(i) = (x(1i), . . . , x(Ti)). We distinguish the binary classification problem, where Y = {-1, 1}, from the sequential prediction

problem, by   

where  Rq,

Y = (Rp)T and y(i) = (y1(i), where  is a compact set. To

. . . , yT(i)). clarify the

The RNN is assumed to be parameterized notation, we use a  subscript whenever a

quantity depends on  (e.g., f for f , etc.). In line with Section 2, it is assumed that the tensor field F associated with f satisfies (A1) and (A2), keeping in mind that Proposition 5 guarantees that

these requirements are fulfilled by a feedforward recurrent network with a smooth activation function.

Let g : (Rd)T  Y denote the output of the recurrent network. The parameter  is fitted by empirical risk minimization using a loss function : Y × Y  R+. The theoretical and empirical risks are respectively defined, for any   , by

R() = E[ (y, g(x))]

and

Rn()

=

1 n

n

y(i), g(x(i)) ,

i=1

where the expectation E is evaluated with respect to the distribution of the generic random pair (x, y).
We let n  argmin Rn() and aim at upper bounding P(y = gn (x)) in the classification regime (Theorem 2) and R(n) in the sequential regime (Theorem 3). To reach this goal, our strategy is to approximate the RNN by its continuous version and then use the RKHS machinery of Section 2.

Binary classification. In this context, the network outputs a real number g(x) = (hT )  R and the predicted class is 1(zT > 0). The loss : R × R  R+ is assumed to satisfy the assumptions of

Bartlett and Mendelson (2002, Theorem 7), that is, for any y  {-1, 1}, (y, g(x)) = (yg(x)),

where (u)  1(u  0), (0) = 0, and  is Lipschitz-continuous with constant K . For example,

the cross-entropy approximates the

loss satisfies such assumptions. We RNN with parameter . Thus, zT 

let (X¯

H )=

be  ,

the function of Theorem 1 that S(X¯ ) T , up to a O(1/T ) term.

Theorem 2. Assume that for all   , (A1) and (A2) are verified. Assume, in addition, that there

exists a constant B > 0 such that for any   ,  H  B. Then with probability at least 1 - ,

P y = gn (x)



Rn(n)

+

c2 T

+

(11-6KL)Bn

+

2BK 1-L

log(1/) 2n

,

(12)

where c2 = K sup  opKf eKf L + f eKf .

Close to our result are the bounds obtained by Zhang et al. (2018), Tu et al. (2019), and Chen et al. (2020). The main difference is that the term in 1/T does not usually appear, since it comes from the continuous viewpoint on RNN, whereas the speed in 1/n is more classical. The take-home message
is that the detour by continuous-time neural ODE provides a theoretical framework adapted to RNN, at the modest price of an additional O(1/T ) term. Moreover, we note that the bound (12) is `simple'
and holds under mild conditions for a large class of RNN. More precisely, for any recurrent network of the form (1), provided (A1) and (A2) are satisfied, then (12) is valid with constants c2 and B depending on the architecture. Such constants are given below in the example of a feedforward RNN.

Example 4. Take a feedforward RNN with logistic activation, and  = {(W, b, ) | W F 

KW < (1 - L)/32d, b  Kb,  op  K}. Then, Proposition 5 states that (A2) is satisfied and,

with Theorem 1, ensures that



sup




H



1

2K(1 - L) - L - 32dKW

:= B,

Kf = max( U op, V op),

and

f  = 1.

Sequence-to-sequence learning. We conclude by showing how to extend both the RKHS embedding of Theorem 1 and the generalization bound of Theorem 2 to the setting of sequence-to-sequence learning. In this case, the output of the network is a sequence
g(x) = (z1, . . . , zT )  (Rp)T .

7

An immediate extension of Theorem 1 ensures that there exist p elements 1,, . . . , p,  T such that, for any 1  j  T ,

zj - 1,, S[0,j/T ](X¯ ) T , . . . , p,, S[0,j/T ](X¯ ) T





op

c1 T

.

(13)

The properties of the signature guarantee that S[0,j/T](X) = S(X~[j]) where X~[j] is the process equal to X¯ on [0, j/T ] and then constant on [j/T , 1]--see Appendix A.6. With this trick, we have, for any 1   p,  ,, S[0,j/T](X¯ ) T =  ,, S(X~[j]) T , so that we are back in H . Observe that the
only difference with (11) is that we consider vector-valued sequential outputs, which requires to introduce the process X~[j], but that the rationale is exactly the same.

We let : (Rp)T × (Rp)T  R+ be the L2 distance, that is, for any y = (y1, . . . , yT ), y =

(y1, . . . , yT ),

(y, y )

=

1 T

T j=1

yj - yj

2.

It is assumed that y takes its values in a compact

subset of Rq, i.e., there exists Ky > 0 such that yj  Ky.

Theorem 3. Assume that for all   , (A1) and (A2) are verified. Assume, in addition, that there exists a constant B > 0 such that for any 1   p,   ,  , H  B . Then with probability at least 1 - ,

R(n)



Rn(n)

+

c3 T

+

8pc4B(1 - n

L)-1

+

2c5

log(1/) n

,

(14)

where c3 = sup c1, +  op f  + 2pB(1 - L)-1 + 2Ky, c4 = B(1 - L)-1 + Ky, and

c5 = 4pB(1 - L)-1c4 + Ky2.

3.2 Regularization and stability

In addition to providing a sound theoretical framework, framing deep learning in a RKHS provides a natural norm, which can be used for regularization, as shown for example in the context of convolutional neural networks by Bietti et al. (2019). This regularization ensures stability of predictions, which is crucial in particular in a small sample regime or in the presence of adversarial examples (Gao et al., 2018; Ko et al., 2019). In our binary classification setting, for any inputs x, x  (Rd)T , by the Cauchy-Schwartz inequality, we have

zT - zT

 2  op

c1 T

+

 (X¯ ) -  (X¯ )

 2  op

c1 T

+



H

S(X¯ ) - S(X¯ ) T .

If x and x are close, so are their associated continuous processes X and X (which can be approx-

imated for example by taking a piecewise linear interpolation), and so are their signatures. The term S(X¯ ) - S(X¯ ) T is therefore small (Friz and Victoir, 2010, Proposition 7.66). Therefore, when T is large, we see that the magnitude of  H determines how close the predictions are.
A natural training strategy to ensure stable predictions, for the types of networks covered in the

present article, is then to penalize the problem by minimizing the loss Rn() + 



2 H

.

From

a

computational point of view, it is possible to compute the norm in H , up to a truncation at N of the

Taylor expansion, which we know by Proposition 4 to be reasonable. It remains that computing this

norm is a non-trivial task, and implementing smart surrogates is an interesting problem for the future.

4 Numerical illustrations
This section is here for illustration purposes. Our objective is not to achieve competitive performance, but rather to illustrate the theoretical results. We refer to Appendix D for implementation details.
Convergence of the Taylor expansion towards the solution of the ODE. We illustrate Proposition 4 on a toy example. The process X is a 2-dimensional spiral, and we take feedforward RNN with 2 hidden units. Repeating this procedure with 103 uniform random weight initializations, we observe in Figure 1a that the signature approximation converges exponentially fast in N . As seen in Figure 1b, the rate of convergence depends in particular on the norm of the weight matrices, as predicted by Proposition 5. However, condition (10) seems to be over-restrictive, since convergence happens even for weights with norm larger than the bound (we have 1/(8a2d) 0.01 here).

8

10-1 10-2

Activation sigmoid tanh

10-3 10-4 10-5

Error for N = 5

Error

10-3

10-6

10-4
10-5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 Step N

10-7 10-8

0.4

0.6

0.8

1.0

1.2

1.4

1.6

Frobenius norm of the weights

(a) Error on a logarithmic scale as a function of N

(b) Error as a function of the norm of the weights

Figure 1: Approximation of the RNN ODE by the step-N Taylor expansion

RNN 0.9
Penalized RNN 0.8

Adversarial accuracy

0.7

0.6

0.5

0.4

0.0

0.2

0.4

0.6

0.8

1.0



Figure 2: Adversarial accuracy as a function of the adversarial perturbation 

Adversarial robustness. We illustrate the penalization proposed in Section 3.2 on a toy task that consists in classifying the rotation direction of 2dimensional spirals. We take a feedforward RNN with 32 hidden units and hyperbolic tangent activation. It is trained on 50 examples, with and without penalization, for 200 epochs. Once trained, the RNN is tested on adversarial examples, generated with the projected gradient descent algorithm with Frobenius norm (Madry et al., 2018), which modifies test examples to maximize the error while staying in a ball of radius . We observe in Figure 2 that adding the penalization seems to make the network more stable.

Comparison of the trained networks. The evolution of the Frobenius norm of the weights W F and the RKHS norm  H during training is shown in Figure 3. This points out that the penalization, which forces the RNN to keep a small norm in H , leads indeed to learning different weights than the
non-penalized RNN. The results also suggest that the Frobenius and RKHS norms are decoupled,
since both networks have Frobenius norms of similar magnitude but very different RKHS norms. The
figures show one random run, but we observe similar qualitative behavior on others.

RKHS norm (N=3)

Frobenius norm of the weights

30 103
25
101 20

15

10-1

10

RNN

5

Penalized RNN

0

25 50 75 100 125 150 175 200

Epoch

10-3

RNN Penalized RNN

0

25 50 75 100 125 150 175 200

Epoch

Figure 3: Evolution of the Frobenius norm of the weights and of the RKHS norm during training

5 Conclusion
By bringing together the theory of neural ODE, the signature transform, and kernel methods, we have shown that a recurrent network can be framed in the continuous-time limit as a linear function in a well-chosen RKHS. In addition to giving theoretical insights on the function learned by the network and providing generalization guarantees, this framing suggests regularization strategies to obtain
9

more robust RNN. We have only scratched the surface of the potentialities of leveraging this theory to practical applications, which is a subject of its own and will be tackled in future work.
Acknowledgements
Authors thank Thierry Lévy for his inputs on the Picard-Lindelöf theorem. A. Fermanian has been supported by a grant from Région Ile-de-France and P. Marion by a stipend from Corps des Mines.
References
N.-J. Akpinar, B. Kratzwald, and S. Feuerriegel. Sample complexity bounds for recurrent neural networks with application to combinatorial graph problems. arXiv:1901.10289, 2019.
P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3:463­482, 2002.
M. Belkin, S. Ma, and S. Mandal. To understand deep learning we need to understand kernel learning. In J. Dy and A. Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80, pages 541­549. PMLR, 2018.
A. Bietti and J. Mairal. Invariance and stability of deep convolutional representations. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30, pages 6210­6220. Curran Associates, Inc., 2017.
A. Bietti and J. Mairal. Group invariance, stability to deformations, and complexity of deep convolutional representations. Journal of Machine Learning Research, 20:1­49, 2019.
A. Bietti, G. Mialon, D. Chen, and J. Mairal. A kernel perspective for regularizing deep neural networks. In K. Chaudhuri and R. Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97, pages 664­674. PMLR, 2019.
T. Cass, T. Lyons, C. Salvi, and W. Yang. Computing the untruncated signature kernel as the solution of a Goursat problem. arXiv:2006.14794, 2020.
B. Chang, M. Chen, E. Haber, and E. H. Chi. AntisymmetricRNN: A dynamical system view on recurrent neural networks. In International Conference on Learning Representations, 2019.
K.-T. Chen. Integration of paths­a faithful representation of paths by non-commutative formal power series. Transactions of the American Mathematical Society, 89:395­407, 1958.
M. Chen, X. Li, and T. Zhao. On generalization bounds of a family of recurrent neural networks. In S. Chiappa and R. Calandra, editors, Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, volume 108, pages 1233­1243, 2020.
R. T. Q. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duvenaud. Neural ordinary differential equations. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31, pages 6572­6583. Curran Associates, Inc., 2018.
I. Chevyrev and A. Kormilitzin. A primer on the signature method in machine learning. arXiv:1603.03788, 2016.
K. Cho, B. van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 1724­1734. Association for Computational Linguistics, 2014.
Y. Cho and L. Saul. Kernel methods for deep learning. In Y. Bengio, D. Schuurmans, J. Lafferty, C. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems, volume 22, pages 342­350. Curran Associates, Inc., 2009.
10

R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493­2537, 2011.
E. De Brouwer, J. Simm, A. Arany, and Y. Moreau. GRU-ODE-Bayes: Continuous modeling of sporadically-observed time series. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32, pages 7379­7390. Curran Associates, Inc., 2019.
N. B. Erichson, O. Azencot, A. Queiruga, L. Hodgkinson, and M. W. Mahoney. Lipschitz recurrent neural networks. In International Conference on Learning Representations, 2021.
A. Fermanian. Embedding and learning with signatures. Computational Statistics & Data Analysis, 157:107148, 2021.
P. Friz and N. Victoir. Euler estimates for rough differential equations. Journal of Differential Equations, 244:388­412, 2008.
P. K. Friz and N. B. Victoir. Multidimensional Stochastic Processes as Rough Paths: Theory and Applications, volume 120 of Cambridge Studies in Advanced Mathematics. Cambridge University Press, Cambridge, 2010.
J. Gao, J. Lanchantin, M. L. Soffa, and Y. Qi. Black-box generation of adversarial text sequences to evade deep learning classifiers. In 2018 IEEE Security and Privacy Workshops, pages 50­56, 2018.
A. Graves, A.-r. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural networks. In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 6645­ 6649, 2013.
C. Herrera, F. Krach, and J. Teichmann. Theoretical guarantees for learning conditional expectation using controlled ODE-RNN. arXiv:2006.04727, 2020.
G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Processing Magazine, 29:82­97, 2012.
S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9:1735­1780, 1997.
A. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31, pages 8580­8589. Curran Associates, Inc., 2018.
J. Kelly, J. Bettencourt, M. J. Johnson, and D. K. Duvenaud. Learning differential equations that are easy to solve. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 4370­4380. Curran Associates, Inc., 2020.
P. Kidger and T. Lyons. Signatory: Differentiable computations of the signature and logsignature transforms, on both CPU and GPU. In International Conference on Learning Representations, 2021.
P. Kidger, P. Bonnier, I. Perez Arribas, C. Salvi, and T. Lyons. Deep signature transforms. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32, pages 3099­3109. Curran Associates, Inc., 2019.
P. Kidger, J. Morrill, J. Foster, and T. Lyons. Neural controlled differential equations for irregular time series. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 6696­6707. Curran Associates, Inc., 2020.
D. P. Kingma and J. Ba. Adam: A Method for Stochastic Optimization. In Proceedings of the 3rd International Conference on Learning Representations (ICLR), 2015.
11

F. J. Király and H. Oberhauser. Kernels for sequentially ordered data. Journal of Machine Learning Research, 20:1­45, 2019.
Klaus Greff, Aaron Klein, Martin Chovanec, Frank Hutter, and Jürgen Schmidhuber. The Sacred Infrastructure for Computational Research. In Katy Huff, David Lippa, Dillon Niederhut, and M. Pacer, editors, Proceedings of the 16th Python in Science Conference, pages 49 ­ 56, 2017.
C.-Y. Ko, Z. Lyu, L. Weng, L. Daniel, N. Wong, and D. Lin. POPQORN: Quantifying robustness of recurrent neural networks. In K. Chaudhuri and R. Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97, pages 3468­3477. PMLR, 2019.
D. Levin, T. Lyons, and H. Ni. Learning from the past, predicting the statistics for the future, learning an evolving system. arXiv:1309.0260, 2013.
S. Liao, T. Lyons, W. Yang, and H. Ni. Learning stochastic differential equations using RNN with log signature features. arXiv:1908.08286, 2019.
S. H. Lim. Understanding recurrent neural networks using nonequilibrium response theory. Journal of Machine Learning Research, 22:1­48, 2021.
T. Lyons. Rough paths, signatures and the modelling of functions on streams. arXiv:1405.4537, 2014.
T. J. Lyons, M. J. Caruana, and T. Lévy. Differential Equations Driven by Rough Paths, volume 1908 of Lecture Notes in Mathematics. Springer, Berlin, 2007.
A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018.
T. Mikolov, M. Karafiát, L. Burget, J. C ernocky`, and S. Khudanpur. Recurrent neural network based language model. In Proceedings of the 11th Annual Conference of the International Speech Communication Association, volume 2, pages 1045­1048, 2010.
A. A. Minai and R. D. Williams. On the derivatives of the sigmoid. Neural Networks, 6:845­853, 1993.
J. Morrill, C. Salvi, P. Kidger, J. Foster, and T. Lyons. Neural rough differential equations for long time series. arXiv:2009.08295, 2020a.
J. H. Morrill, A. Kormilitzin, A. J. Nevado-Holgado, S. Swaminathan, S. D. Howison, and T. J. Lyons. Utilization of the signature method to identify the early onset of sepsis from multivariate physiological time series in critical care monitoring. Critical Care Medicine, 48:e976­e981, 2020b.
A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32, pages 8024­8035. Curran Associates, Inc., 2019.
I. Perez Arribas. Derivatives pricing using signature payoffs. arXiv:1809.09466, 2018.
J. F. Reizenstein and B. Graham. Algorithm 1004: The iisignature library: Efficient calculation of iterated-integral signatures and log signatures. ACM Transactions on Mathematical Software, 46: article 8, 2020.
J. Riordan. An Introduction to Combinatorial Analysis. John Wiley & Sons, New York, 1958.
Y. Rubanova, R. T. Q. Chen, and D. K. Duvenaud. Latent ordinary differential equations for irregularly-sampled time series. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32, pages 5320­5330. Curran Associates, Inc., 2019.
B. Schölkopf and A. J. Smola. Learning with kernels: support vector machines, regularization, optimization, and beyond. MIT press, Cambridge, Massachusetts, 2002.
12

C. Toth and H. Oberhauser. Bayesian learning from sequential data using Gaussian processes with signature covariances. In H. Daumé III and A. Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119, pages 9548­9560, 2020.
Z. Tu, F. He, and D. Tao. Understanding generalization in recurrent neural networks. In International Conference on Learning Representations, 2019.
P. Virtanen, R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy, D. Cournapeau, E. Burovski, P. Peterson, W. Weckesser, J. Bright, S. J. van der Walt, M. Brett, J. Wilson, K. J. Millman, N. Mayorov, A. R. J. Nelson, E. Jones, R. Kern, E. Larson, C. J. Carey, I. Polat, Y. Feng, E. W. Moore, J. VanderPlas, D. Laxalde, J. Perktold, R. Cimrman, I. Henriksen, E. A. Quintero, C. R. Harris, A. M. Archibald, A. H. Ribeiro, F. Pedregosa, P. van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods, 17: 261­272, 2020.
B. Wang, M. Liakata, H. Ni, T. Lyons, A. J. Nevado-Holgado, and K. Saunders. A path signature approach for speech emotion recognition. In Proceedings of Interspeech 2019, pages 1661­1665, 2019.
W. Yang, L. Jin, and M. Liu. DeepWriterID: An end-to-end online text-independent writer identification system. IEEE Intelligent Systems, 31:45­53, 2016.
W. Yang, T. Lyons, H. Ni, C. Schmid, and L. Jin. Developing the path signature methodology and its application to landmark-based human action recognition. arXiv:1707.03993, 2017.
B. Yue, J. Fu, and J. Liang. Residual recurrent neural networks for learning sequential representations. Information, 9:56, 2018.
J. Zhang, Q. Lei, and I. Dhillon. Stabilizing gradients for deep neural networks via efficient SVD parameterization. In J. Dy and A. Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80, pages 5806­5814. PMLR, 2018.
13

A Mathematical details
A.1 Writing the GRU and LSTM in the neural ODE framework
GRU. Recall that the equations of a GRU take the following form: for any 1  j  T ,
rj+1 = (Wrxj+1 + br + Urhj ) zj+1 = (Wzxj+1 + bz + Uzhj ) nj+1 = tanh Wnxj+1 + bn + rj+1  (Unhj + cn) hj+1 = (1 - zj+1)  hj + zj+1  nj+1,
where  is the logistic activation, tanh the hyperbolic tangent,  the Hadamard product, rj the reset gate vector, zj the update gate vector, Wr, Ur, Wz, Uz, Wn, Un weight matrices, and br, bz, bn, cn biases. Since rj+1, zj+1, and nj+1 depend only on xj+1 and hj, it is clear that these equations can be rewritten in the form
hj+1 = hj + f (hj , xj+1). We then obtain equation (1) by normalizing f by 1/T .
LSTM. The LSTM networks are defined, for any 1  j  T , by
ij+1 = (Wixj+1 + bi + Uihj ) fj+1 = (Wf xj+1 + bf + Uf hj ) gj+1 = tanh(Wgxj+1 + bg + Ughj ) oj+1 = (Woxj+1 + bo + Uohj ) cj+1 = fj+1  cj + ij+1  gj+1 hj+1 = oj+1  tanh(cj+1),
where  is the logistic activation, tanh the hyperbolic tangent,  the Hadamard product, ij the input gate, fj the forget gate, gj the cell gate, oj the output gate, cj the cell state, Wi, Ui, Wf , Uf , Wg, Ug Wo, Uo weight matrices, and bi, bf , bg, bo biases. Since ij+1, fj+1, gj+1, oj+1 depend only on xj+1 and hj, these equations can be rewritten in the form
hj+1 = f1(hj , xj+1, cj+1) cj+1 = f2(hj , xj+1, cj ).
Let h~j = (hj , cj ) be the hidden state defined by stacking the hidden and cell state. Then, clearly, h~ follows an equation of the form
h~j+1 = f (h~j , xj+1). We obtain (1) by subtracting h~j and normalizing by 1/T .
A.2 Picard-Lindelöf theorem
Consider a CDE of the form (8). We recall the Picard-Lindelöf theorem as given by Lyons et al. (2007, Theorem 1.3), and provide a proof for the sake of completeness. Theorem 4 (Picard-Lindelöf theorem). Assume that X  BV ([0, 1], Rd) and that F is Lipschitzcontinuous with constant KF. Then, for any H0  Re, the differential equation (8) admits a unique solution H : [0, 1]  Re.
Proof. Let C ([s, t]), Re) be the set of continuous functions from [s, t] to Re. For any [s, t]  [0, 1],   Re, let  be the function
 : C ([s, t]), Re)  C ([s, t], Re)
v
Y  v   + F(Yu)dXu .
s
14

For any Y, Y  C ([s, t]), Re), v  [s, t],

(Y )v - (Y )v

v



F(Yu) - F(Yu) dXu

s

v



F(Yu) - F(Yu) op dXu

s

v

 KF Yu - Yu dXu
s v

 KF Y - Y 

dXu

s

 KF Y - Y  X T V ;[s,t].

This shows that the function  is Lipschitz-continuous on C ([s, t]), Re) endowed with the supremum norm, with Lipschitz constant KF X T V ;[s,t]. Clearly, the function t  X T V ;[0,t] is nondecreasing and uniformly continuous on the compact interval [0, 1]. Therefore, for any  > 0, there exists  > 0 such that

|t - s| <   X T V ;[0,t] - X T V ;[0,s] < .

Take  = 1/KF. Then on any interval [s, t] of length smaller than , one has X T V ;[s,t] = X T V ;[0,t] - X T V ;[0,s] < 1/KF, so that the function  is a contraction. By the Banach fixedpoint theorem, for any initial value ,  has a unique fixed point. Hence, there exists a solution to (8) on any interval of length  with any initial condition. To obtain a solution on [0, 1] it is sufficient to
concatenate these solutions.

A corollary of this theorem is a Picard-Lindelöf theorem for initial value problems of the form

dHt = f (Ht, Xt)dt, H0 = ,

(15)

where f : Re × Rd  Re,   Re.
Corollary 1. Assume that f is Lipschitz continuous in its first variable. Then, for any   Re, the initial value problem (15) admits a unique solution.

Proof. Let fX : (h, t)  f (h, Xt). Then the solution of (15) is solution of the differential equation dHt = fX (Ht, t)dt.
Let d = 1, e¯ = e + 1, and F be the vector field defined by

F:h

fX (h1:e, he+1) 1

,

where h1:e denotes the projection of h on its first e coordinates. Then, since fX is Lipschitz, so is the vector field F. Theorem 4 therefore applies to the differential equation

dHt = F(Ht)dt, H0 = ( , 0) . Projecting this differential equation on the last coordinate gives dHte+1 = dt, that is, Hte+1 = t. Projecting on the first e coordinates exactly provides equation (15), which therefore has a unique solution, equal to H1:e.

A.3 Operator norm
Definition 3. Let (E, · E) and (F, · F ) be two normed vector spaces and let f  L (E, F ), where L (E, F ) is the space of linear functions from E to F . The operator norm of f is defined by
f op = sup f (u) F .
uE, u E =1
Equipped with this norm, L (E, F ) is a normed vector space.
This definition is valid when f is represented by a matrix.

15

A.4 Tensor Hilbert space

Let us first briefly recall some elements on tensor spaces. If e1, . . . , ed is the canonical basis of Rd,

then (ei1  · · ·  eik )1i1,...,ikd is a basis of (Rd)k. Any element a  (Rd)k can therefore be

written as

a=

a(i1,...,ik)ei1  · · ·  eik ,

1i1 ,...,ik d

where a(i1,...,ik)  R. The tensor space (Rd)k is a Hilbert space of dimension dk, with scalar product

a, b (Rd)k =

a(i1 ,...,ik ) b(i1 ,...,ik )

1i1 ,...,ik d

and associated norm · . (Rd)k

We now consider the space T defined by (6). The sum, multiplication by a scalar, and scalar product
on T are defined as follows: for any a = (a0, . . . , ak, . . . )  T , b = (b0, . . . , bk, . . . )  T ,   R,

a + b = (a0 + b0, . . . , ak + bk, . . . ) and



a, b T =

ak, bk , (Rd)k

k=0

with the convention (Rd)0 = R.

Proposition 6. (T , +, ·, ·, · T ) is a Hilbert space.

Proof. By the Cauchy-Schwartz inequality, ·, · T is well-defined: for any a, b  T ,





| a, b T |  | ak, bk | (Rd)k 

a b k (Rd)k k (Rd)k

k=0

k=0



1/2 



a2
k (Rd)k

k=0

k=0

b2
k (Rd)k

1/2
< .

Moreover, T is a vector space: for any a, b  T ,   R, since

a + b = (a0 + b0, . . . , ak + bk, . . . ),

and







ak + bk

= 2
(Rd )k

ak

2 (Rd )k

+

2

b2
k (Rd)k

k=0

k=0

k=0



+ 2

ak, bk (Rd)k

k=0







ak

2 (Rd )k

+

2

bk

2 (Rd )k

+ 2

a, b

T

< ,

k=0

k=0

we see that a + b  T . The operation ·, · T is also bilinear, symmetric, and positive definite:



a, a T = 0 

ak

2 (Rd )k

=

0



k



N,

ak

2 (Rd )k

=

0



k



N, ak

=

0



a

=

0.

k=0

Therefore ·, · T is an inner product on T . Finally, let (a(n))nN be a Cauchy sequence in T . Then, for any n, m  0,



a(n) - a(m)

2 T

=

a(kn) - a(km)

, 2
(Rd )k

k=0

so for any k  N, the sequence (a(kn))nN is Cauchy in (Rd)k. Since (Rd)k is a Hilbert space, (a(kn))nN converges to a limit a(k)  (Rd)k. Let a() = (a(0), . . . , a(k), . . . ). To finish the

16

proof, we need to show that a()  T and that a(n) converges to a() in T . First, note that there exists a constant B > 0 such that for any n  N,

a(n) T  B.
To see this, observe that for  > 0, there exists N  N such that for any n  N , a(n) - a(N) T < , and so a(n) T   + a(N) T . Take B = max( a(1) T , . . . , a(N) T ,  + a(N) T ). Then, for any K  N,

K

a(kn)

 2
(Rd )k

a(n) T  B.

k=0

Letting K  , we obtain that a() T  B, and therefore a()  T . Finally, let  > 0 and let N  N be such that for any n, m  N , a(n) - a(m) T < . Clearly, for any K  N,

Letting m   leads to

K

a(kn) - a(km)

2 (Rd )k

<

2.

k=0

and letting K   gives

K

a(kn) - a(k)

2 (Rd )k

<

2,

k=1

a(n) - a() T < ,

which completes the proof.

A.5 Bounding the derivatives of the logistic and hyperbolic tangent activations

Lemma 1. Let  be the logistic function defined, for any x  R, by (x) = 1/(1+e-x). Then, for any n  0,
(n)   2n-1n! .

Proof. For any x  R, one has (Minai and Williams, 1993, Theorem 2)

n+1
(n)(x) = (-1)k-1(k - 1)!

n+1 k

 (x)k ,

k=1

where

n k

stands for the Stirling number of the second kind (see, e.g., Riordan, 1958). Let

n+1
un = (k - 1)!

n+1 k

k=1

for n  1 and u0 = 1. Since 0  (x)  1, it is clear that |(n)(x)|  un. Using the fact that the Stirling numbers satisfy the recurrence relation

n+1 k

=k

n k

+

n k-1

,

valid for all 0  k  n, we have

un =

n
(k - 1)!

k

n k

k=1

(since

n 0

= 0)

 2n

n
(k - 1)!

n k

k=1

+

n k-1

= 2nun-1.

+ n! =

n

k!

n k

k=1

n-1
+ k!

n k

k=0

Thus, by induction, un  2n-1n!, from which the claim follows.

+ n! = 2

n

k!

n k

k=1

17

Lemma 2. Let tanh be the hyperbolic tangent function. Then, for any n  0, tanh(n)   4nn! .

Proof. Let  be the logistic function. Straightforward calculations yield the equality, valid for any x  R,
tanh(x) = 2(2x) - 1.

But, for any n  1,

tanh(n)(x) = 2n+1(n)(2x),

and thus, by Lemma 1,

tanh(n)   2n+1 (n)   4nn! .

The inequality is also true for n = 0 since tanh   1.

A.6 Chen's formula
First, note that it is straightforward to extend the definition of the signature to any interval [s, t]  [0, 1]. The next proposition, known as Chen's formula (Lyons et al., 2007, Theorem 2.9), tells us that the signature can be computed iteratively as tensor products of signatures on subintervals. Proposition 7. Let X  BV ([s, t], Rd) and u  (s, t). Then
S[s,t](X) = S[s,u](X)  S[u,t](X).

Next, it is clear that the signature of a constant path is equal to 1 = (1, 0, . . . , 0, . . . ) which is the null element in T . Indeed, let Y  BV ([s, t], Rd) be a constant path. Then, for any k  1,

Yk[s,t] = k!

···

dYu1  · · ·  dYuk = k!

···

0  · · ·  0 = 0.

su1 <···<uk t

su1 <···<uk t

Now let X  BV ([0, 1], Rd) and consider the path X~[j] equal to the time-augmented path X¯ on [0, j/T ] and then constant on [j/T , 1]--see Figure 4. We have by Proposition 7
S[0,1](X~[j]) = S[0,j/T ](X~[j])  S[j/T ,1](X~[j]) = S[0,j/T ](X¯ )  1 = S[0,j/T ](X¯ ).

0.8

X

0.8

0.7

0.7

0.6

0.6

X

0.5

0.5

X~[1]

X~[2]

0.4

0.4

X~[j]

0.3

0.3

0.2

0.2

0.0

0.2

0.4

0.6

0.8

1.0

Time t

0.0

0.2

0.4

0.6

0.8

1.0

Time t

Figure 4: Example of a path X  BV ([0, 1], R) (left) and its corresponding paths X~[j], plotted against time, for different values of j  {1, . . . , T } (right)

B Proofs
B.1 Proof of Proposition 1 According to Assumption (A1), for any h1, h2  Re, x1, x2  Rd, one has
f (h1, x1) - f (h2, x1)  Kf h1 - h2 and f (h1, x1) - f (h1, x2)  Kf x1 - x2 .
18

Under assumption (A1), by Corollary 1, the initial value problem (3) admits a unique solution H. Let us first show that for any t  [0, 1], Ht is bounded independently of X. For any t  [0, 1],

Ht - H0 =

t
f (Hu, Xu)du
0

t

0 t
=
0 t

0
 Kf

f (Hu, Xu) du

f (Hu, Xu) - f (H0, Xu) + f (H0, Xu) du

t

f (Hu, Xu) - f (H0, Xu) + f (H0, Xu) du

0

t

Hu - H0 du + t sup f (H0, x) .

0

x L

Applying Grönwall's inequality to the function t  Ht - H0 yields

Ht - H0  t sup f (H0, x) exp
x L

t

Kf du  sup f (H0, x) eKf := M.

0

x L

Given that H0 = h0 = 0, we conclude that Ht  M .

Next, let

f  = sup f (h, x).
x L, h M

By similar arguments, for any [s, t]  [0, 1], Grönwall's inequality applied to the function t  Ht - Hs yields
Ht - Hs  (t - s) f eKf .
Therefore, for any partition (t0, . . . , tk) of [s, t],

k

k

Hti - Hti-1  f eKf (ti - ti-1)  f eKf (t - s),

i=1

i=1

and, taking the supremum over all partitions of [s, t], H T V ;[s,t]  f eKf (t - s). In other
words, H is of bounded variation on any interval [s, t]  [0, 1]. Let (t0, . . . , tT ) denote the regular partition of [0, 1] with tj = j/T . For any 1  j  T , we have

Htj - hj

=

Htj-1 +

tj tj-1

f (Hu,

Xu)du

-

hj-1

-

1 T

f (hj-1,

xj )

tj

 Htj-1 - hj-1 +

f (Hu, Xu) - f (hj-1, xj) du.

tj-1

Writing

f (Hu, Xu) - f (hj-1, xj)

= f (Hu, Xu) - f (Hu, xj) + f (Hu, xj) - f (hj-1, xj)  f (Hu, Xu) - f (Hu, xj) + f (Hu, xj) - f (hj-1, xj)  Kf Xu - xj + Kf Hu - hj-1 ,

we obtain Htj - hj

tj

tj

 Htj-1 - hj-1 + Kf

Hu - hj-1 du + Kf

Xu - xj du

tj-1

tj-1

tj

 Htj-1 - hj-1 + Kf

Hu - Htj-1 + Htj-1 - hj-1 du

tj-1

+

Kf T

X T V ;[tj-1,tj ]



1

+

Kf T

Htj-1 - hj-1

+

Kf T

H + T V ;[tj-1,tj ] X . T V ;[tj-1,tj ]

19

By induction, we are led to

Htj - hj



Kf T

j-1

1

+

Kf T

k

k=0

H + T V ;[tk,tk+1] X T V ;[tk,tk+1]



Kf T

1

+

Kf T

T

X T V ;[0,1] + H T V ;[0,1]

 Kf eKf T

L+

f

eKf

,

which concludes the proof.

B.2 Proof of Proposition 2

Let h¯  Re¯ and let h¯i:j = (h¯i, . . . , h¯j) be its projection on a subset of coordinates. It is sufficient to

take F defined by

F(h¯) =

0e×d Id×d

2 1-L

f

(h¯ 1:e ,

h¯ e+1:e+d )

0d×1

,

where Id×d denotes the identity matrix and 0·×· the matrix full of zeros. The function H¯ is then

solution of

dH¯t =

0e×d Id×d

2 1-L

f

(H¯ t1:e

,

H¯ te+1:e+d

)

0d×1

dXt

1-L 2

dt

.

Note that theorem

(uTnhdeeorraesmsu4m)pstioonth(aAt 1H¯),

the tensor field F is well-defined.

satisfies the assumptions of the Picard-Lindelöf The projection of this equation on the last d

coordinates gives dH¯te+1:e+d = dXt, H¯0e+1:e+d = X0,
and therefore H¯te+1:e+d = Xt. The projection on the first e coordinates gives

dH¯ t1:e

=

1

2 -

L f (H¯t1:e, Xt) 1

- 2

L dt

=

f (H¯t1:e, Xt)dt,

H¯01:e = h0,

which is exactly (3).

B.3 Proof of Proposition 3

According to Lyons (2014, Lemma 5.1), one has

X¯ k[0,t]  (Rd)k

X¯

k T

V

;[0,t]

.

Let (t0, . . . , tk) be a partition of [0, t]. Then

k
X¯tj - X¯tj-1
j=1

k
=
j=1

Xtj - Xtj-1 2 +

1-L 2

2
(tj - tj-1)2

k


Xtj - Xtj-1

+

1

- 2

L

k
(tj - tj-1)

j=1

j=1

k
=

Xtj - Xtj-1

+

1

- 2

L t.

j=1

Taking the supremum over any partition of [0, t] we obtain

and thus

X¯ T V ;[0,t] 

X

TV

;[0,t]

+

1

- 2

Lt



L

+

1

- 2

L

=

1+L 2

< 1,

X¯ k[0,t]  (Rd)k

1+L 2

k
. It is then clear that

S[0,t](X¯ ) T =

 k=0

X¯ k[0,t]

2 (Rd )k

1/2





k=0


X¯ k[0,t]  (Rd)k
k=0

1+L 2

k = 2(1 - L)-1.

20

B.4 Proof of Proposition 4

We first recall the fundamental theorem of calculus for line integrals (also known as gradient theorem).
Theorem 5. Let g : Re  R be a continuously differentiable function, and let  : [a, b]  Re be a smooth curve in Re. Then
b
g(t)dt = g(b) - g(a),
a
where g denotes the gradient of g.

The identity above immediately generalizes to a function g : Re  Re:
b
J (g)(t)dt = g(b) - g(a),
a
where J(g)  Re×e is the Jacobian matrix of g. Let us apply Theorem 5 to the vector field F i between 0 and t, with  = H. We have

t

t

d

F i(Ht) - F i(H0) = J (F i)(Hu)dHu = J (F i)(Hu) F j(Hu)dXu

0

0

j=1

dt

dt

=

J (F i)(Hu)F j(Hu)dXu =

F j F i(Hu)dXu.

j=1 0

j=1 0

Iterating this procedure (N - 1) times for the vector fields F 1, . . . , F d yields

d
Ht = H0 +
i=1 d
= H0 +
i=1

t
F i(Hu)dXui
0

t

d

F i(H0)dXui +

0

i=1

td 0 j=1

u
Fj
0

F i(Hv)dXvj dXui

d

= H0 + F i(H0)S(i)(X)[0,t] +

Fj

i=1

1i,jd 0vut

= ···

F i(Hv)dXvj dXui

N
= H0 +

F i1

···

F

ik

(H0

)

1 k!

S[(0i,1t,]...,ik

)

(X

)

k=1 1i1,...,ikd

+

F i1

···

F

iN+1 (Hu1 )dXui11

·

·

·

dX iN+1
uN +1

,

1i1,...,iN+1d N+1;[0,t]

where N;[0,t] := {(u1, · · · , uN )  [0, t]N | 0  u1 < · · · < uN  t} is the simplex in [0, t]N . The first (N + 1) terms equal HtN . Hence,
Ht - HtN

=

F i1

···

F

iN+1 (Hu1 )dXui11

·

·

·

dX iN+1
uN +1

1i1,...,iN+1d N+1;[0,t]



F i1 · · · F iN+1 (Hu1 ) |dXui11 | · · · |dXuiNN++11 |

1i1,...,iN+1d N+1;[0,t]



sup

F i1

1i1,...,iN+1d N+1;[0,t] 1i1,...,iN+1d, h M

···

F iN+1 (h) |dXui11 | · · · |dXuiNN++11 |

 N+1(F)

|dXui11 | · · · |dXuiNN++11 |.

1i1,...,iN+1d N+1;[0,t]

21

Thus,

Ht - HtN  N+1(F)

|dXui11 | · · · |dXuiNN++11 |

1i1,...,iN+1d N+1;[0,t]

 N+1(F)

dXu1 · · · dXuN+1

1i1,...,iN+1d N+1;[0,t]

=

N +1 (F)

dN +1 (N + 1)!

[0,t]N +1

dXu1

· · · dXuN+1

=

N +1 (F)

dN +1 (N + 1)!

t
dXu
0

N +1

=

N +1 (F)

dN +1 (N + 1)!

X

N +1 T V ;[0,t]



N

+1(F)

dN +1 (N + 1)!

.

B.5 Proof of Proposition 5

For simplicity of notation, since the context is clear, we now use the notation · instead of · . (Re)k

According to Proposition ourselves in the ball BM¯

1, the solution H¯ of . Recall that for any

(4) verifies 1  i1, . . .

,

H¯ t iN

M  d, h¯

+ 

L := BM¯ ,

M¯ .

We

therefore

place

F i1 · · · F iN (h¯) = J (F i2 · · · F iN )(h¯)F i1 (h¯).

(16)

Linear case. We start with the proof of the linear case before moving on to the general case.

When  is chosen to be the identity function, FRiNN(h¯) = Wih¯ + bi, where Wi = 0e¯×e¯, bi is

each the i

FRiNN + dth

is an affine vector field, in the vector of the canonical basis of

sense Re+d,

that and

Wd+1 =

2 1-L

W

0d×e¯

and

bd+1 =

2 1-L

b

0d

.

Since J (FRiNN) = Wi, we have, for any h¯  Re+d and any 1  i1, . . . , ik  d,

FRi1NN · · · FRikNN(h¯) = Wik · · · Wi2 (Wi1 h¯ + bi1 ). Thus, for any h¯  BM¯ ,

FRi1NN · · · FRikNN(h¯)  Wik op · · · Wi2 op( Wi1 opM¯ + bi1 ).

For i = d + 1, Wi1 op = 0, and so

k(FRNN)  C Wd+1 kop-1,

with C = Wd+1 opM¯ + max(1, 2(1 - L)-1 b ). Therefore,



dk k!

k (FRNN )



C

d



1 k!

2d(1 - L)-1

W

op

k-1 < .

k=1

k=0

General case. In the general case, the proof is two-fold. First, we upper bound (16) by a function of the norms of higher-order Jacobians of F i1 , . . . , F iN . We then apply this bound to the specific case F = FRNN. We refer to Appendix C for details on higher-order derivatives in tensor spaces. Let F : Re  Re be a smooth vector field. If F (h) = (F1(h), . . . , Fe(h)) , each of its coordinates Fi is a function from Re to R, C  with respect to all its input variables. We define the derivative of order k of F as the tensor field
J k(F ) : Re  (Re)k+1
h  Jk(F )(h),

where

Jk(F )(h)

=

1j,i1 ,...,ik e

 k Fj (h) hi1 . . . hik

ej



ei1



·

·

·



eik .

We take the convention J0(F ) = F , and note that J(F ) = J1(F ) is the Jacobian matrix, and that

J k(J k (F )) = J k+k (F ).

22

Lemma 3. Let A1, . . . , Ak : Re  Re be smooth vector fields. Then, for any h  Re

Ak · · · A1(h) 

C(k; n1, . . . , nk) J n1 (A1)(h) · · · J nk (Ak)(h) ,

n1 +···+nk =k-1

where C(k; n1, . . . , nk) is defined by the following recurrence on k: C(1; 0) = 1 and for any n1, . . . , nk+1  0,

k

C(k + 1; n1, . . . , nk+1) = C(k; n1, . . . , n - 1, . . . , nk)

if nk+1 = 0, (17)

=1

C(k + 1; n1, . . . , nk+1) = 0

otherwise.

Proof. We refer to Appendix C for the definitions of the tensor dot product and tensor permutations,
as well as for computation rules involving these operations. We show in fact by induction a stronger result, namely that there exist tensor permutations p such that

Ak · · · A1(h) =

p J n1 (A1)(h) · · · J nk (Ak)(h) . (18)

n1+···+nk=k-1 1pC(k;n1,...,nk)

Note that we do not make explicit the permutations nor the axes of the tensor dot operations since we
are only interested in bounding the norm of the iterated star products. Also, for simplicity, we denote all permutations by , even though they may change from line to line.

We proceed by induction on k. For k = 1, the formula is clear. Assume that the formula is true at order k. Then
J (Ak · · · A1)

=

J p[ J n1 (A1) · · · J nk (Ak) ]

n1+···+nk=k-1 1pC(k;n1,...,nk)

=

p J [ J n1 (A1) · · · J nk (Ak) ]

n1+···+nk=k-1 1pC(k;n1,...,nk)

=
n1+···+nk=k-1 1pC(k;n1,...,nk)

k
p  
=1

J n1 (A1)

· · · J n +1(A ) · · ·

J nk (Ak) .

In the inner sum, we introduce the change of variable pi = ni for i = and p = n + 1. This yields J (Ak · · · A1)

=
p1 +···+pk =k

k
p  
=1 1pC(k;p1,...,p -1,...,pk)

J n1 (A1)

· · · J n +1(A ) · · · J nk (Ak)

=

q J n1 (A1) · · · J pk (Ak) ,

p1+···+pk+1=k 1qC(k+1;p1,...,pk+1)

where in the last sum the only non-zero term is for pk+1 = 0. To conclude the induction, it remains to note that

Ak+1 · · · A1 = J (Ak · · · A1) Ak+1 = J (Ak · · · A1) J 0(Ak+1).

Hence, Ak+1 · · · A1

=

q J n1 (A1) · · · J pk (Ak) J pk+1 (Ak+1)

p1+···+pk+1=k 1qC(k+1;p1,...,pk+1)

=

q J n1 (A1) · · · J pk (Ak) J pk+1 (Ak+1) .

p1+···+pk+1=k 1qC(k+1;p1,...,pk+1)

The result is then a consequence of (18) and of Lemma 6.

23

We now restrict ourselves to the case F = FRNN as defined by (5) and give an upper bound on the higher-order derivatives of the tensor fields F i1 , . . . , F iN . Lemma 4. For any i  {1, . . . , d + 1}, h¯  BM¯ , for any k  0,

J k(FRiNN)(h¯) 

2 1-L

W

F

k

(k) .

Proof. For any 1  i  d, FRiNN(h¯) is constant, so J k(FR1NN) = · · · = J k(FRdNN) = 0. For i = d+1, we have, for any 1  j  e,

kFRdN+N1,j (h¯) h¯i1 . . . h¯ik

=

2 1-L

k
Wji1

·

·

·

Wjik

(k)(Wj·h¯

+

b),

where Wj· denotes the jth row of W and for e + 1  j  e¯, Fjd+1 = 0. Therefore,

J k(FRdN+N1)(h¯) 2  = 

2 1-L
2 1-L
2 1-L

2k

|Wji1 · · · Wjik (k)(Wj·h¯ + b)|2

1j,i1 ,...,ik e

2k

(k)

2 

j

|Wji|2 k
i

2k

(k)

2 

W

2k F

.

We are now in a position to conclude the proof using condition (10). By Lemma 3 and 4, for any 1  i1, . . . , iN  d + 1,

FRi1NN · · · FRiNNN(h¯)



C(N ; nN , . . . , n1) J nN (FRiNNN)(h¯) · · · J n1 (FRi1NN)(h¯)

n1+···+nN =N -1



2 1-L

W

F

N -1

C(N ; nN , . . . , n1)an1+1n1! · · · anN +1nN !

n1+···+nN =N -1

a

1

2 -

L

a2

W

F

N -1

C(N ; nN , . . . , n1)n1! · · · nN ! .

n1+···+nN =N -1

Assume for the moment that C(N ; nN , . . . , n1) is smaller than the multinomial coefficient

N nN ,...,n1

.

Then, using the fact that there are

n+k-1 k-1

weak compositions of n in k parts and Stirling's approxi-

mation, we have

N (F)  a

1

2 -

L

a2

W

F

N -1
N ! × Card {n1 + · · · + nN = N - 1}

a

1

2 -

L

a2

W

F

N -1
N!

2N - 2 N -1



a 2

1

2 -

L

a2

W

F

N -1
N!

2N N



a

2e 

1

8 -

L

a2

W

F

N-1 N ! 

.

N

Hence, provided

W F < (1-L)/8a2d,





dk k!

k (F)



ad

2e 



k=1

k=1

8da2 W F 1-L

k-1 1 

< ,

k

and (A2) is verified.

To conclude the proof, it remains to prove the following lemma.

24

Lemma 5. For any k  1 and n1, . . . , nk  0, C(k; n1, . . . , nk) 

k-1 n1 ,...,nk

.

Proof. The proof is done by induction, by comparing the recurrence formula (17) with the following recurrence formula for multinomial coefficients:

k n1, . . . , nk+1

k+1
=
=1

k-1 n1, . . . , n - 1, . . . , nk+1

.

More precisely, for k = 1, C(1; 0) = 1 

0 0

= 1 and C(1; 1) = 0 

0 1

= 0. Assume

that the formula is true at order k. Then, at order k + 1, there are two cases. If nk+1 = 0,

C(k + 1; n1, . . . , nk+1) = 0, and the result is clear. On the other hand, if nk+1 = 0,

k

C(k + 1; n1, . . . , nk, 0) = C(k; n1, . . . , n - 1, . . . , nk)

=1

k

k-1



=1 n1, . . . , n - 1, . . . , nk

k+1

k-1


=1

n1, . . . , n - 1, . . . , nk+1



k n1, . . . , nk+1

.

B.6 Proof of Theorem 1

First, Propositions 1 and 2 state that if H¯ is the solution of (4) and Proj denotes the projection on the first e coordinates, then

zT -  Proj(H¯1)

= (hT ) -  Proj(H¯1])



 op hT - Proj(H¯1)





op

c1 T

.

For any 1  k  N , we let Dk(H¯0) : (Rd)k  Re be the linear function defined by

D k(H¯0)(ei1  · · ·  eik ) = F i1 · · · F ik (H¯0),

(19)

where e1, . . . , ed denotes the canonical basis of Rd¯. Then, under assumptions (A1) and (A2), if X¯ k

denotes the signature

of

order k

of the path X¯t

=

(Xt

,

1-L 2

t)

, according to Propositions 4 and 5,

H¯1 = H¯0 +



1 k!

S[(0i,1t,]...,ik)(X )F i1

···

F ik (H¯0) =



1 k!

D

k

(H¯ 0

)(Xk[0,t]

),

k=1 1i1,...,ikd

k=1

and

  Proj(H¯1) =   Proj



1 k!

D

k

(H¯ 0

)(X¯ k

)

=



1 k!





Proj

D k(H¯0)(X¯ k)

,

k=0

k=0

by linearity of  and Proj. Since the maps Dk(H¯0) : (Rd)k  Re are linear, the above equality takes the form



  Proj(H¯1) =

k, X¯ k , (Rd)k

(20)

k=0

25

where

k



(Rd)k

is

the

coefficient

of

the

linear

map

1 k!





Proj



D k(H¯0)

in

the

canonical

basis.

Let  = (0, . . . , k, . . . ). Under assumption (A2),





k

 2
(Rd )k

1 k!

2



2 op

F i1

···

k=0

k=0 1i1,...,ikd







2 op

1 k!

2
k

(F)2

k=0 1i1,...,ikd







2 op

dk k!

k

(F)

2
< .

k=0

F ik (H¯0) 2

This shows that   T , and therefore, using (20), we conclude

zT - , S(X¯ ) T





op

c1 T

.

B.7 Proof of Theorem 2

Let G = g : (Rd)T  R | g(x) = zT ,   

be the function class of (discrete) RNN and

S =  : X  R |  (X) = , S(X¯ ) T ,    ,

be the class of their RKHS embeddings, where  is defined by (20). For any   , we let RG () = E[ (y, g(x))], and RS () = E[ (y,  (X¯ ))],
and denote by Rn,G and Rn,S the corresponding empirical risks. We also let G , S , n,G , and n,S be the corresponding minimizers. We have

P y = gn,G (x) - Rn,G (n,G )  E (y, gn,G (x)) - Rn,G (n,G )

= RG (n,G ) - Rn,G (n,G )

= RG (n,G ) - RS (n,G ) + RS (n,G ) - Rn,S (n,G )

+ Rn,S (n,G ) - Rn,G (n,G )

 sup|RG () - RS ()| + sup|RS () - Rn,S ()|





+ sup|Rn,G () - Rn,S ()|.


Using Theorem 1, we have

sup|RG () - RS ()| = sup E (y, g(x)) - (y,  (X¯ ))





 supE |(yg(x)) - (y (X¯ ))|


 supE K |y| × |g(x) -  (X¯ )|


K

sup(




op

c1,

)

1 T

:=

c2 2T

,

where c1, = Kf eKf L + f eKf (the infinite norm f  is taken on the balls BL and BM ). One proves with similar arguments that

sup|Rn,G ()


-

Rn,S ()|



c2 2T

.

26

Under the assumption of the theorem, there exists a ball B  H of radius B such that S  B. This yields

sup|RS () - Rn,S ()|  sup |RB() - Rn,B()|,



T ,  T B

where

RB() = E[ (Y, (X¯ ))]

and

Rn,B()

=

1 n

n

(Y (i), (X¯ )).

i=1

We now have reached a familiar situation where the supremum is over a ball in a RKHS. It is known (see, e.g., Bartlett and Mendelson, 2002, Theorem 8) that with probability at least 1 - ,

sup |RB() - Rn,B()|  4K ERadn(B) + 2BK (1 - L)-1
T ,  T B

log(1/) 2n

,

where Radn(B) denotes the Rademacher complexity of B. Observe that we have used the fact that the loss is bounded by K B(1 - L)-1 since, for any   B, by the Cauchy-Schwartz inequality,
(y, (X¯ )) = (y , S(X¯ ) T )  K |y , S(X¯ ) T |  K  T S(X¯ ) T  2K B(1 - L)-1.

Finally, the proof follows by noting that Rademacher complexity of B is bounded by

Radn(B)



2B n

n

K(X(i), X(i))

=

2B n

i=1

n i=1

S(X¯ (i))

2 T



4B

(1 - 

L)-1

n

.

B.8 Proof of Theorem 3

Let G = g : (Rd)T  (Rp)T | g(x) = z1, . . . , zT ,   

be the function class of discrete RNN in a sequential setting. Let

S =  : X  (Rp)T | (X) = (X~[1]), . . . , (X~[T ]) ,

be the class of their RKHS embeddings, where X~[j] is the path equal to X on [0, j/T ] and then constant on [j/T , 1] (see Figure 4). For any X  X ,

 1,, S(X¯ ) T  1, (X)

 (a)

=

 

...

= 

...

  Rp, 

p,, S(X¯ ) T

p, (X )

where (1,, . . . , p,)



(T

)p

are

the

coefficients

of

the

linear

maps

1 k!





Proj



D k(H¯0)

:

(Rd)k  Rp, k  0, in the canonical basis, where Dk is defined by (19).

We start the proof as in Theorem 2, until we obtain

RG (n,G ) - Rn,G (n,G )  sup|RG () - RS ()| + sup|RS () - Rn,S ()|





+ sup|Rn,G () - Rn,S ()|.


27

By definition of the loss, for any   ,

|RG () - RS ()| = E y, g(x) - y, (X)

1T E T
j=1

yj - zj 2 - yj - (X~[j]) 2

1T E T

zj + (X~[j]) - 2yj , zj - (X~[j])

j=1

1T E T

zj + (X~[j]) - 2yj × zj - (X~[j])

j=1

(by the Cauchy-Schwartz inequality).

According to inequality (13), one has

zj - (X~[j])





op

c1, T

,

where c1, = Kf eKf L + f eKf . Moreover,

p

p

(X~[j]) 2 =

 ,, S(X~[j]) T 2 

 ,

2 T

S(X~[j])

2 T

 pB2

2(1 - L)-1

2,

=1

=1

since S(X~[j]) T = S[0,j/T](X¯ ) T  S(X¯ ) T . This yields

zj + (X~[j]) - 2yj  zj + (X~[j]) + 2 yj   op f  + 2pB(1 - L)-1 + 2Ky.

Finally,

sup|RG ()


-

RS ()|



c3 2T

,

where c3 = sup c1, +  op f  + 2pB(1 - L)-1 + 2Ky. One proves with similar arguments



that

sup|Rn,G ()


-

Rn,S ()|



c3 2T

.

We now turn to the term sup|RS () - Rn,S ()|. We have


RS () - Rn,S ()

=

E[

(y, (X))]

-

1 n

n

(y(i), (X(i)))

i=1

=

1 T

T

E[

yj

- (X~[j])]

2-

1 n

n

yj(i) - (X~[(ji])) 2 .

j=1

i=1

Therefore,

sup|RS () - Rn,S ()| 


1 T

T
sup
j=1 

E[

yj

- (X~[j])]

2-

1 n

n i=1

yj(i) - (X~[(ji])) 2 .

Note that for a fixed j, the pairs (X~[(ji]), yj(i)) are i.i.d. Under the assumptions of the theorem, there exists a ball B  H such that for any 1   p,   ,  ,  B . We denote by Bp the sum of p such spaces, that is,

Bp = f : X  Rp | f(X) = (f1 (X), . . . , fp (X)) , f  B .

28

Clearly,   Bp, and it follows that

sup E[


yj - (X~[j])]

2-

1 n

n i=1

yj(i) - (X~[(ji])) 2

 sup E
f Bp

yj - f(X~[j]) 2

-

1 n

n

yj(i) - f(X~[(ji])) 2 .

i=1

We have once again reached a familiar situation, which can be dealt with by an easy extension of Bartlett and Mendelson (2002, Theorem 12). For any f  Bp, let ~  f : X × Rp : (X, y)  y - f(X) 2 - y 2. Then, ~  f is upper bounded by

|~  f(X, y)| =

y - f(X) 2 - y 2  f(X) f(X) + 2 y  2pB(1 - L)-1(2pB(1 - L)-1 + 2Ky)  4pB(1 - L)-1(B(1 - L)-1 + Ky).

Let c4 = B(1 - L)-1 + Ky and c5 = 4pB(1 - L)-1c4 + Ky2. Then with probability at least 1 - ,

sup E yj - f(X~[j])
f Bp

1n -n

yj(i) - f(X~[(ji]))

i=1

 Radn(~  Bp) +

2c5

log(1/) n

,

where ~  Bp = (X, y)  ~  f(X, y)|f  Bp . Elementary computations on Rademacher complexities yield

Radn(~  Bp)  2pc4Radn(B)  8pc4B(1n- L)-1 ,

which concludes the proof.

C Differentiation with higher-order tensors

C.1 Definition

We define the generalization of matrix product between square tensors of order k and .
Definition 4. Let a  (Re)k, b  (Re) , p  {1, . . . , k}, q  {1, . . . , }. Then the tensor dot product along (p, q), denoted by a p,q b  (Re)(k+ -2), is defined by

e

(a p,q b)(i1,...,ik-1,j1,...,j -1) =

a b . (i1,...,ip-1,j,ip,...,ik-1) (j1,...,jq-1,j,jq ,...,j -1)

j=1

This operation just consists in computing a  b, and then summing the pth coordinate of a with the qth coordinate of b. The operator is not associative. To simplify notation, we take the convention that it is evaluated from left to right, that is, we write a b c for (a b) c. Definition 5. Let a  (Re)k. For a given permutation  of {1, . . . , k}, we denote by (a) the permuted tensor in (Re)k such that
(a)(i1,...,ik) = a . (i(1),...,i(k))
Example 5. If A is a matrix, then AT = (A), with  defined by (1) = 2, (2) = 1.

C.2 Computation rules
We need to obtain two computation rules for the tensor dot product: bounding the norm (Lemma 6) and differentiating (Lemma 7). Lemma 6. Let a  (Re)k, b  (Re) . Then, for all p, q,
a b  p,q (Re)k+ -2d a (Re)k b (Re) .

29

Proof. By the Cauchy-Schwartz inequality,

a b2
p,q (Re)k+ -2

=

(a

1i1,...,ik-1,j1,...,j -1e

p,q b)2(i1,...,ik-1,j1,...,j -1)

=
1i1,...,ik-1,j1,...,j -1e

2
a b (i1,...,ip-1,j,ip,...,ik-1) (j1,...,jq-1,j,jq ,...,j -1)
1je


i1,...,ik-1,j1,...,j -1

a2
(i1 ,...,ip-1 ,j,ip ,...,ik-1 ) j

b2
(j1,...,jq-1,j,jq ,...,j -1) j



a2
(i1 ,...,ip-1 ,j,ip ,...,ik-1 )

b2
(j1,...,jq-1,j,jq ,...,j -1)

i1 ,...,ik-1 ,j

j1,...,j -1,j



a

2 (Re )k

b

2 (Re )

.

Lemma 7. Let A : Re  (Re)k, B : Re  (Re) be smooth vector fields, p  {1, . . . , k}, q  {1, . . . , }. Let A p,q B : Re  (Re)k+ -2 be defined by A p,q B(h) = A(h) p,q B(h). Then there exists a permutation  such that
J (A p,q B) = (J (A) p,q B) + A p,q J (B).

Proof. The left-hand side takes the form

(J (A p,q B))i1,...,ik-1,j1,...,j -1,m =
j
The first term of the right-hand side writes

A hm

B(j1,...,jq-1,j,jq ,...,j
(i1 ,...,ip-1 ,j,ip ,...,ik-1 )

-1 )

+

A(i1 ,...,ip-1 ,j,ip ,...,ik-1 )

B hm (j1,...,jq-1,j,jq,...,j

-1 )

.

(J (A) p,q B)i1,...,ik-1,m,j1,...,j -1 =
j

A hm

B(j1,...,jq-1,j,jq ,...,j
(i1 ,...,ip-1 ,j,ip ,...,ik-1 )

-1 )

,

and the second one
(A p,q J (B))i1,...,ik-1,j1,...,j -1,m =
j

A(i1 ,...,ip-1 ,j,ip ,...,ik-1 )

B hm

(j1,...,jq-1,j,jq ,...,j

-1 )

.

Let us introduce the permutation  which keeps the first (k - 1) axes unmoved, and rotates the remaining ones such that the last axis ends up in kth position. Then

(J (A) p,q B)i1,...,ik-1,j1,...,j -1,m =
j

A hm

B(j1,...,jq-1,j,jq ,...,j
(i1 ,...,ip-1 ,j,ip ,...,ik-1 )

-1 )

.

Hence J (A p,q B) = (J (A) p,q B) + A p,q J (B), which concludes the proof.

The following two lemmas show how to compose the Jacobian and the tensor dot operations with permutations. Their proofs follow elementary operations and are therefore omitted.
Lemma 8. Let A : Re  (Re)k and  a permutation of {1, . . . , k}. Then there exists a permutation ~ of {1, . . . , k + 1} such that
J((A)) = ~(J(A)).
Lemma 9. Let a  (Re)k, b  (Re) , p  {1, . . . , k}, q  {1, . . . , },  a permutation of {1, . . . , k}. Then there exists p~  {1, . . . , k}, q~  {1, . . . , }, and a permutation ~ of {1, . . . , k +
- 2} such that (a) p,q b = ~(a p~,q~ b).

30

The following result is a generalization of Lemma 7 to the case of a dot product of several tensors. Lemma 10. For  {1, . . . , k}, n  N, let A : Re  (Re)n be smooth tensor fields. For any (p )1 k-1 and (q )1 k-1 such that p  {1, . . . , n }, q  {1, . . . , n +1}, there exist k permutations ( )1 k such that
k
J (A1 p1,q1 A2 p2,q2 · · · pk-1,qk-1 Ak) =  [A1 A2 · · · J (A ) · · · Ak] ,
=1
where the dot products of the right-hand side are along some axes that are not specify for simplicity.

Proof. The proof is done by induction on k. The formula for k = 1 is straightforward. Assume that the formula is true at order k. As before, we do not specify indexes for tensor dot products as we are
only interested in their existence. By Lemma 9, we have

J (A1 · · · Ak+1) = J ((A1 · · · Ak) Ak+1) = (J (A1 · · · Ak) Ak+1) + A1 · · · Ak J (Ak+1)

k

=

 [A1 A2 · · · J (A ) · · · Ak] Ak+1 + A1 · · · Ak J (Ak+1)

=1

k

=

~ [A1 A2 · · · J (A ) · · · Ak Ak+1] + A1 · · · Ak J (Ak+1)

=1

k

= ^ [A1 A2 · · · J (A ) · · · Ak Ak+1] + A1 · · · Ak J (Ak+1)

=1

(where ^ =   ~)

k+1
= ^ [A1 A2 · · · J (A ) · · · Ak Ak+1] .
=1

D Experimental details
All the code to reproduce the experiments is available on GitHub at https://github.com/ afermanian/rnn-kernel. Our experiments are based on the PyTorch (Paszke et al., 2019) framework. When not specified, the default parameters of PyTorch are used.
Convergence of the Taylor expansion. For Figure 1, 103 random RNN with 2 hidden units are generated, with the default weight initialization. The activation is either the logistic or the hyperbolic tangent. In Figure 1b, only the results with the logistic activation are plotted. The process X is taken as a 2-dimensional spiral. The reference solution to the ODE (3) is computed with a numerical integration method from SciPy (Virtanen et al., 2020, scipy.integrate.solve_ivp with the `LSODA' method). The signature in the step-N Taylor expansion is computed with the package Signatory (Kidger and Lyons, 2021).
The step-N Taylor expansion requires computing higher-order derivatives of tensor fields (up to order N ). This is a highly non-trivial task since standard deep learning frameworks are optimized for first-order differentiation only. We refer to, for example, Kelly et al. (2020), for a discussion on higher-order differentiation in the context of a deep learning framework. To compute it efficiently, we manually implement forward-mode higher-order automatic differentiation for the operations needed in our context (described in Appendix C). A more efficient and general approach is left for future work. Our code is optimized for GPU.
Penalization on a toy example. For Figure 2, the RNN is taken with 32 hidden units and hyperbolic tangent activation. The data are 50 examples of spirals, sampled at 100 points and labeled ±1
31

according to their rotation direction. We do not use batching and the loss is taken as the cross entropy. It is trained for 200 epochs with Adam (Kingma and Ba, 2015) with an initial learning rate of 0.1. The learning rate is divided by 2 every 40 epochs. For the penalized RNN, the RKHS norm is truncated at N = 3 and the regularization parameter is selected at  = 0.1. Earlier experiments show that this order of magnitude is sensible. We do not perform hyperparameter optimization since our goal is not to achieve high performance. The initial hidden state h0 is learned (for simplicity of presentation, our theoretical results were written with h0 = 0 but they extend to this case). The accuracy is computed on a test set of size 1000. We generate adversarial examples using 50 steps of projected gradient descent (following Bietti et al., 2019). The whole methodology (data generation + training) is repeated 20 times. The average training time on a Tesla V100 GPU for the RNN is 8.5 seconds and for the penalized RNN 12 seconds.
Figure 3 is obtained by selecting randomly one run among the 20 of Figure 2.
Libraries. We use PyTorch (Paszke et al., 2019) as our overall framework, Signatory (Kidger and Lyons, 2021) to compute the signatures, and SciPy (Virtanen et al., 2020) for ODE integration. We use Sacred (Klaus Greff et al., 2017) for experiment management. The links and licences for the assets are given in the following table:

Name
PyTorch Sacred SciPy Signatory

Homepage link
GitHub repository GitHub repository GitHub repository GitHub repository

License
BSD-style License MIT License
BSD 3-Clause "New" or "Revised" License Apache License 2.0

32

