Fast improvement of TEM image with low-dose electrons by
deep learning
Hiroyasu Katsuno1, Yuki Kimura1, Tomoya Yamazaki1 and Ichigaku Takigawa2,3
1Institute of Low Temperature Science, Hokkaido University, Kita-19, Nishi-8, Kita-ku, Sapporo, Hokkaido, 060-0819, Japan 2RIKEN, Center for Advanced Intelligence Project, 1-4-1 Nihonbashi, Chuo-Ku, Tokyo 103-0027, Japan 3Institute for Chemical Reaction Design and Discovery (WPI-ICReDD), Hokkaido University, N21 W10, Kita-ku, Sapporo, Hokkaido, 001-0021, Japan Corresponding Author: Hiroyasu Katsuno <katsuno@lowtem.hokudai.ac.jp>
Abstract Low-electron-dose observation is indispensable for observing various samples using a transmission electron microscope; consequently, image processing has been used to improve transmission electron microscopy (TEM) images. To apply such image processing to in situ observations, we here apply a convolutional neural network to TEM imaging. Using a dataset that includes short-exposure images and long-exposure images, we develop a pipeline for processed short-exposure images, based on end-to-end training. The quality of images acquired with a total dose of approximately 5 e- per pixel becomes comparable to that of images acquired with a total dose of approximately 1000 e- per pixel. Because the conversion time is approximately 8 ms, in situ observation at 125 fps is possible. This imaging technique enables in situ observation of electron-beam-sensitive specimens. Key Words: deep learning, transmission electron microscopy, fast improvement, nanoparticles (Received XX Y 20ZZ; revised XX Y 20ZZ; accepted XX Y 20ZZ)

arXiv:2106.01718v1 [eess.IV] 3 Jun 2021

Introduction
Transmission electron microscopy (TEM) is a powerful tool in the field of materials science and provides structural information through atomic-level visualization (Kisielowski et al., 2008; Morishita et al., 2018). To obtain clearer images, researchers have implemented hardware improvements in both the columns [e.g., by introducing an aberration corrector (Haider et al., 1998) and a phase plate (Danev & Nagayama, 2001)] and cameras (Faruqi et al., 2003) incorporated into transmission electron microscopes. TEM has become a useful technique in various fields such as biology, electrochemistry, fluids, geology, and the environmental sciences because various sample holders that enable control of the sample environments have been developed. One of the current limitations for the further application of TEM is the influence of electron irradiation on a sample: A beam-sensitive material will o en lose its characteristic structure under electron irradiation before an image can be acquired. Cryo-TEM observation is an outstanding technique to reduce the electron-induced damage on a sample (Morán & Dahl, 1952; Chlanda & Sachse, 2014). Nevertheless, an additional technique is needed for low-dose observation of various samples and for in situ imaging. In particular, in situ imaging using liquid-cell TEM requires both low-dose observation and high temporal resolution to reduce the

beam e ect, which is the radiolysis of the solution sample (Schneider et al., 2014).
Possible low-electron-dose TEM imaging methods include the dictionary learning method based on sparse coding (Elad & Aharon, 2006). Achieving the desired improvement requires the construction of an appropriate dictionary. When an appropriate dictionary is used, a fragment of the image is replaced by a linear combination of basic elements. Sparse coding has been successfully used to improve electron tomography images (Binev et al., 2012), scanning TEM imaging (Stevens et al., 2014), and electron holography (Anada et al., 2019).
Over the past decade, image taken by general cameras have been dramatically improved by machine learning. In addition to sparse coding, regarding image denoising, numerous methods have been proposed for image denoising, including sub-pixel convolutional neural network (CNNs) (Shi et al., 2016), nuclear norm minimization (Gu et al., 2014), and domain filtering (Dabov et al., 2007). These techniques are based on topics such as smoothness and/or sparsity and useful for still images. However, these techniques have a disadvantage for in site observations because of computational cost. For example, the time necessary for image improvement via sparse cording is about 1­10 s (Anderson et al., 2013).
Recently, a new technique of deep learning has been proposed for low-light image enhancement. In this

1

: Fast improvement by deep learning

Table 1. The number of images for training and validation.

Set No. 1 2 3 4 5

Material Ni
Fe­Ni SiC
Silicate Alumina

Training 224 260 140 176 200

Validation 36 40 20 24 40

method, an images is acquired with a low-light image so that the original image can be used as ground truth for comparison (Lim et al., 2015). A CNN is introduced, and an image set of short-exposure and long-exposure images are prepared for training (Chen et al., 2018). The corresponding results have shown that not only the machine learning model but also datasets need improvement.
In general, image conversion is fast, although it takes a long time to train a CNN model. Using a CNN may help increase the speed of image improvement, enabling in situ observation. In the present work, we apply a CNN model to TEM imaging and evaluate the image quality and the speed for image improvement.
Methods
We used a transmission electron microscope equipped with a field-emission gun (JEM-2100F, JEOL, Tokyo) operated at an acceleration voltage of 200 kV and a CMOS camera, OneView IS (Gatan, Inc., Pleasanton, CA, USA). For training, we prepare sets of high-dose-electron (HDE) and low-dose-electron (LDE) TEM images with the same field of view. All images were acquired with dri correction using the function incorporated into the so ware (Digital Micrograph, Gatan. Inc., Pleasanton, CA, USA) used to operate the CMOS camera. The HDE image resolution was 4096 × 4096 pixels, and its exposure time was 5 s. A er an HDE image was acquired, the corresponding LDE image is taken at a resolution of 512 × 512 pixels and with an exposure time of 3.3 ms. The typical total doses in each view were 1010 e- and 106 e- in HDE and LDE images, respectively, on the camera. The dose rate was calibrated using a Faraday cage (JEOL Ltd., Tokyo, Japan). The numbers of electrons on each pixel were  1000 e- and  5 ­ 10 e- in the HDE and LDE images, respectively. Typical magnifications used in the present study were 25, 000× and 30, 000×; the dose rate on the samples was therefore  102 e-nm-2s-1 in case of LDE observation. The number of images for training data is summarized in Table 1. The typical samples were particles of Ni, FeNi alloy, SiC, silicate, and alumina with diameters of 30­200 nm.
The original binary data of the digital micrograph were converted to grayscale images in ti format with

the dark current subtracted and including the intensity with 16-bit expression in each pixel. The obtained images were preprocessed with intensity rescaling. Before training, the position of each HDE image was adjusted so that its position fit the corresponding LDE image, thereby compensating for sample dri in the sequences.
Our deep learning model has the U-Net architecture (Ronneberger et al., 2015) with the ResNet encoder/decoder (He et al., 2016) using the segmentation package in PyTorch (Yakubovskiy, 2020). The schematic of our image improvment is shown in Fig. 1. The model parameters were fine-tuned against the pretrained Resnet-18 model from the ImageNet data using the L1 loss function and the Adam optimizer (Kingma & Ba, 2015) with the learning rate of 10-4. The concrete form of L1 function for two images is wri en as

L1

=

1 N

x

|IA(x) - IB(x)|,

(1)

where x represents the position of a pixel. IA (x ) and IB (x ) are the normalized intensity of two images at the position x , and N is the total number of pixels. In each iteration, an LDE image was randomly flipped horizontally and/or vertically for data augmentation and the random crop was not applied. All training was conducted on a Linux machine with 10-core Intel i9-9900X 3.5 GHz CPU and an NVIDIA adro RTX 8000 graphics card (see Machine No. 1 in Table 3).

Results & Discussion
Overview of the Image Improvement
Figure 2 shows the epoch dependence of the loss of the training dataset and the validation dataset. As the training proceeded, the value of the loss decreased until 100 epochs. Whereas the loss value for the training data decreased a er 100 epochs, that for the validation data was steady. We also checked the loss in more than 1000 epochs, and observed a slight increase in the loss value of the validation data because of overfi ing. In the present study, we used the finetuned parameter at 900 epochs.
Figure 3 shows some examples of HDE images, LDE images, and output images obtained using our model from corresponding LDE images. These HDE and LDE image sets were not used for training. Images of Ni, FeNi, SiC, silicate, and alumina are displayed from the top row to the bo om row in Fig. 3. Various sizes and contrasts of nanoparticles are observed in HDE images. The preprocessed LDE images are similar to the images displayed by the camera so ware (Gatan Digital Micrograph) during low-dose-rate observations. All of the images show noise resembling a sandstorm. A common feature of all of the output images is that the noise is removed, although

2

: Fast improvement by deep learning

Input LDE Image
Random Flip

U-Net

CNN Encoder

CNN Decoder

CNN Head

Output Image

Truth HDE Image

Evaluation
L1 Loss

Fig. 1. Schematic of image improvement using our deep learning model.

Table 2. alitative comparison of output images for validation using the mean absolute error (MAE) and the peak signal-to-noise ratio (PSNR).

Set no. 1 2 3 4 5

Material Ni FeNi SiC
Silicate Alumina (Average)

MAE 0.075 ± 0.026 0.034 ± 0.011 0.074 ± 0.020 0.062 ± 0.025 0.075 ± 0.026 0.063 ± 0.028

PSNR 20.82 ± 2.13 25.10 ± 1.86 20.80 ± 1.60 21.60 ± 2.82 20.69 ± 2.06 21.97 ± 2.79

Fig. 2. Training loss and validation loss.
a haze remained in the background of the images in the third and fi h rows of Fig. 3. The overlapping nanoparticles, such as those in the fourth row and first column image of Fig. 3, were reproduced in the output images. Thus, we obtained an image comparable to an HDE image from a sandstorm image, although the improvement failed in some cases: the appearance of imaginary nanoparticles indicated by the arrow in the image in the fi h row and first column of Fig. 3, and the disappearance of nanoparticles, such as in the boxed area in the image in the third row and third column of Fig. 3.
The quality of the improvement appears to depend on the sample. For a qualitative comparison of the quality of the output images, the mean absolute error (MAE) and the peak signal-to-noise ratio (PSNR) are listed in

Table 2. The MAE is the di erence between images at the pixel level (see details in the next subsection) and the large MAE value indicates a large di erence between images. The PSNR is a logarithm of the inverse of the mean square error where a large value of PSNR indicates that two images are similar. Both indicators show that our model is more e ective for set No. 2 (FeNi) than the other sets. In set No. 2, the magnitude of the electron count on nanoparticles is less than 50% of that on the background in HDE images. However, in sets No. 3 (SiC) and No. 5 (alumina), the magnitude of the electron count on the nanoparticles is almost 90% of that on the background in HDE images; i.e., most of the SiC and alumina nanoparticles exhibit relatively weaker contrast than FeNi particles. The small di erence in contrast between the background and the nanoparticles, which is only 10%, may prevent the image improvement. In sets No. 1 (Ni) and No. 4 (silicate), the HDE images tend to aggregate nanoparticles with weak and strong contrast. In particular, the large standard deviation of the PSNR of set No. 4 might be reflected the variety of images. At least, the statistical data indicate that our model provides the same level of image quality for various materials.
Example of Visualization of LDE Image
In a microscope image, indicators of the performance include the accuracy of the size of an object and the accuracy of the separation of two adjacent objects. We studied the improvement rate and these two indicators in detail as an example in set No. 4.
Figure 4 shows an example of (a) an HDE image, (b) an LDE image, and (c) a corresponding output image

3

: Fast improvement by deep learning
Set No. 1: Ni
500 nm
Set No. 2: FeNi
500 nm
Set No. 3: SiC
500 nm
Set No. 4: Silicate
500 nm
Set No. 5: Alumina
500 nm
Fig. 3. Examples of HDE, LDE and output images (le to right); images of Ni, FeNi, SiC, silicate, and alumina displayed top to bo om. Arrows and boxes indicate examples of the failure results of the improvement. The presented images were used for validation, not training.

(a)

I

II C

A

B

D

(b)

I

II C

A B

D

(c)

I

II C

A B

D

500 nm
Fig. 4. Examples of (a) an HDE image, (b) a LDE image and (c) an output image of set No. 4. The average dose is about 1200 e- per pixel and 4.7 e- per pixel in a box. The insets in the figures show enlarged images of parts of region I and II. The presented images were used for validation, not training.
4

: Fast improvement by deep learning

(a)

(b)

LDE

HDE

(c)

(d)

Fig. 5. (a) Raw intensity histograms of the HDE and LDE images shown in Fig. 4. Scaled intensity histograms of (b) the HDE image, (c) the corresponding LDE image, and (d) the output image in Figs. 4(a)­(c), respectively. The abscissa is the electron count and the ordinate is the number of pixels. The data is plo ed as semi-log plots.

from (b). In Fig. 4(a), there are nine silicate nanoparticles in region I indicated by the dashed-line box and three nanoparticles in region II. The nanoparticles are typically 100 nm in diameter. Figure 4(b) shows a preprocessed LDE image. Sandstorm-like noise is present in the whole image. Only two particles are recognized in region I, and it is di icult to recognize a particle in region II. Figure 4(c) is the output image generated from Fig. 4(b) by our model. Particles are recognized in both regions I and II, and no particles are observed elsewhere, as in Fig. 4(a).
All images have been normalized and have intensity values from 0 to 1 in each pixel, and L1 function indicates the di erence in the intensity at pixels (see Eq. 1). The maximum value of L1 is 1, where an image is composed of only black and white pixels and another image is composed of the opposite color, e.g., a set of all black and all white images. On the contrary, the minimum value is 0; where two images are in complete agreement. The value of the L1 loss in Fig. 4(a) and Fig. 4(b) is 0.34. A er the training, the value decreases to 0.02 in the case of Fig. 4(a) and Fig. 4(c). The image is improved by our model.
Figure 5(a) shows the raw intensity histograms of the HDE and LDE images shown in Fig. 4 as semi-log plots. As the minimum electron count of the HDE image is 396 and the maximum value of the LDE image is 24, the histogram of the LDE image is barely visible in this plot. The

histograms of the HDE and the LDE images are scaled as shown in Figs. 5(b) and 5(c), respectively. On the histogram corresponding to the HDE image [Fig. 5(b)] shows two peaks. The sharp peak arises from the background, whose value is about 0.7­0.8. Another broad peak at 0.3 originates from the nanoparticles. Two peaks are clearly separated on the intensity histogram. However, the intensity histogram corresponding to the LDE image [Fig. 5(c)] shows a single peak. The intensity of the nanoparticles is not substantially di erent from that of the background. Figure 5(d) shows the histogram of the output image. Despite the conversion of the LDE image [Fig. 4(b)], whose histogram shows a broad single peak [Fig. 5(c)], a sharp peak at 0.7 and a broad peak at 0.4 appear. The shape of the histogram is similar to that of the histogram of the HDE image although there is no intensity greater than 0.8 or smaller than 0.15.
We here focus on the discrimination of nanoparticles from viewpoint of image improvement. We first investigated whether the size of nanoparticles could be accurately reproduced in the output images. A magnified view of location A­B in the LDE image is shown in the bo om-le inset in Fig. 4. Although all nanoparticles are easily found in the HDE and output images, all of the nanoparticles are di icult to observe in the LDE image. The line profiles corresponding to A­B in Fig. 4 are shown

5

(a)

102.1 nm

(b)

: Fast improvement by deep learning
(c)
106.1 nm

Intensity

d = 95.2 nm

A

B

A

Position [nm]

d = 96.0 nm

B

A

B

Position [nm]

Position [nm]

Fig. 6. Line profile of A­B shown in Fig. 4: (a) the HDE image, (b) the LDE image, and (c) the output image. Circles are the data used to calculate the diameter of the nanoparticle. The do ed line shows the average of the background.

in Fig. 6. In the HDE image [Fig. 6(a)] and the output

image [Fig. 6(c)], a concave curve appears at the center

because of the nanoparticle. In the LDE image shown in

Fig. 6(b), random noise is present and the concave curve

does not appear. When the shape of the nanoparticle is

assumed to be spherical, the diameter of the nanoparti-

cle is found to be 95.2 nm in the HDE image and 96.0

nm in the output image on the basis of the data corre-

sponding to the center of the concave curve. The output

image generated by our model reproduces the size of the

nanoparticle with an accuracy within 1%.

In addition, we investigated the edge width of a

nanoparticle. The edge width was estimated by compar-

ing a region of strong contrast and the estimated diam-

eter of a particle with an assumed spherical shape. The

distance between crosspoints composed of the line pro-

file (blue solid line) and the background (horizontal dot-

ted line) in Figs. 6(a) and 6(c) was assumed to represent

the pseudo-diameter of the nanoparticle indicated by A­

B in Fig. 4. By subtracting the estimated particle diame-

ter from the pseudo-diameter of the nanoparticle, we ob-

tained the value of twice the edge width. The obtained

edge widths were 6.9 nm and 10.1 nm for the nanopar-

ticle in the HDE image and that in the output image, re-

spectively. Thus, the magnitude of the edge width in the

output image is comparable to that in the HDE image.

We also investigated whether two adjacent nanopar-

ticles could be distinguished. The line profiles corre-

sponding to C­D in Fig. 4 are shown in Fig. 7. The line

profile of the LDE image [Fig. 7(b)] shows random noise,

as does that in Fig. 6(b). At location C­D, nanoparti-

cles are di icult to recognize in the LDE image. By con-

trast, nanoparticles are recognized in the HDE and out-

put images. From the line profile of the HDE image

[Fig.

7(a)],

we

obtained

diameters

of

d

HDE 1

=

58.3 nm

and

d

HDE 2

=

73.0 nm for the nanoparticles in the HDE

image. From the nanoparticles' center position indicated

by do ed vertical lines shown in Fig. 7(a), the distance between two nanoparticles is obtained as l HDE = 78.4

nm.

The

condition

(d

HDE 1

+

d2HDE)/2

<

l HDE is satis-

fied; that is, the system has su icient resolution to dis-

tinguish two nanoparticles. In the case of the output im-
age in Fig. 7(c), the sizes of nanoparticles are d1out = 72.5 nm and d2out = 53.8 nm. The di erence in nanoparticle size determined from the HDE and output images is

more than 20%. The distance between nanoparticles is

l out = 72.9 nm, which is close to the l HDE. The condition

(d1out

+

d

out 2

)/2

<

l out

is

also

satisfied

for

output

image.

A er applying our CNN model, we could distinguish be-

tween two adjacent nanoparticles at this size scale.

Our model removes noise from LDE images, clearly

revealing the presence of nanoparticles. The center po-

sition and the distance between adjacent nanoparticles

can be reproduced at this size scale. Although the size

of the nanoparticles in the output image matches that

in the HDE image, the nanoparticles' shape is uncertain

(see Fig. 4). Moreover, the intensity of the line profile due

to the internal structure of the nanoparticles can disap-

peared during the image processing because our model

is a CNN that appears to spatially average the intensity.

Therefore, restoring the image of agglomerated nanopar-

ticles like those in region I of Fig. 4, tends to be di icult,

although isolated nanoparticles are correctly reproduced

a er our image processing. For instance, we can count

only five or seven nanoparticles in region I of the output

image [Fig.4(c)] against nine nanoparticles in the corre-

sponding HDE image [Fig. 4(a)].

Waiting Time for Improvement
For in situ TEM observations with low electron doses, improving the LDE image on a timescale that approaches the camera speed is important. In maintain high performance of the TEM observation, the output image should be generated from the LDE image faster than the frame rate of the camera. The framerates of the CMOS camera (OneView IS) are 25 and 300 frames per second in normal mode (output size: 4096 × 4096 pixels) and bin-

6

: Fast improvement by deep learning

Intensity

(a)
d1HDE=58.3 nm d2HDE =73.0 nm

C

lHDE = 78.4 nm

D

Position [nm]

(b)

C

D

Position [nm]

(c)
d1out=72.5 nm d2out=53.8 nm

lout = 72.9 nm

C

D

Position [nm]

Fig. 7. Line profile of C­D shown in Fig. 4: (a) the HDE image, (b) the LDE image, and (c) the output image. Circles are the data used to calculate the diameter of the nanoparticle. The do ed line shows the average of the background.

Table 3. Waiting time for an output image in various processors. The unit is in milliseconds. The values were obtained as an average from 100 samples.

Machine No. 1
2

Processor

Model number

CPU GPU CPU GPU

Intel Core i9-9900X 3.50 GHz NVIDIA adro RTX 8000 Intel Core i7-9700 3.00 GHz NVIDIA Geforce GTX 1650

Calc.
106 ± 0.7 4.4 ± 0.0 300 ± 10 5.3 ± 0.6

Calc. including data transfer
­ 8.0 ± 0.1
­ 25 ± 0.5

ning 8 mode (output size: 512 × 512 pixels), respectively. Our goal was to convert to one image within 40 ms in the first step and then every 3.3 ms therea er. Table 3 shows the waiting time for converting an LDE image using our model and two types of machines. The first configuration (Machine No. 1) is a calculation machine for numerical calculation, where the training in this study is performed. The second configuration (Machine No. 2) is a personal computer for normal use. In both cases, the CPU calculation requires hundreds of milliseconds. The GPU calculation is fast, and the performance is more than 40 times greater than that of CPU calculation; that is, the conversion time is several milliseconds. The time of conversion using the Geforce GTX 1650 graphics card is approximately the same as that using the adro RTX 8000 graphics card. However, GPU calculation requires the data transfer from the CPU to the GPU and from the GPU to the CPU. When the data transfer is taken into account, the total waiting time is 8 ms with Machine No. 1 and 25 ms with Machine No. 2. Although the waiting time of 8 ms is more than two times longer than the 3.3 ms maximum temporal resolution of the Gatan OneView camera operating in binning 8 mode, it is substantially shorter than the 40 ms estimated from the frame rate of the OneView camera in normal use (25 fps).

Summary
We improved TEM images acquired using LDE by applying a simple CNN. Our model is based on the U-Net architecture with the ResNet encoder. We demonstrated that enabling the observation of objects that are di icult to visualize in the LDE image because our model can reproduce objects from the noise in addition to removing noise. The position of nanoparticles in the HDE images was reproduced in the corresponding output images, and the size and the edge width of nanoparticles were similar in the HDE and output images. In contrast, their shape reproduction requires further improvement. The time necessary for the image conversion is approximately 8 ms, making the method applicable for in situ observation at a frame rate of 125 fps or lower. Our model is e ective for investigating fast dynamic processes such as nucleation from a solution or tracking the motion of nanoparticles via LDE TEM observation. Acknowledgments This work is supported by JSPS KAKENHI Grant Numbers 20H05657 and 21K03379.
References
Anada, S., Nomura, Y., Hiroyama, T. & Yamamoto, K. (2019). Sparse coding and dictionary learning for electron hologram denoising. Ultramicroscopy 206, 112818.
Anderson, H.S., llic-Helms, J., Rohrer, B., Wheeler, J.,

7

: Fast improvement by deep learning

& Larson, K. (2013). Sparse imaging for fast electron microscopy. Proc. SPIE 8657, Computational Imaging XI 86570C.
Binev, P., Dahmen, W., DeVore, R. & Lamby, P. (2012). Compressed sensing and electron microscopy, Modeling Nanoscale Imaging in Electron Microscopy, Vogt, T., Dahmen, W. & Binev, P. (Eds.), pp. 73­126. New York: Springer.
Chen, C., Chen, Q., Xu, J. & Koltun, V. (2018). Learning to See in the Dark. In 2018 IEEE/CVF Conference on Computer Vision and Pa ern Recognition, pp. 3291­3300.
Chlanda, P. & Sachse, M. (2014). Cryo-electron microscopy of viterous sections. Methods Mol Biol 1117, 193­214.
Dabov, K. Foi, A., Katkovnik, V. & Egiazarian, K. (2007). Image denoising by sparse 3-D transform-domain. 2007 IEEE Trans on Image Process 16, 2080­2095.
Danev, R. & Nagayama, K. (2001). Transmission electron microscopy with Zernike phase plate. Ultramicroscopy 88, 243­ 252.
Faruqi, A.R., Ca ermole, D.M., Mikulec, B. & Raeburn, C. (2003). Evaluation of a hybrid pixel detector for electron microscopy. Ultramicroscopy 94, 263­276.
Schneider, N.M., Norton, M.M, Mendel. B.J., Gorgan, J.M., Ross, F.M. & Bau H.H. (2014). Electron-water interactions and implications for liquid cell electron microscopy. J Phys Chem C 118, 22373­22382.
Gu, S., Zhang, L., Zou, W. & Feng, X. (2014). Weighted nuclear norm minimization with application to image denoising. In 2014 IEEE Conference on Computer Vision and Pa ern Recognition, pp. 2862­2869.
Haider, M., Uhlemann, S., Schwan, E., Rose, H., & Kabius, B. (1998). Electron microscopy image enhanced. Nature 392, 768­769.

Lim, J., Kim, J.-H., Sim, J.-Y., & Kim, C.-S. (2015). Robust contrast enhancement of noisy low-light images: denoisingenhancement completion. 2015 IEEE International Conference on Image Processing (ICIP), pp.4131­4135.
Elad, M. & Aharon, M. (2006). Image denoising via sparse and redundant. 2006 IEEE trans Image Process 15, 3736­3745.
Morishita, S., Ishikawa, R., Kohno, Y., Sawada, H., Shibata, N. &Ikuhara,Y. (2018). Resolution achievement of 40.5 pm in scanning transmission electron microscopy using 300 kV microscope with delta corrector. Microsc. Microanal. 24, 120­ 121.
Fernández-Morán, H. & Dahl, A.O. (1952). Electron microscopy of ultrathin frozen sections of pollen grains. Science 116, 465­467.
Ronneberger, O., Fischer P. & Thomas, B. (2015). U-Net: convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention - MICCAI 2015., Lecture Notes in Computer Science , Vol. 9351. Navab, N., Hornegger, J., Wells, W., Frangi, A. (Eds.), pp.234­241. Springer, Cham.
Shi, W., Caballero, J., Hauzar, F., Totz, J., Aitken, A.P., Bishop, R., Rueckert, D. & Wang, Z. (2016). Real-time single image and video super-resolution using an e icient subpixel convolutional neural network. In 2016 IEEE Conference on Computer Vision and Pa ern Recognition, pp. 1874­1883.
Stevens, A., Gang, H., Carin, L. , Arslan, I. & Brown, N.D. (2014). The potential for Bayesian compressive sensing to significantly reduce electron dose in high-resolution STEM images. Microscopy 63, 41­51.
Yakubovskiy, P. (2020). Segmentation Models: Python library with Neural Networks for Image Segmentation based on PyTorch. GitHub repository, https://github.com/qubvel/segmentation_models.pytorch.

He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pa ern Recognition, pp. 770­778.

Kingma, D.P. & Ba, J. (2015). Adam: A Method for Stochastic Optimization. In Proceedings of the 3rd International Conference on Learning; arXiv 1412.6980v5.

Kisieloski, C., Breitag, B., Bischo , M., van Lin, H., Lazar, S., Knippels, G., Tiemeijer, P., van der Stam, M., von Harrach, S., Stekelenburg, M., Haider, M., Uhlemann, S., Müller, H., Hartel, P., Kabius, B., Miller, D., Petrov, I., Olson, E.A., Donchev, T., Kenik, E.A., Lupini, A.R., Bentley, J., Pennycook, S.J., Anderson, I.M, Minor, A.M., Schmid, A.K., Duden, T., Radmilovic, V., Ramasse, Q.M., Vatanabe, M., Erni, R., Stach, E.A., Dense, P. & Dahmen, U. (2008). Detection of single atoms and buried defects in three dimensions by aberration-corrected electron microscope with 0.5-Åinformation limit. Microsc Microanal 14, 469­477.

8

