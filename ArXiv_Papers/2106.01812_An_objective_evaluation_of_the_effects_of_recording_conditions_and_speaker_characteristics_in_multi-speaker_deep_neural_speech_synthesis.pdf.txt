arXiv:2106.01812v1 [eess.AS] 3 Jun 2021

Available online at www.sciencedirect.com

00 (2021) 000­000

www.elsevier.com/locate/procedia

An objective evaluation of the effects of recording conditions and speaker characteristics in multi-speaker deep neural speech synthesis
Bea´ta Lorincza,b,, Adriana Stana, Mircea Giurgiua
aTechnical University, Communications Department, Cluj-Napoca, Romania bBabes, -Bolyai University, Faculty of Mathematics and Computer Science, Cluj-Napoca, Romania

Abstract
Multi-speaker spoken datasets enable the creation of text-to-speech synthesis (TTS) systems which can output several voice identities. The multi-speaker (MSPK) scenario also enables the use of fewer training samples per speaker. However, in the resulting acoustic model, not all speakers exhibit the same synthetic quality, and some of the voice identities cannot be used at all.
In this paper we evaluate the influence of the recording conditions, speaker gender, and speaker particularities over the quality of the synthesised output of a deep neural TTS architecture, namely Tacotron2. The evaluation is possible due to the use of a large Romanian parallel spoken corpus containing over 81 hours of data. Within this setup, we also evaluate the influence of different types of text representations: orthographic, phonetic, and phonetic extended with syllable boundaries and lexical stress markings.
We evaluate the results of the MSPK system using the objective measures of equal error rate (EER) and word error rate (WER), and also look into the distances between natural and synthesised t-SNE projections of the embeddings computed by an accurate speaker verification network. The results show that there is indeed a large correlation between the recording conditions and the speaker's synthetic voice quality. The speaker gender does not influence the output, and that extending the input text representation with syllable boundaries and lexical stress information does not equally enhance the generated audio across all speaker identities. The visualisation of the t-SNE projections of the natural and synthesised speaker embeddings show that the acoustic model shifts some of the speakers' neural representation, but not all of them. As a result, these speakers have lower performances of the output speech.
© 2021 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/) Peer-review under responsibility of the scientific committee of the KES International.
Keywords: text-to-speech synthesis; multi-speaker; deep learning; speaker characteristics; Romanian.

1. Introduction
Text-to-speech (TTS) synthesis systems have been widely studied in the research community with an increased interest since the appearance of the deep learning based solutions. The deep neural network (DNN) based speech synthesis achieved a naturalness close to that of human speech [1], making it suitable for commercial applications as
 Corresponding author E-mail address: beata.lorincz@ubbcluj.ro
1877-0509 © 2021 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/) Peer-review under responsibility of the scientific committee of the KES International.

Author / 00 (2021) 000­000
well. These TTS systems are now extensively used in our daily lives for virtual assistants, navigation systems or the generation of audio content for any written text. While most TTS systems use a single voice identity, speech corpora consisting of recordings belonging to multiple speakers allows the training of the so called multi-speaker speech synthesis systems (MSPK). This synthesis setup has the advantage of integrating multiple voice identities into a single acoustic model. Some of the first approaches to multi-speaker speech synthesis were based on Hidden Markov Models (HMM) and were built upon average voice models trained on data from multiple speakers and were then adapted to new speaker identities [2]. This methodology requires separate acoustic models, and the quality of the output speech is rather poor [3], even though there are methods that aim to improve the quality degradation in HMM-based speech synthesis [4]. The development of systems supporting multiple voice identities in deep neural synthesis was first approached by adapting the neural network architecture fully or partially to new target speakers [5]. Other studies have proposed the training of a speaker encoder network trained jointly with the TTS model [6]. More recently, the speaker identity is most commonly represented with the help of the so-called speaker embeddings that are learnt during training [7]. These representations of the speaker characteristics learn the relevant speaker information for the task of speech synthesis [8]. Precursors for these representations were the fixed embeddings, such as i-vectors [9] and xvectors [10]. Speaker embeddings are advantageous as the number of parameters in the synthesis system are reduced, and therefore require less data from each speaker. Several studies analyse the utility of speaker embeddings: in [7] different types of speaker embeddings are explored and their feasibility for modelling speaker identity is examined. In [11] the authors report that speaker representations can be learnt by latent variable models of deep Gaussian processes, producing representations that efficiently learn the speaker similarities or dissimilarities. Multi-speaker speech synthesis systems implementing speaker embeddings are presented in [8, 12, 13] with some of the models including very large number of speakers [14].
Other approaches to multi-speaker speech synthesis are meta-learning methods or transfer learning. [15] employs a speaker verification network to train the speaker encoder for multi-speaker speech synthesis. The authors of [6] propose a meta-learning method by training a fast-learner network that can be adapted rapidly to new speakers.
Neural speech synthesis, and specifically multi-speaker synthesis requires large amounts of speech data. Corpora that contain multiple voices are usually composed of data of different styles, various recording conditions and hence of varying quality. Evaluating and selecting subsets of the available data has shown to result in better quality speech output compared with models trained on larger sets of data [16].
Starting from this overview we exploit a Romanian parallel corpus that contains speech recorded under different conditions, and evaluate if particular voice characteristics or recording conditions influence the resulting quality of the respective voice identity in a multi-speaker TTS scenario. We also analyse if different text representations fed to the neural TTS system boost or deteriorate the synthesised speech of the different speakers. The main contributions of our work are: 1) an objective evaluation of speaker characteristics' influence in multi-speaker neural speech synthesis; and 2) an evaluation of three different types of text representations fed as input to the neural network.
The paper is organized as follows: Section 2 describes the neural TTS system, the speech corpus and text representations used in the evaluation. Section 3 shows and discusses the results, while conclusions are drawn in Section 4.
2. Evaluation setup
The Tacotron2 TTS system is currently one of the most natural synthesis systems, with a reported MOS score of 4.53 [1]. Its main advantages are drawn from the use of recurrent structures within the neural architecture and a multihead attention. However, the recurrence also makes it unstable in generating long output sequences and increases the inference speed. Yet, if we want to evaluate the influence of speaker particularities over the performance of a TTS system, it is essential to use an architecture which can­hypothetically­reproduce the naturalness and speaker identity as accurate as possible. We also employ the use of a large parallel spoken corpus consisting of over 81 hours of data collected from 47 speakers. Because the text representation has also been found to affect the quality of the synthesised output [17], we look into different linguistic information present at the input of the TTS data. All three components of our evaluation are presented into more detail in the following subsections.

Author / 00 (2021) 000­000
The implementation of the multi-speaker TTS system relies on the architecture of Tacotron2 [1] neural TTS system. We extended the NVIDIA single-speaker implementation1 to support multi-speaker speech synthesis inspired by the Mellotron [18] implementation.2 In this architecture, the speaker embeddings are appended to the text encoder input, and the resulting hidden features are then used to condition the audio decoder, yielding the output Mel spectrogram. The speaker embeddings are 128-dimensional and are learnt during the training process. The multi-speaker model was trained for 250k steps with a learning rate of 10-3 on batches of size 16.
The conversion of the Mel spectrogram into the audio waveform uses the Waveglow [19] flow-based neural vocoder. The model was pre-trained on a large number of English speakers,3 but did not include the SWARA and SWARA 2.0 speakers.
2.1. Speech corpus
High-quality speech corpora is essential in neural speech synthesis. In this work we start from the large parallel Romanian dataset called SWARA [20]. SWARA contains 17 volunteer speakers each reading aloud between 1000 and 1500 utterances (the same across all speakers) in a controlled studio environment. The total duration of the recordings set is around 21 hours, and was previously successfully applied to train various TTS systems.
The initial SWARA dataset was recently extended with 29 new speakers reading the same utterances, as well as an additional short story. The average number of utterances pertaining to each speaker is 1747, and the total duration of the recordings for all speakers amounts to 59 hours and 39 minutes. Due to the current global pandemic situation, the recordings were performed in the speakers' home environments using the RECOApy tool [21], and were lightly checked for errors by the authors. Some of the issues notices in the recordings refer to the reverberation and background noise presence, as well as some utterances which are chopped either in the beginning or in the end, thus yielding incorrect text-to-audio alignments. We refer to this new set of recordings as SWARA 2.0. All utterances in SWARA and SWARA 2.0 are sampled at 48kHz with 16bps, and were downsampled to 22kHz for the Tacotron2 training.
For the multi-speaker neural TTS evaluation we selected a set of 500 sentences uttered by 37 of the 47 available speakers. The total duration of training data used from the selected speakers tallies up to 28 hours and 57 minutes. An additional set of 12 utterances per speaker were set aside for testing purposes. There are 22 female and 15 male voices4 in the selected set. Out of the total, 9 females and 7 males are part of the SWARA corpus, and the rest pertain to SWARA 2.0. The total duration of the selected speech dataset is approximately 30 hours, with an average of 50 minutes of speech per speaker. Each speaker is given a unique ID which does not infer their identity.
2.2. Text representation
The early versions of end-to-end neural network TTS systems relied mostly on grapheme representations [22, 23]. However, this representation is not suitable for phonetically rich languages [24], and especially for the alignment component of sequence-to-sequence architectures. Moving from graphemes to phonemic representations, and even augmenting the data with additional features, such as lexical stress markings, syllable boundaries or part-of-speech tagging [17, 25] enhances the naturalness of the speech output. We also test this hypothesis in the context of the multispeaker Tacotron2 system, and use three types of text representation as input for the TTS: i) orthographic/graphemic representation GR; ii) phonemic representation PH; and iii) phonemic representation extended with the syllable boundary and lexical stress markings EXT. Examples of the text representations for the three scenarios are presented in Table 1. A simple diagram of the general architecture of our TTS system is represented in Figure 1.
To obtain the phonemic and extended text representations, we rely on a Transformed-based model trained on the RoLEX dataset [26]. RoLEX contains over 330,000 manual entries including information about lemma, part-of-speech tagging, syllabification, lexical stress and phonemic transcription. The Transformer model trained on RoLEX achieved
1 https://github.com/NVIDIA/tacotron2 2 https://github.com/NVIDIA/mellotron 3 https://github.com/NVIDIA/waveglow 4 We note that the male/female classification was performed based on the biological gender of the speakers, and did not require a self-assessment on behalf of the speakers.

Author / 00 (2021) 000­000

Table 1. Text representation examples used for the multi-speaker TTS. Syllable boundaries are marked with a hyphen, and the stress is marked with an apostrophe symbol. The phonemic representations use the SAMPA notation.

Text representation ID

Example

graphemes phonemes phonemes + syllabification + lexical stress

GR PH EXT

Acesta se refera ^insa doar la proprietat,ile din capitala. atSesta se refer@ 1ns@ do Xar la propriet@tsile din kapital@. a-tSe's-ta se re-fe'-r@ 1'n-s@ do Xa'r la pro-pri-e-t@'-tsi-le din kapi-ta'-l@.

grapheme phoneme
phoneme, syllabification, lexical stress

+ Tacotron 2
Speaker embeddings

Fig. 1. Overview of the TTS system flow: the input text is fed in one of the three representations: graphemes, phonemes or phonemes with syllabification and lexical stress. The speaker embeddings are appended to Tacotron2's text encoder and condition the audio decoder module.

a word error rate of 3.08%, where a large number of errors were a result of the incorrect lexical stress assignment. This model is a follow-up work for the concurrent prediction task of the same lexical features using recurrent and convolutional architectures [27].

3. Evaluation results
A first evaluation of the synthesised spoken output was performed using two objective measures: equal error rate (EER) pertaining to speaker similarity, and word error rate (WER) pertaining to speech intelligibility. For both measures we synthesised the 12 test utterances from each speaker within the dataset using the three text input representations for the TTS systems. Audio samples for selected voices are available on our website: https://speech.utcluj.ro/multispeaker_tts_kes2021/. The EER measure was computed by employing the speaker verification (SV) network described in [28].5 This network was trained on 5994 speakers from the Voxceleb dataset [29] and reports an EER of 2.21% for the best performing model. In the EER evaluation we paired the 12 synthesised test utterances from each speaker with natural counterparts from the same speaker and also from the other speakers. This way we obtained a set of 888 pairs of natural vs synthesised neural embedding comparisons. The resulting EER values for each speaker are summarized in Table 2. The table groups the studio vs home recording conditions, and male vs female identity, and also reports the values obtained for the different text representations used as input to the Tacotron2 network. Best results in each column are highlighted in boldface.
The WER calculations used the same set of 12 synthesised sentences per system from each speaker. The synthesised sentences were transcribed using a general purpose high quality automatic speech recognition tool for Romanian [30].
5 We used the implementation and pre-trained models available at: https://github.com/clovaai/voxceleb trainer.

Author / 00 (2021) 000­000

Table 2. EER [%] values for each speaker grouped by recording conditions and gender for each of the three text representations used as input for the TTS system: GR - graphemes; PH - phonemes; and EXT - phonemes plus syllabification and lexical stress markings. The best performing speakers in each column are highlighted with boldface.

Female

Studio recording

Male

Female

Home recording

Male

Speaker GR PH EXT Speaker GR PH EXT Speaker GR PH EXT Speaker GR PH EXT

BAS BEA DCS DDM EME HTM MAR PMM SAM

16.66 8.33 16.66 8.33 8.33 8.33 8.33 16.66 0.00

25.00 16.66 25.00 8.33 8.33 8.33 16.66 16.66 0.00

16.66 0.00 16.66 16.66 8.33 8.33 0.00 16.66 0.00

FDS PCS PSS RMS SDS SGS TSS

16.66 0.00 8.33 0.00 8.33 8.33 16.66

16.66 16.66 0.00 0.00 0.00 0.00 16.66

8.33 0.00 0.00 0.00 16.66 16.66 8.33

BGL BMM CCL CMM DOL GAM GIM GNM MAL MRL OGL PBL SMM

16.66 0.00 8.33 41.66 33.33 50.00 16.66 16.66 16.66 33.33 0.00 16.66 16.66

16.66 8.33 8.33 41.66 16.66 58.33 8.33 16.66 25.00 41.66 0.00 16.66 0.00

16.66 16.66 0.00 41.66 16.66 58.33 16.66 16.66 25.00 33.33 0.00 16.66 0.00

BIM BVL MGL NLL PDL PTL SRL ZPL

8.33 50.00 16.66 16.66 8.33 25.00 25.00 16.66

8.33 41.66 16.66 16.66 16.66 25.00 25.00 8.33

16.66 41.66 16.66 0.00 25.00 16.66 16.66 16.66

Table 3. WER [%] values for each speaker grouped by recording conditions and gender for each of the three text representations used as input for the TTS system: GR - graphemes; PH - phonemes; and EXT - phonemes plus syllabification and lexical stress markings. The best performing speaker in each column is highlighted with boldface.

Female

Studio recording

Male

Female

Home recording

Male

Speaker GR PH EXT Speaker GR PH EXT Speaker GR PH EXT Speaker GR PH EXT

BAS BEA DCS DDM EME HTM MAR PMM SAM

19.67 12.94 13.75 15.36 14.32 12.56 8.85 11.40 9.69

19.76 16.84 21.47 16.42 13.14 19.66 12.67 10.96 11.05

31.71 10.13 14.76 15.59 17.22 24.64 16.21 17.91 17.07

FDS PCS PSS RMS SDS SGS TSS

16.98 36.17 14.84 10.61 22.05 29.95 20.91

16.86 28.47 14.64 12.21 13.83 20.19 14.29

16.26 30.55 20.20 10.13 21.93 21.47 14.76

BGL BMM CCL CMM DOL GAM GIM GNM MAL MRL OGL PBL SMM

41.45 20.82 30.32 20.54 19.82 30.19 34.74 31.23 20.89 37.21 26.54 67.93 23.47

51.99 20.68 26.22 23.78 12.81 34.72 30.31 30.21 19.16 33.55 17.27 71.94 21.40

42.40 25.29 33.86 20.54 11.05 38.69 22.02 33.58 18.00 14.87 30.38 42.84 30.75

BIM BVL MGL NLL PDL PTL SRL ZPL

18.69 83.96 16.52 14.76 54.41 25.78 46.64 26.15

14.99 89.23 20.73 18.81 37.85 18.00 26.64 27.30

20.57 98.80 16.10 17.10 54.00 34.96 42.40 26.24

Table 4. Average EER[%] and WER[%] values across the different evaluation criteria.

GR: 15.76

PH: 15.98

EXT: 14.63

EER

Studio recording: 9.54

Female: 11.10

Male: 7.53

Home recording: 19.96

Female: 20.08

Male: 19.78

GR PH EXT GR PH EXT GR PH EXT GR PH EXT

10.18 13.88 9.25 8.33 7.14 7.14 20.50 18.86 19.86 20.83 19.78 18.74

WER

GR: 26.00

PH: 24.59

EXT: 26.35

Studio recording: 17.35

Home recording: 31.96

Female: 15.76

Male: 19.39

Female: 29.83

Male: 35.44

GR PH EXT GR PH EXT GR PH EXT GR PH EXT

13.17 15.77 18.36 21.64 17.21 19.32 31.16 30.31 28.02 35.86 31.69 38.77

The ASR's WER on the natural test samples is 10.45%. The WER values for each speaker, recording condition, speaker's gender and text input representation are shown in Table 3.
The EER and WER values averaged across all the different evaluation criteria are shown in Table 4. At first inspection of the results in Table 4 it can be noticed that the recording conditions have the most impact over the quality of the resulting voice both in terms of EER and WER. The studio recorded speakers exhibit on average a 50% relative improvement over the EER and WER. With respect to the speaker's gender, for the studio recordings, the male voices performed better in terms of EER, and the female voices in terms of WER. For the home recordings, the female voices perform slightly better than the male voices in the WER evaluation. With respect to the text representation, the EXT

Author / 00 (2021) 000­000

(a) Average t-SNE projections for natural samples' embeddings
BGL
30

DDM

20

BAS

DCS

OGL

MAR MAL

HTM

CCL

10

GAM

DOL EMEMRL

BMM

PMM SMM

GIM

CMM GNM

PBL

SAM

(b) Average t-SNE projections for GR samples' embeddings

BGL

CCL

30

EME

BAS

DDM

MAL

20

MAR

DCS DOL

OGL GIM PBL

PMMMRL HTM

SMM

BMM

CMM

GNM

10

SAM

0

BEA

0

BEA

10

BVL PSS

RMS

20

FDS SDS

PTL

SRPLDL

MZPGLL TSSNLL SGS

PCS

BIM

10

20

SRL

PDL

PTL

30 PCS

MGL NLL

ZPL

BIM

SGS

40 FDS

PSS RMS TSSSDS

GAM BVL

(c) Avera1g0 e t-SNE proj5ections for PH0 samples' em5 beddings 10

BGL

CCL

30

EME

BAS

DDM

GIM

20

MMARRL

MAL DOLDCS

PMM HTM

SMM

OGL

PBL BMM
CMM

GNM
10

SAM

0

BEA

(d20) Average t-S1N0 E projection0s for EXT sam10ples' embedd20ings

BGL CCL

30

EME

DDM

MRL MAL

BAS

GIM

20

MAR

DCS DOL

OGL PBL

SMM

BMM

PMM HTM

GNM

CMM

10

SAM

0

BEA

10

PSS

RMS

20

PDL

SRL TSSSDS

PTL

NLL

30 PCS

MGLZPL BIM

SGS

40 FDS

GAM BVL

10

20

SRL

PPDTLL

30 PCS

MGL NLL

ZPL

BIM

SGS

40 FDS

PSS
RMS TSS SDS

GAM BVL

20

10

0

10

20

20

10

0

10

20

Fig. 2. t-SNE visualizations of the average speaker embedding for each speaker as obtained from the natural samples, and from the three TTS systems which use the different text representations (i.e. GR - graphemes, PH - phonemes, EXT - phonemes plus syllabification and lexical stress). The 'x' marks female speakers, and 'o' marks the male speakers.

transcription yields the best results for speaker similarity (EER), and the PH transcription for intelligibility (WER). But the differences between the systems are not statistically significant. This means that on average the text representation is not a determinant factor in the synthesis quality. The lowest WER (13.17%) is achieved by the grapheme-based system with female voices.6 The fact that the EXT text representation does not perform better than the PH system could be interpreted as the fact that although the representation offers more information regarding the exact pronunciation of the text input, the network is not able to learn this additional correspondence.
6 We should note here that Romanian has rather simple letter-to-sound rules, and that these results may not be extended to more phonetically complex languages, such as English.

Author / 00 (2021) 000­000

40

(a) Studio recording - female speakers

35

30

25

20

15

10

5

0 GR BPAHS EXT GR BPEHA EXT GR DPCHS EXT GR DPDHM EXT GR EPMHE EXT GR HPTHM EXT GR MPAHR EXT GR PPMHM EXT GR SPAHM EXT

40

(b) Studio recording - male speakers

35

30

25

20

15

10

5

0 GR FPDHS EXT GR PPCHS EXT GR PPSHS EXT GR RPMHS EXT GR SPDHS EXT GR SPGHS EXT GR TPSHS EXT

40

(c) Home recording - female speakers

35

30 25

20

15 10 5

0 GR BPGHL EXT GR BPMHM EXT GR CPCHL EXT GR CPMHM EXT GR DPOHL EXT GR GPAHM EXT GR GPIHM EXT GR GPNHM EXT GR MPAHL EXT GR MPRHL EXT GR OPGHL EXT GR PPBHL EXT GR SPMHM EXT

40

(d) Home recording - male speakers

35

30

25

20

15

10

5

0 GR BPIHM EXT GR BPVHL EXT GR MPGHL EXT GR NPLHL EXT GR PPDHL EXT GR PPTHL EXT GR SPRHL EXT GR ZPPHL EXT

Fig. 3. Boxplots of the Euclidean distances calculated between the 2D t-SNE projections of the SV pairs of embeddings from natural and corresponding synthesised utterance. The plots are grouped by recording conditions, speaker gender and text input representations. Lower values are better.

Author / 00 (2021) 000­000
Breaking the above results at speaker level, we can notice some interesting facts. One speaker stands out in both the EER and WER evaluations, namely BVL. Its error rates are extremely high (EER : 44.44% , WER : 90.66%) compared to all the other speakers. By inspecting its recordings more thoroughly, we discovered that the speaker consistently trimmed the beginning and ends of the utterances. This makes them unusable for the current purpose, or any other application where the correct speech-to-text alignment is required. Therefore, we removed the speaker from the SWARA 2.0 dataset.
There are 3 voices which have perfect EER values: SAM, RMS and OGL across all text representation systems. RMS also performs best in terms of WER within its group (i.e. male speakers recorded in a studio environment), but it is not the case for SAM and OGL. Because the absolute error rates can be hard to interpret as is, we also examine the individual speaker embeddings. For each speaker we passed the 12 natural and 36 synthesised test samples through the SV network and obtained the associated speaker embeddings. These embeddings were then projected into a 2D space using t-SNE [31]. Figure 2 plots the t-SNE visualisation of the average speaker embedding for each speaker as extracted from the natural or synthesised samples. The t-SNE projections were computed from all samples of all speakers and TTS systems so that the coordinates are directly comparable across the plots. It can be noticed that there is a rather clear separation between the male (marked with an 'o') and female (marked with a 'x') embeddings. Also, the embeddings of the synthesised samples (Figure 2(b-d)) are slightly shifted from the natural ones (Figure 2(a)), yet there are only minor shifts in between the synthesised samples' embeddings. To evaluate the shift of the synthesised samples versus the natural ones, we compute the Euclidean distance between each of the 12 natural test samples and their corresponding synthesised ones, and plot their statistics in Figure 3. The plots should be interpreted as how much does each speaker's synthesised output differ in terms of speaker identity with respect to its natural samples. The higher the values, the lower the speaker similarity. With very few exceptions, the distances between the natural and synthesised samples are consistent across the three TTS systems, and have rather tight distributions. For the studio recordings, there is a correlation between these distances and the EER values obtained by each of the 16 speakers. The Pearson correlation coefficient (PCC) for these values is 0.4. However, it is interesting to notice that this correlation is not preserved in the home recordings group of speakers (PCC=0.05). For example, speaker SRL has some of the lowest distances, yet its average EER and WER values are among the highest. On the other hand, speaker CCL exhibits very high distances, yet its EER is at most 8.33% for all systems. Also, there is no correlation between the EER and WER values for the studio conditions (PCC=-0.01), but the home recordings' EER and WER are somewhat correlated, having a PCC of 0.35.
4. Conclusions
In this paper we performed an elaborate analysis and evaluation of the influence of speaker characteristics, recording conditions and text representations over the performance of a multi-speaker DNN-based text-to-speech synthesis system, namely the Tacotron2 architecture. We looked into the EER and WER values for each individual speaker, and plotted the distances between the embeddings of natural and synthesised samples as obtained from an accurate speaker verification network.
The results showed that a major role in the output of the synthesis is played by the availability of studio-quality recordings. All speakers which were recorded using their home setups had higher EER and WER rates. The text representation fed to the neural TTS system did not seem to influence the overall objective quality of the voices. With respect to the individual speakers there seem to be a few voice identities which are best suited for this TTS system, at least. It would be interesting to analyse their adequacy for other types of systems in order to draw definite conclusions regarding their spectral and prosodic characteristics. Another conclusion drawn from these experiments is the potential use of objective measures in order to asses the quality of the training data. Such as it was the case of the BVL speaker which at the first check seemed to have recorded the sentences correctly. Yet at the evaluation, the high EER and WER values revealed that the speaker consistently trimmed the start and end of the utterances. This deemed a set of recordings which were not truly aligned with the transcript, resulting in the erroneous synthesis output.
Within this evaluation we did not perform any subjective listening tests, as the number of comparisons and samples needed to be evaluated would have made it unfeasible for a comfortable listener evaluation. However, we do plan to perform a series of further analyses of these results and select a subset of systems and speakers for a subjective evaluation. As future work we would like to investigate even further how much does the voice identity truly influence

Author / 00 (2021) 000­000
the results of a TTS system: is it just a matter of chance, or are there any definite spectral and prosodic characteristics which influence the output. There is also the issue of augmenting the data of the less performing speakers, and how can a TTS system overcome the potential issues arising within the provided recordings.
Acknowledgements
This work was supported by a grant of the Romanian Ministry of Research and Innovation, PCCDI ­ UEFISCDI, project number PN-III-P1-1.2-PCCDI-2017-0818/73, within PNCDI III.
References
[1] J. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang, Z. Chen, Y. Zhang, Y. Wang, R. Skerrv-Ryan, et al., Natural TTS Synthesis by Conditioning WaveNet on MEL Spectrogram Predictions, in: 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2018, pp. 4779­4783.
[2] J. Yamagishi, T. Nose, H. Zen, Z.-H. Ling, T. Toda, K. Tokuda, S. King, S. Renals, Robust speaker-adaptive HMM-based text-to-speech synthesis, IEEE Transactions on Audio, Speech, and Language Processing 17 (6) (2009) 1208­1230.
[3] O. Watts, G. E. Henter, T. Merritt, Z. Wu, S. King, From HMMs to DNNs: where do the improvements come from?, in: 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2016, pp. 5505­5509.
[4] G. Biagetti, P. Crippa, L. Falaschetti, C. Turchetti, Hmm speech synthesis based on mdct representation, International Journal of Speech Technology 21 (4) (2018) 1045­1055.
[5] Z. Kons, S. Shechtman, A. Sorin, C. Rabinovitz, R. Hoory, High quality, lightweight and adaptable TTS using LPCNet, arXiv preprint arXiv:1905.00590 (2019).
[6] Y. Chen, Y. Assael, B. Shillingford, D. Budden, S. Reed, H. Zen, Q. Wang, L. C. Cobo, A. Trask, B. Laurie, et al., Sample efficient adaptive text-to-speech, arXiv preprint arXiv:1809.10460 (2018).
[7] E. Cooper, C.-I. Lai, Y. Yasuda, F. Fang, X. Wang, N. Chen, J. Yamagishi, Zero-shot multi-speaker text-to-speech with state-of-the-art neural speaker embeddings, in: ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2020, pp. 6184­6188.
[8] A. Gibiansky, S. O¨ . Arik, G. F. Diamos, J. Miller, K. Peng, W. Ping, J. Raiman, Y. Zhou, Deep Voice 2: Multi-Speaker Neural Text-to-Speech., in: NIPS, 2017.
[9] P. Cardinal, N. Dehak, Y. Zhang, J. Glass, Speaker adaptation using the i-vector technique for bottleneck features, in: Sixteenth Annual Conference of the International Speech Communication Association, 2015.
[10] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, S. Khudanpur, X-vectors: Robust DNN embeddings for speaker recognition, in: 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2018, pp. 5329­5333.
[11] K. Mitsui, T. Koriyama, H. Saruwatari, Multi-speaker Text-to-speech Synthesis Using Deep Gaussian Processes, arXiv preprint arXiv:2008.02950 (2020).
[12] J. Park, K. Zhao, K. Peng, W. Ping, Multi-speaker end-to-end speech synthesis, arXiv preprint arXiv:1907.04462 (2019). [13] M. Chen, X. Tan, Y. Ren, J. Xu, H. Sun, S. Zhao, T. Qin, MultiSpeech: Multi-speaker text to speech with transformer, arXiv preprint
arXiv:2006.04664 (2020). [14] W. Ping, K. Peng, A. Gibiansky, S. O. Arik, A. Kannan, S. Narang, J. Raiman, J. Miller, Deep voice 3: 2000-speaker neural text-to-speech,
Proc. ICLR (2018) 214­217. [15] Y. Jia, Y. Zhang, R. J. Weiss, Q. Wang, J. Shen, F. Ren, Z. Chen, P. Nguyen, R. Pang, I. L. Moreno, et al., Transfer learning from speaker
verification to multispeaker text-to-speech synthesis, arXiv preprint arXiv:1806.04558 (2018). [16] P. O. Gallegos, J. Williams, J. Rownicka, S. King, An Unsupervised Method to Select a Speaker Subset from Large Multi-Speaker Speech
Synthesis Datasets, Proc. Interspeech 2020 (2020) 1758­1762. [17] A` . Peiro´ Lilja, M. Farru´s, Naturalness enhancement with linguistic information in end-to-end TTS using unsupervised parallel encoding,
Proceedings of Interspeech 2020; 2020 Oct 25-29; Shanghai, China.[Baixas]: ISCA; 2020. (2020). [18] R. Valle, J. Li, R. Prenger, B. Catanzaro, Mellotron: Multispeaker expressive voice synthesis by conditioning on rhythm, pitch and global
style tokens, in: ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2020, pp. 6189­6193. [19] R. Prenger, R. Valle, B. Catanzaro, Waveglow: A flow-based generative network for speech synthesis, in: ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2019, pp. 3617­3621. [20] A. Stan, F. Dinescu, C. T¸ iple, S¸ . Meza, B. Orza, M. Chirila, M. Giurgiu, The SWARA speech corpus: A large parallel Romanian read speech dataset, in: 2017 International Conference on Speech Technology and Human-Computer Dialogue (SpeD), IEEE, 2017, pp. 1­6. [21] A. Stan, RECOApy: Data recording, pre-processing and phonetic transcription for end-to-end speech-based applications, 2020. [22] J. Sotelo, S. Mehri, K. Kumar, J. F. Santos, K. Kastner, A. Courville, Y. Bengio, Char2wav: End-to-end speech synthesis (2017). [23] Y. Wang, R. Skerry-Ryan, D. Stanton, Y. Wu, R. J. Weiss, N. Jaitly, Z. Yang, Y. Xiao, Z. Chen, S. Bengio, et al., Tacotron: Towards end-to-end speech synthesis, arXiv preprint arXiv:1703.10135 (2017). [24] K. Vythelingum, Y. Este`ve, O. Rosec, Acoustic-dependent Phonemic Transcription for Text-to-speech Synthesis., in: Interspeech, 2018, pp. 2489­2493.

Author / 00 (2021) 000­000
[25] J. Taylor, K. Richmond, Enhancing Sequence-to-Sequence Text-to-Speech with Morphology, Submitted to IEEE ICASSP (2020). [26] B. Lorincz, E. Irimia, A. Stan, V. Barbu-Mititelu, RoLEX: The development of an extended Romanian lexical dataset and its evaluation at
predicting concurrent lexical information, Computer Speech and Language (2021). [27] B. Lorincz, Concurrent phonetic transcription, lexical stress assignment and syllabification with deep neural networks, Procedia Computer
Science 176 (2020) 108­117. [28] J. S. Chung, J. Huh, S. Mun, M. Lee, H. S. Heo, S. Choe, C. Ham, S. Jung, B.-J. Lee, I. Han, In defence of metric learning for speaker
recognition, in: Interspeech, 2020. [29] A. Nagrani, J. S. Chung, A. Zisserman, Voxceleb: a large-scale speaker identification dataset, arXiv preprint arXiv:1706.08612 (2017). [30] A.-L. Georgescu, H. Cucu, C. Burileanu, Kaldi-based DNN architectures for speech recognition in Romanian, in: 2019 International Confer-
ence on Speech Technology and Human-Computer Dialogue (SpeD), IEEE, 2019, pp. 1­6. [31] L. Van der Maaten, G. Hinton, Visualizing data using t-SNE., Journal of machine learning research 9 (11) (2008).

