Testing Group Fairness via Optimal Transport Projections

arXiv:2106.01070v1 [stat.ML] 2 Jun 2021

Nian Si1 Karthyek Murthy 2 Jose Blanchet1 Viet Anh Nguyen1 3

Abstract
We present a statistical testing framework to detect if a given machine learning classifier fails to satisfy a wide range of group fairness notions. The proposed test is a flexible, interpretable, and statistically rigorous tool for auditing whether exhibited biases are intrinsic to the algorithm or due to the randomness in the data. The statistical challenges, which may arise from multiple impact criteria that define group fairness and which are discontinuous on model parameters, are conveniently tackled by projecting the empirical measure onto the set of group-fair probability models using optimal transport. This statistic is efficiently computed using linear programming and its asymptotic distribution is explicitly obtained. The proposed framework can also be used to test for testing composite fairness hypotheses and fairness with multiple sensitive attributes. The optimal transport testing formulation improves interpretability by characterizing the minimal covariate perturbations that eliminate the bias observed in the audit.
1. Introduction
Algorithmic decisions are commonly conceived to have the potential of being more objective than a human's decisions, since they are generated by logical instructions and the rules of algebra. However, recent studies indicate that this may not be the case. For example, an algorithm which helps the US criminal justice system to predict recidivism rates has been shown to falsely give a higher risk for African-Americans than white Americans (Chouldechova, 2017; MultiMedia LLC, 2016). Similar biases are exhibited against female candidates in a hiring-help system developed by Amazon AI (Dastin, 2018) and an ad-targeting algorithm used by Google (Datta et al., 2015).
1Department of Management Science & Engineering, Stanford University 2Engineering Systems and Design pillar, Singapore University of Technology and Design 3VinAI Research, Vietnam. Correspondence to: Nian Si <niansi@stanford.edu>.
Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s).

A natural first explanation for the reported algorithmic biases is that the data used to train the algorithms may already be corrupted by human biases (Buolamwini & Gebru, 2018; Manrai et al., 2016). Deeper inquests have revealed insights on how common learning procedures intrinsically perpetuate the biases and potentially introduce fresh ones. The usual practice of training by minimizing empirical risk, while geared towards yielding predictions that are best when averaged over the entire population, often under-represents minority subgroups in the datasets. Moreover, even though certain sensitive attributes are forbidden by law to be used in the algorithm, the strong correlations between the sensitive attributes and other features potentially lead to biases in predictions. As reported in the studies in (Grgic-Hlaca et al., 2016; Garg et al., 2019; Barocas & Selbst, 2016; Black et al., 2020; Kleinberg et al., 2018; Lipton et al., 2018), merely masking the sensitive attributes does not address the problem.
The aforementioned biases and their impacts have sparked substantial interests in the pursuit of algorithmic fairness (Berk et al., 2018; Chouldechova & Roth, 2020; Corbett-Davies et al., 2017; Mehrabi et al., 2019). Testing whether a given machine learning algorithm is fair emerges as a question of first-order importance. In turn, designing this test for a wide range of group fairness notions (discussed in the sequel) is the main task of this paper.
Our proposed statistical hypothesis testing framework (testing framework for short) allows the auditors to systematically determine whether the biases exhibited in the audit procedure, if any, are intrinsic to the algorithm or due to the randomness in data. Moreover, our framework can be implemented as a black-box, without knowing the exact structure of the classification algorithm used.
For settings where sensitive attributes are not explicitly used as input to classification, fairness is measured on impact either at a group level or at an individual level (Barocas & Selbst, 2016). Group fairness notions seek to measure the differences in impacts across different groups and constitute the prominent means of assessing discrimination associated with group memberships. Individual fairness, on the other hand, seeks to assess if similar users are treated similarly (Dwork et al.,

Testing Group Fairness via Optimal Transport Projections

2012; John et al., 2020; Xue et al., 2020). Common examples of group fairness notions include disparate impact (Zafar et al., 2017), demographic parity (statistical parity) (Calders & Verwer, 2010), equality of opportunity (Hardt et al., 2016), equalized odds (Hardt et al., 2016), etc. The specific choice is usually driven by the philosophical, sociological or legal constraints binding the application considered.
Our testing framework applies to a generic notion of group fairness which encompasses all of the above specific group fairness notions as examples. This unifying approach can also be used in contexts requiring the use of different fairness notions simultaneously and settings with multiple groups.
Since a single fairness criterion among two groups can be reduced to testing the equality in two sample conditional means, one may consider employing a Welch's t-test or a permutation test. Further, a suitable adjustment of the randomness in sample sizes, as in DiCiccio et al. (2020); Tramer et al. (2017) can be applied. Some other existing methods such as Besse et al. (2018) also only apply to onedimensional criterion. Extensions to multiple impact criteria are not immediate, as is the equalized odds case criterion (Hardt et al., 2016), or in the presence of multiple groups. Algorithmic approaches, such as in (Saleiro et al., 2018), lack the control of the type-I (false positive error). In contrast, the framework proposed here is applicable under general multiple impact criteria and controls the type-I error exactly.
The statistical challenges, which may arise from the presence of multiple impact criteria, are conveniently handled in our framework by utilizing the machinery of optimal transport projections. This involves computing the test statistic by projecting, or in other words, optimally transporting the empirical measure to the set of probability models which satisfy the group fairness notion (or notions) considered. This gives a measure of plausibility of the classifier in satisfying the fairness criterion under the datagenerating distribution and the fairness hypothesis is duly rejected if the test statistic exceeds a suitable threshold determined by the significance level. This threshold is determined from the limiting distribution of the test statistic obtained as one of the main results of this paper.
Performing statistical inference with a projection criterion is prevalent in statistics: Owen (2001) serves as a comprehensive reference for projections, or profile functions, that are computed based on likelihood ratio metrics or the Kullback-Liebler divergence. Blanchet et al. (2019) and Cisneros-Velarde et al. (2020) study statistical inference with optimal transport projections. Recently, optimal transport divergences (Villani, 2008) become an attractive tool in many recent machine learning studies, including

missing data imputation, geodesic PCA, point embeddings, and repairing data with Wasserstein barycenters for training fair classifiers (Silvia et al., 2020; Gordaliza et al., 2019; Zehlike et al., 2020).
Taskesen et al. (2021) uses optimal transport projections to test a smooth relaxation of the equal opportunity criterion called probabilistic fairness; see Pleiss et al. (2017). This relaxation is required in Taskesen et al. (2021) to overcome the discontinuities in the classification boundaries which create technical complications when computing the optimal transport projections. Further, the resulting test statistic involves a non-convex optimization problem which is difficult to compute. In contrast, our work resolves the technical challenges arising from the discontinuities in classification boundaries. Moreover, our test statistic is the optimal value of a linear program, whose optimal solution offers interpretability by characterizing the minimal covariate perturbations that eliminate the bias observed in the audit. We emphasize that addressing boundary discontinuities is not simply a technical improvement. As we discuss in Section 3.3, our results show different qualitative behaviors both in the scaling and the interpretation of the optimal transport projection in terms of group fairness using optimal transport projections. In addition to enabling exact, computationally tractable, and interpretable fairness assessment for general deterministic classifiers, the technical analysis serves as a stepping stone for statistical inference in estimation tasks such as quantile regression which involve discontinuous estimating equations.
The main contributions are summarized as follows:
(1) We develop a statistical hypothesis test for assessing group fairness as per a generic notion that includes commonly used fairness criteria as special cases. The test is computationally tractable and interpretable. Besides being applicable to settings involving multiple groups, our framework is also applicable to any classifier algorithms, including but not limited to the logistic regression, SVMs, kernel methods, and nearest neighbors.
(2) We develop an extension of the statistical test for the testing problem with composite null hypothesis, addressed here as -fairness.
(3) The framework facilitates the exact use of fairness criterion, thus obviating the need to invoke relaxations in the absence of smoothness in the impact criteria defining group fairness. The resulting qualitative difference, in terms of the rate of convergence for resulting optimal transport projections, is previously unreported and could be of interest from the technical standpoint of analysing profile functions with discontinuous score functions.
The remainder of the paper is structured as follows. In Section 2, we introduce a generic notion of group fairness and

Testing Group Fairness via Optimal Transport Projections

discuss the theory of optimal transport. Section 3 details the proposed statistical test for the simple fairness null hypothesis. Section 4 extends our approach to composite hypotheses. Section 5 discusses computational methods associated with the test. Numerical experiments presented in Section 6 serve to demonstrate the efficacy of the test. All technical proofs are relegated to Appendix A.
Notations. We use ·  to denote the dual norm of · . We denote (x)- = min {x, 0} and (x)+ = max{x, 0}. We use , -p and -a.s. to denote convergence in distribution, in probability and convergence almost surely, respectively. The support of the distribution of X is represented by supp(X). We use [n] to denote the set {1, 2, . . . , n} and Rm + to denote the positive orthant {x  Rm + : x  0}. (x,a,y) denotes a Dirac measure on a fixed point (x, a, y).
2. Problem Setup and Preliminaries

Note that C(X) = I{C(X) = 1}, and thus equation (1) can be seen as EQ[I{C(X) = 1}(U, EQ[U ])] = 0. At the first glance, equation (1) seems asymmetric as it only considers the positive prediction label C(X) = 1. However, it is easy to check that EQ[(U, EQ[U ])] = 0 in all the group fairness notions in Examples 1 - 6. By taking the difference, we get the symmetric guarantee that EQ[I{C(X) = 0}(U, EQ[U ])] = 0.
Various useful notions of fairness can be obtained by varying the choice of (U, ) as illustrated in Examples 1 - 6 below. We take A = {0, 1} in Examples 1 - 4.
Example 1 (Equal opportunity (Hardt et al., 2016)). A classifier C : X  {0, 1} satisfies the equal opportunity criterion relative to a distribution Q if
Q (C(X) = 1|A = 1, Y = 1) - Q (C(X) = 1|A = 0, Y = 1) = 0.

Throughout this paper we consider the classification settings in which the deterministic classifier C : X  Y maps the input features from X  Rd to output labels in the set Y = {0, 1}. Evaluation of fairness is considered with respect to a sensitive attribute A taking values in a finite set A. For simplicity, we consider A = {0, 1} where A = 1 can be taken to identify the reference group. The statistical test developed in this paper and the main results are applicable more generally to settings involving a non-binary sensitive attribute (or) multiple sensitive attributes. Most notions of group fairness are stated in terms of the joint distribution Q of (X, A, Y ), where X is the vector of input features, A is the sensitive attribute, and Y is the class label of a random sample from the population.
2.1. Notions of Group Fairness
A general reference to fairness notions can be found in Makhlouf et al. (2020, Table 14). The statistical notion of group fairness that we consider, encapsulated in Definition 1 below, is stated flexibly to include commonly used notions such as equal opportunity (Hardt et al., 2016), predictive equality (Corbett-Davies et al., 2017), equalized odds (Hardt et al., 2016), and statistical parity (Dwork et al., 2012), etc., as special cases. This flexibility is achieved by stating the definition in terms of a tuple (U, ), where U is an Rs-valued random vector completely dependent on (A, Y ) and  : Rs × Rs  Rm is a function chosen to discern the differences in performance of the classifier across groups. We address (U, ) as the discerning tuple.
Definition 1 (Generic notion of group fairness). A classifier C : X  {0, 1} is fair with respect to the discerning tuple (U, ) under a probability distribution Q if

EQ[C(X)(U, EQ[U ])] = 0.

(1)

This criterion coincides with condition (1) with the choice
U = (I(1,1)(A, Y ); I(0,1)(A, Y )), where I(a,y)(A, Y ) denotes the indicator I(A = a, Y = y), and



:

(U, EQ[U ])



U1 EQ[U1]

-

U2 EQ[U2]

.

(2)

Example 2 (Predictive equality (Corbett-Davies et al., 2017)). A classifier C : X  {0, 1} satisfies the predictive equality criterion relative to a distribution Q if

Q (C(X) = 1|A = 0, Y = 0) - Q (C(X) = 1|A = 1, Y = 0) = 0.

This criterion coincides with condition (1) with the choice U = [I(1,0)(A, Y ); I(0,0)(A, Y )] and  takes the same form as the function (2).
Example 3 (Equalized odds (Hardt et al., 2016)). A classifier C : X  {0, 1} satisfies the equalized odds criterion relative to a distribution Q if it satisfies both equal opportunity and predictive equality criteria. This criterion coincides with condition (1) with U = [I(1,1)(A, Y ); I(0,1)(A, Y ); I(1,0)(A, Y ); I(0,0)(A, Y )] and

 : (U, EQ[U ]) 

U1 EQ[U1]

-

U2 EQ[U2]

;

U3 EQ[U3]

-

U4 EQ[U4

]

.

Example 4 (Statistical parity (Dwork et al., 2012)). A classifier C : X  {0, 1} satisfies the statistical parity criterion relative to a distribution Q if

Q (C(X) = 1|A = 1) - Q (C(X) = 1|A = 0) = 0.

This criterion coincides with condition (1) with the choice U = [I1(A); I0(A)] and  takes the same form as the function (2).

Testing Group Fairness via Optimal Transport Projections

If the sensitive attribute takes multiple values or there are multiple sensitive attributes, we can still define the associated fairness notions, which correspond to different choices of (U, ).
Example 5 (Equal opportunity with a non-binary sensitive attribute). Let A = {0, 1, 2, . . . , k}. A classifier C : X  {0, 1} satisfies the equal opportunity criterion relative to a probability measure Q if

Q (C(X) = 1|A = t, Y = 1) - Q (C(X) = 1|A = 0, Y = 1) = 0 t  A\{0}.

This criterion coincides with condition (1) with the choice
U = (I(0,1)(A, Y ); I(1,1)(A, Y ); . . . I(k,1)(A, Y )) and  = (1, 2, . . . , t) with

t

:

(U, EQ[U ])



Ut EQ[Ut]

-

U1 EQ[U1]

t  A\{0}.

If c(·, ·) is a metric on Z, then Wc(·) is the type-1 Wasserstein distance; see Villani (2008, Chapter 6). The quantity Wc(Q1, Q2) can be interpreted as the least transportation cost incurred in transporting mass from Q1 to Q2, when the cost of transporting unit mass from location z  Z to location z  Z is given by c(z, z).
Throughout the paper, we assume that the function c is decomposable as

c ((x, a, y), (x, a, y)) = c¯(x, x) +  · |a - a| +  · |y - y|,

for some c¯ : X × X  [0, ] satisfying (i) c¯(x, x) = 0 if and only if x = x and (ii) c¯(x, x) = c¯(x, x) for all x, x  X . In the above expression, we interpret ×0 = 0.
Examples of c¯(·, ·) that are useful in our context include

c¯(x, x) = x - x ,

(3a)

Example 6 (Equal opportunity with multiple sensitive
attributes). Suppose we have K sensitive attributes, A1, A2, . . . , AK , all taking values in a superset A. A classifier C : X  {0, 1} satisfies the equal opportunity criterion relative to a probability measure Q if

Q (C(X) = 1|At = 1, Y = 1) - Q (C(X) = 1|At = 0, Y = 1) = 0 t  [K].

This criterion coincides with condition (1) with the choice

Ut = I(1,1)(At, Y ) and Ut+K = I(0,1)(At, Y ) t  [K]

and  = (1, 2, . . . , t) with

t

:

(U, EQ[U ])



Ut EQ[Ut]

-

Ut+K EQ[Ut+K ]

t  [K].

2.2. Optimal Transport and the Wasserstein Distance
We next introduce the notion of optimal transport costs, of which Wasserstein distances is a special case. Let P (Z) denote the set of all probability distributions on Z X × A × Y.
Definition 2 (Optimal transport costs, Wasserstein distances). Given a lower semicontinuous function c : Z × Z  [0, ], the optimal transportation cost Wc(Q1, Q2) between any two distributions Q1, Q2  P (Z) is given by,
Wc(Q1, Q2) = min E [c (Z, Z)] ,
(Q1 ,Q2 )
where (Q1, Q2) is the set of all joint distributions of (Z, Z) such that the law of Z = (X, A, Y ) is Q1 and that of Z = (X, A, Y ) is Q2.

and also

c¯(x, x) = k(x, x) - 2k(x, x) + k(x, x), (3b)

where k : X × X  R is a suitable reproducing kernel. Another useful example of c¯(·) is specified in terms of the
discrete metric suitable for use in the presence of discrete categorical features: Suppose that the feature vector X = (XD, XC ), with XD denoting the set of discrete features taking values in a countable set D  Rd1 and XC denoting the set of continuous features taking values in Rd2. We have d1 + d2 = d. In this instance, it is feasible to restrict the transportation to elements in D × Rd2 by considering

c¯((xD, xC ), (xD, xC ))

= xC - xC + I{{xD, xD}  D, xD = xD}

+  · I{{xD, xD} D, xD = xD},

(4)

for some  > 0. Further, we allow the cost function to be dependent on the sensitive attribute. Following the same line, Hui et al. (2021) recently demonstrates a test power gain by tuning properly a sensitive-attributedependent transportation cost function.

3. Test For Simple Null Hypothesis via Optimal Transport
Recall that P (Z) denotes the set of all probability distributions on Z X × A × Y. Let
F = {Q  P (Z) : EQ[C(X)(U, EQ[U ])] = 0}
be the collection of distributions under which the classifier C(·) is fair, as deemed by Definition 1. Given N independent samples {xi, ai, yi}Ni=1 from a distribution P of

Testing Group Fairness via Optimal Transport Projections

(X, A, Y ), we are interested in the statistical test with the hypotheses

H0 : P  F against H1 : P  F .

With the null hypothesis H0 being that the classifier C(·) is fair, our statistical test will detect the failure of C(·) in meet-

ing the fairness criterion (in Definition 1) under the data

generating distribution. To develop a suitable test statistic,

let P^N = N -1

N i=1

(xi,ai,yi)

denote

the

empirical

mea-

sure of the samples obtained from a distribution P  P (Z).

We define the projection of P^N onto F as

P(P^N ) inf Wc(Q, P^N )
QF

=

inf Wc(Q, P^N ) s.t. EQ[C(X)(U, EQ[U ])] = 0.

(5)

We adopt the statistical hypothesis framework: for a prespecified significance level ,
reject H0 if sN > 1-,
where sN is a test statistic that depends on the projection distance P(P^N ), and 1- is the (1 - ) × 100% quantile of a limiting distribution.

Proposition 1 (Primal reformulation). The projection distance P(P^N ) is equal to the optimal value of a linear program. More specifically, we have



 

minp









  

s.t.

P(P^N ) = 

         

1 N

pi d(xi )

i[N ]

p  [0, 1]N ,

[1 - 2C(xi)](ui, EP^N [U ])pi
i[N ]

=-

C(xi)(ui, EP^N [U ]).

i[N ]

(7)

Naturally, one may study the above linear program by considering its dual formulation. Define the following function

D(P^N )

max
 Rm

1 N

i[N] (ui, EP^N [U ])C(xi)+

d(xi) + [1 - 2C(xi)] (ui, EP^N [U ]) -

,

where recall the notation that (x)- = min{x, 0}. Strong duality of linear programming asserts that P(P^N ) and D(P^N ) are dual to each other.
Proposition 2 (Strong duality). Strong duality holds, i.e., P(P^N ) = D(P^N ).

3.1. Linear Programming Formulation for Projection
Our aim here is to reformulate the infinite dimensional projection formulation (5) as a finite dimensional linear program. For this purpose, let us define d : X  [0, ] as
d(x) inf {c¯(x, x) : x  X , C(x) = 1 - C(x)} , (6)

which gives a measure of distance to the region with classifier label different from that at x, and d(x) = 0 means that x on the decision boundary. The value d(x) is readily computed for commonly used classifiers such as linear classifiers (as shown in the proof of Lemma 1 below in the supplement) and kernelized classifiers. In the case of a classifier defined in terms of kernels, say as in,

n

C(x) = I

ik(xi, x) + b  0 ,

i=1

one may use the transportation cost (3b) and d(x) admits a closed-form expression

3.2. Asymptotic Behavior of the Projection Distance
The goal of this subsection is to study the limiting behavior of the projection distance P(P^N ) as the sample size N increases. Proposition 2 implies that it is sufficient to examine the asymptotic behavior of D(P^N ). To present the regularity assumptions under which the limiting behavior can be unravelled, we set µ = EP[U ] and define  as
 : X  R, (X) = (2C(X) - 1)d(X).
Assumption 1 (Continuous density and derivatives). There exists  > 0 such that the below conditions are satisfied:

a) The probability distribution of (X) has a positive

continuous density f (·) in the interval (-v, v), i.e.,

P((X)  [-v, u)) =

u -v

f ()d

for

u



(-v,

v).

b) For every u  supp(U ), the function (u, z) has a
continuous derivative (Jacobian matrix) z(u, z) in the neighborhood z satisfying z - µ 2 < v. In addition, 1 EP[(U, µ)(U, µ)| d(X) = 0]  0.

d(x) =

n

2

ik(xi, x) + b /(K),

i=1

where K is an n × n matrix with entries Ki,j = k(xi, xj).

Assumption 2 (Continuous conditional probability). For
the case where supp(U ) is a finite set, the conditional probability P(U = u | (X) = t) is continuous around t = 0 for every u  supp(U ).

Testing Group Fairness via Optimal Transport Projections

We are now ready to state the main result of this section
concerning the asymptotic behavior of the projection dis-
tance.
Theorem 1 (Limit theorem for D(P^N )). Suppose that {X1, U1} , ..., {Xn, Un} are independently obtained from the distribution P and that Assumptions 1 and 2 are satisfied. Then under the null hypothesis H0,

N × D(P^N )  max
Rm

V

-

1 2



S



=

1 2

V

S

-1V

,

where S = f (0)1, V  N (0, ),  is the covariance matrix of (U, µ)C(X)+ EP [z(U, µ)C(X)] U , and  denotes the convergence in distribution.

The finite cardinality of the outcome space of U in As-

sumption 2 is not restrictive: Theorem 1 still holds under

infinite cardinality under an equivalent assumption. Details

can be found in Appendix A. For the fairness notions in

Examples 1 - 4, we report in Corollary 1 below the specific

closed-form limit distributions obtained from Theorem 1.

Corollary 1.

Suppose that (U, µ)

=

U1 µ1

-

U2 µ2

with U1, U2

satisfying U1U2 = 0 (with probability 1), as in Examples

1, 2, and 4. Then we have the limiting distribution,

V S-1V /2

=

2f

(0)

(µ22EP[U1|d(X

22(1) ) = 0] +

µ21EP[U2|d(X

)

=

0])

,

where 2(1) is a chi-squared distribution with one degree of freedom and

2 =var {C(X) (µ2U1 - µ1U2) + U2EP [U1C(X)] - U1EP [U2C(X)]} .

For Example 3, we have

V S-1V /2

=

2f

(0)

211(1)2 (µ22EP[U1|d(X) = 0] + µ21EP[U2|d(X

)

=

0])

+

2f

(0)

222(1)2 (µ24EP[U3|d(X) = 0] + µ23EP[U4|d(X

)

=

0])

.

where 21(1) and 22(1) are two independent chi-squared distributions with one degree of freedom and

21 =var {C(X) (µ2U1 - µ1U2) + U2EP [U1C(X)] - U1EP [U2C(X)]} ,
22 =var {C(X) (µ4U3 - µ3U4) + U4EP [U3C(X)] - U3EP [U4C(X)]} .

Assumption 1a) is satisfied for a broad class of classification models interesting in practice. Lemma 1 below identifies that Assumption 1a) is satisfied even if there are some discrete features.

Lemma 1. Suppose that X = (XD, XC), where XD takes values in a finite subset D  Rd1, d = d1 + d2 and XC is an Rd2-valued random vector whose conditional distribution XC | XD = x has a positive density in Rd2 for every x  D. Let the cost function c¯(·) be given by (3a) or (4). Further, if the classifier is written as C(X) = I{(X)   } where (·) is a continuous and increasing function with limx+ (x) >  > limx- (x) and  = (D, C ) with C = 0, we have that Assumption 1.a) is satisfied.
With P(P^N ) = D(P^N ) as in Proposition 2, Theorem 1 reveals that one can use sN N × P(P^N ) as a test statistic to reject H0. In particular, for a prespecified significance level   (0, 1), let 1- denote the (1 - ) × 100% quantile of the generalized chi-squared distribution given by the law of V S-1V /2. Specific computation and estimation procedures required to compute the statistic sN and the quantile 1- are discussed in Section 5.

3.3. The Structure of the Wasserstein Projection
In this subsection, we characterize the projection measure Q and provide a heuristic justification for the convergence rate of Theorem 1. Note that the proof of Proposition 1 also leads to an -optimizer sequence for (5).
Proposition 3 (-optimizer). Suppose d(xi) < + for every i  [N ]. Let xi be an -optimizer obtained by solving (6) with x = xi, for i  [N ]. Let {pi }Ni=1 be an optimal solution of the problem (7). Then, the measure

Q

1 N

N
(1 - pi )(xi,ai,yi) + pi (xi ,ai,yi)

i=1

is an -optimizer of the problem (5).

Proposition 3 indicates in the optimal transportation plan, the transporter moves mass from xi to xi when pi = 0. Since the optimal solution of a linear programming problem occurs at corner points, most of pi should be either zero or one. Further, the proof of Theorem 1 shows that the number of non-zero values in {pi }Ni=1 is of the order Op(N 1/2) and for each non-zero pi , the moving distance d(xi) is of the order Op(N -1/2) under the null hypothesis. Therefore, D(P^N ) is of the order Op(N -1). This statistical phenomenon is due to the discontinuity of the estimating function C(X)(U, EQ[U ]) in X, where the transporter is able to move a small amount of probability
mass, but the move results in a significant change of the
value for the estimating function around the discontinuity region. The Op(N -1) convergence rate is in contrast to the rate in Blanchet et al. (2019), Taskesen et al. (2021)
and Cisneros-Velarde et al. (2020), where the estimating
function is assumed to be continuous. For the continu-
ous estimating function, it is optimal to move every point

Testing Group Fairness via Optimal Transport Projections

Op(N -1/2) distances, which results in a Op(N -1/2) convergence rate. Therefore, let us emphasize again the key qualitative difference between our contributions and those of Taskesen et al. (2021). A statistical noise gives the empirical appearance of unfairness in two ways: (A) small statistical fluctuations around all data points; (B) a small sub-population with large outcome fluctuations around the decision boundary. Taskesen et al. (2021) studied scenario (A) and our paper studies scenario (B).

4. Test For Composite Null Hypothesis via Optimal Transport
In settings where the notion of exact group fairness becomes restrictive or unattainable, it becomes attractive to test whether the deviation from fairness, if any, from a given group's viewpoint is not more than a prespecified small extent. The question of verifying -fairness, from an one-sided perspective, can be similarly formulated as follows. Following Section 3, we define

F = {Q : EQ[C(X)(U, EQ[U ])]  } ,
where   Rm + is a tolerance level prespecified by the fairness auditor. We are interested in the statistical test with the hypotheses

H0 : P  F against H1 : P  F.

(8)

A suitable statistical test for this formulation will serve the purpose of detecting failure of C(·) in meeting the onesided fairness condition within an  tolerance. Similar to Problem 5, we define the projection of P^N onto F as

P(P^N )

inf Wc(Q, P^N ) s.t. EQ[C(X)(U, EQ[U ])]  .

4.1. Linear Programming Formulation for Projection

Proposition 4 (Primal reformulation). The projection distance P(P^N ) is equal to the optimal value of a linear program. More specifically, we have

P(P^N )



 

minp









  

s.t.



=

         

(9)

1 N

pi d(xi )

i[N ]

p  [0, 1]N

(1 - 2C(xi))(ui, EP^N [U ])pi
i[N ]

+

C(xi)(ui, EP^N [U ])  N .

i[N ]

As in Section 3, P(P^N ) is amenable to be studied via the

respective dual function,

D(P^N )

max
 Rm +

-



+

1 N

i[N ]

(ui, EP^N [U ])C(xi)

+ d(xi) + (1 - 2C(xi)) (ui, EP^N [U ]) - .

Proposition 5 (Strong duality). Strong duality holds, i.e., P(P^N ) = D(P^N ).

4.2. Asymptotic Behavior of the Projection Distance
We next study the limit of D(P^N ) as the sample size N tends to infinity. In order to state the theorem, let us introduce notation for asymptotic stochastic ordering. We say that a sequence of random elements {An}n1 satisfies An D B if for every continuous and bounded nondecreasing function g,

lim sup E [g(An)]  E [g(B)] .
n

Theorem 2. Suppose that {X1, U1} , ..., {Xn, Un} are independently obtained from the distribution P and that Assumptions 1 and 2 are satisfied. Then under the null hy-
pothesis H0,

N × D(P^N )

D

max
Rm +

V

-

1 2

S



,

(10)

where S = f (0)1, V  N (0, ), and  is the covariance
of (U, µ)C(X) + EP [z(U, µ)C(X)] U. In particular, if (U, µ) is one-dimensional (m = 1), we have

max
 Rm +

V

-

1 2



S



=

1 2

S

-1

V

2I{V

 0}.

With P(P^N ) = D(P^N ) as in Proposition 5, Theorem 2 reveals that one can use sN () N × P(P^N ) as a test statistic to reject H0 and 1-, defined by the (1 - ) × 100% quantile of the right hand side bounding variable in (10), as a threshold. We then follow the same hypothesis
testing procedure defined in Section 3. Since Theorem 2
only provides a stochastic upper-bound, we actually use a
conservative quantile and the type I error is less than or equal to the desired significance level  asymptotically.

5. Computation and Estimation Procedures
5.1. Computations of the Test Statistic
Based on Proposition 1, we propose a sorting-based algorithm for computing P(P^N ) for one-dimensional (·) (that is, m = 1). The steps involve transporting points which are close to the decision boundary and have significant contributions towards improving fairness if prediction labels

Testing Group Fairness via Optimal Transport Projections

are flipped. The exact steps are described in Algorithm
1. Note that the algorithm requires only information on {C(xi), d(xi) : i  [N ]}, instead of the whole functional structure of the classifier C.

Algorithm 1 Computing P(P^N ) for one-dimensional (·)
1: Input: Data {d(xi), C(xi), (ui, EP^N [U ])}Ni=1. 2: Output: the optimal value P(P^N ). 3: Let s  - i[N] C(xi)(ui, EP^N [U ]); 4: For i  [N ], compute

ti  d(xi)-1(1 - 2C(xi))(ui, EP^N [U ])sgn(s);

5: Sort t1, . . . , tN in descending order, where t(i) denotes the i-th largest one and let d(i) be the corresponding distance;

6: Initialize V = 0 and let s  |s|;

7: for i  1 to T do

8: if t(i)d(i) < s then

9:

s  s - t(i)d(i) and V  V + d(i);

10: else

11:

V  V + t-(i)1s and break;

12: end if

13: end for 14: Output P(P^N )  V /N .

With the output P(P^N ) returned by Algorithm 1, computation of the test statistic sN = N × P(P^N ) is immediate. To obtain P(P^N ) similarly, one may modify Line 3 in Algorithm 1 as in



+

s  -  C(xi)(ui, EP^N [U ]) - N  .
i[N ]

With sgn(0) = 0 assigning ti = 0 for all i  [N ] in Step 4, we take 0/0 = 0 in Line 11 in Algorithm 1 in order to obtain P(P^N ). It is easy to see that the time complexity of Algorithm 1 is the same of the time complexity of the sorting algorithm, which is generally O(N log N ). For instances where m > 1, one may solve either problem (7) or problem (9) with a standard linear program solver to obtain the respective values P(P^N ) or P(P^N ), which is also solvable in polynomial time. Therefore, our hypothe-
sis test is more computationally efficient than the test pro-
posed in Taskesen et al. (2021), which requires solving a
non-convex optimization problem.

5.2. Computations of the Quantile of the Limiting Distributions
We use the conditional density estimator and the NadarayaWatson estimator (Tsybakov, 2008, Section 1) to estimate

f (0) and 1, i.e.,

f^(0)

=

1 Nh

K

(xi) h

, and

i[N ]

K ^ 1 = i[N]

(xi) h

 ui, EP^N [U ]  ui, EP^N [U ] 

K

(xi ) h

,

i[N ]

where h > 0 is the bandwidth parameter, and K(·) is a kernel function that is symmetric and integrates to one. By combining the above two estimates, an empirical estimate for S, denoted by S^ is computed via,

1 Nh

K

(xi ) h

 ui, EP^N [U ]  ui, EP^N [U ]  .

i[N ]

Under some mild conditions, by choosing h = O(N -1/5), we have S - S^ = O(N -2/5); see, for example, Härdle (1990, Theorem 4.2.1) and Tsybakov (2008, Proposition 1.7). By combining the empirical covariance estimator for , we obtain a quantile estimate ^1-.

6. Numerical Experiments
Our experiments use the following three datasets: Arrhythmia (Dua & Graff, 2017), COMPAS (MultiMedia LLC, 2016) and Drug (Fehrman et al., 2017). The details of the datasets are provided in Appendix B.2.
In the first experiment, we test the fairness of the Tikhonovregularized logistic and SVM classifiers by the equal opportunity criterion. We randomly split 70%-30% of the data as a train-test set. Figure 1 reports the test statistics, fairness rejection threshold, and the accuracy of the classifier. Figure 1(a) shows the result of regularized logistics classifier in COMPAS dataset, while Figure 1(b) shows the result of SVM classifier in the Drug dataset. We observe that a strong regularization only reduces the test statistics very mildly, and the Wasserstein projection tests suggest we reject the fair null hypothesis even when the regularization power is sufficiently large, which presents a different phenomenon from the probabilistic fairness test results shown in Taskesen et al. (2021). We here provide a heuristic explanation for this difference. Consider a logistic classifier C(x) = I{1/(1 + exp(-x))  0.5}. Since regularization usually induces shrinkage, the regularized classifier could be approximated by C(x) = I{1/(1 + exp(-x))  0.5}, and large regularization power corresponds to small . Note that C(x) = C(x) no matter how small  > 0 is. However, for the probabilistic notion, C will output approximately equal probabilities for both labels, which tends to be probabilistic fair when  is very

Testing Group Fairness via Optimal Transport Projections

small. The experiment thus demonstrates probabilistic fairness does not imply the exact fairness in general.

Statistic value

Accuracy

10 8 6 4 2 0

0.670

0.665

0.660

0.655

0.650

0.645

0.640

20

40

60

80

100

(a) Regularized logistics classifier in COMPAS

Table 1. Rejection percentage of the Naive SVM and the method in Donini et al. (2018) at the significance level  = 0.05 according to the equal opportunity criterion using our test.
Arrhythmia COMPAS Drug

Naive SVM Donini et al. (2018)

68.4% 11.6%

100% 30.1% 16.6% 21.6%

Table 2. Rejection percentage of the Naive SVM and the method in Donini et al. (2018) at the significance level  = 0.05 according to the equalized odds criterion using our test.
Arrhythmia COMPAS Drug

Naive SVM Donini et al. (2018)

75.1% 13.7%

100% 30.5% 21.7% 17.2%

Accuracy

Statistic value

1.050 1.025 1.000 0.975 0.950 0.925 0.900

0.814

0.812

0.810

0.808

0.806

0.804

0.802

0.800

5

10

15

(b) SVM classifier in Drug dataset
Figure 1. Test statistics and accuracy of regularized classifiers on test data with a rejection threshold. The green line is the test statistics; the pink dashed line is the rejection threshold at the significance level  = 0.05; the purple line is the test accuracy;  denotes the regularization parameter, where larger  means stronger regularization power.

In the second experiment, we compare a fair algorithm proposed in Donini et al. (2018) with a naive SVM classifier (parametrized by the ridge regularization ) in three datasets: Arrhythmia, COMPAS and Drug. We randomly split 70%-30% of the data as a train-test set and we replicate this procedure 1,000 times. We will test the fairness in terms of the equal opportunity and equalized odds criteria, and for the equal opportunity criteria, we will further show results using Welch's test (Welch's test is not applicable for multi-dimensional equalized odds criteria). Tables 1 and 2 show a rejection percentage of the naive SVM and the method in Donini et al. (2018) at the significance level  = 0.05 in those 1,000 replications using our test according to the equal opportunity and equalized odds criteria, respectively. Table 3 shows the test results using Welch's test according to the equal opportunity criterion. Our test results demonstrate that the method in Donini et al. (2018) has a significantly lower rejection rate, which means it is substantially more fair than the naive method.

Table 3. Rejection percentage of the Naive SVM and the method in Donini et al. (2018) at the significance level  = 0.05 according to the equal opportunity criterion using Welch's test.
Arrhythmia COMPAS Drug

Naive SVM Donini et al. (2018)

76.1% 14.0%

100% 35.5% 16.1% 23.0%

More experiments are conducted in Appendix B.1 to empirically validate the convergence result in Theorem 1 and our proposed hypothesis test method.
Acknowledgement
Material in this paper is based upon work supported by the Air Force Office of Scientific Research under award number FA9550-20-1-0397. Additional support is gratefully acknowledged from NSF grants 1915967, 1820942, and 1838576 and Singapore Ministry of Education's AcRF grant MOE2019-T2-2-163.

Testing Group Fairness via Optimal Transport Projections

Appendix A. Proofs
We prove Theorems 1 and 2 under a more general assumption below. Assumption 2 (Continuous conditional measure). For the case where supp(U ) is potentially an infinite set, the cost function c is decomposable as
c ((x, u), (x, u)) = c¯(x, x) +  · u - u ,
and the following conditions are satisfied:

a)

the moments EP

U

22, EP

(U, µ)

2 2

and

EP

z(U, µ)

2 are finite.

b) for z such that z - µ 2 < v, the derivative z(·) satisfies,

where EP[M (U )] < +.

z(u, z) - z(u, µ) 2  M (u) z - µ 2 ,

(A.1)

c) The (regular) conditional probability measure t of (U, µ)|(X) = t converges in terms of the type 1-Wasserstein distance as t  0: i.e., there exist a set B  R with P((X)  B) = 1 and 0 > 0 such that

lim
t0

W1

(t,

0)

1{t



B}

=

0

and suptB EP[

(U, µ)

2+0 2

|(X

)=t]

is

finite,

where

type

1-Wasserstein

distance

W1(·,

·)

is

Wc(·,

·)

with

the

cost

function being a metric.

Remark 1. If supp(U ) is a finite set, and U is completely dependent on (X, Y ), the simpler Assumption 2 is equivalent to Assumption 2.

Appendix A.1. Proofs of Section 3

Proof of Proposition 1. Since the cost to move U is +, we have EQ[U ] = EP^N [U ]. Then, consider any probability measure Q such that

EQ[C(X)(U, EP^N [U ])] = 0,

and let  be the optimal coupling between P^N and Q. Because P^N is the empirical measure, the coupling  can be written

as



=

1 N

i[N] i  (xi,ui). For any value  > 0, construct now the measure

Q

=

1 N

(1 - pi)(xi,ui) + pi(xi ,ui),

i[N ]

(A.2)

where the mass pi is set to

pi =

i(dx) = i(X1-C(xi))  [0, 1]

X1-C(xi )

i  [N ]

and xi is an -optimizer of the problem infxX1-C(x) c(xi, x). Then, it is easy to see that

and that

EQ [C(X)(U, EP^N [U ])] = EQ[C(X)(U, EP^N [U ])] = 0

EQ [C(X)(U, EP^N [U ])]





=

1 N



(1 - pi)C(xi)(ui, EP^N [U ]) + pi(1 - C(xi))(ui, EP^N [U ])

i[N ]





=

1 N



(1 - 2C(xi))(ui, EP^N [U ])pi +

C(xi)(ui, EP^N [U ]) .

i[N ]

i[N ]

Testing Group Fairness via Optimal Transport Projections

Since d(xi) 

xi - xi



x - xi

+



for

any

x



X1-C(xi),

this

implies

that

1 N

i[N] pid(xi)  W (Q, P^N ) + .

Since  can be chosen arbitrarily, this implies that



 

min







P(P^N )   s.t.

     

1 N

pi d(xi )

i[N ]

p  [0, 1]N

(1 - 2C(xi))(ui, EP^N [U ])pi = -

C(xi)(ui, EP^N [U ]).

i[N ]

i[N ]

(A.3)

On the other hand, for any {pi}Ni=1 satisfying the constraints in the linear programming (A.3), we can construct the measure Q according to (A.2). Since Q is a feasible solution of the primal problem (5), we have the other direction of the inequality.

Proof of Proposition 2. Case 1: d(xi) < + for i  [N ]. The primal problem has a feasible solution (pi = 0 if C(X) = 0; pi = 1, otherwise) and is bounded, thus it has an optimal solution and the strong duality holds. By the strong
duality, we have





  

max

P(P^N ) = 

 

s.t.











1 N

 i
i[N ]

+

 C(xi)(ui, EP^N [U ])


i  0 i  [N ] i - (1 - 2C(xi)) (ui, EP^N [U ])  d(xi)

i  [N ].

(A.4)

Then, we have

i = d(xi) + (1 - 2C(xi)) (ui, EP^N [U ]) - ,

which gives the desired results.

Case 2: i  [N ] such that d(xi) = +. The primal problem is equivalent to



 

min











P(P^N ) =  s.t.

       

1 N

pi d(xi ),

i[N ]

p  [0, 1]N ,

pi = 0 for d(xi) = +

(1 - 2C(xi))(ui, EP^N [U ])pi = -

C(xi)(ui, EP^N [U ]),

i[N ]

i[N ]

(A.5)

with the convention that if the problem is infeasible, the optimal value of the minimization problem is +. We have the

dual problem

max s.t.





1 N

 i
d(xi)<+

 + C(xi)(ui, EP^N [U ])


i  0 for d(xi) < + i - (1 - 2C(xi)) (ui, EP^N [U ])  d(xi)

for d(xi) < +.

(A.6)

Since the problem (A.6) is also feasible, if it is bounded, then the strong duality holds. If the problem (A.6) is unbounded,

the primal problem (A.5) is infeasible, which means the primal and the dual both have optimal value +. Finally, because

d(xi) + (1 - 2C(xi)) (ui, EP^N [U ]) - = 0 for d(xi) = +, we have the optimal value of problem (A.6) equals to D(P^N ).

Proof of Lemma 1. Since XC XD = x has positive density in Rd2 for every x  D, we have C XC XD = x has positive density in R for every x  D. Therefore, X has a density

fX (x) =

pvf C XC v(x - Dv) > 0,

vD

Testing Group Fairness via Optimal Transport Projections

where

pv

=

P (XD

=

v)

and

f C XC

(·)
v

denotes the

conditional density

of

C XC

XD

=

x.

Further, let w = -1( ). For the cost function c¯(·) given by (3a), we have by Hölder inequality

d(x) = inf
  x =w

x - x

=

 - 1|x - w|.

Therefore, P has a continuous density f (·) =  fX (   × · + w) with f (0) > 0. For the cost function c¯(·) given by (4), when d(x) < , we have

d(x) = inf c¯(x, x) =

inf

c¯(x, x) =

inf

  x =w

xD =xD, C x=w- D xD

 C x=w- D xD

xC - xC

=

C

-1 

|

x

-

w|.

The last equality is again due to Hölder inequality. Therefore, P has a continuous density f (·) = C fX ( C  × · + w) with f (0) > 0, which completes the proof.

Lemmas A2 and A3 are useful for the proof of Theorem 1, whose proofs are presented in Appendix A.3.
Lemma A2. Suppose Assumption 2 is enforced. Then, we have  N EP^N  U, EP^N [U ] C(X) - EP [(U, µ)C(X)]  N (0, cov (EP [z(U, µ)C(X)] U + (U, µ)C(X))).

Lemma A3. Suppose Assumption 1 and 2 are enforced. Then, we have

 N EP^N

-

U, EP^N [U ]



-

+ N d(X) C(X)

-p

-

1 2

f

(0)EP

 (U, µ) 2 I{ (U, µ)  0} d(X) = 0 ,

uniformly over  2  B.

We are now ready to prove Theorem 1.

Proof of Theorem 1. Recall that





D(P^N )

=

1

max
 Rm

N

i[N ]

d(xi) + (1 - 2C(xi)) (ui, EP^N [U ])

-

+

C(xi)(ui,

EP^N

[U

 ])





=

1

max
 Rm



N

C(xi)(ui, EP^N [U ])
i[N ]

+



1 N
i[N ]

d(xi) - (ui, EP^N [U ]) - C(xi) +

d(xi) + (ui, EP^N [U ])

-

(1

-

C

 (xi))

.



 We first rescale    N and thus

ND(P^N )

=

N max
Rm

  EP^N

 U, EP^N [U ] C(X)

+



-



-

EP^N

N d(X) - (U, EP^N [U ]) C(X) + N d(X) + (U, EP^N [U ]) (1 - C(X)) .

To ease the notation, we denote i =  ui, EP^N [U ] . By Lemma A3, we have

1 N N i=1

-
-i + N 1/2d(xi) (1 - C(xi))

-p

-

1 2

f

(0)EP

 (U, µ) 2 I{ (U, µ)  0} d(X) = 0

=

-

1 2

f

(0)EP

 (U, µ) 2 I{ (U, µ)  0} d(X) = 0 ,

Testing Group Fairness via Optimal Transport Projections

and similarly, we have

1 N N i=1

i + N 1/2d(xi)

-

(1

-

C(xi))

-p

-

1 2

f

(0)EP

 (U, µ) 2 I{ (U, µ) < 0} d(X) = 0 .

Therefore, we have

 N EP^N

 N d(X) - (U, EP^N [U ])

-
C(X) +

 N d(X) + (U, EP^N [U ])

-
(1 - C(X))

-p

-

1 2

f

(0)EP

 (U, µ) 2 d(X) = 0 .

We denote

 VN = N EP^N  U, EP^N [U ] C(X) , and

MN ()

=

1 N N i=1

-i + N 1/2d(xi)

-
C(X)} +

i + N 1/2d(xi)

-
(1 - C(X))

.

To proceed, we rely on the following lemma.

Lemma A4. Suppose Assumption 1 is enforced. Then, for every  > 0, there exists N0 > 0 and b  (0, ) such that for all N  N0,
P sup VN + MN () > 0  .
 2>b

The proof of Lemma A4 is furnished in Appendix A.3. Notice that D(P^N )  0 (choosing  = 0), Lemma A4 implies that when N  N0,

P N D(P^N ) = sup VN + MN ()
 2b

 1 - .

By Lemmas A2 and A3, we have

sup VN + MN ()
 2b



sup
 2b

V

-

1 2

f

(0)E

 (U, µ) 2 d(X) = 0

=

sup
 2b

V

-

1 2



S



,

where

S = f (0)EP  (U, µ)  (U, µ) d(X) = 0 ,

and V is normally distributed with mean zero and covariance matrix

cov (EP [z(U, µ)C(X)] U + (U, µ)C(X)) . By the arbitrariness of , we have the desired result:

This completes the proof.

N × D(P^N )  sup


V

-

1 2



S

.

Testing Group Fairness via Optimal Transport Projections
Appendix A.2. Proofs of Section 4 The proofs of Propositions 4 and 5 are not presented because they follow the same lines as the proofs of Propositions 1 and 2.

Proof of Theorem 2. Let  = EP [C(X) (U, EP[U ])]. By following the similar arguments with the proof of Theorem 1, we have

N D(P^N )







=

N

sup
Rm +

 -


+

1 N

 (1
i[N ]

-

2C(xi))(ui, EP^N [U ])

+



C(xi)(ui, EP^N [U ])

i[N ]





=

N sup
 Rm +



EP^N

 U, EP^N [U ] C(X)} - 

+



-



-

EP^N -(U, EP^N [U ]) + N d(X) C(X) + (U, EP^N [U ]) + N d(X) (1 - C(X))

 = sup VN + N  ( - ) + MN () .
 Rm +

Similarly, we still have

VN

+

MN ()



V

-

1 2





S

,

uniformly over  :   Rm + ,  2  B . Therefore, we must enforce  ( - ) = 0 here. Then, we have

N

D(P^N

)



max
Rm + ,(-)=0

V

-

1 2





S



max
Rm +

V

-

1 2

S



.

This completes the proof.

Appendix A.3. Proofs of Technical Results Proof of Lemma A2. By adding and subtracting the term EP^N [(U, µ)C(X)], we find

EP^N (U, EP^N [U ])C(X) - EP [(U, µ)C(X)] = EP^N (U, EP^N [U ])C(X) - (U, µ)C(X) + EP^N [(U, µ)C(X)] - EP [(U, µ)C(X)]

(A.7)

Under Assumption 2 and the fundamental theorem of calculus, the first term in the right-hand side of (A.7) becomes

EP^N (U, EP^N [U ])C(X) - (U, µ)C(X) = EP^N Thanks to Assumption 2, we have that

1
z U, µ + t EP^N [U ] - µ
0

EP^N [U ] - µ C(X)dt .

1

EP^N

z U, µ + t EP^N [U ] - µ EP^N [U ] - µ C(X)dt

0

1

-EP^N

z (U, µ) EP^N [(U )] - µ C(X)dt

0

2



1 2

EP^N

[M (U )]

EP^N [U ] - µ

2 2

,

whenever EP^N [U ] - µ 2 < µ. Then, notice that we have

lim
N 

1 2

 N EP^N

[M

(U

)]

EP^N [(U )] - µ

2 2

=

0

almost

surely,

(A.8)

Testing Group Fairness via Optimal Transport Projections

and

1

EP^N

z (U, µ) EP^N [U ] - µ C(X)dt = EP^N [z (U, µ) C(X)] EP^N [U ] - µ }

0

= (EP [z (U, µ) C(X)] + op (1)) EP^N [(U )] - µ .

 By multiplying N to both sides of equation (A.7), we have



N 

EP^N



U, EP^N [U ]

C (X )

- EP [(U, µ)C(X)]

=

N (EP [z (U, µ) C(X)] + op (1)) 

EP^N [U ] - µ

+ EP^N [(U, µ)C(X)] - EP [(U, µ)C(X)] + op(1)

= N EP^N EP z (U, µ) C(X) (U - µ) + (U, µ)C(X) - EP (U, µ)C(X) + op(1)

 N (0, ),

where  is the covariance matrix of EP [z(U, µ)C(X)] U + (U, µ)C(X), namely  = cov (EP [z(U, µ)C(X)] U + (U, µ)C(X)) .

This completes the proof.

Proof of Lemma A3. Step 1: we first show





-



N EP^N - U, EP^N [U ] + N d(X) C(X) - N EP^N

 - (U, µ) + N d(X)

-
C (X )

-p 0,

uniformly over  2  B. When EP^N [U ] - µ 2 < µ, we have





-



N EP^N - U, EP^N [U ] + N d(X) C(X) - N EP^N

-



(U,

µ)

+

 N

d(X

)

-
C (X )

N
 N -1/2  2
i=1

 ui, EP^N [(U )] -  (ui, µ) 2 I{Ei} ,

where the events Ei are defined by 
Ei = {  2 (  (ui, µ) 2 + ( z (ui, µ) 2 + M (ui)µ) µ)  N d(xi)}.

By a similar derivation with the proof of Lemma A2, we have

N -1/2

N
2
i=1



ui, EP^N [U ]

-  (ui, µ)

- 2

I{Ei}

=

 2 N N i=1

1
z ui, µ + t EP^N [U ] - µ
0

EP^N [U ] - µ dt I{Ei}



 2 N

EP^N [U ] - µ

EP^N

[z

(U,

µ) I{Ei}]

+

1 2

  2 N EP^N [M (U )]

EP^N [U ] - µ

2 2

.

Since I{Ei}  0 almost surely and EP[z(U, µ)] < +, we have



 2N

EP^N [U ] - µ

EP^N [z (U, µ) I{Ei}] -p 0,

uniformly over  2  B. By combining

1 2

  2 N EP^N [M (U )]

EP^N [U ] - µ

2 2



0

almost

surely,

uniformly over  2  B, we finish step 1.

Step 2: We claim that

Testing Group Fairness via Optimal Transport Projections

 N EP

 - (U, µ) + N d(X)

-
C (X )

-

-

1 2

f

(0)E

 (U, µ) 2 I{ (U, µ)  0} d(X) = 0 .

Notice that for any c > 0, we have

 N EP

-



(U,

µ)

+

 N

d(X

)

-
C (X )

 =N

+
EP

 - (U, µ) + N d(X)

-

d(X) (2C(X) - 1) = t

dP(t).

0



c/ N

=N

EP

-





(U,

µ)

+

 N

d(X

)

-

(X) = t

dP(t)

0



+

+ N  EP

c/ N

 - (U, µ) + N d(X)

-

(X) = t

dP(t)

(A.9) (A.10)

 We first analyze the first term in (A.9). By Assumption 1.a), when N is sufficient large such that c/ N < v, we have





c/ N

N

EP

0



c/ N

=N

EP

0

 - (U, µ) + N d(X)

-

(X) = t

dP(t)

 - (U, µ) + N d(X)

-

(X) = t

f (t)dt.

 By changing of the variable s = N t , we have



 N

c/ N
EP

 - (U, µ) + N dB(X)

-

(X) = t

f (t)dt

0

=

c
EP

- (U, µ) + s - (X) = N -1/2s f (N -1/2s)ds

0

By Assumption 1.c), we have for any  > 0, any 0 < c < +, there exists N0, such that for N > N0 and s  c,

EP - (U, µ) + s - (X) = N -1/2s - EP - (U, µ) + s - (X) = 0    EP  (U, µ)| (X) = N -1/2s - EP [  (U, µ)| (X) = 0]    W1  (U, µ)| (X) = N -1/2s,  (U, µ)| (X) = 0  .

Therefore, by taking   0, we have

c
EP

- (U, µ) + s - I{h (X)  } (X) = N -1/2s f (N -1/2s)ds

0

-

c
EP

- (U, µ) + s - I{h (X)  } (X) = 0 f (N -1/2s)ds -p 0.

0

Testing Group Fairness via Optimal Transport Projections

Then, the basic algebra and the mean value theorem for integrals give us

c
EP
0 c
= EP
0
=f ()EP
=f ()EP

- (U, µ) + s - (X) = 0 f (N -1/2s)ds
- (U, µ) + s - d(X) = 0 f (N -1/2s)ds
c - (U, µ) + s - d(X) = 0 ds
0 min(c, (U,µ)|(X )=0)
- (U, µ) + s I{ (U, µ)  0} d(X) = 0 ds
0



-

1 2

f

(0)EP

min{c,  (U, µ)}

 (U, µ) +

 (U, µ) - c +

I{ (U, µ)  0} d(X) = 0

,

where   [0, N -1/2c] and (x)+ = max{x, 0}. We then deal with the second term in (A.9). Let

M = ess sup EP  (U, µ) 2+0 |(X)=t .
t0

For any c  0, we have



+

N  EP

c/ N

-

(U,

µ)

+

 N

t

-

(X) = t

dP(t)



+



 - N  EP  (U, µ) I{  (U, µ)  N t (X) = t dP(t)

c/ N





+

-N 

c/ N

1 Nt

1+0
EP

 (U, µ)

2+0

I{  (U, µ)



 Nt

(X) = t

dP(t)



-

 N

-0

+  c/ N

1 t1+0

EP

[

 (U, µ)

2+0

|(X )

=

t]dP(t)



-

 N

-0
M

+  c/ N

1 t1+0

dP

(t).

We pick  > 0 such that P (·) has density in [0, ]. Then, we have

 N

-0
M

+  c/ N

1 t1+0

f

(t)dt

 -0 = M N

+ 

1 t1+0

f (t)dP(t)

+

  c/ N

1 t1+0

f

(t)dt

 M = M

 -0 1

1  -0 

0

N

1+0 + 0

N

N /c f ()

 N

-0

1 1+0

+

1 c0 0

f

(

)

,

 where   (c/ N , ). By taking   0, we have



+

lim inf
N +

N

 EP
c/ N

 - (U, µ) + N t

-

(X) = t

dP(t)



-

Mf (0) c0 0

.

Finally, by taking c  +, we conclude step 2.

(A.11)

Testing Group Fairness via Optimal Transport Projections

Step 3: We then apply weak law of triangular arrays Durrett (2019, Theorem 2.2.11). We need to check





N × P - - (U, µ) + N d(X) C(X) > N  0, and

E

 - (U, µ) + N d(X)

-

2
C (X )

 0.

For condition (A.12a), we have

NP

-

 - (U, µ) + N d(xi)

-



C(X) > N

  N P  (U, µ)  N



E

 (U, µ) 2+0

 0



M 

0

 0.

N

N

For condition (A.12b), we have

E

 - (U, µ) + N d(X)

-

2
C (X )



E

 (U, µ)

2

I{





(U,

µ)

 N

d(X

)}

=

+
EP

 (U, µ) 2 I

  (U, µ)  N t

(X) = t dP(t).

0

We pick  > 0 such that P (·) has density in [0, ]. Then, we have

+
EP

 (U, µ) 2 I

  (U, µ)  N t

(X) = t dP(t)

0

=


EP

 (U, µ) 2 I

  (U, µ)  N t

(X) = t f (t)dt

0

+

+
EP

 (U, µ) 2 I

  (U, µ)  N t

(X) = t dP(t).



For the first term (A.13), we have


EP

 (U, µ)

2

I{



(U,

µ)



 Nt

(X) = t

f (t)dt  M2/(2+0)f (),

0

where   [0, ]. For the second term (A.14) we have

+
EP

 (U, µ)

2 I{  (U, µ)



 Nt

(X) = t

dP(t)



+ EP  (U, µ) 2+0 (X) = t





0

dP(t)



Nt



M0 0  0.

N

By taking   0, we have

+
EP

 (U, µ)

2

I{



(U,

µ)



 Nt

(X) = t

dP(t)  0.

0

(A.12a) (A.12b)
(A.13) (A.14)

Testing Group Fairness via Optimal Transport Projections

We then apply Durrett (2019, Theorem 2.2.11) to obtain the weak law for each .

Step 4: We establish the Lipschitz continuity of

 N EP

 - (U, µ) + N d(X)

-
C (X )

for  2  B, which ensures the tightness. For any 1, 2 satisfying 1 2  B and 2 2  B, we have

 N EP

-1 

(U,

µ)

+

 N

d(X

)

-
C (X )

- EP

 -2  (U, µ) + N d(X)

-
C (X )}





 N 1 - 2 2  (U, µ) 2 C(X)I B  (U, µ) 2  N d(X) .

By following similar lines with steps 2 and 3, we have





N EP^N  (U, µ) 2 C(X)I B  (U, µ) 2  N d(X)

-p f (0)EP

 (U, µ)

2 2

d(X) = 0

.

Then, by Billingsley (2013, Theorem 7.5), we have the desired uniform convergence result.

Proof of Lemma A4. Due to E  (U, µ)  (U, µ) d(X) = 0  0, there exists  > 0 and c0  (0, +) such that

inf E min c0,  (U, µ)  (U, µ) > .
 2=1

for all  2 = 1. And

inf E  (U, µ) > 0,
 2=1

since the unit circle is compact. Let  = inf  2=1 E  (U, µ) . For any  > 0, there exists N1 > 0 and b < +, such that
P( VN 2  b) < /2,

for any N > N0. Recalling Lemma A3 and equation (A.11), there exists N0 > N1 such that

P

 :



2

=

b

such

that

MN

()



-

1 4

E

min

bc0,  (U, µ)

 (U, µ)

< /2

for any N > N0. Then, we have
inf E min bc0,  (U, µ)  (U, µ)
 2=b
 b2 inf E min c0,  (U, µ)  (U, µ) > b2.
 2=1
Let b = 4b/. We have

Notice that for any  2 > b,

P sup MN ()  -bb < /2.
 2=b

MN () 

 b

2

MN

b 

2





2 b

sup MN ().
 2=b

(A.15) (A.16)

By combining inequalities (A.15) and (A.16), we have

P ( :  2 > b, such that MN ()  -  2 b) < /2.

Therefore,

Testing Group Fairness via Optimal Transport Projections

P sup VN + MN () > 0
 2>b
 P sup {  2 VN 2 + MN ()} > 0
 2>b
 P( VN 2  b) + P ( :  2 > b, such that MN ()  -  2 b)  .
This completes the proof.

Appendix B. Additional Details for Numerical Experiments

Appendix B.1. Validation of the Hypothesis Test

In this section, we empirically validate the convergence result in Theorem 1 and our proposed hypothesis test method. we use a simple logistic classifier in the form





 C(x) = I

1

  .

 1 + exp -x



Then, the decision boundary is

x : x = - log

1 

-

1

. We denote w = - log

1 

-

1

. Then, we borrowed the

example in Taskesen et al. (2021). Let

p11 = 0.4, p01 = 0.1, p10 = 0.4, p00 = 0.1.

Moreover, conditioning on (A, Y ), the feature X follows a Gaussian distribution of the form

X|A = 1, Y = 1 N ([6, 0], [3.5, 0; 0, 5]), X|A = 0, Y = 1 N ([-2, 0], [5, 0; 0, 5]), X|A = 1, Y = 0 N ([6, 0], [3.5, 0; 0, 5]), X|A = 0, Y = 0 N ([-4, 0], [5, 0; 0, 5]).

The true distribution P is thus a mixture of Gaussian. A simple algebraic calculation indicates that a logistic classifier with  = (0, 1) and  = 0.5 is fair with respect to the equal opportunity criterion in Example 1. Let (·) denotes the density
of the standard normal distribution and we denote µay and ay to be the conditional mean and variable defined above, respectively. For any , the density of X becomes

  ay 

-1/2
pay 

ay -1/2 x - µay

.

a,y{0,1}2

And thus the density of  (·) becomes

f (z) =  

 ay 

-1/2
pay 

ay -1/2 (z   + w) - µay

.

a,y{0,1}2

By Bayes' formula, we have

pay|d(X)=0 = f (0)-1 ay -1/2   pay ay -1/2 w - µay

Testing Group Fairness via Optimal Transport Projections

(a) N = 30

(b) N = 100

(c) N = 500

Figure 2. Empirical distribution N × D(P^N ) over 2,000 replications (histogram) versus the limiting Chi-square distribution (blue curve) with different sample sizes N .

for a  {0, 1} and y  {0, 1}, where pay|d(X)=0 = E[I(a,y)(A, Y )|d(X) = 0]. In the first experiments, we generate N  {30, 100, 500} i.i.d. samples from P and then calculate N × D(P^N ). We replicate this process for 2,000 times and compare the empirical distribution of N × D(P^N ) with the limiting distribution defined in Theorem 1. Figure 2 shows that finite-sample empirical estimates are closed to the theoretical limiting distributions even when N is as small as 30.
In the second experiments, we show that our proposed Wasserstein projection hypothesis test has the desired coverage property. We generate N  {30, 100, 500, 1000, 2000} i.i.d. samples from P and compute the estimate S^ defined in Section 5.2 and the empirical covariance using the sample data. For the kernel estimator S^, we use the standard Gaussian kernel and choose the bandwidth h = N -1/5, where the results listed below are not sensitive to the constant. We repeat the procedure for 2,000 replications and report the rejection probability at different significant values of   {0.1, 0.05, 0.01} in Table 4. We can observe that when N > 100, the rejection probability is closed to the desired level .

Table 4. Comparison of the null rejection probabilities of probabilistic equal opportunity tests with different significance levels  and test sample sizes N .



0.10 0.05 0.01

N = 30 N = 100 N = 500 N = 1000 N = 2000

0.2875 0.0945 0.0895 0.0900 0.0870

0.2255 0.0540 0.0450 0.0430 0.0460

0.1415 0.0250 0.0085 0.0065 0.0080

Appendix B.2. The Description of Datasets
Followings show brief descriptions of datasets: Arrhythmia, COMPAS and Drug (Fehrman et al., 2017) provided in Section 6.
· Arrhythmia is from UCI repository1, where the aim of this data set is to distinguish between the presence and absence of cardiac arrhythmia and classify it in one of the 16 groups. The dataset consists of 452 samples and we use the first 12 features among which the gender is the sensitive feature. For our purpose, we construct binary labels between `class 01' (`normal') and all other classes (different classes of arrhythmia and unclassified ones).
· COMPAS (Correctional Offender Management Profiling for Alternative Sanctions)2 is a commerical tool used by judges, probation and parole officers to estimate a criminal defendant's likelihood to re-offend algorithmically. The COMPAS dataset contains the criminal records within 2 years after the decision. We use race (African-American and Caucasian, which accounts for 5278 samples) as the sensitive attribute.
1https://archive.ics.uci.edu/ml/datasets/arrhythmia 2https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis

Testing Group Fairness via Optimal Transport Projections
· Drug (Fehrman et al., 2017) contains answers of 1885 participants on their use of 17 legal and illegal drugs. We concern the cannabis usage as a binary problem, where the label is `Never used' VS `Others' (`used'). There are 12 features including age, gender, education, country, ethnicity, NEO-FFI-R measurements, impulsiveness measured by BIS-11 and sensation seeing measured by ImpSS. Among those, we choose ethnicity (black vs others) as the sensitive attribute.
References
Barocas, S. and Selbst, A. D. Big data's disparate impact. California Law Review, 104:671­732, 2016.
Berk, R., Heidari, H., Jabbari, S., Kearns, M., and Roth, A. Fairness in criminal justice risk assessments: The state of the art. Sociological Methods & Research, pp. 0049124118782533, 2018.
Besse, P., del Barrio, E., Gordaliza, P., and Loubes, J.-M. Confidence intervals for testing disparate impact in fair learning. arXiv preprint arXiv:1807.06362, 2018.
Billingsley, P. Convergence of Probability Measures. John Wiley & Sons, 2013.
Black, E., Yeom, S., and Fredrikson, M. Fliptest: fairness testing via optimal transport. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, pp. 111­121, 2020.
Blanchet, J., Kang, Y., and Murthy, K. Robust Wasserstein profile inference and applications to machine learning. Journal of Applied Probability, 56(3):830­857, 2019.
Buolamwini, J. and Gebru, T. Gender shades: Intersectional accuracy disparities in commercial gender classification. In Conference on Fairness, Accountability and Transparency, pp. 77­91, 2018.
Calders, T. and Verwer, S. Three naive Bayes approaches for discrimination-free classification. Data Mining and Knowledge Discovery, 21(2):277­292, 2010.
Chouldechova, A. Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. Big Data, 5 (2):153­163, 2017.
Chouldechova, A. and Roth, A. A snapshot of the frontiers of fairness in machine learning. Communications of the ACM, 63(5):82­89, 2020.
Cisneros-Velarde, P., Petersen, A., and Oh, S.-Y. Distributionally robust formulation and model selection for the graphical lasso. In International Conference on Artificial Intelligence and Statistics, pp. 756­765. PMLR, 2020.
Corbett-Davies, S., Pierson, E., Feller, A., Goel, S., and Huq, A. Algorithmic decision making and the cost of fairness. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 797­806, 2017.
Dastin, J. Amazon scraps secret AI recruiting tool that showed bias against women. San Fransico, CA: Reuters. Retrieved on October, 9:2018, 2018.
Datta, A., Tschantz, M. C., and Datta, A. Automated experiments on ad privacy settings: A tale of opacity, choice, and discrimination. Proceedings on Privacy Enhancing Technologies, 2015(1):92­112, 2015.
DiCiccio, C., Vasudevan, S., Basu, K., Kenthapadi, K., and Agarwal, D. Evaluating fairness using permutation tests. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 1467­ 1477, 2020.
Donini, M., Oneto, L., Ben-David, S., Shawe-Taylor, J. S., and Pontil, M. Empirical risk minimization under fairness constraints. In Advances in Neural Information Processing Systems, pp. 2791­2801, 2018.
Dua, D. and Graff, C. UCI machine learning repository, 2017. URL http://archive.ics.uci.edu/ml.
Durrett, R. Probability: Theory and Examples. Cambridge University Press, 2019.

Testing Group Fairness via Optimal Transport Projections
Dwork, C., Hardt, M., Pitassi, T., Reingold, O., and Zemel, R. Fairness through awareness. In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference, pp. 214­226, 2012.
Fehrman, E., Muhammad, A. K., Mirkes, E. M., Egan, V., and Gorban, A. N. The five factor model of personality and evaluation of drug consumption risk. In Data Science, pp. 231­242. Springer, 2017.
Garg, S., Perot, V., Limtiaco, N., Taly, A., Chi, E. H., and Beutel, A. Counterfactual fairness in text classification through robustness. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, pp. 219­226, 2019.
Gordaliza, P., Barrio, E. D., Fabrice, G., and Loubes, J.-M. Obtaining fairness using optimal transport theory. In Proceedings of the 36th International Conference on Machine Learning, pp. 2357­2365, 2019.
Grgic-Hlaca, N., Zafar, M. B., Gummadi, K. P., and Weller, A. The case for process fairness in learning: Feature selection for fair decision making. In NIPS Symposium on Machine Learning and the Law, volume 1, pp. 2, 2016.
Härdle, W. Applied Nonparametric Regression. Cambridge University Press, 1990.
Hardt, M., Price, E., Price, E., and Srebro, N. Equality of opportunity in supervised learning. In Advances in Neural Information Processing Systems 29, pp. 3315­3323, 2016.
Hui, Y., Xie, J., Blanchet, J., and Glynn, P. Empirical optimal transport projections with non-symmetric costs. preprint, 2021.
John, P. G., Vijaykeerthy, D., and Saha, D. Verifying individual fairness in machine learning models. In Conference on Uncertainty in Artificial Intelligence, pp. 749­758. PMLR, 2020.
Kleinberg, J., Ludwig, J., Mullainathan, S., and Rambachan, A. Algorithmic fairness. In AEA Papers and Proceedings, volume 108, pp. 22­27, 2018.
Lipton, Z., McAuley, J., and Chouldechova, A. Does mitigating ML's impact disparity require treatment disparity? In Advances in Neural Information Processing Systems, pp. 8125­8135, 2018.
Makhlouf, K., Zhioua, S., and Palamidessi, C. On the applicability of ML fairness notions. arXiv preprint arXiv:2006.16745, 2020.
Manrai, A. K., Funke, B. H., Rehm, H. L., Olesen, M. S., Maron, B. A., Szolovits, P., Margulies, D. M., Loscalzo, J., and Kohane, I. S. Genetic misdiagnoses and the potential for health disparities. New England Journal of Medicine, 375(7): 655­665, 2016.
Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., and Galstyan, A. A survey on bias and fairness in machine learning. arXiv preprint arXiv:1908.09635, 2019.
MultiMedia LLC. Machine Bias, 2016. Available at https://www.propublica.org/article/machine-bias-risk-assessments-incriminal-sentencing.
Owen, A. B. Empirical Likelihood. CRC Press, 2001.
Pleiss, G., Raghavan, M., Wu, F., Kleinberg, J., and Weinberger, K. Q. On fairness and calibration. In Advances in Neural Information Processing Systems, pp. 5680­5689, 2017.
Saleiro, P., Kuester, B., Hinkson, L., London, J., Stevens, A., Anisfeld, A., Rodolfa, K. T., and Ghani, R. Aequitas: A bias and fairness audit toolkit. arXiv preprint arXiv:1811.05577, 2018.
Silvia, C., Ray, J., Tom, S., Aldo, P., Heinrich, J., and John, A. A general approach to fairness with optimal transport. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 3633­3640, 2020.
Taskesen, B., Blanchet, J., Kuhn, D., and Nguyen, V. A. A statistical test of probabilistic fairness. Accepted to ACM Conference on Fairness, Accountability, and Transparency, 2021.
Tramer, F., Atlidakis, V., Geambasu, R., Hsu, D., Hubaux, J.-P., Humbert, M., Juels, A., and Lin, H. Fairtest: Discovering unwarranted associations in data-driven applications. In 2017 IEEE European Symposium on Security and Privacy (EuroS&P), pp. 401­416. IEEE, 2017.

Testing Group Fairness via Optimal Transport Projections
Tsybakov, A. B. Introduction to Nonparametric Estimation. Springer, 2008.
Villani, C. Optimal Transport: Old and New, volume 338. Springer, 2008.
Xue, S., Yurochkin, M., and Sun, Y. Auditing ML models for individual bias and unfairness. In International Conference on Artificial Intelligence and Statistics, pp. 4552­4562. PMLR, 2020.
Zafar, M. B., Valera, I., Gomez Rodriguez, M., and Gummadi, K. P. Fairness beyond disparate treatment & disparate impact: Learning classification without disparate mistreatment. In Proceedings of the 26th International Conference on World Wide Web, pp. 1171­1180, 2017.
Zehlike, M., Hacker, P., and Wiedemann, E. Matching code and law: achieving algorithmic fairness with optimal transport. Data Mining and Knowledge Discovery, 34(1):163­200, 2020.

