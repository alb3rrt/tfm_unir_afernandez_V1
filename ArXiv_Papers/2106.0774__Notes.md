
# Optimizing Functionals on the Space of Probabilities with Input Convex Neural Networks

[arXiv](https://arxiv.org/abs/2106.0774), [PDF](https://arxiv.org/pdf/2106.0774.pdf)

## Authors

- David Alvarez-Melis
- Yair Schiff
- Youssef Mroueh

## Abstract

Gradient flows are a powerful tool for optimizing functionals in general metric spaces, including the space of probabilities endowed with the Wasserstein metric. A typical approach to solving this optimization problem relies on its connection to the dynamic formulation of optimal transport and the celebrated Jordan-Kinderlehrer-Otto (JKO) scheme. However, this formulation involves optimization over convex functions, which is challenging, especially in high dimensions. In this work, we propose an approach that relies on the recently introduced input-convex neural networks (ICNN) to parameterize the space of convex functions in order to approximate the JKO scheme, as well as in designing functionals over measures that enjoy convergence guarantees. We derive a computationally efficient implementation of this JKO-ICNN framework and use various experiments to demonstrate its feasibility and validity in approximating solutions of low-dimensional partial differential equations with known solutions. We also explore the use of our JKO-ICNN approach in high dimensions with an experiment in controlled generation for molecular discovery.

## Comments



## Source Code

Official Code



Community Code



## Bibtex

```tex
@misc{alvarezmelis2021optimizing,
      title={Optimizing Functionals on the Space of Probabilities with Input Convex Neural Networks}, 
      author={David Alvarez-Melis and Yair Schiff and Youssef Mroueh},
      year={2021},
      eprint={2106.00774},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
```

## Notes

Type your reading notes here...

