A Non-commutative Extension of Lee-Seung's Algorithm for Positive Semidefinite Factorizations

Yong Sheng Soh 

Antonios Varvitsiotis

June 2, 2021

arXiv:2106.00293v1 [math.OC] 1 Jun 2021

Abstract
Given a data matrix X  Rm +×n with non-negative entries, a Positive Semidefinite (PSD) factorization of X is a collection of r × r-dimensional PSD matrices {Ai} and {Bj} satisfying the condition Xij = tr(AiBj) for all i  [m], j  [n]. PSD factorizations are fundamentally linked to understanding the expressiveness of semidefinite programs as well as the power and limitations of quantum resources in information theory. The PSD factorization task generalizes the Non-negative Matrix Factorization (NMF) problem in which we seek a collection of rdimensional non-negative vectors {ai} and {bj} satisfying Xij = aTi bj, for all i  [m], j  [n] ­ one can recover the latter problem by choosing matrices in the PSD factorization to be diagonal. The most widely used algorithm for computing NMFs of a matrix is the Multiplicative Update algorithm developed by Lee and Seung, in which non-negativity of the updates is preserved by scaling with positive diagonal matrices. In this paper, we describe a non-commutative extension of Lee-Seung's algorithm, which we call the Matrix Multiplicative Update (MMU) algorithm, for computing PSD factorizations. The MMU algorithm ensures that updates remain PSD by congruence scaling with the matrix geometric mean of appropriate PSD matrices, and it retains the simplicity of implementation that the multiplicative update algorithm for NMF enjoys. Building on the Majorization-Minimization framework, we show that under our update scheme the squared loss objective is non-increasing and fixed points correspond to critical points. The analysis relies on Lieb's Concavity Theorem. Beyond PSD factorizations, we show that the MMU algorithm can be also used as a primitive to calculate block-diagonal PSD factorizations and tensor PSD factorizations. We demonstrate the utility of our method with experiments on real and synthetic data.

1 Introduction

Let X  R+m×n be a m × n dimensional matrix with non-negative entries and r  N a user-specified parameter. An r-dimensional positive semidefinite (PSD) factorization of X is given by two families
of r × r PSD matrices A1, . . . , Am and B1, . . . , Bn satisfying

Xij = tr(AiBj), i  [m], j  [n].

(1)

Every non-negative matrix admits an r-dimensional PSD factorization for an appropriate value of r  N­we may, for instance, take Ai = diag(Xi:) and Bj = diag(ej), a choice that corresponds
Y. S. Soh (email: matsys@nus.edu.sg) is with the Department of Mathematics, National University of Singapore and the Institute of High Performance Computing, Agency for Science, Technology and Research.
A. Varvitsiotis (email: avarvits@gmail.com) is with the Engineering Systems and Design Pillar, Singapore University of Technology and Design.

1

to an n-dimensional PSD factorization. The smallest r  N for which X admits an r-dimensional PSD factorization is called the PSD-rank [7].
PSD factorizations are of fundamental importance to a wide range of areas, most notably towards understanding the expressive power of linear optimization over the cone of positive semidefinite matrices, [13, 10], studying the power and limitations of quantum resources within the framework of information theory [16, 10], and as a natural non-commutative generalization of the extremely popular dimensionality reduction technique of non-negative matrix factorizations (NMFs) [30, 24, 25]. We elaborate further on the relevance of PSD factorizations to each of these areas below.

Links to Semidefinite Programming. A Semidefinite Program (SDP) is a convex optimization problem in which we minimize a linear function over the set of PSD matrices intersected with an affine subspace. SDPs are a powerful generalization of Linear Programs with extensive modeling power and tractable algorithms for solving them, e.g., see [36] and references therein. SDPs are frequently used as convex relaxations to combinatorial problems, and have many important applications including optimal power flow computation [22], robustness certification to adversarial examples in neural networks [31], and inference in graphical models [6].
Given a bounded polytope P = {x  Rd : ci x  di, i  [ ]} = conv(v1, .., vk), a basic question concerning the expressive power of SDPs is to find the smallest possible SDP description of P , i.e., the minimum r  N for which we can express P as the projection of an the affine slice of the cone of r × r PSD matrices. Concretely, the goal in this setting is to express P as:

P = (Sr+  L),

(2)

where Sr+ is the cone of r × r PSD matrices, L is an affine subspace of the space of r × r symmetric matrices and  a linear projection from the space of r×r symmetric matrices to Rd. A representation of the form (2) is called an extended formulation or PSD-lift of P and is extremely useful for
optimization purposes. Indeed, the existence of a PSD-lift immediately implies that min{ c, x : x  P } = min{  (c), y : y  Sr+  L}, and consequently, linear optimization over P (which can be hard) corresponds to an SDP (which can be solved efficiently).
To describe the connection of PSD factorizations with SDP-lifts, let SP be the slack matrix of P , namely, SP is a rectangular matrix whose rows are indexed by the facets of P , its columns indexed by extreme points vj, and the ij-entry of SP corresponds to the slack between the i-th facet and the j-th vertex, Sij = di - ci vj. Generalizing a seminal result by Yannakakis for LPs [38], it was shown independently in [13] and [10] that if SP admits an r-dimensional PSD factorization, the polytope P admits a PSD-lift over the cone of r × r PSD matrices. The proof is also constructive
­ given a PSD factorization of S, there is an explicit description of L and  that gives rise to P .
An important special case of the PSD factorization problem is when the PSD factors are block-
diagonal PSD matrices, where both the number of blocks and the size of each block is fixed, i.e., Ai, Bj  (Sr+)k.For a fixed and user-specified r  N, the least k  N for which X  Rm+×n admits a PSD factorization with PSD-factors in (Sr+)k is called the r-block diagonal PSD-rank of X. In terms of the geometric interpretation of PSD factorizations, block-diagonal PSD factorizations of the
slack matrix SP correspond to extended formulations of P over a Cartesian product of PSD cones, i.e., P = ((Sr+)k  L). In terms of relevance to optimization, extended formulations over (Sr+)k allow to perform linear optimization over P by solving block-diagonal SDPs, which can be solved
numerically much faster compared to dense SDPs. In fact, most interior-point algorithms for SDPs
are designed to exploit block-diagonal structure if it present in the problem. The first systematic
study of block-diagonal PSD-lifts was given in [8], where the focus was mainly on lower bounds.

2

Links to quantum information theory. Consider two parties, Alice and Bob, that try to generate samples (i, j) following some joint distribution P (i, j). For this section, it is crucial to think of the distribution P (i, j) as being arranged in an entrywise non-negative matrix, and we use P to simultaneously refer to both the distribution and its matrix representation. Clearly, if P is not a product distribution, Alice and Bob should either communicate or share some common information to be able to generate samples according to P . In the correlation generation problem, the goal is to find the least amount of shared resources that are needed to achieve this task. The considered resources can be either classical (shared randomness), quantum (shared entangled state) or hybrid.
In the quantum case, correlation generation boils down to finding a quantum state   Sr+2 with tr() = 1 and quantum measurements Ei, Fj (i.e., Ei, Fj  Sr+ and i Ei = j Fj = I) such that

P (i, j) = tr((Ei  Fj)), i  [m], j  [n].

(3)

The least r  N for which a factorization of the form (3) is possible is given by the (logarithm) of the psd-rank of the matrix P [16]. Moreover, the proof of [16] is constructive, in the sense that,
given a r-dimensional PSD factorization of P , there is an explicit description of a quantum state   Sr+2 and measurement operators acting on Cr that satisfy (3), e.g., see [7, Proposition 3.8].
Moving beyond purely quantum protocols, there has been recent interest in hybrid classical-
quantum protocols, motivated by the fact that near-term quantum devices can only operate reliably
on a limited number of qubits [27]. Specifically, assuming that their quantum capabilities are limited
to manipulating s qubits, hybrid classical-quantum protocols that allow to generate samples from a
joint distribution P (i, j), correspond to PSD factorizations of P where the PSD factors are blockdiagonal, with block-size at most 2s. Moreover, the minimum amount of classical resources required in a classical-quatum protocol with s qubits, is given by the 2s-block diagonal PSD-rank of P .

Links to nonnegative matrix factorizations. An r-dimensional nonnegative matrix factorization (NMF) of X  Rm+×n [30, 24] is specified by two families of r-dimensional entrywise nonnegative vectors a1, . . . , am  Rr+ and b1, . . . , bn  Rr+ satisfying

Xij = ai, bj , i  [m], j  [n].

(4)

NMF is a widely used dimensionality reduction tool that gives parts-based representation of the
input data, as it only allows for additive, not subtractive, combinations. To make this point clear, note that an equivalent reformulation of an r-dimensional NMF (4) is X = AB where A  Rm+×r is the matrix whose rows are the ai's and the matrix B  Rr+×n has as columns the bj's, or equivalently,

X:j  cone(A:1, . . . , A:r), j  [n].

(5)

The equivalent viewpoint for NMFs given in (5) is more amenable to interpretation, as it gives a representation of each column of X (i.e., each data point) as nonnegative (and thus additive) combination of the r columns of A, and the columns of B give the coefficients of the conic combination. NMF factorizations have applications in many areas, notable examples including document clustering [37], music analysis [9], speech-source separation [32] and cancer-class identification [11]. For a comprehensive discussion on NMFs the reader is referred to the survey [12] and references therein.
NMF factorizations are a special case of PSD factorizations where the r × r PSD matrices Ai and Bj are diagonal, i.e., we have that Ai = diag(ai) and Bj = diag(bj) for some vectors ai, bj  Rr+ (recall that a diagonal matrix is PSD iff its diagonal entries are nonnegative). Moreover, given a PSD factorization of X, (i.e., Xij = tr(AiBj)) for which all the PSD factors Ai, Bj commute (and thus can be simultaneously diagonalized), corresponds to an NMF factorization. In this sense, PSD factorizations are a natural non-commutative generalization of NMF factorizations.

3

Interpretability of PSD factorizations. An equivalent way to define an r-dimensional NMF for a data matrix X (cf. (4)) is through the existence of a liner mapping A : Rr  Rm satisfying

X:j  A(Rr+) for all j  [n] and A(Rr+)  Rn+.

(6)

Consequently, the mapping A (or rather, the image of the extreme rays of the cone Rr+ under A), describe a latent space that can generate all data points X:j via nonnegative combinations.
Analogously, in the setting of PSD factorizations, the existence of an r-dimensional PSD factorization of X (cf. (1)) is equivalent to the existence of a linear mapping A : Sr  Rm satisfying

X:j  A(Sr+) for all j  [n] and A(Sr+)  Rn+.

(7)

Comparing (6) and (7), the difference between NMF and PSD factorizations is immediately appar-

ent. In the setting of PSD factorizations the latent pace is infinite-dimensional, and specifically, it

is the image of the extreme rays of the cone of r × r PSD matrices (i.e., all matrices uu where

u  Rr) under A. In this latent space, each data point X:j is represented by a PSD matrix

Bj  Sr+, and using its spectral decomposition Bj =

r i=1

iuiui

,

leads

to

the

representation

X:j = i i A(uiui ). Additional details and explicit examples demonstrating the qualitative dif-

ference in expressive power between NMF and PSD factorizations are given in Section 6.

2 Prior Works on PSD Factorizations and Summary of Results

A canonical starting point for finding an (approximate) r-dimensional PSD factorization of a given matrix X  Rm+×n is to solve the non-convex optimization problem

inf (Xij - tr(AiBj))2 s.t. A1, . . . , Am, B1, . . . Bn  Sr+,

(8)

i,j

aiming to find an approximate r-dimensional PSD factorization that minimizes the square loss over all entries of X. Fixing one of the two families of matrix variables, say the Ai's, problem (8) is separable with respect to B1, . . . , Bm. Consequently, a reasonable solution approach for (8) is to alternate between updating the Ai's and Bj's by solving the sub-problems:

Ai  arg inf

(Xij - tr(AiBj))2 s.t. A1, . . . , Am  Sr+

(9)

i,j

Bj  arg inf

(Xij - tr(AiBj))2 s.t. B1, . . . , Bn  Sr+

(10)

i,j

The two sub-problems in each update step are symmetric in the variables Ai and Bj, with the small modification where we replace X with its transpose. As such, for the remainder of this discussion, we only focus on the sub-problem (10) corresponding to fixing the Ai's and updating the Bj's. Moreover, (10) is separable with respect to each variable Bi, so it suffices to focus on

inf (Xij - tr(AiBj))2 s.t. Bj  Sr+.

(11)

i

Lastly, to simplify notation we omit subscripts, and specifically, we denote by x the j-th column of X and by B the PSD matrix variable Bj. Defining A : Sr  Rm to be the linear map A(Z) = ( A1, Z , . . . , Am, Z ) , problem (11) can be then equivalently written as

inf

x - A(B)

2 2

s.t. B  Sr+.

(12)

The optimization problem (12) is convex, and in fact, falls within the well-studied class of convex quadratic SDPs. Nevertheless, there is no closed-form solution for this family of optimization problems, and consequently, typical solution strategies rely on numerical optimization, e.g., see [33].

4

Summary of results. In this paper we introduce and study an iterative algorithm (Algorithm 1) we call the Matrix Multiplicative Update (MMU) algorithm for computing PSD factorizations. The MMU algorithm builds on the Majorization-Minimization framework, and as discussed in the previous section, the main workhorse is an iterative algorithm for the convex quadratic SDP (12).
From a computational perspective, the iterates of the MMU algorithm are updated via conjugation with appropriately defined matrices, so our method has the advantage of being simple to implement and moreover, the PSDness of the iterates is automatically guaranteed. From a theoretical perspective, the squared loss objective is non-increasing along the algorithms' trajectories (Theorem 1) and moreover, its fixed points satisfy the first-order optimality conditions (Theorem 2). The analysis of the MMU algorithm relies on the use of several operator trace inequalities (including Von Neumann's trace inequality and Lieb's Concavity Theorem).
An important feature of the MMU algorithm is that if it is initialized with block-diagonal PSD matrices, the same block-diagonal structure is preserved throughout its execution, which leads to an algorithm for calculating block-diagonal PSD factorizations. In particular, in Section 5 we show that if the MMU algorithm is initialized with diagonal PSD matrices, the iterates remain diagonal PSD throughout, and as it turns out, our algorithm in this case reduces to Lee-Seung's seminal Multiplicative Update algorithm for computing NMFs [25]. Moreover, we show how to the MMU algorithm can be used as a primitive to calculate PSD factorizations of nonnegative tensors. In terms of numerical experiments, in Section 6 we demonstrate the utility of our method for both synthetic (random Euclidean Distance matrices) and real data (CBCL image dataset).
Existing work. All existing algorithms for computing PSD factorizations employ the alternating minimization approach described in the previous section, where we fix one set of variables and minimize over the other, and essentially boil down into finding algorithms for the convex problem (12).
Projected Gradient Method (PGM). The first approach for computing PSD factorizations is based on applying PGM to (12), alternating between a gradient step to minimize the objective and a projection step onto the set of PSD matrices [35]. The latter projection step uses the following useful fact: Given the spectral decomposition C = U diag(i)U of a matrix C  Sn, the projection onto the PSD cone is U diag(max(0, i))U [14]. The vanilla PGM has slow convergence rate, so the authors in [35] also propose an accelerated variant that incorporates a momentum term.
Coordinate Descent. The authors in [35] also propose a different algorithm combining the ideas of coordinate descent and a change of variables that allows them to also control the rank of the PSD factors, which was popularized by the seminal work of Burer and Monteiro for solving rankconstrained SDPs [3]. Concretely, the authors use the parameterization Ai = aiai , and Bj = bjbj , where ai  Rr×rAi , and bj  Rr×rBj for some fixed rAi, rBj  N, and optimize using a coordinate descent scheme over the entries of the matrices ai and bj. In this setting, problem (12) is a quartic polynomial in the entries of b. Thus, its gradient is a cubic polynomial, and its roots can be found using Cardano's method (and careful book-keeping).
Connections to Affine Rank Minimization and Phase Retrieval. A different set of algorithms developed in [18, 19, 20] is based on the connections between computing PSD factorizations with the affine rank minimization (ARM) and the phase retrieval (PR) in signal procesing. First, recall that the PSD-ARM problem focuses on recovering a low-rank matrix from affine measurements:
min rank(B) s.t. A(B) = x, B  Sr+.
Here, A is a known linear map representing measurements while x is known vector of observations. Due to the non-convexity of the rank function, a useful heuristic initially popularized in the control
5

community is to replace the rank by the trace function, e.g., see [28] and [29], in which case the resulting problem is an instance of an SDP. A different heuristic for PSD-ARM is to find a PSD matrix of rank at most k that minimizes the squared loss function, i.e.,

inf

x - A(B)

2 2

s.t.

B  Sr+, rank(B)  k,

(13)

where alternatively, the rank constraint can be enforced by parametrizing the PSD matrix variable B  Sr+ as B = bb with b  Rr×k. The point of departure for the works [18, 20, 19] is that problem (13) corresponds exactly to the sub-problem (12) encountered in any alternate minimization strategy for computing PSD factorizations, albeit with an additional rank constraint. In view of this, any algorithm from the signal processing literature developed for ARM can be applied to (12).
The main algorithms considered in [18, 19, 20] are Singular Value Projection (SVP) [15], Procrustes Flow [34], and variants thereof. In terms of convergence guarantees, for affine maps A obeying the Restricted Isometry Property [4], both algorithms converge to an optimal solution. Nevertheless, it is unclear whether these guarantees carry over when applied to the PSD factorization problem.

Roadmap. In Section 3 we derive our MMU algorithm for computing PSD factorizations and in Section 4 we show that its fixed points correspond to KKT points. In Section 5 we give various theoretical applications of the MMU algorithm and in Section 6 we go from theory to practise and apply the MMU algorithm to synthetic and real datasets.

3 A Matrix Multiplicative Update Algorithm for PSD Factorizations

In this section we describe our algorithm for computing (approximate) PSD factorizations of a matrix X. As already discussed, we employ an alternate optimization approach where we alternate between optimizing with respect to the Ai's and the Bj's. The sub-problem in each update step symmetric in the variables Ai and Bj, with the small modification where we replace X with its transpose. As such, in the remainder of this discussion, we assume without loss of generality that the Ai's are fixed and we update the Bj's. The resulting sub-problem we need to solve is (12).

Majorization-Minimization (MM) Framework. Our algorithm is an instance of the (MM) framework, e.g. see [21] and references therein. To briefly describe this approach, suppose we need to solve the optimization problem min{F (x) : x  X }. The MM framework relies on the existence of a parametrized family of auxilliary functions ux : X  R, one for each x  X , where:

F (y)  ux(y), for all y  X and F (x) = ux(x).

(14)

Based on these two properties, F is nonincresasing under the update rule:

xnew = argmin{uxold (y) : y  X },

(15)

as can be easily seen by the following chain of inequalities

f (xnew)  uxold (xnew)  uxold (xold) = f (xold).

We conclude with two important remarks concerning the MM framework. First, note that although the iterates generated by the MM update rule (23) are nonincreasing in objective function value, there is in general no guarantee that they converge to a minimizer. Secondly, for the MM approach to be of any use, the auxilliary functions employed at each iteration need to be easy to optimize.

6

Matrix Geometric Mean. Our choice of auxilliary functions relies on the well-studied notion of a geometric mean between a pair of positive definite matrices, whose definition we recall next. For additional details and omitted proofs the reader is referred to [23] and [2]. The matrix geometric mean of two positive definite matrices C and D is given by

C#D = C1/2(C-1/2DC-1/2)1/2C1/2,

(16)

or equivalently, it is the unique positive definite solution of the Riccati equation

XC-1X = D,

(17)

in the matrix variable X. The matrix geometric mean also has a nice geometric interpretation in terms of the Riemannian geometry of the manifold of positive definite matrices, and specifically, C#D is the midpoint of the unique geodesic joining C and D. Finally, the matrix geometric mean is symmetric in its two arguments C#D = D#C and also satisfies (C#D)-1 = C-1#D-1.

The MMU Algorithm for PSD Factorizations. The main step for deriving our algorithm for approximately computing PSD factorizations is to apply the MM framework, with a meticulously chosen auxilliary function, to the convex quadratic SDP (12). Our main result is the following:

Theorem 1. Consider a fixed vector x  Rm+ and let A : Sr  Rm be the linear map defined by

Z  A(Z) = (tr(A1Z), . . . , tr(AmZ)) , for some fixed r × r positive definite matrices A1, . . . , Am.

Then, the objective function

x - A(B)

2 2

is

non-increasing

under

the

update

rule

Bnew = W (A x)W, where W = ([A A](Bold))-1#(Bold),

and moreover, if initialized with a positive definite matrix, the iterates remain positive definite.

Proof. First, note that if the Ai's and Bold are all positive definite, the update rule is well-defined.

Indeed, we have [A A](Bold) =

m k=1

tr(Ak

Bold)Ak

is

also

positive

definite,

and

thus

invertible.

Set F (B) :=

x - A(B)

2 2

and

define

the

function

uBold (B) := F (Bold) + F (Bold), B - Bold + B - Bold, T (B - Bold)),

(18)

where T : Sr  Sr is the operator given by

T (Z) = W -1ZW -1 and W = ([A A](Bold))-1#(Bold).

The claim of the theorem will follow as an immediate consequence of the MM framework, as long as
we establish that uBold(B) is an auxilliary function, i.e., it satisfies the two properties given in (14). Clearly, we have that uBold(Bold) = F (Bold), so it only remains to show the domination property,
that is, uBold(B)  F (B), for all B  Sr+. In fact, we show a slightly stronger result, namely that uBold(B)  F (B) holds for all symmetric matrices B  Sr. To see this we use the second order Taylor expansion of F at Bold, which as F is quadratic in B, is given by

F (B) = F (Bold) + F (Bold), B - Bold + A(B - Bold) 22.

(19)

Comparing the expressions (18) and (19), to show that F (B)  uBold(B) for all B  Sr it suffices to check that the operator T - A A is positive; i.e., Z, [T - A A](Z)  0 for any matrix Z  Sr. This claim is the main technical part of the proof, deferred to Lemma 3 in the Appendix.

7

Furthermore, the fact that T -A A is a positive operator, also implies that T is itself a positive operator. Consequently, the MM update (23) obtained by using the auxilliary function (18) can be calculated just by setting the gradient equal to zero, and is given by

Bnew = Bold - T -1 [A A](Bold) - A (x) .

(20)

Moreover, as T -1(Z) = W ZW and W = ([A A](Bold))-1#(Bold) it follows that

Bold = W ([A A](Bold))W = T -1([A A](Bold)),

(21)

where for the first equality we use the unicity property of the matrix geometric mean (recall (17)). Subsequently, using (21), the MM update rule in (20) simplifies to the following:

Bnew = T -1(A x) = W (A x)W.

(22)

Lastly, since the Ai's are PSD, it follows that A x = i xiAi is a conic combination of PSD matrices (recall that x  Rm+ ), and thus, it is itself PSD. Consequently, Bnew is PSD. In fact, if the matrices Ai and Bold are positive definite, the updated matrix Bnew is also positive definite.

Having established an iterative method for problem (12) that is non-increasing in value and retains PSDness, we can incorporate this as a sub-routine in our alternating optimization scheme for computing PSD factorizations. The pseudocode of the resulting method is given in Algorithm 1.

Algorithm 1 Matrix Multiplicative algorithm for computing PSD factorizations Input: A matrix X  Rm0×n, parameter r  N Output: {A1, . . . , Am}, {B1, . . . , Bn}  Sr+, Xij  tr(AiBj) for all i, j
while stopping criterion not satisfied:

Ai  Vi(B xi)Vi where Vi = ([B B](Ai))-1#Ai, xi = Xi:

Bj  Wj(A xj)Wj where Wj = ([A A](Bj))-1#(Bj), xj = X:j

(23)

4 Fixed Points of the MMU Algorithm

In this section, we show that the fixed points of the MMU algorithm satisfy the Karush-KuhnTucker (KKT) optimality conditions for problem (8). Letting {Ai }i[m], {Mi}i[m] and {Bj}j[n], {j }j[n] be pairs of primal-dual optimal solutions of (8) with zero duality gap, it is straightforward to verify
that the KKT conditions are

tr(Ai Mi) = tr Bjj = 0, i  [m], j  [n] B (X:i) - [B B](Ai ) = Mi, i  [m] A (X:j) - [A A](Bj) = j , j  [n].

Furthermore, assuming that the primal optimal solutions {Ai }i[m] and {Bj}j[n] are all positive definite, it follows immediately from the complementary slackness conditions that Mi = j = 0 for all i  [m], j  [n]. Consequently, in the special case of positive definite optimal solutions {Ai }i[m] and {Bj}j[n] , the KKT conditions reduce to

B (X:i) = [B B](Ai ), i  [m] and A (X:j) = [A A](Bj), j  [n].

(24)

8

Based on the preceding discussion, in the next result (whose proof follows by Lemma 4 in the Appendix) shows that we can interpret our MMU algorithm as a fixed-point method for satisfying the KKT optimality conditions corresponding to problem (8).
Theorem 2. If {Ai}i[m] and {Bj}j[n] are positive definite fixed points of the update rule of the MWU algorithm given in (23), then they also satisfy the KKT conditions (24).

5 Applications of the MMU algorithm

Block-diagonal (BD) PSD factorizations. If the MMU algorithm is initialized with BD

positive definite matrices with the same block structure, the BD structure is preserved at each

update. Indeed, as [A A](Bold) =

m k=1

tr(Ak Bold

)Ak

we

see

that

[A

A](Bold)-1,

and

thus,

([A A](Bj))-1#(Bj) share the same block structure. Lastly, by definition of the MMU algo-

rithm (23), Bnew is also block-diagonal with the same structure. Thus, if initialized with BD-PSD

matrices, the MMU algorithm gives a method for computing a BD-PSD factorization.

Recovering Lee-Seung's algorithm for NMF. Diagonal matrices can be considered as blockdiagonal in a trivial manner. Nevertheless, by the preceding discussion, if initialized with diagonal PSD matrices, the iterates of the MMU algorithm remain diagonal PSD throughout. In this special case, our MMU algorithm reduces to Lee-Seung's (LS) seminal Multiplicative Update algorithm for computing NMFs [25]. LS's algorithm is perhaps the most widely used method for computing NMFs as it has succeeded to identify meaningful features in a diverse collection of real-life data sets and is extremely simple to implement. Specifically, LS's updates are

XB

AX

AA

and B  B 

,

(25)

ABB

A AB

where X  Y, X/Y denote the componentwise multiplication, division of two matrices respectively. Setting Ai = diag(ai) and Bj = diag(bj), the MMU algorithm updates Bj as

m

-1 m

Bj  Bj

ai, bj Ai

Xij Ai ,

i=1

i=1

which is also a diagonal PSD matrix. Setting A = a1 . . . am and B = b1 . . . bn , we immediately recover LS's update rule (25).

PSD factorizations for nonnegative tensors. Motivated by PSD factorizations of nonnegative
matrices, [17] define an r-dimensional PSD factorization of a nonnegative tensor T (with n indices of dimension d) as a collection of PSD matrices Ci(11), . . . , Ci(nn)  Sr+ for all ik  [d] such that

r

Ti1,...,in =

Ci(11)(i, j) · · · Ci(nn)(i, j), for all ik  [d].

i,j=1

(26)

Equivalently, and more succinctly, we may write (26) as

Ti1...in = sum(Ci(11)  · · ·  Ci(nn)), for all ik  [d],
where  denotes the Schur product of matrices and sum(X) = ij Xij. The motivation for studying tensor PSD factorizations comes from the fact that they characterize the quantum correlation

9

complexity for generating multipartite classical distributions [17]. We now show how our MMU algorithm can be used as a primitive to calculate tensor PSD factorizations. For simplicity of presentation we restrict to n = 3 and consider

inf

Ti1i2i3 - sum(Ci(11)  Ci(22)  Ci(33)) 2

i1,i2,i3

subject to Ci(11), Ci(22), Ci(33)  Sr+.

(27)

As in the case of PSD factorizations, to solve (27) we employ a block coordinate descent approach.
Specifically, fixing all matrices Ci(11), Ci(22) the optimization problem (27) is separable wrt each Ci(33) for all i3  [d]. Thus, we need to solve the optimization problem

arg inf
C

Ti1i2i3 - Ci(11)  Ci(22), C 2

i1,i2

subject to C  Sr+.

(28)

Defining the map A : Sr  Rd2 where X  ( X, Ci(11)  Ci(22) )i1,i2, problem (28) is equivalent to

arg inf

vec(T::i3) - A(C)

2 2

subject to C  Sr+,

(29)

for all i3  [d]. Note that A : Rd2  Sr where x = (xx1x2 )  i1,i2[d] xi1i2 Ci(11)  Ci(22) is a PSD matrix, as the Schur product of PSD matrices is PSD. Thus, Theorem 1 gives an update rule that
preserves PSDness and for which the objective function (29) is nonincreasing.

6 Numerical experiments

Distance matrices. Let v  Rn be a vector and let M be a n × n matrix whose entries are Mij = (vi - vj)2. M is known as a distance matrix and a 2-dimensional PSD factorization is

Mij = tr(AiBj), where Ai =

1 vi

1 vi

and Bj =

-vj 1

-vj 1

. We generate a random v  Rn

with n = 20 where each entry is drawn from the standard normal distribution. We apply our

algorithm to compute a 2-dimensional factorization whereby we perform 500 iterations over 50

random initializations. For conditioning reasons, we perform a damped update whereby we add

I, = 10-8, to each matrix variable at every iterate. We compute the normalized squared

error loss of the factorization from the data matrix, and we plot the error over each iteration

Err = i,j(tr(AiBj) - Mij)2/ i,j Mi2j in Figure 1. Our experiments suggest that, with sufficient random initializations, our algorithm finds a PSD factorization that is close to being exact.

Normalized Squared Error Log Normalized Squared Error

0.5 0.4 0.3 0.2 0.1 0.0 0

100

200

300

400

500

Number of Iterates

1

2

3

4

5

6

0

100

200

300

400

500

Number of Iterates

Figure 1: Performance of the MWU algorithm for computing a PSD factorization of a distance matrix. Different curves correspond to different random initializations.

10

CBCL Face Image Dataset. In our second experiment we apply the MMU method to compute

a PSD factorization of face image data from the CBCL Face Database [1]. The objective of this

experiment is to explain how computing a PSD factorization can be viewed as a representation

learning algorithm that generalizes NMF. The CBCL dataset comprises 2429 images of faces, each

of size 19 × 19 pixels, linearly scaled so that the pixel intensity has mean 0.5 and standard deviation

0.25, with values subsequently clipped at [0, 1]. The resulting data matrix has size 361 × 2429. Our

experiments were conducted in Python on an Intel 7-th Gen i7 processor at 2.8GHz.

Recall that an r-dimensional PSD factorization of X given by {Ai}, {Bj} gives rise to a repre-

sentation X:j =

i i A(uiui ), where Bj =

r i=1

i

ui

ui

is the spectral decomposition of Bj and

A : Sr  Rm is the linear mapping satisfying Z  (tr(A1Z), . . . , tr(AmZ)). In all figures in this

section, given a specific image X:j in the CBCL dataset, we illustrate the images corresponding to

the building blocks identified by the MMU algorithm, namely A(uiui ) for all eigenvectors of Bj. As our baseline, we compute a 27-dimensional NMF of the CBCL data matrix over 500 iter-

ations. In Figure 2 below we illustrate the decomposition of one of images from the dataset into

these 27 constituents.

Figure 2: Image decomposition into building blocks learned from 27-dimensional NMF. Next, we calculate a 7-dimensional PSD factorization of the CBCL data matrix over 500 iterations and illustrate the decomposition of the same image in the new basis in Figure 3. We compute a factorization of size 7 to match the number of degrees of freedom in Figure 2. We note that the constituents learned from the PSD factorization are less interpretable than those learned using NMF ­ here, we use the word `interpretable' in a loose sense to mean that some factors learned from NMF are easily interpreted to describe local facial features such as the eyes or the nose whereas the constituents learned from PSD are of a global nature. Nevertheless, we note that this phenomenon that has been been also observed for NMF applied to datasets beyond CBCL [26].
Figure 3: Image decomposition into building blocks learned from 7-dimensional PSD factorization. Lastly, we use the MMU algorithm to compute a block-diagonal PSD factorization with 9
blocks, each of size 2, over 500 iterations (the number of degrees of freedom is comparable), which we illustrate the decomposition in Figure 4. In this instance, the constituents contain more local-
11

ized features.
Figure 4: Image decomposition into building blocks learned using 2 × 2-block PSD factorization.
An advantage of learning a continuum of basic building blocks is that one can express certain geometries in the data that is otherwise not possible using a finite number of building blocks ­ as an illustration of this intuition, in Figure 5, we show a continuum of atoms corresponding to a single 2 × 2 block, which capture a continuum between the nose and the nostrils.
Figure 5: Visualization of continuum of building blocks learned using 2×2-block PSD factorization.
Acknowledgments. YS gratefully acknowledges Ministry of Education (Singapore) Academic Research Fund (Tier 1) R-146-000-329-133. AV gratefully acknowledges Ministry of Education (Singapore) Start-Up Research Grant SRG ESD 2020 154 and NRF2019-NRF-ANR095 ALIAS grant.
References
[1] MIT-CBCL Face Database, Center for Biological and Computational Learning, Massachussetts Institute of Technology. http://cbcl.mit.edu/software-datasets/FaceData2.html.
[2] Rajendra Bhatia. Positive Definite Matrices. Princeton University Press, 2007. [3] Samuel Burer and Renato D. C. Monteiro. A Nonlinear Programming Algorithm for Solving
Semidefinite Programs via Low-rank Factorization. Mathematical Programming, 95(2):329­ 357, 2003. [4] Emmanuel J. Candes and Terence Tao. Decoding by Linear Programming. IEEE Transactions on Information Theory, 51:4203­4215, 2004. [5] Eric Carlen. Trace Inequalities and Quantum Entropy: An Introductory Course, 2009. [6] Murat A. Erdogdu, Yash Deshpande, and Andrea Montanari. Inference in graphical models via semidefinite programming hierarchies. In Advances in Neural Information Processing Systems, 2017. [7] Hamza Fawzi, Jo~ao Gouveia, Pablo A. Parrilo, Richard Z. Robinson, and Rekha R. Thomas. Positive Semidefinite Rank. Mathematical Programming, 153(1):133­177, 2015. [8] Hamza Fawzi and Pablo A. Parrilo. Exponential lower bounds on fixed-size psd rank and semidefinite extension complexity. https://arxiv.org/pdf/1311.2571.pdf, 2013.
12

[9] C´edric F´evotte, Nancy Bertin, and Jean-Louis Durrieu. Nonnegative Matrix Factorization with the Itakura-Saito Divergence: With Application to Music Analysis. Neural Computation, 21(3):793­830, 2009.
[10] Samuel Fiorini, Serge Massar, Sebastian Pokutta, Hans Raj Tiwary, and Ronald de Wolf. Exponential Lower Bounds for Polytopes in Combinatorial Optimization. Journal of the ACM, (17), 2015.
[11] Yuan Gao and George Church. Improving Molecular Cancer Class Discovery Through Sparse Non-negative Matrix Factorization. Bioinformatics, 21(21):3970­3975, 2005.
[12] Nicolas Gillis. The Why and How of Nonnegative Matrix Factorization, chapter Regularization, Optimization, Kernels, and Support Vector Machines, pages 257­291. Number Chapman and Hall, CRC in Machine Learning and Pattern Recognition Series. 2014.
[13] Jo~ao Gouveia, Pablo A. Parrilo, and Rekha Thomas. Lifts of Convex Sets and Cone Factorizations. Mathematics of Operations Research, 38(2):248­264, 2013.
[14] Nicholas J. Higham. Computing a Nearest Symmetric Positive Semidefinite Matrix. Linear Algebra and its Applications, 103:03­118, 1988.
[15] Prateek Jain, Raghu Meka, and Inderjit Dhillon. Guaranteed Rank Minimization via Singular Value Projection. In Advances in Neural Information Processing Systems, 2010.
[16] Rahul Jain, Yaoyun Shi, Zhaohui Wei, and Shengyu Zhang. Efficient Protocols for Generating Bipartite Classical Distributions and Quantum States. IEEE Transctions on Information Theory, 59:5171­5178, 2013.
[17] Rahul Jain, Zhaohui Wei, Penghui Yao, and Shengyu Zhang. Multipartite quantum correlation and communication complexities. computational complexity volume, 26:199­228, 2017.
[18] Dana Lahat and C´edric F´evotte. Positive Semidefinite Matrix Factorization: A Link to Phase Retrieval and a Block Gradient Algorithm. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020.
[19] Dana Lahat and C´edric F´evotte. Positive Semidefinite Matrix Factorization Based on Truncated Wirtinger Flow. In 28th European Signal Processing Conference (EUSIPCO), 2020.
[20] Dana Lahat, Yanbin Lang, Vincent Y. F. Tan, and C´edric F´evotte. Positive Semidefinite Matrix Factorization: A Connection with Phase Retrieval and Affine Rank Minimization. IEEE Transactions on Signal Processing, in press, 2021.
[21] Kenneth Lange. MM Optimization Algorithms. SIAM, 2016.
[22] Javad Lavaei and Steven H. Low. Zero Duality Gap in Optimal Power Flow Problem. IEEE Transactions on Power Systems, 27(1), 2012.
[23] Jimmie D. Lawson and Yongdo Lim. The Geometric Mean, Metrics, and More. In The American Mathematical Monthly, volume 108, pages 797­812, 2001.
[24] Daniel D. Lee and H. Sebastian Seung. Learning the Parts of Objects by Non-negative Matrix Factorization. Nature, 401, 1999.
13

[25] Daniel D. Lee and H. Sebastian Seung. Algorithms for Non-negative Matrix Factorization. In Advances in Neural Information Processing Systems 13, 2000.
[26] S. Z. Li, Xin Wen Hou, Hong Jiang Zhang, and Qian Sheng Cheng. Learning spatially localized, parts-based representation. In Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition., 2001.
[27] Xiaodie Lin, Zhaohui Wei, and Penghui Yao. Quantum and Classical Hybrid Generations for Classical Correlations. https://arxiv.org/abs/2007.10673, 2020.
[28] Mehran Mesbahi and George P. Papavassilopoulos. On the Rank Minimization Problem Over a Positive Semidefinite Linear Matrix Inequality. IEEE Transactions on Automatic Control, 42(2), 1997.
[29] Karthik Mohan and Maryam Fazel. New Restricted Isometry Results for Noisy Low-rank Recovery. In IEEE International Symposium on Information Theory (ISIT), 2010.
[30] Pentti Paatero and Unto Tapper. Positive matrix factorization: A non-negative factor model with optimal utilization of error. Environmentrics, 5:111­126, 1994.
[31] Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Semidefinite relaxations for certifying robustness to adversarial examples. In Advances in Neural Information Processing Systems 31, 2018.
[32] Mikkel N. Schmidt and Rasmus K. Olsson. Single-channel Speech Separation Using Sparse Non-negative Matrix Factorization. In Ninth International Conference on Spoken Language Processing, 2006.
[33] Kim-Chuan Toh. An inexact primal­dual path following algorithm for convex quadratic sdp. Mathematical Programming, 112:221­254, 2008.
[34] Stephen Tu, Ross Boczar, Max Simchowitz, Mahdi Soltanolkotabi, and Benjamin Recht. Lowrank Solutions of Linear Matrix Equations via Procrustes Flow. In Proceedings of The 33rd International Conference on Machine Learning, pages 964­973, 2016.
[35] Arnaud Vandaele, Fran¸cois Glineur, and Nicolas Gillis. Algorithms for Positive Semidefinite Factorization. Computational Optimization and Applications, 71(1):193­219, 2018.
[36] Lieven Vandenberghe and Stephen P. Boyd. Semidefinite Programming. SIAM Review, 48(1):49­95, 1996.
[37] Wei Xu, Xin Liu, and Yihong Gong. Document Clustering Based on Non-negative Matrix Factorization. In the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval, pages 267­273, 2003.
[38] Mihalis Yannakakis. Expressing Combinatorial Optimization Problems by Linear Programs. Journal of Computer and System Sciences, 43:441­466., 1991.
A Proof of the domination property
In this section we prove the domination property, which is the last remaining ingredient in the proof of Theorem 1.
14

Lemma 3. Fix r × r positive definite matrices A1, . . . , Am and consider the linear mapping A : Sr  Rm where Z  ( A1, Z , . . . , Am, Z ) .

Moreover, fix an r × r positive definite matrix B and set W = [A A](B)#B-1.

Then, we have that

Z, W ZW  Z, [A A](Z) , for all Z  Sr.

(30)

Proof. First note that W is well-defined as [A A](B) =

m k=1

Ak, B

Ak

is

positive

definite.

First,

we reduce the theorem to the special case where B = I. For this, define the linear map

A~ : Sr  Rm, Z  A(B1/2ZB1/2).

Noting that it follows that
Furthermore, setting we get that

A~(I) = A(B) and A~ (Z) = B1/2 A (Z)B1/2,
W = [A A](B)#B-1 = B-1#[A A](B) = B-1/2(B1/2[A A](B)B1/2)1/2B-1/2 = B-1/2([A~ A~](I))1/2B-1/2.
Z~ = B-1/2ZB-1/2,

Z, W ZW = tr(ZW ZW ) = tr ZB-1/2([A~ A~](I))1/2B-1/2ZB-1/2([A~ A~](I))1/2B-1/2 = tr B-1/2ZB-1/2([A~ A~](I))1/2B-1/2ZB-1/2([A~ A~](I))1/2 = Z~, ([A~ A~](I))1/2Z~([A~ A~](I))1/2 .

Similarly

Z, [A A](Z) = B1/2B-1/2ZB-1/2B1/2, [A A](B1/2B-1/2ZB-1/2B1/2) = Z~, B1/2[A A](B1/2Z~B1/2)B1/2 = Z~, [A~ A~](Z~) .

To conclude the proof it remains to prove the theorem statement in the special case where B = I. Note that when B = I we have
W = [A A](I)#I-1 = [A A](I)#I = I#[A A](I) = ([A A](I))1/2,

where in the second to last equality we used that the geometric mean is symmetric in its two arguments and the least equality follows by the definition of the geometric mean (16).
Furthermore, by the definition of the map A we have that
m
[A A](Z) = Ak, Z Ak,
k=1

15

which in particular implies that

W = ([A A](I))1/2 =

1/2
tr(Ak)Ak .
k

Thus, in the case where B = I, the inequality we need to prove (cf. Equation (30)) specializes to:



1/2

1/2

tr Z

tr(Ak)Ak Z

tr(Ak)Ak   tr(AkZ)2, for all Z  Sr. (31)

k

k

k

As a special case of Lieb's concavity theorem (e.g. see [5, Theorem 6.1]) we have that for any fixed real symmetric matrix Z  Sn, the real-valued map

(X, Y )  tr ZX1/2ZY 1/2

is concave over the domain Sn++ × Sn++. In particular, for any (X1, Y1), (X2, Y2)  Sn++ × Sn++ and   [0, 1] we have that

tr Z(X1 + (1 - )X2)1/2Z(Y1 + (1 - )Y2)1/2   tr ZX11/2ZY11/2 +(1-) tr ZX21/2ZY21/2 .

In

turn,

setting

X1

=

Y1

=

tr(A1 

)

A1

and

X2

=

Y2

=

tr(A2 1-

)

A2

we

get

that:

tr Z(tr(A1)A1 + tr(A2)A2)1/2Z(tr(A1)A1 + tr(A2)A2)1/2  tr(A1) tr ZA11/2ZA11/2 + tr(A2) tr ZA12/2ZA21/2 .

Finally, by induction it follows that:

tr Z( tr(Ak)Ak)1/2Z( (tr(Ak)Ak)1/2  tr(Ak) tr ZA1k/2ZAk1/2 ,

(32)

k

k

k

for any family of positive definite matrices A1, . . . , Am and any fixed matrix Z. Based on (32), to prove (31) it remains to show that

tr(Ak) tr ZA1k/2ZA1k/2  tr(AkZ)2.

(33)

k

k

We prove inequality (33) term-by-term, i.e., we show that

tr(Ak) tr ZA1k/2ZA1k/2 = tr(Ak) tr (ZA1k/2)2  tr(AkZ)2.

(34)

For this we use that for any X, Y  Sn we have

tr(XY )2  tr X2 · tr Y 2 .

(35)

Indeed, let 1  . . .  n be the singular values of X and µ1  . . .  µn be the singular values of

Y . Then

n

2

n

n

tr(XY )2 

iµi 

i2

µ2i = tr X2 · tr Y 2 .

i=1

i=1

i=1

16

The first inequality follows from the von Neumann's trace inequality (see e.g. [2]) and the second inequality follows from the Cauchy-Schwarz inequality.
Lastly, by (35) we have that
tr(AkZ)2 = tr A1k/2(Ak1/2Z)  tr(Ak) tr (ZAk1/2)2 ,
which is exactly (34).

B Fixed-points of the MWU algorithm are KKT points

In this section we give the proof of Theorem 2.

Lemma 4. Let A be the linear map A(Z) = (tr(A1Z), . . . , tr(AmZ)) , where Ai are positive definite matrices, and let x be a vector with positive entries. Suppose B is a positive definite matrix satisfying B = W (A x)W , where W = ([A A](B))-1#B. Then A x = [A A](B).

Proof. Since B is positive definite, it follows that [A A](B) =

m i=1

Ai, B

Ai

is

also

positive

definite, and thus W = ([A A](B))-1#B is positive definite. We then have that

A x = W -1BW -1 = (B-1#[A A](B))B(B-1#[A A](B)) = [A A](B),

where the last equality follows by the unicity property of the matrix geometric mean (17).

17

