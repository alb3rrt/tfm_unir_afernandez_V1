DEEP LEARNING BASED FULL-REFERENCE AND NO-REFERENCE QUALITY ASSESSMENT MODELS FOR COMPRESSED UGC VIDEOS
Wei Sun, Tao Wang, Xiongkuo Min, Fuwang Yi, and Guangtao Zhai
Institue of Image Communication and Information Processing, Shanghai Jiao Tong University, China Email: sunguwei@sjtu.edu.cn

arXiv:2106.01111v1 [eess.IV] 2 Jun 2021

ABSTRACT
In this paper, we propose a deep learning based video quality assessment (VQA) framework to evaluate the quality of the compressed user's generated content (UGC) videos. The proposed VQA framework consists of three modules, the feature extraction module, the quality regression module, and the quality pooling module. For the feature extraction module, we fuse the features from intermediate layers of the convolutional neural network (CNN) network into final qualityaware feature representation, which enables the model to make full use of visual information from low-level to highlevel. Specifically, the structure and texture similarities of feature maps extracted from all intermediate layers are calculated as the feature representation for the full reference (FR) VQA model, and the global mean and standard deviation of the final feature maps fused by intermediate feature maps are calculated as the feature representation for the no reference (NR) VQA model. For the quality regression module, we use the fully connected (FC) layer to regress the quality-aware features into frame-level scores. Finally, a subjectively-inspired temporal pooling strategy is adopted to pool frame-level scores into the video-level score. The proposed model achieves the best performance among the stateof-the-art FR and NR VQA models on the Compressed UGC VQA database and also achieves pretty good performance on the in-the-wild UGC VQA databases.
Index Terms-- Video quality assessment, UGC videos, compressed videos, deep learning, feature fusion
1. INTRODUCTION
With the rapid development of mobile devices and wireless networks in recent years, watching and sharing user's generated content (UGC) videos on social media applications has been a popular daily activity for contemporary people. To reduce the cost of video storage and transmission, the service providers will first compress UGC videos before they are transmitted to the user client, which inevitably degrades the quality of UGC videos. Therefore, it is necessary to develop
This work was supported by the National Natural Science Foundation of (No.61901260, 61831005, 61831015, 61771305, and U1908210).

an effective video quality assessment (VQA) model to evaluate the quality of compressed UGC videos, which on one hand can help the service provider recommend high-quality videos to users, and on the other hand can guide the development of more effective compression algorithms.
Although objective VQA algorithms have been studied for many years, most of them are developed for professionally generated content (PGC) videos, which are shot by photographers using the professional photographic equipment. As a result, the pristine PGC videos are usually of high quality and the quality of compressed PGC videos mainly depends on the degree of distortions introduced during compression and transmission. However, UGC videos are usually captured by amateur users using smartphone cameras under various shooting environments. Therefore, the quality of pristine UGC videos is affected by the video content such as its aesthetic characteristics and authentic distortions such as low visibility, noise, jitter, etc. The compressed UGC videos are further degraded by compression artifacts, which poses a challenge to existing VQA models.
Generally speaking, objective VQA can be divided into full-reference VQA (FR VQA), reduced-referenced IQA (RR VQA), and no-reference (NR VQA) according to whether to access the reference information. FR VQA and RR VQA models require full and part reference video information respectively, while NR VQA models only take the distorted video as the input, which is more difficult but is also more practical in real applications.
FR VQA models usually measure the fidelity between the reference and distorted frames of videos as the video quality. The most widely used fidelity metrics are PSNR and SSIM [1], which respectively calculate the pixel-wised Euclidean distance and structural similarity between each reference and distorted frame. Then, the average pooling strategy is usually adopted to pool them into the video score. However, the perceptual video quality is also related to the temporal information such as the motion, and these image quality assessment (IQA) [2] based methods cannot model the qualityaware feature of the time domain. Therefore, some studies try to extend existing IQA methods to the video domain by incorporating the temporal information. For example, Moorthy and Bovik propose a motion compensated SSIM (MC-

Feature Extraction Module

Quality Regression Module

Quality Pooling Module

video frame ref exaction

FR Feature Extractor

...

Frame score 1 Frame score 2

Regressor Pooling

video frame dis exaction

NR Feature Extractor

...

Frame score 3
...

video score

Frame score n

Fig. 1. The proposed FR and NR VQA framework. The framework includes the feature extraction module, the quality regression module, and the quality pooling module.

SSIM) [3] by evaluating structural retention between motioncompensated regions. Vu et al. consider human perception on motion distortions and then incorporate it to the most apparent distortion (MAD) index to obtain spatial-temporal MAD (ST-MAD) [4]. Recently, learning based methods show great abilities in the VQA field. Video Multi-method Assessment Fusion (VMAF) extracts multiple IQA features as well as motion features and learns a Support Vector Regressor (SVR) to map these features into the video quality. DeepVQA [5] evaluates the spatio-temporal visual quality via a convolutional neural network (CNN) and a convolutional neural aggregation network (CNAN). C3DVQA [6] utilizes the 3D CNN to learn spatio-temporal features for video quality evaluation.
A naive NR VQA method is to compute the quality of each frame via popular NR IQA methods such as NIQE [7], BRISQUE [8], etc., and then pool them into the video quality score. A comparative study of various temporal pooling strategies on popular NR IQA methods can refer to [9]. As discussed above, the temporal information is also important for VQA. V-BLIINDS [10] is a spatio-temporal natural scene statistics (NSS) model for videos by quantifying the NSS feature of frame-differences and motion coherency characteristic. TLVQM [11] extracts abundant spatio-temporal features such as motion, jerkiness, blurriness, noise, blockiness, color, etc. at two levels of high and low complexity. VIDEVAL [12] further combines the selected features from typical NR I/VQA methods to train a SVR model to regress them into the video quality. Since video content also affects its quality, especially for UGC videos, understanding the video content is beneficial to NR VQA. Hence, some studies [13, 14] extract semantic-level features of each frame using the pretrained CNN model and then use the sequential modeling method such as recurrent neural network (RNN) to learn the temporal relationship of each frame and regress them into the video score.
Though lots of VQA models have been proposed in the

literature, there are still some critical issues that need to be considered. First, previous studies [15][16] have demonstrated that both low-level visual features and high-level semantic information affect the quality of images/videos. However, most existing VQA models only consider one of these factors. For example, traditional hand-crafted feature based models [10][11] extract the low-level visual features, but it is difficult to understand the video content, while recent deep learning based methods [13][14] utilize the pretrained semantic features but ignore the effect of low-level visual features. Second, most existing VQA models are two stage methods, which first extract hand-craft features or deep semantic features and then regress them into the video quality via machine learning methods like SVR, RNN, etc. It lacks an end-to-end learning method to learn the relationship between the video quality and raw pixels of video frames.
In this paper, we propose an effective deep learning based VQA framework, which allows it to be trained in an end-toend manner. The proposed framework is illustrated in Fig. 1, which contains three modules, the feature extraction module, the quality regression module, and the quality pooling module. For the feature extraction module, we fuse the features from intermediate layers of the CNN network into final feature representation, which makes the model take full advantage of visual information from low-level to high-level. Specifically, for the FR VQA task, we calculate the structure and texture similarities of each layer of the CNN network as the quality-aware feature representation. For the NR VQA task, we hierarchically add the feature maps from intermediate layers into the final feature maps and calculate their global mean and standard deviation as the quality-aware feature representation. We use the fully connected (FC) layer to regress the quality-aware features into the frame-level quality score. Finally, a subjectively-inspired temporal pooling strategy is adopted to obtain the final video quality. The experimental results on the Compressed UGC VQA database show that the

2

Ref frames

Stage 1

Stage 2

Stage 3

Stage 4

Stage 5

Dis frames

Stage 1

Stage 2

Stage 3

Stage 4

Stage 5

FR feature C
C concatenate Structure Similarity Texture Similarity

... ...

Stage 1
Input frames

Stage 2

Stage 3

Stage 4

Stage 5 +

NR feature
C

+

+

+

+

C concatenate 1x1, 3x3, 1x1 conv Average Pooling Standard Devation Pooling

+
Staircase Structure

+

+

+

+

Fig. 2. The network architecture of the FR feature extraction module.
proposed model outperforms other state-of-the-art VQA models by a large margin.
2. PROPOSED METHOD
The framework of the proposed model is illustrated in Fig. 1, which consists of the feature extraction module, the quality regression module, and the quality pooling module. Since the adjacent frames of the video contain lots of redundancy information, we just extract N frames from each video to calculate their quality-aware feature representation via the feature extraction module. Then the quality regression module is used to map the quality-aware features into frame-level quality scores. Finally, we perform a subjectively-inspired temporal pooling strategy to obtain the video quality score.
2.1. Feature Extraction Module
Since the features extracted from different stages of a CNN model represent different visual information [17], recently proposed IQA models try to fuse the features extracted from intermediate layers of the CNN model, which can make the model fully utilize the visual information from low-level to high-level and learn better quality-aware feature representations. We also follow this routine to design our FR and NR feature extraction module.
2.1.1. FR Feature Extraction Network
As stated in Introduction Section, FR VQA models are actually fidelity metrics that can reflect the perceptual quality. Traditional VQA models calculate the structural similarities of two images as the quality, but they are not always consistent with human perception. [18] finds that deep features are very effective to represent the quality characteristics and they propose the perceptual similarity which calculates the L2 distance of feature maps of each pair extracted by intermediate layers of a CNN model. Inspired by the form SSIM, [19] proposes the deep structure and texture similarities through computing the global means and the global correlation of feature maps of each pair. In this paper, we also adopt this measure to evaluate the quality distance between two feature

Fig. 3. The network architecture of the NR feature extraction module.

maps. Assume that there are Ns stages in a CNN model, and
Fi is the feature maps extracted from the i-th stage, where i  [1, 2, ..., Ns]. Fij is the j-th feature map in Fi. Then, the structure and texture similarities are defined as:

ftexture fstrcture

Frj,i, Fdj,i Frj,i, Fdj,i

=

2µ µ Frj,i Fdj,i + c1

,

(µFrj,i )2 + (µFdj,i )2 + c1

=

2Frj,iFdj,i + c2

,

(Frj,i )2 + (Fdj,i )2 + c2

(1)

where µFrj,i , µFdj,i , (Frj,i )2, (Fdj,i )2, and Frj,iFdj,i are the global means and variances of feature maps Frj,i and Fdj,i, and the global covariance between Frj,i and Fdj,i, respectively. Frj,i and Fdj,i are the feature maps of the reference and distorted frames respectively. c1 and c2 are the small constants
to avoid numerical instability. Finally, we obtain the struc-
ture and texture similarities of feature maps extracted by each
stage of the CNN model and all of them constitute the quality-
aware feature representation of our FR VQA model ffr. We
illustrate the FR feature extraction network in Fig. 2.

2.1.2. NR Feature Extraction Network
Since the NR feature extraction module needs to obtain the features that can represent distortions and content just using the distorted frames, we utilize the recently proposed staircase structure [20] to obtain the quality-aware feature representation of the NR VQA model, which considers both the image distortion and content. The staircase structure is illustrated in Fig. 3, which hierarchically integrates the features from intermediate layers into the final feature representation. To be more specific, the feature maps F1 will be continually added to the feature maps Fi until to the final feature maps FNs to derive F1, where 1 < i < Ns. Then we do the same operation on the feature maps Fi to derive Fi. Hence, the final fusing feature maps Fs is calculated as:

Ns -1

Fs =

Fi + FNs .

(2)

i=1

3

Table 1. The performance of the proposed FR VQA model and the compared models on the validation set of the Compressed UGC VQA database.

Methods SSIM [1] LPIPS [18] DISTS[19] C3DVQA [6] Proposed

SROCC 0.8385 0.8870 0.9013 0.9054 0.9166

KROCC 0.6395 0.7080 0.7330 0.7365 0.7560

PLCC 0.8375 0.9191 0.9424 0.9471 0.9587

RMSE 0.6934 0.4999 0.4244 0. 4070 0.3610

Table 2. The performance of the proposed NR VQA model and the other compared models on the validation set of the Compressed UGC VQA database.

Methods NIQE [7] BRISQUE [8] RAPIQUE [21] VSFA [13] Proposed

SROCC 0.5929 0.8346 0.8647 0.8925 0.9352

KROCC 0.4173 0.6477 0.6898 0.7280 0.7937

PLCC 0.5977 0.8886 0.9247 0.9587 0.9826

RMSE 0.9764 0.5587 0.4742 0.3463 0.2260

For most popular CNN models, the dimension of the feature maps at the current stage is half that of the previous stage while the number of channels is twice that of the previous stage. To make the number of channels and the dimension of feature maps at different stages the same, we introduce a bottleneck structure consisting of three convolution operations to downscale the dimension and increase the channels. Specifically, we first reduce the channels of feature map Fi to a quarter through the 1×1 convolution layer to decrease the computation complexities of the whole procedure. Then we utilize the 3×3 convolution layer with a stride of 2 to reduce the resolution of Fi to half. Finally, Fi is passed through the 1×1 convolution layer to increase the number of channels for eight times.
We apply the spatial global average pooling (GPmean) and spatial global standard deviation pooling (GPstd) on the extracted feature maps Fs to obtain the final quality-aware feature representation fnr:

fmean = GPmean(Fs),

fstd = GPstd(Fs),

(3)

fnr = cat(fmean, fstd),

where cat is the concatenation operation.

2.2. Quality Regression Module
After extracting quality-aware feature representation by the feature extraction module, we need to map these features to the quality scores with a regression model. In this paper, we use two fully connected (FC) layers as the regression model to obtain the frame-level quality due to its simplicity. The two FC layers consist of 128 and 1 neurons respectively. Therefore, we can obtain the frame-level quality score via

qt = WF C (f ), f  {ffr, fnr},

(4)

where WF C denotes the function of the two FC layers and qt is the quality of the t-th frame, and t  [1, 2, 3, .., N ].

2.3. Quality Pooling Module
Since human is sensitive to drops in video quality and is dull to improvements in video quality, the contribution of the qual-

ity of each frame to the overall quality is different. So, we use the quality pooling module to assign the weights to different frames to represent their contribution to the overall quality. In this paper, we use the subjectively-inspired temporal pooling strategy proposed in [13], which considers the memory effect of previous frames and the hysteresis effect of next frames to the current frame. The detailed introduction of the subjectively-inspired temporal pooling strategy can refer to [13]. Here, we denote it as TP. Then, the overall video quality is calculated as

qt = TP(qt),

1N

(5)

Q= N

qt,

t=1

where qt is the quality of the t-th frame which considers the memory effect and the hysteresis effect. Q is the video quality score predicted by the proposed model.
The Euclidean distance is used as the loss function of the proposed model:

L = Q - Qlabel 2 ,

(6)

where Qlabel is the ground-truth quality score obtained from subjective experiments.

3. EXPERIMENTAL VALIDATION
3.1. Experimental Protocol
3.1.1. Database
The proposed model is mainly validated on the Compressed UGC VQA database, which is provided by the ICME 2021 Grand Challenge [22]. The database consists of 6,400 video clips for training, 800 video clips for validation, and 800 video clips for the test. The reference video is compressed into 7 distorted ones by H.264/AVC with different CRF settings. Each video clip is rated by at least 50 subjects.
Besides the Compressed UGC VQA database, we also validate the proposed NR VQA model on two in-the-wild UGC VQA databases, KoNViD-1k[23] and LIVE-VQC[24]. The KoNViD-1k database includes 1,200 public-domain

4

Table 3. The performance of the proposed NR VQA model and the compared models on the KoNViD-1k database.

Methods BRISQUE [8] V-BLIINDS [10] TLVQM [11] VIDEVAL [12]
Proposed

SROCC 0.6567 0.7101 0.7729 0.7832 0.8134

KROCC 0.4761 0.5188 0.5770 0.5845 0.6201

PLCC 0.6576 0.7037 0.7688 0.7803 0.8143

RMSE 0.4813 0.4595 0.4102 0.4026 0.3695

Table 4. The performance of the proposed NR VQA model and the compared models on the LIVE-VQC database.

Methods BRISQUE [8] V-BLIINDS [10] TLVQM [11] VIDEVAL [12]
Proposed

SROCC 0.5925 0.6939 0.7988 0.7522 0.7466

KROCC 0.4162 0.5078 0.6080 0.5639 0.5518

PLCC 0.6380 0.7178 0.8025 0.7514 0.7815

RMSE 13.1004 11.7659 10.1454 11.1004 13.5143

videos sampled from the YFCC100M database, and is annotated by 642 subjects on the crowdsourcing platform. The LIVE-VQC database consists of 585 in-the-wild videos, which are labeled by 4,776 subjects on the crowdsourcing platform.
3.1.2. Evaluation Criteria
Four common criteria are adopted to evaluate the performance of VQA models, which are Spearman Rank Order Correlation Coefficient (SROCC), Kendall Rank Order Correlation Coefficient (KROCC), Pearson Linear Correlation Coefficient (PLCC) and Root Mean Squared Error (RMSE). SROCC and KROCC indicate the prediction monotonicity of the VQA algorithm, PLCC reflects the prediction linearity, and RMSE represents the predication accuracy. Before calculating the evaluation criteria, we follow the same procedure in [13] to map the objective score to the subject score using a fourparameter logistic function.
3.1.3. Experimental Settings
For the Compressed UGC VQA database, we extract one frame every half second for the feature extraction, and for the KoNViD-1k and LIVE-VQC databases, we extract one frame every second. We use ResNet50 [25] as the backbone for both the FR and NR feature extraction networks. The weights of the backbone are initialized by training on ImageNet, and other weights are randomly initialized. For all databases, we first resize the resolution of the minimum dimension of videos as 520 while maintaining their aspect ratios. In the training stage, the input video is randomly cropped with a resolution of 448×448, and in the test stage, each video is cropped at the same resolution of 448×448 at the center. The Adam optimizer with the initial learning rate 0.00001 and batch size 6 is used for training the proposed model on a server with NVIDIA V100.
Since there are no training and validation splits for the KoNViD-1k and LIVE-VQC databases, we randomly split the databases into the training set with 80% videos and the validation set with 20% videos for 10 times, and report the average values of four evaluation criteria as the final result.

3.2. Experimental results
Full Reference. We list the performance of the proposed FR VQA model and other state-of-the-art FR I/VQA models on the validation set of the Compressed UGC VQA database in Table 1. From Table 1, it is seen that the proposed model achieves the best performance among all compared models, which demonstrates the effectiveness of the proposed model. We notice that some FR IQA models such as LPIPS and DISTS also have a certain ability to evaluate the quality of the compressed videos since they both extract features from the intermediate layers of the CNN network. C3DVQA uses the 3D CNN network to learn the spatial-temporal features of video, whose performance is superior to other FR IQA models but is interior to the proposed model.
No Reference. The performance of the proposed NR VQA model and the compared methods on the validation set of the Compressed UGC VQA database is listed in Table 2 and the performance on the KoNViD-1k and LIVE-VQC databases are listed in Table 3 and 4 respectively. From Table 2, we observe that the proposed NR VQA model outperforms the compared NR I/VQA models by a large margin, which indicates the proposed NR VQA model has a strong ability to evaluate the quality of the compressed UGC videos. From Table 3 and Table 4, we find that the proposed model performs the best on the KoNViD-1k database, and is just inferior to TLVQM and VIDEVAl on the LIVE-VQC databases, which indicates that the proposed model is also suitable for evaluating the quality of in-the-wild UGC videos. Since our model does not extract the motion features of the videos and the videos in the LIVE-VQC database usually contain many camera motion or object motion scenes, our model does not achieve the best on the LIVE-VQC database.
4. CONCLUSION
In this paper, we propose an effective deep learning based VQA framework, which is suitable for both FR and NR VQA models. The proposed VQA framework contains three modules, which are the feature extraction module, quality regression module, and quality pooling regression module. For the feature extraction module, we incorporate the features from the intermediate layers of the CNN model into the final quality-aware feature representation, of which the struc-

5

ture and texture similarity of feature maps are calculated as the feature representation for the FR VQA model, and the staircase structure is adopted to hierarchically add the feature maps from intermediate layers into the final feature maps and their global mean and standard deviation are calculated as the feature representation for the NR VQA model. For the quality regression module, we use the FC layer to regress the quality-aware features into the frame-level score. Finally, a subjectively-inspired temporal pooling strategy is adopted to obtain the final video score. The proposed model achieves the best performance on the Compressed UGC VQA database when compared with other state-of-the-art VQA models and also achieves pretty good performance on the in-the-wild UGC VQA databases.
5. REFERENCES
[1] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli, "Image quality assessment: from error visibility to structural similarity," IEEE transactions on image processing, vol. 13, no. 4, pp. 600­612, 2004.
[2] Guangtao Zhai and Xiongkuo Min, "Perceptual image quality assessment: a survey," Science China Information Sciences, vol. 63, pp. 1­52, 2020.
[3] Anush Krishna Moorthy and Alan Conrad Bovik, "Efficient video quality assessment along temporal trajectories," IEEE transactions on circuits and systems for video technology, vol. 20, no. 11, pp. 1653­1658, 2010.
[4] Phong V Vu, Cuong T Vu, and Damon M Chandler, "A spatiotemporal most-apparent-distortion model for video quality assessment," in 2011 18th IEEE International Conference on Image Processing. IEEE, 2011, pp. 2505­2508.
[5] Woojae Kim, Jongyoo Kim, Sewoong Ahn, Jinwoo Kim, and Sanghoon Lee, "Deep video quality assessor: From spatiotemporal visual sensitivity to a convolutional neural aggregation network," in Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 219­234.
[6] Munan Xu, Junming Chen, Haiqiang Wang, Shan Liu, Ge Li, and Zhiqiang Bai, "C3dvqa: Full-reference video quality assessment with 3d convolutional neural network," in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 4447­4451.
[7] Anish Mittal, Rajiv Soundararajan, and Alan C Bovik, "Making a "completely blind" image quality analyzer," IEEE Signal processing letters, vol. 20, no. 3, pp. 209­212, 2012.
[8] Anish Mittal, Anush Krishna Moorthy, and Alan Conrad Bovik, "No-reference image quality assessment in the spatial domain," IEEE Transactions on image processing, vol. 21, no. 12, pp. 4695­4708, 2012.
[9] Zhengzhong Tu, Chia-Ju Chen, Li-Heng Chen, Neil Birkbeck, Balu Adsumilli, and Alan C Bovik, "A comparative evaluation of temporal pooling methods for blind video quality assessment," in 2020 IEEE International Conference on Image Processing (ICIP). IEEE, 2020, pp. 141­145.
[10] Michele A Saad, Alan C Bovik, and Christophe Charrier, "Blind prediction of natural video quality," IEEE Transactions on Image Processing, vol. 23, no. 3, pp. 1352­1365, 2014.

[11] Jari Korhonen, "Two-level approach for no-reference consumer video quality assessment," IEEE Transactions on Image Processing, vol. 28, no. 12, pp. 5923­5938, 2019.
[12] Zhengzhong Tu, Yilin Wang, Neil Birkbeck, Balu Adsumilli, and Alan C Bovik, "Ugc-vqa: Benchmarking blind video quality assessment for user generated content," IEEE Transactions on Image Processing, 2021.
[13] Dingquan Li, Tingting Jiang, and Ming Jiang, "Quality assessment of in-the-wild videos," in Proceedings of the 27th ACM International Conference on Multimedia, 2019, pp. 2351­ 2359.
[14] Pengfei Chen, Leida Li, Lei Ma, Jinjian Wu, and Guangming Shi, "Rirnet: Recurrent-in-recurrent network for video quality assessment," in Proceedings of the 28th ACM International Conference on Multimedia, 2020, pp. 834­842.
[15] Fei Gao, Yi Wang, Panpeng Li, Min Tan, Jun Yu, and Yani Zhu, "Deepsim: Deep similarity for image quality assessment," Neurocomputing, vol. 257, pp. 104­114, 2017.
[16] Wei Sun, Xiongkuo Min, Guangtao Zhai, Ke Gu, Huiyu Duan, and Siwei Ma, "Mc360iqa: a multi-channel cnn for blind 360degree image quality assessment," IEEE Journal of Selected Topics in Signal Processing, vol. 14, no. 1, pp. 64­77, 2019.
[17] Matthew D Zeiler and Rob Fergus, "Visualizing and understanding convolutional networks," in European conference on computer vision. Springer, 2014, pp. 818­833.
[18] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang, "The unreasonable effectiveness of deep features as a perceptual metric," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 586­595.
[19] Keyan Ding, Kede Ma, Shiqi Wang, and Eero P Simoncelli, "Image quality assessment: Unifying structure and texture similarity," arXiv preprint arXiv:2004.07728, 2020.
[20] Wei Sun, Xiongkuo Min, Zhai Guangtao, and Siwei Ma, "Blind quality assessment for in-the-wild images via hierarchical feature fusion and iterative mixed database training," arXiv preprint arXiv:2105.14550, 2021.
[21] Zhengzhong Tu, Xiangxu Yu, Yilin Wang, Neil Birkbeck, Balu Adsumilli, and Alan C Bovik, "Rapique: Rapid and accurate video quality prediction of user generated content," arXiv preprint arXiv:2101.10955, 2021.
[22] Haiqiang Wang, Gary Li, Shan Liu, and C.-C. Jay Kuo, "Challenge on quality assessment of compressed ugc videos," Website, 2021, http://ugcvqa.com/.
[23] Vlad Hosu, Franz Hahn, Mohsen Jenadeleh, Hanhe Lin, Hui Men, Tama´s Szira´nyi, Shujun Li, and Dietmar Saupe, "The konstanz natural video database (konvid-1k)," in 2017 Ninth international conference on quality of multimedia experience (QoMEX). IEEE, 2017, pp. 1­6.
[24] Zeina Sinno and Alan Conrad Bovik, "Large-scale study of perceptual video quality," IEEE Transactions on Image Processing, vol. 28, no. 2, pp. 612­627, 2018.
[25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, "Deep residual learning for image recognition," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770­778.

6

