
# ZmBART: An Unsupervised Cross-lingual Transfer Framework for Language Generation

[arXiv](https://arxiv.org/abs/2106.01597), [PDF](https://arxiv.org/pdf/2106.01597.pdf)

## Authors

- Kaushal Kumar Maurya
- Maunendra Sankar Desarkar
- Yoshinobu Kano
- Kumari Deepshikha

## Abstract

Despite the recent advancement in NLP research, cross-lingual transfer for natural language generation is relatively understudied. In this work, we transfer supervision from high resource language (HRL) to multiple low-resource languages (LRLs) for natural language generation (NLG). We consider four NLG tasks (text summarization, question generation, news headline generation, and distractor generation) and three syntactically diverse languages, i.e., English, Hindi, and Japanese. We propose an unsupervised cross-lingual language generation framework (called ZmBART) that does not use any parallel or pseudo-parallel/back-translated data. In this framework, we further pre-train mBART sequence-to-sequence denoising auto-encoder model with an auxiliary task using monolingual data of three languages. The objective function of the auxiliary task is close to the target tasks which enriches the multi-lingual latent representation of mBART and provides good initialization for target tasks. Then, this model is fine-tuned with task-specific supervised English data and directly evaluated with low-resource languages in the Zero-shot setting. To overcome catastrophic forgetting and spurious correlation issues, we applied freezing model component and data argumentation approaches respectively. This simple modeling approach gave us promising results.We experimented with few-shot training (with 1000 supervised data points) which boosted the model performance further. We performed several ablations and cross-lingual transferability analyses to demonstrate the robustness of ZmBART.

## Comments

Accepted in Findings of ACL-IJCNLP 2021

## Source Code

Official Code



Community Code

- [https://paperswithcode.com/paper/zmbart-an-unsupervised-cross-lingual-transfer](https://paperswithcode.com/paper/zmbart-an-unsupervised-cross-lingual-transfer)

## Bibtex

```tex
@misc{maurya2021zmbart,
      title={ZmBART: An Unsupervised Cross-lingual Transfer Framework for Language Generation}, 
      author={Kaushal Kumar Maurya and Maunendra Sankar Desarkar and Yoshinobu Kano and Kumari Deepshikha},
      year={2021},
      eprint={2106.01597},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```

## Notes

Type your reading notes here...

