arXiv:2106.00156v1 [cs.FL] 1 Jun 2021

DETECTING A SINGLE FAULT IN A DETERMINISTIC FINITE AUTOMATON
ARTUR POLASKI
Institute of Computer Science and Computer Mathematics Jagiellonian University in Cracow; Kraków, Poland
ERYK LIPKA
Institute of Mathematics Pedagogical University of Cracow; Kraków, Poland
Abstract. Given a DFA and its implementation with at most one single fault, that we can test on a set of inputs, we provide an algorithm to find a test set that guarantees finding whether the fault exists.
1. Introduction In 2017 A. Roman considered in [1] the problem of finding a possible fault in an implementation of a DFA, providing a heuristic algorithm for finding a small test set recognizing whether the fault indeed exists. Unfortunately, that paper has a few errors and imprecisely described reasoning. We aim to correct those, as well as provide both an exponential time algorithm for finding the smallest possible test set, as well as a polynomial time heuristic algorithm that usually finds smaller test sets than one presented in [1].
2. Notation and terminology · For an alphabet  we denote the set of words over  by . · We denote an empty word by . · We call a set o words a language. Definition 1. A 5-tuple A = (Q, , , q0, F ) is called a deterministic finite automaton (DFA for short) iff it consists of · a nonempty finite set of states Q, · a nonempty finite set of input letters  called the alphabet,
E-mail addresses: artur.polanski@uj.edu.pl, eryklipka0@gmail.com. Key words and phrases. finite automata; system testing; fault detection. Research of the second author was supported by a grant of the National Science Centre, Poland, no. UMO-2019/34/E/ST1/00094.
1

2

A. POLASKI AND E. LIPKA

· a transition function  : Q ×   Q, · an initial state q0  Q, · a set of final (accept, terminal) states F  Q.
The function  can be extended on the set of words , abusing the notation we use  for the extension.
For a word w consisting of a single letter we use the definition above. For q  Q we define (q, ) = q. For a word w   of the form w = au, where a  , u   we define (q, w) = ((q, a), u).
We say that a word w   is accepted (by A) iff (q0, w)  F , otherwise we say that it is rejected (by A).
An automaton A is called minimal iff there is no automaton
A = (Q, , , q0 , F )
with |Q| < |Q| such that the sets of words accepted by A and A are equal. For any automaton there exists, up to isomorphism, exactly one minimal automaton which accepts the same set of words (see e.g. [2]).
For Q  Q and q1  Q we say that a word w   is synchronizing set Q to state q1 iff qQ(q, w) = q1.
We call a state q  Q a sink state (or a sinkhole state) iff for any a   we have (q, a) = q. If a sinkhole q is a final state we call it a positive sinkhole and we call it a negative sinkhole otherwise.
When talking about automatons it is often convenient to use their graph representation, below there is an example of an automaton
A = ({1, 2, 3, A, X}, {a, b}, , 1, {A}).
The function  is depicted by arrows, the initial state is the only one with an arrow pointing to it that is not a transition, and a double circle around a node represents final states. That exact automaton is used in [1], which we will also use as an example later.

start

b

a

1a2

3 aA

b

b

a,b

X

a,b Figure 1. Example automaton from [1]

DETECTING A SINGLE FAULT IN A DFA

3

3. preliminaries
Firstly, we define what we mean by a fault in an automaton. Let A = (Q, , , q0, F ) be a DFA, and assume that its implementation (SUT - system under testing) represents A = (Q, , , q0, F ), a DFA that may be different. When thinking about possible differences between A and A, there are a few natural candidates, for example:
· an incorrect transition, i.e. (q, a) = (q, a) for some q  Q, a  , · a missing transition (first possible interpretation), i.e. after (q, a)
is used for some q  Q, a  , the word being read will be rejected, · a missing transition (second possible interpretation), i.e. after (q, a)
is used, the state will not change, when in A it does; this is covered by our first candidate, an incorrect transition.
If we ease the requirements, so not only transition functions differ we can also consider:
· incorrect initial state, · incorrect set of accepting states, · incorrect set of states;
but such cases are out of the scope of this paper. Consider an incorrect transition first. If A is not minimal, we may not be
able to conclude whether the fault occurred. For example, if F is empty (or F = Q) and |Q| > 1, then all words are rejected (or accepted), an incorrect transition (or, for that matter, any number of incorrect transitions) will not change that. On the other hand, we have the following.

Proposition 1. A single incorrect transition in a minimal DFA can always be detected.
Proof. Let A = (Q, , , q0, F ) be a DFA, and A = (Q, , , q0, F ) be its implementation with a single incorrect transition. Assume that that fault cannot be detected. There exist q, q1, q2  Q, a   such that (q, a) = q1 and (q, a) = q2, where q1 = q2. Since the fault cannot be detected, then {w   : (q1, w)  F } = {w   : (q2, w)  F }. It follows that if we remove q2 from Q and replace all transitions to q2 with transitions to q1 (possibly changing the initial state to q1, if q2 was initial), we will get an automaton with fewer states that accepts the same language as A, a contradiction with minimality.

That is the reason why, from now on, we assume that our DFA is minimal. In [1] we read that the first interpretation of a missing transition can also be realised by an incorrect transition. That is not always the case however. Imagine a DFA with only one state that is final. An incorrect transition in that case is not even possible, but a missing transition (first interpretation) is. To be more precise, such realisation is possible iff A has a negative sinkhole.

4

A. POLASKI AND E. LIPKA

Going forward, we assume that the only possible fault is an incorrect transition.

4. finding the minimal test set
Let us state our goal as a decision problem.
Problem 1. For a given integer k, a DFA A and its implementation B that is either the same as A or differs from A by a single fault, verify the existence of a set of at most k words that can detect is B differs from A.
For this part we need to know that each DFA corresponds (one-to-one) with a regular expression that represents the language accepted by that DFA.
Theorem 1 (Kleene, [3]). A language is regular if and only if there exists DFA, which accepts this language.
Consider A = (Q, , , q0, F ), a DFA. For every a   and q, q  Q such that (q, a) = q we can construct an automaton with a single fault A(q,a,q) = (Q, , (q,a,q), q0, F ) such that (q,a,q)(q, a) = q is its only fault. For any such automaton we can consider
A(q,a,q) = (Q × Q, , (q,a,q), (q0, q0), F × (Q \ F )  (Q \ F ) × F ),
where (q,a,q)((q1, q2), w) = ((q1, w), (q,a,q)(q2, w)), which accepts a nonempty language (by Proposition 1) of words that recognize the fault
(q,a,q)(q, a) = q, because these are precisely the words that are accepted by A and not A, or the other way around. That way, out problem can be restated as finding a minimal set of words for a given finite set of regular expressions (i.e. languages accepted by the automatons A(q,a,q)), such that for each given regular expression at least one word from our set satisfies it. Unfortunately, we have the following.

Proposition 2. Given a finite set of regular expressions R and a positive
integer k, the problem if there exists a set of words W , such that |W | k
and a language represented by each regular expression in R has a nonempty
intersection with W is NP-complete.
Proof. If we are provided with a set W of at most k words it is easy to check in polynomial time whether each regular expression in R represents a language that has at least one word in W , therefore the problem is in NP.
To show that it is NP-complete, we will reduce a dominating set problem to it. Recall that a dominating set of a graph is a subset A of its vertices such that any vertex is either in A or has a neighbour in A. Let G = (V, E) be a graph such that V = {v1, . . . , vn}. We construct a set of regular expressions as follows. For i  {1, . . . , n} let ri be a regular expression over the alphabet V such that it is a sum of vi and all its neighbours in G. We need to show

DETECTING A SINGLE FAULT IN A DFA

5

that a vertex cover V   V such that |V | k exists iff there exists a set of words W  such that |W | k and each regular expression from {r1, . . . , rn} is represented.
Indeed, if we can represent all regular expressions r1, . . . , rn with a set of words |W | such that |W | k (note that each word must be a single letter vi), then a corresponding set |V | of vertices with labels corresponding to W  is a vertex cover and |V | = |W |.
On the other hand, given a vertex cover V , if we take W  as letters vi such that a vertex with that label is in V  we get a set of words as needed.
As dominating set problem is NP-complete (problem GT2 in [4]), that concludes the proof.
That means of course, that we can find a smallest set of words that recognize a fault in DFA, but unless P=NP, proceeding as described above will not produce the needed set in polynomial time. We propose the following.

Problem 2. Decide whether problem 1 is NP-complete.

5. heuristic algorithm
5.1. Algorithm proposed by Roman. The algorithm presented in [1] is the following:
As for the terminology used there (following [1]), a path p is a sequence of states p = (q1, q2, . . . , qk) such that for each i  {1, 2, . . . , k - 1} there exists ai   such that M (qi, ai) = qi+1 (it corresponds of course to a directed path in a transition graph for A). The letters ai form a word a1 . . . ak-1 which is denoted by (p) and a sequence of pairs (p1, a1) . . . (pk-1, ak-1) is denoted by e(p). We will later use the same notation in our algorithm.
It will be helpful to summarize how the algorithm works, as later we will do the same for our version, the conceptual differences should be easier to see that way. We first find a path through the transition graph ending in a final state that covers the most transitions possible. We then explore all possible faults that may cause a deviation in the path taken through the transition graph with the same input word. If the deviation causes the automaton to end in a state that is not final, then the original word is "good" for finding that particular fault. If not (i.e. even though there was a deviation, we still ended up in a final state), then our word is not "good" for that particular fault, and we just find a word that is. After that we can repeat the procedure, but considering all the transitions our word passed through as "done". That way we can eventually label on transitions that can be used as part of a path from an initial to a final state. That, of course, may still not be the end, since all the words we find take us to a final state, there can still be transitions left that are not covered by the procedure thus far (for example, those leading to a negative sinkhole). We therefore find a path again covering the most transitions not yet "done" (necessarily we end in a non-final state). Now we have a similar situation to the one before, either a single deviation will send us to a final state (in which case

6

A. POLASKI AND E. LIPKA

Algorithm 1 GENERATE from [1] Input: a finite automaton A = (Q, , M , q0, F ) Output: a set T   of test cases that detect all possible "incorrect transition" faults

1: E = {(q, a)  Q × }, T = 

2: while E =  do

3: find a path p such that M (q0, (p))  F and which maximizes

|e(p)  E| and for which e(p)  E = .

4: if such p exists then

5:

for (r, a)  e(p) do

6:

E = E \ {(r, a)}

7:

let (p) = uw, where M (q0, u) = r

8:

for all q  Q \ {M (r, a)} do

9:

M = M , M (r, a) = q

10:

if M (q0, (p))  Q \ F then

11:

T = T  (p)

12:

else

13:

find w   and b   \ {a} such that only one of

states M (q0, ubw), M (q0, ubw) is a terminal state.

14:

T = T  {ubw}

15: else if E =  then

16:

find a path p that covers as many elements from E as possible

(ending in Q \ F )

17:

for all (r, a)  e(p) do

18:

E = E \ {(r, a)}, S = Q \ (r, a)

19:

let (p) = uw, where M (q0, u) = r

20:

while S =  do

21:

find a word w that synchronizes to some accepting state

from F as many states from Q \ (r, a) as possible and remove them

from S

22:

T = T  {uaw}

23: return T

that one is covered by the word we are in process of analyzing), or not. Here however, when considering a certain state and possible faults that have it as a starting point, that result in paths leading to a non-final state, we attempt to lower the number of words using synchronization. To be more precise, if considering a path p through a state q we find that changing a single transition can result in multiple paths ending in a non-final state, instead of adding a word for each case we use synchronization to find one word that synchronizes as many states that can be reached just after the fault as possible into one final state.
Unfortunately, there are a few problems with this algorithm.

DETECTING A SINGLE FAULT IN A DFA

7

(1) In line 13 of the algorithm ubw should be changed to uaw, b is not needed, indeed as written, algorithm may not work.
(2) In line 21, should be noted what automaton that synchronization applies to (what about the incorrect transition? we are synchronizing across multiple possibilities for it).
(3) There is no exact procedure for "find a path" in 3. Even though there can be more than one path covering a maximal number of transitions, the final result may depend on which is chosen.
(4) If we aim to deliver the smallest possible set of words, why use the optimization in only one of two cases? It is easy to see that a minimal automaton remains minimal after changing all non-final states into final and vice-versa. It is therefore peculiar that when considering paths that end in a final state we do not optimize by synchronization as in the case of paths that end in a non-final state, even though they are, in a sense, symmetric.
There are unfortunately also problems in the example shown in [1] (using the algorithm on a concrete automaton, the one from figure 1). The words are wrong (probably because M and M got mixed up). If we assume that the synchronization is done in such a way that the incorrect transition is not used, then the final set achieved by this approach may be for example
{ababaa, aa, bbaa, baaa,b, baaaa, babaa, ba, babaaa, babbaa, bab,
aaaaaaa, aaaabaa, aaaa, aaabaaa, aaabaa, aaab}.
What set we obtain does depends on the approach used in "find" statements (as we suggested earlier). For example, if the first word we try is aabbaa instead of ababaa we no longer need aa.
We will now proceed to describe possible improvements to the algorithm.

5.2. Improvements to the algorithm (without synchronization). As an improvement to algorithm 1 we propose algorithm 2. We will describe how it works just like we did for algorithm 1.
The very beginning is the same, we start with a path through the transition graph covering the maximal possible number of transitions, that end in a final state, let us denote the corresponding word by w. Since what we want to minimize is the number of words in the final test set, we then proceed from the longest to the shortest prefix of w when considering the possible faults that may affect w. The reason is, if we find a word that is accepted in A, but rejected when a given fault occurs after a long prefix, it may also work when considering possible faults occurring after shorter prefixes.
Note that in line 22 we are indeed able to consider the edge again later, because we can only reach this instruction if the transition occured more than once in p.
After we exhaust words that are accepted by A, since we are left with a case that is essentially symmetric to the one just done (when in a minimal automaton we change all final states to non-final and vice versa, the resulting

8

A. POLASKI AND E. LIPKA

Algorithm 2 Generating test cases (without synchronization) Input: a finite automaton A = (Q, , M , q0, F ) Output: a set T   of test cases that detect all possible "incorrect transition" faults

1: E = {(q, a)  Q × }, T = 

2: while E =  do

3: find a path p such that M (q0, (p))  F and which maximizes

|e(p)  E| and for which e(p)  E = .

4: if such p exists then

5:

Tacc = , Trej = 

6:

for (r, a)  e(p), from the last edge of p to the first do

7:

E = E \ {(r, a)}

8:

let (p) = uw, where M (q0, u) = r

9:

for q  Q \ {M (r, a)} do

10:

M = M , M (r, a) = q

11:

if M (q0, (p))  Q \ F then

12:

T = T  (p)

13:

else

14:

if M (q0, Tacc)  Q \ F =  or M (q0, Trej )  F = 

then do nothing

15:

else

16:

if there exists w   such that M (q0, uaw)  F

and M (q0, uaw)  Q \ F then

17:

Tacc = Tacc  {uaw}

18:

else

19:

if there exists w   such that M (q0, uaw) 

Q \ F and M (q0, uaw)  F then

20:

Trej = Trej  {uaw}

21:

else

22:

E = E  {(r, a)} and consider the edge again

when it appears in the list of edges of p

23:

T = T  Tacc  Trej

24: else

25:

if E =  then

26:

find a path p that covers as many elements from E as possible

(ending in Q \ F ) and continue similarly to the previous case

27: return T

automaton is still minimal), we can proceed in a way mirroring the "accepted words" case. In particular that means that we can try to use this algorithm on the "complemented" DFA and check whether it yields a smaller set of test cases (of course we also need to swap acceptance and rejection for the implementation, but that can be done after the implementation actually gives us a result).

DETECTING A SINGLE FAULT IN A DFA

9

That also means that we can use synchronization to try limiting the size of our final set further in both "accepting" and "rejecting" cases, not just in one like in algorithm 1.

5.3. Improvements to the algorithm (with synchronization). When we consider a transition and where it might lead if the fault replacing this exact one, some options are already taken care of by the word chosen for this transition (like we discussed before, we may reach a final state where the original word was rejected or vice-versa). Instead of choosing a word for each of the other options, we can try to "synchronize" to a rejecting state if the word chosen for the transition is accepted, or to an accepting state otherwise. In [1] it was done only in a latter case. It was not stated explicitly in [1] but can be presumed that the synchronization was done on the automaton with the transition considered removed, starting from a set of states that were all options for where the fault may lead, with the ones that were already covered by the word omitted.
It can be done better, however. Consider a word of the form ua, where a   is the letter that in A corresponds to the transition we have yet to cover in an accepted word uaw. For all the options not covered by the words so far, we can try to find a small set of words W  such that members of the set {uaw : w  W } cover all of those options. We can do this by constructing regular expressions for each of the options that reject the word if started there and try to find a smallest possible set of words such that each of the expressions has a representative. Since we do not want to find a minimal set for this problem (exponential complexity), we can, for example, do it greedily. Synchronization can be used here as a preliminary, because if a set of states can be synchronized, the language of words satisfying all of their corresponding regular expressions is nonempty.

5.4. Using synchronization to avoid masking. In both algorithms we find paths p which maximize |e(p)  E|. There can, of course, be more then one such path. What is more, when considering two paths, using one of them can be better for our purposes then using the other.
For a path p in the transition graph of A, and an edge x -a y in p, we say that p is masking a faulty transition x -a z if y = z and replacing x -a y with x -a z in M does not change (p) being accepted by A (i.e. it is accepted both before and after or rejected both before and after introducing the fault).
It is unrealistic to try and find a path p that maximizes |e(p)  E| that does not mask anything, as it would imply that we can cover all possible faulty transitions that disrupt p just by (p), but we can try to avoid some easy to find maskings.
Consider the example shown in figure 2. In the situation shown there, M (q0, wa) and q1 can be synchronized using the word v (suppose that the path q1 -v M (q1, v) does not use the edge M (q0, w) -a M (q0, wa)). In

10

A. POLASKI AND E. LIPKA

M (q0, w) a M (q0, wa)

a

v

u2

q1

v

M (q0, wav) u1

A

Figure 2. Example of masking
that case any path with a prefix wav is masking, so we may want to avoid it (as we said, it may not be possible to avoid every possible masking). Before generating test cases we can find which pairs of states can be synchronized in A and which words are used to synchronize them. This can be done by constructing a subautomaton P[2](A) of the power automaton (eg. we take 2- and 1-element subsets of Q as states) and then finding paths from two element states to singletons. For each state q1 we can store a regular expression describing words w, which synchronize q1 with some other state. When choosing among paths that maximize |e(p)  E| we then prioritize those that are not masking according to what we found.
In Roman's example (figure 1) in first iteration of algorithm we can choose two different possible paths p1, p2 which maximize |e(p)E|, namely (p1) = ababaa, (p2) = aabbaa. By looking at the automaton in figure 3 we can easily see that
M (2, b) = M (3, b) = 2.
This is why we do not want to use path with prefix ab, as it would mask the faulty transition 1 -a 3. Hence we should use p2 instead of p1 to avoid the need for another test case recognizing this fault like aa or aaa.
We also have M (1, ab) = M (2, ab) = 2, so we could be masking some transition going into state 1 or 2. There are three such transitions, namely 1 -a 2, 2 -b 2, 3 -b 2, of which the first two are used in synchronizing 1 and 2 by ab. The faulty transition 2 -b 1 will not be masked by p2, but will be by p1. The fault 1 -a 1 will make the SUT trivially reject everything. Hence, the only possible error that could be masked because of the synchronization by ab is 3 -b 1 and this would happen only if our path follows bab from state 3, like in abababaa. Again it is better to take aabbaa, which also maximizes the number of used edges and does not mask.
We can have a similar situation when considered word rejected, for instance in the example automaton there is a synchronization to the negative

DETECTING A SINGLE FAULT IN A DFA

11

sinkhole state (M (Q, aaaa) = X), but again, if a method words for accepted words, it can be trivially modified to work for rejected ones.

5.5. Transitions to a sinkhole. If our automaton has a sinkhole, then it is easy to see that we must have at least one word for each transition leading to it in out test set. Even worse, for each possible fault of each of those transitions we must ensure that we have at least one word that is accepted (for a negative sinkhole) or rejected (for a positive sinkhole) by A. We will describe the procedure for the negative sinkhole, but again, we can easily adapt it for a positive one.
Let Q = {1, 2, . . . , n, X} where X is the negative sinkhole. For any given state q  Q \ {X} and x   such that M (q, x) = X we need to test if this transition holds in SUT. To do this, for any r  Q \ {X} we can take words u, w satisfying M (q0, u) = q, M (r, w)  F and then uxw will test for faulty transition q -x r. This way we generate n test cases for every existing transition from non-sinkhole to sinkhole, but there is a way to use fewer words.
First we find a partition of Q \{X} into subsets S1, . . . , Sm such that each of them can be synchronized to an accepting state; let li be a synchronizing word for Si. Now, for each q  Q \ {X} and x   we take a word w with M (q0, w) = q and add test cases wxl1, . . . , wxlm.
Of course 1  m  n and for any automaton there exists the smallest possible value of m. Finding this value is NP-hard, but even greedy approach gives better results than adding n test cases for every non-sinkholeto-sinkhole transition.
It is worth noting, that there are still sinkhole-to-sinkhole loop transitions that may or may not be covered by this set of test cases. Using this as a preliminary to algorithm 2 we get algorithm 3.

5.6. Summary and an example. To sum up, our proposal for a heuristic polynomial time solution to problem 1 is the algorithm 2, that can be further modified by adding synchronization, choosing the words more carefully and considering faults of transitions leading to sinkholes first (each described in more detail in their respective subsections). All of the presented procedures assumed that we want to minimize the final set, in practice we may of course be satisfied with a more crude approximation of the optimal solution. If not we may try to improve it and still retain polynomial time. If it is still not enough, the exponential algorithm for finding the optimal solution may always be used if the size of the automaton permits it.
Now we will show how our algorithm (with mentioned possible modifications) works on a concrete automaton, we will reuse the one from [1]. On figure 3 we see P[2](A) , it is easy to check that the only 2-element sets, that can
be synchronized to an accepting state are {1, 2} -a-b-aa {A}, {2, 3} -b-aa {A}. We pick a partition S1 = {1, 2}, S2 = {3}, S3 = {A} and generate test cases
Tsink = {babaa, ba, b, aaaaabaa, aaaaa, aaaa, aaababaa, aaaba, aaab}.

12

A. POLASKI AND E. LIPKA

Algorithm 3 Generating test cases Input: a finite automaton A = (Q, , M , q0, F ) Output: a set T   of test cases that detect all possible "incorrect transition" faults

1: construct automaton P[2](A)

2: find synchronization patterns for 2-element sets of states

3: Tsink = , Esink =  4: if X  Q, X is negative sinkhole then

5: find partition of Q \ {X} into S1, . . . , Sm and words l1, . . . , lm such, that li synchronizes Si to an accepting state
6: for q  Q, x   : (q, x) = X do

7:

find w   : (q0, w) = q

8:

Tsink = Tsink  {wxl1, wxl2, . . . wxlm}

9:

Esink = Esink  {(q, x)}

10: run algorithm 2 but with E = E \ Esink, knowing the synchronization patterns and that some faults on sinkhole-to-sinkhole loops could already

be covered by Tsink

11: return T  Tsink

We then enter algorithm 2. In the first pass we pick the word aabbaa instead of ababaa, because we know that it would mask something ( {2, 3} -a {2}). In the second pass we are only left with E = {(X, a), (X, b)}, first of them is tested by set {baaaa, baaa, baa, ba} and second by {bbaaa, bbaa, bba, bb}. However, as mentioned before, some faults on this edges could already be covered by Tsink. In fact babaa covers M (X, a) = 2  M (X, a) = 3  M (X, b) = 2 and ba already is included; we only need to add words baaaa, bbaaa, bba, bb.
This way, we get a 14 element test set
T = {babaa, ba, b, aaaaabaa,aaaaa, aaaa, aaababaa, aaaba, aaab,
aabbaa, baaaa, bbaaa, bba, bb}.
In comparison to Roman's approach it saves only 3 test cases, but due to direct computation we know that minimal set for this particular automaton has at least 12 elements.
6. Further studies
It seems there are more possible ways to improve the heuristic algorithm. One approach may be by exhausting the set of possible faults instead of the set of edges. There is also possibility to extend the algorithm to detect other types of faults.
From theoretical point of view, the question if finding the minimal set of tests is NP-complete remains open (we suspect it is NP-complete).

DETECTING A SINGLE FAULT IN A DFA

13

start

b

a

1a2

3 aA

b

b a,b

b

a,b

X

b

1,2

b

b

a,b

a

b

a

1,X a 2,X

3,X a A,X

2,3

abb

b ab

1,A

1,3 a 2,A

a

a

3,A

Figure 3. Power automaton for Roman's example
References
[1] A. Roman, Data validation using model-based testing and finite automata synchronization, AIP Conference Proceedings 1836, 020007 (2017).
[2] J.E. Hopcroft, R. Motwani, J.D. Ullman, Automata Theory, Languages, and Computation, 3rd ed., Pearson (2014).
[3] S. C. Kleene, Representation of Events in Nerve Nets and Finite Automate, Automata Studies, Annals of Math. Studies. Princeton Univ. Press. 34, sect.9, p.37-40 (1956).
[4] M. R. Garey, D. S. Johnson Computers and Intractability: A Guide to the Theory of NP-Completeness, W. H. Freeman (1979).

