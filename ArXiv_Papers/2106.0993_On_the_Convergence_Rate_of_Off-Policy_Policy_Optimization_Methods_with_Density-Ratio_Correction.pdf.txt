On the Convergence Rate of Off-Policy Policy Optimization Methods with Density-Ratio Correction

arXiv:2106.00993v1 [cs.LG] 2 Jun 2021

Jiawei Huang Department of Computer Science University of Illinois at Urbana-Champaign
Urbana, IL 61801 jiaweih@illinois.edu

Nan Jiang Department of Computer Science University of Illinois at Urbana-Champaign
Urbana, IL 61801 nanjiang@illinois.edu

Abstract
In this paper, we study the convergence properties of off-policy policy improvement algorithms with state-action density ratio correction under function approximation setting, where the objective function is formulated as a max-max-min optimization problem. We characterize the bias of the learning objective and present two strategies with finite-time convergence guarantees. In our first strategy, we present algorithm P-SREDA with convergence rate O(-3), whose dependency on  is optimal. In our second strategy, we propose a new off-policy actor-critic style algorithm named O-SPIM. We prove that O-SPIM converges to a stationary point with total complexity O(-4), which matches the convergence rate of some recent actor-critic algorithms in the on-policy setting.

1 Introduction

Policy improvement is a popular class of methods in empirical reinforcement-learning (RL) research, and has attracted significant attention from the theoretical community recently [Agarwal et al., 2019], with many results analyzing the convergence properties of policy gradientstyle algorithms [Xu et al., 2019b,a, Yuan et al., 2020, Huang et al., 2020b] and actor-critic algorithms [Fu et al., 2020, Wu et al., 2020]. Most existing results require on-policy roll-outs, which are not available in offline RL, a paradigm considered crucial to applying RL in real-world problems [Levine et al., 2020]. While offline policy optimization algorithms also exist [Liu et al., 2019, Imani et al., 2018, Zhang et al., 2019c], the understanding on their finite-time convergence property is still limited.

To close this gap, in this paper, we propose two strategies for off-policy policy optimization with convergence guarantess, based on recent development in marginalized importance sampling (MIS) methods [Liu et al., 2018, Zhang et al., 2019a, Uehara et al., 2019, Yang et al., 2020]. We assume the agent can only get access to a fixed dataset D collected by some unknown policies, and formulate the off-policy learning problem as a max-max-min objective function below: (similar objectives have been considered by Nachum et al. [2019b], Jiang and Huang [2020], Yang et al. [2020] without convergence rate analyses)

max max min LD(, w, Q) := max max min LD(, w , Q)

 wW QQ

 Z 

:=(1 - )Es0D [Q(s0, )] + EdD [w (s, a) r + Q(s, ) - Q(s, a) ]

+

Q 2

EdD

[Q2

(s,

a)]

-

w 2

EdD [w2(s,

a)].

(1)

Preprint.

The early version of this paper published in Offline Reinforcement Learn-

ing Workshop at Neural Information Processing Systems, 2020 can be found in

https://offline-rl-neurips.github.io/program/offrl_3.html.

In this objective, we optimize a parameterized policy    with    being its parameters, and the policy class  can be non-convex. We do so with the help of linearly parameterized functions w  W and Q  Q, which are respectively parameterized by (, )  Z ×  and serve as approximators of the density ratio and the value functions. We assume the parameter spaces   Rdim(), Z  Rdim(Z) and   Rdim() are all convex sets. D and dD are the empirical approximations of the initial state distribution and the state-action distribution in the data; see Sec. 2.1 for a formal definition. Q(s, ) is short for Ea(·|s)[Q(s, a)]. 1
The main contributions of this paper is three-fold, which we summarize below:

A Detailed Analysis of Bias We identify the inevitable bias between the stationary points of LD(, w, Q) and J(), where J() is the expected return of . We separate out and characterize the bias terms due to regularization (reg), generalization (func), and mis-specification (data), as will be detailed in Sec. 3. Besides, our analysis also reveals how Q and w affect the trade-off between the convergence speed and the magnitude of bias. Since the bias is unavoidable,
we focus on the convergence to the points satisfying the following inequality, which we call the
biased stationary points of J() up to  error.

E[ J () ]   + data + func + reg

(2)

where O(·) only suppresses absolute constants. 2 The rest of the paper then presents two optimization strategies for solving Eq.(1) and provides their convergence analyses.

First Strategy: P-SREDA In our first strategy, we convert the original max-max-min problem to a standard non-convex-strongly-concave problem min(,)×Z max -L(, w , Q) by simultaneously optimizing  and  in the outer min. Unfortunately, most of the existing non-convexstrongly-concave optimization algorithms can not be adapted directly here, because they focus on minxRd maxyY f (x, y) where the first player can play an arbitrary vector in Rd, while our objective function requires boundedness of  and  in order to guarantee the smoothness of LD, as we will show in the proof of Property B.1 in Appendix B. To tackle this challenge, we propose P-SREDA,
which is adapted from the SREDA algorithm [Luo et al., 2020] by including a projection step every time after updating . The proof of P-SREDA is non-trivial because the projection step will incur additional error and some crucial steps in the original proof no longer hold in our case. We overcome these difficulties by leveraging the properties of LD and carefully choosing the projection sets for Z and , and prove that its convergence rate remains O(-3), which matches the lower bound in non-convex optimization [Arjevani et al., 2019].

Second Strategy: O-SPIM In our second strategy for solving Eq.(1), we study a novel actor-critic style framework called O-SPIM (Oracle-based Stochastic Policy Improvement with Momentum). In O-SPIM, we alternate between updating  and  (the critic update) and updating  with momentum to achieve a better trade-off (the actor update). The main technique difficulty is that different from policy gradient and actor-critic algorithms in the on-policy setting, here we need to coordinate the updates of three objects ,  and , and the loss LD((·), wt , Qt)--when viewed solely as a function of the policy parameter  with fixed parameters for w and Q--varies across iterations because t, t are updated in the critic step. We handle this difficulty by considering a family of critic update rules that satisfy a general condition (Condition 5.1), which enables us to relate the variations
t+1 - t and t+1 - t with t+1 - t and is crucial to establishing convergence guarantee. We use ORACLE to refer to all critic update subroutines satisfying such a condition, and present two concrete instantiations and analyze their convergence properties in Appendices E.2 and E.3. The first one is a least-square algorithm, and the second one is a first-order algorithm by extending the SVRE [Chavdarova et al., 2019] from finite-sum setting to stochastic setting. Using either instantiation (or possibly other subroutines that satisfy our conditions), the convergence rate of our second
1In this paper, we only give analysis for max maxwW minQQ LD(, w, Q). In fact, we may swap the role of w and Q function and obtain another objective function max maxQQ minwW LD(, w, Q), where LD is the same as L except that both Q and w are negative. In general, the solutions of Eq.(5) and those of this new objective are not the same (see [Jiang and Huang, 2020] for their connections), but we can give a similar anlysis for L with the techniques in this paper and establish similar convergence results.
2All norms · in this paper is 2 norm for vectors and operator norm for matrices unless specified otherwise. The expectation is over the randomness of the algorithm (e.g., the randomness in SGD) and not that of the data.

2

strategy is O(-4), which matches the rate of recent actor-critic algorithms in the on-policy setting [Xu et al., 2020, Fu et al., 2020]
1.1 Related works
Along with the progress of variance reduction techniques for non-convex optimization, there are emerging works analyzing convergence rates for on-policy policy gradient methods [Papini et al., 2018, Xu et al., 2019a,b, Yuan et al., 2020, Huang et al., 2020a]. Besides, in another line of work [Fu et al., 2020, Wu et al., 2020, Hong et al., 2020, Xu et al., 2020], finite-time guarantees are established for actor-critic style algorithms. However, all of these works require online interaction with the environment, whereas our focus is the offline setting.
Turning to the offline setting, there have been substantial empirical developments and success for off-policy PG or actor-critic algorithms [Degris et al., 2012, Lillicrap et al., 2015, Haarnoja et al., 2018, Fujimoto et al., 2018], but most of these works ignore the distribution-shift issue and do not provide convergence guarantees. Recently, there has been a lot of interest in MIS methods for offpolicy evaluation [Liu et al., 2018, Uehara et al., 2019, Zhang et al., 2019a, Nachum et al., 2019a] and turning them into off-policy policy-optimization algorithms. Among them, Liu et al. [2019] presented OPPOSD with convergence guarantees, but the convergence relies on accurately estimating the density ratio and the value function via MIS, which were treated as a black box without further analysis. Nachum et al. [2019b], Jiang and Huang [2020] discussed policy optimization given arbitrary off-policy dataset, but no convergence analysis was performed. Besides, [Zhang et al., 2019c] presented a provably convergent algorithm under a similar linear setting with emphatic weighting [Imani et al., 2018], but only showed asymptotic convergence behavior and did not establish finite convergence rate.
In another work concurrent to ours [Xu et al., 2021], the authors designed DR-Off-PAC, which is motivated by the doubly-robust estimator and their objective function is similar to ours. However, they fixed the coefficient of regularization terms and their bias analysis is much coarser than ours. As a result, their analysis cannot distinguish between the errors resulting from regularization and mis-specification, and do not characterize how the regularization weights affect the trade-off between bias and convergence speed. Besides, the convergence rate of their algorithm is O(-4), which matches our second strategy but worse than our first strategy. Recently, [Lyu et al., 2020] developped VOMPS/ACE-STORM based on Geoff-PAC [Zhang et al., 2019b] and the STORM estimator [Cutkosky and Orabona, 2020]. However, as also pointed by [Xu et al., 2021], the unbiasness of their estimator only holds asymptotically, and the convergence property of their algorithms is still unclear.
We summarize the comparison to closely related works in Table 1.

Table 1: Comparison of Convergence Guarantee of Recent Methods

Algorithms

Convergence Rate Off-Policy? Detailed Bias Analysis?

SVRPG [Xu et al., 2019a]

O(-10/3 )

×

SRVR-PG [Xu et al., 2019b]

O(-3)

×

STORM-PG [Yuan et al., 2020]

O(-3)

×

N.A.

MBPG [Huang et al., 2020b]

O(-3)

×

On Policy AC/NAC [Fu et al., 2020, Xu et al., 2020]

O(-4)

×

DR-Off-PAC [Xu et al., 2021]

O(-4)

×

P-SREDA (Ours)

O(-3)

O-SPIM (Ours)

O(-4)

3

2 Preliminaries

2.1 Markov Decision Process

We consider an infinite-horizon discounted MDP (S, A, R, P, , 0), where S and A are the state and action spaces, respectively, which we assume to be finite but can be arbitrarily large. R :

S × A  ([0, 1]) is the reward function. P : S × A  (S) is the transition function,  is the

discount factor and 0 denotes the initial state distribution.

Fixing an arbitrary policy , we use d(s, a) = (1 - )E,s00 [

 t=0

tp(st

=

s, at

=

a)]

to denote the normalized discounted state-action occupancy, where   , s0  0 means a tra-

jectory  = {s0, a0, s1, a1, ...} is sampled according to the rule that s0  0, a0  (·|s0), s1 

P (·|s0, a0), a1  (·|s1), ..., and p(st = s, at = a) denotes the

pair are exactly Q-function of .

(s, a). We also use Q(s, a) = It is well-known that Q satisfies

E ,s0=s,a0 the Bellman

probability that the

=a[

 t=0



tr(st

,

at

Equation:

t-th state-action )] to denote the

Q(s, a) =T Q(s, a) := ErR(s,a),sP (·|s,a),a(·|s)[r + Q(s, a)].

Define

J ()

=

Es0

,a(·|s0

)

[Q

(s,

a)]=

1 1-

Es,ad

[r(s,

a)]

as

the

expected

return

of

policy

.

If  is parameterized by  and differentiable, we have

J ()

=

1

1 -

 Es,ad [Q(s, a)

log (a|s)]

=

1

1 -

 Es,aµ[w(s, a)Q(s, a)

log (a|s)].

where the first step is the policy-gradient theorem [Sutton et al., 2000], and in the second step, we

replace d with distribution µ, a state-action distribution generated by some behavior policies, and

introduce the

density

ratio

w(s, a)

:=

d (s,a) µ(s,a)

.

In the rest of the paper, we assume we are only provided with a fixed off-line dataset D =

{(si, ai, ri, si)}|iD=1| , where each tuple is sampled according to si, ai  µ, ri  R(si, ai), si  P (·|si, ai). Besides, we do not require the knowledge of behavior policies that generate µ. As we also mentioned in the introduction, we will use dD to denote the empirical state-action distribution

induced

from

the

dataset,

which

is

defined

by

dD(s, a)

=

1 |D|

(si,ai,ri,si)D I[si = s, ai = a].

Since the state-action space can be very large, we consider generalization via linear function approximation:

Definition 2.1 (Linear function classes). Suppose we have two feature maps {W : S × A  Rdim(Z)} and {Q : S × A  Rdim()} subject to w(·, ·)  1, Q(·, ·)  1. Let Z  Rdim(Z),   Rdim() be the corresponding parameter spaces, respectively, satisfying CW := max{1, maxZ  } <  and CQ := max  < . The approximated value function Q and density ratio w are represented by
w(·, ·) = w(·, ·), Q(·, ·) = Q(·, ·).

We use w  R|S||A|×dim(Z) to denote the matrix whose (s, a)-th row is w(s, a), and use Kw to denote wDw, where D is a diagonal matrix whose diagonal elements are dD(·, ·). Similarly, we define Q and KQ for Q(·, ·). Besides, M denotes w D(I - PD)Q, where PD is the empirical transition matrix induced from the data distribution. Notes that if we never see some s in dataset, then the corresponding element in D should be 0, and therefore, the corresponding row in PD can just be set arbitrarily without having any effects on the loss function. Under linear function classes, we can rewrite LD in Eq.(1) as:

LD (,

,

)

=(1

-

)(D )Q

+

w DR

-

  M 

+

Q 2

KQ

-

w 2

  Kw  .

(3)

We will also denote  as the diagonal matrix whose diagonal elements are µ(·, ·), and denote P  as

the transition matrix of .

2.2 Assumptions
Our first assumption is about the policy class, which is practical and frequently considered in the literature [Zhang et al., 2019c, Wu et al., 2020]. In this paper, we choose  = Rdim() and we will interchangeably use  and Rdim() to denote the space of policy parameter .

4

Assumption A (Smoothness of policy function). For any s, a  S × A and    = Rdim(), (s, a) is second-order differentiable w.r.t. , and there exist constants L, G and H, s.t. for arbitrary 1, 2,   :
1 (·|s) - 2 (·|s) 1  L 1 - 2 ,  log (a|s)  G, 2 log (a|s)  H. (4)

We also assume Kw, KQ and M are non-singular.
Assumption B. Denote vmin(·) as the function to return the minimum singular value. We assume that vw := vmin(Kw) > 0 and vQ := vmin(KQ) > 0. Beisdes, there exists a positive constant vM that, for arbitrary    (i.e.   ), the matrix M is invertible, and vmin(M)  vM > 0.

The next assumption characterizes the coverage of µ using a constant C, often often known as the concentrability coefficient in the literature [Szepesva´ri and Munos, 2005, Chen and Jiang, 2019].

Assumption C (Exploratory Data). Recall the behavior policy is denoted as µ. We assume there exists a constant C > 0, for arbitrary    and any (s, a)  S × A, we have

w(s, a)

:=

d(s, a) µ(s, a)



C,

wµ(s, a)

:=

dµ(s, a) µ(s, a)



C

where dµ(s, a) := (1-)E ,s0,a0µ(·,·)[

 t=0

tp(st

=

s,

at

=

a)]

is

the

normalized

discounted

state-action occupancy by treating µ as initial distribution.

Finally, since w and Q has bounded norm, the following variance terms are therefore bounded, and we use (·) to denote their minimal upper bounds: Assumption D. [Variance] There are constants K, M, R and  satisfying
Es,adD [ Kw - w(s, a)w(s, a) 2]  K2 , Es,adD [ KQ - Q(s, a)Q(s, a) 2]  K2 , Es,adD [ w DR - w(s, a)r(s, a) 2]  R2 , Es0D,a(·|s)[ QD - Q(s, a) ]  2, Es,a,sdD,a(·|s)[ M - w(s, a)(Q(s, a) - Q(s, a)) ]  M 2 .

2.3 Useful Properties

Under Assumptions above, we have the following important property regarding the convexity, concavity and smoothness of our objective function, which will be central to our proofs. Due to space limits, we defer a more detailed version (Property B.1) in Appendix B, in addition to some other useful properties and their proofs.
Property 2.2 (Convexity, Concavity and Smoothness). For arbitrary   ,   Z, LD is µstrongly convex w.r.t.   , and for arbitrary   ,   , LD is µ-strongly concave w.r.t.   Z, where µ = QvQ and µ = wvw. Besides, given that CQ and CW in Definition 2.1 is finite, LD is L-smooth with finite L.
In the rest of paper, we will use  := /L and  := /L as a short note of the condition number.

3 The biased stationary points

As alluded to in the introduction, even if we optimize Eq.(1), we do not expect that it converges to the a true stationary point w.r.t. J() due to regularization, function-class mis-specification, and finite-sample effects, which result in the three error terms in Eq.(2). More concretely, by the
triangular inequality,

J ()





max
wW

min
QQ

LD

(

,

w,

Q)

+

 J

(

)

-



max
wW

min
QQ

LD

(

,

w,

Q)

.

Optimizing our loss function LD(, w, Q) reduces the first term but leaves the second term intact. In the rest of this section we bound the second term by breaking it down to more basic quantities that can be bounded under further standard assumptions. To do so, we need some additional definitions: We use L(w, , Q) (as well as L(, w, Q)) to denote the asymptotic version of LD as |D|  :

max max min L(, w, Q) := max max min L(, w, Q)

 wW QQ

 Z 

5

:=(1 - )Es00 [Q(s0, )] + Eµ[w (s, a) r + Q(s, ) - Q(s, a) ]

+

Q 2

Eµ [Q2 (s,

a)]

-

w 2

Eµ[w2(s,

a)].

(5)

Definition 3.1 (Generalization Error). We define ¯data to be the minimal value satisfying that, for arbitrary , w , Q   × W × Q, we have:

|L(, w , Q) - LD(, w , Q)|  ¯data, L(, wµ , Qµ) - LD(, wµ , Qµ) 2  ¯data where (wµ , Qµ) := arg maxwW minQQ L(, w, Q).

¯data characterizes the uniform deviation between L and LD (L and LD, resp.) and can be bounded by the sample size and the complexity of the function classes. Such uniform convergence
bounds are standard and we do not further analyze it in this paper.

Definition 3.2 (Mis-specification Error). Denote ·  as the 2-norm weighted by , and define wL = arg maxwR|S||A| minQR|S||A| L(, w, Q) given arbitrary   . We define

1 := max min
 wW

w - wL

2 

,

2 := max
wW ,

min
QQ

Q-

arg min

L(, w, Q)

2 

QR|S||A|

Based on these definitions and Assumptions in Section 2.2, in the following theorem, we provide an upper bound for the bias between the stationary points of our objective function LD and J(), which is one of our main contributions. The proofs are deferred to Appendix C.
Theorem 3.3. [Bias] Based Assumptions in Section 2.2 and Definitions above, for arbitrary   , we have:



max
wW

min
QQ

LD

(

,

w,

Q)

-



J

(

)

 reg + func + data

The bias terms reg, func and data are defined by

f unc

:=

G 1-

CQ + CW

QC 1-

+

QW C 1-

 + CQ W

reg

:=

1

G -



C2 (1 - 

)

(

w Q 1-

+ w)

+

C(Q + QwC) (1 - )3

+

C

2

(Q (1

+ -

Qw )3

C

)

(

w Q 1-

+ w)



data := (2  + 2 + 2 + 2/2) 2¯data

where  and  is the condition number, and

W

:=

4

2max Qw

1

+

2

max µ

2

,

Q

:=

8

3max 2Qw

1

+

(2

+

4

2max Qµ

)2

with max := max{Q, w}.

C 1-

As we can see,  maxwW minQQ LD(, w, Q)-J () can be controlled by three terms. data reflects the generalization error and decreases with sample size (assuming all function classes have bounded capacity and allow uniform convergence). reg depends on the magnitude of regularization, and will decrease if w and Q decreases. As for func, it depends on the approximation error W and Q, which are proportional to 1 and 2. Besides, because µ should be proportional to w, the coefficients before 1 and 2 will not vary significant as we change w and Q, as long as w  Q. (1 and 2 themselves may change with w and Q). Overall, a large dataset, wellspecified function classes, and small w and Q will result in a small total bias, while small w and Q can lead to weaker strong-concavity or strong-convexity of the loss function, resulting in slower convergence.
Based on the discussion above, our goal is to find stochastic optimization algorithms, which return a policy  after consuming poly(-1) samples from dataset (we omit the dependence on quantities such as µ and µ), satisfying the biased stationary condition in Eq.(2)

6

Global Convergence Under Polyak-Lojasiewicz Condition If we assume the policy function class satisfies certain additional conditions, we can establish the global convergence guarantee. Take the Polyak-Lojasiewicz condition as an example, which requires that, there exists a constant cP L > 0, s.t., for arbitrary   

1 2

J ()

2  cP L(J  - J ()).

(6)

where J is the value of the optimal policy. As a result, if the policy class satisfies the PL inequality,

a biased stationary point  in Eq.(2) indicates that,

E[J ()]



J

-

1 cP L

(2

+

2data

+

2f unc

+

2reg ).

Such condition has been considered in previous works [Mei et al., 2020], and it is provably weaker than the strongly convexity/concavity [Karimi et al., 2020].

4 Strategy 1: Projected-Stochastic Recursive Gradient Descent Ascent

In our first optimization strategy, we rewrite the original max-max-min problem as a max-min:

arg max
Rd

max
Z

min


LD (,

,

)



arg max
,Rd ×Z

min


LD (,

,

)

=

arg min
,Rd ×Z

max


LD- (,

,

)

where we use LD- as a shorthand of -LD. Given Assumptions in Sec. 2.2, we know arg min,Rd ×Z max LD- (, , ) is a non-convex-strongly-concave problem, which has been the focus of some prior works [Lin et al., 2019, Luo et al., 2020]. That said, most of them target at minxRd maxyY f (x, y) where the first player can play an arbitrary vector in Rd, while in our setting,  and  have to be constrained in bounded sets to guarantee that LD (and LD- ) is L-smooth with finite L (see the proof of Property B.1 in Appendix B for details).

Therefore, we introduce Projected-SREDA (Algorithm 1), where we project  back to the convex

set Z every time after update. However, the proofs for original SREDA [Luo et al., 2020] cannot be

adapted directly because the projection step will incur extra error. To overcome this difficulty, we

first study the following property of our objective function, which illustrate that the saddle-points of LD given arbitrary    have bounded l2-norm.

Property 4.1. Denote Z0 = {|   R} and 0 = {|   R} with R :=

( 1

1- 2

w Q vw +vM 2 vQ

+ Q)

and

R

:=

w

Q

1 vQ

+vM 2

((1

-

)w

+

1+ vw

).

For arbitrary 

 ,

we have (,  ) := arg maxRdim(Z) minRdim() LD(, w , Q)  Z0 × 0.

As a result, we have Lemma 4.2 below, which is a crucial step to controlling the projection error. We carefully choose the projection set Z and ; as we increase the diameter of Z (i.e. R) and fix the diameter of  as R0, the angle between  LD- (k, k, k) and the projection direction k+1 - k++1 will increase, which reflects the decrease of the relative projection error.

Lemma 4.2. According to the updated rule of Alg 1, at iteration k, we have k  Z, k++1 =

k - kvk and k+1 = PZ (k++1). Denote R0 := Property 2.2. If we choose  = 0 and Z = {| 

1
wRvw

(1 + (1 +  } with R 

)R R0,

) where R we have:

is

defined

in

 LD- (k, k, k), k+1 - k++1



k

R0 R

+ +

k k

vk vk

 LD- (k, k, k) vk .

Based on this lemma and an appropriate choice of R, we can show that Algorithm 1 converges to the biased stationary points. We omit the detail of the hyper-parameter choices due to space limits, and a detailed version of the following theorem and its proof are deferred to Appendix D.
Theorem 4.3 (Informal). For  < 1, under Assumptions in Section 2.2, by choosing  = 0 and Z = {|   R := 8 max{R0, 1}}, Algorithm 1 will return  satisfy the following with O(-3) stochastic gradient evaluations.

E[ J () ]   + reg + func + gen

7

Algorithm 1: Projected SREDA

1 Input: initial point (0, 0), learning rates k,  > 0, batch size S1, S2 > 0; periods q, m > 0, number of initial iterations K0; Convex Sets  and Z; Two functions PiSARAH and ConcaveMaximizer in [Luo et al., 2020].
2 0 = PiSARAH(-LD- (0, 0, ·), K0) 3 for k = 0, ..., K - 1 do

4 if mod(k, q) = 0 then

5

draw S1 samples and compute:

6

vk := (vk , vk ) = , LS-1 (k, k, k), uk = LS-1 (k, k, k)

7 end

8 else

9

vk = vk , uk = uk

10 end

11

k+1 = k - kvk , k++1 = k - kvk , k+1 = PZ (k++1)

12 (vk +1, uk+1) = ConcaveMaximizer(k, m, S2, (k, k), (k+1, k+1), k, uk, vk, )

13 end

14 Output: (, ) chosen uniformly at random from {(k, k)}Kk=-01

5 Strategy 2: Oracle-based Stochastic Policy Improvement with Momentum

A natural question is that, can we solve max-max-min objective directly without merging max max together? We answer it firmly in this section by proposing an off-policy actor-critic style algorithm named O-SPIM in Algorithm 2, where we solve  and  with an abstract subroutine ORACLE in the critic step, and update  with momentum in the actor step. Our analysis of
O-SPIM also contributes to the understanding of convergence properties of actor-critic algorithms
in the off-policy setting.

Different from the on-policy setting, where distribution shift is not a problem, our objective function has three variables to optimize, which causes difficulty in analyzing convergence property. Therefore, we enforce the following condition on ORACLE to coordinate the actor and critic steps.
Condition 5.1. For any strongly-concave-strongly-convex objective maxZ min LD(, , ) with saddle point (, )  Z × , and arbitrary 0    1 and c > 0, starting from a random initializer (¯, ¯)  Z × , ORACLE can return a solution (, ) satisfying

E[  -  2 +

 - 

2] 

 2

E[

¯ - 

2+

¯ - 

2] + c.

(7)

5.1 Analysis of O-SPIM

The key insight of Condition 5.1 is the following lemma, in which we make a connection between the actor step and the critic step. As a result, in addition to the smoothness of LD, we can control the shift between LD(t, t, t) and LD(t+1, t+1, t+1) only with the shift of  (i.e. g ) and hyper-parameters  and c, which paves the way to comparing LD(, t, t) with J() and finally establishing the convergence guarantee.
Lemma 5.2. [Relate the shift of t and t with t] Denote (t, t, t) as the parameter value at the beginning at the step t in Algorithm 2, and denote (t, t) = arg maxZ min LD(t, , ) as the only saddle point given t. Under Assumptions in Section 2.2, we have:

E[ t+1 - t 2 +

t+1 - t 2] 6t+1d2 + 62C,

t

t- E[

g

2]

+

1

6c -

.

 =0

where d := max{CW , CQ} is the maximum of diameters of Z and , C, is a short note of

2µ(

+ 1)2

+

2 (µ

+

1)2,

g

:=

1 

(

+1

-

 ),

and



is

the

step

size

of

.

In the actor step, we introduce another hyper-parameter  and adopt a momentum-based update rule, aiming at a better trade-off between the variance of the gradient estimation in the current step

8

and the bias of using accumulative gradient in previous steps. As we will show in the proof of our main theorem (Theorem F.4 in Appendix F), by choosing  appropriately, although the trade-off cannot improve the dependence of  in the convergence rate, it can indeed reduce the upper bound of E[ J() ] comparing with the case without momentum (i.e.  = 1)
Now, we are ready to state the main theorem of our second strategy. We defer the formal version including the hyper-parameter choices and its proof to Theorem F.4 in Appendix F.
Theorem 5.3 (Informal). Under Assumptions in Section 2.2, given arbitrary , with appropriate hyper-parameter choices, by using either Algorithm 3 or Algorithm 4 as Oracle, Algorithm 2 will return us a policy  with total complexity O(-4), satisfying
E[ J () ]   + reg + data + func.
5.2 Concrete Examples of ORACLE
We provide two concrete examples of ORACLE, and defer the algorithm details and the discussion about related works to Appendix E. In the linear setting, the saddle-point (, ) has a closed form, which can be regarded as the regularized LSTD-Q solution [Kolter and Ng, 2009] and can be represented by Kw, KQ, M and so on. Therefore, an natural solution is to estimate these matrices from (a subsample of) the dataset and plug them into the closed-form solution. We prove that such an algorithm satisfies our definition of ORACLE and we defer the discussion to Appendix E.2.
However, similar to the LSTD, the per-step computational complexity of the first solver is quadratic to the dimension of the feature, which can be expensive for high dimensional features. Therefore, in Appendix E.3, we present a first-order algorithm inspired from the Stochastic Variance Reduced Extra-gradient [Chavdarova et al., 2019], which reduces the per-step complexity to O(d) and also satisfies the ORACLE condition. Besides, our second example of ORACLE can also handle general strongly-convex-strongly-concave problem beyond the linear setting.

Algorithm 2: Stochastic Momentum Policy Improvement with Oracle

1 Input: Total number of iteration T ; Learning rate , , ; Dataset distribution dD; ORACLE

parameter .

2 Set Z = Z0 and  = 0 with Z0 and 0 defined in Property 4.1. 3 Initialize 0, -1, -1

4 0, 0  Oracle(T1,  , , 0, -1, -1, dD)

5 Sample B0  dD with batch size |B0| and estimate batch gradient g0 = LB0 (0, 0, 0) 6 for t = 0, 1, 2, ...T - 1 do

7

t+1  t + gt ;

8 t+1, t+1  Oracle(, t+1, t, t, dD);

9 Sample B  dD;

10 gt+1  (1 - )gt + LB(t+1, t+1, t+1) // update g with batch gradient

11 end

12 Output: Sample   Unif{0, 1, ..., T } and output .

6 Conclusion
In this paper, we study two natural optimization strategies for density-ratio based off-policy policy improvement, establish their convergence rates, and characterize the quality of the results. In the future, it will be interesting to extend the results to other settings with milder assumptions, and improve the dependence on  on the convergence rate of our second strategy.

9

References
Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. Optimality and approximation with policy gradient methods in markov decision processes. arXiv preprint arXiv:1908.00261, 2019.
Yossi Arjevani, Yair Carmon, John C Duchi, Dylan J Foster, Nathan Srebro, and Blake Woodworth. Lower bounds for non-convex stochastic optimization. arXiv preprint arXiv:1912.02365, 2019.
Tatjana Chavdarova, Gauthier Gidel, Franc¸ois Fleuret, and Simon Lacoste-Julien. Reducing noise in gan training with variance reduced extragradient. In Advances in Neural Information Processing Systems, pages 393­403, 2019.
Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning. In Proceedings of the 36th International Conference on Machine Learning, pages 1042­1051, 2019.
Ashok Cutkosky and Francesco Orabona. Momentum-based variance reduction in non-convex sgd, 2020.
Thomas Degris, Martha White, and Richard S. Sutton. Off-policy actor-critic. CoRR, abs/1205.4839, 2012. URL http://arxiv.org/abs/1205.4839.
Simon S. Du, Jianshu Chen, Lihong Li, Lin Xiao, and Dengyong Zhou. Stochastic variance reduction methods for policy evaluation. volume 70 of Proceedings of Machine Learning Research, pages 1049­1058, International Convention Centre, Sydney, Australia, 06­11 Aug 2017. PMLR. URL http://proceedings.mlr.press/v70/du17a.html.
Zuyue Fu, Zhuoran Yang, and Zhaoran Wang. Single-timescale actor-critic provably finds globally optimal policy, 2020.
Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in actor-critic methods, 2018.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor, 2018.
Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. A two-timescale framework for bilevel optimization: Complexity analysis and application to actor-critic, 2020.
F. Huang, Shangqian Gao, Jian Pei, and H. Huang. Momentum-based policy gradient methods. ArXiv, abs/2007.06680, 2020a.
Feihu Huang, Shangqian Gao, Jian Pei, and Heng Huang. Momentum-based policy gradient methods. In Hal Daume´ III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 4422­4433. PMLR, 13­18 Jul 2020b. URL http://proceedings.mlr.press/v119/huang20a.html.
Ehsan Imani, Eric Graves, and Martha White. An off-policy policy gradient theorem using emphatic weightings. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montre´al, Canada, pages 96­106, 2018.
Nan Jiang and Jiawei Huang. Minimax confidence interval for off-policy evaluation and policy optimization. arXiv preprint arXiv:2002.02081, 2020.
Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximalgradient methods under the polyak-lojasiewicz condition, 2020.
J. Z. Kolter and A. Ng. Regularization and feature selection in least-squares temporal difference learning. In ICML '09, 2009.
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.
10

Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning, 2015.
Tianyi Lin, Chi Jin, and Michael I Jordan. On gradient descent ascent for nonconvex-concave minimax problems. arXiv preprint arXiv:1906.00331, 2019.
Tianyi Lin, Chi Jin, Michael Jordan, et al. Near-optimal algorithms for minimax optimization. arXiv preprint arXiv:2002.02417, 2020.
Bo Liu, Ji Liu, Mohammad Ghavamzadeh, Sridhar Mahadevan, and Marek Petrik. Finite-sample analysis of proximal gradient td algorithms, 2020.
Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou. Breaking the curse of horizon: Infinitehorizon off-policy estimation. In Advances in Neural Information Processing Systems, pages 5361­5371, 2018.
Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill. Off-policy policy gradient with state distribution correction. CoRR, abs/1904.08473, 2019. URL http://arxiv.org/abs/1904.08473.
Luo Luo, Ye Haishan, and Zhang Tong. Stochastic recursive gradient descent ascent for stochastic nonconvex-strongly-concave minimax problems. 2020.
Daoming Lyu, Qi Qi, Mohammad Ghavamzadeh, Hengshuai Yao, Tianbao Yang, and Bo Liu. Variance-reduced off-policy memory-efficient policy search, 2020.
Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. On the global convergence rates of softmax policy gradient methods. In Hal Daume´ III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119, pages 6820­6829, 2020.
Ofir Nachum, Yinlam Chow, Bo Dai, and Lihong Li. Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, pages 2315­2325, 2019a.
Ofir Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans. Algaedice: Policy gradient from arbitrary experience. arXiv preprint arXiv:1912.02074, 2019b.
Matteo Papini, Damiano Binaghi, Giuseppe Canonaco, Matteo Pirotta, and Marcello Restelli. Stochastic variance-reduced policy gradient. arXiv preprint arXiv:1806.05618, 2018.
Richard S Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In S. A. Solla, T. K. Leen, and K. Mu¨ller, editors, Advances in Neural Information Processing Systems 12, pages 1057­1063. MIT Press, 2000.
Csaba Szepesva´ri and Re´mi Munos. Finite time bounds for sampling based fitted value iteration. In Proceedings of the 22nd international conference on Machine learning, pages 880­887, 2005.
Joel A. Tropp. An introduction to matrix concentration inequalities, 2015.
Masatoshi Uehara, Jiawei Huang, and Nan Jiang. Minimax weight and q-function learning for off-policy evaluation. arXiv preprint arXiv:1910.12809, 2019.
Yue Wu, Weitong Zhang, Pan Xu, and Quanquan Gu. A finite time analysis of two time-scale actor critic methods, 2020.
Pan Xu, Felicia Gao, and Quanquan Gu. An improved convergence analysis of stochastic variancereduced policy gradient. arXiv preprint arXiv:1905.12615, 2019a.
Pan Xu, Felicia Gao, and Quanquan Gu. Sample efficient policy gradient methods with recursive variance reduction. arXiv preprint arXiv:1909.08610, 2019b.
Tengyu Xu, Zhe Wang, and Yingbin Liang. Non-asymptotic convergence analysis of two time-scale (natural) actor-critic algorithms, 2020.
11

Tengyu Xu, Zhuoran Yang, Zhaoran Wang, and Yingbin Liang. Doubly robust off-policy actorcritic: Convergence and optimality, 2021.
Mengjiao Yang, Ofir Nachum, Bo Dai, Lihong Li, and Dale Schuurmans. Off-policy evaluation via the regularized lagrangian. arXiv preprint arXiv:2007.03438, 2020.
Huizhuo Yuan, Xiangru Lian, Ji Liu, and Yuren Zhou. Stochastic recursive momentum for policy gradient methods. arXiv preprint arXiv:2003.04302, 2020.
Ruiyi Zhang, Bo Dai, Lihong Li, and Dale Schuurmans. Gendice: Generalized offline estimation of stationary values. In International Conference on Learning Representations, 2019a.
Shangtong Zhang, Wendelin Boehmer, and Shimon Whiteson. Generalized off-policy actor-critic, 2019b.
Shangtong Zhang, Bo Liu, Hengshuai Yao, and Shimon Whiteson. Provably convergent twotimescale off-policy actor-critic with function approximation, 2019c.
12

A Useful Lemma

Lemma A.1 (Lemma B.2 in [Lin et al., 2020]). Define

 ( )

=

min


LD (,

,

)

 ( )

=

max
Z

LD

(,

,

)

() = arg min LD(, , ),

() = arg max LD(, , ),
Z

f or   Rdim(Z) f or   Rdim()

Under Assumption A and B, for fixed , we have:

(1)

The

function

 (·)

is



=

L µ

-Lipschitz.

(2) The function (·) is 2L

=

2

L2 µ

-smooth

and

µ -strongly

concave

with

 (·)

:=

 LD(, , ()).

(3)

The

function

(·)

is



=

L µ

-Lipschitz.

(4) The function (·) is 2L

=

2

L2 µ

-smooth

and

µ -strongly

convex

with

(·)

:=

LD(, (), ).

Remark A.2 (For clarification). According to Danskin's Theorem,

(·) :=  LD(, , ()) =  LD(, , )|=() Therefore, when we compute LD(, , ()), we can treat () as a constant. Then, for arbitrary , , based on Assumption A, we always have:

(·) -  LD(, , )  L  -  + L () - 

We have a similar clarification w.r.t. (). Lemma A.3. For -strongly-convex function f (x) and -strongly-concave function g(x) w.r.t. x  X, where X  Rn is a convex set, we have

x - xf



1 

xf (x)

,

 2

x - xf

2  f (x) - f (xf )

(8)

x - xg



1 

xg(x) ,

 2

x - xf

2  g(xg) - g(x)

(9)

where xf and xg the minimum and maximum of f (x) and g(x), respectively.

Proof. Since f (x) is -strongly-convex, we have

(xf (x) - xf (xf ))(x - xf )   x - xf 2,

f (x)



f (xf )

+

xf (xf )(x

-

xf )

+

 2

x - xf

2.

Since xf is the minimizer of f (x), we know that xf (xf )(x - xf )  0, x  X. Combining all the above inequalities together and we obtain

x - xf

2



1 

x

f

(x)(x

- xf )



1 

xf (x)

which implies

x - xf ,

f (x)



f (xf )

+

 2

x - xf

2.

x - xf



1 

xf (x)

,

 2

x - xf

2  f (x) - f (xf ).

By applying the above results for -g(x) which is a -strongly-convex function and we can complete

the proof.

Lemma A.4. For positive definite matrix A, and arbitrary  > 0, we have:

(AA)-1  (I + A)(I + A) -1

13

Proof. Suppose for symmetric matrix A and B, we have the relationship A  B  0. According to the inverse matrix lemma, we have
B-1 - A-1 = B-1 - (B + (A - B))-1 = (B + B(A - B)-1B)-1

Because A  B  0, we have (B + B(A - B)-1B)-1  0, therefore B-1  A-1. Then, we only need to prove
(I + A)(I + A) AA

We have

(I + A)(I + A) = 2I + (A + A) + AA

Combining A = A  0 and  > 0, we can finish the proof.

Lemma A.5 (Non-negative Elements). We use pose of the transition kernel. All the elements

P in

= (I

(P)  R|S||A|×|S||A| to denote the trans- P )-1 are non-negative. Moreover, the

element indexed by (si, aj) in row and (sp, aq) in column equals to the unnormalized discounted

state-action occupancy of (si, aj) starting from (sp, aq) and executing .

Proof. For arbitrary initial state-action distribution vector µ0  R|S||A|×1, (I - P )-1µ0 is a vector whose elements are unnormalized state-action occupancy with µ0 as initial distribution, which is larger or equal to 0. As a result, by choosing standard basis vector as µ0, we can finish the
proof.

B Useful Properties Implied by Assumptions in 2.2
In this section, we first prove several properties implied by our basic assumptions in Section 2.2. Property B.1. [A detailed version of Property 2.2] Under Assumption A and B, given that CW := max{1, maxZ  } and CQ := max  are finite, we have:
(a) For arbitrary   ,   Z, LD is µ-strongly convex w.r.t.   , and for arbitrary   ,   , LD is µ-strongly concave w.r.t.   Z, where µ = QvQ and µ = wvw.
(b) For any , 1, 2  , , 1, 2  Z, (s, a)  S × A, |Q(s, a)|  CQ; |Q1 (s, a) - Q2 (s, a)|  1 - 2 ; |w (s, a)|  CW ; |w1 (s, a) - w2 (s, a)|  1 - 2 .
(c) For any 1, 2  Z, 1, 2  , 1, 2  , LD defined in Eq.(1) is differentiable, and there exists constant L s.t.
LD(1, 1, 1) - LD(2, 2, 2) +  LD(1, 1, 1) -  LD(2, 2, 2) + LD(1, 1, 1) - LD(2, 2, 2) L 1 - 2 + L 1 - 2 + L 1 - 2 .
In other words, LD is L-smooth when   Z,   ,   .

Proof.
Proof of (a) Since LD is second-order differentible w.r.t. arbitrary   Rd and   Rd , under Assumption B, we have:
2 LD = -wKw  -wvwI, 2 LD = QKQ  QvQI. where I is the identity matrix.

14

Proof of (b) Because w and Q are linear and features has bounded l2-norm, and Z and  are all convex sets with bounded radius, we have:
|Q(s, a)|  Q(s, a)  CQ, |Q1 (s, a) - Q2 (s, a)|  1 - 2 ; |w (s, a)|  w(s, a)  CW , |w1 (s, a) - w2 (s, a)|  1 - 2 ;
Therefore, Property B.1-(b) holds.

Proof of (c) We will use , w1, Q1, 1 and LD1 as shortnotes of w1 , Q1 , 1 and LD(1, 1, 1), and the meaning of w2, Q2, 2 and LD2 are similar.
LD1 - LD2 = (1 - )ED,a1(·|s0)[Q1(s0, a) log 1(a|s0)] - (1 - )ED,a2(·|s0)[Q2(s0, a) log 2(a|s0)]
+ EdD,a1(·|s)[w1(s, a)Q1(s, a) log 1(a|s)] - EdD,a2(·|s)[w2(s, a)Q2(s, a) log 2(a|s)]
(1 - ) ED,a1(·|s0)[ Q1(s0, a) - Q2(s0, a)  log 1(a|s0)]

+ ED,a1(·|s0)-2(·|s0)[Q2(s0, a) log 1(a|s0)] + ED ,a2(·|s0)[Q2(s0, a)  log 1(a|s0) -  log 2(a|s0) ]
+  EdD,a1(·|s)[ w1(s, a) - w2(s, a) Q1(s, a) log 1(a|s)] + EdD,a1(·|s)[w2(s, a) Q1(s, a) - Q2(s, a)  log 1(a|s)] + EdD,a1(·|s)-2(·|s)[w2(s, a)Q2(s, a) log 1(a|s)] + EdD,a2(·|s)[w2(s, a)Q2(s, a)  log 1(a|s) -  log 2(a|s) ]

(1 - ) G 1 - 2 + GCQED [ (·|s0) - (·|s0) 1] + HCQ 1 - 2

+  GCQ 1 - 2 + GCW 1 - 2 + GCW CQEdD [ 1(·|s) - 2(·|s) 1] + HCW CQ 1 - 2

CW CQ(GL + H) 1 - 2 + GCQ 1 - 2 + GCW 2 - 2

In the last inequality, we use CW  1 and 0 <   1. Besides,

 LD1 -  LD2 LD1 - LD2

= (M1 - M2 )1 + M2 (1 - 2) - wKw(1 - 2) CQEdD [ 1(·|s) - 2(·|s) 1] + (1 + ) 1 - 2 + w 1 - 2 CQL 1 - 2 + (1 + ) 1 - 2 + w 1 - 2
= (M1 - M2 )1 + M2 (1 - 2) + QKQ(1 - 2) CW EdD [ 1(·|s) - 2(·|s) 1] + (1 + ) 1 - 2 + Q 1 - 2 CW L 1 - 2 + (1 + ) 1 - 2 + Q 1 - 2

As a result, LD(1, 1, 1) - LD(2, 2, 2) +  LD(1, 1, 1) -  LD(2, 2, 2) + LD(1, 1, 1) - LD(2, 2, 2)

 CW CQ(GL + H) + (CQ + CW )L 1 - 2 + GCQ + (1 + ) + w 1 - 2

+ GCW + (1 + ) + Q 1 - 2

Therefore, Condition A-(b) holds with
L = max{CW CQ(GL + H) + (CQ + CW )L, GCQ + (1 + ) + w, GCW + (1 + ) + Q} (10)

Property 4.1. Denote Z0 = {|   R} and 0 = {|   R} with R :=

( 1

1- 2

w Q vw +vM 2 vQ

+ Q)

and

R

:=

w

Q

1 vQ

+vM 2

((1

-

)w

+

1+ vw

).

For arbitrary 

 ,

we have (,  ) := arg maxRdim(Z) minRdim() LD(, w , Q)  Z0 × 0.

15

Proof. Recall the definition of LD in Eq.(3):

LD(, ,

)

=(1

-

)(D )Q

+

w DR

-

  M 

+

Q 2

KQ

-

w 2

  Kw  .

by taking derivatives w.r.t.  and setting it to be zero, we have:



=

1 Q

K-Q1

M  - (1 - )QD

Plug it into LD:

-

w 2

  Kw 

-

1 2Q

M  - (1 - )QD K-Q1 M  - (1 - )QD

+ w DR

Taking the derivative of  and set it to be zero, we have:

 = wQKw + MK-Q1M -1 - (1 - )MKQ-1QD + Qw DR

and therefore,



=

1 Q

K-Q1

M  - (1 - )QD

=

1 Q

K-Q1

M

wQKw + MK-Q1M

-1
·

- (1 - )MK-Q1QD + Qw DR

+

(1

-



)

1 Q

K-Q1Q

(D )

=(1 - )w wQKQ + M K-w1M -1QD + K-Q1M wQKw + MK-Q1M -1w DR

= wQKQ + M K-w1M -1 (1 - )wQD + M K-w1w DR

where in the third step, we use the inverse matrix lemma:

(w Q KQ

+

M K-w1M)-1

=

1 w Q

K-Q1

-

1 w Q

K-Q1M

(w

Q

Kw

+ MK-Q1M )MK-Q1

Because (·, ·)  1, it's easy to prove that, for arbitrary vector x  Rd,

max{ Mx , M x }  (1 + ) x

Therefore,

 (1 - ) wQKw + MK-Q1M -1MK-Q1 · QD + wQKw + MK-Q1M -1 · Qw DR



w

1 Qvw

+

vM 2 ( 1

- 2 vQ

+

Q)

:=

R

(11)

 (1 - )w wQKQ + M K-w1M -1 · QD

+ wQKQ + M K-w1M -1M K-w1 w DR



w

1 QvQ

+

vM 2

((1

-

)w

+

1+ vw

)

:=

R

(12)

Given the special property of Z0 and 0, we force Z and  satisfying the following condition: Condition B.2. Z0  Z, 0  .
16

As a direct result, we have:



max
 Ru

min
Ru

LD (,

 ,

)

=



max
Z

min


LD (,

 ,

)

= 0,



max
 Ru

min
Ru

LD

(,

 ,

)

=



max
Z

min


LD (,

 ,

)

= 0.

Property B.3. [Variance of Estimated Gradient] Under Assumption A, B and D, given convex sets , Z, , where Z and  have finite diameter CW and CQ, then there exists constants ,  , , such that, for arbitrary , ,    × Z × , we have:
Es,a,r,s,a,s0,a0 [ L(s,a,r,s,a,s0,a0)(, , ) - LD(, , ) 2]  2; Es,a,r,s,a,s0,a0 [  L(s,a,r,s,a,s0,a0)(, , ) -  LD (, , ) 2]  2; Es,a,r,s,a,s0,a0 [ L(s,a,r,s,a,s0,a0)(, , ) -  LD(, , ) 2]  2.

Here we use Es,a,r,s,a,s0,a0 [·] as a shorthand of E(s,a,r,s)dD,a(·|s),s00D,a0(·|s0)[·], and use L(s,a,r,s,a,s0,a0)(, , ) to denote the stochastic gradient estimated using a single data point:
L(s,a,r,s,a,s0,a0)(, , ) = (1 - )Q(s0, a0) log (a0|s) + w (s, a)Q(s, a) log (a|s),  L(s,a,r,s,a,s0,a0)(, , ) = r + Q(s, a) - Q(s, a)  w (s, a) - ww (s, a) w (s, a), L(s,a,r,s,a,s0,a0)(, , ) = (1 - )Q(s0, a0) + w (s, a) Q(s, a) - Q(s, a) + QQ(s, a)Q(s, a).
(13)

Proof. Under Linear case and Assumption D, we should have
Es,a,r,s,a,s0,a0 [ L(s,a,r,s,a,s0,a0)(, , ) - LD (, , ) 2] 2(1 - )2E[ Q(s0, a0) log (a0|s0) - E[Q(s0, a0) log (a0|s0)] 2]
+ 22E[ w (s, a)Q(s, a) log (a|s) - E[w (s, a)Q(s, a) log (a|s)] 2] 2(1 - )2E[ Q(s0, a0) log (a0|s0) 2] + 22E[ w (s, a)Q(s, a) log (a|s)] 2] 2(1 - )2CQ2 G2 + 22CW 2 CQ2 G2
Es,a,r,s,a,s0,a0 [  L(s,a,r,s,a,s0,a0)(, , ) -  LD (, , ) 2] 3E[ w(s, a)(Q(s, a) - Q(s, a)) - E[w(s, a)(Q(s, a) - Q(s, a))] 2]
+ 3E[ w(s, a)r - E[w(s, a)r] 2] + 32wE[ w(s, a)w(s, a) - E[w(s, a)w(s, a)] 2] 3R2 + 3M 2 CQ2 + 32wK2 CW 2
Es,a,r,s,a,s0,a0 [ L(s,a,r,s,a,s0,a0)(, , ) -  LD(, , ) 2] 3(1 - )2E[ Q(s0, a0) - E[Q(s0, a0)] 2] + 32QE[ Q(s, a)Q(s, a) - E[Q(s, a)Q(s, a)] 2]
+ 3E[ w(s, a)(Q(s, a) - Q(s, a)) - E[w(s, a)(Q(s, a) - Q(s, a))] 2] 3(1 - )22 + 3M 2 CW 2 + 32QK2 CQ2
which finishes the proof.
In Linear setting, 2, 2, 2 can be chosen as:
2 =2(1 - )22G2CQ2 + 22M 2 G2CW 2 CQ2 , 2 =3R2 + 3M 2 CQ2 + 32wK2 CW 2 , 2 =3(1 - )22 + 3M 2 CW 2 + 32QK2 CQ2 .
In the following, we will use  to refer to the max{, , } value satisfying Property B.3.

17

Finally, we prove a condition which is useful in the analysis of our second strategy. We first introduce some new notations. Suppose we have a mini batch data N sampled according to dD whose batch size is constant |N |. Then, we denote the average batch gradients as

LN (, , )

=

1 |N

|

|N |

L(si,ai,ri,si,ai,si0,ai0)(, , )

i=1

where L(si,ai,ri,si,ai,si0,ai0)(, , ) is defined in Eq.(13).

Property B.4. Under Assumption A and B, there exists two constants two constants L¯ and L¯, such that:

ENdD [  LN (, 1, 1) -  LN (, 2, 2) 2 + LN (, 1, 1) - LN (, 2, 2) 2]

EN dD

L¯ 

 LN (, 1, 1) -  LN (, 2, 2)


(2

-

1)

+

L¯ 

LN (, 1, 1) - LN (, 2, 2)


(1 - 2)

,

ENdD [  LN (, 1, 1) -  LN (, 2, 2) 2 + LN (, 1, 1) - LN (, 2, 2) 2] L¯2 1 - 2 2 + L¯2 1 - 2 2.

Proof. For simplicity, we use KNw to denote matrix EN [w(s, a)w(s, a)] (KNQ is similar) and use MN to denote EN [(s, a)(s, a) - (s, a)(s, )]

 LN (, 1, 1) -  LN (, 2, 2) = -wKNw (1 - 2) - MN (1 - 2)

LN (, 1, 1) - LN (, 2, 2) = QKNQ (1 - 2) - MN (1 - 2)

Therefore,

ENdD [  LN (, 1, 1) -  LN (, 2, 2) 2 + LN (, 1, 2) - LN (, 2, 2) 2]

2ENdD [(1 - 2)(2w(KNw )KNw + MN MN )(1 - 2)]

+ 2ENdD [(1 - 2)(2Q(KNQ )KNQ + MN MN )(1 - 2)]

2ENdD [(1 - 2)(2w(KNw )2 + (1 + )2I)(1 - 2)] + 2ENdD [(1 - 2)(2Q(KNQ )2 + (1 + )2I)(1 - 2)]

2ENdD [(1 - 2)(2w(KNw )2 + (1 + )2I)(1 - 2)] + 2ENdD [(1 - 2)(2Q(KNQ )2 + (1 + )2I)(1 - 2)]

=(1 - 2)(22wKw + 2(1 + )2I)(1 - 2) + (1 - 2)(22QKQ + 2(1 + )2I)(1 - 2)

In the first inequality, we use Young's inequality; in the second one, we use the fact that the largest singular value of MN is less than (1 + ); the third one is because all eigenvalues of KNw and KNQ locate in [0, 1], and we should have I  KNw  (KNw )2 and I  KNQ  (KNQ )2. Notice that,

EN dD

-

 LN (, 1, 1) -  LN (, 2, 2)


(1 - 2)

+

LN (, 1, 1) - LN (, 2, 2)


(1 - 2)

=w(1 - 2)Kw(1 - 2) + Q(1 - 2)KQ(1 - 2)

Therefore,

(1 - 2)(22wKw + 2(1 + )2I)(1 - 2) + (1 - 2)(22QKQ + 2(1 + )2I)(1 - 2)



2

max{2w, 2Q} + 2(1 + min{wvw, QvQ}



)2

w(1 - 2)Kw(1 - 2) + Q(1 - 2)KQ(1 - 2)

Moreover,

(1 - 2)(22wKw + 2(1 + )2I)(1 - 2) + (1 - 2)(22QKQ + 2(1 + )2I)(1 - 2)

(2 max{2w, 2Q} + 2(1 + )2) (1 - 2)(1 - 2) + (1 - 2)(1 - 2)

As a result, Assumption B.4 holds with

L¯ 

=

L¯ 

=

max{

2

max{2w, 2Q} + 2(1 min{wvw, QvQ

+ }

)2 ,

2 max{2w, 2Q} + 2(1 + )2}

18

C The Analysis of Bias

We first prove two propositions, which are crucial for analyzing the biases due to the finite dataset and mis-specified function classes.

Proposition C.1. For abitrary   , we have:

 max min L(, w, Q) -  max min LD(, w, Q)

wW QQ

wW QQ

  (2 + 2 + 2 + 2/2) 2¯data
denoted as data

where ¯data is defined in Definition 3.1.

Proof. For the simplicity of notation, we give the proof for a fixed .

Denote (wµ , Qµ) parameterized by (µ, µ) as arg maxwW minQQ L(, w, Q) and denote (w, Q) parameterized by (, ) as arg maxwW minQQ LD(, w, Q). First, we try to
bound  - µ. We use Qw and QDw (parameterized by w and wD) as the short notes of arg minQQ L(, w, Q) and arg minQQ LD(, w, Q), respectively. Then,

|L(, w, Qw) - LD(, w, QDw )|  max{L(, w, QDw ) - LD(, w, QDw ), LD(, w, Qw) - L(, w, Qw)}  ¯data

As a result,

0

LD (,

w,

Q)

-

min
QQ

LD (,

wµ ,

Q)

LD

(,

w,

Q

)

-

min
QQ

L(,

w

,

Q)

+

L(,

wµ

,

Qµ

)

-

min
QQ

LD

(,

wµ

,

Q)

2¯data

According to Lemma A.1, minQQ LD(, w, Q) is µ-strongly concave. Therefore,

 - µ



2 µ

LD(, w, Q) -

min
QQ

LD

(,

wµ

,

Q)



2 µ

 2¯data

Next, we bound  - µ . For arbitrary    and w  W, we have:

0  LD(, w, Qw) - LD(, w, QDw )  LD(, w, Qw) - L(, w, Qw) + L(, w, QDw ) - LD(, w, QDw )  2¯data

Since LD is µ strongly-convex, as a result of Lemma A.3, for arbitrary w,

w - wD



2 µ

 2¯data

(14)

Then, we have

 - µ





-

arg

min


LD

(,

wµ ,

Q )

+

arg

min


LD

(,

wµ

,

Q

)

-

µ

=



-

arg

min


LD

(,

wµ ,

Q )

+

arg

min


LD

(,

wµ

,

Q

)

-

arg

min


L(,

wµ

,

Q

)



L µ

 - µ

+

2 µ

 2¯data

(

2L µ µ

+

2 µ

 ) 2¯data

where in the last but two step, we use Lemma A.1-(1).

As a directly application of Property B.1, we have:



max
wW

min
QQ

L(

,

w,

Q)

-



max
wW

min
QQ

LD

(

,

w,

Q)

=¯daLta(+ ,Lwµ,Q-µ)µ-

 LD ( , + L  - 

wµ , µ

Qµ)

+

LD(, wµ , Qµ) - LD(, w, Q)

(2  + 2 + 2 + 2/2) 2¯data

19

Proposition C.2. For arbitrary   , we have:

Edµ [|wµ (s, a)

-

wL (s, a)|2]



W

:=

4

2max Qw

1

+

2

max µ

2

Edµ [|Qµ(s,

a)

-

QL(s,

a)|2]



Q

:=

8

3max 2Qw

1

+

(2

+

4 2max Qµ

)2

where (wµ , Qµ) denotes the saddle point of L(, w, Q) constrained by w, Q  W × Q (i.e.   Z,   ), (wL , QL) denotes the saddle point of L(, w, Q) without any constraint on w and Q (i.e. w and Q can be arbitrary vectors in R|S||A|), max = max{Q, w}, µ is defined in Property 2.2.

Proof. In the following, we will frequently consider two loss functions. The first one is L(, w, Q) defined in Eq.(5), where w and Q are parameterized by  and , respectively, and we will write (w, Q)  W × Q. The second one is F (, x, y) defined by:

F (, x, y) =(1 - )(0)-1/2y + x 1/2R - (I - 1/2P -1/2)y

+

Q 2

yy

-

w 2

xx

where (x, y)  R|S||A| × R|S||A|. For simplification, in the following, we will use maxx miny as a short note of maxxR|S||A| minyR|S||A| .

As we can see, the difference between L(, w, Q) and F (, x, y) is not only that we don't have any constraint on x and y, but also that we absorb one 1/2 into vector x and y. In another word, for arbitrary , w, Q, we have
L(, w, Q) = F (, 1/2w, 1/2Q).

Obviously, F (, x, y) is w-strongly-concave-Q-strongly-convex and max-smooth w.r.t. x, y  R|S||A|.

In the following, we use wR parameterized by R to denote arg maxwW miny F (, 1/2w, y).

According

to

Lemma

A.1,

miny

F (, x, y)

is

a

2

2max Q

-smooth

and

w -strongly-concave

function

with gradient x miny F (, x, y). Since xF (, 1/2wL , 1/2QL) = 0, we have,

w 2

1/2wR - 1/2wL

2

F

(,

1/2wL

,

1/2QL)

-

min
y

F

(,

1/2

wR

,

y)

(Strong concavity of miny F (, x, y))

=F

(,

1/2wL

,

1/2QL)

-

max
wW

min
y

F

(,

1/2w,

y)

F

(,

1/2

wL

,

1/2

QL

)

-

min
y

F

(,

1/2

w



,

y)



2max Q

1/2w - 1/2wL

2

=

2max Q

w - wL

2 

=

2max Q

1

(w is defined in Def. 3.2) (Smoothness of miny F (, x, y))
(see definition of 1 in Def.3.2)

which implies

1/2wR - 1/2wL

2



2

2max Qw

1

(15)

Applying Lemma A.1 for (w, Q)  W × Q, we know min L(, w , Q) is µ-strongly-concave w.r.t. . Since  is the minimizer of min L(, w , Q) and Z is a convex set, we have

µ 2

 - R

2

L(,

wµ ,

Qµ)

-

min
QQ

L(, wR ,

Q)

(Strong concavity of minQQ L(, w, Q); Lemma A.3)

=F (,

1/2wµ ,

1/2Qµ)

-

min
QQ

F

(,

1/2wR ,

1/2Q)

20

F

(,

1/2wµ

,

1/2Qµ)

-

min
y

F

(,

1/2

wR ,

y)

F

(,

1/2wµ

,

1/2Qµ)

-

min
y

F

(,

1/2

wµ

,

y

)

(Because wR = arg maxwW miny F (, 1/2w, y))



max 2

1/2Qµ

-

arg

min
y

F

(,

1/2wµ ,

y)

2

(Smoothness of F (, x, y) for fixed x and y miny F = 0)



max 2

2

In the last but two inequality, we use the fact that F (, 1/2wµ , ·) is max-smooth and y miny F (, 1/2wµ , Q) = 0; in the last equality, we use the definition of 2 in Def. 3.2. Combing (b) in Property B.1 with Lw = 1, for arbitrary s, a  S × A, we have:

|wµ (s, a) - wR (s, a)|2 

 - R

2



max µ

2

(16)

Therefore, as a result of Eq.(15) and Eq.(16):

Edµ [|wµ - wL |2] 2Edµ [|wR - wL |2] + 2Edµ [|wR - wµ |2]

=2 1/2wR - 1/2wL 2 + 2Edµ [|wR - wµ |2]

4

2max Qw

1

+

2

max µ

2

According

to

Lemma

A.1

again,

arg miny

F (, x, y)

is

max Q

-Lipschitz

w.r.t.

x,

we

have

Edµ [|Qµ - QL|2] = 1/2Qµ - 1/2QL 2

2

1/2Qµ

-

arg

min
y

F

(,

1/2wµ ,

Q)

2 +2

arg

min
y

F

(,

1/2wµ ,

y)

-

1/2

QL

2

bounded by 2

22

+

2

max Q

1/2wµ - 1/2wL

2



8

3max 2Qw

1

+

(2

+

4

2max Qµ

)2

As a result,

W

=

4

2max Qw

1

+

2

max µ

2;

Q

=

8

3max 2Qw

1

+

(2

+

4

2max Qµ

)2

Theorem C.3 (Bias resulting from regularization). Let's rewrite Eq.(5) in a vector-matrix form:

max
wW

min
QQ

L(,

w,

Q)

:=

(1

-

 )(0 ) Q

+

w

R - (I - P)Q

+

Q 2

QQ

-

w 2

ww

where 0 and P denotes the initial state-action distribution and the transition matrix w.r.t. policy , respectively;   R|S||A|×|S||A| denotes the diagonal matrix whose diagonal elements are dµ(·, ·). Denote (wL , QL) as the saddle point of L(, w, Q) without any constraint on w and Q (i.e. W = Q = R|S||A|), then we have:

wL =w + wQI + (I - P)-1(I - P ) -1 QR - Qww QL =Q - wQI + -1(I - P )(I - P) -1 wQQ + w(1 - )-10)

where

w

=

d dµ

is

the

density ratio

and Q

is

the Q

function of .

we

use

P

=

(P )

to

denote

the transpose of the transition matrix.

21

Proof. Recall the loss function

L(,

w, Q)

=

(1

-

 )(0 ) Q

+

wR

-

w(I

-

P)Q

+

Q 2

QQ

-

w 2

ww

By taking the derivatives w.r.t. Q, since  is invertible, the optimal choice of Q should be:

Q

=

1 Q

-1((I

-

P )w

-

(1

-

 )0 )

Plug this result in, and we have

L(,

w,

Q)

=

-

1 2Q

(1 - )0 - (I - P )w -1 (1 - )(0) - (I - P )w

+

wR

-

w 2

ww

Taking the derivative w.r.t. w, and set it to 0:

0

=

1 Q

(I

- P)-1

(1 - )(0) - (I - P )w

+ R - ww

As a result,

wL =

w I

+

1 Q

(I

-

 P )-1 (I

-

P )

-1

1 Q

(I

-

P)-1(1

-

)0

+

R

= wQI + (I - P)-1(I - P ) -1 (I - P)-1(I - P )-1(I - P )-1(1 - )0 + QR

=w + wQI + (I - P)-1(I - P ) -1 QR - Qww

and

QL

=

1 Q

-1

(I - P )wL - (1 - )0

=

1 Q

-1

(I - P )wL - (I - P )w

=

1 Q

-1(I

-

P )

Qw + (I - P)-1(I - P )

-1

QR - Qww

= wQ(I - P )-1 + (I - P) -1 R - ww

= wQ(I - P )-1 + (I - P) -1 (I - P)Q - ww

=Q - wQ(I - P )-1 + (I - P) -1 wQ(I - P )-1Q + ww

=Q - wQI + -1(I - P )(I - P) -1 wQQ + w(1 - )-10)

Lemma C.4. Under Assumption C:

w - wL

2 



C2(Q + QwC)2 (1 - )4

,

Q - QL

2 



(1

C2 - )2

(

w Q 1-

+ w)2

where (w, Q) and (wL , QL) are defined in Theorem C.3. x  = xx denotes the norm of column vector x weighted by .

Proof. From Theorem C.3, we have wL =w + wQI + (I - P)-1(I - P ) -1 QR - Qww QL =Q - wQI + -1(I - P )(I - P) -1 wQQ + w(1 - )-10)

22

We use 1  R|S||A|×1 to denote a vector whose all elements are 1. Then, we have

w - wL

2 

=

wQI + (I - P)-1(I - P ) -1 QR - Qww

2 

= wQI + 1/2(I - P)-1(I - P )1/2 -11/2 QR - Qww 2

 -1/2(I - P )-1(I - P)-1 QR - Qww 2

= -1/2(I - P )-1Q 2



(Q

+ (1

Qw - )2

C

)2

-1(I - P )-11

2 

=

(Q

+ (1

Qw - )2

C

)2

-1(I - P )-1dµ

2 

=

(Q

+ (1

Qw - )4

C

)2

wdµ

2 



C2(Q + QwC)2 (1 - )4

where in the first inequality, we use Lemma A.4; in the third equality, we use Q to denote the Q function after replacing true rewards with QR - Qww; in the second inequality, we use Lemma A.5 and the result that |QR - Qww|  Q + QwC given Assumption C; in the last inequality, we use Assumption C again. Similarly,

Q - QL

2 



wQI + -1(I - P )(I - P) -1 wQQ + w(1 - )-10)

2 

-1
= QwI + -1/2(I - P )(I - P)-1/2 1/2 QwQ + w(1 - )-10) 2

 1/2(I - P)-1-1(I - P )-1 wQQ + w(1 - )0) 2

= wQ1/2(I - P)-1-1(I - P )-1Q + w1/2(I - P)-1w) 2



w Q 1-

1/2(I

-

 P )-1 -1 (I

-

P )-11

+

w 1/2 (I

-

 P )-1 w )

2



(I - P)-1

w Q 1-

wdµ

+

w w

2 

 (1

C2 - )2

(

w Q 1-

+

w )2

where in the last but third inequality, we use Lemma A.5 and the fact that w is also non-negative.

Lemma C.5. Under Assumption C, for arbitrary function f (s, a),

(1 - )Es00,a0[f (s0, a0)] + Es,a,sdµ,a[w (s, a)f (s, a)] = Edµ [w (s, a)f (s, a)]

(17)

Es,a,sdµ,a[f 2(s, a)]



1

1 -



Es,addµ

[f

2(s,

a)]



1

C -



Es,adµ

[f

2

(s,

a)]

(18)

where ddµ := (1 - )E ,s0,a0dµ(·,·)[

 t=0



t

p(st

=

s, at

=

a)]

is

the

normalized

discounted

state-action occupancy by treating dµ(·, ·) as initial distribution; s, a, s  dµ, a   is a short

note of s, a  dµ, s  P (s|s, a), a  (·|s).

Proof. Eq.(17) can be proved by the equation:

d(s, a) = (1 - )0(s)(a|s) +  p(s|s, a)d(s, a)(a|s)
s ,a

For Eq.(18), the first step is because 

s,a dµ(s, a)p(s|s, a)(a|s)



1 1-

ddµ

(s,

a),

and the

second step is the result of Assumption C.

23

Theorem 3.3. [Bias] Based Assumptions in Section 2.2 and Definitions above, for arbitrary   , we have:



max
wW

min
QQ

LD

(

,

w,

Q)

-



J

(

)

 reg + func + data

The bias terms reg, func and data are defined by

f unc

:=

G 1-

CQ + CW

QC 1-

+

QW C 1-

 + CQ W

reg

:=

1

G -



C2 (1 - 

)

(

w Q 1-

+ w)

+

C(Q + QwC) (1 - )3

+

C

2

(Q (1

+ -

Qw )3

C

)

(

w Q 1-

+ w)



data := (2  + 2 + 2 + 2/2) 2¯data

where  and  is the condition number, and

W

:=

4

2max Qw

1

+

2

max µ

2

,

Q

:=

8

3max 2Qw

1

+

(2

+

4

2max Qµ

)2

with max := max{Q, w}.

C 1-

Proof. Firstly, by applying the triangle inequality:



max
wW

min
QQ

LD

(

,

w,

Q)

-



J

(

)





max
wW

min
QQ

LD

(

,

w,

Q)

-



max
wW

min
QQ

L(

,

w,

Q)

Bounded in P roposition C.1

+



max
w

min
Q

L(

,

w,

Q)

-



max
wW

min
QQ

L(

,

w,

Q)

t1

+



J

(

)

-



max
w

min
Q

L(

,

w,

Q)

t2

where we use maxw minQ as a short note of maxwR|S||A| minQR|S||A| .

In the following, we constraint on w and

Qag,aianndusues(ew(Lwµ, ,QQLµ ))

to to

denote the saddle point of L(, w, Q) without any denote the saddle point of L(, w, Q). Next, we

upper bound t1 and t2 one by one. For simplicity, we use s, a, s  dµ, a   as a short note of

s, a  dµ, s  P (s|s, a), a  (·|s).

Upper bound t1 With misspecification Definition 3.2, we can easily bound t1:

t1 = L( , wµ , Qµ) - L(, wL , QL )



1

1 -



(1 - )E0 [

Qµ(s0, a0) - QL (s0, a0)

 log (a0|s0)]

+

1

 -



Es,a,sdµ,a[wµ (s, a)

Qµ(s, a) - QL (s, a)

 log (a|s)]

+

1

 -



Es,a,sdµ,a[(wµ (s, a) - wL (s, a))

Qµ(s, a) - QL (s, a)

 log (a|s)]

+

1

 -



Es,a,sdµ,a [(wµ (s, a) - wL (s, a))Qµ(s, a) log (a|s)]

1

G -

 E0 [|Qµ(s, a)

-

QL (s, a)|]

+

CW G 1-

Es,a,s

dµ,a



[|Qµ

(s

,

a

)

-

QL (s, a)|]

((1 - )0(s, a)  d(s, a)  Cdµ(s, a))

+

G 1-

Es,a,sdµ,a [|(wµ (s, a)

-

wL (s,

a))

Qµ(s, a) - QL (s, a)

|]

24

+

CQG 1-

Es,a,s

dµ

,a



[|wµ

(s,

a)

-

wL (s, a)|]



1

G -



E0 [|Qµ(s,

a)

-

QL (s, a)|2]

+

CW G 1-

Es,a,sdµ,a [|Qµ(s, a) - QL (s, a)|2]

+

G 1-

Edµ [|wL (s, a) - wµ (s, a)|2]Es,a,sdµ,a [|Q (s, a) - QL (s, a)|2|]

+

CQG 1-

Edµ [|wµ (s, a) - wL (s, a))|2]



1

G -



CEdµ [|Qµ(s,

a)

-

QL (s,

a)|2]

+

CW G 1-

C 1-

Edµ

[|Qµ

(s,

a)

-

QL

(s,

a)|2

]

+

1

G -



C 1-

Edµ

[|wL

(s,

a)

-

wµ

(s,

a)|2

]Edµ

[|Q

(s,

a)

-

QL

(s,

a)|2|]

+

CQG 1-

Edµ [|wµ (s, a) - wL (s, a))|2]



1

G -



CQ + CW

QC 1-

+

QW C 1-

+ CQW

In the last equation, we first use Eq.(18) in Lemma C.5, and then apply Proposition C.2.

Upper bound t2 Similarly, we can give a bound for t2:

t2 = J () - L(, wL , QL ))



1

1 -



(1 - )E0 [

Q (s0, a0) - QL (s0, a0)

 log (a0|s0)]

+ Edµ [w (s, a) Q (s, a) - QL (s, a)  log (a|s)]

+

1

 -



Edµ [(w (s, a) - wL (s, a))

Q (s, a) - QL (s, a)

 log (a|s)]

+

1

 -



Edµ [(w (s, a) - wL (s, a))Q (s, a) log (a|s)]

=

1

1 -



Edµ [w (s, a)

Q (s, a) - QL (s, a)

 log (a|s)]

(Eq.(17) in Lemma C.5)

+

1

 -



Es,a,sdµ,a [(w (s, a) - wL (s, a))

Q (s, a) - QL (s, a)

 log (a|s)]

+

1

 -



Es,a,sdµ,a [(w (s, a) - wL (s, a))Q (s, a) log (a|s)]



CG 1-

Edµ

[|Q

(s,

a)

-

QL

(s,

a)|]

+

G 1-

Es,a,sdµ,a [|(w (s,

a)

-

wL (s,

a))

Q (s, a) - QL (s, a)

|]

+

(1

G -

)2

Es,a,s

dµ

,a



[|w

(s,

a)

-

wL (s,

a)|]



CG 1-

Edµ [|Q

-

QL |2]

+

(1

G - )2

Edµ [|(w (s, a) - wL (s, a)|2]

+

G 1-

Edµ [|wL (s, a) - w (s, a)|2]Es,a,sdµ,a [|Q (s, a) - QL (s, a)|2|]



CG 1-

Edµ [|Q

-

QL |2]

+

(1

G - )2

Edµ [|(w (s, a) - wL (s, a)|2]

25

+

1

G -



C 1-

Edµ

[|wL

(s,

a)

-

w

(s,

a)|2

]Edµ

[|Q

(s,

a)

-

QL

(s,

a)|2

|]

(Eq.18 in Lemma C.5)



1

G -



C2 (1 - 

)

(

w Q 1-

+

w )

+

C(Q + QwC) (1 - )3

+

C

2(Q (1

+ -

Qw )3

C

)

(

w Q 1-

+

w )

C 1-

D Missing Examples and Proofs for Strategy 1

D.1 Equivalence between Stationary Points

Theorem D.1. [Equivalence Between Stationary Points] Under Assumptions in Section 2.2, given Z and  with finite CQ and CW , suppose there is an Algorithm provides us with one stationary point (T , T , T ) of the non-concave-strongly-convex objective max, min LD(, , ) after running T iterations, statisfying the following conditions in expectation over the randomness of algorithm.

E[ ,LD(T , T , T (T )) ] :=E[ LD(T , T , T (T )) +  LD(T , T , T (T )) ]



(

+

 1)(

+

1)

(19)

where () = arg min LD(, , ) and  and  are the condition numbers, then in expectation T is a biased stationary point satisfying Eq.(2).

Proof. Eq.(19) implies that

max{E[

LD(T , T , T (T ))

], E[

 LD(T , T , T (T ))

]} 

 ( + 1)( + 1)

(20)

We can upper bounded E[ J(T ) ] with the triangle inequality: E[ J (T ) ]  E[ LD(T , T , T (T )) ] +E[ LD(T , , ) - LD(T , T , T (T ))) ]

Bounded in Eq.(20)
+ E[ LD(T , , ) - J (T ) ]

Bounded in T heorem3.3

 (

+

 1)(

+

1)

+

f unc

+

reg

+

data

+

E[

LD(T , , ) - LD(T , T , T (T )))

]

where we use ,  to denote the saddle-point of maxZ min LD(T , , ); in the last inequality we use Eq.(20) and Theorem 3.3.

Next, we try to bound the last term. According to the definition,  is also the maximum of function T (·) = min LD(T , ·, ) defined in Lemma A.1. Applying Property (2) in Lemma A.1, Lemma A.3, and inequality (20), we obtain that

T - 



1 µ

T (T )

=

1 µ

 LD(T , T , T (T ))



µ (

+

 1)(

+

1)

Then we can bound:

LD(T , , ) - LD(T , T , T (T ))

L T -  + L  - T (T )) = L T - 

(L + L) T - 



 1 + 

+ L T () - T (T ))

where in the first inequality we use the smoothness Assumption A, and in the second inequality we

use (1) in Lemma A.1. As a result,

E[

J (T )

]

 (

+

 1)(

+

1)

+

 1 + 

+ func + reg + data

26

 + func + reg + data

In the following subsections, we will introduce the Projected-SREDA Algorithm revised from [Luo et al., 2020] and prove that it provide us a stationary points required by Theorem D.1.
We choose  = Rd ,  = 0, and Z = {|   R}, where 0 is defined in Property 4.1 and R will be determined later. For simplicity, we use LD- = -LD to denote the minus of original loss function, which should be a non-convex-strongly-concave problem and aligns with the setting of [Luo et al., 2020].

D.2 Verification of the Assumptions in [Luo et al., 2020]
In this section, we verify that Assumptions 1-5 in [Luo et al., 2020] are satisfied under our Assumption A, B and D.

Assumption 1

inf
Rd ,Z

max


LD-

(,



,

)

 - max (1 - )
Rd ,Z,

(D )Q

+



wDR + 

M



+

Q 2

 2 KQ

+

w 2



Kw

-

max (1 - ) +  + (1 + )   + + Q  2 + w 

Rd ,Z,

2

2

Because  and  are bounded for arbitrary ,   Z × , Assumption 1 holds.

Assumption 2 The proof is almost identical to the proof of Property B.1-(b) and we omit here. Assumption 2 holds by choosing L according to (10).

Assumption 3 Under our linear function classes setting, it holds obviously.
Assumption 4 Hold directly by choosing µ = QvQ.
Assumption 5 Identicial to the Condition B.3. We prove Condition B.3 holds in Appendix B under our Assumptions.

D.3 Useful Lemma
In this subsection, we first prove several useful lemma. Lemma D.2. Under Assumption B, for arbitrary    and   , the solution for maxRdZ LD(, , ) has bounded 2 norm.

Proof. Recall the loss function LD:

LD(, ,

)

=

(1

-

)(D )Q

+

w DR

-

  M 

+

Q 2

KQ

-

w 2

  Kw 

Taking the derivative w.r.t.  and set it to 0, we have:



=

1 w

K-w1(w

D

R

-

M)

Given that   R for   , we have:





1 w

K-w1 ( DR

+

M



)

1 w vw

(1

+

(1

+



)R

)

27

In the following, we will

use R0

:=

1 w vw

(1

+

(1

+

 )R )

as

a

shortnote.

Next,

we

are ready to

prove the following lemma which is crucial for the analysis of the effect of projection step.

Lemma 4.2. According to the updated rule of Alg 1, at iteration k, we have k  Z, k++1 =

k - kvk and k+1 = PZ (k++1). Denote R0 := Property 2.2. If we choose  = 0 and Z = {| 

1
wRvw

(1 + (1 +  } with R 

)R R0,

) where R we have:

is

defined

in

 LD- (k, k, k), k+1 - k++1



k

R0 R

+ +

k k

vk vk

 LD- (k, k, k) vk .

Proof. First of all, if k++1  Z, then k+1 - k++1 = 0, and the Lemma holds. Therefore, in the following, we only consider the case when k++1 / Z. Because k  Z, in the case, we must have
vk > 0.

Because we are considering Z is a high dimensional ball. For k++1 / Z, we have

k+1 = PZ (k++1) = k++1

R k++1

which means,

k+1 - k++1 = (

R k++1

- 1)k++1

Denote k = minZ LD- (k, , k). Then we have:  LD- (k, k, k), k - k  0,

k  R0

Then we have:

 LD- (k, k, k), k+1 - k++1

 LD- (k, k, k) vk

=(

R k++1

- 1)

 LD- (k, k, k), k++1  LD- (k, k, k) vk

=(

R k++1

- 1)

 LD- (k, k, k), k - kvk ± k  LD- (k, k, k) vk

=(

R k++1

- 1)

 LD- (k, k, k), k - k LD- (k, k, k) vk

+(

R k++1

- 1)

 LD- (k, k, k), k - kvk  LD- (k, k, k) vk

smaller than 0

larger than 0

(

R k++1

- 1)

 LD- (k, k, k), k - kvk  LD- (k, k, k) vk

(1 -

R k++1

) k

+ k vk vk

(1 -

R

R + k

vk

) R0

+ k vk

vk

=k

R0 R

+ +

k k

vk vk

( k++1  R and a, b  a b ) ( k++1  R + k vk )

Because R = 8 max{R0, 1}, and vk  vk , we have

R0 + k vk R + k vk



R0 R

+ +

k k

vk vk



R0

+

 5 L

R

+

 5 L



R0 + 1 R + 1



1 4

where in the third inequality we use the constraint that  < 1 and the fact that  > 1, L > 1 in our

setting.

28

D.4 Main Proofs for Theorem 3.1

The proofs for Theorem D.5 is almost the same as those for the original SREDA algorithm. We will only show those key Lemmas or Theorems in [Luo et al., 2020] which need to be modified as a result of the additional projection step, and omit those untouched. In the following, we will frequently use x+k+1 to denote (k+1, k++1) before the projection and use xk+1 to denote (k+1, k+1) after the projection.

First of all, the following condition still holds

xk+1 - xk 2 

x+k+1 - xk

2



2 252

where the first inequality results from the property of projection and the second one holds because of the choice of learning rate k. The condition above corresponds to the xk+1 - xk 2  2x in [Luo et al., 2020]. As a result, all the Lemmas and Theorems in the Appendix B of [Luo et al.,
2020] still hold for our Projected-SREDA. Besides, because the PiSARAH will not be effected by
our projection step, the results in Appendix C of [Luo et al., 2020] also holds.

Similarly, we consider the following decomposition:

LD- (xk+1, k+1) - LD- (xk, k) = LD- (xk+1, k) - LD- (xk, k) + LD- (xk+1, k+1) - LD- (xk+1, k)

Ak

Bk

Because the proof of Lemma 14 and Lemma 15 in [Luo et al., 2020] only depends on the previous lemmas, they still hold and we list them here.

Lemma D.3.

Under Assumptions of Theorem D.5, we have E[Bk]



1342  L

for any k



1

Lemma D.4. Under Assumptions of Theorem D.5, we have

E

,

max


LD-

(,



,

)

 E vk

+

15 7



However, the final proof for Theorem D.5 can not be adapted from [Luo et al., 2020] directly because of the projection. In the following, we show our proof targeted at our Projected-SREDA:

Theorem D.5. For  < 1, under Assumption A, B, D, with the following parameter choices:

R

=

8 max{ 1 (1 w vw

+

(1

+

)R), 1}, R

=

1 w Q vQ

+

vM 2 ((1

-

)w

+

1 +  ), vw

k = min

 5L vk

,

1 10L

,



=

1 8L

,

S1

=



2250 19

2

2 2

,

S2

=



3687 76



q,

q = -1,

LD-

=

max
,Z,

LD-

(,



,

)

-

LD- (0, 0, 0),

K

=



50

LLD- 2



and m = 1024

and the same parameter choices for PiSADAH as in [Luo et al., 2020], Algorithm 3 outputs ,  such that

E[

,

max


LD-

(,



,



)

]  O()

with O(3-3) stochastic gradient evaluations.

Proof of Theorem D.5. Based on the update rule of  and  in Algorithm 1, we have:

Ak  , LD- (xk, k), xk+1 - xk

+

L 2

xk+1 - xk

2

(Smoothness of LD- )

 , LD- (xk, k), x+k+1 - xk

+

, LD- (xk, k), xk+1 - x+k+1

+

L 2

x+k+1 - xk

2

(the property of projection)

= - k , LD- (xk, k), vk

+

 LD- (xk, k), k+1 - k++1

+

Lk2 2

vk

2

29

 - k , LD- (xk, k), vk

+

k 4

 LD- (xk, k)

vk

+

Lk2 2

vk 2

(Lemma 4.2)

 - k , LD- (xk, k), vk

+

k 4

vk

2

+

k 4

,LD- (xk, k) - vk

(Triangle Ineq.; vk  vk ; LD-  ,LD- ; LD- - vk

vk

+

Lk2 2

vk 2

 ,LD- - vk )



k 2

,LD- (xk, k) - vk

2

-

(

k 4

-

Lk2 2

)

vk

2

+

k 4

,LD- (xk, k) - vk

vk

( ,LD-  0)

The choice of the step size implies that

(

k 4

-

Lk2 2

)

vk

2



min{

1 40

L

-

1 2002L

,

 20 L

vk



min{

1 50L

,

3 100L

vk

}

vk

2



2 50L

min

vk 

,

vk 2 22



2 50L

(

vk 

- 2))

=

1 50L

(

vk

- 22)

-

2 502 L

vk

2}

vk

2

(min(|x|, x2/2)  |x| - 2)

Therefore,

E[Ak ]



1 20L

E[

,LD- (xk, k) - vk

2]

-

1 50

L

(

vk



1 20

L

E[

,LD- (xk, k) - vk

2]

-

1 50

L

(

vk

-

22)

+

 20

L

E[

,LD- (xk, k) - vk

]

-

22)

+

 20L

E[ ,LD- (xk, k) - vk 2]



1 20

L

·

19 1125

-22

-

1 50L

(

vk

-

22)

+

 20L

·

19 1125

- 1

(Corollary 2 in [Luo et al., 2020])



2 20

L

-

1 50L



vk

Therefore, combining with Lemma D.3 and taking average over K, we have

1 K

K -1
E[LD- (xk+1, k+1)
k=0

-

LD- (xk, k)]



1 K

K -1
(
k=0

2 20

L

-

1 50

L



vk

+

1342  L

)

Consequently, we have: which means

 1 K-1 50L K k=0

vk



1352  L

+

LD- K

1 K-1 K

vk



6750

+

50LLD- K

k=0

Under Assumptions A and B and Condition that that both Z and  have finite diameter, LD- is a

finite

constant.

By

choosing

K

=

 50LLD -
2

,

we

have:

E

,

max


LD-

(,



,

)

=

1 K

K -1
E

,

max


LD- (k,

k ,

)



1 K

K -1
(E

vk

+

15 7

)



6754

k=0

k=0

which finishes the proof.

30

E Concrete Examples for Saddle-Point Solver Oracle

E.1 Connection with Previous Methods
The inner optimization oracle in our second strategy essentially solves the off-policy policy evaluation problem, i.e., given a policy, compute its value function and marginalized importance weighting function. Among the plethora of works studying off-policy policy evaluation with linear function approximation, [Liu et al., 2020] connected the GTD family and stochastic gradient optimization, and established finite-sample analysis for their off-policy algorithms. Their convergence rate is worse than ours, because their objective is only convex-strongly-concave, whereas our objective is strongly-convex-strongly-concave thanks to the regularization on the parameters of the Q function.
Besides, [Du et al., 2017] adapted variance-reduced stochastic gradient optimization algorithms for policy evaluation, which can be extended to off-policy setting. However, they focused on the finitesum case and their algorithms will be inefficient for our purpose when the dataset is prohibitively large. Besides, they did not analyze the bias resulting from regularization and approximation error, which we do in this paper.

E.2 The First Example for Saddle-Point Solver Oracle

In the proof of Property 4.1, we show that in our linear setting, it's possible to derive a close form solution for ,  = arg maxZ min LD(, , ) for arbitrary   :
 = wQKw + MK-Q1M -1 - (1 - )MK-Q1QD + Qw DR ,  = wQKQ + M K-w1M -1 (1 - )wQD + M K-w1w DR .

Therefore, estimate (

we can , ).

directly use our sample data to estimate K, M, DR and D In this section, we provide an algorithm based on this idea, and prove

and that

then such

algorithm satisfies the requirement of the ORACLE in Definition 5.1.

E.2.1 The Least-Square Oracle
In the following, we will use uR to denote w DR and use u to denote QD . Besides, we use (·) to denote the empirical version of the matrices/vectors estimated via samples.

Algorithm 3: Projected Least-Square Oracle

1 Input: Distribution dD; Batch Size Nall;

2   wQKw + MK-Q1M -1 - (1 - )MKQ-1u + QuR

-1

3   wQKQ + M K-w1M

(1 - )wu + M K-w1uR

4 Output: PZ (), P()

where P(x) means projected x into set 

E.2.2 Estimation Error of Least-Square Oracle

We first show a useful Lemma:

Lemma E.1 (Matrix Bernstein Theorem (Theorem 6.1.1 in [Tropp, 2015])). Consider a finite sequence {Sk} of independent, random matrices with common dimension d1 × d2. Assume that

ESk = 0 and Sk  L f or each index k

Introduce the random matrix

Z = Sk
k

31

Let v(Z) be the matrix variance statistic of the sum:

v(Z) = max{ E(ZZ) , E(ZZ) }

= max{

E(SkSk) ,

E(SkSk) }

k

k

Then,

EZ  Furthermore, for all t  0

2v(Z)

log(d1

+

d2)

+

1 3

L

log(d1

+

d2)

P{ Z

 t}  (d1 + d2) exp

-t2/2 v(Z) + Lt/3

For Algorithm 3, we have the following guarantee:

Proposition E.2. Suppose we have N - (s, a, r, s, a, a0) tuples independently sampling according

to

dD ×,

and N



max{2K2

+

4min 3

,

2M 2

+

8(1+ )min 3

}

4 m 2 in

log

2d 

.

For arbitrary 1/5

>



>

0,

with probability at least 1 - 5, Kw, KQ, M are invertible and their smallest eigenvalues (for M

we consider the smallest sigular values) are larger than vw/2, vQ/2 and vM /2, respectively, while

the following conditions hold at the same time:

Kw - Kw



2 3Nall

log

2d 

+

2K2 Nall

log

2d 

=

O( K ) Nall

KQ - KQ



2 3Nall

log

2d 

+

2K2 Nall

log

2d 

=

O( K ) Nall

M - M

=

M - M



4(1 +  3Nall

)

log

2d 

+

2M 2 Nall

log

2d 

=

O( M ) Nall

u - u



4 3Nall

log

2d 

+

22 Nall

log

2d 

=

O(  ) Nall

uR - uR



4 3Nall

log

2d 

+

2R2 Nall

log

2d 

=

O( R ) Nall

Proof. First, we try to bound Kw - Kw . We take a look at a series of random matrices
Ai = Kw - w(si, ai)w(si, ai), i = 1, 2, ...
where (si, ai) are sampled from dD independently. Easy to verify that Ai has the following properties as a result of w(s, a)  1:
E[Ai] = 0, Ai = Kw - w(s, a)w(si, ai)  1

Under Assumption D, according to the Matrix Bernstein Theorem and the union bound, we have:

P(

N i=1

Ai

Nall

N

N

 ) P (max( Ai)  Nall) + P (max( -Ai)  Nall)

i=1

i=1

2d

exp(-

Nall2/2 K2 + /3

)

which implies that, with probability 1 - :

Kw - Kw



2 3Nall

log

2d 

+

2K2 Nall

log

2d 

=

O( K Nall

)

32

The discussion for KQ is similar and we omit here. As for M - M , notice that,
M - (w(s, a)Q(s, a) - w(s, a)Q(s, a))  2(1 + ) M - (Q(s, a)w(s, a) - Q(s, a)w(s, a))  2(1 + ) Therefore, w.p. 1 - ,

M - M

=

M - M



4(1 + ) 3Nall

log

2d 

+

2M 2 Nall

log

2d 

=

O( M ) Nall

As for the bounds for the difference of vectors u - u and uR - uR , since vectors are special cases of matrices (or we can concatenate the vector with a zero-vector to make a n × 2 matrix and make the proof more rigorous), again, we can apply the same technique again. Notice that,
uR - w(s, a)r(s, a)  2 u - Q(s0, a0)  2
Besides, for random column vectors x with bounded norm, we should have Ex[xx]  tr(Ex[xx]) = Ex[tr(xx)] = Ex[xx] = Ex[xx]
As a result, combining Assumption D, we have

u - u



4 3Nall

log

2d 

+

22 Nall

log

2d 

=

O(  ), Nall

w.p. 1 - 

uR - uR



4 3Nall

log

2d 

+

2R2 Nall

log

2d 

=

O( R ), Nall

w.p. 1 - 

Therefore, w.p. 1-2 the concentration results in this Proposition hold. Next, we derive the smallest
eigenvalues of Kw, KQ and M when n is large enough. For arbitrary x  Rd×1 with x = 1, we have:

xKwx =xKwx + x(Kw - Kw)x  vw - Kw - Kw xKQx =xKQx + x(KQ - KQ)x  vQ - KQ - KQ |xMx| |xMx| - |x(M - M)x|  vM - M - M

Therefore, easy

to

verify that,

when

the

concentration results

hold,and N



max{8

K 2 vw2

+

16 3vw

,

8

K 2 vQ 2

+

16 3vQ

,

8

K 2 vM 2

+

32(1+ 3vM

)

}

log

2d 

,

Kw ,

KQ,

M

are invertible and

their

smallest

eigenvalues (for M we

consider the smallest sigular values) are larger than vw/2, vQ/2 and vM /2

Next, we are ready to show that Algorithm 3 is also an example of the Oracle Algorithm. Theorem E.3. Algorithm 3 satisfies the Oracle Condition 5.1 with  = 0 and arbitrary c > 0.

Proof. Denote ,  as the saddle-point of LD given , and use ,  to denote the  and  in Algorithm 3 before the projection. Given the proposition above, we are ready to bound  -  and  -  . We list two properties below which we will use frequently later. Firstly, for arbitrary invertible d × d matrices A and B, we have:
A-1 - B-1 = A-1(B - A)B-1  A-1 B - A B-1
Secondly, for arbitrary d × d matrices X1, Y1, X2, Y2, we have:
X1Y1 - X2Y2 = X1(Y1 - Y2) + Y2(X1 - X2)  X1 Y1 - Y2 + Y2 X1 - X2

Then, we have

33

 - 

(1 - )

w QKw + MK-Q1M

-1MK-Q1 -

wQKw + MK-Q1M

-1
M

K-Q1

u

+ (1 - )

wQKw + MK-Q1M

-1
M

K-Q1(u

-

u )

+

wQKw + MK-Q1M

-1
Q(uR - uR)

+

wQKw + MK-Q1M

-1
-

(1 - ) wQKw + MK-Q1M -1

wQKw + MK-Q1M -1 QuR wQKw + MK-Q1M - wQKw - MK-Q1M

· wQKw + MK-Q1M -1 MK-Q1

+ (1 - )

wQKw + MK-Q1M

-1
M

K-Q1

MK-Q1 - MK-Q1

+ (1 - )  N

wQKw + MK-Q1M

-1
M

KQ-1

+

Q

R Nall

wQKw + MK-Q1M -1

+ Q wQKw + MKQ-1M -1 wQKw + MK-Q1M - wQKw - MK-Q1M

· wQKw + MKQ-1M -1

=O

vQ (w Q vw

1 +

vM 2

)2

 Nall

(w

Q

K

+

M vQ

+

K vQ2

)

+

 vQ(wQvw +

vM 2 )Nall

+

(w

Q

QR vw + vM 2

 ) Nall

+

(w Q vw

Q + vM 2 )2

 Nall

(w

Q

K

+

M vQ

+

K vQ2

)

log

2d 

=O

vQ

1 + Q (w Q vw

vQ +

vM 2

)2

(w

Q

K

+

M vQ

+

K vQ2

)

+

 vQ (w Q vw

+ vM 2 )

+

QR (wQvw + vM 2 )

Nall

log

2d 

where we omit constant number in O(·).

Similarly, we have

 - 

 (1 - )w

wQKQ + M K-w1M

-1
-

w QKQ + M K-w1M

-1 u

+ (1 - )w wQKQ + M K-w1M -1(u - u )

-1

-1

+

wQKQ + M K-w1M M K-w1 - wQKQ + M Kw-1M M K-w1 uR

+ wQKQ + M K-w1M -1M K-w1(uR - uR)

=O

vw

wvw + 1 (wQvQ + vM 2

)2

(w

Q

K

+

M vw

+

K vw2

)

+

R vw (w Q vQ

+ vM 2 )

+

w  (wQvQ + vM 2 )

Nall

log

2d 

As a result, for arbitrary 1/5 >  > 0, w.p. at least 1 - 5,

 -  2 +  -  2   -  +  - 

=O

vQ

(w

1+ Qvw

QvQ + vM 2

)2

 Nall

(w

Q

K

+

M vQ

+

K vQ2

)

+

 vQ(wQvw +

vM 2 )Nall

+

(w

Q

QR vw + vM 2

 ) Nall

+

vw

wvw + 1 (wQvQ +

vM 2 )2

(w

Q

K

+

M vw

+

K vw2

)

+

R vw (w Q vQ

+

vM 2 )

+

w  (wQvQ +

vM 2 )

Nall

log

2d 

34

(21)

For simplicity, we use CLS to as a shorthand of

O

vQ

1 + Q (w Q vw

vQ +

vM 2

)2

(w

QK

+

M vQ

+

K vQ2

)

+

 vQ (w Q vw

+ vM 2 )

+

QR (wQvw + vM 2 )

+

vw

wvw + 1 (wQvQ + vM 2

)2

(w

QK

+

M vw

+

K vw2

)

+

R vw (w Q vQ

+

vM 2 )

+

w  (wQvQ +

vM 2 )

Next, we try to convert the above high probability bound to an upper bound for expectation. We use X to denote  -  2 +  -  2 and treat it as a r.v.. Then, we have:

P (X



CL2 S Nall

log2

2d 

)



1 - 5

or equivalently,



P (X  x)  1 - 10d exp(-

xNall CLS

)

Therefore, computing the expectation of the distribution described by the following C.D.F. can pro-

vide us an upper bound for E[X]:



FX (x) = P (X  x) = 1 - 10d exp(-

xNall CLS

)

As a result,

E[X] 


(1 - FX (x))dx =
x=0

 x=0

10d

exp(-

 xNall CLS

)dx

=10d

 x=0

exp(-x)d

CL2 S Nall

x2

=

20d

CL2 S Nall

which means that,

E[  -  2 +

 - 

2]

=

20d

CL2 S Nall

(22)

Because   Z and   , the projection can only shrink the distance:

E[  - PZ () 2 +  - P() 2]  E[  -  2 +  -  2]

As

a

result,

for

arbitrary c,

we

can choose

Nall

=

20d

CL2 S c

in

Algorithm 3,

and

Algorithm 3

satisfies

the oracle condition with  = 0:

E[ PZ () -  2 + P() -  2]  c

(23)

E.3 The Second Example for Saddle-Point Solver Oracle
In this section, we provide another example for the oracle in Definition 5.1 based on first-order optimization, which is inspired by SVRE[Chavdarova et al., 2019].
E.3.1 Stochastic Variance-Reduced Extragradient with Batch Data where P and P are projection operator; LN (, , ) denotes the average gradient over samples from mini batch data N . Besides, we define:
dN (1, 1, 2, 2) = LN (, 1, 1) -  LN (, 2, 2) dN (1, 1, 2, 2) =LN (, 1, 1) - LN (, 2, 2)
35

Algorithm 4: Stochastic Variance-Reduced Extragradient with Mini Batch Data (SVREB)

1 Input: Stopping time K; learning rates , ; Initial wegihts 0, 0; Distribution dD; Batch size |N |.

2 Sample mini batch N, N  dD with batch size |N |

3 g0   LN (, 0, 0), g0  LN (, 0, 0) 4 1  P (0 +  g0 ) 5 1  P(0 - g0) 6 m1, m1   LN (, 0, 0), LN (, 0, 0) 7 for k = 1, 2, ...K + 1 do

8 Sample mini batch N, N  dD with batch size |N |

9

gk = mk + dN  (k, k, k-1, k-1)

10

gk = mk + dN  (k, k, k-1, k-1)

11

k+1/2 = P (k +  gk )

12

k+1/2 = P(k - gk )

13 Sample mini batch N , N  dD with batch size |N |

14

gk+1/2 = mk + dN (k+1/2, k+1/2, k-1, k-1)

15

gk+1/2 = mk + dN (k+1/2, k+1/2, k-1, k-1)

16

k+1 = P (k +  gk+1/2)

17

k+1 = P(k - gk+1/2)

18 // The following has been computed in step 9 and 10

19

mk+1, mk+1   LN (, k, k), LN (, k, k)

20 k  k + 1

21 end 22 Output: K , K

Obviously,

E[gk ] =  LD(, k, k),

E[gk+1/2] =  LD(, k+1/2, k+1/2)

where the expectation only concerns the randomness of sample when computing g. The above relationship also holds if we consider gradient w.r.t. .

For this Algorithm 4, we have the following theorem:

Theorem E.4. Under Assumptions in Section 2.2, in Algorithm 4, if step sizes satisfy





50

1 max{L¯

,

µ

}

,





1 50 max{L¯, µ}

after K iterations, the algorithm will return us (K , K ):

E[ K -  2 +

K - 

2]



201 100

1

-

µ 4

K
E[

0 - 

2+

0 -  2]

+

min{

82

, µ  µ 

4

4

}|N

|

(

 µ

+

 µ

)

where (, ) is the saddle point of LD(, , ) given input .

We defer the proof to the next sub-section.

E.3.2 Proofs for Algorithm 4
For simplification, we will use  vector concatenated by  and .

= [, ]  Z ×  :=  to denote the Similarly, gt = [-gt, gt], and FN () =

36

E(s,a,r,s,a0,a)N {[- L(s,a,r,s,a0,a)(, , ),  L(s,a,r,s,a0,a)(, , )]}, where N is the mini batch data sampled according to dD, and L(s,a,r,s,a0,a)(, , ) is the gradient computed with one sample (s, a, r, s, a0, a). We use F () := END[FN ()] to denote the gradient expected over entire dataset distribution. Besides,

gt =[- gt , gt]; L¯2 w 2 =L¯2  2 + L¯2  2;

2  2 = 2  2 + 2  2; 2L¯2 = 2L¯2 + 2L¯2;

µ  2 = µ  2 + µ  2 µ =  µ + µ

The update rule for Algorithm 4 can be summarized as

Extrapolation : t+1/2 = P(t - gt) Update : t+1 = P(t - gt+1/2)

Besides, in this section, the expectation E concerns all the randomness starting from the beginning of the algorithm. Lemma E.5 (Lemma 1 in [Chavdarova et al., 2019]). Let    and + := P(w + u), then for all w  , we have
+ -  2   -  2 + 2u(+ - ) - + -  2
Lemma E.6 (Adapted from Lemma 3 in [Chavdarova et al., 2019]). For any w  , when t > 0, we have
t+1 -  2  t -  2 - 2gt+1/2(t+1/2 - ) + 2 gt - gt+1/2 - t+1/2 - t 2 and when t = 0, we have
1 -  2  0 -  2 - 2g0(1 - )
Proof. For t = 0, by simply applying Lemma E.5 for (, u, +, ) = (0, -g0, 1, ), we have: 1 -  2  0 -  2 - 2g0(1 - ) - 1 - 0 2  0 -  2 - 2g0(1 - )
For t > 0, the proof is exactly the same as Lemma 3 in [Chavdarova et al., 2019]

Lemma E.7 (Bound gt - gt+1/2 2). For t > 0, we have: E[ gt - gt+1/2 2] 10E[ FN (wt) - FN () 2] + 10E[ FN () - FN (wt-1) 2] + 5L¯2E[ wt - wt+1/2 ]
Proof. For t > 0: E[ gt - gt+1/2 2]
=E[ FN (wt) - FN (wt-1) + mt - FN (wt+1/2) + FN (wt-1) - mt 2] =E[ FN (wt) ± FN (w) - FN (wt-1) - FN (wt+1/2) ± FN (wt) ± FN (w) + FN (wt-1) 2] 5E[ FN (wt) - FN () 2] + 5E[ FN () - FN (wt-1) 2]
+ 5E[ FN (wt+1/2) - FN (wt) 2]] + 5E[ FN (wt) - FN () 2] + 5E[ FN () - FN (wt-1) 2] =10E[ FN (wt) - FN () 2] + 10E[ FN () - FN (wt-1) 2] + 5E[ FN (wt+1/2) - FN (wt) 2]]
where in the inequality we use the extended Young's inequality; in the last equation we use the fact that
END[ FN (wt) - FN (w) 2] = END[ FN (wt) - FN (w) 2], w  
Besides, according to Assumption B.4 E[ FN (wt+1/2) - FN (wt) 2]]  L¯2E[ wt - wt+1/2 ]
As a result, E[ gt - gt+1/2 2] 10E[ FN (wt) - FN () 2] + 10E[ FN () - FN (wt-1) 2] + 5L¯2E[ wt - wt+1/2 ]

37

Proposition E.8. With Property B.1, for arbitrary , the operator F () satisfying:


F (1) - F (2) 1 - 2

 µ 1 - 2 2

Proof. Based on Property B.1, we have:

-LD (,

1,

2)



-

LD (,

2,

2)

-

 LD(,

1, 1)(2

-

1)

+

µ 2

2 - 1

2

-LD (,

2,

1)



-

LD (,

2,

2)

-

 LD(,

2, 2)(1

-

2)

+

µ 2

2 - 1

2

LD (,

1,

2)

LD (,

1,

1)

+

 LD (,

1,

1 ) (2

-

1)

+

µ 2

2 - 1

2

LD (,

2,

1)

LD (,

2,

2)

+

 LD (,

2,

2 ) (1

-

2)

+

µ 2

2 - 1

2

Sum up and we can obtain





F (1) - F (2) 1 - 2 := F (1, 1) - F (2, 2) [1, 1] - [2, 2]

=-

 LD(, 1, 1) -  LD(, 2, 2)


(1 - 2) +

LD(, 1, 1) - LD(, 2, 2)


(1 - 2)

µ 2 - 1 2 + µ 2 - 1 2 := µ 1 - 2 2

Theorem E.4. Under Assumptions in Section 2.2, in Algorithm 4, if step sizes satisfy





50

1 max{L¯

,

µ

}

,





1 50 max{L¯, µ}

after K iterations, the algorithm will return us (K , K ):

E[ K -  2 +

K - 

2]



201 100

1

-

µ 4

K
E[ 0 -  2 +

0 -  2]

+

min{

82

, µ  µ 

4

4

}|N

|

(

 µ

+

 µ

)

where (, ) is the saddle point of LD(, , ) given input .

Proof. When t > 0, from Lemma E.6, we have t+1 -  2  t -  2 - 2gt+1/2(t+1/2 - ) - t+1/2 - t 2 + 2 gt - gt+1/2 2
Next, we use Pt+1 to denote E[ t+1 -  2] +  E[ FN () - FN (wt) 2], where  will be determined later, then we have

Pt+1 =E[ t+1 -  2] +  E[ FN () - FN (wt) 2] E[ t -  2] - 2E[F (t+1/2)(t+1/2 - )] - E[ t+1/2 - t 2] + 2E[ gt - gt+1/2 2] +  E[ FN () - FN (wt) 2] - 2E[(mt - F (t-1))(t+1/2 - )] (E[gt+1/2(t+1/2 - )] = E[(F (t+1/2) - F (t-1) + mt)(t+1/2 - )]) E[ t -  2] - 2E[F (t+1/2)(t+1/2 - )] - (1 - 52L¯2)E[ t+1/2 - t 2] + ( + 102)E[ FN (wt) - FN () 2] + 102E[ FN () - FN (wt-1) 2]
+ 2 E[ mt - F (t-1) 2]E[ t+1/2 -  2] (Lemma E.7 and Cauthy Inequality: E[ab|c]  E[ a 2|c]E[ b 2|c])
38

E[ t -  2] - 2E[F (t+1/2)(t+1/2 - )] - (1 - 52L¯2)E[ t+1/2 - t 2]

+ ( + 102)E[ FN (wt) - FN () 2] + 102E[ FN () - FN (wt-1) 2]

+

8 µ

E[

mt - F (t-1)

2]

+

µ 8

E[

t+1/2 - 

2]

(2 |ab|  a 2 + b 2)

E[ t -  2] - 2E[F (t+1/2)(t+1/2 - )] - (1 - 252L¯2 - 2 L¯2)E[ t+1/2 - t 2]

+ (2 + 202)E[ FN (wt+1/2) - FN () 2] + 102E[ FN () - FN (wt-1) 2]

+

82 |N |

(

 µ

+

 µ

)

+

µ 4

(E[

t+1/2 - t

2 + E[

t - 

2])

(Condition B.3; Young's Inequality; E[ FN (t+1/2) - FN (t) 2]  L¯2E[ t+1/2 - t 2])

E[ t -  2] - 2E[F (t+1/2)(t+1/2 - )] - (1 - 252L¯2 - 2 L¯2)E[ t+1/2 - t 2]

+ (2 L¯ + 202L¯)E[(FN () - FN (wt+1/2))( - wt+1/2)] + 102E[ FN () - FN (wt-1) 2]

+

82 |N |

(

 µ

+

 µ

)

+

µ 4

(E[

t+1/2 - t

2 + E[

t - 

2])

(Assumption B.4)

=E[ t -  2] - (2 - 20L¯2 - 2 L¯)E[(F (t+1/2) - F (w))(t+1/2 - )]

- (1 - 252L¯2 - 2 L¯2)E[ t+1/2 - t 2] + 102E[ FN () - FN (wt-1) 2]

+

82 |N |

(

 µ

+

 µ

)

+

µ 4

(E[

t+1/2 - t

2 + E[

t - 

2])

By Prop. E.8, we have:

(F (w) - F (t+1/2))( - t+1/2)  µ

 - t+1/2

2



µ 2

wt - 

2-µ

wt+1/2 - wt

2

(24)

By

choosing 0

<





50

1 max{L¯ 

,µ

}

,

0

<





50

1 max{L¯  ,µ

}

,

and



=

152,

we

know

2 - 20L¯2 - 2 L¯ = 2 - 50L¯2  0

As a result, we can use (24) to get:

Pt+1

(1

-

µ

+

10µ2L¯

+

 µL¯

+

µ 4

)E[

t - 

2]

-

(1

-

25 2 L¯ 2

-

2 L¯2

-

2µ

+

20µL¯2

+

2µ L¯

-

µ 4

)E[

t+1/2 - t

2]

+

102 



E[

FN () - FN (wt-1)

2] +

82 |N |

(

 µ

+

 µ

)

 (1

-

3 4

µ

+

25µ2L¯)

E[

t - 

2]

+

(55 2 L¯ 2

+

9 4

µ

-

50µL¯2

-

1) E[

t+1/2 - t

2]

p1

p2

+

2 3



E[

FN () - FN (wt-1)

2]

+

82 |N |

(

 µ

+

 µ

)

since 0 < µ  1/50 and 0 < L¯  1/50

p1

1

-

3 4

µ

+

25µ 50

=

1

-

µ 4

p2



11 500

+

9 200

-

1



11 500

+

9 200

-

1



0

As a result

Pt+1

(1

-

µ 4

)E[

wt - 

2]

+

2 3



E[

FN () - FN (wt-1)

2]

+

82 |N |

(

 µ

+

 µ

)

39



1

-

min{

µ 4

,

1 3

}

Pt

+

82 |N |

(

 µ

+

 µ

)

=

1

-

µ 4

Pt

+

82 |N |

(

 µ

+

 µ

)

(µ  1/50)



1

-

µ 4

t
P1

+

min{

82

, µ  µ 

4

4

}|N

|

(

 µ

+

 µ

)

=

1

-

µ 4

t
(E[

w1 - 

] +  E[

FN () - FN (0)

2])

+

min{

82

, µ  µ 

4

4

}|N

|

(

 µ

+

 µ

)

Next, we take a look at E[ w1 -  ], from Lemma E.6, we have:

E[ 1 -  2]

E[ 0 -  2 - 2g0(1 - )]

=E[ 0 -  2] + 2E[(F (0) - g0)(1 - )] + 2E[(F () - F (0))(1 - )]

E[ 0 -  2] + 2E[ F (0) - g0 2]E[ 1 -  2] + 2E[(F () - F (0))(1 - )]

E[

0 - 

2]

+

22 |N |

E[

1 - 

] + 32E[

F () - F (0)

2]

+

1 3

E[

1 - 

2]

E[

0 - 

2]

+

µ 4

E[

1 - 

] + 32E[

F () - F (0)

2]

+

1 3

E[

1 - 

2]

E[

0 - 

2]

+

1 2

E[

1 - 

] + 32E[

F () - F (0)

2]

(µ < 1/50)

Therefore, E[ 1 -  2]  2E[ 0 -  2] + 62E[ F () - F (0) 2]
Finally, using the fact that E[ FN () - FN (0) 2]  L¯2E[  - 0 2], we have

E[

t+1 - 

2]

-

82

min{

µ  4

,

µ  4

}|N

|

(

 µ

+

 µ

)



Pt+1

-

82

min{

µ  4

,

µ  4

}|N

|

(

 µ

+

 µ

)



1

-

µ 4

t
(2E[

0 - 

2] + 62E[

F () - F (0)

2] +  E[

FN () - FN (0)

2])

=

1

-

µ 4

t
(2E[

0 - 

2] + 62L¯2E[

 - 0

2] + 152L¯2E[

 - 0

2])



1

-

µ 4

t
(2

+

6 2500

+

15 2500

)E[

 - 0

2]



201 100

1

-

µ 4

t
E[

 - 0

2]

which finishes the proof.

F Missing details for Algorithm 2

In the following, we will

LD (t , and 

LtBt,

t), as a

where t, shortnote

usteisLtDth,e

LBt and LDt  as only one saddle

of the gradient averaged

shortnotes of LD(t, t, t), LB(t, t, t) and

point of over dD

LD and

(t, the

, ). Besides, we gradient averaged

use LDt over batch,

respectively.

Lemma F.1. Denote (1, 1) and (2, 2) as the saddle-point of maxZ min LD(1, , ) and maxZ min LD(2, , ) respectively. Under Assumptions in Section 2.2 and Condition 4.1, we have

1 - 2   ( + 1) 1 - 2 , 1 - 2  ( + 1) 1 - 2

Proof. With Condition 4.1, we have

 LD(2, 1, 1) =  LD(1, 1, 1) -  LD(2, 1, 1)  L 1 - 2

(25)

40

LD(2, 1, 1) = LD(1, 1, 1) - LD(2, 1, 1)  L 1 - 2

(26)

Recall in Lemma A.1, we know 2() should be a µ-strongly-concave function. Then, we have

1 - 2



1 µ

 2 (1)

=

1 µ

 LD(2, 1, 2 (1))

1 µ

 LD(2, 1, 2 (1)) -  LD(2, 1, 1)



1 µ

 LD(2, 1, 2 (1)) -  LD(2, 1, 1)



L µ

2 (1) - 1

+

L µ

1 - 2



L µ µ

LD(2, 1, 1)

+

L µ

1 - 2

 ( + 1) 1 - 2

+1 µ

+

L µ

LD(2, 1, 1)) 1 - 2

where in the first step, we use Lemma A.3; in the fourth and fifth inequalities, we use the Property 2.2; in the last inequality, we use Eq.(25) again.

We can give a similarly discussion for 1 - 2 :

1 - 2



1 µ

 2 (1 )

=

1 µ

LD(2, 2 (1), 1)



1 µ

LD(2, 2 (1), 1) - LD(2, 1, 1)



1 µ

LD(2, 2 (1), 1) - LD(2, 1, 1)



L µ

1 - 2 (1)

+

L µ

1 - 2



L µ µ

 LD(2, 1, 1)

+

L µ

1 - 2

( + 1) 1 - 2

+

1 µ

+

L µ

LD(2, 1, 1)) 1 - 2

Lemma 5.2. [Relate the shift of t and t with t] Denote (t, t, t) as the parameter value at the beginning at the step t in Algorithm 2, and denote (t, t) = arg maxZ min LD(t, , ) as the only saddle point given t. Under Assumptions in Section 2.2, we have:

E[ t+1 - t 2 +

t+1 - t 2] 6t+1d2 + 62C,

t

t- E[

g

2]

+

1

6c -

.

 =0

where d := max{CW , CQ} is the maximum of diameters of Z and , C, is a short note of

2µ(

+ 1)2

+

2 (µ

+

1)2,

g

:=

1 

(

+1

-

 ),

and



is

the

step

size

of

.

Proof. We will use t(, ) to denote E[  - t 2 +  - t 2]. We first study some useful properties of t(, ).
Property 1 For t  1 t(t-1, t-1) =E[ t - t-1 2 + t - t-1 2]  C,E[ t - t-1 2] = 2C,E[ gt-1 2] where in the inequality, we use Lemma F.1; and the last equality results from the update rule t = t-1 + gt-1

41

Property 2 For t  0,

t(t,

t)



 2

t(t-1,

t-1)

+

c

=

 2

E[

t-1 - t

2+

t-1 - t

2] + c

E[ t-1 - t-1 2 + t-1 - t-1 2 + t - t-1 2 + t - t-1 2] + c

=t-1(t-1, t-1) + t(t-1, t-1) + c

t

t-1

t0(0, 0) + t- +1 (-1, -1) +  c

 =1

 =0

t-1

t

t+1d2 + 2C, t- E[ g 2] +  c

 =0

 =0

t-1

t+1d2 + 2C,

t- E[

g

2]

+

1

c -



 =0

where the first inequality is because of the property of the Oracle; for the second inequality we use Young's inequality; In the last step, we use

0(0, 0) = E[ 0 - 0

2+

0 - 0

2] 

 2

E[

-1 - 0

2+

-1 - 0

2] + c  d2 + c

With the two properties above, we can bound:

E[ t+1 - t 2 + t+1 - t 2]

3E[ t+1 - t+1 2 + t+1 - t+1 2 + t+1 - t 2 + t+1 - t 2 + t - t 2 + t - t 2] =3t+1(t+1, t+1) + 3t+1(t, t) + 3t(t, t)

t

t-1

3t+2d2 + 32C,

t- +1E[ g 2] + 32C,E[ gt 2] + 3t+1d2 + 32C,

t- E[ g 2]

 =0

 =0

+

1

6c -

=3(1 + )t+1d2 + 32C,

t
(1 + )t- E[ g

2]

+

1

6c -

 =0

6t+1d2 + 62C,

t

t- E[

g

2]

+

1

6c -

 =0

where for the first one we use an extended version of Young's inequality

k i=1

xi

2



k

k i=1

xi

2; in the second inequality, we use the Property 1 and 2 to give the upper bound; in

the third inequality, we use the fact that 0 <   1.

Lemma F.2. Define LD() := maxZ min LD(, , ). Under the same condition of Lemma 5.2 above, with an additional constraint   (1 - )2/2, for t  0, we have:

E[

gt+1 - LD(t+1)

2] 2t+1E[

g0 - LD0

2

]

+

2

(1

22 - )|B|

+ 4L2

 t+2 d2

+

1

c -

+

(1

3c - )(1 - )

+

3d2t+2 -

+ 22L2E

t

t-i+1

6C

,



 -



+1

+ 2C,t-i+1

i=0

gi 2

where data, func, reg are the same as those in Theorem 3.3, and  as a shortnote of 2(1 - )2.

Proof.  LD

(Rte,catl,ltth)a, tweLDwi(lltu)serespecLtBtiv,ely,aLnDtd

and LDt  as in Property B.3,

a shortnote of LB (t, t, t), we have already shown the vari-

42

ance of batch gradient can be bounded. First we can use the Young's inequality to obtain

E[ gt+1 - LDt+1 2]  2 E[ gt+1 - LDt+1 2] +2 E[ LDt+1 - LDt+1 2]

p1

p2

Since the first term has already been bounded in Theorem 3.3. Next, we bound p1 and p2:

Upper bound p1 We again use C, as a short note of 2µ( + 1)2 + 2( + 1)2. From Lemma 5.2, we know that,

E[ t+1 - t 2 +

t+1 - t 2] 6t+1d2 + 62C,

t

t- E[

g

2]

+

1

6c -

 =0

In the following, we will use  as a shortnote of 2(1 - )2.

p1 = E[ gt+1 - LDt+1 2]
2
=E (1 - )gt + LBt+1 - LDt+1 ± (1 - ) LDt

=E (1 - )(gt - LDt ) + ( LBt+1 - LDt+1) + (1 - )( LDt - LDt+1) 2

=E (1 - )(gt - LDt ) + (1 - )( LDt - LDt+1) 2 + E ( LBt+1 - LDt+1) 2

(Drop 0 expectation)

E

2
(gt - LDt ) + E

2
(LDt - LDt+1)

+

22 |B|

(Young's Ineq.)

E

(gt - LDt ) 2

+ L2E[ t+1 - t 2 +

t+1 - t 2 +

t+1 - t

2]

+

22 |B|

(Smoothness of LD)

t+1E[

g0 - LD0

2]

+

22 |B|

1

- t+1 1-

+E

t
t-i+1 L2

i=0

i+1 - i 2 + i+1 - i 2 + i+1 - i 2

t+1E[

g0 - LD0

2]

+

(1

22 - )|B|

+

E

t

t-i+1L2 6i+1d2 + 62C,

i

i- E[

g

2]

+

1

6c -

+

2

gi

2

i=0

 =0

t

t

t+1E[ g0 - LD0 2] + E

2L2 6C,

t- +1 -i + t-i+1 gi 2

i=0

 =i

+

(1

22 - )|B|

+

(1

6cL2 - )(1 -

)

+

6L2d2t+2 -

t+1E[ g0 - LD0 2] + E

t

2 L2 t-i+1

6C, 

 -



+

1

i=0

+

(1

6cL2 - )(1 -

)

+

6L2d2t+2 -

gi

2

+

(1

22 - )|B|

where the fourth equality because E[LBt ] = LDt holds for all t and so the cross terms has 0 expectation; the first inequality is because variance is less than the second momentum; the second inequality we apply Assumption A; in the last but two inequality, we apply the summation formula of equal ratio sequence and use the fact that 0 <   1,   1; in the last step, we use our condition   (1 - )2/2

Upper bound p2 Next, we give an upper bound for p2. From the Property 2 in Lemma 5.2, we know that
t+1(t+1, t+1) =E[ t+1 - t+1 2] + E[ t+1 - t+1 2]

43

t

t+2d2 + 2C,

t- +1E[

 =0

g

2]

+

1

c -



As a result

p2 =E[ LDt+1 - LDt+1 2]  2L2E[ t+1 - t+1 2 + t+1 - t+1 2]

2L2 t+2d2 + 2C,

t

t- +1E[

g

2]

+

1

c -



)

 =0

Combine these two results we can finish the proof:

E[ gt+1 - LD(t+1) 2]  2p1 + 2p2

2t+1E[ g0 - LD0 2] + 2E

t

2 L2 t-i+1

6C,



 -



+

1

i=0

gi 2

+

2 (1

22 - )|B|

+

(1

12cL2 - )(1 -

)

+

12L2d2t+2 -

+

4L2

t+2d2 + 2C,

t

t- +1E[

g

2]

+

1

c -



)

 =0

2t+1E[

g0 - LD0

2]

+

2 (1

22 - )|B|

+

4L2

t+2d2

+

1

c -



+

(1

-

3c )(1

-

)

+

3d2t+2 -

+ 22L2E

t

t-i+1

6C, 

 -



+

1

+ 2C,t-i+1

i=0

gi 2

Proposition F.3. Under Assumption A and B, define LD() := maxZ min LD(, , ) is L, smooth with L, := L(1 +  ( + 1) +  ( + 1)).

Proof. For arg maxZ

arbitrary 1, 2 min LD(, ,

 , ) when

denote (1,  = 1 and 

1) and (2, 2) = 2, respectively,

as the saddle-point according to Lemma

of F.1

we have:

LD(1) - LD(2) L( 1 - 2 + 1 - 2 + 1 - 2 ) L(1 +  ( + 1) +  ( + 1)) 1 - 2

Theorem F.4. Under Assumption A, B and C, given arbitrary , Algorithm 2 will return us a policy , satisfying
E[ J () ]   + reg + data + func
if the hyper-parameters in Alg. 2 satisfy the following constraints:

T

=max{96,

16LDL 2

,

,

16LDL 28C, 2

+

2 }

=

O(-2),

|B|

=



362 2

]

=

O(-2),

 = 0.5,  = 2(1 - )2 = 0.5,



=

min{

2 L2d2

,

(1

-

)2,

1/2},



=

min{

1 2L,

,

1- 6L2(14C,

+

1) }.

where LD := max LD() - LD(0), C, = 2µ( + 1)2 + 2( + 1)2, and coracle is an independent constant.

Besides, the total gradient computation for O-SRM to obtain  should be O(-4), if:

(1) either we choose Algorithm 3 in Appendix E.2 as the ORACLE and set the hyper-parameter |Nall| in Algorithm 3 to be

|Nall|

=

480

d(2L2

+ 2

1)CL2 S

= O(-2)

44

where CLS is a constant defined in Appendix E.2,

(2) or we choose Algorithm 4 in Appendix E.3 as the ORACLE and set the hyper-parameters in

Algorithm 4 to be:

|N |

=



240(L2

min{

µ  4

+ 1)2

,

µ  4

}2

(

 µ

+

 µ

),

K

=

coracle

log(

1 

),





50

1 max{L¯

,

µ

}

,





50

1 max{L¯

,

µ

}

.

where L¯ and L¯ are defined in Condition B.4.

Proof. Recall the definition of LD in Prop F.3 and LD is L + C,-smooth.

LD(T +1) =LD(T + gT )

LD(T )

+

(gT )LD(T )

-

2 L , 2

gT

2

=LD(T

)

+

 2

LD(T )

2-

 2

gT

- LD(T )

2

+

(

 2

-

2L, 2

)

gT

2

LD(T

)

+

 2

LD(T )

2-

 2

gT

- LD(T )

2+

 4

gT

2

LD(0)

+

 2

T

LD(T )

2-

 2

T

gt - LD(T )

2

-

1 2

gt

2

t=0

t=0

p

where in the second equation, we use the fact that (gT )LD(T )

=

1 2

LD(T ) 2 +

1 2

gT

2-

1 2

gT

- LD(T )

2;

in the second inequality,

we

add a

constraint for



that





; 1
2L,

Next, we give a upper bound for p with Lemma F.2:

T
p=

g - LD(t)

2

-

1 2

gt

2

t=0

T


2t+1E[

g0 - LD0

2] + 2 22 + 4L2 (1 - )|B|

t+2d2 + c +

3c

+ 3d2t+2

1 -  (1 - )(1 - )  - 

t=0

+ 22L2E

t i=0

t-i+1

6C, 

 -



+

1

+ 2C,t-i+1

gi 2

-

1 2

E[

gt

2]

2(T

+

1) (1

22 - )|B|

+

4(T

+

1)cL2( 1

1 -



+

(1

-

3 )(1

-

) )

+

2(T + 1) 1-

E[

g0 - LD0

2]

+

4L2d2

(

1

2 -



+

(1

-

32 )( -

))

+

T t=0

E[

gt

2]

-

1 2

+

22L2

T -t+1
E

i

6C, 

 -

+1

i=0

2(T

+

1) (1

22 - )|B|

+

4(T

+

1)cL2( 1

1 -



+

(1

-

3 )(1

-

) )

+

2(T + 1) 1-

E[

g0 - LD0

2]

+ 2C,i

+

4L2d2

(

1

2 -



+

(1

-

32 )( -

))

+

T t=0

E[

gt

2]

-

1 2

+

22L2

1 1-

6C, 

 -



+

1

+

2C

,

1

1 -



2(T

+

1) (1

22 - )|B|

+

4(T

+

1)cL2( 1

1 -



+

(1

-

3 )(1

-

) )

+

2(T + 1) 1-

E[

g0 - LD0

2]

+

4L2d2

(

1

2 -



+

6 1-

)

+

T

E[

gt

2]

t=0

-

1 2

+

22L2 1-

14C, + 1

(



(1 -

)2

=

 2



)

45

2(T

+

1) (1

22 - )|B|

+

4(T

+

1)cL2( 1

1 -



+

(1

-

3 )(1

-

) )

+

2(T + 1) 1-

E[

g0 - LD0

2]

+

4L2d2

(

1

2 -



+

6 1-

).

In the fourth inequality, we add the following constraint to drop the terms containing g :

 

1- 4L2(14C,

+

1)

.

(27)

Therefore,

1T T +1
t=0

LD( )

2

 (T

2 + 1)

(LD(T

)

-

LD

(0))

+

T

1T +1
 =0

g - LD( )

2

-

1 2

g

2

4cL2( 1

1 -



+

(1

-

3 )(1

-

) )

+

2LD (T + 1)

+

1

2 -

 E[

g0 - LD0

2]

+

2 (1

22 - )|B|

+

4L2d2

(

1

2 -



+

6 1-

)



4cL2

(6 + 1) 1-

+

2LD (T + 1)

+

2(1 1

+ -

2) 

2 |B|

+ 24L2d2 (T

+

 1)(1

-

)

.

p0

p1

p2

p3

(  1/2,   , E[g0] = LD0 )

The choice of  In order to guarantee that  = 2(1 - )2 < 1 and  < 1, we first need  

(1 -

2 2

,

1).

Easy

to

observe

that,

the

coefficient

of

p2:

1 + 2 1-

=

1 + 2 1 - 2(1 - )2

=

1/2 + 2 -1 + 4 - 22

- 1/2.

By taking the derivative w.r.t. , we have:

(1

1 + 2 - 2(1 - )2

)

=

2(-1

+

4

- 22) - (1/2 + 2)(4 (-1 + 4 - 22)2

-

4)

=

42 (-1 +

+ 2 4 -

-4 22

)2

.

We

can

observe

easily

that

the

zero

point

of

the

first-order

derivative

should

located

in

interval

(

1 2

,

1).

Besides, p0, p1, p3 are decreasing as  approaching 1 (i.e.  approaching 0). Therefore, the optimal

choice

of



should

locates

in

(

1 2

,

1).

As we can see, the introduction of  is to balancing the bias of momentum term and the variance in

the current step to provide a better estimation of the gradient in current step. In the following, for

simplicity, we directly choose  = 1/2.

Next, we want to carefully choose other hyper-parameters to make sure p0, p1, p2, p3  2/4. We

consider





min{

2 L2d2

,

(1

-

)2,

1/2}.

Control p0 Since  < 1/2, we know

p0  32cL2.

To make sure p0  2/4, we need

c



2 128L2

.

If we choose Algorithm 3 as ORACLE, the hypyer-parameter |N | (total sample size) in Algorithm 3 should be set to:

|N

|



20d

CL2 S c

= O(-2)

46

If we choose Algorithm 2, the hyper-parameter |N | (mini batch size) and K (total iteration) in Algorithm 3 should be set to:

min{

82

, µ  µ 

4

4

}|N

|

(

 µ

+

 µ

)



2 128L2

,

where coracle is an independent constant. Therefore,

K

=

coracle

log(1/)

=

O(log

1 

).

|N |



240(L2

min{

µ  4

+ 1)2

,

µ  4

}2

(

 µ

+

 µ

)

=

O(-2).

Control p1

Since

we

have

two

constrains on

 :





1 2L,

and





sure

p1



2 4

,

we

need,

1- 4L2 (14C ,

+1)

.

To

make

2LD (T + 1)

max{

4L2(14C, 1-

+

1) , 2L,}



2 4

.

Therefore, we requirement

T



max{

16LDL 2

,

,

16LDL 28C, 2

+

2 }.

(28)

Control p2

Recall that we choose  = 1/2, we need:

p2

=

2(1 + )2 1-

2 |B|

=

9

2 |B|



2 4

.

which requires:

|B|



362 2

(29)

Control p3

Because

K

(iteration

number

of

Oracle

Algorithm)

only

depends

on

log

1 

,

we

choose





. 2
L2d2

Then, as

long

as

T



168, we

have:

p3

=

24L2d2 (T

 + 1)(1

- )

=

24L2d2 T +1



2 4

(30)

Combine (28)-(30), we need

T



max{96,

16LDL 2

,

,

16LDL 28C, 2

+

2 },

|B|



362 2

;

As a result, if we use Algorithm 3 as ORACLE, the total computation before obtaining out should be

|B| · (T + 1) + |N | · T = O(-2) + O(-4) + O(-2) · O(-2) = O(-4)

and if we use Algorithm 4 as ORACLE, it should be

|B|

·

(T

+ 1) +

|N | · K

·T

=

O(-2) +

O(-4) + O(-2)

· O(log

1 

)

·

O(-2

)

=

O(-4)

where we omit log terms in O(·).

Guarantee of E[ J() ] By applying the hyper-parameters discussed above, we have: E[ J () ] E[ LD() ] + reg + func + data

 E[ LD() 2] + reg + func + data

=

T

1 +1

T
E[
t=0

 LD (t )

2] + reg + func + data

 + reg + func + data

47

