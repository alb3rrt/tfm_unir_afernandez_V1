CIDER: Commonsense Inference for Dialogue Explanation and Reasoning
Deepanway Ghosal, Pengfei Hong, Siqi Shen , Navonil Majumder, Rada Mihalcea , Soujanya Poria
 Singapore University of Technology and Design, Singapore University of Michigan, USA
{deepanway ghosal, pengfei hong}@mymail.sutd.edu.sg {navonil majumder, sporia}@sutd.edu.sg {shensq,mihalcea}@umich.edu
causes
Gordon, you're ever so late. Yes, I am sorry. I missed the bus. But there's a bus every ten minutes, and you are over 1 hour late. causes

Well, I missed several buses. How on earth can you miss several buses? I, ah ..., I have got late.

causes

Oh, come on, Gordon, it's the afternoon now. Why were you late really? before

Well, I ... I lost my wallet, and I ... Have you got it now? causes

Yes, I found it again. When? This morning. I mean ... This tardiness causes embarrassment every time.

arXiv:2106.00510v1 [cs.CL] 1 Jun 2021

Triplet Types: Implicit Explicit Speaker: Speaker A Speaker B

Figure 1: Example of various types of knowledge triplets explaining a dyadic dialogue using commonsense inference; the purple and yellow relations signify implicit and explicit triplets, respectively.

Abstract

1 Introduction

Commonsense inference to understand and explain human language is a fundamental research problem in natural language processing. Explaining human conversations poses a great challenge as it requires contextual understanding, planning, inference, and several aspects of reasoning including causal, temporal, and commonsense reasoning. In this work, we introduce CIDER ­ a manually curated dataset that contains dyadic dialogue explanations in the form of implicit and explicit knowledge triplets inferred using contextual commonsense inference. Extracting such rich explanations from conversations can be conducive to improving several downstream applications. The annotated triplets are categorized by the type of commonsense knowledge present (e.g., causal, conditional, temporal). We set up three different tasks conditioned on the annotated dataset: Dialogue-level Natural Language Inference, Span Extraction, and Multi-choice Span Selection. Baseline results obtained with transformer-based models reveal that the tasks are difficult, paving the way for promising future research. The dataset and the baseline implementations are publicly available at https://github.com/ declare-lab/CIDER.

Understanding and explaining a conversation requires the decomposition of dialogue concepts -- entities, events and actions, and also connecting them through definitive relations. The process of breaking down dialogues into such explanations is grounded in the conversational context and often requires commonsense inference. Such explanations, when expressed in the form of structured knowledge triplets1 (Fig. 1), can describe the exact commonsense relation (causal/temporal/conditional/others) through which the concepts are related in the particular conversational context. Establishing such concept links that help explain the dialogue demands two distinct forms of commonsense inference: i) Explicit -- the explanation is verbatim in the triplet. Such triplets can be easily extracted out by a parser (e.g., syntactic, pattern matching). These triplets are also prevalent in existing commonsense knowledge graphs (Speer et al., 2017; Sap et al., 2019); and ii) Implicit -- the explanation is entirely contextual, making it more difficult for machines to infer as
1Knowledge triplets, and triplets are used interchangeably in this paper. In the context of this work, they mean the same.

it requires complex multi-hop commonsense reasoning skills. Our goal is to explain a dialogue by the means of these commonsense inferred triplets. This form of explanation may not be complete, but can give a substantial understanding of the dialogue by breaking it down into contextual triplets. The key element of the dialogue explanation using such triplets is the aspect of contextuality. The triplets extracted from a dialogue using commonsense inference are contextual and are grounded exclusively in that particular dialogue. From our world knowledge, we know that missing a bus could cause being late, but (missed the bus, causes, late) is grounded and definitive only in the dialogue illustrated in Fig. 1. This particular triplet may not be valid in a different dialogue, where the cause of being late could be something different. Similarly, losing wallet could cause a different consequence (apart from being late) e.g., getting anxious in the context of another dialogue. It is also important to highlight that some extracted triplets could be persona-specific. For instance, (tardiness, causes, embarrassment) is grounded in the conversation of Fig. 1, but tardiness may not cause embarrassment for every listener.
In literature, there has been much work on extracting structured knowledge triplets from natural language text. However, there has been only little research to distinguish implicit triplets from explicit triplets present in the text. Explicit triplets can be relatively easily parsed out using semantic parsing (Speer et al., 2017) and simple co-reference resolution (Joshi et al., 2019). Implicit triplets, however, involve non-trivial inference, which becomes even more challenging on dialogue data due to the contextual interplay and latent background knowledge shared between the speakers. Extraction of both explicit and implicit triplets can be conducive to improved dialogue understanding leading to better question-answering systems and richer knowledge bases. To this end, we construct a dataset of Commonsense Inference for Dialogue Explanation and Reasoning (CIDER) ­ as illustrated in Fig. 1 ­ which captures the relations between textual concepts or spans appearing in a dialogue. A concept or span can constitute one or multiple entities, objects, actions, states, or events that can be extracted from the dialogue. The relations are commonsense based, as elaborated in §3.2. Each triplet is tagged as explicit or implicit.
Through this dataset, we aim to evaluate whether state-of-the-art natural language processing models can really read, understand, and comprehend

the conversational context of dialogues. We define three tasks on this dataset that require dialoguelevel contextual commonsense reasoning -- (i) Dialogue-level Natural Language Inference, (ii) Span Extraction, and (iii) Multi-choice Span Selection. All three tasks require an overall contextual understanding of the dialogue with commonsense reasoning and inference. We setup different state-ofthe-art transformer language models as baselines and found that the tasks are challenging to solve. The Importance of this Dataset: The immediate aim of this research is to develop a rich corpus of dialogues with structured explanations in the form of implicit and explicit triplets, and then use this corpus to perform commonsense inference and reasoning. We formulate non-trivial natural language inference (NLI) and question answering (QA) tasks that can be used to benchmark such reasoning capabilities of natural language processing models.
2 Related Work
Recently, language models have been scaled up a lot and have seen a performance improvement on various tasks (Brown et al., 2020; Raffel et al., 2020). However, it has been proved that declarative knowledge is still valuable, especially implicit relationships that are hardly acquired by the state of the art models (Hwang et al., 2020).
Widely used commonsense knowledge bases such as ConceptNet (Speer et al., 2017) and ATOMIC (Sap et al., 2019) are mainly based on crowd-sourced effort. ConceptNet is a semantic network with nodes composed of common words or phrases in their natural language form. It contains 34 relations, including taxonomic, temporal, and causal ones, such as MotivatedByGoal and Causes. However, the knowledge in ConceptNet is annotated solely based on the first entity without any other context, making it difficult to capture the long-tail knowledge outside of the most common ones. ATOMIC focus on inferential knowledge and consists of nine relations, such as xIntent (the intent for personX's action) and xEffect (the effect of the event on personX). It covers knowledge around agents involved in the event for if-then reasoning, including subsequent events, mental state, and persona. However, it ignores causal relationships between events not carried out by a person. In contrast, our work captures relationships between spans across multiple turns in dialogues. As a result of the dialogue aspect of our data, we also manage to cover implicit knowledge that requires context

1 (B) 1 (B)

This tardiness causes embarrassment every

time.

Causes

IsA
Your main duty is to answer the phone calls and transfer them to the person wanted .

1 (A) Good morning , sir . Can I help you ? 2 (B) Yes , I want to deposit 1000 Yuan in my bank account .
HasPrerequisite
3 (A) Please ll out this deposit form , rst .

CausesDesire

Buy a new cell phone

Awful ! My cell phone is out of service again . Fred , can 
 1 (A)

you lend me some money ?

HasPrerequiste

MotivatedByGoal

(a)

(b)

(c)

Figure 2: (a) Explicit and (b) implicit triplets from dialogues. (c) Intermediate latent spans and triplets.

fi

fi

from conversations to make sense.

More recent work such as GLUCOSE

(Mostafazadeh et al., 2020) which is annotated

based on ROCstories (Mostafazadeh et al., 2016)

captures implicit knowledge across multiple sen-

tences. Our work instead annotates on dialogues,

which have more complicated sentences and

spoken conversational exchange.

3 Background
The primary impetus behind this dataset is the contextualized structured explanation of a dialogue in the form of concept triplets that can be inferred only through commonsense reasoning. The triplets are considered to be the commonsense explanations of different aspects and events that occur in the dialogue. Such aspects would include attributional, comparative, temporal knowledge, and the events may range from physical events involving physical entities, conditional and causal chains, social interactions, persona, etc.
We focus on conversations as our data source, with the choice being motivated by the fact that part of the context in conversations is naturally implicit and interlocutor dependent (Grice, 1975). Commonsense knowledge is considered to be the set of all facts and knowledge about the everyday world which is assumed to be known by all humans (Davis, 2014). For this very reason, human-tohuman dialogues ­ typically guided by the Gricean maxims of human interactions ­ tend to avoid explicit mentions of commonsense knowledge and the associated reasoning steps. It is thus reasonable to assume that conversations are generally likely to hold more context-specific inferable implicit knowledge than other genres. This ensures a rich dataset with plenty of contextual implicit triplets and a reasonable amount of explicit triplets.
Two distinct spans (e.g., events, entities) in a dialogue may have an implicit connection that can be trivial for humans to interpret using commonsense reasoning and contextual understanding, but can be challenging for machines. Uncovering implicit ex-

planations has the potential to enable many important tasks, which we focus on later on. In this work, we propose a dataset that contains manually labeled implicit explanations present in dyadic dialogues that require commonsense reasoning to infer. We use this dataset to evaluate the ability of pre-trained language models' to perform commonsense-based implicit reasoning tasks.
The extracted triplets or explanations, of the form (h, r, t) or alternatively h -r t, consist of a head (h) and a tail (t) span and the directed relation (r) between them. These spans are representative of some events, actions, objects, entities, and so on. The directed relation r comes from a predefined set of relations R that explain or describe the relationship between the head and tail spans within the context of the conversation -- illustrated in Fig. 1 with the arrows between spans. Notably, the relation set R is intended to be generic in nature, rather than specifically factual or taxonomic, so as to accommodate wide categories of knowledge (§3.2) inferred from the context of the conversation.
3.1 Types of Triplets
The extracted triplets are either explicit or implicit as defined below:
Explicit triplets represent explanations (see Fig. 2a) that are overtly expressed in an utterance in a dialogue. Fig. 1 illustrates one such annotated instance in utterance 13 -- tardiness -C-a-u-ses embarrassment -- where the triplet is worded verbatim in a head-relation-tail sequence. The head and tail span may contain some pronouns that can be decoded by simple co-reference resolution. In the presence of complex co-reference however the context suggests many possible candidates, and the triplet is implicit.
Implicit triplets, on the other hand, are not directly expressed in the dialogue and must be inferable through commonsense reasoning using the contextual information present in the dialogue. Instances of such triplets are shown in Fig. 1 and 2b with the relations in purple font.

Why Focus on Implicit Triplets? As pointed out earlier, extracting explicit triplets from a conversation or any natural language text is relatively straightforward and has been studied in much detail in the literature (Auer et al., 2007; Carlson et al., 2010; Speer et al., 2017). The much more challenging problem, however, is to extract implicit triplets or explanations. For example, in Fig. 1 the triplet miss several buses -C-a-u-ses over 1 hour late requires commonsense reasoning and knowledge about the world. Similarly, extracting another triplet lost my wallet -C-a-u-ses late requires multiutterance reasoning with contextual understanding. Such distillation is not covered by the explicittriplet extraction framework.
The decomposition of a dialogue into such implicit explanations also requires contextual understanding and complex commonsense reasoning involving multiple steps and utterances. Thus, the extraction of implicit explanations is challenging and a focus of this work.
Latent Spans and Differences with GLUCOSE (Mostafazadeh et al., 2020): As argued earlier, annotating implicit triplets often requires multi-step reasoning. In such cases, one or more intermediate spans (which may not be present in the dialogue) may be required to explain the relation between the constituting spans; see Fig. 2c for one such example. Annotators were given the freedom to identify such intermediate steps when they deemed so. However, such cases are infrequent in our dataset and thus we have chosen to omit the intermediate spans in our experimental studies for the sake of simplicity. We leave the intermediate step modelling as a direction for future work.
In this context, it is also important to highlight the fundamental differences between our dataset and GLUCOSE (Mostafazadeh et al., 2020). First, in our dataset, the knowledge represented by the spans and the relation connecting them is true (valid) given the context, but establishing this connection using an explicit relation requires complex commonsense inference and understanding of the discourse. The resulting triplet is thus valid in the context and grounded by the context. This is similar to deductive commonsense reasoning (Davis, 2014). GLUCOSE however focuses on abductive commonsense inference, where given an event/state and its context, the annotators provided inferred speculative causal explanations of the event (state) according to their world and common-

sense knowledge. These explanations, although they may fit in the given context, may not always be entailed by it. As a consequence, GLUCOSE is conducive to generative modeling, whereas our dataset leads to extractive modeling. Second, GLUCOSE has a limited set of relations, where inference is only performed across the following dimensions: cause, enable, and result in. In contrast, we have a much more diverse set of relations (§3.2). Finally, we construct our dataset based on conversations between two humans, while GLUCOSE is built using monologue-like stories that have significant differences with respect to the discourse structure and semantics.
3.2 Types of Relations
Our proposed CIDER dataset contains 25 main and 6 negated relations. Among the main 25 relations, 19 have been adopted from ConceptNet (Speer et al., 2017). We introduce 6 new relations to cover some aspects that are not covered by ConceptNet. Brief explanations, examples, and the new relations we introduce are shown in Table 1. We categorize the different relations as follows: Attribution. Relations that indicate attributes, properties, and definitions of concepts: (i) Capable Of, (ii) Depends On, (iii) Has A, (iv) Has Property, (v) Has Subevent, (vi) Is A, and (vii) Manner Of . Causal. Relations the indicate cause and effect of events: (i) Causes, (ii) Causes Desire, and (iii) Implies. Comparison. Relations that indicate comparison, similarity, or dissimilarity between concepts: (i) Antonym, (ii) Distinct From, (iii) Similar To, and (iv) Synonym. Conditional. This category, having only one relation Has Prerequisite, indicates dependency of one event on the other. Intentional. Relations indicating intent or usage of an entity or a person: (i) Desires, (ii) Motivated By Goal, (iii) Obstructed By, and (iv) Used For. Social. The category involves social commonsense relations specifying social rules, conventions, norms, and suggestions. The relation in this category is: (i) Social Rule. Spatial. This category encompasses relations which signifies spatial properties, such as location of events, entities, actions. The relations include: (i) At Location, and (ii) Located Near. Temporal. This category involves the idea of time considering the start, end, duration, and order of events. The constituent relations are: (i) Before,

Category Attribution
Causal
Comparison Conditional Intentional Social Spatial Temporal

Relation Capable Of Depends On* Has A Has Property Has Subevent Is A Manner Of Causes Causes Desire Implies*
Antonym
Distinct From Similar To
Synonym
Has Prerequisite
Desires
Motivated By Goal Obstructed By Used For Social Rule* At Location Located Near Before* Happens On* Simultaneous*

Explanation

Example

Something that A can typically do is B. A depends on B. B belongs to A, either as an inherent part or due to a social construct of possession. A has B as a property; A can be described as B. A and B are events, and B happens as a subevent of A. A is a subtype or a specific instance of B; every A is a B. A is a specific way to do B. Similar to "Is A", but for verbs.

knife  cut postage fee  weight of the post bird  wing; pen  ink ice  cold eating  chewing car  vehicle; Chicago  city auction  sale

A causes B to happen. A makes someone want B. A implies B.

exercise  sweat having no food  buy food wet cloth  caught in rain

A and B are opposites in some relevant way, such as being opposite ends of a scale, or fundamentally similar things with a key difference between them. Counter-intuitively, two concepts must be quite similar before people consider them antonyms. A and B are distinct member of a set; something that is A is not B. A is similar to B. A and B have very similar meanings. They may be translations of each other in different languages.

black  white; hot  cold
red  blue; August  September mixer  food processor sunlight  sunshine

In order for A to happen, B needs to happen; B is a dependency of A.

dream  sleep

A is a conscious entity that typically wants B. Many assertions of this type use the appropriate language's word for "person" as A. Someone does A because they want result B; A is a step toward accomplishing the goal B. A is a goal that can be prevented by B; B is an obstacle in the way of A. A is used for B; the purpose of A is B.

person  love
compete  win sleep  noise bridge  cross water

A is the social norm for when B happens or during B.

apology  late

A happens at location B, or B is a typical location for A. A and B are typically found near each other.

try clothes  changing room table  chairs

A starts/ends before B. A happens during B. A and B happens at the same time.

brush teeth  go to bed celebration  birthday heavy sports  heavy breath

Table 1: Annotated relations in our dataset. * indicates new relations introduced by us that are not present in ConceptNet.  in the examples indicate symmetric relations. In addition to the above, we also have a few negation relations as illustrated in §3.3.

(ii) Happens On, and (iii) Simultaneous.
3.3 Negative and Symmetric Relations
Apart from the relations in Table 1, the negations of some of these relations are necessary to form the triplets during annotation. These negated relations are (i) Not Causes, (ii) Not Causes Desire, (iii) Not Has Property, (iv) Not Implies, (v) Not Is A, and (vi) Not Motivated By Goal.
It should be noted that there are some symmetric relations in our relation set. A relation R is considered symmetric if the validity of A -R B implies the validity of B -R A and vice versa. The set of symmetric relations RS contains (i) Antonym, (ii) Distinct From, (iii) Similar To, (iv) Synonym, (v) Located Near, and (vi) Simultaneous.
4 Dataset Construction
4.1 Source Datasets of Dialogues
The annotation is performed on the following datasets containing dyadic dialogues: DailyDialog (Li et al., 2017) is aimed towards emotion and dialogue-act classification at utterance level. The conversations cover various topics ranging from ordinary life, work, and relationships, to tourism, finance and politics. MuTual (Cui et al., 2020) is a manually annotated dataset for multi-turn dialogue reasoning. It was introduced to evaluate several aspects of dialogue-

level reasoning in terms of next utterance prediction given a dialogue history. These aspects include attitude reasoning, intent prediction, situation reasoning, multi-fact reasoning, and others. DREAM (Sun et al., 2019) is a dialogue-based multiple-choice reading-comprehension dataset collected from exams of English as a foreign language. This dataset presents several challenges as it contains non-extractive answers that require commonsense reasoning beyond a single sentence.
In total, we sampled 807 dialogues from the three datasets. Each sampled dialogue has 5 to 12 utterances, and each constituent utterance has no more than 30 words.
4.2 Annotation Process
Annotation guidelines. The annotators are instructed to identify either explicit or implicit triplets in a dialogue (§3.1). Such a triplet consists of a pair of spans, say A and B, and an appropriate relation R between them, denoted as A -R B. A span is defined as a word, phrase, or a sub-sentence unit of an utterance that represents some concept such as an entity, event, action. The annotators are instructed to meet the following constraints during the annotation:
· The extracted triplets must be entailed by the conversation to be valid.
· The spans of a triplet should be as short and

concise as possible. Also, a triplet may connect a pair of spans from distinct utterances in a dialogue.
· Multiple distinct valid relations between the same pair of spans are allowed. All these relations correspond to distinct triplets.
We used a web-based tool called BRAT (Stenetorp et al., 2012) for the annotation. The annotators are three PhD students who have thorough knowledge about the task. They were first briefed about the annotation rules, followed by a trial with a few samples to evaluate their understanding of the annotation guidelines and ability to extract both explicit and implicit triplets. Although annotators extract both types, they were instructed to focus more on annotating implicit triplets since extracting those are more challenging. The trial stage was conducted to ensure that annotators are well-versed in annotating high quality triplets in the final phase.
4.3 Annotation Verification and Agreement
Each dialogue is primarily annotated by a single annotator. We then verify the validity of the annotated triplets using the following strategy:
1. All extracted triplets are independently validated by two other validation annotators, in terms of their inferability from their source dialogues.
2. Unanimously agreed-upon valid triplets are kept, while unanimously agreed-upon invalid triplets are discarded. In the case of a disagreement, we bring in a third annotator to break the tie.
3. The final set of valid triplets is labelled as being explicit or implicit by the same two annotators as in step (1). The majority vote is assigned as the final label. Similar to the previous step, in case of a disagreement, we bring in a third annotator to break the tie.
After this stage, we obtained a Cohen's Kappa intervalidation-annotator agreement of 0.91 for triplet verification and 0.93 for relation type labelling. We found that the number of explicit triplets (4.5%) in the final annotated dataset is significantly less than implicit triplets (95.5%). The reason is the informal nature of the source datasets' conversations, which enables the extraction of much more frequent implicit triplets than explicit ones. Statistics of the annotated dataset are shown in Table 2.
5 Experimental Setup and Results
We formulate three tasks on the CIDER dataset: 1) Dialogue-level Natural Language Inference; 2)

Description
# Dialogues/# triplets in DailyDialog # Dialogues/# triplets in MuTual # Dialogues/# triplets in DREAM # Dialogues/# triplets Total
# Dialogues with # triplets < 3 # Dialogues with # triplets between 3-5 # Dialogues with # triplets between 5-10 # Dialogues with # triplets > 10 Average # triplets per dialogue
# Triplets with spans from Utt. distance = 0 # Triplets with spans from Utt. distance = 1 # Triplets with spans from Utt. distance between 2-5 # Triplets with spans from Utt. distance between 6-8 # Triplets with spans from Utt. distance > 8
# Triplets having spans from same speaker # Triplets having spans from different speakers
# Span pairs with single relation # Span pairs with multiple relations

Instances
245/1286 182/658 380/2595 807/4539
142 312 281 72 5.62
1009 1490 1501 401 138
2475 2064
4203 164

Table 2: Statistics on our dataset CIDER. Please refer to the appendix for frequency statistics of the relations.

Span Extraction; 3) Multi-choice Span Selection.
5.1 Dialogue-level Cross Validation
We consider a dialogue-level cross-validation strategy to benchmark our models. We partition the annotated dialogues into five disjoint and roughly equal-sized folds. Per cross-validation round, the triplets from four folds are considered for training, and the remaining one fold is used for test.
5.2 Task 1: Dialogue-level Natural Language Inference (DNLI)
Textual entailment, later renamed as natural language inference (NLI), is the task of identifying if a "hypothesis" is true (entailment), false (contradiction), or undetermined (independent) given a "premise". We extend this definition to conversations and propose Dialogue-level Natural Language Inference (DNLI), which is the task of determining whether a triplet (hypothesis) is true or false given a dialogue (premise) (see Fig. 3a).
It should be noted that most NLI datasets such as SNLI (Bowman et al., 2015), MultiNLI (Williams et al., 2017), SciTail (Khot et al., 2018) consist of a single sentence hypothesis and premise, whereas for DNLI the hypothesis and the premise are a triplet and a conversation, respectively.
For our experiments, the hypothesis is formed by concatenating the elements of the triplet h -r t in h, r, t order. Similarly, the premise is formed by concatenating the utterances of the dialogue.
5.2.1 Creating Negative Examples Let C be a conversation, T be the set of all valid triplets in C, and A -R B be one such valid triplet in T . We denote R: set of all relations; RS: set of symmetric relations. The samples with valid triplets as hypotheses are termed as positive exam-

(a)

(b)

Figure 3: (a) Subtask 1: Dialogue-level Natural Language Inference (DNLI). (b) Subtask 2: Span Extraction.

ples. The contradicting triplets/hypotheses for the negative samples are created from T as follows:

Split Train Test

Label
Positive Negative
Positive Negative

Fold1
3627 6441
912 7425

Fold2
3630 6469
909 6876

Fold3
3631 6527
908 6989

Fold4
3630 6492
909 7061

Fold5
3638 6470
901 7187

Table 3: Cross validation fold statistics for Task 1: DNLI.

Reverse Relation Direction. In A -R B, if R / RS, then B -R A is a contradicting hypothesis. Substitute Relation Type. For A -R B, another relation Q is randomly sampled from R \ {R} and A -Q B is considered a contradicting hypothesis. Substitute Span. For A -R B, either A or B is replaced with another random span X from the other triplets in set T . X -R B or A -R X is then considered a contradicting hypothesis. Combination of All. A combination of the above three strategies can also be used to create the contradicting hypothesis. We ensure that the contrived contradicting hypotheses do not appear in the set of annotated triplets T .
The above strategies allow us to create multiple negative samples from a positive sample. In our experiments, we had two and eight negative samples per positive sample in the training and test split, respectively. We intentionally keep fewer negative samples in the training data to evaluate the generalization capacity of the models on a more diverse range of negative samples in the test data. Foldwise statistics are shown in Table 3. An example of the DNLI task is illustrated Fig. 3a.
5.2.2 Baseline
RoBERTa-large Fine-tuned on MNLI. We use the pretrained roberta-large-mnli model (Liu et al., 2019) to benchmark this task. The input to the model is: <CLS> Premise

<SEP> Hypothesis <SEP>. The classification is performed on the <CLS> token vector from the final layer. We choose this model as it has been fine-tuned on the MNLI dataset and shows impressive performance on a number of NLI tasks.
The performance of the RoBERTa-MNLI model is reported in Table 5. As DNLI is a classification task, we report macro F1, weighted F1, and precision and recall over the positive examples (with valid triplets). We notice that the metrics are quite consistent across the five different folds and thus we report our conclusion against the average score. We obtained an average weighted F1 score of 85.78%. However, the macro F1 score is noticeably lower at 69.83%, suggesting that the model performs poorly on the less-frequent positive examples. The recall score suggests that 76.85% of the valid hypotheses are correctly identified by the model. However, the precision score is quite low at 37.25%, suggesting that almost 2/3-rd of the predicted valid hypothesis are in-fact invalid. Without fine-tuning, the model produces much lower macro F1 of 17.76%, precision of 15.06%, and recall of 47.4%. The state-of-the-art RoBERTa MNLI model is thus not very capable of correctly identifying triplets entailed by the conversation. We conclude that inference from conversational context based on commonsense reasoning is not straightforward for pretrained language models.
5.3 Task 2: Span Extraction
Span Extraction is defined as identifying the tail span B, given the head span A, the relation R between A and B, and the conversation C where A -R B is encoded. It is analogous to the task of node prediction in knowledge bases, where the missing tail node B in A -R ? is to be predicted. Fig. 3b depicts an example of this subtask.
In this paper, Span Extraction is formulated as

a Machine Reading Comprehension (MRC) task similar to SQuAD (Rajpurkar et al., 2016) where a question is to be answered from a given passage of text or more generally context. The equivalencies with MRC are defined as follows: Context. The entire conversation C is treated as the context, as the span B in the triplet A -R B can come from any utterance of C. Question and Answer. For each relation type R, we create a question template that includes a placeholder for span A and asks for span B as the answer. The templates are filled with the appropriate valid triplets to generate the question-answer pairs. Please refer to the question template in appendix.
5.3.1 Baselines
We use two pretrained transformer-based models to benchmark the Span Extraction task. The methodology described in BERT QA models (Devlin et al., 2019) is used to extract the tail-spans/answers.

RoBERTa Base. We use the roberta-base model (Liu et al., 2019) as a baseline model. SpanBERT Fine-tuned on SQuAD. We use SpanBERT (Joshi et al., 2020) fine-tuned on SQuAD 2.0 dataset as the other baseline model.
5.3.2 Evaluation Metrics EM (Exact Match). % of the predicted answers that are identical to the gold answers. NM (No Match). % of the predicted answers that bear no match with the gold answer. F1: The F1 score introduced by Rajpurkar et al. (2016) to evaluate word-level overlap of predictions with the gold answers for extractive QA models.
5.3.3 Results

Model SpanBERT RoBERTa

Metric
EM NM F1
EM NM F1

Fold1
29.2 46.47 43.72
15.87 57.36 31.31

Fold2
28.35 48.71 42.27
13.18 56.71 30.83

Fold3
26.57 52.91 39.31
12.1 61.57 28.93

Fold4
31.54 47.48 44.22
15.12 53.22 34.38

Fold5
26.37 50.0 40.77
13.48 57.4 31.86

Avg.
28.41 49.11 42.06
13.95 57.25 31.46

Metric
Macro F1 Weighted F1 Precision Positive Recall Positive

Fold1
69.15 86.76 35.79 77.55

Fold2
71.07 85.48 39.18 78.54

Fold3
68.14 84.07 34.87 77.56

Fold4
71.29 86.42 39.37 78.16

Fold5
69.49 86.17 37.05 72.45

Avg.
69.83 85.78 37.25 76.85

Table 5: Results for the RoBERTa-MNLI model in Task 1: Dialogue-level Natural Language Inference (DNLI).

model is still subpar. The EM score suggests that the model extracts the exact correct answer less than 1/3-rd of the time. The NM score also indicates that the extracted answer and the actual answer have no overlap around half of the time. Without fine-tuning, the SpanBERT model produces an EM score of 7.96% and a F1 score of 20.78%, much lesser than the fine-tuned model. We conclude that the state-of-the-art pretrained language models struggle with extracting missing spans.

5.4 Task 3: Multi-choice Span Selection
Multi-choice Span Selection is motivated by the SWAG commonsense inference task (Zellers et al., 2018). In SWAG, given a partial description of a situation, the appropriate ending is to be selected from a given list of choices using commonsense inference. In our case, Multi-choice Span Selection is formulated as a multiple-choice question answering task. Similar to the previous task, given a conversation C and partial information about a triplet A -R ?, the goal is to predict the missing span B as an answer to a question created from A and R. However, in contrast to task 2, the missing span B has to be selected from a list of four possible answers S = {s1, ..., s4}. We show an example of this task in Fig. 4. The context, question, and answers for this task are created as follows:

Table 4: Results for Span Extraction task. Higher EM, F1, and lower NM scores are better.

The results for this task is reported in Table 4. We notice that the SpanBERT model performs significantly better than the RoBERTa model. This is expected as SpanBERT has been pretrained with a different objective function and it particularly excels at span extraction tasks, such as, question answering. However, the EM score of 28.41% and the F1 score of 42.06% for the superior SpanBERT

Figure 4: Subtask 3: Multi-choice Span Selection.
Context and Question: Both the context and the question construction follow §5.3. Correct and Confounding Options: The options include the target answer and the three confounding options that are extracted from the same context .

Relation Type
Attribution Causal Comparison Conditional Intentional Social Spatial Temporal

Subtask 1
74.97 67.26 68.75 68.51 70.49 58.97 79.06 71.56

Subtask 2
43.34 38.04 36.78 38.97 46.70 28.34 57.41 54.26

Subtask 3
64.64 61.20 58.76 55.72 63.34 58.00 71.20 54.53

Table 6: Average five-fold Macro-F1, F1, and Accuracy score over the relation categories. We report results for RoBERTaMNLI, SpanBERT and RoBERTa models for the three tasks.

Model BERT RoBERTa Human

Setting
C&Q Q
C&Q Q
C&Q Q

Fold1
60.35 47.21
61.16 51.05
89.90 69.39

Fold2
58.96 50.89
51.05 62.04
82.69 67.31

Fold3
51.84 51.25
65.28 56.60
83.02 60.00

Fold4
61.62 54.46
73.31 58.92
80.77 65.38

Fold5
60.55 47.84
62.04 55.76
80.78 71.15

Avg.
58.66 50.33
62.57 56.87
83.43 66.45

Table 7: Results for Multi-choice Span Selection. C&Q - model input is Context, Question; Q - input is only Question.

5.4.1 Creating Confounding Options To mitigate the stylistic artifacts that could give away the target answer (Gururangan et al., 2018; Poliak et al., 2018), the confounding options are generated in an adversarial fashion.
Generating Confounding-option Candidates. We first select a large number of spans from C to form a confounding-option collection N by leveraging the SpanBERT fine-tuned on the samples of Task 2 (§5.3). We feed each individual utterance as the context, and the question created from A and R to the SpanBERT fine-tuned for Task-2. This leads to one or two candidate answers (spans) per contextual utterance per question, averaging around 30 confounding spans per question. We discard the spans that form a valid triplet with A and R.
Adversarial Filtering. Once we have the collection N , we follow Zellers et al. (2018) to filter the confounding options generated in §5.4.1. Please check Appendix Section A for more details. We use the roberta-base model to filter out stylistic patterns. During the filtering process, discriminator prediction accuracy decreased from 0.55 to 0.27, suggesting the method's effectiveness in removing easy confounding candidates with stylistic patterns.
5.4.2 Baseline We experiment with bert-base-uncased and roberta-base on the adversarially created dataset. The input to the models is the concatenation of conversation C, question Q, and candidate answers Aj, j  {1, ..., 4}: <CLS> C <SEP> Q <SEP> A j <SEP>. Each score is predicted

from the corresponding <CLS> token vector and the highest scoring one is selected as answer.
5.4.3 Results The results reported in Table 7 indicate the importance of contextual information in improving models' performance. Our human verifiers could also predict the answers significantly more accurately when contextual information was available. It is worth noting that all the pre-trained language models perform poorly in this task and the obtained results are far from reaching the humanlevel performance. Besides, the accuracy score for bert-base-uncased and roberta-base without fine-tuning are 25.60% and 26.22% respectively which is similar to a random baseline (25.00%), confirming the conclusion in Task 2 (§5.3) that current language models have difficulties in predicting the missing span.
Performance across Relation Categories. We report the results across different relation categories for each task with the corresponding best performing models in Table 6. We notice that Spatial is one of the top-performing categories across all three tasks. Performance in Attribution and Temporal category are also reasonably well in Task 1 and Task 1, 2 respectively. Interestingly, the result of Temporal category in Task 3 is the worst. The performance in Causal and Conditional category is around the average mark across all three tasks. This implies that pretrained language models find it difficult to understand the concept of causal events or dependent events. Finally, we observe that the performance in Social category is the worst or among the worst for all the tasks, suggesting that the models find it very challenging to reason about social norms, rules, and conventions.
6 Conclusion
In this work, we introduced CIDER--a new dataset that focuses on commonsense-based implicit explanation extraction from dialogues. The dataset consists of more than 4,500 manually annotated triplets from over 800 dialogues. We also introduced dialogue-level NLI and QA tasks, along with pre-trained transformer-based baselines to evaluate their inference and reasoning capabilities.
Acknowledgements
This research is supported by A*STAR under its RIE 2020 AME programmatic grant, Award No.­ A19E2b0098.

References
So¨ren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives. 2007. Dbpedia: A nucleus for a web of open data. In The semantic web, pages 722­735. Springer.
Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. 2015. A large annotated corpus for learning natural language inference. arXiv preprint arXiv:1508.05326.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam Hruschka, and Tom Mitchell. 2010. Toward an architecture for never-ending language learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 24.
Leyang Cui, Yu Wu, Shujie Liu, Yue Zhang, and Ming Zhou. 2020. Mutual: A dataset for multi-turn dialogue reasoning. In Proceedings of the 58th Conference of the Association for Computational Linguistics. Association for Computational Linguistics.
Ernest Davis. 2014. Representations of commonsense knowledge. Morgan Kaufmann.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171­4186. Association for Computational Linguistics.
Herbert P. Grice. 1975. Logic and conversation. Speech acts, pages 41­58.
Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel R Bowman, and Noah A Smith. 2018. Annotation artifacts in natural language inference data. arXiv preprint arXiv:1803.02324.
Jena D Hwang, Chandra Bhagavatula, Ronan Le Bras, Jeff Da, Keisuke Sakaguchi, Antoine Bosselut, and Yejin Choi. 2020. Comet-atomic 2020: On symbolic and neural commonsense knowledge graphs. arXiv preprint arXiv:2010.05953.
Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. 2020. Spanbert: Improving pre-training by representing and predicting spans.
Mandar Joshi, Omer Levy, Luke Zettlemoyer, and Daniel S Weld. 2019. Bert for coreference resolution: Baselines and analysis. In Proceedings of

the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5807­5812.
Tushar Khot, Ashish Sabharwal, and Peter Clark. 2018. Scitail: A textual entailment dataset from science question answering.
Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. 2017. Dailydialog: A manually labelled multi-turn dialogue dataset. In Proceedings of the Eighth International Joint Conference on Natural Language Processing, IJCNLP 2017, Taipei, Taiwan, November 27 - December 1, 2017 - Volume 1: Long Papers, pages 986­995. Asian Federation of Natural Language Processing.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.
Ilya Loshchilov and Frank Hutter. 2018. Decoupled weight decay regularization. In International Conference on Learning Representations.
Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. 2016. A corpus and cloze evaluation for deeper understanding of commonsense stories. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 839­849, San Diego, California. Association for Computational Linguistics.
Nasrin Mostafazadeh, Aditya Kalyanpur, Lori Moon, David Buchanan, Lauren Berkowitz, Or Biran, and Jennifer Chu-Carroll. 2020. GLUCOSE: GeneraLized and COntextualized story explanations. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online.
Adam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, and Benjamin Van Durme. 2018. Hypothesis only baselines in natural language inference. arXiv preprint arXiv:1805.01042.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-totext transformer. Journal of Machine Learning Research, 21(140):1­67.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text.
Maarten Sap, Ronan Le Bras, Emily Allaway, Chandra Bhagavatula, Nicholas Lourie, Hannah Rashkin, Brendan Roof, Noah A Smith, and Yejin Choi. 2019.

Atomic: An atlas of machine commonsense for ifthen reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 3027­3035.
Robyn Speer, Joshua Chin, and Catherine Havasi. 2017. Conceptnet 5.5: An open multilingual graph of general knowledge. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 31.
Pontus Stenetorp, Sampo Pyysalo, Goran Topic´, Tomoko Ohta, Sophia Ananiadou, and Jun'ichi Tsujii. 2012. brat: a web-based tool for NLP-assisted text annotation. In Proceedings of the Demonstrations Session at EACL 2012, Avignon, France. Association for Computational Linguistics.
Kai Sun, Dian Yu, Jianshu Chen, Dong Yu, Yejin Choi, and Claire Cardie. 2019. Dream: A challenge data set and models for dialogue-based reading comprehension. Transactions of the Association for Computational Linguistics, 7:217­231.
Adina Williams, Nikita Nangia, and Samuel R Bowman. 2017. A broad-coverage challenge corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426.
Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. Swag: A large-scale adversarial dataset for grounded commonsense inference. arXiv preprint arXiv:1808.05326.

A Adversarial Filtering.
For Task 3: Multi-choice Span Selection, once we have the collection N , we follow Zellers et al. (2018) to filter the confounding options generated in an iterative fashion. We follow the procedure below:
1. Initially, We select 3 random candidates from N and the correct answer to form a fake dataset.
2. We split our fake dataset randomly into train and test set following a 1:2 ratio.
3. We used our discriminator D to filter out confounding options with unwanted stylistic patterns. Then, we train our discriminator D on the dummy train set and score each option with a probability in the dummy test set.
4. We replace the easiest confounding option (lowest probability) with another option from N .
5. We merge our dummy train set and dummy test set after replacement together to form our fake dataset for the next iteration
6. Step 2,3,4,5 is repeated until the discriminator's cross-entropy loss converges.
We designed the input feed to D as a combination of context C and relation R, specifically we feed <CLS> Conversation <SEP> Relation <SEP> Option i <SEP> as input. Here Option i means the ith option in options. The probability score is given on the final layer vector corresponding to the <CLS> token. We posit by excluding A in our model input; the model can only pick up on low-level stylistic patterns with respect to the relation R and context C while not possessing reasoning abilities. Therefore, Our model can filter solely leveraging on low-level patterns while not based on the high-level inference. We use roberta-base model to filter out stylistic patterns. During the filtering process, discriminator prediction accuracy decreased from 0.55 to 0.27, suggesting the method's effectiveness in removing easy confounding candidates with stylistic patterns.
B Additional Task
B.1 Task 4: Relation Prediction
The fourth task of our interest is Relation Prediction between two spans from a conversation. Given two spans A and B from a conversation C, the task is to predict the unknown relation R between them in A -? B.

We propose two different settings to evaluate the relation prediction task: 1) Without Conversational Context and 2) With Conversational Context.
B.1.1 Task Description Without Conversational Context. This setting is similar to the standard relation prediction task in knowledge graphs. Given the input spans (A, B), the task is to predict the relation R between A and B.
With Conversational Context. We surmise that the conversational context from C is key to predict relation between any two given spans. This task setting is thus designed to evaluate that hypothesis. In this case, given the input spans and the conversation -- (A, B, C), the task is to predict the commonsense relation R between A and B.
B.1.2 Models We use pretrained transformer based models to benchmark this task as well. In particular, we used the bert-base and the roberta-base models. The input for the models is formulated as follows -- <CLS> A <SEP> B <SEP> in the without conversational context setting, and <CLS> A <SEP> B <SEP> C <SEP> in the with conversational context setting. The relation category R is classified from the final layer vector corresponding to the <CLS> token.

BERT W/ Context

RoBERTa W/O Context W/ Context

Metric Fold1 Fold2 Fold3 Fold4 Fold5 Avg.

Accuracy Precision Recall F1

35.37 20.2 17.01 16.93

34.70 18.68 19.30 18.2

36.33 15.99 16.58 15.73

37.43 16.43 16.33 16.03

35.13 15.16 16.14 15.15

35.79 17.29 17.07 16.41

Accuracy Precision Recall F1 Accuracy Precision Recall F1

49.55 24.1 26.71 24.44 39.46 17.00 18.51 16.29

51.60 29.88 31.32 29.91 41.32 19.72 17.22 18.26

49.09 24.51 29.51 25.64 36.33 14.44 15.90 13.52

53.01 26.42 25.21 25.41 40.39 16.77 14.36 15.28

48.11 25.34 28.43 25.49 39.49 15.99 16.27 14.77

50.27 26.05 28.24 26.18 39.40 16.78 16.45 15.62

Table 8: Results for Task 2: Relation Prediction. All precision, recall and F1 scores are macro level measures.

B.1.3 Results
The results for the relation prediction task is shown in Table 8. We report accuracy and other macro level scores in Table 8. We observe that the macro level scores are quite sub-par partly due to the fact that we have a lot of relations in the annotated dataset. It is also to be noticed that the incorporation of context brings a large improvement across

all the evaluation metrics. The results support our hypothesis that contextual information is substantially important in predicting the relation between spans.
C Hyperparameters
We use the AdamW (Loshchilov and Hutter, 2018) optimizer to train the models for all the tasks. More details about learning rate, batch size and epochs are given below.
C.1 Hyperparameters for Task 1: NLI
The roberta-large-mnli model is trained with a learning rate of 1e-5 and batch size of 8 for 10 epochs.
C.2 Hyperparameters for Task 2: Span Extraction
The roberta-base and span-bert model are both trained with a learning rate of 1e-5 and batch size of 16 for 12 epochs.
C.3 Hyperparameters for Task 3: Multi-choice Span Selection
Generating Confounding-Option Candidates. We used SpanBERT fine-tuned on SQUAD2.0 dataset, we trained using learning rate of 1e-5 and batch size of 16 for 20 epochs.
Adversarial Filtering. We split dummy train and test portion randomly by using 2/3rd of dataset as train and 1/3rd of dataset as test. Every iteration, we only replace the option with lowest output score with other candidates. We continued for around 35 iteration before the loss converges. We fine-tuned roberta-base model with learning rate of 5e-5, batch size of 16 and 3 epochs.
Answer Prediction. In the C&Q set up, We trained bert-base with learning rate of 5e-5 and batch size of 16 for 10 epochs. We trained roberta-base with learning rate of 1e-5 and batch size of 48 for 20 epochs. In the Q set up, we used learning rate of 5e-5 and batch size of 16 for 3 epochs for bert-base. For roberta-base, we used learning rate of 1e-5 and batch size of 16 for 3 epochs.

Category
Attribution
Causal
Comparison Conditional Intentional Social Spatial Temporal Total

Relation
Capable Of Depends On Has A Has Property Has Subevent Is A Manner Of NotHasProperty NotIsA
Causes Causes Desire Implies NotCauses NotCauseDesire NotImplies
Antonym Distinct From Similar To Synonym
Has Prerequisite
Desires Motivated By Goal Obstructed By Used For NotMotivatedByGoal
Social Rule
At Location Located Near Before Happens On Simultaneous

Instances
25 16 41 284 58 227 70 21 13
1126 459 339
24 12 14
30 27 33 28
308
22 366 249 180
12
96
187 10
124 106
32
4539

Category Total
755
1974
118 308 829 96 197 262 4539

Table 9: Frequency of annotated relations in the dataset.

D Relation Count
The frequency of the categorized relations in the final annotated dataset is shown in Table 9.
E Question Template

Category Relation

Question

Attribution

Capable Of Depends On Has A Has Property Has Subevent Is A Manner Of

What is X capable of? What does X depend on? What does X have? What property does X have? What subevent does X have? What is X? What is X a manner of?

Causal

Causes Causes Desire Implies

What does X cause? What desire is caused by X? What is implied by X?

Comparison

Antonym Distinct From Similar To Synonym

What is an antonym of X? What is X distinct from? What is X similar to? What is a synonym of X?

Conditional Has Prerequisite What prerequisite does X have?

Intentional

Desires Motivated By Goal Obstructed By Used For

What does X desire? Which goal motivates the act/action X? What is X obstructed by? What is X used for?

Social

Social Rule

What is X the social norm for?

Spatial

At Location Located Near

Where is X located? What is X located near?

Temporal

Before Happens On Simultaneous

What happens after X? When does X happen? What does X cooccur with?

Table 10: Question template in Task 2 and 3 for the various relations; X is the placeholder for head span A.

C.4 Hyperparameters for Task 4: Relation Prediction
For both bert-base and roberta-base, we used learning rate of 2e-5 and batch size of 32 for 40 epochs.

The question template used in Task 2: Span Extraction and Task 3: Multi-choice Span Selection is shown in Table 10. The placeholder X in the Question column is replaced with the actual annotated span A.

