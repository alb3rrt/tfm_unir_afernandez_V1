arXiv:2106.01834v1 [cs.LG] 3 Jun 2021

Continual Learning in Deep Networks: an Analysis of the Last Layer
Timothée Lesort, Thomas George, and Irina Rish
Université de Montréal, MILA - Quebec AI Institute
Abstract We study how different output layer types of a deep neural network learn and forget in continual learning settings. We describe the three factors affecting catastrophic forgetting in the output layer: (1) weights modifications, (2) interferences, and (3) projection drift. Our goal is to provide more insights into how different types of output layers can address (1) and (2). We also propose potential solutions and evaluate them on several benchmarks. We show that the best-performing output layer type depends on the data distribution drifts or the amount of data available. In particular, in some cases where a standard linear layer would fail, it is sufficient to change the parametrization and get significantly better performance while still training with SGD. Our results and analysis shed light on the dynamics of the output layer in continual learning scenarios and help select the best-suited output layer for a given scenario.
1 Introduction
Continual deep learning algorithms usually rely on end-to-end training of deep neural networks, making them difficult to analyze given the complexity (non-linearity, non-convexity) of the training dynamics. In this work, we instead propose to evaluate the ability of a network to learn or forget in a simplified learning setting: We decompose our model as a feature extractor part consisting of all but the final layer, and a classifier part which is given by the output layer. Using this decomposition, we can isolate the role of the output layer parameterization in continual learning scenarios.
Recent works on continual learning point out some drawbacks of parameterizing the output layer as a linear layer, especially in incremental settings [52, 56, 19]. Among these problems are the unbalance of bias and/or norm of the output layer's vectors. These works propose some solutions but do not study them independently from the feature extractor. Indeed, in continual learning scenarios, the function learned by the feature extractor changes, and therefore the embedding space changes accordingly; we call this change the projection drift. This drift characterizes the change through time of the embedding on a given observation.
By isolating the output layer from the feature extractor, we can study it in a controlled environment, i.e. an environment without projection drifts. Decoupling the feature extractor from a sub-network has shown to be useful, for example, in reinforcement learning [29, 37, 48].
We now list our contributions:
· We propose an evaluation of a large panel of output layer types in continual scenarios: we review the capacity of each layer to learn continually or from a subset of samples.
· We describe the different sources of performance decrease in continual learning for the output layer: forgetting, interferences, and projection drifts.
· We review and propose different solutions to address catastrophic forgetting in the output layer. In particular, we introduce a simplified weight normalization layer, two masking strategies, and an alternative to Nearest Mean Classifier using median vectors.
2 Related Works
In most deep continual learning papers, the last layer of the deep neural network is implemented as a regular linear output layer as in typical classifier architectures in i.i.d. settings such as VGG [47], ResNet [18], and GoogleNet [49]. Recently, several continual learning approaches questioned this approach, showing that the norm or the bias can be unbalanced for the last classes observed in class-incremental settings. For example, BIC [52] proposed to train two additional parameters using a subset of the training dataset after training the rest of the model to correct
1

for the unbalance of the bias. In the same spirit, [56] proposed to compute a Weight Aligning value to apply to vector to balance the norm of past classes vectors and the new classes vectors of a linear layer. [56] also experiments with a layer, the weight normalization layer [43], that decouples the norm of the weight matrix from its direction in order to normalize the vector of linear output layers. [19, 5] proposed to apply a cosine normalization of the layer to avoid the norm and bias unbalance.
Linear Discriminant Analysis (LDA) Classifiers in incremental settings have been used for dozens of years [1, 8, 11]. The streaming version SLDA from [45], has recently been revisited in deep continual learning in [17]. SLDA uses a combination of the mean and the covariance of the training data features to classify new observations. A more straightforward approach, as implemented in iCaRL [39] is to average the features for each class and apply a nearest neighbor classifier also called Nearest Mean Classifier (NMC).
In this work, we study the capability of those various types of top layers with a fixed pre-trained model in incremental and lifelong settings as defined in [27].
Continual learning research field studied many strategies, such as rehearsal [26, 9, 2, 3, 52, 19, 6], generative replay [46, 53, 28, 30, 40], dynamic architecture [16, 42, 32] or regularization [22, 55, 41, 44]. In this study, we do not aim at modifying the training process or proposing a new one. We study how different types of layers can learn in various continual scenarios without a continual learning approach.

3 Output Layer Types
We here review the output layer parameterizations studied in this work. We introduce the notations, and we use them to present the main characteristics of linear layers. Then, we explain how standard layer parameterization can struggle to learn in a continual learning scenario, and we present different parameterizations to overcome the problems. Finally, we present several other types of output layers mainly based on nearest neighbor classifiers.

3.1 Notations
We will study functions f(·) parameterized by a vector of parameters  representing the set of weight matrices and bias vectors of a deep network. In continual learning, the goal is to train f(·) on a sequence of task [T0, T1, ..., TT -1], such that (xt, yt)  Tt (t  [0, T - 1]), f(x) = y.
We can decompose f(·) into two parts, (1) a feature extractor - (·) which encodes the observation xt into a feature vector zt, (2) an output layer g+ (·) which transforms the feature vector into a non-normalized vector ot (the logits). - and + are two complementary subsets of . ot is then used to make new predictions on unseen samples, by taking the argmax.

3.2 Linear Layer

A linear layer is parameterized by a weight matrix A and bias vector b, respectively of size N × h and N , where h is the size of the latent vector (the activations of the penultimate layer) and N is the number of classes. For z a latent vector the output layer computes the operation o = Az + b. We can formulate this operation for a single class i with z, Ai + bi = oi, where · is the euclidean scalar product, Ai is the ith row of the weight matrix viewed as a vector and bi is the corresponding scalar bias. It can be rewritten:

z Ai · cos((z, Ai)) + bi = oi

(1)

Where (·, ·) is the angle between two vectors and · denotes here the euclidean norm of a vector. Eq. 1 highlights the 3 components that need to be learned in order for the network to make a correct prediction
y^ = argmaxi(oi): The norm of Ai, its angle with the latent representation, and the bias. In particular, if the training dynamics encourage a particular Ai to be larger than others, then the network will only predict the class i. While this problem is not present in an i.i.d setting where all parameters are learned using all classes, in incremental learning the vectors Ai are learned sequentially. This induces an effet where past classes will continuously see their norm Ai pushed to 0, while classes present in the task will see their norm Ai grow, which renders impossible to correctly predict past tasks classes.

3.3 Linear classifiers in continual learning and reparameterizations
In continual learning, there are two main cases of data distribution drift: either we get new data of known classes (domain drift - lifelong scenario ) or receive new data from new classes (virtual concept drift - incremental scenario).
For a linear output layer, these scenarios do not forget in the same way. In incremental learning, the vectors Ai are learned sequentially (one by one or set by set). Hence, when learning a new Ai, we can not check if it does not interfere with past classes (because they are not available simultaneously). Indeed, suppose two classes are similar

2

in the latent space. In that case, their respective vector in the output layer might be similar, and interferences might happen between the two classes (as illustrated in Fig.3). Interference might be due to forgetting but not exclusively. One solution found in the bibliography to avoid interference is to counteract unbalance between the

Figure 1: Illustration of norm and bias unbalance at the end of a CIFAR10 continual experiment. 5 tasks with 2 classes each. (left) the norm of each output vector at the end of the last task (middle) bias at the end if the last task (right) the difference between last task's bias and current bias. We can see a clear unbalance in norm and bias for the last active vectors (classes 8 and 9). The right figure show us that the imbalance of bias is largely due to the modification of bias from the previous task (classes 6 and 7).

norms of vectors or bias. Hence, we can modify eq. 1 in several was to try to avoid such unbalance: Removing the

bias (Linear_no_bias layer):

z Ai · cos((zt, Ai)) = oi

(2)

Normalizing output vectors (WeightNorm layer):

z · cos((zt, Ai)) = oi

(3)

Measuring only the angle (CosLayer ):

cos((z, Ai)) = oi

(4)

We can note that eq. 3 is similar to the original weight-norm (denoted here Original WeightNorm) from [43] and

experimented in [56]. However, the original weight norm uses two additional parameters: a gamma parameter 

and a bias, which are both learned.

 z · cos((zt, Ai)) + b = oi

(5)

We can note that the  parameter is similar to the parameters of the batch-normalization [20], which has been shown to hurt training of intermediate layers in continual learning [34]. Hence, to avoid similar problems, we propose to use the formula eq. 3 to perform a weight-normalization and have a unit norm for all vectors.
Another type of perturbation can affect past classes, which is the gradient computed on observation from new classes. The gradient can force the output vectors from past classes to change their weights to minimize the loss on the current. In the bibliography, the regularization strategies such as [22, 41] aims at avoiding such forgetting of important weights. However, recently [31, 23] showed that regularization were not sufficient for incremental scenarios. In this paper, we propose to apply a new strategy to avoid that the update on a specific class affects

(a) Linear

(b) Linear Masked

Figure 2: Illustration of Forgetting: we plot in this figure the difference between first 50 weights after task 44 and after task 45 in the scenario Core10Mix. The task 45 is composed of one class different from 44's one. Those two figures illustrate the impact of masking. Indeed it avoid the modifying/forgetting of weights of other classes.

another past one. The strategy consists of masking some classes for back-propagation to apply the gradient only on a subset of classes. We propose two types of masking, single masking, we apply the gradient only on one output vector (the one from the good class) and the group masking, when we learn with a batch we mask all classes that are not in the batch. The idea is that in continual learning with the following strategies, the gradient descent can not change the norm and bias of past classes and potentially avoid the unbalance we can see in a classical linear layer. Our experiment tested masking on the linear layer and its variants presented earlier in this section. In Figure 2, we illustrate the impact of single masking in a task with only one class from one experiment of the paper.
The masking process is similar to multi-head architecture [50] for gradient descent. The gradient is computed only for a subset of selected classes; however, there is only one head, and the inference is achieved without the help of any external supervision, such as a task label.

3

(a) Angles between vectors

(b) Mean Angles vectors-data.

(c) Interference Risks

Figure 3: Illustration of Interference risks: we analysis a linear layer and the risk of interferences. In those 3 figures we plot the angles (in degree) between the output vectors (left), the mean angle between the data of each class and the vectors of the output layers (middle) and a visual figure that highlights the risk of interferences (right). For this last figure, we compute the ratio between the values in the diagonal of middle figure and the values of the same row, higher ratio is worse. We compare it to weightnorm in appendix A.3

In lifelong learning, forgetting happens when new instances of a known classes make the output layer forget features that were important for past instances or modify the direction of the output vectors. However, those problems are usually less prone to lead to catastrophic forgetting but may still have a significant impact in complex scenarios. One of the reasons that catastrophic forgetting is less important is that all classes are available simultaneously, which makes it possible for the output vectors to be coordinated and avoid interferences.

3.4 Learning without gradient

To avoid the interferences and forgetting due to gradient descent, we can use another type of layer that relies on

proximity between training latent vectors and testing latent vectors.

One of the most popular classifiers that can easily be adapted into a continual or online learning setting is the

k-nearest neighbor (KNN) classifier. This classifier searches for the k nearest exemplars of a given observation in

the training set and predicts the class based on the class of the k neighbors. This classifier is parameterized by k

and by the distance used (here the usual euclidean norm).

A lighter strategy is the nearest mean classifier (denoted MeanLayer ) as proposed in iCarL [39], this strategy

consists of saving the mean of the features of each class k while learning (called a prototype µk) and applying a

1-NN classifier to predict. This layer is denoted as MeanLayer in this paper.

In this paper, we propose to evaluate the performance of a median classifier, where we replace the mean of the

features with the median of the features. The median vector is computed by selecting the median value on each

dimension. The idea behind this approach is that the prototype might be more central in the feature distribution

and maybe avoid interference with similar classes more effectively. This layer is denoted as MedianLayer in this

paper.

An alternative is the linear discriminant analysis classifier, which uses both the mean of the features and a

covariance. The online version of this algorithm is streaming linear discriminant analysis (SLDA) [45]. We used the

approach with online updates of the covariance matrix of [10] as in [17]. This approach stores one mean vector per

class µk with a counter ck  R as well as a covariance matrix of the same size as A (common to all classes). The

covariance matrix's update is done with:

t+1

=

tt + t t+1

(6)

Where t is:

t

=

t(zt - µk)2 t+1

(7)

For prediction, SLDA use a precision matrix  = [(1 - ) + I]-1, where = 10-4 is called the shrinkage parameter

and I  Rd×d is the identity matrix.

Then

two

vectors

w

and

b

are

computed

wk

=

µk ,

bk

=

-

1 2

(µk

µk )

And finally: ok = zt, wk + bk

This type of classifier has been found very effective in the literature. However, it cannot be used to train the

feature extractor. Hence, they need to store samples from past tasks as rehearsal strategies or be applied in a

setting where we ignore the projection drift.

In this section, we introduced the different output layer types that we will use in experiments. We introduce three modifications: (1) a simplified weightnorm (referred to as WeightNorm), (2) two masking strategies, single

4

masking and group masking, compatible with any reparametrization of the linear layer, and (3) a variant of mean layer (Nearest Mean Classifier) that use the median vector instead of the mean.
4 Experiments
In these experiments, we will review how the different output layer types, presented in section 3, can solve various continual scenarios. We isolate the output layer from the remaining of the neural network by using a frozen pre-trained model. This methodology consists of creating a scenario where no projection drift occurs. Our findings can potentially be transferred to end-to-end training scenarios (with projection drifts). Indeed, projection drifts may only lead to a deterioration of performance. Hence our study can eliminate approaches that would, in all cases, fail under such conditions. Our findings can also be directly transferred to settings where the projection drift is negligible. We evaluate the various parameterizations of the linear layer and the layers that are not trained by gradient descent.
In a preliminary experiment, we benchmark all output layer types with all data in an i.i.d. setting. The goal was to select optimal pre-trained models and a learning rate value. The description of this experiment is in appendix A.1 and the results in Table 3 but the conclusion of those experiments are presented in section 4.1. In a second setup, we train the various output layers in continual learning scenarios. Finally, we experiment how training with only a subset of data affects those layers.
4.1 Datasets and pre-trained Models
We conduct our experiments on CIFAR10/CIFAR100 (MIT License) [24] and Core50 (CC BY 4.0 Licence) [33] datasets . Core50 is a dataset composed of 50 objects that can be gathered into 10 categories of objects (e.g. glasses, phone, cup...), we denote Core10 the 10 classes version, and Core50 the 50 classes version. We use both to create various types of scenarios. In Core50, all objects are presented in 8 training domains (backgrounds) and 3 eval domains.
The preliminary experiments consist of testing the basic output layer (without masking) to select a learning rate (among 0.1, 0.01 and 0.001 ) and architecture for pre-trained models (among ResNet, VGG16 and GoogleNet available on the torchvision library [35]). Following the results (appendix A.1), we choose a learning rate of 0.01 for the original linear layer and original linear layer without bias (Linear_no_bias) as well as for their masked counterpart and 0.1 for the other layers. Secondly, we found that for Core50 and Core10 experiments, the ResNet model is the best. We also eliminate the CIFAR100 dataset from our continual experiments because the pre-trained model we tested was not suited for the dataset. We kept CIFAR10, Core10, and Core50 for the following experiments. Moreover, we have seen that 5 epochs per task are sufficient to learn a correct solution on these datasets, and we will keep this number of epochs for all experiments.
4.2 Continual Experiments Settings
In these experiments, we evaluate the capacity of every output layer to learn continually with a fixed feature extractor. We evaluate the continual learning performance on virtual concept drifts (incremental scenario), on domain drifts (lifelong scenario) as described in [27]. Incremental settings consists of a sequence of tasks where all new tasks bring new unknown classes. Lifelong settings consists of a sequence of tasks where each new task brings new examples of known classes. In the end, we also evaluate the best layers on a mixed scenario with both kinds of drifts. All our experiments are achieved in the single head framework [14]. We do not use task labels while training nor testing. The change in labels can signal a change of tasks. However, it does not trigger any continual mechanisms in our experiments and all training are done with a simple stochastic gradient descent with a momentum of 0.9.
Incremental Scenarios: CIFAR10, Core50: Incremental scenarios are characterized by a virtual concept drift between two tasks. These settings evaluate the capacity of learning incremental new classes and distinguishing them from the others. The core50 scenario is composed of 10 tasks; each of them is composed of 5 classes. The CIFAR10 scenario is composed of 5 tasks with 2 classes each.
Lifelong Scenario: Core10Lifelong: Lifelong settings are settings that are characterized by a domain drift between two tasks. The classes stay the same, but the instances change. These settings evaluate the capacity of improving at classifying with new data. The scenarios we use are composed of 8 tasks; each task contains all classes in a given domain, each new task, we visit a new domain.
Mixed Scenario: Core10Mix: In this scenario, a new task is triggered by either a virtual concept drift or a domain drift. In this case, the domain drift is characterized by the transition from one object to another in the same category. This scenario is composed of 50 tasks, where each task correspond to a new object (with category annotation) in all its training domain. So a new task is always a new object, but it is either data from a new class or from a known class visited through another object.
5

The results for those experiments are in section 5.1. For easy reproducibility, we put in appendix A.5, how the scenarios are created with the continuum library [13].
4.3 Subset Experiments Settings
In many continual learning approaches, a subset of the training is saved, either for rehearsal purposes or for validation purposes [4, 12, 36]. In this experiment, we evaluate the capacity of each layer to be trained from scratch with such subsets. It could be more efficient to continually train the feature extractor in a continual end-to-end setting and learn a suitable out layer afterward from a subset of data. This procedure is very fast, and it avoids all problems that projection drifts or any other drift can create. The only (not so simple) problem to solve is data selection and their amount. Nevertheless, the successes in few-shot learning [25, 15, 51] or fast adaptation [7] show that when the feature space is good enough, only a small set of data is enough to identify a class.
5 Results
5.1 Continual Experiments
We split the results of continual experiments into two group of figures; the first groups only compares the various parameterizations of a linear layer presented in section 3.3. The second group compares the best performing layers of the first group to layers that are not trained by gradient descent, namely: KNN, MeanLayer, MedianLayer and SLDA.

(a) CIFAR10, 5 tasks

(b) Core10Lifelong, 8 tasks

(c) Core50, 10 tasks

Figure 4: Experiments on CIFAR10, Core50, and Core10Lifelong on 8 different task orders. We plot the test accuracy on the full test set for each epoch. We compare the different parameterizations of the linear layer. Vertical lines represent task transition. The red vertical line represents the end of the first training epoch.

In the first group of experiments (Fig. 4), we can see that in the lifelong experiment (Fig. 4b), only three layer types completely failed. We can note that all of them were masked layers, but on the other hand, the Coslayer Masked was good performing. In such context, all classes are available simultaneously; hence masking can prevent a good organization of output vectors together and make the training unstable. Note that we removed the layer masked by group in the lifelong experiment because it is useless when all classes are available at the same time. In the incremental experiments (Fig. 4a and Fig. 4c), the layer that successfully learn continually are the layer that use only the angles for inferences and not the norm nor the bias: namely WeightNorm and Coslayer or masked layer. We can note the the original weight normalization (OriginalWeightNorm) was not performing well as expected (c.f. section 3.3). The simplified WeightNorm is among the best performing solution for Cifar10 experiment and the best for Core50 experiment. We can also see that the masking procedure we proposed in this paper is always the best performing solution in incremental scenarios. A tendency that emerges from these experiments is that all layers are compatible with lifelong learning. In incremental learning, the layers that rely only on angles between data and output vectors are more performant. The masking strategy (single and group) seems to be efficient for linear and linear no bias (they are superposed in figure 4c). Howevers in some cases, it might create some instability, notably in lifelong training. For the later experiments we will keep WeightNorm and Coslayer and their masked counterparts. We found in all our experiments that removing the bias from the linear layer has almost no impact on results. One explanation for this result could be that data are far from the origin 0 and, therefore, the bias has no impact on it. Indeed, the mean norm of the latent vectors is 26.1 with ResNet architecture, while the bias values of the output layer are usually in a range of [-1.0, 1.0] as in figure 1. For the second group of experiments, we only keep WeightNorm and Coslayer and their masked variants to compare them to other layers.

6

(a) CIFAR10 split, 5 tasks

(b) Core10Lifelong, 8 tasks

(c) Core50, 10 tasks

(d) Core10Mix, 50 tasks
Figure 5: Comparison between layers trained by gradient descent and layers trained without gradients. Experiments on CIFAR10, Core50, Core10Lifelong and Core10Mix on 8 different task orders (cf Appendix A.4). We plot the test accuracy on the full test set for each epochs.
The second group of experiments (Fig. 5), shows that the best performing layers of the first group perform less well than layer types trained without gradient descent. Nevertheless, we should not forget that those layers are not differentiable with respect to the input; hence they cannot be used to train a full deep network in an end-to-end fashion. They should be associated with another layer to train the feature extractor as in iCARL [39] or with a pre-trained models. This model can be pre-trained on classification tasks, or in an unsupervised [38] or meta-learning [21] pre-training.
The results of the Core10Mix (described in Section 4) show us that SLDA is the best performing solution even in mixed scenarios. Indeed, in the end, SLDA, but also MeanLayer, MedianLayer, and KNN are not dependant on the task order but just on the full content of data. These characteristics make them strong in continual learning, at least when the projection drift can be ignored. On the other hand, we can see that CosLayer Masked and WeightNorm performed quite well in this challenging setting without any regularization or replay process. As seen in previous experiences, the masking seems to benefit Coslayer but not WeightNorm. Both of them only rely on the angle between data and output vectors. The only difference is that in WeightNorm, the gradient is weighted by the norm of the data, which may change the learning dynamic. This experience on 50 tasks with 8 different tasks shows us that layers relying only on the angle between data and output vectors are very promising for continual learning. Moreover, it shows that masking might help the CosLayer greatly. It also shows that layers trained without gradient descent are very effective, especially SLDA.
5.2 Training with a Subset
In these experiments, we randomly select a sub-sample of the training dataset, and we train the output layer with this subset. The results of these experiments are gathered in Table 1. The goal is to measure how various layers can learn with a low amount of data. Among the different parameterization of the linear layers, we can see that the original linear layer, its variant without bias, CosLayer Masked, Original WeightNorm and the WeightNorm layer are very well-performing. On the other hand, except for CosLayer, we can see that masked approaches are not performing well in such settings. The masking seems to be bad for many layers. Among the other layers, we can see that the SLDA approach, which was very well performing in continual settings, works less well than KNN, median, and mean layers with few data points. The median layer works quite well in our experiment but shows no real advantages toward the mean layer; moreover, the mean layer is easier to compute online. Maybe in a dataset with many outliers, the median layer would show some advantages toward the mean layer but not in our experiments.
These experiments show another time the asset of the proposed weight norm layer and masking in a continual
7

Table 1: Subset experiments: Mean Accuracy and standard deviation on 8 runs with different seeds.

OutLayer Linear
Linear-no-bias CosLayer
WeightNorm OriginalWeightNorm
Linear-Masked Linear-no-bias-Masked
CosLayer-Masked WeightNorm-Masked OriginalWeightNorm-Masked
KNN SLDA MeanLayer MedianLayer

Dataset \ Subset Core50 Core50 Core50 Core50 Core50 Core50 Core50 Core50 Core50 Core50 Core50 Core50 Core50 Core50

100 54.13±15.37 54.14±15.37 18.74±14.40 54.29±16.14 54.79±15.61 11.49±5.44 11.51±5.41 47.46±12.87 15.77±3.76 16.79±4.65 42.77±12.63 40.74±21.55 49.87±15.33 43.77±13.50

200 55.97±16.23 55.97±16.23 23.03±12.79 56.10±16.58 56.23±15.92 13.35±6.25 13.32±6.25 48.33±13.63 19.80±4.66 19.12±5.51 43.22±13.23 36.41±24.11 50.86±15.28 45.15±13.57

500 57.05±16.74 57.05±16.75 24.58±18.01 56.91±17.40 57.57±17.07 20.05±23.75 20.04±23.66 47.21±12.45 24.63±21.56 16.57±5.82 44.51±14.65 39.01±26.69 52.06±16.68 46.59±16.08

1000 58.60±15.56 58.60±15.56 25.05±17.66 58.49±16.18 59.04±15.33 20.34±23.59 20.37±23.58 49.27±11.61 20.90±6.31 17.00±5.71 46.19±14.67 42.37±24.83 53.93±15.01 47.81±15.12

All 59.04±15.71 59.04±15.71 29.84±15.44 58.98±16.21 59.66±15.42 13.40±3.00 13.43±3.00 48.75±11.74 20.42±4.71 21.50±4.80 46.56±13.95 45.40±24.72 54.03±14.78 45.07±13.38

related setting. On the other hand, the approaches like MeanLayer, KNN, and MedianLayer also seem a good alternative.
6 Discussion and Limitations
In this paper, we studied and proposed various approaches that can help to avoid catastrophic forgetting in the output layer. We have seen that layers that do not rely on a bias or a norm to make a prediction can better avoid interference, such as the Coslayer or the proposed variation of WeigthNorm; On the other hand, masking avoids forgetting by preventing the modification weights of certain classes. Finally, the last cause of catastrophic forgetting is projection drift. Unfortunately, it can take many forms both in incremental and lifelong settings, and there is no general way to address it. Some approaches from other papers aim at reducing it [5] or compensating it as in [54]. In this paper, we used a frozen pre-trained model to avoid any projection drift. However, even if it was convenient for evaluation purposes in this paper to ignore projection drift, it stays fundamental to be able to train a feature extractor continually and to continue research in this direction.
The good results of almost all approaches in the lifelong scenario are probably due to the similarity of the different domains of Core50. The results show that all layers can learn continually in such context, but the results could be significantly different in scenarios where domains are very inconsistent. Nevertheless, we can assume that the environment does not change significantly through time in many lifelong scenarios, e.g., a fixed video camera that learns to count cars and bikes. Therefore, under such conditions, our results stay valid.
This paper experiments on data of objects and animals which might have no direct negative societal impacts. On the other hand, the study aims to make progress in understanding continual learning but can, a priori, not be directly applied in malicious applications. The amount of computation needed in this study is quite significant, with more than 16 days of GPU use to produce presented (and not presented) results. However, we believe that this amount of computation was necessary to test the various output layers and make a fair comparison.
7 Conclusion
In this paper, we conduct an empirical evaluation of the various output layers of deep neural networks. This evaluation is the first to be conducted where output layers are evaluated independently from feature extractor training. It gives us clear insights into how output layers learn continually. We also showed how different data distribution drift might affect the output layers differently in lifelong and incremental settings.
We describe three different factors that might cause catastrophic forgetting in a linear output layer in a continual classification task. (1) The modification of important weight (forgetting), (2) The interferences between output vectors, and (3) the projection drift: the feature extractor learn new features that change the representation space and may make the output vectors unsuitable.
We review different methods that aim to address the issues (1) and (2), and we also proposed a simplified version of weight norm and a masking strategy that improves the baselines. Our results show that some output layer types can learn continually when there is no projection drift with stochastic gradient training. This study can be helpful for continual strategies in general but especially for few-shot continual learning, continual learning with pre-trained network, or transfer between unsupervised pre-training and continual learning.
Future work would study the combination of layer types with a continual training approach such as replay, regularization, and testing the training with another optimizer that standard stochastic gradient descent to confirm

8

assets and pitfalls.
References
[1] Y. Aliyari Ghassabeh, F. Rudzicz, and H. A. Moghaddam. Fast incremental lda feature extraction. Pattern Recognition, 48(6):1999­2012, 2015.
[2] R. Aljundi, L. , E. Belilovsky, M. Caccia, M. Lin, L. Charlin, and T. Tuytelaars. Online continual learning with maximal interfered retrieval. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 11849­11860. Curran Associates, Inc., 2019.
[3] E. Belouadah and A. Popescu. Deesil: Deep-shallow incremental learning. In Proceedings of the European Conference on Computer Vision (ECCV), pages 0­0, 2018.
[4] P. Buzzega, M. Boschini, A. Porrello, D. Abati, and S. Calderara. Dark experience for general continual learning: a strong, simple baseline. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems 33, pages 15920­15930. Curran Associates, Inc., 2020.
[5] L. Caccia, R. Aljundi, T. Tuytelaars, J. Pineau, and E. Belilovsky. Reducing representation drift in online continual learning. arXiv preprint arXiv:2104.05025, 2021.
[6] L. Caccia, E. Belilovsky, M. Caccia, and J. Pineau. Online learned continual compression with adaptative quantization module. arXiv preprint arXiv:1911.08019, 2019.
[7] M. Caccia, P. Rodriguez, O. Ostapenko, F. Normandin, M. Lin, L. Caccia, I. Laradji, I. Rish, A. Lacoste, D. Vazquez, and L. Charlin. Online fast adaptation and knowledge accumulation: a new approach to continual learning. NeurIPS, 2020.
[8] C. Chatterjee and V. Roychowdhury. On self-organizing algorithms and networks for class-separability features. IEEE Transactions on Neural Networks, 8(3):663­678, 1997.
[9] A. Chaudhry, M. Ranzato, M. Rohrbach, and M. Elhoseiny. Efficient lifelong learning with a-gem. In ICLR, 2019.
[10] S. Dasgupta and D. Hsu. On-line estimation with the multivariate gaussian distribution. In International Conference on Computational Learning Theory, pages 278­292. Springer, 2007.
[11] G. Demir and K. Ozmehmet. Online local learning algorithms for linear discriminant analysis. Pattern Recognition Letters, 26(4):421­431, 2005. ICAPR 2003.
[12] A. Douillard, M. Cord, C. Ollion, T. Robert, and E. Valle. Podnet: Pooled outputs distillation for small-tasks incremental learning. In Proceedings of the IEEE European Conference on Computer Vision (ECCV), 2020.
[13] A. Douillard and T. Lesort. Continuum: Simple management of complex continual learning scenarios, 2021.
[14] S. Farquhar and Y. Gal. Towards robust evaluations of continual learning. arXiv preprint arXiv:1805.09733, 2018.
[15] L. Fei-Fei, R. Fergus, and P. Perona. One-shot learning of object categories. IEEE transactions on pattern analysis and machine intelligence, 28(4):594­611, 2006.
[16] C. Fernando, D. Banarse, C. Blundell, Y. Zwols, D. Ha, A. A. Rusu, A. Pritzel, and D. Wierstra. Pathnet: Evolution channels gradient descent in super neural networks. CoRR, abs/1701.08734, 2017.
[17] T. L. Hayes and C. Kanan. Lifelong machine learning with deep streaming linear discriminant analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 220­221, 2020.
[18] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770­778, 2016.
[19] S. Hou, X. Pan, C. C. Loy, Z. Wang, and D. Lin. Learning a unified classifier incrementally via rebalancing. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
9

[20] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pages 448­456. PMLR, 2015.
[21] K. Javed and M. White. Meta-learning representations for continual learning. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 1818­1828. Curran Associates, Inc., 2019.
[22] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proc. of the national academy of sciences, 2017.
[23] J. Knoblauch, H. Husain, and T. Diethe. Optimal continual learning has perfect memory and is np-hard. arXiv preprint arXiv:2006.05188, 2020.
[24] A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Technical Report, 2009.
[25] B. Lake, R. Salakhutdinov, J. Gross, and J. Tenenbaum. One shot learning of simple visual concepts. In Proceedings of the Annual Meeting of the Cognitive Science Society, volume 33, 2011.
[26] T. Lesort. Continual learning: Tackling catastrophic forgetting in deep neural networks with replay processes, 2020.
[27] T. Lesort, M. Caccia, and I. Rish. Understanding continual learning settings with data distribution drift analysis. arXiv preprint arXiv:2104.01678, 2021.
[28] T. Lesort, H. Caselles-Dupré, M. Garcia-Ortiz, J.-F. Goudou, and D. Filliat. Generative models from the perspective of continual learning. In IJCNN - International Joint Conference on Neural Networks, Budapest, Hungary, Jul 2019.
[29] T. Lesort, N. Díaz-Rodríguez, J.-F. Goudou, and D. Filliat. State representation learning for control: An overview. Neural Networks, 2018.
[30] T. Lesort, A. Gepperth, A. Stoian, and D. Filliat. Marginal replay vs conditional replay for continual learning. In International Conference on Artificial Neural Networks, pages 466­480. Springer, 2019.
[31] T. Lesort, A. Stoian, and D. Filliat. Regularization shortcomings for continual learning. arXiv preprint arXiv:1912.03049, 2019.
[32] Z. Li and D. Hoiem. Learning without forgetting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017.
[33] V. Lomonaco and D. Maltoni. CORe50: a New Dataset and Benchmark for Continuous Object Recognition. In S. Levine, V. Vanhoucke, and K. Goldberg, editors, Proceedings of the 1st Annual Conference on Robot Learning, volume 78 of Proceedings of Machine Learning Research, pages 17­26. PMLR, 13­15 Nov 2017.
[34] V. Lomonaco, D. Maltoni, and L. Pellegrini. Rehearsal-free continual learning over small non-iid batches. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 989­998. IEEE Computer Society, 2020.
[35] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlché Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 8024­8035. Curran Associates, Inc., 2019.
[36] A. Prabhu, P. H. Torr, and P. K. Dokania. Gdumb: A simple approach that questions our progress in continual learning. In European Conference on Computer Vision, pages 524­540. Springer, 2020.
[37] A. Raffin, A. Hill, K. R. Traoré, T. Lesort, N. Díaz-Rodríguez, and D. Filliat. Decoupling feature extraction from policy learning: assessing benefits of state representation learning in goal based robotics. Workshop on "Structure and Priors in Reinforcement Learning" (SPiRL) at ICLR, 2019.
[38] D. Rao, F. Visin, A. A. Rusu, Y. W. Teh, R. Pascanu, and R. Hadsell. Continual unsupervised representation learning, 2019.
[39] S.-A. Rebuffi, A. Kolesnikov, G. Sperl, and C. H. Lampert. icarl: Incremental classifier and representation learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2001­2010, 2017.
10

[40] A. Rios and L. Itti. Closed-loop memory gan for continual learning. In Proceedings of the 28th International Joint Conference on Artificial Intelligence, IJCAI'19, pages 3332­3338. AAAI Press, 2019.
[41] H. Ritter, A. Botev, and D. Barber. Online structured laplace approximations for overcoming catastrophic forgetting. In Advances in Neural Information Processing Systems, pages 3738­3748, 2018.
[42] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, and R. Hadsell. Progressive neural networks. ArXiv e-prints, jun 2016.
[43] T. Salimans and D. P. Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. arXiv preprint arXiv:1602.07868, 2016.
[44] J. Schwarz, J. Luketina, W. M. Czarnecki, A. Grabska-Barwinska, Y. W. Teh, R. Pascanu, and R. Hadsell. Progress & compress: A scalable framework for continual learning. In ICML, 2018.
[45] Shaoning Pang, S. Ozawa, and N. Kasabov. Incremental linear discriminant analysis for classification of data streams. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 35(5):905­914, 2005.
[46] H. Shin, J. K. Lee, J. Kim, and J. Kim. Continual learning with deep generative replay. In Advances in Neural Information Processing Systems, pages 2990­2999, 2017.
[47] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.
[48] A. Stooke, K. Lee, P. Abbeel, and M. Laskin. Decoupling representation learning from reinforcement learning. arXiv preprint arXiv:2009.08319, 2020.
[49] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions, 2014.
[50] G. M. van de Ven and A. S. Tolias. Three scenarios for continual learning. arXiv preprint arXiv:1904.07734, 2019.
[51] W. Wang, V. W. Zheng, H. Yu, and C. Miao. A survey of zero-shot learning: Settings, methods, and applications. ACM Trans. Intell. Syst. Technol., 10(2):13:1­13:37, Jan. 2019.
[52] Y. Wu, Y. Chen, L. Wang, Y. Ye, Z. Liu, Y. Guo, and Y. Fu. Large scale incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 374­382, 2019.
[53] Y. Wu, Y. Chen, L. Wang, Y. Ye, Z. Liu, Y. Guo, Z. Zhang, and Y. Fu. Incremental classifier learning with generative adversarial networks. CoRR, abs/1802.00853, 2018.
[54] L. Yu, B. Twardowski, X. Liu, L. Herranz, K. Wang, Y. Cheng, S. Jui, and J. v. d. Weijer. Semantic drift compensation for class-incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6982­6991, 2020.
[55] F. Zenke, B. Poole, and S. Ganguli. Continual learning through synaptic intelligence. In D. Precup and Y. W. Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 3987­3995, International Convention Centre, Sydney, Australia, 06­11 Aug 2017. PMLR.
[56] B. Zhao, X. Xiao, G. Gan, B. Zhang, and S.-T. Xia. Maintaining discrimination and fairness in class incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13208­13217, 2020.
11

A Appendix
A.1 Preliminary Experiments
A.1.1 Preliminary Experiments Setting
The preliminary experiments are designed to see how the different output layers work without continual learning constraints. This experiment is also conducted to select the learning rate and architecture for further experiments.
We experimented with CIFAR10 and CIFAR100 datasets, and we use resnet20 pre-trained model from here 1 (BSD 3-Clause License). The main idea is to compare those results with our results. We take the frozen pre-trained model and replace the output layer with a new one we train on all data. (1) We experiment with training on CIFAR10 with a neural network pre-trained on CIFAR100 and the opposite. (2) We also trained on CIFAR10 with the model pre-trained on CIFAR10 to see if we would recover the original accuracy, and we did the same for CIFAR100. Doing continual learning on a dataset with a model pre-trained on this dataset is not recommended since it means having access to the whole data from the beginning of training. We did not use (2) setting in the further experiments, and it was just used as a reference.
(3) We also experiment also with Core50 dataset [33]. We used models pre-trained on ImageNet available on the torchvision library [35]. We used VGG16, GoogleNet and Resnet, which have a latent space of dimension 2048, 1024, and 512. We experimented with the Core50 dataset, Core50 setting with 50 classes corresponding to the object id, and Core10 setting with 10 classes corresponding to the object category.
In this preliminary experiments, we evaluate all layers without their masking variant to verify that the pre-trained models were suited to the continual learning scenarios.
A.1.2 Preliminary Experiments
The summary of the preliminary results is reported in Table 2. The full results can be found in Table 3. As described in section A.1.1, we experiment with the simplest output layer for this preliminary experiment, i.e., linear layer, linear layer without bias (Linear_NB), CosLayer, WeightNorm layer, Original WeightNorm (OWeightNorm), KNN, SLDA, and MeanLayer.
The results of those preliminary experiments are that globally the learning rate of 0.1 is adapted to all settings. Secondly, we found that for Core50 and Core10 experiments, the ResNet model was the best suited. Moreover, we see that 5 epochs per task are sufficient to learn a correct solution. Concerning CIFAR experiments, the reported top-1 accuracy on CIFAR10 and CIFAR100 of the original pre-trained models are respectively 92.60%, 69.83%. The settings CIFAR10 pre-trained on CIFAR10 (same for CIFAR100) show us that the training procedure can recover almost the best performance. In CIFAR100, there is a drop of 6% of accuracy, but it stays reasonable in a 100-way classification task.
Even if not very surprising, one interesting result is that training on CIFAR100 with a model pre-trained on CIFAR10 does not work at all. It illustrates that using a pre-trained model is not always a solution in machine learning or continual learning. They should be selected carefully, even if the data might look similar to CIFAR10 and CIFAR100. On the other hand, the experiment on CIFAR10 with CIFAR100 models shows a significant decrease in the performance for all layers.
These preliminary experiments make us able to fix some hyper-parameters to study all layers in a common setting. It also offers us interesting insight into the drop of performance in simple experiments when using a pre-trained model instead of training end-to-end. The results of this experiment can be used as a baseline for continual experiments and subsets experiments.

1https://github.com/chenyaofo/pytorch-cifar-models

Table 2: Preliminary experiments to select learning rate and architectures with existing type of layers. Experiment realized on 8 seeds [0, 1, 2, 3, 4, 5, 6, 7].

Dataset

LR Architecture Pretraining Linear Linear_NB CosLayer WeightNorm OWeightNorm KNN

SLDA MeanLayer

CIFAR10 0.1

resnet

CIFAR10 92.55±0.02 92.55±0.02 92.49±0.05 92.56±0.03

92.55±0.04

91.79±0.26 92.37±0.00 92.44±0.00

CIFAR10 0.1

resnet

CIFAR100 65.37±2.74 68.58±9.55 66.55±9.83 68.55±0.21

69.21±0.36

60.26±0.28 66.90±0.02 63.27±0.00

CIFAR100 0.1

resnet

CIFAR10 17.32±0.23 17.14±0.22 13.09±0.22 15.11±0.22

16.88±0.30

14.20±0.17 26.44±0.03 14.48±0.00

CIFAR100 0.1

resnet

CIFAR100 63.97±0.10 63.94±0.11 60.91±0.24 63.62±0.09

63.71±0.15

60.69±0.12 61.56±0.01 61.84±0.00

Core50

0.1

resnet

ImageNet 76.58±0.41 76.57±0.41 53.93±0.90 77.04±0.33

76.85±0.61

65.50±0.14 78.55±0.03 71.51±0.00

Core50

0.1

vgg

ImageNet 71.69±0.50 71.44±0.30 61.13±0.39 72.13±0.45

71.53±0.23

56.86±0.08 69.98±0.03 66.67±0.00

Core50

0.1 googlenet ImageNet 70.13±0.27 70.13±0.27 47.81±3.26 66.37±0.74

69.27±0.35

59.45±0.11 70.60±0.11 60.85±0.15

Core10Lifelong 0.1

resnet

ImageNet 85.45±0.51 85.62±0.58 78.76±0.36 86.29±0.29

86.07±0.51

75.71±1.50 88.06±0.01 79.61±0.00

Core10Lifelong 0.1

vgg

ImageNet 85.52±0.57 85.34±0.57 79.39±0.18 85.95±0.61

85.56±0.83

75.14±0.88 83.69±0.03 78.55±0.00

Core10Lifelong 0.1 googlenet ImageNet 85.08±0.47 85.08±0.47 78.23±0.33 83.05±0.38

84.97±0.29

77.57±0.96 86.33±0.18 76.92±0.10

12

Table 3: Preliminary experiments to select learning rate and architectures with existing type of layers

Dataset CIFAR10 CIFAR10 CIFAR10 CIFAR10 CIFAR10 CIFAR10 CIFAR10 CIFAR10 CIFAR100 CIFAR100 CIFAR100 CIFAR100 CIFAR100 CIFAR100 CIFAR100 CIFAR100
Core50 Core50 Core50 Core50 Core50 Core50 Core50 Core50 Core50 Core50 Core50 Core50 Core10Lifelong Core10Lifelong Core10Lifelong Core10Lifelong Core10Lifelong Core10Lifelong Core10Lifelong Core10Lifelong Core10Lifelong Core10Lifelong Core10Lifelong Core10Lifelong

LR 0.1 0.01 0.001 n/a 0.1 0.01 0.001 n/a 0.1 0.01 0.001 n/a 0.1 0.01 0.001 n/a 0.1 0.01 0.001 n/a 0.1 0.01 0.001 n/a 0.1 0.01 0.001 n/a 0.1 0.01 0.001 n/a 0.1 0.01 0.001 n/a 0.1 0.01 0.001 n/a

Architecture resnet resnet resnet resnet resnet resnet resnet resnet resnet resnet resnet resnet resnet resnet resnet resnet resnet resnet resnet resnet vgg vgg vgg vgg
googlenet googlenet googlenet googlenet
resnet resnet resnet resnet
vgg vgg vgg vgg googlenet googlenet googlenet googlenet

Pretraining CIFAR10 CIFAR10 CIFAR10 CIFAR10 CIFAR100 CIFAR100 CIFAR100 CIFAR100 CIFAR10 CIFAR10 CIFAR10 CIFAR10 CIFAR100 CIFAR100 CIFAR100 CIFAR100 ImageNet ImageNet ImageNet ImageNet ImageNet ImageNet ImageNet ImageNet ImageNet ImageNet ImageNet ImageNet ImageNet ImageNet ImageNet ImageNet ImageNet ImageNet ImageNet ImageNet ImageNet ImageNet ImageNet ImageNet

Linear 92.55±0.02 92.59±0.04 92.49±0.04
65.37±2.74 75.13±10.09 73.28±11.11
17.32±0.23 14.86±0.20 6.71±0.48
63.97±0.10 63.70±0.18 54.58±0.32
76.58±0.41 76.29±0.34 62.64±0.72
71.69±0.50 72.29±0.28 68.79±0.32
70.13±0.27 65.99±0.50 49.61±0.85
85.45±0.51 86.19±0.33 82.53±0.52
85.52±0.57 85.85±0.30 84.20±0.14
85.08±0.47 83.67±0.26 79.51±0.36
-

Linear_NB 92.55±0.02 92.59±0.03 92.49±0.04
68.58±9.55 75.13±10.08 73.30±11.10
17.14±0.22 14.86±0.18 6.71±0.48
63.94±0.11 63.71±0.19 54.54±0.32
76.57±0.41 76.29±0.34 62.65±0.72
71.44±0.30 72.29±0.28 68.78±0.31
70.13±0.27 65.99±0.50 49.62±0.85
85.62±0.58 86.19±0.33 82.53±0.52
85.34±0.57 85.89±0.27 84.20±0.14
85.08±0.47 83.67±0.25 79.52±0.35
-

CosLayer 92.49±0.05 92.48±0.03 67.03±10.29
66.55±9.83 66.46±15.03 18.85±12.54
13.09±0.22 3.02±0.43 1.22±0.33
60.91±0.24 6.95±1.19 1.29±0.25
53.93±0.90 5.80±0.89 2.21±0.56
61.13±0.39 27.15±1.66 3.47±0.79
47.81±3.26 8.71±1.31 2.55±0.64
78.76±0.36 55.26±1.84 13.38±3.59
79.39±0.18 72.44±1.92 24.59±3.17
78.23±0.33 64.27±2.53 14.85±2.09
-

WeightNorm 92.56±0.03 92.54±0.02 92.39±0.10 68.55±0.21 68.43±0.08 60.71±0.41 15.11±0.22 12.18±0.28 2.40±0.37 63.62±0.09 62.45±0.26 21.35±1.20 77.04±0.33 73.01±0.51 30.55±1.05 72.13±0.45 71.43±0.35 59.54±0.43 66.37±0.74 61.77±0.21 21.91±1.17 86.29±0.29 85.46±0.40 76.42±1.03 85.95±0.61 85.37±0.24 82.14±0.47 83.05±0.38 82.31±0.28 73.90±0.63 -

OWeightNorm 92.55±0.04 92.60±0.03 92.57±0.04 69.21±0.36 69.27±0.25 67.79±0.14 16.88±0.30 15.20±0.23 6.77±0.44 63.71±0.15 63.96±0.11 55.54±0.48 76.85±0.61 76.98±0.29 63.42±0.66 71.53±0.23 71.91±0.37 69.06±0.32 69.27±0.35 66.65±0.52 49.76±0.66 86.07±0.51 86.44±0.36 83.41±0.45 85.56±0.83 85.74±0.65 84.51±0.11 84.97±0.29 83.97±0.33 80.13±0.30 -

KNN -
91.79±0.26 -
60.26±0.28 -
14.20±0.17 -
60.69±0.12 -
65.50±0.14 -
56.86±0.08 -
59.45±0.11 -
75.71±1.50 -
75.14±0.88 -
77.57±0.96

SLDA -
92.37±0.00 -
66.90±0.02 -
26.44±0.03 -
61.56±0.01 -
78.55±0.03 -
69.98±0.03 -
70.60±0.11 -
88.06±0.01 -
83.69±0.03 -
86.33±0.18

MeanLayer -
92.44±0.00 -
63.27±0.00 -
14.48±0.00 -
61.84±0.00 -
71.51±0.00 -
66.67±0.00 -
60.85±0.15 -
79.61±0.00 -
78.55±0.00 -
76.92±0.10

A.2 Full subsets results

13

Table 4: Subset experiments: Mean Accuracy and standard deviation on 8 runs with different seeds.

OutLayer Linear
Linear-no-bias CosLayer
WeightNorm OriginalWeightNorm
Linear-Masked Linear-no-bias-Masked
CosLayer-Masked WeightNorm-Masked OriginalWeightNorm-Masked
KNN SLDA MeanLayer MedianLayer Linear Linear-no-bias CosLayer WeightNorm OriginalWeightNorm Linear-Masked Linear-no-bias-Masked CosLayer-Masked WeightNorm-Masked OriginalWeightNorm-Masked KNN SLDA MeanLayer MedianLayer Linear Linear-no-bias CosLayer WeightNorm OriginalWeightNorm Linear-Masked Linear-no-bias-Masked CosLayer-Masked WeightNorm-Masked OriginalWeightNorm-Masked KNN SLDA MeanLayer MedianLayer

Dataset \ Subset CIFAR10 CIFAR10 CIFAR10 CIFAR10 CIFAR10 CIFAR10 CIFAR10 CIFAR10 CIFAR10 CIFAR10 CIFAR10 CIFAR10 CIFAR10 CIFAR10 Core50 Core50 Core50 Core50 Core50 Core50 Core50 Core50 Core50 Core50 Core50 Core50 Core50 Core50
Core10Lifelong Core10Lifelong Core10Lifelong Core10Lifelong Core10Lifelong Core10Lifelong Core10Lifelong Core10Lifelong Core10Lifelong Core10Lifelong Core10Lifelong Core10Lifelong Core10Lifelong Core10Lifelong

100 57.80±7.66 57.83±7.68 44.59±13.58 59.50±6.88 56.30±7.92 28.69±3.79 28.89±3.71 58.90±3.23 39.61±3.17 25.08±7.74 51.95±5.59 63.95±17.45 67.28±14.76 66.77±16.44 54.13±15.37 54.14±15.37 18.74±14.40 54.29±16.14 54.79±15.61 11.49±5.44 11.51±5.41 47.46±12.87 15.77±3.76 16.79±4.65 42.77±12.63 40.74±21.55 49.87±15.33 43.77±13.50 77.79±6.58 77.79±6.58 53.41±18.17 78.34±6.70 77.31±7.05 27.65±9.67 27.32±9.49 72.79±5.24 51.13±6.49 28.62±9.71 68.04±7.47 66.05±13.93 73.17±5.43 70.03±5.60

200 58.72±6.59 58.70±6.61 46.23±10.28 60.30±6.27 57.35±7.40 25.10±7.47 25.12±7.47 58.97±3.96 39.15±3.26 15.87±5.22 52.82±5.81 64.80±17.68 67.92±14.46 62.67±12.67 55.97±16.23 55.97±16.23 23.03±12.79 56.10±16.58 56.23±15.92 13.35±6.25 13.32±6.25 48.33±13.63 19.80±4.66 19.12±5.51 43.22±13.23 36.41±24.11 50.86±15.28 45.15±13.57 79.74±4.63 79.74±4.63 55.63±14.13 80.20±4.75 79.45±4.70 28.75±3.07 28.41±2.52 74.82±3.66 52.39±7.57 29.84±6.83 69.11±8.86 62.86±17.05 74.85±4.01 71.38±5.10

500 62.94±12.86 62.95±12.85 49.07±11.52 61.21±6.55 59.66±6.50 38.30±22.92 38.26±22.99 58.98±4.49 41.71±1.89 27.34±10.61 53.11±5.77 59.65±16.42 63.01±11.95 65.97±17.20 57.05±16.74 57.05±16.75 24.58±18.01 56.91±17.40 57.57±17.07 20.05±23.75 20.04±23.66 47.21±12.45 24.63±21.56 16.57±5.82 44.51±14.65 39.01±26.69 52.06±16.68 46.59±16.08 79.21±6.05 79.21±6.05 53.53±20.38 79.30±6.16 78.89±5.95 34.78±22.17 34.85±22.09 73.32±6.08 55.22±14.10 33.70±10.60 68.05±8.23 62.56±19.28 73.79±6.37 70.94±7.15

1000 62.38±13.22 62.38±13.20 51.54±18.17 61.20±6.25 57.57±8.24 36.54±23.19 36.74±23.12 63.83±12.02 40.19±3.30 27.18±4.15 54.02±5.09 62.03±14.87 64.02±11.20 67.20±16.32 58.60±15.56 58.60±15.56 25.05±17.66 58.49±16.18 59.04±15.33 20.34±23.59 20.37±23.58 49.27±11.61 20.90±6.31 17.00±5.71 46.19±14.67 42.37±24.83 53.93±15.01 47.81±15.12 78.36±8.18 78.36±8.18 48.55±19.89 78.95±8.17 77.45±8.87 31.93±22.81 31.59±22.94 72.65±7.07 54.49±15.97 31.09±6.83 69.19±9.69 63.64±18.93 73.20±7.26 70.37±8.44

All 60.91±6.50 60.88±6.51 48.24±10.82 62.11±5.66 61.44±6.67 22.50±6.77 22.64±6.84 59.56±3.21 39.22±3.79 17.21±5.56 54.21±4.84 66.14±17.20 68.18±14.32 69.22±16.66 59.04±15.71 59.04±15.71 29.84±15.44 58.98±16.21 59.66±15.42 13.40±3.00 13.43±3.00 48.75±11.74 20.42±4.71 21.50±4.80 46.56±13.95 45.40±24.72 54.03±14.78 45.07±13.38 79.76±6.49 79.76±6.49 59.93±17.01 80.08±6.74 79.08±6.72 28.56±12.65 28.66±12.72 74.90±4.82 49.63±9.15 31.49±6.65 69.88±7.43 68.91±15.80 75.55±5.03 72.53±5.17

14

A.3 Visualizations

(a) Linear: Angles between the differ-(b) Linear: Mean Angles between the (c) Linear: Interference Risks

ent vectors of the output layer

vectors and data class by class.

(d) WeightNorm: Angles between the (e) WeightNorm: Mean Angles be- (f) WeightNorm: Interference Risks different vectors of the output layer tween the vectors and data class by
class.

15

from continuum.scenarios import create_subscenario import numpy as np # we suppose scenario , num_tasks and seed already set np . random . seed ( seed ) task_order = np.arange(num_tasks) scenario = create_subscenario(scenario , task_order) (a) Code to change task order of the original scenario. We used seeds [0, 1, 2, 3, 4, 5, 6, 7]. Note: for seed 0, we use original order without modification.
A.4 Task orders A.5 Scenario reproducibilty
16

from continuum.datasets import Core50 from continuum import ClassIncremental dataset = dataset = Core50("./datasets",
download=True , train=train) scenario = ClassIncremental(dataset , nb_tasks=10 , transformations=transform)
(a) Code for Core50 scenario from continuum.datasets import Core50 from continuum import ContinualScenario dataset = Core50("./datasets",
scenario="domains", classification="category", train=train) scenario = ContinualScenario(dataset ,
nb_tasks=8, transformations=transform)
(b) Code for Core10Lifelong scenario from continuum.datasets import Core50 from continuum import ContinualScenario dataset = Core50("./datasets",
scenario="objects", classification="category", train=train) scenario = ContinualScenario(dataset ,
nb_tasks=50 , transformations=transform)
(c) Code for Core10Mix scenario Figure 8: Code to reproduce the scenarios used in the paper with continuum library.
17

