DECOUPLING SHAPE AND DENSITY FOR LIVER LESION SYNTHESIS USING CONDITIONAL GENERATIVE ADVERSARIAL NETWORKS
Dario Augusto Borges Oliveira
IBM Research Rua Tutoia 1157, Vila Mariana, Sao Paulo, Brazil

arXiv:2106.00629v1 [eess.IV] 1 Jun 2021

ABSTRACT
Lesion synthesis received much attention with the rise of efficient generative models for augmenting training data, drawing lesion evolution scenarios, or aiding expert training. The quality and diversity of synthesized data are highly dependent on the annotated data used to train the models, which not rarely struggle to derive very different yet realistic samples from the training ones. That adds an inherent bias to lesion segmentation algorithms and limits synthesizing lesion evolution scenarios efficiently. This paper presents a method for decoupling shape and density for liver lesion synthesis, creating a framework that allows straight-forwardly driving the synthesis. We offer qualitative results that show the synthesis control by modifying shape and density individually, and quantitative results that demonstrate that embedding the density information in the generator model helps to increase lesion segmentation performance compared to using the shape solely.
Index Terms-- Liver lesion synthesis, Generative adversarial networks, Controllable synthesis
1. INTRODUCTION
Early detection and efficient monitoring are crucial for treatment success for different kinds of cancer, and the visual inspection of CT images is one of the typical supporting procedures. Automated liver lesions identification in CT exams is especially challenging due to the high variability of shapes, densities, locations, and heterogeneity observed [1, 2, 3]. In this context, efficient synthesis tools might serve as a tool for creating synthetic yet realistic lesion evolution scenarios or deriving a varied set of samples for training more robust lesion segmentation and classification models.
An extensive list of approaches has been proposed for lesion synthesis, as covered in different reviews for lesion synthesis found in the literature [1, 4]. The current state-of-art for lesion synthesis often use generative adversarial networks (GANs), as reported in [5, 6]. One of the challenges observed in lesion synthesis is how to increase the synthesis control to derive lesions with desired shapes, densities, or structures.

This paper proposes a method for decoupling shape and density for liver lesion synthesis, allowing straight-forward synthesis control. Our results illustrate the control mechanism by modifying shape and density individually and demonstrate that the synthesized data can help increasing lesion segmentation performance on the publicly available LIST dataset in comparison to using only shape information, which indirectly points to more efficient synthesis.
2. METHODOLOGY
Our method implements shape and density decoupling for liver lesion synthesis by proposing a modification in the generator of conditional GANs, as exposed in the following subsections.
2.1. Lesion Synthesis Using Conditional GANs
Generative Adversarial Networks (GANs) are composed of two networks: a generator (G) that synthesizes images y, and a discriminator (D) that learns to classify a given image as synthetic or real. Such networks are trained in an adversarial scheme: while G learns to produce realistic images to fool D, D tries to discriminate images generated by G from real images. Formally, given any data distribution pdata(x), the generator G learns a distribution pmodel(w) such that the discriminator can hardly distinguish between samples coming from pdata(x) and pmodel(w).
Conditional GANs (cGANs) are an extension of GANs where the input to the discriminator consists of samples from two domains (x and y), and the generator synthesizes samples from one of those domains (say y). The loss function for conditional GANs is expressed by Equation 1.

LcGAN (G, D) = Ex,ypdata(x,y)[logD(x, y)]

(1)

+Exp(x),zp(z) [log(1 - D(x, G(x, z))]

The solution to Equation 1 is implemented by training the generator G and discriminator D alternately. The discriminator training uses images produced by the last trained generator and real ones. The generator is trained using the outcome of

Fig. 1. For decoupling shape and density, we propose to create a branch for handling densities and connecting it to the original U-Net architecture using dense layers. The synthesis outcome will depend not only on the shape information but also on the densities.

the previously trained discriminator and learns to synthesize realistic images. At the end of several training cycles, the generator is supposedly capable of producing images that the discriminator cannot distinguish from real ones.
2.2. Generator Modification
A widespread use of cGANs for lesion synthesis consists of merely using a given mask and train the generator to create characteristic lesion textures respecting the mask shape [7]. Here, to enable controlling the density and shape of lesion synthesis, we propose modifying the U-Net Pix2Pix generator architecture and embed a branch for handling an input histogram of densities. We expect this branch acts as an encoder or densities and structures that would enable creating different lesion appearances for the same shape and derive more diverse sample synthesis.
Figure 1 depicts a regular U-Net generator architecture and our proposed modification with a branch for densities connected to the mask decoder using dense layers. One can notice the regular U-Net with encode blocks, consisting of convolutional with stride of two, batch normalization and leaky linear unit activation, followed by decoder blocks with skip connections, consisting of upsampling, convolutional layers, batch normalization and leaky linear unit activation. In our modified version, a histogram represents the densities with 100 bins, which is fed into a dense layer with 100 nodes. We flatten the last U-Net decode block output, connect to a dense layer that is concatenated to the histogram dense layer, which is finally connected to a dense layer with 128x128 nodes reshaped into a 128x128 matrix and fed to the output convolutional layer.
In this generator, input samples consist of a data pair: an input mask that encodes the lesion shape and a histogram of

Fig. 2. Lesion synthesis results. The image presents three different shapes, represented by the top row, and four different patterns for densities, represented by the first column. It is possible to observe that the synthesis follows the input shapes and more interestingly, that modifying the histogram impacts the lesion appearance.
densities that encodes the densities observed in the synthesized lesion. In training mode, an input segmented lesion is decomposed in a mask and a histogram of densities within the mask, and the network learns to reconstruct the original lesion out of these input data. In testing mode, modifying the shape or the histogram will enable synthesizing different lesions accordingly.
3. EXPERIMENTS AND DISCUSSION
For running our experiments, we used the public challenge LITS dataset [8], which sets 100 exams for training and validation, and a testing set with 30 exams used with no modification for evaluating the semantic segmentation network performances. The training set comprises a total of around 10.000 liver CT slices and 4.400 samples of lesion CT slices.
We report our results from two perspectives: a visual inspection of how the manipulation of shape and density impacts the lesion synthesis and a quantitative assessment of the improvement in a segmentation network's performance when we add the density information to the synthesis compared to merely using the shape.

3.1. Lesion Synthesis
We trained two models for lesion synthesis: a baseline using a regular Pix2Pix to synthesize mask into lesions, and our modified model to synthesize masks with density histograms into lesions. For training them, we run 150 epochs using Adam optimizer with an initial learning rate of 0.0002 and a momentum of 0.5. The parameters for the generator loss function were set to GAN weight=1 and L1 weight=100. The parameters for loss functions were set as the default proposed in the paper [9].
We combined different input masks and histograms and fed into our trained model to demonstrate how the synthesis reacts to different shapes and histograms. The results are depicted in Figure 2. We can observe that the network respects the input masks shapes in all examples, and more interestingly, the variation observed in the histograms derive different realistic lesion patterns. They are not a mere histogram transformation, and the network learned to create textures for more homogeneous or heterogeneous lesions. In the first, second, and third rows, one can observe a simple variation in the average density, and in the fourth row, we notice that creating more modals in the histogram enables creating more complex lesion structures in the synthesized sample.
This framing allows controlling lesion synthesis in two ways: modifying the synthetic lesion shape by adjusting the input mask and modifying the lesion appearance by changing the input histogram of densities.
3.2. Lesion Segmentation
To better understand the implications and possible impacts of this framing, we trained models using the popular PSPNet [10] multi-scale semantic segmentation architecture to evaluate different training set configurations. First, we used the original LITS dataset to set a ceiling performance value. Then, we derived two synthetic datasets, one using the regular U-Net mask to lesion schema, and the other using our approach. The datasets are created by implanting lesions in existing healthy liver CT slices, up to the same amount available in the original training data, as in [11]. That allows us to compare the performance of models trained using synthesized data from regular Pix2Pix, our proposed approach, and the ceiling performance achieved using real data. We selected lesions shapes and histogram of densities at random. We also rotated and scaled the lesion at random and picked a random coordinate for implanting the lesion.
The impact of the more diverse training set created using our approach into the segmentation network performance was encouraging and points to the benefit of controlling the lesion appearance using the histogram of densities. We observed an increase in performance of around 6% in F1-score compared to using only the mask (40.13% versus 34.09%) with a regular U-Net generator setup, and a training set using only synthetic data derived 67% of the performance observed when using

Table 1. Liver lesion segmentation F1 score for different training sets, using only synthetic data for synthesis from mask, synthesis from mask+density, and using only original data.
PSP F1-Score

Original Data Pix2Pix Synthesis from Mask Pix2Pix Synthesis from Mask+Density

0.5996 0.3409 0.4013

real data. This result points to a more diverse lesion dataset using density information for controlling the synthesis.
A critical remark is that our focus in this paper is to study the impact of decoupling shape and density for lesion synthesis control. With that in mind, we envisage that exploring different models for synthesis would potentially derive better quality lesion synthesis outcome, and we also understand that other models for liver lesion segmentation deliver better results than the ones observed in our baseline. Still, we consider the models used adequate for evaluating our core contribution.

4. CONCLUSIONS
This paper presents a method for improving lesion synthesis control by decoupling lesion shape and density using conditional adversarial networks. We proposed modifying the popular Pix2Pix architecture generator and created a branch for handling densities in addition to the branch for the widespread mask-to-lesion synthesis.
We presented two different sets of results for evaluating our model: one for the visual inspection of synthesis control and the other for the numerical impact in performance considering a commonly used semantic segmentation network. First, we showed that our model could create synthetic lesions consistent with the input shape and densities, represented by a lesion mask and a histogram of densities, respectively. Variations on the densities enable more or less dense lesion synthesis and complex and heterogeneous lesion synthesis when different modals are observed in the histogram.
Then, we trained semantic segmentation networks using different training sets: one with original data, which should deliver the ceiling performance, and two using only synthetic data considering the original Pix2Pix setup for synthesis from masks, and our design that also considers the densities. We reported a gain of 6% in performance in our model, which points to a more diverse training set.
As further research, we intend to explore more robust models for data synthesis and synthesize 3D samples to verify if the findings will be consistent with those observed in this study.

Fig. 3. Lesion implant procedure in healthy liver CT slices. Each synthesized lesion is rotated and scaled at random and blended to the liver parenchyma.

5. REFERENCES
[1] Christoph Baur, Shadi Albarqouni, and Nassir Navab, "Generating highly realistic images of skin lesions with gans," in OR 2.0 Context-Aware Operating Theaters, Computer Assisted Robotic Endoscopy, Clinical ImageBased Procedures, and Skin Image Analysis, Cham, 2018, pp. 260­267, Springer International Publishing.
[2] Dario A. B. Oliveira, Raul Q. Feitosa, and Mauro M. Correia, "Automatic couinaud liver and veins segmentation from ct images," in BIOSIGNALS. International Society for Optics and Photonics, 2008, pp. 249 ­ 252, SPIE.
[3] Da´rio AB Oliveira, Raul Q Feitosa, and Mauro M Correia, "Genetic adaptation of level sets parameters for medical imaging segmentation," in Biomedical image analysis and machine learning technologies: applications and techniques, pp. 150­166. IGI Global, 2010.
[4] Changhee Han, Yoshiro Kitamura, Akira Kudo, Akimichi Ichinose, Leonardo Rundo, Yujiro Furukawa, Kazuki Umemoto, Hideki Nakayama, and Yuanzhong Li, "Synthesizing diverse lung nodules wherever massively: 3d multi-conditional gan-based CT image augmentation for object detection," CoRR, vol. abs/1906.04962, 2019.

[6] Xin Yi, Ekta Walia, and Paul Babyn, "Generative adversarial network in medical imaging: A review," Medical Image Analysis, vol. 58, pp. 101552, 2019.
[7] Kumar Abhishek and G. Hamarneh, "Mask2lesion: Mask-constrained adversarial skin lesion image synthesis," in SASHIMI@MICCAI, 2019.
[8] Patrick Bilic and Patrick Ferdinand Christ et al, "The liver tumor segmentation benchmark (lits)," CoRR, vol. abs/1901.04056, 2019.
[9] P. Isola, J. Zhu, T. Zhou, and A. A. Efros, "Imageto-image translation with conditional adversarial networks," in 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 5967­5976.
[10] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, "Pyramid scene parsing network," in 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 6230­6239.
[11] Dario A. B. Oliveira, "Implanting synthetic lesions for improving liver lesion segmentation in ct exams," ArXiv, vol. abs/2008.04690, 2020.

[5] Karim Armanious, Chenming Jiang, Marc Fischer, Thomas Koestner, Tobias Hepp, Konstantin Nikolaou, Sergios Gatidis, and Bin Yang, "Medgan: Medical image translation using gans," Computerized Medical Imaging and Graphics, vol. 79, pp. 101684, 2020.

