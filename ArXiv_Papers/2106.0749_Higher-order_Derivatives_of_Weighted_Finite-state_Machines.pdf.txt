Higher-order Derivatives of Weighted Finite-state Machines
Ran Zmigrod Tim Vieira Ryan Cotterell , University of Cambridge Johns Hopkins University ETH Zu¨rich
rz279@cam.ac.uk tim.f.vieira@gmail.com ryan.cotterell@inf.ethz.ch

arXiv:2106.00749v1 [cs.CL] 1 Jun 2021

Abstract
Weighted finite-state machines are a fundamental building block of NLP systems. They have withstood the test of time--from their early use in noisy channel models in the 1990s up to modern-day neurally parameterized conditional random fields. This work examines the computation of higher-order derivatives with respect to the normalization constant for weighted finite-state machines. We provide a general algorithm for evaluating derivatives of all orders, which has not been previously described in the literature. In the case of second-order derivatives, our scheme runs in the optimal O(A2N 4) time where A is the alphabet size and N is the number of states. Our algorithm is significantly faster than prior algorithms. Additionally, our approach leads to a significantly faster algorithm for computing second-order expectations, such as covariance matrices and gradients of first-order expectations.
1 Introduction
Weighted finite-state machines (WFSMs) have a storied role in NLP. They are a useful formalism for speech recognition (Mohri et al., 2002), machine transliteration (Knight and Graehl, 1998), morphology (Geyken and Hanneforth, 2005; Linde´n et al., 2009) and phonology (Cotterell et al., 2015) inter alia. Indeed, WFSMs have been "neuralized" (Rastogi et al., 2016; Hannun et al., 2020; Schwartz et al., 2018) and are still of practical use to the NLP modeler. Moreover, many popular sequence models, e.g., conditional random fields for part-ofspeech tagging (Lafferty et al., 2001), are naturally viewed as special cases of WFSMs. For this reason, we consider the study of algorithms for the WFSMs of interest in se for the NLP community.
This paper considers inference algorithms for WSFMs. When WFSMs are acyclic, there exist

simple linear-time dynamic programs, e.g., the forward algorithm (Rabiner, 1989), for inference. However, in general, WFSMs may contain cycles and such approaches are not applicable. Our work considers this general case and provides a method for efficient computation of mth-order derivatives over a cyclic WFSM. To the best of our knowledge, no algorithm for higher-order derivatives has been presented in the literature beyond a generalpurpose method from automatic differentiation. In contrast to many presentations of WFSMs (Mohri, 1997), our work provides a purely linear-algebraic take on them. And, indeed, it is this connection that allows us to develop our general algorithm.
We provide a thorough analysis of the soundness, runtime, and space complexity of our algorithm. In the special case of second-order derivatives, our algorithm runs optimally in O(A2N 4) time and space where A is the size of the alphabet, and N is the number of states. In contrast, the second-order expectation semiring of Li and Eisner (2009) provides an O(A2N 7) solution and automatic differentiation (Griewank, 1989) yields a slightly faster O(AN 5+A2N 4) solution. Additionally, we provide a speed-up for the general family of second-order expectations. Indeed, we believe our algorithm is the fastest known for computing common quantities, e.g., a covariance matrix.1
2 Weighted Finite-State Machines
In this section we briefly provide important notation for WFSMs and a classic result that efficiently finds the normalization constant for the probability distribution of a WFSM.
Definition 1. A weighted finite-state machine M is a tuple , {W(a)}aA,  where A is an al-
1Due to space constraints, we keep the discussion of our paper theoretical, though applications of expectations that we can compute are discussed in Li and Eisner (2009), Sa´nchez and Romero (2020), and Zmigrod et al. (2021).

phabet of size A, A d=ef A  {}, each a  A has a symbol-specific transition matrix W(a)  R0N×N where N is the number of states, and ,   R0N are column vectors of start and end weights, respectively. We define the matrix W d=ef aA W(a).
Definition 2. A trajectory i is an ordered sequence of transitions from state i to state . Visually,
we can represent a trajectory by

i d=ef i -a j · · · k -a

The weight of a trajectory is





w(i

)

d=ef

i

 

Wj(ak) 

(1)

(j-a k)i

We denote the (possibly infinite) set of trajectories
from i to by Ti and the set of all trajectories by T d=ef i, [N] Ti .2 Consequently, when we say i  T , we make i and implicit arguments to which Ti we are accessing.
We define the probability of a trajectory i  T ,

p(i

) d=ef w(i Z

)

(2)

where



Z d=ef 

Wk 

(3)

k=0

Of course, p is only well-defined when 0 < Z < .3 Intuitively,  Wk  is the total weight of all trajectories of length k. Thus, Z is the total weight of all possible trajectories as it sums over the total weight for each possible trajectory length.

Theorem 1 (Corollary 4.2, Lehmann (1977)).



W d=ef Wk = (I - W)-1

(4)

k=0

Thus, we can solve the infinite summation that defines W by matrix inversion in O(N 3) time.4
Corollary 1.

Z= W 

(5)

2|T | is infinite if and only if M is cyclic. 2Another formulation for Z is i T w(i ). 3This requirement is equivalent to W having a spectral radius < 1. 4This solution technique may be extended to closed semir-
ings (Kleene, 1956; Lehmann, 1977).

Proof. Follows from (4) in Theorem 1. By Corollary 1, we can find Z in O(N 3 + AN 2).5

Strings versus Trajectories. Importantly, WFSMs can be regarded as weighted finite-state acceptors (WFSAs) which accept strings as their input. Each trajectory i  T has a yield (i ) which is the concatenation of the alphabet symbols of the trajectory. The yield of a trajectory ignores any  symbols, a discussion regarding the semantics of  is given in Hopcroft et al. (2001). As we focus on distributions over trajectories, we do not need special considerations for  transitions. We do not consider distributions over yields in this work as such a distribution requires constructing a latent-variable model

1

p() = Z

w(i )

(6)

i T ,

(i )=

where   A and (i ) is the yield of the trajectory. While marginal likelihood can be found efficiently,6 many quantities, such as the entropy of the distribution over yields, are intractable to compute (Cortes et al., 2008).

3 Computing the Hessian (and Beyond)

In this section, we explore algorithms for efficiently computing the Hessian matrix 2Z. We briefly describe two inefficient algorithms, which are derived by forward-mode and reverse-mode automatic differentiation. Next, we propose an efficient algorithm which is based on a key differential identity.
3.1 An O(A2N 7) Algorithm with Forward-Mode Automatic Differentiation
One proposal for computing the Hessian comes from Li and Eisner (2009) who introduce a method based on semirings for computing a general family of quantities known as second-order expectations (defined formally in §4). When applied to the computation of the Hessian their method reduces precisely to forward-mode automatic differentiation (AD; Griewank and Walther, 2008, Chap 3.1). This
5Throughout this paper, we assume a dense weight matrix and that matrix inversion is O(N 3) time. We note, however, that when the weight matrix is sparse and structured, faster matrix-inversion algorithms exist that exploit the strongly connected components decomposition of the graph (Mohri et al., 2000). We are agnostic to the specific inversion algorithm, but for simplicity we assume the aforementioned running time.
6This is done by intersecting the WFSA with another WFSA that only accepts .

approach requires that we "lift" the computation of Z to operate over a richer numeric representation
known as dual numbers (Clifford, 1871; Pearlmut-
ter and Siskind, 2007). Unfortunately, the second-
order dual numbers that we require to compute the Hessian introduce an overhead of O(A2N 4) per numeric operation of the O(N 3) algorithm that computes Z, which results in O(A2N 7) time.

3.2 An O(AN 5 +A2N 4) Algorithm with
Reverse-Mode Automatic Differentiation
Another method for materializing the Hessian 2Z is through reverse-mode automatic differ-
entiation (AD). Recall that we can compute Z in O(N 3 + AN 2), and can consequently find Z in O(N 3 + AN 2) using one pass of reverse-
mode AD (Griewank and Walther, 2008, Chap-
ter 3.3). We can repeat differentiation to materialize 2Z. Specifically, we run reverse-mode AD
once for each element i of Z. Taking the gra-
dient of (Z)i gives a row of the Hessian matrix, [(Z)i] = [2Z](i,:). Since each of these passes takes time O(N 3+AN 2) (i.e., the same as the cost of Z), and Z has size AN 2, the overall time is O(AN 5 +A2N 4).

3.3 Our Optimal O(A2N 4) Algorithm
In this section, we will provide an O(A2N 4)-time and space algorithm for computing the Hessian. Since the Hessian has size O(A2N 4), no algorithm can run faster than this bound; thus, our algorithm's time and space complexities are optimal. Our algorithm hinges on the following lemma, which shows that the each of partial derivatives of W can be cheaply computed given W .
Lemma 1. For i, j, k,  [N ] and a  A

Wi Wj(ak)

=

Wij W. j(ak)Wk

(7)

where W. j(ak) is shorthand for Wj(ak).
Proof.

Wi Wj(ak)

=

 Wj(ak)

(I - W)-i 1

 = -Wij Wj(ak) [(I - W)] Wk
= Wij W. j(ak)Wk

The second step uses Equation 40 of the Matrix Cookbook (Petersen and Pedersen, 2008).

We now extend Lemma 1 to express higher-

oinrdLeermdmeraiv1a,tiwveeswinillteursme sW.oi(fja)Was

. a

Note that as shorthand for

the partial derivative Wi(ja).

Theorem 2. For m  1 and m-tuple of transitions  = i1 -a1 j1, . . . , im -a-m  jm

mZ

= Wi(1aj11) · · · Wi(mamjm)

(8)

i1-a1 j1,··· ,im-a-m jm S

si1 W. i(1aj11)Wj1i2 W. i(2aj22) · · · Wjm-1im W. i(mamjm) ejm

where s =  W , e = W  and S is the multiset of permutations of  .7 Proof. See App. A.1
Corollary 2. For i, j, k, l  [N ] and a, b  A

2Z

Wi(ja)Wk(bl) =

(9)

siW. i(ja)WjkW. k(bl)el + skW. k(bl)WliW. i(ja)ej

Proof. Application of Theorem 2 for the m=2 case.

Theorem 2 shows that, if we have already computed W , each element of the mth derivative can be found in O(m m!) time: We must sum over O(m!) permutations, where each summand is the product of O(m) items. Importantly, for the Hessian (m = 2), we can find each element in O(1) using Corollary 2. Algorithm Dm in Fig. 1 provides pseudocode for materializing the tensor containing the mth derivatives of Z.
Theorem 3. For m  1, algorithm Dm computes mZ in O(N 3 + m m! AmN 2m) time and O(AmN 2m) space.
Proof. Correctness of algorithm Dm follows from Theorem 2. The runtime and space bounds follow by needing to compute and store each combination of transitions. Each line of the algorithm is annotated with its running time.
Corollary 3. The Hessian 2Z can be materialized in O(A2N 4) time and O(A2N 4) space. Note that these bounds are optimal.
Proof. Application of Theorem 3 for the m=2 case.

7As  may have duplicates, S can also have duplicates and so must be a multi-set.

1: def Dm(W, , ) :

2:

Compute the tensor of mth-order derivative of a

WFSM; requires O(N 3 + m m! AmN 2m) time,

O(AmN 2m) space.

3: W  (I - W)-1

O(N 3)

4: s   W ; e  W 

O(N 2)

5: D  0

6: for   ([N ]×[N ]×A)m : O(mm!AmN2m)

7: 8:

forDi1+-=a1·s·ij·11W W,..ij(.1am.j1-1,)i1Wmimj-W 1a-.im 2i(Wma.mjji(mm2)aj2e2)jWm jS2i3 :

9: return D

10: def E2(W, , , r, t) :

11:

Compute the second-order expectation of a

WFSM; requires O(N 3 + N 2(R T + AR T ))

time, O(N 2 + RT + N (R + T )) space where

R d=ef min(N R , R) and T d=ef min(N T , T ).
12: Compute W , s, and e as in Dm O(N3)

13: Z   W 

14: rs  0; re  0; ts  0; te  0

15: 16: 17: 18: 19:

forrrttsieiiisei,++++j====sWsW.j.j[WWNi(i(..jaja)a()]j(ei,ejai)jaj)WWWW. j(i(j(aj(ijaaAiai))))ttrW(j(i:aji(jaai))i()ja) ri(ja)

O(AN 2) O(R ) O(R ) O(T ) O(T )

20:

return

1 Z

N i,j=0

ris

Wij

tej

+ tsi Wij rje

+ aA siW. i(ja)ej Wi(ja)ri(ja)t(ija)

O(N 2(R T +AR T ))

Figure 1: Algorithms
4 Second-Order Expectations

In this section, we leverage the algorithms of the previous section to efficiently compute a family expectations, known as a second-order expectations. To begin, we define an additively decomposable function r: T  RR as any function expressed as

r(i ) =

rj(ak)

(10)

(j-a k)i

where each rj(ak) is an R-dimensional vector. Since many r of interest are sparse, we analyze our al-

gorithms in terms of R and its maximum den-

sity R

d=ef

max
j

-a k

rj(ak) 0. Previous work has

considered expectations of such functions (Eisner,

2001) and the product of two such functions (Li

and Eisner, 2009), better known as second-order ex-

pectations. Formally, given two additively decom-

posable functions r: T  RR and t: T  RT , a second-order expectation is

Ei r(i )t(i ) d=ef

(11)

p(i )r(i )t(i )
i T

Examples of second-order expectations include the Fisher information matrix and the gradients of firstorder expectations (e.g., expected cost, entropy, and the Kullback­Leibler divergence).
Our algorithm is based on two fundamental concepts. Firstly, expectations for probability distributions as described in (1), can be decomposed as expectations over transitions (Zmigrod et al., 2021). Secondly, the marginal probabilities of transitions are connected to derivatives of Z.8
Lemma 2. For m  1 and m-tuple of transitions  = i1 -a1 j1, . . . , im -a-m  jm

p( )

=

1 Z

m n=1



 Wi(1aj11) .

nZ ..

Wi(nanjn)

n
Wi(kajkk)
k=1

(12)

Proof. See App. A.2.

We formalize our algorithm as E2 in Fig. 1. Note that we achieve an additional speed-up by exploiting associativity (see App. A.3).
Theorem 4. Algorithm E2 computes the secondorder expectation of additively decomposable functions r: T  RR and t: T  RT in:

O(N 3 +N 2(R T +AR T )) time O(N 2 +RT +N (R + T )) space

where R= min(N R , R) and T = min(N T , T ).
Proof. Correctness of algorithm E2 is given in App. A.3. The runtime bounds are annotated on each line of the algorithm. We note that each r and t is R and T sparse. O(N 2) space is required to store W , O(RT ) is required to store the expectation, and O(N (R + T )) space is required to store the various r and t quantities.
Previous approaches for computing secondorder expectations are significantly slower than E2. Specifically, using Li and Eisner (2009)'s secondorder expectation semiring requires augmenting the arc weights to be R × T matrices and so runs in O(N 3RT + AN 2RT ). Alternatively, we can
8This is commonly used in the case of single transition marginals, which can be found by  log Z

use AD, as in §3.2, to materialize the Hessian and compute the pairwise transition marginals.
This would result in a total runtime of O(AN 5+ A2N 4R T ).
5 Conclusion
We have presented efficient methods that exploit properties of the derivative of a matrix inverse to find m-order derivatives for WFSMs. Additionally, we provided an explicit, novel, algorithm for materializing the Hessian in its optimal complexity, O(A2N 4). We also showed how this could be utilized to efficiently compute second-order expectations of distributions under WFSMs, such as covariance matrices and the gradient of entropy. We hope that our paper encourages future research to use the Hessian and second-order expectations of WFSM systems, which have previously been disadvantaged by inefficient algorithms.
Acknowledgments
We would like to thank the reviewers for engagine with our work and providing valuable feedback. The first author is supported by the University of Cambridge School of Technology ViceChancellor's Scholarship as well as by the University of Cambridge Department of Computer Science and Technology's EPSRC.
Ethical Concerns
We do not foresee how the more efficient algorithms presented this work exacerbate any existing ethical concerns with NLP systems.
References
W. K. Clifford. 1871. Preliminary sketch of biquaternions. Proceedings of the London Mathematical Society, 1.
Corinna Cortes, Mehryar Mohri, Ashish Rastogi, and Michael Riley. 2008. On the computation of the relative entropy of probabilistic automata. International Journal of Foundations of Computer Science, 19.
Ryan Cotterell, Nanyun Peng, and Jason Eisner. 2015. Modeling word forms using latent underlying morphs and phonology. Transactions of the Association for Computational Linguistics, 3.
Jason Eisner. 2001. Expectation semirings: Flexible EM for learning finite-state transducers. In Proceedings of the European Summer School in Logic, Language and Information Workshop on Finite-state Methods in Natural Language Processing.

Alexander Geyken and Thomas Hanneforth. 2005. TAGH: A complete morphology for German based on weighted finite state automata. In Finite-State Methods and Natural Language Processing, 5th International Workshop.
Andreas Griewank. 1989. On automatic differentiation. Mathematical Programming: Recent Developments and Applications, 6.
Andreas Griewank and Andrea Walther. 2008. Evaluating Derivatives­Principles and Techniques of Algorithmic Differentiation, 2nd edition. Society for Industrial and Applied Mathematics.
Awni Hannun, Vineel Pratap, Jacob Kahn, and WeiNing Hsu. 2020. Differentiable weighted finite-state transducers. CoRR, abs/2010.01003.
John E. Hopcroft, Rajeev Motwani, and Jeffrey D. Ullman. 2001. Introduction to automata theory, languages, and computation, 2nd Edition. AddisonWesley series in computer science. Addison-WesleyLongman.
Stephen C. Kleene. 1956. Representation of events in nerve nets and finite automata. Automata Studies.
Kevin Knight and Jonathan Graehl. 1998. Machine transliteration. Computational Linguistics, 24.
John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning.
Daniel J. Lehmann. 1977. Algebraic structures for transitive closure. Theoretical Computer Science, 4.
Zhifei Li and Jason Eisner. 2009. First- and secondorder expectation semirings with applications to minimum-risk training on translation forests. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.
Krister Linde´n, Miikka Silfverberg, and Tommi A. Pirinen. 2009. HFST tools for morphology - an efficient open-source package for construction of morphological analyzers. In Proceedings of the State of the Art in Computational Morphology - Workshop on Systems and Frameworks for Computational Morphology.
Mehryar Mohri. 1997. Finite-state transducers in language and speech processing. Computational Linguistics, 23.
Mehryar Mohri, Fernando Pereira, and Michael Riley. 2002. Weighted finite-state transducers in speech recognition. Computer Speech and Language, 16.
Mehryar Mohri, Fernando C. N. Pereira, and Michael Riley. 2000. The design principles of a weighted finite-state transducer library. Theoretical Computer Science, 231.

Barak A. Pearlmutter and Jeffrey Mark Siskind. 2007. Lazy multivariate higher-order forward-mode AD. In Proceedings of the 34th Association for Computer Machinery Special Interest Group on Programming Languages and Special Interest Group on Algorithms and Computation Theory Symposium on Principles of Programming Languages.
K. B. Petersen and M. S. Pedersen. 2008. The matrix cookbook. Version 20081110.
Lawrence R. Rabiner. 1989. A tutorial on hidden Markov models and selected applications in speech recognition. Proceedings of the Institute of Electrical and Electronics Engineers, 77.
Pushpendre Rastogi, Ryan Cotterell, and Jason Eisner. 2016. Weighting finite-state transductions with neural context. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics.
Joan-Andreu Sa´nchez and Vero´nica Romero. 2020. Computation of moments for probabilistic finitestate automata. Information Sciences, 516.
Roy Schwartz, Sam Thomson, and Noah A. Smith. 2018. Bridging CNNs, RNNs, and weighted finitestate machines. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, volume 1.
Ran Zmigrod, Tim Vieira, and Ryan Cotterell. 2021. Efficient computation of expectations under spanning tree distributions. Transactions of the Association for Computational Linguistics.

A Proofs

A.1 Theorem 2. For m  1 and m-tuple of transitions  = i1 -a1 j1, . . . , im -a-m  jm

mZ =
Wi(1aj11) . . . Wi(mamjm)

si1 .Wi(1aj11)Wj1i2 W. i(2aj22) · · · Wjm-1im W. i(mamjm) ejm

i1-a1 j1,...,im-a-m jm S

where s =  W , e = W  and S is the multi-set of permutations of  . Proof. We prove this by induction on m. Base Case: m = 1 and  = i -a j :





Z Wi(ja)

=

 Wi(ja)



N
kWkll =

N kWkiW. i(ja)Wjll

k,l=0

k,l=0

=

siW. i(ja)ej

Inductive Step: Assume that the expression holds for m. Let  = i1 -a1 j1, . . . , im -a-m  jm and consider the tuple  , the concatenation of (i -a j) and  .

m+1Z Wi(ja)Wi(1aj11) . . . Wi(mamjm)

 =
Wi(ja)

si1 W. i(1aj11)Wj1i2 · · · W. i(mamjm) ejm

i1-a1 j1,...,im-a-m jm S

Consider the derivative of each summand with respect to Wi(ja). By the product rule, we have

 Wi(ja)

si1 W. i(1aj11)Wj1i2 · · · W. i(mamjm) ejm

=

si ·· ··

W ··.++i(jass)iiW11 W·.j·ii(·11aWWj1.1)Wji(k1aji1j1W)1.Wi2i(ja·j)1·Wi·2W·j.i·ki(·m +aWm1j.m)·i(·maW·mjemj)jmm eij+Wm. +i(ja)

ej

The above expression is equal to inserting i -a j in every spot of the induction hypothesis's permutation, thereby creating a permutation over  . Reassembling with the expression for the derivative,



Wi(ja)



m+1 Wi(1aj11) .

Z =
. . Wi(mamjm) i1-a1 j1,...,im+1

si1 W. i(1aj11)Wj1
-a-m-+1 jm+1 )S

i2

W. i(2aj22)

·

·

·

.W(am+1) im+1 jm+1

ejm+1

A.2 Lemma 2. For m  1 and m-tuple of transitions  = i1 -a1 j1, . . . , im -a-m  jm

p( )

=

1m Z
n=1

nZ Wi(1aj11) . . . Wi(nanjn)

n
Wi(kajkk)
k=1

(10)

Proof. Let T be the set of trajectories such that i 1
p( ) = Z
i

 T    i
w(i )
T

. Then,

We prove the lemma by induction on m. Base Case: Then, m = 1 and  = i1 -a1 j1 . We have that



1 Z Z Wi(1aj11)

Wi(1aj11)

=

1 Z Wi(1aj11)


i

w(i
T





) Wi(1aj11)

(=a)

1 Z



i

w(i
T

 ) = p(i1 -a1 j1)

Step (a) holds because taking the derivative of Z with respect to Wi(1aj11) yields the sum of the weights all trajectories which include i1 -a1 j1 where we exclude Wi(1aj11) from the computation of the weight.
Then, we can push the outer Wi(1aj11) into the equation to obtain the sum of the weights of all trajectories containing i1 -a1 j1. Inductive Step: Suppose that (10) holds for any m-tuple. Let  = i1 -a1 j1, . . . , im+1 -a-m-+1 jm+1 . Without loss of generality, fix i1 -a1 j1 and let  be  without i1 -a1 j1.

1 m+1

nZ

Z n=1 Wi(1aj11) . . . Wi(nanjn)

n
Wi(kajkk)
k=1

(=b)

Wi(1aj11)

  Wi(1aj11)

1 m+1

(n-1)Z

Z n=2 Wi(2aj22) . . . Wi(nanjn)

n
Wi(kajkk)
k=2

Inductive hypothesis





(c)
=

Wi(1aj11)



 Wi(1aj11)

1 Z

i

w(i
T

)





(d) 1 

=

Z

 Wi(1aj11)


i

w(i
T

) Wi(1aj11)

(e)
= p( )

Step

(b)

pushes

1 Z

and

n k=2

Wi(kajkk)

as

constants

into

the

derivative

and

step

(c)

uses

our

induction

hypothesis

on



.

Then,

step

(d)

takes

1 Z

out

of

the

derivative

as

we

pushed

it

in

as

a

constant.

Finally,

step (e) follows by the same reasoning as step (a) in the base case above.

A.3
Theorem 4. Algorithm E2 computes the second-order expectation of additively decomposable functions r: T  RR and t: T  RT in:
O(N 3 +N 2(R T +AR T )) time O(N 2 +RT +N (R + T )) space

where R= min(N R , R) and T = min(N T , T ). Proof. We provide a proof of correctness (the time and space bounds are discussed in the main paper). Zmigrod et al. (2021) show that we can find second-order expectations over by finding the expectations over pairs of transitions. That is,

N
Ei r(i )t(i ) =

p i -a j, k -b l ri(ja)t(kbl)

i,j,k,l=0 a,bA

We can use Lemma 2 for the m = 2 case, to find that the expectation is given by

Ei r(i )t(i )

1 =
Z

N i,j=0

aA

Z Wi(ja)

Wi(ja)ri(ja)t(ija)

N
+
i,j,k,l=0

a,bA

2Z Wi(ja)  Wk(bl)

Wi(ja)Wk(bl)

ri(ja)t(kbl)

The first summand can be rewritten as

N i,j=0

aA



Z Wi(ja)

Wi(ja)ri(ja)t(ija)

N
=

siW. i(ja)ej Wi(ja)ri(ja)t(ija)

i,j=0 aA

The second summand can be rewritten as

N i,j,k,l=0

2Z a,bA  Wi(ja)  Wk(bl)

Wi(ja)Wk(bl)ri(ja)t(kbl)

N
=

siW. i(ja)WjkW. k(bl)elWi(ja)Wk(bl)ri(ja)t(kbl)

i,j,k,l=0 a,bA

+ skW. k(bl)WliW. i(ja)ej Wi(ja)Wk(bl)ri(ja)t(kbl)

Consider the first summand of the above expression

N

siW. i(ja)WjkW. k(bl)elWi(ja)Wk(bl)ri(ja)t(kbl)

i,j,k,l=0 a,bA


NN
=


siW. i(ja)Wi(ja)ri(ja)Wjk N


W. k(bl)elWk(bl)t(kbl)

j,k=0 i=0aA

l=0 bA

N
= rjsWjktek
j,k=0

d=ef rjs

d=ef tek

Similarly, the second summand can be written as

N
rke Wj k tsj
j,k=0

Finally, recomposing all the pieces together,

Ei

r(i )t(i )

1 =
Z

N
risWij tej + rjeWij tsi +

siW. i(ja)ej Wi(ja)ri(ja)t(ija)

i,j=0

aA

