arXiv:2106.01858v1 [stat.ML] 3 Jun 2021

STATISTICAL EMBEDDING: BEYOND PRINCIPAL COMPONENTS
DAG TJØSTHEIM1, MARTIN JULLUM2, AND ANDERS LØLAND3
Abstract. There has been an intense recent activity in embedding of very high dimensional and nonlinear data structures, much of it in the data science and machine learning literature. We survey this activity in four parts. In the first part we cover nonlinear methods such as principal curves, multidimensional scaling, local linear methods, ISOMAP, graph based methods and kernel based methods. The second part is concerned with topological embedding methods, in particular mapping topological properties into persistence diagrams. Another type of data sets with a tremendous growth is very high-dimensional network data. The task considered in part three is how to embed such data in a vector space of moderate dimension to make the data amenable to traditional techniques such as cluster and classification techniques. The final part of the survey deals with embedding in R2, which is visualization. Three methods are presented: t-SNE, UMAP and LargeVis based on methods in parts one, two and three, respectively. The methods are illustrated and compared on two simulated data sets; one consisting of a triple of noisy Ranunculoid curves, and one consisting of networks of increasing complexity and with two types of nodes.
Key words and phrases: Statistical embedding, principal component, nonlinear principal component, multidimensional scaling, local linear method, ISOMAP, graph spectral theory, reproducing kernel Hilbert space, topological data analysis and embedding, persistent homology, persistence diagram, network embedding, spectral embedding, Skip-Gram, neighborhood sampling strategies, visualization, t-SNE, LargeVis, UMAP
1. Introduction
With the advent of the Big Data revolution, the availability of data has exploded. Dimension of the data can be in the thousands, if not in the millions, and the relationships between data vectors can be exceedingly complex. Also, data are arriving in new forms. One recent addition to data types is network data, often internet based, sometimes with millions of nodes, and literally billions of edges (relationships between nodes). How does one understand the structure of such data sets? How can the essential structure of the data be preserved and characterized in an embedding in a possibly still high dimension, but much lower dimension than that of the original data set? How does one describe the interaction between various types of nodes in a way amenable to analysis? Another example is the analysis of porous media, in oil exploration say, or of astronomical or physiological data. Such data contains cavities and complicated geometric structures. Still another example is in natural languages with texts containing million of words. Is it possible to characterize language segments so as to discriminate one type of text from another?
All of these examples have to do with the characterization and simplification of highly complex and often unorganized data. From a mathematical and statistical point of view these tasks are examples of embedding problems. One classic method is the traditional principal component analysis. It is still a much (the most?) used approach. For a number of lowdimensional characterization problems it works well, but in other situations it fails or simply cannot be applied, and other embedding methods must be sought. There has been a recent surge in methods beyond principal components initiated by the Big Data revolution, and the aim and motivation behind this paper is to make a concentrated survey of such methods.
1University of Bergen and Norwegian Computing Center 2,3Norwegian Computing Center E-mail addresses: Dag.Tjostheim@uib.no, Martin.Jullum@nr.no, Anders.Loland@nr.no.
1

STATISTICAL EMBEDDING: BEYOND PRINCIPAL COMPONENTS

2

The aim of the survey could be said to be two-fold. First, to try to give a quite comprehensive survey of embedding methods and applications of these methods. Much of recent developments have taken place in the so-called data science literature, including machine learning and often published in proceedings of conferences. With some exceptions there has not been that much corresponding activity in the more traditional statistical (or mathematical statistics) literature. The second objective of this article has been to make the statistical community more aware of current methods in this branch of data science. We believe that there are potential synergy effects to be harvested. More specifically, one may think of finding good and adequate measures of uncertainty inherent in some of these methods, and to find possible better rationales for some of the methods that presently do include ad hoc choices to be made in their implementation and potentially bring fresh insight to issues such as heterogeneity and nonstationarity.
Here is a brief overview of the contents of the paper. Section 2 gives a brief summary of principal components and points out some strengths and weaknesses. Included in the theory are often assumptions of multivariate normal distribution and the use of linear transformations. There are now a number of novel nonlinear methods, some of them in fact with roots going far back in time. In Section 3, we look in particular at methods such as principal curves and surfaces, multidimensional scaling, local linear embedding, embedding via graphs (note that in this survey the terms "graph" and "network" will be used interchangeably), ISOMAP and Laplace eigenmaps, and kernel principal components using reproducing kernel Hilbert spaces, the last one popular in the machine learning community. Section 4 has to do with the emerging field of topological data analysis and topological manifold embedding. The idea is to seek a type of embedding where topological and/or geometrical patterns, including cavities of the data sets, are well described. Section 5 deals with embedding of network data, especially ultra high dimensional networks. This is a topic of great practical interest, as can be understood from the recent advances within social network analysis. Networks have been examined spanning from the famous karate club example, from networks of books on American politics, both analyzed in Newman (2006), to criminal fraud networks, hidden cells in terrorist networks (see e.g. Mornelli et al. (2005) and Budur et al. (2015)) and natural language analysis (Mikolov et al. (2013)). Some of the network research publications have rapidly resulted in thousands of citations in relevant fora, which are often found to be conference proceedings and internet based publications. Open problems in heterogeneous, directed and dynamic networks are also briefly covered in this section.
Finally, in Section 6, we go on to the extreme case of having an embedding of dimension 2, the plane. This has to do with visualization, of course, and we are presenting three visualization methods, t-SNE, LargeVis, and UMAP, whose basis can be found in each of the preceding sections, namely nonlinear type embedding, network embedding and topological embedding. They are compared to principal component visualization.
To avoid an overlong paper some of the more technical and detailed aspects of the surveyed methods are relegated to the Supplement (Tjøstheim et al., 2021a). We will also refer to previous review articles covering parts of our material, again found mainly in the data science literature and with the emphasis more on algorithms rather than statistical properties and concepts. To our knowledge ours is the first of such broad coverage as the present. There are many unsolved statistical problems, and we will try to point out some of these as we proceed.
We have chosen to illustrate our methods by two types of simulation experiments. First, a triple of noisy Ranunculoid (a concept originating in flower forms in botany) curves encapsulated in one another, cf. Fig. 1a, (a situation in which principal components do not work) illustrates a number of the nonlinear methods of Section 3 and the topological embedding of Section 4. As a second example we have included a network based simulation with two types of nodes and varying degrees of complexity in their interaction. Among other things these are used to illustrate and compare the three visualization methods of Section 6, for several choices of their input parameters. In the paper we also refer to real data experiments that have been conducted especially in the network embedding literature.

STATISTICAL EMBEDDING: BEYOND PRINCIPAL COMPONENTS

3

2. Principal components

Principal component analysis (PCA) was invented by Pearson (1901) as an analogue of

the analysis of principal axes in mechanics. It was later independently developed by Harold

Hotelling in the 1930s, see e.g. Hotelling (1933) and Hotelling (1936).

Given p-dimensional observations X1, . . . , Xn, the Hotelling approach was along the lines

that have since become standard: Let Xi, i = 1, . . . , n have components Xij, j = 1, . . . , p. The

first principal component V1 = {aj1} consists of the weights which gives the linear combination

p j=1

aj1Xij

maximum

variance

subject

to

the

constraint

that

the

Euclidean

norm

||V1||

=

1.

The kth principal component Vk = {ajk} corresponds to the linear combination

p j=1

aj k Xij

with the maximum variance subject to ||Vk|| = 1, and it being orthogonal to previously found

Vj, 1  j  k - 1. Or said in another way, the principal components constitute a sequence of

projections in Rp of the data, mutually uncorrelated and ordered in variance.

Let  be the p × p population covariance matrix. Then it is well known, see e.g. Joliffe

(2002), that the principal components Vk are obtained by solving the eigenvalue problem

Vk = kVk,

where the largest eigenvalue 1 corresponds to the first principal component V1, and where the

variance explained by the kth principal component is given by k/

p i=1

i.

The estimated principal components are obtained by considering an estimate of . Let X

be the n × p centered data matrix X = {(Xij - X¯j)} with X¯j = n-1 i Xij, then an estimate

of  is obtained from n-1[XT X], and the estimated eigenvectors and eigenvalues are obtained

from

XT XV^ = ^V^ .

In practice, especially for high dimensions, it is computationally faster to use a singular value decomposition of the data matrix X itself.
The approach of Pearson is different, and the essence of his method is that he looks at a set of m principal components as spanning a hyper-plane of rank m in Rp such that the sum of the distances from the data points to this hyper-plane is minimized. The first principal component is then the line in Rp obtained by such a minimization. As will be seen it is the Pearson approach which is most amenable to generalizations to the nonlinear case.
Before we close this section there is cause to ask why linear principal component analysis is so useful. It is clearly the most used statistical embedding method. Why? There are several reasons for this. One is its potential to reduce the dimension of the original data. If a few principal components explain a large percentage of the variation, this in many cases means that the ensuing analysis can be concentrated to those components. These components can also be used henceforth in a factor analysis. And the number of needed components can often be decided by a clear cut percentage of variation explained, which, as was seen above, is straightforward to compute given the eigenvalues of the covariance matrix.
The collection of m principal components can also be used as feature extractors in clustering and classification problems where there is more than one class of observations involved. But it should be noted that the first principal components, although explaining most of the variance, may not generally be the best feature extractors for classification. This is because several classes may have essentially the same set of largest principal components, and that the discriminatory power is more concentrated in components with a lower degree of explained variance.
Principal components have been used with great success in a number of different fields, so diverse as e.g. quantitative finance, neuroscience, meteorology, chemistry, and recognition of handwritten characters. Many applications and the basis of the theory are given in the book by Joliffe (2002). It is also quite robust and can work reasonably well for certain types of nonlinear systems as seen in the comparative review by van der Maaten et al. (2009).
However, there are also several shortcomings of linear principal components, which have inspired much recent research. The most obvious fault is the fact that it is a linear method, and data are often nonlinearly generated or located on or close to a submanifold of Rp. This is sometimes aggravated by the fact that the PCA is based on the covariance matrix, and it is

STATISTICAL EMBEDDING: BEYOND PRINCIPAL COMPONENTS

4

well-known that a covariance between two stochastic variables is not always a good measure of statistical dependence. This has been particularly stressed in recent dependence literature, a survey of which is given in Tjøstheim et al. (2021b). Especially there exist statistical models and data where the covariance is zero although there may be a strong statistical dependence. An example is the so-called ARCH/GARCH time series models for financial risk whose dependence cannot be measured in terms of autocorrelation and cross-correlation function.
To do statistical inference in PCA often a Gaussian assumption is added as well. For Gaussian variables the covariance matrix describes the dependence relations completely, so that it would be impossible to improve on the PCA embedding by a nonlinear embedding. But increasingly, data sets are appearing where the Gaussian assumption is not even approximately true. Moreover, in the age of Big Data the dimension of data may be extremely large, not making it amenable to principal component analysis which involves the solution of a p-dimensional eigenvalue problem. Finally, data may come in other forms such as networks. It is not clear how principal components can be applied under such circumstances.
As stated in the introduction, the purpose of this paper is to review a number of methods that can handle these problems. There has been an enormous growth in the literature recently both methodologically and in applications to new situations.

3. Nonlinear embeddings

3.1. Principal curves and surfaces. As mentioned in Section 2 it is the Pearson's hyper-
plane fitting that is perhaps the best point of departure for nonlinear PCA. Principal curves
and surfaces were introduced in Hastie (1984) and Hastie and Stuetzle (1989). A brief summary
is given in Hastie et al. (2019, pp. 541-544). Essentially, the idea is to replace the hyper-plane
by a hyper-surface. It is simplest in the case of principal curves, generalizing the first principal component. Let f (s) be a parameterized smooth curve in Rp. The parameter s in this case is a scalar and can for instance be arc-length along the curve. For each p-dimensional data value
X, one lets sf (X) be the point on the curve closest to X. Then f (s) is called a principal curve for the distribution of the random vector X if

f (s) = E(X|sf (X) = s).

This means that f (s) is the average of all data points that project onto it. This is known as the
self-consistency property. In practice it turns out (Duchamp and Stuetzle (1996)) that there
are infinitely many principal curves for a given multivariate distribution, but one is interested
mainly in the smooth ones.
To find one principal curve f (s) in practice, one can consider the coordinate functions f (s) = [f1(s), . . . , fp(s)] and let X be the p-dimensional observational vector given by XT = (X1, . . . , Xp). Next, the following alternating steps are implemented:

(1)

E(Xj|s^f (X) = s)  f^j(s); j = 1, . . . , p

and

(2)

argmins ||X - f^(s )||2  s^f (X).

Here the first step fixes s and enforces the self-consistency requirement. The second step fixes the curve and finds the closest point on the curve to each data point. The principal curve algorithm starts with the first linear principal component, and iterates the two steps in (1) and (2) until convergence is obtained using a given tolerated error. The conditional expectation in step (1) is determined by a scatter plot smoother by smoothing each Xj as a function of arc-length s^(X), and the projection in (2) is done for each of the observed data points.
There are unsolved mathematical problems inherent in this method and proving convergence is in general difficult.
Principal surfaces have the same form as principal curves. The most commonly used is the two-dimensional principal surface with coordinate functions

f (y1, y2) = [f1(y1, y2), . . . , fp(y1, y2)].

STATISTICAL EMBEDDING: BEYOND PRINCIPAL COMPONENTS

5

The estimates in step (1) and (2) above are obtained from two-dimensional surface smoothers. The scheme with a quantification of percentage reduction of variance seems to be lost in a principal curve and principal surface set-up. A different but related approach is taken by Ozertem and Erdogmus (2011), where principal curves and surfaces are studied in terms of density ridges.
In Fig. 1 we present a data set that will be used for illustration purposes throughout this section and also in Section 4 on topological data analysis. The raw data are presented in Fig. 1a. It consists of parts of three parametric curves, each being obtained from the socalled Ranunculoid, but with three different parameter sets. In addition the curves have been perturbed by Gaussian noise. In Fig. 1b we have illustrated the construction of a principal curve on the innermost curve of Fig. 1a. It is seen that the main one-dimensional structure of the curve is well picked up, but it does not quite get all the indentions of the original curve. Compared to a linear principal regression curve it is a big improvement. (Note that a nonparametric regression is not an option here, since the x- and y-coordinates of Fig. 1a are on the same basis, and there are several y-values for many of the x-values.)

3.2. Multidimensional scaling. The idea of multidimensional scaling (MDS) goes far back,
but it, or similar ideas, has recently got a revival in statistical embedding through algorithms
such as LLE, ISOMAP (see the next sub-sections), and t-SNE (see Section 6). It can be roughly
formulated as finding suitable coordinates for a set of points given their mutual distances.
This problem was first considered by Young and Householder (1938). These methods were
further developed and applied to scaling of psychometric distances between pairs of stimuli
by Torgerson (1952). A fine review of the essentials of multidimensional scaling is given in
Hastie et al. (2019, pp. 570-572). Here the emphasis is on viewing multidimensional scaling as a general method for dimensionality reduction of data in Rp. They therefore start with a set of observations X1, . . . , Xn  Rp where dij is some form of distance measure (not necessarily Euclidean) between observation Xi and Xj. In fact, in the general theory of multidimensional scaling the dij may be considered as a dissimilarity measure between objects (e.g psychological stimuli) i and j. One example can be found in Kuno and Suga (1966), where judgment of
psychometric distance between piano pieces were represented as a configuration of points in
two-dimensional Euclidean space. From a dimension reduction point of view, multidimensional scaling seeks values Y1, . . . , Yn 
Rm, often m = 2, as in the above piano piece example, for visualization purposes, by minimizing the so-called stress function

S(Y1, . . . , Yn) = (dij - ||Yi - Yj||)2.
i=j

This is known as the least squares or Kruskal-Shephard scaling. A gradient descent algorithm can be used to minimize S. A variation on this is the so-called Sammon mapping, Sammon (1969), which minimizes

SSm(Y1, . . . , Yn)

=

i=j

(dij

-

||Yi dij

-

Yj

||)2

.

In so-called classical scaling one starts instead with similarities sij. Classical scaling is not
equivalent to least squares scaling. The loss functions are different, and the latter mapping
can, in contradistinction to PCA, be nonlinear.
It should be noticed that in the case of multidimensional scaling it is an embedding from Rp to Rm, we have an embedding from one Euclidean space to another. On the other hand principal surfaces as in Section 3.1 and many of the other methods in this survey consider embedding from Rp to a lower dimensional manifold. In particular this is the case, in Sections 3.3 and 3.4 below.

3.3. LLE Local linear embedding. Principal curves and surfaces represent an early example of local modeling and manifold embedding. Manifold embedding will be taken up from a more general point of view in Section 4 with its connections to recent advances in TDA (Topological

y

STATISTICAL EMBEDDING: BEYOND PRINCIPAL COMPONENTS

6

20

40

15

10

20

y

5

0 0

class 1

-5

class 2

-20

class 3 -10

-40 -20 0 x

20 40

-15 -10 -5 0 5 10 15 x

(a) The raw data.

(b) Principal curve on the innermost curve of Fig. 1a.

100

0.5

80

0.0

60

-0.5

40

MDS2

-1.0

20

-1.5

0

-2.0

-20

-1.5

-0.5

0.5

1.5

Y1

(c) Local linear embedding.

-60

-20 0 20 40 60 80

MDS1

(d) ISOMAP.

Y2

2nd principal component

10
5
0
-5
-10 -15 -10 -5 0 5 10 15 1st principal component
(e) Kernel principal components (with a Bessel kernel).
Figure 1. Four different embedding methods applied to three parametric curves from the so-called Ranunculoid and perturbed by Gaussian noise with a standard deviation of 1/2.

STATISTICAL EMBEDDING: BEYOND PRINCIPAL COMPONENTS

7

Data Analysis). However, it is convenient at this point to briefly mention the early work of Roweis and Saul (2000) that resembles the principal surface methodology in that it is a local method. In fact, it is a local linear model, and locally linear methods are well known and much used in nonparametric regression. But here the viewpoint is different since there is no clearly defined dependent variable. Actually in that respect, it is like the recent local Gaussian modeling of Tjøstheim et al. (2021c).
Local linear embedding (LLE) recovers global nonlinear structure from locally linear fits. Suppose that the data X1, . . . , Xn are p-dimensional vectors sampled from an inherent mdimensional manifold. One assumes that each data point lies on or close to a locally linear patch of the manifold. The local geometry of these patches is characterized by linear coefficients that reconstruct each data point from its neighbors. The neighbors can for example be given by a nearest neighborhood algorithm. Reconstruction errors in Rp are measured by a cost function

(3)

M (w) = ||Xi -

wij Xj ||2 ,

i

Xj N (i)

where N (i) is a neighborhood of Xi. The weights wij in (3) are computed by minimizing the cost function subject to the constraint that wij = 0 if xj does not belong to the set of neighbors of Xi, and such that j wij = 1.
The assumption of Roweis and Saul (2000) is that one can expect the wij-characterization of local geometry in the original data space to be equally valid for local patches of the manifold.
In particular, the same weights wij that reconstruct the ith data point in p dimensions should also reconstruct its embedded manifold coordinates in m dimensions.
In the final step of the LLE algorithm each high dimensional observation Xi is mapped to a low-dimensional vector Yi representing global internal coordinates on the manifold. This is done by choosing m-dimensional coordinates to minimize the embedding cost function

(4)

M (Y ) = ||Yi - wijYj||2.

i

j

It should be noticed that this is a minimization problem over Y . The wij weights are known and equal to those obtained in the first step minimization in (3). Then, optimizing with respect to Yi in (4) can be achieved by solving a sparse n × n eigenvalue problem. It will be seen later in this survey that such a two-step procedure is used also in other dimension reduction algorithms, as in the t-SNE visualization routine described in Section 5. From Fig. 1c it is seen that the three parts of the Ranunculoid in Fig. 1a are clearly separated with LLE, especially in the Y2-direction.

3.4. Embedding via graphs and ISOMAP. Some of the primary purposes of statistical
embedding is to use the embedded vectors or coordinates for feature extraction, clustering and
classification. The most used clustering method is probably the K-means algorithm. (See e.g.
Hastie et al. (2019, chapter 14.3).) This method does not work well if the clusters form non-
convex subsets of the data space. Examples of this is the clusters consisting of 3 concentric noisy circles in R2, or of the more complicated structure of the three curves in Fig. 1a.
For a given point cloud in Rp a method of circumventing such problems is to embed the points in a similarity graph or network. Given a set of data points X1, . . . , Xn, a similarity measure sij  0 between Xi and Xj can simply be the Euclidean distance between Xi and Xj, or there could be other similarity measures. The intuitive goal of clustering is to divide the
points into groups such that the similarity between two groups is weak, whereas the similarity
between points within a group is typically strong. If we do have similarity information between
the points, a convenient way to represent this is to form a similarity graph G = (V, E). Each node vi  V in the graph represents a data point Xi. Two nodes in the graph are connected if their similarity sij is positive or exceeds a threshold. The similarities sij then are weights wij on the edges E of the graph. The graph is undirected if wij = wji. The problem of clustering can now be reformulated using the similarity graph: one wants to find a partition of the graph

STATISTICAL EMBEDDING: BEYOND PRINCIPAL COMPONENTS

8

such that the edges between different groups have low weight, and the edges within a group have high weights.
Given a point cloud in Rp there are several ways of constructing a corresponding similarity graph:
i) The -neighborhood graph: Here one connects all points, and give them weight wij = 1, that have pairwise distances less then . One can see at once that this would represent a possible solution to the clustering of three noisy concentric circles mentioned above if the noise is moderate.
ii) k-nearest neighbor graph: Here one can connect node vi with node vj if vj are among the k nearest neighbors of vi. Symmetrization leads to an undirected graph and wij = sij.
iii) The fully connected graph: All points with positive similarity are connected with each other, and we take wij = sij. As an example of a similarity measure one can take sij = exp(-||Xi - Xj||/22), where  is a parameter that controls the strength of the similarity.
Two early references for the use of graph embedding are Tenenbaum et al. (2000) and de Silva and Tenenbaum (2002). The ISOMAP algorithm is described in Tenenbaum et al. (2000). Apart from clustering, it has gained considerable use as a nonlinear dimension reduction method, by combining graph representation with multidimensional scaling, see op. cit. references for details. The results of applying the ISOMAP algorithm on the curves in Fig. 1a are given in Fig. 1d. It is seen that the curves are well-separated both in the MDS1 and MDS2 directions.

3.5. Graph representation and Laplace eigenmaps. In this sub-section we will just give

a brief presentation of Laplace eigenmaps and graph spectral theory mainly based on Belkin

and Niyogi (2002, 2003). Here the point of departure is, as it is for all of this section, a point cloud in Rp, and then the aim is to reduce the dimension by searching for a manifold embedding

of lower dimension.

In Section 4 we will start with a network and use graph spectral theory to find an embedding

of the network in Euclidean space or on a manifold such that it can subsequently be used for

purposes of clustering and classification. A few more details of graph spectral theory will be

given then.

To introduce Laplacian eigenmaps we need some more graph notation: The weighted adja-

cency matrix of the graph is the matrix A = {aij}, i, j = 1, . . . , n, where aij = wij is the weight on the edge between nodes vi and vj. If aij = 0, this means that the nodes vi and vj are not connected by an edge. We still assume that the graph is undirected so that aij = aji. The degree of a node vi  V is defined as

n

n

(5)

di = aij = wij ,

j=1

j=1

with aii = 0. The degree matrix D is defined as the diagonal matrix with the degrees d1, . . . , dn along the diagonal.
The Laplacian eigenmap algorithm consists of three steps. In the first step a graph is constructed using the strategy outlined in (i), (ii) or (iii) of Section 3.4. This is used to establish the edges of the graph. In the second step the weights of the edges are determined. Belkin and Niyogi (2003) present two choices. The first choice, as in Section 3.4, is to choose the so-called heat kernel

(6)

wij = exp-||Xi-Xj||/t

if the nodes are connected using the -strategy of Section 3.4, and putting wij = 0 if they are not connected. The kernel parameter t  R is up to the user to choose. A second alternative is just to let wij = 1 if vi and vj are connected, and wij = 0 if not.
In the third step assume that the graph G as constructed above is connected. If not, use
the algorithm given below for each connected component. Define the Laplacian matrix by
L = D - A, where D = {dii} is the degree matrix defined above and A = {aij} are the weights

STATISTICAL EMBEDDING: BEYOND PRINCIPAL COMPONENTS

9

of the adjacency matrix. The Laplacian is symmetric, positive semidefinite and can be thought of as an operator acting on functions defined on the nodes of the graph G. The Laplacian eigenmaps are then obtained by solving the eigenvalue problem
Lfi = iDfi, i = 0, 1, . . . , p - 1,
with 0 = 0  1  p-1,
where it is easily verified that 0 is a trivial eigenvalue corresponding to the eigenvector f0 = [1, 1, . . . , 1]. This eigenvector is left out, and the next m eigenvectors are used for an embedding in m-dimensional Euclidean space
m
Xi  Xi, fj fj,
j=1
where ·, · is the inner product in Rp. The Laplacian eigenmaps preserve local information optimally in a certain sense (Belkin and Niyogi, 2003).

3.6. Kernel principal components. The standard linear Fisher discriminant seeks to dis-

criminate between two or more populations by using the global Gaussian likelihood ratio

method in an attempt to separate the populations linearly by separating super-planes. This is

of course not possible for the data in Fig. 1a. An alternative is to use a local Gaussian Fisher

discriminant which leads to nonlinear hyper-surfaces (Otneim et al., 2020). Still another pos-

sibility is to use transformations of the original data into nonlinear features and then try to

find linear hyper-planes in this feature space. To find the linear hyper-planes scalar products

between vectors are used; this being the case both in the linear Fisher discriminant and in case

there is a nonlinear feature space. As a function of the original coordinates of observations,

the inner product in the feature space is termed a kernel. The support vector machine (SVM)

discrimination analysis is based on such an idea.

An analog procedure can be used in so-called kernel PCA (Sch¨olkopf et al., 2005). Consider

a set of data vectors X1, . . . , Xn with Xi  Rp that sums to the zero-vector. Recall that in ordinary principal components analysis the estimated principal components are found by

solving the eigenvalue problem Cf = f , where, C is the empirical p × p covariance matrix

given by

C

=

1 n

n

XiXiT ,

i=1

and corresponding to the matrix XT X in Section 1. In kernel PCA the starting point is to

map the data vector Xi into a nonlinear feature vector (Xi),  : Rp  F , where F is an inner

product space in general different from Rp, such that

n i=1

(Xi)

=

0.

Consider the n × n matrix K = { (Xi), (Xj) } and the eigenvalue problem

(7)

K = n,

where  is the column vector with entries i, . . . , n. Let f l be the lth eigenvector corresponding to non-zero eigenvalues. It can be shown that (Sch¨olkopf et al., 2005) for principal components
extraction, one can compute the projections of the image of a data point X onto the eigenvectors f l according to

n

(8)

f l, (X) = il (Xi), (X) .

i=1

It is just

very important needs to know

to observe the values

tkh(aXt ,nYei)th=.er

(7) nor (8) (X), (Y

requires the (Xi) in explicit form. One ) of their inner product. The function

k(X, Y ) is the kernel and using it instead of explicit values of (X) and (Y ) is the content

of the so-called kernel trick (Aizerman et al. (1956), Boser et al. (1992)). The point is that

one can start with a suitable kernel instead of having to do the mapping (X). It can be

shown by methods of functional analysis that there exists for any positive definite kernel k,

STATISTICAL EMBEDDING: BEYOND PRINCIPAL COMPONENTS

10

a map  into some inner product space F , such that k constitutes the inner product of this
space. This space would in general be of infinite dimension (function space), so here it is
the opposite of dimensionality reduction. To show that this works and to put this into a
rigorous mathematical context, one uses the framework and the properties of a reproducing
kernel Hilbert space (RKHS). A recent tutorial is given in Gretton (2019). Some common choices of kernels include the polynomial kernel k(X, Y ) = ( X, Y )d, for some integer d and inner product ·, · in Rp, and the radial basis functions k(X, Y ) = exp(-||X - Y ||2/22). The latter should be compared to the heat kernel weighting function of Laplacians as surveyed in
the previous sub-section. Finally there are the sigmoid kernels k(X, Y ) = tanh( X, Y + )
for some tuning parameters  and .
Substituting kernel functions for (X), (Y ) one obtains the following algorithm for kernel
PCA: One computes the dot product matrix

K = (Xi), (Xj) = k(Xi, Xj),
solve the eigenvalue problem for K, normalize the eigenvector expansion coefficient k, and extract principal components (corresponding to the kernel k, of which there are several choices) of an observational point X by computing projections on the eigenvectors as in Equation (8). Kernel PCA has the advantage that no nonlinear optimization is involved; one only has to solve an eigenvalue problem as in the case of the standard PCA with a feature space that is fixed a priori by choosing a kernel function. The general question of choosing an optimal kernel for a given problem is unsolved both for kernel PCA and SVM, but the three kernels mentioned above generally perform well, (Sch¨olkopf et al., 2005). The results of using the kernel principal component method on the data in Fig. 1a can be seen in Fig. 1e. It is seen that the curves are clearly separated along the second kernel principal component. The two dents in the two innermost curves of Fig. 1a are also reproduced.
It is of interest to look at the curves in Fig. 1a and their nonlinear representations when the noise is increased. This is done in Fig. 2. In Fig. 2a it is seen that with the increased noise the two innermost curves are not separated any more, but rather forms a quite complicated closed curve. The principal curve for the innermost curve (with the other two removed) is seen in Fig. 2b. The overlap of the two innermost curves is clearly seen for the local linear embedding, the ISOMAP and the kernel principal component in Figs. 2a-2c. It seems that only kernel principal component is close to separating the original three curves. For the two others the two innermost curves coalesce. In fact for local linear embedding the innermost curve more or less degenerates to two points.
The ISOMAP picture is also interesting. The innermost curve is split into two opposite curves. This is consistent with the gap in the innermost curve in the middle of it. It is also worth noting that the loop formed on the left hand side of the two innermost curves is reproduced at the bottom of the ISOMAP plot.

3.7. A few other techniques. There are several other alternative methods in nonlinear dimension reduction. Perhaps the most used one is Independent Components Analysis (ICA). The main concepts of the method are described in a much cited paper by Hyv¨arinen and Oja (2000).
In traditional factor analysis latent factors are obtained as latent variables in an eigenvector PCA decomposition. The latent factors are uncorrelated but not unique, since the uncorrelatedness is preserved by orthogonal matrix transformations of the factors, the so-called factor rotation, and in psychometry various factor rotations, varimax and quartimax, see Joliffe (2002), have been given special interpretations. This non-uniqueness is intrinsically linked to the Gaussian distribution where independence is equivalent to uncorrelatedness.
In ICA the aim is again to obtain latent factors, and in format the decomposition is the same as the PCA decomposition except that the components are now required to be independent. This means that not only the second order crossmoment (covariance) is assumed to vanish but also all higher order crossmoments. This results in uniqueness. The derivation of the

y

STATISTICAL EMBEDDING: BEYOND PRINCIPAL COMPONENTS

11

20

40

15

10

20

y

5

0
-20 -60

class 1 class 2 class 3

-40 -20 0 x

20 40

(a) The raw data.

0
-5
-10 -15 -10 -5 0 5 10 15 x
(b) Principal curve on the innermost curve of Fig. 2a.

1

0

-1

-2

-2

-1

0

1

Y1

(c) Local linear embedding.

MDS2

50

0

-50

-50

0

50

MDS1

(d) ISOMAP.

Y2

2nd principal component

10
5
0
-5
-10 -15 -10 -5 0 5 10 15 1st principal component
(e) Kernel principal components (with a Bessel kernel).
Figure 2. Four different embedding methods applied to three parametric curves from the so-called Ranunculoid and perturbed by more Gaussian noise than in Fig. 1a (we have used a standard deviation of 2 instead of 1/2 for the noise here).

STATISTICAL EMBEDDING: BEYOND PRINCIPAL COMPONENTS

12

decomposition is done using entropy concepts such as the mutual information, and the KullbackLeibler distance between probability densities. One might remark that ICA essentially starts from a factor analysis solution to dimension reduction and looks for rotations that lead to independent components. From this point of view ICA is just another factor rotation along with the traditional varimax and quartimax.
Two other methods will be very briefly mentioned. These are both neural network based methods. One of them consists in so-called autoencoding in deep neural networks, and can be represented by Hinton and Salakhutdinov (2006). The other is the method of Self Organizing Maps which can be said to have originated by another much cited paper, Kohonen (1982). The latter is also covered in Hastie et al. (2019, chapter 14.4).

4. Topological embeddings and topological data analysis (TDA)
The present section concerns topological embeddings and data analysis. We will divide our exposition in two parts, manifold learning and persistent homology. It is the latter part that is usually identified with TDA. Our point of departure is in both cases a point cloud in Rp. In part one the objective is to examine whether there is a possibility of embedding the point cloud in a lower dimensional manifold. In the latter the aim is to try to find additional topological features that may characterize the point cloud and its embedding. In order to avoid an overlong paper, parts of the TDA survey have been moved to the Supplement (Tjøstheim et al., 2021a). A main introductory reference to manifold learning and TDA is Wasserman (2018).

4.1. Manifold learning. Already in the Pearson (1901) treatment of principal components, the point cloud of data is embedded on a hyper-plane in Rp. The approach of ISOMAP and local linear embedding are early examples of representing the data in a lower dimensional
manifold.
A main aspect of manifold learning is that one looks for a non-Euclidean subspace to make an embedding that may not easily be achieved in an Euclidean space Rm, but more efficiently on a manifold. One trivial example is the case where the point cloud in the plane is concentrated
on a circle with only small additional perturbations. The data can then essentially be reduced from two-dimensional space (the plane), not to the line (R ), but to the circle which is a onedimensional manifold. For a more complex example we refer to the Ranunculoid of Fig. 1a.
An extension to the perturbed circle example is the Swiss roll as a two-dimensional manifold in R3.
In the more general case manifold learning consists in finding a smooth compact sub-manifold S of Rp on which the point cloud data may be reasonably located. "Finding" in general comprise both estimating the dimension of S and estimating S itself. But often the dimension is assumed
known.
One may estimate S by trying to cover the data cloud by a collection of balls of radius ,
such that

(9)

S^ = ni=1B(Xi, ),

where n is the number of observations and B(Xi, ) = {x : ||x - Xi||  }, and where Xi is observation number i of the point cloud. This was suggested by Devroye and Wise (1980) in another context. If the observations Xi are all exactly on S and with  depending on n, it is possible to prove convergence of S^ to S at the rate of OP (log n/n)1/r, where r is the dimension of S, and the distance between S and S^ is measured in terms of the Hausdorff distance between sets.
It is not likely that a sample will fall precisely on S. A more realistic model is that one observes Yi = Xi + i, where Xi comes from a distribution with support on S, and i are samples from a noise distribution. In this case the convergence rate of the estimation of S is very slow; see Genovese et al. (2012). An interesting example of two-dimensional data, but where there is a set S of dimension 1 with a high concentration of data, is the data set of galaxies treated in Chen et al. (2015b,c).

STATISTICAL EMBEDDING: BEYOND PRINCIPAL COMPONENTS

13

It may be possible to estimate an r-dimensional and high density region R that is close to S. One way to make this more precise is through the idea of density ridges. A density ridge is a low-dimensional set with large density.
The ridge set can then be estimated by the ridge of the kernel density estimator. The properties of this estimator is studied in Genovese et al. (2014) and Chen et al. (2015a). An algorithm for finding the ridge set estimator P^h was given by Ozertem and Erdogmus (2011).
As mentioned, in a theoretical analysis, often the dimension r of the embedding manifold is assumed known. In practice one may need to estimate r; see Levina and Bickel (2005), Little et al. (2011), and Kim et al. (2019).

4.2. Persistent homology and persistence diagrams. In our context the concept of homology can be seen as coming from a desire to answer the question of whether two sets are topologically similar. For instance is an estimate S^ of S topologically similar to S, or is it at all possible to find an estimate of S that is topologically similar to S? The answer to this question depends on what is meant by "similar".
Two sets S and T equipped with topologies are homeomorphic if there exists a bi-continuous map from S to T . Markov (1958) proved that, in general, the question of whether two spaces are homeomorphic is undecidable for dimension greater than 4.
However, it is possible to use the weaker notion of homology, and it is much easier to determine whether two spaces are homologically equivalent. Strictly speaking homology is a way of defining topological features algebraically using group theory. See e.g. Carlsson (2009) for a precise definition. Intuitively it means that one can compare connected components, holes and voids for two spaces. The zeroth order homology of a set corresponds to its connected components. The first order homology corresponds to one-dimensional holes (like a donut), whereas the second order homology corresponds to two dimensional holes (like a soccer ball) and so on for higher dimensions. If two sets are homeomorphic, then they are homologically equivalent, but not vice versa.
Homology is a main topic of TDA. To establish a link with the previous sub-section, consider the estimate S^ = ni=1B(Xi, ) of Equation (9). One of the first results about topology and statistics is due to Niyogi et al. (2008). They showed that under certain technical conditions the set S^ has the same homology as S with high probability.
In many ways topological data analysis has been identified with the subject of persistent homology. This is concerned with the homological structure of data clouds at various scales of the data, and to see how the homology changes (how persistent it is) over these various scales. Two main introductory sources are Wasserman (2018) and Chazal and Michel (2017).
The field of TDA is new. It has emerged from research in applied topology and computational geometry initiated in the first decade of this century. Pioneering works are Edelsbrunner et al. (2002) and Zomordian and Carlsson (2005). An early survey paper at a relatively advanced mathematical level but with a number of interesting and illustrative examples is Carlsson (2009). Wasserman (2018) and Chazal and Michel (2017) are somewhat less technical and more oriented towards statistics. See also Ghrist (2017).
For our purposes of statistical embedding, TDA brings in some new aspects in that topological properties are emphasized in the embedding. This is done to start with in so-called persistence diagrams which depict the persistence, or lack thereof, of certain topological features as the scale in describing a data cloud changes. In complicated situations persistence diagrams can be computed from simplical complexes. This is a particularly interesting concept since it generalizes the embedding of a point cloud in a graph. A one dimensional simplical complex can be identified with a graph, whereas generalizations allow for describing cycles and voids of the data. This is of special interest for certain types of data, such as porous media and physiological or cell data.
To introduce the persistence diagram, recall the estimator S^ in (9) as a union of balls B(Xi, ) of radius . One may question what happens to this set as the radius of the balls increases. Consider for example a data cloud that contains a number n of isolated points that resembles a circular structure. Let each point be surrounded by a neighborhood consisting of

STATISTICAL EMBEDDING: BEYOND PRINCIPAL COMPONENTS

14

a ball centered at each data point and having radius . Then initially and for a small enough radius , the set ni=1B(Xi, ) will consists of n distinct connected sets (homology zero). But as the radius of the points increases, some of the balls will have non-zero intersection, and the number of connected sets will decrease. For  big enough one can easily imagine that the set ni=1B(Xi, ) is large enough so that it covers the entire circular structure obtaining an annulus-like structure of homology 1, but such that there still may exist isolated connected sets (of homology 0) apart from the annulus. Continuing to increase the radius, one will eventually end up with one connected set of zero homology.
This process, then, involves a series of births (at -radius zero n sets are born) and deaths of sets as the isolated sets coalesce. A useful plot is the persistence diagram, which has the time (radius) of birth on the horizontal axis and the time (radius) of death on the vertical axis. The birth and death of each feature is represented by a point in the diagram. All points will be above or on the diagonal then. For the circle example mentioned above the birth and death of the hole will be well above the diagonal, and it has a time of death which may be considerably larger than its time of birth. The birth and death points of the connected components on the other hand may be quite close to the diagonal if the distances between points are small enough.
We will go through the steps of this procedure in a rather more complicated example than the circle, namely that of the noisy Ranunculoid structure of Fig. 1a. We will start by considering each of the three curves, then pair of curves and finally all three curves. The corresponding persistence diagrams are displayed in Fig. 3, and these diagrams furnish the topological embedding signature of the data, which is rather different from and presents additional information compared to the embeddings in Figures 1 and 2.
Consider first the individual curves in Figures 3b-3d (and where Fig. 3a is identical to Fig. 1a). Here, class 1, 2 and 3 in Figures 3b-3d represent the persistence diagram of the innermost to the outermost curves, respectively. The gray points represent sets of homology zero (isolated sets) and black points represent sets of homology one, i.e., one-dimensional holes. The gray column at the left is just the time of death for all the sets around the individual points as the radius for the individual neighborhoods increase. Naturally the column is highest for the outermost curve in Fig. 3d, where the distances between points are largest. The black points at the right hand side of the columns mark small holes that temporarily arise in this process due to indents in the point spreads. Probably these points would not have been there if circles had been used instead of the Ranunculoid curves. For the innermost curve there is a black point at the far right with a short lifetime. This is due to the opening in this curve, which is just great enough for there to form an annulus as the radii increase.
Next, to the diagram of the pairwise curves: The pair (1,2) consists of the two innermost curves, and the persistence diagram is displayed in Fig. 3e. The points of curve 1 can again be found. In addition at birth time zero, there is a gray point above the gray column. This is just due to the fact that there are two curves at the starting point. As time (and radii) increase the two curves coalesce and we have a death at the gray point above the gray column. The three black points being born at approximate time 6 and living for about time 6 to time 12 come from holes that are created as curve 1 and 2 are approximating each other. The explanation for the pair (2,3) is much the same. In this case it takes more time before the curves 2 an 3 coalesce, so the gray point at time zero are farther up. Here too 3 holes are formed as the curves 2 and 3 approach each other. One hole has very short lifetime, it is almost on the diagonal, where as the two others almost coincide and have far longer lifetime. This has to do with the different levels of indention on the two curves. Finally, for the pair (1,3), the gray point at zero is even farther up, reflecting the increased distance between the curves 1 and 3. Again the pattern of curve 1 is dominating as for the pair (1,2). The indents of curve 1 are small in comparison with the indents of curve 3, and this explains that it takes longer time for holes to appear as these two curves are approaching each other.
The diagram for the triple of curves (1,2,3) in Fig. 3h is roughly obtained by superposition of the pattern for the pairwise curves. There is a difference at birth time zero, though. The uppermost point for the pair (1,3) has disappeared. The explanation is obvious. The curves 1 and 2 coalesce first due to least distance between them. Curve 3 is then coalescing with the

STATISTICAL EMBEDDING: BEYOND PRINCIPAL COMPONENTS

15

y feature disappearance (death)

40

30

20

20

0

10

class 1

class 2

-20

class 3

-40 -20 0 x

20 40

0 0

dimension 0 dimension 1

10

20

30

feature appearance (birth)

(a) The raw data.

(b) Class 1.

30

20

10

0 0

10

20

30

(c) Class 2.

30

20

10

0 0

10

20

30

(d) Class 3.

30

20

10

0 0

10

20

30

(e) Classes (1,2).

30

20

10

0 0

10

20

30

(f) Classes (2,3).

30

30

30

death death

20

20

20

10

10

10

0 0

10

20

30

(g) Classes (1,3).

0 0

10

20

30

birth

(h) Classes (1,2,3).

0 0

10

20

30

birth

(i) Classes (1,2,3) from Fig. 2a.

Figure 3. Persistence diagrams for combinations of classes 1, 2 and 3.

set combined curve 1 and 2, which has a distance from curve 3 equal to the distance between 2 and 3, such that the second gray point at zero correspond to the gray point at zero for the pair (2,3).
One can also construct a persistence diagram for the more noisy curves of Fig. 2. This is shown in Fig. 3i for the triple of curves (1,2,3). The pattern is a bit more complex as is expected, but the individual points can be interpreted as before.
Note that the data set composed of the three Ranunculoids carry topological information that is revealed by the persistence diagrams embeddings of Fig. 3, but which cannot be discerned in the embeddings of Figs. 1 and 2.
The idea is that this description of a point cloud in the plane, as indicated above, may be generalized to higher dimensions and much more complicated structures with multiple holes and voids of increasing homology. The number of sets of different homologies are described by the so-called Betti numbers, 0, 1, . . .. In a non-technical jargon 0 is the number of connected

STATISTICAL EMBEDDING: BEYOND PRINCIPAL COMPONENTS

16

components (0 = n, n being the number of isolated points in the start of our example), 1 is the number of one-dimensional holes, so 1 = 1 if there is only one connected ring structure, and 0 = 1, 1 = 0 when the radius is so great that there is only one connected set altogether. The hole is one-dimensional since it suffices with a one-dimensional curve to enclose it, whereas the inside of soccer ball is two-dimensional, it can be surrounded by a two-dimensional surface, and has 0 = 1, 1 = 0 and 2 = 1. A torus has 0 = 1, 1 = 2, 2 = 1. In Fig. 3 it is a trivial exercise to find the Betti numbers (0 or 1) for any chosen interval of time (radius) of this figure.
The extension of the persistence diagram to higher dimensions and to more general structures requires relatively advanced use of mathematical tools. We only indicate some main concepts in Section 1 of the Supplement (Tjøstheim et al., 2021a). The space of persistence diagrams is not a function space, and is sometimes replaced by persistence landscapes which do form a function space and may be more amenable to machine learning and statistical analysis. The latter also brings in the need for statistical inference, and recently statistical tools like the bootstrap has been introduced in TDA. We refer to Sections 1.2 and 1.3 in the Supplement for more information and a number of references. Section 1 of that Supplement is concluded by formulating some explicit and open statistical problems in TDA.

5. Embedding of networks
In Sections 3.4 and 3.5 graphs (or networks) were used as a tool in embedding a point cloud in Rp, making it possible among other things to do cluster analysis involving non-convex clusters. In the present section the starting point is a network or collection of networks and the task is to embed the network in an Euclidean space Rm or to map it to a manifold. This is used to obtain a vector representation of each node of the network.
Why is it important to be able to embed a network in such a way? The main reason is simply that for many purposes it is easier to work with a set of n vectors than with a network consisting of n nodes. One has standard methods for dealing with vectors. For example one can do clustering of vectors, which in a social network could correspond to finding and grouping communities in the network. And one can also compare and classify networks by looking at their embedded sets of n-dimensional vectors.
With the increasing use of the internet and Big Data, the analysis of large networks is becoming more and more important. There is a very wide field of applications ranging over such diverse areas as e.g. finance, medicine and sociology, including criminal networks. A broad overview can be found in the recent book by Newman (2020). A fine detailed survey is Cui et al. (2019).
With ultra-high dimension and large very data sets, there is a need for fast methods. With the recent technique of Skip-Gram, described in some detail in Section 5.3 and in Section 2 in the Supplement (Tjøstheim et al., 2021a), one is able to handle networks with millions of nodes and billions of edges such that each node is represented by a vector of dimension 500-600, say. On such vectors one can use standard discrimination and clustering. One may also do further embedding to lower dimensional vectors, as described in Section 6, to visualize data of very high dimension.
In our survey of network embedding methods, we will start with spectral graph methods in Section 5.2 after a brief introduction on characterization of graphs in Section 5.1. The spectral method requires the solution of an eigenvalue problem, and this puts a limitation on the number of nodes and edges. This restriction is to a large degree bypassed in neural network based methods, in particular in the Skip-Gram algorithm. This algorithm was originally introduced in natural language analysis, which has independent interest in that the words in a language text can be embedded in a vector in Rm reflecting not only the word count in a text but also the syntax of the text. A language text is not a network, and therefore the detailed embedding analysis of a language text is covered in Section 2 of the Supplement. Ideas and methods developed in such a framework have proved vitally important, however, for fast and efficient embedding of networks as is demonstrated in Section 5.3. That section is chiefly concerned with symmetric undirected networks, but briefly mentioning directed networks, heterogeneous

STATISTICAL EMBEDDING: BEYOND PRINCIPAL COMPONENTS

17

networks and dynamic networks, where there are many open statistical and data processing problems, in the ensuing sections.
The research on embedding of networks has mainly been published in machine learning journals and conference proceedings. There are several issues of statistical interest, and one may think that there is a potential synergy effect that both the statistics and machine learning community could benefit from. We will try to make this more clear in the sequel.

5.1. A few elementary concepts of graph theory and matrix representations. We have already introduced some elementary graph concepts in Sections 3.4 and 3.5. In this brief introductory section we supplement these to more fully explain the spectral based clustering algorithms for networks.
We consider a graph G = (V, E), where V and E are the sets of nodes and edges, respectively. The graph is supposed to be undirected, which means that an edge goes in both directions between two neighboring nodes. Let n = |V | be the number of nodes (V, E). Then the graph can be represented by a n × n matrix M, such that an element Mij of this matrix represents some property of the pair of nodes vi and vj. When V is large, this matrix may be huge. Later, representation matrices of dimension m × n will be introduced where m << n. Diagonal elements Mii encode information of the node vi only, such as the degree of vi (number of edges emanating from vi or more generally as in Equation (5) for a weighted graph).
A simple example of such a matrix is the adjacency matrix A, which was mentioned in Section 3.5.
It is clearly desirable for network patterns to be independent of the way one labels the nodes, and one is therefore interested in quantities that can be derived from M that are invariant to permutations. One such quantity is the spectrum of M. It is well known from elementary linear algebra that the spectrum of a matrix is invariant to similarity transformations such as that produced by a permutation matrix. Unfortunately, cospectrality of two adjacency matrices does not necessarily mean that the corresponding graphs are isomorphic.1 This may not be a serious problem in practice, though, as is indicated in Wilson and Zhu (2008), and following recent literature on embedding of networks it will be ignored in the sequel.
An adjacency matrix A for an undirected graph is symmetric with real eigenvalues, both negative and positive. In many applications it is useful to have a non-negative definite matrix. Such a matrix has non-negative eigenvalues. One example of such a matrix is the Laplace matrix, a version of which was introduced in Section 3.5 for a general weighted undirected graph. It is given by

(10)

L = D - A,

where A is the adjacency matrix and D = diag(di) is the diagonal matrix having the degree of

the nodes along the diagonal.

The normalized Laplacian LNis defined by 1

if i = j

LN,ij

=

-1/ didj 0

if i and j are adjacent otherwise.

This matrix can also be written LN = D-1/2LD-1/2. It is non-negative definite and it has all its eigenvalues 0    2.

5.2. Spectral embedding and graph clustering. A basic task in network clustering is community structure detection. It is perhaps best thought of as a data technique used to throw light on the structure of large-scale network data sets, such as social networks, web data networks or biochemical networks. It is normally assumed that the network of interest divides naturally into subgroups, and the task is to find those groups.
For the purpose of community grouping and division a criterion is required that can measure both the internal structure within each group, where the goal is to maximize the dependence

1Two graphs G and H are isomorphic if there exists a bijection f from G to H such that two nodes u and v in G are adjacent if and only if f (u) and f (v) are adjacent in H.

STATISTICAL EMBEDDING: BEYOND PRINCIPAL COMPONENTS

18

between members of a group, but such that the dependence between each group is minimized. There are two main methods for doing this, either by minimizing the so-called cut between the groups, the mincut problem or by maximizing the modularity. Both are discussed below using network spectral embedding.

5.2.1. Minimizing the cut functional. A useful tutorial on spectral clustering is given by Luxburg

(2007). A more recent alternative account is given in Zheng (2016).

Given a graph G = (V, E) with adjacency matrix A we will like to find a partition of V in

groups V1, . . . , Vk such that the number of edges between each group is minimized. This leads

to

the Let

Wmi(nVciu,tVjp)ro=.bl21em. mVi,lVj

wml,

where

wml

is

the

weight

for

the

edge

between

the

nodes

vm and vl. In the un-weighted situation wml is 1 if there is an edge between vm and vl and 0 if

not. Let V¯i be the complement of Vi. The mincut approach to clustering is simply defined for

a given k by choosing the partition V1, . . . , Vk which minimizes the normalized cut size

NCut(V1, . . . , Vk)

=.

1 2

k i=1

W (Vi, V¯i) vol(Vi)

=

k i=1

cut(Vi, V¯i vol(Vi)

)

,

where vol(Vi) = vlVi dl, dl being the weighted degree of vl. A similar criterion is the RatioCut criterion.
The normalized Laplace matrix can be written as LN = D-1/2LD-1/2. Let H be the n × d matrix whose columns are the d eigenvectors corresponding to the d first (non-zero) eigenvalues
of LN . The n d-dimensional row vectors of H then constitute an embedding of the nodes of the graph minimizing the normalized cut-functional of the graph. These embedding vectors
are then used as a point of departure for clustering and classification of nodes.

5.2.2. Maximizing the modularity. Modularity is an alternative concept in the use of spectral

methods in clustering. Modularity was introduced by the highly cited papers of Girvan and

Newman (2002) and Newman and Girvan (2004), and after that has been further developed as

in Newman (2006). See also Bickel and Chen (2009) for an alternative using a nonparametric

point of view.

It was seen in the previous sub-section that the principle underlying the cut-size algorithms

is that a good division of a network is one in which there are few edges between communities.

Newman (2006) states that this is not necessarily what one should look for. He argues that a

good division is one in which there are fewer than expected edges between communities.

This idea, then, is quantified using the measure of modularity. Assume first that there are

two potential classes. Again we suppose that the network contains n = |V | nodes, and we

introduce the vector s, whose ith component is given by si = 1 if node vi belongs to group 1 and si = -1 if it belongs to group 2. The edge between nodes vi and vj is characterized

by the adjacency matrix A. The element Aij then represents the "number of edges" between

vi and vj. The expected number of edges between vi and vj if edges are placed at random is

di dj /2m,

where

di

and

dj

are

the

degrees

of

the

nodes

and

m

=

1 2

i di (undirected network).

The modularity is then defined by

Q

=

1 4m

Aij

-

didj 2m

sisj

=

1 4m

sT

Bs,

ij

where the matrix B is defined by

Bij

=

Aij

-

didj 2m

.

This is easily generalized to the case of k classes, and the modularity is maximized by computing

the eigenvectors of the B matrix. Corresponding to H, let S be the n×d matrix whose columns

are the eigenvectors corresponding to the top d eigenvalues of B. The n d-dimensional row

vectors of S then constitute an embedding of the n nodes of the network maximizing the

modularity.

STATISTICAL EMBEDDING: BEYOND PRINCIPAL COMPONENTS

19

5.2.3. The Louvain method for community detection. The so-called Louvain method for community detection based on modularity was introduced in a paper by Blondel et al. (2008). They start with a network with n nodes, and where each node defines a community. Then one goes successively through the nodes of the net and for each node vi, with neighbors vj one investigates the gain in modularity if vi is removed from its community and placed in the community of vj. The node vi is then placed in the community for which this gain is maximum (in case of a tie, a breaking rule is used). An updating formula for the change in the modularity Q is given in Blondel et al. (2008). This is continued until the whole graph has been covered. In the next round the procedure in the first round is repeated, but this time with the communities formed in the first step as entities. This is continued until there is no increase in Q.
There is no eigenvalue problem that needs to be solved in this algorithm. This makes it possible to apply the Louvain algorithm for substantially larger networks. One example that the authors refer to is a mobile phone company with a network composed of 2.6 million users.

5.3. Embedding a network using Skip-Gram. It should be noted at first that for large networks the cut-size spectral clustering method and the modular method (possibly with the exception of the Louvain method) run into problems because it is costly to solve eigenvalue problems for the high dimensions that may occur in network embedding.
These problems are to a large degree alleviated in a neural net based Skip-Gram procedure. This procedure was first developed in word embedding in a language text (from this the nomenclature "Skip-Gram"). Here the eigenvalue problem is eliminated altogether, and the neural net training is speeded up using so-called negative sampling or hierarchical processing. At this point the reader may wish to browse through Section 2 in the Supplement (Tjøstheim et al., 2021a), which contains a relatively detailed account of natural language embedding. This may, we believe, be of some independent interest. An effort will be made to make the current section on embedding of networks self-contained, just leaving some details to Section 2 of the Supplement.
What is needed, then, to extend word processing to networks where words are replaced by nodes and the vocabulary with the network itself? The simple answer is the concept of a neighborhood.
In natural language processing, defining a neighborhood of a word in a text is not difficult: simply taking n1 and n2, ni  0 context words in front and after the word respectively. One may think that a corresponding neighborhood around a node is easily defined, but not quite so, because here there is no natural "past" or "future".
Before embarking on the neighborhood problem, partly to define notation, let us formally write up the analog of the Skip-Gram model, presented in some detail in the language analysis in Section 2 of the Supplement, for a network. The notation N (v) is used for the neighborhood of a node v  V in a network G = (E, V ). Neighborhoods are more precisely defined in Section 5.3.2. The analysis to be presented next applies mainly to the static undirected case. Extensions to directed, heterogeneous and dynamic networks are briefly discussed in separate sub-sections.
We let f be the mapping from V to the embedding feature space Rm. The goal is to associate each node v in V with a feature vector f (v) in Rm. When representing the whole network in this way we obtain a n × m matrix with n = |V |.

5.3.1. The Skip-Gram. We proceed to formulate the Skip-Gram architecture for an undirected symmetric network. One seeks to optimize an objective function in finding a representation f (v) such that the conditional probability for obtaining individually the elements in N (v), given an input node v, is maximized; i.e, find f such that

(11)

log P (N (v)|f (v))

vV

is maximized. The maximization is done by training a one-layer hidden neural network which has as possible
inputs n vectors, one for each node in the network. A fixed input vector has as desired output

STATISTICAL EMBEDDING: BEYOND PRINCIPAL COMPONENTS

20

a probability distribution on the nodes. It should be concentrated as well as possible to the neighbors (suitably defined) of the input node. The idea is to train the neural net through its hidden layer so that this is achieved to the highest possible degree. Only linear transformations are used from the input layer to the hidden layer and essentially also from the hidden layer to the output, although a logistic type transformations is used to transform the outputs to probabilities. A few basic facts of neural networks are given in Section 2.1 of the Supplement (Tjøstheim et al., 2021a).
The training is done successively by going through this process for each input node several times and is stopped when the deviation from the obtained probability distribution on the outputs is close enough to the ideal desired one, which is completely concentrated on the sought neighboring nodes. At each step of this procedure each node has an input vector representation and an output vector representation. It is the output vector representation that is of interest since it describes the relation between a node and its neighbors. This training process strives to maximize the function in (11).
To make this optimization problem tractable, the following two assumptions are made (not always made explicitly in the language processing papers).
1) Conditional independence: The conditional likelihood is factorized as

(12)

p(N (v)|f (v)) =

P (ni|f (v)).

niN (v)

2) Symmetry in feature space and softmax: A source node and a neighborhood node have a symmetric effect on each other in the embedding feature space. Accordingly, the conditional likelihood for every source-neighborhood pair is modeled as a softmax unit, parameterized by a dot product of their features

(13)

P (ni|f (v)) =

exp(f (ni) · f (v)) uV exp(f (u) · f (v))

.

This is nothing but a suitable parametrization of the multinomial logistic regression model, but in the data science literature "softmax unit" is preferred. Formula (13) may be compared to the development in Section 2.4 in the supplementary material (Tjøstheim et al., 2021a).

With the above assumptions and taking logarithms in (13), the objective function in Equation (11) simplifies to

(14)

max - log( exp(f (v) · f (u))) +

f (ni) · f (v) .

f

vV

uV

niN (v)

In the training of the neural net one avoids solving a high dimensional eigenvalue problem,

but there is an obvious computational issue involved. As the size of the network increases

with n, the neural net with the associated input and output vectors representations becomes

heavy to update. For each step of the training, in principle, all of these representations have

to be updated. The updating of the node input vectors is cheap, but learning the output

vectors, which are the vectors of interest, is expensive. For each training instance one has to

iterate through every node of the network, cf. the summation over u in (13) and (14), compute

the output and the prediction error and finally use the prediction error in a gradient descent

algorithm to find the new output vector representation.

The idea of negative sampling, first introduced in Mikolov et al. (2013) in text analysis, makes

the training process amenable by not sampling over the entire network for each update of a

node, but rather a small sample of nodes. Obviously, the output nodes in the neighborhood of a

given node should be included in the update sample, i.e., the last sum of (14). They represent

the ground truth and are termed positive samples. In addition a small number k of nodes

(noise or negative samples) should be updated. Mikolov et al. (2013) suggest that k = 5 - 20 are useful for small training sets, whereas for large training sets k = 2 - 5 may be sufficient,

see Section 2.6 of the Supplement for more details (Tjøstheim et al., 2021a). The sampling

is via a probability mechanism where each word (node) is sampled according to its frequency

STATISTICAL EMBEDDING: BEYOND PRINCIPAL COMPONENTS

21

in the text. It will be seen below how this can be done in the network case. In addition, Mikolov et al. (2013) recommends, from empirical experience, that in the further analysis each frequency should be raised to the power of 3/4 (cf. again Section 2.6 of the Supplement). This seems also to have been adopted in the network version of negative sampling. Clearly, a more thorough statistical analysis, also including the choice of k, would be of interest.
We will return to the question of negative sampling in the next sub-section, where a sampling strategy S is introduced for creating neighborhoods of a node v, such that the neighborhood NV (S) depends on S.

5.3.2. Neighborhood sampling strategies. Various authors have suggested different sampling strategies of the nodes of a network. We will go through three main strategies which seem to be representative of this field as of the last 5 years. All of these contain parameters for which, to our knowledge, an optimality theory is lacking.
Perozzi et al. (2014) device a sampling strategy they call "DeepWalk". Consider a node v, and denote by wvu the weight of its (undirected) edge with another node u. Let the degree variable be dv = u wvu. Then start a random walk from v by letting it choose the one-step neighbor u with probability P (u|v) = wuv/dv. Next, repeat this for the node u, and so on until L steps, say, have been obtained. The walk may return to v for one or more of its steps. This procedure is now repeated  times obtaining  random walks starting in v. These may be compared to text segments in natural language processing. Analog to a moving window in a language text we now let a window of size 2K + 1, where 2K + 1  L, glide along the random walk paths. For each window, there is a center node numbered u , K  u  L - K, and we define a neighborhood NS(u ) and K nodes prior to u and K nodes after u in the considered random walk path. For each such configuration we apply the Skip-Gram procedure (11) - (14). In this way, for each node v we generate  × (L - 2K) segments of nodes. Note that this creation of segments in paths of random walks can be carried out before the optimization process takes place. When applied to all of the nodes of the network it results in a collection of n ×  × (L - 2K) segments of nodes that correspond to windows of words in a language text. This sets up a frequency distribution over the nodes corresponding to the frequency distribution of words in the vocabulary in a text. Negative sampling of nodes can then be applied to this frequency distribution of nodes.
In the DeepWalk set-up the random walk can only go to one of the nearest neighbors in the next step with the probability P (u|v) = wvu/dv. Grover and Leskovec (2016) argue that a combination of so-called breadth first sampling, BFS and depth first sampling, DFS, should be used.
The LINE (Large-scale Information Network Embedding) was introduced by Tang et al. (2015b). They use a slightly different optimization criterion than (12). Somewhat similarly to Grover and Leskovec (2016) LINE introduces the concepts of first and second order proximities. In each of these papers on sampling strategies there are a number of comparative experiments on information networks such as Wikipedia, Flickr, YouTube to evaluate the properties of each method.
Qiu et al. (2018) obtain a unifying view of the DeepWalk and LINE among other algorithms. In the recent Qiu et al. (2019) they look at the practical and algorithmic aspects of an implied matrix factorization with associated sparse matrices, resulting in the algorithm NetSMF. Software packages are available for all of the algorithms mentioned in this section.

5.4. Directed network. In many applications of networks one deals with a directed network, e.g. in causality networks. This is a network where the weight on edges between nodes vi and vj may be different, so that wij = wji, and one may even have wij > 0 but wji = 0. Rohe et al. (2016) have looked at this from a spectral graph point of view. For a directed graph the adjacency matrix, giving the edge weights wij, is not symmetric. When the adjacency matrix is not symmetric, the left and right eigenvectors are in general not the same. This can be given an interpretation as "sending" and "receiving" nodes, and it can be argued that these should be clustered separately.

STATISTICAL EMBEDDING: BEYOND PRINCIPAL COMPONENTS

22

Directed graphs have also been attempted incorporated in the Skip-Gram procedure, see
e.g. Zhou et al. (2017, p. 2944). The undirected sampling strategy described in Section 5.3.2
can again essentially be used. To illustrate, let wij be the weight of the edge in a transition from vi to vj. In a money laundering investigation, for example, where the nodes may be bank accounts, wij may be proportional to the number of transactions from account i to account j. Similarly, one may define wji. The probability of going from node vi to vj can then be given as pij = wij/di, where di = jNS(i) wij and NS(i) is the first order neighborhood of vi. This is extendable to higher order neighborhoods as in Grover and Leskovec (2016).

5.5. Heterogeneous network representation. Heterogeneous here refers to a situation where there are different types of nodes in a network, and there may be different types of edges. If these are treated with homogeneous techniques neglecting the heterogeneity, inferior results may result.
Two papers will be briefly mentioned, one is an extension of the LINE approach, the other is an extension of the DeepWalk methodology. In these two papers the Skip-Gram algorithm is applied on so-called metapaths, paths consisting of a sequence of relations defined between different node types. The introduction of metapaths to heterogeneous graphs came before the Skip-Gram procedure. See Sun et al. (2012).
It is natural also to mention the extension of LINE found in the PTE (Predictive Text Embedding) of Tang et al. (2015a). PTE deals with a text network embedding, but the method is applicable to a general network.
In Dong et al. (2017) the authors introduce a form of random walk sampling for heterogeneous networks which is analogous to or extends the sampling procedures in Perozzi et al. (2014) and Grover and Leskovec (2016). Skip-Gram is combined with the metapath sampling as discussed by Sun et al. (2012).
Although there are different types of nodes in V , their representations are all mapped into the same latent space Rm.

5.6. Embedding of dynamic networks. Most of the work on embedding of networks has been done on static networks. There is no time dimension involved to trace the dynamic evolution of the network. In many situations this is of course not very realistic. Consider for example a bank network. New accounts are opened, other accounts are closed. New types of transactions between accounts are appearing, others are becoming old and less relevant. Or in more general network language: New nodes are coming into the network, others are removed. New edges are created, others are discarded. Weights between edges may easily change in time. In a heterogeneous network new types of nodes may enter the system, others may leave. An early empirical investigation of changes in social networks is contained in Kossinets and Watts (2006). See also Greene and Cunningham (2011).
An obvious brute force solution is to use a moving window and then do an embedding, and possible clustering in each window. But clearly such a procedure is time consuming and nonefficient if there are many (overlapping) windows. One would like to have an updating algorithm that can keep information in the previous window and combine it with new information in the new window. To our knowledge the literature here is quite limited.
Zhou et al. (2018) consider triads as basic units of a network. A triadic closure process is aiming to capture the network dynamics and to learn representation vectors for each node at different time steps.
There is also a recent attempt to generalize the entire Skip-Gram methodology to a dynamic framework. This can be seen in Du et al. (2018). They utilize that a network may not change much during a short time in dynamic situations, thus the embedding spaces should not change too much either. A related paper venturing into heterogeneous networks meta paths is Bian et al. (2019). Zhu et al. (2017), takes a more statistical modeling point of view on dynamic networks. The paper is briefly reviewed in the next sub-section. Clearly, the theme of dynamic networks is an open and challenging field for data scientists and statisticians.

STATISTICAL EMBEDDING: BEYOND PRINCIPAL COMPONENTS

23

5.7. Network embedding: Data science versus statistical modeling. Virtually all of the literature on network embedding can be found in the machine learning literature and in proceedings on data and computational science. The emphasis has been on deriving methods that "work", i.e. can be used in practical applications. Some of these results obtained are quite ad hoc such as the argument in Mikolov et al. (2013) where from empirical evidence the word count is raised to 3/4 power in the distribution forming the basis of the negative sampling. This has been followed up in later literature and does seem to work well. But it is not clear why. Moreover, there are few quantitative expressions of uncertainty or on statistical properties of the obtained results.
This seems to lead to a gap between data/computational science and more traditional (and modern) statistical thinking. There is a clear need for results bridging this gap. This should be helpful for both disciplines. There have been some attempts in this direction. Some early key publications are Hoff et al. (2002) and Bickel and Chen (2009).
A recent example is the paper by Zhang and Chen (2018). They have extended the modularitybtioned in Section 5.2 to the heterogeneous case. The paper is different from most of the other papers discussed on network embedding in that it contains a rigorous statistical inference. For instance, the proposed modularity function is shown to be consistent in a heterogeneous stochastic block model framework. It is related to the Bickel and Chen (2009) paper.
Another recent example of rigorous statistical modeling of dynamic networks is Zhu et al. (2017). They model the network structure by a network vector autoregressive model. This model assumes that the response of each node at a given time point is a linear combination of (a) its previous value, (b) the average of connected neighbors, (c) a set of node-specific covariates and (d) independent noise. More precisely, if n is the network size, Y letit be the response collected from the ith subject (node) at time t. Further, assume that a q dimensional node-specific random vector Zi = (Zi1, . . . , Ziq)T  Rq can be observed. Then the model for Yit is given by

(15)

n
Yit = 0 + ZiT  + 1n-i 1 aij Yj,t-1 + 2Yi,t-1 + it.
j=1

Here, ni = j=i aij, aii = 0, is the total number of neighbors of the node vi associated with

Yi, so n-i 1

it is the degree

n j=1

aij

Yj,t-1

is

of vi. The term 0 + ZiT  the average impact from

is the impact of covariates on node vi, whereas the neighbors of vi. The term 2Yi,t-1 is the

standard autoregressive impact. Finally the error term it is assumed to be independent of the

covariates and iid normally distributed.

Given this framework, conditions for stationarity are obtained, and least squares estimates

of parameters are derived and their asymptotic distribution found.

They give an example analyzing a Sina Weibo data set, which is the largest twitter-like social

medium in China. The data set contains weekly observations of n = 2,982 active followers of

an official Weibo account.

An extension of the model (15) is contained in Zhu and Pan (2020).

There are a number of differences between the network vector autoregression modeled by

Equation (15) and the dynamic network embeddings treated in Section 5.6. First of all, (15)

treats the dynamics of the nodes themselves and not of an embedding. Even if the autoregressive

model do introduce some (stationary) dynamics in time, the parameters are static; i.e. no new

nodes are allowed, and the relationship between them is also static as modelled by the matrix

A = {aij}. From this point of view, as the authors are fully aware of, the model (15) is not

realistic for the dynamics that takes place in practice for many networks. On the other hand the

introduction of a stochastic model that can be analyzed by traditional methods of inference

is to be lauded. A worthwhile next step is to try to combine more realistic models with a

stochastic structure (regime type models for the parameters?) that is amenable to statistical

inference.

STATISTICAL EMBEDDING: BEYOND PRINCIPAL COMPONENTS

24

6. Embedding in 2 or 3 dimensions and visualization
Visualization is an important part of data analysis. The problem can be stated as finding a good 2- or 3-dimensional representation of high dimensional data and often with a large number of samples. Principal component analysis offers one possibility where the data are projected on the 2 or 3 first principal components. Although very useful, since it is linear and projects on a hyper plane, it generally fails to give a good characterization in cases where the data are concentrated on a nonlinear manifold which is a subset of Rp.
It is convenient to conclude this survey on embedding with visualization because we will base ourselves on three methods that are powerful and much used, and which are based on main ideas in Sections 3, 4 and 5, respectively. The t-SNE algorithm was developed by van der Maaten and Hinton (2008) and van der Maaten (2014). It is based on ideas handling the connection between a high dimensional x-scale and a low dimensional y-scale which are inherent already in multidimensional scaling. But unlike most earlier attempts t-SNE is based on comparisons of probability distributions on the x and y-scale, which seems much more sensible in a nonlinear problem than applying moments and covariances.
Tang et al. (2016) introduced LargeVis which is based on techniques reviewed in Section 5, especially the Skip-Gram procedure treated in Section 5.3. Finally, McInnes et al. (2018) use methods from topological data analysis akin to ideas in Section 4 to derive their algorithm UMAP. Illustrations of the use of the three methods are given in Section 6.4.

6.1. t-SNE. SNE is an acronym for Stochastic Neighbor Embedding. That embedding and visualization technique was introduced by Hinton and Roweis (2002). The t in t-SNE refers to further developments in van der Maaten and Hinton (2008) using a t-distribution approximation on the y-scale.
Starting with SNE, the similarities between the points on the x-scale and y-scale is sought expressed in terms of pairwise Gaussian approximations. On the x-scale high dimensional Euclidean distances are expressed in conditional probabilities. The similarity of a data point Xi to a data point Xj is expressed as a Gaussian conditional probability pj|i such that for pairs of nearby data points, pj|i would be relatively high, whereas for widely separated points, pj|i could be infinitesimally small. The essential idea is to preserve the internal structure of the high-dimensional data by keeping similar data points close and dissimilar data points far apart, in the low-dimensional space. Mathematically pj|i is given by

(16)

pj|i = pj|i(xj |xi) =

exp(-||xj - xi||2/2i2) k=i exp(-||xk - xi||2/2i2)

,

where i2 is the variance of the Gaussian that is centered on the data point xi. The parameter i is chosen so that the probability distribution Pi, induced by pj|i for all j-s different from i, has a perplexity specified by the user. Here the perplexity of Pi is given by

Perpi = 2- . j pj|i log2 pj|i

See Hinton and Roweis (2002) for more details. The similarities on the x-scale is sought mapped into corresponding similarities in the low
dimensional y-scale by modeling the conditional probabilities by

qj|i =

exp(-||yj - yi||2) k=i exp(-||yk - yi||2)

.

The coordinates Yi of a data point Xi, i = 1, . . . , n are then sought determined by minimizing

the Kullback-Leibler distance (or cross entropy) between the pj|i and qj|i, i.e. by minimizing

the cost function

C=

i

KL(Pi||Qi) =

i,j

pj|i

log

pj|i qj|i

.

The minimization of the cost function with respect to the y-coordinates can be done by using a gradient descent method.

STATISTICAL EMBEDDING: BEYOND PRINCIPAL COMPONENTS

25

The SNE algorithm is hampered by a cost function which is quite difficult to optimize in
practice, and there is a so-called "crowding" problem in the sense that far apart points on the
x-scale may be mapped in such a way that the joint probability qij may be even smaller than pij. These problems are attacked in t-SNE by symmetrization, modeling joint probabilities pij and qij and by using a t-distribution as an approximation at the y-scale having points in the tails mapped such that qij is larger than pij to avoid the crowding effect. This trick is also present for other local techniques for multidimensional scaling.
To avoid problems that may be caused by outliers on the x-scale the "joint probabilities" on
the x-scale are in fact computed as pij = (pi|j + pj|i)/2n, which ensures j pij > 1/2n for all data points Xi, such that each data point makes a significant contribution to the cost function. Further, on the y-scale a t distribution structure of one degree of freedom is used,

qij =

(1
k=

+ (1

||yi - yj||2)-1 + ||yk - y ||2)-1

,

where it should be noted that a double sum is now used in the denominator. The cost function

is given by

C=

i,j

pij

log

pij qij

.

The details of the optimization can again be found in van der Maaten and Hinton (2008). In that paper there is also a series of experiments comparing t-SNE with the Sammon mapping of MDS and the ISOMAP and LLE, where the t-SNE does extremely well.
The t-SNE algorithm is speeded up in the paper by van der Maaten (2014) by not going over all possible pairs (xi, xj) but only essentially over nearest neighbors.

6.2. LargeVis. Tang et al. (2016) propose a new algorithm for visualization, LargeVis. It

starts with a speeded up approximate nearest neighbor algorithm that has complexity O(n)

as compared to O(n log n) for the speeded up nearest neighbor algorithms of van der Maaten

(2014). The Tang et al. (2016) algorithm is built upon random projection trees but significantly

improved by using neighbor exploring. The basic idea of this, similarly to the LINE construct

in Tang et al. (2015b) and referenced in Section 5.3.2, is that "the neighbor of my neighbor is

also likely to be my neighbor". Specifically, a few random projection trees are built to construct

an approximate k-nearest neighbor graph, the accuracy of which may not be so high. Then

for each node of the graph, the neighbors of its neighbor are searched, which are also likely

to be candidates of its nearest neighbor. The accuracy may then be improved by multiple

iterations. The claim is that the accuracy of this k-nearest neighbor graph quickly improves to

almost 100% without investing in many trees. For the weights of the nearest neighbor graph

essentially the same procedure as in t-SNE is used. The graph is symmetrized by setting the

weights between xi

and xj

to wij

=

, pj |i +pi|j
2n

where

pi|j

and pj|i

are defined via (16).

Before

using the LargeVis algorithm itself a pre-processing step can be used where the dimension is

reduced to say 100 by using the Skip-Gram network embedding technique explained in Section

5.3. The negative sampling technique of Mikolov et al. (2013) is used in the Skip-Gram step.

For the time complexity of the optimization, done with asynchronous stochastic gradient

descent, each stochastic gradient step takes O(sM ), where M is the number of negative samples,

say M is from 5 - 10, and s is the number of dimensions of the low dimensional space, s = 2, 3.

Therefore the overall complexity is O(sM n), which is linear in the number of nodes.

6.3. UMAP. Sections 4.1 and 4.2 were concerned with topological methods in manifold learn-
ing and persistence homology. In particular, filters of simplicial complexes were used in Section
1.2 of the Supplement (Tjøstheim et al., 2021a). In the first part of McInnes et al. (2018), these
filters are generalized to simplicial sets. In addition, components of fuzzy set theory, category
theory and functor theory are used to compute a fuzzy topological representations. Letting {Y1, . . . , Yn}  Rm and {X1, . . . , Xn}  Rp with m p, in visualization we have a
situation where m is 2 or 3.

STATISTICAL EMBEDDING: BEYOND PRINCIPAL COMPONENTS

26

To compare two fuzzy sets generated by {X1, . . . , Xn} and {Y1, . . . , Yn}, respectively, fuzzy set cross entropy is used in UMAP. The use of advanced concepts of algebraic topology makes the first part of this paper hard to read. In the computational part of the paper, however, inspired by motivations and ideas of the first part, the authors specialize to a k-neighborhood graph situation where the analogy with t-SNE and LargeVis is easier to appreciate.
As with other k-neighbor graph based algorithms, UMAP, can be described in two phases. In the first phase a particular weighted k-neighbor graph is constructed. In the second phase a low dimensional layout of this graph is made. The theoretical basis for UMAP in the first part of McInnes et al. (2018) provides novel approaches to both of these phases.
Let {X1, . . . , Xn} be the input data set with a jointly given matrix D that can be thought of as consisting of Euclidean distances between the data vectors. For each Xi one can compute the set of k nearest neighbors {Xi1, . . . , Xik }. There are many choices of a nearest neighbor algorithm. McInnes et al. (2018) use the algorithm of Dong et al. (2018).
This can be used to define a weighted directed graph G = (V, E, w). The nodes of G are the set {X1, . . . , Xn} the directed edges are {(Xi, Xij )|1  j  k, 1  i  n} and a weight function defined in McInnes et al. (2018). Let A be the weighted adjacency matrix of G . An undirected graph G is obtained by introducing the symmetric adjacency matrix
B = A + AT - A  AT ,
where  denotes the Hadamard (pointwise) product. The {X1, . . . , Xn} data set is next connected to a low dimensional data set {Y1, . . . , Yn},
where the dimension is 2 or 3 if visualization is considered. The transition from {X1, . . . , Xn} to {Y1, . . . , Yn} is accomplished by a force directed graph layout algorithm. The history of this kind of graph layout goes far back, Tutte (1963). A more recent account can be found in Kobourov (2012). The details of the algorithm as used in UMAP with an iterative application of attractive and repulsive forces are given in McInnes et al. (2018, p. 14). It should be noted that the terminology of attractive and repulsive forces is used in van der Maaten and Hinton (2008) as well, but unlike their paper where there is a random set-like initialization, in UMAP a spectral layout (cf. Sections 3.5 and 5.2) is used to initialize the embedding. This is claimed to provide faster convergence and greater stability within the algorithm. It should be noted that also for the implementation of their algorithm negative sampling, as treated in Section 5.3, plays an important role in reducing the computational burden.
A number of experiments were performed in McInnes et al. (2018) with a comparison to t-SNE and LargeVis. The UMAP works on par with or better than these algorithms for those examples.
All of the embedding algorithms have been demonstrated to work well in a number of quite complicated situations. Nevertheless, as pointed out by McInnes et al., it is important to be aware of some weaknesses of these algorithms that could create fruitful challenges for further research.
t-SNE, LargeVis and UMAP all lack the strong interpretability of PCA and it is difficult to see that something like a factor analysis can be performed.
One of the core assumptions is that it is assumed that there exists a lower dimensional manifold structure in the data. If this is not so, there is always the danger that a spurious noise driven embedding can be the result. This danger is reduced as the sample size increases. Developing an asymptotic analysis and finding more robust algorithms is clearly a challenge.
For all three algorithms a number of approximations are made, such as the use of approximate nearest neighbor algorithms and negative sampling used in optimization. Particularly for small sample sets the effect of these approximations may be non-negligible.

6.4. An illustrating example. The illustrating example consists of two networks, each having two different types of nodes (colored red and blue, respectively) corresponding to two different communities. The first one, the homogeneous graph in Fig. 4a, is very simple and is simulated from a degree corrected stochastic block model (Karrer and Newman, 2011), with 2 communities, 100 nodes, average node degree d = 10, ratio of between-community edges over

STATISTICAL EMBEDDING: BEYOND PRINCIPAL COMPONENTS

27

within-community edges  = 0.4, and node degree of 1 with probability 1 -  = 0.99 and node degree of 0.2 with probability of  = 0.01. In other words, the number of edges per node is Poisson distributed with expected number of edges of 1 (with a probability of 0.99) and 0.2 (with a probability of 0.01), respectively. This simple network has very little overlap between the two types of nodes.
The second one is somewhat more complex, the heterogeneous graph in Fig. 4b, and is simulated from three degree corrected subgraphs a, b and c, each with 2 communities and node degree of 1 with probability 1 -  = 0.99 and node degree of 0.2 with probability of  = 0.01:
Graph a: 30 nodes, average node degree d = 7, ratio of between-block edges over withinblock edges  = 0.2
Graph b: 30 nodes, average node degree d = 15, ratio of between-block edges over withinblock edges  = 0.4
Graph c: 40 nodes, average node degree d = 7, ratio of between-block edges over withinblock edges  = 0.2, and an unbalanced community proportion; a probability of 3/4 for community 1 and a probability of 1/4 for community 2
To link graphs a, b and c, some random edges are added between nodes from the same community2.
The purpose of the illustrating example is to examine how well these network structures are managed by t-SNE, LargeVis and UMAP, how robust they are to parameter choices inherent in the three methods, and how they compare with traditional principal component analysis (PCA) visualization.
The visualization is done in two steps. First the networks are embedded in Rm with m = 64 using the Skip-Gram routine node2vec with (cf. Section 5.3.2) L = 30 nodes in each random walk and  = 200 walks per node, and a word2vec window length of K = 5 where all nodes are included. The second step is to reduce the point cloud in R64 to R2, i.e., the visualization step using PCA and the three visualization algorithms with a selection of different tuning parameters. (In t-SNE, p is the perplexity parameter; in LargeVis n is the number of negative samples, p the total weight of positive interactions; in UMAP n is the number of nearestneighbors, m is a distance parameter, where low m gives clumpier embeddings.) The results are given in Figs. 4a and 4b.
Underneath the figures are given classification scores for the two types of nodes (communities) in the study. These are classified on a neighborhood basis. In the first line of each sub-table the class of a node is determined using the average of the 5 nearest neighbors; in the second by the majority vote among these 5 nearest neighbors. The first column "org embedding" gives the classification results for the 64-dimensional embedding in step 1.
For the simple network, PCA does well, on par with the three other visualization algorithms, both visually and in the classification. The tuning parameters does not seem to make much of a difference with the exception of t-SNE with p = 5. For the more complicated network, PCA is in trouble both visually and with respect to classification. In this case the dependence on tuning parameters seems to be greater, but most of the visualizations manage to pick out the three subgraphs a, b, and c. For all values of the tuning parameters t-SNE, LargeVis and UMAP all do clearly better than PCA. Somewhat surprisingly, perhaps, the embedding in 64 dimensions gives result not very different from those of the three visualizations routines. We also did experiments with other embedding dimensions ranging from 2 to 256. Again the classification results were not much different. This could be due to the fact that the number of nodes and links in these experiments are very modest compared to the real data experiments in the Skip-Gram references given in Sections 5.3.1 and 5.3.2, which has number of nodes and links of an entirely different order. A more involved illustrating example (but still with a moderate number of nodes) is given in Section 3 of the Supplement (Tjøstheim et al., 2021a).

2For each pair of nodes between a pair of graphs, say Graph a and c, a new link is randomly sampled with a probability of 0.01, and links connecting two nodes from the same community are kept.

STATISTICAL EMBEDDING: BEYOND PRINCIPAL COMPONENTS

28

Graph
community 1 2

y

2D visualizations

pca 2.5

tsne(p=10) 10
5

tsne(p=25) 6
3

tsne(p=5) 20

0.0 -2.5
-4 -2 0

0

0

-5 -3

-10

24

-6 -10 -5 0 5 10

0 -20 -2.5 0.0 2.5 5.0 -30 -20 -10 0 10 20

umap(n=25,m=0.01) 2

umap(n=25,m=0.75) 4

1

2

0

0

-1

-2

-4

-2 -1 0 1 2

-2

0

2

umap(n=5,m=0.01)

2 1 0 -1 -2

-2

0

2

umap(n=5,m=0.75)
2.5 0.0 -2.5 -5.0
-5.0 -2.5 0.0 2.5

largeVis(n=15,p=5) 10
5 0 -5 -10
-10 -5 0 5

largeVis(n=15,p=50)

largeVis(n=5,p=5)

15

0

10

-5

5

-10

0

-15

-5

-20

-10

-15 -10 -5 0 5 -10 -5 0 5 10

x

largeVis(n=5,p=50) 10
0 -10 -5 0 5 10 15

Classification scores

org_embedding pca tsne(p=10) tsne(p=25) tsne(p=5) umap(n=25,m=0.01) umap(n=25,m=0.75) umap(n=5,m=0.01) umap(n=5,m=0.75) largeVis(n=15,p=5) largeVis(n=15,p=50) largeVis(n=5,p=5) largeVis(n=5,p=50)

prop community correct

0.814

0.864

0.776

0.852

0.772

0.88

0.844

0.808

0.82

0.902

0.872

0.846

0.818

prop majority vote correct

0.94

0.91

0.82

0.91

0.84

0.94

0.93

0.85

0.89

0.94

0.91

0.88

0.9

Graph

(a) Homogeneous graph from degree corrected stochastic block model.

2D visualizations

pca 5.0
10 2.5

0.0

0

-2.5

-10

tsne(p=10)

-5.0 -6 -3 0 3
umap(n=25,m=0.01) 5.0

-20

-10

0

10

umap(n=25,m=0.75) 5.0

2.5

2.5

0.0

0.0

-2.5

-2.5

-5.0 -2 -1 0 1 2

-4 -2

0

2

largeVis(n=15,p=5)

largeVis(n=15,p=50) 0

-5 0
-10

-10

-15

-20

-10

0

0

10

20

tsne(p=25)

tsne(p=5)

4

30

2

20

10

0

0

-2

-10

-20 -4

-4

0

4

-20 -10 0 10 20

umap(n=5,m=0.01) 6

umap(n=5,m=0.75) 6

4

3

2 0
0 -3
-2

-4 -2 0 2 4 6

largeVis(n=5,p=5)

10

20

5

10

0

-5

0

-10

-20 -10 0 10 20 x

-4

0

4

largeVis(n=5,p=50)

-10

0

10

y

community 1 2

Classification scores

org_embedding pca tsne(p=10) tsne(p=25) tsne(p=5) umap(n=25,m=0.01) umap(n=25,m=0.75) umap(n=5,m=0.01) umap(n=5,m=0.75) largeVis(n=15,p=5) largeVis(n=15,p=50) largeVis(n=5,p=5) largeVis(n=5,p=50)

prop community correct

0.878

0.772

0.892

0.89

0.868

0.872

0.852

0.864

0.87

0.826

0.81

0.882

0.866

prop majority vote correct

0.92

0.8

0.91

0.91

0.88

0.92

0.94

0.88

0.93

0.9

0.85

0.9

0.9

(b) Heterogeneous graph from a combination of three degree corrected stochastic block models.

Figure 4. Graphs, visualizations and classification results with a k-nearest neighbors algorithm with k = 5.

STATISTICAL EMBEDDING: BEYOND PRINCIPAL COMPONENTS

29

7. Some concluding remarks
Principal components work well for linearly generated Gaussian data. It may also work well for other types of data and is probably still the most important statistical embedding method. But, on the other hand, it is not difficult to find examples where it does not work. The search for nonlinear extensions started long ago with the MDS method. In fact, multidimensional scaling methods contain ideas that have been found relevant in several recent nonlinear algorithms.
The field of statistical embedding has had an explosive development, not the least because of the need to interpret, represent, cluster and classify very large data sets, this being an important part of the Big Data revolution. This may be particularly true for the embedding of networks, since an increasing part of ultra-large data sets comes in the form of networks, such networks being of importance in an expanding number of applications.
In this paper we have covered selected methods of nonlinear embedding generalizing PCA, topological embeddings in persistence diagrams, network embedding and embedding to dimension 2 (i.e., visualization). In addition, in the course of the review, we have pointed to some cases of an apparent and arguably widening gap between developments in data science, including computer and algorithmic based methods, and more traditional statistical methods. We think there is a need for traditional as well as modern methods of statistical inference to better evaluate uncertainty and to put some of the algorithms on a firmer statistical basis, including a search for their optimality. On the other hand, statisticians could clearly benefit from recent methods and algorithms developed in data science. The hope is that interaction between machine learning, data science and statistics could bring about a synergy effect for all of the three disciplines.

Funding This work was supported by the Norwegian Research Council grant 237718 (BigInsight).

Supplementary material
The Supplement (Tjøstheim et al., 2021a) contains more details on persistence diagrams, simplical complexes and word embedding, as well as a more involved variant of the network example in Section 6.4.

References
Aizerman, M. A., Braverman, E., and Rozonoer, L. (1956). Theoretical foundations of the potential function method in pattern recognition learning. Automation and Remote Control, 25:821­137.
Belkin, M. and Niyogi, P. (2002). Laplacian eigenmaps and spectral techniques for embedding and clustering. In Advances in Information Processing Systems. MIT Press, Cambridge. T.K. Leen and T.G. Dietterich and V. Treps eds.
Belkin, M. and Niyogi, P. (2003). Laplacian eigenmaps for dimensionality reduction and data representation. Neural Computation, 15:1373­1396.
Bian, R., Koh, Y., Dobbie, G., and Divoli, G. (2019). Network embedding and change modeling in dynamic heterogeneous networks. SIGIR 19, July 21-25, 2019, Paris, France.
Bickel, P. and Chen, A. (2009). A nonparametric view of network models and Newman-Girvan and other modularities. Proceedings National Academy of Science. PNAS 0907096106.
Blondel, V., Guillaume, J.-L., Lambiotte, R., and Lefebvre, E. (2008). Fast unfolding of communication in large networks. arXiv:0803.0476v2.
Boser, B., Guyon, I., and Vapnik, V. (1992). A training algorithm for optimal margin classifiers. In Fifth Annual Workshop on COLT, Pittsburgh, ACM.
Budur, E., Lee, S., and Peti, K. (2015). Structural analysis of criminal network and predicting hidden links using machine learning. arXiv:1507.05739v3.
Carlsson, G. (2009). Topology and data. Bulletin of the American Mathematical Society, 46:255­308.

STATISTICAL EMBEDDING: BEYOND PRINCIPAL COMPONENTS

30

Chazal, F. and Michel, B. (2017). An introduction to topological data analysis: fundamental and practical aspects for data scientists. arXiv: 1710.04019v1.
Chen, Y., Genovese, C., and Wasserman, L. (2015a). Asymptotic theory for density ridges. Annals of Statistics, 43:1896­1928.
Chen, Y., Ho, S., Freemen, P., Genovese, C., and Wasserman, L. (2015b). Cosmic web reconstruction through density ridges: methods and algorithm. Monthly Notices of the Royal Astronomical Society, 454:1140­1156.
Chen, Y., Ho, S., Tenneti, A., Mandelbaum, R., andT. DiMatteo, R. C., Freeman, P., Genovese, C., and Wasserman, L. (2015c). Investigating galaxy-filament alignments in hydrodynamic simulations using density ridges. Monthly Notices of the Royal Astronomical Society, 454:3341­3350.
Cui, P., Wang, X., Pei, J., and Zhu, W. (2019). A survey on network embedding. IEEE transactions and Knowledge Engineering, 31:833­852.
de Silva, V. and Tenenbaum, J. (2002). Global versus local methods in nonlinear dimensionality reduction. NIPS paper.
Devroye, L. and Wise, G. (1980). Detection of anormal behavior via nonparametric estimation of the support. Siam Journal of Applied Mathematics, 38:480­488.
Dong, W., Moses, C., and Li, K. (2018). Efficient k-nearest neighbour graph construction for generic similarity measures. In Proceedings of the 20th International Conference of the World Wide Web, pp. 577-586, New York.
Dong, Y., Chawla, N., and Swami, A. (2017). Metapath2vec: Scalable representation learning for heterogeneous networks. Kid 17, 2017, Halifax, NS, Canada.
Du, L., Wang, Y., Song, G., Lu, Z., and Wang, J. (2018). Dynamic network embedding: An extended approach for skip-gram based network embedding. Proceedings of the 27th International joint conference on Artificial Intelligence, IJ(AI-18).
Duchamp, T. and Stuetzle, W. (1996). Extremal properties of principal curves. Annals of Statistics, 24:1511­1520.
Edelsbrunner, H., Letcher, D., and Zomorodian, A. (2002). Topological persistence and simplification. Discrete Computational Geometry, 28:511­533.
Genovese, C., Perone-Pacifico, M., Verdinelli, I., and Wasserman, L. (2012). Manifold estimation and singular deconvolution under Hausdorff loss. Annals of Statistics, 40:941­963.
Genovese, C., Perone-Pacifico, M., Verdinelli, I., and Wasserman, L. (2014). Nonparametric ridge estimation. Annals of Statistics, 42:1511­1545.
Ghrist, R. (2017). Homological algebra and data. Lecture notes. Girvan, M. and Newman, M. (2002). Community structure in social and biological networks.
Proc. Natl. Acad. Sci. (PNAS), 99:7821­7826. Greene, D. and Cunningham, P. (2011). Tracking the evolution of communities in dynamic
social networks. Report Idiro Technologies, Dublin, Ireland. Gretton, A. (2019). Introduction to RKHS, and some simple kernel algorithms. Lecture notes. Grover, A. and Leskovec, J. (2016). node2vec: Scalable feature learning for networks. Kdd' 16,
August 13-17, San Francisco, CA, USA. Hastie, T. (1984). Principal curves and surfaces. Laboratory for Computational Statistics
Technical Report 11, Stanford University, Department of Statistics. Hastie, T. and Stuetzle, W. (1989). Principal curves. Journal of the American Statistical
Association, 84:502­516. Hastie, T., Tibshirani, R., and Friedman, J. (2019). The Elements of Statistical Learning.
Springer, New York. Hinton, G. and Roweis, S. (2002). Stochastic neighbour embedding. Advances in Neural
Information Processing Systems, 15:833­840. Hinton, G. and Salakhutdinov, R. (2006). Reducing the dimensionality of data with neural
networks. Science, 313:504­507. Hoff, P., Raftery, A., and Handcock, M. (2002). Latent space approaches to social network
analysis. Journal of the American Statistical Association, 97:1090­1098.

STATISTICAL EMBEDDING: BEYOND PRINCIPAL COMPONENTS

31

Hotelling, H. (1933). Analysis of a complex of statistical variables into principal components. Journal of Educational Psychology, 24:417­441.
Hotelling, H. (1936). Relations between two sets of variates. Biometrika, 28:321­377. Hyv¨arinen, A. and Oja, E. (2000). Independent component analysis: algorithms and applica-
tions. Neural Networks, 13:411­430. Joliffe, I. (2002). Principal Component Analysis. Springer, New York. Karrer, B. and Newman, M. (2011). Stochastic blockmodels and community structure in
networks. Physical Review E, 83. Kim, J., Rinaldo, A., and Wasserman, L. (2019). Minimax rates for estimating the dimension
of a manifold. Journal of Computational Geometry. Kobourov, S. (2012). Spring embedders and forced directed graph drawing algorithms. arXiv
1201.3011K. Kohonen, T. (1982). Self-organized formation of topologically correct feature map. Biological
Cybernetics, 43:59­69. Kossinets, G. and Watts, D. (2006). Empirical analysis of an evolving social network. Science,
311:88­90. Kuno, U. and Suga, Y. (1966). Multidimensional mapping of piano pieces. Japanese Psycho-
logical Research, 8:119­124. Levina, E. and Bickel, P. (2005). Maximum likelihood estimation of intrinsic dimension. NIPS
Proceedings. Little, A., Maggioni, M., and Rosasco, L. (2011). Multiscale geometric methods for estimating
intrinsic dimension. Proc. SampTA 4:2. Luxburg, U. V. (2007). A tutorial on spectral clustering. Statistics and Computing, 17:395­416. Markov, A. (1958). Insolubility of the problem of homeomorphy. Proc. Intern Congress of
Mathematicians. McInnes, L., Healy, J., and Melville, J. (2018). UMAP: Uniform manifold approximation for
dimension reduction. arXiv:1802.03426v2. Mikolov, T., Sutskever, I., Chen, K., Corrado, G., and Dean, J. (2013). Distributed repre-
sentation of words and phrases and their composability. In Advances in Neural Information Processing Systems 26: Proceedings Annual 27th Conference on Neural Information Processing Systems. Lake Tahoe, Nevada, USA. Mornelli, C., Giguere, C., and Petit, K. (2005). The efficiency/security trade-off in criminal networks. Social Networks, 29:143­153. Newman, M. (2006). Modularity and community structure in networks. Proceedings of the National Academy of Science, PNAS, 103:8577­8582. Newman, M. (2020). Networks. Oxford University Press. 2nd revised edition. Newman, M. and Girvan, M. (2004). Finding and evaluating community networks. Physical Review E, 69:02613­1 ­ 02613­15. Niyogi, P., Smale, S., and Weinberger, S. (2008). Finding the homology of submanifolds with high confidence from random samples. Discrete and Computational Geometry, 39:419­441. Otneim, H., Jullum, M., and Tjøstheim, D. (2020). Pairwise local Fisher and naive Bayes: Improving two standard discriminants. Journal of Econometrics, 216:284­304. Ozertem, U. and Erdogmus, D. (2011). Locally defined principal curves and surfaces. Journal of Machine Learning Research, 12:1249­1286. Pearson, K. (1901). On lines and planes of closest fit to systems of points in space. Philosophical Magazine, 2:559­572. Perozzi, B., Al-Rfou, R., and Skiena, S. (2014). Deepwalk: Online learning of social representations. KDD' 14, , http://dx.doi.org/10.1145/2623330.2623732. Qiu, J., Dong, Y., Ma, H., Li, J., Wang, K., and Tang, J. (2018). Network embedding as matrix factorization.: unifying deepwalk, LINE, PTE, and node2vec. Proceedings WSDM, ACM, New Tork, NY, USA. Qiu, J., Dong, Y., Ma, H., Li, J., Wang, K., and Tang, J. (2019). NetSMF: Large-scale network embedding as sparse matrix factorization. In Proceedings of the 2019 World Wide Web Conference, May 13-17, San Francisco, CA, USA.

STATISTICAL EMBEDDING: BEYOND PRINCIPAL COMPONENTS

32

Rohe, K., Qin, T., and Yu, B. (2016). Co-clustering directed graphs to discover asymmetries and directional communities. Proceeding National Academy of Science, 113:12679­12684.
Roweis, S. and Saul, L. (2000). Nonlinear dimensionality reduction by locally linear embedding. Science, 290:2323­2326.
Sammon, J. (1969). A nonlinear mapping for data structure analysis. IEEE Transactions on Computers, 18:403­409.
Sch¨olkopf, B., Smola, A., and Mu¨ller, K.-L. (2005). Kernel principal components. Lecture Notes in Computer Science, 1327:583­588.
Sun, Y., Norick, B., Han, J., Yan, X., Yu, P., and Yu, X. (2012). Integrating meta-path selection with user-guided object clustering in heterogeneous information networks. Kdd 12, Beijing China.
Tang, J., Liu, J., Zhang, M., and Mei, Q. (2016). Visualizing large-scale and high-dimensional data. arXiv. 1602.00370v2.
Tang, J., Qu, M., and Mei, Q. (2015a). PTE: Predictive text embedding through large-scale heterogeneous text networks. arXiv. 1508.00200v1.
Tang, J., Qu, M., Wang, M., Zhang, M., Yan, J., and Mei, Q. (2015b). LINE: Large-scale information network embedding. WWW 2015 Conference, May 18-May 22, Florence, Italy.
Tenenbaum, J., de Silva, V., and Langford, J. (2000). A global geometric framework for nonlinear dimensionality reduction. Science, 290:2319­2323.
Tjøstheim, D., Jullum, M., and Løland, A. (2021a). Supplement to "Statistical embedding: Beyond principal components".
Tjøstheim, D., Otneim, H., and Støve, B. (2021b). Statistical dependence: beyond Pearson's . Statistical Science. to appear.
Tjøstheim, D., Otneim, H., and Støve, B. (2021c). Statistical Modeling Using Local Gaussian Approximation. Elsevier. to appear.
Torgerson, W. (1952). Multidimensional scaling: 1 theory and method. Psychometrica, 29:1­27. Tutte, W. (1963). How to draw a graph. Proceedings of the London Mathematical Society,
13:743­768. van der Maaten, L. (2014). Accelerating t-SNE using tree-based algorithms. Journal of Machine
Learning Research, pages 3221­3245. van der Maaten, L. and Hinton, G. (2008). Visualizing data using t-SNE. Journal of Machine
Learning research, 9:2579­2605. van der Maaten, L., Postma, E., and van der Herik, J. (2009). Dimensionality reduction: A
comparative review. Tilburg Centre for Creative Computing, TiCC TR 2009.005. Wasserman, L. (2018). Topological data analysis. Annual Review of Statistics and its Applica-
tions, 5:501­532. Wilson, R. and Zhu, P. (2008). A study of graph spectra for computing graphs and trees.
Journal of Pattern Recognition, 4:2833­2841. Young, G. and Householder, A. (1938). Discussion of a set of points in terms of their mutual
distances. Psychometrika, 3:19­22. Zhang, J. and Chen, Y. (2018). Modularity based community detection in heterogeneous
networks. arXiv 1803.07961v1. Zheng, Q. (2016). Spectral techniques for heterogeneous social networks. PhD thesis, Queen's
University, Ontario, Canada. Zhou, C., Liu, Y., Liu, X., and J.Gao (2017). Scalable graph embedding for asymmetric
proximity. Proceedings of the 31st AAAI Conference on Artificial Intelligence. Zhou, L., Yang, Y., Ren, X., Wu, F., and Zhuang, Y. (2018). Dynamic network embedding by
modeling triadic closure process. 32nd AAAI Conference on Artificial Intelligence. Zhu, X. and Pan, R. (2020). Grouped network vector autoregression. Statistica Sinica. to
appear. Zhu, X., Pan, R., Li, G., Liu, Y., and Wang, H. (2017). Network vector autoregression. Annals
of Statistics, 45:1096­1123. Zomordian, A. and Carlsson, G. (2005). Computing persistent homology. Discrete Computa-
tional Geometry, 33:249­274.

SUPPLEMENT TO "STATISTICAL EMBEDDING: BEYOND PRINCIPAL COMPONENTS"
DAG TJØSTHEIM1, MARTIN JULLUM2, AND ANDERS LØLAND3

arXiv:2106.01858v1 [stat.ML] 3 Jun 2021

1. Persistence diagrams and simplical complexes

Assume that we observe a sample X1, . . . , Xn drawn from a distribution P supported on a set S, and let us define the empirical distance function

d^(x)

=

min
1in

||x

-

Xi||.

It should be noted that lower level sets L^ defined by L^ = {x : d^(x)  } are precisely the union of balls described in Equation (9) in the main paper, i.e.,

L^ = {x : d^(x)  } = ni=1B(Xi, ).

The persistence diagram D^ defined by these lower level sets is an estimate of the underlying

diagram D.

The empirical distance function is often used for defining the persistence diagram of a data

set in computational topology. However, as pointed out by Wasserman (2018), from a statistical

point of view this is a poor choice, as it is highly non-robust. Wasserman points out several

more robust alternatives. One of them is the so called DTM distance introduced by Chazal

et al. (2011) given by

d^2m(x)

=

1 k

k

||x - Xi(x)||2,

i=1

where k = [mn] is the largest integer less than or equal to mn and with 0  m  1 being

a scale parameter. Further, Xj(x) denotes the data after re-ordering them so that ||X1(x) - x||  ||X2(x) - x||  · · · . This means that d^2m(x) is the average squared distance to the knearest neighbours. Other alternative references to a robustified distance measure are given in

Wasserman (2018).

Actually, in more complicated situations, the persistence diagram is not computed directly from L^, but from so-called simplical complexes. This approach is particularly interesting since it generalizes the embedding of a point cloud in a graph as described in Sections 3.4 and 3.5 in

the main manuscript. We will give a brief description here. Much more details can be found

in Chazal and Michel (2017). First, recall the definition of a simplex: Given a set X = {X0, . . . , Xk}  Rp of k + 1 "affinely
independent" (i.e., the vectors (X0, X1, . . . Xk) are linearly independent), the k-dimensional simplex  = [X0, . . . , Xk] spanned by X is the convex hull of X. For instance, for k = 1 the simplex is simply given by the line from X0 to X1. The points of X are called the nodes of  and the simplices spanned by the subsets of X are called the faces of . A geometric simplical complex K in Rp is a collection of simplices such that (i) any face of a simplex of K is a simplex

of K, (ii) the intersection of any two simplices of K is either empty or a common face of both.

As seen in Sections 3.4 and 3.5 in the main paper, connecting pairs of nearby data points

by edges leads to the standard notion of a neighbouring graph from which the connectivity of

the data can be analyzed and clustering can be obtained, including non-convex situations, as

described in Section 3.4. Using simplical complexes, where simplical complexes of dimension 1

1University of Bergen and Norwegian Computing Center 2,3Norwegian Computing Center E-mail addresses: Dag.Tjostheim@uib.no, Martin.Jullum@nr.no, Anders.Loland@nr.no.
1

SUPPLEMENT TO "STATISTICAL EMBEDDING: BEYOND PRINCIPAL COMPONENTS"

2

are graphs, one can go beyond this simple form of connectivity. In fact a central idea in TDA is
to build higher dimensional equivalents of neighbouring graphs by not only connecting pairs but
also (k + 1)-tuples of nearby data points. This enables one to identify new topological features
such as cycles and voids and their higher dimensional counterparts. Regarding embedding of
networks, as treated in Section 5, such a technique could possibly be used to discover cycles in
networks such as criminal rings in fraud detection, say.
Simplical complexes are mathematical objects that have both topological and algebraic prop-
erties. This makes them especially useful for TDA There are two main examples of complexes in use. They are the Vietoris-Rips complex and the C ech complex. The Vietoris-Rips complex V(X) can be introduced in a metric space (M, d). It is the set of simplices X = [X0, . . . , Xk] such that dX(Xi, Xj)   for all (i, j). The C ech complex C(X) is defined as the simplices [X0, . . . , Xk] such that the k + 1 balls B(Xi, ) have a nonempty intersection.
These definitions should be compared to the use of ball-coverings in Section 4 of the main
paper and level sets defined in the present subsection. It can in fact be shown that the homology of L^ is the same as the homology of C. The homology of C can be computed using basic matrix operations. All relevant computations can be reduced to linear algebra. This gives a
method of computing homology and persistent homology relating the complexes as  varies as
briefly mentioned in our simple introductory example of chain of circles, or the more involved
example involving Ranunculoids, in Section 4.2 of the main paper (see Edelsbrunner and Harer
(2010)). In fact, it is computationally easier to work out the algebra for the Vietoris-Rips
complex V. It can be shown that the persistent homology defined by V approximates the persistent homology defined by C.
Given a subset X of a compact metric space (M, d), the families of Vietoris-Rips complexes, {V(X)}R and the family of C ech complexes, {C(X)}R are filtrations, that is, nested families of complexes. As indicated earlier, the parameter  can be considered as a data resolution level at which one considers the data set X. For example if X is a point cloud in Rp, the filtration {C} encodes the topology of the whole family of unions of balls X = XXB(X, ) as  goes from 0 to .
As in the introductory example in Section 3.1 of the main paper, the homology of a filtration {F} changes as  increases: new connected components can appear, existing components can merge, loops and cavities may appear or be filled. Persistence homology tracks these changes,
identifies the appearing features, and attaches a lifetime to them. The resulting information can be encoded as a set of intervals, the bar-code, or equivalently, as a multiset of points in R2, where the coordinates of each point is the start and end point of the corresponding interval.
In Chazal and Michel (2017) a formal definition of bar-code and persistence diagram is given
via the concept of persistence module which again is defined in terms of an indexed family of
vector spaces and a doubly-indexed family of linear maps.

1.1. Persistent landscapes, functional spaces and applications. The space of persistence diagrams is not a function space in the sense that it is not a Hilbert space. This may make it more difficult to directly apply methods from statistics and machine learning. For example, the definition of a mean persistence diagram is not obvious and unique (Chazal and Michel, 2017, p. 28). Further, according to Chazal and Michel (2017, p. 29) the highly nonlinear nature of diagrams prevents them from being used as a standard feature of machine learning algorithms. An exception, however, is Obayashi and Hiraoka (2017).
Bubenik (2015) introduced persistence landscapes. The persistence landscape is a collection of continuous linear functions obtained by transforming the points of the persistence diagram into tent functions. This function space can be given a Hilbert space structure (in fact a more general structure of a separable Banach space in Bubenik's original paper). The random structure created by X1, . . . , Xn may then be represented by Hilbert space variables, and it becomes meaningful to consider means, variances and a central limit theorem. The vector space structure of persistent landscapes and similar constructions may appear to be more directly extendable to machine learning, in particular to kernel methods, cf. also Section 3.6 in the main paper, in reproducing kernel Hilbert space (see for instance Reininghaus et al. (2015), Kusano

SUPPLEMENT TO "STATISTICAL EMBEDDING: BEYOND PRINCIPAL COMPONENTS"

3

and Hiraoka (2016) and Carriere and Oudot (2019)). It can safely be stated that combining TDA and persistence homology with machine learning is becoming an active research direction with results having potential for unsolved practical problems.
Clearly, the bar codes, the persistence diagrams and Betti numbers can also be used directly as feature extractors for classification problems. In particular, these have been used for network characterizations in Cartsens and Horadam (2013). Possibly such features can be used as a supplement to the network embedding and clustering methods presented in Section 5 in the main paper of this survey.
Connections between persistent homology and deep learning has also started to be explored. Umeda (2017) has done this in a time series context. Another application to time series is Ravisshanker and Chen (2019).
For applications to specific problems we refer to references in Wasserman (2018) and Chazal and Michel (2017). Wasserman discusses briefly applications to the cosmic web, images and proteins, Chazal and Michel discuss applications to protein binding configurations and classification of sensor data.

1.2. Statistical inference. A central concept in inference for persistence diagrams is the bottleneck distance. Given two diagrams C1 and C2, the bottleneck distance is defined by

(C1,

C2)

=

inf


sup
zC1

||z

-

(z)||,

where  ranges over all bijections between C1 and C2. Intuitively, this is like overlaying the two diagrams and asking how much one has to shift the diagrams to make them the same (Wasserman, 2018). The practical computation of the bottleneck distance amounts to the computation of perfect matching in a bipartite graph for which classical algorithms can be used (Chazal and Michel, 2017).
The bottleneck distance is a natural tool to express stability of persistence diagrams. An alternative distance measure is the Wasserstein distance. The bottleneck distance is also a natural tool in statistical inference on persistent landscapes, cf. Chazal et al. (2015).
The (estimated) persistence diagram C^ is based on a finite collection of random variables X1, . . . , Xn. One might think of a true persistence diagram C as n  . A central question is then whether there is such a thing as consistency, and is it possible to introduce confidence intervals? Such questions have been considered by Chazal and Michel (2017, Section 5.7; see especially Section 5.7.4) and is based on the bottleneck distance between C^ and C.
For many applications, in particular when the point cloud does not come from a (perturbation of) a geometric structure, the persistence diagram will look quite complicated. In particular, there will be a number of cases where the life time is quite short and consequently with representative points close to the diagonal. The question then arises whether these points can be considered as noise and should therefore be eliminated from the diagram. One needs a concept of statistical significance to make such an evaluation, and again the bottleneck distance can be used as a tool. When estimating a persistence diagram C with an estimator C^ one may look for a quantile type number  such that

(1)

P (d  )  ,

for   (0, 1). This can be taken as a point of departure for computation of confidence intervals
and significance tests.
It is necessary to translate (1) into something that can be computed. This can be done by the bootstrap as in Chazal et al. (2016). Let (X1, . . . , Xn) be a sample from the empirical measure defined from the observations (X1, . . . , Xn). Moreover, let C^ be the persistence diagram derived from this sample. One can then take as an estimate of  the quantity ^ defined by
P [d(C^, C^) > ^|X1, . . . , Xn] = ,

where it is straightforward to estimate ^ by Monte Carlo integration. Chazal et al. (2016) have shown that the bootstrap is valid when computing the sub-level sets of a density estimator.
Using the bottleneck bootstrap and given a certain significance level, a band can be constructed

SUPPLEMENT TO "STATISTICAL EMBEDDING: BEYOND PRINCIPAL COMPONENTS"

4

parallel to the diagonal of the persistence diagram, and such that points in this level are considered as noise. A bootstrap algorithm can also be used to construct confidence bands for landscapes as shown in Chazal et al. (2016).
In conclusion, there are a number of problems of interest for statisticians in TDA. Chazal in particular mentions four topics:
(1) Proving consistency and studying the convergence rates of TDA methods. (2) Providing confidence regions for topological features and discussing the significance of
estimated topological quantities. (3) Selecting relevant scales (i.e. selecting  in the examples discussed above) at which
topological phenomenona should be considered as functions of observed data. (4) Dealing with outliers and providing robust methods for TDA.
In addition, one may want to introduce the block bootstrap to take better care of dependence stuctures.

2. Embedding and word feature representation of a language text
Sections 5.2 and 5.3 of the main paper describe the importance of embedding of networks and its use in feature extraction, in clustering, characterization and classification for ultra-large data sets. It was pointed out in Section 5.3 that a main methodology for this is the Skip-Gram procedure which was developed in the context of word embedding for a natural language. The purpose of the present section is twofold. First, language processing is of considerable independent interest. Second, it provides more details on the Skip-Gram procedure, its background and its use. Although this material is couched in terms of language analysis, we believe that when read in conjunction with Section 5.3 of the main paper, it will also provide added insight into the details of network embedding.

2.1. A few basic facts of neural nets. The Skip-Gram procedure is based on a neural network with a single hidden layer, and we therefore include a brief summary of neural networks in this supplement.
Neural networks are used for a number of problems in prediction, classification and clustering. The developments perhaps stagnated somewhat in the early seventies, but received renewed interest the last decades, following a massive increase in computational power. Currently, there is an intense activity involving among other things deep learning, where some remarkable results have been obtained. See Schmidhuber (2015) for a relatively recent overview.
Assume that we are given an n-vector x as input. In a neural network approach one is interested in transforming x via linear combinations of its components and possibly a nonlinear transformation of these linear combinations. This transformation constitutes what is called a hidden layer. Then this might be sent through a new transformation of the same type to create a new hidden layer and eventually to an output layer y which should be as close as possible to a target vector t. If there is more than one hidden layer, it is said to be a deep network, its analysis being a base for so-called deep learning. In this supplement, mainly dealing with the background of the Skip-Gram, only the case of one hidden layer will be treated, that, in our context, will be formed by a linear transformation.
Given the input layer, the first step in forming the hidden layer is to form linear combinations

n

(2)

hi = wijxj,

j=1

where i = 1, . . . , m. Note that implicitly, there may be a constant term by taking x1, say, equal to 1. (This is sometimes termed the bias term of the linear combination.)
In the case of one hidden layer, the output layer is given by

m
yj = wijhi,
i=1

SUPPLEMENT TO "STATISTICAL EMBEDDING: BEYOND PRINCIPAL COMPONENTS"

5

for j = 1, . . . , q. In subsequent applications for language and network embedding models q = dim(y) = dim(x) = n.
In a classification problem, yj may be associated with an unnormalized probability for a class j, which in Section 5.3 of the main paper is the appropriate neighbourhood of a node vj in a network. In such cases the output layer is also transformed. A common transformation is the so-called softmax function given by

(3)

softmax(yj) =

exp(yj )

n i=1

exp(yi

)

.

This is recognized (if there is no hidden layer) as the multinomial logistic regression model

which is a standard tool in classification.

Using a training set, the coefficients (or weights) wij and wij are determined by a penalty function measuring the distance between the output y and the target vector t, for example measured by the loss function E = ||y - t||2. In a classification and clustering problem the

training set consists of input vectors x belonging to known classes i (known words in the

vocabulary in the text). The target vector is a so-called "one hot" vector having 1 at the

component j for the given target word and zeros elsewhere. The weights are adjusted such

that the output vector is as close as possible to this vector, which means that the softmax

function should be maximized for this particular component and ideally exp(yi)  0 for i = j. The error function is evaluated for each of the samples coming in as inputs, and the gradient

of the error function with respect to y is evaluated with the weights being re-computed and

updated in the direction of the gradient by stochastic gradient descent.

The weights wij for the output layer is computed first and then wij by the chain differentiation rule using so-called back propagation. Details are given in e.g. the appendix of Rong (2016).

Schematically this may be represented by

wi(jnew)

=

wi(jold)

-



E wij

and similarly for wij. Initial values for the weights can be chosen by drawing from a set of uniform variables. Below the updating scheme will be illustrated on word representation of natural languages, which next can be applied to embedding of networks.

2.2. Word feature representation of natural languages. Consider a natural language

text. We start with a set of input vectors xi, i = 1, . . . , n, where n is the number of words in the vocabulary of the text, and xi represents word i in the vocabulary. Each vector is of dimension n, where xi has a one in position i of the vector and zeros elsewhere ("one-hot" encoded vector). Let m be the dimension of the desired word embedding feature representation.

The dimension may be quite large. Common choices are in the range 100 - 1000. Let the one-

hot vector for the word wi, word number i in the vocabulary, be xi. Further, consider a n × m weight matrix W. Define the m-dimensional hidden units hi, i = 1, . . . , m (without a nonlinear transformation) by

(4)

hi = WT xi =. vwTi ,

which is essentially copying the m-dimensional ith row of W to hi. The vector vwi is the input word representation vector for word number i in the vocabulary, or the feature vector fi of the
word wi . This means that the link (activation) function of the hidden layer units is simply
linear. The weights, i.e., the vector word representation can then be learned by the neural

network given appropriate targets and a penalty function.

An obvious question is whether a nonlinear transformation is needed. Bengio et al. (2003),

in their pioneering paper suggest an added nonlinearity, whereas the approach of Mikolov et al.

(2013a,c) is entirely linear, but using the softmax transformation henceforth. The latter papers

also have some other ingredients which have made them extremely influential.

An essential feature of the papers by Mikolov et al. (2013a,c) and related papers is that they

have found clever approximations to simplify and speed up the calculations of Bengio et al.

(2003).

SUPPLEMENT TO "STATISTICAL EMBEDDING: BEYOND PRINCIPAL COMPONENTS"

6

2.3. The Mikolov et al. approach: word2vec. We have already presented the input linear representation of word vectors as rows of the weight matrix W, see (4). The output layer should consist of conditional probabilities of words in the vocabulary as in Bengio et al. (2003), but Mikolov et al. has a purely linear transformation to the output layer prior to the softmax transformation.
As a further simplification we assume that we have a window passing over a given text with the window consisting of just two words wt, wt-1 in position t and t - 1 of the text. Here, wt is the target word of the text wO, wt-1 is the input word wI , and the conditional probability P (wt|wt-1) can also be written P (wO|wI ). This means that there is only one context word wI for the output word, whereas in the case of Bengio et al. (2003) there were l - 1 context words. (Note that in Skip-Gram, and the use of it in network embedding, the context words are more naturally being thought of as target words belonging to the output.) To describe the transition from the hidden layer to the output layer we introduce a new m × n dimensional weight matrix W = {wij}. Let vwj be the jth column of the matrix W (it has dimension m). It is the output vector representation of word number j in the vocabulary. Then the n-dimensional output vector is defined by
y = (W )T h,

where h = vwI . Component yj is given by

(5)

yj = (vwj )T h, j = 1, . . . , n.

To obtain the posterior distribution one uses softmax as defined in (3),

(6)

P (wj|wI ) =. uj =

exp(yj )

n i=1

exp(yi)

,

where now uj is the transformed output of the jth unit in the output layer. By substitution, one obtains

(7)

P (wj|wI ) =

exp (vwj )T vwI

.

n i=1

exp

wi)T vwI

It should be noted that one gets two distinct word representations vw and vw for each word w in the vocabulary, one input and one output word vector. The output vector is the relevant
one in the sense that the context relations are baked into it. Since the system is completely
linear, there are no extra parameters to be learned from the network, "just" the matrices W
and W .
The network is trained by stochastic gradient descent as in Bengio et al. (2003) and most
other neural network applications. Given the input word wI and the output word wO, one is interested in maximizing the conditional probability P (wO|wI ); i.e., finding the index j = j and the corresponding probability uj in the output layer so that, using (6),

n

(8)

max uj = max P (wO|wI ) or max log uj = yj - log exp(yi).

i=1

By taking derivatives one gets the update equation

(wij)(new) = (wij)(old) - ejhi,

or

(9)

(vwj )(new) = (vwj )(old) - ejhi.

for j = 1, . . . , n, where  > 0 is the learning rate and ej = uj - tj with tj = 1(j = j ). One has to go through every word in the vocabulary, check its output probability uj, and compare uj with its targeted output, either 0 or 1.

SUPPLEMENT TO "STATISTICAL EMBEDDING: BEYOND PRINCIPAL COMPONENTS"

7

Going through the same exercise for the transition between the input and the hidden layer, one obtains (see Rong (2016) for details) for the update equation if wI = wi

vw(ni ew) = vw(oild) - F,

where F is the vector whose ith component, using back propagation, is given by

n j=1

ej

wij

.

Recall that vwTI is a row of W, the "input word vector" of the only context word wI = wi, and

it is the only row of W whose derivative is non-zero. All the other rows will remain unchanged

after this iteration, since their derivatives are zero.

The generalization from a one word context to a context with several words is quite straight-

forward in the Mikolov et al. (2013a,c) set-up. They distinguish between two ways of doing

this, the CBOW and the Skip-Gram model.

Traditional text classification is based solely on frequencies in the text of words in the

vocabulary. This is the bag of words (BOW) approach. Mikolov et al. (2013a,c) take context

into account resulting in a continuous bag of words (CBOW). We are then essentially back to

the situation in Bengio et al. (2003) where there are C = l - 1 context words and we want to

maximize P (wO|w1, . . . , wC), but Mikolov et al. assume linearity in the concatenated C words

in such a way that the concatenated word vector corresponding to [w1, . . . , wC] is simply given

by

the

average

1 C

(vw1

+ · · · + vwC )

of

the

individual

pairwise

word

vectors.

The

hidden

layer

is

then given by

h

=

1 C

WT

(x1

+

x2

+

· · · xC )

(10)

=

1 C

(vw1

+

···

+

vwC ).

This is the CBOW assumption. With this assumption one is more or less back to the onecontext word updates. The loss function can be written (cf. (5) and (8)),

E = - log P (wO|w1, · · · wC )

n

n

(11)

= -yj + log exp(yi) = -(vwO )T h + log exp((vwi)T h),

i=1

i=1

which is the same as (8), the objective of the one-word context model, except that h is different, being defined as in (10) instead of in (4). This leads to an update equation for the output words which is identical to (9), whereas the update equation for input words has to be updated separately for every word wc, c = 1, . . . , C, namely

vw(ncew)

=

vw(ocld)

-

1 C

F,

where F is defined as before.

2.4. The Skip-Gram model. The Skip-Gram model is in a sense the opposite of the CBOW
model, and this is the situation considered in the network embedding in Section 5.3. It is also
different from the Bengio model. For a window centered at the word wI , the window contains C/2 (with C being an even number) words before the center word wI and C/2 word after the center word, so that in the notation of Bengio et al. (2003) the window consists of the words
[wt+C/2, . . . , wt, . . . , wt-C/2]. Sliding the window, the objective is to predict each of the C context words (i.e. maximize the conditional probability) [wt+C/2, . . . , wt+1, wt-1, . . . , wt-C/2] given the input word wI = wt. Here, conditional independence is assumed, so that the conditional probability for each context word is maximized separately.
For the input word representation the derivation in the two word case is the same as the
present situation for the input word and with the same definition of the hidden layer h, so that we still have hI = vwTI . Instead of outputting one (multinomial) distribution, we are outputting C (multinomial) distributions. But, importantly, each output is computed using the same

SUPPLEMENT TO "STATISTICAL EMBEDDING: BEYOND PRINCIPAL COMPONENTS"

8

matrix W mapping the hidden layer into the output layer. (This means that the sequencing of the context words does not matter, only which words are there in the window). Moreover,

P (wc,j|wI ) =

exp(yc,j )

n i=1

exp(yi)

,

where wc,j, c = 1, . . . , C, j = 1, . . . , n, and where the index j is referring to the number in the vocabulary of the word wO,c. Further for h = vwi,

yc,j = (vwj )T h,

for c = 1, . . . , C, where vwj is the output vector for the jth word wj in the vocabulary, and also vwj is taken from the jth column of weight matrix W transforming the hidden layer to the output layer.
The derivations of the parameter update equations are similar to the one-word context.
Assuming conditional independence, the loss function in (11) is changed to

C

n

E = - log P (wO,1, . . . , wO,C |wI ) = - (vwc )T vwI + C log exp{(vwi )T vwI }.

c=1

i=1

The updating equations can be derived by taking derivatives similarly to the CBOW case, and
we refer to Rong (2016) for details.
In spite of the relatively simple linear structure of CBOW and Skip-Gram, it makes for some
quite astonishing properties that goes beyond simple syntactic regularities. This is obtained using just very simple algebraic operations in the word representation space Rm, such that for example the embedded word vector("King")-word vector("Man")+word vector("Woman") has
a high probability of having the word vector("Queen") as its closest word vector, as measured by cosine distance in word feature space Rm. Several similar examples are given in Mikolov et al. (2013a,c), and they have also examined quite systematically the capabilities of CBOW
and Skip-Gram compared to other word representation routines in solving such tasks.

2.5. The computational issue. For all of the word models presented so far, there is a com-
putational issue. As the size of the vocabulary and the size of the training text set increase,
they are heavy to update. For the two-word, the CBOW and the Skip-Gram models there
are two vector representations for each word in the vocabulary: the input vector vw and the output vector vw. Learning the input vectors is cheap, but learning the output vectors is expensive. From the update equations (6), (7), (8) and (9) it is seen that to update vw for each training instance, one has to iterate through every word wj in the vocabulary, compute yj, the prediction error ej and finally use the prediction error to update the output vector vwj .
Such kind of computations makes it difficult to scale up to large vocabularies or large training
corpora. The obvious solution to circumvent this problem is to limit the number of output vec-
tors that must be updated per training instance. There are two main approaches for doing this,
hierarchical softmax and negative sampling. Both approaches optimize only the computation
for updates for output vectors.
Hierarchical softmax is an efficient way of computing softmax (Morin and Bengio, 2005; Mnih
and Hinton, 2008). With this method the frequency of words appearing in texts is taken into
account. In hierarchical softmax the list of words from word 1 to word n is replaced by a binary
Huffman encoded tree with the n words appearing at the leaves (outer branches) of the tree.
The probability of the occurrence of a word given an input word is computed from a probability
path from the root of the tree to the given word. This reduces the number of operations in an update from n to log2 n, e.g. for n = 1 million = 106, the number of operations are reduced to 6 log2 10  20. We refer to Morin and Bengio (2005) and Mnih and Hinton (2008) for a detailed description of hierarchical softmax.

2.6. Negative sampling. The idea of negative sampling is far more straightforward than hierarchical softmax. It is sampling-based, and for each updating instance, only a sample of

SUPPLEMENT TO "STATISTICAL EMBEDDING: BEYOND PRINCIPAL COMPONENTS"

9

output vectors are used. This seems to be an, perhaps the, essential idea that makes Skip-Gram

work so well.

Obviously the output words; i.e. wO in CBOW and each of the words wO,c for c = 1, . . . , C

in the Skip-Gram procedure should be included in the updating sample. They represent the

ground truth and are termed positive samples. In addition, a certain number k of word vectors

(noise or negative samples) are updated, such that k = 5 - 20 are useful for small training

sets, whereas for large training sets, k = 2 - 5 may be sufficient (Mikolov et al., 2013c). The

sampling is carried out via a probability mechanism where each word is sampled according to its

frequency f (wi) in the text. In addition, Mikolov et al. recommend from empirical experience

that each word is given a weight equal to its frequency (word count) raised to the 3/4 power.

The probability for selecting a word (vector) is just its weight divided by the sum of weights

for all words, i.e.,

Pn(wi) =

f (wi)3/4

n j=1

f

(wj

)3/4

.

In addition, in word2vec, instead of using the loss functions (8) and (11) constructed from

multinomial distributions, the authors argue that the following simplified training objective is

capable of producing high-quality word embeddings:

(12)

E = - log ((vwO )T h) -

log (-(vwj )T h),

wj Wneg

where (u) is the logistic function given by (u) = 1/(1 + exp(-u)) and Wneg is the collection

of negative samples for the given update. Further, wO is the output word (the positive sample),

vwO

is

the

output

vector;

h

is

the

value

of

the

hidden

layer

with

h

=

1 C

C c=1

vwc

in

the

CBOW

model and h = vwI in the Skip-Gram model. Note that Mikolov et al. write (12) as

k
E = - log ((vwO )T h) - EwiPn(w) log (-(vwi )T h).
i=1

To obtain the update equations we again use the chain rule of differentiation. First, the derivative of E with respect to (vwj )T h is computed as

E ((vwj )T h)

=

((vwj )T h) - 1 if wj = wO ((vwj )T h) if wj  Wneg

,

which results in the derivative being equal to ((vwj )T h) - tj where tj is the label of word wj such that tj = 1 if wj is a positive sample, and 0 otherwise. Next, we take the derivative of E
with regard to the output vector of the word wj,

E vwj

=

E ((vwj )T h) ((vwj )T h) vwj

=

((vwj )T h) - tj h.

This results in the following update equation for the output vector

vw(jnew) = vw(jold) -  ((vwj )T h) - tj h,

which only needs to be applied to wj  {wO}  Wneg instead of every word in the vocabulary. This equation can be used both for CBOW and the Skip-Gram model. For the Skip-Gram
model, the equation has to be applied for one context word at a time.
To back-propagate the error to the hidden layer and thus update the input vectors of words,
it is necessary to take the derivative of E with regard to the hidden layer's output, obtaining

E h

= wj {wO}Wneg

E (vwj )T h (vwj )T h h

=

((vwj )T h) - tj vwj =. F.

wj {wO}Wneg

SUPPLEMENT TO "STATISTICAL EMBEDDING: BEYOND PRINCIPAL COMPONENTS"

10

Using this, one can obtain update equations for the input vectors of the CBOW and Skip-Gram models.

2.7. Some results. There are a number of results for variously structured text data sets in Mikolov et al. (2013a,c), where it is seen that CBOW and Skip-Gram perform well compared to other methods and that with negative sampling or hierarchical softmax the methods can be applied to vocabularies in the millions and text samples in the billions of words. Choices of parameters such as the number of context words (not much greater than 10), sample size of negative samples, and dimension of word vectors are discussed. Further, there are several experiments analyzing the sensitivity of the results on applications to empirical data. The Skip-Gram is a slightly heuristic method when combined with negative sampling (such as a sudden shift from one objective function to another one, raising the empirical frequencies to an exponent of 3/4). The authors justify this from the empirical results obtained, which are quite impressive. There are several papers attempting to simplify and complement the rather brief description in the papers by Mikolov et al. (2013a,c), and trying to give it a firmer mathematical basis. We have found Rong (2016) useful. The shift of objective function is sought explained in Goldberger and Levy (2014).
There are extensions to classification of text extending the context of word-vector to the concept of paragraph-vector in Le and Mikolov (2014), but it is very concisely written. There is also a paper on machine translation by Mikolov et al. (2013b). Software is easily available for all of the algorithms described in this section.

3. A more involved illustrating example
Fig. 1 contains more involved variants of the graphs in Fig. 4 in the main paper. The homogenous graph in Fig. 1a is simulated from a degree corrected stochastic block model with 2 communities, 100 nodes, average node degree d = 10, ratio of between-community edges over within-community edges  = 0.75, and node degree of 1 with probability 1 -  = 0.99 and node degree of 0.2 with probability of  = 0.01. That means that the number of edges per node is Poisson distributed with expected number of edges equal to 1 (with a probability of 0.99) and 0.2 (with a probability of 0.01), respectively. As for Fig. 4 in the main paper, embeddings with dimension 64 were computed using node2vec with 30 nodes in each walk and 200 walks per node, and a word2vec window length of 10 where all words are included. The accompanying 2-dimensional visualizations of the embeddings are done with PCA and t-SNE, UMAP and LargeVis, all with different tuning parameters.
Compared to Fig. 4a in the main paper, the PCA is far inferior to the three other embedded visualizations for this more involved example. Similarly to Fig. 4b in the main paper, the heterogenous graph in Fig. 1b is simulated from three degree corrected stochastic block models (three subraphs a, b and c, each with 2 communities and node degree 1 with probability 1 -  = 0.99 and node degree of 0.2 with probability of  = 0.01:
Graph a: 30 nodes, average node degree d = 7, ratio of between-community edges over within-community edges  = 0.1
Graph b: 30 nodes, average node degree d = 15, ratio of between-community edges over within-community edges  = 0.2
Graph c: 40 nodes, average node degree d = 7, ratio of between-community edges over within-community edges  = 0.1, and an unbalanced community proportion; a probability of 3/4 for community 1 and a probability of 1/4 for community 2
To link graphs a, b and c, some random edges are added between nodes from the same community1. The results are somewhat similar to those of Fig. 4b of the main paper. Again, PCA is inferior to the three other methods, but it is closer than in Fig. 1b.
1For each pair of nodes bewteen a pair of graphs, say Graph a and c, a new link is randomly sampled with a probability of 0.01, and links connecting two nodes from the same community are kept.

SUPPLEMENT TO "STATISTICAL EMBEDDING: BEYOND PRINCIPAL COMPONENTS"

11

Graph
community 1 2

y

2D visualizations

pca

tsne(p=10)

tsne(p=25)

5.0

4

10

5

2.5

2

0

0.0

0

-5

-2.5

-2

-10

-5.0

-4 -2 0 2 4 -10 -5 0 5 10

-5.0 -2.5 0.0 2.5

umap(n=25,m=0.01) 2
1
0
-1

umap(n=25,m=0.75) 2 0 -2

umap(n=5,m=0.01)
2 1 0 -1

-4

-2 -1 0 1 2

-2

0

2

-2 -2 -1 0 1 2

largeVis(n=15,p=5)

largeVis(n=15,p=50)

largeVis(n=5,p=5)

tsne(p=5) 20 10
0 -10

-20 -10 0 10
umap(n=5,m=0.75) 4

2

0

-2

-4

-4 -2 0

2

largeVis(n=5,p=50)

15

0

10

10

0

5 -10
0

0

-10

-5

0 5 10 15 20

-15 -10 -5 0 5

-10 -5 0 5 10 -5 0 5 10 15

x

Classification scores

org_embedding pca tsne(p=10) tsne(p=25) tsne(p=5) umap(n=25,m=0.01) umap(n=25,m=0.75) umap(n=5,m=0.01) umap(n=5,m=0.75) largeVis(n=15,p=5) largeVis(n=15,p=50) largeVis(n=5,p=5) largeVis(n=5,p=50)

prop community correct

0.562

0.472

0.48

0.536

0.518

0.508

0.522

0.504

0.512

0.554

0.572

0.484

0.542

prop majority vote correct

0.58

0.42

0.45

0.51

0.58

0.57

0.56

0.54

0.55

0.6

0.62

0.47

0.58

Graph

(a) Homogeneous graph from degree corrected stochastic block model.

2D visualizations

pca 6

tsne(p=10)

tsne(p=25)

tsne(p=5)

3

10

3

20

0

0

0

-10

0

-3

-3

-20 -6

-20

-3 0

3

-20

0

20

-6 -3 0 3 6

-20 -10 0 10 20

umap(n=25,m=0.01)

umap(n=25,m=0.75)

umap(n=5,m=0.01)

umap(n=5,m=0.75)

5

5

2

2.5

0

0

0

0.0

-5

-5

-2

-2.5

-10

-10

-2.5 0.0 2.5

-5.0 -2.5 0.0 2.5 5.0

0

5

-5

0

5

largeVis(n=15,p=5)
10 5 0 -5

largeVis(n=15,p=50) 20

0

10

0

-10

-10

largeVis(n=5,p=5)

largeVis(n=5,p=50) 25
0
-25

-10

-10

0

10

-20

-10

0

10 20

-10 0 10 20 30

x

-10

0

y

community 1 2

Classification scores

org_embedding pca tsne(p=10) tsne(p=25) tsne(p=5) umap(n=25,m=0.01) umap(n=25,m=0.75) umap(n=5,m=0.01) umap(n=5,m=0.75) largeVis(n=15,p=5) largeVis(n=15,p=50) largeVis(n=5,p=5) largeVis(n=5,p=50)

prop community correct

0.912

0.876

0.912

0.926

0.904

0.934

0.892

0.894

0.902

0.872

0.882

0.894

0.886

prop majority vote correct

0.95

0.9

0.94

0.95

0.94

0.96

0.96

0.94

0.94

0.93

0.93

0.93

0.92

(b) Heterogeneosus graph from a combination of three degree corrected stochastic block models.

Figure 1. Graphs, visualisations and classification results with a k-nearest neighbors algorithm with k = 5.

SUPPLEMENT TO "STATISTICAL EMBEDDING: BEYOND PRINCIPAL COMPONENTS"

12

References
Bengio, Y., Ducharme, R., Vincent, P., and Jauvin, C. (2003). A neural probabilistic language model. Journal of Machine Learning Research, 3:1137­1155.
Bubenik, P. (2015). Statistical topological data analysis using persistence landscapes. Journal of Machine Learning Research, 16:77­102.
Carriere, M. and Oudot, S. (2019). Sliced Wasserstein kernel for persistence diagrams. arXiv 1803.07961v1.
Cartsens, C. and Horadam, K. (2013). Persistent homology of collaboration networks. Mathematical Problems in Engineering, pages 1­7.
Chazal, F., Cohen-Steiner, D., and M´egot, Q. (2011). Geometric inference for probability measures. Foundations of Computational Mathematics, 11:733­751.
Chazal, F., Fasy, B., Lecci, F., Rinaldo, A., and Wasserman, L. (2015). Stochastic convergence of persistence landscapes and silhouettes. Journal of Computational Geometry, 6:140­161.
Chazal, F., Massart, P., and Michel, B. (2016). Rates of convergence for robust geometric inference. Electronic Journal of Statistics, 10:2243­2286.
Chazal, F. and Michel, B. (2017). An introduction to topological data analysis: fundamental and practical aspects for data scientists. arXiv: 1710.04019v1.
Edelsbrunner, H. and Harer, J. (2010). Computational Topology: An Introduction. American Mathematical Society.
Goldberger, Y. and Levy, O. (2014). word2vec explained: Deriving mikolov et al.'s negativesampling word-embedding method. arXiv: 1402.3722v.1.
Kusano, G. and Hiraoka, Y. (2016). Persistence weighted gaussian kernel for topological data analysis. Proceedings of the 33rd International Conference on Machine Learning, New York.
Le, Q. and Mikolov, T. (2014). Distributed representations of sentences and documents. arXiv:1405.4053v2.
Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013a). Efficient estimation of word representations in vector space. CoRR, abs/1301,3781.
Mikolov, T., Le, V., and Sutskever, I. (2013b). Exploiting similarities among languages for machine translation. arXiv:1309.4168.
Mikolov, T., Sutskever, I., Chen, K., Corrado, G., and Dean, J. (2013c). Distributed representation of words and phrases and their composability. In Advances in Neural Information Processing Systems 26: Proceedings Annual 27th Conference on Neural Information Processing Systems. Lake Tahoe, Nevada, USA.
Mnih, A. and Hinton, G. (2008). A scalable hierarchical distributed language model. NIPS Proceedings 2008.
Morin, F. and Bengio, Y. (2005). Hierarchical probabilistic neural network language model. AISTATS.
Obayashi, I. and Hiraoka, Y. (2017). Persistence diagrams with linear machine learning models. arXiv preprint 1706.10082.
Ravisshanker, N. and Chen, R. (2019). Topological data analysis (tda) for time series. arXiv: 1909.10604v1.
Reininghaus, J., Huber, S., Bauer, U., and Kwitt, R. (2015). A stable multi-scale kernel for topological machine learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.
Rong, X. (2016). word2vec parameter learning explained. arXiv:1411.2738v4. Schmidhuber, J. (2015). Deep learning in neural networks: An overview. Neural Networks,
61:85­117. Umeda, Y. (2017). Time series classification via topological data analysis. Transactions of the
Japanese society for Artificial Intelligence, 32:1­12. Wasserman, L. (2018). Topological data analysis. Annual Review of Statistics and its Applica-
tions, 5:501­532.

