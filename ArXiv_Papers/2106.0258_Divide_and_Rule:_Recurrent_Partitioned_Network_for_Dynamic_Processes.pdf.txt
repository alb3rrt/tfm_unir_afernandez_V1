Divide and Rule: Recurrent Partitioned Network for Dynamic Processes

arXiv:2106.00258v1 [cs.AI] 1 Jun 2021

Qianyu Feng ReLER Lab University of Technology Sydney
Yi Yang Zhejiang University

Bang Zhang DAMO Academy
Alibaba Group

Abstract
In general, many dynamic processes are involved with interacting variables, from physical systems to sociological analysis. The interplay of components in the system can give rise to confounding dynamic behavior. Many approaches model temporal sequences holistically ignoring the internal interaction which are impotent in capturing the protogenic actuation. Differently, our goal is to represent a system with a part-whole hierarchy and discover the implied dependencies among intrasystem variables: inferring the interactions that possess causal effects on the sub-system behavior with RECURRENT PARTITIONED NETWORK (REIN). The proposed architecture consists of (i) a perceptive module that extracts a hierarchical and temporally consistent representation of the observation at multiple levels, (ii) a deductive module for determining the relational connection between neurons at each level, and (iii) a statistical module that can predict the future by conditioning on the temporal distributional estimation. Our model is demonstrated to be effective in identifying the componential interactions with limited observation and stable in long-term future predictions experimented with diverse physical systems.
1 Introduction
Dynamics simulation involves multiple bodies are widely adopted in engineering fields of research, especially in physics, robotics [9, 26, 28] and biotechnology [2, 21]. In many practical applications, the complexity for modeling the dynamics emerges within the combination and interaction of submodules in physical systems. If we want to enable the agent to simulate the real process, it is a prerequisite to figuring out the underlying functions for controlling the system dynamically. Some systems may be adequately described by deterministic equations. However, it will be intractable if the system possesses observational noise, unobserved factors, or intrinsic randomness.
Under the dynamic settings, we consider a physical system as the composition of constituents varying over time. As an important property, multi-body system formalisms usually offer an algorithmic, computer-aided way to model, analyze, simulate and optimize the arbitrary motion of possibly large amount of inter-connected bodies. So, how do models succeed in capturing the dynamic patterns? In a system with general purpose, it is required to perform specific functionalities or behavior. It is straightforward that the system could be structured with a graph network with correspondence to its physical connections. Each part of the system can be divided into several sub-parts accordingly. This facilitates the components to be controlled in a hierarchical way. In addition, it is important to keep the connections dynamic which requires the network to infer the relationship between variables on the fly. Rather than allocating neurons to represent the system parts with a parse tree and assigning pointers to its ancestor and descendants, we consider using an appropriate vector to exchange information with

the relatives. Formed in a hierarchical structure, we propose RECURRENT PARTITIVED NETWORK (REIN) consisting of stacks of auto-encoders learning hierarchical representations. Take the human skeleton for example, each joint represents the lowest level of the system. At a higher level, the limbs and trunk are composed of different groups of joints.
In this paper, we are primarily concerned with modeling dynamic processes with interacting variables based on the observation. The system can be compartmented into different levels of sub-systems. What are the underlying formational mechanism of variables in the interacting system? There are mainly two types of causality considered in the context of dynamic sequences. The most common is Granger causality [5] which has been widely applied in the industry. However, it often fails in the presence of contemporaneous effects, which has motivated the development of dynamiccausality [20, 25]. To recover a dynamic causal model from observation, we need to first learn a compact state representation of sub-systems and then infer a causal graph among the variables to identify hidden confounders. In order to learn the dynamic mechanism in an end-to-end style, we target at simulating data samples with different causality but with a shared causal inference module.
To sum up, our main contribution exists in proposing a hierarchical recurrent architecture, RECURRENT PARTITIVED NETWORK (REIN), for modeling the interplay of variables in dynamic processes. The proposed architecture consists of (i) a perceptive module that extracts a hierarchical and temporally consistent representation of the observation at multiple levels, (ii) a deductive module for determining the relational connection between neurons at each level, and (iii) a statistical module that can predict the future by conditioning on the temporal distributional estimation. Different scenarios and settings are considered in the evaluation, which provides various combinations of variables, i.e., data from unknown interventions on the underlying system. Our model is demonstrated to be effective in identifying the interactions from limited observation and carry out long-term future predictions in diverse physical systems. In a range of experiments on physical simulations, we show that REIN possesses a favorable inductive bias that allows it to discover ground-truth physical interactions with high accuracy in a completely unsupervised way. We further show on real motion capture data that our model can accurately predict the dynamics many time steps into the future.
2 Background and Related Work
Hierarchical representing the system with part-whole structure It is at almost no cost for people to parse the scene and object at different levels of integrity. We need to build a model that can understand the mechanism and manipulate data so that the model is able to reproduce the dynamic behavior from the observation. In a computer that has general purpose and dynamic memory, the obvious way to represent the part-whole hierarchy [1, 3, 22] for a specific structure is to create a graph structure [7, 16, 19, 23, 27] by dynamically allocating pieces of the memory to the nodes in the graph and giving each node pointers to the nodes it is connected to. Combining this type of dynamically constructed graphs [4, 18] with neu- Figure 1: Example of the hierarchical part-whole structure ral network learning techniques has in human skeleton. recently shown great promise, but if the whole computer is a neural network, it is far less obvious how to represent hierarchies that are different for every sample if we want the structure of the neural net to be identical for all samples.
Causal effects in dynamic interacting processes There exist several typical approaches to causal structural discovery: score-based, constraint-based, methods leveraging the structural asymmetries and those exploiting various intervention mechanisms. From a perspective, methods can be categorized as local, whereby edges are tested one at a time, or global, whereby an entire graph candidate is tested. It is only applicable if separability holds. Separability refers to the independence of the variables in the absence of causal interactions. Unfortunately, this is rarely the case in dynamic systems, where the current state of a variable may be heavily determined by the past of another. The authors of CausalVAE [24] state that whilst many disentangled representation learning methods
2

R
t-1

 
R
t

R
t+1

 M
 M MM

0, 1...

Figure 2: Architecture of Recurrent Partitioned Network mainly consists of embeddings from : (i) upward direction: states aggregated from the embeddings of the level below; (ii) downward direction: messages propagated from the embeddings of the level above; (iii) sibling communication: remnant states from the previous time steps at the same level. R represents for the recurrent cell and M denotes multi-headed attention layers. For reconstruction tasks, at each time step, the states of each neuron are further fed into a decoder to predict future steps.
assume independence between latent factors [8,13,15], most latent factors behind real-world phenomena exhibit causal dependencies. They propose the use of a Variational Auto-Encoder [12, 17]. The latent space of a VAE is usually parameterized by a set of exogenous factors. CausalVAE integrates a Causal Layer which transforms these exogenous latent factors into endogenous factors which reflect the causal semantics of the data.
3 REIN: Recurrent Partitioned Network for Dynamic Process
In this paper, we propose RECURRENT PARTITIVED NETWORK (REIN), for modeling the interacting variables of dynamic processes in a hierarchical way. In REIN, each neuron represents the states of a sub-system. The system can be first divided into multiple levels and constituents at each level can be learned with like-minded embeddings. In this way, the process can be modeled dynamically to capture the dynamics in the observation.
3.1 Hierarchical Part-whole Partitioning
Considering observation sampled from the measurable variables generated by a latent implicit function, we apply the partitioned neurons to recurrently model the dynamic states of each part. In a dynamic process, each data point is generated with unseen interventions on the underlying structure and parameters. In this way, dynamics can be controlled part-to-part instead of generating as a whole. In general, the highest level represents the system as a whole while the lowest level represents each variable that can be identified as the smallest unit. The input control signal is fed into the global representation to control the system level-by-level and intervene in the dynamics of the lowest level. The generation of dynamics can be formed by passing the controlling messages in the top-down direction. Each neuron should be activated and updated when the input message is relevant to its embedding. The attention layers are used to activate the parts to be controlled.
3

Figure 2 depicts the architecture of our framework, which is composed of recurrent partitioned neurons for learning the representation hierarchically with information aggregated from top, bottom and peer communication. The input vector contains a sub-vector as the current states and also the control signal. In addition, we also have decoders for reconstructing the dynamic process.
The neurons in REIN can universally represent whatever takes place at the relevant location. The bottom-up aggregation can be implemented with simple full-connection layers or GNNs. In the top-down direction, the attention mechanism is adopted to allocate representational and directional messages. Each lower level acts as a query to retrieve the relevant information from the level above. It is also explainable that each neuron represents a set of entities or elements. The embedding of each component is associated with a key and value from the upper level. In this way, each parent node can select the activated neurons to function on the fly and the best suited ones according to the attentive scores. So, at each time step, the embedding of each neuron at a level can be updated with information from:
· Upward direction: local states that to be aggregated from the embeddings at the level below. The bottom-up modules extract valuable features from the lower-level neurons that get combined at the upper level;
· Downward direction: messages delivered from the embeddings at the level above. The top-down modules generate different outputs for lower level neurons that might perform different dynamics;
· Sibling communication: remnant states propagated from the previous time steps at the same level, thus making reasonable and stable predictions over time.

3.2 Peer propagation with causal effects

Apart from relying to the physical structure, we aim to discover the invisible relationship among
the neurons at the same level. For this purpose, we learn a Directional Acyclic Graph (DAG), G = (V1:T , E1:T ) for connecting the neurons, where V1:T denotes the sub-components at different time steps. E1:T is also learnable over time, denoting the varying relationships between the constituting components. Specifically, for each directed edge (vm,i, vm,j)  E1:T , there are hidden confounders denoting the parameters of the relationship that determines the dynamical relationship and affects
the behavior of the system. In a dynamic process, the edges among nodes represent the relationship
among the changing variables which could crash and recover during the procedure. At level m, the
bottom-up information can be aggregated with multiple implementations, e.g., MLPs, CNNs and
GNNs, referred to as zu. In the other direction, we utilize an attention mechanism that selects and then activates only a subset of the cells for each time step, denoted as zd.

Our goal is to perform the recovery of structural causal graph in a short sequence of data samples and

simultaneously learn a shared dynamics model which makes counterfactual predictions into the future. Vt = {otm,i}Ni=1  R2 represents the constituting components of level m in the system at time step t. Then, we use an inference module parameterized by , that takes the sequence of components at level m as input and predicts the edge set, E~ = f(V~1:T ), V~1:T and E~ together constitute the discovered
causal effects, conditioned on which, a regressive module, f, parameterized by , aims to predict the state of the keypoints at time t + 1, zc = f(V~1:T , E~). We formalize our model as a Variational
Auto-Encoder (VAE) to estimate the distribution. In training phase, the latent embeddings at time

step t is calculated as:

ht = [zu, zd, zc]|t,

µ(t), (t) = GRU(ht),

(1)

zr  N (µ(t), (t)),

zr is a learnable variable re-parameterized from the learned Gaussian distribution N (µ(t), (t)). Then, together with the input control signal a, e.g., the action label to be generated,

gt = En(zr, a, ht),

(2)

st+1 = GRU(gt).

The perception module, f, the inference module, f, and the dynamics modules, f, are shared among all episodes in the dataset consisting of various causal graphs with different discrete and

4

continuous hidden confounders, which enables one-shot adaptation to an unseen graph at test time and allows counterfactual predictions by intervening on the identified graph and rolling into the future using the dynamics module. The inference module and the dynamics module are trained together by minimizing the following objective:

min

L(V~t+1, f(V~1:t, E~)).

(3)

,

mt

Algorithm 1: Training procedure of RECURRENT PARTITIONED NETWORK (REIN)
Input: Training set with samples from the observation S = [s1, ..., sT ], initial parameters , , , , G = (V1:T , E1:T ), batch-size b, total iteration number N
for i = 1, . . . , N do for t = 1, . . . , T do Calculate upward embeddings zu with bottom-up modules Learn the current and prior distributional parameters from N (0, I), and calculate the KL divergence Calculate downward embeddings zt with top-down modules Learn the dynamic graph at level m and infer the causal effects zc Use the RNN decoder to calculate the current prediction with equations in 2 and measure MSE loss Update , , ,  using the gradients
Return: , , , 

3.3 Distributional Estimation with Temporal Variational Auto-Encoder
With VAE handling the prediction at each time step, RNN could better model the temporal dependence over time. Formally, given a sampled sequence S = [s1, ..., sT ], VAE aims to maximize the probability sampled from the learned model distribution. At time t, RNN module p(st|s1:t-1, z1:t) predicts the current pose pt having latent variables z1:t , and conditioned on previous states s1:t-1. We rely on a variational neural network q(zt|s1:t) to approximate the true unknown posterior distribution p(zt|s1:t). This way, the objective of maximizing the data likelihood over the real sequence could be achieved as the following variational lower bound:

log p(s) = log p(s | z)p(z)  Eq(z|s) log p(s | z) - DKL (q(z | s) p(z))

z

(4)

=

Eq(zt|s1:t) log p (st | s1:t-1, z1:t) - DKL (q (zt | s1:t) p (zt)) .

t

Here, the first term of the lower bounding function encourages the generated sample to be sufficiently

close to the real sample; the second term penalizes the KL-divergence between prior and posterior

distributions. A simple form of the prior p(zt) in VAE is a Gaussian with unitary variance, N (0, I). In practice, however, the prior distribution often vary with time. Take motions from the walk category

for example, sometimes the pose variance could be small (e.g., roaming); sometimes could be large

(e.g., directional change of directions or velocity). The prior thus needs to be flexible enough to

accommodate these variations. Intuitively, the prior of present time could be guessed given the context

of previous time steps. We parameterize the prior with a variational neural network p(zt|s1:t-1) conditioned on previous steps s1:t-1. Therefore, the variational lower bound of the sequence could be re-written as

log p(s) 

Eq(zt|s1:t) log p (st | s1:t-1, z1:t)

t

(5)

-DKL (q (zt | s1:t) p (zt | s1:t-1))] .

The constraint between prior and posterior distribution further encourages temporal consistency.

4 Experiments
Implementation Our encoder implementation uses MLPs or GNNs with attentive pooling as our message passing function. For the decoder, fully-connected networks or a recurrent decoder are

5

Table 1: Mean squared error (MSE) in predicting future states for simulations with 5 interacting objects. indicates model (MLP) learned with ground truth graph.

Springs

Steps

1

10

20

50

GT

1.38e-11

Static 7.93e-5

LSTM 4.13e-8

NRI [11] 3.12e-8

1.14e-9 7.59e-3 2.19e-5 3.29e-6

6.63e-6 2.82e-2 7.02e-4 2.13e-5

4.25e-5 4.63e-2 6.73e-3 1.30e-4

REIN 2.69e-9 5.32e-7 7.06e-6 8.34e-5

Charged

1

10

20

50

1.06e-3 5.09e-3 1.68e-3 1.05e-3

3.20e-3 2.26e-2 6.45e-3 3.21e-3

5.28e-3 5.42e-2 1.49e-2 7.06e-3

9.47e-3 8.85e-2 4.61e-2 2.18e-2

1.24e-3 1.63e-3 6.71e-3 1.71e-2

applied. All modules are built in Pytorch1 and trained in an end-to-end style. Optimization was performed using the Adam [10]. We perform experiments in simple physical systems and human motion synthesis. The level number of division is 2 in simulating physical systems with limited constituents and 3 levels are used in modeling the interaction of joints in the human motion sequences. We provide more details about the data sources in the appendix.

4.1 Physical Simulations

We first carry out experiments simulating three different physical systems: particles connected by springs, charged particles and multi-ball interaction. These settings allow us to attempt to learn the dynamics and interactions when the interactions are known and elements can be easily measured. Such systems, manipulated by simple rules, can perform complex dynamics. Example trajectories can be seen in Fig. 3.

Springs & particles 50k training examples, and 10k validation and test examples are generated and sampled for all tasks. We note that the simulations are differentiable and so we can use it as a ground-truth decoder to train the encoder. The charged particles simulation, however, suffers from instability which led to some performance issues when calculating gradients.

Multi-ball system In the multi-ball interaction,

there are 5 balls of different colors moving around. At the beginning of each episode, we Table 2: Accuracy measured for multi-ball system.

sample the invisible physical relations between

Accuracy-edge MSE-location

each pair of balls independently, giving us the ground truth that is fixed throughout the episode. For each pair of balls, there is a one-third probability that they are not connected or linked by a

Static LSTM NRI

0.736±.021 0.873±.014 0.925±.019

8.437±e-2 3.264±e-3 6.853±e-5

rigid rod or a spring. We also sample the contin- REIN

0.947±.012

2.309±e-5

uous parameters for each existing edge and fix

them within the episode, e.g., the length of the

rigid relation or the rest length of the spring.

We evaluate the proposed REIN on all three simulated physical systems and compared our performance, both in future state prediction and in the accuracy of estimating the causal connection between nodes in the graph with no supervision. Additionally, we also compare to other baseline methods: MLPs model with the ground-truth simulation decoder and two correlation-based baselines, Static and LSTM. In order to have a fair comparison, we generate longer test trajectories in steps (1, 10, 20, 50) and only evaluate on the last part unseen by the encoder. We first input in the LSTM a number of samples, and then predict the location in next time steps. We show mean squared error (MSE) results in Table 1. It is observed that our results are better than others especially for long term prediction and are also comparable to the model trained with ground truth graph connections. We also show some generated trajectories in Fig. 3 compared to the ground-truth trajectories.

For the multi-ball system, we report the accuracy of predicted edges and also the MSE loss of final predictions in Table 2. After the mapping, we evaluate the model in predicting the continuous

1https://pytorch.org/

6

Figure 3: Comparison between the GT and predicted trajectories. The encoder are trained on the first time steps, then predict the following unseen time steps. The estimated trajectories and velocities are well matched with the ground truth.
confounder, by computing its correlation with the ground truth physical parameters. With more samples observed, the classification accuracy increases, and the uncertainty decreases, which verifies that with more observations from the environment, the model can better estimate the dynamic formulation that governs the behavior of the system. It is proved that our model outperforms the baselines, which also indicates the importance of causal relationship inference.
4.2 Human 3D Motion Synthesis
Motion sequences can be regarded as the coordination of all the joints in the skeleton which provides a suitable bedrock for the evaluation of the motivation of REIN.The datasets adopted in our experiments possess various action categories, where each type contains a considerable amount of motions in diverse styles, recorded with delicate annotations. In the datasets, skeletons are all composed of 5 kinematic chains: {spine, left arm, right arm, left leg, right leg}, and the root joint is located at the end of the hipbone. Our decoder first produces the Lie algebraic parameters and root translation via linear layers and graph networks, then obtains the 3D positions of the skeleton via the forward kinematics. The motion states at each time step include the 3D location of each joint as well as the velocity by calculating the difference value with the previous step.
Datasets NTU-RGB-D [14] originally contains 120 action types of 106 subjects. Its pose representation (3D joint positions) is from MS Kinect readout, which is known to unreliable and temporally inconsistent. It is sufficient here to be perceptually natural and realistic. We adopted a subset of 13 distinct actions including e.g. cheer up, pick up, salute, constituting 3,900 motion clips. HumanAct12 [6] is the in-house dataset, consisting of 1,191 motion clips and 90,099 frames in total, with hierarchical action type annotations. To be specific, all motions are organized into 12 action categories, including warm up, lift dumbbell, and 34 subcategories including jump handsup, eat with left hand) which offer more detailed commands of motion. Compared to NTU-RGB-D, the 3D position annotations are more accurate, and the pose sequences are more stable.
Evaluation metrics Frechet Inception Distance (FID) is an important metric widely used to evaluate the overall quality of generated motions. Features are extracted from generated motions and real motions by sampling with replacement from the test set. Then FID is calculated between the feature distribution of generated motions vs. that of the real motions. Recognition Accuracy is calculated with a pre-trained RNN action recognition classifier to classify the motions, and calculate the overall recognition accuracy. The recognition accuracy indicates the correlation of the motion and its action type. Diversity measures the variance of the generated motions across all action categories. From a set of all generated motions from various action types, two subsets of the same size d are randomly sampled. Their respective sets of motion feature vectors {v1, ..., vd} and {v1, ..., vd} are extracted.
7

Table 3: Performance evaluation on HumanAct12 and NTU-RGB-D. indicates model learned with ground truth human kinetic chain.

HumanAct12

FID

Accuracy Diversity

NTU-RGB-D

FID

Accuracy Diversity

GT motions 0.092±.007 0.997±.001 6.853±.053 0.031±.004 0.999±.001 7.108±.048

CondGRU Two-stage Act-MoCo t-VAE [6]

40.61±.144 10.48±.089 5.610±.113 2.458±.079

0.080±.002 0.421±.006 0.793±.004 0.923±.002

2.381±.020 5.960±.049 6.752±.071 7.032±.038

28.31±.138 13.86±.091 2.723±.019 0.330±.008

0.078±.001 0.202±.003 0.997±.001 0.949±.001

3.663±.024 5.328±.039 6.920±.061 7.065±.043

REIN REIN

0.392±.034 0.933±.005 6.417±.046 0.270±.013 0.947±.002 7.309±.038

0.340±.027 0.832±.004 6.514±.049 0.130±.011 0.949±.001 7.723±.043

(a)

(b)

(c)

Figure 4: Left: Examples of predicted scores for edges in the causal graph for peer propagation at the limb level (spine, left arm, right arm, left leg, right leg) averaged over time (a) for action "Sit" and (b) for "Jump". (c) shows the loss during training with hierarchical level number from 1 to 4.

The diversity of this set of motions is defined as follows where d is set to 200 as in [6]:

1d

Diversity = d

vi - vi 2 .

(6)

i=1

With results reported in Table 3, it proves that our method qualitatively overwhelms other methods not only in the FID metric and categorical accuracy but also in the diversity of the generated poses. To evaluate the effectiveness of our structure-aware modules, we replaced the graph convolution with a standard 1D convolution with full support over the channels axis, and the skeletal pooling and unpooling are discarded and upsampling and downsampling is performed only on the temporal axis. Hierarchical partitioning the overall structure into functional units as shown in Fig. 4. (c) shows the total loss during training where we can find that with 3 levels for the partitioning, REIN can be better optimized (see Appendix for more details). In Fig. 5, we show the learn causal connection of sub-parts in human skeleton. In order to support the structure of a shared latent space, we simply modify the number of output and input channels in each encoder and decoder, respectively, such that both auto-encoders share the same latent space size, which equals to the one in the original distribution.

4.3 Ablation and Further Discussion

Here we would like to share some reflection on the design details of the proposed architecture. With recurrent partitioned neurons, the system can be modeled with a part-whole hierarchy with predefined levels. We conduct ablation study on HumanAct12 to evaluate the effectiveness of each module in REIN with results reported in Table 4. With a single branch in upward or downward, REIN is not able to take full control of the dynamics generated and performs barely satisfactory. We also compared the peer communication layer to verify the learned connections compared to randomly connected
8

(a) Position of root joint (hipbone) on axis z in walking

walking phoning

greeting

direction

(b) Learned connections for different action

Figure 5: The z-axis position of the root joint at the beginning steps and middle period which has high correlation with the movement of two feet. On the right, we show the causality learned among the joints with several examples of different action types.

ones. It profits from learning representations from the high global level into the structural sub-part levels that possess like-minded embeddings.

First, how fine-grained should the inner

components be divided? In our experiment, we leverage the common-sense Table 4: Ablation study of modules in REIN for motion understanding of the physical structure synthesis on HumanAct12.

and combination. For example, joints

Accuracy

Diversity

in the spine are grouped together and so do joints in the arms and legs. More de-

GT motions

0.997±.001

6.853±.053

tailly, each pair of two adjacent joints in a bone can be taken as a group and perform rigidly together. However, limitations could be foreseen when the physical structure is hard to observe or incompatible with the underlying dynamic

REIN (upward) REIN (downward) REIN (p-random) REIN (p-learned)
REIN

0.762±.035 0.850±.028 0.904±.007 0.904±.007
0.947±.002

3.697±.085 4.386±.062 6.263±.049 6.263±.049
7.309±.038

hierarchy.

Secondly, how to efficiently maintain the most valuable information during the propagation? One valuable question is whether the bottom-up and top-down channels can be shared across levels as well as across locations. This would not work for lower levels if the extraction of features are not compact or lost during the transmit. An advantage in message propagating across levels exists in that the representation at different level and steps would be aligned to keep the consistency and distributional attributes. This would make it easier to seek for the gold in the fine details as well as having a sense of what is happening at the high levels.

5 Conclusions
This paper carries out a study on how to simulate the dynamic process by representing the system in a hierarchical structure and inferring the componential embeddings with structural causality. In the experiments, our method first establishes a structured graph-based representation from observation, identifies the causal relationships between different variables, and makes predictions based on the bottom-up, top-down and peer communication messages. Instead of relying directly on the full supervision of the inner relationship, REIN learns to discover the dependency structures and model the causal mechanisms dynamically, which can better cope with the simulation of more complex and versatile dynamic processes.

References
[1] Ian Bateman, Alistair Munro, Bruce Rhodes, Chris Starmer, and Robert Sugden. Does part­whole bias exist? an experimental investigation. The Economic Journal, 107(441):322­332, 1997.
[2] Stephen P Ellner and John Guckenheimer. Dynamic models in biology. Princeton University Press, 2011.

9

[3] Roxana Girju, Adriana Badulescu, and Dan Moldovan. Learning semantic constraints for the automatic discovery of part-whole relations. In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 80­87, 2003.
[4] Palash Goyal, Sujit Rokka Chhetri, and Arquimedes Canedo. dyngraph2vec: Capturing network dynamics using dynamic graph representation learning. Knowledge-Based Systems, 187:104816, 2020.
[5] Clive WJ Granger. Testing for causality: a personal viewpoint. Journal of Economic Dynamics and control, 2:329­352, 1980.
[6] Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao Sun, Annan Deng, Minglun Gong, and Li Cheng. Action2motion: Conditioned generation of 3d human motions. In Proceedings of the 28th ACM International Conference on Multimedia (MM '20), 2020.
[7] Ehsan Hajiramezanali, Arman Hasanzadeh, Nick Duffield, Krishna R Narayanan, Mingyuan Zhou, and Xiaoning Qian. Variational graph recurrent neural networks. arXiv preprint arXiv:1908.09710, 2019.
[8] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework. 2016.
[9] Oussama Khatib, Emel Demircan, Vincent De Sapio, Luis Sentis, Thor Besier, and Scott Delp. Roboticsbased synthesis of human motion. Journal of Physiology-Paris, 103(3-5):211­219, 2009.
[10] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2015.
[11] Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard Zemel. Neural relational inference for interacting systems. In International Conference on Machine Learning, pages 2688­2697. PMLR, 2018.
[12] Abhishek Kumar, Prasanna Sattigeri, and Avinash Balakrishnan. Variational inference of disentangled latent concepts from unlabeled observations. arXiv preprint arXiv:1711.00848, 2017.
[13] Trent Kyono, Yao Zhang, and Mihaela van der Schaar. Castle: Regularization via auxiliary causal graph discovery. arXiv preprint arXiv:2009.13180, 2020.
[14] Jun Liu, Amir Shahroudy, Mauricio Lisboa Perez, Gang Wang, Ling-Yu Duan, and Alex Kot Chichung. Ntu rgb+d 120: A large-scale benchmark for 3d human activity understanding. IEEE transactions on pattern analysis and machine intelligence, 2019.
[15] Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard Scho¨lkopf, and Olivier Bachem. Challenging common assumptions in the unsupervised learning of disentangled representations. In international conference on machine learning, pages 4114­4124. PMLR, 2019.
[16] Wenjuan Luo, Han Zhang, Xiaodi Yang, Lin Bo, Xiaoqing Yang, Zang Li, Xiaohu Qie, and Jieping Ye. Dynamic heterogeneous graph neural network for real-time event prediction. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 3213­3223, 2020.
[17] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In International conference on machine learning, pages 1278­1286. PMLR, 2014.
[18] Youngjoo Seo, Michae¨l Defferrard, Pierre Vandergheynst, and Xavier Bresson. Structured sequence modeling with graph convolutional recurrent networks. In International Conference on Neural Information Processing, pages 362­373. Springer, 2018.
[19] Weiping Song, Zhiping Xiao, Yifan Wang, Laurent Charlin, Ming Zhang, and Jian Tang. Session-based social recommendation via dynamic graph attention networks. In Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining, pages 555­563, 2019.
[20] George Sugihara, Robert May, Hao Ye, Chih-hao Hsieh, Ethan Deyle, Michael Fogarty, and Stephan Munch. Detecting causality in complex ecosystems. Science, 338(6106):496­500, 2012.
[21] Alejandro F Villaverde, Antonio Barreiro, and Antonis Papachristodoulou. Structural identifiability of dynamic systems biology models. PLoS computational biology, 12(10):e1005153, 2016.
[22] Morton E Winston, Roger Chaffin, and Douglas Herrmann. A taxonomy of part-whole relations. Cognitive science, 11(4):417­444, 1987.
[23] Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, and Chengqi Zhang. Graph wavenet for deep spatial-temporal graph modeling. arXiv preprint arXiv:1906.00121, 2019.
[24] Mengyue Yang, Furui Liu, Zhitang Chen, Xinwei Shen, Jianye Hao, and Jun Wang. Causalvae: Structured causal disentanglement in variational autoencoder. arXiv preprint arXiv:2004.08697, 2020.
[25] Hao Ye, Ethan R Deyle, Luis J Gilarranz, and George Sugihara. Distinguishing time-delayed causal interactions using convergent cross mapping. Scientific reports, 5(1):1­9, 2015.
[26] K-KD Young. A variable structure model following control design for robotics applications. IEEE Journal on Robotics and Automation, 4(5):556­561, 1988.
[27] Xiao Zhang, Cristopher Moore, and Mark EJ Newman. Random graph models for dynamic networks. The European Physical Journal B, 90(10):1­14, 2017.
[28] Leon Z lajpah. Simulation in robotics. Mathematics and Computers in Simulation, 79(4):879­897, 2008.
10

