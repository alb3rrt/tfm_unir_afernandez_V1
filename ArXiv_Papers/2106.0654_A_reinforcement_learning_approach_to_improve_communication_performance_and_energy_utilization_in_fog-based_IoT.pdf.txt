A reinforcement learning approach to improve communication performance and energy utilization
in fog-based IoT
Babatunji Omoniwa, Maxime Guériau and Ivana Dusparic School of Computer Science and Statistics Trinity College Dublin Dublin, Ireland
omoniwab@tcd.ie, {maxime.gueriau, ivana.dusparic}@scss.tcd.ie

arXiv:2106.00654v1 [cs.LG] 1 Jun 2021

Abstract--Recent research has shown the potential of using available mobile fog devices (such as smartphones, drones, domestic and industrial robots) as relays to minimize communication outages between sensors and destination devices, where localized Internet-of-Things services (e.g., manufacturing process control, health and security monitoring) are delivered. However, these mobile relays deplete energy when they move and transmit to distant destinations. As such, power-control mechanisms and intelligent mobility of the relay devices are critical in improving communication performance and energy utilization. In this paper, we propose a Q-learning-based decentralized approach where each mobile fog relay agent (MFRA) is controlled by an autonomous agent which uses reinforcement learning to simultaneously improve communication performance and energy utilization. Each autonomous agent learns based on the feedback from the destination and its own energy levels whether to remain active and forward the message, or become passive for that transmission phase. We evaluate the approach by comparing with the centralized approach, and observe that with lesser number of MFRAs, our approach is able to ensure reliable delivery of data and reduce overall energy cost by 56.76% ­ 88.03%.
Index Terms--IoT sensors, mobile fog relay agent, Q-learning, communication, energy
I. INTRODUCTION
R ECENTLY, the fog/edge computing-based Internet-ofThings paradigm was introduced to move computation, control, and decision-making from centralized data centers and IP backbone networks closer to IoT end-users at the network edge [1]. Despite the proximity of "things" to local service providers at the edge of the network, communication outages may occur due to obstacles or long distances between a source (IoT sensor) and a remote destination node (where some IoT services are delivered) as illustrated in Fig. 1. The use of mobile fog devices as relays to forward IoT traffic between distant sources and destinations is shown to improve the overall network performance [2], [3]. However, these relays are often power-constrained and deplete energy when they move and transmit to distant destinations. As such, power-control mechanisms and intelligent mobility of the relay devices are critical in improving communication performance and energy utilization.

Source B
O1
2
R1

Remote device offering localized services to the
IoT devices Destination
O2

R1, R2,..., Rn : MFRA Sources A, B, C : IoT sensors
O1, O2,..., O n : Obstacles : Two-way communication

Source C

Source A

Fig. 1. The use of mobile relays in forwarding IoT traffic from source to destination.

Several relay-based research have tried to address the issue of communication outages [4], [5], [6], energy usage [7], [8], [9], relay selection [4], [5], [6], [10], mobility [4], [10], latency [5], [9] and congestion [9] within the IoT ecosystem. However, these research considered centralized approaches which are prone to several challenges. Such challenges include scalability, failure or downtime in the central entity. In addition, the overhead resulting from periodic updates and synchronization of nodes with the central controller often leads to inefficient energy utilization and decreased communication performance within the network. We further discuss these issues in Section II.
Several approaches have utilized learning to address communication outage and energy utilization, in order to allow devices to learn the optimal behaviours rather than being predefined. Reinforcement learning (RL) is an unsupervised learning technique in which an agent learns optimal behaviour through interaction with the environment [11]. In [12], deep Q-learning was used to prolong the lifetime of IoT sensors by improving energy utilization. Moreover, the RL-based approach does not require a centralized entity to control agents

TABLE I CLASSIFICATION OF PROBLEMS ADDRESSED IN RELATED WORK

Reference
Omoniwa et al. [4] Simiscuka et al. [5] Alsharoa et al. [7] Manzoor et al. [10] Lv et al. [8] Kawabata et al. [6] Behdad et al. [9] Hribar et al. [12] Wilhelmi et al. [14] Azari et al. [15] Our approach

Energy Outage SelectionMobilityLatency Traffic Sources Relays DestinaOtiobnjective

-

- - 1 >1 1 QoS

-

-

- >1 >1 >1 QoS

- - - - - >1 >1 1 Energy, Planning

--

- - >1 1 1 Relay selection

- - - - - 1 >1 >1 Energy

-

- - - >1 >1 >1 QoS

---

1 1 1 Energy, Congestion

----

>1 - 1 Energy

-

- - - >1 - 1 Aggregate throughput

- - - - >1 - 1 QoS, Energy

- - >1 >1 >1 QoS, Energy

Approach
Steepest Descent method Re-clustering Algorithm Genetic Algorithm Prototype design Numerical Stochastic geometry Analytical Deep Q-learning Stateless Q-learning Multi-armed bandit Decentralized Q-learning

since each agent can be fully autonomous and take actions based on local information to improve overall system performance [13] ­ [15]. In [12] and [15], a direct communication link between the IoT sensors and destination was considered which may not exist in reality due to obstacles and distance, as illustrated in Fig. 1.
In this paper, we take into account the distance and obstacles between source and destination, and leverage available mobile fog relays to simultaneously improve communication performance and energy utilization within the fog-based IoT network. The main contributions of this paper are as follows:
1) A fully-decentralised reinforcement learning approach where each autonomous mobile fog relay agent (MFRA) learns to minimize communication outage within a fogbased IoT architecture.
2) Taking into account mobility and death of MRFA (i.e. when the MFRA runs out of battery power) over time, we introduce a feedback mechanism where each destination device sends observations to neighbouring MFRAs. Based on local observation and feedback from the destination, each MFRA learns to either forward the message or become passive for a transmission phase, when it learns that there is a better performing (e.g., with higher probability of delivered packets) MFRA in its neighbourhood likely to forward the message instead. In this way, MFRAs effectively perform self-selection on which relay to forward messages at each transmission cycle, rather than the relay selection being done centrally. This strategy improves energy utilization within the network.
II. RELATED WORK
In this section, we present related work that address some challenges within the IoT domain. Table I highlights the most closely related work, addressing some of the key challenges within a relay-based IoT network. In [4], the Steepest Descent method was used to minimize the communication outage in a fog-based IoT architecture. A mobile fog device is selected to serve as a relay from randomly distributed relaying candidates. However, the selection of an active relay is done centrally.

In [5], a centralized cloud-deployed platform is responsible for selecting an optimal IoT smart gateway/relay node to improve network QoS. However, these research [4], [5] did not consider energy efficiency of these power-constrained mobile relays.
An energy-efficient relaying scheme for IoT communications was presented in [7] to minimize energy consumption within an IoT network by solving the relay planning and QoS problems. A genetic algorithm-based approach was used to arrive at a near-optimal low-complexity solution. Using numerical methods, the authors in [8] considered the energy-efficient design of a relay-based IoT network, in which multiple sources simultaneously transmit their information to the corresponding destinations via a relay equipped with a large array antenna. In [9], an analytical approach to maximize network effective lifetime by improving energy utilization was proposed. However, the relays considered in these approaches [7], [8], [9] are static and may not be suitable in a dynamic IoT environment.
The authors in [10] designed a prototype, a mobile relay architecture for low-power IoT devices, which exploits thirdparty unknown mobile relays for the forwarding of medical data generated by Bluetooth low energy sensors to some central server in the cloud. In [6], a relay selection scheme for large-scale energy-harvesting IoT networks was proposed. The work applied a stochastic geometric approach and attempts to minimize the outage probability using a novel energy harvesting relay selection scheme. These approaches assume a centralized entity that has global knowledge of the network.
However, related work [4] ­ [9] mostly employ centralized approaches, which may not be suitable within the IoT domain due to several challenges, i.e. operational failure in central controller, management complexities, and increased signaling overhead. Several approaches [12], [14], [15] have utilized learning to address communication outage and energy utilization, in order to allow devices to learn the optimal behaviours rather than being predefined. In [12], deep Q-learning was used to prolong the lifetime of IoT sensors. A stateless variation of the Q-learning algorithm was presented in [14] to improve aggregate throughput of four coexisting network agents, however, this approach may not be adequate to solve

more complex problems with larger state space. Another RLbased approach [15] proposed a distributed learning solution (a multi-armed bandit approach) against the centralized coordination. The work was able to achieve significant improvement in probability of success in data transmission and energy utilization of devices. However, a direct communication link between the IoT sensors and destination was considered in [12] and [15], which may not exist in reality due to obstacles and distance, as illustrated in Fig. 1.
With the possibility of leveraging available MFRs within the network, the aforementioned problems of relying on a central controller or assuming a direct line-of-sight communication link between source and destination can be well addressed. To this end, we present a decentralised reinforcement learning approach where each autonomous mobile fog relay agent (MFRA) uses local information to learn to simultaneously improve communication performance and energy utilization within a fog-based IoT architecture.
III. ENVIRONMENT MODEL
In this section, we describe the system model. We consider a network topology with randomly deployed low-cost IoT sensors having sensed data to transmit to remote destinations. However, due to long distances or obstacles between source and destination nodes, we consider a scenario where available mobile fog relay (MFR) nodes are used to forward these data/service request from IoT sensor to destination devices, thus, minimizing communication outage. However, MFRs energy-constrained and deplete energy when they move and transmit to distant destinations.
We assume the scenario presented in Fig. 1 in Section I, with x number of sources (A, B, C, ..., x), with x number of destinations, and the number of relays varying from 1 ­ x. We observe that source A can reach the destination (local server) via two MFRs, source B via one MFR, and source C via three MFRs. For instance, when source A has data to send to the local server, it broadcasts a beacon via the wireless medium to all reachable neighbouring MFRs, NA  N , where NA denotes the set of all relays in the neighbourhood of source A i.e. NA = {R1, R2} and N denotes all available MFRs in the network i.e. N = {R1, R2..., Rn}. We assume that each MFR, Ri  N is capable of forwarding a message from a single source during a transmission phase. When NA receives the message from source A, it forwards same message to the destination. In a centralized approach, the central controller is assumed to have global knowledge of all network devices and selects the best performing relay based on some pre-defined quality-of-service (QoS) metrics at the time of evaluation [12]. However, we propose a decentralized approach where the local server broadcasts a feedback to all its neighbours, N D  N , where N D denotes the set of all MFRs in the neighbourhood of the destination D i.e. N D = {R1, R2, R3}. Since R3  NA, it discards the feedback message and remains passive for that transmission phase.

We model the outage behaviour from the environment using Equation (1) as in [4], which gives an estimate of the communication outage when the agent takes an action.

Pout = 1 - (1 + 22 ln ) exp

-

N0~ PI (DI + )-

,

(1)

where we define  = (N0~)/(PR(DS + )-). The outage probability Pout is defined as the probability that the signal-tonoise ratio falls below a predefined threshold ~. PI is transmit power of the IoT sensor, PR is transmit power of the fog relay agent, DI is the distance between IoT sensor and fog relay agent, and DS is the distance between fog relay agent and destination node.  denotes the small positional change moved by the fog relay, N0 is the channel noise power, and  is the path-loss exponent.

IV. MOBILE FOG RELAY AGENT (MFRA)
In this section, we present the reinforcement learning (RL) approach and briefly discuss the MFRA's states, actions and reward. In our scenario, the agents are the available mobile fog relays. The goal of the agent first, is to be alive during the transmission phase. Second, to relay message received from source to destination at a reasonably QoS by ensuring that the packets received in each transmission phase do not fall below some pre-defined threshold, which is set at 95%. Finally, endeavour to be active when there are no potential relays to convey same message from IoT sensor to remote destination. An agent observes the environment locally (the energy consumed by itself) and from feedback updates from the destination device (the communication performance and availability of potential neighbours forwarding same traffic), which is explained in more details in this section.

A. Reinforcement Learning Approach

We apply the Q-Learning algorithm, an RL approach similar

to [13] which requires no prior knowledge of the environment

by the agent. In Q-learning, the agent interacts with the

environment over periods of time according to a policy . At

every time-step k  K, the environment produces an observa-

tion sk  RDs . By sampling, the agent then picks an action ak

over (sk), ak  RDa , which is applied to the environment.

The environment consequently produces a reward R(sk, ak)

and may end the episode at state sK or transits to a new

state sk+1. The agent's goal is to maximize the expected cu-

mulative reward, max Es0,a0,s1,a1,...,sK

K i=0

i

Rk+i+1

,

where 0    1 is the discount factor, and Rk+i+1 is the

reward received at each state.

First, the agent takes an initial random action ak and gets

observations from the environment which corresponds to that

action, as well as a reward. The agent then updates it's Q-

values at each time-step k following Equation (2).

Q(sk, ak) := Q(sk, ak)
+  Rk+1 +  max Q(sk+1, a) - Q(sk, ak) ,
a
(2)

where  is the learning rate, which determines the impact of new experience on the Q-value, Rk+1 is the reward the agent receives by being in sk+1 from sk. Based on the policy followed by the agent, it gets observations and rewards from the environment.
B. Agent Design
The agent's states, actions and reward are explained below. States: The states are defined as a tuple, Outage communication cost (Pout) /Energy status of the fog relay /Neighbour potential to relay message (Availability of redundant nodes) . The agent then discretize the continuous observations emanating from the environment into a 3 × 3 × 3 state space.
· Outage communication cost: Outage observations from the environment is estimated using (1) from [4], which gives an estimate of the communication outage when the agent takes an action, such as move and transmit. The agent discretizes this observation into 3 states: 0% ­ 33%, 34% ­ 66%, and 67% ­ 100%.
· Energy status of the fog relay: This observation gives the agent insight on how much energy is consumed by the fog agent when following policy i  fog. If the fog agent continues to take sub-optimal actions, it depletes its energy and dies out. The agent discretizes this observation into 3 states: 0% ­ 33%, 34% ­ 66%, and 67% ­ 100%.
· Neighbour potential to relay message (Availability of redundant nodes): This observation gives the agent insight on the availability of redundant nodes that can help in relaying same type of message emanating from a particular IoT sensor. If there are no potential relay agent to convey message from an IoT sensor to a remote destination, then the agent should learn to remain active for that transmission phase. However, if there are one or more potential relays agents, the agent should learn to take no action to help conserve energy and improve the longevity of the network. The agent discretizes this observation into 3 states: no available redundant relay, 1 available redundant relay, and more than one available redundant relay.
Action space: The actions for the fog relay agent are move closer (-) and transmit, move farther (+) and transmit, and do nothing (become passive). An action is executed in each step until the end of an episode.
Goal: The agent's goal is to be alive during the transmission phase and ensure that packets received in each transmission phase do not fall below some pre-defined threshold, which is set at 95%.
Reward R: The reward function used is given in Equation (3) as

Environment

TABLE II SIMULATION PARAMETERS

Parameter
Simulation space PI PR  MFRA mobility bound Noise power N0 Path-loss exponent  Predefined threshold ~ Discount factor  Learning rate  Episodes N Maximum iteration runs Policy

Values
80 × 80 metres [0.001, 0.3] Watts 0.3 Watts ±0.25 metres [-30, 30] metres 2 × 10-7 Watts 3 1 0.9 0.1 100 100000 e-0.0015N

Agent

to taking sub-optimal actions without reaching the goal, or when the maximum step for an episode is reached. Intuitively, an MFRA should take an action of doing nothing if there are other available agents more capable of transmitting during that transmission phase, so as to conserve energy. On the contrary, if the fog relay agent continues to move and transmit even when there is sufficient redundancy within the network to relay same message, it may die out sooner, therefore causing a pointof-failure in the network.

Algorithm 1 MFRA Learning Process

1: Initialize: maxStep = 100000 2: for Episodes = 1, 2, 3, ... do 3: %% An episode ends when goal is Reached or

Agent dies or maxStep is reached

4: ResetEnvironment()

5: while goal not Reached and Agent alive and maxStep not reached do

6:

state  MapLocalObservationToState(Env)

7:

action  QLearning.SelectAction(state)

8:

%% AgentExecutesActionInState

9:

action.execute(Env)

10:

if action == "move close and Tx" or "move away and Tx" then

11:

%% MapToNewState

12:

Env.EstimateOutage using (1)

13:

Env.EstimateFogEnergyUsed

14:

Env.CheckAvailableActiveFogNeighbour

15:

state  MapObservationToState(Env)

16:

else if action == "do nothing" then

17:

%% MapToNewState

18:

Env.Outage  100%

19:

Env.FogEnergyUsed  0%

20:

Env.CheckAvailableActiveFogNeighbour

21:

state  MapObservationToState(Env)

22:

endif

23:

UpdateQLearningProcedure() (2)

24:

EvaluateReward() (3)

25: endwhile 26: endfor

R = 100, if goal is Reached

(3)

0, otherwise.

The MFRA's learning process is summarized in Algorithm 1. A learning episode is terminated when the agent attains the pre-defined goal or when the MFRA dies, possibly due

V. SIMULATION SETUP
In this section, we present the simulation setup. We carry out experiments using Python to evaluate the performance of the proposed Q-learning approach. The simulation parameters are summarized in Table II. In our scenario, we set each mobile fog relay (MFR) with a degree of 6, this implies that each

MFR can have a maximum of 6 neighbouring sources (IoT sensors). In each transmission phase, any of the 6 IoT sensors can transmit sensed data from its environment to neighbouring MFRs NA  N within its communication range, and each MFR is capable of receiving data from a single source A and forward the data to a destination device. It is possible for two or more MFRs, which are neighbours NA, to receive same message from a single source A as depicted in Fig. 1.
We compare the performance of the proposed RL-based approach with a baseline centralised approach based on [4], [5]. In a centralized approach, the central controller is assumed to have global knowledge of the entire network and selects a potential MFR that satisfies the highest QoS requirement (the communication link via a MFR with minimum outage) to become an active relay for that transmission phase. For instance, when the destination device receives the message from NA, the network controller examines the number of successfully delivered packets and selects the best MFR based on the successful packets delivery. Rather than have selection done by the central entity, we introduce a feedback mechanism where each destination device sends observations (the percentage of successfully delivered packets and the availability of potential redundant MFR) to its neighbours N D. Based on this feedback and local observation (which gives the energy status of the MFRA), each MFRA performs self-selection and learns to become passive for a particular transmission phase when there is an active MFRA within its neighbourhood.
We apply the Q-learning algorithm using 100 learning episodes. An episode ends either when the MFRA reaches its goal, dies-out due to energy outage, or when the maximum step for an episode is reached. We evaluate the performance of the proposed approach using two metrics, namely:
· Successful packets delivery: We evaluate this metric as the percentage of received packets at the destination node with respect to that transmitted by the IoT sensor via the active MFRA.
· Total energy consumption: We evaluate this metric as the percentage of total energy consumed with respect to the initial capacity of an active MFRA.
VI. RESULTS
In this section, we present experimental results and discussions. Fig. 2 show the number of iterations per cumulative reward of each agent over 100 learning episodes. From the figure, convergence for each MFRA is achieved in about 50 episodes. Without loss of generality, we consider results from the last 40 episodes of the simulation since each agent is expected to have learnt to behave. Furthermore, we carry out 50 runs of experiments to validate our findings.
Fig. 3 shows percentage of successfully delivered packets when different number of fog relays are used in the network. From the figure, the performance of both approaches was same when three or more mobile fog relays were used, however, the centralized approach performed poorly when compared with the proposed approach with two or less number of relays. Intuitively, when more redundant relays are deployed in the

1.0

Iterations/cummulative reward

0.8

0.6

MFRA - 1

MFRA - 2

MFRA - 3

0.4

MFRA - 4 MFRA - 5

MFRA - 6

MFRA - 7

0.2

MFRA - 8

MFRA - 9

MFRA - 10

0.0

episodes > 59

0

20

40

60

80

100

Episodes

Fig. 2. Number of iterations per cumulative reward of each agent vs. the number of episodes.

Successful packets delivery (%)

95

94

93

92

91

Q-learning

90

Centralized

Goal @ 95%

1 2 3 4 5 6 7 8 9 10 Number of fog relays

Fig. 3. Successful packets delivery vs. the number of fog relays.

Total energy consumption (%)

40

Q-learning

Centralized

35

30

25

20

15

10

5

0 1 2 3 4 5 6 7 8 9 10 Number of fog relays

Fig. 4. Total energy consumption vs. the number of fog relays.

network, we may observe better performance, but at higher hardware cost. However, the poor performance in the centralized approach shows the limitation of the central controller in selection. Fig. 4 illustrates the energy usage of active mobile fog relays when the number of fog relays in the network are varied. Interestingly, we observe an exponential decay in the consumed energy for both approaches, however, the energy utilization of the proposed approach outperformed the centralized approach with an improvement of about 56.76% ­ 88.03%. When more MFRs are deployed to the network, we

400

Q-learning agents total Q-learning per single agent

Centralized

300

Total computational time (seconds)

200

100

0 1 2 3 4 5 6 7 8 9 10 Number of fog relays
Fig. 5. Total computational time of different approaches vs. the number of fog relays.

observe lesser energy consumption. Intuitively, when there are available redundant relays within the network, lesser amount of energy is required by individual MFRs.
In Fig. 5, we show the computational time it takes the mobile fog relay to complete all episodes. From the figure, we observe that both approaches exhibit linear growth in total computational time as the number of fog relays increased. However, while the overall computation time increases, the decentralized approach effectively distributes computation across the MFRAs as seen in the Q-learning per single agent unlike the centralized approach. So in time-constrained scenarios, it is much quicker to achieve efficient energy utilization and successful packets delivery in the decentralized RL-based approach than the centralized approach.
VII. CONCLUSION AND FUTURE WORK
Communication outage within the IoT network can be minimized by leveraging available mobile fog relays (MFRs). However, these MFRs are power-constrained and deplete energy when they move and transmit to distant destinations. As such, power-control mechanisms and intelligent mobility of the relay devices are critical in improving communication performance and energy utilization. In this paper, we investigated a Qlearning-based decentralized approach where each autonomous mobile fog relay agent (MFRA) performs self-selection based on a feedback mechanism from the destination and use local information to simultaneously learn to improve communication performance and energy utilization. Comparative analysis was carried out with a baseline centralized approach. Results from simulations reveal that our proposed reinforcement learning (RL)-based approach achieved about 56.76% ­ 88.03% improvement in energy utilization as compared to the baseline. We observe that energy improvement may be achieved in the centralized approach only when more redundant relays are utilized, which is not cost effective. Furthermore, with lesser number of MFRs, the centralized approach failed to ensure reliable data delivery, unlike the proposed RL-based approach. However, increasing the traffic emanating from the heterogenous IoT sensors may have some impact on the performance of the network.

One of the most interesting directions for future work
is to evaluate the impact of learning on increased network
traffic from heterogenous sources and take into consideration
mission-critical IoT applications in order to meet stringent
QoS requirements.
ACKNOWLEDGEMENT
This publication has emanated from research supported in
part by a research grant from Science Foundation Ireland (SFI)
under Grant Number 13/RC/2077 and 16/SP/3804 and by Irish
Research Council through Surpass New Horizons award.
REFERENCES
[1] B. Omoniwa, R. Hussain, M. A. Javed, S. H. Bouk and S. A. Malik, "Fog/Edge Computing-based IoT (FECIoT): Architecture, Applications, and Research Issues," IEEE Internet of Things Journal, vol. 6, no. 3, pp. 4118-4149, June 2019.
[2] M. Chiang and T. Zhang, "Fog and IoT: An Overview of Research Opportuinities," IEEE Internet of Things Journal, vol. 3, no. 6, pp. 854864, Dec. 2016.
[3] A. BenMimoune and M. Kadoch, "Relay Technology for 5G Networks and IoT Applications," Internet of Things: Novel Advances and Envisioned Applications, vol. 25, pp. 3-26, April 2017.
[4] B. Omoniwa et al., "An Optimal Relay Scheme for Outage Minimization in Fog-based Internet-of-Things (IoT) Networks," IEEE Internet of Things Journal, vol. 6, no. 2, pp. 3044-3054, April 2019.
[5] A. A. Simiscuka and G. Muntean, "A Relay and Mobility Scheme for QoS Improvement in IoT Communications," 2018 IEEE International Conference on Communications Workshops (ICC Workshops), Kansas City, MO, 2018, pp. 1-6.
[6] H. Kawabata, K. Ishibashi, S. Vuppala and G. T. F. de Abreu, "Robust Relay Selection for Large-Scale Energy-Harvesting IoT Networks," IEEE Internet of Things Journal, vol. 4, no. 2, pp. 384-392, April 2017.
[7] A. Alsharoa, X. Zhang, D. Qiao and A. Kamal, "An Energy-Efficient Relaying Scheme for Internet of Things Communications," 2018 IEEE International Conference on Communications (ICC), Kansas City, MO, 2018, pp. 1-6.
[8] T. Lv, Z. Lin, P. Huang and J. Zeng, "Optimization of the EnergyEfficient Relay-Based Massive IoT Network," IEEE Internet of Things Journal, vol. 5, no. 4, pp. 3043-3058, Aug. 2018.
[9] Z. Behdad, M. Mahdavi and N. Razmi, "A New Relay Policy in RF Energy Harvesting for IoT Networks ­ A Cooperative Network Approach," IEEE Internet of Things Journal, vol. 5, no. 4, pp. 27152728, Aug. 2018.
[10] A. Manzoor, P. Porambage, M. Liyanage, M. Ylianttila and A. Gurtov, "DEMO: Mobile Relay Architecture for Low-Power IoT Devices," 2018 IEEE 19th International Symposium on "A World of Wireless, Mobile and Multimedia Networks" (WoWMoM), Chania, 2018, pp. 14-16.
[11] R. S. Sutton and A. G. Barto, Introduction to Reinforcement Learning, 1st ed. Cambridge, MA, USA: MIT Press, 1998.
[12] J. Hribar, A. Marinescu, G. A. Ropokis and L. A. DaSilva, "Using Deep Q-learning To Prolong the Lifetime of Correlated Internet of Things Devices" in arXiv:1902.02850
[13] M. Guériau and I. Dusparic, "SAMoD: Shared Autonomous Mobilityon-Demand using Decentralized Reinforcement Learning," 2018 21st International Conference on Intelligent Transportation Systems (ITSC), Maui, HI, 2018, pp. 1558-1563.
[14] F. Wilhelmi, B. Bellalta, C. Cano and A. Jonsson, "Implications of decentralized Q-learning resource allocation in wireless networks," 2017 IEEE 28th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC), Montreal, QC, 2017, pp. 1-5.
[15] A. Azari and C. Cavdar, "Self-Organized Low-Power IoT Networks: A Distributed Learning Approach," 2018 IEEE Global Communications Conference (GLOBECOM), Abu Dhabi, United Arab Emirates, 2018, pp. 1-7.

