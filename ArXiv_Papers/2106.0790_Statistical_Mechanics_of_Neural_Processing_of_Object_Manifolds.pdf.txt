arXiv:2106.00790v1 [q-bio.NC] 1 Jun 2021

Statistical Mechanics of Neural Processing of Object Manifolds
a dissertation presented by
SueYeon Chung to
The School of Engineering and Applied Sciences
in partial fulfillment of the requirements for the degree of
Doctor of Philosophy in the subject of Applied Physics
Harvard University Cambridge, Massachusetts
April 2017

©2017 ­ SueYeon Chung all rights reserved.

Thesis advisor: Haim Sompolinsky

SueYeon Chung

Statistical Mechanics of Neural Processing of Object Manifolds
Abstract
Invariant object recognition is one of the most fundamental cognitive tasks performed by the brain. In the neural state space, different objects with stimulus variabilities are represented as different manifolds. In this geometrical perspective, object recognition becomes the problem of linearly separating different object manifolds. In feedforward visual hierarchy, it has been suggested that the object manifold representations are reformatted across the layers, to become more linearly separable. Thus, a complete theory of perception requires characterizing the ability of linear readout networks to classify object manifolds from variable neural responses.
A theoretical understanding of the perceptron of isolated points was pioneered by Elizabeth Gardner who formulated it as a statistical mechanics problem and analyzed it using replica theory. In this thesis, we generalize the statistical mechanical analysis and establish a theory of linear classification of manifolds synthesizing statistical and geometric properties of high dimensional signals.
First, we study the theory of linear classification of simple spherical manifolds, such as line segments, L2 balls, or L1 balls. We provide analytical formula for classification capacity of balls, as a function of dimension, radius, and margin. We also find that the notion of support vectors needs to be generalized, and identify different support configurations of the manifolds, which has implications in generalization error.
Next, we present a Maximum Margin Manifold Machine (M4), an efficient iterative algorithm that can find a maximum margin linear binary classifier for manifolds with
iii

Thesis advisor: Haim Sompolinsky

SueYeon Chung

an uncountable set of training samples per each manifold. We provide a convergence proof with a polynomial bound on the convergence time. We further generalize M4 for non-separable manifolds with slack variables. We report that the number of training examples required to achieve the same generalization error is much smaller for M4, compared with traditional support vector machines.
Next, we generalize our theory further to linear classification of random general manifolds. We start with classification capacity of random ellipsoids, and generalize to classification capacity of general smooth and non-smooth manifolds. We identify that the capacity of a manifold is determined that effective radius, RM , and effective dimension, DM .
Finally, we show extensions to directions relevant for applications to real data. We have extended our general manifold classification theory to incorporate correlated manifolds, mixtures of manifold geometries, sparse labels and nonlinear classifications. Then, we analyze how object-based manifolds reformat in a conventional deep network (GoogLeNet). We find that the deep network indeed changes the manifolds in the direction that the capacity is increased.
This thesis lays the groundwork for a computational theory of neuronal processing of objects, providing quantitative measures for linear separability of object manifolds. We hope that our theory will provide new insights into the computational principles underlying processing of sensory representations in the brain. As manifold representations of the sensory world are ubiquitous in both biological and artificial neural systems, exciting future work lies ahead.

iv

Contents

1 Introduction

2

1.1 Invariant Object Computation . . . . . . . . . . . . . . . . . . . . . . . 2

1.2 Object Manifolds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4

1.3 Linear Separability of Manifolds . . . . . . . . . . . . . . . . . . . . . . 4

1.4 Theory of Linear Classification . . . . . . . . . . . . . . . . . . . . . . . 6

1.5 Outline of Thesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10

1.6 Chapter 2: Linear Separation of LpBalls . . . . . . . . . . . . . . . . . . 11

1.7 Chapter 3: The Max Margin Manifold Machine . . . . . . . . . . . . . 13

1.8 Chapter 4: Linear Classification of General Low Dimensional Manifolds 14

1.9 Chapter 5: Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . 17

1.10 Conclusion and Future Direction . . . . . . . . . . . . . . . . . . . . . 20

2 Linear Classification of Spherical Manifolds

22

2.1 Line Segments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23

2.2 D-dimensional Balls . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27

2.3 Lp Balls . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31

2.4 Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32

3 The Maximum Margin Manifold Machines

60

3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60

3.2 Maximum Margin Manifold Machines with Hard Margin . . . . . . . . 63

3.3 M 4 with Slack Variables . . . . . . . . . . . . . . . . . . . . . . . . . . 68

3.4 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75

3.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78

4 Linear Classification of General Manifolds

81

4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81

4.2 L2 Ellipsoids . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83

4.3 General Smooth Convex Manifolds . . . . . . . . . . . . . . . . . . . . 99

4.4 General Manifolds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106

v

4.5 Numerical Investigations . . . . . . . . . . . . . . . . . . . . . . . . . . 110 4.6 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121 4.7 Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123

5 Extensions

125

5.1 Correlated Manifolds . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125

5.2 Mixtures of Shapes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132

5.3 Class Imbalance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133

5.4 Building Multi Layer Networks of Sparse Classifiers . . . . . . . . . . . 140

5.5 Kernel Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145

5.6 Generalization Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151

5.7 Analysis of GoogLeNet Manifolds . . . . . . . . . . . . . . . . . . . . . 158

5.8 Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163

6 Appendix A: Symbols and Notations

166

6.1 Notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166

6.2 Mathematical Conventions . . . . . . . . . . . . . . . . . . . . . . . . . 169

7 Appendix B: Gardner's Replica Theory of Isolated Points 171

References

191

vi

Acknowledgments
This thesis is a testament of the immense amount of support I received from my advisors, mentors, friends, and family throughout the graduate school.
I have been extremely fortunate to have Haim Sompolinsky as my dissertation advisor, or more suitably, an academic father. Learning from Haim during the formative years of my academic career was one of the best things that have happened in my life. I am deeply indebted to Haim, for generously sharing his knowledge, ideas, feedback and time with me, and teaching me by example how to pick and ask interesting scientific questions and turning them into solvable ones. Most importantly, I am thankful to Haim for showing me the joy and fun in life dedicated to science.
I am also deeply thankful to Daniel D. Lee, whom I had an honor to work with, on most of the work presented in this thesis. I have learned a lot about formulating a problem into a mathematically attackable one by seeing how Dan does it elegantly. Dan has also been very generous with his help and suggestions ­ I am grateful for his warm mentorship and guidance.
I would like to thank Ryan Adams, my SEAS co-advisor, whose support has been vital for my graduate work at Harvard. I took my first machine learning class with Ryan, whose passion for the study of intelligent systems was an inspiration. I have also learned a lot by attending his group meetings and discussing ideas with him and his group members. I am thankful to Ryan for generous sharing of his time, feedback and advice.
vii

I would also like to thank the members of my thesis committee, Les Valiant and David Cox, whose guidance and advice have been invaluable. Their generous sharing of time and ideas have had an important impact on the direction and perspective of my research.
I would like to also thank Yoram Burak, with whom I have worked on the first project in graduate school on perceptual invariance as optimal decoding with neural noise, got me thinking about invariance. I would also like to thank Uri Cohen, with whom I had many valuable discussions on perceptual invariance. I have learned a lot from these collaborations.
I am indebted to the members of Sompolinsky lab and Center for Brain Science. I am grateful to Ran Rubin, Baktash Babadi, Cengiz Pehlevan, Julijana Gjorgjieva, James Fitzgerald, Yu Hu, Andrew Saxe, Madhu Advani, Dyuti Bhattacharya, Jonathan Kadmon, Itamar Landau, Gadi Mintz, Ariel Furstenberg for many discussions, feedback and encouragement. Many of them were my teachers and also great friends, and it has been a pleasure and privilege to be part of such an inspiring and supportive group.
I also thank Kenneth Blum, Grace Cho from CBS and Irit Dagan from Hebrew University. I also thank administrators at Harvard SEAS, GSAS and HIO for providing an essential support for my graduate work at Harvard.
I thank my friends at Harvard and elsewhere ­ Seongmin, Jean, Jeongmo, Jake Lee, Soomin, Sunha ­ for their support, encouragement and friendship. I am also grateful to my friends in Israel ­ Siwei, Neta, and Merav ­ for their caring friendship and making Jerusalem another home.
I am particularly thankful to Jo for always challenging me to be a better version of myself, through the continued support and encouragement, and sharing the great adventure of graduate school.
I am indebted to my parents, Youngmi and Oonjoo, who have always encouraged me to pursue my dream. I am deeply grateful for the opportunities they have given
viii

me. I am also thankful to my siblings, HaeYeon and Yunsoo, for being the best friends and cohort for life. I am forever grateful for my family's unwavering love, support and encouragement.
ix

Further Acknowledgements by Chapter
Chapter 2 appeared in Physical Review E as: Chung, SueYeon, Daniel D. Lee, and Haim Sompolinsky. "Linear readout of object
manifolds." Physical Review E 93.6 (2016): 060301.
Chapter 3 has been submitted for peer review as: Chung, SueYeon, Uri Cohen, Haim Sompolinsky, and Daniel D. Lee. "The Maximum
Margin Manifold Machine: Efficient Learning with Uncountable Sets of Training Examples"
An edited version of Chapters 4 will be submitted for a peer reviewed journal under the title:
Chung, SueYeon, Daniel D. Lee and Haim Sompolinsky. "Neural Processing of Object Manifolds"
Chapter 5 contains contributions from Daniel D. Lee and Haim Sompolinsky.
1

Chapter 1
Introduction
1.1 Invariant Object Computation
Object recognition is one of the most fundamental cognitive tasks performed by the brain. A successful object recognition requires a brain to discriminate between different classes of objects despite variabilities in the stimulus space. For example, a mammalian visual system can recognize objects despite a variation in the orientation, position, and background context, etc. Such impressive robustness to noise is not only specific to visual object recognition, but also similar tasks done by other brain modalities. Auditory systems are able to recognize auditory 'objects' such as songs, and languages, despite variabilities in the sound intensity, relative pitches, or sound textures (such as voice of a person). In general, human perception has to operate with discrete entities such as objects, faces, words, smells, and tasks. Hence, it is of fundamental interest to understand to evaluate the emergence of neural representations of these entities along the sensory hierarchies. Artificial intelligent systems aim to solve similar perceptual tasks. The recent success of Deep Networks has been foremost their ability to perform object recognition tasks despite the immense variabilities in the signals input representations,
2

in both training and testing examples1. An artificial face recognition tasks have to be done despite variabilities of facial expressions, image scale and occlusion, etc. Autonomous driving systems have to recognize objects in the driving environment fast and accurately, despite the various conditions such as speed of approach, location, confounding objects. Likewise, voice recognition systems need to overcome enormous variability in many stimulus dimensions. Indeed, it is a common practice in Machine Learning to augment the training set by performing a variety of transformations representing the natural inherent variability in the relevant object domain ('data augmentation'). Therefore, understanding how brain achieves an invariant object recognition tasks is not only important scientific challenge, but may also provide insight on how to improve artificial intelligent systems.
Figure 1.1: Activity Patterns as Points and Manifolds in the Neural State Space. (a) (Illustration) Firing Rate of N neurons (Neuron Index: 1,...,N), responding to two objects. Neural activity pattern shown as red line is a population response to a cat, and blue to a dog. (b) In the N dimensional neural state space, the population response is a N -dimensional vector, representing a point. The blue line in (a) , representing a response to a dog, is a point, rdog, in RN space. Likewise, the red line in (a) representing a cat is a point rcat in RN space (only 3 axis are shown for illustration). (c) When the stimulus variabilities are introduced, such as change in orientation, the neural responses undergo a smooth change, causing the point representing each object in the state space move around, forming an object manifold in the neural state space. Blue manifold is a set of neural activities representing a dog at different orientations, and a red manifold is a set of neural activities representing a cat in different orientations.
3

1.2 Object Manifolds
Consider a set of neurons responding to different objects (Figure 1.3(a)). Without additional variabilities, two stimuli belonging to different classes are mapped into two points in the neural state space, RN (Figure. 1.3(b)). We will occasionally call each such point, a neural state or an activity pattern. If however, one varies continuously the physical parameters in the stimulus space which do not change the object class, e.g., orientation, location, distortion, the neural state vector will vary so that the set of neural states or activity patterns that correspond to an object can be thought of as a manifold in the neural state space (Figure 1.3(a)). In this geometrical perspective, object recognition and discrimination can be viewed as the the task of discriminating or recognition of manifolds. These manifolds vary as the signals propagate from one processing stage to another. We will therefore refer to these manifolds also as neural manifolds or manifold representations, when dealing with object manifolds as they are reflected in the state space of a specific neural stage.
1.3 Linear Separability of Manifolds
So, how does the brain and the deep networks overcome stimulus variabilities in object recognition? In the feedforward visual hierarchy, it has been suggested that the stages of nonlinear transformations reformat the object manifolds so that they become increasingly easier to be readout out by a simple downstream neural systems2. The downstream circuit is assumed to implement a biologically plausible linear readout. Hence, the reformatting of object manifolds is translated as 'untangling' them so that they are eventually amenable to be separated by a linear classifier. The idea that 'intermediate' neural representations help to discriminate complex stimuli by a linear readout, has been applied to explain features of a variety of sensory representations in the brain (including 'mixed representations' in prefrontal cortex3, sparse expansions in
4

Figure 1.2: Untanglement of Object Manifolds in Sensory Hierarchy. Manifolds corresponding to different objects are thought to be highly tangled in the first stage of sensory processing (such as pixel layer representation), and undergo transformations across different layers in the sensory hierarchy and become more linearly separable in the downstream of the sensory processing.
neocortical4, memory allocations in hippocampal and cerebellar systems5). Deep Networks for object recognition has similarly employed an architecture where at the top layer a linear classifier operates as a readout of the networks.
Linear separation of neural manifolds can be described by a decision hyperplane that separates entire manifolds to one of the two sides of the hyperplane, fig. 1.3. The separating hyperplane is determined by the vector w, a direction vector normal to the hyperplane. The components of this vectors are the synaptic weights of the Linear Readout, also known as the Perceptron, as it computes the weighted sum of each vector and thresholds the result to produce a binary output. One of the focus of this work is to evaluate what aspects of the the neural manifolds representation gives a better linear separability. Before continuing it is important to emphasize that by separating manifolds we mean separating all points on the manifolds according to a rule that assigns
5

Figure 1.3: Invariant Object Discrimination as Linear Separation of Manifolds. (a) A perceptron with a weight w, which can classify different objects corresponds to a hyperplane characterized by the orthogonal vector w which separates the two object points in the state space (b). (c) In this perspective, a perceptron weight w, which can classify different objects with their variabilities, corresponds to a hyperplane characterized by the orthogonal vector w, which separates the two object manifolds in the state space.
to all points belonging to a single manifold the same label. Thus, at any given time, the system classifies a single input vector.
1.4 Theory of Linear Classification
Quantifying linear separability has been extensively studied in the context of linearly classifying isolated points. Perceptron capacity, first introduced by Cover6. He asked the following question in his formulation of Cover's Theorem. Suppose there are P points in an N-dimensional ambient space, and they are in general position. Each of the point represent a distinct pattern, and half of the points are labeled positive, and the other half negative. Then, what is the maximum number of P where most of the dichotomies are linearly separable? If there are only a few points, it is easy to find a linearly separable solution, and with an increasing number of points, it becomes harder to find linearly separable solution. When there are too many points, they become linearly non-separable. He derived an analytic formula for the probability that a random
6

Figure 1.4: Linear Separability of Points: Cover's Theorem of Perceptron Capacity. Cover's theorem specifies the perceptron capacity for isolated points in general position, as the maximum number of points P (in dimension N ) for which at least half of the possible dichotomies have a linear classifier.

classification of P points in N dimensions can be implemented by perceptron as

2 C(P, N )
2P =





N-1 P - 1

k=0 



k

2P

(1.1)

and the notion of perceptron capacity deals with the question of what is the maximum number of patterns allowed for linear such that almost all dichotomies are linearly separable (Figure 1.4). Cover's perceptron capacity refers to the maximum number of patterns Pmax allowed per ambient dimension N , also known as load ( = Pmax/N ),such that the probability of linear separability is larger than 0.5. VC dimension refers to the maximum load  such that the probability of linear separability is 1(Figure 1.4).

7

A statistical mechanical theory of the perceptron was first introduced by Elizabeth Gardner7,8. Gardner's theory is extremely important as it provides accurate estimates of the Perceptron capacity beyond the Cover theorem. In particular, the theory allows to evaluate the capacity for solutions with a given robustness measures. Similar to Support Vector Machines9. robustness of linear classifiers can be quantified by the margin, ie., the distance between the separating hyperplane and the closest point. And the solutions with maximum margins are known as the SVM solutions.
Formally, Gardner's theory evaluates the maximal number of points in RN for which there is a vector w that obeys the following set of inequalities

yµ (w · xµ + b) /||w|| >   0; yµ = ±1

(1.2)

Unlike Cover result, the answer to this question depends on the statistics of the inputs and labels. The simplest case is where all components xµi are iid with zero mean and finite variance (which can be taken as 1). (The shape of the distribution are less important as long as mild conditions are obeyed). The labels are randomly assigned to these points each with probability 1/2 . Finally, the theory becomes exact in the the thermodynamic limit N, P  , while , , = O(1) . Using replica theory in the theory of spin glasses (more detailed treatment is in the appendix to the chapter), Gardner has evaluated analytically the volume of possible solutions for a given load  and margin  . The volume is exponentially large (in N ) below the capacity, G() and is zero above it. The maximal margin solution is right at the border between the two regimes. Using the vanishing volume condition, Gardner obtained an elegant expression for the perceptron capacity with finite margin 

G() =



-1

Dt (t + )2

-

(1.3)

where

Dt

=

exp(-

1 2

t2)

(2)1/2

dt

(Figure.

1.5(a)).

Furthermore, the Gardner framework

8

Figure 1.5: Gardner's Perceptron Theory: Capacity and Support Vectors. (a) Gardner's replica analysis specifies the perceptron capacity  = P/N as a function of margin . P is number of points, N is the network size. (b) The fraction of support vectors amongst the total number of points can be calculated as a function of margin  . At zero margin, half of the points are support vectors (black solid), and the other half are interior points, that are in the space shattered by the hyperplane (black dashed). The fraction of support vectors in crease with increasing margin .
allows for the calculation of fraction of support vectors on the margin, which has an important bearing on its robustness and generalization performance (Ref) (Figure. 1.5(b)).
Gardner theory is also applicable to more complex statistical ensembles, such as the case of sparse labels where the labels are not uniformly distributed. However, the current theory is inapplicable to the problem of manifold classification, where the strong correlations between points belonging to the same manifold is of primary importance. The thesis addresses the following questions:
1. What is the capacity of manifolds, and the nature of solution? What geometric features of the manifolds are relevant for the manifold capacity?
2. How to implement the practical aspects of analyzing data manifolds numerically? In order to test the manifold capacity with simulation, what is the most efficient algorithm to find a classification solution for manifolds? To get an estimate of the manifold
9

capacity, how to numerically solve it? 3. What are the necessary extensions required to understand and analyze more
realistic problems? We extend it to manifold classification problem with sparse labeling, correlation, classification with nonlinearities such as multilayer and nonlinear kernels, and apply the theory to realistic data.
1.5 Outline of Thesis
This thesis introduces a theory that generalizes Gardner's analysis of perceptron capacity for isolated points to the perceptron capacity for manifolds. The theory assumes (most of the time) that the manifolds span a low dimensional hyperspace (strictly speaking the embedding dimension is held finite as N  ). In the following chapters, we introduce a set of investigations that lays groundwork for a comprehensive theory of linear manifold classification. In chapter 2, we provides the basic tools for applying the replica theory to compute linear classification of manifolds. Here we focus on the simple manifolds: line segments, L2 balls, and Lp balls. In chapter 3, we address the numerical question of how to solve max margin problems on manifolds, which consists of uncountable set of training examples. We use methods from Quadratic Semi-Infinite Programming (QSIP) to develop a novel algorithm denoted M4 (Max Margin Manifolds Machines). In chapter 4, we generalize the theory of chapter 2 to address more complex manifold geometries, for both smooth and non-smooth manifolds. In chapter 5, we present a set of important extensions of the theory to cover more realistic conditions, such as correlated manifolds, and sparse coding tasks. We also discuss extensions to nonlinear manifold classifications. Finally, we demonstrate how the theory can be applied to analyze deep networks for in visual object recognition.
10

1.6 Chapter 2: Linear Separation of LpBalls

Figure 1.6: Linear Classification of Lp balls. (a) Classification of line segments with length 2R. (Example of D = 1 balls with radius R). (b) Classification of D-dimensional L2 balls with radius R. (c) Classification of D-dimensional L1 balls with radius R.

In this chapter we lay the ground for the statistical mechanical theory of linear classification of manifolds. We consider manifolds which can be described as Lp balls in D dimensions with a radius R . We write points on the manifolds as,

xµ =

D
xµ0 + R siuµi
i=1

(1.4)

where xµ0 is the center of the µth ball, µ = 1, ..., P . The axes of the balls are given by the D vectors uµi where i = 1, ..., D. The vector s parameterizes the point on the

ball and obeys the constraint ||s||p  1 . The case of p = 2 corresponds to the usual

Euclidean balls in D dimensions. The case of D = 1 is the special case of line segments

with length 2R. Other examples are shown in fig. 1.6. As we show in this chapter,

linear classification of these balls corresponds to the requirements that the closest points

on each manifolds obeys inequalities, eq 1.4 above. For the Lp balls, with 1 < p  1,

this amounts to the following constraints (where we consider zero bias for simplicity)

hµ0 - R||hµ||q  ,

(1.5)

q = p/(p - 1), 1 < p < 
11

(1.6)

q=0<p1

(1.7)

where hµ0 = ||w||-1yµw·xµ are the fields induced by the centers and hµi = ||w||-1yµw· uiµ are the fields induced by the i th basis vectors of µth manifold,  is the margin of the linear classifier.
Importantly, linear classification of manifolds depends on the geometric properties of the convex hulls of the data manifolds. Thus, when p  1, the convex hull of the manifold becomes faceted, consisting of vertices, flat edges and faces. For these geometries, the constraints on the fields associated with a solution vector w becomes: hµ0 - R maxi hµi   for all p < 1 (Fig. 1.6(c)).
The statistical mechanical theory evaluates the average of the log of solution volume,

V=

P

dN w 

w 2=N

µ=1

hµ0 - R||hµ||q - 

.

(1.8)

and identifying the point of vanishing volume allowed us to evaluate the capacity,

in the form of Bp(, R, D) for various norms Lp . Beyond the capacity, the theory provides an important insight into the nature of the max margin solution. In particular

it generalizes the notion of support vectors to support manifolds. As we show, some of

the support manifolds are fully embedded in the margin hyperplanes, some are touching

the planes in a single point, while in the case of L1balls, they may have edges or faces in the hyperplanes. These properties have important implications for noise robustness

of the solutions. Finally, these examples already reveal the tradeoff between D and R,

and the effect of large R and large D. Specifically, we show that for large D L2 balls,

 B(, R, D 1) = (1 + R2)G( + R D), D 1

(1.9)

relating linear separation of balls to linear separation of points with an additional effec
tive margin R D.

12

1.7 Chapter 3: The Max Margin Manifold Machine
Figure 1.7: The Maximum Margin Manifold Machine Performance. (a) Object-based manifolds are created by affine transformations of images from ImageNet dataset. (b) Generalization error versus number of training samples, for the linear SV M (blue markers) and the Ms4imple algorithm, when the manifolds are in the separable regime. (b) Generalization error versus the number of training samples for slack-SVM (blue) and Ms4lack algorithm when manifolds are in the non-separable regime (above capacity). Our Ms4imple and Ms4lack show superior generalization error performance for the same number of training samples.
Most learning algorithms assume the number of training examples is finite. In this work, we consider the problem of classifying data manifolds utilizing the underlying manifold structure consisting of an uncountable number of points. We propose an efficient iterative algorithm called M 4 that solves a quadratic semi-infinite programming problem to find the maximum margin solution. Our method is based upon a cutting-plane approach which converges to an approximate solution in a finite number of iterations. We provide a proof of convergence as well as a polynomial bound on the number of iterations and training examples required for a desired tolerance in the objective function. The efficiency and performance of M 4 are demonstrated on high-dimensional synthetic data in addition to object manifolds generated by continuous transformations of images from the ImageNet dataset. Our results indicate that M 4 is able to rapidly learn good classifiers and shows superior generalization performance than traditional support vector machines using data augmentation methods (Fig. 1.7).
13

1.8 Chapter 4: Linear Classification of General Low
Dimensional Manifolds
In this chapter we generalize the perceptron capacity for the classification of manifolds further, to classification of general manifolds. The theory is exact in the thermodynamic limit, i.e., N, P  ,  = P/N is finite as in the Gardner's analysis. In addition, for the mean field theory to be exact, the dimensionality of the manifolds D has to be finite in this limit (note: this holds except for the special case of parallel manifolds, section 5.1.1, where D is proportional to N ). To set the stage, we first consider linear classification capacity of L2 ellipsoids. We present explicit analytical solution to the classification problem, and show that the capacity and solution properties depend in general on all D radii. Like the balls, the max margin solution is characterized by two types of support ellipsoids (touching or fully embedded).
Effective Sizes and Dimensions: In general, manifolds considered here are characterized by several dimensionalities. All points on all manifolds are in RN , so N is the ambient dimension. All points on a given manifold (relative to its center) span D dimensions, thus D is the manifold embedding dimension. In addition, manifolds may be characterized by intrinsic dimensionality which may be much smaller than D. See Fig. 1.8(a) for an example of a string in D dimension. This intrinsic dimension is important practically, but will not play an important role in the theory of linear classification. In addition to the above, the manifold classification properties may be described in certain regime by effective dimensions and effective size (Fig. 1.8(b)).
Here we present the results for ellipsoids in the important limit of large D. In this limit we find that the capacity can be well approximated as,

E(, R) = (1 + RE2 )G( + RE DE), D 1

(1.10)

where E stands for ellipsoids, and with effective ellipsoid radius RE and effective

14

Figure 1.8: Manifold's Sizes and Dimensions. (a) Dimensions of a Random String. A random string's degree of freedom (intrinsic dimension) is 1, but is spanning D-dimensional (embedding dimension) and defined in N ambient dimension. (b) Effective manifold radius RM and effective manifold dimension DM , which are relevant properties for the manifold's linear classification capacity.

ellipsoid dimension DE given by,

RE2

=

D i=1

(1

Ri4 + Ri2)2

/

D j=1

(1

Rj2 + Rj2)2

(1.11)

DE =

D Ri2 i=1 1 + Ri2

2

/

D i=1

(1

Ri4 + Ri2)2

(1.12)

where Ri are the different radii of the ellipsoid. Finally, when the radii are small, Ri 1

(i.e., relative to the center norms which is normalized here to 1). these quantities reduce

to the simple formulae

RE2 =

i Ri4 i Ri2

(1.13)

15

DE = (

i Ri2)2 i Ri4

=

Dsvd

(1.14)

where Dsvd is the participation ratio evaluated from the SVD of the ellipsoids (with a

uniform measure). These results set the stage for a derivation of a theory applicable

to general low dimensional manifolds. Briefly, general smooth convex manifolds behave

qualitatively the same as the ellipsoids, for the geometric reason that they can either

be interior to, fully embedded in or touching the margin planes.

Non-smooth manifold can have a large spectrum of overlaps with the planes (as the

example of L1 ball indicates). Nevertheless, we have derived self consistent mean field

equations that describe the capacity (and solution properties) for a general manifold,

and present numerical procedures to solve these equations iteratively. Here we briefly

discuss the theoretical prediction for the limit of large D. In this regime, capacity is

well approximated by

M () = (1 + RM 2 )G( + RM DM ), D 1

(1.15)

with self consistent equations for RM and DM , which need to be solved numerically by iterative mean field methods. Remarkably, in the regime where RM 1, RM and DM simplify to the quantities shown in Fig. 1.8(b) and are related to the well known Gaussian Mean Width of convex bodies (Fig. 4.3).
An important application of this theory is finite point cloud manifolds that arise when subsampled points of each potentially continuous manifold is given. In this case, RM and DM (of the training manifolds) can be estimated from the given finite training set. The interesting question of how these quantities are related to the effective radius and dimension underlying full manifold is touched upon in the following section. An interesting example is the case of L1balls in D dimensions with radius R. In the limit

16

of large D and small R, the effective radius is simply R but the dimension is

DM = 2 log D

(1.16)

In general, in other point cloud manifolds we expect that DM  log m where m is the number of samples per manifold.
Infinite size manifolds: Finally, it should be noted that as the manifold size grows to infinity (in all dimensions), their geometric details don't matter; only the number of dimensions they span. Here we obtain

DM  D

(1.17)

reflecting the need of the classifying weight vector to be orthogonal to all the DP dimensional hyperspace that the manifolds span, namely the capacity reduces to

M

=

1 D

(1.18)

where D denotes the embedding dimensions of the manifolds (where we assume for simplicity that the manifolds are not bounded in any of the D directions).

1.9 Chapter 5: Extensions
In Chapter 5, we further extend the theory in directions likely relevant to applications to real data. We have extended our general manifold classification theory to incorporate correlated manifolds, mixtures of manifold geometries, sparse labels and nonlinear classification, see Fig. 1.9. We highlight here briefly several important results. 1. Correlated manifolds: when manifold axes are strongly parallel (fig. 1.9(a)) we expect the capacity to be relatively large. For example if their spanning spaces are fully aligned but they are large in extent, w can solve the problem by orthogonalize to the D common
17

Figure 1.9: Extensions of Manifold Classification Theory. (a) Classification of Correlated Manifolds. (b) Classification of Mixtures of Manifolds. (c) Sparse Coding (Classification with Sparse Labels) and Object Recognition Limit (One versus All Classification). (d) Extension to Kernel Framework. (Red/black) 2D disks in the kernel input space, transformed to (blue/black) 5D manifolds in the quadratic kernel's feature space.

directions (rather than DP in the uncorrelated case). Interestingly, for high dimensional parallel balls we find a phase transition whereby above some finite critical radius the max margin solution fully orthogonalize to the manifolds subspace. In real data we expect positive correlations but not full alignment of the different manifolds.

2. Sparse labels: In this case, the fraction of say plus manifolds, f , is smaller than

that of the minus ones. In many real life tasks this is to be expected. An extreme case is

that of object recognition task defined as classifying one manifold as one and the rest as

minus one.

This can be

viewed as a

binary

classification with f

=

1 P

.

As in Gardner's

theory the capacity grows as f  0. However, we show that the size of the manifolds

substantially limits this growth. For instance, in balls with large radius R , the f is

small but larger than 1/R the capacity remains of order unity.

3. Nonlinear manifold separation: We consider two schemes of two layer classification of manifolds in cases where they are not linearly separable. One is in the form of a nonlinear kernel, similar to Kernel SVM. For this we present a version of the M 4algorithm in a 'dual' form, appropriate for kernels. We briefly discuss the effect of the kernel on the geometry of the manifold and the classification capacity. The second architecture is that of hidden layer of binary units, forming a sparse intermediate representation of the

18

manifolds. We show how this extra layer formed by unsupervised learning can enhance the capacity and robustness of the classification of the manifolds.

4. Generalization properties : Computation with manifolds raises a specific type of generalization problem, namely how training with a subsampled training points perform when new points from the same underlying manifolds are presented in the test phase. Exact analytical expression for the generalization error is complicated; also the error depends on the assumed sampling measure on the manifold (whereas the separability problem is measure invariant). However, in the case of linearly separable manifolds with high D we can use the insight from the above theory (the notions of effective dimensions and radii) to derive a particularly simple approximation. Assume  is such that the full manifolds are linearly separable with a max margin . Then the generalization error will eventually vanish as more samples per manifold , m , are presented. In the limit of large m , we obtain,



g (m)



exp[- 2 m

log

m]

(1.19)

Interestingly, this decay is faster than the generic power law, g(m)  m-1 of general-

ization bounds in linearly separable problem and reflects the presence of finite margin

of the entire manifold. We also discuss the generalization error of subsampled manifolds

in the case where the full manifolds are not linearly separable.

5. Application to Deep Networks: We close this section by applying some of the theoretical concepts to Deep Networks trained to perform visual object recognition tasks. We show how the theory can be used to characterize the change in the geometry of the manifolds, and changes in the manifold correlation structure at different stages of the network (using ImageNet10 as an example).

19

1.10 Conclusion and Future Direction
In this thesis, we generalized Gardner's theory of linear classification of points to the classification of general randomly oriented low dimensional manifolds. The theory, exact in the thermodynamic limit, describes the relation between the detailed geometry of the convex hulls of the data manifolds and the ability to linearly classify them. The problem simplifies considerably when the manifold dimension is high. In this limit, the classification properties depend on two geometric parameters of the convex hulls: the effective dimension DM and effective radius RM . In high dimensional manifold with small sizes,
 capacity depends on RM and DM mainly through the scaling relation RM DM . This quantity is related to the well known Gaussian Mean Width of convex bodies. Optimal solution exhibits support manifold structures with potential consequences for noise robustness. We developed a novel efficient training algorithm, the Maximum Margin Manifold Machines, for finding the maximum margin solution for classifying manifolds with uncountable number of training samples, and provide convergence proof with polynomial bounds on the number of iterations required for convergence. Our theory has been extended to incorporate correlations in the manifolds, mixtures of shapes, sparse coding, nonlinear processing such as multilayer network or kernel framework, as well as an analysis of manifold generalization error. With these extensions, our theory provides qualitative and quantitative measures for assessing the ability to decode object information from the different stages of Deep biological and artificial neural networks.
Ongoing work includes suggesting design principles for deep networks by taking into account the network size, dimension, sparsity, as well as role of nonlinearities in reformatting of the manifolds such that the capacity is increased. Whether manifold capacity can be used as an object function of the training of a network is an interesting question to pursue. We are exploring applications of our theory on several neural data bases from IT and other areas in visual cortex, responding to different object stimuli with a variety of physical transformations. We hope that our theory will provide new insights into the
20

computational principles underlying processing of sensory representations in the brain. As manifold representations of the sensory world are ubiquitous in both biological and artificial neural systems, exciting future work lies ahead.
21

Chapter 2
Linear Classification of Spherical
Manifolds
High-level perception in the brain involves classifying or identifying objects which are represented by continuous manifolds of neuronal states in all stages of sensory hierarchies2,11­16 Each state in an object manifold corresponds to the vector of firing rates of responses to a particular variant of physical attributes which do not change object's identity, e.g., intensity, location, scale, and orientation. It has been hypothesized that object identity can be decoded from high level representations, but not from low level ones, by simple downstream readout networks2,11,15,17­21. A particularly simple decoder is the perceptron, which performs classification by thresholding a linear weighted sum of its input activities22,23. However, it is unclear what makes certain representations well suited for invariant decoding by simple readouts such as perceptrons. Similar questions apply to the hierarchy of artificial deep neural networks for object recognition19,24­27. Thus, a complete theory of perception requires characterizing the ability of linear readout networks to classify objects from variable neural responses in their upstream layer.
A theoretical understanding of the perceptron was pioneered by Elizabeth Gardner
22

who formulated it as a statistical mechanics problem and analyzed it using replica theory7,28­35. In this work, we generalize the statistical mechanical analysis and establish a theory of linear classification of manifolds synthesizing statistical and geometric properties of high dimensional signals. We apply the theory to simple classes of manifolds and show how changes in the dimensionality, size, and shape of the object manifolds affect their readout by downstream perceptrons.
2.1 Line Segments
One-dimensional object manifolds arise naturally from variation of stimulus intensity, such as visual contrast, which leads to approximate linear modulation of the neuronal responses of each object. We model these manifolds as line segments and consider classifying P such segments in N dimensions, expressed as {xµ0 + Rsuµ}, -1  s  1, µ = 1, ..., P . The N -dimensional vectors xµ  RN and uµ  RN denote respectively, the centers and directions of the µ-th segment, and the scalar s parameterizes the continuum of points along the segment. The parameter R measures the extent of the segments relative to the distance between the centers (Fig. 2.1).
We seek to partition the different line segments into two classes defined by binary labels yµ = ±1 . To classify the segments, a weight vector w  RN must obey yµw · (xµ + Rsuµ)   for all µ and s. The parameter   0 is known as the margin; in general, a larger  indicates that the perceptron solution will be more robust to noise and display better generalization properties9. Hence, we are interested in maximum margin solutions, i.e., weight vectors w that yield the maximum possible value for . Since line segments are convex, only the endpoints of each line segment need to be checked, namely min hµ0 ± Rhµ = hµ0 - R |hµ|   where hµ0 = ||w||-1yµw · xµ are the fields induced by the centers and hµ = ||w||-1yµw · uµ are the fields induced by the line directions.
23

Figure 2.1: (a) Linear classification of points. (solid) points on the margin, (striped) internal points. (b) Linear classification of line segments. (solid) lines embedded in the margin, (dotted) lines touching the margin, (striped) interior lines. (c) Capacity  = P/N of a network N = 200 as a function of R with margins  = 0 (red) and  = 0.5 (blue). Theoretical predictions (lines) and numerical simulation (markers, see Appendix for details) are shown. (d) Fraction of different line configurations at capacity with  = 0. (red) lines in the margin, (blue) lines touching the margin, (black) internal lines.
24

2.1.1 Replica Theory

The existence of a weight vector w that can successfully classify the line segments depends upon the statistics of the segments. We consider random line segments where the components of xµ and uµ are i.i.d. Gaussians with zero mean and unit variance, and random binary labels yµ. We study the thermodynamic limit where the dimensionality N   and number of segments P   with finite  = P/N and R. Following Gardner7 we compute the average of log V where V is the volume of the space of perceptron solutions:

V=

P

w

2=N

dN w

 (hµ0
µ=1

-

R

hµ

- ) .

(2.1)

(x) is the Heaviside step function. According to replica theory, the fields are described as sums of random Gaussian fields hµ0 = tµ0 + z0µ and hµ = tµ + zµ where t0 and t are quenched components arising from fluctuations in the input vectors xµ and uµ respectively, and the z0, z fields represent the variability in hµ0 and hµ resulting from different solutions of w. These fields must obey the constraint z0+t0-R |z + t|  . The capacity function L(, R) (the subscript L denotes the line) describes for which P/N ratio the perceptron solution volume shrinks to a unique weight vector. The reciprocal
of the capacity is given by the replica symmetric calculation (details provided in the
Appendix 2.4.1):

L-1(, R) =

1 min z0+t0-R|z+t| 2

z02 + z2

t0,t

(2.2)

where the average is over the Gaussian statistics of t0 and t. To compute Eq. (2.2), three regimes need to be considered. First, when t0 is large enough so that t0 > +R |t|, the minimum occurs at z0 = z = 0 which does not contribute to the capacity. In this regime, hµ0 >  and hµ > 0 implying that neither of the two segment endpoints reach the

25

margin. In the other extreme, when t0 < -R-1|t|, the minimum is given by z0 = -t0 and z = - |t|, i.e. hµ0 =  and hµ = 0 indicating that both endpoints of the line segment lie on the margin planes. In the intermediate regime where  - R-1 |t| < t0 <  + R |t|, z0 =  - t0 + R|z + t--, i.e., hµ0 - R|hµ| =  but hµ0 > , corresponding to only one of the line segment endpoints touching the margin. In this regime, the solution is given by minimizing the function (R |z + t| +  - t0)2 + z2 with respect to z. Combining these
contributions, we can write the perceptron capacity of line segments:

L-1(, R) = +


Dt
-

Dt
-

+R|t| -R-1|t|

Dt0

(R

|t| + R2

 +

- 1

t0)2

-R-1|t|
Dt0 ( - t0)2 + t2
-

(2.3)

with integrations over the Gaussian measure, Dx  1

e-

1 2

x2

dx.

It

is

instructive

to

2

consider special limits. When R  0, Eq. (2.3) reduces to L(, 0) = 0() where 0()

is Gardner's original capacity result for perceptrons classifying P points (the subscript

0 stands for zero-dimensional manifolds) with margin  2.1-(a). Interestingly, when

R

=

1,

then

L(, 1)

=

1 2

 0(/ 2).

This

is

because

when

R

=

1

there

are

no

statistical

correlations between the line segment endpoints and the problem becomes equivalent 
to classifying 2P random points with average norm 2N .

Finally, when R  , the capacity is further reduced: L-1(, ) = 0-1() + 1. This is because when R is large, the segments become unbounded lines. In this case,

the only solution is for w to be orthogonal to all P line directions. The problem is then

equivalent to classifying P center points in the N - P null space of the line directions,

so that at capacity P = 0()(N - P ).

We see this most simply at zero margin,  = 0. In this case, Eq. (2.3) reduces to

a

simple

analytic

expression

for

the

capacity:

L-1(0, R)

=

1 2

+

2 

arctan

R

(Appendix

2.4.1). The capacity is seen to decrease from L(0, R = 0) = 2 to L(0, R = 1) = 1

and L(0, R = ) =

2 3

for

unbounded lines.

We have

also

calculated analytically

the

26

distribution of the center and direction fields hµ0 and hµ 36. The distribution consists of three contributions, corresponding to the regimes that determine the capacity. One component corresponds to line segments fully embedded in these planes. The fraction of these manifolds is simply the volume of phase space of t and t0 in the last term of Eq. (2.3). Another fraction, given by the volume of phase space in the first integral of (2.3) corresponds to line segments touching the margin planes at only one endpoint. The remainder of the manifolds are those interior to the margin planes. Fig. 2.1 shows that our theoretical calculations correspond nicely with our numerical simulations for the perceptron capacity of line segments, even with modest input dimensionality N = 200. Note that as R  , half of the manifolds lie in the plane while half only touch it; however, the angles between these segments and the margin planes approach zero in this limit. As R  0 , half of the points lie in the plane36.

2.2 D-dimensional Balls

Higher dimensional manifolds arise from multiple sources of variability and their non-

linear effects on the neural responses. An example is varying stimulus orientation,

resulting in two-dimensional object manifolds under the cosine tuning function (Fig.

2.2(a)). Linear classification of these manifolds depends only upon the properties of

their convex hulls37. We consider simple convex hull geometries as D-dimensional balls

embedded in N -dimensions:

x0µ + R

D i=1

si

uµi

, so that the µ-th manifold is cen-

tered at the vector xµ  RN and its extent is described by a set of D basis vectors

uµi  RN , i = 1, ..., D . The points in each manifold are parameterized by the D-

dimensional vector s  RD whose Euclidean norm is constrained by: s  1 and the

radius of the balls are quantified by R . Statistically, all components of xµ0 and uµi are i.i.d. Gaussian random variables with
zero mean and unit variance. We define hµ0 = N -1/2yµw · xµ as the field induced by the

27

manifold centers and hµi = N -1/2yµw · uµi as the D fields induced by each of the basis 
vectors and with normalization w = N . To classify all the points on the manifolds correctly with margin , w  RN must satisfy the inequality hµ0 - R||hµ||   where ||hµ|| is the Euclidean norm of the D-dimensional vector hµ whose components are hµi . This corresponds to the requirement that the field induced by the points on the µ-th manifold with the smallest projection on w be larger than the margin .
We solve the replica theory in the limit of N, P   with finite  = P/N , D, and R. The fields for each of the manifolds can be written as sums of Gaussian quenched and entropic components, t0  R, t  RD and z0  R, z  RD , respectively. The capacity for D-dimensional manifolds is given by the replica symmetric calculation (Appendix 2.4.2):

B-1(, R, D) =

1 min t0+z0-R t+z > 2

z02 +

z2

t0,t

(2.4)

where B stands for L2balls. The capacity calculation can be partitioned into three

regimes. For large t0 >  + Rt, where t = t , z0 = 0 and z = 0 corresponding to

manifolds which lie interior to the margin planes of the perceptron. On the other hand,

when t0 <  - R-1t, the minimum is obtained at z0 =  - t0 and z = -t corresponding

to manifolds which are fully embedded in the margin planes. Finally, in the intermediate

regime, when  - R-1t < t0 <  + Rt, z0 = R t + z - t0 +  but z = -t indicating

that these manifolds only touch the margin plane. Decomposing the capacity over these

regimes and integrating out the angular components, the capacity of the perceptron can

be written as:

B-1(, R, D) = +


dt D(t)
0

+Rt

-

1 R

t

Dt0

(Rt

+  - t0 R2 + 1

)2


dt D(t)

-

1 R

t

Dt0

( - t0)2 + t2

0

-

(2.5)

28

where

D (t)

=

21-

D 2

(

D 2

)

tD-1

e-

1 2

t2

is

the

D-Dimensional

Chi

probability

density

function.

For large R  , Eq. (2.5) reduces to: B-1(, R = , D) = 0-1()+D which indicates

that w must be in the null space of the P D basis vectors {uµi } in this limit. This case

is equivalent to the classification of P points (the projections of the manifold centers)

by a perceptron in the N - P D dimensional null space.

To probe the fields, we consider the joint distribution of the field induced by the

center, h0, and the norm of the fields induced by the manifold directions, h  h . There are three contributions. The first term corresponds to h0 - Rh > , i.e. balls

that lie interior to the perceptron margin planes; the second component corresponds to

h0-Rh =  but h > 0, i.e. balls that touch the margin planes; and the third contribution represents the fraction of balls obeying h0 =  and h = 0, i.e. balls fully embedded in the margin. The dependence of these contributions on R for D = 2 is shown in Fig.

2.2(b). Interestingly, when  = 0 , the case of R = 1 is particularly simple for all D

. The capacity is B(R = 1, D) = 2/(D + 1) ; in addition, the fraction of embedded

and interior balls are equal and the fraction of touching balls have a maximum, see Fig.

2.2(b) and Appendix.

In a number of realistic problems, the dimensionality D of the object manifolds

could be quite large. Hence, we analyze the limit D 1. In this situation, for the

capacity

to

remain

finite,

R

has

to

be

small,

scaling

as

R



D-

1 2

,

and

the

capacity

 is B(, R, D)  0( + R D). In other words, the problem of separating P high

dimensional balls with margin  is equivalent to separating P points but with a margin 
 + R D. This is because when the distance of the closest point on the D-dimensional 
ball to the margin plane is , the distance of the center is  + R D (see Fig. 2.2).

When R is larger, the capacity vanishes as B(0, R, D)  1 + R-2 /D. When D is

large, making w orthogonal to a significant fraction of high dimensional manifolds incurs

a prohibitive loss in the effective dimensionality. Hence, in this limit, the fraction of

manifolds that lie in the margin plane is zero. Interestingly, when R is sufficiently large,

29

Figure 2.2: Random D-dimensional balls: (a) Linear classification of D = 2 balls. (b) Fraction of 2-D ball configurations as a function of R at capacity with  = 0, comparing theory (lines) with simulations (markers). (red) balls embedded in the plane, (blue) balls touching the plane, (black) interior balls. (c) Linear classification of balls with D = N at margin  (black circles) is equivalent to point classification of centers with effective margin +R N (purple points). (d) Capacity  = P/N for  = 0 for large D = 50 and R  D-1/2 as a function of R D. (blue solid) B( = 0, R, D) compared with 0( = R D) (red square). (Inset) Capacity  at  = 0 for 0.35  R  20 and D = 20: (blue) theoretical  compared with approximate form (1 + R-2)/D (red dashed).
30

 R  D, it becomes advantageous for w to be orthogonal to a finite fraction of the manifolds.

2.3 Lp Balls

To study the effect of changing the geometrical shape of the manifolds, we replace the

Euclidean norm constraint on the manifold boundary by a constraint on their Lp norm.

Specifically, we consider D-dimensional manifolds

xµ0 + R

D i=1

si

uµi

where the D

dimensional vector s parameterizing points on the manifolds is constrained: s p  1.

For 1 < p < , these Lp manifolds are smooth and convex. Their linear classification by a vector w is determined by the field constraints hµ0 - R||hµ||q   where, as before, hµ0 are the fields induced by the centers, and ||hµ||q, q = p/(p-1), are the Lq dual norms of the D-dimensional fields induced by uµi (Appendix 2.4). The resultant solutions are

qualitatively similar to what we observed with L2 ball manifolds.

However, when p  1, the convex hull of the manifold becomes faceted, consisting

of vertices, flat edges and faces. For these geometries, the constraints on the fields associated with a solution vector w becomes: hµ0 - R maxi hµi   for all p < 1 . We have solved in detail the case of D = 2 (Appendix 2.4.3). There are four manifold

classes: interior; touching the margin plane at a single vertex point; a flat side embedded

in the margin; and fully embedded. The fractions of these classes are shown in Fig. 2.3.

Discussion: We have extended Gardner's theory of the linear classification of isolated
points to the classification of continuous manifolds. Our analysis shows how linear separability of the manifolds depends intimately upon the dimensionality, size and shape of the convex hulls of the manifolds. Some or all of these properties are expected to differ at different stages in the sensory hierarchy. Thus, our theory enables systematic analysis of the degree to which this reformatting enhances the capacity for object classification at the higher stages of the hierarchy.
31

Figure 2.3: L1 balls: (a) Linear classification of 2-D L1 balls. (b) Fraction of manifold configurations as a function of radius R at capacity with  = 0 comparing theory (lines) to simulations (markers). (red) entire manifold embedded, (blue) manifold touching margin at a single vertex, (gray) manifold touching with two corners (one side), (purple) interior manifold.
We focused here on the classification of fully observed manifolds and have not addressed the problem of generalization from finite input sampling of the manifolds. Nevertheless, our results about the properties of maximum margin solutions can be readily utilized to estimate generalization from finite samples. The current theory can be extended in several important ways. Additional geometric features can be incorporated, such as non-uniform radii for the manifolds as well as heterogeneous mixtures of manifolds. The influence of correlations in the structure of the manifolds as well as the effect of sparse labels can also be considered. The present work lays the groundwork for a computational theory of neuronal processing of objects, providing quantitative measures for assessing the properties of representations in biological and artificial neural networks.
2.4 Appendix
2.4.1 Perceptron Capacity of Line Segments.
The simplest example of linear separability of manifolds is when the manifolds are line segments. Specifically,
32

we consider the problem of classification of P line segments of length 2R, given by

{xµ0 + Rsuµ} , |s|  1, µ = 1, ...P

(2.6)

the N -dimensional vectors x0µ and uµ, which are, respectively, the centers and the directions of the µ segment. [We use the boldface style to denote N -dim vectors]. We consider random line segments, specifically assume that the components of all x0µ and uµ are i.i.d. normally distributed (with zero mean and unit variance). The target classification labels of the manifolds are yµ = ±1 and are drawn randomly with equal probability of ±1.
We search for an N -dimenional weight vector w that classifies correctly the line segments. Since the line segments are convex this is equivalent to the requirement that w classifies correctly the end points of each segments, This condition can be written using two local fields for each segment. One is the field induced by the center of the line xµ, giving

hµ0 = ||w||-1yµw · x0µ The other is the field induced by the direction vector uµ,

(2.7)

hµ = ||w||-1yµw · uµ

(2.8)

Note that all the fields are defined with the target label yµ, and they are normalized by the norm of w. With these definitions, hµ0 ± Rhµ are the signed distance of the endpoints of the segment from the separating plane which is the plane orthogonal to w . Thus, w has to obey,

hµ0 - R hµi  

(2.9)

33

where hµ0 - R hµi is the field of the endpoint with the smallest (signed) distance to the plane. The parameter  > 0 is a parameter defining two margin planes. According to Eq. (2.9) all the positively labeled inputs must lie either above the 'positive' margin plane. Conversely the negatively labeled points must lie below the negative margin plane (See Fig. 1).

Replica Theory
We consider a thermodynamic limit where N, P   whereas  = P/N , and R are finite. We use the Gardner framework to compute the volume of space of solutions.

V = dN w(w2 - N )Pµ=1(hµ0 - R hµ - )

(2.10)

where  is the Heaviside function. According to replica theory, ln V

= limn0

V

n
n

-1

,

where V n can be written as,

n
Vn =
=1

P







dw(w2 - N )

dhµ+

dhµ-

dh~ µ+

dh~ µ- X(2.11)

µ=1 



-

-

X =e

{ } ± ih~µ±

hµ± -

1 N

yµwT (x0µ±Ruµ)

(2.12)

where hµ+ = hµ0 + Rhµ, hµ- = hµ0 - Rhµ. Averaging over the random inputs xµ and uµ , the above fields can be written as sums of two random fields, where t0 and t are the quenched component resulting from the quenched random variables, namely the input vectors xµ0 and uµi , while the z0and z fields represent the variability of different w's within the volume of solutions for each realization of inputs and labels,

hµ0 = qtµ0 + 1 - qz0µ, hµ = qtµ + 1 - qzµ

(2.13)

34

where

the

replica

symmetric

order

parameter

q

is

q

=

1 N

w

·

w ,



=



.

The

resultant 'free energy' G is:

where,

V n t0,t = eN n[G(q)] = eN n[G0(q)+G1(q)]

(2.14)

G0(q)

=

1 2

ln(1

-

q)

+

q 2(1 -

q)

(2.15)

is the entropic term representing the volume of w subject to the constraint that

q

=

1 N

w

· w

.

The

classification

constraints

contributes

G1(q) = lnZ(q, t0,t) t0,t

(2.16)





Z(q, t0, t) = Dz0 Dz

-

-

qt0 +

1 - qz0 - R qt +

1 - qz -  (2.17)

where

Dx



dx 2

exp

-

x2 2

and

the

average

wrt

t0,

t

denotes

integrals

over

the

gaussian

variables t0, t with measures Dt0 and Dt, respectively. Finally, q is determined by

solving

G q

=

0

.

Solution

with

q

<

1

indicates

a

finite

volume

of

solutions.

For

each



there is a maximum value of  where a solution exists. As  approaches this maximal

value, q  1 indicating the existence of a unique solution, which is the max margin

solution for this .

In this chapter we focus on the properties of the max margin solution, i.e., on the

limit q  1.

35

q  1 Limit We define

Q

=

q 1-q

and

study

the

limit

of

Q



.

In

this

limit

the

leading

order

is

G0

=

Q 2

.

(2.18)

ln V

=

Q 2

[1

-

g(t0, t)

t0,t]

(2.19)

where,

g



-

2 Q

log Z

is

independent

of

Q

and

is

given

by

replacing

the

integrals

in

Eq. (4.16) by their saddle point, yielding

g(t0, t) =

min [z02 + z2]

z0+t0-R|z+t|

(2.20)

 Note that here we have scaled variables z0 and z such that z0  Qz0 and similarly

for z.

Finally, at the capacity, ln V vanishes, hence

1-1(, R) = g(t0, t) t0,t where we have denoted the capacity for one dimensional manifolds as 1.

(2.21)

Capacity
The nature of solution of Eq. (2.20) depends on the values of t and t0. There are three regimes.

a) Regime 1:

t0 -  > R t

(2.22)

in which case the solution is z0 = z = 0 which does not contribute to Eq. (2.21).

36

For values of t0 -   R||t||, the solution obeys z0 + t0 - R||z + t|| =  , meaning that one of the endpoints touches the margin plane. This regime is further divided into two cases.

b) Regime 2:

-R-1||t|| < t0 -  < R||t||

(2.23)

Here, the center field z0 + t0 is larger than the margin (i.e., the center points are interior) and the fields can be determined by minimizing Eq. (2.20) (R|z+t|-t0+)2+z2 w.r.t. z yielding

z

=

R2||t|| + R( 1 + R2

-

t0)

(2.24)

z0

=

R||t|| +  - 1 + R2

t0

and its contribution to Eq. (2.21) is

(2.25)

g

=

(

-

t0 + R||t||)2 1 + R2

(2.26)

c) Regime 3:

t0 -  < -R-1||t||

(2.27)

Here the center points are also on the margin plane, hence z = -||t|| and z0 + t0 = , contributing

g = (t0 - )2 + t2 Finally, combining the contributions from Regimes 2 and 3 yields,

(2.28)

37



L-1(, R) =

Dt

-

+R|t| -|t|R-1

Dt0

(|t|R - (t0 - (1 + R2)

))2

+

-|t|R-1
Dt0((t0 - )2 + t2)
-
(2.29)

For  = 0, this expression reduces to



L-1(0, R) =

Dt

-

R|t| -|t|R-1

Dt0

(|t|R - t0)2 (1 + R2)

+

-|t|R-1
Dt0(t20 + t2)
-

(2.30)

By switching to polar coordinates: t = r cos , t0 = r sin , these integrals reduce to

L-1(R)

=

1 2

+

2 

arctan R

(2.31)

Limits of R

In the limit of R  0, Eq. (2.29) reduces to L(, R = 0) = 0() where 0() is the

Gardner's result for classifying P random points.

Interestingly,

L(, 1)

=

1 2

 0(/ 2)

.

This is because when R = 1 the distance

between edge points on the line segments is statistically the same as that between

points of different segments, hence the problem is equivalent to classifying randomly 2P 
points with norms 2N .

Finally, when R  , the capacity becomes







L-1(, R = ) =

Dt

Dt0t2 +

Dt0((t0 - )2 + t2)

-



-

(2.32)

38



=1+

Dt0((t0 - )2 = 1 + 0-1()

-

(2.33)

The reason for this is that when R is large, the manifolds are essentially unbounded lines. The only way to classify them correctly is for w to be orthogonal to all P lines, reducing the problem to classifying P points which are the projections of the centers on the null space of the lines. Thus, this is equivalent to classifying random points in a space with dimensionality N - P = N (1 - ) from which Eq. (2.32) follows. These limits can be readily seen in the simple case of  = 0. It is readily seen from Eq. (2.31) that  = 2, 1, and 2/3 for R = 0,1, and  respectively.
Distribution of Fields
It is instructive to calculate the distribution of fields P (h0, h) induced by the manifolds with the max margin solution w. Using the above theory, we find that

1







P (h0, h) =

Z

Dz0
-

Dz
-

qt0 + 1 - qz0 - R qt +

(h0 - qt0 - 1 - qz0)(h - qt - 1 - qz) t,t0

1 - qz -(2.34) (2.35)

Considering the three above regimes for (t, t0), we obtain the dominant contribution in the limit of Q  ,

P (h0, h) = A(h0, h)(h0 - R||h|| - ) + B(h0)(||h|| - R-1(h0 - )) + C(h0 - )(h) (2.36)

A(h0,

h)

=

exp(-

1 2

(h20

2

+

h2) ,

h0

-

R|h|

-





0

(2.37)

39

B(h0) = 2

1

+ R-2 2

H

(-R-1

)

exp

- (1 + R-2)(h0 - R-2 )2 2

, h0  0

(2.38)

C=

where H(x) =

 x

Dz

,

and

-R-1||z||

Dz

Dt

-

The integrated weights are:



=

 1 + R-2


dh0dhA(h0, h) = 2 DtH( + Rt)
0

(2.39) (2.40) (2.41)



+R|t|



dh0B(h0) =

Dt

Dt0 = 2 Dt H( - tR-1) - H( + Rt) (2.42)

-

-|t|R-1

0



-|t|R-1



C = Dt

Dt0 = 1 - 2 DtH( - tR-1)

-

-

0

(2.43)

The first term represents the fraction of line segments that are interior to the margin

plane (corresponding to Regime 1); the second component corresponds to segments that

touch the margin planes but do not lie on the margin plane (Regime 2); the third term

corresponds to the segments that lie completely on the margin planes (see Fig. 1 in

main text). When R  , we obtain,

dh0B(h0) = H()

(2.44)

40

C = 1 - H()

(2.45)

The reason for this is as follows. when R  , w becomes increasingly orthogonal to all the directors, hence the fraction of interior points vanish. The value of B represents the fraction of segments that touch the margin planes. The fields associated with the centers is finite, larger than . However, the angle between the segments and w vanish, since the angle is roughly ||h|| which is R-1(h0 - ) . In contrast, the fields of the segments represented by C equal , hence they lie in the margin planes. Thus, in this limit, the fields are the same as the separation of the centers in the null space (of dimension N - P ).

2.4.2 Perceptron Capacity of D-dimensional Balls

We now consider linear classification of higher dimensional manifolds, modeling them as D dimensional balls with radius R,

D
x0µ + R siuµi , s, ||s||  1
i=1

(2.46)

[We use sign to denote D - dimensional vectors and ||...|| for L2 norm ]. For each manifold, the center xµ0 , and the D basis vectors {uµi } are N dimensional vectors (i = 1, .., D), the components of which are all independent Gaussian random variables with
zero mean and unit variance. The target labels of the manifolds are random assignments
of yµ = ±1. To classify all the points on the manifolds correctly (with a given margin), 
the weight vector w (normalized for convenience by ||w|| = N ), must satisfy

D

hµ0

+

R

s,

min
||s||2=1

sihµi  

i=1

(2.47)

41

where hµ0 = N -1/2yµw · xµ is the field induced by the manifold centers and hµi =

N -1/2yµw · uµi i = 1, ..., D are D fields induced by each of the basis vectors. Differ-

entiating

D i=1

sihµi

+



i s2i (where  is a Lagrange multiplier enforcing the norm

constraint) wrt si, we obtain,

si

=

- hµi ||hµ||

where ||hµ|| is the L2 norm of the D-dimensional vector hµi , hence and the constraints can be written as

(2.48) i sihµi = -||hµ||

hµ0 - R||hµ||  

(2.49)

Geometrically, the LHS corresponds to the field induced by the point on the manifold µ which has the smallest (signed) projection on w. We consider a thermodynamic limit where N, P   while  = P/N , D, and R are finite.

Capacity
The replica theory as outlined above, yields

where as before, and

V n t0,t = eN n[G(q)] = eN n[G0(q)+G1(q)]

G0(q)

=

1 2

ln(1

-

q)

+

q 2(1 -

q)

(2.50) (2.51)

G1(q) = ln Z(q, t0,t) t0,t

(2.52)

42





Z(q, t0, t) =

Dz0

Dz

-

-

qt0 +

1 - qz0 - R||qt +

1 - qz|| -  (2.53)

where

hµ0 = qtµ0 + 1 - qz0µ, hµi = qtµi + 1 - qziµ, i = 1, ..., D

(2.54)

and ||...|| is the L2 norm of the D-dimensional vectors. All variables z0, t0, z, t are normally distributed.





ln Z(q, t0, t) = ln Dz0 Dz 

-

-

Qt0 + z0 - R||qt + 1 - qz|| - 

where

the

saddle

point

behavior

in

the

limit

of

q



1,

Q





gives

g



-

2 Q

log Z,

1

g(t0,t)

=

min
t0+z0-R||t+z||>

2

z02 +

z2

and the capacity is given by

(2.55)

B-1(, R, D) = g(t0, t) t0,~t Again, there are three regimes.

(2.56)

a) Regime 1: Defining t = ||t||, when t0 -  > Rt: then z0  0, z  0 g  0
corresponding to manifolds which obey the inequality (not equality) of Eq. (4.7), hence are interior to the plane.

43

b) Regime 2: When -R-1t < t0 -  < Rt: then z0 = R||t + z|| - t0 +  and

z = -zt/t

the scalar z can be calculated by

g  min 1 z2

(R(t - z) - t0 + )2 + z2

(2.57) (2.58)

z

=

R2t

- R( - 1 + R2

t0)

(2.59)

z0

=

Rt +  - 1 + R2

t0

(2.60)

g

=

(

- t0 - Rt)2 1 + R2

(2.61)

c) Regime 3:

When

t0

<

-

1 R

t:

then

z

=

t

and

z0

=

 - t0

so

that

g



(t0 - )2 + t2

.

Combining these contributions, the capacity is:



B-1(, R, D) =

dtD(t)

0

+Rt

-

1 R

t

Dt0

(Rt

- t0 + R2 + 1

)2

+

-

1 R

t

Dt0([t0

-

]2

+

t2)

-

(2.62)

where D is the D-dim Chi distribution,

D(t) =

Dt

t - ||t||

21-

D 2

tD-1e-

1 2

t2

=

(

D 2

)

Distribution of Fields

(2.63)

We consider the joint distribution of two fields: h0 which is the field induced by the manifold centers, and h  ||h||, namely the L2 norm of the D dimensional vector of

44

fields induced by the D ui's. Taking into account the above three regimes, we have,

P (h0, h) = A(h0, h)(h0 - Rh - ) + B(h0)(h - R-1(h0 - )) + C(h0 - )(h) (2.64)

1. Field Distribution for  = 0.

A(h0, h)

=

exp(- 

1 2

h20)

2

D

(h),

h0

-

R|h|



0

(2.65)


B(h0) = (1 + R2)-1 Dt0 dtD(t) (t0 - (1 + R-2)h0 - R-1t)
h

2. Integrated Weights:

C=

-R-1|z|

Dz

Dt

-


dh0dhA(h0, h) = dtPD(t)H( + Rt)
0

(2.66) (2.67) (2.68)

dh0B(h0) =


dtD(t)
0

+Rt

Dt0 =

-

1 R

t


dtD(t)
0

H (

-

t R)

-

H (

+

Rt)

(2.69)

C=


dtD(t)
0

-

t R

Dt0 = 1 -

-

 0

dtD (t)H (

-

t R)

(2.70)

As in the case of line segments, the first term corresponds to the fraction of D-dim

balls that lie in the interior space; the second component corresponds to the fraction of

balls that touch the margin planes, whereas C stands for the fraction of balls that are

45

fully embedded in these planes.
R=1 1. Capacity for  = 0 In the case of R = 1, the capacity obtains a simple form:



B-1( = 0, R = 1, D) =

dtD(t)

0

+t -t

Dt0 t2

+ 2

t20

-

+t
Dt0tt0 +
-t

-t
Dt0(t02 + t2)
-





B-1( = 0, R = 1, D) =

dtD(t)

Dt0(t02 + t2)

0

0

B-1(

=

0, R

=

1, D)

=

D

+ 2

1

2. Manifold Geometry Configurations for  = 0

a) Interior vs. Embedded: The fraction of embedded manifolds:



-t

pembedded = dtD(t)

Dt0

0

-

Fraction of interior manifolds:





pinterior = dtD(t) Dt0 = pembedded

0

t

The fraction of touching manifolds:



t

ptouching = dtD(t) Dt0

0

-t

Thus, the fraction of interior manifolds and embedded manifolds are equal. .

46

b) Touching Manifolds: In general,


ptouching = dtD(t)
0

+Rt

Dt0

-

1 R

t


= dtD(t) [1 - H(Rt) - H(t/R)]
0
The radius R at which ptouchingis at maximum can be found by

 R

ptouching

=


dtD(t) -tH (Rt) + tR-2H (t/R) = 0
0

The solution for above is R = 1 for all D. For D = 2 , ptouching(R = 1, D = 2) =

 0

dt2(t)[1

-

2H

(t)]



0.7.

Therefore, at R = 1, the fraction of touching disks is at maximum, and for D = 2,

the value is about 0.7. .

Large R Limit
In the limit of large R, Eq. (2.62) reduces to:







B-1(, R = , D) =

dtD(t)

Dt0t2 +

Dt0([t0 - ]2 + t2)

0



-

(2.71)





=

dtD(t)t2 +

Dt0(t0 - )2 = 0-1() + D

0

-

(2.72)

which reflects the fact that when R is large w must be in the null space of the P D vectors uµi ; thus, the classification problem is that of P points (i.e., the projections of the centers onto the null space) in N - P D dimensions. Likewise, in this limit A

vanishes and the angle between the manifold centers and the margin planes vanish.

47

Limit of Large D

In many realistic problems it is expected that the dimension of the object manifolds is

large, hence it is of interest to examine the results in the limit of D 
D(t) is centered around t = D, yielding

1. In his limit,

B-1,D

1=





+R


D Dt0 (R

-

D R

D - t0 + R2 + 1

)2

+



-

D R

Dt0([t0

- ]2

+ D)

-

 As long as R D , the second term in Eq. (2.73) vanishes and yields

(2.73)

B-1,D

1=

 +R D -

Dt0

 (R D - t0 +
R2 + 1

)2

=

0-1

(

+

 R D)

1 + R2

(2.74)

Thus, remains finite in the limit of large D only if R is not larger than the order of 
D-1/2. If, on the other hand, R D 1, Eq. (2.74) implies

B-1,D

R2D 1 = 1 + R2

(2.75)

(where we have used the asymptote 0-1(x)  x for large x).
Numerically, this approximation works very well for R  0.5 and all D (as long as 
R D).

Field Distribution in Large D : In the limit of large D , the fraction of manifolds
that lie on the margin plane, C, is zero. The overall fraction of interior manifolds 
is H( + R D) whereas the fraction of manifolds that touch the margin planes is 
1 - H( + R D) .

48

Figure 2.4: Lp balls. Illustration of Lp balls of norm p for (a) p = 0.5, (b) p =1.5, (c) p = 1. Notice that rotation of the axis introduces the reduction of effective R by factor of 1/ 2.


Large R : In the limit of R  D,

B-1,R

1=D



 Dt0 + D

-

D R



-

D R

Dt0 = D

-

(2.76)

Note that in this case, both terms in Eq. (2.73) contribute. This reflects the fact 
that when R is O( D) it is again advantageous for w to be orthogonal to some of the

spheres. This is seen in the field distribution. In this limit, it consists of a fraction of





H( D/R) lying on the plane whereas the fraction of touching balls is 1 - H( D/R).

 Finally, when R/ D is large, most of the spheres lie on the margin, as expected.

2.4.3 Perceptron Capacity of Lp Manifolds

We consider manifolds defined with Lp norm,
D
x0µ + R siuµi , s, s p  1
i=1

(2.77)

49

where s p is the Lpnorm of s . Linear classification requires

D

hµ0

+

R

s,

min
s p=1

i=1

sihµi





1 < p < : Differentiating i sihµi +  i si p wrt si yields,

(2.78)

si

=

-sign(hµi )

||hµi ||1/(p-1) (||hµ ||q )1/p

(2.79)

where

q

=

p p-1

is

the

dual

norm

of

p

,

hence,

mins,

s p=1

D i=1

si

hµi

=

-

hµ

q.

Thus, linear classification of Lp manifolds is equivalent to the constraints on the fields,

hµ0 - R hµ q  

(2.80)

Smoothness of the Lq norm guarantees that the solution will be qualitatively similar to spheres (i.e., p = q = 2). (See Fig. 2.4(a) in the Appendix)

0 < p  1: In this regime differentiating with respect to si does not minimize i sihµi .
Instead, the minima are at the D extremal points: si = 1, sj = 0, j = i corresponding to the corners of the manifolds (see Fig. 2.4(b)). Thus, for all p  1 the linear classification constraint is the same and is given by the corner with the smallest projection on w , namely

hµ0

-

R

max
i

hµi





We can now use the replica theory, where now the capacity is given by

(2.81)

B-p1(, R, D) = g(t0, t) t0,t

(2.82)

50

where Bp stands for Balls with Lp norm,

where,

g(t0,t)

=

min
t0+z0-R maxi(zi+ti)>

1 2

z02 +

z2

(2.83)

z0 + t0 - R max(zi + ti)  
i

(2.84)

L1 in D = 2
Rotated Coordinates Without loss of generality, we assume the ti are ordered: t2 
t1  0 and similarly for zi + ti . It is easier to consider the following transformation

t1

=

1 
2

(t2

-

t1)

(2.85)

t2

=

1 (t2 2

+

t1)

(2.86)

z1

=

1 
2

(z2

-

z1)

(2.87)

z2

=

1 
2

(z2

+

z1)

 R = R/ 2 see the geometry of the rotation in Fig. 2.4 (c).

(2.88) (2.89)

51

In

these

coordinates

and

convention,

maxi(zi

+

ti)

=

z2

+

t2

=

1 2

i(zi + ti) hence,

g(t0,t) =

min

t0+z0-R z+t

1 1> 2

z02 +

z

2

where we have dropped the primes.

General solution:

(2.90)

z = -a, a > 0

(2.91)

The sign is well defined only for t + z = 0 . Hence the general solution takes the form

a) t0 -  > R||t||1 :

zi = - min(a, ti), a  0

(2.92)

b) t0 -  < R|t|1 :

z0, z = 0

(2.93)

z0 =  - t0 + R| z + t 1

(2.94)

This is consistent if

a=

R(R

t 1 - t0 + ) 1 + 2R2

(R g=

t 1 - t0 + )2 (1 + 2R2)

0<

R(R

t 1 - t0 + ) 1 + 2R2

< t1

(2.95) (2.96) (2.97)

52

t0 -  > R t 1 - R-1(1 + 2R2)t1 = t1

c) t0 < t1

t0 -  > t1

Assume

t1 = R(t1 + t2) - R-1(1 + 2R2)t1 = R(t2 - t1) - R-1t1

z2 = -a, z1 = -t1, t2 > a > t1

a

=

R(Rt2 - t0 + 1 + R2

)

g

=

t21

+

(Rt2 - t0 + )2 (1 + R2)

d) t0 < t2

t0 -  > -R-1t2 = t2 z = -t

g = (t0 - )2 + t2

(2.98) (2.99)
(2.100)
(2.101) (2.102) (2.103) (2.104)
(2.105) (2.106)

53

Capacity Finally, converting back the above regimes and values of g to the original
coordinates, we have

B-11 = 8


Dt2
0

t2
Dt1
0

+Rt2 +Rt1-R-1

(t2- t1

)

Dt0

(Rt2 - t0 + (1 + R2)

)2

(2.107)


+8 Dt2
0

t2
Dt1
0

+Rt1-R-1(t2-t1)
Dt0
-R-1(t1+t2)

(t2

- t1)2 2

+

(

1 2

R(t1

+

t2)

-

t0

(1

+

1 2

R2)

+ )2

(2.108)



t2

-R-1(t1+t2)

+8 Dt2 Dt1

Dt0 (t0 - )2 + t21 + t22

0

0

-

(2.109)

where the subscript for B-11 is used to denote capacity for balls with L1 norm. 1. R  0

2. R  



t2



B-11 = 8 0

Dt2
0

Dt1

Dt0(-t0 + )2 = 0-1()

-

(2.110)

B-11 = 8


Dt2
0

t2
Dt1
0


Dt0


(t2 - t1)2 + (t1 + t2)2 2

(2.111)



t2



+8 Dt2 Dt1

Dt0 [t0 - ]2 + t21 + t22

0

0

-

(2.112)













=

Dt2

Dt1

Dt0 t21 + t22 +

Dt2

Dt1

Dt0[t0 - ]2 (2.113)

-

-

-

-

-

-

B-11 = 2 + 0-1()
54

(2.114)

as expected in this case of D = 2. The effective dimensionality is N - 2P .

Fields. The integrated weight of manifolds that touch the margin planes is



t2

+Rt2

8 Dt2 Dt1

Dt0

0

0

+Rt1-R-1(t2-t1)

The integrated weight of manifold that have a side on the planes is

(2.115)



t2

+Rt1-R-1(t2-t1)

8 Dt2 Dt1

Dt0

0

0

-R-1(t1+t2)

The fraction of manifolds that lie on the planes is



t2

-R-1(t1+t2)

8 Dt2 Dt1

Dt0

0

0

-

(2.116) (2.117)

2.4.4 Simulation Details
Linear Classification of Line Segments
Linear Classification of Line Segments. The classification problem of P line
segments is cast in the form of linear classification of the 2P endpoints where each pair of endpoints receive the same target label. These labeled inputs were classified using IBM cplex package which uses quadratic programming solving the primal support vector problem. To compute the network capacity  = P/N , 100 trials were used for each P and the fraction of converged trials was computed. P was gradually increased. Maximum capacity was defined as the value of P for which the convergence rate reached 0.5. In Fig. 1(c) (main text) N = 200 was used. To obtain the capacity for  = 0.5, P was varied until SVM's maximum margins averaged over 100 runs was close to  = 0.5.

Fraction of Line Segment Configurations. Once the data was determined to
be separable, the fraction of the different line segment configurations was computed.

55

Each line segment's configuration was determined based on the number of endpoints on the main plane. Endpoints were considered to be on the margin plane iff their field was larger than the margin by an amount smaller than a tolerance of = 10-8 .
Linear Classification of D-dimensional L2 balls Sampling of L2 balls of Dimension D. Unlike the case of the line segments,
where it is sufficient to consider the endpoints, finding SVM solution for classifying D dimensional L2 norm balls requires an iterative algorithm to sample the points on the L2 balls so that the decision plane is efficiently determined. First, we sample randomly a number of points on all manifolds and find the max margin solution w and its margin , for this set of points. Next, for each manifold we find analytically the point on the boundary which has the minimum (signed) distance from the decision plane given by w. If the field of this point lies below the margin this point is added to the training data and a new w is computed. This iterative procedure stops when all the minimal points lie above or on the current margin, guaranteeing the correct classifications of the entire manifolds. For details, see Alg.1.
Fraction of ball configurations were computed similar to the line case.
Simulation Results: Fig. 2-(b) (main text): We have used network of size
N = 200, and m = 20 initial points on each manifold. Each point (marker) displayed is an average over 50 trials.
Numerical Results for High D L2 Balls. The test of the network capacity for
large dimensional balls, we performed simulations to evaluate the capacity for balls with 1 < D < 25 and R = 1, 5, 10 . Here the capacity was estimated using 20 trials. Good agreement was achieved with the theory, See Fig. 2.5.
56

Algorithm 1 Pseudocode for linear classification of L2 spheres.

Initialize:

xµ, u~µ  N orm(0, 1) ( µ = 1, ..., P ) [Sample centers and direction vectors]

yµ  sign {unif(-1, 1)} ( µ = 1, ..., P ) [Sample labels for manifolds]

ski  unif (-1, 1) and ||sk|| = 1 k = 1, ..., m. [Sample m coefficient vectors]

xµ + R

D i=1

ski uµi 

Ddata

[Construct

m

points

on

each

manifold]

t=0; wstvm = svmsolver(Ddata, Y ) [Check separability, find SVM solution]

t=0; htmin = argminµ,i,k||wstvm||-1/2yµwT xµ + R

D i=1

ski

uµi

[Get margin]

Repeat: while t < tmax

t=t+1

Repeat: for µ = 1 : P [For each manifold]

smi in

=

- hµi (wstvm) ||hµ(wstvm)||

[Coefficients

of

point

with

a

minimum

field]

dµmin = xµ + R

D i=1

smi inuµi

[Point

with

smallest

(signed)

distance

to

the

current margin plane]

If

yµ wstvm·dµmin |wst vm |

<

htmin

then

add

dµmin

to

Ddata

End

wstvm = svmsolver(Ddata, Y ) [Check separability, find new SVM solution]

htmin = argminµ,i,k||wstvm||-1/2yµwT xµ + R

D i=1

ski

uµi

[Get new margin]

End

Continue: until no more points are added

57

Figure 2.5: Capacity of high dimensional L2 balls.  = P/N at capacity with  = 0 as a function of D. (red) R = 1 (blue) R = 5 (green) R = 10. (markers) Simulation results. (dashed) full evaluation of B( = 0, R, D). Note that for R > 5, B-1( = 0, R 1, D)  D .
58

Linear Classification of L1 Balls Sampling of L1 Balls. Because the sides of L1 balls are straight lines, if all the
vertices are on the same side of the plane, all the points in the interior of L1 ball are on the same side of the plane as well. Therefore linear classifications of the entire L1ball is equivalent to linear classification of all the vertices. In Fig. 3 (main text), we consider D = 2, thus, we simulated SVM solutions of the 4P points, zµ = x0µ ±Ruµ1 ±Ruµ2 where each set of 4 points on the same manifold receive the same label. In the simulations shown in the figure, network size of N = 200 was used and the simulation was repeated 100 times to get the convergence rate (of 0.5 for estimating capacity). The fractions of manifold geometry configurations were computed similarly to the previous cases. Here, however, there are 4 configurations, corresponding to configurations with 0, 1, 2, or 4 number of vertices on the margin plane.
59

Chapter 3
The Maximum Margin Manifold
Machines
3.1 Introduction
Handling object variability is a major challenge for machine learning systems. For example, in visual recognition tasks, changes in pose, lighting, identity or background can result in large variability in the appearance of objects38. Techniques to deal with this variability has been the focus of much recent work, especially with convolutional neural networks consisting of many layers. The manifold hypothesis states that natural data variability can be modeled as lower-dimensional manifolds embedded in higher dimensional feature representations39. A deep neural network can then be understood as disentangling or flattening the data manifolds so that they can be more easily read out in the final layer40. Manifold representations of stimuli have also been utilized in neuroscience, where different brain areas are believed to untangle and reformat their representations 2,11,17,24,41.
This chapter addresses the problem of efficiently utilizing manifold structures to learn
60

classifiers. The manifold structures may be known from prior knowledge, or could be estimated from data using a variety of manifold learning algorithms42­47. Based upon knowledge of these structures, some areas of prior research have focused on building invariant representations48 or constructing invariant metrics49. On the other hand, most approaches today rely upon data augmentation by explicitly generating "virtual" examples from these manifolds50,51. Unfortunately, the number of samples needed to successfully learn the underlying manifolds may increase the original training set by more than a thousand-fold1.

y = +1

w~

y = +1

y = -1
y = -1
Figure 3.1: The maximum margin manifold binary classification problem. The optimal linear hyperplane is parameterized by the weight vector w which separates positively labeled manifolds from negatively labeled manifolds. Traditional data augmentation techniques would sample a large number of points from each manifold to train a conventional SVM.
We propose a new method, called the Maximum Margin Manifold Machine or M 4, that uses knowledge of the manifolds to efficiently learn a maximum margin classifier. Figure 3.1 illustrates the problem in its simplest form, binary classification of manifolds with a linear hyperplane. Given a number of manifolds embedded in a feature space, the M 4 algorithm learns a weight vector w that separates positively labeled manifolds from negatively labeled manifolds with the maximum margin. Although the manifolds consist of uncountable sets of points, the M 4 algorithm is able to find a good solution
61

in a provably finite number of iterations and training examples. Support vector machines (SVM) are widely used method to learn a maximum margin
classifier based upon a set of training examples 9. However, the standard SVM algorithm quickly becomes computationally intractable in time and memory as the number of training examples increases, rendering data augmentation methods impractical for SVMs. Methods to reduce the space complexity of SVM have been studied before, in the context of dealing with large-scale datasets. Chunking makes large problems solvable by breaking up the problem into subproblems52, but the resultant kernel matrix may still be very large. One method that subsamples the training data include the reduced SVM (RSVM), which utilize a random rectangular subset of the kernel matrix53. But this approach and other methods that attempt to reduce the number of training samples 52,54 may result in suboptimal solutions that do not generalize well.
Our M 4 algorithm directly handles the uncountable set of points in the manifolds by solving a quadratic semi-infinite programming problem (QSIP). M 4 is based upon a cutting-plane method which iteratively refines a finite set of training examples to solve the underlying QSIP55­57. The cutting-plane method was also previously shown to efficiently handle learning problems with a finite number of examples but an exponentially large number of constraints 58. We provide a novel analysis of the convergence of M 4 with both hard and soft margins. When the problem is realizable, the convergence bound explicitly depends upon the margin value whereas with a soft margin and slack variables, the bound depends linearly on the number of manifolds.
The chapter is organized as follows. We first consider the hard margin problem and analyze the simplest form of the M 4 algorithm. Next, we introduce slack variables in M 4, one for each manifold, and analyze its convergence with those additional auxiliary variables. We then demonstrate application of M 4 to both synthetic data where the manifold geometry is known as well as to actual object images undergoing a variety of warpings. We compare its performance, both in efficiency and generalization error,
62

with conventional SVMs using data augmentation techniques. Finally, we discuss some natural extensions and potential future work on M 4 and its applications.

3.2 Maximum Margin Manifold Machines with Hard
Margin
In this section, we first consider the problem of classifying a set of manifolds when they are linearly separable. This allows us to introduce the simplest version of the M 4 algorithm along with the appropriate definitions and QSIP formulation. We analyze the convergence of the simple algorithm and prove an upper bound on the number of errors the algorithm can make in this setting.

3.2.1 Hard Margin QSIP
Formally, we are given a set of P manifolds Mp  RN , p = 1, . . . , P with binary labels yp = ±1 (all points in the same manifold share the same label). Each Mp is defined by a parametrization x = Mp(s) where s  Sp, Sp is a compact subset of RD, Mp(s) : RD  RN is a continuous function of s  Sp so that the manifolds are bounded: s, Mp(s) < L by some L. We would like to solve the following semi-infinite quadratic programming problem for the weight vector w  RN :

SV

Msimple

:

argmin

1 2

w2

w

s.t. p, x  Mp : yp w, x  1

(3.1)

This

is

the

primal

formulation

of

the

problem,

where

maximizing

the

margin



=

1 ||w||

is

equivalent

to

minimizing

the

squared

norm

1 2

||w||2

.

We

denote

the

maximum

margin

attainable by , and the optimal solution as w. For simplicity, we do not explicitly

include the bias term here. A non-zero bias can be modeled by adding an additional

63

feature of constant value as a component to all the x. Note that the dual formulation of this QSIP is more complicated, involving optimization of non-negative measures over the manifolds. In order to solve the hard margin QSIP, we propose the following simple M 4 algorithm.

3.2.2 Ms4imple Algorithm
The Ms4imple algorithm is an iterative algorithm to find the optimal w in (3.1), based upon a cutting plane method for solving the QSIP. The general idea behind Ms4imple is to start with a finite number of training examples, find the maximum margin solution for that training set, augment the training set by looking for a point on the manifolds that most violates the constraints, and iterating this process until a tolerance criterion is reached.
At each stage k of the algorithm there is a finite set of training points and associated labels. The training set at the k-th iteration is denoted by the set: Tk =
xi  Mpi, ypi with i = 1, . . . , |Tk| examples. For the i-th pattern in Tk, pi is the index of the manifold, and ypi is its associated label.
On this set of examples, we solve the following finite quadratic programming problem:

SV

MTk

:

argmin

1 2

w2

w

s.t.xi  Tk : ypi w, xi  1

(3.2)

to obtain the optimal weights w(k) on the training set Tk. We then find a worst constraint-violating point xk+1  Mpk+1 from one of the manifolds such that

{pk+1, xk+1} : argminyp w(k), x < 1 - 
p, xMp

(3.3)

with a required tolerance  > 0. If there is no such point, the Ms4imple algorithm
64

terminates. If such a point exists, it is added to the training set, defining the new set
Tk+1 = Tk  xk+1, ypk+1 . The algorithm then proceeds at the next iteration to solve SV MTk+1to obtain w(k+1). For k = 1, the set T1 is initialized with at least one point from each manifold. The pseudocode for Ms4imple is shown in 2.

Algorithm 2 Pseudocode for the Ms4imple algorithm.

1. Input:  (tolerance), P manifolds and labels {Mp, yp = ±1}, p = 1, ..., P .

2. Initialize the iteration number k = 1, and the set T1 = {(xi  Mpi, ypi)} with at least one sample from each manifold Mp.

3. Solve for w(k) in SV MTk :

min

1 2

w 2 such that ypi w,xi

 1 for all

(xi, ypi)  Tk.

4. Find a worst point xk+1  Mpk+1 among the manifolds {pk+1 = 1, ..., P } with a margin smaller than 1 - :

xk+1 =

argmin

ypk+1 w(k), x < 1 - 

xMpk+1 ,pk+1=1,...P

5. If there is no such point, then stop. Else, augment the point set: Tk+1 = Tk  xk+1, ypk+1 .
6. k  k + 1 and go to 3.

In step 4 of the Ms4imple algorithm, a point among the manifolds needs to be found with the worst margin constraint violation. This is particularly convenient if the manifolds are given by analytic parametric forms, where this point could be computed analytically as for the case of manifolds with L2 balls or ellipses. However, for the algorithm to converge it is sufficient that a constraint violation point is found. Thus, local optimization procedures such as gradient descent may be used to search for such a point. However,
65

the speed of convergence in the latter stages of Ms4imple might be improved by a larger difference in the constraint violation of the point found in this step.

3.2.3 Convergence of Ms4imple
The Ms4imple algorithm will converge asymptotically to an optimal solution when it exists. Here we show that the w(k) asymptotically converges to an optimal w . Denote the change in the weight vector in the k-th iteration as w(k) = w(k+1) - w(k). First we have the following lemma:

Lemma 1. The change in the weights satisfies w(k), w(k)  0 .

Proof. Define w() = w(k) + w(k). Then for all 0    1, w() satisfies the constraints on the point set Tk: ypi w(), xi  1 for all (xi, ypi)  Tk. However, if w(k), w(k) < 0, there exists a 0 <  < 1 such that w( ) 2 < w(k) 2, contradicting the fact that w(k) is a solution to SV MTk .
Next, we show that the norm w(k) 2 must monotonically increase by a finite amount at each iteration.

Theorem 2. In the kth iteration of Ms4imple algorithm, the increase in the norm of w(k)

is lower bounded by

w(k+1) 2 

w(k)

2

+

k2 L2

,

where

k

=

1

-

ypk+1

w(k), xk+1

and

xk+1  L .

Proof. First, note that k >   0 , otherwise the algorithm stops. We have: w(k+1) 2 = w(k) + w(k) 2= w(k) 2 + w(k) 2 + 2 w(k), w(k)  w(k) 2 + w(k) 2 due
to Lemma 1. Now we consider the point added to set Tk+1 = Tk  xk+1, ypk+1 . At this point, we know ypk+1 w(k+1), -x k+1  1, ypk+1 w(k), -x k+1 = 1 - k, hence ypk+1 w(k), xk+1  k. Then, from the Cauchy-Schwartz inequality,

w(k)

2


k2 xk+1

2

>

k2 L2

>

2 L2

(3.4)

66

Since the optimal solution w

satisfies the constraints for Tk, we have

w(k)



1 

.

We thus have a sequence of iterations whose norms monotonically increase and are

upper

bounded

by

1 

.

Due to convexity, there is a single global optimum and the

Ms4imple algorithm is guaranteed to converge, asymptotically if the tolerance  = 0, and

in a finite number of steps if  > 0 .

As a corollary, we see that this procedure is guaranteed to find a realizable solution if it exists in a finite number of steps.

Corollary 3. The Ms4imple algorithm converges to a zero error classifier in less than

L2 ( )2

iterations,

where



is the optimal margin and L bounds the norm of the points on

the manifolds.

Proof. When there is an error, we have k > 1, and

w(k+1) 2 

w(k)

2

+

1 L2

(See

(3.4)).

This

implies

the

total

number

of

possible

errors

is

upper

bounded

by

L2 ( )2

.

With a finite tolerance  > 0, we obtain a bound on the number of iterations required

for convergence:

Corollary 4. The Ms4imple algorithm for a given tolerance  > 0 will terminate after

a

finite

number

of

iterations,

with

less

than

L2 ( )2

iterations

where



is the optimal

margin and L bounds the norm of the points on the manifolds.

Proof. Again,

wk 2 

w

2

=

1 ( )2

and

each

iteration

increases

the

squared

norm

by

at

least

2 L2

.

We can also bracket the error in the objective function after Ms4imple terminates:

Corollary 5. With tolerance  > 0, after Ms4imple terminates with solution wM4, the optimal value w of SV Msimple is bracketed by:

wM4 2  w

2



(1

1 - )2

wM4 2 .

(3.5)

67

Proof. The lower bound on w 2 is as before. Since Ms4imple has terminated, setting

w

=

1 (1-)

wM

4

would make w

feasible for SV Msimple, resulting in the upper bound on

w 2.

3.3 M 4 with Slack Variables

In many classification problems, the manifolds may not be linearly separable due to their dimensionality, size, and/or correlations. In these situations, Ms4imple will not even be able to find a feasible solution. To handle these problems, the classic approach is to introduce slack variables. Naively, we could introduce a slack variable for every point on the manifolds as below:

SV

Mnslaaicvke

:

argmin

1 2

w 2+C

w,p(x)

P p=1

xMp p(x)

s.t. p, x  Mp : yp w, x + p(x)  1,

p(x)  0

The parameter C represents the tradeoff between fitting the manifolds to obey the

margin constraints while allowing some set of points to be misclassified. This approach

cannot be used when training data consists of entire manifolds as in general, it would

require replacing the sum over a finite number of training points in the cost function,

to an integral with an appropriate measure over the manifolds. Thus, we formulate an

alternative version of the QSIP with slack variables below.

3.3.1 QSIP with Manifold Slacks
In this work, we propose using only one slack variable per manifold for classification problems with non-separable manifolds. This formulation demands that all the points on

68

each manifold x  Mp obey an inequality constraint with one manifold slack variable, yp w, x + p  1. As we see below, solving for this constraint is tractable and the algorithm has good convergence guarantees.
However, this single slack requirement for each manifold by itself may not be sufficient for good generalization performance. Indeed, our empirical studies show that generalization performance can be improved if we additionally demand that some representative points xp  Mp on each manifold also obey the margin constraint: yp w, xp  1. In this work, we implement this intuition by specifying appropriate center points xcp for each manifold Mp. This center point could be the center of mass of the manifold, a representative point, or an exemplar used to generate the manifolds 1. For simplicity, we demand that these points strictly obey the margin inequalities corresponding to their manifold label, but we could have alternatively introduced additional slack variables for these constraints. Formally, the QSIP optimization problem is summarized below, where the objective function is minimized over the weight vector w  RN and slack variables   RP :

SV Mmslaanckif old

:

argminF (w, ) =

1 2

w 2+C

P p=1

p

w,

s.t. p, x  Mp : yp w, x + p  1 (manifolds)

p : yp w, xcp  1 (centers)

p : p  0

3.3.2 Ms4lack Algorithm
With these definitions, we introduce our Ms4lack algorithm with slack variables below.

The proposed Ms4lack algorithm modifies the cutting plane approach to solve a semiinfinite, semi-definite quadratic program. Each iteration involves a finite set: Tk =
xi  Mpi, ypi with i = 1, . . . , |Tk| examples that is used to define the following soft

69

Algorithm 3 Pseudocode for the Ms4lack algorithm. 1. Input:  (tolerance), P manifolds and labels {Mp, yp = ±1}, and centers xcp

2. Initialize the iteration number k = 1, and the set T1 = {(xi  Mpi, ypi)} with at least one sample from each manifold Mp.

3.

Solve

for

w(k),(k):

min

1 2

w 2+C

P p=1

p

such

that

ypµ

w,xµ

+ pµ  1

for all xµ, ypµ  Tk and yp w,xcp  1 for all p.

4. Find a point xk+1  Mpk+1 among the manifolds {p = 1, ..., P } with slack violation larger than the tolerance :

ypk+1 w(k), xk+1 + pk+1 < 1 - 

5. If there is no such point, then stop. Else, augment the point set: Tk+1 = Tk  xk+1, ypk+1 .
6. k  k + 1 and go to 3.

margin SVM:

SV

MTsklack

:

argmin

1 2

w 2+C

P p=1

p

w,

s.t.  xi, ypi  Tk : ypi w, xi + pi  1

p : yp w, xcp  1 (centers)

p : p  0

to obtain the weights w(k) and slacks (k) at each iteration. We then find a point xk+1  Mpk+1 from one of the manifolds so that:

ypk+1 w(k), xk+1 + p(kk+) 1 = 1 - k

(3.6)

70

where k > . If there is no such a point, the Ms4lack algorithm terminates. Otherwise, the point xk+1 is added as a training example to the set Tk+1 = Tk  xk+1, ypk+1 and the algorithm proceeds to solve SV MTskla+c1k to obtain w(k+1) and (k+1). Note that Ms4lack embodies the fact that for the algorithm to converge, it is not necessary to find the point with the worst constraint violation at each iteration.

3.3.3 Convergence of Ms4lack

Here we show that the objective function F

w, 

=

1 2

w 2+C

P p=1

p

is

guaranteed

to increase by a finite amount with each iteration. This result is similar to59, but here

we demonstrate a proof in the primal domain over an infinite number of examples. We

first have the following lemmas,

Lemma 6. The change in the weights and slacks satisfy:

w(k), w(k) + C p(k)  0
p

(3.7)

where w(k) = w(k+1) - w(k) and (k) = (k+1) - (k).
Proof. Define w() = w(k) + w(k) and () = (k) + (k). Then for all 0    1, w() and () satisfy the constraints for SV MTsklack. The resulting change in the objective function is given by:

F w(), () - F w(k), (k) =

 w(k), w(k) + C

p(k)

+ 1 2 w(k) 2 2

(3.8)

p

If (3.7) is not satisfied, then there is some 0 <  < 1 such that F w( ), ( ) < F w(k), (k) , which contradicts the fact that w(k) and (k) are a solution to SV MTk .
71

Lemma 7. In each iteration of Ms4lack algorithm, the added point xk+1, ypk+1 must be a support vector for the new weights and slacks, s.t.

ypk+1 w(k+1), xk+1 + p(kk++11) = 1

(3.9)

and

ypk+1 w(k), xk+1 + p(kk+) 1 = k

(3.10)

Proof. Suppose ypk+1 w(k+1), xk+1 + p(kk++11) = 1 + for some > 0. Then we can

choose 

=

k k +

< 1 so that w( ) = w(k) +  w(k) and ( ) = (k) +  (k)

satisfy the constraints for SV MTskla+c1k. But, from Lemma 6, we have F w( ), ( ) < F w(k+1), (k+1) which contradicts the fact that w(k+1) and (k+1) are a solution to

SV MTk+1. Thus, = 0 and the point xk+1, ypk+1 must be a support vector for SV MTk+1. (3.10) results from subtracting (3.6) from (3.9).

We also derive a bound on the following quadratic function over nonnegative values:

Lemma 8. Given K > 0, > 0, then x  0

1 (x - )2 + Kx  min 1 2, 1 K

2

22

(3.11)

Proof. The minimum value occurs when x = [ - K]+. When K  , then x = 0 and

the

minimum

is

1 2

2.

When

K

<

,

the

minimum

occurs

at

K



-

1 2

K



1 2

K

.

Thus,

the lower bound is the smaller of these two values.

Theorem 9. In each iteration k of Ms4lack algorithm, the increase in the objective

72

function for SV Mmslaanckifold is lower bounded by

F (k)  min

1 8

k2 L2

,

1 2

C

k

(3.12)

where F (k) = F w(k+1), (k+1) - F w(k), (k) .

Proof. (Sketch) First, if w(k) = 0, Lemma 6 completes the proof. If w(k) = 0, then

p(kk) = k from Lemma 7 and p(k=)pk = 0 since (k) is the solution for SV MTk . So for

w(k) = 0, F (k) = Ck.

The added point xk+1 is from a particular manifold Mpk+1. If p(kk+) 1  0, we have

ypk+1

w(k), xk+1

 k ( Lemma 7). Then,

w(k)

2



k2 L2

,

yielding

F (k)



1 2

k2 L2

.

We next analyze p(kk+) 1 > 0 and consider the finite set of points (x, pk+1)  Tk,

coming from the pk+1 manifold. Each of these points obeys the constraints:

ypk+1

w(k), x

+ p(kk+) 1 = 1 +

(k) 

ypk+1

w(k+1), x

+ p(kk++11) = 1 +

(k+1) 

(k),

(k+1) 



0

(3.13) (3.14) (3.15)

We consider the minimum value of the thresholds:  = min (k).

If  > 0, none of the x points are support vectors for SV MTsklack. In this case, we define a linear set of slack variables: p() = p(kk) for p = pk, and p() = p(k) + p(k)
for p = pk, and w() = w(k) + w(k), which satisfy the constraints for SV MTk . Then,

this implies

w(k), w(k) + C

p(k)  0

p=pk

(3.16)

73

which implies F (k)  min

1 2L2

k2,

1 2

C

k

.

If  = 0, at least one support vector lies in Mpk+1.

Consider



=

min

(k) 

=0

(k+1) 



0.

We then define p() = p(kk) +  p(k) -  for p = pk, and p() = p(k) + p(k) for p = pk, and w() = w(k) + w(k). Then, there exists 0    min for which () and w() satisfy the constraints for SV MTk , so that

w(k), w(k) + C p(k)  C
p

(3.17)

We also have a support vector (x , pk+1)  Tk, with ypk w(k), x + p(kk) = ,

then

w(k)

2



1 4L2

(k

- )2

by

using

Lemma

7.

Then, by using Lemma (8) on , we get

F (k)  min

1 8L2

k2

,

1 2

C

k

Thus, we obtain the final bound combining results from two cases of .

(3.18)

Since the Ms4lack algorithm is guaranteed to increase the objective by a finite amount, it will terminate in a finite number of iterations if we require k >  for some positive  > 0.

Corollary 10. The Ms4lack algorithm for a given  > 0 will terminate after at most

P · max

8C L2 2

,

2 

iterations where P is the number of manifolds, L bounds the norm

of the points on the manifolds.

Proof. w = 0 and p = 1 is a feasible solution for SV Mmslaanckifold. Therefore, the optimal objective function is upper-bounded by F w = 0,  = 1 = P C. The upper bound on the number of iterations is then provided by Theorem (9).

We can also bound the error in the objective function after Ms4lack terminates:

74

Corollary 11. With  > 0, after Ms4lack terminates with solution wM4, slack M4 and value FM4 = F wM4, M4 , then the optimal value F of SV Mmslaanckifold is bracketed by:

FM4  F  FM4 + P C.

(3.19)

Proof. The lower bound on F is apparent since SV Mmslaanckifold includes SV MTsklack constraints for all k. Setting the slacks p = M4,p +  will make the solution feasible for SV Mmslaanckifold resulting in the upper bound.

3.4 Experiments

3.4.1 Synthetic Manifolds

Random L2 balls As an illustration of our method, we have generated manifolds

consisting of random D-dimensional Euclidean balls with a given radius. Each manifold

Mp is described by a center vector xcp  RN and D basis vectors upi  RN , i = 1, ..., D .

The points on the manifold can be parameterized as Mp = x x = xcp + R

D i=1

siupi

where R is the radius of the ball and s  RD are normalized so that

D i=1

s2i

=

1.

Simulations We compare the performance of M 4 to the conventional point SVM

with samples uniformly drawn from the L2 ball manifolds. Performance is measured by

generalization error as a function of the number of samples used by the algorithm.

For these manifolds, the worst constraint-violating point can easily be computed

by taking the derivative of the constraint yp w · xp0 + R

D i=1

siupi

+ p  1 with

respect to s for all p. This results in the analytic solution spi ,worst = -

ypw·upi

D i=1

w·upi

2

. For problems with non-separable manifolds in Ms4lack, we used an additional single

margin constraint per manifold given by the center xcp.

75

We used the following parameters in the simulations shown below: embedding dimension N = 500, manifold dimension D = 10, radius R = 20. With these parameters, the critical manifold capacity for linear classification is estimated to be Pcritical = 48.360, hence we consider P = 46 to test Ms4imple and P = 50 for the Ms4lack simulations.
The results are presented in figure 3.2 for the separable case and non-separable case.
Figure 3.2: Generalization error of the M 4 solution for L2 ball manifolds, shown as a function of the total number of training samples per manifold (red solid) compared with that of conventional point SVM (blue dashed). N = 500, D = 10, R = 20, and (a) P = 46 is used for Ms4impleand (b) P = 48 for Ms4lack with C = 100. (a)-(Inset) k is shown for the kth added point in Ms4imple. The critical capacity with these parameters is Pc  48.
3.4.2 ImageNet Dataset
Image-based Object Manifolds We also apply the M 4 algorithm to a more realistic
class of object manifolds. Here each object manifold is defined by the infinite set of images created by applying 2-D affine transformations on a single template image. In order to create object manifolds, P template images were sampled from the ImageNet 2012 data set for which exact object bounding boxes are available10, and each image
76

was cropped and scaled such that the object occupies the middle 48 × 48 pixels of the template image.
Each sample from the object manifold is a 64 × 64 gray-scale image created by applying a 2-D affine transformation on the template image. Those transformations are defined as a composition of seven basic transformations: horizontal or vertical translation, horizontal or vertical scaling, horizontal or vertical shear, and rotation. The range of each basic transformations was chosen so that the largest pixel displacement was equivalent to 8 pixels. The composition of these seven basic transformations thus defines a 7-D highly non-linear object manifold for each object.
Simulations We compared the performance of M 4 to point SVM in classification of
samples from the object manifolds. Performance is measured as above by generalization error as a function of the number of samples each algorithm uses. We used object manifolds with up to M = 10000 samples drawn from each manifold, using 80% of the samples as training set and 20% as a testing set. Rather than performing classification directly on image pixels the samples were projected to the space defined by their N = 500 largest principal components. For this data set the classification problem is separable for P = 4 and non-separable for P = 6.
Point SVM (defined as SV Mnslaaicvke above) was trained with varying numbers of training set samples, with C obtained through cross validation. The training was repeated 11 times with different samples to estimate the variability of the generalization error. M 4 was trained with a constraint per manifold given by the center of mass at the training set
and C obtained through cross-validation. At each iteration of the algorithm, the worst-violating constraint point was found using local search. Initialized with a random sample from each manifold, it was compared to a set of K neighboring samples in the space of potential transformation (K = 5 was used throughout). This process is iterated
77

until a set of local minima were found, and these points were candidates to be added to the active set of the M 4 algorithm.
Figure 3.3-b compares the generalization error for a separable classification problem (at P = 4) while Figure 3.3-c compare those for a non-separable classification problem (at P = 6). Those representative results illustrate that in both cases M 4 achieve a very low generalization error (compared to point SVM) already at very small number of samples.
Figure 3.3: Image-based object manifolds. (a) Basic affine transformation: a template image (middle) surrounded with changes along 4 axes defined by basic affine transformation. A 48 × 48 square marking the object bounding box was added to the template image for illustration purposes. (b-c) Generalization error of the M 4 solution for 7-D image-based object manifolds, shown as a function of the number of training samples per manifold (solid line) compared with that of conventional point SVM (blue squares). At N = 300 the problem is separable for P = 4 (b) and non-separable for P = 6 (c).
3.5 Discussion
We described and analyzed a novel algorithm for finding the maximum margin solution for classifying manifolds. The algorithm, called M 4, is based upon a cutting-plane method and iterates between adding the worst violating point to a finite training set,
78

and updating a maximum margin solution. There are two versions of the algorithm, one without slack variable appropriate for separable manifolds, and a slack version for non-separable manifolds. We proved the convergence of M 4, and provided bounds on the number of iterations required and the deviation from the optimal objective function. On experiments with both synthetic manifolds and with actual image manifolds, our empirical results demonstrate the efficiency of M 4 and the superior performance in terms of generalization error, compared to conventional SVM's, using data augmentation techniques with many virtual examples. Ongoing work includes theoretical research to understand how M 4 explicitly scales with the number of manifolds and the embedding dimensionality.
There is natural extension of M 4 to nonlinear classifiers via the kernel trick, as all our operations involve dot products between the weight vector w and manifold points Mp(s). At each iteration, the algorithm would solve the dual version of the SV MTk problem which is readily kernelized. In addition, the M 4 algorithm relies upon finding a point on a manifold with sufficiently strong violation of the constraints. Since a local minimization of the constraint violation at each stage is sufficient in the relaxed version of the algorithm, we expect that this step of M 4 will be practical for simpler kernel functions. However, we note that with infinite-dimensional kernels such as RBF's, the full manifold optimization problem becomes a fully infinite quadratic programming problem, no longer a QSIP which requires further theoretical work to establish the existence and properties of optimal solutions.
Beyond binary classification, variations of M 4 can also be used to solve other machine learning problems including multi-class classification, ranking, one-class learning, etc. In this work, we have shown how M 4 can be used to classify image manifolds at pixel input representations. We can also use this algorithm to evaluate the computational benefits of manifold representations at successive layers of deep networks in both machine learning and in brain sensory hierarchies. We also anticipate using M 4 to build novel hierarchical
79

architectures that can incrementally reformat the manifold representations through the layers for better overall performance in machine learning tasks.
We anticipate this work will make an important contribution to the understanding of how neural architectures can learn to process high dimensional real-world signal ensembles and cope with large variability due to continuous modulation of the underlying physical parameters.
80

Chapter 4
Linear Classification of General
Manifolds
4.1 Introduction
In chapter 2, we applied methods from statistical mechanics of spin glasses to solve the problem of linear, max margin, classification of manifolds with simple geometries such as lines, as well as L2and Lp balls embedded in embedded in a linear subspace with dimensions D, where D is much smaller than the ambient dimension N . In chapter 3, we presented a new efficient algorithm for finding max margin linear classifier of manifolds.
In this chapter, we return to the theory and consider the problem of linear classification of general manifolds, again with embedding dimension much smaller than N . To set the stage of more complex geometries, we begin by considering the classification of D-dimensional L2 ellipsoids. The results from the analysis of ellipsoids are readily extended to the case general smooth convex manifolds. We then move to consider classification of non-smooth low dimensional manifolds, which exhibit a more complex
81

solution structure. characterized by variety of 'support' structures. Nevertheless, we derive a set of mean field equations that apply to general low dimensional smooth as well as non-smooth manifolds, including also manifolds consisting of finite number of points (point clouds). We identify key geometric descriptors of the manifolds: the effective manifold dimension DM and the effective manifold radius RM , two geometric 'order parameters' which determine the capacity of linear classification of general manifolds (when their dimensionality is high), and provide an iterative algorithm that can efficiently solve for DM and RM , as well as general manifold capacity M . Finally, we emphasize that although the data manifolds may not be convex, any hyperplane that separates them must also separate their convex hull. Hence, all geometric properties discussed in this chapter refer to convex manifolds.
Note that in general, the capacity of manifolds embedded in D dimension can be upper and lower bounded by

2 1 + 2D

<

c

<

2

(4.1)

This is because in the limit where extents of a manifold in all D embedding dimensions go to zero, a manifold becomes a point, whose perceptron capacity is 0 = 27. In the limit where extents of a manifold in all D embedding dimensions go to infinity,

then the linear classifier w has be in the subspace orthogonal to all D directions of the P manifolds60. Since P of the D-dimensional manifolds occupy P D dimension, the

classification becomes point classification in N - P D dimension, resulting in the max-

imum number of manifolds linear separable P = 2(N - P D), resulting in the capacity

P /N

=

2 1+2D

.

These asymptotic bounds of a manifold capacity apply for arbitrary

manifold shapes. Now let us focus on simplest extension of classification of L2 balls,

classification of D -dimensional ellipsoids.

82

Figure 4.1: Linear Classification of Ellipsoids (Illustration). In N -dimensional ambient space, the solution hyperplane (blue plane) has to separate between red D-dimensional ellipsoids and blue ellipsoids with margin . Margin  is the distance between the closest point on the ellipsoids and the solution hyperplane. We refer to all points of such distance to the solution hyperplane as "margin planes" (grey planes). Different patterns are used to denote different support configuration of manifolds. Solid pattern: ellipsoids embedded in the margin plane, diamond pattern: ellipsoids touching the margin plane with one point, striped pattern: interior ellipsoids (ellipsoids that are in the interior space shattered by margin planes).
4.2 L2 Ellipsoids
4.2.1 Model
Consider the problem of linearly classifying D-dimensional ellipsoids (Figure. 4.1) in N -dimensional ambient space, where each point within the µth ellipsoid is expressed as

D
xµ0 + siuµi , yµ = ±1
i=1

(4.2)

For each µ, xµ0  RN is N-dim vector representing the center of the ellipsoid, the set of D N-dim vectors, uµi  RN , for i = 1, ..., D , are the ellipsoid's basis vectors. The

83

vectors s  RD parametrize the points on the manifolds and obey the constraint

where,

f (s)  0

(4.3)

D
f (s) = s2i Ri-2 - 1
i=1

(4.4)

Ri represent the ellipsoid's radii in the i th direction, relative to the center norm. In
order to evaluate the ability of the perceptron to classify the ellipsoids, we need to specify their statistical properties. Here we assume that each component of xµ0 , uµi are independent Gaussian random variables with unit variance. With these assumptions,
 and assuming large N , the norm of the centers is (approximately) N and the ui's are
 (approximately) orthogonal vectors with norms N .
We assume the ellipsoids are assigned binary labels (which are therefore the same for
all points on the ellipsoid) denoted as yµ = ±1 . We search of a set of weight vectors w  RN that obey the following inequalities,

yµwT

D
x0µ + siuµi
i=1

 w

s, f (s)  0

(4.5)

The maximum  that admits a solution w will be called the margin of the system. Here we assume the labels for the manifolds are assigned randomly i.i.d with probability half for yµ = ±1 . The case where the fraction of positive and negative labels are not equal (sparse labels) will be covered in Chapter 5.

84

4.2.2 Fields of the Closest Point
To classify all the points on the ellipsoids correctly, a necessary and sufficient condition is that the weight vector w, satisfies the constraints on the 'worst' points on each ellipsoid , namely the ones which are closest to the separating plane defined by w. To find this point for the µth manifold we define the fields hµ0 = w -1 yµw · xµ0, which are the field (the protection on w) induced by the manifold's center, and hµi = w -1 yµw · uµi i = 1, ..., D , which are the D fields induced by the basis vectors of the manifold. In terms of these fields, 4.5 can be written as

where

hµ0 + (hµ)  0

(4.6)

where

(h) = s~(h) · h - 

(4.7)

s~(h) = arg min {s · h}
s, f (s)=0

(4.8)

gives the point on the manifold which has the smallest (signed) projection on the hy-

perplane w. In order to evaluate s~(h), we differentiate

D i=1

sihi

+

f (s)

with

respect

to si, where  is a Lagrange multiplier enforcing the manifold constraint, yielding in

the case of ellipsoids,

s~i = -

hiRi2

= - hiRi2

j h2j Rj2

hR

(4.9)

where  denotes element-wise product, and ||h  R|| is the L2 norm of the D-dimensional vector whose components are {hiRi}. To evaluate , we note that s~ · h = - h  R ,

85

hence,

(h) = - h  R - 

(4.10)

4.2.3 Mean Field Theory
We consider a thermodynamic limit where N, P   whereas  = P/N , D, and R are finite. Following Gardner's framework, we compute the average of lnV , where V is the volume of the space of the solutions, which in our case, can be written as:

V = dN w(w2 - N )Pµ=1µ(hµ0 + (hµ))

(4.11)

where  is the Heaviside function. We use replica theory, ln V

= limn0

V

n
n

-1

,

where

refers to the average over the 'quenched random variables: the input parameters,

x0µ and uµ and the labels, to evaluate ln V via the replica symmetric saddle point

equations. The saddle point approximation is exact in the thermodynamic limit and

the replica symmetric ansatz holds for convex problems such as ours. These equations

are expressed in terms

of the order parameter, q

=

1 N

w · w,  = ,

where w

and

w are two typical solutions of the classification problem.

The 'free energy' G associated with V n is given by,

where,

V n x0,u,y  eN n[G(q)] = eN n[G0(q)+G1(q)]

(4.12)

G0(q)

=

1 2

ln(1

-

q)

+

q 2(1 -

q)

(4.13)

is the entropic term representing the volume of w subject to the constraint that q =

1 N

w

·

w

.

G1(q) embodies the constraints imposed by the classification task and is

86

expressed in terms of the fields hµ0 and hµ. In the considered limit, these fields can be written as sums of two random fields, where t0 and t are the quenched component resulting from the quenched random variables, namely the input vectors xµ0 and uµi , while the z0and z are the fields representing the variability of different w's within the
volume of solutions for each realization of inputs and labels:

and,

hµ0 = qtµ0 + 1 - qz0µ, hµ = qtµ + 1 - qzµ

(4.14)

G1(q) = lnZ(q, t0,t) t0,t

(4.15)

where the average wrt t0, t denotes integrals over the gaussian variables t0, t with measures Dt0 and Dt = iDti, respectively, and

Z(q, t0, t) =


Dz0


Dz

qt0 +

1 - qz0 +  qt +

1 - qz

-

-

(4.16)

Finally, q is determined by solving

G q

=0.

Solution with q < 1 indicates a finite

volume of solutions. For each  there is a maximum value of  where a solution exists.

As  approaches this maximal value, q  1 indicating the existence of a unique solution,

which is the max margin solution for this . We focus on the properties of the max

margin solution, i.e., on the limit q  1.

4.2.4 The Capacity Limit
We define

Q

=

q 1-q

87

(4.17)

and

study

the

limit

of

Q

 .

In

this

limit,

the

leading

order

for

G0

term

is

G0

=

Q 2

and G1 can be evaluated by a saddle point approximation of the z0and z integrals,

ln

Z (t0 ,

t)

=

-

z0

 ,z, Qt0

+zm0+in(Qt+z)>0

1 2

z02 +

z2

(4.18)





Scaling the variables z0 and z such that z0  Qz0 and z  Qz and using the fact

that

(h)

is

linear

in

the

magnitude

of

h

to

write

lnZ (t0 ,

t)

=

-

Q 2

F

(t0

,

t)

,

F (t0,t) =

min

z02 + z 2

z0,z,t0+z0+(t+z)>0

where (h) = - h  R -  (Eqn. 4.10). Finally,

(4.19)

ln V

Q =
2

1 -  F (t0, t) t0,t

so the capacity, defined by vanishing ln V is given by,

(4.20)

E-1() = F (t0, t) t0,t

(4.21)

where the subscript E stands for ellipsoids. For each t the nature of the solution to the min operation in 4.21 depends on t0 yielding three regimes of t0 with qualitatively different contributions to the capacity, as described below.

Regime 1 (Interior Manifolds): t0 -  > T
where,

T =tR In this case, the solution is z0 = z = 0 and does not contribute to Eq. 4.21.

(4.22)

88

For values of t0 -   T , the solution obeys

t0 + z0 +  t + z = 0

(4.23)

meaning that the closest point is on the margin plane. This regimes is divided into two cases:

Regime 2 (Touching Manifolds): tC < t0 -  < T
where,

tC = -

Ri-2t2i
i

(4.24)

Here, t0 + z0 +  t + z = 0 but h0 = t0 + z0 > , implying that the ellipsoid center is an interior point; in other words, the ellipsoid touches the margin plane only at a single point. Thus, for a given t0 and t we need to solve

min
z

z02 +

z2

(4.25)

where z0 = -t0 -  t + z . Differentiating with respect to z yields, z = z0z = z0h{s · h}, namely,

z = z0s

(4.26)

where from now on, unless otherwise specified, swill be a shorthand of s~(h) = s~(t + z). Note that this is a self consistent equation for s due to 4.26. This yields also,  = s · (t + z0s) - , hence

z0

=

(

- t0 - t · (1 + s2)

s)

(4.27)

89

Finally, z02 + z2 = z02(1 + s2) yielding,

F (t0,t)

=

(

-t·s- 1 + s2

t0)2

(4.28)

To conclude the evaluation of F we need to calculate s. Eq, 4.9, for the ellipsoid, yields,

si = - HiRi H

(4.29)

with H  h  R = (t + z0s)  R. Substituting in the above equation, one obtains,

si

=

-

Ri2ti

||H|| + z0Ri2

(4.30)

which yields an equation of s(t) in terms of ||H|| and z0. These two scalars are related

through

z0 = -t0 -  t + z = - - t0 + ||H||

(4.31)

where ||H|| = s · (t + z0s) and  is given by Eqn. 4.10. Finally, an equation for z0 can be derived from the ellipsoid constraint f (s) = 0,

1 = s2i Ri-2
i

(4.32)

To summarize, Eqns. 4.30 -4.32 yields s(t, t0) which we use to evaluate F , Eq. 4.28.

Regime 3 (Embedded Manifolds): t0 -  < tC
Here h = t + z = 0, and h0 = t0 + z0 = , implying that the center as well as the entire manifold is on the margin plane, hence

F (t0, t) = (t0 - )2 + t 2
90

(4.33)

Finally, combining contributions from regimes 2 and 3, the expression of the capacity is

E-1() =

Dt

+|T |
Dt0
+tC (t)

( - t · s - t0)2 1 + s2

+

Dt

+tC (t)
Dt0

(t0 - )2 +

t2

-

(4.34)

|T | =

Ri2t2i
i

(4.35)

tC = - In the first integral, s is given by,

Ri-2t2i
i

(4.36)

si

=

-

+

t0

Ri2ti + z0(1

+

Ri2)

and z0(t, t0) is evaluated by solving,

(4.37)

1=

i

Ri2t2i ( + t0 + z0(1 + Ri2))2

(4.38)

4.2.5 The Large D limit

If the size of the ellipsoid is not small, we expect the capacity to be small (of order 1/D, see Eqn. 4.47). On the other hand, when the radii are small the capacity should be order 1 as in the case of points. We inquire how small Ri's should be in order to yield a finite capacity even when D is large. The answer is provided by a scaling analysis, below.

91

Large D , Ri = O(1)
In this limit, ||T ||, -tC = O(D1/2), so integral bounds in the first term of 4.34 can be taken to ±. From 4.32, si = O(D-1/2) and from Eqns 4.31, z0  ||H|| = O(D1/2) .

and from the normalization,

si

=

-

z0

Ri2ti (1 + Ri2)

(4.39)

z02 

z02

=

D i=1

Ri2t2i (1 + Ri2)2



D i=1

Ri2 (1 + Ri2)2

where we have replaced t2i  1 under the summation. Similarly,

s2 

s2

=

1 z02

D i=1

Ri4 (1 + Ri2)2

=

O(1)

(4.40) (4.41)

Hence,

t·s

t·s

=

1 - z0

D i=1

Ri2 (1 + Ri2)

=

O(D1/2)

(4.42)

E-1



t· 1+

s2 s2

when D

1, Ri = O(1)

(4.43)

which is of order D as expected.

Effective Dimensionality and Radius: These results suggest to express the capacity by introducing the ellipsoid effective dimension (DE) and radius (RE), as follows,

E-1

=

RE2 DE 1 + RE2

when D

1, Ri = O(1)

(4.44)

92

RE2 = s2 =

i

(1

Ri4 + Ri2)2

/

j

Rj2 (1 + Rj2)2

when D

1, Ri = O(1)

(4.45)

DE =

i

Ri2 1 + Ri2

2
/
i

Ri4 (1 + Ri2)2

when D

1, Ri = O(1)

(4.46)

Thus, the capacity of ellipsoids in large D is equivalent to that of L2balls with radii

REand dimensionality DE.

Large D, Large R Regime
Finally, when most of the Ri are large, RE 1 and

E-1 = DE = D when D 1, Ri 1 In this case, w is orthogonal to the basis vectors with large Ri.

(4.47)

Scaling Regime: Large D, Ri  D-1/2
The above results suggest that when the radii are small such that, RE  DE-1/2 the capacity becomes order 1. Thus, the scaling relation Ri  D-1/2 denotes the regime of finite capacity, namely the balance between large dimension and small size that maintains a finite capacity. This regime requires its own analysis of the various terms that contributes to the capacity. First,

||T ||  ||R|| = O(1)

(4.48)

-tC = O(D1/2)

(4.49)

93

So the integral bounds in the first term of 4.34 is from - to  + ||R|| and the second

term is negligible. From 4.32, si = O(D-1), and ||H|| =

i h2i Ri2 = O(1) and from

Eqns 4.31, z0   - t0 + ||H|| = O(1). Hence, z0Ri2 = O(D-1). Then, from 4.30,

si



- Ri2ti ||H ||

=

O(D-1)

as expected.

And from this normalization,

(4.50)

s2  s2 = 1 ||H ||2

i

Ri4t2i

=

1 ||H ||2

i

Ri4 = O(1)O(D  D-2) = O(D-1)

(4.51)

t·s t·s =-

Ri2

||R||2

=-

= 0(1)

i ||H||

||H ||

and from normalization,

(4.52)

1=

i

s2i Ri-2



1 ||H ||2

Ri2
i

implying ||H|| = ||R|| and t · s = -||R|| . Hence,

(4.53)

E-1 =

+||R|| -

Dt0(

+

||R||

-

t0

)2

1 + s2

(4.54)

Although s2 = ||R  R||2/||R||2 is a correction of order D-1 , we will keep it because

it turns out to be important to keep in simulations.

We can express these results in terms of the effective dimensionality and radius introduced above. In the limit of small Rs these quantities reduce to,

RE2 =

i Ri4 i Ri2

=

O(D-1)

when D

1, Ri O D-1/2

(4.55)

94

and,

DE = (

i Ri2)2 i Ri4

=

O(D)

when D

1, Ri O D-1/2

(4.56)

||R||2 = RE2 DE

(4.57)

Using these quantities, and the formula for the point capacity, 0-1() =

 -

Dt(t

-

)2, we can write,

Capacity for Ellipsoids:

E() = (1 + RE2 )0( + RE DE) when D 1, Ri O D-1/2

(4.58)

with RE and DE are defined by Eqns 4.55-4.56 when D 1, Ri D-1/2, and





RE DE behaves like an additional margin E = RE DE introduced by the ellipsoid

structure.

Interestingly, in the scaling regime, the effective dimension for the ellipsoids is equiv-

alent to another measure of dimension, Dsvd, called the participation ratio61­63, defined

by

Dsvd =

D i=1

2i

2

D i=1

4i

(4.59)

where i is an eigenvalue of single value decomposition (not normalized), whereas forDE,

Ri is a radius in i th dimension of an ellipsoid. Ri and i are closely related, as both

definitions are measures for how extended data are in the ith dimension. Particularly

when siRi-1 are uniformly sampled from a sphere, then Ri and i are proportional to

each other (Lemma 12 in Appendix to the chapter) .

95

Combined Expression for the Capacity in Large D
Finally, we note that we can combine the results for all the above regimes can be expressed by a single set of equations. For large D,

E() = (1 + RE2 )0( + E)

(4.60)

E = RE DE

(4.61)

RE2 = s2 =

i

(1

Ri4 + Ri2)2

/

j

Rj2 (1 + Rj2)2

(4.62)

DE =

i

Ri2 1 + Ri2

2
/
i

Ri4 (1 + Ri2)2

(4.63)

Finally, we note that the definition of the ellipsoid dimension above, DE, is not

invariant to a global scale of all radii, except in the regime of small Ri . The reason is

that the separation of the manifolds depend not only on their intrinsic geometry but

also on their distance from the common origin. Thus both dimensionality and radii take

into account the center norm. Indeed, the size scale 1 appearing in the definition of DE

above is the scale of the center norm. This is reflected in the numerical evaluation of

DE in Figure 4.5 below.

96

4.2.6 Support Manifold Structures
It is instructive to consider the types of manifold support structures that arise. In general, the fraction of touching ellipsoids is

pEtouching =

+||T ||

Dt

Dt0

+tC (t)

The fraction of embedded ellipsoids is

(4.64)

pEembedded = The fraction of interior ellipsoids is

+tC (t)

Dt

Dt0

-

(4.65)

pEinterior =



Dt

Dt0

+||T ||

(4.66)

Large D Limit

Here we consider the limit of large D, and analyze the behavior of support structures in different regimes of Ri.

Large D , Ri = O(1) In this limit ||T ||, -tC = O(D1/2), so that

pEtouching =

+||T ||

Dt

Dt0 =

+tC (t)



Dt

Dt0 = 1

-

, and

pEembedded =

+tC (t)

Dt

Dt0 =

-

-

Dt

Dt0 = 0

-

and

pEinterior =



Dt

Dt0 = 0

+||T ||

implying that all of the manifolds are touching the hyperplane in this regime.

(4.67) (4.68) (4.69)

97

Large D , Ri  D-1: Scaling Regime In this limit, ||T ||  ||R|| = O(1) and
-tC = O(D1/2). Therefore,

pEtouching =

+||T ||

Dt

Dt0 =

+tC (t)

+||T ||

Dt

Dt0 = 1 - H( + |t  R|}) (4.70)

-

t

pEembedded =

+tC (t)

Dt

Dt0 =

-

-

Dt

Dt0 = 0

-

and

(4.71)

pEinterior = 1 - pEembedded - pEtouching =

H( + |t  R|})
t

(4.72)

implying that there is no manifold embedded, but most of the manifolds are either

touching the margin plane or in the interior space.

4.2.7 Remarks
It is notable that the capacity of ellipsoids in the high D limit (Eqn. 4.44) resembles that of L2 balls (Eqn. 2.75), with an effective dimension DE and radius RE. The support structures of the ellipsoids also behave similarly to the spherical L2 balls in Chapter 2, exhibiting three regimes of support structures (embedded, touching and interior) and in high D, non of the manifolds are embedded in the margin plane, and the fraction
 of touching manifolds increase like 1 - H( + R D). In the next section, this analogy extends to more general case of arbitrary smooth convex manifolds, and we show the replica treatment of the smooth convex manifolds.

98

4.3 General Smooth Convex Manifolds

4.3.1 Model

We now consider the problem of linear binary classification of points on convex smooth manifolds. We define a smooth convex manifold as a compact convex manifold in Euclidean space with convex and twice differentiable bounding curve. It is useful to parametrize such a manifold as the set of all points, x in RN , of the form

D
x0 + siui
i=1

(4.73)

where x0 and ui are (linearly independent) vectors in RN and the D-dimensional vector s obeys the constraint f (s)  0, where f : RD  R is a twice differentiable convex

function. We will refer to x0 as the center of the manifold and to ui as its D axes.

Examples of smooth and non-smooth convex manifolds are provided (Smooth: Figures

4.1-4.2, Non-smooth: Figure 4.4). Our data consists of P such manifolds and their

target binary labels denoted as yµ, µ = 1, ..., P . We search of a set of weights w  RN

that obey the following inequalities,

yµwT

D
x0µ + siuµi
i=1

 w

s, f (s)  0

(4.74)

In order to evaluate the ability of the perceptron to classify the manifolds, we need to specify their statistical properties. As before, we assume that each component of xµ0 , uµi
are independent Gaussian random variables with unit variance. With these assumptions, 
and assuming large N , the centers have norm N and the ui's are orthogonal vectors 
with norms N .
Similar to the replica calculation for ellipsoids, we consider the thermodynamic limit
N, P  . We assume the manifold embedding dimension, D is finite in the thermo-

99

dynamic limit, and that the function f (s) is independent of N .

4.3.2 Fields of the Closest Point

Given w, we define the fields induced by the centers hµ0 and the fields induced by the basis vectors hµ as hµ0 = w -1 yµw · xµ0, which are the field induced by the manifold centers, and hµi = w -1 yµw · uµi i = 1, ..., D . Using these fields we can express the
constraints 4.74 by Eqn 4.6 and 4.7 corresponding to the point on the manifolds with the

smallest projection on the margin hyperplane of w. The evaluation of s~(h) requires the

differentiation of

D i=1

sihi

+

f (s)

with

respect

to

si,

where



is

a

Lagrange

multiplier

enforcing the manifold constraint, yielding,

hi = -sif (s), f (s) = 0

(4.75)

which needs to be solved for s~ = s and substitute in (h) = s~(h) · h - . The relation between the vector hand s~(h) is shown in Fig. 4.2(a).

4.3.3 Mean Field Equations of the Capacity
We use the replica theory to evaluate the limit where the volume of solutions vanishes. Similar to the replica calculation of ellipsoids, above, the equation capacity is given by

M -1() = F (t0, t) t0,t where M stands for manifolds

where

F (t0,t) =

min

z02 + z 2

z0,z,t0+z0+(t+z)>0

(h) = s~(h) · h - 
100

(4.76) (4.77) (4.78)

and s~ is the parameterization of the point on the manifold that is closest to the solution hyperplane characterized by h (minimizing ), given by 4.75 (Figure . 4.2(a))
Regime 1 (Interior Manifolds): t0 -  > -t · s(t^)
In this regime, z0 = z = 0 so that the fields h0and hare simply t0 and t, s=s~(t) and (t+z) = (t). This regime corresponds to the case where all manifolds are interior and do not contribute to F . The regime exists until the inequality t0 + (t)  0 becomes equality, i.e., t0 -  =-t · s(t^).
Regime 2 (Touching Manifolds): tC < t0 -  < -t · s(t^)
Here, t0 + z0 +  t + z = 0 but h0 = t0 + z0 = , implying that the manifold' center is an interior point; in other words, the manifold touches the margin plane only at a single point. Thus, for a given t0and t we need to solve

min
z

z02 +

z2

(4.79)

where z0 = -t0 -  t + z . Differentiating with respect to z yields, z = z0z = z0h{s · h}, namely,

z = z0s

(4.80)

(where we have changed notation from s~to s). This yields also,  = s · (t + z0s) - , hence

z0 = ( - t0 - t · s)(1 + s2)-1

(4.81)

Finally, z02 + z2 = z02(1 + s2) yielding,
101

F (t0,t)

=

(

-t·s- 1 + s2

t0)2

(4.82)

z0 = - (t0 - ) + (z + t)

(4.83)

This regime holds as long as the interior fields t + z are non zero. The lower limit of this regime is when t0 is such that these fields vanish, i.e.,z  -t, hence,

t + z0s(h)  0 so that sitself is antiparallel to t,

(4.84)

s(h) = -z0-1t

(4.85)

and where z0 =  - t0 , hence t = -z0s yielding for the lower limit of this regime,

tC

=



-

t sC (t)

(4.86)

where, sC(t) is the magnitude of the point s(h) defined by a vector h such that s(h)

is parallel to t. Thus, sC(t) is simply the magnitude of the intersection of t with the

manifold, Figure 4.2(b).

F

=

(-t

·

s - (t0 - 1 + s2

))2

where the D-dim vector s has to be calculated self-consistently through,

(4.87)

s = s~(t - z0s)

(4.88)

102

z0

=

-t

·s+- 1 + s2

t0

(4.89)

Regime 3 (Embedded Manifolds): t0 -  < -tC
Here h = t + z = 0, and h0 = t0 + z0 = , implying that the center point as well as the entire manifold is on the margin plane, hence

F (t0, t) = (t0 - )2 + t 2 Putting results from the two regimes, we get:

(4.90)

M -1 =

Dt

-t·s(t) -t/sC

Dt0 (-t

·

s - t0 + 1 + s2

)2

+

-t/sC

Dt

Dt0([t0 - ]2 + t2) (4.91)

-

where,

s = s~(t - z0s)

(4.92)

z0

=

-t

·s- 1+

t0 s2

+



and s2 = ||s||2.sC is the magnitude of the intersection of t with the manifold. Here, s~(h) is defined via

(4.93)

s~(h^) = arg mins s · h^, f (s ) = 0

(4.94)

4.3.4 Large D Limit
 We assume that D is large but the size of the manifold is such that s D,

M -1 =

Dt

-t·s -

Dt0

(

-t 1

·s- + s2

t0)2

,

when D

1

(4.95)

103

Figure 4.2: Geometrical Interpretation. (a) Relationship between different fields. h0 = w -1 yw · x0: field induced by the center of the manifold x0, i.e. the distance between the
center x0 and the solution hyperplane characterized by w . h is the vector of fields induced by basis vectors, i.e. hµi = w -1 yµw · uµi . Together, h0 + s · h determines the distance between the solution hyperplane and the closest point on the manifold characterized by the manifold shape
constraint f (s) = 0. (b) Geometric interpretation of different regimes. Purple line denotes the range ofs(t^) when the manifold is in the touching regime, from the point 1 to point 2. From point 2 to 3 denotes the range of s(t^) when the manifold is in the embedded regime.

s = s~(t - z0s)

z0

=

-t · s 1+

- t0 s2

Since t · s is large we can approximate

z0

=

-t · s 1 + s2

=

h·s

and assume self averaging,

z0

=

-t 1+

·s s2

=

h·s

We can introduce manifold dimensions and radii,

104

(4.96) (4.97) (4.98) (4.99)

Manifold Radius and Dimension We can now express the above results in terms
of the effective manifold dimensionality and radius. In the limit of large D, we can define

RM 2 = s2 t

t·s 2

DM =

t
RM 2

wheres is defined by the coupled equations

(4.100) (4.101)

s = s~(t - z0s)

(4.102)

z0

=

-t 1+

·s s2

so that the capacity can be expressed as

(4.103)

M = (1 + RM 2 )0( + RM DM )

(4.104)

 where RM DM behaves like an additional margin M introduced by the manifold

structure.

Scaling Regime
In the scaling regime, si = O(D-1) and z0is O(1) so t - z0s  t. In this regime, Eqns. 4.100-4.104 hold but the expression for the manifold radius and dimension are simpler, since s simply becomes

s = s~(t)

(4.105)

105

Here, the expressions for effective radius and dimension is given with Eqn. 4.105,

RW2 = s2 t, in scaling regime
t·s 2 DW = RM 2 t , in scaling regime with the excess margin

(4.106) (4.107)

W = RW DW in scaling regime where W stands for widths, see Figure 4.3.

(4.108)

Mean Width Interestingly, the excess margin W is related to a well known measure
of a size of convex manifolds, known as the mean widths64, and is defined as

Gaussian Mean Width =

max
s1,s2M

t · (s1 - s2)

t  2RW

DW = 2W

(4.109)

where s1 and s2 are points on a given manifold M in RN and t is a Gaussian random vector  I(0, IDt) .
The relationship between the manifold dimension DW and manifold radius RW and Gaussian Mean Width is illustrated in Fig. 4.3.

4.4 General Manifolds
Smooth networks are simple in that they can touch a hyperplane by a single point or be fully embedded in it. This is not true for non-smooth manifolds, as there are many facets that can be partially embedded due to the non-smoothness. In other words, a

106

Figure 4.3: Relationship between the Gaussian Mean Width the Effective Manifold Radius

and Dimension in the Scaling Regime. (a) Effective Radius RW = s~(t) 2 t is the mean of max

projection points s~(t) along the random directions t, while the effective dimension DW is defined

as

|t·s~(t)|
RW 2

2
t.

(b) Gaussian Mean Width is defined as GM W = 

definition, GM W = RW DW .

s~1(t) - s~2(t) · t t, and in this

non-smooth manifold can touch the hyperplane by a point, line segment, a facet, or multiple facets (Fig. 4.4).

4.4.1 Capacity of Smooth and Non-smooth Manifolds
Given the complicated geometric relations between non smooth manifolds and the margin planes, explicit expression for the capacity that delineates the different regimes in t and t0 is cumbersome and depends on the specific details of the manifold at hand. Here we note that for any manifold, we can write down the capacity in the following universal form

107

Figure 4.4: Embedding (support) structures of non-smooth manifolds: L1 manifolds. (a) Interior manifolds. (b) Touching with a point. (c) Touching with a line. (d) Touching with a facet. (e) Embedded in the margin plane.

Universal mean field equation for manifold capacity

M -1 =

Dt

-t·s(t) -

Dt0

(-t

·

s 1

- +

t0 + s2

)2

where, s is defined via

(4.110)

s = s~(t - z0s)

(4.111)

z0

=

-t

·s- 1+

t0 s2

+



(4.112)

The key point is that the solution for s changes its nature as t0 decreases (for a given

t) and automatically dissects the range of integration over t0 to the specific domains

(touching with points, lines, facets, etc). Note that the fully embedded regime is also

incorporated in 4.110. In this regime, t + z = t - z0s = 0, and z0 =  - t0, hence,

s = t/( - t0) , which in the embedding regime will be a point inside the convex

manifold in the direction of t, see Fig. 4.2(b).

108

4.4.2 Large D Approximation for a General Manifold
In the case of smooth manifolds, we have shown that in the limit of large D the capacity can be approximated by Eqns. 4.110-4.103. Here we note that the same approximation applies to general, smooth as well as non-smooth manifolds. Specifically, we approximate the capacity as Capacity for General Manifolds in high D:

M = (1 + RM 2 )0( + M )

(4.113)

M = RM DM

(4.114)

RM 2 = s2

(4.115)

where

DM =

t·s 2 RM 2

(4.116)

s = s~(t - z0s)

(4.117)

z0

=

-t 1+

·s s2

(4.118)

and averages are over gaussian D - dimensional vectors t. As is the case of smooth

manifolds, in the scaling regime where RM is O(D-1/2), RM and DM are given via s

where s is simply s~(t) , hence they coincide with RW and DW and are related to the

Gaussian Mean Width as in the smooth case above (Figure 4.3).

109

4.5 Numerical Investigations
4.5.1 Numerical Solutions of the Mean Field Equations
In simple cases analytical expressions can be used to solve numerically the mean field equations, as is the case of ellipsoids discussed above. Here we show how to use the analytical formulae to solve the mean field equations for the ellipsoids. For a general manifold, calculating s · t and s2 for s on the manifold for each t and t0 can be done by iterative methods. Here we present such an algorithm, adequate for general manifolds in the large D regime. In the limit of large D, t · s |t0| , , and we can use Eqns 4.113-4.118. Furthermore, in this limit, t · s, and s2 are self averaged with respect to t. The pseudocode for the algorithm is given in Alg. 4 below.
Iterative Numerical Solution
In order to solve for s and z0 (Eqn 4.117- 4.118), we use an algorithm for each t, that essentially iterates between updating z0 given the current estimate of s, using Eqn 4.118 and updating the estimate of s given the new estimate of z0 and the current estimate of s, Eqn 4.117.
Solving eq. 4.117: First we note that evaluating the min operation in Eqn 4.117 can be done explicitly in simple cases (in particular, for convex smooth manifolds with known parametrization). In general, one can search numerically for the max projection points (or signed min projection points). If the manifold is non-smooth and has a finite number of vertices, then one can simply iterate over all vertices. Otherwise, a local search using a gradient can be done, and since the search is on the convex hull, the local search guarantees the convergence to the global optimum. This appears as a maxproj function in Alg. 4.
Note that s we are solving is not simply a max projection point on t, but a max projection on h = t - z0s (Eqn. 4.117). The solution s may come from anywhere
110

inside of the convex hull or the surface of the manifold, hence we allow the search ons to be a linear combination of the vertices. To search for s, in the next step we define sk = sh + (1 - )sk-1 which is a linear sum of sk-1 in the previous step k - 1 and the max projection in the direction of h at time k, sh. If the difference between sk and sk-1 is smaller than a given tolerance 0, then s converged, as well as z0. Otherwise, continue to the k + 1 th step, where the new h is computed with the new s. The algorithm for this is summarized in the pseudocode (Alg. 4).

Algorithm 4 Iterative method for approximating capacity of general manifolds

in high D

[M ,RM , DM ] = function manifold capacity(D, nt,, nmax , 0, S)

Input: {Manifold dimension D, number of t s nt, learning rate , max iteration nmax, tolerance 0, the manifold dataS defined in RD×M }

for i=1 tont do t = t(i)  N orm(0, ID])

Set k = 0, ts = 

sk = maxproj(t,S)

while k < nmax and s > 0

k = k+1

z0

=

-t·s 1+s2

h = t - z0s

sh = maxproj(h, S)

sk = sh + (1 - )sk-1

s = ||sk - sk-1||/||sk||

end

s(i) = sk

end RM = sqrt

s(i)

2 i

{ } t(i)·s(i) i 2

D = M

RM 2



M = (1 + RM 2 )0( + RM DM )

Output = [M ,RM , DM ]

In the following sections, we show the specific manifold examples and how the input S of maxproj(t, S) , as well as the details of the max projection search differ based on
111

the type of the problem.

4.5.2 Simulation Results

Ellipsoids

Consider the ellipsoids give by Eqn. 4.2 and constraint, Eqn. 4.4. In this case, the
manifold parameterization is known, then sk = argmaxt · s has an analytical solution
s
given by the minus of Eqn. 4.8 and Eqn. 4.9 for ellipsoids. In this case, the input to the max projection operation for ellipsoid, called maxprojellipsoidshould include the radii vector. Pseudocode for maxprojellipsoid is given in Alg. 5. Furthermore, in the
case of ellipsoids, it is possible to solve for z0 analytically in the of the large D (i.e.
Eqn.4.40). However, to test the effectiveness of the iterative algorithm, we proceed to
test the iterative algorithm in the simulations below.

Algorithm 5 Maximum Projection Point on Ellipsoid S

s~ = function maxprojellipsoid(t, S)

Input: D-dimensional direction vector t, S = {D-dimensional radii vector R}

for i=1 toD do

s~ =  tiRi2

i

j t2j Rj2

end

Output: The point on the manifold with max projection in t , s~

Using the iterative methods described above, we calculated the theoretical estimate of linear classification capacity of ellipsoids, with embedding dimension D = 50 in ambient dimension N = 100 . The radii for each ellipsoid is given by the Ri = U nif [0.5R0, 1.5R0] for each i = 1, ..., D . R0 is shown in the x axis of the Fig 4.5 as Ri i. We compare the capacity estimated by the iterative algorithm using the mean field approximation (noted as iter), with the capacities estimated by the expressions of effective radius and effective dimension in different regimes (in the regime of Ri  O(1), Eqns. 4.45-4.46, and in the scaling regime, Eqns 4.55-4.56. We also calculated the simulation capacity
112

in the similar manner to M4 algorithms for L2 balls described in Chapter 2-3, with the worst point analytically calculated by Eqn. 4.8-4.9. Using the ambient dimension of N = 100 for the ellipsoids, the critical value of P was determined by finding the value of P such that the average number of being separable was half the times of the total number of 50 repetitions.
The iterative algorithm capacity matches the estimated capacities using effective radii and dimensions well, as well as simulation capacities described above. In the limit of large Ri, the capacity approaches 1/D, where the hyperplane is orthogonal to all embedded dimensions of an ellipsoid.
We also compare the measures of dimensions relevant for the classification capacity of ellipsoids (Fig. 4.5). The dimensions calculated from the iterative algorithm (DM , M for manifolds) is compared with the approximate effective ellipsoid dimensions DE in different regimes (Eqn 4.46 and 4.56), and they agree well. Furthermore, these estimated effective ellipsoid dimensions match the participation ratio (4.59), Dsvd , when Ri is in the scaling regime, and match the actual full embedding dimension D when Ri is large. Intuitively, this means that Dsvd is a relevant measure for linear classification capacity when Ri's are small ( D-1/2). In the case where Ri is large, the embedding dimension D is the relevant measure for the capacity, because the solution has to orthogonalize all embedding dimensions independent of the structure.
We also compare the ratio between effective radius and the actual scale of the ellipsoid (in this case R0 = Ri i), for RM calculated from the iterative algorithm and Reff calculated from approximations in each regime of Ri  D-1/2 (scaling) and Ri  O(1) (Fig. 4.5). As before, the agreement between the RM , Reff 's are good. Furthermore, the ratio between effective radius (for capacity) and the mean of the radii start out above 1, and decreases with increasing R0 . This means that the larger the ellipsoids get, the more fraction of ellipsoids get embedded, hence effective radius Reff contributing to the effective increase in the margin eff = Reff Deff gets smaller due to the embedding
113

Figure 4.5: Linear Classification of Ellipsoids. (a) Linear Classification Capacity of D-dimensional ellipsoids, E = P/N , where N is the ambient dimension, and P is maximum the number of ellipsoids such that P ellipsoids are linearly separable. The embedding dimension of ellipsoids used was D = 50 and Ri  U nif [0.5R0, 1.5R0] for i = 1, ..., D and R0 = Ri i is shown in the x-axis. (Red) Mean field approximation capacity iter , evaluated by the iterative algorithm given in Alg. 4 and Alg. 5. (Blue dashed) Approximation of the ellipsoid capacity as the capacity of a ball using a large D and Ri  O(1) approximation for RE and DE given by Eqns. 4.45- 4.46. (black dashed) Ellipsoid capacity approximated using REand DE approximation when Ri is in the scaling regime, given by Eqns. 4.55-4.56. (Green) Capacity approximation for large Ri, 1/D, where all of the ellipsoid embedding dimensions are orthogonalized by the solution. (Yellow) Simulation capacity computed
with N = 100 and 50 repetitions. (b) Dimensions of the ellipsoid. (Blue) Embedding dimension D. (Red) Dimension of the ellipsoid evaluated by the iterative algorithm, DM . (Orange) Participation ratio, DSV D, given by Eqn. 4.59 using Ri as eigenvalues. (Pink) DE approximation in large D Ri  O(1) regime given by Eqn.4.46 (Green) DE approximation in large D and scaling regime given by Eqn. 4.56. (c) Size (Radius) of Ellipsoids. (Red) Effective manifold radius RM evaluated by the iterative algorithm, divided by the overall scale Ri i. (Pink) RE approximation in large D, Ri  O(1) regime given by Eqn. 4.45(Green) RE approximation in large D and scaling regime given by Eqn. 4.55

configuration.

D Dimensional L1 Manifolds

Consider the problem of linearly classifying P of D-dimensional L1 manifolds where the

point on the L1 manifold is given by Eqn. 4.2 where f (s) =

D i=1

|si

/Ri

|

-

1

=

0

.

The

explicit expression for classifying L1 manifolds is considered in60, and in this section we

focus on finding their perceptron capacity as an example of manifolds that are defined by

114

their vertices (Fig. 4.4). In this case, there are only 2D vertices (2 extreme points along the direction vector ui), and finding the max projection point in the direction of t from the set of points S is given by sk = argmaxt · Sl , simply the search over all 2D vertices
l
whose computation time is linear in the number of points. In this case, the input for the max projection operation in the iterative algorithm, called maxprojsetpoints, should include the set of points. Pseudocode for maxprojsetpoints is given by Alg. 6.
Algorithm 6 Maximum Projection Point on a set of points S s~ = function maxprojsetpoints(t, S) Input: D-dimensional direction vectort, A set of M points in D dimensional basis which define the vertices of the convex hullS = {X  RD×M } il = argmaxlt · X(:, l) s~ = X(:, il) Output: The point on the manifold with max projection in t , s~
Using the iterative methods described above, we calculated the linear classification of capacity of L1 manifolds, with the embedding dimension D = 20 in the ambient dimension N = 100. The radii for each direction is set to be equal, i.e. Ri = R (all vertices are distance R away from the center ) for all i = 1, ...D. R is shown in the x axis of the Fig. 4.6.
We compare the capacity estimated by the iterative algorithm using the mean field approximation (noted as Iter), with the capacities estimated as that of a ball using the effective radius and effective dimension of L1 manifolds in the scaling regime. In the scaling regime, the replica analysis gives us RM = R and DM = 2log(D), and the derivations are given in the appendix to the chapter (section 4.7.2). The estimated capacity using the iterative algorithm agrees well with the simulation capacity, as well as approximations in the scaling regime, and large R regime.
We also compare the measures of dimensions relevant for the classification capacity of L1 manifolds. The dimension estimated by the iterative algorithm matches the approximation of DM (L1)  2log(D) in the scaling regime (due to the extreme value theory,
115

Figure 4.6: Linear Classification of Non-smooth manifolds: D-dimensional L1 Manifolds of Radius R. (a) Linear Classification Capacity of D-dimensional L1 manifolds,  = P/N , where N is the ambient dimension, and P is the maximum number of manifolds such that P L1manifolds are linearly separable. The embedding dimension of L1manifolds used was D = 20 and the number of subsamples used for testing was m = 40, two endpoints in each basis vector direction. R is shown in the x-axis. (Red) Mean field approximation capacity iter , evaluated by the iterative algorithm given in Alg. 4 and Alg. 6. (Blue dashed) Approximation of the L1 capacity as the capacity of a ball using R as the actual R and DM = logD, which is the approximation of effective manifold properties in the large D regime. (blue markers) Simulation capacity calculated with N = 100 and 50 iterations to compute the fraction of linear separability. (Green) Capacity approximation for large R, 1/D, where all of the L1 manifold embedding dimensions are orthogonalized by the solution. (b) Dimensions of the L1 manifolds. (Green) Embedding dimension D. (Red dashed) Participation ratio, DSV D, given by Eqn. 4.59 using Ri = R as eigenvalues. (Blue) Dimension of the L1 manifolds evaluated by the iterative algorithm, DM . (Black marker) DB1 approximation in large D regime, 2logD (Derivation in appendix). (c) Size (Radius) of L1 manifolds divided by R. (Red) Effective manifold radius RM evaluated by the iterative algorithm, divided by R, compared with unity (Blue).
details in the Appendix to the chapter), and in the regime of large R it matches the embedding dimension D, which in this case is equivalent to the participation ratio Dsvd (as all Ri = R ).
Furthermore, in the scaling regime, the effective manifold radius found by the iterative algorithm RM is close to R, as predicted by the theory (details in the appendix). RM /R transitions from 1 (in the scaling regime) to a value much smaller than 1 (in the large R regime), due to the increased fraction of L1 manifolds that are embedded.
116

Random Strings
Consider the problem of linearly classifying P of random strings, whose intrinsic dimension is 1, but the embedding dimension is D, and the ambient dimension of N . Each point on the random string is parameterized by the vector s whose components are

s2k = Rkcos {k ( - k)} ;

(4.119)

s2k+1 = Rksin {k ( - k)} This can be re-written as

(4.120)

D/2

xµi = (x0)µi +

Rnejn(-n)ui

(4.121)

n=1

 (where the j is used as an imaginary -1 to distinguish from the index i ).

Figure 4.7 illustrates an example of a random string. This definition has an interesting

analogy with the activity patterns of the population of orientation tuned neurons. For

instance, the value of point x in i th dimension, xi can be thought of as i th neuron's

activity, where each neuron is tuned to a different orientation (middle panel). The

heterogenous Ri can be thought of as different amplitudes of the neural activity. In this

analogy, if you take an object at one angle, then all neurons will have different levels

of activations, and the slice of the activity patterns correspond to a point on a random

string, parametrized by , which is essentially the angle of an object. If you change the

orientation of the stimulus, then the activity patterns correspond to a different slice,

which corresponds to a different point on a -parametrized random string. Once you

rotate the stimulus the full 360°, then the activity pattern comes back to the original slice,

and you come back to the original point on the random string. Note that this particular

random string lies on the surface of a ball, whose radius is Rstring =

D/2 n=1

Rn2

.

117

Figure 4.7: Random Strings (Illustration). (a) Random String in Neural State Space. The values in each ambient dimension is given by each neuron's activity. The random string's degree of freedom is 1, while the embedding dimension is D. Each point on the sample manifold represents neural activity of a same object, with different latent variable such as orientation of an object. (b) Neural Interpretation of Random Strings. The random string manifolds given by Eqns. 4.119-4.121 can be interpreted as orientation tuned neurons with different amplitudes and frequencies. (c) Illustration of Rn for nth basis vector, which is similar to the amplitude of the neural tuning curve for n th neuron.
118

Figure 4.8: Linear Classification of D-dimensional Random Strings. (a) Linear Classification Capacity of D-dimensional Random Strings,  = P/N , where N is the ambient dimension, and P is the maximum number of random strings such that P random strings are linearly separable. The embedding dimension of random strings used was D = 20 and the number of subsamples used for testing was m = 200. Rstring, overall scale of the random string, and the radius of the D-dimensional ball that the random string is on, is shown in the x-axis. (Red dashed) Mean field approximation capacity iter , evaluated by the iterative algorithm given in Alg. 4 and Alg. 6. (Yellow marker) Simulation capacity calculated with N = 100 and 50 iterations to compute the fraction of linear separability. (b) Dimensions of the Random Strings. (Green) Embedding dimension D. (Red dashed) Participation ratio, DSV D, given by Eqn. 4.59 using Ri = R as eigenvalues. (Blue) Dimension of the random strings evaluated by the iterative algorithm, DM . (Black marker) 1+logD. (c) Size (Radius) of random strings divided by R. (Red) Effective manifold radius RM evaluated by the iterative algorithm, divided by R, compared with unity (Blue).

What should be an effective dimension of this string with heterogenous scales R? And,

if these strings are in random positions and directions, what should be their capacity?

To test this, we calculated the classification capacity of samples of random strings in

embedding dimension D, ambient dimension N = 100, where the number of samples

used

was

m

=

200,

such

that

m

=

2 m

,

and

Rn

=

R

for

all

n.

We find that the classification capacity M = iter found by an iterative algorithm

matches the simulation capacity of random strings (Figure 4.8(a)). In the case of random

strings, the manifold effective dimension DM (also found via the iterative algorithm)

has a very low effective dimension in the scaling regime, due to the fact that it is a

string whose intrinsic dimension is merely 1 and is not filling the space spanned by D

basis vectors, although when R is large, the solution has to orthogonalize the manifolds

119

Figure 4.9: Effective Properties Random Strings for Different Number of Samples Per Manifold. (a) Random String Dimension versus the Number of Samples Per Manifold (m), for N = 100, D = 20, R = 0.224( 1/ D)). Both participation ratio, Dsvd (red) and the effective manifold dimension found by the Mean Field iterative algorithm, DM , saturates around m = 50, indicating that m = 200 used in the Figure 4.8 is already a saturated, m -independant properties. (b) Random String Radius versus the Number of Samples Per Manifold (m). The effective manifold radius (RM ) found by the Mean Field iterative algorithm saturates around m = 100.
and the effective dimension approaches the embedding dimension D, and it is reflected in the DM found via the iterative algorithm (Figure 4.8(b)). Note that in this case, since all Ri has the same size, Dsvd = D . Furthermore, RM is R in the scaling regime, and RM goes to a value much smaller than R in the large R regime, reflecting the fact that many of the random strings must be orthogonalized by the solution, and therefore more of them are embedded, resulting in smaller RM /R seen by the centers.
In the simulations for Figure 4.8, we used m = 200 training samples per each manifold. In the above figure we find that the manifold dimensions DM and radius RM is in the saturated regime as a function of m, due to the fact that the samples are coming from a string, and the sampling is already dense at m = 200. This is essentially like doing a local search for the worst points (although not explicit), because we are in the densely sampled regime.
120

4.6 Discussion
We have presented a mean field theoretic calculation of linear classification of general manifolds, extending Gardner's replica theory of classification of random isolated points8,23 and the recently developed theory of classification of random balls60. This theory characterize the capacity as the inverse rate of reduction in the entropy of the weight vector space by the separability constraints per manifold. These constraints are expressed in terms of special points on the manifolds that have minimum projections on self consistent D-dimensional field vector. Importantly, we were able to derive a set of universal mean field equations applicable to all low dimensional convex manifolds, 4.110-4.112. The key point is that for a given manifold geometry, the position of the worst point on the manifold represented by s = s~(t - z0s) changes as the field on the center represented by t0 spanning the sequence of increasingly large overlap between the manifold and the the margin plane. This sequence depends of course on the details of the geometry of the convex manifold. These equations cannot be solved analytically except for the simplest geometries. We have developed an iterative algorithm to solve these self consistent equations, and in our experience, their converge is remarkably fast even when dealing with D dimensional manifolds with D in the range of 10 - 100.
Of particular interest is the case of manifolds with high dimension (D 1) In this case, the key parameters are Manifold Radius, RM and Dimensionality DM . The manifold capacity is equivalent to that of L2 balls with dimensionality and radius equal to DM and RM respectively. These quantities appear in the capacity mainly through
 the excess margin M = RM DM . The reason for this combination is the following. Consider first, an N dimensional ball. The margin is not dimensionless but depends on the norm of the inputs. Thus, if we demand a margin this is equivalent to demanding
 a margin  + R N from the centers, since, if the distance of the points on the circum-
 ference of the ball from the center is R N . Now when the ball is low dimensional, a reduction in the required excess margin occurs due to the tilt of the ball with respect
121

to the hyperplane, so that the projection of the center should increase only by a factor 
N D/N = R D. A similar argument holds for a general manifold in this limit. We have noted that the geometric parameters RM and DM are not intrinsic geometric
measures but depend also on the overall size of the manifolds relative to the center norm. Thus, if the manifold is increased by scaling all the points by a global factor, say r, relative to the center, then when r grows, eventually DM approaches the full embedding dimension D, reflecting the need of a solution to be orthogonal to the entire manifold subspace. Conversely, when r decreases, eventually the manifold reaches the scaling
 regime where capacity is finite despite the high dimension, and RM DM approach the
 Gaussian Mean Width value RW DW = 0.5M W , see 4.3 The change in the Manifold Dimension as r increases may be dramatic. For instance, in the random string example (as well as L1balls), DM increases from DM  log D for small r (the scaling regime) to DM = D for large r, see 4.8b.
In conclusion, the generality of the theory developed in this Chapter opens the door for applications of the derived results and methods for the investigation of neuronal representations of perceptual manifolds in biological as well as artificial neuronal networks. However, in order to do so, some limitations of the current theory might need to be relaxed. For instance, the present results deal with random labels where the two classes are of roughly equal size. In many real problems this may not be the case. Another issue is the assumption of random orientation of the manifolds. It would be important to understand the role of correlation between the manifolds. Also, it will be interesting to explore extensions to manifold representations which are not linearly separable. These issues and others are discussed in the next Chapter.
122

4.7 Appendix

4.7.1 Equivalence between Ri and i

Lemma 12. If

s2i Ri-2

=

1 D

then,

Ri

 i.

Proof. From the definition we know that

s2i

=

Ri2 D

.

Consider

the

covariance

C

of

the

manifold data X from many realizations of s

Then,

X = x0 + siui
i

C = {X - X } {X - X }T = sisj uiuTj
ij
i are by definition,

Hence,

1 N

2i

=

s2i

2i

=

N D

Ri2

4.7.2 Effective Dimension of L1 manifolds
In the scaling regime, s(t) for L1 manifolds is the i-th vertex where i = argmax ti. So,
i
s · t = Rmax ti. Given D normally distributed ti's, the max value is centered in large D
i

max ti  2 log D
i
123

(4.122)

due to the extreme value theory. Hence, we obtain DM = 2 log D
RM = R for L1 manifolds in the scaling regime.

(4.123) (4.124)

124

Chapter 5 Extensions

5.1 Correlated Manifolds
So far, we have considered randomly oriented manifolds. In real world data we expect that the manifolds will be correlated; in particular, that the subspaces spanned by the different manifolds will be partially aligned. We first consider the simple case of D dimensional spheres that all share the same subspace in the ambient dimension RN .

5.1.1 Parallel Spheres
Consider a perceptron classifying parallel D-dimensional discs, embedded in N dimensions. The training data is given by:

x0µ + R

1 D

D

uisi

 |s|2  1

i=1

(5.1)

where as before the components of x0µ and ui are i.i.d. normally distributed random 
variables. For reasons that will be clear later on, it is convenient to scale R to R/ D.

It is also convenient to rotate the axes so that ui are along the standard first D axes,

125

so that the corresponding fields are

hµ(s)

=

1 

yµ

N

wT xµ0 + R

N D

D
wisi

i=1

>0

 |s|2  1

where here we restrict ourselves to zero (fixed) margin. Minimizing with respect to si, we get

(5.2)

si = -yµwi/ Therefore, the minimum of LHS of 5.2 is

wj2
j

(5.3)

min hµ(s)
s

=

1 N

yµwT xµ0

-

R

1 D

D

wi2

i=1

Thus, the constraints are reduced to

(5.4)

where

1 
N

yµwT xµ0

>



(5.5)

 = R

(5.6)

and



=

1 D

D

wi2

i=1

(5.7)

which is an average of dot products between the solution w and direction vectors (in

a rotated coordinate). Let us call  the overlap parameter.

Note that the total variance of each manifold is R2N/D while the square distance

between center pairs is: 2N . Thus, their ratio is R2/2D . In contrast, in the random

spheres, the total variance of each manifold is R2N whereas the square distance is as

126

before 2N . Hence the ratio is R2/2 . It is important to note that if D N then the solution vector w can be orthogonal to
all manifolds by zeroing the corresponding D components for any size of the manifold R, without sacrificing significant degrees of freedom. In this case, we expect the capacity to be the same as for the capacity with the centers only, in the reduced dimension of N - D.
Thus, the problem of parallel manifolds is interesting only when D  N (D scales with N ). Let us define the relative dimension parameter d, such that

D = dN

(5.8)

In this regime, zeroing all D components of w is costly, so we expect that the nature
of w depends on d and R. Note that in this case (since D 1) the relevant scale of 
the radius should be radius over D. In our normalization above it means that R is of
order 1.

Capacity
The basic constraint 5.5 is equivalent to a Gardner's theory with margin  which however is not a fixed parameter but assumes self consistent value, set by the order parameter  which measures the overlap between w and the manifold subspaces. To evaluate  (Eqn. 5.7) we need to evaluate the entropy of the solution space given the constraint that the solutions' average projection on the common manifold subspaces is  . We denote this entropy (per N ) byS0(). The analog of 4.20 is

1 N

log V

= S0() - 0-1()

(5.9)

where 0 refers to the capacity for points. As before, in the capacity limit, log V

127

vanishes and from 5.9 it follows that the capacity is

|| = 0()S0-1() where the symbol || stands for parallel manifolds and the entropy term is

(5.10)

S0()

=

(x

-

1

+

dx2 d)2 +

d(1

-

d)

where x is related to  through,

(5.11)



=

d[(x

(x - 1 + d)2 - 1 + d)2 + d(1

-

d)]

(5.12)

Different overlap  yields different capacities, so optimizing the capacity || with respect to  yields the following equation (for  or x), as || is the maximum possible capacity



(x

x(1 - x) - 1 + d)2 +

d d(1

-

d)

=

||R

 H (- )

+

exp - 

1 2

2

2

(5.13)

Since there are 3 equations (Eqns 5.10-5.11, 5.12 and 5.13), and 3 unknowns (||, x , ), one can solve for || or .

Phase Transition
The solution for the capacity above shows dependence on the overlap parameter , which needs to be solved via x as a function of R. It turns out that there are two regimes of solutions for , one where  decreases with increasing R, up to R < Rc, and another where  = 0 for R > Rc. Qualitatively, this means that for the parallel manifolds whose radii are smaller than Rc , the overlap between w and manifold subspace is nonzero, and the overlap increases with increasing R. However, when the manifold R is beyond a critical value Rc, all manifold subspaces are orthogonal to w and the overlap becomes

128

0, due to the tradeoff between orthogonalizing and sacrificing the degrees of freedom. The geometric intuition for two different phases is given in Fig. 5.1 (a)-(b).
Let us consider the value of Rc(d) such that  =   0 as R  Rc+. At this value, the solution must be orthogonal to the manifold, therefore  = 2(1 - d) (Fig. 5.1(c), Regime R > Rc ). Also, for  to vanish, x = 1 - d, yielding,

Rc(d) =

d2 2(1 - d)

(5.14)

Thus for R > Rc(d), || = 2(1 - d) and  =  = 0. See Figure 5.1-(c). As the figure

5.1shows, the predictions agree well with numerical simulations.

Field Distribution
Since the field distribution is determined by the set of constraints on the fields, in our case they should be equivalent to the distribution of fields in the Gardner's theory with margin given by 5.6 (see 5.9). This means that as long as  > 0 i.e. R < Rc , w is not in the null space of any manifold. The fraction of manifolds that touch the margin is given (as in the Gardner theory) by

B = H()

(5.15)

and the fraction that are interior is H(-). When R > Rc , w is in the null space of the manifolds. Half of the manifold center are on the margin plane and half are not. See Figure 5.1(d).

129

Figure 5.1: The Phase Transition of Linear Classification of Parallel Manifolds. (a) When R < Rc , manifolds are either interior (solid) or touching the margin planes (striped). (b) When R > Rc , manifolds are either interior or embedded in the margin planes (diamonds). (c) Phase transition of overlap and capacity at critical radius Rc, for classification of parallel balls with D = 5 in ambient dimension N = 100. The overlap ( , Eqn. 5.7) between the manifold axes and the w denoting the solution hyperplane vanishes as the ball's radius R approaches Rc. The capacity parallel also goes through phase transition as well, however the value of capacity is stays large, as the embedding dimensions by all manifolds are limited to D. (d) Phase transition of manifold configurations at crucial radius Rc. When R < Rc , most manifolds are either interior or touching the margin plane (as shown in (a)), and when R > Rc , most manifolds are either embedded or touching the margin plane (as shown in (b)). The simulations and theory show good agreement.
130

5.1.2 Discussion
We can generalize this analysis to partially parallel correlated balls, where only a fraction of dimensions (D1 = d1N < D) are shared between them and the rest of the subspaces (D2 = D - D1) are random. In this case, in the regime where the shared subspaces are orthogonalized, the problem remains as the classification of balls in the null space of the shared subspaces. In this case, the solution will take the form of

|| = B(, R, D)S0-1()

(5.16)

where B is the capacity of balls and  is the effective margin in the null space due to the radii in the parallel subspaces and Ris the effective radius in the null space due to the random directions and D is the effective ball dimension in the null space due to the random directions. Furthermore, the problem can be generalized to other types of correlations (i.e. correlation between the manifold subspace and the center of the manifold, or the correlation between centers.) We hope to explore these issues of various types of correlations to take into account the structures in the realistic data.

131

5.2 Mixtures of Shapes

Figure 5.2: Linear Classification of Mixtures of Shapes (Illustration). The linear classification capacity of mixtures of shapes is given by Eqn. 5.17.

So far, we have discussed the classification of manifolds with same shapes and sizes. In this section, we generalize the problem to the classification of manifolds with different shapes and sizes. Suppose there are C different manifold types, and each of these types have the capacity of s where s = 1, ..., C (s for shape).
In this problem, the self-consistent term G1 (the free energy term) is an average of each G1 of the classification problem of manifolds of the shape s. Recall that G1 of each shape determines the capacity for each shape, through s-1.
Then, the linear classification capacity of the mixtures of C different manifold types can be simplified to

m-1ixture = s-1 s

(5.17)

where s = 1, ..., C refers to the index for each manifold type. This remarkably simple and general theoretical result opens doors to the treatment of a vastly diverse set of manifold classification problems, from classification of manifolds different shapes and sizes to different ratios of labels.

132

5.3 Class Imbalance

So far, we have covered the binary classification of manifolds where the number of positively labeled manifolds is equal to the number of negatively labeled manifolds. Here we consider the class imbalance problem, where the number of positive labels is far less than the number of negative labels, or vice versa. This is also known as classification with sparsity in the labels. In the theory of classification of points, increasing the sparsity of labels has been known to increase the point classification capacity by orders of magnitude (7). In this section, we ask the question whether increasing the sparsity of manifold labels also improve the manifold classification capacity.
Note that classification of manifolds with sparse labels is an important example of classification with inhomogeneous manifolds (Section 5.2). Notice that sparsity term f is defined as the fraction of positively (or negatively) labeled manifolds out of the total number of manifolds, so a large sparsity actually refers to a small f (Fig. 5.3).
One important thing to note is that in the balanced binary classification case where the number of positive and negative labels are equal (f = 0.5), the bias term, b, of the linear classification

D
yµ wT x0µ + siuµi + b  ||w||
i=1

(5.18)

was ignored because the optimal bias term which maximizes the classification capacity

with balanced labels is b = 0. However, in the sparse label case, this is no longer true,

and the nonzero bias term needs to be included in the evaluation of the capacity, and

the bias term needs to be optimized.

5.3.1 Sparse General Manifolds
Using the similar analysis as the calculation of perceptron capacity for mixtures of shapes (Section 5.2), we average the inverse of capacities for the positive and negative
133

labels, and arrive at the expression for the manifold capacity with sparse labels.
Sparse General Manifolds With this, we can express capacity of general manifolds
with label sparsity f as

M -1(, f ) = f M -1( + bmax) + (1 - f )M -1( - bmax).

(5.19)

where

bmax = argmaxbM (, f, b)

and
M -1(, f, b) = f M -1( + b) + (1 - f )M -1( - b)
Sparse D-dimensional Balls As an example, we can express capacity of D-dimensional
L2 balls of radius R with label sparsity f as

B-1(, R, D, f ) = f B-1( + bmax, R, D) + (1 - f )B-1( - bmax, R, D).

(5.20)

where bmax needs to be found so that it maximizes the capacity with sparsity f , i.e.

bmax = argmaxbB(, R, D, f, b)

(5.21)

134

where

B-1(, R, D, f, b) = f B-1( + b, R, D) + (1 - f )B-1( - b, R, D).

(5.22)

This result implies that similarly to the case of the points, the manifold capacity increases significantly with the increased sparsity (reduced f ) (Fig. 5.3).

Fraction of Support Structures In a similar manner to the replica calculation of
fraction of support structures in60, the fraction of support structures for sparse labels can be calculated. Note that due to the asymmetry in the number of positive and negative labels and the non-zero bias, the terms with majority labels and non-majority labels are different. We give here the expression for the fraction of support structures for D-dimensional balls. First, manifolds with non-majority labels, which consists of f of the total manifolds can be either embedded, touching, or in the interior side of the shattered space. All together, they consist of the first (non-majority) term of the capacity expression with coefficient f , in Eqn. 5.20.
With this, we can derive the fraction of support structures of manifolds with sparse labels. Unlike the problem with dense labels, the minority manifolds (red manifolds in Fig. 5.3(a)) and the majority manifolds (blue manifolds in Fig. 5.3(a)) have different behaviors.
First, fraction of embedded manifolds with non-majority labels is



pmeminbor = f

dtD(t)

0

+b-

1 R

t

Dt0

-

(5.23)

Then, fraction of manifolds with non-majority labels that touch the margin plane is



pmtouincohr = f

dtD(t)

0

+b+Rt

Dt0

+b-

1 R

t

(5.24)

The rest of the non-majority manifolds are those in the interior space shattered by

135

the margin planes



pminitneroiror = f

dtD(t)

0


Dt0
+b+Rt

(5.25)

Note that pmeminbor +pmtouincohr +pminitneroiror = f . Similarly, the fraction of embedded manifolds

with majority labels is



pmemabjor = (1 - f )

dtD(t)

0

-b-

1 R

t

Dt0

-

(5.26)

The fraction of manifolds that touch the margin plane with majority labels is



pmtouajchor = (1 - f )

dtD(t)

0

-b+Rt

Dt0

-b-

1 R

t

(5.27)

The fraction of the manifolds with majority labels that are in the interior space

shattered by the margin planes is



pminatejroiror = (1 - f )

dtD(t)

0


Dt0
-b+Rt

(5.28)

Like with the sparse case, Note that pmemabjor + pmtouajchor + pminatejroiror = 1 - f . The example of this theoretical prediction is tested in Fig. 5.3(c), where we show that it matches

well the fraction of sparse manifold structures in the case of 1D manifolds (lines).

Simulations The linear classification capacity for 2 dimensional balls with sparsity
f has been evaluated numerically and theoretically. For the numerical evaluation, M4 algorithm (Chapter 3) has been used with sparse labels. For the fraction of support structures, 1D line manifolds with sparse labels were used. For the theoretical evaluation, the Eqn. 5.20 and Eqn.5.23 - 5.28 has been used, and agree well with the simulations.

136

Figure 5.3: Linear Classification of Balls with Sparse Labels. (a) (Illustration) The solution hyperplane (grey) separates manifolds, where the fraction of positively labeled manifolds out of the total number of manifolds are given by f 1 . (b) Capacity of 2D L2 balls with sparsity f = 0.001(blue) and f = 0.01 (red). Theory (line) matches simulations (markers) well. (c) Support configurations of 1D line segments with sparsity f = 0.01, for majority labels (denoted as nf , solid line in the legend) and non-majority (sparse) labels (denoted as f , dashed line in the legend). Theory matches simulations well. As R is increased, the fraction of embedded line segments becomes 1, and the transition happens at smaller R in manifolds with non-majority label compared with manifolds with majority labels.
Note that in Fig. 5.3(c), when R starts out small, most of the non-majority manifolds are touching, and most of the majority manifolds are interior, and as R is increased, the phase transition where most of the sparse manifolds become embedded happens first, and then, the phase transition where most of the non-sparse manifolds become touching happens.
5.3.2 Small f Regime
Let us focus on the D = 1, the classification of lines with sparsity f . What is the behavior of the line capacity L , in the case of extreme sparsity, i.e. f  0? The dominant term analysis in different regimes of R gives the following analytical approximations for capacity with  = 0.
137

1. R = O(1)

L-1(R, f ) = 2(1 + R2)f | log f |

(5.29)

2. f -1/2 R 1

L-1(R, f ) = 2R2f | log R2f |

(5.30)

3. f -1 R f -1/2 4. R f -1, R  

L-1(R,

f)



1

-

2 R2f

L-1(R, f ) = 1 + 2f | log f |

(5.31) (5.32)

It is interesting to note that in the limit of large R, the capacity does not depend on the R any more.

Object Recognition Limit, f = 1/P
Particularly interesting regime is when the sparsity f is equivalent to 1/P where P is number of manifolds. This is analogous to the one-vs-all task in the multi-class classification problem, where the output unit is activated only when the input comes from the correct class out of the possible P classes. Can we estimate the capacity of object manifolds, in this relevant sparse object recognition limit?
Using our theory, the minimum network size N required in order to classify one manifold out of P given manifolds can be estimated to be

N  = P/M

f

=

1 P

(5.33)

where we can use 1/P as sparsity f .

One can also estimate the largest allowed size of the manifolds if one is given with the network size N and the number of object manifolds P . That is, solve for R and D

138

such that

P N

=

B(,

R,

D,

1 P

)

which can be found numerically.

(5.34)

Simulation Results and Discussion We tested below the capacity of line seg-
ments, in the one-vs-all object recognition task limit. In the simulations where critical P (number of classes/manifolds) had to be found, we fixed the network size, N = 100 and N = 200, and P was varied to find P  at which the probability of finding a linear classifier goes from 1 to 0, given 100 repetitions. The theoretical prediction matches the simulation capacity well (Fig. 5.4(a)).

Figure 5.4: Line Classification of Line Segments in Object Recognition Limit (f = 1/P ). (a) Capacity (L(f = 1/P )) as a function of size R for different network sizes. (b) Minimum network size required for linear separability (N ) as a function of number of manifolds/objects (P ), for different sizes R (c) Capacity as a function of number of manifolds/objects (P ).
If we want to classify P = 1000 classes, whose object manifolds are 1 dimensional, with one-versus-all task, how many neurons are required for the problem to be linearly separable? Figure 5.4(b) provides an answer to that question with our theoretical estimate of minimum required network size N . In the limit of point, R = 0, roughly N  = 10 is enough to classify 1000 objects. However, if R > 50, at least N  = 1000 is required to be linearly separable.
139

Notice that the capacity in the units of load (P/N ) shows an interesting behavior in the limit of large R in Figure 5.4(c). Notice that in large R , the capacity is dominated by 1/D and the improvement due to sparsity is smaller than when R is small. In other words, if the manifold sizes are large, making the labels sparse does not improve the capacity as much as when the manifold sizes are small. Therefore, the effect of size of the manifold (R) on the capacity is more dramatic in the case of classification with sparsity in the object recognition limit (compared to the dense label classification).
5.4 Building Multi Layer Networks of Sparse Clas-
sifiers
In the section 5.3, we showed how introducing the sparsity in the manifold labels improves substantially the classification capacity of manifolds. Here we show that we can use this feature to solve dense classification task. The general idea is as follows. Suppose we have an input neural layer with size N representing P manifolds and a dense classification task (i.e. label sparsity f is  0.5), such that a linear classifier applied directly to this input layer fails to classify all stimuli correctly, namely, the manifolds in the input representations are not linearly separable.
To solve the task we add a single hidden layer with M binary units (Fig. 5.5(a)). We would like to generate a hidden layer representation of the manifolds that is invariant, namely that all inputs from a given manifold are mapped to a single activity pattern in the hidden layer. If we can achieve this, the invariant representations in the hidden layer can easily be linearly separable. In order to generate this invariant representations in the intermediate layer, we generate M random sparse labels for each manifold, and learn the connection from the input layer to the intermediate layer as a sparse linear classification in each unit in the intermediate layer, which we assume is below the capacity and therefore can be implemented without error. In the following subsection
140

below, we analyze the range of parameters and the performance of this two layer network.
141

Figure 5.5: Sparse Intermediate Representation Enhances the Invariant Processing of Manifolds. (a) Input layer with dimension N , where input vectors are drawn from a set of P manifolds. The weight matrix, V , from the input to the intermediate layer is constructed so that each node in the intermediate layer is activated in invariant manner by randomly chosen fP manifolds (where f is sparsity). This yields a sparse representation where all inputs from the same manifold activates
the same fM intermediate nodes. Output node classifies the manifolds with desired dense binary labels. (b) Perceptron capacity of manifold classification,  = P/N versus manifold ball dimension D at R = 1, for different sparsity (f ) and margin (). The X marker denotes the working point for simulation in (c-d). (c-d): Robustness to noise. (c) Probability of error at output layer, versus the standard deviation of the input additive Gaussian noise (noise) for different intermediate layer sparsity f . Input manifold dimension and radius are: D = 20 and R = 1. For simulation, N = M = 500, and number of manifolds P = 250 (which is five times the single layer capacity, PSL = SLN  48, for  = 0, for these manifolds). Markers indicate the simulation results for the error for different sparsities. Robustness to noise is achieved by ensuring a significant margin int at the intermediate nodes; the margins int are shown as vertical lines. (dashed) simulation (solid) analytical prediction. Higher sparsity (f = 0.01) ensures larger margin, and more robustness to input layer variability. (d) Output margin (out) versus the smoothness parameter (T ) of the sigmoid in
the intermediate layer (1 + e-x/T )-1, for differ1e4n2t number of manifolds, P . T = 0 is the binary
limit. For the simulation, N = 500, M = 500, f = 0.01, R = 1, D = 20 was used. Values of P are an order of magnitude larger than the single layer capacity of the input layer which is PSL  48. (vertical lines) intermediate layer margins. For the same f , smaller number of manifolds (P = 500) allows larger margin, and higher robustness to the smoothing of the intermediate layer responses and
the resultant manifold variability.

5.4.1 Capacity of Two-Layer Network with Sparse Invariant
Representation
As a simple example, we focus on input manifolds of D dimensional balls with radius R such that P (number of balls) is larger than the linear classification capacity of such balls, N B(R, D). We first compute the perceptron capacity for manifolds with sparse labels, parametrized as above by  D, R and f , B(, R, D, f ) , where f is the fraction of positive examples. Similar to classification of points4,7, the perceptron capacity of manifolds with sparse f 1, is much higher than when the labels are dense (f = 0.5) (Fig. 5.5(a)). Consider now the task of invariant classification of manifolds where the task labels are dense, (e.g., f = 0.5, and  = 0). If the number of manifolds P relative to the size of the input layer, N is above B(, R, D, 0.5), then the single layer architecture will be unable to solve the linear classification task. However, we can use the improved perceptron capacity for sparse labels (Fig. 5.5(b)), to construct an intermediate representation of M nodes, each one of them trained for a randomly chosen sparse labeling of the manifolds. These sparse labels are unrelated to the task labels; they are used solely for building the intermediate representation. As long as P/N is below the perceptron capacity for sparse classification of manifolds, the resultant intermediate layer generates an invariant representation. The subsequent single layer readout at the output layer can then perform the required dense classification as long as P/M < 0(, f = 0.5), namely the perceptron capacity for dense labels of points. The overall capacity for classification of manifolds of this two-layer network is given by BT L(0, R, f = 0.5) = min (2M, N D(0, R, f )) /N , where T L stands for two layer, much higher than that of the single layer (SL), SL = B(0, R, D, 0.5) (for M  N ) (Fig. 5.5(c-d) captions). In addition, below the zero-margin capacity, DT L(0, R, f = 0.5), the
143

maximum margin out at the output node is given by 0(out, 0.5) = P/M .
5.4.2 Robustness to Noise
This two-layer architecture shows not only enhanced capacity, but can also enhance the system's robustness to noise. To achieve robustness, the sparsity of the intermediate representation should be sufficiently large to have a significant margin, int > 0 in their representation (where subscript 'int' stands for intermediate layer), i.e, P/N < D(int, R, f ). Then adding noise may cause the sparse representation in the intermediate layer to be only approximately invariant to the input manifold degrees of freedom, nevertheless the effect on the performance will be small provided the noise level is small compared with int. We demonstrate this in two cases. First, additive full-rank Gaussian noise was introduced to the input layer. As shown in (Fig. 5.5(c)) the two layer network is robust to a range of noise values, and networks with sparser intermediate representations have output error probabilities close to zero for a larger range of noise. For output noise in the intermediate layer we haven't analyzed the performance explicitly, but we present the heuristic analysis based on the idea that the level of robustness to noise in each unit should be determined by the size of each intermediate unit's classification margin int. The details of the numerics are given in the appendix to the chapter (Section 5.8.1). Next, stochasticity in the activation of the intermediate binary units is modeled by changing their activation function to smooth sigmoidal units with gain parameter 1/T . Smooth rate functions are also more representative of biologically realistic rate-based models of neural networks. Fig. 5.5(d) shows that for a broad range of T the output readout was able to correctly classify the manifolds. The critical value of T above which classification fails is roughly given by the margin of the intermediate layer. These examples show how to construct a network able to classify manifolds with small error even when the intermediate layer is not completely invariant to the manifold representation.
144

5.4.3 Discussion
In this section, we have shown that by using classifiers of manifolds with sparse labels, a two layer network can be constructed with enhanced manifold processing capacity and robustness to noise. Thus, our theory provides a biologically plausible simple feedforward network model that is capable of processing object relation information in an invariant manner. The current theory can be extended to in several important ways. Here we focused on training the network weights with full manifolds and adding an additive noise after training, but the network weights can be trained with subsamples of manifolds, or with noisy realizations of manifolds. Here we focused on the intermediate layer nonlinearity to be a sigmoid function, but other types of nonlinearities such as ReLu can be considered, which can have a different effect on the reformatted shape of the manifold. Intermediate layer invariance can be achieved by different methods such as max-pooling or polynomial nonlinearities, and we hope to explore the role of such processing on the reformatting of the manifolds using the manifold capacity framework in the future.
5.5 Kernel Extensions
In section 5.4, we showed how an additional intermediate layer with sparsity can improve the output readout capacity for manifolds. This is an example where a nonlinearity in each unit in the intermediate layer created a new representation that is easier to be read out by the output unit. In this section, we show another example of how nonlinear processing reformats the input manifolds so that the output linear separability is improved, by using kernels.
Traditionally, nonlinear kernels have been used in the SVM dual framework to allow for nonlinear classification of points9. Here we show that when input patterns are on manifolds, how nonlinear kernels achieve the non-linear classification can be analyzed as
145

improved manifold classification capacity of reformatted manifolds in the kernel feature space (Fig. 5.665). We also extend M 4 algorithm provided in Chapter 3 to show that an iterative method with the same principle can be used to find a kernel-SVM solution for manifold classification (kernel-M 4).

5.5.1 Manifold Capacity under a Quadratic Kernel

The effect of the kernel operation on the geometric properties of the manifolds depends

on the kernels. As a simple example of a non-linear kernel, we study the improved clas-

sification capacity of manifolds with quadratic kernels. We extend our theory to provide

upper and lower bounds for the classification capacity of manifolds with dimensionality

D in input space after their transformation to a quadratic feature space.

Consider arbitrary manifolds embedded in D-dimension, where each point in the

manifold can be expressed as x = x0 +

D i=1

siui,

where

x0

is

a

N -dimensional

center

vector and ui (i = 1, ..., D) are the basis vectors, and parametrized by s, where si cor-

responds to the i th basis vector. The feature space of a homogenous quadratic kernel

is {xixj, i  j} which has Nf = (N + 1)N/2 unique components. The feature space,

x0 +

D k=1

sk uk

i

x0 +

D l=1

slul

, can be expanded as x0ix0j+
j

k sk uki x0j + x0iukj +

D k=1

D l=1

sksluki ulj .

This

is

Nf -dimensional

vector

with

a

center

x0ix0j

and

(D+3)D/2

basis vectors. The basis vectors consist of the two classes: ukul where there are

(D + 1)D/2 of them, and uki x0j + x0iukj where there are D of them. Therefore, for input space dimension N , the ambient dimension in this feature space is Nf = (N +1)N/2. On the other hand, the dimensionality of the manifolds in feature space is D(D + 3)/2.

In order to illustrate the effect of kernels on the learning of classifier of the manifolds,

we present 2 simple examples. Quadratic kernels applied to 1D lines and 2D circles.

The geometric illustration of how1D lines and 2D circles with radius R map from input

space to quadratic kernel's feature space is provided in Fig. 5.6(a),(c). For R = 0,

the (zero margin) capacity P increases from 2N to (N + 1)N , as given by the capacity

146

for classifying points. On the other hand, for R = , the weight vector has to be

orthogonal to all the dimensions spanned by the reformatted manifolds, yielding the

capacity

P

=

N (N +1) (1+D(D+3))

in

this

limit.

For an intermediate R, the capacity will be

affected by the extent and geometry of the manifolds in feature space. The predicted

bounds are compared with numerical simulations in Fig. 5.6. These considerations can

be easily generalized to polynomial kernels with higher degree.

Figure 5.6: Manifold Classification with a Quadratic Kernel. (a) Classification of Lines embedded in N -dim input space (black versus red, bottom) maps to 2D curves (black vs. blue, top) embedded in Nf = (N + 1)N/2 dim kernel feature space. (b) Line capacity in input space (red), and quadratic kernel space (blue), shown as P (number of manifolds) over Nf , and the bounds on the kernel capacity:  = 2 (R = 0) and  = 2/5(R = , D = 2) (dashed lines). (c) Classification of 2D circles (black v. red, bottom) maps to 5D manifolds (black vs. blue, top). (d) Manifold capacity of 2D circles in input space (red) and quadratic kernel space (blue), and the bounds on the kernel capacity:  = 2 (R = 0) and  = 2/11(R = , D = 5). In both the line and 2D circles, the manifold capacities are improved by the quadratic kernel operation.
5.5.2 Kernel-M 4 Algorithm
General Framework The Kernel-M 4 algorithm applies the same logic as the M 4
algorithm in the chapter 3, but in the dual SVM framework. The separating hyperplane is represented implicitly by dual coefficients  and point examples (xl, yl) and a bias b, and the field induced by an input x is given as l lylK (x, xl) + b. The Kernel-M 4 algorithm iteratively calls a quadratic optimization solver on a finite number of labeled examples in the dual framework. Given a current estimate of  and b , the algorithm
147

searches for the point on the manifolds with the worst margin. If the margin of the new point is worse than the previous estimate, the point set is augmented, i.e. the kernel matrix is increased by one column and row, and the dual SVM solver is run to update values for  and b. The pseudocode for kernel-M 4is given by Alg. 7.

Algorithm 7 Pseudocode for kernel-M 4

[,b] = function kernel-M4(K , S , )

Input: kernel type K , data manifold parameters S , tolerance

Initialize: ,b, 

t=0

while  > do

1. t = t + 1

2.t = maxl0

M l=1

l

-

1 2

M j=1

M k=1

j

k

yj

yk

K

(xj

,

xk)

for  < t and and

tT y = 0.

3. Compute b and htmin.

4.

Search

for

the

new

pattern

such

that

( ) htmin new
||w||

=

{mins,µyµ ( l lylK (xl, xµ(s)) + b)}. 5.  = (htmin)new - htmin
end

Output: , b

In general, finding the point with the worst margin in the kernel feature space may be hard as the convexity of the input manifolds may be lost by the nonlinear kernel operation. If the manifolds are given by finite sets of points, then, the search over all points to find the worst point can be performed, where each search is upper bounded by the number of examples. If the input manifolds are uncountable sets of points, where the complete parameterization for the shape f (s) = 0 is given, then the search for the worst point is limited to finding the worst s, and sometimes s may be found analytically. If the parameterization is not available, then one may need to find the worst point with a local search using a gradient, but if there is no estimate of the convex hull, this operation is not necessarily a convex problem, which may be investigated further in the future.
With certain manifolds and kernel functions, the worst point operation can still be
148

done efficiently. As an illustrative example, we demonstrate an example of the maximum
margin classification of line segments and 2D circles under a homogenous quadratic ker-
2
nel, K (xj, xk) = xTj xk . For these examples, we can reduce the worst point operation to a finite set of analytical solutions, which is as efficient as regularM 4operation. This
computation can also be generalized to D-dimensional balls.

D-dimensional Balls and Quadratic Kernel M 4
Here we show an example of quadratic kernel-M 4with D-dimensional balls with radius R. In this case, finding the smallest distance to the solution plane from each point on the manifold in the kernel space is:





M

argmin h(s) = argmin yµ  jyjK (xj, x(s)) + b

s,fµ(s)=0

s,fµ(s)=0

j=1

(5.35)

where x = xµ0 +

D i=1

siuµi ,

and

fµ(s)

=

0

is

the

shape

constraint

of

the

µth

ball.

Now, the closet point in the D-dimensional ball manifold µ can be found by consid-

ering





 





D

D

s = argmins yµ 

lylK xl, xµ0 + R

sjuµj  + b +  

 s2j - 1  (5.36)

l=1

j=1

j=1



This is in general hard, but analytically solvable for quadratic kernel and a ball. For a homogenous quadratic kernel K(xn, xm) = xTn xm 2, we solve for s by taking a derivative of h(s).
The worst point s on the µth D-dimensional balls are found to be



sµj

 = Aµj +

D
Bjµj sµj k

j =1

149

where


Aµj = yµ lyl
l=1

xTl xµ0

xTl uµj



Bjµj = Ryµ

lyl xTl uµj

l=1

xTl uµj

with normalization on s = 1, and j, j = {1, ..., D}, and  , y are given. l = 1, ..., 

is an index of all the training points added so far,

And for line segments, the worst point s on the µ th line is

aµ + bµ (sµ) =  (sµ)

where


a = yµ lyl
l=1

xTl xµ0

xTl uµ



b = Ryµ

lyl xTl uµ

l=1

with normalization on s = 1.

xTl uµ

In general, the solution for s has two classes. One is where the closest point to the

hyperplane in the kernel space comes from the interior area of the convex hulls of the

manifold in the input space ( = 0 ), and the other is when s comes from the convex

hull in the input space (nonzero ). Generally, one can solve for both cases, and check

which s gives a smaller field, by computing Eqn. 5.35 for each µ and do this over all

manifolds, and with P candidate points, find the smallest one again. This is the step 4

of Alg. 7, for D-dimensional balls.

150

5.5.3 Discussion
In this section, we showed how the role of nonlinear kernel on data manifolds with small manifold capacity (in other words, linearly non-separable manifolds) in the input layer can be viewed as reformatting them to increase the manifolds capacity in the nonlinear feature space, using quadratic kernels and simple 1D and 2D balls in the input space as examples. Which kernels are best suited for the classification problem depends on the types of the data, which, in our case, are manifold structures. We laid the ground here for future analysis with manifolds capacity in kernel feature space, by formulating the iterative algorithm for finding the max margin solution for manifolds with kernels and demonstrating the simple examples of 1D and 2D balls, as well as providing bounds on their manifold capacities in input and feature space. Enabled with our theory of capacity of general manifolds (Chapter 4), we hope to extend our manifold capacity analysis for more complex manifolds to study the role of nonlinear kernels.
5.6 Generalization Error
So far, we have considered different aspects and extensions of manifold classification capacity, mainly motivated by a linear classifier that achieves zero training error. However, another important aspect of the linear separability of data manifolds is the generalization error of a linear classifier. This is particularly relevant in more realistic settings, where the manifolds given for training are not full manifolds, but only a subset of the manifolds. An example of this would be when the training data are convex hulls of subsamples of underlying manifolds. Another relevant case is when there is noise in the input. By focusing on the distribution of fields arising from manifold structures, here we show how the our theory allows for the estimation of generalization error for the manifold classification problem.
151

5.6.1 Generalization Error for General Manifolds, Given Weights.
Exact analytical expression for the generalization error for general manifolds is complicated; furthermore, the error depends on the assumed sampling measure on the manifold (whereas the separability problem is measure invariant). However, in the case of linearly separable manifolds with high D we can use the insight from the above theory (the notions of effective dimensions and radii) to derive a particularly simple approximation.
Assume we have obtained a set of weights from learning manifolds either from subsampling or from full unrealizable manifolds, so that we have a vector w . Then, the generalization error can be expressed as

µ G

=



-hµ0 -

sihµi

i

s

(5.37)

where h0are the field on the center x0and hiare the D fields on the basis vectorui,

generated by the trained w. The average over s is an integration with constraint f (s) = 0

. In other words,

F (s) s = dsp(s)(f (s))F (s)

(5.38)

and we assume dsp(s)(f (s))=1 . An important point to note is that the generalization error is in general sensitive to the choice of measure to use in this average i.e., p(s) .
The dependence on the weight w is via the fields h0 and h. Depending on the learning rule used to generate w, the above fields induced by w in general will not be the same for all manifolds but will vary with a distribution P (h0, h) even if all manifolds have the same geometry. For example if w is trained by the max margin classification of subsampled manifolds, P (h0) will be the distribution described in Chapter 4 for finite point manifolds. Therefore, for a given w, the generalization error can be expressed as

152

a double averaging

G=

 -h0 - sihi
i

s h0,hi

(5.39)

5.6.2 Gaussian Approximation in High Dimensional Manifolds.

In high dimensional manifolds, we assume that hi are distributed as projections of gaussian w on the uis . Hence hi themselves are i.i.d. Gaussian distributed with norm 1. If we make this assumption we get,

In other words,

µ G

=

 (-hµ0 - z) z s

(5.40)

G=

H

h0 

h0

where the width of the distribution is roughly

(5.41)

2  s · s  RW2

(5.42)

in the crude approximation.

We might want to take into account the fact that the his are bounded so they are 
not unbounded Gaussians. We can use the mean width of the manifold RW DW (Fig.

5.7(a)) as a measure of the bound. Finally, we can estimate the generalization error to

be

G=

H

(

h0(w) RW

)

-

H 

 DW

1 - 2H DW

h0

(5.43)

where we added a normalization so that G = 0.5 for h0 = 0. Note that h0 is the

field from the center, and DW and RW are effective dimension and effective radius of a

manifold identified in Chapter 4. Intuitively, the generalization error shows the fraction

153

of a manifold on the wrong side of the hyperplane, which is approximated by the area under one end of the tail of a Gaussian distribution outside of the size of the center field induced by the solution w, divided by the area under the Gaussian distribution with tails cut at the size of the mean width from both end (approximately 1 in high DW regime). This relation is shown in the Figure 5.7.
The generalization error shows two regimes. In the case where full (underlying) manifolds are linearly separable with margin , Then the generalization error will eventually vanish as more samples per manifold , m , are presented during the training.
One thing to note is that as we increase the number of subsamples for training m, the size of h0 must grow, as the hyperplane is always outside the mean width of the subsampled manifolds. According to the extreme value theory, we expect the max
 distance between the manifold center and extreme tail to grow like 2logm, like the mth maximum value of samples of Gaussian iid distribution66. In the limit of large m , we obtain,



g(m)  H((m) +

2logm)



exp[- 2 m

log

m]

(5.44)

Interestingly, this decay is faster than the generic power law, g(m)  m-1 of gener-

alization bounds in linearly separable problem and reflects the presence of finite margin

of the entire manifold. This dependence is demonstrated in Figure 5.8 (a1-a2) using

ellipsoids.

In the case where the full manifolds are not separable and  is above the capacity, even

subsampled manifolds with m points are not necessarily separable. Because the solution 
hyperplane may intersect the sampled manifolds, h0 no longer scales like 2logm. Per-
haps for this reason, the m dependence of generalization error given inseparable under-

lying manifolds is more like conventional general power law, g(m) - g(m = )  m-1.

This is shown in 5.8 (b1-b2) using ellipsoids.

154

Figure 5.7: Generalization Error from Each Manifold. (a) The solution hyperplane (dashed line)

is determined by the training samples given by the manifold (Orange manifold), and the distance

between the center of the original manifold (assuming the center of the testing manifold and original

manifold is the same) and the solution hyperplane is given by h0 (along the direction of w ), the field induced by the testing manifold center. The sizeof the original (testing) manifold, along the direction of w , is approximated by the Mean Width, RW DW . (b) The generalization error is the fraction of samples on the correct side of the hyperplane out of the total samples (which is measure-dependent).

Assuming the projections of the manifold along w are Gaussian, we approximate generalization error

as ratio between the blue area and blue plus red area under the gaussian. The width of the gaussian

is estimated to be  = RW asa crude approximation (assuming hi's are Gaussians with norm 1), with the tails truncated atRW DW , and the distance between the peak and the location of the

hyperplane at h0 (the separation that  is 1, we get the expression

between correct and incorrect

G

H

(

h0
RW

 )-H( DW


)

.

H(- DW )-H( DW )

labels).

Rescaling the x axis such

155

5.6.3 Numerical Investigations
As a simple example, we computed the generalization error for a binary dense classification of P ellipsoids, where Ri (radius in i th embedding dimension) is sampled from an uniform distribution of U nif [0.5R0, 1.5R0], and centers and axes are random Gaussian distribution. From each ellipsoid, m training samples and mg test samples were sampled, so that siRi-1 is from a uniform spherical distribution. With these mP finite training samples, max margin solution was found using a standard slack-SVM solver, and generalization error was computed using mgP test samples. Using the centers of the ellipsoids and the max margin solution w, theoretical estimation of the generalization error was computed using Eqn. 5.43. We show the results of this simulation in Fig. 5.8.
156

Figure 5.8: Generalization Error of Ellipsoids Classification as a Function of Number of

Subsamples Per the generalization

Manifold (m ). error approaches

(a1-a2) In the regime zero as m is increased,

where at the

the full rate of

mG anifmo1ldexspa[-resep2aloragbmle],.

The manifolds used were D-dimensional ellipsoids with Ri  U nif [0.5R0, 1.5R0] and P = 10, N =

50, R0 = 1.00, D = 50. (b1-b2) In the regime where the full manifolds are not separable, the generalization error approaches at the rate of G - G(m = )  m-1 . The parameters for the ellipsoids

used for the simulation were P = 20, N = 50, R0 = 2.00, D = 50. In all simulations generalization

error was tested with mg = 1000 per manifold. The theoretical predictions for generalization error

patches the generalization error calculated by the simulation well in this regime.

We find that the Gaussian approximation of the generalization error works quite

well, and the estimation matches the generalization error using simulations. We find

that indeed in the separable case, the

G

is

close

to

1 m

 exp[- 2logm],

and

in

the

157

inseparable case, g(m) - g(m = ) is approximately 1/m.
5.6.4 Discussion
Here we demonstrated how the insights from the manifolds capacity theory can be used to compute the crude approximation of generalization error in the high embedding dimension, which works surprisingly well in the case of classification of ellipsoids. Clearly, we made some assumptions for the sake of the approximation (i.e. hi are Gaussians). The fully general replica theory of generalization error for manifolds will require considerations of the actual manifold geometry, and the measure of the samples on the manifold, p(s), and using the manifold-dependent distribution of fields h0 and hi from the replica theory.
5.7 Analysis of GoogLeNet Manifolds
So far, we have considered various extensions of the theory of manifold classification for the analysis and application to real data. In this section, we show how our manifold capacity theory can be applied to realistic data, by analyzing manifold representations in conventional deep networks as an illustrative example. In the recent years, the performance of the artificial systems for visual classification tasks has been focused on the generalization error of the final layer on the test dataset. However, the underlying goal for training such system is to create representations such that different objects are easy to distinguish from each other. This idea is closely related to our notion of manifold classification capacity. Using our theory, we analyze how data representations reformat across different layers of GoogLeNet67 , one of the widely used deep networks for a popular visual recognition task, ImageNet10 classification task. Using different object classes of ImageNet dataset as manifolds, we show how the quantities that contribute to the manifold classification capacity, i.e. effective dimension, effective radius, Gaussian
158

mean width, and various correlations, change across the hierarchy of the layers.
5.7.1 Methods
We defined an object manifold as convex hulls of training samples from different object classes from ImageNet classification task dataset10. ImageNet Dataset has 1000 object classes with roughly 1000 training samples in each object class. Here, we computed the center of mass of each class (such that µth manifold's center is xµ0 and mean of the centers are set to be the origin), and selected a small set of object classes (P ) such that their center-to-center overlap, or center-to-center correlation, (center = x^µ0 · x^0) is smaller than a threshold value. The correlation between centers of the image object manifolds are surprisingly high, and there is a tradeoff between a low threshold value  and the number of object classes P. In our simulation, we used P = 22 and center was roughly 0.3.
To study how ImageNet object manifolds reformat in a network that is guaranteed to achieve a high classification performance, we chose GoogLeNet67, a winner of ImageNet Large Scale Visual Recognition Challenge (ILSVRC). We used pre-trained GoogLeNet weights available via MatConvNet framework68 to extract and analyze the ImageNet Object Manifolds in different layers. As the network size is extremely large, we focused on the layers after the pooling layers. We randomly selected Nsub units from the total units from each layer, and computed RW , DW using the set of points (defined in Chapter 4, Eqns 4.106-4.107). We also computed correlation coefficients between the centers of manifolds in each layer, as well as the overlap between centers and their own axis (self center-axis correlation) and centers and axis of the rest of the object manifolds (cross center-axis correlation). In the case of center-axis correlation, because object manifolds are high dimensional and have different sizes of extents along different embedding dimensions, the overlap measures were scaled with the eigenvalue of the covariance matrix of the object manifold.
159

where

cc-c = x^µ0 · x^0 µ

csc-eluf =

µi ||µ||

x^µ0

·

u^µi

; µ = 1, ..., P i = 1, ..., D
i,µ

cccr-ouss =

µi ||µ||

x^0

·

u^µi

;  = 1, ..., P, µ = , i = 1, ..., D

where µi are the i th eigenvalue of the covariance matrix of the samples of theµ th manifold.

5.7.2 Results and Discussions
In the figure below, we show the summary of the results from the analysis described. We know from the perceptron capacity of random general manifolds, that effective dimension and effective radius need to be reduced in order to increase the manifold classification capacity. We also have an ongoing theoretical work indicating that correlations between manifolds reduce the effective ambient dimension of the data, resulting in the reduced critical number of manifolds that can be separated.

160

Figure 5.9: Analysis of Manifold Properties in Different Layers of Deep Networks. Using the ImageNet Dataset for different class as object manifolds, effective manifold properties were analyzed in different layers of GoogLeNet. (a) Effective Dimension DW and Effective Radius RW for different layers of GoogLeNet. (c) Half of Gaussian Mean Width (RW DW ) for different layers of GoogLeNet. (c) Correlation between manifold centers (d) Correlation between each manifold center and its axes, averaged over manifolds. (e) Correlation between each manifold center and the axes of the other manifolds, averaged over manifolds.
Interestingly, we find that the mean of the effective radius RW of ImageNet object manifolds decrease systematically while being processed by the layers of the deep network (Fig 5.9(a)). Particularly, the most dramatic improvement in RW appears in the early part of the processing. On the contrary, the mean effective dimension DW of ImageNet manifolds doesn't decrease as significantly (Fig 5.9(a)). The values of DW at different layers are quite close to 2logm throughout (where m is the number of training samples in each object class), and this may imply that the effective dimension of object manifolds is determined by the number of training samples of the data. Further investigations on what determines DW of the realistic data is ongoing work.
 Recall that RW DW , half of the Gaussian mean width, is directly related to the
161

 capacity of manifolds. There is a systematic drop in the size of RW DW across layers, where the most dramatic improvement is in the early stage and there is a consistent improvement in the rest of the layers (Fig 5.9(b)). Supposedly the utility of the deep structure of the network is increasing the manifold capacity, by reducing the Gaussian mean width.
 Note that in RW and RW DW , the best values are in the output (readout) layer, and the worst values are in the input (pixel) layer (Fig 5.9(a)-(b)). Center-center correlations (cc-c) also show systematic decrease across layers (Fig 5.9(c)), implying that deep network gradually decorrelates the centers of the object manifolds. Note that when there is more correlation between the centers, the total space spanned by the manifold centers will be more skewed (compared to spherical), and the effective ambient dimension spanned by the centers will be smaller, making the maximum critical number of manifolds smaller63. Therefore, the deep network works towards increasing the manifold capacity by reducing the correlation between the centers. Other types of correlations (such as center to axes, within the manifold, csc-eluf , or across manifolds, ccc-rouss) show not as strong trends across the layers, and we hope to address this further in the future.
5.7.3 Future Work
In the future, we hope to connect the relationship between the theory and the experimental manifold capacity, and take into account various types of classifications, such as classification with sparse labels in the object recognition limit. Many deep networks have similar computational building blocks (i.e. convolution, max pooling, ReLu, dropouts, sparsity, etc), and we hope to address the role of each computational building block in terms of how they change the data manifolds. The change in the manifold shapes and manifold capacity during the learning is also an important future direction, in order to study what different types of learning can achieve in each layer, with a quantitative
162

measure (manifold capacity). Finally, we hope to analyze the manifold representations in neural data, particularly in different areas of the brain (i.e. object representations in different stages of the visual hierarchy in the cortex2).

5.8 Appendix

5.8.1 Multi-Layer Networks with Sparse Intermediate Layer
In this section, we show the details for the numerical demonstrations shown in Section. 5.4.

Training Two-Layer Network with Sparse Intermediate Layer

Input layer activity is N dimensional and organized as manifolds such that they can be

described as zµin = x0µ + R

D i=1

siuµi



RN

where

subscript

'in'

denotes

input

layer,

xµ0,uµi ,s,R are from 2. We draw a set of i.i.d., random sparse binary labels {yiµ} (where

i = 1, ..., M , M being the dimensionality of the intermediate layer) with probability f

of being 1. The output of the i-th nodes in the intermediate layer is binary 0, 1 with

zi =  V(i) · zin + bi where (x) is Heaviside step function. The weight vector to the i th node,V(i) , and bias, bi are found as a max margin (SVM) solution for D-dimensional balls to the set of labeled pairs {zµin, yiµ}, using the same method as in Algorithm 1. In Fig. 5.5, the network size used was N = M = 500. Initial training data for the

SVM solver for spheres had m = 2D samples per manifold, and then more points were

added until the iteration converged (Algorithm 1). The output node is a linear readout

of the intermediate layer representation, zout = sign [w¯ · z¯ + bf ] . [We use overline to denote M -dim vectors.] The weight vector from second layer to the output node, w¯ ,

and bias, bf (where subscript f stands for final), is trained as an SVM solution to set of the labeled pairs, {zµ, yoµut}, with the task dense labels yoµut = ±1 with f = 0.5. For

163

details, see Algorithm 8.
Robustness to Input Noise
Once the two layered network is fully trained (without additive noise), we evaluate the probability of error in the output by adding additive full-rank gaussian noise (with standard deviation of noise) to each input node and measuring the output node classification error as a function of the variance of the noise. For 5.5 (c), about 100 samples per manifold (m) were used and number of trials was 5. For details, see Alg. 8.
Robustness to Noise in the Intermediate Layer
We test the robustness to the introduction of smooth sigmoidal units in the intermediate layer, by first evaluating the intermediate weight matrix V to generate a sparse binary representation in the intermediate layer. After training of the intermediate layer, we replace the Heaviside step function of the intermediate layer nodes with T (x) = 1 + e-x/T -1, which can be interpreted as the result of smoothing the binary function by stochastic noise. The readout weights are calculated as SVM solutions to the the densely labeled task pairs {zµ, yoµut} where zµ are the intermediate layer sigmoidal responses to sampled inputs. For 5.5(d), about 100 samples per manifold were used and number of trials was 5. The effect of the smoothness of the intermediate level on the classification performance of the binary output unit is measured as a function of the gain parameter T . For details, see Algorithm 8.
164

Algorithm 8 Pseudocode for Two-Layer Network for Invariant Manifolds Clas-
sification Initialize:
x0µ, u~µ  N orm(0, 1) ( µ = 1, ..., P ) [Sample centers and direction vectors] yoµut  sign {Unif(-1, 1)} ( µ = 1, ..., P ) [Sample dense labels for manifolds for output layer]

Train V (Input to Intermediate Layer): ski ,µ  Unif (-1, 1) and ||sk,µ|| = 1 i = 1...D,k = 1, ..., m, µ = 1...P . [Sample
m coefficient vectors from each manifold]
repeat: forj = 1, ..., M [for each node in intermediate layer] (yµ)(j)  P (yµ) = f (yµ - 1) + (1 - f )(yµ + 1). (µ = 1, ..., P ) [Sample
sparse labels for manifolds for intermediate layer] V(j), b(j) = MB4 (x0µ, u~µ, (yµ)(i) , R,m) [MB4 : M 4 algorithm for L2 balls, see
Chapter 3]
end

Train w (Intermediate to Output Layer):

ski ,µ  Unif (-1, 1) and ||sk,µ|| = 1 for all i, k, µ

zkin,µ =x0µ + R

D i=1

ski ,µuµi

for

all

µ, k

[Sample

m

points

on

each

manifold

(first

layer activity)]

zik,µ =  V(i) · zink,µ + bi for all i = 1, ..., M and k, µ [Intermediate layer

activity]

w¯, bf = svmsolver(z¯k,µ, yoku,µt) [Find SVM solution with Intermediate layer

activity and dense labels.]

Test Robustness to Input Noise (for noise)

ski ,µ  Unif (-1, 1) and ||sk,µ|| = 1 for all i, k, µ

zkin,µ =x0µ + R

D i=1

ski ,µuµi

+



,

where





N orm(0, noiseI)

for all µ, k

[Sample m points on each manifold with additive Gaussian noise (input layer

activity with noise)]

zik,µ =  V(i) · zink,µ + bi for all i, k, µ

zoku,µt = sign w¯ · z¯k,µ + bf

return: G =

1 4

zoku,µt - yoku,µt 2

k,µ,trials

Test Robustness to Noise in the Intermediate Layer (for T  0)

ski ,µ  unif (-1, 1) and ||sk,µ|| = 1 for all i, k, µ

zkin,µ =x0µ + R

D i=1

ski ,µ

uµi

for all µ, k

zik,µ = T layer]

V(i) · zink,µ + bi

for all i, k, µ [Use smooth sigmoid for intermediate 165

w¯, bf , margin = svmsolver(z¯k,µ, yoku,µt) [SVM solution for intermediate layer ac-

tivity and labels]

return: output  margin [Output margin with SVM solution]

Chapter 6 Appendix A: Symbols and Notations

6.1 Notations

N P D  R Ri  = P/N C G,0 L

Ambient Dimension for Data Number of Manifolds Embedding Dimension Margin Radius of a ball i th radius of an ellipsoid (i = 1, ...D for D dimensional ellipsoid) Load (Number of Manifolds, P /Ambient Dimension, N ) Critical capacity (general expression) Gardner's perceptron capacity for points Capacity for Line Segments

166

B ,B2 Bp E M iter || µ, p x x0 ui v v w b y V h0 hi h

Capacity for L2 Balls Capacity for Lp Balls Capacity for Ellipsoids Capacity for General Manifolds Capacity for General Manifolds M found via iterative algorithm Capacity for Parallel Manifolds Index of the µth (pth) manifold Point on a manifold Center of a manifold i th basis vector of a manifold (i = 1, ..., D) N -dimensional vector v for an arbitrary vector D-dimensinoal vector v for an arbitrary vector Solution of a linear separating problem. bias of a linear perceptron binary (±1) labels Volume of space of solutions field induced by a center of a manifold x0 field induced by the i th axis of a manifold ui field induced by a data point x
167

RE DE RM
DM
RW
DW
g, G t, T m p smin(v), s~(v) s E M W
Rc

Effective radius for an ellipsoid
Effective dimension for an ellispoid.
Effective manifold capacity radius for a general manifold (using selfconsistent equations)
Effective manifold capacity dimension for a general manifold (using self-consistent equations)
Effective manifold capacity radius for a general manifold (using max projections on Gaussian vectors, related to the mean width)
Effective manifold capacity dimension for a general manifold (using max projections on Gaussian vectors)
Generalization error
Training error
number of subsamples per manifold
Slack parameter for the pth manifold argmins · v
s,f (s)=0
s solved via self-consistent equations on z0 and s 
Excess margin for ellipsoids defined via RE DE 
Excess margin for general manifolds defined via RM DM Excess margin for general manifolds in scaling regime defined via
 RW DW Critical radius for phase transition
168

d

Embedding dimension per ambient dimension D/N



Overlap

between

the

solution

w

and

axes

ui,

1 D

D i=1

(w

·

ui)2

f

Sparsity of labels (Number of posive (or negative) labels/ Total num-

ber of labels)

int

Margin in the intermediate layer unit (if intermediate unit is like a

binary classifier)

m

Number of subsamples (training samples) per manifold

M

Intermediate layer size

6.2 Mathematical Conventions

v vp (x) X x, y xy |S| |x| Dx D (t)

L2 norm of a vector v Lp norm of a vector v

Heaviside step function

Average of a random variable X

Inner product between two vectors x, y

Hadamard (element-wise) product

Cardinality (number of elements) of a set S

Absolute value of a scalar x

Gaussian measure, 1

e-

1 2

x2

dx

2

Chi

distribution,

21-

D 2

(

D 2

)

tD-1e-

1 2

t2

169

H (x) v^ v

 x

Dz

=

1 2

 x

dze-

z2 2

v/||v||

||v||

170

Chapter 7
Appendix B: Gardner's Replica Theory of Isolated Points

Consider a perceptron with P input points xµ  RN and P labels yµ = ±1 , µ = 1, ..., P . Assume that each component ofxµ are Gaussian i.i.d. The weight plane w  RN needs to classify all the input points such that all points are at least  away from the solution hyperplane. For simplicity, consider the regime where the number of positive labels and negative labels are equal. [This regime is called "dense classification" regime, where sparsity is f = 0.5.] In this regime, optimal bias for maximum capacity is b = 0 by symmetry, so let us ignore the bias term b for now.
The problem is now to find w such that

hµ

=

yµwT xµ N

>



(7.1)

where  is a margin. hµ, which we refer to as a field from a pattern xµ is a measure
of distance between the pattern xµ and the solution hyperplane denoted by w. Note 
that the denominator N is introduced so that hµ does not grow with the network size 
N . In general, if w and x are random, then the scale of wT x is N . For now let us

171

consider  = 0 case. Gardner calculated V , which is the volume of solutions (weight vectors w), which
satisfy the classification constraint Eqn. 7.1 with  = 0. If V goes to 0, then there is no solution and the network is beyond capacity. In order to compute V , each component of wi needs to be integrated with a term that is 0 when the constraint is not satisfied, hence

V=

P
dwN (w2 - N ) (hµ)
µ=1

(7.2)

where  is a delta function,  is a Heaviside step fuction, and the norm on w,

(w2 - N ), is imposed to count w in the same direction only once. Using the expression

for the field (Eqn. 7.1), we get

V=

dwN (w2 - N )

P

yµwT xµ (  )

µ=1

N

(7.3)

where (hµ) is one if w is a separating solution, and zero if it is not a solution, as

the field (input to the (x)) will be negative.

Note that V involves a product of many random contributions. Products of indepen-

dent random numbers are known to possess distributions with long tails for which the

average and the most probable value are markedly different. However, the logarithm of

such quantities is a large sum of independent terms, hence is expected to have a Gaus-

sian distribution so that its average and the most probable value match. Therefore, the

most typical value of V is expected to be

Vtypical  exp [ logV ]

(7.4)

Therefore, in order to get the typical behavior, we are interested in the average of

172

logV 28. Hence, we need to calculate

log V xµ

(7.5)

which is the average of log V over the quenched distribution of patterns. We can do this by the following formula:

log V

Vn -1

= lim
n0

n

(7.6)

called "replica trick", originally developed to calculate quenched averages in the the-

ory of disordered solides. Let us then calculate the expectation of V n, which, in the case of a natural n, can be
expanded as

Vn =

n

Pn

dw(w2 - N )

(hµ) xµ

=1

µ

(7.7)

where  is an index for each one of the n replicas of the original system with the

same realization of random samples. We note that the question of how to go from a

natural n to the limit of n  0 is in general a hard problem, and more on this matter can be found in69.

We can replace  using the integral representation of the  function,

n
dw(w2 - N )
=1

 P,n
dhµ
 µ,

 -

P,n µ,

dh^ µ 2

e

[ih^ µ

hµ

-ih^µ wT N

] xµ yµ

xµ

C

(7.8)

173

But because  = 0 and h^ > 0, we can change the range of the integrals to

n
dw(w2 - N )
=1

 P,n
dhµ
0 µ,

 0

P,n µ,

dh^ µ 2

e

[ih^ µ

hµ -ih^µ wT N

] xµ yµ

xµ

C

(7.9)

(7.10)

Now average over the random patterns xµonly affect the term noted as C, that is,

P
C=
µ=1

n
dh^ µ
=1

dhµ

e · e n

-ih^µ wT xµyµ

N

-ih^µ hµ
N xµ

=1

(7.11)

Now change the product of exponentials to the sum of powers, and taking out terms that don't depend on xµ, we get

P
C=
µ=1

n
dh^ µ
=1

dh e · e 
µ

n =1

-ih^ µ

wT 

xµ yµ

N

xµ

-ih^µ hµ N

(7.12)

Since w is a vector, we expand the vector dot product wT xµ as sum of each vector's

elements, then

P
C=
µ=1

n
dh^ µ
=1

dhµ e

· e n
=1

-ih^ µ 

j wjxµj yµ

N

xµ

-ih^µ hµ N

Then the sum j in the power can be the product of exponentials, then

P
C=
µ=1

n
dh^ µ
=1

dhµ

-i
e
j

· e n=1h^µ wjxµj yµ

N

xµ

-ih^µ hµ N

(7.13) (7.14)

174

Becasue each element is independent, we can take out product over j

P
C=
µ=1

n
dh^ µ
=1

dhµ
j

-i
e

n=1h^µ wjxµj yµ N

· e -ih^µ hµ

xµj

N

(7.15)

Now use the fact that DxeixA = e-A2/2 , we get rid of x  Dx . Note that A2

is like

So we have:

(-yµ) h^µwj

2
=



N

h^µ wj

2
=

N

 h^µwj 2 N

(7.16)

P
C=
µ=1

n
dh^ µ
=1

which can be re-written as

N -( dhµ e
j=1

n
) =1

h^ µ

wj

2

· e 2N

-ih^µ hµ N

(7.17)

P
C=
µ=1

n
dh^ µ
=1

N-
dhµ e
j=1

· e n
,=1

h^µ h^µwjwj

2N

-ih^µ hµ N

Define

q

=

1 N

j wjwj which is the replica symmetric order parameter.

Define

P
C=
µ=1

n
dh^ µ
=1

dhµ

e-

1 2

n =1

·e n
=1

h^µ q-1

h^ µ

-ih^µ hµ N

X

(7.18) (7.19)

X (h^ µ )

=

e-

1 2

n ,=1

h^µ q-1h^µ

(7.20)

P n

C=

dhµ

µ=1 0 =1



n

dh^ µ X

(h^ µ

,

....)e

-ih^µ hµ N

0 =1

(Here we changed the orders of h and h^.)

(7.21)

175

By integrating out dh^, we get

P
C=
µ=1

n
dhµ
0 =1

X (h^ µ

=

hµ )

1 det

q

(7.22)

where we used the delta function identity, Eqn. 7.20,( X(h^µ = hµ)) and n-dimensional

Gaussian

integration

with

matrix

q

to

get

1 det q

term.

We

re-write

Eqn.

7.22

such

that:

which is:

P
C=

n

dhµ

X (h^ µ

=

hµ

)e-

1 2

log det q

µ=1 0 =1

(7.23)

where



P
C=



n

dhµ

[e-

1 2

µ=1 0 =1



]e  n
,=1

hµ q-1hµ

-

1 2

log

det

q

X

(7.24)

X

=

e-

1 2

n ,=1

hµ

q-1

hµ

(7.25)

Then, C can be simplied with X,

P
C=

n

dhµ

X e-

1 2

log

det

q

µ=1 0 =1

(7.26)

Now going back to V N , in Eqn. 7.9, is now (after the above calculations):

VN =

N

P

dw(w2 - N )

=1

µ=1

P

dhµ X

e-

1 2

log

det

q

0 =1

C

(7.27)

176

where X is given as Eqn. 7.25. We note that the term C has two parts

and

P
C(X) =

P
dhµ

e-

1 2

µ=1 0 =1

n ,=1

hµ q-1hµ

X

(7.28)

C (0)

=

e-

P 2

log det q

(7.29)

for later use.

Because

it's

an

integral

of

q

=

1 N

j wjwj = q(w) which is a complex function

of w, we write it in terms of integral of deltas of q. Intuitively, q is generally thought

of as a function of the overlaps between the solution of weights.

Now, we can re-write the V N as functions of q , by introducing

dq dq^ eiq^ -q N + j wjwj

(7.30)

And we get:

V n = dqdq^ e-iq q^ N eiq^

n
dw (w2 - N )

=1

j

e wjwj

-

P 2

log det qC(X)

where C(X) is defined in Eqn. 7.28. Let us use the replica symmetric ansatz

q = (1 - q) + q

(7.31) (7.32)
(7.33)

177

Now going back to evaluating C(X),

P
C(X) =



dhµ

[e-

1 2

µ=1 0 

where, given our ansatz (Eqn. 7.33),

] n
,=1

hµ q-1hµ

(7.34)

q-1

=

1 1 - q 

+

-q (1 - q) (1 + (n - 1)q)

(7.35)

where n is a dimension of the matrix. However, we are in the limit n  0, therefore

q-1

=

1

1 -

q 

+

-q (1 - q)2

With Eqn. 7.36, Eqn. 7.34 becomes

(7.36)

P
C(X) =



n

dhµ

[e-

1 2(1-q)

µ=1 0 =1

( ) ( ) ] 

hµ

2

+

q 2(1-q)2

 hµ 2

Let

us

introduce

h

=

h 1-q

(for

simplicity),

then

(7.37)

P
C(X) =



n

dhµ

[e-

1 2

µ=1 0 =1

( ) ( 

hµ

2

+

q 2(1-q)

 hµ)2 ]

(7.38)

Using Hubbard­Stratonovich transformation, eA2/2 =

 -

dt

e-

1 2

t2

+At

,

2

we

intro-

duce additional expansion,

P
C(X) =
µ=1

n
Dtµ



dhµ[e-

1 2

=1 0

( )  hµ 2+

q 1-q

tµ

 hµ ]

which, finally, can be simplified to

(7.39)

P

C(X) =

Dtµ

µ=1



dhµ[e-

1 2

(hµ)2+

0

] q
1-q

tµ hµ

n

(7.40)

178

In other words,

where Now

P
C(X) =
µ=1

Dtµ elog Z(tµ,q) n

Z(tµ, q) =

 dh 

[e-

1 2

h2

+

0 2

] q
1-q

tµh

C(X) =

P
Dt en log Z(t,q)

And by simple power expansion,

(7.41) (7.42) (7.43)

C(X) = eP log(1+n log Z(t,q) ) Expanding log, we get:

(7.44)

C(X) = eP n log Z(t,q)

(7.45)

Note, we are trying to evaluate

log V

Vn -1

= lim
n0

n

(7.46)

by using Eqn. 7.27 and which has C(X) (Eqn.7.28) and C(0) (Eqn. 7.29). We need to now evaluate C(0) with Eqn. 7.29.
Now, noting that logdetq is related to the sum of log of eigenvalues,

log det q = log l
l

(7.47)

where l, l = 1, ... are eigenvalues of q. With the replica symmetric ansatz of q

is given by Eqn. 7.33, the first eigenvalues are 1 + (n - 1)q and the rest of the (n - 1)

179

eigenvalues are 1 - q , Therefore

log

det

q

=

n

log

(1

-

q)

+

nq (1 - q)

(7.48)

where the linear term in n is dominant. Then,using Eqn. 7.48, C(0) from Eqn. 7.29

is now,

C(0) = exp

-1P 2

n

log

(1

-

q)

-

1 2

P nq (1 - q)

Now, back to the original equation.

(7.49)

V n = dqdq^ e-iq q^ N eiq^

n
dw (w2 - N )
=1
j wjwj C(0)C(X)

Expanding (w2 - N ) to an exponential form with a new variable :

V n = dqdq^

d


n
dwexp i(w2 - N )
=1


-iqq^N + iq^ wjwj C(0)C(X)

j



We change the definition of C(X) slightly, so that

C(X) = eP n log Z(t,q) = eNn log Z(t,q)

(7.50) (7.51)
(7.52) (7.53)
(7.54)

180

where

 = P/N
is the capacity we wish to get. Doing the integral over w gives the term:

e{-

N 2

log det[ +q^ ]}

And absorbing i into the new variable, that is, use   i, we get,

(7.55) (7.56)

where

V n = dqdq^ dexp [F (q, q^, )]

(7.57)

F

=

-N

+

q q^ N

-

N 2

log det [

+

q^ ]

+

N n

log Z(t, q)

(7.58)

Where the dependence is only on non-local variables like N , P , n  0. Let us now

evaluate the integral using the saddle point approximation,

I=

dxexp [-g(x)]  exp [-g(x0)]

2 g (x0)

(7.59)

whereg(x) is at its minimum at x0 . In the limit of N  , we can use the following ansatz assuming replica symmetry,

q = (1 - q) + q

(7.60)

q^ = (q^0 - q^1)  + q^1

(7.61)

where we denote q^0 - q^1 as q^, and find the saddle point by taking the derivative of

181

F

F 

=0

:

F q^

=0:

1

=



1 + q^

-

(

q^1 + q^)2

q = (1 - q)  + q =

1  + q^



-

(

q^1 + q^)2

Thus, we get

(7.62) (7.63)

q

=

(

-q^1 + q^)2

(7.64)

Note that doing saddle points with  and q^ are easier, and we did these operations

first. Then, from Eqn. 7.62, we obtain

1

=



1 + q^

+

q

(7.65)

If we go back to the original equation, Eqn. 7.57, and plug in back the saddle points,

then, we get

N n [( + q^) - log ( + q^)] + N n log Z(t, q) 2
which eventually leads us to the expression for the term

(7.66)

where

V n = eN n[G0(q)+G1(q)]

(7.67)

182

G0(q)

=

1 2

1 1-q

+

log (1 - q)

(7.68)

G1(q) =

log

 dh

e-

1 2

0 2

h-

= q
1-q

t

2

t

log H(

Qt) t

(7.69)

H(

q 1-q

t)=H

 ( Qt)

Note that G0 is an entropic term and doesn't change with constraints, but for clas-

sification with additional constraints (such as manifolds classification), G1 does change 
and and computing log H( Qt) t becomes important.

Therefore, by L'Hospital's rule, we get in the limit of n  0,

log V = N [G0(q) + G1(q)] Note that G0 is an entropic term, and G1 needs to be self-consistent.

G0 q

=

q 2(1 - q)2

(7.70) (7.71)



G1 q

=



Q q

G1 Q

(7.72)

Now

plugging

the

expressions

back

to

the

self-consistency

requirement

(

G0 q

=



G1 q

)

we get:

(1 - q) 1= q

e-Qt2/2

2

 2H( Qt)

t

(7.73)

And the final step for capacity is we send q  1 (Q  ) because there is only one solution and in this limit the overlap is 1. With such limit, we get  = 2 , which is the capacity of isolated points for zero-margin solution.
Remarks. If  = 0 then q has to be zero from Eqn 7.73. Intuitively, this means that

183

as the number of points P approach zero (or the network size N approaches infinity compared to the number of points P ), the problem becomes very easy and there is a lot of solutions, making the overlap between solutions q go to 0.
184

References
[1] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097­1105, 2012.
[2] James J DiCarlo and David D Cox. Untangling invariant object recognition. Trends in cognitive sciences, 11(8):333­341, 2007.
[3] Mattia Rigotti, Omri Barak, Melissa R Warden, Xiao-Jing Wang, Nathaniel D Daw, Earl K Miller, and Stefano Fusi. The importance of mixed selectivity in complex cognitive tasks. Nature, 497(7451):585­590, 2013.
[4] Baktash Babadi and Haim Sompolinsky. Sparseness and expansion in sensory representations. Neuron, 83(5):1213­1226, 2014.
[5] Leslie G Valiant. The hippocampus as a stable memory allocator for cortex. Neural computation, 24(11):2873­2899, 2012.
[6] Thomas M Cover. Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition. IEEE transactions on electronic computers, (3):326­334, 1965.
[7] Elizabeth Gardner. The space of interactions in neural network models. Journal of physics A: Mathematical and general, 21(1):257, 1988.
[8] E. Gardner. Maximum storage capacity in neural networks. EPL (Europhysics Letters), 4(4):481, 1987. URL http://stacks.iop.org/0295-5075/4/i=4/a= 016.
[9] Vladimir Vapnik. Statistical learning theory, volume 1. Wiley New York, 1998.
185

[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248­255. IEEE, 2009.
[11] Marino Pagan, Luke S Urban, Margot P Wohl, and Nicole C Rust. Signals in inferotemporal and perirhinal cortex suggest an untangling of visual target information. Nature neuroscience, 16(8):1132­1139, 2013.
[12] Alireza Alemi-Neissi, Federica Bianca Rosselli, and Davide Zoccolan. Multifeatural shape processing in rats engaged in invariant visual object recognition. Journal of Neuroscience, 33(14):5939­5956, 2013.
[13] Jennifer K Bizley and Yale E Cohen. The what, where and how of auditory-object perception. Nature Reviews Neuroscience, 14(10):693­707, 2013.
[14] Ethan M Meyers, Mia Borzello, Winrich A Freiwald, and Doris Tsao. Intelligent information loss: The coding of facial identity, head pose, and non-face information in the macaque face patch system. The Journal of Neuroscience, 35(18): 7069­7081, 2015.
[15] Rebecca F Schwarzlose, Jascha D Swisher, Sabin Dang, and Nancy Kanwisher. The distribution of category and location information across object-selective regions in human visual cortex. Proceedings of the National Academy of Sciences, 105(11):4447­4452, 2008.
[16] Jay A Gottfried. Central mechanisms of odour object perception. Nature Reviews Neuroscience, 11(9):628­641, 2010.
[17] Chou P Hung, Gabriel Kreiman, Tomaso Poggio, and James J DiCarlo. Fast readout of object identity from macaque inferior temporal cortex. Science, 310 (5749):863­866, 2005.
[18] Winrich A Freiwald and Doris Y Tsao. Functional compartmentalization and viewpoint generalization within the macaque face-processing system. Science, 330(6005):845­851, 2010.
[19] Charles F Cadieu, Ha Hong, Daniel LK Yamins, Nicolas Pinto, Diego Ardila, Ethan A Solomon, Najib J Majaj, and James J DiCarlo. Deep neural networks
186

rival the representation of primate it cortex for core visual object recognition. PLoS Comput Biol, 10(12):e1003963, 2014.
[20] Eucaly Kobatake and Keiji Tanaka. Neuronal selectivities to complex object features in the ventral visual pathway of the macaque cerebral cortex. Journal of neurophysiology, 71(3):856­867, 1994.
[21] Nicole C Rust and James J DiCarlo. Selectivity and tolerance ("invariance") both increase as visual information propagates from cortical area v4 to it. Journal of Neuroscience, 30(39):12978­12995, 2010.
[22] Marvin L Minsky and Seymour A Papert. Perceptrons - Expanded Edition: An Introduction to Computational Geometry. MIT press Boston, MA:, 1987.
[23] Elizabeth Gardner. Maximum storage capacity in neural networks. Europhysics Letters, 4(4):481, 1987.
[24] Thomas Serre, Lior Wolf, and Tomaso Poggio. Object recognition with features inspired by visual cortex. In Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 2, pages 994­1000. IEEE, 2005.
[25] Ian Goodfellow, Honglak Lee, Quoc V Le, Andrew Saxe, and Andrew Y Ng. Measuring invariances in deep networks. In Advances in neural information processing systems, pages 646­654, 2009.
[26] Marc Aurelio Ranzato, Fu Jie Huang, Y-Lan Boureau, and Yann LeCun. Unsupervised learning of invariant feature hierarchies with applications to object recognition. In Computer Vision and Pattern Recognition, 2007. CVPR'07. IEEE Conference on, pages 1­8. IEEE, 2007.
[27] Yoshua Bengio. Learning deep architectures for ai. Foundations and trends® in Machine Learning, 2(1):1­127, 2009.
[28] Andreas Engel, Christian Van den Broeck, and Christian Broeck. Statistical mechanics of learning. Cambridge University Press, 2001.
[29] Madhu Advani, Subhaneil Lahiri, and Surya Ganguli. Statistical mechanics of complex neural systems and high dimensional data. Journal of Statistical Mechanics: Theory and Experiment, 2013(03):P03014, 2013.
187

[30] Nicolas Brunel, Vincent Hakim, Philippe Isope, Jean-Pierre Nadal, and Boris Barbour. Optimal information storage and the distribution of synaptic weights: perceptron versus purkinje cell. Neuron, 43(5):745­757, 2004.
[31] Haim Sompolinsky, Naftali Tishby, and H Sebastian Seung. Learning from examples in large neural networks. Physical Review Letters, 65(13):1683, 1990.
[32] Manfred Opper and David Haussler. Generalization performance of bayes optimal classification algorithm for learning a perceptron. Physical Review Letters, 66(20): 2677, 1991.
[33] Ran Rubin, R´emi Monasson, and Haim Sompolinsky. Theory of spike timingbased neural classifiers. Physical review letters, 105(21):218102, 2010.
[34] Daniel J Amit, KYM Wong, and C Campbell. Perceptron learning with signconstrained weights. Journal of Physics A: Mathematical and General, 22(12): 2039, 1989.
[35] R´emi Monasson. Properties of neural networks storing spatially correlated patterns. Journal of Physics A: Mathematical and General, 25(13):3701, 1992.
[36] Laurence F Abbott and Thomas B Kepler. Universality in the space of interactions for network models. Journal of Physics A: Mathematical and General, 22(12): 2031, 1989.
[37] Mark De Berg, Marc Van Kreveld, Mark Overmars, and Otfried Cheong Schwarzkopf. Computational geometry. In Computational geometry, pages 1­17. Springer, 2000.
[38] Geoffrey E Hinton, Peter Dayan, and Michael Revow. Modeling the manifolds of images of handwritten digits. Neural Networks, IEEE Transactions on, 8(1): 65­74, 1997.
[39] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798­1828, 2013.
[40] Pratik Prabhanjan Brahma, Dapeng Wu, and Yiyuan She. Why deep learning works: A manifold disentanglement perspective. IEEE transactions on neural networks and learning systems, 27(10):1997­2008, 2016.
188

[41] Maximilian Riesenhuber and Tomaso Poggio. Hierarchical models of object recognition in cortex. Nature neuroscience, 2(11):1019­1025, 1999.
[42] Joshua B Tenenbaum et al. Mapping a manifold of perceptual observations. Advances in neural information processing systems, pages 682­688, 1998.
[43] Sam T Roweis and Lawrence K Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323­2326, 2000.
[44] Joshua B Tenenbaum, Vin De Silva, and John C Langford. A global geometric framework for nonlinear dimensionality reduction. science, 290(5500):2319­2323, 2000.
[45] Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. Neural computation, 15(6):1373­1396, 2003.
[46] Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. The Journal of Machine Learning Research, 7:2399­2434, 2006.
[47] Guillermo Canas, Tomaso Poggio, and Lorenzo Rosasco. Learning manifolds with k-means and k-flats. In Advances in Neural Information Processing Systems, pages 2465­2473, 2012.
[48] Fabio Anselmi, Joel Z Leibo, Lorenzo Rosasco, Jim Mutch, Andrea Tacchetti, and Tomaso Poggio. Unsupervised learning of invariant representations in hierarchical architectures. arXiv preprint arXiv:1311.4158, 2013.
[49] Patrice Y Simard, Yann Le Cun, and John S Denker. Memory-based character recognition using a transformation invariant metric. In Pattern Recognition, 1994. Vol. 2-Conference B: Computer Vision & Image Processing., Proceedings of the 12th IAPR International. Conference on, volume 2, pages 262­267. IEEE, 1994.
[50] Partha Niyogi, Federico Girosi, and Tomaso Poggio. Incorporating prior information in machine learning by creating virtual examples. Proceedings of the IEEE, 86(11):2196­2209, 1998.
[51] Bernhard Sch¨olkopf, Chris Burges, and Vladimir Vapnik. Incorporating invariances in support vector learning machines. In International Conference on Artificial Neural Networks, pages 47­52. Springer, 1996.
189

[52] Alex J Smola and Bernhard Sch¨olkopf. Learning with kernels. Citeseer, 1998.
[53] Yuh-Jye Lee and Olvi L Mangasarian. Rsvm: Reduced support vector machines. In Proceedings of the 2001 SIAM International Conference on Data Mining, pages 1­17. SIAM, 2001.
[54] Jigang Wang, Predrag Neskovic, and Leon N Cooper. Training data selection for support vector machines. In International Conference on Natural Computation, pages 554­564. Springer, 2005.
[55] Shu-Cherng Fang, Chih-Jen Lin, and Soon-Yi Wu. Solving quadratic semi-infinite programming problems by using relaxed cutting-plane scheme. Journal of computational and applied mathematics, 129(1):89­104, 2001.
[56] Kenneth O Kortanek and Hoon No. A central cutting plane algorithm for convex semi-infinite programming problems. SIAM Journal on Optimization, 3(4):901­ 918, 1993.
[57] Y Liu, Kok Lay Teo, and Soon-Yi Wu. A new quadratic semi-infinite programming algorithm based on dual parametrization. Journal of Global Optimization, 29(4): 401­413, 2004.
[58] Thorsten Joachims. Training linear svms in linear time. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 217­226. ACM, 2006.
[59] Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, and Yasemin Altun. Large margin methods for structured and interdependent output variables. Journal of machine learning research, 6(Sep):1453­1484, 2005.
[60] SueYeon Chung, Daniel D Lee, and Haim Sompolinsky. Linear readout of object manifolds. Physical Review E, 93(6):060301, 2016.
[61] Ariel Amir, Naomichi Hatano, and David R Nelson. Non-hermitian localization in biological networks. Physical Review E, 93(4):042310, 2016.
[62] Kanaka Rajan, LF Abbott, and Haim Sompolinsky. Stimulus-dependent suppression of chaos in recurrent neural networks. Physical Review E, 82(1):011903, 2010.
190

[63] Ashok Litwin-Kumar, Kameron Decker Harris, Richard Axel, Haim Sompolinsky, and LF Abbott. Optimal degrees of synaptic connectivity. Neuron, 93(5):1153­ 1164, 2017.
[64] Roman Vershynin. Estimation in high dimensions: a geometric perspective. In Sampling theory, a renaissance, pages 3­66. Springer, 2015.
[65] Christopher JC Burges. Geometry and invariance in kernel based methods. Advances in kernel methods--support vector learning, pages 89­116, 1999.
[66] Anton Bovier. Extreme values of random processes. Lecture Notes Technische Universit¨at Berlin, 2005.
[67] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1­9, 2015.
[68] Andrea Vedaldi and Karel Lenc. Matconvnet: Convolutional neural networks for matlab. In Proceedings of the 23rd ACM international conference on Multimedia, pages 689­692. ACM, 2015.
[69] Marc Mezard, Giorgio Parisi, Miguel Angel Virasoro, and David J Thouless. Spin glass theory and beyond. Physics Today, 41:109, 1988.
191

