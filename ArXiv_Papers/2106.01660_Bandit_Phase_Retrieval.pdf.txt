arXiv:2106.01660v2 [stat.ML] 4 Jun 2021

Bandit Phase Retrieval
Tor Lattimore and Botao Hao DeepMind, London
{lattimore,bhao}@deepmind.com

Abstract

We study a bandit version of phase retrieval where the learner chooses

actions (At)nt=1 in the d-dimensional unit ball and the expected reward is

At,  2 minimax

where   cumulative

Rd is an regret in

uthniksnporwobnlepmaraism~e(tdervne)c,tworh.icWh eimpprorvoveetshoant

the the

breegsrtekt nisow~n(db/ounnd) sabnyd

a factor of that this is

d. We also show that the minimax simple only achievable by an adaptive algorithm.

Our analysis shows that an apparently convincing heuristic for guessing lower

bounds can be misleading and that uniform bounds on the information ratio

for information-directed sampling [Russo and Van Roy, 2014] are not suffi-

cient for optimal regret.

1 Introduction
We study an instantiation of the low-rank bandit problem [Jun et al., 2019] that in the statistical setting is called phase retrieval. Although this model is interesting in its own right, our main focus is on the curious information structure of this problem and how it impacts algorithm design choices. Notably, we were not able to prove optimal regret for standard approaches based on optimism, Thompson sampling or even information-directed sampling. Instead, our algorithm is a variant of explorethen-commit with an adaptive exploration phase that learns to gain information at a faster rate than what is achievable with non-adaptive exploration.
Problem setting Let · be the standard euclidean norm and Bdr = {x  Rd : x  r} and Sdr-1 = {x  Rd : x = r}. At the start of the game the environment secretly chooses a vector   Sdr-1 with r  [0, 1] a constant that is known to the learner. The assumption that r is known can be relaxed at no cost (Section 8). The game then proceeds over n rounds. In round t the learner chooses an action At  Bd1 and observes a reward
Xt = At,  2 + t ,
where (t)nt=1 is a sequence of independent standard Gaussian random variables. As is standard in bandit problems, the conditional law of At should be chosen as a

1

(measurable) function of the previous actions (As)ts-=11 and rewards (Xs)ts-=11. The performance of a policy  is measured in terms of the expected regret,

n

Rn(, ) = max E

aBd1

t=1

a,  2 - At,  2

n

= r2 - E

At,  2 .

t=1

The minimax regret is Rn = supr[0,1] inf supSdr-1 Rn(, ), where the infimum is over all policies.
We also study the pure exploration setting, where at the end of the game the learner uses the observed data (At)nt=1 and (Xt)nt=1 to make a prediction A  Bd1 of the optimal action. The simple regret of policy  is

rn(, ) = max E a,  2 - A,  2 = r2 - E A,  2 .
aBd1
As expected, the minimax simple regret is rn = supr[0,1] inf supSdr-1 rn(, ).

Contributions Our main contribution is nearly matching upper and lower bounds on Rn. For the simple regret we provide a near-optimal upper bound anda lower bound showing that non-adaptive policies must be at least a factor of ( d) suboptimal. In all of the following, const is a universal non-negative constant that
may vary from one expression to the next.

Theorem 1. Rn  const d n log(n) log(d).

Theorem

2.

Rn



 const d n

whenever

d

is

larger

than

a

suitable

universal

constant and n is sufficiently large.

Theorem 3. rn  const d log(n) log(d)/n.
Theorem 4. Assume that n  d  8. Then there exists an r  [0, 1] such that for all policies  with (At)nt=1 independent of (Xt)nt=1, supSdr-1 rn(, )  const d3/n.

We also show that worst-case bounds on the information ratio for informationdirected sampling are not sufficient to achieve optimal regret. Our results suggest that the conjectured lower bounds for low-rank bandits [Jun et al., 2019, Lu et al., 2021] are not true and that existing upper bounds are loose. The same phenomenon may explain the gap between upper and lower bounds for bandit principle component analysis [Kotlowski and Neu, 2019], as we discuss in Section 8.

Notation The first n integers are [n] = {1, 2, . . . , n} and the standard basis vectors in Rd are e1, . . . , ed. The span of a collection of vectors is denoted by span(v1, . . . , vm) and the orthogonal complement of a linear subspace V  Rd is V  = {x  Rd : x, y = 0 for all y  V }. The mutual information between
random elements X and Y on the same probability space is I(X; Y ) and the
relative entropy between probability measures P and Q on the same measurable space is KL(P, Q). The dimension of a set   Rd is defined as the dimension of
the affine hull of .

2

2 Related work

Phase retrieval Phase retrieval is a classical problem in signal processing and
statistics [Candès et al., 2015b,a, Cai et al., 2016, Chen and Candès, 2017, Chen et al.,
2019, Sun et al., 2018]. These works are focused on learning  where the covariates (At)nt=1 are uncontrolled, either random or fixed design.

Linear bandits Our problem can be written as a stochastic linear bandit by

noticing that At,  2 = AtAt ,  , where the inner product between matrices on the right-hand side should be interpreted coordinate-wise and the action set

is {aa : a  Bd1}. There is an enormous literature on stochastic linear bandits [Auer, 2002, Dani et al., 2008, Rusmevichientong and Tsitsiklis, 2010, Chu et al.,

2b0o1u1n,dAobnbtahsei-Ymaidnkimoraixetreaglr.,et20o1f 1O].(dT2hins

reduction log(n)).

immediately

yields

an

upper

Low-rank bandits Low-rank bandits are a kind of linear bandit where the

environment is determined by an unknown matrix and the actions of the learner

are also matrices. Let E  Rd1×d2 and A  Rd1×d2. A low-rank bandit problem

over E and with actions A is characterised by a matrix   E. The learner plays actions At  A and the reward is Xt = At,  + t, where t is noise and the inner product between matrices is interpreted coordinate-wise. So far this

is nothing more than a complicated way of defining a linear bandit. The name

comes from the fact that in general elements of E are assumed to be low-rank.

The precise nature of the problem is determined by assumptions on E and the

action set A. Our setup is recovered by assuming that E = {xx : x  Sdr-1} and A = {xx : x  Bd1}.
Jun et al. [2019] assume that E consists of rank p matrices and A = {xy : x 

X,y that

theYr}egforertsiosmbeourneadseodnbayblOy~(b(odu1 n+dde2d)3s/e2ts pXn).

Rd1 and Y  Rd2 . They prove These results cannot be applied

directly to the phase retrieval bandit because of the product assumption on the

action set. Lu et al. [2021] retain the assumption that E consists of rank-p matrices,

but relax the product form of the action set (while also allowing for generalised

linear models). Relying only the regret can be bounded by

on mild O~((d1 +

db2o)u3n/2dedpnne)s.s

assumptions, they show that For the bandit phase retrieval

problem, regret for

db1an=didt2p=hadsearnedtripev=al1o,fsOo~(tdh3i/s2algno).ritBhomthyJieulndsetanalu. [p2p0e1r9]baonudndLuonetthael.

[2021] conjecture that their upper bounds are optimal. Our results show that this

is not true for this sub-problem, despite the fact that the heuristic argument used

by these authors holds in this case, as we explain in Section 3. We summarize

these comparisons in Table 1.

Some authors use a model where the noise is in the parameter rather than

additive, which means the reward is At, t with (t)nt=1 an independent and identically distributed sequence of low-rank matrices (with unknown distribution).

For example, Katariya et al. [2017b,a] and Trinh et al. [2020] assume that t is rank-1 almost surely and A = {eiej : 1  i, j  d}, which means the learner is

3

Table factors

1a:ndFourntivheersloawl c-roannsktabnat.ndNitostec,oltuhme nd,pp nis

the rank. We ignore lower bound derived

logarithmic by Lu et al.

[2021] does not apply to bandit phase retrieval because it makes use of the richer

structure of the more general model.

Upper bounds Abbasi-Yadkori et al. [2011] Jun et al. [2019], Lu et al. [2021] This work Lower bounds Lu et al. [2021] This work This work (non-adaptive learning)

Bandit phase retrieval

O(d2n)

O(d3/2

 n)



O(d n)

N/A (dn)
N/A

Low-rank bandits

O(d2

 n)

O(d3/2 pn)

N/A

 (dp n) (dn)
N/A

Pure exploration N/A N/A
O(d/n)
N/A 
(d/ n) (d3/2/n)

trying to identify the largest entry in a matrix.

Adversarial setting A similar problem has been studied in the adversarial

framework by Kotlowski and Neu [2019]. They assume that (t)nt=1 is a sequence

of vectors chosen in secret by an adversary at the start of the game and the

mleaorsnteOr~(odbsenr)v,ews hiAlet,thte

2. They design an best lower bound is

algorithm ( dn).

for

which

the

regret

is

at

3 Information-theoretic heuristics and informationdirected sampling

Jun et al. [2019, §5] argue by comparing the signal to noise ratios between linear

arentdrielvoawl-rsahnokuldbabnedliotswetrhabtoutnhdeedmibnyima(xd1r.5egrne)t.

for We

problems like bandit make this argument

phase a little

more formal and explain why it does not yield the right answer in this instance.

Suppose that  is sampled uniformly from Sdr-1 and the learner takes an action A  Bd1 and observes X = A,  2 +  with   N (0, 1). What is the information gained by the learner? A symmetry argument shows that all actions on the unit sphere have the same information gain, so let's just fix some A  Sd1-1. A Taylor series expansion yields the approximation

I(; Y ) = E[KL(PX|, PX )] 

1 2

E

(E[X|] - E[X])2

=

r4 2

d2

3 +

2d

-

1 d2



r4 d2

.

(1)

Note

that

d 2

log(n)

bits

are

needed

to

code



to

reasonable

accuracy.

So

if

we

pre-

sume that the rate of information learned stays the same throughout the learning

process, then over n rounds the learner can only obtain O(nr4/d2) bits by Eq. (1).

By setting r2 = d3/2 log(n)/n one could be led to believe that the learner cannot

identify the optimal direction and the regret would be (nr2) = (d3/2 n log(n)).

4

The main point is that the rate of information accumulated by a careful learner increases over time.

Information-directed sampling Our upper bound and the observation above
has an important implication. Suppose as above that  is sampled uniformly from Sdr-1 and let A be a possibly randomised action. Since the learner cannot know the realisation of  initially, her expected regret for any action a  Bd1 is (a) = r2 - E[ a,  2] = r2(1 - a 2/d)  r2(1 - 1/d). On the other hand, as outlined above, the information gain about  is about r4/d2. Together these
results show that the information ratio is bounded by



E[(A)]2 I(; X, A)

=

(d2) .

Since the entropy of a suitable approximation of the optimal action is about

d 2

log(n),

an

application

of

the

information

theoretic

analysis

by

Russo

and

Van

Roy

[2014] suggests that the Bayesian regret can be bounded by O(d3/2 n log(n)),

which is suboptimal. This time the problem is that we have used the worst-case

bound on the information ratio, without taking into account the possibility that

the information ratio might decrease over time. We should mention here that a

decreasing information ratio was exploited by Devraj et al. [2021] in a recent anal-

ysis of Thompson sampling for finite-armed bandits, but there the gain was less

dramatic (a logarithm of the number of arms) and no changes to the algorithm

were required.

4 Algorithm for bandit phase retrieval

We start by showing that Theorem 1 holds if the learner is given an action that is constant-factor optimal. In the next section we explain how such an action
can be identified with low regret. Our algorithm uses the explore-then-commit design principle, which is usually only sufficient for O(n2/3) regret. The reason we are able to obtain O(n1/2) regret is because of the curvature of the action set,
a property that was exploited in a similar way in online learning by Huang et al. [2017] and in partial monitoring by Kirschner et al. [2020].

Theorem 5. Suppose the learner is given an action Aw  Bd1 such that Aw,  2  r2 for some universal constant   (0, 1]. Then there exists a policy  for which the regret is at most Rn(, )  const ·d n log(n).

Proof. By choosing the sign of , assume without loss of generality that Aw,   r . Let

m = 4d n log(n)/r2

and

 = min

1 2

,

 4

.

If m  n, then the regret of any policy is upper bounded nr2  const d n log(n), so for the rest of the proof we assume that m < n. For the first m rounds the

5

policy cycles over the 2d actions {(1 - )Aw ± ek : k  [d]}. The constrained

least squares estimator of  based on the data collected over m rounds is

^ = arg min{L() :   Bdr and

Aw, 

  r } ,

(2)

where

L()

=

1 2

m t=1(Xt - At,  2)2. For the remaining n - m rounds the algo-

rithm plays At = A = ^/ ^ . Then,

 9 + d log(98m)

m

E

At, ^ -  2 At, ^ +  2

t=1





 4

2
E

m
At, ^ -  2

t=1



2  2

2

m 2d

-

1

E

^ -  2 .

where in Eq. (4) we used that for some k  [d],

(3) (by Corollary 11)
(4) (5)

At, ^ + 

= 

(1 - )Aw ± ek, ^ + 
2(1 - ) r - 2r 

 2

 (1 

- .

)

Aw, ^ +  -  ^ +  (by definition of

)

Eq. (5) follows because for any k  [d],

(1 - )Aw + ek, ^ -  2 = 2(1 - )2 Aw, ^ -  2 + 22 ek, ^ -  2
±1
 22 ek, ^ -  2 ,

which implies that

m
At, ^ -  2 

m 2d

d

22 ek, ^ -  2

 22

m 2d

-

1

t=1

k=1

Rearranging Eq. (5) and using the definition of  shows that

^ -  2 .

E

^ -  2



2 2 

2

1

m 2d

-

1



const

d2 log(m) m  2

.

Letting a = /  be the optimal action, the regret is bounded by

Rn(, )  m  2 + nE a,  2 - A,  2

= m  2 + nE a - A,  a + A, 

 m  2 + 2nE a - A,  

 m  2 + 4nE  - ^ 2

m



2

+

const

nd2 m

log(m)  2

 const d

(by Lemma 7) n log(n) .

6

5 Finding a constant-factor optimal action

To establish Theorem 1 we show there exists an algorithm that interacts with the
bandit for a random number of rounds and outputs an action Aw that with high probability satisfies Aw,  2  r2/64. Furthermore, the procedure suffers small regret in expectation.

Theorem 6. Let T be the random number of rounds that Algorithm 1 interacts with the bandit, which cannot be more than n, and let Aw  Sd1-1 be its output. Then,

1.

E[T

]



const

d2 r4

log(n) log(d).

2. With probability at least 1 - 1/n, either T = n or

Aw, 

2



r2 64

.

What is interesting about Algorithm 1 is that it uses what it has learned in early iterations to increase the statistical efficiency of its estimation.

1

  = r/ d and m =

8 4

log

2n2

2 do

3

sample v u n i f o r m l y from Sd1-1

4 p l a y At = v fo r m r o unds and compute a v e r a g e reward X¯

5 6

Elo=op{vuXn¯t}i l X¯   2

7 for k = 2 to d

8

 = 9(log(98) + 4 log(n)) and m =

64d2 kr4

and u =

wE w wE w

9 do

10

sample v u n i f o r ml y from span(E)  Sd1-1

11

p l a y At  {(u + v)/ 2, (u - v)/ 2, u} fo r 3m r o unds

12

f i n d l e a s t s q u a r e s e s t i m a t o r ^ c o n s t r a i n e d to Bd1

13 loop u n t i l v, ^ 2   2

14 E = E  { v, ^ v}

15 loop u n t i l wE w 2  r2/16

16 return Aw =

wE w wE w

Algorithm 1: The procedure operates in d iterations. The first iteration is implemented in Lines 1­5 and the remaining d - 1 iterations in Lines 7­15.

Proof. Note that the vectors u and v computed in each iteration are orthogonal, which means that (u + v)/ 2 = (u - v)/ 2 = v = 1. Hence the actions of the algorithm are always in Bd1. The main argument of the proof is based on an induction to show that with high probability when the execution of the algorithm
ends, there exists an s  {±1} such that for all w  E,

7

(a) w  E, w 2   2; and

(b) s w, 



[

1 2

w 2, 2 w 2].

We proceed in five steps. First, we prove that if the above holds and the algorithm halts before n rounds are over, then the vector returned is a suitable approximation of /  . Second, we upper bound the probability of certain bad events. In the third and fourth steps we prove the base case and induction step for (a) and (b). In the last step we bound the expected running time.

Step 1: Correctness Suppose that (a) and (b) above hold and the algorithm halts at the end of iteration k. Then,

Aw,  2 =

wE w wE w

, 

2



1 4

wE

w

2

r2 64

.

where the first inequality follows from orthogonality of w  E and (b) above. The second inequality follows from the stopping condition in Line 15 of Algorithm 1, (a) above and the definition of  . Part (2) of the theorem follows by showing that (a) and (b) above hold with probability at least 1 - 1/n.

Step 2: Failure events The algorithm computes some kind of estimator at the end of each do/loop. Since the algorithm cannot play more than n actions, the number of estimators computed is naively upper bounded by n. A union bound over all estimates and the concentration bounds in Lemma 8 and Corollary 11 show that with probability at least 1 - 1/n the following both hold:
· For all v sampled in the first iteration and corresponding average rewards X¯ ,

X¯ - v,  2 

2 log(2n2) m

=

2 2

,

(6)

where m is defined in Line 1 of Algorithm 1.

· Let D = (As)s be the actions played in the inner loop of some iteration k  2 and ^ be the corresponding least-squares estimator. Then,

a, ^ -  2 a, ^ +  2  9(log(98) + 4 log(n))  .

(7)

aD

We assume both of the above hold in all relevant iterations for the remainder.

Step 3: Base case The next step is to show that (a) and (b) hold with high

probability after the first iteration. Consider the operation of the algorithm in

the inner loop. After sampling v  Sd1-1, the algorithm plays v for m rounds and

computes halts and

the w

=avevragX¯e

reward. Let v be the last sampled . By the stopping condition in

action before Line 5, w 2

the =

iteration X¯   2.

8

Without loss of generality, we choose the sign of  so that v,   0. Then by Eq. (6),

w,  =

X¯ v, 



1 2

w

2, 2

w

2

.

This establishes the base case.

Step 4: Inductive step Assume that (a) and (b) above hold for E at the end of iteration k. Let u be the value computed in Line 8 of Algorithm 1. Then,

u,  =

wE w,  wE w 2



1 2

 w 2   2k .
wE





Let A = {(u + v)/ 2, (u - v)/ 2, v}, which is the set of actions played in the inner

loop of iteration k+1 after sampling v for the last time. Let ^ be the corresponding

least-squares estimate. We consider two cases. First, if u, ^ +   2| v, ^ +  |,

then by Eq. (7),

 m



a,  - ^ 2 a,  + ^ 2

aA



1 16

u + v,  - ^ 2

u, 

2+

1 16

u - v,  - ^ 2

u, 

2



1 8

v,  - ^

2

u, 

2



kr2 16d

v,  - ^

2.

Rearranging shows that

v,  - ^

2



16d mkr2



2 4

.

(8)

For the second case, u, ^ +   2| v, ^ +  |. Then,

 m



a,  - ^ 2

a,  + ^ 2



1 4

v, ^ - ^ 2

u, ^ + 



kr2 8d

v,  - ^ 2 .

aA

And again, Eq. (8) holds. Summarising, v, ^ is an estimator of v,  up to accuracy  /2. By the definition of the algorithm, the iteration only ends if | v, ^ |   . Therefore, with w = v, ^ v, we have w 2 = v, ^ 2   2. Furthermore, w,  = v, ^ v,   [ w 2/2, 2 w 2] . Therefore if (a) and (b) hold for E computed after iteration k, they also hold for E computed after iteration k + 1.

Step 5: Running time The length of an iteration is determined by the corresponding value of m and the number of samples of v. The former is an iterationdependent constant, while the latter depends principally on how many samples are needed before | v,  | is suitably large. The law of  is the uniform distribution

9

on Sd1-1  span(E), which is the uniform distribution on a sphere of dimension d-1-|E| embedded in Rd. The squared norm of the projection of  onto span(E) is

 2 -

, w 2 w2



r2

-4

w

2

r2 2

,

wE

wE

where we used (a) of the induction and the stopping condition in Line 15. Therefore, when v is sampled uniformly from Sd1-1  span(E), by Lemma 9,

P v,  2  3 2/2 = P

v, 

2



3r2 2d

 const > 0 ,

Furthermore, by the concentration analysis in the previous step, an iteration will end once a v has been sampled for which v,  2  3 2/2. Hence, the expected number of times the algorithm samples v per iteration is constant and a simple calculation using the definition of m in Lines 1 and 8 shows that the expected
number of rounds used by the algorithm is at most

E[T

]



const

d2 r4

log

(n)

log(d)

.

6 Proof of Theorem 1 and Theorem 3

Proof of Theorem 1. Run Algorithm 1 and if it halts, feed the returned action to the input of the explore-then-commit algorithm analysed in Theorem 5. Algorithm 1 fails to return suitable action with probability at most 1/n, so the contribution of this event to the regret is negligible. By Theorem 6, the regret incurred by Algorithm 1 is bounded by

T

E

r2 - At,  2

t=1

 r2E[T ]

 min

nr2,

const

d2 r4

log(d)

log(n)

 const d n log(d) log(n) .

Combining this with the regret bound established in Theorem 5 yields the result.

Proof of Theorem 3. We use a standard reduction [Lattimore and Szepesvári, 2020,
Chapter 33]. Let  be the policy used in the proof of Theorem 1 with A sampled uniformly from (At)nt=1. By Theorem 1,

rn(, )

=

1 n

nr2 - E

n
At,  2

t=1

=

Rn(, ) n



const d

log(n) log(d) n

.

10

7 Lower bounds

The proof of Theorem 2 is based on a direct analysis of the Bayesian regret via a change of measure argument. Compared to the usual arguments based on Pinsker's
inequality, the idea used here does not rely on any boundedness assumptions. What is important to show is that when  is sampled uniformly from Sdr-1, then for suitably small r the learner cannot learn much about . The proof of Theorem 4 makes use of Fano's inequality and can be found in Appendix C. The technique
used in the proof below also yields a lower bound that matches the upper bound in Theorem 3 up to logarithmic factors.

Proof of Theorem 2. Let r be a positive constant to be tuned subsequently and

 be the uniform (Haar) measure on Sdr-1. Let  be an arbitrary policy. Given   Rd, let P be the measure on interaction sequences induced by the interaction

between  and the bandit determined by . Let P =  P be the product of  and

probability kernel (P :   Sdr-1). respect to P and E the expectation

As usual, let E denote with respect to P. Let Zt

the =

expectation

t s=1

Xs

As,

with  2-

1 2

As, 

4.

Then,

E[ At,  41(Zt-1  )] =

E[ At,  41(Zt-1  )] d()

Sdr -1

=

E0

Sdr -1

At,  41(Zt-1  ) exp

-

1 2

t-1

(Xs -

As,  2)2 - Xs2

s=1

d()

=

E0 At,  41(Zt-1  ) exp(Zt-1) d()

Sdr -1

 exp()

E0

Sdr -1

At,  4

d()

=

3 exp()r4 d2 + 2d

.

Next, define a random time

Now,

t

 = min t : Zt   or

As,  4  1

s=1

with  = 1 + log(4) .

{  n} 


Xs
t=1

As, 

2-

3 2

As, 

4

-1



n
As,  41(  t)  1
t=1

.

By a union bound,

P(  n)  P



Xs

As, 

2-

3 2

As, 

4 -1

t=1

(A)

n

+P

As,  41(  t)  1 .

t=1

(B)

11

By the definition of Xs, Markov's inequality and the moment generating function of a Gaussian,

(A)  exp(1 - )E exp



Xs

As, 

2-

3 2

As, 

4

t=1

Meanwhile, by Markov's inequality,



exp(1

-

)

=

1 4

.

(B)  E

n
At,  41(  t)
t=1



6 exp()nr4 d2 + 2d

=

24 exp(1)nr4 d2 + 2d

=

1 4

,

where the last equality follows by choosing r2 = (d2 + 2d)/(96 exp(1)n). Hence, Sdr-1 P(  n) d() = P(  n)  1/2 and so there exists a   Sdr-1 such that

Suppose that

n

P

At,  4  1  1/2 .

t=1

n t=1

At, 

4



1,

then

by

Cauchy-Schwarz,

Therefore, P(

n
At,  2 
t=1

n



n At,  4  n .

t=1

n t=1

At, 

2



n)



1/2

and

n

Rn(, ) = E

r2 - At,  2

t=1

 P

n

At, 

2



 n



nr2

t=1 -n 2

.

(nr2

-

 n)

The result follows from choice of r.

8 Discussion

Unknown radius The assumption that r =  is known to the learner is

easily relaxed by estimating  . Note first that all our analysis holds with only

trivial

modifications

if

r



[

1 2



,



].

Next, if A is sampled uniformly from

Sd1-1 and X =

A, 

2+

and



is

a

standard

Gaussian,

then

E[X ]

=

1 d

 2

and V[X] = 1 + 2(d - 1)/(d3 + 2d2) = (1). Therefore  can be estimated

to within an arbitrary multiplicative factor and at confidence level 1 - 1/n using

const d2/  4 log(n) interactions with the bandit.

12

Computation complexity The only computational challenge is finding the least squares estimates, which is a non-convex optimisation problem. Candès et al. [2015b] proposed a Wirtinger flow algorithm that starts with a spectral initialization, and then refines this initial estimate using a local update like gradient descent. The computational complexity of the Wirtinger flow algorithm with -accuracy is O(nd2 log(1/)) where n is the number of samples.
Adversarial setting Kotlowski and Neu [2019] study the adversarial version of this problem, where the learner observes At, t 2 and (t)nt=1 is an adversarially sequence with t  Bd1 forall t. They prove an upper bound of Rn = O(d n log(n)) and a lower bound of ( dn). Natural attempts at improving the lower bound all fail. We believe that the upper bound is loose, but proving this remains delicate. No warm starting procedure will work anymore because the information gained may be useless in the presence of a change point. New ideas are needed.
Rank-p Perhaps the most natural open question is whether or not our analysis can be extended to the low rank bandit problem without our particular assumptions on the action set and environments matrices.
Principled algorithms Can optimism or information-directed sampling be made to work? The main challenge is to understand the sample paths of these algoritms before learning takes place.
References
Y. Abbasi-Yadkori, D. Pál, and Cs. Szepesvári. Improved algorithms for linear stochastic bandits. In Advances in Neural Information Processing Systems, pages 2312­2320, 2011.
P. Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine Learning Research, 3(Nov):397­422, 2002.
S. Boucheron, G. Lugosi, and P. Massart. Concentration inequalities: A nonasymptotic theory of independence. OUP Oxford, 2013.
T. Cai, X. Li, and Z. Ma. Optimal rates of convergence for noisy sparse phase retrieval via thresholded wirtinger flow. The Annals of Statistics, 44(5):2221­ 2251, 2016.
E. J. Candès, Y. C. Eldar, T. Strohmer, and V. Voroninski. Phase retrieval via matrix completion. SIAM review, 57(2):225­251, 2015a.
E. J. Candès, X. Li, and M. Soltanolkotabi. Phase retrieval via Wirtinger flow: Theory and algorithms. IEEE Transactions on Information Theory, 61(4):1985­ 2007, 2015b.
13

Y. Chen and E. J. Candès. Solving random quadratic systems of equations is nearly as easy as solving linear systems. Communications on pure and applied mathematics, 70(5):822­883, 2017.
Y. Chen, Y. Chi, J. Fan, and C. Ma. Gradient descent with random initialization: Fast global convergence for nonconvex phase retrieval. Mathematical Programming, 176(1):5­37, 2019.
W. Chu, L. Li, L. Reyzin, and R. Schapire. Contextual bandits with linear payoff functions. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pages 208­214, 2011.
V. Dani, T. Hayes, and S. Kakade. Stochastic linear optimization under bandit feedback. 2008.
S. Dasgupta and A. Gupta. An elementary proof of a theorem of johnson and lindenstrauss. Random Structures & Algorithms, 22(1):60­65, 2003.
A. M. Devraj, B. Van Roy, and K. Xu. A bit better? quantifying information for bandit learning. arXiv preprint arXiv:2102.09488, 2021.
S. Gerchinovitz, P. Ménard, and G. Stoltz. Fano's inequality for random variables. Statistical Science, 35(2):178­201, 2020.
R. Huang, T. Lattimore, A. György, and Cs. Szepesvári. Following the leader and fast rates in online linear prediction: Curved constraint sets and other regularities. Journal of Machine Learning Research, 18:1­31, 2017.
K-S Jun, R. Willett, S. Wright, and R. Nowak. Bilinear bandits with low-rank structure. In International Conference on Machine Learning, pages 3163­3172. PMLR, 2019.
S. Katariya, B. Kveton, Cs. Szepesvári, C. Vernade, and Z. Wen. Bernoulli rank1 bandits for click feedback. In Proceedings of the 26th International Joint Conference on Artificial Intelligence, pages 2001­2007, 2017a.
S. Katariya, B. Kveton, Cs. Szepesvári, C. Vernade, and Z. Wen. Stochastic rank-1 bandits. In Artificial Intelligence and Statistics, pages 392­401. PMLR, 2017b.
J. Kirschner, T. Lattimore, and A. Krause. Information directed sampling for linear partial monitoring. In Jacob Abernethy and Shivani Agarwal, editors, Proceedings of Thirty Third Conference on Learning Theory, volume 125 of Proceedings of Machine Learning Research, pages 2328­2369. PMLR, 2020.
W. Kotlowski and G. Neu. Bandit principal component analysis. In Alina Beygelzimer and Daniel Hsu, editors, Proceedings of the Thirty-Second Conference on Learning Theory, volume 99 of Proceedings of Machine Learning Research, pages 1994­2024, Phoenix, USA, 25­28 Jun 2019. PMLR.
T. Lattimore and Cs. Szepesvári. Bandit algorithms. Cambridge University Press, 2020.
14

B. Laurent and P. Massart. Adaptive estimation of a quadratic functional by model selection. Annals of Statistics, pages 1302­1338, 2000.
Y. Lu, A. Meisami, and A. Tewari. Low-rank generalized linear bandit problems. In International Conference on Artificial Intelligence and Statistics, pages 460­ 468. PMLR, 2021.
P. Rusmevichientong and J. N. Tsitsiklis. Linearly parameterized bandits. Mathematics of Operations Research, 35(2):395­411, 2010.
D. Russo and B. Van Roy. Learning to optimize via information-directed sampling. Operations Research, 66(1):230­252, 2018.
D. Russo and B. Van Roy. Learning to optimize via information-directed sampling. In Advances in Neural Information Processing Systems, pages 1583­1591. Curran Associates, Inc., 2014.
Ju Sun, Qing Qu, and John Wright. A geometric analysis of phase retrieval. Foundations of Computational Mathematics, 18(5):1131­1198, 2018.
C. Trinh, E. Kaufmann, C. Vernade, and R. Combes. Solving bernoulli rank-one bandits with unimodal thompson sampling. 31st International Conference on Algorithmic Learning Theory, pages 1­28, 2020.

A Technical lemma

Lemma 7 (Kirschner et al. 2020).

 

-

 

,



2 

 -  2.

Lemma 8 (Boucheron et al. 2013). Let (Xt)nt=1 be independent standard Gaussian random variables and (at)nt=1 be constants. Then,

P

1 n

n

atXt



2

n t=1

a2t

log(2/

)

n

 .

t=1

Lemma 9. Let V  Rd be a m-dimensional subspace and let X be sampled uniformly from Sd1-1  V . Then for all   V ,

P

X,  2 

2 m

 const > 0 .

Proof. Use the fact that if Z  Rm is a standard Gaussian, then

X, 

2

=d

Z1  Z

2

.

Then use standard concentration for the Gaussian and -squared distributions and naive union bounding [Laurent and Massart, 2000]. Alternatively, use the explicit form for the distribution of X in combination with elementary bounds on the regularised incomplete beta function.

15

B Ordinary least squares

Here we provide some routine results for least-squares estimation of . Suppose that (At)nt=1 are fixed and (t)nt=1 are independent 1-subgaussian random variables and Xt = At,  2 + t. The least-squares estimator of  constrained to   Bd1
is

^ = arg max L()
Bd

with

L()

=

arg max


1 2

n t=1

Xt -

At,  2

2.

The symmetry of the problem means that L() = L(-) for all   Rd, which means there is no hope that ^ might be close to . What is true is that for suitably exploratory (At), ^ is close to either  or -.
Theorem 10. Suppose that   Bd and ^ = arg min L(). Then, for any   (0, 1), with probability at least 1 - ,

P

n

^ - , At 2 ^ + , At 2  9 log

N1/(32n)() 

t=1

 ,

where N() = min{|C| : C  Rd, x  , minyC x - y  }.

Proof. Since   Bd by assumption, it follows that

0  L() - L(^)

=

-

1 2

n

n
At, ^ -  2 At, ^ +  2 + t At, ^ - 

t=1

t=1

At, ^ +  .

Let  = 1/(32n) and C  Rd be such that for all x   there exists a y  C such that x - y   and |C| = N(). Since At are fixed, by a union bound and standard Gaussian tail bounds, with probability at least 1 - |C|,

n
t At,  -  At,  +  
t=1

n
2 At,  -  2 At,  +  2 log
t=1

1 

.

16

On this event and letting   C be such that  - ^  . Then, with  =  - ^,

n
At, ^ -  2 At, ^ +  2 
t=1

n
8 At,  -  2 At,  +  2 log

1 

t=1

=

n
8

At,  + ^ -  2 At,  + ^ +  2 log

1 

t=1

n

8

At, ^ -  2 + 2 + 2

t=1

At, ^ +  2 + 4 + 2 log

1 

n
8

At, ^ -  2 At, ^ +  2 + 12 + 132 + 63 + 4 log

1 

t=1



n
8
t=1

At, ^ -  2 At, ^ +  2 + 32 log

1 



n
1 + 8 At, ^ -  2 At, ^ +  2
t=1

log

1 

,

where in the final inequality we chose  = 1/(32n). Solving for the left-hand side and naive simplification shows that

n
At, ^ -  2  9 log

1 

.

t=1

To summarise we have shown that with probability at least 1 - ,

n
At, ^ -  2 At, ^ +  2  9 log
t=1

|C| 

= 9 log

N() 

.

Standard results show that when   Bd1 has dimension k, then log N()  m log(3/). From this one obtains the following corollary:

Corollary 11. Under the same conditions as Theorem 10 and when   Bd1 and dim(span(A1, . . . , An)) = k:

n

(a) P

At, ^ -  2 At, ^ +  2  9 (log(1/) + k log(98n))  .

t=1

n

(b) E

At, ^ -  2 At, ^ +  2  9 (1 + k log(98n)).

t=1

17

C Proof of Theorem 4

Let  be a fixed policy and for   Rd let P be the measure on the sequence of outcomes Hn = (A1, X1, . . . , An, Xn) induced by the interaction between  and
the phase retrieval model determined by . Let E denote the expectation with
respect to P. Let r be a positive constant to be tuned subsequently and  be the uniform (Haar) measure on Sdr-1. Let Q = P d() be the Bayesian mixture measure. For   Rd, let E be the event given by

E =

A, 

2



3 4

r2

.

By Fano's inequality [Gerchinovitz et al., 2020, Lemma 5],

log 2 + P(E) d() 

Sdr-1 KL(P, Q) d() .

(9)

Sdr -1

- log Sdr-1 Q(E) d()

We now bound the numerator and denominator in Eq. (9) to show that the righthand side is at most 1/2 and then complete the proof using the definition of the regret and E.

Step 1: Bounding the denominator in Eq. (9) By exchanging the order of integrals in the denominator of Eq. (9), it follows that

- log

Q(E) d() = - log

Sdr -1

1E d() dQ .
Sdr -1

(10)

If U is sampled uniformly from Sd1-1, then by a concentration bound for spherical measures [Dasgupta and Gupta, 2003, Lemma 2.2],

P U12  /d  exp(-/4) for all  > 6 .

By

scaling

and

rotating

and

choosing



=

3 4

d,

it

follows

that

for

any

A



Bd1 ,

1
Sdr -1

A, 

2



3r2 4

d()  exp (-3d/16) .

Therefore, by Eq. (10),

- log

Q(E) d()
Sdr -1



3d 16

.

Step 2: Bounding the numerator in Eq. (9) By the convexity of KLdivergence,

KL(P, Q) d() =

KL P,

P d() d()

Sdr -1

Sdr -1

Sdr -1



KL (P, P) d() d() .

Sdr-1 Sdr-1

18

By the chain rule of KL-divergence,

n

KL (P, P) = E

KL (P(Yt = ·|At), P(Yt = ·|At)) .

t=1

A straightforward computation leads to

KL (P, P) = E

n

1 2

(At )2 - (At )2 2

t=1

=

1 2 E

n
(At )4 - 2(At )2(At )2 + (At )4
t=1

.

Since (At)nt=1 are independent of (Xt)nt=1, we can interchange the expectation and integral such that

KL(P, Q) d()
Sdr -1

n

E

(At )4 - 2(At )2(At )2 + (At )4 d() d() ,

t=1



where the expectation is with respect to (At)nt=1, which does not depend on  by assumption. When  is uniformly on Sdr-1 and A  Bd1 is arbitrary,

Sdr -1

At, 

4 d() =

3r4 d2 + 2d

and

Sdr -1

At, 

2 d() =

1 d2

,

where the expectation is taken with respect to . Therefore,

Sd-1

KL(P, Q) d()



3nr4 d2

.



(11)

Step 3: Lower bounding the regret Let r2 = d3/(32n) Combining the previous two steps shows that

P(E) d()
Sdr -1



16nr2 d3



1 2

.

Therefore there exists a   Sdr-1 with P(E)  1/2, which implies that

rn(, ) = r2 - E

A,  2



r2 8



const

d3/2 n

.

19

