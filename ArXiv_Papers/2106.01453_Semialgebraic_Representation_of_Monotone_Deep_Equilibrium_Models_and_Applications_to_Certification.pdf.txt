Semialgebraic Representation of Monotone Deep Equilibrium Models and Applications to Certification

arXiv:2106.01453v1 [math.OC] 2 Jun 2021

Tong Chen LAAS-CNRS Université de Toulouse 31400 Toulouse, France tchen@laas.fr
Victor Magron LAAS-CNRS Université de Toulouse 31400 Toulouse, France vmagron@laas.fr

Jean-Bernard Lasserre LAAS-CNRS & IMT Université de Toulouse 31400 Toulouse, France lasserre@laas.fr
Edouard Pauwels IRIT & IMT
Université de Toulouse 31400 Toulouse, France edouard.pauwels@irit.fr

Abstract
Deep equilibrium models are based on implicitly defined functional relations and have shown competitive performance compared with the traditional deep networks. Monotone operator equilibrium networks (monDEQ) retain interesting performance with additional theoretical guaranties. Existing certification tools for classical deep networks cannot directly be applied to monDEQs for which much fewer tools exist. We introduce a semialgebraic representation for ReLU based monDEQs which allows to approximate the corresponding input output relation by semidefinite programming (SDP). We present several applications to network certification and obtain SDP models for the following problems : robustness certification, Lipschitz constant estimation, ellipsoidal uncertainty propagation. We use these models to certify robustness of monDEQs w.r.t. a general Lq norm. Experimental results show that the proposed models outperform existing approaches for monDEQ certification. Furthermore, our investigations suggest that monDEQs are much more robust to L2 perturbations than L perturbations.
1 Introduction
With the increasing success of Deep Neural Networks (DNN) (e.g. computer vision, natural language processing), one witnesses a significant increase in size and complexity (topology and activation functions). This generates difficulties for theoretical analysis and a posteriori performance evaluation. This is problematic for applications where robustness issues are crucial, for example inverse problems (IP) in scientific computing. Indeed such IPs are notoriously ill-posed and as stressed in the March 2021 issue of SIAM News [1], "Yet DL has an Achilles' heel. Current implementations can be highly unstable, meaning that a certain small perturbation to the input of a trained neural network can cause substantial change in its output. This phenomenon is both a nuisance and a major concern for the safety and robustness of DL-based systems in critical applications--like healthcare--where reliable computations are essential". Indeed, the Instability Theorem [1] predicts unavoidable lower bound on Lipschitz contants, which may explain the lack of stability of some DNNs, over-performing on training sets. This underlines the need to evaluate precisely a posteriori critical indicators, such as Lipschitz constants of DNNs. However, obtaining an accurate upper bounds on the Lipschitz constant of a DNN is a hard problem, it reduces to certifying globally an inequality "(x)  0 for all x in
Preprint. Under review.

a domain", i.e., to provide a certificate of positivity for a function , which has no simple explicit expression. Even for modest size DNNs this task is practically challenging, previous successful attempts [7, 22] were restricted in practice to no more than two hidden layers with less than a hundred nodes. More broadly existing attempts to DNN certification rely either on zonotope calculus, linear programming (LP) or hierarchies of SDP based on positivity certificates from algebraic geometry [26], which may suffer from the curse of dimensionality.
Recently, Deep Equilibrium Models (DEQ) [2] have emerged as a potential alternative to classical DNNs. With their much simpler layer structure, they provide competitive results on machine learning benchmarks [2, 3]. The training of DEQs involves solving fix-point equations for which algorithmic success requires conditions. Fortunately, Monotone operator equilibrium network (monDEQ) introduced in [36] satisfies such conditions. Moreover, the authors in [25] provide explicit bounds on global Lipschitz constant of monDEQs (w.r.t. the L2-norm) which can be used for robustness certification.
From a certification point of view, DEQs have the definite advantage of being relatively small in size compared to DNNs and therefore potentially more amenable to sophisticated techniques (e.g. algebraic certificates of positivity) which rapidly face their limit even in modest size classical DNNs. Therefore monDEQs constitute a class of DNNs for which robustness certification, uncertainty propagation or Lipschicity could potentially be investigated in a more satisfactory way than classical networks. Contrary to DNNs, for which a variety of tools have been developped, certification of DEQ modeld is relatively open, the only available tool is the Lipschitz bound in [25].
Contribution
We present three general semialgebraic models of ReLU monDEQ for certification (p  Z+ {+}):
· Robustness Model for network Lp robustness certification.
· Lipschitz Model for network Lipschitz constant estimation with respect to any Lp norm.
· Ellipsoid Model for ellipsoidal outer-approximation of the image by the network of a polyhedra or an ellipsoid.
All these models can be used for robustness certification, a common task which we consider experimentally. Both Lipschitz and Ellipsoid models can in addition be used for further a posteriori analyses. Interestingly, all three models are given by solutions of semidefinite programs (SDP), obtained by Shor relaxation of a common semialgebraic representation of ReLU monDEQs. Our models are all evaluated to simple ReLU monDEQs on MNIST dataset similar as [36, 25] on the task of robustness certification. We demonstrate that all three models ourperform the approach of [25] and the Robustness Model being the most efficient. Our experiments also suggest that DEQs are much less robust to L perturbations than L2 perturbations, in contrast with classical DNNs [28].
Related works
Neural network certification is a challenging topic in machine learning, contributions include:
Robustness certification of DNNs Even with high test accuracy, DNNs are very sensitive to tiny input perturbations, see e.g. [33, 13]. Robustness to input perturbation has been investigated in many different works with various techniques, including SDP relaxation in SDP-cert [28], abstract interpretation with ERAN [12, 32], LP relaxation in Reluplex [19], analytical certification with Fast-lin [35] and CROWN [38], and their extension to convolutional neural networks with CNN-Cert [6]. All these methods are restricted to DNNs or CNNs and do not directly apply to DEQs.
Lipschitz constant estimation of DNNs Lipschitz constant of DNNs can be used for robustness certification [33, 15]. Existing contributions include naive layer-wise product bound [18, 34], the LP relaxations for CNNs [39] and DNNs [22], as well as SDP relaxations [27, 11, 7].
Lipschitz constant estimation of DEQs The authors in [25] provide an optimization-free upper bound of the Lipschitz constant of monDEQs depending on network weights. This bound is valid for L2 norm and we present a general model for arbitrary Lp norm.
2

2 Preliminary Background and Notations

We consider the monotone operator equilibrium network (monDEQ) [36], with a single implicit hidden layer. The main difference between monDEQ and deep equilibrium network (DEQ) [2] is that strong monotonicity is enforced on weight matrix and activation functions to guarantee the convergence of fixed point iterations. The authors in [36] proposed various structures of implicit layer, we only consider fully-connected layers, investigation of more advanced convolutional layers is in our list of future works.
Network description: Denote by F : Rp0  RK a fully-connected monDEQ for classification, where p0 is the input dimension and K is the number of labels. Let x0  Rp0 be the input variable and z  Rp be the variable in the implicit layer. We consider the ReLU activation function, which is simply defined as ReLU(x) = max{0, x}, and the output of the monDEQs can be written as

F (x0) = Cz + c, z = ReLU(Wz + Ux0 + u),

(monDEQ)

where W  Rp×p, U  Rp×p0 , u  Rp, C  RK×p, c  RK are parameters of the network. The vector-valued function F (x0) provides a score for each label i  {1, . . . , K} associated to the input x0, the prediction corresponds to the highest score, i.e., yx0 = arg maxi=1,...,K F (x0)i. As in [36], the matrix Ip - W is strongly monotone: there is a known m > 0 such that Ip - W mIp, this constraint can be enforced by specific parametrization of the matrix W. With the monotonicity assumption, the solution to equation z = ReLU(Wz + Ux0 + u) is unique and can be evaluated using convergent algorithms, see [36] for more details.
Robustness of monDEQs: Given an input x0  Rp0 , a norm · , and a network F : Rp0  RK , let y0 be the label of input x0, i.e., y0 = arg maxi=1,...,K F (x0)i. For  > 0, denote by E = B(x0, , · ) the ball centered at x0 with radius  for norm · . If for all inputs x  E, the label of x equals y0, i.e., y = arg maxi=1,...,K F (x)i = y0, then we say that the network F is -robust at input x0 for norm · . An equivalent way to verify whether the network F is -robust is to check that for all labels i = y0, F (x)i - F (x)y0 < 0.
Semialgebraicity of ReLU function: The key reason why neural networks with ReLU activation function can be tackled using polynomial optimization techniques is semialgebraicity of the ReLU function, i.e., it can be expressed with a system of polynomial (in)equalities. For x, y  R, we have y = ReLU(x) = max{0, x} if and only if y(y - x) = 0, y  x, y  0. For x, y  Rn, we denote by ReLU(x) the coordinate-wise evaluation of ReLU function, and by xy the coordinate-wise product of x and y. A subset of Rn defined by a finite conjunction of polynomial (in)equalities is called a basic closed semialgebraic set. The graph of the ReLU function is a basic closed semialgebraic set.
Going back to equation (monDEQ), we have the following equivalence:

z = ReLU(Wz + Ux0 + u)  z(z - Wz - Ux0 - u) = 0, z  Wz + Ux0 + u, z  0, (1)

where the right hand side is a system of polynomial (in)equalities. For the rest of the paper, mention of the ReLU function will refer to the equivalent polynomial system in (1).
POP and Putinar's positivity certificate: In general, a polynomial optimization problem (POP) has the form

 = max{f (x) : gi(x)  0, i = 1, . . . , p} ,
xRn

(POP0)

where f and gi are polynomials whose degree is denoted by deg. The robustness certification model (3.1), Lipischitz constant model (3.2) and ellipsoid model (3.3) are all POPs.

In most cases, the POPs are non-linear and non-convex problems, which makes them NP-hard. A typical approach to reduce the complexity of these problems is replacing the positivity constraints by Putinar's positivity certificate [26]. The problem (POP0) is equivalent to

 = min{ :  - f (x)  0, gi(x)  0, i = 1, . . . , p, x  Rn} .
R

(POP)

In order to reduce the size of the feasible set of problem (POP), we replace the positivity constraint - f (x)  0 by a weighted sum-of-square (SOS) polynomial decomposition, involving the polynomials gi. Let d be a non-negative integer. Denote by 0d(x), id(x) some SOS polynomials of degree at

3

most 2d, for each i = 1, . . . , p. Note that if d = 0, such polynomials are non-negative real numbers. Then the positivity of  - f (x) is implied by the following decomposition

p

 - f (x) = 0d(x) + id-i (x)gi(x) , i = deg gi/2 , x  Rn ,

(2)

i=1

for any d  maxi i. Equation (2) is called the order-d Putinar's certificate. By replacing the positivity constraint f (x) -   0 in problem (POP) by Putinar's certificate (2), we have for d  maxi i,

p

d

=

min{
R

:



-

f (x)

=

0d(x)

+

id-i (x)gi(x) , i = deg gi/2 , x  Rn} . (POP-d)

i=1

It is obvious that d   for all d  maxi i. Under certain conditions (slightly stronger than compactness of the set of constraints), it is shown that limd d =  [21]. The main advantage of relaxing problem (POP) to (POP-d) is that problem (POP-d) can be efficiently solved by semidefinite
programming (SDP). Indeed a polynomial f of degree at most 2d is SOS if and only if there exists a positive semidefinte (PSD) matrix M (called a Gram matrix) such that f (x) = v(x)T Mv(x), for all x  Rp, where v(x) is the vector of monomials of degree at most d.

Problem (POP-d) is also called the order-d Lasserre's relaxation. When the input polynomials are quadratic, the order-1 Lasserre's relaxation is also known as Shor's relaxation [31]. All our models are obtained using variations of Shor's relaxation applied to different POPs, see Section 3.3 for more details.

3 Semialgebraic Models for Certifying Robustness of Neural Networks

In this section, we introduce several models for certification of monDEQs. All the models are based on semialgebraicity of ReLU and ReLU (the subgradient of ReLU, see Section 3.2) to translate our targeted problems to POPs. Then we use Putinar's certificates, defined in Section 2 to relax the non-convex problems to convex SDPs which can be solved efficiently using modern solvers. Each model can be eventually used to certify robustness but they also have their own independent interest.

Notations: Throughout this section, we consider a monDEQ for classification, denoted by F , with

fixed, given parameters, W  Rp×p, U  Rp×p0 , u  Rp, C  RK×p, c  RK , where p0 is the

number of input neurons, p is the number of hidden neurons, and K is the number of labels. For

q  Z+  {+}, · Throughout this section

q

is >

the Lq norm defined by 0 and x0  Rp0 are fixed,

x we

q := ( denote

p0 i=1

|xi

|q

)1/q

by E := B(x0,

for ,

all x  Rp0 . · q) = {x 

Rp0 : x - x0 q  } the ball centered at x0 with radius  for Lq norm, a perturbation region. If

q < +, i.e., q is a positive integer, x - x0 q   is equivalent to the polynomial inequality

x - x0

q q



q; if q

=

,

x - x0 q   is equivalent to |x - x0|2 

2 (where |x| denotes

the vector of absolute values of coordinates of x) which is a system of p0 polynomial inequalities.

Hence the input set E is a semialgebraic set for all considered Lq norms. For a matrix A  Rm×n,

its operator norm induced by the norm · is given by |||A||| := inf{ : Ax   x , x  Rn}.

3.1 Robustness Model

Let y0 be the label of x0 and let z  Rp be the variables in the monDEQ implicit layer. The proposed
model directly estimates upper bounds on the gap between the score of label y0 and the score of labels different from y0. Precisely, for i  {1, . . . , K} such that i = y0, denote by i = (Ci,: -Cy0,:)T . For x  E, the gap between its score of label i and label y0 is F (x)i - F (x)y0 = iT z. The Robustness Model for monDEQ reads:

i

:=

max
xRp0 ,zRp

{iT

z

:

z

=

ReLU(Wz

+

Ux

+

u),

x



E}

.

(CertMON-i)

Using the semialgebraicity of both ReLU in (1), and set E, problem (CertMON-i) is a POP for all i.
As discussed in Section 2, one is able to derive a sequence of SDPs (Lasserre's relaxation) to obtain a converging serie of upper bounds of the optimal solution of (CertMON-i). For Robustness Model, we consider only the order-1 Lasserre's relaxation (Shor's relaxation), and denote by ~i the upper

4

bound of i by Shor's relaxation, i.e., i  ~i. Recall that if for all label i different from y0, we have F (x)i < F (x)y0 , then the label of x is still y0. This justifies the following claim:
Certification criterion: If ~i < 0 for all i = y0, then the network F is -robust at x0.
Robustness Model for DNNs has already been investigated in [28], where the authors also use Shor's relaxation as we do. Different from DNNs, we only have one implicit layer in monDEQ. Therefore, the number of variables in problem (CertMON-i) only depends on the number of input neurons p0 and hidden neurons p.

3.2 Lipschitz Model

We bound the Lipschitz constant of monDEQ with respect to input perturbation. Recall that the Lipschitz constant of the vector-valued function F (resp. z) w.r.t. the Lq norm and input ball S  E, denoted by LqF,S (resp. Lqz,S ), is the smallest value of L such that F (x) - F (y) q  L x - y q (resp. z(x) - z(y) q  L x - y q) for all x, y  S. For x, x0  S, with x - x0 q  , we can estimate the perturbation of the output as follows:

F (x) - F (x0) q  LqF,S · x - x0 q  LqF,S ,

(3)

F (x) - F (x0) q  |||C|||q · Lqz,S · x - x0 q  |||C|||q · Lqz,S .

(4)

The authors in [25] use inequality (4) with q = 2, as they provide an upper bound of L2z,S . In contrast, our model provides upper bounds on Lipschitz constants of F or z for arbitrary Lq norms. We directly focus on estimating the value of LqF,S instead of Lqz,S .

Since the ReLU function is non-smooth, we define its subgradient, denoted by ReLU, as the set-valued map ReLU(x) = 0 for x < 0, ReLU(x) = 1 for x > 0, and ReLU(x) = [0, 1] for x = 0. Similar to ReLU function, ReLU is also semialgebraic.

Semialgebraicity of ReLU: If x, y  R, we have y  ReLU(x), if and only if y(y-1)  0, xy  0, x(y - 1)  0. If x, y  Rn, then ReLU(x) denotes the coordinate-wise evaluation of ReLU. Going back to monDEQ, let s be any subgradient of the implicit variable: s  ReLU(Wz+Ux+u).
We can write equivalently a system of polynomial inequalities:

s(s - 1)  0, s(Wz + Ux + u)  0, (s - 1)(Wz + Ux + u)  0 .

(5)

For the following discussion, ReLU will refer to the equivalent polynomial systems (5).

With the semialgebraicity of ReLU in (1) and ReLU in (5), one is able to compute upper bounds of the Lipschitz constant of F via POPs. The proof of the next Lemma is postponed to Appendix A.1.

Lemma 1 Define L~qF,S = max{tT UT y : t, x  Rp0 , s, z, y, r  Rp, v, w  RK , x  S, t q  1, wT v  1, w q  1, r - WT y = CT v, y = diag(s) · r; s  ReLU(Wz + Ux + u), z = ReLU(Wz + Ux + u)} . (LipMON)
Then L~qF,S is an upper bound of the Lipschitz constant of F w.r.t. the Lq norm, i.e., LqF,S  L~qF,S .

Since problem (LipMON) is a POP, we also consider Shor's relaxation and denote by L^qF,S the upper bound of L~qF,S by Shor's relaxation, i.e., L~qF,S  L^qF,S . Define  := L^qF,S . By equations (3), (4), if E  S, using Lemma 1 and the fact that ·   · q, we have F (x) - F (x0)   , yielding the following criterion:
Certification criterion: Let y0 be the label of x0. Define  := F (x0)y0 - maxk=y0 F (x0)k. If 2 <  , then the network F is -robust at x0.
Remark: In order to avoid some possible numerical issues, we add some bound constraints and redundant constraints to problem (LipMON), see Appendix A.2 for details. Shor's relaxation of Lipschitz Model for DNNs has already been extensively investigated in [11, 7]. If one want to certify robustness for several input test examples, then one may choose S to be a big ball containing all such examples with an additional margin of .

5

3.3 Ellipsoid Model

As above, the input region E is a neighborhood of input x0  Rp0 with radius  for the Lq norm, i.e., E = {x  Rp : x - x0 q  }. More generally, E could be other general semialgebraic sets, such as polytopes or zonotopes. Denote by F (E) the image of E by F . In this section, we aim at finding a
semialgebraic set C such that F (E)  C. We choose C to be an ellipsoid which can in turn be used
for robustness certification. Our goal is to find such outer-approximation ellipsoid with minimum volume. Let C := {  RK : Q + b 2  1} be an ellipsoid in the output space RK parametrized by Q  SK and b  RK , where SK is the set of PSD matrices of size K × K. The problem of finding the minimum-volume ellipsoid containing the image F (E) can be formulated as

max {det(Q) : x  E, Q(Cz + c) + b 2  1, z = ReLU(Wz + Ux + u)} .
QSK ,bRK
(EllipMON-POP)

By semialgebraicity of both ReLU in (1) and C, problem (EllipMON-POP) can be cast as a POP
(the determinant being a polynomial). We no longer apply Shor's relaxation, we rather replace the non-negativity output constraint 1 - Q(Cz + c) + b 2  0 by a stronger Putinar's certificate related to both ReLU and input constraints. We can then relax the non-convex problem (EllipMON-POP) to
a problem with SOS constraints, which can be reformulated by (convex) SDP constraints. This is due to the fact that a polynomial f of degree at most 2d is SOS if and only if there exists a PSD matrix M (called a Gram matrix) such that f (x) = v(x)T Mv(x), for all x  Rp, with v(x) being the vector containing all monomials of degree at most d. In summary, we relax problem (EllipMON-POP) to an
SOS constrained problem keeping the determinant unchanged:

max {det(Q) : 1 -
QSK ,bRK

Q(Cz + c) + b

2 2

= 0(x, z) + 1(x, z)T gq(x - x0)

+  (x, z)T (z(z - Wz - Ux - u)) + 2(x, z)T (z - Wz - Ux - u) + 3(x, z)T z} . (EllipMON-SOS-d)

where gq(x) = q -

x

q q

for q

<

+ and gq(x) = 2 - |x|2

for q

=

+, 0 is a vector of SOS

polynomials of degree at most 2d, 1 is a vector of SOS polynomials of degree at most 2(d - q/2 )

for q < + and 2d - 2 for q = +, 2, 3 are vectors of SOS polynomials of degrees at most

2d - 2,  is a vector of polynomials of degree at most 2d - 2. Problem (EllipMON-SOS-d) provides

an ellipsoid feasible for (EllipMON-POP), that is an ellipsoid which contains F (E). In practice,

the determinant is replaced by a log-det objective because there exist efficient solver dedicated

to optimize such objectives on SDP constraints. By increasing the relaxation order d in problem

(EllipMON-SOS-d), one is able to obtain a hierarchy of log-det objected SDP problems for which

the outer-approximation ellipsoids have decreasing volumes.

In this paper, we only consider the case p = 2, , and order-1 relaxation (d = 1). Therefore, i and  are all (vectors of) real (non-negative) numbers for i = 1, 2, 3, except that 0 is an SOS polynomial of degree at most 2. In this case, problem (EllipMON-SOS-d) is equivalent to a problem with log-det
objective and SDP constraints, as the following lemma states (proof postponed to Appendix A.3):

Lemma 2 For p = 2 or p = , problem (EllipMON-SOS-d) with d = 1 is equivalent to

max

{log det(Q) : -M 0} .

QSK ,bRK ,1,2,30, Rp

(EllipMON-SDP)

where M  S(p0+p+1)×(p0+p+1) is a symmetric matrix parametrized by the decision variables (Q, b), the coefficients (1, 2, 3,  ), and the parameters of the network (W, U, u, C, c).

Since the outer-approximation ellipsoid C = {  RK : Q + b 2  1} contains the image F (E), i.e., all possible outputs of the input region E, one is able to certify robustness by solving the
following optimization problems.

Certification criterion: Let y0 be the label of x0. For i = y0, define i := maxRK {i - y0 : Q + b 2  1}. If i < 0 for all i = y0, then the network F is -robust at x0.

The certification criterion for Ellipsoid Model has a geometric explanation: for i = y0, denote by Pi the projection map from output space RK to its 2-dimensional subspace Ry0 × Ri, i.e.,
Pi() = [y0 , i]T for all   RK . Let Li be the line in subspace Ry0 × Ri defined by {[y0 , i]T  Ry0 × Ri : y0 = i}. Then the network F is -robust if and only if the projection Pi(C) lies strictly below the line Li for all i = y0. We give an explicit example in Section 4.3 to visually illustrate this.

6

3.4 Summary of the Models
We have already presented three models which can all be dedicated to certify robustness of neural networks. However, the size and complexity of each model are different. We summarize the number of variables in each model and the maximum size of PSD matrices in the resulting Shor's relaxation, see Table 1. The complexity of our models only depends on the number of neurons in the input layer and implicit layer. The size of PSD matrices is a limiting factor for SDP solvers, our models are practically restricted to network for which such size can be handled by SDP solvers. For the popular dataset MNIST [37], whose input dimension is 28 × 28 = 784, we are able to apply our model on monDEQs with moderate size implicit layers (87) and report the corresponding computation time.

Table 1: Summary of the number of variables the three models

Robustness Model Lipschitz Model

Ellipsoid Model

Num. of variables Max. size of PSD matrices

p0 + p 1 + p0 + p

2p0 + 4p + 2K p0 + 3p + K + K2

1 + 2p0 + 4p + 2K

1 + p0 + p

4 Experiments
In this section, we present the experimental results of Robustness Model, Lipschitz Model and Ellipsoid Model described in Section 3 for a pretrained monDEQ on MNIST dataset. The network we use consists of a fully-connected implicit layer with 87 neurons and we set its monotonicity parameter m to be 20. The training hyperparameters are set to be the same as in Table D1 of [36], where the training code (in Python) is available at https://github.com/locuslab/monotone_op_net. Training is based on the normalized MNIST database in [36], we use the same normalization setting on each test example with mean µ = 0.1307 and standard deviation  = 0.3081, which means that each input is an image of size 28 × 28 with entries varying from -0.42 to 2.82. And for every perturbation , we also take the normalization into account, i.e., we use the normalized perturbation / for each input.
Since all our three models can be applied to certify robustness of neural networks, we first compare the performance of each model in certification of the first 100 test MNIST examples. Then we compare the upper bounds of Lipschitz Model with the upper bounds proposed in [25]. Finally we show that Ellipsoid Model can also be applied for reachability analysis. For Certification model and Lipschitz model, we implement them in Julia [4] with JuMP [9] package; for Ellipsoid model, we implement it in Matlab [30] with CVX [14] package. For all the three models, we use Mosek [23] as a backend to solve the targeted POPs. All experiments are performed on a personal laptop with an Intel 8-Core i7-8665U CPU @ 1.90GHz Ubuntu 18.04.5 LTS, 32GB RAM. The code of all our models is available at https://github.com/NeurIPS2021Paper4075/SemiMonDEQ.
4.1 Robustness certification
We consider  = 0.1 for the L2 norm and  = 0.1, 0.05, 0.01 for the L norm. For each model, we compute the ratio of certified test examples among the first 100 test inputs. Following [25], we also compute the projected gradient descent (PGD) attack accuracy using Foolbox library [29], which indicates the ratio of non-successful attacks among our 100 inputs. Note that the ratio of certified examples should always be less or equal than the ratio of non-successful attacks. The gaps between them shows how many test examples there are for which we are neither able to certify robustness nor find adversarial attacks.
Remark: For Lipschitz Model, we use inequality (3) to test robustness, i.e., we compute directly the upper bound of the Lipschitz constant of F rather than z (seen as a function of x) where S is a big ball containing all test examples.
From Table 2, we see that the monDEQ is robust to all the 100 test examples for the L2 norm and  = 0.1 (the only example that we can not certify is because the label itself is wrong). However, it is not robust for the L norm at the same level of perturbation (all our three models cannot certify any examples, and the PGD algorithm finds adversarial examples for 85% of the inputs. The network
7

becomes robust again for the L norm when we reduce the perturbation  to 0.01. Overall, we see that Robustness Model is the best model as it provides the highest ratio, Ellipsoid Model is the second best model compared to Robustness Model, and Lipschitz Model provides the lowest ratio. As a trade-off, for each test example, Robustness Model requires to consider at most 9 optimization problems, each one being solved in around 150 seconds, while Ellipsoid Model requires to consider only one problem, which is solved in around 500 seconds. We only need to calculate one (global) Lipschitz constant, which takes around 1500 seconds, so that we are able to certify any number of inputs. Each model we propose provide better or equal certification accuracy compared to [25], and significant improvements for L perturbations.

Table 2: Ratio of certified test examples and running time per example by different methods. We consider L2 norm with  = 0.1 and L norm with  = 0.1, 0.05, 0.01. The ratio is based on the first 100 MNIST test examples, and we count the average computation time (with unit second) for one example of each method. The ratio in parentheses of the column "Lipschitz Model" are computed by the Lipschitz constant given in [25] (see Section 4.2 for details). Exact binomial 95% confidence intervals are given in bracket.

Norm



Robustness Model (1350s / example)

Lipschitz Model (1500s in total)

Ellipsoid Model (500s / example)

PGD Attack

L2 0.1

99% [>94]

91% (91% ) [>83]

99%[>94]

99%[>94]

0.1
L 0.05 0.01

0% [<4] 24% [16, 34] 99% [>94]

0% (0%) [<4] 0% (0%) [<4] 24% [16, 34] (0% [<4])

0%[<4] 0% [<4] 92% [>84]

15% [8, 24] 82% [73, 89] 99% [>94]

Figure 2 in Appendix A.4 shows the original image of the first test example (2a) and an adversarial attack (2b) for the L norm with  = 0.1 found by the PGD algorithm in [29].

4.2 Comparison with Lipschitz constants

In this section, we compare the upper bounds of Lipschitz constants computed by Lipschitz Model

with the upper bounds proposed in [25]. Notice that the upper bounds in [25] only involve the function

z(x), hence we are only able to use inequality (4) to test robustness. In fact the quantity |||C|||q · Lqz,S

can be regarded as an upper bound of LqF,S , the Lipschitz constant of F . We denote by UB2z the upper

bound of the Lipschitz constant of z w.r.t. the L2 norm, given by UB2z = |||U|||2/m according to [25],

where U is the parameter the upper bound w.r.t. the

of the network and L norm by UB z

m =

is pth0e·

monotonicity factor. UB2z where p0 is the

We can then compute input dimension. The

upper bound of Lipschitz constant of F is computed via the upper bound of z: UBqF = |||C|||q · UBqz.

Denote similarly by SemiUBqF the upper bounds of Lipschitz constants of F provided by Lipschitz

Model, w.r.t. the Lq norm.

Table 3: Comparison of upper bounds of Lipschitz constant for L2 and L norm, and the corresponding computation time (with unit second).

q=2 bound time (s)

q= bound time (s)

SemUBiUqFBqF

4.80 4.67

1756.58

824.14

-

108.84 1898.65

From Table 3, we see that Lipschitz Model provides consistently tighter upper bounds than the ones in [25]. Especially for L norm, the upper bound computed by |||C||| · UB z is rather crude compared to the bound obtained directly by Lipschitz Model. Therefore, we are able to certify more examples using SemiUBqF than UBqF , see Table 2.
8

4.3 Outer ellipsoid approximation
In this section, we provide a visible illustration of how Ellipsoid Model can be applied to certify robustness of neural networks. Recall that Ellipsoid Model computes a minimum-volume ellipsoid C = {  RK : Q + b 2  1} in the output space RK that contains the image of the input region E by F , i.e., F (E)  C. The certification criterion for Ellipsoid Model claims that the network F is -robust at input x0 if and only if the projection of C onto Ry0 × Ri lies below the line Li for all labels i = y0.
Take the first MNIST test example (which is classified as 7) for illustration. For  = 0.1, this example is certified to be robust for the L2 norm but not for the L norm. We show the landscape of the projections onto R7 × R3, i.e., the x-axis indicates label 7 and the y-axis indicates label 3. In Figure 1, the red points are projections of points in the image F (E), for E an L2 or L norm perturbation zone, the black circles are projections of some (successful and unsuccessful) adversarial examples found by the PGD algorithm. Notice that the adversarial examples also lie in the image F (E). The blue curve is the boundary of the projection of the outer-approximation ellipsoid (which is an ellipse), and the blue dashed line plays the role of a certification threshold. Figure 1a shows the landscape for the L2 norm, we see that the ellipse lies strictly below the threshold line, which means that for all points   C, we have 3 < 7. Hence for all   F (E), we also have 3 < 7. On the other hand, for the L norm, we see from Figure 1b that the threshold line crosses the ellipse, which means that we are not able to certify robustness of this example by Ellipsoid Model. Indeed, we can find adversarial examples with the PGD algorithm, as shown in Figure 1b by the black circles that lie above the threshold line. The visualization of one of the attack examples is shown in Figure 2 in Appendix A.4.

(a) Certified example for the L2 norm

(b) Non-certified example for the L norm

Figure 1: Visualization of the outer-approximation ellipsoids and outputs with  = 0.1 for L2 norm (left) and L norm (right). The red points are image of the input region, the blue curve is the ellipsoid we compute, the blue dashed line is the threshold line used for certifying robustness of inputs, and
the black circles are attack examples found by PGD algorithm.

5 Conclusion and Future Works
In this paper, we introduce semialgebraic representations of monDEQ and propose several POP models that are useful for certifying robustness, estimating Lipschitz constants and computing outerapproximation ellipsoids. For each model, there are several hierarchies of relaxations that allow us to improve the results by increasing the relaxation order. Even though we simply consider the order-1 relaxation, we obtain tighter upper bounds of Lipschitz constants compared to the results in [25]. Consequently, we are able to certify robustness of more examples.
Our models are based on SDP relaxation, hence requires an efficient SDP solver. However, the statof-the-art SDP solver Mosek (by interior-point method) can only handle PSD matrices of moderate size (smaller than 5000). This is the main limitation of our method if the dimension of the input gets larger. Moreover, we only consider the fully-connected monDEQ based on MNIST datasets for
9

illustration. One important and interesting future work is to generalize our model to single and multi convolutional monDEQ, and to other datasets such as CIFAR [20] and SVHN [24]. A convolutional layer can be regarded as a fully-connected layer with a larger sparse weight matrix. Hence one is able to build similar models via sparse polynomial optimization tools.
The authors in [25] showed that we can train DEQs with small Lipschitz constants for the L2 norm, by controlling the monotonicity of the weight matrix. This guarantees the robustness of monDEQ w.r.t. the L2 norm but not for the L norm. A natural investigation track is to adapt this training technique to the L norm with a better control of the associated Lipschitz constant.
Acknowledgments and Disclosure of Funding
This work has benefited from the Tremplin ERC Stg Grant ANR-18-ERC2-0004-01 (T-COPS project), the European Union's Horizon 2020 research and innovation programme under the Marie SklodowskaCurie Actions, grant agreement 813211 (POEMA) as well as from the AI Interdisciplinary Institute ANITI funding, through the French "Investing for the Future PIA3" program under the Grant agreement nANR-19-PI3A-0004. The third author was supported by the FMJH Program PGMO (EPICS project) and EDF, Thales, Orange et Criteo. The fourth author acknowledge the support of Air Force Office of Scientific Research, Air Force Material Command, USAF, under grant numbers FA9550-19-1-7026, FA9550-18-1-0226, and ANR MasDol.
References
[1] Vegard Antun, Nina M. Gottschling, Anders C. Hansen, and Ben Adcock. Deep learning in scientific computing: Understanding the instability mystery. SIAM NEWS MARCH 2021, 2021.
[2] Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. Deep equilibrium models. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.
[3] Shaojie Bai, Vladlen Koltun, and J. Zico Kolter. Multiscale deep equilibrium models. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 5238­5250. Curran Associates, Inc., 2020.
[4] Jeff Bezanson, Alan Edelman, Stefan Karpinski, and Viral B Shah. Julia: A fresh approach to numerical computing. SIAM review, 59(1):65­98, 2017.
[5] Jérôme Bolte and Edouard Pauwels. Conservative set valued fields, automatic differentiation, stochastic gradient methods and deep learning. Mathematical Programming, pages 1­33, 2020.
[6] Akhilan Boopathy, Tsui-Wei Weng, Pin-Yu Chen, Sijia Liu, and Luca Daniel. Cnn-cert: An efficient framework for certifying robustness of convolutional neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 3240­3247, 2019.
[7] Tong Chen, Jean B Lasserre, Victor Magron, and Edouard Pauwels. Semialgebraic optimization for lipschitz constants of relu networks. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 19189­19200. Curran Associates, Inc., 2020.
[8] F. H. Clarke. Optimization and Nonsmooth Analysis. Wiley New York, 1983.
[9] Iain Dunning, Joey Huchette, and Miles Lubin. Jump: A modeling language for mathematical optimization. SIAM review, 59(2):295­320, 2017.
[10] Mahyar Fazlyab, Manfred Morari, and George J. Pappas. Probabilistic verification and reachability analysis of neural networks via semidefinite programming. In 2019 IEEE 58th Conference on Decision and Control (CDC), pages 2726­2731, 2019.
[11] Mahyar Fazlyab, Alexander Robey, Hamed Hassani, Manfred Morari, and George Pappas. Efficient and accurate estimation of lipschitz constants for deep neural networks. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.
10

[12] Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, and Martin Vechev. Ai2: Safety and robustness certification of neural networks with abstract interpretation. In 2018 IEEE Symposium on Security and Privacy (SP), pages 3­18. IEEE, 2018.
[13] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In International Conference on Learning Representations, 2015.
[14] Michael Grant and Stephen Boyd. Cvx: Matlab software for disciplined convex programming, version 2.1, 2014.
[15] Matthias Hein and Maksym Andriushchenko. Formal guarantees on the robustness of a classifier against adversarial manipulation. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.
[16] Didier Henrion, Jean-Bernard Lasserre, and Johan Löfberg. Gloptipoly 3: moments, optimization and semidefinite programming. Optimization Methods & Software, 24(4-5):761­779, 2009.
[17] Haimin Hu, Mahyar Fazlyab, Manfred Morari, and George J. Pappas. Reach-sdp: Reachability analysis of closed-loop systems with neural network controllers via semidefinite programming. In 2020 59th IEEE Conference on Decision and Control (CDC), pages 5929­5934. IEEE, 2020.
[18] Todd Huster, Cho-Yu Jason Chiang, and Ritu Chadha. Limitations of the lipschitz constant as a defense against adversarial examples. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 16­29. Springer, 2018.
[19] Guy Katz, Clark Barrett, David L. Dill, Kyle Julian, and Mykel J Kochenderfer. Reluplex: An efficient smt solver for verifying deep neural networks. In International Conference on Computer Aided Verification, pages 97­117. Springer, 2017.
[20] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
[21] Jean B Lasserre. Global optimization with polynomials and the problem of moments. SIAM Journal on optimization, 11(3):796­817, 2001.
[22] Fabian Latorre, Paul Rolland, and Volkan Cevher. Lipschitz constant estimation of neural networks via sparse polynomial optimization. In International Conference on Learning Representations, 2020.
[23] ApS Mosek. The mosek optimization toolbox for matlab manual, 2015.
[24] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. 2011.
[25] Chirag Pabbaraju, Ezra Winston, and J. Zico Kolter. Estimating lipschitz constants of monotone deep equilibrium models. In International Conference on Learning Representations, 2021.
[26] Mihai Putinar. Positive polynomials on compact semi-algebraic sets. Indiana University Mathematics Journal, 42(3):969­984, 1993.
[27] Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial examples. In International Conference on Learning Representations, 2018.
[28] Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Semidefinite relaxations for certifying robustness to adversarial examples. In Advances in Neural Information Processing Systems, pages 10877­10887, 2018.
[29] Jonas Rauber, Roland Zimmermann, Matthias Bethge, and Wieland Brendel. Foolbox native: Fast adversarial attacks to benchmark the robustness of machine learning models in pytorch, tensorflow, and jax. Journal of Open Source Software, 5(53):2607, 2020.
[30] Gaurav Sharma and Jos Martin. Matlab®: a language for parallel computing. International Journal of Parallel Programming, 37(1):3­36, 2009.
11

[31] Naum Z. Shor. Quadratic optimization problems. Soviet Journal of Computer and Systems Sciences, 25:1­11, 1987.
[32] Gagandeep Singh, Timon Gehr, Matthew Mirman, Markus Püschel, and Martin Vechev. Fast and effective robustness certification. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.
[33] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning Representations, 2014.
[34] Aladin Virmaux and Kevin Scaman. Lipschitz regularity of deep neural networks: analysis and efficient estimation. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.
[35] Lily Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel, Duane Boning, and Inderjit Dhillon. Towards fast computation of certified robustness for relu networks. In International Conference on Machine Learning, pages 5276­5285. PMLR, 2018.
[36] Ezra Winston and J. Zico Kolter. Monotone operator equilibrium networks. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 10718­10728. Curran Associates, Inc., 2020.
[37] LeCun Yann, Cortes Corinna, and Burges Christopher J. C. Mnist handwritten digit database. 2010. [ATT Labs Online].
[38] Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. Efficient neural network robustness certification with general activation functions. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.
[39] Dongmian Zou, Radu Balan, and Maneesh Singh. On lipschitz bounds of general convolutional neural networks. IEEE Transactions on Information Theory, 66(3):1738­1759, 2019.

A Appendix

A.1 Proof of Lemma 1

Definition 1 (Clarke's generalized Jacobian) [8] Let f : Rn  Rm be a locally Lipschitz vectorvalued function, denote by f any zero measure set such that f is differentiable outside f . For x / f , denote by Jf (x) the Jacobian matrix of f evaluated at x. For any x  Rn, the generalized Jacobian, or Clarke Jacobian, of f evaluated at x, denoted by JfC(x), is defined as the convex hull of all m × n matrices obtained as the limit of a sequence of the form Jf (xi) with xi  x and xi / f . Symbolically, one has
JfC (x) := conv{lim Jf (xi) : xi  x, xi / f }.

In order to estimate the Lipschitz constant LqF,S , we need the following lemma:

Lemma 3 Let F : Rp0  RK , x  Cz(x) be the fully-connected monDEQ. Its Lipschitz constant is upper bounded by the supremum of the operator norm of its generalized Jacobian, i.e., define

L¯qF,S :=

sup

{tT JT CT v : t q  1, wT v  1, w q  1, x  S} , (6)

t,xRp0 ,v,wRK ,JJzC (x)

then LqF,S  L¯qF,S .

12

Proof : Since z(x) = ReLU(Wz(x) + Ux + u) by definition of monDEQ, z(x) is Lipschitz

according to [25, Theorem 1]. Furthermore, z(x) is semialgebraic by the semialgebraicity of ReLU

in (1). Therefore, the Clarke Jacobian of z is conservative. Indeed by [8, Proposition 2.6.2], the

Clarke Jacobian is included in the product of subgradients of its coordinates which is a conservative

field by [5, Lemma 3, Theorems 2 and 3]. Since F = C  z, the mapping CJzC : x  CJ, where J  JzC, is conservative for F by [5, Lemma 5]. So it satisfies an integration formula along segments. Let x1, x2  E, and let  : [0, 1]  Rp0 be a parametrization of the segment defined

by (t) = x1 + t(x2 - x1) (which is absolutely continuous). For almost all t  [0, 1], we have

d dt

F

((t))

=

CJ

(t)

=

CJ(x2

-

x1)

for

all

J



JzC ((t)).

Let M = supxS,JJzC(x) |||CJ|||q be the supremum of the operator norm |||CJ|||q for all J  JzC (x)
and all x  S. We prove that M < +. Indeed, z(x) is Lipschitz, hence there exists N > 0 such that |||J|||q < N for all J  JzC (x) and all x  S. The value M is thus upper bounded by |||C|||qN .

Therefore, for almost all t  [0, 1],

d dt

F

(

(t))

q M

x2 - x1

q, and by integration,

1d

1d

F (x2) - F (x1) q =

F ((t))dt 

0 dt

q

0

F ((t)) dt

dt  M
q

x2 - x1

q,

(7)

which proves that LqF,S  M . Let us show that M = L¯qF,S . Fix x  Rp0 and J  JzC (x). By the definition of operator norm,

|||CJ|||q =

(CJ)T



q

=

max {
vRK

JT CT v

 q

:

v

 q



1}

= max {tT JT CT v :
tRp0 ,vRK

t q  1,

v

 q



1}

= max {tT JT CT v : t q  1, wT v  1, w q  1} ,

(8)

tRp0 ,v,wRK

where

·

 q

denotes

the

dual

norm

of

·

q defined by

v

 q

:=

supwRK {wT v

:

w q  1} for

all v  RK, and the first equality is due to the fact that the operator norm of matrix CJ induced by

norm

· q is equal to the operator norm of its transpose (CJ)T induced by the dual norm

·

 q

.

Indeed, by definition of operator norm and dual norm, we have

|||CJ|||q = sup { CJx q : x q  1} =

sup

{yT CJx :

x q  1,

y

 q

 1}

xRp0

xRp0 ,yRp

=

sup

{xT (CJ)T y :

x q  1,

y

 q

 1} =

sup {

(CJ)T y

 q

:

y

 q

 1}

xRp0 ,yRp

yRp

=

(CJ)T

 q

.

The quantity L¯qF,S is just the maximization of Equation (8) for all x  Rp0 and all J  JzC (x) and therefore equals M .

The function z is semialgebraic, and therefore, there exists a closed zero measure set z such that z is continuously differentiable on the complement of z. For any x  z, since z is C1 at x, we have JzC (x) = {Jz(x)} by definition of the Clarke Jacobian. Fix x  z arbitrary. According to the Corollary of Theorem 2.6.6, on page 75 of [8], we have

JzC (x)  conv{JRCeLU(Wz(x) + Ux + u) · JW C z(x)+Ux+u(x)}

= conv{JRCeLU(Wz(x) + Ux + u) · (W · Jz(x) + U)

= JRCeLU(Wz(x) + Ux + u) · (W · Jz(x) + U),

(9)

where the first inclusion is from the cited Corollary, the first equality is because z is C1 at x so that the chain rule applies, and the last one is because the Clarke Jacobian is convex.

Fix any any x¯  Rp0 , then by definition JzC (x¯) = conv{lim Jz(xi) : xi  x¯, i  +, xi / z}.
Let {xi}iN be a sequence not in z converging to x¯, for each xi / z, we have by (9) that Jz(xi)  JRCeLU(Wz(xi) + Uxi + u) · (W · Jz(xi) + U), i.e., there exists Yi  JRCeLU(Wz(xi) + Uxi + u) such that Jz(xi) = Yi(W · Jz(xi) + U). By [8, proposition 2.6.2 (b)], JRCeLU has closed graph. Therefore, by continuity of z, up to a subsequence, Yi  Y  JRCeLU(Wz(x¯) + Ux¯ + u) for i  +, which means

JzC (x¯)  {J : Y  JRCeLU(Wz(x¯) + Ux¯ + u), J = Y(WJ + U)} ,

(10)

13

for all x¯  Rp0 . Let Y  JRCeLU(Wz + Ux + u), since we have coordinate-wise applications of ReLU, we have that Y = diag(s) with s  ReLU(Wz + Ux + u). By equation (10), the right-hand side of equation (6) is upper bounded by

max

{tT JT CT v : t q  1, wT v  1, w q  1, x  S,

t,xRp0 ,s,zRp,v,wRK ,JRp×p0

s  ReLU(Wz + Ux + u), z = ReLU(Wz + Ux + u),

J = diag(s) · (W · J + U)} .

(LipMON-a)

Notice that in problem (LipMON-a), we have a matrix variable J of size p × p0, i.e., containing p × p0 many variables, which is too large for any SDP solvers. To reduce the size, we use the
vector-matrix product trick introduced in [36] to reduce the size of the unknown variables. From equation J = diag(s) · (W · J + U), we have J = (Ip - diag(s) · W)-1 · diag(s) · U. This inversion makes sense because of the strong monotonicity of Ip - W and the fact that all entries of s lie in [0, 1] [36, Proposition 1]. Hence

vT CJ = vT C · (Ip - diag(s) · W)-1 · diag(s) · U = rT · diag(s) · U ,

(11)

where rT = vT C·(Ip-diag(s)·W)-1, which means r-WT ·diag(s)·r = CT v. Set y = diag(s)·r and transpose both sides of equation (11), we have JT CT v = UT y with r - WT · y = CT v. We can then rewrite the objective function of (LipMON-a) as tT UT y, leading to the following equivalent
problem

max

{tT UT y : t q  1, wT v  1, w q  1, x  S,

t,xRp0 ,s,z,y,rRp,v,wRK

s  ReLU(Wz + Ux + u), z = ReLU(Wz + Ux + u),

r - WT y = CT v, y = diag(s) · r} .

(LipMON-b)

We have shown that (LipMON-b) is the right hand side of Equation (LipMON) in Lemma 1 and is an upper bound of the right hand side of Equation (6) in Lemma 3, i.e., L¯qF,S  L~qF,S .

A.2 Redundant Constraints of the Lipschitz Model

In order to avoid possible numerical issues of problem (LipMON), and to improve the bounds, we add some redundant constraints to it. For variables r and y. Note that r = (Ip - WT · diag(s))-1 · CT v,
hence r 2  (Ip - WT · diag(s))-1 2 · CT 2 · v 2. The operator norm of a matrix induced by L2 norm is its largest singular value. Hence the operator norm of (Ip - WT · diag(s))-1 is the smallest singular value of matrix Ip - WT · diag(s), which is smaller or equal than 1 from

the recent work [36]. In summary, we have r 2  |||C|||2 · v 2 and y 2  |||C|||2 · v 2. For

Lipschitz Model w.r.t. L2 norm, we have v 2  1; for Lipschitz Model w.r.t. L norm, we

have

v

 

=

v 1  1, thus

v 2

v 1  1. Therefore, for both L2 and L norm, we can

bound the L2 norm of variables r and y by |||C|||2. Moreover, we multiply the equality constraint

r - WT · y = CT v coordinate-wisely with variables s, z, y, r to produce redundant constraints and

improve the results. This strengthening technique is already included in the software Gloptipoly3

[16]. With all the discussion above, we now write the strengthened version of problem (LipMON-b)

as follows:

max

{tT UT y : t q  1, wT v  1, w q  1, x  S,

t,xRp0 ,s,z,y,rRp,v,wRK

s  ReLU(Wz + Ux + u), z = ReLU(Wz + Ux + u),

r - WT y = CT v, y = diag(s) · r, y 2  |||C|||2 · v 2, r 2  |||C|||2 · v 2, s(r - WT y) = s(CT v), z(r - WT y) = z(CT v),

y(r - WT y) = y(CT v), r(r - WT y) = r(CT v)} .

(LipMON-c)

14

A.3 Proof of Lemma 2 The SOS constraint in problem (EllipMON-SOS-d) can be written as

0(x, z) = -

Q(Cz + c) + b

2 2

-

1

+ 1(x, z)T gq(x - x0)

(=: f1(x, z)) (=: f2(x, z))

+  (x, z)T (z(z - Wz - Ux - u)) (=: f3(x, z)) + 2(x, z)T (z - Wz - Ux - u) (=: f4(x, z))

+ 3(x, z)T z

(=: f5(x, z))

= - f1(x, z) + f2(x, z) + f3(x, z) + f4(x, z) + f5(x, z) =: -f (x, z) .

For d = 1, denote by Mi the Gram matrix of polynomial fi(x, z) for i = 1, . . . , 5 and M the Gram

matrix of polynomial f (x, z), with basis [xT , zT , 1]. We have explicitly M =

5 i=1

Mi,

where

Mi

has the following form

0p0×p0 M1 =  0p×p0
01×p0

0p0×p CT Q2C cT Q2C + bT QC

0p0×1



CT Q2c + CT Qb

,

cT Q2c + 2bT Qc + bT b - 1

 

-diag(1)

0p0×p diag(1) · x0



  

0p×p0

0p×p

0p×1  ,

  M2 =


xT0 · diag(1) 01×p  -Ip0 0p0×p

1T (2 - x20) x0 



1 

0p×p0

0p×p

0p×1  ,

 

xT0

01×p 2 - xT0 x0

for L-norm, for L2-norm,

 0p0×p0

M3

=

-

1 2

diag( )U

01×p0

-

1 2

UT

diag(

)

diag( )(Ip - W)

-

1 2

uT

·

diag( )

0p0×1



-

1 2

diag(

)

·

u

,

0

 0p0×p0

M4 =  0p×p0

-

1 2

3T

U

0p0×p

0p×p

1 2

3T

(Ip

-

W)

-

1 2

UT

3



1 2

(Ip

-

WT

)3

,

-3T u

0p0×p0 M5 =  0p×p0
01×p0

0p0×p

0p×p

1 2

2T

0p0×1

1 2

2



.

0

Moreover, in order to improve the quality of the ellipsoid, we can also use the slope restriction
condition of ReLU function as proposed in [17]: (zj - zi)(Wj,:z + Uj,:x + uj - Wi,:z - Ui,:x - ui) - (zj - zi)2  0 for i = j. The Gram matrix of the SOS combination of these constraints with basis [xT , zT , 1] has the form

M6 =

U
0p×p0 01×p0

W
Ip 01×p

uT
0p×1 1

0p0 ×p0 T
01×p0

T -2T 01×p

0p0×1 0p×1
0

UWu

0p×p0 Ip 0p×1 ,

01×p0 01×p

1

where T =

p-1 i=1

p j=i+1

ij (ei

-

ej )(ei

-

ej )T

with

ij



0

for

all

i

<

j,

and

{ei}pi=1



Rp

is

the canonical basis of Rp. Since 0(x, z) is an SOS polynomial of degree at most 2, we conclude that

-M 0. According to Lemma 5 in [10], the constraint -M 0 is equivalent to an SDP constraint

using Schur complements, which finishes the proof of Lemma 2.

15

A.4 An Adversarial Example

(a) Original example, classified as 7

(b) Adversarial example, classified as 3

Figure 2: An adversarial example of the first test MNIST input found by PGD algorithm for L norm with  = 0.1.

A.5 Licenses of Used Assets

Table 4: Summary of the licenses of used assets

Software

License

Julia JuMP Matlab CVX Python Pytorch Mosek Our code

MIT License Mozilla Public License Proprietary Software CVX Standard License Python Software Foundation License Berkeley Software Distribution Proprietary Software CeCILL Free Software License

16

