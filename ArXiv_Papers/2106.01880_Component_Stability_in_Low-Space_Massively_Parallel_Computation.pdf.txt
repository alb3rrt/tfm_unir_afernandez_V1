Component Stability in Low-Space Massively Parallel Computation

Artur Czumaj University of Warwick A.Czumaj@warwick.ac.uk

Peter Davies IST Austria
Peter.Davies@ist.ac.at
June 4, 2021

Merav Parter Weizmann Institute of Science Merav.Parter@weizmann.ac.il

arXiv:2106.01880v1 [cs.DS] 3 Jun 2021

Abstract
In this paper, we study the power and limitations of component-stable algorithms in the low-space model of Massively Parallel Computation (MPC). Recently Ghaffari, Kuhn and Uitto (FOCS 2019) introduced the class of component-stable low-space MPC algorithms, which are, informally, defined as algorithms for which the outputs reported by the nodes in different connected components are required to be independent. This very natural notion was introduced to capture most (if not all) of the known efficient MPC algorithms to date, and it was the first general class of MPC algorithms for which one can show non-trivial conditional lower bounds. In this paper we enhance the framework of component-stable algorithms and investigate its effect on the complexity of randomized and deterministic low-space MPC. Our key contributions include:
· We revise and formalize the lifting approach of Ghaffari, Kuhn and Uitto. This requires a very delicate amendment of the notion of component stability, which allows us to fill in gaps in the earlier arguments.
· We also extend the framework to obtain conditional lower bounds for deterministic algorithms and fine-grained lower bounds that depend on the maximum degree .
· We demonstrate a collection of natural graph problems for which deterministic noncomponent-stable algorithms break the conditional lower bound obtained for componentstable algorithms. This implies that, in the context of deterministic algorithms, componentstable algorithms are conditionally weaker than the non-component-stable ones.
· We also show that the restriction to component-stable algorithms has an impact in the randomized setting. We present a natural problem which can be solved in O(1) rounds by a component-unstable MPC algorithm, but requires (log log n) rounds for any componentstable algorithm, conditioned on the connectivity conjecture.
Altogether our results imply that component-stability might limit the computational power of the low-space MPC model, at least in certain contexts, paving the way for improved upper bounds that escape the conditional lower bound setting of Ghaffari, Kuhn, and Uitto.
This work is partially supported by a Weizmann-UK Making Connections Grant, the Centre for Discrete Mathematics and its Applications (DIMAP), IBM Faculty Award, EPSRC award EP/V01305X/1, European Research Council (ERC) Grant No. 949083, and the European Union's Horizon 2020 programme under the Marie SklodowskaCurie grant agreement No 754411.
1

1 Introduction
The central goal of this paper is to advance our understanding of the computational power of low-space algorithms in the Massively Parallel Computation (MPC) model. Our main focus is on the notion of component-stable low-space MPC algorithms introduced recently by Ghaffari, Kuhn and Uitto [GKU19] as the first general class of MPC algorithms for which non-trivial conditional lower bounds can be obtained. Roughly speaking, in this class of algorithms the output of nodes in different connected components are required to be independent. While this definition has been introduced to capture most (if not all) of the known MPC algorithms to date, and the notion of component-stable algorithms seems quite natural and unlimited, we demonstrate its effect on the complexity of randomized and deterministic low-space MPC. Our main finding is that the notion of component-stability as defined in [GKU19] is rather fragile and needs to be studied with care, leading us to a revision of this framework to make it robust. Our amended framework of componentstable algorithms allows us to fill in gaps in the earlier arguments and make it more applicable. In particular, the revised setup enables us to extend the framework of (conditional) lower bounds from [GKU19] for component-stable randomized algorithms relating LOCAL algorithms and lowspace MPC algorithms: we demonstrate that it can be parameterized with respect to the maximum graph degree  and holds also for deterministic algorithms, thereby making the framework more broadly applicable and proving for the first time a host of conditional lower bounds for a number of deterministic low-space component-stable MPC algorithms.
Next, we will show that for some natural problems there are low-space component-unstable MPC algorithms (both randomized and deterministic) that are significantly more powerful than their component-stable counterparts. So, rather than being a technical triviality, component-stability is in fact a significant restriction on the power of the low-space MPC model.
Background. The rapid growth of massively parallel computation frameworks, such as MapReduce [DG04], Hadoop [Whi12], Dryad [IBY+07], or Spark [ZCF+10] resulted in the need of active research for understanding the computational power of such systems. The Massively Parallel Computation (MPC) model, first introduced by Karloff et al. [KSV10] (and later refined in [ANOY14, BKS13, GSZ11]) has became the standard theoretical model of algorithmic study, as it provides a clean abstraction of these frameworks. Over the past years, this model has been receiving a major amount of interest by several independent communities in theory and beyond. In comparison to the classical PRAM model, the MPC model allows for a lot of local computation (in principle, unbounded) and enabled it to capture a more "coarse­grained" and meaningful aspect of parallelism (see, e.g., [ASS+18, BHH19, CLM+18, GGK+18, GU19]).
In the MPC model, there are M machines and each of them has S words of local space at its disposal. Initially, each machine receives its share of the input. In the context of graph problems where the input is a collection V of nodes and E of edges, |V | = n, |E| = m, the input is arbitrarily distributed among the machines (and so S · M  n + m). In this model, the computation proceeds in synchronous rounds in which each machine processes its local data and performs an arbitrary local computation on its data. At the end of each round, machines exchange messages. Each message is sent only to a single machine specified by the machine that is sending the message. All messages sent and received by each machine in each round, as well as the output have to fit into the machine's local space S.
Our focus in this paper is on the low-space setting where the local space of each machine is
1

strongly sublinear in the number of nodes, i.e., S = n for some   (0, 1). Our lower bounds

will be against algorithms with any polynomial number of machines (i.e., M = poly(n)), while our

upper bounds (with the exception of the non-uniform general derandomization of Lemma 55) will

use

at

most

O(m

+

n1+)

global

space

(i.e.,

M

=

O(

m n

+

n)).

The low-space regime is particularly challenging due to the fact that a node's edges cannot neces-

sarily be stored on a single machine, but rather are scattered over several machines. Nevertheless,

for many classical graph problems, poly(log n)-round algorithms can be obtained, and recently

we have also seen even sublogarithmic solutions. Ghaffari and Uitto [GU19] (see also [Ona18]) presented a randomized graph sparsification technique resulting in O( log ) round algorithms

for maximal matching and MIS, where  is the maximum degree. This should be compared,

for example, with maximal matching algorithms with significantly more local space: Lattanzi et

al. [LMSV11] presented an O(1/)-round randomized algorithm using O(n1+) local space and

Behnezhad et al. [BHH19] gave an O(log log n)-round randomized algorithm using O(n) local space

(see also [ABB+19, CLM+18, GGK+18]). For the problem of ( + 1)-vertex coloring Chang et al.

[CFG+19] showed a randomized low-space MPC algorithm that works in O(log log log n) rounds,

when combined with the network decomposition result of Rohzon and Ghaffari [RG20].

While we have seen some major advances in the design of low-space MPC algorithms, no (uncon-

ditional) hardness results are known for any of the above problems in the low-space MPC setting.

A seminal work by Roughgarden et al. [RVW18] provides an explanation for the lack of such lower

bound results. They showed that obtaining any unconditional lower bound in the low-space MPC

(for algorithms with an arbitrarily polynomial number of machines) setting ultimately leads to a

breakthrough result in circuit complexity, namely that NC1 P. This work has opened up a new

avenue towards proving conditional hardness results that are based on the widely believed connec-

tivity conjecture. This conjecture (extensively used in our current paper) states that there is no

o(log n)-round (randomized) low-space MPC algorithm (even using any polynomial global space)

for

distinguishing

between

the

input

graph

G

being

an

n-length

cycle

and

two

n 2

-length

cycles.

The first conditional lower bounds in the low-space MPC setting were presented by a recent

insightful paper of Ghaffari, Kuhn and Uitto [GKU19]. This work provides a collection of condi-

tional hardness results for classical local problems by drawing a new connection between the round

complexity of a given problem in the LOCAL model [Lin92], and its corresponding complexity in

the low-space MPC model. Unlike the low-space MPC setting, for the LOCAL model, arguably one

of the most extensively studied model in distributed computing, there is a rich collection of (uncon-

ditional) lower bound results. To enjoy these LOCAL lower bound results in our context, [GKU19]

presented a quite general technique that for many graph problems translates an (r)-round LOCAL

lower bound (with an additional requirement of using shared randomness) into an (log r)-round

lower bound in the low-space MPC model conditioned on the connectivity conjecture. This beautiful

lifting argument is surprisingly quite general, capturing the classical lower bounds for problems like

MIS, maximal matching [KMW06], LLL (Lov´asz Local Lemma), and sinkless orientation [BFH+16].

For example, one very strong implication of their technique is that conditioned on the connectivity

conjecture, it shows that there is no randomized low-space MPC algorithm for (even approximate)

maximal matching or MIS problems using o(log log n) rounds.

The framework of Ghaffari, Kuhn and Uitto [GKU19] has one main caveat, which at first glance

appears to be quite negligible, a mere technicality. Their conditional lower bounds do not hold for

any algorithms but rather only for the special class of component-stable MPC algorithms. The

key property of these algorithms is that the output of the nodes in one connected component is

2

independent of other components. More formally, in component-stable algorithms, the output of each node v is allowed to depend (deterministically) only on the node v itself, the initial distribution, the ID assignment of the connected component of v, and on the shared randomness. The class of component-stable algorithms is indeed quite natural, and at least by the time of publication of [GKU19], it appeared to capture most, if not all, of the existing MPC algorithms, as explicitly noted by the authors:
[GKU19] To the best of our knowledge, all known algorithms in the literature are component-stable or can easily be made component-stable with no asymptotic increase in the round complexity.
In this view, it appeared that the restriction to component-stable algorithms is no more than a minor technicality rather than an actual limitation on the low-space MPC model.
The first indication that component-stability might actually matter was provided by recent works [CDP20a, CDP20b], which present deterministic low-space component-unstable MPC algorithms for several classic graph problems, even though the validity of solutions to these problems depends only on local information. Specifically, by derandomizing a basic graph sparsification technique, one can obtain O(log  + log log n)-round deterministic low-space component-unstable MPC algorithms for MIS, maximal matching, and ( + 1)-coloring. A key ingredient of these algorithms is a global agreement on a logarithmic length seed, to be used by all nodes in order to simulate their randomized decisions. This global seed selection involves coordination between all the nodes, regardless of their components, thus yielding component-unstable algorithms. The component-instability here seems to be quite inherent to the derandomization technique, and it is unclear whether any component-stable method could perform equally well.
1.1 Our aims
In this paper we thoroughly investigate the concept of component-stability and its impact on randomized and deterministic low-space MPC algorithms.
Upon examining the notion of component-stability in detail and after attempts to broaden its applications, it becomes apparent that the concept is highly sensitive to the exact definition used, and that one must be very careful in specifying what information the outputs of component-stable algorithms may depend on. For example, we must precisely specify whether we allow componentstable algorithms' outputs to depend on the input size n, and we find that either choice here holds problematic implications for the current lower bounds and for the analysis due to Ghaffari et al. [GKU19].
This raises the first main question of our work:
Question 1. Can we revise the lifting framework and amend the definition of componentstability which both captures a wide array of algorithms, and also allows us to prove robust lower bounds?
Having fixed such a definition, we ask to what extent component-stability restricts MPC algorithms, and whether the concept is indeed a technicality or actually a significant limitation. The lifting arguments of [GKU19] are designed for randomized algorithms, which raises the following question:
3

Question 2. Does component-instability help for obtaining improved randomized low-space MPC algorithms for graph problems? Is there any separation between randomized componentstable and component-unstable MPC algorithms?
We then turn to consider the impact of component-stability on deterministic low-space MPC algorithms. Since the recent derandomization technique of [CDP20a, CDP20b] leads to inherently component-unstable algorithms, we ask:
Question 3. Does component-instability help for obtaining improved deterministic low-space MPC algorithms for graph problems? Is there any separation between deterministic component-stable and component-unstable MPC algorithms?
Understanding the gap between randomized and deterministic solutions is one of the most fundamental and long-standing questions in graph algorithms. In a very related context, the last few years provided a sequences of major breakthrough results which almost tightly characterize the gap between the deterministic and randomized complexities for distributed computation (in the LOCAL model). Rohzon and Ghaffari [RG20] settled a several-decades-old open problems by presenting a deterministic polylogarithmic algorithm for network decomposition. Their result implies that any polylogarithmic-time randomized algorithm for LCL problems [CKP19, NS95] can be derandomized to a polylogarithmic-time deterministic algorithm. In other words, randomization does not help in the polylogarithmic-time regime. On the other hand, Chang, Kopelowitz and Pettie [CKP19] showed that in the sub-logarithmic-time regime, randomization might provide an exponential benefit. For example, for -coloring of trees of maximum degree  there are (log log n) randomized-round algorithms and a deterministic lower bound of (log n) rounds. Balliu et al. [BBH+19] recently showed that maximal matching and maximal independent sets cannot be found by o(+log log n/ log log log n)-rounds randomized algorithms, and deterministically in o(+log n) rounds, thus provide an exponential gap in the lower bound for bounded-degree graphs. Finally, [CHL+20] presented additional separation results for edge-coloring: (2 - 2)-edge-coloring requires (log log n) randomized rounds, and (log n) deterministic rounds. In view of these separation results, we therefore ask in the context of related MPC computation:
Question 4. Is there a gap between component-stable randomized algorithms vs. componentstable deterministic algorithms?
In this paper, we answer all four questions in the affirmative.
1.2 Our contributions
In this paper, we study the power and limitations of component-stable algorithms in the low-space MPC model. This class of algorithms is to this date the only class for which conditional lower bound for local graph problems can be obtained. Our main contribution is in demonstrating the impact of this property on the complexity of randomized and deterministic local graph problems. Towards that goal, we informally define the following four complexity classes:
· DetMPC: deterministic low-space component-unstable MPC algorithms,
· S-DetMPC: deterministic low-space component-stable MPC algorithms,
4

· RandMPC: randomized low-space component-unstable MPC algorithms, · S-RandMPC: randomized low-space component-stable MPC algorithms. We refer to Section 2.5 for the precise definitions.

A robust lifting framework. We rectify the framework of low-space component-stable MPC algorithms due to Ghaffari et al. [GKU19]. We study the framework in detail and demonstrate that in order to be broadly applicable, many aspects of the original setting are highly sensitive to the exact definition used, and require amendments to carefully specify what information the outputs of component-stable algorithms may depend on. We present a modified framework, reaching a revised definition of component-stability (see Definition 13) which both encompasses many existing MPC algorithms, and for which robust conditional lower bounds can be shown. This answers Question 1.

Extensions to deterministic and degree-dependent lower bounds. Our revised framework not only recovers all main results from the framework of Ghaffari et al. [GKU19], but also extends the arguments to include conditional lower bounds for deterministic algorithms and fine-grained lower bounds that depend on . While our main theorem (Theorem 14) lifting LOCAL lower bounds to component-stable MPC algorithms has several subtle assumptions, the main, informal claim is that for many graph problems P, if P has a T (n, )-round (randomized or deterministic) lower bound in the LOCAL model, then assuming the connectivity conjecture, any low-space componentstable (respectively, randomized or deterministic) MPC algorithm solving P requires (log T (n, )) rounds.

Instability helps randomized MPC algorithms. To address Question 2, we consider the problem of finding a large (specifically, of size (n/) independent set. This problem has been recently studied by Kawarabayashi et al. [KKSS20], who provided a randomized lower bound of (log n) rounds (for a specific range of ). We show that their lower bound can be adapted to our revised lower-bound lifting framework of Theorem 14, obtaining a conditional lower bound of (log log n) rounds for component-stable MPC algorithms.1 In contrast, we present a very simple O(1)-round component-unstable randomized algorithm for the problem. In fact, this algorithm can further be derandomized within O(1) rounds (see Theorem 53), demonstrating an instance in which deterministic component-unstable algorithms are more powerful even than randomized component-stable algorithms (i.e., DetMPC S-RandMPC).

Theorem 5. Conditioned on the connectivity conjecture, any component-stable low-space MPC

algorithm for computing an independent set of size (n/) on n-node graphs (for the full range

of   [1, n)) and succeeding with probability

at

least

1

-

1 n

,

requires

(log log n)

rounds.

This

problem admits a simple O(1)-round randomized low-space MPC algorithm which is component-

unstable; additionally, the algorithm can be derandomized within O(1) rounds.

Stated in complexity language, Theorem 5 provides a separation between the class of RandMPC and S-RandMPC (conditioned on the connectivity conjecture, see Theorem 20). The basic observation providing this separation is the fact that one can easily compute in O(1) rounds (even in the

1The original lifting arguments of [GKU19] only hold for LOCAL lower bounds that hold under exact knowledge of n; the lower bound of [KKSS20] does not, but holds only under knowing a polynomial estimate of n, which is allowed for in our framework.

5

LOCAL model) an independent set of (n/) nodes in expectation. In the LOCAL model, we need

provably longer

to achieve a high-probability success guarantee of

1

-

1 n

.

In the low-space

MPC

model, however, we can perform the process of success probabiliy amplification: we run (log n)

parallel repetitions of the basic algorithm, and choose a successful one if such exists, amplifying the

success

probability

to

1

-

1 n

without any

slow-down.

This powerful

process,

though,

is

inherently

component-unstable, since it relies on globally agreeing on one of the repetitions to use2.

Instability helps deterministic MPC algorithms. We then turn to consider the effect of component-stability on deterministic MPC algorithms. While the original setup of Ghaffari et al. [GKU19] had been designed only for randomized algorithms, the revised framework developed in our paper in Section 2.4.3 extends naturally to the deterministic setting, providing a robust deterministic lifting analog in Theorem 14. Theorem 14 provides a general framework lifting unconditional deterministic lower bounds for the LOCAL model for many natural graph problems to conditional lower bounds for low-space component-stable MPC algorithms in the same way as the randomized framework in [GKU19].
We then turn to show that with component-instability one can in fact surpass these conditional lower bounds and present several results showing a separation between DetMPC and S-DetMPC (conditioned on the connectivity conjecture) and positively answering Question 3. In Section 4.2, we show that for several problems closely related to LLL, including sinkless orientation and some variants of edge-coloring and vertex-coloring, component-instability helps for deterministic algorithms. Finally, in Section 4.3, we demonstrate a similar result for the class of all LOCAL extendable algorithms by combining the lifting of deterministic LOCAL lower bounds in Theorem 14 with a derandomization technique using pseudorandom generators. To demonstrate the applicability of this derandomization recipe, we show how it can be used to improve the deterministic running times of two cornerstone problems in low-space MPC: maximal independent set and maximal matching. And so, on one hand we prove (Theorem 48) that conditioned on the connectivity conjecture, there is no deterministic low-space component-stable MPC algorithm that computes a maximal matching or maximal independent set, even in forests, in o(log  + log log n) rounds, and on the other hand, we give a deterministic low-space component-unstable MPC algorithm for these problem running in O(log log  + log log log n) rounds (when  = 2logo(1) n, Corollary 47). (The resulting MPC algorithm must either perform heavy local computations, or alternatively, the underlying PRGs can be hard-coded in the MPC machines for a non-uniform but computationally-efficient algorithm.)

Relations between randomized and deterministic MPC algorithms. Finally, we consider the interesting gap between randomized and deterministic algorithms in the low-space MPC setting. As observed by [CKP19] and [GK19] (and see Lemma 54 for a sketch of the argument), randomized algorithms that succeed with probability of 1 - 1/2n2 can be turned into non-uniform deterministic algorithms. This result can also be extended to the low-space MPC setting, with some caveat. In contrast to the LOCAL model where the space of the nodes is unlimited, in the low-space MPC setting, the transformation implied by [CKP19] and [GK19] yields a non-explicit algorithm (see Lemma 54). By using success probability amplification with poly(n) machines, one can boost the success guarantee of any randomized MPC algorithm from 1 - 1/ poly(n) to 1 - 1/2n2 without any slowdown in the round complexity. From the complexity perspective, restricting to non-uniform
2Indeed, this causes an issue with the proof of Lemma III.1 of [GKU19], where success probability amplification is used in an algorithm AMPC that is later (in Lemma IV.2 in [GKU19]) assumed to be component-stable.

6

and non-explicit computation, one therefore finds that DetMPC = RandMPC, see Theorem 22. For some specific problems, we can perform more careful derandomization methods that do not cause the resulting deterministic algorithms to be non-uniform, non-explicit, or to use excessive global space, demonstrating that component-stability restricts power even without these allowances.
As we have noted, success probability amplification is inherently component-unstable. Turning our focus to low-space component-stable MPC algorithms, here we can provide a conditional separation between randomized and deterministic algorithms (Theorem 29), positively answering Question 4. This separation follows by combining (i) the conditional lifting for randomized component-stable algorithms and deterministic component-stable algorithm with (ii) local problems for which there is provable gap in their randomized and deterministic LOCAL complexity.
Complexity summary: Let us summarize the complexity results, assuming the connectivity conjecture, and allowing non-uniform MPC algorithms. Our study demonstrates that in low-space MPC, component-unstable algorithms are provably stronger than their component-stable counterparts, both for deterministic and randomized algorithms (Theorems 19 and 20). Further, for component-stable algorithms, randomized algorithms are provably stronger than their deterministic counterparts (Theorem 21). However, for arbitrary (possibly component-unstable) algorithms this is not the case: any randomized algorithm can be efficiently simulated by a deterministic one (Lemma 55 and Theorem 22).
2 Revised Framework of Component Stability
In this section we suggest an array of changes that may be made to the framework of componentstable algorithms due to Ghaffari et al. [GKU19], in order to reach a revised definition of componentstability (Definition 13) which both encompasses many existing randomized MPC algorithms, and for which robust conditional lower bounds can be shown. These changes also allow us to extend the original setting to both deterministic algorithms and those that have running-time dependency on maximum degree .
2.1 Discussion of definitions of component stability
Let us first present the description of component stability from [GKU19, Section II]:
Formally, assume that for a graph G, DG denotes the initial distribution of the edges of G among the M machines and the assignment of unique IDs to the nodes of G. For a subgraph H of G let DH be defined as DG restricted to the nodes and edges of H. Let Hv be the connected component of node v. An MPC algorithm A is called component-stable if for each node v  V , the output of v depends (deterministically) on the node v itself, the initial distribution and ID assignment DHv of the connected component Hv of v, and on the shared randomness SM .
We informally sketch the line of argument of [GKU19] that leverages this definition to lift LOCAL lower bounds to MPC: first, it is shown that if there is a LOCAL lower bound for a problem P , and an MPC algorithm AMP C is able to solve P faster than the log of the LOCAL lower bound, there there must exist two graphs G and G, which are locally indistinguishable but on which AMP C's output must differ (at least with some sufficiently large probability). In the terminology of [GKU19], AMP C must be `farsighted': able to somehow make use of information from far-away nodes.
7

These graphs G and G, and the assumed algorithm AMP C, are then ingeniously used to construct an algorithm Bst-conn that solves a connectivity problem conjectured to be hard. Specifically, Bst-conn constructs a pair of simulation graphs based on its input to the connectivity problem. These simulation graphs consist of many disjoint copies of induced subgraphs of G and G respectively. The construction is made in such a way that a full copy of G and G only appears if two
particular nodes (designated s and t) are connected in the input graph for the connectivity problem.
Bst-conn simulates AMP C on this pair of simulation graphs. If s and t are connected, then full copies of G and G are present as connected components in the simulation graphs, and AMP C should return different outputs on them with sufficiently high probability. Otherwise, there are no full copies of G and G, and AMP C returns the same outputs on both simulation graphs. This difference in behavior is exploited to allow Bst-conn to determine whether s and t are connected, and solve the connectivity problem faster than is conjectured to be possible.
The property of component-stability is crucial in this last step: we require that AMP C behaves the same on G and G when they are connected components of the (much larger) simulation
graphs as it does when they are the entire input (as was the case when showing that AMP C was farsighted). Otherwise, we could not say anything about AMP C's output on the simulation graphs. It transpires that this argument is quite fragile, and highly sensitive to the precise definition of
component-stability used. We discuss some of the issues below.

Randomized component-stable algorithms must be allowed dependency on n. The

first major point of consideration is that, as defined in [GKU19], the output of a node v under a

component-stable algorithm must depend only on shared randomness, the IDs of v and its com-

ponent, and the input distribution of edges to machines. In particular, no provision is made for

dependency on the number of nodes n in the input graph, and indeed, the arguments of [GKU19]

seem to forbid it.3 This is somewhat counter-intuitive for MPC algorithms: while a LOCAL algo-

rithm can never determine n unless it is given as input (and therefore it is commonly assumed that

we provide the algorithm with at least a polynomial estimate of n), an MPC algorithm can easily

do so in O(1) rounds, by simply summing counts of the number of nodes held on each machine.

We can therefore assume any such algorithm has knowledge of the exact value of n, and natural

algorithmic approaches would generally make use of this knowledge.

Furthermore, the success probability of correct randomized algorithms is defined to be at least

1-

1 n

,

in

accordance

with

the

standard

definition

of

with

high

probability

correctness.

This

causes

a contradiction for algorithms with no output dependency on n:

Consider a correct component-stable MPC algorithm A for a problem in which the validity of

a node's output can be verified by seeing its connected component (we will formalize this notion

later), running on a n-node graph. This algorithm must produce a valid output for each node in

the

graph

with

probability

at

least

1

-

1 n

.

We now add  disconnected nodes to the input graph. If A's output does not have any de-

pendency on n, then it must be unchanged at each of the original nodes, since they are in an

unchanged connected component. However, A must succeed on the new graph with probability at

least

1

-

1 n+

.

Since the problem is component-stable, the probability that A succeeds on all of

the nodes of the original graph is at least as high as the probability that it succeeds on the whole

3Specifically in proof of [GKU19, Lemma IV.2], where algorithm AMP C is simulated on large simulation graphs containing smaller components G and G, as discussed above, its behavior on G and G as components is only identical to when run on them as sole input if no dependency on n is permitted.

8

graph,

i.e.,

1-

1 n+

.

So,

A

must

succeed

on

the

original

graph

with

probability

at

least

1-

1 n+

,

and since we can set  arbitrarily high, must therefore succeed with certainty; this requires A to

be deterministic!

Another problem with disallowing dependency on n is that running times generally depend on n.

While, for some (deterministic) algorithms, this dependency could be replaced by other parameters

(e.g., the maximum number of nodes in a connected component, or size of the ID space), this is

not the case for randomized algorithms, where the running time usually directly affects the success

probability, which itself must depend on n.

So, in short, a definition of component-stability which does not allow any dependency on n

includes essentially no non-trivial randomized algorithms.

If we allow dependency on n, we must restrict the class of problems in order to obtain MPC lower bounds. We have seen that, to give results which apply to probabilistic algorithms, we must allow dependency on n. However, we cannot then hope to obtain a result akin to Theorem I.4 of [GKU19] for all graph problems.
As an example, consider the following problem: each node must output YES if the entire graph is a simple path with consecutive node IDs, and NO otherwise. Note that there is only one possible correct output for each node v, and that this output is a deterministic function of its component and the value of n (since v's output should be YES iff its component is an n-node path with consecutive IDs). Furthermore, there is an O(1)-round MPC algorithm for the problem: it is straightforward to check whether there are two nodes of degree 1, n - 2 nodes of degree 2, and that each node's 1-hop neighborhood is consistent with being in a path of consecutive IDs. So, if component-stability is defined to allow dependency on n, an O(1)-round deterministic component-stable algorithm for the problem exists.
However, the problem has a trivial n - 1-round (randomized) LOCAL lower bound, since a YES instance can be changed to a NO instance by only altering the ID of one endpoint of the path, and the other endpoint requires n - 1 rounds to detect this change. Hence, we cannot hope for a universal method of lifting LOCAL lower bounds to non-trivial component-stable MPC lower bounds if component-stability allows dependency on n.
We will see, though, that such counterexamples are necessarily quite contrived, and that we can prove such a result for a class that includes most problems of interest (such as, e.g., all locallycheckable (LCL) problems, see Section 2.3).

Uniqueness of identifiers. It is common in both LOCAL and MPC to assume that nodes of the input graph are equipped with identifiers (IDs) that are unique throughout the entire graph. This assumption, however, is somewhat at odds with the concept of component-stability: if, for example, a disconnected node is added to a valid graph, sharing an ID with an existing node, then the input becomes invalid. So, outputs for the original nodes are now allowed to change arbitrarily, even though their components have not altered.
We could instead require that IDs are only component-unique (i.e., they are allowed to be shared by disconnected nodes, but not connected ones). This is a weaker assumption which aligns well with component-stability, and is still sufficient for LOCAL algorithms (in which nodes have no interaction with or dependency on disconnected nodes).
This approach, though, presents a problem in MPC. Unlike in LOCAL, where nodes are inherently separate computational entities which only need IDs for symmetry-breaking (particularly for

9

deterministic or shared randomness algorithms), in MPC an input graph node essentially is its ID. The input is given only as a binary encoding of the IDs of nodes and edges, and so any two nodes with the same ID will be contracted to a single node when this input is interpreted as a graph. As a consequence, MPC algorithms cannot work with graphs in which IDs are only component-unique.
Our solution to this problem is to separate the two functions of IDs. We will assume that IDs are only component-unique, and that component-stable MPC algorithms can depend on these. However, we also provide MPC algorithms with fully-unique names for nodes, whose purpose is only to allow the algorithm to distinguish the input graph's nodes as separate objects. Accordingly, we do not allow the output of component-stable algorithms to depend on the names.4
Initial distribution of input. The definition of [GKU19] allows MPC algorithms' outputs to depend on the initial distribution of the input to the machines. While this is natural to do, we observe that under our definition it is not necessary: given a component-stable algorithm AMP C whose output depends on this distribution, we can always create a new component-stable BMP C which does not.
Specifically, since the nodes have unique poly(n) names (and we can also give edges unique poly(n) names based on their endpoints), and we are allowed any poly(n) number of machines, algorithm BMP C can first (in one round) redistribute each node and edge of the input to its own dedicated machine, with the same name as the corresponding node or edge. Then, it simulates AMP C, and reaches a valid output, which is now independent of the initial distribution. Since AMP C 's output is component-stable, BMP C's is also.5
Therefore, a lower bound for component-stable algorithms that depend on input distribution implies a lower bound for those that do not. So, we can disallow this dependency from the definition without weakening our results.
2.2 Graph families
In this section, we make some definitions concerning the input graphs on which MPC algorithms run. Firstly, to address the problem concerning uniqueness of identifiers, we define legal graphs to be those with separate unique node names and component-unique node IDs as discussed:
Definition 6. A graph G is called legal if it is equipped with functions ID, name : V (G)  [poly(n)] providing nodes with IDs and names, such that all names are fully unique and all IDs are unique in every connected component.
Throughout the paper, we will always assume that input graphs for MPC are legal (and we will ensure it when constructing inputs ourselves). For component-stable algorithms, this is to allow a weaker dependency on the IDs and not the names, as discussed above. For non-component-stable algorithms, it is no different from the standard (fully-unique IDs) requirement, since their outputs are allowed to depend on the names, and so we can simply use the names as IDs.
4Note that, unlike the changes regarding dependency on n and problem class, this change is not necessary to show a general framework for conditional MPC lower bounds -- the same results could be proven assuming fully-unique IDs (at least for randomized algorithms) using techniques from [GKU19]. However, we feel that this definition better captures the `spirit' of component-stability.
5We will define component-stable outputs to not depend on the names of machines -- this is not a major restriction, since we are not aware of any MPC algorithms which are not independent of renaming machines. However, it is an important point here, since BMP C 's machine names now depend on node names, upon which BMP C 's output must not depend.
10

Next, we make a definition which will allow us to show MPC lower bounds on specific families of graphs. LOCAL lower bounds are often proven using graphs from some specific family H as the "hard instances": in particular, many such bounds are proven on trees. Lower bounds on restricted families of graphs are stronger than those on the class of all graphs, and can also provide more meaningful hardness results for problems which are only possible on restricted families (such as vertex coloring, see Theorem 42). When lifting LOCAL lower bounds on restricted graph families to MPC, we therefore wish to preserve the family on which the lower bound holds.
As discussed in Section 2.1, the lines of argument made both by [GKU19] and this work involve construction of a simulation graph H as the "hard instance" in MPC. These simulation graphs (from the proof of Lemma IV.2 of [GKU19] and Lemma 27 here) are constructed differently, but both consist of disjoint unions of induced subgraphs of some hard instance G for the LOCAL lower bound.
The simulation graph H is not necessarily in H, if H is an arbitrary graph family, and is not therefore a valid input for an MPC algorithm defined to run on H. This is not necessarily a problem for [GKU19], since one could require that their definition of component-stability should hold even if algorithms are given an invalid input. Under this definition, the output of a component-stable MPC algorithm AMP C on H must be the disjoint union of AMP C's outputs on its connected components (which, in this case, are in H) separately, and this circumvents the need for H itself to be in H.
However, as we notice in Section 2.1, to incorporate non-trivial randomized algorithms, any suitable definition of component-stable algorithms must allow dependency on n. Then, the output of AMP C of H need not be the union of that on H's connected components, since the inputs have differing values of n. This necessitates several changes from the proofs of [GKU19], one of which is that we do require H  H.
To ensure that this is the case, we prove our lower-bound lifting argument only for the following families of graphs:
Definition 7 (Normal families of graphs). A class of graphs H is called normal if it is hereditary (i.e., closed under removal of nodes) and closed under disjoint union.
The set of all graphs is a normal family, and can always be used in the worst case. Further, observe that the class of all trees is not a normal family of graphs; however, the family of all forests is normal. Therefore, for example, Theorem 14 implies that LOCAL lower bounds on trees can be lifted to conditional MPC lower bounds on forests (but not trees).
2.3 Types of graph problems and replicability
We next define the types of problem we will encompass with this work. We will focus on graph problems, and let G be the collection of all legal input graph instances.
We consider only graph problems where each node of the input graph must output some label from a finite set . For example, for the vertex coloring problem the label of a node corresponds to its color, and for the independent set problem, the label corresponds to the indicator variable whether the node is in the independent set returned.
As in [GKU19], we do not explicitly output labels for edges. However, to apply our results to problems in which only edges are given output labels (such as matching, edge coloring, or sinkless orientation), we can simply redefine the problem as vertex-labeling on the line graph (where the vertices represent edges of the input graph, with IDs and names given by Cartesian products of the IDs and names of their endpoints). For any normal graph class H, the family of LH of line graphs
11

of graphs from H is also normal. Working on the line graph increases number of nodes n at most quadratically and maximum degree  at most by a factor of 2, and in LOCAL requires only 1 extra round. We will see that performing this conversion to the line graph will allow us to obtain results for edge-labeling problems without any asymptotic change in bounds. We can then convert back to recover a solution to the problem on the original graph.
A graph problem is then defined by a collection of valid outputs for each possible pair (topology, IDs) of a legal input graph. Importantly, we do not allow validity to be dependent on the names of graph nodes (though these names are part of any legal input). That is, given a particular input graph topology and set of IDs, the collection of valid outputs must be consistent regardless of node names. This is because component-stable outputs are not allowed to depend on names, so most problems which allowed solution-dependency on names would be trivially unsolvable by component-stable algorithms. In any case, names were introduced solely to allow MPC algorithms to distinguish nodes as objects, and should not be considered part of problems.
The goal of any algorithm for the problem is then to provide a valid output for the specific legal input it was given. For many problems it is useful to have a concept of the output of a particular node being valid. The overall output is then valid if all nodes' outputs are valid. To capture this concept, we define the following sub-class of problems:
Definition 8. For r  N, an r-radius checkable problem is defined by a collection of valid outputs for each r-radius centered graph equipped with unique IDs.6 The output of a node v in input graph G is deemed valid if the centered graph given by its r-radius ball, and the outputs thereof, is a member of this valid collection. An overall output on G is valid if all nodes' outputs are valid.
An r-radius centered graph here is simply a connected graph with a designated center node, from which all other nodes are of distance at most r.
One can see that r-radius checkable problems are precisely those whose solutions can be verified in r rounds of LOCAL. Note that the majority of graph problems of interest are r-radius-checkable for some r  n: for example, the vertex coloring problem requires that each node outputs a color distinct from the colors output by its neighboring nodes, and thus is easily 1-radius-checkable. Similarly, all LCL (locally-checkable labeling, see, e.g., [CKP19, NS95]) problems, a commonly studied class particularly from a lower-bounds perspective, are O(1)-radius checkable. Still, some natural problems are not n-radius checkable problems: most notably, approximation problems are not, since there is no notion of a node's validity, and nor can nodes determine overall validity by seeing their n-radius ball (i.e., their entire connected component). So, while some of our results concern r-radius checkable problems (such as those in Section 4.3), our main lower bounds results will use a more general class of problems, see below, in order to incorporate approximation problems.
2.3.1 Replicable graph problems
We have seen, from Section 2.1, that to transfer LOCAL lower bounds to MPC, under a definition of component-stability that includes randomized algorithms (and so allows dependency on n), one must restrict the class of problems, since some (contrived) problems have (n)-round LOCAL lower bounds and O(1)-round MPC algorithms. Our goal in this section is to make the minimal restriction needed to facilitate such lower-bound lifting arguments.
6r-radius graphs are, by definition, connected, so component-unique IDs are unique IDs.
12

During proof of Theorem 14 (our main lower-bound lifting theorem), we will consider multiple disjoint copies of the input graph enhanced by isolated nodes. To facilitate this concept in our analysis, we introduce the notion of replicable graph problems.

Definition 9. A graph problem is R-replicable if it satisfies the following property. For any

· graph G  G with |V (G)|  2,

· output labeling L : V (G)  ,

· individual output label   , and

· graph G which is a disjoint union of at least |V (G)|R disjoint copies of G (with the same IDs as G) and fewer than |V (G)| isolated nodes (with the same ID as each other),

let output labeling L on G be given by L on each copy of G, and  on each isolated node. Then, if L is valid on G, L must be valid on G.

Note that replicability is monotonic in R, i.e., if P is R-replicable then P is also (R+1)-replicable, since any G satisfying the construction of (R + 1)-replicability also satisfies the construction for R-replicability.
The definition of replicability may seem unnatural: it is designed to align with a specific construction needed in proof of Theorem 25 (and ultimately Theorem 14). However, we argue that the vast majority of natural graph problems are replicable. We first show all that r-radius-checkable problems (and hence all LCL problems [CKP19, NS95]) are replicable;

Lemma 10. Any r-radius-checkable problem is 0-replicable.

Proof. For any r-radius-checkable problem, the validity of the output of a connected component
depends only on the IDs and topology of the connected component. Any G satisfying the construction of 0-replicability contains at least one copy of G as a connected component. So, for the output on L on G to be valid, the output L must be valid on G.

Further, a major strength of our framework is that most approximation problems are also replicable. As an example, we show replicability for the problem of finding an independent set of size (n/) (which is an (1/)-approximation of the maximum independent set), a problem for which we will later (in Section 5) show a separation between component-stable and non-component-stable algorithms.

Lemma 11. The problem of finding an independent set of size (n/) (on graphs with   1) is 2-replicable.

Proof. Let c > 0 be some fixed constant. Consider a graph G on n nodes, with maximum degree , and a graph G consisting of k  n2 copies of G and fewer than n isolated nodes. Note that G has at least kn nodes. Assume we have some output valid labeling L on G, which corresponds to an independent set of size at least ckn/, and in which each copy of G is labeled identically, as is each isolated node. More than ckn/ - n of the nodes in the independent set must be in
copies of G (since there are fewer than n nodes not in copies of G). Since each copy of G is labeled identically, each must contain more than

ckn/ k

-

n



cn 

-

1 n



cn 2

13

nodes in the independent set (for n  2/c), and therefore the output on G is a valid (n/)independent set.

Similarly, we have a related lemma for approximate matching, one of the central problems demonstrating the power of our framework summarized in Theorem 14 (which will yield also the conditional hardness of the approximate maximum matching problem on MPC). The same arguments can also straightforwardly show that (1)-approximation of maximum matching and minimum vertex cover are O(1)-replicable.

Lemma 12. The problem of finding an (1)-approximation of maximum matching is 2-replicable.

Proof. To fit maximal matching into our vertex-labeling definition for problems, we characterize
it as maximal independent set on the (normal) family of line graphs, as discussed above. An
(1)-approximation of maximum matching on an input graph corresponds to an (1)-approximate
maximal independent set on its line graph. Let G be such a line graph, on n nodes, and let c > 0 be some fixed constant. We consider a graph G consisting of k  n2 copies of G and fewer than n isolated nodes. Again, G has at least kn nodes, and we note that (denoting (H) to be the size of the MIS of a graph H) (G)  k(G). Assume we have some output valid labeling L on G, which corresponds to an independent set of size at least c(G), and in which each copy of G is labeled identically, as is each isolated node. More than c(G) - n of the nodes in the independent set must be in copies of G (since there are fewer than n nodes not in copies of G). Since each of
the k copies of G is labeled identically, each must contain more than

c(G) k

-

n



ck(G) k

-

1 n



c(G) 2

nodes in the independent set (for n  2/c), and therefore the output on G is a valid (1)-

approximate MIS, corresponding to an (1)-approximate maximal matching in the input graph.

2.4 Algorithm definitions, and revised definition of component-stability
Once we have defined problems, as considered in our paper, we may define LOCAL and MPC algorithms that solve them, and in particular, give a formal, amended definition of componentstable algorithms used in this paper, taking into account the discussion above.

2.4.1 LOCAL algorithms Our formal definition of algorithms in the LOCAL model used in this paper is as follows:

Input. LOCAL algorithms receive as input an n-node graph G, with component-unique IDs for each node. Randomized algorithms also provide each node with access to a shared, unbounded, random seed S. Algorithms are provided with the exact value of the maximum degree , and an input size estimate N of n such that n  N  poly(n).7
The nodes of the input graph are the computational entities, and each initially has knowledge of its adjacent edges in G (i.e., the IDs of their other endpoints). The computation proceeds in synchronous rounds, and in each round, a node may send an arbitrary message along each of its adjacent edges. At the termination of the algorithm, each node must give an output label from .
7The reason we assume that only a polynomial estimate N of n is known here is that some LOCAL lower bounds to which we wish to apply our lifting result only hold without exact knowledge of n (e.g., that of [KKSS20]). Most LOCAL lower bounds, however, do hold under exact knowledge, and in these cases we can simply set N = n.

14

Output. Correct deterministic algorithms must always provide a valid overall output labeling for

the problem, on every output; randomized algorithms must give a valid labeling with probability

at

least

1

-

1 N

,

over

the

distribution

of

the

random

seed

S,

for

any

input.

Shared randomness. Given that MPC algorithms naturally allow shared randomness, it is important for our study of randomized LOCAL algorithms to allow the nodes to have access to shared randomness too. The use of shared randomness is non-standard in the LOCAL model, where one typically assumes only private randomness. However, as shown by Ghaffari et al. [GKU19, Section V], many of the existing LOCAL lower bounds can be extended (and without any asymptotic loss in their LOCAL round complexity) also if the nodes have access to shared randomness. (Notice that the notion of shared randomness is only relevant to randomized algorithms, and hence, for deterministic complexity one can use the existing deterministic LOCAL lower bounds without any constraints, as black box results.)

2.4.2 MPC algorithms
We use the standard definition of MPC algorithms (see, e.g., [ASS+18, BHH19, CLM+18, GGK+18, GU19, KSV10]) amended to fit the framework of low-space MPCs used in the paper.

Input. MPC algorithms receive as input a legal n-node graph G, distributed arbitrarily over poly(n) machines, each with local space O(n) for some  < 1. Randomized algorithms also provide each node with access to a shared, random seed S of poly(n) bits (again distributed arbitrarily among machines). We do not assume that maximum  or n are given explicitly as input, but MPC algorithms can determine them easily in O(1) rounds, so we may assume knowledge thereof.
Computation proceeds in synchronous rounds, and in each round, a machine first perform an arbitrary local computations on its local data and then may send and receive a total of O(n) information, divided between any other machines as desired. At the termination of the algorithm, each machine must give an output label from  for each node it received in the initial distribution.

Output. Correct deterministic algorithms must always provide a valid overall output labeling for

the problem, on every output; randomized algorithms must give a valid labeling with probability

at

least

1

-

1 n

,

over

the

distribution

of

the

random

seed

S,

for

any

input.

Computation in MPC algorithms. While we are mainly using the most standard setup of MPC algorithms, closely following, e.g., [ASS+18, BHH19, CLM+18, GGK+18, GU19, KSV10], occasionally we will use some features which (while often standard) are less commonly used.
The standard MPC model assumes that in each synchronous rounds, each machine performs arbitrary local computations on its data (fitting its local memory of size S = O(n)) and then the machines simultaneously exchange messages, in a way that each machine is sender and receiver of up to O(S) messages. While some papers also consider the sequential running time of any single MPC machine in every round, the main focus of our study is primarily on the information theoretic aspects of understanding the round complexity in MPC algorithms. (Notice that unbounded local computation assumption is standard in the classical distributed models as LOCAL, CONGEST, and CONGESTED CLIQUE.) As the result, while many of our algorithms perform only poly(n)-time computations, occasionally we will allow MPC machines to perform heavy local computations, up to

15

2O(S) local computations in a round; still, the space used on a single machine remains S = O(n). Our results show that allowing such heavy computations might provide advantageous in the context of deterministic algorithms and derandomization, however they are not necessary to find examples of component-unstable deterministic algorithms which surpass component-stable conditional lower bounds.
Furthermore, while typically one is concerned with the design of uniform MPC algorithms, as it has been observed by Fish et al. [FKL+15], the original setup of MPC (e.g., [KSV10]) leads naturally to the non-uniform model of computation. Most of the MPC algorithms presented in our paper are uniform, but occasionally we use non-uniform algorithms. In our setting, this means that the MPC algorithm, on each single machine initially knows the number of nodes n (or its estimation), and possibly different algorithms are used for different values of n. This can be also seen as having some non-uniform advice hardwired in the algorithms on individual MPC machines (or as Boolean circuits; for more details, see, e.g., Section 7.1.1 in [Vad12]).
Finally, some of the non-uniform MPC algorithms we use are also non-explicit. That is, we will be showing that there is a low-space MPC algorithm for a specific task, but we will not be able to provide a procedure to explicitly design it (there is generally a brute-force procedure obvious from the proof, but one that requires at least exponential computation, and possibly also too much space to perform in low-space MPC). In this paper, non-uniform and non-explicit MPC algorithms will be occasionally used in the context of derandomization.
2.4.3 Component-stable MPC algorithms
Now, after our discussion in Sections 2.1 to 2.3, we are ready to provide a new definition of component-stable MPC algorithms used in this paper.
Definition 13 (Component-stable MPC algorithms). A randomized MPC algorithm AMP C is component-stable if its output at any node v is entirely, deterministically, dependent on the topology and IDs (but independent of names) of v's connected component (which we will denote CC(v)), v itself, the exact number of nodes n and maximum degree  in the entire input graph, and the input random seed S. That is, the output of AMP C at v can be expressed as a deterministic function AMP C(CC(v), v, n, , S).
A deterministic MPC algorithm AMP C is component-stable under the same definition, but omitting dependency on the random seed S.
Finally, let us state the main technical result demonstrating the power of our revised framework of component-stable MPC algorithms, lifting unconditional lower bounds from the LOCAL model to conditional lower bounds for low-space component-stable MPC algorithms. The following theorem extends the main result in the component-stable algorithms framework due to Ghaffari et al. [GKU19, Theorem I.4] to our framework and enhances it to include lower bounds against deterministic algorithms, and lower bounds with dependency on maximum input degree . Informally, similarly to [GKU19, Theorem I.4], Theorem 14 below states that, conditioned on the connectivity conjecture, for O(1)-replicable graph problems, any T (N, )-round lower bound in the LOCAL model yields a (log T (N, ))-round lower bound for any low-space component-stable MPC algorithm AMPC. Furthermore, the claim holds for both randomized and deterministic algorithms (deterministic algorithms were not studied in [GKU19]).
The proof of Theorem 14 is given in Section 3 (the notion of constrained functions is defined in Definition 26).
16

Theorem 14 (Lifting LOCAL lower bounds to component-stable MPC algorithms). Let P

be a O(1)-replicable graph problem that has a T (N, )-round lower bound in the randomized LOCAL

model with shared randomness, for constrained function T , on graphs with input estimate N and

maximum degree , from some normal family G. Suppose that there is a randomized o(log T (n, ))-

round low-space component-stable MPC algorithm AMPC for solving P on legal n-node graphs with

maximum degree  from G, succeeding with probability at least 1 -

space randomized MPC algorithm A that can distinguish one n-node

in

o(log

n)

rounds,

succeeding

with

probability

at

least

1

-

1 n

.

1 n

.

Then,

there

cycle from two

exists

n 2

-node

a lowcycles

The same holds if the LOCAL lower bound and algorithm AMPC are both deterministic (but the

obtained algorithm A remains randomized).

2.5 Landscape of MPC complexity classes and component-stability
In this section we define MPC complexity classes considered in this paper. We study the MPC model introduced in Section 1 and described in details Section 2.4, focusing on low-space MPCs.
Let us begin with reminding the reader a fundamental obstacle to fully understand the computational complexity of problems in the low-space MPC setting: a seminal work of Roughgarden et al. [RVW18] showed that obtaining any unconditional lower bound in the low-space MPC setting ultimately leads to breakthrough results in circuit complexity. In view of that, we will take a more modest approach and will rely on conditional hardness results based on the widely believed connectivity conjecture. The core hardness result considered in this paper is a revised framework lifting unconditional lower bounds from the LOCAL model to conditional lower bounds for lowspace component-stable MPC algorithms (see Section 2 and Section 3, which amend the framework developed earlier in [GKU19]). In particular, this framework can be used to obtain a number of deterministic and randomized lower bounds for low-space component-stable MPC algorithms (these bounds are conditioned on the connectivity conjecture). Therefore, providing low-space non-component-stable MPC algorithms that beat these bounds will demonstrate the conditional complexity gap between low-space component-stable and non-component-stable MPC algorithms -- which is the ultimate goal of this section.
We focus on upper bound round complexity, local space, global space, and success probability which depend only on the number n of the graph's nodes in the input instance from G. Since our main focus is to study the low-space MPC regime, we consider the following definitions for MPC algorithms and MPC component-stable algorithms (see Section 2.4).

Definition 15 (DetMPC). Denote by DetMPC(T (n)) the class of all graph problems for which there is a (possibly non-uniform and non-explicit) deterministic low-space MPC algorithm A, such that for some positive constant c, algorithm A has round complexity at most c · T (n).

Definition 16 (S-DetMPC). Denote by S-DetMPC(T (n)) the subclass of DetMPC(T (n)) restricted to component-stable algorithms. That is, S-DetMPC(T (n)) is the class of all graph problems for which there is a (possibly non-uniform and non-explicit) deterministic low-space component-stable MPC algorithm A, such that for some positive constant c, algorithm A has round complexity at most c · T (n).

Definition 17 (RandMPC). Denote by RandMPC(T (n)) the class of all graph problems P for

which there is a randomized low-space MPC algorithm A, such that for some positive constant c,

algorithm

A

solves

P

with

probability

1-

1 n

and

has

round

complexity

at

most

c · T (n).

17

Definition 18 (S-RandMPC). S-RandMPC(T (n)) is the subclass of RandMPC(T (n)) restricted

to component-stable algorithms. That is, S-RandMPC(T (n)) is the class of all graph problems P

for which there is a randomized low-space component-stable MPC algorithm A, such that for some

positive

constant

c,

algorithm

A

solves

P

with

probability

1-

1 n

and

has

round

complexity

at

most

c · T (n).

Clearly, observe that for any function T (n), we have both S-DetMPC(T (n))  DetMPC(T (n)) and S-RandMPC(T (n))  RandMPC(T (n)). However, as mentioned in Introduction, it has been informally argued (for example, by Ghaffari et al. [GKU19]) that most, if not all, MPC algorithms are or can be easily made component-stable. Thus one could imagine that the pairs of sets S-DetMPC(T (n)) and DetMPC(T (n)), and S-RandMPC(T (n)) and RandMPC(T (n)) are in fact identical, especially in the regime of small T (n). However, we will demonstrate that (assuming the connectivity conjecture) this is not the case, and that some low-space general MPC algorithms can do better than their component-stable counterparts.
We begin with the study of deterministic complexity. Our first lemma shows that, informally, S-DetMPC = DetMPC (when omitting functions T (n) during complexity class comparisons, we mean that there exists some T (n) for which the comparison holds, when both classes are parameterized by T (n)). In Section 4.2.2 we consider a sinkless orientation problem for which, on one hand, (assuming the connectivity conjecture) there is no deterministic low-space component-stable MPC algorithm running in o(log log n) rounds (Theorem 38) and, on the other hand, which can be solved by a low-space deterministic MPC algorithm running in poly() + O(log log log n) rounds (Theorem 39), surpassing the component-stable lower bound for  = logo(1) log n. In fact, these results hold even for forests. Similar results are also shown for some variants of edge-coloring (Theorems 40 and 41) and vertex-coloring (Theorems 42 and 43). Further, while the main deterministic upper bounds here use heavy local computation, for bounded degree graphs their local computation is poly(n), demonstrating that component-instability helps for deterministic algorithms even using polynomial computation. (In Section 4.3, we demonstrate a similar result for the class of LOCAL extendable algorithms, showing that instability also helps in deterministic algorithms for two cornerstone problems in low-space MPC: maximal independent set and maximal matching.) This gives the following.

Theorem 19. There is some function T (n) such that, conditioned on the connectivity conjecture,

S-DetMPC(T (n)) DetMPC(T (n)) .

Next, we move to the study of randomized algorithms and show that, informally, S-RandMPC =

RandMPC. This result follows directly from our Theorem 5, which shows that some variant of

the independent set problem has a deterministic low-space non-stable constant-rounds MPC algo-

rithm and conditioned on the connectivity conjecture, there is no o(log log n)-rounds low-space

component-stable

MPC

algorithm

that

succeeds

with

probability

at

least

1

-

1 n

.

Theorem 20. There is some function T (n) such that, conditioned on the connectivity conjecture,

S-RandMPC(T (n)) RandMPC(T (n)) .

Next, we provide a conditional separation between randomized and deterministic algorithms (Theorem 29). This separation follows by combining (i) the conditional lifting for randomized component-stable algorithms and deterministic component-stable algorithm using the framework

18

in Theorem 14 with (ii) local problems for which there is provable gap in their randomized and deterministic LOCAL complexity (e.g., [BBH+19, CHL+20, CKP19]). This yields the following.
Theorem 21. There is some function T (n) such that, conditioned on the connectivity conjecture,
S-DetMPC(T (n)) S-RandMPC(T (n)) .
Further, in Section 6, we study the power of deterministic low-space component-unstable MPC algorithms and their relationship to the randomized ones. In Lemma 55 we prove that if there is a randomized MPC algorithm that solves a graph problem P on n-node graphs with maximum degree  in T (n, ) rounds, then one can derandomized such algorithm to solve the same problem in O(T (n, )) rounds. The resulting deterministic MPC algorithm is component-unstable, nonuniform, and non-explicit, and it has the same local space as that of the randomized MPC algorithm, and uses an O(n2)-factor more machines. This yields the following.
Theorem 22. DetMPC(T (n)) = RandMPC(T (n)).
Let us summarize the results in this section, assuming the connectivity conjecture, and allowing non-uniform MPC algorithms. Our study demonstrates that for low-space MPCs, componentunstable algorithms are provably stronger than their component-stable counterparts, both for deterministic and randomized algorithms (Theorems 19 and 20). Further, we see that for componentstable algorithms, randomized algorithms are provably stronger than their deterministic counterparts (Theorem 21), however, for arbitrary (possibly component-unstable) algorithms this is not the case: any randomized algorithm can be efficiently simulated by a deterministic one (Lemma 55 and Theorem 22).
3 Conditional MPC Lower Bounds from LOCAL
In this section, we present a framework lifting unconditional lower bounds from the LOCAL model to conditional lower bounds for low-space component-stable MPC algorithms, extending and revising the analysis of Ghaffari et al. [GKU19] to prove Theorem 14.
While on a high level, our analysis follows closely the approach from [GKU19], our arguments diverge in several subtle but crucial places. On a technical level, we rely on the central but also quite subtle notions of replicable graph problems, normal graph families, (D, , n, )-sensitive MPC algorithms, and a revised definition of component stability (see Definition 13). The two major reasons behind these changes are:
· to make the arguments robust against the issues we have identified concerning componentstability, and incorporate the definitional changes that these issues necessitated, and
· to extend the arguments to include lower bounds against deterministic algorithms, and lower bounds with dependency on maximum input degree .
After introducing some useful notation in Section 3.1, we show in Section 3.2 that in our setting, for R-replicable graph problems for normal graph families, lower bounds for LOCAL algorithms imply the existence of some graphs which can be distinguished by component-stable MPC algorithms only by relying on non-local information. Then, in Section 3.3, we apply this non-locality to provide a conditional MPC lower bound for component-stable MPC algorithms for O(1)-replicable graph
19

problems in our setting (for normal graph families). In particular, conditioned on the connectivity conjecture, our main result (Theorem 14) states, informally, that for O(1)-replicable graph problems, any T (N, )-round lower bound in the LOCAL model yields a (log T (N, ))-round lower bound for any low-space component-stable MPC algorithm AMPC. Furthermore, the claim holds for both randomized and deterministic algorithms.

3.1 Basic definitions: normal graph families and sensitive MPC algorithms
Definition 23. Two connected graphs G = (V, E) and G = (V , E), with center nodes v  V and v  V  respectively, are D-radius-identical if the topologies and node IDs (but not necessarily names) of the D-radius balls around v and v are identical.
Our next definition (related to [GKU19, Definition III.1], though set up in our framework), formalizes the notion of MPC algorithms depending on non-local information in the graph.
Definition 24 ((D, , n, )-sensitive MPC algorithms). For integers D, n,   0 and some   [0, 1], a component-stable MPC algorithm A for some graph problem is called (D, , n, )-sensitive with respect to two D-radius-identical centered graphs G = (V, E) and G = (V , E), with centers v  V and v  V , if the probability (over choice of seed S) that A(G, v, n, , S) = A(G, v, n, , S) is at least .
We can apply this definition also to deterministic component-stable algorithms. Since these do not depend on the random seed, any (D, , n, )-sensitive deterministic component-stable MPC algorithm with  > 0 is (D, 1, n, )-sensitive.
As one final point of notation, in this section our arguments will often involve several different graphs. For clarity, for any graph G we will denote by nG its number of nodes.

3.2 LOCAL hardness yields indistinguishability of graphs locally

Our next lemma (cf. [GKU19, Lemma III.1]) shows that for any R-replicable graph problem, a lower bound for a normal graph family for any LOCAL algorithm implies some useful property for any component-stable MPC algorithm for that problem: to distinguish some graphs from that normal family one needs to rely on non-local information, in the sense of Definition 24.

Lemma 25. For any N, , R  N, let P be an R-replicable graph problem for which there is no

T (N, )-round LOCAL algorithm (with shared randomness) that solves P on graphs from some

normal family G of maximum degree , with input size estimate N (i.e., satisfying nH  N 

poly(nH )),

and

with

probability

at

least

1

-

1 N

.

Suppose there is a component-stable MPC algorithm AMPC that solves P on all graphs G  G,

with

probability

at

least

1-

1 nG

.

Then,

there

are

two

T (N, )-radius-identical

centered

graphs

G, G



G with at sensitive

most N nodes and with respect to G,

maximum G .

degree

(exactly)

,

such

that

AMPC

is

(T

(N,

),

1 4N 2

,

N

R+2,

)-

The same claim holds if the LOCAL lower bound and algorithm AMPC are both deterministic.

Proof.

The proof is by contradiction.

Denote D = T (N, ) and  =

1 4N

2

.

Let us assume that there

are no two D-radius-identical centered graphs G, G  G with at most N nodes and maximum

degree  such that the given MPC algorithm AMPC is (D, , N R+2, )-sensitive with respect to G

and G. We use this fact to construct a D-round randomized LOCAL algorithm ALOCAL to solve P

20

on

graphs

with

valid

size

estimate

N

with

probability

at

least

1-

1 N

,

which

is

a

contradiction

to

the assumption of the lemma.

Design of the LOCAL algorithm. The input for ALOCAL is an unbounded random seed S, a graph H  G, integers N , , and the assignment of the IDs to the nodes of H, such that:

· N is the input size estimate, that is, nH  N  poly(nH ),

·  is the maximum degree of H,

· all IDs are in [poly(N )] and are component-unique.

For a fixed H, let us take an arbitrary node v and describe how to determine v's output under

ALOCAL, on a particular input I = H, S . Node v first collects its D-radius ball BD(v) in H. It then considers the family Hv of all graphs with at most N nodes, centered at v, with component-

unique IDs from [poly(N )], and maximum degree  that are D-radius-identical to BD(v), i.e., the

set of possible inputs to ALOCAL for which v would see BD(v) as its D-radius ball. For each G  Hv,

v

creates

a

simulation

graph

G

(see

Definition

9)

as

follows:

G

consists

of



N R+2 nG



 nRG

disjoint

copies of G. One of these copies is arbitrarily designated the "true" copy, and its nodes use the

IDs of G as their own IDs and names. All other copies use the same IDs, but as names they use

arbitrary unique values in [poly(N )]. We also add enough isolated nodes to raise the number of

nodes

to

exactly

N R+2

(i.e.,

N R+2

-

nG

N R+2 nG



<

nG

isolated

nodes),

sharing

the

same

arbitrary

ID, and with arbitrary unique names, in [poly(N )]. Note that this construction corresponds closely

to Definition 9 of problem replicability, and is the reason we chose such a definition.

In order to determine the output of ALOCAL at v, we will be running AMPC on the simulation graph G. For the supply of randomness, denote S to be S curtailed to the length required by AMPC, i.e., the first poly(N ) bits. Since AMPC is component-stable, its output at v on G, using seed S, is AMPC(G, v, N R+2, , S).

Then, we set

ALOCAL(H, v, S)

:=

arg

max
x

|{G



Hv

: AMPC(G, v, N R+2, , S) = x}|

,

i.e., v's output label output by ALOCAL is that returned by the most AMPC simulations.

Correctness of the LOCAL algorithm. We use our assumption that there are no two D-radiusidentical centered graphs G, G  G with at most N nodes and maximum degree  for which AMPC is (D, , N R+2, )-sensitive with respect to G and G. Hence, for any G, G  Hv, with probability at least 1 -  over choice of S, AMPC(G, v, N R+2, , S) = AMPC(G, v, N R+2, , S). In particular,
since the actual input graph H is certainly in Hv,

ES

|{G  Hv : AMPC(G, v, N R+2, , S) = AMPC(H, v, N R+2, , S)}| |Hv|

 .

Then, for the random seed S, the probability that

|{G



Hv

:

AMPC(G, v, N R+2, , S) |Hv |

=

AMPC(H, v, N R+2, , S)}|



1 2

.

21

is at most 2 by Markov's inequality. We therefore have

Pr ALOCAL(H, v, S) = AMPC(H, v, N R+2, , S)  1 - 2 .

Taking a union bound over all nodes v  V (H), we have that

Pr for all v  V (H): ALOCAL(H, v, S) = AMPC(H, v, N R+2, , S)  1 - 2N  .

In this case, the overall output of ALOCAL(H, S) is equal to the output of AMPC on the true copy of H when run on H with seed S. Since H  G (as it is a disjoint union of members of G, see Definition 7), AMPC returns a valid output on H with probability 1 - 1/nH = 1 - N -(R+2). Note that H is a disjoint union of at least nRH copies of H and fewer than nH extra isolated nodes. This satisfies the construction in the definition of R-replicability.8 Since AMPC is component-stable, it

must return the same output labeling in all copies of H in H (since they share the same topology

and IDs), and likewise must return the same output label on all isolated nodes. So, the output

labeling given on H is of the form required in the replicability definition, and so AMPC returns a

valid output on the true copy (and, indeed, on all copies) of H in H .

Then, by a union bound, ALOCAL's output is correct with probability at least 1 - 2N  -

N -(R+2)



1-

1 N

,

reaching

a

contradiction

to

our

assumption

of

a

LOCAL

lower

bound.

This

proves the randomized part of the lemma.

To prove the deterministic analogue, we perform exactly the same construction, except that

ALOCAL does not have the random seed S, and does not pass it to AMPC. Since our algorithm ALOCAL is now deterministic, we have ALOCAL(H, v) = AMPC(H, v, N R+2, ) with certainty for all v, i.e., ALOCAL's output is identical to AMPC's output on an N R+2-node graph containing H as a

connected component. Since AMPC is also deterministic, this is a valid output on H with certainty.

We have then constructed a deterministic LOCAL algorithm solving P, contradicting our initial

assumption. So, we must instead have that AMPC is (T (N, ), 1, N R+2, )-sensitive, and therefore, since it is deterministic, (T (N, ), 1, N R+2, )-sensitive.

3.3 Conditional MPC lower bounds from LOCAL (proving Theorem 14)
In this section we apply Lemma 25 to obtain conditional lower bounds for component-stable MPC algorithms for O(1)-replicable graph problems through their hardness in the LOCAL model. Our main result, Theorem 14, can be seen as a revision and extension of the main arguments due to Ghaffari et al. [GKU19] in their Theorem I.4, applied to low-space component-stable MPC algorithms for O(1)-replicable graph problems and for normal graph families.
Since the complexity of our algorithms depends on two parameters, input size estimate N and maximum degree , we define the type of functions for which we will study lower bounds:
Definition 26. We call a function T : N × N  N constrained if T (N, ) = O(log N ) for all   N and   (0, 1), and T (N c, )  c · T (N, ) for any c  1.
This is similar to the corresponding requirement T (N ) = O(log N ) in [GKU19], but allowing for dependency on . The definition of constrained functions further incorporates a `smoothness' condition, which is also implicitly required by [GKU19] (there, it is used that when T (N ) =
8When nH  2; for nH  1 Lemma 25 is trivial since there can be no LOCAL lower bounds on such graphs.

22

O(log N ), T (poly(N )) = O(T (N )); while this is true for smooth functions, it is not technically true in general).9
With the notion of constrained functions at hand, we can leverage Lemma 25 to relate the complexity of LOCAL algorithms for R-replicable graph problems in normal graph families to the connectivity conjecture. In particular, given the conjectured hardness of this problem in the MPC model, this will imply (see Theorem 14 for a precise statement) a lower bound of (log T (n, ))rounds for any low-space component-stable MPC algorithm for any O(1)-replicable graph problem in normal graph families with no T (N, )-round LOCAL algorithm.
We begin with an auxiliary lemma that relates Lemma 25 to the complexity of low-space MPC algorithms for the D-diameter s-t connectivity problem.10

Lemma 27. Let P be a O(1)-replicable graph problem that has a T (N, )-round lower bound in

the randomized LOCAL model with shared randomness, for constrained function T , on graphs with

input size estimate N and maximum degree , from some normal family G. Suppose that there is a

randomized o(log T (n, ))-round low-space component-stable MPC algorithm AMPC for solving P on

legal

n-node

graphs

with

maximum

degree



from

G,

succeeding

with

probability

at

least

1-

1 n

.

Then,

there exists a low-space randomized MPC algorithm Bst-conn with round complexity o(log T (n, ))

for T (n, )-diameter s-t connectivity on graphs with n nodes, succeeding with probability at least

1

-

1 n

.

The same holds if the LOCAL lower bound and algorithm AMPC are both deterministic (but the

obtained MPC algorithm Bst-conn is still randomized).

One point of note in the theorem statement is that we do not make any constraint on the maximum degree of the s-t connectivity instance; indeed, for that problem any node of degree over 2 can be immediately discarded, so maximum degree is largely irrelevant.  appears only in the expression for the diameter D.

Proof.

Let c be a sufficiently large constant.

We will require that P is

c 

-replicable;

since

replicability

is monotone, this will be the case when c is sufficiently large. We then construct our randomized

MPC algorithm for T (n, )-diameter s-t connectivity Bst-conn as follows:

We are given an n-node input graph H, and poly(n)-bit random seed S, for T (n, )-diameter

s-t connectivity. We set N = n/2, denote D := T (N, ), and apply Lemma 25 to obtain11 two

D-radius-identical centered graphs G, G  G with at most N nodes and maximum degree , such

that

AMPC

is

(D,

1 4N

2

,

nc/+2

,

)-sensitive

with

respect

to

G, G.

9As a counterexample, let T (N ) be a tower of 2s of height log N - 3. Then, T (N ) = O(log log N ), but

lim sup

T (N+1) T (N)

= .

N 

10The D-diameter s-t connectivity problem is formally defined in [GKU19, Definition IV.1] as follows: Given two

special nodes s and t in the graph, the goal is to determine whether s and t are connected or not, in a way that

provides the following guarantee: If s and t are in the same connected component and that component is a path

of length at most D with s and t at its two endpoints, then the algorithm should output YES, with probability

1-

1 poly(n)

.

If s and t are in different connected components, the algorithm should output NO, with probability

1-

1 poly(n)

.

In

other

cases,

the

output

can

be

arbitrary.

11Lemma 25 shows the existence of such graphs, but a uniform MPC algorithm must also be able to find them (a

non-uniform algorithm can simply have them `hard-coded' in). To do so, we can run a brute-force search on each

machine; O(n) space is required to identify each of the 2O(n) pairs of graphs and check the sensitivity property.

However, we require 2O(n) local computation (which is also implicitly the case in [GKU19]). For specific lower

bounds there may be efficient constructions, but this must be determined on a case-by-case basis.

23

Our algorithm Bst-conn will work by combining the results of several parallel simulations, run on disjoint groups of machines. We first divide the seed S equally among these simulations; by choosing our input poly(n) seed length sufficiently high, we can guarantee that each simulation is given sufficient randomness. In each simulation, each node in v  V (H) uses part of the random seed assigned to the simulation to independently, uniformly randomly, choose a value h(v)  [D].
For each simulation we now construct a pair of graphs GH and GH on which to run AMPC. In each case we begin with the graph H, and discard all nodes of degree higher than 2. Then, for each remaining node in H we collect its radius-1 neighborhood onto a machine (this takes O(1) rounds and does not increase asymptotic space usage). Each node of H then drops out if its radius1 neighborhood is not consistent with being part of an s - t path with consecutively increasing values of h along the path (other than the value of h(t), which we make no requirements for). In particular:
· Nodes s and t in H must have degree 1 (and otherwise we terminate the simulation immediately and output NO).
· Other nodes must have degree 2, and the values of h in their radius-1 neighborhood must be a consecutive triplet (other than h(t)).
Now, every remaining node (i.e., satisfying these conditions) in H is assigned (copies of) nodes in G and G as follows:
· Node s in H is assigned all nodes of distance at most h(s) from v in G and G.
· Node t in H is assigned all nodes of distance more than D from v in G and G.
· Other nodes u in H are assigned the nodes of distance exactly h(u) from v in G and G.
We can now define the new graphs GH and GH as follows: The node sets of GH and GH consist of every assigned copy of nodes in G and G respectively. We also add to GH and GH an extra, full, copy of G, disconnected from the rest of the graph, whose sole purpose is to ensure that the maximum degree of GH and GH is exactly , and enough isolated nodes to raise the number of nodes (currently O(nN ) = O(n1+/2)) to exactly nc/+2 (which is larger for sufficiently large constant c). Let wu in GH denote a copy of w in G, assigned to node u in H. We place an edge between wu and w^u^ in GH iff w G w^ and u H u^ (or u = u^). As IDs, for a simulated node wu, we use the original ID of w. We will show that these IDs remain componentunique in GH and GH . As names, we use (name(u), name(w)); these names are guaranteed to be fully unique. We arbitrarily choose fully unique names and IDs for the extra isolated nodes and copy of G. The graphs GH and GH can easily be constructed in O(1) rounds of MPC, using O(n) local space, since machines each need only store G, G, and O(n) edges of H. We construct GH from G analogously. We see that the connected components of GH and GH are isomorphic to induced subgraphs of G and G respectively. Furthermore, the IDs of nodes in these components are identical to the relevant subgraphs of G and G, and are therefore component-unique (as they were in G and G). In particular, we consider the topology and IDs of the connected components CC(vs) and CC(vs) containing vs in GH and GH respectively:
24

1. If s and t are endpoints of a path of p  D + 1 nodes (and therefore of length at most D) in H, h(s) = p - D, and h(ud) = p - D + d for each ud = t which is the node at distance d along the path from s, then CC(vs) = G and CC(vs) = G;
2. otherwise, CC(vs) = CC(vs).

In the first case, this is because the nodes on the s - t path are together assigned precisely one

copy of each on the nodes in G and G, in such away that every two adjacent nodes in G and G

are assigned to adjacent nodes on this path. In the second case, the only nodes that differ in G and

G are assigned only to t, and no outputs of h can cause these to be connected to vs. Therefore the

graphs of nodes that are connected to vs are identical. We now simulate AMPC on GH and GH using a sufficiently large poly(n)-length part of S
allocated to this simulation as the random seed (the same seed for both GH and GH ). Since GH and GH are in the normal family G (they are disjoint unions of induced subgraphs of G, G  G), these are valid inputs for AMPC.

If s and t are endpoints of a path of length at most D in H, then the probability that all

nodes v on this path choose the `correct' value h(v) (i.e., as specified in point 1 above) is at

least D-D = N -o(1). In this case, we have CC(vs) = G  G and CC(vs) = G  G. Since

AMPC is component-stable, its output for vs in each case is equal to AMPC(G, v, nc/+2, , S) and

AMPC(G, v, nc/+2, , S)

respectively,

and

by

sensitivity,

with

probability

at

least

1 4N

2

,

these

are

different. If this occurs, the simulation output, and the overall output, is correctly YES.

If s and t are not endpoints of a path of length at most D in H, then in all simulations we

have CC(vs) = CC(vs). Since AMPC is component-stable, its output depends (deterministically) on these connected components (as well as equal values of , random seed S, and nGH = nGH = nc/+2), and is therefore the same in both cases. The output of all simulations is then correctly NO.

The final output for Bst-conn is as follows: after running poly(n)-many concurrent simulations,

we return YES if any simulation returned YES, and NO otherwise. We have already shown that

if s and t are not endpoints of a path of length at most D in H, all simulations will return NO,

so Bst-conn will correctly output NO. It remains to show that Bst-conn is also correct with high

probability on YES instances.

Each simulation has a D-D = N -o(1) probability of the nodes in the s-t path choosing the correct

ivsalautesleoafsth(av)4,N1a3ndpraob4Na1b2 iplitryoboafbtilhiteysoimf rueltautrionninrgedtuiffrnerinengt

outputs on GH and GH . YES. Since simulations

Therefore, there use independent

randomness, and we can run sufficiently large poly(n) many of them in parallel, we can amplify the

total

probability

of

success

to

1-

1 n

as

required.

The round complexity of Bst-conn is dominated by simulating AMPC on graphs with O(nc/+2)

nodes and maximum degree , which, using that T is a constrained function, is O(T (nc/+2, )) =

O(T (n, )).

While Lemma 27 already provides a strong conditional lower bound for O(1)-replicable graph problems in our setting, the lower bound is conditioned on the complexity of solving D-diameter s-t connectivity. However, we can apply the framework [GKU19] to extend the analysis to condition on the more standard connectivity conjecture -- to conclude our analysis with the result stated in Theorem 14.

Proof of Theorem 14. This reduction from Lemma 27 is taken directly from Section V in [GKU19]. First, we notice that [GKU19, Lemma IV.1] applies to any MPC algorithm, i.e., is not affected by

25

our changes to the definition of component-stability. Therefore, assuming that any low-space MPC

algorithms distinguishing one n-node cycle from two obtain that any low-space MPC algorithm that solves

n 2

-node

cycles

requires

(log n)

D-diameter s-t connectivity for

rounds, we D  log n

for a constant   (0, 1) requires (log D) rounds. By setting D = T (n, ), we can combine this

fact with Lemma 27 to conclude the proof of the theorem.

3.4 Applications of Theorem 14: conditional MPC lower bounds

Similarly as was been demonstrated by Ghaffari et al. [GKU19], the lifting arguments in Theorem 14, when combined with the known lower bounds for LOCAL algorithms, lead to a host of conditional lower bounds for low-space component-stable MPC algorithms for many natural graph problems. In particular, we can recover the main applications of Ghaffari et al. [GKU19] (see Theorems I.1, I.2, and I.3 there):

Theorem 28. Assuming the connectivity conjecture (that no low-space randomized MPC algorithm

can distinguish one

ity

at

most

1 n

),

the

n-node cycle from two following lower bounds

n 2

-node

cycles

in

o(log n)

rounds,

with

failure

probabil-

hold for randomized low-space component-stable MPC

algorithms:

· (log log n) rounds for a constant approximation of maximum matching (even on forests), a constant approximation of vertex cover, a maximal independent set;

· (log log log n) rounds for ( + 1)-coloring, unless the deterministic complexity of ( + 1)coloring in the LOCAL model is 2logo(1) log n;

· (log log log n) rounds for the Lov´asz Local Lemma problem, or even for the special case of sinkless orientation where we should orient the edges of a d-regular graph, where d  4, such that each node has at least one outgoing edge.

Proof. First, we notice that Ghaffari et al. [GKU19] proved the following lower bounds for the randomized LOCAL model with shared randomness:

· ( log n/ log log n) rounds for a polylog(n)-approximate solution for the minimum vertex cover, maximum matching, or minimum dominating set problem, and for finding a maximal matching (even on trees) or a maximal independent set (Theorem V.1 and Corollary V.2 in [GKU19]); 
· ( log log n) rounds for ( + 1)-coloring unless the deterministic complexity of ( + 1)coloring in the LOCAL model is o(log n) (indirectly in Corollary V.4 in [GKU19]);

· (log log n) rounds for the Lov´asz Local Lemma problem, or even for the special instance known as sinkless orientation where we should orient the edges of a d-regular graph, where d  4, such that each node has at least one outgoing edge [GKU19, Corollary V.5].

The proof of Theorem 28 now follows immediately by combining these LOCAL lower bound to Theorem 14, by taking each time G to be the set of all graphs, and by noticing that all the problems considered are O(1)-replicable (see Section 2.3.1, and for example, Lemma 10 for LCLs and Lemma 12 for approximate maximum matching). The only issue worth mentioning is that since the class of trees is not normal (see Definition 7), the lifting of the LOCAL lower bound for

26

finding a maximal matching on trees extends only to forests, that is, that any randomized low-space component-stable MPC algorithm that returns a constant approximation of maximum matching on forests has (log log n) rounds complexity.
Comparing the results claimed by Ghaffari et al. [GKU19] (Theorems I.1, I.2, and I.3) with those proven in Theorem 28, there are only two differences: firstly, our lower bound for matching holds only for forests, while [GKU19] claimed it for trees (for discussion of the reason for this difference, see Section 2.2), and secondly, our result for ( + 1)-coloring is updated to hold on a weaker conjecture of no 2logo(1) log n-round deterministic LOCAL algorithm for ( + 1)-coloring (since the original result in [GKU19] conditioned on the 2o( log n)-deterministic hardness, which was recently disproved in [RG20]).
3.4.1 Deterministic conditional MPC lower bounds
Finally, observe that Theorem 14 can be directly combined with existing deterministic LOCAL lower bounds to obtain deterministic lower bounds for low-space deterministic MPC algorithms. (Note that unlike in the case of randomized MPC algorithms above, one can directly apply deterministic LOCAL lower bounds as black-box in Theorem 14, since the issue of shared randomness is of no importance for deterministic algorithms.) For example, we apply it in Theorem 48 to give (log  + log log n)-round lower bounds for deterministic low-space component-stable MPC algorithms for maximal matching or maximal independent set (conditioned on the connectivity conjecture). Similarly, we present deterministic conditional lower bounds of (log log n) rounds for low-space component-stable MPC algorithms for sinkless orientation, (2-2)-edge coloring, and -vertex coloring, holding even in forests (Theorems 38, 40, and 42). For some more lower bounds to which the framework is applicable, see, e.g., the recent deterministic LOCAL lower bounds for ruling sets in [BBO20].
4 Separation between Stable and Unstable Deterministic MPC
We start by presenting a general statement for characterization of the graph family of local problems for which component-unstable MPC algorithms provably help. Specifically, this includes problems for which there is a provable exponential gap between their LOCAL deterministic and randomized complexities (e.g., as shown in [BBH+19, CHL+20, CKP19]). Notice that the obtained MPC algorithms are non-unform.
Theorem 29. Let P be a O(1)-replicable graph problem. Let Tr(N ) and Td(N ) be the randomized and deterministic, respectively, LOCAL round complexity of P on bounded-degree graphs with n nodes, with an input size estimate N and exact knowledge of . If Tr(N ) < log N for some constant   (0, 1), and if there is a provable exponential gap between Tr(N ) and Td(N ), then, assuming the connectivity conjecture, there is an exponential gap between component-stable and component-unstable deterministic low-space MPC complexities for solving P.
Proof. Since Tr(N ) < log N for some constant   (0, 1) (see Definition 26), by Theorem 14, conditioning on the connectivity conjecture, any deterministic low-space component-stable MPC algorithm for P requires (log(Td(n))) rounds. We will show that there exists a deterministic low-space component-unstable MPC algorithm for P running in O(log(Tr(n))) rounds using an n2 factor more machines (the algorithm is possibly non-uniform and non-explicit).
27

We use Lemma 54, which says that it is sufficient to show a randomized low-space MPC algorithm for P that runs in O(log(Tr(n))) rounds and succeeds with probability at least 1 - 2-n2. The algorithm employs n2 parallel repetitions of procedure B, each uses a disjoint set of machines and a

distinct random coins determined by a shared random string of length poly(n). The final output is

determined by picking the most successful repetition (i.e., with the maximal number of legal node

outputs). Procedure B simply collects the 2Tr(n) ball of each node u  V (G), which can be done in

O(log(Tr(n))) rounds. The machines then simulate the randomized local algorithm on balls using

the shared seed. This completes the description of the simulation. We first show that this algorithm succeeds with probability 1 - 2-n2. By the properties of the

LOCAL

algorithm,

a

single

repetition

succeeds

with

probability

at

least

1-

1 n

(over

the

randomness

of the shared seed). Thus, by employing n2 independent repetitions, we get the probability that

all these repetitions fail is less than 2-n2.

Notice that since Tr(n) = o(log n) and the input graph is bounded-degree, the 2Tr(n)-ball of each

node indeed fits the local space of each machine, and thus the MPC algorithm can be implemented

with no(1) local space, and hence is low-space. Finally, let us observe that the obtained algorithm

is component-unstable, since it relies on globally agreeing on the outcome of all repetitions.

Notice that Theorem 29 implies Theorem 19, that is, demonstrates that, assuming the connectivity conjecture, there are some graph problems for which there are deterministic low-space component-unstable MPC algorithms that are provably faster than their component-stable counterparts. However, the weakness of Theorem 29 is that the obtained deterministic low-space component-unstable MPC algorithms are non-unform. In the rest of this section we will address this issue and show that a similar claim holds also for uniform deterministic MPC algorithms, and those which use almost-optimal global space.

4.1 Derandomization tools
In this section we present some basic derandomization tools used in our analysis.

4.1.1 Strongly (, k)-wise independent hash functions

We will use the notion of strongly (, k)-wise independent hash functions which are defined as follows:

Definition 30. Let A and B be two sets and let   0. A family H of functions A  B is strongly (, k)-wise independent if, when h  H is chosen uniformly at random, for any t  k and for any t distinct x1, . . . , xt  A and any (not necessarily distinct) y1, . . . , yt  B, we have

Pr

[h(xi)

=

yi, 1



i



t]

-

1 |B|t

 .

That is, the probability of any particular t  k outputs of a random function from H differs from what one would expect under uniform full randomness only by . In our applications, we will choose  = n-c for sufficiently large constant c, and we can then assume that these outputs are indeed fully independent (since the statistical difference is far below the failure probability of our algorithms).
Kurosawa et al. [KJS01] presented the following result showing the existence of strongly (, k)wise independent families of size (and therefore requiring few bits to specify):

28

Theorem 31 ([KJS01]). For any sets A, B, positive integer k, and positive real , there exists a strongly (, k)-wise independent family H of size

2O(log

log

|A|+k

log

|B|+log

1 

)

.

Furthermore,

each

h



H

can

be

specified

and

evaluated

on

any

input

in

polylog(|A|, |B|,

2k ,

1 

)

time

and space.

Proof. As noted in the proof of Theorem 2.9 of [KJS01], by applying propositions 1.2 and 2.3 of [KJS01] an (, k)-wise independent family of the desired size can be constructed from the independent sample spaces of Alon et al. [AGHP92].

4.1.2 Pseudorandom generators
A Pseudorandom Generator (PRG) is a function that gets a short random seed and expands it to a long one which look random, in the sense that it is indistinguishable from a random seed of the same length for such a formula.
Definition 32 (Computational indistinguishability, Definition 7.1 in [Vad12]). Random variables X and Y taking values in {0, 1}m are (t, ) indistinguishable if for every non-uniform algorithm T running in time at most t, we have | Pr[T (X) = 1] - Pr[T (Y ) = 1]|  .
Definition 33 (PRG, Definition 7.3 in [Vad12]). A deterministic function G : {0, 1}d  {0, 1}m is an (t, ) pseudorandom generator (PRG) if (1) d < m, and (2) G(Ud) and Um are (t, ) indistinguishable.
The following proposition uses the probabilistic method to show the existence of good PGRs.
Proposition 34 (Proposition 7.8 in [Vad12]). For all m  N and  > 0, there exists a (non-explicit) (m, ) pseudorandom generator G : {0, 1}d  {0, 1}m with seed length d = (log m + log(1/)).
We next notice that (m, ) PRGs of Proposition 34 can be computed by exhaustive search.
Lemma 35 (Time and space complexity of perfect PRGs). For all m  N and  > 0, there exists an algorithm for computing the (m, ) PRG of Proposition 34 in time exp(poly(m/)) and space poly(m/).
Proof. Our goal is to find an (m, ) PRG from {0, 1}d to {0, 1}m with seed length d = (log m + log 1/), and the existence of one such PRG follows from Proposition 34.
The procedure for computing the PRG iterates over all different functions G : {0, 1}d  {0, 1}m according to some fixed order. For each fixed function G : {0, 1}d  {0, 1}m, it iterates over all non-uniform algorithms A running in time m according to some fixed order, as well. To examine if G -fools A (in the sense of Definition 33), it is required to compare the output distribution of A run on a random m-bit string to the m-bit pseudo-random string obtained by evaluating the function G on a random d-bit vector. This is done by computing the output of algorithm A under each G(x) for every x  {0, 1}d, as well as evaluating the algorithm A under each y  {0, 1}m. The procedure picks the PRG function G : {0, 1}d  {0, 1}m that -fools all m-time algorithms; the existence of one such G follows from Proposition 34.
We next bound the time and space complexity of this procedure. There are (2m)2d = 2O(m2d) different functions G mapping d bits into m bits, and there are 2O(m log m) non-uniform algorithms

29

running in time m (i.e., Boolean circuits of size m). The time complexity is bounded by 2O(m2d) · 2O(m log m) · 2d · 2d = 2poly(m/).
We next bound the space complexity. To iterate over all the possible PRG candidates G : {0, 1}d  {0, 1}m, one needs to store the index of the current candidate function and its representation, which uses O(m · 2d) bits. To iterate over all the m-time algorithms according to some
fixed order, one needs to store the index of the current A, and its representation which can be
done with O(m log m) bits. Fixing the candidate PRG function to G and the given m-algorithm to A, the evaluation of the algorithm A under each G(x) for every x  {0, 1}d can be done using O(m · 2d) space. Finally, in order to evaluate A under each y  {0, 1}m, it is sufficient to store the current index of the vector y considered. Altogether, the space requirements is O(m · 2d); since
d = (log m + log(1/)), this is poly(m/).

4.2 Problems related to the Lov´asz Local Lemma
We first demonstrate a deterministic complexity separation between component-stable and componentunstable algorithms for a group of problems related to the distributed Lov´asz Local Lemma: sinkless orientation, ( + o())-edge coloring, and o()-coloring triangle-free graphs.
These problems are known to be hard in the LOCAL model via proofs based on the round elimination technique [CHL+20]. We show that by derandomizing an algorithm for the constructive Lov´asz Local Lemma and plugging this result into known algorithms for the problems, we can surpass the component-stable lower bounds we obtain when lifting the LOCAL lower bounds to low-space MPC.

4.2.1 Algorithmic Lov´asz Local Lemma

We first present a deterministic low-space component-unstable MPC algorithm for the Lov´asz Local Lemma which uses heavy local computations to obtain a small number of MPC rounds (though for bounded-degree graphs, computation is still polynomial in n). Later we will demonstrate that this algorithm can be applied to obtained similar MPC algorithms for sinkless orientation, edge-coloring, and vertex-coloring.
The algorithmic Lov´asz Local Lemma (LLL) is defined as follows:

Definition 36. Consider a set V of independent random variables, and a family X of n (bad) events on these variables. Each event A  X depends on some subset V(A)  V of variables. Define the dependency graph GX = (X , {(A, B) | V(A)  V(B) = }) that connects any two events which share at least one variable. Let d be the maximum degree in this graph, i.e., each event A  X shares variables with at most d other events B  X . Finally, define p = maxAX Pr [A].
The Lov´asz Local Lemma shows that Pr AX A¯ > 0 (i.e., there is some assignment of variables that does not satisfy any of the bad events) under the LLL criterion that epd  1. Our algorithmic goal is to find such an assignment of the variables (possibly under a stronger criterion).

We give the following deterministic low-space MPC algorithm.

Lemma

37.

Any

LLL

instance

with

d

=

logo(1) n

and

p



1 C

d-C

(for

sufficiently

high

constant

C), in which each bad event has poly(d) dependent variables, and these variables are independent

fair random bits, can be solved in poly(d) + O(log log log n) rounds in deterministic low-space MPC, using n1+o(1) global space and npoly(d) local computation.

30

Proof. We derandomize the LOCAL algorithm of Fischer and Ghaffari [FG17]. The algorithm consists of an O~(d) + log n initial deterministic coloring procedure from [FHK16], an O(d2)-round randomized pre-shattering part, and then a polyloglog(n)-round deterministic post-shattering part (which follows from plugging the network decomposition result of [RG20] into the derandomization framework of [GHK18] applied to the LLL algorithm of [CPS17]; see [RG20] for details). We first collect the O(d2 + polyloglog(n))-radius ball around each event onto a single machine; these balls are of size dO(d2+polyloglog(n)) = no(1), and each event's dependence on its variables can be specified in 2poly(d) = no(1) bits, so the balls fit on single machines. This collection process can be performed in O(log d + log log log n) rounds by graph exponentiation.
We can now directly and immediately simulate the initial coloring, and we will also be able to directly simulate the deterministic post-shattering part, so it remains to derandomize the preshattering part. We do so using hash functions. First, note that the proof of the Shattering Lemma [FG17, Lemma 6], giving the crucial shattering property of the randomized part, requires independence only between the dependent variables of groups of O(poly(d) log n) events at a time (the O(1)-radius neighborhoods of groups of O(log n) events). So, it requires variables to be sampled from their distributions with only (poly(d) · log n)-wise independence.
By Theorem 31, for any constant c  N, there exists a strongly (n-c, poly(d) log n)-wise independent family H of functions [poly(n)]  {0, 1} of size 2poly(d) log n, i.e., requiring poly(d) log n bits to specify each function. We use such a function to generate the values taken by all variables. By taking c to be sufficiently high, we can ensure that the statistical error of the hash functions in negligible. Since the algorithm of [FG17] succeeds with high probability in n when variables are sampled independently at random, we have that using a uniformly random function from H to provide variable values also causes the algorithm to succeed w.h.p.
We can therefore perform a distributed implementation of the method of conditional expectations to deterministically fix a function which causes the algorithm to succeed. [CDP20a, CDP20b] show how to implement this method, in low-space MPC, in such a way that (log n) bits specifying the function can be fixed in a single round, provided success at any node, under any function from H, can be checked locally on a machine. Here, we can check success locally by simply running the algorithm of [FG17] to completion using the given function, since we have already collected sufficiently large local neighborhoods onto single machines.
We therefore require poly(d) MPC rounds to perform the method of conditional expectations and derandomize the LLL algorithm. So, the total round complexity is poly(d) + O(log log log n). The global space usage is dominated by the storage of an O(d2 + polyloglog(n))-radius ball around each node, i.e., ndO(d2+polyloglog(n)) = n1+o(1) global space. The local computation is dominated by the method of conditional expectations, which requires evaluating all functions from H, taking npoly(d) computation.
4.2.2 Sinkless orientation
In this section, we combine the lifting of the LOCAL lower bounds to deterministic low-space component-stable MPC algorithms (Theorem 14) with derandomization of the constructive Lov´asz Local Lemma (Lemma 37) to show that conditioned on the connectivity conjecture, deterministic low-space component-unstable MPC algorithms for sinkless orientation are provably faster than their component-stable counterparts.
We define sinkless orientation to be the problem of orienting the edges of a graph of minimum degree at least 3, such that each node has at least one outgoing edge (this minimum degree criterion
31

is necessary, since the problem is not possible, e.g., on a path). A lower bound for sinkless orientation in the LOCAL model was first proven by [BFH+16],
and extended to a stronger bound for deterministic algorithms by [CKP19]. When combined with Theorem 14, we obtain the following theorem.
Theorem 38. Assuming the connectivity conjecture, there is no deterministic component-stable low-space MPC algorithm that computes a sinkless orientation in o(log log n) rounds, even in forests.
Proof. By [BFH+16] and [CKP19], there is an (log N ) lower bound for deterministic sinkless orientiation even in -regular forests, and this holds even if nodes have unique O(log N )-bit IDs and know the exact values of n and . Since sinkless orientation is an edge-labeling problem, we must work on the line graph of the input graph in order to meet our vertex-labeling framework. We therefore let G be the (normal) class of all line graphs of forests, and, setting T (N, ) := log1/3 N (which is a constrained function) we have an T (N, )-round LOCAL lower bound for the vertexlabeling version on line graphs. By Theorem 14, we obtain an (log(T (N, ))) = (log log n)round lower bound for deterministic low-space MPC algorithms, conditioned on the connectivity conjecture. Converting back from the line graph formulation to the original input graphs, this lower bound is on the family of forests.
Theorem 38 is complemented by the following result providing a deterministic low-space componentunstable MPC algorithm for sinkless orientation that surpasses the component-stable lower bound for low-space MPC.
Theorem 39. There is a deterministic low-space MPC algorithm that computes a sinkless orientation, in any graph of  = logo(1) log n maximum degree, in poly()+O(log log log n) = o(log log n) rounds, using n1+o(1) global space. The algorithm is component-unstable, and uses npoly() local computations.
Proof. We concentrate on the algorithm for the case of d-regular graphs with d > 500; Ghaffari and Su [GS17] show how extend to irregular graphs with minimum degree at least 3 using O(log n) rounds of deterministic pre-processing and O(1) rounds of deterministic post-processing in LOCAL, which we can simulate directly in low-space MPC.
In this case, sinkless orientation can be solved by a single application of the distributed LLL (see e.g. [BFH+16]), with d =  and p = 2- (setting d > 500 is more than sufficient to make this instance satisfy the LLL criterion of Fischer and Ghaffari's algorithm), and the underlying variables are simple uniformly random choices of orientation for each edge (so can be represented with a single fair random bit). Applying Lemma 37, we obtain a poly() + O(log log log n)-round deterministic low-space MPC algorithm using npoly() local computation.
We also remark that, for   no(1/ log log n), an O(log log log n)-round component-stable randomized algorithm exists for the problem, by simply collecting (log log n)-radius balls onto machines via graph exponentiation, and then simulating the randomized LOCAL algorithm of [GS17]. Sinkless orientiation is therefore an example of a problem with a (conditional) separation between randomized and deterministic algorithms, proving Theorem 21
32

4.2.3 Edge-coloring

Similarly to the sinkless orientation problem, the framework combining the lifting of the LOCAL lower bounds to deterministic low-space component-stable MPC algorithms (Theorem 14) with derandomization of the constructive Lov´asz Local Lemma (Lemma 37) can be used to show that assuming the connectivity conjecture, for the classical edge-coloring there are deterministic lowspace component-unstable MPC algorithms that are provably faster than their component-stable counterparts. We begin with the following application of Theorem 14.

Theorem 40. Assuming the connectivity conjecture, there is no deterministic component-stable
low-space MPC algorithm that computes a (2 - 2)-edge coloring, even in forests, in o(log log n) rounds.

Proof. Chang et al. [CHL+20] give a deterministic LOCAL lower bound of (log N ) for (2 - 2)edge coloring, even in forests. To fit (2 - 2)-edge coloring into our framework (in which problem

outputs are labels on nodes) we must define it as vertex-coloring on the line graph. In LOCAL,

operations on the line graph can be simulated in the original graph and vice versa with only one

round additive overhead. So, we have an (log N )-round deterministic lower bound for `(2 - 2)coloring the line graphs of forests with maximum degree '.

The family of line graphs of forests is both hereditary (deleting a node in the line graph corre-

sponds to deleting an edge in the original graph, and forests are closed under edge deletion) and

closed under disjoint union (which corresponds to disjoint union in the original graph), so it is nor-

mal under Definition 7. We set T (N, ) := log1/3 N , a constrained function. Then, by Theorem 14, we obtain a conditional (log log n) deterministic lower bound for (2 - 2)-edge coloring forests with n nodes and maximum degree .

Theorem

41.

There

is

a

deterministic

low-space

MPC

algorithm

that

computes

a

(

+

 

log3

)-

edge coloring, in any graph of maximum degree  = logo(1) log n, in O(poly()+log  log log log n) =

o(log log n) rounds, using n1+o(1) global space. The algorithm is component-unstable and uses npoly() local computation.

Proof.

We

apply

the

algorithm

of [CHL+20].

Setting

=

log3  

in

Theorem

4

of

[CHL+20]

gives

a running time dominated by O(log ) applications of LLL, with d = poly() and p = -(1).

Furthermore, the variables in the LLL instances are uniformly random choices of colors from each

edge's palette (of some current palette size P ). These can be generated from poly(d) fair random

bits in such a way that the probability of choosing any particular color differs from 1/P by at most

2- poly(d); the probability of each bad event therefore also incurs error of 2- poly(d), and so remains

-(1).

Applying Lemma 37, we can perform all applications of LLL in O(poly() + log  log log log n)

rounds in deterministic low-space MPC with n1+o(1) global space. The algorithm of [CHL+20]

then applies a final 5-edge coloring algorithm to finish off the remaining graph (where  is the maximum degree of this graph); we can do this in O(log n) rounds by simulating Linial's

(deterministic) vertex-coloring algorithm [Lin92] on the line graph. The total round complexity is

O(poly() + log  log log log n), the global space is n1+o(1), and the local computation is npoly().

33

4.2.4 Vertex-coloring triangle-free graphs

A combination of Theorem 14 and Lemma 37 can be used to show a similar complexity gap between deterministic component-stable and component-unstable MPC algorithms for vertex coloring.

Theorem 42. Assuming the connectivity conjecture, there is no deterministic component-stable low-space MPC algorithm that computes a -vertex coloring, even in forests, in o(log log n) rounds.

Proof. Chang, Kopelowitz, and Pettie [CKP19] give an (log N )-round deterministic lower bound for -coloring trees (and therefore also the normal class of forests) of maximum degree  (for  
3). Setting T (N, ) := log1/3 N (a constrained function), by Theorem 14, we obtain a conditional (log log n) deterministic lower bound for -coloring forests with n nodes and maximum degree
.

Theorem

43.

There

is

a

deterministic

low-space

MPC

algorithm

that

computes

a

O(

 log 

)-vertex

coloring, in any triangle-free graph of  = logo(1) log n maximum degree, in poly()+O(log  log log log n) =

o(log log n) rounds, using n1+o(1) global space. The algorithm is component-unstable and uses npoly() local computations.

Proof. We plug in our LLL algorithm into the algorithm of [PS15]. When  = o(log n), this

algorithm consists of O(k + log n) applications of LLL

k



(

1 4

- o(1)) ln .

We

choose

some

such

k

=

(log ).

in

order

to

O(

 k

)-color

the

graph,

where

Similarly to the proof of Theorem 41, the LLL instances required have d = poly() and p =

-(1), and the variables are uniformly random color choices, which we can generate from poly(d)

fair coins while only increasing the probability of any bad event by 2- poly(d). So, we can apple

Lemma

37,

and

obtain

a

deterministic

low-space

MPC

algorithm

for

O(

 log 

)-coloring

triangle-free

graphs in O(poly() + log  log log log n) rounds, using n1+o(1) global space, with npoly() local

computation.

For all the problems above, sinkless orientation, edge-coloring, and vertex-coloring, we have obtained component-unstable algorithms which surpass the conditional lower bounds for componentstable algorithms when  = logo(1) log n. Furthermore, though in general these algorithms use heavy local computation, for bounded degree ( = O(1)) graphs their local computation is poly(n). Since we still surpass the lower bounds for sufficiently large constant , this demonstrates that component-instability helps for deterministic algorithms even using polynomial computation.

4.3 Extendable algorithms
We next describe an explicit derandomization recipe for a particular class of local algorithms. This recipe allows one to derandomize r-round local algorithms within O(log r) low-space MPC rounds, provided that the r-radius ball of each node in the graph G fits the local space of the machines. As a consequence, we show component-unstable algorithms for maximal independent set and maximal matching that surpass the lower bounds for component-stable algorithms.
We call the class of LOCAL algorithms we consider extendable. Roughly speaking, these algorithms can extend any partial legal solution (e.g., a collection of legally colored nodes) into a complete solution (similar to the notion of greedy algorithms). In addition, the local computation performed at each node in these algorithms must be polynomial. Even though the LOCAL model

34

does not account for the local computation time, most of the LOCAL algorithms are in fact efficient in this regard. We next define this notion more formally.

Definition 44 (Extendable algorithms). Let A be a randomized LOCAL algorithm for an LCL problem P with round complexity at most T (n, ) (where T is non-decreasing in n and ) on every n-node graph, when provided with the exact value of n and maximum degree . Then, A is extendable if:

(i) Any partially decided subgraph can be extended into a global solution for P. Formally, A returns an output labeling on G giving each node a label in L  {}, where L is the output alphabet permitted by the problem P. Then, re-labeling the nodes labeled  with any valid output labeling on their induced graph must give a valid output labeling on G. This must hold with certainty (even though A is a randomized algorithm).

(ii)

In

expectation,

A

labels

less

than

1 2

nodes

.

(iii) In every round i of A, each node u performs at most O(T (n,)) internal computation to determine its output for that round (e.g., the messages to be sent in the next round, internal states, and its final output in the last round).

We next show that using the PRG construction of Lemma 35 any extendable LOCAL algorithm running in t = T (n, ) rounds can be simulated deterministically within O(log t) MPC-rounds. This has various applications for derandomizing LOCAL algorithms on low-degree graphs, which consequently yields a separation between stable and unstable deterministic MPC algorithms. For simplicity, we consider LOCAL algorithms for LCL problems, however, this can be extended for the approximation variants of LCL problems (e.g., approximate max-IS and maximal matching).

Theorem 45. For every constant   (0, 1), there is some constant  such that any T (n, )round extendable local algorithm A satisfying that ·T (n,)  n, in which each node uses at most O(T (n,)) bits of randomness, can be derandomized12 within O(log(T (n, )) + log n) MPC rounds with local space O(n) and global space O(n1+).

Proof. Consider a fixed n-node graph G = (V, E), with maximum degree , and let t := T (n, ).
First, the MPC algorithm allocates a separate machine Mu to each node u that stores its 2t-radius ball in G. This can be done in O(log t) rounds, by the standard graph exponentiation technique. Next, the algorithm computes a 4t-coloring in the graph G2t. The purpose of this step is to reduce
the name space of the nodes from O(log N ) bits into O(t log ) bits, such that in each 2t-radius
ball, the new IDs of the nodes (i.e., their colors) are unique. This coloring can be implemented within O(log N ) deterministic rounds [Kuh09].
We will simulate O(1) iterations of algorithm A, each time running on the subset of nodes labeled with  in the last iteration. That is, we start on our input graph G, and provide A with the values n and , and in subsequent iterations provide the current values n  n and   . By extendability of A, we know that the resulting labeling can be extended to a valid full solution by relabeling  with any valid labeling on their induced graph. It remains only to show that after O(1) iterations, we can ensure that no nodes remain labeled .

12The deterministic MPC algorithm in the derandomization relies on the PRG construction in Lemma 35, and therefore it may perform heavy local computations, or alternatively, this result can be stated as a non-uniform derandomization where the PRG function is hard-coded in the MPC machines, in which case local computation is polynomial.

35

In each iteration of A we let machine Mu determine the output for node u, and we use the PRG

construction of Lemma 35 in order to deterministically fix a good random seed for the algorithm.

We need to provide each node with O(t) random bits, and we do so based on the new IDs (i.e.,

nodes with the same new IDs receive the same random bits, but since these nodes are of distance

at least 2t apart, this does not cause any dependency issues). So, our PRG will need to generate

O(t) total pseudorandom bits.

By Lemma 35, a (O(t), n-) PRG can be constructed using eO(t log + log n) space. Setting 

to be a sufficiently small constant, this is O(n). Note that we always use this PRG (i.e., with

parameters in terms of original number of nodes n), and do not update to the current value n.

When we run A, with a uniformly random seed from the PRG, on a graph of size n  n (since we run only on the induced graphs of subsets of nodes labeled ), the output at each node

is (O(t), n-) indistinguishable from its output under full randomness. So, the expected number

of

nodes

which

output



is

at

most

1 2

+ n · n-.

After

O(1/)

iterations

we

therefore

reduce

the

expected number of nodes labeled  below 1.

Now it remains only to deterministically choose a seed from the PRG that achieves this expected

value of undecided nodes (and, since the number of undecided nodes is an integer, it must then be 0).

To do so, we use a distributed implementation of the classical method of conditional expectations,

a means of having all machines agree on some seed which achieves at most the expected value of

some cost measure (in this case, the number of nodes labeled ). [CDP20a, CDP20b] show how to

implement this method, in low-space MPC, in such a way that (log n) bits specifying the seed can

be fixed in a single round, provided that the global cost measure is the sum of functions computable

locally on single machines. Here, each machine Mu can locally compute the indicator variable for the event {u is labeled } under A using any particular seed, and the global cost function is the

sum (over all nodes u  V ) of these variables.

Since the seeds for the PRG are O(t log  +  log n) = O(log n) bits long, we can perform the

method of conditional expectations in O(1) rounds in each iteration, fixing a globally agreed seed

from our PRG which ensures that the number of nodes labeled  is at most its expectation. Then,

after

O(

1 

)

=

O(1)

iterations,

we

have

no

remaining

nodes

labeled

,

and

so

have

a

complete

valid

solution for the problem. The total running time of the deterministic MPC algorithm is therefore

O(log t) (for initially collecting balls) plus O(log n) (for coloring).

4.3.1 Application to maximal independent set and maximal matching
To demonstrate the applicability of this derandomization recipe, we show how it can be used to improve the deterministic running times of two cornerstone problems in low-space MPC: maximal independent set and maximal matching. The best prior round complexities for both problems is O(log  + log log n) [CDP20a]; here we improve the dependency on n to triple-logarithmic, and surpass the component-stable lower bound for  = 2logo(1) n.
Theorem 46. For any constant  > 0, a maximal independent set and maximal matching can be found deterministically in low-space MPC in O(log log  + log log log n) rounds when  = 2logo(1) n, with local space O(n) and global space O(n1+).
Proof. We focus on maximal independent set, since the results for maximal matching will then follow by reduction.
We will use the LOCAL algorithm of Ghaffari [Gha16], which, when combined with the polylogarithmic network decomposition of [RG20], runs in t = O(log  + polyloglog(n)) rounds, on graphs

36

with n nodes, when provided with the values n and , succeeding (globally) with probability at

least

1-

1 n2

.

We use the following final labeling:

nodes which have been placed in the output

independent set are labeled IN, adjacent nodes are labeled OUT, and all other nodes are labeled .

Ghaffari's algorithm has the following important property: even when the algorithm probabilis-

tically fails, it never places two adjacent nodes in the output independent set; instead it merely

fails to decide the status of some nodes. Therefore, any output labeling produced by Ghaffari's

algorithm in this way is extendable to a full solution by any valid output (i.e., a valid MIS) on the

induced graph of undecided nodes (those labeled ).

Another property we need is a bound on the number of random bits used by each node. Ghaf-

fari's algorithm uses O(t log ) random bits per node (O(log ) per round i), since a node v's only

random choice each round is to `mark' itself with some probability pt(v), which is equal to 2-ki for some ki  [log ]. To perform this choice, v can take ki random bits and mark itself if they are

all 0.

no

Furthermore, nodes ), the

since

with

probability

at

least

1-

1 n2

the

expected number of nodes labeled  is at

algorithm succeeds

most

n n2

<

1 2

.

globally

(i.e.,

labels

Therefore the algorithm is extendable. We have O(t) = 2logo(1) N  N , and so we can apply

Theorem 45 to solve MIS in O(log t) = O(log log  + log log log n) rounds deterministically in MPC

with local space O(n) and global space O(n1+).

To perform Maximal Matching, we use standard reduction of finding an MIS on the line graph

of the input graph. It is well-known that this corresponds to a maximal matching in the original

graph, and in LOCAL only increases the round complexity by 1, which means that we still adhere

to our MPC space bounds.

Corollary 47. For any constant  > 0, a maximal independent set and maximal matching can be found deterministically in low-space MPC in O(log  + log log log n) rounds, with local space n and global space n1+.

Proof. We use Theorem 46 when  = 2logo(1) n; otherwise, we can use the low-space MPC algorithm of [CDP20a], with running time O(log  + log log n) = O(log ).

Theorem 48. Assuming the connectivity conjecture, there is no deterministic low-space componentstable MPC algorithm that computes a maximal matching or maximal independent set in o(log  + log log n) rounds.

Proof. Balliu et al.

[BBH+19]

show

an

(min{,

log N log log N

})-round13

deterministic

LOCAL

lower

bound for both problems, even when exact values of n and  are known. Using constrained function

T (N, ) = min{, log N }, we apply Theorem 14 to obtain an (min{log , log log N })-round

conditional lower bound for component-stable low-space deterministic MPC algorithms, (directly

for maximal independent set, and using the standard conversion to the line graph for maximal

matching).

Hence, Theorem 46 surpasses this component-stable conditional lower bound when  = 2logo(1) n.

13That

is,

a

deterministic

LOCAL

algorithm

cannot

simultaneously

have

round

complexity

o()

and

o(

log N log log N

);

algorithms do exist which are faster in one parameter at the expense of the other.

37

5 Separation between Stable and Unstable Randomized MPC

In this section, we demonstrate the existence of a natural problem for which there is a (conditional)

gap between randomized component-stable and component-unstable algorithms. This will give us

a proof of Theorem 5.

We consider the task of computing large independent sets in n-node graphs with maximum

degree . Recently, [KKSS20] has shown that for any n, there exists n-node graphs with maximum

degree  = (n/ log n), for which any randomized LOCAL algorithm for computing an independent

set of size (n/) with success

of

1

-

1 log

n ),

requires

(log

n)

probability 1 - rounds. (Here,

1 n

(in

fact,

even

reaching

a

weaker

success

guarantee

and throughout this section, we assume that   1

so that the problem is well-defined.) Since the cardinality of the maximum independent set in their

lower bound graph can be bounded by O(n/), their result can also be stated as a lower bound

for computing a constant approximation for the maximum independent set. We start by providing

a mild adaptation to the lower bound proof of [KKSS20] so that it would fit the framework from

Section 2 and from [GKU19]. In particular, to be able to lift this LOCAL lower bound into the

MPC setting from Theorem 14 in Section 2 and from [GKU19], it is required for the lower bound to

hold even if nodes are equipped with shared randomness, and with an estimate N on the number

of nodes n which is at least more than a log N -factor loose, i.e., nodes cannot distinguish between

n = (N/ log N ) and n = (N ). We begin with the following fact implicit in Corollary V.4 of

[GKU19].

Fact 49 ([GKU19]). Any randomized LOCAL algorithm to compute a maximum independent set in n-node cycle graph requires (log n) rounds; this holds even if the nodes know n (and, naturally,
know  = 2) and even if they have access to an unlimited amount of shared randomness.

We can then plug this lower bound, strengthened to hold against shared randomness, into the result of [KKSS20], yielding Theorem 50. The proof is identical to Theorem 4 of [KKSS20]; it is unaffected by the change to shared randomness.

Theorem 50 ([KKSS20]). Any randomized LOCAL algorithm to compute an independent set of size (n/) in all n-node graphs (i.e., over the full range of   [1, n]) requires (log n) rounds.
This holds even if the nodes have the exact maximum degree , access to an unlimited amount of
shared randomness, and an input size estimate N of n which is more than a log N -factor loose,
i.e., they cannot distinguish between n = (N/ log N ) and n = (N ).

We can now combine Theorem 50 and Lemma 11 with the lifting argument in Theorem 14, using the constrained function T (N, ) := log N , to obtain the following lower bound for component-
stable MPC algorithms.

Lemma 51 (Super-constant lower bound for component-stable IS). Assuming that the connectivity

conjecture holds, there is no o(log log n)-round low-space component-stable MPC algorithm that

computes an independent set of size (n/), in all graphs with n nodes and   [1, n), with

success

probability

at

least

1

-

1 n

.

Finally, we show that there is a very simple (component-unstable) MPC algorithm for computing large independent sets in a constant number of rounds.

38

O(1)-round randomized algorithm. We first give a O(1)-round randomized algorithm that computes an independent set of (n/) nodes, in expectation. By running c log n independent repetitions of this step (simultaneously in parallel) for some sufficiently large constant c, we get an independent set of cardinality (n/)) with high probability. First, each node computes its degree, which can be done in O(1) rounds. The algorithm then consists of a single step of Luby's algorithm (see [Lub86]), where each node v picks a number v  [0, 1] uniformly at random. A node v joins the independent set if v < u for every neighbor u of v. This can be verified in O(1) rounds in the low-space MPC setting. We next observe that the probability that a node v has the minimum v value in its neighborhood is at least 1/(dv + 1)  1/( + 1). Thus, in expectation, the number of nodes that joins the independent set is at least n/. By Markov inequality, it holds that w.h.p. at least one of the c log n independent repetitions computes an independent set with at least n/2( + 1) nodes.

O(1)-round deterministic algorithm. We slightly change the algorithm described above so that it would work with pairwise independence, and hence to become deterministic.

Claim 52. Consider a simulation of a single step of Luby's algorithm with pairwise independence. Then, the expected number of independent set nodes is n/(4 + 1).

Proof. For a node v we consider the event that v < 1/(2) and that u  1/(2) for every neighbor u of v. Note that if this event occurs then v joins the independent set. To bound
the probability of this event for node v, we first bound the probability that u  1/(2) for every neighbor u conditioned on the event that v < 1/(2). Due to pairwise independence, for every neighbor u, it holds that u  1/(2) with probability 1/2 even when conditioning on v < 1/(2). Thus, conditioned on v < 1/(2), by the union bound, the probability that v has some neighbor u with u  1/2 is at most 1/2. Overall, the probability for the event to hold is 1/4.

This step can be derandomized by applying O(1) steps of graph sparsification, exactly in the same manner as done for the derandomization of Luby's step in the maximal independent set algorithm of [CDP20a]. We therefore have the following:

Theorem 53. There is a deterministic low-space MPC algorithm that in O(1) rounds computes an independent set of size (n/), in all graphs on n nodes with  = [1, n].

Proof Sketch. Let  > 0 be a constant sufficiently smaller than . If  > n, then by the determin-

istic sparsification framework of [CDP20a], we can derandomize the subsampling of each node with

probability

n 

,

using

O(1)-wise

independent

hash

functions

coupled

with

the

method

of

conditional

expectations, in such a way that:

·

The

number

of

nodes

in

the

resulting

sparsified

graph

is

(

n1+ 

);

· The maximum (induced) degree in the sparsified graph is O(n).

Since degrees are now low enough to fit 2-hop neighborhoods onto single machines (as  is

sufficiently lower than ), we can now derandomize a step of Luby's algorithm on this subsampled

graph (or on the original graph if   n) exactly as in [CDP20a]. In doing so, we find an

independent

set

of

size

(

n1+ 

/n

)

=

(

n +1

).

39

6 General Non-Uniform and Non-Explicit Derandomization of MPC Algorithms

The conditional lifting arguments for component-stable MPC algorithms in Theorem 14 imply that for some (O(1)-replicable, and hence, e.g., LCLs) graph problems there is a (conditional) exponential gap between randomized and deterministic algorithms, e.g., for problems that admit such an exponential gap in the LOCAL model [CHL+20, CKP19]. In this section we provide a proof of Theorem 22 that in the MPC model with polynomially many machines, no such gap exists from a complexity perspective. That is, we show that any randomized low-space MPC algorithm with round complexity T (n, ) and which uses a polynomial number of machines, can be transformed into a deterministic MPC algorithm that runs in O(T (n, )) rounds and also uses a polynomial number of machines. This deterministic algorithm is component-unstable, and it is also non-uniform and non-explicit.
We begin with an auxiliary lemma that adapts the approach for LOCAL algorithms from [CKP19] (see also [GK19]) to MPC algorithms. (Whereas in [CKP19, GK19] this claim holds only for LCL problems, in our case we will use it in Lemma 55 for any problem whose solution can be verified efficiently in the MPC setting.)

Lemma 54 (Implicit in [CKP19]). Let A be (a possibly non-uniform) randomized MPC algorithm
that solves a graph problem P on n-node graphs with maximum degree  in T (n, ) rounds, with probability at least 1-2-n2 . Then, there is a non-uniform, non-explicit deterministic MPC algorithm A that solves P on n-node graphs with maximum degree  in O(T (n, )) rounds.
The local and global space of algorithm A is asymptotically the same as that of A.

Proof Sketch. As we defined it in Section 2.4.2, the randomized MPC algorithm A has access to shared randomness S of polynomial length. Once the string S is fixed, A is deterministic. Let Gn, be the family of graphs with at most n nodes and maximum degree . Notice than |Gn,|  2n2. Therefore, if we run A on each seed and each possible seed fails on at least one graph in Gn,, the success guarantee of the algorithm A cannot be better than 1 - 1/2n2. Therefore, since A succeeds with probability at least 1 - 1/2n2 , there must be at least one seed S that when provided to the MPC algorithm, the algorithm is correct for every graph in Gn,.
This gives us a deterministic algorithm A, which uses the seed S to solve P on every graph in Gn,. Note that the resulting deterministic algorithm is non-explicit in the sense that the desired seed should be hard-coded into the machines. Furthermore, it is non-uniform in the sense that a
different seed is hard-coded for each n, and so we use a different algorithm for each n hard-code
seeds for all possible n into the same algorithm (in order to fit within our poly(n) global space
bound).

Lemma 55. Let A be (a possibly non-uniform) randomized MPC algorithm that solves a graph

problem P on n-node graphs with maximum degree  in T (n, ) rounds, with probability at most

1

-

1 n

using n local space and n global space.

In addition, assume that the correctness of the

algorithm can be checked in T (n, ) MPC rounds, using n local space and n global space. Then,

there exists a component-unstable, non-uniform, non-explicit deterministic MPC algorithm A that

solves P on n-node graphs with maximum degree  in O(T (n, )) rounds. The local space of A is n and the global space is O~(n2+).

40

Proof. We first boost the success guarantee of A by running multiple parallel simulations: we will

run  = O(n2) simulations of algorithm A in parallel, using a distinct set of machines for each

simulation. We can determine the correctness of all simulations in T (n, ) MPC rounds, and the

final output is determined by choosing an arbitrary correct simulation, if one exists. The probability

that

the

algorithm

fails

in

all

these

simulations

is

at

most

(

1 n

)

=

2-(n2 ) .

Thus,

if

we

incorporate

this algorithm in Lemma 54, there exists a non-uniform, non-explicit deterministic MPC algorithm

A that solves the problem in T (n, ) rounds using the same local space as A and O~(n2+) global

space.

Let us finally mention that the obtained algorithm A is component-unstable, since it relies on

globally agreeing on the outcome of all simulations.

Now Lemma 55 yields the proof of Theorem 22; informally, DetMPC = RandMPC holds in the regime of polynomially many machines and for non-uniform, non-explicit low-space MPC algorithms.

7 Conclusions
In this paper, we investigate the power of component-instability for solving local graph problems in the low-space MPC model. Our main conclusion is that component instability is useful mainly in two (possibly related) aspects: amplification of the success guarantee and derandomization. In the context of randomized algorithms, it allows one to boost the success guarantee of the algorithm. This appears to be useful especially for approximation problems (e.g., maximizing or minimizing a subset of edges or vertices with a given property). As we note, it is unlikely to help in reducing the complexity for LCL problems. In the context of derandomization, it allows one to efficiently simulate the randomized local algorithm while globally foraging for a short seed. Amplification and derandomization are both obtained by a global computation regardless of the connected components of the graph. A major open problem left by this work is to provide conditional hardness results for graph problems (say, first, for LCL problems) that hold also for unstable algorithms.

References
[ABB+19] Sepehr Assadi, MohammadHossein Bateni, Aaron Bernstein, Vahab Mirrokni, and Cliff Stein. Coresets meet EDCS: Algorithms for matching and vertex cover on massive graphs. In Proceedings of the 30th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1616­1635, 2019.
[AGHP92] Noga Alon, Oded Goldreich, Johan H°astad, and Ren´e Peralta. Simple constructions of almost k-wise independent random variables. Random Structures & Algorithms, 3(3):289­304, 1992.
[ANOY14] Alexandr Andoni, Aleksandar Nikolov, Krzysztof Onak, and Grigory Yaroslavtsev. Parallel algorithms for geometric graph problems. In Proceedings of the 46th Annual ACM Symposium on Theory of Computing (STOC), pages 574­583, 2014.
[ASS+18] Alexandr Andoni, Zhao Song, Clifford Stein, Zhengyu Wang, and Peilin Zhong. Parallel graph connectivity in log diameter rounds. In Proceedings of the 59th IEEE Symposium on Foundations of Computer Science (FOCS), pages 674­685, 2018.

41

[BBH+19] Alkida Balliu, Sebastian Brandt, Juho Hirvonen, Dennis Olivetti, Mika¨el Rabie, and Jukka Suomela. Lower bounds for maximal matchings and maximal independent sets. In Proceedings of the 60th IEEE Symposium on Foundations of Computer Science (FOCS), pages 481­497, 2019.

[BBO20] Alkida Balliu, Sebastian Brandt, and Dennis Olivetti. Distributed lower bounds for ruling sets. In Proceedings of the 61st IEEE Symposium on Foundations of Computer Science (FOCS), pages 365­376, 2020.

[BFH+16] Sebastian Brandt, Orr Fischer, Juho Hirvonen, Barbara Keller, Tuomo Lempi¨ainen, Joel Rybicki, Jukka Suomela, and Jara Uitto. A lower bound for the distributed Lov´asz Local Lemma. In Proceedings of the 48th Annual ACM Symposium on Theory of Computing (STOC), pages 479­488, 2016.

[BHH19] Soheil Behnezhad, MohammadTaghi Hajiaghayi, and David G. Harris. Exponentially faster massively parallel maximal matching. In Proceedings of the 60th IEEE Symposium on Foundations of Computer Science (FOCS), pages 1637­1649, 2019.

[BKS13]

Paul Beame, Paraschos Koutris, and Dan Suciu. Communication steps for parallel query processing. In Proceedings of the 32nd ACM SIGMOD Symposium on Principles of Database Systems (PODS), pages 273­284, 2013.

[CDP20a] Artur Czumaj, Peter Davies, and Merav Parter. Graph sparsification for derandomizing massively parallel computation with low space. In Proceedings of the 32nd Annual ACM Symposium on Parallelism in Algorithms and Architectures (SPAA), pages 175­185, 2020.

[CDP20b] Artur Czumaj, Peter Davies, and Merav Parter. Simple, deterministic, constant-round coloring in the congested clique. In Proceedings of the 39th ACM Symposium on Principles of Distributed Computing (PODC), pages 309­318, 2020.

[CFG+19] Yi-Jun Chang, Manuela Fischer, Mohsen Ghaffari, Jara Uitto, and Yufan Zheng. The complexity of ( + 1) coloring in Congested Clique, massively parallel computation, and centralized local computation. In Proceedings of the 38th ACM Symposium on Principles of Distributed Computing (PODC), pages 471­480, 2019.

[CHL+20] Yi-Jun Chang, Qizheng He, Wenzheng Li, Seth Pettie, and Jara Uitto. Distributed edge coloring and a special case of the constructive Lov´asz Local Lemma. ACM Transactions on Algorithms, 16(1):8:1­8:51, January 2020.

[CKP19] Yi-Jun Chang, Tsvi Kopelowitz, and Seth Pettie. An exponential separation between randomized and deterministic complexity in the LOCAL model. SIAM Journal on Computing, 48(1):122­143, 2019.

[CLM+18] Artur Czumaj, Jakub La¸cki, Aleksander Ma¸dry, Slobodan Mitrovi´c, Krzysztof Onak, and Piotr Sankowski. Round compression for parallel matching algorithms. In Proceedings of the 50th Annual ACM Symposium on Theory of Computing (STOC), pages 471­484, 2018.

42

[CPS17] Kai-Min Chung, Seth Pettie, and Hsin-Hao Su. Distributed algorithms for the Lov´asz Local Lemma and graph coloring. Distributed Computing, 30(4):261­280, 2017.

[DG04]

Jeffrey Dean and Sanjay Ghemawat. MapReduce: Simplified data processing on large clusters. In Proceedings of the 6th Conference on Symposium on Opearting Systems Design & Implementation (OSDI), pages 137­149, 2004.

[FG17]

Manuela Fischer and Mohsen Ghaffari. Sublogarithmic distributed algorithms for Lov´asz Local Lemma, and the complexity hierarchy. In Proceedings of the 31st International Symposium on Distributed Computing (DISC), pages 18:1­18:16, 2017.

[FHK16] Pierre Fraigniaud, Marc Heinrich, and Adrian Kosowski. Local conflict coloring. In 2016 IEEE 57th annual symposium on foundations of computer science (FOCS), pages 625­634. IEEE, 2016.
[FKL+15] Benjamin Fish, Jeremy Kun, A´ d´am D´aniel Lelkes, Lev Reyzin, and Gy¨orgy Tura´n. On the computational complexity of MapReduce. In Proceedings of the 29th International Symposium on Distributed Computing (DISC), pages 1­15, 2015.

[GGK+18] Mohsen Ghaffari, Themis Gouleakis, Christian Konrad, Slobodan Mitrovi´c, and Ronitt Rubinfeld. Improved massively parallel computation algorithms for MIS, matching, and vertex cover. In Proceedings of the 37th ACM Symposium on Principles of Distributed Computing (PODC), pages 129­138, 2018.

[Gha16]

Mohsen Ghaffari. An improved distributed algorithm for maximal independent set. In Proceedings of the 27th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 270­277, 2016.

[GHK18] Mohsen Ghaffari, David G. Harris, and Fabian Kuhn. On derandomizing local distributed algorithms. In Proceedings of the 59th IEEE Symposium on Foundations of Computer Science (FOCS), pages 662­673, 2018.

[GK19]

Mohsen Ghaffari and Fabian Kuhn. On the use of randomness in local distributed graph algorithms. In Proceedings of the 38th ACM Symposium on Principles of Distributed Computing (PODC), pages 290­299, 2019.

[GKU19] Mohsen Ghaffari, Fabian Kuhn, and Jara Uitto. Conditional hardness results for massively parallel computation from distributed lower bounds. In Proceedings of the 60th IEEE Symposium on Foundations of Computer Science (FOCS), pages 1650­1663, 2019.

[GS17]

Mohsen Ghaffari and Hsin-Hao Su. Distributed degree splitting, edge coloring, and orientations. In Proceedings of the 28th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 2505­2523, 2017.

[GSZ11]

Michael T. Goodrich, Nodari Sitchinava, and Qin Zhang. Sorting, searching, and simulation in the MapReduce framework. In Proceedings of the 22nd International Symposium on Algorithms and Computation (ISAAC), pages 374­383, 2011.

43

[GU19]

Mohsen Ghaffari and Jara Uitto. Sparsifying distributed algorithms with ramifications in massively parallel computation and centralized local computation. In Proceedings of the 30th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1636­1653, 2019.

[IBY+07] Michael Isard, Mihai Budiu, Yuan Yu, Andrew Birrell, and Dennis Fetterly. Dryad: Distributed data-parallel programs from sequential building blocks. SIGOPS Operating Systems Review, 41(3):59­72, March 2007.

[KJS01]

Kaoru Kurosawa, Thomas Johansson, and Douglas R. Stinson. Almost k-wise independent sample spaces and their cryptologic applications. Journal of Cryptology, 14(4):231­ 253, September 2001.

[KKSS20] Ken-ichi Kawarabayashi, Seri Khoury, Aaron Schild, and Gregory Schwartzman. Improved distributed approximation to maximum independent set. In Proceedings of the 34th International Symposium on Distributed Computing (DISC), pages 35:1­35:16, 2020.

[KMW06] Fabian Kuhn, Thomas Moscibroda, and Roger Wattenhofer. The price of being nearsighted. In Proceedings of the 17th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 980­989, 2006.

[KSV10]

Howard J. Karloff, Siddharth Suri, and Sergei Vassilvitskii. A model of computation for MapReduce. In Proceedings of the 21st Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 938­948, 2010.

[Kuh09]

Fabian Kuhn. Weak graph colorings: Distributed algorithms and applications. In Proceedings of the 21st Annual ACM Symposium on Parallelism in Algorithms and Architectures (SPAA), pages 138­144, 2009.

[Lin92]

Nathan Linial. Locality in distributed graph algorithms. SIAM Journal on Computing, 21(1):193­201, February 1992.

[LMSV11] Silvio Lattanzi, Benjamin Moseley, Siddharth Suri, and Sergei Vassilvitskii. Filtering: A method for solving graph problems in MapReduce. In Proceedings of the 23rd Annual ACM Symposium on Parallelism in Algorithms and Architectures (SPAA), pages 85­94, 2011.

[Lub86] Michael Luby. A simple parallel algorithm for the maximal independent set problem. SIAM Journal on Computing, 15(4):1036­1053, 1986.

[NS95]

Moni Naor and Larry J. Stockmeyer. What can be computed locally? SIAM Journal on Computing, 24(6):1259­1277, 1995.

[Ona18] Krzysztof Onak. Round compression for parallel graph algorithms in strongly sublinear space. CoRR abs/1807.08745, 2018.

[PS15]

Seth Pettie and Hsin-Hao Su. Distributed coloring algorithms for triangle-free graphs. Information and Computation, 243:263­280, 2015.

44

[RG20]

V´aclav Rozhon and Mohsen Ghaffari. Polylogarithmic-time deterministic network decomposition and distributed derandomization. In Proceedings of the 52nd Annual ACM Symposium on Theory of Computing (STOC), pages 350­363, 2020.

[RVW18] Tim Roughgarden, Sergei Vassilvitski, and Joshua R. Wang. Shuffles and circuits (on lower bounds for modern parallel computation). Journal of the ACM, 65(6):41:1­41:24, November 2018.

[Vad12] Salil P. Vadhan. Pseudorandomness. Foundations and Trends in Theoretical Computer Science, 7(1-3):1­336, 2012.

[Whi12] Tom White. Hadoop: The Definitive Guide. O'Reilly Media, Inc., 2012.

[ZCF+10] Matei Zaharia, Mosharaf Chowdhury, Michael J. Franklin, Scott Shenker, and Ion Stoica. Spark: Cluster computing with working sets. In Proceedings of the 2nd USENIX Workshop on Hot Topics in Cloud Computing (HotCloud), 2010.

45

