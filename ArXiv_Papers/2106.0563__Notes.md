
# IID-GAN: an IID Sampling Perspective for Regularizing Mode Collapse

[arXiv](https://arxiv.org/abs/2106.0563), [PDF](https://arxiv.org/pdf/2106.0563.pdf)

## Authors

- Liangliang Shi
- Yang Li
- Junchi Yan

## Abstract

Despite its success, generative adversarial networks (GANs) still suffer from mode collapse, namely the generator can only map latent variables to a partial set of modes of the target distribution. In this paper, we analyze and try to regularize this issue with an independent and identically distributed (IID) sampling perspective and emphasize that holding the IID property for generation in target space (i.e. real data) can naturally avoid mode collapse. This is based on the basic IID assumption for real data in machine learning. However, though the source samples $\mathbf{z}$ obey IID, the target generation $G(\mathbf{z})$ may not necessarily be IID. Based on this observation, we provide a new loss to encourage the closeness between the inverse source from generation, and a standard Gaussian distribution in the latent space, as a way of regularizing the generation to be IID. The logic is that the inverse samples back from target data should also be IID for source distribution. Experiments on both synthetic and real-world data show the superiority and robustness of our model.

## Comments



## Source Code

Official Code



Community Code



## Bibtex

```tex
@misc{shi2021iidgan,
      title={IID-GAN: an IID Sampling Perspective for Regularizing Mode Collapse}, 
      author={Liangliang Shi and Yang Li and Junchi Yan},
      year={2021},
      eprint={2106.00563},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```

## Notes

Type your reading notes here...

