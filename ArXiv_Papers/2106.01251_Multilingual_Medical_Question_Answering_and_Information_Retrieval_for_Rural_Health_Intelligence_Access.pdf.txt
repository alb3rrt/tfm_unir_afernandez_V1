arXiv:2106.01251v1 [cs.CL] 2 Jun 2021

Published as a workshop paper at ICLR 2021
MULTILINGUAL MEDICAL QUESTION ANSWERING AND INFORMATION RETRIEVAL FOR RURAL HEALTH INTELLIGENCE ACCESS
Vishal Vinod*, Susmit Agrawal*, Vipul Gaurav, Pallavi R. & Savita Choudhary Sir M. Visvesvaraya Institute of Technology Bengaluru, India pro.vishalvinod@gmail.com {susmit600, vipul 1mv16cs124, pallavi cs, savitha cs}@sirmvit.edu
ABSTRACT
In rural regions of several developing countries, access to quality healthcare, medical infrastructure, and professional diagnosis is largely unavailable. Many of these regions are gradually gaining access to internet infrastructure, although not with a strong enough connection to allow for sustained communication with a medical practitioner. Several deaths resulting from this lack of medical access, absence of patient's previous health records, and the unavailability of information in indigenous languages can be easily prevented. In this paper, we describe an approach leveraging the phenomenal progress in Machine Learning and NLP (Natural Language Processing) techniques to design a model that is low-resource, multilingual, and a preliminary first-point-of-contact medical assistant. Our contribution includes defining the NLP pipeline required for named-entity-recognition, language-agnostic sentence embedding, natural language translation, information retrieval, question answering, and generative pre-training for final query processing. We obtain promising results for this pipeline and preliminary results for EHR (Electronic Health Record) analysis with text summarization for medical practitioners to peruse for their diagnosis. Through this NLP pipeline, we aim to provide preliminary medical information to the user and do not claim to supplant diagnosis from qualified medical practitioners. Using the input from subject matter experts, we have compiled a large corpus to pre-train and fine-tune our BioBERT based NLP model for the specific tasks. We expect recent advances in NLP architectures, several of which are efficient and privacy-preserving models, to further the impact of our solution and improve on individual task performance.
1 INTRODUCTION
Several rural regions have a severe shortage of doctors (Sharma, 2015). Even in places where a practitioner is available, they have to attend to an abnormally large number of patients daily, constraining the quality of the diagnosis and resulting in severely overworked practitioners. In several regions, patients need to travel long distances to neighboring villages even for simple and common consultations. For long-standing health conditions, medical practitioners require a detailed health record for the patient, which is more often than not unavailable or completely absent. Many of the health conditions, deficiencies, and symptoms can be identified early, and preventable treatment can be undertaken at early stages to avert several deaths stemming from the lack of awareness. The growing internet penetration rates are a big positive since it allows patients from even remote regions to access information on the internet, although still constrained by the language barrier in many regions. Thus, there is a need for a solution that is easily accessible, that provides accurate medical information in indigenous languages, can store information from previous doctor interactions and provide advisory information and as a result reduce the accessibility problem in rural healthcare.
Equal contribution
1

Published as a workshop paper at ICLR 2021
Access is a challenging problems to solve primarily because of the various privacy and ethical concerns surrounding a medical application that provides information as a question-answer system and stores sensitive medical information from previous interactions with a doctor (to aid their diagnosis of the individual). We are transparent with our aim that a first-point-of-contact medical advisory assistant cannot, and will not supplant the diagnosis and advice from a trained medical practitioner. We only aim to reduce the information accessibility gap by leveraging the latest advances in NLP and Deep Learning to design a medical assistant that can be used offline, is resource efficient, multilingual (for vernacular languages), and is accurate in its associated tasks such as NER (Named Entity Recognition), RE (Relation Extraction), QA (Question Answering), NMT (Neural Machine Translation) and generative pre-training for query processing and providing a human-like final response. The medical corpus and the BERT-based model for question-answering and encoding onto an embedding space must be effectively trained with a large and reliable corpus of trusted medical information. We believe that such a dialogue representation model can be an excellent enabler for rural regions to improve their understanding of diseases. Accurate access to health information in vernacular languages can prevent avoidable deaths by identifying problems early.
2 RELATED WORK
Advances in NLP have been progressing rapidly towards model architectures that are domain and task agnostic leading to a general language model that can be pre-trained and fine-tuned for downstream domain-specific tasks. This progress has been influenced by the seminal works on sequenceto-sequence models (Sutskever et al., 2014), transformer models (Vaswani et al., 2017), and memory architectures. More recently, the BERT (Bi-directional Encoder Representation for Transformer) (Devlin et al., 2018) masked language model (MLM) has become one of the popular NLP architectures for NER (Named Entity Recognition), relation extraction, and question-answering. BERT is a general MLM that can be further pre-trained for domain-specific language representation and fine-tuned for specific tasks. The BioBERT architecture (Lee et al., 2020) is a contextualized MLM that is a pre-trained BERT on a biomedical corpus. The pre-training leverages the general language representations in BERT that is transferable across domains. BioBERT is pre-rained on PubMed abstracts (4.5 Billion words) and PMC articles (13.5 Billion words), which makes it a strong candidate for our question-answer embedding and information retrieval use-case.
The medical reports and clinical data are much more complicated to general medical terms and features included in the deep representations of the clinical medical text and embedding. ClinicalBERT (Huang et al., 2019) identifies the unique sequence of tokens and has a self-attention mechanism that outperforms BioBERT in modeling notes from medical practitioners. Such an implementation would be useful for our application to store information regarding previous interactions with medical practitioners. XLNET (Yang et al., 2019) showed that the capability of bi-directional modeling of contexts and denoising auto-encoding based pretraining like BERT achieves better performance over pretraining approaches based on auto-regressive language modeling. XLNET enables learning bi-directional contexts by maximizing the expected likelihood of all permutations of the factorization order. Although XLNET outperforms BERT-based models on several language tasks, we have used the BioBERT architecture owing to its ease-of-use and the option to further fine-tune on tasks required for our architecture. We use the GPT-2 architecture (Radford et al., 2019) for generative pretraining to generate natural language answers from our information retrieval component.
LABSE (language-agnostic BERT Sentence Embed-ding) (Feng et al., 2020) is a BERT architecture trained as an MLM (Masked Language Model) and a TLM (Translation Language Model) on a translation ranking task to perform language translation with a 500k token vocabulary onto 109 different languages. The input and target are encoded using a shared transformer embedding, forcing both to equal representation. Prior work (Olvera-Lobo & Gutie´rrez-Artacho, 2011; Lai et al., 2018; Daniel et al., 2019; Arivazhagan et al., 2019) has discussed the applicability of English for query processing and hence we use LABSE only for input and output query translation. This multilingual architecture can enable inclusive healthcare for all and improve patient participation in healthcare. Several other more recent architectures such as GPT-3 (Brown et al., 2020), Reformer (Kitaev et al., 2020), DistillBERT (Sanh et al., 2019), and PRADO (Kaliamoorthi et al., 2019) show great promise in terms of efficient language modeling and performance on QA and information retrieval when made fully accessible. The DistillBERT model can be used for extractive summarization.
2

Published as a workshop paper at ICLR 2021
3 METHODOLOGY
Figure 1: (a) Proposed architecture of the medical QA system; (b) The Question-Answer module. The proposed model architecture builds on the BioBERT (Lee et al., 2020), LABSE (Feng et al., 2020), and GPT-2 (Radford et al., 2019) model architectures for multi-lingual question answering and information retrieval. We use the LABSE model over NMT (Neural Machine Translation) because LABSE is a BERT based architecture an can be pre-trained on biomedical corpora to effectively translate medical queries keeping the domain-specific context for several tasks. LABSE is available for easy use from TensorFlow Hub. The model architecture has five main stages: Compile the dataset. The large dataset of question-answer pairs are for reference to the model to be able to generalize well even in cases of lack of information. These pairs provide the model with additional context, which becomes necessary when patients cannot fully and accurately describe their condition and symptoms. The proposed architecture also has an EHR (Electronic Health Record) summarizer that takes the patient's previous diagnosis and status information as input and summarizes any prevailing medication or treatment. This module uses the DistillBERT abstractive summarizer fine-tuned on Springer Nature Digital Medicine abstracts. The Question-Answer pairs dataset is compiled from forums such as Reddit, WebMD, and Question Doctors and are compiled to create a dataset consisting of 325,000 unique pairs. The questionanswer pairs are encoded onto an intermediate embedding space using the BioBERT model onto a Dense layer encoding, which can then be used for information retrieval based on a similarity-search. Text processing pipeline. Creating a text pre-processing pipeline using a corpus of words; the system needs a predetermined set of words that it can use to contextualize the meaning of a query in natural language. Several such public datasets are available. However, our work requires that the corpus used must contain a large set of clinical and biomedical terms in its vocabulary. For this purpose, we use the PubMed dataset. PubMed is primarily based on the MEDLINE database of references and abstracts on life sciences and biomedical topics. The stage involves the following steps: text tokenization, creating embeddings for the tokens, and creating special tokens such as
3

Published as a workshop paper at ICLR 2021

Dataset

BERT

BioBERT BioBERT Proposed

(WIKI+Books) (+PubMed) (+PMC) (+PubMed)

BioASQ 4b BioASQ 5b BioASQ 6b

27.33 39.33 33.54

25.47 41.33 43.48

26.09 42.00 41.61

27.78 42.61 42.38

Table 1: Strict Accuracy (S) reported for the respective datasets. The best scores are in bold and the second best scores are underlined.

EOS and UNK, as well as their embeddings. Word embeddings are then created out of the words in vocabulary. Each em-bedding is assigned a positive integer ID. Once the sentence is obtained, we use the LABSE model to translate the response to the English language for the BioBERT model.
Training the model architecture. We initialize the BERT model with BioBERT weights. During training, we use the question and answer pairs from the compiled train dataset above to obtain their intermediate representations in embedding space. We share the weights for both question head as well as the answer head borrowing the model architecture from DocProduct (Gupta. et al., 2019). These embeddings are then encoded onto separate Dense layers, one each for the question and answer head, used later for similarity lookup.
Generating natural-language answers. We use the GPT-2 (117M) parameter architecture to generate natural language answers to our retrieved answer by bolstering it with relevant text. During training, the retrieved queries are pre-processed and input to the GPT-2 model to compute the loss to train the embedding heads. Same as DocProduct, we use the dot product of the question-answer embedding pair from the similarity lookup and then compute the Softmax of the rows for each question. The loss is comput-ed using cross-entropy, and then gradients are propagated to train the model layers. We expect the dense embedding layers to capture the similarity of the question-answer pairs.
Conveying the results. In this stage, we use the output from GPT-2, which is the bolstered similar question-answer text obtained from our inference time similarity lookup. This is again processed by the LABSE model to obtain the answer in the required language to provide to the user. Additionally, we also use the extractive summaries of previous doctor consultations and use the DistillBERT model to embed the sentences, cluster them, and then to find sentences that are closest to the cluster centers. These mini-summaries can be useful for doctors to provide more in-depth diagnostics to the patient. (Note: This part of the architecture is still under active development)
4 RESULTS AND DISCUSSION
The outcome of this work is an application of dialogue representation system designed to provide quality medical consultations solving the problem of first point contact of patients with doctors. Our work obtains promising results for biomedical question answering (in English) for the BioASQ 4b. BioASQ 5b and BioASQ 6b datasets with Strict Accuracy (S) of 27.78, 42.61 and 42.38 respectively with only the PubMed dataset and BioBERT model as tabulated in Table 1. Our work can be implemented as an API endpoint for quick access to question-answers and user specific account information. We have also worked on making the model efficient with model pruning, post-training quantization and NNAPI delegates for deployment on Android smartphones with tflite acceleration for optimized offline performance. The promising QA results in English and out-of-the-box translation wrappers for input speech-to-text and output text translation powered by LABSE enables wide adoption of the multilingual dialogue model especially in rural health intelligence.
With the COVID-19 induced digital adoption across the world, rural populations are most affected by the information access gap. We expect our solution to be a small contribution toward empowering people through advances in Machine Intelligence and NLP to get up-to-date-information. NLP is evolving to help the market of healthcare substantially to improve personalized solutions such as the work by Bornea et al. (2020), and with the recent advancements in training language models with LABSE, BioBERT, and GPT-2, the ability of QA to understand the human queries.
4

Published as a workshop paper at ICLR 2021
REFERENCES
Naveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin Johnson, Maxim Krikun, Mia Xu Chen, Yuan Cao, George Foster, Colin Cherry, et al. Massively multilingual neural machine translation in the wild: Findings and challenges. arXiv preprint arXiv:1907.05019, 2019.
Mihaela Bornea, Lin Pan, Sara Rosenthal, Radu Florian, and Avirup Sil. Multilingual transfer learning for qa using translation as data augmentation. arXiv preprint arXiv:2012.05958, 2020.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.
Jeanne E Daniel, Willie Brink, Ryan Eloff, and Charles Copley. Towards automating healthcare question answering in a noisy multilingual low-resource setting. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 948­953, 2019.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. Languageagnostic bert sentence embedding. arXiv preprint arXiv:2007.01852, 2020.
Santosh. Gupta., J. Yip., L. Fostiropoulos., F. Schoonjans, and Sanket Gupta. Docproduct [open source software]. https://github.com/re-search/DocProduct, 2019.
Kexin Huang, Jaan Altosaar, and Rajesh Ranganath. Clinicalbert: Modeling clinical notes and predicting hospital readmission. arXiv preprint arXiv:1904.05342, 2019.
Prabhu Kaliamoorthi, Sujith Ravi, and Zornitsa Kozareva. Prado: Projection attention networks for document classification on-device. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 5012­5021, 2019.
Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451, 2020.
Tuan Lai, Trung Bui, and Sheng Li. A review on deep learning techniques applied to answer selection. In Proceedings of the 27th international conference on computational linguistics, pp. 2132­2144, 2018.
Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. Biobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36(4):1234­1240, 2020.
Mar´ia-Dolores Olvera-Lobo and Juncal Gutie´rrez-Artacho. Multilingual question-answering system in biomedical domain on the web: an evaluation. In International Conference of the CrossLanguage Evaluation Forum for European Languages, pp. 83­88. Springer, 2011.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.
Dinesh C Sharma. India still struggles with rural doctor shortages. The Lancet, 386(10011):2381­ 2382, 2015.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. arXiv preprint arXiv:1409.3215, 2014.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.
5

Published as a workshop paper at ICLR 2021 Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V
Le. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237, 2019.
6

