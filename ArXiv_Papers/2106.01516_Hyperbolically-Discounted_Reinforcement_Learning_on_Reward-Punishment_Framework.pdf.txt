Hyperbolically-Discounted Reinforcement Learning on Reward-Punishment Framework
Taisuke Kobayashi1

arXiv:2106.01516v1 [cs.LG] 3 Jun 2021

Abstract-- This paper proposes a new reinforcement learning with hyperbolic discounting. Combining a new temporal difference error with the hyperbolic discounting in recursive manner and reward-punishment framework, a new scheme to learn the optimal policy is derived. In simulations, it is found that the proposal outperforms the standard reinforcement learning, although the performance depends on the design of reward and punishment. In addition, the averages of discount factors w.r.t. reward and punishment are different from each other, like a sign effect in animal behaviors.
I. INTRODUCTION
Reinforcement learning (RL) basically acquires the optimal policy so as to maximize a return, which accumulates future rewards with exponential discounting [1]. Recent work has shown that RL is an excellent approach to achieve complicated tasks by robots [2], [3]. The reason why the exponential discounting is used is that it is mathematically easy to handle with a recursive manner.
However, animals show behaviors, which cannot be explained when using the exponential discounting [4]. Specifically, immediate and small reward is preferred to future and large reward, but such a decision is reversed when a moderate offset (delay) is added to the time. To explain such behaviors, a hyperbolic discounting is proposed in the context of behavioral economics.
It is natural to judge that the hyperbolic discounting has some advantages (e.g., the above decision making, longtailed discounting of the future reward, etc.) if animals certainly use it. Following this intuition, this paper aims to switch RL from with the exponential discounting to with the hyperbolic discounting. To this end, a temporal difference (TD) error for learning is redefined with the hyperbolic discounting from the literature [5]. Its original definition, however, assumes that the value function (i.e., the expectatoin of the return) an reward are positive. To handle real number of reward for generality, a rewardpunishment framework in RL [6], [7], which divides real reward into positive one (called reward) and negative one (called punishment), is employed. Note that this framework is also proposed from biological features.
The proposed method is simply investigated in numerical simulations. The results imply that it can outperform the conventional RL depending on the design of reward and punishment. In addition, it is found that the average discount factors for reward and punishment are in asymmetry, which is a similar feature to animals, called a sign effect [8].
1T. Kobayashi is with the Division of Information Science, Nara Institute of Science and Technology, 8916-5 Takayama-cho, Ikoma, Nara 630-0192, Japan kobayashi@is.naist.jp

II. PROPOSAL

A. Hyperbolically-discounted temporal difference

Let's define the value function (the return), V , with the hyperbolic discounting as follows:

Vt = E



rt+k 1 + k

| st

(1)

k=0

where r and s denote reward and state, respectively, and  is

the hyperparameter related to the discount factor  ( = 1-

when the exponential discounting).

Actually, this equation is difficult to solve with the re-

cursive manner. The literature [5], however, has derived a

hyperbolically-discounted TD error, , as follows:

t = rt +

1

-

Vt (µr + b)p

Vt+1 - Vt

(2)

where the bias b is given below in this paper.

b = r

(3)

µr and r mean the statistics of reward (mean and standard

deviation, respectively).  and p are the hyperparameter.

Here,



is

defined

as

1-

. Vt
(µr +r )p

Here, two problems in this definition are raised. One is the

difference of the scale between r (and µr) and V . From the

sum of geometric progression, V would be 1/(1 - ) times

larger than r if  is constant. To compensate this scale gap,

reward to calculate TD error is multiplied with 1 - ¯ where

¯ the average discount factor.

B. Reward-punishment framework
Another problem is signs of numerator and denominator. Reward in RL is defined as real number, so there is the possibility to get  > 1 when V and µr have the different signs. To avoid this without loss of generality, the rewardpunishment framework is employed. Specifically, real reward is divided into positive one r, called reward, and negative one with inverted sign p, called punishment, in environment [6] side or agent side [7]. In that case, both reward and punishment are defined to be positive real numbers, and therefore,  > 1 would never be caused.
However, the value functions for reward and punishment, Vr and Vp, are actually approximated using some function approximators (e.g., deep neural networks). The approximated values should have correct domain, i.e., the positive real numbers, as follows:

Vr,p = max(0, yr,p)

(4)

where yr,p are the outputs of the function approximators.

Return Discount factor
Return Discount factor

exponential 0.20

hyperbolic

exponential2

0.15

0.10

0.05

0.00

-0.05

-0.10

100

200

300

400

500

Episode

(a) Learning curves of Acrobot

0.998 0.997 0.996 0.995 0.994 0.993 0.992 0.991 0.990

reward

punishment

100

200

300

400

500

Episode

(b) Discount factors of Acrobot

exponential

hyperbolic

exponential2

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.0

100

200

300

400

500

Episode

(c) Learning curves of CartPole

Fig. 1. Simulation results

0.998 0.997 0.996 0.995 0.994 0.993 0.992 0.991 0.990

reward

punishment

100

200

300

400

500

Episode

(d) Discount factors of CartPole

III. SIMULATIONS
A. Conditions
In this paper, the performance of RL with the hyperbolic discounting is investigated. To do so, two environments for numerical simulations are prepared: Acrobot and CartPole (see https://github.com/kbys-t/gym_rp). In Acrobot, reward is given only when the target motion is achieved while punishment is continuously given. On the other hand, CartPole has continuous reward and event-based punishment. This setting is because, as shown in eq. (2), TD error depends on the statics of reward and punishment, which are affected by how to be given.
The hyperparameters , p, and  are empirically set as 0.01, 1, and 0.1, respectively. The other parameters for RL is decided to succeeded in learning with the conventional RL. For comparison, two cases, where the conventional RL has r,p = 0.99 or r,p the averages of the case with the hyperbolic discounting, are ocnducted.
B. Results
Fig. 1 summarized the simulation results of 50 trials per each case. The case of exonential2 (with the averages of the discount factors in the hyperbolic discounting) had the worst performance in both environments. Namely, it is expected that the discount factors depending on the value function or state contribute to the learning performance.
The proposal in CartPole outperformed the other cases, although it in Acrobot was inferior to the result of exponential1 (with  = 0.99). Focusing on the average discount factors during each episode (see (b) and (d) in Fig. 1), one for reward was likely to be greater than one for punishment in Acrobot; in contrast, their relation was reversed in CartPole. This difference may be given from the design of reward and punishment: i.e., the continuous design would make the discount factor small; and the event-based design would be a vice versa. This thought comes from the bias defined in eq. (3) as the variance of reward (or punishment), namely, the event-based or sparse design would cause large variance, thereby increasing the discount factor, as expected in eq. (2). Indeed, when learning progressed to some extent, reward was stable gained, so its variance became small, which made its discount factor small accordingly.

If the continuous reward and the event-based punishment are generally better for learning like these results, the asymmetry of the discount factors between them would be expected in this scheme. That is, if so, the hyperbolicallydiscounted reinforcement learning on the reward-punishment framework would be related to a sign effect in animals [8], and would contribute to analyze such animals behaviors mathematically.
IV. CONCLUSION
This paper proposed hyperbolically-discounted RL on the reward-punishment framework. Combining the hyperbolically-discounted TD error and the rewardpunishment framework, optimization by RL was enabled. In simulations, the proposal outperformed the conventional RL, although the performance depends on the design of reward and punishment. In addition, the averages of discount factors for reward and punishment were in asymmetry, like the sign effect in animal behaviors.
Future work is further analyses of the proposed scheme and to establish the way to design reward and punishment.
REFERENCES
[1] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction. MIT press Cambridge, 1998.
[2] J. Luo, R. Edmunds, F. Rice, and A. M. Agogino, "Tensegrity robot locomotion under limited sensory inputs via deep reinforcement learning," in IEEE International Conference on Robotics and Automation. IEEE, 2018, pp. 6260­6267.
[3] Y. Tsurumine, Y. Cui, E. Uchibe, and T. Matsubara, "Deep reinforcement learning with smooth policy update: Application to robotic cloth manipulation," Robotics and Autonomous Systems, vol. 112, pp. 72­83, 2019.
[4] S. Kobayashi and W. Schultz, "Influence of reward delays on responses of dopamine neurons," Journal of neuroscience, vol. 28, no. 31, pp. 7837­7846, 2008.
[5] W. H. Alexander and J. W. Brown, "Hyperbolically discounted temporal difference learning," Neural computation, vol. 22, no. 6, pp. 1511­1527, 2010.
[6] H. Okada, H. Yamakawa, and T. Omori, "Two dimensional evaluation reinforcement learning," in International Work-Conference on Artificial Neural Networks. Springer, 2001, pp. 370­377.
[7] S. Elfwing and B. Seymour, "Parallel reward and punishment control in humans and robots: Safe reinforcement learning using the maxpain algorithm," in Joint IEEE International Conference on Development and Learning and Epigenetic Robotics. IEEE, 2017, pp. 140­147.
[8] S. C. Tanaka, K. Yamada, H. Yoneda, and F. Ohtake, "Neural mechanisms of gain­loss asymmetry in temporal discounting," Journal of Neuroscience, vol. 34, no. 16, pp. 5595­5602, 2014.

