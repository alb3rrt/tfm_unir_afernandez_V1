
# A Systematic Investigation of KB-Text Embedding Alignment at Scale

[arXiv](https://arxiv.org/abs/2106.01586), [PDF](https://arxiv.org/pdf/2106.01586.pdf)

## Authors

- Vardaan Pahuja
- Yu Gu
- Wenhu Chen
- Mehdi Bahrami
- Lei Liu
- Wei-Peng Chen
- Yu Su

## Abstract

Knowledge bases (KBs) and text often contain complementary knowledge: KBs store structured knowledge that can support long range reasoning, while text stores more comprehensive and timely knowledge in an unstructured way. Separately embedding the individual knowledge sources into vector spaces has demonstrated tremendous successes in encoding the respective knowledge, but how to jointly embed and reason with both knowledge sources to fully leverage the complementary information is still largely an open problem. We conduct a large-scale, systematic investigation of aligning KB and text embeddings for joint reasoning. We set up a novel evaluation framework with two evaluation tasks, few-shot link prediction and analogical reasoning, and evaluate an array of KB-text embedding alignment methods. We also demonstrate how such alignment can infuse textual information into KB embeddings for more accurate link prediction on emerging entities and events, using COVID-19 as a case study.

## Comments

Accepted to ACL-IJCNLP 2021. 11 pages, 2 figures

## Source Code

Official Code

- [https://github.com/dki-lab/joint-kb-text-embedding](https://github.com/dki-lab/joint-kb-text-embedding)

Community Code

- [https://paperswithcode.com/paper/a-systematic-investigation-of-kb-text](https://paperswithcode.com/paper/a-systematic-investigation-of-kb-text)

## Bibtex

```tex
@misc{pahuja2021systematic,
      title={A Systematic Investigation of KB-Text Embedding Alignment at Scale}, 
      author={Vardaan Pahuja and Yu Gu and Wenhu Chen and Mehdi Bahrami and Lei Liu and Wei-Peng Chen and Yu Su},
      year={2021},
      eprint={2106.01586},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```

## Notes

Type your reading notes here...

