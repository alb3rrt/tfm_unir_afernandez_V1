arXiv:2106.01821v2 [stat.ME] 6 Jun 2021

A New Measure of Overlap: An Alternative to the p­value
Stephen G. Walker Department of Mathematics University of Texas at Austin, USA e-mail: s.g.walker@math.utexas.edu
Abstract In this paper we present a new measure for the overlap of two density functions which provides motivation and interpretation currently lacking with benchmark measures based on the proportion of similar response, also known as the overlap coefficient. We use this new measure to present an alternative to the p­value as a guide to the choice of treatment in a comparative trial; where a current treatment and a new treatment are undergoing investigation. We show that it is possible to reject the null hypothesis; i.e. the new treatment is significantly different in response to the old treatment, while the proposed new summary for the same experiment indicates that as low as one in ten individuals subject to the new treatment behave differently to individuals on the old one.
Keywords: Comparative trial; Overlap coefficient; Probability; P ­value; Significance.
1

1 Introduction
There has recently been a substantial amount of criticism of the statistical hypothesis test, largely focusing on the p­value, though also on the notion of significance. For example, see the recent discussion in the journal The American Statistician, Volume 73 (Supplementary Issue), headed by the Editorial; Wasserstein et al. (2019). Such discussions are by no means restricted to statistics journals; but are also prevalent in many scientific journals, with a good number appearing in medical journals, such as Cohen (2011).
To provide some background; the p­value is currently, though its future is in doubt, see Hasley (2019), the benchmark for reporting an outcome of an experiment. At a heuristic level, a small value indicates a significant outcome, i.e. to reject a null hypothesis, whereas a large value indicates a lack of significance. The cut­off mark, which has become the standard choice, is 0.05. However, recently, this procedure has come under closer scrutiny and, despite much having been written on the subject, a consensus is far from apparent.
Much of the debate on p­values is generated through a lack of clear knowledge on what is being discussed, namely a rigorous definition of a p­value. Invariably, it is a heuristic definition based on "seeing something larger than what was observed in a future experiment" on which articles predicate their discussion. However, such a heuristic fails even in a standard two­sided hypothesis test of a normal variance; see Gibbons and Pratt (1975). On the other hand, a rigorous definition of the p­value is provided by Lehmann and Romano (2005); see also Kuffner and Walker (2019).
With a rigorous definition of objects associated with a hypothesis test, there is nothing controversial about such a test per se; see Savage (1972). The basic idea, well known, is to decide whether the observed statistic is compatible with the assumption of the hypothesis. The definition of "compatible" here amounts to setting a type I error, also known as the level of significance. This has typically been set at 0.05. Such a specification amounts to acknowledging that from all true hypotheses tested, 1 in 20 will yield a significant and most likely non­reproducible result. It may be this lack of reproducibility which is really causing the unease among scientists; yet when decisions need to be made with imperfect information, mistakes can be made. That lack of reproducbility should immediately be classified as evidence of an irregular experiment, see for example Colquhoun (2018) and Nuzzo (2014), is unfortunate.
Once the hypothesis test framework has been determined to decide on a choice, concepts such as sufficient statistics, critical values, type I and II errors, power functions, become necessary parts of the decision process. It is a theory well grounded, though most likely highly misunderstood among its many users. Nevertheless, the point behind the recent volume 73 of The American Statistician has been to think beyond the p­value and to consider alternative procedures for determining decisions related to outcomes of experiments; such as the comparison of a new treatment with an established treatment or with a placebo.
The aim of this paper is to use the notion of overlap as a means to compare treatment outcomes. Standard measures of overlap lack motivation in terms of interpretation. We resolve this problem by introducing a new measure of the overlap between two density functions; the interpretation being the overlap measures the probability we can replace one outcome from one density with an outcome from the other density. Translating this
2

idea to the experiment, it would be the probability of being able to replace one individual on one treatment with an individual on the other treatment. If the probability is 1, the treatments are deemed the same. If the probablity is 0, the treatments are as different as can be (though 0 and 1 would never be achieved in reality). Anything falling inbetween will provide a concrete interpretation as to the efficacy of the new treatment under investigation; i.e. what proportion of the population it benefits. This is the contribution of the paper.
Our argument is that interpretability is everything. The use of statistics lacking clarity concerning outcomes, such as current measures of overlap, add to confusion. What is required is a statistic which has clear interpretation concerning outcome.
For sake of clarity and ease of exposition, we discuss and demonstrate the new notion of overlap specifically to two normal density functions; though other distributions will be equally viable. If densities are not fully known, they are assumed to be estimable from data. One of the densities will the normal distribution under the null, current treatment, and the other normal density being the one of outcomes from the new treatment. Measures of overlap date back at least to Pearson (1895). In particular, Rom and Hwang (1996) employed the notion of overlap as a means of assessing similarity of treatments; the comparative trial becoming a common way of assessing the efficacy of two or more treatments. These authors used the standard Overlap Coefficient for densities p0 and p1, defined as

OVL(p0, p1) = min{p0(x), p1(x)} dx;

(1)

also known as the proportion of similar responses (PSR). It is now quite a popular measure in medical data analysis; see, for example, Mizuno et al (2005), Lei and Olsen (2010), and Giacoletti and Heyse (2015). The measure (1) is also used in ecology, measuring the so­called niche overlap; see, for example, Mason et al (2011). It also arises in economics, as a meaure of polarization between two groups; see Anderson (2004). Apparently, in economics, it was first seen in a technical report by M. Weitzman in 1970 who was looking at measures of overlap of income distributions.
This said, Inman and Bradley (1989), state that the OVL, has "no major philosophical motivation". For this reason, a formal use for detemining the choice of treatment from a comparative study will be elusive. With this as our cue, the main contribution of the present paper is to introduce a novel measure of overlap which has definitive and clear motivation. In fact, it represents the probability that a random outcome from one density can be replaced by a random outcome from the other density. Following PSR, we therefore label the new measure as the proportion of interchangeable responses (PIR). It is defined as

OM (p0, p1) =

min{p0(x)p1(y), p0(y)p1(x)} dx dy,

which can be seen as a variant of OVL; i.e. OM (p0, p1) = OVL(pa, pb), where pa(x, y) = p0(x)p1(y) and pb(x, y) = p0(y)p1(x). We believe OM to be a new measure of overlap and provides a measure with better motivation and interpretation than that provided by PSR.
For the layout of the paper; in section 2 we set the background for the test. In section 3 we present the theory for how we compare outcomes from the two treatments, presenting a measure of quantification for a notion of overlap; i.e. the OM (p0, p1), and provide motivation

3

for it. This intuitive measure of an outcome of an experiment can be severely at odds with the outcome of the hypothesis test. Section 4 presents an illustration and section 5 concludes with a brief discussion.

2 Background on p­values

To make the present paper concrete, we keep to a specific setting in which the use of the
p­value is ubiquitous. Outcomes of a standard treatment T0 among a population is known and represented by a normal distribution with known mean 0 and known variance 2. Represent this via the density function p0(x).
A new treatment T1 is proposed and an experiment conducted on n individuals from the population. The new treatment will present outcomes from a normal distribution with unknown mean  and known variance 2. If indeed  is unknown, the argument which
follows will apply, but with some more complicated technical details. The hypothesis of
interest is
H0 :  = 0 vs H1 :  > 0,

and, without loss of generality, we will take 0 = 0. This is a standard set­up and the theory using p­values well documented. See, for example, Cardinal (2016) and Pocock
(2006). Now H0 will come under suspicion of being wrong if X¯ , the sufficient statistic for
, which is itself the sample mean of the observations from the individuals allocated to
treatment T1, is too large. Specifically, for a type I error of , the critical value is

cn,

=

 n

-1(1

-

),

where  is the standard normal distribution function. That is, reject H0 if X¯ > cn,.

The p­value corresponding to this experiment is given by the value of  which yields

X¯ = cn,; i.e.

X¯

 n

p=1-

.



Nothing new here; reject H0 if p < . The current debate about the p­value is whether it is appropriate as a summary out-
come of an experiment, and whether the benchmark 0.05 is a good choice. A further complication is that the p­value is often misinterpreted and, even among statisticians, it has an ambiguous definition. See Panagiotakos (2008).
The underlying concern to the sciences is that if a mistake is made; i.e. the experiment is invoking the type I error, in which a null hypothesis is true yet the sufficient statistic lands in the critical region, which can happen with probability , then there is a lack of reproducibility. However, a level of lack of reproducibility is, as we have detailed, a factored aspect of the test. To elaborate further, suppose the 0.05 cut­off has been universally selected. Of all hypotheses which are true, one on twenty will yield a significant outcome. The up­shot is that one in twenty significant outcomes among experiments for which the

4

null hypothesis is true, will not be reproducible. This seems a heavy price to pay. On the other hand, when decisions need to be made, mistakes can happen, inevitably.
While this concern has been well documented; the aim in the present paper is to provide a further criticism of the notion of the statistical test and to propose a solution.

3 A new measure of overlap

We have assumed that the distribution of individual outcomes under the standard treatment

T0 is a normal distribution with mean 0 and variance 2, written as p0. Under treatment T1 the distribution of outcomes is normal with mean  and variance 2, written as p1. Let

X1 denote a random outcome from p1 and X0 denote a random outcome from p0. We want to quantify just exactly how X1 compares to X0.
We do this in a universal way; so actually whatever the distribution of X0 and X1 are

we can evaluate the measure of overlap we are about to describe. Let N be a large number and let (X0(j))Nj=1 be independent and identically distributed samples from p0. For each j
we sample a X1(j), and then according to some criterion either replace X0(j) with X1(j) or else leave it as it is. Hence, we end up with a sample (Z(j))Nj=1 which will be distributed as
p0, as we shall see, and comprised of the "accepted" (X1(j)) and the left alone (X0(j)). We define qN as the proportion of (X1(j)) in the sample (Z(j)). We show that qN has
a well defined limit as N   which while difficult to compute directly, can always at

least be estimated using Monte Carlo methods. We refer to q as the overlap of p1 into p0. Effectively, the probability that a random outcome from p1 can replace a random outcome

from p0; which we write as

q = P(X1 r X0).

(2)

So if q = 1 then p0 and p1 are identical; whereas if q = 0 then the overlap of the support of the two densities is empty.
The q is defined as

q=

min{p0(x)p1(y), p0(y)p1(x)} dx dy

(3)

which, using the equality |a - b| = a + b - 2 min(a, b), can also be written as

q

=

1

-

1 2

|p0(x) p1(y) - p0(y) p1(x)| dx dy,

which is 0 if p0 and p1 have no common support points and is 1 if p0 = p1. Note, as we would require, that this definition is symmetric in p0 and p1. We now motivate this choice of q, which we also previously referred to as OM (p0, p1).
Consider the conditional density

p(y|x) = (x, y) p1(y) + (1 - r(x)) 1(y = x),

where

(x, y) = min 1, p0(y) p1(x) ,

(4)

p1(y) p0(x)

5

and r(x) = (x, y) p1(y) dy. Observers will recognize this as the transition density for a Metropolis­Hastings algorithm (Metropolis et al., 1953; Hastings, 1970; Tierney, 1994). The point here is that if X is distributed as p0, and Y is distributed as p1, then
 Y with probability (X, Y )  Z=  X with probability 1 - (X, Y )
is distributed as p0. Clearly of interest here is the probability we accept Z as Y , since it replaces X, which
is distributed as p0, with Y distributed as p1, and recall Z is also distributed as p0. This probability is given by

q = r(x) p0(x) x. =

(x, y) p1(y) p0(x) dx dy,

which is precisely (3). So, in short, the q is the probability an outcome from p1 replaces an outcome from p0. Hence, if q is large, the two treatments are not too different in that an outcome from an individual under T1 can with high probability replace an outcome from an individual under T0. On the other hand, if q is small, then the probability of this replacement is small indicating the outcomes from the two treatments are different.

4 Illustration
Let us consider a specific case; taking  = 1, n = 100, and  = 0.05. Under these settings the null hypothesis H0 :  = 0 vs H1 :  > 0 is rejected when
X¯ > 1 -1(0.95) = 0.164. 10
So now let us put  = 0.164 and compute the value of q which indicates just how differently X1 under treatment T1 behaves compared to X0 under treatment T0.
We have q = P(X1 r X0) given by
q() = min{(x) (y - ), (y) (x - )} dx dy,
where (x - ) is the normal density function with mean  and variance 1. In general with any choice of p0 and p1 it would be difficult to compute q directly, and so it would need to be computed using Monte Carlo methods. However, for the normal case here it is computable directly and
q() = 2 1 -   . 2
With  = 0.164, we get q = 0.91. This is an interesting result. It means that while H0 is rejected at the 5% level of
significance for this value of  = 0.164, indicating T1 is preferred to T0; on the other hand,
6

also true is that only 1 in 10 individuals behave differently under T1 than they do under T0. On this basis we question the relevance of the outcome of the hypothesis test and indeed whether the test is in itself a reasonable course of action to take. As espoused by Dom and Hwang (1996), a measure of similarity would seem more insightful and informative.

1.0

0.8

0.6

q(theta)

0.4

0.2

0.0

0

1

2

3

4

theta

Figure 1: Plot of q() for varying  in normal illustration

Here we plot the probabilty q() as a function of  for  > 0, and is provided in Fig. 1.

One can see that the probability an individual under treatment T1 replacing an individual

under In

treatment T0 dips below practice, the experiment

1 2

for





1;

would yield

a value substantially larger than 0.164. X¯ , the estimate of the mean  from the

obser-

vations. As a consequence, if there is a need to avoid levels of significance and p­values, one can favor T1 if X¯ > 1, for example. Under this scenario there is no Type I error and no

notion of a probability of making an error. An evaluation is being made of treatment T1

and how it relates in terms of performance compared to T0. The question becomes whether this performance is sufficiently different to merit changing treatment.

In general, if it is determined that P(X1 r X0) < q0 for some specified q0, then

one would accept the new treatment if q() < q0; in the example, this would be  > 2-1(1 - q0/2). With a Bayesian or bootstrap approach, a probability for this event can
be calculated. Alternatively, one could test the hypothesis H0 :  > q-1(q0).
An interesting note to add here is that to recover the hypothesis H0 :  > 0, to determine

whether the new treatment is better or not, would be tantamount to selecting q0 = 1, which

7

makes no sense. The point being that the hypothesis H0 :  > 0 can be rejected even with a  being marginally larger than 0 which equates to essentially no benefit. Whereas by asking that for some q0 it is demonstrated that  > q-1(q0) not only has a benefit been suggested but also one of sufficient magnitude.

5 Summary and discussion

The hypothesis test, with associated summaries such as the p­value and level of significance, has been shown to be deficient as a means to determing whether a new treatment is suitably different to an existing one. It has been argued, by many authors, that decisions based on p­values and statistical significance have many shortcomings.
A further shortcoming of the relevance of the p­value and level of significance has been presented in this paper. Indeed, we argue that the overlap of outcomes, as measured by a probability q, under the two treatments is a highly relevant statistic of an experiment, shedding light by computing the probability an individual under the new treatment can replace an individual on the old treatment. This is given by the q in (3), where p0 is the density of outcomes under T0 and p1 the (estimated) density of outcomes under T1. That is, if  is the sample estimate of , we compute the probability of a random outcome from p being a replacement for a random outcome from p0 as

q=

min{p0(x) p(y), p0(y) p(x)} dx dy.

This can be close to 1, indicating no substantial difference between the density functions, whereas the  lies in the critical region for rejecting H0.
A criticism of this q as it stands is that it fails to take the uncertainty in the data into account, relying as it does on . The bootstrap (Efron, 1979; Efron, 2012) would be one
way to take the uncertainty of  into consideration. This would involve, for the parametric bootstrap, the most suitable bootstrap under the circumstances, sampling a new data set (X1, . . . , Xn) independently and identically distributed from p(.) and computing the maximum likelihood estimator  from this sample. Each  would provide a

q=

min{p0(x) p(y), p0(y) p(x)} dx dy.

A collection of the (q), an arbitrarily large number, can be used to account now for the
uncertainty in . It is worth taking the value of q as a decision summary for the experiment alone. An
approach considered in Resier and Faraggi (1999), who tested the equivalence of two populations using PSR. For example, if more than half of individuals under the new treatment behave as though they were on the old treatment, the new treatment might be judged not good enough. Nevertheless, in the context of the problem for which the treatments are being used, it might be that even a small proportion of individuals behaving sufficiently differently makes the switch from old to new justifiable.

8

Comments on an earlier version of this paper suggest that a large scale simulation study is in order. This misses the point which is that a motivated statistic is required. One may or may not think in any given experiment that the specific motivation is appropriate. But whichever opinion is presented, a table of numbers providing measures of overlap which have no interpretation only adds to a sense of statistics being used out of place.

Appendix
We present a number of pieces of further information about the new measure of overlap. In Appendix A we consider the distance associated with the new measure of overlap and connect it to the Hellinger distance. In Appendix B we consider yet an alternative overlap measure which can be seen as being motivated by an alternative to the Metropolis­Hastings algorithm; the Barker algorithm. In Appendix C we provide a bound between OM and OV L and in Appendix D we connect up the new overlap measure with the Youden index. Finally, in Appendix E we use the overlap measure to provide a new measure of overlap between two finite sets.

Appendix A
Here we discuss more formally the q = P(X1 r X0) as defined by

q=

min{p0(x) p1(y), p0(y) p1(x)} dx dy.

First note the necessary property that P(X1 r X0) = P(X0 r X1); since if X0 can replace X1 with probability q, so with the same probability X0 can replace X1. Also note that

d(p0, p1) = 1 -

min{p0(x) p1(y), p0(y) p1(x)} dx dy

is a distance between density functions p0 and p1.
So the idea is that while 0 and  are far enough apart for the null hypothesis to be
rejected, the distance between p0 and p is small. However, it is imperative the distance used to compare p0 and p is interpretable and is calibrated in terms of a relevant unit relating to treatment outcomes. This is the idea being q = P(X1 r X0).
We can also show that d(p0, p1) is equivalent to the Hellinger distance

dH (p0, p1) =

2

1/2

p0(x) - p1(x) dx

and so

p0(x)

p1(x)

dx

=

1

-

1 2

d2H

(p0

,

p1).

Recall, setting p(x, y) = p0(x) p1(y),

d(p0, p1)

=

1 2

|p(x, y) - p(y, x)| dx dy = 1 -

min{p(x, y), p(y, x)} dx dy.

9

Using

 1 - a/b = (1 - a/b)(1 + a/b) and min{a, b}  ab,

we can show that

d2H (p0, p1) 1 - d2H (p0, p1)/4  d(p0, p1)  4d2H (p0, p1).

Appendix B

An alternative to the Metropolis­Hastings algorithm would be the Barker algorithm (Barker, 1965). Here instead of the  appearing in (4), we have

(x, y) =

p0(y)p1(x)

,

p0(y)p1(x) + p0(x)p1(y)

and not that

(x, y) p0(x)p1(y) = (y, x) p0(y)p1(x).

The measure of overlap in this case is

OB(p0, p1) = 2

p0(y)p1(x) p0(x)p1(y) dx dy, p0(y)p1(x) + p0(x)p1(y)

the specific form ensuring that the measure is 1 when p0  p1 and OB is upper bounded by 1. It is interesting to note that this measure is connected to the Crossmatch statistic (Rosenbaum, 1995). The crossmatch statistic is for testing the equality of two distributions represented by two independent samples X1:n and Y1:n. The statistic is the number of pairings between the two samples which minimizes a sum of distances, and the limit of the statistic, as n  , is given by

OC(p0, p1) = 2

p0(x)p1(x) dx, p0(x) + p1(x)

where p0 is the density of the X's and p1 the density of Y 's. So just as we obtained OM from OVL, so we obtain OB from OC; i.e. OB(p0, p1) = OC(p0(x)p1(y), p0(y)p1(x)).
Now OB does not need to be estimated by first estimating the appropriate density functions, which is needed for the overlap coefficient. Such estimation of the densities can
be done using kernel smoothing, as in Stine and Heyse (2001). Consider data X1:n and Y1:n from p0 and p1, respectively. To estimate OB directly from the samples, consider the two bivariate samples

A=

X1 · · · Xn/2 Y1 · · · Yn/2

and B =

Yn/2+1 · · · Yn Xn/2+1 · · · Xn

and construct the n × n symmetric matrix M with elements

M (i, j) = dE(Ai, Aj), M (i, n/2 + j) = dE(Aj, Bj), M (n/2 + i, n/2 + j) = dE(Bi, Bj),
for 1  i, j  n/2, where dE denotes the usual Euclidean distance. Then evaluate the idempotent permutation  on {1, . . . , n} which minimizes l() = i=1:n M (i, (i)). The relevant statistic which converges to OB is min{1, 4nc/n}, where nc = #{i : (i) > n/2}. See Arias­Castro and Pelletier (2016).

10

Appendix C
 Here we find a bound between OM (f, g) and OV L(f, g). Now min{a, b}  ab so

OM (f, g) 

2
f (x) g(x) dx .

Also, |a - b| = a|1 - b/a|(1 + b/a), and so using Jensen inequality

|f - g|  2 1 -

2
fg .

Hence,

22

OM (f, g) 

1-

1 2

|f - g|

and using |a - b| = a + b - 2 min(a, b) we recover OM (f, g)  1 - (1 - OV L(f, g))2 2 .

Appendix D

In Samawi et al (2017) a relation between the Youden index (Youden, 1950) and the OVL

overlap measure was explained. The Youden index for densities f and g, with corresponding

probability measures/distribution functions F and G, respectively, and with cut­off mark

c, is given by

J(c) = F (A(c)) + G¯(A(c)),

where G¯  1 - G and A(c) = (-, c). One aim here is to estimate c by maximizing J(c); see, for example, Fluss et al (2005).
Now OV L(f, g) can be written as

OV L(f, g) = F (A) + G¯(A)

where A = {x : f (x)/g(x)  1}. In fact, if we considered J(A) = F (A) + G¯(A) and attempted to maximize this over all sets A, the solution would indeed be the set {x : f (x)/g(x)  1}.
The OM (f, g) measure is more general again; it can be written as

OM (f, g) = F (A(y)) g(y) dy + G¯(A(y)) f (y) dy,

where

A(y) = {x : f (x)/g(x)  f (y)/g(y)}.

In this context, OM (f, g) can be seen as a natural extension of the Youden index and OVL measure.

11

Appendix E

Consider two finite sets A and B. The overlap coefficient is a measure of overlap between
the two sets and given by |S|
O(A, B) = min{|A|, |B|}

where S = A  B and | · | denotes the size of the set. An alternative measure is the Jaccard

index

|S|

J(A, B) =

.

|A| + |B| - |S|

Overlap or similarity measures are routinely used in medical imaging; see for example, Almodovar­Rivera and Maitra (2019).
We can use OM here by creating two discrete probability mass functions pA and pB by putting mass 1/|A| on each element of A and mass 1/|B| on each element of B, respectively. Then, define the new measure of overlap for two finite sets A and B as

OM (A, B) =

OM (pA, pB) =

|A| |B| min{piApjB, piBpjA} = |A||B|
i=1 j=1

|S| .
|A| |B|

It is now interesting to note that OM sits precisely in between O and J; i.e.

J(A, B)  OM (A, B)  O(A, B).

This follows since for any integers n, m with k  min{n, m}, 
k  nm  n + m - k.

References
Almodovar­Rivera, I. and Maitra, R. (2019). FAST adaptive smoothing and thresholding for improved activation detection in low-signal fMRI, IEEE Transactions on Medical Imaging, 38, 2821­2828.
Anderson, G.J. (2004), Toward an empirical analysis of polarization, Journal of Econometrics 122, 1­26.
Barker, A.A. (1965), Monte Carlo calculations of the radial distribution functions for a proton­electron plasma, Australian Journal of Physics, 18, 119­133.
Cardinal, L.J. (2016), Determining true difference between treatment groups, Journal of Community Hospital Internal Medicine Perspectives, 6, 10.3402/jchimp.v6.30284.
Arias­Castro, E. and Pelletier, B. (2016), On the consistency of the crossmatch test, Journal of Statistical Planning and Inference 171, 184­190.

12

Cohen, H.W. (2011), P ­values: Use and misuse in medical literature, American Journal of Hypertension, 24, 18­23.
Colquhoun, D. (2018), Reproducibility of science: Fraud, impact factors and carelessness, Journal of Molecular and Cellular Cardiology, 114, 364­368.
Efron, B. (1979), Bootstrap methods: another look at the jackknife, Annals of Statistics, 7, 1­26.
Efron, B. (2012), Bayesian inference and the parametric bootstrap, Annals of Applied Statistics, 4, 1971­1997.
Fluss, R., Faraggi, D. and Reiser, B. (2005), Estimation of the Youden index and its associated cutoff point, Biometrical Journal 4, 458­472.
Giacoletti, K.E.D. and Heyse, J. (2015), Using proportions of similar response to evaluate correlates of protection for vaccine efficacy, Statistical Methods in Medical Research 24, 273­286.
Gibbons, J.D. and Pratt, J.W., (1975), P ­values: Interpretation and methodology, The American Statistician, 29, 20­25.
Hasley, L.G. (2019), The reign of the p­value is over: what alternative analyses could we employ to fill the power vacuum?, Biology Letters, 15, 2018174.
Hastings, W.K. (1970), Monte Carlo sampling methods using Markov chains and their applications, Biometrika, 57, 97­109.
Inman, H.F. and Bradley, E.L. (1989). The overlapping coefficient as a measure of agreement between probability distributions and point estimation of the overlap of two normal densities. Communications in Statistics ­ Theory and Methods, 18, 3851­ 3874.
Kuffner, T. and Walker, S.G., (2019), Why are p­values controversial?, The American Statistician, 73, 1­3.
Lehmann. E.L. and Romano, J.P. (2005), Testing Statistical Hypotheses, Third Edition, Springer Texts in Statistics.
Lei, L. and Olsen, K. (2010), Evaluating statistical methods to establish clinical similarity of two biologics, Journal of Biopharmaceutical Statistics 20, 62­74.
Mason, N.W.H., de Bello, F., Dolezal, J. and Leps, J. (2011), Niche overlap reveals the effects of competition, disturbance and contrasting assembly processes in experimental grassland communities, Journal of Ecology 99, 788­796.
Metropolis, N., Rosenbluth, A.W., Rosenbluth, M.N., and Teller, A.H. (1953), Equation of state calculations by fast computing machines, Journal of Chemical Physics, 21, 1087­1092.
13

Nuzzo, R. (2014), Scientific method: Statistical errors, Nature, 506, 150­152. Panagiotakos, D.B. (2008), The value of p­value in biomedical research, The Open Cari-
ovascular Medicine Journal, 2, 97­99. Pearson, K. (1895), Contributions to the mathematical theory of evolution, II: skew vari-
ation in homogeneous matterial, Philosophical Transactions of the Royal Society of London, Series A, 186, 343­414. Pocock, S.J. (2006), The simplest statistical test: how to check for a difference between treatments, The British Medical Journal, 332, 1256­1258. Resier, B. and Faraggi, D. (1999), Confidence intervals for the overlapping coefficient: the normal equal variance case, The Statistician 48, 413­418. Rom, D.M. and Hwang, E. (1996), Testing for individual and population equivalence based on the proportion of similar response, Statistics in Medicine 15, 1489­1505. Rosenbaum, P. R. (2005), An exact distribution­free test comparing two multivariate distributions based on adjacency, Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67, 515­530. Samawi, H.M., Yin, J., Rochani, H. and Panchal, V. (2017), Notes on the overlap measure as an alternative to the Youden index: How are they related? Statistics in Medicine 36, 4230­4240. Savage, L.J. (1972). The Foundations of Statistics, Second Revised Edition, Dover Publications, NY. Stine, R.A. and Heyse, J.F. (2001), Non­parametric estimates of overlap, Statistics in Medicine 20, 215­236. Tanaka­Mizuno, S., Yamaguchi, T., Fukushima, Y., and Matsuyama, Y. (2005), Overlap coefficient for assessing the similarity of pharmacokinetic data between ethnically different populations, Clinical Trials 2, 174­181. Tierney, L. (1994), Markov chains for exploring posterior distributions, The Annals of Statistics, 22, 1701­1728. Wasserstein, R.L., Schirm, A.L. and Lazar, N.A. (2019), Moving to a world beyond p < 0.05, The American Statistician, 73(Supp), 1­19. Youden, W.J. (1950), Index for rating diagnostic tests, Cancer 3, 32­35.
14

