1
T-BERT - Model for Sentiment Analysis of Micro-blogs
Integrating Topic Model and BERT
Sarojadevi Palani, Prabhu Rajagopal, and Sidharth Pancholi, Member, IEEE

arXiv:2106.01097v1 [cs.CL] 2 Jun 2021

Abstract--Sentiment analysis (SA) has become an extensive research area in recent years impacting diverse fields including ecommerce, consumer business, and politics, driven by increasing adoption and usage of social media platforms. It is challenging to extract topics and sentiments from unsupervised short texts emerging in such contexts, as they may contain figurative words, strident data, and co-existence of many possible meanings for a single word or phrase, all contributing to obtaining incorrect topics. Most prior research is based on a specific theme/rhetoric/focused-content on a clean dataset. In the work reported here, the effectiveness of BERT(Bidirectional Encoder Representations from Transformers) in sentiment classification tasks from a raw live dataset taken from a popular microblogging platform is demonstrated. A novel T-BERT framework is proposed to show the enhanced performance obtainable by combining latent topics with contextual BERT embeddings. Numerical experiments were conducted on an ensemble with about 42000 datasets using NimbleBox.ai platform with a hardware configuration consisting of Nvidia Tesla K80(CUDA), 4 core CPU, 15GB RAM running on an isolated Google Cloud Platform instance. The empirical results show that the model improves in performance while adding topics to BERT and an accuracy rate of 90.81% on sentiment classification using BERT with the proposed approach.
Index Terms--BERT, Sentiment Analysis, Contextual topics, topic model.
I. INTRODUCTION
T HE FIELD OF SENTIMENT ANALYSIS (SA) studies opinions expressed in terms of micro-blogs, texts, people's interest or opinion towards a topic, about product reviews, government policies, religion, etc. in natural human language [1]­[4].
Businesses seek to use these contents in real time to understand how customers perceive their products and formulate strategies accordingly. This information can also be used to resolve product restrictions or monitor people's attitudes before new products are released [5]. The proliferation of these and also the advancements in Natural Language Processing (NLP) has provided researchers in giving more solutions to many engineered problems related to human language focusing on people emotions and opinions. The explosion of online social
Sarojadevi Palani is with Center for Nondestructive Evaluation and Department of Mechanical Engineering, Indian Institute of Technology, Madras, India (e-mail: saroojadevi@gmail.com)
Prof.Prabhu Rajagopal is with Center for Nondestructive Evaluation and Department of Mechanical Engineering, Indian Institute of Technology, Madras, India (e-mail: prajagopal@iitm.ac.in)
Dr.Sidharth Pancholi is with the Department of Electrical Engineering, Indian Institute of Technology, Delhi, India (e-mail: s.pancholi@ieee.org)

networking [3] on daily basis in turn demands the need of high Cloud computing Graphics Processing Unit (GPU) and Tensor Processing Unit (TPU) processors with a specialized purpose and architecture to store, process and analyse them [6]­[10]. However, the critical part lies in mining people's opinions, intents towards discussions, emotions, any hidden patterns or latent information in the form of any natural human-human readable canonical language. Also, helpful data is covered in disarranged, inadequate, and unstructured instant messages. Some researchers, propose to arrange a lot of these messages or micro-blogs into groups with significant bunch marks, along these lines give an outline of the substance to address clients data needs [11], [12]. Statistical topic models fail to derive these patterns accurately when it comes to large and heavytailed corpus texts [13], [14]. However, transformer based pre-trained models such as BERT (Bidirectional Encoder Representations from Transformers) overcomes the need of supervised learning pattern. The transformers-based models [5] use a pre-trained sentence embedding vectors which is trained on large corpora of data so that the problem of supervised learning is not highly essential in here. It has been argued that, today's techniques limit the power of pretrained representations, especially for fine-tuning approaches due to the restriction imposed on the standard uni-directional language models. For example, in OpenAI GPT, where left-toright architecture is used for pre-training ends up in looking for only the previous tokens in the attention layers of the transformers [4], [15]. This could be an acceptable approach while taking sentence level tasks but while considering token level tasks such as question answering, the fine-tuning tasks gets crucial as it involves processing from both the directions [4]. In this research, a flexible framework to combine latent topic information with BERT embeddings (T-BERT) to extract the contextual topics from live twitter dataset is proposed to classify the sentiments using BERT on the same. The main contributions of this proposed approach are a) To build and develop modelS to extract latent topics b) To develop BERT sentence embeddings to draw the contexts based on the semantic similarity in the microblogs and merge them with LDA latent topics using a deep learning auto-encoder. c) To propose and build a BERT Sentiment classifier and to finally merge both contextual topics and sentiments for each microblog using software engineering techniques. Some of the traditional latent topics extraction methods plays excellent role in the areas of Information Retrieval (IR), Cyberbullying, analysing political texts from newspaper articles, large image

2

Fig. 1: Overall approach of proposed T-BERT

content extraction, summarization etc. This study could favour some real time applications as shown in Fig 1 where the Topic/Aspect-based Sentiment analysis is taken advantage of: 1. Recommendation systems 2. Market Research 3. Brand Monitoring 4. Earth quake detection algorithms 5. Financial analysis 6. Bio-informatics 7. Social Network Analysis (SNA) 8. Software engineering. The rest of the paper is structured as follows. Section II Related Work walks through several Sentiment analysis, Topic models, BERT based recent research papers. Section III; Explains algorithms used and work methodology. Section IV; Explains the setup for numerical experiments and various model hyper-parameters utilized. Empirical results of the BERT contextual topics and BERT Sentiments are discussed in Section V, after which the paper concludes with directions for further work.
II. RELATED WORK
Sentiment analysis on unstructured data such as micro-blogs or short-texts or other inherently sparse data are conducted on various standard data sets where the data is collected under specific theme or topics or politics or movie reviews or using popular SemEval datasets [5], [16]­[19]. However, in this study the theme or topics or genre is unknown in the microblogs. [20] conducted a theoretical study and comparison on the 3 levels of SA tasks on short texts and gave a comprehensive state-of-art results based on the algorithms implemented by multiple researchers. In recent years, research on analysing short-texts and micro-blogs has peaked [13], [21], [22] due to the mass usage of smart gadgets and social media platforms. [21] conducted numerical experiments on Health Care Reform and United States Presidential election debates in the year 2009 by taking polarity labeled datasets and proposed a novel SA on micro-blogs incorporating homophily [23] in social networks, additionally introducing topic context to cater the semantic relations between them. In 2007, [14], [24] presented a sentiment polarity based TM to analyze public satisfaction at the government in Surabaya city. A methodology based on "Markov chain" was proposed by [14] where all the words in the same sentence and its consecutive sentences falls under same topics and the model was trained and inferred using Hidden Markov tools. A comparative study

on various TM techniques and Tool-kits from 2015 to 2020 evaluating performance metrics on major application areas with an excellent high-level report are presented in [25]­[29].
With the advancements in the Deep Learning Neural Networks and the increase in the demand for many Natural Language Processing(NLP) tasks from various domains and business intakes have led researches to make much deeper analyses and helped develop the Transformers based architectures on top of DL networks which could solve many NLP tasks in a much efficient and faster way incorporating attention mechanism for robust training [30]. [?] introduced a novel Transformer based architecture that uses the same encoderdecoder network similar to seq2seq model with attention mechanism but does not connote any LSTM, GRU or RNN networks. The Transformer architecture model completely relies on Attention mechanism thereby avoiding recurrence to draw global dependencies between the input and the output which in turn allows parallelism. In late 2018, inspired by the Transformers architecture Google [15] research team introduced a new deep Bidirectional Encoder Representations from Transformers called BERT to alleviate the constraints imposed on unidirectional methods. Sentiment analysis [22], [31], [32] outperformed with improved F1 scores while pretraining with BERT classifiers. [32] performed a sophisticated fine-grained sentiment analysis on publicly available popular Stanford Sentiment Treebank (SST) dataset. A new algorithm formulating specific training dataset for neural networks using Word2Vec and Bert to detect sentiment values was introduced by [4]. Numerical experiments conducted on AspectBased Sentiment Analysis (ABSA) [31], [33] by fine-tuning the intermediate layers of BERT in the context of semantic detection between two sentences [33]. BERT [15] performance on Semantic Textual Similarity(STS) tasks such as sentencepair regression had proven to be a state-art for many NLP tasks but requires both the sentences to be fed to a crossencoder transformer network simultaneously to predict the output [34]. In this study, SBERT(Sentence-BERT) introduced by [34] uses a different architecture called Siamese and triplet network thereby comparing the semantically meaningful sentences using cosine-similarity drastically reducing 65 hours of work load to 5 seconds, with no compromise in accuracy

3

from BERT is utilized for generating sentence embedding In order to obtain the contextual topics, the topic vectors ()

vectors to ease computational power.

of LDA model are merged with a collective contextual word

embedding vectors (H) from Sentence-BERT model using a

III. METHODS

gamma () hyper-parameter to add relative importance to both

Inspired by BERT and the recent advancements in Transform- vectors as shown in (2). H = BERT (x1, ..., xT )

ers architecture [4] T-BERT framework is proposed to show improved performance on topic model and aims at predicting the sentiment polarity (positive, negative or neutral) of the microblogs in study.

Contextual Topic Vectors(t) =  + H

(2)

where x1, ..., xT is a collective word vector of word embedding, segment embedding, and position embedding of each tweet token; Trm stands for Transformer encoder unit;

A. DATASET AND PRE-PROCESSING
Data is extracted from popular micro-blogging platform Twitter via the link https://apps.twitter.com/ using Tweepy API [35] credentials, python web scrapping techniques and libraries without any search keywords or specific topics of interest. In this study, around 40k raw microblog without any user information are extracted into a spreadsheet and processed for data analysis. The following pre-processing steps are performed before being fed into model algorithms. 1. Canonicalization (Removal of numbers, Non-Ascii characters, punctuation symbols, accents, and convert microblogs to lowercase.) 2. Removal of

H = (w1, ..., wT ), and wi the averaged output from 12 multi-headed transformer blocks given as token's contextual embedding vector representation. The combined vectors(t) are passed into a deep learning auto-encoder latent vector space to ensure dimensionality reduction and noise to arrive at the best topic clusters. The output of the auto-encoder is a cluster of keywords, each falling into a specific unique topic category using a K-Means clustering algorithm are labeled manually with unique topics. This approach is expected to provide more accurate topic semantic information for simulation on short tweet texts.

URL links 3. Removal of Stop words 4. Replace emoticons with their original text form 5. Stemming/Lemmatization ­ Converting to root words 6. Tokenization of microblogs using tokenizer specific to BERT input and spacy libraries for LDA topics.

C. BERT Sentiment Classification
The sentiment analysis model in Fig 3 aims to detect accurately the emotions of the microblogs, grasp the semantic characteristics and rules of the emotional evolution, and assist the contextual topics by adding value to the business in

P (W, Z, ,  | , ) =

terms of recommendations, aspect-based sentiments, under-

stand user intents to post relevant and appropriate ads etc.

M

K

N

For the purpose of BERT pre-training, SMILE Twitter Emo-

P (j; ) P (i; ) P (Zj,t | j), P (Wj,t | Zj,t) (1) tion dataset adapted from https://figshare.com/articles/dataset/

j=1

j=1

t=1

smile annotations final csv/3187909 is utilized and later the

model is predicted for sentiments classes using twitter dataset

used for topics extraction in this study. Firstly, SMILE Twitter

emotion corpus is pre-trained in the BERT, after data pre-

processing like Canonicalization, tokenization using Bert to-

kenizer and finally the [CLS] and [SEP] tokens are added at

the appropriate positions by the model in itself. Secondly, the

B. LDA + BERT CONTEXTUAL TOPICS
LDA adopts a general probabilistic approach to model rich corpora of data. Each tweet as input is represented as random mixture of latent topics (z), where each topic is a collection or distribution of words (w) related to topics. The goal of LDA is to arrive at a probabilistic model for a corpus of documents that assigns high probability to the members of the documents and also assigning high probability to other "similar" documents in the corpus. The model algorithm works as to arrive at the topic's distribution as referred in the expanded equation in (1): Thus,  is the parameter of the Dirichlet prior on the per-document topic distributions,  is the parameter of the Dirichlet prior on the per-topic word distribution. The latent topic words are limited by the traditional structure of the LDA bag-of-words and PoS tagging model, which cannot effectively compare the semantic similarities in the tweet sentences and retrieve contextual information of the text.

tokenized corpus is used to perform in-depth pre-training of the BERT target field on the constructed sentiment classifier. Fig 3 explains the attention mechanism showing the weights of each word but the other words in the tweet sentence and the words with contextual importance is highlighted in darker color for each layer of the transformer blocks. For example, the word `Self' carries importance when the word `you' is read from left to right from which the model learns the semantic similarities of the sentence thus providing more accurate meaning behind the context, grammar and sentiments being discussed. In order to learn semantic information from the twitter texts, the transformer encoder connects the multiheaded self attention and feed-forward through a residual network structure. The multi-headed mechanism performs multiple linear transformations on the input vector to obtain different linear values, and then calculates the attention weight as shown in (3) and (4).

Perhaps, BERT could alleviate this when combined with LDA and a detailed process workflow of the same is shown in Fig. 2.

MultiHead (Q, K, V) = Concat(h1, h2, ..., hT )W M (3)

4

Fig. 2: Detailed workflow of BERT + LDA using deep learning auto-encoder.

Fig. 3: Workflow of BERT Sentiment Analysis detailing Self Attention

HF = Attention(QW Q, KW K , V W V )

(4) sentiments easily.

where Q, K, V are the input word vector matrix of queries, keys and values respectively. Attention is calculated after Q, K, and V are mapped through the parameter matrix, and the calculation results are spliced after repeating t times. W M is the weight matrix. W Q, W K , W V represents the weight matrix corresponding to the T th hyper-parameter head. Thus, the Transformer encoder learns and stores the semantic relationship and grammatical structure information of the microblogs and based on this input the SoftMax classifies the

IV. SETUP FOR NUMERICAL EXPERIMENTS
All numerical experiments were conducted with BERTBASE(uncased) with 12 transformer blocks, 768 hidden units, 12 self-attention heads and overall 110M trainable parameters. Out of 42,626 raw tweets 23,329 tweets after pre-processing are fed to LDA and Bert for contextual topic assessment and as test dataset for sentiments predictions.

5

A. EVALUATION METRICS

The following metrics are used for Sentiment analysis

(precision × recall)

1) F1-Score = 2 ×

 (0, 1) (5)

(precision + recall)

(T N + T P )

2) Accuracy =

 (0, 1) (6)

(T N + T P + F N + F P )

Where TP is True Positive, FP is False Positive and FN is False Negative. For topic model coherence scores measures and for Bert embeddings K-Means clustering Silhouette metrics are considered.

CoherenceScore = score(wi, wj)

(7)

i<j

B. Model Building and Hyper-parameters
In Engineering, LDA is set to automatically classify documents and estimate their relevance to various latent topics hidden in them. In this study, document term matrix after preprocessing is achieved using genism doc2bow (bag of words) toolkit for a dictionary of 11057 unique words. Multiple numerical experiments were conducted on LDA with random values of () alpha ranging between 0 <  < 1 (0.01, 0.1, 0.2 and 0.3) and the optimal topic model(k) is arrived as shown from Fig 4. Autoencoder Hyperparameters: The list of hyper parameters to fine-tune auto encoder are:
· Batch size · Epochs · Kernal regularizers (L1 and L2) to add weights to the
layers. · Dropouts to control over fitting. · Learning Rate. · Gamma parameter to control the relative importance of
both LDA and Bert vectors.
Extensive numerical experiments were conducted with various hyper-parameters for fine tuning the auto-encoder layers for topics ranging from 5 ­ 17 are displayed in Table I to compare the performance on each (k) value.

V. RESULTS AND DISCUSSION
In this section, the empirical results of the numerical experiments and related findings are evaluated to assess the effectiveness of the proposed framework T-BERT. In particular the results of LDA are evaluated against BERT and LDA+BERT. The metrics for each topic(k) shown in Fig 4 clearly indicating a gradual rise in the coherence scores until the cut-off point, with k=8 being optimal value. The dataset being unsupervised, BERT contextual embedding results are clustered and visualized using K-Means clustering algorithm grouping similar topics as shown in Fig 5 and the evaluation metrics from Table II. The weighted method obtains better accuracy's in terms of coherence values adding contexts to the topics(k=8) which makes the clustering of similar topics look more meaningful

Fig. 4: Optimal Coherent model output showing (k) value 8 as cut-off point
than LDA topics. Adding weights to auto-encoder layers using L1 Regularizers resulted in over-fitting. However, a dropout of 0.01 and Adam optimizer [?], [33], [34] at the input encoder layer randomly turned off some redundant layers in the network avoiding over-fitting and to ensure a robust model output displayed in Table III.
The goal of this research to demonstrate performance improvement by adding contextual sentence embedding of BERT with LDA topic model with proven accuracy improvements as shown from Table IV. Fig 6a shows the clustering results of BERT and Fig 6b the improved results of LDA+BERT showing a balanced and quite separated clusters. The gains between each model are relatively small, but the improvements of the method are reasonably straight forward and flexible which makes it easier to apply to a variety of other tasks. Visualization of word clouds for final 8 topics in Fig 7 shows that each cluster highlights the proportionality and the frequency of words in similar topics are grouped together. The words bigger in size directly relates to its probability of the word occurrences within the topic. The goal for the numerical experiments on BERT sentiment analysis is two-fold. First, is to evaluate whether the training on the labeled is useful for validation on sentiments classifier on Smile twitter dataset. Second, to evaluate the performance and accuracy on testing the proposed twitter dataset and how well the sentiments are classified on the test data. The model architecture is built in such a way the data loader for both training and validation is sampled from a random and sequential sampler respectively with a batch size of 32 using pytorch pre-trained bert uncased model on a 12GB GPU machines with a fully connected layer. The summary of parameters used in the numerical experiments are shown in the Table V. BERT significantly performs well in classifying the emotions in terms of accuracies (%) as shown from Table VI. In this this study, BERT pre-trained model with its bi-directional and attention mechanism outperforms the predictions of emotions with its improved semantic search approach compared to other state-of-art models.

6

TABLE I: Summary of Auto-encoder Hyper-parameters used in LDA+BERT numerical experiments

Topics K (5,6,7,8,10,12,15,17), Optimizer = Adam, Gamma = 15

Epochs Batch size

Regularizers

Dropout Rate

50

128

L1(10e-4)

0.01

100

128

L1(10e-4)

0.01

200

128

L1(10e-4)

0.01

300

128

L1(10e-2)

0.001

50

128

L2(10e-4)

0.1

500

128

Bias regularizer L2(1e-5), Activity regularizer L1(10e-2)

0.001

Fig. 5: Visualization of BERT Clustering experimented for varying number of topic values (5,6,7,8,10,12,15,17)

TABLE II: Experimental results of LDA and BERT Coherence scores for each topic(k) value in (%)

# of Topics(k) 5 6 7 8 10 12 15 17

LDA 0.402 0.430 0.474 0.501 0.483 0.537 0.567 0.621

BERT 0.481 0.490 0.451 0.521 0.504 0.459 0.441 0.460

TABLE III: Final Auto-encoder Hyper-parameters

Parameters Latent dimension Activation Function Epochs Batch size Regularizers Gamma Dropout Rate Optimizer

Values 64
RELU 50 128
L1(10e-4) 15 0.01
Adam

7

TABLE IV: Final Metrics (%) for Contextual Topic Model (k = 8) comparing LDA, BERT and BERT+LDA

Metrics / Scores
CV (Coherence scores) Silhouette

LDA 0.501 NA

BERT + Clustering 0.521 0.044

LDA + BERT + Clustering
0.56
0.46

(c) Training and Validation Loss of Auto-

(a) BERT

(b) LDA + BERT

encoder

Fig. 6: Final Clusters for topic value k=8 along with Training and validation loss of Auto-encoder

Fig. 7: Visualization of word clouds after LDA and BERT merge. The contextual topic key words for optimal value k= 8 with each word cloud representing each topic clusters.
TABLE V: BERT Hyper-parameters for Sentiments Classification

Optimizer Epochs Batch size Learning rate Adam's epsilon

AdamW (Pytorch) 10 32 1e-5 1e-8

TABLE VI: Accuracy per class ­ Validation results

Class Positive Negative Neutral

Validation Accuracy (%) 0.96 0.78 0.69

Fig. 8: BERT Sentiment predictions on test data-set

VI. CONCLUSION
The potential of leveraging the BERT attention mechanism is investigated in this work, and the recommended contextual subjects and sentiments underlying those topics (positive,

negative, or neutral) are realized through the enhanced performance of T-BERT. Results from numerical experiments demonstrate the effectiveness and generality of the proposed approach. Future study could concentrate on merging direct

8

topic information into BERT pre-trained models with more advanced ways to manage complicated data features, linguistic skills, and semantic search, as well as identifying attitudes in languages other than English.
ACKNOWLEDGMENT
The first author would like to thank Liverpool John Moores University(LJMU), UK for approving this research work as part of her Masters thesis dissertation.
REFERENCES
[1] W. Medhat, A. Hassan, and H. Korashy, "Sentiment analysis algorithms and applications: A survey," Ain Shams Engineering Journal, vol. 5, no. 4, pp. 1093­1113, 12 2014. [Online]. Available: http://dx.doi.org/10.1016/j.asej.2014.04.011https: //linkinghub.elsevier.com/retrieve/pii/S2090447914000550
[2] V. Sharma, V. Sharma, D. Shukla, P. Tanwar, and B. Kumar, "Live Twitter Sentiment Analysis," SSRN Electronic Journal, 2020. [Online]. Available: https://www.ssrn.com/abstract=3609792
[3] A. Hasan, S. Moin, A. Karim, and S. Shamshirband, "Machine LearningBased Sentiment Analysis for Twitter Accounts," Mathematical and Computational Applications, vol. 23, no. 1, p. 11, 2018.
[4] V. Moshkin, A. Konstantinov, and N. Yarushkina, "Application of the BERT Language Model for Sentiment Analysis of Social Network Posts," in NAACL HLT 2019 - 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference, J. J. Park, L. T. Yang, Y.-S. Jeong, and F. Hao, Eds. Singapore: Springer Singapore, 2020, vol. 1, no. Mlm, pp. 274­283. [Online]. Available: http://link.springer.com/10.1007/978-3-030-59535-7 20
[5] U. Naseem, I. Razzak, K. Musial, and M. Imran, "Transformer based Deep Intelligent Contextual Embedding for Twitter sentiment analysis," Future Generation Computer Systems, vol. 113, pp. 58­69, 12 2020. [Online]. Available: https://doi.org/10.1016/j.future.2020.06.050https: //linkinghub.elsevier.com/retrieve/pii/S0167739X2030306X
[6] Q. Li, S. Li, S. Zhang, J. Hu, and J. Hu, "A Review of Text Corpus-Based Tourism Big Data Mining," Applied Sciences, vol. 9, no. 16, p. 3300, 8 2019. [Online]. Available: https://www.mdpi.com/2076-3417/9/16/3300
[7] S. N. Balaji, P. V. Paul, and R. Saravanan, "Survey on sentiment analysis based stock prediction using big data analytics," in 2017 Innovations in Power and Advanced Computing Technologies (i-PACT), vol. 2017-Janua. IEEE, 4 2017, pp. 1­5. [Online]. Available: http://ieeexplore.ieee.org/document/8244943/
[8] I. E. Alaoui, Y. Gahi, and R. Messoussi, "Full Consideration of Big Data Characteristics in Sentiment Analysis Context," in 2019 IEEE 4th International Conference on Cloud Computing and Big Data Analysis (ICCCBDA). IEEE, 4 2019, pp. 126­130. [Online]. Available: https://ieeexplore.ieee.org/document/8725728/
[9] C. K.-S. Leung, F. Jiang, A. G. M. Pazdor, and A. M. Peddle, "Parallel social network mining for interesting `following' patterns," Concurrency and Computation: Practice and Experience, vol. 28, no. 15, pp. 3994­4012, 10 2016. [Online]. Available: http://doi.wiley. com/10.1002/cpe.3773
[10] S. Dwivedi and V. S. K. Roshni, "Recommender system for big data in education," in 2017 5th National Conference on E-Learning & E-Learning Technologies (ELELTECH), vol. 2. IEEE, 8 2017, pp. 1­4. [Online]. Available: http://ieeexplore.ieee.org/document/8074993/
[11] Z. Shah and A. G. Dunn, "Event detection on Twitter by mapping unexpected changes in streaming data into a spatiotemporal lattice," IEEE Transactions on Big Data, vol. 1, no. 1, pp. 1­1, 3 2019. [Online]. Available: http://ieeexplore.ieee.org/document/7153539/https: //ieeexplore.ieee.org/document/8883055/
[12] X. Hu, L. Tang, and H. Liu, "Embracing information explosion without choking: Clustering and labeling in microblogging," IEEE Transactions on Big Data, vol. 1, no. 1, pp. 35­46, 2015.
[13] X. Cheng, X. Yan, Y. Lan, and J. Guo, "BTM: Topic Modeling over Short Texts," IEEE Transactions on Knowledge and Data Engineering, vol. 26, no. 12, pp. 2928­2941, 12 2014. [Online]. Available: http://ieeexplore.ieee.org/document/6778764/
[14] G. Amit, R.-Z. Michal, and Y. Weiss, "Hidden Markoc Topic Models," 2007.

[15] J. Devlin, M. W. Chang, K. Lee, and K. Toutanova, "BERT: Pre-training of deep bidirectional transformers for language understanding," in NAACL HLT 2019 - 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference, vol. 1, 10 2019, pp. 4171­4186. [Online]. Available: http://arxiv.org/abs/1810.04805
[16] Q. Chen and M. Sokolova, "Word2Vec and Doc2Vec in Unsupervised Sentiment Analysis of Clinical Discharge Summaries," 5 2018. [Online]. Available: http://arxiv.org/abs/1805.00352
[17] Z. Jianqiang, G. Xiaolin, and Z. Xuejun, "Deep Convolution Neural Networks for Twitter Sentiment Analysis," IEEE Access, vol. 6, pp. 23 253­23 260, 2018. [Online]. Available: https://ieeexplore.ieee.org/ document/8244338/
[18] A. Abdi, S. M. Shamsuddin, S. Hasan, and J. Piran, "Deep learning-based sentiment classification of evaluative text based on Multi-feature fusion," Information Processing & Management, vol. 56, no. 4, pp. 1245­1259, 7 2019. [Online]. Available: https://doi.org/10.1016/j.ipm.2019.02.018https: //linkinghub.elsevier.com/retrieve/pii/S0306457318307416
[19] P. Ray and A. Chakrabarti, "A Mixed approach of Deep Learning method and Rule-Based method to improve Aspect Level Sentiment Analysis," Applied Computing and Informatics, vol. ahead-of-p, no. ahead-of-print, 8 2020. [Online]. Available: https://www.emerald.com/ insight/content/doi/10.1016/j.aci.2019.02.002/full/html
[20] A. Alsaeedi and M. Z. Khan, "A study on sentiment analysis techniques of Twitter data," International Journal of Advanced Computer Science and Applications, vol. 10, no. 2, pp. 361­374, 2019.
[21] X. Zou, J. Yang, and J. Zhang, "Microblog sentiment analysis using social and topic context," PLoS ONE, vol. 13, no. 2, pp. 1­24, 2018.
[22] J. Zheng, X. Chen, Y. Du, X. Li, and J. Zhang, "Short Text Sentiment Analysis of Micro-blog Based on BERT," in Lecture Notes in Electrical Engineering. Springer Verlag, 2020, vol. 590, pp. 390­396. [Online]. Available: http://link.springer.com/10.1007/978-981-32-9244-4 56
[23] F. Ren and Y. Wu, "Predicting user-topic opinions in twitter with social and topical context," IEEE Transactions on Affective Computing, vol. 4, no. 4, pp. 412­424, 2013.
[24] M. N. Aziz, A. Firmanto, A. M. Fajrin, and R. V. Hari Ginardi, "Sentiment Analysis and Topic Modelling for Identification of Government Service Satisfaction," in 2018 5th International Conference on Information Technology, Computer, and Electrical Engineering (ICITACEE). IEEE, 9 2018, pp. 125­130. [Online]. Available: https://ieeexplore.ieee.org/document/8576974/
[25] R. Albalawi, T. H. Yeap, and M. Benyoucef, "Using Topic Modeling Methods for Short-Text Data: A Comparative Analysis," Frontiers in Artificial Intelligence, vol. 3, no. July, pp. 1­14, 7 2020. [Online]. Available: https://www.frontiersin.org/article/10.3389/ frai.2020.00042/full
[26] C. Li, H. Wang, Z. Zhang, A. Sun, and Z. Ma, "Topic modeling for short texts with auxiliary word embeddings," SIGIR 2016 - Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 165­174, 2016.
[27] W. Gao, M. Peng, H. Wang, Y. Zhang, Q. Xie, and G. Tian, "Incorporating word embeddings into topic modeling of short text," Knowledge and Information Systems, vol. 61, no. 2, pp. 1123­1145, 2019. [Online]. Available: https://doi.org/10.1007/s10115-018-1314-7
[28] A. B. Dieng, F. J. R. Ruiz, and D. M. Blei, "Topic Modeling in Embedding Spaces," Transactions of the Association for Computational Linguistics, vol. 8, pp. 439­453, 2020.
[29] F. Hu, G.-s. Xia, W. Yang, and L. Zhang, "Mining Deep Semantic Representations for Scene Classification of High-Resolution Remote Sensing Imagery," IEEE Transactions on Big Data, vol. 6, no. 3, pp. 522­536, 9 2020. [Online]. Available: https://ieeexplore.ieee.org/ document/8713925/
[30] I. Sutskever, O. Vinyals, and Q. V. Le, "Sequence to sequence learning with neural networks," Advances in Neural Information Processing Systems, vol. 4, no. January, pp. 3104­3112, 2014.
[31] M. Hoang, O. A. Bihorac, and J. Rouces, "Aspect-Based Sentiment Analysis using {BERT}," Proceedings of the 22nd Nordic Conference on Computational Linguistics, pp. 187­196, 2019. [Online]. Available: https://www.aclweb.org/anthology/W19-6120
[32] M. Munikar, S. Shakya, and A. Shrestha, "Fine-grained Sentiment Classification using BERT," International Conference on Artificial Intelligence for Transforming Business and Society, AITB 2019, pp. 2­5, 2019.
[33] Y. Song, J. Wang, Z. Liang, Z. Liu, and T. Jiang, "Utilizing BERT Intermediate Layers for Aspect Based Sentiment Analysis

9

and Natural Language Inference," 2 2020. [Online]. Available: https://arxiv.org/abs/2002.04815http://arxiv.org/abs/2002.04815 [34] N. Reimers and I. Gurevych, "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks," in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Stroudsburg,

PA, USA: Association for Computational Linguistics, 8 2019, pp. 3980­3990. [Online]. Available: https://arxiv.org/abs/1908.10084http: //arxiv.org/abs/1908.10084https://www.aclweb.org/anthology/D19-1410
[35] B. Lwowski, P. Rad, and K.-K. R. Choo, "Geospatial event detection by grouping emotion contagion in social media," IEEE Transactions on Big Data, vol. 6, no. 1, pp. 159­170, 2018.

