TRANSFERABLE ADVERSARIAL EXAMPLES FOR ANCHOR FREE OBJECT DETECTION
Quanyu Liao1, Xin Wang2, Bin Kong2, Siwei Lyu3, Bin Zhu4, Youbing Yin2, Qi Song2, Xi Wu1
1 Chengdu University of Information Technology, Chengdu, China 2 Keya Medical, Seattle, USA
3 University at Buffalo, State University of New York, USA 4 Microsoft Research Asia, Beijing, China

arXiv:2106.01618v2 [cs.CV] 4 Jun 2021

ABSTRACT
Deep neural networks have been demonstrated to be vulnerable to adversarial attacks: subtle perturbation can completely change prediction result. The vulnerability has led to a surge of research in this direction, including adversarial attacks on object detection networks. However, previous studies are dedicated to attacking anchor-based object detectors. In this paper, we present the first adversarial attack on anchor-free object detectors. It conducts category-wise, instead of previously instance-wise, attacks on object detectors, and leverages high-level semantic information to efficiently generate transferable adversarial examples, which can also be transferred to attack other object detectors, even anchor-based detectors such as Faster R-CNN. Experimental results on two benchmark datasets demonstrate that our proposed method achieves state-of-the-art performance and transferability.
Index Terms-- Category-wise attacks, adversarial attacks, object detection, anchor-free object detection
1. INTRODUCTION
The development of deep neural network has significantly improved the performance of many computer vision tasks. However, many recent works show that deep-learning-based algorithms are vulnerable to adversarial attacks [1, 2, 3, 4, 5]. The vulnerability of deep networks is observed in many different problems [6, 7], including object detection, one of the most fundamental tasks in computer vision.
Regarding the investigation of the vulnerability of deep models in object detection, previous efforts mainly focus on classical anchor-based networks such as Faster-RCNN [8]. However, the performance of these anchor-based networks is limited by the choice of anchor boxes. Fewer anchors lead to faster speed but lower accuracy. Thus, advanced anchorfree models such as CornerNet [9] and CenterNet [10] are becoming increasingly popular, achieving competitive accuracy with traditional anchor-based models yet with faster speed
 Corresponding authors: Xin Wang (xinw@keyamedna.com), Xi Wu (xi.wu@cuit.edu.cn). This work was supported by Sichuan Science and Technology Program 2019ZDZX0007, 2019YFG0399, and 2019YFG0496.

Overall Heatmap

Person

Car

Traffic Light Truck

Stop Sign

Fig. 1. First row: The detected results (left) and the proposals (right) of Faster R-CNN [8]. Second row: The detected results (left) and the overall heatmap (right) of CenterNet [10]. Third row: Selected target pixels (red) for each category by our method.
and stronger adaptability. However, to the best of our knowledge, there is no published work on investigating the vulnerability of anchor-free networks.
Previous work DAG [11] achieved high white-box attack performance on the FasterRCNN, but DAG is hardly to complete an effective black-box attack. DAG also has the disadvantages of high time-consuming, these two shortcomings make DAG difficult to be used in real scenes. These two shortcomings of DAG principally because DAG only attacks one proposal in each attack iteration. It will make the generated adversarial perturbation only effective for one proposal, which leads to bad transferring attack performance and consumes an amount of iterations to attack all objects.
Meanwhile, attack an anchor-based detector is unlike to attack an anchor-free detector, which select top proposals from a set of anchors for the objects, anchor-free object detectors detect objects by finding objects' keypoints via the heatmap mechanism (see Fig. 1), using them to generate corresponding bounding boxes, and selecting the most probable keypoints to generate final detection results. This process is completely different from anchor-based detectors, mak-

978-1-6654-3864-3/21/$31.00 ©2021 IEEE

ing anchor-based adversarial attacks unable to directly attack anchor-free detectors.
To solve above two problems, we propose a novel algorithm, Category-wise Attack (CW-Attack), to attack anchorfree object detectors. It attacks all instances in a category simultaneously by attacking a set of target pixels in an image, as shown in Fig. 1. The target pixel set includes not only all detected pixels, which are highly informative pixels as they contain higher-level semantic information of the objects, but also "runner-up pixels" that have a high probability to become rightly detected pixels under small perturbation.Our approach guarantees success of adversarial attacks. Our CW-Attack is formulated as a general framework that minimizes Lp of perturbation, where Lp can be L0, L1, L2, L, etc., to flexibly generate different types of perturbations, such as dense or sparse perturbations. Our experimental results on two benchmark datasets, PascalVOC [12] and MS-COCO [13], show that our method outperforms previous state-of-the-art methods and generates robust adversarial examples with superior transferability.
Our CW-Attack disables object detection by driving feature pixels of objects into wrong categories. This behavior is similar to but the essence is completely different from attacking semantic segmentation approaches [11]. First, they have different targets to optimize: the goal is to change the category of an object's bounding box in our attack and a detected pixel's category in attacking semantic segmentation. Second, they have different relationships to attack success: once pixels have changed their categories, the attack is successful for attacking semantic segmentation but not yet for our attack. As we will see in Fig. 3, objects can still be detected even when all heatmap pixels have been driven into wrong categories.
This paper has the following major contributions: (i) We propose the first adversarial attack on anchor-free object detection. It attacks all objects in a category simultaneously instead of only one object at a time, which avoids perturbation over-fitting on one object and increases transferability of generated perturbation. (ii) Our CW-Attack is designed as a general Lp norm optimization framework. When minimizing perturbation's L0 norm (see Sec. 2), it generates sparse adversarial samples by only modifying less than 1% pixels. While minimizing its L norm (detail in supplement materials), it can attack all objects of all categories simultaneously, which further improves the attacking efficiency. (iii) Our method generates more transferable and robust adversarial examples than previous attacks. It achieves the state-of-the-art attack performance for both white-box and black-box attacks on two public benchmark datasets, MS-COCO and PascalVOC.
2. OUR CATEGORY-WISE ATTACK
In this section, we first define the optimization problem of attacking anchor-free detectors and then provide a detailed description of our Category-wise Attack (CW-Attack).

Target Pixel Selection

False
Attack Success?

True

Person Chair

CenterNet

SCA

Perturbation

Heatmap Detection Result

Table

...

SCA/DCA Attack
DCA

Attack result

Fig. 2. Overview of CW-Attack. Target pixel sets {S1, S2, ..., Sk} are first extracted from the heatmap for all object categories. SCA or DCA is then used to generate perturbation, depending on minimizing perturbation's L0 or L norm. Finally, we check whether the attack is successful. If not, a new perturbation is generated from the current adversarial example in the next iteration.
Problem Formulation. Suppose there exist k object categories, {C1, C2, ..., Ck}, with detected object instances. We use Starget to denote the target pixel set of category Ctarget whose detected object instances will be attacked, leading to k target pixel sets: {S1, S2, ..., Sk}. The category-wise attack for anchor-free detectors is formulated as the following constrained optimization problem:

minimize
r

rp

s.t. k, s  Starget  {S1, S2, ..., Sk} (1)

arg maxn{fn(x + r, s)} = Ctarget

where r is an adversarial perturbation, · p is the Lp norm, p  {0, 1, 2, }, x is a clean input image, x+r is an adversarial example, f (x + r, s) is the classification score vector (logistic) and fn(x + r, s) is its nth value, arg maxn{fn(x + r, s)}) denotes the predicted object category on a target pixel s  Starget of adversarial example x + r.
The overview of the proposed CW-Attack is shown in Fig. 2. In the following description of our method, we assume the task is a non-target multi-class attack. If the task is a target attack, our method can be described in a similar manner. Category-wise Target Pixel Set Selection. In solving our optimization problem (1), it is natural to use all detected pixels of category Ctarget as target pixel set Starget. The detected pixels are selected from the heatmap of category Ctarget generated by an anchor-free detector such as CenterNet [10] with their probability scores higher than the detector's preset visual threshold and being detected as right objects. Unfortunately, it does not work. After attacking all detected pixels into wrong categories, we expect that the detector should not detect any correct object, yet our experiments with CenterNet turn out that it still can.
Further investigation reveals two explanations: (1) Neighboring background pixels of the heatmap not attacked can become detected pixels with the correct category. Since their

2

After Attack

After Attack

Previous

(a) New Detected

Previous

(b) New Detected

Detected Object

Object

Detected Object

Objects

Fig. 3. Blue points denote originally detected keypoints before the attack. Red points denote newly detected keypoint after the attack. (a)-Left & (b)-Left: a detected object and a detected keypoint at the center of the person before the attack. (a)-Right & (b)-Right: detection results after attacking only detected pixels. After attacking all detected pixels, a neighboring pixel of the previously detected keypoint is detected as the correct object for (a)-Right, and the centers of the top half and the bottom half of the person appear as newly detected keypoints still detected as a person for (b)-Right. In both cases, mAP is barely reduced.

detected box is close to the old detected object, CenterNet can still detect the object even though all the previously detected pixels are detected into wrong categories. An example is shown in Fig. 3-(a). (2) CenterNet regards center pixels of an object as keypoints. After attacking detected pixels located around the center of an object, newly detected pixels may appear in other positions of the object, making the detector still be able to detect multiple local parts of the correct object with barely reduced mAP. An example is shown in Fig. 3-(b).
Pixels that can produce one of the above two changes are referred to as runner-up pixels. We find that almost all runner-up pixels have a common characteristic: their probability scores are only a little below the visual threshold. Based on this characteristic, our CW-Attack sets an attacking threshold, tattack, lower than the visual threshold, and then selects all the pixels from the heatmap whose probability score is above tattack into Sk. This makes Sk include all detected pixels and runner-up pixels. Perturbation generated in this way can also improve robustness and transferable attacking performance. Sparse Category-wise Attack. The goal of the sparse attack is to fool the detector while perturbing a minimum number of pixels in the input image. It is equivalent to setting p = 0 in our optimization problem (1), i.e. minimizing r 0 according to Starget. Unfortunately, this is an NP-hard problem. To solve this problem, SparseFool [14] relaxes this NP-hard problem by iteratively approximating the classifier as a local linear function in generating sparse adversarial perturbation for image classification.
Motivated by the success of SparseFool on image classification, we propose Sparse Category-wise Attack (SCA) to generate sparse perturbations for anchor-free object detectors. It is an iterative process. In each iteration, one target pixel set is selected from category-wise target pixel sets to attack.
More specifically, given an input image x and current

1 2
0 1

1 1
0

0

0

Fig. 4. Illustration of SCA with the `Car' category of Fig. 1. The black solid line denotes the real decision boundary of the object detector. Blue points denote adversarial examples that have not attacked all objects successfully. Red point denote adversarial example that have already attacked all objects successfully. This figure illustrates two iterations of the attack, x0  x1 and x1  x2. Take x0  x1 for example, SCA first generates dense adversarial example xB0 (yellow point) by CW-DF and approximated linear decision boundary 0 (green dash lines). Then it uses LinearSolver (purple dash lines) to add a sparse perturbation to support x0 to approximate decision boundary 0 by satisfying  = {x : wT (x - xB) = 0} until a valid sparse adversarial example x1 is obtained. The two images are the visualization of the normal vector w, and the yellow boxes on the two images indicate that the weights for the 'Car' object are reduced.

category-wise target pixel sets {S1, S2, ..., Sk}, SCA selects the pixel set that has the highest probability score from {S1, S2, ..., Sk} as target pixel set Starget and use CategoryWise DeepFool (CW-DF)1 to generate dense adversarial example xB by computing perturbation on Starget. CW-DF is adapted from DeepFool [15] to become a category-wise at-
tack algorithm for anchor-free object detection.
Then, SCA uses the ApproxBoundary to approximate the
decision boundary, which is locally approximated with a hyperplane  passing through xB:

 = {x : wT (x - xB) = 0},

(2)

where w is the normal vector of hyperplane  and approximated with the following equation [14]:

n

w :=  fargmaxnfn(xB,s)(xB , s)

i=1 n

(3)

-

fargmaxnfn(x,s)(xB , s).

i=1

The sparse adversarial perturbation can then be computed via the LinearSolver process [14]. The process of generating perturbation through the ApproxBoundary and the LinearSolver of SCA is illustrated in Fig. 4.
After attacking Starget, SCA uses RemovePixels to update Starget by removing the pixels that are no longer detected.

1See the supplement materials for the detail of CW-DF, ApproxBoundary, LinearSolver and RemovePixels.

3

Algorithm 1 Sparse Category-wise Attack (SCA) Input: image x, target pixel set {S1, S2, ..., Sk},
available categories {C1, C2, ..., Ck} Output: perturbation r
Initialize: x1  x, i  1, j  1, S0  S while {S1, S2, ..., Sk}   do
target = argmaxk sSk softmaxCk f (xi, s) Starget,1  Starget
xi,j  xi while j  Ms or Starget,j   do
xBj = CW-DF (xi,j ) wj = ApproxBoundary (xBj , Starget,j ) xi,j+1 = LinearSolver (xi,j , wj , xBj ) Starget = RemovePixels (xi,j , xi,j+1, Starget)
j =j+1
end while
xi+1  xi,j
i=i+1
end while
return r = xi - x1
Specifically, it takes xi,j, xi,j+1, and Starget as input. RemovePixels first generates a new heatmap for perturbed image xi,j+1 with the detector. Then, it checks whether the probability score of each pixel in Starget is still higher than tattack on the new heatmap. Pixels whose probability score is lower than tattack are removed from Starget, while the remaining pixels are retained in Starget. Target pixel set Starget is thus updated. If {S1, S2, ..., Sk}  , which indicates that no correct objectcan be detected after the attack, the attack for all objects of x is successful, and we output the generated adversarial example.
The SCA algorithm is summarized in Alg. 1. Note that SCA will not fall into an endless loop. In an iteration, if SCA fails to attack any pixels of Starget in the inner loop, SCA will attack the same Starget in the next iteration. During this process, SCA keeps accumulating perturbations on these pixels, with the probability score of each pixel in Starget keeping reducing, until the probability score of every pixel in Starget is lower than tattack. By then, Starget is attacked successfully. Dense Category-wise Attack. It is interesting to investigate our optimization problem (1) for p = . FGSM [16] and PGD [17] are two most widely used attacks by minimizing L. PGD iteratively takes smaller steps in the direction of the gradient. It achieves a higher attack performance and generates smaller L perturbations than FGSM. Our adversarial perturbation generation procedure is base on PGD and is

named as Dense Category-wise Attack (DCA) since it generates dense perturbations compared to SCA.
Given an input image x and category-wise target pixel sets {S1, S2, ..., Sk}, DCA2 applies two iterative loops to generate adversarial perturbations: each inner loop iteration j computes the local gradient for each category Sj and generates a total gradient for all detected categories; while each outer loop iteration i uses the total gradient generated in the inner loop iteration to generate a perturbation for all the objects of all detected categories.
Specifically, in each inner loop iteration j, DCA computes the gradient for every pixel in Sj to attack all object instances in Cj as follows: DCA first computes the total loss of all pixels in target pixel set Sj corresponding to each available category Cj:

losssum = CrossEntropy (f (xi, s), Cj),

(4)

sSj

and then computes local adversarial gradient rj of Sj on losssum and normalizes it with L, yielding rj:

rj = xi losssum, rj =

rj . rj 

(5)

After that, DCA adds up all rj to generate total adversarial gradient G. Finally, in the outer loop iteration i, DCA com-
putes perturbation perti by applying sign operation to the total adversarial gradient G [17]:

perti

=

D
MD

·

sign(G),

(6)

where MD denotes the maximum number of cycles of the

outer loop,

term

D
MD

is

optimal

max-norm

constrained

weight

to constraint the amplitude of perti [16]. At the end of the

outer loop, DCA uses RemoveP ixels to remove the target

pixels that have already been attacked successfully on xi+1

from of {S1, S2, ..., Sk}.

Since an adversarial perturbation in DCA is generated

from normalized adversarial gradients of all categories' ob-

jects, DCA attacks all object instances of all the categories

simultaneously. It is more efficient than SCA.

3. EXPERIMENTAL EVALUATION
Dataset. Our method is evaluated on two object detection benchmarks: PascalVOC [12] and MS-COCO [13]. Evaluation Metrics. i) Attack Success Rate (ASR): ASR = 1-mAPattack/mAPclean, where mAPattack and mAPclean are the mAP of the adversarial example and the clean input, respectively. ii) Attack Transfer Ratio (ATR): It is evaluated as follows: AT R = ASRtarget/ASRorigin, where ASRtarget
2DCA is summarized in Alg. 5 in the supplement materials. Fig. 1 in the supplement materials shows the perturbation generation process of DCA.

4

Fig. 5. Qualitative comparison between DAG and our methods. Each row is an example. Column 1: Detection results of clean inputs on CenterNet. Column 2&3: DAG perturbations and DAG attacked results on Faster-RCNN. Column 4&5: DCA perturbations and DCA attacked results on CenterNet. Column 6&7: SCA perturbations and SCA attacked results on CenterNet. Note that in Column 6, the percentage of perturbed pixels for the SCA perturbations is 3.4% and 3.51% from top to bottom. We can see that the perturbations of DCA and SCA are smaller than DAG's. Notably, the proposed SCA changes only a few percentage of pixels. To better show perturbations, we have multiplied the intensity of all perturbation images by 10.

MS-COCO PascalVOC

Table 1. White-box performance comparison. The top row denotes

the metrics. Clean and Attack denote the mAP of clean input and

adversarial examples, respectively. Time is the average time to gen-

erate an adversarial example.

Method Network Clean Attack ASR Time (s)

DAG UEA SCA SCA DCA DCA SCA SCA DCA DCA

FR FR R18 DLA34 R18 DLA34 R18 DLA34 R18 DLA34

0.70 0.050 0.92 0.70 0.050 0.93 0.67 0.060 0.91 0.77 0.110 0.86 0.67 0.070 0.90 0.77 0.050 0.94 0.29 0.027 0.91 0.37 0.030 0.92 0.29 0.002 0.99 0.37 0.002 0.99

9.8 ­ 20.1 91.5 0.3 0.7 50.4 216.0 1.5 2.4

is the ASR of the target object detector to be black-box attacked, and ASRorigin is the ASR of the detector that generates the adversarial example. iii) Perceptibility: The perceptibility of an adversarial perturbation is quantified by its PL2 and PL0 norm. a) PL2 : PL2 = 1/N rN2 , where the N is the number of image pixels. We normalize the PL2 from [0, 255] to [0, 1]. b) PL0 : PL0 is computed by measuring the proportion of perturbed pixels after attack. White-Box Attack3. We have conducted white-box attacks on two popular object detection methods. Both use CenterNet but with different backbones: one, denoted as R18, with Resdcn18 [18] and the other, DLA34 [19], with Hourglass [20].
Table. 1 shows the white-box attack results on both PascalVOC and MS-COCO. For comparison, it also contains the reported attack results of DAG and UEA attacking FasterRCNN with VGG16 [21] backbone, denoted as FR, on PascalVOC. There is no reported attack performance on MSCOCO for DAG and UEA. UEA's average attack time in Table. 1 is marked as "­" (unavailable) because, as a GANbased apporach, UEA's average attack time should include GAN's training time, which is unavailable. Compare with optimization-based attack methods [11], a GAN-based attack method consumes a lot of time for training and needs to re-

3More experimental results and hyperparameters analysis of DCA and SCA are included in the supplement material.

Table 2. Black-box attack results on the PascalVOC dataset. From

in the leftmost column denotes the models where adversarial exam-

ples are generated from. To in the top row means the attacked models

that adversarial examples transfer to.

From

To Resdcn18

DLA34

Resdcn101 Faster-RCNN SSD300

mAP ATR mAP ATR mAP ATR mAP ATR mAP ATR

Clean DAG [11] R18-DCA DLA34-DCA R18-SCA DLA34-SCA

0.67 ­ 0.65 0.19 0.10 1.00 0.50 0.28 0.31 1.00 0.42 0.90

0.77 ­ 0.75 0.16 0.62 0.23 0.07 1.00 0.62 0.36 0.41 1.00

0.76 ­ 0.74 0.16 0.65 0.17 0.62 0.2 0.61 0.37 0.53 0.65

0.71 ­ 0.60 1.00 0.61 0.17 0.53 0.28 0.55 0.42 0.44 0.82

0.77 ­ 0.76 0.08 0.72 0.08 0.67 0.14 0.70 0.17 0.62 0.42

Table 3. Black-box attack results on the MS-COCO dataset. From in the leftmost column denotes the models where adversarial examples are generated from. To in the top row means the attacked models that adversarial examples transfer to.

From

To Resdcn18

DLA34

Resdcn101 CornerNet

mAP ATR mAP ATR mAP ATR mAP ATR

Clean R18-DCA DLA34-DCA R18-SCA DLA34-SCA

0.29 ­ 0.37 ­ 0.37 ­ 0.43 ­ 0.01 1.00 0.29 0.21 0.28 0.25 0.38 0.12 0.10 0.67 0.01 1.00 0.12 0.69 0.13 0.72 0.11 1.00 0.27 0.41 0.24 0.57 0.35 0.30 0.07 0.92 0.06 1.00 0.09 0.92 0.12 0.88

train a new model to attack another task. Thus a GAN-based attack method sacrifices attack flexibility and cannot be used in some scenarios with high flexibility requirements.
The top half of Table. 1 shows the attack performance on PascalVOC. We can see that: (1) DCA achieves higher ASR than DAG and UEA, and SCA achieves the best ASR performance. (2) DCA is 14 times faster than DAG. We cannot compare with UEA since its attack time is unavailable. Qualitative comparison between DAG and our methods in shown in Fig. 5. The bottom half of Table. 1 shows the attack performance of our methods on MS-COCO. SCA's ASR on both R18 and DLA34 is in the same ballpark as the ASR of DAG and UEA on PascalVOC, while DCA achieves the highest ASR, 99.0%. We conclude that both DCA and SCA achieve the state-of-the-art attack performance. Black-Box Attack and Transferability. Black-box attacks can be classified into two categories: cross-backbone and cross-network. For cross-backbone attacks, we evaluate the

5

Table 4. Perceptibility of the perturbation.

Network
DAG R18-Pascal DLA34-Pascal R18-COCO DLA34-COCO

PL2
2.8 × 10-3 5.1 × 10-3 (DCA) 5.1 × 10-3 (DCA) 4.8 × 10-3 (DCA) 5.2 × 10-3 (DCA)

PL0
 99.0% 0.22% (SCA) 0.27% (SCA) 0.39% (SCA) 0.65% (SCA)

transferability with Resdcn101 [18] on PascalVOC and MSCOCO. For cross-network attack, we evaluate with not only anchor-free object detector CornerNet [9] but also two-stage anchor-based detectors, Faster-RCNN [8] and SSD300 [22]. Faster-RCNN and SSD300 are tested on PascalVOC. CornerNet is tested on MS-COCO with backbone Hourglass [20].
To simulate a real-world attack transferring scenario, we generate adversarial examples on the CenterNet and save them in the JPEG format, which may cause them to lose the ability to attack target models [23] as some key detailed information may get lost due to the lossy JPEG compression. Then, we reload them to attack target models and compute mAP . This process has a more strict demand on adversarial examples but should improve their transferability.
i) Attack transferability on PascalVOC. Adversarial examples are generated on CenterNet with Resdcn18 and DLA34 backbones for both SCA and DCA. For comparison, DAG is also used to generate adversarial examples on Faster-RCNN. These adversarial examples are then used to attack the other four models. All the five models are trained on PascalVOC. Table. 2 shows the experimental results. We can see from the table that adversarial examples generated by our method can successfully transfer to not only CenterNet with different backbones but also completely different types of object detectors, Faster-RCNN and SSD. We can also see that DCA is more robust to the JPEG compression than SCA, while SCA achieves higher ATR than DCA in the black-box test. Table. 2 indicates that DAG is sensitive to the JPEG compression, especially when its adversarial examples are used to attack Faster-RCNN, and has a very poor transferability in attacking CenterNet and SSD300. We conclude that both DCA and SCA perform better than DAG on both transferability and robustness to the JPEG compression.
ii) Attack Transferability on MS-COCO. Similar to the above experiments, adversarial examples are generated on Centernet with Resdcn18 and DLA34 backbones and then used to attack other object detection models. The experimental results are summarized in Table. 3. The table indicates that generated adversarial examples can attack not only CenterNet with different backbones but also CornerNet. Perceptibility. The perceptibility results of adversarial perturbations of DCA and SCA are shown on Table. 4. We can see that PL0 of SCA is lower than 1%, meaning that SCA can fool the detectors by perturbing only a few number of pixels. Although DCA has a higher PL2 than DAG, perturbations generated by DCA are still hard for humans to perceive. We also provide qualitative examples for comparison in Fig. 5.

4. CONCLUSION
In this paper, we propose a category-wise attack to attack anchor-free object detectors. To the best of our knowledge, it is the first adversarial attack on anchor-free object detectors. Our attack manifests in two forms, SCA and DCA, when minimizing the L0 and L norms, respectively. Both SCA and DCA focus on global and high-level semantic information to generate adversarial perturbations. Our experiments with CenterNet on two public object detection benchmarks indicate that both SCA and DCA achieve the state-of-the-art attack performance and transferability.
5. REFERENCES
[1] Nicholas Carlini and David Wagner, "Towards evaluating the robustness of neural networks," in IEEE SP, 2017.
[2] Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, and et al, "Boosting adversarial attacks with momentum," in CVPR, 2018.
[3] Cihang Xie, Zhishuai Zhang, Yuyin Zhou, Song Bai, Jianyu Wang, Zhou Ren, and Alan L Yuille, "Improving transferability of adversarial examples with input diversity," in CVPR, 2019.
[4] Francesco Croce and Matthias Hein, "Minimally distorted adversarial examples with a fast adaptive boundary attack," in ICML, 2020.
[5] Yinpeng Dong, Jun Zhu, and et al, "Evading defenses to transferable adversarial examples by translationinvariant attacks," in CVPR, 2019.
[6] Avishek Joey Bose and Parham Aarabi, "Adversarial attacks on face detectors using neural net based constrained optimization," in MMSP, 2018.
[7] Shang-Tse Chen, Cory Cornelius, and et al, "Robust physical adversarial attack on faster r-cnn object detector," in ECMLKDD, 2018.
[8] Shaoqing Ren, Kaiming He, and et al, "Faster r-cnn: Towards real-time object detection with region proposal networks," in NIPS, 2015.
[9] Hei Law and Jia Deng, "Cornernet: Detecting objects as paired keypoints," in IJCV, 2019.
[10] Xingyi Zhou, Dequan Wang, and Philipp Kra¨henbu¨hl, "Objects as points," in CVPR, 2019.
[11] Cihang Xie, Jianyu Wang, Zhishuai Zhang, Yuyin Zhou, Lingxi Xie, and Alan Yuille, "Adversarial examples for semantic segmentation and object detection," in ICCV, 2017.
[12] Mark Everingham, SM Ali Eslami, and et al, "The pascal visual object classes challenge: A retrospective," in IJCV, 2015.
[13] Tsung-Yi Lin, Michael Maire, and et al, "Microsoft coco: Common objects in context," in ECCV, 2014.

6

[14] Apostolos Modas, Seyed-Mohsen Moosavi-Dezfooli, and et al, "Sparsefool: a few pixels make a big difference," in CVPR, 2019.
[15] Seyed-Mohsen Moosavi-Dezfooli and et al, "Deepfool: a simple and accurate method to fool deep neural networks," in CVPR, 2016.
[16] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy, "Explaining and harnessing adversarial examples," in ICLR, 2015.
[17] Aleksander Madry, Aleksandar Makelov, and et al, "Towards deep learning models resistant to adversarial attacks.," in ICLR, 2018.
[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, "Deep residual learning for image recognition," in CVPR, 2016.
[19] Fisher Yu, Dequan Wang, Evan Shelhamer, and Trevor Darrell, "Deep layer aggregation," in CVPR, 2018.
[20] Alejandro Newell, Kaiyu Yang, and Jia Deng, "Stacked hourglass networks for human pose estimation," in ECCV, 2016.
[21] Karen Simonyan and Andrew Zisserman, "Very deep convolutional networks for large-scale image recognition," in ICLR, 2014.
[22] Wei Liu, Dragomir Anguelov, and et al, "Ssd: Single shot multibox detector," in ECCV, 2016.
[23] Gintare Karolina Dziugaite, Zoubin Ghahramani, and Daniel M Roy, "A study of the effect of jpg compression on adversarial images," in CVPR, 2016.
7

