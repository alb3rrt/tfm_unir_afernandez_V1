arXiv:2106.00199v1 [physics.soc-ph] 1 Jun 2021

Agent mental models and Bayesian rules as a tool to create opinion dynamics models
Andr´e C. R. Martins GRIFE ­ EACH ­ Universidade de S~ao Paulo, Rua Arlindo B´etio, 1000, 03828­000, S~ao Paulo, Brazil
June 2, 2021
Abstract
Traditional opinion dynamics models are simple and yet, enough to explore the consequences in basic scenarios. But, to better describe problems such as polarization and extremism, we might need to include details about human biases and other cognitive characteristics. In this paper, I explain how we can describe and use mental models and assumptions of the agents using Bayesian-inspired model building. The relationship between human rationality and Bayesian methods will be explored, and we will see that Bayesian ideas can indeed be used to explain how humans reason. We will see how to use Bayesian-inspired rules using the simplest version of the Continuous Opinions and Discrete Actions (CODA) model. From that, we will explore how we can obtain update rules that include human behavioral characteristics such as confirmation bias, motivated reasoning, or our tendency to change opinions much less than we should.
Keywords: Opinion dynamics,Bayesian methods, Cognition, CODA, Agent-based models
1 Introduction: The need for general methods
Opinion dynamics models [1, 2, 3, 4, 5, 6, 7, 8, 9] try to describe how opinions spread through a society. Traditional problems in the field include the formation of consensus in societies [10], the emergence of polarization [11, 12, 13, 14] and the different ways we can define it [15]. They also describe the appearance and spread of extreme opinions [16, 17, 18, 19, 20, 21, 22, 23, 24]. Extremism can be defined as the end of a range over continuous variable [7, 25], or as inflexibles who do not change their minds[26], or using mixed models [8, 9, 27]. To explore the problem of extremism in the real world, not only opinions matter [28] and we must also consider actions as part of what defines an extremist [29, 30].
However, there are still many aspects in opinion dynamics that have been introduced only in occasional models and where community efforts are very much
1

needed [31]. And most models tend to be comparable with similar implementations only. There is a lack of translation between different types. While attempts to propose general frameworks and universal formulas do exist [9, 32, 33], they are, so far, isolated efforts. If we want to move further, we need to understand how different types of models relate to each other [34]. And we need ways to automatically add new effects and assumptions to our models. For example, we know we must incorporate aspects that allow polarization to persist even outside echo-chambers. That can happen due to several factors, such as confirmation biases or limitations to the processing of complex information [35]. Decisions and behavioral aspects can be crucial in opinions [36, 37]. To include those, we might need ways to model aspects of the mental models of the agents. That is, how they see the world, the assumptions they make about the opinions of their neighbors, and how they incorporate that information in their own views. One way to do that is to use Bayesian-inspired models [9].
Bayesian rules to model opinions have been introduced both in the opinion dynamics community, as extensions of the Continuous Opinions and Discrete Actions (CODA) [8, 9] and similar opinion models [9, 21, 27, 34, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 28], by the use of Bayesian belief networks [68], as well as, independently, in models associated to economical reasoning [69, 70, 71, 72, 73, 74]. Despite their popularity, there are two aspects of Bayesian-inspired models for opinion dynamics that have not been properly debated so far. They are how to turn assumptions on how the agents reason on dynamical model equations and the problem of the relationship between Bayesianism and rationality.
In this paper, I will explore both aspects of the problem. First, I will provide a brief explanation on why the use of Bayesian-inspired rules is both supported by experimental evidence [75, 76, 77, 78, 73] and not the same as assuming rationality [79, 80]. And, to illustrate how Bayesian rules can be used in a general problem, I will explain how we can create mental models for the agents. And we will see how models can include any kind of bias and bounded rationality effects [81, 82], and turn those assumptions into update rules.
2 Bayesian models and rationality
Bayesian methods are one of our golden standards for rationality and inductive arguments [80]. If we start with simple rules about how induction about plausibilities must be performed, we can show that plausibilities should be updated using Bayes theorem [83, 84]. The same theorem can be obtained from other axioms, such as maximization of entropy [85] or the much weaker basis of "dutch books". However, using those ideas to describe how people reason has been considered problematic [86]. On the other hand, Bayesian ideas can be used both in a hard way, strictly following its rules, or as a soft version, where its basic ideas are used to represent aspects such as updating subjective opinions [87]. That poses the question of whether Bayesian rules can be used to describe our reasoning well. Of course, we should also ask about the requirements we
2

impose from our rationality models. Even the definition of bounded rationality can be challenged, as it assumes there is someone to judge if any behavior is entirely rational [88]. Indeed, using Bayesian methods with perfection is impossible as they require infinite abilities [80]. We can only approximate them by considering a limited set of possibilities, and that is compatible with how our brains work.
There is good evidence that we reason in ways that are similar to Bayesian methods [75, 76, 77, 78, 73]. But, if we want to use it in mathematical and computational models, we need to go further than just similarity. Indeed, it is clear that humans are not perfectly rational nor good at statistics. We sometimes fail at easy problems [89, 90] and we tend to be too confident about our mental skills [91]. At first, experiments about our cognitive abilities seemed to point at a remarkable amount of incompetence.
But that is not the whole story. When we look closer, some of our mistakes are not as serious as they look. While we are not good abstract logicians, when the same problems are presented associated with normal day-to-day circumstances, we answer them correctly [92]. And there is evidence that many of our mistakes can be described as the use of reasonable heuristics [93], shortcuts that allow us to arrive at answers fast and with less effort [94, 95]. As simplified rules, heuristics fail under some circumstances. If we go looking for those cases, we will undoubtedly find them. But they are not a sign of complete incompetence.
That does not explain our overconfidence problems, of course. But we have also observed that our reasoning skills might not have evolved to find the best answers, even if we can use them for that purpose. Instead, humans show a tendency to defend their identity-defining beliefs [96, 97]. More than that, our ancestors had solid reasons to be good at fitting inside their groups and, if possible, ascend socially inside those. Our reasoning and argumentative skills were more valuable from an evolutionary perspective if they worked towards agreeing or convincing our peers. Group belonging mattered more than being right about things that would not affect direct survival [98, 99]. Being sure and defending ideas from our social group would have been more important than looking for better but unpopular answers.
And there is one more issue. In many laboratory problems where we observed humans make mistakes, scientists used questions that would never appear in real life [79]. Take the case of the observation of weighting functions, that we seem to alter probability values that we hear [100]. Using changed values might seem to serve no purpose at first. However, the scientists who performed those experiments assumed the values they presented were known with certainty. But there is no such certainty in real life. If someone tells you there is a 30% chance of rain tomorrow, even if based on very well-calibrated models, you know that is an estimate. As with any estimates, at best, the actual value is close to 30%. A Bayesian assessment of the problem would combine our previous estimate of rain with the information about the forecast to obtain a final opinion. If we do that with the many experiments that showed we use probability values wrong, we see that our behavior is compatible with Bayesian rules. The observed changes
3

match reasonable assumptions for everyday situations when we hear uncertain estimates [76]. Doing that would be wrong in artificial cases, such as those in the laboratory experiments, where there is no (or very little) uncertainty about the probability values. Even our tendency to change our opinions far less than we should, called conservatism (no relationship to politics implied in the technical term) [101] can easily be explained. We just need to include in a Bayesian model a tendency to skepticism about the data we hear [76]. Our brains might just have heuristics that mimick Bayesian estimates for an uncertain and unreliable world.
That is, we are not perfect, but our bounded abilities are not those of incompetents. We make reasonable approximations. We are motivated reasoners, more interested in defending ideas than looking for better answers. Given the right preferences, that can even be described as rational, despite ethical considerations. Even when it seemed we were making mistakes, we might have been behaving closer to Bayesian rules than it was initially assumed.
3 Update rules from the agent mental models
We are not perfectly rational, but we can still be described by Bayesian rules. Therefore, it makes sense to try Bayesian methods as a way to represent human opinions. Thus, the next question we must answer is how we can include our biases and cognitive characteristics in our models. To do that, we must consider how the agents think, what they show to others, and what they expect to see from their neighbors. That is, we need to describe their mental models. But, first, it makes sense to ask how Bayesian methods work.
3.1 A very short introduction to Bayesian methods
While Bayesian statistics, done correctly, can become complicated fast, it is based on an elementary, almost trivial basis, the Bayes Theorem. It works like this. We have an issue we want to learn about, and we represent it by a variable random X, where each x represents one possible value. Here, x can be a quantity, but it can also be nothing more than a label. We start with a probability distribution, our initial guess, on how likely each possible x is, represented by a probability distribution f (x), called the prior opinion. Once we observe data D, we must change our opinions on x. To do that, we need to know, for each possible value x, how likely it would have been that we would observe D. That is, we need the likelihood, f (D|x). From that, calculating the posterior estimate f (x|D) is done just by a simple multiplication f (x|D)  f (x)f (D|x). The proportionality constant is calculated by imposing that the final distribution must add (or integrate) to one. Everything in Bayesian methods is a consequence of that update rule and considerations on how to use it.
To illustrate how it is done, let us look at the demonstration of the CODA model rules. In CODA, the agents try to decide between two possible choices, A or B (sometimes represented as values of a spin, +1 or -1). Each agent i has,
4

a time t, a probability opinion pi(t) that A is better than B (and, 1 - pi(t) that B is better). But, instead of expressing their probabilistic opinion, they only show their neighbors the option they considered more probably better. They also assume their neighbors have a larger than 50% chance  to pick the best option. In principle, there could be assymetric different chances,  to choose A when A is better and  to choose B when B is better. But we will assume the  =  symmetry here. From Bayes rule, we get the update model for pi(t + 1)  pi(t) and, similarly, 1 - pi(t + 1)  (1 - pi(t))(1 - ). As the probabilities must add one, we divide by their sum and get the update rule

pi(t

+

1)

=

pi(t)

+

pi(t) (1 - pi(t))(1

-

) .

(1)

The

update

rule

is

much

simpler

if

we

make

the

transformation



=

ln(

p 1-p

).

The denominators cancel and we get

(t + 1) = (t) ± C,

(2)

where

C

=

ln(

 1-

)

and

the

sign

on

the

sum

depends

on

whether

the

neighbor

prefers A (+) or B (-). We can get an even simpler model by renormalizing

 and making C = 1. And that is it. We start from the initial opinion, use

the Bayes theorem and we get an update rule. In this case, the final rule is to

add one when an neighbor prefer A and subtract one when it prefers B, flipping

opinions at  = 0.

3.2 Agent communication rules and their mental assump-
tions
There were two major assumptions in the CODA model. One was how agents communicate. They have a probabilistic estimate of which option is better, but they only give their best estimate. The second assumption is the mental model of the agents. They think their neighbors will pick the best choice with probability . In principle,  > 0.5, but if we introduce some agents that think their neighbors are more likely to be wrong, that is,  < 0.5, we have just included contrarians [40], agents who tend to disagreement [102].
Making the model assumption explicit makes it easier to investigate what happens if agents behave or act differently. For example, we could have a situation where agents look for the best choice, but they fully communicate their probability estimates. In that case, while we can keep the probability pi(t) that A is better as the agent opinion, we need a probability distribution over the possible probability values, a distribution probability of probabilities, to represent the mental model of the agent. In mathematical terms, we need a model that says, assuming A is better, how likely it would be the neighbor j would have an opinion pj if A is better, f (pj|A). Of course, we also need f (pj|B), but, in most situations, that can be obtained by symmetry assumptions. This was implemented originally by assuming f (pj|A) was a Beta function. In this new model, extreme opinions become much stronger [53, 28].

5

While we did need a new likelihood, that was caused by a change in the communication. But it would be possible for the agents to have very different assumptions in their mental model. Assume that, instead of having "wisher" agents looking for the best option between A and B, each "mixer" agent has an estimate about the best mixture between A and B. In this case, pi is the percentage of A in the best blend of two options and, as such, each value 0  pi  1 must have a probability. We need probability densities f (p) as prior and as posterior distributions. The easier way to implement such a model is to look for conjugate distributions, that is, cases where, for a certain likelihood, the prior and the posterior will be represented by the same function. In that case, update rules can just update parameters and not complete distributions. Interestingly, while the dynamics of the preferred choice, given the more straightforward choice of proper functions for the prior and the likelihood, mimics the original CODA dynamics, probability values never go to the same extreme values [53].
Other variations are possible and have been explored. An initial approach to trust was implemented in a fully connected setting by adding one assumption to the agent mental model [47]. Instead of assuming that every other agent had a probability  to get the best answer, each agent assumed there were two types of agents. Agent i assumed there was a probability ij that agent j was a reliable source, who would pick the best option with chance . But other agents could also be untrustworthy (or useless) and pick the best option with probability µ so that µ < , possibly even 0.5 or lower. That is, if A was the best choice, instead of a chance  neighbor j would prefer A, there was a chance given by ij + µ(1 - ij). Applying Bayes theorem with this new likelihood led to update rules both for pi and ij. Each agent updated both its opinions on whether A or B would be better. And it also changed its estimates about the trustworthiness of the observed agent. The update rule could not be simplified by a transformation of variables because no exact way to uncouple the evolution of the opinion and how much agents trusted their neighbors was found.
A similar idea was used to make a Bayesian version for continuous communication, and "mixer"-type of agents. That model [38]led to an evolution of opinions qualitatively equivalent to what we observe in Bounded Confidence models [7, 25]. That continuous model was later extended to study the problem of several independent issues when agents adjusted their trust based not only on the subject being debated but also on their neighbor positions on the other matters [66]. Interestingly, that did cause opinions to become more clustered and aligned, similar to the irrational consistency we observe in humans [103].
Even the agent's influence on its neighbors can be used for their mental models. That was introduced as a simple version by assuming that there were different chances a and c that a neighbor would prefer A in the case a was indeed better, depending on whether the observed agent also preferred A or not [34]. That actually weakened the reinforcement effects of agreement, as the other agent could think A was better not because it was but because the observer also thought that. In the limit of strong influence, the dynamics of the voter model [104, 105] ­ or other types of discrete models, such as majority [106, 107] or Sznajd [6] rules, depending on the interaction rules ­ was recovered. That
6

shows that Bayesian-inspired models are much more general than the traditional discrete versions.

3.3 Introducing behavioral questions

Bayesian rules can help us explain how humans reason. And we just saw a few examples about how to introduce extra details in the agent mental models so that new assumptions can be included. In this subsection, I will briefly discuss how we can go ahead and model some of the biases we observe in human behavior.
Let us start with the easier one, confirmation bias [108], as that does not even need new mental models. Confirmation bias is simply our tendency to look for information from sources who agree with us. As such, it can be better modeled by introducing rules that reconnect the network of influence so that agents will more likely be surrounded by those who agree with them. The co-evolution of the CODA model over a network that evolved with thermal noise based on the agreement or disagreement of the agents and their physical location was studied, and, depending on the noise, the tendency to polarization and confirmation bias was very clear [63].
Motivated reasoning [96], on the other hand, is not only about who we learn from but about how we interpret information depending on whether we agree with it or not. That can be implemented in more than one way. A simple version is the approach where trust was introduced in the CODA model [47]. In that model, depending on the initial conditions, as agents become more confident about their estimates, they would eventually distrust those who disagreed with them, even when they met for the first time.
But there are other possibilities, including more heavy-handed approaches. For example, agents might think that being untrustworthy was associated with one of the two options. Instead of a trust matrix signaling how much each agent i trusts each agent j, we can introduce trust based on the possible choices. For two options, A and B, we can assume that each agent has a prior preference. Each agent believes that untrustworthy people will defend only the side the agent has a bias against. That can be represented by a small addition to CODA model. Assume agent i prefers A and it thinks that people would only go wrong to defend B. One way to describe that is to assume that there is a proportion  of reasonable agents who behave like CODA. That is, they pick the better alternative with chance  > 0.5. However, the remaining 1 -  choose B more often, regardless of whether it is true or not, with probability  > 0.5. That is, for agents biased towards A, the chance a neighbor would choose A, represented by CA, if A (or B) is better is given by the equations

P (CA|A) =  + (1 - )(1 - ),

and Also,

P (CA|B) = (1 - ) + (1 - )(1 - ). P (CB|A) = (1 - ) + (1 - )

7

and P (CB|B) =  + (1 - ).

In principle, we could introduce an update rule for both the probability pi that A

is better and . However, for these exercise, let us assume a fixed value for . For

example, we can suppose there is a majority of honest people given by  = 0.8

Let us also assume that honest people get the better answer with a chance of

 = 0.6, while biased people provide their wrong estimate of B with a probability

of  = 0.9 That makes P (CA|A) = 0.5, P (CA|B) = 0.34, P (CB|A) = 0.5 and

P (CB|B) = 0.66. So, if agent i observes someone who prefers A, it will update

its

opinion

by

pi(t + 1) =

pi

pi (t)0.5 (t)0.5+(1-pi

)0.34

.

That

is,

for

the

CODA

transformed

variable



=

ln(

p 1-p

),

we

will

have

i(t

+

1)

=

i(t)

+

ln(

0.5 0.34

)



i(t)

+

0.386.

On the other hand, if B is observed, the update rule will provide (again for

an

agent

biased

towards

A),

i(t + 1)

=

i(t)

+

ln(

0.5 0.66

)



i(t) - 0.278.

That

means that steps in favor of A will be larger than steps in favor of B for such an

agent. While that agent can be convinced by a majority, if there is a tie in the

neighbors, the agent will move towards its preference. Depending on the exact

values of the parameters, the ratio between the step sides can become larger.

As a final example, let us see how we can introduce the effect called conser-

vatism [101], where people change their opinions less than they should. That

can be quickly introduced using a mental model where the agent thinks there

is a chance the data is reliable and a chance that the information is only non-

informative noise. When that happens, it is only natural that the size of the

update will be much smaller. The larger the chance associated with noisy data,

the smaller the size of the update.

4 Conclusion
Approximations to the complete but impossible Bayesian rules can describe human behavior well if we assume people do not really trust others perfectly and reason in motivated ways. That means we can use those methods to create reasonably realistic models of human behavior. Of course, the examples I presented here are too simplistic to be labeled actual mental models for humans. Still, they are enough to describe specific biases. And we have seen how to obtain update rules from assumptions about the agent's mental models. Another important aspect of using Bayesian-inspired rules is that the relationship between distinct models becomes clearer. By investigating which assumptions lead to our current opinion models, we can learn how they relate to each other. That also allows us to understand better the circumstances where each model might be more helpful.

5 Acknowledgments
This work was supported by the Funda¸ca~o de Amparo a Pesquisa do Estado de Sa~o Paulo (FAPESP) under grant 2019/26987-2.

8

References
[1] C. Castellano, S. Fortunato, and V. Loreto. Statistical physics of social dynamics. Reviews of Modern Physics, 81:591­646, 2009.
[2] Serge Galam. Sociophysics: A Physicist's Modeling of Psycho-political Phenomena. Springer, 2012.
[3] B. Latan´e. The psychology of social impact. Am. Psychol., 36:343­365, 1981.
[4] S. Galam, Y. Gefen, and Y. Shapir. Sociophysics: A new approach of sociological collective behavior: Mean-behavior description of a strike. J. Math. Sociol., 9:1­13, 1982.
[5] S. Galam and S. Moscovici. Towards a theory of collective phenomena: Consensus and attitude changes in groups. Eur. J. Soc. Psychol., 21:49­ 74, 1991.
[6] K. Sznajd-Weron and J. Sznajd. Opinion evolution in a closed community. Int. J. Mod. Phys. C, 11:1157, 2000.
[7] G. Deffuant, D. Neau, F. Amblard, and G. Weisbuch. Mixing beliefs among interacting agents. Adv. Compl. Sys., 3:87­98, 2000.
[8] Andr´e C. R. Martins. Continuous opinions and discrete actions in opinion dynamics problems. Int. J. of Mod. Phys. C, 19(4):617­624, 2008.
[9] Andr´e C. R. Martins. Bayesian updating as basis for opinion dynamics models. AIP Conf. Proc., 1490:212­221, 2012.
[10] Hendrik Schawe, Sylvain Fontaine, and Laura Hern´andez. The bridges to consensus: Network effects in a bounded confidence opinion dynamics model, 2021.
[11] Paul DiMaggio, John Evans, and Bethany Bryson. Have american's social attitudes become more polarized? American Journal of Sociology, 102(3):690­755, 1996.
[12] Delia Baldassarri and Andrew Gelman. Partisans without constraint: Political polarization and trends in american public opinion. American Journal of Sociology, 114(2):408­446, 2008.
[13] Charles S. Taber, Damon Cann, and Simona Kucsova. The motivated processing of political arguments. Political Behavior, 31(2):137­155, Jun 2009.
[14] Philipp Dreyer and Johann Bauer. Does voter polarisation induce party extremism? the moderating role of abstention. West European Politics, 42(4):824­847, 2019.
9

[15] Aaron Bramson, Patrick Grim, Daniel J. Singer, William J. Berger, Graham Sack, Steven Fisher, Carissa Flocken, and Bennett Holman. Understanding polarization: Meanings, measures, and model evaluation. Philosophy of Science, 84(1):115­159, 2017.
[16] G. Deffuant, F. Amblard, and T. Weisbuch, G.and Faure. How can extremism prevail? a study based on the relative agreement interaction model. JASSS-The Journal Of Artificial Societies And Social Simulation, 5(4):1, 2002.
[17] F. Amblard and G. Deffuant. The role of network topology on extremism propagation with the relative agreement opinion dynamics. Physica A, 343:725­738, 2004.
[18] S. Galam. Heterogeneous beliefs, segregation, and extremism in the making of public opinions. Physical Review E, 71:046123, 2005.
[19] G. Weisbuch, G. Deffuant, and F. Amblard. Persuasion dynamics. Physica A, 353:555­575, 2005.
[20] Daniel W. Franks, Jason Noble, Peter Kaufmann, and Sigrid Stagl. Extremism propagation in social networks with hubs. Adaptive Behavior, 16(4):264­274, 2008.
[21] Andr´e C. R. Martins. Mobility and social network effects on extremist opinions. Phys. Rev. E, 78:036104, 2008.
[22] L. Li, A. Scaglione, A. Swami, and Q. Zhao. Consensus, polarization and clustering of opinions in social networks. IEEE Journal on Selected Areas in Communications, 31(6):1072­1083, June 2013.
[23] S. E. Parsegov, A. V. Proskurnikov, R. Tempo, and N. E. Friedkin. Novel multidimensional models of opinion dynamics in social networks. IEEE Transactions on Automatic Control, 62(5):2270­2285, May 2017.
[24] V. Amelkin, F. Bullo, and A. K. Singh. Polar opinion dynamics in social networks. IEEE Transactions on Automatic Control, 62(11):5650­5665, Nov 2017.
[25] R. Hegselmann and U. Krause. Opinion dynamics and bounded confidence models, analysis and simulation. Journal of Artificial Societies and Social Simulations, 5(3):3, 2002.
[26] S. Galam and F. Jacobs. The role of inflexible minorities in the breaking of democratic opinion dynamics. Physica A, 381:366­376, 2007.
[27] Andr´e C. R. Martins and Serge Galam. The building up of individual inflexibility in opinion dynamics. Phys. Rev. E, 87:042807, 2013. arXiv:1208.3290.
10

[28] Andr´e C. R. Martins. Extremism definitions in opinion dynamics models, 2020. arXiv:2004.14548.
[29] Cristian Tileaga. Representing the 'other': A discurive analysis of prejudice and moral exclusion in talk about romanies. Journal of Community & Applied Social Psychology, 16:19­41, 2006.
[30] Joseph Bafumi and Michael C. Herron. Leapfrog representation and extremism: A study of american voters and their members in congress. American Political Science Review, 104(3):519­542, 2010.
[31] Pawel Sobkowicz. Whither now, opinion modelers? Frontiers in Physics, 8:461, 2020.
[32] Lucas B¨ottcher, Jan Nagler, and Hans J. Herrmann. Critical behaviors in contagion dynamics. Physical Review Letters, 118:088301, 2017.
[33] Serge Galam and Taksu Cheon. Tipping points in opinion dynamics: A universal formula in five dimensions. Frontiers in Physics, 8:446, 2020.
[34] Andr´e C. R. Martins. Discrete opinion models as a limit case of the coda model. Physica A, 395:352­357, 2014.
[35] Nika Haghtalab, Matthew O. Jackson, and Ariel D. Procaccia. Belief polarization in a complex world: A learning theory perspective. Proceedings of the National Academy of Sciences, 118(19), 2021.
[36] Anna Kowalska-Pyzalska, Katarzyna Maciejowska, Karol Suszczyn´ski, Katarzyna Sznajd-Weron, and Rafal Weron. Turning green: Agent-based modeling of the adoption of dynamic electricity tariffs. Energy Policy, 72:164­174, 2014.
[37] F. Mu¨ller-Hansen, M. Schlu¨ter, M. Ma¨s, J. F. Donges, J. J. Kolb, K. Thonicke, and J. Heitzig. Towards representing human behavior and decision making in earth system models ­ an overview of techniques and approaches. Earth System Dynamics, 8(4):977­1007, 2017.
[38] Andr´e C. R. Martins. Bayesian updating rules in continuous opinion dynamics models. Journal of Statistical Mechanics: Theory and Experiment, 2009(02):P02017, 2009. arXiv:0807.4972v1.
[39] Andr´e C. R. Martins, Carlos de B. Pereira, and R. Vicente. An opinion dynamics model for the diffusion of innovations. Physica A, 388:3225­ 3232, 2009.
[40] Andr´e C. R. Martins and Cleber D. Kuba. The importance of disagreeing: Contrarians and extremism in the coda model. Adv. Compl. Sys., 13:621­ 634, 2010.
11

[41] R. Vicente, Andr´e C. R. Martins, and N. Caticha. Opinion dynamics of learning agents: Does seeking consensus lead to disagreement? Journal of Statistical Mechanics: Theory and Experiment, 2009:P03015, 2009. arXiv:0811.2099.
[42] Xia-Meng Si, Yun Liua, Fei Xionga, Yan-Chao Zhang, Fei Ding, and Hui Cheng. Effects of selective attention on continuous opinions and discrete decisions. Physica A, 389(18):3711­3719, 2010.
[43] Xia-Meng Si, Yun, Hui Cheng, and Yan-Chao Zhang. An opinion dynamics model for online mass incident. In 2010 3rd International Conference on Advanced Computer Theory and Engineering(ICACTE), volume 5, pages V5­96­V5­99, 2010.
[44] Andr´e C. R. Martins. A middle option for choices in the continuous opinions and discrete actions model. Advances and Applications in Statistical Sciences, 2:333­346, 2010.
[45] Andr´e C. R. Martins. Modeling scientific agents for a better science. Adv. Compl. Sys., 13:519­533, 2010.
[46] Lei Deng, Yun Liu, and Fei Xiong. An opinion diffusion model with clustered early adopters. Physica A: Statistical Mechanics and its Applications, 392(17):3546 ­ 3554, 2013.
[47] Andr´e C. R. Martins. Trust in the coda model: Opinion dynamics and the reliability of other agents. Physics Letters A, 377(37):2333­2339, 2013. arXiv:1304.3518.
[48] Su-Meng Diao, Yun Liu, Qing-An Zeng, Gui-Xun Luo, and Fei Xiong. A novel opinion dynamics model based on expanded observation ranges and individuals' social influences in social networks. Physica A: Statistical Mechanics and its Applications, 415:220­228, 2014.
[49] Gui-Xun Luo, Yun Liu, Qing-An Zeng, Su-Meng Diao, and Fei Xiong. A dynamic evolution model of human opinion as affected by advertising. Physica A, 414:254­262, 2014.
[50] Nestor Caticha, Jonatas Cesar, and Renato Vicente. For whom will the bayesian agents vote? Frontiers in Physics, 3:25, 2015.
[51] Andr´e C. R. Martins. Opinion particles: Classical physics and opinion dynamics. Physics Letters A, 379(3):89­94, 2015. arXiv:1307.3304.
[52] Xi Lu, Hongming Mo, and Yong Deng. An evidential opinion dynamics model based on heterogeneous social influential power. Chaos, Solitons & Fractals, 73:98 ­ 107, 2015.
[53] Andr´e C. R. Martins. Thou shalt not take sides: Cognition, logic and the need for changing how we believe. Frontiers in Physics, 4(7), 2016.
12

[54] N. R. Chowdhury, I.-C. Morarescu, S. Martin, and S. Srikant. Continuous opinions and discrete actions in social networks: A multi-agent system approach. In 2016 IEEE 55th Conference on Decision and Control (CDC), pages 1739­1744, Dec 2016.
[55] Zhichao Cheng, Yang Xiong, and Yiwen Xu. An opinion diffusion model with decision-making groups: The influence of the opinion's acceptability. Physica A: Statistical Mechanics and its Applications, 461:429­438, 2016.
[56] Chuanchao Huang, Bin Hu, Guoyin Jiang, and Ruixian Yang. Modeling of agent-based complex network under cyber-violence. Physica A: Statistical Mechanics and its Applications, 458:399­411, 2016.
[57] Leandro M. T. Garcia, Ana V. Diez Roux, Andr´e C. R. Martins, Yong Yang, and Alex A. Florindo. Development of a dynamic framework to explain population patterns of leisure-time physical activity through agentbased modeling. International Journal of Behavioral Nutrition and Physical Activity, 14:111, 2017.
[58] Ruoyan Sun and David Mendez. An application of the continuous opinions and discrete actions (coda) model to adolescent smoking initiation. PLOS ONE, 12(10):1­11, 10 2017.
[59] Pawel Sobkowicz. Opinion dynamics model based on cognitive biases of complex agents. Journal of Artificial Societies and Social Simulation, 21(4):8, 2018.
[60] Hyun Keun Lee and Yong Woon Kim. Public opinion by a poll process: model study and bayesian view. Journal of Statistical Mechanics: Theory and Experiment, page 053402, 2018.
[61] Leandro M. T. Garcia, Ana V. Diez Roux, Andr´e C. R. Martins, Yong Yang, and Alex A. Florindo. Exploring the emergence and evolution of population patterns of leisure-time physical activity through agent-based modelling. International Journal of Behavioral Nutrition and Physical Activity, 15(1):112, Nov 2018.
[62] Tanzhe Tang and Caspar G. Chorus. Learning opinions by observing actions: Simulation of opinion dynamics using an action-opinion inference model. Journal of Artificial Societies and Social Simulation, 22(3):2, 2019.
[63] Andr´e C. R. Martins. Network generation and evolution based on spatial and opinion dynamics components. International Journal of Modern Physics C, 30(9):1950077, 2019.
[64] Andr´e C. R. Martins. Discrete opinion dynamics with m choices. The European Physical Journal B, 93(1):1, 2020. arXiv:1905.10878.
13

[65] F.J. Le´on-Medina, J. Tena-S´anchez, and F.J. Miguel. Fakers becoming believers: how opinion dynamics are shaped by preference falsification, impression management and coherence heuristics. Quality and Quantity, 54:385­412, 2020.
[66] Marcelo V. Maciel and Andr´e C. R. Martins. Ideologically motivated biases in a multiple issues opinion model. Physica A, page 124293, 2020. https://arxiv.org/abs/1908.10450.
[67] Aili Fang, Kehua Yuan, Jinhua Geng, , and Xinjiang Wei. Opinion dynamics with bayesian learning. Complexity, page 8261392, 2020.
[68] Zhanli Sun and Daniel Mu¨ller. A framework for modeling payments for ecosystem services with agent-based models, bayesian belief networks and opinion dynamics models. Environmental Modelling and Software, 45:15­28, 2013. Thematic Issue on Spatial Agent-Based Models for SocioEcological Systems.
[69] Andr´e Orl´ean. Bayesian interactions and collective dynamics of opinion: Herd behavior and mimetic contagion. Journal of Economic Behavior and Organization, 28:257­274, 1995.
[70] Matthew Rabin and Joel L. Schrag. First Impressions Matter: A Model of Confirmatory Bias*. The Quarterly Journal of Economics, 114(1):37­82, 02 1999.
[71] James Andreoni and Tymofiy Mylovanov. Diverging opinions. American Economic Journal: Microeconomics, 4(1):209­32, February 2012.
[72] Ryosuke Nishi and Naoki Masuda. Collective opinion formation model under bayesian updating and confirmation bias. Phys. Rev. E, 87:062123, Jun 2013.
[73] V´ictor M. Egu´iluz, Naoki Masuda, and Juan Ferna´ndez-Gracia. Bayesian decision making in human collectives with binary choices. PLOS ONE, 10(4):e0121332, 2015.
[74] Yunlong Wang, Lingqing Gan, and Petar M. Djuri´c. Opinion dynamics in multi-agent systems with binary decision exchanges. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4588­4592, 2016.
[75] David C. Knill and Alexandre Pouget. The bayesian brain: the role of uncertainty in neural coding and computation. Trends in Neurosciences, 27(12):712­719, December 2004.
[76] Andr´e C. R. Martins. Probabilistic biases as bayesian inference. Judgment And Decision Making, 1(2):108­117, 2006.
14

[77] J. B. Tenenbaum, C. Kemp, and P. Shafto. Theory-based bayesian models of inductive reasoning. In A. Feeney and E. Heit, editors, Inductive reasoning. Cambridge University Press., 2007.
[78] Joshua B. Tenenbaum, Charles Kemp, Thomas L. Griffiths, and Noah D. Goodman. How to grow a mind: Statistics, structure, and abstraction. Science, 331(6022):1279­1285, 2011.
[79] Andr´e C. R. Martins. Arguments, Cognition, and Science: Need and consequences of probabilistic induction in science. Rowman & Littlefield, 2020.
[80] Andr´e C. R. Martins. Embracing undecidability: Cognitive needs and theory evaluation, 2020. arXiv:2006.02020.
[81] H. A. Simon. Rational choice and the structure of environments. Psych. Rev., 63:129­138, 1956.
[82] R. Selten. What is bounded rationality? In G. Gigerenzer and R. Selten, editors, Bounded rationality: The adaptive toolbox, Dahlem Workshop Report, pages 147­171. Cambridge, Mass, MIT Press, 2001.
[83] R. T. Cox. The Algebra of Probable Inference. John Hopkins University Press, 1961.
[84] E.T. Jaynes. Probability Theory: The Logic of Science. Cambridge, Cambridge University Press, 2003.
[85] Ariel Caticha and Adom Giffin. Updating probabilities. In A. MohammadDjafari, editor, Bayesian Inference and Maximum Entropy Methods in Science and Engineering, volume 872 of AIP Conf. Proc., page 31, 2007.
[86] Frederick Eberhardt and David Danks. Confirmation in the cognitive sciences: The problematic case of bayesian models. Minds and Machines, 21(3):389­410, 2011.
[87] Shira Elqayam and Jonathan St. B. T. Evans. Rationality in the new paradigm: Strict versus soft bayesian approaches. Thinking & Reasoning, 19(3-4):453­470, 2013.
[88] Nick Chater, Teppo Felin, David C. Funder, Gerd Gigerenzer, Jan J. Koenderink, Joachim I. Krueger, Denis Noble, Samuel A. Nordli, Mike Oaksford, Barry Schwartz, Keith E. Stanovich, and Peter M. Todd. Mind, rationality, and cognition: An interdisciplinary debate. Psychonomic Bulletin and Review, 25(2):793­826, 2018.
[89] P.C. Watson and P. Johnson-Laird. Psychology of Reasoning: Structure and Content. Harvard University Press, 1972.
15

[90] A. Tversky and D. Kahneman. Extension versus intuituive reasoning: The conjuction fallacy in probability judgement. Psych. Rev., 90:293­ 315, 1983.
[91] Stuart Oskamp. Overconfidence in case-study judgments. Journal of Consulting Psychology, 29(3):261­265, 1965.
[92] P. N. Johnson-Laird, Paolo Legrenzi, and Maria Sonino Legrenzi. Reasoning and a sense of reality. British Journal of Psychology, 6(3):395­400, 1972.
[93] Gerd Gigerenzer, Peter M. Todd, and The ABC Research Group. Simple Heuristics That Make Us Smart. Oxford University Press, 2000.
[94] Amos Tversky and Daniel Kahneman. Availability: A heuristic for judging frequency and probability. Cognitive Psychology, 5(2):207­232, 1973.
[95] G. Gigerenzer and D. G. Goldstein. Reasoning the fast and frugal way: Models of bounded rationality. Psych. Rev., 103:650­669, 1996.
[96] Dan M. Kahan. Ideology, motivated reasoning, and cognitive reflection. Judgment and Decision Making, 8:407­424, 2013.
[97] Dan M. Kahan. The expressive rationality of inaccurate perceptions. Behavioral and Brain Sciences, 40:e6, 2017.
[98] Hugo Mercier and Dan Sperber. Why do humans reason? arguments for an argumentative theory. Behavioral and Brain Sciences, 34:57­111, 2011.
[99] Hugo Mercier and Dan Sperber. The Enigma of Reason. Harvard University Press, 2017.
[100] D. Kahneman and A. Tversky. Prospect theory: An analysis of decision under risk. Econometrica, 47:263­291, 1979.
[101] W. Edwards. Conservatism in human information processing. In B. Kleinmuntz, editor, Formal Representation of Human Judgment. John Wiley and Sons, 1968.
[102] S. Galam. Contrarian deterministic effect: the hung elections scenario. Physica A, 333:453­460, 2004.
[103] Robert Jervis. Perception and Misperception in International Politics. Princeton University Press, 1976.
[104] P. Clifford and A. Sudbury. A model for spatial conflict. Biometrika, 60:581­588, 1973.
[105] R. Holley and T. M. Liggett. Ergodic theorems for weakly interacting systems and the voter model. Ann. Probab., 3:643­663, 1975.
16

[106] S. Galam. Modelling rumors: the no plane pentagon french hoax case. Physica A, 320:571­580, 2003.
[107] S. Galam. Opinion dynamics, minority spreading and heterogeneous beliefs. In B. K. Chakrabarti, A. Chakraborti, and A. Chatterjee, editors, Econophysics & Sociophysics: Trends & Perspectives., pages 363­387. Wiley, 2006.
[108] Raymond S. Nickerson. Confirmation bias: A ubiquitous phenomenon in many guises. Review of General Psychology, 2(2):175­220, 1998.
17

