arXiv:2106.01004v1 [math.ST] 2 Jun 2021

Semiparametric tail-index estimation for randomly right-truncated heavy-tailed data
Saida Mancer, Abdelhakim Necir, Souad Benchaira
Laboratory of Applied Mathematics, Mohamed Khider University, Biskra, Algeria
Abstract It was shown that when one disposes of a parametric information of the truncation distribution, the semiparametric estimator of the distribution function for truncated data (Wang, 1989) is more efficient than the nonparametric one. On the basis of this estimation method, we derive an estimator for the tail index of Paretotype distributions that are randomly right-truncated and establish its consistency and asymptotic normality. The finite sample behavior of the proposed estimator is carried out by simulation study. We point out that, in terms of both bias and root of the mean squared error, our estimator performs better than those based on nonparametric estimation methods. An application to a real dataset of induction times of AIDS diseases is given as well.
Keywords: Extreme value index; Product-limit estimator; Semiparametric; TailEmpirical process; Truncated data.
*Corresponding author: necirabdelhakim@yahoo.fr E-mail address: mancer.saida731@gmail.com (S. Mancer) benchaira.s@hotmail.fr (S. Benchaira)
1

2

1. Introduction

Let (Xi, Yi) , i = 1, ..., N  1 be a sample from a couple (X, Y) of independent positive random variables (rv's) defined over a probability space (, A, P) , with

continuous distribution functions (df's) F and G respectively. Suppose that X is

right-truncated by Y, in the sense that Xi is only observed when Xi  Yi. Thus, let us denote (Xi, Yi) , i = 1, ..., n to be the observed data, as copies of a couple of dependent rv's (X, Y ) corresponding to the truncated sample (Xi, Yi) , i = 1, ..., N, where n = nN is a random sequence of discrete rv's. By the weak law of large numbers, we have


n/N P p := P (X  Y) = F (w) dG (w) , as N  ,
0

(1.1)

where the notation P stands for the convergence in probability. The constant p

corresponds to the probability of observed sample which is supposed to be non-

null, otherwise nothing is observed. The truncation phenomena frequently occurs

in medical studies, when one wants to study the length of survival after the start

of the disease: if Y denotes the elapsed time between the onset of the disease and

death, and if the follow-up period starts X units of time after the onset of the

disease then, clearly, X is right-truncated by Y. For concrete examples of truncated

data in medical treatments one refers, among others, to Lagakos et al. (1988) and

Wang (1989). Truncated data schemes may also occur in many other fields, namely

actuarial sciences, astronomy, demography and epidemiology, see for instance the

textbook of Lawless (2002).

From Gardes and Stupfler (2015) the marginal df's F  and G corresponding to the

joint df of (X, Y ) are given by

x

x

F  (x) := p-1 G (w) dF (w) and G (x) := p-1 F (w) dG (w) .

0

0

By the previous first equation we derive a representation of the underlying df F as

follows:

F (x) = p x dF  (w) , 0 G (w)

(1.2)

which will be for a great interest thereafter. In the sequel, we are dealing with the

concept of regular variation. A function  is said to be regularly varying at infinity

with negative index -1/, notation   RV (-1/) , if

 (st) / (t)  s-1/, as t  ,

(1.3)

3
for s > 0. This convergence is known as the first-order condition of regular variation and its corresponding uniform convergence is formulated in terms of "Potter's inequalities" as follows: for any small  > 0, there exists t0 > 0 such that for any t  t0 and s  1, we have

(1 - ) s-1/- <  (st) / (t) < (1 + ) s-1/+.

(1.4)

See for instance Proposition B.1.9 (assertion 5, page 367) in de Haan and Ferreira (2006). The second-order condition (see de Haan and Stadtmu¨ller, 1996) expresses the rate of the convergence (1.3) above. For any x > 0, we have

 (tx) / (t) - x-1/ A (t)



x-1/

x

/ - 

1,

as

t  ,

(1.5)

where  < 0 denotes the second-order parameter and A is a function tending to zero and not changing signs near infinity with regularly varying absolute value with positive index  /. A function  that satisfies assumption (1.5) is denoted   RV2 (-1/; , A) . We now have enough material to tackle the main goal of the paper. To begin, let us assume that the tails of both df's F and G are regularly varying. That is

F  RV (-1/1) and G  RV (-1/2) , with 1, 2 > 0.

(1.6)

Under this assumption, Gardes and Stupfler (2015) showed that

F   RV (-1/1) and G  RV (-1/2) ,

(1.7)

where



:=

12 1 + 2

.

(1.8)

For details on the proof of this statement, on refers to Benchaira et al. (2016a)

(Lemma A1). The estimation of the tail index 1 was recently addressed for the first time in Gardes and Stupfler (2015) where the authors used equation (1.8) to

propose an estimator to 1 as a ratio of Hill estimators (Hill, 1975) of the tail indices  and 2. These estimators are based on the top order statistics Xn-k:n  ...  Xn:n and Yn-k:n  ...  Yn:n pertaining to the samples (X1, ..., Xn) and (Y1, ..., Yn) respectively. The sample fraction k = kn being a sequence of integers such that, kn   and kn/n  0 as n  . The asymptotic normality of the given estimator is established in Benchaira et al. (2015) by considering both the tail dependence and

4

the second-order conditions of regular variation. By using a Lynden-bell integral,

Worms and Worms (2016) proposed the following estimator for the tail index 1 :

1(W) (u)

:=

1 Fn(1) (u)

n i=1

1 (Xi

>

u)

F(n1) (Xi) Cn (Xi)

log

Xi u

,

where u > 0 is a given deterministic threshold and

F(n1) (x) :=
Xi>x

1

-

1 nCn (Xi)

,

and

Cn

(x)

:=

1 n

n

1 (Xi  x  Yi) ,

i=1

is the well-known nonparametric maximum likelihood estimator introduced in the

well-known work Lynden-Bell (1971). Independently, Benchaira et al. (2016a) used

a Woodroofe-integral with a random threshold, to derive the following estimator

1(BMN)

:=

1 F(n2) (Xn-k:n)

k i=1

F(n2) (Xn-i+1:n) Cn (Xn-i+1:n)

log

Xn-i+1:n Xn-k:n

,

(1.9)

where

F(n2) (x) :=

exp

Xi>x

-1 nCn (Xi)

,

is the so-called Woodroofe's nonparametric estimator (Woodroofe, 1985) of df F.

To improve the performance of 1(BML), Benchaira et al. (2016b) and Haouas et al.

(2019) respectively proposed a Kernel-smoothed and a reduced-biais versions of this

estimator and establish their consistency and asymptotic normality. It is worth mentioning that the Lynden-Bell integral estimator 1(W) with a random threshold u = Xn-k:n becomes

1(W)

:=

1 Fn(1) (Xn-k:n)

k i=1

F(n1) (Xn-i+1:n) log Xn-i+1:n .

Cn (Xn-i+1:n)

Xn-k:n

(1.10)

In a simulation study, Haouas et al. (2018) compared this estimator with 1(BMN). They pointed out that both estimators have similar behaviors in terms of biases a

nd mean squared errors.

Recall that the nonparametric Lynden-Bell estimator F(n1) was constructed on the basis of the fact that F and G are both unknown. In this paper, we are dealing with the situation when F is unknown but G is parametrized by a known model G,     Rd, d  1 having a density g with respect to Lebesgue measure. Wang

5

(1989) considered this assumption and introduced a semiparametric estimator for

df F defined by

Fn

x; n

:= Pn



1 n

n i=1

1 (Xi  x) , G (Xi)

(1.11)

where 1/Pn  := n-1

n i=1

1/G

(Xi)

and

n
 := arg max g (Yi) /G (Xi) ,  i=1

(1.12)

denoting the conditional maximum likelihood estimator (CMLE) of , which is consistent and asymptotically normal, see for instance Andersen (1970). On the other hand, Wang (1989) showed that Fn x; n is a uniformly consistent estimator over the x-axis and established, under suitable regularity assumptions, its asymptotic normality. Both Wang (1989) and Moreira and de Un~a-A´ lvarez (2010) pointed out that the semiparametric estimate has greater efficiency uniformly over the x-axis. In the light of a simulation study, the authors suggest that the semiparametric estimate is a better choice when parametric information of the truncation distribution is available. Since the apparition of this estimation method many papers are devoted to the statistical inference with truncation data, see for instance Bilker and Wang (1996), Li et al. (1997), Qin et al. (2001), Shen (2010), Moreira et al. (2014), and Shen and Hsu (2020).

Motivated by the features of the semiparametric estimation, we next propose a new

an estimator for 1 by means of a suitable functional of Fn x; n . We start our construction by noting that from Theorem 1.2.2 in de de Haan and Ferreira (2006),

the first-order condition (1.6) (for F) implies that

lim 1 t F (t)


log (x/t) dF (x) = 1.
t

(1.13)

In other words, 1 may viewed as a functional t (F) , for a large t, where

t

(F)

:=

1 F (t)


log (x/t) dF (x) .
t

Replacing F by Fn ·; n and letting t = Xn-k:n yield

1 = Xn-k:n Fn ·; n

=

1

Fn Xn-k:n; n


log (x/Xn-k:n) dFn x; n ,
Xn-k:n

(1.14) (1.15)

6

as new estimator for 1. Observe that



log (x/t) dFn x; n

t



= Pn 

log (x/Xn-k:n) 1 (x  Xn-k) dFn x; n ,

Xn-k:n

which may be rewritten into

Pn  n
= Pn

1n
i=1

 Xn-k:n

log (x/Xn-k:n) 1 (x G (Xi)



Xn-k) d1 (Xi



x)



1 n

k i=1

log (Xn-i+1/Xn-k:n) . G (Xn-i+1:n)

On the other hand, F Xn-k:n; n equals

Pn



1 n

n i=1

1 (Xi:n  Xn-k:n) G (Xi:n)

= Pn



1 n

n-k

1/G

(Xi:n)

.

i=1

Hence

1 n

n

1/G

(Xi:n)

-

1 n

n-k
1/G

(Xi:n)

F Xn-k:n; n = i=1

i=1

1 n

n

1/G (Xi:n)

i=1

= Pn



1 n

k

1/G (Xn-i+1:n) .

i=1

Thereby, the final form of our new estimator is

1 =

k i=1

G

(Xn-i+1:n) -1 log (Xn-i+1/Xn-k:n)

k i=1

G (Xn-i+1:n) -1

.

(1.16)

The asymptotic behavior of 1 will be established by means of the following tail

empirical process Dn x; ; 1



:=

 k

 Fn

xXn-k:n; 

Fn Xn-k:n; 

 - x-1/1  , for x > 1.

This method was already used to establish the asymptotic behavior of Hill's estimator for complete data (de Haan and Ferreira, 2006, page 162) that we will adapt to the truncation case. Indeed, an integration by parts of the integral (1.14) , yields

1 =  x-1 Fn xXn-k:n;  dx,

1

Fn Xn-k:n; 

7

and therefore





k (1 - 1) =

x-1Dn x; ; 1 dx.

1

(1.17)

Thus for a suitable weighted weak approximation to Dn ·;  , we may easily deduce the consistency and asymptotic normality of 1. This process may also contribute to the goodness-of-fit test to fitting heavy-tailed distributions via, among others, the

Kolmogorov-Smirnov and Cramer-Von Mises type statistics

sup Dn x; , 1
x>1



and

D2n x; , 1 dx-1/1 .

1

More precisely, these statistics are used when testing the null hypothesis H0 :"both F and G are heavy-tailed" versus the alternative one H1 : "at least one of F and G is not heavy-tailed", that is H0 :"(1.6) holds" versus H1 : "(1.6) does not hold". This problem has been already addressed by Drees et al. (2006) and Koning and Peng

(2008) in the case of complete data. The (uniform) weighted weak convergence

of Dn x; , 1 and the asymptotic normality of 1, stated below, will be of great interest to establish the limit distributions of the aforementioned test statistics. This

is out of the scope of this paper whose remainder is structured as follows. In Section

2, we present our main results which consist in the consistency and asymptotic

normality of estimator 1. The performance of the proposed estimator is checked by simulation in Section 3. An application to a real dataset composed of induction

times of AIDS diseases is given in Section 4. All proofs are gathered in Section 5.

The proofs of two useful lemmas are postponed to the Appendix.

2. Main results
The regularity assumptions, denoted [A0] , concerning the existence, consistency and asymptotic normality of the CLME estimator , given in (1.12) , are discussed in Andersen (1970). Here we only state additional conditions on df G corresponding to Pareto-type models which are required to establish the asymptotic behavior of our newly estimator 1.
· [A1] For each fixed y, the function   G (y) is continuously differentiable of partial derivatives G(j) =: G/j, j = 1, ..., d.
· [A2] G(j)  RV (-1/2) . · [A3] y-G(j) (y) /G (y)  0, as y  , for any  > 0.
For commonly Pareto-type models, one may easily checked that there exist some constants aj  0, cj and dj, such that G(j) (y)  cj y-1/2 + dj log y, for all large

8
x. Then one may consider that the assumptions [A1] - [A3] are not very restrictive and they may be acceptable in the extreme value theory.

Theorem 2.1. Assume that F  RV2 (-1/1; 1, A) and G  RV (-1/2) satisfying the assumptions [A0] - [A3] , and suppose that 1 < 2. Then on the probability space (, A, P) , there exists a standard Wiener process {W (s) , 0  s  1} such that, for any small 0 <  < 1/2 :

sup x Dn
x>1

x; , 1

-



(x;

W

)

-

x-1/1

x1/1 - 11

1

 kA

(ak)

P 0,

 provided that kA (ak) = O (1) , where

 (x; W )

:=

 x-1/1 1

x1/ W

x-1/

- W (1)

+

1

 +

2 x-1/1

1
s-/2-1
0

x1/ W

x-1/ s

- W (s)

ds,

is a centred Gaussian process and ak := F  (1 - k/n) , where

F  (s) := inf {x : F  (x)  s} , 0 < s < 1,

denotes the quantile (or the generalized inverse) function pertaining to df F .

By a strength application of this weak approximation, we establish both consistency and asymptotic normality of our newly estimator 1, that we state there in the following Theorem.

Theorem 2.2. Under the assumptions of Theorem 2.1, we have

1 - 1

= k-1/2


x-1 (x; W ) dx + A (ak)
1

1



x-1/1 -1

x1/1 - 11

1

dx

+

oP

k-1/2

,

this

implies

that

1

P

1.

Whenever

 kA (ak)





<

,

we

get

 k

(1

-

1)

D

N

1

 -

1

,

2

,

where 2 := 2 (1 + 1/2) 1 + (1/2)2 (1 - 1/2)3 1 (1 < 2) , and 1 (A) stands for the indicator function pertaining to a set A.

9
3. Simulation study

In this section we will perform a simulation study in order to compare the finite sample behavior of our the newly semiparametric estimator 1, given in (1.16) , with the Woodrofee and the Lynden-Bell integral estimators 1(BMN) and 1(W), given respectively in (1.9) and (1.10) . The truncation and truncated distributions functions F and G, will be chosen among the following two models:
· Burr (, ) distribution with right-tail function:
H (x) = 1 + x1/ -/ , x  0,  > 0,  > 0;

· Fr´echet () distribution with right-tail function:

H (x) = 1 - exp -x-1/ , x > 0,  > 0.

The simulation study be made in fours scenarios following to the choice of the underlying df's F and G:

· [S1] Burr (1, ) truncated by Burr (2, ) ; with  = (2, ) · [S2] Fr´echet (1) truncated by Fr´echet (2) ; with  = 2 · [S3] Fr´echet (1) truncated by Burr (2, ) ; with  = (2, ) · [S4] Burr (1, ) truncated by Fr´echet (2) ; with  = 2

To this end, we fix  = 1/4 and choose the values 0.6 and 0.8 for 1 and 55% and 90% for the portions of observed truncated data given in (1.1) by


p = F (w) dG (w) ,
0

(3.18)

so that the assumption 1 < 2 stated in Theorem 2.1 be hold. In other terms

the values of p have to be greater than 50%. For each couple (1, p) , we solve the

equation (3.18) to get the pertaining 2-value, which we summarize as follows:

(p, 1, 2) = (55%, 0.6, 1.4) , (90%, 0.6, 5.4) , (55%, 0.8, 1.9) , (90%, 0.8, 7.2) . (3.19)

For each scenario, we simulated 1000 random samples of size N = 300 and compute the root mean squared error (RMSE) and the absolute bias (ABIAS) corresponding to each estimator 1, 1(BMN) and 1(W). The comparison is carry out by plotting the ABIAS and RMSE as functions of the sample fraction k which is vary from 2 to 120. The end points of this range is chosen so that it contains the optimal number of upper extremes k used in the computation of the tail index estimate. There are many heuristic methods to select the optimal choice of k, see for instance Caeiro and Gomes (2015), here we use the algorithm proposed by Reiss and Thomas

10

(2007) in page 137, which is incorporated in the R software "Xtremes"package. Note that the computation the CLME of  is made by means of the syntax "maxLik" of the maxLik R software package. The optimal sample fraction k is defined, in this procedure, by

k

:=

arg

min
1<k<n

1 k

k

i | (i) - median { (1) , ...,  (k)}| ,

i=1

for suitable constant 0    1/2, where  (i) corresponds to an estimator of tail

index , based on the i upper order statistics, of a Pareto-type model. We observed,

in our simulation study, that  = 0.3 allows better results both in terms of bias and

rmse. It is worth mentioning that making N vary did not provide notable findings,

therefore we kept the size N be fixed. The finite sample behavior of the above

mentioned estimators are illustrated in Figures 3.1-3.8. On the overall, the biases

of three estimators are almost equal, however in the case of moderate truncation

(p  50%) the RMSE of our newly semiparametric 1 is clearly the smaller compared that of 1(BMN) and 1(W). Actually, the moderate truncation situation is the most frequently in real data, while up to our knowledge the strong truncation remains

theoretic. In this sense, we may consider that the semiparametric estimator is more efficient than the two other ones. We point out that the two estimators 1(BMN) and 1(W) have almost the same behavior which actually is noticed before by Haouas et al. (2018). The optimal sample fractions k of each tail index estimator are given in

Tables 1-4.

k 1

k 1(BMN) k 1(W)

S1 44 0.600 41 0.599 40 0.600

S2 18 0.601 17 0.600 16 0.597

S3 21 0.601 20 0.601 19 0.599

S4 30 0.603 27 0.600 25 0.598

Table 1. Optimal sample fraction k and the estimated value of each

estimator of the tail index 1 = 0.6 based on 1000 samples for the four scenarios with p = 0.55 .

4. Real data example
In this section, we give an application to the AIDS data set, available in the "DTDA" R package, used before by Lagakos et al. (1988). The data present the infection and

11

0.35

RMSE

ABIAS 0.00 0.10 0.20

0.25

0 20 40 60 80

120

k

0 20 40 60 80

120

k

RMSE 0.10 0.25 0.40

ABIAS 0.00 0.02 0.04

0 20 40 60 80

120

k

0 20 40 60 80

120

k

Figure 3.1. Absolute bias (left two panels) and RMSE (right two panels) of 1 (black) and 1(BMN) (red) and 1(W)(blue), corresponding to two situations of scenario S1 : (1 = 0.6, p = 55%) and (1 = 0.6, p = 90%) based on 1000 samples of size 300.

k 1

k 1(BMN) k 1(W)

S1 82 0.610 82 0.611 82 0.611

S2 37 0.640 37 0.640 37 0.640

S3 46 0.633 37 0.625 37 0.625

S4 52 0.610 52 0.610 52 0.610

Table 2. Optimal sample fraction k and the estimated value of each

estimator of the tail index 1 = 0.6 based on 1000 samples for the four scenarios with p = 0.9.

induction times for n = 258 adults who were infected with HIV virus and developed AIDS by June 30, 1986. The time in years, measured from April 1, 1978, when adults were infected by the virus from a contaminated blood transfusion and the waiting time to development of AIDS measured from the date of infection. We interest here to the estimation of the end-time of the induction of the AIDS virus which corresponds to the high quantile in basis the given observations. The variable of interest here is the time of induction T of the disease duration which elapses between

12

RMSE 0.30 0.40 0.50

ABIAS 0.00 0.10 0.20

0 20 40 60 80

120

k

0 20 40 60 80

120

k

RMSE 0.1 0.3 0.5

0.04

ABIAS

0.00

0 20 40 60 80

120

k

0 20 40 60 80

120

k

Figure 3.2. Absolute bias (left two panels) and RMSE (right two panels) of 1 (black) and 1(BMN) (red) and 1(W)(blue), corresponding to two situations of scenario S1 : (1 = 0.8, p = 55%) and (1 = 0.8, p = 90%) based on 1000 samples of size 300.

k 1

k 1(BMN) k 1(W)

S1 59 0.799 57 0.800 54 0.799

S2 21 0.803 21 0.803 20 0.799

S3 24 0.802 22 0.798 22 0.801

S4 51 0.799 52 0.800 50 0.801

Table 3. Optimal sample fraction k and the estimated value of each

estimator of the tail index 1 = 0.8 based on 1000 samples for the four scenarios with p = 0.55 .

the date of infection M and the date M + T of the declaration of the disease. The sample (T1, M1), ..., (Tn, Mn) are taken between two fixed dates: "0" and "8", i.e. between April 1, 1978, and June 30, 1986. The initial date "0" denotes an infection occurring in the three months: from April 1, 1978, to June 30, 1978. Let us assume that M and T are the observed rv's, corresponding to the underlying rv's M and T, given by the truncation scheme 0  M + T  8, which in turn may be rewritten

13

0.35

RMSE

ABIAS 0.00 0.15 0.30

0.25

0 20 40 60 80

120

k

0 20 40 60 80

120

k

RMSE 0.10 0.20 0.30 0.40

0.10

ABIAS

0.00

0 20 40 60 80

120

k

0 20 40 60 80

120

k

Figure 3.3. Absolute bias (left two panels) and RMSE (right two panels) of 1 (black) and 1(BMN) (red) and 1(W)(blue), corresponding to two situations of scenario S2 : (1 = 0.6, p = 55%) and (1 = 0.6, p = 90%) based on 1000 samples of size 300.

k 1

k 1(BMN) k 1(W)

S1 90 0.804 90 0.806 90 0.807

S2 34 0.845 34 0.846 34 0.846

S3 40 0.831 40 0.831 40 0.831

S4 71 0.814 71 0.814 71 0.815

Table 4. Optimal sample fraction k and the estimated value of each

estimator of the tail index 1 = 0.8 based on 1000 samples for the four scenarios with p = 0.9.

into

0  M  S,

(4.20)

where S := 8 - T. To work within the framework of the present paper, let us make the following transformations:

X

:=

1 S+

and Y

:=

M

1 +



,

(4.21)

14

RMSE 0.35 0.40 0.45 0.50

ABIAS 0.0 0.1 0.2 0.3 0.4

0 20 40 60 80 100 120 k

0 20 40 60 80 100 120 k

RMSE 0.2 0.3 0.4 0.5

0.20

ABIAS

0.10

0.00

0 20 40 60 80 100 120 k

0 20 40 60 80 100 120 k

Figure 3.4. Absolute bias (left two panels) and RMSE (right two panels) of 1 (black) and 1(BMN) (red) and 1(W)(blue), corresponding to two situations of scenario S2 : (1 = 0.8, p = 55%) and (1 = 0.8, p = 90%) based on 1000 samples of size 300.

where  = 0.05 so that the two denominators be non-null. Thus, in view of (4.20), we have X  Y, which means that X is randomly right-truncated by Y. Thereby, for the given sample (T1, M1), ..., (Tn, Mn), from (T, M) , the previous transformations produce us a new ones (X1, Y1), ..., (Xn, Yn) from (X, Y ) .

Let us now denote by F and G the df's of the underling rv's X and Y corresponding to the truncated rv's X and Y, respectively. By using parametric likelihood methods, Lui et al. (1986) fit both df's of M and S by the two-parameter Weibull model, this implies that the df's of F and G by may be fitted by two-parameter Fr´echet model, namely H(a.r) (x) = exp (-arx-r) , x > 0, a > 0, r > 0, hence both F and G are heavy-tailed. The estimated parameters corresponding to the fitting of df G are a0 = 0.004 and r0 = 2.1, see also Lagakos et al. (1988) page 520. Thus on may consider that df G is known and equals G = H(a0,r0), where  = (a0, r0) . By using the Thomas and Reiss algorithm, given above, we compute the optimal sample fraction k corresponds to the tail index estimator 1 of df F is 1. We find

k = 19, Xn-k:n = 0.356 and 1 = 0.917.

(4.22)

15

0.20

RMSE 0.25 0.30 0.35

0.10

ABIAS

0.00

0 20 40 60 80 100 k

0 20 40 60 80 100 k

RMSE 0.10 0.25 0.40

ABIAS 0.00 0.04 0.08

0 20 40 60 80 100 k

0 20 40 60 80 100 k

Figure 3.5. Absolute bias (left two panels) and RMSE (right two panels) of 1 (black) and 1(MBN) (red) and 1(W)(blue), corresponding to two situations of scenario S3 : (1 = 0.6, p = 55%) and (1 = 0.6, p = 90%) based on 1000 samples of size 300.

The well-known Weissman estimator (Weissman, 1978) of the high quantile, qv := F-1 (1 - vn) , corresponding to the underling df F is given by

qv := Xn-k:n

v

-1
,

Fn (Xn-k:n)

where v = 1/ (2n) , and Fn is the semiparametric estimator of df F of X given in (1.11) . From the values (4.22) , we get qv = 0.061. Let us now compute the high quantile of T based on the original data, T1, ..., Tn. Recall that P (X  qv) = v and X = 1/ (8 - T + ) , this implies that P (T  1/qv - 8 + ) = v, this means that 1/qv - 8 +  is the high quantile of T, which corresponds to the end-time tend that we want to estimate. Thereby tend = 1/qv - 8 + 10-2 = 1/0.061 - 8 + 10-2 = 8.40, the value the end time of induction of AIDS is: 8 years, 4 months and 24 days.

16

0.45

RMSE

ABIAS 0.00 0.10 0.20

0.35

0 20 40 60 80 100 k

0 20 40 60 80 100 k

0.4

RMSE

ABIAS 0.00 0.04 0.08

0.2

0 20 40 60 80 100 k

0 20 40 60 80 100 k

Figure 3.6. Absolute bias (left two panels) and RMSE (right two panels) of 1 (black) and 1(BMN) (red) and 1(W)(blue), corresponding to two situations of scenario S3 : (1 = 0.8, p = 55%) and (1 = 0.8, p = 90%) based on 1000 samples of size 300.

5. Proof of Theorems

5.1. Proof of Theorem 2.1. Let us first notice that the semiparametric estimator of df F given in (1.12) may be rewritten into

Fn x; n = Pn 

x dFn (w) , 0 G (w)

(5.23)

and 1/Pn



=

 0

dFn

(w)

/G

(w)

,

where

Fn

(w)

:=

n-1

n i=1

1

(Xi



w)

denotes

the usual empirical df pertaining to the observed sample X1, ..., Xn. It is worth

mentioning that by using the strong law of large numbers Pn   P () (almost

surely) as n  , where P () = 1/

 0

dF  (w) /G (w)

(see

e.g.

Lemma 3.2 in

Wang (1989)). On the other hand by using the first equation in (1.2) , we deduce that

p = 1/

 0

dF



(w)

/G

(w

)

,

it

follows

that

p



P

()

because

we

already

assumed

that G  G. Next we use the distribution tail

F (x; ) = P ()  dF  (w), x G (w)

(5.24)

17

0.35

RMSE

ABIAS 0.00 0.10 0.20

0.25

0 20 40 60 80 100 k

0 20 40 60 80 100 k

RMSE 0.10 0.25 0.40

ABIAS 0.00 0.02 0.04 0.06

0 20 40 60 80 100 k

0 20 40 60 80 100 k

Figure 3.7. Absolute bias (left two panels) and RMSE (right two panels) of 1 (black) and 1(BMN) (red) and 1(W)(blue), corresponding to two situations of scenario S4 : (1 = 0.6, p = 55%) and (1 = 0.6, p = 90%) based on 1000 samples of size 300.

and its empirical counterpart Fn x;  = Pn 

 dFn (w) . x G (w)

To begin let us decompose k-1/2Dn x;  for x > 1 into the sum of

Fn xXn-k:n;  - Fn (xXn-k:n; )

Mn1 (x) := x-1/1

F (xXn-k:n; )

,

Mn2

(x)

:=

x-1/1

Fn

(xXn-k:n; ) - F (xXn-k:n; F (xXn-k:n; )

) ,

Mn3

(x)

:=

-

F (xXn-k:n; Fn (Xn-k:n;

) )

Fn

(Xn-k:n; ) - F (Xn-k:n; F (Xn-k:n; )

)

,

Mn4 (x) :=

F (xXn-k:n; ) - x-1/1 Fn (Xn-k:n; )

Fn (xXn-k:n; ) - F (xXn-k:n; ) F (xXn-k:n; )

and

Mn5 (x)

:=

F (xXn-k:n; ) F (Xn-k:n; )

- x-1/1 .

18

RMSE 0.30 0.40 0.50

ABIAS 0.00 0.10 0.20

0 20 40 60 80 100 k

0 20 40 60 80 100 k

RMSE 0.1 0.3 0.5

ABIAS 0.000 0.015 0.030

0 20 40 60 80 100 k

0 20 40 60 80 100 k

Figure 3.8. Absolute bias (left two panels) and RMSE (right two panels) of 1 (black) and 1(BMN) (red) and 1(W)(blue), corresponding to two situations of scenario S4 : (1 = 0.8, p = 55%) and (1 = 0.8, p = 90%) based on 1000 samples of size 300.

Our goal is to provide a weighted weak approximation to the tail empirical process Dn x; ; 1 . To begin, let i := F  (Xi) , i = 1, ..., n be a sequence of independent

and identically rv's. Recall that both df's F and G are assumed to be continuous, this implies that F  is continuous as well, therefore P (i  u) = u, this means that

(i)i=1,n are uniformly distributed on (0, 1) . Let us now define the corresponding uniform tail empirical process

 n (s) := k (Un (s) - s) , for 0  s  1,

(5.25)

where

n
Un (s) := k-1 1 (i < ks/n)
i=1

(5.26)

denotes the tail empirical df pertaining to the sample (i)i=1,n . In view of Proposition 3.1 of Einmahl et al. (2006), there exists a Wiener process W such that for every

0   < 1/2,

sup s- |n (s) - W (s)| P 0, as n  .
0s<1

(5.27)

19

Let us fix a sufficiently small 0 <  < 1/2. We will successively show that, under the

first-order of regular variation conditions (1.6), uniformly on x  1, for all large n :

 kMn2 (x)

=

 x1/2 W 1

t-1/

+

 1


W t-2/ dt + oP
x1/2

x , 1 2

1 2

-

1 1

+

(5.28)

and

 kMn3 (x) = -x-1/1

 1

W

(1)

+

 1


W
1

t-2/ dt

+ oP x-1/1+ . (5.29)

while





kMn1 (x) = oP x-1/1+ , kMn4 (x) = oP

x1 2

1 2

-

1 1

+

,

(5.30)

and

 kMn5

(x)

=

x-1/1

x1/1 - 11

1

 kA

(ak)

+

oP

x-1/1

.

(5.31)

Throughout the proof, without loss of generality, we assume that a  , for any

constant a > 0. We point out that all the rest terms of the previous approximations

are negligible in probability, uniformly on x > 1. Let us begin by the term Mn1 (x) which may be made into

F

x-1/1 (xXn-k:n;

)

Pn



 dFn (Xn-k:nw) -  dFn (Xn-k:nw)

x G (Xn-k:nw)

x G (Xn-k:nw)

=

F

x-1/1 (xXn-k:n

;

)

Pn



 x

1

-

1

G (Xn-k:nw) G (Xn-k:nw)

dFn (Xn-k:nw) .

By applying the mean value theorem (for several variables) to function   1/G (·) ,

yields

1

-

1

d
=

G (z) G (z) i=1

i - i

G(i) (z) G2 (z)

,

for

any

z > 1,

where  is such that i is between i and i, for i = 1, ..., d, therefore

Mn1 (x)

=

F

x-1/1 (xXn-k:n;

)

Pn



d i=1

i - i

 x

G(i) (Xn-k:nw) G2 (Xn-k:nw)

dFn

(Xn-k:nw) .

Recall that by assumptions (1.6) and [A2] both G and G(i) are regularly varying with the same index (-1/2) and on the other hand, Xn-k:n P  and w > 1, then Xn-k:nw P . Then by applying Pooter's inequalities (1.4) , yields

G (Xn-k:nw) G (Xn-k:n)

=

(1

+

oP (1)) w-1/2+

=

G(i) (Xn-k:nw) G(i) (Xn-k:n)

,

20

it follows that

Mn1 (x) = (1 + oP (1)) Pn



x-1/1 G (Xn-k:n) F (xXn-k:n; )

× d G(i) (Xn-k:n) i=1 G (Xn-k:n)

i - i


w1/2-dFn (Xn-k:nw) .
x

 For some regularity assumptions, Andersen (1970) stated that n

-

is asymp-

totically a centred multivariate normal rv, which implies that i - i = OP n-1/2 thus  P . On the other hand, by the law of large numbers Pn () P P () as n  , then we may readily show that Pn  P P () as n   as well.

Note since  is consistent estimator for  then  it is, then by using the fact that Xn-k:n P , and the two assumptions [A1] and [A3] together, we show readily that

(Xn-k:n)-

G(i) (Xn-k:n) G (Xn-k:n)

P 0,

as n  ,

and G (Xn-k:n) /G (Xn-k:n) P 1. In view of Lemma A1 in Benchaira et al. (2016a), we infer that Xn-k:n = (1 + oP (1)) (k/n)- , thus

Mn1 (x) = (k/n)- oP n-1/2 Mn1 (x) ,

where

Mn1 (x)

:=

G

x-1/1 P () (Xn-k:n) F (xXn-k:n; )


w1/2-dFn (Xn-k:nw) .
x

Making use of representation (5.24) , we write

Mn1 (x) = x-1/1

 x

G (Xn-k:n) G (Xn-k:nw)

d

F  (Xn-k:nw) F  (Xn-k:n)

-1

×

 x

w1/2-d

Fn (Xn-k:nw) F  (Xn-k:n)

.

(5.32)

Once again by using the routine manipulations of Potter's inequalities, we show that the first quantity between two brackets is

(1 + oP (1))

x



w

1/2

+/2

d

F F

 

(Xn-k:nw) (Xn-k:nw)

.

By using an integration by parts to the previous integral yields

w1/2

+/2

F F

 

(Xn-k:nx) (Xn-k:nx)

+

(1/2

+

/2)

 x

w1/2+/2-1

F F

 

(Xn-k:nx) (Xn-k:nx)

dw.

21

Recall that from (1.7) we have F   RV(-1/), then

F F

(Xn-k:nw) (Xn-k:nw)

=

(1

+

oP

(1))

w-1/+/2,

uniformly on w > 1. Therefore the previous quantity reduces into

(1 + oP (1))

1

+

1/2 + /2 -1/1 + 

x-1/1 + .

Thereby the first expression between two brackets in (5.32) equals OP x1/1- . Let

us consider the second factor in (5.32) . By similar arguments as used for the first

factor, we show that

x1/2

+/2

F F


n 

(Xn-k:nx) (Xn-k:nx)

+

(1/2

+

/2)

x



w

1/2

+/2

F F


n 

(Xn-k:nx) (Xn-k:nx)

dw

,

multiplied by (1 + oP (1)) , uniformly on x > 1. From Lemma 7.1, we have

F

 n

(Xn-k:nw)

F  (Xn-k:n)

=

OP

w-1/+/2

,

which implies that the previous expression equals OP x-1/1+ , thus Mn1 (x) =

OP w-1/+ and therefore

 kMn1

(x)

=

(k/n)1/2-

OP

w-1/1+

.

 By assumption k/n  0, it follows that kMn1 (x) = oP x-1/1+ which meets the

result of (5.32) . Let now consider the second term Mn2 (x) which may be rewritten

into

-

x-1/1

F



k/n (Xn-k:n)

F (Xn-k:n; ) F (xXn-k:n; )

G

(Xn-k:n) /F  (Xn-k:n) F (Xn-k:n; )

×

 x

G (Xn-k:n)

d

F

 n

G (Xn-k:nw)

(Xn-k:nw) - F  k/n

(Xn-k:nw) .

In view of Potter's inequalities, it is clear that

F

F (Xn-k:n; ) (Xn-k:n) /G (Xn-k:n)

P

1 

P

()

and F (Xn-k:n; ) P x1/1 . F (xXn-k:n; )

Note that F  (Xn-k:n) =d k+1:n and Smirnov's lemma (see, e.g., Lemma 2.2.3 in de

Haan

and

Feriera,

2006)

implies

that

n k

k+1:n

P

1,

hence

n k

F



(Xn-k:n)

=

1 + oP (1) .

Therefore

Mn2

(x)

=

-

(1

+

oP

(1))

 1

 x

G (Xn-k:n)

d

F

 n

G (Xn-k:nw)

(Xn-k:nw) - F  k/n

(Xn-k:nw) .

22

On the other hand, by using an integration by parts yields

Mn2

(x)

=

(1

+

oP

(1))

1 

M(n12) (x) + M(n22) (x)

,

where

M(n12) (x) :=

 x

F

 n

(Xn-k:nw;

) - F k/n



(Xn-k:nw;

) d G (Xn-k:n) G (Xn-k:nw)

and

M(n22) (x)

:=

G (Xn-k:n) G (Xn-k:nx)

F

 n

(Xn-k:nx;

) - F k/n



(xXn-k:n;

)

.

By using the change of variables t = G (Xn-k:n) /G (Xn-k:nw) , it is easy to verify

that

M(n12) (x) =



n

k G (Xn-k:n )

F

 n

(n

(t;

))

-

F



(n

(t;

))

dt,

G (Xn-k:nx)

where

n

(t; )

:=

n k

F



G 

1 - G (Xn-k:n) t-1

. Observe that

M(n12) (x) =


G(Xn-k:n) (Un (n (t; )) - n (t; )) dt,
G (Xn-k:nx)

where Un being the tail empirical df given in (5.26) , thereby

kM(n12) (x) =


G(Xn-k:n) n (n (t; )) dt,
G (Xn-k:nx)

where n being the tail empirical process defined in (5.25) . Let us decompose the

previous integral into


G(Xn-k:n) (n (n (t; )) - W (n (t; ))) dt +
G (Xn-k:nx)


G(Xn-k:n) W (n (t; )) dt
G (Xn-k:nx)

= Sn + Rn.

By applying weak approximation (5.27) we get

Sn = oP (1)


G(Xn-k:n) (n (t; ))1/2- dt.
G (Xn-k:nx)

Observe that F  G  - 1 - G (Xn-k:n) = F  (Xn-k:n) , thereby

n

(t; )

=

n k

F



(Xn-k:n)

F  G  - 1 F  G  -

- G (Xn-k:n) t-1 1 - G (Xn-k:n)

.

It is easy to check that F  (G  - (1 - ·))  RV (2/) , then once again by means of

Pooter's inequality, we show that n (t; ) = (1 + oP (1)) t-2/+, therefore

Sn = oP (1)


G (Xn-k:n) G (Xn-k:nx)

t-2/+ 1/2- dt.

23

By using an elementary integration we get

Sn = oP (1)

G (Xn-k:n) G (Xn-k:nx)

(-2/+)(1/2-)+1

= oP

x1 2

-

1 2

+

.

By replacing  by its by its expression given in (1.8) , we end up with

Sn = oP

x1 2

1 2

-

1 1

+

.

The term Rn may be decomposed into

x1/2
G(Xn-k:n) W (n (t; )) dt +
G (Xn-k:nx)


W (n (t; )) dt = Rn1 + Rn2.
x1/2

It is clear that 



|Rn1|

<



 sup t>

G (Xn-k:n) G (Xn-k:nx)

|W (n (t; ))|  (n (t; )) 

x1/2
G(Xn-k:n) (n (t; )) dt.
G (Xn-k:nx)

It is ready to check, by using the change of variables n (t; ) = s, that the previous

first factor between the curly brackets equals

sup

0<s<

n k

F

 (Xn-k:n

x;)

|W (s)| s

<

sup

0<s<

n k

F



(Xn-k:n ;)

|W (s)| s

.

From Lemma 3.2 in Einmahl et al. (2006) sup0<s1 s- |W (s)| = OP (1) , for any 0 <  < 1/2, then since nF  (Xn-k:n; ) /k P 1, as n  , we infer that

sup

s- |W (s)| = OP (1) .

0<s<

n k

F



(Xn-k:n

;)

for all large n. On the other hand, we already pointed out above that

n (t; ) = (1 + oP (1)) t-2/+,

which implies that the second factor is equal to

OP (1)

x1/2
G (Xn-k:n) G (Xn-k:nx)

t-2/+  dt = OP (1)

which after integration yields

x1/2
t-2/+dt,
G (Xn-k:n)
G (Xn-k:nx)

OP (1)

G (Xn-k:n)

-2/++1
- x-1/ -2/++1

.

G (Xn-k:nx)

Recall that from formula (1.8) we have 2/ > 1, then by using the mean value

theorem and Pooter's inequalities, we get Rn1 = oP (x-) . The second term Rn2

may be decomposed into



Rn2 =

W (n (t; )) - W t-2/

x1/2



dt +

W t-2/ dt.

x1/2

24
From Proposition B.1.10 in de Haan and Ferreira (2006), with high probability,

cn (t; ) := n (t; ) - t-2/  t-2/-, as n  ,

(5.33)

this means that supx>1 supt>x1/2 cn (t; ) P 0, as n  . This implies by using Levy's modulus of continuity of the Wiener process (see, e.g., Theorem 1.1.1 in Cs¨orgo and R´ev´esz, 1981), that

W (n (t; )) - W t-2/  2 cn (t; ) log (1/cn (t; )),

with high probability. By using the fact that log s < s-, for s  0 together with inequality (5.33) , we show that

W (n (t; )) - W t-2/ < 2t-(2/-)/2,

uniformly on t > x1/2 , it follows that


W (n (t; )) - W t-2/
x1/2

dt = oP (1)


t-(2/-)/2dt .
x1/2

Recall that the assumption 1 < 2 together with the equation 1/ = 1/1+1/2, im-

ply that 2/ (2) > 1, thus - (2/ - ) /2 + 1 < 0, therefore

 x1/2

t-(2/-)/2dt

=

oP x-1/1- . Then we showed that



Rn1 = oP x- and Rn2 =

W t-2/ dt + oP x-1/1- ,

x1/2

hence

kM(n12) (x) = Rn + Sn =



W t-2/ dt + oP x-1/1- + oP

x1 2

1 2

-

1 1

+

.

x1/2

It is clear that

-

1 1

-



-

1 2

1 2

-

1 1

+

=

- 1

+

2 + 412 212

<

0.

then

kM(n12) =



W t-2/ dt + oP

x1 2

1 2

-

1 1

+

.

x1/2

By using similar arguments we end up with

kM(n22) (x) = x1/2 W

t-1/

+ oP

x-

1 1

+

,

therefore we omit further details.

 kMn2

=

 x1/2 W 1

t-1/

+ 1


W t-2/ dt + oP
x1/2

x1 2

1 2

-

1 1

+

.

25

Let us now focus of the term Mn3. By letting x = 1 in the previous weak approxi-

mation we infer that

 k

Fn

(Xn-k:n; ) - F (Xn-k:n; ) F (Xn-k:n; )

=

 1

W

(1)

+

 1


W
1

t-2/

dt + oP (1) . (5.34)

which implies that

 k

Fn

(Xn-k:n;

)

-

F

(Xn-k:n;

)

F (Xn-k:n; )

=

OP (1) .

In other terms, we have

Fn (Xn-k:n; ) F (Xn-k:n; )

=

1

+

OP

k-1/2

.

The regular variation of F (·; ) and (5.35) together imply that

(5.35)

F (xXn-k:n; ) Fn (Xn-k:n; )

=

x-1/1

+

oP

x-1/1 +

.

(5.36)

By combining the results (5.34) and (5.36) we get

 kMn3 (x) = -x-1/2

 1

W

(1)

+

 1


W
1

t-2/

dt

+ oP x-1/1+ .

For the forth term we write

 kMn4 (x) =

F (xXn-k:n; ) - x-1/1 Fn (Xn-k:n; )

 k

Fn

(xXn-k:n;

)

-

F

(xXn-k:n;

)

.

F (xXn-k:n; )

From (5.36) the first factor of the previous equation equals oP x-1/1+ . On the other hand by using the change of variables s = t-2/, yields

 W t-2/ dt = 

x-1/
s-/2-1W (s) ds.

x1/2

2 0

Since sup0<s<1 s-1/2+ |W (s)| = OP (1) , then we easily show that


W t-2/ dt = OP

x1 2

1 2

-

1 1

+

,

x1/2

 it follows that kMn2 = OP

x1 2

1 2

-

1 1

+

as well. Therefore

 k

Fn

(xXn-k:n;

)

-

F

(xXn-k:n;

)

F (xXn-k:n; )

=

x1/1 OP

x1 2

1 2

-

1 1

+

= OP

x1 2

+

.

Hence we have

 kMn4 (x) = oP

x-1/1+

OP

x1 2

+

= oP

x1 2

1 2

-

1 1

+

.

26

By assumption F satisfies the second order condition of regular variation function (1.5) , this means that for

lim F (tx) /F (t) - x-1/1 = x-1/1 x1/1 - 1 ,

t

A (t)

11

(5.37)

for any x > 0, where 1 < 0 is the second-order parameter and A is RV (1/1) . The uniform inequality corresponding to (5.37) says: there exist t0 > 0, such that for any t > t0, we have

F (tx) /F (t) - x-1/1 A (t)

-

x-1/1

x1/1 - 11

1

< x-1/1+1/1+,

see for instance assertion (2.3.23) of Theorem 2.3.9 in de Haan and Ferreira (2006). It is easy to check that the later inequality implies that





kMn5 (x) = k

F (xXn-k:n; ) - x-1/1 F (Xn-k:n; )

=

x-1/1

x1/1 - 11

1

 kA

(Xn-k:n)

+

oP

x-1/1

x1/1 - 11

1

 kA

(Xn-k:n)

.

Recall that ak := F  (1 - k/n) and notice that Xn-k:n/ak P 1 as n  , then in

view

of

the

regular

variation

of

A

we 

infer

that

A (Xn-k:n)

=

(1

+ oP (1)) A (ak) .

On the other hand, by assumption kA (ak) is asymptotically bounded, therefore

 kMn5

(x)

=

x-1/1

x1/1 - 11

1

 kA

(ak)

+

oP

x-1/1

.

To summarize, as this stage we showed that

Dn

x; 

=  x1/2 W 1

t-1/

+

 1


W
x1/2

t-2/ dt

- x-1/2  W (1) + 


W t-2/ dt

1

1 1

+

x-1/1

x1/1 - 11

1

 kA

(ak)

+



(x)

,

where  (x) := oP x-1/1+ + oP x-1/1

+ oP

x1 2

1 2

-

1 1

+

. By using a change

of variables, we show that sum of the first three terms equals the Gaussian process

 (x; W ) stated in Theorem 2.1. Recall that 1 < 2 and

1 1 - 1 +  < 0, 2 2 1

27

then it is easy to verify that  (x) = oP

x1 2

1 2

-

1 1

+

. It follows that

x

Dn x; 

-



(x;

W

)

-

x-1/1

x1/1 - 11

1

 kA

(ak

)

= oP

x1 2

1 2

-

1 1

+2

= oP (1) ,

uniformly on x > 1, therefore

sup x Dn
x>1

x; 

-



(x;

W

)

-

x-1/1

x1/1 - 11

1

 kA

(ak

)

= oP (1) ,

for any samll 0 <  < 1/2, which completes the proof of Theorem 2.1.

5.2. Proof of Theorem 2.2. From the representation (1.17) we write

1 - 1 = Tn1 + Tn2 + Tn3,

where



Tn1 := k-1/2

x-1

1

Dn x; ; 1

-



(x;

W

)

-

x-1/1

x1/1 - 11

1

 kA

(ak

)

dx



Tn2 := k-1/2

x-1 (x; W ) dx

1

and

Tn3 := -A (ak)

1



x-1/1 -1

x1/1 - 11

1

dx.

By using 2.1 yields Tn1 = oP k-1/2

 1

x-1+

dx

=

oP

k-1/2 .

Since E |W (s)| 

s1/2, then it is easy to show that

 1

x-1

(x;

W)

dx

=

OP

(1)

,

it

follows

that

Tn2

=

OP

k-1/2

. By using an elementary integration, we get Tn3 :=

. A(ak )
1-1

Since

both

k-1/2 and A (ak) tend to zero as n  , then Tn3 = o (1) , it follows that 1 P 1,

which gives the first result of Theorem. To establish the asymptotic normality









k (1 - 1) = kTn1 + kTn2 + kTn3,

where







kTn1 = oP (1) , kTn2 =

x-1 (x; W ) dx

1

and

 kTn2

=



kA 1-

(ak 1

)

.

Note that  (x; W ) is a centred Gaussian process and by using the assumption 
kA (ak)   < , we end up with

 k

(1

-

1)

D

N

1

 -

1

,

E



2

x-1 (x; W ) dx .

1

28

By using elementary calculation we show that E

 1

x-1

(x;

W

)

dx

2

=

2,

that

we omit the details.

6. Conclusion
In basis on a semiparametric estimator of the underlying distribution function, we proposed a new estimation method to the tail index of Pareto-type distributions for randomly right-truncated data. Compared with the existing ones, this estimator behaves well both in terms of bias and rmse. A useful weak approximation of the corresponding tail empirical process allowed us to establish both the consistency and asymptotic normality of the proposed estimator.

References
Andersen, E. B., 1970. Asymptotic properties of conditional maximum-likelihood estimators. J. Roy. Statist. Soc. Ser. B 32, 283-301.
Benchaira, S., Meraghni, D., Necir, A., 2015. On the asymptotic normality of the extreme value index for right-truncated data. Statist. Probab. Lett. 107, 378-384.
Benchaira, S., Meraghni, D., Necir, A., 2016a. Tail product-limit process for truncated data with application to extreme value index estimation. Extremes, 19, 219-251.
Benchaira, S., Meraghni, D., Necir, A., 2016b. Kernel estimation of the tail index of a right-truncated Pareto-type distribution. Statist. Probab. Lett. 119, 186-193.
Bilker W. B.,Wang, M. C., 1996.Asemiparametric extension of theMann­Whitney test for randomly truncated data. Biometrics 52, 10-20
Caeiro, F., Gomes, M.I, 2015. Threshold Selection in Extreme Value Analysis. Chapter in: Dipak Dey and Jun Yan, Extreme Value Modeling and Risk Analysis: Methods and Applications, Chapman-Hall/CRC, ISBN 9781498701297, pp. 69-87.
Cs¨orgo,M., R´ev´esz, P., 1981. Strong Approximations in Probability and Statistics. Probability andMathematical Statistics. Academic Press, Inc. [Harcourt Brace Jovanovich, Publishers], New York-London.
Drees, H., de Haan, L., Li, D., 2006. Approximations to the tail empirical distribution function with application to testing extreme value conditions. J. Statist. Plann. Inference 136, 3498-3538.
Einmahl, J.H.J., de Haan, L., Li, D., 2006. Weighted approximations of tail copula processes with application to testing the bivariate extreme value condition. Ann. Statist. 34, 1987­2014.

29
Gardes, L., Stupfler, G., 2015. Estimating extreme quantiles under random truncation. TEST 24, 207-227.
de Haan, L., Stadtmu¨ller, U., 1996. Generalized regular variation of second order. J. Australian Math. Soc. (Series A) 61, 381-395.
de Haan, L., Ferreira, A., 2006. Extreme Value Theory: An Introduction. Springer. Hill, B.M., 1975. A simple general approach to inference about the tail of a distri-
bution. Ann. Statist. 3, 1163-1174. Haouas, N., Necir, A, Meraghni, D, Brahimi, B., 2018. A Lynden-Bell integral
estimator for the tail index of right-truncated data with a random threshold. Afr. Stat. 12, 1159-1170. Haouas, N., Necir, A., Brahimi, B., 2019. Estimating the second-order parameter of regular variation and bias reduction in tail index estimation under random truncation. J. Stat. Theory Pract. 13. He, S., Yang, G. L., 1998. Estimation of the truncation probability in the random truncation model. Ann. Statist. 26, 1011-027. Hua, L., Joe, H., 2011. Second order regular variation and conditional tail expectation of multiple risks. Insurance Math. Econom. 49, 537-546. Koning, A. J., Peng, L., 2008. Goodness-of-fit tests for a heavy tailed distribution. J. Statist. Plann. Inference 138, 3960-3981 Lagakos, S. W., Barraj, L. M., De Gruttola, V., 1988. Nonparametric analysis of truncated survival data, with applications to AIDS. Biometrika 75, 515­523. Lawless, J.F., 2002. Statistical Models and Methods for Lifetime Data, Second Edition. Wiley Series in Probability and Statistics. Li, G., Qin, J., Tiwari, R. C., 1997. Semiparametric likelihood ratio-based inferences for truncated data. J. Amer. Statist. Assoc. 92, 236­245. Lui, K. J., Lawrence, D. N., Morgan, W. M., Peterman, T. A., Haverkos, H. H., Breakman, D. J., 1986. A model-based approach for estimating the mean incubation period of transfusion-associated acquired immunodeficiency syndrome. Proc. Nat. Acad. Sc. 83, 2913-7. Lynden-Bell, D.,1971. A method of allowing for known observational selection in small samples applied to 3CR quasars. Monthly Notices Roy. Astron. Soc. 155, 95-118. Neves, C., Fraga Alves, M.I., 2004. Reiss and Thomas' automatic selection of the number of extremes. Comput. Statist. Data Anal. 47, 689-704.

30
Moreira, C., de Un~a-A´ lvarez, J., 2010. A semiparametric estimator of survival for doubly truncated data. Stat. Med. 29, 3147­3159
Moreira, C., de Un~a-A´ lvarez, J., Van Keilegom, I., 2014. Goodness-of-fit tests for a semiparametric model under random double truncation. Comput. Statist. 29, 1365­137
Reiss, R.D., Thomas, M., 2007. Statistical Analysis of Extreme Values with Applications to Insurance, Finance, Hydrology and Other Fields, 3rd ed. Birkh¨auser Verlag, Basel, Boston, Berlin.
Shen, P-S., 2010. Semiparametric analysis of doubly truncated data. Comm. Statist. Theory Methods 39, 3178­3190.
Shen, P-S, Hsu, H., 2020. Conditional maximum likelihood estimation for semiparametric transformation models with doubly truncated data. Comput. Statist. Data Anal. 144, 106862, 15 pp.
Shorack, G.R.,Wellner, J.A., 1986. Empirical Processes with Applications to Statistics.Wiley, New York.
Qin, J., Wang, M-C., 2001, Semiparametric analysis of truncated data. Lifetime Data Anal. 7, no. 3, 225­242.
Wang, M.-C., 1989. A semiparametric model for randomly truncated data. J. Amer. Statist. Assoc. 84:742­748.
Weissman, I., 1978. Estimation of parameters and large quantiles based on the k largest observations. J. Am. Statist. Assoc. 73. 812-815.
Worms, J., Worms, R., 2016. A Lynden-Bell integral estimator for extremes of randomly truncated data. Statist. Probab. Lett. 109, 106-117.
Woodroofe, M., 1985. Estimating a distribution function with truncated data. Ann. Statist. 13, 163-177.

7. Appendix

Lemma 7.1. For any small  > 0, we have

F

 n

(Xn-k:nw)

F  (Xn-k:n)

=

OP

w-1/+/2

,

uniformly on w  1.

Proof. Let Vn (t) := n-1

n i=1

1

(i



t)

be

the

uniform

empirical

df

pertaining

to

the sample i := F  (Xi) , i = 1, ..., n, of iid uniform(0, 1) rv's. It is clear that, for

an arbitrary x, we have Vn F  (x)

=

F

 n

(x)

almost

surely.

From Assertion 7 in

Shorack and Wellner (1986) (page 415), Vn (t) /t = OP (1) uniformly on 1/n  t  1,

this implies that

F

 n

(Xn-k:nw)

F  (Xn-k:nw)

=

OP (1) ,

uniformly

on

w



1.

On the other hand, by applying Potter's inequalities (1.4) to F , we get

F  (Xn-k:nw) F  (Xn-k:n)

=

OP

w-1/+/2

,

uniformly on w  1.

Combining the two statements (7.38) and (7.39) gives the desired results.

31
(7.38) (7.39)

Estimated gamma

0.0

0.5

1.0

1.5

2.0

0

gamma gamma2

50

100

Kn

150

200

250

