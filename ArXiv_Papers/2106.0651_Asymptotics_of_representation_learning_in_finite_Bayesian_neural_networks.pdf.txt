arXiv:2106.00651v1 [cs.LG] 1 Jun 2021

Asymptotics of representation learning in finite Bayesian neural networks
Jacob A. Zavatone-Veth1,2, Abdulkadir Canatar1,2, and Cengiz Pehlevan2,3
1Department of Physics, Harvard University, Cambridge, MA 02138 2Center for Brain Science, Harvard University, Cambridge, MA 02138 3John A. Paulson School of Engineering and Applied Sciences, Harvard University, Cambridge, MA 02138
June 2, 2021
Abstract Recent works have suggested that finite Bayesian neural networks may outperform their infinite cousins because finite networks can flexibly adapt their internal representations. However, our theoretical understanding of how the learned hidden layer representations of finite networks differ from the fixed representations of infinite networks remains incomplete. Perturbative finite-width corrections to the network prior and posterior have been studied, but the asymptotics of learned features have not been fully characterized. Here, we argue that the leading finite-width corrections to the average feature kernels for any Bayesian network with linear readout and quadratic cost have a largely universal form. We illustrate this explicitly for two classes of fully connected networks: deep linear networks and networks with a single nonlinear hidden layer. Our results begin to elucidate which features of data wide Bayesian neural networks learn to represent.
1 Introduction
The expressive power of deep neural networks critically depends on their ability to learn to represent the features of data [1­22]. However, the structure of their hidden layer representations is only theoretically well-understood in certain infinite-width limits, in which these representations cannot flexibly adapt to learn data dependent features [3­10, 22]. In the Bayesian setting, these representations are described by fixed, deterministic kernels [3­10]. As a result of this inflexibility, recent works have suggested that finite Bayesian neural networks may generalize better than their infinite counterparts because of their ability to learn representations [9].
Theoretical exploration of how finite and infinite Bayesian neural networks differ has largely focused on the properties of the prior and posterior distributions over network outputs [11­16]. In particular, several works have studied the leading perturbative finite-width corrections to these distributions [11­ 15]. Yet, the corresponding asymptotic corrections to the feature kernels have only been studied in a few special cases [15]. Therefore, the structure of these corrections, as well as their dependence on network architecture, remain poorly understood. In this paper, we make the following contributions towards the goal of a complete understanding of feature learning at asymptotically large but finite widths:
· We argue that the leading finite-width corrections to the average hidden layer kernels of any Bayesian neural network with linear readout and least-squares loss have a largely prescribed form (Conjecture
jzavatoneveth@g.harvard.edu canatara@g.harvard.edu cpehlevan@seas.harvard.edu
1

1). In particular, we argue that these assumptions fix the dependency of the correction on the target outputs, as well as the fact that the size of the correction should scale with the ratio of the number of outputs to the typical hidden layer width. We show analytically and numerically that these constraints should hold for fully-connected feedforward networks, and conjecture that they should extend to other architectures.
· We explicitly compute the leading finite-width corrections for deep linear networks (§4.1) and networks with a single nonlinear hidden layer (§4.2), showing that they are of the general form claimed. These tractable examples show how certain directions in the output kernel matrix can be enhanced or suppressed in the hidden layer representations depending on the structure of the input similarities. Moreover, even in this simple setting, different choices of nonlinearity can affect the particular combinations of input and output similarities on which each element of the resulting feature kernel depends.
Our results begin to elucidate the structure of learned representations in wide Bayesian neural networks. The assumptions of our general argument are satisfied in many regression settings, hence our qualitative conclusions should be broadly applicable.

2 Preliminaries
We begin by defining our notation, setup, and assumptions. We will index training and test examples by Greek subscripts µ, , . . ., and layer dimensions (that is, neurons) by Latin subscripts j, l, . . .. Layers will be indexed by the script Latin letter . Matrix- or vector-valued quantities corresponding to a given layer will be indexed with a parenthesized superscript, while scalar quantities that depend only on the layer will be indexed with a subscript. Depending on context, · will denote the 2 norm on vectors or the Frobenius norm on matrices. We denote the standard Euclidean inner product of two vectors a, b  Rn by a · b.

2.1 Bayesian neural networks

Our primary results concern deep Bayesian neural networks with fully connected linear readout. Such a network f : Rn0  Rnd with d layers can be written as

f (x; W d, W)

=

1 nd-1

W

(d)

(x;

W

),

(1)

where the feature map (·; W) : Rn0  Rnd-1 includes all d - 1 hidden layers, collectively parameterized by W. Here,  can be some combination of fully-connected feedforward networks, convolutional networks, recurrent networks, et cetera, such that it is expressible within the Tensor Programs framework of Yang [7]. We assume isotropic Gaussian priors over the trainable parameters [1­21], with

Wi(jd)


i.i.d.

N (0,

d2)

(2)

in particular. In our analysis, we fix an arbitrary training dataset D = {(xµ, yµ)}pµ=1 of p examples. For analytical
tractability, we consider a least-squares cost

1p E(; D) =
2

f (xµ) - yµ 2,

(3)

µ=1

2

and take the un-normalized likelihood to be p(D | )  exp(-E), where   0 is an inverse temperature parameter that tempers the likelihood and  = {W (d), W}. We then introduce the Bayes posterior over
parameters given these data:

p(D | )p()

p( | D) =

;

(4)

p(D)

we denote averages with respect to this distribution by · . This is equivalent to considering the equilibrium distribution of Langevin sampling of the parameters at inverse temperature  with temperaturedependent weight decay:

d( )(t) = -(-1 + E)dt + 2-1dB( )(t)

(5)

where B( )(t) is a standard Wiener process and  is the diagonal matrix of prior variances. Though this choice of temperature dependence may a priori seem to be one of convenience, we show in Appendices F and G that alternative choices yield pathological low-temperature behavior in deep linear networks. Moreover, by tuning , one can then adjust whether the posterior (4) is dominated by the prior ( 1) or the likelihood ( 1).

2.2 Feature kernels and the Gaussian process limit

We quantify representations using the hidden layer feature kernels, which measure how the similarities between inputs evolve as they are propagated through the network. Schematically, these kernels are defined as

Kµ( )



1 n



(h(µ ))

·



(h(

))

( = 1, . . . , d - 1),

(6)

where h(µ ) is the n -dimensional vector of preactivations at the -th hidden layer for the µ-th training input,  is the activation function, and the inner product is taken over internal indices of the layer. These kernels are the central objects of our study; our concrete goal is to study their averages K( ) with respect to the Bayes posterior.
We consider the limit of large hidden layer widths n1, n2, . . . , nd-1   with n0, nd, p, and d fixed. More precisely, we consider a limit in which n =  n for = 1, . . . , d - 1, where   (0, ) and n  , as studied by [3­14, 16­18, 22] and others. Importantly, we note that size of n0 relative to n is unimportant for our results, whereas nd/n and d/n must be small [9, 11, 16]. As mentioned above, we assume that  is such that the network can be expressed in the Tensor Programs framework of Yang [7]. Then, in this infinite-width limit, the kernels converge in probability to the deterministic Neural Network Gaussian Process (GP) kernels [3­8], defined recursively as

[KG( P) ]µ = E  (h(µ )) (h( )) : h(µ )  N (0, 2KG( P-1) + 211 )

(7)

with base case KG(0P) = Gxx given by the Gram matrix [Gxx]µ  n-0 1xµ · x of the inputs. Thus, in the GP limit, the hidden layer kernels are independent of the target outputs.
One heuristically expects the kernels to provide a reasonably complete description of feature learning at large widths. As a possible complication, one could imagine a situation in which the activation distributions are `rotated' during inference without changing the kernels. However, Yang and Hu [8] have shown that this cannot happen in the infinite-width limit for a restricted class of nonlinearities, and conjecture that it is forbidden more broadly. Therefore, our goal is to compute the leading asymptotic finite-width corrections to the NNGP kernels for n 1 as a step towards understanding feature learning in finite width networks.

3

3 Universal aspects of the asymptotics of learned representations
We first present our main result, which shows that the form of the leading perturbative correction to the average hidden layer kernels of a Bayesian neural network is tightly constrained by the assumptions that the readout is linear, that the cost is quadratic, and that the NNGP limit is well-defined.

3.1 Pertubative corrections to the average feature kernels

Our main result is as follows:

Conjecture 1 Consider a Bayesian neural network with linear readout of the form (1), with posterior (4). Assume that this network admits a well-defined GP limit, i.e., that its architecture is covered in the convergence proofs of Yang [7]. Then, the averages of the hidden layer kernels with respect to the posterior have well-behaved asymptotic series expansions at large width, with leading terms

Kµ( )

=

EW Kµ( )

+

1 2 nd

p
[2-1Gyy-1 - -1] cWov(Kµ( ), K(d-1)) + . . . ,

(8)

,=1

where [Gyy]µ  n-d 1yµ · y is the Gram matrix of the outputs and   Ip + d2KG(dP-1). Here, the cumulants of the kernels are computed with respect to the prior, and are themselves given by asymptotic
series at large widths. The ellipsis denotes terms that are of subleading order in the inverse hidden layer
widths.

In Appendix B, we derive this result perturbatively by expanding the posterior cumulant generating
function of the kernels in powers of deviations from their GP values. However, the resulting perturbation
series may not rigorously be an asymptotic series for the average kernels, and this method does not
yield quantitative bounds for the width-dependence of the terms. We therefore frame it as a conjecture.
For fully-connected networks, also known as multilayer perceptrons (MLPs), a quantitative version of this result follows from the work of Yaida [11]. For MLPs, he showed that EW K( ) = KG( P) + O(n-1), covW (Kµ( ), K(d-1)) = O(n-1), and that higher cumulants are of O(n-2).
Conjecture 1 posits that there are two possible types of leading finite-width corrections to the GP kernels. The first class of corrections are deviations of EW K( ) from KG( P) . These terms reflect corrections to the prior, and do not reflect non-trivial representation learning as they are independent of the outputs. The second class of corrections are those from the output-dependent covW (Kµ(), K(d-1)) term. For deep linear MLPs or MLPs with a single hidden layer, EW K( ) is exactly equal to KG( P) at any width (see Appendix B) [11], and only the covariance term contributes. More broadly, the results of Yaida [11] show that these two terms should be of the same order for any MLP; EW K( ) - KG( P) should not dominate the covariance term.
The leading output-dependent correction has several interesting features. First, it includes a factor
of nd, hence, at least for MLPs, it scales with nd/n and not with 1/n alone. This mirrors the findings of Aitchison [9]; feature learning in wide Bayesian networks with many outputs is qualitatively different
from that in networks with few outputs relative to their hidden layer width. If nd/n does not tend to zero with increasing n, the infinite-width behavior is not described by a standard GP [7, 9]. Moreover, we note that the matrix  is invertible at any finite temperature, even when KG(dP-1) is singular. Therefore, provided that one can extend the GP kernel by continuity to non-invertible Gxx, this result can be applied in the data-dense regime n0 < p as well as the data-sparse regime n0 > p. Finally, we observe that the correction depends on the outputs only through their Gram matrix Gyy. This result is intuitively sensible, since with the prior (2) and cost (3) the induced posterior over the output activations is invariant under
simultaneous rotation of the output activations and targets. Furthermore, Gyy is transformed by factors of the matrix -1, hence the correction depends on certain interactions between the output similarities and the GP kernel KG(dP-1).

4

3.2 High- and low-temperature limits of the leading correction

To gain some intuition for the properties of the leading finite-width corrections, we consider their highand low-temperature limits. These limits correspond to tuning the posterior (4) to be dominated by the prior and the likelihood, respectively. At high temperatures ( 1), expanding -1 as a Neumann series (see Appendix A and [23]) yields

2-1Gyy-1 - -1 = -Ip + 2(Gyy + d2KG(dP-1)) + O(3).

(9)

Thus, at high temperatures, the outputs only influence the average kernels of Conjecture 1 to subleading
order in both width and , which is intuitively sensible given that the likelihood is discounted in this regime. Moreover, the leading output-dependent contribution averages together Gyy and KG(dP-1), hence, intuitively, there is no way to `cancel' the GP contributions to the average kernels. We note that, at
infinite temperature ( = 0), the posterior reduces to the prior, and all finite-width corrections to the average kernels arise from the discrepancy between EW K( ) and KG( P) .
At low temperatures ( 1), the behavior of -1 differs depending on whether or not KG(dP-1) is of full rank. In particular, let P be an orthogonal projection matrix from Rp to the rk KG(dP-1)-dimensional image of KG(dP-1). As KG(dP-1) is a positive-semidefinite real matrix, it has a positive-semidefinite Moore-Penrose pseudoinverse [KG(dP-1)]+ satisfying KG(dP-1)[KG(dP-1)]+ = [KG(dP-1)]+KG(dP-1) = P . If KG(dP-1) is full-rank, P = Ip and [KG(dP-1)]+ = [KG(dP-1)]-1. Then, we have

2-1Gyy-1 - -1 = d-4[KG(dP-1)]+(Gyy - d2KG(dP-1))[KG(dP-1)]+ + O(-1).

(10)

Therefore, the leading-order low temperature correction depends on the difference between the target
and NNGP kernels, while the leading non-trivial high temperature correction depends on their sum. Furthermore, the low-temperature correction projects the target outputs into the image of KG(dP-1).

4 Learned representations in tractable network architectures
Having derived the general form of the leading perturbative finite-width correction to the average feature kernels, we now consider several example network architectures. For these tractable examples, we explicitly show that Conjecture 1 holds perturbatively, and provide concrete formulas for the corrections.

4.1 Deep linear networks
We first consider deep linear networks with no bias terms, for which the required computations are particularly simple applications of Isserlis' theorem (see Appendix A and [24, 25]). For such a network, we have KG( P) = m2Gxx, where m2  22-1 · · · 12 is the product of prior variances. Deferring the details of our calculation to Appendix D, we find that the assumptions of our general argument are satisfied, and that (8) simplifies to

1 m2

K( )

= Gxx +

nd n

m2d2Gxx-1

Gyy

-

m2dGxx

-

1  Ip

-1Gxx + O(n-2),

=1

(11)

where   Ip + m2dGxx. Thus, the leading corrections to the normalized average kernels K( ) /m2 are identical up to a scalar factor that encodes the width-dependence of the correction. This sum-of-inversewidths dependence was previously noted by Yaida [11] in his study of the corrections to the prior of a deep linear network. For a network with hidden layers of equal width n, we have the simple linear dependence
=1(nd/n ) = nd /n. If one instead includes a narrow bottleneck in an otherwise wide network, this

5

dependence predicts that the kernels before the bottleneck should be close to their GP values, while those after the bottleneck should deviate strongly. In Appendix H, we show that the correction remains of the same form even if one allows arbitrary forward skip connections, though the dependence on width and depth is given by a more complex recurrence relation.
This result simplifies further at low temperatures, where, by the result of §3.2, we have

1 m2

K( )

= Gxx +

nd n
=1

m-d 2Gyy - Gxx + O(n-2, -1),

(12)

in the regime in which Gxx is invertible. We thus obtain the simple qualitative picture that the lowtemperature average kernels linearly interpolate between the input and output Gram matrices. In Appendix E, we show that this limiting result can be recovered from the recurrence relation derived through other methods by Aitchison [9], who did not use it to compute finite-width corrections.
We can gain some additional understanding of the structure of the correction by using the eigendecomposition of Gxx. As Gxx is by definition a real positive semidefinite matrix, it admits a unitary eigendecomposition Gxx = U U  with non-negative eigenvalues µµ. In this basis, the average kernel is

1 m2

U



K(

)

U

=+

nd n

m-d 2~ U GyyU ~ - ~  + O(n-2),

(13)

=1

where we have defined the diagonal matrix

~



Ip

m2d + m2d

.

(14)

As m2d  0, the diagonal elements of ~ are bounded as 0  ~ µµ  1. Thus, the factors of -1Gxx by which Gyy is conjugated have the effect of suppressing directions in the projection of Gyy onto the
eigenspace of Gxx with small eigenvalues. We can see that this effect will be enhanced at high temperatures ( 1) and small scalings (m2d 1), and suppressed at low temperatures and large scalings. For this linear network, similarities are not enhanced, only suppressed. Moreover, if Gxx is diagonal, then a given
element of the average kernel will depend only on the corresponding element of Gyy.

4.2 Networks with a single nonlinear hidden layer
We now would like to characterize how including nonlinearity affects the structure of learned representations. For a general deep MLP, it is generally not possible to analytically compute covW (Kµ(), K(d-1)) to the required perturbative order [8, 11, 17, 18]. However, we can make some analytical progress in a network with a single nonlinear hidden layer and no bias terms. As detailed in Appendix I, for such a network we have the exact expressions

[KGP]µ = E (h(µ1))(h(1))

(15)

and

1

cov(Kµ , K)
W

=

n1

E (h(µ1))(h(1))(h(1))(h(1)) - [KGP]µ [KGP]

,

(16)

where we drop the layer index on the kernel for brevity as there is only one hidden layer, and expectations are taken over the p-dimensional Gaussian random vector h(µ1), which has mean zero and covariance cov(h(µ1), h(1)) = 12(Gxx)µ .
Though these expressions are easy to define, it is not possible to evaluate the four-point expectation
in closed form for general Gram matrices Gxx and nonlinearities . This obstacle has been noted in

6

previous studies [8, 11, 14], and makes it challenging to extend approaches similar to those used here to deeper nonlinear networks. For polynomial activation functions, the required expectations can be evaluated using Isserlis' theorem (see Appendix A). However, even for a quadratic activation function (x) = x2, the resulting formula for the kernel will involve many elementwise matrix products, and cannot be simplified into an intuitively comprehensible form.
To gain some intuition for how different choices of nonlinear activation function affect the learned representations, we consider the case in which Gxx is diagonal. In this special case, the four-point term simplifies dramatically. In particular, we have

(KGP)µ = var[(h(µ1))]µ + E[(h(µ1))]E[(h(1))]

(17)

and

1

cov(Kµ , K)
W

=

n1

var[(h(µ1))2]µ µµ

+ var[(h(µ1))] var[(h(1))](1 - µ )(µ + µ) ,

(18)

which yields

Kµ

=

(KGP)µ

+

1 2

n2 n1

22

(2-1Gyy

-1

-

-1)µ

× var[(h(µ1))2]µ + 2 var[(h(µ1))] var[(h(1))](1 - µ ) + O(n-1 2).

(19)

Moreover, applying the Sherman-Morrison formula [23], we have

-µ1

=

µ µ

-

1+

p =1

1 E[(h(1))]2/

E[(h(µ1))] µ

E[(h(1))] , 

(20)

where we have defined the vector µ  1 + 22 var[(h(µ1))] for brevity. These results reveal an interesting distinction between the behavior of activation functions that yield
E(h) = 0 and those that yield E(h) = 0. In particular, for those that have E(h) = 0, the NNGP kernel is diagonal, and a given element of the NLO correction depends only on the corresponding element
of Gyy. However, if E(h) = 0, then the NNGP kernel includes a rank-1 component, and each element of the NLO correction depends on all elements of Gyy. Moreover, this means that the case in which Gxx is diagonal is qualitatively distinct from the case in which there is only a single training input.

5 Numerical experiments
In this section, we numerically probe the predictions of Conjecture 1, starting with simulations of deep linear networks. We devise an artificial task in which n0 = 10-dimensional inputs xµ are chosen randomly from a standard Gaussian distribution and are randomly assigned to nd = 10 dimensional one-hot labels xµ. This generative model yields structured output correlations. Then, the weights of a depth-d linear network are trained using a discretization of the Langevin dynamics in (5). We provide a detailed discussion of our numerical methods in Appendix J [26]. As illustrated in Figure 1, we find excellent agreement with our theory for widths as low as 150.
Finally, we study how narrow bottlenecks affect representation learning in a more realistic nonlinear network. We train a network with three hidden layers and error function (erf) activations on a subset of the MNIST dataset [27]. Despite its analytical simplicity, erf is among the activation functions for which the covariance term in Conjecture 1 cannot be evaluated in closed form (see §4.2). However, it is
7

a)

b)

Figure 1: 3-hidden-layer neural network with linear activations trained via Langevin sampling on an artificial dataset (see Appendix J). (a) The Frobenius norm of the deviation of the empirical average kernel of each layer from its GP value (in this case, simply Gxx) for varying widths. We see perfect match with theoretical predictions, which are shown as solid lines. We obtain the predicted 1/n decay with increasing width and the linear scaling with the depth. Error bars represent the standard deviation of the mean over three realizations but are smaller than the markers. (b) Scatter plot of individual elements of the experimental (ordinate) and theoretical (abscissa) kernels. For low widths a slight deviation is visible between experiment and theory, while for larger widths the agreement is better.
easy to simulate numerically. Consistent with the predictions of our theory for linear networks, we find that introducing a narrow bottleneck leads to more representation learning in subsequent hidden layers, even if those layers are quite wide (Figure 2). Quantitatively, if one increases the width of the hidden layers between which the fixed-width bottleneck is sandwiched, the deviation of the first layer's kernel from its GP value decays roughly as 1/n with increasing width, while the deviations for the bottleneck and subsequent layers remain roughly constant. In contrast, the kernel deviations throughout a network with equal-width hidden layers decay roughly as 1/n (Figure 2). These observations are qualitatively consistent with the width-dependence of the linear network kernel (11). Precise characterization of nonlinear networks will be an interesting objective for future work.
6 Related work
Our work is closely related to several recent analytical studies of finite-width neural networks. First, Aitchison [9] argued that the kernel flexibility afforded by finite-width Bayesian networks can be advantageous. He derived a recurrence relation for the learned feature kernels in deep linear networks, which he solved in the limits of infinite width and few outputs, narrow width and many outputs, and infinite width and many outputs. As discussed in §4.1 and in Appendix E, our results on deep linear networks extend those of his work. Moreover, our numerical results support his suggestion that networks with narrow bottlenecks may learn interesting features.
Our approach and the asymptotic regime we consider mirror recent perturbative studies of finitewidth Bayesian networks. As noted in §3 and Appendix B, we make use of the results of Yaida [11], who derived recurrence relations for the pertubative corrections to the cumulants of the finite-width prior for an MLP. However, Yaida did not attempt to study the statistics of learned features; the goal of his work was to establish a general framework for the study of finite-width corrections. Perturbative corrections to the network prior and posterior have also been studied by Halverson et al. [12] and Naveh
8

a)

b)

c)

Figure 2: 3-hidden layer neural network with erf activations trained via Langevin sampling on MNIST
(see Appendix J). (a) The empirical average kernels compared to the GP kernels for all layers with
varying widths. Labels on the y-axes indicate the widths of each layer and the last column shows the output kernel. We observe that for networks with bottleneck layers, the deviation from KG( P) is largest at the bottleneck indicating representation learning; without a bottleneck deviations are considerably
less (the last row). The off-diagonal elements in the 3rd layer for bottleneck networks acquire smaller
corrections than the diagonal ones, indicating learning of output correlations Gyy. (b) Hidden layer kernel deviation from GP kernels as a function of width for bottleneck networks. While the first layer shows 1/n scaling, the bottleneck layer and the 3rd layer deviations stay almost constant. This behavior is predicted
analytically for linear networks. (c) As in (b) for networks without a bottleneck. Consistent with our
theory, all layers display 1/n decay.

9

et al. [14], respectively. Our work builds upon these studies by perturbatively characterizing the internal representations that are learned upon inference.
Li and Sompolinsky [15] considered deep linear networks in an alternative asymptotic regime in which the input dimensionality, hidden layer width, and number of training examples were all taken to infinity with fixed ratios, i.e., n1, n, p   with n1 = n2 = · · · = nd-1 = n, n0/n  (0, ), and p/n  (0, ). This asymptotic regime is standard in studies of neural networks from the standpoint of statistical physics [28], though most studies of wide Bayesian neural networks consider the fixed-p regime investigated here [3­14, 16­18, 22]. In Appendix E, we show that our result (12) for the low-temperature kernel can be recovered as the p/n  0 limit of their result. As the dataset size p appears only implicitly in our approach, we leave the incorporation of corrections at large dataset size as an interesting objective for future work.
Finally, we have recently derived exact formulas for the prior densities over the outputs of finite Bayesian networks with linear or ReLU activation functions [16]. There, we showed that the priors for a single training input have heavy tails that are not captured by leading perturbative corrections to the density. This leaves open the possibility that there could be non-perturbative corrections to the average kernels that are not captured by Conjecture 1. However, if the target outputs are in some sense sufficiently small and the likelihood sufficiently concentrated near the origin, these effects may be strongly suppressed in the posterior provided that the width is large. It will be interesting to investigate the possibility of non-perturbative effects on feature learning in future work.
7 Conclusions
In this paper, we have shown that the leading perturbative feature learning corrections to the NNGP kernels of wide Bayesian neural networks with linear readout and least-squares cost should be of a tightly constrained form. We demonstrate analytically and with numerical experiments that these results hold for MLPs, and conjecture that they should extend to more general network architectures that admit a well-defined NNGP limit.
Limitations. We emphasize that our perturbative argument for Conjecture 1 is not rigorous, and, as discussed above, does not account for possible non-perturbative effects. Moreover, it is non-quantitative: we have not obtained rigorous asymptotic bounds on the remainder for general network architectures. We leave rigorous proofs of the applicability of our results to more general architectures and of the smallness of the remainder as an objective for future work. Furthermore, we have considered only one possible asymptotic regime: that in which the width is taken to infinity with a finite training dataset and small output dimensionality. As discussed above in reference to the work of Aitchison [9] and Li and Sompolinsky [15], investigation of alternative limits in which output dimension, dataset size, depth, and hidden layer width are all taken to infinity with fixed ratios may be an interesting subject for future work.
8 Acknowledgements
We thank B. Bordelon for helpful comments on our manuscript. JAZ-V acknowledges support from the NSF-Simons Center for Mathematical and Statistical Analysis of Biology at Harvard and the Harvard Quantitative Biology Initiative. CP thanks Intel, Google, and the Harvard Data Science Initiative for support.
References
[1] Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning. MIT Press, Cambridge, MA, USA, 2016.
10

[2] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436­444, 2015.
[3] Radford M Neal. Priors for infinite networks. In Bayesian Learning for Neural Networks, pages 29­53. Springer, 1996.
[4] Christopher KI Williams. Computing with infinite networks. Advances in neural information processing systems, pages 295­301, 1997.
[5] Alexander G de G Matthews, Mark Rowland, Jiri Hron, Richard E Turner, and Zoubin Ghahramani. Gaussian process behaviour in wide deep neural networks. arXiv preprint arXiv:1804.11271, 2018.
[6] Roman Novak, Lechao Xiao, Yasaman Bahri, Jaehoon Lee, Greg Yang, Daniel A. Abolafia, Jeffrey Pennington, and Jascha Sohl-Dickstein. Bayesian deep convolutional networks with many channels are Gaussian processes. In International Conference on Learning Representations, 2019. URL https: //openreview.net/forum?id=B1g30j0qF7.
[7] Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior, gradient independence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760, 2019.
[8] Greg Yang and Edward J Hu. Feature learning in infinite-width neural networks. arXiv preprint arXiv:2011.14522, 2020.
[9] Laurence Aitchison. Why bigger is not always better: on finite and infinite neural networks. In Hal Daum´e III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 156­164. PMLR, July 2020. URL http://proceedings.mlr.press/v119/aitchison20a.html.
[10] Andrew Gordon Wilson and Pavel Izmailov. Bayesian deep learning and a probabilistic perspective of generalization. arXiv preprint arXiv:2002.08791, 2020.
[11] Sho Yaida. Non-Gaussian processes and neural networks at finite widths. In Jianfeng Lu and Rachel Ward, editors, Proceedings of The First Mathematical and Scientific Machine Learning Conference, volume 107 of Proceedings of Machine Learning Research, pages 165­192, Princeton University, Princeton, NJ, USA, July 2020. PMLR. URL http://proceedings.mlr.press/v107/yaida20a. html.
[12] James Halverson, Anindita Maiti, and Keegan Stoner. Neural networks and quantum field theory. Machine Learning: Science and Technology, 2021.
[13] Joseph M Antognini. Finite size corrections for neural network Gaussian processes. arXiv preprint arXiv:1908.10030, 2019.
[14] Gadi Naveh, Oded Ben-David, Haim Sompolinsky, and Zohar Ringel. Predicting the outputs of finite networks trained with noisy gradients. arXiv preprint arXiv:2004.01190, 2020.
[15] Qianyi Li and Haim Sompolinsky. Statistical mechanics of deep linear neural networks: The backpropagating renormalization group. arXiv preprint arXiv:2012.04030, 2020.
[16] Jacob A Zavatone-Veth and Cengiz Pehlevan. Exact priors of finite neural networks. arXiv preprint arXiv:2104.11734, 2021.
[17] Ethan Dyer and Guy Gur-Ari. Asymptotics of wide networks from Feynman diagrams. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id= S1gFvANKDS.
11

[18] Kyle Aitken and Guy Gur-Ari. On the asymptotics of wide networks with polynomial activations. arXiv preprint arXiv:2006.06687, 2020.
[19] Florian Wenzel, Kevin Roth, Bastiaan Veeling, Jakub Swiatkowski, Linh Tran, Stephan Mandt, Jasper Snoek, Tim Salimans, Rodolphe Jenatton, and Sebastian Nowozin. How good is the Bayes posterior in deep neural networks really? In Hal Daum´e III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 10248­10259. PMLR, 13­18 Jul 2020. URL http://proceedings.mlr. press/v119/wenzel20a.html.
[20] Vincent Fortuin, Adri`a Garriga-Alonso, Florian Wenzel, Gunnar R¨atsch, Richard Turner, Mark van der Wilk, and Laurence Aitchison. Bayesian neural network priors revisited. arXiv preprint arXiv:2102.06571, 2021.
[21] Pavel Izmailov, Sharad Vikram, Matthew D Hoffman, and Andrew Gordon Wilson. What are Bayesian neural network posteriors really like? arXiv preprint arXiv:2104.14421, 2021.
[22] Arthur Jacot, Franck Gabriel, and Cl´ement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. arXiv preprint arXiv:1806.07572, 2018.
[23] Roger A Horn and Charles R Johnson. Matrix Analysis. Cambridge University Press, 2012.
[24] Leon Isserlis. On a formula for the product-moment coefficient of any order of a normal frequency distribution in any number of variables. Biometrika, 12(1/2):134­139, 1918.
[25] Gian-Carlo Wick. The evaluation of the collision matrix. Physical Review, 80(2):268, 1950.
[26] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, highperformance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´eBuc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/ file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf.
[27] Yann LeCun, Corinna Cortes, and CJ Burges. MNIST handwritten digit database. ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist, 2, 2010.
[28] Andreas Engel and Christian Van den Broeck. Statistical mechanics of learning. Cambridge University Press, 2001.
[29] Peter E Kloeden and Eckhard Platen. Stochastic differential equations. In Numerical Solution of Stochastic Differential Equations, pages 103­160. Springer, 1992.
[30] Stefan Van der Walt, Johannes L Sch¨onberger, Juan Nunez-Iglesias, Fran¸cois Boulogne, Joshua D Warner, Neil Yager, Emmanuelle Gouillart, and Tony Yu. scikit-image: image processing in python. PeerJ, 2:e453, 2014.
12

Supplemental Information

A Preliminary technical results

S1

B Perturbative derivation of Conjecture 1

S2

C The cumulant generating function of learned features for a MLP

S7

D Derivation of the average hidden layer kernels for a deep linear network

S7

E Comparison to the results of Aitchison (2020) and Li & Sompolinsky (2020)

S15

F Predictor statistics and generalization in deep linear networks

S18

G Effects of alternative regularization temperature-dependence

S20

H Average kernels in a deep feedforward linear network with skip connections

S21

I Derivation of the average kernels for a depth-two network

S26

J Numerical methods

S28

A Preliminary technical results
In this appendix, we review useful technical results upon which our calculations rely.

A.1 Isserlis' theorem for Gaussian moments
Let (x1, x2, . . . , xn) be a zero-mean Gaussian random vector. Then, Isserlis' theorem [24] states that

E[x1x2 · · · xn] =

pPn2 (i,j)p cov(xi, xj ) n even

0

n odd,

(A.1)

where the sum is over all pairings p of {1, 2, . . . , n} and the product is over all pairs contained in p. In particular, for n = 4, we have

E[x1x2x3x4] = cov(x1, x2) cov(x3, x4) + cov(x1, x3) cov(x2x4) + cov(x1, x4) cov(x2, x3). In physics, Isserlis' theorem is often known as Wick's probability theorem [25].

(A.2)

A.2 Neumann series for matrix inverses near the identity

The Neumann series is the generalization of the geometric series to bounded linear operators, including square matrices. In particular, let A be a p × p square matrix. Then, we have


(Ip - A)-1 = Ak
k=0

(A.3)

provided that the series converges in the operator norm [23]. We will use this result without concern for rigorous convergence conditions, as we are interested only in asymptotic expansions.

S1

A.3 Series expansion of the log-determinant near the identity

Let A be a p × p square matrix, and let t be a small parameter. Then, we have

log det(Ip + tA) =



(-1)k+1 tr(Ak)tk k

k=1

(A.4)

assuming that the series converges. We will not concern ourselves with rigorous convergence conditions, as we will use this expansion formally.
This result follows from the fact that

k tk

log

det(Ip

+

tA)

=

(-1)k+1(k

-

1)!

tr((Ip

+

tA)-k Ak )

(k = 1, 2, . . .).

(A.5)

The base case k = 1 is given by Jacobi's formula [23]:

Then, using the identity

 t

log

det(Ip

+

tA)

=

tr((Ip

+

tA)-1A).

(A.6)

 t (Ip

+

tA)-1

=

-(Ip

+

tA)-1A(Ip

+

tA)-1

(A.7)

and the fact that A commutes with (Ip + tA)-1, we find that the claim holds by induction. Then, as log det(Ip + tA)|t=0 = 0, we obtain the desired Maclaurin series.

B Perturbative derivation of Conjecture 1
In this appendix, we derive Conjecture 1.

B.1 Integrating out the readout layer
We first show that the readout layer can be integrated out exactly. Our starting point is the partition function Z of the Bayes posterior (4) for the network (1), including a source term:

Z = EW (d) EW exp (-E + SJ) ,

(B.1)

where W denotes all of the parameters except for the readout weight matrix W (d) and expectation is taken with respect to the Gaussian prior. The source term SJ is defined as

d-1 p

SJ =

Jµ()Kµ( )

=1 µ,=1

(B.2)

where the sources J( ) are symmetric matrices. The logarithm of the partition function is the cumulant generating function of the hidden layer kernels, with the average kernels given as

K( )

 log Z =

.

J ( ) J(1)=···=J(d-1)=0

As the source term is independent of W (d), Fubini's theorem yields

(B.3)

Z = EW exp(SJ) EW (d) exp(-E) .

(B.4)

S2

The expectation over W d is a Gaussian integral, hence it is easy to evaluate exactly:

EW (d) exp(-E)  1p
= EW (d) exp - 2 
µ=1



1 
n

W

(d)µ

-

yµ

2


1 = exp -  tr(Y Y )
2

nd
×
j=1

dwj (2d2)n

exp

1 - 2 wj

d-2In

+

 
n





wj

+

 (Y n

)j·wj

= det

In

+

d2  n



-nd/2

× exp

1 2d2 tr Y 2n



In

+

d2 n





-1


Y

1 -  tr(Y Y )
2

,

(B.5) (B.6)
(B.7)
(B.8)

where we abbreviate µ  (xµ; W) and introduce the matrices µj  µ,j and Yµj  yµ,j. Here, we have used the fact that the matrix In + (d2/n)  is invertible at any finite temperature. By the Weinstein­Aronszajn identity [23],

det

In

+

d2  n



= det

Ip

+

d2 n



= det(Ip + d2K(d-1)),

(B.9)

where we introduce the (non-constant) kernel

By the push-through identity [23],

K (d-1)



1 

.

n

(B.10)

1 
n

In

+

d2 n





-1


=

Ip

+

d2 n



-1 1 
n

= (Ip + d2K(d-1))-1K(d-1),

(B.11)

hence, using the cyclic property of the trace,

1 2d2 tr Y 2n



In

+

d2 n





-1


Y

1 -  tr(Y
2

Y)

1 = 2 nd tr

d2(Ip + d2K(d-1))-1K(d-1) - Ip Gyy

=

-

1 2

nd

tr[(Ip

+

 d2 K (d-1) )-1 Gyy ],

where we have defined the normalized Gram matrix of the outputs

(B.12) (B.13)

and noticed that

1

Gyy



YY nd

(B.14)

Ip - d2(Ip + d2K(d-1))-1K(d-1) = (Ip + d2K(d-1))-1.

(B.15)

Therefore, we conclude that

Z = EW exp

SJ

-

1 2 nd tr[(Ip

+

 d2 K (d-1) )-1 Gyy ]

-

1 2 nd

log det(Ip

+

 d2 K (d-1) )

(B.16)

at any width.

S3

B.2 Perturbative expansion

We now consider how this expression behaves in the large-width limit. We assume that this limit is well-defined in the sense that the kernels K( ) tend in probability to their constant NNGP values KG( P) [5­7]. Then, we formally write K( ) as the NNGP kernel plus a correction which is small at large hidden
layer widths:

K( ) = KG( P) +  K( ),

(B.17)

where the parameter  is used to track powers of K( ).
We first treat the term resulting from integrating out the readout layer pertrubatively. We define the constant matrix   Ip + KG(dP-1), which is invertible at any finite temperature. Then, using the series expansion of the log-determinant near the identity and the Neumann series for a matrix inverse (see Appendix A) [23], we have (Ip + K(d-1))-1 = -1 - -1K(d-1)-1 + O(2) and log det(Ip + K(d-1)) = log det() +  tr(-1K(d-1)) + O(2), hence

-

1 2 nd

tr[(Ip

+

 d2 K (d-1) )-1 Gyy ]

-

1 2 nd

log

det(Ip

+

 d2 K (d-1) )

=

1 - 2 nd

tr(-1 Gyy )

-

1 2 nd

log

det()

1 + 2 nd tr

(2-1Gyy-1 - -1) K(d-1)

+ O(2).

(B.18)

We note that this expansion is valid at any temperature, even as   . Recognizing the NNGP partition function

ZGP = det()-nd/2 exp

-

1 2



nd

tr(-1

Gyy

)

,

(B.19)

we thus formally have

Z = ZGP EW exp

SJ

+

1 2 nd

tr(

 K (d-1) )

+

O(2)

at large widths, where we have defined the matrix

  2-1Gyy-1 - -1

(B.20) (B.21)

for brevity. We now also treat the source term perturbatively, which gives

d-1

d-1

d-1

SJ = tr(KG( P) J ( )) +  tr(K( )J ( ))  SJ,GP +  tr(K( )J ( )).

=1

=1

=1

Then, we have

(B.22)

log Z = log ZGP + SJ,GP

+ log EW exp



d-1

tr(J (

)

K(

))

+

1 2 nd

tr( K(d-1))

+

O(2)

.

=1

(B.23)

S4

The log-expectation term is a cumulant generating function, hence it has a formal series expansion in  given by

log EW exp



d-1

tr(J (

)

K(

))

+

1 2 nd

tr(

 K (d-1) )

+

O(2)

=1

= EW

d-1

tr(J (

)

K(

))

+

1 2 nd

tr( K(d-1))

+

O()

=1

+ 1 2 var 2W

d-1

tr(J (

)

K(

))

+

1 2 nd

tr(

 K (d-1) )

+

O()

=1

+ O(3).

(B.24)

It is easy to see that the contribution of the mean term to the average kernel is

 J ( ) EW

d-1

tr(J (

)

K(

))

+

1 2 nd

tr(

 K (d-1) )

+

O()

=1

= EW K( )
J (1)=···=J (d-1)=0

to all orders in . Similarly, the contribution of the variance term is

(B.25)



J(

)

var
W

d-1

tr(J (

)

K(

))

+

1 2 nd

tr( K(d-1))

+

O()

=1

J (1)=···=J (d-1)=0

 = J ( ) EW

d-1
tr[J ( ) (K( ) - EW K( ))]
=1

+

1 2 nd

tr[ (K(d-1)

-

EW K(d-1))]

+

O()

2 J (1)=···=J (d-1)=0

= ndEW {tr[ (K(d-1) - EW K(d-1))](K( ) - EW K( ))} + O().

(B.26)

Therefore, using the fact that

by definition, we conclude that

SJ,GP J( )

= KG( P)
J (1)=···=J (d-1)=0

(B.27)

Kµ( )

=

[KG( P) ]µ

+

EW Kµ( )

+

1 2

2nd

p

 cWov(Kµ( ), K(d-1)) + O(3)

,=1

=

EW Kµ( )

+

1 2 nd

p

 cWov(Kµ( ), K(d-1)) + O(3),

,=1

(B.28) (B.29)

where we have recognized that 2 covW (Kµ( ), K(d-1)) = covW (Kµ( ), K(d-1)). Substituting in the definition of , this yields the expression (8) given in the main text.
From the structure of these expressions, we can see that higher-order terms (in ) will involve higher joint cumulants of the kernel deviations K( ), which can in turn be converted into joint cumulants of the kernels K( ). Therefore, to show that the perturbative expansion yields a valid asymptotic series for K( ) , one would need to show that these joint cumulants themselves have asymptotic series expansions at large width, with leading terms that are successively suppressed by powers of n-1. For MLPs, this follows from the work of Yaida [11], who showed that EW K( ) = KG( P) + O(n-1), covW (Kµ( ), K(d-1)) = O(n-1), and that higher cumulants are O(n-2).

S5

B.3 Example: a deep linear network

In this brief subsection, we provide a self-contained derivation of the behavior of the prior cumulants of the kernels of a deep linear network with no bias terms. This is a special case of Yaida [11]'s results, and provides some intution for his results on general MLPs. In such a network, we have

EW Kµ( ) =

n

·

1 ··

n0

EW

xµ

(W

(1))

=

12

·

·

·

2

xµ · x n0

= [KG( P) ]µ

· · · (W ( ))

W ( ) · · · W (1)x

(B.30) (B.31) (B.32)

at any width, as EW ( )(W ( )) W ( )/n = 2In -1. We now consider the second moments of the kernels. We first note that

EW Kµ( )K(+ ) = 2+ · · · 2+1EW Kµ( )K()

(B.33)

for any   1. By Isserlis' theorem (see Appendix A), we have

EW ( ) Wi(k )Wi(l )Wj(m)Wj(r) = 4ij (kmlr + jrlm) + 4klmr,

hence we have the exact recursion

EW Kµ( )K() = (n

·

1 ··

n0)2

EW

n i,j=1

n -1 k,l,m,r=1

Wi(k )Wi(l

)Wj(m)Wj(r)

× [W ( -1) · · · W (1)xµ]k[W ( -1) · · · W (1)x ]l

× [W ( -1) · · · W (1)x]m[W ( -1) · · · W (1)x]r

=

4EW Kµ( -1)K(-1)

+

1 n

4(EW Kµ( -1)K(-1)

+

EW

Kµ( -1)

K(

-1) 

)

(B.34)
(B.35) (B.36)

with base case

EW Kµ(1)K(1)

=

1 (n1n0)2

n1

n0

EW Wi(k1)Wi(l1)Wj(m1)Wj(r1)xµ,kx,lx,mx,r

i,j=1 k,l,m,r=1

=

14

xµ · x n0

x · x n0

+

1 n1

14

xµ · x x · x + xµ · x x · x

n0

n0

n0

n0

=

[KG(1P) ]µ [KG(1P) ]

+

1 n1

[KG(1P) ]µ[KG(1P) ] + [KG(1P) ]µ[KG(1P) ]

(B.37) (B.38) (B.39)

for the second moments of the kernels at each layer. This recurrence relation is in principle exactly solvable for any finite width, but we are interested only in its leading-order behavior at large widths. In particular, we can read off that

cWov(Kµ( ), K()) = 2+ · · · 2+1

1 n
=1

[KG( P) ]µ[KG( P) ] + [KG( P) ]µ[KG( P) ] + O(n-2). (B.40)

Moreover, one can see by Isserlis' theorem that the third and higher cumulants will be O(n-2).

S6

C The cumulant generating function of learned features for a MLP

In this appendix, we briefly describe the full partition function of the Bayes posterior for an MLP. An MLP f : Rn0  Rnd with d layers, no biases, and parameters  = {W ( )}d=1 can be defined recursively in terms of its layer-wise preactivations h( )  Rn as

h(0) = x,

h(

)

=

1 n

-1

W

(

)

-1(h(

-1))

f = d(h(d)),

( = 1, . . . , d),

(C.1) (C.2) (C.3)

where the activation functions  act elementwise. We focus on networks with linear readout, i.e., d(x) = x, and assume Gaussian priors over the weights:

Wi(j

)


i.i.d.

N (0,

2).

(C.4)

We enforce the definition of the network architecture via Fourier representations of the Dirac distribution, with q(µ ) being the Lagrange multiplier that enforces the definition of the preactivation h(µ ). Then, after integrating out the weights using the fact that the relevant integrals are Gaussian, this allows us to write
the partition function as

Z=

p

d

dh(µ ) dq(µ (2)n

)

exp

S({h(µ )}, {q(µ )})

,

µ=1 =1

(C.5)

where the "effective action" for the preactivations and Lagrange multipliers is

1p S=- 
2

d
h(µd) - yµ 2 +

p
iq(µ ) · h(µ )

µ=1

=1 µ=1

1 -
2

d =1

2 n -1

p
q(µ )
µ,=1

·

q(

)

-1(h(µ -1))

·



-1(h(

-1)).

(C.6)

As described in Appendix B, source terms can be added to the effective action to allow computation of various averages. For deep linear networks, it is convenient to scale the source terms by an overall factor of -1/2, for which we must correct when computing the averages:

1 d-1

SJ

=

- 2

p

Jµ() (h(µ )) ·  (h( )).

=1 µ,=1

(C.7)

For an MLP, our task is therefore to integrate out the preactivations and corresponding Lagrange multipliers. We will do so sequentially from the first layer to the last, keeping terms up to the desired order at each step, akin to the approach of [11]. So long as nd and d are fixed and small relative to the width of the hidden layers, this is a consistent perturbative approach, as noted by Yaida [11].

D Derivation of the average hidden layer kernels for a deep linear network
In this appendix, we provide a self-contained derivation of the average hidden layer kernels of a deep linear network. This derivation relies upon neither the results of Appendix B nor those of Yaida [11].

S7

D.1 General form of the perturbative layer integrals for a deep linear network

In this section, we evaluate the general form of the integrals required to perturbatively marginalize out a given layer of a deep linear network to O(n-1). These integrals are generically of the form

I=

p

dhµ dqµ (2)n2

exp

p

1p

p

iqµ · hµ - 2

Gµ (qµ · q ) + jµ · hµ

µ=1

µ=1

µ,=1

µ=1

11 p

-

2

n2

Aµ (hµ
µ,=1

·

h )

1g p

+

4

n1

Gµ (q
µ,,,=1

·

q)G(q

·

qµ)

11 p

+

2

n1

Bµ (qµ
µ,=1

·

q )

,

(D.1)

where hµ, qµ  Rn2. Here, G is a positive semidefinite matrix, while A and B are symmetric matrices
that need not be positive semidefinite. Furthermore, jµ is some source, while g is a coupling constant. We will first evaluate this integral up to terms of O(n-1 1) for n1 1, assuming that G, A, B, jµ, and g are O(1) functions of n1, and then evaluate it up to terms of O(n-1 1, n-2 1) for n1, n2 1, assuming that G, A, B, jµ, and g are also O(1) functions of n2.
We will proceed by evaluating the integrals for G invertible, and then infer the general case by a
continuity argument. We treat the quartic term perturbatively, and all other terms directly. Writing

1 C  G - B,
n1

the leading term in the integral over qµ is





1

1

(2)n2p/2 det(C)n2/2 exp - 2

p

Cµ-1(hµ · h ) .

µ,=1

(D.2) (D.3)

Multiplying and dividing by this quantity, we can compute the perturbative correction from the quartic

term using the fact that qµ then behaves as a Gaussian random vector of mean q¯µ = i

p =1

Cµ-1h

and covariance Cµ-1In2. Denoting expectation with respect to this distribution as · q and writing

q~µ  qµ - q¯µ, Isserlis' theorem yields

(q · q)(q · qµ) q = ([q~ + q¯] · [q~ + q¯])([q~ + q¯] · [q~µ + q¯µ]) q = (q~ · q~ + q~ · q¯ + q¯ · q~ + q¯ · q¯) × (q~ · q~µ + q~ · q¯µ + q¯ · q~µ + q¯ · q¯µ) q = (q~ · q~)(q~ · q~µ) q + (q~ · q~) q(q¯ · q¯µ) + (q~ · q¯)(q~ · q¯µ) q + (q~ · q¯)(q¯ · q~µ) q + (q¯ · q~)(q~ · q¯µ) q + (q¯ · q~)(q¯ · q~µ) q + (q¯ · q¯) (q~ · q~µ) q + (q¯ · q¯)(q¯ · q¯µ) = n22C-1C-µ1 + n2C-1C-µ1 + n2C-µ1C-1 + n2C-1(q¯ · q¯µ) + C-1(q¯ · q¯µ) + C-µ1(q¯ · q¯) + C-1(q¯ · q¯µ) + C-µ1(q¯ · q¯) + n2(q¯ · q¯)Cµ-1 + (q¯ · q¯)(q¯ · q¯µ).

(D.4) (D.5) (D.6)
(D.7)

S8

Then, the quartic correction to the integral over qµ is proportional to
p
Gµ G (q · q)(q · qµ) q = n2(n2 + 1) tr(GC-1GC-1) + n2 tr(GC-1)2
µ,,,=1
- 2(n2 + 1) tr(GC-1GC-1HC-1) - 2 tr(GC-1) tr(GC-1HC-1) + tr(GC-1HC-1GC-1HC-1),

(D.8)

where we write Hµ  hµ · h. We now must integrate over hµ. The leading term is simply





det(CD)-n2/2 exp  1 2

p

Dµ-1Jµ 

µ,=1

(D.9)

where we have defined

D  C-1 +

1 A.

n2

(D.10)

and Jµ  jµ · j. Multiplying and dividing by this quantity, we can compute the perturbative correction

from the quartic term using the fact that hµ then behaves as a Gaussian random vector of mean h¯µ =

p =1

Dµ-1j

and

covariance

Dµ-1In2 .

We

denote

expectations

with

respect

to

this

distribution

by

· h,

and define h~µ  hµ - h¯µ. Then, we have

Hµ h = hµ · h h = h¯µ · h¯ + n2Dµ-1,

(D.11)

and, by analogy to the corresponding four-point average for qµ,

(h · h)(h · hµ) h = n22D-1D-µ1 + n2D-1D-µ1 + n2D-µ1D-1 + n2D-1(h¯ · h¯µ) + D-1(h¯ · h¯µ) + D-µ1(h¯ · h¯) + D-1(h¯ · h¯µ) + D-µ1(h¯ · h¯) + n2(h¯ · h¯)Dµ-1 + (h¯ · h¯)(h¯ · h¯µ).

(D.12)

Then, the correction to the integral over hµ is proportional to

p
Gµ G (q · q)(q · qµ)
µ,,,=1

= n2(n2 + 1) tr(GC-1GC-1) + n2 tr(GC-1)2
- 2(n2 + 1) tr(GC-1GC-1D-1J D-1C-1) - 2n2(n2 + 1) tr(GC-1GC-1D-1C-1) - 2 tr(GC-1) tr(GC-1D-1J D-1C-1) - 2n2 tr(GC-1) tr(GC-1D-1C-1) + n2(n2 + 1) tr(C-1GC-1D-1C-1GC-1D-1) + n2 tr(C-1GC-1D-1)2 + 2(n2 + 1) tr(C-1GC-1D-1C-1GC-1D-1J D-1) + 2 tr(C-1GC-1D-1) tr(C-1GC-1D-1J D-1) + tr(C-1GC-1D-1J D-1C-1GC-1D-1J D-1),

(D.13)

S9

where we have noted that

tr(GC-1HC-1GC-1HC-1) h

p

=

(C-1GC-1)µ (C-1GC-1) (h · h)(h · hµ) h

µ,,,=1

= n2(n2 + 1) tr(C-1GC-1D-1C-1GC-1D-1) + n2 tr(C-1GC-1D-1)2 + 2(n2 + 1) tr(C-1GC-1D-1C-1GC-1D-1J D-1)

+ 2 tr(C-1GC-1D-1) tr(C-1GC-1D-1J D-1)

+ tr(C-1GC-1D-1J D-1C-1GC-1D-1J D-1)

(D.14) (D.15)

by analogy with the corresponding quartic expectation for qµ. We must now expand our results in n-1 1. The inverses of the matrices C and D have Neumann series

C -1

=

G-1

+

1 G-1BG-1 n1

+

O(n-1 2)

(D.16)

and

D-1 =

C-1 +

1 A

-1

n2

=

G-1

+

1 G-1BG-1 n1

+

1 n2

A

+

O(n-1 2)

-1

= F -1G - 1 F -1BF - n1

+ O(n-1 2)

(D.17) (D.18) (D.19)

where we have defined

1

F



Ip

+

GA n2

(D.20)

and we write F - = (F -1) = (F )-1. Then, using the series expansion of the log-determinant, we find that the logarithm of the leading term expands as

1 2

tr(D-1J )

-

1 2 n2

log det(CD)

=

1 2

tr(F -1GJ )

-

1 2 n2

log det(F )

1 -

1

tr(F -1BF -

1 J) +

1

tr(F -1BA)

2 n1

2 n1

+ O(n-1 2),

(D.21)

while the quartic correction simplifies to

1g p

4

n1

Gµ G
µ,,,=1

(q · q)(q · qµ)

1g = 4 n1 n2(n2 + p + 1)p

+ 1 n2g 4 n1

(n2 + 1) tr(F -2) + tr(F -1)2 - 2(n2 + p + 1) tr(F -1)

1g +
2 n1

(n2 + 1) tr(F -3GJ ) + tr(F -1) tr(F -2GJ ) - (n2 + p + 1) tr(F -2GJ )

1 +

g

tr(F -2GJ F -2GJ )

4 n1

+ O(n-1 2).

(D.22)

S10

Combining these results, we find that the result of integrating out the layer to O(n-1 1) is

log I

=

1 2

tr(F -1GJ )

-

1 2 n2

log det(F )

1 -

1

tr(F -1BF -

1 J) +

1

tr(F -1BA)

2 n1

2 n1

1g + 4 n1 n2(n2 + p + 1)p

+ 1 n2g 4 n1

(n2 + 1) tr(F -2) + tr(F -1)2 - 2(n2 + p + 1) tr(F -1)

1g +
2 n1

(n2 + 1) tr(F -3GJ ) + tr(F -1) tr(F -2GJ ) - (n2 + p + 1) tr(F -2GJ )

1 +

g

tr(F -2GJ F -2GJ )

4 n1

+ O(n-1 2).

(D.23)

As this result is a continuous function of G, as the set of full-rank positive definite matrices is dense in
the space of positive semidefinite matrices, this result holds for all positive-semidefinite G. We now further expand this result in n-2 1. This yields

F

-1

=

Ip

-

1 n2

GA

+

1 n22

GAGA

+

O(n-2 3)

(D.24)

and

log det(F )

=

1 n2

tr(GA)

-

1 2

1 n22

tr(GAGA)

+

O(n-2 3),

(D.25)

hence we find that the logarithm of the leading term yields

1 2

tr(D-1J )

-

1 2 n2

log

det(C D)

=

1 2

tr(GJ )

-

1 2

tr(GA)

+

1 4

1 n2

tr(GAGA)

11

11

- tr(GAGJ) + tr(B(A - J))

2 n2

2 n1

+ O(n-1 2, n-2 2, n-1 1n-2 1).

(D.26)

After some straightforward but tedious algebra, the quartic term reduces to

1g p

4

n1

Gµ G
µ,,,=1

(q · q)(q · qµ)

1g = tr(G(A - J)G(A - J))
4 n1 + O(n-1 2, n-2 2, n-1 1n-2 1).

(D.27)

Combining these results, we find that the result of integrating out the layer is

1

1

11

log I = tr(GJ) - tr(GA) +

1 + n2 g tr(GAGA)

2

2

4 n2

n1

1 -

1 + n2 g

11

11

tr(GAGJ) + tr(B(A - J)) + g tr(GJGJ)

2

n1

2 n1

4 n1

+ O(n-1 2, n-2 2, n-1 1n-2 1).

(D.28)

Again, this result is continuous in G, hence it holds even if G is rank-deficient.

S11

D.2 Perturbative computation of the partition function of a deep linear network

We now apply the results of Appendix D.1 to compute the partition function for a deep linear network to the desired order. Our starting point is the effective action before any of the layers have been integrated out, including a source term:

1p S=- 
2

d
h(µd) - yµ 2 +

p

iq(µ

)

·

h(µ )

-

1 2

p
(12Gxx)µ (q(µ1) · q(1))

µ=1

=1 µ=1

µ,=1

1 d-1 1 -
2n

p
(Jµ() + 2+1q(µ +1) · q( +1))(h(µ ) · h( )).

=1 µ,=1

(D.29)

Applying the results of Appendix D.1 with

G = 12Gxx, jµ = 0, A = J (1) + 22Q(2), B = 0, and
g = 0,

(D.30)

we find that the effective action after integrating out the first layer is

S (1)

=

1 -

p

2

d
h(µd) - yµ 2 +

p

iq(µ )

·

h(µ )

-

1 2

p
(m22Gxx)µ (q(µ2) · q(2))

µ=1

=2 µ=1

µ,=1

1 d-1 1 -
2n

p
(Jµ() + 2+1q(µ +1) · q( +1))(h(µ ) · h( ))

=2 µ,=1

+

1 4

g1 n1

m42

tr(GxxQ(2)GxxQ(2))

+

1 2

g1 n1

m22

tr(GxxJ~1GxxQ(2))

-

1 2

tr(m21GxxJ (1))

+

1 4

g1 n1

m41

tr(GxxJ (1)GxxJ (1))

+ O(n-2),

(D.31)

where we have defined

m1  1, m2  2m1, g1  1, and J~1  m21J (1).

(D.32)

Assuming that the network has more than one hidden layer, if we now again apply the results of Appendix D.1 with

G = m22Gxx, jµ = 0, A = J (2) + 32Q(3), B = g1m22GxxJ~1Gxx, and g = g1,

(D.33)

S12

we find that the effective action after integrating out the first two layers is

S (2)

=

1 -

p

2

d
h(µd) - yµ 2 +

p

iq(µ )

·

h(µ )

-

1 2

p
(m23Gxx)µ (q(µ3) · q(3))

µ=1

=3 µ=1

µ,=1

1 d-1 1 -
2n

p
(Jµ() + 2+1q(µ +1) · q( +1))(h(µ ) · h( ))

=3 µ,=1

+

1 4

g2 n2

m43

tr(GxxQ(3)GxxQ(3))

+

1 2

g2 n2

m23

tr(GxxJ~2GxxQ(3))

-

1 2

tr(m21GxxJ (1))

-

1 2

tr(m22GxxJ (2))

+

1 4

g1 n1

m41

tr(GxxJ (1)GxxJ (1))

+

1 4

g2 n2

m42

tr(GxxJ (2)GxxJ (2))

+

1 2

g1 n1

m22

tr(GxxJ~1GxxJ (2))

+ O(n-2),

(D.34)

where we have defined

m3  3m2,

g2



1

+

n2 n1

g1,

and

J~2



m22J (2)

+

n2 n1

g1 g2

J~1.

(D.35)

Then, by induction, we can see that we can iterate this procedure to integrate out all of the hidden layers, yielding

S (d-1)

=

1 -

p

2

h(µd) - yµ 2 +

p

iq(µd)

·

h(µd)

-

1 2

p
(m2dGxx)µ (q(µd) · q(d))

µ=1

µ=1

µ,=1

+

1 4

gd-1 nd-1

m4d

tr(GxxQ(d)GxxQ(d))

+

1 2

gd-1 nd-1

m2d

tr(GxxJ~d-1GxxQ(d))

-

1 2

d-1

tr(m2GxxJ (

))

=1

+

1 d-1 g 4n

m4 tr(GxxJ (

)GxxJ (

))

=1

+

1 d-2 g 2n

m2+1 tr(GxxJ~ GxxJ (

+1))

=1

+ O(n-2),

(D.36)

S13

where md, gd-1, and J~d-1 are defined by the closed recurrences

m   m -1,

n g  1 + n -1 g -1, and

J~



m2J (

)+

n n -1

g -1 g

J~

-1.

Applying the results of Appendix D.1 one final time with

(D.37) (D.38) (D.39)

G = m2dGxx, jµ = y, A = ndIp, B = gd-1m2dGxxJ~d-1Gxx, and g = gd-1,

(D.40)

we conclude that

log

Z

=

1 - 2 nd

tr(-1 Gyy )

-

1 2 nd

log det()

+ 1 ndgd-1 4 nd-1

(nd + p + 1)p + (nd + 1) tr(-2) + tr(-1)2 - 2(nd + p + 1) tr(-1)

+

1 2

gd-1 nd-1

2ndm2d

(nd + 1) tr(-3GxxGyy) + tr(-1) tr(-2GxxGyy)

- (nd + p + 1) tr(-2GxxGyy)

+

1 4

gd-1 nd-1

4n2dm4d

tr(-2 Gxx Gyy -2 Gxx Gyy )

-

1 2

gd-1 nd-1

ndm2d

tr

2Gxx-1Gyy-1Gxx - Gxx-1Gxx

-

1 2

d-1

tr(m2GxxJ (

))

=1

+

1 d-1 g 4n

m4 tr(GxxJ (

)GxxJ (

))

=1

+

1 d-2 g 2n

m2+1 tr(GxxJ~ GxxJ (

+1))

=1

+ O(n-2),

J~d-1

(D.41)

where we have defined the matrix

  Ip + m2dGxx.

(D.42)

As was the case for the individual layer integrals, a continuity argument implies that this expression can be applied even if Gxx is rank-deficient.

S14

D.3 Computing the average hidden layer kernels of a deep linear network

With the relevant partition function in hand, we can finally compute the average hidden layer kernels. In particular, we can immediately read off that

K( ) = m2Gxx

+

gd-1 nd-1

ndm2d

tr

+ O(n-2),

2Gxx-1Gyy-1Gxx - Gxx-1Gxx

J~d-1 J ( ) J( )=0

(D.43)

hence our only task is to determine how the effective source J~d-1 depends on the source for a given layer. Fortunately, the recurrence relation for the effective source is extremely easy to solve, yielding

Thus, we find that

J~d-1

=

d-1 =1

m2 nd-1 n

g J( gd-1

).

(D.44)

K( )

=

m2Gxx

+

g n

ndm2dm2

2Gxx-1Gyy-1Gxx - Gxx-1Gxx

+ O(n-2).

(D.45)

To obtain the expression listed in the main text, we note that g = 1 + g -1 , n n n -1
hence we have

(D.46)

g

1

=

,

n

n

=1

(D.47)

mirroring the width dependence found by Yaida [11] in his study of the prior of deep linear networks.

E Comparison to the results of Aitchison [9] and Li and Sompolinsky [15]
In this appendix, we compare our results for the average kernels of deep linear networks to those of Aitchison [9] and Li and Sompolinsky [15].

E.1 Comparison to the results of Aitchison [9]

We first show that our result (12) for the low-temperature limit of the average kernels of a deep linear network can be recovered from the results of Aitchison [9]. Working in what corresponds to the zerotemperature limit of our setup, Aitchison derives the following implicit recurrence

0 = -(n +1 - n )(K( ))-1 + n +1(K( ))-1(K( +1))(K( ))-1 - n (K( -1))-1,

(E.1)

for = 1, . . . , d - 1, where the boundary conditions of the recurrence are K(0) = Gxx and K(d) = Gyy. We will self-consistently solve this recurrence relation in the limit n1, . . . , nd-1  , n0, nd = O(1). Concretely, we make the ansatz that the zero-temperature kernels are of the form

K(

)

=

KG( P)

+

1 n

K1(

)

+

O(n-2),

(E.2)

S15

and solve the recurrence relations order-by-order using the resulting Neumann series

(K (

))-1

=

(KG( P) )-1

-

1 n

(KG( P) )-1K1(

)(KG( P) )-1

+

O(n-2).

(E.3)

The leading-order recurrence is simply

0=

1 - n +1 n

(KG( P) )-1

+

n +1 n

(KG( P) )-1

(KG( P+1))(KG( P) )-1

-

(KG( P-1))-1,

(E.4)

with boundary conditions KG(0P) = Gxx and KG(dP) = Gyy. For the last hidden layer, we have n +1/n = nd/nd-1  0, hence the recurrence reduces to

KG(dP-1) = KG(dP-2).

(E.5)

If we iterate this procedure backwards through the network, it is easy to see that the n +1/n -dependent terms at each layer will cancel, leaving

KG(dP-1) = KG(dP-2) = · · · = KG(1P) = Gxx.

(E.6)

We now consider the leading finite-width correction. For the last hidden layer, we obtain

0

=

nd(Gyy

-

Gxx)

-

K1(d-1)

+

nd-1 nd-2

K1(d-2)

(E.7)

after dropping all terms that are of O(n-2) and multiplying on the left and right by Gxx. For the first hidden layer, we have

0 = K1(2) -

1 + n2 n1

K1(1).

(E.8)

Finally, for intermediate hidden layers (i.e., = 2, 3, . . . , d - 2), we have

0 = K1( +1) -

1 + n +1 n

K1(

)

+

n n -1

K1(

-1).

Based on the form of these recurrences, we make the ansatz that the solution is of the form

(E.9)

K1( ) = nda (Gyy - Gxx)

(E.10)

for some sequence a , where we assume that Gyy = Gxx. Then, the recurrence for the last hidden layer is satisfied provided that

ad-1

=

1

+

nd-1 nd-2

ad-2,

(E.11)

those for the intermediate layers if

0 = a +1 -

1 + n +1 n

n

a

+

n

a
-1

-1,

and that for the first hidden layer if

(E.12)

a2 =

1 + n2 n1

a1.

(E.13)

S16

Substituting the expression for ad-1 into the condition resulting from the recurrence relation centered on

ad-2, we find that we must have

ad-2

=

1

+

nd-2 nd-3

ad-3,

(E.14)

hence we can iterate this process backwards to the second hidden layer, yielding

n

a

=

1

+

n

a
-1

-1

(E.15)

for = 2, 3, . . . , d - 1. Then, the condition relating a2 and a1 resulting from the recurrence relation for the first layer implies that we must have a1 = 1. Thus, we recover our zero-temperature result from solving Aitchison's recurrence relations order-by-order.

E.2 Comparison to the results of Li and Sompolinsky [15]

We now show that our result (12) for the low-temperature limit of the average kernels of a deep linear network can be recovered as a limiting case of the result of Li and Sompolinsky [15]. Their result for the zero-temperature kernel in the limit n0, n, p   with n1 = n2 = · · · = nd-1 = n, n0/n  (0, ),   p/n  (0, ), and 1 = · · · = d =  is, in our notation,

-2( +1) K( )  1 - nd n

Gxx

+

1 -2dY n

V

M

V

Y

,

(E.16)

where Y  Rp×nd is the matrix of targets and M  Rnd×nd is a diagonal matrix with non-zero elements

[M

]kk

=

zk-(d-1)

zk zk

- -

1 .
1

Here, the orthogonal matrix V is the matrix of eigenvectors of

(E.17)

1 R = 2pY

G+xxY = V V

,

(E.18)

for G+xx the pseudoinverse of Gxx, and the scalars zk are in turn defined in terms of the eigenvalues kk = k as

1 -  = zk - -2(d-1)zk-(d-1)k;

(E.19)

we note that Li and Sompolinsky [15] use variables uk0 = 2zk. We note that we have inverted the sign of the second term in their equation (43), which appears to be a typo as it is inconsistent with (42).

As we are interested in the limit   0, it is useful to write the implicit equation for zk as

zk = 1 + (-2Lzk-(d-1)k - 1),

(E.20)

hence we expect zk  1 as   0. Thus, we have

[M ]kk  ,

(E.21)

which gives

V M V  Ind .

(E.22)

Using the expansion (1 - nd/n) = 1 - nd /n + O(n-2), we therefore find that

-2( +1) K( )



Gxx

+

nd n

(-2dGyy - Gxx)

(E.23)

in the limit in which nd/n  0 and p/n  0. Therefore, combining this result with that of the previous subsection, our result (12) agrees with those of Aitchison [9] and of Li and Sompolinsky [15] in the

appropriate limit. Whether the full result of Li and Sompolinsky [15] agrees with that of Aitchison [9]

is an interesting question, but is well beyond the scope of our work. We note, however, that Aitchison's

recurrence relations should remain valid in the large-p limit provided that p/n < 1.

S17

F Predictor statistics and generalization in deep linear networks

Though the main focus of our work is on the asymptotics of representation learning, we have also computed the leading finite-width corrections to the predictor statistics for deep linear networks we consider. As discussed in Appendix G, these results motivate our choice of () = 1/ regularization. As the calculation of predictor statistics mirrors our approach in Appendix D and the study of the corresponding corrections to predictor statistics under maximum a posteriori estimation by Yaida [11], we only briefly summarize the results.
In short, we fix a test dataset D^ = {(x^µ, y^µ)}pµ^=1 of p^ examples, and define the Gram matrices

(Gx^x^)µ^^  n-0 1x^µ^ · x^^, (Gy^y^)µ^^  n-d 1y^µ^ · y^^, (Gxx^)µµ^  n-0 1xµ · x^µ^, and (Gyy^)µ^  n-d 1yµ · y^^.

(F.1) (F.2) (F.3) (F.4)

Introducing appropriate source terms to allow us to compute predictor statistics, we then proceed to perturbatively integrate out the hidden layers as before, assuming that the combined input Gram matrix

Gxx Gxx^ Gxx^ Gx^x^

(F.5)

is invertible. Again, the final result can be extended to the case in which this matrix is not invertible by a continuity argument.

F.1 Predictor statistics

With the matrix   Ip + m2dGxx as before, one finds that the mean training set predictor is

f (xµ)

=

m2d

p
(-1Gxx)µ y
=1

+

m2dgd-1 nd-1

p
(M Gxx)µ y
=1

+

O(n-2),

where

M  tr(-1)-2 - (nd + p + 1)-2 + (nd + 1)-3 + nd2m2d-2GxxGyy-2.

Similarly, the covariance between two training set predictions is

cov[fl(xµ),

fr(x )]

=

klm2d[-1Gxx]µ

+

m2dgd-1 nd-1

Clr,µ

+

O(n-2),

for

Clr,µ  (M Gxx)µ lr

p

+ m2d2

(-2Gxx)µ(-2Gxx)y,ly,r

,=1

p

+ m2d2(-2Gxx)µ

(-2 Gxx ) y,l y,r .

,=1

The mean test set predictor is

f (x^µ^)

=

m2d

p
(-1 Gxx^ ) µ^ y
=1

+

m2dgd-1 nd-1

p
(M Gxx^)µ^y
=1

+

O(n-2),

(F.6) (F.7) (F.8)
(F.9) (F.10)

S18

and the covariance of two test set predictions is

where

cov[fl(x^µ^)fr(x^^)] = klm2d(Gx^x^ - m2dGxx^-1Gxx^)µ^^

+

m2dgd-1 nd-1

C^lr,µ^^

+

O(n-2),

C^lr,µ^^  (M^ )µ^^lr

p

+ m2d2

(Gxx^-2)µ^(Gxx^-2)^y,ly,r

,=1

+ m2d2(Gx^x^ - m2dGxx^-1Gxx^)µ^^ (-2Gxx)y,ly,r
,

- 3m4d(Gxx^-2Gxx^)µ^^ (-2Gxx)y,ly,r
,

and

M^  (tr(-1) - p)(Gx^x^ - m2dGxx^-1Gxx^) - (tr(-1) - p)(m2dGxx^-2Gxx^) - (nd + 1)(m2dGxx^-3Gxx^) + 2m2dGxx^-2ndGyy-2Gxx^.

F.2 Thermal bias-variance decompositions of the training and test errors

These results allow us to define thermal bias-variance decompositions of the form

1p E=
2

f (xµ)

- yµ

2 2

+

1 2

p

nd
cov[fk(xµ), fk(xµ)]  Eb + Ev

µ=1

µ=1 k=1

for the training and test errors. For the training error, we have

Eb

=

1 2

nd

tr(-2Gyy

)

-

ndm2dgd-1 nd-1



tr(M

Gxx Gyy -1 )

+

O(n-2)

and

Ev

=

1 2

ndm2d

tr(-1Gxx)

+ ndgd-1m2d nd-1

1 2

tr(M Gxx)

+

1 2

m2d2

tr(-2 Gxx Gyy -2 Gxx )

+

1 2

m2d2

tr(-2Gxx)

tr(-2 Gxx Gyy )

+ O(n-2),

while for the test error, we have

E^ b

=

1 2

nd

2

m4d

tr(Gxx^-1

Gyy

-1Gxx^)

-

nd

m2d

tr(Gxx^-1

Gyy^)

+

1 2 nd tr(Gy^y^)

+ ndgd-1m2d nd-1

2m2d tr(Gxx^-1GyyM Gxx^) -  tr(Gyy^M Gxx^)

+ O(n-2)

S19

(F.11)
(F.12) (F.13) (F.14) (F.15)
(F.16) (F.17)

and

E^ v

=

1 2

ndm2d

tr(Gx^x^

- m2dGxx^-1Gxx^)

+ ndgd-1m2d nd-1

1 2

tr(M^ )

+

1 2

m2d

2

tr(Gxx^ -2 Gyy -2 Gxx^ )

+

1 2

m2d2

tr(Gx^x^

-

m2dGxx^-1Gxx^) tr(-2GxxGyy)

-

1 2

3m4d

tr(Gxx^-2Gxx^) tr(-2GxxGyy)

+ O(n-2);

one can check that these results agree upon taking D^ = D.

(F.18)

F.3 Low-temperature asymptotics of the training and test errors

To gain some intuition for the expressions derived in the previous section, we consider their behavior in the low-temperature limit. As in the main text, we let P be an orthogonal projection matrix onto the image of Gxx. Then, the training error has simple low-temperature behavior, with

Eb

=

1 2 nd

tr((Ip

-

P )Gyy)

+

O(-2, n-2)

(F.19)

and

Ev

=

1 2

nd



-1

rk(Gxx)

+

O(-2, n-2).

(F.20)

The low-temperature behavior of the test error is somewhat more complicated, with

E^ b

=

1 2

nd

tr(Gxx^

G+xx Gyy G+xx Gxx^ )

-

nd

tr(Gxx^G+xx

Gyy^)

+

1 2

nd

tr(Gy^y^)

+

O(-1

,

n-2

)

and

E^ v

=

1 2

ndm2d

tr(G~x^x^

)

+

1 ndgd-1m2d 2 nd-1

m-d 2 tr(G+xxGyy) - rk(Gxx)

tr(G~x^x^) + O(-1, n-2)

(F.21) (F.22)

to leading order, where we have defined the matrix

G~x^x^  Gx^x^ - Gxx^G+xxGxx^.

(F.23)

Importantly, we note that the matrix G~x^x^ vanishes identically in the overdetermined regime (i.e., the
regime in which the system Y = XW is overdetermined, for which Gxx is not invertible), meaning that E^v is O(-1) in that case. In the underdetermined regime, the variance component of the test error does
not vanish at small temperatures. In this regime, we find that to leading order the test error decreases with increasing width if tr(G-xx1Gyy) < m2dp, and increases with increasing width if tr(G-xx1Gyy) > m2dp. This condition is the generalization of that found by Li and Sompolinsky [15] to our asymptotic regime.

G Effects of alternative regularization temperature-dependence
In this appendix, we provide a detailed discussion of the reasoning behind our choice of () = 1/. From a heuristic perspective, this choice means that the ridgeless limit coincides with the low-nose limit. To gain a more detailed understanding of this effect, we characterize how alternative choices affect the zero-temperature limits of the leading-order predictor statistics. In this analysis, we focus on

S20

the underdetermined regime (in which Gxx is invertible), in which the thermal variance of the test set predictions need not vanish. Moreover, it suffices to consider only the GP contributions; though the finite-width corrections from Appendix F can also be incorporated, they do not change the qualitative results. In these statistics, the case of general () is related to () = 1/ by the replacement

m2d



m2d d(

)d

.

(G.1)

Then, if we assume a low-temperature power-law dependence ()   for simplicity, we find that the zero-temperature limits of the training set predictor mean and covariance are



0





lim


f (xµ)

GP

=

m2d yµ

p=1((Ip + m2dGxx)-1Gxx)µ y

 > 1/d - 1  = 1/d - 1  < 1/d - 1

(G.2)

and

lim cov[fk(xµ), fl(x)]GP = 0,


respectively, while those of the test set mean and covariance are



0



lim


f (xµ)

GP

=

 m2d

p=1((Ip + m2dGxx)-1Gxx^)µ^y

 

p =1 (G-xx1 Gxx^ ) µ^ y

 > 1/d - 1  = 1/d - 1  < 1/d - 1

and



0





lim cov[fk(x^µ), fl(x^)]GP


=

kl m2d (G~ x^x^ )µ^^ 



 > -1  = -1  < -1,

(G.3) (G.4) (G.5)

respectively. Therefore, taking () = 1/ yields sensible zero-temperature infinite-width behavior for a linear network of any depth in the underdetermined regime.

H Average kernels in a deep feedforward linear network with skip connections

In this appendix, we show that Conjecture 1 holds perturbatively for a linear feedforward network with arbitrary skip connections. Concretely, we consider a network defined as

h(0) = x
h( ) = -1  n, W ( , )h( )
=0
f = h(d),

= 1, . . . , d

(H.1) (H.2) (H.3)

where  , is positive if layer receives input from an earlier layer < , and zero otherwise.

S21

H.1 Perturbative computation of the partition function

Upon integrating out the weights, we obtain an effective action for the preactivations and the corresponding Lagrange multipliers of

p

pd

S = - (h(µd), yµ) +

iq(µ ) · h(µ )

µ=1

µ=1 =1

1 d-1 1 p -
2n

d

J( ) +

2 , (q(µ ) · q( )) (h(µ ) · h( ))

=1 µ,=1

= +1

1 -
2

d

2,0

p
(Gxx)µ (q(µ ) · q( )).

=1

µ,=1

(H.4)

Applying the result of Appendix D.1 with

G = 12,0Gxx, jµ = 0,
d
A = J (1) + 2 ,1Q( ),
=2
B = 0, and
g = 0,

(H.5)

we find that the effective action after integrating out the first layer is

p

pd

S(1) = - (h(µd), yµ) +

iq(µ ) · h(µ )

µ=1

µ=1 =2

1 d-1 1 p -
2n

d

J( ) +

2 , (q(µ ) · q( )) (h(µ ) · h( ))

=2 µ,=1

= +1

1 -
2

d =2

m2,1

tr(GxxQ(

))

+

1 4

1 n1

d
g,
, =2

,1 tr(GxxQ( )GxxQ(

))

11 +
2 n1

d
tr(GxxJ~ ,1GxxQ( ))
=2

-

1 2

m21,0

tr(GxxJ (1))

+

1 4

1 n1

14,0

tr(GxxJ (1)GxxJ (1))

+ O(n-2),

(H.6)

where we have defined

m2,0  2,0, m2,1  m2,0 + 2,1m21,0 g , ,1  2,12 ,114,0, and J~ ,1  2,114,0J (1),

(H.7) (H.8) (H.9) (H.10)

S22

where , > 1 for all cases but m21,0. Assuming the network has more than one hidden layer, if we now again apply the results of Appendix D.1 with

G = m22,1Gxx,
jµ = 0,
d
A = J (2) + 2 ,2Q( ),
=3 d
B = GxxJ~2,1Gxx + g ,2,1GxxQ( )Gxx, and
=3
g = g2,2,1/m42,1,

(H.11)

we find that the effective action after integrating out the first two layers of the network is

p

pd

S(2) = - (h(µd), yµ) +

iq(µ ) · h(µ )

µ=1

µ=1 =1

1 d-1 1 p -
2n

d

Jµ() +

2 , (q(µ ) · q( )) (h(µ ) · h( ))

=3 µ,=1

= +1

1 -
2

d =3

m2,2

tr(GxxQ(

))

+

1 4

1 n2

d
g,
, =3

,2 tr(GxxQ( )GxxQ(

))

11 +
2 n2

d
tr(GxxJ~ ,2GxxQ( ))
=3

1 -
2

2

m2, -1 tr(GxxJ ( ))

=1

+

1 4

1 n1

m41,0

tr(GxxJ (1)GxxJ (1))

+

1 4

1 n2

+

1 2

1 n1

tr(GxxJ~2,1GxxJ (2))

+ O(n-2),

m42,1

+

n2 n1

g2,2,1

tr(GxxJ (2)GxxJ (2))

(H.12)

where we now define

m2,2  m2,1 + m22,12,2,

g,

,2



m42,1

+

n2 n1

g , ,1 + g2,2,12,22 ,2 + g ,2,12 ,2 + 2,2g2, ,1

,

and

J~ ,2



n2 n1

J~

,1

+

m42,1

+

n2 n1

g2,2,1

2,2J (2)

+

n2 n1

2,2J~2,1

+

n2 g n1

,2,1J (2)

(H.13) (H.14) (H.15)

S23

for , > 2. We can now see that we can repeat this procedure to integrate out all of the hidden layers of the network, yielding an effective action of

p

p

S(d-1) = - (h(µd), yµ) + iq(µd) · h(µd)

µ=1

µ=1

-

1 2

m2d,d-1

tr(GxxQ(d))

+

1 4

1 nd-1

gd,d,d-1

tr(GxxQ(d)GxxQ(d))

+

1 2

1 nd-1

tr(GxxJ~d,d-1GxxQ(d))

-

1 2

d-1

m2, -1

tr(GxxJ ())

 =1

1 d-1 +
4
 =2

1 n

m4, -1

+

1 n -1 g,, -1

tr(GxxJ ()GxxJ ())

+

1 2

d-1  =2

1 n -1

tr(GxxJ~, -1GxxJ ())

+ O(n-2),

(H.16)

where the coupling constants and effective source obey the recurrences

m2,  m2, -1 + m2, -12, ,

g , ,  m4, -12, 2 ,

+ n n -1

g , , -1 + g,, -12, 2 , + g ,, -12 , + 2, g, , -1

,

J~ ,



n n -1

J~

,

-1

+

n n -1

2,

J~,

-1

+

m4, -12,

+

n n -1

g,

,

-1

2,

+

n g n -1

,, -1

J ( )

and

for , >  . Applying the results of Appendix D.1 once more with

G = m2d,d-1Gxx, jµ = yµ, A = ndIp, B = GxxJ~d,d-1Gxx, and g = gd,d,d-1/m4d,d-1,

(H.17) (H.18) (H.19)
(H.20)

S24

we find the source-dependent terms in the logarithm of the partition function are

log Z



1 -
2

1 n1



2

nd

tr(-1 Gxx J~d,d-1 Gxx -1 Gyy )

+

1 2

1 nd-1

nd

tr(-1Gxx

J~d,d-1Gxx

)

-

1 2

d-1

m2, -1

tr(GxxJ ())

 =1

1 d-1 +
4
 =2

1 n

m4, -1

+

1 n -1 g,, -1

tr(GxxJ ()GxxJ ())

+

1 2

d-1  =2

1 n -1

tr(GxxJ~,-1GxxJ ())

+ O(n-2),

(H.21)

where

  Ip + m2d,d-1Gxx.

(H.22)

H.2 Computing the average hidden layer kernels

With the source-dependent terms of the relevant partition function in hand, we can compute the average hidden layer kernels for a feedforward linear network with arbitrary skip connections. We can immediately read off that

K( ) = m2, -1Gxx + nd tr nd-1 + O(n-2),

2Gxx-1Gyy-1Gxx - Gxx-1Gxx

J~d,d-1 J ( ) J( )=0

(H.23)

hence our only task is to compute the derivative of the effective source J~d,d-1 with respect to the source for the -th hidden layer. Singling out the -th layer, we can set all sources except J( ) to zero. Then,
the `earliest' effective source to be non-zero is

J~ , =

m4, -12 ,

+

n g
n -1

,

,

-12 ,

n +g
n -1

, , -1

J( ),

(H.24)

for > , and the recurrence relation for  > is

J~

,

=

n n -1

J~ , -1 + 2 , J~, -1

.

From the form of these recurrences, we can see that

(H.25)

K( )

= m2, -1Gxx

+

nd g~ nd-1

Gxx ( 2 -1 Gyy -1

-

-1)Gxx

+ O(n-2),

(H.26)

where g~ is a layer-dependent scalar. Even without explicitly solving the recurrences to obtain g~ , this shows that Conjecture 1 holds perturbatively for linear networks with arbitrary skip connections. We leave detailed study of these recurrences--and therefore of the precise dependence of the corrections on width, depth, and skip connection structure--as an interesting objective for future work.

S25

I Derivation of the average kernels for a depth-two network

In this appendix, we derive the average feature kernel for a network with a single (possibly nonlinear) hidden layer and a linear readout. This derivation mirrors the perturbative derivation of Conjecture 1 in Appendix B, except for the fact that expansion can be performed directly in powers of the inverse hidden layer width. We consider a network defined as

h(1) = 1 W (1)x n0
h(2) = 2 W (2)(h(1)) n1
f = h(2).

(I.1) (I.2) (I.3)

In this simple setting, we can integrate out all of the fields and Lagrange multipliers in the MLP partition function of Appendix C except the hidden layer preactivations {h(µ1)}, as the integrals over {q(µ1)}, {q(µ2)}, and {h(µ2)} are Gaussian. In particular, if we assume that Gxx and the un-normalized kernel matrix

µ  (h(µ1)) · (h(1))

(I.4)

are invertible, we find that the partition function can be expressed as

log Z = log E[exp()],

(I.5)

where

1   - 2 n2 tr

Ip

+



22 n1



-1
Gyy

1 - 2 n2 log det

Ip

+



22 n1



(I.6)

and expectation is taken with respect to {h(µ1)}, which behave as a collection of Gaussian random variables with mean zero and covariance

cov(h(µ1,j) , h(1,k)) = jk12(Gxx)µ .

(I.7)

As  is continuous in , we can retroactively justify our evaluation of the other integrals for invertible . This result also follows from integrating out the readout weights directly, as shown in Appendix B.
To study representation learning, we add a source term

1

tr(JK) = tr(J)

(I.8)

n1

to the effective action, which simply adds to the source-free form of . We now observe that, applying the law of the unconscious statistician, this result means that log Z is
the cumulant generating function of the random variable , hence we have the formal expansion

1

log Z = E + 2 var() + . . . .

(I.9)

However, this expression alone is not particularly useful, as it will be challenging to compute the required
moments of matrix inverses and log-determinants. We will therefore proceed perturbatively.
We start by noting that, as the hidden unit preactivations for different neurons are independent and identically distributed, the k-th raw moment of  will be O(nk1), while the k-th central moment will be O(n1). Therefore, we would like to set up a perturbative expansion that will involve central moments of . To do so, we define ~   - E, and define the (fixed) matrix





Ip

+



22 n1

E,

(I.10)

S26

such that

Ip

+



22 n1



=



+



22 n1

~ .

(I.11)

Under reasonable conditions on  and Gxx, the matrix  is invertible at any finite temperature, hence we have the formal Neumann series



Ip

+



22 n1



-1

=

-1

-

2

22 -1~ -1 n1

+

3

24 n21

-1~ -1~ -1

+

...

.

(I.12)

Similarly, we have

log det

Ip

+



22 n1



= log det() + log det

Ip

+



22 n1

-1~

=

log

det()

+



22 n1

tr(-1~ )

-

12 2

24 n21

tr(-1~ -1~ )

+

.

.

.

.

(I.13) (I.14)

As    at low temperatures, we expect these series to be well-behaved even in the zero-temperature limit.
We then find that

E

=

1 - 2 n2

tr(-1 Gyy )

-

1 2 n2

log

det()

-

1 2

n23

24 n21

E

tr(-1~ -1~ -1Gyy

)

+

1 4

n2



2

24 n21

E

tr(-1~ -1~ )

1 + n1 tr(JE)

+ O(n-1 2)

(I.15)

which yields

~   - E

=

1 2

n22

22 n1

tr(-1~ -1Gyy)

-

1 2

n2

22 n1

tr(-1~ )

-

1 2

n23

24 n21

tr(-1~ -1~ -1Gyy) - E tr(-1~ -1~ -1Gyy)

+

1 4

n22

24 n21

tr(-1~ -1~ ) - E tr(-1~ -1~ )

+ 1 tr(J~ ) n1

+ O(n-1 2).

(I.16)

Thus, we have

var()

=

1 4

2

24 n21

E

 tr(-1~ -1Gyy) - tr(-1~ )

2

+

1 n1

n2

22 n1

E

tr(J~ )

 tr(-1~ -1Gyy) - tr(-1~ )

+

1 n21

E

tr(J~ )2

+ O(n-1 2),

(I.17)

S27

0

Gxx

20

40

60

80

0 20 40 60 80

0

Gyy

20

40

60

80

0 20 40 60 80

Figure 3: Covariance matrices of inputs and outputs for Figure 1 in main text.

and we can see that the third and higher cumulants are O(n-1 2). We can then immediately read off the average hidden layer kernel as

K

=

 log Z J

=
J =0

1 n1 E

+

1 2 n2

22 n21

E

~ tr (-1Gyy-1 - -1)~

+ O(n-1 2).

(I.18)

Expanding this result in components, we obtain the expression given in the main text. We now note that, depending on the nonlinearity, this result may be continuous in Gxx, and therefore extensible to the non-invertible case via a continuity argument. In particular, as noted in Appendix D, this holds for a linear network.

J Numerical methods

We perform our simulations by sampling network parameters at each times step of the Langevin update 5 after some large burn-in period when the loss function stabilizes around a fixed number. We used Euler-Maruyama method [29] to obtain the discretized Langevin equation:

(t + 1) = --1(t)dt - E(t)dt +  2-1dt,

(J.1)

where   N (0, 1) is a standard Gaussian random variable sampled i.i.d. at each time step and dt is the time step. The first, second and last terms represent the weight decay, the gradient descent update and the stochastic Weiner process, respectively.
We used PyTorch deep learning library [26] to generate the neural networks and trained them according to the discretized Langevin update rule. A typical burn-in time was  4 × 106 iterations and after that the parameters were sampled over  4 × 106 iterations where we chose a learning rate of dt  10-4.
Simulations have been performed on a cluster with NVIDIA Tesla V100 GPU's with 32 GB RAM and a typical simulation run for  1 hr.
For Figure 1, we generated 100 training samples with random inputs of dimension n0 = 10 and structured labels of dimension nd = 10 with the covariance structure Gxx and Gyy shown in Figure 3. The network consisted of 3-hidden layers with linear activations and the inverse temperature was set to  = 1.
For Figure 2, we generated a 4-layer neural network with error function activations and used 100 randomly chosen MNIST digits [27] as the training set. The Langevin sampling had been performed with  = 106. The digits were resized from 28×28 pixels to 14×14 pixels for reducing the input dimensionality to speed up the computations using [30].

S28

