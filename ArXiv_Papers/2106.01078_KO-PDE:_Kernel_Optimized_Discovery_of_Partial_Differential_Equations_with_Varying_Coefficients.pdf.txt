arXiv:2106.01078v1 [cs.LG] 2 Jun 2021

KO-PDE: Kernel Optimized Discovery of Partial Differential Equations with Varying Coefficients
Yingtao Luo1, Qiang Liu2,3, Yuntian Chen4, Wenbo Hu5, Jun Zhu6, 1University of Washington
2Institute of Automation, Chinese Academy of Sciences 3School of Artificial Intelligence, University of Chinese Academy of Sciences
4Frontier Research Center, Peng Cheng Laboratory 5RealAI 6Dept. of Comp. Sci. & Tech., Institute for AI, BNRist Lab,
Bosch-Tsinghua Joint ML Center, Tsinghua University yl3851@uw.edu,qiang.liu@nlpr.ia.ac.cn,
chenyt01@pcl.ac.cn,i@wbhu.net,dcszj@mail.tsinghua.edu.cn
Abstract
Partial differential equations (PDEs) fitting scientific data can represent physical laws with explainable mechanisms for various mathematically-oriented subjects. Most natural dynamics are expressed by PDEs with varying coefficients (PDEsVC), which highlights the importance of PDE discovery. Previous algorithms can discover some simple instances of PDEs-VC but fail in the discovery of PDEs with coefficients of higher complexity, as a result of coefficient estimation inaccuracy. In this paper, we propose KO-PDE, a kernel optimized regression method that incorporates the kernel density estimation of adjacent coefficients to reduce the coefficient estimation error. KO-PDE can discover PDEs-VC on which previous baselines fail and is more robust against inevitable noise in data. In experiments, the PDEs-VC of seven challenging spatiotemporal scientific datasets in fluid dynamics are all discovered by KO-PDE, while the three baselines render false results in most cases. With state-of-the-art performance, KO-PDE sheds light on the automatic description of natural phenomenons using discovered PDEs in the real world.
1 Introduction
Partial differential equations (PDEs) are ubiquitous in many areas, such as physics, engineering, and finance. PDEs are highly concise and understandable expressions of physical mechanisms, which are essential for deepening our understanding of the world and predicting future responses. The discovery of some typical PDEs is considered as milestones of scientific advances, such as the Navier-Stokes equations and Kuramoto­Sivashinsky equations in fluid dynamics, the Maxwell's equations and Helmholtz equations in electrodynamics, and the Schrödinger's equations in quantum mechanics. Nevertheless, there are still a lot of unknown complex phenomena in modern science such as the micro-scale seepage and turbulence governing equations that await PDEs for description.
Traditionally, PDEs such as Maxwell's equations are discovered by: 1) physical laws or principles, such as the conservation laws and the minimum energy principles; and 2) observations. With the large scale volume of physical observation data and the complicated PDE expressions, the PDE discovery is becoming increasingly challenging, which motivates people to take advantage of machine learning methods. Recently, two types of machine learning methods have been introduced to discover PDEs. The first type is the sparse regression methods that perform feature selection from candidate partial
Corresponding Author
Preprint. Under review.

PDEs-CC

PDEs-VC

(a)

(b)

(c)

(d)

Figure 1: Schematic diagram of PDE coefficients. From left to right, the complexity of coefficient fields increases: (a) a constant value 1, (b) -1/x, (c) -1/x + sin y, (d) complex coefficient field described by the Karhunen-Loève expansion of plenty of smooth basis functions [7, 8].

derivative terms and estimate the coefficients [1­3]. The other type of methods use neural operators as substitutions of differential operators for PDE discovery [4­6].
Although the aforementioned works show promise in discovering PDEs with constant coefficients (PDEs-CC) and some simple instances of PDEs with varying coefficients (PDEs-VC), they do not suffice to discover PDEs with varying coefficients of higher complexity. Complex coefficients can reflect physical meanings. For example, the coefficients of the spatial derivative terms in the PDEs of the seepage problem corresponds to permeability, which is a spatial field that is too complex to be expressed explicitly by a simple function [7, 8]. An example of complex coefficient field is shown in Fig. 1(d). The challenge in discovering the complex coefficients lies in overfitting, as shown in the differences between train and test error in Fig. 2. When the variance in some dimensions of the observed data is small, many observations are equivalent in these dimensions (i.e., linearly dependent observations), so the size of the maximal linearly independent set is smaller than the feature number (candidate terms). It results in an undetermined system [1, 9, 10], challenging current models that separately fit data at each coordinate such as PDE-net [4] and SGTR [2].
A more recent baseline A-DLGA [3] adopts a local averaging assumption of coefficients in sparse regression. It merges adjacent data of varying dimensions to increase data samples, which alleviates the overfitting to some extend. However, the merge assumes coefficients within each local grid to be equal, which is contrary to the reality that coefficients are merely locally smoothing but still changing along these dimensions [7, 8]. As shown in the large training error in Fig. 2, it causes underfitting.
In this paper, we present a simple yet effective PDE discovery approach that uses a local kernel estimation for adjacent coefficients in the sparse regression. Kernel smoothing can make full use of the local smoothing property of the coefficients, as shown in the locally smooth coefficient magnitude in Fig. 1(b-d), to improve estimation accuracy. With the accurate coefficient estimation, the sparse regression model selects more proper features, i.e., the proper PDE structure of the candidate terms. We show that this method reduces the expected estimation error over three strong baselines, namely, PDE-net [4], SGTR [2] and A-DLGA [3]. In the experiments, we consider the PDE discovery of PDEs-CC and PDEs-VC of different complexities. The results show that our method can discover PDEs-CC and PDEs-VC of all instances with small coefficient errors, while other baselines yield false results for some complex PDEs-VC with excessively high coefficient errors.
In summary, our contributions are:
· We propose a kernel optimized sparse regression algorithm that imposes a kernel smoothing of adjacent coefficients while estimating coefficients.
· We theoretically prove that it reduces the coefficient estimation error of three baselines and is more robust against noise in data.
· We report experiments on representative datasets with comparison against strong baselines. The results show that our method has lower coefficient estimation error and can discover all the test PDEs with varying coefficients from data while previous baselines cannot.
2

2 Related Work
PDE solver Machine learning is widely leveraged to predict the future response of desired physical fields from data [11, 12]. When we have no prior knowledge about the data, scientists can obtain the future response by solving a partial differential equation (PDE) that describes the natural phenomenon concisely. Machine learning provides an accurate and efficient way to solve PDEs accurately and efficiently [13­15]. Recently, neural networks are used as PDE surrogate models to recover the inputs and outputs of PDEs [16­18]. Deep learning algorithms [19] render giving symbolic solutions of PDEs possible and demonstrate higher accuracy in solving PDEs [20­22]. PDE solvers are widely used in applications. Equation solver [23] is proposed to compute the neural networks. Trainable nonlinear diffusion is proposed using PDEs [24, 25] for fast image restoration and video prediction.
PDE discovery In many cases, scientists can observe the data but have no prior knowledge about the dynamics or the PDE forms. The goal of PDE discovery aims to discover a concise PDE from data to understand the underlying mechanism. For example, if a spatial two-order derivative term appears, we can relate it to the diffusion model in the heat transfer scenario and solve it using known tools. The discovery of PDEs is a prerequisite for the use of plenty of PDE solvers, which highlights its importance. Early trials for equation discovery [26, 27] can discover close-formed functions by comparing differentiation of the experimental data with analytic derivatives of candidate function. PDE discovery is more complex. For PDEs with varying coefficients, we need to determine their PDE structures (the partial derivative terms that form the PDE) and coefficients (the varying coefficients that multiply each partial derivative term in the PDE) at the same time.
Sparse linear regressions [1, 28­30] are proposed to discover PDEs by selecting term candidates and learning the coefficients that fit the data. Gaussian process [31] is leveraged to learn the unknown parameters by introducing regularity, given that the form of nonlinear response is known. Evolutionary algorithm [32] is also proposed to start with an incomplete library and evolve through generations. These algorithms can only discover the PDE structure for PDEs with constant coefficients. Later works start to work on the discovery of PDEs with varying coefficients. PDE-Net [4, 5], Graph-PDE [6] and Differential Spectral Normalization [33] are proposed to use neural blocks to discover the PDEs models. Sequential Group Threshold Regression [2] combines coefficient regression and term selection to find PDEs with varying coefficients. In addition, DLrSR [10] solves the noise problem by separating the clean low-rank data and outliers. Up until now, the current state-of-the-art PDE discovery can discover PDE structures and varying coefficients at the same time for PDEs-CC and part of PDEs-VC, but the PDEs-VC discovery remains a challenge [2, 3, 5] due to the overfitting of the sparse regressions. DL-SN [3] proposes a local averaging assumption of coefficients of PDEs-VC within a grid. It alleviates data linear dependency at the sacrifice of estimation error.

3 Methods

3.1 Problem description

A physical field dataset u(x, y, t) is defined with respect to some input coordinates (x, y, t), where

x  [1, ..., n] and y  [1, ..., m] are spatial coordinates and t  [1, ..., h] is a temporal coordinate. An

example of physical field data is shown in the observation box in the Fig. 3. We consider the task of

discovering two kinds of PDEs: (1) PDEs with constant coefficients, PDEs-CC; and (2) PDEs with

varying coefficients, PDEs-VC. For simplicity, partial derivative terms are denoted by forms like ux

and

uxx,

which

are

equivalent

to

u x

and

2u x2

.

The

time

derivatives

such

as

ut

(i.e.,

u t

)

of

a

PDE

nearly always exist [32], therefore we follow prior works and set ut as the regression label. Let p

denote the number of partial derivative candidate terms considered in the task.

Definition 1 (PDEs with constant coefficients, PDEs-CC). PDEs-CC are the simplest PDEs, whose coefficients i are fixed along all coordinates:

p

ut = Nii, Ni  [u, ux, uy, uxx, ...].

(1)

i=1

3

Definition 2 (PDEs with varying coefficients, PDEs-VC). The coefficients of PDEs-VC are changing in some dimensions, e.g., the spatial dimensions:

p

ut = Nii(x, y), Ni  [u, ux, uy, uxx, ...].

(2)

i=1

A simple example of explicit function is i(x, y) = sin x + cos y and other i(x, y) may be anistropic random fields [9] that are hard to express by explicit functions.
We can see that a PDE has two parts: the set of Ni for i is the PDE structure, while the set of i(x, y) for i is the PDE coefficients. Here, each Ni represents a monomial basis function of u or the combination of two monomial basis functions of u. We consider monomial basis functions only up to the third derivative since higher-order derivatives can be inaccurate due to differential precision [1]. In Eqs.(1-2), the coefficient (x, y) changes w.r.t. spatial coordinates x and y. In this paper, we discuss the case of spatial variations. If the task is to capture variations in the temporal dimension, we can simply replace (x, y) with (t).
Accordingly, the goal of PDE discovery is to determine:
· Structure: which coefficient i is nonzero so that Ni exists in the PDE structure;
· Coefficients: the exact values of all nonzero coefficients at each spatial coordinate.

Naturally, the accuracy of coefficient estimation would affect the correctness of determining which coefficient is nonzero. This coupling motivates us to choose methods that can perform structure learning and coefficient estimation simultaneously (e.g., sparse regression). Moreover, since the simplicity of PDE is important, we are looking for the PDE with fewest terms. For example, ut = ux is simpler than ut = ux + uy under similar data fitting. PDEs with more terms are regarded as redundant PDEs that either correlate to the simplest PDE or are the sub-optimal results.

3.2 Linear Regression w/o Local Averaging Estimation

PDE-net and SGTR use the linear regression to estimate PDE coefficients. Many linear regressions are separately performed for coefficients at different spatial coordinates (x, y):

Y = XW + ,  N (0, 2)  Rh,

(3)

W = argmin

Y - XW

2 2

+



W

2 2

,

(4)

W

where Y = [Y1, Y2, ..., Yh]  Rh denotes ut of all the h samples along the temporal dimension, Xji denotes Ni of the j-th sample in X  Rh×p, W = [W1, W2, ..., Wp]  Rp denotes all the coefficients i of the p candidate terms, simulates the noise in data that is i.i.d. across different samples and spatio-temporal dimensions. Here, Eq.3 and Eq.4 repeat n × m times along the spatial
dimensions x and y to get every W [x,y]. We can also simulate noise by Y = XW × (1 + ).

A more recent work A-DLGA divides spatial coefficients into grids and assumes that the coefficients

within each grid are equal. The local estimation used in A-DLGA shares the same equations as Eq.3

and Eq.4, but it only repeats

n 2q

×

m 2q

times,

where q is the half side length of the local grid.

In

A-DLGA, Y  R4q2h and each X:,i  R4q2h, and the estimated W  in a grid are the same.

3.3 The challenge of coefficient estimation accuracy
As stated above, we aim to have a correct PDE structure and a low coefficient estimation error. The weights of a linear model indicate the importance of each feature (candidate PDE term). Thus, the realization of coefficient estimation accuracy ensures PDE structure correctness. With correctness of PDE structure, we can also in turn estimate coefficients accurately in a positive cycle.
Though the above methods have been effective for some PDEs with simple varying coefficients, they still have difficulty in discovering PDEs with complex coefficients, due to overfitting or underfitting to the data. To illustrate this, we use the mean absolute error (MAE) to measure the error of target fitting (See Definition 5 in Appendix A for details) across training, developing and testing sets, as shown in

4

Useful Samples Linear Dependent Observations

Ground-Truth

=0

 = 10  = 20  = 30  = 40

A-DLGA PDE-net

Figure 2: Schematic diagram of overfitting and underfitting in previous baselines.

Fig. 2. With correctness of PDE structure and accurate coefficient estimation, we shall obtain low target fitting MAE. Data details are shown in Appendix B. As the fitting methods in PDE-net and SGTR are similar, we only illustrate the results of PDE-net. More results can be found in Section 4.2.
As shown in Fig. 2, most observation data are linearly dependent along the temporal dimension since the coefficient fields that determine the observation are not changing along time [1, 9, 10]. Linear dependent observations make the linear equation Y = XW with rank(X)  p an underdetermined system that causes overfitting. Fig. 2 also shows that the estimated coefficients are irregular and cannot match the ground truth, and the test target fitting MAE of PDE-net is 15-300 times larger than the training MAE. The overfitting deviates it from converging to the ground-truth coefficients.
A-DLGA merges adjacent data to increase data samples to alleviate linear dependency and overfitting, but it causes underfitting due to the wrong estimation of the coefficients. Note that the coefficients within each grid are still varying along the dimensions, not the same. However, as shown in Fig. 2, A-DLGA assumes coefficients within each grid to be equal and suffers from this coarse-grained regression. Its training MAE is much higher than PDE-net and its test MAE is about 5-8 times larger than training MAE. The estimated coefficients can roughly reflect the magnitude of the ground-truth coefficients. These show that A-DLGA alleviates overfitting but introduces underfitting.
As shown in Fig. 1 (b-d), coefficients are locally smooth despite the ups and downs in magnitude. A possible improvement is to use locally smooth kernel functions for estimating coefficients in the regression and leverage the kernel that is in line with the local smoothing to consider the potential correlation among adjacent coefficients. We define the local smoothing hypothesis in Definition 3. Local smoothing is very common in fluid dynamics [7, 9], the area in which PDEs are most frequently used. The hypothesis in Definition 3 is true when it applies for each local area of the coefficient fields.
Definition 3 (Local smoothing hypothesis). Given coefficient (x, y), we make two assumptions. (i.) the change of coefficient value along the spatial dimensions is locally consistent, i.e., (x, y) = ((x - v, y - v ) + (x + v, y + v ))/2 for x  [q, n - q], y  [q, m - q], v, v  [1, 2, ..., q]; (ii.) the change of coefficient along spatial dimensions is positively correlated to the distance, i.e., (x, y) - (x - v, y - v ) =  S(x, y) - S(x - v, y - v ) with a small  for x  [q, n - q], y  [q, m - q], v, v  [1, 2, ..., q], where we denote the spatial coordinate vector as S(x, y) and denote the distance between two spatial coordinates (i, j) and (i , j ) as S(i, j) - S(i , j ) .

3.4 Linear Regression with Local Kernel Density Estimation

For j  [j - q, ..., j + q], k  [k - q, ..., k + q], the proposed model is computed as

 = argmin

Y - X

2 2

,

[ij ,k ] =



Ki[j,k]Wi[j,k] Ki[j,k]

.

(5)

Ki[j,k]

=

D[j,k] exp(-
2

),

D[j,k] =

S(i, j) - S(i , j

)

2 2

.

(6)

In Eq.5 and Eq.6, [j , k ] denotes the spatial coordinate of the target coefficient while [j, k] denotes
each spatial coordinate of the adjacent coefficients. q is the half side length of the kernel size. Here, W  Rn×m is the model parameters while   Rn×m is an intermediate tensor.  and q are both hyperparameters. We use Radius Basis Function (RBF) kernel as K. A schematic diagram of PDE
discovery based on this method is shown in Fig. 3.

5

Observation Time Derivative

Candidate Terms , ,  , , , ,......
Filtered Terms , , , 
Coefficient Fitting
Kernel estimation Local coefficient ×
Kernel function

Discovered Terms  = 1 + 2 + 4+5
Ground-truth Terms  = 1 + 2 + 4+ 5
Discovered Coefficients
Ground-truth Coefficients

Figure 3: Schematic diagram of the proposed KO-PDE.

When performing the linear regression at each spatial coordinate, we use the local kernel estimated  of spatially adjacent coefficients as the regression weight to calculate the model output Y = X, instead of using W . Because W at each spatial coordinate participates in the calculation of all the q-step , the learning of W at each spatial coordinate is dependent on q-step adjacent X and Y . Thus the proposed method uses adjacent spatial data to address overfitting. We show the advantages of the proposed method with the following theorems. We give proofs in the Appendix A.
First, we prove in Theorem 1 that using local averaging estimation for coefficient at each spatial coordinate instead of dividing the spatial dimensions into grids can strictly reduce its coefficient estimation error. We rename this improvement as the "sliding window" for clarification. Theorem 1 (Reduction on coefficient error by sliding window). Assume that the coefficients estimated by "sliding window" local averaging estimation are W M and the coefficients estimated by the original local averaging estimation are W , we have |W M - true| < |W  - true|.
We further demonstrate in Theorem 2 that the "sliding window" local kernel estimation reduces the coefficient estimation error of the linear regression with "sliding window" local averaging estimation. Our model is more accurate than "sliding window" A-DLGA, so it is more accurate than A-DLGA. Theorem 2 (Reduction on coefficient error by local kernel). Assume that the coefficients estimated by local kernel density estimation are  and the coefficients estimated by "sliding window" local averaging estimation are W M , we have | - true| < |W M - true|.
Moreover, local kernel estimation reduces the coefficient estimation error caused by noise in the linear regression, which is proved in Theorem 3. This makes the PDE discovery more robust. Theorem 3 (Reduction on coefficient error caused by noise). Assume that the coefficient estimation error is only caused by noise so that  = W = true if  = 0, = 0, then | - true| < |W - true|.

3.5 Iterative One-Out Sparse Regression

We use an iterative one-out regression that filters out one X;,i with the least ||i|| at each iteration of coefficient estimation. The iteration ends when there are only L coefficients left in the regression.
This aims to filter out the most irrelevant i to avoid its intervention in estimating coefficients. If we use A to denote the indexes of reserved coefficients, the iterative one-out regression repeats

2

A = A - [i] if i = min{j} for  j  A,  = argmin Y - X:,ii . (7)



iA

2

Iterative one-out regression is an approximation of sparse group regression that improves the accuracy in determining the nonzero i without the interference of irrelevant terms, as a similar method has pointed out [2]. In all, linear regression with local kernel density estimation and iterative one-out regression together form our KO-PDE. The overall algorithm is expressed in Algorithm 1.

6

Algorithm 1 The KO-PDE Approach.
Require: Target time derivative term ut(x, y, t) and candidate equation terms Ni(x, y, t) w.r.t. x  [1, ..., n], y  [1, ..., m] and t  [1, ..., h]. p that A = [1, 2, ..., p], i  A,  that  N (0, 2), , , q, L.
1: For convenience, denote ut as Y and denote Ni as X:,i. 2: while size(A) > L do 3: Compute  by Eqs.(3-6) using all X:,i and Y with i  A;
4: A = A - [j] if j = min{i} for i  A;
5: end while 6: Compute best by Eqs.(3-6) using all X:,i and Y with i  A. 7: return A, best.

4 Experiments

4.1 Experimental Setting
Setup Our experiments aim to discover PDEs and the corresponding coefficients. We compare our proposed method KO-PDE with PDE-net [4], SGTR [2] and A-DLGA [3] in the experiment. We split the first 30% data in the time axis as the training set, the next 30% data as the validation set, and the last 40% as the test set. We use i to denote Wi and use i to denote i for simplicity. We use an Intel Core i7 CPU. We solve the linear regression problems using the matrix inversion directly, so our results are analytical without error bars. We perform an additional experiment on the robustness by adding random noise to the data in Appendix C and find that KO-PDE performs well under low intensity Gaussian noise. For each model on each dataset, we tune the hyperparameters, i.e. , q and , via grid search, so that it has the lowest target fitting error on the validation (Dev) set. Moreover, we find that the model performs well in a wide range of hyperparameters, with details in Appendix C.

Datasets We experiment on one dataset of PDEs-CC (1-CC) and seven datasets of PDEs-VC (1-VC, 2-VC, ..., 7-VC). We consider 10 candidate terms for 1-CC and (1-2)-VCs and 28 candidate terms for (3-7)-VCs. (1-2)-VCs are the simple PDEs-VC that can be expressed explicitly by a simple function (see 1(c)). (3-7)-VCs are the complex PDEs-VC with the random fields that are hard to express (see 1(d)). The details of the dataset generation are provided in Appendix B.

Table 1: Comparison of the PDE structures discovered by different methods.

Datasets
1-CC
1-VC 2-VC
3-VC 4-VC 5-VC 6-VC 7-VC

SGTR
[] ut = -uux + 0.1000 uxx
[] ut = ^1u + ^2ux [] ut = ^1u + ^2ux
[] ut = ^4uy + ^5uyy + ^10uuy + ^11uuyy [] ut = ^4uy + ^5uyy + ^10uuy + ^12uuyy [] ut = ^4uy + ^5uyy + ^10uuy + ^11uuyy [] ut = ^4uy + ^5uyy + ^10uuy + ^11uuyy [] ut = ^4uy + ^5uyy + ^10uuy + ^11uuyy

A-DLGA [] ut = -uux + 0.1000 uxx
[] ut = ^2ux + ^3uxx [] ut = ^2ux + ^3uxx [] ut = ^4uy + ^6uyyy + ^10uuy + ^11uuyy [] ut = ^1ux + ^2uxx + ^7uux + ^8uuxx [] ut = ^4uy + ^5uyy + ^10uuy + ^11uuyy [] ut = ^1ux + ^4uy + ^7uux + ^10uuy [] ut = ^2uxx + ^4uy + ^8uuxx + ^10uuy

Datasets
1-CC
1-VC 2-VC
3-VC 4-VC 5-VC 6-VC 7-VC

PDE-net
[] ut = -uux + 0.1000 uxx
[] ut = ^1u + ^2ux [] ut = ^1u + ^2ux
[] ut = ^4uy + ^5uyy + ^10uuy + ^11uuyy [] ut = ^4uy + ^5uyy + ^10uuy + ^11uuyy [] ut = ^4uy + ^5uyy + ^10uuy + ^11uuyy [] ut = ^4uy + ^5uyy + ^10uuy + ^11uuyy [] ut = ^4uy + ^5uyy + ^10uuy + ^11uuyy

KO-PDE
[] ut = -uux + 0.1000 uxx
[] ut = ^2ux + ^3uxx [] ut = ^2ux + ^3uxx
[] ut = ^1ux + ^2uxx + ^4uy + ^5uyy [] ut = ^1ux + ^2uxx + ^4uy + ^5uyy [] ut = ^1ux + ^2uxx + ^4uy + ^5uyy [] ut = ^1ux + ^2uxx + ^4uy + ^5uyy [] ut = ^1ux + ^2uxx + ^4uy + ^5uyy

7

Table 2: Mean Errors of the estimated PDE coefficients by different methods.

Datasets
1-CC
1-VC 2-VC
3-VC 4-VC 5-VC 6-VC 7-VC

SGTR
0.0144 (×10-4)
285.62 392.63
5419.9 6817.9 11319 4690.0 2877.7

PDE-net
0.0144 (×10-4)
285.10 397.45
5416.6 6811.4 11359 4687.1 2876.9

A-DLGA
0.0144 (×10-4)
2.9363 12.205
684.48 1936.1 18961 2859.7 2155.9

KO-PDE
0.0144 (×10-4)
0.0880 (×10-4) 2.0871 (×10-4)
0.7089 1.4378 0.1901 0.3550 0.3717

Evaluation Metrics We use three metrics for evaluation:
1. Discovered PDE structure, namely the terms Ni that have non-zero coefficients i; 2. Coefficient error of the discovered PDE which is detailed in Definition 4 in Appendix A; 3. Target fitting error on test set which is detailed in Definition 5 in Appendix A.
To clarify, the coefficient error refers to the error after the the iterative sparse regression as the final results. Target fitting error tests how well the discovered PDEs generalizes to fit the target ut. An ideal model should performs well on all three metrics. Because the ground truth PDE structure and coefficients are the optimal to fit the physical field, its test target fitting error should be lowest. Coefficient error tests if the discovered PDE is in line with the optimal PDE.

4.2 Main Results

We first show the discovered PDE structures of the three methods on all datasets in Table 1. We add a check mark if the discovered PDE structure is identical to the ground truth, otherwise we add a cross mark. As can be seen, all methods can correctly discover the simple PDEs-CC and the three baselines fail to discover the PDEs-VC except A-DLGA discovers the simple PDEs-VC, i.e., (1-2)-VC. Our KO-PDE can discover all the PDE structures correctly.

Table 3: Training and test target fitting errors by different methods.

Datasets 3-VC 4-VC 5-VC 6-VC 7-VC

MAE
Train Dev Test
Train Dev Test
Train Dev Test
Train Dev Test
Train Dev Test

SGTR
1.148(×10-3) 3.907(×10-3) 28.25(×10-3)
2.283(×10-3) 26.87(×10-3) 106.1(×10-3)
0.1340(×10-3) 1.588(×10-3) 9.095(×10-3)
0.3357(×10-3) 11.52(×10-3) 94.47(×10-3)
1.218(×10-3) 6.624(×10-3) 13.63(×10-3)

PDE-net
2.190(×10-3) 10.85(×10-3) 31.80(×10-3)
2.697(×10-3) 42.23(×10-3) 169.2(×10-3)
0.1294(×10-3) 1.563(×10-3) 9.005(×10-3)
0.3014(×10-3) 10.13(×10-3) 85.60(×10-3)
1.506(×10-3) 7.508(×10-3) 15.17(×10-3)

A-DLGA
16.70(×10-3) 47.39(×10-3) 136.8(×10-3)
19.83(×10-3) 43.04(×10-3) 123.8(×10-3)
1.033(×10-3) 2.661(×10-3) 7.872(×10-3)
21.50(×10-3) 37.61(×10-3) 150.0(×10-3)
20.66(×10-3) 42.72(×10-3) 109.6(×10-3)

KO-PDE
3.314(×10-3) 3.919(×10-3) 3.686(×10-3)
3.794(×10-3) 3.794(×10-3) 3.585(×10-3)
0.3306(×10-3) 0.3422(×10-3) 0.3433(×10-3)
1.733(×10-3) 1.729(×10-3) 1.703(×10-3)
1.940(×10-3) 1.984(×10-3) 1.733(×10-3)

In Table 2, we give the mean absolute errors of the estimated coefficients by different methods. The results is consistent with the results in Table 1 that KO-PDE is the only model that not only discovers
8

1

2

4

5





- 
Figure 4: Comparison of estimated and correct nonzero coefficients of 3-VC. Three rows represent the results of Ground Truth, KO-PDE and their residual error, respectively. Four columns represent four i that are nonzero in reality. The colors in each map are individually normalized to reflect coefficient values at each (x,y) spatial coordinate. Approximately, | - ^|/|| = 1%.
the correct PDE structures but also estimates the PDE coefficients accurately for all datasets. In contrast, the coefficient estimation error of the three baseline methods on are all excessively high, showing that they fail to discover the optimal PDE. Moreover, although A-DLGA discovers the PDE structures of (1-2)-VCs correctly, its coefficient errors are much larger than KO-PDE, showing that KO-PDE effectively reduces the coefficient estimation error.
In Table 3, we give the target fitting errors of all methods on all (3-7)-VC. The test target fitting errors of the three baselines are much larger than KO-PDE, reflecting the fact that wrong PDE structure leads to sub-optimal PDE to explain the physics. The test target fitting errors of SGTR and PDE-net are much larger than their training errors, reflecting overfitting. The training fitting errors of A-DLGA are larger than the other three methods, showing that A-DLGA causes underfitting. KO-PDE generalizes well and its test error is the smallest, showing that KO-PDE discovers the optimal PDE successfully.
We visualize the estimated coefficients, ground-truth coefficients, and their residual error of 3-VC in Fig. 4 as an example. The relative error of the coefficient estimation is approximately 1%, which is negligible. This result further validates the superior performance of our KO-PDE. The other results of coefficient estimation on PDEs-VC are visualized in the Appendix D.
5 Conclusion and Future Work
How to discover Partial Differential Equations (PDEs) with varying coefficients (PDEs-VC) from data is an important task for various mathematically-oriented subjects. To address both the overfitting caused by the linear dependency of the observation data and the underfitting of the local averaging estimation in previous baselines, we rationally exploit kernel density estimation that is in line with the local smoothing hypothesis in fluid dynamics when merging adjacent coefficients to increase data samples. The proposed KO-PDE fully considers the coefficient differences correlated with a kernel function of spatial distances. We theoretically prove that it strictly reduces the coefficient estimation error of previous baselines and is also more robust against noise. In experiments, the PDEs-VC of seven scientific datasets in fluid dynamics are all discovered by KO-PDE, while previous baselines yield false results on PDEs with varying coefficients(PDEs-VC). KO-PDE performs well with a wide range of hyperparameters and controllable noise. With the state-of-the-art performance, our method brings hope to discover complex PDEs in the real world to help scientists understand unknown complex phenomena. Because this work is only proposed for scientific discovery purposes, it involves no direct negative social impact at the moment.
9

In the future, how to resist a stronger noise and how to avoid the intervention of correlated terms remain an important issue for real-world PDE discovery. The differential accuracy in calculating the partial derivative candidate terms is another challenge that limits the discovery of PDEs with higher-order derivative terms. In addition, the scope of PDE discovery can be further expanded to the discovery of simultaneous PDEs, PDEs with nested forms, PDEs with plenty of partial derivative terms, and PDEs with statistically random coefficients for a wider variety of problems.
References
[1] Samuel H Rudy, Steven L Brunton, Joshua L Proctor, and J Nathan Kutz. Data-driven discovery of partial differential equations. Science Advances, 3(4):e1602614, 2017.
[2] Samuel Rudy, Alessandro Alla, Steven L Brunton, and J Nathan Kutz. Data-driven identification of parametric partial differential equations. SIAM Journal on Applied Dynamical Systems, 18(2):643­660, 2019.
[3] Hao Xu, Dongxiao Zhang, and Junsheng Zeng. Deep-learning of parametric partial differential equations from sparse and noisy data. Physics of Fluids, 33(3):037132, 2021.
[4] Zichao Long, Yiping Lu, Xianzhong Ma, and Bin Dong. Pde-net: Learning pdes from data. In International Conference on Machine Learning, pages 3208­3216. PMLR, 2018.
[5] Zichao Long, Yiping Lu, and Bin Dong. Pde-net 2.0: Learning pdes from data with a numericsymbolic hybrid deep network. Journal of Computational Physics, 399:108925, 2019.
[6] Valerii Iakovlev, Markus Heinonen, and Harri Lähdesmäki. Learning continuous-time {pde}s from sparse data with graph neural networks. In International Conference on Learning Representations, 2021.
[7] Dongxiao Zhang and Zhiming Lu. An efficient, high-order perturbation approach for flow in random porous media via karhunen­loeve and polynomial expansions. Journal of Computational Physics, 194(2):773­794, 2004.
[8] SP Huang, ST Quek, and KK Phoon. Convergence study of the truncated karhunen­loeve expansion for simulation of stochastic processes. International journal for numerical methods in engineering, 52(9):1029­1043, 2001.
[9] Dongxiao Zhang. Stochastic methods for flow in porous media: coping with uncertainties. Elsevier, 2001.
[10] Jun Li, Gan Sun, Guoshuai Zhao, and H Lehman Li-wei. Robust low-rank discovery of datadriven partial differential equations. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 767­774, 2020.
[11] Jeremy Morton, Freddie D Witherden, Antony Jameson, and Mykel J Kochenderfer. Deep dynamical modeling and control of unsteady fluid flows. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, pages 9278­9288, 2018.
[12] Yunzhu Li, Hao He, Jiajun Wu, Dina Katabi, and Antonio Torralba. Learning compositional koopman operators for model-based control. In International Conference on Learning Representations, 2019.
[13] Xiaoxiao Guo, Wei Li, and Francesco Iorio. Convolutional neural networks for steady flow approximation. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 481­490, 2016.
[14] J Nathan Kutz. Deep learning in fluid dynamics. Journal of Fluid Mechanics, 814:1­4, 2017.
[15] Brandon Amos and J Zico Kolter. Optnet: Differentiable optimization as a layer in neural networks. In International Conference on Machine Learning, pages 136­145. PMLR, 2017.
[16] Justin Sirignano and Konstantinos Spiliopoulos. Dgm: A deep learning algorithm for solving partial differential equations. Journal of computational physics, 375:1339­1364, 2018.
10

[17] Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational Physics, 378:686­707, 2019.
[18] Maziar Raissi, Alireza Yazdani, and George Em Karniadakis. Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations. Science, 367(6481):1026­1030, 2020.
[19] Guillaume Lample and François Charton. Deep learning for symbolic mathematics. In International Conference on Learning Representations, 2019.
[20] Martin Magill, Faisal Qureshi, and Hendrick W de Haan. Neural networks trained to solve differential equations learn general representations. In 32nd Conference on Neural Information Processing Systems, 2018.
[21] Kiwon Um, Robert Brand, Philipp Holl, Nils Thuerey, et al. Solver-in-the-loop: Learning from differentiable physics to interact with iterative pde-solvers. In 34nd Conference on Neural Information Processing Systems, 2020.
[22] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Multipole graph neural operator for parametric partial differential equations. In 34nd Conference on Neural Information Processing Systems, 2020.
[23] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary differential equations. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, pages 6572­6583, 2018.
[24] Yunjin Chen and Thomas Pock. Trainable nonlinear reaction diffusion: A flexible framework for fast and effective image restoration. IEEE transactions on pattern analysis and machine intelligence, 39(6):1256­1272, 2016.
[25] Vincent Le Guen and Nicolas Thome. Disentangling physical dynamics from unknown factors for unsupervised video prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11474­11484, 2020.
[26] Josh Bongard and Hod Lipson. Automated reverse engineering of nonlinear dynamical systems. Proceedings of the National Academy of Sciences, 104(24):9943­9948, 2007.
[27] Michael Schmidt and Hod Lipson. Distilling free-form natural laws from experimental data. Science, 324(5923):81­85, 2009.
[28] Hayden Schaeffer. Learning partial differential equations via data discovery and sparse optimization. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 473(2197):20160446, 2017.
[29] Steven L Brunton, Joshua L Proctor, and J Nathan Kutz. Discovering governing equations from data by sparse identification of nonlinear dynamical systems. Proceedings of the national academy of sciences, 113(15):3932­3937, 2016.
[30] Yohai Bar-Sinai, Stephan Hoyer, Jason Hickey, and Michael P Brenner. Learning data-driven discretizations for partial differential equations. Proceedings of the National Academy of Sciences, 116(31):15344­15349, 2019.
[31] Maziar Raissi and George Em Karniadakis. Hidden physics models: Machine learning of nonlinear partial differential equations. Journal of Computational Physics, 357:125­141, 2018.
[32] Hao Xu, Haibin Chang, and Dongxiao Zhang. Dlga-pde: Discovery of pdes with incomplete candidate library via combination of deep learning and genetic algorithm. Journal of Computational Physics, 418:109584, 2020.
[33] Chi Chiu So, Tsz On Li, Chufang Wu, and Siu Pang Yung. Differential spectral normalization (dsn) for pde discovery. In Proceedings of the AAAI Conference on Artificial Intelligence, 2021.
[34] Nanzhe Wang, Dongxiao Zhang, Haibin Chang, and Heng Li. Deep learning of subsurface flow via theory-guided neural network. Journal of Hydrology, 584:124700, 2020.
11

Appendix

A Proofs to the theorems

We show the proofs to the three theorems presented in the main manuscript.
Theorem 1 (Reduction on coefficient error by sliding window). Assume that the coefficients estimated by "sliding window" local averaging estimation are W M and the coefficients estimated by the original local averaging estimation are W , we have |W M - true| < |W  - true|.

Proof. Assume that the Local Smoothing Hypothesis in Definition 3 applies. We can consider the estimation of coefficient (x, y) for x, y. Assume that the ground-truth coefficients are true and (x, y) - (x - v, y - v ) =  S(x, y) - S(x - v, y - v ) as given by the second assumption in
the Local Smoothing Hypothesis. For W  of a spatial coordinate S(1), the upper bond of estimation error should be the case where S(1) locates at the corner of a local grid with half side length q that

sup(|W  - true|) =

 S(x, y) - S(x - v, y - v ) .

v,v [1,...,2q]

The lower bond is line with the case where S(1) locates at the center of a local grid that

inf (|W  - true|) =

 S(x, y) - S(x - v, y - v ) .

v,v [-q,...,q]

However, "sliding window" local averaging estimation uses adjacent local averaging assumption per coefficient. For W M of a spatial coordinate S(1), both the upper and lower bond should be

sup(|W M - true|) = inf (|W M - true|) =

 S(x, y) - S(x - v, y - v ) .

v,v [-q,...,q]

Therefore, the upper bond coefficient estimation error of "sliding window" is lower. The comparison of the expectation of coefficient estimation error is E(|W M - true|) < E(|W  - true|).

Theorem 2 (Reduction on coefficient error by local kernel). Assume that the coefficients estimated by local kernel density estimation are  and the coefficients estimated by "sliding window" local averaging estimation are W M , we have | - true| < |W M - true|.

Proof. Assume that the Local Smoothing Hypothesis in Definition 3 applies. We can consider the estimation of coefficient (x, y) for x, y. Assume that the ground-truth coefficients are true and (x, y) - (x - v, y - v ) =  S(x, y) - S(x - v, y - v ) as given by the second assumption in
the Local Smoothing Hypothesis. For W M of a spatial coordinate S(1), the estimation error with local window of half side length q is

|W M - true| =

 S(x, y) - S(x - v, y - v ) .

v,v [-q,...,q]

However, local kernel estimation gives higher weights to adjacent coefficients that are closer to the estimated coefficient, which can alleviate the fact that the difference between coefficients grows as
their distances increase. For  of a spatial coordinate S(1), the coefficient estimation error should be

| - true| =



v,v [-q,...,q]

K[x-v,y-v ] S(x, y) - S(x - v, y - v )

K[x-v,y-v ]

.

By definition, K[x-v,y-v ] = exp(-

S(x,y)-S(x-v,y-v ) 2

2
2 ). By the negative correlation between

K and coefficient distance, we can easily show that | - true| < |W M - true|.

Theorem 3 (Reduction on coefficient error caused by noise). Assume that the coefficient estimation
error is only caused by noise so that  = W = true if  = 0, = 0, then | - true| < |W - true| if  = 0,  N (0, 2)  Rh.

12

Proof. Consider  Nh(0, 2) in estimation that W = (XT X + I)XT (Y + ). |W - true| = (XT X + I)XT . For coefficient at a spatial coordinate [j , k ], its estimation is affected by both the noise in its Y and the noises in Y at adjacent coordinates. For simplicity,  denotes true.

For each [ij,k] we can find another [i2j -j,2k -k] that has the same D to [ij ,k ]. According to the local smoothing hypothesis in Definition 3 (ii.), the two coefficients share the same value. We can divide ^ i into multiple pairs denoted by G with each pair sharing the same value. Estimated
coefficients should be

|[ij ,k ] - i[j ,k ]| =

G Ki[j,k](XT X + I)XT i,j Ki[j,k]

2 i=1

 |[ij ,k ] - i[j ,k ]| = |Wi[j ,k ] - i[j ,k ]| = const.

E(|¯|)

E(|¯|)

For

2 i=1

i in each pair, while each

i  N (0, i2) is i.i.d.,

 N (0, i2). When each i is

the

same,

¯

 2

N

(0,

22),

E(|¯|)

=

 2

2 × 2/ < 

2/ = E(| |). Therefore, | - true| <

|W - true|.

Definition 4 (Coefficient Estimation Error). The mean absolute error between the discovered PDE

coefficient

and

ground-truth

coefficient

is

computed

as

1 p

p i=1

|i

-

i|,

where



denotes

true.

Definition 5 (Target Fitting Error). The mean absolute error between the estimated output and label

is

1 nmh

x,y,t

|ut(x, y, t)

-

ut(x, y, t)|

=

1 nmh

x,y,t |ut(x, y, t) -

p i=1

Ni

(x,

y,

t)i

(x,

y)|.

B Data statistics

We consider candidate terms Ni  [u, ux, uxx, uxxx, uy, ..., uux, ..., uyyuyyy] with 28 terms for

(3-7)-VCs. For other datasets, we consider Ni  [u, ux, uxx, uxxx, uux, ..., uxxuxxx] that contains

10 candidate terms in total. The subsurface flows in the field of fluid mechanics with different

coefficients are taken as (3-7)-VCs to perform the experiments. The governing equation for the data

is:

u 

u 

u

Ss t

=

(K(x, y) ) +

x

x

(K(x, y) ).

y

y

where Ss denotes the specific storage; K(x, y) denotes the hydraulic conductivity field; and u denotes the hydraulic head. In our work, u is the physical field the desired PDE describes and K is the coefficient field. This equation is also used by PDE-net [4] but its coefficient field is much simpler. The two varying coefficient fields used in PDE-net are 0.5(cos(y) + x(2 - x) sin(x)) + 0.6 and b(x, y) = 2(cos(y) + sin(x)) + 0.8. The hydraulic conductivity field K(x, y) in the governing equation is set to be heterogeneous to better simulate real situations in practice. The heterogeneous fields are often regarded as random fields with higher complexity following a specific distribution with corresponding covariance [7­9, 34].

In detail, in our paper a two-dimensional transient saturated flow in porous medium is considered. The domain is a square, which is evenly divided into 51 × 51 grid blocks and the length in both directions is 1020 [L], where [L] denotes any consistent length unit. The left and right boundaries are set as constant pressure boundaries and the hydraulic head takes values of Hx=0 = 202 [L] and Hx=1020=200 [L], respectively. Furthermore, the two lateral boundaries are assigned as noflow boundaries. The specific storage is assumed as a constant, taking a value of SS=0.0001 [L-1]. The total simulation time is 10 [T], where [T] denotes any consistent time unit, with each time step being 0.2 [T], resulting in 50 time steps. The initial conditions are Ht=0,x=0 = 202 [L] and Ht=0,x=0 = 200 [L]. The mean and variance of the log hydraulic conductivity are given as 0 and 1, respectively. In addition, the correlation length of the field is 408[L]. The hydraulic conductivity field is parameterized through KLE and 20 terms are retained in the expansion. Therefore, this field is represented by 20 random variables  = 1( ), 2( ), ..., 20( ) in the considered cases. The conductivity field obtained through KLE is shown in Fig. A1(a), which exhibits strong anisotropy. MODFLOW software is adopted to perform the simulations to obtain the dataset, and the data distributions at two time steps are presented in Fig. A1(b) and (c).

13

Figure A1: Color maps of conductivity field hydraulic pressure field.

For (1-2)-VCs, we simply simulate the two-dimensional PDE ut = -uux + 0.1uxx and ut =

-

1 x

ux

+

0.1uxx,

2-VC

is

ut

=

-

1 (x+sin

x)

ux

+

0.1uxx

with

toy

coefficients

-

1 x

and

-

1 (x+sin

x)

that do not have physical meanings or references in the nature. (1-2)-VCs are also generated by

MODFLOW, with the similar complexity of the PDEs-VC used in PDE-net. All the data used are

available in the Supplementary Materials.

C Hyperparameter and Robustness Analysis
In addition to the main experiment and the robustness experiment, we also conduct a hyperparameter analysis to discuss the suitable range of hyperparameters to ensure model performance. We set the window size within [2, 5, 10] and set the  value of the Gaussian kernel within [0.03, 0.1, 0.3, 1] to find out whether KO-PDE is stable over a wide hyperparameter range. The window size win = 2q + 1 means the side length of the squared kernel window we use to update the coefficients. The  value determines the variance of the kernel smoothing. When   0, the kernel approximates a sliding window of local averaging. It should be noted that the sliding window of local averaging is not entirely equivalent to local averaging in grids. The former one estimates each coefficient using the averaged value of adjacent coefficients, while the latter one estimates a grid of coefficients at the same time using their averaged value. When   , the kernel estimation degrades to separate regression at each spatial coordinate. We find that as the value of  decreases, the coefficient error increases. This reflects the gradual approximation to the "sliding window" local averaging estimation since the values of the kernel function at each coordinate are almost the same. Therefore, although a wide range of hyperparameters can all give the correct PDE structure, we can tune the hyperparameters for each specific case to obtain the best performance.
Table A5 shows that win = 5,  = 1 is the best hyperparameter setting for 3-VC in the experiment. We visualize the heat maps of kernel functions win = 10,  = 1; win = 5,  = 1; and win = 2,  = 1 in Fig. A2. These kernel functions that have tiny coefficient estimation error mainly use adjacent coefficients within three steps for smoothing, because the simulation of PDEs-RVC datasets is coarse-grained in spatial discretization. The optimal values of both win and  are determined by the graininess of the local smoothing hypothesis in real practice. If the window is too large, the kernel will no longer be "local" to match the hypothesis. In all, KO-PDE performs well within a wide range of hyperparameters. The hyperparameters we use in the main experiments are win = 10,  = 1. We show the full results of hyperparameter analysis of (3-7)-VCs in the following.

0.30

0.30

0.25

0.25

0.20

0.20

0.15

0.15

0.10

0.10

0.05

0.05

0.00

Figure A2: Heat maps of kernel functions win = 10,  = 1; win = 5,  = 1; and win = 2,  = 3. Due to the coarse-grained dataset, in our case the kernel only considers the 1-step adjacent coefficients.

14

Table A1: PDE structures and coefficients discovered w.r.t. hyperparameters for 3-VC.

Hyperparameters
win = 10,  = 1 win = 10,  = 0.3 win = 10,  = 0.1 win = 10,  = 0.03
win = 5,  = 1 win = 5,  = 0.3 win = 5,  = 0.1 win = 5,  = 0.03 win = 2,  = 1 win = 2,  = 0.3 win = 2,  = 0.1 win = 2,  = 0.03

PDE structure
ut = ^1ux + ^2uxx + ^4uy + d uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy

Coefficient error
0.7090 2.2296 5.2279 8.2493 0.5962 1.4790 2.0122 2.2271 1.7029 2.6926 3.0050 3.1153

Table A2: PDE structures and coefficients discovered w.r.t. hyperparameters for 4-VC.

Hyperparameters
win = 10,  = 1 win = 10,  = 0.3 win = 10,  = 0.1 win = 10,  = 0.03
win = 5,  = 1 win = 5,  = 0.3 win = 5,  = 0.1 win = 5,  = 0.03 win = 2,  = 1 win = 2,  = 0.3 win = 2,  = 0.1 win = 2,  = 0.03

PDE structure
ut = ^1ux + ^2uxx + ^4uy + d uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy

Coefficient error
0.1183 0.3749 0.8757 1.3561 0.5962 1.4790 2.0122 2.2271 1.7029 2.6926 3.0050 3.1153

Table A3: PDE structures and coefficients discovered w.r.t. hyperparameters for 5-VC.

Hyperparameters
win = 10,  = 1 win = 10,  = 0.3 win = 10,  = 0.1 win = 10,  = 0.03
win = 5,  = 1 win = 5,  = 0.3 win = 5,  = 0.1 win = 5,  = 0.03 win = 2,  = 1 win = 2,  = 0.3 win = 2,  = 0.1 win = 2,  = 0.03

PDE structure
ut = ^1ux + ^2uxx + ^4uy + d uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy

Coefficient error
0.0272 0.0874 0.2093 1.3561 0.0206 0.0514 0.0701 0.0776 0.0933 0.1478 0.1650 0.1711

15

Table A4: PDE structures and coefficients discovered w.r.t. hyperparameters for 6-VC.

Hyperparameters
win = 10,  = 1 win = 10,  = 0.3 win = 10,  = 0.1 win = 10,  = 0.03
win = 5,  = 1 win = 5,  = 0.3 win = 5,  = 0.1 win = 5,  = 0.03 win = 2,  = 1 win = 2,  = 0.3 win = 2,  = 0.1 win = 2,  = 0.03

PDE structure
ut = ^1ux + ^2uxx + ^4uy + d uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy

Coefficient error
0.0416 0.1317 0.3103 0.4879 0.0330 0.0823 0.1121 0.1242 0.1046 0.1656 0.1848 0.1916

Table A5: PDE structures and coefficients discovered w.r.t. hyperparameters for 7-VC.

Hyperparameters
win = 10,  = 1 win = 10,  = 0.3 win = 10,  = 0.1 win = 10,  = 0.03
win = 5,  = 1 win = 5,  = 0.3 win = 5,  = 0.1 win = 5,  = 0.03 win = 2,  = 1 win = 2,  = 0.3 win = 2,  = 0.1 win = 2,  = 0.03

PDE structure
ut = ^1ux + ^2uxx + ^4uy + d uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy

Coefficient error
0.0270 0.0871 0.2169 0.3618 0.0304 0.0756 0.1030 0.1140 0.1156 0.1828 0.2039 0.2114

Figure A3: The similar distributions of partial derivative term uxx and uux, respectively.
The main experiments are conducted without simulated noise. However, in real applications, the model can be affected by inevitable noise. We conduct a robustness analysis to discuss the noise intensity that the proposed algorithm can withstand. We set the noise intensity  within [1e-5, 3e-5, 1e-4, 3e-4, 1e-3] for  N (0, 2) in Eq.3 and Eqs.(5-6). The difference in random seeds does not alter the discovered PDE structures, so we report the averaged coefficient error of ten different seeds. We use all the data to perform the noise experiment.
16

We report the discovered PDE structure and coefficient error of the 3-VC dataset in Table A6.  = 3 × 10-4 is the maximum noise intensity that the model can withstand. When  = 10-3, the KO-PDE model optimizes to a very similar PDE structure that uses uux instead of uxx. From the table, we find that the target fitting error of the wrong PDE structure is unequivocally higher than the correct PDE structures under noise. We show in Fig. A3 that these two terms have similar distributions. This shows the difficulty to discover complex PDEs-VC since many candidate terms can be correlated with each other, which intervenes the one-out sparse regression iteration. Our algorithm still performs well under controllable noise, unlike SGTR that quickly fails with the tiniest noise [10]. We show the robustness analysis of all the most complex (3-7)-VCs in the following.

Table A6: PDE structures and coefficients discovered under different noise intensity of 3-VC.

Noise intensity
=0  = 10-5  = 3 × 10-5  = 10-4  = 3 × 10-4  = 10-3

PDE structure
ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^4uy + ^7uux + ^10uuy

Coefficient error
0.7090 0.7090 0.7080 0.7069 0.7039 8817.2

Target fitting error
4.7515 (×10-3) 4.7516 (×10-3) 4.7517 (×10-3) 4.7518 (×10-3) 4.7527 (×10-3) 5.2841 (×10-3)

Table A7: PDE structures and coefficients discovered under different noise intensity of 4-VC.

Noise intensity
=0  = 10-5  = 3 × 10-5  = 10-4  = 3 × 10-4  = 10-3

PDE structure
ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^4uy + ^7uux + ^10uuy

Coefficient error
1.4378 1.4379 1.4381 1.4394 1.4421 77038

Target fitting error
4.2487 (×10-3) 4.2487 (×10-3) 4.2488 (×10-3) 4.2494 (×10-3) 4.2583 (×10-3) 59.331 (×10-3)

Table A8: PDE structures and coefficients discovered under different noise intensity of 5-VC.

Noise intensity
=0  = 10-5  = 3 × 10-5

PDE structure
ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^4uy + ^5uyy + ^10uuy + ^11uuyy

Coefficient error
0.1901 0.1900 19035

Target fitting error
0.3466 (×10-3) 0.3493 (×10-3) 0.6106 (×10-3)

Table A9: PDE structures and coefficients discovered under different noise intensity of 6-VC.

Noise intensity
=0  = 10-5  = 3 × 10-5  = 10-4

PDE structure
ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^4uy + ^5uyy + ^10uuy + ^11uuyy

Coefficient error
0.3550 0.3550 0.3552 3620.3

Target fitting error
2.0807 (×10-3) 2.0807 (×10-3) 2.0808 (×10-3) 2.5067 (×10-3)

17

Table A10: PDE structures and coefficients discovered under different noise intensity of 7-VC.

Noise intensity
=0  = 10-5  = 3 × 10-5  = 10-4  = 3 × 10-4

PDE structure
ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^1ux + ^2uxx + ^4uy + ^5uyy ut = ^2uxx + ^4uy + ^5uyy + ^10uuy

Coefficient error
0.3717 0.3716 0.3715 0.3712 77038

Target fitting error
1.7233 (×10-3) 1.7233 (×10-3) 1.7234 (×10-3) 1.7236 (×10-3) 59.331 (×10-3)

D More Experimental Results
In this section, we visualize the estimated coefficients and compare them with the ground-truth coefficients. The visualized residual errors of the estimated coefficients are 100 times smaller than ground-truths, which shows that the proposed KO-PDE is accurate in coefficiente estimation. The results of 3-VC is shown in Fig. 4 in the main manuscript, so we only show the results of the other six datasets here.
The estimated coefficients of all datasets are all obtained with the same hyperparameters reported in the main experiment in the main manuscript. For the 2D (1-2)-VCs datasets, we use a line of side length 10 that includes 10 adjacent data samples, instead of a square used for 3D datasets. In the following, we show the KO-PDE result of (1-2)-VCs. The residual errors of the estimated coefficients are 105 times smaller than ground-truths.

1-

2

3



2-

2

3



- 
Figure A4: Comparison of estimated and correct nonzero coefficients of 1-VC (left) and 2-VC (right). Three rows represent the results of Ground Truth, KO-PDE and their residual errors, respectively. The two columns in each side represent two i that are nonzero in reality. Each sub-figure shows the value at each (x,) spatial coordinate. The scales in some sub-figures are different.
Since A-DLGA also discovers the correct PDE structure for (1-2)-VCs, we also show the estimated coefficients by A-DLGA in Fig. A5. It can be seen that the estimated coefficients show stepped shapes caused by the local averaging estimation. More importantly, this assumption is not accurate, ignoring the fact that the coefficients are changing all along the varying dimension smoothly.
18

1-

2

3



2-

2

3



- 
Figure A5: Comparison of estimated and correct nonzero coefficients of 1-VC (left) and 2-VC (right). Three rows represent the results of Ground Truth, KO-PDE and their residual errors, respectively. The two columns in each side represent two i that are nonzero in reality. Each sub-figure shows the value at each (x,) spatial coordinate. The scales in some sub-figures are different.

1

2

4

5





- 
Figure A6: Comparison of estimated and correct nonzero coefficients of 4-VC. Three rows represent the results of Ground Truth, KO-PDE and their residual error, respectively. Four columns represent four i that are nonzero in reality. Each sub-figure shows the value at each (x,y) spatial coordinate.
19

1

2

4

5





- 
Figure A7: Comparison of estimated and correct nonzero coefficients of 5-VC. Three rows represent the results of Ground Truth, KO-PDE and their residual error, respectively. Four columns represent four i that are nonzero in reality. Each sub-figure shows the value at each (x,y) spatial coordinate.

1

2

4

5





- 
Figure A8: Comparison of estimated and correct nonzero coefficients of 6-VC. Three rows represent the results of Ground Truth, KO-PDE and their residual error, respectively. Four columns represent four i that are nonzero in reality. Each sub-figure shows the value at each (x,y) spatial coordinate.
20

1

2

4

5





- 
Figure A9: Comparison of estimated and correct nonzero coefficients of 7-VC. Three rows represent the results of Ground Truth, KO-PDE and their residual error, respectively. Four columns represent four i that are nonzero in reality. Each sub-figure shows the value at each (x,y) spatial coordinate.

21

