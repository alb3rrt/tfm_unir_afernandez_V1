
# Improving Compositionality of Neural Networks by Decoding Representations to Inputs

[arXiv](https://arxiv.org/abs/2106.0769), [PDF](https://arxiv.org/pdf/2106.0769.pdf)

## Authors

- Mike Wu
- Noah Goodman
- Stefano Ermon

## Abstract

In traditional software programs, we take for granted how easy it is to debug code by tracing program logic from variables back to input, apply unit tests and assertion statements to block erroneous behavior, and compose programs together. But as the programs we write grow more complex, it becomes hard to apply traditional software to applications like computer vision or natural language. Although deep learning programs have demonstrated strong performance on these applications, they sacrifice many of the functionalities of traditional software programs. In this paper, we work towards bridging the benefits of traditional and deep learning programs by jointly training a generative model to constrain neural network activations to "decode" back to inputs. Doing so enables practitioners to probe and track information encoded in activation(s), apply assertion-like constraints on what information is encoded in an activation, and compose separate neural networks together in a plug-and-play fashion. In our experiments, we demonstrate applications of decodable representations to out-of-distribution detection, adversarial examples, calibration, and fairness -- while matching standard neural networks in accuracy.

## Comments

9 pages content; 2 pages appendix

## Source Code

Official Code



Community Code



## Bibtex

```tex
@misc{wu2021improving,
      title={Improving Compositionality of Neural Networks by Decoding Representations to Inputs}, 
      author={Mike Wu and Noah Goodman and Stefano Ermon},
      year={2021},
      eprint={2106.00769},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```

## Notes

Type your reading notes here...

