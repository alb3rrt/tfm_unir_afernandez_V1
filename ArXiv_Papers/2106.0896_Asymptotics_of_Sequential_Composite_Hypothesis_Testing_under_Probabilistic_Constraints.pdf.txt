arXiv:2106.00896v1 [cs.IT] 2 Jun 2021

1
Asymptotics of Sequential Composite Hypothesis
Testing under Probabilistic Constraints
Jiachun Pan, Yonglong Li, Vincent Y. F. Tan
Department of Electrical and Computer Engineering, National University of Singapore Emails: pan.jiachun@u.nus.edu, elelong@nus.edu.sg, vtan@nus.edu.sg
Abstract
We consider the sequential composite binary hypothesis testing problem in which one of the hypotheses is governed by a single distribution while the other is governed by a family of distributions whose parameters belong to a known set . We would like to design a test to decide which hypothesis is in effect. Under the constraints that the probabilities that the length of the test, a stopping time, exceeds n are bounded by a certain threshold , we obtain certain fundamental limits on the asymptotic behavior of the sequential test as n tends to infinity. Assuming that  is a convex and compact set, we obtain the set of all first-order error exponents for the problem. We also prove a strong converse. Additionally, we obtain the set of second-order error exponents under the assumption that X is a finite alphabet. In the proof of second-order asymptotics, a main technical contribution is the derivation of a central limit-type result for a maximum of an uncountable set of log-likelihood ratios under suitable conditions. This result may be of independent interest. We also show that some important statistical models satisfy the conditions.
Index Terms
Sequential composite hypothesis testing, Error exponents, Second-order asymptotics, Generalized sequential probability ratio test
I. INTRODUCTION
Hypothesis testing is a fundamental problem in information theory and statistics [1]. Usually, one assumes that complete descriptions of the probability distributions of the observations are available under each hypothesis. However, in practice, it is more often the case that each hypothesis represents a family of distributions parametrized by a vector that belongs to an uncertainty set. Problems of this type are known as composite hypothesis testing and it has many applications. For example, in clinical applications, the alternative hypothesis may represent the state of malignant tumor that manifests in different forms that a clinician observes. In classical signal detection problems, even though an exact model of the received signal may be available, it often includes some unknown parameters, such as the amplitude and the phase [2]. These parameters reside in some set that can be assumed to be known.
It is also often the case that observations arrive at the detector in a sequential manner and decisions have to be made on the fly. This paper is precisely concerned with this setting in which the length of the composite hypothesis test is a (random) stopping time  that is required to satisfy what we call the probabilistic constraints. Here, we constrain the probabilities (under both hypotheses) that the length of the stopping time exceeds n to be no larger than some prescribed threshold  (0, 1). We let n tend to infinity to exploit various asymptotic and concentration results.
More specifically, this paper is concerned with a sequential composite hypothesis testing problem in which i.i.d. observations are drawn from the null hypothesis which is represented by a single distribution, or from the alternative hypothesis in which the observations are assumed to be drawn from an unknown distribution residing in a known uncertainty set. Based on these observations, we would like to decide whether to continue to draw observations, if we are not sufficiently confident in making a decision, or to declare that the null or alternative hypothesis is in effect. Since we have to eventually make a decision, we have to place a constraint on the stopping time, i.e., the time that we decide whether the null or alternative is true. We impose the above-mentioned probabilistic constraints. When a decision is made, there are two types of error. The type-I error refers to the event that the decision rule declares that the alternative hypothesis is true when in fact the null hypothesis is true. The type-II error represents the other error scenario in which the alternative hypothesis is true but we declare the null; in this case, the error probability must be maximized over all parameters in the uncertainty set .
A natural question concerns the optimal performance in terms of the tradeoff between the two types of error probabilities--or more precisely, their exponents--as a function of n and the permissible threshold on the probability for the stopping time to exceed n, i.e., . We allow n to grow without bound to exploit various asymptotics and measure concentration theorems. However, motivated by practical considerations in which the length of the test must be finite, we also consider the second-order asymptotic regime. The results we obtain under this regime delineate the backoff from the first-order error exponents and hence provide a more refined picture of the finite-length performance of sequential tests when one of the hypotheses is composite.
This work is partially funded by a Singapore National Research Foundation Fellowship (R-263-000-D02-281). The paper was presented in part at the 2021 International Symposium on Information Theory (ISIT).

2
A. Related works
For the first-order asymptotics (exponents of the two types of error probabilities), there is a vast literature on binary hypothesis testing. In the fixed-length hypothesis testing where the length of the vector of observations is fixed, the NeymanPearson lemma [3] states that the likelihood ratio test is optimal and Chernoff-Stein lemma [4, Theorem 13.1] shows that if we constrain the type-I error to be less than any  (0, 1), the best (maximum) type-II error exponent is the relative entropy D(p0 p1), where p0 and p1 are respectively the distributions under the null and alternative hypotheses respectively. If we require the type-I error exponent to be at least r > 0, i.e., the type-I error probability is upper bounded by exp(-nr), the maximum type-II error exponent is min{D(q p0) : D(q p1)  r} [1]. In this regard, we see that there is a trade-off between two error exponents, i.e., they cannot be simultaneously large. However, in the sequential case where the length of the test sample is a stopping time and its expectation is bounded by n, the trade-off can be eradicated. Wald and Wolfowitz [5] showed that when the expectations of sample length under H0 and H1 are bounded by a common integer n (these are known as the expectation constraints) and n tends to infinity, the set of achievable error exponents is {(E0, E1) : E0  D(p1 p0), E1  D(p0 p1)}. In addition, the corner point (D(p1 p0), D(p0 p1)) is attained by a sequence of sequential probability ratio tests (SPRTs). Lalitha and Javidi [6] considered an interesting setting that interpolates between the fixed-length hypothesis testing and sequential hypothesis testing. They considered the almost-fixed-length hypothesis testing problem, in which the stopping time is allowed to be larger than a prescribed integer n with exponentially small probability exp(-n) for different  > 0. The probabilistic constraints we employ in this paper are analogous to those in [6], but instead of allowing the event that the stopping time to be larger than n to have exponentially small probability, we only require this event to have probability at most  (0, 1), a fixed constant. This allows us to ask questions ranging from strong converses to second-order asymptotics. In [7], Haghifam, Tan, and Khisti considered sequential classification which is similar to sequential hypothesis testing apart from the fact that true distributions are only partially known in the form of training samples.
For the composite hypothesis testing, Zeitouni, Ziv, and Merhav [8] investigated the generalized likelihood ratio test (GLRT) and proposed conditions for asymptotic optimality of the GLRT in the Neyman-Pearson sense. For the sequential case, Lai [9] analyzed different sequential testing problems and obtained a unified asymptotic theory that results in certain generalized sequential likelihood ratio tests to be asymptotically optimal solutions to these problem. Li, Nitinawarat and Veeravalli [10] considered a universal outlier hypothesis testing problem in the fixed-length setting; universality here refers to the fact that the distributions are unknown and have to be estimated on the fly. They then extended their work to the sequential setting [11] but under expectation constraints on the stopping time. The work that is closest to ours is that by Li, Liu, and Ying [12] who provided the first-order asymptotics (error exponent) for sequential composite hypothesis testing under expectation constraints. They showed that the generalized sequential probability ratio test is asymptotically optimal by making use of optimality results of sequential probability ratio tests (SPRTs).
Concerning the second-order asymptotic regime, in fixed-length binary hypothesis testing in which the type-I error is bounded by a fixed constant  (0, 1), Strassen [13] showed that the second-order term can be quantified via the relative entropy variance [14] and the inverse of the Gaussian cumulative distribution function. For the sequential case, Li and Tan [15] recently established the second-order asymptotics of sequential binary hypothesis testing under probabilistic and expectation constraints on the stopping time, showing that the former (resp. latter) set of constraints results in a (1/ n) (resp. (1/n)) backoff from the relative entropies. These are estimates of the costs of operating in the finite-length setting. In this paper, we seek to extend these results to sequential composite hypothesis testing.
B. Main contributions
Our main contributions consist in obtaining the first-order and second-order asymptotics for sequential composite hypothesis testing under the probabilistic constraints, i.e., we constrain the probabilities that the lengths of observations exceed n is no larger than some prescribed  (0, 1).
· First, while Li, Liu, and Ying [12] obtained the first-order asymptotic results (error exponents) under expectation constraints, i.e., the expectation of the stopping time is bounded by n, we obtain the first-order asymptotics under the probabilistic constraints. We show that the corner points of the optimal error exponent regions are identical under both types of constraints.
· Second, Li, Liu, and Ying [12] only proved the generalized sequential probability ratio test is asymptotically optimal by making use of the optimality results of sequential probability ratio test (SPRT). Here we prove a strong converse result, namely that the exponents stay unchanged even if the probability that the stopping time exceeds n is smaller than for all  (0, 1). We do so using information-theoretic techniques and, in particular, change-of-measure (Lemma 2).
· Third, and most importantly, we obtain the second-order asymptotics of the error exponents when we assume that the observations take values on a finite alphabet. A main technical contribution here is that we obtain a new central limit-type result for a maximum of an uncountable set of log-likelihood ratios under suitable conditions (Proposition 2). We contrast our central limit-type result to classical statistical results such as Wilks' theorem [16, Chapter 16].

3

C. Paper Outline
The rest of the paper is structured as follows. In Section II, we formulate the composite sequential hypothesis testing problem precisely and state the probabilistic constraints on the stopping time. In Section III, we list some mild assumptions on the distributions and uncertainty set in order to state and prove our first-order asymptotic results. In Section IV, we consider the second-order asymptotics of the same problem by augmenting to the assumptions stated in Section III. We state a central limittype theorem for the maximum of a set of log-likelihood ratios and our main result concerning the second-order asymptotics. We relegate the more technical calculations (such as proofs of lemmata) to the appendix.

II. PROBLEM FORMULATION
Let {Xi} i=1 be an observed i.i.d. sequence, where each Xi follows a density p with respect to a base measure µ on the alphabet X . We consider the problem of composite hypothesis testing:

H0 : p = p0 and H1 : p  {p :   },

where p0 and p are density functions with respect to µ and p0 / {p :   }. We assume that p0 and p are mutually absolutely continuous for all   . Denote P0 and P as the probability measures associated to p0 and p, respectively. Let F (Xn) be the -algebra generated by Xn = (X1, X2, . . . , Xn). Let  be a stopping time adapted to the filtration {F (Xn)} n=1 and let F be the -algebra associated with  . Let  be a {0, 1}-valued F -measurable function. The pair (,  ) constitutes a sequential hypothesis test, where  is called the decision function and  is the stopping time. When  = 0 (resp.  = 1), the
decision is made in favor of H0 (resp. H1). The type-I and maximal type-II error probabilities are defined as

P1|0(,  ) := P0( = 1) and P0|1(,  ) := sup P( = 0).

In other words, P1|0(,  ) is the error probability that the true density is p0 but  = 1 and P0|1(,  ) is the maximal error probability over all parameters    that the true density is p but the decision made  = 0 based on the observations up to time  .
In this paper, we seek the first-order and second-order asymptotics of exponents of the error probabilities under probabilistic constraints on stopping time  . The probabilistic constraints dictate that, for every error tolerance 0 < < 1, there exists an integer n0( ) such that for all n > n0( ), the stopping time  satisfies

P0( > n) < and sup P( > n) < .

(1)



In the following, all logarithms are natural logarithms, i.e., with respect to base e.

III. FIRST-ORDER ASYMPTOTICS

We say that an exponent pair (E0, E1) is -achievable under the probabilistic constraints if there exists a sequence of sequential hypothesis tests {(n, n)} n=1 that satisfies the probabilistic constraints on the stopping time in (1) and

1

1

1

1

E0



lim inf
n

n

log

P1|0(n,

n)

and

E1



lim inf
n

n

log

P0|1(n,

. n)

The set of all -achievable (E0, E1) is denoted as E (p0, ). For simple (non-composite) binary sequential hypothesis testing under the expectation constraints (i.e., maxi=0,1 EPi [ ]  n), the set of all achievable error exponent pairs, as shown by Wald and Wolfowitz [5] (also see [6], [15]), is

E~ (p0, p1) = {(E0, E1) : E0  D(p1 p0), E1  D(p0 p1)}.

(2)

The corner point (D(p1 p0), D(p0 p1)) can be achieved by a sequence of sequential probability ratio tests [5]. We define the log-likelihood ratio and maximal log-likelihood ratio respectively as

Sn()

:=

n i=1

log

p (Xi) p0(Xi)

and

Sn := sup Sn().


For two positive numbers A and B, we define the stopping time  as

 := inf{n : Sn > A or Sn < -B},

and the decision rule as

 := 0, if S < -B, 1, if S > A.

4

We term the above test given by (,  ) as a generalized sequential probability ratio test (GSPRT) with thresholds A and B. For the above GSPRT, we define type-I error probability and maximal type-II error probability respectively as
P1|0(, ) := P0(S > A) and P0|1(, ) := sup P(S < -B).

We introduce some assumptions on the distributions and . (A1) The parameter set   Rd is convex and compact. (A2) Assume that   D(p p0) and   D(p0 p) are twice continuously differentiable on . For each   , the
solutions to the minimizations min D(p0 p) and min D(p p0) are unique. Their existences are guaranteed by the compactness of  and the continuity of   D(p p0) and   D(p0 p) on . In addition, min D(p0 p) > 0 and min D(p p0) > 0 for some 0 > 0. (A3) Let () = log p(X) - log p0(X) be the log-likelihood ratio. We assume that E[max |()|2] <  and for each   , E0[exp(-t())] <  and E[exp(t())] <  for all t  [-t0, t0] for some t0 > 0. Besides, there exist  > 1 and x0  R such that for all   , and x > x0

P0 max | ()| > x  e-| log x| ,

(3)



where |()| is the 2 norm of the gradient vector ()  Rd We present some examples that satisfy Conditions (A1)­(A3). Example 1 (Gaussian distributions). We consider that

H0 : N (0, 1); H1 : N (, 1),    = [1, 2], 0 / .

Then D(p0 p) = D(p p0) = 2/2. Without loss of generality, we assume 1 > 0. The set  is convex and compact.

Besides,

Condition

(A2)

holds.

For

Condition

(A3),

()

=

X

-

2 2

.

Then

E[max

|()|2]

<



and

E0[exp(-t())]

<

, E[exp(t())] <  for any t  R for all   . In addition, |()| = |X - |, if we set x > 2 - 1 then

P0 max |X - | > x  P0(X < 2 - x) + P0(X > 1 + x)

 2P0(X > x)  2e-x2/2.

Thus, for any  > 1, we can find a sufficiently large x0 such that when x > x0, (3) holds. Example 2 (Exponential distributions). We consider that

H0 : Exp(1); H1 : Exp(),    = [1, 2], 1 / .

Then D(p0

p )

=

log

1 

+(-1)

and

D(p

p0)

=

log



+

1- 

.

Without

loss

of

generality,

we

assume

1

>

1.

The

set



is

convex

and compact. Besides, Condition (A2) holds. For Condition (A3), () = (1 - )X + log . Then E[max |()|2] <  and

for

each





,

E0[exp(-t())]

<



when

t

<

1 -1

,

E [exp(t( ))]

<



when

t

>

 1-

.

In

addition,

|()|

=

|-X +

1 

|

and if we set x > 1/1, then

1

1

P0

max - X + > x





 P0

X > +x 1

 P0 (X > x) = e-x.

Thus, for any  > 1, we can find a sufficiently large x0 such that when x > x0, (3) holds.
Our first main result is Theorem 1 which characterizes the set of first-order error exponents under the probabilistic constraints on the stopping time in (1).
Theorem 1. For fixed 0 < < 1 and if Conditions (A1)­(A3) are satisfied, the set of -achievable pair of error exponents is

E (p0, ) = (E0, E1) : E0  min D(p p0), E1  min D(p0 p) .





Furthermore, the corner point of this set is achieved by an appropriately defined sequence of GSPRTs.

Theorem 1 shows that the -achievable error exponent region is a rectangle. In addition, Theorem 1 shows a strong converse result because the region does not depend on the permissible error probability 0 < < 1.

5

A. Proof of Achievability of Theorem 1

Let 0 and 1 be two positive numbers such that 0  0, min D(p p0) and 1  0, min D(p0 p) . Let (n, n) be the GSPRT with the thresholds An := n(min D(p p0) - 0) and Bn := n(min D(p0 p) - 1). Since Conditions (A1)­(A3) are satisfied, then from [12, Theorem 2.1] we have that

1

1

lim inf
n

n

log

P0(Sn

>

An)



min D(p


p0) - 0,

(4)

1

1

lim inf
n

log n sup P (Sn

< -Bn)

 min D(p0


p ) - 1.

(5)

Next we prove that the two probabilistic constraints are satisfied for the GSPRT (n, n) with thresholds An and Bn. We first introduce the uniform weak law of large numbers (UWLLN) [17, Theorem 6.10].

Lemma 1. Let {Xj} j=1 be a sequence of i.i.d. random vectors, and let    be a nonrandom vector lying in a compact subset   Rd. Moreover, let g(x, ) be a Borel-measurable function on X ×  such that for each x, g(x, ) is continuous
on . Finally, assume that E [max |g(Xj, )|] < . Then for any  > 0,





1n

lim P max

n



n

g(Xj, ) - E[g(X, )]   = 0.

j=1

Let  := inf{k : Sk < -Bn}. We observe that   n, so we have

P0(n > n)  P0( > n) = P0 max Sn()  -Bn .


Because

max


n i=1

log

p (Xi) p0(Xi)

+

n min


D(p0

p)  max


n i=1

log

p (Xi) p0(Xi)

-

nE0

log p(X) p0 (X )

,

we have

P0 max Sn()  -n min D(p0 p) - 1





 P0

max


n i=1

log

p (Xi) p0(Xi)

-

nE0

log p(X) p0 (X )

 n1 .

Then by UWLLN, for 0 < < 1, there exists an n0( ), such that when n > n0( ),

P0

max


n i=1

log

p (Xi) p0(Xi)

-

nE0

log p(X) p0 (X )

 n1 < .

Therefore, P0(n > n)  P0( > n) < . We now prove that sup P(n > n) < . Define  := inf{k : max Sk() > An}. We also have   n. Then for
each 0   and t < 0, we have

P0 (n > n)  P0 ( > n)

 P0

max Sn()  An


 P0 Sn(0)  n(D(p0 p0) - 0)

 e-tn(D(p0 p0)-0)E0 etSn(0) ,

where the last step follows from the Chernoff bound [4]. Since E0 [exp(t(0))] <  for t  [-t0, t0] for some t0 > 0 according to Condition (A3), we can use Taylor's theorem [18] to write

e-t(D(p0 p0)-0)E0 et(0) =

1 - t(D(p0

p0)

-

0)

+

t~21 2

(D(p0

p0) - 0)2

1 + tD(p0

p0)

+

t~22 2

E0 [(0)2]

t2

t3

t4

 1 + t0 + C1

2

- C2

2

+ C3

, 4

(6)

where t~1  (t, 0) and t~2  (t, 0). By the compactness of , the continuity of relative entropies with respect to  and E[max |()|2] <  in Conditions (A1)­(A3), we can find constants C1, C2, C3 that do not depend on  and satisfy the
upper bound in (6). Thus, we can find a t < 0 which does also not depend on  and sufficiently close to 0 such that

sup e-t(D(p p0)-0)E et() < 1.

(7)



6

Therefore, there exists an n1( ) such that when n > n1( ), sup P(n > n) < . We have shown that when n > max{n0( ), n1( )}, the two probabilistic constraints are satisfied. Then together with (4), (5) and the arbitrariness of 0 and
1, we show that any exponent pair (E0, E1) such that E0  min D(p p0) and E1  min D(p0 p) is in E (p0, ).

B. Proof of Strong Converse of Theorem 1 The following lemma is taken from Li and Tan [15].

Lemma 2. Let (,  ) be a sequential hypothesis test such that P0( < ) = 1 and sup P( < ) = 1. Then for any event F  F ,  > 0 and for each 0   we have

P0(F ) - P0 (F )  P0(S (0)  - log ), 1
P0 (F ) -  P0(F )  P0 (S (0)  - log ).

Then we use Lemma 2 to prove the converse part. Let (E0, E1)  E (p0, ) such that min{E0, E1} > 0. Without loss of
generality and by passing to a subsequence if necessary, we assume that there exists a sequence of sequential hypothesis tests {(n, n)} n=1 such that

1

1

E0

=

lim
n

n

log

P1|0(n,

, n)

(8)

1

1

E1

=

lim
n

n

log

P0|1(n,

. n)

Let Ai(n) = {n = i} for i = 0, 1. Then P1|0(n, n) = P0(A1(n)) and P0|1(n, n) = sup P (A0(n)). Using Lemma 2 with the event F = A0(n), for each 0   we have that

1 - P0(A1(n)) - P0 (A0(n))  P0(Sn (0)  - log )  P0(Sn (0)  - log , n  n) + P0(n > n),

which further implies that

log P0 (A0(n))  log 1 - P0(A1(n)) - P0(n > n) - P0(Sn (0)  - log , n  n) / .

(9)

Similarly, for each 0  , we have that

log P0(A1(n))  log  1 - P0 (A0(n)) - P0 (n > n) - P0 (Sn (0)  - log , n  n) .

(10)

Now to bound

P0(Sn (0)  - log , n  n)  P0

max k log p0(Xi)  log 

1kn i=1

p0 (Xi)

we first show that

n i=1

log

p0 (Xi ) p0 (Xi)

n1 is a submartingale. We have

E0

n log p0(Xi)

i=1

p0 (Xi)

F (Xn-1)

=

n-1
log
i=1

p0(Xi) p0 (Xi)

+

E0

log p0(Xn) p0 (Xn)

F (Xn-1)

=

n-1
log
i=1

p0(Xi) p0 (Xi)

+

D(p0

p0 )

(11)

n-1
 log

p0(Xi)

,

i=1

p0 (Xi)

where (11) follows from the fact that {Xi}ni=1 is an i.i.d. sequence. We also know that the composition of a non-decreasing convex function and a submartingale is a submartingale [19]. Applying this fact to the exponential function and using Doob's
maximal inequality [20], we obtain for t > 0,

P0

max k log p0(Xi)  log 

1kn i=1

p0 (Xi)

= P0

max exp t k log p0(Xi)

1kn

i=1

p0 (Xi)

 t

 -tE0

exp

t n log p0(Xi)

i=1

p0 (Xi)

.

7

Let  be an arbitrary positive number and let log  = n (D(p0 p0 ) + ). By the similar analysis as in the derivation of (7), we can find a t > 0 which does not depend on  and sufficiently close to 0 such that

sup e-t(D(p p0)+)E0 e-t() < 1.


Then we have that

lim
n

P0

max k log p0(Xi)  log 

1kn i=1

p0 (Xi)

= 0.

(12)

When we set - log  = n(D(p0 p0) + ) in (10), using similar arguments as in the derivation of (12), we obtain

lim
n

P0

max Sk(0)  - log 
1kn

= 0.

From (9) we have that

1

-

n

sup


log

P (A0(n))



min(D(p0


p) + )

1 - log
n

1 - P1|0(n, n) -

- sup P0


max k log p0(Xi)  log 

1kn i=1

p (Xi)

.

From (8) it follows that limn P1|0(n, n) = 0, which together with (12) implies that

1

E1

=

lim
n

- n

P0|1(n,

n)



min


D(p0

p) + .

Similarly, we also obtain

1

E0

=

lim - n n

log

P1|0(n, n)



min


D(p

p0) + .

Due to the arbitrariness of , letting   0+, we have that E0  min D(p p0) and E1  min D(p0 p), completing the proof of the strong converse as desired.

IV. SECOND-ORDER ASYMPTOTICS
In the previous section, we considered the (first-order) error exponents of the sequential composite hypothesis testing problem under probabilistic constraints. While the result (Theorem 1) is conclusive, there is often substantial motivation [14] to consider higher-order asymptotics due to finite-length considerations. To wit, the probabilistic bound observation length of the sequence n might be short and thus the exponents derived in the previous section will be overly optimistic. In this section, we quantify the backoff from the optimal first-order exponents by examining the second-order asymptotics. To do so, we make a set of somewhat more stringent conditions on the distributions and the uncertainty set . We first assume that the alphabet of the observations is the finite set X = {1, 2, . . . , d}. Let PX be the set of probability mass functions with alphabet X . In other words, PX is the probability simplex given by
d
PX := (q(1), q(2), . . . , q(d)) : q(i) = 1, q(i)  0,  i  X .
i=1
Similarly, define the open probability simplex
d
PX+ := (q(1), q(2), . . . , q(d)) : q(i) = 1, q(i) > 0,  i  X .
i=1
Under hypothesis H0, the underlying probability mass function is given by {p0(i)}di=1 and we assume that p0(i) > 0 for all i  X . Under hypothesis H1, the underlying probability mass function belongs to the set   PX . For any q~  PX and positive constant , let B(q~, ) := {q  PX : |q(i) - q~(i)| < ,  i  X } be the open -neighborhood of the point q~. Let  be such that D(p0 p ) = min D(p0 p ). See Fig. 1 for an illustration of this projection.

8

A. Other Assumptions and Preliminary Results

We assume that , which contains multinomial distributions on X , satisfies the following conditions:

(A1') The set  is equal to { = {i}di=1 : F ()  0}  PX for some piece-wise smooth convex function F : PX  R. (A2') There exists a fixed constant c0 > 0 such that miniX i  c0 for all   .
(A3') The function F is smooth on B( , ) for some  > 0.

The key tool used in the derivation of the second-order terms is a central limit-type result for max

n k=1

log

, p (Xk)
p0 (Xk )

the

maximum of log-likelihood ratios of the observations over . To simplify this quantity, we define the empirical distribution or

type [21, Chapter 11] of Xn as Q(i; Xn) =

n k=1

1{Xk

=

i}/n,

for

i

=

1, 2, .

.

.,

d.

In

the

following,

for

the

sake

of

brevity,

we often suppress the dependence on the sequence Xn and write Q(i) in place of Q(i; Xn), but we note that Q is a random

distribution induced by the observations Xn. Since X is a finite set, we have

Sn

=

n
max
  k=1

log

p (Xk) p0 (Xk )

=

max
 

d i=1

n k=1

1{Xk

=

i}

log

i p0(i)

d
= n max Q(i) log

i

.

(13)

  i=1

p0(i)

 The key in obtaining the central limit-type result for the sequence of random variables {Sn/ n}nN is to solve the optimization

problem in (13), or more precisely, to understand the properties of the optimizer to (13). Now we study the following

optimization problem for a generic q  PX :

min d q(i) log p0(i)

 i=1

i

d

s.t. i = 1,

(14)

i=1

F ()  0.

Let ~ be an optimizer to the optimization problem (14). The properties of ~ are provided in the following three lemmas.

Lemma 3. If q  PX+ and q  , then the optimizer ~ is unique.

The existence and uniqueness of the optimizer of the optimization problem (14) follows from the strictly convexity of the

function  

d i=1

q(i)

log

p0 (i) i

on

the

compact

convex

(uncertainty)

set

.

As the optimizer ~ is unique, we can define the function

g(q) = (g1(q), . . . , gd(q)) =: ~.

For the sake of convenience in what follows, define

f (q) := d q(i) log p0(i) .

(15)

i=1

gi(q)

Some key properties of g(q) are provided in the following lemma.

Lemma 4. If q  PX+ and q  , then the following properties of the optimizer ~ = g(q) hold. (i) The function g(q) is continuous on PX+ \ ; (ii) There exists ^ > 0 such that for q  B(p0, ^), F is differentiable at ~;
(iii) For q  B(p0, ^), the optimizer ~ is such that F (~) = 0 (i.e., ~ is on the boundary of the uncertainty set);
(iv) For q  B(p0, ^), there exists a symbol j  X such that

F (~) d F (~)

j

- ~i
i=1

i

= 0;

(16)

(v) For q  B(p0, ^), i  X and i = j (j  X is the symbol that satisfies (16) in Part (iv) above),

q(i) = ~i + ~j

(q(j) - ~j)~i

F (~) j

-

d k=1

~k

F (~) k

F (~) d F (~)

i

- ~k
k=1

k

.

(17)

9

From the continuity of g(q), it follows that g(p0) =  . Without loss of generality, we assume

F ( ) d F ( )

1

- i
i=1

i

= 0.

Then there exists 0 < ¯ < ^ such that for q  B(p0, ¯), the following equation holds

F (~) d F (~)

~1

- ~i
i=1

~i

= 0.

Then for q  B(p0, ¯), the Jacobian of (q(2), . . . , q(d)) with respect to (~2, . . . , ~d) is



q(2) ~2

q(2) ~3

···

q(2) 
~d





  J(q) =    

q(3) ~2
...

q(3) ~3
...

··· ...

q(3) 

~d
...



 



R(d-1)×(d-1).









q(d) ~2

q(d) ~3

···

q(d) ~d

We now introduce the following regularity condition on the function F at the point p0: (A4') The Jacobian matrix J(p0) is of full rank (i.e., rank(J(p0)) = d - 1).

Lemma 5. Let ^ be as given in Lemma 4. Suppose q  B(p0, ^), and  satisfies (A1')­(A4'). Then, (i) The function g(q) is smooth on B(p0, ) for some  > 0 and satisfies the following equality

d q(j) gj(q) = 0, j=1 gj(q) q(i)

for all q  B(p0, ). (ii) The function f , defined in (15), is smooth on B(p0, ) and its first- and second-order derivatives are

f (q) = log p0(j) + d q(i) gi(q) ,

q(j)

gj(q) i=1 gi(q) q(j)

2f (q) q(j)2

=

2 -
gj (q)

 gj (q) q(j)

-

d i=1

q(i) - gi(q)2

gi(q) q(j)

2

+

q(i) gi(q)

2gi(q) q(j)2

,

and

2f (q)

1 =-

gj(q) -

1

gi(q) - d

q(j)q(i)

gj(q) q(i) gi(q) q(i) l=1

-

q(l) gl(q)2

gl(q) q(j)

gl(q) q(i)

+

q(l) gl(q)

2gl(q) q(j)q(i)

(18) for i = j. (19)

The proofs of Lemmas 4 and 5 can be found in Appendix A and Appendix B, respectively.
One may wonder whether the new assumptions we have stated are overly restrictive. In fact, they are not and there exist
interesting families of uncertainty sets that satisfy Assumptions (A1')­(A4'). A canonical example of an uncertainty set  that satisfies these conditions is when F is piece-wise linear on the set PX+. Thus,  is similar to a linear family [22], an important class of statistical models.

Example 3. Let {Fk}lk=1 be a set of l linear functions with domain Rd and let {k}lk=1 be a set of l real numbers. Let

=

l k=1

{(y1

,

.

.

.

,

yd

)

:

Fk(y1, . . . , yd)



k }.

Assume

{Fk }lk=1

and

{k }lk=1

satisfy

the

following

three

conditions:

· The set   PX+ and Fk(p0) > k for some k;

· The minimizer  = arg min D(p0 p ) is such that F1( ) = 1 and Fk( ) < k for k = 1;

· Let F1(y1, . . . , yd) =

d i=1

wi

yi

for

some

real

coefficients

w1, . . . , wd.

One

of

the

coefficients

of

F1,

i.e.,

one

of

the

numbers in the set {wi}di=1, is not equal to 1.

Intuitively,  defined as the intersection of halfspaces (linear inequality constraints) is a polyhedron contained in the relative interior of PX . Fig. 1 provides an illustration for the ternary case X = {1, 2, 3}.

Proposition 1. The set  described in Example 3 satisfies Conditions (A1')­(A4').

The proof of Proposition 1 is provided in Appendix C. Now we are ready to state the promised central limit-type result for Sn, defined in (13).

10

(0, 0, 1)

F1

F3

F2

(1, 0, 0)





D(p0 p )



p0

D(p p0)

PX

(0, 1, 0)

Fig. 1: The set  formed by the intersection of three halfspaces defined by F1, F2, and F3. See Example 3.

Proposition 2. Under Conditions (A1')­(A4'), if {Xi} i=1 is a sequence of i.i.d. random variables with P (X1 = i) = p0(i) for all i  X , then {Sn} n=1, defined in (13), satisfies

 n

Sn n

- D(p0

p )

-d N 0, V (p0 p ) .

The proof of Proposition 2 can be found in Appendix D.
A major result in the statistics literature that bears some semblance to Proposition 2 is known as Wilks' theorem (see [16, Chapter 16] for example). For the case in which the null hypothesis is simple,1 Wilks' theorem states that if the sequence of random variables {Xi} i=1 is independently drawn from p0 (the distribution of the null hypothesis), then (two times) the log-likelihood ratio statistic

n

2 max

log

{p0} k=1

p (Xk) p0 (Xk )

-d

2d-1,

where 2d-1 is the chi-squared distribution with d - 1 degrees of freedom. This result differs from Proposition 2 because in Sn the maximization is taken over  whereas in Wilks' theorem, it is taken over   {p0}. Thisresults in different normalizations in the statements on convergence in distributions; in Proposition 2, Sn is normalized by n but there is no normalization of the log-likelihood ratio statistic in Wilks' theorem. This is because, for the former (our result), the dominant term is the
first-order term in the Taylor expansion, but in the latter (Wilks' theorem), the dominant term is the second-order term.

Proposition 3. Conditions (A1')­(A4') imply Conditions (A1)­(A3) in Section III.

The proof of Proposition 3 is provided in Appendix E. Thus, we see that the assumptions used to derive the first-order results are less restrictive than those for the second-order result that we are going to state in the next subsection.

B. Definition and Main Results

We say that a second-order exponent pair (G0, G1) is -achievable under the probabilistic constraints if there exists a sequence of sequential hypothesis tests {(n, n)} n=1 that satisfies the probabilistic constraints on the stopping time in (1) and

1

G0



lim inf
n

 n

1

G1



lim inf
n

 n

1 log P1|0(n, n) - nD(p p0) ,

1

log P0|1(n, n)

- nD(p0

p

)

,

1Wilks' theorem also applies to the case in which both the null and alternative hypotheses are composite, but we are only concerned with the simpler setting here.

11

where  = arg min D(p p0), which is unique (see Proposition 3 which implies that Condition (A2) is satisfied). The

set of all -achievable second-order exponent pairs (G0, G1) is denoted as G (p0, ), the second-order error exponent region.

Define the relative entropy variance [14]

p(X )

V (p q) := Varp

log q(X )

and the Gaussian cumulative distribution function (y) :=

y -

1 e-u2/2 du.
2

The

set

of

second-order

error

exponents

G (p0, ) is stated in the following theorem.

Theorem 2. If Conditions (A1')­(A4') are satisfied, for any 0 < < 1, the second-order error exponent region is

G (p0, ) =

(G0, G1)  R2 :

G0  -1( ) G1  -1( )

V (p p0) V (p0 p )

.

Furthermore, the boundary of this set is achieved by an appropriately defined sequence of GSPRTs. 
This theorem states that the backoffs from the (first-order) error exponents are of orders (1/ n) and the implied constants are given by -1( ) V (p p0) and -1( ) V (p0 p ). Thus, we have stated a set of sufficient conditions on the distributions and the uncertainty set  (namely (A1')­(A4')) for which the second-order terms are analogous to that for simple sequential hypothesis testing under the probabilistic constraints derived by Li and Tan [15]. However, the techniques used to derive Theorem 2 are more involved compared to those for the probabilistic constraints in [15]. This is because we have to derive the asymptotic distribution of the maximum of a set of log-likelihood ratio terms (cf. Proposition 2). This constitutes our main contribution in this part of the paper.

C. Proof of the Achievability Part of Theorem 2
The proof of achievability consists of two parts. We first prove the desired upper bound on type-I error probability and the maximal type-II error probability under an appropriately defined sequence of GSPRTs. Then we prove that the probabilistic constraints are satisfied.
We start with the proof of the first part. Let 0 and 1 be such that 0, 1  (0, ). Let (n, n) be the GSPRT with thresholds

An := n min D(p p0) + -1( - 0)
 

V (p p0) , n

and

Bn := n min D(p0 p ) + -1( - 1)
 

V (p0 p ) . n

Based on Proposition 3, we know that (A1)­(A3) are satisfied. Hence, from [12, Theorem 2.1] we have that

P0(Sn > An)  e-An and sup P (Sn < -Bn)  e-Bn .
 

To simplify An and Bn, we introduce an approximation lemma from [23, Lemma 48].

Lemma 6. Let  be a compact metric space. Suppose h :   R and k :   R are continuous, then we have

max[nh()

+

 nk()]

=

nh

+

nk

+

 o( n),

 

where h := max h() and k := sup:h()=h k().

Here we take h() = -D(p0 p ) and k() = --1( - 1) V (p0 p ). Based on Lemma 6 and the fact that   D(p0 p ) has a unique minimizer  (see Assumption (A2) which is implied by Proposition 3), we have

min nD(p0 p ) +

nV (p0 p )-1( - 1)

= nD(p0 p ) + -1( - 1)

 nV (p0 p ) + o( n).

(20)

 

Similarly, we have

min nD(p p0) +

nV (p p0)-1( - 0)

= nD(p p0) + -1( - 0)

 nV (p p0) + o( n).

(21)

 

Thus, based on (20) and (21), the arbitrariness of 0 and 1 and the continuity of -1, we obtain

1 lim inf  n n

1 log P0(Sn > An) - nD(p p0)

 -1( )

V (p p0),

(22)

12

and

1 lim inf  n n

1

log sup P (Sn

< -Bn) - nD(p0

p )

 -1( )

V (p0 p ).

(23)

Next we prove that the probabilistic constraints for the sequence of GSPRTs {(n, n)} n=1 are satisfied. Let  := inf{k : max Sk() < -Bn}. We observe that   n with probability 1. Thus, we have

P0(n > n)  P0( > n)

 P0 max Sn()  -Bn
 

= P0  P0

min n
 

d i=1

Q(i) log

p0(i) i



Bn

 min n
 

d i=1

Q(i)

log

p0(i) i

-

D(p0

p )

 -1( - 1)

V (p0 p )

 - 1

(24)

<,

(25)

where (24) is from Proposition 2. Hence, P0(n > n) < for sufficiently large n.
We now prove that sup P(n > n) < . Let  := inf{k : max Sk() > An}. We also have   n with probability 1. Then by the Berry-Esseen Theorem [24], for any 0  , we have

P0 (n > n)  P0 ( > n)

 P0

max Sn()  An
 

 P0 Sn(0)  n D(p0 p0) +

V (p0 n

p0) -1(

- 0)



-

0

+

T1 , n

(26)

where T1 is a positive finite constant depending only on Var0 ((0)) and E0 [|(0)|3]. As stated in Condition (A2' ) (i.e., that i  c0 > 0, i = 1, . . . , d) and p0(i) > 0, i = 1, . . . , d, thus E [|()|3] is uniformly bounded on . Then for every
0 < < 1, there exists an integer n1( ) which does not depend on , such that when n > n1( ), P0 ( > n)  - 0/2 < . Since 0   is arbitrary, sup P ( > n) < .
We have shown that the two probabilistic constraints (25) and (26) are satisfied for sufficiently large n. Then together with (22) and (23), we have shown that any second-order error exponent pair (G0, G1) such that G0  -1( ) V (p p0) and G1  -1( ) V (p0 p ) belongs to G (p0, ).

D. Proof of the Converse Part of Theorem 2

For each 0  , from [15], we know that

1



-

 n

log

P0 (A0(n))



nD(p0 p0 ) +

V (p0 p0 )-1( ) + n,

where n  0 as n  . Now we want to find the optimal upper bound for all   , which means we need to obtain

1



-

 n

sup
 

log

P

(A0(n))



min
 

nD(p0 p ) +

V (p0 p )-1( ) + n .

Similar to the analysis in achievability part, we use Lemma 6 and obtain that

1 lim sup  n n

1

log P1|0(n, n)

- nD(p0

p

)

 -1( )

V (p0 p ).

Similarly, we have that

1 lim sup  n n

1 log P0|1(n, n) - nD(p p0)

 -1( )

V (p p0),

which completes the proof of the converse.

13

APPENDIX In the appendix, we provide the proofs of Lemmas 4 and 5 as well as the proofs of Propositions 1, 2, and 3.

A. Proof of Lemma 4

We first prove Part (i) of Lemma 4. Assume, to the contrary, that g(q) is not continuous at some q  PX+ \ . Then there

exists a positive number  and a sequence {qk} k=1  PX+ \  such that qk  q as k   and

d i=1

|gi (qk )

-

gi(q)|





for

all k  N. From the definition of g(qk) and the fact that p0  P+, there exists ^ > 0 such that

d
qk(i) log
i=1

p0(i) gi (qk )

<

d i=1

qk(i) log

p0(i) gi(q)

- ^,

(27)

for all k  N. From Condition (A2') and the fact that {g(qk)} k=1  , there exists a constant M <  such that

sup log p0(i)  M,

kN,iX

gi (qk )

which further implies that

lim sup
k

d i=1

qk(i) log

p0(i) gi (qk )

=

lim sup
k

d i=1

q(i) + (qk(i) - q(i))

log p0(i) gi (qk )

= lim sup d q(i) log p0(i) .

k i=1

gi (qk )

(28)

Combining (27) and (28), we have that

lim sup
k

d i=1

q(i) log

p0(i) gi (qk )



lim sup
k

d i=1

qk(i) log

p0(i) gi(q)

-

^

=

d i=1

q(i) log

p0(i) gi(q)

-

^,

which contradicts the fact that

d q(i) log p0(i)  d q(i) log p0(i) .

i=1

gi(qk) i=1

gi(q)

Hence g(q) is continuous on PX+ \ . We next prove Part (ii) of Lemma 4. From the continuity of g(q) (as proved above), there exists ^ > 0 such that

{~ : ~ = g(q) for some q  B(p0, ^)}  B(g(p0), ),

which, together with Condition (A3') implies Part (ii) of Lemma 4. We now proceed to prove Part (iii) of Lemma 4. Recall that the optimizer ~ is obtained from the optimization problem (14).
Its corresponding Lagrangian is

L(, , µ) = d q(i) log p0(i) + 

i=1

i

d
i - 1
i=1

+ µF ().

For q  B(g(q), ^), F () is smooth at ~ (the previous part). Hence using the Karush­Kuhn­Tucker (KKT) conditions [25], the optimizer ~ satisfies the first-order stationary conditions, which are

q(i)

F ()

- ++µ

= 0,  i = 1, . . . , d.

(29)

~i

i =~

The complementary slackness condition is µF (~) = 0, which implies that either µ = 0 or F (~) = 0. When µ = 0, we have

q(i) = ~i   = 1  ~i = q(i),

which is impossible as q / . Thus, it holds that F (~) = 0, which means the optimizer lies on the boundary of the set . We then proceed to prove Part (iv) of Lemma 4. If

F (~) d F (~)

j

- ~i
i=1

i

=0

for all j  X , then {F (~)/j}dj=1 are all equal. Combining this fact with (29), we have that q = ~, which contradicts the fact that q  .
Finally, we prove Part (v) of Lemma 4. Combining the constraints in (14) and (29), we can obtain q in terms of 2 as:

d F ()

F ()

q(j) = ~j - µ~j ~i
i=1

~i

+ µ~j ~j ,

 j = 1, 2, . . . , d.

(30)

14

Then we obtain µ in terms of q(j) as:

1 F () d F () -1

µ= ~j

~j

- ~i
i=1

~i

(q(j) - ~j).

(31)

Then substituting (31) into (30), we have the desired formula. This completes the proof of Lemma 4.

B. Proof of Lemma 5

Now we prove Part (i) of Lemma 5. As F () is smooth and J(p0) is of full rank, there exists  > 0 such that J(q) is of full rank for all q  B(p0, ). Then by the inverse function theorem [26, Theorem 2.11], ~ = g(q) is differentiable in q. We multiply gj(q)/q(i) on both sides of (29) and sum from j = 1 to d to obtain

d q(j) gj(q) =  d gj(q) + µ d F (g(q)) gj(q) .

(32)

j=1 gj(q) q(i)

q(i)
j=1

j=1 gj(q) q(i)

We differentiate the first constraint

d i=1

~i

=

d i=1

gi(q)

=

1

with

respect

to

q

on

both

sides

to

obtain

d gj(q) = 0.

(33)

q(i)

j=1

From Part (iii) of Lemma 4 it follows that F (~) = F (g(q)) = 0, which means that the function formed by the composition of F and g is always 0 for all the q  B(p0, ^) . Therefore, the derivative of the composition of F and g with respect to q is 0, i.e.,

F (g(q)) =

d

F (g(q)) gj(q) = 0.

(34)

q(i)

j=1 gj(q) q(i)

Substituting (33) and (34) back into (32), we have that

d q(j) gj(q) = 0, j=1 gj(q) q(i)

as desired. Part (ii) of Lemma 5 follows from straightforward, albeit tedious, calculations. This completes the proof of Lemma 5.

C. Proof of Proposition 1

Assume F1() = F1(1, . . . , d) =

d i=1

wii

.

Without

loss

of

generality,

we

assume

w1

=

1.

Conditions

(A1')­(A3')

clearly hold. Hence from Part (ii) of Lemma 4 there exists ^ such that for all q  B(p0, ^), the optimizer ~ of the optimization

problem (14) is such that F1(~) = 1 and that Fk(~) < k for all k = 1. Note that F1()/i = wi. Then for q  B(p0, ^), using the KKT conditions, we obtain the first-order optimality conditions for the optimizer ~:

d

~i = 1,

i=1

d

wi~i = 1,

(35)

i=1

q(i) = 1~i + 2~iwi.

Hence,

2

=

q(1) - ~1 . ~1(w1 - 1)

(36)

Substituting (36) into (35), we obtain

q(i) = ~i

1 + (q(1) - ~1)(wi - 1) ~1(w1 - 1)

.

Thus, the Jacobian of (q(2), . . . , q(d)) at (~2, . . . , ~d) is the following (d - 1) × (d - 1) diagonal matrix:

J(q) = diag 1 + (q(1) - ~1)(w2 - 1) , 1 + (q(1) - ~1)(w3 - 1) , . . . , 1 + (q(1) - ~1)(wd - 1) .

~1(w1 - 1)

~1(w1 - 1)

~1(w1 - 1)

Since p0(i) > 0 for all i = 1, 2, . . . , d, the diagonal terms in the Jacobian J(p0) are non-zero. Thus, det(J(p0)) = 0, which proves that Condition (A4') holds for the set  in Example 3.

15

D. Proof of Proposition 2
 We now prove the promised central limit-type result for the sequence of random variables {Sn/ n}nN. Let z  (0, 1). Let  be given as in Part (i) of Lemma 5 and define the -typical set

T(n) = T(n)(p0) := xn  X n :

1 n

n
1{xk = i}

- p0(i) < ,  i  X

.

k=1

This is the set of sequences whose types are near p0. The key idea is to perform a Taylor expansion of the function f (Q) =

d i=1

Q(i)

log

p0 (i) gi (Q)

(defined

in

For brevity, define the deviation

(15)) at the of the type

point Q of

Q = p0 and analyze the asymptotics of the various Xn from the true distribution at symbol i  X as

terms

in

the

expansion.

i := Q(i) - p0(i).

For q  B(p0, ), let H(q)  Rd×d be the Hessian matrix of f (q). This is well defined because f (·) is twice continuously differentiable on B(p0, ) according to Part (ii) of Lemma 5. If xn  T(n), then Q  B(p0, ). Thus for Q  B(p0, ), using
Taylor's theorem we have the expansion

f (Q) = f (p0) + f (p0)

(Q

-

p0)

+

1 (Q
2

-

p0)H(Q~)(Q

-

p0)

=

d
p0(i) log
i=1

p0(i) gi(p0)

+

d
log
i=1

p0(i) gi(p0)

i

-

d i=1

d j=1

p0(j) gj(p0) gj(p0) q(i)

i
q=p0

+

1 2

(Q

-

p0)H(Q~)(Q

-

p0

)

(37)

= D(p0

p

)+

d
log
i=1

p0(i) gi(p0)

i

-

d i=1

d j=1

p0(j) gj(q) gj(p0) q(i)

i
q=p0

+

1 2

(Q

-

p0)H(Q~)(Q

-

p0

)

,

(38)

where Q~ lies on the line segment between Q and p0 and (37) follows from (18) in Lemma 5. Note that we represent probability mass functions as row vectors.
Let

bi

:=

log

p0(i) gi(p0)

d
-
j=1

p0(j) gj(Q) gj(p0) q(i)

,
Q=p0

 i = 1, . . . , d.

As p0  , then it follows from Part (i) of Lemma 5 that

bi

=

log

p0(i) gi(p0)

.

(39)

Recall from (15) that f (Q) = min

d i=1

Q(i)

log

p0 (i) i

.

Then

for

Q



B(p0,

 ),

combining

(38)

and

(39)

we

have

that

min
 

 n

d

Q(i) log p0(i)

 i=1

i

 - nD(p0 p )

= n f (Q) - D(p0 p )

=

d
log
i=1

p0(i) gi(p0)

 ni

-

d i=1

d j=1

p0(j) gj(q) gj(p0) q(i)

 ni
q=p0

+



n 2

(Q

-

p0)H(Q~)(Q

-

p0

)

=

d i=1

 ni

log

p0(i) gi(p0)

+

 n (Q 2

-

p0)H(Q~)(Q

-

p0)

.

(40)

Let min(H(q)) and max(H(q)) be the smallest and largest eigenvalues of H(q), respectively. From Part (i) of Lemma 5, it follows that f (·) is smooth on B(q0, ), which implies that there exists two constants c~ and C~ such that

- < c~ < min min(H(q))  max max(H(q)) < C~ < .

(41)

q B(q0 , )

q B(q0 , )

16

Then we have that

P0

 min n
 

d i=1

Q(i)

log

p0(i) i

-

D(p0

p

)

 -1(z)

V (p0 p )

 P0

 min n
 

d i=1

Q(i)

log

p0(i) i

-

D(p0

p )

 -1(z)

V (p0 p ), Xn  T(n)

+ P0(Xn / T(n))

= P0

d



 nbii +

n 2

(Q

-

p0)H(Q~)(Q

-

p0

)

 -1(z)

V (p0 p ), Xn  T(n)

+ P0(Xn / T(n))

(42)

i=1

 P0

d i=1

 ni

log

p0(i) gi(p0)

+

min(H(Q~)) 2

d i=1

n2i



-1(z)

V (p0 p ), Xn  T(n)

+ P0(Xn / T(n))

 P0

d i=1

 ni

log

p0(i) gi(p0)

+

c~ 2

d i=1

n2i



-1(z)

V (p0 p ), Xn  T(n)

+ P0(Xn / T(n))

(43)

 P0

d i=1

 ni

log

p0(i) gi(p0)

+

c~ 2

d i=1

n2i



-1(z)

V (p0 p )

+ d exp(-2n2),

(44)

where (42) follows from the fact that Q  B(p0, ) for xn  T(n) and (40), (43) follows from (41), and (44) holds by the union bound and Hoeffding's inequality. Similarly,

P0

 min n
 

d i=1

Q(i)

log

p0(i) i

-

D(p0

p )

 -1(z)

V (p0 p )

 P0

d

 nbii

+

 n 2

(Q

-

p0)H(Q~)(Q

-

p0)

 -1(z)

V (p0 p ), Xn  T(n)

i=1

 P0

d

 nbii

+

max(H(Q~)) 2

d

n2i  -1(z)

V (p0 p ), Xn  T(n)

i=1

i=1

 P0

d i=1

 ni log

p0(i) gi(p0)

+

C~ 2

d i=1

n2i



-1(z)

V (p0 p ), Xn  T(n)

 P0

d i=1

 ni log

p0(i) gi(p0)

+

C~ 2

d i=1

n2i



-1(z)

V (p0 p )

- P0 Xn  T(n)

 P0

d i=1

 ni log

p0(i) gi(p0)

+

C~ 2

d i=1

n2i



-1(z)

V (p0 p )

- d exp(-2n2).

(45)

One checks that

n

d i=1

i log

p0(i) gi(p0)

=

n k=1

d i=1

1{Xk = i} - p0(i)

log p0(i) gi(p0)

(46)

and

Var0

d
(1{X1
i=1

=

i}

-

p0(i))

log

p0(i) gi(p0)

= E0

d
(1{X1
i=1

=

i}

-

p0(i))

log

p0(i) gi(p0)

2

(47)

= E0

d

b2i (1{X1 = i} - p0(i))2 + 2 (1{X1 = i} - p0(i))(1{X1 = j} - p0(j))

i=1

j=i

log p0(i) gi(p0)

log p0(j) gj (p0)

=

d
(1
i=1

-

p0(i))p0(i) log2

p0(i) gi(p0)

-

2

i=j

p0(i)p0(j)

log p0(i) gi(p0)

log p0(j) gj (p0)

(48)

17

=

d i=1

p0(i) log2

p0(i) gi(p0)

-

d i=1

p0(i)2

log2

p0(i) gi(p0)

-

2

i=j

p0(i)p0(j)

log p0(i) gi(p0)

= V (p0 p ),

log p0(j) gj (p0)

where (47) follows from and (48) follows from

E0

d
(1{X1
i=1

=

i}

-

p0(i))

log

p0(i) gi(p0)

= 0,

E0
i=j

(1{X1 = i} - p0(i))(1{X1 = j} - p0(j))

log p0(i) gi(p0)

= - p0(i)p0(j)
i=j

log p0(i) gi(p0)

log p0(j) gj (p0)

log p0(j) gj (p0)

and

E0

d
(1{Xk
i=1

=

i}

-

p0(i))2

log2

p0(i) gi(p0)

=

d
(1
i=1

-

p0(i))p0(i)

log2

p0(i) . gi(p0)

Therefore n

d i=1

i

log

p0 (i) gi (p0 )

is

a

sum

of

i.i.d.

random

variables

and variance V (p0 p ). Hence, by the central limit theorem,

di=1(1{Xk

=

i} - p0(i)) log

p0 (i) gi (p0 )

n k=1

with

mean

0

Together with the fact that

d i=1

 ni

log

p0(i) gi(p0)

-d N (0, V (p0

p )).

d i=1

n2i



0

almost

surely,

this

implies

that

d i=1

 ni

log

p0(i) gi(p0)

+

c~ 2

d i=1

n2i

-d

N (0,

V

(p0

p )),

and

(49)

d i=1

 ni

log

p0(i) gi(p0)

+

C~ 2

d n2i -d N (0, V (p0
i=1

p )).

(50)

Then combining (44), (45), (49) and (50), we have that

lim sup P0
n

 min n
 

d i=1

Q(i)

log

p0(i) i

-

D(p0

p

)

 -1(z)

V (p0 p )

 z,

and

(51)

lim inf P0
n

 min n
 

d i=1

Q(i)

log

p0(i) i

-

D(p0

p

)

 -1(z)

V (p0 p )

 z.

(52)

Since z  (0, 1) is arbitrary, it follows from (51) and (52) that

 min n
 

d i=1

Q(i) log

p0(i) i

-

D(p0

p )

which completes the proof of Proposition 2.

-d N (0, V (p0 p )),

E. Proof of Proposition 3
We now show that Conditions (A1')­(A4') imply Conditions (A1)­(A3). Condition (A1) is easily verified by Condition (A1'). As X = {1, 2, . . . , d}, we have

D(p0

p )

=

d i=1

p0(i)

log

p0(i) i

and

D(p

p0)

=

d i=1

i

log

i . p0(i)

Combining Condition (A2') which says that mini=1,...,d i  c0 > 0 for all    and mini=1,...,d p0(i) > 0, D(p0 p ) and D(p p0) are uniformly bounded and twice continuously differentiable on . As p0 / , D(p0 p ) > 0 and D(p p0) > 0, which together with the compactness of , implies that

min D(p0 p ) > 0 and min D(p p0) > 0.

(53)

 

 

18

From [21, Theorem 2.7.2], D(p0 p ) is strictly convex in (p0, ), which, together with the fact that  is compact and convex,

implies the uniqueness of the minimizers to the two optimization problems in (53).

For Condition (A3), as X is a finite alphabet and Condition (A2') holds, it can be easily checked that E[max |()|2] <

 and that 

for (

each )=

  ,

(

1{X =1}
1

,

E. .0.[e, x1p{X(-d=td}()

))] < . We

 can

and E [exp(t())] <  for all t  [-t0, t0] for some define the finite number x0 := max maxiX 1/i 

t0 > 0. Note 1/c0 (because

Condition (A2') mandates that mini=1,...,d i  c0 > 0 for all   ). With this choice, trivially, for all x > x0,

P0 max | ()| > x = 0,
 
which shows that Condition (A3) clearly holds.

REFERENCES
[1] R. Blahut, "Hypothesis testing and information theory," IEEE Transactions on Information Theory, vol. 20, no. 4, pp. 405­417, 1974. [2] B. C. Levy, Principles of Signal Detection and Parameter Estimation. Springer Science & Business Media, 2008. [3] J. Neyman and E. S. Pearson, "On the problem of the most efficient tests of statistical hypotheses," Philosophical Transactions of the Royal Society of
London (Series A), vol. 231, pp. 289­337, 1933. [4] Y. Polyanskiy and Y. Wu, "Lecture notes on information theory," Lecture Notes for ECE563 (UIUC), 2014. [5] A. Wald and J. Wolfowitz, "Optimum character of the sequential probability ratio test," Ann. Math. Statist., vol. 19, no. 3, pp. 326­339, 1948. [6] A. Lalitha and T. Javidi, "Reliability of sequential hypothesis testing can be achieved by an almost-fixed-length test," in IEEE International Symposium
on Information Theory. IEEE, 2016, pp. 1710­1714. [7] M. Haghifam, V. Y. F. Tan, and A. Khisti, "Sequential classification with empirically observed statistics," IEEE Transactions on Information Theory,
vol. 67, no. 5, pp. 3095­3113, 2021. [8] O. Zeitouni, J. Ziv, and N. Merhav, "When is the generalized likelihood ratio test optimal?" IEEE Transactions on Information Theory, vol. 38, no. 5,
pp. 1597­1602, 1992. [9] T.-L. Lai, "Asymptotic optimality of generalized sequential likelihood ratio tests in some classical sequential testing problems," Sequential Analysis,
vol. 21, no. 4, pp. 219­247, 2002. [10] Y. Li, S. Nitinawarat, and V. V. Veeravalli, "Universal outlier hypothesis testing," IEEE Transactions on Information Theory, vol. 60, no. 7, pp. 4066­4082,
2014. [11] Y. Li, S. Nitinawarat, and V. V. Veeravalli, "Universal sequential outlier hypothesis testing," Sequential Analysis, vol. 36, no. 3, pp. 309­344, 2017. [12] X. Li, J. Liu, and Z. Ying, "Generalized sequential probability ratio test for separate families of hypotheses," Sequential Analysis, vol. 33, no. 4, pp.
539­563, 2014. [13] V. Strassen, "Asymptotische abschatzugen in Shannon's informationstheorie," in Transactions of the Third Prague Conference on Information Theory
etc, 1962. Czechoslovak Academy of Sciences, Prague, 1962, pp. 689­723. [14] V. Y. F. Tan, "Asymptotic estimates in information theory with non-vanishing error probabilities," Foundations and Trends® in Communications and
Information Theory, vol. 11, no. 1-2, pp. 1­184, 2014. [15] Y. Li and V. Y. F. Tan, "Second-order asymptotics of sequential hypothesis testing," IEEE Transactions on Information Theory, vol. 66, no. 11, pp.
7222­7230, 2020. [16] A. W. van der Vaart, Asymptotic Statistics. Cambridge University Press, 1998. [17] H. J. Bierens, Modes of Convergence, ser. Themes in Modern Econometrics. Cambridge University Press, 2004, pp. 137­178. [18] J. K. Blitzstein and J. Hwang, Introduction to Probability. CRC Press, 2019. [19] I. Karatzas and S. E. Shreve, Brownian Motion. Springer, 1998. [20] R. Durrett, Probability: Theory and Examples. Duxbury Press, 2004. [21] T. M. Cover and J. A. Thomas, Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing). USA: Wiley-Interscience,
2006. [22] S.-I. Amari and H. Nagaoka, Methods of Information Geometry, ser. Translations of Mathematical Monographs. American Mathematical Society, 2007. [23] Y. Polyanskiy, Channel Coding: Non-Asymptotic Fundamental Limits. Princeton University, 2010. [24] A. C. Berry, "The accuracy of the Gaussian approximation to the sum of independent variates," Transactions of the American Mathematical Society,
vol. 49, no. 1, pp. 122­136, 1941. [25] S. Boyd and L. Vandenberghe, Convex Optimization. Cambridge University Press, 2004. [26] M. Spivak, Calculus On Manifolds: A Modern Approach To Classical Theorems Of Advanced Calculus. Taylor & Francis Inc, 1971.

