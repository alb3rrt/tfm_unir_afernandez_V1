Unsupervised detection of mouse behavioural anomalies using two-stream convolutional autoencoders

Ezechukwu I Nwokedi School of Computer Science University of Lincoln, UK
enwokedi@lincoln.ac.uk

Rasneer S Bains MRC Harwell Institute
Harwell, UK
r.bains@har.mrc.ac.uk

Sara Wells MRC Harwell Institute
Harwell, UK
s.wells@har.mrc.ac.uk

Xujiong Ye School of Computer Science University of Lincoln, UK
xye@lincoln.ac.uk

Luc Bidaut School of Computer Science University of Lincoln, UK
lbidaut@lincoln.ac.uk
James M Brown School of Computer Science University of Lincoln, UK
jamesbrown@lincoln.ac.uk

arXiv:2106.00598v1 [cs.CV] 28 May 2021

Abstract
This paper explores the application of unsupervised learning to detecting anomalies in mouse video data. The two models presented in this paper are a dual stream, 3D convolutional autoencoder (with residual connections) and a dual stream, 2D convolutional autoencoder. The publicly available dataset used here contains twelve videos of a single home-caged mice alongside frame level annotations. Under the pretext that the autoencoder only sees normal events, the video data was handcrafted to treat each behaviour as a pseudo-anomaly thereby eliminating them from the others during training. The results are presented for one conspicuous behaviour (hang) and one inconspicuous behaviour (groom). The performance of these models is compared to a single stream autoencoder and a supervised learning model, which are both based on the custom CAE encoder. Both models are also tested on the CUHK Avenue dataset were found to perform as well as some state-of-theart architectures.
1. Introduction
Over the years, the application of machine learning to studies of animal behaviour has gained momentum with increased availability of data and more innovative and robust techniques. Aside from methods which could aid various processes and automate workflows [13], machine learning models could promote safer and less intrusive means of carrying out research involving animals. As such, many organisations continue to push for their implementation in various animal-centric laboratories and institutes.
One of the main ethical challenges which is vital to stud-

ies involving animals is the identification of welfare concerns. These could take on several forms, but can be broadly classed as either physiological or behavioural anomalies. In behavioural studies, these anomalies refer to any deviations from the perceived normal behaviour by the subject. For instance, review studies by [6] show that rodents can exhibit footshock, flinch, hyper/hypoactivity and forced swim behaviours (amongst a host of others) as symptoms of varying anxiety disorders. However, the definition of what is classed as a behavioural anomaly widely varies across research.
For classification of specific activities from videos, several supervised algorithms have been proposed over the years [5, 10, 12]. However, such models require fully labelled data which, for videos, is an arduous and labourintensive task. Furthermore, this may not be a viable option in scenarios where events are not clearly defined. However, autoencoders and generative models have been found to be very effective for semi-supervised and unsupervised anomaly detection. Based on the construct in [11], the authors of [15] built a 3D convolutional autoencoder which uses two decoders (one for reconstructing input and the other for predicting future sequences) and achieved high accuracy detection. In addition, [3] also achieved good anomaly detection results using a 3D convolutional autoencoder for spatial encoding-decoding, and a 2D convolutional long short-term memory for the temporal encodingdecoding. Furthermore, [9] proved the effectiveness of conditional generative adversarial networks (GAN) in spotting anomalies by learning only normal spatio-temporal representations.
The focus of this paper is to present a proof-of-concept for anomaly detection in mouse videos using autoencoders. This is achieved using two autoencoders to detect be-

1

havioural anomalies. Unlike common two-stream models, the autoencoders proposed here are fused at the bottleneck (to allow shared representation learning at lower dimensions) and then separated again, like in [14]. The dataset used contains twelve videos of over 10 hours in length, of a individually housed mice with annotations for eight different behaviours [4]. The annotated behaviours are drink, eat, groom, hang, rear, rest, micromovement and walk. In our case, the detection models are applied to detect one conspicuous behaviour (hang) and one less conspicuous behaviour (groom) as pseudo-anomalies (Figure 1). Comparisons are also made between these two stream models, a single stream CAE and a fully supervised 2D convolutional network. Additionally, the performance of the two stream models on a public anomaly dataset is presented.
Figure 1: Example frames of the hang (left) and groom (right) pseudo-anomalous behaviours. Dataset from [4].
2. Methods
2.1. Architectures
The two models proposed are a 3D residual autoencoder (RAE) and a time-distributed 2D convolutional autoencoder (CAE). The 3D RAE is a deep fusion model made up of 3D convolutions and residual connections within both the encoding and decoding sub-models. This model bears some similarities to the work by [14] however theirs does not make use of residual connections. This ensures that residual features are enforced at each stage of the encoding/decoding process, thus preserving the bulk of the information. This is essential because most behaviours being treated as anomalies are best observed in their dynamic representations [8] and not a point or instance occurrence.
On the other hand, the dual CAE was made to thoroughly map spatial features of each of the video frames. By simultaneously applying 2D convolutions to several video frames at a time, the network operates on spatio-temporal cubes like a 3D model. Both models have dual streams that are fused midway. This network was created to push the limits of spatial encoder-decoder mapping and compare the performance against its 3D counterpart (Figure 2). The single stream CAE and supervised model, used for comparison, make use of only the grayscale images.

2.2. Data & pre-processing
The training data for the two fusion models is comprised of the raw video and optical flow images. The video frames are initially resized 32x32 grayscale images, and brightened via gamma correction to deal with variations in illumination and hue. This allows for a cleaner dense optical flow [2] to be computed between successive frames. Both grayscale and dense optical flow images are augmented using horizontal flips and then normalised between 0 and 1. The images are also centrally cropped to 24x32 to remove some redundant regions. Finally, with the aid of the annotations, frames related to the chosen anomalous class are removed from the training data and reshaped into a 4D tensors having 8 frames per sequence.
2.3. Training and testing
Despite having twelve videos available in the dataset, only six are used in training and validating the models to help reduce the heavy imbalance which exists between the labels (detailed distribution of video frames to their classes available in [8]). Continuous strides of five frames (or approximately one-fifth of every second) and no overlaps were also applied throughout the training data to further reduce its size. For the test, half of one of the videos (whose duration is > 1 hour) was used, with no skips included. This was chosen because it fulfilled the requirement of having all the annotated behaviours. The final training set for the hang behaviour had 9577 sequences while the validation set had 2395 sequences. The final data for the groom behaviour had 7312 and 1828 sequences for training and validation, respectively. The testing data had 6989 sequences.
2.4. Hyperparameters and loss function

Loss function Loss Image weights Optical flow Optimiser Learning rate (lr) Momentum Epochs Batch size Dilation rate (convolutions)

RAE MSE 0.75 1.0 Adam 0.0002 0.5 120 64
1

CAE MSE 1.0 1.0 Adam 0.0002 0.5 60 32 2 (last two layers)

Table 1: Hyperparameters used for model training.

The hyperparameters used in both models can be found in Table 1. We utilise a standard mean squared error (MSE) loss function for training:

2

Figure 2: The two-stream convolutional autoencoder (CAE) architecture.

1 L(x, x^) =

(x - x^)2

(1)

n

where is the input x is either the raw video/optical flow and x^ is the decoded reconstruction. The loss L is computed for each stream independently and a weighted sum computed to produce a combined loss Ld. Unlike the CAE, the RAE was found to perform best with a loss-weight ratio of 0.75:1.0 for the grayscale and dense flow images, respectively. In addition, homogenous kernels of (3x3x3) were used throughout the 3D RAE residual layers while the CAE used non-homogenous kernels (3x3, 5x5, etc.) in its 2D convolutions.

2.5. Metrics
Regularity scores were computed via the 2 norm of the MSE between the input data and their reconstructions, averaged across both streams. The area under the receiver operator characteristic curve (AUC) is one of the most widely used metrics used in evaluating anomaly detection and hence, was adopted in this paper for all the models.

2.6. Equipment
All code was written in Python 3.8 using the TensorFlow (Keras) library [1]. All models were trained on a single NVIDIA TITAN V GPU.

3. Results
3.1. Performance on benchmark dataset
The benchmark used used to test the models was the CUHK Avenue dataset [7]. Besides normalisation, no additional pre-processing was carried out on this dataset. The same hyperparameters used on the mouse dataset were also used here. The CAE architecture achieved better AUCs than the 3D RAE overall. The AUCs achieved (Table 2) are in

Method 2D CAE Detection at 150 FPS Two-stream 3D Conv AE STAE (spatial only) STAE (optical flow only) DSTN Our single-stream CAE (spatial only) Our two-stream 3D RAE Our two-stream CAE

AUC 0.702 0.809 0.866 0.771 0.809 0.879 0.784 0.826 0.832

Table 2: Comparison of methods on the CUHK Avenue dataset.

fact on par with those achieved by mainstream anomaly detection models, even better in some cases. This lends credence to the validity of the proposed models and the ideology behind them for detecting anomalies.
3.2. Performance on less conspicuous behaviour
The groom behaviour is, unlike hang, a less perceptible mouse behaviour. From a static point of view, it can easily be confused for a number of others, including micromovement and eating. The challenge in identifying this behaviour was also observed by the authors of the dataset [3], who recorded 57% accuracy in human detection and about 70% accuracy from their supervised approach. Thus, the results achieved were not desirable, but still marginally better than a random guesswork (AUC=0.5).
3.3. Performance on more conspicuous behaviour
Here, the two-stream models achieved excellent results having AUCs of over 0.9. Though anomalies are not always as noticeable as hang, the performance of the models demonstrates that the two-stream autoencoders can detect obvious behavioural mouse anomalies with high pefor-

3

Figure 3: Two-stream models on groom: residual autoencoder (left) and convolutional autoencoder (right)
Figure 4: Two-stream models on hang: residual autoencoder (left) and convolutional autoencoder (right)
mance. 3.4. Performance of alternate networks
Two alternative models were also developed, derived from the dual CAE, to compare their performance with the two stream models; these were done for supervised and unsupervised learning modes. The unsupervised model explores how well the single stream autoencoder works using spatial data (grayscale). It made use of the same hyperparameters as the two-stream CAE. The supervised model made use of the CAEs encoder and an appended fully connected network. This was trained for 120 epochs using stochastic gradient descent and a learning rate of 0.0005.
The supervised model achieved AUCs of 0.99 and 0.66 on hang and groom respectively, while the one-stream CAE had 0.959 and 0.644. In comparison to the dual stream models, these two approaches achieved higher AUCs on both behaviours. This was expected of the supervised model because it has the benefit of the annotations to aid mapping. Furthermore, its effectiveness also confirms the ability of the proposed 2D CAE's encoder in spatial representation. However, the improved performance of a one-stream model was unexpected, and suggests that further pre-processing may be needed to the video prior to calculation of optical flow, or refinement of the dual loss weighting.
4. Conclusion
The two-stream models treated in this paper prove the applicability of fully unsupervised autoencoders in detecting mouse behaviours. In the initial validation study, the

models achieved results comparable with the state-of-theart on the CUHK Avenue benchmark dataset. While the problem formulation for mouse anomalies was artificial, it demonstrated the utility of the proposed approach for genuine anomalies. This study provides a foundation for more intensive investigation into improvements from both the data processing and the model tunning ends. For the next steps, exploring unsupervised forms of even deeper networks and other generative models are promising alternatives to detecting subtle behavioural anomalies.
5. Acknowledgements
This research was funded by the National Centre for the Replacement, Refinement & Reduction of Animals in Research (NC/T002050/1).
References
[1] Franc¸ois Chollet et al. Keras. https://keras.io, 2015. 3
[2] Gunnar Farneba¨ck. Two-frame motion estimation based on polynomial expansion. In Scandinavian conference on Image analysis, pages 363­370. Springer, 2003. 2
[3] Jiangpeng Fu, Wentao Fan, and Nizar Bouguila. A novel approach for anomaly event detection in videos based on autoencoders and se networks. In 2018 9th International Symposium on Signal, Image, Video and Communications (ISIVC), pages 179­184. IEEE, 2018. 1
[4] Hueihan Jhuang, Estibaliz Garrote, Xinlin Yu, Vinita Khilnani, Tomaso Poggio, Andrew D Steele, and Thomas Serre. Automated home-cage behavioural phenotyping of mice. Nature communications, 1(1):1­10, 2010. 2
[5] Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, and Li Fei-Fei. Large-scale video classification with convolutional neural networks. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 1725­1732, 2014. 1
[6] Kimberly R Lezak, Galen Missig, and William A Carlezon Jr. Behavioral methods to study anxiety in rodents. Dialogues in clinical neuroscience, 19(2):181, 2017. 1
[7] Cewu Lu, Jianping Shi, and Jiaya Jia. Abnormal event detection at 150 fps in matlab. In Proceedings of the IEEE international conference on computer vision, pages 2720­2727, 2013. 3
[8] Ngoc G Nguyen, Dau Phan, Favorisen R Lumbanraja, Mohammad Reza Faisal, Bahriddin Abapihi, Bedy Purnama, Mera K Delimayanti, Kunti R Mahmudah, Mamoru Kubo, and Kenji Satou. Applying deep learning models to mouse behavior recognition. Journal of Biomedical Science and Engineering, 12(2):183­196, 2019. 2
[9] Mahdyar Ravanbakhsh, Moin Nabi, Enver Sangineto, Lucio Marcenaro, Carlo Regazzoni, and Nicu Sebe. Abnormal event detection in videos using generative adversarial nets. In 2017 IEEE International Conference on Image Processing (ICIP), pages 1577­1581. IEEE, 2017. 1

4

[10] Karen Simonyan and Andrew Zisserman. Two-stream convolutional networks for action recognition in videos. arXiv preprint arXiv:1406.2199, 2014. 1
[11] Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudinov. Unsupervised learning of video representations using lstms. In International conference on machine learning, pages 843­852. PMLR, 2015. 1
[12] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spatiotemporal features with 3d convolutional networks. In Proceedings of the IEEE international conference on computer vision, pages 4489­4497, 2015. 1
[13] John Joseph Valletta, Colin Torney, Michael Kings, Alex Thornton, and Joah Madden. Applications of machine learning in animal behaviour studies. Animal Behaviour, 124:203­220, 2017. 1
[14] Peng Wu, Jing Liu, and Fang Shen. A deep one-class neural network for anomalous event detection in complex scenes. IEEE transactions on neural networks and learning systems, 31(7):2609­2622, 2019. 2
[15] Yiru Zhao, Bing Deng, Chen Shen, Yao Liu, Hongtao Lu, and Xian-Sheng Hua. Spatio-temporal autoencoder for video anomaly detection. In Proceedings of the 25th ACM international conference on Multimedia, pages 1933­1941, 2017. 1
5

