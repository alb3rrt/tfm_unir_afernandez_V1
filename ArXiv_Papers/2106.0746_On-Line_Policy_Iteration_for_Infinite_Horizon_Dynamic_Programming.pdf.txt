arXiv:2106.00746v1 [math.OC] 1 Jun 2021

May 2021
On-Line Policy Iteration for Infinite Horizon Dynamic Programming
Dimitri Bertsekas
Abstract In this paper we propose an on-line policy iteration (PI) algorithm for finite-state infinite horizon discounted dynamic programming, whereby the policy improvement operation is done on-line, only for the states that are encountered during operation of the system. This allows the continuous updating/improvement of the current policy, thus resulting in a form of on-line PI that incorporates the improved controls into the current policy as new states and controls are generated. The algorithm converges in a finite number of stages to a type of locally optimal policy, and suggests the possibility of variants of PI and multiagent PI where the policy improvement is simplified. Moreover, the algorithm can be used with on-line replanning, and is also well-suited for on-line PI algorithms with value and policy approximations.
1. SIMPLIFIED ON-LINE POLICY ITERATION FOR DISCOUNTED PROBLEMS
We introduce new on-line variants of the classical policy iteration (PI) algorithm for finite-state discounted infinite horizon dynamic programming (DP) problems. The common characteristic of these variants is that, in addition to being suitable for on-line implementation, they are simplified in two ways:
(a) They perform policy improvement operations only for the states that are encountered during the on-line operation of the system.
(b) The policy improvement operation is simplified in that it uses approximate minimization over the Q-factors of the current policy at the current state.
Despite these simplifications, we show that our algorithms generate a sequence of improved policies, which converge to a policy with a local optimality property. Moreover, with an enhancement of the policy improvement operation, which involves a form of exploration, they converge to a globally optimal policy.
The motivation for our work comes from the rollout algorithm; see the author's reinforcement learning books [Ber19] and [Ber20], which provide many additional references. This algorithm starts from some available "base policy" and implements on-line an improved "rollout policy," which is the one that would be obtained from the first iteration of the PI algorithm starting from the base policy. In the algorithm of the
 Fulton Professor of Computational Decision Making, ASU, Tempe, AZ, and McAfee Professor of Engineering, MIT, Cambridge, MA.
1

present paper, the data accumulated from the rollout implementation is used to improve on-line the base policy, and to asymptotically obtain a policy that is either locally or globally optimal.

We assume a discrete-time dynamic system with states 1, . . . , n, and we use a transition probability notation. We denote states by the symbol x and successor states by the symbol y. The control/action is denoted by u, and is constrained to take values in a given finite constraint set U (x), which may depend on the current state x. The use of a control u at state x specifies the transition probability pxy(u) to the next state y, at a cost g(x, u, y).

A policy  = {µ0, µ1, . . .} is a sequence of functions from state to control that satisfies the control constraint, i.e., µk(x)  U (x) for all x and k. Given a policy  and an initial state x0, the system becomes a Markov chain whose generated trajectory under , denoted {x0, x1, . . .}, has a well-defined probability distribution. The corresponding total expected cost is

J(x0) = lim E
N 

N -1
kg xk, µk(xk), xk+1
k=0

x0,  ,

x0 = 1, . . . , n,

where  < 1 is the discount factor. The above expected value is taken with respect to the joint distribution

of the states x1, x2, . . ., conditioned on the initial state being x0 and the use of . The optimal cost starting from a state x, i.e., the minimum of J(x) over all policies , is denoted by J*(x). We will view J* as the vector of the n-dimensional space n that has components J*(1), . . . , J*(n).

A stationary policy is a policy of the form  = {µ, µ, . . .}, and for brevity, it is referred to as the "policy µ." The cost of µ starting from state x is denoted by Jµ(x), and is given by

Jµ(x0) = lim E
N 

N -1
kg xk, µ(xk), xk+1
k=0

x0, µ ,

x0 = 1, . . . , n.

We can view Jµ as the vector in n that has components Jµ(1), . . . , Jµ(n). We say that µ is optimal if

Jµ(x) = J *(x) = min J(x),


x = 1, . . . , n.

It is well known that there exists an optimal stationary policy; see e.g., the books [Ber12], [Put94], which provide an extensive analysis of discounted finite-state infinite horizon DP problems.

The theory and algorithms for our problem are conveniently stated with the use of abstract notation,

as in the book [Ber18]. In particular, for each policy µ, we introduce the operator Tµ : n  n, which maps a vector J  n to the vector TµJ  n that has components

n
(TµJ)(x) = pxy µ(x) g(x, µ(x), y) + J(y) ,
y=1

x = 1, . . . , n.

(1.1)

 In our notation, n is the n-dimensional Euclidean space and all vectors in n are viewed as column vectors. Moreover, all vector inequalities J  J are meant be componentwise, i.e., J(x)  J(x) for all x = 1, . . . , n.

2

We also introduce the operator T : n  n defined by
n
(T J)(x) = min pxy(u) g(x, u, y) + J(y) ,
uU(x) y=1

x = 1, . . . , n.

An important property is that Tµ and T are monotone, i.e., that for all J, J  n,

(1.2)

TµJ  TµJ , T J  T J , if J  J .

Another important property is that Tµ and T are sup-norm contractions, so that the costs Jµ(x), x = 1, . . . , n, are the unique solution of Bellman's equation

n
Jµ(x) = pxy µ(x) g(x, µ(x), y) + Jµ(y) ,
y=1

x = 1, . . . , n,

(1.3)

which can also be written as the fixed point equation Jµ = TµJµ. Similarly, the optimal costs J*(x), x = 1, . . . , n, are the unique solution of Bellman's equation

n
J (x) = min pxy(u) g(x, u, y) + J (y) ,
uU(x) y=1

x = 1, . . . , n,

(1.4)

or J* = T J*. A consequence of this is that the following optimality conditions hold

TµJ* = T J* if and only if µ is optimal,

(1.5)

TµJµ = T Jµ if and only if µ is optimal. The contraction property also implies that the value iteration (VI) algorithms

(1.6)

J k+1 = TµJ k,

J k+1 = T J k

generate sequences {Jk} that converge to Jµ and J, respectively, from any starting vector J0  n. Policy iteration (PI) is a major alternative to VI, and generates a sequence of policies. In the classical
form of the algorithm, the current policy µ is improved by finding µ~ that satisfies

Tµ~Jµ = T Jµ

[i.e., by minimizing for all x in the right-hand side of Eq. (1.2) with Jµ in place of J]. The improved policy µ~ is evaluated by solving the linear system of equations Jµ~ = Tµ~Jµ~, and then (Jµ~, µ~) becomes the new cost vector-policy pair, which is used to start a new iteration. Thus the PI algorithm starts with a policy µ0 and generates a sequence {µk} according to

Jµk = Tµk Jµk ,

Tµk+1 Jµk = T Jµk , 3

(1.7)

where on the left we have the policy evaluation equation, and on the right we have the policy improvement equation.

The preceding PI algorithm has several weaknesses, which make it unsuitable for problems with a

large number of states n. The principal difficulty is that at each iteration k, the current policy µk must

be evaluated at all states, i.e., Jµk (x) must be computed for all x. A second difficulty is that the policy improvement operation

n

µk+1(x)  arg min pxy(u)
uU(x) y=1

g(x, u, y) + Jµk (y)

,

x = 1, . . . , n,

(1.8)

must be performed at each state x, which makes it unsuitable for problems where the number of state-control

pairs (x, u) is large. The purpose of this paper is to propose variants of the PI algorithm that alleviate both

of these difficulties.

2. ON-LINE VARIANTS OF POLICY ITERATION

We introduce a PI algorithm, which starts at time 0 with a state-policy pair (x0, µ0) and generates on-line a sequence of state-policy pairs (xk, µk). We view xk as the current state of a system operating on line under the influence of the policies µ1, µ2, . . .. In our algorithm, µk+1 may differ from µk only at state xk. The control µk+1(xk) and the state xk+1 are generated as follows:

We consider the Q-factors

n
Qµk (xk, u) = pxky(u) g(xk, u, y) + Jµk (y) ,
y=1

(2.1)

and we select the control µk+1(xk) from within the constraint set U (xk) with sufficient accuracy to satisfy the sequential improvement condition

Qµk xk, µk+1(xk)  Jµk (xk), with strict inequality whenever this is possible. For x = xk the policy controls are not changed:

(2.2)

µk+1(x) = µk(x) for all x = xk.  By this we mean that if minuU(xk) Qµk (xk, u) < Jµk (xk) we select a control uk that satisfies
Qµk (xk, uk) < Jµk (xk),

(2.3)

and set µk+1(xk) = uk, and otherwise we set µk+1(xk) = µk(xk) [so Eq. (2.2) is satisfied]. Such a control selection may be obtained by a number of schemes, including brute force calculation and random search based on Bayesian optimization. The needed values of the Q-factor Qµk and cost Jµk may be obtained in several ways, depending on the problem at hand, including by on-line simulation.

4

The next state xk+1 is generated randomly according to the transition probabilities pxkxk+1 µk+1(xk) . We first show that the current policy is monotonically improved.

Proposition 2.1: We have Jµk+1 (x)  Jµk (x), for all x and k,
with strict inequality for x = xk (and possibly other values of x) if minuU(xk) Qµk (xk, u) < Jµk (xk).

Proof: The policy update is done under the condition (2.2). By using the monotonicity of Tµk+1, we have

for all   1,

Tµk++11 Jµk  Tµk+1 Jµk  Jµk ,

(2.4)

so by taking the limit as    and by using the convergence property of VI (Tµk+1 J  Jµk+1 for any J ), we obtain Jµk+1  Jµk . Moreover, the algorithm selects µk+1(xk) so that

(Tµk+1 Jµk )(xk) = Qµk (xk, uk) < Jµk (xk)

if

min
uU (xk)

Qµk (xk,

u)

<

Jµk

(xk ),

[cf. Eq. (2.3)], so that by using Eq. (2.4), we have Jµk+1 (xk) < Jµk (xk). Q.E.D.

Convergence to a Locally Optimal Policy
We next discuss the convergence and optimality properties of the algorithm. We introduce a definition of local optimality of a policy, whereby the policy selects controls optimally only within a subset of states.

Definition 2.1: Given a subset of states X and a policy µ, we say that µ is locally optimal over X if µ is optimal for the problem where the control is restricted to take the value µ(x) at the states x / X, and is allowed to take any value u  U (x) at the states x  X.
Roughly speaking, µ is locally optimal over X, if µ is acting optimally within X, but under the (incorrect) assumption that once the state of the system gets to a state x outside X, there will be no option to select control other than µ(x). Thus if the choices of µ outside X are poor, its choices within X may also be poor.
5

Mathematically, µ is locally optimal over X if

n
Jµ(x) = min pxy(u) g(x, u, y) + Jµ(y) ,
uU(x) y=1

n
Jµ(x) = pxy µ(x)
y=1
which can be written compactly as

g x, µ(x), y + Jµ(y) ,

for all x  X, for all x / X,

(TµJµ)(x) = (T Jµ)(x), for all x  X.

(2.5)

Note that this is different than (global) optimality of µ, which holds if and only if the above condition holds for all x = 1, . . . , n, rather than just for x  X [cf. Eq. (1.6)]. However, it can be seen from Definition 2.1 that a (globally) optimal policy is also locally optimal within any subset of states.
Our main convergence result is the following.

Proposition 2.2: Let X be the subset of states that are repeated infinitely often within the sequence {xk}. Then the corresponding sequence {µk} converges finitely to some policy µ in the sense that µk = µ for all k after some index k. Moreover µ is locally optimal within X, while X is invariant under µ, in the sense that
pxy µ(x) = 0 for all x  X and y / X.

Proof: The cost function sequence {Jµk } is monotonically nondecreasing (cf. Prop. 2.1). The number of policies µ is finite in view of the finiteness of the state and control spaces. Therefore, the number of corresponding functions Jµ is also finite, so Jµk converges in a finite number of steps to some J¯, which in view of the algorithm's construction [selecting uk = µk(xk) if minuU(xk) Qµk (xk, u) = Jµk (xk); cf. Eq. (2.3)], implies that µk will remain unchanged at some µ with Jµ = J¯ after some sufficiently large k.
We will show that the local optimality condition (2.5) holds for X = X and µ = µ. In particular, we have xk  X and µk = µ for all k greater than some index, while for every x  X, we have xk = x for infinitely many k. It follows that for all x  X,

Qµ x, µ(x) = Jµ(x),

(2.6)

while by the construction of the algorithm,

Qµ x, u  Jµ(x), for all u  U (x), 6

(2.7)

since the reverse would imply that µk+1(x) = µk(x) for infinitely many k [cf. Eq. (2.3)]. Condition (2.6) can be written as Jµ(x) = (TµJµ)(x) for all x  X, and combined with Eq. (2.7), implies that (TµJµ)(x) = (T Jµ)(x) for all x  X. This is the local optimality condition (2.5) with X = X and µ = µ.
To show that X is invariant under µ, we argue by contradiction: if this were not so, there would exist a state x  X and a state y / X such that
pxy µ(x) > 0,
implying that y would be generated following the occurrence of x infinitely often within the sequence {xk}, and hence would have to belong to X (by the definition of X). Q.E.D.

Note an implication of the invariance property of the set X shown in the preceding proposition. We have that µ is (globally) optimal under the assumption that for every policy there is no strict subset of states that is invariant.

A Counterexample to Global Optimality

The following deterministic example  shows that the policy µ obtained by the algorithm need not be (globally) optimal. Here there are three states 1, 2, and 3. From state 1 we can go to state 2 at cost 1, and to state 3 at cost 0, from state 2 we can go to states 1 and 3 at cost 0, and from state 3 we can go to state 2 at cost 0 or stay in 3 at a high cost (say 10). The discount factor is  = 0.9. Then it can be verified that the optimal policy is
µ(1) : Go to 3, µ(2) : Go to 3, µ(3) : Go to 2,

with optimal costs

J (1) = J (2) = J (3) = 0,

while the policy

µ(1) : Go to 2, µ(2) : Go to 1, µ(3) : Stay at 3,

is strictly suboptimal, but is locally optimal over the set of states X = {1, 2}. Moreover our on-line PI algorithm, starting from state 1 and the policy µ0 = µ, oscillates between the states 1 and 2, and leaves the policy µ0 unchanged. Note also that X is invariant under µ, consistently with Prop. 2.2.

On-Line Variants of Policy Iteration with Global Optimality Properties
To address the local versus global convergence issue illustrated by the preceding example, we consider an alternative scheme, whereby in addition to uk, we generate an additional control at a randomly chosen
 Thanks are due to Yuchao Li for constructing this example and for commenting on other aspects of the paper. 7

state xk = xk. In particular, assume that at each time k, in addition to uk and xk+1 that are generated according to Eq. (2.3), the algorithm generates randomly another state xk (all states are selected with positive probability), performs a policy improvement operation at that state as well, and modifies accordingly µk+1(xk). Thus, in addition to a policy improvement operation at each state within the generated sequence {xk}, there is an additional policy improvement operation at each state within the randomly generated sequence {xk}.
Because of the random mechanism of selecting xk, it follows that at every state there will be a policy improvement operation infinitely often, which implies that the policy µ ultimately obtained is (globally) optimal. Note also that we may view the random generation of the sequence {xk} as a form of exploration. The probabilistic mechanism for generating the random sequence {xk} may be guided by some heuristic reasoning, which aims to explore states with high cost improvement potential.
3. CONCLUDING REMARKS
We have developed a new on-line PI algorithm, which has a cost improvement property at each iteration, and offers some optimality guarantees. The algorithm is, to our knowledge, the first on-line proposal of the PI algorithm, which in its exact form relies on off-line computation. The algorithm is motivated and inspired by the rollout algorithm (a single iteration of PI), which is well-suited for on-line implementation; see the books [BeT96], [Ber17], [Ber19], [Ber20], and the references given there. Note that if there were no policy updates at all, our algorithm would become equivalent to the rollout algorithm with base policy µ0. Thus our algorithm may be viewed as an enhanced version of a simplified rollout algorithm, which uses information collected on-line to improve the current base policy (at essentially no additional computational cost). A logical conclusion is that our algorithm should perform no worse than the rollout algorithm, which is known to perform well in practice, as it can be viewed as a single step of Newton's method, which underlies the PI algorithm [Kle68], [PoA69], [Hew71], [PuB78], [PuB79], [Ber20].
The structure of our algorithm suggests interesting possibilities for exploration, as well as combinations with approximation in policy and value space schemes, whereby policy and cost function approximations are constructed using the data that is collected on-line. Value and policy space approximation could also be applied to problems with infinite state and control spaces. There are many possibilities for the use of parallel computation within these contexts. Moreover, the on-line structure of our algorithm makes it suitable for adaptive control contexts, where some of the system and cost parameters may be changing over time.
Other possible variations of our algorithm include PI schemes with multistep lookahead, as well as
 It is also possible to choose multiple additional states at time k for a policy improvement operation, and this is well-suited for the use of parallel computation.
8

optimistic variants, whereby the Q-factors (2.1) are evaluated approximately by using truncated simulation. Extensions to other infinite horizon DP models, such as semi-Markov and average cost problems, are also possible. A particularly interesting class of models is deterministic and stochastic shortest path problems with an additional termination state t. For such problems, each generated system trajectory consists of a finite number of states and terminates at t, so the variant given at the end of the preceding section, which guarantees global optimality in the limit, does not apply. However, it is possible to construct a globally optimal policy by restarting the system from a randomly chosen initial state each time it reaches t. These possibilities are subjects for further research and computational experimentation.
4. REFERENCES
[BeT96] Bertsekas, D. P., and Tsitsiklis, J. N., 1996. Neuro-Dynamic Programming, Athena Scientific, Belmont, MA.
[Ber12] Bertsekas, D. P., 2012. Dynamic Programming and Optimal Control, Vol. II, 4th edition, Athena Scientific, Belmont, MA.
[Ber17] Bertsekas, D. P., 2017. Dynamic Programming and Optimal Control, Vol. I, 4th Edition, Athena Scientific, Belmont, MA.
[Ber18] Bertsekas, D. P., 2018. Abstract Dynamic Programming, 2nd Edition, Athena Scientific, Belmont, MA (can be downloaded from the author's website).
[Ber19] Bertsekas, D. P., 2019. Reinforcement Learning and Optimal Control, Athena Scientific, Belmont, MA.
[Ber20] Bertsekas, D. P., 2020. Rollout, Policy Iteration, and Distributed Reinforcement Learning, Athena Scientific, Belmont, MA.
[Kle68] Kleinman, D. L., 1968. "On an Iterative Technique for Riccati Equation Computations," IEEE Trans. Aut. Control, Vol. AC-13, pp. 114-115.
[Hew71] Hewer, G., 1971. "An Iterative Technique for the Computation of the Steady State Gains for the Discrete Optimal Regulator," IEEE Trans. on Automatic Control, Vol. 16, pp. 382-384.
[PoA69] Pollatschek, M. A., and Avi-Itzhak, B., 1969. "Algorithms for Stochastic Games with Geometrical Interpretation," Management Science, Vol. 15, pp. 399-415.
[PuB78] Puterman, M. L., and Brumelle, S. L., 1978. "The Analytic Theory of Policy Iteration," in Dynamic Programming and Its Applications, M. L. Puterman (ed.), Academic Press, N. Y.
9

[PuB79] Puterman, M. L., and Brumelle, S. L., 1979. "On the Convergence of Policy Iteration in Stationary Dynamic Programming," Mathematics of Operations Research, Vol. 4, pp. 60-69. [Put94] Puterman, M. L., 1994. Markovian Decision Problems, J. Wiley, N. Y.
10

