Locally Valid and Discriminative Confidence Intervals for Deep Learning Models

arXiv:2106.00225v1 [cs.LG] 1 Jun 2021

Zhen Lin University of Illinois at Urbana-Champaign
Urbana, IL 61801 zhenlin4@illinois.edu

Shubhendu Trivedi MIT
Cambridge, MA 02139 shubhendu@csail.mit.edu

Jimeng Sun University of Illinois at Urbana-Champaign
Urbana, IL 61801 jimeng@illinois.edu

Abstract
Crucial for building trust in deep learning models for critical real-world applications is efficient and theoretically sound uncertainty quantification, a task that continues to be challenging. Useful uncertainty information is expected to have two key properties: It should be valid (guaranteeing coverage) and discriminative (more uncertain when the expected risk is high). Moreover, when combined with deep learning (DL) methods, it should be scalable and affect the DL model performance minimally. Most existing Bayesian methods lack frequentist coverage guarantees and usually affect model performance. The few available frequentist methods are rarely discriminative and/or violate coverage guarantees due to unrealistic assumptions. Moreover, many methods are expensive or require substantial modifications to the base neural network. Building upon recent advances in conformal prediction and leveraging the classical idea of kernel regression, we propose Locally Valid and Discriminative confidence intervals (LVD), a simple, efficient and lightweight method to construct discriminative confidence intervals (CIs) for almost any DL model. With no assumptions on the data distribution, such CIs also offer finitesample local coverage guarantees (contrasted to the simpler marginal coverage). Using a diverse set of datasets, we empirically verify that besides being the only locally valid method, LVD also exceeds or matches the performance (including coverage rate and prediction accuracy) of existing uncertainty quantification methods, while offering additional benefits in scalability and flexibility.
1 Introduction
Consider a training set Strain = {(Xi, Yi)}Ni=1 and a test example (XN+1, YN+1), all drawn i.i.d from an arbitrary joint distribution P, with (Xi, Yi)  X × Y for some X  Rd and Y  R. We are interested in the problem of predictive inference: On observing Strain and XN+1, our task is to construct a confidence interval (CI) estimate C^(XN+1) that contains the true value of YN+1 with a (pre-specified) high probability.
The construction of actionable CIs involves two general challenges: First, C^ should be valid, meaning that if the specified probability is 1 - , we expect C^(XN+1) to cover YN+1 at least 1 -  of the time. Moreover, C^ should be discriminative i.e., we expect C^(XN+1) to be narrower for confident
Preprint. Under review.

<5HVSRQVH <5HVSRQVH <5HVSRQVH

0DUJLQDOO\9DOLGEXWQRWGLVFULPLQDWLYH 1RWYDOLGEXWGLVFULPLQDWLYH &RQGLWLRQDOO\YDOLG GLVFULPLQDWLYH

y
PLVVHG FRYHUHG &,

;,QSXW

;,QSXW

;,QSXW

Figure 1: Illustration of possible (good and bad) CIs. The CI on the left is valid, as it covers 90%

of the data. It is however only marginally valid, not reflecting the poor model prediction near the

red cluster. The middle CI is discriminative, and reflects the high error near the red cluster, but its

coverage rate is much lower than the target (thus not valid). The CI on the right addresses both

challenges by stretching "just enough" near the cluster, making it not only discriminative, but also

conditionally valid. We seek to construct CIs of the last type, but constructing exact conditionally

valid CIs in a distribution-free setting is theoretically impossible. We thus relax this goal by instead

aiming for local validity (more details in Section 2).

cases and vice-versa. The width of the CI C^(XN+1) is thus a quantification of the uncertainty. Figure 1 provides an illustration of these notions, with more details in Section 2.2 and 2.3.
While deep learning (DL) models have demonstrated impressive performance over a range of complicated tasks and data modalities, it has remained difficult to quantify the uncertainty for their predictions. For DL predictions to be actionable, uncertainty information is however indispensable, especially in domains like medicine and finance [2]. Apart from requiring validity and discrimination as discussed earlier, two additional challenges exist specifically for DL models. Obviously, any uncertainty estimation method needs to finish reasonably fast to be useful, so the third challenge is scalability. The fourth challenge is accuracy: The uncertainty estimation should not decrease the prediction accuracy of the DL model. Post-hoc methods are ideal because they do not interfere with the base NN prediction at all. These four requirements together constitute a set of essential desiderata for uncertainty quantification in DL.
Existing uncertainty estimation methods for DL rarely address more than one or two of the above requirements. Credible intervals given by posteriors of approximate Bayesian methods such as [42, 15] and Monte-Carlo Dropout [12] are not valid in the frequentist sense [7]. Non-Bayesian methods (like deep ensemble [19]) can share this limitation as well. Most existing methods also interfere with the original model design, loss function and/or training, which could be expensive and decrease the model performance (as verified in our experiments)[12, 1, 7, 19].
To address these requirements, we leverage recent advances in conformal prediction and the classical idea of kernel regression. Conformal prediction, pioneered by Vovk [39], is a powerful approach for constructing valid CIs. Such methods usually use the prediction errors from a hold-out set to construct C^(XN+1), which would be valid if future data point (XN+1, YN+1) follows the same distribution as data in the hold-out set. This framework is particularly suitable for deep learning due to its distribution-free nature, and motivated many recent uncertainty quantification works in deep learning for both classification and regression tasks [1, 23, 3, 11]. However, most conformal methods are only marginally valid [25, 39, 20, 4]. We however seek to construct a CI conditioning on the input (similar to the third CI in Fig. 1). Moreover, less-than-meticulous applications to DL can break distributional assumptions and theoretical validity, as in the case of [1] (See discussion in Appendix).
Summary of Contributions: We propose Locally Valid Discriminative Confidence Interval (LVD), a simple uncertainty estimation method combining recent advances in conformal prediction and the classical idea of kernel regression. LVD applies to almost all DL models, and is the first method that satisfies all four aforementioned requirements:

· Validity: LVD has frequentist coverage guarantee (not just marginal, but approximately conditional).

· Discrimination: The width of the CIs given by LVD adapts to the risk/uncertainty level of XN+1.

· Scalability: LVD is lightweight, adding limited overhead to the base DL model.

· Accuracy: LVD is post-hoc without requiring model retraining, and does not affect the base

performance of the DL model.

2

2 Preliminaries
2.1 Learning Setup and Assumptions
We assume data and response pairs (X, Y )  X × Y have a joint distribution denoted P, with the marginal distributions of Y and X and the conditional distribution Y |X denoted as PY , PX , and PY |X , respectively. Further, we will define Zi := (Xi, Yi) for concision.
Assuming that we already have an algorithm (with all the training protocols folded in), such as a Deep Neural Network (DNN), that provides a mean estimator µ^NN (x) : X  Y. Given a target coverage level 1 -   (0, 1), our task is to also construct a confidence interval estimator function C^(x) : X  {subset of Y} that has the validity and discrimination properties as defined below.

2.2 Validity (Frequentist Coverage)

There are several (related) notions for a CI to be valid - marginal, conditional, and local. Given target level 1 - , we say C^ has the marginal coverage guarantee (or, equivalently, is marginally valid) if

P{YN+1  C^(XN+1)}  1 - 

(1)

where the probability is taken over the training data and (the unseen) (XN+1, YN+1).

A limitation of marginal coverage is that it is not conditioned on XN+1. A more desirable, albeit stronger, property would be conditional coverage at 1 - :

P{YN+1  C^(XN+1)|XN+1 = x}  1 -  for almost all x  X

(2)

Here the probability is taken over the training data and YN+1 (with XN+1 fixed). It is thus clear that conditional coverage implies marginal coverage but not the other way around. Indeed, a C^ with marginal coverage property only implies a 1 -  chance of being accurate on average across
all data points (marginalizing over XN+1) ­ there might be a sub-population in the data for which the coverage is completely missed. Unfortunately, it is impossible to achieve distribution-free finite-
sample conditional coverage (E.q. 2) in a non-trivial way. Indeed, it is known that a finite-sample estimated C^(x) cannot achieve conditional coverage, unless it produces infinitely wide confidence intervals in expectation under any non-discrete distribution P [38, 22, 5].

It is thus reasonable to instead seek approximate conditional coverage. As might be apparent, there is considerable freedom in defining an appropriate notion of "approximate", depending on the specific tasks and domains. However, a sufficiently general-purpose and natural notion involves using a kernel function K : X × X  R and a center x  X , like the relaxation given in [32]:

P{YN+1  C^(x )|XN+1 = x}K(x, x )dPX (x)  1 - 

(3)

K(x, x )dPX (x)

with the probability (P in the integral) taken over all training samples and YN+1, with

(XN+1, YN+1)  P~ = P~X × PY |X . Here P~X is just the distribution re-weighted by the ker-

nel with a center x , defined by

dP~X (x) dx



dPX (x) dx

K (x

,

x).

Instead

of

choosing

x

beforehand, if we

let the center be XN+1 and fold the integral into P like in [13], we arrive at the definition of local

coverage:

P{Y~N+1  C^(XN+1)|XN+1 = x }  1 - .

(4)

Here the probability integrates over all training data and an additional (X~N+1, Y~N+1)  P~ defined above. Intuitively, this definition means C^(XN+1) is valid "on average" within a small neighborhood
of XN+1. Note that Eqs. 1 and 2 reduce to Eq. 4 with K being constant and delta functions, respectively. In the rest of the paper, we will call C^ marginally/conditionally/locally valid if it
satisfies Eq. 1/2/4 respectively, and we will pursue finite-sample local validity.

2.3 Discrimination

The idea of discrimination is simple: If the error of our prediction µ^(x) is high for an input x, the CI should be wide, and vice versa. Formally, following [1], we require

E[W (C^(x))]  E[W (C^(x ))]  E[ (y, µ^(x))]  E[ (y , µ^(x ))]

(5)

3

Here the expectation is taken over the training data, W is a measure of the width of the CI, and is a loss function such as MSE. This property can be verified (as shown in Section 4) by checking how well W (C^(x)) could predict the magnitude of the error. Discrimination could be considered a measure of efficiency, as a good C^ could "save" some width when the expected risk is low. However, it only makes sense to compare efficiency if all else is equal (i.e. two marginally valid CIs estimators with the same error)Note that although discrimination could be related to conditional/local validity, they are not the same - e.g., a CI that is always infinitely wide is conditionally valid, but not discriminative.
Our goal is to achieve both local validity and discrimination without making any assumptions about the underlying distribution P (i.e., in a distribution-free setting). As noted in Section 1, our method should also run fast and not affect the performance of underlying neural network model µ^NN .

3 Method: Locally Valid Disciminative confidence intervals (LVD)

Overview: We first train a deep neural network (DNN) µ^NN (if not already given), followed by a post-hoc training of an appropriately chosen kernel function K. Specifically, we learn K in a non-parametric kernel regression setting using embeddings from the deep learning model while optimizing for the underlying distance metric that the kernel function leverages. Both of these steps are explicated in more detail in Section 3.1. Armed with µ^NN , we proceed to utilize a hold-out set to collect prediction residuals, which are used with the learned K (along with its distance metric) to build the final CI for any datum at inference time (Section 3.2). We then show the finite-sample local validity and asymptotic conditional validity in Section 3.4.

3.1 Training

At the onset, we partition Strain of N data points into two sets - Sembed and Sconformal. We will denote Sembed as {Zi}ni=1 and Sconformal as {Zn+i}m i=1, where m = N - n. Sembed is used to learn an embedding function f and a kernel K, and Sconformal is used for conformal prediction.
[Optional] Training an Embedding Function: Instead of training a deep kernel in a kernel regres-
sion directly, which can be prohibitively expensive, we split the training task into two steps: training the (expensive) DNN, and training the kernel K. Specifically, we first train a DNN mean estimator µ^NN : X  Y to solve the supervised regression task with the mean squared error (MSE) loss. Note that µ^NN can be based on any existing model. Moreover, this step could be skipped if we are already provided with a pre-trained µ^NN . Then, we remove the last layer of µ^NN and produce an embedding function f : X  Rh for some positive integer h. If the original model µ^NN is good, usually such an embedding provides a rich and discriminative representation of the input (as will be
verified empirically in Section 4).

Training the Kernel: Fixing the embedding funtion f , we perform the leave-one-out NadarayaWatson [24][14][40] kernel regression with a learnable Gaussian kernel on Sembed:

y^iKR =

j=i,j[n] yj Kf (xi, xj ) j=i,j[n] Kf (xi, xj )

(6)

1

-d(f (xi),f (xj ))

where Kf (xi, xj) = K(f (xi), f (xj)) =  e  2

2

and [n] := {1, . . . , n} (7)

Here d(·, ·) is a Mahalanobis distance parameterized by a positive-semidefinite matrix W 0, which

is learned. To avoid solving an expensive semi-definite program, instead of working with W directly,

we work with a low-rank matrix A  Rh×k such that W = AT A, yielding the following equivalent

distance formulation:

d(f (xi), f (xj)) = A(f (xi) - f (xj)) 2

(8)

This parameterization of K is similar to that in [41]. Finally, to train K, we minimize the MSE loss.

Residual Collection: In this step, we take the trained embedding function and kernel, denoted as Kf for simplicity, and apply it on Sconformal. i  [m], we compute the absolute residual

Ri = |yn+i - y^n+i|

(9)

It is important to remark that y^ does not have to be y^KR. The main purpose of the previous step is to train the K, and y^ could still be obtained through the original DNN y^ = µ^NN (x), or any estimator

4

not trained on Sconformal. As a result, the accuracy can only improve (if y^KR turns out to be a better mean estimator)1.

3.2 Inference

Before proceeding further, we recall a useful definition and fix some necessary notation. For

a distribution with cumulative density function (cdf) F defined on the augmented real line R  {-, }, the quantile function is defined as Q(, F ) = F -1(). This definition is the same for

a finite distribution like the empirical distribution . Suppose the empirical distribution consists of

R1, . . . , Rm, then we denote the empirical distribution F^ and the empirical quantile Q(, F^) as:

F^ = 1 m

m

Ri

and

Q(, F^) = inf F^(r)  
r

(10)

i=1

where R(r) = 1{r  R}. Note that we treat {Ri}m i=1 as an unordered list. Besides, Ri can be
±, and can repeat. Finally, we can assign weights to Ri, and define the quantiles for a weighted

distribution:

m

m

F~ = wiRi where

wi = 1

(11)

i=1

i=1

Split Conformal: Before presenting the detailed construction of the CI in LVD, it would be particularly instructive first to consider a special case. Specifically, when K returns a constant number for any (xi, xj), we recover the well-known "split conformal" method [25, 39, 20], which uses the 1 -  quantile of the residuals as the CI width. Following our setup, the split conformal CI is given by:

C^split(XN+1) =

y  R : |y - y^N+1|  Q

1 1 - ,
m+1

m
 + Ri

(12)

i=1

Because the residuals {Ri}i[m]  {RN+1} are i.i.d., RN+1's ranking among them is uniformly distributed. We cannot know RN+1, so we use  instead to be "safe" (r  R, (r) = 0). It follows that C^split is (1 - ) marginally valid [25].
Local Conformal: In order to achieve the local coverage property, all we need to do is to simply re-weigh the residuals. The new CI is given by:

m

C^LV D(XN+1) = y  R : |y - y^N+1|  Q 1 - , wN+1 + wn+iRi

(13)

i=1

where

wj

=

Kf (xj , xN+1)

Kf (xN+1, xN+1) +

m i=1

Kf

(xn+i

,

xN

+1)

(14)

In other words, we first assign weights to {Ri} based on the similarity between {Xn+i} and XN+1
using Kf , and then set the width to be the weighted quantile . Note that with , C^LV D(XN+1) will be infinitely wide if data is scarce around XN+1. However, as argued in [13], this is desired.

3.3 Implementation Details
Parameterization and Training: Since A is intricately linked to the computation of the weights assigned by the Gaussian kernel K (E.q. 8), it is implemented as K(f (xi), f (xj)) = e-||A(f(xi)-f(xj))||2 . In order to optimize for A, we treat it as a usual linear layer in a neural network and perform gradient descent.
Smoothness Requirement: In the context of obtaining locally valid confidence intervals, a potential drawback of using the Nadaraya-Watson kernel regression framework is that the Kf will not meaningfully learn similarity of any input with itself. For example, we can arbitrarily define Kf (x, x) to be any value, including . In the context of only regression, the fitted function's performance will not change as long as there are no two identical xi. With Gaussian kernel, this issue is somewhat mitigated. However, during the training, the K(xi, xi) can still be too high compared with K(xi, xi + ),
1As will be shown in the Appendix, y^KR is often preferable because of the distance information it encodes.

5

resulting in a less meaningful definition for local coverage. We could then enforce a regularization by replacing the y^i in E.q. 6 with

y^iKR

=

y-iKf (xi, xi) + Kf (xi, xi) +

j=i,j[n] yj Kf (xi, xj ) j=i,j[n] Kf (xi, xj )

1

where y-i = n - 1

yj

j=i,j[n]

(15)

This can be considered an explicit bias term towards the (leave-one-out) sample mean.

Complexity: To facilitate training, we use stochastic gradient descent instead of gradient descent with batch size denoted as B1. Furthermore, if the dataset size is prohibitively large, we can also randomly sample a subset of B2 points {xj}j=i to predict y^i. The total complexity is O(B1B2hk) where h and k, defined earlier, denote the dimensionality of the embedding before/after it is multiplied by A. Note that B2 = O(1) or o(N ). The inference time for each data point can be improved from O(B2hk) to O(B2k + hk) by storing Axj instead of A and xj separately.
Denoting the number of parameter of the base NN as P , since DL models are usually overparameterized, the additional training time for each descent could be comparable or shorter than training the base NN (depending on the relation between P and B2hk)2, and the additional inference time would be much shorter than that of the base NN model. In addition, most of these factors (especially B2) can be easily parallelized. The full procedure is summarized below in Algorithm 1.

Algorithm 1 LVD

Input: Strain: A set of observations {Zi = (Xi, Yi)}Ni=1 : Parameter specifying (local) target coverage rate
XN+1: Unseen data point Output: A locally valid CI, C^(XN+1).
Training:

[Optional] Randomly split Strain into Sembed and Sconformal. Denote Sconformal as {Zn+i}m i=1 [Optional] Train a NN regression model µ^NN on Sembed. Remove the last layer of µ^NN to get an embedding function f
Train A on f (Sembed) in a Nadaraya-Watson kernel regression setting, with kernel Kf (x1, x2) = e-||A(f (x1)-f (x2))||2
Collect residuals Ri = |yn+i - y^n+i| for i  [m]

Inference:

Compute CI as C^(XN+1) = y  R : |y - y^N+1|  Q

where wj

=

Kf (xj , xN+1)

Kf (xN+1, xN+1) +

m i=1

Kf

(xn+i

,

xN

+1)

m
1 - , wN+1 + wn+iRi
i=1

3.4 Theoretical Guarantees

We conclude this section by showing that C^LV D provides the local coverage property. We adapt the Theorem 5.1 in [13] and results in [32] to our setting. The detailed proof is deferred to the Appendix:

Theorem 3.1. Conditional on XN+1, the CI obtained from Algorithm 1, C^LV D(XN+1), satisfies

P{Y~N+1  C^LV D(XN+1)|XN+1 = x }  1 -  for any x

(16)

where the probability is taken over all the training samples i.i.d. P = PY |X ×PX , and (X~N+1, Y~N+1)

with distribution X~N+1|XN+1



P XN+1
X

and Y~N+1|X~N+1



PY |X .

Here

P XN+1
X

means the

localized

distribution

with

dPXXN+1 (x) dx



dPX (x) dx

Kf

(XN

+1

,

x).

With some regularity assumptions like in [22], we also have asymptotic conditional coverage:

2In practice, since f is already well-trained, the training of Kf converges very fast.

6

Table 1: Features of different methods. CQR and MADSplit achieve strict finite-sample marginal
coverage. Unlike LVD, no baseline is locally valid. LVD, MADSplit and CQR are discriminative,
DJ is not, and DE/CMDP/PBP are supposed to be but usually fail to in our experiment. Among the post-hoc methods, LVD and MADSplit have reasonable overhead, DJ's overhead is usually O(N ) where N is the number of training data (and extremely large memory consumption). CQR is not post-hoc because its CI may not contain µ^NN (X).

LVD MADSplit CQR

DJ

DE

MCDP

PBP

Valid Discriminative
Post-hoc Overhead (if post-hoc)

Local Low

Marginal Low

Marginal
× N/A

no guarantee ×
Very High

× sometimes
× N/A

× sometimes
× N/A

× sometimes
× N/A

LVD (Covered 93.0%)

MADSplit (Covered 88.0%)

CQR (Covered 97.0%)

DJ (Covered 98.0%)

20

20

20

20

15

15

15

15

10

10

10

10

5

5

5

5

0

0

0

0

5

5

5

5

10

10

10

10

1.0 0.5 D0.0E (C0.o5ve1r.e0d 51.65.0%2.0) 2.5 3.0 1.0 0.5M0C.0DP0(.C5 ov1e.0red1.155.20.%0 ) 2.5 3.0 1.0 0.5 P0B.0P (0C.5ove1r.0ed 13.57.02%.0) 2.5 3.0 1.0 0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0

20

20

20

15

15

15

10

10

10

5

5

5

0

0

0

5

5

5

y missed covered 90% CI

10

10

10

1.0 0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 1.0 0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 1.0 0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0

Figure 2: LVD, CQR and MADSplit achieve marginal coverage. Although without theoretical

guarantee, DJ usually marginally covers in practice with near constant width (not discriminative).

DE, MCDP and PBP do not show validity as expected. Among valid CIs, only LVD tries to capture

the less representative points by wider CIs, arguably showing the most useful discriminative pattern.

Theorem 3.2. With appropriate assumptions, C^LV D is asymptotically conditional valid.
The detailed assumptions, formal statement and proof of Theorem 3.2 are deferred to the Appendix.
Remark: Roughly speaking, Theorem 3.1 tell us that the response Y of a new data point sampled "near" XN+1 will fall in our CI with high probability. Theorem 3.2 further states that, under suitable assumptions and enough data, C^LV D also covers YN+1 (i.e. no re-sampling) with high probability.

4 Experiments
Baselines: We compare LVD with the following baselines (with a qualitative comparison in Table 1):
1. Discriminative Jackknife (DJ) [1], which claims to be both discriminative and marginally valid, but is neither (See Appendix C).
2. Deep Ensemble (DE)[19], a non-bayesian method that trains an ensemble of networks to estimate both variance and mean.
3. Monte-Carlo Dropout (MCDP) [12], a popular bayesian method for NN that performs Dropout [31] at inference time for the predictive variance estimate.
4. Probabilistic Backpropagation (PBP)[15], a successful method to train Bayesian Neural Networks by computing a forward propagation of probabilities before a backward computation of gradients.
5. Conforamlized Quantile Regression (CQR)[29], an efficient (narrow CI) marginally valid conformal method that takes quantile predictors instead of mean predictors. This comes with a huge cost: one needs to retrain the predictor for each  if more than one coverage level is desired.
6. MAD-Normalized Split Conformal (MADSplit)[21, 8], a variant of the well-known split-conformal method that requires an estimator for the mean absolute deviation (MAD), and perform the conformal prediction on the MAD-normalized residuals.
In our experiments, CIs for non-valid methods are obtained from the quantile functions of the posterior for target coverage 1 -  like in [1].

7

4.1 Synthetic Data
We will first examine the dynamics of different uncertainty methods with synthetic data. The formula we use is the same as in [15, 1]: y = x3 + . Here,  N (0, 42), and x comes from U nif [-1, 1] with probability 0.9, and half-normal distribution on [1, ) with  = 1 with probability 0.1. We used this PX to illustrate local validity. The results are shown in Figure 2. We observe that LVD, CQR, MADSplit, and DJ all achieve close to 90% coverage. However, LVD gives a more meaningful discriminative confidence band: Specifically, near the boundaries, it will give us wider intervals (sometimes ) because there is little similar data around, which is desirable for local validity. Although CQR and MADSplit can be discriminative, they are still only marginally valid, so we can see that despite the varying width, they actually get narrower when x is more eccentric, which is clearly an issue. DJ essentially gives CIs of constant width, as estimated from the quantile of the residuals. DE also does not give meaningful uncertainty estimates, giving almost constant CIs that cover well below 90%. For Bayesian methods, MCDP behaves much like a Gaussian Process (as claimed in [12]), with low coverage rate, whereas PBP is mildly discriminative and not valid3.

4.2 Real Datasets

Table 2: Size of each dataset. Size of the test set is in parenthesis.

Yacht Housing Energy Bike

Kin8nm Concrete QM8

QM9

308 (62) 506 (101) 768 (154) 17379(3476) 8192 (1638) 1030 (206) 21786 (4357) 133719 (26744)
We will be using a series of standard benchmark datasets in the uncertainty literature[1, 29, 15], including: UCI Yacht Hydrodynamics (Yacht)[37], UCI Bikesharing (Bike) [34], UCI Energy Efficiency (Energy)[36], UCI Concrete Compressive Strength (Concrete)[35], Boston Housing (Housing)[9], Kin8nm[16].We also use QM8 (16 sub-tasks) and QM9 (12 sub-tasks) [28, 30, 27] as examples of more complicated datasets. In each experiment, 20% of the data is used for testing. The sizes of datasets used are shown in Table 2. We use the same DNN model for all baselines, which has 2 layers, 100 hidden nodes each layer, and ReLU activation for the non-QM datasets. For QM8 and QM9, we use the molecule model implemented in [43] and apply applicable baselines. Missing baselines ("­" in the tables) are either too expensive (time and/or memory) or requires significant redesign of the training and NN, which is beyond the scope of this paper.

Evaluation Metrics: The evaluation is based on validity and discrimination. For validity, we check the marginal coverage rate (MCR) and the tail coverage rate (TCR), which is defined as the coverage rate for data whose Y falls in the top and bottom 10%. The motivation behind TCR is that, if our local validity is very close to conditional validity, then LVD's coverage rate would be above target in any pre-defined sub-samples, including those with extreme Y s. For discrimination, to verify Eq. 5, which is a prediction task, we compute the AUROC of using the CI width to predict whether the absolute residual is in the top half of all residuals. AUROC alone is misleading, however, as a bad predictor can easily be discriminative (e.g. by randomly adding to both its prediction and CI width a huge constant). Therefore, we also check mean absolute deviation/residual (MAD).

We repeat all experiments 10 times and report mean and standard deviations. For QM8 and QM9, we report the average numbers across all sub-tasks, with breakdown on each sub-task in the Appendix.

Results: For validity, as shown in Table 3, LVD achieves marginal coverage empirically, as well as MADSplit4 , CQR, and DJ. However, for tail coverage rate, only LVD consistently covers at or above
target coverage rates. For the larger datasets, both coverage rates tend to get close to 90% for LVD.
DE, MCDP and PBP do not achieve meaningful coverage (either too high or too low).

For discrimination (Table 4), LVD is generally in the top two while maintaining the lowest MAD almost always. MADSplit has the same MAD as LVD (using the same µ^NN ), and has similar
AUROC as LVD despite explicitly modeling MAD. Other baselines occasionally show significant
discriminative property, but usually have much higher MAD. Despite training an ensemble of models,

3Sometimes it may be possible to calibrate Bayesian methods[29]. However, one needs to calibrate the entire posterior for the Bayesian method to makes sense. Moreover, from our experiments it is impossible to do this in the case of MCDP, when it behaves like a Gaussian process and predicts zero variance near known data.
4It is worth noting that MADSplit, despite the theoretical guarantee, misses on the Yacht dataset, because the MAD-predictor predicts a "negative" absolute residual for some subset of the data, thus creating extremely narrow CIs, even after requiring the prediction to be positive and the "practitioner's trick" mentioned in [29].

8

Table 3: Marginal coverage rate (MCR) and Tail coverage rate (TCR) (coverage rate for left and right 10% tail for test label) with target at 90%. "­" represents not-applicable models (see Section 4.2). Coverage rates not significantly lower than target at p = 0.05 are in bold (good). Note that the too high is not better. For example, MCDP either greatly over- or under-covers with MCR either 100% or well below 90%.

MCR
Yacht Housing Energy Bike Kin8nm Concrete QM8* QM9*
TCR
Yacht Housing Energy Bike Kin8nm Concrete QM8* QM9*

LVD
96.8±2.2 96.8±2.9 94.0±1.6 90.4±0.8 98.0±0.6 97.4±1.3 92.6±0.9 90.3±0.6
LVD
98.5±3.2 96.2±4.4 86.8±5.8 90.2±1.7 97.2±1.6 97.1±3.4 90.8±1.9 89.7±2.5

MADSplit
82.4±7.1 90.6±3.5 90.3±2.5 89.9±0.6 90.0±0.8 88.8±2.8 90.0±0.7 90.0±0.2
MADSplit
65.4±23.8 87.6±8.8 78.4±10.9 89.2±3.5 86.4±2.6 83.9±7.3 86.3±2.4 86.1±3.0

CQR
91.5±4.7 91.7±3.3 90.3±2.2 89.8±0.7 90.2±0.6 88.5±2.3 90.0±0.6 90.0±0.3
CQR
77.7±12.3 82.9±8.2 73.5±12.0 58.7±7.3 85.2±2.2 85.4±6.2 80.0±5.9 79.7±8.9

DJ
95.0±2.1 97.6±1.4 96.2±1.8 95.2±0.6 94.7±0.4 98.0±1.6
­ ­
DJ
76.2±9.9 90.0±5.2 90.0±6.5 85.6±3.3 88.1±1.8 95.6±3.6
­ ­

DE
22.7±6.0 96.0±1.9 98.0±2.7 100.0±0.0 100.0±0.1 97.8±1.0 100.0±0.0 60.7±46.8
DE
1.5±4.9 81.9±9.7 95.8±6.3 100.0±0.0 99.9±0.3 91.7±4.8 100.0±0.0 60.3±46.5

MCDP
87.4±4.2 100.0±0.0 100.0±0.0 71.9±0.7 100.0±0.0 100.0±0.0
­ ­
MCDP
50.0±9.8 100.0±0.0 100.0±0.0 49.9±0.0 100.0±0.0 100.0±0.0
­ ­

PBP
80.2±10.8 8.1±4.3 7.2±5.9 0.6±0.2 100.0±0.0 3.3±0.8 ­ ­
PBP
70.0±14.3 1.0±3.0 9.7±12.7 0.0±0.0 100.0±0.0 3.4±5.7
­ ­

Table 4: At p = 0.05, AUROCs (in predicting error being greater than 50% percentile) that are significantly higher than 50%, and mean absolute deviations (MAD) significantly lower than the second-best, are in bold. LVD, MADSplit and CQR are consistently discriminative, but CQR sometimes incurs high MAD. DJ is not discriminative, whereas other methods occasionally demonstrate discrimination but usually have high MADs as well.

AUROC LVD MADSplit

CQR

DJ

DE

MCDP

PBP

Yacht Housing Energy Bike Kin8nm Concrete QM8* QM9*

83.5±5.8 59.2±8.5 73.5±6.3 68.2±11.0 60.3±1.1 64.0±6.1 71.3±9.4 62.7±3.6

77.7±9.0 62.0±8.3 72.9±5.6 71.7±8.5 60.4±1.9 63.8±5.7 73.5±6.8 64.9±3.5

84.9±4.6 62.5±6.7 72.1±8.2 84.8±33.5 60.0±2.1 66.0±7.1 65.5±10.3 55.0±14.4

50.0±10.5 49.6±5.9 57.5±8.1 45.8±6.2 49.3±2.2 46.2±4.9
­ ­

59.8±6.4 60.0±6.8 56.1±11.0 86.2±12.5 50.5±2.6 55.9±6.2 91.7±16.9 56.8±28.4

47.2±7.7 42.5±7.8 54.6±5.5 94.3±1.0 54.1±2.5 51.9±3.5
­ ­

82.8±8.8 47.4±3.6 48.2±2.6 48.3±1.0 53.6±4.8 49.7±3.9
­ ­

MAD

LVD MADSplit

CQR

DJ

DE

MCDP

PBP

Yacht 1.90±0.48 1.90±0.48 3.55±0.85 10.15±0.84 11.25±0.81 10.92±0.73 1.80±0.30

Housing 3.31±0.53 3.31±0.53 3.44±0.33 3.69±0.33 4.42±0.39 6.04±0.54 7.94±1.97

Energy 2.99±0.75 2.99±0.75 3.44±1.04 3.19±0.51 3.79±0.31 8.12±0.59 11.66±2.24

Bike 0.04±0.03 0.04±0.03 7.34±3.51 0.05±0.03 3.37±2.90 124.57±2.68 162.21±2.58

Kin8nm 0.07±0.00 0.07±0.00 0.08±0.01 0.09±0.01 0.19±0.01 0.18±0.00 0.22±0.12

Concrete 5.44±0.53 5.44±0.53 6.21±1.05 5.58±0.58 7.22±0.76 13.75±0.69 20.59±3.57

QM8* 0.01±0.01 0.01±0.01 0.03±0.02

­

3.28±5.12

­

­

QM9* 3.69±9.09 3.69±9.09 32.11±50.42

­

268.32±357.01

­

­

DE incurs huge prediction error in many datasets. As noted earlier, AUROC alone is misleading if the MAD is high: MCDP and CQR seem highly discriminative on the Bike dataset, mostly due to the high model error (epistemic uncertainty).
Scalability: For the largest dataset, QM9, the extra inference time of LVD vs inference time of the original NN is 0.65 vs 0.75 second per 1000 samples5 on an NVIDIA 2080Ti GPU. MADSplit on the other hand takes 0.93 second overhead in the most optimized case. That said, any method that finishes within O(1) multiple of the original NN model is usable in practice. For LVD, extra vs original training time is about 1.5 vs 0.75 second per 1000 samples, but because the f is already highly informative, the training of the kernel Kf finishes in very few iterations, resulting in < 5% overhead of the total training time. Note that MADSplit will take strictly  1× time in total, because it needs to train a second model to predict residuals. Like MADSplit, CQR needs to train at least one quantile predictor6, but it needs to train a new predictor for every , which is a huge cost.
5We use the full Sconformal for CI construction, as the inference time is short enough without sampling. 6That is, if one is willing to have mean estimate outside the CI occasionally and consider CQR post-hoc.

9

5 Conclusion
This paper introduces LVD, the first locally valid and discriminative CI estimator for DL, which is also scalable and post-hoc. Because LVD is both valid and discriminative, it can provide actionable uncertainty information for real world application of DL regression models. Moreover, it is easy to apply LVD to almost any DL model, without any negative impact on the accuracy due to its post-hoc nature. Our experiments confirm that LVD generates locally valid CIs that covers subgroups of data all other methods fail to. It also exceeds or matches the performance in discriminative power, while offering additional benefits in scalability and flexibility. We foresee that LVD can enable more real-world applications of DL models, by providing users actionable uncertainty information.
10

References
[1] Ahmed Alaa and Mihaela Van Der Schaar. Discriminative Jackknife: Quantifying Uncertainty in Deep Learning via Higher-Order Influence Functions. In Hal Daumé III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 165­174. PMLR, 2020.
[2] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. Concrete problems in ai safety, 2016.
[3] Anastasios Angelopoulos, Stephen Bates, Jitendra Malik, and Michael I. Jordan. Uncertainty sets for image classifiers using conformal prediction. CoRR, abs/2009.14193, 2020.
[4] Rina Foygel Barber, Emmanuel J Candès, Aaditya Ramdas, and Ryan J Tibshirani. Predictive inference with the jackknife+. The Annals of Statistics, 49(1):486­507, 2021.
[5] Rina Foygel Barber, Emmanuel J. Candès, Aaditya Ramdas, and Ryan J. Tibshirani. The limits of distribution-free conditional predictive inference, 2020.
[6] Samyadeep Basu, Philip Pope, and Soheil Feizi. Influence Functions in Deep Learning Are Fragile, 2021.
[7] M. J. Bayarri and J. O. Berger. The Interplay of Bayesian and Frequentist Analysis. Statistical Science, 19(1):58 ­ 80, 2004.
[8] Anthony Bellotti. Constructing normalized nonconformity measures based on maximizing predictive efficiency. In Alexander Gammerman, Vladimir Vovk, Zhiyuan Luo, Evgueni Smirnov, and Giovanni Cherubin, editors, Proceedings of the Ninth Symposium on Conformal and Probabilistic Prediction and Applications, volume 128 of Proceedings of Machine Learning Research, pages 41­54. PMLR, 09­11 Sep 2020.
[9] The boston housing dataset. http://lib.stat.cmu.edu/datasets/boston. Accessed: 2021-05-27.
[10] Hadi Fanaee-T and Joao Gama. Event labeling combining ensemble detectors and background knowledge. Progress in Artificial Intelligence, pages 1­15, 2013.
[11] Adam Fisch, Tal Schuster, Tommi S. Jaakkola, and Regina Barzilay. Efficient conformal prediction via cascaded inference with expanded admission. In International Conference on Learning Representations, 2021.
[12] Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model uncertainty in deep learning. In 33rd International Conference on Machine Learning, ICML 2016, 2016.
[13] Leying Guan. Conformal prediction with localization, 2020.
[14] László Györfi, Michael Kohler, Adam Krzyzak, and Harro Walk. A Distribution-Free Theory of Nonparametric Regression. Springer Science & Business Media, 2006.
[15] José Miguel Hernández-Lobato and Ryan P. Adams. Probabilistic backpropagation for scalable learning of bayesian neural networks. In Francis R. Bach and David M. Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, volume 37 of JMLR Workshop and Conference Proceedings, pages 1861­1869. JMLR.org, 2015.
[16] Kin family of datasets. http://www.cs.toronto.edu/~delve/data/kin/desc.html. Accessed: 2021-05-27.
[17] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.
[18] Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1885­1894. PMLR, 06­11 Aug 2017.
[19] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In Advances in Neural Information Processing Systems, 2017.
11

[20] Jing Lei, Max G'Sell, Alessandro Rinaldo, Ryan J. Tibshirani, and Larry Wasserman. Distribution-free predictive inference for regression, 2017.
[21] Jing Lei, Max G'Sell, Alessandro Rinaldo, Ryan J. Tibshirani, and Larry Wasserman. Distribution-Free Predictive Inference for Regression. Journal of the American Statistical Association, 2018.
[22] Jing Lei and Larry Wasserman. Distribution-free prediction bands for non-parametric regression. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 76(1):71­96, 2014.
[23] Zhen Lin, Cao Xiao, Lucas Glass, M. Brandon Westover, and Jimeng Sun. SCRIB: set-classifier with class-specific risk bounds for blackbox models. CoRR, abs/2103.03945, 2021.
[24] Elizbar Nadaraya. Nonparametric Estimation of Probability Densities and Regression Curves. Kluwer Academic Publishers, 1989.
[25] Harris Papadopoulos, Kostas Proedrou, Volodya Vovk, and Alex Gammerman. Inductive confidence machines for regression. In Tapio Elomaa, Heikki Mannila, and Hannu Toivonen, editors, Machine Learning: ECML 2002, pages 345­356, Berlin, Heidelberg, 2002. Springer Berlin Heidelberg.
[26] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, highperformance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'AlchéBuc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 8024­8035. Curran Associates, Inc., 2019.
[27] Raghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole von Lilienfeld. Quantum chemistry structures and properties of 134 kilo molecules. Scientific Data, 1, 2014.
[28] Raghunathan Ramakrishnan, Mia Hartmann, Enrico Tapavicza, and O. Anatole von Lilienfeld. Electronic spectra from tddft and machine learning in chemical space. The Journal of Chemical Physics, 143(8):084111, 2015.
[29] Yaniv Romano, Evan Patterson, and Emmanuel Candes. Conformalized quantile regression. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.
[30] Lars Ruddigkeit, Ruud van Deursen, Lorenz C. Blum, and Jean-Louis Reymond. Enumeration of 166 billion organic small molecules in the chemical universe database gdb-17. Journal of Chemical Information and Modeling, 52(11):2864­2875, 2012. PMID: 23088335.
[31] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(56):1929­1958, 2014.
[32] Ryan J. Tibshirani, Rina Foygel Barber, Emmanuel J. Candes, and Aaditya Ramdas. Conformal prediction under covariate shift, 2020.
[33] Athanasios Tsanas and Angeliki Xifara. Accurate quantitative estimation of energy performance of residential buildings using statistical machine learning tools. Energy and Buildings, 49:560­ 567, 2012.
[34] Bike sharing data set. https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+ Dataset. Accessed: 2021-05-27.
[35] Concrete compressive strength data set. http://archive.ics.uci.edu/ml/datasets/ concrete+compressive+strength. Accessed: 2021-05-27.
[36] Energy efficiency data set. https://archive.ics.uci.edu/ml/datasets/energy+ efficiency. Accessed: 2021-05-27.
[37] Yacht hydrodynamics data set. http://archive.ics.uci.edu/ml/datasets/yacht+ hydrodynamics. Accessed: 2021-05-27.
[38] Vladimir Vovk. Conditional validity of inductive conformal predictors. In Steven C. H. Hoi and Wray Buntine, editors, Proceedings of the Asian Conference on Machine Learning, volume 25 of Proceedings of Machine Learning Research, pages 475­490, Singapore Management University, Singapore, 04­06 Nov 2012. PMLR.
12

[39] Vladimir Vovk, Alexander Gammerman, and Glenn Shafer. Algorithmic learning in a random world. Springer US, 2005.
[40] Geoffrey S. Watson. Smooth regression analysis. Sankhya¯: The Indian Journal of Statistics, Series A (1961-2002), 26(4):359­372, 1964.
[41] Kilian Q. Weinberger and Gerald Tesauro. Metric learning for kernel regression. In Marina Meila and Xiaotong Shen, editors, Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics, volume 2 of Proceedings of Machine Learning Research, pages 612­619, San Juan, Puerto Rico, 21­24 Mar 2007. PMLR.
[42] Max Welling and Yee Whye Teh. Bayesian learning via stochastic gradient langevin dynamics. In Proceedings of the 28th International Conference on Machine Learning, ICML 2011, 2011.
[43] Kevin Yang, Kyle Swanson, Wengong Jin, Connor Coley, Philipp Eiden, Hua Gao, Angel Guzman-Perez, Timothy Hopper, Brian Kelley, Miriam Mathea, Andrew Palmer, Volker Settels, Tommi Jaakkola, Klavs Jensen, and Regina Barzilay. Analyzing Learned Molecular Representations for Property Prediction. Journal of Chemical Information and Modeling, 59(8):3370­3388, 2019.
[44] I.-C. Yeh. Modeling of strength of high-performance concrete using artificial neural networks. Cement and Concrete Research, 28(12):1797­1808, 1998.
13

A Proofs

A.1 Proof for Theorem 3.1

In this section we will prove Theorem 3.1. The key idea behind the proof is that since µ^ (let it be µ^NN or µ^KR) and Kf are independent of Sconformal, the residuals (Ri) collected on Sconformal follow the same distribution as a new test residual. Thus, re-weighting PX by Kf precisely fits into the
covariate-shift setting studied by [32] (further studied in [13]), which in turn implies that under the
new (localized) distribution for X, the coverage guarantee holds (Theorem 3.1).

We first introduce a few definitions following the same notation as in [13]. To begin, we de-
fine the score function as V (x, y) := |y - µ^(x)|. Then, we write the localizer function as
H(x, x ) := Kf (x, x ). For convenience, we also will rewrite subscripts of the data, so we have Z1 = (X1, Y1 ), . . . , Zm+1 = (Xm+1, Ym+1), where Zi is just Zn+i for i  [m] and Zm+1 is ZN+1. For {Zi}m i=+11, both V and H would be considered fixed because the training did not use any information from Sconformal. [13] allows for a more general form of H that can depend on Sconformal, which is not needed in our setting.

We proceed to define the weighted residual distributions like in [13]:

m+1

F^i :=

pHi,j V (Xj ,Yj )

j=1

(17)

where pHi,j :=

H(Xi, Xj)

m+1 k=1

H

(Xi ,

Xk )

(18)

Finally, F^ is defined as pHm+1,m+1 +

m i=1

pHm+1,iV

(Xi ,Yi

).

V

(Xm+1,

Ym+1)

can

be

considered

set to , because we don't know the value of Ym+1 and want to be conservative.

Now, our construction of the CI could be rewritten in the following form:

C^LV D(Xm+1) := {y : V (Xm+1, y)  Q(1 - , F^)}

(19)

This is precisely the setup of Theorem 5.1 in [13], and Theorem 3.1 follows from Theorem 5.1 in [13].

A.2 Asymptotic Conditional Validity (Theorem 3.2)

Before we discuss the asymptotic property of C^LV D, we formally define asymptotic conditional validity (from [22]).
Definition 1. (Aymptotic Conditional Validity) Given training data (X1, Y1), . . . , (Xm, Ym), a CI estimator C^m, is asymptotically conditionally valid if

sup P{Ym+1  Cm,(x)|Xm+1 = x} -  P 0

(20)

x

+

as m  , where the sup is taken over the support of PX .

Here, we add the subscript m to C^ to emphasize the dependence on the sample size. If a CI estimator is asymptotically conditionally valid at level 1 - , then given enough samples (as m  ), the probability of C^m, missing the next response Ym+1 converges to  in probability.
To facilitate the discussion, we add a subscript m and denote the CI given by LVD as C^mLV,D. With the setup mentioned in Section A.1, we obtain a result similar to Theorem 5.1 (b) in [13] to for C^mLV,D as well.

Assumptions: We need to make the following assumptions:

(1) Denote W := f (X) as a new random variable in Rh. W is (assumed to be) on [0, 1]h with marginal density bounded from two sides by two constants b1 < b2. In other words, 0 < b1  pW (w)  b2 < .

14

(2) The conditional density of R (the residual) given W is Lipschitz in W . In other words, w, w , pR|W (·|w) - pR|W (·|w )   L w - w .

As might be clear, (1) and (2) are standard regularity assumptions (as in [13, 22]), but stated for our setting. For assumption (1), if W does not fall in [0, 1]h, we can easily fix it by adding a normalization
layer to f . Compared with [13, 22], (2) is not any less likely to hold, as we usually only have one linear layer after f in µ^NN .

Bandwidth (h): To clearly state the theorem, we also need to decompose/unfold our transform

matrix

A

into

two

steps

-

projection

and

rescaling:

A(w - w

)

:=

1 h

A1(w

-

w

),

where

A1

2 = 1.

Note that in our learning, we are mostly learning A1, and h is in fact chosen. In our experiment we

implicitly folded h into A, as changing h entails making an explicit decision on how "local" one

wants the coverage to be when the data is limited, and we do not have a strong prior on this. However,

for the sake of this discussion, as N  , if we keep the same ratio between n = |Sembed| and m = |Sconformal|, then:

· A1 would converge to some fixed unit-norm matrix in Rh×k, and · we could let h  0 like in [13] and [22], because if we have m  , then the number of validation
residuals is large, so we could afford a much more "local" validity with few infinitely wide CIs.

With the assumptions stated above, we are in a position to state the following theorem regarding the asymptotic conditional validity of C^mLV,D:
Theorem A.1. (Asymptotic Conditional Validity. Re-statement of Theorem 3.2): With assumptions (1) and (2), and m  , if we also let h  0, then

 - P{Ym+1  C^mLV,D(Xm+1)} + P 0.

(21)

The proof is essentially the same as that in [13], with key difference that in [13], the Gaussian kernel
only has one bandwidth h, which goes to 0 asymptotically. This has been discussed in the "Bandwidth
(h)" section above. Note the key difference between Theorem A.1 and 3.1 is that the response Ym+1 now belongs to Xm+1, which is used to construct the CI.

15

B Additional Experimental Details
B.1 Training Details
As noted in the paper, the DNN used for most datasets (except QM8 and QM9) has 2 layers, 100 hidden nodes and uses ReLU for the activation function. This is the same architecture as in [1], but with the difference that the activation is ReLU instead of tanh. We make this choice because the code accompanying [1] uses ReLU, and tanh does not train for most of the datasets in our experiments. Recall that the learnable matrix A reduces dimension from h to k. For QM8 and QM9, please refer to [43] for detailed description of the architecture and training protocols. We make the following modifications in order to run some baselines:
· MADSplit: We train a second model after the model in [43] that has the same architecture and training protocol, but tries to predict the absolute error of the first model.
· CQR: We replace the MSE loss with the "pinball" loss mentioned in [29] and simultaneously train two quantiles for the same . For different , we re-train a model.
· DE: We replace the loss with the negative log-likelihood (NLL) loss as suggested in [19], and train an ensemble of 5 models for each experiment.
For all experiments, h is given by the DNN, and we set k = 10. The training of the kernel follows the following protocol: we first take embedding from the training data, compute and fix the mean µi and standard deviation si for each dimension i  [h]. Dimensions with standard deviation <1e-3 are ignored as they are most likely dead nodes (due to ReLU). The embeddings are then always normalized using µi and si before passing through A.
A is implemented as a torch.nn.Linear layer using PyTorch[26] and follows the default initialization. We restrict the kernel regression to use the top 3000 (or all) similar datapoints so the computation can be fast (like in [41]). We use an Adam optimizer [17] implemented in PyTorch, with a learning rate set to 1e-2, and batch size 100. We repeat the process for 1000 up to batches, and stop early if the loss does not improve for 50 consecutive batches.
For each setup, we repeat the experiment 10 times by randomly re-splitting training, validation and test set with random seed from 0 to 9. For LVD, MADSplit, and CQR (which require a hold-out set for comformal prediction), we use 60% for training, 20% for validation/hold-out set and 20% for test. For all other methods, we use 80% for training and 20% for test.
B.2 Average CI Width
It is hard to compare efficiency, because LVD achieves a much more demanding type of coverage, MADSplit and CQR achieve marginal coverage, and the rest of the methods are not valid (thus not comparable). We thus restrict the comparison to only valid methods (LVD, MADSplit, and CQR) and the subset of data for which all CIs are finite in Table 5. We can see that, as expected, LVD tends to give infinite CI for small datasets at 90% target level ("# finite" is low for a few datasets), because it requires some weighted observation in a local neighborhood. (Note that the # of finite CIs could be tuned by a bandwidth h as discussed in Section A.2.) However, despite providing a stronger coverage guarantee, LVD still managed to be the most efficient on Bike and QM9.
The most efficient method seems to be CQR, but the results are not very stable (very wide CIs for CQR in the Bike dataset for example), and most of the time the difference in average width is not significant. However, as noted earlier in the main text, the potential efficiency of CQR comes with a huge cost: CQR requires re-training the model for each . Moreover, there is no guarantee that the estimate of the lower bound of the CI is actually lower than the upper bound ("quantile crossing", see [29]), nor that a mean estimate actually falls in the CI either. In our experiments, we had to take the mean of the lower and upper bound as the mean estimator to ensure the mean estimator is always within the CI.
We also include the average width of all baselines in Table 6 for reference, although it is not very meaningful to compare valid and non-valid methods.
16

Table 5: Average width of different conformal methods. Width significantly shorter than the second-best at p = 0.05 are in bold.

Data (Count)
Yacht(62) Housing(101) Energy(154) Bike(3476) Kin8nm(1638) Concrete(206) QM8*(4357) QM9*(26744)

# finite
61.90±0.32 98.30±3.06 154.00±0.00 3475.20±1.23 1610.10±10.18 200.10±4.01 4317.33±22.90 26616.72±39.77

50%-CI Width

LVD

MADSplit

3.99±0.79 6.70±0.97 6.06±1.41 0.06±0.05 0.14±0.01 10.92±1.85 0.02±0.01 5.12±13.17

3.17±0.84 6.02±1.23 5.77±1.37 0.07±0.05 0.12±0.01 9.77±1.66 0.02±0.01 5.75±14.78

CQR
2.82±0.74 4.98±0.72 5.18±1.47 5.62±4.41 0.12±0.01 9.35±2.84 0.04±0.01 37.32±65.01

# finite
40.90±2.85 69.00±19.11 145.10±11.05 3467.50±4.40 938.00±123.72 133.80±20.13 4041.63±136.85 26146.95±151.58

90%-CI Width

LVD

MADSplit

3.47±1.36 15.94±2.62 12.89±2.02 0.15±0.13 0.34±0.02 27.93±3.59 0.05±0.03 15.06±38.94

3.29±1.04 16.81±7.44 12.19±2.71 0.19±0.11 0.28±0.02 21.79±2.93 0.05±0.03 14.77±37.04

CQR
4.52±2.08 13.70±1.84 13.76±2.80 33.65±21.52 0.28±0.02 22.87±3.48 0.11±0.03 129.63±207.46

Table 6: Average width of all baselines methods, without restriction to the subsample for which LVD gives finite CIs.

Width @ 50%
Yacht Housing Energy Bike Kin8nm Concrete QM8* QM9*

MADSplit
3.18±0.82 6.06±1.22 5.77±1.37 0.07±0.05 0.12±0.01 9.84±1.70 0.05±0.03 14.77±37.04

CQR
2.83±0.72 5.00±0.71 5.18±1.47 5.62±4.41 0.12±0.01 9.39±2.82 0.11±0.03 129.63±207.46

DJ
19.10±1.26 11.50±1.41 9.83±1.39 0.14±0.07 0.25±0.02 47.98±84.01
­ ­

DE
5.26±0.78 10.23±1.50 10.52±1.59 13.62±6.61 0.80±0.03 18.33±2.96 42.17±28.01 465.17±919.56

MCDP
14.34±0.71 30.58±0.33 30.14±0.24 115.47±0.84 0.98±0.02 47.82±0.30
­ ­

PBP
2.16±0.31 0.74±0.08 0.78±0.04 0.84±0.27 1.29±0.14 0.78±0.06
­ ­

Width @ 90%
Yacht Housing Energy Bike Kin8nm Concrete QM8* QM9*

MADSplit
8.02±0.98 18.71±9.91 12.24±2.78 0.19±0.11 0.31±0.02 22.52±2.93 0.05±0.03 14.77±37.04

CQR
12.31±1.79 15.10±1.79 13.75±2.88 33.96±21.87 0.32±0.01 23.29±3.32 0.11±0.03 129.63±207.46

DJ
73.14±1.75 26.31±1.87 18.54±2.10 0.32±0.18 0.48±0.03 199.19±369.33
­ ­

DE
13.13±1.24 24.97±2.58 25.65±5.58 38.50±13.26 1.89±0.15 44.02±4.73 42.17±28.01 465.17±919.56

MCDP
34.96±1.73 74.57±0.81 73.50±0.60 281.24±1.66 2.39±0.06 116.62±0.74
­ ­

PBP
5.26±0.77 1.82±0.19 1.91±0.09 2.04±0.65 3.15±0.35 1.90±0.16
­ ­

B.3 Additional Results of Different Variants of LVD

Although we consider MADSplit as a baseline, our method could be combined with it as well, by

simply

replacing

Ri

with

a

normalized

Ri

:=

yn+i -y^n+i ^ (xn+i )

like

that

in

MADSplit.

One

key

observation

is that using embedding given by a pre-trained DL model can simultaneously keep most of the

performance of the base model and combine it with many conformal methods with acceptable

overhead.

In this section, we will change different settings of LVD and compare the effects. Specifically, there are 3 independent choices:

· Whether we use the kernel regression prediction y^KR or the base DNN predictor µ^NN (KR vs NN) · Whether we apply the smoothness requirement as mentioned in Section 3.3 (No-smooth vs Smooth) · Whether we normalize the residuals by an extra prediction of MAD or not. We will denote the
version described in main text as "base". For the MAD-Normalized case ("MN"), similar to MADSplit [21, 8], the non-conformity score, and the final CI construction, are replaced by

Ri

:=

yn+i - y^n+i ^(xn+i)

C^MN (XN+1 :=

1

y



R

:

|y

-

y^N +1 |



Q ^ (XN +1 )

(22)

m
1 - , wN+1 + wn+iRi
i=1

(23)

This potentially can make the CI more discriminative by modeling the heteroscedasticity explicitly.

17

As a reminder, all results shown in the main text is using µ^NN , with smoothing, and not normalized by MAD prediction (NN, Smooth, NM). Also, all choices will not break any theoretical guarantees including Theorem 3.1 and 3.2.
The results are presented in Table 7 and 8, with the version shown in the main text boxed. All methods achieve target coverage rates as measured by MCR and TCR empirically. In general, we found that using y^KR tends to give higher AUROC, with similar or lower MAD. It should be noted that the MAD prediction in "MN" requires a base classifier, which is y^NN in our case. In other words, there is a mis-match in the "MN" version with y^KR. We conjecture that if the MAD predictor is properly trained for y^KR, the AUROC for this combination would be even higher (at no cost to other metrics). We also include the average width and count of finite CIs in Table 9 and 10. For the

Table 7: MCR and TCR for different variants of LVD.

MCR
Yacht Housing Energy Bike Kin8nm Concrete QM8* QM9*

No-smooth

y^KR

Smooth

MN
96.6±4.5 96.8±3.4 92.8±2.7 91.6±1.2 100.0±0.0 99.6±0.7 94.9±1.4 94.0±1.7

base
97.4±2.0 97.1±2.8 92.4±2.9 93.8±0.8 100.0±0.0 99.6±0.8 95.3±1.3 94.1±1.6

MN
95.2±4.6 96.0±3.9 92.5±2.6 91.6±1.2 97.9±0.7 96.7±2.3 92.3±0.8 90.6±0.4

base
95.5±2.3 95.7±3.5 92.4±2.9 94.1±0.8 97.9±0.8 97.0±1.1 92.9±0.9 90.4±0.5

No-smooth

y^N N

Smooth

MN
96.1±4.9 97.3±2.2 94.0±1.7 90.6±0.5 100.0±0.0 99.7±0.6 94.7±1.5 93.5±1.5

base
97.9±1.7 97.8±2.1 94.1±1.6 90.5±0.8 100.0±0.0 99.6±0.7 95.1±1.4 93.7±1.5

MN
94.7±5.0 96.1±2.6 93.9±1.7 90.4±0.6 97.9±0.6 97.0±2.1 92.0±0.9 90.3±0.4

base
96.8±2.2 96.8±2.9 94.0±1.6 90.4±0.8 98.0±0.6 97.4±1.3 92.6±0.9 90.3±0.6

TCR
Yacht Housing Energy Bike Kin8nm Concrete QM8* QM9*

96.9±4.0 95.7±4.2 86.8±6.2 91.8±1.0 100.0±0.0 99.0±2.1 94.6±2.1 94.4±4.4

96.9±5.4 96.2±4.4 83.5±9.7 91.9±2.1 100.0±0.0 99.3±1.6 95.6±2.4 94.3±4.5

93.1±7.6 93.3±9.3 85.8±6.1 90.9±1.6 95.7±1.6 93.9±4.5 90.4±2.0 88.5±3.7

94.6±5.2 91.4±8.3 83.2±10.2 91.6±2.7 95.0±2.1 94.4±3.6 91.4±2.6 88.0±3.7

95.4±7.4 98.1±3.3 88.1±4.8 92.0±1.3 100.0±0.0 99.5±1.0 94.9±1.9 94.9±3.1

99.2±2.4 97.1±4.0 87.1±5.9 90.8±1.5 100.0±0.0 99.5±1.5 94.8±2.2 94.8±3.4

93.8±7.1 98.1±2.5 87.7±4.8 91.6±1.3 97.1±1.5 96.8±3.8 91.2±1.7 90.1±2.3

98.5±3.2 96.2±4.4 86.8±5.8 90.2±1.7 97.2±1.6 97.1±3.4 90.8±1.9 89.7±2.5

Table 8: AUROC and MAD for different variants of LVD. Best AUROCs are in bold, and all are significantly higher than 50 (at p = 0.05). For MAD, the best for each task, if significantly better than the second-best (at p = 0.05), are in bold.

AUROC
Yacht Housing Energy Bike Kin8nm Concrete QM8* QM9*

No-smooth

y^KR

Smooth

MN
71.1±7.1 58.7±7.1 61.8±5.0 73.5±7.8 55.7±1.8 60.4±3.5 73.2±9.6 68.2±7.6

base
74.2±3.5 62.0±6.6 60.8±2.9 86.6±3.6 55.9±2.1 60.1±3.4 72.8±11.9 67.3±8.9

MN
61.5±12.4 61.4±6.7 63.3±5.3 73.7±7.5 60.5±1.9 62.8±4.6 75.9±9.0 66.5±3.5

base
67.5±7.1 64.4±5.6 62.9±4.6 87.5±3.2 61.8±1.6 61.8±5.0 75.2±11.9 66.4±5.4

No-smooth

y^N N

Smooth

MN
81.0±6.1 62.6±7.9 74.3±7.2 72.3±8.5 57.1±2.4 62.7±8.4 72.9±7.7 66.2±3.5

base
83.8±5.4 60.0±7.0 73.5±6.3 68.1±11.1 56.8±2.4 62.4±8.4 71.3±9.5 64.1±3.7

MN
80.9±6.1 62.1±9.0 74.3±7.2 72.4±8.5 61.6±1.6 65.4±6.1 74.1±6.9 66.3±3.5

base
83.5±5.8 59.2±8.5 73.5±6.3 68.2±11.0 60.3±1.1 64.0±6.1 71.3±9.4 62.7±3.6

MAD
Yacht Housing Energy Bike Kin8nm Concrete QM8* QM9*

0.79±0.09 2.86±0.31 2.34±0.07 2.47±0.72 0.06±0.00 4.76±0.26 0.01±0.01 3.58±9.71

0.79±0.09 2.86±0.31 2.34±0.07 2.47±0.72 0.06±0.00 4.76±0.26 0.01±0.01 3.58±9.71

1.14±0.13 3.00±0.32 2.35±0.08 3.79±0.49 0.07±0.00 5.20±0.29 0.01±0.01 4.92±11.39

1.14±0.13 3.00±0.32 2.35±0.08 3.79±0.49 0.07±0.00 5.20±0.29 0.01±0.01 4.92±11.39

1.90±0.48 3.31±0.53 2.99±0.75 0.04±0.03 0.07±0.00 5.44±0.53 0.01±0.01 3.69±9.09

1.90±0.48 3.31±0.53 2.99±0.75 0.04±0.03 0.07±0.00 5.44±0.53 0.01±0.01 3.69±9.09

1.90±0.48 3.31±0.53 2.99±0.75 0.04±0.03 0.07±0.00 5.44±0.53 0.01±0.01 3.69±9.09

1.90±0.48 3.31±0.53 2.99±0.75 0.04±0.03 0.07±0.00 5.44±0.53 0.01±0.01 3.69±9.09

most experiments adding smoothness requirement and using y^KR seems to achieve narrow CI, high AUROC and low MAD. As noted earlier, training a separate model to model the residual of y^KR might give additional discrimination (and possibly narrower CIs as well).
18

Table 9: Counts of finite CIs and average width for different variants of LVD (restricted to the subset for which all CIs are finite), with  = 0.5. In the count table, the size of the test set is included in the parenthesis, and the lowest count (which is used for width computation) is underscored.

No-smooth

y^KR

Smooth

No-smooth

y^N N

Smooth

# finite @ 50%
Yacht(62) Housing(101) Energy(154) Bike(3476) Kin8nm(1638) Concrete(206) QM8*(4357) QM9*(26744)

MN
60.7±1.9 93.5±5.7 154.0±0.0 3473.0±2.8 844.3±181.0 176.5±21.7 4002.4±210.2 25376.7±792.3

base
60.7±1.9 93.5±5.7 154.0±0.0 3473.0±2.8 844.3±181.0 176.5±21.7 4002.4±210.2 25376.7±792.3

MN
61.9±0.3 98.3±3.1 154.0±0.0 3475.2±1.2 1610.1±10.2 200.1±4.0 4317.3±22.9 26616.7±39.8

base
61.9±0.3 98.3±3.1 154.0±0.0 3475.2±1.2 1610.1±10.2 200.1±4.0 4317.3±22.9 26616.7±39.8

MN
60.7±1.9 93.5±5.7 154.0±0.0 3473.0±2.8 844.3±181.0 176.5±21.7 4002.4±210.2 25376.7±792.3

base
60.7±1.9 93.5±5.7 154.0±0.0 3473.0±2.8 844.3±181.0 176.5±21.7 4002.4±210.2 25376.7±792.3

MN
61.9±0.3 98.3±3.1 154.0±0.0 3475.2±1.2 1610.1±10.2 200.1±4.0 4317.3±22.9 26616.7±39.8

base
61.9±0.3 98.3±3.1 154.0±0.0 3475.2±1.2 1610.1±10.2 200.1±4.0 4317.3±22.9 26616.7±39.8

Width @ 50%
Yacht Housing Energy Bike Kin8nm Concrete QM8* QM9*

1.8±0.5 5.9±0.6 4.3±0.5 5.4±1.5 0.2±0.0 11.5±1.2 0.0±0.0 6.1±17.3

1.7±0.5 5.7±0.8 4.3±0.3 4.8±1.3 0.2±0.0 11.2±1.1 0.0±0.0 5.5±15.6

2.2±0.5 5.5±0.6 4.3±0.4 8.3±1.4 0.1±0.0 10.1±1.1 0.0±0.0 7.6±18.6

2.1±0.4 5.3±0.7 4.3±0.3 7.3±0.9 0.1±0.0 9.8±1.1 0.0±0.0 6.8±16.7

4.5±0.9 7.2±1.1 6.2±1.5 0.1±0.1 0.2±0.0 13.1±2.1 0.0±0.0 6.3±15.8

4.4±0.9 7.0±1.1 6.1±1.4 0.1±0.0 0.2±0.0 12.8±2.2 0.0±0.0 5.6±14.2

4.1±0.9 6.7±1.3 6.2±1.5 0.1±0.1 0.1±0.0 10.8±1.8 0.0±0.0 5.7±14.7

3.8±0.9 6.4±1.0 6.1±1.4 0.1±0.0 0.1±0.0 10.5±1.8 0.0±0.0 5.0±12.9

Table 10: Same as Table 9, but with  = 0.1.

No-smooth

y^KR

Smooth

No-smooth

y^N N

Smooth

# finite @ 90%
Yacht(62) Housing(101) Energy(154) Bike(3476) Kin8nm(1638) Concrete(206) QM8*(4357) QM9*(26744)

MN
35.1±5.8 54.5±22.1 145.1±11.0 3458.5±11.4 0.4±1.0 28.8±24.8 2936.9±587.9 21347.5±2850.8

base
35.1±5.8 54.5±22.1 145.1±11.0 3458.5±11.4 0.4±1.0 28.8±24.8 2936.9±587.9 21347.5±2850.8

MN
40.9±2.8 69.0±19.1 145.1±11.0 3467.5±4.4 938.0±123.7 133.8±20.1 4041.6±136.8 26147.0±151.6

base
40.9±2.8 69.0±19.1 145.1±11.0 3467.5±4.4 938.0±123.7 133.8±20.1 4041.6±136.8 26147.0±151.6

MN
35.1±5.8 54.5±22.1 145.1±11.0 3458.5±11.4
0.4±1.0 28.8±24.8 2936.9±587.9 21347.5±2850.8

base
35.1±5.8 54.5±22.1 145.1±11.0 3458.5±11.4 0.4±1.0 28.8±24.8 2936.9±587.9 21347.5±2850.8

MN
40.9±2.8 69.0±19.1 145.1±11.0 3467.5±4.4 938.0±123.7 133.8±20.1 4041.6±136.8 26147.0±151.6

base
40.9±2.8 69.0±19.1 145.1±11.0 3467.5±4.4 938.0±123.7 133.8±20.1 4041.6±136.8 26147.0±151.6

Width @ 90%
Yacht Housing Energy Bike Kin8nm Concrete QM8* QM9*

5.69±2.98 32.33±34.63 14.61±7.54 21.46±11.96
0.44±0.03 27.25±4.77 0.05±0.03 14.87±41.31

2.21±0.58 14.14±2.09 12.47±1.48 8.33±2.43 0.36±0.02 26.30±4.71 0.04±0.02 15.01±42.57

7.68±5.57 22.67±21.69 15.07±8.52 35.62±21.29 0.27±0.12 20.55±4.06 0.04±0.02 17.86±43.59

2.33±0.41 13.29±1.85 12.49±1.47 11.78±1.72 0.25±0.03 21.76±2.67 0.04±0.02 17.20±43.19

5.82±3.29 43.46±56.19 15.94±10.07 0.19±0.11
0.39±0.08 28.79±6.79 0.05±0.03 14.84±38.27

3.03±1.40 15.50±2.89 12.91±2.03 0.15±0.13 0.37±0.02 28.25±5.16 0.04±0.02 15.22±40.44

4.85±2.08 25.76±20.69 15.92±10.08 0.19±0.12 0.25±0.11 21.56±4.00 0.04±0.02 13.65±35.71

2.97±1.41 14.48±2.64 12.89±2.02 0.15±0.13 0.24±0.02 23.41±3.50 0.04±0.02 14.04±37.44

B.4 Additional Results on QM8/QM9 sub-tasks
Table 12 and 11 show the metrics for validity and discrimination, respectively, of different variants of LVD, and the two valid baselines. Table 13 shows the number of widths of CIs by different methods on the QM subtasks.

19

Table 11: AUROC and MAD for QM8 and QM9 sub-tasks. The best AUROCs are in bold, and AUROCs not significantly higher than 50 at p = 0.05 are underscored. For MAD, the best MADs, if significantly better than the second baseline at p = 0.05, are in bold.

No-smooth

y^KR

Smooth

LVD

No-smooth

y^N N

Smooth

Conformal Baselines

AUROC

MN

base

MN

base

MN

base

MN

base MADSplit

CQR

QM8(E1-CC2) QM8(E2-CC2) QM8(f1-CC2) QM8(f2-CC2) QM8(E1-PBE0) QM8(E2-PBE0) QM8(f1-PBE0) QM8(f2-PBE0) QM8(E1-PBE0.1) QM8(E2-PBE0.1) QM8(f1-PBE0.1) QM8(f2-PBE0.1) QM8(E1-CAM) QM8(E2-CAM) QM8(f1-CAM) QM8(f2-CAM) QM9(mu) QM9(alpha) QM9(homo) QM9(lumo) QM9(gap) QM9(r2) QM9(zpve) QM9(u0) QM9(u298) QM9(h298) QM9(g298) QM9(cv)

64.3±1.4 62.5±1.5 83.8±2.1 81.4±2.6 64.7±1.6 63.9±1.4 83.8±2.2 80.1±2.3 64.8±1.6 63.8±1.4 83.8±2.2 80.1±2.3 63.4±1.6 63.9±2.2 85.8±1.5 81.5±2.1 71.7±0.6 61.6±1.2 61.2±0.9 60.6±0.7 62.3±1.0 67.8±1.1 60.5±1.7 77.6±3.4 77.6±3.3 77.5±3.2 77.6±3.3 62.6±1.0

61.8±1.5 60.2±1.3 85.7±2.5 82.9±2.2 61.8±1.6 60.2±1.2 86.0±2.0 82.1±2.8 61.8±1.6 60.2±1.2 86.0±2.0 82.1±2.8 61.4±1.2 61.2±1.7 87.7±1.5 83.0±2.2 67.6±0.5 60.3±1.3 58.3±0.5 58.6±0.7 60.4±1.0 64.1±1.1 60.7±1.7 79.2±2.7 79.1±2.6 79.1±2.6 79.2±2.7 60.8±0.9

67.9±0.9 66.7±0.8 85.7±1.0 84.3±1.3 67.6±0.8 66.0±1.0 85.2±1.4 82.8±1.1 67.6±0.9 65.8±1.0 85.1±1.4 82.9±1.1 67.7±1.1 66.6±0.8 87.2±1.0 84.8±1.0 71.8±1.0 66.2±1.3 61.9±0.5 61.5±0.7 62.8±0.5 68.5±1.2 65.4±0.9 68.9±2.4 68.8±2.5 68.9±2.5 68.9±2.4 65.0±1.1

64.9±1.0 63.2±1.2 87.9±0.8 86.0±0.7 63.9±1.6 61.8±1.2 87.8±0.7 85.2±0.9 63.9±1.6 61.8±1.2 87.8±0.7 85.2±0.9 64.8±1.0 63.3±1.8 89.5±0.5 86.8±0.8 66.6±1.7 65.9±1.9 58.4±0.8 59.3±0.7 60.5±0.8 65.0±1.0 65.6±1.4 72.9±1.6 72.9±1.7 72.9±1.6 72.9±1.7 64.0±0.9

66.2±1.0 64.3±1.2 80.2±1.0 81.7±1.0 66.2±1.1 65.1±1.3 79.0±1.1 80.8±0.9 66.3±1.4 64.6±1.3 79.5±1.5 81.1±0.7 65.4±1.2 65.1±2.1 78.7±1.2 82.7±1.1 72.6±1.1 65.3±0.8 62.2±0.4 61.2±0.8 62.8±0.9 69.3±0.7 62.7±0.5 68.3±0.5 68.4±0.6 68.4±0.7 68.6±0.7 64.6±1.1

63.2±1.2 61.1±1.2 80.5±1.2 82.1±0.7 62.6±1.1 61.3±1.4 78.8±1.0 81.0±0.8 62.6±1.1 60.8±1.5 79.3±1.2 81.3±0.8 62.2±1.0 61.7±1.6 79.2±1.2 82.9±1.0 68.2±0.5 63.2±0.9 58.6±0.5 58.5±0.4 60.1±0.7 64.7±0.6 61.8±0.7 67.7±0.4 67.9±0.6 67.9±0.7 68.0±0.7 62.3±0.8

68.2±0.9 66.8±1.1 80.5±1.2 82.1±0.8 68.3±0.9 66.6±1.2 79.1±1.1 81.2±1.2 68.2±0.7 66.1±0.9 79.7±1.4 81.6±1.0 68.2±1.0 66.9±1.4 78.7±1.3 83.2±1.0 73.9±0.7 65.3±0.6 62.9±0.3 62.4±0.4 63.5±0.5 69.8±0.5 61.3±0.3 67.8±0.6 67.9±0.8 67.9±0.8 68.0±0.8 65.3±0.7

62.9±1.4 61.7±1.2 80.1±1.4 82.3±0.5 62.6±1.1 61.2±1.5 78.1±1.7 81.1±1.1 62.4±0.8 60.8±1.5 78.7±1.8 81.4±1.1 63.2±0.7 62.4±2.1 78.8±1.2 83.3±1.1 68.0±1.1 61.3±1.4 58.1±0.9 58.1±0.4 59.6±0.6 63.1±0.9 58.8±0.6 66.1±1.0 66.0±1.2 66.2±1.2 66.1±1.1 60.9±0.8

67.7±0.8 66.2±1.0 80.0±1.0 80.9±1.1 67.6±0.9 66.3±1.2 79.0±1.0 80.3±1.0 67.4±0.8 66.0±1.1 79.4±1.3 80.6±0.9 67.4±1.0 66.5±1.5 78.4±1.2 82.1±1.0 73.7±0.7 64.0±0.5 62.6±0.4 62.2±0.4 63.1±0.5 69.5±0.5 60.4±0.4 64.6±0.9 64.7±1.0 64.7±0.9 64.8±0.9 64.6±0.7

57.9±3.8 56.8±3.9 70.5±2.9 78.5±5.7 55.2±5.0 55.9±2.2 67.2±2.7 79.9±3.1 56.9±2.5 55.2±2.3 68.1±3.8 76.7±6.9 58.0±3.9 56.8±3.3 73.1±2.0 81.4±2.3 57.4±3.4 45.9±7.7 38.7±20.8 58.4±20.1 60.2±25.0 63.8±0.9 67.5±19.1 54.9±2.0 55.8±2.2 55.7±2.0 54.5±1.8 47.5±9.1

MAD

QM8(E1-CC2) 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.02±0.00

QM8(E2-CC2) 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.02±0.00

QM8(f1-CC2) 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.03±0.00

QM8(f2-CC2) 0.03±0.00 0.03±0.00 0.03±0.00 0.03±0.00 0.03±0.00 0.03±0.00 0.03±0.00 0.03±0.00 0.03±0.00 0.06±0.01

QM8(E1-PBE0) 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.02±0.00

QM8(E2-PBE0) 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.02±0.00

QM8(f1-PBE0) 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.03±0.00

QM8(f2-PBE0) 0.02±0.00 0.02±0.00 0.02±0.00 0.02±0.00 0.02±0.00 0.02±0.00 0.02±0.00 0.02±0.00 0.02±0.00 0.05±0.01

QM8(E1-PBE0.1) 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.02±0.00

QM8(E2-PBE0.1) 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.02±0.00

QM8(f1-PBE0.1) 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.03±0.01

QM8(f2-PBE0.1) 0.02±0.00 0.02±0.00 0.02±0.00 0.02±0.00 0.02±0.00 0.02±0.00 0.02±0.00 0.02±0.00 0.02±0.00 0.05±0.01

QM8(E1-CAM) 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.02±0.01

QM8(E2-CAM) 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.02±0.00

QM8(f1-CAM) 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.04±0.01

QM8(f2-CAM) 0.02±0.00 0.02±0.00 0.02±0.00 0.02±0.00 0.02±0.00 0.02±0.00 0.02±0.00 0.02±0.00 0.02±0.00 0.05±0.01

QM9(mu)

0.49±0.01 0.49±0.01 0.51±0.01 0.51±0.01 0.48±0.00 0.48±0.00 0.48±0.00 0.48±0.00 0.48±0.00 2.01±1.24

QM9(alpha)

0.69±0.03 0.69±0.03 1.10±0.07 1.10±0.07 0.70±0.01 0.70±0.01 0.70±0.01 0.70±0.01 0.70±0.01 9.79±0.83

QM9(homo)

0.00±0.00 0.00±0.00 0.00±0.00 0.00±0.00 0.00±0.00 0.00±0.00 0.00±0.00 0.00±0.00 0.00±0.00 2.13±2.01

QM9(lumo)

0.00±0.00 0.00±0.00 0.01±0.00 0.01±0.00 0.00±0.00 0.00±0.00 0.00±0.00 0.00±0.00 0.00±0.00 5.65±3.33

QM9(gap)

0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 0.01±0.00 4.34±3.35

QM9(r2)

35.56±1.29 35.56±1.29 42.12±1.56 42.12±1.56 33.53±0.36 33.53±0.36 33.53±0.36 33.53±0.36 33.53±0.36 188.64±9.50

QM9(zpve)

0.00±0.00 0.00±0.00 0.00±0.00 0.00±0.00 0.00±0.00 0.00±0.00 0.00±0.00 0.00±0.00 0.00±0.00 4.42±3.93

QM9(u0)

1.46±0.18 1.46±0.18 3.69±0.33 3.69±0.33 2.29±0.07 2.29±0.07 2.29±0.07 2.29±0.07 2.29±0.07 40.85±3.49

QM9(u298)

1.46±0.18 1.46±0.18 3.69±0.33 3.69±0.33 2.29±0.06 2.29±0.06 2.29±0.06 2.29±0.06 2.29±0.06 40.64±3.81

QM9(h298)

1.45±0.18 1.45±0.18 3.69±0.33 3.69±0.33 2.29±0.06 2.29±0.06 2.29±0.06 2.29±0.06 2.29±0.06 40.84±3.03

QM9(g298)

1.46±0.18 1.46±0.18 3.70±0.33 3.70±0.33 2.29±0.07 2.29±0.07 2.29±0.07 2.29±0.07 2.29±0.07 41.02±3.81

QM9(cv)

0.34±0.01 0.34±0.01 0.47±0.02 0.47±0.02 0.33±0.01 0.33±0.01 0.33±0.01 0.33±0.01 0.33±0.01 5.03±1.55

20

Table 12: MCR and TCR for QM8.

y^KR No-smooth

Smooth

LVD

y^N N No-smooth

Smooth

Conformal Baselines

MCR

MN

base

MN

base

MN

base

MN base MADSplit CQR

QM8(E1-CC2) 96.3±0.7 96.3±0.8 92.6±0.9 92.7±0.6 96.2±0.8 96.2±0.6 92.3±0.6 92.5±0.5 90.0±0.5 90.1±0.7

QM8(E2-CC2) 96.2±1.0 96.2±0.8 92.0±0.5 92.2±0.7 96.1±1.0 96.2±0.7 92.0±0.5 92.2±0.5 89.8±0.6 90.1±0.5

QM8(f1-CC2) 93.9±0.9 94.8±0.9 92.3±0.7 93.8±0.8 93.7±1.0 94.0±1.2 92.2±0.8 92.5±1.1 90.1±0.7 90.2±0.7

QM8(f2-CC2) 94.8±1.3 95.1±1.1 93.0±0.6 93.6±0.6 94.3±1.3 95.1±1.3 92.5±0.8 93.6±0.6 89.9±0.7 89.9±0.7

QM8(E1-PBE0) 95.9±0.5 96.0±0.6 92.3±0.7 92.7±0.6 95.7±0.7 95.9±0.6 91.9±0.8 92.5±0.6 89.7±0.5 90.2±0.5

QM8(E2-PBE0) 95.0±0.9 95.4±0.9 91.8±0.5 92.4±0.8 95.0±0.9 95.3±0.9 91.7±0.6 92.3±0.6 90.0±0.5 90.0±0.6

QM8(f1-PBE0) 93.7±1.1 94.3±1.2 92.4±1.0 93.1±1.0 93.4±1.2 93.9±1.4 91.8±1.3 92.4±1.2 90.3±0.8 90.2±0.9

QM8(f2-PBE0) 94.0±1.8 94.6±1.8 91.9±1.1 92.5±0.9 93.9±1.9 94.8±1.6 91.6±1.2 92.8±0.9 89.9±0.9 89.7±0.5

QM8(E1-PBE0.1) 95.8±0.6 96.0±0.6 92.2±0.6 92.7±0.6 95.6±0.7 95.9±0.5 91.9±0.8 92.5±0.7 89.8±0.5 89.8±0.6

QM8(E2-PBE0.1) 94.9±0.8 95.4±0.9 91.7±0.4 92.4±0.8 95.0±1.0 95.3±0.9 91.8±0.5 92.3±0.6 90.2±0.5 90.0±0.4

QM8(f1-PBE0.1) 93.6±1.0 94.3±1.2 92.3±0.9 93.1±1.0 93.1±1.4 93.9±1.3 91.6±1.3 92.3±1.4 89.9±1.0 90.0±0.4

QM8(f2-PBE0.1) 94.2±1.8 94.6±1.8 92.0±1.1 92.5±0.9 94.0±1.9 94.8±1.7 91.8±1.1 92.8±0.9 89.9±0.7 89.7±0.8

QM8(E1-CAM) 96.3±1.1 96.5±0.9 92.7±0.9 93.2±0.8 96.2±1.2 96.4±0.9 92.3±0.8 92.8±0.6 89.9±0.5 90.2±0.4

QM8(E2-CAM) 95.3±1.0 95.6±1.1 92.1±0.7 92.6±0.8 95.4±0.8 95.6±0.9 92.1±0.5 92.5±0.7 90.0±0.5 90.0±0.6

QM8(f1-CAM) 93.7±1.0 94.7±1.2 92.2±0.6 93.5±0.8 93.3±0.9 93.8±1.2 91.7±0.6 92.2±0.7 89.8±0.7 89.9±0.7

QM8(f2-CAM) 94.7±1.1 95.0±0.8 92.7±0.7 93.4±0.6 94.5±1.1 95.2±0.9 92.5±0.7 93.5±0.7 90.0±0.9 90.3±0.8

QM9(mu)

92.5±1.7 93.2±1.6 90.4±0.4 91.3±0.5 92.6±1.7 93.3±1.5 90.6±0.4 91.4±0.4 90.1±0.2 90.0±0.3

QM9(alpha)

94.7±1.5 94.6±1.6 90.5±0.4 89.7±0.4 94.5±1.5 94.6±1.6 90.1±0.3 89.7±0.4 90.0±0.2 89.9±0.2

QM9(homo)

92.3±0.6 92.5±0.7 90.4±0.4 90.4±0.2 92.4±0.6 92.6±0.7 90.6±0.3 90.7±0.3 90.0±0.3 89.9±0.3

QM9(lumo)

93.5±0.9 93.5±1.0 90.5±0.3 90.4±0.3 93.5±0.9 93.6±0.9 90.7±0.4 90.6±0.3 90.1±0.3 89.9±0.4

QM9(gap)

93.0±1.2 93.1±1.2 90.5±0.3 90.4±0.2 93.0±1.1 93.2±1.1 90.6±0.3 90.7±0.3 90.1±0.2 89.9±0.3

QM9(r2)

92.6±1.1 93.0±1.2 90.1±0.3 90.5±0.3 92.6±1.2 93.1±1.1 90.4±0.4 90.8±0.5 89.9±0.4 90.1±0.3

QM9(zpve)

95.3±1.2 95.4±1.2 90.5±0.1 90.1±0.3 95.2±1.3 95.3±1.4 90.3±0.2 89.9±0.2 90.0±0.2 90.0±0.2

QM9(u0)

94.9±1.5 94.8±1.6 90.9±0.5 90.6±0.5 93.3±1.3 93.4±1.3 90.1±0.2 90.0±0.3 89.9±0.2 90.1±0.2

QM9(u298)

95.1±1.8 95.0±1.9 90.9±0.5 90.6±0.5 93.6±1.7 93.7±1.8 90.0±0.2 89.9±0.3 89.9±0.2 90.1±0.2

QM9(h298)

95.0±1.6 94.9±1.7 90.9±0.5 90.6±0.5 93.5±1.4 93.6±1.4 90.1±0.2 90.0±0.3 89.9±0.2 90.1±0.2

QM9(g298)

94.9±1.5 94.8±1.6 90.8±0.5 90.6±0.5 93.3±1.4 93.4±1.3 90.0±0.2 89.9±0.3 89.9±0.2 90.1±0.2

QM9(cv)

94.3±1.4 94.4±1.5 90.4±0.3 89.9±0.5 94.2±1.5 94.4±1.5 90.4±0.4 90.1±0.4 90.0±0.2 90.0±0.4

TCR

QM8(E1-CC2) 94.4±1.2 94.3±1.4 89.6±1.9 89.3±0.9 95.2±0.9 94.3±1.5 91.3±1.2 90.2±1.3 88.1±1.5 83.5±6.4

QM8(E2-CC2) 95.5±1.5 94.9±1.7 89.6±1.2 88.6±1.4 95.8±1.6 95.6±1.5 90.7±1.7 90.1±1.7 87.4±2.2 84.4±4.4

QM8(f1-CC2) 94.5±2.2 97.0±1.7 90.4±1.5 93.6±1.4 94.9±1.3 94.1±2.0 91.4±1.2 90.5±1.8 85.0±1.8 80.1±2.8

QM8(f2-CC2) 95.6±2.5 96.2±2.7 91.8±1.6 92.3±1.7 95.7±2.6 95.2±2.9 92.4±1.6 91.4±1.5 84.9±2.1 73.3±4.8

QM8(E1-PBE0) 94.0±1.4 94.2±1.2 89.9±1.8 89.9±1.5 94.4±0.8 94.2±1.5 90.9±1.2 90.3±1.5 87.8±1.8 80.9±4.6

QM8(E2-PBE0) 94.1±1.6 93.8±2.0 90.0±1.6 89.7±2.0 94.8±1.4 94.2±1.9 90.9±1.7 90.5±2.0 87.5±1.8 82.1±5.8

QM8(f1-PBE0) 94.3±2.3 97.3±1.5 90.9±3.0 94.4±1.8 94.5±2.0 95.2±1.9 91.1±2.7 91.8±2.1 85.1±1.9 80.4±2.5

QM8(f2-PBE0) 94.5±3.4 95.6±3.4 90.1±2.4 90.9±1.5 95.1±3.2 95.1±3.6 90.9±2.4 90.2±1.8 84.4±2.6 74.6±6.2

QM8(E1-PBE0.1) 94.0±1.3 94.2±1.2 89.8±1.5 89.9±1.5 94.3±1.2 93.9±1.6 90.7±1.4 90.3±1.9 87.8±2.0 83.4±4.4

QM8(E2-PBE0.1) 94.2±1.4 93.8±2.0 90.1±1.6 89.7±2.0 94.9±1.5 94.3±2.1 91.0±1.5 90.4±2.2 87.9±1.8 82.0±5.5

QM8(f1-PBE0.1) 94.2±2.3 97.3±1.5 90.7±2.8 94.4±1.8 93.9±2.1 95.2±1.8 90.6±2.7 91.7±2.5 84.1±2.0 81.1±2.4

QM8(f2-PBE0.1) 94.7±3.2 95.6±3.4 90.0±2.1 90.9±1.5 95.0±3.2 95.0±3.7 90.8±2.3 90.3±1.9 84.2±2.4 73.3±3.8

QM8(E1-CAM) 94.0±1.7 94.1±1.7 89.1±2.4 90.0±1.3 94.3±1.5 93.8±1.9 90.2±1.4 89.8±1.7 86.5±1.4 81.4±5.8

QM8(E2-CAM) 95.1±1.4 95.0±1.8 90.7±1.2 90.4±1.5 95.4±1.4 95.4±1.9 91.8±1.0 91.1±1.5 88.1±1.7 83.5±5.0

QM8(f1-CAM) 95.3±2.3 98.8±1.2 92.1±1.4 96.2±0.7 95.1±1.6 95.9±1.4 92.3±0.8 93.0±1.3 86.7±1.1 81.9±3.3

QM8(f2-CAM) 95.7±1.7 96.8±1.8 91.5±0.9 92.8±0.9 95.9±1.7 96.0±1.9 91.7±1.5 91.2±1.5 85.0±2.0 73.5±4.6

QM9(mu)

83.2±4.7 82.9±4.6 77.4±1.5 77.1±1.7 87.0±3.5 86.0±3.4 83.1±1.5 81.9±1.1 79.8±1.5 64.7±4.2

QM9(alpha)

96.7±1.3 96.7±1.5 88.9±0.7 87.9±0.7 97.4±1.1 97.5±1.2 90.7±0.9 90.3±0.9 87.3±0.8 69.9±2.3

QM9(homo)

91.7±1.4 91.4±1.5 87.2±0.8 86.7±1.0 93.0±1.0 92.9±0.9 89.7±0.7 89.4±1.0 86.5±0.5 89.8±2.7

QM9(lumo)

93.4±1.3 93.5±1.2 88.2±0.8 88.1±0.7 94.7±0.7 94.7±0.8 91.1±0.9 90.7±1.0 89.7±0.9 87.5±1.9

QM9(gap)

92.0±1.8 91.9±2.0 87.6±0.6 86.9±0.9 93.7±1.4 93.6±1.5 90.3±0.8 90.0±1.0 87.9±0.8 88.1±1.3

QM9(r2)

94.1±2.1 93.7±2.0 88.2±0.8 88.0±1.2 95.3±1.7 95.0±1.7 90.9±0.7 90.6±1.2 87.6±0.8 67.0±4.3

QM9(zpve)

97.0±1.0 97.1±0.9 90.4±0.7 90.1±0.7 96.7±1.2 96.9±1.2 91.2±0.9 90.9±0.8 90.5±0.6 88.3±4.5

QM9(u0)

97.3±1.5 97.2±1.6 91.2±0.9 90.7±0.9 95.9±1.5 96.1±1.7 90.7±0.5 90.4±0.8 84.1±1.0 80.1±3.2

QM9(u298)

97.4±1.7 97.4±1.8 91.2±0.8 90.7±0.9 96.2±1.8 96.2±1.9 90.6±0.4 90.4±0.8 84.0±0.9 79.8±3.3

QM9(h298)

97.4±1.7 97.3±1.7 91.2±0.8 90.7±0.9 96.1±1.8 96.1±1.9 90.7±0.4 90.4±0.7 84.0±1.1 80.3±2.9

QM9(g298)

97.3±1.6 97.2±1.7 91.1±0.7 90.7±0.9 96.0±1.6 96.0±1.7 90.7±0.4 90.3±0.7 84.0±1.1 80.0±2.3

QM9(cv)

95.9±1.8 95.9±2.0 89.2±0.7 88.2±1.0 96.5±1.5 96.6±1.5 91.0±0.7 90.8±0.7 87.8±0.6 80.4±7.7

21

Table 13: Average width for 50% and 90% CIs of different methods. Narrowest CIs are in bold, and further underscored if significantly narrower than the second best (at p = 0.05). As a reminder, there are 4357 data in QM8's test set and 26744 in QM9's. Among these valid methods, LVD's 50% CIs are the narrowest, and still competitive at 90% despite satisfying a stronger coverage requirement.

22

No-smooth

y^KR

Smooth

LVD

No-smooth

y^N N

Smooth

Conformal Baselines

Width @ 50%

# finite

MN

base

MN

base

MN

base

MN

base

MADSplit

CQR

QM8(E1-CC2) 3791.5±189.4 1.2e-02±4.0e-04 1.1e-02±3.0e-04 1.1e-02±6.0e-04 9.8e-03±5.0e-04 1.3e-02±5.0e-04 1.2e-02±4.0e-04

QM8(E2-CC2) 3829.8±126.6 1.4e-02±7.0e-04 1.3e-02±5.0e-04 1.3e-02±4.0e-04 1.2e-02±4.0e-04 1.5e-02±9.0e-04 1.4e-02±7.0e-04

QM8(f1-CC2)

4066.0±179.7 2.1e-02±1.6e-03 1.9e-02±1.2e-03 1.8e-02±2.1e-03 1.6e-02±1.6e-03 2.4e-02±1.2e-03 2.1e-02±1.0e-03

QM8(f2-CC2)

3998.1±231.6 5.2e-02±3.1e-03 4.6e-02±2.5e-03 4.6e-02±4.9e-03 4.1e-02±4.3e-03 5.5e-02±3.2e-03 4.9e-02±3.2e-03

QM8(E1-PBE0) 3932.1±133.8 1.2e-02±3.0e-04 1.1e-02±2.0e-04 1.1e-02±6.0e-04 9.6e-03±3.0e-04 1.3e-02±5.0e-04 1.1e-02±4.0e-04

QM8(E2-PBE0) 4063.2±133.4 1.3e-02±4.0e-04 1.2e-02±3.0e-04 1.2e-02±3.0e-04 1.1e-02±3.0e-04 1.3e-02±6.0e-04 1.2e-02±6.0e-04

QM8(f1-PBE0) 4152.1±159.3 1.8e-02±9.0e-04 1.5e-02±9.0e-04 1.6e-02±2.0e-03 1.4e-02±1.7e-03 2.0e-02±1.0e-03 1.7e-02±1.0e-03

QM8(f2-PBE0) 4059.8±240.7 3.9e-02±1.0e-03 3.4e-02±1.0e-03 3.4e-02±3.5e-03 3.0e-02±2.9e-03 4.1e-02±1.6e-03 3.6e-02±1.7e-03

QM8(E1-PBE0.1) 3932.1±133.8 1.2e-02±2.0e-04 1.1e-02±2.0e-04 1.1e-02±4.0e-04 9.6e-03±3.0e-04 1.3e-02±5.0e-04 1.1e-02±4.0e-04

QM8(E2-PBE0.1) 4063.2±133.4 1.3e-02±4.0e-04 1.2e-02±3.0e-04 1.2e-02±3.0e-04 1.1e-02±3.0e-04 1.3e-02±5.0e-04 1.2e-02±5.0e-04

QM8(f1-PBE0.1) 4152.1±159.3 1.8e-02±1.1e-03 1.5e-02±9.0e-04 1.6e-02±2.0e-03 1.4e-02±1.7e-03 1.9e-02±1.1e-03 1.7e-02±1.0e-03

QM8(f2-PBE0.1) 4059.8±240.7 3.9e-02±1.3e-03 3.4e-02±1.0e-03 3.3e-02±3.5e-03 3.0e-02±2.9e-03 4.1e-02±1.9e-03 3.6e-02±1.8e-03

QM8(E1-CAM) 3782.7±252.3 1.1e-02±4.0e-04 1.0e-02±3.0e-04 1.0e-02±3.0e-04 9.1e-03±2.0e-04 1.2e-02±5.0e-04 1.1e-02±6.0e-04

QM8(E2-CAM) 4004.4±151.5 1.2e-02±3.0e-04 1.1e-02±3.0e-04 1.1e-02±3.0e-04 1.0e-02±3.0e-04 1.3e-02±6.0e-04 1.2e-02±7.0e-04

QM8(f1-CAM) 4125.5±161.7 1.9e-02±1.2e-03 1.6e-02±1.1e-03 1.6e-02±2.0e-03 1.4e-02±1.7e-03 2.2e-02±1.0e-03 1.9e-02±1.0e-03

QM8(f2-CAM) 4001.1±224.0 4.1e-02±1.8e-03 3.7e-02±1.3e-03 3.6e-02±3.0e-03 3.2e-02±2.6e-03 4.4e-02±1.2e-03 4.0e-02±1.3e-03

QM9(mu)

25845.8±660.3 8.9e-01±5.7e-02 8.1e-01±3.1e-02 8.4e-01±2.7e-02 7.6e-01±1.9e-02 8.7e-01±5.3e-02 7.8e-01±4.2e-02

QM9(alpha)

24665.0±1191.1 1.2e+00±3.3e-02 1.1e+00±4.1e-02 1.6e+00±1.3e-01 1.4e+00±1.2e-01 1.2e+00±3.2e-02 1.1e+00±4.8e-02

QM9(homo)

26024.3±221.8 7.5e-03±3.0e-04 7.1e-03±3.0e-04 7.4e-03±2.0e-04 7.0e-03±2.0e-04 7.0e-03±1.0e-04 6.6e-03±1.0e-04

QM9(lumo)

25608.8±416.6 8.5e-03±2.0e-04 8.1e-03±2.0e-04 8.3e-03±2.0e-04 7.9e-03±3.0e-04 7.9e-03±2.0e-04 7.5e-03±2.0e-04

QM9(gap)

25862.9±476.1 1.1e-02±2.0e-04 1.0e-02±2.0e-04 1.1e-02±4.0e-04 9.8e-03±3.0e-04 1.0e-02±5.0e-04 9.6e-03±6.0e-04

QM9(r2)

25736.3±537.4 6.3e+01±2.0e+00 5.7e+01±1.6e+00 6.9e+01±2.5e+00 6.2e+01±2.7e+00 5.8e+01±1.9e+00 5.2e+01±1.9e+00

QM9(zpve)

24486.6±870.8 1.6e-03±2.0e-04 1.5e-03±1.0e-04 3.8e-03±4.0e-04 3.6e-03±4.0e-04 2.8e-03±1.0e-04 2.7e-03±1.0e-04

QM9(u0)

25375.4±499.9 1.9e+00±2.4e-01 1.7e+00±2.1e-01 4.9e+00±6.5e-01 4.2e+00±5.2e-01 3.7e+00±1.4e-01 3.2e+00±1.4e-01

QM9(u298)

25204.3±754.3 1.9e+00±2.5e-01 1.7e+00±2.0e-01 4.9e+00±6.9e-01 4.2e+00±5.3e-01 3.7e+00±1.3e-01 3.2e+00±1.5e-01

QM9(h298)

25288.9±581.3 1.9e+00±2.4e-01 1.7e+00±2.0e-01 4.9e+00±6.8e-01 4.2e+00±5.3e-01 3.7e+00±1.3e-01 3.2e+00±1.4e-01

QM9(g298)

25345.2±543.2 1.9e+00±2.5e-01 1.7e+00±2.1e-01 4.9e+00±6.5e-01 4.2e+00±5.2e-01 3.7e+00±1.3e-01 3.2e+00±1.4e-01

QM9(cv)

25051.2±829.4 6.0e-01±2.1e-02 5.5e-01±2.0e-02 6.9e-01±5.5e-02 6.3e-01±4.7e-02 5.7e-01±1.8e-02 5.3e-01±2.0e-02

1.1e-02±5.0e-04 1.2e-02±3.0e-04 1.9e-02±2.1e-03 4.7e-02±5.1e-03 1.1e-02±3.0e-04 1.2e-02±3.0e-04 1.6e-02±2.3e-03 3.4e-02±3.5e-03 1.1e-02±4.0e-04 1.2e-02±3.0e-04 1.6e-02±2.2e-03 3.4e-02±3.4e-03 1.0e-02±4.0e-04 1.1e-02±3.0e-04 1.8e-02±1.4e-03 3.8e-02±3.0e-03 7.9e-01±2.1e-02 9.9e-01±4.2e-02 6.6e-03±1.0e-04 7.2e-03±1.0e-04 9.5e-03±2.0e-04 5.4e+01±7.8e-01 2.4e-03±1.0e-04 3.0e+00±1.3e-01 3.0e+00±1.5e-01 3.0e+00±1.3e-01 3.1e+00±1.3e-01 4.9e-01±1.5e-02

9.6e-03±4.0e-04 1.1e-02±2.0e-04 1.6e-02±1.4e-03 4.2e-02±4.7e-03 9.6e-03±2.0e-04 1.1e-02±3.0e-04 1.4e-02±2.0e-03 3.0e-02±2.8e-03 9.6e-03±2.0e-04 1.1e-02±3.0e-04 1.4e-02±2.0e-03 3.0e-02±3.0e-03 9.3e-03±3.0e-04 1.0e-02±3.0e-04 1.6e-02±1.2e-03 3.3e-02±2.8e-03 7.1e-01±1.7e-02 8.8e-01±2.2e-02 6.2e-03±1.0e-04 6.8e-03±1.0e-04 8.8e-03±3.0e-04 4.8e+01±6.1e-01 2.2e-03±1.0e-04 2.5e+00±9.2e-02 2.5e+00±8.5e-02 2.5e+00±8.1e-02 2.5e+00±8.8e-02 4.4e-01±8.6e-03

1.0e-02±4.0e-04 1.2e-02±3.0e-04 1.8e-02±1.6e-03 4.1e-02±3.2e-03 1.1e-02±3.0e-04 1.2e-02±2.0e-04 1.4e-02±1.6e-03 3.0e-02±2.3e-03 1.1e-02±3.0e-04 1.2e-02±3.0e-04 1.4e-02±1.6e-03 3.0e-02±2.5e-03 1.0e-02±3.0e-04 1.1e-02±3.0e-04 1.6e-02±9.0e-04 3.3e-02±2.3e-03 7.7e-01±2.0e-02 1.0e+00±3.3e-02 6.6e-03±1.0e-04 7.2e-03±1.0e-04 9.4e-03±2.0e-04 5.4e+01±9.5e-01 2.4e-03±1.0e-04 2.9e+00±1.3e-01 2.9e+00±1.3e-01 2.9e+00±1.1e-01 2.9e+00±1.3e-01 4.9e-01±1.5e-02

3.2e-02±4.1e-03 2.8e-02±3.3e-03 3.0e-02±5.4e-03 5.7e-02±1.1e-02 3.4e-02±9.1e-03 2.9e-02±7.9e-03 3.2e-02±6.7e-03 4.2e-02±5.9e-03 3.1e-02±4.8e-03 2.9e-02±3.3e-03 3.5e-02±8.0e-03 4.3e-02±7.6e-03 2.9e-02±3.1e-03 2.7e-02±3.2e-03 3.1e-02±4.1e-03 5.1e-02±5.8e-03 2.1e+00±4.2e-01 1.4e+01±8.4e-01 2.2e+00±1.7e+00 2.1e+00±1.5e+00 1.1e+00±5.9e-01 2.4e+02±1.1e+01 1.9e+00±1.6e+00 4.4e+01±3.7e+00 4.5e+01±3.7e+00 4.4e+01±3.8e+00 4.4e+01±3.7e+00 5.2e+00±1.3e+00

Width @ 90%

QM8(E1-CC2) 2373.2±351.0 2.8e-02±2.1e-03 2.7e-02±8.0e-04 2.6e-02±1.4e-03 2.5e-02±9.0e-04 2.9e-02±2.6e-03 2.7e-02±8.0e-04 2.5e-02±1.2e-03 2.4e-02±7.0e-04 2.5e-02±1.4e-03 8.4e-02±1.4e-02

QM8(E2-CC2) 2356.4±368.3 3.3e-02±1.2e-03 3.3e-02±8.0e-04 3.0e-02±1.2e-03 2.9e-02±1.0e-03 3.3e-02±1.2e-03 3.3e-02±1.0e-03 2.9e-02±2.0e-04 2.9e-02±5.0e-04 2.8e-02±6.0e-04 8.8e-02±1.5e-02

QM8(f1-CC2)

3239.5±414.0 4.4e-02±7.7e-03 3.2e-02±5.9e-03 3.6e-02±7.0e-03 2.9e-02±5.0e-03 5.6e-02±8.8e-03 3.6e-02±6.1e-03 4.0e-02±8.0e-03 3.2e-02±5.4e-03 4.0e-02±6.9e-03 8.8e-02±9.8e-03

QM8(f2-CC2)

3061.2±504.8 1.1e-01±1.5e-02 8.6e-02±1.3e-02 9.3e-02±1.8e-02 7.7e-02±1.5e-02 1.1e-01±1.9e-02 8.8e-02±1.3e-02 9.2e-02±1.7e-02 8.1e-02±1.4e-02 8.5e-02±1.2e-02 1.5e-01±3.2e-02

QM8(E1-PBE0) 2557.4±218.2 2.9e-02±2.1e-03 2.8e-02±8.0e-04 2.6e-02±1.5e-03 2.6e-02±8.0e-04 2.9e-02±2.6e-03 2.8e-02±9.0e-04 2.5e-02±1.2e-03 2.6e-02±8.0e-04 2.6e-02±1.1e-03 9.1e-02±5.9e-03

QM8(E2-PBE0) 2956.5±408.3 3.2e-02±1.0e-03 3.2e-02±7.0e-04 2.9e-02±1.1e-03 2.9e-02±1.0e-03 3.2e-02±9.0e-04 3.2e-02±6.0e-04 2.8e-02±9.0e-04 2.9e-02±7.0e-04 2.8e-02±4.0e-04 7.5e-02±7.2e-03

QM8(f1-PBE0) 3493.5±488.2 3.6e-02±6.8e-03 2.8e-02±6.9e-03 3.3e-02±7.6e-03 2.6e-02±7.3e-03 4.0e-02±6.0e-03 3.1e-02±6.9e-03 3.4e-02±8.7e-03 3.0e-02±7.2e-03 3.2e-02±5.3e-03 9.1e-02±1.3e-02

QM8(f2-PBE0) 3155.5±646.3 8.4e-02±1.8e-02 6.8e-02±1.2e-02 7.1e-02±1.9e-02 5.9e-02±1.4e-02 8.3e-02±1.6e-02 6.9e-02±1.2e-02 6.9e-02±1.8e-02 6.2e-02±1.4e-02 6.8e-02±1.3e-02 1.3e-01±2.6e-02

QM8(E1-PBE0.1) 2557.4±218.2 3.0e-02±5.1e-03 2.8e-02±8.0e-04 2.6e-02±1.5e-03 2.6e-02±8.0e-04 2.9e-02±2.7e-03 2.8e-02±9.0e-04 2.5e-02±1.2e-03 2.6e-02±7.0e-04 2.6e-02±1.1e-03 8.1e-02±1.2e-02

QM8(E2-PBE0.1) 2956.5±408.3 3.2e-02±9.0e-04 3.2e-02±7.0e-04 2.9e-02±1.1e-03 2.9e-02±1.0e-03 3.2e-02±9.0e-04 3.2e-02±6.0e-04 2.9e-02±9.0e-04 2.9e-02±7.0e-04 2.8e-02±5.0e-04 8.4e-02±7.6e-03

QM8(f1-PBE0.1) 3493.5±488.2 3.5e-02±8.4e-03 2.8e-02±6.9e-03 3.3e-02±7.9e-03 2.6e-02±7.3e-03 3.8e-02±7.5e-03 3.1e-02±6.9e-03 3.4e-02±8.3e-03 2.9e-02±7.2e-03 3.2e-02±5.6e-03 8.9e-02±1.7e-02

QM8(f2-PBE0.1) 3155.5±646.3 8.4e-02±2.1e-02 6.8e-02±1.2e-02 7.2e-02±2.0e-02 5.9e-02±1.4e-02 8.3e-02±1.7e-02 6.9e-02±1.1e-02 6.9e-02±1.8e-02 6.1e-02±1.4e-02 6.8e-02±1.3e-02 1.2e-01±2.0e-02

QM8(E1-CAM) 2317.5±472.6 2.8e-02±2.3e-03 2.6e-02±9.0e-04 2.5e-02±1.1e-03 2.4e-02±7.0e-04 2.8e-02±2.0e-03 2.7e-02±9.0e-04 2.4e-02±1.2e-03 2.4e-02±8.0e-04 2.4e-02±1.1e-03 9.0e-02±2.5e-02

QM8(E2-CAM) 2791.8±491.5 2.9e-02±1.0e-03 3.0e-02±7.0e-04 2.7e-02±1.2e-03 2.7e-02±9.0e-04 2.9e-02±1.0e-03 3.0e-02±8.0e-04 2.7e-02±1.2e-03 2.7e-02±8.0e-04 2.6e-02±6.0e-04 7.6e-02±1.0e-02

QM8(f1-CAM) 3366.6±448.1 3.4e-02±9.3e-03 2.8e-02±7.5e-03 3.0e-02±8.9e-03 2.5e-02±7.2e-03 4.3e-02±9.2e-03 3.2e-02±7.0e-03 3.4e-02±1.0e-02 2.9e-02±6.9e-03 3.4e-02±8.1e-03 1.0e-01±2.5e-02

QM8(f2-CAM) 3059.9±482.0 8.8e-02±1.6e-02 7.0e-02±1.3e-02 7.4e-02±1.6e-02 6.2e-02±1.4e-02 9.0e-02±1.4e-02 7.1e-02±1.3e-02 7.3e-02±1.7e-02 6.4e-02±1.3e-02 6.9e-02±1.3e-02 1.4e-01±1.3e-02

QM9(mu)

23224.8±2625.8 2.2e+00±6.9e-02 2.2e+00±5.7e-02 2.1e+00±5.7e-02 2.1e+00±4.1e-02 2.1e+00±1.1e-01 2.2e+00±7.5e-02 1.9e+00±4.8e-02 2.1e+00±3.7e-02 2.0e+00±3.7e-02 7.1e+00±2.8e+00

QM9(alpha)

19039.0±3410.6 2.7e+00±1.1e-01 2.6e+00±7.5e-02 3.5e+00±3.6e-01 3.4e+00±2.8e-01 2.6e+00±9.3e-02 2.5e+00±7.7e-02 2.3e+00±1.2e-01 2.3e+00±7.0e-02 2.4e+00±9.0e-02 4.0e+01±2.7e+00

QM9(homo)

23865.0±834.2 1.8e-02±6.0e-04 1.8e-02±4.0e-04 1.8e-02±4.0e-04 1.8e-02±3.0e-04 1.7e-02±3.0e-04 1.8e-02±3.0e-04 1.7e-02±3.0e-04 1.7e-02±1.0e-04 1.7e-02±2.0e-04 6.7e+00±3.8e+00

QM9(lumo)

21867.7±1906.7 2.1e-02±5.0e-04 2.1e-02±5.0e-04 2.0e-02±5.0e-04 2.0e-02±5.0e-04 1.9e-02±4.0e-04 1.9e-02±5.0e-04 1.8e-02±3.0e-04 1.8e-02±2.0e-04 1.8e-02±2.0e-04 1.3e+01±7.1e+00

QM9(gap)

22868.0±2105.0 2.7e-02±7.0e-04 2.7e-02±8.0e-04 2.6e-02±7.0e-04 2.5e-02±6.0e-04 2.5e-02±1.0e-03 2.5e-02±1.1e-03 2.3e-02±4.0e-04 2.3e-02±5.0e-04 2.4e-02±4.0e-04 1.1e+01±6.1e+00

QM9(r2)

22850.8±1979.8 1.5e+02±4.8e+00 1.6e+02±3.4e+00 1.6e+02±8.0e+00 1.6e+02±6.6e+00 1.4e+02±2.6e+00 1.5e+02±3.0e+00 1.3e+02±2.7e+00 1.4e+02±2.4e+00 1.3e+02±2.5e+00 7.6e+02±2.9e+01

QM9(zpve)

17884.2±2703.3 3.6e-03±5.0e-04 3.5e-03±4.0e-04 8.8e-03±9.0e-04 8.7e-03±8.0e-04 6.0e-03±2.0e-04 6.0e-03±2.0e-04 5.6e-03±2.0e-04 5.6e-03±2.0e-04 5.8e-03±2.0e-04 1.1e+01±7.8e+00

QM9(u0)

21369.4±1980.2 5.3e+00±9.9e-01 4.6e+00±7.7e-01 1.2e+01±1.4e+00 1.0e+01±1.3e+00 7.8e+00±5.4e-01 7.0e+00±3.1e-01 6.8e+00±5.5e-01 6.4e+00±4.6e-01 7.6e+00±3.0e-01 1.7e+02±1.5e+01

QM9(u298)

20728.2±2926.0 5.3e+00±1.0e+00 4.6e+00±7.8e-01 1.2e+01±1.6e+00 1.0e+01±1.4e+00 7.8e+00±5.5e-01 7.1e+00±3.0e-01 6.8e+00±6.1e-01 6.4e+00±5.1e-01 7.5e+00±3.1e-01 1.7e+02±1.5e+01

QM9(h298)

21053.4±2274.2 5.2e+00±1.0e+00 4.6e+00±8.1e-01 1.2e+01±1.5e+00 1.0e+01±1.4e+00 7.8e+00±5.6e-01 7.0e+00±3.4e-01 6.8e+00±5.6e-01 6.4e+00±4.8e-01 7.5e+00±2.9e-01 1.7e+02±1.3e+01

QM9(g298)

21269.7±2092.8 5.3e+00±9.8e-01 4.6e+00±7.7e-01 1.2e+01±1.4e+00 1.0e+01±1.3e+00 7.8e+00±5.4e-01 7.0e+00±3.1e-01 6.8e+00±5.5e-01 6.4e+00±4.6e-01 7.6e+00±3.0e-01 1.7e+02±1.5e+01

QM9(cv)

20084.8±2942.8 1.4e+00±6.2e-02 1.4e+00±6.0e-02 1.6e+00±1.5e-01 1.5e+00±1.1e-01 1.3e+00±2.5e-02 1.3e+00±3.4e-02 1.2e+00±3.7e-02 1.2e+00±2.8e-02 1.2e+00±3.3e-02 2.1e+01±5.5e+00

C Discussion on Discriminative Jackknife

Discriminative Jackknife (DJ) was recently proposed as a post-hoc method to construct confidence intervals for regression deep learning models [1]. [1] claims that DJ is simultaneously marginally valid and discriminative. Unfortunately, neither claim is true, and it has other practical issues, as we will discuss in detail in this section.

C.1 Jackknife+ vs. DJ

Although it is out-of-scope for this paper, we would like to briefly explain where the finite-sample coverage guarantee comes from, or rather should have come from. It is highly recommended that the readers read the original work of Jackknife+, [4] which lays the theoretical foundation for [1] more details.

Suppose we have training data {Zi}ni=1 where Zi = (Xi, Yi), and (X, Y )  P for some unknown distribution P. Suppose we have an order-invariant algorithm A that trains a mean-estimator given
some data. We will denote the full estimator as µ^, the leave-one-out (LOO) estimator as µ^-i, and the LOO residual as RiLOO, defined as:

µ^ := A {(Xj , Yj )}j[n]

(24)

µ^-i := A {(Xj , Yj )}j[n]\{i}

(25)

RiLOO := |Yi - µ^-i(Xi)|

(26)

We will also define q^n+,{vi} as the (n + 1) -th smallest (close to the -th quantile) of v1, . . . , vn, and q^n-,{vi} as the (n + 1) smallest value7.

The original Jackknife+ [4] does the following to construct a CI with finite-sample coverage guarantee (at level 1 - 2, but empirically usually covers 1 -  of the time):

· Step 1: Train the LOO estimator µ^-i for i  [n]. · Step 2: Collect the LOO residuals RiLOO for i  [n]. · Step 3 (inference): For a new data point (Xn+1, Yn+1), the Jackknife+ CI would be
C^Jackknife+(Xn+1) := [q^n-,{µ^-i(Xn+1) - RiLOO}, q^n+,1-{µ^-i(Xn+1) + RiLOO}] (27)

Assuming exchangeability of {Zi}ni=+11, [4] proves that

P{Yn+1  C^Jackknife+(Xn+1)}  1 - 2

(28)

Here the probability is taken over all training samples and the test data.

DJ aims to apply the above for deep learning algorithm A. The only difference between DJ and Jackknife+ is that replaces step 1 with step 1b below:

· Step 1b: replace µ^-i with µ^H-iOIF , which is estimated using µ^ and higher-order influence function (HOIF) without actually retraining the deep learning algorithm A.

C.2 Validity
Although using influence function (IF) to estimate µ^-i is possible, in practice there is almost no way to do this. For the coverage guarantee (Theorem 1 in [4]) to hold, it is important that for µ^-i, Zi and Zn+1 are also "exchangeable". In other words, µ^-i cannot see Zi at all, which is crucial in Step 2 of the proof of Theorem 1 (Section 6 in [4]). If µ^-i actually "remember" Zi somehow, then the last step of Step 2 in the proof breaks.
Unfortunately, µ^H-iOIF does "remember" the Zi it saw. The original paper [18] also uses IF to estimate the LOO models, but it only applies this to understand which training sample has more influence on the model, or in some qualitative assessment settings (as the name of the paper suggests). Even for such use case, [6] summarizes several issues with using IF in deep learning, one of which
7Note the + and - signs are used to distinguish the · and · operations.

23

is the error in estimating just the ranking of the influences even with first order IF estimated with
exact inverse-Hessian vector product (HVP). In [1], the HOIFs are computed recursively, and every
IF is computed with approximate HVP, which means there is little understanding in the quality of such estimates8. To actually achieve the theoretical guarantee in this setting, we need to eliminate completely the influence of Zi on the model parameters of µ^, which requires infinite-order exact IF and is clearly unrealistic.

C.3 Discrimination

The short answer to this is DJ is actually not discriminative, or at least not in practice. This can be found in our experiments in Section 4.2. [1] reports high AUPRC due to a code error9. It is worth
noting that the exact version of Jackknife+ does not show discrimination in the way claimed in [1]
either (See comparison in Figure 3). The varying width of the CI is a by-product of the construction

DJ (Covered 98.0%)

Jackknife+ (Covered 92.0%)

Split (Covered 95.0%)

20

20

20

15

15

15

10

10

10

5

5

5

0

0

0

5

5

5

10

10

10

1

0

1

2

3

1

0

1

2

3

1

0

1

2

3

Figure 3: DJ, Jackknife+ and Split conformal on the synthetic dataset.

y missed covered 90% CI

and proof, and is usually close to constant in practice. Intuitively, as n  , µ^-i  µ^, and the variance of the CI width would  0. Here are two simple thought experiments:
1. As n  , µ^-i  µ^, and the variance of the CI width would  0. 2. Suppose X follows a uniform distribution from {-10, -9, . . . , 0, . . . , 9, 10}, and Y = |X|.
Suppose A is a linear regression algorithm without intercept. As long as n is big enough, the CI for any input Xn+1 would be [-9, 9]. The error would however  |Xn+1| (because µ^(x)  0), so there is not discrimination at all.
As a result, DJ, the approximated version, could only potentially be discriminative due to some numerical instability and/or some effect that is orthogonal to the LOO procedure and the construction of the CI, which requires more exploration and detailed explanation.

C.4 Other Considerations
Order-invariance for A is rarely satisfied for the deep learning model. This is because deep learning model usually uses some variants of stochastic gradient descent (SGD) instead of gradient descent, which means permuting the input data would result in different µ^. However, it is also required for the proof in [4]. [1] did not mention this at all, which results in an incomplete proof even if every stated above are fixed. That said, the proof of [4] could easily be extended to training DNN with SGD as well, however it is out of scope of this discussion.
Scalability of the proposed method in DJ is not practical even the employed approximations. At training time, at least for the experiments in [1], directly performing the LOO procedure is faster than actually computing influence functions and estimating µ^-i. This of course depends on the number of training data-points vs. the number of parameters of the DNN. However, as we will discuss in Section C.5, there is no strong argument of using DJ in any scenario. Moreover, if we do not store all the LOO model weights (which has large space requirement), we would need to compute the IFs on the fly for each test data, which is prohibitively expensive.
Stability is another concern. In using the influence function, inverting Hessian is very expensive, so DJ follows [18] in using a stochastic Hessian Vector Product (HVP) method. However, one would
8In fact, based on the experiment, the errors seem to build up, as we will discuss in Section C.3. 9https://github.com/ahmedmalaa/discriminative-jackknife/blob/ e012d0a359aa8dac16fe03a99fa586966cf86ffe/UCI_experiments.py#L82

24

also need to get a good estimate of the eigenvalue of the Hessian10 in order for the HVP estimation process to converge meaningfully. In our experiment (and in the codes published by the authors of [1]), exact Hessian with small NNs have to be used, instead of HVP, due to stability issues. C.5 Conclusion If we take a step back, Jackknife+ was proposed as an improved version of the classical Jackknife that has finite-sample marginal coverage guarantee. The question it tries to address however is not just concerning finite-sample marginal coverage, but also about data scarcity: As noted in the original Jackknife+ paper [4], split conformal already has a finite-sample guarantee (at 1 -  level as opposed to 1 - 2 of Jackknife+), but the limitation is that it requires reserving a hold-out set. When the model requires more data to train, this might result in a poor fit. Of course, it is desirable to use all the data we have to train the base model. However, in many cases we only need a small portion of the data as the validation/calibration set. If data is abundant, this is not a concern, so one could use split conformal (or CQR, MADSplit, LVD, etc.). If the data is actually very scarce, then usually the model cannot be too complicated, so directly performing the LOO cross-validation with Jackknife+ would not be too expensive and will keep the theoretical guarantee. If we use DJ, we spend more time on a much more complicated algorithm that ends up with a CI without the theoretical guarantee.
10which can be very large and thus unstable to estimate according to [6] 25

D Data
In this section, we will try our best to list the licenses of the public datasets we use and how the consent was obtained. · UCI Yacht Hydrodynamics (Yacht)[37]: We could not find the license for this dataset. The dataset
was created by "Ship Hydromechanics Laboratory, Maritime and Transport Technology Department, Technical University of Delft", and donated by "Dr Roberto Lopez" per [37]. · UCI Bikesharing (Bike) [34, 10]: The original data was provided according to the Capital Bikeshare Data License Agreement https://www.capitalbikeshare.com/data-license-agreement. We could not find details on how the data was obtained. · UCI Energy Efficiency (Energy)[36, 33]: We could not find the license for this dataset. The dataset was created by Angeliki Xifara (angxifara '@' gmail.com, Civil/Structural Engineer) and was processed by Athanasios Tsanas (tsanasthanasis '@' gmail.com, Oxford Centre for Industrial and Applied Mathematics, University of Oxford, UK). · UCI Concrete Compressive Strength (Concrete)[35, 44]: We could not find the license for this dataset. The dataset was original owned and donated by Prof. I-Cheng Yeh at Department of Information Management at Chung-Hua University, Taiwan, R.O.C. · Boston Housing (Housing)[9]: We could not find the license for this dataset. This dataset contains information collected by the U.S Census Service concerning housing in the area of Boston Mass11. · Kin8nm[16]: We could not find the license for this dataset. The original parent dataset (the "kin" dataset) was contributed by Zoubin Ghahramani12. · QM8 [28, 30] and QM9 [30, 27]: We could not find the original license for these datasets, but they are discributed under CC By 4.0 13. They are obtained in [28] and [27].
11https://www.cs.toronto.edu/ delve/data/boston/bostonDetail.html 12https://www.cs.toronto.edu/ delve/data/kin/desc.html 13https://tdcommons.ai/single_pred_tasks/qm/
26

