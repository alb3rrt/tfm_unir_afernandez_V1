Discrete Cosine Transform as Universal Sentence Encoder
Nada Almarwani 1,2 and Mona Diab 1,3 1 Dep. of Computer Science, The George Washington University 2 Dep. of Computer Science, College of Computer Science and Engineering, Taibah University
3 Facebook AI Research nmarwani@taibah.edu.sa, mdiab@fb.com

arXiv:2106.00934v1 [cs.CL] 2 Jun 2021

Abstract
Modern sentence encoders are used to generate dense vector representations that capture the underlying linguistic characteristics for a sequence of words, including phrases, sentences, or paragraphs. These kinds of representations are ideal for training a classifier for an end task such as sentiment analysis, question answering and text classification. Different models have been proposed to efficiently generate general purpose sentence representations to be used in pretraining protocols. While averaging is the most commonly used efficient sentence encoder, Discrete Cosine Transform (DCT) was recently proposed as an alternative that captures the underlying syntactic characteristics of a given text without compromising practical efficiency compared to averaging. However, as with most other sentence encoders, the DCT sentence encoder was only evaluated in English. To this end, we utilize DCT encoder to generate universal sentence representation for different languages such as German, French, Spanish and Russian. The experimental results clearly show the superior effectiveness of DCT encoding in which consistent performance improvements are achieved over strong baselines on multiple standardized datasets.
1 Introduction
Recently, a number of sentence encoding representations have been developed to accommodate the need of sentence-level understanding; some of these models are discussed in (Hill et al., 2016; Logeswaran and Lee, 2018; Conneau et al., 2017), yet most of these representations have focused on English only.
To generate sentence representations in different languages, the most obvious solution is to train monolingual sentence encoders for each language. However, training a heavily parameterized mono-

lingual sentence encoder for every language is inefficient and computationally expensive, let alone the impact on the environment. Thus, utilizing a non-parameterized model with ready-to-use word embeddings is an efficient alternative to generate sentence representations in various languages.
A number of non-parameterized models have been proposed to derive sentence representations from pre-trained word embeddings (Ru¨ckle´ et al., 2018; Yang et al., 2019; Kayal and Tsatsaronis, 2019). However, most of these models, including averaging, disregard structure information, which is an important aspect of any given language. Recently, Almarwani et al. (2019) proposed a structure-sensitive sentence encoder, which utilizes Discrete Cosine Transform (DCT) as an efficient alternative to averaging. The authors show that this approach is versatile and scalable because it relies only on word embeddings, which can be easily obtained from large unlabeled data. Hence, in principle, this approach can be adapted to different languages. Furthermore, having an efficient, readyto-use language-independent sentence encoder can enable knowledge transfer between different languages in cross-lingual settings, empowering the development of efficient and performant NLP models for low-resource languages.
In this paper, we empirically investigate the generality of DCT representations across languages as both a single language model and a cross-lingual model in order to assess the effectiveness of DCT across different languages.
2 DCT as sentence Encoder
In signal processing domain DCT is used to decompose signal into component frequencies revealing dynamics that make up the signal and transitions within (Shu et al., 2017). Recently, DCT has been adopted as a way to compress textual information

(Kayal and Tsatsaronis, 2019; Almarwani et al., 2019). A key observation in NLP is that word vectors obey laws of algebra King ­ Man + Woman = (approx.) Queen (Mikolov et al., 2013). Thus, given word embeddings, cast a sentence as a multidimensional signal over time, in which DCT is used to summarize the general feature patterns in word sequences and compress them into fixed-length vectors (Kayal and Tsatsaronis, 2019; Almarwani et al., 2019).

Mathematically, DCT is an invertible function that maps an input sequence of N real numbers to the coefficients of N orthogonal cosine basis functions of increasing frequencies (Ahmed et al., 1974). The DCT components are arranged in order of significance. The first coefficient (c[0]) represents the sum of the input sequence normalized by the square length, which is proportional to the average of the sequence (Ahmed et al., 1974). The lower-order coefficients represent lower signal frequencies which correspond to the overall patterns in the sequence. For example, DCT is used for compression by preserving only the coefficients with large magnitudes. These coefficients can be used to reconstruct the original sequence exactly using the inverse transform (Watson, 1994).

In NLP, Kayal and Tsatsaronis (2019) applied DCT at the word level to reduce the dimensionality of the embeddings size, while Almarwani et al. (2019) applied it along the sentence length as a way to compress each feature in the embedding space independently. In both implementations, the top coefficients are concatenated to generate the final representation for a sentence. As shown in (Almarwani et al., 2019), applying DCT along the features in the embeddings space renders representations that yield better results. Also, Zhu and de Melo (2020) noted that similar to vector averaging the DCT model proposed by (Almarwani et al., 2019) yields better overall performance compared to more complex encoders, thus, in this work, we adopt their implementation to extract sentencelevel representations.

Specifically, given a sentence matrix N × d, a se-

quence of DCT coefficients c[0], c[1], ..., c[K] are

calculated by applying the DCT type II along the

d-dimensional word embeddings, where c[K] =

2 N

N -1 n=0

vn

cos

 N

(n

+

1 2

)K

(Shao

and

John-

son, 2008). Finally, a fixed-length sentence vector

of size Kd is generated by concatenating the first

Task

Description

SentLen Length prediction

WC

Word Content analysis

BShift Word order analysis

TreeDepth Tree depth prediction

Tense

Verb tense prediction

CoordInv Coordination Inversion

SubjNum Subject number prediction

ObjNum Object number prediction

SOMO Semantic odd man out

Table 1: Probing Tasks as described in (Conneau et al., 2018; Ravishankar et al., 2019).

K DCT coefficients, which we refer to as c[0 : K].1

3 Multi-lingual DCT Embeddings
3.1 Experimental Setups and Results
In our study, DCT is used to learn a separate encoder for each language from existing monolingual word embeddings. To evaluate DCT embeddings across different languages, we used the probing benchmark provided by Ravishankar et al. (2019), which includes a set of multi-lingual probing datasets.2 The benchmark covers five languages: English, French, German, Spanish and Russian, derived from Wikipedia. The task set comprises 9 probing tasks, summarized in Table 1, that address varieties of linguistic properties including surface, syntactic, and semantic information (Conneau et al., 2018; Ravishankar et al., 2019). Ravishankar et al. (2019) used the datasets to evaluate different sentence encoders trained by mapping sentence representations to English. Unlike Ravishankar et al. (2019), we use the datasets to evaluate DCT embeddings for each language independently. As a baseline, in addition to the DCT embeddings, we use vector averaging to extract sentence representations from the pre-trained embeddings.
For model evaluations, we utilize the SentEval framework introduced in (Conneau and Kiela, 2018). In all experiments, we use a single-layer MLP on top of DCT sentence embeddings with the following parameters: kfold=10, batch size=128, nhid=50, optim=adam, tenacity=5, epoch size=4.
1Unlike (Almarwani et al., 2019), we note no further improvements with larger coefficients, thus, we only report the results of 1  K  4.
2Refer to (Conneau et al., 2018) and (Ravishankar et al., 2019) for more details about the probing tasks.

Figure 1: Results of the probing tasks comparing XX languages performance relative to English. White indicates a value of 1, demonstrating parity in performance with English. Red indicates better English performance while green indicates better XX Lang results.

For the word embeddings, we relied on the publicly available pre-trained FastText embeddings introduced in (Grave et al., 2018).3
Results: Figure 1 shows a heat-map reflecting the probing results of the different languages relative to English. Overall, French (FR) seems to be the closest to English (EN) followed by Spanish (ES) then German (DE) and then finally Russian (RU) across the various DCT coefficients. Higher coefficients reflect majority better performance across most tasks for FR, ES and DE. We see the most variation with worse results than English on the syntactic tasks of TreeDepth, CoordInv, Tense, SubjNum and ObjNum for RU. SOMO stands out for RU where it outperforms EN. The variation in Russian might be due to the nature of RU being a more complex language that is morphologically rich with flexible word order (Toldova et al., 2015).
In terms of the performance per number of DCT coefficients, we observe consistent performance gain across different languages that is similar to the English result trends. Specifically, for the surface level tasks, among the DCT models the c[0] model significantly outperforms the AV G with an increase of 30 percentage points in all languages. The surface level tasks (SentLen and WC) show the most notable variance in performance, in which the highest results are obtained using the c[0] model. However, the performance decreases in all languages when K is increased. On the other hand, for all languages, we observe a positive effect on the model's performance with larger K in both the syntactic and semantic tasks. The complete numerical results are presented in the Appendix in Table
3Available at: https://fasttext.cc.

5.
4 Cross-lingual Mapping based on DCT Encoding
4.1 Approach
Aldarmaki and Diab (2019)proposed sentence-level transformation approaches to learn context-aware representations for cross-lingual mappings. While the word-level cross-lingual transformations utilize an aligned dictionary of word embeddings to learn the mapping, the sentence-level transformations utilize a large dictionary of parallel sentence embeddings. Since sentences provide contexts that are useful for disambiguation for the individual word's specific meaning, sentence-level mapping yields a better cross-lingual representation compared to word-level mappings.
A simple model like sentence averaging can be used to learn transformations between two languages as shown in (Aldarmaki and Diab, 2019). However, the resulting vectors fail to capture structural information such as word order, which may result in poor cross-lingual alignment. Therefore, guided by the results shown in (Aldarmaki and Diab, 2019), we further utilize DCT to construct sentence representations for the sentencelevel cross-lingual modeling.
4.2 Experiments Setups and Results
For model evaluation, we use the same crosslingual evaluation framework introduced in (Aldarmaki and Diab, 2019). Intuitively, sentences tend to be clustered with their translations when their vectors exist in a well-aligned cross-lingual space. Thus, in this framework, cross-lingual mapping ap-

proaches are evaluated using sentence translation retrieval by calculating the accuracy of correct sentence retrieval. Formally, the cosine similarity is used to find the nearest neighbor for a given source sentence from the target side of the parallel corpus.
4.3 Evaluation Datasets and Results
To demonstrate the efficacy of cross-lingual mapping using the sentence-level representation generated by DCT models, similarly to Aldarmaki and Diab (2019), we used the WMT'13 data set that includes EN, ES and DE languages (Bojar et al., 2013). We further used five language pairs from the WMT'17 translation task to evaluate the effectiveness of DCT-based embeddings. Specifically, we used a sample of 1 million parallel sentences from WMT'13 common-crawl data; this subset is the same one used in (Aldarmaki and Diab, 2019).4 To assess efficacy of the DCT models for the crosslingual mapping, we reported the performances of the sentence translation retrieval task within the WMT'13 test set, which includes EN, ES, and DE as test languages (Bojar et al., 2013). Specifically, we first used the 1M parallel sentences for the alignment between source languages (ES and DE) to a target language (EN) independently. We evaluated the translation retrieval performance in all language directions, from source languages to English: ESEN and DE-EN, as well as between the sources languages: ES-DE.
Similarly, we conduct a series of experiments on 5 different language pairs from the WMT'17 translation task, which includes DE, Latvian (LV), Finnish (FI), Czech (CS), and Russian (RU), each of which is associated with an English translation (Zhang et al., 2018).5 For each language pair, we sampled 1M parallel sentences from their training corpus for the cross-lingual alignment between each source language and EN. Also, we used the test set available for each language pair to evaluate the translation retrieval performances.
In our experiments, we evaluate the translation retrieval performance in all language directions using three type of word embeddings: 1- a publicly available pre-trained word embeddings in which we show the performance of DCT against averaging, which we refer to hereafter as out-of-domain
4Evaluation scripts and WMT'13 dataset as described in (Aldarmaki and Diab, 2019) are available in https://github.com/h-aldarmaki/sent translation retrieval
5The pre-processed version of the WMT'17 dataset was used. For more information refer to (Zhang et al., 2018).

Lang pair AV G LangEN ESEN 65.67 DEEN 51.80 RUEN 45.22 CSEN 41.87 FIEN 40.46 LVEN 21.26 ENLang ENES 69.97 ENDE 67.50 ENRU 38.09 ENCS 39.73 ENFI 39.34 ENLV 15.83 Lang1Lang2 DEES 43.80 ESDE 57.67

c[0]
64.87 50.30 52.75 42.50 42.00 40.13
69.50 66.23 44.29 40.40 42.52 33.55
42.20 56.46

c[0 : 1]
71.26 57.23 61.91 52.89 47.57 51.42
73.73 69.27 54.73 50.99 51.67 47.08
49.50 60.53

c[0 : 2]
71.80 58.13 64.35 54.99 47.80 56.37
73.87 68.70 59.51 54.00 52.59 53.22
51.20 59.83

c[0 : 3]
70.13 56.57 63.33 55.05 46.16 60.16
71.73 65.83 60.94 54.12 51.74 55.72
51.17 57.87

Table 2: Sentence translation retrieval accuracy based on out of domain pre-trained Fasttext embeddings. Arrows indicate the direction, with English (EN ), Spanish (ES), German (DE), Russian (RU ), Czech (CS), Finnish (F I) , Turkish (T R), and Latvian (LV ).

embeddings as shown in Table 2. 2- Also, we ran additional experiments in which we used a domain specific word embedding (that we trained on genre that is similar to the translation task) and 3-contextualized word embedding, which we refer to hereafter as in-domain embeddings as shown in Table 3.
Out-of-domain embeddings: For all language pairs, DCT-based models outperform AVG and c[0] models in the sentence translation retrieval task. In the direction  EN , while the c[0:2] model achieve the highest accuracy for ES, DE, RU, and FI languages, the c[0:3] model achieved the highest accuracy for CS and LV languages. Specifically, the c[0:2] model yields increases of 5.59%-30% in the direction from source languages (ES, DE, RU, and FI) to English compared to the AVG model. Also, while the c[0:3] model yielded an increase of 13% gains over the baseline for CS, it provides the most notable increase of 38% for LV. For the opposite directions EN  source, the DCT-based embeddings model also outperformed AVG and c[0] models. In particular, we observed accuracy gains of at least 3.81% points using more coefficients in DCT-based models compared to the AVG and c[0] models for all languages. A similar trend is observed in the zero-shot translation retrieval between the two non English languages (ES and DE), in which DCT-based models outperform the AVG and c[0] models.

Lang pair Embed LangEN ESEN FT
BERT DEEN FT
BERT ENLang ENES FT
BERT ENDE FT
BERT Lang1Lang2 DEES FT
BERT ESDE FT
BERT

AV G
82.97 92.10 79.33 89.76
82.33 93.63 74.73 91.30
73.27 87.80 68.90 87.70

c[0]
82.40 92.00 78.73 89.66
82.07 93.66 74.50 91.43
72.20 87.57 68.07 87.70

c[0 : 1]
84.50 93.23 81.87 91.83
85.47 94.10 79.10 91.90
77.43 90.23 73.97 89.67

c[0 : 2]
83.97 93.13 80.20 91.20
84.60 94.00 78.70 91.53
75.96 90.36 73.10 89.50

c[0 : 3]
82.90 92.20 77.93 90.57
83.17 92.80 76.90 90.30
74.60 88.96 72.43 88.53

Model FastText (dict) [ALD2019] ELMo (word) [ALD2019] FastText (word) [ALD2019] FastText AV G (sent) [ALD2019] ELMo AV G (sent) [ALD2019] FastText c[0] (sent) FastText c[0 : 1] (sent) FastText c[0 : 2] (sent) FastText c[0 : 3] (sent) mBERT AV G (sent) mBERT c[0] (sent) mBERT c[0 : 1] (sent) mBERT c[0 : 2] (sent) mBERT c[0 : 3] (sent)

Average Accuracy 69.04 82.23 74.00 76.92 84.03 76.33 80.39 79.42 77.99 90.38 90.34 91.83 91.62 90.56

Table 3: Accuracy using in-domain FastText (FT) and Contextualized mBERT embeddings. The best results for each row in Bold & for each direction in gray .
In-domain embeddings: To ensure comparability to state-of-the-art results, we further utilized indomain FastText embeddings as those used in (Aldarmaki and Diab, 2019) as well as contextualizedbased word embeddings. For the in-domain FastText embeddings, the FastText (Bojanowski et al., 2017) is utilized to generate word embeddings from 1 Billion Word benchmark (Chelba et al., 2014) for English, and equivalent subsets of about 400 million tokens from WMT'13 (Bojar et al., 2013) news crawl data. For the contextualized-based embeddings, we utilized multilingual BERT (mBERT) introduced in (Devlin et al., 2019) as contextual word embeddings, in which representations from the last BERT layer are taken as word embeddings. As shown in Table 3, using in-domain word embeddings yields stronger results compared to the pre-trained embeddings we use in the previous experiments as illustrated in Table 2. On the other hand, we observe additional improvements using mBERT as word embeddings on all models. Furthermore, increasing K has positive effect on both embeddings, in which c[0 : 1] demonstrate performance gains compared to other models in all language directions. This trend is clearly observed in the zero-shot performance between the non English languages.

Table 4: The average accuracy of various models across all language retrieval directions as reported in (Aldarmaki and Diab, 2019), refer to as [ALD2019] in the table, along with the different DCT-based models in this work, in which (word) refers to word-level mapping, (sent) refers to sentence-level mapping, and (dict) refers to the baseline (using a static dictionary for mapping). Bold shows the best overall result.
5 Conclusion
In this paper, we extended the application of DCT encoder to multi- and cross-lingual settings. Experimental results across different languages showed that similar to English using DCT outperform the vector averaging. We further presented a sentencelevel-based approach for cross-lingual mapping without any additional training parameters. In this context, the DCT embedding is used to generate sentence representations, which are then used in the alignment process. Moreover, we have shown that incorporating structural information encoded in the lower-order coefficients yields significant performance gains compared to the AVG in sentence translation retrieval.
Acknowledgments
We thank Hanan Aldarmaki for providing us the in-domain FastText embeddings and for sharing many helpful insights. We would also like to thank 3 anonymous reviewers for their constructive feedback on this work.

Furthermore, as shown in Table 4, we obtained a state-of-the-art result using mBERT c[0 : 1] with 91.83% average accuracy across all translation directions compared to the 84.03% average accuracy of ELMo as reported in (Aldarmaki and Diab, 2019).

References
Nasir Ahmed, T Natarajan, and Kamisetty R Rao. 1974. Discrete cosine transform. IEEE transactions on Computers, 100(1):90­93.
Hanan Aldarmaki and Mona Diab. 2019. Context-

aware cross-lingual mapping. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3906­3911, Minneapolis, Minnesota. Association for Computational Linguistics.
Nada Almarwani, Hanan Aldarmaki, and Mona Diab. 2019. Efficient sentence embedding using discrete cosine transform. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pages 3672­3678, Hong Kong, China. Association for Computational Linguistics.
Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with subword information. Transactions of the Association for Computational Linguistics, 5:135­146.
Ondrej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 1­44, Sofia, Bulgaria. Association for Computational Linguistics.
Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. 2014. One billion word benchmark for measuring progress in statistical language modeling. In INTERSPEECH 2014, 15th Annual Conference of the International Speech Communication Association, Singapore, September 14-18, 2014, pages 2635­ 2639. ISCA.
Alexis Conneau and Douwe Kiela. 2018. SentEval: An evaluation toolkit for universal sentence representations. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).
Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo¨ic Barrault, and Antoine Bordes. 2017. Supervised learning of universal sentence representations from natural language inference data. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 670­680, Copenhagen, Denmark. Association for Computational Linguistics.
Alexis Conneau, German Kruszewski, Guillaume Lample, Lo¨ic Barrault, and Marco Baroni. 2018. What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2126­2136, Melbourne, Australia. Association for Computational Linguistics.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of

deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171­4186, Minneapolis, Minnesota. Association for Computational Linguistics.
Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Armand Joulin, and Tomas Mikolov. 2018. Learning word vectors for 157 languages. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).
Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016. Learning distributed representations of sentences from unlabelled data. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1367­1377, San Diego, California. Association for Computational Linguistics.
Subhradeep Kayal and George Tsatsaronis. 2019. EigenSent: Spectral sentence embeddings using higher-order dynamic mode decomposition. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4536­ 4546, Florence, Italy. Association for Computational Linguistics.
Lajanugen Logeswaran and Honglak Lee. 2018. An efficient framework for learning sentence representations.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111­3119.
Vinit Ravishankar, Lilja Øvrelid, and Erik Velldal. 2019. Probing multilingual sentence representations with X-probe. In Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP2019), pages 156­168, Florence, Italy. Association for Computational Linguistics.
Andreas Ru¨ckle´, Steffen Eger, Maxime Peyrard, and Iryna Gurevych. 2018. Concatenated power mean word embeddings as universal crosslingual sentence representations. arXiv preprint arXiv:1803.01400.
Xuancheng Shao and Steven G Johnson. 2008. Typeii/iii dct/dst algorithms with reduced number of arithmetic operations. Signal Processing, 88(6):1553­ 1564.
Xiao Shu, Xiaolin Wu, and Bolin Liu. 2017. A study on quantization effects of dct based compression. In 2017 IEEE International Conference on Image Processing (ICIP), pages 3500­3504. IEEE.

S Toldova, O Lyashevskaya, A Bonch-Osmolovskaya, and M Ionov. 2015. Evaluation for morphologically rich language: Russian nlp. In Proceedings on the International Conference on Artificial Intelligence (ICAI), page 300. The Steering Committee of The World Congress in Computer Science, Computer . . . .
Andrew B Watson. 1994. Image compression using the discrete cosine transform. Mathematica journal, 4(1):81.
Ziyi Yang, Chenguang Zhu, and Weizhu Chen. 2019. Parameter-free sentence embedding via orthogonal basis. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 638­648, Hong Kong, China. Association for Computational Linguistics.
Biao Zhang, Deyi Xiong, and Jinsong Su. 2018. Accelerating neural transformer via an average attention network. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1789­1798, Melbourne, Australia. Association for Computational Linguistics.
Xunjie Zhu and Gerard de Melo. 2020. Sentence analogies: Linguistic regularities in sentence embeddings. In Proceedings of the 28th International Conference on Computational Linguistics, pages 3389­ 3400, Barcelona, Spain (Online). International Committee on Computational Linguistics.
A Appendices
Table 5 shows the complete numerical results for the probing tasks on all languages.

SentLen WC Bshift
TreeDepth Tense
CoordInv SubjNum ObjNum SOMO

Language EN ES FR DE RU EN ES FR DE RU EN ES FR DE RU EN ES FR DE RU EN ES FR DE RU EN ES FR DE RU EN ES FR DE RU EN ES FR DE RU EN ES FR DE RU

AVG 56.28 59.92 57.9 53.41 54.42 26.97 25.4 27.14 29.33 36.33 54.78 54.7 54.69 54.23 56.48 41.34 42.9 41.06 37.06 35.27 86.49 94.52 91.96 94.13 86.07 73.47 67.08 71.06 74.25 60.33 76.46 86.4 88.48 75.94 70.47 68.44 78.31 77.47 68.38 63.9 50.12 51.7 50.7 50.57 52.49

c[0] 89.03 89.59 93.72 88.81 89.66 66.69 64.80 68.60 64.99 67.50 54.98 54.52 54.7 54.22 56.8 45.18 48.53 47.68 41.97 39.21 89.23 95.97 94.06 94.71 86.39 74.22 68.13 71.12 74.33 60.77 77.41 86.68 88.62 75.78 70.44 69.71 79.23 78.5 68.74 63.79 50.91 51.98 48.85 50.47 52.91

c[0:1] 88.91 90.00 93.44 88.36 89.12 64.55 62.18 66.13 64.52 65.58 54.58 54.53 54.68 54.35 56.81 48.64 52.29 50.05 45.14 40.76 91.83 96.68 95.7 95.82 90.28 84.56 81.61 85.97 89.99 79.95 80.49 89.34 91.05 78.79 72.31 71.78 82.21 83.74 69.88 65.33 51.72 51.34 48.87 49.99 52.86

c[0:2] 88.95 89.8 93.14 88.16 89.18 62.49 60.62 64.71 63.93 64.69 54.86 54.21 54.91 54.43 56.28 49.84 53.34 51.65 47.33 41.02 92.17 96.67 95.96 96.44 90.4 87.20 84.15 88.03 92.52 83.13 81.68 90.42 92.23 78.9 72.81 73.24 83.96 85.82 70.41 65.32 51.71 49.62 49.44 49.99 52.8

c[0:3] 88.7 89.73 92.82 87.54 88.26 60.39 58.76 62.8 63.12 62.69 54.81 54.71 55.53 54.6 57.4 49.44 53.87 52.27 47.55 40.65 92.26 96.62 96.12 96.28 90.16 87.03 85.17 89.21 93.45 84.03 81.76 90.12 92.72 79.25 73.12 73.98 85.2 86.92 71.14 65.54 51.36 50.71 49.56 49.99 53.07

c[0:4] 88.08 90.05 92.38 87.69 88.04 59.08 57.64 61.04 61.54 61.32 55.58 55.77 56.50 56.46 58.51 50.47 53.54 52.15 47.36 40.51 92.21 96.53 95.99 95.92 90.38 87.19 85.77 89.61 94.09 84.34 82.31 90.84 92.76 79.28 73.13 74.93 85.7 88.1 71.90 65.11 50.42 53.07 49.36 49.99 53.13

Table 5: DCT embeddings Performance per language compared to AVG. EN=English, ES=Spanish, FR=French, DE=German, and RU=Russian

