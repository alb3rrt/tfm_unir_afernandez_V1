arXiv:2106.00517v3 [cs.AI] 3 Jun 2021

Cooperative Multi-Agent Transfer Learning with Level-Adaptive Credit Assignment
Tianze Zhou1 Fubiao Zhang1 Kun Shao2 Kai Li3 Wenhan Huang3
Jun Luo2 Weixun Wang4 Yaodong Yang2 Hangyu Mao2
Bin Wang2 Dong Li2 Wulong Liu2 Jianye Hao 2
1 Beijing Institute of Technology {tianzezhou, fubiao.zhang}@bit.edu.cn 2 Noah's Ark Lab, Huawei Technologoies {shaokun2, haojianye}@huawei.com
3 Shanghai Jiao Tong University 4 Tianjin University
Abstract
Extending transfer learning to cooperative multi-agent reinforcement learning (MARL) has recently received much attention. In contrast to the single-agent setting, the coordination indispensable in cooperative MARL constrains each agent's policy. However, existing transfer methods focus exclusively on agent policy and ignores coordination knowledge. We propose a new architecture that realizes robust coordination knowledge transfer through appropriate decomposition of the overall coordination into several coordination patterns. We use a novel mixing network named level-adaptive QTransformer (LA-QTransformer) to realize agent coordination that considers credit assignment, with appropriate coordination patterns for different agents realized by a novel level-adaptive Transformer (LA-Transformer) dedicated to the transfer of coordination knowledge. In addition, we use a novel agent network named Population Invariant agent with Transformer (PIT) to realize the coordination transfer in more varieties of scenarios. Extensive experiments in StarCraft II micro-management show that LA-QTransformer together with PIT achieves superior performance compared with state-of-the-art baselines.
1 Introduction
Coordination in multi-agent reinforcement learning (MARL) is a popular topic in fields ranging from robotics [9, 8], computer games [2, 33] to recommendation systems [35]. Centralized training with decentralized execution (CTDE) is a popular regime in cooperative MARL to realize efficient agent coordination. Existing CTDE research covers important topics such as division of agents [27], diversification [32] and exploration [19]. Recent works [29, 11, 1, 17, 16] have also started to make progress in transfer learning in cooperative MARL. For example, Liu et al. [16] use policy distillation [22] to achieve fixed agent transfer learning. However, the agent population varies in different tasks in most cases. To solve this problem, DyAN [29] uses a graph neural network to adapt to dynamic agent population. UPDeT [11] uses Transformer[24] to realize a universal and transferable agent
*Work done during an internship at Noah's Ark Lab, Huawei Technologoies.
Preprint. Under review.

policy network to achieve agent-level knowledge transfer. However, these methods all focus on the transfer of individual agent policy and ignore the coordination knowledge. Unlike single-agent tasks, cooperative multi-agent tasks require the coordination of multiple agents. Ignoring coordination knowledge may lead to biased transfer because the difference in coordination implies a difference in agent policy.

In cooperative MARL, while the joint policy differs from task to task, the underlying coordination may be decomposed into several patterns that remain valid across different tasks. Figure 1 illustrates this point in StarCraft II, where agents tend to form three different coalition patterns [10] with different coalition patterns accomplishing different sub-tasks. By leveraging decomposition according to coordination patterns, we may achieve robust coordination knowledge transfer. The example in Figure 1 also suggests that the coordination patterns tend to involve a regular number of agents, such as pairwise coordination patterns, triplet coordination, etc. This means knowledge transfer on coordination policy is manageable in terms of scale.

Transformer [24] is a popular module to capture the relationship among elements and is widely used in nature language process [14] and computer vision [5]. In this paper, we use Transformer to capture the correlation between agents and construct the coordination patterns. However, the traditional Transformer module can only construct the pairwise coordination pattern. While stacking multiple Transformer modules could allow us to go beyond the pairwise pattern, this approach is not suitable for large-scale multi-agent scenarios due to the enormous computational cost. Instead, we propose the leveladaptive Transformer (LA-Transformer), which can adaptively capture agent-specific coordination levels and realize coordination patterns involving a variable number of agents. We realize the LA-Transformer using both hard attention and a hybrid-based method. The hard LA-Transformer focuses on the most appropriate coordination level, while the hybrid LATransformer merges the features from multiple levels.

1. three coalition patterns are formed 2. different patterns move independently 3. same patterns attack enemies cooperatively

Proper credit assignment is essential for coordination among

multiple agents in both policy-based [6, 30] and value-based

[23] cooperative MARL. The credit each agent receives must

time step

reflect their contribution towards the coordinated performance.

In CTDE for MARL, a trainable mixing network is often used

to implement the required credit assignment. We follow this

practice and introduce LA-Transformer into the design of a Figure 1: Coordination patterns in

novel mixing network named level-adaptive QTransformer StarCraft II.

(LA-QTransformer). Compared with other mixing networks

such as QMIX [21] and Qatten [31], LA-QTransformer is a

more expressive mechanism for coordination policy learning and coordination knowledge transfer.

However, redesigning the mixing network alone cannot achieve adequate coordination transfer because the dynamic agent population size limits the agent policy to be reused. Previous methods implicitly assume that the joint action space has a fixed dimension or only small and medium-sized scenarios need to be handled. Such assumptions make it hard to apply them in real scenarios. To handle this problem, we design a novel agent structure called Population Invariant agent with Transformer (PIT) to realize generalized coordination knowledge transfer in scenarios of different scales with variable agent numbers.

Evaluation of our new method with the SMAC benchmark [25] shows that it outperforms current SOTA methods in transfer scenarios as well as non-transfer scenarios. In addition, a curricular training experiment with an increasing number of agents validated the robustness of our method. Finally, we demonstrate the interpretability of the proposed modules and confirm the contribution of LA-Transformer with ablation studies.

2

2 Backgrounds

2.1 Cooperative multi-agent Q-learning

The fully cooperative MARL task can be formulated as a Dec-POMDP [20]. A tuple can represent
Dec-POMDP I, S, U, Z, P, R, O, n,  , where s  S represents the global state of the environment. At any time, each agent i  I  {1, ..., n} interacts with the environment by generating corresponding action ui  U through it's local observation vector zi  Z according to the observation function O(s, i). The overall objective is to maximize the cumulative reward R from environmental feedback. The environment receives the joint action a, and transfers to the next state s according to the state transition function P (s | s, a). n defines the number of agents, and  represents the discount factor.

Centralized training with decentralized execution [15] (CTDE) is a popular regime to address the

Dec-POMDP problem. In the CTDE framework, the mixing network is introduced to merge all individual Q values into Qtot:

Qtot(, u, s; ) = f ([Qi( i, ui)]ni , s; ).

(1)

And then TD-learning is used to train the whole network

b

L() =

yitot - Qtot(, u, s; ) 2 ,

(2)

i=1

where b is the batch size of replay buffer, ytot = r +  maxu Qtot ( , u , s ; -), and - is the parameter of target network.

2.2 Multi-agent transfer learning

The basic idea behind transfer learning is that the knowledge acquired from previous tasks can be reused to accelerate learning drastically, and it makes the learning of complex tasks feasible [4, 3]. Due to the complexity of MARL, multi-agent transfer learning is not a straightforward extension of single-agent transfer learning. In the multi-agent setting, the policy mapping expands from a single agent to multiple agents, and the dimension of the mapping is varying with specific tasks:

Jp  Jc : A1 × . . . × An  A1 × . . . × Am

(3)

where Jp and Jc represent the joint-policy in the previous task and the current task, respectively, and n, m shows the number of agents in these tasks.

[4] divides multi-agent transfer learning (MATL) into two main types: the intra-agent transfer and the inter-agent transfer. The intra-agent transfer focuses on the relationship between the source tasks and the target tasks, while the inter-agent transfer pays more attention to reusing knowledge received from communication with other agents.

2.3 Transformer

The Transformer is an attention-based neural network structure widely used in nature language process and computer vision. The traditional Transformer module consists of two sub-structures, the attention module and the feed-forward network. Soft attention and hard attention are two approaches to realize the attention mechanism. Soft attention takes the softmax function to calculate the input elements relationship.

Attention (Q, K, V) = softmax

QKT 

V,

(4)

dk

 where Q, K, V represents the query, keys, values of input elements respectively and dk is the

normalization coefficient.

Due to element weights is calculated directly, soft attention is fully differentiable. However, the softmax function weakens the ground truth element's weight, limiting the actual performance. Hard attention overcomes the limitation of soft attention by selecting the sole element. However, this selecting operation is non-differentiable. Gumbel softmax [12] is a popular trick to approximate hard attention performance while keeping the neural network back-propagation differentiable. To

3

enhance the representation of the embedding features, Transformer utilizes a feed-forward network. The feed-forward network contains a 1-D convolutional layer and a layer-normalization module.
Transformer is suitable to capture elements relationship in cooperative MARL. Due to Transformer's flexible I/O characteristic, it can handle dynamic element inputs. Compared with RNN-based methods, Transformer does not care about the order of elements and can process elements in parallel.

3 Methods

In this section, we design a novel value-based framework to realize coordination knowledge transfer in cooperative MARL. Figure 2 describes the whole structure of our methods. It contains a mixing network level-adaptive QTransformer (LA-QTransformer) that utilizes the level-adaptive Transformer (LA-Transformer) module to realize the coordination knowledge transfer and the agent network, Population Invariant agent with Transformer (PIT), to achieve coordination transfer in universal scenarios.

Self Action

Interaction Action

MLP

e g <latexitsha1_base64="wXiblHLVlmRu4CTIY7JB2oIRM7I=">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0WPRi8cK9gPaUDbbSbt0swm7G6GE/ggvHhTx6u/x5r9x0+agrQ8GHu/NMDMvSATXxnW/ndLa+sbmVnm7srO7t39QPTxq6zhVDFssFrHqBlSj4BJbhhuB3UQhjQKBnWByl/udJ1Sax/LRTBP0IzqSPOSMGit1cJCNVDobVGtu3Z2DrBKvIDUo0BxUv/rDmKURSsME1brnuYnxM6oMZwJnlX6qMaFsQkfYs1TSCLWfzc+dkTOrDEkYK1vSkLn6eyKjkdbTKLCdETVjvezl4n9eLzXhjZ9xmaQGJVssClNBTEzy38mQK2RGTC2hTHF7K2FjqigzNqGKDcFbfnmVtC/q3lXdfbisNW6LOMpwAqdwDh5cQwPuoQktYDCBZ3iFNydxXpx352PRWnKKmWP4A+fzB7NHj84=</latexit>

ru

esel f <latexitsha1_base64="iQkPRZktHnPzJKrKfXeb8bFbHp0=">AAAB73icbVDLSgMxFL1TX7W+qi7dBIvgqsyIosuiG5cV7APaoWTSO21oJjNNMkIZ+hNuXCji1t9x59+YtrPQ1gOBwzn3kHtPkAiujet+O4W19Y3NreJ2aWd3b/+gfHjU1HGqGDZYLGLVDqhGwSU2DDcC24lCGgUCW8Hobua3nlBpHstHM0nQj+hA8pAzaqzUxl5ms+G0V664VXcOskq8nFQgR71X/ur2Y5ZGKA0TVOuO5ybGz6gynAmclrqpxoSyER1gx1JJI9R+Nt93Ss6s0idhrOyThszV34mMRlpPosBORtQM9bI3E//zOqkJb/yMyyQ1KNniozAVxMRkdjzpc4XMiIkllCludyVsSBVlxlZUsiV4yyevkuZF1buqug+XldptXkcRTuAUzsGDa6jBPdShAQwEPMMrvDlj58V5dz4WowUnzxzDHzifP2agkDQ=</latexit>

GRU Core
ea <latexitsha1_base64="p9voPEUzx+UfIkRQh8gxuYr1NZU=">AAAB83icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0WPRi8cK9gOaUDabTbt0sxt2N0IJ+RtePCji1T/jzX/jts1BWx8MPN6bYWZemHKmjet+O5W19Y3Nrep2bWd3b/+gfnjU1TJThHaI5FL1Q6wpZ4J2DDOc9lNFcRJy2gsndzO/90SVZlI8mmlKgwSPBIsZwcZKvh9KHuV0mOOiGNYbbtOdA60SryQNKNEe1r/8SJIsocIQjrUeeG5qghwrwwinRc3PNE0xmeARHVgqcEJ1kM9vLtCZVSIUS2VLGDRXf0/kONF6moS2M8FmrJe9mfifN8hMfBPkTKSZoYIsFsUZR0aiWQAoYooSw6eWYKKYvRWRMVaYGBtTzYbgLb+8SroXTe+q6T5cNlq3ZRxVOIFTOAcPrqEF99CGDhBI4Rle4c3JnBfn3flYtFaccuYY/sD5/AGShpII</latexit>

Action Mask CCrorossss
em <latexitsha1_base64="/w8L7oGqGYXoLaimjpGsZ83/9cI=">AAAB83icbVDLSsNAFL2pr1pfVZdugkVwVRJRdFl047KCfUATymQyaYfOI8xMhBLyG25cKOLWn3Hn3zhts9DWAxcO59zLvfdEKaPaeN63U1lb39jcqm7Xdnb39g/qh0ddLTOFSQdLJlU/QpowKkjHUMNIP1UE8YiRXjS5m/m9J6I0leLRTFMScjQSNKEYGSsFQSRZnJNhzotiWG94TW8Od5X4JWlAifaw/hXEEmecCIMZ0nrge6kJc6QMxYwUtSDTJEV4gkZkYKlAnOgwn99cuGdWid1EKlvCuHP190SOuNZTHtlOjsxYL3sz8T9vkJnkJsypSDNDBF4sSjLmGunOAnBjqgg2bGoJworaW108RgphY2Oq2RD85ZdXSfei6V81vYfLRuu2jKMKJ3AK5+DDNbTgHtrQAQwpPMMrvDmZ8+K8Ox+L1opTzhzDHzifP6TOkhQ=</latexit>

Adaptive Action Module

MLP

Attention Mean

esel fk <latexitsha1_base64="l0p9Nwigb4He2XNpelTDLnQ5apI=">AAACAXicbVDLSsNAFJ3UV62vqBvBzWARXJVEFF0W3bisYB/QhjCZ3rRDJ5MwMxFqiBt/xY0LRdz6F+78G6dtFtp6YOBwzj3cuSdIOFPacb6t0tLyyupaeb2ysbm1vWPv7rVUnEoKTRrzWHYCooAzAU3NNIdOIoFEAYd2MLqe+O17kIrF4k6PE/AiMhAsZJRoI/n2AfiZSYd+1lNUskQr9gB4lOe+XXVqzhR4kbgFqaICDd/+6vVjmkYgNOVEqa7rJNrLiNSMcsgrvVRBQuiIDKBrqCARKC+bXpDjY6P0cRhL84TGU/V3IiORUuMoMJMR0UM1703E/7xuqsNLL2MiSTUIOlsUphzrGE/qwH0mgWo+NoSY+81fMR0SSag2pVVMCe78yYukdVpzz2vO7Vm1flXUUUaH6AidIBddoDq6QQ3URBQ9omf0it6sJ+vFerc+ZqMlq8jsoz+wPn8AhoOXkw==</latexit>
MLP

Transformer

e<latexitsha1_base64="muQ0Wnd/pqKQ4oJT4yd9ooQgbxI=">AAAB73icbVBNS8NAEJ3Ur1q/qh69BIvgqSSi6LHoxWMFWwttKJvNpF262Y27G6GE/gkvHhTx6t/x5r9x2+agrQ8GHu/NMDMvTDnTxvO+ndLK6tr6RnmzsrW9s7tX3T9oa5kpii0quVSdkGjkTGDLMMOxkyokScjxIRzdTP2HJ1SaSXFvxikGCRkIFjNKjJU6vVDyKMdJv1rz6t4M7jLxC1KDAs1+9asXSZolKAzlROuu76UmyIkyjHKcVHqZxpTQERlg11JBEtRBPrt34p5YJXJjqWwJ487U3xM5SbQeJ6HtTIgZ6kVvKv7ndTMTXwU5E2lmUND5ojjjrpHu9Hk3Ygqp4WNLCFXM3urSIVGEGhtRxYbgL768TNpndf+i7t2d1xrXRRxlOIJjOAUfLqEBt9CEFlDg8Ayv8OY8Oi/Ou/Mxby05xcwh/IHz+QNUBZAo</latexit>

Property Group Module

o s <latexitsha1_base64="tvUH8j4o3O+IO8FqQYjxYvftN3I=">AAAB6nicbVC7SgNBFL0bXzG+VgUbm8EgWIVdC7UMsbFM0DwgWcLsZDYZMjuzzMwKYckn2FgoYmvrX/gFdjZ+i5NHoYkHLhzOuZd77wkTzrTxvC8nt7K6tr6R3yxsbe/s7rn7Bw0tU0VonUguVSvEmnImaN0ww2krURTHIafNcHg98Zv3VGkmxZ0ZJTSIcV+wiBFsrHQru7rrFr2SNwVaJv6cFMtHtW/2Xvmodt3PTk+SNKbCEI61bvteYoIMK8MIp+NCJ9U0wWSI+7RtqcAx1UE2PXWMTq3SQ5FUtoRBU/X3RIZjrUdxaDtjbAZ60ZuI/3nt1ERXQcZEkhoqyGxRlHJkJJr8jXpMUWL4yBJMFLO3IjLAChNj0ynYEPzFl5dJ47zkX5S8mk2jAjPk4RhO4Ax8uIQy3EAV6kCgDw/wBM8Odx6dF+d11ppz5jOH8AfO2w9CUJF7</latexit>

<latexit sha1_base64="yR6MwevylPHw9BULd0YylUkWg8g=">AAACVHicdVHPT9swGHXCGKxsrGNHLhbVJA5TlCA0OFbjsmOR1hapiSLH/dJadezI/oJUovyRcEDiL9mFA+6PAyv0SZae3nufPvs5K6WwGIZPnr/zYffj3v6n1sHnL4df29+OBlZXhkOfa6nNTcYsSKGgjwIl3JQGWJFJGGazq4U/vAVjhVZ/cV5CUrCJErngDJ2UtmexhBxHcabluNZpzdM6ttyIEq24Axo1TfOTbnPPlm4QBNsjykVobMRkikna7oRBuAR9S6I16ZA1emn7IR5rXhWgkEtm7SgKS0xqZlBwCU0rriyUjM/YBEaOKlaATeplKQ394ZQxzbVxRyFdqq8nalZYOy8ylywYTu2mtxDf80YV5pdJLVRZISi+WpRXkqKmi4bpWBjgKOeOMFeEuyvlU2YYR/cPLVdCtPnkt2RwFkS/gvD6vNP9va5jnxyTE3JKInJBuuQP6ZE+4eSe/POI53mP3rO/4++uor63nvlO/oN/+ALjrbU+</latexit>
[oc1

,

oc2 ,

...,

ocn ]

Q t o t <latexitsha1_base64="i3TTeYQsgT8Sb073NFZwqQqI03c=">AAAB73icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0WPRi8cW7Ae0oWy223bpZhN3J0IJ/RNePCji1b/jzX/jps1BWx8MPN6bYWZeEEth0HW/ncLa+sbmVnG7tLO7t39QPjxqmSjRjDdZJCPdCajhUijeRIGSd2LNaRhI3g4md5nffuLaiEg94DTmfkhHSgwFo2ilTqOfYoSzUr9ccavuHGSVeDmpQI56v/zVG0QsCblCJqkxXc+N0U+pRsEkn5V6ieExZRM64l1LFQ258dP5vTNyZpUBGUbalkIyV39PpDQ0ZhoGtjOkODbLXib+53UTHN74qVBxglyxxaJhIglGJHueDITmDOXUEsq0sLcSNqaaMrQRZSF4yy+vktZF1buquo3LSu02j6MIJ3AK5+DBNdTgHurQBAYSnuEV3pxH58V5dz4WrQUnnzmGP3A+fwDZWo/X</latexit>

LA-QTransformer

s t <latexitsha1_base64="xyFsL8iq1E+ORQvuVaV1DLqzbN8=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0WPRi8cK9gPaUDbbTbt0dxN2J0IJ/QtePCji1T/kzX9j0uagrQ8GHu/NMDMviKWw6LrfTmltfWNzq7xd2dnd2z+oHh61bZQYxlsskpHpBtRyKTRvoUDJu7HhVAWSd4LJXe53nrixItKPOI25r+hIi1AwirlkB1gZVGtu3Z2DrBKvIDUo0BxUv/rDiCWKa2SSWtvz3Bj9lBoUTPJZpZ9YHlM2oSPey6imils/nd86I2eZMiRhZLLSSObq74mUKmunKsg6FcWxXfZy8T+vl2B446dCxwlyzRaLwkQSjEj+OBkKwxnKaUYoMyK7lbAxNZRhFk8egrf88ippX9S9q7r7cFlr3BZxlOEETuEcPLiGBtxDE1rAYAzP8ApvjnJenHfnY9FacoqZY/gD5/MHoEaN9g==</latexit>

<latexit sha1_base64="DUPQgZTAmatnPtYSsXhH93FNxbs=">AAAB9HicbVBNSwMxEJ2tX7V+VT16CRahgpRdUfRY9OKxBfsB7bJk02wbmk3WJFsopb/DiwdFvPpjvPlvTNs9aOuDgcd7M8zMCxPOtHHdbye3tr6xuZXfLuzs7u0fFA+PmlqmitAGkVyqdog15UzQhmGG03aiKI5DTlvh8H7mt0ZUaSbFoxkn1I9xX7CIEWys5NcDrywDcYFwIM6DYsmtuHOgVeJlpAQZakHxq9uTJI2pMIRjrTuemxh/gpVhhNNpoZtqmmAyxH3asVTgmGp/Mj96is6s0kORVLaEQXP198QEx1qP49B2xtgM9LI3E//zOqmJbv0JE0lqqCCLRVHKkZFolgDqMUWJ4WNLMFHM3orIACtMjM2pYEPwll9eJc3LinddcetXpepdFkceTuAUyuDBDVThAWrQAAJP8Ayv8OaMnBfn3flYtOacbOYY/sD5/AHwa5Do</latexit>
Q1

(on

,

an)

PIT

<latexit sha1_base64="v+ht+4P+ZOzgGyBXxUmDKFSiPiQ=">AAAB9HicbVBNSwMxEJ2tX7V+VT16CRahgpRdUfRY9OKxBfsB7bJk02wbmk3WJFsopb/DiwdFvPpjvPlvTNs9aOuDgcd7M8zMCxPOtHHdbye3tr6xuZXfLuzs7u0fFA+PmlqmitAGkVyqdog15UzQhmGG03aiKI5DTlvh8H7mt0ZUaSbFoxkn1I9xX7CIEWys5NcDUZaBuEA4EOdBseRW3DnQKvEyUoIMtaD41e1JksZUGMKx1h3PTYw/wcowwum00E01TTAZ4j7tWCpwTLU/mR89RWdW6aFIKlvCoLn6e2KCY63HcWg7Y2wGetmbif95ndREt/6EiSQ1VJDFoijlyEg0SwD1mKLE8LElmChmb0VkgBUmxuZUsCF4yy+vkuZlxbuuuPWrUvUuiyMPJ3AKZfDgBqrwADVoAIEneIZXeHNGzovz7nwsWnNONnMMf+B8/gBPUJEl</latexit>
Qn

(on

,

an)

PIT

Q t o t <latexitsha1_base64="i3TTeYQsgT8Sb073NFZwqQqI03c=">AAAB73icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0WPRi8cW7Ae0oWy223bpZhN3J0IJ/RNePCji1b/jzX/jps1BWx8MPN6bYWZeEEth0HW/ncLa+sbmVnG7tLO7t39QPjxqmSjRjDdZJCPdCajhUijeRIGSd2LNaRhI3g4md5nffuLaiEg94DTmfkhHSgwFo2ilTqOfYoSzUr9ccavuHGSVeDmpQI56v/zVG0QsCblCJqkxXc+N0U+pRsEkn5V6ieExZRM64l1LFQ258dP5vTNyZpUBGUbalkIyV39PpDQ0ZhoGtjOkODbLXib+53UTHN74qVBxglyxxaJhIglGJHueDITmDOXUEsq0sLcSNqaaMrQRZSF4yy+vktZF1buquo3LSu02j6MIJ3AK5+DBNdTgHurQBAYSnuEV3pxH58V5dz4WrQUnnzmGP3A+fwDZWo/X</latexit>

q 1 <latexitsha1_base64="z+kdAAe1Sd1YziU8B0wrVnnrf3M=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0WPRi8eK9gPaUDbbSbt0s4m7G6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSATXxnW/ncLK6tr6RnGztLW9s7tX3j9o6jhVDBssFrFqB1Sj4BIbhhuB7UQhjQKBrWB0M/VbT6g0j+WDGSfoR3QgecgZNVa6f+x5vXLFrbozkGXi5aQCOeq98le3H7M0QmmYoFp3PDcxfkaV4UzgpNRNNSaUjegAO5ZKGqH2s9mpE3JilT4JY2VLGjJTf09kNNJ6HAW2M6JmqBe9qfif10lNeOVnXCapQcnmi8JUEBOT6d+kzxUyI8aWUKa4vZWwIVWUGZtOyYbgLb68TJpnVe+i6t6dV2rXeRxFOIJjOAUPLqEGt1CHBjAYwDO8wpsjnBfn3fmYtxacfOYQ/sD5/AECWI2d</latexit>
· · ·
q n <latexitsha1_base64="YMxKTMSiVtb5sJz/w4MPSqXFUJE=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0WPRi8eK9gPaUDbbSbt0s4m7G6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSATXxnW/ncLK6tr6RnGztLW9s7tX3j9o6jhVDBssFrFqB1Sj4BIbhhuB7UQhjQKBrWB0M/VbT6g0j+WDGSfoR3QgecgZNVa6f+zJXrniVt0ZyDLxclKBHPVe+avbj1kaoTRMUK07npsYP6PKcCZwUuqmGhPKRnSAHUsljVD72ezUCTmxSp+EsbIlDZmpvycyGmk9jgLbGVEz1IveVPzP66QmvPIzLpPUoGTzRWEqiInJ9G/S5wqZEWNLKFPc3krYkCrKjE2nZEPwFl9eJs2zqndRde/OK7XrPI4iHMExnIIHl1CDW6hDAxgM4Ble4c0Rzovz7nzMWwtOPnMIf+B8/gBezI3a</latexit>

M

M

L

L

P

P

M

M

L

L

P

P

Attention Weight Multi-head Attention cfinal <latexitsha1_base64="3A6IkmBjYF0j0YsCaQM1kJUKNOo=">AAAB+XicbVBNS8NAEJ3Ur1q/oh69LBbBU0lE1GPRi8cK9gPaEDabTbt0sxt2N4US+k+8eFDEq//Em//GbZuDVh8MPN6bYWZelHGmjed9OZW19Y3Nrep2bWd3b//APTzqaJkrQttEcql6EdaUM0HbhhlOe5miOI047Ubju7nfnVClmRSPZprRIMVDwRJGsLFS6LqDSPK4IGGRMIH5bBa6da/hLYD+Er8kdSjRCt3PQSxJnlJhCMda930vM0GBlWGE01ltkGuaYTLGQ9q3VOCU6qBYXD5DZ1aJUSKVLWHQQv05UeBU62ka2c4Um5Fe9ebif14/N8lNUDCR5YYKslyU5BwZieYxoJgpSgyfWoKJYvZWREZYYWJsWDUbgr/68l/SuWj4Vw3v4bLevC3jqMIJnMI5+HANTbiHFrSBwASe4AVencJ5dt6c92VrxSlnjuEXnI9vNiaUCQ==</latexit> LA-Transformer

Mean & Std

Encoder

<latexit sha1_base64="M7Ny/GbFWLt2AWMcH3HlBX63bg0=">AAACVnicdVFNS8QwFEy7fqzrV9Wjl+AieJDSiqhH0YtHBVeFbSlp9nU3mKYleRXW0j+pF/0pXsTsugc/BwLDzDxeMklLKQwGwavjtubmFxbbS53lldW1dW9j88YUlebQ44Us9F3KDEihoIcCJdyVGlieSrhN788n/u0DaCMKdY3jEuKcDZXIBGdopcTLIwkZ9qO0kIPaJDVP6shwLUo04hFo2DTNPv3PPZi6vu//H1E2QiMthiOMaSfxuoEfTEF/k3BGumSGy8R7igYFr3JQyCUzph8GJcY10yi4hKYTVQZKxu/ZEPqWKpaDietpLQ3dtcqAZoW2RyGdql8napYbM85Tm8wZjsxPbyL+5fUrzE7iWqiyQlD8c1FWSYoFnXRMB0IDRzm2hNkq7F0pHzHNONqfmJQQ/nzyb3Jz4IdHfnB12D09m9XRJttkh+yRkByTU3JBLkmPcPJM3hzXaTkvzrs77y5+Rl1nNrNFvsH1PgDXHrWI</latexit>
[sc1

,

sc2 ,

...,

scn ]

Figure 2: Left: The structure of PIT. Firstly, The observation separated explicitly is flowed into the property group module to generate the different class embedding features. The GRU core is utilized to merge all embedding features. Finally, the adaptive action module utilizes the GRU embedding feature and the property group module embedding features to generate dynamic agent actions. Middle: The whole structure of our methods. Right: The structure of LA-QTransformer. LA-QTransformer first separates the state features into different class entities' features and utilizes the encoder layers to encode the features into the same dimension. Then the LA-Transformer module is used to generate multi-level coordination patterns and merges(selects) the appropriate coordination patterns. The multi-head attention module then integrates the coordination patterns and generates agent credit values.

3.1 Level-Adaptive QTransformer
3.1.1 Level-Adaptive Transformer
The Transformer module can be applied to generate the pairwise coordination patterns via capturing the relationship of input elements. However, only considering the pairwise coordination patterns can

Figure 3: Two implementations of LA-Transformer. 4

not achieve general coordination transfer. To generate coordination patterns on multiple levels, a native method is stacking Transformer modules. However, this is unrealistic in large-scale multiagent scenarios due to its massive memory consumption. Besides, stacking modules blurs different level relationships and makes it impossible to determine which level works. In this section, we propose an efficient module named level-adaptive Transformer (LA-Transformer) to generate multilevel coordination patterns and capture the most suitable coordination pattern. We implement LA-Transformer with two methods, including hard attention and the hybrid method, as shown in Figure 3.

In the traditional Transformer, the pairwise coordination patterns are generated via:

Q, K, V = MLPQ,K,V (s),

(5)

c = Softmax

QKT 

V,

(6)

dk

where c is the coordination pattern.

QKT dk

can be approximated as the coordination relationship.

Com-

pared with traditional stacking operations, we fix k and v to realize linearly increasing coordination

level and prevent feature blurring, respectively. The specific coordination level is calculated via:

ci = Softmax

ci-1KT dk

V,

(7)

where ci-1 is the coordination pattern of previous level.

Furthermore, we analyze that the upper bound of the coordination pattern level does not need to be huge. The relationship mapping of the adjacent levels tends to be stationary as the level increases according to the Brouwer Fixpoint Theorem [13]. With the coordination level increasing, there exist two coordination patterns with adjacent level ci, cj that their difference tends to zero:

ci - cj 2  , i, j, i  j  0,

(8)

Due to the difference among agents, we propose two methods to generate different levels of coordination patterns for different agents.

LA-Transformer (hard). LA-Transformer (hard) utilizes the hard attention mechanism to select the most appropriate coordination patterns. To achieve the hard attention-based LA-Transformer, we first utilize the MLP function to encode the initial embedding features to get the key of the hard attention, i.e., ke, and then take the Gumbel softmax function to generate the mask on different levels.

ke = MLPK (s),

(9)

mask = gumbel_softmax(ke[c1, ...ci])

(10)

Then, we take the mask to select the agent-specific coordination pattern.

chard = mask × [c1, ..., ci]

(11)

The advantage of utilizing the mask to select is that it can explicitly provide the value of the level.

LA-Transformer (hybrid). Although the LA-Transformer (hard) can explicitly select a coordination pattern, it inevitably ignores some essential details. So we propose the hybrid level-adaptive Transformer, which generates the level-adaptive coordination pattern via adaptively fusing coordination patterns from all levels. Specifically, we take the MLP function to do the fuse operation.

chybrid = MLP([c1, ..., ci])

(12)

Finally, the coordination patterns chard or chybrid flows into a FeedForward Module to enhance the representation ability and generate cfinal.

3.1.2 Level-Adaptive Transformer-based mixing network (LA-QTransformer)

Limited by the dueling structure mixing network (such as QPLEX) large search space, we take

the popular QMIX-like monotonic mixing network as the baseline model. Our framework can be

expressed as Qtot =

m i

wi(s)qi(oi,

i),

where

wi(s)

is

the

non-negative

parameter

realized

by

the

proposed mixing network (LA-QTransformer), and qi(oi, i) is the individual Q value of agent i.

5

The right part of figure 2 shows the structure of LA-QTransformer. LA-QTransformer has two essential modules, the coordination decomposition module and the coordination integration module. In the coordination decomposition module, LA-QTransformer utilizes the LA-Transformer module to generate different coordination patterns for different agents. After that, LA-QTransformer utilizes a multi-head attention module to combine all agents' coordination patterns and generate agents' credits.
Due to the input entity's different classes and dimensions, we first divide the state features into several entities via the entity class and utilize the MLP-based encoder to embed all entities' features into the same dimension. Then we take the preprocessed state features into the LA-Transformer module to generate suitable coordination patterns. After that, we utilize the multi-head attention module to combine all coordination patterns and generate the credit assignment weights. Finally, LA-QTransformer takes the dot product operation to merge agents' Q values and generate the total Q value Qtotal. In addition, a bias term is used to make up for the residual.
3.2 Population invariant agent via Transformer (PIT)
To realize the coordination transfer in more general scenarios, we design the Population Invariant agent via Transformer (PIT), as shown in the left of Figure 2. PIT has three main parts, the Property Group Module, the GRU Core, and the Adaptive Action Module. In the Property Group Module, PIT explicitly groups the input entities via entities' property and generates group embeddings. The GRU Core merges all group embeddings. The Adaptive Action Module makes it adaptive to the dynamic action space from different scenarios.
Property Group Module. We first divide the observation into the agent attribute features oself , and several group feature sets ospecial via the entity's property. For example, in SMAC, the role property, such as ally and enemy, can be used to divide groups. Due to solid relevance in the same groups, we introduce the Transformer module to generate adaptable and general relevant embeddings. To solve the dynamic entity population problem, we unify all entities in the same groups and represent these features on the group level. Inspired by the mean-field method and attention mechanism, we represent the group features with the mean features of all inner group entities em and the most relevant features ei.
GRU Core. The GRU Core utilizes the agent-self features eself and different group representation em,i, ea,i to capture the temporal change of the group feature and merge all of these features.
Adaptive Action Module. Inspired by the Action Semantics Network [28], we classify actions to adapt to dynamic action space. We divide the total action space into self-related actions, such as move and no-op, and interacting actions, such as attack. Considering that the interacting action is highly related to interacting entities, we utilize the embedding features generated from Transformer to construct interacting action. Note that some properties do not include the action attribute, so we add an action mask module to block unrelated actions.
The details of PIT can be seen in Appendix.

4 Experimental results

Test Win % Test Win % Test Win %

UPDeT+QMIX

PIT+QMIX

PIT+LA-QTransformer(hybrid)

PIT+LA-QTransformer(hard)

QMIX

Qatten

(a) 3m

(b) 8m9m

(c) 2s3z

1.0

1.0

1.0

0.8

0.8

0.8

0.6

0.6

0.6

0.4

0.4

0.4

0.2

0.2

0.2

0.0

0.0

0.0

0.00 0.25 0.50 0.75Time1.0S0teps1.25 1.50 1.75 2.010e6

0

1

2 Time3Steps 4

5

61e6

0.0

0.5

1.0

1.5Time2.S0teps 2.5

3.0

3.5

4.01e6

Figure 4: The performance of PIT LA-QTransformer and PIT in SMAC. We compare our methods with several baselines in homogeneous and heterogeneous scenarios.

We evaluate the performance of PIT and LA-QTransformer in SMAC, which is a popular MARL benchmark based on the real-time strategy game StarCraft II. In SMAC, each unit is controlled by

6

an independent agent with local observation. In contrast, the opponent's units are controlled by the built-in rule-based AI. To test the robustness of our methods, all experiments are run with five random seeds and evaluated under seven threads in parallel.

4.1 Baseline performance
Figure 4 shows the performance in small-sized homogeneous, large-sized homogeneous, and mediansize heterogeneous scenarios, respectively. In the simple 3m scenario, both LA-QTransformer(hybrid) and LA-QTransformer(hard) achieve excellent performance. In more challenging scenarios, LAQTransformer(hybrid) outperforms other baseline methods. However, due to missing details of selecting one specific coordination pattern, LA-QTransformer(hard) method shows suboptimal performance.

4.2 Performance in transfer learning setting

Test Win %

UPDeT+QMIX

PIT+QMIX

PIT+LA-QTransformer(hybrid)

(a) 5m(from 3m)
1.0

(b) 7m(from 3m)
1.0

0.8 0.6 0.4 0.2
0.0
1.0

0.2

0T.4ime Steps0.6

0.8

(d) 8m9m(from 5m6m)

0.8

Test Win %

0.6

0.4

0.2

0.0

1.10e6

0.0

1.0

0.2

0T.4ime Steps0.6

0.8

(e) 1s2z(from 2s3z)

0.8

0.8

Test Win %

0.6

0.6

0.4

0.4

0.2

0.2

0.0

0.0

0.00 0.25 0.50 0.75Time1.0S0teps1.25 1.50 1.75 2.010e6

0.0

0.2

0T.4ime Steps0.6

0.8

PIT+LA-QTransformer(hard)

(c) 5m6m(from 8m9m)

0.8

Test Win %

0.6

0.4

0.2

0.0

1.10e6

0.00 0.25 0.50 0.75Time1.0S0teps1.25 1.50 1.75 2.010e6

(f) 2s3z(from 1s2z)
1.0

0.8

Test Win %

0.6

0.4

0.2

0.0

1.10e6

0.0

0.2

0T.4ime Steps0.6

0.8

1.10e6

Test Win %

Figure 5: The experimental results of PIT and UPDeT with mixing modules, LA-QTransformer and QMIX. Agents are well-trained to converge in the original scenarios and then fine-tuned in the latter scenarios.

Figure 5 shows the evaluated performance of PIT with LA-Transformer and QMIX and current state-of-the-arts multi-agent transfer learning method UPDeT. The experimental results present the performance of different modules in the transfer learning setting.
Transfer in different scales. Figure 5(a) shows that both two LA-QTransformer methods can achieve excellent coordination knowledge transfer while QMIX is unstable. In Figure 5(b), we conduct a more extensive transfer test in different scales (more than two times). Due to the enormous scale changes, LA-QTransformer needs to regenerate proper coordination patterns to adapt to the difference of scenarios, which shows little instability at the beginning of the training process. However, LAQTransformer has strong adaptability and can quickly converge to the optimal policy. Besides, the experimental result demonstrates that PIT is superior to UPDeT.
Transfer in different difficulty levels. We evaluate the performance of coordination knowledge transfer in two scenarios with different difficulty levels (8m_vs_9m is simple and 5m_vs_6m is complex). Figure 5(c) and 5(d) show the transfer from simple to complex and from complex to simple respectively. LA-QTransformer shows an advantage in the jumpstart (the initial performance) and the asymptotic performance. Due to no coordination knowledge transfer, QMIX's performance is unstable in transferring from simple to complex.
Transfer in heterogeneous scenarios. As shown in Figure 5(e) and 5(f), we test the performance in two heterogeneous scenarios with different scales. The LA-Transformer(hybrid) outperforms other baselines because the coordination patterns in heterogeneous scenarios are also helpful. In hetero-

7

geneous scenarios, the coordination patterns in different types of agents have obvious differences, making LA-Transformer(hard) more likely to miss valuable information. This results in suboptimal performance. Besides, the empirical results show that UPDeT does not perform well in heterogeneous scenarios.

4.3 Performance with curricular transfer learning

Test Win %

100

PIT+LA-QTransformer(hybrid)

PIT+LA-QTransformer(hard)

PIT+QMIX

80

UpDET+QMIX

Curricular Transfer Training

60

40

20

5m_vs_6m

8m_vs_9m

10m_vs_11m

0

0.0

2.0

4.0

6.0

8.0 8.1 Steps (1e68.)99

10.0 10.1

10.99

12.0

Figure 6: The curricular transfer experiment in SMAC scenarios.

We extend our methods to curricular learning and make the scaling up experiment. Figure 6 shows that agents are firstly trained in the 5m_vs_6m scenario and then transferred to the 8m_vs_9m scenario with 2M training steps, and finally tested in the 10m_vs_11m scenario. According to the results, curricular learning can correct coordination patterns and generate more general coordination patterns. The performance in the 10m_vs_11m scenario shows that LA-QTransformer with PIT achieves excellent performance, even without any further training.

4.4 Interpretation of LA-QTransformer
Transfer ability. Figure 7 (a) shows the temporal credit assignment values of LA-QTransformer, and it verifies the feasibility of coordination transfer. The Figure shows that the second half credit assignment values of the 8m_vs_9m scenario have a remarkable similarity with that in the 5m_vs_6m scenario, demonstrating the coordination of the coordination policies.
Coordination level of LA-QTransformer. In Figure 7 (b), we present the pairwise coordination weight in LA-QTransformer and find that the complex scenario needs higher-level coordination. Because the average pairwise coordination weight of 5m_vs_6m scenario is smaller than that in the 8m_vs_9m scenario. This also indicates that learning the whole coordination policy has difficulty in realizing the coordination knowledge transfer due to policy overfitting.
Initial win rate analysis. Table 1 emphasizes the jumpstart performance of different mixing networks under the PIT. The jumpstart metric can measure the agent performance without training and shows method generalization. Compared with QMIX, LA-QTransformer can learn a more robust coordination policy via coordination decomposition.

8m9m scenario

5m6m scenario

(a) Credit assignment values in two different scenarios

8m9m scenario

5m6m scenario

(b) Weight of pairwise-coordination.

Figure 7: The interpretation of LA-QTransformer.

8

new (origin)
5m (3m) 7m (3m) 8m9m (5m6m) 5m6m (8m9m) 1s2z (2s3z) 2s3z (1s2z)

Table 1: Test win rate of PIT without training.

LA-QTransformer(hybrid) LA-QTransformer(hard)

97.2% 97.2% 95.1% 83.2% 98.1% 90.8%

97.1% 96.9% 97.1% 77.0% 97.1% 47.7%

QMIX
95.3% 95.9% 96.1% 21.7% 91.5% 84.8%

4.5 Ablation study on ML-Transformer
We evaluate the difference between the LA-Transformer and the traditional Transformer stacking method. Figure 8 shows that stacking two Transformer layers does not lead to any performance improvement. In cooperative MARL, stacking Transformer modules blurs different level relationships and has difficulty in capturing the proper coordination level. However, LA-Transformer can explicitly distinguish the differences between different coordination levels and perceive the suitable coordination level. LA-Transformer(hard) can be explained as the combination of the two stacking Transformer modules and select the proper coordination level, while LA-Transformer(hybrid) implicitly generates the coordination level. The initial win rate verifies that.

5 Related work

Credit assignment methods in cooperative multi-agent Q-learning. VDN [23] uses a simple sum operation to generate the global Q value. QMIX [21], as an extension of VDN, introduces the monotonicity hypothesis to satisfy the IGM condition and uses the hyper-network to achieve it [7]. Qatten [31] introduces the multi-head attention mechanism to construct the mixing network to obtain better performance. QPLEX [26] uses the dueling network to avoid direct optimization from monotonicity assumptions. Previous methods always focus on the whole coordination policy and can achieve excellent performance in several cooperative tasks. However, learning on the whole coordination level may lead to the over-fitting coordination policy and is unsuitable for transfer learning tasks.

Test Win %

8m9m(from 5m6m)
100

90

80

70

60

50

PIT+LA-QTransformer(hybrid)

PIT+LA-QTransformer(1-layer)

40

PIT+LA-QTransformer(2-layer)

PIT+LA-QTransformer(hard)

30

0.00 0.25 0.50 0.7T5ime1.0S0tep1.s25 1.50 1.75 2.010e6

Figure 8: Ablation study on the Transformer stacking method.

Multi-agent transfer learning methods. Current MATL has two branches: the auxiliary training technique and the adaptive network structure. Reusing replay buffer and policy distillation are the prevalent auxiliary training methods. [29] improves the efficiency of value-based MATL by reusing the transition data generated in previous scenarios. Inspired by policy distillation [22], Liu et al. [16] proposes to transfer the knowledge learned in a single agent to multiple agents and uses the n-step return to approximate the difference of the local environment dynamics. This can achieve selective migration and avoid the negative transfer.

In adaptive network structure methods, agents can directly reload the previous knowledge via adapting to the dynamic observation and action shape. DyAN [29] uses the graph neural network to decompose the observation into each entity node to deal with the uncertain population of entities. Unlike DyAN, EPC-MADDPG [17] merges varying entity features to fixed-dimensional features with the attention mechanism. UPDeT [11] firstly proposes to use Transformer to handle dynamic features. It separates the observation features into several entity-based features and uses the Transformer module to generate different actions. However, previous methods are all limited in realizing the coordination knowledge transfer.

9

6 Conclusion
In this paper, we propose a novel mixing network for cooperative MARL, called LA-QTransformer, to achieve coordination knowledge transfer. Compared with the agent-level knowledge transfer, coordination transfer has better generalization and scalability. Our network first decomposes the correlations among agents into a series of agent-specific coordination patterns via the level-adaptive Transformer (LA-Transformer) and then integrates the coordination patterns for the purpose of credit assignment. To ensure the coordination knowledge transfer in more varieties of scenarios, we design a novel agent structure named population invariant agent with Transformer. Experiments on the SMAC benchmarks show that LA-QTransformer can achieve excellent coordination policy transfer and outperforms current SOTA baselines.
Through experiments, we notice that curriculum learning can correct the coordination patterns and realize efficient coordination transfer. An interesting question then is how to design a systematic curriculum to achieve more efficient coordination transfer. Moreover, it should be noted that while we only realized coordination knowledge transfer in multi-agent Q-learning, we may also consider extending our approach to policy-based methods, such as MADDPG [18] and MAPPO [34].
References
[1] Akshat Agarwal, Sumit Kumar, and Katia Sycara. Learning transferable cooperative behavior in multi-agent teams. arXiv preprint arXiv:1906.01202, 2019.
[2] Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.
[3] Georgios Boutsioukis, Ioannis Partalas, and Ioannis Vlahavas. Transfer learning in multi-agent reinforcement learning domains. In European Workshop on Reinforcement Learning, pages 249­260. Springer, 2011.
[4] Felipe Leno Da Silva and Anna Helena Reali Costa. A survey on transfer learning for multiagent reinforcement learning systems. Journal of Artificial Intelligence Research, 64:645­703, 2019.
[5] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.
[6] Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. Counterfactual multi-agent policy gradients. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.
[7] David Ha, Andrew Dai, and Quoc V Le. Hypernetworks. arXiv preprint arXiv:1609.09106, 2016.
[8] Tuomas Haarnoja, Sehoon Ha, Aurick Zhou, Jie Tan, George Tucker, and Sergey Levine. Learning to walk via deep reinforcement learning. arXiv preprint arXiv:1812.11103, 2018.
[9] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290, 2018.
[10] Bryan Horling and Victor Lesser. A survey of multi-agent organizational paradigms. Knowledge Engineering Review, 19(4):281­316, 2004.
[11] Siyi Hu, Fengda Zhu, Xiaojun Chang, and Xiaodan Liang. Updet: Universal multi-agent reinforcement learning via policy decoupling with transformers. arXiv preprint arXiv:2101.08001, 2021.
[12] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016.
10

[13] R Bruce Kellogg, Tien-Yien Li, and James Yorke. A constructive proof of the brouwer fixedpoint theorem and computational results. SIAM Journal on Numerical Analysis, 13(4):473­483, 1976.
[14] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451, 2020.
[15] Landon Kraemer and Bikramjit Banerjee. Multi-agent reinforcement learning as a rehearsal for decentralized planning. Neurocomputing, 190:82­94, 2016.
[16] Yong Liu, Yujing Hu, Yang Gao, Yingfeng Chen, and Changjie Fan. Value function transfer for deep multi-agent reinforcement learning based on n-step returns. In IJCAI, pages 457­463, 2019.
[17] Qian Long, Zihan Zhou, Abhibav Gupta, Fei Fang, Yi Wu, and Xiaolong Wang. Evolutionary population curriculum for scaling multi-agent reinforcement learning. arXiv preprint arXiv:2003.10423, 2020.
[18] Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. arXiv preprint arXiv:1706.02275, 2017.
[19] Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson. Maven: Multi-agent variational exploration. arXiv preprint arXiv:1910.07483, 2019.
[20] Frans A Oliehoek, Christopher Amato, et al. A concise introduction to decentralized POMDPs, volume 1. Springer, 2016.
[21] Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. arXiv preprint arXiv:1803.11485, 2018.
[22] Andrei A Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins, James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and Raia Hadsell. Policy distillation. arXiv preprint arXiv:1511.06295, 2015.
[23] Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Valuedecomposition networks for cooperative multi-agent learning. arXiv preprint arXiv:1706.05296, 2017.
[24] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.
[25] Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets, Michelle Yeo, Alireza Makhzani, Heinrich Küttler, John Agapiou, Julian Schrittwieser, et al. Starcraft ii: A new challenge for reinforcement learning. arXiv preprint arXiv:1708.04782, 2017.
[26] Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. Qplex: Duplex dueling multi-agent q-learning. arXiv preprint arXiv:2008.01062, 2020.
[27] Tonghan Wang, Heng Dong, Victor Lesser, and Chongjie Zhang. Roma: Multi-agent reinforcement learning with emergent roles. In Proceedings of the 37th International Conference on Machine Learning, 2020.
[28] Weixun Wang, Tianpei Yang Yong Liu, Jianye Hao, Xiaotian Hao, Yujing Hu, Yingfeng Chen, Changjie Fan, and Yang Gao. Action semantics network: Considering the effects of actions in multiagent systems. arXiv preprint arXiv:1907.11461, 2019.
[29] Weixun Wang, Tianpei Yang, Yong Liu, Jianye Hao, Xiaotian Hao, Yujing Hu, Yingfeng Chen, Changjie Fan, and Yang Gao. From few to more: Large-scale dynamic multiagent curriculum learning. In AAAI, pages 7293­7300, 2020.
11

[30] Yihan Wang, Beining Han, Tonghan Wang, Heng Dong, and Chongjie Zhang. Off-policy multi-agent decomposed policy gradients. arXiv preprint arXiv:2007.12322, 2020.
[31] Yaodong Yang, Jianye Hao, Ben Liao, Kun Shao, Guangyong Chen, Wulong Liu, and Hongyao Tang. Qatten: A general framework for cooperative multiagent reinforcement learning. arXiv preprint arXiv:2002.03939, 2020.
[32] Yaodong Yang, Ying Wen, Jun Wang, Liheng Chen, Kun Shao, David Mguni, and Weinan Zhang. Multi-agent determinantal q-learning. In International Conference on Machine Learning, pages 10757­10766. PMLR, 2020.
[33] Deheng Ye, Zhao Liu, Mingfei Sun, Bei Shi, Peilin Zhao, Hao Wu, Hongsheng Yu, Shaojie Yang, Xipeng Wu, Qingwei Guo, et al. Mastering complex control in moba games with deep reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 6672­6679, 2020.
[34] Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising effectiveness of mappo in cooperative, multi-agent games. arXiv preprint arXiv:2103.01955, 2021.
[35] Xiangyu Zhao, Liang Zhang, Long Xia, Zhuoye Ding, Dawei Yin, and Jiliang Tang. Deep reinforcement learning for list-wise recommendations. arXiv preprint arXiv:1801.00209, 2017.
12

