
# Invariant Policy Learning: A Causal Perspective

[arXiv](https://arxiv.org/abs/2106.0808), [PDF](https://arxiv.org/pdf/2106.0808.pdf)

## Authors

- Sorawit Saengkyongam
- Nikolaj Thams
- Jonas Peters
- Niklas Pfister

## Abstract

In the past decade, contextual bandit and reinforcement learning algorithms have been successfully used in various interactive learning systems such as online advertising, recommender systems, and dynamic pricing. However, they have yet to be widely adopted in high-stakes application domains, such as healthcare. One reason may be that existing approaches assume that the underlying mechanisms are static in the sense that they do not change over time or over different environments. In many real world systems, however, the mechanisms are subject to shifts across environments which may invalidate the static environment assumption. In this paper, we tackle the problem of environmental shifts under the framework of offline contextual bandits. We view the environmental shift problem through the lens of causality and propose multi-environment contextual bandits that allow for changes in the underlying mechanisms. We adopt the concept of invariance from the causality literature and introduce the notion of policy invariance. We argue that policy invariance is only relevant if unobserved confounders are present and show that, in that case, an optimal invariant policy is guaranteed, under certain assumptions, to generalize across environments. Our results do not only provide a solution to the environmental shift problem but also establish concrete connections among causality, invariance and contextual bandits.

## Comments



## Source Code

Official Code



Community Code



## Bibtex

```tex
@misc{saengkyongam2021invariant,
      title={Invariant Policy Learning: A Causal Perspective}, 
      author={Sorawit Saengkyongam and Nikolaj Thams and Jonas Peters and Niklas Pfister},
      year={2021},
      eprint={2106.00808},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```

## Notes

Type your reading notes here...

