arXiv:2106.01770v1 [cs.LG] 3 Jun 2021

A Normative Model of Classifier Fusion
Susanne Trick Centre for Cognitive Science & Institute of Psychology
TU Darmstadt Darmstadt, Germany susanne.trick@cogsci.tu-darmstadt.de
Constantin A. Rothkopf Centre for Cognitive Science & Institute of Psychology
TU Darmstadt Darmstadt, Germany Frankfurt Institute for Advanced Studies Goethe University Frankfurt Frankfurt, Germany constantin.rothkopf@cogsci.tu-darmstadt.de
Abstract
Combining the outputs of multiple classifiers or experts into a single probabilistic classification is a fundamental task in machine learning with broad applications from classifier fusion to expert opinion pooling. Here we present a hierarchical Bayesian model of probabilistic classifier fusion based on a new correlated Dirichlet distribution. This distribution explicitly models positive correlations between marginally Dirichlet-distributed random vectors thereby allowing normative modeling of correlations between base classifiers or experts. The proposed model naturally accommodates the classic Independent Opinion Pool and other independent fusion algorithms as special cases. It is evaluated by uncertainty reduction and correctness of fusion on synthetic and real-world data sets. We show that a change in performance of the fused classifier due to uncertainty reduction can be Bayes optimal even for highly correlated base classifiers.
1 Introduction
Classification is one of the fundamental tasks in machine learning with broad applicability in many domains. The most successful classification methods, e.g. in machine learning competitions, have proven to be classifier ensembles, which combine different classifiers to improve classification performance [19, 5, 25, 28]. Apart from the selection and training of individual classifiers, the fusion method used for classifier combination is of particular importance for the success of an ensemble, as individual classifiers can be biased or highly variable. Such fusion methods can equivalently be applied for fusing human experts' opinions. However, for convenience, most common fusion methods assume independent classifiers [32, 25], although in practice, classifiers trained on the same target as well as human experts are highly correlated [15].
Different strategies for coping with correlated classifiers have been proposed, such as selecting only those classifiers with the lowest correlation [27, 30, 11, 8, 34], explicitly decorrelating the classifiers before fusion [41], or weighting the classifiers according to their correlation [35, 37, 21, 31]. While there are several non-Bayesian models of improved fusion of correlated classifiers [6, 17, 2, 42, 36, 24], Kim and Ghahramani [18] introduced a Bayesian model for fusing dependent discrete classifier outputs, albeit not probabilistic outputs, thereby disregarding potentially valuable information about
Preprint. Under review.

the uncertainty of decisions. Pirs and Strumbelj [28] extend the work of Kim and Ghahramani [18] by allowing probabilistic classifier outputs. But, their focus is on outperforming related fusion algorithms using an approximate model of dependent classifiers rather than developing a theoretically justified normative model of how correlated classifier fusion should work. In particular, Pirs and Strumbelj [28] conclude that a fusion method should not outperform the base classifiers if these are highly correlated. However, while it is known that there should be no fusion gain for a correlation of r = 1 between classifiers [6, 39, 20, 27, 2, 45], this has not been shown normatively for probabilistic classifiers. Here we clarify how the correlation between classifiers affects uncertainty reduction through fusion in general, which is well known in the case of fusing independent probabilistic classifier outputs [7].
Therefore, in order to show how correlated probabilistic classifier outputs should be fused Bayes optimally, in this work we introduce a hierarchical fully Bayesian normative model of the fusion of correlated probabilistic classifiers. We model the classifiers to be fused with a new correlated Dirichlet distribution, which is able to model Dirichlet-distributed random vectors with positive correlation. We show that previously proposed fusion methods such as Independent Opinion Pool are special cases of this model. Evaluations on simulated as well as real data reveal that fusion should reduce uncertainty the less, the higher the classifiers are correlated. In particular, if the classifiers' correlation is 1, there should be no uncertainty reduction through fusion. Still, since we learn a model of each base classifier, this does not necessarily mean that the fused distribution equals the base distributions. Empirical evaluations show the approach's superiority on real-world fusion problems.
2 Related work
Bayesian models of classifier fusion are known as Supra-Bayesian fusion approaches [15]. For combining expert opinions, they have already been proposed before machine learning methods emerged. Considering the opinions as data, a probability distribution is learned over them, conditional on the true outcome. From this expert model, a decision maker can compute the likelihood of observed opinions and combine it with its prior using Bayes' rule. The resulting posterior distribution over the possible classes is the fusion result [10]. For instance, Lindley [23] modeled experts' opinions conditionally on an event using a multivariate normal distribution on the log-odds, which enabled explicit modeling of their correlations. Similarly, French [9] and Winkler [43] also updated a decision maker's belief given some expert opinions by jointly modeling them with a multivariate normal distribution, while Jouini and Clemen [16] used copulas to model experts' correlations.
Such Supra-Bayesian approaches have also been proposed for classifier fusion. Kim and Ghahramani [18] model independent discrete classifier outputs by learning a multinomial distribution over each row of the classifiers' confusion matrices, conditioned on the true class label. This Independent Bayesian Classifier Combination Model (IBCC) is additionally extended to a Dependent Bayesian Classifier Combination Model (DBCC), which uses Markov networks to model correlations. Inference is realized with Gibbs Sampling, and training is unsupervised. Several authors have extended the work of [18]. However, most of them extend the IBCC method, which assumes independent classifiers. For example, Simpson et al. [33] infer the IBCC parameters with variational inference instead of Gibbs Sampling. Hamed and Akbari [12] instead presented a supervised extension of IBCC. Ueda et al. [40] additionally introduce another latent variable into the original IBCC model that determines a classifier's effectiveness, i.e. whether it always outputs the same label for a class or varies considerably. Still, as in [18], this line of work considers discrete classifier outputs without utilizing classifiers' uncertainties for fusion. Thus, Nazabal et al. [26] introduced a Bayesian model for fusing probabilistic classifiers that output categorical distributions instead of only discrete class labels. The output distributions of each classifier are modeled with a Dirichlet distribution conditioned on the true class label. Parameter inference is realized with Gibbs Sampling on labeled training data. However, similar to the approaches above, the model assumes independent base classifiers and disregards potential correlations.
In contrast, Pirs and Strumbelj [28] explicitly model correlations between probabilistic classifiers. They transform the classifiers' categorical output distributions with the inverse additive logistic transform and model the resulting real-valued vectors with mixtures of multivariate normal distributions with means and covariances conditioned on the true class labels. While [28] shows that this model outperforms other Bayesian fusion methods on most data sets, the model does not provide a normative account of how fusion of correlated probabilistic classifiers should work Bayes optimally. In partic-
2

ular, they conclude that a fused classifier cannot outperform the base classifiers if these are highly correlated and provide empirical evidence for this conclusion based on one data set. However, this has not been proven for probabilistic classifiers, where a special focus should be on uncertainty reduction through fusion. To investigate how this uncertainty reduction should be affected by correlation, we propose a normative hierarchical Bayesian generative model of the fusion of correlated probabilistic classifiers. The model's structure resembles the structure presented in [28] up to a newly introduced conjugate prior of the categorical distribution, a correlated Dirichlet distribution for jointly modeling the classifier outputs. In contrast to [28], we do not require any transformation of the classifier outputs or mixture distributions and show through the normative explainability of the parameters that the fused classifier can outperform the base classifiers, even for highly correlated base classifiers.

3 Bayesian models of classifier fusion
Throughout this work, we assume K base classifiers Ck, k = 1, ..., K to be given and fixed. For a given example i, each base classifier Ck receives observation oki . The true class label for example i is ti = 1, ..., J. Based on observation oki , each classifier Ck outputs the respective probability distribution P (ti|oki ), which is a J-dimensional categorical distribution. The goal of the present work is to fuse these given classifier outputs P (ti|oki ) in order to obtain P (ti|o1i , ..., oKi ). Accordingly, in the following we investigate Bayes optimal fusion methods with successively more general assumptions, which can be generalized to the proposed Correlated Fusion Model. In Section 3.1 we start with assuming independent classifiers whose behavior is not known. In Section 3.2 we proceed by modeling each individual classifier's behavior while still assuming independence. The resulting Independent Fusion Model is finally extended to the Correlated Fusion Model in Section 3.3, which explicitly models correlations between classifiers. We show the relationships to previously published fusion models and use all these models in subsequent empirical evaluations.

3.1 Independent Opinion Pool

If we assume that the outputs of all base classifiers are conditionally independent given ti with an uninformed prior, by applying Bayes' rule we can transform the sought P (ti|o1i , ..., oKi ) to:

K

P (ti|o1i , ..., oKi )  P (ti|oki ),

(1)

k=1

which needs to be renormalized to sum to 1. This fusion rule is known as Independent Opinion Pool (IOP) [3, 1]. Its properties are appealing regarding uncertainty reduction through fusion. Nonconflicting base distributions reinforce each other in a way that the fused categorical distribution's uncertainty is reduced [1], and the more uncertain a base distribution, the less it affects the resulting fused distribution [13]. Assuming conditional independence and an uninformed prior over the true class label ti, IOP is therefore not only Bayes optimal but also leads to intuitive results regarding uncertainty reduction.

3.2 Independent Fusion Model
Although IOP is Bayes optimal given conditionally independent base classifiers and an uninformed prior, note that it is an ad-hoc method. Thus, only information given by the current output distributions can be exploited for fusion. The individual classifiers' properties, quantified by their bias, variance, and uncertainty, cannot be considered. To overcome this, the Independent Fusion Model (IFM) additionally models the behavior of the classifiers to be fused, while still assuming conditional independence of classifiers and an uninformed prior over classes. Since modeling each classifier's behavior requires considering their categorical output distributions as data, here we assume them as given and fixed and define them as xki = P (ti|oki ) for each base classifier Ck and example i. By observing multiple training examples of classifier outputs xki , a probability distribution over them conditional on the true class label ti can be learned, P (xki |ti). We set this distribution to be a Dirichlet distribution. Thus, if ti can take J different values, each base classifier's outputs are modeled by J different Dirichlet distributions, P (xki |ti = 1), ..., P (xki |ti = J). The graphical model for the proposed IFM is shown in Figure 1(a). The true label ti of example i is modeled with

3

a categorical distribution with parameter p. If sufficient knowledge about the data is available, the

prior p over true labels ti can be chosen accordingly. For the subsequent experiments we chose an

uninformed

prior

with

p

=

(

1 J

,

...,

1 J

).



holds

the

parameters

of

the

Dirichlet

distributions

that

model the classifiers' outputs. kj with jkl > 0 for all l = 1, ..., J thereby contains the parameters of

the Dirichlet distribution over the outputs of classifier Ck if ti = j. Hence, the output xki of classifier

Ck for example i with true label ti = j is Dirichlet-distributed with parameter vector kj .

A similar model was proposed by Nazabal et al. [26] for classifier fusion in human activity recognition. However, their model uses more parameters since they chose the parameters of the Dirichlet distributions modeling the classifiers to be a product of two parameters.

3.2.1 Parameter inference
For learning the classifier model parameters , the posterior distribution over  conditioned on observed classifier outputs x and the corresponding true labels t, P (|x, t), needs to be inferred. The training data x consist of I examples composed of K categorical output distributions xki , and t holds I true labels ti respectively. Inference is performed with Gibbs Sampling. As an uninformed prior for all elements of kj we chose a vague gamma prior with both shape and scale set to 10-3. Of course, one could choose any other prior given additional domain knowledge about the data. In the following, we take the expectations of inferred posterior distributions as point estimates for kj .

3.2.2 Normative fusion behavior

For fusion, the posterior distribution over ti given all K classifier outputs xki and the learned model parameters , P (ti|x1i , ..., xK i , ), needs to be inferred. Since the IFM is a generative model for independent categorical classifier outputs, performing fusion in this way is Bayes optimal given the model assumptions. The posterior fused distribution can be derived analytically:

p(ti

=

j|xi, j )



K k=1

Dirichlet(xki ; kj )



K k=1

1 B(kj )

J l=1

(xki l)kj l-1.

(2)

This unnormalized posterior probability can now be computed for all ti = j for j = 1, ..., J and normalizing these values to make them sum to 1 gives the posterior fused categorical distribution.

As (2) shows, fusion itself is still performed by multiplication as for IOP. However, using the IFM, we do not multiply the categorical output distributions of the base classifiers to be fused but their probabilities conditioned on the modeling Dirichlet distributions in (2). Thus, fusion can take into account potential learned biases. Moreover, also the learned variances and uncertainties of the base classifiers can be considered for fusion.

This can be demonstrated with the following example. If a classifier C1 is modeled by three Dirichlet distributions with parameters 11 = (a + n, a, a) for ti = 1, 12 = (a, a + n, a) for ti = 2, and 13 = (a, a, a + n) for ti = 3, and a classifier C2 is modeled equivalently with 21 = (b + m, b, b), 22 = (b, b + m, b), and 23 = (b, b, b + m), with a, b, n, m > 0, we can simplify (2) to:

p(ti = j|xi, j )  (x1i j )n(x2i j )m

(3)

for j = 1, 2, 3. This case, which was not considered by Nazabal et al. [26], is of particular interest, because if we set parameters n = m = 1, the IFM reduces to IOP. However, increasing n and m
results in lower uncertainty of the fused distribution if non-conflicting base distributions are fused. In addition, if n > m, C1 has a higher impact on the fused result than C2.

How n and m are related to variance and uncertainty of a classifier can be quantified with two

properties of the Dirichlet distribution, its precision and the entropy of its expectation, which is

a categorical distribution. The precision of a Dirichlet distribution with parameter , defined as

J j=1

j ,

is

higher,

the

more

concentrated

the

distribution

is

around

the

Dirichlet's

expectation

[14].

Thus, a Dirichlet distribution with a high precision models a classifier with a low variance. On the

other hand, the entropy of a Dirichlet's expectation quantifies the average uncertainty of the modeled

classifier. If we keep a fixed and increase n, the precision of the corresponding Dirichlet distribution

increases. Also, it can be shown that its expectation uncertainty decreases. Thus, the lower classifier

C1's variance and uncertainty, the higher is its fusion impact and uncertainty reduction through

4

p

kj

ti

J classes

K classifiers

xki

I examples

ti  Categorical(p) xki |ti = j  Dirichlet(kj )
(a) IFM

J classes

j

p

kj

ti

K classifiers

xki I examples

ti  Categorical(p) xki |ti = j  CorrDirichlet(kj , j)
(b) CFM

J classes
jk l Akj il

L=J

xki l

dimensions

K = 2 classifiers

j l
I examples
Dj il
ti p

ti  Categorical(p)

Akj il  Gamma(jkl - j l, 1)

Djil  Gamma(jl, 1)

xki l|ti=j 

Akj il + Dj il

J n=1

Akj in

+

Dj in

(c) CFM with latent variables of correlated Dirichlet

Figure 1: Graphical models of the IFM (a), CFM (b) and a detailed CFM for K = 2 classifiers with all latent variables (c).

fusion. If we instead increase a while keeping n fixed, this again increases precision and reduces C1's variance, but also increases its mean uncertainty. Hence, a classifier with low variance and high uncertainty has the same fusion impact as a classifier with high variance and low uncertainty.
Note that if we set K = 1 in (2), the IFM can also be used as a meta classifier for a single classifier C1. This meta classifier classifies a given example i based on C1's output distribution x1i . Thus, we only learn a Dirichlet model of the individual classifier C1 instead of multiple classifiers. Conditioned on the learned model parameters 1 and the single base classifier's output distribution x1i , then the posterior distribution over all possible class labels, P (ti = j|x1i , 1j ), is computed, which is the meta classifier's result.

3.3 Correlated Fusion Model
The IFM introduced in Section 3.2 enables optimal fusion of categorical output distributions of conditionally independent base classifiers. However, in practice most classifiers trained on the same target are highly correlated [15]. Therefore, we extend the IFM to a Correlated Fusion Model (CFM) to explicitly model the correlations between different classifiers' outputs. As in the IFM, we also model the categorical classifier outputs xki given the true label ti as a probability distribution. However, instead of modeling all classifiers independently with individual Dirichlet distributions, we model the joint distribution P (x1i , ..., xK i |ti) with a new correlated Dirichlet distribution that can express correlations between the classifiers' outputs.

3.3.1 Correlated Dirichlet distribution

For modeling correlated classifiers' categorical output distributions with their conjugate prior, a
distribution is required that can model correlations between marginally Dirichlet-distributed random
variables. While previous generalizations of the Dirichlet distribution focused on more flexible correlations between individual random vector entries x1, ..., xJ of a Dirichlet variate x [4, 44, 22], Trick et al. [38] introduced a correlated Dirichlet distribution that models correlations between two random vectors x1 = (x11, . . . , x1J ) and x2 = (x21, . . . , x2J ) with arbitrary marginal Dirichlet distributions.

A J-dimensional correlated Dirichlet distribution is thereby constructed from 3J independent

gamma variates 1, ..., J2 - J

A11, , 1,

..., ...,

A1J J

, A21, with

...l1,,A2Jl2,,

D1, ..., DJ with l > 0, l1, l2

shape > l,

parameters and equal

11 - scale

1, ..., J1 - J parameter 1.

, 12 x1

- =

(x11, ..., x1J ) and x2 = (x21, ..., x2J ) with:

x1l =

A1l + Dl

J n=1

A1n

+

J n=1

Dn

and x2l

=

J n=1

A2l A2n

+ +

Dl
J n=1

Dn

,

l = 1, ..., J,

(4)

Tarheeimr aprogsiintiavlelycDorirreiclahtlieotn-d, iis.etr.ibpuotesditivweitchorDreirliacthiolents(xb1e;twe11e, n...,x1l J1a)ndanxd2l

Dirichlet(x2; 12, for l = 1, ..., J,

..., is

J2 ). gen-

erated by the shared variables D1, ..., DJ with the correlation parameters 1, ..., J . If l tends to

zero for l = 1, ..., J, x1 and x2 are independent and each follow a standard Dirichlet distribution.

If x1 and x2 have the same marginal distributions with 1 = 2, their correlation tends to 1 if

 tends to 1 = 2. Thus, if x1 and x2 have different marginal distributions, the correlation is

5

limited below 1. While no closed-form solution for the distribution is available, sampling from it is straightforward so that it can be applied to the CFM.

Figure 1(b) shows the CFM's graphical model. The only difference to the IFM's graphical model

in Figure 1(a) is that classifier outputs x1i , ..., xK i are jointly correlated-Dirichlet-distributed with

parameters kj and j if ti = j. As in the IFM, kj with jkl > 0 holds the parameter vector of the marginal Dirichlet distribution of classifier Ck if ti = j. The new parameter j is responsible for the

pairwise correlation between the classifier outputs if ti = j. Its dimensionality is

K 2

× J and it must

hold that jm,l > 0 and jm,l < jkl for m = 1, ...,

K 2

, l = 1, ..., J, k = 1, ..., K. For the reduced

case of K = 2 classifiers, in Figure 1(c), we additionally show a more detailed graphical model of

the CFM including the latent variables required for constructing the correlated Dirichlet distribution.

3.3.2 Parameter inference
We learn the joint classifier model by inferring the posterior distribution over parameters  and  given observed classifier outputs x and their true labels t, P (, |x, t), using Gibbs Sampling. For all elements of kj and j, we chose a vague gamma prior with shape and scale set to 10-3, which however can be set differently according to prior knowledge about the data. We split up inference in two steps. First, we infer the marginal Dirichlet parameters  as described in Section 3.2.1. Subsequently we consider these marginal parameters as observed and in a second step infer the posterior distribution over the correlation parameters P (|x, t, ). This step-wise inference gives the same results as full inference on data generated from the CFM, but was observed to be more robust empirically on real data since it guarantees correctly inferred marginal distributions. As for the IFM, we use the expectation of the inferred posterior distributions as point estimates for kj and j.

3.3.3 Normative fusion behavior
The fusion of K categorical base distributions x1i , ..., xK i is performed by inferring the posterior distribution over the true label ti conditioned on the base distributions xki and the learned model parameters  and , P (ti|x1i , ..., xK i , , ). Different from the IFM, here we cannot derive the fused distribution analytically because we do not have a closed-form solution for the probability density function of the correlated Dirichlet distribution. However, by assuming , , and x1i , ..., xK i to be observed, inference of latent ti can be performed with Gibbs Sampling. From a sufficient number of samples of ti we can infer the categorical distribution over ti, which is the fused result. Alternatively inferring ti with variational methods in order to speed up fusion is left for future work.
Note that if we let all correlation parameters j tend to zero, the CFM reduces to the IFM, and its fusion behavior coincides with the one we derived analytically for the IFM in Section 3.2.2. Thus, bias, variance, and uncertainty of the individual classifiers similarly influence the fusion when fusing with the CFM. Additionally, in contrast to previous fusion algorithms, our model can be used to investigate how uncertainty reduction through fusion should be affected by the correlation of the fused classifiers in a normative way. We examine this in detail with the two examples in the following.

categ. distributions categ. distributions

1.0 0.8 0.6 0.4 0.2 0.0
x1i x2i base

0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 m1i m2i

fused dependent on r

meta

(a)

1.0 0.8 0.6 0.4 0.2 0.0
x1i x2i base

p(ti = 1) p(ti = 2) p(ti = 3)

0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 m1i m2i

fused dependent on r

meta

(b)

Figure 2: Two base distributions x1i =x2i =(0.6, 0.2, 0.2) are fused using the CFM assuming IOP marginal parameters (a) and marginals that imply stronger uncertainty reduction (b). We progressively
increase the assumed correlation between classifiers from 0.0 to 1.0 and show the corresponding fused distributions as well as the results of the meta classifiers m1i and m2i .

6

Specifically, we compare the fusion behavior of the CFM for systematically varied correlations between two base classifiers. We implement inference using JAGS [29]. Figure 2(a) shows an example where the marginal parameters of the correlated Dirichlet distributions are chosen to replicate IOP fusion behavior for zero correlation (n = m = 1 in (3)). The results show that the higher the correlation between the two classifiers, the smaller the uncertainty reduction through fusion. In particular, there is no uncertainty reduction if the correlation is r = 1. In this case, the fused distribution equals the two base distributions. In Figure 2(b) we show the fusion results given different correlation levels for marginal parameters that imply increased uncertainty reduction compared to IOP (n = m = 2 according to (3)) for zero correlation because of lower classifier variance and uncertainty. As can be seen, there is also less uncertainty reduction, the higher the correlation between both classifiers. However, for r = 1, the fused distribution is not identical to the two base distributions; its uncertainty is reduced despite the high correlation. Yet, the reason for this is not fusion but the Dirichlet models we learned for each individual classifier. The resulting fused distribution for r = 1 is similar to the resulting distributions we get if we use the IFM as a meta classifier individually for each base distribution (see Section 3.2.2). Hence, the fusion of two highly correlated classifiers does not additionally reduce the uncertainty. This also applies to the example shown in Figure 2(a). However, in this case, due to the chosen marginal distributions, the meta classifier results are equal to the base distributions. Both examples reveal that the uncertainty reduction through fusion should decrease progressively if the base classifiers' correlation increases. For a correlation of r = 1, fusion should not reduce the uncertainty at all. Still, the fused distribution might be less uncertain than the base distributions since uncertainty cannot only be reduced by fusion but also as a result of modeling each individual classifier's behavior, i.e. bias, variance and uncertainty.
4 Evaluation
In order to show the normative fusion behavior of the CFM, we additionally evaluate our model on simulated and real data sets. The fused distributions returned by the CFM are compared to those of the IFM and IOP and the base distributions. In addition, we compare the fusion performances to the performances of each classifier's meta classifier and the method proposed in [28]. As performance measures, we consider entropy and log-loss for quantifying uncertainty reduction through fusion and correctness of the classifications. All evaluated test sets consist of the output distributions of K = 2 classifiers with a target variable ti with J = 3 possible outcomes.
4.1 Simulated data sets
We created different simulated data sets by generating random samples of output distributions x1i and x2i of two classifiers for different given marginal parameters , correlation parameters  and true class labels ti according to the generative model of the CFM (Figure 1(b)). To show the normative fusion behavior depending on the base classifiers' correlation, for two sets of marginal parameters

entropy log-loss entropy log-loss

0.95

1.2

r = 0.00

r = 0.25

r = 0.50

0.90

1.1

r = 0.75 r = 1.00

0.85

1.0

0.80
r = 0.00

r = 0.25

0.75

r = 0.50

r = 0.75

0.70

r = 1.00

C1 C2 IOP IFM CFM M1 M2 base fusion methods meta

0.9 0.8
C1 C2 IOP IFM CFM M1 M2 base fusion methods meta

(a) 1 = 2 = ((3, 2, 2), (2, 3, 2), (2, 2, 3))

1.0

r = 0.00 r = 0.25

r = 0.50

r = 0.75

0.8

0.8

r = 1.00

0.6

0.6

r = 0.00

r = 0.25

r = 0.50

0.4

0.4

r = 0.75

r = 1.00

C1 C2 IOP IFM CFM M1 M2 base fusion methods meta

C1 C2 IOP IFM CFM M1 M2 base fusion methods meta

(b) 1 = 2 = ((12, 8, 8), (8, 12, 8), (8, 8, 12))

Figure 3: Fusion performances on simulated data in terms of mean entropy and log-loss. We compare the performance of base classifiers C1, C2, the three fusion methods IOP, IFM, and CFM, and the meta classifiers M1, M2. We show the fusion behavior for five levels of correlation between the base classifiers and different marginal model parameters, implying IOP fusion (a) and higher reinforcement
due to decreased classifier variance and uncertainty (b). Standard deviations are shown as error bars.

7

, we chose different correlation parameters  respectively that correspond to the correlations 0.0, 0.25, 0.5, 0.75, 1.0 between the two classifiers' outputs. For all five correlation levels, we generated 25 simulated random test sets on which we evaluate, each consisting of 60 test examples (20 per class) composed of two categorical distributions and their corresponding class label. Since the true parameters of the data were known, no training data were required. We chose the marginal parameters to represent two prototype cases of classifier models in order to demonstrate that the effect of correlation on the fusion behavior also depends on the individual classifiers' marginal Dirichlet models. One of the chosen classifier models leads to IOP fusion for zero correlation, one represents two classifiers with decreased variance and uncertainty.
For the first simulated data set (SIM 1), we determine the marginal parameters  of the CFM in a way that it reduces to IOP if the correlation is set to zero. As can be seen in Figure 3(a), therefore, the results of IOP and the IFM are equal, both regarding entropy and log-loss. The shown entropies reveal that the higher the correlation between the classifiers is, the more uncertainty is reduced by fusing with IOP or the IFM. The higher the correlation between two classifiers is, the more similar are their classifier outputs, and the more they reinforce each other when fused assuming independence. In contrast, when fusing with the CFM, we see less uncertainty reduction through fusion, the higher the correlation between the classifiers is. Particularly, for a correlation of r = 1, there is no uncertainty reduction. The mean entropy is the same as for the two meta classifiers. Also, the mean log-loss of the CFM is equal to that of the meta classifiers if r = 1. Thus, as expected, we see no change in performance through fusion for highly correlated classifiers when using the CFM. Since we chose the marginals according to IOP fusion, the CFM's performance also equals the performances of the base classifiers. In general, the CFM performs best at all correlation levels. Particularly for high correlations, it outperforms the other fusion methods, which assume independence, overestimate uncertainty reduction, and therefore perform even worse than the base classifiers.
The second simulated data set (SIM 2) was generated setting the marginal parameters  of the CFM according to the example in (3) with n = m = 4, which leads to increased uncertainty reduction through fusion in comparison to IOP for independent classifiers since the modeled base classifiers' variance and uncertainty is decreased. Accordingly, Figure 3(b) shows significantly lower mean entropies for the IFM than for IOP for all correlation levels. In contrast, for the CFM, the fused distributions' mean entropy increases with the correlation such as for the first simulated data set. If r = 1, the CFM again shows the same entropy as the two meta classifiers. Hence, the fusion of two highly correlated base classifiers does not reduce the uncertainty. This is confirmed by the log-loss (Figure 3(b)). However, in contrast to the first simulated data set, here, the meta classifiers' performances are increased compared to the base classifiers, and uncertainty is reduced. Therefore, the CFM outperforms the base classifiers also for a correlation of r = 1. Note that, again, the CFM achieves the lowest log-loss and thus the best performance for all correlation levels.
4.2 Real data sets
In addition to simulated data sets, we also evaluated the CFM on 5 real data sets, Bookies A, Bookies B, DNA A, DNA B, DNA C. Both Bookies data sets are composed of two bookmakers' odds for football matches of the English Premier League1 (Bookies A) and the German Bundesliga2 (Bookies B). The target variable has three possible outcomes, and for each match example, the odds were transformed to a 3-dimensional categorical probability distribution by normalizing their reciprocals. Thus, each bookie is considered as a base classifier and each example in the Bookies data sets is composed of two categorical distributions and a true class label. The correlation between the bookmakers' predictions is approximately 1 in both data sets; it ranges from 0.955 to 0.996 in different dimensions and for different values of ti.
The DNA data set from the StatLog project3 with a target variable with J = 3 possible outcomes was used to construct three more data sets for evaluating the CFM. For each, we trained two different classifiers on this data set. Their categorical output distributions on the corresponding test data set form the respective data set DNA A, DNA B, DNA C. For DNA A, we trained two highly correlated classifiers by using the same classification method (kNN) and the same training data but different hyperparameters (k = 120 and k = 150). The correlation between both base classifiers is
1https://www.football-data.co.uk/englandm.php 2https://www.football-data.co.uk/germanym.php 3https://archive.ics.uci.edu/ml/datasets/Molecular+Biology+(Splice-junction+Gene+Sequences)
8

entropy log-loss

1.0

0.8

0.6

0.4

Bookies A

Bookies B

0.2

DNA A DNA B

DNA C

C1 C2 IOP IFM CFM M1 M2 base fusion methods meta

Bookies A

1.5

Bookies B

1.0

DNA A DNA B DNA C

0.5

0.0 C1 C2 IOP IFM CFM M1 M2 base fusion methods meta

Figure 4: Fusion performances on real data in
terms of mean entropy and log-loss. We compare the performance of base classifiers C1, C2, the three fusion methods IOP, IFM, and CFM, and the meta classifiers M1, M2. Standard deviations are shown as error bars.

Table 1: Comparison of log-losses of the CFM and Pirs' method [28] on simulated and real data.

data set SIM 1 r=0.0 SIM 1 r=0.5 SIM 1 r=1.0 SIM 2 r=0.0 SIM 2 r=0.5 SIM 2 r=1.0 Bookies A Bookies B DNA A DNA B DNA C

CFM (µ ± )
0.834 ± 0.067 0.89 ± 0.065 0.944 ± 0.065 0.412 ± 0.085 0.583 ± 0.092 0.672 ± 0.058 1.056 ± 0.067 1.108 ± 0.085 0.169 ± 0.078 0.301 ± 0.067 0.298 ± 0.178

Pirs (µ ± )
0.915 ± 0.03 0.938 ± 0.039 0.96 ± 0.056 0.582 ± 0.048 0.66 ± 0.065 0.717 ± 0.041 1.165 ± 0.035 1.176 ± 0.052 0.177 ± 0.021 0.421 ± 0.043 0.351 ± 0.092

approximately 1; it ranges from 0.962 to 0.986 for different dimensions and values for ti. For DNA B, we trained two classifiers by using the same classification method (kNN, k = 50) but different training data. The correlation between both base classifiers ranges from 0.463 to 0.709 for different dimensions and values for ti. DNA C was created by training two different classifiers, one kNN classifier (k = 50) and one Random Forest classifier, on the same training set. The correlation between the two base classifiers' output distributions ranges from 0.5 to 0.693 in different dimensions and for different values of ti.
We randomly split each real data set into a test and training set, while the test set contains 60 examples (20 per class) and the training set contains all remaining examples. The random splitting was repeated five times with different random seeds, and expectations and standard deviations of the resulting performance measures were computed. On each random training split the model parameters  and  were inferred, which were then used to fuse the distributions in the test set. For all data sets, the performances of base classifiers, different fusion methods, and meta classifiers are shown in Figure 4.
For the three highly correlated data sets, Bookies A, Bookies B, DNA A, the CFM's performance is equal to the performances of the meta classifiers, both regarding entropy and log-loss. Thus, also on real data we confirm that fusion causes no uncertainty reduction and no change in performance if the base classifiers are highly correlated. However, this does not necessarily result in equal performances of the CFM and the base classifiers. Depending on the Dirichlet models learned for the individual classifiers, the CFM can still outperform highly correlated base classifiers, which we see for the DNA A data set. Also, the CFM can perform worse than the base classifiers, e.g. for Bookies B, which is an effect of too similar Dirichlet models for different class labels ti, as also noticed in [28].
For the less correlated data sets, DNA B and DNA C, we see that the CFM reduces less uncertainty than the IFM but is more certain than the meta classifiers. Also, the CFM performs best of all fusion methods and better than base and meta classifiers.
4.3 Comparison to the approach by Pirs and Strumbelj
The model introduced by Pirs and Strumbelj [28], which relies on transforming the classifier outputs and modeling them with a multivariate normal mixture, is the only comparable Bayesian method for fusing correlated probabilistic classifiers. Contrary to [28], on simulated as well as real data we show that although fusion should not reduce the uncertainty if r = 1, in a normative framework fused classifiers can outperform highly correlated base classifiers due to the models learned for the individual classifiers. Moreover, we additionally compared the performances of the CFM and Pirs' model in terms of log-loss. As can be seen in Table 1, the CFM outperforms on all tested data sets. The simulated data sets not displayed in the table showed similar results but are left out for brevity.
5 Conclusion
In this work, we introduced a normative Bayesian model of classifier fusion based on a new correlated Dirichlet distribution. We derived Bayes optimal fusion behavior for probabilistic classifiers that
9

output categorical distributions, which considers the classifiers' bias, variance, uncertainty, and correlation. We showed that uncertainty reduction through fusion should be the lower, the higher the correlation between the classifiers is, resulting in no uncertainty reduction through fusion if r = 1. However, this does not necessarily lead to equal performances of the fused classifier and the base classifiers if a model for each classifier is learned. The proposed normative fusion model offers a new perspective on Bayesian combination of probabilistic classifiers, thereby clarifying how the correlation between classifiers affects uncertainty reduction through fusion and subsuming well known pioneering expert opinion aggregation techniques. Since it additionally outperforms the only comparable model it should be the method of choice if Bayes optimal fusion is the goal. However, as classification could potentially be used in conjunction with data and tasks with negative societal impact, we encourage responsible deployment of the proposed approach.
Acknowledgments and Disclosure of Funding
This work has been funded by the German Federal Ministry of Education and Research (BMBF) (projects Kobo34 [16SV7984] and IKIDA [01IS20045]). Additionally, we acknowledge support by the Hessian Ministry of Higher Education, Research, Science, and the Arts (HMWK) (projects "The Third Wave of AI" and "The Adaptive Mind") and the Hessian research priority program LOEWE within the project "WhiteBox".
References
[1] Tiana Rakotovao Andriamahefa. Integer Occupancy Grids: a probabilistic multi-sensor fusion framework for embedded perception. PhD thesis, 2017.
[2] Brian A Baertlein, Wen-Jiao Liao, and De-Hui Chen. Predicting sensor fusion performance using theoretical models. In Detection and Remediation Technologies for Mines and Minelike Targets VI, volume 4394, pages 1035­1046. International Society for Optics and Photonics, 2001.
[3] James O. Berger. Statistical Decision Theory and Bayesian Analysis. Springer, London, 1985.
[4] Robert J Connor and James E Mosimann. Concepts of independence for proportions with a generalization of the Dirichlet distribution. Journal of the American Statistical Association, 64(325):194­206, 1969.
[5] Thomas G Dietterich. Ensemble methods in machine learning. In International Workshop on Multiple Classifier Systems, pages 1­15. Springer, 2000.
[6] E Drakopoulos and Chung Chieh Lee. Optimum fusion of correlated local decisions. In Proceedings of the 27th IEEE Conference on Decision and Control, pages 2489­2494. IEEE, 1988.
[7] Wilfried Elmenreich. An introduction to sensor fusion. Vienna University of Technology, Austria, 502: 1­28, 2002.
[8] Fabio A Faria, Jefersson A dos Santos, Sudeep Sarkar, Anderson Rocha, and Ricardo da S Torres. Classifier selection based on the correlation of diversity measures: When fewer is more. In 2013 XXVI Conference on Graphics, Patterns and Images, pages 16­23. IEEE, 2013.
[9] Simon French. Updating of belief in the light of someone else's opinion. Journal of the Royal Statistical Society: Series A (General), 143(1):43­48, 1980.
[10] Christian Genest, James V Zidek, et al. Combining probability distributions: A critique and an annotated bibliography. Statistical Science, 1(1):114­135, 1986.
[11] Kai Goebel and Weizhong Yan. Choosing classifiers for decision fusion. In Proceedings of the 7th International Conference on Information Fusion, volume 1, pages 563­568, 2004.
[12] Mohammad Ghasemi Hamed and Ahmad Akbari. Hierarchical Bayesian classifier combination. In International Conference on Machine Learning and Data Mining in Pattern Recognition, pages 113­125. Springer, 2018.
[13] Eric Hayman and Jan-Olof Eklundh. Probabilistic and voting approaches to cue integration for figureground segmentation. In Anders Heyden, Gunnar Sparr, Mads Nielsen, and Peter Johansen, editors, Computer Vision -- ECCV 2002, pages 469­486, Berlin, Heidelberg, 2002. Springer Berlin Heidelberg. ISBN 978-3-540-47977-2.
10

[14] Jonathan Huang. Maximum likelihood estimation of dirichlet distribution parameters. CMU Technique Report, 2005.
[15] Robert A Jacobs. Methods for combining experts' probability assessments. Neural Computation, 7(5): 867­888, 1995.
[16] Mohamed N Jouini and Robert T Clemen. Copula models for aggregating expert opinions. Operations Research, 44(3):444­457, 1996.
[17] Moshe Kam, Qiang Zhu, and W Steven Gray. On distributed detection with correlated local detectors. In 1991 American Control Conference, pages 2174­2175. IEEE, 1991.
[18] Hyun-Chul Kim and Zoubin Ghahramani. Bayesian classifier combination. In Artificial Intelligence and Statistics, pages 619­627, 2012.
[19] Josef Kittler, Mohamad Hatef, Robert PW Duin, and Jiri Matas. On combining classifiers. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(3):226­239, 1998.
[20] Ludmila I Kuncheva and Lakhmi C Jain. Designing classifier fusion systems by genetic algorithms. IEEE Transactions on Evolutionary Computation, 4(4):327­336, 2000.
[21] Alexandre Lacoste, Mario Marchand, François Laviolette, and Hugo Larochelle. Agnostic Bayesian learning of ensembles. In International Conference on Machine Learning, pages 611­619, 2014.
[22] Scott Linderman, Matthew J Johnson, and Ryan P Adams. Dependent multinomial models made easy: Stick-breaking with the polya-gamma augmentation. In Advances in Neural Information Processing Systems, pages 3456­3464, 2015.
[23] Dennis V Lindley. Reconciliation of discrete probability distributions. Bayesian Statistics, 2:375­390, 1985.
[24] Andy Jinhua Ma, Pong C Yuen, and Jian-Huang Lai. Linear dependency modeling for classifier fusion and feature combination. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(5):1135­1148, 2013.
[25] Mohamed Mohandes, Mohamed Deriche, and Salihu O Aliyu. Classifiers combination techniques: A comprehensive review. IEEE Access, 6:19626­19639, 2018.
[26] Alfredo Nazabal, Pablo Garcia-Moreno, Antonio Artes-Rodriguez, and Zoubin Ghahramani. Human activity recognition by combining a small number of classifiers. IEEE Journal of Biomedical and Health Informatics, 20(5):1342­1351, 2016.
[27] Michalis Petrakos, Ioannis Kannelopoulos, Jon Atli Benediktsson, and Martino Pesaresi. The effect of correlation on the accuracy of the combined classifier in decision level fusion. In IGARSS 2000. IEEE 2000 International Geoscience and Remote Sensing Symposium. Taking the Pulse of the Planet: The Role of Remote Sensing in Managing the Environment. Proceedings, volume 6, pages 2623­2625. IEEE, 2000.
[28] Gregor Pirs and Erik Strumbelj. Bayesian combination of probabilistic classifiers using multivariate normal mixtures. Journal of Machine Learning Research, 20:51­1, 2019.
[29] Martyn Plummer et al. JAGS: A program for analysis of Bayesian graphical models using Gibbs sampling. In Proceedings of the 3rd International Workshop on Distributed Statistical Computing, volume 124, pages 1­10. Vienna, Austria., 2003.
[30] Salil Prabhakar and Anil K Jain. Decision-level fusion in fingerprint verification. Pattern Recognition, 35 (4):861­874, 2002.
[31] Gonzalo Safont, Addisson Salazar, and Luis Vergara. Multiclass alpha integration of scores from multiple classifiers. Neural Computation, 31(4):806­825, 2019.
[32] Christine M Schubert, Nathan J Leap, Mark E Oxley, and Kenneth W Bauer Jr. Quantifying the correlation effects of fused classifiers. In Signal Processing, Sensor Fusion, and Target Recognition XIII, volume 5429, pages 373­383. International Society for Optics and Photonics, 2004.
[33] Edwin Simpson, Stephen Roberts, Ioannis Psorakis, and Arfon Smith. Dynamic Bayesian combination of multiple imperfect classifiers. In Decision Making and Imperfection, pages 1­35. Springer, 2013.
[34] Pawan Kumar Singh, Ram Sarkar, and Mita Nasipuri. Correlation-based classifier combination in the field of pattern recognition. Computational Intelligence, 34(3):839­874, 2018.
11

[35] Nisha Srinivas, Kalyan Veeramachaneni, and Lisa Ann Osadciw. Fusing correlated data from multiple classifiers for improved biometric verification. In 2009 12th International Conference on Information Fusion, pages 1504­1511. IEEE, 2009.
[36] Ashok Sundaresan, Pramod K Varshney, and Nageswara SV Rao. Copula-based fusion of correlated decisions. IEEE Transactions on Aerospace and Electronic Systems, 47(1):454­471, 2011.
[37] Oriol Ramos Terrades, Ernest Valveny, and Salvatore Tabbone. Optimal classifier fusion in a non-Bayesian probabilistic framework. IEEE Transactions on Pattern Analysis and Machine Intelligence, 31(9):1630­ 1644, 2009.
[38] Susanne Trick, Frank Jäkel, and Constantin A. Rothkopf. A correlated Dirichlet distribution. submitted, 2021.
[39] Kagan Tumer and Joydeep Ghosh. Theoretical foundations of linear and order statistics combiners for neural pattern classifiers. Technical report, IEEE Transactions on Neural Networks, 1996.
[40] Naonori Ueda, Yusuke Tanaka, and Akinori Fujino. Robust naive Bayes combination of multiple classifications. In The Impact of Applications on Mathematics, pages 141­155. Springer, 2014.
[41] Aydin Ulas¸, Olcay Taner Yildiz, and Ethem Alpaydin. Eigenclassifiers for combining correlated classifiers. Information Sciences, 187:109­120, 2012.
[42] Kalyan Veeramachaneni, Lisa Osadciw, Arun Ross, and Nisha Srinivas. Decision-level fusion strategies for correlated biometric classifiers. In 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, pages 1­6. IEEE, 2008.
[43] Robert L Winkler. Combining probability distributions from dependent information sources. Management Science, 27(4):479­488, 1981.
[44] Tzu-Tsung Wong. Generalized Dirichlet distribution in Bayesian analysis. Applied Mathematics and Computation, 97(2-3):165­181, 1998.
[45] Zhi-Hua Zhou. Ensemble methods: foundations and algorithms. CRC press, 2012.
12

