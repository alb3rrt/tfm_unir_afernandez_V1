Cleaning and Structuring the Label Space of the iMet Collection 2020
Vivien Nguyen* and Sunnie S. Y. Kim* Princeton University
{vivienn, sunniesuhyoung}@princeton.edu

arXiv:2106.00815v1 [cs.CV] 1 Jun 2021

Abstract
The iMet 2020 [22] dataset is a valuable resource in the space of fine-grained art attribution recognition, but we believe it has yet to reach its true potential. We document the unique properties of the dataset and observe that many of the attribute labels are noisy, more than is implied by the dataset description. Oftentimes, there are also semantic relationships between the labels (e.g., identical, mutual exclusion, subsumption, overlap with uncertainty) which we believe are underutilized. We propose an approach to cleaning and structuring the iMet 2020 labels, and discuss the implications and value of doing so. Further, we demonstrate the benefits of our proposed approach through several experiments. Our code and cleaned labels are available at https://github.com/sunniesuhyoung/ iMet2020cleaned.
1. Introduction
Fine-grained art attribute recognition is a novel area of research within fine-grained visual categorization (FGVC) that concerns images with low inter-class variation and high intra-class variation. It is an important area that can assist museums create and maintain their artwork collections. Since there is a limited number of museum experts, if a visual recognition system can automatically label new images with relevant attributes and optionally have the experts verify the labels, we can likely reduce costs of the cataloging process by a great amount. It can also enable searches for visually related artworks and serve as a useful tool for art history and cultural heritage studies.
The iMet Collection [22] is a dataset for fine-grained art attribute recognition introduced in the 6th FGVC Workshop at CVPR 2019. It is the first high-quality artwork image dataset with professional photographs of artworks from The Metropolitan Museum of Art (The Met) and research-grade attribute labels curated or verified by experts. However, while there have been two iMet competitions and another
*Equal contribution. A shorter version of this work was accepted to the CVPR 2021 The Eight Workshop on Fine-Grained Visual Categorization.

in progress, we were able to find surprisingly little discussion on the dataset, task, or proposed solutions.
In this work, we study the unique challenges of finegrained art attribute recognition presented by the 2020 version of the iMet Collection dataset (iMet 2020). We start with identifying the unique properties of the dataset (Section 3). Specifically, we document instances where the annotations are incomplete, inconsistent, or redundant. Based on our observations, we argue that we should clean and structure its label space and discuss our motivations (Section 4). We then propose concrete suggestions to improve the iMet 2020 label space (Section 5). Finally, we demonstrate the benefits of our proposed changes through several experiments (Sections 6 and 7).
2. Related work
Fine-grained art attribute recognition: Fine-grained art attribute recognition is a novel area of research within FGVC. It is also a relatively new area in artwork recognition where most previous works in the space focus on object retrieval in paintings [5], 3D-based content retrieval in artworks [10], or artistic style classification [17, 18]. Recently, many museums have made their collection available online.1 Most don't include images or attribute labels, however, so they are not suitable for art attribute recognition.
Two datasets similar to iMet 2020 are the Behance Artistic Media (BAM) [21] dataset and the WikiArt2 online database. BAM consists of 65 million images of contemporary, mostly non-photorealistic artworks from Behance. WikiArt primarily, but not exclusively, contains paintings, and is similar to iMet 2020 in that it centers academic art and contains several attribute labels, such as country, date, style, and medium. The level of specificity and tags available for each individual artwork also vary drastically. In contrast to both of these, iMet 2020 contains a smaller but more diverse set of works and objects. iMet 2020 is also annotated with a much larger and more diverse set of attributes compared to either BAM or WikiArt.
1https://www.artnome.com/art-data 2https://wikiart.org

1

Figure 1: The iMet 2020 dataset has a long-tailed distribution over the 3,474 attributes.
Hierarchical classification in cultural heritage: Belhi et al. [1] argue that a hierarchical classification framework is better suited for the diversity of objects and label availability in cultural heritage. They propose a two-stage framework where they first classify each artwork's type, then further classify attributes such as artist, year, genre, style, and medium for paintings. They demonstrate their framework on the WikiArt, Met, and Rijksmuseum datasets. Similarly, we argue that hierarchical classification is more suitable for fine-grained art attribute recognition with the iMet 2020 dataset. However, we note that [1] is concerned with a two-level hierarchy of artwork type and attributes and treats all attributes in the same way, whereas we propose structuring and doing hierarchical classification of the attributes.
Learning with structured label spaces: Deng et al. [6] and Ding et al. [9] model label relations with Hierarchy and Exclusion graphs and demonstrate that they improve classification performance. More recent work by Hu et al. [13] models label relations with a Structured Inference Network and report further improvements. Dhall [8] studies different methods of injecting label-hierarchy information to visual classifiers and proposes jointly embedding labels and images using embedding models. We expect to gain similar improvements when we leverage the structured label space of iMet 2020 during learning, and believe developing such methods is a promising direction for future work.
3. Analysis of iMet 2020's label space
The iMet 2020 dataset consists of 3,474 binary attribute labels for 226,966 professional photographs of artworks supplied by The Met. The attribute annotations are from subject matter experts or vendor annotators. The annotators were provided with a label taxonomy and access to The Met's online collection, and were instructed to annotate labels which can be visually inferred and to avoid adding labels already present [22]. We highlight that the dataset has a long-tailed distribution over the attributes (Figure 1). Fur-

Figure 2: Positive labels are sparse in the iMet 2020 dataset. Although there are 3,474 attributes in the label space, the median number of annotated attributes per sample is 4.
thermore, positive labels are sparse; the median number of annotated attributes per image is 4 (Figure 2).
The 2019 dataset paper [22] describes the iMet Collection as a "high quality, research grade dataset," while the organizers of the 2020 competition note that participants should consider the annotations noisy.3 We find that both characterizations are true, but we believe the incompleteness and noisiness of the iMet Collection labels inhibit its effectiveness as a dataset for either art historical scholarship or deployable museum systems.
While the attributes that an artwork is annotated with are almost always accurate, we have found many instances where the annotations or labels are incomplete, inconsistent, or redundant. In this section, we document some concrete examples of these issues for each of the five categories. We also explicitly note other unique or ambiguous aspects of iMet 2020 for future users of the dataset.
country (100 attributes): First, it is unclear if the attribute label refers to the country where the artwork was (presumably) created, where it was found, or something else. Furthermore, we found that labels vary in specificity (e.g., central italy, present-day greece) and some explicitly encode uncertainty (e.g., egypt or iraq). We discuss inconsistencies surrounding such labels.
For example, there exist labels for both italy and central italy. Differentiating the two may be useful: Italy was not a nation-state until 1861, or annotators may want to provide the specific provenance of the work, if known. However, there are inconsistencies between iMet 2020 and The Met's online collection, such as the "TwoHandled Jar" from Central Italy according to the online collection4 but only tagged italy in iMet 2020 (Figure 3a).
We interpret that labels such as egypt or iraq imply the provenance is uncertain, and that experts believe the art-
3https://www.kaggle.com/c/imet-2020-fgvc7/data 4https://www.metmuseum.org/art/collection/search/468166

2

Figure 3: Examples of inconsistencies in iMet 2020. While this is a handpicked subset, we find these inconsistencies are quite pervasive throughout the dataset; certainly more than is implied by either the competition or the original dataset paper [22].

(a) An early 15th C. Italian jar, provenance listed as Florence, Italy (Central Italy) on The Met's website, but tagged only as country:italy in iMet 2020.

(b) This jug is listed as "Possibly made in England; Possibly made in United States" on The Met's website but tagged with both countries in iMet 2020.

(c) This Worcester porcelain features a variety of flowers, but is only tagged with tags:roses, while other flower tags exist, as well as a generic catch-all, tags:flowers.

work could be from either country. We note that samples tagged as egypt or iraq in iMet 2020 are not tagged with the egypt or iraq. However, other works with similar uncertainty according to The Met's online collection are tagged with their individual labels to represent multiple possible origins. The "Cream Jug" described as possibly made in England or the United States in the online collection5 is tagged both england and united states in the dataset (Figure 3b).
culture (681 attributes): Similarly, we found culture attribute labels to have substantial overlap. For example, there are at least 13 attribute labels related to India (e.g., india, india (bengal) or bangladesh, india (madhya pradesh), indian or nepalese) with varying levels of specificity and uncertainty.
As expected, country and culture attribute labels are often correlated. This correlation does not imply that the labels are redundant, but for evaluation purposes, it may be useful to represent the relationship between related countries and cultures. We discuss this point further in Section 4.
dimension (5 attributes): The five dimension attributes are tiny, small, medium, large, and very large. 101,954 of 142,119 images (72%) are given one of the 5 labels, so they are some of the most common attributes in the dataset. The dimension attributes were added in 2020, hence they are not discussed in the 2019 dataset paper [22]. We were unable to find discussion around the binning of dimension attributes. The Met's website lists exact dimensions of items, while these five attributes are relative labels.
medium (1,920 attributes): Medium attributes describe the materials utilized in the object's creation and are by far the noisiest. We found more than 100 pairs of attributes that
5https://www.metmuseum.org/art/collection/search/2601

have the same meaning; many are pairs whose spelling differs by one letter such as (watercolor, watercolour). There are also typos (e.g., (commercial lithograph, commerical lithograph)) and encoding issues (e.g., (copper-gold alloy (shakud\x8d¯o), copper-gold alloy (shakud¯o))).
Once again, there are varying levels of specificity and uncertainty among these attributes. For example, there is black chalk, but also black chalk and charcoal, black chalk on blue paper, and black chalk or graphite. Of the 682 artworks in the training set tagged black chalk, only one is also tagged black; of the 6 objects in the training set tagged black chalk on blue paper, none are tagged with black chalk.
tags (768 attributes): Finally, there are tags that cover a wide range of subjects, such as religious icons, historical events, plants and animals. While less overlapping than culture or medium, we still find many hierarchical relationships, such as for named entities (goddesses as well as aphrodite), objects (flowers as well as roses), and events (wars as well as Trojan war), among others.
While a more general tag such as flowers could either be used for all flowers or unspecific flowers, we found that usage is inconsistent (Figure 3c).
4. Motivations for label cleaning/structuring
Based on our observations in Section 3, we propose two improvements to the labels of iMet 2020. The first is more straightforward: merge identical labels, annotate super/sublabels, ensure that samples are annotated as completely as possible, and develop a consistent encoding of uncertainty. We refer to this as cleaning and completing the iMet 2020 labels. The second is more nuanced: we should organize the label space itself, by providing both hierarchi-

3

cal structure to the labels as well as looser relational annotations. We refer to this as structuring the iMet 2020 labels. In this section, we discuss our motivations for both, then describe our proposed approach in Section 5.
4.1. Why clean the iMet 2020 labels?
Concretely, if we merge identical attributes and annotate supercategories, we can increase the number of samples for many labels. For example, there are 8 training images with the bronze gilt label and 9 with bronze-gilt, with no overlap between the images. When we merge the two labels, we get more images for bronze(-)gilt. Similarly, we can add a supercategory label for the images with its subcategory label (e.g., for images labeled black chalk on blue paper also label its supercategory, black chalk). This will increase the number of samples for the supercategory and likely lead to improved recognition performance of it. These simple modifications alleviate some of the challenges associated with label sparsity and long-tailed distributions.
While the existing attribute labels are not incorrect, they are certainly incomplete and noisier than the competition description suggests. Completing and cleaning the iMet labels will reduce inconsistencies, which can provide clearer training signals to models trained on the dataset. Perhaps more importantly, fixing the test set labels will improve iMet's quality as a benchmarking dataset [19, 20].
Having a complete set of labels could also enable us to apply hierarchy extraction algorithms [3] to automatically determine at least some label relationships.
4.2. Why structure the iMet 2020 labels?
Leveraging label relations will lead to better tools for artwork documentation and curation. After identifying more specific hierarchies beyond the current five categories and structuring the label space, future annotators will be able to annotate a more clean and comprehensive set of attributes for each artwork. For example, an annotator could visually traverse a label hierarchy to easily locate or discover tags.
We believe leveraging semantic relations between labels will lead to more accurate and consistent recognition systems. Aside from the benefits from cleaning the iMet labels, modeling label relations can lead to more consistent predictions. For example, we can enforce simultaneous prediction of a supercategory whenever it predicts its subcategory in an image, or enforce mutual exclusion over a set of attributes (such as dimension). Though we usually assume that a recognition model will learn these co-occurrences naturally, previous works [6, 8, 9, 13] have demonstrated that modeling label relations lead to improvements in classification performance.
Nevertheless, it's possible that a structured label space does not directly yield immediate improvements in the F2

score, which is currently employed as the metric for the iMet competition. In particular, the F2 score prioritizes recall over precision which may incentivize prediction patterns such as predicting multiple dimensions in the "hopes" of getting one of them right.
However, using a hierarchical label space itself lends itself not only to different quantitative performance measures, but also more qualitative insights into a model's performance [16]. For example, modeling weak relations between labels (such as the relationship between a geographic country and its corresponding culture) can allow us to select models based on the kinds of errors that they make. Intuitively, some errors are more wrong than others, but that goes uncaptured in a flat label space where every label is given equal weight, relative to the others.
We argue that a diverse set of performance measures is necessary to meaningfully evaluate a FGVC system.
5. Proposed approach
We propose the following cleaning and structuring of the iMet 2020 labels. Our goal is not to learn a hierarchy from existing labels (which would only be possible after cleaning) but to correct the inconsistencies and propose a general approach for handling likewise noisy label spaces in FGVC.
Identify and merge identical attributes: As noted before, we found more than 100 pairs of attributes that have the same meaning in the medium category alone. We found these pairs through fuzzy string matching and manual verification. A relatively naive strategy of calculating the similarity ratio, based on Levenshtein edit distance, of all pairs of attributes worked well. In Section 6.3 we demonstrate performance gains from the merging.
Identify super/subcategories and label supercategories in the presence of subcategories: We can also use string matching supplemented with manual verification to identify supercategories (e.g., black) and subcategories (e.g., black ink, black chalk). Note that the subcategories may continue branching into more specific attributes, and also that hierarchy need not form a tree. Both black and chalk could be considered parents of black chalk. In Section 6.4 we demonstrate that labeling the supercategory can provide a helpful training signal.
Encode and relationships as separate labels: 334 of 3,474 attributes have "and" in their names. We argue these attributes should be consistently separated into two (or more) labels. That is, wool and silk should be separated into wool and silk. We differentiate this from the above case, where a supercategory subsumes the subcategory. In Section 6.5 we show that encoding and relationships as separate labels yields performance improvements.
Encode or relationships: 117 of 3,474 attributes have "or"

4

in their names. To the best of our knowledge, there is no established way of encoding or relationships. One possibility is to label each attribute separately, but encode the or relationship in a label relations graph or some other structure so that we can apply a special loss function. For example, for french or spanish, we can treat a model's prediction correct when it has at least one of french and spanish, and incorrect when it has neither. In Section 7.1 we demonstrate this simple evaluation scheme change.
Encode mutually exclusive relationships: Some attributes, such as the five dimensions attributes, are mutually exclusive in nature. We believe mutual exclusion is an important semantic relationship that should be encoded, such that we can get valid predictions. In Section 6.6, we conduct a brief case study with the dimensions attributes.
Use hierarchical performance measures: Desirable properties of performance measures (PMs) for hierarchical classification are outlined in [15], and hierarchical PMs are compared to flat performance measures further in [4, 2]. PMs based on distance, depth, semantics, or hierarchy are known to provide more discriminative power between types of errors. Based on these insights, we explore graph-based performance measures in Section 7.2.
6. Experiments at training time
We conduct several proof-of-concept experiments to demonstrate the benefits of our proposed changes. We describe our experimental setup in Section 6.1, and the baseline model and its performance in Section 6.2. We then discuss experiments that involve altering the dataset and/or the training procedure in Sections 6.3­ 6.6.
6.1. Experimental setup
Dataset: We use the iMet 2020 dataset which contains 3,474 attribute labels for 226,966 images (142,119 for training and 84,847 for testing). We do a random 80-20 split of the original training set and use the 80 split as our training set (113,694 images) and the 20 split as our validation set (28,425 images). We only use the test set to report final results. We note that we use a smaller version of the test set (25,958 images) based on the sample submission.csv file from the iMet 2020 competition.
Implementation and training details: For all attribute classifiers, we use ResNet-50 [12] pre-trained on ImageNet [7] classification as the base architecture. We train them using the Adam optimizer, a batch size of 200, and a learning rate of 1e-4 with standard step decay for up to 25 epochs. We use the binary cross entropy (BCE) loss to train all models, unless noted otherwise.6
6Our implementation is inspired by Yandex Praktikum's code shared in the competition site: https://www.kaggle.com/alimbekovkz/

Table 1: Analysis of the baseline model. When applicable, we repeat the experiment three times with different random seeds, and report the mean and its standard error.

Metric Micro-averaged F2 score Macro-averaged F2 score # of classes with NaN F2 score # of classes with zero F2 score # of classes with positive F2 score Micro-averaged accuracy Overall deviation of F2 score Per-class deviation of F2 score

Value 64.96 ± 0.13 18.65 ± 0.55
48 1857 1569 99.85 ± 0.00 28.33 4.14

Evaluation details: We use 0.1 as the decision threshold for all classes to make the final predictions. Our main evaluation metric is the micro-averaged F2 score, the official evaluation metric for the iMet competitions.
6.2. Baseline model
Our baseline model is a multi-label attribute classifier trained with the BCE loss on the unmodified training set with 3,474 classes (attributes). We train it three times with different random seeds. Table 1 summarizes the results.
Our baseline model achieves a micro-averaged F2 score of 64.96. Note that this score is much higher than the macro-averaged F2 score of 18.65. This difference is explained by the high number of classes with zero or NaN F2 scores and the histograms of per-class F2 scores in Figure 4 which show that the majority of the classes have low F2 scores. Still, our model is able to achieve a near-perfect accuracy of 99.85 because labels are sparse. Nonetheless, accuracy is neither a robust nor a discriminative metric; an all-negative prediction achieves an even higher accuracy of 99.86. We only report micro-averaged accuracy to highlight the extreme sparsity of positive labels.
Following the suggestion by Gwilliam et al. [11], whose work quantifies variance in FGVC results, we also calculate the overall deviation (i.e. standard deviation of the 3×3,474 per-class F2 scores) and the per-class deviation (i.e. standard deviation of per-class F2 scores across 3 runs, averaged over all classes). Consistent with the trend shown in the Figure 4 histograms, the overall deviation is high (28.33). The per-class deviation of 4.14 also suggest that per-class F2 scores fluctuate quite a bit across training runs.
6.3. Merge of identical attributes
In this section, we explore the effect of merging identical attributes. In each experiment, we merge watercolor and watercolour, emerald and emeralds, garnet and garnets, and train a new multi-label classifier that has one
yandex-praktikum-pytorch-train-baseline-lb-0-699.

5

Figure 4: Histograms of all (left) and positive-only (right) per-class F2 scores of the baseline model.

Table 3: We report F2 score (mean and standard error across three runs) for all attributes, the black supercategory, and 83 subcategories before and after labeling the supercategory attribute. Modification improves the F2 score on all sets.

Classes All 3,474 1 supercategory 83 subcategories

Baseline 64.96 ± 0.13 39.69 ± 5.18 65.80 ± 0.92

New model 65.09 ± 0.23 77.62 ± 1.01 67.10 ± 0.61

Table 2: We report the number of positive training images and F2 score (mean and standard error across three runs) before and after merging identical attributes. Modifications maintain or improve the F2 score for all three attribute pairs.

Attribute
watercolor watercolour
emerald emeralds
garnet garnets

Num of images

Before After

2094 8

2102

21 36

57

62 34

96

F2 score

Before

After

82.42 ± 0.82 83.00 ± 0.60

4.07 ± 5.75 13.38 ± 10.28

16.33 ± 2.11 21.41 ± 3.33

less attribute than the baseline (i.e. 3473 attributes). In Table 2, we report the number of positive training images, accuracy, and F2 score for each attribute pair before and after the merge. We note that "before" refers to the baseline model and "after" refers to the new model, and that all models have comparable overall F2 score of 64.80­66.43.
We see that accuracy is not a discriminative metric for these pairs because the true negatives dominate in all cases; however, the F2 scores show that merging the identical attributes leads to improved performance for all three pairs. Hence, we conclude that, as expected, merging identical attributes is a simple yet effective strategy to gain performance improvements and a cleaner label space.
6.4. Labeling of supercategory attributes
Next, we explore the effect of labeling supercategory attributes through one example set of super- and subcategories. In this case study, we treat black as the supercategory and 83 other attributes with black in their names (e.g., black chalk, almost black) as the subcategories. Whenever a subcategory is labeled, we also label the supercategory. This modification increases the number of samples with black in the training set from 169 to 3767.
We train a model on this modified training set, and in Table 3, compare it to the baseline on all 3474 attributes, the supercategory black, and the 83 subcategories. Note that we evaluate the baseline on the unmodified test set and the new model on the modified test set so that the training and

test distributions are consistent in each case. We see that both models perform similarly overall, but the new model performs better on the supercategory and the subcategories. The results suggest that the simple modification of labeling the supercategoy provides a more helpful training signal.
6.5. Encoding of and relationships
We also experiment with encoding and relationships as separate labels. For the 334 attributes that have and in their names, we separate their names into smaller tokens using and as the separator. For 170 attributes, all of their smaller tokens are existing attributes (e.g., german and italian is split into german and italian both of which are attributes in iMet 2020). For 30 attributes, none of their smaller tokens are existing attributes (e.g., weights and measures). For the remaining 134 attributes, only a part of their smaller tokens are existing attributes.
Our modification is simple. Whenever a smaller token is an existing attribute, we label that attribute. To reduce redundancy, we remove the 170 attributes whose smaller tokens are all existing attributes. We then train a model with 3, 474 - 170 = 3, 304 attributes. This model achieves a micro-averaged F2 score of 66.03 ± 0.50, higher than the baseline's 64.96 ± 0.13, suggesting that encoding and relationships as separate labels is useful in practice.
6.6. Mutual exclusion of dimension attributes
Next, we explore the effects of utilizing the knowledge that dimensions are mutually exclusive. We don't modify any of the labels, since the dimension attributes are already exclusive (or unlabeled). We do make a change to how the attribute classifier is trained. Specifically, we enforce mutual exclusion by applying the softmax operator across the five dimensions attributes before computing the BCE loss. We call this new classifier the "exclusive dim model."
For clarity, we restrict our analysis to (1) only the dimension attributes, and (2) only the test samples for which a ground truth dimension label exists. We analyze three sets of predictions: (1) predictions from the baseline model where scores are thresholded at 0.1 (such that multiple or no predictions are also possible), (2) predictions from the baseline model where the top score is picked (such that the

6

Table 4: Results on enforcing mutual exclusion among dimensions attributes. We report accuracy and F2 score. Baseline model is the best performing according to the F2 score, but enforcing a single prediction (from the baseline model) yields the best accuracy.

Prediction type Baseline model Baseline, top dim only Exclusive dim model

Accuracy 43.16 ± 0.37 42.58 ± 0.26 42.49 ± 0.17

F2 score 72.23 ± 0.06 59.27 ± 0.36 59.18 ± 0.22

Figure 5: Subset of our country label relations graph. In this subgraph, china has no graph neighbors, so no partial credit can be given for any answer except the exact true label. On the other hand, a path exists between french and present-day france via france, and so have some notion of "similarity".

sudan

sudan and egypt

egypt

egypt or syria

syria

model will always predict exactly one dimension), and (3) predictions from the exclusive dimensions model where the top score is picked (again, such that the model will always predict exactly one dimension). Accuracy and F2 scores for these predictions are shown in Table 4.
The difference between accuracy and F2 score for the baseline model suggests that not restricting the model does give some benefit. The baseline model seems to be able to pick up some extra correct predictions if not restricted to only one. However, all three models perform about the same when considering accuracy. This insight can help researchers tune models along a particular attribute vertical.
7. Experiments at evaluation time
While we hope that incorporating hierarchical knowledge/relationships can improve any model, realistically, many models suffer in performance after incorporating knowledge graphs due to the "preferences" of flat performance measures. Thus, it's important to also consider alternate evaluation metrics.
Furthermore, it's possible that structuring the label space only at evaluation time, without changing the training of the model, can give us new measures to gain new insights into model performance. In this section, we explore such experiments and interpret how comparing these measures with the flat F2-score can give further intuition into where the model is making errors.
7.1. Encoding of or relationships
Similar to how we analyzed and relationships, for the 117 attributes that have or in their names, we separate their names into smaller tokens using or as the separator. For 77 attributes, all of their smaller tokens are existing attribues (e.g., british or irish). For 11 attributes, none of their smaller tokens are existing attributes (e.g., spanish or mexican). For the remaining 29 attributes, only a part of their smaller tokens are existing attributes. We note that most or attributes are country or culture attributes.
We explore changing the evaluation scheme for or relationships. That is, for the attribute british or irish, we consider a prediction is correct if it includes one of

french

france
presentday
france

france and
germany

germany china

british or irish, british, or irish. For the baseline model, this evaluation scheme changes the F2 score of the 117 or attributes from 43.35 ± 1.41 to 73.45 ± 0.28.
Naturally, these scores are not directly comparable; for one, we only alter the way true positives are counted, and not false positives or false negatives. However, the increase in score itself provides valuable insight into the model's performance. In particular, this indicates that the model is predicting many related attributes, even if not correctly predicting the or attribute. Depending on the desired results, researchers can re-label samples, re-encode uncertainty, or focus on teaching the model to learn to discriminate between certain/uncertain samples.
7.2. Using graph-based performance measures
As described in Section 4.2, intuitively, some errors are better (or worse) than others. In this section, we describe a metric that gives "partial credit" to models for predicting "reasonable" (but wrong) labels.
We focus on country attributes and construct a graph to model connections between existing attributes. To make the process as straightforward as possible, we focus on attributes involving and or or connections, and attributes with "obvious" relationships. For example, consider the label sudan and egypt: since sudan and egypt already exist in the label set, we can draw an undirected edge between each of those labels and sudan and egypt. An example of an `obvious" relationship between labels french and france. We also draw edges between (united kingdom, england) and (united kingdom, scotland). A small subset of the resulting graph can be seen in Figure 5.
Kosmopoulos et al. [16] review and extend several met-

7

rics for evaluating hierarchical classification systems. Their work includes multi-label classification problems, but focuses on non-cyclical and explicit hierarchies, i.e. a directed graph where descendants and ancestors can be established for a particular node.
The graph we create is undirected; thus we can't use metrics that evaluate the performance of the model based on the level of specificity the model is able to predict. However, we can simply use the distance in the graph between the true and predicted label as a proxy for similarity. Likewise, we can punish the model less harshly for predicting incorrect but close labels.
To be precise, we redefine the calculation of true positives (TP), false positives (FP), and false negatives (FN) which are then used to calculate the F2 score. Let d(T, P ) be the distance in the graph between true label T and predicted label P . d(T, P ) = 0 implies that T == P , and d(T, P ) =  implies no path exists between T and P . We compute for each sample:

1

T P = max

(1)

P d(T, P ) + 1

T

1

F N = max(1 -

)

(2)

P

d(T, P ) + 1

T

1

FP =

(3)

d(T, P ) + 1

P

and calculate the F2 score with these counts. We refer to this metric as the "graph" F2 score, and compare it to the "normal" F2 score.

7.2.1 Comparing metrics
To compare metrics, we use two notions suggested by Huang and Ling [14], consistency and discriminancy, defined below. Suppose we want to compare two different performance measures, f and g, on their evaluation of two different models, A and B.
Definition 1: Consistency: Two measures f and g are strictly consistent with one another if there exist no models A and B where f (A) > f (B) but g(A) < g(B). We can also loosen this definition to establish a statistical degree of consistency:
R = {(A, B)|f (A) > f (B), g(A) > g(B)} (4)
S = {(A, B)|f (A) > f (B), g(A) <= g(B)} (5)

|R|

DoC(f, g) =

(6)

|R| + |S|

DoC(f, g) = DoC(g, f )

(7)

Definition 2: Discriminancy: A performance measure f is strictly more discriminative than a measure g if there exists

Figure 6: Parts of the graph that are flat are intervals where that metric has low discriminancy. Particularly around threshold 0.1, we see that the graph metric has improved discriminancy (i.e. a slightly steeper slope).

models A and B for which f (A) > f (B) but g(A) = g(B), and no models A and B for which g(A) > g(B) but f (A) = f (B). Again, we can establish a statistical degree7 of discriminancy:
P = {(A, B)|f (A) > f (B), g(A)  g(B)} (8)
Q = {(A, B)|g(A) > g(B), f (A)  f (B)} (9)

|P |

DoD(f, g) =

(10)

|Q|

1

DoD(f, g) =

(11)

DoD(g, f )

According to [14], if DoC(f, g) > .5 and DoD(f, g) > 1, then intuitively, metric f is better than metric g.

7.2.2 Graph-based metrics are experimentally consistent and more discriminative
To quickly obtain a large number of models to compare the graph F2 score and normal F2 core, we use a single baseline ResNet-50 model (as described in Section 6.1) but vary the prediction threshold between [0.0025, 0.5]. We then compute that DoD(graph, normal) = 1.77 and DoC(graph, normal) = 0.92. Thus, both metrics are consistent with one another, and the graph F2 score is slightly more discriminative than the normal F2 score in this setting.
8. Conclusion
In this work, we studied the unique challenges of finegrained art attribute recognition presented by the iMet Collection 2020 [22] dataset. We first documented the unique
7Huang and Ling [14] use a strict equality in these equations.

8

properties of the dataset for each of the five attribute categories. We then discussed the motivations for cleaning and structuring the label space, and proposed an approach for them. Through several experiments, we also demonstrated the benefits of our proposed changes. We hope our work helps future users of the dataset and practitioners in finegrained art attribute recognition, and serves as a useful resource in handling other noisy label spaces in FGVC.
Acknowledgments: This work was done as a final project for the COS 529 Advanced Computer Vision course at Princeton University. We thank our advisors Szymon Rusinkiewicz and Olga Russakovsky for their support. We also thank members of the UC Berkeley Ng Lab and the Princeton Visual AI Lab for their helpful comments. Finally, we thank the organizers of the iMet Competition, particularly Chenyang Zhang, and the FGVC8 workshop reviewers for suggestions and support.
References
[1] Abdelhak Belhi, Abdelaziz Bouras, and Sebti Foufou. Towards a hierarchical multitask classification framework for cultural heritage. In International Conference on Computer Systems and Applications (AICCSA), 2018. 2
[2] Florian Brucker, Fernando Benites, and Elena Sapozhnikova. An empirical comparison of flat and hierarchical performance measures for multi-label classification with hierarchy extraction. In International Conference on KnowledgeBased and Intelligent Information and Engineering Systems (KES), 2011. 5
[3] Florian Brucker, Fernando Benites, and Elena Sapozhnikova. Multi-label classification and extracting predicted class hierarchies. Pattern Recognition, 44(3):724­738, 2011. 4
[4] Eduardo Costa, Ana Lorena, ACPLF Carvalho, and Alex Freitas. A review of performance evaluation measures for hierarchical classifiers. In AAAI 2007 Workshop: Evaluation Methods for Machine Learning II. 5
[5] Elliot J. Crowley and Andrew Zisserman. In search of art. In ECCV 2014 Workshop on Computer Vision for Art Analysis. 1
[6] Jia Deng, Nan Ding, Yangqing Jia, Andrea Frome, Kevin Murphy, Samy Bengio, Yuan Li, Hartmut Neven, and Hartwig Adam. Large-scale object classification using label relation graphs. In European Conference on Computer Vision (ECCV), 2014. 2, 4
[7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In Conference on Computer Vision and Pattern Recognition (CVPR), 2009. 5
[8] Ankit Dhall. Learning representations for images with hierarchical labels, 2020. 2, 4
[9] Nan Ding, Jia Deng, Kevin Murphy, and Hartmut Neven. Probabilistic label relation graphs with ising models. In International Conference on Computer Vision (ICCV), 2015. 2, 4

[10] David Gorisse, Matthieu Cord, Michel Jordan, Sylvie Philipp-Foliguet, and Frederic Precioso. 3d content-based retrieval in artwork databases. In 3DTV Conference, 2007. 1
[11] Matthew Gwilliam, Adam Teuscher, Connor Anderson, and Ryan Farrell. Fair comparison: Quantifying variance in results for fine-grained visual categorization. In Winter Conference on Applications of Computer Vision (WACV), January 2021. 5
[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 5
[13] Hexiang Hu, Guang-Tong Zhou, Zhiwei Deng, Zicheng Liao, and Greg Mori. Learning structured inference neural networks with label relations. In Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 2, 4
[14] Jin Huang and C.X. Ling. Using auc and accuracy in evaluating learning algorithms. IEEE Transactions on Knowledge and Data Engineering, 17(3):299­310, 2005. 8
[15] Svetlana Kiritchenko, Stan Matwin, Richard Nock, and A. Fazel Famili. Learning and evaluation in the presence of class hierarchies: Application to text categorization. In Advances in Artificial Intelligence, pages 395­406, Berlin, Heidelberg, 2006. Springer Berlin Heidelberg. 5
[16] Aris Kosmopoulos, Ioannis Partalas, Eric Gaussier, Georgios Paliouras, and Ion Androutsopoulos. Evaluation measures for hierarchical classification: A unified view and novel approaches. In Data Mining and Knowledge Discovery, 2015. 4, 7
[17] Adrian Lecoutre, Benjamin Negrevergne, and Florian Yger. Recognizing art style automatically in painting with deep learning. In Asian Conference on Machine Learning (ACML), 2017. 1
[18] Thomas Edward Lombardi. The classification of style in fine-art painting, 2005. 1
[19] Curtis G. Northcutt, Anish Athalye, and Jonas Mueller. Pervasive label errors in test sets destabilize machine learning benchmarks, 2021. 4
[20] Curtis G. Northcutt, Lu Jiang, and Isaac L. Chuang. Confident learning: Estimating uncertainty in dataset labels, 2021. 4
[21] Michael J. Wilber, Chen Fang, Hailin Jin, Aaron Hertzmann, John Collomosse, and Serge Belongie. BAM! the behance artistic media dataset for recognition beyond photography. In International Conference on Computer Vision (ICCV), 2017. 1
[22] Chenyang Zhang, Christine Kaeser-Chen, Grace Vesom, Jennie Choi, Maria Kessler, and Serge Belongie. The iMet Collection 2019 Challenge Dataset, 2019. 1, 2, 3, 8

9

